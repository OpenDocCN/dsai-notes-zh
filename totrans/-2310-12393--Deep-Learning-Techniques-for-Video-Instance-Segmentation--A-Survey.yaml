- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:36:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:36:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2310.12393] Deep Learning Techniques for Video Instance Segmentation: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2310.12393] 视频实例分割的深度学习技术：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.12393](https://ar5iv.labs.arxiv.org/html/2310.12393)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.12393](https://ar5iv.labs.arxiv.org/html/2310.12393)
- en: 'Deep Learning Techniques for Video Instance Segmentation: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频实例分割的深度学习技术：综述
- en: Chenhao Xu [chenhao.xu@deakin.edu.au](mailto:chenhao.xu@deakin.edu.au) Chang-Tsun Li
    [changtsun.li@deakin.edu.au](mailto:changtsun.li@deakin.edu.au) Yongjian Hu [eeyjhu@scut.edu.cn](mailto:eeyjhu@scut.edu.cn)
    Chee Peng Lim [chee.lim@deakin.edu.au](mailto:chee.lim@deakin.edu.au) Douglas Creighton
    [douglas.creighton@deakin.edu.au](mailto:douglas.creighton@deakin.edu.au)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 陈浩 Xu [chenhao.xu@deakin.edu.au](mailto:chenhao.xu@deakin.edu.au) 常云津 Li [changtsun.li@deakin.edu.au](mailto:changtsun.li@deakin.edu.au)
    庸建 Hu [eeyjhu@scut.edu.cn](mailto:eeyjhu@scut.edu.cn) 李志鹏 Lim [chee.lim@deakin.edu.au](mailto:chee.lim@deakin.edu.au)
    道格拉斯·克雷顿 Creighton [douglas.creighton@deakin.edu.au](mailto:douglas.creighton@deakin.edu.au)
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Video instance segmentation, also known as multi-object tracking and segmentation,
    is an emerging computer vision research area introduced in 2019, aiming at detecting,
    segmenting, and tracking instances in videos simultaneously. By tackling the video
    instance segmentation tasks through effective analysis and utilization of visual
    information in videos, a range of computer vision-enabled applications (e.g.,
    human action recognition, medical image processing, autonomous vehicle navigation,
    surveillance, etc) can be implemented. As deep-learning techniques take a dominant
    role in various computer vision areas, a plethora of deep-learning-based video
    instance segmentation schemes have been proposed. This survey offers a multifaceted
    view of deep-learning schemes for video instance segmentation, covering various
    architectural paradigms, along with comparisons of functional performance, model
    complexity, and computational overheads. In addition to the common architectural
    designs, auxiliary techniques for improving the performance of deep-learning models
    for video instance segmentation are compiled and discussed. Finally, we discuss
    a range of major challenges and directions for further investigations to help
    advance this promising research field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 视频实例分割，也称为多目标跟踪和分割，是一个新兴的计算机视觉研究领域，始于2019年，旨在同时检测、分割和跟踪视频中的实例。通过有效分析和利用视频中的视觉信息来处理视频实例分割任务，可以实现一系列计算机视觉应用（如人类动作识别、医学图像处理、自动驾驶导航、监控等）。随着深度学习技术在各种计算机视觉领域中占据主导地位，提出了大量基于深度学习的视频实例分割方案。本综述提供了对视频实例分割的深度学习方案的多方面视角，涵盖了各种架构范式，并对功能性能、模型复杂性和计算开销进行了比较。除了常见的架构设计，还汇编并讨论了用于提高深度学习模型在视频实例分割中性能的辅助技术。最后，我们讨论了该领域主要的挑战和进一步研究的方向，以帮助推进这一有前景的研究领域。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'deep learning , video instance segmentation , multi-object tracking and segmentation
    , video segmentation , instance segmentation^†^†journal: Pattern Recognition\affiliation'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，视频实例分割，多目标跟踪和分割，视频分割，实例分割^†^†期刊：模式识别\affiliation
- en: '[label1]organization=School of Information Technology, Deakin University,city=Geelong,
    postcode=3216, state=VIC, country=Australia \affiliation[label2]organization=School
    of Electronic and Information Engineering, South China University of Technology,city=Guangzhou,
    postcode=510641, state=Guangdong, country=China \affiliation[label3]organization=Institute
    for Intelligent Systems Research and Innovation, Deakin University,city=Geelong,
    postcode=3216, state=VIC, country=Australia'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[label1]组织=信息技术学院，迪肯大学，城市=吉朗，邮政编码=3216，州=维多利亚州，国家=澳大利亚 \affiliation[label2]组织=电子与信息工程学院，华南理工大学，城市=广州，邮政编码=510641，州=广东省，国家=中国
    \affiliation[label3]组织=智能系统研究与创新学院，迪肯大学，城市=吉朗，邮政编码=3216，州=维多利亚州，国家=澳大利亚'
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Extending from image segmentation [[1](#bib.bib1), [2](#bib.bib2)], Video Instance
    Segmentation (VIS) was initially proposed in [[3](#bib.bib3)] in 2019\. In contrast
    to image segmentation, which only detects and segments objects in images, VIS
    involves more sophisticated and challenging instance tracking across video frames.
    VIS plays an important role in various real-world applications. As an example,
    by acquiring better representations of instances in videos, VIS assists in human
    action recognition and person (re-)identification, enhancing security for surveillance
    systems [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]. Given that Tesla is
    producing its DOJO supercomputer [[7](#bib.bib7)] to improve its driver-assistance
    system, VIS helps vehicles recognize and track other vehicles and pedestrians,
    boosting autonomous driving [[8](#bib.bib8), [9](#bib.bib9)]. In the healthcare
    sector, VIS supports biomedical image analysis, pathology detection, and surgical
    automation [[10](#bib.bib10), [11](#bib.bib11)]. Furthermore, VIS demonstrates
    its potential to improve productivity, security, and user experience in the fields
    of agriculture [[12](#bib.bib12)], construction [[13](#bib.bib13)], and entertainment [[14](#bib.bib14)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像分割[[1](#bib.bib1), [2](#bib.bib2)]扩展而来的视频实例分割（VIS）最初在2019年由[[3](#bib.bib3)]提出。与仅检测和分割图像中的对象的图像分割不同，VIS涉及跨视频帧的更复杂和更具挑战性的实例跟踪。VIS在各种现实应用中发挥着重要作用。例如，通过获取视频中实例的更好表示，VIS有助于人类动作识别和人物（重新）识别，增强了监控系统的安全性[[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)]。鉴于特斯拉正在生产其DOJO超级计算机[[7](#bib.bib7)]以改善其驾驶辅助系统，VIS帮助车辆识别和跟踪其他车辆和行人，从而提升自动驾驶能力[[8](#bib.bib8),
    [9](#bib.bib9)]。在医疗领域，VIS支持生物医学图像分析、病理检测和手术自动化[[10](#bib.bib10), [11](#bib.bib11)]。此外，VIS在农业[[12](#bib.bib12)]、建筑[[13](#bib.bib13)]和娱乐[[14](#bib.bib14)]领域展示了其提高生产力、安全性和用户体验的潜力。
- en: Deep learning is a machine learning methodology based on deep neural networks
    that consist of multiple layers and processing nodes [[15](#bib.bib15), [16](#bib.bib16)].
    Various deep neural networks, such as convolutional neural networks (CNN), recurrent
    neural networks (RNN), graph neural networks (GNN), and Transformers, have been
    increasingly adopted in deep-learning schemes for tackling challenges in the fields
    of computer vision [[17](#bib.bib17)], natural language processing [[18](#bib.bib18)],
    etc. These emerging deep-learning solutions usually demonstrate better performance
    than traditional machine-learning approaches [[19](#bib.bib19)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一种基于深度神经网络的机器学习方法，这些网络由多个层和处理节点组成[[15](#bib.bib15), [16](#bib.bib16)]。各种深度神经网络，如卷积神经网络（CNN）、递归神经网络（RNN）、图神经网络（GNN）和变换器（Transformers），在深度学习方案中越来越多地被采用，以应对计算机视觉[[17](#bib.bib17)]、自然语言处理[[18](#bib.bib18)]等领域的挑战。这些新兴的深度学习解决方案通常表现出比传统机器学习方法更优的性能[[19](#bib.bib19)]。
- en: In recent years, numerous deep-learning schemes have been proposed for VIS.
    Typically, researchers propose novel deep-learning architectures by assembling
    mature deep neural networks, in order to more effectively extract features and
    aggregate spatiotemporal information. Besides, some researchers focused on auxiliary
    techniques, such as datasets and representation learning methodologies, to improve
    the performance of deep-learning models for VIS. In light of the rapidly expanding
    research attention on VIS, this paper reviews the existing works pertaining to
    deep-learning techniques for VIS.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，针对VIS提出了大量深度学习方案。通常，研究人员通过组装成熟的深度神经网络来提出新颖的深度学习架构，以更有效地提取特征和汇聚时空信息。此外，一些研究人员还专注于辅助技术，如数据集和表示学习方法，以提高深度学习模型在VIS中的性能。鉴于对VIS研究关注的快速扩展，本文回顾了现有的与VIS相关的深度学习技术工作。
- en: 'Numerous surveys on instance segmentation and object detection have been published
    in the literature. However, most of them focus on image segmentation [[20](#bib.bib20)],
    Transformers [[21](#bib.bib21)], or video object tracking techniques [[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24)], with limited attention on the emerging VIS
    field. To close this gap, in this survey, deep-learning schemes for VIS are comprehensively
    reviewed, with key challenges and promising research directions identified. A
    comparison between this survey and existing survey papers is listed in Table [1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Deep Learning Techniques for Video Instance Segmentation:
    A Survey").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '文献中已经发表了大量关于实例分割和目标检测的调查。然而，它们大多数集中在图像分割[[20](#bib.bib20)]、变换器[[21](#bib.bib21)]，或视频目标跟踪技术[[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24)]，对新兴的视频实例分割（VIS）领域关注有限。为了填补这一空白，本调查全面回顾了VIS的深度学习方案，并识别了关键挑战和有前景的研究方向。表[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Deep Learning Techniques for Video Instance Segmentation:
    A Survey")中列出了本调查与现有调查论文的比较。'
- en: 'Table 1: Comparison with Existing Survey Papers'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：与现有调查论文的比较
- en: '| Ref | Year | Review Breadth | Review Depth |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 引用 | 年份 | 综述广度 | 综述深度 |'
- en: '| [[25](#bib.bib25)] | 2020 | Multi-Object Tracking | Deep Learning Techniques
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| [[25](#bib.bib25)] | 2020 | 多目标跟踪 | 深度学习技术 |'
- en: '| [[26](#bib.bib26)] | 2020 | Video Object Segmentation and Tracking | Separate
    Segmentation and Tracking Methods |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| [[26](#bib.bib26)] | 2020 | 视频目标分割与跟踪 | 分离的分割和跟踪方法 |'
- en: '| [[27](#bib.bib27)] | 2021 | Multi-Object Tracking | Real-Time Deep Learning
    Techniques |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| [[27](#bib.bib27)] | 2021 | 多目标跟踪 | 实时深度学习技术 |'
- en: '| [[22](#bib.bib22)] | 2021 | Multi-Object Tracking | Similarity Computation
    and Re-identification Techniques |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| [[22](#bib.bib22)] | 2021 | 多目标跟踪 | 相似性计算和重识别技术 |'
- en: '| [[24](#bib.bib24)] | 2022 | Multi-Object Tracking | Discriminative Filters
    and Siamese Networks |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| [[24](#bib.bib24)] | 2022 | 多目标跟踪 | 判别滤波器和孪生网络 |'
- en: '| [[23](#bib.bib23)] | 2022 | Multi-Object Tracking | Data Association Methods
    |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| [[23](#bib.bib23)] | 2022 | 多目标跟踪 | 数据关联方法 |'
- en: '| [[19](#bib.bib19)] | 2022 | Video Object Segmentation & Video Semantic Segmentation
    | Deep Learning Techniques |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| [[19](#bib.bib19)] | 2022 | 视频目标分割与视频语义分割 | 深度学习技术 |'
- en: '| [[28](#bib.bib28)] | 2022 | Multi-Object Tracking | Embedding Methods |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| [[28](#bib.bib28)] | 2022 | 多目标跟踪 | 嵌入方法 |'
- en: '| [[29](#bib.bib29)] | 2022 | Multi-Object Tracking | Object Detection and
    Association Methods |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] | 2022 | 多目标跟踪 | 目标检测和关联方法 |'
- en: '| [[30](#bib.bib30)] | 2023 | Video Object Segmentation | Deep Learning Techniques
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| [[30](#bib.bib30)] | 2023 | 视频目标分割 | 深度学习技术 |'
- en: '| [[31](#bib.bib31)] | 2023 | Moving Object Segmentation | Efficient Deep Learning
    Techniques |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| [[31](#bib.bib31)] | 2023 | 移动目标分割 | 高效深度学习技术 |'
- en: '| [[21](#bib.bib21)] | 2023 | Visual Segmentation | Transformer-Based Methods
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| [[21](#bib.bib21)] | 2023 | 视觉分割 | 基于变换器的方法 |'
- en: '| Ours | - | Video Instance Segmentation | Deep Learning Techniques |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | - | 视频实例分割 | 深度学习技术 |'
- en: In summary, the contributions of this paper are as follows. Firstly, deep-learning
    techniques for VIS are reviewed and qualitatively compared from the architectural
    perspective. Secondly, auxiliary techniques that improve the performance of deep-learning
    models for VIS are outlined. Thirdly, a number of challenges and potential research
    directions are highlighted to promote further research in the field of VIS.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文的贡献如下。首先，从结构角度回顾和定性比较了VIS的深度学习技术。其次，概述了提高深度学习模型在VIS中性能的辅助技术。第三，突出了一些挑战和潜在的研究方向，以促进VIS领域的进一步研究。
- en: 'This paper is organized as follows. Section [2](#S2 "2 Preliminaries ‣ Deep
    Learning Techniques for Video Instance Segmentation: A Survey") provides the background
    knowledge for readers to better understand related techniques. Section [3](#S3
    "3 Deep Learning Architectures for Video Instance Segmentation ‣ Deep Learning
    Techniques for Video Instance Segmentation: A Survey") analyzes, compares, and
    summarizes different deep-learning schemes for VIS from the perspective of architecture,
    while Section [4](#S4 "4 Auxiliary Techniques for Enhancing Video Instance Segmentation
    ‣ Deep Learning Techniques for Video Instance Segmentation: A Survey") reviews
    auxiliary techniques used to enhance the performance of deep-learning models for
    VIS. Section [5](#S5 "5 Challenges and Future Research Directions ‣ Deep Learning
    Techniques for Video Instance Segmentation: A Survey") sheds light on several
    challenges and future research directions. Finally, Section [6](#S6 "6 Conclusion
    ‣ Deep Learning Techniques for Video Instance Segmentation: A Survey") concludes
    this survey. The abbreviations used in this survey are summarized in Table [2](#S1.T2
    "Table 2 ‣ 1 Introduction ‣ Deep Learning Techniques for Video Instance Segmentation:
    A Survey").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '本文组织如下。第[2](#S2 "2 Preliminaries ‣ Deep Learning Techniques for Video Instance
    Segmentation: A Survey")节提供了背景知识，以帮助读者更好地理解相关技术。第[3](#S3 "3 Deep Learning Architectures
    for Video Instance Segmentation ‣ Deep Learning Techniques for Video Instance
    Segmentation: A Survey")节从架构的角度分析、比较和总结了不同的深度学习方案。第[4](#S4 "4 Auxiliary Techniques
    for Enhancing Video Instance Segmentation ‣ Deep Learning Techniques for Video
    Instance Segmentation: A Survey")节回顾了用于提高深度学习模型性能的辅助技术。第[5](#S5 "5 Challenges
    and Future Research Directions ‣ Deep Learning Techniques for Video Instance Segmentation:
    A Survey")节阐述了若干挑战和未来研究方向。最后，第[6](#S6 "6 Conclusion ‣ Deep Learning Techniques
    for Video Instance Segmentation: A Survey")节总结了本次调查。本调查中使用的缩略语总结在表[2](#S1.T2 "Table
    2 ‣ 1 Introduction ‣ Deep Learning Techniques for Video Instance Segmentation:
    A Survey")中。'
- en: 'Table 2: Abbreviation and Description'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：缩略语及说明
- en: '| Abbreviation | Description |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 缩略语 | 说明 |'
- en: '| CNN | Convolutional Neural Network |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 卷积神经网络 |'
- en: '| FPN | Feature Pyramid Network |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| FPN | 特征金字塔网络 |'
- en: '| GNN | Graph Neural Network |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| GNN | 图神经网络 |'
- en: '| LSTM | Long Short-Term Memory Network |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | 长短期记忆网络 |'
- en: '| MOT | Multi-Object Tracking |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| MOT | 多目标跟踪 |'
- en: '| MOTS | Multi-Object Tracking and Segmentation |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| MOTS | 多目标跟踪与分割 |'
- en: '| RNN | Recurrent Neural Network |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| RNN | 循环神经网络 |'
- en: '| RoI | Region of Interest |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| RoI | 感兴趣区域 |'
- en: '| VIS | Video Instance Segmentation |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| VIS | 视频实例分割 |'
- en: '| ViT | Vision Transformer |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ViT | 视觉变换器 |'
- en: '| VOS | Video Object Segmentation |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| VOS | 视频目标分割 |'
- en: '| VPS | Video Panoptic Segmentation |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| VPS | 视频全景分割 |'
- en: '| VSS | Video Semantic Segmentation |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| VSS | 视频语义分割 |'
- en: 2 Preliminaries
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 前言
- en: Before diving into analyzing recent studies in VIS, it is essential to explain
    the fundamental concepts relevant to this survey, including various video segmentation
    tasks and deep neural networks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入分析最近的视频实例分割（VIS）研究之前，解释与本调查相关的基本概念是至关重要的，包括各种视频分割任务和深度神经网络。
- en: 2.1 Video Segmentation
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 视频分割
- en: 'Video segmentation aims to isolate and identify elements within a video. In
    particular, video segmentation encompasses several distinct tasks: video object
    segmentation, video semantic segmentation, and video instance segmentation, as
    shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1 Video Segmentation ‣ 2 Preliminaries
    ‣ Deep Learning Techniques for Video Instance Segmentation: A Survey"). To help
    readers better grasp the extent of this survey, this section elaborates and compares
    these tasks. It is also important to note the difference between ”objects” and
    ”instances”. An object in the context of VIS refers to a general category of items
    in a video frame (e.g., in a street scene, the objects could be pedestrians, cars,
    traffic signs, and buildings). An ”instance” refers to an individual occurrence
    of an object category. For example, if there are multiple cars in a video frame,
    VIS would differentiate between each individual car by assigning it a unique label.
    Therefore, an object category encompasses multiple instances of that class of
    object.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '视频分割旨在隔离和识别视频中的元素。特别是，视频分割包括几个不同的任务：视频对象分割、视频语义分割和视频实例分割，如图[1](#S2.F1 "Figure
    1 ‣ 2.1 Video Segmentation ‣ 2 Preliminaries ‣ Deep Learning Techniques for Video
    Instance Segmentation: A Survey")所示。为了帮助读者更好地掌握本调查的范围，本节详细阐述并比较这些任务。还需要注意“对象”和“实例”之间的区别。在VIS的上下文中，对象指的是视频帧中的一般类别（例如，在街道场景中，对象可以是行人、汽车、交通标志和建筑物）。而“实例”指的是对象类别的单独出现。例如，如果视频帧中有多辆车，VIS将通过分配唯一标签来区分每辆车。因此，一个对象类别包括该类别的多个实例。'
- en: '![Refer to caption](img/000192bebd99c7178e29c46bfd8cf040.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/000192bebd99c7178e29c46bfd8cf040.png)'
- en: (a) VOS
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: (a) VOS
- en: '![Refer to caption](img/e508eb4731042587e8820eb931279845.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e508eb4731042587e8820eb931279845.png)'
- en: (b) VSS
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (b) VSS
- en: '![Refer to caption](img/6685e7ff0bef3c1231c4f8dee1d4040a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6685e7ff0bef3c1231c4f8dee1d4040a.png)'
- en: (c) VIS
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (c) VIS
- en: 'Figure 1: Video segmentation tasks.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：视频分割任务。
- en: 'Video Object Segmentation: Video Object Segmentation (VOS) is a binary segmentation
    task that requires the model to segment foreground objects from the background
    of a video [[32](#bib.bib32), [26](#bib.bib26), [19](#bib.bib19)]. In other words,
    rather than segmenting every pixel in a frame, VOS only segments those pixels
    associated with the salient object. The classification results are binary, without
    separation of different instances of the same class of objects. VOS is the earliest
    video segmentation task and it serves as the basis for others.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 视频对象分割：视频对象分割（VOS）是一种二值分割任务，要求模型将前景对象从视频的背景中分割出来[[32](#bib.bib32), [26](#bib.bib26),
    [19](#bib.bib19)]。换句话说，VOS不是分割帧中的每个像素，而只是分割与显著对象相关的像素。分类结果是二值的，没有区分相同对象类别的不同实例。VOS是最早的视频分割任务，它为其他任务奠定了基础。
- en: 'Video Semantic Segmentation: Semantic segmentation was originally proposed
    for image processing, which requires the model to categorize each pixel in an
    image into a class [[33](#bib.bib33), [34](#bib.bib34)]. Later, the concept of
    semantic segmentation was applied to videos, known as video semantic segmentation
    (VSS) [[35](#bib.bib35), [36](#bib.bib36)]. Compared with VOS, VSS associates
    every pixel in a frame with one of multiple semantic categories. It is not necessary
    to discriminate different instances.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 视频语义分割：语义分割最初是为图像处理提出的，要求模型将图像中的每个像素分类到一个类别中[[33](#bib.bib33), [34](#bib.bib34)]。后来，语义分割的概念应用于视频中，称为视频语义分割（VSS）[[35](#bib.bib35),
    [36](#bib.bib36)]。与VOS相比，VSS将帧中的每个像素与多个语义类别之一关联起来。无需区分不同的实例。
- en: 'Video Instance Segmentation: In 2019, Yang et al. [[3](#bib.bib3)] introduced
    the VIS task, which requires the detection, segmentation, and tracking of individual
    instances of objects in videos. In the same year, Voigtlaender et al. [[37](#bib.bib37)]
    extended the Multi-Object Tracking (MOT) [[6](#bib.bib6)] to instance segmentation
    tracking and coined the term “Multi-Object Tracking and Segmentation” (MOTS),
    which is similar to VIS. The only difference between VIS and MOTS is that MOTS
    requires that masks do not overlap during evaluation [[38](#bib.bib38)]. Therefore,
    in this survey, the two terms, VIS and MOTS, are used interchangeably. Compared
    with VOS and VSS, VIS segments salient objects in each frame and allocates the
    results into multiple classes, while identifying and tracking individual instances.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 视频实例分割（VIS）：2019 年，Yang 等人 [[3](#bib.bib3)] 引入了 VIS 任务，该任务要求检测、分割和跟踪视频中单独实例的物体。同年，Voigtlaender
    等人 [[37](#bib.bib37)] 将多目标跟踪（MOT） [[6](#bib.bib6)] 扩展到实例分割跟踪，并创造了“多目标跟踪与分割”（MOTS）这一术语，这与
    VIS 类似。VIS 和 MOTS 的唯一区别在于 MOTS 要求在评估期间掩码不重叠 [[38](#bib.bib38)]。因此，在本调查中，VIS 和
    MOTS 这两个术语可以互换使用。与 VOS 和 VSS 相比，VIS 在每帧中分割显著物体，并将结果分配到多个类别，同时识别和跟踪单独的实例。
- en: 2.2 Deep Neural Networks
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 深度神经网络
- en: As there are numerous deep neural networks, the most popular ones used in existing
    deep-learning schemes for VIS are introduced below.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度神经网络种类繁多，下面介绍了现有深度学习方案中最常用的一些网络。
- en: 'Convolutional Neural Network (CNN): A CNN is a popular deep neural network
    that automatically extracts features from images using its convolution kernels [[39](#bib.bib39),
    [16](#bib.bib16), [40](#bib.bib40)]. CNNs are widely applied in image and video
    regions, such as object detection, object tracking, action recognition, etc [[40](#bib.bib40)].'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）：CNN 是一种流行的深度神经网络，通过其卷积核自动从图像中提取特征 [[39](#bib.bib39), [16](#bib.bib16),
    [40](#bib.bib40)]。CNN 广泛应用于图像和视频领域，如物体检测、物体跟踪、动作识别等 [[40](#bib.bib40)]。
- en: 'Recurrent Neural Network (RNN): An RNN is a deep neural network designed for
    sequential data or time series data, as it retains context information via cycles
    in the network [[41](#bib.bib41)]. As a result, several famous RNNs, such as the
    Long Short-Term Memory Network (LSTM), are widely adopted in VIS schemes to learn
    sequential visual features frame-by-frame with the help of CNNs [[42](#bib.bib42)].'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）：RNN 是一种针对序列数据或时间序列数据设计的深度神经网络，通过网络中的循环保留上下文信息 [[41](#bib.bib41)]。因此，许多著名的
    RNN，如长短期记忆网络（LSTM），在 VIS 方案中广泛应用，以帮助 CNN 逐帧学习序列视觉特征 [[42](#bib.bib42)]。
- en: 'Graph Neural Network (GNN): A GNN is a deep neural network designed for graph
    data, which captures dependency relationships among nodes via message passing
    between the nodes of graphs and conducts node, edge, and graph level predictions [[43](#bib.bib43)].
    In VIS, the GNN is typically used to model the relationships among instances for
    better instance tracking and segmentation [[44](#bib.bib44)].'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNN）：GNN 是一种针对图数据设计的深度神经网络，通过图节点之间的消息传递捕捉节点之间的依赖关系，并进行节点、边和图层级的预测 [[43](#bib.bib43)]。在
    VIS 中，GNN 通常用于建模实例之间的关系，以便更好地进行实例跟踪和分割 [[44](#bib.bib44)]。
- en: 'Transformers: A Transformer is a popular deep neural network with a self-attention
    mechanism that enables the global perception of a long sequence of tokenized inputs
    by automatically amplifying the key tokens [[21](#bib.bib21)]. Vision Transformer
    (ViT) is a kind of Transformer that breaks down input images into a sequence of
    patches and then tokenizes them. Because of its outstanding performance, ViT is
    used for a growing number of computer vision tasks, such as image classification [[45](#bib.bib45)],
    object detection [[46](#bib.bib46), [47](#bib.bib47)], and VIS [[48](#bib.bib48)].
    Transformer is able to detect and segment objects at the frame level following
    the design of DEtection TRansformer (DETR) [[46](#bib.bib46)]. Transformer also
    offers long-range dependency modeling and temporal feature linkage for better
    instance tracking [[49](#bib.bib49)].'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer：Transformer 是一种流行的深度神经网络，具有自注意力机制，通过自动放大关键标记，实现对长序列的全局感知 [[21](#bib.bib21)]。视觉
    Transformer（ViT）是一种 Transformer，它将输入图像拆分成一系列的补丁，然后对其进行标记。由于其卓越的性能，ViT 被用于越来越多的计算机视觉任务，如图像分类
    [[45](#bib.bib45)]、物体检测 [[46](#bib.bib46), [47](#bib.bib47)] 和 VIS [[48](#bib.bib48)]。Transformer
    能够根据 DEtection TRansformer（DETR）的设计在帧级别检测和分割物体 [[46](#bib.bib46)]。Transformer
    还提供了长距离依赖建模和时间特征关联，以便更好地进行实例跟踪 [[49](#bib.bib49)]。
- en: 'Backbone, Neck, and Head: To complete complex tasks in computer vision, deep-learning
    paradigms are typically composed of multiple kinds of deep neural networks organized
    as backbone, neck, and head [[19](#bib.bib19), [21](#bib.bib21)]. In particular,
    a backbone is responsible for extracting features from the input, a neck aggregates
    and refines the features extracted by the backbone, while a head is responsible
    for making predictions. These concepts are carried over into the deep-learning
    paradigms for VIS.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 骨干，Neck和Head：为了完成计算机视觉中的复杂任务，通常由多种深度神经网络组成，组织为骨干，neck和head [[19](#bib.bib19),
    [21](#bib.bib21)]。特别是，骨干负责从输入中提取特征，neck聚合和优化骨干提取的特征，而head负责进行预测。这些概念延续到视频实例分割的深度学习框架中。
- en: 3 Deep Learning Architectures for Video Instance Segmentation
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3种视频实例分割的深度学习架构
- en: 'In this section, the recent deep-learning schemes for VIS are analyzed and
    categorized from the perspective of architecture. In particular, as the backbone
    of the deep-learning schemes for VIS usually has a similar design for extracting
    frame-level features, the classification criteria mostly rely on the feature processing
    design in the neck. Specifically, deep-learning schemes for VIS can be broadly
    categorized into multi-stage, multi-branch, hybrid, integrated, and recurrent
    types. Table [3](#S3.T3 "Table 3 ‣ 3 Deep Learning Architectures for Video Instance
    Segmentation ‣ Deep Learning Techniques for Video Instance Segmentation: A Survey")
    outlines the pros and cons of different deep-learning architectures. Besides,
    Table [3](#S3.T3 "Table 3 ‣ 3 Deep Learning Architectures for Video Instance Segmentation
    ‣ Deep Learning Techniques for Video Instance Segmentation: A Survey") presents
    the design ideas for each deep-learning architecture and the corresponding works.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，从架构的角度分析并分类了最近的视频实例分割深度学习方案。特别是，由于视频实例分割的深度学习方案的骨干通常具有类似的设计用于提取帧级特征，分类标准主要依赖于neck中特征处理的设计。具体来说，视频实例分割的深度学习方案可以广泛分类为多阶段，多分支，混合，集成和循环类型。表[3](#S3.T3
    "表3 ‣ 3种视频实例分割的深度学习架构 ‣ 视频实例分割的深度学习技术：综述")概述了不同深度学习架构的优缺点。此外，表[3](#S3.T3 "表3 ‣
    3种视频实例分割的深度学习架构 ‣ 视频实例分割的深度学习技术：综述")提供了每种深度学习架构的设计思想和相应的工作。
- en: 'Table 3: Comparison of Deep-Learning Architectures for Video Instance Segmentation'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：视频实例分割的深度学习架构比较
- en: '| Arch. | Design Ideas | Work | Pros ($\ast$) and Cons (-) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 设计思想 | 工作 | 优点（$\ast$）和缺点（-） |'
- en: '| M-Stage | Mask R-CNN | [[3](#bib.bib3), [37](#bib.bib37), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [38](#bib.bib38), [54](#bib.bib54)]
    | $\ast$ Effective in extracting both low- and high-level features. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| M-Stage | Mask R-CNN | [[3](#bib.bib3), [37](#bib.bib37), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [38](#bib.bib38), [54](#bib.bib54)]
    | $\ast$ 有效提取低级和高级特征。'
- en: '| Mask Propagation | [[55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57),
    [58](#bib.bib58)] | $\ast$ Easily replace sub-networks to suit various applications.
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Mask Propagation | [[55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57),
    [58](#bib.bib58)] | $\ast$ 轻松替换子网络以适应各种应用。'
- en: '| IRNet | [[59](#bib.bib59), [60](#bib.bib60)] | - More processing stages increase
    computational complexity. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| IRNet | [[59](#bib.bib59), [60](#bib.bib60)] | - 更多的处理阶段增加了计算复杂性。'
- en: '| Attention Mechanism | [[61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63),
    [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66)] |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 注意力机制 | [[61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65), [66](#bib.bib66)] |  |'
- en: '| Polamask & FCOS | [[67](#bib.bib67), [68](#bib.bib68)] |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Polamask & FCOS | [[67](#bib.bib67), [68](#bib.bib68)] |  |'
- en: '| M-Branch | Instance + Object Segment | [[69](#bib.bib69), [70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72)] | $\ast$ Effective for spatiotemporal feature
    processing. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| M-Branch | 实例 + 物体分割 | [[69](#bib.bib69), [70](#bib.bib70), [71](#bib.bib71),
    [72](#bib.bib72)] | $\ast$ 有效的时空特征处理。'
- en: '| Detection + Tracking | [[73](#bib.bib73), [74](#bib.bib74)] | $\ast$ Effective
    for multi-modal feature processing. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 检测 + 追踪 | [[73](#bib.bib73), [74](#bib.bib74)] | $\ast$ 有效的多模态特征处理。'
- en: '| YOLACT | [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)] | - Increased
    architectural complexity |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| YOLACT | [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)] | - 增加架构复杂性
    |'
- en: '| Siamese Network | [[82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84),
    [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89),
    [90](#bib.bib90), [85](#bib.bib85), [84](#bib.bib84), [91](#bib.bib91), [92](#bib.bib92)]
    | - Requiring careful design and tuning to balance the branches. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Siamese 网络 | [[82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90),
    [85](#bib.bib85), [84](#bib.bib84), [91](#bib.bib91), [92](#bib.bib92)] | - 需要仔细设计和调整以平衡各个分支。
    |'
- en: '| Knowledge Distillation | [[93](#bib.bib93)] |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 知识蒸馏 | [[93](#bib.bib93)] |  |'
- en: '| Point Clouds | [[94](#bib.bib94), [95](#bib.bib95)] |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 点云 | [[94](#bib.bib94), [95](#bib.bib95)] |  |'
- en: '| Hybrid | M-Branch Encoder & Decoder | [[96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100)] | $\ast$ Better utilize
    the strengths of different types of networks. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 混合 | M-Branch 编码器 & 解码器 | [[96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100)] | $\ast$ 更好地利用不同类型网络的优势。 |'
- en: '| $\ast$ Effective for learning robust and generalized presentations. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| $\ast$ 有效学习稳健和广泛的表现。 |'
- en: '| - Increased complexity and computational overheads. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| - 增加了复杂性和计算开销。 |'
- en: '| - Requiring careful selection and design of sub-networks. |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| - 需要仔细选择和设计子网络。 |'
- en: '| Integrated | 3D-CNN & GNN | [[101](#bib.bib101), [102](#bib.bib102)] | $\ast$
    Integrated feature processing for specific data distribution. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 综合 | 3D-CNN & GNN | [[101](#bib.bib101), [102](#bib.bib102)] | $\ast$ 针对特定数据分布的综合特征处理。
    |'
- en: '| ViT | [[103](#bib.bib103), [104](#bib.bib104), [49](#bib.bib49), [105](#bib.bib105),
    [106](#bib.bib106), [107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113)]
    | - Requires a large dataset and long training for an ideal model. - Not flexible
    enough to adjust for different purposes. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ViT | [[103](#bib.bib103), [104](#bib.bib104), [49](#bib.bib49), [105](#bib.bib105),
    [106](#bib.bib106), [107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113)]
    | - 需要大量数据集和较长的训练时间才能达到理想模型。 - 不够灵活，难以调整以满足不同的用途。 |'
- en: '| Recurrent | LSTM & GNN | [[114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116),
    [117](#bib.bib117), [44](#bib.bib44)] | $\ast$ Effective for capturing temporal
    dependencies and context. |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 循环 | LSTM & GNN | [[114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116),
    [117](#bib.bib117), [44](#bib.bib44)] | $\ast$ 有效捕捉时间依赖性和上下文。 |'
- en: '| ViT with Query Propagation | [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [48](#bib.bib48), [122](#bib.bib122), [123](#bib.bib123)]
    | - Longer contextual understanding entails more computational overheads. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 带查询传播的 ViT | [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [48](#bib.bib48), [122](#bib.bib122), [123](#bib.bib123)]
    | - 更长的上下文理解需要更多的计算开销。 |'
- en: 3.1 Multi-Stage Feature Processing Architecture
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 多阶段特征处理架构
- en: Multi-stage feature processing involves multiple feature processing and transformation
    stages in the neck, with each stage building upon the representations learned
    in the previous one. Earlier stages typically capture frame-level features and
    propose several Regions of Interests (RoIs), while later stages aggregate features
    and process abstract patterns and semantic information for tasks, such as object
    detection, object classification, instance segmentation, and instance tracking
    across frames. Popular multi-stage feature processing architectures for VIS include
    MaskTrack R-CNN [[3](#bib.bib3)] and TrackR-CNN [[37](#bib.bib37)], which are
    extended from a famous image instance segmentation network Mask R-CNN [[124](#bib.bib124)].
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 多阶段特征处理涉及多个特征处理和转换阶段，每个阶段都建立在之前阶段学到的表示基础上。早期阶段通常捕捉帧级特征并提出多个感兴趣区域（RoIs），而后期阶段则聚合特征并处理抽象模式和语义信息，用于任务如目标检测、目标分类、实例分割和跨帧实例跟踪。流行的多阶段特征处理架构包括
    MaskTrack R-CNN [[3](#bib.bib3)] 和 TrackR-CNN [[37](#bib.bib37)]，它们扩展自著名的图像实例分割网络
    Mask R-CNN [[124](#bib.bib124)]。
- en: When proposing the VIS task in [[3](#bib.bib3)], Yang et al. extended Mask R-CNN [[124](#bib.bib124)]
    to MaskTrack R-CNN by adding a post-processing stage for tracking instances across
    video frames. Specifically, they utilize the memory queue to store the features
    of previously identified instances. A tracking head is embedded into Mask R-CNN
    to compare the similarity between the newly detected instance and the identified
    instances. When introducing the MOTS task in [[37](#bib.bib37)], the authors proposed
    a TrackR-CNN network extended from Mask R-CNN [[124](#bib.bib124)]. In particular,
    the 3D-CNN is used for feature extraction [[36](#bib.bib36)]. On top of the feature
    maps, the first stage utilizes a Region Proposal Network (RPN) [[125](#bib.bib125)]
    to generate the proposal of target objects. In the second stage, several headers
    are used to predict the class, box, binary mask, and association vector for each
    RoI. The Euclidean distances between the association vectors are computed to associate
    the detected instances over time into tracks. As early works on VIS, MaskTrack
    R-CNN and TrackR-CNN [[3](#bib.bib3), [37](#bib.bib37)] simply extend the image
    instance segmentation architecture (Mask R-CNN), thus exhibiting several shortcomings,
    including segmentation precision, instance tracking consistency, occlusion resistance,
    and computational complexity. Most subsequent VIS schemes improve certain aspects
    and take these two schemes as benchmarks.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[3](#bib.bib3)]中提出VIS任务时，Yang等人通过为跟踪视频帧中的实例添加后处理阶段，将Mask R-CNN [[124](#bib.bib124)]
    扩展为 MaskTrack R-CNN。具体而言，他们利用内存队列存储先前识别实例的特征。在Mask R-CNN中嵌入了一个跟踪头，用于比较新检测到的实例与已识别实例之间的相似度。当介绍MOTS任务时[[37](#bib.bib37)]，作者提出了一个从Mask
    R-CNN扩展出来的TrackR-CNN网络[[124](#bib.bib124)]。特别地，使用了3D-CNN进行特征提取[[36](#bib.bib36)]。在特征映射之上，第一阶段利用区域建议网络(RPN)
    [[125](#bib.bib125)] 生成目标对象的建议。在第二阶段，使用几个头部来预测每个RoI的类别、框、二进制掩码和关联向量。计算关联向量之间的欧氏距离，将检测到的实例随时间关联到轨道上。作为VIS的早期作品，MaskTrack
    R-CNN和TrackR-CNN [[3](#bib.bib3), [37](#bib.bib37)] 简单地扩展了图像实例分割架构(Mask R-CNN)，因此存在几个缺陷，包括分割精度、实例跟踪一致性、遮挡抵抗能力和计算复杂性。大多数后续的VIS方案都改进了某些方面，并将这两个方案作为基准。
- en: In terms of instance tracking, the authors of [[50](#bib.bib50)] utilized a
    CNN to extract features from multiple frames simultaneously and a Siamese network [[5](#bib.bib5)]
    with cosine similarity to track the temporal features. Theoretically, this scheme
    avoids using the computationally expensive 3D-CNN, thus improving computing efficiency
    to some extent. However, the additional overheads for getting RoI proposals from
    multiple frames and the ensuing feature similarity comparison increase computational
    complexity. Besides, the randomly sampled frames from the video sequence cannot
    ensure the reliability of feature comparison. Similar to [[50](#bib.bib50)], Porzi
    et al. [[51](#bib.bib51)] introduced a tracking head component to Mask R-CNN.
    The method accepts output from both the region segmentation head and the corresponding
    RoI features from the Feature Pyramid Network (FPN) [[126](#bib.bib126)]. While
    the method eliminates the need for memory caching for existing instances, it also
    limits the capability of identifying re-appearing instances. To improve the performance
    of instance tracking and re-identification, the authors of [[52](#bib.bib52)]
    proposed a bi-directional tracker, known as Instance-Pixel Dual-Tracker (IPDT).
    Based on the RoI proposals, the categories of objects are first calibrated to
    filter out false-positive classes within the global context of the video. Then,
    IPDT bidirectionally tracks instance-level and pixel-level embeddings, aiming
    at infusing the instance-level concept and discriminating overlapping instances.
    Instead of treating a video as an array of frames, in [[53](#bib.bib53)], the
    authors treat it as a tree composed of multiple tracklets in order to better track
    the re-appearing instances. In particular, a tracklet association algorithm, UnOVOST,
    was proposed based on Mask R-CNN [[53](#bib.bib53)]. In the first stage, segments
    in consecutive frames are combined into short tracklets that contain segments
    from the same object by using spatiotemporal consistency cues. In the second stage,
    these short tracklets are merged into long-term tracks using decision trees, which
    are pruned based on the appearance similarity. Their approach improves the performance
    of long-distance instance tracking. However, the computational complexity and
    memory overhead are relatively higher when compared with those from frame-level
    tracking approaches. By adopting UnOVOST, in [[38](#bib.bib38)], the authors obtained
    first place in the 2019 YouTube-VIS challenge. Apart from decision trees, researchers
    have employed dynamic programming to identify global optimal assignments of instances
    across frames [[54](#bib.bib54)]. A prominent advantage of these schemes is the
    improved understanding of instances across frames and better segmentation performance
    on overlaps, as compared with those from early VIS schemes MaskTrack R-CNN and
    TrackR-CNN.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例跟踪方面，作者们利用了[[50](#bib.bib50)]中的CNN从多个帧中同时提取特征，并使用了具有余弦相似度的Siamese网络[[5](#bib.bib5)]来跟踪时间特征。从理论上讲，该方案避免了使用计算量大的3D-CNN，从而在一定程度上提高了计算效率。然而，从多个帧获取RoI提议的额外开销以及随后的特征相似度比较增加了计算复杂性。此外，从视频序列中随机采样的帧无法确保特征比较的可靠性。类似于[[50](#bib.bib50)]，Porzi等人[[51](#bib.bib51)]向Mask
    R-CNN中引入了一个跟踪头组件。该方法接受来自区域分割头的输出和来自特征金字塔网络（FPN）[[126](#bib.bib126)]的相应RoI特征。虽然该方法消除了对现有实例的内存缓存的需求，但也限制了识别重新出现的实例的能力。为了提高实例跟踪和重新识别的性能，[[52](#bib.bib52)]的作者提出了一种双向跟踪器，即实例像素双重跟踪器（IPDT）。基于RoI提议，首先校准对象类别以过滤掉全局视频上下文中的假阳性类别。然后，IPDT双向跟踪实例级别和像素级别的嵌入，旨在注入实例级别的概念并区分重叠实例。在[[53](#bib.bib53)]中，作者们没有将视频视为一系列帧，而是将其视为由多个跟踪小片段组成的树，以更好地跟踪重新出现的实例。特别是，基于Mask
    R-CNN[[53](#bib.bib53)]提出了一种跟踪小片段关联算法UnOVOST。在第一阶段，利用时空一致性线索将连续帧中的片段组合成包含相同对象片段的短期跟踪小片段。在第二阶段，这些短期跟踪小片段使用决策树合并为长期轨迹，决策树基于外观相似性进行修剪。该方法提高了远距离实例跟踪的性能。然而，与帧级跟踪方法相比，其计算复杂性和内存开销较高。通过采用UnOVOST，在[[38](#bib.bib38)]中，作者们在2019年YouTube-VIS挑战赛中获得了第一名。除了决策树，研究人员还采用了动态规划来识别跨帧的全局最佳分配[[54](#bib.bib54)]。这些方案的一个显著优势是改善了对跨帧实例的理解，并且在重叠区域上具有更好的分割性能，与早期VIS方案MaskTrack
    R-CNN和TrackR-CNN相比。
- en: A typical way to train a semi-supervised VIS model is to propagate masks of
    one or several keyframes to the entire video or video clips. As an example, a
    bi-directional instance segmentation method was proposed by Tran et al. in [[55](#bib.bib55)].
    The work formulates a forward and backward propagation strategy to utilize masks
    in neighboring frames as references for instance segmentation at the current frame.
    The following year, Tran et al. introduced a multi-referenced guided instance
    segmentation scheme [[56](#bib.bib56)]. After the first round of mask propagation,
    reliable frames are cached in memory as references for the second round of mask
    propagation. However, this two-pass processing approach is inefficient for dealing
    with long videos. Similarly, in [[57](#bib.bib57)], Bertasius et al. proposed
    to propagate the instance features in a specific frame to the whole video clip
    by comparing the difference of the feature tensors, which enables a clip-level
    instance tracking. The approach performs better than MaskTrack R-CNN when handling
    overlapping instances. However, it appears to be challenging to separate dense
    instances in videos and identify fine-grained instance features [[127](#bib.bib127)],
    as it relies too heavily on the mask prediction of Mask R-CNN. Inspired by the
    findings in [[128](#bib.bib128)], the authors of [[58](#bib.bib58)] introduced
    a propose-reduce paradigm for semi-supervised VIS. Specifically, based on Mask
    R-CNN, a sequence propagation head is appended to generate instance sequence proposals
    based on multiple keyframes in the video. Then, redundant proposals involving
    the same instances are reduced through various non-maximum suppression (NMS) techniques.
    This propose-reduce strategy is straightforward for tracking instances in long
    videos. Nonetheless, it is by no means trivial to strike a good balance between
    computing overhead and instance tracking performance when adjusting the number
    of sequence proposals. A more effective keyframe selection method is required
    to improve performance and computational efficiency.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 训练半监督视频实例分割（VIS）模型的一个典型方法是将一个或几个关键帧的掩码传播到整个视频或视频片段。例如，Tran 等人提出了一种双向实例分割方法[[55](#bib.bib55)]。该方法制定了一种前向和后向传播策略，以利用邻近帧中的掩码作为当前帧实例分割的参考。次年，Tran
    等人介绍了一种多参考引导实例分割方案[[56](#bib.bib56)]。在第一次掩码传播之后，可靠的帧被缓存到内存中，作为第二轮掩码传播的参考。然而，这种双重处理方法在处理长视频时效率较低。同样，在[[57](#bib.bib57)]中，Bertasius
    等人提出通过比较特征张量的差异，将特定帧中的实例特征传播到整个视频片段，这使得片段级实例跟踪成为可能。这种方法在处理重叠实例时表现优于 MaskTrack
    R-CNN。然而，在视频中分离密集实例和识别细粒度实例特征[[127](#bib.bib127)]似乎具有挑战性，因为它过于依赖 Mask R-CNN 的掩码预测。受到[[128](#bib.bib128)]研究发现的启发，[[58](#bib.bib58)]的作者引入了一种提出-减少范式用于半监督
    VIS。具体来说，基于 Mask R-CNN，添加了一个序列传播头，根据视频中的多个关键帧生成实例序列提案。然后，通过各种非最大抑制（NMS）技术减少涉及相同实例的冗余提案。这种提出-减少策略在跟踪长视频中的实例时比较直接。然而，在调整序列提案数量时，要在计算开销和实例跟踪性能之间取得良好的平衡并非易事。需要更有效的关键帧选择方法以提高性能和计算效率。
- en: Instead of mask propagation, another way to train a semi-supervised VIS model
    is to utilize the Inter-pixel Relation Network (IRNet) [[129](#bib.bib129)]. IRNet
    is a CNN architecture built on a Class Attention Map (CAM). The CAM locates distinct
    instances and approximates their rough boundaries with only image-level supervision.
    In [[59](#bib.bib59)], Liu et al. adopted IRNet with optical flow to assign similar
    labels to pixels with similar motion. Temporal consistency is leveraged to propagate
    trustworthy predictions between adjacent frames, in order to recover missing instances
    between frames. The method is superior to other image instance segmentation schemes,
    however, there is still a gap in precision when compared with supervised VIS schemes.
    Similarly, in [[60](#bib.bib60)], Ruiz et al. proposed a weakly supervised learning
    strategy for MOTS based on gradient-weighted CAM (Grad-CAM) [[130](#bib.bib130)].
    In essence, Grad-CAM is a class-discriminative localization technique that predicts
    a coarse localization heat map for the target concept. Since multi-task learning
    is incorporated in their scheme for predicting masks, bounding boxes, and classes
    simultaneously, Grad-CAM is utilized to locate the foreground mask based on the
    output of the classification branch.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了掩码传播，训练半监督 VIS 模型的另一种方法是利用像素间关系网络（IRNet）[[129](#bib.bib129)]。IRNet 是一种基于类别注意力图（CAM）的
    CNN 架构。CAM 通过仅使用图像级别的监督来定位不同的实例并大致估计它们的边界。在 [[59](#bib.bib59)] 中，刘等人采用了 IRNet
    和光流，将相似的标签分配给具有相似运动的像素。时间一致性被利用于在相邻帧之间传播可靠的预测，以恢复帧之间缺失的实例。该方法优于其他图像实例分割方案，但与监督
    VIS 方案相比，仍存在精度差距。同样，在 [[60](#bib.bib60)] 中，Ruiz 等人提出了一种基于梯度加权 CAM（Grad-CAM）[[130](#bib.bib130)]
    的弱监督学习策略。实质上，Grad-CAM 是一种类别区分的定位技术，用于预测目标概念的粗略定位热图。由于他们的方案中结合了多任务学习来同时预测掩码、边界框和类别，Grad-CAM
    被用于根据分类分支的输出定位前景掩码。
- en: Several studies leverage attention mechanisms to improve the detection, segmentation,
    and tracking capabilities of conventional multi-stage feature processing schemes.
    In [[61](#bib.bib61)], Liu et al. embedded a spatiotemporal attention network
    to MaskTrack R-CNN. The aim is to focus on the instances belonging to pre-defined
    categories by estimating the attention on two consecutive frames. In [[62](#bib.bib62)],
    Fu et al. introduced a frame-level attention module and an object-level attention
    module to Mask R-CNN for better RoI and object proposals. In [[63](#bib.bib63)],
    Abrantes et al. adopted the Transformer to conduct frame-level instance segmentation,
    followed by the temporal attention refinement of masks within tracklets. Similarly,
    in [[64](#bib.bib64)], a temporal attention module and a spatial attention module
    were incorporated into Mask R-CNN to refine the instance-aware representations
    in videos. A recurrent Transformer-based refinement strategy was used with Mask
    R-CNN to predict low-dimensional mask embeddings and improve performance [[65](#bib.bib65)].
    However, such partially attention-improved schemes typically bring further complexity
    and computational overhead to classical multi-stage feature processing frameworks,
    with marginal improvement in performance. By abolishing the classical multi-stage
    feature-processing design, the work in [[66](#bib.bib66)] decoupled the VIS schemes
    into three sub-tasks, i.e. segmentation, tracking, and refinement, with each of
    them handled by a different attention module. After segmenting instances at the
    frame level, a cross-attention mechanism [[46](#bib.bib46)] is leveraged to model
    the inter-frame association, thus tracking instances and achieving online VIS.
    Besides, an offline refiner module based on Transformer is devised for exploiting
    the context information from the entire video to refine the output of instance
    tracking.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 几项研究利用注意力机制来改进传统的多阶段特征处理方案的检测、分割和跟踪能力。在[[61](#bib.bib61)]中，刘等人嵌入了一个时空注意力网络到MaskTrack
    R-CNN中。旨在通过估计两个连续帧上的注意力来聚焦于预定义类别的实例。在[[62](#bib.bib62)]中，付等人为了更好地RoI和对象提议，引入了一个基于帧级别的注意力模块和一个基于对象级别的注意力模块到Mask
    R-CNN中。在[[63](#bib.bib63)]中，Abrantes等人采用Transformer对帧级别实例分割进行处理，随后对轨迹中的蒙版进行时间注意力调整。同样，在[[64](#bib.bib64)]中，一个时间注意力模块和一个空间注意力模块被结合到Mask
    R-CNN中，以改进视频中实例感知的表示。在Mask R-CNN中使用了基于循环的Transformer改进策略，用于预测低维蒙版嵌入并提高性能[[65](#bib.bib65)]。然而，这种部分注意力改进的方案通常会给传统的多阶段特征处理框架带来进一步的复杂性和计算开销，而性能的提升却很有限。通过废除传统的多阶段特征处理设计，[66](#bib.bib66)]的工作将VIS方案分解为三个子任务，即分割、跟踪和细化，每个子任务都由不同的注意力模块处理。在帧级别对实例进行分割后，利用交叉注意力机制[[46](#bib.bib46)]来建模帧间的关联，从而追踪实例并实现在线视觉实例分割。此外，还设计了一个基于Transformer的离线优化模块，用于从整个视频中利用上下文信息来完善实例跟踪的输出。
- en: In terms of efficiency, some researchers focus on reducing the computational
    complexity of models for VIS so that they can be applied to vehicles and other
    edge devices. Dong et al. [[67](#bib.bib67)] introduced a lightweight VIS network,
    which regresses a group of fixed edge points in a polar coordinate system, rather
    than predicting conventional instance masks. After extracting features with the
    FPN, the centroid of instances is predicted based on the heat map, which is then
    utilized for predicting polar masks. The approach is capable of real-time tracking
    on mobile edge computing platforms. Another idea to improve computational efficiency
    is eliminating the predefined set of anchor boxes [[131](#bib.bib131)]. Liu et
    al. [[68](#bib.bib68)] extended FCOS [[131](#bib.bib131)], an anchor box-free
    and proposal-free object detection model, to VIS by incorporating an additional
    tracking head and a mask head. Target instances are dynamically divided into sub-regions
    based on their bounding box for fine-grained instance segmentation. The tracking
    head directly models object movements using object detection centers generated
    with FCOS to track instances. However, for these efficiency-focused methods, overcoming
    the occlusion and motion blur issues as well as improving segmentation precision
    remain challenging tasks.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在效率方面，一些研究人员关注于降低视觉实例分割（VIS）模型的计算复杂度，以便它们能够应用于车辆和其他边缘设备。Dong等人[[67](#bib.bib67)]提出了一种轻量级的VIS网络，该网络在极坐标系中回归一组固定的边缘点，而不是预测传统的实例掩膜。在用FPN提取特征后，基于热图预测实例的质心，随后用于预测极坐标掩膜。这种方法能够在移动边缘计算平台上实现实时跟踪。提高计算效率的另一个想法是消除预定义的锚框[[131](#bib.bib131)]。Liu等人[[68](#bib.bib68)]通过引入额外的跟踪头和掩膜头，将无锚框和无提议的物体检测模型FCOS[[131](#bib.bib131)]扩展到VIS。目标实例根据其边界框动态划分为子区域，以进行细粒度实例分割。跟踪头直接使用FCOS生成的物体检测中心来建模物体运动以跟踪实例。然而，对于这些以效率为重点的方法，克服遮挡和运动模糊问题以及提高分割精度仍然是具有挑战性的任务。
- en: 3.2 Multi-Branch Feature Processing Architecture
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 多分支特征处理架构
- en: The multi-branch feature processing architecture consists of multiple branches
    working in parallel to process different aspects or representations of the input
    data. The branches typically process features for different subtasks, and the
    outputs of these branches are often fused or combined to achieve VIS. Through
    multiple branches, the model can capture complementary information and learn robust
    and discriminative representations. In fact, multi-branch architectures typically
    can yield improved performance as compared to single-branch architectures, but
    at the expense of increased parameters and computational overhead during training
    and inference. Some representative studies utilizing this architecture are as
    follows.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 多分支特征处理架构由多个并行工作的分支组成，以处理输入数据的不同方面或表示。这些分支通常处理不同子任务的特征，这些分支的输出通常会融合或组合，以实现VIS。通过多个分支，模型可以捕捉互补信息，学习到稳健且具有辨别力的表示。实际上，多分支架构通常可以比单一分支架构获得更好的性能，但代价是增加了训练和推理过程中的参数和计算开销。一些利用这种架构的代表性研究如下。
- en: The saliency map is a crucial cue for VIS to focus its search on salient regions
    in each frame [[132](#bib.bib132), [35](#bib.bib35)]. Standing on the viewpoint
    of two subtasks, semantic instance segmentation (SIS) and salient object segmentation
    (SOS), Le et al. [[69](#bib.bib69)] proposed a Semantic Instance - Salient Object
    (SISO) framework with two branches in charge of these two subtasks, respectively.
    In terms of semantic instance segmentation, instance masks with high confidence
    at each frame are propagated and integrated into instances in later frames. In
    terms of salient object segmentation, a 3D fully convolutional network (3D-FCN) [[133](#bib.bib133)]
    is adopted to compute salient region masks. By fusing the features from these
    two branches and integrating an identity-tracking module, semantic salient instances
    in the video are finally segmented. In [[70](#bib.bib70)], Lin et al. proposed
    a similar idea that simultaneously captures features shared by all instances (i.e.,
    SOS) and discriminates different instances by instance-specific features (i.e.
    SIS). The framework consists of two branches, one of which is dedicated to an
    instance-agnostic module, and the other to an instance-specific module. The features
    from both branches are then fused by an attention-guided decoder, followed by
    a final prediction module. Similarly, Ge et al. [[71](#bib.bib71)] designed two
    branches based on the correlation matrix, with one generating coarse instance
    score maps and the other separating the foreground from the background. In [[72](#bib.bib72)],
    the authors argued that instance understanding matters in VOS and developed a
    two-branch network. The instance segmentation branch explores the instance features
    of the current frame while the VOS branch performs spatiotemporal matching with
    the memory bank. Despite yielding a good performance by adopting models pre-trained
    on public datasets, fine-tuning the models for both branches is more challenging
    than for multi-stage schemes. Besides, compared to single-branch approaches, multi-branch
    schemes typically have more parameters and lower processing efficiency.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性图是视觉系统在每帧中关注显著区域的关键线索[[132](#bib.bib132), [35](#bib.bib35)]。从语义实例分割（SIS）和显著对象分割（SOS）这两个子任务的角度出发，Le等人[[69](#bib.bib69)]提出了一个语义实例-显著对象（SISO）框架，该框架包含两个分支，分别负责这两个子任务。在语义实例分割方面，每帧的高置信度实例掩膜被传播并整合到后续帧中的实例里。在显著对象分割方面，采用了3D全卷积网络（3D-FCN）[[133](#bib.bib133)]来计算显著区域掩膜。通过融合这两个分支的特征并整合身份追踪模块，视频中的语义显著实例最终被分割。在[[70](#bib.bib70)]中，Lin等人提出了一个类似的想法，既捕捉所有实例共享的特征（即SOS），又通过实例特定的特征区分不同的实例（即SIS）。该框架包含两个分支，其中一个专注于与实例无关的模块，另一个则专注于实例特定的模块。然后，利用注意力引导解码器融合两个分支的特征，接着是最终预测模块。同样，Ge等人[[71](#bib.bib71)]基于相关矩阵设计了两个分支，一个生成粗略实例评分图，另一个则将前景与背景分开。在[[72](#bib.bib72)]中，作者认为实例理解在视频目标分割（VOS）中很重要，并开发了一个两分支网络。实例分割分支探索当前帧的实例特征，而VOS分支则与记忆库进行时空匹配。尽管通过采用在公共数据集上预训练的模型取得了良好性能，但对两个分支的模型进行微调比多阶段方案更具挑战。此外，与单分支方法相比，多分支方案通常具有更多的参数和较低的处理效率。
- en: Another method for solving VIS is assigning two branches to perform the detect
    and track operations, respectively. In [[73](#bib.bib73)], a Hybrid Task Cascade
    (HTC) [[134](#bib.bib134)] method was devised to conduct image instance segmentation,
    and SiamMask [[135](#bib.bib135)] was utilized to track objects. Two original
    SiamMasks are cascaded to better predict the mask of instances. However, when
    dealing with overlapping instances, the performance is significantly influenced
    by the image instance segmentation module. To deal with situations where instances
    vanish and then re-appear, some researchers introduce a memory bank beside the
    instance mask prediction branch to store representative motion patterns [[74](#bib.bib74)].
    Before being fed to a decoder to predict the target mask, the encoded mask features
    in the current frame are combined with the motion pattern representations retrieved
    from the memory bank, which improves robustness toward occlusion and fast-moving
    objects. However, the performance is subject to the motion patterns learned in
    previous frames. The extra computational and storage overheads caused by ConvLSTM
    and the memory bank are also non-negligible.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决VIS的方法是分配两个分支分别执行检测和跟踪操作。在[[73](#bib.bib73)]中，设计了一种混合任务级联（HTC）[[134](#bib.bib134)]方法来进行图像实例分割，并且使用了SiamMask[[135](#bib.bib135)]来跟踪对象。两个原始的SiamMask被级联在一起，以更好地预测实例的掩码。然而，当处理重叠实例时，性能会受到图像实例分割模块的显著影响。为了应对实例消失后再出现的情况，一些研究者在实例掩码预测分支旁引入了一个记忆库来存储代表性的运动模式[[74](#bib.bib74)]。在被送入解码器以预测目标掩码之前，当前帧中的编码掩码特征与从记忆库中检索到的运动模式表示相结合，从而提高了对遮挡和快速移动物体的鲁棒性。然而，性能受制于之前帧中学习到的运动模式。ConvLSTM和记忆库所带来的额外计算和存储开销也不可忽视。
- en: As the You Only Look Once (YOLO) [[136](#bib.bib136)] algorithm becomes popular
    for object detection in images, an extended variant, named YOLACT [[75](#bib.bib75),
    [76](#bib.bib76)], demonstrated its effectiveness in VIS. The two-branch design
    of YOLACT for prototype mask generating and mask coefficient prediction enables
    efficient image-level instance segmentation. The effectiveness of YOLACT under
    severe occlusions was validated in [[77](#bib.bib77)]. Based on YOLACT, in [[78](#bib.bib78),
    [79](#bib.bib79)], the authors proposed SipMask. The model makes greater preservation
    of the spatial information contained within an instance by dividing mask prediction
    into several sub-mask predictions. In addition to image-level instance segmentation
    tasks, SipMask is validated as effective for VIS tasks. Although it is an efficient
    solution for VIS, the scheme lacks the utilization of temporal information across
    frames in video. An idea to improve YOLACT was proposed by adding another tracking
    decoder branch to produce embedding vectors for all instances [[137](#bib.bib137)].
    Denoted as YolTrack, it improves the inference speed to a real-time level, but
    at the expense of accuracy and precision. Moreover, in [[80](#bib.bib80)], the
    FPN features from two adjacent frames were fused to explore temporal correlations.
    The approach comprises two branches of YOLACT for frame-level instance segmentation
    and an additional temporal branch for fusion features between two consecutive
    frames. Nevertheless, the approach lacks the comprehension and tracking of instances
    in long videos, especially when dealing with instances that disappear and re-appear.
    On the other hand, to improve the efficiency of YOLACT for use on edge devices,
    the authors of [[81](#bib.bib81)] proposed YolactEdge. It reduces the computational
    overhead on non-keyframes of the video by computing only a subset of the features.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 随着“你只看一次（YOLO）”[[136](#bib.bib136)]算法在图像对象检测中的流行，一个扩展变体YOLACT[[75](#bib.bib75),
    [76](#bib.bib76)]展示了其在VIS中的有效性。YOLACT的双分支设计用于原型掩模生成和掩模系数预测，实现了高效的图像级实例分割。YOLACT在严重遮挡下的有效性在[[77](#bib.bib77)]中得到了验证。基于YOLACT，在[[78](#bib.bib78),
    [79](#bib.bib79)]中，作者提出了SipMask。该模型通过将掩模预测划分为多个子掩模预测，更好地保留了实例中的空间信息。除了图像级实例分割任务，SipMask还被验证对VIS任务有效。尽管这是一个高效的VIS解决方案，但该方案缺乏对视频中帧间时间信息的利用。为了改进YOLACT，提出了通过添加另一个跟踪解码器分支来生成所有实例的嵌入向量[[137](#bib.bib137)]。被称为YolTrack，它将推断速度提高到实时水平，但以牺牲准确性和精度为代价。此外，在[[80](#bib.bib80)]中，将两个相邻帧的FPN特征进行了融合，以探索时间相关性。该方法包括两个YOLACT分支用于帧级实例分割以及一个额外的时间分支用于融合两个连续帧之间的特征。然而，该方法在处理长视频中的实例，尤其是处理消失和重新出现的实例时，缺乏对实例的理解和跟踪。另一方面，为了提高YOLACT在边缘设备上的效率，[[81](#bib.bib81)]的作者提出了YolactEdge。它通过仅计算特征的一个子集来减少视频非关键帧的计算开销。
- en: Multi-branch design of the Siamese network [[5](#bib.bib5)] is naturally ideal
    for tracking instances across different frames by comparing their feature embeddings.
    To allow the tracking clues to better assist in detection, the work in [[82](#bib.bib82)]
    proposed an association module based on a Siamese network. It extracts re-identification
    embedding features in consecutive frames to improve segmentation in the current
    processing frame. In [[83](#bib.bib83)], a novel crossover learning scheme was
    devised for VIS based on the Siamese network, named CrossVIS. In particular, crossover
    learning enables dynamic filters to learn background-irrelevant representations
    of the same instance at two different frames. Based on the Siamese network, contrastive
    learning appears to be an efficient way to learn representations. The work in [[84](#bib.bib84)]
    made the embeddings of the same instance closer and the embeddings of different
    instances farther apart in the embedding space by comparing the adjacent frames.
    The method achieved an overall first place in the 2022 YouTube-VIS challenge [[85](#bib.bib85)].
    Another contrastive learning-based VIS strategy proposed in [[86](#bib.bib86)]
    aimed to improve instance association accuracy. Inspired by [[138](#bib.bib138)],
    a bi-directional spatiotemporal learning scheme was introduced for training. Although
    the Siamese network provides efficient feature comparison for instance tracking
    between consecutive frames in a video, it is nevertheless challenging for these
    schemes to understand a long video and re-identify those vanish and re-appearing
    instances. To improve robustness against occlusions and reappearance, identification
    and association modules were leveraged to predict identification numbers and track
    instances [[87](#bib.bib87)]. The identification module detects new instances,
    assigns them identities, and encodes them into embeddings for further propagation
    across frames. With embeddings of the last frame stored, the association module
    effectively aids in propagating information from previous frames to the current
    frame. The approach obtains good performance for solving occluded VIS tasks. Subsequently,
    in [[88](#bib.bib88)], the authors proposed a grid-structured VIS model, i.e.,
    VISOLO, based on the image instance segmentation scheme, SOLO [[139](#bib.bib139),
    [140](#bib.bib140)]. Specifically, each frame in the video is divided into uniform
    grids. They serve as the basic unit for creating semantic category scores and
    instance masks. Then, the memory-matching module, which caches feature maps of
    previous frames, calculates the similarity measure between grids in different
    frames for instance tracking. The grid-level features, in comparison with convolutional
    features, are easier to reuse and share across numerous modules. This allows the
    method to preserve a longer history of feature maps and improve robustness against
    occlusions and reappearance.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 孪生网络的多分支设计[[5](#bib.bib5)] 自然适合通过比较特征嵌入在不同帧之间跟踪实例。为了使跟踪线索更好地辅助检测，[[82](#bib.bib82)]的工作提出了一个基于孪生网络的关联模块。该模块提取连续帧中的再识别嵌入特征，以改善当前处理帧中的分割。在[[83](#bib.bib83)]中，提出了一种基于孪生网络的创新交叉学习方案，称为CrossVIS。特别是，交叉学习使动态滤波器能够学习在两个不同帧中的同一实例的背景无关表示。基于孪生网络，对比学习似乎是一种有效的表示学习方式。[[84](#bib.bib84)]的工作通过比较相邻帧，使同一实例的嵌入更接近，不同实例的嵌入更远离。这种方法在2022年YouTube-VIS挑战赛中获得了整体第一名[[85](#bib.bib85)]。另一种基于对比学习的VIS策略在[[86](#bib.bib86)]中提出，旨在提高实例关联的准确性。受[[138](#bib.bib138)]的启发，引入了一种双向时空学习方案进行训练。尽管孪生网络为视频中连续帧之间的实例跟踪提供了高效的特征比较，但这些方案在理解长视频以及重新识别消失和重新出现的实例时仍然具有挑战性。为了提高对遮挡和重现的鲁棒性，利用了识别和关联模块来预测识别编号并跟踪实例[[87](#bib.bib87)]。识别模块检测新实例，分配身份，并将其编码为嵌入，以便在帧之间进一步传播。通过存储最后一帧的嵌入，关联模块有效地帮助将信息从之前的帧传播到当前帧。该方法在解决遮挡VIS任务时表现良好。随后，在[[88](#bib.bib88)]中，作者提出了一种基于图像实例分割方案SOLO[[139](#bib.bib139),
    [140](#bib.bib140)]的网格结构VIS模型，即VISOLO。具体而言，视频中的每一帧都被划分为均匀的网格。这些网格作为创建语义类别分数和实例掩码的基本单元。然后，记忆匹配模块缓存之前帧的特征图，计算不同帧中网格之间的相似性度量以进行实例跟踪。与卷积特征相比，网格级特征更容易在多个模块之间重用和共享。这使得该方法能够保留更长时间的特征图历史，提高了对遮挡和重现的鲁棒性。
- en: As Transformers arise in VIS, there are various proposed schemes that use the
    Siamese network to build inter-frame attention between the target frame and the
    referring frame. In [[89](#bib.bib89)], the authors introduced an intra-frame
    attention module with shared weight to the Siamese network for linking both instance-
    and pixel-level features in each frame. Besides, an inter-frame attention module
    is used to fuse hybrid temporal information and learn temporal consistency across
    frames. Similar attention-based temporal context fusion was adopted in studies
    reported in [[90](#bib.bib90), [85](#bib.bib85), [84](#bib.bib84), [91](#bib.bib91)]
    for inter-frame instance association. Although inter-frame attention is useful
    for tracking instances across different frames, it is challenging to effectively
    select reference keyframes in long videos in a way that reduces computational
    complexity while improving the accuracy of instance tracking. As a result, an
    inter-clip attention scheme based on Transformers was proposed in [[92](#bib.bib92)].
    Specifically, by comparing the similarity of features of both target and referring
    clips, the instance sequences in the target video are learned in a few-shot manner.
    However, only limited evidence on the effectiveness of the method was provided.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Transformers在VIS中的出现，提出了各种使用Siamese网络在目标帧和参考帧之间建立帧间注意力的方案。在[[89](#bib.bib89)]中，作者为Siamese网络引入了具有共享权重的帧内注意力模块，以在每一帧中链接实例级别和像素级别的特征。此外，还使用了一个帧间注意力模块来融合混合的时间信息，并学习跨帧的时间一致性。在[[90](#bib.bib90),
    [85](#bib.bib85), [84](#bib.bib84), [91](#bib.bib91)]中报告的研究中也采用了类似的基于注意力的时间上下文融合方法用于帧间实例关联。虽然帧间注意力对于跨不同帧跟踪实例非常有用，但在长视频中有效选择参考关键帧以减少计算复杂性同时提高实例跟踪准确性仍然具有挑战性。因此，在[[92](#bib.bib92)]中提出了一种基于Transformers的片段间注意力方案。具体而言，通过比较目标片段和参考片段特征的相似性，以少量样本的方式学习目标视频中的实例序列。然而，关于该方法有效性的证据仅限于少量。
- en: Knowledge distillation [[141](#bib.bib141)] is a machine learning method that
    transfers knowledge from a large model (teacher) to a smaller one (student). It
    allows an online VIS model to learn a wealth of knowledge from an offline model
    for consistent instance tracking and segmentation. Kim et al. [[93](#bib.bib93)]
    proposed an offline-to-online knowledge distillation (OOKD) for VIS. The devised
    query filtering and association (QFA) module filters out bad queries and links
    the instance features between offline and online models. By encoding object-centric
    features from a single frame and augmenting them with long-range global context
    distilled from the teacher model, the model demonstrates state-of-the-art feature
    matching and instance tracking capabilities.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏[[141](#bib.bib141)]是一种将知识从大模型（教师）转移到小模型（学生）的机器学习方法。它允许一个在线VIS模型从离线模型中学习大量知识，以实现一致的实例跟踪和分割。Kim等人[[93](#bib.bib93)]提出了一种用于VIS的离线到在线知识蒸馏（OOKD）方法。设计的查询过滤和关联（QFA）模块过滤掉不良查询，并将离线和在线模型之间的实例特征连接起来。通过从单帧中编码对象中心特征，并结合从教师模型中提取的长距离全局上下文，该模型展示了最先进的特征匹配和实例跟踪能力。
- en: Point cloud is also an effective way to learn instance representation. In [[94](#bib.bib94),
    [95](#bib.bib95)], the authors built two 2D point clouds in two separate branches
    to learn features from the foreground and surrounding area. The features for segmentation,
    such as offset, color, category, and position, can be extracted from cloud points.
    However, the precision of the scheme highly relies on the number of points utilized,
    while the use of more points results in a significantly heavier computational
    burden.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 点云也是学习实例表示的有效方法。在[[94](#bib.bib94), [95](#bib.bib95)]中，作者在两个独立的分支中构建了两个二维点云，以从前景和周围区域学习特征。可以从点云中提取出用于分割的特征，如偏移量、颜色、类别和位置。然而，该方案的精度高度依赖于所使用的点的数量，而使用更多的点会导致显著更重的计算负担。
- en: 3.3 Hybrid Feature Processing Architecture
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 混合特征处理架构
- en: Hybrid feature processing architectures integrate multi-stage and multi-branch
    architectures into an integrated framework. With multi-stage processing in each
    branch, the features are aggregated and processed at higher semantic levels, enabling
    better performance on each subtask. On the other hand, with multi-branch processing
    different subtasks of VIS, the features are learned in a robust and discriminative
    manner. However, hybrid architectures are typically more complex than multi-stage
    and multi-branch architectures, raising concerns among researchers on whether
    the improved performance compensates for the higher computational burden. The
    works discussed below are some examples of hybrid feature processing architectures
    characterized by a multi-branch encoder-decoder design.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 混合特征处理架构将多阶段和多分支架构集成到一个综合框架中。通过在每个分支中的多阶段处理，特征在更高的语义层级上被聚合和处理，从而在每个子任务上实现更好的性能。另一方面，通过多分支处理VIS的不同子任务，特征以稳健且具有区分性的方法进行学习。然而，混合架构通常比多阶段和多分支架构更复杂，研究人员担心其提高的性能是否能弥补更高的计算负担。以下讨论的工作是一些具有多分支编码器-解码器设计特征的混合特征处理架构的例子。
- en: Without initial masks in the first frame, the variational autoencoder was incorporated
    into Mask R-CNN to aid in capturing spatial and motion information shared by all
    instances [[96](#bib.bib96)]. Specifically, there is one encoder in the architecture
    for generating latent distribution, along with three parallel decoders assigned
    to three different branches. These branches are in charge of learning semantic
    information, providing attentive cues to reduce false negatives, and aggregating
    features from the encoder. However, the architecture almost doubles the complexity
    of the original Mask R-CNN model. Another similar work based on the encoder-decoder
    model is [[97](#bib.bib97), [98](#bib.bib98)]. Based on features from the image
    encoder, three decoders, spike decoder, position decoder, and appearance decoder,
    produce the latent distribution, offset vectors, and appearance embedding, respectively.
    Empirical results show that the approach outperforms MaskTrack R-CNN in AP by
    $3.5\%$ while being two times slower.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一帧中没有初始掩码的情况下，将变分自编码器集成到Mask R-CNN中，以帮助捕捉所有实例共享的空间和运动信息[[96](#bib.bib96)]。具体而言，该架构中有一个编码器用于生成潜在分布，还有三个并行解码器分配给三个不同的分支。这些分支负责学习语义信息，提供关注线索以减少假阴性，并从编码器中聚合特征。然而，该架构几乎使原始的Mask
    R-CNN模型的复杂性增加了一倍。另一个基于编码器-解码器模型的类似工作是[[97](#bib.bib97), [98](#bib.bib98)]。基于来自图像编码器的特征，三个解码器：脉冲解码器、位置解码器和外观解码器分别生成潜在分布、偏移向量和外观嵌入。实证结果表明，该方法在AP上比MaskTrack
    R-CNN提高了$3.5\%$，但速度慢了两倍。
- en: Considering the high annotation cost for VIS, a two-stage network was proposed
    in [[99](#bib.bib99)]. It includes a two-branch discrimination network (D-Net)
    for video object proposals and a two-branch target-aware tracking network (T-Net)
    for associating object proposals. In D-Net, one branch estimates the salient objects,
    whereas the other branch predicts the instance pixels. By comparing the object
    proposals in the current frame with historical tracking results, T-Net generates
    a segmentation score prediction for the target object. Although accuracy is slightly
    improved, the method places a large amount of computing overhead on both training
    and inference due to the lengthy and complex design. On the other hand, a semi-supervised
    framework requiring only bounding-box labels was developed in [[100](#bib.bib100)].
    Optical flow in a branch is exploited to capture the temporal motion among instances,
    while depth estimation is used in another branch to provide spatial correlation
    between instances. A series of pseudo labels for salient instances are generated
    by leveraging the features from both optical flow and depth estimation branches.
    A bounding-box supervised puzzle solver further refines and assembles the sub-optimal
    masks and recovers the original instances. The method is comparable to fully supervised
    TrackR-CNN and MaskTrack R-CNN in performance. Nevertheless, there is still room
    for improvement in terms of long-video understanding and identifying re-appearing
    instances. Besides, it is promising to remove the reliance on bounding box labels
    during training.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到VIS的高标注成本，[[99](#bib.bib99)]中提出了一种两阶段网络。该网络包括一个用于视频对象提议的双分支判别网络（D-Net）和一个用于关联对象提议的双分支目标感知跟踪网络（T-Net）。在D-Net中，一个分支估计显著对象，而另一个分支预测实例像素。通过比较当前帧中的对象提议与历史跟踪结果，T-Net为目标对象生成分割分数预测。尽管准确性有所提高，但由于设计复杂且冗长，该方法在训练和推理中都带来了大量的计算开销。另一方面，[[100](#bib.bib100)]中开发了一种仅需边界框标签的半监督框架。该框架在一个分支中利用光流捕捉实例之间的时间运动，而在另一个分支中使用深度估计提供实例之间的空间关联。通过利用来自光流和深度估计分支的特征，生成了一系列显著实例的伪标签。边界框监督的拼图求解器进一步细化和组装次优掩码，并恢复原始实例。该方法在性能上与完全监督的TrackR-CNN和MaskTrack
    R-CNN相当。然而，在长视频理解和识别重新出现的实例方面仍有改进的空间。此外，去除训练过程中的边界框标签依赖性也是一种有前景的方向。
- en: 3.4 Integrated Feature Processing Architecture
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 综合特征处理架构
- en: The integrated feature processing architecture typically extracts features of
    all frames in a video or clip together to build a 3D spatiotemporal feature volume.
    By aggregating the spatiotemporal features, the model, which typically consists
    of an encoder-decoder design, automatically learns the high-level representations
    of diverse instances across time and space, followed by a final prediction. Integrated
    architecture offers an elegant design when compared with multi-stage and multi-branch
    architectures, and gains popularity as the self-attention mechanism becomes widely
    used. However, it usually necessitates a lengthy training process, more training
    data, and more computational and memory resources. The works that utilize integrated
    feature processing architecture are reviewed, as follows.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 综合特征处理架构通常将视频或剪辑中的所有帧的特征一起提取，以构建一个3D时空特征体。通过聚合时空特征，该模型通常由一个编码器-解码器设计组成，自动学习跨时间和空间的多样实例的高级表示，然后进行最终预测。与多阶段和多分支架构相比，综合架构提供了一种优雅的设计，并且随着自注意力机制的广泛使用而受到越来越多的关注。然而，它通常需要较长的训练过程、更多的训练数据以及更多的计算和内存资源。以下是利用综合特征处理架构的相关工作回顾。
- en: In 2020, Athar et al. [[101](#bib.bib101)] adopted an FPN to extract different
    scales of feature maps. The feature maps are then stacked along the temporal dimension
    for decoding using a 3D-CNN model. However, the performance of this method largely
    depends on the capability of the 3D-CNN model to build the 3D mask tube. Precisely
    segmenting the 3D mask tube requires large memory consumption for storing more
    fine-grained spatiotemporal features. In order to extract background-irrelevant
    features and track instances throughout the entire scene, Brasó et al. [[102](#bib.bib102)]
    constructed a graph on a set of frames with each node representing an object detection.
    Then, feature embeddings obtained by the CNN are propagated across the graph for
    several iterations using neural message passing [[142](#bib.bib142)] for predicting
    instance masks for each RoI. Although the GNN is a promising solution for learning
    instance association and background-irrelevant features at the video level, its
    computational complexity is highly sensitive to the instance density in frames
    and the video length.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年，Athar 等人 [[101](#bib.bib101)] 采用了一个 FPN 来提取不同尺度的特征图。然后将这些特征图沿时间维度堆叠，以便使用
    3D-CNN 模型进行解码。然而，这种方法的性能在很大程度上依赖于 3D-CNN 模型构建 3D 掩模管道的能力。精确分割 3D 掩模管道需要大量的内存消耗，以存储更多的细粒度时空特征。为了提取与背景无关的特征并在整个场景中跟踪实例，Brasó
    等人 [[102](#bib.bib102)] 在一组帧上构建了一个图，每个节点代表一个对象检测。然后，通过 CNN 获得的特征嵌入在图上经过多次迭代的神经消息传递
    [[142](#bib.bib142)]，用于预测每个 RoI 的实例掩模。尽管 GNN 是一种有前途的解决方案，用于在视频级别学习实例关联和背景无关特征，但其计算复杂度对帧中的实例密度和视频长度高度敏感。
- en: As the self-attention mechanism of Transformers helps direct attention to the
    features of target instances at the image level [[46](#bib.bib46)], many attempts
    have been made to exploit this merit for 3D spatiotemporal feature extraction
    in VIS. In [[103](#bib.bib103)], Cheng et al. extended a Transformer-based image
    instance segmentation model, i.e., Mask2Former [[143](#bib.bib143)], to VIS. Masked
    attention is applied to the 3D spatiotemporal features for directly predicting
    a 3D mask for each instance across time. Choudhuri et al. [[104](#bib.bib104)]
    demonstrated that employing absolute position encoding like [[103](#bib.bib103)]
    could cause object queries to heavily rely on the positions of the instances,
    causing an inability to recognize instance position changes. Thus, they proposed
    relative object queries on relative positional encoding for the Transformer to
    better capture an instance’s position changes over frames. In [[49](#bib.bib49)],
    the authors extended the Transformer-based image object detection model DETR [[46](#bib.bib46)]
    to VIS. Denoted as VisTR, the method consists of an instance sequence matching
    module to supervise the instance sequence across frames, as well as an instance
    sequence segmentation module to accumulate the mask features and predict the final
    mask sequences. These Transformer-based schemes generate a sequence of instance
    predictions concurrently using all frames in a video or clip as the input, resulting
    in a substantial computational and memory overhead. To reduce computational and
    storage overhead, Hwang et al. [[105](#bib.bib105)] proposed a clip processing
    pipeline. It yields better performance over those per-frame methods and less memory
    usage over those per-video methods. Specifically, two Transformers are designed,
    one encodes each frame independently, and the other exchanges information between
    frames.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Transformers 的自注意力机制有助于在图像级别将注意力集中在目标实例的特征上 [[46](#bib.bib46)]，因此许多尝试已被提出以利用这一优点进行
    VIS 中的 3D 时空特征提取。在 [[103](#bib.bib103)] 中，Cheng 等人将基于 Transformer 的图像实例分割模型，即
    Mask2Former [[143](#bib.bib143)]，扩展到 VIS。对 3D 时空特征应用了掩模注意力，以直接预测每个实例在时间上的 3D 掩模。Choudhuri
    等人 [[104](#bib.bib104)] 证明，采用绝对位置编码像 [[103](#bib.bib103)] 可能会导致对象查询严重依赖于实例的位置，从而无法识别实例位置的变化。因此，他们提出了相对位置编码上的相对对象查询，以使
    Transformer 更好地捕捉实例在帧之间的位置变化。在 [[49](#bib.bib49)] 中，作者将基于 Transformer 的图像对象检测模型
    DETR [[46](#bib.bib46)] 扩展到 VIS。该方法称为 VisTR，包含一个实例序列匹配模块，以监督帧之间的实例序列，以及一个实例序列分割模块，用于累积掩模特征并预测最终的掩模序列。这些基于
    Transformer 的方案使用视频或剪辑中的所有帧作为输入，同时生成一系列实例预测，从而导致显著的计算和内存开销。为了减少计算和存储开销，Hwang 等人
    [[105](#bib.bib105)] 提出了一个剪辑处理管道。该管道在性能上优于那些逐帧方法，并且在内存使用上低于那些逐视频方法。具体来说，设计了两个
    Transformer，一个独立地编码每一帧，另一个在帧之间交换信息。
- en: Note that the self-attention mechanism of Transformers typically involves explosive
    computations and memory overheads over the space-time inputs of the entire video,
    as it has quadratic complexity with respect to the input sequence [[144](#bib.bib144)].
    Deformable attention [[144](#bib.bib144)] achieves smaller computational complexity
    than full attention, as it only pays attention to a small number of key sampling
    points around a reference point assigned to each query. As a result, a multi-level
    deformable attention scheme, named SeqFormer [[106](#bib.bib106)], was designed
    to encompass both frame- and instance-level attention queries on videos. In particular,
    SeqFormer first performs independent frame-level box queries using deformable
    attention [[144](#bib.bib144)]. Then, the instance query is conducted based on
    the features extracted by box queries on each frame, which generates the final
    segmentation mask sequence. Following SeqFormer [[106](#bib.bib106)], Zhang et
    al. [[107](#bib.bib107)] indicated the importance of multi-scale temporal information
    for VIS. They proposed TAFormer to incorporate both spatial and temporal multi-scale
    deformable attention modules in an encoder. While TAFormer performs marginally
    better than SeqFormer, it has more tuning parameters and computational complexity.
    Apart from deformable attention, the MSG-Transformer [[145](#bib.bib145)] is a
    computation-efficient variant of the self-attention mechanism in computer vision.
    Instead of applying full attention to images, MSG-Transformer adopts local attention
    to subregions, and introduces an additional messenger token to each subregion
    for exchanging information across different subregions. As a result, the work
    in [[108](#bib.bib108)] extended the MSG-Transformer to VIS to enable efficient
    computation, named TeViT. In particular, TeViT constructs patch tokens along with
    messenger tokens on all frames in the video, and shifts messenger tokens across
    the time dimension for capturing temporal contextual information. Compared with
    VisTR, TeViT achieves better video processing speed and instance segmentation
    precision. Additionally, SeaFormer [[146](#bib.bib146)], a lightweight ViT with
    squeeze-enhanced axial attention, was leveraged to produce an efficient VIS scheme
    for mobile devices [[109](#bib.bib109)]. To speed up the convergence of VisTR,
    EfficientVIS was proposed in [[110](#bib.bib110)] by leveraging the clip processing
    pipeline. In particular, EfficientVIS extends Sparse R-CNN [[47](#bib.bib47)]
    with self-attention to support clip-level queries and proposals. However, the
    performance of clip-level queries and proposals in EfficientVIS is highly subject
    to the precision of spatiotemporal RoI [[124](#bib.bib124)] on video clips. On
    the other hand, considering that the dense spatiotemporal features extracted from
    videos are the key reasons for high computational complexity, VITA [[111](#bib.bib111)]
    was formulated to extract only object-aware context through a frame-level object
    detector. By collecting the frame-level object tokens for the entire video, VITA
    builds the relationships between every detected object and achieves global video
    understanding. Moreover, a simple and computation-efficient Transformer-based
    VIS scheme, i.e., MinVIS [[112](#bib.bib112)], was developed. MinVIS only trains
    a query-based image instance segmentation model. In the post-processing step,
    the instances are tracked by bipartite matching of query embeddings across frames.
    MinVIS also supports sub-sampling the annotated frames in training videos to further
    improve training efficiency.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Transformers的自注意力机制通常涉及对整个视频的时空输入进行爆炸性的计算和内存开销，因为它对输入序列具有平方复杂度[[144](#bib.bib144)]。可变形注意力[[144](#bib.bib144)]相比于全注意力，具有更小的计算复杂度，因为它仅关注分配给每个查询的参考点周围的少量关键采样点。因此，设计了一个多级可变形注意力方案，名为SeqFormer[[106](#bib.bib106)]，以涵盖视频的帧级和实例级注意力查询。具体而言，SeqFormer首先使用可变形注意力[[144](#bib.bib144)]执行独立的帧级框查询。然后，实例查询基于从每帧的框查询中提取的特征进行，这生成最终的分割掩码序列。继SeqFormer[[106](#bib.bib106)]之后，Zhang等人[[107](#bib.bib107)]指出了多尺度时间信息对VIS的重要性。他们提出了TAFormer，将空间和时间多尺度可变形注意力模块结合在一个编码器中。虽然TAFormer的表现略好于SeqFormer，但它有更多的调整参数和计算复杂度。除了可变形注意力，MSG-Transformer[[145](#bib.bib145)]是计算高效的自注意力机制变体。在处理图像时，MSG-Transformer不采用全注意力，而是采用局部注意力对子区域进行处理，并在每个子区域引入一个额外的信使标记，以便在不同子区域之间交换信息。因此，在[[108](#bib.bib108)]中，将MSG-Transformer扩展到VIS以实现高效计算，命名为TeViT。具体而言，TeViT在视频中的所有帧上构建补丁标记和信使标记，并在时间维度上移动信使标记以捕捉时间上下文信息。与VisTR相比，TeViT在视频处理速度和实例分割精度方面表现更好。此外，SeaFormer[[146](#bib.bib146)]，一种具有压缩增强轴向注意力的轻量级ViT，被用来为移动设备生成高效的VIS方案[[109](#bib.bib109)]。为了加速VisTR的收敛，EfficientVIS在[[110](#bib.bib110)]中通过利用剪辑处理管道被提出。具体而言，EfficientVIS将自注意力扩展到Sparse
    R-CNN[[47](#bib.bib47)]，以支持剪辑级查询和建议。然而，EfficientVIS中的剪辑级查询和建议的性能高度依赖于视频剪辑上时空RoI[[124](#bib.bib124)]的精度。另一方面，考虑到从视频中提取的密集时空特征是计算复杂度高的主要原因，VITA[[111](#bib.bib111)]被制定为通过帧级目标检测器仅提取对象感知上下文。通过收集整个视频的帧级目标标记，VITA建立了每个检测到的对象之间的关系，并实现了全局视频理解。此外，还开发了一种简单且计算高效的基于Transformer的VIS方案，即MinVIS[[112](#bib.bib112)]。MinVIS仅训练基于查询的图像实例分割模型。在后处理步骤中，通过跨帧的查询嵌入的二分匹配来跟踪实例。MinVIS还支持在训练视频中对子采样已标注的帧，以进一步提高训练效率。
- en: In terms of annotation-efficient VIS, based on Mask2Former [[103](#bib.bib103)],
    the work in [[113](#bib.bib113)] introduced MaskFreeVIS to substitute the requirement
    for mask annotations with bounding box annotations during the training. Specifically,
    BoxInst [[147](#bib.bib147)], a bounding-box supervised image instance segmentation
    approach, is extended with the Temporal KNN-patch Loss (TK-Loss). The method identifies
    one-to-many matches across frames through an efficient patch-matching step, followed
    by a K-nearest neighbor selection. Empirical studies demonstrate that MaskFreeVIS
    outperforms certain fully-supervised models like EfficientVIS [[110](#bib.bib110)].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在注释高效的视觉目标检测方面，基于 Mask2Former [[103](#bib.bib103)]，在 [[113](#bib.bib113)] 的研究中引入了
    MaskFreeVIS，通过使用边界框注释替代掩模注释的方式来进行训练。具体来说，BoxInst [[147](#bib.bib147)]，一种基于边界框的图像实例分割方法，通过引入时序
    KNN-patch 损失（TK-Loss）进行了扩展。该方法通过高效的补丁匹配步骤识别帧间的一对多匹配，然后进行 K-最近邻选择。实证研究表明，MaskFreeVIS
    的性能优于某些完全监督的模型，如 EfficientVIS [[110](#bib.bib110)]。
- en: 3.5 Recurrent Feature Processing Architecture
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 循环特征处理架构
- en: The recurrent feature processing architecture involves recurrently extracting
    and processing features from frames along the temporal axes. By recurrently propagating
    the features of past frames to the current frame, the recurrent architecture design
    allows a model to track instances in videos with marginal memory overhead. In
    addition to RNNs, as ViT becomes prominent, some works also propagate the object
    queries in Transformers in this way, therefore they are included in this section.
    The following are some studies that utilize recurrent feature processing architectures.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 循环特征处理架构涉及从帧中反复提取和处理特征，沿时间轴进行处理。通过将过去帧的特征循环传播到当前帧，循环架构设计使模型能够以微小的内存开销跟踪视频中的实例。除了
    RNNs，当 ViT 成为主流时，一些研究也以这种方式在 Transformers 中传播对象查询，因此它们被包含在本节中。以下是一些利用循环特征处理架构的研究。
- en: The temporal dimension of videos allows features to be processed in a recurrent
    manner according to the temporal flow of frames. One recurrent model to process
    spatiotemporal features in video is ConvLSTM [[148](#bib.bib148), [114](#bib.bib114)].
    It extends LSTM with convolutional structures to better capture spatiotemporal
    correlations. Specifically, Sun et al. [[114](#bib.bib114)] proposed a contextual
    pyramid ConvLSTMs to process multi-level spatiotemporal features extracted by
    the FPN, followed by a Mask R-CNN header [[124](#bib.bib124)] for predicting the
    instances in the next frame. The method has the benefit of being fast for real-time
    applications and for making fine-grained use of the features. There is another
    scheme, namely APANet, that improves ConvLSTM by adaptively aggregating spatiotemporal
    contextual information acquired at various scales to more accurately predict future
    frames [[115](#bib.bib115)]. The connections among each pair of ConvLSTM units
    are determined by neural architecture search (NAS). In a nutshell, these ConvLSTM-based
    schemes require significant memory due to many spatiotemporal features being cached,
    causing them to struggle to comprehend long videos.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 视频的时间维度允许特征根据帧的时间流动以循环的方式进行处理。处理视频中时空特征的一个循环模型是 ConvLSTM [[148](#bib.bib148),
    [114](#bib.bib114)]。它通过卷积结构扩展 LSTM，以更好地捕捉时空相关性。具体而言，Sun 等人 [[114](#bib.bib114)]
    提出了上下文金字塔 ConvLSTM，用于处理 FPN 提取的多层次时空特征，然后通过 Mask R-CNN 头 [[124](#bib.bib124)]
    预测下一帧中的实例。这种方法在实时应用中具有快速的优势，并且能够细粒度地利用特征。还有一种方案，即 APANet，通过自适应地聚合在不同尺度上获得的时空上下文信息，以更准确地预测未来帧 [[115](#bib.bib115)]。每对
    ConvLSTM 单元之间的连接由神经架构搜索（NAS）确定。总之，这些基于 ConvLSTM 的方案由于缓存了大量的时空特征，导致它们在处理长视频时存在显著的内存需求。
- en: In addition to ConvLSTM, several researchers also employed a GNN along with
    LSTM to propagate information for tracking [[116](#bib.bib116), [117](#bib.bib117)].
    In particular, a graph is built on the past and current detected instances. It
    is then utilized to produce output embeddings for association. The embeddings
    are then fed to LSTM for historical information aggregating and future tracking.
    It is obvious that GNN aids in building better associations of instances across
    frames. However, the approach heavily depends on the accuracy of instance detection.
    A false or missed detection in specific frames may have a huge impact on the scoring
    of instance connections, thus affecting the tracking and segmentation of instances
    in a video. A similar idea of adopting GNN for VIS was proposed in [[44](#bib.bib44)].
    Two consecutive frames, a reference frame and a target frame, are utilized to
    construct a graph and obtain aggregated spatiotemporal features. Without the help
    of ConvLSTM or LSTM, the method caches the history mask information in memory
    and achieves a similar effect of mask propagation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 ConvLSTM，几位研究人员还结合使用 GNN 和 LSTM 传播信息以进行跟踪[[116](#bib.bib116), [117](#bib.bib117)]。具体来说，基于过去和当前检测到的实例建立图形。然后，利用该图生成用于关联的输出嵌入。这些嵌入随后输入
    LSTM 进行历史信息聚合和未来跟踪。显然，GNN 有助于在帧间建立更好的实例关联。然而，这种方法严重依赖于实例检测的准确性。在特定帧中的错误或漏检可能对实例连接的评分产生巨大影响，从而影响视频中实例的跟踪和分割。类似地，在[[44](#bib.bib44)]
    中提出了采用 GNN 进行 VIS 的想法。使用两帧连续的参考帧和目标帧来构建图形并获得聚合的时空特征。在没有 ConvLSTM 或 LSTM 的帮助下，该方法将历史掩码信息缓存到内存中，并实现了类似的掩码传播效果。
- en: Recently, the self-attention mechanism is being increasingly utilized to construct
    query-based VIS schemes [[149](#bib.bib149)], where query proposals are typically
    propagated across frames for tracking instances [[104](#bib.bib104)]. Meinhardt
    et al. [[118](#bib.bib118)] proposed a query-based VIS scheme, i.e., TrackFormer,
    based on Deformable DETR [[144](#bib.bib144)]. It enables the transformer to detect
    and track objects in videos in a frame-by-frame manner. The tracking-by-attention
    paradigm and the concept of auto-regressive track queries are defined. Similarly,
    Koner et al. [[119](#bib.bib119)] proposed a Transformer-based online VIS framework,
    denoted InstanceFormer. It incorporates a memory queue to propagate the representation,
    location, and semantic information of prior instances to achieve better instance
    tracking consistency. Considering the aforementioned methods only handle inter-frame
    associations, Heo et al. [[120](#bib.bib120)] argued that the main bottleneck
    in processing long videos is building inter-clip associations. As a result, they
    proposed a clip-level query propagation approach, i.e., GenVIS, based on VITA [[111](#bib.bib111)].
    In particular, GenVIS stores clip-level decoded object queries in the memory.
    With the joint effort of decoded object queries propagated from the latest clip,
    GenVIS achieves state-of-the-art performance in long-distance instance tracking
    with a small computational overhead.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，自注意力机制在构建基于查询的视觉实例分割（VIS）方案中得到越来越多的应用[[149](#bib.bib149)]，其中查询提案通常在帧间传播以跟踪实例[[104](#bib.bib104)]。Meinhardt
    等人[[118](#bib.bib118)] 提出了一个基于查询的 VIS 方案，即 TrackFormer，基于可变形 DETR[[144](#bib.bib144)]。它使得变换器能够逐帧检测和跟踪视频中的物体。定义了基于注意力的跟踪范式和自回归跟踪查询的概念。类似地，Koner
    等人[[119](#bib.bib119)] 提出了一个基于变换器的在线 VIS 框架，称为 InstanceFormer。它结合了一个内存队列，以传播先前实例的表示、位置和语义信息，以实现更好的实例跟踪一致性。考虑到上述方法仅处理帧间关联，Heo
    等人[[120](#bib.bib120)] 认为处理长视频的主要瓶颈是构建剪辑间的关联。因此，他们提出了一种基于 VITA[[111](#bib.bib111)]
    的剪辑级查询传播方法，即 GenVIS。特别是，GenVIS 将剪辑级解码物体查询存储在内存中。通过最新剪辑传播的解码物体查询的共同努力，GenVIS 实现了在小计算开销下的长距离实例跟踪的最先进性能。
- en: To enhance temporal consistency in query propagation across frames, the work
    in [[121](#bib.bib121)] employed additional clip-level queries for fusing information
    from all the frames. The scheme combines the designs of recurrent and integrated
    instance queries, thus improving the temporal consistency and robustness of query
    propagation on VIS tasks. However, the design increases computational complexity
    and memory overhead, and sacrifices the ability of real-time inference. Apart
    from introducing additional clip-level queries, caching instance features from
    previous frames is also helpful in improving temporal consistency. The work in [[48](#bib.bib48)]
    introduced InsPro, which propagates query-proposal pairs from the previous frame
    to the current frame based on a set of instance queries. By caching instance features
    in historical frames and calculating intra-query attention, the method takes advantage
    of temporal clues in videos and copes well with occlusion and motion blur. In [[122](#bib.bib122)],
    the authors constructed contrastive items and added noise to the relevant embeddings
    in the memory bank during training to simulate identity switching in real-world
    scenarios, in order to better associate instances across time. Another way to
    improve the temporal consistency and robustness of query propagation is to directly
    rectify the effect of noisy features accumulated during occlusion and abrupt changes.
    Hannan et al. [[123](#bib.bib123)] proposed Gated Residual Attention for VIS (GRAtt-VIS),
    which uses gate activation as a mask for self-attention. The mask restricts the
    unrepresentative instance queries in the self-attention and keeps crucial information
    for long-term tracking. Compared with [[121](#bib.bib121)], the approach reduces
    computational complexity, alleviates memory overhead, and supports online processing.
    However, the shortcoming appears in tracking identities pertaining to crossover
    trajectories.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强跨帧查询传播的时间一致性，文献[[121](#bib.bib121)]采用了额外的剪辑级查询来融合所有帧的信息。该方案结合了递归和集成实例查询的设计，从而改善了VIS任务中查询传播的时间一致性和鲁棒性。然而，该设计增加了计算复杂度和内存开销，并牺牲了实时推断能力。除了引入额外的剪辑级查询，缓存来自前一帧的实例特征也有助于提高时间一致性。文献[[48](#bib.bib48)]引入了InsPro，它根据一组实例查询将查询提议对从前一帧传播到当前帧。通过缓存历史帧中的实例特征并计算查询内注意力，该方法利用了视频中的时间线索，并且能很好地应对遮挡和运动模糊。在[[122](#bib.bib122)]中，作者构建了对比项目，并在训练过程中向记忆库中的相关嵌入添加噪声，以模拟现实场景中的身份切换，从而更好地关联跨时间的实例。另一种改善查询传播时间一致性和鲁棒性的方法是直接修正遮挡和突发变化期间累积的噪声特征的影响。Hannan等人[[123](#bib.bib123)]提出了用于VIS的门控残差注意力（GRAtt-VIS），它使用门控激活作为自注意力的掩膜。掩膜限制了自注意力中不具代表性的实例查询，并保留了长期跟踪所需的重要信息。与[[121](#bib.bib121)]相比，该方法降低了计算复杂度，减轻了内存开销，并支持在线处理。然而，短期缺陷出现在跟踪涉及交叉轨迹的身份时。
- en: 4 Auxiliary Techniques for Enhancing Video Instance Segmentation
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增强视频实例分割的4种辅助技术
- en: In addition to the aforementioned architecture designs, there are several auxiliary
    techniques that can improve the performance of VIS, such as new datasets and representation
    learning techniques.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述架构设计，还有一些辅助技术可以提高VIS的性能，例如新的数据集和表示学习技术。
- en: 'Datasets: Despite the fact that there are numerous datasets for instance segmentation,
    object detection, and semantic segmentation, most of them are prepared at the
    image level and only a few are specifically made for VIS. Table [4](#S4.T4 "Table
    4 ‣ 4 Auxiliary Techniques for Enhancing Video Instance Segmentation ‣ Deep Learning
    Techniques for Video Instance Segmentation: A Survey") summarizes the primary
    datasets for VIS that feature videos with multiple categories and with distinct
    instances annotated. In particular, YouTube-VIS [[3](#bib.bib3)] is the first
    large-scale and the most extensively adopted dataset for VIS, which is now updated
    to its 2022 edition. In [[150](#bib.bib150)], the authors refined the masks in
    YouTube-VIS to High-Quality YTVIS (HQ-YTVIS). NuImages [[151](#bib.bib151)] is
    distinguished by its attribute annotations, such as whether a motorcycle has a
    rider, the pose of a pedestrian, and the activity of a vehicle. OVIS [[152](#bib.bib152)]
    is a large-scale VIS dataset with a high percentage of occluded instances, which
    poses great challenges for VIS models.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集：尽管针对实例分割、目标检测和语义分割有许多数据集，但大多数数据集都是在图像级别上准备的，只有少数是专门为视频实例分割（VIS）制作的。表 [4](#S4.T4
    "Table 4 ‣ 4 Auxiliary Techniques for Enhancing Video Instance Segmentation ‣
    Deep Learning Techniques for Video Instance Segmentation: A Survey") 总结了主要的视频实例分割数据集，这些数据集包括具有多个类别和不同实例标注的视频。特别是，YouTube-VIS [[3](#bib.bib3)]
    是第一个大规模且被广泛采用的VIS数据集，目前已更新至2022年版。在 [[150](#bib.bib150)] 中，作者将YouTube-VIS中的掩膜精炼为高质量的YTVIS（HQ-YTVIS）。NuImages [[151](#bib.bib151)]
    以其属性标注而著称，例如摩托车是否有骑手、行人的姿势和车辆的活动。OVIS [[152](#bib.bib152)] 是一个大规模的VIS数据集，具有很高的遮挡实例比例，这对VIS模型提出了很大挑战。'
- en: 'Table 4: Datasets for Video Instance Segmentation'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：视频实例分割数据集
- en: '| Dataset | Year¹ | #Video | #Class | #Mask | Scenario | Highlight |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份¹ | 视频数量 | 类别数量 | 掩膜数量 | 场景 | 亮点 |'
- en: '| KITTI MOTS [[37](#bib.bib37)] | 2019 | 21 | 2 | 38k | Driving |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| KITTI MOTS [[37](#bib.bib37)] | 2019 | 21 | 2 | 38k | 驾驶 |  |'
- en: '| SESIV [[69](#bib.bib69)] | 2019 | 84 | 29 | 12k | General |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| SESIV [[69](#bib.bib69)] | 2019 | 84 | 29 | 12k | 一般 |  |'
- en: '| BDD100K MOTS [[153](#bib.bib153)] | 2020 | 90 | 10 | 129k | Driving |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| BDD100K MOTS [[153](#bib.bib153)] | 2020 | 90 | 10 | 129k | 驾驶 |  |'
- en: '| NuImages [[151](#bib.bib151)] | 2020 | 1,000 | 23 | 800k | Driving | Attribute
    annotations |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| NuImages [[151](#bib.bib151)] | 2020 | 1,000 | 23 | 800k | 驾驶 | 属性标注 |'
- en: '| UVO [[154](#bib.bib154)] | 2021 | 11,361 | - | 1,676k | General | Open-world
    mask |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| UVO [[154](#bib.bib154)] | 2021 | 11,361 | - | 1,676k | 一般 | 开放世界掩膜 |'
- en: '| YouTube-VIS [[3](#bib.bib3)] | 2022 | 4,019 | 40 | 266k | General |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| YouTube-VIS [[3](#bib.bib3)] | 2022 | 4,019 | 40 | 266k | 一般 |  |'
- en: '| OVIS [[152](#bib.bib152)] | 2022 | 901 | 25 | 296k | General | Heavy occlusion
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| OVIS [[152](#bib.bib152)] | 2022 | 901 | 25 | 296k | 一般 | 严重遮挡 |'
- en: '| VIPSeg [[155](#bib.bib155)] | 2022 | 3,536 | 124 | 926k | General | VPS |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| VIPSeg [[155](#bib.bib155)] | 2022 | 3,536 | 124 | 926k | 一般 | VPS |'
- en: '| HQ-YTVIS [[150](#bib.bib150)] | 2022 | 2,238 | 40 | 131k | General | Fine-grained
    mask |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| HQ-YTVIS [[150](#bib.bib150)] | 2022 | 2,238 | 40 | 131k | 一般 | 精细掩膜 |'
- en: '| BURST [[156](#bib.bib156)] | 2023 | 2,914 | 482 | 600k | General |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| BURST [[156](#bib.bib156)] | 2023 | 2,914 | 482 | 600k | 一般 |  |'
- en: ¹ The release year of the latest version.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 最新版本的发布日期。
- en: 'Representation Learning: In VIS, representation learning is a technique that
    helps VIS schemes to better extract features, capture motion patterns, reduce
    data requirements, and improve robustness and generalization. Several related
    works in this area are summarized as follows.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表示学习：在视频实例分割中，表示学习是一种技术，帮助VIS方案更好地提取特征、捕捉运动模式、减少数据需求，并提高鲁棒性和泛化能力。以下总结了该领域的一些相关工作。
- en: As the FPN has been increasingly adopted in various VIS schemes, the authors
    of [[157](#bib.bib157)] proposed a Temporal Pyramid Routing (TPR) strategy that
    learns temporal and multi-scale representations altogether. Specifically, TPR
    accepts two feature pyramids from two adjacent frames as inputs. A Dynamic Aligned
    Cell Routing strategy is designed for aligning and gating the pyramid features
    across the temporal dimension. A Cross Pyramid Routing strategy is also proposed
    for transferring temporally aggregated features across the scale dimension. By
    incorporating the features from multiple frames, these representation-learning
    techniques improve clip-level instance understanding. However, they also impose
    additional memory overhead and computational complexity.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 随着FPN在各种VIS方案中的应用越来越广泛，[[157](#bib.bib157)]的作者提出了一种时间金字塔路由（TPR）策略，该策略同时学习时间和多尺度表示。具体而言，TPR接收来自两个相邻帧的两个特征金字塔作为输入。设计了一种动态对齐单元路由策略，用于在时间维度上对齐和门控金字塔特征。还提出了一种交叉金字塔路由策略，用于在尺度维度上转移时间聚合特征。通过结合来自多个帧的特征，这些表示学习技术改善了剪辑级别的实例理解。然而，它们也带来了额外的内存开销和计算复杂性。
- en: To learn high-quality embedded features, the connection between the instance
    segmenter and the tracker has been investigated. In particular, to increase randomness
    in training and encourage the tracker to learn more discriminative features, a
    sparse training and dense testing strategy was developed in [[158](#bib.bib158)].
    The number of points sampled for training is fewer than that for testing. Additionally,
    a time-series sampling strategy that samples at random intervals ensures effective
    learning of temporal information. The approach not only facilitates the learning
    of more generalized and robust representations, but also reduces memory consumption
    during the training.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习高质量的嵌入特征，研究了实例分割器与跟踪器之间的连接。特别是，为了在训练中增加随机性，并鼓励跟踪器学习更多的判别特征，[[158](#bib.bib158)]中提出了一种稀疏训练和密集测试策略。用于训练的采样点数少于用于测试的点数。此外，一种时间序列采样策略以随机间隔进行采样，确保有效学习时间信息。这种方法不仅有助于学习更多的泛化和鲁棒表示，还减少了训练过程中的内存消耗。
- en: To fully exploit the pixel-wise annotations and increase the number of instances
    during the training, a data augmentation strategy, named continuous copy-paste
    (CCP), was proposed for VIS [[159](#bib.bib159)]. In particular, CCP retrieves
    several instance blocks from near frames and past them onto their original positions
    while mimicking their emerging and leaving by shifting two of them to the boundary.
    By preserving the relative offset of crops and the original positions of instances
    without modeling the surrounding visual context, CCP produces high-quality triplets
    for tracking. On the other hand, Yoon and Choi [[160](#bib.bib160)] believed that
    models trained from representative frames with less redundancy could achieve comparable
    performance to that trained from dense datasets, thus reducing the cost of data
    acquisition and annotation. Specifically, an adaptive frame sampling (AFS) scheme
    is devised for extracting keyframes based on the visual or semantic dissimilarity
    between consecutive frames. With a simple copy-paste data augmentation on the
    keyframes, the performance gap caused by frame reduction is bridged.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用像素级标注并增加训练过程中实例的数量，提出了一种数据增强策略，称为连续复制粘贴（CCP），用于VIS[[159](#bib.bib159)]。特别是，CCP从邻近帧中提取几个实例块，并将它们粘贴到原始位置，同时通过将两个块移动到边界来模拟它们的出现和消失。通过保留裁剪的相对偏移量和实例的原始位置，而不建模周围的视觉上下文，CCP生成高质量的跟踪三元组。另一方面，Yoon和Choi[[160](#bib.bib160)]认为，从具有较少冗余的代表性帧训练的模型能够实现与从密集数据集训练的模型相当的性能，从而降低数据获取和标注的成本。具体而言，设计了一种自适应帧采样（AFS）方案，用于根据连续帧之间的视觉或语义差异提取关键帧。通过对关键帧进行简单的复制粘贴数据增强，弥合了由于帧减少而导致的性能差距。
- en: 5 Challenges and Future Research Directions
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个挑战和未来研究方向
- en: Although substantial progress has been made in VIS in recent years, there still
    remain numerous challenges. This section uncovers these challenges and proposes
    directions for future research and innovations in VIS.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近年来在视觉目标跟踪（VIS）领域取得了显著进展，但仍然存在许多挑战。本节揭示了这些挑战，并提出了VIS领域未来研究和创新的方向。
- en: 'Occluded Video Instance Segmentation: It is a challenge to segment highly-occluded
    instances in videos [[161](#bib.bib161)]. The advent of the OVIS [[152](#bib.bib152)]
    dataset paves the way for further study in this area. In particular, the authors
    of OVIS defined a metric named Bounding-box Occlusion Rate (BOR) to reflect the
    degree of occlusion between objects, showing that OVIS has three times higher
    occlusions than the popular YouTube-VIS dataset. Based on OVIS, Ke et al. [[162](#bib.bib162),
    [163](#bib.bib163)] addressed occlusion by treating each frame as a composition
    of two overlapping layers. In particular, a bilayer convolutional network is devised,
    which feeds the RoI features [[131](#bib.bib131)] into two branches for segmenting
    occluding objects (occluders) and partially occluded instances (occludees), respectively.
    In contrast to other amodal methods, which regress single occluded object boundary
    directly on the single-layered image, this approach takes into account interactions
    between the occluder and occludee. Nonetheless, there is still room for performance
    improvement by further utilizing contextual information propagated from adjacent
    frames.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 遮挡视频实例分割：在视频中分割高度遮挡的实例是一项挑战[[161](#bib.bib161)]。OVIS [[152](#bib.bib152)] 数据集的出现为该领域的进一步研究铺平了道路。特别地，OVIS
    的作者定义了一种名为边界框遮挡率（BOR）的度量来反映对象之间的遮挡程度，显示出 OVIS 的遮挡率是流行的 YouTube-VIS 数据集的三倍。基于 OVIS，Ke
    等人[[162](#bib.bib162), [163](#bib.bib163)] 通过将每一帧视为两个重叠层的组合来解决遮挡问题。具体而言，设计了一种双层卷积网络，该网络将
    RoI 特征[[131](#bib.bib131)] 输入到两个分支中，分别用于分割遮挡对象（遮挡者）和部分遮挡实例（被遮挡者）。与其他模态方法直接在单层图像上回归单个遮挡对象边界不同，这种方法考虑了遮挡者和被遮挡者之间的相互作用。然而，通过进一步利用从相邻帧传播的上下文信息，仍有提升性能的空间。
- en: 'Motion-Blurred Video Instance Segmentation: Motion blur refers to the appearance
    of objects in a frame as being smeared or distorted due to a moving subject or
    camera, which usually occurs in sports videos and can adversely affect the performance
    of VIS [[164](#bib.bib164), [80](#bib.bib80)]. Since no datasets have been created
    specifically for this challenge, data augmentation can be exploited to synthesize
    the appearance of motion blur, and a metric to assess the degree of motion blur
    is also required. To precisely segment motion-blurred instances in videos, several
    directions of research are necessary, such as deblurring, motion estimation, blur-invariant
    feature extraction, and multimodal feature fusion. In [[80](#bib.bib80)], the
    authors fused temporal features from two adjacent frames to estimate the motion
    directions for better tracking instances in motion-blurred videos. While the method
    is useful, a systematic assessment and analysis of the VIS performance in terms
    of motion blur is necessary.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 运动模糊视频实例分割：运动模糊指的是由于运动的主体或相机导致帧中对象的外观模糊或失真，这通常发生在体育视频中，并可能对 VIS 的性能产生不利影响[[164](#bib.bib164),
    [80](#bib.bib80)]。由于尚未专门为此挑战创建数据集，因此可以利用数据增强来合成运动模糊的外观，同时也需要一种度量来评估运动模糊的程度。为了准确分割运动模糊视频中的实例，需要几个研究方向，如去模糊、运动估计、模糊不变特征提取和多模态特征融合。在[[80](#bib.bib80)]中，作者融合了来自两个相邻帧的时间特征，以估计运动方向，从而更好地跟踪运动模糊视频中的实例。尽管该方法很有用，但仍需对
    VIS 在运动模糊方面的性能进行系统的评估和分析。
- en: 'Annotation-Efficient Video Instance Segmentation: Given the high annotation
    cost for videos, it is encouraging to develop annotation-efficient VIS schemes,
    such as self-supervised [[165](#bib.bib165)], weakly-supervised, or unsupervised
    VIS schemes [[166](#bib.bib166)]. Caron et al. [[167](#bib.bib167)] demonstrated
    that self-supervised ViT features contain explicit information pertaining to the
    semantic segmentation of an image. Without using any labels, their proposed knowledge
    distillation approach, denoted as DINO [[167](#bib.bib167)], automatically learns
    class-specific features in images by predicting the output of a teacher network
    using a cross-entropy loss. Based on DINO [[167](#bib.bib167)], the work in [[168](#bib.bib168)]
    proposed an unsupervised image segmentation scheme, i.e., CutLER, and applied
    it to VIS. CutLER outperforms other unsupervised VIS schemes significantly. However,
    there are still gaps between annotation-efficient VIS schemes and fully supervised
    VIS schemes in terms of performance, prompting researchers to further exploit
    the available information in videos and make better use of weak annotations.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注释高效的视频实例分割：鉴于视频的高注释成本，开发注释高效的VIS方案（如自监督[[165](#bib.bib165)]、弱监督或无监督VIS方案[[166](#bib.bib166)]）是令人鼓舞的。Caron等人[[167](#bib.bib167)]证明了自监督ViT特征包含与图像语义分割相关的显式信息。通过预测教师网络输出并使用交叉熵损失，他们提出的知识蒸馏方法，称为DINO[[167](#bib.bib167)]，无需任何标签即可自动学习图像中的类别特征。基于DINO[[167](#bib.bib167)]，在[[168](#bib.bib168)]中提出了一种无监督图像分割方案，即CutLER，并将其应用于VIS。CutLER显著优于其他无监督VIS方案。然而，注释高效的VIS方案与完全监督的VIS方案在性能上仍存在差距，促使研究人员进一步挖掘视频中的可用信息并更好地利用弱注释。
- en: 'Video Panoptic Segmentation: In 2020, Kim et al. [[169](#bib.bib169)] introduced
    the term “Video Panoptic Segmentation” (VPS) as image panoptic segmentation began
    to gain popularity. In addition to the requirements in VIS, VPS demands models
    to segment every pixel in frames, including background elements. Although several
    VPS schemes have been proposed [[169](#bib.bib169), [170](#bib.bib170), [155](#bib.bib155),
    [171](#bib.bib171)], there is still room for improvement in prediction accuracy,
    segmentation refinement, training and inference efficiency, dataset diversity,
    and annotation efficiency. Particularly, in 2023, Athar et al. proposed a unified
    scheme for multiple video segmentation tasks, including VOS, VIS, and VPS [[172](#bib.bib172)].
    By modeling the targets of various tasks as different abstract queries of a Transformer,
    the method offers a viable path for a unified video segmentation solution and
    narrows the gap between VPS and VIS.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 视频全景分割：2020年，Kim等人[[169](#bib.bib169)]引入了“视频全景分割”（VPS）这一术语，因为图像全景分割开始受到关注。除了VIS的要求，VPS还要求模型对每一帧中的每一个像素进行分割，包括背景元素。尽管已经提出了几种VPS方案[[169](#bib.bib169),
    [170](#bib.bib170), [155](#bib.bib155), [171](#bib.bib171)]，但在预测准确性、分割精度、训练和推断效率、数据集多样性以及注释效率方面仍有改进空间。特别是，2023年，Athar等人提出了一种统一的多视频分割任务方案，包括VOS、VIS和VPS[[172](#bib.bib172)]。通过将各种任务的目标建模为Transformer的不同抽象查询，该方法提供了一个统一的视频分割解决方案的可行路径，并缩小了VPS和VIS之间的差距。
- en: 'Open-Vocabulary Video Instance Segmentation: Open-vocabulary VIS is a novel
    video segmentation task that requires the model to detect, segment, and track
    instances from open-set vocabulary categories, including novel categories unseen
    during training [[173](#bib.bib173), [174](#bib.bib174)]. Open-vocabulary VIS
    is highly valuable in real-world applications, especially when the object vocabulary
    is not fixed, such as surveillance and autonomous driving. In [[174](#bib.bib174)],
    Wang et al. proposed a Large-Vocabulary Video Instance Segmentation (LV-VIS) dataset
    along with a benchmark approach. The work paves the way for further research in
    this direction. Despite the fact that several early Transformer-based schemes
    have been proposed [[173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175)],
    the performance of Open-Vocabulary VIS lags behind that of classical VIS, owing
    to challenges in object diversity, data annotation, and semantic understanding.
    Several research directions, including zero-shot learning, adaptive learning,
    and multimodal learning, have great potential for developing more general Open-Vocabulary
    VIS models.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 开放词汇视频实例分割：开放词汇 VIS 是一种新颖的视频分割任务，要求模型从开放集词汇类别中检测、分割和跟踪实例，包括在训练期间未见过的新类别[[173](#bib.bib173),
    [174](#bib.bib174)]。开放词汇 VIS 在实际应用中具有重要价值，尤其是在对象词汇不固定的情况下，如监控和自动驾驶。在[[174](#bib.bib174)]中，Wang
    等人提出了一个大词汇量视频实例分割（LV-VIS）数据集及其基准方法。这项工作为进一步的研究铺平了道路。尽管已有几种早期基于 Transformer 的方案被提出[[173](#bib.bib173),
    [174](#bib.bib174), [175](#bib.bib175)]，但开放词汇 VIS 的性能仍落后于经典 VIS，原因在于对象多样性、数据标注和语义理解方面的挑战。包括零样本学习、自适应学习和多模态学习在内的几个研究方向，具有开发更通用开放词汇
    VIS 模型的巨大潜力。
- en: 'Multimodal Video Instance Segmentation: Multimodal VIS requires models to fuse
    features from various modalities and utilize their complementary properties [[176](#bib.bib176)].
    As the Transformer is effective in modeling global and long-range dependencies
    across different tokens [[177](#bib.bib177)], some researchers have utilized the
    Transformer to build multimodal VIS schemes. Botach et al. [[178](#bib.bib178)]
    and Chen et al. [[179](#bib.bib179)] investigated the fusion of video and language
    features, while Li et al. [[180](#bib.bib180)] focused on the fusion of video
    and audio features. Nevertheless, multimodal VIS still faces multiple challenges,
    such as multimodal data fusion and alignment, diverse data representation handling,
    and cross-modal data annotation collection. Incorporating generative models, like
    Make-A-Video [[181](#bib.bib181)], which generates temporally coherent video clips
    from text, has the potential to mitigate the data-hungry issue of multimodal VIS.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态视频实例分割：多模态 VIS 需要模型融合来自各种模态的特征，并利用其互补属性[[176](#bib.bib176)]。由于 Transformer
    在建模不同标记之间的全局和长程依赖方面表现有效[[177](#bib.bib177)]，一些研究人员利用 Transformer 构建了多模态 VIS 方案。Botach
    等人[[178](#bib.bib178)] 和 Chen 等人[[179](#bib.bib179)] 研究了视频和语言特征的融合，而 Li 等人[[180](#bib.bib180)]
    则集中于视频和音频特征的融合。然而，多模态 VIS 仍面临诸多挑战，如多模态数据融合和对齐、多样化数据表示处理以及跨模态数据标注收集。结合生成模型，如 Make-A-Video[[181](#bib.bib181)]，它可以从文本生成时间上连贯的视频片段，有可能缓解多模态
    VIS 对数据的高需求问题。
- en: 'Promptable Video Segmentation: In 2023, Kirillov et al. [[182](#bib.bib182)]
    proposed a promptable segmentation task for images, which requires a model to
    accept flexible prompting (points, boxes, text, and masks) and return a valid
    segmentation mask in real time. With an innovative data engine for promptable
    segmentation, an incredibly huge and diverse set of masks has been created to
    train a Segment Anything Model (SAM). SAM enables zero-shot generalization, addressing
    novel visual concepts while resolving a variety of downstream segmentation issues.
    With the great success of promptable segmentation in image, promptable video segmentation
    holds promise for providing uniform solutions to various video segmentation tasks.
    However, compared with promptable segmentation in images, it is challenging to
    design video prompts. This is because it is difficult for mouse-driven points
    to follow an instance in a video consistently, which can easily lead to ambiguity.
    Besides, promptable video segmentation necessitates additional real-time tracking,
    prediction, and re-identification for instances across frames, posing challenges
    to real-time video understanding.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 可提示视频分割：2023年，Kirillov 等人 [[182](#bib.bib182)] 提出了一个针对图像的可提示分割任务，该任务要求模型接受灵活的提示（点、框、文本和掩模），并实时返回有效的分割掩模。借助创新的数据引擎，用于可提示分割创建了一个极其庞大且多样化的掩模集合，用以训练“分割任何东西”模型（SAM）。SAM
    实现了零样本泛化，能够处理新的视觉概念，同时解决各种下游分割问题。由于图像中可提示分割的巨大成功，可提示视频分割有望为各种视频分割任务提供统一的解决方案。然而，与图像中的可提示分割相比，设计视频提示具有挑战性。这是因为基于鼠标的点很难在视频中持续跟踪实例，容易导致歧义。此外，可提示视频分割需要额外的实时跟踪、预测和实例跨帧重新识别，给实时视频理解带来了挑战。
- en: 6 Conclusion
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: VIS is a fundamental computer vision task with extensive applications in numerous
    domains. VIS has made significant progress over the years, in line with the rapid
    development of deep-learning techniques and rising computing power around the
    world. To help researchers better understand the methodologies in this emerging
    field, this survey systematically reviews, analyzes, and compares existing deep-learning
    schemes from the perspective of architecture. Specifically, the reviewed schemes
    are divided into multi-stage, multi-branch, hybrid, integrated, and recurrent
    varieties according to their feature processing modes. Several auxiliary techniques
    for improving VIS performance, including specialized datasets and representation
    learning approaches, are scrutinized and discussed, providing readers with comprehensive
    research views on VIS. This survey also reveals several promising research directions
    by examining the key challenges faced by VIS, offering researchers with valuable
    insights into the advancement of video segmentation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 视频实例分割（VIS）是计算机视觉中的一个基本任务，广泛应用于众多领域。随着深度学习技术的快速发展和全球计算能力的提升，VIS 多年来取得了显著进展。为帮助研究人员更好地理解这一新兴领域的方法，本调查从架构的角度系统地回顾、分析和比较了现有的深度学习方案。具体而言，所评审的方案根据其特征处理模式被分为多阶段、多分支、混合、集成和递归等类型。文中还详细审视和讨论了包括专用数据集和表示学习方法在内的若干辅助技术，这些技术有助于提高VIS的性能，为读者提供了全面的VIS研究视角。本调查还通过考察VIS面临的关键挑战，揭示了几个有前景的研究方向，为研究人员提供了有关视频分割进展的宝贵见解。
- en: References
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. Wilson, C.-T. Li, A class of discrete multiresolution random fields
    and its application to image segmentation, IEEE Transactions on Pattern Analysis
    and Machine Intelligence 25 (1) (2003) 42–56.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. Wilson, C.-T. Li, 一类离散多分辨率随机场及其在图像分割中的应用，IEEE 模式分析与机器智能交易 25 (1) (2003)
    42–56。'
- en: '[2] A. Khadidos, V. Sanchez, C.-T. Li, Weighted level set evolution based on
    local edge features for medical image segmentation, IEEE Transactions on Image
    Processing 26 (4) (2017) 1979–1991.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Khadidos, V. Sanchez, C.-T. Li, 基于局部边缘特征的加权水平集演化用于医学图像分割，IEEE 图像处理交易
    26 (4) (2017) 1979–1991。'
- en: '[3] L. Yang, Y. Fan, N. Xu, Video instance segmentation, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, 2019, pp. 5188–5197.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] L. Yang, Y. Fan, N. Xu, 视频实例分割，载于：IEEE/CVF 国际计算机视觉大会论文集，2019，第5188–5197页。'
- en: '[4] A. M. Algamdi, V. Sanchez, C.-T. Li, Learning temporal information from
    spatial information using capsnets for human action recognition, in: ICASSP 2019-2019
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    IEEE, 2019, pp. 3867–3871.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] A. M. Algamdi, V. Sanchez, C.-T. Li, 使用CapsNets从空间信息中学习时间信息以进行人类动作识别，ICASSP
    2019-2019 IEEE国际声学、语音与信号处理会议（ICASSP），IEEE，2019，第3867–3871页。'
- en: '[5] S. Lin, C.-T. Li, A. C. Kot, Multi-domain adversarial feature generalization
    for person re-identification, IEEE Transactions on Image Processing 30 (2020)
    1596–1607.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Lin, C.-T. Li, A. C. Kot, 多领域对抗特征泛化用于行人再识别，IEEE图像处理汇刊 30 (2020) 1596–1607。'
- en: '[6] X. Lin, C.-T. Li, V. Sanchez, C. Maple, On the detection-to-track association
    for online multi-object tracking, Pattern Recognition Letters 146 (2021) 200–207.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] X. Lin, C.-T. Li, V. Sanchez, C. Maple, 在线多目标跟踪的检测与跟踪关联，模式识别快报 146 (2021)
    200–207。'
- en: '[7] E. Talpes, D. D. Sarma, D. Williams, S. Arora, T. Kunjan, B. Floering,
    A. Jalote, C. Hsiong, C. Poorna, V. Samant, et al., The microarchitecture of dojo,
    tesla’s exa-scale computer, IEEE Micro (2023).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] E. Talpes, D. D. Sarma, D. Williams, S. Arora, T. Kunjan, B. Floering,
    A. Jalote, C. Hsiong, C. Poorna, V. Samant 等，Dojo的微架构，特斯拉的超大规模计算机，IEEE Micro (2023)。'
- en: '[8] S. Alfasly, B. Liu, Y. Hu, Y. Wang, C.-T. Li, Auto-zooming cnn-based framework
    for real-time pedestrian detection in outdoor surveillance videos, IEEE access
    7 (2019) 105816–105826.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. Alfasly, B. Liu, Y. Hu, Y. Wang, C.-T. Li, 基于自动缩放的CNN框架用于实时行人检测，IEEE
    Access 7 (2019) 105816–105826。'
- en: '[9] B. Zhang, J. Zhang, A traffic surveillance system for obtaining comprehensive
    information of the passing vehicles based on instance segmentation, IEEE Transactions
    on Intelligent Transportation Systems 22 (11) (2020) 7040–7055.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] B. Zhang, J. Zhang, 基于实例分割的交通监控系统以获取经过车辆的全面信息，IEEE智能交通系统汇刊 22 (11) (2020)
    7040–7055。'
- en: '[10] T. Y. Tan, L. Zhang, C. P. Lim, B. Fielding, Y. Yu, E. Anderson, Evolving
    ensemble models for image segmentation using enhanced particle swarm optimization,
    IEEE access 7 (2019) 34004–34019.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] T. Y. Tan, L. Zhang, C. P. Lim, B. Fielding, Y. Yu, E. Anderson, 使用增强粒子群优化的图像分割进化集成模型，IEEE
    Access 7 (2019) 34004–34019。'
- en: '[11] A. Arbelle, S. Cohen, T. R. Raviv, Dual-task convlstm-unet for instance
    segmentation of weakly annotated microscopy videos, IEEE Transactions on Medical
    Imaging 41 (8) (2022) 1948–1960.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Arbelle, S. Cohen, T. R. Raviv, 用于弱注释显微镜视频实例分割的双任务ConvLSTM-UNet，IEEE医学成像汇刊
    41 (8) (2022) 1948–1960。'
- en: '[12] H. Gan, M. Ou, C. Li, X. Wang, J. Guo, A. Mao, M. C. Ceballos, T. D. Parsons,
    K. Liu, Y. Xue, Automated detection and analysis of piglet suckling behaviour
    using high-accuracy amodal instance segmentation, Computers and Electronics in
    Agriculture 199 (2022) 107162.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] H. Gan, M. Ou, C. Li, X. Wang, J. Guo, A. Mao, M. C. Ceballos, T. D. Parsons,
    K. Liu, Y. Xue, 使用高精度模态实例分割进行小猪吮吸行为的自动检测与分析，计算机与电子农业 199 (2022) 107162。'
- en: '[13] B. Xiao, H. Xiao, J. Wang, Y. Chen, Vision-based method for tracking workers
    by integrating deep learning instance segmentation in off-site construction, Automation
    in Construction 136 (2022) 104148.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] B. Xiao, H. Xiao, J. Wang, Y. Chen, 通过在场外施工中集成深度学习实例分割的视觉方法跟踪工人，施工自动化
    136 (2022) 104148。'
- en: '[14] Y. Ghasemi, H. Jeong, S. H. Choi, K.-B. Park, J. Y. Lee, Deep learning-based
    object detection in augmented reality: A systematic review, Computers in Industry
    139 (2022) 103661.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Ghasemi, H. Jeong, S. H. Choi, K.-B. Park, J. Y. Lee, 基于深度学习的增强现实中的物体检测：系统性综述，工业计算机
    139 (2022) 103661。'
- en: '[15] S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, D. Terzopoulos,
    Image segmentation using deep learning: A survey, IEEE transactions on pattern
    analysis and machine intelligence 44 (7) (2021) 3523–3542.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, D. Terzopoulos,
    图像分割使用深度学习：一项综述，IEEE模式分析与机器智能汇刊 44 (7) (2021) 3523–3542。'
- en: '[16] C. Xu, J. Ge, Y. Li, Y. Deng, L. Gao, M. Zhang, Y. Xiang, X. Zheng, Scei:
    A smart-contract driven edge intelligence framework for iot systems, IEEE Transactions
    on Mobile Computing (2023).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] C. Xu, J. Ge, Y. Li, Y. Deng, L. Gao, M. Zhang, Y. Xiang, X. Zheng, SCEI：一种用于物联网系统的智能合约驱动边缘智能框架，IEEE移动计算汇刊
    (2023)。'
- en: '[17] H. Wang, V. Sanchez, C.-T. Li, Improving face-based age estimation with
    attention-based dynamic patch fusion, IEEE Transactions on Image Processing 31
    (2022) 1084–1096.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] H. Wang, V. Sanchez, C.-T. Li, 通过基于注意力的动态补丁融合改进基于面部的年龄估计，IEEE图像处理汇刊 31
    (2022) 1084–1096。'
- en: '[18] D. W. Otter, J. R. Medina, J. K. Kalita, A survey of the usages of deep
    learning for natural language processing, IEEE transactions on neural networks
    and learning systems 32 (2) (2020) 604–624.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] D. W. Otter, J. R. Medina, J. K. Kalita, 深度学习在自然语言处理中的应用调查，IEEE Transactions
    on Neural Networks and Learning Systems 32 (2) (2020) 604–624。'
- en: '[19] T. Zhou, F. Porikli, D. J. Crandall, L. Van Gool, W. Wang, A survey on
    deep learning technique for video segmentation, IEEE Transactions on Pattern Analysis
    and Machine Intelligence 45 (6) (2022) 7099–7122.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] T. Zhou, F. Porikli, D. J. Crandall, L. Van Gool, 关于视频分割的深度学习技术综述，IEEE
    Transactions on Pattern Analysis and Machine Intelligence 45 (6) (2022) 7099–7122。'
- en: '[20] W. Gu, S. Bai, L. Kong, A review on 2d instance segmentation based on
    deep neural networks, Image and Vision Computing 120 (2022) 104401.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] W. Gu, S. Bai, L. Kong, 基于深度神经网络的二维实例分割综述，Image and Vision Computing 120
    (2022) 104401。'
- en: '[21] X. Li, H. Ding, W. Zhang, H. Yuan, J. Pang, G. Cheng, K. Chen, Z. Liu,
    C. C. Loy, Transformer-based visual segmentation: A survey, arXiv preprint arXiv:2304.09854
    (2023).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] X. Li, H. Ding, W. Zhang, H. Yuan, J. Pang, G. Cheng, K. Chen, Z. Liu,
    C. C. Loy, 基于Transformer的视觉分割：综述，arXiv预印本 arXiv:2304.09854 (2023)。'
- en: '[22] W. Luo, J. Xing, A. Milan, X. Zhang, W. Liu, T.-K. Kim, Multiple object
    tracking: A literature review, Artificial intelligence 293 (2021) 103448.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] W. Luo, J. Xing, A. Milan, X. Zhang, W. Liu, T.-K. Kim, 多目标跟踪：文献综述，Artificial
    Intelligence 293 (2021) 103448。'
- en: '[23] L. Rakai, H. Song, S. Sun, W. Zhang, Y. Yang, Data association in multiple
    object tracking: A survey of recent techniques, Expert Systems with Applications
    192 (2022) 116300.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] L. Rakai, H. Song, S. Sun, W. Zhang, Y. Yang, 多目标跟踪中的数据关联：近期技术综述，Expert
    Systems with Applications 192 (2022) 116300。'
- en: '[24] S. Javed, M. Danelljan, F. S. Khan, M. H. Khan, M. Felsberg, J. Matas,
    Visual object tracking with discriminative filters and siamese networks: a survey
    and outlook, IEEE Transactions on Pattern Analysis and Machine Intelligence 45 (5)
    (2022) 6552–6574.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Javed, M. Danelljan, F. S. Khan, M. H. Khan, M. Felsberg, J. Matas,
    使用判别性滤波器和孪生网络的视觉目标跟踪：综述与展望，IEEE Transactions on Pattern Analysis and Machine Intelligence
    45 (5) (2022) 6552–6574。'
- en: '[25] G. Ciaparrone, F. L. Sánchez, S. Tabik, L. Troiano, R. Tagliaferri, F. Herrera,
    Deep learning in video multi-object tracking: A survey, Neurocomputing 381 (2020)
    61–88.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] G. Ciaparrone, F. L. Sánchez, S. Tabik, L. Troiano, R. Tagliaferri, F.
    Herrera, 深度学习在视频多目标跟踪中的应用：综述，Neurocomputing 381 (2020) 61–88。'
- en: '[26] R. Yao, G. Lin, S. Xia, J. Zhao, Y. Zhou, Video object segmentation and
    tracking: A survey, ACM Transactions on Intelligent Systems and Technology (TIST)
    11 (4) (2020) 1–47.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] R. Yao, G. Lin, S. Xia, J. Zhao, Y. Zhou, 视频对象分割与跟踪：综述，ACM Transactions
    on Intelligent Systems and Technology (TIST) 11 (4) (2020) 1–47。'
- en: '[27] L. Kalake, W. Wan, L. Hou, Analysis based on recent deep learning approaches
    applied in real-time multi-object tracking: a review, IEEE Access 9 (2021) 32650–32671.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] L. Kalake, W. Wan, L. Hou, 基于近期深度学习方法的实时多目标跟踪分析：综述，IEEE Access 9 (2021)
    32650–32671。'
- en: '[28] G. Wang, M. Song, J.-N. Hwang, Recent advances in embedding methods for
    multi-object tracking: a survey, arXiv preprint arXiv:2205.10766 (2022).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] G. Wang, M. Song, J.-N. Hwang, 多目标跟踪嵌入方法的最新进展：综述，arXiv预印本 arXiv:2205.10766
    (2022)。'
- en: '[29] M. Bashar, S. Islam, K. K. Hussain, M. B. Hasan, A. Rahman, M. H. Kabir,
    Multiple object tracking in recent times: a literature review, arXiv preprint
    arXiv:2209.04796 (2022).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] M. Bashar, S. Islam, K. K. Hussain, M. B. Hasan, A. Rahman, M. H. Kabir,
    近期多目标跟踪综述，arXiv预印本 arXiv:2209.04796 (2022)。'
- en: '[30] M. Gao, F. Zheng, J. J. Yu, C. Shan, G. Ding, J. Han, Deep learning for
    video object segmentation: a review, Artificial Intelligence Review 56 (1) (2023)
    457–531.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] M. Gao, F. Zheng, J. J. Yu, C. Shan, G. Ding, J. Han, 视频对象分割的深度学习：综述，Artificial
    Intelligence Review 56 (1) (2023) 457–531。'
- en: '[31] B. Hou, Y. Liu, N. Ling, Y. Ren, L. Liu, et al., A survey of efficient
    deep learning models for moving object segmentation, APSIPA Transactions on Signal
    and Information Processing 12 (1) (2023).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] B. Hou, Y. Liu, N. Ling, Y. Ren, L. Liu, 等，针对移动对象分割的高效深度学习模型综述，APSIPA
    Transactions on Signal and Information Processing 12 (1) (2023)。'
- en: '[32] R. Leyva, V. Sanchez, C.-T. Li, Video anomaly detection with compact feature
    sets for online performance, IEEE Transactions on Image Processing 26 (7) (2017)
    3463–3478.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] R. Leyva, V. Sanchez, C.-T. Li, 使用紧凑特征集进行视频异常检测以实现在线性能，IEEE Transactions
    on Image Processing 26 (7) (2017) 3463–3478。'
- en: '[33] R. Girshick, J. Donahue, T. Darrell, J. Malik, Rich feature hierarchies
    for accurate object detection and semantic segmentation, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, 2014, pp. 580–587.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] R. Girshick, J. Donahue, T. Darrell, J. Malik, 用于精确目标检测和语义分割的丰富特征层次，见：IEEE计算机视觉与模式识别会议论文集，2014年，页580–587。'
- en: '[34] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic
    segmentation, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, 2015, pp. 3431–3440.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. Long, E. Shelhamer, T. Darrell, 用于语义分割的全卷积网络，见：IEEE 计算机视觉与模式识别会议论文集，2015，第3431–3440页。'
- en: '[35] L. Zhang, S. Slade, C. P. Lim, H. Asadi, S. Nahavandi, H. Huang, H. Ruan,
    Semantic segmentation using firefly algorithm-based evolving ensemble deep neural
    networks, Knowledge-Based Systems 277 (2023) 110828.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] L. Zhang, S. Slade, C. P. Lim, H. Asadi, S. Nahavandi, H. Huang, H. Ruan,
    使用萤火虫算法的演变集成深度神经网络进行语义分割，知识库系统 277 (2023) 110828。'
- en: '[36] S. Slade, L. Zhang, H. Huang, H. Asadi, C. P. Lim, Y. Yu, D. Zhao, H. Lin,
    R. Gao, Neural inference search for multiloss segmentation models, IEEE Transactions
    on Neural Networks and Learning Systems (2023).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Slade, L. Zhang, H. Huang, H. Asadi, C. P. Lim, Y. Yu, D. Zhao, H.
    Lin, R. Gao, 神经推理搜索用于多损失分割模型，IEEE 神经网络与学习系统汇刊 (2023)。'
- en: '[37] P. Voigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar, A. Geiger,
    B. Leibe, Mots: Multi-object tracking and segmentation, in: Proceedings of the
    ieee/cvf conference on computer vision and pattern recognition, 2019, pp. 7942–7951.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] P. Voigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar, A. Geiger,
    B. Leibe, Mots：多目标跟踪和分割，见：IEEE/CVF 计算机视觉与模式识别会议论文集，2019，第7942–7951页。'
- en: '[38] J. Luiten, P. Torr, B. Leibe, Video instance segmentation 2019: A winning
    approach for combined detection, segmentation, classification and tracking., in:
    Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops,
    2019, pp. 0–0.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] J. Luiten, P. Torr, B. Leibe, 视频实例分割2019：一个结合检测、分割、分类和跟踪的获胜方法，见：IEEE/CVF
    国际计算机视觉会议工作坊论文集，2019，第0–0页。'
- en: '[39] C. Xu, Y. Qu, T. H. Luan, P. W. Eklund, Y. Xiang, L. Gao, An efficient
    and reliable asynchronous federated learning scheme for smart public transportation,
    IEEE Transactions on Vehicular Technology (2022).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C. Xu, Y. Qu, T. H. Luan, P. W. Eklund, Y. Xiang, L. Gao, 一种高效可靠的异步联邦学习方案用于智能公共交通，IEEE
    车辆技术汇刊 (2022)。'
- en: '[40] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai, et al., Recent advances in convolutional neural networks, Pattern
    recognition 77 (2018) 354–377.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai 等，卷积神经网络的最新进展，模式识别 77 (2018) 354–377。'
- en: '[41] N. Aafaq, A. Mian, W. Liu, S. Z. Gilani, M. Shah, Video description: A
    survey of methods, datasets, and evaluation metrics, ACM Computing Surveys (CSUR)
    52 (6) (2019) 1–37.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] N. Aafaq, A. Mian, W. Liu, S. Z. Gilani, M. Shah, 视频描述：方法、数据集和评价指标的调查，ACM
    计算机调查 (CSUR) 52 (6) (2019) 1–37。'
- en: '[42] G. Rafiq, M. Rafiq, G. S. Choi, Video description: A comprehensive survey
    of deep learning approaches, Artificial Intelligence Review (2023) 1–80.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] G. Rafiq, M. Rafiq, G. S. Choi, 视频描述：深度学习方法的全面调查，人工智能评论 (2023) 1–80。'
- en: '[43] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, S. Y. Philip, A comprehensive
    survey on graph neural networks, IEEE transactions on neural networks and learning
    systems 32 (1) (2020) 4–24.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, S. Y. Philip, 图神经网络的全面调查，IEEE
    神经网络与学习系统汇刊 32 (1) (2020) 4–24。'
- en: '[44] T. Wang, N. Xu, K. Chen, W. Lin, End-to-end video instance segmentation
    via spatial-temporal graph neural networks, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2021, pp. 10797–10806.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] T. Wang, N. Xu, K. Chen, W. Lin, 通过时空图神经网络进行端到端视频实例分割，见：IEEE/CVF 国际计算机视觉会议论文集，2021，第10797–10806页。'
- en: '[45] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16
    words: Transformers for image recognition at scale, in: International Conference
    on Learning Representations, 2020, pp. 1–21.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly 等，图像的价值是16x16个词：用于大规模图像识别的变换器，见：国际学习表示会议，2020，第1–21页。'
- en: '[46] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, S. Zagoruyko,
    End-to-end object detection with transformers, in: European conference on computer
    vision, Springer, 2020, pp. 213–229.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, S. Zagoruyko,
    基于变换器的端到端目标检测，见：欧洲计算机视觉会议，Springer，2020，第213–229页。'
- en: '[47] P. Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka, L. Li,
    Z. Yuan, C. Wang, et al., Sparse r-cnn: End-to-end object detection with learnable
    proposals, in: Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition, 2021, pp. 14454–14463.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] P. Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka, L. Li,
    Z. Yuan, C. Wang 等，《稀疏 r-cnn：具有可学习提议的端到端目标检测》，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2021年，第14454–14463页。'
- en: '[48] F. He, H. Zhang, N. Gao, J. Jia, Y. Shan, X. Zhao, K. Huang, Inspro: Propagating
    instance query and proposal for online video instance segmentation, Advances in
    Neural Information Processing Systems 35 (2022) 19370–19383.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] F. He, H. Zhang, N. Gao, J. Jia, Y. Shan, X. Zhao, K. Huang，《Inspro：用于在线视频实例分割的实例查询和提议传播》，发表于：神经信息处理系统进展35（2022年）第19370–19383页。'
- en: '[49] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, H. Xia, End-to-end
    video instance segmentation with transformers, in: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, 2021, pp. 8741–8750.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, H. Xia，《基于变压器的端到端视频实例分割》，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2021年，第8741–8750页。'
- en: '[50] M. Dong, J. Wang, Y. Huang, D. Yu, K. Su, K. Zhou, J. Shao, S. Wen, C. Wang,
    Temporal feature augmented network for video instance segmentation, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision Workshops, 2019, pp.
    0–0.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] M. Dong, J. Wang, Y. Huang, D. Yu, K. Su, K. Zhou, J. Shao, S. Wen, C.
    Wang，《用于视频实例分割的时间特征增强网络》，发表于：IEEE/CVF国际计算机视觉会议论文集，2019年，第0–0页。'
- en: '[51] L. Porzi, M. Hofinger, I. Ruiz, J. Serrat, S. R. Bulo, P. Kontschieder,
    Learning multi-object tracking and segmentation from automatic annotations, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2020, pp. 6846–6855.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] L. Porzi, M. Hofinger, I. Ruiz, J. Serrat, S. R. Bulo, P. Kontschieder，《从自动标注中学习多目标跟踪和分割》，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，第6846–6855页。'
- en: '[52] Q. Feng, Z. Yang, P. Li, Y. Wei, Y. Yang, Dual embedding learning for
    video instance segmentation, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision Workshops, 2019, pp. 0–0.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Q. Feng, Z. Yang, P. Li, Y. Wei, Y. Yang，《用于视频实例分割的双重嵌入学习》，发表于：IEEE/CVF国际计算机视觉会议论文集，2019年，第0–0页。'
- en: '[53] J. Luiten, I. E. Zulfikar, B. Leibe, Unovost: Unsupervised offline video
    object segmentation and tracking, in: Proceedings of the IEEE/CVF winter conference
    on applications of computer vision, 2020, pp. 2000–2009.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. Luiten, I. E. Zulfikar, B. Leibe，《Unovost：无监督离线视频目标分割和跟踪》，发表于：IEEE/CVF冬季计算机视觉应用会议论文集，2020年，第2000–2009页。'
- en: '[54] A. Choudhuri, G. Chowdhary, A. G. Schwing, Assignment-space-based multi-object
    tracking and segmentation, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2021, pp. 13598–13607.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Choudhuri, G. Chowdhary, A. G. Schwing，《基于分配空间的多目标跟踪和分割》，发表于：IEEE/CVF国际计算机视觉会议论文集，2021年，第13598–13607页。'
- en: '[55] M.-T. Tran, T.-N. Le, T. V. Nguyen, V. Ton-That, T.-H. Hoang, N.-M. Bui,
    T.-L. Do, Q.-A. Luong, V.-T. Nguyen, D. A. Duong, et al., Guided instance segmentation
    framework for semi-supervised video instance segmentation, in: CVPR Workshops,
    2019, pp. 1–4.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] M.-T. Tran, T.-N. Le, T. V. Nguyen, V. Ton-That, T.-H. Hoang, N.-M. Bui,
    T.-L. Do, Q.-A. Luong, V.-T. Nguyen, D. A. Duong 等，《用于半监督视频实例分割的引导实例分割框架》，发表于：CVPR研讨会，2019年，第1–4页。'
- en: '[56] M.-T. Tran, T. Hoang, T. V. Nguyen, T.-N. Le, E. Nguyen, M. Le, H. Nguyen-Dinh,
    X. Hoang, M. N. Do, Multi-referenced guided instance segmentation framework for
    semi-supervised video instance segmentation, in: CVPR Workshops, 2020, pp. 1–4.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] M.-T. Tran, T. Hoang, T. V. Nguyen, T.-N. Le, E. Nguyen, M. Le, H. Nguyen-Dinh,
    X. Hoang, M. N. Do，《用于半监督视频实例分割的多参考引导实例分割框架》，发表于：CVPR研讨会，2020年，第1–4页。'
- en: '[57] G. Bertasius, L. Torresani, Classifying, segmenting, and tracking object
    instances in video with mask propagation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2020, pp. 9739–9748.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] G. Bertasius, L. Torresani，《通过掩码传播进行视频中目标实例的分类、分割和跟踪》，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，第9739–9748页。'
- en: '[58] H. Lin, R. Wu, S. Liu, J. Lu, J. Jia, Video instance segmentation with
    a propose-reduce paradigm, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2021, pp. 1739–1748.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] H. Lin, R. Wu, S. Liu, J. Lu, J. Jia，《具有提议-减少范式的视频实例分割》，发表于：IEEE/CVF国际计算机视觉会议论文集，2021年，第1739–1748页。'
- en: '[59] Q. Liu, V. Ramanathan, D. Mahajan, A. Yuille, Z. Yang, Weakly supervised
    instance segmentation for videos with temporal mask consistency, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp.
    13968–13978.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Q. Liu, V. Ramanathan, D. Mahajan, A. Yuille, Z. Yang, 基于弱监督的视频实例分割与时间掩码一致性，见：IEEE/CVF
    计算机视觉与模式识别会议论文集，2021，pp. 13968–13978。'
- en: '[60] I. Ruiz, L. Porzi, S. R. Bulo, P. Kontschieder, J. Serrat, Weakly supervised
    multi-object tracking and segmentation, in: Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision, 2021, pp. 125–133.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] I. Ruiz, L. Porzi, S. R. Bulo, P. Kontschieder, J. Serrat, 基于弱监督的多目标跟踪与分割，见：IEEE/CVF
    冬季计算机视觉应用会议论文集，2021，pp. 125–133。'
- en: '[61] X. Liu, H. Ren, T. Ye, Spatio-temporal attention network for video instance
    segmentation, in: Proceedings of the IEEE/CVF international conference on computer
    vision workshops, 2019, pp. 1–3.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] X. Liu, H. Ren, T. Ye, 基于时空注意力网络的视频实例分割，见：IEEE/CVF 国际计算机视觉研讨会论文集，2019，pp.
    1–3。'
- en: '[62] Y. Fu, L. Yang, D. Liu, T. S. Huang, H. Shi, Compfeat: Comprehensive feature
    aggregation for video instance segmentation, Proceedings of the AAAI Conference
    on Artificial Intelligence 35 (2) (2021) 1361–1369.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Y. Fu, L. Yang, D. Liu, T. S. Huang, H. Shi, Compfeat: 用于视频实例分割的综合特征聚合，《人工智能
    AAAI 会议论文集》35 (2) (2021) 1361–1369。'
- en: '[63] A. Abrantes, J. Wang, P. Chu, Q. You, Z. Liu, Refinevis: Video instance
    segmentation with temporal attention refinement, arXiv preprint arXiv:2306.04774
    (2023).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] A. Abrantes, J. Wang, P. Chu, Q. You, Z. Liu, Refinevis: 基于时间注意力的改进视频实例分割，arXiv
    预印本 arXiv:2306.04774 (2023)。'
- en: '[64] J. Cai, Y. Wang, H.-M. Hsu, H. Zhang, J.-N. Hwang, Dior: Distill observations
    to representations for multi-object tracking and segmentation, in: Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp.
    520–529.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. Cai, Y. Wang, H.-M. Hsu, H. Zhang, J.-N. Hwang, Dior: 将观察提炼为表示以进行多目标跟踪与分割，见：IEEE/CVF
    冬季计算机视觉应用会议论文集，2022，pp. 520–529。'
- en: '[65] J. Hu, L. Cao, Y. Lu, S. Zhang, Y. Wang, K. Li, F. Huang, L. Shao, R. Ji,
    Istr: End-to-end instance segmentation with transformers, arXiv preprint arXiv:2105.00637
    (2021).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] J. Hu, L. Cao, Y. Lu, S. Zhang, Y. Wang, K. Li, F. Huang, L. Shao, R.
    Ji, Istr: 基于变换器的端到端实例分割，arXiv 预印本 arXiv:2105.00637 (2021)。'
- en: '[66] T. Zhang, X. Tian, Y. Wu, S. Ji, X. Wang, Y. Zhang, P. Wan, Dvis: Decoupled
    video instance segmentation framework, arXiv preprint arXiv:2306.03413 (2023).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] T. Zhang, X. Tian, Y. Wu, S. Ji, X. Wang, Y. Zhang, P. Wan, Dvis: 解耦的视频实例分割框架，arXiv
    预印本 arXiv:2306.03413 (2023)。'
- en: '[67] X. Dong, Z. Ouyang, Z. Guo, J. Niu, Polarmask-tracker: Lightweight multi-object
    tracking and segmentation model for edge device, in: 2021 IEEE Intl Conf on Parallel
    & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable
    Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom),
    IEEE, 2021, pp. 689–696.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] X. Dong, Z. Ouyang, Z. Guo, J. Niu, Polarmask-tracker: 面向边缘设备的轻量级多目标跟踪与分割模型，见：2021
    年 IEEE 国际并行与分布处理应用、大数据与云计算、可持续计算与通信、社会计算与网络（ISPA/BDCloud/SocialCom/SustainCom）会议，IEEE，2021，pp.
    689–696。'
- en: '[68] D. Liu, Y. Cui, W. Tan, Y. Chen, Sg-net: Spatial granularity network for
    one-stage video instance segmentation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2021, pp. 9816–9825.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] D. Liu, Y. Cui, W. Tan, Y. Chen, Sg-net: 一阶段视频实例分割的空间粒度网络，见：IEEE/CVF 计算机视觉与模式识别会议论文集，2021，pp.
    9816–9825。'
- en: '[69] T.-N. Le, A. Sugimoto, Semantic instance meets salient object: Study on
    video semantic salient instance segmentation, in: 2019 IEEE Winter Conference
    on Applications of Computer Vision (WACV), IEEE, 2019, pp. 1779–1788.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] T.-N. Le, A. Sugimoto, 语义实例遇上显著目标：视频语义显著实例分割研究，见：2019 年 IEEE 冬季计算机视觉应用会议
    (WACV)，IEEE，2019，pp. 1779–1788。'
- en: '[70] H. Lin, X. Qi, J. Jia, Agss-vos: Attention guided single-shot video object
    segmentation, in: Proceedings of the IEEE/CVF international conference on computer
    vision, 2019, pp. 3949–3957.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] H. Lin, X. Qi, J. Jia, Agss-vos: 注意力引导的单次视频目标分割，见：IEEE/CVF 国际计算机视觉会议论文集，2019，pp.
    3949–3957。'
- en: '[71] W. Ge, X. Lu, J. Shen, Video object segmentation using global and instance
    embedding learning, in: Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition, 2021, pp. 16836–16845.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] W. Ge, X. Lu, J. Shen, 利用全局与实例嵌入学习的视频目标分割，见：IEEE/CVF 计算机视觉与模式识别会议论文集，2021，pp.
    16836–16845。'
- en: '[72] J. Wang, D. Chen, Z. Wu, C. Luo, C. Tang, X. Dai, Y. Zhao, Y. Xie, L. Yuan,
    Y.-G. Jiang, Look before you match: Instance understanding matters in video object
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, 2023, pp. 2268–2278.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] J. Wang, D. Chen, Z. Wu, C. Luo, C. Tang, X. Dai, Y. Zhao, Y. Xie, L. Yuan,
    Y.-G. Jiang, 匹配前的观察：实例理解在视频目标分割中的重要性，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，2023，第2268–2278页。'
- en: '[73] Q. Wang, Y. He, X. Yang, Z. Yang, P. Torr, An empirical study of detection-based
    video instance segmentation, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision Workshops, 2019, pp. 0–0.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Q. Wang, Y. He, X. Yang, Z. Yang, P. Torr, 基于检测的视频实例分割的实证研究，收录于：IEEE/CVF国际计算机视觉会议工作坊论文集，2019，第0–0页。'
- en: '[74] Q. Liu, J. Wu, Y. Jiang, X. Bai, A. L. Yuille, S. Bai, Instmove: Instance
    motion for object-centric video segmentation, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2023, pp. 6344–6354.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Q. Liu, J. Wu, Y. Jiang, X. Bai, A. L. Yuille, S. Bai, Instmove: 面向对象的视频分割中的实例运动，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，2023，第6344–6354页。'
- en: '[75] D. Bolya, C. Zhou, F. Xiao, Y. J. Lee, Yolact: Real-time instance segmentation,
    in: Proceedings of the IEEE/CVF international conference on computer vision, 2019,
    pp. 9157–9166.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] D. Bolya, C. Zhou, F. Xiao, Y. J. Lee, Yolact: 实时实例分割，收录于：IEEE/CVF国际计算机视觉会议论文集，2019，第9157–9166页。'
- en: '[76] D. Bolya, C. Zhou, F. Xiao, Y. J. Lee, Yolact++ better real-time instance
    segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence 44 (2)
    (2022) 1108–1121.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] D. Bolya, C. Zhou, F. Xiao, Y. J. Lee, Yolact++ 更佳的实时实例分割，IEEE模式分析与机器智能学报
    44 (2) (2022) 1108–1121。'
- en: '[77] H. Bae, S. Song, J. Park, Occluded video instance segmentation with set
    prediction approach, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2021, pp. 3850–3853.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] H. Bae, S. Song, J. Park, 遮挡视频实例分割与集合预测方法，收录于：IEEE/CVF国际计算机视觉会议论文集，2021，第3850–3853页。'
- en: '[78] J. Cao, R. M. Anwer, H. Cholakkal, F. S. Khan, Y. Pang, L. Shao, Sipmask:
    Spatial information preservation for fast image and video instance segmentation,
    in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part XIV 16, Springer, 2020, pp. 1–18.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] J. Cao, R. M. Anwer, H. Cholakkal, F. S. Khan, Y. Pang, L. Shao, Sipmask:
    快速图像和视频实例分割的空间信息保留，收录于：计算机视觉–ECCV 2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第XIV部分，第16页，Springer，2020，第1–18页。'
- en: '[79] J. Cao, Y. Pang, R. M. Anwer, H. Cholakkal, F. S. Khan, L. Shao, Sipmaskv2:
    Enhanced fast image and video instance segmentation, IEEE Transactions on Pattern
    Analysis and Machine Intelligence 45 (3) (2022) 3798–3812.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] J. Cao, Y. Pang, R. M. Anwer, H. Cholakkal, F. S. Khan, L. Shao, Sipmaskv2:
    增强的快速图像和视频实例分割，IEEE模式分析与机器智能学报 45 (3) (2022) 3798–3812。'
- en: '[80] M. Li, S. Li, L. Li, L. Zhang, Spatial feature calibration and temporal
    fusion for effective one-stage video instance segmentation, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp.
    11215–11224.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] M. Li, S. Li, L. Li, L. Zhang, 空间特征校准与时间融合以实现有效的一阶段视频实例分割，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，2021，第11215–11224页。'
- en: '[81] H. Liu, R. A. R. Soto, F. Xiao, Y. J. Lee, Yolactedge: Real-time instance
    segmentation on the edge, in: 2021 IEEE International Conference on Robotics and
    Automation (ICRA), IEEE, 2021, pp. 9579–9585.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] H. Liu, R. A. R. Soto, F. Xiao, Y. J. Lee, Yolactedge: 边缘上的实时实例分割，收录于：2021年IEEE国际机器人与自动化大会（ICRA），IEEE，2021，第9579–9585页。'
- en: '[82] J. Wu, J. Cao, L. Song, Y. Wang, M. Yang, J. Yuan, Track to detect and
    segment: An online multi-object tracker, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2021, pp. 12352–12361.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] J. Wu, J. Cao, L. Song, Y. Wang, M. Yang, J. Yuan, 跟踪以检测和分割：一种在线多目标跟踪器，收录于：IEEE/CVF计算机视觉与模式识别会议论文集，2021，第12352–12361页。'
- en: '[83] S. Yang, Y. Fang, X. Wang, Y. Li, C. Fang, Y. Shan, B. Feng, W. Liu, Crossover
    learning for fast online video instance segmentation, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2021, pp. 8043–8052.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] S. Yang, Y. Fang, X. Wang, Y. Li, C. Fang, Y. Shan, B. Feng, W. Liu, 快速在线视频实例分割的交叉学习，收录于：IEEE/CVF国际计算机视觉会议论文集，2021，第8043–8052页。'
- en: '[84] J. Wu, Q. Liu, Y. Jiang, S. Bai, A. Yuille, X. Bai, In defense of online
    models for video instance segmentation, in: European Conference on Computer Vision,
    Springer, 2022, pp. 588–605.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] J. Wu, Q. Liu, Y. Jiang, S. Bai, A. Yuille, 在视频实例分割的在线模型辩护中，收录于：欧洲计算机视觉会议，Springer，2022，第588–605页。'
- en: '[85] J. Wu, X. Bai, Y. Jiang, Q. Liu, Z. Yuan, S. Bai, 1st place solution for
    youtubevos challenge 2022: video instance segmentation, in: CVPR Workshops, 2022,
    pp. 1–4.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. Wu, X. Bai, Y. Jiang, Q. Liu, Z. Yuan, S. Bai, YouTubeVOS挑战赛2022年第1名解决方案：视频实例分割，见：CVPR研讨会，2022年，第1–4页。'
- en: '[86] Z. Jiang, Z. Gu, J. Peng, H. Zhou, L. Liu, Y. Wang, Y. Tai, C. Wang, L. Zhang,
    Stc: spatio-temporal contrastive learning for video instance segmentation, in:
    European Conference on Computer Vision, Springer, 2022, pp. 539–556.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Z. Jiang, Z. Gu, J. Peng, H. Zhou, L. Liu, Y. Wang, Y. Tai, C. Wang, L.
    Zhang, STC：用于视频实例分割的时空对比学习，见：欧洲计算机视觉会议，Springer，2022年，第539–556页。'
- en: '[87] F. Zhu, Z. Yang, X. Yu, Y. Yang, Y. Wei, Instance as identity: A generic
    online paradigm for video instance segmentation, in: European Conference on Computer
    Vision, Springer, 2022, pp. 524–540.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] F. Zhu, Z. Yang, X. Yu, Y. Yang, Y. Wei, 实例即身份：一种通用的在线视频实例分割范式，见：欧洲计算机视觉会议，Springer，2022年，第524–540页。'
- en: '[88] S. H. Han, S. Hwang, S. W. Oh, Y. Park, H. Kim, M.-J. Kim, S. J. Kim,
    Visolo: Grid-based space-time aggregation for efficient online video instance
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, 2022, pp. 2896–2905.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] S. H. Han, S. Hwang, S. W. Oh, Y. Park, H. Kim, M.-J. Kim, S. J. Kim,
    VISolo：基于网格的时空聚合用于高效在线视频实例分割，见：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第2896–2905页。'
- en: '[89] X. Li, J. Wang, X. Li, Y. Lu, Hybrid instance-aware temporal fusion for
    online video instance segmentation, Proceedings of the AAAI Conference on Artificial
    Intelligence 36 (2) (2022) 1429–1437.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] X. Li, J. Wang, X. Li, Y. Lu, 在线视频实例分割的混合实例感知时间融合，《AAAI人工智能会议论文集》36 (2)
    (2022) 1429–1437。'
- en: '[90] X. Li, J. Wang, X. Li, Y. Lu, Video instance segmentation by instance
    flow assembly, IEEE Transactions on Multimedia (2022).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] X. Li, J. Wang, X. Li, Y. Lu, 通过实例流汇总进行视频实例分割，《IEEE多媒体汇刊》(2022)。'
- en: '[91] B. Yan, Y. Jiang, P. Sun, D. Wang, Z. Yuan, P. Luo, H. Lu, Towards grand
    unification of object tracking, in: European Conference on Computer Vision, Springer,
    2022, pp. 733–751.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] B. Yan, Y. Jiang, P. Sun, D. Wang, Z. Yuan, P. Luo, H. Lu, 迈向目标跟踪的宏大统一，见：欧洲计算机视觉会议，Springer，2022年，第733–751页。'
- en: '[92] P. Yang, Y. M. Asano, P. Mettes, C. G. Snoek, Less than few: Self-shot
    video instance segmentation, in: European Conference on Computer Vision, Springer,
    2022, pp. 449–466.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] P. Yang, Y. M. Asano, P. Mettes, C. G. Snoek, 少于几个：自拍视频实例分割，见：欧洲计算机视觉会议，Springer，2022年，第449–466页。'
- en: '[93] H. Kim, S. Lee, S. Im, Offline-to-online knowledge distillation for video
    instance segmentation, arXiv preprint arXiv:2302.07516 (2023).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] H. Kim, S. Lee, S. Im, 从离线到在线的知识蒸馏用于视频实例分割，arXiv预印本 arXiv:2302.07516 (2023)。'
- en: '[94] Z. Xu, W. Zhang, X. Tan, W. Yang, H. Huang, S. Wen, E. Ding, L. Huang,
    Segment as points for efficient online multi-object tracking and segmentation,
    in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,
    2020, Proceedings, Part I 16, Springer, 2020, pp. 264–281.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Z. Xu, W. Zhang, X. Tan, W. Yang, H. Huang, S. Wen, E. Ding, L. Huang,
    将段落视作点以实现高效的在线多目标跟踪与分割，见：《计算机视觉–ECCV 2020：第16届欧洲会议》，英国格拉斯哥，2020年8月23–28日，论文集，第16卷，Springer，2020年，第264–281页。'
- en: '[95] Z. Xu, W. Yang, W. Zhang, X. Tan, H. Huang, L. Huang, Segment as points
    for efficient and effective online multi-object tracking and segmentation, IEEE
    Transactions on Pattern Analysis and Machine Intelligence 44 (10) (2021) 6424–6437.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Z. Xu, W. Yang, W. Zhang, X. Tan, H. Huang, L. Huang, 将段落视作点以实现高效和有效的在线多目标跟踪与分割，《IEEE模式分析与机器智能汇刊》44
    (10) (2021) 6424–6437。'
- en: '[96] C.-C. Lin, Y. Hung, R. Feris, L. He, Video instance segmentation tracking
    with a modified vae architecture, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, 2020, pp. 13147–13157.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C.-C. Lin, Y. Hung, R. Feris, L. He, 视频实例分割跟踪与修改的VAE架构，见：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，第13147–13157页。'
- en: '[97] Z. Qin, X. Lu, X. Nie, X. Zhen, Y. Yin, Learning hierarchical embedding
    for video instance segmentation, in: Proceedings of the 29th ACM International
    Conference on Multimedia, 2021, pp. 1884–1892.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Z. Qin, X. Lu, X. Nie, X. Zhen, Y. Yin, 为视频实例分割学习层次嵌入，见：第29届ACM国际多媒体会议论文集，2021年，第1884–1892页。'
- en: '[98] Z. Qin, X. Lu, X. Nie, D. Liu, Y. Yin, W. Wang, Coarse-to-fine video instance
    segmentation with factorized conditional appearance flows, IEEE/CAA Journal of
    Automatica Sinica 10 (5) (2023) 1192–1208.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Z. Qin, X. Lu, X. Nie, D. Liu, Y. Yin, W. Wang, 粗到细的视频实例分割与因子化条件外观流，《IEEE/CAA自动化学报》10
    (5) (2023) 1192–1208。'
- en: '[99] T. Zhou, J. Li, X. Li, L. Shao, Target-aware object discovery and association
    for unsupervised video multi-object segmentation, in: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, 2021, pp. 6985–6994.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] T. 周，J. 李，X. 李，L. 邵，面向无监督视频多目标分割的目标感知对象发现与关联，载于：IEEE/CVF 计算机视觉与模式识别会议论文集，2021，页
    6985–6994。'
- en: '[100] L. Yan, Q. Wang, S. Ma, J. Wang, C. Yu, Solve the puzzle of instance
    segmentation in videos: A weakly supervised framework with spatio-temporal collaboration,
    IEEE Transactions on Circuits and Systems for Video Technology 33 (1) (2022) 393–406.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] L. 闫，Q. 王，S. 马，J. 王，C. 余，解决视频中实例分割的难题：一种具有时空协作的弱监督框架，《IEEE 电路与系统视频技术汇刊》33
    (1) (2022) 393–406。'
- en: '[101] A. Athar, S. Mahadevan, A. Osep, L. Leal-Taixé, B. Leibe, Stem-seg: Spatio-temporal
    embeddings for instance segmentation in videos, in: Computer Vision–ECCV 2020:
    16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI
    16, Springer, 2020, pp. 158–177.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] A. 阿特哈，S. 马哈德万，A. 奥赛普，L. 利尔-塔伊克，B. 莱布，Stem-seg: 用于视频实例分割的时空嵌入，载于：计算机视觉–ECCV
    2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，Part XI 16，Springer，2020，页 158–177。'
- en: '[102] G. Brasó, O. Cetintas, L. Leal-Taixé, Multi-object tracking and segmentation
    via neural message passing, International Journal of Computer Vision 130 (12)
    (2022) 3035–3053.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] G. 布拉索，O. 切廷塔斯，L. 利尔-塔伊克，通过神经消息传递的多目标跟踪与分割，《计算机视觉国际期刊》130 (12) (2022)
    3035–3053。'
- en: '[103] B. Cheng, A. Choudhuri, I. Misra, A. Kirillov, R. Girdhar, A. G. Schwing,
    Mask2former for video instance segmentation, arXiv preprint arXiv:2112.10764 (2021).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] B. 程，A. 周，I. 密斯拉，A. 基里洛夫，R. 吉尔达，A. G. 施温，Mask2former 用于视频实例分割，arXiv 预印本
    arXiv:2112.10764 (2021)。'
- en: '[104] A. Choudhuri, G. Chowdhary, A. G. Schwing, Context-aware relative object
    queries to unify video instance and panoptic segmentation, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp.
    6377–6386.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] A. 周，G. 周达里，A. G. 施温，利用上下文感知的相对对象查询统一视频实例和全景分割，载于：IEEE/CVF 计算机视觉与模式识别会议论文集，2023，页
    6377–6386。'
- en: '[105] S. Hwang, M. Heo, S. W. Oh, S. J. Kim, Video instance segmentation using
    inter-frame communication transformers, Advances in Neural Information Processing
    Systems 34 (2021) 13352–13363.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] S. 黄，M. 许，S. W. 吴，S. J. 金，使用帧间通信变换器的视频实例分割，《神经信息处理系统进展》34 (2021) 13352–13363。'
- en: '[106] J. Wu, Y. Jiang, S. Bai, W. Zhang, X. Bai, Seqformer: Sequential transformer
    for video instance segmentation, in: European Conference on Computer Vision, Springer,
    2022, pp. 553–569.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] J. 吴，Y. 姜，S. 白，W. 张，X. 白，Seqformer: 用于视频实例分割的序列变换器，载于：欧洲计算机视觉会议，Springer，2022，页
    553–569。'
- en: '[107] Z. Zhang, F. Shao, Z. Dai, S. Zhu, Towards robust video instance segmentation
    with temporal-aware transformer, arXiv preprint arXiv:2301.09416 (2023).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Z. 张，F. 邵，Z. 戴，S. 朱，面向具有时间感知变换器的鲁棒视频实例分割，arXiv 预印本 arXiv:2301.09416 (2023)。'
- en: '[108] S. Yang, X. Wang, Y. Li, Y. Fang, J. Fang, W. Liu, X. Zhao, Y. Shan,
    Temporally efficient vision transformer for video instance segmentation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.
    2885–2895.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] S. 杨，X. 王，Y. 李，Y. 方，J. 方，W. 刘，X. 赵，Y. 山，面向视频实例分割的时间高效视觉变换器，载于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2022，页 2885–2895。'
- en: '[109] R. Zhang, T. Cheng, S. Yang, H. Jiang, S. Zhang, J. Lyu, X. Li, X. Ying,
    D. Gao, W. Liu, et al., Mobileinst: Video instance segmentation on the mobile,
    arXiv preprint arXiv:2303.17594 (2023).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] R. 张，T. 程，S. 杨，H. 姜，S. 张，J. 吕，X. 李，X. 应，D. 高，W. 刘，等，Mobileinst: 移动端的视频实例分割，arXiv
    预印本 arXiv:2303.17594 (2023)。'
- en: '[110] J. Wu, S. Yarram, H. Liang, T. Lan, J. Yuan, J. Eledath, G. Medioni,
    Efficient video instance segmentation via tracklet query and proposal, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.
    959–968.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] J. 吴，S. 亚拉姆，H. 梁，T. 蓝，J. 袁，J. 埃莱达，G. 美迪奥尼，通过轨迹查询和提议的高效视频实例分割，载于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2022，页 959–968。'
- en: '[111] M. Heo, S. Hwang, S. W. Oh, J.-Y. Lee, S. J. Kim, Vita: Video instance
    segmentation via object token association, Advances in Neural Information Processing
    Systems 35 (2022) 23109–23120.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] M. 许，S. 黄，S. W. 吴，J.-Y. 李，S. J. 金，Vita: 通过对象令牌关联的视频实例分割，《神经信息处理系统进展》35
    (2022) 23109–23120。'
- en: '[112] D.-A. Huang, Z. Yu, A. Anandkumar, Minvis: A minimal video instance segmentation
    framework without video-based training, Advances in Neural Information Processing
    Systems 35 (2022) 31265–31277.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] D.-A. 黄，Z. 余，A. 安南库马尔，Minvis: 一个没有基于视频训练的最简视频实例分割框架，《神经信息处理系统进展》35 (2022)
    31265–31277。'
- en: '[113] L. Ke, M. Danelljan, H. Ding, Y.-W. Tai, C.-K. Tang, F. Yu, Mask-free
    video instance segmentation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2023, pp. 22857–22866.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] L. Ke, M. Danelljan, H. Ding, Y.-W. Tai, C.-K. Tang, F. Yu, 无掩码视频实例分割，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2023年，第22857–22866页。'
- en: '[114] J. Sun, J. Xie, J.-F. Hu, Z. Lin, J. Lai, W. Zeng, W.-s. Zheng, Predicting
    future instance segmentation with contextual pyramid convlstms, in: Proceedings
    of the 27th acm international conference on multimedia, 2019, pp. 2043–2051.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] J. Sun, J. Xie, J.-F. Hu, Z. Lin, J. Lai, W. Zeng, W.-s. Zheng, 使用上下文金字塔
    convlstms 预测未来实例分割，发表于：第27届 ACM 国际多媒体会议论文集，2019年，第2043–2051页。'
- en: '[115] J.-F. Hu, J. Sun, Z. Lin, J.-H. Lai, W. Zeng, W.-S. Zheng, Apanet: Auto-path
    aggregation for future instance segmentation prediction, IEEE Transactions on
    Pattern Analysis and Machine Intelligence 44 (7) (2021) 3386–3403.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J.-F. Hu, J. Sun, Z. Lin, J.-H. Lai, W. Zeng, W.-S. Zheng, Apanet: 面向未来实例分割预测的自适应路径聚合，IEEE
    模式分析与机器智能汇刊 44 (7) (2021) 3386–3403。'
- en: '[116] J. Johnander, E. Brissman, M. Danelljan, M. Felsberg, Video instance
    segmentation with recurrent graph neural networks, in: DAGM German Conference
    on Pattern Recognition, Springer, 2021, pp. 206–221.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] J. Johnander, E. Brissman, M. Danelljan, M. Felsberg, 使用递归图神经网络的视频实例分割，发表于：DAGM
    德国模式识别会议，Springer，2021年，第206–221页。'
- en: '[117] E. Brissman, J. Johnander, M. Danelljan, M. Felsberg, Recurrent graph
    neural networks for video instance segmentation, International Journal of Computer
    Vision 131 (2) (2023) 471–495.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] E. Brissman, J. Johnander, M. Danelljan, M. Felsberg, 递归图神经网络用于视频实例分割，计算机视觉国际期刊
    131 (2) (2023) 471–495。'
- en: '[118] T. Meinhardt, A. Kirillov, L. Leal-Taixe, C. Feichtenhofer, Trackformer:
    Multi-object tracking with transformers, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2022, pp. 8844–8854.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] T. Meinhardt, A. Kirillov, L. Leal-Taixe, C. Feichtenhofer, Trackformer:
    使用变换器的多目标跟踪，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2022年，第8844–8854页。'
- en: '[119] R. Koner, T. Hannan, S. Shit, S. Sharifzadeh, M. Schubert, T. Seidl,
    V. Tresp, Instanceformer: An online video instance segmentation framework, Proceedings
    of the AAAI Conference on Artificial Intelligence 37 (1) (2023) 1188–1195.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] R. Koner, T. Hannan, S. Shit, S. Sharifzadeh, M. Schubert, T. Seidl,
    V. Tresp, Instanceformer: 一个在线视频实例分割框架，AAAI 人工智能会议论文集 37 (1) (2023) 1188–1195。'
- en: '[120] M. Heo, S. Hwang, J. Hyun, H. Kim, S. W. Oh, J.-Y. Lee, S. J. Kim, A
    generalized framework for video instance segmentation, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 14623–14632.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] M. Heo, S. Hwang, J. Hyun, H. Kim, S. W. Oh, J.-Y. Lee, S. J. Kim, 视频实例分割的通用框架，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2023年，第14623–14632页。'
- en: '[121] Q. You, J. Wang, P. Chu, A. Abrantes, Z. Liu, Consistent video instance
    segmentation with inter-frame recurrent attention, arXiv preprint arXiv:2206.07011
    (2022).'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Q. You, J. Wang, P. Chu, A. Abrantes, Z. Liu, 一致性视频实例分割与帧间递归注意力，arXiv
    预印本 arXiv:2206.07011 (2022)。'
- en: '[122] K. Ying, Q. Zhong, W. Mao, Z. Wang, H. Chen, L. Y. Wu, Y. Liu, C. Fan,
    Y. Zhuge, C. Shen, Ctvis: Consistent training for online video instance segmentation,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023,
    pp. 899–908.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] K. Ying, Q. Zhong, W. Mao, Z. Wang, H. Chen, L. Y. Wu, Y. Liu, C. Fan,
    Y. Zhuge, C. Shen, Ctvis: 一致性训练用于在线视频实例分割，发表于：IEEE/CVF 国际计算机视觉会议论文集，2023年，第899–908页。'
- en: '[123] T. Hannan, R. Koner, M. Bernhard, S. Shit, B. Menze, V. Tresp, M. Schubert,
    T. Seidl, Gratt-vis: Gated residual attention for auto rectifying video instance
    segmentation, arXiv preprint arXiv:2305.17096 (2023).'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] T. Hannan, R. Koner, M. Bernhard, S. Shit, B. Menze, V. Tresp, M. Schubert,
    T. Seidl, Gratt-vis: 用于自动修正视频实例分割的门控残差注意力，arXiv 预印本 arXiv:2305.17096 (2023)。'
- en: '[124] K. He, G. Gkioxari, P. Dollár, R. Girshick, Mask r-cnn, in: Proceedings
    of the IEEE international conference on computer vision, 2017, pp. 2961–2969.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] K. He, G. Gkioxari, P. Dollár, R. Girshick, Mask r-cnn，发表于：IEEE 国际计算机视觉会议论文集，2017年，第2961–2969页。'
- en: '[125] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object
    detection with region proposal networks, Advances in neural information processing
    systems 28 (2015).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: 基于区域提议网络的实时物体检测，神经信息处理系统进展
    28 (2015)。'
- en: '[126] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, S. Belongie,
    Feature pyramid networks for object detection, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, 2017, pp. 2117–2125.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, S. Belongie,
    特征金字塔网络用于物体检测，发表于：IEEE 计算机视觉与模式识别会议论文集，2017年，第2117–2125页。'
- en: '[127] G. Zhang, X. Lu, J. Tan, J. Li, Z. Zhang, Q. Li, X. Hu, Refinemask: Towards
    high-quality instance segmentation with fine-grained features, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp.
    6861–6869.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] G. Zhang, X. Lu, J. Tan, J. Li, Z. Zhang, Q. Li, X. Hu，《RefineMask：通过细粒度特征实现高质量实例分割》，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2021年，第6861–6869页。'
- en: '[128] S. W. Oh, J.-Y. Lee, N. Xu, S. J. Kim, Video object segmentation using
    space-time memory networks, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2019, pp. 9226–9235.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] S. W. Oh, J.-Y. Lee, N. Xu, S. J. Kim，《利用时空记忆网络的视频对象分割》，发表于：IEEE/CVF
    国际计算机视觉会议论文集，2019年，第9226–9235页。'
- en: '[129] J. Ahn, S. Cho, S. Kwak, Weakly supervised learning of instance segmentation
    with inter-pixel relations, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, 2019, pp. 2209–2218.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] J. Ahn, S. Cho, S. Kwak，《基于像素间关系的弱监督实例分割学习》，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2019年，第2209–2218页。'
- en: '[130] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra,
    Grad-cam: Visual explanations from deep networks via gradient-based localization,
    in: Proceedings of the IEEE international conference on computer vision, 2017,
    pp. 618–626.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra，《Grad-CAM：通过基于梯度的定位从深度网络获得的可视化解释》，发表于：IEEE
    国际计算机视觉会议论文集，2017年，第618–626页。'
- en: '[131] Z. Tian, C. Shen, H. Chen, T. He, Fcos: Fully convolutional one-stage
    object detection, in: Proceedings of the IEEE/CVF international conference on
    computer vision, 2019, pp. 9627–9636.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Z. Tian, C. Shen, H. Chen, T. He，《FCOS：全卷积单阶段对象检测》，发表于：IEEE/CVF 国际计算机视觉会议论文集，2019年，第9627–9636页。'
- en: '[132] M. Hossny, S. Nahavandi, D. Creighton, C. Lim, A. Bhatti, Enhanced decision
    fusion of semantically segmented images via local majority saliency map, Electronics
    Letters 53 (15) (2017) 1036–1038.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] M. Hossny, S. Nahavandi, D. Creighton, C. Lim, A. Bhatti，《通过局部多数显著性图增强语义分割图像的决策融合》，《电子通信信函》53
    (15) (2017) 1036–1038。'
- en: '[133] T.-N. Le, A. Sugimoto, Deeply supervised 3d recurrent fcn for salient
    object detection in videos., in: BMVC, Vol. 1, 2017, p. 3.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] T.-N. Le, A. Sugimoto，《深度监督的3D递归FCN用于视频中的显著性目标检测》，发表于：BMVC，第1卷，2017年，第3页。'
- en: '[134] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu,
    J. Shi, W. Ouyang, et al., Hybrid task cascade for instance segmentation, in:
    Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2019, pp. 4974–4983.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu,
    J. Shi, W. Ouyang 等，《用于实例分割的混合任务级联》，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2019年，第4974–4983页。'
- en: '[135] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, P. H. Torr, Fast online object
    tracking and segmentation: A unifying approach, in: Proceedings of the IEEE/CVF
    conference on Computer Vision and Pattern Recognition, 2019, pp. 1328–1338.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, P. H. Torr，《快速在线对象跟踪与分割：一种统一方法》，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2019年，第1328–1338页。'
- en: '[136] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once: Unified,
    real-time object detection, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2016, pp. 779–788.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] J. Redmon, S. Divvala, R. Girshick, A. Farhadi，《你只看一次：统一的实时对象检测》，发表于：IEEE
    计算机视觉与模式识别会议论文集，2016年，第779–788页。'
- en: '[137] X. Chang, H. Pan, W. Sun, H. Gao, Yoltrack: Multitask learning based
    real-time multiobject tracking and segmentation for autonomous vehicles, IEEE
    Transactions on Neural Networks and Learning Systems 32 (12) (2021) 5323–5333.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] X. Chang, H. Pan, W. Sun, H. Gao，《Yoltrack：基于多任务学习的实时多对象跟踪与分割用于自动驾驶车辆》，《IEEE
    神经网络与学习系统汇刊》32 (12) (2021) 5323–5333。'
- en: '[138] L. Zhu, Z. Xu, Y. Yang, Bidirectional multirate reconstruction for temporal
    modeling in videos, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2017, pp. 2653–2662.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] L. Zhu, Z. Xu, Y. Yang，《用于视频时间建模的双向多速率重建》，发表于：IEEE 计算机视觉与模式识别会议论文集，2017年，第2653–2662页。'
- en: '[139] X. Wang, T. Kong, C. Shen, Y. Jiang, L. Li, Solo: Segmenting objects
    by locations, in: Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
    UK, August 23–28, 2020, Proceedings, Part XVIII 16, Springer, 2020, pp. 649–665.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] X. Wang, T. Kong, C. Shen, Y. Jiang, L. Li，《Solo：通过位置分割对象》，发表于：计算机视觉–ECCV
    2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，会议论文集，第XVIII部分 16，Springer，2020年，第649–665页。'
- en: '[140] X. Wang, R. Zhang, T. Kong, L. Li, C. Shen, Solov2: Dynamic and fast
    instance segmentation, Advances in Neural information processing systems 33 (2020)
    17721–17732.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] X. Wang, R. Zhang, T. Kong, L. Li, C. Shen, SOLOv2：动态且快速的实例分割，《神经信息处理系统进展》33
    (2020) 17721–17732。'
- en: '[141] L. Wang, K.-J. Yoon, Knowledge distillation and student-teacher learning
    for visual intelligence: A review and new outlooks, IEEE transactions on pattern
    analysis and machine intelligence 44 (6) (2021) 3048–3068.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] L. Wang, K.-J. Yoon, 知识蒸馏和学生-教师学习用于视觉智能：综述与新展望，《IEEE模式分析与机器智能汇刊》44 (6)
    (2021) 3048–3068。'
- en: '[142] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, G. E. Dahl, Neural
    message passing for quantum chemistry, in: International conference on machine
    learning, PMLR, 2017, pp. 1263–1272.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, G. E. Dahl, 用于量子化学的神经消息传递，发表于：国际机器学习会议，PMLR，2017，页码
    1263–1272。'
- en: '[143] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, R. Girdhar, Masked-attention
    mask transformer for universal image segmentation, in: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, 2022, pp. 1290–1299.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, R. Girdhar, 掩码注意力掩码变换器用于通用图像分割，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2022，页码
    1290–1299。'
- en: '[144] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, J. Dai, Deformable detr: Deformable
    transformers for end-to-end object detection, in: International Conference on
    Learning Representations, 2021, pp. 1–16.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, J. Dai, 可变形 DETR：用于端到端物体检测的可变形变换器，发表于：国际学习表征会议，2021，页码
    1–16。'
- en: '[145] J. Fang, L. Xie, X. Wang, X. Zhang, W. Liu, Q. Tian, Msg-transformer:
    Exchanging local spatial information by manipulating messenger tokens, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.
    12063–12072.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] J. Fang, L. Xie, X. Wang, X. Zhang, W. Liu, Q. Tian, MSG-变换器：通过操控信使令牌交换局部空间信息，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2022，页码
    12063–12072。'
- en: '[146] Q. Wan, Z. Huang, J. Lu, Y. Gang, L. Zhang, Seaformer: Squeeze-enhanced
    axial transformer for mobile semantic segmentation, in: The Eleventh International
    Conference on Learning Representations, 2023, pp. 1–19.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Q. Wan, Z. Huang, J. Lu, Y. Gang, L. Zhang, Seaformer：用于移动语义分割的压缩增强轴向变换器，发表于：第十一届国际学习表征会议，2023，页码
    1–19。'
- en: '[147] Z. Tian, C. Shen, X. Wang, H. Chen, Boxinst: High-performance instance
    segmentation with box annotations, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2021, pp. 5443–5452.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Z. Tian, C. Shen, X. Wang, H. Chen, Boxinst：基于框注释的高性能实例分割，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2021，页码
    5443–5452。'
- en: '[148] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, W.-c. Woo, Convolutional
    lstm network: A machine learning approach for precipitation nowcasting, Advances
    in neural information processing systems 28 (2015).'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, W.-c. Woo, 卷积 LSTM
    网络：一种用于降水即刻预测的机器学习方法，《神经信息处理系统进展》28 (2015)。'
- en: '[149] Y. Fang, S. Yang, X. Wang, Y. Li, C. Fang, Y. Shan, B. Feng, W. Liu,
    Instances as queries, in: Proceedings of the IEEE/CVF international conference
    on computer vision, 2021, pp. 6910–6919.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. Fang, S. Yang, X. Wang, Y. Li, C. Fang, Y. Shan, B. Feng, W. Liu,
    实例作为查询，发表于：IEEE/CVF国际计算机视觉会议论文集，2021，页码 6910–6919。'
- en: '[150] L. Ke, H. Ding, M. Danelljan, Y.-W. Tai, C.-K. Tang, F. Yu, Video mask
    transfiner for high-quality video instance segmentation, in: European Conference
    on Computer Vision, Springer, 2022, pp. 731–747.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] L. Ke, H. Ding, M. Danelljan, Y.-W. Tai, C.-K. Tang, F. Yu, 视频掩码变换器用于高质量视频实例分割，发表于：欧洲计算机视觉会议，Springer，2022，页码
    731–747。'
- en: '[151] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, O. Beijbom, nuscenes: A multimodal dataset for autonomous driving,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2020, pp. 11621–11631.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, O. Beijbom, nuscenes：用于自动驾驶的多模态数据集，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2020，页码
    11621–11631。'
- en: '[152] J. Qi, Y. Gao, Y. Hu, X. Wang, X. Liu, X. Bai, S. Belongie, A. Yuille,
    P. H. Torr, S. Bai, Occluded video instance segmentation: A benchmark, International
    Journal of Computer Vision 130 (8) (2022) 2022–2039.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] J. Qi, Y. Gao, Y. Hu, X. Wang, X. Liu, X. Bai, S. Belongie, A. Yuille,
    P. H. Torr, S. Bai, 遮挡视频实例分割：一个基准，《计算机视觉国际期刊》130 (8) (2022) 2022–2039。'
- en: '[153] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, T. Darrell,
    Bdd100k: A diverse driving dataset for heterogeneous multitask learning, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp.
    2636–2645.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, T. Darrell,
    Bdd100k：用于异构多任务学习的多样化驾驶数据集，《IEEE/CVF计算机视觉与模式识别会议论文集》，2020年，页码2636–2645。'
- en: '[154] W. Wang, M. Feiszli, H. Wang, D. Tran, Unidentified video objects: A
    benchmark for dense, open-world segmentation, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2021, pp. 10776–10785.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] W. Wang, M. Feiszli, H. Wang, D. Tran, 未识别的视频物体：密集开放世界分割的基准，《IEEE/CVF国际计算机视觉会议论文集》，2021年，页码10776–10785。'
- en: '[155] J. Miao, X. Wang, Y. Wu, W. Li, X. Zhang, Y. Wei, Y. Yang, Large-scale
    video panoptic segmentation in the wild: A benchmark, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2022, pp. 21033–21043.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] J. Miao, X. Wang, Y. Wu, W. Li, X. Zhang, Y. Wei, Y. Yang, 大规模视频全景分割基准：野外应用，《IEEE/CVF计算机视觉与模式识别会议论文集》，2022年，页码21033–21043。'
- en: '[156] A. Athar, J. Luiten, P. Voigtlaender, T. Khurana, A. Dave, B. Leibe,
    D. Ramanan, Burst: A benchmark for unifying object recognition, segmentation and
    tracking in video, in: Proceedings of the IEEE/CVF Winter Conference on Applications
    of Computer Vision, 2023, pp. 1674–1683.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] A. Athar, J. Luiten, P. Voigtlaender, T. Khurana, A. Dave, B. Leibe,
    D. Ramanan, Burst：统一视频中的物体识别、分割和跟踪的基准，《IEEE/CVF计算机视觉应用冬季会议论文集》，2023年，页码1674–1683。'
- en: '[157] X. Li, H. He, Y. Yang, H. Ding, K. Yang, G. Cheng, Y. Tong, D. Tao, Improving
    video instance segmentation via temporal pyramid routing, IEEE Transactions on
    Pattern Analysis and Machine Intelligence 45 (5) (2022) 6594–6601.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] X. Li, H. He, Y. Yang, H. Ding, K. Yang, G. Cheng, Y. Tong, D. Tao, 通过时间金字塔路由改进视频实例分割，《IEEE模式分析与机器智能汇刊》45(5)
    (2022) 6594–6601。'
- en: '[158] Y. Gao, H. Xu, Y. Zheng, J. Li, X. Gao, An object point set inductive
    tracker for multi-object tracking and segmentation, IEEE Transactions on Image
    Processing 31 (2022) 6083–6096.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Y. Gao, H. Xu, Y. Zheng, J. Li, X. Gao, 一种用于多物体跟踪和分割的物体点集归纳跟踪器，《IEEE图像处理汇刊》31
    (2022) 6083–6096。'
- en: '[159] Z. Xu, A. Meng, Z. Shi, W. Yang, Z. Chen, L. Huang, Continuous copy-paste
    for one-stage multi-object tracking and segmentation, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2021, pp. 15323–15332.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Z. Xu, A. Meng, Z. Shi, W. Yang, Z. Chen, L. Huang, 连续复制粘贴用于一阶段多物体跟踪和分割，《IEEE/CVF国际计算机视觉会议论文集》，2021年，页码15323–15332。'
- en: '[160] J. Yoon, M.-K. Choi, Exploring video frame redundancies for efficient
    data sampling and annotation in instance segmentation, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 3307–3316.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] J. Yoon, M.-K. Choi, 探索视频帧冗余以实现高效数据采样和实例分割注释，《IEEE/CVF计算机视觉与模式识别会议论文集》，2023年，页码3307–3316。'
- en: '[161] X. Wei, C.-T. Li, Z. Lei, D. Yi, S. Z. Li, Dynamic image-to-class warping
    for occluded face recognition, IEEE Transactions on Information Forensics and
    Security 9 (12) (2014) 2035–2050.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] X. Wei, C.-T. Li, Z. Lei, D. Yi, S. Z. Li, 动态图像到类别的变形用于遮挡人脸识别，《IEEE信息取证与安全汇刊》9(12)
    (2014) 2035–2050。'
- en: '[162] L. Ke, Y.-W. Tai, C.-K. Tang, Deep occlusion-aware instance segmentation
    with overlapping bilayers, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, 2021, pp. 4019–4028.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] L. Ke, Y.-W. Tai, C.-K. Tang, 深度遮挡感知实例分割与重叠双层，《IEEE/CVF计算机视觉与模式识别会议论文集》，2021年，页码4019–4028。'
- en: '[163] L. Ke, Y.-W. Tai, C.-K. Tang, Occlusion-aware instance segmentation via
    bilayer network architectures, IEEE Transactions on Pattern Analysis and Machine
    Intelligence (2023).'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] L. Ke, Y.-W. Tai, C.-K. Tang, 遮挡感知实例分割通过双层网络架构，《IEEE模式分析与机器智能汇刊》 (2023)。'
- en: '[164] R. Leyva, V. Sanchez, C.-T. Li, Compact and low-complexity binary feature
    descriptor and fisher vectors for video analytics, IEEE Transactions on Image
    Processing 28 (12) (2019) 6169–6184.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] R. Leyva, V. Sanchez, C.-T. Li, 紧凑且低复杂度的二进制特征描述符和视频分析中的Fisher向量，《IEEE图像处理汇刊》28(12)
    (2019) 6169–6184。'
- en: '[165] X. Lin, C.-T. Li, S. Adams, A. Z. Kouzani, R. Jiang, L. He, Y. Hu, M. Vernon,
    E. Doeven, L. Webb, et al., Self-supervised leaf segmentation under complex lighting
    conditions, Pattern Recognition 135 (2023) 109021.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] X. Lin, C.-T. Li, S. Adams, A. Z. Kouzani, R. Jiang, L. He, Y. Hu, M.
    Vernon, E. Doeven, L. Webb, 等，自监督叶片分割在复杂光照条件下，《模式识别》135 (2023) 109021。'
- en: '[166] F. Pourpanah, M. Abdar, Y. Luo, X. Zhou, R. Wang, C. P. Lim, X.-Z. Wang,
    Q. J. Wu, A review of generalized zero-shot learning methods, IEEE transactions
    on pattern analysis and machine intelligence (2022).'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] F. Pourpanah, M. Abdar, Y. Luo, X. Zhou, R. Wang, C. P. Lim, X.-Z. Wang,
    Q. J. Wu, 一种广义零样本学习方法的综述，IEEE模式分析与机器智能学报 (2022)。'
- en: '[167] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, A. Joulin,
    Emerging properties in self-supervised vision transformers, in: Proceedings of
    the IEEE/CVF international conference on computer vision, 2021, pp. 9650–9660.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, A.
    Joulin, 自监督视觉变换器中的新兴特性，发表于：IEEE/CVF国际计算机视觉会议论文集，2021年，第9650–9660页。'
- en: '[168] X. Wang, R. Girdhar, S. X. Yu, I. Misra, Cut and learn for unsupervised
    object detection and instance segmentation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2023, pp. 3124–3134.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] X. Wang, R. Girdhar, S. X. Yu, I. Misra, 通过剪切和学习进行无监督对象检测和实例分割，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2023年，第3124–3134页。'
- en: '[169] D. Kim, S. Woo, J.-Y. Lee, I. S. Kweon, Video panoptic segmentation,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2020, pp. 9859–9868.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] D. Kim, S. Woo, J.-Y. Lee, I. S. Kweon, 视频全景分割，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2020年，第9859–9868页。'
- en: '[170] S. Qiao, Y. Zhu, H. Adam, A. Yuille, L.-C. Chen, Vip-deeplab: Learning
    visual perception with depth-aware video panoptic segmentation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp.
    3997–4008.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] S. Qiao, Y. Zhu, H. Adam, A. Yuille, L.-C. Chen, Vip-deeplab: 通过深度感知视频全景分割学习视觉感知，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2021年，第3997–4008页。'
- en: '[171] X. Li, W. Zhang, J. Pang, K. Chen, G. Cheng, Y. Tong, C. C. Loy, Video
    k-net: A simple, strong, and unified baseline for video segmentation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp.
    18847–18857.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] X. Li, W. Zhang, J. Pang, K. Chen, G. Cheng, Y. Tong, C. C. Loy, Video
    k-net: 一种简单、强大且统一的视频分割基线，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第18847–18857页。'
- en: '[172] A. Athar, A. Hermans, J. Luiten, D. Ramanan, B. Leibe, Tarvis: A unified
    approach for target-based video segmentation, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2023, pp. 18738–18748.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] A. Athar, A. Hermans, J. Luiten, D. Ramanan, B. Leibe, Tarvis: 基于目标的视频分割统一方法，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2023年，第18738–18748页。'
- en: '[173] O. Thawakar, S. Narayan, H. Cholakkal, R. M. Anwer, S. Khan, J. Laaksonen,
    M. Shah, F. S. Khan, Video instance segmentation in an open-world, arXiv preprint
    arXiv:2304.01200 (2023).'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] O. Thawakar, S. Narayan, H. Cholakkal, R. M. Anwer, S. Khan, J. Laaksonen,
    M. Shah, F. S. Khan, 开放世界中的视频实例分割，arXiv预印本 arXiv:2304.01200 (2023)。'
- en: '[174] H. Wang, S. Wang, C. Yan, X. Jiang, X. Tang, Y. Hu, W. Xie, E. Gavves,
    Towards open-vocabulary video instance segmentation, arXiv preprint arXiv:2304.01715
    (2023).'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] H. Wang, S. Wang, C. Yan, X. Jiang, X. Tang, Y. Hu, W. Xie, E. Gavves,
    面向开放词汇的视频实例分割，arXiv预印本 arXiv:2304.01715 (2023)。'
- en: '[175] P. Guo, T. Huang, P. He, X. Liu, T. Xiao, Z. Chen, W. Zhang, Openvis:
    Open-vocabulary video instance segmentation, arXiv preprint arXiv:2305.16835 (2023).'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] P. Guo, T. Huang, P. He, X. Liu, T. Xiao, Z. Chen, W. Zhang, Openvis:
    开放词汇的视频实例分割，arXiv预印本 arXiv:2305.16835 (2023)。'
- en: '[176] N. Jaafar, Z. Lachiri, Multimodal fusion methods with deep neural networks
    and meta-information for aggression detection in surveillance, Expert Systems
    with Applications 211 (2023) 118523.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] N. Jaafar, Z. Lachiri, 使用深度神经网络和元信息进行攻击检测的多模态融合方法，专家系统与应用 211 (2023)
    118523。'
- en: '[177] P. Xu, X. Zhu, D. A. Clifton, Multimodal learning with transformers:
    A survey, IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] P. Xu, X. Zhu, D. A. Clifton, 基于变换器的多模态学习：综述，IEEE模式分析与机器智能学报 (2023)。'
- en: '[178] A. Botach, E. Zheltonozhskii, C. Baskin, End-to-end referring video object
    segmentation with multimodal transformers, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2022, pp. 4985–4995.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] A. Botach, E. Zheltonozhskii, C. Baskin, 基于多模态变换器的端到端视频目标分割，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2022年，第4985–4995页。'
- en: '[179] R. Chen, S. Liu, J. Chen, B. Guo, F. Zhang, Vlkp: Video instance segmentation
    with visual-linguistic knowledge prompts, in: ICASSP 2023-2023 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2023, pp.
    1–5.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] R. Chen, S. Liu, J. Chen, B. Guo, F. Zhang, Vlkp: 利用视觉-语言知识提示进行视频实例分割，发表于：ICASSP
    2023-2023 IEEE国际声学、语音与信号处理会议 (ICASSP)，IEEE，2023年，第1–5页。'
- en: '[180] X. Li, J. Wang, X. Xu, B. Raj, Y. Lu, Online video instance segmentation
    via robust context fusion, arXiv preprint arXiv:2207.05580 (2022).'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] X. Li, J. Wang, X. Xu, B. Raj, Y. Lu, 在线视频实例分割通过鲁棒的上下文融合，arXiv 预印本 arXiv:2207.05580
    (2022)。'
- en: '[181] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang,
    O. Ashual, O. Gafni, et al., Make-a-video: Text-to-video generation without text-video
    data, arXiv preprint arXiv:2209.14792 (2022).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang,
    O. Ashual, O. Gafni, 等，Make-a-video: 文本到视频生成，无需文本-视频数据，arXiv 预印本 arXiv:2209.14792
    (2022)。'
- en: '[182] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
    S. Whitehead, A. C. Berg, W.-Y. Lo, et al., Segment anything, arXiv preprint arXiv:2304.02643
    (2023).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T.
    Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, 等，Segment anything，arXiv 预印本 arXiv:2304.02643
    (2023)。'
