- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:48:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:48:49
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2112.14911] A Survey of Deep Learning Techniques for Dynamic Branch Prediction'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2112.14911] 深度学习技术在动态分支预测中的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2112.14911](https://ar5iv.labs.arxiv.org/html/2112.14911)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2112.14911](https://ar5iv.labs.arxiv.org/html/2112.14911)
- en: A Survey of Deep Learning Techniques for Dynamic Branch Prediction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习技术在动态分支预测中的调查
- en: 'Rinu Joseph Rinu Joseph Department of Electrical and Computer Engineering,
    College of Engineering and University of Texas at San Antonio, San Antonio Texas
    USA, e-mail: rinu.joseph@my.utsa.edu'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Rinu Joseph Rinu Joseph，德克萨斯大学圣安东尼奥分校工程学院电气与计算机工程系，圣安东尼奥，德克萨斯，美国，电子邮件：rinu.joseph@my.utsa.edu
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Branch prediction is an architectural feature that speeds up the execution of
    branch instruction on pipeline processors and reduces the cost of branching. Recent
    advancements of Deep Learning (DL) in the post Moore’s Law era is accelerating
    areas of automated chip design, low-power computer architectures, and much more.
    Traditional computer architecture design and algorithms could benefit from dynamic
    predictors based on deep learning algorithms which learns from experience by optimizing
    its parameters on large number of data. In this survey paper, we focus on traditional
    branch prediction algorithms, analyzes its limitations, and presents a literature
    survey of how deep learning techniques can be applied to create dynamic branch
    predictors capable of predicting conditional branch instructions. Prior surveys
    in this field [[1](#bib.bib1)] focus on dynamic branch prediction techniques based
    on neural network perceptrons. We plan to improve the survey based on latest research
    in DL and advanced Machine Learning (ML) based branch predictors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 分支预测是一种体系结构特性，它加速了在流水线处理器上执行分支指令的速度，并减少了分支的成本。深度学习（DL）在摩尔定律后时代的最新进展正在加速自动化芯片设计、低功耗计算机体系结构等领域的发展。传统的计算机体系结构设计和算法可以从基于深度学习算法的动态预测器中受益，这些算法通过在大量数据上优化其参数来从经验中学习。在这篇调查论文中，我们关注传统的分支预测算法，分析其局限性，并介绍了如何将深度学习技术应用于创建能够预测条件分支指令的动态分支预测器的文献综述。之前的调查[[1](#bib.bib1)]关注于基于神经网络感知器的动态分支预测技术。我们计划基于最新的DL研究和先进的机器学习（ML）基于分支预测器的研究来改进这一调查。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep Learning, Machine Learning, Computer Architecture, Branch Prediction
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，机器学习，计算机体系结构，分支预测
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Microprocessors with superscalar and very long instruction word (VLIW) architecture
    adapts instruction level parallelism (ILP) to achieve high performance. ILP executes
    multiple instructions of a program concurrently in one CPU cycle (clock cycle)
    to accelerate memory references and computation [[2](#bib.bib2)] which can result
    in improved performance of the processors. But one of the limitations in exploiting
    ILP is the high frequency of branching instructions in a program. According to
    Haque et al. in [[3](#bib.bib3)] 15% to 25% of all instructions in a program are
    branch instructions. Three types of branch instructions are 1) jump, 2) call and
    3) return. Each of these instructions can be classified as conditional and unconditional
    branch instructions. Branch instructions can interrupt the sequential flow of
    instruction execution and instead of incrementing the instruction pointer (IP)
    to next instruction, IP is loaded with the address of an instruction in a specified
    location in the memory. This interruption in execution flow can result in control
    hazards which causes delay by filling up the pipeline with instructions which
    need to be discarded subsequently.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 超标量和非常长指令字（VLIW）架构的微处理器采用指令级并行（ILP）以实现高性能。ILP在一个CPU周期（时钟周期）内同时执行程序中的多条指令，以加速内存引用和计算[[2](#bib.bib2)]，这可以改善处理器的性能。但利用ILP的一个局限性是程序中分支指令的高频率。根据Haque等人[[3](#bib.bib3)]的研究，程序中15%到25%的指令是分支指令。分支指令有三种类型：1）跳转，2）调用和3）返回。每种指令可以被分类为条件和无条件分支指令。分支指令可以打断指令执行的顺序流，且不会将指令指针（IP）递增到下一条指令，而是将IP加载到内存中指定位置的指令地址。执行流的这种中断可能导致控制冒险，导致延迟，因为管道被填充了随后需要丢弃的指令。
- en: To mitigate the issues caused by branching instructions in the code, an approach
    known as Branch Prediction (BP) is implemented in modern architectural designs.
    Branch prediction increases the instruction execution speed on processors by minimizing
    pipeline penalties. In BP, before the completing the execution of a branch instruction
    the outcome is predicted (whether the branch is taken or not-taken) and then starts
    executing the instructions from the predicted path prematurely. If prediction
    of BP process is correct then it is referred as branch hit. One of the advantage
    of branch hit is the effective usage of instruction pipeline. if the speculation
    is wrong (branch miss or branch penalty) the executed instructions from the wrongly
    predicted path must be discarded and instructions from right path need to be fetched
    to the pipeline. This wrong prediction can causes a branch overhead which will
    slowdown the process [[4](#bib.bib4)],[[5](#bib.bib5)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解代码中分支指令引发的问题，现代架构设计中实现了一种称为分支预测（BP）的方法。分支预测通过最小化流水线惩罚来提高处理器上的指令执行速度。在分支预测中，在完成分支指令的执行之前，预测结果（分支是否被采取）并提前开始执行预测路径上的指令。如果分支预测过程的预测是正确的，则称为分支命中。分支命中的一个优点是指令流水线的有效利用。如果预测错误（分支未命中或分支惩罚），则需要丢弃从错误预测路径上执行的指令，并从正确路径上获取指令进入流水线。这种错误预测可能导致分支开销，从而减慢处理过程[[4](#bib.bib4)],[[5](#bib.bib5)]。
- en: Emerging technologies like machine learning and deep learning played a vital
    role in the advancements in the field of computer architecture. Deep Learning,
    a branch of Artificial Intelligence (AI), uses representation learning methods
    to process the raw data to discover representations required for classification
    problems. DL comes with multiple levels of representations that are obtained by
    the composition of non-linear modules that transform representation at each level.
    With these composition of transformations complex functions can be learned. DL
    is applicable to domains like science, business and government etc [[6](#bib.bib6)].
    Machine learning another sub-field of AI, enables computers and machines to learn
    the structure of input data and enable them to make decisions based on the complex
    data patterns without programming them explicitly [[7](#bib.bib7)],[[8](#bib.bib8)].
    The advancements in the field of ML and DL over the past few decades resulted
    in building more accurate systems. The increased applicability of DL models on
    real-world problems lead to a demand for high system performance [[9](#bib.bib9)].
    Researchers, system engineers and computer architects continue to design appropriate
    hardware which meets the computational requirement for training deep neural networks
    and ML algorithms on large dataset [[10](#bib.bib10)]. While the efficiency of
    computer systems are improved for training purposes, there has been some research
    done on how to utilize ML/DL techniques to enhance the system performance. Some
    of the research works were focused on the area of cache replacement, CPU scheduling,
    branch prediction and workload performance prediction on an x86 processor [[11](#bib.bib11)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 新兴技术如机器学习和深度学习在计算机架构领域的进步中发挥了重要作用。深度学习，作为人工智能（AI）的一个分支，使用表示学习方法来处理原始数据，以发现分类问题所需的表示。深度学习具有多层次的表示，这些表示通过非线性模块的组合获得，这些模块在每一层转换表示。通过这些转换组合，可以学习复杂的函数。深度学习适用于科学、商业和政府等领域[[6](#bib.bib6)]。机器学习，作为人工智能的另一个子领域，使计算机和机器能够学习输入数据的结构，并基于复杂的数据模式做出决策，而无需明确编程[[7](#bib.bib7)],[[8](#bib.bib8)]。过去几十年中，机器学习和深度学习领域的进展导致了更精确系统的构建。深度学习模型在现实世界问题上的应用增加了对高系统性能的需求[[9](#bib.bib9)]。研究人员、系统工程师和计算机架构师继续设计满足训练深度神经网络和机器学习算法的大型数据集计算要求的适当硬件[[10](#bib.bib10)]。虽然计算机系统在训练目的上的效率得到了提高，但也有一些研究致力于如何利用机器学习/深度学习技术来提升系统性能。部分研究工作集中在x86处理器上的缓存替换、CPU调度、分支预测和工作负载性能预测领域[[11](#bib.bib11)]。
- en: This research report on various ML/DL techniques for branch prediction is organised
    as follows. The following section (section 2) gives a background information about
    branch prediction and different branch prediction strategies and its functionalities.
    In section 3 we analyze various ML/DL methodologies proposed for branch prediction.
    Section 5 discusses about the current limitations and scope of new advancements.
    This report is concluded in section 6.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究报告组织了关于分支预测的各种机器学习/深度学习技术。以下部分（第2节）提供了有关分支预测及其不同策略和功能的背景信息。在第3节中，我们分析了为分支预测提出的各种机器学习/深度学习方法。第5节讨论了当前的局限性和新进展的范围。本报告在第6节中做出总结。
- en: II Branch Prediction Strategies
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 分支预测策略
- en: 'To reduce the delay caused by the branch instructions in instruction fetching,
    instruction decoding and instruction execution, the outcome of branch i.e whether
    the branch will be taken or not, if it is take what is the direction of branch
    is predicted before the branch is executed and decision is made. This prediction
    is known as Branch Prediction mechanism [[12](#bib.bib12)]. The two main techniques
    of branch prediction are:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少由分支指令引起的指令获取、指令解码和指令执行中的延迟，分支的结果，即分支是否会被取，若取则分支的方向，会在分支执行和决策之前进行预测。这种预测称为分支预测机制[[12](#bib.bib12)]。分支预测的两种主要技术是：
- en: II-A Static Branch Prediction
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 静态分支预测
- en: It is software based prediction strategy. This approach assumes either the branch
    is always take on branch is always not taken. This is a very simple and cost effective
    prediction technique. Since this approach does not maintains a history table of
    previous branch decisions it takes only less energy for instruction execution
    when compared to other branch prediction techniques [[13](#bib.bib13)]. Following
    are the schemes of static (software-based) branch prediction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于软件的预测策略。这种方法假设分支要么总是被取，要么总是不被取。这是一种非常简单且成本效益高的预测技术。由于这种方法不维护先前分支决策的历史表，因此与其他分支预测技术相比，它在指令执行时消耗的能量较少[[13](#bib.bib13)]。以下是静态（基于软件）分支预测的方案。
- en: II-A1 Single direction prediction
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 单方向预测
- en: In this scheme, the direction of all branch instructions will be in a similar
    way whether or not the branch is taken. This approach has simple implementation
    but low accuracy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方案中，无论分支是否被取，所有分支指令的方向都将以相似的方式进行。这种方法实现简单，但准确性低。
- en: II-A2 Backward taken forward not taken (BTFT)
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 向后预测被取，向前预测不被取（BTFT）
- en: The target address of backward branch is lower than the current address. This
    approach presumes that all the backward loops are taken and forward branches are
    not taken.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 向后分支的目标地址低于当前地址。这种方法假设所有向后循环都被取，向前分支都不被取。
- en: II-A3 Program based prediction
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 基于程序的预测
- en: The prediction of direction of the branch instructions is based on some heuristics
    which are derived from opcode, operand and information of branch instructions
    which are executed.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对分支指令方向的预测基于一些从操作码、操作数和已执行的分支指令信息中得出的启发式方法。
- en: II-A4 Profile-based branch prediction
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A4 基于配置文件的分支预测
- en: In this scheme of static branch prediction the information from program’s previous
    execution is used to determine the direction of branch instruction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种静态分支预测方案中，利用程序的先前执行信息来确定分支指令的方向。
- en: II-B Dynamic Branch Prediction
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 动态分支预测
- en: 'This hardware-based prediction strategy is based on the history of branch executions.
    Dynamic branch predictions are complex compared to static prediction approach
    but has high accuracy rate. The performance is based on the prediction accuracy
    and penalty rate. Different dynamic (hardware-based) branch prediction strategies
    are:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于硬件的预测策略基于分支执行的历史记录。与静态预测方法相比，动态分支预测更为复杂，但具有较高的准确率。性能取决于预测准确性和惩罚率。不同的动态（基于硬件）分支预测策略包括：
- en: II-B1 One-bit branch prediction buffer
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 一位分支预测缓冲区
- en: In this prediction mechanism, if one assumption goes wrong branch predictor
    changes its prediction. i.e if a branch instruction is predicted as taken but
    actually the instruction is not taken, then next time predictor hardware assumes
    the instruction to be no taken.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种预测机制中，如果一个假设错误，分支预测器会改变其预测。即如果分支指令被预测为取，但实际指令没有取，那么下次预测器硬件会假设指令为不取。
- en: II-B2 Two-bit branch prediction buffer
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 两位分支预测缓冲区
- en: This prediction method is similar to one-bit branch prediction buffer. The difference
    is that assumption is changed only after two consecutive miss-predictions [[14](#bib.bib14)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种预测方法类似于单比特分支预测缓冲区。不同之处在于，仅在出现连续两个错误预测后，假设才会改变 [[14](#bib.bib14)]。
- en: II-B3 Correlation-based branch predictor
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B3 基于相关性的分支预测器
- en: It is also known as two-level branch predictor. Accuracy of one-bit and two-bit
    branch prediction buffers are low. In correlating branch prediction accuracy is
    significantly improved by considering behaviour of recently executed branches
    for making prediction. It uses previously fetched branch target address, a local
    history table and a local prediction table to make accurate predictions for branch
    instructions [[14](#bib.bib14)].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 也被称为两级分支预测器。单比特和双比特分支预测缓冲区的准确性较低。在相关的分支预测中，通过考虑最近执行的分支的行为来显著提高预测准确性。它使用先前获取的分支目标地址、本地历史表和本地预测表来对分支指令进行准确预测
    [[14](#bib.bib14)]。
- en: III Deep Learning/Machine Learning for Dynamic Branch Prediction
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 深度学习/机器学习在动态分支预测中的应用
- en: III-A Perceptron Based Dynamic Branch Prediction
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 基于感知机的动态分支预测
- en: In 1957 Frank Rosenblatt introduced perceptrons which was inspired by learning
    abilities of a biological neuron [[15](#bib.bib15)]. Perceptron is a single layer
    neural network which consists of a processor that takes multiple inputs with weights
    and biases to generate single output. As illustrated in Figure [1](#S3.F1 "Figure
    1 ‣ III-A Perceptron Based Dynamic Branch Prediction ‣ III Deep Learning/Machine
    Learning for Dynamic Branch Prediction ‣ A Survey of Deep Learning Techniques
    for Dynamic Branch Prediction") Perceptrons work by multiplying inputs with weights
    and weighted sum of all multiplied values are taken and applied to an activation
    function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 1957 年，Frank Rosenblatt 引入了感知机，这一概念受到生物神经元学习能力的启发 [[15](#bib.bib15)]。感知机是一个单层神经网络，包括一个处理器，它接受带有权重和偏置的多个输入以生成单一输出。如图
    [1](#S3.F1 "图 1 ‣ III-A 基于感知机的动态分支预测 ‣ III 深度学习/机器学习在动态分支预测中的应用 ‣ 动态分支预测的深度学习技术概述")
    所示，感知机通过将输入与权重相乘来工作，所有相乘值的加权和会被计算，并应用于激活函数。
- en: '![Refer to caption](img/784d04d838220830c40797e418dfbed8.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/784d04d838220830c40797e418dfbed8.png)'
- en: 'Figure 1: Perceptron'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 感知机'
- en: Jimenez et al. [[16](#bib.bib16)] proposed a perceptron based branch prediction
    method which can be used as an alternative to traditional two-bit branch prediction
    buffers. The proposed perceptron predictor is the first dynamic predictor to use
    neural networks for branch prediction.The predictor make use of long branch history
    which is made possible by the hardware resources which can scale the history length
    linearly to improve the accuracy of prediction method. Basically the design space
    of two-level branch predictor (correlation-based branch predictor) is explored
    and instead of pattern history table, table with perceptrons are used.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Jimenez 等人 [[16](#bib.bib16)] 提出了基于感知机的分支预测方法，这可以作为传统两位分支预测缓冲区的替代方案。提出的感知机预测器是第一个使用神经网络进行分支预测的动态预测器。该预测器利用了长分支历史，这得益于硬件资源，硬件可以线性扩展历史长度，以提高预测方法的准确性。基本上，探索了两级分支预测器（基于相关性的分支预测器）的设计空间，而不是使用模式历史表，而是使用了感知机表。
- en: '![Refer to caption](img/6f2f4aeec8a1c7603fcb14b3ed6214ad.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6f2f4aeec8a1c7603fcb14b3ed6214ad.png)'
- en: 'Figure 2: Perceptron Predictor Block Diagram [[16](#bib.bib16)]'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 感知机预测器框图 [[16](#bib.bib16)]'
- en: In this study the branch behaviour is classified as linearly separable or linearly
    inseparable. Perceptron predictor’s performance was better with linearly separable
    branches. Decisions made by perceptrons are easy to understand when compared to
    complex neural networks. Another factor is choosing perceptron as predictor is
    because of efficient hardware implementation. Other popular neural architectures
    like ADALINE and Hebb were used in this study but performance was low due to low
    hardware efficiency and low accuracy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，分支行为被分类为线性可分或线性不可分。感知机预测器在处理线性可分分支时表现更好。与复杂的神经网络相比，感知机做出的决策更易于理解。另一个选择感知机作为预测器的因素是其高效的硬件实现。其他流行的神经网络架构如
    ADALINE 和 Hebb 在这项研究中也被使用，但由于硬件效率低和准确性差，性能较差。
- en: Figure [2](#S3.F2 "Figure 2 ‣ III-A Perceptron Based Dynamic Branch Prediction
    ‣ III Deep Learning/Machine Learning for Dynamic Branch Prediction ‣ A Survey
    of Deep Learning Techniques for Dynamic Branch Prediction") is the block diagram
    for proposed perceptron predictor. The system processor maintains a table of perceptrons
    in SRAM like two-bit counters. Based on number of weights the and budget for hardware
    the number of perceptrons in the table is fixed. When a branch instruction is
    fetched an index is produced in the perceptrons table by hashing the branch address
    and the perceptron at this index is moved to a vector register of weights (In
    this experiment signed weights are used).The dot product of weight and global
    history register is computed to produce output. If the output value is negative
    then the branch is predicted to be not taken or if the value is positive then
    branch is taken. When actual outcome is known, the weights are updated based on
    the actual result and predicted value by the training algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S3.F2 "Figure 2 ‣ III-A Perceptron Based Dynamic Branch Prediction ‣ III
    Deep Learning/Machine Learning for Dynamic Branch Prediction ‣ A Survey of Deep
    Learning Techniques for Dynamic Branch Prediction")是提出的感知机预测器的框图。系统处理器在SRAM中维护一个感知机表，类似于两位计数器。根据权重的数量和硬件预算，表中的感知机数量是固定的。当提取分支指令时，通过对分支地址进行哈希，生成感知机表中的一个索引，并将该索引的感知机移动到权重向量寄存器中（在此实验中使用了有符号权重）。计算权重和全局历史寄存器的点积以产生输出。如果输出值为负，则预测分支未被采取；如果值为正，则预测分支被采取。当实际结果已知时，权重会根据实际结果和预测值通过训练算法进行更新。
- en: To achieve best performance three parameters are considered and tuned, 1) History
    Length, 2) Weight Representations, 3) Threshold. The increase in history length
    yielded more accuracy in predictions. history lengths from 12 to 62 reported best
    outcome for this training algorithms. As mentioned earlier the weights are signed
    integers and they simplify the architecture design.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳性能，考虑并调整了三个参数：1) 历史长度，2) 权重表示，3) 阈值。增加历史长度提高了预测的准确性。历史长度从12到62为此训练算法报告了最佳结果。如前所述，权重为有符号整数，这简化了架构设计。
- en: '![Refer to caption](img/0a9e46190bc21f22e74380d17f754ff4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0a9e46190bc21f22e74380d17f754ff4.png)'
- en: 'Figure 3: Performance vs History Length [[16](#bib.bib16)]'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：性能与历史长度 [[16](#bib.bib16)]
- en: '![Refer to caption](img/e1e45af010f170cf7d8aa335979bed01.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e1e45af010f170cf7d8aa335979bed01.png)'
- en: 'Figure 4: Performance vs Linear Separability [[16](#bib.bib16)]'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：性能与线性可分性 [[16](#bib.bib16)]
- en: In [[16](#bib.bib16)] two dynamic predictors gshare and bi-mode were chosen
    to compare with perceptron predictor. As seen in the figure [3](#S3.F3 "Figure
    3 ‣ III-A Perceptron Based Dynamic Branch Prediction ‣ III Deep Learning/Machine
    Learning for Dynamic Branch Prediction ‣ A Survey of Deep Learning Techniques
    for Dynamic Branch Prediction"), when longer histories are considered gshare shows
    poor performance and perceptron predictor improves the performance. This paper
    is concluded by stating that although perceptrons are not capable of learning
    linearly inseparable behaviour of branches (Figure [4](#S3.F4 "Figure 4 ‣ III-A
    Perceptron Based Dynamic Branch Prediction ‣ III Deep Learning/Machine Learning
    for Dynamic Branch Prediction ‣ A Survey of Deep Learning Techniques for Dynamic
    Branch Prediction")) and they are complex when compared to two-bit counters, perceptrons
    can use long history tables without any exponential resources and also achieve
    a low misprediction rates and higher accuracy when compared to existing dynamic
    predictors.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[16](#bib.bib16)]中，选择了两种动态预测器gshare和bi-mode来与感知机预测器进行比较。如图[3](#S3.F3 "Figure
    3 ‣ III-A Perceptron Based Dynamic Branch Prediction ‣ III Deep Learning/Machine
    Learning for Dynamic Branch Prediction ‣ A Survey of Deep Learning Techniques
    for Dynamic Branch Prediction")所示，当考虑较长的历史时，gshare表现较差，而感知机预测器则提高了性能。本文总结指出，尽管感知机无法学习分支的线性不可分行为（图[4](#S3.F4
    "Figure 4 ‣ III-A Perceptron Based Dynamic Branch Prediction ‣ III Deep Learning/Machine
    Learning for Dynamic Branch Prediction ‣ A Survey of Deep Learning Techniques
    for Dynamic Branch Prediction")），并且与两位计数器相比较复杂，但感知机可以使用长历史表而不需要任何指数资源，并且在现有动态预测器中实现了较低的误预测率和更高的准确性。
- en: Akkary et al. in [[17](#bib.bib17)] proposed a perceptron based branch confidence
    estimator to reduce the misprediction of branch instructions. One of the factor
    that contribute to high performance of a system is execution of unresolved branches
    based on some speculations made by branch prediction techniques. Mispredicted
    executions negatively impact the system by taking up the resources, creating stalls
    in execution and also affect the power of the system because more instructions
    are executed when there is mis-speculation occurs. In deeper pipeline processors,
    pipelining gating plays a vital role in reducing wasted executions based on wrong
    speculative decisions made by predictors. With perceptron-based branch confidence
    estimator predictions provide multi-valued outputs. i.e the classify the branch
    instructions into “strongly low confident” and “weakly low confident”.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Akkary等人在[[17](#bib.bib17)]中提出了一种基于感知器的分支置信度估计器，以减少分支指令的错误预测。系统高性能的一个因素是根据分支预测技术做出的某些推测来执行未解决的分支。错误预测的执行对系统产生负面影响，占用资源、造成执行停滞，并且由于错误推测，系统的功耗也会增加，因为需要执行更多的指令。在更深的流水线处理器中，流水线门控在减少由于预测器做出的错误推测决定而浪费的执行中发挥着至关重要的作用。通过基于感知器的分支置信度估计器，预测提供多值输出，即将分支指令分类为“强低置信度”和“弱低置信度”。
- en: With confidence estimation, instruction fetching can be stalled when low confidence
    unresolved branch instructions are encountered. Estimation is correct when low
    confident branch is actually mispredicted. This correct estimation can reduce
    the execution wastage. When estimation is wrong (i.e low confident branch is correctly
    predicted) there will be a pipeline stall which can lead to performance loss.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过置信度估计，当遇到低置信度未解决的分支指令时，可以暂停指令获取。当低置信度分支实际上被错误预测时，估计是正确的。这种正确的估计可以减少执行浪费。当估计错误（即低置信度分支被正确预测）时，将会有一个流水线停滞，这可能导致性能损失。
- en: '![Refer to caption](img/7718a12d565070d0576f3439052bdd76.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/7718a12d565070d0576f3439052bdd76.png)'
- en: 'Figure 5: Perceptron based confidence estimator [[17](#bib.bib17)]'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：基于感知器的置信度估计器 [[17](#bib.bib17)]
- en: As seen in the figure [5](#S3.F5 "Figure 5 ‣ III-A Perceptron Based Dynamic
    Branch Prediction ‣ III Deep Learning/Machine Learning for Dynamic Branch Prediction
    ‣ A Survey of Deep Learning Techniques for Dynamic Branch Prediction"), index
    of array of perceptrons are based on conditional branch memory address. In this
    method global history register which contains taken branch represented as 1 and
    not-taken branch represented as -1 is passed as a input vector tho the perceptron.
    Dot product of weight vector in perceptron and this input vector is used to generate
    the output. As mentioned earlier the output of this approach is multi-valued.
    i.e, branches can be further classified as low or high confidence. The generated
    output is then compared against a specific threshold $\lambda$ . When output is
    larger then prediction can be wrong or if the output is smaller than the threshold
    then the prediction is more likely to be correct.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，数组感知器的索引基于条件分支的内存地址。在这种方法中，包含已取分支（表示为1）和未取分支（表示为-1）的全局历史寄存器作为输入向量传递给感知器。感知器中的权重向量与该输入向量的点积用于生成输出。如前所述，这种方法的输出是多值的，即分支可以进一步分类为低置信度或高置信度。生成的输出与特定阈值$\lambda$进行比较。当输出值较大时，预测可能会出错；如果输出值小于阈值，则预测更可能是正确的。
- en: Specificity and Predictive value of a negative test (PVN) are the primary metrics
    used for perceptron based branch confidence estimator. Specificity is the percentage
    of mispredicted branches which were classified as low confidence by branch confidence
    estimator. PVN is the probability of correctly mispredicted low confidence branches
    (as a measure of accuracy).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 负测试的特异性和预测值（PVN）是用于基于感知器的分支置信度估计器的主要指标。特异性是被错误预测为低置信度的分支的百分比。PVN是正确错误预测的低置信度分支的概率（作为准确度的衡量）。
- en: III-B Branch Prediction with Convolutional Neural Networks and Deep Belief Networks
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 使用卷积神经网络和深度置信网络进行分支预测
- en: Tarsa et al. in [[18](#bib.bib18)] says that although branch prediction attained
    99% accuracy in prediction static branches, Hard-to -predict(H2P) branches are
    still a major bottleneck in the system performance. Authors suggested machine
    learning approach to that could work along with standard branch predictors to
    predict H2P. They developed helper predictors mainly convolutional neural network
    based helpers to improve the process of pattern matching in global history data.
    Unlike the TAGE and preceptron based BPs, CNN based helper predictors are deployed
    offline.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Tarsa等人在[[18](#bib.bib18)]中提到，尽管分支预测在预测静态分支时达到了99%的准确率，但难以预测的（H2P）分支仍然是系统性能的主要瓶颈。作者建议可以采用机器学习方法，与标准分支预测器一起工作，以预测H2P。他们开发了主要基于卷积神经网络的辅助预测器，以改进全球历史数据中的模式匹配过程。与基于TAGE和感知器的BP不同，基于CNN的辅助预测器是离线部署的。
- en: This paper illustrated two examples of program in which the data dependency
    of iteration counts creates H2Ps. Variations available in data can confuse the
    state-of-the-art predictors but are handled by convolutional filters. The global
    history (instruction pointer and direction of previous branches) are converted
    to vector representations. The input to CNN is generated by concatenating the
    vectors to from a matrix for global history. CNN calculates the dot product of
    data vector, weight vector,filter and bias to perform the pattern matching. Score
    computer by each filter against the each history matrix column are passed to filter
    in the linear layer (similar to perceptron layer) to match the output of layer
    1 in two-layer CNN. If the output of layer 2 is ¿ 0 then the branch is taken or
    it is not-taken. the first layer (layer 1) in CNN identifies which instruction
    pointer and direction in history table correlates with the h2P’s direction and
    layer 2 identifies which position in history contribute to branch prediction process.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文展示了两个程序示例，其中迭代计数的数据依赖性会产生H2P。数据中的变异可能会困扰最先进的预测器，但卷积滤波器能够处理这些变异。全球历史（指令指针和前一个分支的方向）被转换为向量表示。CNN的输入是通过将这些向量连接成一个全球历史矩阵生成的。CNN通过计算数据向量、权重向量、滤波器和偏差的点积来执行模式匹配。每个滤波器对每个历史矩阵列计算的得分被传递到线性层（类似于感知层）以匹配两层CNN中层1的输出。如果层2的输出为¿
    0，则分支被采纳，否则不采纳。CNN的第一层（层1）识别历史表中哪个指令指针和方向与H2P的方向相关联，第二层识别历史中的哪个位置有助于分支预测过程。
- en: This paper concluded by stating the how two-layer CNN along with the branch
    predictors can reduce the positional variants in global history data which can
    lead to branch mispredictions. They also provided some insights like passing more
    data like register values to network to improve the accuracy for future development
    of ML-based branch predictors.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文总结了如何通过使用两层CNN和分支预测器来减少全球历史数据中的位置变异，从而减少分支错误预测。他们还提供了一些见解，比如传递更多的数据如寄存器值给网络，以提高未来ML基础的分支预测器的准确性。
- en: Based on the work [[18](#bib.bib18)] by Tarsa et al, Zangeneh et al. developed
    a convolutional neural network called as BranchNet [[19](#bib.bib19)] for hard-to-predict
    branches in 2020\. As mentioned in [[18](#bib.bib18)] traditional branch predictors
    are updated in run-time which makes it difficult to find the correlations in branch
    history. Exisiting predictors cannot find correlations from a noisy history while
    CNN can identify those correlations correctly. This capability of CNN improves
    the prediction rates but requires high computational cost and large dataset for
    training. BranchNet can be trained offline and this models are attached to the
    program so that the predictors can use this model in runtime. Authors [[19](#bib.bib19)]
    popose CNN architecture in two-ways. 1)Using geometric history length as input
    to the model, 2)Sum-pooling for the compression of global history information.
    The designed CNN architecture is storage efficient and same latency as TAGE-SC-L.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Tarsa等人的工作[[18](#bib.bib18)]，Zangeneh等人于2020年开发了一种称为BranchNet的卷积神经网络[[19](#bib.bib19)]，用于难以预测的分支。如[[18](#bib.bib18)]中提到的，传统的分支预测器在运行时更新，这使得难以找到分支历史中的相关性。现有的预测器无法从噪声历史中发现相关性，而CNN可以正确识别这些相关性。CNN的这种能力提高了预测率，但需要高计算成本和大量的数据集进行训练。BranchNet可以离线训练，并将这些模型附加到程序上，以便预测器可以在运行时使用这些模型。作者[[19](#bib.bib19)]提出了两种CNN架构。1)使用几何历史长度作为模型的输入，2)通过总池化压缩全球历史信息。设计的CNN架构在存储上高效，延迟与TAGE-SC-L相同。
- en: '![Refer to caption](img/7ad4c96034749d6d83659989cf8bc6ac.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7ad4c96034749d6d83659989cf8bc6ac.png)'
- en: 'Figure 6: Big-BranchNet Network Architecture [[19](#bib.bib19)]'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：Big-BranchNet 网络架构 [[19](#bib.bib19)]
- en: '![Refer to caption](img/b13aefad352ee76fb29859236f457289.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b13aefad352ee76fb29859236f457289.png)'
- en: 'Figure 7: Mini-BranchNet Network Architecture [[19](#bib.bib19)]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：Mini-BranchNet 网络架构 [[19](#bib.bib19)]
- en: In this paper two-variants of BranchNet are proposed. *Big-BranchNet*, consists
    of two fully-connected layers and five feature extraction layers (slice) (see
    [6](#S3.F6 "Figure 6 ‣ III-B Branch Prediction with Convolutional Neural Networks
    and Deep Belief Networks ‣ III Deep Learning/Machine Learning for Dynamic Branch
    Prediction ‣ A Survey of Deep Learning Techniques for Dynamic Branch Prediction")).
    Slice consists of embedding layer, sum-pooling layer and convolutional layer for
    feature extraction from branch history. Each slice works on different history
    length to form a geometric series. Concatenated output from all slices are passed
    to FC layer for prediction output. Authors do not recommend the practical use
    of this variant as a branch predictor as this is a software model. Another variant
    of BranchNet is *mini-BranchNet* is co-designed with an inference engine. The
    architecture of min-BranchNet is similar to big-BranchNet except that it has knobs
    that can be tuned to reduce the latency and storage. Mini-BranchNet can work as
    a branch predictor.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了两种 BranchNet 的变体。*Big-BranchNet* 由两个全连接层和五个特征提取层（切片）组成（见 [6](#S3.F6 "图
    6 ‣ III-B 使用卷积神经网络和深度置信网络进行分支预测 ‣ III 深度学习/机器学习用于动态分支预测 ‣ 动态分支预测的深度学习技术调查")）。切片由嵌入层、和池化层以及卷积层组成，用于从分支历史中提取特征。每个切片在不同的历史长度上工作，以形成几何级数。来自所有切片的拼接输出被传递到
    FC 层进行预测输出。作者不推荐将此变体作为分支预测器的实际使用，因为这是一个软件模型。另一个变体是 *mini-BranchNet*，它与推理引擎共同设计。mini-BranchNet
    的架构类似于 big-BranchNet，只是它具有可以调整的旋钮，以减少延迟和存储。Mini-BranchNet 可以作为分支预测器工作。
- en: The key achievement described in this paper is that with BranchNet deep learning
    network architectures can be trained in offline mode and this approach is very
    useful in eliminating the issues of state-of-the-art branch predictors when executed
    in runtime.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本文描述的关键成就是通过 BranchNet 深度学习网络架构可以在离线模式下进行训练，这种方法对于消除在运行时执行的最先进分支预测器的问题非常有用。
- en: '*Deep Belief Networks* (DBN) are stack of Restricted Boltzmann Machine(RBM)
    or Autoencoders which can be used as a solution to mitigate the vanishing gradient
    problem caused by backpropagation process. DBNs can be defined as a combination
    of probability and statistics with neural network architectures. The two layers
    (top) in this generative hybrid model are undirected and the layers are connected
    to next following layers [[20](#bib.bib20)]. Figure [8](#S3.F8 "Figure 8 ‣ III-B
    Branch Prediction with Convolutional Neural Networks and Deep Belief Networks
    ‣ III Deep Learning/Machine Learning for Dynamic Branch Prediction ‣ A Survey
    of Deep Learning Techniques for Dynamic Branch Prediction") is commonly used DBN
    architecture. It has four RBM layers and an output layer. Layer 1 and layer 3
    has same size. Size of input layer and layer 4 are also same. The last layer in
    the network architecture is a perceptron (single layer neural network). Deep Belief
    Networks uses unsupervised leaning techinque, where it trains the network layers
    to reconstruct the input data in last RBM layer. The inner layer extract the features
    from input to regenerate it with minimum error.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度置信网络*（DBN）是受限玻尔兹曼机（RBM）或自编码器的堆叠，这些可以作为解决反向传播过程导致的梯度消失问题的方案。DBN 可以定义为概率和统计学与神经网络架构的结合。在这个生成混合模型中，顶部的两层是无向的，并且这些层与下一个跟随的层连接
    [[20](#bib.bib20)]。图 [8](#S3.F8 "图 8 ‣ III-B 使用卷积神经网络和深度置信网络进行分支预测 ‣ III 深度学习/机器学习用于动态分支预测
    ‣ 动态分支预测的深度学习技术调查") 是常用的 DBN 架构。它有四个 RBM 层和一个输出层。层 1 和层 3 的大小相同。输入层和层 4 的大小也相同。网络架构中的最后一层是感知器（单层神经网络）。深度置信网络使用无监督学习技术，其中它训练网络层在最后的
    RBM 层中重建输入数据。内部层从输入中提取特征，以最小的误差重新生成它。'
- en: '![Refer to caption](img/2b7813b57135ba62fd733fb79dcd6d07.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2b7813b57135ba62fd733fb79dcd6d07.png)'
- en: 'Figure 8: Deep Belief Network Architecture [[21](#bib.bib21)]'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：深度置信网络架构 [[21](#bib.bib21)]
- en: In 2017 Mao et al. proposed the idea of using neural networks with deep belief
    networks for branch prediction [[22](#bib.bib22)] which could outperform perceptron
    based BPs in terms of reducing misprediction rate. Later in 2017 [[21](#bib.bib21)]
    Mao et al. discuss about the possibility of using advanced deep neural networks
    like convolutional neural network along with deep belief network for branch prediction.
    Branch prediction problem is considered as a classification problem to implement
    different deep learning network architectures and impact of global history register
    (GHR), branch global address, length of program counter of the classifier on the
    rate of mis-speculated branch is studied . 90% of data is used for training set
    and remaining 10% is used for testing and validation purpose. For branch prediction
    two CNN and one DBN models were created.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，Mao等人提出了使用深度信念网络的神经网络进行分支预测的想法 [[22](#bib.bib22)]，这种方法可以在降低误预测率方面超越基于感知器的BPs。随后在2017年
    [[21](#bib.bib21)]，Mao等人讨论了将先进的深度神经网络，如卷积神经网络，与深度信念网络结合使用进行分支预测的可能性。将分支预测问题视为分类问题，实施不同的深度学习网络架构，并研究全局历史寄存器（GHR）、分支全局地址、分类器的程序计数器长度对误判分支率的影响。90%的数据用于训练集，剩余10%用于测试和验证。为进行分支预测创建了两个CNN模型和一个DBN模型。
- en: '![Refer to caption](img/37be55eb9898af0d0c69c01dac27d14c.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/37be55eb9898af0d0c69c01dac27d14c.png)'
- en: 'Figure 9: Deep Belief Network Architecture for Branch Prediction [[21](#bib.bib21)]'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：用于分支预测的深度信念网络架构 [[21](#bib.bib21)]
- en: '![Refer to caption](img/549d4129f6ce8cca8c604c56c5605efc.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/549d4129f6ce8cca8c604c56c5605efc.png)'
- en: 'Figure 10: Convolutional Network Network Architecture for Branch Prediction
    [[21](#bib.bib21)]'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：用于分支预测的卷积网络架构 [[21](#bib.bib21)]
- en: Figure [9](#S3.F9 "Figure 9 ‣ III-B Branch Prediction with Convolutional Neural
    Networks and Deep Belief Networks ‣ III Deep Learning/Machine Learning for Dynamic
    Branch Prediction ‣ A Survey of Deep Learning Techniques for Dynamic Branch Prediction")
    is the DBN architecture. Neurons in the fully connected layers (FC) 1 and 3 are
    same (630). Layer 4 is same as the input layer. Figure [10](#S3.F10 "Figure 10
    ‣ III-B Branch Prediction with Convolutional Neural Networks and Deep Belief Networks
    ‣ III Deep Learning/Machine Learning for Dynamic Branch Prediction ‣ A Survey
    of Deep Learning Techniques for Dynamic Branch Prediction") shows the 2 differnt
    CNN architectures used for this experiment. First CNN architecture is based on
    *LeNet* and second one is based on *AlexNet*. The results of these network architectures
    are compared against state-of-the-art branch predictors.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#S3.F9 "图 9 ‣ III-B 基于卷积神经网络和深度信念网络的分支预测 ‣ III 深度学习/机器学习用于动态分支预测 ‣ 动态分支预测的深度学习技术综述")是DBN架构。完全连接层（FC）1和3中的神经元相同（630）。层4与输入层相同。图[10](#S3.F10
    "图 10 ‣ III-B 基于卷积神经网络和深度信念网络的分支预测 ‣ III 深度学习/机器学习用于动态分支预测 ‣ 动态分支预测的深度学习技术综述")展示了实验中使用的两种不同的CNN架构。第一种CNN架构基于*LeNet*，第二种基于*AlexNet*。这些网络架构的结果与最先进的分支预测器进行了比较。
- en: This paper is concluded by stating that performance of deep neural network architecture
    based branch predictors are better than perceptron based BPs. Deeper CNN architectures
    can outperform state-of-the-art branch predictors like Multi-poTAGE+SC and MTAGE+SC.
    When the performance of DBN and CNN are compared, CNN architecture outperformed
    DBN based branch predictors.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本文总结指出，基于深度神经网络架构的分支预测器的性能优于基于感知器的BPs。更深的CNN架构能够超越如Multi-poTAGE+SC和MTAGE+SC等最先进的分支预测器。当比较DBN和CNN的性能时，CNN架构优于基于DBN的分支预测器。
- en: III-C Reinforcement Learning for Branch Prediction
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 基于强化学习的分支预测
- en: '*Reinforcement Learning* is a machine learning methodology in which the training
    algorithm estimates the error based on the penalty or reward in particular situation.
    The penalty is high and reward is low when the error is high. Reward is high when
    the calculated error is low.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*是一种机器学习方法，其中训练算法基于特定情境下的惩罚或奖励来估计误差。当误差较高时，惩罚较高，奖励较低。当计算误差较低时，奖励较高。'
- en: In [[23](#bib.bib23)] Zouzias et al. proposed the idea of using machine learning
    technique Reinforcement Learning (RL) formulation in improving branch predictor
    designs. Today branch predictors in high-end processors are perceptron based or
    variants of TAGE. But there are not significant improvements in recent years to
    the prediction mechanism of these two variants. [[23](#bib.bib23)] suggests that
    branch prediction can be viewed as reinforcement learning problem because they
    have similar theoretical principles.We can consider branch predictor as an agent
    which can closely observe the control flow of the code (i.e the history of branch
    results) and learns a policy to improve the accuracy in future predictions. Execution
    environment of processor is considered as environment for branch predictor agent.
    Branch prediction with RL enables to explore design of branch predictor in the
    aspects of predictor’s decision making policy, state representation and misprediction
    minimization strategy.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[23](#bib.bib23)] 中，Zouzias 等人提出了使用机器学习技术——强化学习（RL）形式来改进分支预测器设计的想法。如今，高端处理器中的分支预测器是基于感知器的或TAGE的变体。然而，这两种变体的预测机制在近年来没有显著的改进。[[23](#bib.bib23)]
    提出分支预测可以被视为强化学习问题，因为它们具有类似的理论原理。我们可以将分支预测器视为一个代理，它可以密切观察代码的控制流（即分支结果的历史）并学习一个策略，以提高未来预测的准确性。处理器的执行环境被视为分支预测器代理的环境。使用RL的分支预测使得在预测器决策策略、状态表示和误预测最小化策略方面探索分支预测器的设计成为可能。
- en: The environment communicates with the agent (branch predictor) so that agent
    can choose and action from state space. Later, based on the correctness of the
    action selected by agent environment responds back with a reward. This reward
    helps to update the predictor. Normally the state space from where agent selects
    action contains PC address of branch, local/global history of branch instructions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 环境与代理（分支预测器）进行通信，以便代理可以从状态空间中选择一个动作。随后，根据代理所选择动作的正确性，环境会返回一个奖励。这个奖励有助于更新预测器。通常，代理选择动作的状态空间包含分支的PC地址、分支指令的本地/全局历史记录。
- en: '![Refer to caption](img/6aad9b571a9c5eda9be4fe243909b9f1.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6aad9b571a9c5eda9be4fe243909b9f1.png)'
- en: 'Figure 11: Classification of branch predictors based on RL methods [[23](#bib.bib23)]'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 基于RL方法的分支预测器分类 [[23](#bib.bib23)]'
- en: Based on the RL methods, the branch predictors are classified into two class
    categories 1) tabular, 2) functional. TAGE based predicors, gshare and bimodal
    predictors are categorized as tabular and perceptron based predictors are categorized
    as functional (O-GHEL, hased perceptron and multiperspective predictors). From
    figure [11](#S3.F11 "Figure 11 ‣ III-C Reinforcement Learning for Branch Prediction
    ‣ III Deep Learning/Machine Learning for Dynamic Branch Prediction ‣ A Survey
    of Deep Learning Techniques for Dynamic Branch Prediction") tabular predictors
    can be formulated with Q-learning and functional predictors with policy-gradient
    methods.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基于RL方法，分支预测器被分为两类：1）表格型，2）函数型。TAGE 基于预测器、gshare 和 bimodal 预测器被归类为表格型，而基于感知器的预测器被归类为函数型（O-GHEL、哈希感知器和多视角预测器）。从图
    [11](#S3.F11 "图 11 ‣ III-C 强化学习用于分支预测 ‣ III 深度学习/机器学习用于动态分支预测 ‣ 动态分支预测的深度学习技术调查")
    中可以看出，表格型预测器可以通过 Q-learning 进行建模，而函数型预测器则可以通过策略梯度方法进行建模。
- en: 'This paper provide information on how to use reinforcement learning in designing
    future branch predictors. Key aspects to be considered for the development are:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了如何在设计未来分支预测器中使用强化学习的信息。需要考虑的关键方面包括：
- en: III-C1 Policy
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 策策
- en: A function which can be represented as linear or non-linear model from current
    state to branch result. One of the advantage of this model is the easy hardware
    implementation . As mentioned in [[16](#bib.bib16)] perceptron based models cannot
    capture non-linear correlations. But in non-linear models micro-architectural
    implementations are hard and expensive.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以表示为线性或非线性模型的函数，从当前状态到分支结果。这个模型的一个优势是硬件实现简单。如在[[16](#bib.bib16)]中提到的，基于感知器的模型无法捕捉非线性相关性。但在非线性模型中，微架构实现困难且昂贵。
- en: III-C2 State Representation
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 状态表示
- en: State for branch predictions may contain branch address, local/global history
    and loop counters.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 分支预测的状态可能包含分支地址、本地/全局历史和循环计数器。
- en: III-C3 Loss function
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C3 损失函数
- en: For branch predictors loss function (objective function) plays a vital role
    in achieving goal i.e reducing the mispredictions or reducing misprediction probability.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分支预测器，损失函数（目标函数）在实现目标方面发挥着至关重要的作用，即减少误预测或降低误预测概率。
- en: III-C4 Optimization Strategy
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C4 优化策略
- en: The optimizer in RL methodology tunes learning rate to minimize the loss functions
    to reduce the miprediction probability. Online gradient descent optimizer is used
    in perceptron based branch predictors to minimize hinge loss function to adjust
    the learning rate.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: RL方法中的优化器调整学习率以最小化损失函数，从而减少误预测概率。在线梯度下降优化器在基于感知机的分支预测器中用于最小化铰链损失函数以调整学习率。
- en: Authors illustrated how RL methods can be applied in BP design by applying policy
    gradient to design perceptron based predictor. The RL based predictor is known
    as Policy Gradient Agent BP (PolGAg).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们展示了如何通过应用策略梯度在BP设计中应用强化学习（RL）方法，以设计基于感知机的预测器。基于RL的预测器被称为策略梯度代理BP（PolGAg）。
- en: IV Discussion
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 讨论
- en: Advancements in machine learning and deep learning has benefited computer architecture
    in many ways. Different neural networks has enhanced the branch prediction methodolgies.
    In [[16](#bib.bib16)] and [[17](#bib.bib17)] we have seen that single layer neural
    network called perceptron could be used for branch prediction and branch confidence
    estimator. Variants of state-of-the-art branch predictor TAGE and perceptron based
    branch predictor have latency and higher misprediction rate in hard-to-predict
    branches (H2P). To overcome this issue Tarsa et al. in [[18](#bib.bib18)] proposed
    a convolutional neural network based architecture which could be trained offline
    to predict H2Ps. Later in 2020 based on [[18](#bib.bib18)] work another convolutional
    network called BranchNet was proposed in [[19](#bib.bib19)] for the same purpose.
    This paper designed 2 architecture Big-BranchNet and Mini-BranchNet for H2Ps.
    In [[22](#bib.bib22)] two architectures similar to *LeNet* and *AlexNet* are implemented.
    Although we have seen some of the researches on this topic, more study can be
    conducted on dynamic branch prediction and improve efficiency. Possibility of
    popular deep learning architectures like *ResNet* and *VGGnet* can be studied
    to implement them with branch preditors.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和深度学习的进步在许多方面惠及了计算机架构。不同的神经网络提升了分支预测方法。在[[16](#bib.bib16)]和[[17](#bib.bib17)]中，我们看到单层神经网络感知机可以用于分支预测和分支置信度估计。最先进的分支预测器TAGE的变体和基于感知机的分支预测器在难以预测的分支（H2P）中具有较高的延迟和更高的误预测率。为了解决这个问题，Tarsa等人在[[18](#bib.bib18)]中提出了一种基于卷积神经网络的架构，该架构可以离线训练以预测H2P。之后在2020年，基于[[18](#bib.bib18)]的工作，[[19](#bib.bib19)]中提出了另一种卷积网络称为BranchNet，目的是相同的。本文设计了两个架构Big-BranchNet和Mini-BranchNet用于H2P。在[[22](#bib.bib22)]中，实施了类似于*LeNet*和*AlexNet*的两个架构。虽然我们已经看到了一些关于这个主题的研究，但仍可以对动态分支预测进行更多研究，以提高效率。可以研究*ResNet*和*VGGnet*等流行深度学习架构的可能性，以将它们与分支预测器结合使用。
- en: V Conclusion
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: This survey focused on how different machine learning techniques and deep learning
    network architectures can be used to enhance the predictive ability of branch
    predictors. Many state-of-the-art predictors have achieved significant accuracy
    in branch outcome prediction. But the papers reviewed in this survey shows that,
    with different network architectures like perceptron, convolutional neural networks,
    deep, belied networks the performance of the system can be improved significantly
    by reducing the latency and misprediction rate.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这项调查集中于如何利用不同的机器学习技术和深度学习网络架构来提升分支预测器的预测能力。许多最先进的预测器在分支结果预测中取得了显著的准确性。但这项调查中审阅的论文表明，通过使用感知机、卷积神经网络、深度网络等不同的网络架构，可以显著提高系统的性能，从而减少延迟和误预测率。
- en: References
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Sparsh Mittal. A survey of techniques for dynamic branch prediction. Concurrency
    and Computation: Practice and Experience, 31(1):e4666, 2019.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Sparsh Mittal. 一项关于动态分支预测技术的调查。《并发与计算：实践与经验》，31(1):e4666，2019年。'
- en: '[2] Vijay Sadananda Pai. Exploiting Instruction-Level Parallelism for Memory
    System Performance. Rice University, 2000.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Vijay Sadananda Pai. 利用指令级并行性提升内存系统性能。赖斯大学，2000年。'
- en: '[3] Md Sarwar M Haque, Md Rafiul Hassan, Muhammad Sulaiman, Salami Onoruoiza,
    Joarder Kamruzzaman, and Md Arifuzzaman. Enhancing branch predictors using genetic
    algorithm. In 2019 8th International Conference on Modeling Simulation and Applied
    Optimization (ICMSAO), pages 1–5, 2019.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Md Sarwar M Haque, Md Rafiul Hassan, Muhammad Sulaiman, Salami Onoruoiza,
    Joarder Kamruzzaman, 和 Md Arifuzzaman. 利用遗传算法增强分支预测器。发表于2019年第8届国际建模仿真与应用优化会议（ICMSAO），第1–5页，2019年。'
- en: '[4] Jim Kukunas. Power and Performance: Software Analysis and Optimization.
    Morgan Kaufmann, 2015.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Jim Kukunas. 功率与性能：软件分析与优化。Morgan Kaufmann, 2015年。'
- en: '[5] Brad Calder, Dirk Grunwald, and Joel Emer. A system level perspective on
    branch architecture performance. In Proceedings of the 28th Annual International
    Symposium on Microarchitecture, pages 199–206\. IEEE, 1995.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Brad Calder, Dirk Grunwald, 和 Joel Emer. 从系统级别看分支架构性能。发表于第28届年度国际微架构研讨会，第199–206页。IEEE,
    1995年。'
- en: '[6] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature 521
    (7553), 436-444. Google Scholar Google Scholar Cross Ref Cross Ref, 2015.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Yann LeCun, Yoshua Bengio, 和 Geoffrey Hinton. 深度学习。自然 521 (7553), 436-444。谷歌学术，2015年。'
- en: '[7] Introduction to machine learning and deep learning. — by sanchit tanwar
    — medium. https://medium.com/@sanchittanwar75/introduction-to-machine-learning-and-deep-learning-bd25b792e488.
    (Accessed on 11/25/2021).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 机器学习与深度学习简介 — by sanchit tanwar — medium。https://medium.com/@sanchittanwar75/introduction-to-machine-learning-and-deep-learning-bd25b792e488。（访问日期：2021年11月25日）。'
- en: '[8] An introduction to machine learning — digitalocean. https://www.digitalocean.com/community/tutorials/an-introduction-to-machine-learning.
    (Accessed on 11/25/2021).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 机器学习简介 — digitalocean。https://www.digitalocean.com/community/tutorials/an-introduction-to-machine-learning。（访问日期：2021年11月25日）。'
- en: '[9] Jeffrey Dean. 1.1 the deep learning revolution and its implications for
    computer architecture and chip design. In 2020 IEEE International Solid- State
    Circuits Conference - (ISSCC), pages 8–14, 2020.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Jeffrey Dean. 1.1 深度学习革命及其对计算机架构和芯片设计的影响。发表于2020年IEEE国际固态电路会议（ISSCC），第8–14页，2020年。'
- en: '[10] Jeff Dean, David Patterson, and Cliff Young. A new golden age in computer
    architecture: Empowering the machine-learning revolution. IEEE Micro, 38(2):21–29,
    2018.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Jeff Dean, David Patterson, 和 Cliff Young. 计算机架构的新黄金时代：赋能机器学习革命。《IEEE微型计算机》，38(2):21–29,
    2018。'
- en: '[11] Daniel Nemirovsky, Tugberk Arkose, Nikola Markovic, Mario Nemirovsky,
    Osman Unsal, Adrian Cristal, and Mateo Valero. A general guide to applying machine
    learning to computer architecture. Supercomputing Frontiers and Innovations, 5(1):95–115,
    2018.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Daniel Nemirovsky, Tugberk Arkose, Nikola Markovic, Mario Nemirovsky,
    Osman Unsal, Adrian Cristal, 和 Mateo Valero. 一般性指南：将机器学习应用于计算机架构。《超级计算前沿与创新》，5(1):95–115,
    2018。'
- en: '[12] Gang Luo and Hongfei Guo. Software-based and hardware-based branch prediction
    strategies and performance evaluation.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Gang Luo 和 Hongfei Guo. 基于软件和硬件的分支预测策略及性能评估。'
- en: '[13] Sweety and Prachi Chaudhary. Implemented static branch prediction schemes
    for the parallelism processors. In 2019 International Conference on Machine Learning,
    Big Data, Cloud and Parallel Computing (COMITCon), pages 79–83, 2019.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Sweety 和 Prachi Chaudhary. 实现了并行处理器的静态分支预测方案。发表于2019年国际机器学习、大数据、云计算与并行计算会议（COMITCon），第79–83页，2019年。'
- en: '[14] Correlating branch prediction - geeksforgeeks. https://www.geeksforgeeks.org/correlating-branch-prediction/.
    (Accessed on 11/30/2021).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 相关分支预测 - geeksforgeeks。https://www.geeksforgeeks.org/correlating-branch-prediction/。（访问日期：2021年11月30日）。'
- en: '[15] Frank Rosenblatt. The perceptron: a probabilistic model for information
    storage and organization in the brain. Psychological review, 65(6):386, 1958.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Frank Rosenblatt. 感知机：信息存储和组织的概率模型。心理学评论，65(6):386, 1958年。'
- en: '[16] Daniel A Jiménez and Calvin Lin. Dynamic branch prediction with perceptrons.
    In Proceedings HPCA Seventh International Symposium on High-Performance Computer
    Architecture, pages 197–206\. IEEE, 2001.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Daniel A Jiménez 和 Calvin Lin. 使用感知机进行动态分支预测。发表于HPCA第七届国际高性能计算机架构研讨会，第197–206页。IEEE,
    2001年。'
- en: '[17] H. Akkary, S.T. Srinivasan, R. Koltur, Y. Patil, and W. Refaai. Perceptron-based
    branch confidence estimation. In 10th International Symposium on High Performance
    Computer Architecture (HPCA’04), pages 265–265, 2004.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] H. Akkary, S.T. Srinivasan, R. Koltur, Y. Patil, 和 W. Refaai. 基于感知机的分支置信度估计。发表于第10届国际高性能计算机架构研讨会（HPCA’04），第265–265页，2004年。'
- en: '[18] Stephen J Tarsa, Chit-Kwan Lin, Gokce Keskin, Gautham Chinya, and Hong
    Wang. Improving branch prediction by modeling global history with convolutional
    neural networks. arXiv preprint arXiv:1906.09889, 2019.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 斯蒂芬·J·塔尔萨、钱克湾·林、戈克切·凯斯金、戈塔姆·钦亚和洪·王。通过使用卷积神经网络建模全局历史来改进分支预测。arXiv 预印本 arXiv:1906.09889,
    2019。'
- en: '[19] Siavash Zangeneh, Stephen Pruett, Sangkug Lym, and Yale N. Patt. Branchnet:
    A convolutional neural network to predict hard-to-predict branches. In 2020 53rd
    Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 118–130,
    2020.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 西阿瓦什·赞根赫、斯蒂芬·普鲁埃特、桑库格·林和耶尔·N·帕特。Branchnet：一种预测难以预测分支的卷积神经网络。在2020年第53届IEEE/ACM国际微体系结构研讨会（MICRO）上，页面118–130,
    2020。'
- en: '[20] Deep learning — deep belief network (dbn) — by renu khandelwal — datadriveninvestor.
    https://medium.datadriveninvestor.com/deep-learning-deep-belief-network-dbn-ab715b5b8afc.
    (Accessed on 12/04/2021).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 深度学习 — 深度信念网络（dbn）— 由**Renu Khandelwal** — [数据驱动投资者](https://medium.datadriveninvestor.com/deep-learning-deep-belief-network-dbn-ab715b5b8afc)。
    （访问日期：2021年12月4日）。'
- en: '[21] Yonghua Mao, Z Huiyang, and Xiaolin Gui. Exploring deep neural networks
    for branch prediction. ECE Department, NC University, 2017.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 毛永华、Z Huiyang 和桂晓林。探索深度神经网络在分支预测中的应用。ECE Department, NC University, 2017。'
- en: '[22] Yonghua Mao, Junjie Shen, and Xiaolin Gui. A study on deep belief net
    for branch prediction. IEEE Access, 6:10779–10786, 2018.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 毛永华、申俊杰和桂晓林。关于深度信念网络在分支预测中的研究。IEEE Access, 6:10779–10786, 2018。'
- en: '[23] Anastasios Zouzias, Kleovoulos Kalaitzidis, and Boris Grot. Branch prediction
    as a reinforcement learning problem: Why, how and case studies. arXiv preprint
    arXiv:2106.13429, 2021.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 阿纳斯塔西奥斯·祖齐亚斯、克莱奥沃罗斯·卡莱齐迪斯和鲍里斯·格罗特。将分支预测视为强化学习问题：为何、如何及案例研究。arXiv 预印本 arXiv:2106.13429,
    2021。'
