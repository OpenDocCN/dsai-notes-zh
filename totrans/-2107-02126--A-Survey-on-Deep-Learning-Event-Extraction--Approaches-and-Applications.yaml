- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:53:11'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2107.02126] A Survey on Deep Learning Event Extraction: Approaches and Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2107.02126](https://ar5iv.labs.arxiv.org/html/2107.02126)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A Survey on Deep Learning Event Extraction: Approaches and Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Qian Li, Jianxin Li, Jiawei Sheng, Shiyao Cui, Jia Wu, Yiming Hei, Hao Peng,
    Shu Guo, Lihong Wang, Amin Beheshti, and Philip S. Yu, Qian Li, and Jianxin Li
    are with the School of Computer Science and Engineering, and Beijing Advanced
    Innovation Center for Big Data and Brain Computing in Beihang University, Beijing
    100083, China. E-mail: {liqian@act.buaa.edu.cn, lijx@buaa.edu.cn} (*Corresponding
    author: Jianxin Li.*)Jiawei Sheng and Shiyao Cui are with the Institute of Information
    Engineering, Chinese Academy of Sciences, Beijing 100083, China, and the School
    of Cyber Security, University of Chinese Academy of Sciences, Beijing 100083,
    China. E-mail: {shengjiawei,cuishiyao}@iie.ac.cn.Yiming Hei is with the School
    of Cyber Science and Technology, Beihang University, Beijing 100083, China. E-mail:
    black@buaa.edu.cn.Hao Peng is with Beijing Advanced Innovation Center for Big
    Data and Brain Computing in Beihang University, Beijing 100083, China. E-mail:
    penghao@act.buaa.edu.cn.Shu Guo and Lihong Wang are with the National Computer
    Network Emergency Response Technical Team/Coordination Center of China, Beijing
    100029, China. E-mail: {guoshu, wlh}@cert.org.cn.Jia Wu and Amin Beheshti are
    with the School of Computing, Macquarie University, Sydney, Australia. E-mail:
    {jia.wu, amin.beheshti} @mq.edu.au.Philip S. Yu is with the Department of Computer
    Science, University of Illinois at Chicago, Chicago 60607, USA. E-mail: psyu@uic.edu.Manuscript
    received August 9, 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Event extraction is a crucial research task for promptly apprehending event
    information from massive textual data. With the rapid development of deep learning,
    event extraction based on deep learning technology has become a research hotspot.
    Numerous methods, datasets, and evaluation metrics have been proposed in the literature,
    raising the need for a comprehensive and updated survey. This paper fills the
    research gap by reviewing the state-of-the-art approaches, especially focusing
    on the general domain event extraction based on deep learning models. We introduce
    a new literature classification of current general domain event extraction research
    according to the task definition. Afterwards, we summarize the paradigm and models
    of event extraction approaches, and then discuss each of them in detail. As an
    important aspect, we summarize the benchmarks that support tests of predictions
    and evaluation metrics. A comprehensive comparison among different approaches
    is also provided in this survey. Finally, we conclude by summarizing future research
    directions facing the research area.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Event extraction, deep learning, evaluation metrics, research trends
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Event Extraction (EE) is an important yet challenging task in information extraction
    research. As a particular form of information, an event refers to a specific occurrence
    of something that happens in a certain time and a certain place involving one
    or more participants, which can usually be described as a change of state [[1](#bib.bib1)].
    The event extraction task aims at extracting such event information from unstructured
    plain texts into a structured form, which mostly describes “who, when, where,
    what, why” and “how” of real-world events that happened. In terms of application,
    the task facilitates people to retrieve event information and analyze people’s
    behaviors, arousing information retrieval [[2](#bib.bib2), [3](#bib.bib3)], recommendation
    [[4](#bib.bib4), [5](#bib.bib5)], intelligent question answering [[6](#bib.bib6),
    [7](#bib.bib7)], knowledge graph construction [[8](#bib.bib8), [9](#bib.bib9)],
    and other event-related applications [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Event extraction can be divided into two groups: close-domain event extraction
    [[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)] and open-domain event extraction
    [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)]. Events are usually considered
    in a predefined event schema, where some specific people and objects are interacted
    at a specific time and place. The close-domain event extraction task aims to find
    words that belong to a specific event schema, which refers to an action or state
    change that occurs, and its extraction targets include time, place, person, and
    action, etc. In the open-domain event extraction task, events are considered as
    a set of related descriptions of a topic, which can be formulated into a classification
    or clustering task. Open-domain event extraction refers to acquiring a series
    of events related to a specific theme, usually composed of multiple events. Whether
    the close-domain or open-domain event extraction task, the purpose of event extraction
    is to capture the event types that we are interested in from numerous texts and
    show the essential arguments of events in a structured form.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning event extraction on general domain has a lot of work and has been
    a relatively mature research taxonomy. It discovers event mentions from texts
    and extracts events containing event triggers and event arguments, where event
    mentions are termed as sentences containing one or more triggers and arguments.
    Event extraction requires to identify the event, classify event type, identify
    the argument, and judge the argument role. Specifically, trigger identification
    and trigger classification are usually formed as the event detection task [[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)], while argument identification
    and argument role classification are usually defined as an argument extraction
    task. The trigger classification is a multi-classification classification [[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)] task to classify the type of each event. The
    role classification task is a multi-class classification task based on word pairs,
    determining the role relationship between any pair of triggers and entities in
    a sentence. From a technical perspective, event extraction can depend on some
    other foundational natural language processing (NLP) tasks such as named entity
    recognition (NER) [[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)], semantic
    parsing [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)], and relation extraction
    [[32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/718e27a3b6bd54d2b7470ac5553bac57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The flowchart of deep learning event extraction on general domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We give the flow chart of deep learning event extraction on the general domain,
    as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning
    Event Extraction: Approaches and Applications"). The event extraction is to find
    the focused event type and extract its arguments with it roles. For a pipeline
    paradigm event extraction, it is necessary to distinguish the event type in the
    text for a given text, called trigger classification. For different event types,
    different event schema is designed. Then, event arguments are extracted according
    to the schema, which includes argument identification and argument role classification
    sub-tasks. In the earliest stage, argument role classification is regarded as
    a word classification task, and each word in the text is classified. In addition,
    there are sequence labeling, machine reading comprehension (MRC) and sequence-to-structure
    generation methods. For a joint paradigm event extraction, the model classifies
    the event type and argument roles simultaneously to avoid error coming from trigger
    classification sub-task.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the traditional event extraction method, the feature designing is necessary,
    while for the deep learning event extraction method on general domain, the features
    can be end-to-end extracted by deep learning models. The existing reviews mainly
    introduce the extraction of subject events, where there are few event extraction
    methods based on deep learning models [[35](#bib.bib35), [18](#bib.bib18), [36](#bib.bib36)].
    In recent years, a large number of event extraction methods have been proposed,
    and the event extraction methods based on Transformer have achieved significant
    improvement [[37](#bib.bib37)]. Furthermore, event extraction is no longer limited
    to classification and sequence annotation manner [[38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40)], but can also be formulated in machine reading comprehension
    and generation [[41](#bib.bib41), [42](#bib.bib42)] manner. Therefore, we comprehensively
    analyze the existing deep learning-based event extraction methods on general domain
    and outlook for future research work. The main contributions of this paper are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We introduce the general domain event extraction technology, review the development
    history of event extraction methods, and point out that the event extraction methods
    with deep learning have become the mainstream. We summarize the necessary information
    of deep learning models according to year of publication in Table [I](#S3.T1 "TABLE
    I ‣ 3.2 Joint-based Paradigm ‣ 3 Event Extraction Paradigm ‣ A Survey on Deep
    Learning Event Extraction: Approaches and Applications").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We analyze various deep learning-based extraction paradigm and models, including
    their advantages and disadvantages in detail. We introduce the currently available
    datasets and give the formulation of main evaluation metrics. We summarize the
    necessary information of primary datasets in Table [II](#S5.T2 "TABLE II ‣ 5.5
    Chinese Event Extraction Scenario ‣ 5 Event Extraction Scenarios ‣ A Survey on
    Deep Learning Event Extraction: Approaches and Applications").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We summarize event extraction accuracy scores on ACE 2005 dataset in Table [IV](#S6.T4
    "TABLE IV ‣ 6.2 Sentence-level ‣ 6 Event Extraction Corpus ‣ A Survey on Deep
    Learning Event Extraction: Approaches and Applications") and event extraction
    applications. We conclude the review by discussing the future research trends
    facing the event extraction.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2bd7449d7b6060d94afc943a84335af6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A diagram of event extraction. The example can be divided into two
    type of event. The type of Die is triggered by “died” with three argument roles
    of Place, Victim and Instrument and the type of Attack is triggered by “fired”
    with three argument roles of Place, Target and Instrument.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Organization of the Survey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The rest of the survey is organized as follows. Section [2](#S2 "2 Preliminary
    ‣ A Survey on Deep Learning Event Extraction: Approaches and Applications") introduces
    the concepts and task definitions of event extraction. Section [3](#S3 "3 Event
    Extraction Paradigm ‣ A Survey on Deep Learning Event Extraction: Approaches and
    Applications") summarizes the existing paradigm related to event extraction, including
    pipeline-based methods and joint-based methods, constituting a summary table.
    Section [4](#S4 "4 Deep Learning Event Extraction Models ‣ A Survey on Deep Learning
    Event Extraction: Approaches and Applications") introduces traditional event extraction
    and deep learning-based event extraction with a comparison. Section [5](#S5 "5
    Event Extraction Scenarios ‣ A Survey on Deep Learning Event Extraction: Approaches
    and Applications") introduces the event extraction on different scenarios. Section [6](#S6
    "6 Event Extraction Corpus ‣ A Survey on Deep Learning Event Extraction: Approaches
    and Applications") and Section [7](#S7 "7 Metrics ‣ A Survey on Deep Learning
    Event Extraction: Approaches and Applications") primary event extraction corpus
    and metrics. We then give quantitative results of the leading models in classic
    event extraction datasets in Section [8](#S8 "8 Quantitative Results ‣ A Survey
    on Deep Learning Event Extraction: Approaches and Applications"). Finally, we
    summarize event extraction applications and main challenges for event extraction
    in Section [9](#S9 "9 Event Extraction Applications ‣ A Survey on Deep Learning
    Event Extraction: Approaches and Applications") and Section [10](#S10 "10 Future
    Research Trends ‣ A Survey on Deep Learning Event Extraction: Approaches and Applications")
    before concluding the article in Section [11](#S11 "11 Conclusion ‣ A Survey on
    Deep Learning Event Extraction: Approaches and Applications").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section introduces concepts, sub-tasks and model manners in current event
    extraction researches.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Concepts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An event indicates an occurrence of an action or state change, often driven
    by verbs or gerunds. It contains the primary components involving in the action,
    such as time, place, and character. Event extraction technology extracts events
    that users are interested in from unstructured texts and presents them to users
    in a structured form [[39](#bib.bib39)]. In short, event extraction detects event
    with its type and extracts the core arguments from the text, as shown in Fig.
    [2](#S1.F2 "Figure 2 ‣ 1.1 Contributions ‣ 1 Introduction ‣ A Survey on Deep Learning
    Event Extraction: Approaches and Applications"). Given a text, an event extraction
    technology can predict the events mentions in the text, the triggers and arguments
    corresponding to each event, and classify the role of each argument. Event extraction
    requires to recognize the two events (Die and Attack), triggered by the words
    “died” and “fired” respectively, as shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1.1 Contributions
    ‣ 1 Introduction ‣ A Survey on Deep Learning Event Extraction: Approaches and
    Applications"). For Die event type, we recognize that “Baghdad”, “cameraman” and
    “American tank” take on the event argument roles Place, Victim and Instrument,
    respectively. For Attack, “Baghdad” and “American tank” take on the event argument
    roles Place and Instrument respectively. And “cameraman” and “Palestine Hotel”
    take on the event argument roles Target.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Event extraction involves many frontier disciplines, such as machine learning,
    pattern matching, and NLP. At the same time, event extraction in various fields
    can help relevant personnel quickly extract relevant content from massive information,
    improve work timeliness, and provide technical support for quantitative analysis.
    Therefore, event extraction has a broad application prospect in various fields.
    Typically, Automatic Content Extraction (ACE) describes an event extraction task
    holding the following terminologies:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Entity`: The entity is an object or group of objects in a semantic category.
    Entity mainly includes people, organizations, places, times, things, etc. In Fig.
    [2](#S1.F2 "Figure 2 ‣ 1.1 Contributions ‣ 1 Introduction ‣ A Survey on Deep Learning
    Event Extraction: Approaches and Applications"), the words“Baghdad”,“cameraman”,“American
    tank”, and “Palestine Hotel” are Entity.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Event mentions`: The phrases or sentences that describe the event contains
    a trigger and corresponding arguments.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Event type`: The event type describes the nature of the event and refers to
    the category to which the event corresponds, usually represented by the type of
    the event trigger. For sentence in Fig. [2](#S1.F2 "Figure 2 ‣ 1.1 Contributions
    ‣ 1 Introduction ‣ A Survey on Deep Learning Event Extraction: Approaches and
    Applications"), it contains Die and Attack event types.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Event trigger`: Event trigger refers to the core unit in event extraction,
    a verb or a noun. Trigger identification is a key step in pipeline-based event
    extraction. For event Die in Fig. [2](#S1.F2 "Figure 2 ‣ 1.1 Contributions ‣ 1
    Introduction ‣ A Survey on Deep Learning Event Extraction: Approaches and Applications"),
    the event trigger is “died”.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Event argument`: Event argument is the main attribute of events. It includes
    entities, nonentity participants, and time, and so on. For event Die in Fig. [2](#S1.F2
    "Figure 2 ‣ 1.1 Contributions ‣ 1 Introduction ‣ A Survey on Deep Learning Event
    Extraction: Approaches and Applications"), the event arguments are “Baghdad”,
    “cameraman”, and “American tank”.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Argument role`: An argument role is a role played by an argument in an event,
    that is, the relationship representation between the event arguments and the event
    triggers. For argument “Baghdad” of Die event in Fig. [2](#S1.F2 "Figure 2 ‣ 1.1
    Contributions ‣ 1 Introduction ‣ A Survey on Deep Learning Event Extraction: Approaches
    and Applications"), the argument role is Place.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.2 Sub-tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Event extraction includes four sub-tasks: trigger classification, trigger identification,
    argument identification and argument role classification.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Trigger identification`: It is generally considered that the trigger is the
    core unit in event extraction that can clearly express an event’s occurrence.
    The trigger identification subtask is to find the trigger from the text. In Fig.
    [2](#S1.F2 "Figure 2 ‣ 1.1 Contributions ‣ 1 Introduction ‣ A Survey on Deep Learning
    Event Extraction: Approaches and Applications"), trigger identification is to
    identify the trigger “died” and “fired”.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Trigger classification`: Trigger classification is to determine whether each
    sentence is an event according to existing triggers. Furthermore, if the sentence
    is an event, we need to determine one or several events types the sentence belongs
    to. For example of Fig. [2](#S1.F2 "Figure 2 ‣ 1.1 Contributions ‣ 1 Introduction
    ‣ A Survey on Deep Learning Event Extraction: Approaches and Applications"), the
    subtask aims to classify the event type of trigger “died” and “fired”, which respectively
    corresponds to Die and Attack. Therefore, the trigger classification sub-task
    can be seen as a multi-label text classification task.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Argument identification`: Argument identification is to identify all the arguments
    contained in an event type from the text. Argument identification usually depends
    on the result of trigger classification and trigger identification. For example
    of Die event in Fig. [2](#S1.F2 "Figure 2 ‣ 1.1 Contributions ‣ 1 Introduction
    ‣ A Survey on Deep Learning Event Extraction: Approaches and Applications"), argument
    identification is to extract the words “Baghdad”, “cameraman” and “American tank”.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Argument role classification`: Argument role classification is based on the
    arguments contained in the event extraction schema, and the category of each argument
    is classified according to the identified arguments. For the extracted words of
    Fig. [2](#S1.F2 "Figure 2 ‣ 1.1 Contributions ‣ 1 Introduction ‣ A Survey on Deep
    Learning Event Extraction: Approaches and Applications"), such as “cameraman”,
    this subtask is to classify the word to Object category. Thus, it also can be
    seen as a multi-label text classification task.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e4ff535b4463d6e3c3c51a83ad37b705.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Classification-based task.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/734295d2d327c0dab580502dfc8254cc.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Question answering-based task.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a1c839cfc0ad57b15e4f45a9ca9d2de.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Sequence labeling-based task.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/03d5afac3e0b4db42af2b387987864c8.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Sequence-to-structure generation-based task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: How to implement argument extraction for Die event on classification-based,
    sequence labeling-based, question answering-based and sequence-to-structure generation-based
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Event Extraction Manner
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Event extraction is a very representative hot topic in information extraction,
    which studies how to extract a specific type of event information from unstructured
    text containing event information (news, blog, etc.). It can be simplified as
    multiple classification tasks, which determine the type of event and the argument
    role that each entity belongs to. For example, the word “cameraman” in Fig. [2](#S1.F2
    "Figure 2 ‣ 1.1 Contributions ‣ 1 Introduction ‣ A Survey on Deep Learning Event
    Extraction: Approaches and Applications"), classification based methods are to
    classify which argument role it belongs to in a given role set. The classification
    method depends on Named Entity Recognition (NER), leading to the propagation of
    error information. Based on this, an event extraction method based on sequence
    labeling is proposed, which labels the start and end position of each argument.
    Sequence labeling based methods give a label in BIO of the word “cameraman”, where
    B stands for ’beginning’, I stands for ’inside’ and O stands for ’outside’. The
    task of event extraction is complex, and arguments are closely related to each
    other. Machine Reading Comprehension (MRC) is adopted to learn association, and
    each argument is found through question and answering pairs. MRC-based methods
    generate a question for argument role, such as Object, the model is to find the
    word play the argument role Object. Therefore, event extraction task can be regarded
    as classification task, sequence labeling task and machine reading comprehension
    task. Recently, some works focus on using a generative way [[43](#bib.bib43),
    [44](#bib.bib44)]. The definitions of these four tasks in more detail are as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Classification-based Task
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the classification task [[45](#bib.bib45), [46](#bib.bib46)], authors usually
    predefine $n$ event types and their corresponding argument roles, $e.g.$ the event
    $e_{i}$ ($i\in[1,n]$) contains a set of argument roles [$r_{i,1},r_{i,2}...,r_{i,l}$].
    Given an input event mention $m$, the model needs to output a result vector $T$,
    where the $i$-th argument $T_{i}$ represents the probability that $m$ belongs
    to the event $e_{i}$. In the classification-based task, the trigger identification
    is to classify whether a word is a trigger. After obtaining the final event (or
    a set) $e_{k}$ of $m$, the model outputs a matrix $R$ where the argument $R_{i,j}$
    means the probability that the extracted argument $a_{i}$ belongs to argument
    roles $r_{k,j}$. As shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Sub-tasks ‣ 2 Preliminary
    ‣ A Survey on Deep Learning Event Extraction: Approaches and Applications")(a),
    it classifies each entity to a predefined argument role.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Machine Reading Comprehension-based Task
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The machine reading comprehension model [[47](#bib.bib47), [48](#bib.bib48),
    [49](#bib.bib49)] can understand a piece of text in natural language and answer
    questions about it [[50](#bib.bib50)]. In the machine reading comprehension-based
    task, the trigger identification is also to classify whether a word is a trigger.
    Firstly, a question schema is designed for each argument role $r$, called $Q_{r}$.
    Since different event types have different arguments, the model needs to first
    identify the event type to which the text belongs. Then, the argument roles to
    be extracted are determined according to the event types. Finally, the event extraction
    method based on machine reading comprehension is to input the text $T$, and apply
    the designed questions $Q_{r}$ one by one to the extraction model, as shown in
    Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Sub-tasks ‣ 2 Preliminary ‣ A Survey on Deep Learning
    Event Extraction: Approaches and Applications")(b). The model extracts the answer
    $A_{r}$, which is the corresponding argument for each argument role $r$.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 Sequence labeling-based Task
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sequence labeling task [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)]
    is a multi-classification task [[54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56)]
    based on word level, which can directly match event arguments based on word level
    event type extraction. The event extraction mainly includes two core tasks: identifying
    and classifying event categories and extracting event arguments. Event extraction
    based on sequence labeling can simply and quickly realize the matching of event
    type and event argument without additional features. In the sequence labeling-based
    task, the trigger identification is to label a word is a trigger. The sequence
    labeling method marks out the target from the text, which is suitable for the
    event extraction task. As shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Sub-tasks ‣
    2 Preliminary ‣ A Survey on Deep Learning Event Extraction: Approaches and Applications")(c),
    for a given text $T={x_{1},x_{2},\dots,x_{N}}$ and event schema, the argument
    role $r$ corresponding to the argument is labeled with the sequence labeling model.
    The output $y={y_{1},y_{2},\dots,y_{N}}$ of sequence labeling model is to tag
    all words in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.4 Sequence-to-structure Generation-based Task
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The sequence-to-structure generation-based event extraction extracts events
    from the text in an end-to-end manner [[43](#bib.bib43)]. In the sequence-to-structure
    generation-based task, the trigger identification is to generate a trigger. It
    uniformly models all tasks in a single model and universally predicts different
    labels. As shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Sub-tasks ‣ 2 Preliminary
    ‣ A Survey on Deep Learning Event Extraction: Approaches and Applications")(d),
    the sequence-to-structure generation-based methods directly generate all arguments
    and their roles. It usually adopts encoder-decoder models [[43](#bib.bib43)],
    which is an easy way to convert text into a structured form.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2d9924d3334b948d540d83e32f77581f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: An example of pipeline-based event extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Event Extraction Paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Event extraction includes four sub-tasks: trigger identification, event type
    classification, argument identification, and argument role classification. According
    to the procedure to settle these four subtasks, the event extraction task is divided
    into pipeline-based event extraction and joint-based event extraction. The pipeline
    based method is first adopted [[39](#bib.bib39), [57](#bib.bib57)]. It first detects
    the triggers, and judges the event type according to the triggers. The argument
    extraction model then extracts arguments and classifies argument roles according
    to the prediction results of event type and the triggers. To overcome the propagation
    of error information caused by event detection, researchers propose a joint-based
    event extraction paradigm [[58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60)].
    It reduces the propagation of error information by combining event detection and
    argument extraction tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Pipeline-based Paradigm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The pipeline-based method treats all sub-tasks as independent classification
    problems [[61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63)]. The pipeline
    approach is widely used since it simplifies the entire event extraction task.
    The pipeline-based event extraction method, as shown in Fig. [4](#S2.F4 "Figure
    4 ‣ 2.3.4 Sequence-to-structure Generation-based Task ‣ 2.3 Event Extraction Manner
    ‣ 2 Preliminary ‣ A Survey on Deep Learning Event Extraction: Approaches and Applications"),
    converts event extraction tasks into a multi-stage classification problem. The
    required classifiers include: 1) A trigger classifier is used to determine whether
    the term is the event trigger and the type of event. 2) An argument classifier
    is used to determine whether the word is the argument of the event. 3) An argument
    role classifier is used to determine the category of arguments.'
  prefs: []
  type: TYPE_NORMAL
- en: The classical deep learning-based event extraction model Dynamic Multi-Pooling
    Convolutional Neural Network (DMCNN) [[39](#bib.bib39)] uses two dynamic multi-pooling
    convolutional neural networks for trigger classification and argument classification.
    The trigger classification model identifies the trigger. If there is a trigger,
    the argument classification model is used to identify arguments and their roles.
    PLMEE [[37](#bib.bib37)] also uses two models employing trigger extraction and
    argument extraction. Argument extractor uses the result of trigger extraction
    to reason. It performs well through introducing Bidirectional Encoder Representation
    from Transformers (BERT) [[64](#bib.bib64)].
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline-based event extraction methods provide additional information for subsequent
    sub-tasks through previous sub-tasks, and take advantage of dependencies between
    subtasks. Du et al. [[41](#bib.bib41)] adopt a question answering method to implement
    event extraction. Firstly, the model identifies the trigger in the input sentence
    through the designed question template of the trigger. The input of the model
    includes the input sentence and question. Then, it classifies the event type according
    to the identified trigger. The trigger can provide additional information for
    trigger classification, but the result of wrong trigger identification can also
    affect trigger classification. Finally, the model identifies the event argument
    and classifies argument roles according to the schema corresponding to the event
    type. In argument extraction, the model utilizes the answers of the previous round
    of history content.
  prefs: []
  type: TYPE_NORMAL
- en: The most significant defect of this method is error propagation. Intuitively,
    if there is an error in trigger identification in the first step, then the accuracy
    of argument identification will be lowed. Therefore, when using pipelines to extract
    events, there will be error cascading and task splitting problems. The pipeline
    event extraction method can extract event arguments by using the information of
    triggers. However, this requires high accuracy of trigger identification. A wrong
    trigger will seriously affect the accuracy rate of argument extraction. Therefore,
    the pipeline event extraction method considers the trigger as the core of an event.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. The pipeline-based method transforms the event extraction task into
    a multi-stage classification problem. The pipeline-based event extraction method
    first identifies the triggers, and argument identification is based on the result
    of the trigger identification. It considers the trigger as the core of an event.
    Yet, this staged strategy will lead to error propagation. The recognition error
    of the trigger will be passed to the argument classification stage, which will
    lead to the degradation of the overall performance. Moreover, because the trigger
    detection always precedes the argument detection, the argument won’t be considered
    while detecting triggers. Therefore, each link is independent and lacks interaction,
    ignoring the impact between them. Thus, the overall dependency relationship cannot
    be handled. The classic case is DMCNN [[39](#bib.bib39)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f9debdf16fe4e07d4f79c8b237cb99d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The simplified architecture of joint-based event extraction paradigm.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Joint-based Paradigm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Event extraction is of great practical value in NLP. Before using deep learning
    to model event extraction tasks, the joint learning method has been studied in
    event extraction. As shown in Fig. [5](#S3.F5 "Figure 5 ‣ 3.1 Pipeline-based Paradigm
    ‣ 3 Event Extraction Paradigm ‣ A Survey on Deep Learning Event Extraction: Approaches
    and Applications"), this method identifies the triggers and arguments according
    to candidate triggers and entities in the first stage. In the second stage, to
    avoid the error information propagation from event type, trigger classification
    and argument role classification are realized simultaneously. It classifies trigger
    “died” to Die event type, and argument “Baghdad” to Place argument role, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: The deep learning event extraction method based on the joint model mainly uses
    the deep learning and the joint learning to interact with the feature learning,
    which can avoid the extended learning time and the complex feature engineering [[65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67)]. Li et al. [[38](#bib.bib38)] study the joint
    learning of trigger extraction and argument extraction tasks based on the traditional
    feature extraction method and obtain the optimal result through the structured
    perceptron model. Zhu et al. [[68](#bib.bib68)] design efficient discrete features,
    including local features of all information contained in feature words and global
    features that can connect trigger with argument information. Nguyen et al. [[40](#bib.bib40)]
    successfully construct local features and global features through deep learning
    and joint learning. It uses a recurrent neural network to combine event recognition
    and argument role classification. The local features constructed are text sequence
    features and local window features. The input text consists of word vectors, entity
    vectors, and event arguments. Then the text is transferred to the recurrent neural
    network model to obtain the sequence characteristics of the deep learning. A deep
    learning model with memory is also proposed to model it. It mainly aimed at the
    global characteristics between event triggers, between event arguments, and between
    event triggers and event arguments to improve the performance of tasks simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Event extraction involves related tasks such as entity recognition, which helps
    improve event extraction. Liu et al. [[69](#bib.bib69)] use the local characteristics
    of arguments to assist role classification. They adopted a joint learning task
    for entities for the first time, aiming to reduce the complexity of the task.
    The previous methods input the dataset with characteristics which are marked and
    output the event. Chen et al. [[70](#bib.bib70)] simplify the process, namely
    plain text input and output. In the middle of the process, it is the joint learning
    on event arguments. This joint learning factor mainly provides the relationship
    and entity information of different events within each input event.
  prefs: []
  type: TYPE_NORMAL
- en: The above joint learning method can achieve joint modeling event extraction
    of triggers and arguments. However, in the actual work process, the extraction
    of triggers and arguments is carried out successively rather than concurrently,
    which is an urgent problem to be discussed later. Besides, if an end-to-end mode
    is added to the deep learning, the feature selection workload will be significantly
    reduced, which will also be discussed later. The joint event extraction method
    avoids the influence of trigger identification error on event argument extraction,
    considering trigger and argument are equally important, but it cannot use the
    information of triggers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. In order to overcome the shortcomings of the pipeline method, researchers
    proposed a joint method. The joint method constructs a joint learning model to
    trigger recognition and argument recognition, where the trigger and argument can
    mutually promote each other’s extraction effect. The experiment proves that the
    effect of the joint learning method is better than the pipeline learning method.
    The classic case is Joint Event Extraction via Recurrent Neural Networks (JRNN)
    [[40](#bib.bib40)]. The joint event extraction method avoids trigger identification
    on event argument extraction, but it cannot use the information of trigger. The
    joint event extraction method considers that the trigger and argument in an event
    are equally important. However, neither pipeline-based event extraction nor joint-based
    event extraction can avoid the impact of event type prediction errors on the performance
    of argument extraction. Moreover, these methods can not share information among
    different event types and learn each type independently, which is disadvantageous
    to the event extraction with only a small amount of labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Basic information of different models. ED: event detection, AE: argument
    extraction, NER: named entity recognition, MRC: machine reading comprehension.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Model | Setting | Manner | Venue | Datasets | ED | AE | NER |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | TEXT2EVENT [[43](#bib.bib43)] | supervised | generation | ACL | ACE05-EN,
    ERE-EN | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| CasEE [[15](#bib.bib15)] | supervised | sequence labeling | ACL(Findings)
    | FewFC | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| CLEVE [[71](#bib.bib71)] | supervised | classification | ACL | ACE, MAVEN
    | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| FEAE [[72](#bib.bib72)] | supervised | MRC | ACL | RAMS | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| GIT [[73](#bib.bib73)] | supervised | classification | ACL | ChFinAnn ¹¹1http://www.cninfo.com.cn/new/index
    | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| NoFPFN [[74](#bib.bib74)] | supervised | classification | ACL(Findings) |
    ChFinAnn | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| DualQA [[75](#bib.bib75)] | semi-supervised | MRC | AAAI | ACE, FewFC | -
    | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| GRIT [[76](#bib.bib76)] | supervised | generation | EACL | (Message Understanding
    Conference) MUC-4 | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wen et al.[[77](#bib.bib77)] | supervised | classification | NAACL | ACE
    | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | HPNet [[78](#bib.bib78)] | supervised | sequence labeling | COLING | ACE2005,
    Text Analysis Conference 2015 (TAC2015) | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | M2E2 [[79](#bib.bib79)] | weakly supervised | classification | ACL | M2E2
    | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | MQAEE [[42](#bib.bib42)] | supervised | MRC | EMNLP | ACE | ✓ | ✓ | -
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Du et al. [[41](#bib.bib41)] | supervised | MRC | EMNLP | ACE | ✓ | ✓
    | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | Min et al. [[80](#bib.bib80)] | supervised | classification | LREC | ACE
    | - | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | Chen et al. [[81](#bib.bib81)] | supervised | MRC | EMNLP | ACE | - |
    ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | EEGCN [[82](#bib.bib82)] | supervised | sequence labeling | EMNLP(Findings)
    | ACE | ✓ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Doc2EDAG²²2The EDAG means entity-based directed acyclic graph. [[83](#bib.bib83)]
    | supervised | generation | EMNLP | ChFinAnn | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al.[[81](#bib.bib81)] | supervised | MRC | arXiv | ACE | ✓ | ✓ |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| GAIL-ELMo³³3The ELMo means embeddings from language models. [[84](#bib.bib84)]
    | supervised | sequence labeling | Data Intell. | ACE | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| DYGIE++ [[85](#bib.bib85)] | supervised | sequence labeling | EMNLP | ACE,
    SciERC, etc. | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| HMEAE [[86](#bib.bib86)] | supervised | classification | EMNLP | ACE, TAC-KBP
    | - | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Han et al. [[87](#bib.bib87)] | supervised | classification | EMNLP | TB-Dense,
    MATRES | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| PLMEE [[37](#bib.bib37)] | supervised | sequence labeling | ACL | ACE | ✓
    | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| JointTransition [[58](#bib.bib58)] | supervised | classification | IJCAI
    | ACE | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Joint3EE [[88](#bib.bib88)] | supervised | sequence labeling | AAAI | ACE
    | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| Chan et al. [[89](#bib.bib89)] | supervised | classification | ACL | ACE
    | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[90](#bib.bib90)] | supervised | MRC | ACL | ACE, CoNLL04 | ✓
    | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | DCFEE⁴⁴4The DCFEE means document-level Chinese financial event extraction. [[91](#bib.bib91)]
    | distance supervision | sequence labeling | ACL | NO.(ANN, POS, NEG) | ✓ | ✓
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zeng et al. [[92](#bib.bib92)] | distance supervision | sequence labeling
    | AAAI | FBWiki, ACE | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al.[[61](#bib.bib61)] | supervised | classification | ACL | ACE
    | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| DEEB-RNN⁵⁵5The DEEB means document embedding enhanced Bi-RNN. [[93](#bib.bib93)]
    | supervised | classification | ACL | ACE | ✓ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SELF [[94](#bib.bib94)] | supervised | classification | ACL | ACE, TAC-KBP
    | ✓ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DBRNN [[95](#bib.bib95)] | supervised | classification | AAAI | ACE | ✓ |
    ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| JMEE [[96](#bib.bib96)] | supervised | sequence labeling | EMNLP | ACE |
    ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Ferguson et al.[[14](#bib.bib14)] | semi-supervised | classification | NAACL
    | ACE, TAC-KBP | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | DMCNN-MIL⁶⁶6The DMCNN-MIL means dynamic multi-pooling convolutional
    neural network with multi-instance learning. [[70](#bib.bib70)] | distance supervision
    | classification | ACL | ACE | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al.[[97](#bib.bib97)] | supervised | classification | ACL | ACE |
    ✓ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | RBPB⁷⁷footnotemark: 7 [[98](#bib.bib98)] | supervised | classification
    | ACL | ACE | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zeng et al.[[99](#bib.bib99)] | supervised | sequence labeling | NLPCC |
    ACE | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| JRNN [[40](#bib.bib40)] | supervised | sequence labeling | NAACL | ACE |
    ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| JOINTEVENTENTITY [[13](#bib.bib13)] | supervised | sequence labeling | NAACL
    | ACE | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| BDLSTM-TNNs [[100](#bib.bib100)] | supervised | classification | CCL | ACE
    | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[69](#bib.bib69)] | supervised | classification | ACL | ACE |
    ✓ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | DMCNN  [[39](#bib.bib39)] | supervised | classification | ACL | ACE
    | ✓ | ✓ | ✓ | ⁶⁶footnotetext: The RBPB means regularization-based pattern balancing
    method for event extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Deep Learning Event Extraction Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Traditional event extraction methods are challenging to learn in-depth features,
    making it difficult to improve the task of event extraction that depends on complex
    semantic relations. Most recent event extraction works are based on a deep learning
    architecture like Convolutional Neural Networks (CNN) [[39](#bib.bib39), [101](#bib.bib101)],
    Recurrent Neural Network (RNN) [[102](#bib.bib102), [95](#bib.bib95)], Graph Neural
    Network (GNN) [[96](#bib.bib96), [82](#bib.bib82), [103](#bib.bib103), [104](#bib.bib104),
    [105](#bib.bib105)], Transformer [[37](#bib.bib37), [106](#bib.bib106), [107](#bib.bib107)],
    or other networks [[58](#bib.bib58), [78](#bib.bib78)]. As shown in TABLE [I](#S3.T1
    "TABLE I ‣ 3.2 Joint-based Paradigm ‣ 3 Event Extraction Paradigm ‣ A Survey on
    Deep Learning Event Extraction: Approaches and Applications"), we show the basic
    information of existing models according to the publish year. It includes the
    domain which is the model exploring, venue the model published, and datasets the
    model used. Furthermore, we conclude whether each model contains event detection,
    argument extraction and named entity recognition. The deep learning method can
    capture complex semantic relations and significantly improve multiple event extraction
    data sets. We introduce several typical event extraction models.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 CNN-based Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b9ec43d14770cba8076bda70f47d44b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The architecture of CNN-based argument extraction. It illustrates
    processing of one instance with predicted trigger ’fired’ and candidate argument
    ’cameraman’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To automatically extract lexical and sentence-level features without using
    complex natural language processing tools, Chen et al. [[39](#bib.bib39)] introduce
    a word representation model, called DMCNN. It captures the meaningful semantic
    rules of words and adopts a framework based on a CNN to capture sentence-level
    clues. However, CNN can only capture the essential information in a sentence,
    and it uses a dynamic multi-pool layer to store more critical information based
    on event triggers and arguments. Event extraction is a two-stage multi-class classification
    realized by a dynamic multi-pool convolutional neural network with automatic learning
    features. The first stage is trigger classification. DMCNN classifies each word
    in the sentence to identify triggers. For a sentence having a trigger, this phase
    applies a similar DMCNN to assign arguments to the trigger and align the arguments’
    roles. Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 CNN-based Models ‣ 4 Deep Learning Event
    Extraction Models ‣ A Survey on Deep Learning Event Extraction: Approaches and
    Applications") depicts the architecture of argument classification. Lexical-level
    feature representation and sentence-level features extraction are used to capture
    lexical clues and learn the sentences’ compositional semantic features.'
  prefs: []
  type: TYPE_NORMAL
- en: CNN induces the underlying structures of the k-grams in the sentences. Thus,
    some researchers also study event extraction techniques based on convolutional
    neural networks. Nguyen et al. [[108](#bib.bib108)] use CNN to investigate the
    event detection task, which overcomes complex feature engineering and error propagation
    limitations compared with traditional feature-based approaches. But it relies
    extensively on other supervised modules and manual resources to obtain features.
    It is significantly superior to the feature-based method in terms of cross-domain
    generalization performance. Furthermore, to consider non-consecutive k-grams,
    Nguyen et al. [[102](#bib.bib102)] introduce non-consecutive CNN. CNN models apply
    in pipeline-based and joint-based paradigm through structured predictions with
    rich local and global characteristics to automatically learn hidden feature representations.
    Joint-based paradigm can mitigate error propagation problems compared with the
    pipeline-based approach and exploit the interdependencies between event triggers
    and argument roles.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 RNN-based Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to the CNN-based event extraction method, some other researches
    are carried out on RNN. The RNN is used for modeling sequence information to extract
    arguments in the event, as shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.2 RNN-based
    Models ‣ 4 Deep Learning Event Extraction Models ‣ A Survey on Deep Learning Event
    Extraction: Approaches and Applications"). JRNN [[40](#bib.bib40)] is proposed
    with a bidirectional RNN for event extraction in a joint-based paradigm. It has
    an encoding stage and prediction stage. In the encoding stage, it uses RNN to
    summarize the context information. Furthermore, it predicts both trigger and argument
    roles in the prediction stage.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ffc32eaa885887c4f57d527592680419.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The simplified architecture of RNN-based argument extraction for
    the input sentence “a man died when a tank fired in Baghdad” for candidate trigger
    “fired”.'
  prefs: []
  type: TYPE_NORMAL
- en: Previous approaches relied heavily on language-specific knowledge and existing
    NLP tools. A more promising method from data automatically learning useful features,
    Feng et al. [[109](#bib.bib109)] develop a hybrid neural network to capture in
    the context of a specific sequence and pieces of information and use them for
    training a multilingual event detector. The model uses a Bidirectional long short-term
    memory (LSTM) to obtain the document’s sequence information that needs to be recognized.
    Then it uses the convolutional neural network to get the phrase chunk information
    in the document, combine the two kinds of information, and finally identify the
    trigger. The method are robust, efficient, and accurate detection for multiple
    languages (English, Chinese, and Spanish). The composite model is superior to
    the traditional feature-based approach in terms of cross-language generalization
    performance. The tree structure and sequence structure in a deep learning have
    better performance than a sequential structure. To avoid over-reliance on lexical
    and syntactic features, dependence bridge recursive neural network (DBRNN) [[95](#bib.bib95)]
    is based on bidirectional RNNs for event extraction. The DBRNN is enhanced by
    relying on bridging grammar-related words. DBRNN is an RNN-based framework that
    leverages the dependency graph information to extract event triggers and argument
    roles.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Attention-based Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The automatic extraction of event features by deep learning model and the enhancement
    of event features by external resources mainly focus on the information of event
    triggers, and less on the information of event arguments and inter-word dependencies.
    Sentence-level sequential modeling suffer a lot from the low efficiency in capturing
    very long-range dependencies. Furthermore, RNN-based and CNN-based models do not
    fully model the associations between events. The modeling of structural information
    in the attention mechanism has gradually attracted the attention of researchers.
    As research methods are constantly proposed, models that add attention mechanisms
    appear gradually, as shown in Fig. [8](#S4.F8 "Figure 8 ‣ 4.3 Attention-based
    Models ‣ 4 Deep Learning Event Extraction Models ‣ A Survey on Deep Learning Event
    Extraction: Approaches and Applications"). The attention mechanism’s feature determines
    that it can use global information to model local context without considering
    location information. It has a good application effect when updating the semantic
    representation of words.'
  prefs: []
  type: TYPE_NORMAL
- en: By controlling the different weight information of each part of the sentence,
    the attention mechanism makes the model pay attention to the important feature
    information of the sentence while ignoring other unimportant feature information,
    and rationally allocate resources to extract more accurate results. At the same
    time, the attention mechanism itself can be used as a kind of alignment, explaining
    the alignment between input and output in the end-to-end model, to make the model
    more interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d5be0502a615ee9d184400f0197c4f30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The architecture of attention-based event extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some researchers also use a hierarchical attention mechanism to conduct the
    global aggregation of information. The jointly multiple event extraction (JMEE)
    [[96](#bib.bib96)] composes of four modules: word representation, syntactic graph
    convolution network, self-attention trigger classification, and argument classification
    modules. The information flow is enhanced by introducing a syntax shortcut arc.
    The graph convolution network based on attention is used to jointly model the
    graph information to extract multiple event triggers and arguments. Furthermore,
    it optimizes a biased loss function when jointly extract event triggers and arguments
    to settle the dataset imbalances.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Graph Convolutional Network-based (GCN-based) Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Syntactic representations present an efficient method for straight linking words
    to their informative context for event detection in sentences [[110](#bib.bib110),
    [111](#bib.bib111), [82](#bib.bib82), [33](#bib.bib33)]. Nguyen et al. [[110](#bib.bib110)]
    which investigate a convolutional neural network based on dependency trees to
    perform event detection are the first to integrate dependency tree relation information
    into neural event detection. The model uses the proposed model with graph convolutional
    networks (GCNs) [[111](#bib.bib111)] and entity mention-based pooling. They propose
    a novel pooling method that relies on entity mentions to aggregate convolution
    vectors. The model operates a pooling over the graph-based convolution vectors
    of the current word and the entity mentions in the sentences. The model aggregates
    convolution vectors to generate a single vector representation for event type
    prediction. The model is to explicitly model the information from entity mentions
    to improve performance for event detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[77](#bib.bib77)], the Text Analysis Conference Knowledge Base Population
    (TAC-KBP) time slot is used to fill the quaternary time representation proposed
    in the task, and the model predicts the earliest and latest start and end times
    of the event, thus representing the ambiguous time span of the event. The model
    constructs a document-level event graph for each input document based on shared
    arguments and time relationships and uses a graph-based attention network method
    to propagate time information on the graph, as shown in Fig. [9](#S4.F9 "Figure
    9 ‣ 4.4 Graph Convolutional Network-based (GCN-based) Models ‣ 4 Deep Learning
    Event Extraction Models ‣ A Survey on Deep Learning Event Extraction: Approaches
    and Applications"), where entities are underlined and events are in bold face.
    Wen et al. construct a document-level event diagram method based on event-event
    relationships for input documents. The event arguments in the document are extracted.
    The events then are arranged in the order of time according to keywords such as
    Before and After and the time logic of the occurrence of the events. Entity argument
    are shared among different events. The model implementation incorporates events
    into a more accurate timeline.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e64904d7124ce6f0fc5d94dcfc8007a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The example event graph. The solid line consists of event arguments,
    while the dashed line graph is constructed based on time relationships [[77](#bib.bib77)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ef5160e078342f8ef31b7653f8b7121e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The architecture of PLMEE [[37](#bib.bib37)] for extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Transformer-based Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is challenging to exploit one argument that plays different roles in various
    events to improve event extraction. Yang et al. [[37](#bib.bib37)] employ a method
    to separate the argument prediction in terms of argument roles for overcoming
    the roles overlap problem. Moreover, this method automatically generates labeled
    data by editing prototypes and screening developed samples through ranking the
    quality due to inadequate training data. They present a framework, Pre-trained
    Language Model-based Event Extractor (PLMEE) [[37](#bib.bib37)], as shown in Fig.
    [10](#S4.F10 "Figure 10 ‣ 4.4 Graph Convolutional Network-based (GCN-based) Models
    ‣ 4 Deep Learning Event Extraction Models ‣ A Survey on Deep Learning Event Extraction:
    Approaches and Applications"). The PLMEE promotes event extraction by using a
    combination of an extraction model and a generation method based on pre-trained
    language models. It is a two-stage task, including trigger extraction and argument
    extraction, and consists of a trigger extractor and an argument extractor, both
    of which rely on BERT’s feature representation. Then it exploits the importance
    of roles to re-weight the loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: GAIL [[84](#bib.bib84)] is an ELMo-based [[112](#bib.bib112)] model utilizing
    a generative adversarial network to help the model focus on harder-to-detect events.
    They propose an entity and event extraction framework based on generative adversarial
    imitation learning. It is an inverse reinforcement learning (IRL) method employing
    generative adversarial networks (GAN). The model directly evaluates the correct
    and incorrect labeling of instances in entity and event extraction through a dynamic
    mechanism using IRL.
  prefs: []
  type: TYPE_NORMAL
- en: DYGIE++¹¹1The DYGIE means dynamic graph information extraction. [[85](#bib.bib85)]
    is a BERT-based framework that models text spans and captures within-sentence
    and cross-sentence context. Much information extraction tasks, such as named entity
    recognition, relationship extraction, event extraction, and co-reference resolution,
    can benefit from the global context across sentences or from phrases that are
    not locally dependent. They carry out event extraction as additional task and
    span update in the relation graph of event trigger and its argument. The span
    representation is constructed on the basis of multi-sentence BERT coding.
  prefs: []
  type: TYPE_NORMAL
- en: Summary. Most of the traditional event extraction methods adopt the artificial
    construction method for feature representation and use the classification model
    to classify triggers and identify the role of the argument. In recent years, the
    deep learning has shown outstanding effects in image processing, speech recognition,
    and natural language processing, etc. To settle drawbacks of traditional methods,
    deep learning-based event extraction is systematically discussed. Before the emergence
    of BERT model, the mainstream method is to find the trigger from the text and
    judge the event type of the text according to the trigger. Recently, with the
    introduction of the event extraction model by BERT, the method of identifying
    event types based on the full text has become mainstream. It is because BERT has
    outstanding contextual representation ability and performs well in text classification
    tasks, especially when there is only a small amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Event Extraction Scenarios
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are some works focus on document-level, low-resource, multilingual, and
    Chinese event extraction, and their goal is to improve the ability of event extraction
    on different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Document-level Event Extraction Scenario
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Document-level Event Extraction (DEE) aims to extract events across an article.
    Comparing with sentence-level event extraction (SEE), two challenges are proposed:
    (i) Arguments-scattering: arguments of one event may be scattered in multiple
    sentences in the document, which means that one event record can not be extracted
    from a single sentence; (ii) Multi-events: one document may simultaneously contain
    multiple events, which demands a holistic modeling about inter-dependency among
    the events. By far, existing DEE researches could be generally grouped into two
    lines.'
  prefs: []
  type: TYPE_NORMAL
- en: The first line mainly focuses on extracting the scattering event arguments in
    the document, namely the first challenge. Early works [[113](#bib.bib113), [114](#bib.bib114)]
    cast document-level argument extraction as a slot-filling paradigm following the
    task setting of MUC-4 [[115](#bib.bib115)]. Further, researchers [[116](#bib.bib116),
    [117](#bib.bib117)] cast document-level event arguments as an Argument-linking
    problem in RAMS [[116](#bib.bib116)] dataset, which seeks to identify event arguments
    throughout the document of given event triggers. Still, the works above are conducted
    under the assumption that the event type or triggers are given in advance, which
    may be unrealistic in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of directly identifying event arguments, the second line of works follow
    the detect-then-extraction paradigm similar in SEE to extract events from the
    document. Specifically, Yang et al. [[118](#bib.bib118)], Huang et al. [[119](#bib.bib119)]
    and Li et al. [[120](#bib.bib120)] first identify the specific event triggers
    to decide the event type, and then extract the event arguments beyond the sentence
    boundaries. Further, researchers [[83](#bib.bib83), [121](#bib.bib121), [122](#bib.bib122)]
    also attempt to conduct DEE in a trigger-free manner in ChFinAnn dataset [[83](#bib.bib83)],
    where event types are directly judged based on the document semantics. Du et al. [[123](#bib.bib123)]
    propose to simultaneously identify the event type and arguments in a generative
    template manner. These methods attempt to simultaneously tackle the two challenges
    of DEE, and have drawn much research attention.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Open-domain Event Extraction Scenario
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the absence of a predefined event pattern, open domain event extraction is
    designed to detect events from text and, in most cases, to cluster similar events
    through extracted event keywords. Event keywords are those words/phrases that
    primarily describe events, sometimes further divided into triggers and parameters.
    Open-domain event extraction [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)]
    does not have a fixed argument role template. Therefore, arguments are often obtained
    by extracting key words. Chau et al.[[16](#bib.bib16)] propose a method to filter
    irrelevant headlines and perform preliminary event extraction, relying on public
    news headlines. Both price and text are fed back into a 3D convolutional neural
    network to learn correlations between events and market movements. Liu et al.
    [[17](#bib.bib17)] design a novel latent variable neural model using a unsupervised
    generative method to explore latent event type vectors and entity mention redundancy.
    Experimental results show that it is scalable to very large corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Low-Resource Event Extraction Scenario
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Due to the arduously expensive annotation in data emerging, there usually only
    exist insufficient data to train an accurate EE model with fully-supervised methods.
    In this survey, we term such a situation as the low-resource scenario. To alleviate
    the data sparsity, existing researches have explored distant supervision [[91](#bib.bib91)]
    methods to boost annotation data. Besides, several promising methods also investigate
    semi-supervised methods [[75](#bib.bib75), [14](#bib.bib14), [124](#bib.bib124)]
    or multilingual methods (See Sec. [5.4](#S5.SS4 "5.4 Multilingual Event Extraction
    Scenario ‣ 5 Event Extraction Scenarios ‣ A Survey on Deep Learning Event Extraction:
    Approaches and Applications")) to enrich supervision information. Recently, several
    researches have explored EE in three typical low-resource settings, including
    few-shot learning setting, zero-shot learning setting and incremental learning
    setting. In this section, we will briefly introduce recent EE methods in the above
    settings as a quick reference.'
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot Learning Setting. Recently, few-shot learning methods [[125](#bib.bib125),
    [126](#bib.bib126)] have been widely researched in several NLP tasks [[127](#bib.bib127),
    [128](#bib.bib128)], which aims to conduct task predictions with extremely limited
    (few-shot, like 1-shot, 3-shot, … ) observed training samples. In event extraction
    area, most existing studies focus on the event detection subtask applied in the
    few-shot learning setting (FSED). The first line of works [[129](#bib.bib129),
    [130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132), [133](#bib.bib133),
    [134](#bib.bib134)] aims to conduct trigger classification given the candidate
    triggers with meta-learning methods. To approach real applications, the second
    line of works [[135](#bib.bib135), [136](#bib.bib136), [137](#bib.bib137)] jointly
    conducts trigger identification and trigger classification with only plain textual
    data. Among them, Cong et al. [[136](#bib.bib136)] further learn robust sequence
    label transition scores with a prototypical amortized conditional random fields
    (CRF), achieving significant improvements.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot Learning Setting. Most of the previous supervised EE methods rely
    on features derived from manual annotations, which cannot handle new event types
    without additional annotations. An extremely challenging low-resource scenario
    is to achieve EE without any available labeled data. To investigate the possibility
    of this scenario, recent researches [[61](#bib.bib61), [138](#bib.bib138)] explore
    zero-shot learning (ZSL) for event extraction. Huang et al. [[61](#bib.bib61)]
    firstly address this problem, which exploits the structural ontology of event
    mentions and types for representations, and conducts predictions with a semantic
    similarity measurement. Lyu et al. [[138](#bib.bib138)] further investigates transfer
    learning methods for new events, which formulates EE into textual entailment (TE)
    and question answering (QA) queries (e.g. “A city was attacked” entails “There
    is an attack”), and exploits pretrained TE/QA models for direct transfer. Though
    these methods still have a large gap from supervised approaches, they reveal an
    insightful vision and provide possible improvement directions for the extremely
    low-resource EE.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental Learning Setting. Existing ED methods usually require a fixed number
    of event types, and perform once-and-for-all training on a fixed dataset. Such
    a paradigm usually encounters challenges when there continually occur new event
    types along with new emerging data. For realistic consideration, a practical ED
    system ought to incrementally learn new event types and simultaneously remain
    predictive on existing types, instead of requiring a fixed dataset to re-train
    all the event types again. Recent researches [[139](#bib.bib139), [140](#bib.bib140)]
    on incremental learning (also called continual learning or lifelong learning)
    focus on the catastrophic forgetting, where the learned system usually suffers
    from significant performance drop on old types when it adapts to new types. Cao
    et al. [[141](#bib.bib141)] is the first work to tackle the incremental ED, which
    solves catastrophic forgetting and semantic ambiguity issues by a proposed knowledge
    consolidation network, achieving effective performance on incremental ED.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Multilingual Event Extraction Scenario
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Monolingual training for event extraction is also an effective method in low
    resource environments [[142](#bib.bib142), [143](#bib.bib143), [33](#bib.bib33)].
    Liu et al. [[143](#bib.bib143)] propose a new multilingual approach called gated
    multilingual attention (GMLATT) framework to address both problems simultaneously
    and develop consistent information in multilingual data through contextual attention
    mechanisms. It uses consistent evidence in multilingual data, models the credibility
    of cues provided by other languages, and controls information integration in various
    languages. Ahmad et al. [[33](#bib.bib33)] propose a graph attention transformer
    encoder (GATE) framework, which uses GCNS to learn language-independent sentences.
    The model embeds the dependency structure into the contextual representation.
    It introduces a self-attention mechanism to learn the dependencies between words
    with different syntactic distances. The method can capture the long distance dependencies
    and then calculate the syntactic distance matrix between words through the mask
    algorithm. It performs well in cross-language sentence-level relationships and
    event extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Chinese Event Extraction Scenario
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Comparing with event extraction in English corpus, Chinese Event Extraction
    can be regarded as a special case of event extraction, having particular properties
    and challenges. Early methods [[144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147)] conduct Chinese EE using elaborately designed linguistic features.
    In the following, neural network based approaches [[99](#bib.bib99), [148](#bib.bib148)]
    are proposed to reduce the heavy rely on feature engineering. Note that comparing
    with English EE, Chinese EE suffer from the absence of natural word delimiters
    and are thus conducted at token-wise instead of word-wise. To alleviate the semantic
    limitation at token-wise in Chinese, exquisite methods are designed to incorporate
    the word-level information to enrich the token semantics. Specifically, Lin et
    al. [[149](#bib.bib149)] proposes Nugget Proposal Networks (NPN), which derives
    hybrid character representations for event trigger tagging, by capturing both
    the structural and semantic information from characters and words. Still, the
    scope of event triggers in NPN are restricted within a fix-sized window, making
    it inflexible and suffering from the overlapping between event triggers. Consequently,
    Ding et al. [[150](#bib.bib150)] propose Trigger-aware Lattice Neural Network
    (TLNN), which makes advantage of the Lattice-structure [[151](#bib.bib151)] to
    incorporate the word and character semantics. Since NPN and TLNN limit that each
    character could interact with only one matched word, Cui et al. [[152](#bib.bib152)]
    propose a heterogeneous graph equipped with two types of nodes (words/characters)
    and three kinds of edges to maximally preserve word-character interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Summary statistics for the datasets. (Doc denotes the number of documents
    in dataset, Sen denotes the number of sentences in dataset).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Datasets | Doc | Sen | Event Type | Language | Related Papers |'
  prefs: []
  type: TYPE_TB
- en: '| MUC-4 | 1700 | - | 5 | - | [[115](#bib.bib115)] |'
  prefs: []
  type: TYPE_TB
- en: '| Google | 11,909 | - | 30 | English | [[153](#bib.bib153)] |'
  prefs: []
  type: TYPE_TB
- en: '| Twitter | 1,000 | - | 20 | English | [[153](#bib.bib153)] |'
  prefs: []
  type: TYPE_TB
- en: '| NO.ANN, NO.POS, NO.NEG (DCFEE) | 2,976 | - | 4 | Chinese | [[91](#bib.bib91)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| ChFinAnn (Doc2EDAG) | 32,040 | - | 5 | Chinese | [[83](#bib.bib83)] |'
  prefs: []
  type: TYPE_TB
- en: '| ACE 2005 | 599 | 18,117 | 33 | Multi-language | [[33](#bib.bib33), [75](#bib.bib75),
    [42](#bib.bib42), [80](#bib.bib80), [89](#bib.bib89), [84](#bib.bib84), [143](#bib.bib143),
    [95](#bib.bib95), [154](#bib.bib154)] |'
  prefs: []
  type: TYPE_TB
- en: '| TAC KBP 2015 | 360 | 12,976 | 38 | English | [[14](#bib.bib14), [78](#bib.bib78)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| TAC KBP 2016 | 500 | 9,042 | 18 | Multi-language | [[86](#bib.bib86)] |'
  prefs: []
  type: TYPE_TB
- en: '| Rich ERE | 50 |  |  | English | [[155](#bib.bib155)] |'
  prefs: []
  type: TYPE_TB
- en: '| FSED | - | 70,852 | 100 | English | [[156](#bib.bib156)] |'
  prefs: []
  type: TYPE_TB
- en: '| GNBusiness | 12,985 | 1,450,336 | - | English | [[17](#bib.bib17)] |'
  prefs: []
  type: TYPE_TB
- en: '| FSD | - | 2,453 | 20 | English | [[153](#bib.bib153)] |'
  prefs: []
  type: TYPE_TB
- en: '| FBI dataset | - | - | 3 | English | [[157](#bib.bib157)] |'
  prefs: []
  type: TYPE_TB
- en: '| RAMS | 3,993 | - | 139 | English | [[116](#bib.bib116)] |'
  prefs: []
  type: TYPE_TB
- en: '| WIKIEVENTS | 246 | 6,132 | - | English | [[120](#bib.bib120)] |'
  prefs: []
  type: TYPE_TB
- en: '| MAVEN | 4,480 | 49,873 | 168 | English | [[22](#bib.bib22)] [[71](#bib.bib71)]
    [[158](#bib.bib158)] |'
  prefs: []
  type: TYPE_TB
- en: 6 Event Extraction Corpus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The availability of labeled datasets for event extraction has become the main
    driving force behind the fast advancement. In this section, we summarize these
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Document-level
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MUC-4. MUC-4 is proposed in the fourth Message Understanding Conference [[115](#bib.bib115)].
    The dataset consists of 1,700 documents, where five types of event are annotated
    with associated role filler templates.
  prefs: []
  type: TYPE_NORMAL
- en: Google. Google dataset ²²2http://data.gdeltproject.org/events/index.html is
    a subset of global database of events, language and tone (GDELT) Event Database,
    event-related words retrieve documents with 30 event types containing 11,909 news
    articles.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter. The Twitter dataset is collected from tweets published in December
    2010 applying Twitter streaming application programming interface (API), including
    20 event types with 1,000 tweets.
  prefs: []
  type: TYPE_NORMAL
- en: 'NO.ANN, NO.POS, NO.NEG (DCFEE). In paper [[91](#bib.bib91)], researchers carry
    out experiments on four types of financial events: Equity Freeze event, Equity
    Pledge event, Equity Repurchase event and Equity Overweight event. A total of
    2976 announcements have been labeled by automatically generating data. The number
    of announcements (NO.ANN) represents the number of announcements can be labeled
    automatically for each event type. The number of positive case (NO.POS) represents
    the total number of positive case mentions. On the contrary, the number of negative
    mentions (NO.NEG) represents the number of negative mentions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ChFinAnn (Doc2EDAG). In [[83](#bib.bib83)], a distant supervision-based (DS-based)
    event labeling is conducted based on ten years ChFinAnn4 documents ³³3http://www.cninfo.com.cn/new/index
    and human-summarized event knowledge bases. The new Chinese event dataset includes
    32,040 documents and 5 event types: Equity Freeze, Equity Repurchase, Equity Underweight,
    Equity Overweight and Equity Pledge.'
  prefs: []
  type: TYPE_NORMAL
- en: RAMS. Roles Across Multiple Sentences (RAMS) is released by Eber et al. [[116](#bib.bib116)]
    for Argument-Linking task, which aims to identify event arguments of given event
    triggers from a 5-sentence window. The dataset contains 3,194 documents, where
    9,124 events are annotated from news based on an ontology of 139 event types and
    65 roles.
  prefs: []
  type: TYPE_NORMAL
- en: WIKIEVENTS. It is released by Li et al. [[120](#bib.bib120)] as a document-level
    benchmark dataset. The dataset is collected from English Wikipedia articles which
    describe real world events.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Sentence-level
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatic Content Extraction (ACE) [[1](#bib.bib1)]. The ACE 2005 is the most
    widely-used dataset in event extraction. It contains a complete set of training
    data in English, Arabic, and Chinese for the ACE 2005 technology evaluation. The
    corpus consists of various types of data annotated for entities, relationships,
    and events by the Language Data Alliance (LDC). It includes 599 documents with
    8 event types, 33 event subtypes, and 35 argument roles ⁴⁴4https://catalog.ldc.upenn.edu/LDC2006T06.
  prefs: []
  type: TYPE_NORMAL
- en: 'Text Analysis Conference Knowledge base Filling (TAC KBP). As a standalone
    component task in KBP, the goal of TAC KBP event tracking (from 2015 to 2017)
    is to extract information about the event so that it is suitable for input into
    the knowledge base. TAC KBP 2015 ⁵⁵5https://tac.nist.gov/2015/KBP/data.html defines
    9 different event types and 38 event subtypes in English. TAC KBP 2016 ⁶⁶6https://tac.nist.gov/2016/KBP/data.html
    and TAC KBP 2017 ⁷⁷7https://tac.nist.gov/2017/KBP/data.html have corpora in three
    languages: English, Chinese, and Spanish, where they own 8 event types and 18
    event subtypes.'
  prefs: []
  type: TYPE_NORMAL
- en: Rich ERE. It extends entities, relationships, and event ontologies, and extends
    the concept of what is Taggable. Rich ERE also introduced the concept of event
    jumping to address the pervasive challenge of event co-referencing, particularly
    with regard to event references within and between documents and granularity changes
    in event arguments, paving the way for the creation of (hierarchical or nested)
    cross-document representations of events.
  prefs: []
  type: TYPE_NORMAL
- en: FSED. Based on ACE 2005 and TAC KBP 2017, FSED dataset [[135](#bib.bib135)]
    is a generated dataset tailored particularly for few-shot scenario. In details,
    it contains 70,852 mentions with 19 event types and 100 event subtypes.
  prefs: []
  type: TYPE_NORMAL
- en: GNBusiness. GNBusiness [[17](#bib.bib17)] collects news reports from Google
    Business News to describe each event from different sources. It obtains 55,618
    business articles with 13,047 news clusters in 288 batches from Oct. 17, 2018,
    to Jan. 22, 2019. The full text corpus is released as GNBusinessFull-Text ⁸⁸8https://github.com/lx865712528/ACL2019-ODEE.
  prefs: []
  type: TYPE_NORMAL
- en: FSD. The first story detection (FSD) dataset [[153](#bib.bib153)] is a story
    detection dataset including 2,499 tweets. Researchers filter out events mentioned
    in fewer than 15 samples considering events mentioned in several samples are usually
    not important. It includes 2,453 tweets with 20 events types.
  prefs: []
  type: TYPE_NORMAL
- en: FBI dataset. The FBI’s city-level hate crime reports (FBI) dataset [[157](#bib.bib157)]
    is built by scraping about 370k unlabeled news articles in the “Fire and Crime”
    category of Patch. It contains two classes for classifying if there is a specific
    hate crime in the text. Furthermore, it labels the attributes of hate crime articles.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: The notations used in evaluation metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Notations | Descriptions |'
  prefs: []
  type: TYPE_TB
- en: '| $T$ | The reference trigger |'
  prefs: []
  type: TYPE_TB
- en: '| $TD$ | The detected trigger |'
  prefs: []
  type: TYPE_TB
- en: '| $N_{T}$ | The actual number of triggers |'
  prefs: []
  type: TYPE_TB
- en: '| $N_{TD}$ | The number of detected triggers |'
  prefs: []
  type: TYPE_TB
- en: '| $T_{t}$ | The true event type |'
  prefs: []
  type: TYPE_TB
- en: '| $TD_{t}$ | The detected event type |'
  prefs: []
  type: TYPE_TB
- en: '| $A$ | The reference argument |'
  prefs: []
  type: TYPE_TB
- en: '| $AD$ | The detected argument |'
  prefs: []
  type: TYPE_TB
- en: '| $N_{A}$ | The actual number of arguments |'
  prefs: []
  type: TYPE_TB
- en: '| $N_{AD}$ | The number of detected arguments |'
  prefs: []
  type: TYPE_TB
- en: '| $A_{r}$ | The detected argument role |'
  prefs: []
  type: TYPE_TB
- en: '| $AD_{r}$ | The number of detected arguments |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Comparison of event extraction methods on ACE 2005 using entity annotations.
    We show the performance of trigger classification and argument role classification
    sub-tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year-Method | Neural Network | External Resource | Paradigm | Trigger Classification
    | Role Classification |'
  prefs: []
  type: TYPE_TB
- en: '| P | R | F1 | P | R | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2008 - Ji et al. [[159](#bib.bib159)] | - | - | - | 60.2 | 76.4 | 67.3 |
    51.3 | 36.4 | 42.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2010 - Liao et al. [[160](#bib.bib160)] | - | - | - | 68.7 | 68.9 | 68.8
    | 45.1 | 44.1 | 44.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 2011 - Hong et al. [[161](#bib.bib161)] | - | - | - | 72.9 | 64.3 | 68.3
    | 51.6 | 45.5 | 48.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 - Li et al. [[38](#bib.bib38)] | - | - | - | 73.7 | 62.3 | 67.5 | 64.7
    | 44.4 | 52.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 - Nguyen et al. [[108](#bib.bib108)] | ✓ | - | - | 71.8 | 66.4 | 69.0
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 - DMCNN [[39](#bib.bib39)] | ✓ | - | Pipeline | 75.6 | 63.6 | 69.1 |
    62.2 | 46.9 | 53.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 - JRNN [[40](#bib.bib40)] | ✓ | - | Joint | 66.0 | 73.0 | 69.3 | 54.2
    | 56.7 | 55.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 - JOINTEVENTENTIT [[13](#bib.bib13)] | - | - | Joint | 75.1 | 63.3 |
    68.7 | 70.6 | 36.9 | 48.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 - NC-CNN [[102](#bib.bib102)] | ✓ | - | - | - | - | 71.3 | - | - | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 - HNN [[109](#bib.bib109)] | ✓ | - | - | 84.6 | 64.9 | 73.4 | - | -
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 - BDLSTM-TNNs [[100](#bib.bib100)] | ✓ | - | Joint | 75.3 | 63.4 | 68.9
    | 62.9 | 47.5 | 54.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 - DMCNN-MIL [[70](#bib.bib70)] | ✓ | ✓ | Joint | 75.5 | 66.0 | 70.5
    | 62.8 | 50.1 | 55.7 |'
  prefs: []
  type: TYPE_TB
- en: '| - 2018 - DEEB-RNN [[93](#bib.bib93)] | ✓ | - | Pipeline | 72.3 | 75.8 | 74
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 - SELF [[94](#bib.bib94)] | ✓ | - | Pipeline | 71.3 | 74.7 | 73.0 |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 - GMLATT [[143](#bib.bib143)] | ✓ | - | Joint | 78.9 | 66.9 | 72.4 |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 - Zeng et al. [[92](#bib.bib92)] | ✓ | ✓ | Pipeline | 85.3 | 79.9 |
    82.5 | 41.9 | 34.6 | 37.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 - Liu et al.[[162](#bib.bib162)] | ✓ | - | Joint | 62.5 | 35.7 | 45.4
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 - GAIL-ELMo [[84](#bib.bib84)] | ✓ | - | Joint | 74.8 | 69.4 | 72.0
    | 61.6 | 45.7 | 52.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 - HMEAE [[86](#bib.bib86)] | ✓ | - | Joint | - | - | - | 62.2 | 56.6
    | 59.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 - JointTransition [[58](#bib.bib58)] | ✓ | - | Joint | 74.4 | 73.2 |
    73.8 | 55.7 | 51.1 | 53.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 - PLMEE [[37](#bib.bib37)] | ✓ | - | Joint | 81.0 | 80.4 | 80.7 | 62.3
    | 54.2 | 58.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021-Li et al. [[163](#bib.bib163)] | ✓ | - | - | - | - | 71.1 | - | - |
    53.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021-GATE (En2ZH) [[33](#bib.bib33)] | ✓ | - | Joint | - | - | - | - | -
    | 63.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021-CasEE [[15](#bib.bib15)] | ✓ | - | Joint | 77.9 | 78.5 | 78.2 | 71.3
    | 71.5 | 71.4 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Comparison of event extraction methods on ACE 2005 without using entity
    annotations. Even Text2Event model does not use token annotations. We show the
    performance of trigger classification and argument role classification sub-tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year-Method | Neural Network | External Resource | Paradigm | Trigger Classification
    | Role Classification |'
  prefs: []
  type: TYPE_TB
- en: '| P | R | F1 | P | R | F1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 - Liu et al. [[69](#bib.bib69)] | ✓ | ✓ | Joint | 77.6 | 65.2 | 70.7
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 - Huang et al.[[155](#bib.bib155)] | ✓ | ✓ | Joint | 80.7 | 50.1 | 61.8
    | 51.9 | 39.4 | 44.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 - RBPB [[98](#bib.bib98)] | ✓ | - | Pipeline | 70.3 | 67.5 | 68.9 |
    54.1 | 53.5 | 53.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 - Liu et al. [[97](#bib.bib97)] | ✓ | - | Pipeline | 78.0 | 66.3 | 71.7
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 - DEEB-RNN [[93](#bib.bib93)] | ✓ | - | Pipeline | 72.3 | 75.8 | 74
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 - SELF [[94](#bib.bib94)] | ✓ | - | Pipeline | 71.3 | 74.7 | 73.0 |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 - GMLATT [[143](#bib.bib143)] | ✓ | - | Joint | 78.9 | 66.9 | 72.4 |
    - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 - Joint3EE [[88](#bib.bib88)] | ✓ | - | Joint | 68.0 | 71.8 | 69.8 |
    52.1 | 52.1 | 52.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 - Chen et al.[[81](#bib.bib81)] | ✓ | - | Joint | 66.7 | 74.7 | 70.5
    | 44.3 | 40.7 | 42.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 - DYGIE++ [[85](#bib.bib85)] | ✓ | - | Joint | - | - | 69.7 | - | -
    | 48.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 - Chen et al. [[81](#bib.bib81)] | ✓ | - | Pipeline | 66.7 | 74.7 |
    70.5 | 44.3 | 40.7 | 42.4 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 - MQAEE [[42](#bib.bib42)] | ✓ | - | Pipeline | - | - | 73.8 | - | -
    | 55.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 - Du et al. [[41](#bib.bib41)] | ✓ | - | Pipeline | 71.1 | 73.7 | 72.3
    | 56.7 | 50.2 | 53.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021-Text2Event [[43](#bib.bib43)] | ✓ | - | Joint | 69.6 | 74.4 | 71.9 |
    52.5 | 55.2 | 53.8 |'
  prefs: []
  type: TYPE_TB
- en: 7 Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the four sub-tasks [[15](#bib.bib15), [37](#bib.bib37), [164](#bib.bib164),
    [100](#bib.bib100)] defined in event extraction, three metrics including Precision
    (P), Recall (R), and F1 are used to measure the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we denote an Indicator function $I(boolean)$: $I(True)=$1 and $I(False)=0$.'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Trigger Identification (TI): a trigger is correctly identified if its span
    offsets exactly match a reference trigger. The corresponding metrics include:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\footnotesize P_{TI}=\frac{\sum{I(TD=T\wedge TD_{L}=T_{L}\wedge TD_{R}=T_{R})}}{N_{TD}},$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\footnotesize R_{TI}=\frac{\sum{I(TD=T\wedge TD_{L}=T_{L}\wedge TD_{R}=T_{R})}}{N_{T}},$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\footnotesize F1_{TI}=\frac{2*P_{TI}*R_{TI}}{(P_{TI}+R_{TI})},$ |  |
    (3) |'
  prefs: []
  type: TYPE_TB
- en: where $TD$ is the detected trigger, $TD_{L}$ and $TD_{R}$ are the left and right
    boundaries of $TD$, $T$ is the reference trigger, $T_{L}$ and $T_{R}$ are the
    left and right boundaries of $T$, $N_{TD}$ and $N_{T}$ denotes the number of detected
    triggers and the actual number of triggers.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Trigger Classification (TC): a trigger is correctly classified if its span
    offsets and event subtype exactly match a reference trigger. The corresponding
    metrics include:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\footnotesize P_{TC}=\frac{\sum{I(TD=T\wedge TD_{t}=T_{t}\wedge TD_{L}=T_{L}\wedge
    TD_{R}=T_{R})}}{N_{TD}},$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\footnotesize R_{TC}=\frac{\sum{I(TD=T\wedge TD_{t}=T_{t}\wedge TD_{L}=T_{L}\wedge
    TD_{R}=T_{R})}}{N_{T}},$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\footnotesize F1_{TC}=\frac{2*P_{TC}*R_{TC}}{(P_{TC}+R_{TC})},$ |  |
    (6) |'
  prefs: []
  type: TYPE_TB
- en: where $TD_{type}$ and $T_{type}$ denote the detected event type and the true
    event type.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Argument Identification (AI): an argument is correctly identified if its
    span offsets and corresponding event subtype exactly match a reference argument.
    The corresponding metrics include:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\footnotesize P_{AI}=\frac{\sum{I(AD=A\wedge TD_{t}=T_{t}\wedge AD_{L}=A_{L}\wedge
    AD_{R}=A_{R})}}{N_{AD}},$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\footnotesize R_{AI}=\frac{\sum{I(AD=A\wedge TD_{t}=T_{t}\wedge AD_{L}=A_{L}\wedge
    AD_{R}=A_{R})}}{N_{A}},$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\footnotesize F1_{AI}=\frac{2*P_{AI}*R_{AI}}{(P_{AI}+R_{AI})},$ |  |
    (9) |'
  prefs: []
  type: TYPE_TB
- en: where $AD$ is the detected argument, $AD_{L}$ and $AD_{R}$ are the left and
    right boundaries of $AD$, $A$ is the reference argument, $A_{L}$ and $A_{R}$ are
    the left and right boundaries of $A$, $N_{AD}$ and $N_{A}$ denotes the number
    of detected arguments and the actual number of arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Argument Classification (AC): an argument is correctly classified if its
    span offsets, corresponding event subtype, and argument role exactly match a reference
    argument. Its corresponding metrics include:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\footnotesize\begin{array}[]{l}\text{P}_{AC}=\frac{\sum I\left(AD=A\wedge
    TD_{t}=T_{t}\wedge AD_{r}=A_{r}\wedge AD_{L}=A_{L}\wedge AD_{R}=A_{R}\right)}{N_{AD}},\end{array}$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\footnotesize\begin{array}[]{l}\operatorname{R}_{AC}=\frac{\sum I\left(AD=A\wedge
    TD_{\text{t }}=T_{\text{t }}\wedge AD_{\text{r}}=A_{\text{r}}\wedge AD_{L}=A_{L}\wedge
    AD_{R}=A_{R}\right)}{N_{A}},\end{array}$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\footnotesize F1_{AC}=\frac{2*P_{AC}*R_{AC}}{(P_{AC}+R_{AC})},$ |  |
    (12) |'
  prefs: []
  type: TYPE_TB
- en: where $AD_{r}$ and $A_{r}$ denote the detected argument role and the true argument
    role.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Quantitative Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section mainly summarizes existing event extraction work and compares
    performance on the ACE 2005 dataset, as shown in Table [IV](#S6.T4 "TABLE IV ‣
    6.2 Sentence-level ‣ 6 Event Extraction Corpus ‣ A Survey on Deep Learning Event
    Extraction: Approaches and Applications") and [V](#S6.T5 "TABLE V ‣ 6.2 Sentence-level
    ‣ 6 Event Extraction Corpus ‣ A Survey on Deep Learning Event Extraction: Approaches
    and Applications"). The evaluation metrics include precision, recall, and F1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, event extraction methods are primarily based on deep learning
    models. As shown in Table [IV](#S6.T4 "TABLE IV ‣ 6.2 Sentence-level ‣ 6 Event
    Extraction Corpus ‣ A Survey on Deep Learning Event Extraction: Approaches and
    Applications"), in terms of the value of F1, the deep learning-based method is
    superior to the machine learning-based method and pattern matching method in both
    event detection and argument extraction. GATE (En2ZH)⁹⁹9En2ZH means that the model
    trained on English and evaluated on Chinese. [[33](#bib.bib33)] is under single-source
    transfer from English to Chinese, which performs well on argument role classification
    task. Li et al. [[163](#bib.bib163)] propose a document-level neural event argument
    extraction model. It is applied for ACE 2005 for zero-shot event extraction seen
    all event types. We can get the validity of the event extraction method based
    on deep learning models. It may indicate that the deep learning-based method can
    better learn the dependencies among arguments in the event extraction task. In
    the deep learning-based model, the BERT-based approach performs the best, both
    in Table [IV](#S6.T4 "TABLE IV ‣ 6.2 Sentence-level ‣ 6 Event Extraction Corpus
    ‣ A Survey on Deep Learning Event Extraction: Approaches and Applications") and
    [V](#S6.T5 "TABLE V ‣ 6.2 Sentence-level ‣ 6 Event Extraction Corpus ‣ A Survey
    on Deep Learning Event Extraction: Approaches and Applications"). It shows that
    BERT can better learn the context information of the sentence and learn word representation
    according to the current text. It better learns the semantic association of words
    in the current context and helps to learn the association between arguments.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the pipeline based methods (RBPB [[98](#bib.bib98)], and DEEB-RNN
    [[93](#bib.bib93)]) with the join based methods (JRNN [[40](#bib.bib40)], and
    DBRNN [[95](#bib.bib95)]) without Transformer [[165](#bib.bib165)], it can be
    seen that the event extraction method of the joint model is better than the pipeline
    model, especially for the argument role classification task. From DMCNN [[39](#bib.bib39)],
    and DMCNN-MIL [[70](#bib.bib70)], it can be concluded that when external resources
    are used on deep learning-based methods, the effect is significantly improved
    and slightly higher than the joint model. Zeng et al. [[92](#bib.bib92)] introduce
    external resources, improving the performance of trigger classification on precision
    and F1\. Thus, it may show that increasing external knowledge is an effective
    method, but it still needs to be explored to introduce external knowledge into
    the argument extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Event Extraction Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce several event-related applications, which can
    be regarded as direct downstream tasks of event extraction. Generally, the identified
    events can be used for event graph construction [[166](#bib.bib166)], event evolution
    analysis [[167](#bib.bib167)] and other event-based NLP applications [[168](#bib.bib168)],
    such as question answering [[169](#bib.bib169), [170](#bib.bib170)] and reading
    comprehension [[171](#bib.bib171)]. Among the tasks, we focus on three widely
    researched tasks associated with events, namely script event prediction (SEP),
    event factuality identification (EFI) and event relation extraction (ERE).
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Event Factuality Identification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Event factuality identification (EFI) aims to identify the degree of certainty
    about whether events actually occur or not in the real world, which can be seen
    as a downstream task of EE in event knowledge graph construction [[172](#bib.bib172)].
    Generally, event factuality can be classified into five categories [[173](#bib.bib173)]:
    certain positive (certainly happening, CT+), certain negative (certainly not happening,
    CT-), possible positive (possibly happening, PS+), possible negative (possibly
    not happening, PS-) and underspecified (events’ factuality cannot be identified,
    Uu). Therefore, an EFI model ought to be able to predict the factuality of the
    event that is PS+.'
  prefs: []
  type: TYPE_NORMAL
- en: Most existing EFI studies focus on the sentence-level task [[174](#bib.bib174),
    [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177)]. The early works on
    this task mainly employ rule-based methods [[178](#bib.bib178), [173](#bib.bib173),
    [179](#bib.bib179)] or machine learning methods with manually designed features [[180](#bib.bib180),
    [181](#bib.bib181), [175](#bib.bib175), [174](#bib.bib174), [182](#bib.bib182),
    [183](#bib.bib183)]. In recent years, neural networks have been introduced into
    the EFI task, and achieve state-of-the-art performance [[176](#bib.bib176), [184](#bib.bib184),
    [185](#bib.bib185), [186](#bib.bib186), [177](#bib.bib177)], which usually adopt
    generative adversarial networks [[187](#bib.bib187)] or graph neural networks [[188](#bib.bib188)]
    to capture enriched textual information. Despite these successful efforts, sentence-level
    event factuality can easily encounter expression conflicts in texts. To this end,
    Qian et al. [[189](#bib.bib189)] propose the document-level EFI task with adversarial
    neural network. Besides, Cao et al. [[172](#bib.bib172)] further exploits the
    uncertainty of local information and the global structure within documents, and
    achieves significant improvements on document-level EFI task.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Event Relation Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Extracting event relations is an important yet challenging task for constructing
    event knowledge graph [[166](#bib.bib166)], which aims to detect the relations
    between the identified events, and thus can also be seen as the downstream tasks
    of event extraction. Generally, existing event relation extraction studies (ERE)
    mainly focus on three event relation types, including co-referential relation,
    causal relation and temporal relation. Since the three relation types are usually
    investigated separately and have no consistent task formulation so far, this section
    will briefly introduce the above three event relation extraction problems separately
    as different tasks. For more detailed event relation extraction reviews, we recommend
    the readers to Liu et al. [[166](#bib.bib166)].
  prefs: []
  type: TYPE_NORMAL
- en: Event Coreference Resolution. Event coreference resolution (ECR) aims to identify
    whether the candidate events refer to the same event in the real-world, where
    those events may appear across several sentences. Existing methods [[190](#bib.bib190),
    [191](#bib.bib191)] usually formulate ECR as a classification or ranking problem,
    and mainly focus on the contextual features around the two events, such as syntactic
    features, event topic information and linguistic features [[192](#bib.bib192)].
    To enrich the clues for resolution, existing works also exploit document-level
    or topical structures [[193](#bib.bib193)], event argument information [[194](#bib.bib194),
    [195](#bib.bib195), [196](#bib.bib196), [197](#bib.bib197), [194](#bib.bib194)]
    and other event-related task information [[198](#bib.bib198), [199](#bib.bib199),
    [200](#bib.bib200)], such as event detection [[201](#bib.bib201)] and entity recognition [[202](#bib.bib202)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Event Causal Relation Extraction. Event causal relation extraction (ECE) aims
    to identify the event causal relation [[203](#bib.bib203)] and distinguish the
    cause and effect between two events, which benefits to the real-world event evolution
    understanding and thereby promotes event detection and event prediction. According
    to the utilized evidence, the ECE methods can be manifested in two groups: 1)
    the methods exploiting internal information, which assume that the textual contexts
    contain sufficient clues for causal relation extraction, where the contextual
    features including syntactic features, lexical features, explicit causal patterns [[204](#bib.bib204),
    [205](#bib.bib205), [206](#bib.bib206)], statistic causal association [[207](#bib.bib207),
    [208](#bib.bib208), [209](#bib.bib209)], and document-level structures [[210](#bib.bib210)].
    2) the methods exploiting external information, which enhances the textual representation
    with external knowledge, such as pre-trained language model [[211](#bib.bib211)],
    and causal-related commonsense or knowledge base [[212](#bib.bib212), [213](#bib.bib213),
    [214](#bib.bib214), [215](#bib.bib215)]. There are also studies [[216](#bib.bib216),
    [217](#bib.bib217)] employing distant supervision from knowledge base to alleviate
    the data sparsity issue in ECE.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Event Temporal Relation Extraction. Event temporal relation extraction (ETE)
    aims to understand the temporal order among events in the texts. Most existing
    studies on ETE follow the TimeML format [[218](#bib.bib218)], which is widely
    used to markup events, time expressions and temporal relations. Generally, existing
    ETE studies can be roughly divided into three groups: 1) rules-based methods,
    which infers the temporal relation for events relying on temporal rules, such
    as syntactic analyzers [[219](#bib.bib219)], regular expression patterns over
    tokens [[220](#bib.bib220)], and other linguistic rules [[221](#bib.bib221)].
    2) machine learning-based methods, which leverages statistical temporal contextual
    features and achieve the task with statistical classifiers [[222](#bib.bib222),
    [223](#bib.bib223)]. 3) neural models, which captures temporal relations with
    neural networks, such as CNNs and LSTMs [[224](#bib.bib224), [225](#bib.bib225)].
    More external features are also considered in neural models, including dependency
    paths [[226](#bib.bib226)], domain knowledge [[227](#bib.bib227)], contextualized
    language models [[228](#bib.bib228)] and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Script Event Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Script [[229](#bib.bib229)] is a chain of ordered events describing activities
    about a protagonist, and Script Event Prediction (SEP) aims to predict the subsequent
    event of a given chain from a candidate event list. As an important task to understand
    the evolutionary patterns among events, SEP has supported various downstream applications,
    including anaphora resolution [[230](#bib.bib230)], story generation [[231](#bib.bib231)]
    and finalcial analysis [[232](#bib.bib232)]. In SEP, each event is represented
    in the form of a tuple $e=v(s,o,p)$, where $v,s,o,p$ are the event arguments respectively
    denoting the verb, subject, object and indirect-object of the event. For example,
    $e=give(waiter,bob,water)$ means that “A waiter gives bob water”. By far, the
    most widely used benchmark for SEP is NYT dataset [[233](#bib.bib233)], where
    the event chains are extracted from the New York Times (NYT) portion of the Gigaword
    corpus [[234](#bib.bib234)]. The dataset consists 140,331/10,000/10,000 event
    chains for training/validation/test. Each event chain contains 8 events and has
    5 candidate events where only one is the correct subsequent event. Next, we introduce
    the details about existing SEP works.
  prefs: []
  type: TYPE_NORMAL
- en: Existing SEP works could be categorized into two groups. The first line of works
    mainly focus on the event co-occurrence relation to predict the subsequent from
    three aspects. Specifically, early works [[235](#bib.bib235), [236](#bib.bib236),
    [237](#bib.bib237), [238](#bib.bib238), [233](#bib.bib233)] model the event-pair-level
    semantic relation to predict the subsequent event of the given event chain. Further,
    to alleviate semantics limitation to event chain, Lv et al. [[239](#bib.bib239)]
    regard the given event chain as a combination of several event-segments and capture
    clues from diverse event segments to facilitate the event prediction. In the following,
    researchers encode the full event-chain [[240](#bib.bib240), [241](#bib.bib241),
    [242](#bib.bib242)] to grasp semantic signals to predict subsequent event. Wang
    et al. [[242](#bib.bib242)] and Zheng et al. [[243](#bib.bib243)] also employ
    the graph structure to model the event chain. The second line of works integrate
    external knowledge to help understand the scripts, since the absence of text contexts
    makes the semantics in scripts more sparse than normal texts. Specifically, Ding
    et al. [[244](#bib.bib244)] utilize knowledge bases, Event2Mind [[245](#bib.bib245)]
    and ATlas Of MachIne Commonsens (ATOMIC) [[246](#bib.bib246)], to refine sentiment
    and intention information to enrich the semantics of the script. Further, Lv et
    al.  [[247](#bib.bib247)] incorporate event knowledge base ASER (Activities, States,
    Events and their Relations) [[248](#bib.bib248)] to provide causal and temporal
    relations between events to predict the subsequent event, achieving great success
    in this task.
  prefs: []
  type: TYPE_NORMAL
- en: 10 Future Research Trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Event extraction is an essential and challenging task in text mining, which
    mainly learns the structured representation of events from the relevant text describing
    the events. Event extraction is mainly divided into two sub-tasks: event detection
    and argument extraction. The core of event extraction is identifying the event-related
    words in the text and classifying them into appropriate categories. The event
    extraction method based on the deep learning model automatically extracts features
    and avoids the tedious work of designing features manually. Event extraction tasks
    are constructed as an end-to-end system, using word vectors with rich language
    features as input, to reduce the errors caused by the underlying NLP tools. Previous
    methods focus on studying effective features to capture the lexical, syntactic,
    and semantic information of candidate triggers, candidate arguments. Furthermore,
    they explore the dependence between triggers and multiple entities related to
    the same trigger, and the relationship between multiple triggers associated with
    the same entity. According to the characteristics of the event extraction and
    the current research status, we summarize the following technical challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Challenges from Event Extraction Corpus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Event Extraction Dataset Construction. The event extraction task is complex,
    and the existing pre-training model lacks the learning of the event extraction
    task. The existing event extraction data sets have a few labeled data, and manual
    annotation of event extraction data set has a high time cost. Therefore, the construction
    of large-scale event extraction data set or the design of automatic construction
    event extraction data set is also a future research trend.
  prefs: []
  type: TYPE_NORMAL
- en: External Resources. The data set of event extraction is small. Deep learning
    combining external resources and constructing a large-scale dataset has achieved
    good results. Due to the difficulties in constructing labeled data sets and the
    small size of data sets, it is also an urgent research direction that how to make
    better use of deep learning to extract events effectively with help of external
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: Event Extraction Schema. Event extraction methods can be divided into close-domain
    event extraction methods and open-domain event extraction methods. The effect
    of event extraction methods without schema is challenging to evaluate, and template-based
    event extraction methods need to design different event schema according to different
    event types. Therefore, how to design a general event extraction schema based
    on event characteristics is an essential means to overcome the difficulty in constructing
    event extraction data set and sharing knowledge among classes.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Challenges from Event Extraction Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dependency Learning. The event extraction method using BERT has become mainstream
    at present. However, event extraction is different from the task learned by the
    BERT model in pre-training. Argument extraction needs to consider the relationship
    between the event argument roles to extract different roles under the same event
    type. It requires the event extraction model to learn the syntactic dependencies
    of the text. Therefore, making the dependency relationship between the event arguments
    is an urgent problem to solve to comprehensively and accurately extract the arguments
    of each event type.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-End Learning Model. The advantage of the deep learning method based on
    the joint model over the traditional approach is the joint representation form.
    The event extraction depends on the label of entities. So this paper believes
    that establishing a end-to-end autonomous learning model based on deep learning
    is a direction worthy of research and exploration, and how to design multi-task
    and multi-federation is a major challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-event Extraction. According to the different granularity of event extraction,
    event extraction can be divided into sentence-level event extraction and document-level
    event extraction. There have been a lot of researches on sentence-level event
    extraction. However, the document-level event extraction is still in the exploratory
    stage, and the document-level event extraction is closer to the practical application.
    Therefore, how to design the multi-event extraction method for the text is of
    great research significance.
  prefs: []
  type: TYPE_NORMAL
- en: Domain Event Extraction. The domain text often contains numerous technical terms,
    which increases the difficulty of domain event extraction [[249](#bib.bib249)].
    For example, Biomedical EE (BEE) aims to extract events capturing an interplay
    between biomedical entities [[250](#bib.bib250), [251](#bib.bib251), [252](#bib.bib252),
    [253](#bib.bib253)]. Extracting and harnessing them is beneficial for medical
    research and disease prevention [[254](#bib.bib254), [255](#bib.bib255)]. Therefore,
    how to design effective methods to understand the deep semantic information and
    context correspondence in the domain text has become an urgent problem to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability for Event Extraction. Event extraction includes four sub-tasks,
    and the existing event extraction often considers how to improve the accuracy
    of extraction, but there is little research on the interpretability of event extraction
    [[256](#bib.bib256)]. Due to the fact that the task of event extraction is complex,
    it is difficult to understand directly why the model divides a word into a certain
    argument role for a complex text. This requires the event extraction model to
    be interpretable in order to facilitate the manual discrimination of the predicted
    results, which is very important in the biological and medical fields [[257](#bib.bib257)].
  prefs: []
  type: TYPE_NORMAL
- en: 11 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper principally introduces the existing deep learning models for event
    extraction tasks. Firstly, we introduce concepts and definitions from three aspects
    of event extraction. Then we divide the deep learning-based event extraction paradigm
    into the pipeline and joint parts and introduce them, respectively. Deep learning-based
    models enhance performance by improving the presentation learning method, model
    structure, and additional data and knowledge. Then, we introduce the datasets
    with a summary table and evaluation metrics. Furthermore, we give the quantitative
    results of the leading models in a summary table on ACE 2005 datasets. Finally,
    we summarize the possible future research trends of event extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The corresponding author is Jianxin Li. The authors of this paper were supported
    by the NSFC through grant No.U20B2053, 62106059 and the Academic Excellence Foundation
    of Beihang University for PhD Students. Philip S. Yu was supported by the NSF
    under grants III-1763325, III-1909323, III-2106758, and SaTC-1930941.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] G. R. Doddington, A. Mitchell, M. A. Przybocki, L. A. Ramshaw, S. M. Strassel,
    and R. M. Weischedel, “The automatic content extraction (ACE) program - tasks,
    data, and evaluation,” in LREC, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] W. Zhang, X. Zhao, L. Zhao, D. Yin, and G. H. Yang, “DRL4IR: 2nd workshop
    on deep reinforcement learning for information retrieval,” in ACM SIGIR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Kuhnle, M. Aroca-Ouellette, A. Basu, M. Sensoy, J. Reid, and D. Zhang,
    “Reinforcement learning for information retrieval,” in ACM SIGIR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] C. Liu, C. Zhou, J. Wu, H. Xie, Y. Hu, and L. Guo, “CPMF: A collective
    pairwise matrix factorization model for upcoming event recommendation,” in IJCNN,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] L. Gao, J. Wu, Z. Qiao, C. Zhou, H. Yang, and Y. Hu, “Collaborative social
    group influence for event recommendation,” in CIKM, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] J. L. Boyd-Graber and B. Börschinger, “What question answering can learn
    from trivia nerds,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Q. Cao, H. Trivedi, A. Balasubramanian, and N. Balasubramanian, “Deformer:
    Decomposing pre-trained transformers for faster question answering,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] X. Wu, J. Wu, X. Fu, J. Li, P. Zhou, and X. Jiang, “Automatic knowledge
    graph construction: A report on the 2019 ICDM/ICBK contest,” in ICDM, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Bosselut, R. L. Bras, and Y. Choi, “Dynamic neuro-symbolic knowledge
    graph construction for zero-shot commonsense question answering,” in AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] X. Su, S. Xue, F. Liu, J. Wu, J. Yang, C. Zhou, W. Hu, C. Paris, S. Nepal,
    D. Jin, Q. Z. Sheng, and P. S. Yu, “A comprehensive survey on community detection
    with deep learning,” CoRR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] F. Liu, S. Xue, J. Wu, C. Zhou, W. Hu, C. Paris, S. Nepal, J. Yang, and
    P. S. Yu, “Deep learning for community detection: Progress, challenges and opportunities,”
    in IJCAI, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] X. Ma, J. Wu, S. Xue, J. Yang, Q. Z. Sheng, and H. Xiong, “A comprehensive
    survey on graph anomaly detection with deep learning,” CoRR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] B. Yang and T. M. Mitchell, “Joint extraction of events and entities within
    a document context,” in NAACL HLT, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. Ferguson, C. Lockard, D. S. Weld, and H. Hajishirzi, “Semi-supervised
    event extraction with paraphrase clusters,” in NAACL-HLT, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] J. Sheng, S. Guo, B. Yu, Q. Li, Y. Hei, L. Wang, T. Liu, and H. Xu, “Casee:
    A joint learning framework with cascade decoding for overlapping event extraction,”
    in ACL/IJCNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. T. Chau, D. Esteves, and J. Lehmann, “Open-domain event extraction
    and embedding for natural gas market prediction,” CoRR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] X. Liu, H. Huang, and Y. Zhang, “Open domain event extraction using neural
    latent variable models,” in ACL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] M. Mejri and J. Akaichi, “A survey of textual event extraction from social
    networks,” in LPKM, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Z. Li, X. Chang, L. Yao, S. Pan, Z. Ge, and H. Zhang, “Grounding visual
    concepts for zero-shot event detection and event captioning,” in ACM SIGKDD, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] J. Liao, X. Zhao, X. Li, L. Zhang, and J. Tang, “Learning discriminative
    neural representations for event detection,” in ACM SIGIR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] H. Lin, Y. Lu, X. Han, and L. Sun, “Cost-sensitive regularization for
    label confusion-aware event detection,” in ACL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Y. Cao, H. Peng, J. Wu, Y. Dou, J. Li, and P. S. Yu, “Knowledge-preserving
    incremental social event detection via heterogeneous gnns,” in The Web Conference,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] R. Aly, S. Remus, and C. Biemann, “Hierarchical multi-label classification
    of text with capsule networks,” in ACL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] I. Chalkidis, M. Fergadiotis, P. Malakasiotis, and I. Androutsopoulos,
    “Large-scale multi-label text classification on EU legislation,” in ACL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] W. Chang, H. Yu, K. Zhong, Y. Yang, and I. S. Dhillon, “Taming pretrained
    transformers for extreme multi-label text classification,” in ACM SIGKDD, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, and J. Li, “A unified MRC framework
    for named entity recognition,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] J. Yu, B. Bohnet, and M. Poesio, “Named entity recognition as dependency
    parsing,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] B. Y. Lin, D. Lee, M. Shen, R. Moreno, X. Huang, P. Shiralkar, and X. Ren,
    “Triggerner: Learning with entity triggers as explanations for named entity recognition,”
    in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] R. Cao, S. Zhu, C. Yang, C. Liu, R. Ma, Y. Zhao, L. Chen, and K. Yu, “Unsupervised
    dual paraphrasing for two-stage semantic parsing,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] E. Stengel-Eskin, A. S. White, S. Zhang, and B. V. Durme, “Universal decompositional
    semantic parsing,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] I. Abdelaziz, S. Ravishankar, P. Kapanipathi, S. Roukos, and A. G. Gray,
    “A semantic parsing and reasoning-based approach to knowledge base question answering,”
    in AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] T. Chen, H. Shi, L. Liu, S. Tang, J. Shao, Z. Chen, and Y. Zhuang, “Empower
    distantly supervised relation extraction with collaborative adversarial training,”
    in AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] W. U. Ahmad, N. Peng, and K. Chang, “GATE: graph attention transformer
    encoder for cross-lingual relation and event extraction,” in AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] K. Sun, R. Zhang, S. Mensah, Y. Mao, and X. Liu, “Progressive multi-task
    learning with controlled information flow for joint entity and relation extraction,”
    in AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] F. Hogenboom, F. Frasincar, U. Kaymak, F. de Jong, and E. Caron, “A survey
    of event extraction methods from text for decision support systems,” Decis. Support
    Syst., 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] W. Xiang and B. Wang, “A survey of event extraction from text,” IEEE Access,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Yang, D. Feng, L. Qiao, Z. Kan, and D. Li, “Exploring pre-trained language
    models for event extraction and generation,” in ACL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Q. Li, H. Ji, and L. Huang, “Joint event extraction via structured prediction
    with global features,” in ACL, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y. Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao, “Event extraction via dynamic
    multi-pooling convolutional neural networks,” in ACL, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] T. H. Nguyen, K. Cho, and R. Grishman, “Joint event extraction via recurrent
    neural networks,” in NAACL HLT, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] X. Du and C. Cardie, “Event extraction by answering (almost) natural questions,”
    in EMNLP, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] F. Li, W. Peng, Y. Chen, Q. Wang, L. Pan, Y. Lyu, and Y. Zhu, “Event extraction
    as multi-turn question answering,” in EMNLP, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Y. Lu, H. Lin, J. Xu, X. Han, J. Tang, A. Li, L. Sun, M. Liao, and S. Chen,
    “Text2event: Controllable sequence-to-structure generation for end-to-end event
    extraction,” in ACL/IJCNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] I. Hsu, K.-H. Huang, E. Boschee, S. Miller, P. Natarajan, K.-W. Chang,
    N. Peng, et al., “Degree: A data-efficient generative event extraction model,”
    CoRR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] D. Mekala and J. Shang, “Contextualized weak supervision for text classification,”
    in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] B. Guo, S. Han, X. Han, H. Huang, and T. Lu, “Label confusion learning
    to enhance text classification models,” in AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] S. Guo, R. Li, H. Tan, X. Li, Y. Guan, H. Zhao, and Y. Zhang, “A frame-based
    sentence representation for machine reading comprehension,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] B. Zheng, H. Wen, Y. Liang, N. Duan, W. Che, D. Jiang, M. Zhou, and T. Liu,
    “Document modeling with graph attention networks for multi-grained machine reading
    comprehension,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] S. Chen, Y. Wang, J. Liu, and Y. Wang, “Bidirectional machine reading
    comprehension for aspect sentiment triplet extraction,” in AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] W. Yuan, T. He, and X. Dai, “Improving neural question generation using
    deep linguistic representation,” in WWW, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] L. Chen, W. Ruan, X. Liu, and J. Lu, “Seqvat: Virtual adversarial training
    for semi-supervised sequence labeling,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] T. Gui, J. Ye, Q. Zhang, Z. Li, Z. Fei, Y. Gong, and X. Huang, “Uncertainty-aware
    label refinement for sequence labeling,” in EMNLP, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] A. Ramponi, R. van der Goot, R. Lombardo, and B. Plank, “Biomedical event
    extraction as sequence labeling,” in EMNLP, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] T. Jiang, D. Wang, L. Sun, H. Yang, Z. Zhao, and F. Zhuang, “Lightxml:
    Transformer with dynamic negative sampling for high-performance extreme multi-label
    text classification,” in AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] L. Xiao, X. Zhang, L. Jing, C. Huang, and M. Song, “Does head label help
    for long-tailed multi-label text classification,” in AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] X. Liang, D. Cheng, F. Yang, Y. Luo, W. Qian, and A. Zhou, “F-HMTC: detecting
    financial events for investment decisions based on neural hierarchical multi-label
    text classification,” in IJCAI, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] A. Subburathinam, D. Lu, H. Ji, J. May, S. Chang, A. Sil, and C. R. Voss,
    “Cross-lingual structure transfer for relation and event extraction,” in EMNLP-IJCNLP,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] J. Zhang, Y. Qin, Y. Zhang, M. Liu, and D. Ji, “Extracting entities and
    events as a single task using a transition-based neural model,” in IJCAI, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] D. Li, L. Huang, H. Ji, and J. Han, “Biomedical event extraction based
    on knowledge-driven tree-lstm,” in NAACL-HLT, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] J. Sheng, Q. Li, Y. Hei, S. Guo, B. Yu, L. Wang, M. He, T. Liu, and H. Xu,
    “A joint learning framework for the CCKS-2020 financial event extraction task,”
    Data Intell., 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] L. Huang, H. Ji, K. Cho, I. Dagan, S. Riedel, and C. R. Voss, “Zero-shot
    transfer learning for event extraction,” in ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] H. Funke, S. Breß, S. Noll, V. Markl, and J. Teubner, “Pipelined query
    processing in coprocessor environments,” in SIGMOD Conference, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. Gasmi, O. Mosbahi, M. Khalgui, L. Gomes, and Z. Li, “R-node: New pipelined
    approach for an effective reconfigurable wireless sensor node,” IEEE Trans. Syst.
    Man Cybern. Syst., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of
    deep bidirectional transformers for language understanding,” in NAACL-HLT, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Y. Lin, H. Ji, F. Huang, and L. Wu, “A joint neural model for information
    extraction with global features,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] M. V. Nguyen, V. D. Lai, and T. H. Nguyen, “Cross-task instance representation
    interactions and label dependencies for joint information extraction with graph
    convolutional networks,” in NAACL-HLT, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Z. Zhang and H. Ji, “Abstract meaning representation guided graph encoding
    and decoding for joint information extraction,” in NAACL-HLT, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Z. Zhu, S. Li, G. Zhou, and R. Xia, “Bilingual event extraction: a case
    study on trigger type determination,” in ACL, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] S. Liu, Y. Chen, S. He, K. Liu, and J. Zhao, “Leveraging framenet to improve
    automatic event detection,” in ACL, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Y. Chen, S. Liu, X. Zhang, K. Liu, and J. Zhao, “Automatically labeled
    data generation for large scale event extraction,” in ACL, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Z. Wang, X. Wang, X. Han, Y. Lin, L. Hou, Z. Liu, P. Li, J. Li, and J. Zhou,
    “Cleve: Contrastive pre-training for event extraction,” CoRR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] K. Wei, X. Sun, Z. Zhang, J. Zhang, G. Zhi, and L. Jin, “Trigger is not
    sufficient: Exploiting frame-aware knowledge for implicit event argument extraction,”
    in ACL-IJCNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] R. Xu, T. Liu, L. Li, and B. Chang, “Document-level event extraction via
    heterogeneous graph-based interaction model with a tracker,” CoRR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] S. Zheng, W. Cao, W. Xu, and J. Bian, “Revisiting the evaluation of end-to-end
    event extraction,” in ACL-IJCNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Y. Zhou, Y. Chen, J. Zhao, Y. Wu, J. Xu, and J. Li, “What the role is
    vs. what plays the role: Semi-supervised event argument extraction via dual question
    answering,” in AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] X. Du, A. M. Rush, and C. Cardie, “GRIT: generative role-filler transformers
    for document-level event entity extraction,” in EACL, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] H. Wen, Y. Qu, H. Ji, Q. Ning, J. Han, A. Sil, H. Tong, and D. Roth, “Event
    time extraction and propagation via graph attention networks,” in NAACL-HLT, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] P. Huang, X. Zhao, R. Takanobu, Z. Tan, and W. Xiao, “Joint event extraction
    with hierarchical policy network,” in COLING, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] M. Li, A. Zareian, Q. Zeng, S. Whitehead, D. Lu, H. Ji, and S. Chang,
    “Cross-media structured common space for multimedia event extraction,” in ACL,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] B. Min, Y. S. Chan, and L. Zhao, “Towards few-shot event mention retrieval:
    An evaluation framework and A siamese network approach,” in LREC, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. Chen, T. Chen, S. Ebner, A. S. White, and B. V. Durme, “Reading the
    manual: Event extraction as definition comprehension,” in NLP@EMNLP, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] S. Cui, B. Yu, T. Liu, Z. Zhang, X. Wang, and J. Shi, “Edge-enhanced graph
    convolution networks for event detection with syntactic relation,” in EMNLP (Findings),
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Zheng, W. Cao, W. Xu, and J. Bian, “Doc2edag: An end-to-end document-level
    framework for chinese financial event extraction,” in EMNLP-IJCNLP, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] T. Zhang, H. Ji, and A. Sil, “Joint entity and event extraction with generative
    adversarial imitation learning,” Data Intell., no. 2, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] D. Wadden, U. Wennberg, Y. Luan, and H. Hajishirzi, “Entity, relation,
    and event extraction with contextualized span representations,” in EMNLP-IJCNLP,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] X. Wang, Z. Wang, X. Han, Z. Liu, J. Li, P. Li, M. Sun, J. Zhou, and X. Ren,
    “HMEAE: hierarchical modular event argument extraction,” in EMNLP-IJCNLP, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] R. Han, Q. Ning, and N. Peng, “Joint event and temporal relation extraction
    with shared representations and structured prediction,” in EMNLP-IJCNLP, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] T. M. Nguyen and T. H. Nguyen, “One for all: Neural joint modeling of
    entities and events,” in AAAI, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. S. Chan, J. Fasching, H. Qiu, and B. Min, “Rapid customization for
    event extraction,” in ACL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] X. Li, F. Yin, Z. Sun, X. Li, A. Yuan, D. Chai, M. Zhou, and J. Li, “Entity-relation
    extraction as multi-turn question answering,” in ACL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] H. Yang, Y. Chen, K. Liu, Y. Xiao, and J. Zhao, “DCFEE: A document-level
    chinese financial event extraction system based on automatically labeled training
    data,” in ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Y. Zeng, Y. Feng, R. Ma, Z. Wang, R. Yan, C. Shi, and D. Zhao, “Scale
    up event extraction learning via automatic training data generation,” in AAAI,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Y. Zhao, X. Jin, Y. Wang, and X. Cheng, “Document embedding enhanced event
    detection with hierarchical and supervised attention,” in ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Y. Hong, W. Zhou, J. Zhang, Q. Zhu, and G. Zhou, “Self-regulation: Employing
    a generative adversarial network to improve event detection,” in ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] L. Sha, F. Qian, B. Chang, and Z. Sui, “Jointly extracting event triggers
    and arguments by dependency-bridge RNN and tensor-based argument interaction,”
    in AAAI, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] X. Liu, Z. Luo, and H. Huang, “Jointly multiple events extraction via
    attention-based graph information aggregation,” in EMNLP, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] S. Liu, Y. Chen, K. Liu, and J. Zhao, “Exploiting argument information
    to improve event detection via supervised attention mechanisms,” in ACL, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] L. Sha, J. Liu, C. Lin, S. Li, B. Chang, and Z. Sui, “RBPB: regularization-based
    pattern balancing method for event extraction,” in ACL, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Y. Zeng, H. Yang, Y. Feng, Z. Wang, and D. Zhao, “A convolution bilstm
    neural network model for chinese event extraction,” in NLPCC, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Y. Chen, S. Liu, S. He, K. Liu, and J. Zhao, “Event extraction via bidirectional
    long short-term memory tensor neural networks,” in CCL, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Z. Zhang, W. Xu, and Q. Chen, “Joint event extraction based on skip-window
    convolutional neural networks,” in NLPCC, 2016, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] T. H. Nguyen and R. Grishman, “Modeling skip-grams for event detection
    with convolutional neural networks,” in EMNLP, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y. Hei, R. Yang, H. Peng, L. Wang, X. Xu, J. Liu, H. Liu, J. Xu, and
    L. Sun, “HAWK: rapid android malware detection through heterogeneous graph attention
    networks,” CoRR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Q. Sun, J. Li, H. Peng, J. Wu, Y. Ning, P. S. Yu, and L. He, “SUGAR:
    subgraph neural network with reinforcement pooling and self-supervised mutual
    information mechanism,” in WWW, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Q. Sun, J. Li, H. Peng, J. Wu, X. Fu, C. Ji, and P. S. Yu, “Graph structure
    learning with variational information bottleneck,” in AAAI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. Liu, Y. Chen, K. Liu, W. Bi, and X. Liu, “Event extraction as machine
    reading comprehension,” in EMNLP, 2020, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Q. Li, S. Guo, J. Wu, J. Li, J. Sheng, L. Wang, X. Dong, and H. Peng,
    “Event extraction by associating event types and argument roles,” CoRR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] T. H. Nguyen and R. Grishman, “Event detection and domain adaptation
    with convolutional neural networks,” in ACL, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] X. Feng, L. Huang, D. Tang, H. Ji, B. Qin, and T. Liu, “A language-independent
    neural network for event detection,” in ACL, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] T. H. Nguyen and R. Grishman, “Graph convolutional networks with argument-aware
    pooling for event detection,” in AAAI, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] L. Yao, C. Mao, and Y. Luo, “Graph convolutional networks for text classification,”
    in AAAI, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and
    L. Zettlemoyer, “Deep contextualized word representations,” in NAACL-HLT, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] X. Du and C. Cardie, “Document-level event role filler extraction using
    multi-granularity contextualized encoding,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] X. Du, A. Rush, and C. Cardie, “GRIT: Generative role-filler transformers
    for document-level event entity extraction,” in ACL, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Fourth Message Uunderstanding Conference (MUC-4): Proceedings of a Conference
    Held in McLean, Virginia, June 16-18, 1992, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, “Multi-sentence
    argument linking,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Z. Zhang, X. Kong, Z. Liu, X. Ma, and E. Hovy, “A two-step approach for
    implicit event argument detection,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] H. Yang, Y. Chen, K. Liu, Y. Xiao, and J. Zhao, “DCFEE: A document-level
    Chinese financial event extraction system based on automatically labeled training
    data,” in ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] K.-H. Huang and N. Peng, “Document-level event extraction with efficient
    end-to-end learning of cross-event dependencies,” in Proceedings of the Third
    Workshop on Narrative Understanding, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] S. Li, H. Ji, and J. Han, “Document-level event argument extraction by
    conditional generation,” in ACL, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] H. Yang, D. Sui, Y. Chen, K. Liu, J. Zhao, and T. Wang, “Document-level
    event extraction via parallel prediction networks,” in ACL-IJCNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Y. Huang and W. Jia, “Exploring sentence community for document-level
    event extraction,” in EMNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] X. Du, A. Rush, and C. Cardie, “Template filling with generative transformers,”
    in ACL, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] L. Huang and H. Ji, “Semi-supervised new event type induction and event
    detection,” in EMNLP, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al., “Matching
    networks for one shot learning,” Advances in neural information processing systems,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in International Conference on Machine Learning,
    PMLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J. Sheng, S. Guo, Z. Chen, J. Yue, L. Wang, T. Liu, and H. Xu, “Adaptive
    attentional network for few-shot knowledge graph completion,” in EMNLP, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Z. Ye and Z. Ling, “Multi-level matching and aggregation network for
    few-shot relation classification,” in ACL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] V. D. Lai, F. Dernoncourt, and T. H. Nguyen, “Exploiting the matching
    information in the support set for few shot event classification,” Advances in
    Knowledge Discovery and Data Mining, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] V. D. Lai, F. Dernoncourt, and T. H. Nguyen, “Extensively matching for
    few-shot learning event detection,” CoRR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] S. Shen, T. Wu, G. Qi, Y. Li, G. Haffari, and S. Bi, “Adaptive knowledge-enhanced
    bayesian meta-learning for few-shot event detection,” in ACL/IJCNLP, Findings
    of ACL, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] V. Lai, F. Dernoncourt, and T. H. Nguyen, “Learning prototype representations
    across few-shot tasks for event detection,” in EMNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Zheng, F. Cai, W. Chen, W. Lei, and H. Chen, “Taxonomy-aware learning
    for few-shot event detection,” in WWW, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] V. D. Lai, M. V. Nguyen, T. H. Nguyen, and F. Dernoncourt, “Graph learning
    regularization and transfer learning for few-shot event detection,” in ACM SIGIR,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] S. Deng, N. Zhang, J. Kang, Y. Zhang, W. Zhang, and H. Chen, “Meta-learning
    with dynamic-memory-based prototypical network for few-shot event detection,”
    in WSDM, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] X. Cong, S. Cui, B. Yu, T. Liu, Y. Wang, and B. Wang, “Few-shot event
    detection with prototypical amortized conditional random field,” CoRR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] J. Chen, H. Lin, X. Han, and L. Sun, “Honey or poison? solving the trigger
    curse in few-shot event detection via causal intervention,” in EMNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Q. Lyu, H. Zhang, E. Sulem, and D. Roth, “Zero-shot event extraction
    via transfer learning: Challenges and insights,” in ACL/IJCNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] M. B. Ring et al., “Continual learning in reinforcement environments,”
    1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] S. Thrun, “Lifelong learning algorithms,” in Learning to learn, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] P. Cao, Y. Chen, J. Zhao, and T. Wang, “Incremental event detection via
    knowledge consolidation networks,” in EMNLP, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] A. Hsi, Y. Yang, J. G. Carbonell, and R. Xu, “Leveraging multilingual
    training for limited resource event extraction,” in COLING, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] J. Liu, Y. Chen, K. Liu, and J. Zhao, “Event detection via gated multilingual
    attention mechanism,” in AAAI, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Z. Chen and H. Ji, “Language specific issue and feature exploration in
    Chinese event extraction,” in HLT-NAACL (Short Papers), 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] P. Li, G. Zhou, Q. Zhu, and L. Hou, “Employing compositional semantics
    and discourse consistency in Chinese event extraction,” in EMNLP-CoNLL, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] B. Qin, Y. Zhao, X. Ding, T. Liu, and G. Zhai, “Event type recognition
    based on trigger expansion,” Tsinghua Science & Technology, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] P. Li and G. Zhou, “Employing morphological structures and sememes for
    Chinese event extraction,” in COLING, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] N. Xu, H. Xie, and D. Zhao, “A novel joint framework for multiple Chinese
    events extraction,” in CNCL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] H. Lin, Y. Lu, X. Han, and L. Sun, “Nugget proposal networks for Chinese
    event detection,” in ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] N. Ding, Z. Li, Z. Liu, H. Zheng, and Z. Lin, “Event detection with trigger-aware
    lattice neural network,” in EMNLP-IJCNLP, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Y. Zhang and J. Yang, “Chinese NER using lattice LSTM,” in ACL, (Melbourne,
    Australia), 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] S. Cui, B. Yu, X. Cong, T. Liu, Q. Li, and J. Shi, “Label enhanced event
    detection with heterogeneous graph attention networks,” ArXiv, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] S. Petrovic, M. Osborne, R. McCreadie, C. Macdonald, I. Ounis, and L. Shrimpton,
    “Can twitter replace newswire for breaking news?,” in ICWSM, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] V. Lai, M. Van Nguyen, H. Kaufman, and T. H. Nguyen, “Event extraction
    from historical texts: A new dataset for black rebellions,” in ACL-IJCNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] L. Huang, T. Cassidy, X. Feng, H. Ji, C. R. Voss, J. Han, and A. Sil,
    “Liberal event extraction and event schema induction,” in ACL, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] S. Deng, N. Zhang, J. Kang, Y. Zhang, W. Zhang, and H. Chen, “Meta-learning
    with dynamic-memory-based prototypical network for few-shot event detection,”
    in WSDM, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] A. M. Davani, L. Yeh, M. Atari, B. Kennedy, G. Portillo-Wightman, E. Gonzalez,
    N. Delong, R. Bhatia, A. Mirinjian, X. Ren, and M. Dehghani, “Reporting the unreported:
    Event extraction for analyzing the local representation of hate crimes,” in EMNLP-IJCNLP,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] X. Wang, Z. Wang, X. Han, W. Jiang, R. Han, Z. Liu, J. Li, P. Li, Y. Lin,
    and J. Zhou, “Maven: A massive general domain event detection dataset,” CoRR,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] H. Ji and R. Grishman, “Refining event extraction through cross-document
    inference,” in ACL, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] S. Liao and R. Grishman, “Using document level cross-event inference
    to improve event extraction,” in ACL, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Y. Hong, J. Zhang, B. Ma, J. Yao, G. Zhou, and Q. Zhu, “Using cross-entity
    inference to improve event extraction,” in ACL, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] J. Liu, Y. Chen, K. Liu, and J. Zhao, “Neural cross-lingual event detection
    with minimal parallel resources,” in EMNLP-IJCNLP, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] S. Li, H. Ji, and J. Han, “Document-level event argument extraction by
    conditional generation,” in NAACL-HLT, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] W. Li, D. Cheng, L. He, Y. Wang, and X. Jin, “Joint event extraction
    based on hierarchical event schemas from framenet,” IEEE Access, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] K. Liu, Y. Chen, J. Liu, X. Zuo, and J. Zhao, “Extracting events and
    their relations from texts: A survey on recent research progress and challenges,”
    AI Open, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] X. Ding, Z. Li, T. Liu, and K. Liao, “ELG: an event logic graph,” CoRR,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] J. Liu, L. Min, and X. Huang, “An overview of event extraction and its
    applications,” CoRR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] T. S. Costa, S. Gottschalk, and E. Demidova, “Event-qa: A dataset for
    event-centric question answering over knowledge graphs,” in CIKM, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] J. Wang, A. Jatowt, M. Färber, and M. Yoshikawa, “Answering event-related
    questions over long-term news article archives,” in ECIR, Lecture Notes in Computer
    Science, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] R. Han, I. Hsu, J. Sun, J. Baylon, Q. Ning, D. Roth, and N. Peng, “ESTER:
    A machine reading comprehension dataset for reasoning about event semantic relations,”
    in EMNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] P. Cao, Y. Chen, Y. Yang, K. Liu, and J. Zhao, “Uncertain local-to-global
    networks for document-level event factuality identification,” in EMNLP, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] J. Pustejovsky and R. Saurí, “A factuality profiler for eventualities
    in text,” 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] R. Saurí and J. Pustejovsky, “Are you sure that this happened? assessing
    the factuality degree of events in text,” Comput. Linguistics, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] M. de Marneffe, C. D. Manning, and C. Potts, “Did it happen? the pragmatic
    complexity of veridicality assessment,” Comput. Linguistics, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] R. Rudinger, A. S. White, and B. V. Durme, “Neural models of factuality,”
    in NAACL-HLT, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] A. P. B. Veyseh, T. H. Nguyen, and D. Dou, “Graph based neural networks
    for event factuality prediction using syntactic and semantic structures,” in ACL,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] R. Nairn, C. Condoravdi, and L. Karttunen, “Computing relative polarity
    for textual inference,” in Proceedings of the Fifth International Workshop on
    Inference in Computational Semantics (ICoS-5), 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] A. Lotan, A. Stern, and I. Dagan, “Truthteller: Annotating predicate
    truth,” in NAACL-HLT, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] M. T. Diab, L. S. Levin, T. Mitamura, O. Rambow, V. Prabhakaran, and
    W. Guo, “Committed belief annotation and tagging,” in LAW, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] V. Prabhakaran, O. Rambow, and M. T. Diab, “Automatic committed belief
    tagging,” in COLING, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] K. Lee, Y. Artzi, Y. Choi, and L. Zettlemoyer, “Event detection and factuality
    assessment with non-expert supervision,” in EMNLP, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Z. Qian, P. Li, and Q. Zhu, “A two-step approach for event factuality
    identification,” in IALP, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Z. Qian, P. Li, Y. Zhang, G. Zhou, and Q. Zhu, “Event factuality identification
    via generative adversarial networks with auxiliary classification,” in IJCAI,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] J. Sheng, B. Zou, Z. Gong, Y. Hong, and G. Zhou, “Chinese event factuality
    detection,” in NLPCC, Lecture Notes in Computer Science, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] R. Huang, B. Zou, H. Wang, P. Li, and G. Zhou, “Event factuality detection
    in discourse,” in NLPCC, Lecture Notes in Computer Science, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. C. Courville, and Y. Bengio, “Generative adversarial nets,” in NeurIPS,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in ICLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Z. Qian, P. Li, Q. Zhu, and G. Zhou, “Document-level event factuality
    identification via adversarial neural network,” in NAACL-HLT, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] A. Cybulska and P. Vossen, “Translating granularity of event slots into
    features for event coreference resolution.,” in EVENTS@HLP-NAACL, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] J. Lu and V. Ng, “Learning antecedent structures for event coreference
    resolution,” in ICMLA, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] C. A. Bejan and S. M. Harabagiu, “Unsupervised event coreference resolution
    with rich linguistic features,” in ACL, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] P. K. Choubey and R. Huang, “Improving event coreference resolution by
    modeling correlations between event coreference chains and document topic structures,”
    in ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Y. J. Huang, J. Lu, S. Kurohashi, and V. Ng, “Improving event coreference
    resolution by learning argument compatibility from unlabeled data,” in ACL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Z. Chen, H. Ji, and R. M. Haralick, “A pairwise event coreference model,
    feature impact and evaluation for event coreference resolution,” in Proceedings
    of the workshop on events in emerging text types, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Z. Chen and H. Ji, “Graph-based event coreference resolution,” in Graph-based
    Methods for Natural Language Processing (TextGraphs-4), 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] P. K. Choubey and R. Huang, “Event coreference resolution by iteratively
    unfolding inter-dependencies among events,” CoRR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] C. Chen and V. Ng, “Joint inference over a lightly supervised information
    extraction pipeline: Towards event coreference resolution for resource-scarce
    languages,” in AAAI, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] J. Lu, D. Venugopal, V. Gogate, and V. Ng, “Joint inference for event
    coreference resolution,” in COLING, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] J. Lu and V. Ng, “Joint learning for event coreference resolution,” in
    ACL, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] J. Araki and T. Mitamura, “Joint event trigger identification and event
    coreference resolution with structured perceptron,” in EMNLP, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] S. Barhom, V. Shwartz, A. Eirew, M. Bugert, N. Reimers, and I. Dagan,
    “Revisiting joint modeling of cross-document entity and event coreference resolution,”
    CoRR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] P. Mirza, “Extracting temporal and causal relations between events,”
    CoRR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] M. Riaz and R. Girju, “Another look at causality: Discovering scenario-specific
    contingency relationships with no supervision,” in 2010 IEEE Fourth International
    Conference on Semantic Computing, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] M. Riaz and R. Girju, “Toward a better understanding of causality between
    verbal events: Extraction and analysis of the causal power of verb-verb associations,”
    in SIGDIAL Conference, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] C. Hashimoto, K. Torisawa, J. Kloetzer, M. Sano, I. Varga, J.-H. Oh,
    and Y. Kidawara, “Toward future scenario generation: Extracting event causality
    exploiting semantic relation, context, and association features,” in ACL, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] P. Mirza and S. Tonelli, “An analysis of causality between events and
    its relation to temporal information,” in COLING, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] P. Mirza and S. Tonelli, “Catena: Causal and temporal relation extraction
    from natural language texts,” in COLING, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Z. Hu and M. Walker, “Inferring narrative causality between event pairs
    in films,” in SIGDIAL Conference, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] L. Gao, P. K. Choubey, and R. Huang, “Modeling document-level causal
    structures for event causal relation identification,” in ACL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] K. Kadowaki, R. Iida, K. Torisawa, J.-H. Oh, and J. Kloetzer, “Event
    causality recognition exploiting multiple annotators’ judgments and background
    knowledge,” in EMNLP-IJCNLP, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] H. Rashkin, M. Sap, E. Allaway, N. A. Smith, and Y. Choi, “Event2mind:
    Commonsense inference on events, intents, and reactions,” CoRR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] N. Mostafazadeh, A. Kalyanpur, L. Moon, D. Buchanan, L. Berkowitz, O. Biran,
    and J. Chu-Carroll, “Glucose: Generalized and contextualized story explanations,”
    CoRR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] J. Liu, Y. Chen, and J. Zhao, “Knowledge enhanced event causality identification
    with mention masking generalizations.,” in IJCAI, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] P. Cao, X. Zuo, Y. Chen, K. Liu, J. Zhao, Y. Chen, and W. Peng, “Knowledge-enriched
    event causality identification via latent structure induction networks,” in ACL/IJCNLP,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] M. Riaz and R. Girju, “Recognizing causality in verb-noun pairs via noun
    and verb semantics,” in EACL, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] X. Zuo, Y. Chen, K. Liu, and J. Zhao, “KnowDis: Knowledge enhanced data
    augmentation for event causality detection via distant supervision,” in COLING,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] J. Pustejovsky, J. M. Castano, R. Ingria, R. Sauri, R. J. Gaizauskas,
    A. Setzer, G. Katz, and D. R. Radev, “Timeml: Robust specification of event and
    temporal expressions in text.,” New directions in question answering, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] C. Hagège and X. Tannier, “Xrce-t: Xip temporal module for tempeval campaign.,”
    in SemEval, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] A. Chang and C. D. Manning, “Sutime: Evaluation in tempeval-3,” in SemEval,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] N. Chambers, T. Cassidy, B. McDowell, and S. Bethard, “Dense event ordering
    with a multi-pass architecture,” TACL, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and J. Pustejovsky, “Machine
    learning of temporal relations,” in ACL, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Q. Ning, Z. Feng, and D. Roth, “A structured learning approach to temporal
    relation extraction,” CoRR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] D. Dligach, T. Miller, C. Lin, S. Bethard, and G. Savova, “Neural temporal
    relation extraction,” in ACL (Short Papers), 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] J. Tourille, O. Ferret, A. Neveol, and X. Tannier, “Neural architecture
    for temporal relation extraction: A bi-lstm approach for detecting narrative containers,”
    in ACL, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] F. Cheng and Y. Miyao, “Classifying temporal relations by bidirectional
    lstm over dependency paths,” in ACL (Short Papers), 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] R. Han, Y. Zhou, and N. Peng, “Domain knowledge empowered structured
    neural net for end-to-end event temporal relation extraction,” CoRR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] H. Ross, J. Cai, and B. Min, “Exploring contextualized neural language
    models for temporal dependency parsing,” CoRR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] R. C. Schank and R. P. Abelson, “Scripts, plans, goals, and understanding,”
    1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] D. Bean and E. Riloff, “Unsupervised learning of contextual role knowledge
    for coreference resolution,” in HLT-NAACL, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] S. Chaturvedi, H. Peng, and D. Roth, “Story comprehension for predicting
    what happens next,” in EMNLP, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] Y. Yang, Z. Wei, Q. Chen, and L. Wu, “Using external knowledge for financial
    event prediction based on graph neural networks,” CIKM, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] M. Granroth-Wilding and S. Clark, “What happens next? event prediction
    using a compositional neural network model,” in AAAI, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] G. David and C. Cieri, “Graff david and christopher cieri,” in Philadelphia:
    Linguistic Data Consortium, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] N. Chambers and D. Jurafsky, “Unsupervised learning of narrative event
    chains,” in ACL, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] B. Jans, S. Bethard, I. Vulić, and M. F. Moens, “Skip n-grams and ranking
    functions for predicting script events,” in ACL, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] K. Pichotta and R. Mooney, “Statistical script learning with multi-argument
    events,” in ACL, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] R. Rudinger, P. Rastogi, F. Ferraro, and B. Van Durme, “Script induction
    as language modeling,” in EMNLP, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] S. Lv, W. Qian, L. Huang, J. Han, and S. Hu, “Sam-net: Integrating event-level
    and chain-level attentions to predict what happens next,” in AAAI, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] K. Pichotta and R. Mooney, “Learning statistical scripts with lstm recurrent
    neural networks,” in AAAI, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Z. Wang, Y. Zhang, and C.-Y. Chang, “Integrating order information and
    event relation for script event prediction,” in EMNLP, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] L. Wang, J. Yue, S. Guo, J. Sheng, Q. Mao, Z. Chen, S. Zhong, and C. Li,
    “Multi-level connection enhanced representation learning for script event prediction,”
    WWW, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] J. Zheng, F. Cai, Y. Ling, and H. Chen, “Heterogeneous graph neural networks
    to predict what happen next,” in COLING, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] X. Ding, K. Liao, T. Liu, Z. Li, and J. Duan, “Event representation learning
    enhanced with external commonsense knowledge,” in EMNLP-IJCNLP, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] H. Rashkin, M. Sap, E. Allaway, N. A. Smith, and Y. Choi, “Event2Mind:
    Commonsense inference on events, intents, and reactions,” in ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] M. Sap, R. L. Bras, E. Allaway, C. Bhagavatula, N. Lourie, H. Rashkin,
    B. Roof, N. A. Smith, and Y. Choi, “Atomic: An atlas of machine commonsense for
    if-then reasoning,” ArXiv, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] S. Lv, F. Zhu, and S. Hu, “Integrating external event knowledge for script
    learning,” in COLING, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] H. Zhang, X. Liu, H. Pan, Y. Song, and C. Leung, “Aser: A large-scale
    eventuality knowledge graph,” WWW, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] K. Huang, M. Yang, and N. Peng, “Biomedical event extraction on graph
    edge-conditioned attention networks with hierarchical knowledge graphs,” in EMNLP,
    Findings of ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] J. A. Vanegas, S. Matos, F. A. González, and J. L. Oliveira, “An overview
    of biomolecular event extraction from scientific documents,” Comput. Math. Methods
    Medicine, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] J. Björne and T. Salakoski, “Generalizing biomedical event extraction,”
    in BioNLP@ACL (Shared Task), 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] N. Chambers, T. Cassidy, B. McDowell, and S. Bethard, “Dense event ordering
    with a multi-pass architecture,” Trans. Assoc. Comput. Linguistics, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] J. Björne and T. Salakoski, “Biomedical event extraction using convolutional
    neural networks and dependency parsing,” in BioNLP, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] L. Li, Y. Liu, and M. Qin, “Extracting biomedical events with parallel
    multi-pooling convolutional neural networks,” IEEE ACM Trans. Comput. Biol. Bioinform.,
    no. 2, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] H. Trieu, T. T. Tran, A. D. Nguyen, A. Nguyen, M. Miwa, and S. Ananiadou,
    “Deepeventmine: end-to-end neural nested event extraction from biomedical texts,”
    Bioinform., no. 19, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] Z. Tang, G. Hahn-Powell, and M. Surdeanu, “Exploring interpretability
    in event extraction: Multitask learning of a neural event classifier and an explanation
    decoder,” in ACL, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] G. Frisoni, G. Moro, and A. Carbonaro, “A survey on event extraction
    for natural language understanding: Riding the biomedical literature wave,” IEEE
    Access, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/823b0b8549860eb8ea3e66fb25d363e5.png) | Qian Li
    is currently pursuing the Ph.D. degree with the School of Computer Science and
    Engineering, and Beijing Advanced Innovation Center for Big Data and Brain Computing
    in Beihang University. Her research interests include knowledge graph and information
    extraction. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/4ecedda64eb1c5a3536dcaa38b876ff0.png) | Jianxin
    Li is currently a Professor with the School of Computer Science and Engineering,
    and Beijing Advanced Innovation Center for Big Data and Brain Computing in Beihang
    University. His current research interests include social networks, machine learning,
    big data, and trustworthy computing. Dr. Li has published research papers in top-tier
    journals and conferences, including the IEEE TKDE, TDSC, Journal of Artificial
    Intelligence Research (JAIR), Association for Computing Machinery Transactions
    on Information Systems (ACM TOIS), ACM Transactions on Knowledge Discovery from
    Data (TKDD), Knowledge Discovery and Data Mining (KDD), Association for the Advancement
    of Artificial Intelligence (AAAI), and The International Conference of World Wide
    Web (WWW). |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/300d846e7ef3527db673225e186f3e4e.png) | Jiawei
    Sheng is currently pursuing the Ph.D. degree in the Institute of Information Engineering,
    Chinese Academy of Sciences. His current research interests include Information
    Extraction, Knowledge Graph Embedding and Knowledge Acquisition. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/b88a056ac81796a364fb6b989d993c5c.png) | Shiyao
    Cui is currently pursuing the Ph.D. degree in the Institute of Information Engineering,
    Chinese Academy of Sciences. Her current research interests include Event Extraction,
    Event Relation Identification and Script Event Prediction. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/d14b580f6365d9231f97712446464d93.png) | Jia Wu
    received the Ph.D. degree in computer science from the University of Technology
    Sydney, Ultimo, NSW, Australia. Dr Wu is currently an Australian Research Council
    Discovery Early Career Researcher Award (ARC DECRA) Fellow in the School of Computing,
    Macquarie University, Sydney, Australia. His current research interests include
    data mining and machine learning. Since 2009, he has published 100+ refereed journal
    and conference papers, including IEEE TPAMI, IEEE TKDE, IEEE TNNLS, IEEE TMM,
    ACM TKDD, Neural Information Processing Systems (NIPS), WWW, and ACM’s Special
    Interest Group on Knowledge Discovery and Data Mining (ACM SIGKDD). |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/ec1f229aca96ebf51dd2593ca793fffe.png) | Yiming
    Hei is currently pursuing the Ph.D. degree in the School of Cyber Science and
    Technology, Beihang University. His research interests include Graph Embedding,
    Information Extraction and Application Security. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/a02734b6f865d140fde9cd6e4dfeb71e.png) | Hao Peng
    is currently an Assistant Professor at the School of Cyber Science and Technology,
    and Beijing Advanced Innovation Center for BigData and Brain Computing in Beihang
    University. His research interests include representation learning, machine learning
    and graph mining. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/e96bb2a8ab7d5c096327bbe58e1ab863.png) | Shu Guo
    received Ph.D. degree from the Institute of Information Engineering, Chinese Academy
    of Sciences. She is currently working at the National Computer Network Emergency
    Response Technical Team/Coordination Center of China. Her research interests include
    Knowledge Graph Embedding, Knowledge Acquisition and Web Mining. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/a4707415ea98235dbb19e577eb9ee060.png) | Lihong
    Wang is currently a Professor with the National Computer Network Emergency Response
    Technical Team/Coordination Center of China. Her current research interests include
    information security, cloud computing, big data mining and analytics, information
    retrieval, and data mining. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/b6a5d92f6efc5ce8a5f151321ef8232b.png) | Amin Beheshti
    is the Director of AI-enabled Processes (AIP) Research Centre and the head of
    the Data Analytics Research Lab, Department of Computing, Macquarie University.
    He is also a Senior Lecturer in Data Science at Macquarie University and an Adjunct
    Academic in Computer Science at UNSW Sydney. Amin completed his Ph.D. and Postdoc
    in Computer Science and Engineering in UNSW Sydney and held a Master and Bachelor
    in Computer Science both with First Class Honours. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/9d1ab7a34eda5556b62479e475df34b3.png) | Philip
    S. Yu is a Distinguished Professor and the Wexler Chair in Information Technology
    at the Department of Computer Science, University of Illinois at Chicago and also
    holds the Wexler Chair in Information Technology. Before joining UIC, he was at
    the IBM Watson Research Center. He is a Fellow of the ACM and IEEE. Dr. Yu was
    the Editor-in-Chiefs of ACM Transactions on Knowledge Discovery from Data (2011-2017)
    and IEEE Transactions on Knowledge and Data Engineering (2001-2004). |'
  prefs: []
  type: TYPE_TB
