- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:00:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:00:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2007.09637] Survey on Deep Learning-based Kuzushiji Recognition'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2007.09637] 深度学习基础的楷书识别调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2007.09637](https://ar5iv.labs.arxiv.org/html/2007.09637)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2007.09637](https://ar5iv.labs.arxiv.org/html/2007.09637)
- en: Survey on Deep Learning-based
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习基础的楷书识别调查
- en: Kuzushiji Recognition
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 楷书识别
- en: Kazuya Ueki School of Information Science
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 上木和也 信息科学学院
- en: Meisei University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 明星大学
- en: 'Email: kazuya.ueki@meisei-u.ac.jp    Tomoka Kojima School of Information Science'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 邮箱：kazuya.ueki@meisei-u.ac.jp    小岛智佳 信息科学学院
- en: Meisei University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 明星大学
- en: 'Email: 18j5061@stu.meisei-u.ac.jp'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 邮箱：18j5061@stu.meisei-u.ac.jp
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Owing to the overwhelming accuracy of the deep learning method demonstrated
    at the 2012 image classification competition, deep learning has been successfully
    applied to a variety of other tasks. The high-precision detection and recognition
    of Kuzushiji, a Japanese cursive script used for transcribing historical documents,
    has been made possible through the use of deep learning. In recent years, competitions
    on Kuzushiji recognition have been held, and many researchers have proposed various
    recognition methods. This study examines recent research trends, current problems,
    and future prospects in Kuzushiji recognition using deep learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于2012年图像分类竞赛中展示的深度学习方法的超高准确性，深度学习已经成功应用于多种其他任务。通过使用深度学习，已实现对楷书这一用于转录历史文档的日本草书脚本的高精度检测和识别。近年来，楷书识别竞赛已经举行，许多研究人员提出了各种识别方法。本研究考察了使用深度学习进行楷书识别的近期研究趋势、当前问题和未来前景。
- en: I Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Kuzushiji has been commonly used in Japan for more than a thousand years. However,
    since the 1900s, schools have no longer been teaching Kuzushiji, and only a few
    thousand people in Japan can currently read and understand it. Hiragana characters¹¹1Hiragana
    is one of the three different character sets used in Japanese writing. Each Hiragana
    character represents a particular syllable. There are 46 basic characters. have
    a root Kanji²²2Kanji is another one of the three character sets used in the Japanese
    language. Along with syllabaries, Kanji is made up of ideographic characters,
    and each letter symbolizes a specific meaning. Most Kanji characters were imported
    from China, although some were developed in Japan. Although there are approximately
    50,000 Kanji characters, only approximately 2,500 are actually used in daily life
    in Japan. called a Jibo³³3A Jibo is a root Kanji character of Hiragana. For example,
    the character “あ” is derived from different Jibos including “安” and “阿.”, leading
    to various shapes for a single character; training is required to read characters
    that differ from modern Hiragana. For this reason, many researchers have been
    working on Kuzushiji recognition using machine learning techniques. Recently,
    with the advent of deep learning, research on Kuzushiji recognition has accelerated
    and the accuracy of the methods has significantly improved. In this paper, we
    present a survey and analysis of recent methods of Kuzushiji recognition based
    on deep learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 楷书在日本已经使用了超过一千年。然而，自1900年代起，学校不再教授楷书，目前日本只有几千人能读懂和理解它。平假名¹¹平假名是日语书写中使用的三种字符集之一。每个平假名字符代表一个特定的音节，共有46个基本字符。具有根汉字²²汉字是日语使用的三种字符集之一。与音节字母一起，汉字由表意字符组成，每个字母象征特定的含义。大多数汉字来自中国，尽管有些是在日本发展出来的。虽然汉字约有50,000个，但实际上在日本日常生活中只使用约2,500个。称为字母³³字母是平假名的根汉字。例如，“あ”字符来源于不同的字母，包括“安”和“阿”。，导致一个字符有多种形状；阅读与现代平假名不同的字符需要训练。因此，许多研究人员一直在使用机器学习技术进行楷书识别。近年来，随着深度学习的出现，楷书识别的研究加速了，方法的准确性也显著提高。本文对基于深度学习的楷书识别的近期方法进行了调查和分析。
- en: II Representative Research on Kuzushiji Recognition
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 楷书识别的代表性研究
- en: Many studies on Kuzushiji recognition were conducted prior to the introduction
    of deep learning. The “Historical Character Recognition Project” [[1](#bib.bib1)],
    which was initiated in 1999, reported the development of a system to support the
    transcription of ancient documents. In this project, to develop a historical document
    research support system, the authors studied a character database, corpus, character
    segmentation, character recognition, intellectual transcription support system,
    and a digital dictionary. Specifically, they developed a computerized historical
    character dictionary using stroke information [[2](#bib.bib2)] as well as Japanese
    off-line hand-written optical character recognition (OCR) technology, and implemented
    a Kuzushiji recognition system for 67,739 categories by combining on-line and
    off-line recognition methods [[3](#bib.bib3)]. Other methods, such as [[4](#bib.bib4)][[5](#bib.bib5)],
    which is a recognition method using self-organizing maps, and [[6](#bib.bib6)],
    which is a recognition method using a neocognitron, have also been proposed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习引入之前，已有许多关于草书识别的研究。1999年启动的“历史字符识别项目”[[1](#bib.bib1)]报告了一个支持古文献转录的系统的开发。在这个项目中，为了开发一个历史文献研究支持系统，作者研究了字符数据库、语料库、字符分割、字符识别、智能转录支持系统和数字词典。具体来说，他们利用笔画信息[[2](#bib.bib2)]开发了一个计算机化的历史字符词典，并实现了67,739个类别的草书识别系统，结合了在线和离线识别方法[[3](#bib.bib3)]。其他方法，如使用自组织映射的识别方法[[4](#bib.bib4)][[5](#bib.bib5)]以及使用新认知网络的识别方法[[6](#bib.bib6)]，也已被提出。
- en: Since the introduction of deep learning, further research on Kuzushiji recognition
    has become increasingly active, and various methods have been proposed. During
    the early introductory stage of deep learning, most recognition methods [[7](#bib.bib7)][[8](#bib.bib8)]
    were based on approximately 50 different Hiragana images with recognition rates
    of 75% to 90%. There were also studies in which more than 1,000 characters including
    Hiragana, and Katakana⁴⁴4In the same way as Hiragana, Katakana is one of the three
    different character sets used in Japanese. Katakana is also a phonetic syllabary,
    in which each letter represents the sound of a syllable. There are also 46 basic
    characters. and Kanji, were recognized [[9](#bib.bib9)], along with the results
    of character recognition in documents of the Taiwan Governor’s Office, which dealt
    with more than 3,000 characters [[10](#bib.bib10)]. In these studies, the problems
    of large numbers of classes, an unbalanced number of data between classes, and
    a variation of characters were solved through a data augmentation commonly used
    in deep learning training.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自深度学习引入以来，对**草书识别**的进一步研究变得越来越活跃，并提出了各种方法。在深度学习的早期引入阶段，大多数识别方法[[7](#bib.bib7)][[8](#bib.bib8)]
    基于大约50种不同的平假名图像，识别率为75%到90%。也有研究识别了包含平假名、片假名⁴⁴以及汉字的1000多个字符[[9](#bib.bib9)]，还有处理超过3000个字符的台湾省政府办公室文件中的字符识别结果[[10](#bib.bib10)]。在这些研究中，通过深度学习训练中常用的数据增强技术解决了类别数量多、类别间数据不平衡以及字符变异的问题。
- en: In addition, a network that outputs a three-character string has also been proposed
    as a method for recognizing consecutive characters [[11](#bib.bib11)]. This method
    uses single-character and binary classifiers to distinguish between characters;
    the character strings are then recognized using bidirectional long short-term
    memory (BLSTM). The authors reported that the recognition rate of a single character
    was approximately 92%; however, the recognition rate of three characters was only
    approximately 76%. Similarly, a method for recognizing a string of three consecutive
    characters using a sliding window and BLSTM was proposed [[12](#bib.bib12)]. The
    authors used the tendency in which the maximum output probability of a neural
    network is not particularly high for a misaligned character image but is high
    for an accurately aligned image, and increased the recognition rate to 86% by
    integrating multiple results. In addition, a deep learning method for recognizing
    a series of Kuzushiji phrases using an image from “The Tale of Genji” was proposed
    [[13](#bib.bib13)]. An end-to-end method with an attention mechanism was applied
    to recognize consecutive Kuzushiji characters within phrases. This method can
    recognize phrases written in Hiragana (47 different characters) with 78.92% accuracy,
    and phrases containing both Kanji (63 different characters) and Hiragana with
    59.80% accuracy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，已经提出了一种输出三字符字符串的网络作为识别连续字符的方法 [[11](#bib.bib11)]。该方法使用单字符和二进制分类器来区分字符；然后使用双向长短期记忆网络（BLSTM）对字符字符串进行识别。作者报告称，单字符的识别率约为92%；然而，三字符的识别率仅约为76%。类似地，还提出了一种使用滑动窗口和BLSTM来识别连续三字符字符串的方法
    [[12](#bib.bib12)]。作者利用神经网络的最大输出概率在字符图像对齐不准确时不会特别高，但在对齐准确时会很高的趋势，通过整合多个结果将识别率提高到86%。此外，还提出了一种使用《源氏物语》图像来识别一系列古字文短语的深度学习方法
    [[13](#bib.bib13)]。该方法应用了具有注意机制的端到端方法来识别短语中的连续古字文字符。该方法可以以78.92%的准确率识别用平假名（47个不同字符）书写的短语，以及以59.80%的准确率识别同时包含汉字（63个不同字符）和平假名的短语。
- en: In recent years, research on Kuzushiji recognition has become increasingly active
    since the Kuzushiji dataset first became publicly available [[14](#bib.bib14)].
    With the development of this database, a PRMU algorithm contest described in [IV-A](#S4.SS1
    "IV-A PRMU Algorithm Contest ‣ IV Benchmarks ‣ Survey on Deep Learning-based Kuzushiji
    Recognition") and a Kaggle competition introduced in [IV-B](#S4.SS2 "IV-B Kaggle
    Competition ‣ IV Benchmarks ‣ Survey on Deep Learning-based Kuzushiji Recognition")
    were held, and many researchers have started to work on Kuzushiji recognition.
    The preparation, progress, and results of a Kuzushiji recognition competition,
    knowledge obtained from the competition, and the value of utilizing machine learning
    competitions have also been reported [[15](#bib.bib15)][[16](#bib.bib16)]. In
    these reports, the results of the Kuzushiji recognition competition showed that
    existing object detection algorithms such as a Faster R-CNN [[17](#bib.bib17)]
    and cascade R-CNN [[18](#bib.bib18)] are also effective for Kuzushiji detection.
    At the forefront of Kuzushiji recognition, end-to-end approaches for actual transcriptions
    are becoming the mainstream. As a representative method, an end-to-end method,
    KuroNet, was proposed to recognize whole pages of text using the U-Net architecture
    [[19](#bib.bib19)][[20](#bib.bib20)]. The authors demonstrated that KuroNet can
    handle long-range contexts, large vocabularies, and non-standardized character
    layouts by predicting the location and identity of all characters given a page
    of text without any preprocessing. To recognize multiple lines of historical documents,
    a document reading system inspired by human eye movements was proposed, and the
    results of evaluations of the PRMU algorithm contest database described in [IV-A](#S4.SS1
    "IV-A PRMU Algorithm Contest ‣ IV Benchmarks ‣ Survey on Deep Learning-based Kuzushiji
    Recognition") [[21](#bib.bib21)] and the Kaggle competition database described
    in [IV-B](#S4.SS2 "IV-B Kaggle Competition ‣ IV Benchmarks ‣ Survey on Deep Learning-based
    Kuzushiji Recognition") [[22](#bib.bib22)] were reported. In addition, a two-dimensional
    context box proposal network used to detect Kuzushiji in historical documents
    was proposed [[23](#bib.bib23)]. The authors employed VGG16 to extract features
    from an input image and BLSTM [[24](#bib.bib24)] for exploring the vertical and
    horizontal dimensions, and then predicted the bounding boxes from the output of
    the two-dimensional context.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，自从Kuzushiji数据集首次公开以来，对Kuzushiji识别的研究变得越来越活跃[[14](#bib.bib14)]。随着这一数据库的发展，描述于[IV-A](#S4.SS1
    "IV-A PRMU Algorithm Contest ‣ IV Benchmarks ‣ Survey on Deep Learning-based Kuzushiji
    Recognition")的PRMU算法竞赛和在[IV-B](#S4.SS2 "IV-B Kaggle Competition ‣ IV Benchmarks
    ‣ Survey on Deep Learning-based Kuzushiji Recognition")中引入的Kaggle竞赛相继举行，许多研究人员开始从事Kuzushiji识别工作。关于Kuzushiji识别竞赛的准备、进展和结果，竞赛中获得的知识，以及利用机器学习竞赛的价值也已被报道[[15](#bib.bib15)][[16](#bib.bib16)]。在这些报告中，Kuzushiji识别竞赛的结果显示，现有的目标检测算法如Faster
    R-CNN[[17](#bib.bib17)]和cascade R-CNN[[18](#bib.bib18)]对Kuzushiji检测也有效。在Kuzushiji识别的前沿，端到端的实际转录方法正在成为主流。作为代表性方法，提出了一种端到端方法KuroNet，利用U-Net架构识别整页文本[[19](#bib.bib19)][[20](#bib.bib20)]。作者展示了KuroNet可以处理长距离上下文、大词汇量和非标准化字符布局，通过预测整页文本中所有字符的位置和身份，无需任何预处理。为了识别多行历史文献，提出了一种受人眼运动启发的文档阅读系统，并报告了在[IV-A](#S4.SS1
    "IV-A PRMU Algorithm Contest ‣ IV Benchmarks ‣ Survey on Deep Learning-based Kuzushiji
    Recognition")[[21](#bib.bib21)]和[IV-B](#S4.SS2 "IV-B Kaggle Competition ‣ IV Benchmarks
    ‣ Survey on Deep Learning-based Kuzushiji Recognition")[[22](#bib.bib22)]中描述的PRMU算法竞赛数据库和Kaggle竞赛数据库的评估结果。此外，还提出了一种用于检测历史文献中Kuzushiji的二维上下文框提议网络[[23](#bib.bib23)]。作者使用VGG16从输入图像中提取特征，并采用BLSTM[[24](#bib.bib24)]探索纵向和横向维度，然后从二维上下文的输出中预测边界框。
- en: In a study on the practical aspects of transcription, a new type of OCR technology
    was proposed to reduce the labor of a high-load transcription [[25](#bib.bib25)].
    This technology is not a fully automated process but aims to save labor by dividing
    tasks between experts and non-experts and applying an automatic processing. The
    authors stated that a translation can be made quickly and accurately by not aiming
    at a decoding accuracy of 100% using only the OCR of automatic processing, but
    by leaving the characters with a low degree of certainty as a “〓 (geta)” and entrusting
    the evaluation to experts in the following process. A method for automatically
    determining which characters should be held for evaluation using a machine learning
    technique was also proposed [[26](#bib.bib26)][[27](#bib.bib27)]. This method
    can automatically identify difficult-to-recognize characters or characters that
    were not used during training based on the confidence level obtained from the
    neural network.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于抄写实际方面的研究中，提出了一种新的OCR技术来减少高负荷抄写的劳动[[25](#bib.bib25)]。该技术不是完全自动化的过程，而是通过将任务分配给专家和非专家并应用自动处理来节省劳动。作者指出，通过不追求100%的解码准确率，仅使用自动处理的OCR，而是将不确定度高的字符标记为“〓（geta）”并在后续过程中交给专家评估，可以快速准确地进行翻译。还提出了一种利用机器学习技术自动确定哪些字符应保留进行评估的方法[[26](#bib.bib26)][[27](#bib.bib27)]。该方法可以根据从神经网络获得的置信度自动识别难以识别的字符或训练期间未使用的字符。
- en: A study on an interface of a Kuzushiji recognition system also introduced a
    system that can operate on a Raspberry Pi without problems and with almost the
    same processing time and accuracy as in previous studies; there was also no need
    for a high-performance computer [[28](#bib.bib28)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一项关于Kuzushiji识别系统界面的研究还介绍了一个能够在Raspberry Pi上运行的系统，该系统在处理时间和准确性上几乎与之前的研究相同，并且不需要高性能计算机[[28](#bib.bib28)]。
- en: As another area of focus, a framework for assisting humans in reading Japanese
    historical manuscripts, formulated as a constraint satisfaction problem, and a
    system for transcribing Kuzushiji and its graphical user interface have also been
    introduced [[29](#bib.bib29)][[30](#bib.bib30)].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个研究重点，还介绍了一个旨在帮助人类阅读日本历史手稿的框架，该框架被表述为约束满足问题，并且还介绍了一个用于抄写Kuzushiji及其图形用户界面的系统[[29](#bib.bib29)][[30](#bib.bib30)]。
- en: An interactive system was also been proposed to assist in the transcription
    of digitized Japanese historical woodblock-printed books [[31](#bib.bib31)]. This
    system includes a layout analysis, character segmentation, transcription, and
    the generation of a character image database. The procedures for applying the
    system consist of two major phases. During the first phase, the system automatically
    produces provisional character segmentation data, and users interactively edit
    and transcribe the data into text data for storage in the character image database.
    During the second phase, the system conducts automatic character segmentation
    and transcription using the database generated during the first phase. By repeating
    the first and second phases with a variety of materials, the contents of the character
    image database can be enhanced and the performance of the system in terms of character
    segmentation and transcription will increase.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 还提出了一个互动系统来辅助数字化日本历史木刻书籍的抄写[[31](#bib.bib31)]。该系统包括布局分析、字符分割、抄写和字符图像数据库生成。应用该系统的过程包括两个主要阶段。在第一个阶段，系统自动生成临时字符分割数据，用户互动地编辑和抄写这些数据，将其存储在字符图像数据库中。在第二个阶段，系统使用在第一个阶段生成的数据库进行自动字符分割和抄写。通过在各种材料上重复第一阶段和第二阶段，可以增强字符图像数据库的内容，并提高系统在字符分割和抄写方面的性能。
- en: III Datasets
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 数据集
- en: III-A Kuzushiji Dataset
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A Kuzushiji 数据集
- en: '![Refer to caption](img/121eecc3e1a04b4081ea973fc09675b4.png)![Refer to caption](img/53e7ba7156f429fb4b4bbe718072ec7e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/121eecc3e1a04b4081ea973fc09675b4.png)![参考说明](img/53e7ba7156f429fb4b4bbe718072ec7e.png)'
- en: 'Figure 1: Example images of Kuzushiji dataset'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Kuzushiji 数据集的示例图像
- en: 'The Kuzushiji dataset⁵⁵5http://codh.rois.ac.jp/char-shape/ consists of 6,151
    pages of image data of 44 classical books held by National Institute of Japanese
    Literature and published by ROIS-DS Center for Open Data in the Humanities (CODH).
    Example images included in the Kuzushiji dataset are shown in Fig. [1](#S3.F1
    "Figure 1 ‣ III-A Kuzushiji Dataset ‣ III Datasets ‣ Survey on Deep Learning-based
    Kuzushiji Recognition"). This Kuzushiji database comprises bounding boxes for
    characters, including 4,328 character types and 1,086,326 characters. There is
    a large bias in the number of data depending on the class: The class with the
    largest number of images is “の” (character code, U+306B), which has 41,293 images;
    many classes have extremely few images, and 790 classes only have 1 image.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 古文数据集⁵⁵5http://codh.rois.ac.jp/char-shape/ 包含了由日本文学研究所保存并由ROIS-DS人文学科开放数据中心（CODH）发布的44本经典书籍的6,151页图像数据。古文数据集中包含的示例图像见图
    [1](#S3.F1 "图 1 ‣ III-A 古文数据集 ‣ III 数据集 ‣ 基于深度学习的古文识别调查")。该古文数据库包括了字符的边界框，涵盖4,328种字符类型和1,086,326个字符。数据量在不同类别之间存在很大偏差：数据量最大的类别是“の”（字符代码U+306B），共有41,293张图像；许多类别的图像极少，其中790个类别仅有1张图像。
- en: Kuzushiji-MNIST, Kuzushiji-49, and Kuzushiji-Kanji were also provided as a subset
    of the above dataset⁶⁶6https://github.com/rois-codh/kmnist [[32](#bib.bib32)].
    These datasets not only serve as a benchmark for advanced classification algorithms,
    they can also be used in more creative areas such as generative modeling, adversarial
    examples, few-shot learning, transfer learning, and domain adaptation. Kuzushiji-MNIST
    has 70,000 28$\times$28 grayscale images with 10 Hiragana character classes. Kuzushiji-49
    is an imbalanced dataset that has 49 classes (28$\times$28 grayscale, 270,912
    images) containing 48 Hiragana characters and one Hiragana iteration mark. Kuzushiji-Kanji
    is an imbalanced dataset with a total of 3,832 Kanji characters (64$\times$64
    grayscale, 140,426 images), ranging from 1,766 examples to only a single example
    per class.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kuzushiji-MNIST、Kuzushiji-49和Kuzushiji-Kanji也作为上述数据集的子集提供⁶⁶6https://github.com/rois-codh/kmnist
    [[32](#bib.bib32)]。这些数据集不仅作为高级分类算法的基准，还可以用于生成建模、对抗样本、少样本学习、迁移学习和领域适应等更具创造性的领域。Kuzushiji-MNIST包含70,000张28$\times$28的灰度图像，涵盖10个平假名字符类别。Kuzushiji-49是一个不平衡的数据集，包含49个类别（28$\times$28灰度图像，270,912张图像），其中有48个平假名字符和一个平假名迭代标记。Kuzushiji-Kanji是一个不平衡的数据集，总共有3,832个汉字字符（64$\times$64灰度图像，140,426张图像），每个类别的样本数量从1,766个到仅一个不等。
- en: III-B Electronic Kuzushiji Dictionary Database
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 电子古文辞典数据库
- en: The Electronic Kuzushiji Dictionary Database⁷⁷7https://wwwap.hi.u-tokyo.ac.jp/ships/shipscontroller
    is a database of glyphs and fonts collected from ancient documents and records
    in the Historiographical Institute of the University of Tokyo; it contains approximately
    6,000 different characters, 2,600 different vocabularies, and 280,000 character
    image files. This database contains character forms from various periods from
    the Nara period (8th century) to the Edo period (18th century).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 电子古文辞典数据库⁷⁷7https://wwwap.hi.u-tokyo.ac.jp/ships/shipscontroller 是一个收集自东京大学历史编纂研究所的古代文献和记录的字形和字体数据库；它包含大约6,000个不同字符、2,600个不同词汇和280,000个字符图像文件。该数据库涵盖了从奈良时代（8世纪）到江户时代（18世纪）的不同历史时期的字符形式。
- en: III-C Wooden Tablet Database
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 木牌数据库
- en: '![Refer to caption](img/a799968a65a2f9d435892e36e8a523a0.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a799968a65a2f9d435892e36e8a523a0.png)'
- en: 'Figure 2: “Electronic Kuzushiji dictionary database” and “wooden tablet database”
    collaborative search function'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: “电子古文辞典数据库”和“木牌数据库”协同搜索功能'
- en: The Nara National Research Institute for Cultural Properties has developed and
    published a database that collects images of glyphs and fonts allowing the recognition
    of inscriptions written on wooden blocks excavated from underground sites. The
    database contains approximately 24,000 characters, 1,500 character types, and
    35,000 character images. It contains information from the Asuka-Nara period, which
    is not often included in the Electronic Kuzushiji Dictionary Database described
    in [III-B](#S3.SS2 "III-B Electronic Kuzushiji Dictionary Database ‣ III Datasets
    ‣ Survey on Deep Learning-based Kuzushiji Recognition"). For this reason, the
    “Electronic Kuzushiji Dictionary Database” and “wooden tablet database” collaborative
    search function⁸⁸8http://clioapi.hi.u-tokyo.ac.jp/ships/ZClient/W34/z_srch.php
    shown in Fig. [2](#S3.F2 "Figure 2 ‣ III-C Wooden Tablet Database ‣ III Datasets
    ‣ Survey on Deep Learning-based Kuzushiji Recognition"), which integrates the
    two databases, was provided for convenience.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 奈良国家文化财研究所开发并发布了一个数据库，该数据库收集了符号和字体的图像，允许识别刻写在从地下遗址中挖掘出的木块上的铭文。该数据库包含大约24,000个字符、1,500种字符类型和35,000个字符图像。它包含了飞鸟-奈良时期的信息，这些信息通常不包含在[III-B](#S3.SS2
    "III-B 电子草体字典数据库 ‣ III 数据集 ‣ 基于深度学习的草体字识别调查")描述的电子草体字典数据库中。因此，为了方便，提供了集成两个数据库的“电子草体字典数据库”和“木板数据库”联合搜索功能⁸⁸8http://clioapi.hi.u-tokyo.ac.jp/ships/ZClient/W34/z_srch.php，如图[2](#S3.F2
    "图2 ‣ III-C 木板数据库 ‣ III 数据集 ‣ 基于深度学习的草体字识别调查")所示。
- en: IV Benchmarks
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV基准
- en: IV-A PRMU Algorithm Contest
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A PRMU算法竞赛
- en: The tasks used in the 2017 and 2019 PRMU Algorithm Contest⁹⁹9A contest held
    annually by the Pattern Recognition and Media Understanding (PRMU) for the purpose
    of revitalizing research group activities. required recognizing Kuzushiji contained
    in the designated region of an image of a classical Japanese book and outputting
    the Unicode of each character. In this contest, a total of 46 types of Hiragana
    characters that do not include Katakana or Kanji needed to be recognized. The
    2017 contest had three tasks for three different difficulty levels, levels 1,
    2, and 3, depending on the number of characters contained in the rectangle. The
    participants were required to recognize single segmented characters in level 1,
    three consecutive characters in the vertical direction in level 2, and three or
    more characters in the vertical and horizontal directions in level 3. In the 2019
    contest, the task was to recognize three consecutive characters in the vertical
    direction as in level 2 in 2017.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年和2019年PRMU算法竞赛⁹⁹9A是由模式识别与媒体理解（PRMU）每年举办的竞赛，旨在振兴研究小组活动。竞赛任务要求识别经典日本书籍图像中指定区域的**草体字**，并输出每个字符的Unicode。在此竞赛中，需要识别总共46种平假名字符，不包括片假名或汉字。2017年的竞赛有三个任务，针对不同的难度级别，即级别1、2和3，取决于矩形中包含的字符数量。参与者需要在级别1中识别单个分段字符，在级别2中识别垂直方向上的三个连续字符，在级别3中识别垂直和水平方向上的三个或更多字符。在2019年的竞赛中，任务是识别与2017年级别2相同的垂直方向上的三个连续字符。
- en: Herein, we introduce the tasks of the 2017 contest and the methods used by the
    best performing teams [[33](#bib.bib33)]. The dataset applied in 2017 was constructed
    from 2,222 scanned pages of 15 historical books provided by CODH. One of the 15
    books was used as the test data because it contained many fragmented and noisy
    patterns, as well as various backgrounds. The dataset for level 1 consisted of
    228,334 single Hiragana images, and test data of 47,950 were selected from the
    dataset. To improve the accuracy, multiple models were trained and a voting-based
    ensemble method was employed to integrate the results of many different models.
    The level 2 dataset consists of 92,813 images of three consecutive characters,
    and a test set of 13,648 images was selected from the dataset. A combined architecture
    of a CNN , BLSTM, and connectionist temporal classification (CTC) was employed
    [[34](#bib.bib34)], and a sequence error rate (SER) of 31.60% was achieved. In
    the level 3 dataset, there are 12,583 images from which a test set of 1,340 images
    were selected. The authors employed a combination of vertical line segmentation
    and multiple line concatenation before applying a deep convolutional recurrent
    network. The SER is 82.57%, and there is still a significant need for improvement.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍2017年竞赛的任务以及表现最好的团队使用的方法[[33](#bib.bib33)]。2017年使用的数据集由CODH提供的15本历史书籍的2,222页扫描图像构成。由于其中一本书包含了许多碎片化和噪声图案以及各种背景，因此被用作测试数据。一级数据集包括228,334张单个平假名图像，从中选择了47,950张作为测试数据。为了提高准确性，训练了多个模型，并采用了基于投票的集成方法来整合多个不同模型的结果。二级数据集由92,813张包含三个连续字符的图像组成，从数据集中选择了13,648张作为测试集。采用了CNN、BLSTM和连接时序分类（CTC）的组合架构[[34](#bib.bib34)]，达到了31.60%的序列错误率（SER）。在三级数据集中，有12,583张图像，从中选择了1,340张作为测试集。作者在应用深度卷积递归网络之前，采用了垂直线分割和多行连接的组合。SER为82.57%，仍需显著改进。
- en: 'Another report evaluated a historical document recognition system inspired
    by human eye movements using the dataset from the 2017 PRMU algorithm contest
    [[21](#bib.bib21)]. This system includes two modules: a CNN for feature extraction
    and an LSTM decoder with an attention model for generating the target characters.
    The authors achieved SERs of 9.87% and 53.81% at levels 2 and 3 of the dataset,
    respectively.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一份报告评估了一个受到人眼运动启发的历史文档识别系统，该系统使用了2017年PRMU算法竞赛的数据集[[21](#bib.bib21)]。该系统包括两个模块：用于特征提取的CNN和带有注意力模型的LSTM解码器，用于生成目标字符。作者在数据集的二级和三级分别达到了9.87%和53.81%的SER。
- en: Now, we introduce the methods of the first through third place teams in the
    2019 contest. The dataset used in 2019 was also composed of 48 Hiragana character
    images cropped from the books in the Kuzushiji dataset provided by CODH. Single-character
    images and images containing three consecutive characters in the vertical direction
    were provided as data for training. A total of 388,146 single-character images
    were applied, and 119,997 images with three consecutive characters were used for
    training and 16,387 were used for testing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们介绍2019年竞赛中第一至第三名团队的方法。2019年使用的数据集也由48张从CODH提供的Kuzushiji数据集中裁剪的平假名字符图像组成。提供了单字符图像和垂直方向上包含三个连续字符的图像作为训练数据。共使用了388,146张单字符图像，119,997张包含三个连续字符的图像用于训练，16,387张用于测试。
- en: The first place team adopted a method dividing the characters into three images
    through a preprocessing, inputting each image into the CNN to extract the features,
    and recognizing three consecutive characters using two layers of a bidirectional
    gated recurrent unit (GRU) [[35](#bib.bib35)]^(10)^(10)10https://github.com/katsura-jp/alcon23.
    They achieved a rate of 90.63% through a combination of three backbone models
    (SE-ResNeXt, DenseNet, and Inception-v4) . As data augmentation methods, in addition
    to a random crop and a random shift, the division position was randomly shifted
    up and down slightly during training for robustness to the division position.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一名团队采用了一种通过预处理将字符分割成三张图像的方法，将每张图像输入到CNN中以提取特征，并使用两个层的双向门控递归单元（GRU）识别三个连续字符[[35](#bib.bib35)]^(10)^(10)10https://github.com/katsura-jp/alcon23。他们通过三种骨干模型（SE-ResNeXt、DenseNet和Inception-v4）的组合达到了90.63%的准确率。作为数据增强方法，除了随机裁剪和随机偏移外，在训练过程中，还会将分割位置上下随机微调，以增强对分割位置的鲁棒性。
- en: The second place team used a CNN, BLSTM, and CTC in the first step and output
    three characters by majority voting during the second step. In the first step,
    a CNN (six layers) was used to extract the features, BLSTM (two layers) was used
    to convert the features into sequential data, and CTC was used to output the text.
    To improve the accuracy, data augmentation such as a random rotation, random zoom,
    parallel shift, random noise, and random erasing [[36](#bib.bib36)] were used.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第二名团队在第一步中使用了 CNN、BLSTM 和 CTC，并在第二步中通过多数投票输出了三个字符。在第一步中，使用了六层的 CNN 来提取特征，使用了两层的
    BLSTM 将特征转换为序列数据，并使用 CTC 输出文本。为了提高准确性，使用了如随机旋转、随机缩放、平移、随机噪声和随机擦除 [[36](#bib.bib36)]
    等数据增强技术。
- en: The third place team employed an algorithm that applies multi-label image classification.
    In the first step, a multi-label estimation was conducted using an image classification
    model, and three characters were estimated in no particular order. During the
    second step, Grad-CAM [[37](#bib.bib37)] identified and aligned the region of
    interest for each candidate character and output three consecutive characters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第三名团队使用了一种应用多标签图像分类的算法。在第一步中，使用图像分类模型进行了多标签估计，并在没有特定顺序的情况下估计了三个字符。在第二步中，Grad-CAM
    [[37](#bib.bib37)] 识别并对齐了每个候选字符的感兴趣区域，并输出了三个连续的字符。
- en: IV-B Kaggle Competition
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B Kaggle 竞赛
- en: 'TABLE I: Explanation and program implementation of the winning method in the
    Kaggle competition'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: Kaggle 竞赛中获胜方法的解释和程序实现'
- en: '| Rank | F value | URL |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 排名 | F 值 | URL |'
- en: '| --- | --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 0.950 | Explanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112788
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.950 | 解释: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112788
    |'
- en: '| Implementation: https://github.com/tascj/kaggle-kuzushiji-recognition |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 实现: https://github.com/tascj/kaggle-kuzushiji-recognition |'
- en: '| 2 | 0.950 | Explanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112712
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.950 | 解释: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112712
    |'
- en: '| Implementation: https://github.com/lopuhin/kaggle-kuzushiji-2019 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 实现: https://github.com/lopuhin/kaggle-kuzushiji-2019 |'
- en: '| 3 | 0.944 | Explanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/113049
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.944 | 解释: https://www.kaggle.com/c/kuzushiji-recognition/discussion/113049
    |'
- en: '| Implementation: https://github.com/knjcode/kaggle-kuzushiji-recognition-2019
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 实现: https://github.com/knjcode/kaggle-kuzushiji-recognition-2019 |'
- en: '| 4 | 0.942 | Explanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/114764
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.942 | 解释: https://www.kaggle.com/c/kuzushiji-recognition/discussion/114764
    |'
- en: '| Implementation: https://github.com/linhuifj/kaggle-kuzushiji-recognition
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 实现: https://github.com/linhuifj/kaggle-kuzushiji-recognition |'
- en: '| 5 | 0.940 | Explanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112771
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.940 | 解释: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112771
    |'
- en: '| Implementation: https://github.com/see–/kuzushiji-recognition |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 实现: https://github.com/see–/kuzushiji-recognition |'
- en: '| 7 | 0.934 | Explanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112899
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.934 | 解释: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112899
    |'
- en: '| Implementation: https://www.kaggle.com/kmat2019/centernet-keypoint-detector
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 实现: https://www.kaggle.com/kmat2019/centernet-keypoint-detector |'
- en: '| 8 | 0.920 | Explanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/113419
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.920 | 解释: https://www.kaggle.com/c/kuzushiji-recognition/discussion/113419
    |'
- en: '| Implementation: https://github.com/t-hanya/kuzushiji-recognition |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 实现: https://github.com/t-hanya/kuzushiji-recognition |'
- en: '| 9 | 0.910 | Explanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112807
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.910 | 解释: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112807
    |'
- en: '| Implementation: https://github.com/mv-lab/kuzushiji-recognition |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 实现: https://github.com/mv-lab/kuzushiji-recognition |'
- en: '| 13 | 0.901 | Explanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/113518
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 0.901 | 解释: https://www.kaggle.com/c/kuzushiji-recognition/discussion/113518
    |'
- en: '| Implementation: https://github.com/jday96314/Kuzushiji |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 实现: https://github.com/jday96314/Kuzushiji |'
- en: '| 15 | 0.900 | Explanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/114120
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 0.900 | 解释: https://www.kaggle.com/c/kuzushiji-recognition/discussion/114120
    |'
- en: '| Implementation: https://github.com/statsu1990/kuzushiji-recognition |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 实现: https://github.com/statsu1990/kuzushiji-recognition |'
- en: A Kaggle competition called “Opening the door to a thousand years of Japanese
    culture” was held from July 19 to October 14, 2019. Whereas the PRMU algorithm
    competition involved a recognition of single-character images or images containing
    a few characters, the Kaggle competition tackled the more challenging task of
    automatically detecting the position of characters on a page of a classical book
    and correctly recognizing the type of characters. Of the 44 books in the Kuzushiji
    dataset described in [III-A](#S3.SS1 "III-A Kuzushiji Dataset ‣ III Datasets ‣
    Survey on Deep Learning-based Kuzushiji Recognition"), 28 books released before
    the competition were used as training data, and 15 books released after the competition
    were used as the test data^(11)^(11)11One book was eliminated from the competition..
    The F value, which is the harmonic mean of the precision (the percentage of correct
    responses among the characters output by the system) and the recall (the percentage
    of correct responses among the characters in the test data), was used for evaluation.
    For approximately 3 months, many international researchers worked on this competition
    and achieved a practical level of accuracy (F value of greater than 0.9). There
    were two typical methods, namely, a single-stage method that applies detection
    and recognition simultaneously, and a two-stage method that conducts character
    detection and recognition in stages. Most of the top teams adopted the two-stage
    method. The two-stage method applied detectors such as a Faster R-CNN and CenterNet
    [[38](#bib.bib38)] to detect character regions, and models such as ResNet [[39](#bib.bib39)]
    to recognize individual characters. As shown in Table [I](#S4.T1 "TABLE I ‣ IV-B
    Kaggle Competition ‣ IV Benchmarks ‣ Survey on Deep Learning-based Kuzushiji Recognition"),
    the method descriptions and implementations of the top teams were published. We
    now describe the methods of the top winning teams.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一场名为“开启千年日本文化之门”的 Kaggle 竞赛于 2019 年 7 月 19 日至 10 月 14 日举行。与 PRMU 算法竞赛涉及单字符图像或包含少量字符的图像的识别不同，Kaggle
    竞赛则处理了更具挑战性的任务，即自动检测经典书籍页面上字符的位置并正确识别字符类型。在 [III-A](#S3.SS1 "III-A Kuzushiji Dataset
    ‣ III Datasets ‣ Survey on Deep Learning-based Kuzushiji Recognition") 描述的 Kuzushiji
    数据集中 44 本书中，28 本在竞赛之前发布的书籍被用作训练数据，15 本在竞赛之后发布的书籍被用作测试数据^(11)^(11)11 一本书被从竞赛中剔除。F
    值，即精确率（系统输出字符中的正确响应比例）和召回率（测试数据中字符的正确响应比例）的调和平均数，被用于评估。大约 3 个月的时间里，许多国际研究人员参与了这场竞赛，并取得了实用水平的准确性（F
    值大于 0.9）。有两种典型的方法，即单阶段方法同时应用检测和识别，以及两阶段方法分阶段进行字符检测和识别。大多数顶级团队采用了两阶段方法。两阶段方法应用了如
    Faster R-CNN 和 CenterNet [[38](#bib.bib38)] 的检测器来检测字符区域，并应用了如 ResNet [[39](#bib.bib39)]
    的模型来识别单个字符。如表 [I](#S4.T1 "TABLE I ‣ IV-B Kaggle Competition ‣ IV Benchmarks ‣
    Survey on Deep Learning-based Kuzushiji Recognition") 所示，顶级团队的方法描述和实施细节已被发布。我们现在描述获胜团队的方法。
- en: The Chinese team took first place using a straightforward method with a Cascade
    R-CNN. Cascade R-CNN can improve the accuracy of object detection by connecting
    a Faster R-CNN in multiple stages. High-Resolution Net (HRNet) [[40](#bib.bib40)]
    was used as the backbone network of the Cascade R-CNN. HRNet utilizes multi-resolution
    feature maps and can retain high-resolution feature representations without a
    loss. The team was able to achieve a high accuracy while maintaining greater simplicity
    than the methods used by the other teams because the latest techniques were applied,
    including a Cascade R-CNN and HRNet, which showed the highest levels of accuracy.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 中国团队使用简单的方法结合 Cascade R-CNN 获得了第一名。Cascade R-CNN 通过在多个阶段连接 Faster R-CNN 来提高物体检测的准确性。高分辨率网络（HRNet）[[40](#bib.bib40)]
    被用作 Cascade R-CNN 的骨干网络。HRNet 利用多分辨率特征图，能够在没有损失的情况下保留高分辨率特征表示。由于应用了包括 Cascade
    R-CNN 和 HRNet 在内的最新技术，团队能够在保持比其他团队使用的方法更高简洁性的同时取得高准确率，这些技术展现了最高水平的准确性。
- en: The second place team, from Russia, used a two-stage method of detection and
    classification. A Faster R-CNN with a ResNet152 backbone was used for detection.
    ResNet and ResNeXt [[41](#bib.bib41)] were used to estimate the type of characters.
    Various efforts have been made to improve the accuracy of recognition. For example,
    because books of test data are different from books of training data, pseudo labels
    have also been used to adapt to the environment of an unknown book (author). Moreover,
    a new character class, called a detection error character class, was added to
    eliminate the detection error at the classification stage. Finally, the gradient
    boosting methods LightGBM [[42](#bib.bib42)] and XGBoost [[43](#bib.bib43)] were
    also used to further improve the accuracy.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 来自俄罗斯的第二名团队使用了两阶段检测和分类方法。使用了带有ResNet152骨干网的Faster R-CNN进行检测。ResNet和ResNeXt [[41](#bib.bib41)]
    被用于估计字符类型。为了提高识别准确性，进行了各种努力。例如，由于测试数据的书籍与训练数据的书籍不同，因此还使用了伪标签以适应未知书籍（作者）的环境。此外，添加了一种新的字符类别，称为检测错误字符类别，以消除分类阶段的检测错误。最后，还使用了梯度提升方法LightGBM
    [[42](#bib.bib42)] 和 XGBoost [[43](#bib.bib43)]进一步提高准确性。
- en: The third place team, from Japan, adopted a two-stage method of character detection
    using a Faster R-CNN and character-type classification using EfficientNet [[44](#bib.bib44)].
    The team employed several types of data augmentation to increase the number of
    training data, as shown below. First, because color and grayscale images were
    mixed in the training data, they used a random grayscale, which randomly converts
    images into monochrome during training. In addition, the training data were augmented
    using techniques such as combining multiple images by applying mixup [[45](#bib.bib45)]
    and random image cropping and patching (RICAP) [[46](#bib.bib46)], and adding
    some noise to the image by random erasing. Because Furigana^(12)^(12)12Furigana
    is made up of phonetic symbols occasionally written next to difficult or rare
    Kanji to show their pronunciation is not a recognition target, post-processing
    such as the creation of a false positive predictor is used for its removal.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 来自日本的第三名团队采用了两阶段字符检测方法，使用Faster R-CNN和高效网络（EfficientNet）[[44](#bib.bib44)]进行字符类型分类。该团队采用了多种数据增强类型来增加训练数据量，如下所示。首先，由于训练数据中混合了彩色和灰度图像，他们使用了随机灰度转换，这在训练过程中随机将图像转换为单色。此外，训练数据还通过应用mixup[[45](#bib.bib45)]和随机图像裁剪和修补（RICAP）[[46](#bib.bib46)]等技术进行增强，并通过随机擦除添加一些噪声。由于Furigana^(12)^(12)12Furigana由发音符号组成，偶尔写在难读或罕见的汉字旁边以显示其发音，并非识别目标，因此使用了后处理，如创建虚假正例预测器以进行移除。
- en: The fourth place team, from China, adopted a different method than the other
    groups; they used a hybrid task cascade (HTC) [[47](#bib.bib47)] for character
    detection followed by a connectionist text proposal network (CTPN) [[48](#bib.bib48)]
    for line-by-line character recognition. One-line images were resized to 32$\times$800,
    and then fed to a model that recognizes a single line of text. A convolutional
    recurrent neural network (CRNN) model was used for line recognition and had a
    structure with 200 outputs. A six-gram language model was trained using the KenLM
    toolkit [[49](#bib.bib49)], and a beam search was applied to decode the CTC output
    of the model. The positional accuracy of the CTC output was improved using multitask
    learning [[50](#bib.bib50)], which added an attention loss. For the data augmentation
    methods, contrast limited adaptive histogram equalization, random brightness,
    random contrast, random scale, and random distortion were used. A dropout and
    cutout were applied as the regularization methods. Although many other teams reported
    that language models are ineffective, this team reported a slight increase in
    accuracy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 来自中国的第四名团队采用了不同于其他团队的方法；他们使用了混合任务级联（HTC）[[47](#bib.bib47)]进行字符检测，随后使用连接主义文本提议网络（CTPN）[[48](#bib.bib48)]进行逐行字符识别。单行图像被调整为32$\times$800的尺寸，然后输入到识别单行文本的模型中。用于行识别的卷积递归神经网络（CRNN）模型具有200个输出。使用KenLM工具包[[49](#bib.bib49)]训练了一个六元语法模型，并应用了束搜索来解码模型的CTC输出。通过多任务学习[[50](#bib.bib50)]提高了CTC输出的定位精度，添加了一个注意力损失。数据增强方法中使用了对比度限制自适应直方图均衡、随机亮度、随机对比度、随机缩放和随机失真。作为正则化方法应用了dropout和cutout。虽然许多其他团队报告了语言模型的无效性，但该团队报告了准确率的轻微提高。
- en: The fifth place team, from Germany, reported that the one-stage method using
    CenterNet is consistently more accurate, unlike the two-stage method of the other
    top teams. Although, it is common for object detection tasks to deal with approximately
    80 classes of objects, such as the MS COCO dataset, this team showed that detection
    can be achieved without problems even if the number of classes is large. They
    made several modifications to CenterNet, such as creating it from scratch, avoiding
    the use of an HourglassNet, and using the ResNet50 and ResNet101 structures with
    a feature pyramid network. They also reported that the use of high-resolution
    images such as 1536$\times$1536 did not show any improvement.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第五名的团队，来自德国，报告说，使用 CenterNet 的单阶段方法在准确性上始终优于其他顶级团队的两阶段方法。虽然在对象检测任务中，处理大约 80
    类对象是很常见的，比如 MS COCO 数据集，但该团队表明，即使类别数量较多，检测也可以顺利进行。他们对 CenterNet 进行了几项修改，如从头开始创建它，避免使用
    HourglassNet，并使用具有特征金字塔网络的 ResNet50 和 ResNet101 结构。他们还报告说，使用 1536$\times$1536
    等高分辨率图像并没有带来改进。
- en: The seventh place team, from Japan, adopted a two-stage method of character
    region detection using CenterNet followed by ResNet-based character recognition.
    For the data augmentation, flipping, cropping, brightness, and contrast were used
    to create the detector, and cropping, brightness, contrast, and a pseudo-label
    were used to create a classifier. The authors attempted to build their own model
    instead of using a predefined approach. In general, although predefined models
    have been designed to provide local features at a high resolution and a wide field
    of view at a low resolution, it was not necessary to provide a wide field of view
    at a significantly low resolution for the Kuzushiji detection task. The team reported
    that the success of this model was due to the fact that they built their own task-specific
    model with a high degree of freedom.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第七名的团队，来自日本，采用了一个两阶段的方法，首先使用 CenterNet 进行字符区域检测，然后进行基于 ResNet 的字符识别。在数据增强方面，使用了翻转、裁剪、亮度和对比度来创建检测器，并使用了裁剪、亮度、对比度以及伪标签来创建分类器。作者们尝试构建自己的模型，而不是使用预定义的方法。一般来说，虽然预定义模型设计用于提供高分辨率的局部特征和低分辨率的广视场，但对于
    Kuzushiji 检测任务来说，提供显著低分辨率的广视场并不是必要的。团队报告称，这个模型的成功是因为他们建立了一个具有高度自由度的任务特定模型。
- en: The eighth place team, also from Japan, adopted a two-stage method of character
    detection using CenterNet (ResNet18, UNet) and character classification using
    MobileNetV3 [[51](#bib.bib51)]. The character detection was made by maintaining
    the aspect ratio to prevent a separation of characters, and bounding box voting
    [[52](#bib.bib52)] was used to reduce the number of undetected characters. Because
    the appearance of the characters varies significantly depending on the book applied,
    the team augmented the data, such as through a grid distortion, elastic transform,
    and random erasing, to increase the visual variation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 第八名的团队，也来自日本，采用了字符检测的两阶段方法，使用 CenterNet（ResNet18，UNet）进行字符检测，并使用 MobileNetV3
    进行字符分类[[51](#bib.bib51)]。通过保持纵横比来防止字符分离，进行了字符检测，并使用边界框投票[[52](#bib.bib52)]来减少未检测到的字符数量。由于字符的外观因所用书籍而有显著变化，团队通过网格扭曲、弹性变换和随机擦除等数据增强方法来增加视觉变异性。
- en: The nineth place team adopted a simple two-stage method using CenterNet with
    HourglassNet as the backbone for detection and ResNet18 for classification. Because
    the detection accuracy was insufficient, multiple results were combined. For character
    classification, the team treated characters with less than 5 data as pseudo-labels,
    $NaN$, because 276 of the characters were not in the training data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第九名的团队采用了一个简单的两阶段方法，使用 CenterNet 和 HourglassNet 作为检测的骨干网络，以及 ResNet18 进行分类。由于检测精度不足，因此结合了多个结果。在字符分类方面，该团队将数据少于
    5 个的数据视为伪标签 $NaN$，因为 276 个字符不在训练数据中。
- en: The 13th place team used a Faster R-CNN to detect and classify the character
    regions, but did not share the network for the detection and classification. A
    wide ResNet-34 was used as the backbone of the network for both character detection
    and character recognition. The team tried to apply deeper networks, such as ResNet-50
    and ResNet-101, but reported that the wider and shallower networks achieve a better
    accuracy. They also reported that the use of color images slightly improves the
    accuracy of detection but worsens the accuracy of the character recognition. Although
    they attempted to use a language model to correct incorrect labels, it was ineffective.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 第13名的团队使用了 Faster R-CNN 来检测和分类字符区域，但没有共享用于检测和分类的网络。该网络的骨干使用了宽 ResNet-34，用于字符检测和字符识别。该团队尝试使用更深的网络，如
    ResNet-50 和 ResNet-101，但报告称宽而浅的网络能实现更好的准确性。他们还报告说，使用彩色图像可以稍微提高检测的准确性，但会降低字符识别的准确性。虽然他们尝试使用语言模型来纠正错误标签，但效果不佳。
- en: The 15th place team, from Japan, used a two-stage method of detection and classification.
    First, they applied a grayscale conversion, Gaussian filter, gamma correction,
    and Ben’s pre-processing of the images. A two-stage CenterNet with HourglassNet
    as a backbone was applied for character detection. In the first stage of CenterNet,
    the bounding box was estimated, and the outside of the outermost bounding box
    was removed. In the second stage of CenterNet, the bounding box was estimated
    again, and the results of the first and second stages were combined. Random erasing,
    horizontal movement, and a brightness adjustment were used to augment the data
    when creating the detection model. For character classification, the results of
    three types of ResNet-based models also trained using pseudo labels were combined
    and output. Horizontal movement, rotation, zoom, and random erasing were used
    for data augmentation when creating the character classification model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第15名的团队来自日本，使用了检测和分类的两阶段方法。首先，他们应用了灰度转换、高斯滤波、伽玛校正和 Ben 的图像预处理。使用了以 HourglassNet
    为骨干的两阶段 CenterNet 来进行字符检测。在 CenterNet 的第一阶段，估计了边界框，并去除了最外层边界框之外的部分。在第二阶段，重新估计了边界框，并将第一和第二阶段的结果合并。在创建检测模型时，使用了随机擦除、水平移动和亮度调整来增强数据。对于字符分类，将三种基于
    ResNet 的模型的结果也用伪标签训练并结合输出。在创建字符分类模型时，使用了水平移动、旋转、缩放和随机擦除来进行数据增强。
- en: Summarizing the methods of the top teams explained here, we can see that it
    is important to make use of recently proposed detection and classification models.
    However, it is impossible to determine whether a one-stage or two-stage method
    achieves better results. For the detection method, models such as YOLO [[53](#bib.bib53)],
    which are frequently used in object detection, did not perform well, whereas Faster
    R-CNN and CenterNet were successful. As the reason for this, there is almost no
    overlap of characters in the image, which differs from conventional object detection.
    Some teams reported an improved accuracy using the latest models with strong backbones,
    whereas others reported that their own methods were more successful.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这里解释的顶级团队的方法，我们可以看到，利用最近提出的检测和分类模型非常重要。然而，尚无法确定单阶段或两阶段方法是否能取得更好的结果。在检测方法方面，像
    YOLO [[53](#bib.bib53)] 这样在物体检测中常用的模型表现不佳，而 Faster R-CNN 和 CenterNet 则取得了成功。其原因在于图像中的字符几乎没有重叠，这与传统的物体检测不同。一些团队报告说使用最新的强大骨干的模型提高了准确性，而其他团队则报告说他们自己的方法更成功。
- en: V Activities Related to Kuzushiji recognition
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与楷书识别相关的活动
- en: The “Japanese Culture and AI Symposium 2019”^(13)^(13)13http://codh.rois.ac.jp/symposium/japanese-culture-ai-2019/
    was held in November 2019. This symposium introduced leading research being conducted
    on Kuzushiji globally, and the participants discussed the past and present studies
    with an aim toward future research using AI for the reading and understanding
    of Kuzushiji.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: “日本文化与人工智能研讨会 2019”^(13)^(13)13http://codh.rois.ac.jp/symposium/japanese-culture-ai-2019/
    于 2019 年 11 月举行。此次研讨会介绍了全球范围内对楷书进行的前沿研究，参与者讨论了过去和现在的研究，并旨在未来利用人工智能对楷书进行阅读和理解的研究。
- en: In the “Cloud Honkoku (Cloud Transcription)” project, a system that allows correcting
    the transcriptions of other participants was implemented. By cooperating with
    the Kuzushiji learning application using KuLA and AI technology, the methods developed
    by the participants were simplified and made more efficient. The transcribed results
    can also be used as training data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在“云本国（Cloud Transcription）”项目中，实现了一个允许纠正其他参与者转录文本的系统。通过与使用KuLA和AI技术的古文字学习应用程序合作，参与者开发的方法得到了简化和提高效率。转录结果也可以用作训练数据。
- en: CODH provides several online services such as the KuroNet Kuzushiji character
    recognition service, KogumaNet Kuzushiji character recognition service, and International
    Image Interoperability Framework (IIIF)^(14)^(14)14A set of technology standards
    intended to make it easier for researchers, students, and the public at large
    to view, manipulate, compare, and annotate digital images on the web. https://iiif.io/
    compatible Kuzushiji character recognition viewer. The KuroNet Kuzushiji recognition
    service^(15)^(15)15http://codh.rois.ac.jp/kuronet/ provides a multi-character
    Kuzushiji OCR function for IIIF-compliant images. With this service, we do not
    need to upload images to the server, but can test our own images. The IIIF-compatible
    character recognition viewer provides a single-character OCR function for IIIF-compliant
    images. As an advantage of using this viewer, it can be applied immediately while
    viewing an image, allowing not only the top candidate but also other candidates
    to be viewed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: CODH提供了几个在线服务，如KuroNet古文字字符识别服务、KogumaNet古文字字符识别服务和国际图像互操作框架（IIIF）^(14)^(14)14一套旨在使研究人员、学生和公众更容易在网络上查看、操作、比较和注释数字图像的技术标准。
    https://iiif.io/兼容的古文字字符识别查看器。KuroNet古文字识别服务^(15)^(15)15http://codh.rois.ac.jp/kuronet/提供了一个多字符古文字OCR功能，适用于IIIF兼容的图像。使用该服务，我们不需要将图像上传到服务器，而是可以测试自己的图像。IIIF兼容的字符识别查看器为IIIF兼容的图像提供了单字符OCR功能。使用此查看器的一个优势是可以在查看图像时立即应用它，允许不仅查看最优候选项，还可以查看其他候选项。
- en: In the future, it is expected that Kuzushiji transcription will be further enabled
    by individuals in various fields, such as those involved in machine learning,
    the humanities, and the actual application of this historical script.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，预计古文字转录将得到来自机器学习、人文学科以及实际应用该历史文献的各个领域个人的进一步推动。
- en: VI Future Studies on Kuzushiji Recognition
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 未来古文字识别研究
- en: In Japan, historical materials have been preserved for more than a thousand
    years; pre-modern books and historical documents and records have been estimated
    to number approximately three million and one billion, respectively. However,
    most historical materials have not been transcribed. Currently, because the Kuzushiji
    database provided by CODH is mainly limited to documents after the Edo period,
    it is difficult to automatically transcribe books written prior to this period.
    To address this issue, it will be necessary to combine multiple books of various
    periods owned by multiple institutions and rebuild a large-scale Kuzushiji database.
    Looking back further into history, the rules for Kuzushiji increase, and thus
    it is expected that there will be many character types and character forms that
    cannot be found in the database. Therefore, we need a framework for handling unknown
    character types and character forms that do not exist in the training data. In
    addition, as one of the problems in character classification, the number of data
    for each character class is imbalanced, and it is impossible to accurately recognize
    characters in a class with an extremely small number of data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在日本，历史材料已经保存了超过一千年；前现代的书籍和历史文献记录的数量分别估计约为三百万和十亿。然而，大多数历史材料尚未转录。目前，由于CODH提供的古文字数据库主要限于江户时代以后的文献，自动转录这一时期之前书籍变得困难。为了解决这一问题，需要结合多个机构拥有的不同历史时期的书籍，并重建一个大规模的古文字数据库。更远的历史中，古文字规则增加，因此预计会有许多数据库中找不到的字符类型和字符形式。因此，我们需要一个处理训练数据中不存在的未知字符类型和字符形式的框架。此外，作为字符分类中的一个问题，各字符类别的数据数量不平衡，极少数量的数据无法准确识别该类别的字符。
- en: In addition, the quality of the images differs significantly from one book to
    the next, and the writing styles are completely different depending on the author,
    which also makes character recognition difficult to achieve. To this end, it is
    important to increase the number and variation of training data, to incorporate
    techniques such as domain adaptation, and improve the database itself.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图像的质量在不同书籍之间差异显著，作者的书写风格也完全不同，这也使得字符识别变得困难。为此，增加训练数据的数量和多样性，采用领域适应技术，并改进数据库本身是重要的。
- en: There is room for further studies on not only individual character-by-character
    recognition, but also word/phrase/sentence-level recognition based on the surrounding
    characters and context. In the Kaggle competition, although many teams added contextual
    information, such as the use of language models, the accuracy did not significantly
    improve because there are problems specific to Kuzushiji that differ from those
    of modern languages. Therefore, we believe it will be necessary to not only improve
    machine learning techniques such as deep learning, but also learn rules based
    on specialized knowledge regarding Kuzushiji.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于个体字符逐一识别，以及基于周围字符和上下文的词汇/短语/句子级别识别，仍有进一步研究的空间。在Kaggle竞赛中，尽管许多团队添加了上下文信息，例如使用语言模型，但准确率没有显著提高，因为Kuzushiji（草书）的具体问题与现代语言不同。因此，我们认为不仅需要改进如深度学习这样的机器学习技术，还需基于Kuzushiji的专业知识学习规则。
- en: Based on the current state of transcription, it is important to design an interface
    that not only allows a complete and automatic recognition of characters, but also
    allows the user to work effortlessly. It is desirable to have various functions,
    such as a visualization of characters with uncertain estimation results, and output
    alternative candidate characters.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基于当前的转录状态，设计一个不仅允许完全自动识别字符，还能使用户轻松操作的界面至关重要。希望具有各种功能，如对不确定估计结果的字符进行可视化，并输出备用候选字符。
- en: VII Conclusion
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: In this paper, we introduced recent techniques and problems in Kuzushiji recognition
    using deep learning. The introduction of deep learning has dramatically improved
    the detection and recognition rate of Kuzushiji. In particular, the inclusion
    of Kuzushiji recognition in the PRMU algorithm contest and Kaggle competition
    has attracted the attention of numerous researchers, who have contributed to significant
    improvements in accuracy. However, there are still many old manuscripts that need
    to be transcribed, and there are still many issues to be addressed. To solve these
    problems, in addition to improving algorithms such as machine learning, it is
    necessary to further promote the development of a Kuzushiji database and cooperation
    among individuals in different fields.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了使用深度学习进行Kuzushiji识别的最新技术和问题。深度学习的引入显著提高了Kuzushiji的检测和识别率。特别是，Kuzushiji识别的纳入PRMU算法竞赛和Kaggle竞赛引起了众多研究者的关注，他们对提高准确性作出了重要贡献。然而，仍有许多古旧手稿需要转录，还有许多问题需要解决。为了解决这些问题，除了改进机器学习等算法外，还需进一步推动Kuzushiji数据库的发展和不同领域之间的合作。
- en: References
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Yamada, N. Kato, M. Namiki, H. Kawaguchi, S. Hara, Y. Ishitani, K. Kasaya,
    M.Kojima, M. Umeda, K. Yamamoto, M. Shibayama, “Historical Character Recognition
    (HCR) Project Report (2),” IPSJ SIG Computers and the Humanities (CH), vol.50,
    no.2, pp.9–16, 2001\. (in Japanese)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. Yamada, N. Kato, M. Namiki, H. Kawaguchi, S. Hara, Y. Ishitani, K. Kasaya,
    M.Kojima, M. Umeda, K. Yamamoto, M. Shibayama, “历史字符识别（HCR）项目报告（2）,”IPSJ SIG计算机与人文学科（CH），第50卷，第2期，第9–16页，2001年（用日语）。'
- en: '[2] S. Yamada, Y. Waizumi, n. Kato, M. Shibayama, “Development of a digital
    dictionary of historical characters with search function of similar characters,”
    IPSJ SIG Computers and the Humanities (CH), vol.54, no.7, pp.43-50, 2002\. (in
    Japanese)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] S. Yamada, Y. Waizumi, n. Kato, M. Shibayama, “开发具有相似字符搜索功能的历史字符数字字典，”IPSJ
    SIG计算机与人文学科（CH），第54卷，第7期，第43-50页，2002年（用日语）。'
- en: '[3] M. Onuma, B. Zhu, S. Yamada, M. Shibayama, M. Nakagawa, “Development of
    cursive character pattern recognition for accessing a digital dictionary to support
    decoding of historical documents,” IEICE Technical Report, vol.106, no.606, PRMU2006-270,
    pp.91–96, 2007\. (in Japanese)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Onuma, B. Zhu, S. Yamada, M. Shibayama, M. Nakagawa, “开发草书字符模式识别以访问数字字典，支持解码历史文献，”IEICE技术报告，第106卷，第606期，PRMU2006-270，第91–96页，2007年（用日语）。'
- en: '[4] T. Horiuchi, S. Kato, “A Study on Japanese Historical Character Recognition
    Using Modular Neural Networks,” International Journal of Innovative Computing,
    Information and Control, vol.7, no.8, pp.5003–5014, 2011.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] T. Horiuchi, S. Kato, “使用模块化神经网络进行日文历史字符识别的研究”，《国际创新计算、信息与控制期刊》，第7卷，第8期，第5003–5014页，2011年。'
- en: '[5] S. Kato, R. Asano, “A Study on Historical Character Recognition by using
    SOM Template,” In Proc. of 30th Fuzzy System Symposium, pp.242–245, 2014\. (in
    Japanese)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Kato, R. Asano, “使用SOM模板进行历史字符识别的研究”，发表于第30届模糊系统研讨会论文集中，第242–245页，2014年。
    （日文）'
- en: '[6] T. Hayasaka, W. Ohno, Y. Kato, “Recognition of obsolete script in pre-modern
    Japanese texts by Neocognitron,” Journal of Toyota College of Technology, vol.48,
    pp.5–12, 2015\. (in Japanese)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] T. Hayasaka, W. Ohno, Y. Kato, “Neocognitron在前现代日文文本中的过时脚本识别”，《丰田工学院学报》，第48卷，第5–12页，2015年。
    （日文）'
- en: '[7] T. Hayasaka, W. Ohno, Y. Kato, K. Yamamoto, “Recognition of Hentaigana
    by Deep Learning and Trial Production of WWW Application,” In Proc. of IPSJ Symposium
    of Humanities and Computer Symposium, pp.7–12, 2016\. (in Japanese)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] T. Hayasaka, W. Ohno, Y. Kato, K. Yamamoto, “深度学习识别变体仮名及其在WWW应用中的试验生产”，发表于
    IPSJ 人文与计算机研讨会论文集中，第7–12页，2016年。 （日文）'
- en: '[8] K. Ueda, M. Sonogashira, M. Iiyama, “Old Japanese Character Recognition
    by Convolutional Neural Net and Character Aspect Ratio,” ELCAS Journal, vol.3,
    pp.88–90, 2018\. (in Japanese)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] K. Ueda, M. Sonogashira, M. Iiyama, “基于卷积神经网络和字符纵横比的古日文字符识别”，《ELCAS期刊》，第3卷，第88–90页，2018年。
    （日文）'
- en: '[9] T. Kojima, K. Ueki, “Utilization and Analysis of Deep Learning for Kuzushiji
    Translation,” Journal of the Japan Society for Precision Engineering, vol.85,
    no.12, pp.1081–1086, 2019\. (in Japanese)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] T. Kojima, K. Ueki, “深度学习在变体仮名翻译中的应用与分析”，《日本精密工程学会学报》，第85卷，第12期，第1081–1086页，2019年。
    （日文）'
- en: '[10] Z. Yang, K. Doman, M. Yamada, Y. Mekada, “Character recognition of modern
    Japanese official documents using CNN for imblanced learning data,” In Proc. of
    2019 Int. Workshop on Advanced Image Technology (IWAIT), no.74, 2019.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Z. Yang, K. Doman, M. Yamada, Y. Mekada, “使用CNN对现代日文官方文件进行字符识别，解决不平衡学习数据问题”，发表于2019年国际先进图像技术研讨会（IWAIT）论文集中，第74号，2019年。'
- en: '[11] A. Nagai, “Recognizing Three Character String of Old Japanese Cursive
    by Convolutional Neural Networks,” In Proc. of Information Processing Society
    of Japan (IPSJ) Symposium, pp.213–218, 2017\. (in Japanese)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Nagai, “使用卷积神经网络识别古日文连笔字符的三字符字符串”，发表于日本信息处理学会（IPSJ）研讨会论文集中，第213–218页，2017年。
    （日文）'
- en: '[12] K.Ueki, T. Kojima, R. Mutou, R. S. Nezhad, Y. Hagiwara, “Recognition of
    Japanese Connected Cursive Characters Using Multiple Softmax Outputs,” In Proc.
    of International Conference on Multimedia Information Processing and Retrieval,
    2020.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] K.Ueki, T. Kojima, R. Mutou, R. S. Nezhad, Y. Hagiwara, “使用多重Softmax输出识别日文连笔字符”，发表于国际多媒体信息处理与检索会议论文集中，2020年。'
- en: '[13] X. Hu, M. Inamoto, A. Konagaya, “Recognition of Kuzushi-ji with Deep Learning
    Method: A Case Study of Kiritsubo Chapter in the Tale of Genji,” The 33rd Annual
    Conference of the Japanese Society for Artificial Intelligence, 2019.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] X. Hu, M. Inamoto, A. Konagaya, “使用深度学习方法识别变体仮名：以《源氏物语》桐壶篇为例”，日本人工智能学会第33届年会，2019年。'
- en: '[14] A. Kitamoto, T. Clanuwat, T. Miyazaki, K. Yayamoto, “Analysis of Character
    Data: Potential and Impact of Kuzushiji Recognition by Machine Learning,” Journal
    of Institute of Electronics, Information, and Communication Engineers, vol.102,
    no.6, pp.563–568, 2019\. (in Japanese)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. Kitamoto, T. Clanuwat, T. Miyazaki, K. Yayamoto, “字符数据分析：机器学习对变体仮名识别的潜力与影响”，《电子信息通信学会学报》，第102卷，第6期，第563–568页，2019年。
    （日文）'
- en: '[15] A. Kitamoto, T. Clanuwat, A. Lamb, M. Bober-Irizar, “Progress and Results
    of Kaggle Machine Learning Competition for Kuzushiji Recognition,” In Proc. of
    the Computers and the Humanities Symposium, pp.223–230, 2019\. (in Japanese)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] A. Kitamoto, T. Clanuwat, A. Lamb, M. Bober-Irizar, “Kaggle 机器学习竞赛对变体仮名识别的进展与结果”，发表于计算机与人文学科研讨会论文集中，第223–230页，2019年。
    （日文）'
- en: '[16] A. Kitamoto, T. Clanuwat, M. Bober-Irizar, “Kaggle Kuzushiji Recognition
    Competition – Challenges of Hosting a World-Wide Competition in the Digital Humanities
    –,” Journal of the Japanese Society for Artificial Intelligence, vol.35, no.3,
    pp.366–376, 2020\. (in Japanese)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] A. Kitamoto, T. Clanuwat, M. Bober-Irizar, “Kaggle Kuzushiji 识别竞赛——在数字人文学科中举办全球竞赛的挑战——”，《日本人工智能学会学报》，第35卷，第3期，第366–376页，2020年。
    （日文）'
- en: '[17] S. Ren, K. He, R. Girshick, J. Sun, “Faster R-CNN: Towards Real-Time Object
    Detection with Region Proposal Networks,” arXiv:1506.01497, 2015.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Ren, K. He, R. Girshick, J. Sun, “Faster R-CNN: 通过区域提议网络实现实时物体检测，”
    arXiv:1506.01497，2015年。'
- en: '[18] Z. Cai, N. Vasconcelos, “Cascade R-CNN: High Quality Object Detection
    and Instance Segmentation,” arXiv:1906.09756, 2019.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Z. Cai, N. Vasconcelos, “Cascade R-CNN: 高质量物体检测与实例分割，” arXiv:1906.09756，2019年。'
- en: '[19] T. Clanuwat, A. Lamb, A. Kitamoto, “KuroNet: Pre-Modern Japanese Kuzushiji
    Character Recognition with Deep Learning,” In Proc. of International Conference
    on Document Analysis and Recognition (ICDAR2019), 2019.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] T. Clanuwat, A. Lamb, A. Kitamoto, “KuroNet: 基于深度学习的前现代日语草书字符识别，” 见于国际文档分析与识别会议（ICDAR2019）论文集，2019年。'
- en: '[20] A. Lamb, T. Clanuwat, A. Kitamoto, “KuroNet: Regularized Residual U‐Nets
    for End‐to‐End Kuzushiji Character Recognition,” In Proc. of SN Computer Science
    (2020), 2020.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Lamb, T. Clanuwat, A. Kitamoto, “KuroNet: 用于端到端草书字符识别的正则化残差U-Net，”
    见于SN计算机科学论文集（2020），2020年。'
- en: '[21] A. D. Le, T. Clanuwat, A. Kitamoto, “A human-inspired recognition system
    for premodern Japanese historical documents,” arXiv:1905.05377, 2019.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. D. Le, T. Clanuwat, A. Kitamoto, “一种受人类启发的前现代日本历史文档识别系统，” arXiv:1905.05377，2019年。'
- en: '[22] A. D. Le, “Automated Transcription for Pre-Modern Japanese Kuzushiji Documents
    by Random Lines Erasure and Curriculum Learning,” arXiv:2005.02669, 2020.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. D. Le, “通过随机线擦除和课程学习对前现代日语草书文档的自动转录，” arXiv:2005.02669，2020年。'
- en: '[23] A. D. Le, “Detecting Kuzushiji Characters from Historical Documents by
    Two-Dimensional Context Box Proposal Network,” Future Data and Security Engineering,
    pp.731–738.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. D. Le, “通过二维上下文框提议网络从历史文档中检测草书字符，” 未来数据与安全工程，页码731–738。'
- en: '[24] A. Graves, J. Schmidhuber, ”Framewise phoneme classification with bidirectional
    LSTM and other neural network architectures,” Neural Networks, vol.18, no.5–6,
    pp.602–610, 2005.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. Graves, J. Schmidhuber, ”使用双向LSTM及其他神经网络架构的逐帧音素分类，” 神经网络，第18卷，第5–6期，页码602–610，2005年。'
- en: '[25] S. Yamamoto, O. Tomejiro, “Labor saving for reprinting Japanese rare classical
    books,” Journal of Information Processing and Management, vol.58, no.11, pp.819–827,
    2016\. (in Japanese)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Yamamoto, O. Tomejiro, “节省日语稀有古籍重印的劳动力，” 信息处理与管理杂志，第58卷，第11期，页码819–827，2016年。（日文）'
- en: '[26] K. Ueki, T. Kojima, “Feasibility Study of Deep Learning Based Japanese
    Cursive Character Recognition,” IIEEJ Transactions on Image Electronics and Visual
    Computing, vol.8, no.1, pp.10–16, 2020.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] K. Ueki, T. Kojima, “基于深度学习的日语草书字符识别的可行性研究，” IIEEJ 影像电子与视觉计算学报，第8卷，第1期，页码10–16，2020年。'
- en: '[27] K. Ueki, T. Kojima, “Japanese Cursive Character Recognition for Efficient
    Transcription,” In Proc. of the International Conference on Pattern Recognition
    Applications and Methods, 2020.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] K. Ueki, T. Kojima, “高效转录的日语草书字符识别，” 见于国际模式识别应用与方法会议论文集，2020年。'
- en: '[28] M. Takeuchi, T. Hayasaka, W. Ohone, Y. Kato, K. Yamamoto, M. Ishima, T.
    Ishikawa, “Development of Embedded System for Recognizing Kuzushiji by Deep Learning,”
    In Proc. of the 33rd Annual Conference of the Japanese Society for Artificial
    Intelligence, 2019\. (in Japanese)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] M. Takeuchi, T. Hayasaka, W. Ohone, Y. Kato, K. Yamamoto, M. Ishima, T.
    Ishikawa, “开发用于通过深度学习识别草书的嵌入式系统，” 见于第33届日本人工智能学会年会论文集，2019年。（日文）'
- en: '[29] K. Sando, T. Suzuki, A. Aiba, “A Constraint Solving Web Service for Recognizing
    Historical Japanese KANA Texts,” In Proc. the 10th International Conference on
    Agents and Artificial Intelligence (ICAART), 2018.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] K. Sando, T. Suzuki, A. Aiba, “用于识别历史日语假名文本的约束求解网络服务，” 见于第10届国际代理与人工智能会议（ICAART）论文集，2018年。'
- en: '[30] Atsushi Yamazaki, Tetsuya Suzuki, Kazuki Sando, Akira Aiba, “A Handwritten
    Japanese Historical Kana Reprint Support System,” In Proc. the 18th ACM Symposium
    on Document Engineering, 2018.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Atsushi Yamazaki, Tetsuya Suzuki, Kazuki Sando, Akira Aiba, “手写日语历史假名重印支持系统，”
    见于第18届ACM文档工程研讨会论文集，2018年。'
- en: '[31] C. Panichkriangkrai, L. Li, T. Kaneko, R. Akama, K. Hachimura, “Character
    segmentation and transcription system for historical Japanese books with a self-proliferating
    character image database,” International Journal on Document Analysis and Recognition
    (IJDAR), vol.20, pp.241–257, 2017.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] C. Panichkriangkrai, L. Li, T. Kaneko, R. Akama, K. Hachimura, “具有自我扩展字符图像数据库的历史日语书籍字符分割与转录系统，”
    国际文档分析与识别杂志（IJDAR），第20卷，页码241–257，2017年。'
- en: '[32] Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki
    Yamamoto, David Ha, “Deep Learning for Classical Japanese Literature,” arXiv:1812.01718,
    2018.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki
    Yamamoto, David Ha，“用于古典日本文学的深度学习，” arXiv:1812.01718，2018年。'
- en: '[33] H. T. Nguyen, N. T. Ly, K. C. Nguyen, C. T. Nguyen, M. Nakagawa, “Attempts
    to recognize anomalously deformed Kana in Japanese historical documents,” In Proc.
    of the International Workshop on Historical Document Imaging and Processing (HIP
    2017), 2017.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] H. T. Nguyen, N. T. Ly, K. C. Nguyen, C. T. Nguyen, M. Nakagawa，“尝试识别日本历史文献中的异常变形假名，”
    发表在国际历史文献成像与处理研讨会（HIP 2017）论文集中，2017年。'
- en: '[34] A. Graves, S. Fernandez, F. Gomez, J. Schmidhuber, “Connectionist Temporal
    Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks,”
    In Proc. of the International Conference on Machine Learning, pp.369–376, 2006.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. Graves, S. Fernandez, F. Gomez, J. Schmidhuber，“连接主义时间分类：使用递归神经网络对未分割序列数据进行标注，”
    发表在国际机器学习会议论文集中，第369–376页，2006年。'
- en: '[35] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H.
    Schwenk, Y. Bengio, “Learning Phrase Representations using RNN Encoder–Decoder
    for Statistical Machine Translation,” In Proc. of the Conference on Empirical
    Methods in Natural Language Processing (EMNLP), pp.1724–1734, 2014.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H.
    Schwenk, Y. Bengio，“使用RNN编码器-解码器进行短语表示学习用于统计机器翻译，” 发表在自然语言处理会议（EMNLP）论文集中，第1724–1734页，2014年。'
- en: '[36] Z. Zhong, L. Zheng, G. Kang, S. Li, Y. Yang, “Random Erasing Data Augmentation,”
    arXiv:1708.04896, 2017.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Z. Zhong, L. Zheng, G. Kang, S. Li, Y. Yang，“随机擦除数据增强，” arXiv:1708.04896，2017年。'
- en: '[37] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra,
    “Grad-CAM: Visual Explanations from Deep Networks via Gradientbased Localization,”
    arXiv:1610.02391, 2016.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra，“Grad-CAM：通过基于梯度的定位从深度网络中获取视觉解释，”
    arXiv:1610.02391，2016年。'
- en: '[38] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, Q. Tian, “CenterNet: Keypoint
    Triplets for Object Detection,” arXiv:1904.08189, 2019.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, Q. Tian，“CenterNet：用于目标检测的关键点三元组，”
    arXiv:1904.08189，2019年。'
- en: '[39] K. He, X. Zhang, S. Ren, and Jian Sun, “Deep Residual Learning for Image
    Recognition,” arXiv:1512.03385, 2015.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] K. He, X. Zhang, S. Ren, 和 Jian Sun，“用于图像识别的深度残差学习，” arXiv:1512.03385，2015年。'
- en: '[40] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu,
    M. Tan, X. Wang, W. Liu, B. Xiao, “Deep High-Resolution Representation Learning
    for Visual Recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
    2020.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu,
    M. Tan, X. Wang, W. Liu, B. Xiao，“用于视觉识别的深度高分辨率表示学习，” IEEE模式分析与机器智能汇刊，2020年。'
- en: '[41] S. Xie, R. Girshick, P. Dollár, Z. Tu, K. He, “Aggregated Residual Transformation
    for Deep Neural Networks,” In Proc. of IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR), 2017.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Xie, R. Girshick, P. Dollár, Z. Tu, K. He，“用于深度神经网络的聚合残差变换，” 发表在IEEE计算机视觉与模式识别会议（CVPR）论文集中，2017年。'
- en: '[42] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T.-Y. Liu,
    “LightGBM: A Highly Efficient Gradient Boosting Decision Tree,” Advances in Neural
    Information Processing Systems (NIPS) 30, pp.3148–3156, 2017.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T.-Y. Liu，“LightGBM：一种高效的梯度提升决策树，”
    神经信息处理系统（NIPS）30卷，第3148–3156页，2017年。'
- en: '[43] T. Chen, C. Guestrin, ”XGBoost: A Scalable Tree Boosting System,” arXiv:1603.02754,
    2016.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] T. Chen, C. Guestrin，“XGBoost：一个可扩展的树提升系统，” arXiv:1603.02754，2016年。'
- en: '[44] M. Tan, Q. V. Le, “EfficientNet: Rethinking Model Scaling for Convolutional
    Neural Networks,” arxiv:1905.11946, 2019.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] M. Tan, Q. V. Le，“EfficientNet：重新思考卷积神经网络的模型扩展，” arxiv:1905.11946，2019年。'
- en: '[45] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, “mixup: Beyond Empirical
    Risk Minimization,” arXiv:1710.09412, 2017.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz，“mixup：超越经验风险最小化，” arXiv:1710.09412，2017年。'
- en: '[46] R. Takahashi, T. Matsubara, K. Uehara, “Data Augmentation using Random
    Image Cropping and Patching for Deep CNNs,” arXiv:1811.09030, 2018.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Takahashi, T. Matsubara, K. Uehara，“通过随机图像裁剪和拼接进行数据增强以适用于深度CNN，” arXiv:1811.09030，2018年。'
- en: '[47] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J.
    Shi, W. Ouyang, C. C. Loy, D. Lin, “Hybrid Task Cascade for Instance Segmentation,”
    In Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
    pp.4974–4983, 2019.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J.
    Shi, W. Ouyang, C. C. Loy, D. Lin，“实例分割的混合任务级联，” 发表在IEEE计算机视觉与模式识别会议（CVPR）论文集中，第4974–4983页，2019年。'
- en: '[48] Z. Tian, W. Huang, T. He, P. He, Y. Qiao, “Detecting Text in Natural Image
    with Connectionist Text Proposal Network,” arXiv:1609.03605, 2016.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Z. Tian, W. Huang, T. He, P. He, Y. Qiao, “使用连接主义文本提议网络检测自然图像中的文本，” arXiv:1609.03605,
    2016。'
- en: '[49] K. Heafield, “KenLM: Faster and Smaller Language Model Queries,” In Proc.
    of the Sixth Workshop on Statistical Machine Translation, pp.187–197, 2011.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] K. Heafield, “KenLM: 更快更小的语言模型查询，” 见第六届统计机器翻译研讨会论文集，第187–197页, 2011。'
- en: '[50] S. Kim, T. Hori, S. Watanabe, “Joint CTC-attention based end-to-end speech
    recognition using multi-task learning,” in Proc. of the IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP), 2017.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Kim, T. Hori, S. Watanabe, “基于 CTC-注意力的端到端语音识别联合多任务学习，” 见 IEEE 国际声学、语音与信号处理会议（ICASSP）论文集,
    2017。'
- en: '[51] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, Weijun Wang,
    Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le, H. Adam, “Searching for MobileNetV3,”
    arXiv:1905.02244, 2019.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, Weijun Wang,
    Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le, H. Adam, “寻找 MobileNetV3，” arXiv:1905.02244,
    2019。'
- en: '[52] S. Gidaris, N. Komodakis, Object detection via a multi-region & semantic
    segmentation-aware CNN model arXiv:1505.01749, 2015.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S. Gidaris, N. Komodakis, 基于多区域和语义分割感知 CNN 模型的目标检测 arXiv:1505.01749, 2015。'
- en: '[53] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, “You Only Look Once: Unified,
    Real-Time Object Detection,” arXiv:1506.02640, 2015.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, “你只需看一次: 统一的实时目标检测，” arXiv:1506.02640,
    2015。'
