- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:02:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:02:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2003.04664] Automatic Curriculum Learning For Deep RL: A Short Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2003.04664] 深度强化学习的自动课程学习：简短调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2003.04664](https://ar5iv.labs.arxiv.org/html/2003.04664)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2003.04664](https://ar5iv.labs.arxiv.org/html/2003.04664)
- en: 'Automatic Curriculum Learning For Deep RL: A Short Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习的自动课程学习：简短调查
- en: Rémy Portelas¹    Cédric Colas¹    Lilian Weng²    Katja Hofmann³&Pierre-Yves
    Oudeyer¹ ¹Inria, France
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Rémy Portelas¹    Cédric Colas¹    Lilian Weng²    Katja Hofmann³&Pierre-Yves
    Oudeyer¹ ¹Inria，法国
- en: ²OpenAI, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²OpenAI，美国
- en: ³Microsoft Research, UK
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³微软研究院，英国
- en: remy.portelas@inria.fr
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: remy.portelas@inria.fr
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Automatic Curriculum Learning (ACL) has become a cornerstone of recent successes
    in Deep Reinforcement Learning (DRL). These methods shape the learning trajectories
    of agents by challenging them with tasks adapted to their capacities. In recent
    years, they have been used to improve sample efficiency and asymptotic performance,
    to organize exploration, to encourage generalization or to solve sparse reward
    problems, among others. To do so, ACL mechanisms can act on many aspects of learning
    problems. They can optimize domain randomization for Sim2Real transfer, organize
    task presentations in multi-task robotic settings, order sequences of opponents
    in multi-agent scenarios, etc. The ambition of this work is dual: 1) to present
    a compact and accessible introduction to the Automatic Curriculum Learning literature
    and 2) to draw a bigger picture of the current state of the art in ACL to encourage
    the cross-breeding of existing concepts and the emergence of new ideas.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自动课程学习（ACL）已成为深度强化学习（DRL）近期成功的基石。这些方法通过挑战代理以适应其能力的任务来塑造学习轨迹。近年来，它们被用于提高样本效率和渐近性能，组织探索，鼓励泛化或解决稀疏奖励问题等。为此，ACL机制可以作用于学习问题的许多方面。它们可以优化领域随机化以实现Sim2Real转移，在多任务机器人环境中组织任务呈现，在多智能体场景中排序对手序列等。本工作的目标是双重的：1）提供对自动课程学习文献的紧凑且易于理解的介绍；2）描绘ACL领域的当前状态，鼓励现有概念的交叉融合和新思想的出现。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Human learning is organized into a curriculum of interdependent learning situations
    of various complexities. For sure, Homer learned to formulate words before he
    could compose the Iliad. This idea was first transferred to machine learning in
    [Selfridge et al.](#bib.bib56) Selfridge et al. ([1985](#bib.bib56)), where authors
    designed a learning scheme to train a cart pole controller: first training on
    long and light poles, then gradually moving towards shorter and heavier poles.
    A related concept was also developed by [Schmidhuber](#bib.bib55) Schmidhuber
    ([1991](#bib.bib55)), who proposed to improve world model learning by organizing
    exploration through artificial curiosity. In the following years, curriculum learning
    was applied to organize the presentation of training examples or the growth in
    model capacity in various supervised learning settings Elman ([1993](#bib.bib15));
    Krueger and Dayan ([2009](#bib.bib32)); Bengio et al. ([2009](#bib.bib7)). In
    parallel, the developmental robotics community proposed learning progress as a
    way to self-organize open-ended developmental trajectories of learning agents Oudeyer
    et al. ([2007](#bib.bib44)). Inspired by these earlier works, the Deep Reinforcement
    Learning (DRL) community developed a family of mechanisms called Automatic Curriculum
    Learning, which we propose to define as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人类学习被组织成一个具有不同复杂度的相互依赖的学习情境课程。毫无疑问，荷马在创作《伊利亚特》之前先学会了构词。这个概念首次被引入机器学习中是由[Selfridge
    et al.](#bib.bib56) Selfridge 等人（[1985](#bib.bib56)）提出的，他们设计了一种学习方案来训练一个小车摆控制器：首先训练长而轻的杆子，然后逐渐过渡到更短更重的杆子。一个相关的概念也由[Schmidhuber](#bib.bib55)
    Schmidhuber（[1991](#bib.bib55)）提出，他建议通过人工好奇心来组织探索，从而改进世界模型学习。在接下来的几年中，课程学习被应用于组织训练示例的呈现或在各种监督学习环境中增加模型容量 Elman（[1993](#bib.bib15)）；Krueger
    和 Dayan（[2009](#bib.bib32)）；Bengio 等人（[2009](#bib.bib7)）。与此同时，发展机器人社区提出了学习进展作为自组织开放式发展轨迹的方式 Oudeyer
    等人（[2007](#bib.bib44)）。受到这些早期工作的启发，深度强化学习（DRL）社区开发了一系列机制，称为自动课程学习，我们建议定义如下：
- en: Automatic Curriculum Learning (ACL) for DRL is a family of mechanisms that automatically
    adapt the distribution of training data by learning to adjust the selection of
    learning situations to the capabilities of DRL agents.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 用于DRL的自动课程学习（ACL）是一类机制，通过学习调整学习情境的选择来自动适应训练数据的分布，以适应DRL代理的能力。
- en: Related fields.
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 相关领域。
- en: ACL shares many connections with other fields. For example, ACL can be used
    in the context of Transfer Learning where agents are trained on one distribution
    of tasks and tested on another Taylor and Stone ([2009](#bib.bib61)). Continual
    Learning trains agents to be robust to unforeseen changes in the environment while
    ACL assumes agents to stay in control of learning scenarios Lesort et al. ([2019](#bib.bib35)).
    Policy Distillation techniques Czarnecki et al. ([2019](#bib.bib14)) form a complementary
    toolbox to target multi-task RL settings, where knowledge can be transferred from
    one policy to another (e.g. from task-expert policies to a generalist policy).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ACL与其他领域有许多联系。例如，ACL可以用于迁移学习的背景，其中代理在一个任务分布上进行训练，在另一个任务分布上进行测试 Taylor and Stone
    ([2009](#bib.bib61))。持续学习训练代理以对环境中不可预见的变化保持鲁棒性，而ACL假设代理能够控制学习场景 Lesort et al.
    ([2019](#bib.bib35))。政策蒸馏技术 Czarnecki et al. ([2019](#bib.bib14)) 形成了一个互补的工具箱，旨在针对多任务RL设置，其中知识可以从一个策略转移到另一个策略（例如，从任务专家策略到通用策略）。
- en: Scope.
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 范围。
- en: This short survey proposes a typology of ACL mechanisms when combined with DRL
    algorithms and, as such, does not review population-based algorithms implementing
    ACL (e.g. [Forestier et al.](#bib.bib22) Forestier et al. ([2017](#bib.bib22)),
    [Wang et al.](#bib.bib64) Wang et al. ([2019](#bib.bib64))). As per our adopted
    definition, ACL refers to mechanisms explicitly optimizing the automatic organization
    of training data. Hence, they should not be confounded with emergent curricula,
    by-products of distinct mechanisms. For instance, the on-policy training of a
    DRL algorithm is not considered ACL, because the shift in the distribution of
    training data emerges as a by-product of policy learning. Given this is a short
    survey, we do not present the details of every particular mechanism. As the current
    ACL literature lacks theoretical foundations to ground proposed approaches in
    a formal framework, this survey focuses on empirical results.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本简短的调查提出了一种ACL机制的分类，当与DRL算法结合时，并且不回顾实施ACL的基于人口的算法（例如，[Forestier et al.](#bib.bib22)
    Forestier et al. ([2017](#bib.bib22))，[Wang et al.](#bib.bib64) Wang et al. ([2019](#bib.bib64))）。根据我们采用的定义，ACL指的是显式优化训练数据自动组织的机制。因此，它们不应与突现课程混淆，后者是不同机制的副产品。例如，DRL算法的按策略训练不被视为ACL，因为训练数据分布的变化是政策学习的副产品。鉴于这是一个简短的调查，我们没有呈现每个具体机制的详细信息。由于当前ACL文献缺乏理论基础来将提出的方法置于正式框架中，本调查重点关注实证结果。
- en: 2 Automatic Curriculum Learning for DRL
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 自动课程学习用于DRL
- en: This section formalizes the definition of ACL for Deep RL and proposes a classification.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节形式化了深度强化学习（Deep RL）中ACL的定义，并提出了一个分类。
- en: Deep Reinforcement Learning
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: 'is a family of algorithms which leverage deep neural networks for function
    approximation to tackle reinforcement learning problems. DRL agents learn to perform
    sequences of actions $a$ given states $s$ in an environment so as to maximize
    some notion of cumulative reward $r$ Sutton and Barto ([2018](#bib.bib60)). Such
    problems are usually called tasks and formalized as Markov Decision Processes
    (MDPs) of the form $T=\langle\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\rho_{0}\rangle$
    where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $\mathcal{P}:S\times
    A\times S\rightarrow[0,1]$ is a transition function characterizing the probability
    of switching from the current state $s$ to the next state $s^{\prime}$ given action
    $a$, $\mathcal{R}:S\times A\rightarrow\mathbb{R}$ is a reward function and $\rho_{0}$
    is a distribution of initial states. To challenge the generalization capacities
    of agents Cobbe et al. ([2018](#bib.bib10)), the community introduced multi-task
    DRL problems where agents are trained on tasks sampled from a task space: $T\sim\mathcal{T}$.
    In multi-goal DRL, policies and reward functions are conditioned on goals, which
    augments the task-MDP with a goal space $\mathcal{G}$ Schaul et al. ([2015a](#bib.bib53)).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 是一类利用深度神经网络进行函数逼近的算法，以解决强化学习问题。DRL代理在环境中学习根据状态$s$执行一系列动作$a$，以最大化某种累积奖励$r$的概念Sutton和Barto（[2018](#bib.bib60)）。这类问题通常称为任务，并形式化为Markov决策过程（MDPs），其形式为$T=\langle\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\rho_{0}\rangle$，其中$\mathcal{S}$是状态空间，$\mathcal{A}$是动作空间，$\mathcal{P}:S\times
    A\times S\rightarrow[0,1]$是描述从当前状态$s$在给定动作$a$的情况下转移到下一个状态$s^{\prime}$的概率的转移函数，$\mathcal{R}:S\times
    A\rightarrow\mathbb{R}$是奖励函数，$\rho_{0}$是初始状态的分布。为了挑战代理的泛化能力，Cobbe等人（[2018](#bib.bib10)）提出了多任务DRL问题，其中代理在从任务空间中采样的任务上进行训练：$T\sim\mathcal{T}$。在多目标DRL中，策略和奖励函数依赖于目标，这增加了一个目标空间$\mathcal{G}$，从而增强了任务-MDP的功能Schaul等人（[2015a](#bib.bib53)）。
- en: Automatic Curriculum Learning
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自动课程学习
- en: 'mechanisms propose to learn a task selection function $\mathcal{D}:\mathcal{\mathcal{H}\to\mathcal{T}}$
    where $\mathcal{H}$ can contain any information about past interactions. This
    is done with the objective of maximizing a metric $P$ computed over a distribution
    of target tasks $\mathcal{T}_{target}$ after $N$ training steps:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 机制提出了学习任务选择函数$\mathcal{D}:\mathcal{\mathcal{H}\to\mathcal{T}}$的方法，其中$\mathcal{H}$可以包含有关过去交互的任何信息。其目标是在$N$训练步骤后，最大化在目标任务分布$\mathcal{T}_{target}$上的指标$P$：
- en: '|  | $Obj:\max_{\mathcal{D}}\int_{T\sim\mathcal{T}_{target}}\!P_{T}^{N}\,\mathrm{d}T,$
    |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $Obj:\max_{\mathcal{D}}\int_{T\sim\mathcal{T}_{target}}\!P_{T}^{N}\,\mathrm{d}T,$
    |  | (1) |'
- en: where $P_{T}^{N}$ quantifies the agent’s behavior on task $T$ after $N$ training
    steps (e.g. cumulative reward, exploration score). In that sense, ACL can be seen
    as a particular case of meta-learning, where $\mathcal{D}$ is learned along training
    to improve further learning.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$P_{T}^{N}$量化了在任务$T$上经过$N$训练步骤后的代理行为（例如累积奖励、探索得分）。从这个意义上讲，ACL可以看作是元学习的一个特例，其中$\mathcal{D}$在训练过程中被学习，以进一步提高学习效果。
- en: ACL Typology.
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ACL类型学。
- en: 'We propose a classification of ACL mechanisms based on three dimensions:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了基于三个维度的ACL机制分类。
- en: '1.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Why use ACL? We review the different objectives that ACL has been used for
    (Section [3](#S3 "3 Why use ACL? ‣ Automatic Curriculum Learning For Deep RL:
    A Short Survey")).'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '为什么使用ACL？我们回顾了ACL被使用的不同目标（第[3](#S3 "3 Why use ACL? ‣ Automatic Curriculum Learning
    For Deep RL: A Short Survey")节）。'
- en: '2.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'What does ACL control? ACL can target different aspects of the learning problem
    (e.g. environments, goals, reward functions, Section [4](#S4 "4 What does ACL
    control? ‣ Automatic Curriculum Learning For Deep RL: A Short Survey"))'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ACL控制什么？ACL可以针对学习问题的不同方面（例如环境、目标、奖励函数，第[4](#S4 "4 What does ACL control? ‣
    Automatic Curriculum Learning For Deep RL: A Short Survey")节）'
- en: '3.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'What does ACL optimize? ACL mechanisms usually target surrogate objectives
    (e.g. learning progress, diversity) to alleviate the difficulty to optimize the
    main objective $Obj$ directly (Section [5](#S5 "5 What Does ACL Optimize? ‣ Automatic
    Curriculum Learning For Deep RL: A Short Survey")).'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ACL优化什么？ACL机制通常针对代理目标（例如学习进度、多样性）以减轻直接优化主要目标$Obj$的难度（第[5](#S5 "5 What Does
    ACL Optimize? ‣ Automatic Curriculum Learning For Deep RL: A Short Survey")节）。'
- en: 3 Why use ACL?
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 为什么使用ACL？
- en: 'ACL mechanisms can be used for different purposes that can be seen as particular
    instantiations of the general objective defined in Eq [1](#S2.E1 "In Automatic
    Curriculum Learning ‣ 2 Automatic Curriculum Learning for DRL ‣ Automatic Curriculum
    Learning For Deep RL: A Short Survey").'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 'ACL机制可以用于不同的目的，这些目的可以被视为在方程式 [1](#S2.E1 "In Automatic Curriculum Learning ‣
    2 Automatic Curriculum Learning for DRL ‣ Automatic Curriculum Learning For Deep
    RL: A Short Survey")中定义的总体目标的具体实例。'
- en: Improving performance on a restricted task set.
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提高在受限任务集上的性能。
- en: Classical RL problems are about solving a given task, or a restricted task set
    (e.g. which vary by their initial state). In these simple settings, ACL has been
    used to improve sample efficiency or asymptotical performance Schaul et al. ([2015b](#bib.bib54));
    Horgan et al. ([2018](#bib.bib28)); Flet-Berliac and Preux ([2019](#bib.bib19)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 经典RL问题涉及解决给定任务或受限任务集（例如，由初始状态变化的任务）。在这些简单设置中，ACL已被用来提高样本效率或渐近性能 Schauls et al.
    ([2015b](#bib.bib54)); Horgan et al. ([2018](#bib.bib28)); Flet-Berliac and Preux
    ([2019](#bib.bib19))。
- en: Solving hard tasks.
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决困难任务。
- en: Sometimes the target tasks cannot be solved directly (e.g. too hard or sparse
    rewards). In that case, ACL can be used to pose auxiliary tasks to the agent,
    gradually guiding its learning trajectory from simple to difficult tasks until
    the target tasks are solved. In recent works, ACL was used to schedule DRL agents
    from simple mazes to hard ones Matiisen et al. ([2017](#bib.bib38)), or from close-to-success
    initial states to challenging ones in robotic control scenarios Florensa et al.
    ([2017](#bib.bib20)); Ivanovic et al. ([2018](#bib.bib29)) and video games Salimans
    and Chen ([2018](#bib.bib52)). Another line of work proposes to use ACL to organize
    the exploration of the state space so as to solve sparse reward problems Bellemare
    et al. ([2016](#bib.bib6)); Pathak et al. ([2017](#bib.bib45)); Shyam et al. ([2018](#bib.bib57));
    Pathak et al. ([2019](#bib.bib46)); Burda et al. ([2019](#bib.bib8)). In these
    works, the performance reward is augmented with an intrinsic reward guiding the
    agent towards uncertain areas of the state space.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有时目标任务不能直接解决（例如，过于困难或稀疏的奖励）。在这种情况下，可以使用ACL来给智能体设置辅助任务，逐步引导其学习轨迹从简单任务到困难任务，直到目标任务得到解决。在最近的研究中，ACL被用来从简单的迷宫到困难的迷宫调度DRL智能体
    Matiisen et al. ([2017](#bib.bib38))，或在机器人控制场景中从接近成功的初始状态到具有挑战性的状态 Florensa et
    al. ([2017](#bib.bib20)); Ivanovic et al. ([2018](#bib.bib29))，以及视频游戏 Salimans
    and Chen ([2018](#bib.bib52))。另一种研究方向提出使用ACL来组织状态空间的探索，以解决稀疏奖励问题 Bellemare et
    al. ([2016](#bib.bib6)); Pathak et al. ([2017](#bib.bib45)); Shyam et al. ([2018](#bib.bib57));
    Pathak et al. ([2019](#bib.bib46)); Burda et al. ([2019](#bib.bib8))。在这些研究中，性能奖励通过内在奖励进行增强，以引导智能体朝向状态空间的不确定区域。
- en: Training generalist agents.
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练通用智能体。
- en: Generalist agents must be able to solve tasks they have not encountered during
    training (e.g. continuous task spaces or distinct training and testing set). ACL
    can shape learning trajectories to improve generalization, e.g. by avoiding unfeasible
    task subspaces Portelas et al. ([2019](#bib.bib49)). ACL can also help agents
    to generalize from simulation settings to the real world (Sim2Real) OpenAI et
    al. ([2019](#bib.bib43)); Mehta et al. ([2019](#bib.bib39)) or to maximize performance
    and robustness in multi-agent settings via Self-Play Silver et al. ([2017](#bib.bib58));
    Pinto et al. ([2017](#bib.bib47)); Bansal et al. ([2017](#bib.bib3)); Baker et
    al. ([2019](#bib.bib2)); Vinyals et al. ([2019](#bib.bib63)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通用智能体必须能够解决在训练过程中未遇到的任务（例如，连续任务空间或不同的训练和测试集）。ACL可以通过避免不可行的任务子空间来塑造学习轨迹以提高泛化能力，例如
    Portelas et al. ([2019](#bib.bib49))。ACL还可以帮助智能体从仿真设置到现实世界（Sim2Real）进行泛化 OpenAI
    et al. ([2019](#bib.bib43)); Mehta et al. ([2019](#bib.bib39))，或者通过自我对弈在多智能体环境中最大化性能和鲁棒性
    Silver et al. ([2017](#bib.bib58)); Pinto et al. ([2017](#bib.bib47)); Bansal
    et al. ([2017](#bib.bib3)); Baker et al. ([2019](#bib.bib2)); Vinyals et al. ([2019](#bib.bib63))。
- en: Training multi-goal agents.
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练多目标智能体。
- en: In multi-goal RL, agents are trained and tested on tasks that vary by their
    goals. Because agents can control the goals they target, they learn a behavioral
    repertoire through one or several goal-conditioned policies. The adoption of ACL
    in this setting can improve performance on a testing set of pre-defined goals.
    Recent works demonstrated the benefits of using ACL in scenarios such as multi-goal
    robotic arm manipulation Andrychowicz et al. ([2017](#bib.bib1)); Zhao and Tresp
    ([2018](#bib.bib66)); Fournier et al. ([2018](#bib.bib23)); Eppe et al. ([2018](#bib.bib16));
    Zhao and Tresp ([2019](#bib.bib67)); Fang et al. ([2019](#bib.bib18)); Colas et
    al. ([2019](#bib.bib11)) or multi-goal navigation Sukhbaatar et al. ([2017](#bib.bib59));
    Florensa et al. ([2018](#bib.bib21)); Racanière et al. ([2019](#bib.bib50)); Cideron
    et al. ([2019](#bib.bib9)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在多目标强化学习中，代理在目标不同的任务上进行训练和测试。由于代理可以控制他们所针对的目标，他们通过一个或多个目标条件策略来学习行为库。在这种情况下采用ACL可以提高在预定义目标测试集上的表现。近期的研究展示了在诸如多目标机器人臂操作
    Andrychowicz 等人 ([2017](#bib.bib1)); Zhao 和 Tresp ([2018](#bib.bib66)); Fournier
    等人 ([2018](#bib.bib23)); Eppe 等人 ([2018](#bib.bib16)); Zhao 和 Tresp ([2019](#bib.bib67));
    Fang 等人 ([2019](#bib.bib18)); Colas 等人 ([2019](#bib.bib11)) 或多目标导航 Sukhbaatar
    等人 ([2017](#bib.bib59)); Florensa 等人 ([2018](#bib.bib21)); Racanière 等人 ([2019](#bib.bib50));
    Cideron 等人 ([2019](#bib.bib9)) 等场景中使用ACL的好处。
- en: Organizing open-ended exploration.
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 组织开放式探索。
- en: In some multi-goal settings, the space of achievable goals is not known in advance.
    Agents must discover achievable goals as they explore and learn how to represent
    and reach them. For this problem, ACL can be used to organize the discovery and
    acquisition of repertoires of robust and diverse behaviors, e.g. from visual observations Eysenbach
    et al. ([2018](#bib.bib17)); Pong et al. ([2019](#bib.bib48)); Jabri et al. ([2019](#bib.bib30))
    or from natural language interactions with social peers Lair et al. ([2019](#bib.bib33));
    Colas et al. ([2020](#bib.bib12)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些多目标设置中，可实现目标的空间事先并不明确。代理必须在探索过程中发现可实现的目标，并学习如何表示和达成这些目标。针对这个问题，可以使用ACL来组织对强大而多样的行为库的发现和获取，例如通过视觉观察
    Eysenbach 等人 ([2018](#bib.bib17)); Pong 等人 ([2019](#bib.bib48)); Jabri 等人 ([2019](#bib.bib30))
    或通过与社会同行的自然语言互动 Lair 等人 ([2019](#bib.bib33)); Colas 等人 ([2020](#bib.bib12))。
- en: 4 What does ACL control?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 ACL控制什么？
- en: 'While on-policy DRL algorithms directly use training data generated by the
    current behavioral policy, off-policy algorithms can use trajectories collected
    from other sources. This practically decouples data collection from data exploitation.
    Hence, we organize this section into two categories: one reviewing ACL for data
    collection, the other ACL for data exploitation.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略学习（DRL）算法中，在线算法直接使用当前行为策略生成的训练数据，而离线算法可以使用从其他来源收集的轨迹。这在实际操作中将数据收集与数据利用解耦。因此，我们将本节组织为两个类别：一个是回顾用于数据收集的ACL，另一个是回顾用于数据利用的ACL。
- en: '![Refer to caption](img/f5d2359b8183d9690b92c2e07a209412.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/f5d2359b8183d9690b92c2e07a209412.png)'
- en: 'Figure 1: ACL for data collection. ACL can control each elements of task MDPs
    to shape the learning trajectories of agents. Given metrics of the agent’s behavior
    like performance or visited states, ACL methods generate new tasks adapted to
    the agent’s abilities.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：数据收集的ACL。ACL可以控制任务MDP的每个元素，以塑造代理的学习轨迹。根据代理的行为指标，如表现或访问状态，ACL方法生成适应代理能力的新任务。
- en: 4.1 ACL for Data Collection
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据收集中的ACL
- en: 'During data collection, ACL organizes the sequential presentation of tasks
    as a function of the agent’s capabilities. To do so, it generates tasks by acting
    on elements of task MDPs (e.g. $\mathcal{R},\mathcal{P},\rho_{0}$, see Fig. [1](#S4.F1
    "Figure 1 ‣ 4 What does ACL control? ‣ Automatic Curriculum Learning For Deep
    RL: A Short Survey")). The curriculum can be designed on a discrete set of tasks
    or on a continuous task space. In single-task problems, ACL can define a set of
    auxiliary tasks to be used as stepping stones towards the resolution of the main
    task. The following paragraphs organize the literature according to the nature
    of the control exerted by ACL:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据收集过程中，ACL根据代理的能力组织任务的顺序展示。为此，它通过作用于任务MDP的元素（例如 $\mathcal{R},\mathcal{P},\rho_{0}$，参见图
    [1](#S4.F1 "Figure 1 ‣ 4 ACL控制什么？ ‣ 自动化课程学习的深度强化学习：简短综述")）来生成任务。课程可以在离散任务集合或连续任务空间上设计。在单任务问题中，ACL可以定义一组辅助任务，作为解决主要任务的垫脚石。以下段落根据ACL施加的控制性质组织文献：
- en: Initial state $(\rho_{0})$.
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 初始状态 $(\rho_{0})$。
- en: The distribution of initial states $\rho_{0}$ can be controlled to modulate
    the difficulty of a task. Agents start learning from states close to a given target
    (i.e. easier tasks), then move towards harder tasks by gradually increasing the
    distance between the initial states and the target. This approach is especially
    effective to design auxiliary tasks for complex control scenarios with sparse
    rewards Florensa et al. ([2017](#bib.bib20)); Ivanovic et al. ([2018](#bib.bib29));
    Salimans and Chen ([2018](#bib.bib52)).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 初始状态分布 $\rho_{0}$ 可以被控制以调节任务的难度。智能体从接近给定目标的状态开始学习（即较容易的任务），然后通过逐渐增加初始状态与目标之间的距离，向更难的任务过渡。这种方法在设计具有稀疏奖励的复杂控制场景的辅助任务时尤其有效，Florensa
    等人 ([2017](#bib.bib20))；Ivanovic 等人 ([2018](#bib.bib29))；Salimans 和 Chen ([2018](#bib.bib52))。
- en: Reward functions $(\mathcal{R})$.
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 奖励函数 $(\mathcal{R})$。
- en: 'ACL can be used for automatic reward shaping: adapting the reward function
    $\mathcal{R}$ as a function of the learning trajectory of the agent. In curiosity-based
    approaches especially, an internal reward function guides agents towards areas
    associated with high uncertainty to foster exploration Bellemare et al. ([2016](#bib.bib6));
    Pathak et al. ([2017](#bib.bib45)); Shyam et al. ([2018](#bib.bib57)); Pathak
    et al. ([2019](#bib.bib46)); Burda et al. ([2019](#bib.bib8)). As the agent explores,
    uncertain areas –and thus the reward function– change, which automatically devises
    a learning curriculum guiding the exploration of the state space. In [Fournier
    et al.](#bib.bib23) Fournier et al. ([2018](#bib.bib23)), an ACL mechanism controls
    the tolerance in a goal reaching task. Starting with a low accuracy requirement,
    it gradually and automatically shifts towards stronger accuracy requirements as
    the agent progresses. In [Eysenbach et al.](#bib.bib17) Eysenbach et al. ([2018](#bib.bib17))
    and [Jabri et al.](#bib.bib30) Jabri et al. ([2019](#bib.bib30)), authors propose
    to learn a skill space in unsupervised settings (from state space and pixels respectively),
    from which are derived reward functions promoting both behavioral diversity and
    skill separation.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ACL 可用于自动奖励塑造：根据智能体的学习轨迹调整奖励函数 $\mathcal{R}$。特别是在基于好奇心的方法中，内部奖励函数引导智能体朝向与高不确定性相关的区域，以促进探索
    Bellemare 等人 ([2016](#bib.bib6))；Pathak 等人 ([2017](#bib.bib45))；Shyam 等人 ([2018](#bib.bib57))；Pathak
    等人 ([2019](#bib.bib46))；Burda 等人 ([2019](#bib.bib8))。随着智能体的探索，不确定的区域——也即奖励函数——会发生变化，这自动制定了一个学习课程，指导对状态空间的探索。在
    [Fournier 等人](#bib.bib23) Fournier 等人 ([2018](#bib.bib23)) 中，ACL 机制控制目标达成任务中的容忍度。最初设置较低的准确性要求，随着智能体的进展，逐渐并自动地转向更高的准确性要求。在
    [Eysenbach 等人](#bib.bib17) Eysenbach 等人 ([2018](#bib.bib17)) 和 [Jabri 等人](#bib.bib30)
    Jabri 等人 ([2019](#bib.bib30)) 中，作者建议在无监督设置中学习技能空间（分别来自状态空间和像素），从中导出促进行为多样性和技能分离的奖励函数。
- en: Goals $(\mathcal{G})$.
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 目标 $(\mathcal{G})$。
- en: In multi-goal DRL, ACL techniques can be applied to order the selection of goals
    from discrete sets Lair et al. ([2019](#bib.bib33)), continuous goal spaces Sukhbaatar
    et al. ([2017](#bib.bib59)); Florensa et al. ([2018](#bib.bib21)); Pong et al.
    ([2019](#bib.bib48)); Racanière et al. ([2019](#bib.bib50)) or even sets of different
    goal spaces Eppe et al. ([2018](#bib.bib16)); Colas et al. ([2019](#bib.bib11)).
    Although goal spaces are usually pre-defined, recent work proposed to apply ACL
    on a goal space learned from pixels using a generative model Pong et al. ([2019](#bib.bib48)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在多目标深度强化学习（DRL）中，可以应用ACL技术来对离散集合中的目标选择进行排序，Lair 等人 ([2019](#bib.bib33))，连续目标空间
    Sukhbaatar 等人 ([2017](#bib.bib59))；Florensa 等人 ([2018](#bib.bib21))；Pong 等人 ([2019](#bib.bib48))；Racanière
    等人 ([2019](#bib.bib50))，甚至不同目标空间的集合 Eppe 等人 ([2018](#bib.bib16))；Colas 等人 ([2019](#bib.bib11))。虽然目标空间通常是预定义的，但最近的研究建议在通过生成模型从像素学习得到的目标空间上应用ACL，Pong
    等人 ([2019](#bib.bib48))。
- en: Environments $(\mathcal{S},\mathcal{P})$.
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 环境 $(\mathcal{S},\mathcal{P})$。
- en: ACL has been successfully applied to organize the selection of environments
    from a discrete set, e.g. to choose among Minecraft mazes Matiisen et al. ([2017](#bib.bib38))
    or Sonic the Hedgehog levels Mysore et al. ([2018](#bib.bib41)). A more general
    –and arguably more powerful– approach is to leverage parametric Procedural Content
    Generation (PCG) techniques Risi and Togelius ([2019](#bib.bib51)) to generate
    rich task spaces. In that case, ACL allows to detect relevant niches of progress OpenAI
    et al. ([2019](#bib.bib43)); Portelas et al. ([2019](#bib.bib49)); Mehta et al.
    ([2019](#bib.bib39)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ACL 已成功应用于从离散集合中组织环境选择，例如在 Minecraft 迷宫中进行选择 Matiisen et al. ([2017](#bib.bib38))
    或 Sonic the Hedgehog 关卡 Mysore et al. ([2018](#bib.bib41))。一种更为通用——且可以说更强大的——方法是利用参数化程序生成内容
    (PCG) 技术 Risi and Togelius ([2019](#bib.bib51)) 来生成丰富的任务空间。在这种情况下，ACL 可以检测到相关的进展领域
    OpenAI et al. ([2019](#bib.bib43)); Portelas et al. ([2019](#bib.bib49)); Mehta
    et al. ([2019](#bib.bib39))。
- en: Opponents $(\mathcal{S},\mathcal{P})$.
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对手 $(\mathcal{S},\mathcal{P})$。
- en: Self-play algorithms train agents against present or past versions of themselves Silver
    et al. ([2017](#bib.bib58)); Bansal et al. ([2017](#bib.bib3)); Vinyals et al.
    ([2019](#bib.bib63)); Baker et al. ([2019](#bib.bib2)). The set of opponents directly
    maps to a set of tasks, as different opponents results in different transition
    functions $\mathcal{P}$ and possibly state spaces $\mathcal{S}$. Self-play can
    thus be seen as a form of ACL, where the sequence of opponents (i.e. tasks) is
    organized to maximize performance and robustness. In single-agent settings, an
    adversary policy can be trained to perturb the main agent Pinto et al. ([2017](#bib.bib47)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 自我对弈算法训练代理与当前或过去版本的自身对抗 Silver et al. ([2017](#bib.bib58)); Bansal et al. ([2017](#bib.bib3));
    Vinyals et al. ([2019](#bib.bib63)); Baker et al. ([2019](#bib.bib2))。对手的集合直接映射到任务集合，因为不同的对手会导致不同的转换函数
    $\mathcal{P}$ 和可能的状态空间 $\mathcal{S}$。因此，自我对弈可以看作是一种ACL形式，其中对手的序列（即任务）被组织以最大化性能和鲁棒性。在单代理设置中，可以训练一个对抗策略来扰动主要代理
    Pinto et al. ([2017](#bib.bib47))。
- en: 4.2 ACL for Data Exploitation
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 ACL 用于数据利用
- en: 'ACL can also be used in the data exploitation stage, by acting on training
    data previously collected and stored in a replay memory. It enables the agent
    to “mentally experience the effects of its actions without actually executing
    them”, a technique known as experience replay Lin ([1992](#bib.bib36)). At the
    data exploitation level, ACL can exert two types of control on the distribution
    of training data: transition selection and transition modification.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ACL 也可以在数据利用阶段中使用，通过作用于之前收集并存储在回放记忆中的训练数据。它使代理能够“在不实际执行操作的情况下心理上体验其操作的效果”，这是一种称为经验回放的技术
    Lin ([1992](#bib.bib36))。在数据利用层面，ACL 可以对训练数据的分布施加两种控制：转换选择和转换修改。
- en: Transition selection $(\mathcal{S}\times\mathcal{A})$.
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转换选择 $(\mathcal{S}\times\mathcal{A})$。
- en: Inspired from the prioritized sweeping technique that organized the order of
    updates in planning methods Moore and Atkeson ([1993](#bib.bib40)), [Schaul et
    al.](#bib.bib54) Schaul et al. ([2015b](#bib.bib54)) introduced prioritized experience
    replay (PER) for model-free off-policy RL to bias the selection of transitions
    for policy updates, as some transitions might be more informative than others.
    Different ACL methods propose different metrics to evaluate the importance of
    each transition Schaul et al. ([2015b](#bib.bib54)); Zhao and Tresp ([2018](#bib.bib66));
    Colas et al. ([2019](#bib.bib11)); Zhao and Tresp ([2019](#bib.bib67)); Lair et
    al. ([2019](#bib.bib33)); Colas et al. ([2020](#bib.bib12)). Transition selection
    ACL techniques can also be used for on-policy algorithms to filter online learning
    batches Flet-Berliac and Preux ([2019](#bib.bib19)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从优先级更新技术中获得灵感，该技术组织了规划方法中的更新顺序 Moore and Atkeson ([1993](#bib.bib40))，[Schaul
    et al.](#bib.bib54) Schaul et al. ([2015b](#bib.bib54)) 引入了优先级经验回放 (PER)，用于无模型的离策略
    RL，以偏向选择用于策略更新的转换，因为一些转换可能比其他转换更具信息性。不同的ACL方法提出了不同的度量来评估每个转换的重要性 Schaul et al.
    ([2015b](#bib.bib54)); Zhao and Tresp ([2018](#bib.bib66)); Colas et al. ([2019](#bib.bib11));
    Zhao and Tresp ([2019](#bib.bib67)); Lair et al. ([2019](#bib.bib33)); Colas et
    al. ([2020](#bib.bib12))。转换选择ACL技术也可以用于在策略算法中筛选在线学习批次 Flet-Berliac and Preux ([2019](#bib.bib19))。
- en: Transition modification $(\mathcal{G})$.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转换修改 $(\mathcal{G})$。
- en: In multi-goal settings, Hindsight Experience Replay (HER) proposes to reinterpret
    trajectories collected with a given target goal with respect to a different goal
    Andrychowicz et al. ([2017](#bib.bib1)). In practice, HER modifies transitions
    by substituting target goals $g$ with one of the outcomes $g^{\prime}$ achieved
    later in the trajectory, as well as the corresponding reward $r^{\prime}=R_{g^{\prime}}(s,a)$.
    By explicitly biasing goal substitution to increase the probability of sampling
    rewarded transitions, HER shifts the training data distribution from simpler goals
    (achieved now) towards more complex goals as the agent makes progress. Substitute
    goal selection can be guided by other ACL mechanisms (e.g. favoring diversity Fang
    et al. ([2019](#bib.bib18)); Cideron et al. ([2019](#bib.bib9))).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在多目标设置中，Hindsight Experience Replay (HER) 提出重新解释收集到的轨迹，将其与不同的目标进行比较 Andrychowicz
    等 ([2017](#bib.bib1))。在实践中，HER 通过将目标 $g$ 替换为轨迹中稍后实现的结果之一 $g^{\prime}$，以及相应的奖励
    $r^{\prime}=R_{g^{\prime}}(s,a)$ 来修改过渡。通过明确偏向目标替换以增加采样奖励过渡的概率，HER 将训练数据分布从较简单的目标（现在实现的）转移到更复杂的目标，随着智能体的进步。替代目标选择可以由其他
    ACL 机制引导（例如，偏向多样性 Fang 等 ([2019](#bib.bib18)); Cideron 等 ([2019](#bib.bib9))）。
- en: 5 What Does ACL Optimize?
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 ACL 优化什么？
- en: '| Algorithm | Why use ACL? | What does ACL control? | What does ACL optimize?
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 为什么使用 ACL？ | ACL 控制什么？ | ACL 优化什么？ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ACL for Data Collection (§ [4.1](#S4.SS1 "4.1 ACL for Data Collection ‣ 4
    What does ACL control? ‣ Automatic Curriculum Learning For Deep RL: A Short Survey")):
    |  |  |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 数据收集中的 ACL (§ [4.1](#S4.SS1 "4.1 数据收集中的 ACL ‣ 4 ACL 控制什么？ ‣ 自动课程学习用于深度 RL：简短调查")):
    |  |  |  |'
- en: '| ADR (OpenAI)  OpenAI et al. ([2019](#bib.bib43)) | Generalization | Environments
    $(\mathcal{S},\mathcal{P})$ (PCG) | Intermediate difficulty |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| ADR (OpenAI)  OpenAI 等 ([2019](#bib.bib43)) | 泛化 | 环境 $(\mathcal{S},\mathcal{P})$
    (PCG) | 中间难度 |'
- en: '| ADR (Mila)  Mehta et al. ([2019](#bib.bib39)) | Generalization | Environments
    $(\mathcal{P})$ (PCG) | Intermediate diff. & Diversity |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| ADR (Mila)  Mehta 等 ([2019](#bib.bib39)) | 泛化 | 环境 $(\mathcal{P})$ (PCG)
    | 中间难度和 多样性 |'
- en: '| ALP-GMM  Portelas et al. ([2019](#bib.bib49)) | Generalization | Environments
    $(\mathcal{S})$ (PCG) | LP |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| ALP-GMM  Portelas 等 ([2019](#bib.bib49)) | 泛化 | 环境 $(\mathcal{S})$ (PCG)
    | LP |'
- en: '| RARL  Pinto et al. ([2017](#bib.bib47)) | Generalization | Opponents $(\mathcal{P})$
    | ARM |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| RARL  Pinto 等 ([2017](#bib.bib47)) | 泛化 | 对手 $(\mathcal{P})$ | ARM |'
- en: '| AlphaGO Zero  Silver et al. ([2017](#bib.bib58)) | Generalization | Opponents
    $(\mathcal{P})$ | ARM |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| AlphaGO Zero  Silver 等 ([2017](#bib.bib58)) | 泛化 | 对手 $(\mathcal{P})$ | ARM
    |'
- en: '| Hide&Seek  Baker et al. ([2019](#bib.bib2)) | Generalization | Opponents
    $(\mathcal{P})$ | ARM |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Hide&Seek  Baker 等 ([2019](#bib.bib2)) | 泛化 | 对手 $(\mathcal{P})$ | ARM |'
- en: '| AlphaStar  Vinyals et al. ([2019](#bib.bib63)) | Generalization | Opponents
    $(\mathcal{P})$ | ARM & Diversity |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| AlphaStar  Vinyals 等 ([2019](#bib.bib63)) | 泛化 | 对手 $(\mathcal{P})$ | ARM
    和 多样性 |'
- en: '| Competitive SP  Bansal et al. ([2017](#bib.bib3)) | Generalization | Opponents
    $(\mathcal{P})$ | ARM & Diversity |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 竞争 SP  Bansal 等 ([2017](#bib.bib3)) | 泛化 | 对手 $(\mathcal{P})$ | ARM 和 多样性
    |'
- en: '| RgC  Mysore et al. ([2018](#bib.bib41)) | Generalization | Environments $(\mathcal{S})$
    (DS) | LP |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| RgC  Mysore 等 ([2018](#bib.bib41)) | 泛化 | 环境 $(\mathcal{S})$ (DS) | LP |'
- en: '| RC  Florensa et al. ([2017](#bib.bib20)) | Hard Task | Initial states $(\rho_{0})$
    | Intermediate difficulty |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| RC  Florensa 等 ([2017](#bib.bib20)) | 难任务 | 初始状态 $(\rho_{0})$ | 中间难度 |'
- en: '| $1$-demo RC Salimans and Chen ([2018](#bib.bib52)) | Hard Task | Initial
    states $(\rho_{0})$ | Intermediate difficulty |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| $1$-demo RC  Salimans 和 Chen ([2018](#bib.bib52)) | 难任务 | 初始状态 $(\rho_{0})$
    | 中间难度 |'
- en: '| Count-based  Bellemare et al. ([2016](#bib.bib6)) | Hard Task | Reward functions
    $(\mathcal{R})$ | Diversity |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 基于计数  Bellemare 等 ([2016](#bib.bib6)) | 难任务 | 奖励函数 $(\mathcal{R})$ | 多样性
    |'
- en: '| RND  Burda et al. ([2019](#bib.bib8)) | Hard Task | Reward functions $(\mathcal{R})$
    | Surprise (model error) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| RND  Burda 等 ([2019](#bib.bib8)) | 难任务 | 奖励函数 $(\mathcal{R})$ | 惊讶（模型错误）
    |'
- en: '| ICM  Pathak et al. ([2017](#bib.bib45)) | Hard Task | Reward functions $(\mathcal{R})$
    | Surprise (model error) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ICM  Pathak 等 ([2017](#bib.bib45)) | 难任务 | 奖励函数 $(\mathcal{R})$ | 惊讶（模型错误）
    |'
- en: '| Disagreement  Pathak et al. ([2019](#bib.bib46)) | Hard Task | Reward functions
    $(\mathcal{R})$ | Surprise (model disagreement) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 不一致  Pathak 等 ([2019](#bib.bib46)) | 难任务 | 奖励函数 $(\mathcal{R})$ | 惊讶（模型不一致）
    |'
- en: '| MAX  Shyam et al. ([2018](#bib.bib57)) | Hard Task | Reward functions $(\mathcal{R})$
    | Surprise (model disagreement) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| MAX  Shyam 等 ([2018](#bib.bib57)) | 难任务 | 奖励函数 $(\mathcal{R})$ | 惊讶（模型不一致）
    |'
- en: '| BaRC  Ivanovic et al. ([2018](#bib.bib29)) | Hard Task | Initial states $(\rho_{0})$
    | Intermediate difficulty |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **BaRC**  Ivanovic et al. ([2018](#bib.bib29)) | 困难任务 | 初始状态 $(\rho_{0})$
    | 中等难度 |'
- en: '| TSCL  Matiisen et al. ([2017](#bib.bib38)) | Hard Task | Environments $(\mathcal{S})$
    (DS) | LP |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **TSCL**  Matiisen et al. ([2017](#bib.bib38)) | 困难任务 | 环境 $(\mathcal{S})$
    (DS) | LP |'
- en: '| Acc-based CL  Fournier et al. ([2018](#bib.bib23)) | Multi-Goal | Reward
    function $(\mathcal{R})$ | LP |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **Acc-based CL**  Fournier et al. ([2018](#bib.bib23)) | 多目标 | 奖励函数 $(\mathcal{R})$
    | LP |'
- en: '| Asym. SP  Sukhbaatar et al. ([2017](#bib.bib59)) | Multi-Goal | Goals $(\mathcal{G})$,
    initial states $(\rho_{0})$ | Intermediate difficulty |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **Asym. SP**  Sukhbaatar et al. ([2017](#bib.bib59)) | 多目标 | 目标 $(\mathcal{G})$，初始状态
    $(\rho_{0})$ | 中等难度 |'
- en: '| GoalGAN  Florensa et al. ([2018](#bib.bib21)) | Multi-Goal | Goals $(\mathcal{G})$
    | Intermediate difficulty |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **GoalGAN**  Florensa et al. ([2018](#bib.bib21)) | 多目标 | 目标 $(\mathcal{G})$
    | 中等难度 |'
- en: '| Setter-Solver  Racanière et al. ([2019](#bib.bib50)) | Multi-Goal | Goals
    $(\mathcal{G})$ | Intermediate difficulty |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **Setter-Solver**  Racanière et al. ([2019](#bib.bib50)) | 多目标 | 目标 $(\mathcal{G})$
    | 中等难度 |'
- en: '| CGM  Eppe et al. ([2018](#bib.bib16)) | Multi-Goal | Goals $(\mathcal{G})$
    | Intermediate difficulty |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **CGM**  Eppe et al. ([2018](#bib.bib16)) | 多目标 | 目标 $(\mathcal{G})$ | 中等难度
    |'
- en: '| CURIOUS  Colas et al. ([2019](#bib.bib11)) | Multi-Goal | Goals $(\mathcal{G})$
    | LP |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **CURIOUS**  Colas et al. ([2019](#bib.bib11)) | 多目标 | 目标 $(\mathcal{G})$
    | LP |'
- en: '| Skew-fit  Pong et al. ([2019](#bib.bib48)) | Open-Ended Explo. | Goals $(\mathcal{G})$
    (from pixels) | Diversity |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **Skew-fit**  Pong et al. ([2019](#bib.bib48)) | 开放式探索 | 目标 $(\mathcal{G})$（来自像素）
    | 多样性 |'
- en: '| DIAYN Eysenbach et al. ([2018](#bib.bib17)) | Open-Ended Explo. | Reward
    functions $(\mathcal{R})$ | Diversity |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **DIAYN**  Eysenbach et al. ([2018](#bib.bib17)) | 开放式探索 | 奖励函数 $(\mathcal{R})$
    | 多样性 |'
- en: '| CARML  Jabri et al. ([2019](#bib.bib30)) | Open-Ended Explo. | Reward functions
    $(\mathcal{R})$ | Diversity |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **CARML**  Jabri et al. ([2019](#bib.bib30)) | 开放式探索 | 奖励函数 $(\mathcal{R})$
    | 多样性 |'
- en: '| LE2  Lair et al. ([2019](#bib.bib33)) | Open-Ended Explo. | Goals $(\mathcal{G})$
    | Reward & Diversity |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| **LE2**  Lair et al. ([2019](#bib.bib33)) | 开放式探索 | 目标 $(\mathcal{G})$ |
    奖励 和 多样性 |'
- en: '| ACL for Data Exploitation (§ [4.2](#S4.SS2 "4.2 ACL for Data Exploitation
    ‣ 4 What does ACL control? ‣ Automatic Curriculum Learning For Deep RL: A Short
    Survey")): |  |  |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| **ACL for Data Exploitation** (§ [4.2](#S4.SS2 "4.2 ACL for Data Exploitation
    ‣ 4 What does ACL control? ‣ Automatic Curriculum Learning For Deep RL: A Short
    Survey")): |  |  |  |'
- en: '| Prioritized ER  Schaul et al. ([2015b](#bib.bib54)) | Performance boost |
    Transition selection $(\mathcal{S}\times\mathcal{A})$ | Surprise (TD-error) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **Prioritized ER**  Schaul et al. ([2015b](#bib.bib54)) | 性能提升 | 过渡选择 $(\mathcal{S}\times\mathcal{A})$
    | 惊讶（TD-误差） |'
- en: '| SAUNA  Flet-Berliac and Preux ([2019](#bib.bib19)) | Performance boost |
    Transition selection $(\mathcal{S}\times\mathcal{A})$ | Surprise (V-error) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| **SAUNA**  Flet-Berliac and Preux ([2019](#bib.bib19)) | 性能提升 | 过渡选择 $(\mathcal{S}\times\mathcal{A})$
    | 惊讶（V-误差） |'
- en: '| CURIOUS  Colas et al. ([2019](#bib.bib11)) | Multi-goal | Trans. select.
    & mod. $(\mathcal{S}\times\mathcal{A},\mathcal{G})$ | LP & Energy |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **CURIOUS**  Colas et al. ([2019](#bib.bib11)) | 多目标 | 过渡选择与修改 $(\mathcal{S}\times\mathcal{A},\mathcal{G})$
    | LP 和 能量 |'
- en: '| HER  Andrychowicz et al. ([2017](#bib.bib1)) | Multi-goal | Transition modification
    $(\mathcal{G})$ | Reward |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| **HER**  Andrychowicz et al. ([2017](#bib.bib1)) | 多目标 | 过渡修改 $(\mathcal{G})$
    | 奖励 |'
- en: '| HER-curriculum  Fang et al. ([2019](#bib.bib18)) | Multi-goal | Transition
    modification $(\mathcal{G})$ | Diversity |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| **HER-curriculum**  Fang et al. ([2019](#bib.bib18)) | 多目标 | 过渡修改 $(\mathcal{G})$
    | 多样性 |'
- en: '| Language HER  Cideron et al. ([2019](#bib.bib9)) | Multi-goal | Transition
    modification $(\mathcal{G})$ | Reward |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **Language HER**  Cideron et al. ([2019](#bib.bib9)) | 多目标 | 过渡修改 $(\mathcal{G})$
    | 奖励 |'
- en: '| Curiosity Prio.  Zhao and Tresp ([2019](#bib.bib67)) | Multi-goal | Transition
    selection $(\mathcal{S}\times\mathcal{A})$ | Diversity |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **Curiosity Prio.**  Zhao and Tresp ([2019](#bib.bib67)) | 多目标 | 过渡选择 $(\mathcal{S}\times\mathcal{A})$
    | 多样性 |'
- en: '| En. Based ER  Zhao and Tresp ([2018](#bib.bib66)) | Multi-goal | Transition
    selection $(\mathcal{S}\times\mathcal{A})$ | Energy |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **En. Based ER**  Zhao and Tresp ([2018](#bib.bib66)) | 多目标 | 过渡选择 $(\mathcal{S}\times\mathcal{A})$
    | 能量 |'
- en: '| LE2  Lair et al. ([2019](#bib.bib33)) | Open-Ended Explo. | Trans. select.
    & mod. $(\mathcal{S}\times\mathcal{A},\mathcal{G})$ | Reward |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| **LE2**  Lair et al. ([2019](#bib.bib33)) | 开放式探索 | 过渡选择与修改 $(\mathcal{S}\times\mathcal{A},\mathcal{G})$
    | 奖励 |'
- en: '| IMAGINE  Colas et al. ([2020](#bib.bib12)) | Open-Ended Explo. | Trans. select.
    & mod. $(\mathcal{S}\times\mathcal{A},\mathcal{G})$ | Reward |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| **IMAGINE**  Colas et al. ([2020](#bib.bib12)) | 开放式探索 | 过渡选择与修改 $(\mathcal{S}\times\mathcal{A},\mathcal{G})$
    | 奖励 |'
- en: 'Table 1: Classification of the surveyed papers. The classification is organized
    along the three dimensions defined in the above text. In Why use ACL, we only
    report the main objective of each work. When ACL controls the selection of environments,
    we precise whether it is selecting them from a discrete set (DS) or through parametric
    Procedural Content Generation (PCG). We abbreviate adversarial reward maximization
    by ARM and learning progress by LP.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：调查论文的分类。分类是按照上述文本中定义的三个维度组织的。在《为什么使用ACL》中，我们仅报告每项工作的主要目标。当ACL控制环境选择时，我们具体说明它是从离散集合（DS）中选择还是通过参数化程序内容生成（PCG）进行选择。我们将对抗奖励最大化缩写为ARM，将学习进展缩写为LP。
- en: Objectives such as the average performance on a set of testing tasks after $N$
    training steps can be difficult to optimize directly. To alleviate this difficulty,
    ACL methods use a variety of surrogate objectives.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如在经过$N$次训练步骤后对一组测试任务的平均表现等目标，可能难以直接优化。为了解决这个难题，ACL方法使用了多种替代目标。
- en: Reward.
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 奖励。
- en: As DRL algorithms learn from reward signals, rewarded transitions are usually
    considered as more informative than others, especially in sparse reward problems.
    In such problems, ACL methods that act on transition selection may artificially
    increase the ratio of high versus low rewards in the batches of transitions used
    for policy updates Narasimhan et al. ([2015](#bib.bib42)); Jaderberg et al. ([2016](#bib.bib31));
    Colas et al. ([2020](#bib.bib12)). In multi-goal RL settings where some goals
    might be much harder than others, this strategy can be used to balance the proportion
    of positive rewards for each of the goals Colas et al. ([2019](#bib.bib11)); Lair
    et al. ([2019](#bib.bib33)). Transition modification methods favor rewards as
    well, substituting goals to increase the probability of observing rewarded transitions Andrychowicz
    et al. ([2017](#bib.bib1)); Cideron et al. ([2019](#bib.bib9)); Lair et al. ([2019](#bib.bib33));
    Colas et al. ([2020](#bib.bib12)). In data collection however, adapting training
    distributions towards more rewarded experience leads the agent to focus on tasks
    that are already solved. Because collecting data from already solved tasks hinders
    learning, data collection ACL methods rather focus on other surrogate objectives.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DRL算法从奖励信号中学习，奖励过渡通常被认为比其他过渡更具信息性，特别是在奖励稀疏的问题中。在这些问题中，作用于过渡选择的ACL方法可能会人为地增加用于策略更新的过渡批次中高奖励与低奖励的比例 Narasimhan等人（[2015](#bib.bib42)）；Jaderberg等人（[2016](#bib.bib31)）；Colas等人（[2020](#bib.bib12)）。在多目标RL环境中，其中一些目标可能比其他目标难得多，这种策略可以用来平衡每个目标的正奖励比例 Colas等人（[2019](#bib.bib11)）；Lair等人（[2019](#bib.bib33)）。过渡修改方法也偏向于奖励，替换目标以增加观察到奖励过渡的概率 Andrychowicz等人（[2017](#bib.bib1)）；Cideron等人（[2019](#bib.bib9)）；Lair等人（[2019](#bib.bib33)）；Colas等人（[2020](#bib.bib12)）。然而，在数据收集过程中，调整训练分布以获得更多奖励经验会导致代理集中于已解决的任务。由于从已解决的任务中收集数据会阻碍学习，因此数据收集ACL方法更侧重于其他替代目标。
- en: Intermediate difficulty.
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 中等难度。
- en: A more natural surrogate objective for data collection is intermediate difficulty.
    Intuitively, agents should target tasks that are neither too easy (already solved)
    nor too difficult (unsolvable) to maximize their learning progress. Intermediate
    difficulty has been used to adapt the distribution of initial states from which
    to perform a hard task Florensa et al. ([2017](#bib.bib20)); Salimans and Chen
    ([2018](#bib.bib52)); Ivanovic et al. ([2018](#bib.bib29)). This objective is
    also implemented in GoalGAN, where a curriculum generator based on a Generative
    Adversarial Network is trained to propose goals for which the agent reaches intermediate
    performance Florensa et al. ([2018](#bib.bib21)). [Racanière et al.](#bib.bib50) Racanière
    et al. ([2019](#bib.bib50)) further introduced a judge network trained to predict
    the feasibility of a given goal for the current learner. Instead of labelling
    tasks with an intermediate level of difficulty as in GoalGAN, this Setter-Solver
    model generates goals associated to a random feasibility uniformly sampled from
    $[0,1]$. The type of goals varies as the agent progresses, but the agent is always
    asked to perform goals sampled from a distribution balanced in terms of feasibility.
    In [Sukhbaatar et al.](#bib.bib59) Sukhbaatar et al. ([2017](#bib.bib59)), tasks
    are generated by an RL policy trained to propose either goals or initial states
    so that the resulting navigation task is of intermediate difficulty w.r.t. the
    current agent. Intermediate difficulty ACL has also been driving successes in
    Sim2Real applications, where it sequences domain randomizations to train policies
    that are robust enough to generalize from simulators to real-world robots Mehta
    et al. ([2019](#bib.bib39)); OpenAI et al. ([2019](#bib.bib43)). [OpenAI et al.](#bib.bib43) OpenAI
    et al. ([2019](#bib.bib43)) trains a robotic hand control policy to solve a Rubik’s
    cube by automatically adjusting the task distribution so that the agent achieves
    decent performance while still being challenged.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集的一个更自然的替代目标是中等难度。直观上，智能体应当针对那些既不太简单（已经解决）也不太困难（无法解决）的任务，以最大化其学习进展。中等难度已被用来调整从中执行困难任务的初始状态分布（Florensa
    et al. ([2017](#bib.bib20)); Salimans and Chen ([2018](#bib.bib52)); Ivanovic
    et al. ([2018](#bib.bib29))）。这一目标也在GoalGAN中实现，其中一个基于生成对抗网络的课程生成器被训练来提出目标，使得智能体能达到中等表现（Florensa
    et al. ([2018](#bib.bib21))）。[Racanière et al.](#bib.bib50) Racanière et al. ([2019](#bib.bib50))进一步引入了一个判别网络，训练其预测当前学习者对给定目标的可行性。与GoalGAN中将任务标记为中等难度不同，这一Setter-Solver模型生成与从$[0,1]$中均匀采样的随机可行性相关的目标。目标的类型会随着智能体的进展而变化，但智能体始终被要求执行从一个在可行性方面平衡的分布中采样的目标。在[Sukhbaatar
    et al.](#bib.bib59) Sukhbaatar et al. ([2017](#bib.bib59))中，任务由一个RL策略生成，该策略训练以提出目标或初始状态，使得生成的导航任务相对于当前智能体的难度为中等。中等难度ACL还在Sim2Real应用中推动了成功，在这些应用中，它通过对领域随机化的序列化来训练足够鲁棒的策略，以便从模拟器到现实世界机器人进行泛化（Mehta
    et al. ([2019](#bib.bib39)); OpenAI et al. ([2019](#bib.bib43))）。[OpenAI et al.](#bib.bib43)
    OpenAI et al. ([2019](#bib.bib43))训练一个机器人手控制策略，通过自动调整任务分布来解决魔方，使得智能体在仍然受到挑战的同时获得体面的表现。
- en: Learning progress.
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学习进展。
- en: 'The $Obj$ objective of ACL methods can be seen as the maximization of a global
    learning progress: the difference between the final score $\int_{T\sim\mathcal{T}}\!P_{T}^{N}\,\mathrm{d}T$
    and the initial score $\int_{T\sim\mathcal{T}}\!P_{T}^{0}\,\mathrm{d}T$. To approximate
    this complex objective, measures of competence learning progress (LP) localized
    in space and time were proposed in earlier developmental robotics works Baranes
    and Oudeyer ([2013](#bib.bib5)); Forestier et al. ([2017](#bib.bib22)). Like Intermediate
    difficulty, maximizing LP drives learners to practice tasks that are neither too
    easy nor too difficult, but LP does not require a threshold to define what is
    ”intermediate” and is robust to tasks with intermediate scores but where the agent
    cannot improve. LP maximization is usually framed as a multi-armed bandit (MAB)
    problem where tasks are arms and their LP measures are associated values. Maximizing
    LP values was shown optimal under the assumption of concave learning profiles Lopes
    and Oudeyer ([2012](#bib.bib37)). Both [Matiisen et al.](#bib.bib38) Matiisen
    et al. ([2017](#bib.bib38)) and [Mysore et al.](#bib.bib41) Mysore et al. ([2018](#bib.bib41))
    measure LP as the estimated derivative of the performance for each task in a discrete
    set (Minecraft mazes and Sonic the Hedgehog levels respectively) and apply a MAB
    algorithm to automatically build a curriculum for their learning agents. At a
    higher level, CURIOUS uses absolute LP to select goal spaces to sample from in
    a simulated robotic arm setup Colas et al. ([2019](#bib.bib11)) (absolute LP enables
    to redirect learning towards tasks that were forgotten or that changed). There,
    absolute LP is also used to bias the sampling of transition used for policy updates
    towards high-LP goals. ALP-GMM uses absolute LP to organize the presentation of
    procedurally-generated Bipedal-Walker environments sampled from a continuous task
    space through a stochastic parameterization Portelas et al. ([2019](#bib.bib49)).
    They leverage a Gaussian Mixture Model to recover a MAB setup over the continuous
    task space. LP can also be used to guide the choice of accuracy requirements in
    a reaching task Fournier et al. ([2018](#bib.bib23)), or to train a replay policy
    via RL to sample transitions for policy updates Zha et al. ([2019](#bib.bib65)).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ACL 方法的 $Obj$ 目标可以被视为全球学习进展的最大化：即最终得分 $\int_{T\sim\mathcal{T}}\!P_{T}^{N}\,\mathrm{d}T$
    和初始得分 $\int_{T\sim\mathcal{T}}\!P_{T}^{0}\,\mathrm{d}T$ 之间的差异。为了近似这个复杂的目标，早期的发展机器人学研究中提出了在空间和时间上局部的能力学习进展（LP）度量 Baranes
    和 Oudeyer ([2013](#bib.bib5))；Forestier 等 ([2017](#bib.bib22))。与中等难度类似，最大化 LP
    驱使学习者练习那些既不太容易也不太困难的任务，但 LP 不需要一个阈值来定义什么是“中等”的，并且对那些有中等得分但代理无法改进的任务是稳健的。LP 最大化通常被框定为一个多臂老虎机（MAB）问题，其中任务是臂，它们的
    LP 度量是关联值。在假设凹形学习曲线的情况下，最大化 LP 值被证明是最优的 Lopes 和 Oudeyer ([2012](#bib.bib37))。
    [Matiisen et al.](#bib.bib38) Matiisen 等 ([2017](#bib.bib38)) 和 [Mysore et al.](#bib.bib41)
    Mysore 等 ([2018](#bib.bib41)) 将 LP 测量为每个任务在离散集合中的性能估计导数（分别是 Minecraft 迷宫和 Sonic
    the Hedgehog 关卡），并应用 MAB 算法自动为他们的学习代理构建课程。在更高的层次上，CURIOUS 使用绝对 LP 从模拟机器人臂设置中选择目标空间进行采样 Colas
    等 ([2019](#bib.bib11))（绝对 LP 使得学习可以重新指向被遗忘或发生变化的任务）。在那里，绝对 LP 也用于将策略更新中的转移采样偏向于高
    LP 目标。ALP-GMM 使用绝对 LP 来组织通过随机参数化从连续任务空间中采样的程序生成的双足行走环境的呈现 Portelas 等 ([2019](#bib.bib49))。他们利用高斯混合模型在连续任务空间上恢复
    MAB 设置。LP 也可以用于指导在到达任务中的准确性要求选择 Fournier 等 ([2018](#bib.bib23))，或通过 RL 训练重放策略来采样策略更新的转移 Zha
    等 ([2019](#bib.bib65))。
- en: Diversity.
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多样性。
- en: Some ACL methods choose to maximize measures of diversity (also called novelty
    or low density). In multi-goal settings for example, ACL might favor goals from
    low-density areas either as targets Pong et al. ([2019](#bib.bib48)) or as substitute
    goals for data exploitation Fang et al. ([2019](#bib.bib18)). Similarly, [Zhao
    and Tresp](#bib.bib67) Zhao and Tresp ([2019](#bib.bib67)) biases the sampling
    of trajectories falling into low density areas of the trajectory space. In single-task
    RL, count-based approaches introduce internal reward functions as decreasing functions
    of the state visitation count, guiding agent towards rarely visited areas of the
    state space Bellemare et al. ([2016](#bib.bib6)). Through a variational expectation-maximization
    framework, [Jabri et al.](#bib.bib30) Jabri et al. ([2019](#bib.bib30)) propose
    to alternatively update a latent skill representation from experimental data (as
    in [Eysenbach et al.](#bib.bib17) Eysenbach et al. ([2018](#bib.bib17))) and to
    meta-learn a policy to adapt quickly to tasks constructed by deriving a reward
    function from sampled skills. Other algorithms do not optimize directly for diversity
    but use heuristics to maintain it. For instance, [Portelas et al.](#bib.bib49) Portelas
    et al. ([2019](#bib.bib49)) maintains exploration by using a residual uniform
    task sampling and [Bansal et al.](#bib.bib3) Bansal et al. ([2017](#bib.bib3))
    sample opponents from past versions of different policies to maintain diversity.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 ACL 方法选择最大化多样性度量（也称为新颖性或低密度）。例如，在多目标设置中，ACL 可能会倾向于选择来自低密度区域的目标，作为目标 Pong
    等人 ([2019](#bib.bib48)) 或作为数据利用的替代目标 Fang 等人 ([2019](#bib.bib18))。类似地，[Zhao 和
    Tresp](#bib.bib67) Zhao 和 Tresp ([2019](#bib.bib67)) 偏向于对落入轨迹空间低密度区域的轨迹进行采样。在单任务
    RL 中，基于计数的方法通过将内部奖励函数设为状态访问计数的递减函数，引导智能体朝向状态空间中较少访问的区域 Bellemare 等人 ([2016](#bib.bib6))。通过变分期望最大化框架，[Jabri
    等人](#bib.bib30) Jabri 等人 ([2019](#bib.bib30)) 提出从实验数据中交替更新潜在技能表示（如 [Eysenbach
    等人](#bib.bib17) Eysenbach 等人 ([2018](#bib.bib17)))，并元学习一个策略以快速适应通过从采样技能中推导的奖励函数构造的任务。其他算法不直接优化多样性，而是使用启发式方法来维持多样性。例如，[Portelas
    等人](#bib.bib49) Portelas 等人 ([2019](#bib.bib49)) 通过使用残差均匀任务采样来维持探索，[Bansal 等人](#bib.bib3) Bansal
    等人 ([2017](#bib.bib3)) 从过去不同政策的版本中采样对手以保持多样性。
- en: Surprise.
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 惊讶。
- en: Some ACL methods train transition models and compute intrinsic rewards based
    on their prediction errors Pathak et al. ([2017](#bib.bib45)); Burda et al. ([2019](#bib.bib8))
    or based on the disagreement (variance) between several models from an ensemble Shyam
    et al. ([2018](#bib.bib57)); Pathak et al. ([2019](#bib.bib46)). The general idea
    is that models tend to give bad prediction (or disagree) for states rarely visited,
    thus inducing a bias towards less visited states. However, a model might show
    high prediction errors on stochastic parts of the environment (TV problem Pathak
    et al. ([2017](#bib.bib45))), a phenomenon that does not appear with model disagreement,
    as all models of the ensemble eventually learn to predict the (same) mean prediction
    Pathak et al. ([2019](#bib.bib46)). Other works bias the sampling of transitions
    for policy update depending on their temporal-difference error (TD-error), i.e.
    the difference between the transition’s value and its next-step bootstrap estimation Schaul
    et al. ([2015b](#bib.bib54)); Horgan et al. ([2018](#bib.bib28)). Similarly, [Flet-Berliac
    and Preux](#bib.bib19) Flet-Berliac and Preux ([2019](#bib.bib19)) adapt transition
    selection based on the discrepancy between the observed return and the prediction
    of the value function of a PPO learner (V-error). Whether the error computation
    involves value models or transition models, ACL mechanisms favor states related
    to maximal surprise, i.e. a maximal difference between the expected (model prediction)
    and the truth.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 ACL 方法训练转移模型，并根据其预测误差计算内在奖励 Pathak et al. ([2017](#bib.bib45))；Burda et al.
    ([2019](#bib.bib8))，或基于来自集成模型的几个模型之间的分歧（方差） Shyam et al. ([2018](#bib.bib57))；Pathak
    et al. ([2019](#bib.bib46))。一般来说，模型往往对不常访问的状态给出糟糕的预测（或意见不一致），从而导致对少访问状态的偏倚。然而，模型可能在环境的随机部分显示出高预测误差（TV
    问题 Pathak et al. ([2017](#bib.bib45)))，这一现象在模型分歧中不会出现，因为集成的所有模型最终都学习到预测（相同的）平均预测
    Pathak et al. ([2019](#bib.bib46))。其他工作通过其时间差分误差（TD-error）来偏倚策略更新的转移采样，即转移值与其下一步自举估计之间的差异 Schaul
    et al. ([2015b](#bib.bib54))；Horgan et al. ([2018](#bib.bib28))。类似地，[Flet-Berliac
    和 Preux](#bib.bib19) Flet-Berliac 和 Preux ([2019](#bib.bib19)) 根据观察到的回报与 PPO 学习者的价值函数预测之间的差异（V-error）调整转移选择。无论误差计算涉及价值模型还是转移模型，ACL
    机制都倾向于与最大惊讶相关的状态，即预期（模型预测）与实际之间的最大差异。
- en: Energy.
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 能量。
- en: In the data exploitation phase of multi-goal settings, [Zhao and Tresp](#bib.bib66) Zhao
    and Tresp ([2018](#bib.bib66)) prioritize transitions from high-energy trajectories
    (e.g. kinetic energy) while [Colas et al.](#bib.bib11) Colas et al. ([2019](#bib.bib11))
    prioritize transitions where the object relevant to the goal moved (e.g. cube
    movement in a cube pushing task).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在多目标设置的数据利用阶段，[Zhao 和 Tresp](#bib.bib66) Zhao 和 Tresp ([2018](#bib.bib66)) 优先考虑来自高能量轨迹（例如动能）的转移，而
    [Colas et al.](#bib.bib11) Colas et al. ([2019](#bib.bib11)) 优先考虑目标相关的对象移动的转移（例如在立方体推动任务中的立方体移动）。
- en: Adversarial reward maximization (ARM).
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对抗奖励最大化（ARM）。
- en: Self-Play is a form of ACL which optimizes agents’ performance when opposed
    to current or past versions of themselves, an objective that we call adversarial
    reward maximization (ARM) Hernandez et al. ([2019](#bib.bib27)). While agents
    from [Silver et al.](#bib.bib58) Silver et al. ([2017](#bib.bib58)) and [Baker
    et al.](#bib.bib2) Baker et al. ([2019](#bib.bib2)) always oppose copies of themselves,
    [Bansal et al.](#bib.bib3) Bansal et al. ([2017](#bib.bib3)) train several policies
    in parallel and fill a pool of opponents made of current and past versions of
    all policies. This maintains a diversity of opponents, which helps to fight catastrophic
    forgetting and to improve robustness. In the multi-agent game Starcraft II, [Vinyals
    et al.](#bib.bib63) Vinyals et al. ([2019](#bib.bib63)) train three main policies
    in parallel (one for each of the available player types). They maintain a league
    of opponents composed of current and past versions of both the three main policies
    and additional adversary policies. Opponents are not selected at random but to
    be challenging (as measured by winning rates).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 自我对抗是一种 ACL 形式，它在对抗当前或过去版本的自身时优化代理的表现，这一目标我们称之为对抗奖励最大化（ARM） Hernandez 等人 ([2019](#bib.bib27))。虽然
    [Silver et al.](#bib.bib58) Silver et al. ([2017](#bib.bib58)) 和 [Baker et al.](#bib.bib2)
    Baker et al. ([2019](#bib.bib2)) 的代理始终与其自身的副本对抗，[Bansal et al.](#bib.bib3) Bansal
    et al. ([2017](#bib.bib3)) 同时训练多个策略，并填充一个由当前和过去版本的所有策略组成的对手池。这保持了对手的多样性，有助于应对灾难性遗忘并提高鲁棒性。在多人游戏
    Starcraft II 中，[Vinyals et al.](#bib.bib63) Vinyals et al. ([2019](#bib.bib63))
    并行训练三种主要策略（每种可用的玩家类型一种）。他们维持一个由当前和过去版本的三种主要策略及额外对手策略组成的对手联盟。对手不是随机选择的，而是为了具挑战性（通过胜率来衡量）。
- en: 6 Discussion
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: The bigger picture.
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更大的图景。
- en: 'In this survey, we unify the wide range of ACL mechanisms used in symbiosis
    with DRL under a common framework. ACL mechanisms are used with a particular goal
    in mind (e.g. organizing exploration, solving hard tasks, etc. § [3](#S3 "3 Why
    use ACL? ‣ Automatic Curriculum Learning For Deep RL: A Short Survey")). It controls
    a particular element of task MDPs (e.g. $\mathcal{S},\mathcal{R},\rho_{0}$, § [4](#S4
    "4 What does ACL control? ‣ Automatic Curriculum Learning For Deep RL: A Short
    Survey")) and maximizes a surrogate objective to achieve its goal (e.g. diversity,
    learning progress, § [5](#S5 "5 What Does ACL Optimize? ‣ Automatic Curriculum
    Learning For Deep RL: A Short Survey")). Table [1](#S5.T1 "Table 1 ‣ 5 What Does
    ACL Optimize? ‣ Automatic Curriculum Learning For Deep RL: A Short Survey") organizes
    the main works surveyed here along these three dimensions. Both previous sections
    and Table [1](#S5.T1 "Table 1 ‣ 5 What Does ACL Optimize? ‣ Automatic Curriculum
    Learning For Deep RL: A Short Survey") present what has been implemented in the
    past, and thus, by contrast, highlight potential new avenues for ACL.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们将广泛使用的 ACL 机制与 DRL 结合的内容统一到一个共同的框架下。ACL 机制是为了实现特定的目标（例如组织探索、解决困难任务等，§ [3](#S3
    "3 为什么使用 ACL？ ‣ 自动化课程学习在深度强化学习中的应用：简短调查")）。它控制任务 MDPs 的特定元素（例如 $\mathcal{S},\mathcal{R},\rho_{0}$，§ [4](#S4
    "4 ACL 控制什么？ ‣ 自动化课程学习在深度强化学习中的应用：简短调查")），并通过最大化替代目标来实现其目标（例如多样性、学习进展，§ [5](#S5
    "5 ACL 优化什么？ ‣ 自动化课程学习在深度强化学习中的应用：简短调查")）。表 [1](#S5.T1 "表 1 ‣ 5 ACL 优化什么？ ‣ 自动化课程学习在深度强化学习中的应用：简短调查")
    将这里调查的主要工作按这三个维度进行了组织。前面部分和表 [1](#S5.T1 "表 1 ‣ 5 ACL 优化什么？ ‣ 自动化课程学习在深度强化学习中的应用：简短调查")
    展示了过去的实施情况，因此对比之下，突出了 ACL 可能的新途径。
- en: Expanding the set of ACL targets. Inspired by the maturational mechanisms at
    play in human infants, [Elman](#bib.bib15) Elman ([1993](#bib.bib15)) proposed
    to gradually expand the working memory of a recurrent model in a word-to-word
    natural language processing task. The idea of changing the properties of the agent
    (here its memory) was also studied in developmental robotics Baranes and Oudeyer
    ([2011](#bib.bib4)), policy distillation methods Czarnecki et al. ([2018](#bib.bib13),
    [2019](#bib.bib14)) and evolutionary approaches Ha ([2019](#bib.bib25)) but is
    absent from the ACL-DRL literature. ACL mechanisms could indeed be used to control
    the agent’s body ($\mathcal{S},\mathcal{P}$), its action space (how it acts in
    the world, $\mathcal{A}$), its observation space (how it perceives the world,
    $\mathcal{S}$), its learning capacities (e.g. capacities of the memory, or the
    controller) or the way it perceives time (controlling discount factors François-Lavet
    et al. ([2015](#bib.bib24))).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展ACL目标的范围。受到人类婴儿发育机制的启发，[Elman](#bib.bib15) Elman ([1993](#bib.bib15)) 提出了在词到词的自然语言处理任务中逐步扩展递归模型的工作记忆。改变代理（这里是其记忆）属性的想法也在发展机器人学
    Baranes 和 Oudeyer ([2011](#bib.bib4))，策略蒸馏方法 Czarnecki et al. ([2018](#bib.bib13),
    [2019](#bib.bib14)) 和进化方法 Ha ([2019](#bib.bib25)) 中进行了研究，但在ACL-DRL文献中却没有出现。ACL机制确实可以用来控制代理的身体（$\mathcal{S},\mathcal{P}$），其动作空间（它在世界中的行为，$\mathcal{A}$），其观察空间（它如何感知世界，$\mathcal{S}$），其学习能力（例如，记忆的能力或控制器）或它感知时间的方式（控制折扣因子
    François-Lavet et al. ([2015](#bib.bib24))）。
- en: Combining approaches. Many combinations of previously defined ACL mechanisms
    remain to be investigated. Could we use LP to optimize the selection of opponents
    in self-play approaches? To drive goal selection in learned goal spaces (e.g.
    [Laversanne-Finot et al.](#bib.bib34) Laversanne-Finot et al. ([2018](#bib.bib34)),
    population-based)? Could we train an adversarial domain generator to robustify
    policies trained for Sim2Real applications?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 结合方法。许多之前定义的ACL机制组合仍待调查。我们能否使用LP来优化自我对战方法中对手的选择？在学习的目标空间中（例如，[Laversanne-Finot
    et al.](#bib.bib34) Laversanne-Finot et al. ([2018](#bib.bib34)），基于人群的方法），我们能否训练一个对抗域生成器来增强为Sim2Real应用训练的策略？
- en: On the need of systematic ACL studies.
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于系统性ACL研究的必要性。
- en: Given the positive impact that ACL mechanisms can have in complex learning scenarios,
    one can only deplore the lack of comparative studies and standard benchmark environments.
    Besides, although empirical results advocate for their use, a theoretical understanding
    of ACL mechanisms is still missing. Although there have been attempts to frame
    CL in supervised settings Bengio et al. ([2009](#bib.bib7)); Hacohen and Weinshall
    ([2019](#bib.bib26)), more work is needed to see whether such considerations hold
    in DRL scenarios.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于ACL机制在复杂学习场景中的积极影响，人们只能遗憾地看到缺乏比较研究和标准基准环境。此外，尽管实证结果支持其使用，但对ACL机制的理论理解仍然缺失。尽管已有尝试将CL框架应用于监督设置
    Bengio et al. ([2009](#bib.bib7)); Hacohen 和 Weinshall ([2019](#bib.bib26))，但仍需更多工作以确认这些考虑在DRL场景中的适用性。
- en: ACL as a step towards open-ended learning agents.
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ACL作为迈向开放式学习代理的一步。
- en: Alan Turing famously wrote “Instead of trying to produce a programme to simulate
    the adult mind, why not rather try to produce one which simulates the child’s?”
    Turing ([1950](#bib.bib62)). The idea of starting with a simple machine and to
    enable it to learn autonomously is the cornerstone of developmental robotics but
    is rarely considered in DRL Colas et al. ([2020](#bib.bib12)); Eysenbach et al.
    ([2018](#bib.bib17)); Jabri et al. ([2019](#bib.bib30)). Because they actively
    organize learning trajectories as a function of the agent’s properties, ACL mechanisms
    could prove extremely useful in this quest. We could imagine a learning architecture
    leveraging ACL mechanisms to control many aspects of the learning odyssey, guiding
    agents from their simple original state towards fully capable agents able to reach
    a multiplicity of goals. As we saw in this survey, these ACL mechanisms could
    control the development of the agent’s body and capabilities (motor actions, sensory
    apparatus), organize the exploratory behavior towards tasks where agents learn
    the most (maximization of information gain, competence progress) or guide acquisitions
    of behavioral repertoires.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 艾伦·图灵著名地写道：“与其试图编写一个模拟成人思维的程序，不如尝试编写一个模拟儿童思维的程序。” 图灵 ([1950](#bib.bib62))。从简单的机器开始，并使其能够自主学习是发展机器人学的基石，但在
    DRL 中却很少被考虑。Colas 等人 ([2020](#bib.bib12)); Eysenbach 等人 ([2018](#bib.bib17));
    Jabri 等人 ([2019](#bib.bib30))。由于它们主动组织学习轨迹以适应智能体的属性，ACL 机制在这一探索中可能会非常有用。我们可以设想一种学习架构，利用
    ACL 机制来控制学习历程的许多方面，引导智能体从简单的原始状态发展到能够实现多种目标的完全能干的智能体。正如我们在本调查中所见，这些 ACL 机制可以控制智能体身体和能力的发展（运动动作、感官设备），组织探索行为以应对智能体学习最多的任务（信息增益最大化、能力进步）或指导行为谱的获得。
- en: References
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: Andrychowicz et al. [2017] Marcin Andrychowicz et al. Hindsight experience replay.
    In NeurIPS.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrychowicz et al. [2017] Marcin Andrychowicz 等. 后见之明经验回放. 发表在 NeurIPS。
- en: Baker et al. [2019] Bowen Baker et al. Emergent tool use from multi-agent autocurricula.
    arXiv.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baker et al. [2019] Bowen Baker 等. 从多智能体自适应课程中涌现的工具使用. arXiv。
- en: Bansal et al. [2017] Trapit Bansal et al. Emergent complexity via multi-agent
    competition. arXiv.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bansal et al. [2017] Trapit Bansal 等. 通过多智能体竞争产生的复杂性. arXiv。
- en: Baranes and Oudeyer [2011] A. Baranes and P. Oudeyer. The interaction of maturational
    constraints and intrinsic motivations in active motor development. In ICDL.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baranes and Oudeyer [2011] A. Baranes 和 P. Oudeyer. 发育约束与内在动机在主动运动发展中的相互作用.
    发表在 ICDL。
- en: Baranes and Oudeyer [2013] Adrien Baranes and Pierre-Yves Oudeyer. Active learning
    of inverse models with intrinsically motivated goal exploration in robots. Robot.
    Auton. Syst.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baranes and Oudeyer [2013] Adrien Baranes 和 Pierre-Yves Oudeyer. 机器人中内在动机驱动的目标探索的逆向模型的主动学习.
    机器人学与自动化系统。
- en: Bellemare et al. [2016] Marc Bellemare et al. Unifying count-based exploration
    and intrinsic motivation. In NeurIPS.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare et al. [2016] Marc Bellemare 等. 统一基于计数的探索与内在动机. 发表在 NeurIPS。
- en: Bengio et al. [2009] Yoshua Bengio et al. Curriculum learning. In ICML.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio et al. [2009] Yoshua Bengio 等. 课程学习. 发表在 ICML。
- en: Burda et al. [2019] Yuri Burda et al. Exploration by random network distillation.
    ICLR.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burda et al. [2019] Yuri Burda 等. 通过随机网络蒸馏进行探索. ICLR。
- en: Cideron et al. [2019] Geoffrey Cideron et al. Self-educated language agent with
    hindsight experience replay for instruction following. arXiv.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cideron et al. [2019] Geoffrey Cideron 等. 自我教育的语言智能体，通过后见之明经验回放进行指令跟随. arXiv。
- en: Cobbe et al. [2018] Karl Cobbe et al. Quantifying generalization in reinforcement
    learning. arXiv.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. [2018] Karl Cobbe 等. 量化强化学习中的泛化能力. arXiv。
- en: 'Colas et al. [2019] Cédric Colas et al. CURIOUS: Intrinsically motivated modular
    multi-goal reinforcement learning. In ICML.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Colas et al. [2019] Cédric Colas 等. CURIOUS: 内在动机驱动的模块化多目标强化学习. 发表在 ICML。'
- en: Colas et al. [2020] Cédric Colas et al. Language as a cognitive tool to imagine
    goals in curiosity-driven exploration. arXiv.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colas et al. [2020] Cédric Colas 等. 语言作为认知工具来想象好奇驱动探索中的目标. arXiv。
- en: Czarnecki et al. [2018] Wojciech Czarnecki et al. Mix & match agent curricula
    for reinforcement learning. In ICML.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Czarnecki et al. [2018] Wojciech Czarnecki 等. 组合与匹配强化学习的智能体课程. 发表在 ICML。
- en: Czarnecki et al. [2019] Wojciech Marian Czarnecki et al. Distilling policy distillation.
    arXiv.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Czarnecki et al. [2019] Wojciech Marian Czarnecki 等. 政策蒸馏. arXiv。
- en: 'Elman [1993] Jeffrey L. Elman. Learning and development in neural networks:
    the importance of starting small. Cognition.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Elman [1993] Jeffrey L. Elman. 神经网络中的学习与发展: 从小开始的重要性. 认知。'
- en: Eppe et al. [2018] Manfred Eppe et al. Curriculum goal masking for continuous
    deep reinforcement learning. CoRR, abs/1809.06146.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eppe 等 [2018] Manfred Eppe 等. 连续深度强化学习的课程目标掩蔽。CoRR, abs/1809.06146。
- en: 'Eysenbach et al. [2018] Benjamin Eysenbach et al. Diversity is all you need:
    Learning skills without a reward function. arXiv.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eysenbach 等 [2018] Benjamin Eysenbach 等. 多样性就是一切：无需奖励函数的技能学习。arXiv。
- en: Fang et al. [2019] Meng Fang et al. Curriculum-guided hindsight experience replay.
    In NeurIPS.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等 [2019] Meng Fang 等. 课程指导的事后经验回放。见 NeurIPS。
- en: 'Flet-Berliac and Preux [2019] Yannis Flet-Berliac and Philippe Preux. Samples
    are not all useful: Denoising policy gradient updates using variance. CoRR, abs/1904.04025.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flet-Berliac 和 Preux [2019] Yannis Flet-Berliac 和 Philippe Preux. 样本并非都有效：利用方差进行去噪策略梯度更新。CoRR,
    abs/1904.04025。
- en: Florensa et al. [2017] Carlos Florensa et al. Reverse curriculum generation
    for reinforcement learning. CoRL.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florensa 等 [2017] Carlos Florensa 等. 强化学习的逆向课程生成。CoRL。
- en: Florensa et al. [2018] Carlos Florensa et al. Automatic goal generation for
    reinforcement learning agents. In ICML.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florensa 等 [2018] Carlos Florensa 等. 自动目标生成用于强化学习智能体。见 ICML。
- en: Forestier et al. [2017] Sébastien Forestier et al. Intrinsically motivated goal
    exploration processes with automatic curriculum learning. arXiv.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Forestier 等 [2017] Sébastien Forestier 等. 内在动机驱动的目标探索过程与自动课程学习。arXiv。
- en: Fournier et al. [2018] Pierre Fournier et al. Accuracy-based curriculum learning
    in deep reinforcement learning. arXiv.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fournier 等 [2018] Pierre Fournier 等. 基于准确性的课程学习在深度强化学习中的应用。arXiv。
- en: 'François-Lavet et al. [2015] Vincent François-Lavet et al. How to discount
    deep reinforcement learning: Towards new dynamic strategies. arXiv.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: François-Lavet 等 [2015] Vincent François-Lavet 等. 如何折扣深度强化学习：迈向新的动态策略。arXiv。
- en: Ha [2019] David Ha. Reinforcement learning for improving agent design. Arti.
    Life.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ha [2019] David Ha. 用于改善智能体设计的强化学习。Arti. Life。
- en: Hacohen and Weinshall [2019] Guy Hacohen and Daphna Weinshall. On the power
    of curriculum learning in training deep networks. In Kamalika Chaudhuri and Ruslan
    Salakhutdinov, editors, ICML.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hacohen 和 Weinshall [2019] Guy Hacohen 和 Daphna Weinshall. 关于课程学习在训练深度网络中的作用。在
    Kamalika Chaudhuri 和 Ruslan Salakhutdinov 编辑的 ICML 中。
- en: Hernandez et al. [2019] D. Hernandez et al. A generalized framework for self-play
    training. In IEEE CoG.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hernandez 等 [2019] D. Hernandez 等. 自我博弈训练的通用框架。见 IEEE CoG。
- en: Horgan et al. [2018] Dan Horgan et al. Distributed prioritized experience replay.
    arXiv.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horgan 等 [2018] Dan Horgan 等. 分布式优先经验回放。arXiv。
- en: 'Ivanovic et al. [2018] Boris Ivanovic et al. Barc: Backward reachability curriculum
    for robotic reinforcement learning. ICRA.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivanovic 等 [2018] Boris Ivanovic 等. Barc：用于机器人强化学习的逆向可达课程。ICRA。
- en: Jabri et al. [2019] Allan Jabri et al. Unsupervised curricula for visual meta-reinforcement
    learning. In NeurIPS.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jabri 等 [2019] Allan Jabri 等. 用于视觉元强化学习的无监督课程。见 NeurIPS。
- en: Jaderberg et al. [2016] Max Jaderberg et al. Reinforcement learning with unsupervised
    auxiliary tasks. arXiv.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等 [2016] Max Jaderberg 等. 带有无监督辅助任务的强化学习。arXiv。
- en: 'Krueger and Dayan [2009] Kai A. Krueger and Peter Dayan. Flexible shaping:
    How learning in small steps helps. Cognition.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krueger 和 Dayan [2009] Kai A. Krueger 和 Peter Dayan. 灵活的塑形：小步学习如何有助于学习。Cognition。
- en: Lair et al. [2019] Nicolas Lair et al. Language grounding through social interactions
    and curiosity-driven multi-goal learning. arXiv.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lair 等 [2019] Nicolas Lair 等. 通过社会互动和好奇心驱动的多目标学习进行语言基础建设。arXiv。
- en: Laversanne-Finot et al. [2018] Adrien Laversanne-Finot et al. Curiosity driven
    exploration of learned disentangled goal spaces.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laversanne-Finot 等 [2018] Adrien Laversanne-Finot 等. 驱动探索学习的好奇心驱动的目标空间分离。
- en: Lesort et al. [2019] Timothée Lesort et al. Continual learning for robotics.
    arXiv.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lesort 等 [2019] Timothée Lesort 等. 机器人领域的持续学习。arXiv。
- en: Lin [1992] Long-Ji Lin. Self-improving reactive agents based on reinforcement
    learning, planning and teaching. Mach. lear.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin [1992] Long-Ji Lin. 基于强化学习、规划和教学的自我改进反应性智能体。Mach. lear.
- en: Lopes and Oudeyer [2012] Manuel Lopes and Pierre-Yves Oudeyer. The strategic
    student approach for life-long exploration and learning. In ICDL.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lopes 和 Oudeyer [2012] Manuel Lopes 和 Pierre-Yves Oudeyer. 终身探索和学习的战略性学生方法。见
    ICDL。
- en: Matiisen et al. [2017] Tambet Matiisen et al. Teacher-student curriculum learning.
    IEEE TNNLS.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matiisen 等 [2017] Tambet Matiisen 等. 教师-学生课程学习。IEEE TNNLS。
- en: Mehta et al. [2019] Bhairav Mehta et al. Active domain randomization. CoRL.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehta 等 [2019] Bhairav Mehta 等. 主动领域随机化。CoRL。
- en: 'Moore and Atkeson [1993] Andrew W Moore and Christopher G Atkeson. Prioritized
    sweeping: Reinforcement learning with less data and less time. Mach. learn.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moore and Atkeson [1993] Andrew W Moore 和 Christopher G Atkeson. 优先扫描：用更少的数据和时间进行强化学习。Mach.
    learn.
- en: Mysore et al. [2018] S. Mysore et al. Reward-guided curriculum for robust reinforcement
    learning. preprint.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mysore et al. [2018] S. Mysore 等人. 通过奖励引导的课程实现稳健的强化学习。预印本。
- en: Narasimhan et al. [2015] Karthik Narasimhan et al. Language understanding for
    text-based games using deep reinforcement learning. arXiv.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narasimhan et al. [2015] Karthik Narasimhan 等人. 使用深度强化学习的文本游戏语言理解。arXiv。
- en: OpenAI et al. [2019] OpenAI et al. Solving rubik’s cube with a robot hand. arXiv.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI et al. [2019] OpenAI 等人. 用机器人手解决魔方。arXiv。
- en: Oudeyer et al. [2007] Pierre-Yves Oudeyer et al. Intrinsic motivation systems
    for autonomous mental development. IEEE trans. on evolutionary comp.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oudeyer et al. [2007] Pierre-Yves Oudeyer 等人. 自主心理发展的内在动机系统。IEEE 转型于进化计算。
- en: Pathak et al. [2017] Deepak Pathak et al. Curiosity-driven exploration by self-supervised
    prediction. In CVPR.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pathak et al. [2017] Deepak Pathak 等人. 基于自我监督预测的好奇驱动探索。发表于 CVPR。
- en: Pathak et al. [2019] Deepak Pathak et al. Self-supervised exploration via disagreement.
    arXiv.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pathak et al. [2019] Deepak Pathak 等人. 通过不一致性进行自我监督探索。arXiv。
- en: Pinto et al. [2017] Lerrel Pinto et al. Robust adversarial reinforcement learning.
    arXiv.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinto et al. [2017] Lerrel Pinto 等人. 强健的对抗性强化学习。arXiv。
- en: 'Pong et al. [2019] Vitchyr H. Pong et al. Skew-fit: State-covering self-supervised
    reinforcement learning. arXiv.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pong et al. [2019] Vitchyr H. Pong 等人. Skew-fit：状态覆盖的自我监督强化学习。arXiv。
- en: Portelas et al. [2019] Rémy Portelas et al. Teacher algorithms for curriculum
    learning of deep rl in continuously parameterized environments. CoRL.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Portelas et al. [2019] Rémy Portelas 等人. 深度强化学习在连续参数化环境中的课程学习教师算法。CoRL。
- en: Racanière et al. [2019] Sébastien Racanière et al. Automated curricula through
    setter-solver interactions. arXiv.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Racanière et al. [2019] Sébastien Racanière 等人. 通过设置者-解决者互动自动生成课程。arXiv。
- en: 'Risi and Togelius [2019] Sebastian Risi and Julian Togelius. Procedural content
    generation: From automatically generating game levels to increasing generality
    in machine learning. arXiv.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Risi and Togelius [2019] Sebastian Risi 和 Julian Togelius. 程序化内容生成：从自动生成游戏关卡到提高机器学习的一般性。arXiv。
- en: Salimans and Chen [2018] Tim Salimans and Richard Chen. Learning montezuma’s
    revenge from a single demonstration. NeurIPS.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salimans and Chen [2018] Tim Salimans 和 Richard Chen. 从单个示例学习《蒙特祖玛的复仇》。NeurIPS。
- en: Schaul et al. [2015a] Tom Schaul et al. Universal value function approximators.
    In ICML.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaul et al. [2015a] Tom Schaul 等人. 通用价值函数近似器。发表于 ICML。
- en: Schaul et al. [2015b] Tom Schaul et al. Prioritized experience replay. arXiv.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaul et al. [2015b] Tom Schaul 等人. 优先经验重放。arXiv。
- en: Schmidhuber [1991] Jürgen Schmidhuber. Curious model-building control systems.
    In IJCNN. IEEE.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmidhuber [1991] Jürgen Schmidhuber. 好奇的模型构建控制系统。发表于 IJCNN。IEEE。
- en: Selfridge et al. [1985] Oliver G Selfridge et al. Training and tracking in robotics.
    In IJCAI.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Selfridge et al. [1985] Oliver G Selfridge 等人. 机器人中的训练与跟踪。发表于 IJCAI。
- en: Shyam et al. [2018] Pranav Shyam et al. Model-based active exploration. arXiv.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shyam et al. [2018] Pranav Shyam 等人. 基于模型的主动探索。arXiv。
- en: Silver et al. [2017] David Silver et al. Mastering the game of go without human
    knowledge. Nature.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver et al. [2017] David Silver 等人. 在没有人类知识的情况下掌握围棋。Nature。
- en: Sukhbaatar et al. [2017] Sainbayar Sukhbaatar et al. Intrinsic motivation and
    automatic curricula via asymmetric self-play. arXiv.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sukhbaatar et al. [2017] Sainbayar Sukhbaatar 等人. 通过不对称自我对弈实现内在动机和自动课程。arXiv。
- en: 'Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. Reinforcement
    learning: An introduction. MIT press.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton and Barto [2018] Richard S Sutton 和 Andrew G Barto. 强化学习：导论。MIT press。
- en: 'Taylor and Stone [2009] Matthew E. Taylor and Peter Stone. Transfer learning
    for reinforcement learning domains: A survey. JMLR.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taylor and Stone [2009] Matthew E. Taylor 和 Peter Stone. 强化学习领域的迁移学习：综述。JMLR。
- en: Turing [1950] Alan M Turing. Computing machinery and intelligence.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turing [1950] Alan M Turing. 计算机器与智能。
- en: Vinyals et al. [2019] Oriol Vinyals et al. Grandmaster level in StarCraft II
    using multi-agent reinforcement learning. Nature.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals et al. [2019] Oriol Vinyals 等人. 使用多智能体强化学习在《StarCraft II》中达到大宗师级别。Nature。
- en: 'Wang et al. [2019] Rui Wang et al. Paired open-ended trailblazer (POET): endlessly
    generating increasingly complex and diverse learning environments and their solutions.
    arXiv.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2019] Rui Wang 等人. 配对开放式开拓者（POET）：无尽生成日益复杂和多样的学习环境及其解决方案。arXiv。
- en: Zha et al. [2019] Daochen Zha et al. Experience replay optimization. arXiv.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zha et al. [2019] Daochen Zha 等人. 经验重放优化。arXiv。
- en: Zhao and Tresp [2018] Rui Zhao and Volker Tresp. Energy-based hindsight experience
    prioritization. arXiv.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 和 Tresp [2018] Rui Zhao 和 Volker Tresp. 基于能量的事后经验优先排序。arXiv。
- en: Zhao and Tresp [2019] Rui Zhao and Volker Tresp. Curiosity-driven experience
    prioritization via density estimation. arXiv.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 和 Tresp [2019] Rui Zhao 和 Volker Tresp. 通过密度估计的好奇心驱动经验优先排序。arXiv。
