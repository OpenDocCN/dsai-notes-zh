- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:32:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:32:53'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2405.06574] Deep Video Representation Learning: a Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2405.06574] 深度视频表示学习：一项综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06574](https://ar5iv.labs.arxiv.org/html/2405.06574)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.06574](https://ar5iv.labs.arxiv.org/html/2405.06574)
- en: '[3,2]\fnmXin \surLi'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[3,2]\fnmXin \surLi'
- en: 1]\orgdivDivision of Electrical $\&amp;$ Computer Engineering and Center for
    Computation $\&amp;$ Technology, \orgnameLouisiana State University, \orgaddress\cityBaton
    Rouge, \postcode70803, \stateLA, \countryUSA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 1]\orgdiv电气与计算机工程系及计算与技术中心，\orgname路易斯安那州立大学，\orgaddress\city巴吞鲁日，\postcode70803，\stateLA，\country美国
- en: 2]\orgdivDepartment of Computer Science and Engineering, \orgnameTexas A$\&amp;$M
    University, \orgaddress\cityCollege Station, \postcode77843, \stateTX, \countryUSA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2]\orgdiv计算机科学与工程系，\orgname德克萨斯农工大学，\orgaddress\city大学站，\postcode77843，\stateTX，\country美国
- en: 3]\orgdivSection of Visual Computing and Interactive Media, \orgnameTexas A$\&amp;$M
    University, \orgaddress\cityCollege Station, \postcode77843, \stateTX, \countryUSA
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 3]\orgdiv视觉计算与互动媒体系，\orgname德克萨斯农工大学，\orgaddress\city大学站，\postcode77843，\stateTX，\country美国
- en: 'Deep Video Representation Learning: a Survey'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度视频表示学习：一项综述
- en: \fnmElham \surRavanbakhsh [eravan1@lsu.edu](mailto:eravan1@lsu.edu)    \fnmYongqing
    \surLiang [lyq@tamu.edu](mailto:lyq@tamu.edu)    \fnmJ. \surRamanujam [jxr@cct.lsu.edu](mailto:jxr@cct.lsu.edu)
       [xinli@tamu.edu](mailto:xinli@tamu.edu) [ [ [
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \fnmElham \surRavanbakhsh [eravan1@lsu.edu](mailto:eravan1@lsu.edu)    \fnmYongqing
    \surLiang [lyq@tamu.edu](mailto:lyq@tamu.edu)    \fnmJ. \surRamanujam [jxr@cct.lsu.edu](mailto:jxr@cct.lsu.edu)
       [xinli@tamu.edu](mailto:xinli@tamu.edu) [ [ [
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper provides a review on *representation learning for videos*. We classify
    recent spatio-temporal feature learning methods for sequential visual data and
    compare their pros and cons for general video analysis. Building effective features
    for videos is a fundamental problem in computer vision tasks involving video analysis
    and understanding. Existing features can be generally categorized into spatial
    and temporal features. Their effectiveness under variations of illumination, occlusion,
    view and background are discussed. Finally, we discuss the remaining challenges
    in existing deep video representation learning studies.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了*视频表示学习*的综述。我们分类了最近的时空特征学习方法，并比较它们在一般视频分析中的优缺点。构建有效的视频特征是涉及视频分析和理解的计算机视觉任务中的一个基本问题。现有特征通常可以分为空间特征和时间特征。讨论了它们在光照、遮挡、视角和背景变化下的有效性。最后，我们讨论了现有深度视频表示学习研究中的剩余挑战。
- en: 'keywords:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 'keywords:'
- en: Video Representation Learning, Feature Modeling, Video Feature Extraction, Feature
    Learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 视频表示学习，特征建模，视频特征提取，特征学习。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The enormous influence of media and social networking has led to an avalanche
    of videos uploaded on the internet every day. To effectively analyze and use the
    uploaded video data, it is important to construct feature representations for
    videos. Unlike the analysis and understanding of images, the manual modeling of
    video features is often a laborious task. Therefore, there is a need for techniques
    that can automatically extract compact yet descriptive features.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 媒体和社交网络的巨大影响导致了每天互联网上上传大量视频。为了有效分析和利用上传的视频数据，构建视频特征表示非常重要。与图像的分析和理解不同，视频特征的手动建模通常是一项繁重的任务。因此，需要能够自动提取紧凑而描述性特征的技术。
- en: With the recent advances in artificial intelligence and computer vision, deep
    neural networks have achieved significant success in feature modeling. These techniques
    have led to a great breakthrough in practical video analysis applications such
    as tracking [[91](#bib.bib91), [177](#bib.bib177)], action recognition [[187](#bib.bib187)],
    action prediction [[90](#bib.bib90)], and person re-identification [[51](#bib.bib51)].
    To design a deep learning pipeline for these applications, extracting video features
    is often the first step and it plays a critical role in subsequent video processing
    or analysis. Developing deep learning pipelines to extract effective features
    for a given video is referred to as *deep video representation learning*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能和计算机视觉的最新进展，深度神经网络在特征建模方面取得了显著的成功。这些技术在实际的视频分析应用中，如跟踪 [[91](#bib.bib91),
    [177](#bib.bib177)]、动作识别 [[187](#bib.bib187)]、动作预测 [[90](#bib.bib90)] 和人员重新识别
    [[51](#bib.bib51)]，取得了巨大的突破。为了为这些应用设计深度学习管道，提取视频特征通常是第一步，并且在后续的视频处理或分析中起着关键作用。开发深度学习管道以提取给定视频的有效特征被称为*深度视频表示学习*。
- en: The characteristics of videos are often encoded by *spatial features* and *temporal
    features*. Spatial features encode geometric structures, spatial contents, or
    positional information in image frames; whereas, temporal features capture the
    movements, deformation, and various relations between frames in the time domain.
    Depending on the target applications, an algorithm should be able to extract either
    spatial or temporal features, preferably, both. Some applications also require
    the decoupling of spatial and temporal information from the extracted features
    so that some specific characteristics can be more effectively modeled.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 视频的特征通常通过*空间特征*和*时间特征*来编码。空间特征编码图像帧中的几何结构、空间内容或位置信息；而时间特征捕捉时间域中的运动、变形和帧之间的各种关系。根据目标应用程序，算法应能够提取空间特征或时间特征，最好是两者都提取。一些应用还要求从提取的特征中解耦空间和时间信息，以便更有效地建模某些特定特征。
- en: Learning robust representation for videos faces several *major challenges* such
    as
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为视频学习鲁棒表示面临几个*主要挑战*，如
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'occlusion: objects of interests might be partly occluded;'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 遮挡：感兴趣的对象可能会部分被遮挡；
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'illumination: videos might be taken under various lighting conditions or/and
    from changing view angles;'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 照明：视频可能在不同的光照条件下或/和从变化的视角拍摄；
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'view and background variations: foreground objects and background scenes can
    be moving.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视角和背景变化：前景对象和背景场景可能在移动。
- en: 'Therefore, we evaluate the performance of representation learning algorithms
    using *robustness* and *accuracy* under these scenarios. Robustness of different
    algorithms is evaluated under these four challenges: occlusion, view, illumination,
    and background change. As for their accuracy, since different applications use
    different metrics, we adopt accuracy metrics from representative tasks of action
    recognition and video segmentation, in which more expressive features generally
    lead to better accuracy.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用*鲁棒性*和*准确性*来评估表示学习算法在这些场景下的表现。不同算法的鲁棒性在这四个挑战下进行评估：遮挡、视角、照明和背景变化。至于它们的准确性，由于不同应用使用不同的度量标准，我们采用来自动作识别和视频分割的代表性任务的准确性度量，其中更具表现力的特征通常能带来更好的准确性。
- en: 'Comparison with Existing Surveys. Representation learning from images is a
    classic problem in computer vision, and it has been widely studied to facilitate
    various image analysis and understanding tasks. Many survey papers have been published
    to address this problem. But most of these representation learning studies focused
    on features of static images [[112](#bib.bib112), [63](#bib.bib63), [124](#bib.bib124),
    [114](#bib.bib114)]. As summarized in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ Deep Video Representation Learning: a Survey"), while multiple components of
    these studies are closely related to video representation, a systematic survey
    on video features is missing. Some recent surveys  [[43](#bib.bib43), [86](#bib.bib86),
    [61](#bib.bib61)] discussed video representation learning, but most of these have
    focused on a specific type of learning or method. A few other survey papers discussed
    video processing tasks that involve video representation learning [[25](#bib.bib25),
    [144](#bib.bib144), [57](#bib.bib57), [167](#bib.bib167), [64](#bib.bib64)]; however,
    their focuses were mostly on discussing how the developed pipelines perform on
    the targeted task(s). There is a lack of a survey of representation learning in
    a general setting and one that investigates the role of each feature on its embedding
    regardless of its specific task. We believe our survey provides an insightful
    analysis on how to construct effective video features/representations, particularly
    when confronting with aforementioned challenges.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '与现有调查的比较。图像表示学习是计算机视觉中的经典问题，已被广泛研究以促进各种图像分析和理解任务。许多调查论文已发表以解决这一问题。但这些表示学习研究大多数关注静态图像的特征
    [[112](#bib.bib112), [63](#bib.bib63), [124](#bib.bib124), [114](#bib.bib114)]。如表[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Deep Video Representation Learning: a Survey")总结的那样，虽然这些研究的多个组件与视频表示密切相关，但系统的关于视频特征的调查仍然缺失。一些近期的调查
    [[43](#bib.bib43), [86](#bib.bib86), [61](#bib.bib61)] 讨论了视频表示学习，但大多数集中于特定类型的学习或方法。还有一些其他调查论文讨论了涉及视频表示学习的视频处理任务
    [[25](#bib.bib25), [144](#bib.bib144), [57](#bib.bib57), [167](#bib.bib167), [64](#bib.bib64)]；然而，它们主要关注讨论开发的流程在目标任务中的表现。缺乏一个在一般设置下调查表示学习的综述，并且调查每个特征在其嵌入中的作用，而不考虑其特定任务。我们相信我们的调查提供了如何构建有效视频特征/表示的深入分析，特别是在面对上述挑战时。'
- en: '1.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We provide a comprehensive survey of deep video representation learning.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了深度视频表示学习的全面调查。
- en: '2.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We compare different types of representation learning algorithms in terms of
    accuracy and robustness in various practical challenging scenes, and provide some
    observations/suggestions in adopting suitable features for different video processing
    and analysis tasks.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们比较了不同类型的表示学习算法在各种实际挑战场景中的准确性和鲁棒性，并提供了在不同视频处理和分析任务中采用合适特征的一些观察/建议。
- en: 'Table 1: Current surveys related to image and video feature learning. Unlike
    existing surveys that studied representation learning for specific targeting tasks,
    our survey discusses pros and cons of different features for general scope.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：与图像和视频特征学习相关的当前调查。与研究针对特定任务的表示学习的现有调查不同，我们的调查讨论了不同特征的一般范围的优缺点。
- en: '| Reference | Year | Image | Video | General scope | Application |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 年份 | 图像 | 视频 | 一般范围 | 应用 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| [[124](#bib.bib124)] | 2018 | ✓ | - | $\times$ | Visual-based localization
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| [[124](#bib.bib124)] | 2018 | ✓ | - | $\times$ | 基于视觉的定位 |'
- en: '| [[112](#bib.bib112)] | 2021 | ✓ | - | $\times$ | Image matching |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| [[112](#bib.bib112)] | 2021 | ✓ | - | $\times$ | 图像匹配 |'
- en: '| [[114](#bib.bib114)] | 2021 | ✓ | - | $\times$ | Semantic and instance segmentation
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| [[114](#bib.bib114)] | 2021 | ✓ | - | $\times$ | 语义和实例分割 |'
- en: '| [[63](#bib.bib63)] | 2021 | ✓ | - | $\times$ | Content-based image retrieval
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| [[63](#bib.bib63)] | 2021 | ✓ | - | $\times$ | 基于内容的图像检索 |'
- en: '| [[43](#bib.bib43)] | 2017 | ✓ | ✓ | $\times$ | Representation learning on
    graphs |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| [[43](#bib.bib43)] | 2017 | ✓ | ✓ | $\times$ | 图上的表示学习 |'
- en: '| [[86](#bib.bib86)] | 2018 | ✓ | ✓ | $\times$ | Multi modal learning |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| [[86](#bib.bib86)] | 2018 | ✓ | ✓ | $\times$ | 多模态学习 |'
- en: '| [[61](#bib.bib61)] | 2020 | ✓ | ✓ | $\times$ | Self supervised feature learning
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| [[61](#bib.bib61)] | 2020 | ✓ | ✓ | $\times$ | 自监督特征学习 |'
- en: '| [[141](#bib.bib141)] | 2021 | ✓ | ✓ | $\times$ | 2D and 3D pose estimation
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| [[141](#bib.bib141)] | 2021 | ✓ | ✓ | $\times$ | 2D 和 3D 姿态估计 |'
- en: '| [[144](#bib.bib144)] | 2021 | ✓ | ✓ | $\times$ | Multi person pose estimation
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| [[144](#bib.bib144)] | 2021 | ✓ | ✓ | $\times$ | 多人姿态估计 |'
- en: '| [[25](#bib.bib25)] | 2020 | - | ✓ | $\times$ | Soccer video analysis |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| [[25](#bib.bib25)] | 2020 | - | ✓ | $\times$ | 足球视频分析 |'
- en: '| [[57](#bib.bib57)] | 2021 | - | ✓ | $\times$ | Multi-view video summarization
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| [[57](#bib.bib57)] | 2021 | - | ✓ | $\times$ | 多视角视频总结 |'
- en: '| [[64](#bib.bib64)] | 2022 | - | ✓ | $\times$ | Event detection in surveillance
    videos |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| [[64](#bib.bib64)] | 2022 | - | ✓ | $\times$ | 监控视频中的事件检测 |'
- en: '| [[167](#bib.bib167)] | 2022 | - | ✓ | $\times$ | Pedestrian attribute recognition
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| [[167](#bib.bib167)] | 2022 | - | ✓ | $\times$ | 行人属性识别 |'
- en: '| Ours | 2023 | ✓ | ✓ | ✓ | Action recognition + video segmentation |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 2023 | ✓ | ✓ | ✓ | 动作识别 + 视频分割 |'
- en: 'Organization. We present the classification of deep features for videos in
    Section [2](#S2 "2 Classification of Deep Video Features ‣ Deep Video Representation
    Learning: a Survey"), and then compare these features in action recognition and
    video segmentation application in Section [3](#S3 "3 Applying Deep Features in
    Video Analysis Tasks ‣ Deep Video Representation Learning: a Survey"). We conclude
    the paper by discussing remaining challenges and future directions in Section [4](#S4
    "4 Conclusion and Future Work ‣ Deep Video Representation Learning: a Survey").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '组织结构。我们在第[2](#S2 "2 Classification of Deep Video Features ‣ Deep Video Representation
    Learning: a Survey")节中介绍了视频深度特征的分类，然后在第[3](#S3 "3 Applying Deep Features in Video
    Analysis Tasks ‣ Deep Video Representation Learning: a Survey")节中比较了这些特征在动作识别和视频分割中的应用。我们在第[4](#S4
    "4 Conclusion and Future Work ‣ Deep Video Representation Learning: a Survey")节中总结了论文，讨论了剩余挑战和未来方向。'
- en: 2 Classification of Deep Video Features
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度视频特征分类
- en: 'Two main aspects of video data are often considered in video processing and
    analysis tasks: (1) how to encode spatial structures or contents in visual data;
    and (2) how to model temporal coherency or changes among frames. Appearance information
    and geometry structure of the scene or objects are considered as spatial information.
    In representation learning, capturing spatial relation is important in understanding
    the visual concept of the video. Based on spatial information extracted, we generally
    divide features into dense features and sparse features. Spatially dense features
    are contextual data often defined using pixel intensities of the input. Typical
    and widely used dense features can come from RGB images and their variants, such
    as RGBD images. Spatially sparse features are often defined on a smaller set of
    entities such as keypoints, subpatches, or other graph structures. Widely used
    sparse features include those defined on divided patches or structural graphs
    (e.g., media axes for general shapes, skeletal structures of humans/animals).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 视频数据的两个主要方面通常在视频处理和分析任务中被考虑：(1) 如何编码视觉数据中的空间结构或内容；(2) 如何建模帧之间的时间一致性或变化。场景或物体的外观信息和几何结构被视为空间信息。在表示学习中，捕捉空间关系对于理解视频的视觉概念很重要。根据提取的空间信息，我们通常将特征分为密集特征和稀疏特征。空间上密集的特征通常是使用输入的像素强度定义的上下文数据。典型且广泛使用的密集特征可以来自RGB图像及其变体，如RGBD图像。空间上稀疏的特征通常定义在较小的实体集上，如关键点、子块或其他图形结构。广泛使用的稀疏特征包括定义在划分块或结构图（例如，一般形状的媒体轴、人类/动物的骨架结构）上的特征。
- en: '![Refer to caption](img/2efa6770c749e93c6e93d2be515a1317.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2efa6770c749e93c6e93d2be515a1317.png)'
- en: 'Figure 1: Classification of deep video representation learning schemes.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：深度视频表示学习方案的分类。
- en: 'Table 2: Pros and cons of different types of features. Scene noise includes
    occlusion, illumination, background and viewpoint variations.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：不同类型特征的优缺点。场景噪声包括遮挡、照明、背景和视点变化。
- en: '| Features | Pros | Cons |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 优点 | 缺点 |'
- en: '| Dense | Good in capturing appearance information | Sensitive to scene noise
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 密集 | 在捕捉外观信息方面表现良好 | 对场景噪声敏感 |'
- en: '|  |  | High intra-class variations |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 高类内变化 |'
- en: '|  |  | High computational cost |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 高计算成本 |'
- en: '| Sparse | Robust against background and illumination change | Weak in capturing
    appearance information |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | 对背景和照明变化具有鲁棒性 | 在捕捉外观信息方面较弱 |'
- en: '|  | Low intra-class variations |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | 低类内变化 |  |'
- en: '|  | Low computational cost |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | 低计算成本 |  |'
- en: '| Frame-level | Low computational cost | Weak in co-occurrence representation
    learning |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 帧级别 | 低计算成本 | 在共现表示学习中表现较弱 |'
- en: '| Chunk-level | Good in co-occurrence representation learning | High computational
    cost |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 块级别 | 在共现表示学习中表现良好 | 高计算成本 |'
- en: 'The second aspect is temporal information which plays an important role in
    video representation learning, and is a key difference between image features
    and video features. To effectively understand the concept of a video, temporal
    information or temporal coherence across different frames plays a critical role.
    Based on how the temporal information is modeled, we divide temporal features
    into two categories: frame-level and chunk-level features. The former extracts
    features from each frame and constructs a sequence of signatures; while the later
    one encodes spatio-temporal features of a chunk into one signature.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个方面是时间信息，它在视频表示学习中发挥着重要作用，并且是图像特征与视频特征之间的关键区别。为了有效理解视频的概念，时间信息或不同帧之间的时间一致性起着关键作用。根据时间信息的建模方式，我们将时间特征分为两类：帧级特征和片段级特征。前者从每一帧中提取特征，并构建一个签名序列；而后者将一个片段的时空特征编码成一个签名。
- en: 'We illustrate our classification in Fig. [1](#S2.F1 "Figure 1 ‣ 2 Classification
    of Deep Video Features ‣ Deep Video Representation Learning: a Survey"). In the
    following subsections [2.1](#S2.SS1 "2.1 Spatially dense features ‣ 2 Classification
    of Deep Video Features ‣ Deep Video Representation Learning: a Survey") to [2.4](#S2.SS4
    "2.4 Chunk-level features ‣ 2 Classification of Deep Video Features ‣ Deep Video
    Representation Learning: a Survey"), we classify video features based on how their
    spatial and temporal information is modeled, and discuss their pros and cons,
    as summarized in Table [2](#S2.T2 "Table 2 ‣ 2 Classification of Deep Video Features
    ‣ Deep Video Representation Learning: a Survey"). We will discuss how different
    designs affect the features’ general robustness under different scenarios. In
    terms of feature accuracy, it is more application-dependent, and will be discussed
    in the application section.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图 [1](#S2.F1 "Figure 1 ‣ 2 Classification of Deep Video Features ‣ Deep
    Video Representation Learning: a Survey")中说明了我们的分类。在接下来的子节 [2.1](#S2.SS1 "2.1
    Spatially dense features ‣ 2 Classification of Deep Video Features ‣ Deep Video
    Representation Learning: a Survey")至[2.4](#S2.SS4 "2.4 Chunk-level features ‣
    2 Classification of Deep Video Features ‣ Deep Video Representation Learning:
    a Survey")，我们根据空间和时间信息的建模方式对视频特征进行分类，并讨论它们的优缺点，如表 [2](#S2.T2 "Table 2 ‣ 2 Classification
    of Deep Video Features ‣ Deep Video Representation Learning: a Survey")所总结的。我们将讨论不同设计如何影响特征在不同场景下的整体鲁棒性。在特征准确性方面，这更依赖于应用，并将在应用部分讨论。'
- en: 2.1 Spatially dense features
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 空间密集特征
- en: Spatially dense features contain rich information mostly defined by using direct
    pixel intensities of the input in a structured order. RGB video is the most common
    data type in dense features. It contains contextual data about appearance, objects
    and background. People often use a 2D Convolutional Neural Network (CNN) as *a
    standard architecture* for extracting spatial information from dense features.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 空间密集特征包含丰富的信息，主要是通过以结构化顺序使用输入的直接像素强度定义的。RGB视频是密集特征中最常见的数据类型。它包含关于外观、对象和背景的上下文数据。人们通常使用2D卷积神经网络（CNN）作为*标准架构*来从密集特征中提取空间信息。
- en: CNN-based Standard Architectures. With the emergence of deep learning, CNNs
    have become the most common method for feature modeling due to the strong modeling
    capability and superior performance of deep learning-based methods. Various CNN
    architectures including VGGNet [[140](#bib.bib140)] and ResNet-50 [[46](#bib.bib46)]
    have been used for spatial representation learning [[97](#bib.bib97), [200](#bib.bib200),
    [34](#bib.bib34)]. These models provide a high-level spatial representation of
    video frames. Some approaches also use object detection or segmentation algorithms
    to extract local regions of interest in a frame to better exploit the correlation
    of different regions and reduce the chance of encoding redundant information [[5](#bib.bib5)].
    For example, in [[90](#bib.bib90)], an interaction module is proposed to model
    interactions between a (human) object and it’s surroundings. Mask-RCNN [[47](#bib.bib47)]
    is used for semantic segmentation, then these masked features are given to a 2-layer
    convolutional network for further feature learning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 CNN 的标准架构。随着深度学习的兴起，CNN 已成为特征建模的最常用方法，因为深度学习方法具有强大的建模能力和优越的性能。各种 CNN 架构，包括
    VGGNet [[140](#bib.bib140)] 和 ResNet-50 [[46](#bib.bib46)]，已经被用于空间表示学习 [[97](#bib.bib97),
    [200](#bib.bib200), [34](#bib.bib34)]。这些模型提供了视频帧的高级空间表示。一些方法还使用对象检测或分割算法来提取帧中的局部感兴趣区域，以更好地利用不同区域之间的关联，并减少编码冗余信息的可能性
    [[5](#bib.bib5)]。例如，在 [[90](#bib.bib90)] 中，提出了一种交互模块来建模（人）对象与其周围环境之间的交互。使用 Mask-RCNN [[47](#bib.bib47)]
    进行语义分割，然后将这些掩模特征输入到一个 2 层卷积网络中以进一步进行特征学习。
- en: 'Extra modules for better robustness. Effective video features should be robust
    against occlusion, view and background change. Therefore, built upon the above
    *standard architectures*, people also add extra modules to improve feature robustness.
    Recent approaches adopted three general types of additional modules: *part information*,
    *additional input information*, and *attention mechanism*. In the following, we
    will elaborate that given a *standard architecture* X, adding one or multiple
    modules onto X could improve the pipeline’s robustness under different scenarios.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好的鲁棒性，额外的模块是必要的。有效的视频特征应对遮挡、视角和背景变化具有鲁棒性。因此，在上述*标准架构*的基础上，人们还添加了额外的模块以提高特征的鲁棒性。最近的方法采用了三种常见的附加模块类型：*部分信息*、*附加输入信息*和*注意力机制*。接下来，我们将详细说明，在给定一个*标准架构*
    X 的情况下，添加一个或多个模块到 X 上可以在不同场景下提高管道的鲁棒性。
- en: Part information
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 部分信息
- en: Typical video representation learning methods do not take into account the effect
    of partial occlusion. But with partial occlusion in the video, learned features
    are often corrupted due to the inclusion of irrelevant surrounding objects, and
    consequently, cause dramatic performance degradation. A conventional strategy
    is to train ensemble models for various occlusion patterns [[149](#bib.bib149),
    [118](#bib.bib118), [15](#bib.bib15)], and construct a part pool that covers different
    scales of object parts, and then automatically choose important parts to overcome
    occlusions. A main limitation of this strategy is that it is more expensive in
    both training and testing phases. Another strategy is to integrate a set of part/occlusion-specific
    detectors in a joint framework that learns partial occlusions [[203](#bib.bib203),
    [119](#bib.bib119)]. In [[203](#bib.bib203)], a set of part detectors are applied
    and the correlation among them is exploited to improve their performance. Each
    part detector generates a score for a candidate region and the final score is
    computed based on the average among some top scores. The main issue with such
    part-specific detectors is that they are not able to cover all the occlusion patterns
    comprehensively and need to be designed based on some pre-assumptions [[183](#bib.bib183)].
    For example, for pedestrian detection, these part detectors are designed with
    the prior knowledge that pedestrians are usually occluded from the bottom, left,
    and right. However, in practice, occlusion patterns can be irregular, which affects
    the feature’s performance.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的视频表示学习方法没有考虑部分遮挡的影响。但在视频中存在部分遮挡时，由于包含了不相关的周围物体，学习到的特征往往会被破坏，从而导致性能急剧下降。传统的策略是为各种遮挡模式训练集成模型[[149](#bib.bib149),
    [118](#bib.bib118), [15](#bib.bib15)]，并构建一个覆盖不同尺度物体部分的部件池，然后自动选择重要部分以克服遮挡。这种策略的主要限制是它在训练和测试阶段都更昂贵。另一种策略是将一组特定于部件/遮挡的检测器集成到一个联合框架中，该框架学习部分遮挡[[203](#bib.bib203),
    [119](#bib.bib119)]。在[[203](#bib.bib203)]中，应用了一组部件检测器，并利用它们之间的相关性来提高它们的性能。每个部件检测器为候选区域生成一个分数，最终分数是基于一些顶级分数的平均值计算的。这种部件特定检测器的主要问题是它们不能全面覆盖所有的遮挡模式，并且需要基于一些预设假设进行设计[[183](#bib.bib183)]。例如，对于行人检测，这些部件检测器是基于行人通常从底部、左侧和右侧被遮挡的先验知识设计的。然而，在实际应用中，遮挡模式可能是不规则的，这会影响特征的性能。
- en: Additional Information
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 额外信息
- en: Some studies use additional information to compensate for the view and the illumination
    variance of dense features. For example, RGB-D cameras are often used to include
    depth information which makes the representation less sensitive to illumination
    and view variations [[52](#bib.bib52), [100](#bib.bib100)]. Thermal images together
    with color images are also used to improve robustness when suitable light source
    is not available [[71](#bib.bib71)]. Several other works use non-vision information
    as a complementary input. For example, in [[40](#bib.bib40)], audio signals and
    in [[105](#bib.bib105)] signals from wearable sensors are added to improve robustness
    of dense features.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究使用额外的信息来补偿密集特征的视角和光照变化。例如，RGB-D 摄像头常用于包含深度信息，这使得表示对光照和视角变化的敏感性降低[[52](#bib.bib52),
    [100](#bib.bib100)]。热成像和颜色图像也被用来在合适的光源不可用时提高鲁棒性[[71](#bib.bib71)]。其他几项工作使用非视觉信息作为补充输入。例如，在[[40](#bib.bib40)]中，音频信号和在[[105](#bib.bib105)]中，来自可穿戴传感器的信号被添加以提高密集特征的鲁棒性。
- en: Attention Mechanism
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力机制
- en: In most scenarios, key objects/regions are just part of the whole spatial image.
    Being able to use local spatial attention to guide the system to focus on important
    (foreground) object and ignore irrelevant (background) noise is desirable. Many
    recent studies developed attention mechanisms to help feature learning models
    concentrate on important regions in the spatial dimension. For example, in [[183](#bib.bib183)],
    an attentive spatial pooling is used instead of max-pooling which computes similarity
    scores between features to compute attention vectors in the spatial dimension.
    This method allows model to be more attentive on region of interests in image
    level. In [[88](#bib.bib88)], a self-attention mechanism is adopted to generate
    a context-aware feature map. Then the similarity between context-aware feature
    maps and a set of learnable part prototypes are calculated and used as spatial
    attention maps. To identify the object of interest and focus on visible parts,
    some models use spatially sparse features in spatial attention networks to construct
    a robust representation. In [[198](#bib.bib198), [178](#bib.bib178)], sparse features
    aid RGB frames to estimate the attention map and visibility scores to handle various
    occlusion patterns and generate view-invariant representation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，关键对象/区域只是整个空间图像的一部分。能够使用局部空间注意力引导系统关注重要（前景）对象并忽略不相关（背景）噪声是理想的。许多近期研究开发了注意力机制，以帮助特征学习模型集中于空间维度中的重要区域。例如，在[[183](#bib.bib183)]中，使用了注意空间池化来代替最大池化，后者计算特征之间的相似度分数以计算空间维度中的注意力向量。这种方法使模型在图像级别上对感兴趣的区域更加关注。在[[88](#bib.bib88)]中，采用了一种自注意力机制来生成上下文感知特征图。然后，计算上下文感知特征图与一组可学习的部件原型之间的相似度，并将其用作空间注意力图。为了识别感兴趣的对象并关注可见部分，一些模型在空间注意力网络中使用空间稀疏特征来构建稳健的表示。在[[198](#bib.bib198),
    [178](#bib.bib178)]中，稀疏特征帮助RGB帧估计注意力图和可见性分数，以处理各种遮挡模式并生成视图不变的表示。
- en: Summary of Extra Modules
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 附加模块总结
- en: 'Although dense features can encode rich contextual information, their representation
    can sometimes be sensitive to occlusion, view, illumination, and background variance.
    Given a standard architecture $X$ that extracts dense features, various recent
    studies exploited adding extra modules onto $X$ to enhance the robustness of spatial
    feature modeling. Table [3](#S2.T3 "Table 3 ‣ Summary of Extra Modules ‣ 2.1 Spatially
    dense features ‣ 2 Classification of Deep Video Features ‣ Deep Video Representation
    Learning: a Survey") summarizes these strategy discussed above. $X$ + part information
    is often effective in enhancing the model’s robustness against partial occlusion.
    $X$ + additional input information (e.g., depth info) can help the model enhance
    its performance under illumination and view changes. Many recent studies incorporate
    attention mechanism into the standard architecture, such an attention strategy
    often help differentiate foreground objects of interest and the background noise,
    and consequently, helps the model perform better towards view, occlusion, and
    background variance.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管密集特征可以编码丰富的上下文信息，但它们的表示有时对遮挡、视角、照明和背景变化敏感。给定一个提取密集特征的标准架构 $X$，各种近期研究利用在 $X$
    上添加额外模块来增强空间特征建模的鲁棒性。表 [3](#S2.T3 "Table 3 ‣ Summary of Extra Modules ‣ 2.1 Spatially
    dense features ‣ 2 Classification of Deep Video Features ‣ Deep Video Representation
    Learning: a Survey") 总结了上述策略。 $X$ + 部分信息通常在增强模型对部分遮挡的鲁棒性方面有效。 $X$ + 额外输入信息（例如深度信息）可以帮助模型在照明和视角变化下提高性能。许多近期研究将注意力机制纳入标准架构，这种注意力策略通常有助于区分感兴趣的前景对象和背景噪声，从而帮助模型在视角、遮挡和背景变化方面表现更好。'
- en: 'Table 3: Robustness of different models using Dense features. A standard architecture
    is called X. Its limitations against robustness challenges can be addressed/alleviated
    by adding extra modules.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：使用密集特征的不同模型的鲁棒性。一个标准架构称为 X。其在鲁棒性挑战中的局限性可以通过添加附加模块来解决/缓解。
- en: '| Model | View | Occlusion | Background | Illumination |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 视角 | 遮挡 | 背景 | 照明 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| X | - | - | - | - |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| X | - | - | - | - |'
- en: '| X + part information | - | ✓ | - | - |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| X + 部分信息 | - | ✓ | - | - |'
- en: '| X + additional information | ✓ | - | - | ✓ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| X + 额外信息 | ✓ | - | - | ✓ |'
- en: '| X + attention | ✓ | ✓ | ✓ | - |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| X + 注意力 | ✓ | ✓ | ✓ | - |'
- en: 2.2 Spatially sparse features
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 空间稀疏特征
- en: 'Spatially sparse features represent information with a sparse set of feature
    points that are able to describe the geometry of original video frames. There
    are several advantages of using sparse features. First, they describe frames with
    a sparse set of features that leads to a low computational cost. Second, sparse
    features suffer less intra-class variances compared to dense features and are
    more robust to the change of conditions such as appearance, illumination, and
    backgrounds. However, sparse features lack appearance information which in some
    scenarios is essential for feature modeling. Three types of *standard architectures*
    are often adapted to extract sparse features: *RNN, CNN, and GNN/GCN pipelines.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 空间稀疏特征通过稀疏的特征点集来表示信息，这些特征点能够描述原始视频帧的几何形状。使用稀疏特征有几个优点。首先，它们用稀疏的特征集来描述帧，从而降低计算成本。其次，与密集特征相比，稀疏特征在类别内部的方差较小，对条件变化（如外观、光照和背景）更具鲁棒性。然而，稀疏特征缺乏外观信息，在某些场景中，这对于特征建模是必不可少的。通常有三种*标准架构*被适配用于提取稀疏特征：*RNN、CNN和GNN/GCN管道*。
- en: RNN-based Standard Architectures. Due to the recurrent nature of videos, one
    strategy is to use RNN-based architectures for sparse representation learning.
    Spatial feature arrangements determine the proper architecture for spatial extraction.
    If the inherent spatial relations between feature points can be arranged in a
    sequence of vectors or grids, RNNs are effective for spatial modeling [[156](#bib.bib156),
    [32](#bib.bib32)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 基于RNN的标准架构。由于视频的递归特性，一种策略是使用基于RNN的架构进行稀疏表示学习。空间特征的安排决定了适合空间提取的架构。如果特征点之间的固有空间关系可以排列成一系列向量或网格，RNN在空间建模中是有效的[[156](#bib.bib156),
    [32](#bib.bib32)]。
- en: '*Pros and Cons of RNN-based Standard Architectures* RNN-based methods are good
    in dealing with sequential data, but they are not very effective in spatial modeling.
    Therefore, their general performance is not as good as CNN models [[102](#bib.bib102)].
    In several works, RNN-based models take feature maps from CNNs as their input
    rather than using raw input frames [[181](#bib.bib181), [34](#bib.bib34)].'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于RNN的标准架构的优缺点* RNN-based方法在处理序列数据时表现良好，但在空间建模方面并不十分有效。因此，它们的总体性能不如CNN模型[[102](#bib.bib102)]。在一些工作中，基于RNN的模型使用CNN的特征图作为输入，而不是使用原始输入帧[[181](#bib.bib181),
    [34](#bib.bib34)]。'
- en: CNN-based Standard Architectures. There are many studies that deployed CNNs
    for sparse representation learning. However, defining the relations between unstructured
    feature points is challenging. CNNs take their input in a form of an image. To
    satisfy the need of CNNs’ input, some researches model the sparse features into
    multiple 2D pseudo-images [[101](#bib.bib101), [104](#bib.bib104), [75](#bib.bib75)].
    For example, in [[101](#bib.bib101)], sparse features are arranged into an image
    containing the feature points in one dimension and frames in another one, then
    a 7-layer CNN is applied to extract features. In [[164](#bib.bib164)], the frame
    dynamics are mapped into textured images, then a CNN model is used to extract
    information. Some works use a group of feature points to construct a hierarchy
    among features. In [[48](#bib.bib48)], the video frame is divided into multiple
    grids, then a convolutional feature descriptor is run for each cell.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CNN的标准架构。许多研究已将CNN应用于稀疏表示学习。然而，定义非结构化特征点之间的关系是具有挑战性的。CNN以图像的形式接收输入。为了满足CNN输入的需求，一些研究将稀疏特征建模为多个2D伪图像[[101](#bib.bib101),
    [104](#bib.bib104), [75](#bib.bib75)]。例如，在[[101](#bib.bib101)]中，稀疏特征被排列成一个图像，其中一个维度包含特征点，另一个维度包含帧，然后应用7层CNN提取特征。在[[164](#bib.bib164)]中，帧动态被映射到纹理图像中，然后使用CNN模型提取信息。一些工作使用一组特征点在特征之间构建层次结构。在[[48](#bib.bib48)]中，视频帧被划分为多个网格，然后对每个单元格运行卷积特征描述符。
- en: '*Pros and Cons of CNN-based Standard Architectures* CNN-based architectures
    are effective in extracting local features and exploring discriminative patterns
    in data. However, their major drawback is that they are designed for image-based
    input and primarily rely on spatial dependencies between the neighboring points.
    For some sparse features that contain unstructured design, CNNs cannot perform
    very well. Additionally, CNNs have trouble with wildly sparse data as they heavily
    rely on spatial relations of points to learn. In that case, RNNs perform better.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于CNN的标准架构的优缺点* 基于CNN的架构在提取局部特征和探索数据中的判别模式方面非常有效。然而，它们的主要缺点是设计用于基于图像的输入，并且主要依赖于相邻点之间的空间依赖关系。对于一些包含非结构化设计的稀疏特征，CNN表现不是很好。此外，CNN在处理极度稀疏的数据时也会遇到困难，因为它们严重依赖点的空间关系来学习。在这种情况下，RNN表现更好。'
- en: GNN/GCN Standard Architectures. Modeling sparse features in a vector or an image
    may corrupt the spatial relations or add false connections between feature points
    that their relation is not strong enough. Some of these irregular features are
    intrinsically structured as a graph and cannot be modeled in a vector, 2D or 3D
    grid. To address this issue, some researches utilize graph structure, such as
    graph convolutional neural networks (GCNNs)  [[187](#bib.bib187), [76](#bib.bib76),
    [80](#bib.bib80)] or GNNs [[44](#bib.bib44)] to build feature extraction architectures.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: GNN/GCN 标准架构。将稀疏特征建模为向量或图像可能会破坏空间关系，或在特征点之间添加不够强的虚假连接。这些不规则特征本质上结构化为图，不能用向量、2D或3D网格建模。为了解决这个问题，一些研究利用图结构，如图卷积神经网络（GCNNs）
    [[187](#bib.bib187), [76](#bib.bib76), [80](#bib.bib80)] 或GNNs [[44](#bib.bib44)]
    来构建特征提取架构。
- en: Some people use spatio-temporal GCNs by arranging feature points as an indirect
    graph with points as nodes and their relations as edges. Nodes within one frame
    are connected based on the spatial relations of the features and represented by
    an adjacency matrix in spatial dimension. Nodes in the temporal dimension are
    related through the relations of corresponding nodes in consecutive frames [[187](#bib.bib187)].
    A concern with spatio-temporal GCNs is that modeling spatial features just according
    to natural connection of nodes might lose the potential dependency of disconnected
    joints. To solve this issue, in [[80](#bib.bib80), [133](#bib.bib133)], a model
    is introduced to adaptively learn and update the topology of graph for different
    layers and samples. In [[80](#bib.bib80)], a framework is proposed to capture
    richer dependencies among points and neighbors. They used an encoder-decoder structure
    to model dependencies between far-apart points. When modeling sparse features,
    defining connections between different feature nodes is challenging. For example,
    spatio-temporal GCN models the features as an indirect graph, which may not fully
    express the direction and position of points. [[133](#bib.bib133)] used a directed
    acyclic graph to represent sparse data. Then a directed graph neural network is
    used to encode the constructed directed graph, which can propagate the information
    to adjacent nodes and edges and update their associated information in each layer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人通过将特征点安排为间接图，将点作为节点，将它们的关系作为边，使用时空GCNs。一个帧内的节点基于特征的空间关系连接，并由空间维度的邻接矩阵表示。时序维度的节点通过连续帧中对应节点的关系进行关联
    [[187](#bib.bib187)]。使用时空GCNs的一个问题是，仅根据节点的自然连接建模空间特征可能会丧失断开关节的潜在依赖关系。为了解决这个问题，在
    [[80](#bib.bib80), [133](#bib.bib133)] 中，引入了一种模型来自适应地学习和更新不同层和样本的图拓扑。在 [[80](#bib.bib80)]
    中，提出了一种框架以捕捉点和邻居之间更丰富的依赖关系。他们使用了编码器-解码器结构来建模远离点之间的依赖关系。在建模稀疏特征时，定义不同特征节点之间的连接是具有挑战性的。例如，时空GCN将特征建模为间接图，这可能无法完全表达点的方向和位置。[[133](#bib.bib133)]
    使用了有向无环图来表示稀疏数据。然后，使用有向图神经网络对构建的有向图进行编码，这可以将信息传播到相邻的节点和边，并在每层更新它们的相关信息。
- en: '*Pros and cons of GNN/GCN Standard Architectures* GNN/GCNs are helpful in dealing
    unstructured data that have underlying graph structures and are non-Euclidean.
    The non-regularity of data structures in most sparse features have led to superior
    performance of graph neural networks over CNN or RNN architectures. But if sparse
    features can be formulated as 2D grids or vectors, then graph-based models are
    not as efficient as CNN models.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*GNN/GCN 标准架构的优缺点* GNN/GCNs 有助于处理具有潜在图结构且为非欧几里得的非结构化数据。大多数稀疏特征中的数据结构的不规则性导致图神经网络在性能上优于
    CNN 或 RNN 架构。但如果稀疏特征可以被表述为 2D 网格或向量，那么基于图的模型则不如 CNN 模型高效。'
- en: Extra Modules for Better Robustness. The representation of spatially sparse
    features is robust to background and illumination change. To overcome other robustness
    issues, some studies add extra modules on the standard architecture. Standard
    architecture X could be a CNN, RNN or GNN/GCN model. Generally, two extra modules,
    *transformation matrix* and *attention*, are often added to enhance feature robustness.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 提升鲁棒性的额外模块。空间稀疏特征的表示对背景和光照变化具有鲁棒性。为了解决其他鲁棒性问题，一些研究在标准架构上添加了额外的模块。标准架构 X 可以是
    CNN、RNN 或 GNN/GCN 模型。通常，两个额外模块，*变换矩阵*和*注意力机制*，常常被添加以增强特征的鲁棒性。
- en: Transformation matrix
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 变换矩阵
- en: Change of camera view points can change the relative position of feature points.
    Hence, constructing a view-invariant representation remains still a challenge
    in sparse feature modeling. To address this problem, several researches developed
    a transformation method to transform a set of feature points to a standard structure
    [[56](#bib.bib56), [87](#bib.bib87), [104](#bib.bib104)]. In [[87](#bib.bib87)],
    a rotor-based view transformation method is proposed to re-position the original
    features to a standard frontal system. After transformation, a spatio-temporal
    model is applied to construct the shape and motion of each part. In [[104](#bib.bib104)],
    a sequence-based transformation is applied on the features to map them to a standard
    form and make a new view-invariant sequence.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 相机视点的变化会改变特征点的相对位置。因此，在稀疏特征建模中，构建视点不变的表示仍然是一个挑战。为了解决这个问题，一些研究开发了一种变换方法，将一组特征点转换为标准结构
    [[56](#bib.bib56), [87](#bib.bib87), [104](#bib.bib104)]。在 [[87](#bib.bib87)]
    中，提出了一种基于转子视图变换的方法，将原始特征重新定位到标准前视系统。变换后，应用时空模型来构建每个部分的形状和运动。在 [[104](#bib.bib104)]
    中，应用了一种基于序列的变换方法，将特征映射到标准形式，并生成新的视点不变序列。
- en: Attention mechanism
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力机制
- en: Attention aids model in confronting with partial occlusion and view variations.
    Similar to dense features, sparse feature modeling in the presence of occlusion
    is still challenging. Some approaches deploy spatial attention mechanism to focus
    on the points of interest and predict occluded parts by the help of visible feature
    points in the adjacent frames. In [[51](#bib.bib51)], a spatial attention generator
    is proposed to predict occluded parts. The generator is an autoencoder that predicts
    the content of occluded part conditioned on the visible parts of the current frame.
    Some other studies use heatmaps as attention mechanism to focus on informative
    feature points. In [[35](#bib.bib35)], heatmaps for the visible and occluded part
    are generated. Then, using the heatmaps occluded parts are predicted along both
    spatial and temporal dimension. In [[142](#bib.bib142)], class activation maps
    are used as a mask matrix to force network has to learn features from currently
    inactivated points. To achieve a view-invariant representation, some models propose
    transferring attention from reference view to arbitrary views. For example, in
    [[60](#bib.bib60)], attention maps are produced to transfer attention from a reference
    view to arbitrary views. This helps learn effective attention to crucial feature
    points. In [[163](#bib.bib163)], attention directly operates on network parameters
    rather on input features. This allows spatial interactional contexts to be explicitly
    captured in a unified way.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力有助于模型应对部分遮挡和视角变化。与密集特征类似，在存在遮挡的情况下，稀疏特征建模仍然具有挑战性。一些方法部署空间注意力机制来关注感兴趣的点，并通过相邻帧中的可见特征点来预测被遮挡的部分。在
    [[51](#bib.bib51)] 中，提出了一种空间注意力生成器来预测被遮挡的部分。该生成器是一个自编码器，预测被遮挡部分的内容，基于当前帧的可见部分。一些其他研究使用热图作为注意力机制，以关注信息丰富的特征点。在
    [[35](#bib.bib35)] 中，生成了可见部分和遮挡部分的热图。然后，使用热图在空间和时间维度上预测遮挡部分。在 [[142](#bib.bib142)]
    中，使用类别激活图作为掩码矩阵，强制网络学习当前未激活点的特征。为了实现视角不变的表示，一些模型提议将注意力从参考视图转移到任意视图。例如，在 [[60](#bib.bib60)]
    中，生成了注意力图，将注意力从参考视图转移到任意视图。这有助于学习对关键特征点的有效注意力。在 [[163](#bib.bib163)] 中，注意力直接作用于网络参数，而不是输入特征。这允许以统一的方式明确捕捉空间交互上下文。
- en: '*Summary.* Table [4](#S2.T4 "Table 4 ‣ Attention mechanism ‣ 2.2 Spatially
    sparse features ‣ 2 Classification of Deep Video Features ‣ Deep Video Representation
    Learning: a Survey") shows the robustness of standard architecture X and extra
    modules. X could be RNN, CNN or GNN/GCN architecture and is robust against background
    and illumination changes. While pros and cons of X depend on the target task,
    generally RNN is better suited to more sparse data while CNN deals better with
    denser feature points. Typically if the nature of the structure of data is in
    a grid format, CNN is more effective in extracting spatial features. On the other
    hand, if data have a graph scheme, then GNN/GCN performs better. X + transformation
    matrix is robust against view variations by transforming a set of feature points
    to a standard view. X + attention is robust against occlusion and view variations
    by predicting occluded parts from visible points and transfer attention from one
    view to another, respectively.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*总结。* 表 [4](#S2.T4 "表 4 ‣ 注意力机制 ‣ 2.2 空间稀疏特征 ‣ 2 深度视频特征分类 ‣ 深度视频表示学习：综述") 显示了标准架构
    X 和额外模块的鲁棒性。X 可以是 RNN、CNN 或 GNN/GCN 架构，对背景和照明变化具有鲁棒性。虽然 X 的优缺点取决于目标任务，但一般来说，RNN
    更适合处理稀疏数据，而 CNN 更适合处理密集特征点。通常，如果数据的结构是网格格式，则 CNN 在提取空间特征时更有效。另一方面，如果数据具有图形结构，则
    GNN/GCN 表现更好。X + 转换矩阵通过将一组特征点转换到标准视图来应对视角变化。X + 注意力通过从可见点预测遮挡部分并分别从一个视图转移到另一个视图，对遮挡和视角变化具有鲁棒性。'
- en: 'Table 4: Robustness of different models using spatially sparse features. A
    standard architecture is called X.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 使用空间稀疏特征的不同模型的鲁棒性。标准架构称为 X。'
- en: '| model | view | occlusion | background | illumination |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 视图 | 遮挡 | 背景 | 照明 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| X | - | - | ✓ | ✓ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| X | - | - | ✓ | ✓ |'
- en: '| X + transformation matrix | ✓ | - | ✓ | ✓ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| X + 转换矩阵 | ✓ | - | ✓ | ✓ |'
- en: '| X + attention | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| X + 注意力 | ✓ | ✓ | ✓ | ✓ |'
- en: 2.3 Frame-level features
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 帧级特征
- en: Frame-level features are a sequence of signatures that each describes a frame
    individually. Given a video clip, an frame-level feature model often processes
    spatial information frame by frame and encodes the temporal relationship between
    frames. People adopt different strategies, such as *Optical flow*, *CNN-based
    architectures*, *RNN-based architectures*, and *Attention mechanisms* to extract
    frame level features.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 帧级特征是一系列每个单独描述一个帧的签名。给定一个视频片段，帧级特征模型通常逐帧处理空间信息，并编码帧之间的时间关系。人们采用不同的策略，例如*光流*、*基于CNN的架构*、*基于RNN的架构*和*注意机制*来提取帧级特征。
- en: Optical Flow. Optical flow is a feature containing motion information of consecutive
    frames that is useful for describing video dynamics [[49](#bib.bib49)]. It is
    computed by removing the non-moving scene that generates a background invariant
    representation compared to the original frames. As optical flow is computed frame
    by frame we categorize it as a frame-level feature. Most studies use optical flow
    as a temporal data along with their original spatial features to capture movements.
    Studies in [[12](#bib.bib12), [161](#bib.bib161), [160](#bib.bib160)], showed
    that using optical flow and RGB frames achieves a superior performance in modeling
    videos than only using RGB frames. In [[161](#bib.bib161)], optical flow of consecutive
    10 frames and RGB are fed to a CNN. The convolutional filters compute derivatives
    of the optical flow and learn motion dynamics w.r.t the image location. Although
    to some extent, extracting optical flows as an additional information helps models
    be more view-independent, robust to occlusion and cluttered background, it often
    cannot capture relatively long term dependencies. In addition to optical flow,
    there are other features that contain motion information. For example, in [[24](#bib.bib24)],
    a new motion representation called Potion is proposed that provides all the dynamic
    of an instance throughout the video clip in one image. Motion cues are represented
    by different colors which shows the relative time of the frame in the video clip.
    Using Potion along with RGB frames and optical flows aid model to encode relatively
    longer dependencies. In [[210](#bib.bib210)], in addition to optical flow, motion
    saliency [[18](#bib.bib18)] is calculated from consecutive video frames. Pre-computing
    motion information including optical flow is time-consuming and storage-demanding.
    Also, they cannot learn global dependencies among frames.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 光流。光流是一种包含连续帧运动信息的特征，对于描述视频动态非常有用[[49](#bib.bib49)]。它是通过去除非运动场景来计算的，从而生成与原始帧相比背景不变的表示。由于光流是逐帧计算的，我们将其归类为帧级特征。大多数研究将光流作为时间数据与原始空间特征一起使用，以捕捉运动。研究[[12](#bib.bib12),
    [161](#bib.bib161), [160](#bib.bib160)]表明，使用光流和RGB帧在建模视频时优于仅使用RGB帧。在[[161](#bib.bib161)]中，将连续10帧的光流和RGB输入到CNN中。卷积滤波器计算光流的导数并学习与图像位置相关的运动动态。虽然在某种程度上，提取光流作为附加信息有助于模型更具视角独立性，增强对遮挡和混乱背景的鲁棒性，但它往往无法捕捉相对长期的依赖关系。除了光流，还有其他包含运动信息的特征。例如，在[[24](#bib.bib24)]中，提出了一种新的运动表示法叫做Potion，它通过一张图像提供了实例在整个视频片段中的所有动态。运动线索通过不同的颜色表示，显示视频片段中帧的相对时间。结合Potion、RGB帧和光流，有助于模型编码相对较长的依赖关系。在[[210](#bib.bib210)]中，除了光流，还计算了连续视频帧的运动显著性[[18](#bib.bib18)]。预计算包括光流在内的运动信息是耗时且需要大量存储的。此外，它们无法学习帧之间的全局依赖关系。
- en: CNN-based Architectures. As CNNs have been widely used in the image tasks, some
    studies [[45](#bib.bib45), [94](#bib.bib94), [162](#bib.bib162)] adopted 2D CNNs
    to model the video clips. However, 2D CNNs need a fusion module to concatenate
    temporal cues. For example, in [[65](#bib.bib65)], late fusion is adopted which
    fuses information from two different CNNs in the first fully connected layer.
    In [[171](#bib.bib171)], the authors use 2D CNN networks to encode each frame
    into feature maps then concatenate them as a long vector as the video-level feature
    descriptor to holistic understanding of the video. However, using a simple fusion
    or average lacks the learning capability and doesn’t contain useful time-related
    features. In [[94](#bib.bib94)], the channels along the time dimension is shifted
    to improve the performance of temporal modeling with 2D CNNs. In [[186](#bib.bib186)],
    consecutive frames are aggregated with adaptive weights and then fed to convolutional
    networks.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CNN的架构。由于CNN已广泛应用于图像任务，一些研究[[45](#bib.bib45), [94](#bib.bib94), [162](#bib.bib162)]采用了2D
    CNN来建模视频片段。然而，2D CNN需要一个融合模块来连接时间线索。例如，在[[65](#bib.bib65)]中，采用了延迟融合的方法，该方法在第一个全连接层中融合了来自两个不同CNN的信息。在[[171](#bib.bib171)]中，作者使用2D
    CNN网络将每一帧编码为特征图，然后将它们连接成一个长向量，作为视频级特征描述符，以全面理解视频。然而，使用简单的融合或平均缺乏学习能力，并且不包含有用的时间相关特征。在[[94](#bib.bib94)]中，沿时间维度的通道被移位，以提高2D
    CNN的时间建模性能。在[[186](#bib.bib186)]中，将连续帧与自适应权重聚合，然后输入到卷积网络中。
- en: Recent papers extend the single pass feature encoder to the Siamese structure
    for time-related feature learning. The Siamese pipeline takes two frames as inputs
    and uses CNN encoders to extract feature maps. In [[17](#bib.bib17), [54](#bib.bib54)],
    their feature encoders shared the same weights to extract feature maps from the
    input images. Then they compare the similarity between the feature maps to build
    the inter-frame features. In [[117](#bib.bib117), [92](#bib.bib92), [81](#bib.bib81)],
    they use two independent encoders to extract features maps. They encode the features
    into two parts, one is for similarity comparison, the other stores semantic information
    of frames. These video features are more useful in the down-stream tasks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的论文将单次通过的特征编码器扩展到Siamese结构，以进行时间相关特征学习。Siamese管道将两帧作为输入，并使用CNN编码器提取特征图。在[[17](#bib.bib17),
    [54](#bib.bib54)]中，他们的特征编码器共享相同的权重来从输入图像中提取特征图。然后，他们比较特征图之间的相似性，以构建帧间特征。在[[117](#bib.bib117),
    [92](#bib.bib92), [81](#bib.bib81)]中，他们使用两个独立的编码器来提取特征图。他们将特征编码为两个部分，一个用于相似性比较，另一个存储帧的语义信息。这些视频特征在下游任务中更为有用。
- en: RNN-based Architectures. Due to the sequential nature of videos and the ability
    of memorizing the temporal relations in RNN-based architectures, some studies
    used these models for encoding motions in a video. Some studies used parallel
    stream architecture which one stream is responsible for extracting spatial information
    and the other stream is responsible for extracting temporal data. In [[159](#bib.bib159)],
    a two-stream architecture is proposed where the first stream is responsible for
    learning spatial dependency and the second for learning the temporal dynamics.
    The two streams are then aggregated with each other to represent data. As RNNs
    are not effective in learning long-term temporal dependencies, most models adopt
    LSTM models. In [[200](#bib.bib200), [34](#bib.bib34)], LSTM is fed with the spatial
    representation of each frame at each time step. Learning process at each time
    step is based not only on the observations at that time step, but also on the
    previous hidden states that provide temporal context for the video. Some other
    versions of Recurrent-based netwroks were also explored. For example, in [[34](#bib.bib34)],
    an extended GRU is proposed to model temporal relations by using current and old
    information. GRU requires less storage and performs faster.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 基于RNN的体系结构。由于视频的连续性特性以及RNN体系结构能够记忆视频中的时间关系，一些研究使用这些模型来对视频中的运动进行编码。一些研究采用了并行流体系结构，其中一个流负责提取空间信息，另一个流负责提取时间数据。在[[159](#bib.bib159)]中，提出了一个双流架构，第一个流负责学习空间依赖性，第二个流负责学习时间动态。然后这两个流互相聚合以表示数据。由于RNN在学习长期时间依赖性方面不够有效，大多数模型采用了LSTM模型。在[[200](#bib.bib200),
    [34](#bib.bib34)]中，LSTM在每个时间步骤都会受到每帧的空间表示的输入。每个时间步骤的学习过程不仅基于该时间步骤的观察结果，还基于提供视频时间上下文的先前隐藏状态。还探索了一些其他版本的基于循环的网络。例如，在[[34](#bib.bib34)]中，提出了扩展的GRU来通过使用当前和旧信息来建模时间关系。GRU需要更少的存储空间并且执行速度更快。
- en: Attention Mechanism. As not all frames are informative, several studies used
    temporal attention to discriminate and select key frames by assigning weights
    to them. In [[13](#bib.bib13)] a scaled dot product attention module is adopted
    that assigns weights to frame features according to their importance. In [[183](#bib.bib183)],
    all time steps resulting from RNN are combined by an attentive temporal pooling
    to compute an attentive score in temporal dimension to weight frames based on
    their goodness. In [[197](#bib.bib197)], a non-parametric self and collaborative
    attention network is proposed to efficiently calculate the correlation weights
    to align discriminative frames. In [[108](#bib.bib108)], a transformer module
    is adopted that iteratively chooses one frame as query and the rest as key features
    to compute the temporal attention.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制。由于并非所有帧都提供信息，一些研究使用时间注意力来区分和选择关键帧，通过为它们分配权重来实现。在[[13](#bib.bib13)]中，采用了一个缩放的点积注意力模块，根据其重要性为帧特征分配权重。在[[183](#bib.bib183)]中，通过一个具有关注性的时间池化来组合RNN产生的所有时间步骤，以计算时间维度上的注意得分，以便基于其优劣性权衡帧。在[[197](#bib.bib197)]中，提出了一个非参数的自协作关注网络，用于高效地计算相关权重以对准有区分性的帧。在[[108](#bib.bib108)]中，采用了一个变压器模块，通过迭代选择一个帧作为查询，其余帧作为关键特征来计算时间注意力。
- en: 'Table 5: Pros and cons of various architectures used for modeling frame-level
    features'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：用于建模帧级特征的各种架构的优缺点
- en: '| Models | Pros | Cons |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 优点 | 缺点 |'
- en: '| Optical | Effective for simple and local dynamics | Ineffective for long
    and complex dynamics |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 光学 | 对简单和局部动态有效 | 对长期和复杂动态无效 |'
- en: '| flow |  | Hard to handle temporal scale variance |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 流 |  | 难以处理时间尺度的差异 |'
- en: '|  |  | Expensive computation and storage |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 计算和存储消耗大 |'
- en: '| 2D CNN | Effective for simple and local dynamics | Ineffective for long and
    complex dynamics |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 2D CNN | 对简单和局部动态有效 | 对长期和复杂动态无效 |'
- en: '|  |  | hard to handle temporal scale variance |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 难以处理时间尺度的差异 |'
- en: '| RNN | Effective for complex dynamics | Hard to handle temporal scale variance
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| RNN | 对复杂动态有效 | 难以处理时间尺度的差异 |'
- en: '| Attention | Effective for long and complex dynamics | Expensive computation
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 注意力 | 对长期和复杂动态有效 | 计算消耗大 |'
- en: '|  | Effectively handle temporal scale variance |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | 有效处理时间尺度的差异 |  |'
- en: '*Summary.* As shown in table [2](#S2.T2 "Table 2 ‣ 2 Classification of Deep
    Video Features ‣ Deep Video Representation Learning: a Survey"), while frame-level
    features have lower computational costs compared to chunk-level features, they
    decouple spatial and temporal dimensions which leads to lack of co-occurrence
    representation learning. People adopt different methods for learning frame-level
    dynamics in a video as shown in table [5](#S2.T5 "Table 5 ‣ 2.3 Frame-level features
    ‣ 2 Classification of Deep Video Features ‣ Deep Video Representation Learning:
    a Survey"). Optical flow and CNN-based architectures assist network to capture
    short and simple cues. However, they suffer from the lack of memory and therefore
    they are not suitable for capturing long term dependencies. Additionally, optical
    flow is computationally expensive and storage demanding. RNN-based architectures,
    particularly LSTMs, are able to encode complex dynamics, however they treat each
    video clip equally and are invariant to inherent temporal diversities. Attention
    selects informative features and is suited for distinguishing complex tasks.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*总结。* 如表[2](#S2.T2 "Table 2 ‣ 2 Classification of Deep Video Features ‣ Deep
    Video Representation Learning: a Survey")所示，尽管帧级特征相较于块级特征具有较低的计算成本，但它们将空间和时间维度解耦，导致缺乏共现表示学习。人们采用不同的方法来学习视频中的帧级动态，如表[5](#S2.T5
    "Table 5 ‣ 2.3 Frame-level features ‣ 2 Classification of Deep Video Features
    ‣ Deep Video Representation Learning: a Survey")所示。光流和基于CNN的架构帮助网络捕捉短期和简单的线索。然而，它们缺乏记忆，因此不适合捕捉长期依赖性。此外，光流计算昂贵且对存储要求高。基于RNN的架构，特别是LSTM，能够编码复杂的动态，但它们对每个视频片段的处理相同，对固有的时间变化不变。注意力机制选择信息量大的特征，适合区分复杂任务。'
- en: 2.4 Chunk-level features
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 块级特征
- en: 'Table 6: Pros and cons of various dynamics used for modeling chunk-level features'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：用于建模块级特征的各种动态的优缺点
- en: '| Models | Pros | Cons |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 优点 | 缺点 |'
- en: '| --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 3D CNN | effective for coarse level dynamics | ineffective for fine dynamics
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 3D CNN | 对粗糙层次的动态有效 | 对细节动态无效 |'
- en: '|  |  | expensive computation |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 计算开销大 |'
- en: '| Attention | effective for fine and coarse level dynamics | expensive computation
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 注意力机制 | 对细节和粗糙层次的动态有效 | 计算开销大 |'
- en: '|  | effectively handle temporal scale variance |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | 有效处理时间尺度变化 |  |'
- en: 'While frame-level features separate spatial and temporal modeling completely,
    chunk-level features extract appearance and dynamics at the same time by creating
    a hierarchical representations of spatio-temporal data which leads to extracting
    more subject-related information. These features aggregate frames into one signature,
    then apply a deep neural network to extract both spatial and temporal information
    at the same time. People use different strategies to encode chunk-level features:
    *CNN-based architectures* and *Attention*.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然帧级特征完全分离了空间和时间建模，块级特征通过创建时空数据的层次表示，同时提取外观和动态，从而提取更多与主题相关的信息。这些特征将帧汇总为一个特征，然后应用深度神经网络同时提取空间和时间信息。人们使用不同的策略来编码块级特征：*基于CNN的架构*和*注意力机制*。
- en: CNN-based Standard Architectures. 3D CNN encode a chunk of frames into one signature
    and has the kernel size of s×s×d which s and d refers to the kernel spatial size
    and the number of frames in one signature, respectively. In [[72](#bib.bib72),
    [73](#bib.bib73), [184](#bib.bib184)], a video was split into multiple segments,
    each of which is fed to a 3D CNN for feature extraction. This 3D CNN starts by
    focusing on the spatial relations for the first frames and then learns temporal
    dynamics in the following frames [[151](#bib.bib151)]. In [[186](#bib.bib186)],
    two 3D CNN modules are utilized to encode both long and short temporal dependencies
    by taking chunks with different sizes.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CNN的标准架构。3D CNN将一段帧编码为一个特征，并且其卷积核大小为s×s×d，其中s和d分别指代卷积核的空间大小和每个特征中的帧数。在[[72](#bib.bib72),
    [73](#bib.bib73), [184](#bib.bib184)]中，一段视频被拆分为多个片段，每个片段被输入到一个3D CNN中进行特征提取。这个3D
    CNN首先关注前几帧的空间关系，然后在随后的帧中学习时间动态[[151](#bib.bib151)]。在[[186](#bib.bib186)]中，利用两个3D
    CNN模块通过处理不同大小的块来编码长短时间依赖性。
- en: Although 3D CNNs seem like a natural algorithm for modeling video, they have
    some drawbacks. First, they require a large number of parameters due to adding
    temporal dimension which leads to leveraging shallow architectures. In [[12](#bib.bib12)],
    a new model was introduced which used a 3D CNN with pre-trained Inception-V1 as
    a backbone. To bootstrap from the pre-trained ImageNet models [[29](#bib.bib29)],
    the weights of the 2D kernels are repeated along the time dimension. However,
    the fixed geometric structures of 3D convolution limits the learning capacity
    of the 3D networks.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管3D CNN看起来是建模视频的自然算法，但它们也有一些缺点。首先，由于增加了时间维度，它们需要大量的参数，这导致使用较浅的架构。在[[12](#bib.bib12)]中，介绍了一种新模型，该模型使用了以预训练的Inception-V1为骨干的3D
    CNN。为了从预训练的ImageNet模型[[29](#bib.bib29)]中进行引导，2D内核的权重沿时间维度重复。然而，3D卷积的固定几何结构限制了3D网络的学习能力。
- en: Second, chunk-level features learn temporal abstraction of high-level semantics
    directly from videos, however they are not suited for specific tasks that require
    granularity in time. In some applications, one may need to precisely predict dynamics
    in each frame. In this case, chunk level features loose granularity and cannot
    perform well. For instance, the temporal length of an input video is decreased
    by a factor of 8 in layers from conv1a to conv5b in C3D architecture [[151](#bib.bib151)].
    This conforms that local information are lost passing multiple convolutions. To
    address this issue, in [[137](#bib.bib137)], a convolution and deconvolution approach
    is proposed which downsamples and upsamples in space and time, simultaneously.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，块级特征直接从视频中学习高级语义的时间抽象，但它们不适合需要时间细粒度的特定任务。在一些应用中，可能需要精确预测每一帧的动态。在这种情况下，块级特征会失去细粒度，表现不佳。例如，在C3D架构中，从conv1a到conv5b的层中，输入视频的时间长度减少了8倍[[151](#bib.bib151)]。这表明局部信息在经过多次卷积后丢失。为了解决这个问题，在[[137](#bib.bib137)]中，提出了一种卷积和反卷积的方法，该方法在空间和时间上同时进行下采样和上采样。
- en: Last but not least, while 3D CNNs are well suited for capturing global coarse
    motions, they are limited in modeling finer temporal relations in a local spatio-temporal
    window. To address this, people use attention mechanism to exploit the temporal
    discriminative information in a chunk of frames.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，虽然3D CNN非常适合捕捉全局粗糙运动，但在建模局部时空窗口中的细致时间关系时，它们有局限性。为了解决这个问题，人们使用注意力机制来利用块中的时间区分信息。
- en: Attention Mechanism. Chunk-level features usually learn the temporal domain
    by equally treating the consecutive frames in a chunk, while different frames
    might convey different contributions to the related task. Similar to the frame-level
    features, there are several works that explore temporal attention in chunk-level
    features [[78](#bib.bib78), [69](#bib.bib69)]. In [[78](#bib.bib78)], attention
    is learned at channel level by modeling the differences among the temporal channels
    in 3D CNNs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制。块级特征通常通过平等对待块中的连续帧来学习时间域，而不同的帧可能对相关任务有不同的贡献。类似于帧级特征，有几项研究探讨了块级特征中的时间注意力[[78](#bib.bib78),
    [69](#bib.bib69)]。在[[78](#bib.bib78)]中，通过建模3D CNN中的时间通道差异，在通道级别学习了注意力。
- en: '*Summary.* Chunk level features are suitable for capturing structural co-occurrence.
    They connect both spatial and temporal domains and are suitable for learning dynamics
    that differ in the order of their micro-movements. However, compared with frame-level
    features they are less suitable for tasks that require fine granularity in time.
    As shown in table [6](#S2.T6 "Table 6 ‣ 2.4 Chunk-level features ‣ 2 Classification
    of Deep Video Features ‣ Deep Video Representation Learning: a Survey"), 3D CNNs
    can capture coarse level temporal dependencies in one signature. But they require
    a large number of parameters and cannot encode fine details in a local window.
    Attention is used to exploit informative features in both fine and coarse level
    and handle temporal scale variance. However, it adds extra weights to the model
    and is computationally expensive.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*总结。* 块级特征适合捕捉结构性共现。它们连接了空间和时间域，适合学习在其微观运动顺序中有所不同的动态。然而，与帧级特征相比，它们不适合需要时间细粒度的任务。如表[6](#S2.T6
    "Table 6 ‣ 2.4 Chunk-level features ‣ 2 Classification of Deep Video Features
    ‣ Deep Video Representation Learning: a Survey")所示，3D CNN可以在一个签名中捕捉粗粒度的时间依赖性。但它们需要大量参数，无法在局部窗口中编码细节。注意力机制用于利用细粒度和粗粒度中的信息特征，并处理时间尺度的变化。然而，它会增加模型的额外权重，并且计算开销较大。'
- en: 3 Applying Deep Features in Video Analysis Tasks
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 应用深度特征于视频分析任务
- en: After discussing different feature modeling strategies, we compare their usages
    on different applications. We use two applications, namely, *action recognition*
    and *video object segmentation*, to analyze these features’ behaviors under different
    circumstances. *Action recognition* aims to recognize specific actions happened
    in a video and output one (or several, if there are multiple actions) global labels.
    *Video object segmentation* aims to identify and segment in pixel-level the objects
    of interest from background in every frame of a video.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了不同的特征建模策略后，我们比较了它们在不同应用中的使用情况。我们使用了两个应用，即 *动作识别* 和 *视频目标分割*，来分析这些特征在不同情况下的表现。*动作识别*
    旨在识别视频中发生的特定动作，并输出一个（或多个，如果有多个动作）全局标签。*视频目标分割* 旨在在视频的每一帧中从背景中识别并分割出感兴趣的对象，达到像素级别。
- en: In both of these two applications, free-form deformations of objects are common
    in both spatial and temporal dimensions. And sometimes, to achieve better real-timer
    efficiency, temporal (or spatial) sampling is intentionally made sparse. Therefore,
    extracting reliable and powerful features plays a critical role and often directly
    dictates the final performance.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个应用中，对象在空间和时间维度上常常出现自由形变。而有时，为了提高实时效率，时间（或空间）采样会故意变得稀疏。因此，提取可靠且强大的特征至关重要，并且常常直接决定最终性能。
- en: 3.1 Action Recognition
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 动作识别
- en: 'A key challenge in action recognition is how to learn a feature that captures
    the relevant spatial and motion cues with a descriptive yet compact representation.
    In the spatial dimension, some studies adopted dense features while some others
    used sparse features. Likewise, in the temporal dimension, some adopted frame-level
    features, while some others used chunk-level features. There is a lack of study
    that discusses the pros and cons of adopting these features under different circumstances.
    Hence, here we discuss and analyze each type of features, their challenges, limitations,
    and possible solutions as shown in table [7](#S3.T7 "Table 7 ‣ 3.1 Action Recognition
    ‣ 3 Applying Deep Features in Video Analysis Tasks ‣ Deep Video Representation
    Learning: a Survey").'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在动作识别中的一个关键挑战是如何学习一个特征，以便用描述性且紧凑的表示捕捉相关的空间和运动线索。在空间维度上，一些研究采用了密集特征，而另一些则使用了稀疏特征。同样，在时间维度上，一些采用了帧级特征，而另一些使用了块级特征。缺乏研究讨论在不同情况下采用这些特征的优缺点。因此，我们在这里讨论并分析每种特征、它们的挑战、局限性和可能的解决方案，如表
    [7](#S3.T7 "表 7 ‣ 3.1 动作识别 ‣ 3 应用深度特征于视频分析任务 ‣ 深度视频表示学习：综述") 所示。
- en: '![Refer to caption](img/2647fcd72f55b86a9d433053d14ab32b.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2647fcd72f55b86a9d433053d14ab32b.png)'
- en: 'Figure 2: Dense (RGB frames) and sparse (skeleton keypoints) features in Action
    Recognition. Dense features may include background information; while sparse features
    encode mainly essential object structure. The usefulness of background varies:
    it can either distract (e.g., in dancing images) or assist (e.g., activities in
    the right two columns) recognition. RGB images in the upper left four columns
    are from [[123](#bib.bib123)]; we put them in a sequence of frames. Skeleton keypoints
    in the lower left four columns are from [[111](#bib.bib111)]. Images in the right
    two columns are from UCF101 dataset [[143](#bib.bib143)].'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：动作识别中的密集（RGB帧）和稀疏（骨架关键点）特征。密集特征可能包含背景信息；而稀疏特征主要编码对象的基本结构。背景的有用性各不相同：它可以分散注意力（例如，在舞蹈图像中）或协助（例如，在右边两列的活动中）识别。左上四列的RGB图像来自
    [[123](#bib.bib123)]；我们将它们按帧顺序排列。左下四列的骨架关键点图像来自 [[111](#bib.bib111)]。右两列的图像来自
    UCF101 数据集 [[143](#bib.bib143)]。
- en: 'Table 7: Pros and cons of spatial and temporal features in action recognition.
    There are some abbreviations in the table: ap., info., bg, rep. means appearance,
    information, background, and representation respectively.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：动作识别中空间特征和时间特征的优缺点。表中有一些缩写：ap.、info.、bg、rep. 分别代表外观、信息、背景和表示。
- en: '| Features | Pros | Cons | Solution | References |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 优点 | 缺点 | 解决方案 | 参考文献 |'
- en: '| Dense | contains ap. info. | bg noise/redundant info. | additional info.
    | [[166](#bib.bib166), [201](#bib.bib201)] |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 密集 | 包含外观信息 | 背景噪声/冗余信息 | 附加信息 | [[166](#bib.bib166), [201](#bib.bib201)]
    |'
- en: '|  | attention |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | 注意力 |  |'
- en: '| rep. bias | calibrated data | [[85](#bib.bib85), [42](#bib.bib42)] |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 表示偏差 | 校准数据 | [[85](#bib.bib85), [42](#bib.bib42)] |'
- en: '|  | attention |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | 注意力 |  |'
- en: '| Sparse | robust to view/bg change | low reliability | additional info. |
    [[11](#bib.bib11), [33](#bib.bib33)] |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | 对视角/背景变化稳健 | 可靠性低 | 附加信息 | [[11](#bib.bib11), [33](#bib.bib33)] |'
- en: '| low computation cost | lack of scalibility | heatmap rep. | [[24](#bib.bib24),
    [185](#bib.bib185)] |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 低计算成本 | 扩展性不足 | 热力图表示 | [[24](#bib.bib24), [185](#bib.bib185)] |'
- en: '| Frame | low computation cost | co-occurrence rep. | message passing | [[189](#bib.bib189),
    [138](#bib.bib138)] |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 帧 | 低计算成本 | 共现表示 | 消息传递 | [[189](#bib.bib189), [138](#bib.bib138)] |'
- en: '| Chunk | co-occurrence rep. | low computation cost | disentangle kernels |
    [[152](#bib.bib152), [162](#bib.bib162)] |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 块 | 共现表示 | 低计算成本 | 解耦内核 | [[152](#bib.bib152), [162](#bib.bib162)] |'
- en: '| fix-length chunk | multi scale kernels | [[26](#bib.bib26), [58](#bib.bib58)]
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 固定长度块 | 多尺度内核 | [[26](#bib.bib26), [58](#bib.bib58)] |'
- en: Dense Features. Due to the adaptability and availability of RGB video frames,
    such dense pixel-level representations are widely adopted for action recognition [[154](#bib.bib154),
    [38](#bib.bib38), [176](#bib.bib176)].
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 密集特征。由于 RGB 视频帧的适应性和可用性，这种密集的像素级表示在动作识别中被广泛采用 [[154](#bib.bib154), [38](#bib.bib38),
    [176](#bib.bib176)]。
- en: '*Pros.* Dense features contain appearance information which is useful in recognizing
    actions in different scenes. As CNNs have shown their strong ability in capturing
    dense spatial features, majority of studies use either 2D or 3D CNNs to extract
    spatial semantics in video frames. Thanks to CNNs, modeling Dense features are
    straightforward compared to sparse features, however they have their task-related
    limitations.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*优点*。密集特征包含了在不同场景中识别动作所需的外观信息。由于 CNN 在捕捉密集空间特征方面表现出了强大的能力，大多数研究使用二维或三维 CNN
    从视频帧中提取空间语义。得益于 CNN，与稀疏特征相比，建模密集特征更加直观，但它们也存在与任务相关的限制。'
- en: '*Cons.* There are several challenges in using Dense features. First, they may
    contain background noise and redundant information which undermines robustness
    of action representation learning. Another limitation of dense features is “representation
    bias” which refers to recognizing actions based on object/background detection.
    The network may predict correct results based on scene context rather than human
    action. Some actions might be easier to be predicted using the background and
    context, like a ‘basketball shoot’ vs a ‘throw’; some others might require paying
    close attention to objects being interacted by the human, like in the case of
    ‘drinking from mug’ vs ‘drinking from water bottle’ as shown in Fig. [2](#S3.F2
    "Figure 2 ‣ 3.1 Action Recognition ‣ 3 Applying Deep Features in Video Analysis
    Tasks ‣ Deep Video Representation Learning: a Survey"). It is noted to mention
    that representation bias is different from background noise. In background noise,
    the representation for each class of action differs with the change of the scene,
    while representation bias means getting help from the discriminative objects in
    the scene to recognize the action. While some studies in action recognition consider
    representation bias undesired [[23](#bib.bib23), [85](#bib.bib85)], it may be
    useful in some scenarios or similar tasks [[42](#bib.bib42)]. The reason is that
    modeling human actions often requires understanding the people and objects around
    them. For example, recognizing “listening to others” is not possible unless the
    model knows about the existence of another person in the scene saying something.
    The main concern with representation bias is that if the dataset is not generalized
    enough, it can undermine the performance of the model.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*缺点*。使用密集特征存在一些挑战。首先，它们可能包含干扰动作表示学习的背景噪声和冗余信息。密集特征的另一个局限性是“表示偏差”，即基于对象/背景检测来识别动作。网络可能根据场景上下文而不是人类动作来预测正确结果。某些动作可能更容易通过背景和上下文来预测，如“投篮”与“投掷”之间的区别；另一些动作可能需要密切关注人类正在与之互动的对象，如图
    [2](#S3.F2 "Figure 2 ‣ 3.1 Action Recognition ‣ 3 Applying Deep Features in Video
    Analysis Tasks ‣ Deep Video Representation Learning: a Survey") 中所示的“从杯子喝水”与“从水瓶喝水”。需要注意的是，表示偏差与背景噪声是不同的。在背景噪声中，动作每个类别的表示会随着场景的变化而变化，而表示偏差意味着从场景中的区分性对象获取帮助来识别动作。虽然在动作识别的一些研究中，表示偏差被认为是不受欢迎的
    [[23](#bib.bib23), [85](#bib.bib85)], 但在某些场景或类似任务中它可能是有用的 [[42](#bib.bib42)].
    原因在于，建模人类动作通常需要理解周围的人和物体。例如，除非模型知道场景中存在另一个人在说话，否则无法识别“听别人讲话”。表示偏差的主要问题是，如果数据集的泛化能力不足，它可能会削弱模型的性能。'
- en: '*Solutions.* To alleviate the background noise and redundant, several researches
    have augmented additional visual information to guide network from distraction.
    Some approaches adopted depth information to overcome the illumination and viewpoint
    variations [[126](#bib.bib126), [166](#bib.bib166), [165](#bib.bib165)], while
    others leverage skeleton data in the form of local attention to assist network
    in capturing the most representative body postures [[201](#bib.bib201), [110](#bib.bib110)].'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案。* 为了缓解背景噪声和冗余，几项研究增加了额外的视觉信息以引导网络避免干扰。一些方法采用深度信息来克服光照和视角变化[[126](#bib.bib126),
    [166](#bib.bib166), [165](#bib.bib165)]，而其他方法则利用局部注意力形式的骨架数据来协助网络捕捉最具代表性的身体姿态[[201](#bib.bib201),
    [110](#bib.bib110)]。'
- en: Solutions to overcome representation bias include collecting well-calibrated
    datasets [[85](#bib.bib85)], or using an attention mechanism to help the model
    focus on distinguishable parts of action [[132](#bib.bib132)]. Attention networks
    add a dimension of interpretability by capturing where the network is focusing
    when modeling actions. In [[132](#bib.bib132)], the CNN produces a feature cube
    for each video input and predicts a softmax over locations and the label classes
    which determines the probability with which the model believes the corresponding
    region in the input frame is important. In [[42](#bib.bib42)], attention maps
    are produced to focus computation on specific parts of the input. The weighted
    attention pooling layer is plugged in as a replacement for a pooling operation
    in a fully convolutional network. In [[202](#bib.bib202)], a three-stream architecture
    is proposed which includes two attention streams and a global pooling stream.
    A shared ResNet is used to extract spatial features for all three streams. Each
    attention layer employs a fusion layer to combine global and local information
    and produces composite features. Furthermore, global-attention regularization
    is proposed to guide two attention streams to better model dynamics of composite
    features with the reference to the global information.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 克服表示偏差的解决方案包括收集经过良好校准的数据集[[85](#bib.bib85)]，或使用注意力机制帮助模型专注于动作的可区分部分[[132](#bib.bib132)]。注意力网络通过捕捉网络在建模动作时的关注点，增加了解释性。在[[132](#bib.bib132)]中，CNN为每个视频输入生成一个特征立方体，并预测一个软最大值，以确定模型认为输入帧中相应区域重要的概率。在[[42](#bib.bib42)]中，生成注意力图以将计算集中在输入的特定部分。加权注意力池化层被插入作为全卷积网络中池化操作的替代。在[[202](#bib.bib202)]中，提出了一个三流架构，包括两个注意力流和一个全局池化流。共享的ResNet用于提取所有三个流的空间特征。每个注意力层采用融合层来结合全局和局部信息，并生成复合特征。此外，提出了全局注意力正则化，以指导两个注意力流更好地建模复合特征的动态，并参考全局信息。
- en: Sparse Features. Sparse features, particularly skeletons, are very popular in
    action recognition due to their action-focusing nature and compactness. Several
    studies use human skeleton information as a sequence of joint coordinate lists
    [[187](#bib.bib187), [196](#bib.bib196), [79](#bib.bib79)] where the coordinates
    are extracted by pose estimators.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏特征。稀疏特征，尤其是骨架特征，在动作识别中因其动作聚焦特性和紧凑性而非常受欢迎。几项研究使用人体骨架信息作为关节坐标列表的序列[[187](#bib.bib187),
    [196](#bib.bib196), [79](#bib.bib79)]，这些坐标由姿态估计器提取。
- en: 'While using CNN networks to process RGB frames is straightforward, in skeleton-based
    action modeling, network is faced with the challenge of arranging skeleton features.
    Earlier methods [[196](#bib.bib196), [79](#bib.bib79)] simply use the keypoint
    coordinates to generate a sequence of feature vectors. The issue with this method
    is that it focuses on modeling the information in the time domain and doesn’t
    explore the spatial relations between body joints. Other approaches arranged skeleton
    data as a pseudo-image [[68](#bib.bib68), [70](#bib.bib70), [77](#bib.bib77)].
    However, recent works have shown that graph networks can efficiently model non-Euclidean
    data like human skeletons. Performance results from table [8](#S3.T8 "Table 8
    ‣ 3.1 Action Recognition ‣ 3 Applying Deep Features in Video Analysis Tasks ‣
    Deep Video Representation Learning: a Survey") shows the superior performance
    of arranging skeleton data in a graph structure.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然使用 CNN 网络处理 RGB 帧是直接的，但在基于骨架的动作建模中，网络面临着安排骨架特征的挑战。早期的方法 [[196](#bib.bib196),
    [79](#bib.bib79)] 仅使用关键点坐标生成特征向量序列。该方法的问题在于它集中于建模时间域中的信息，而未探索身体关节之间的空间关系。其他方法将骨架数据安排为伪图像
    [[68](#bib.bib68), [70](#bib.bib70), [77](#bib.bib77)]。然而，最近的研究表明，图网络可以有效地建模像人体骨架这样的非欧几里得数据。表
    [8](#S3.T8 "Table 8 ‣ 3.1 Action Recognition ‣ 3 Applying Deep Features in Video
    Analysis Tasks ‣ Deep Video Representation Learning: a Survey") 的性能结果展示了将骨架数据安排为图结构的优越性能。'
- en: 'Table 8: Comparison of skeleton-based action recognition performance for NTU
    RGB-D [[131](#bib.bib131)] dataset with different feature arrangements. mAP refers
    to mean average precision.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：NTU RGB-D [[131](#bib.bib131)] 数据集中基于骨架的动作识别性能比较，针对不同特征安排。mAP 指的是平均精度。
- en: '| model | [[32](#bib.bib32)] | [[131](#bib.bib131)] | [[99](#bib.bib99)] |
    [[70](#bib.bib70)] | [[68](#bib.bib68)] | [[187](#bib.bib187)] | [[169](#bib.bib169)]
    | [[80](#bib.bib80)] |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | [[32](#bib.bib32)] | [[131](#bib.bib131)] | [[99](#bib.bib99)] | [[70](#bib.bib70)]
    | [[68](#bib.bib68)] | [[187](#bib.bib187)] | [[169](#bib.bib169)] | [[80](#bib.bib80)]
    |'
- en: '| arrangement | vector | vector | vector | pseudo-image | pseudo-image | graph
    | graph | graph |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 安排 | 向量 | 向量 | 向量 | 伪图像 | 伪图像 | 图 | 图 | 图 |'
- en: '| mAP | 59.1% | 62.9% | 69.2% | 74.3% | 79.6% | 81.5% | 84.2% | 86.8% |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| mAP | 59.1% | 62.9% | 69.2% | 74.3% | 79.6% | 81.5% | 84.2% | 86.8% |'
- en: With introduction of ST-GCN in [[187](#bib.bib187)], spatio-temporal graph convolutions
    became a research hotspot. The core of this approach is to integrate temporal
    module in the spatial GCN. Several variants of ST-GCN are proposed [[21](#bib.bib21),
    [170](#bib.bib170), [20](#bib.bib20)] to improve the network capacity and computation
    consumption of the network. However, the main limitation of ST-GCN is that it
    ignores the semantic connections among intra-frame joints by using a fixed graph
    structure. Recognizing action lies in looking beyond the local joint connectivity
    as learning not only happens in spatially connected joints, but also in the potential
    dependence of disconnected joints. For example, in “walking” there is a high correlation
    between arms and legs while they are spatially apart. This achieves by extracting
    multi-scale features and long-range temporal dependencies, as joints that are
    spatially apart can also have strong correlations [[106](#bib.bib106)]. In this
    regard, some techniques have been adopted to enhance the flexibility of GCNs.
    Attempts from using adaptive learning graph structure in [[133](#bib.bib133),
    [136](#bib.bib136)] to designing a graph-based search space to explore spatio-temporal
    connections [[122](#bib.bib122)] has been made. In [[27](#bib.bib27)], dilated
    convolutions [[192](#bib.bib192)] are adopted to increase receptive field size
    and capture multi-scale context without increasing model complexity. In [[106](#bib.bib106)]
    this issue is addressed by performing graph convolutions with higher-order polynomials
    of the skeleton adjacency matrix which increases the receptive field of graph
    convolutions. Attention mechanisms are also adopted to improve the ability of
    extracting high-level joints. In [[136](#bib.bib136)] a spatio-temporal channel
    attention module is embedded in each layer of the GCN, which enables model to
    focus on the discriminative details of joints, frames and channels in action recognition.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 随着ST-GCN在[[187](#bib.bib187)]中的引入，时空图卷积成为了研究热点。这种方法的核心在于将时间模块整合到空间GCN中。提出了几种ST-GCN的变体[[21](#bib.bib21),
    [170](#bib.bib170), [20](#bib.bib20)]，以提高网络容量和计算消耗。然而，ST-GCN的主要限制在于通过使用固定图结构忽略了帧内关节之间的语义连接。动作识别在于超越局部关节连接，因为学习不仅发生在空间连接的关节上，还涉及到断开关节的潜在依赖。例如，在“行走”中，尽管手臂和腿部在空间上分开，但它们之间有很高的相关性。这通过提取多尺度特征和长期时间依赖性实现，因为空间上分开的关节也可以有强烈的相关性[[106](#bib.bib106)]。在这方面，一些技术已被采用于增强GCNs的灵活性。尝试使用自适应学习图结构[[133](#bib.bib133),
    [136](#bib.bib136)]以及设计基于图的搜索空间来探索时空连接[[122](#bib.bib122)]。在[[27](#bib.bib27)]中，采用了膨胀卷积[[192](#bib.bib192)]以增加感受野大小，并在不增加模型复杂性的情况下捕获多尺度上下文。在[[106](#bib.bib106)]中，通过执行具有更高阶多项式的骨架邻接矩阵的图卷积来解决这一问题，从而增加图卷积的感受野。注意力机制也被采用于提高提取高级关节的能力。在[[136](#bib.bib136)]中，每层GCN中嵌入了时空通道注意力模块，使模型能够专注于动作识别中的关节、帧和通道的判别细节。
- en: '*Pros.* When sparse features are used, since only pose information is included,
    they contain high-level semantic information in a small amount of data and are
    more robust in dynamic circumstances [[135](#bib.bib135), [59](#bib.bib59)]. As
    skeleton data does not contain color information, it is not affected by the limitations
    of RGB frames [[138](#bib.bib138)], and can provide a stable low-frequency representation
    of human actions.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*优点。* 当使用稀疏特征时，由于仅包含姿势信息，这些特征在少量数据中包含了高级语义信息，并且在动态情况下更为鲁棒[[135](#bib.bib135),
    [59](#bib.bib59)]。由于骨架数据不包含颜色信息，因此不受RGB帧限制的影响[[138](#bib.bib138)]，能够提供稳定的低频人类动作表示。'
- en: '*Cons.* There are some challenges in modeling sparse feature representation.
    A first limitation is the “reliability” which means the recognition ability of
    sparse features is mainly affected by the distribution shift of coordinates. As
    joint coordinates are produced by a pose estimator, applying a different pose
    estimation algorithm may lead to a small perturbation of coordinates which causes
    different predictions [[206](#bib.bib206)]. Also, local subtle motion patterns
    are lost in the process of pose estimation. Therefore, sparse nature of skeleton
    sequences is sometimes not informative enough for describing subtle actions like
    “reading”, “writing”, and “shaking head”.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*缺点。* 在建模稀疏特征表示时存在一些挑战。第一个限制是“可靠性”，这意味着稀疏特征的识别能力主要受坐标分布变化的影响。由于关节坐标是由姿态估计器产生的，应用不同的姿态估计算法可能导致坐标的微小扰动，从而造成不同的预测
    [[206](#bib.bib206)]。此外，姿态估计过程中会丢失局部微妙的运动模式。因此，骨架序列的稀疏性有时不足以描述诸如“阅读”、“书写”和“摇头”等细微动作。'
- en: Another challenge is the lack of scalability of sparse-features. As sparse features
    are defined for every human separately. For example, each joint of human skeleton
    is defined as a node per person, the complexity of network linearly increases
    with increasing the number of persons, which limits its applicability in multi-person
    or group activity recognition.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是稀疏特征的可扩展性不足。由于稀疏特征是针对每个人单独定义的。例如，每个人体骨架的每个关节被定义为一个节点，网络的复杂性随着人数的增加而线性增长，这限制了它在多人人物或群体活动识别中的应用。
- en: '*Solutions.* To overcome reliability issues, many approaches take advantage
    of multi-modal visual resources including RGB frames, depth maps and joint heatmaps
    to compensate for the lack of information in local and global domain [[11](#bib.bib11),
    [33](#bib.bib33)]. People use other forms of representations using heatmap volumes
    to show skeletons to alleviate the scalibility issue of sparse features [[24](#bib.bib24),
    [185](#bib.bib185)].'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案。* 为了克服可靠性问题，许多方法利用多模态视觉资源，包括 RGB 帧、深度图和关节热图，以弥补局部和全局领域中信息的缺乏 [[11](#bib.bib11),
    [33](#bib.bib33)]。人们使用其他形式的表示方法，例如热图体积来展示骨架，以缓解稀疏特征的可扩展性问题 [[24](#bib.bib24),
    [185](#bib.bib185)]。'
- en: 'Frame-level Features. Some approaches extract the motion cues between adjacent
    frames and learn frame-level temporal dependencies in a sequential signature.
    In action recognition, modeling both short-range and long-range motions is sometimes
    required. (1) To this end, some earlier methods firstly extract hand-crafted optical
    flow [[139](#bib.bib139), [161](#bib.bib161)], then use a 2D CNN-based two-stream
    framework to process optical flow and RGB frames in each stream separately. These
    lines of works have several drawbacks: First, computing optical flow is time-consuming
    and storage demanding. Furthermore, the training of spatial and temporal features
    is separated, and the fusion of two streams is performed only at the late layers.
    Several following works had improved this framework by using different mid-level
    links to fuse the features of two separated streams [[38](#bib.bib38), [37](#bib.bib37)].
    However, these methods still require additional time and storage costs for computation
    of optical flow. (2) Another line of work aggregates temporal information by sequence
    learning [[31](#bib.bib31), [89](#bib.bib89)]. The majority of these methods treat
    each frame, or point in time, with equal weight, but not all parts of the video
    are equally important and thus it is also key that we develop feature extraction
    methods that can determine where to extract features from. First, non-uniformly
    extracting features efficiently from only informative temporal episodes is challenging
    as it is required to look at the whole video to determine which parts are informative.
    Some recent work [[74](#bib.bib74), [174](#bib.bib174), [113](#bib.bib113)] have
    proved that a recognition system can benefit from selecting the informative frames
    rather than simply taking the uniformly sampled frames as inputs. However, these
    systems treat the frame selection and feature extraction as two separate stages
    and thus the frame selection can not benefit from the later feature extraction
    thus reducing the descriptive power of the network and adding redundancy in the
    two stages. In order to tackle this challenge, in [[84](#bib.bib84)], a two-branch
    architecture is suggested that maintains both uniformly frame-level features and
    non-uniformly chunk-level features in an end to end manner. To produce non-uniformly
    features, a temporal map is used that non-uniformly projects temporal instances
    to a smaller subset by using self-attention-like module. This component is proposed
    to only sample the most informative frames across time. (3) Another recent line
    of work adopts video transformers that apply self-attention to spatial-temporal
    features. Representative networks include TimeSformer [[6](#bib.bib6)], ViViT
    [[1](#bib.bib1)], Mformer [[121](#bib.bib121)] and MViT [[36](#bib.bib36)]. Combining
    2D backbones and Transformers, VTN [[115](#bib.bib115)] and CARL [[14](#bib.bib14)]
    can efficiently process long video sequences. However, these networks are designed
    to process a batch of frames at once on video tasks which requires large computing
    memory. In [[190](#bib.bib190)], a recursive mechanism is deployed to process
    the videos frame by frame and consume less GPU memory.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 帧级特征。某些方法提取相邻帧之间的运动线索，并在顺序签名中学习帧级时间依赖关系。在动作识别中，有时需要对短期和长期运动进行建模。(1) 为此，一些早期的方法首先提取手工制作的光流[[139](#bib.bib139),
    [161](#bib.bib161)]，然后使用基于2D CNN的双流框架分别处理光流和RGB帧。这些方法存在几个缺点：首先，计算光流是耗时且存储需求大的。此外，空间和时间特征的训练是分开的，两个流的融合仅在后期层进行。随后的一些工作通过使用不同的中间链接来融合两个分离流的特征[[38](#bib.bib38),
    [37](#bib.bib37)]，改善了这一框架。然而，这些方法仍然需要额外的时间和存储成本来计算光流。(2) 另一类工作通过序列学习聚合时间信息[[31](#bib.bib31),
    [89](#bib.bib89)]。这些方法中的大多数将每一帧或时间点视为同等重要，但视频的所有部分并不等同重要，因此开发能够确定从哪里提取特征的方法也很关键。首先，从信息丰富的时间片段中不均匀地提取特征是具有挑战性的，因为需要查看整个视频以确定哪些部分是信息丰富的。一些近期工作[[74](#bib.bib74),
    [174](#bib.bib174), [113](#bib.bib113)]证明，通过选择信息丰富的帧而不是简单地取均匀采样的帧作为输入，识别系统可以受益。然而，这些系统将帧选择和特征提取视为两个独立的阶段，因此帧选择无法从后续特征提取中受益，从而降低了网络的描述能力，并在两个阶段中增加了冗余。为了解决这一挑战，在[[84](#bib.bib84)]中，建议了一种双分支架构，该架构以端到端的方式同时保持均匀的帧级特征和不均匀的块级特征。为了生成不均匀特征，使用了一个时间图，通过自注意力模块不均匀地将时间实例投影到一个更小的子集上。这个组件被提出仅采样时间上的最信息丰富的帧。(3)
    另一类近期工作采用视频变换器，将自注意力应用于时空特征。代表性的网络包括TimeSformer[[6](#bib.bib6)]、ViViT[[1](#bib.bib1)]、Mformer[[121](#bib.bib121)]和MViT[[36](#bib.bib36)]。结合2D骨干网络和变换器，VTN[[115](#bib.bib115)]和CARL[[14](#bib.bib14)]可以高效处理长视频序列。然而，这些网络被设计为一次处理一批帧的视频任务，这需要大量计算内存。在[[190](#bib.bib190)]中，采用递归机制逐帧处理视频，从而消耗更少的GPU内存。
- en: '*Pros.* Generally, standard architectures of frame-level features have lower
    computational costs compared to chunk-level features. Also, frame-level features
    are more concise when aggregating local frame-level features are required for
    a global compact representations.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*优点。* 通常，帧级特征的标准架构相比于块级特征具有较低的计算成本。此外，当需要聚合局部帧级特征以获得全局紧凑表示时，帧级特征更为简洁。'
- en: '*Cons.* The main challenge in extracting frame-level features is “co-occurrence
    representation” learning which in action recognition refers to when a model needs
    to learn a set of human actions with a specific set of spatial features at certain
    times. For example, in the action of ”touching back”, model needs to focus first
    on the hand and then pay attention to the back [[83](#bib.bib83)]. In many of
    existing approaches, a temporal module and a spatial module are designed separately.
    Their typical approach is to use a convolutional network to extract spatial relations
    in each frame, then use a 1D convolution [[134](#bib.bib134), [133](#bib.bib133),
    [80](#bib.bib80)] or LSTM [[82](#bib.bib82), [76](#bib.bib76)] to model temporal
    dependencies. However, such decoupled design restricts the direct information
    flow across space-time for capturing complex regional spatial-temporal joint dependencies.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*缺点。* 提取帧级特征的主要挑战是“共现表示”学习，即在动作识别中指的是当模型需要在特定时间学习一组具有特定空间特征的人体动作。例如，在“摸背”动作中，模型需要首先关注手部，然后关注背部[[83](#bib.bib83)]。在许多现有方法中，时序模块和空间模块是分别设计的。它们的典型方法是使用卷积网络来提取每帧中的空间关系，然后使用1D卷积[[134](#bib.bib134)、[133](#bib.bib133)、[80](#bib.bib80)]或LSTM[[82](#bib.bib82)、[76](#bib.bib76)]来建模时序依赖。然而，这种解耦设计限制了跨时空的直接信息流，从而捕捉复杂的区域时空关节依赖关系。'
- en: '*Solutions.* To help model better learn co-occurrence representation, message
    passing and cross connection strategy is adopted to avoid stacking multiple spatio-temporal
    modules and transfer information. In [[189](#bib.bib189)], the feedback connection
    was integrated into GCN to transfer the high-level semantic features to the low-level
    layer, and gradually transmitted the temporal information to build the global
    spatio-temporal action recognition model. In [[150](#bib.bib150)], attentions
    provided from temporal-stream is used to help spatial stream by cross-link layers.
    In [[83](#bib.bib83)], a coordinate system conversion and spatio-temporal-unit
    feature enhancement is proposed to perform co-occurrence learning. In [[138](#bib.bib138)],
    each joint coordinate is transformed into a spatial feature with a linear layer.
    Then data is augmented with the spatial feature and the difference between spatial
    features between consecutive frames. Then a shared LSTM and three layers of graph
    convolutional LSTM are applied to model co-occurrence representation learning
    between joints.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案。* 为了帮助模型更好地学习共现表示，采用了消息传递和交叉连接策略，以避免堆叠多个时空模块并传递信息。在[[189](#bib.bib189)]中，反馈连接被整合到GCN中，将高级语义特征传递到低级层，并逐步传递时序信息以建立全球时空动作识别模型。在[[150](#bib.bib150)]中，来自时序流的注意力被用来通过交叉连接层帮助空间流。在[[83](#bib.bib83)]中，提出了坐标系统转换和时空单元特征增强，以执行共现学习。在[[138](#bib.bib138)]中，每个关节坐标被转换为具有线性层的空间特征。然后，用空间特征和连续帧之间的空间特征差异来增强数据。接着，应用共享LSTM和三层图卷积LSTM来建模关节之间的共现表示学习。'
- en: Chunk-level Features. Another type of approaches is to extract chunk-level temporal
    features in a global signature. In this case, the chunk is defined as multi-dimensional
    time series of dense/sparse features. Thanks to the ability of 3D CNNs to implicitly
    model motion information along with the semantic features, this line of works
    has seen significant advances in recent years. The first work in this line was
    C3D [[151](#bib.bib151)], which proposed using 3D convolutions to jointly model
    the spatial and temporal features in a global signature. To use pre-trained 2D
    CNNs, in [[12](#bib.bib12)], I3D was proposed that inflates the pre-trained 2D
    convolutions to 3D ones.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 块级特征。另一类方法是提取块级时序特征以获得全球签名。在这种情况下，块被定义为多维时间序列的密集/稀疏特征。由于3D CNN能够隐式建模运动信息以及语义特征，因此这一领域近年来取得了显著进展。该领域的首个工作是C3D[[151](#bib.bib151)]，它提出使用3D卷积联合建模空间和时序特征以获得全球签名。为了使用预训练的2D
    CNN，在[[12](#bib.bib12)]中，提出了I3D，通过将预训练的2D卷积膨胀为3D卷积。
- en: '*Pros.* Generally, chunk-level features benefit from the co-occurrence representation
    learning as there is a link between temporal and spatial channels. There networks
    are potentially more effective in learning fine detailed actions such as “sitting”
    and “standing up”.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*优势。* 一般来说，分块级别的特征从时空通道的共现表示学习中受益，因为在“坐”和“站起来”等细节动作之间存在联系。这些网络可能更有效地学习细节动作。'
- en: '*Cons.* While so many attempts have been done in capturing motion using chunk-level
    features, most of approaches often lack specific consideration in the temporal
    dimension. Therefore, designing an effective temporal module of high motion modeling
    power and low computational consumption is still a challenging problem. First,
    the 3D networks require a substantial amount of computation and time. Also, compared
    to 2D kernels, 3D convolutions have to reduce the spatial resolution to decrease
    memory consumption, which may lead to the loss of finer details.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*缺点。* 尽管已经进行了许多尝试来捕捉使用分块级特征的运动，但大多数方法在时序维度上缺乏具体考虑。因此，设计一个有效的时序模块，具有很强的动作建模能力和较低的计算消耗仍然是一个具有挑战性的问题。首先，三维网络需要大量的计算和时间。此外，与二维卷积相比，三维卷积需要降低空间分辨率以减少内存消耗，这可能会导致细节丢失。'
- en: Moreover, temporal features are typically extracted from a fixed-length clip
    instead of a length-adaptive clip which is not suited for different visual tempos.
    Visual tempo defines the speed of an action, meaning how fast and slow an action
    is performed at the temporal scale which in some cases is crucial for recognizing
    actions e.x. walking, jogging and running.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，时间特征通常是从固定长度的片段中提取的，而不是适合不同视觉节奏的长度自适应片段。视觉节奏定义了一个动作的速度，也就是一个动作在时间尺度上的快慢。这在某些情况下对于识别动作(如行走、慢跑和奔跑)至关重要。
- en: '*Solutions.* To decrease the heavy computations of 3D CNNs, some works proposed
    to factorize the 3D convolution with a 2D spatial convolution and a 1D temporal
    convolution [[94](#bib.bib94), [45](#bib.bib45), [152](#bib.bib152)] or a mixed
    up of 2D CNN and 3D CNN [[176](#bib.bib176), [208](#bib.bib208)]. In [[152](#bib.bib152)],
    a group convolution is used to disentangle channel interactions and spatio-temporal
    interactions, or use separated channel groups to encode both spatial and spatio-temporal
    interactions in parallel with 2D and 3D convolution [[109](#bib.bib109)]. While
    all these existing approaches are designed to deal with the curse of dimension,
    there is a lack of data dependent decision to adaptively guide features through
    different routs in the network. In [[162](#bib.bib162)], two temporal difference
    module is proposed which computes multi-scale and bidirectional motion information
    between frames and chunks. In [[146](#bib.bib146)], features are selectively routed
    through temporal dimension and are combined with each other without any computational
    overhead.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案。* 为了减少三维卷积的复杂计算，一些研究提出了将三维卷积分解为二维空间卷积和一维时序卷积的方法[[94](#bib.bib94), [45](#bib.bib45),
    [152](#bib.bib152)]，或者将二维卷积和三维卷积混合使用的方法[[176](#bib.bib176), [208](#bib.bib208)].
    在[[152](#bib.bib152)]中，使用组卷积来解开通道之间的相互作用和时空相互作用，或者使用单独的通道组同时进行二维和三维卷积来编码空间和时空相互作用[[109](#bib.bib109)]。尽管所有这些现有方法都是为了处理维度诅咒，但缺乏数据相关的决策来有针对性地引导特征通过网络中的不同路径。在[[162](#bib.bib162)]中，提出了两个时序差分模块，用于计算帧和分块之间的多尺度和双向运动信息。在[[146](#bib.bib146)]中，特征被有选择地通过时序维度进行路由，并且在没有任何计算开销的情况下与彼此相结合。'
- en: Some people use multi-scale convolutional kernels to cover various visual tempos.
    In [[188](#bib.bib188)], multi-scale convolutional features are incorporated in
    asymmetric 3D convoultions to improve temporal feature learning capacity. In [[58](#bib.bib58)],
    Timeception layer is designed which temporally convolves each chunk using multi-scale
    temporal convolution module to tolerate a variety of temporal extents in a complex
    action. Some people design a level-specific network frame pyramid to handle the
    variance of visual tempos [[39](#bib.bib39), [193](#bib.bib193)]. In [[26](#bib.bib26)],
    a multi-scale transformer is proposed which is built on top of temporal segments
    using 3D convolutions in a token-based architecture to promote multiple temporal
    scales of tokens. Having different scales allows model to capture both fine-grained
    and composite actions across time. In [[153](#bib.bib153)], a direct attention
    mechanism is incorporated in transformers to exploit the direction of attention
    across frames and correct the incorrectly-ordered frames to the right ones and
    provide an accurate prediction.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人使用多尺度卷积核来覆盖各种视觉节奏。在 [[188](#bib.bib188)] 中，将多尺度卷积特征融入非对称 3D 卷积中，以提高时间特征学习能力。在
    [[58](#bib.bib58)] 中，设计了 Timeception 层，该层使用多尺度时间卷积模块对每个块进行时间卷积，以容忍复杂动作中的各种时间跨度。一些人设计了一个级别特定的网络帧金字塔来处理视觉节奏的变化
    [[39](#bib.bib39), [193](#bib.bib193)]。在 [[26](#bib.bib26)] 中，提出了一种多尺度变换器，其建立在使用
    3D 卷积的时间段之上，在基于令牌的架构中促进令牌的多个时间尺度。具有不同尺度使模型能够捕捉到时间上的细粒度和复合动作。在 [[153](#bib.bib153)]
    中，将直接注意力机制融入变换器中，以利用跨帧的注意力方向，纠正错误排序的帧，并提供准确的预测。
- en: 'Table 9: Comparison of different action recognition performance for NTU RGB-D
    [[131](#bib.bib131)] dataset. CS and CV refer to cross subject (various human
    subjects) and cross view (various camera views) split of the dataset.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: NTU RGB-D [[131](#bib.bib131)] 数据集的不同动作识别性能比较。CS 和 CV 指的是数据集的跨主体（不同人类主体）和跨视角（不同摄像机视角）划分。'
- en: '| Model | Spatial | Temporal | CS accuracy | CV accuracy |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 空间 | 时间 | CS 准确率 | CV 准确率 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| [[4](#bib.bib4)] | Dense | Frame | 86.6% | 93.2% |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| [[4](#bib.bib4)] | 密集 | 帧 | 86.6% | 93.2% |'
- en: '| [[207](#bib.bib207)] | Dense | Chunk | 94.3% | 97.2% |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| [[207](#bib.bib207)] | 密集 | 块 | 94.3% | 97.2% |'
- en: '| [[10](#bib.bib10)] | Sparse | Chunk | 76.5% | 84.7% |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| [[10](#bib.bib10)] | 稀疏 | 块 | 76.5% | 84.7% |'
- en: '| [[148](#bib.bib148)] | Sparse | Chunk | 87.5% | 93.2% |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| [[148](#bib.bib148)] | 稀疏 | 块 | 87.5% | 93.2% |'
- en: '| [[28](#bib.bib28)] | Dense + Sparse | Chunk | 91.8% | 94.9% |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| [[28](#bib.bib28)] | 密集 + 稀疏 | 块 | 91.8% | 94.9% |'
- en: '| [[103](#bib.bib103)] | Dense + Sparse | Frame | 91.7% | 95.2% |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| [[103](#bib.bib103)] | 密集 + 稀疏 | 帧 | 91.7% | 95.2% |'
- en: '| [[145](#bib.bib145)] | Dense + Sparse | Frame | 92.2% | - |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| [[145](#bib.bib145)] | 密集 + 稀疏 | 帧 | 92.2% | - |'
- en: '| [[8](#bib.bib8)] | Dense + Sparse | Chunk | 92.5% | 97.4% |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| [[8](#bib.bib8)] | 密集 + 稀疏 | 块 | 92.5% | 97.4% |'
- en: '| [[33](#bib.bib33)] | Dense + Sparse | Chunk | 97.0% | 99.6% |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| [[33](#bib.bib33)] | 密集 + 稀疏 | 块 | 97.0% | 99.6% |'
- en: '| [[9](#bib.bib9)] | Dense + Sparse | Chunk | 96.0% | 98.8% |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| [[9](#bib.bib9)] | 密集 + 稀疏 | 块 | 96.0% | 98.8% |'
- en: 'Summary. We summarized pros and cons of different features and the possible
    solution in table [7](#S3.T7 "Table 7 ‣ 3.1 Action Recognition ‣ 3 Applying Deep
    Features in Video Analysis Tasks ‣ Deep Video Representation Learning: a Survey").
    Dense features have the advantage of using appearance info in action recognition
    while they suffer from background noise and representation bias. Possible solutions
    for background noise and representation bias include augmenting additional information
    to the input and well calibrated dataset and attention mechanism, respectively.
    Sparse features are more robust against background noise and have lower computational
    cost compared to dense features, while they suffer from lack of reliability and
    scalibility. Possible solutions for their drawbacks could be augmenting additional
    information and visual sources. Researches have shown in spatial domain, using
    multi-modal inputs, particularly accompanied with attention mechanism are very
    helpful in understanding human actions, as humans’ brain adopt all visual inputs
    to recognize an action. The quantitative results are shown in table [9](#S3.T9
    "Table 9 ‣ 3.1 Action Recognition ‣ 3 Applying Deep Features in Video Analysis
    Tasks ‣ Deep Video Representation Learning: a Survey") on NTU RGB-D dataset confirm
    that using multi-modal spatial features outperforms single-modal approaches.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '总结。我们在表[7](#S3.T7 "Table 7 ‣ 3.1 Action Recognition ‣ 3 Applying Deep Features
    in Video Analysis Tasks ‣ Deep Video Representation Learning: a Survey")中总结了不同特征的优缺点及可能的解决方案。密集特征在动作识别中利用了外观信息的优势，但它们受到背景噪声和表示偏差的影响。背景噪声和表示偏差的可能解决方案包括在输入中增强附加信息，以及良好校准的数据集和注意机制。稀疏特征比密集特征对背景噪声更具鲁棒性，并且计算成本较低，但它们缺乏可靠性和可扩展性。解决这些缺点的可能方案包括增强附加信息和视觉源。研究表明，在空间域中，使用多模态输入，特别是伴随注意机制，对于理解人类动作非常有帮助，因为人脑会采纳所有视觉输入来识别动作。定量结果显示在表[9](#S3.T9
    "Table 9 ‣ 3.1 Action Recognition ‣ 3 Applying Deep Features in Video Analysis
    Tasks ‣ Deep Video Representation Learning: a Survey")上的NTU RGB-D数据集确认，使用多模态空间特征优于单模态方法。'
- en: 'In temporal domain, using frame-level features has the advantage of lower computational
    cost, compared to chunk-level features. However, decoupling spatial and temporal
    dimensions restricts co-occurrence representation learning in distinguishing complex
    actions. Some studies alleviate this problem by message passing techniques and
    cross-link connections. On the contrary, chunk-level features allow establish
    of connections and links between temporal and spatial dimensions to learn order
    and co-occurrence of micro-actions. However, typically these models take fixed-length
    instead of adaptive-length chunks. Some works address this issue by using frame
    pyramids and multi-scale convolutions. Another drawback of chunk-level features
    is their high computational costs which could be alleviated by disentangling convolutions
    in different layers of network. To conclude, as shown in table [7](#S3.T7 "Table
    7 ‣ 3.1 Action Recognition ‣ 3 Applying Deep Features in Video Analysis Tasks
    ‣ Deep Video Representation Learning: a Survey"), in presence of sparse features,
    using chunk-level approaches outperforms frame-wise methods due to reducing number
    of parameters, increasing depth of network and co-occurrence representation learning.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '在时间域中，与块级特征相比，使用帧级特征的计算成本较低。然而，将空间和时间维度解耦限制了在区分复杂动作时的共现表示学习。一些研究通过消息传递技术和交叉链接连接来缓解这个问题。相反，块级特征允许建立时间和空间维度之间的连接和链接，以学习微动作的顺序和共现。然而，这些模型通常采用固定长度而不是自适应长度的块。一些研究通过使用帧金字塔和多尺度卷积来解决这个问题。块级特征的另一个缺点是计算成本高，这可以通过在网络的不同层中解缠卷积来缓解。总之，如表[7](#S3.T7
    "Table 7 ‣ 3.1 Action Recognition ‣ 3 Applying Deep Features in Video Analysis
    Tasks ‣ Deep Video Representation Learning: a Survey")所示，在稀疏特征存在的情况下，由于减少了参数数量、增加了网络深度和共现表示学习，块级方法优于帧级方法。'
- en: 3.2 Video Object Segmentation
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 视频对象分割
- en: 'Video object segmentation (VOS) is a video processing technique. The goal of
    VOS is to segment pixel-level masks of foreground objects in every frame of a
    given video. VOS has attracted extensive attention these years because it can
    be applied to diverse fields in computer vision. Recent VOS research can be divided
    into two sub-tasks: semi-supervised and unsupervised. The semi-supervised VOS
    aims to re-locate and segment one or more objects that are given in the first
    frame of a video in pixel-level masks. The unsupervised VOS aims to automatically
    segment the object of interest from the background, usually, the most salient
    object(s) will be segmented. Generally, the input to the video object segmentation
    is a sequence of color frames in RGB format. Feature modeling is the first stage
    of these video object segmentation pipelines. In the following, we discuss pros
    and cons of different features in VOS.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 视频目标分割（VOS）是一种视频处理技术。VOS的目标是在给定视频的每一帧中对前景物体进行像素级掩码分割。近年来，VOS受到广泛关注，因为它可以应用于计算机视觉的各种领域。最近的VOS研究可以分为两个子任务：半监督和无监督。半监督VOS旨在对视频第一帧中给定的一个或多个物体进行像素级掩码的重新定位和分割。无监督VOS旨在自动从背景中分割出感兴趣的物体，通常会分割出最显著的物体。通常，视频目标分割的输入是RGB格式的彩色视频帧序列。特征建模是这些视频目标分割管道的第一阶段。接下来，我们讨论不同特征在VOS中的优缺点。
- en: 'Table 10: Pros and Cons of spatial and temporal features in the video object
    segmentation (VOS) task. There are some abbreviations in the table: ap. and info.
    means appearance and information, respectively.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：视频目标分割（VOS）任务中空间和时间特征的优缺点。表中有一些缩写：ap. 和 info. 分别表示外观和信息。
- en: '| Features | Pros | Cons | Solution | References |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 优点 | 缺点 | 解决方案 | 参考文献 |'
- en: '| Dense | contains rich ap. info. | weak in occlusion-handling. | occlusion-aware
    encoders | [[66](#bib.bib66), [67](#bib.bib67)] |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 密集 | 包含丰富的外观信息 | 处理遮挡能力弱 | 遮挡感知编码器 | [[66](#bib.bib66), [67](#bib.bib67)]
    |'
- en: '| Sparse | fast & low comp. | less accuracy | hybrid algorithm | [[16](#bib.bib16),
    [147](#bib.bib147)] |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | 快速且计算成本低 | 精度较低 | 混合算法 | [[16](#bib.bib16), [147](#bib.bib147)] |'
- en: '| Frame | low computation cost | error accumulation | dynamic feature | [[92](#bib.bib92),
    [81](#bib.bib81)] |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 帧 | 低计算成本 | 错误累积 | 动态特征 | [[92](#bib.bib92), [81](#bib.bib81)] |'
- en: '| low latency | past frame management | space management |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 低延迟 | 过去帧管理 | 空间管理 |  |'
- en: '| Chunk | multi-modal modeling | large computation cost | knowledge distillation
    | [[172](#bib.bib172), [171](#bib.bib171)] |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 块 | 多模态建模 | 大计算开销 | 知识蒸馏 | [[172](#bib.bib172), [171](#bib.bib171)] |'
- en: Dense Features. Dense features are widely used because of the neutrality of
    VOS which is supposed to estimate the pixel-level object masks. These methods
    often apply a pre-trained CNN encoder to extract dense feature maps from each
    frame. Generally, 2D CNN encoder is widely-used to extract feature maps [[98](#bib.bib98),
    [2](#bib.bib2), [120](#bib.bib120), [96](#bib.bib96)]. The CNN encoder is often
    pretrained on ImageNet [[30](#bib.bib30)] and fine-tuned on the video object segmentation
    dataset. After extracting the dense feature maps, these methods utilize a transformer
    [[155](#bib.bib155)] to encode the feature maps into two *keys* and *values*,
    where *keys* contains the semantic code of the object and *values* contains the
    detailed appearance information. The encoded feature maps can be used for matching
    and information retrieval. Besides the appearance feature maps, Zhang et al. [[199](#bib.bib199)]
    introduced perceptual consistency to aid with predicting the pixel-wise correctness
    of the segmentation on an unlabeled frame.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 密集特征。密集特征被广泛使用，因为VOS的中立性使其能够估计像素级的物体掩码。这些方法通常应用预训练的CNN编码器从每一帧中提取密集特征图。一般而言，2D
    CNN编码器被广泛用于提取特征图[[98](#bib.bib98), [2](#bib.bib2), [120](#bib.bib120), [96](#bib.bib96)]。CNN编码器通常在ImageNet[[30](#bib.bib30)]上进行预训练，并在视频目标分割数据集上进行微调。提取密集特征图后，这些方法利用变换器[[155](#bib.bib155)]将特征图编码为两个*键*和*值*，其中*键*包含物体的语义代码，*值*包含详细的外观信息。编码后的特征图可以用于匹配和信息检索。除了外观特征图外，张等人[[199](#bib.bib199)]引入了感知一致性来帮助预测未标记帧上的像素级分割正确性。
- en: '*Pros.* RGB frame provides rich information about the appearances textcolorgreenwhich
    is crucial in VOS. With the help of GPU parallel computing and large-scale pretrained
    model, dense features extraction from RGB frame becomes standard processing in
    video object segmentation nowadays.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*优点.* RGB 帧提供了关于外观的丰富信息，这对于视频目标分割来说至关重要。借助 GPU 并行计算和大规模预训练模型的帮助，从 RGB 帧中提取密集特征已成为当前视频目标分割的标准处理方式。'
- en: '*Cons.* Although dense feature extraction has been widely studied, They may
    contain background noise and irrelevant information. Particularly, occlusion-handling
    is a main challenge of using dense features in VOS.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*缺点.* 尽管密集特征提取已被广泛研究，但它们可能包含背景噪音和无关信息。特别是，遮挡处理是在视频目标分割中使用密集特征面临的主要挑战。'
- en: '*Solutions.* In dealing with occlusions, its imperative to recover occluded
    parts by corresponding shape and appearance in motion rather than irrelevant background.
    For this purpose, Occlusion-aware feature modeling  [[67](#bib.bib67)] is proposed
    which uses shape completion and flow completion modules to inpaint invisible parts
    intelligently. In [[66](#bib.bib66)], a pipeline is proposed that used GCN which
    allows propagation of non-local information across pixels despite the presence
    of occluding regions.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案.* 在处理遮挡时，重要的是通过相应的形状和外观来恢复遮挡的部分，而不是无关的背景。为此，提出了遮挡感知特征建模[[67](#bib.bib67)]，它使用形状补全和流补全模块来智能修补不可见部分。在[[66](#bib.bib66)]中，提出了一个使用
    GCN 的流水线，它允许在存在遮挡区域时跨像素传播非局部信息。'
- en: Sparse Features. Compared with the dense feature extraction, sparse feature
    modeling discards the irrelevant information from input. In VOS, short tracks
    or tracklets in a frame are considered as sparse features. In  [[16](#bib.bib16)],
    a State-Aware Tracker (SAT) is proposed that takes advantage of the inter-frame
    consistency and deal with each target object as a tracklet. Because the irrelevant
    background is discard, they achieve real-time video object segmentation on 39
    FPS which is faster than traditional dense feature methods. In  [[147](#bib.bib147)],
    the video object tracking module is adopted to first locate the object region
    from the background. Then they segment the object masks from the small object
    region.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏特征。与密集特征提取相比，稀疏特征建模会丢弃输入中的无关信息。在视频目标分割中，帧中的短轨迹或轨迹片被视为稀疏特征。在[[16](#bib.bib16)]中，提出了一种状态感知跟踪器
    (SAT)，它利用帧间一致性并将每个目标对象视为一个轨迹片。由于丢弃了无关的背景，他们在 39 帧每秒的实时视频目标分割中取得了比传统的密集特征方法更快的速度。在[[147](#bib.bib147)]中，采用视频目标跟踪模块首先从背景中定位对象区域，然后从小的对象区域分割出对象掩模。
- en: '*Pros.* The advantages of the sparse feature modeling are two fold. Firstly,
    it uses pre-processing or prior knowledge to clean the irrelevant information
    from input, which makes the pipeline more robust to the noises from the background.
    Secondly, using sparse features can accelerate the speed of segmentation, which
    gives the users an option to choose a trade-off between efficiency and accuracy.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*优点.* 稀疏特征建模的优点有两方面。首先，它使用预处理或先验知识来清除输入中的无关信息，这使得流水线对背景噪音更加鲁棒。其次，使用稀疏特征可以加快分割速度，为用户提供了在效率和准确性之间进行权衡的选项。'
- en: '*Cons.* Sparse feature modeling in VOS relies on the data pre-processing. It
    is inevitable that some important data from the object is mistakenly discard in
    this process. Also, sparse features lack the appearance/color information which
    is helpful in VOS.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*缺点.* 视频目标分割中的稀疏特征建模依赖于数据预处理。在这个过程中，不可避免地会误删一些来自对象的重要数据。而且，稀疏特征缺乏在视频目标分割中有用的外观/颜色信息。'
- en: '*Solution.* Compared with dense feature modeling, sparse feature modeling has
    lower accuracy but better runtime performance. Recently, [[173](#bib.bib173)]
    used tracklet query and tracklet proposal that combines RoI features and dense
    frame features by the vision transformer. It achieved state-of-the-art performance
    in YouTube-VIS 2019 val set [[191](#bib.bib191)].'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案.* 与密集特征建模相比，稀疏特征建模具有更低的准确性，但具有更好的运行时性能。最近，在[[173](#bib.bib173)]中使用了轨迹查询和轨迹提案，通过视觉变换器将
    RoI 特征和密集帧特征结合起来。这在 YouTube-VIS 2019 验证集中取得了最先进的性能[[191](#bib.bib191)]。'
- en: 'Table 11: Comparison of different spatial feature performance for DAVIS-2016
    and DAVIS-2017 datasets [[125](#bib.bib125)]. $\mathcal{J}$, $\mathcal{F}$ and
    $\mathcal{J\&amp;F}$ score represent region similarity, contour accuracy and the
    average value of region similarity and contour accuracy, respectively. All models
    use frame-level features in temporal dimension.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: DAVIS-2016 和 DAVIS-2017 数据集[[125](#bib.bib125)]中不同空间特征表现的比较。$\mathcal{J}$、$\mathcal{F}$
    和 $\mathcal{J\&amp;F}$ 分数分别表示区域相似性、轮廓准确性以及区域相似性和轮廓准确性的平均值。所有模型在时间维度上使用帧级特征。'
- en: '| Model | Spatial | DAVIS val 16 | DAVIS val 17 |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 空间 | DAVIS val 16 | DAVIS val 17 |  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| $\mathcal{J}(\%)$ | $\mathcal{F}(\%)$ | $\mathcal{J\&amp;F}(\%)$ | $\mathcal{J}(\%)$
    | $\mathcal{F}(\%)$ | $\mathcal{J\&amp;F}(\%)$ | FPS |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{J}(\%)$ | $\mathcal{F}(\%)$ | $\mathcal{J\&amp;F}(\%)$ | $\mathcal{J}(\%)$
    | $\mathcal{F}(\%)$ | $\mathcal{J\&amp;F}(\%)$ | FPS |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| FEELVOS [[158](#bib.bib158)] | Dense | 81.1 | 82.2 | 81.7 | 69.1 | 74.0 |
    71.5 | 2.2 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| FEELVOS [[158](#bib.bib158)] | 稠密 | 81.1 | 82.2 | 81.7 | 69.1 | 74.0 | 71.5
    | 2.2 |'
- en: '| STM [[117](#bib.bib117)] | Dense | 84.8 | 88.1 | 86.5 | 69.2 | 74.0 | 71.6
    | 6.3 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| STM [[117](#bib.bib117)] | 稠密 | 84.8 | 88.1 | 86.5 | 69.2 | 74.0 | 71.6 |
    6.3 |'
- en: '| AGAME [[62](#bib.bib62)] | Dense | 82.0 | - | - | 67.2 | 72.7 | 70.0 | 14.3
    |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| AGAME [[62](#bib.bib62)] | 稠密 | 82.0 | - | - | 67.2 | 72.7 | 70.0 | 14.3
    |'
- en: '| AGSS-VOS [[93](#bib.bib93)] | Dense | - | - | - | 63.4 | 69.8 | 66.6 | 10.0
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| AGSS-VOS [[93](#bib.bib93)] | 稠密 | - | - | - | 63.4 | 69.8 | 66.6 | 10.0
    |'
- en: '| AFB-URR [[92](#bib.bib92)] | Dense | - | - | - | 73.0 | 76.1 | 74.6 | 4.0
    |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| AFB-URR [[92](#bib.bib92)] | 稠密 | - | - | - | 73.0 | 76.1 | 74.6 | 4.0 |'
- en: '| FRTM [[129](#bib.bib129)] | Dense | - | - | 81.7 | 66.4 | 71.2 | 68.8 | 21.9
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| FRTM [[129](#bib.bib129)] | 稠密 | - | - | 81.7 | 66.4 | 71.2 | 68.8 | 21.9
    |'
- en: '| LCM [[53](#bib.bib53)] | Dense | - | - | - | 73.1 | 77.2 | 75.2 | 8.5 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| LCM [[53](#bib.bib53)] | 稠密 | - | - | - | 73.1 | 77.2 | 75.2 | 8.5 |'
- en: '| RMNet [[175](#bib.bib175)] | Dense | 80.6 | 82.3 | 81.5 | 72.8 | 77.2 | 75.0
    | 11.9 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| RMNet [[175](#bib.bib175)] | 稠密 | 80.6 | 82.3 | 81.5 | 72.8 | 77.2 | 75.0
    | 11.9 |'
- en: '| SWEM [[96](#bib.bib96)] | Dense | 87.3 | 89.0 | 88.1 | 74.5 | 79.8 | 77.2
    | 36 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| SWEM [[96](#bib.bib96)] | 稠密 | 87.3 | 89.0 | 88.1 | 74.5 | 79.8 | 77.2 |
    36 |'
- en: '| BMVOS [[22](#bib.bib22)] | Dense | 82.9 | 81.4 | 82.2 | 70.7 | 74.7 | 72.7
    | 45.9 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| BMVOS [[22](#bib.bib22)] | 稠密 | 82.9 | 81.4 | 82.2 | 70.7 | 74.7 | 72.7 |
    45.9 |'
- en: '| FTM [[147](#bib.bib147)] | Sparse | 77.5 | - | 78.9 | 69.1 | - | 70.6 | 11.1
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| FTM [[147](#bib.bib147)] | 稀疏 | 77.5 | - | 78.9 | 69.1 | - | 70.6 | 11.1
    |'
- en: '| SAT [[16](#bib.bib16)] | Sparse | 82.6 | 83.6 | 83.1 | 68.6 | 76.0 | 72.3
    | 39.0 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| SAT [[16](#bib.bib16)] | 稀疏 | 82.6 | 83.6 | 83.1 | 68.6 | 76.0 | 72.3 | 39.0
    |'
- en: '| SAT-Fast [[16](#bib.bib16)] | Sparse | - | - | - | 65.4 | 73.6 | 69.5 | 60.0
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| SAT-Fast [[16](#bib.bib16)] | 稀疏 | - | - | - | 65.4 | 73.6 | 69.5 | 60.0
    |'
- en: 'Table 12: Comparison of different spatial feature performances for Youtube-VOS [[182](#bib.bib182)]
    dataset. $\mathcal{J}$ and $\mathcal{F}$ represent region similarity, contour
    accuracy and seen and unseen tags indicate seen and unseen objects from the training
    dataset separately. $\mathcal{G}$ measures the overall score of the segmentation
    accuracy. FPS metric reports the runtime speed in frames per second.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: 不同空间特征在 Youtube-VOS [[182](#bib.bib182)] 数据集上的比较。$\mathcal{J}$ 和 $\mathcal{F}$
    代表区域相似性和轮廓准确性，seen 和 unseen 分别表示训练数据集中看到和未看到的对象。$\mathcal{G}$ 衡量分割准确性的总体评分。FPS
    指标报告每秒帧数的运行速度。'
- en: '| Model | Spatial | $\mathcal{J}_{seen}(\%)$ | $\mathcal{J}_{unseen}(\%)$ |
    $\mathcal{F}_{seen}(\%)$ | $\mathcal{F}_{unseen}(\%)$ | $\mathcal{G}$ | FPS |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 空间 | $\mathcal{J}_{seen}(\%)$ | $\mathcal{J}_{unseen}(\%)$ | $\mathcal{F}_{seen}(\%)$
    | $\mathcal{F}_{unseen}(\%)$ | $\mathcal{G}$ | FPS |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| STM [[117](#bib.bib117)] | Dense | 79.7 | 84.2 | 72.8 | 80.9 | 79.4 | - |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| STM [[117](#bib.bib117)] | 稠密 | 79.7 | 84.2 | 72.8 | 80.9 | 79.4 | - |'
- en: '| RVOS [[157](#bib.bib157)] | Dense | 63.6 | 45.5 | 67.2 | 51.0 | 56.8 | 22.7
    |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| RVOS [[157](#bib.bib157)] | 稠密 | 63.6 | 45.5 | 67.2 | 51.0 | 56.8 | 22.7
    |'
- en: '| AGSS-VOS [[93](#bib.bib93)] | Dense | 71.3 | 65.5 | 76.2 | 73.1 | 71.3 |
    12.5 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| AGSS-VOS [[93](#bib.bib93)] | 稠密 | 71.3 | 65.5 | 76.2 | 73.1 | 71.3 | 12.5
    |'
- en: '| AFB-URR [[92](#bib.bib92)] | Dense | 78.8 | 74.1 | 83.1 | 82.6 | 79.6 | 3.3
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| AFB-URR [[92](#bib.bib92)] | 稠密 | 78.8 | 74.1 | 83.1 | 82.6 | 79.6 | 3.3
    |'
- en: '| FRTM [[129](#bib.bib129)] | Dense | 72.3 | 65.9 | 76.2 | 74.1 | 72.1 | -
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| FRTM [[129](#bib.bib129)] | 稠密 | 72.3 | 65.9 | 76.2 | 74.1 | 72.1 | - |'
- en: '| LCM [[53](#bib.bib53)] | Dense | 82.2 | 75.7 | 86.7 | 83.4 | 82.0 | - |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| LCM [[53](#bib.bib53)] | 稠密 | 82.2 | 75.7 | 86.7 | 83.4 | 82.0 | - |'
- en: '| RMNet [[175](#bib.bib175)] | Dense | 82.1 | 85.7 | 75.7 | 82.4 | 81.5 | -
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| RMNet [[175](#bib.bib175)] | 稠密 | 82.1 | 85.7 | 75.7 | 82.4 | 81.5 | - |'
- en: '| SWEM [[96](#bib.bib96)] | Dense | 82.4 | 86.9 | 77.1 | 85.0 | 82.8 | - |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| SWEM [[96](#bib.bib96)] | 稠密 | 82.4 | 86.9 | 77.1 | 85.0 | 82.8 | - |'
- en: '| BMVOS [[22](#bib.bib22)] | Dense | 73.5 | 68.5 | 77.4 | 76.0 | 73.9 | 28.0
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| BMVOS [[22](#bib.bib22)] | Dense | 73.5 | 68.5 | 77.4 | 76.0 | 73.9 | 28.0
    |'
- en: '| SAT [[16](#bib.bib16)] | Sparse | 67.1 | 55.3 | 70.2 | 61.7 | 63.6 | 39.0
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| SAT [[16](#bib.bib16)] | Sparse | 67.1 | 55.3 | 70.2 | 61.7 | 63.6 | 39.0
    |'
- en: Frame-level Features. Several VOS methods predict segmentation masks frame by
    frame. They utilize previous frames information to model the current object appearance
    and motion. Appearance similarity between frames is widely-used to segment objects
    from background [[168](#bib.bib168), [116](#bib.bib116), [180](#bib.bib180), [62](#bib.bib62),
    [199](#bib.bib199), [205](#bib.bib205), [81](#bib.bib81), [2](#bib.bib2)]. The
    intuition behind these papers is that perceptually similar pixels are more likely
    to be in the same class. In frame-level features, it is essential to make full
    use of historical frames in the videos. Some methods use space-time memory banks
    to store the embeddings every several frames [[19](#bib.bib19), [53](#bib.bib53)].
    Other methods fuse pixel embedding of the current frame and the memory bank [[92](#bib.bib92)].
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 帧级特征。几种VOS方法逐帧预测分割掩码。它们利用前面的帧信息来建模当前对象的外观和运动。帧之间的外观相似性被广泛使用来从背景中分割对象 [[168](#bib.bib168),
    [116](#bib.bib116), [180](#bib.bib180), [62](#bib.bib62), [199](#bib.bib199),
    [205](#bib.bib205), [81](#bib.bib81), [2](#bib.bib2)]。这些论文背后的直觉是，感知上相似的像素更可能属于同一类别。在帧级特征中，充分利用视频中的历史帧是至关重要的。一些方法使用时空记忆库来存储每几帧的嵌入
    [[19](#bib.bib19), [53](#bib.bib53)]。其他方法则融合当前帧的像素嵌入和记忆库 [[92](#bib.bib92)]。
- en: In  [[157](#bib.bib157)] a recurrent pipeline (RVOS) is proposed to keep the
    coherence of the segmented objects along time. Due to the RNN’s memory capabilities,
    RVOS is recurrent in the spatial-temporal domain and can handle the instances
    matching at different frames. Recent papers used auxiliary temporal information
    including optical flow  [[195](#bib.bib195), [93](#bib.bib93), [3](#bib.bib3)]
    to aid VOS.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[157](#bib.bib157)] 中，提出了一种递归管道（RVOS），以保持分割对象在时间上的连贯性。由于RNN的记忆能力，RVOS在时空领域中是递归的，并且可以处理不同帧之间的实例匹配。最近的论文使用了包括光流在内的辅助时间信息
    [[195](#bib.bib195), [93](#bib.bib93), [3](#bib.bib3)] 来辅助VOS。
- en: '*Pros.* Most VOS methods follow the frame-level feature modeling because it
    usually has lower computational costs. These features are also suitable to handle
    streaming data such as online video or online meeting because it processes the
    video frame by frames. Using optical flow map as dense map is very common in the
    unsupervised video object segmentation task [[128](#bib.bib128), [194](#bib.bib194)],
    which provide the dense correspondence between similar pixels.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '*优点。* 大多数VOS方法遵循帧级特征建模，因为它通常具有较低的计算成本。这些特征也适合处理流数据，如在线视频或在线会议，因为它逐帧处理视频。使用光流图作为密集图在无监督视频对象分割任务中非常常见
    [[128](#bib.bib128), [194](#bib.bib194)]，它提供了相似像素之间的密集对应关系。'
- en: '*Cons.* Although frame-level feature modeling has lots of benefits, it brings
    some shortcomings and limitations. First of all, as information is processed frame
    by frame, past frames aid model in segmenting objects in the current and future
    frames. Managing past information is very challenging and requires lots of efforts
    in model design, particularly when the length of video increases. Additionally,
    the errors from past frames are accumulated and transferred during video processing.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*缺点。* 尽管帧级特征建模有很多好处，但也带来了一些缺陷和限制。首先，由于信息是逐帧处理的，过去的帧帮助模型在当前和未来的帧中分割对象。管理过去的信息非常具有挑战性，并且在模型设计中需要大量的努力，尤其是当视频长度增加时。此外，过去帧中的错误在视频处理过程中会积累并转移。'
- en: '*Solutions.* Recent research papers proposed adaptive memory bank scheme to
    maintain features from previous frames. In  [[92](#bib.bib92)] an exponential
    moving average (EMA) based scheme is proposed to update the record of historical
    information. This method helps model to better use past information. In  [[81](#bib.bib81)],
    a learning based spatial-temporal aggregation model (SAM) is introduced to distill
    the frame-level features and automatically correct the accumulated errors.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案。* 最近的研究论文提出了自适应记忆库方案，以维护来自以前帧的特征。在 [[92](#bib.bib92)] 中，提出了一种基于指数移动平均（EMA）的方案来更新历史信息的记录。这种方法有助于模型更好地利用过去的信息。在
    [[81](#bib.bib81)] 中，介绍了一种基于学习的时空聚合模型（SAM），用于提炼帧级特征并自动纠正积累的错误。'
- en: Chunk-level Features. Inspired by the human action recognition field, In  [[55](#bib.bib55)],
    a 3D convolutional neural network is used as a backbone to extract chunk-level
    features. Recent chunk-level feature modeling is used in the multimodal video
    segmentation task, which combines text reasoning, video understanding, instance
    segmentation and tracking. Recent papers first use a CNN encoder to extract feature
    maps from each frame [[127](#bib.bib127)], then combine them into the spatial-temporal
    features. In [[7](#bib.bib7), [172](#bib.bib172), [171](#bib.bib171)], a Transformer
    is used to model spatial-temporal features from videos.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 块级特征。受到人体动作识别领域的启发，在 [[55](#bib.bib55)] 中，使用了一个3D卷积神经网络作为骨干网来提取块级特征。近期的块级特征建模应用于多模态视频分割任务，结合了文本推理、视频理解、实例分割和跟踪。近期论文首先使用CNN编码器从每帧
    [[127](#bib.bib127)] 提取特征图，然后将它们组合成时空特征。在 [[7](#bib.bib7), [172](#bib.bib172),
    [171](#bib.bib171)] 中，使用Transformer来建模视频中的时空特征。
- en: '*Pros.* The advantage of chunk-level features in VOS is that it models the
    global semantic information in consecutive frames, which is critical to multi-modal
    tasks.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*优点。* 在视频目标分割（VOS）中，块级特征的优势在于它建模了连续帧中的全局语义信息，这对于多模态任务至关重要。'
- en: '*Cons.* Modeling chunk-level features often requires large memory and computation
    costs compared with frame-level features. Heavy computation requirement limits
    its application on mobile device.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '*缺点。* 与帧级特征相比，建模块级特征通常需要大量内存和计算成本。高计算要求限制了其在移动设备上的应用。'
- en: '*Solutions.* Knowledge distillation [[95](#bib.bib95)] and neural network pruning/search [[204](#bib.bib204)]
    are two active fields aim to relief the burden of neural network computing. They
    can help in finding a smaller network with lower network parameters but similar
    accuracy performance. These methods could be used to optimize the architecture
    of the chunk-level feature encoder to accelerate their computation. In  [[179](#bib.bib179)],
    an acceleration framework is proposed based on video-compressed codec. For each
    chunk, it has three types of I-frames, P-frames, and B-frames. They utilize the
    sparsity of I-frames and motion vectors from P-/B-frames to accelerate the video
    object segmentation.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案。* 知识蒸馏 [[95](#bib.bib95)] 和神经网络剪枝/搜索 [[204](#bib.bib204)] 是两个积极的研究领域，旨在减轻神经网络计算的负担。它们可以帮助找到一个具有较少网络参数但相似准确度的小型网络。这些方法可以用于优化块级特征编码器的结构，以加速其计算。在 [[179](#bib.bib179)]
    中，提出了一种基于视频压缩编解码器的加速框架。对于每个块，它有三种类型的I帧、P帧和B帧。它们利用I帧的稀疏性和来自P/B帧的运动向量来加速视频对象分割。'
- en: 'Table 13: Comparison of different models performance with chunk level features
    for Ref-Youtube-VOS [[130](#bib.bib130)] across different backbones and chunk
    sizes $\omega$. $\mathcal{J}$, $\mathcal{F}$ and $\mathcal{J\&amp;F}$ score represent
    region similarity, contour accuracy and the average value of region similarity
    and contour accuracy, respectively.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：不同模型在块级特征下对Ref-Youtube-VOS [[130](#bib.bib130)]的性能比较，涉及不同的骨干网络和块大小 $\omega$。$\mathcal{J}$、$\mathcal{F}$
    和 $\mathcal{J\&amp;F}$ 分别表示区域相似性、轮廓准确性以及区域相似性和轮廓准确性的平均值。
- en: '| Model | Backbone | $\mathcal{J}(\%)$ | $\mathcal{F}(\%)$ | $\mathcal{J\&amp;F}(\%)$
    |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 骨干网络 | $\mathcal{J}(\%)$ | $\mathcal{F}(\%)$ | $\mathcal{J\&amp;F}(\%)$
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| URVOS [[130](#bib.bib130)] | ResNet-50 | 45.3 | 49.2 | 47.2 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| URVOS [[130](#bib.bib130)] | ResNet-50 | 45.3 | 49.2 | 47.2 |'
- en: '| MLRL [[171](#bib.bib171)] | ResNet-50 | 48.4 | 51.0 | 49.7 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| MLRL [[171](#bib.bib171)] | ResNet-50 | 48.4 | 51.0 | 49.7 |'
- en: '| MTTR ($\omega$=12) [[7](#bib.bib7)] | Video-Swin-T | 54.0 | 56.6 | 55.3 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| MTTR ($\omega$=12) [[7](#bib.bib7)] | Video-Swin-T | 54.0 | 56.6 | 55.3 |'
- en: '| ReferFormer ($\omega$=5) [[172](#bib.bib172)] | Video-Swin-T | 54.8 | 57.3
    | 56.0 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| ReferFormer ($\omega$=5) [[172](#bib.bib172)] | Video-Swin-T | 54.8 | 57.3
    | 56.0 |'
- en: '| ReferFormer [[172](#bib.bib172)] | Video-Swin-B | 61.3 | 64.6 | 62.9 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| ReferFormer [[172](#bib.bib172)] | Video-Swin-B | 61.3 | 64.6 | 62.9 |'
- en: 'Table 14: Comparison of different spatial feature performance for A2D-Sentences [[41](#bib.bib41)].
    $*^{T}$ indicates the backbone architecture is Video-Swin-T. $*^{B}$ indicates
    the backbone architecture is Video-Swin-B. The Precision @K measures the percentage
    of test samples that their whole IoU scores are higher than threshold K.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：不同空间特征在A2D-Sentences [[41](#bib.bib41)] 的性能比较。$*^{T}$ 表示骨干网络架构为 Video-Swin-T。$*^{B}$
    表示骨干网络架构为 Video-Swin-B。精度 @K 衡量测试样本中其整体 IoU 分数高于阈值 K 的百分比。
- en: '| Model | Precision | IoU | mAP |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 精度 | IoU | mAP |'
- en: '| --- | --- | --- | --- |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| P@0.5 | P@0.6 | P@0.7 | P@0.8 | P@0.9 | Overall | Mean |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| P@0.5 | P@0.6 | P@0.7 | P@0.8 | P@0.9 | 总体 | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| MTTR^T($\omega$=8) [[7](#bib.bib7)] | 72.1 | 68.4 | 60.7 | 45.6 | 16.4 |
    70.2 | 61.8 | 44.7 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| MTTR^T($\omega$=8) [[7](#bib.bib7)] | 72.1 | 68.4 | 60.7 | 45.6 | 16.4 |
    70.2 | 61.8 | 44.7 |'
- en: '| MTTR^T($\omega$=10) [[7](#bib.bib7)] | 75.4 | 71.2 | 63.8 | 48.5 | 16.9 |
    72.0 | 64.0 | 46.1 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| MTTR^T($\omega$=10) [[7](#bib.bib7)] | 75.4 | 71.2 | 63.8 | 48.5 | 16.9 |
    72.0 | 64.0 | 46.1 |'
- en: '| ReferFormer^T($\omega$=6) [[172](#bib.bib172)] | 76.0 | 72.2 | 65.4 | 49.8
    | 17.9 | 72.3 | 64.1 | 48.6 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| ReferFormer^T($\omega$=6) [[172](#bib.bib172)] | 76.0 | 72.2 | 65.4 | 49.8
    | 17.9 | 72.3 | 64.1 | 48.6 |'
- en: '| ReferFormer^B($\omega$=5) [[172](#bib.bib172)] | 83.1 | 80.4 | 74.1 | 57.9
    | 21.2 | 78.6 | 70.3 | 55.0 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| ReferFormer^B($\omega$=5) [[172](#bib.bib172)] | 83.1 | 80.4 | 74.1 | 57.9
    | 21.2 | 78.6 | 70.3 | 55.0 |'
- en: Summary. We summarized pros and cons of features and the possible solution in
    table 10\. Dense features Contain rich appearance information which is essential
    in VOS, but they are not robust against occlusion. Some approaches use occlusion-aware
    encoders to overcome this shortcoming. Sparse features disregard irrelevant information
    and have lower computation cost. The accuracy of sparse features are lower in
    VOS task. To overcome this issue, people use hybrid algorithms. Frame-level features
    have lower computational cost compared to frame-level features and are suitable
    for stream processing. However, the computation error is accumulated in these
    features while using past information. Also, managing past frames requires a lot
    of efforts. Some researches adopted dynamic memory to relive the error accumulation.
    Also they used exponential weight smoothing and learning based feature adaption
    to manage past frame information. Chunk-level features model more global information
    than frame-level features but are more computationally expensive. Different methods
    are adopted that use knowledge distillation to alleviate computation burden.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。我们总结了表10中特征的优缺点和可能的解决方案。密集特征包含丰富的外观信息，这在视频目标跟踪（VOS）中至关重要，但它们对遮挡不够鲁棒。一些方法使用遮挡感知编码器来克服这一缺点。稀疏特征忽略了不相关的信息，计算成本较低。然而，稀疏特征在VOS任务中的准确性较低。为了解决这一问题，人们使用了混合算法。与帧级特征相比，块级特征的计算成本较低，更适合流处理。然而，在使用过去信息时，这些特征的计算误差会累积。此外，管理过去的帧需要付出大量的努力。一些研究采用了动态内存来缓解误差累积问题。此外，他们还使用了指数加权平滑和基于学习的特征适应来管理过去的帧信息。块级特征比帧级特征建模更多的全局信息，但计算成本更高。采用了不同的方法，通过知识蒸馏来减轻计算负担。
- en: 'To conclude as shown in Table [11](#S3.T11 "Table 11 ‣ 3.2 Video Object Segmentation
    ‣ 3 Applying Deep Features in Video Analysis Tasks ‣ Deep Video Representation
    Learning: a Survey") and Table [12](#S3.T12 "Table 12 ‣ 3.2 Video Object Segmentation
    ‣ 3 Applying Deep Features in Video Analysis Tasks ‣ Deep Video Representation
    Learning: a Survey"), we compared the performance of different spatial feature
    models. These comparisons are from the spatial aspect, they all belong to the
    frame-level features. The DAVIS’17 datasets [[125](#bib.bib125)] benchmarks contain
    60 videos for training and 30 videos for validation. The YouTube-VOS [[182](#bib.bib182)]
    benchmark contains 3,471 videos for training and 507 for validation. In both benchmarks,
    the video object segmentation task is to segment and track the arbitrary number
    of objects in each video. The groundtruth masks for each object are provided in
    the first frame. For DAVIS datasets, we followed the official evaluation metrics,
    region similarity $\mathcal{J}$ and contour accuracy $\mathcal{F}$. The $\mathcal{J\&amp;F}$
    score is the average value of region similarity and contour accuracy. For YouTube-VOS
    dataset, we used similar metrics $\mathcal{J}$ and $\mathcal{F}$. $seen$ and $unseen$
    tags indicate seen and unseen objects from the training dataset separately. $\mathcal{G}$
    measures the overall score of the segmentation accuracy. FPS metric reports the
    runtime speed in frames per second. The runtime speed was measured on NVIDIA 2080Ti
    and NVIDIA V100 GPUs. SWEM [[96](#bib.bib96)] has the best accuracy performance
    in both DAVIS and YouTube-VOS datasets because it utilizes dense feature modeling,
    while SAT [[16](#bib.bib16)] achieves the best runtime speed because of sparse
    feature modeling.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '总结如表格 [11](#S3.T11 "Table 11 ‣ 3.2 Video Object Segmentation ‣ 3 Applying Deep
    Features in Video Analysis Tasks ‣ Deep Video Representation Learning: a Survey")
    和表格 [12](#S3.T12 "Table 12 ‣ 3.2 Video Object Segmentation ‣ 3 Applying Deep Features
    in Video Analysis Tasks ‣ Deep Video Representation Learning: a Survey") 所示，我们比较了不同空间特征模型的性能。这些比较从空间角度出发，它们都属于帧级特征。DAVIS’17
    数据集 [[125](#bib.bib125)] 基准包含 60 个用于训练的视频和 30 个用于验证的视频。YouTube-VOS [[182](#bib.bib182)]
    基准包含 3,471 个用于训练的视频和 507 个用于验证的视频。在这两个基准中，视频对象分割任务是对每个视频中的任意数量的对象进行分割和跟踪。每个对象的真实掩膜在第一帧中提供。对于
    DAVIS 数据集，我们使用了官方评估指标，区域相似度 $\mathcal{J}$ 和轮廓准确度 $\mathcal{F}$。$\mathcal{J\&amp;F}$
    分数是区域相似度和轮廓准确度的平均值。对于 YouTube-VOS 数据集，我们使用了类似的指标 $\mathcal{J}$ 和 $\mathcal{F}$。$seen$
    和 $unseen$ 标签分别表示训练数据集中见过和未见过的对象。$\mathcal{G}$ 测量分割准确度的整体评分。FPS 指标报告每秒帧数的运行速度。运行速度是在
    NVIDIA 2080Ti 和 NVIDIA V100 GPU 上测量的。SWEM [[96](#bib.bib96)] 在 DAVIS 和 YouTube-VOS
    数据集中具有最佳的准确性表现，因为它利用了密集特征建模，而 SAT [[16](#bib.bib16)] 由于稀疏特征建模，实现了最佳的运行速度。'
- en: 'To compare the chunk-level feature modeling, we chose the popular Ref-Youtube-VOS [[130](#bib.bib130)]
    and A2D-Sentences [[41](#bib.bib41)] benchmarks for comparisons. The Ref-YouTube-VOS
    dataset covers 3,978 videos with around $15K$ language descriptions. We used similar
    metrics to measure the segmentation performance, region similarity $\mathcal{J}$,
    $\mathcal{F}$, and $\mathcal{J\&amp;F}$. The A2D-Sentences dataset contains 3,782
    videos and each video has 3-5 frames annotated with the pixel-level segmentation
    masks. The model is evaluated with criteria of Precision $@K$, Overall IoU, Mean
    IoU and mAP. The Precision $@K$ measures the percentage of test samples whole
    IoU scores are higher than threshold K. Following standard protocol, the thresholds
    are set as 0.5:0.1:0.9. We compared the performance results across different network
    settings: (1) different backbones Video-Swin-T and Video-Swin-B from Video Swin
    Transformer [[107](#bib.bib107)], (2) different chunk sizes $\omega$. From Table [13](#S3.T13
    "Table 13 ‣ 3.2 Video Object Segmentation ‣ 3 Applying Deep Features in Video
    Analysis Tasks ‣ Deep Video Representation Learning: a Survey") and Table [14](#S3.T14
    "Table 14 ‣ 3.2 Video Object Segmentation ‣ 3 Applying Deep Features in Video
    Analysis Tasks ‣ Deep Video Representation Learning: a Survey"), ReferFormer [[172](#bib.bib172)]
    achieve the best performance because it designed the cross-model feature pyramid
    network to extract multi-scale chunk-level features from the input.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '为了比较块级特征建模，我们选择了流行的 Ref-Youtube-VOS [[130](#bib.bib130)] 和 A2D-Sentences [[41](#bib.bib41)]
    基准进行比较。Ref-YouTube-VOS 数据集包含 3,978 个视频和大约 $15K$ 语言描述。我们使用了类似的度量标准来测量分割性能、区域相似度
    $\mathcal{J}$、$\mathcal{F}$ 和 $\mathcal{J\&F}$。A2D-Sentences 数据集包含 3,782 个视频，每个视频都有
    3-5 帧，标注了像素级分割掩码。模型的评估标准包括 Precision $@K$、Overall IoU、Mean IoU 和 mAP。Precision
    $@K$ 测量测试样本中所有 IoU 分数高于阈值 K 的百分比。按照标准协议，阈值设置为 0.5:0.1:0.9。我们比较了不同网络设置下的性能结果：（1）来自
    Video Swin Transformer [[107](#bib.bib107)] 的不同骨干网 Video-Swin-T 和 Video-Swin-B，（2）不同块大小
    $\omega$。从表 [13](#S3.T13 "Table 13 ‣ 3.2 Video Object Segmentation ‣ 3 Applying
    Deep Features in Video Analysis Tasks ‣ Deep Video Representation Learning: a
    Survey") 和表 [14](#S3.T14 "Table 14 ‣ 3.2 Video Object Segmentation ‣ 3 Applying
    Deep Features in Video Analysis Tasks ‣ Deep Video Representation Learning: a
    Survey") 中可以看出，ReferFormer [[172](#bib.bib172)] 实现了最佳性能，因为它设计了交叉模型特征金字塔网络，以从输入中提取多尺度块级特征。'
- en: 4 Conclusion and Future Work
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论与未来工作
- en: We provided an extensive survey on recent studies on deep video representation
    learning. We provided a new taxonomy of these features and classify existing methods
    accordingly. We discussed and compared the effectiveness (robustness) of different
    types of features under scenarios with different types of noise.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了关于深度视频表征学习的近期研究的广泛调查。我们提供了这些特征的新分类法，并相应地分类现有方法。我们讨论并比较了不同类型特征在不同噪声类型场景下的效果（鲁棒性）。
- en: Challenges. Spatially dense features can encode rich contextual information
    but are more sensitive to background noise. Handling dense features in presence
    of intense occlusion and view variations is still challenging. In contrast, sparse
    features are more robust against background noise and illumination variance, but
    arranging sparse topologies in spatial dimension is still challenging. It is still
    an open question if the spatial relations should be defined based on the natural
    inherent relation of the features points or on the correlation of non-adjacent
    points throughout the video. For example, in human body skeleton, should the spatial
    relations between coordinates be defined based on natural relations of human body
    joints or the correlation of non-adjacent joints during the movement? In frame-level
    features, lack of cross connections between temporal and spatial domains is a
    major drawback in capturing complex dynamics. In chunk-level features, improving
    model’s generalizability and high computational cost are the main challenges.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战。空间密集特征能够编码丰富的上下文信息，但对背景噪声更为敏感。在强遮挡和视角变化的情况下处理密集特征仍然具有挑战性。相比之下，稀疏特征对背景噪声和光照变化更为稳健，但在空间维度上排列稀疏拓扑结构仍然具有挑战性。是否应该根据特征点的自然固有关系或视频中非邻近点的相关性来定义空间关系，仍然是一个未解的问题。例如，在人体骨架中，坐标之间的空间关系是否应该基于人体关节的自然关系，还是基于关节在运动中的非邻近关系？在帧级特征中，时间和空间域之间缺乏交叉连接是捕捉复杂动态的主要缺陷。在块级特征中，提高模型的泛化能力和高计算成本是主要挑战。
- en: Future directions. The drawbacks of either using sparse or dense features could
    be solved by using multi modal inputs to some extent. A future direction of these
    studies is on designing new methods for mapping between different modalities’
    feature space, learning effective representations from multiple data modalities,
    and understanding when and where the fusion should happen. Recent studies [[209](#bib.bib209)]
    demonstrated using proper multi-modals clearly improve video analysis performance.
    While current attention methods have achieved progress in video representation
    learning, they often bring higher model complexity and suffer from heavier computational
    burden. Hence, many recent studies are on building more efficient attention models [[50](#bib.bib50)].
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 未来方向。使用稀疏或密集特征的缺陷可以通过在一定程度上使用多模态输入来解决。这些研究的未来方向包括设计新的方法来映射不同模态的特征空间，从多个数据模态中学习有效的表示，并理解融合应在何时何地发生。最近的研究[[209](#bib.bib209)]表明，使用适当的多模态显著提高了视频分析性能。虽然当前的注意力方法在视频表示学习方面取得了进展，但它们通常带来了更高的模型复杂性，并且面临更重的计算负担。因此，许多最近的研究致力于构建更高效的注意力模型[[50](#bib.bib50)]。
- en: Data availability statement
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据可用性声明
- en: All data supporting the findings of this study are available within the paper.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 支持本研究发现的所有数据均可在论文中获得。
- en: Declarations
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 声明
- en: Competing interests We do not have any conflict of interest related to the manuscript.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争利益 我们与手稿无任何利益冲突。
- en: References
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: \bibcommenthead
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \bibcommenthead
- en: 'Arnab et al [2021] Arnab A, Dehghani M, Heigold G, et al (2021) Vivit: A video
    vision transformer. In: ICCV, pp 6836–6846'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arnab et al [2021] Arnab A, Dehghani M, Heigold G, 等 (2021) Vivit: 一种视频视觉变换器。会议录:
    ICCV, 页 6836–6846'
- en: 'Athar et al [2022] Athar A, Luiten J, Hermans A, et al (2022) Hodor: High-level
    object descriptors for object re-segmentation in video learned from static images.
    In: CVPR, pp 3022–3031'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Athar et al [2022] Athar A, Luiten J, Hermans A, 等 (2022) Hodor: 从静态图像中学习的用于视频中的对象重新分割的高级对象描述符。会议录:
    CVPR, 页 3022–3031'
- en: 'Azulay et al [2022] Azulay A, Halperin T, Vantzos O, et al (2022) Temporally
    stable video segmentation without video annotations. In: WACV, pp 3449–3458'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Azulay et al [2022] Azulay A, Halperin T, Vantzos O, 等 (2022) 不依赖视频注释的时间稳定视频分割。会议录:
    WACV, 页 3449–3458'
- en: 'Baradel et al [2018] Baradel F, Wolf C, Mille J, et al (2018) Glimpse clouds:
    Human activity recognition from unstructured feature points. In: CVPR, pp 469–478'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baradel et al [2018] Baradel F, Wolf C, Mille J, 等 (2018) Glimpse clouds: 从非结构化特征点中进行人类活动识别。会议录:
    CVPR, 页 469–478'
- en: 'Bendre et al [2022] Bendre N, Zand N, Bhattarai S, et al (2022) Natural disaster
    analytics using high resolution satellite images. In: World Automation Congress,
    IEEE, pp 371–378'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bendre et al [2022] Bendre N, Zand N, Bhattarai S, 等 (2022) 使用高分辨率卫星图像进行自然灾害分析。会议录:
    世界自动化大会, IEEE, 页 371–378'
- en: 'Bertasius et al [2021] Bertasius G, Wang H, Torresani L (2021) Is space-time
    attention all you need for video understanding? In: ICML, p 4'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bertasius et al [2021] Bertasius G, Wang H, Torresani L (2021) 空间时间注意力是你理解视频所需的一切吗？会议录:
    ICML, 页 4'
- en: 'Botach et al [2022] Botach A, Zheltonozhskii E, Baskin C (2022) End-to-end
    referring video object segmentation with multimodal transformers. In: CVPR, pp
    4985–4995'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Botach et al [2022] Botach A, Zheltonozhskii E, Baskin C (2022) 端到端的参考视频目标分割与多模态变换器。会议录:
    CVPR, 页 4985–4995'
- en: 'Bruce et al [2021] Bruce X, Liu Y, Chan KC (2021) Multimodal fusion via teacher-student
    network for indoor action recognition. In: AAAI, pp 3199–3207'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bruce et al [2021] Bruce X, Liu Y, Chan KC (2021) 通过师生网络进行的多模态融合用于室内动作识别。会议录:
    AAAI, 页 3199–3207'
- en: 'Bruce et al [2022] Bruce X, Liu Y, Zhang X, et al (2022) Mmnet: A model-based
    multimodal network for human action recognition in rgb-d videos. PAMI'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bruce et al [2022] Bruce X, Liu Y, Zhang X, 等 (2022) Mmnet: 一种基于模型的多模态网络用于
    RGB-D 视频中的人类动作识别。PAMI'
- en: 'Caetano et al [2019] Caetano C, Sena J, Brémond F, et al (2019) Skelemotion:
    A new representation of skeleton joint sequences based on motion information for
    3d action recognition. In: International conference on advanced video and signal
    based surveillance, IEEE, pp 1–8'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Caetano et al [2019] Caetano C, Sena J, Brémond F, 等 (2019) Skelemotion: 一种基于运动信息的新型骨架关节序列表示，用于
    3D 动作识别。会议录: 国际高级视频和信号监控会议, IEEE, 页 1–8'
- en: 'Cai et al [2021] Cai J, Jiang N, Han X, et al (2021) Jolo-gcn: mining joint-centered
    light-weight information for skeleton-based action recognition. In: WACV, pp 2735–2744'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai et al [2021] Cai J, Jiang N, Han X, 等 (2021) Jolo-gcn: 挖掘以关节为中心的轻量级信息用于基于骨架的动作识别。会议录:
    WACV, 页 2735–2744'
- en: 'Carreira and Zisserman [2017] Carreira J, Zisserman A (2017) Quo vadis, action
    recognition? a new model and the kinetics dataset. In: CVPR, pp 6299–6308'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Carreira and Zisserman [2017] Carreira J, Zisserman A (2017) Quo vadis, action
    recognition? 一个新模型和 Kinetics 数据集。会议录: CVPR, 页 6299–6308'
- en: 'Chen et al [2018a] Chen D, Li H, Xiao T, et al (2018a) Video person re-identification
    with competitive snippet-similarity aggregation and co-attentive snippet embedding.
    In: CVPR, pp 1169–1178'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2018a] Chen D, Li H, Xiao T, 等 (2018a) 通过竞争片段相似性聚合和共同注意片段嵌入的视频人物重新识别。在:
    CVPR, pp 1169–1178'
- en: 'Chen et al [2022] Chen M, Wei F, Li C, et al (2022) Frame-wise action representations
    for long videos via sequence contrastive learning. In: CVPR, pp 13801–13810'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2022] Chen M, Wei F, Li C, 等 (2022) 通过序列对比学习获得长期视频的帧级动作表示。在: CVPR,
    pp 13801–13810'
- en: 'Chen and Yuille [2015] Chen X, Yuille AL (2015) Parsing occluded people by
    flexible compositions. In: CVPR, pp 3945–3954'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 和 Yuille [2015] Chen X, Yuille AL (2015) 通过灵活组合解析遮挡的人物。在: CVPR, pp 3945–3954'
- en: 'Chen et al [2020] Chen X, Li Z, Yuan Y, et al (2020) State-aware tracker for
    real-time video object segmentation. In: CVPR, pp 9384–9393'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2020] Chen X, Li Z, Yuan Y, 等 (2020) 实时视频目标分割的状态感知跟踪器。在: CVPR, pp 9384–9393'
- en: 'Chen et al [2018b] Chen Y, Pont-Tuset J, Montes A, et al (2018b) Blazingly
    fast video object segmentation with pixel-wise metric learning. In: CVPR, pp 1189–1198'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2018b] Chen Y, Pont-Tuset J, Montes A, 等 (2018b) 利用像素级度量学习进行极速视频目标分割。在:
    CVPR, pp 1189–1198'
- en: Chen et al [2016] Chen Z, Wang X, Sun Z, et al (2016) Motion saliency detection
    using a temporal fourier transform. Optics & Laser Technology 80:1–15
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2016] Chen Z, Wang X, Sun Z, 等 (2016) 使用时间傅里叶变换的运动显著性检测。Optics & Laser
    Technology 80:1–15
- en: 'Cheng et al [2021] Cheng HK, Tai YW, Tang CK (2021) Modular interactive video
    object segmentation: Interaction-to-mask, propagation and difference-aware fusion.
    In: CVPR, pp 5559–5568'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng 等 [2021] Cheng HK, Tai YW, Tang CK (2021) 模块化互动视频目标分割：交互到掩模、传播和差异感知融合。在:
    CVPR, pp 5559–5568'
- en: 'Cheng et al [2020a] Cheng K, Zhang Y, Cao C, et al (2020a) Decoupling gcn with
    dropgraph module for skeleton-based action recognition. In: ECCV, Springer, pp
    536–553'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng 等 [2020a] Cheng K, Zhang Y, Cao C, 等 (2020a) 使用 Dropgraph 模块解耦的 GCN 用于基于骨架的动作识别。在:
    ECCV, Springer, pp 536–553'
- en: 'Cheng et al [2020b] Cheng K, Zhang Y, He X, et al (2020b) Skeleton-based action
    recognition with shift graph convolutional network. In: CVPR, pp 183–192'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cheng 等 [2020b] Cheng K, Zhang Y, He X, 等 (2020b) 基于骨架的动作识别与移位图卷积网络。在: CVPR,
    pp 183–192'
- en: 'Cho et al [2022] Cho S, Lee H, Kim M, et al (2022) Pixel-level bijective matching
    for video object segmentation. In: WACV, pp 129–138'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cho 等 [2022] Cho S, Lee H, Kim M, 等 (2022) 视频目标分割的像素级双射匹配。在: WACV, pp 129–138'
- en: Choi et al [2019] Choi J, Gao C, Messou JC, et al (2019) Why can’t i dance in
    the mall? learning to mitigate scene bias in action recognition. NIPS 32
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等 [2019] Choi J, Gao C, Messou JC, 等 (2019) 为什么我不能在商场里跳舞？学习减轻动作识别中的场景偏差。NIPS
    32
- en: 'Choutas et al [2018] Choutas V, Weinzaepfel P, Revaud J, et al (2018) Potion:
    Pose motion representation for action recognition. In: CVPR, pp 7024–7033'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Choutas 等 [2018] Choutas V, Weinzaepfel P, Revaud J, 等 (2018) Potion: 动作识别的姿态运动表示。在:
    CVPR, pp 7024–7033'
- en: 'Cuevas et al [2020] Cuevas C, Quilón D, García N (2020) Techniques and applications
    for soccer video analysis: A survey. Multimedia Tools and Applications 79(39-40):29685–29721'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cuevas 等 [2020] Cuevas C, Quilón D, García N (2020) 足球视频分析的技术和应用：综述。Multimedia
    Tools and Applications 79(39-40):29685–29721
- en: 'Dai et al [2022] Dai R, Das S, Kahatapitiya K, et al (2022) Ms-tct: Multi-scale
    temporal convtransformer for action detection. In: CVPR, pp 20041–20051'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等 [2022] Dai R, Das S, Kahatapitiya K, 等 (2022) Ms-tct: 多尺度时间卷积变换器用于动作检测。在:
    CVPR, pp 20041–20051'
- en: 'Dai et al [2019] Dai X, Singh B, Ng JYH, et al (2019) Tan: Temporal aggregation
    network for dense multi-label action recognition. In: WACV, IEEE, pp 151–160'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等 [2019] Dai X, Singh B, Ng JYH, 等 (2019) Tan: 用于密集多标签动作识别的时间聚合网络。在: WACV,
    IEEE, pp 151–160'
- en: De Boissiere and Noumeir [2020] De Boissiere AM, Noumeir R (2020) Infrared and
    3d skeleton feature fusion for rgb-d action recognition. IEEE Access 8:168297–168308
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Boissiere 和 Noumeir [2020] De Boissiere AM, Noumeir R (2020) 红外和 3D 骨架特征融合用于
    RGB-D 动作识别。IEEE Access 8:168297–168308
- en: 'Deng et al [2009a] Deng J, Dong W, Socher R, et al (2009a) Imagenet: A large-scale
    hierarchical image database. In: CVPR, pp 248–255, [10.1109/CVPR.2009.5206848](https:/doi.org/10.1109/CVPR.2009.5206848)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng 等 [2009a] Deng J, Dong W, Socher R, 等 (2009a) Imagenet: 大规模层次图像数据库。在:
    CVPR, pp 248–255, [10.1109/CVPR.2009.5206848](https:/doi.org/10.1109/CVPR.2009.5206848)'
- en: 'Deng et al [2009b] Deng J, Dong W, Socher R, et al (2009b) Imagenet: A large-scale
    hierarchical image database. In: CVPR, Ieee, pp 248–255'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng 等 [2009b] Deng J, Dong W, Socher R, 等 (2009b) Imagenet: 大规模层次图像数据库。在:
    CVPR, Ieee, pp 248–255'
- en: 'Donahue et al [2015] Donahue J, Anne Hendricks L, Guadarrama S, et al (2015)
    Long-term recurrent convolutional networks for visual recognition and description.
    In: CVPR, pp 2625–2634'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Donahue 等 [2015] Donahue J, Anne Hendricks L, Guadarrama S, 等 (2015) 用于视觉识别和描述的长期递归卷积网络.
    见: CVPR, 页 2625–2634'
- en: 'Du et al [2015] Du Y, Wang W, Wang L (2015) Hierarchical recurrent neural network
    for skeleton based action recognition. In: CVPR, pp 1110–1118'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等 [2015] Du Y, Wang W, Wang L (2015) 基于骨架的动作识别的层次递归神经网络. 见: CVPR, 页 1110–1118'
- en: 'Duan et al [2022] Duan H, Zhao Y, Chen K, et al (2022) Revisiting skeleton-based
    action recognition. In: CVPR, pp 2969–2978'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duan 等 [2022] Duan H, Zhao Y, Chen K, 等 (2022) 重新审视基于骨架的动作识别. 见: CVPR, 页 2969–2978'
- en: 'Eun et al [2020] Eun H, Moon J, Park J, et al (2020) Learning to discriminate
    information for online action detection. In: CVPR, pp 809–818'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Eun 等 [2020] Eun H, Moon J, Park J, 等 (2020) 学习区分在线动作检测的信息. 见: CVPR, 页 809–818'
- en: 'Fabbri et al [2018] Fabbri M, Lanzi F, Calderara S, et al (2018) Learning to
    detect and track visible and occluded body joints in a virtual world. In: ECCV'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fabbri 等 [2018] Fabbri M, Lanzi F, Calderara S, 等 (2018) 学习在虚拟世界中检测和跟踪可见及遮挡的身体关节.
    见: ECCV'
- en: 'Fan et al [2021] Fan H, Xiong B, Mangalam K, et al (2021) Multiscale vision
    transformers. In: ICCV, pp 6824–6835'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fan 等 [2021] Fan H, Xiong B, Mangalam K, 等 (2021) 多尺度视觉变换器. 见: ICCV, 页 6824–6835'
- en: 'Feichtenhofer et al [2016] Feichtenhofer C, Pinz A, Zisserman A (2016) Convolutional
    two-stream network fusion for video action recognition. In: CVPR, pp 1933–1941'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feichtenhofer 等 [2016] Feichtenhofer C, Pinz A, Zisserman A (2016) 卷积双流网络融合用于视频动作识别.
    见: CVPR, 页 1933–1941'
- en: 'Feichtenhofer et al [2017] Feichtenhofer C, Pinz A, Wildes RP (2017) Spatiotemporal
    multiplier networks for video action recognition. In: CVPR, pp 4768–4777'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feichtenhofer 等 [2017] Feichtenhofer C, Pinz A, Wildes RP (2017) 时空乘法网络用于视频动作识别.
    见: CVPR, 页 4768–4777'
- en: 'Feichtenhofer et al [2019] Feichtenhofer C, Fan H, Malik J, et al (2019) Slowfast
    networks for video recognition. In: ICCV, pp 6202–6211'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feichtenhofer 等 [2019] Feichtenhofer C, Fan H, Malik J, 等 (2019) Slowfast 网络用于视频识别.
    见: ICCV, 页 6202–6211'
- en: 'Gao et al [2020] Gao R, Oh TH, Grauman K, et al (2020) Listen to look: Action
    recognition by previewing audio. In: CVPR, pp 10457–10467'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等 [2020] Gao R, Oh TH, Grauman K, 等 (2020) 听以观之: 通过预览音频进行动作识别. 见: CVPR,
    页 10457–10467'
- en: 'Gavrilyuk et al [2018] Gavrilyuk K, Ghodrati A, Li Z, et al (2018) Actor and
    action video segmentation from a sentence. In: CVPR, pp 5958–5966'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gavrilyuk 等 [2018] Gavrilyuk K, Ghodrati A, Li Z, 等 (2018) 从句子中进行演员和动作视频分割.
    见: CVPR, 页 5958–5966'
- en: Girdhar and Ramanan [2017] Girdhar R, Ramanan D (2017) Attentional pooling for
    action recognition. Advances in Neural Information Processing Systems 30
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girdhar 和 Ramanan [2017] Girdhar R, Ramanan D (2017) 用于动作识别的注意力池化. 《神经信息处理系统进展》
    30
- en: 'Hamilton et al [2017] Hamilton WL, Ying R, Leskovec J (2017) Representation
    learning on graphs: Methods and applications. arXiv preprint arXiv:170905584'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hamilton 等 [2017] Hamilton WL, Ying R, Leskovec J (2017) 图上的表示学习: 方法与应用. arXiv
    预印本 arXiv:170905584'
- en: Hao et al [2021] Hao X, Li J, Guo Y, et al (2021) Hypergraph neural network
    for skeleton-based action recognition. TIP 30:2263–2275
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao 等 [2021] Hao X, Li J, Guo Y, 等 (2021) 基于骨架的动作识别的超图神经网络. TIP 30:2263–2275
- en: 'He et al [2019] He D, Zhou Z, Gan C, et al (2019) Stnet: Local and global spatial-temporal
    modeling for action recognition. In: AAAI, pp 8401–8408'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等 [2019] He D, Zhou Z, Gan C, 等 (2019) Stnet: 动作识别的局部和全局时空建模. 见: AAAI, 页
    8401–8408'
- en: 'He et al [2016] He K, Zhang X, Ren S, et al (2016) Deep residual learning for
    image recognition. In: CVPR, pp 770–778'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等 [2016] He K, Zhang X, Ren S, 等 (2016) 深度残差学习用于图像识别. 见: CVPR, 页 770–778'
- en: 'He et al [2017] He K, Gkioxari G, Dollár P, et al (2017) Mask r-cnn. In: ICCV,
    pp 2961–2969'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等 [2017] He K, Gkioxari G, Dollár P, 等 (2017) Mask r-cnn. 见: ICCV, 页 2961–2969'
- en: 'Herzig et al [2022] Herzig R, Ben-Avraham E, Mangalam K, et al (2022) Object-region
    video transformers. In: CVPR, pp 3148–3159'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Herzig 等 [2022] Herzig R, Ben-Avraham E, Mangalam K, 等 (2022) 对象区域视频变换器. 见:
    CVPR, 页 3148–3159'
- en: Horn and Schunck [1981] Horn BK, Schunck BG (1981) Determining optical flow.
    Artificial intelligence 17(1-3):185–203
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horn 和 Schunck [1981] Horn BK, Schunck BG (1981) 计算光流. 人工智能 17(1-3):185–203
- en: 'Hou et al [2021] Hou Q, Zhou D, Feng J (2021) Coordinate attention for efficient
    mobile network design. In: CVPR, pp 13713–13722'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hou 等 [2021] Hou Q, Zhou D, Feng J (2021) 协调注意力用于高效移动网络设计. 见: CVPR, 页 13713–13722'
- en: 'Hou et al [2019] Hou R, Ma B, Chang H, et al (2019) Vrstc: Occlusion-free video
    person re-identification. In: CVPR, pp 7183–7192'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hou 等 [2019] Hou R, Ma B, Chang H, 等 (2019) Vrstc: 无遮挡视频行人再识别. 见: CVPR, 页 7183–7192'
- en: 'Hu et al [2015] Hu JF, Zheng WS, Lai J, et al (2015) Jointly learning heterogeneous
    features for rgb-d activity recognition. In: CVPR, pp 5344–5352'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2015] Hu JF, Zheng WS, Lai J, 等 (2015) 共同学习异质特征用于RGB-D活动识别. 发表在：CVPR,
    第5344–5352页
- en: 'Hu et al [2021] Hu L, Zhang P, Zhang B, et al (2021) Learning position and
    target consistency for memory-based video object segmentation. In: CVPR, pp 4144–4154'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2021] Hu L, Zhang P, Zhang B, 等 (2021) 学习位置与目标一致性以实现基于记忆的视频目标分割. 发表在：CVPR,
    第4144–4154页
- en: 'Hu et al [2018] Hu YT, Huang JB, Schwing AG (2018) Videomatch: Matching based
    video object segmentation. In: ECCV, pp 54–70'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2018] Hu YT, Huang JB, Schwing AG (2018) Videomatch：基于匹配的视频目标分割. 发表在：ECCV,
    第54–70页
- en: 'Huang et al [2020] Huang X, Xu J, Tai YW, et al (2020) Fast video object segmentation
    with temporal aggregation network and dynamic template matching. In: CVPR, pp
    8879–8889'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2020] Huang X, Xu J, Tai YW, 等 (2020) 快速视频目标分割与时间聚合网络和动态模板匹配. 发表在：CVPR,
    第8879–8889页
- en: 'Huang et al [2017] Huang Z, Wan C, Probst T, et al (2017) Deep learning on
    lie groups for skeleton-based action recognition. In: CVPR, pp 6099–6108'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2017] Huang Z, Wan C, Probst T, 等 (2017) 基于Lie群的骨架动作识别的深度学习. 发表在：CVPR,
    第6099–6108页
- en: Hussain et al [2021] Hussain T, Muhammad K, Ding W, et al (2021) A comprehensive
    survey of multi-view video summarization. Pattern Recognition 109:107567
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hussain 等人 [2021] Hussain T, Muhammad K, Ding W, 等 (2021) 多视角视频总结的综合调查. 计算机模式识别
    109:107567
- en: 'Hussein et al [2019] Hussein N, Gavves E, Smeulders AW (2019) Timeception for
    complex action recognition. In: CVPR'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hussein 等人 [2019] Hussein N, Gavves E, Smeulders AW (2019) 用于复杂动作识别的时间感知. 发表在：CVPR
- en: 'Iqbal et al [2017] Iqbal U, Garbade M, Gall J (2017) Pose for action-action
    for pose. In: International Conference on Automatic Face & Gesture Recognition,
    IEEE, pp 438–445'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iqbal 等人 [2017] Iqbal U, Garbade M, Gall J (2017) 姿势与动作：动作与姿势. 发表在：国际自动面部与姿势识别会议,
    IEEE, 第438–445页
- en: Ji et al [2021] Ji Y, Yang Y, Shen HT, et al (2021) View-invariant action recognition
    via unsupervised attention transfer (uant). Pattern Recognition 113:107807
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等人 [2021] Ji Y, Yang Y, Shen HT, 等 (2021) 基于无监督注意力转移（uant）的视图不变动作识别. 计算机模式识别
    113:107807
- en: 'Jing and Tian [2020] Jing L, Tian Y (2020) Self-supervised visual feature learning
    with deep neural networks: A survey. PAMI'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jing 和 Tian [2020] Jing L, Tian Y (2020) 基于深度神经网络的自监督视觉特征学习：综述. PAMI
- en: 'Johnander et al [2019] Johnander J, Danelljan M, Brissman E, et al (2019) A
    generative appearance model for end-to-end video object segmentation. In: CVPR,
    pp 8953–8962'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnander 等人 [2019] Johnander J, Danelljan M, Brissman E, 等 (2019) 用于端到端视频目标分割的生成外观模型.
    发表在：CVPR, 第8953–8962页
- en: 'Kapoor et al [2021] Kapoor R, Sharma D, Gulati T (2021) State of the art content
    based image retrieval techniques using deep learning: a survey. Multimedia Tools
    and Applications 80(19):29561–29583'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapoor 等人 [2021] Kapoor R, Sharma D, Gulati T (2021) 基于内容的图像检索技术的深度学习现状：综述.
    多媒体工具与应用 80(19):29561–29583
- en: 'Karbalaie et al [2022] Karbalaie A, Abtahi F, Sjöström M (2022) Event detection
    in surveillance videos: a review. Multimedia Tools and Applications 81(24):35463–35501'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karbalaie 等人 [2022] Karbalaie A, Abtahi F, Sjöström M (2022) 监控视频中的事件检测：综述.
    多媒体工具与应用 81(24):35463–35501
- en: 'Karpathy et al [2014] Karpathy A, Toderici G, Shetty S, et al (2014) Large-scale
    video classification with convolutional neural networks. In: CVPR'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpathy 等人 [2014] Karpathy A, Toderici G, Shetty S, 等 (2014) 大规模视频分类与卷积神经网络.
    发表在：CVPR
- en: 'Ke et al [2021a] Ke L, Tai YW, Tang CK (2021a) Deep occlusion-aware instance
    segmentation with overlapping bilayers. In: CVPR, pp 4019–4028'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke 等人 [2021a] Ke L, Tai YW, Tang CK (2021a) 深度遮挡感知实例分割与重叠双层. 发表在：CVPR, 第4019–4028页
- en: 'Ke et al [2021b] Ke L, Tai YW, Tang CK (2021b) Occlusion-aware video object
    inpainting. In: ICCV, pp 14468–14478'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke 等人 [2021b] Ke L, Tai YW, Tang CK (2021b) 遮挡感知的视频目标修复. 发表在：ICCV, 第14468–14478页
- en: 'Ke et al [2017] Ke Q, Bennamoun M, An S, et al (2017) A new representation
    of skeleton sequences for 3d action recognition. In: CVPR, pp 3288–3297'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke 等人 [2017] Ke Q, Bennamoun M, An S, 等 (2017) 用于3D动作识别的骨架序列新表示. 发表在：CVPR, 第3288–3297页
- en: Kim et al [2021] Kim J, Li G, Yun I, et al (2021) Weakly-supervised temporal
    attention 3d network for human action recognition. Pattern Recognition 119:108068
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人 [2021] Kim J, Li G, Yun I, 等 (2021) 弱监督时间注意力3D网络用于人体动作识别. 计算机模式识别 119:108068
- en: 'Kim and Reiter [2017] Kim TS, Reiter A (2017) Interpretable 3d human action
    analysis with temporal convolutional networks. In: CVPR Workshop, IEEE, pp 1623–1631'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 和 Reiter [2017] Kim TS, Reiter A (2017) 基于时间卷积网络的可解释3D人体动作分析. 发表在：CVPR研讨会,
    IEEE, 第1623–1631页
- en: 'Kniaz et al [2018] Kniaz VV, Knyaz VA, Hladuvka J, et al (2018) Thermalgan:
    Multimodal color-to-thermal image translation for person re-identification in
    multispectral dataset. In: ECCV Workshops, pp 0–0'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kniaz 等人 [2018] Kniaz VV, Knyaz VA, Hladuvka J, 等 (2018) Thermalgan: 多模态颜色到热图像翻译用于多光谱数据集中的人脸重识别。见:
    ECCV Workshops, 第 0–0 页'
- en: 'Kong et al [2017] Kong Y, Tao Z, Fu Y (2017) Deep sequential context networks
    for action prediction. In: CVPR, pp 1473–1481'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kong 等人 [2017] Kong Y, Tao Z, Fu Y (2017) 用于动作预测的深度序列上下文网络。见: CVPR, 第 1473–1481
    页'
- en: Kong et al [2018] Kong Y, Tao Z, Fu Y (2018) Adversarial action prediction networks.
    PAMI 42(3):539–553
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kong 等人 [2018] Kong Y, Tao Z, Fu Y (2018) 对抗性动作预测网络。PAMI 42(3):539–553
- en: 'Korbar et al [2019] Korbar B, Tran D, Torresani L (2019) Scsampler: Sampling
    salient clips from video for efficient action recognition. In: ICCV, pp 6232–6242'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Korbar 等人 [2019] Korbar B, Tran D, Torresani L (2019) Scsampler: 从视频中采样显著剪辑以提高动作识别效率。见:
    ICCV, 第 6232–6242 页'
- en: 'Li et al [2017a] Li B, Dai Y, Cheng X, et al (2017a) Skeleton based action
    recognition using translation-scale invariant image mapping and multi-scale deep
    cnn. In: International Conference on Multimedia & Expo Workshops (ICMEW), IEEE,
    pp 601–604'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2017a] Li B, Dai Y, Cheng X, 等 (2017a) 基于骨架的动作识别使用平移尺度不变图像映射和多尺度深度 cnn。见:
    国际多媒体与博览会研讨会 (ICMEW), IEEE, 第 601–604 页'
- en: 'Li et al [2019a] Li B, Li X, Zhang Z, et al (2019a) Spatio-temporal graph routing
    for skeleton-based action recognition. In: AAAI, pp 8561–8568'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2019a] Li B, Li X, Zhang Z, 等 (2019a) 基于骨架的动作识别的时空图路由。见: AAAI, 第 8561–8568
    页'
- en: 'Li et al [2017b] Li C, Zhong Q, Xie D, et al (2017b) Skeleton-based action
    recognition with convolutional neural networks. In: International Conference on
    Multimedia & Expo Workshops, IEEE, pp 597–600'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2017b] Li C, Zhong Q, Xie D, 等 (2017b) 使用卷积神经网络的基于骨架的动作识别。见: 国际多媒体与博览会研讨会,
    IEEE, 第 597–600 页'
- en: Li et al [2020a] Li J, Liu X, Zhang W, et al (2020a) Spatio-temporal attention
    networks for action recognition and detection. IEEE Transactions on Multimedia
    22(11):2990–3001
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2020a] Li J, Liu X, Zhang W, 等 (2020a) 动作识别和检测的时空注意力网络。IEEE 多媒体学报 22(11):2990–3001
- en: Li et al [2018a] Li L, Zheng W, Zhang Z, et al (2018a) Skeleton-based relational
    modeling for action recognition. arXiv preprint arXiv:180502556 1(2):3
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2018a] Li L, Zheng W, Zhang Z, 等 (2018a) 基于骨架的关系建模用于动作识别。arXiv 预印本 arXiv:180502556
    1(2):3
- en: 'Li et al [2019b] Li M, Chen S, Chen X, et al (2019b) Actional-structural graph
    convolutional networks for skeleton-based action recognition. In: CVPR, pp 3595–3603'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2019b] Li M, Chen S, Chen X, 等 (2019b) 基于骨架的动作识别的动作结构图卷积网络。见: CVPR,
    第 3595–3603 页'
- en: 'Li et al [2022a] Li M, Hu L, Xiong Z, et al (2022a) Recurrent dynamic embedding
    for video object segmentation. In: CVPR, pp 1332–1341'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2022a] Li M, Hu L, Xiong Z, 等 (2022a) 视频目标分割的递归动态嵌入。见: CVPR, 第 1332–1341
    页'
- en: 'Li et al [2018b] Li S, Bak S, Carr P, et al (2018b) Diversity regularized spatiotemporal
    attention for video-based person re-identification. In: CVPR'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2018b] Li S, Bak S, Carr P, 等 (2018b) 视频中的多样性正则化时空注意力用于人脸重识别。见: CVPR'
- en: 'Li et al [2020b] Li S, Jiang T, Huang T, et al (2020b) Global co-occurrence
    feature learning and active coordinate system conversion for skeleton-based action
    recognition. In: WACV, pp 586–594'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2020b] Li S, Jiang T, Huang T, 等 (2020b) 基于骨架的动作识别的全局共现特征学习与主动坐标系统转换。见:
    WACV, 第 586–594 页'
- en: 'Li et al [2022b] Li X, Liu C, Shuai B, et al (2022b) Nuta: Non-uniform temporal
    aggregation for action recognition. In: WACV, pp 3683–3692'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2022b] Li X, Liu C, Shuai B, 等 (2022b) Nuta: 用于动作识别的非均匀时间聚合。见: WACV,
    第 3683–3692 页'
- en: 'Li et al [2018c] Li Y, Li Y, Vasconcelos N (2018c) Resound: Towards action
    recognition without representation bias. In: ECCV, pp 513–528'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2018c] Li Y, Li Y, Vasconcelos N (2018c) Resound: 朝着无表示偏见的动作识别。见: ECCV,
    第 513–528 页'
- en: Li et al [2018d] Li Y, Yang M, Zhang Z (2018d) A survey of multi-view representation
    learning. Transactions on knowledge and data engineering 31(10):1863–1883
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2018d] Li Y, Yang M, Zhang Z (2018d) 多视角表示学习的综述。知识与数据工程学报 31(10):1863–1883
- en: Li et al [2020c] Li Y, Xia R, Liu X (2020c) Learning shape and motion representations
    for view invariant skeleton-based action recognition. Pattern Recognition 103:107293
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2020c] Li Y, Xia R, Liu X (2020c) 学习形状和运动表示以实现视角不变的骨架动作识别。模式识别 103:107293
- en: 'Li et al [2021] Li Y, He J, Zhang T, et al (2021) Diverse part discovery: Occluded
    person re-identification with part-aware transformer. In: CVPR, pp 2898–2907'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2021] Li Y, He J, Zhang T, 等 (2021) 多样化部件发现：带有部件感知变换器的遮挡人脸重识别。见: CVPR,
    第 2898–2907 页'
- en: Li et al [2018e] Li Z, Gavrilyuk K, Gavves E, et al (2018e) Videolstm convolves,
    attends and flows for action recognition. Computer Vision and Image Understanding
    166:41–50
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2018e] 李梓, Gavrilyuk K, Gavves E, 等 (2018e) 用于动作识别的 VideoLSTM 卷积、关注和流.
    计算机视觉与图像理解 166:41–50
- en: 'Liang et al [2019] Liang J, Jiang L, Niebles JC, et al (2019) Peeking into
    the future: Predicting future person activities and locations in videos. In: CVPR,
    pp 5725–5734'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang 等人 [2019] 梁江, 蒋梁, Niebles JC, 等 (2019) 窥探未来: 在视频中预测未来的人物活动和位置. In: CVPR,
    pp 5725–5734'
- en: 'Liang et al [2018] Liang W, Zhu Y, Zhu SC (2018) Tracking occluded objects
    and recovering incomplete trajectories by reasoning about containment relations
    and human actions. In: AAAI'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang 等人 [2018] 梁伟, 朱宇, 朱少权 (2018) 追踪被遮挡物体并通过思考容器关系和人类动作恢复缺失轨迹. In: AAAI'
- en: Liang et al [2020] Liang Y, Li X, Jafari N, et al (2020) Video object segmentation
    with adaptive feature bank and uncertain-region refinement. NIPS 33:3430–3441
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 [2020] 梁娱, 李新, Jafari N, 等 (2020) 自适应特征库和不确定区域细化的视频目标分割. NIPS 33:3430–3441
- en: 'Lin et al [2019a] Lin H, Qi X, Jia J (2019a) Agss-vos: Attention guided single-shot
    video object segmentation. In: ICCV, pp 3949–3957'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 [2019a] 林航, 齐心, 贾骏 (2019a) Agss-vos: 注意引导的单 shot 视频目标分割. In: ICCV, pp
    3949–3957'
- en: 'Lin et al [2019b] Lin J, Gan C, Han S (2019b) Tsm: Temporal shift module for
    efficient video understanding. In: ICCV, pp 7083–7093'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 [2019b] 林健伟, 甘超, 韩思 (2019b) TSM: 用于高效视频理解的时序移位模块. In: ICCV, pp 7083–7093'
- en: 'Lin et al [2022a] Lin S, Xie H, Wang B, et al (2022a) Knowledge distillation
    via the target-aware transformer. In: CVPR, pp 10915–10924'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 [2022a] 林胜, 谢华, 王斌, 等 (2022a) 通过目标感知 Transformer 进行知识蒸馏. In: CVPR, pp
    10915–10924'
- en: 'Lin et al [2022b] Lin Z, Yang T, Li M, et al (2022b) Swem: Towards real-time
    video object segmentation with sequential weighted expectation-maximization. In:
    CVPR, pp 1362–1372'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 [2022b] 林杰, 杨涛, 李敏, 等 (2022b) Swem: 基于序列加权期望最大化的实时视频目标分割. In: CVPR,
    pp 1362–1372'
- en: 'Liu et al [2020a] Liu D, Cui Y, Chen Y, et al (2020a) Video object detection
    for autonomous driving: Motion-aid feature calibration. Neurocomputing 409:1–11'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al [2020a] 刘东, 崔燕, 陈玉, 等 (2020a) 自动驾驶的视频目标检测: 运动辅助特征校准. Neurocomputing
    409:1–11'
- en: 'Liu et al [2021a] Liu D, Cui Y, Tan W, et al (2021a) Sg-net: Spatial granularity
    network for one-stage video instance segmentation. In: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp 9816–9825'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2021a] 刘东, 崔燕, 谭伟, 等 (2021a) Sg-net: 用于一阶段视频实例分割的空间粒度网络. In: IEEE/CVF
    计算机视觉与模式识别会议论文集, pp 9816–9825'
- en: 'Liu et al [2016] Liu J, Shahroudy A, Xu D, et al (2016) Spatio-temporal lstm
    with trust gates for 3d human action recognition. In: ECCV, Springer, pp 816–833'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2016] 刘杰, Shahroudy A, 徐冬, 等 (2016) 具有信任门的时空 LSTM 用于 3D 人体动作识别. In:
    ECCV, Springer, pp 816–833'
- en: 'Liu et al [2017a] Liu J, Akhtar N, Mian A (2017a) Viewpoint invariant rgb-d
    human action recognition. In: International Conference on Digital Image Computing:
    Techniques and Applications, IEEE, pp 1–8'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2017a] 刘杰, Akhtar N, Mian A (2017a) 视角不变的 RGB-D 人体动作识别. In: 国际数字图像计算会议:技术与应用,
    IEEE, pp 1–8'
- en: Liu et al [2017b] Liu J, Wang G, Duan LY, et al (2017b) Skeleton-based human
    action recognition with global context-aware attention lstm networks. TIP 27(4):1586–1599
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2017b] 刘杰, 王刚, 段兰云,等 (2017b) 基于骨架的全局上下文关注 lstm 网络的人体动作识别. TIP 27(4):1586–1599
- en: 'Liu et al [2017c] Liu J, Wang G, Hu P, et al (2017c) Global context-aware attention
    lstm networks for 3d action recognition. In: CVPR, pp 1647–1656'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2017c] 刘杰, 王刚, 胡鹏, 等 (2017c) 为 3D 动作识别而设计的全局上下文感知 attention lstm 网络.
    In: CVPR, pp 1647–1656'
- en: 'Liu and Yuan [2018] Liu M, Yuan J (2018) Recognizing human actions as the evolution
    of pose estimation maps. In: CVPR, pp 1159–1168'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu and Yuan [2018] 刘鸣, 袁杰 (2018) 将人体动作识别视为姿态估计图的演化. In: CVPR, pp 1159–1168'
- en: Liu et al [2017d] Liu M, Liu H, Chen C (2017d) Enhanced skeleton visualization
    for view invariant human action recognition. Pattern Recognition 68:346–362
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2017d] 刘梦茜, 刘华, 陈超 (2017d) 用于视角不变的人体动作识别的增强骨架可视化. 图像识别 68:346–362
- en: Liu et al [2021b] Liu Y, Wang K, Li G, et al (2021b) Semantics-aware adaptive
    knowledge distillation for sensor-to-vision action recognition. TIP 30:5573–5588
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2021b] 刘洋, 王凯, 李刚, 等 (2021b) 针对传感器到视觉动作识别的语义感知自适应知识蒸馏. TIP 30:5573–5588
- en: 'Liu et al [2020b] Liu Z, Zhang H, Chen Z, et al (2020b) Disentangling and unifying
    graph convolutions for skeleton-based action recognition. In: CVPR'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2020b] 刘正, 张鸿, 陈哲, 等 (2020b) 解交并统一图卷积进行基于骨架的动作识别. In: CVPR'
- en: 'Liu et al [2022] Liu Z, Ning J, Cao Y, et al (2022) Video swin transformer.
    In: CVPR, pp 3202–3211'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2022] Liu Z, Ning J, Cao Y, 等 (2022) 视频 swin transformer. 见：CVPR, 第 3202–3211
    页
- en: 'Lu et al [2023] Lu Y, Wang Q, Ma S, et al (2023) Transflow: Transformer as
    flow learner. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp 18063–18073'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等 [2023] Lu Y, Wang Q, Ma S, 等 (2023) Transflow: Transformer 作为流学习者. 见：IEEE/CVF
    计算机视觉与模式识别会议论文集, 第 18063–18073 页'
- en: 'Luo and Yuille [2019] Luo C, Yuille AL (2019) Grouped spatial-temporal aggregation
    for efficient action recognition. In: ICCV, pp 5512–5521'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 和 Yuille [2019] Luo C, Yuille AL (2019) 分组时空聚合用于高效动作识别. 见：ICCV, 第 5512–5521
    页
- en: Luvizon et al [2020] Luvizon DC, Picard D, Tabia H (2020) Multi-task deep learning
    for real-time 3d human pose estimation and action recognition. PAMI 43(8):2752–2764
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luvizon 等 [2020] Luvizon DC, Picard D, Tabia H (2020) 实时 3D 人体姿态估计和动作识别的多任务深度学习.
    PAMI 43(8):2752–2764
- en: Lv et al [2022] Lv Z, Ota K, Lloret J, et al (2022) Complexity problems handled
    by advanced computer simulation technology in smart cities 2021
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv 等 [2022] Lv Z, Ota K, Lloret J, 等 (2022) 智能城市中由高级计算机模拟技术处理的复杂问题 2021
- en: 'Ma et al [2021] Ma J, Jiang X, Fan A, et al (2021) Image matching from handcrafted
    to deep features: A survey. IJCV 129(1):23–79'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等 [2021] Ma J, Jiang X, Fan A, 等 (2021) 从手工特征到深度特征的图像匹配：调查. IJCV 129(1):23–79
- en: 'Meng et al [2020] Meng Y, Lin CC, Panda R, et al (2020) Ar-net: Adaptive frame
    resolution for efficient action recognition. In: ECCV, Springer, pp 86–104'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Meng 等 [2020] Meng Y, Lin CC, Panda R, 等 (2020) Ar-net: 自适应帧分辨率用于高效动作识别. 见：ECCV,
    Springer, 第 86–104 页'
- en: 'Minaee et al [2021] Minaee S, Boykov YY, Porikli F, et al (2021) Image segmentation
    using deep learning: A survey. PAMI'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee 等 [2021] Minaee S, Boykov YY, Porikli F, 等 (2021) 使用深度学习的图像分割：调查. PAMI
- en: 'Neimark et al [2021] Neimark D, Bar O, Zohar M, et al (2021) Video transformer
    network. In: ICCV, pp 3163–3172'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neimark 等 [2021] Neimark D, Bar O, Zohar M, 等 (2021) 视频 transformer 网络. 见：ICCV,
    第 3163–3172 页
- en: 'Oh et al [2019a] Oh SW, Lee JY, Xu N, et al (2019a) Fast user-guided video
    object segmentation by interaction-and-propagation networks. In: CVPR, pp 5247–5256'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh 等 [2019a] Oh SW, Lee JY, Xu N, 等 (2019a) 通过交互与传播网络实现快速用户引导的视频目标分割. 见：CVPR,
    第 5247–5256 页
- en: 'Oh et al [2019b] Oh SW, Lee JY, Xu N, et al (2019b) Video object segmentation
    using space-time memory networks. In: ICCV, pp 9226–9235'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh 等 [2019b] Oh SW, Lee JY, Xu N, 等 (2019b) 使用时空记忆网络进行视频目标分割. 见：ICCV, 第 9226–9235
    页
- en: 'Ouyang and Wang [2012] Ouyang W, Wang X (2012) A discriminative deep model
    for pedestrian detection with occlusion handling. In: CVPR, IEEE, pp 3258–3265'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 和 Wang [2012] Ouyang W, Wang X (2012) 一种用于处理遮挡的行人检测判别性深度模型. 见：CVPR, IEEE,
    第 3258–3265 页
- en: 'Ouyang and Wang [2013] Ouyang W, Wang X (2013) Joint deep learning for pedestrian
    detection. In: ICCV, pp 2056–2063'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 和 Wang [2013] Ouyang W, Wang X (2013) 联合深度学习用于行人检测. 见：ICCV, 第 2056–2063
    页
- en: 'Park et al [2022] Park K, Woo S, Oh SW, et al (2022) Per-clip video object
    segmentation. In: CVPR, pp 1352–1361'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 [2022] Park K, Woo S, Oh SW, 等 (2022) 每剪辑视频目标分割. 见：CVPR, 第 1352–1361
    页
- en: 'Patrick et al [2021] Patrick M, Campbell D, Asano Y, et al (2021) Keeping your
    eye on the ball: Trajectory attention in video transformers. NIPS 34:12493–12506'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patrick 等 [2021] Patrick M, Campbell D, Asano Y, 等 (2021) 关注球的轨迹：视频 transformers
    中的轨迹注意力. NIPS 34:12493–12506
- en: 'Peng et al [2020] Peng W, Hong X, Chen H, et al (2020) Learning graph convolutional
    network for skeleton-based human action recognition by neural searching. In: AAAI,
    pp 2669–2676'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等 [2020] Peng W, Hong X, Chen H, 等 (2020) 通过神经搜索学习图卷积网络用于基于骨架的人体动作识别. 见：AAAI,
    第 2669–2676 页
- en: Pexels [n.d.] Pexels (n.d.) Pexels. URL [https://www.pexels.com/](https://www.pexels.com/),
    accessed November 9, 2023
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pexels [n.d.] Pexels (无日期) Pexels. 网址 [https://www.pexels.com/](https://www.pexels.com/),
    访问日期 2023 年 11 月 9 日
- en: 'Piasco et al [2018] Piasco N, Sidibé D, Demonceaux C, et al (2018) A survey
    on visual-based localization: On the benefit of heterogeneous data. Pattern Recognition
    74:90–109'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Piasco 等 [2018] Piasco N, Sidibé D, Demonceaux C, 等 (2018) 基于视觉的定位调查：异质数据的好处.
    Pattern Recognition 74:90–109
- en: Pont-Tuset et al [2017] Pont-Tuset J, Perazzi F, Caelles S, et al (2017) The
    2017 davis challenge on video object segmentation. arXiv preprint arXiv:170400675
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pont-Tuset 等 [2017] Pont-Tuset J, Perazzi F, Caelles S, 等 (2017) 2017 年 Davis
    视频目标分割挑战赛. arXiv 预印本 arXiv:170400675
- en: 'Qin et al [2020] Qin X, Ge Y, Feng J, et al (2020) Dtmmn: Deep transfer multi-metric
    network for rgb-d action recognition. Neurocomputing 406:127–134'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin 等 [2020] Qin X, Ge Y, Feng J, 等 (2020) Dtmmn: 深度迁移多度量网络用于 RGB-D 动作识别. Neurocomputing
    406:127–134'
- en: Qin et al [2023] Qin Z, Lu X, Nie X, et al (2023) Coarse-to-fine video instance
    segmentation with factorized conditional appearance flows. IEEE/CAA Journal of
    Automatica Sinica 10(5):1192–1208
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人 [2023] Qin Z, Lu X, Nie X, 等人 (2023) 从粗到细的视频实例分割与分解的条件外观流。IEEE/CAA 自动化学报
    10(5):1192–1208
- en: 'Ren et al [2021] Ren S, Liu W, Liu Y, et al (2021) Reciprocal transformations
    for unsupervised video object segmentation. In: CVPR, pp 15455–15464'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人 [2021] Ren S, Liu W, Liu Y, 等人 (2021) 递归变换用于无监督视频对象分割。在：CVPR，pp 15455–15464
- en: 'Robinson et al [2020] Robinson A, Lawin FJ, Danelljan M, et al (2020) Learning
    fast and robust target models for video object segmentation. In: CVPR, pp 7406–7415'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robinson 等人 [2020] Robinson A, Lawin FJ, Danelljan M, 等人 (2020) 快速且稳健的目标模型学习用于视频目标分割。在：CVPR，pp
    7406–7415
- en: 'Seo et al [2020] Seo S, Lee JY, Han B (2020) Urvos: Unified referring video
    object segmentation network with a large-scale benchmark. In: ECCV, Springer,
    pp 208–223'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seo 等人 [2020] Seo S, Lee JY, Han B (2020) URVOS：具有大规模基准的统一视频对象分割网络。在：ECCV，Springer，pp
    208–223
- en: 'Shahroudy et al [2016] Shahroudy A, Liu J, Ng TT, et al (2016) Ntu rgb+ d:
    A large scale dataset for 3d human activity analysis. In: CVPR, pp 1010–1019'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shahroudy 等人 [2016] Shahroudy A, Liu J, Ng TT, 等人 (2016) NTU RGB+D：用于3D人类活动分析的大规模数据集。在：CVPR，pp
    1010–1019
- en: Sharma et al [2015] Sharma S, Kiros R, Salakhutdinov R (2015) Action recognition
    using visual attention. arXiv preprint arXiv:151104119
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人 [2015] Sharma S, Kiros R, Salakhutdinov R (2015) 使用视觉注意力的动作识别。arXiv
    预印本 arXiv:151104119
- en: 'Shi et al [2019a] Shi L, Zhang Y, Cheng J, et al (2019a) Skeleton-based action
    recognition with directed graph neural networks. In: CVPR, pp 7912–7921'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人 [2019a] Shi L, Zhang Y, Cheng J, 等人 (2019a) 基于骨架的动作识别与定向图神经网络。在：CVPR，pp
    7912–7921
- en: 'Shi et al [2019b] Shi L, Zhang Y, Cheng J, et al (2019b) Two-stream adaptive
    graph convolutional networks for skeleton-based action recognition. In: CVPR'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人 [2019b] Shi L, Zhang Y, Cheng J, 等人 (2019b) 用于基于骨架的动作识别的双流自适应图卷积网络。在：CVPR
- en: 'Shi et al [2020a] Shi L, Zhang Y, Cheng J, et al (2020a) Decoupled spatial-temporal
    attention network for skeleton-based action-gesture recognition. In: Proceedings
    of the Asian Conference on Computer Vision'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人 [2020a] Shi L, Zhang Y, Cheng J, 等人 (2020a) 解耦的空间-时间注意力网络用于基于骨架的动作-手势识别。在：亚洲计算机视觉会议论文集
- en: Shi et al [2020b] Shi L, Zhang Y, Cheng J, et al (2020b) Skeleton-based action
    recognition with multi-stream adaptive graph convolutional networks. TIP 29:9532–9545
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人 [2020b] Shi L, Zhang Y, Cheng J, 等人 (2020b) 基于骨架的动作识别与多流自适应图卷积网络。TIP
    29:9532–9545
- en: 'Shou et al [2017] Shou Z, Chan J, Zareian A, et al (2017) Cdc: Convolutional-de-convolutional
    networks for precise temporal action localization in untrimmed videos. In: CVPR'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shou 等人 [2017] Shou Z, Chan J, Zareian A, 等人 (2017) CDC：卷积-反卷积网络用于不修剪视频中的精确时间动作定位。在：CVPR
- en: 'Si et al [2019] Si C, Chen W, Wang W, et al (2019) An attention enhanced graph
    convolutional lstm network for skeleton-based action recognition. In: CVPR, pp
    1227–1236'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Si 等人 [2019] Si C, Chen W, Wang W, 等人 (2019) 一种增强注意力的图卷积LSTM网络用于基于骨架的动作识别。在：CVPR，pp
    1227–1236
- en: Simonyan and Zisserman [2014a] Simonyan K, Zisserman A (2014a) Two-stream convolutional
    networks for action recognition in videos. arXiv preprint arXiv:14062199
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman [2014a] Simonyan K, Zisserman A (2014a) 用于视频动作识别的双流卷积网络。arXiv
    预印本 arXiv:14062199
- en: Simonyan and Zisserman [2014b] Simonyan K, Zisserman A (2014b) Very deep convolutional
    networks for large-scale image recognition. arXiv preprint arXiv:14091556
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman [2014b] Simonyan K, Zisserman A (2014b) 用于大规模图像识别的非常深卷积网络。arXiv
    预印本 arXiv:14091556
- en: 'Song et al [2021] Song L, Yu G, Yuan J, et al (2021) Human pose estimation
    and its application to action recognition: A survey. Journal of Visual Communication
    and Image Representation p 103055'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 [2021] Song L, Yu G, Yuan J, 等人 (2021) 人体姿态估计及其在动作识别中的应用：综述。视觉通信与图像表示杂志
    p 103055
- en: 'Song et al [2019] Song YF, Zhang Z, Wang L (2019) Richly activated graph convolutional
    network for action recognition with incomplete skeletons. In: ICIP, IEEE, pp 1–5'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 [2019] Song YF, Zhang Z, Wang L (2019) 富激活图卷积网络用于不完整骨架的动作识别。在：ICIP，IEEE，pp
    1–5
- en: 'Soomro et al [2012] Soomro K, Zamir AR, Shah M (2012) Ucf101: A dataset of
    101 human actions classes from videos in the wild. arXiv preprint arXiv:12120402'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Soomro 等人 [2012] Soomro K, Zamir AR, Shah M (2012) Ucf101：一个包含101个人体动作类别的野外视频数据集。arXiv
    预印本 arXiv:12120402
- en: 'de Souza Reis et al [2021] de Souza Reis E, Seewald LA, Antunes RS, et al (2021)
    Monocular multi-person pose estimation: A survey. Pattern Recognition p 108046'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Souza Reis 等人 [2021] de Souza Reis E, Seewald LA, Antunes RS, 等人 (2021) 单目多人物姿态估计：综述。模式识别
    p 108046
- en: 'Su et al [2020] Su L, Hu C, Li G, et al (2020) Msaf: Multimodal split attention
    fusion. arXiv preprint arXiv:201207175'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等 [2020] Su L, Hu C, Li G, 等 (2020) Msaf：多模态分裂注意力融合。arXiv 预印本 arXiv:201207175
- en: 'Sudhakaran et al [2020] Sudhakaran S, Escalera S, Lanz O (2020) Gate-shift
    networks for video action recognition. In: CVPR, pp 1102–1111'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sudhakaran 等 [2020] Sudhakaran S, Escalera S, Lanz O (2020) 门控移位网络用于视频动作识别。见：CVPR，第
    1102–1111 页
- en: 'Sun et al [2020] Sun M, Xiao J, Lim EG, et al (2020) Fast template matching
    and update for video object tracking and segmentation. In: CVPR, pp 10791–10799'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 [2020] Sun M, Xiao J, Lim EG, 等 (2020) 视频目标跟踪和分割的快速模板匹配和更新。见：CVPR，第 10791–10799
    页
- en: Thakkar and Narayanan [2018] Thakkar K, Narayanan P (2018) Part-based graph
    convolutional network for action recognition. arXiv preprint arXiv:180904983
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thakkar 和 Narayanan [2018] Thakkar K, Narayanan P (2018) 基于部件的图卷积网络用于动作识别。arXiv
    预印本 arXiv:180904983
- en: 'Tian et al [2015] Tian Y, Luo P, Wang X, et al (2015) Deep learning strong
    parts for pedestrian detection. In: ICCV, pp 1904–1912'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等 [2015] Tian Y, Luo P, Wang X, 等 (2015) 深度学习强部件用于行人检测。见：ICCV，第 1904–1912
    页
- en: 'Tran and Cheong [2017] Tran A, Cheong LF (2017) Two-stream flow-guided convolutional
    attention networks for action recognition. In: ICCV Workshops, pp 3110–3119'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 和 Cheong [2017] Tran A, Cheong LF (2017) 双流流引导卷积注意力网络用于动作识别。见：ICCV Workshops，第
    3110–3119 页
- en: 'Tran et al [2015] Tran D, Bourdev L, Fergus R, et al (2015) Learning spatiotemporal
    features with 3d convolutional networks. In: ICCV, pp 4489–4497'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等 [2015] Tran D, Bourdev L, Fergus R, 等 (2015) 使用 3D 卷积网络学习时空特征。见：ICCV，第
    4489–4497 页
- en: 'Tran et al [2019] Tran D, Wang H, Torresani L, et al (2019) Video classification
    with channel-separated convolutional networks. In: ICCV, pp 5552–5561'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 等 [2019] Tran D, Wang H, Torresani L, 等 (2019) 使用通道分离卷积网络进行视频分类。见：ICCV，第
    5552–5561 页
- en: 'Truong et al [2022] Truong TD, Bui QH, Duong CN, et al (2022) Direcformer:
    A directed attention in transformer approach to robust action recognition. In:
    CVPR, pp 20030–20040'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Truong 等 [2022] Truong TD, Bui QH, Duong CN, 等 (2022) Direcformer：一种在 Transformer
    中的定向注意力方法用于稳健的动作识别。见：CVPR，第 20030–20040 页
- en: 'Ullah et al [2021] Ullah A, Muhammad K, Hussain T, et al (2021) Conflux lstms
    network: A novel approach for multi-view action recognition. Neurocomputing 435:321–329'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ullah 等 [2021] Ullah A, Muhammad K, Hussain T, 等 (2021) Conflux lstms 网络：一种用于多视角动作识别的新方法。Neurocomputing
    435:321–329
- en: Vaswani et al [2017] Vaswani A, Shazeer N, Parmar N, et al (2017) Attention
    is all you need. NIPS 30
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 [2017] Vaswani A, Shazeer N, Parmar N, 等 (2017) 注意力即你所需。NIPS 30
- en: 'Veeriah et al [2015] Veeriah V, Zhuang N, Qi GJ (2015) Differential recurrent
    neural networks for action recognition. In: ICCV, pp 4041–4049'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veeriah 等 [2015] Veeriah V, Zhuang N, Qi GJ (2015) 差分递归神经网络用于动作识别。见：ICCV，第 4041–4049
    页
- en: 'Ventura et al [2019] Ventura C, Bellver M, Girbau A, et al (2019) Rvos: End-to-end
    recurrent network for video object segmentation. In: CVPR, pp 5277–5286'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ventura 等 [2019] Ventura C, Bellver M, Girbau A, 等 (2019) Rvos：用于视频目标分割的端到端递归网络。见：CVPR，第
    5277–5286 页
- en: 'Voigtlaender et al [2019] Voigtlaender P, Chai Y, Schroff F, et al (2019) Feelvos:
    Fast end-to-end embedding learning for video object segmentation. In: CVPR, pp
    9481–9490'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voigtlaender 等 [2019] Voigtlaender P, Chai Y, Schroff F, 等 (2019) Feelvos：用于视频目标分割的快速端到端嵌入学习。见：CVPR，第
    9481–9490 页
- en: 'Wang and Wang [2017] Wang H, Wang L (2017) Modeling temporal dynamics and spatial
    configurations of actions using two-stream recurrent neural networks. In: CVPR,
    pp 499–508'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Wang [2017] Wang H, Wang L (2017) 使用双流递归神经网络建模动作的时间动态和空间配置。见：CVPR，第 499–508
    页
- en: Wang et al [2015] Wang L, Xiong Y, Wang Z, et al (2015) Towards good practices
    for very deep two-stream convnets. arXiv preprint arXiv:150702159
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2015] Wang L, Xiong Y, Wang Z, 等 (2015) 实现非常深的双流卷积网络的最佳实践。arXiv 预印本
    arXiv:150702159
- en: 'Wang et al [2016a] Wang L, Xiong Y, Wang Z, et al (2016a) Temporal segment
    networks: Towards good practices for deep action recognition. In: ECCV, Springer,
    pp 20–36'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2016a] Wang L, Xiong Y, Wang Z, 等 (2016a) 时间段网络：实现深度动作识别的最佳实践。见：ECCV，Springer，第
    20–36 页
- en: 'Wang et al [2021] Wang L, Tong Z, Ji B, et al (2021) Tdn: Temporal difference
    networks for efficient action recognition. In: CVPR, pp 1895–1904'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2021] Wang L, Tong Z, Ji B, 等 (2021) Tdn：高效动作识别的时间差网络。见：CVPR，第 1895–1904
    页
- en: Wang et al [2020] Wang M, Ni B, Yang X (2020) Learning multi-view interactional
    skeleton graph for action recognition. PAMI
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2020] Wang M, Ni B, Yang X (2020) 学习多视角交互骨架图用于动作识别。PAMI
- en: 'Wang et al [2016b] Wang P, Li Z, Hou Y, et al (2016b) Action recognition based
    on joint trajectory maps using convolutional neural networks. In: Proceedings
    of the 24th ACM international conference on Multimedia, pp 102–106'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al [2016b] Wang P, Li Z, Hou Y, et al (2016b) 基于卷积神经网络的联合轨迹图的动作识别。In:
    Proceedings of the 24th ACM international conference on Multimedia, pp 102–106'
- en: 'Wang et al [2017a] Wang P, Li W, Gao Z, et al (2017a) Scene flow to action
    map: A new representation for rgb-d based action recognition with convolutional
    neural networks. In: CVPR'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al [2017a] Wang P, Li W, Gao Z, et al (2017a) 场景流到动作图：一种用于基于RGB-D的动作识别的新表示方法。In:
    CVPR'
- en: 'Wang et al [2017b] Wang P, Wang S, Gao Z, et al (2017b) Structured images for
    rgb-d action recognition. In: ICCV Workshops'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al [2017b] Wang P, Wang S, Gao Z, et al (2017b) 结构化图像用于RGB-D动作识别。In:
    ICCV Workshops'
- en: 'Wang et al [2022] Wang X, Zheng S, Yang R, et al (2022) Pedestrian attribute
    recognition: A survey. Pattern Recognition 121:108220. [https://doi.org/10.1016/j.patcog.2021.108220](https:/doi.org/https://doi.org/10.1016/j.patcog.2021.108220)'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al [2022] Wang X, Zheng S, Yang R, et al (2022) 行人属性识别：一项调查。Pattern
    Recognition 121:108220. [https://doi.org/10.1016/j.patcog.2021.108220](https:/doi.org/https://doi.org/10.1016/j.patcog.2021.108220)
- en: 'Wang et al [2019] Wang Z, Xu J, Liu L, et al (2019) Ranet: Ranking attention
    network for fast video object segmentation. In: ICCV, pp 3978–3987'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al [2019] Wang Z, Xu J, Liu L, et al (2019) Ranet: 排名注意力网络用于快速视频目标分割。In:
    ICCV, pp 3978–3987'
- en: 'Wen et al [2019] Wen YH, Gao L, Fu H, et al (2019) Graph cnns with motif and
    variable temporal block for skeleton-based action recognition. In: AAAI, pp 8989–8996'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wen et al [2019] Wen YH, Gao L, Fu H, et al (2019) 带有模版和可变时间块的图卷积网络用于基于骨架的动作识别。In:
    AAAI, pp 8989–8996'
- en: 'Wu et al [2019a] Wu C, Wu XJ, Kittler J (2019a) Spatial residual layer and
    dense connection block enhanced spatial temporal graph convolutional network for
    skeleton-based action recognition. In: ICCV workshops, pp 0–0'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al [2019a] Wu C, Wu XJ, Kittler J (2019a) 空间残差层和密集连接块增强的时空图卷积网络用于基于骨架的动作识别。In:
    ICCV workshops, pp 0–0'
- en: 'Wu et al [2022a] Wu D, Dong X, Shao L, et al (2022a) Multi-level representation
    learning with semantic alignment for referring video object segmentation. In:
    CVPR, pp 4996–5005'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al [2022a] Wu D, Dong X, Shao L, et al (2022a) 具有语义对齐的多级表示学习用于视频目标分割。In:
    CVPR, pp 4996–5005'
- en: 'Wu et al [2022b] Wu J, Jiang Y, Sun P, et al (2022b) Language as queries for
    referring video object segmentation. In: CVPR, pp 4974–4984'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al [2022b] Wu J, Jiang Y, Sun P, et al (2022b) 语言作为查询用于视频目标分割。In: CVPR,
    pp 4974–4984'
- en: 'Wu et al [2022c] Wu J, Yarram S, Liang H, et al (2022c) Efficient video instance
    segmentation via tracklet query and proposal. In: CVPR'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al [2022c] Wu J, Yarram S, Liang H, et al (2022c) 通过轨迹查询和提议实现高效的视频实例分割。In:
    CVPR'
- en: 'Wu et al [2019b] Wu W, He D, Tan X, et al (2019b) Multi-agent reinforcement
    learning based frame sampling for effective untrimmed video recognition. In: ICCV,
    pp 6222–6231'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al [2019b] Wu W, He D, Tan X, et al (2019b) 基于多智能体强化学习的帧采样用于有效的未裁剪视频识别。In:
    ICCV, pp 6222–6231'
- en: 'Xie et al [2021] Xie H, Yao H, Zhou S, et al (2021) Efficient regional memory
    network for video object segmentation. In: CVPR, pp 1286–1295'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie et al [2021] Xie H, Yao H, Zhou S, et al (2021) 高效区域记忆网络用于视频目标分割。In: CVPR,
    pp 1286–1295'
- en: 'Xie et al [2018] Xie S, Sun C, Huang J, et al (2018) Rethinking spatiotemporal
    feature learning: Speed-accuracy trade-offs in video classification. In: ECCV,
    pp 305–321'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie et al [2018] Xie S, Sun C, Huang J, et al (2018) 重新思考时空特征学习：视频分类中的速度-准确性权衡。In:
    ECCV, pp 305–321'
- en: 'Xu et al [2017a] Xu C, Govindarajan LN, Zhang Y, et al (2017a) Lie-x: Depth
    image based articulated object pose estimation, tracking, and action recognition
    on lie groups. IJCV 123(3):454–478'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al [2017a] Xu C, Govindarajan LN, Zhang Y, et al (2017a) Lie-x: 基于深度图像的关节物体姿态估计、跟踪和在Lie群上的动作识别。IJCV
    123(3):454–478'
- en: 'Xu et al [2018a] Xu J, Zhao R, Zhu F, et al (2018a) Attention-aware compositional
    network for person re-identification. In: CVPR, pp 2119–2128'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al [2018a] Xu J, Zhao R, Zhu F, et al (2018a) 注意力感知的组合网络用于行人重识别。In: CVPR,
    pp 2119–2128'
- en: 'Xu and Yao [2022] Xu K, Yao A (2022) Accelerating video object segmentation
    with compressed video. In: CVPR, pp 1342–1351'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu and Yao [2022] Xu K, Yao A (2022) 通过压缩视频加速视频目标分割。In: CVPR, pp 1342–1351'
- en: 'Xu et al [2019a] Xu K, Wen L, Li G, et al (2019a) Spatiotemporal cnn for video
    object segmentation. In: CVPR, pp 1379–1388'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al [2019a] Xu K, Wen L, Li G, et al (2019a) 用于视频目标分割的时空 cnn。In: CVPR,
    pp 1379–1388'
- en: 'Xu et al [2019b] Xu M, Gao M, Chen YT, et al (2019b) Temporal recurrent networks
    for online action detection. In: ICCV, pp 5532–5541'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al [2019b] Xu M, Gao M, Chen YT, et al (2019b) 用于在线动作检测的时间递归网络。In: ICCV,
    pp 5532–5541'
- en: 'Xu et al [2018b] Xu N, Yang L, Fan Y, et al (2018b) Youtube-vos: A large-scale
    video object segmentation benchmark. arXiv preprint arXiv:180903327'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al [2018b] Xu N, Yang L, Fan Y, et al (2018b) Youtube-vos: 一个大规模视频物体分割基准。arXiv
    预印本 arXiv:180903327'
- en: 'Xu et al [2017b] Xu S, Cheng Y, Gu K, et al (2017b) Jointly attentive spatial-temporal
    pooling networks for video-based person re-identification. In: ICCV, pp 4733–4742'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al [2017b] Xu S, Cheng Y, Gu K, et al (2017b) 联合注意的时空池化网络用于视频行人重识别。在：ICCV，页4733–4742
- en: 'Yan et al [2019a] Yan A, Wang Y, Li Z, et al (2019a) Pa3d: Pose-action 3d machine
    for video recognition. In: CVPR'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yan et al [2019a] Yan A, Wang Y, Li Z, et al (2019a) Pa3d: 用于视频识别的姿态-动作3D模型。在：CVPR'
- en: 'Yan et al [2019b] Yan A, Wang Y, Li Z, et al (2019b) Pa3d: Pose-action 3d machine
    for video recognition. In: CVPR, pp 7922–7931'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yan et al [2019b] Yan A, Wang Y, Li Z, et al (2019b) Pa3d: 用于视频识别的姿态-动作3D模型。在：CVPR，页7922–7931'
- en: 'Yan et al [2022] Yan L, Wang Q, Cui Y, et al (2022) Gl-rg: Global-local representation
    granularity for video captioning. arXiv preprint arXiv:220510706'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yan et al [2022] Yan L, Wang Q, Cui Y, et al (2022) Gl-rg: 视频字幕生成的全局-局部表示粒度。arXiv
    预印本 arXiv:220510706'
- en: 'Yan et al [2018] Yan S, Xiong Y, Lin D (2018) Spatial temporal graph convolutional
    networks for skeleton-based action recognition. In: AAAI'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan et al [2018] Yan S, Xiong Y, Lin D (2018) 基于骨架的动作识别的时空图卷积网络。在：AAAI
- en: Yang et al [2019a] Yang H, Yuan C, Li B, et al (2019a) Asymmetric 3d convolutional
    neural networks for action recognition. Pattern Recognition 85:1–12
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al [2019a] Yang H, Yuan C, Li B, et al (2019a) 非对称3D卷积神经网络用于动作识别。模式识别
    85:1–12
- en: Yang et al [2021] Yang H, Yan D, Zhang L, et al (2021) Feedback graph convolutional
    network for skeleton-based action recognition. TIP 31:164–175
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al [2021] Yang H, Yan D, Zhang L, et al (2021) 基于骨架的动作识别的反馈图卷积网络。TIP
    31:164–175
- en: 'Yang et al [2022] Yang J, Dong X, Liu L, et al (2022) Recurring the transformer
    for video action recognition. In: CVPR, pp 14063–14073'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al [2022] Yang J, Dong X, Liu L, et al (2022) 将变压器用于视频动作识别的递归方法。在：CVPR，页14063–14073
- en: 'Yang et al [2019b] Yang L, Fan Y, Xu N (2019b) Video instance segmentation.
    In: CVPR, pp 5188–5197'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al [2019b] Yang L, Fan Y, Xu N (2019b) 视频实例分割。在：CVPR，页5188–5197
- en: Yu and Koltun [2015] Yu F, Koltun V (2015) Multi-scale context aggregation by
    dilated convolutions. arXiv preprint arXiv:151107122
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu and Koltun [2015] Yu F, Koltun V (2015) 通过扩张卷积进行多尺度上下文聚合。arXiv 预印本 arXiv:151107122
- en: 'Zhang et al [2018a] Zhang D, Dai X, Wang YF (2018a) Dynamic temporal pyramid
    network: A closer look at multi-scale modeling for activity detection. In: Asian
    Conference on Computer Vision, Springer, pp 712–728'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al [2018a] Zhang D, Dai X, Wang YF (2018a) 动态时间金字塔网络：对多尺度建模在活动检测中的更深入的了解。在：亚洲计算机视觉会议，Springer，页712–728
- en: 'Zhang et al [2021] Zhang K, Zhao Z, Liu D, et al (2021) Deep transport network
    for unsupervised video object segmentation. In: ICCV, pp 8781–8790'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al [2021] Zhang K, Zhao Z, Liu D, et al (2021) 用于无监督视频物体分割的深度传输网络。在：ICCV，页8781–8790
- en: 'Zhang et al [2019a] Zhang L, Lin Z, Zhang J, et al (2019a) Fast video object
    segmentation via dynamic targeting network. In: ICCV, pp 5582–5591'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al [2019a] Zhang L, Lin Z, Zhang J, et al (2019a) 通过动态目标网络实现快速视频物体分割。在：ICCV，页5582–5591
- en: 'Zhang et al [2017] Zhang P, Lan C, Xing J, et al (2017) View adaptive recurrent
    neural networks for high performance human action recognition from skeleton data.
    In: ICCV, pp 2117–2126'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al [2017] Zhang P, Lan C, Xing J, et al (2017) 用于高性能人类动作识别的视图自适应递归神经网络。
    在：ICCV，页2117–2126
- en: 'Zhang et al [2019b] Zhang R, Li J, Sun H, et al (2019b) Scan: Self-and-collaborative
    attention network for video person re-identification. TIP 28(10):4870–4882'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al [2019b] Zhang R, Li J, Sun H, et al (2019b) Scan: 自我与协作注意力网络用于视频行人重识别。TIP
    28(10):4870–4882'
- en: 'Zhang et al [2018b] Zhang S, Yang J, Schiele B (2018b) Occluded pedestrian
    detection through guided attention in cnns. In: CVPR, pp 6995–7003'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al [2018b] Zhang S, Yang J, Schiele B (2018b) 通过引导注意力的卷积神经网络进行遮挡行人检测。在：CVPR，页6995–7003
- en: 'Zhang et al [2022] Zhang Y, Borse S, Cai H, et al (2022) Perceptual consistency
    in video segmentation. In: WACV, pp 2564–2573'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al [2022] Zhang Y, Borse S, Cai H, et al (2022) 视频分割中的感知一致性。在：WACV，页2564–2573
- en: 'Zhao and Wildes [2019] Zhao H, Wildes RP (2019) Spatiotemporal feature residual
    propagation for action prediction. In: ICCV, pp 7003–7012'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao and Wildes [2019] Zhao H, Wildes RP (2019) 用于动作预测的时空特征残差传播。在：ICCV，页7003–7012
- en: 'Zhao et al [2021] Zhao L, Wang Y, Zhao J, et al (2021) Learning view-disentangled
    human pose representation by contrastive cross-view mutual information maximization.
    In: CVPR, pp 12793–12802'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al [2021] Zhao L, Wang Y, Zhao J, et al (2021) 通过对比跨视图互信息最大化学习视图解耦的人体姿态表示。在：CVPR，页12793–12802
- en: Zheng et al [2020] Zheng Z, An G, Wu D, et al (2020) Global and local knowledge-aware
    attention network for action recognition. IEEE Transactions on Neural Networks
    and Learning Systems 32(1):334–347
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 [2020] Zheng Z, An G, Wu D 等（2020）《全球与局部知识感知注意力网络用于动作识别》。IEEE 神经网络与学习系统汇刊
    32(1):334–347
- en: 'Zhou and Yuan [2017] Zhou C, Yuan J (2017) Multi-label learning of part detectors
    for heavily occluded pedestrian detection. In: ICCV, pp 3486–3495'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 和 Yuan [2017] Zhou C, Yuan J（2017）《针对严重遮挡行人的多标签检测器学习》。载于：ICCV，页3486–3495
- en: 'Zhou et al [2022a] Zhou Q, Sheng K, Zheng X, et al (2022a) Training-free transformer
    architecture search. In: CVPR, pp 10894–10903'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等 [2022a] Zhou Q, Sheng K, Zheng X 等（2022a）《无训练的 Transformer 架构搜索》。载于：CVPR，页10894–10903
- en: 'Zhou et al [2022b] Zhou Y, Zhang H, Lee H, et al (2022b) Slot-vps: Object-centric
    representation learning for video panoptic segmentation. In: CVPR, pp 3093–3103'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等 [2022b] Zhou Y, Zhang H, Lee H 等（2022b）《Slot-vps：面向视频全景分割的对象中心表示学习》。载于：CVPR，页3093–3103
- en: 'Zhu et al [2019] Zhu D, Zhang Z, Cui P, et al (2019) Robust graph convolutional
    networks against adversarial attacks. In: Proceedings of the 25th ACM SIGKDD international
    conference on knowledge discovery & data mining, pp 1399–1407'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 [2019] Zhu D, Zhang Z, Cui P 等（2019）《抗对抗攻击的鲁棒图卷积网络》。载于：第25届 ACM SIGKDD
    国际知识发现与数据挖掘会议论文集，页1399–1407
- en: 'Zhu et al [2018] Zhu J, Zou W, Xu L, et al (2018) Action machine: Rethinking
    action recognition in trimmed videos. arXiv preprint arXiv:181205770'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 [2018] Zhu J, Zou W, Xu L 等（2018）《动作机器：重新思考裁剪视频中的动作识别》。arXiv 预印本 arXiv:181205770
- en: 'Zolfaghari et al [2018] Zolfaghari M, Singh K, Brox T (2018) Eco: Efficient
    convolutional network for online video understanding. In: ECCV, pp 695–712'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zolfaghari 等 [2018] Zolfaghari M, Singh K, Brox T（2018）《Eco：高效卷积网络用于在线视频理解》。载于：ECCV，页695–712
- en: 'Zolfaghari et al [2021] Zolfaghari M, Zhu Y, Gehler P, et al (2021) Crossclr:
    Cross-modal contrastive learning for multi-modal video representations. In: ICCV,
    pp 1450–1459'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zolfaghari 等 [2021] Zolfaghari M, Zhu Y, Gehler P 等（2021）《CrossCLR：跨模态对比学习用于多模态视频表示》。载于：ICCV，页1450–1459
- en: Zong et al [2021] Zong M, Wang R, Chen X, et al (2021) Motion saliency based
    multi-stream multiplier resnets for action recognition. Image and Vision Computing
    107:104108
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zong 等 [2021] Zong M, Wang R, Chen X 等（2021）《基于运动显著性的多流倍增 ResNets 进行动作识别》。图像与视觉计算
    107:104108
