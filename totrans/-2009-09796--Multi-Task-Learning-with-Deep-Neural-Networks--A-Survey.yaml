- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:59:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:59:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2009.09796] Multi-Task Learning with Deep Neural Networks: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2009.09796] 《深度神经网络的多任务学习：综述》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2009.09796](https://ar5iv.labs.arxiv.org/html/2009.09796)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2009.09796](https://ar5iv.labs.arxiv.org/html/2009.09796)
- en: 'Multi-Task Learning with Deep Neural Networks: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《深度神经网络的多任务学习：综述》
- en: Michael Crawshaw
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 迈克尔·克劳肖
- en: Department of Computer Science
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: George Mason University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治·梅森大学
- en: mcrawsha@gmu.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: mcrawsha@gmu.edu
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Multi-task learning (MTL) is a subfield of machine learning in which multiple
    tasks are simultaneously learned by a shared model. Such approaches offer advantages
    like improved data efficiency, reduced overfitting through shared representations,
    and fast learning by leveraging auxiliary information. However, the simultaneous
    learning of multiple tasks presents new design and optimization challenges, and
    choosing which tasks should be learned jointly is in itself a non-trivial problem.
    In this survey, we give an overview of multi-task learning methods for deep neural
    networks, with the aim of summarizing both the well-established and most recent
    directions within the field. Our discussion is structured according to a partition
    of the existing deep MTL techniques into three groups: architectures, optimization
    methods, and task relationship learning. We also provide a summary of common multi-task
    benchmarks.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习（MTL）是机器学习的一个子领域，其中多个任务由一个共享模型同时学习。这种方法提供了诸如改进数据效率、通过共享表示减少过拟合以及利用辅助信息进行快速学习等优势。然而，同时学习多个任务带来了新的设计和优化挑战，而选择哪些任务应共同学习本身就是一个非平凡的问题。在本综述中，我们概述了深度神经网络的多任务学习方法，旨在总结该领域内的既有成就和最新方向。我们的讨论根据现有深度MTL技术的三个组别进行结构化：架构、优化方法和任务关系学习。我们还提供了常见多任务基准的总结。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Multi-task learning is a training paradigm in which machine learning models
    are trained with data from multiple tasks simultaneously, using shared representations
    to learn the common ideas between a collection of related tasks. These shared
    representations increase data efficiency and can potentially yield faster learning
    speed for related or downstream tasks, helping to alleviate the well-known weaknesses
    of deep learning: large-scale data requirements and computational demand. However,
    achieving such effects has not proven easy and is an active area of research today.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习是一种训练范式，在这种范式中，机器学习模型同时使用来自多个任务的数据进行训练，利用共享表示来学习一组相关任务之间的共同思想。这些共享表示提高了数据效率，并可能为相关或下游任务带来更快的学习速度，帮助缓解深度学习的已知弱点：大规模数据需求和计算需求。然而，获得这种效果并不容易，今天仍然是一个活跃的研究领域。
- en: 'We believe that MTL reflects the learning process of human beings more accurately
    than single task learning in that integrating knowledge across domains is a central
    tenant of human intelligence. When a newborn baby learns to walk or use its hands,
    it accumulates general motor skills which rely on abstract notions of balance
    and intuitive physics. Once these motor skills and abstract concepts are learned,
    they can be reused and augmented for more complex tasks later in life, such as
    riding a bike or tightrope walking. Any time that a human attempts to learn something
    new, we bring a tremendous amount of prior knowledge to the table. It’s no wonder
    that neural networks require such numerous training examples and computation time:
    every task is learned from scratch. Imagine trying to learn to tightrope walk
    without first learning to walk! The human ability to rapidly learn with few examples
    is dependent on this process of learning concepts which are generalizable across
    multiple settings and leveraging these concepts for fast learning; we believe
    that developing systems to perform this process should be the goal of multi-task
    learning and the related fields of meta-learning [Hospedales et al., [2020](#bib.bib63)],
    transfer learning [Zhuang et al., [2019](#bib.bib175)], and continuous/lifelong
    learning [Parisi et al., [2019](#bib.bib118)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，多任务学习比单一任务学习更准确地反映了人类学习过程，因为跨领域整合知识是人类智能的核心。当一个新生儿学习走路或使用双手时，它积累了依赖于抽象平衡和直觉物理学的一般性运动技能。一旦学会了这些运动技能和抽象概念，它们可以在以后的生活中被重复利用和增强，比如骑自行车或走钢丝。每当一个人尝试学习新东西时，他们都会带来大量的先前知识。难怪神经网络需要如此多的训练样本和计算时间：每个任务都是从头开始学习的。想象一下在没有先学会走路的情况下尝试学会走钢丝！人类在很少的示例下快速学习的能力依赖于学习可推广到多种环境的概念，并利用这些概念进行快速学习；我们认为开发执行这个过程的系统应该是多任务学习以及元学习[Hospedales
    et al., [2020](#bib.bib63)], 迁移学习[Zhuang et al., [2019](#bib.bib175)] 和连续/终身学习[Parisi
    et al., [2019](#bib.bib118)]等相关领域的目标。
- en: 'Learning concepts for multiple tasks does bring difficulties which aren’t present
    in single task learning. In particular, it may be the case that different tasks
    have conflicting needs. In this case, increasing the performance of a model on
    one task will hurt performance on a task with different needs, a phenomenon referred
    to as negative transfer or destructive interference. Minimizing negative transfer
    is a key goal for MTL methods. Many architectures are designed with specific features
    to decrease negative transfer, such as task-specific feature spaces and attention
    mechanisms, but division of information between tasks is a fine line to walk:
    we want to allow information flow between tasks that yields positive transfer,
    and discourage sharing when it would create negative transfer. The question of
    how exactly to design such a system is being actively investigated in MTL research.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在多任务学习中，学习概念确实带来了单一任务学习中不存在的困难。特别是，不同的任务可能具有冲突的需求。在这种情况下，提高模型在一个任务上的性能将会影响到在另一个具有不同需求的任务上的性能，这种现象被称为负迁移或破坏性干扰。减少负迁移是多任务学习方法的一个关键目标。许多架构都是设计有特定功能以减少负迁移，例如任务特定的特征空间和注意机制，但任务之间的信息划分是一个微妙的平衡：我们希望允许任务之间的信息流产生积极的迁移，并且鼓励分享不会产生负迁移的情况。如何设计这样一个系统的问题正在多任务学习研究中积极探讨中。
- en: 'The existing methods of MTL have often been partitioned into two groups with
    a familiar dichotomy: hard parameter sharing vs. soft parameter sharing. Hard
    parameter sharing is the practice of sharing model weights between multiple tasks,
    so that each weight is trained to jointly minimize multiple loss functions. Under
    soft parameter sharing, different tasks have individual task-specific models with
    separate weights, but the distance between the model parameters of different tasks
    is added to the joint objective function. Though there is no explicit parameter
    sharing, there is an incentive for the task-specific models to have similar parameters.
    This is a useful dichotomy, but the nature of multi-task methods has grown extremely
    diverse in the past few years, and we feel that these two categories alone are
    not broad enough to accurately describe the entire field. Instead, we widen the
    scope of the members of this dichotomy to cover more ground. We generalize the
    class of hard parameter sharing methods to multi-task architectures, while soft
    parameter sharing is broadened into multi-task optimization. When combined, architecture
    design and optimization techniques provide a nearly complete image of modern MTL.
    However, there is still an important direction within the field that is missing
    even from this generalized dichotomy: task relationship learning. Task relationship
    laerning (or TRL) methods focus on learning an explicit representation of the
    relationships between tasks, such as task embeddings or transfer learning affinities,
    and these types of methods don’t quite fit into either architecture design or
    optimization. Broadly speaking, these three directions - architecture design,
    optimization, and task relationship learning - make up the existing methods of
    modern deep multi-task learning.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的MTL方法通常被分为两组，有着熟悉的二分法：硬参数共享与软参数共享。硬参数共享是指在多个任务之间共享模型权重，使得每个权重被训练来共同最小化多个损失函数。在软参数共享下，不同任务有各自的任务特定模型，权重分开，但不同任务的模型参数之间的距离被加入到联合目标函数中。虽然没有显式的参数共享，但任务特定模型有相似参数的激励。这是一种有用的二分法，但近年来多任务方法的性质变得极为多样化，我们认为这两类方法单独并不足以准确描述整个领域。因此，我们扩展了这些二分法的范围，以涵盖更多内容。我们将硬参数共享方法的类别概括为多任务架构，同时将软参数共享扩展为多任务优化。结合起来，架构设计和优化技术提供了现代MTL的几乎完整的图景。然而，领域内仍有一个重要方向甚至在这个概括的二分法中也缺失：任务关系学习。任务关系学习（或TRL）方法专注于学习任务之间关系的显式表示，例如任务嵌入或迁移学习亲和力，这些类型的方法不完全符合架构设计或优化。广义而言，这三个方向——架构设计、优化和任务关系学习——构成了现代深度多任务学习的现有方法。
- en: Many different researchers have used the term multi-task learning to refer to
    different settings, and we feel that it is important to clarify the scope of this
    review. As a convention, we interpret MTL to only contain learning settings in
    which a fixed set of tasks is learned simultaneously, and each task is treated
    equally. This means that we don’t consider training settings that have only a
    single “main task" with one or more auxiliary tasks, as well as settings in which
    the set of tasks to learn changes over time. We may, however, discuss models which
    were designed for such settings, if the ideas from the model are easily applicable
    to MTL.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 许多不同的研究者使用“多任务学习”这一术语来指代不同的设置，我们认为澄清此评审的范围是重要的。作为惯例，我们将MTL解释为仅包含同时学习固定任务集的学习设置，并且每个任务被平等对待。这意味着我们不考虑只有一个“主任务”及一个或多个辅助任务的训练设置，也不考虑任务集随时间变化的设置。然而，如果模型的想法能轻松适用于MTL，我们可能会讨论为这些设置设计的模型。
- en: 'The rest of the survey is outlined as follows. Section [2](#S2 "2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey") contains
    a discussion of neural network architectures for multi-task learning. In section
    [3](#S3 "3 Optimization for Multi-Task Learning ‣ Multi-Task Learning with Deep
    Neural Networks: A Survey"), we discuss MTL optimization strategies, and we discuss
    methods for learning explicit task relationships in section [4](#S4 "4 Task Relationship
    Learning ‣ Multi-Task Learning with Deep Neural Networks: A Survey"). Section
    [5](#S5 "5 Multi-Task Benchmarks ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey") contains an overview of common multi-task benchmark for various domains.
    Finally, we conclude with section [6](#S6 "6 Conclusion ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey"). Within each subsection or subsubsection,
    the methods are mostly presented in order of publication, from earliest to most
    recent. It should be noted that we do not discuss any classical (non-neural) multi-task
    learning methods, though a thorough review can be found in Zhang and Yang [[2017](#bib.bib170)],
    Ruder [[2017](#bib.bib129)].'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的调查内容概述如下。第[2](#S2 "2 多任务架构 ‣ 深度神经网络的多任务学习：一项调查")节包含了关于多任务学习的神经网络架构的讨论。在第[3](#S3
    "3 多任务学习的优化 ‣ 深度神经网络的多任务学习：一项调查")节中，我们讨论了MTL优化策略，而在第[4](#S4 "4 任务关系学习 ‣ 深度神经网络的多任务学习：一项调查")节中，我们探讨了显式任务关系学习的方法。第[5](#S5
    "5 多任务基准 ‣ 深度神经网络的多任务学习：一项调查")节包含了各种领域常见的多任务基准的概述。最后，我们在第[6](#S6 "6 结论 ‣ 深度神经网络的多任务学习：一项调查")节中作总结。在每个子节或子子节中，方法通常按照出版时间的顺序呈现，从最早到最近。需要注意的是，我们不讨论任何经典的（非神经网络）多任务学习方法，尽管张和杨
    [[2017](#bib.bib170)]、鲁德 [[2017](#bib.bib129)] 提供了详尽的回顾。
- en: 2 Multi-Task Architectures
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 多任务架构
- en: 'A large portion of the MTL literature is devoted to the design of multi-task
    neural network architectures. There are many different factors to consider when
    creating a shared architecture, such as the portion of the model’s parameters
    that will be shared between tasks, and how to parameterize and combine task-specific
    and shared modules. More variations arise when considering architectures for a
    specific problem domain, like how to partition convolutional filters into shared
    and task-specific groups for a set of vision tasks. Many of the proposed architectures
    for MTL play a balancing game with the degree of information sharing between tasks:
    Too much sharing will lead to negative transfer and can cause worse performance
    of joint multi-task models than individual models for each task, while too little
    sharing doesn’t allow the model to effectively leverage information between tasks.
    The best performing architectures for MTL are those which balance sharing well.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分MTL文献致力于设计多任务神经网络架构。在创建共享架构时，需要考虑许多不同的因素，例如模型参数的共享比例，以及如何对任务特定模块和共享模块进行参数化和组合。考虑特定问题领域的架构时会出现更多变体，比如如何将卷积滤波器分成共享和任务特定组以用于一组视觉任务。许多提议的MTL架构在任务间的信息共享程度上进行平衡：共享过多会导致负迁移，使联合多任务模型的表现比每个任务的单独模型更差，而共享过少则无法有效利用任务间的信息。表现最好的MTL架构是那些能够很好地平衡共享的架构。
- en: 'We partition the MTL architectures into four groups: architectures for a particular
    task domain, multi-modal architectures, learned architectures, and conditional
    architectures. For single-domain architectures, we consider the domains of computer
    vision, natural language processing, and reinforcement learning. Multi-modal architectures
    handle tasks with input in more than one mode, such as visual question answering
    with both a visual and a language component. It should be noted that we only consider
    multi-modal architectures which handle multiple tasks. For a more complete discussion
    of multi-modal methods, see Baltrusaitis et al. [[2019](#bib.bib11)]. Lastly,
    We make the following distinction between learned architectures and conditional
    architectures: Learned architectures are fixed between steps of architecture learning,
    so the same computation is performed for each input from the same task. In conditional
    architectures, the architecture used for a given piece of data is dependent on
    the data itself.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将多任务学习架构分为四类：特定任务领域的架构、多模态架构、学习到的架构和条件架构。对于单领域架构，我们考虑计算机视觉、自然语言处理和强化学习领域。多模态架构处理输入形式不止一种的任务，例如视觉问答，它包括视觉和语言两个部分。需要注意的是，我们仅考虑处理多任务的多模态架构。有关多模态方法的更全面讨论，请参见
    Baltrusaitis 等人 [[2019](#bib.bib11)]。最后，我们对学习到的架构和条件架构做如下区分：学习到的架构在架构学习的步骤间是固定的，因此对来自相同任务的每个输入执行相同的计算。而在条件架构中，用于给定数据片段的架构依赖于数据本身。
- en: 2.1 Architectures for Computer Vision
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 计算机视觉的架构
- en: In the single-task setting, many major developments for computer vision architectures
    have focused on novel network components and connections to improve optimization
    and extract more meaningful features, such as batch normalization Ioffe and Szegedy
    [[2015](#bib.bib66)], residual networks He et al. [[2016](#bib.bib60)], and squeeze
    and excitation blocks Hu et al. [[2018](#bib.bib64)]. In contrast, many multi-task
    architectures for computer vision focus on partitioning the network into task-specific
    and shared components in a way that allows for generalization through sharing
    and information flow between tasks, while minimizing negative transfer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在单任务设置中，许多计算机视觉架构的重要进展集中在新颖的网络组件和连接上，以提高优化和提取更有意义的特征，例如批归一化 Ioffe 和 Szegedy
    [[2015](#bib.bib66)]、残差网络 He 等人 [[2016](#bib.bib60)] 和压缩与激励块 Hu 等人 [[2018](#bib.bib64)]。相比之下，许多计算机视觉的多任务架构则集中在将网络划分为任务特定组件和共享组件，以便通过共享和任务间的信息流实现泛化，同时最小化负迁移。
- en: 2.1.1 Shared Trunk
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 共享主干
- en: 'Traditionally, many multi-task architectures in computer vision follow a simple
    outline: A global feature extractor made of convolutional layers shared by all
    tasks followed by an individual output branch for each task, as in figure [1](#S2.F1
    "Figure 1 ‣ 2.1.1 Shared Trunk ‣ 2.1 Architectures for Computer Vision ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey"). We
    will refer to this template as a shared trunk.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，计算机视觉中的许多多任务架构遵循一个简单的轮廓：一个由卷积层组成的全局特征提取器被所有任务共享，随后是每个任务的单独输出分支，如图 [1](#S2.F1
    "图 1 ‣ 2.1.1 共享主干 ‣ 2.1 计算机视觉的架构 ‣ 2 多任务架构 ‣ 使用深度神经网络的多任务学习：综述")。我们将这一模板称为共享主干。
- en: '![Refer to caption](img/628983ce7d3d85457300aaa12f1856b3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/628983ce7d3d85457300aaa12f1856b3.png)'
- en: 'Figure 1: Architecture for TCDCN Zhang et al. [[2014](#bib.bib171)]. The base
    feature extractor is made of a series of convolutional layers which are shared
    between all tasks, and the extracted features are used as input to task-specific
    output heads.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：TCDCN Zhang 等人 [[2014](#bib.bib171)] 的架构。基础特征提取器由一系列卷积层构成，这些卷积层在所有任务间共享，提取出的特征被用作任务特定输出头的输入。
- en: 'Zhang et al. [[2014](#bib.bib171)], Dai et al. [[2016](#bib.bib37)], Zhao et al.
    [[2018](#bib.bib173)], Liu et al. [[2019](#bib.bib93)], Ma et al. [[2018](#bib.bib105)]
    propose architectures which are variations on the shared trunk idea. Zhang et al.
    [[2014](#bib.bib171)], the earliest of these works, introduces Tasks-Constrained
    Deep Convolutional Network (TCDCN), whose architecture is shown in figure [1](#S2.F1
    "Figure 1 ‣ 2.1.1 Shared Trunk ‣ 2.1 Architectures for Computer Vision ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey"). The
    authors propose to improve performance on a facial landmark detection task by
    jointly learning head pose estimation and facial attribute inference. Dai et al.
    [[2016](#bib.bib37)] introduces Multi-task Network Cascades (MNCs). The architecture
    of MNCs is similar to TCDCN, with an important difference: the output of each
    task-specific branch is appended to the input of the next task-specific branch,
    forming the “cascade" of information flow after which the method is named. This
    type of architecture is similar to the cascaded information networks for NLP discussed
    in section [2.2.3](#S2.SS2.SSS3 "2.2.3 Cascaded Information ‣ 2.2 Architectures
    for Natural Language Processing ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [[2014](#bib.bib171)]，Dai 等人 [[2016](#bib.bib37)]，Zhao 等人 [[2018](#bib.bib173)]，Liu
    等人 [[2019](#bib.bib93)]，Ma 等人 [[2018](#bib.bib105)] 提出了基于共享主干思想的架构变体。Zhang 等人
    [[2014](#bib.bib171)]，这些工作的最早者，介绍了任务约束深度卷积网络（TCDCN），其架构如图 [1](#S2.F1 "Figure 1
    ‣ 2.1.1 Shared Trunk ‣ 2.1 Architectures for Computer Vision ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey") 所示。作者提出通过共同学习头部姿态估计和面部属性推断来提高面部标志检测任务的性能。Dai
    等人 [[2016](#bib.bib37)] 介绍了多任务网络级联（MNCs）。MNCs 的架构类似于 TCDCN，但有一个重要的区别：每个任务特定分支的输出附加到下一个任务特定分支的输入上，形成了该方法名称来源的“级联”信息流。这种类型的架构类似于在第
    [2.2.3](#S2.SS2.SSS3 "2.2.3 Cascaded Information ‣ 2.2 Architectures for Natural
    Language Processing ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep
    Neural Networks: A Survey") 节讨论的 NLP 级联信息网络。'
- en: '![Refer to caption](img/16daa7aa0c7914ba748f1d388a8aed2e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/16daa7aa0c7914ba748f1d388a8aed2e.png)'
- en: 'Figure 2: Illustration of Multi-task Network Cascades Dai et al. [[2016](#bib.bib37)].
    The output of the first task is used as an input for the second task, the second
    task’s output is used as an input for the third task, and so on.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：多任务网络级联的示意图 Dai 等人 [[2016](#bib.bib37)]。第一个任务的输出作为第二个任务的输入，第二个任务的输出作为第三个任务的输入，以此类推。
- en: Zhao et al. [[2018](#bib.bib173)], Liu et al. [[2019](#bib.bib93)] each build
    on this original template with the introduction of task-specific modules which
    can be placed within existing shared architectures. By doing this, the computation
    of features relies on both the shared parameters of the feature extractor and
    the task-specific parameters of modules placed through the network, so that features
    of different tasks may differ before the task-specific output branches. Zhao et al.
    [[2018](#bib.bib173)] introduces a modulation module in the form of a task-specific
    channel-wise linear projection of feature maps, and the authors design a convolutional
    architecture with these modules following convolutional layers in the latter half
    of the network. Interestingly, it is empirically shown that the inclusion of these
    task-specific projection modules decreases the chance that gradient update directions
    for different tasks point in opposite directions, implying that this architecture
    decreases the occurence of negative transfer. Liu et al. [[2019](#bib.bib93)]
    proposes task-specific attention modules. Each attention module takes as input
    the features from some intermediate layer of the shared network concatenated with
    the output of the previous attention module, if one exists. Each module computes
    an attention map by passing its input through a Conv-BN-ReLU layer followed by
    a Conv-BN-Sigmoid layer. The attention map is then element-wise multiplied with
    features from a successive shared layer, and this product is the output of the
    attention module. This attention module allows the network to emphasize features
    in the network which are more important for the corresponding task, and downplay
    the effect of unimportant features.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人 [[2018](#bib.bib173)]、Liu 等人 [[2019](#bib.bib93)] 都基于这一原始模板，加入了任务特定的模块，这些模块可以放置在现有的共享架构中。通过这样做，特征的计算依赖于特征提取器的共享参数和通过网络放置的任务特定模块的参数，使得不同任务的特征在任务特定输出分支之前可能有所不同。Zhao
    等人 [[2018](#bib.bib173)] 引入了一种调制模块，该模块以任务特定的通道级线性投影形式存在，作者设计了一种卷积架构，将这些模块放置在网络后半部分的卷积层之后。有趣的是，实验证明，加入这些任务特定的投影模块可以减少不同任务的梯度更新方向相反的可能性，这意味着该架构减少了负迁移的发生。Liu
    等人 [[2019](#bib.bib93)] 提出了任务特定的注意力模块。每个注意力模块以来自共享网络某个中间层的特征以及前一个注意力模块的输出（如果存在）作为输入。每个模块通过
    Conv-BN-ReLU 层和 Conv-BN-Sigmoid 层计算一个注意力图。然后，将注意力图与来自后续共享层的特征逐元素相乘，这一乘积即为注意力模块的输出。该注意力模块使网络能够强调对对应任务更重要的特征，并降低不重要特征的影响。
- en: 'Multi-gate Mixture-of-Experts Ma et al. [[2018](#bib.bib105)] is a recently
    proposed shared trunk model, with a twist: the network contains multiple shared
    trunks, and each task-specific output head receives as input a linear combination
    of the outputs of each shared trunk. The weights of the linear combination are
    computed by a separate gating function, which performs a linear transformation
    on the network input to compute the linear combination weights. The gating function
    can either be shared between all tasks, so that each task-specific output head
    receives the same input, or task-specific, so that each output head receives a
    different mixture of the shared trunk outputs. This model bears resemblance to
    Cross-Stitch networks Misra et al. [[2016](#bib.bib113)] (see section [2.1.2](#S2.SS1.SSS2
    "2.1.2 Cross-Talk ‣ 2.1 Architectures for Computer Vision ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey")), but performs a single
    linear combination of shared components instead of multiple feature combinations
    from task-specific layers. This method wasn’t empirically evaluated on computer
    vision tasks, but is discussed here due to its close relationship with the other
    CV architectures Zhang et al. [[2014](#bib.bib171)], Misra et al. [[2016](#bib.bib113)].'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '多门混合专家模型 Ma 等人 [[2018](#bib.bib105)] 是一种最近提出的共享主干模型，具有独特的特点：网络包含多个共享主干，每个任务特定的输出头接收来自每个共享主干输出的线性组合。线性组合的权重由一个单独的门控函数计算，该函数对网络输入执行线性变换以计算线性组合权重。门控函数可以在所有任务之间共享，使得每个任务特定的输出头接收相同的输入，或者是任务特定的，使得每个输出头接收不同的共享主干输出的混合。这种模型类似于
    Cross-Stitch 网络 Misra 等人 [[2016](#bib.bib113)]（见 [2.1.2](#S2.SS1.SSS2 "2.1.2 Cross-Talk
    ‣ 2.1 Architectures for Computer Vision ‣ 2 Multi-Task Architectures ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey")），但执行单一的共享组件线性组合，而不是从任务特定层中的多个特征组合。这种方法在计算机视觉任务中没有经过实证评估，但由于其与其他
    CV 架构 Zhang 等人 [[2014](#bib.bib171)]，Misra 等人 [[2016](#bib.bib113)] 的密切关系，因此在此讨论。'
- en: 2.1.2 Cross-Talk
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 Cross-Talk
- en: 'Not all MTL architectures for computer vision consist of a shared, global feature
    extractor with task-specific output branches or modules. Misra et al. [[2016](#bib.bib113)],
    Ruder et al. [[2019](#bib.bib130)], Gao et al. [[2019](#bib.bib53)] take a separate
    approach. Instead of a single shared extractor, these architectures have a separate
    network for each task, with information flow between parallel layers in the task
    networks, referred to as cross-talk. Figure [3](#S2.F3 "Figure 3 ‣ 2.1.2 Cross-Talk
    ‣ 2.1 Architectures for Computer Vision ‣ 2 Multi-Task Architectures ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey") depicts this idea with the Cross-Stitch
    network architecture from Misra et al. [[2016](#bib.bib113)].'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '并非所有计算机视觉的 MTL 架构都由一个共享的、全局的特征提取器和任务特定的输出分支或模块组成。Misra 等人 [[2016](#bib.bib113)]，Ruder
    等人 [[2019](#bib.bib130)]，Gao 等人 [[2019](#bib.bib53)] 采取了不同的方法。这些架构没有单一的共享提取器，而是为每个任务设置了单独的网络，任务网络中的并行层之间的信息流被称为
    cross-talk。图 [3](#S2.F3 "Figure 3 ‣ 2.1.2 Cross-Talk ‣ 2.1 Architectures for Computer
    Vision ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey") 展示了这种思想，使用了 Misra 等人 [[2016](#bib.bib113)] 的 Cross-Stitch 网络架构。'
- en: '![Refer to caption](img/c49d18e058f50a5e133eef8c771ad3ec.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c49d18e058f50a5e133eef8c771ad3ec.png)'
- en: 'Figure 3: Cross-Stitch network architecture Misra et al. [[2016](#bib.bib113)].
    Each task has a separate network, but cross-stitch units combine information from
    parallel layers of the different task networks with a linear combination.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：Cross-Stitch 网络架构 Misra 等人 [[2016](#bib.bib113)]。每个任务有一个单独的网络，但 cross-stitch
    单元通过线性组合将来自不同任务网络的并行层的信息结合起来。
- en: 'A Cross-Stitch network is composed of individual networks for each task, but
    the input to each layer is a linear combination of the outputs of the previous
    layer from every task network. The weights of each linear combination are learned
    and task-specific, so that each layer can choose which tasks to leverage information
    from. Ruder et al. [[2019](#bib.bib130)] generalizes this idea with the introduction
    of the Sluice network. In the Sluice network, each layer is divided into task-specific
    and shared subspaces, and the input to each layer is a linear combination of the
    task-specific and shared outputs of the previous layer from each task network.
    This way, each layer can choose whether to focus on task specific or shared features
    from previous layers. The task-specific and shared subspaces of each layer are
    also encouraged to be orthogal, by adding an auxiliary term to the loss function
    to minimize the squared Frobenius norm of the product of each task-specific subspace
    with its corresponding shared subspace. It should be noted that Sluice networks
    are presented in a domain-agnostic way, but we discuss them here due to their
    relation to Cross-Stitch networks. Finally, Gao et al. [[2019](#bib.bib53)] generalizes
    the feature fusion operation at parallel layers with Neural Discriminative Dimensionality
    Reduction (NDDR-CNN). Instead of using a linear combination to combine features
    from parallel layers of the task networks, NDDR-CNN concatenates the outputs from
    each layer and pass the result through a 1x1 convolution. The parameters of this
    convolution are task specific, as are the linear combination weights in Cross-Stitch
    networks. A diagram is shown in figure [4](#S2.F4 "Figure 4 ‣ 2.1.2 Cross-Talk
    ‣ 2.1 Architectures for Computer Vision ‣ 2 Multi-Task Architectures ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey"). Note that this method for feature
    fusion is a generalization of Cross-Stitch networks. The 1x1 convolutional parameters
    can be learned in such a way to mimic a Cross-Stitch network, but most parameter
    combinations lead to feature fusion operations which can’t be implemented with
    a Cross-Stitch network.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cross-Stitch 网络由每个任务的独立网络组成，但每层的输入是每个任务网络中前一层输出的线性组合。每个线性组合的权重是学习得到的，并且是任务特定的，因此每层可以选择利用哪些任务的信息。Ruder
    等人 [[2019](#bib.bib130)] 通过引入 Sluice 网络来推广这个想法。在 Sluice 网络中，每层被划分为任务特定子空间和共享子空间，每层的输入是每个任务网络中前一层任务特定输出和共享输出的线性组合。这样，每层可以选择是否关注来自前一层的任务特定或共享特征。每层的任务特定和共享子空间也被鼓励正交，通过在损失函数中添加辅助项来最小化每个任务特定子空间与其对应的共享子空间的乘积的平方
    Frobenius 范数。需要注意的是，Sluice 网络以领域无关的方式呈现，但由于与 Cross-Stitch 网络的关系，我们在这里讨论它们。最后，Gao
    等人 [[2019](#bib.bib53)] 通过神经判别降维（NDDR-CNN）推广了平行层的特征融合操作。NDDR-CNN 不是使用线性组合来融合任务网络的平行层特征，而是将每层的输出拼接并通过
    1x1 卷积处理。这个卷积的参数是任务特定的，就像 Cross-Stitch 网络中的线性组合权重一样。图 [4](#S2.F4 "Figure 4 ‣ 2.1.2
    Cross-Talk ‣ 2.1 Architectures for Computer Vision ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey") 中展示了一个示意图。请注意，这种特征融合方法是
    Cross-Stitch 网络的推广。1x1 卷积参数可以以模仿 Cross-Stitch 网络的方式进行学习，但大多数参数组合导致的特征融合操作是 Cross-Stitch
    网络无法实现的。'
- en: '![Refer to caption](img/68e3c609f3aa3e61fc2a997f8158f208.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/68e3c609f3aa3e61fc2a997f8158f208.png)'
- en: 'Figure 4: NDDR-CNN network architecture Gao et al. [[2019](#bib.bib53)]. Instead
    of combining information from different task networks with a linear combination
    of parallel features (as in Cross-Stitch networks Misra et al. [[2016](#bib.bib113)]),
    NDDR-CNN uses concatenation and a 1x1 convolution to fuse features from separate
    task networks.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：NDDR-CNN 网络架构 Gao 等人 [[2019](#bib.bib53)]。与 Cross-Stitch 网络 Misra 等人 [[2016](#bib.bib113)]
    中使用平行特征的线性组合来结合来自不同任务网络的信息不同，NDDR-CNN 使用拼接和 1x1 卷积来融合来自独立任务网络的特征。
- en: Yang and Hospedales [[2016a](#bib.bib161)] proposes an architecture which is
    related to the cross-talk template, though perhaps only tangentially. In the Sluice
    network, task-specific and shared parameter tensors from each layer are simply
    concatenated to form the layer’s parameters. The architecture of Yang and Hospedales
    [[2016a](#bib.bib161)] also creates an explicit separation between task-specific
    and shared parameters, but does so using tensor factorization, a well-known approach
    in the classical MTL literature Evgeniou and Pontil [[2004](#bib.bib50)], Argyriou
    et al. [[2008](#bib.bib10)], Kumar and Daume III [[2012](#bib.bib79)]. Tensor
    factorization is used in MTL to represent a multi-task model’s parameter tensor
    as a product of two smaller tensors, one shared between tasks and one task-specific,
    which enforces a different type of division of shared/task-specific feature spaces
    than, for example, Sluice networks. Yang and Hospedales [[2016a](#bib.bib161)]
    extends this approach to the deep learning setting in order to learn the sharing
    structure at each layer within a deep network. Unfortunately, there is no empirical
    comparison of this tensor factorization approach with the other cross-talk architectures,
    and there hasn’t been much work extending the tensor factorization approach of
    Yang and Hospedales [[2016a](#bib.bib161)] for deep MTL.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 杨和Hospedales [[2016a](#bib.bib161)] 提出了一个与交叉对话模板相关的架构，尽管可能只是间接相关。在Sluice网络中，来自每一层的任务特定和共享参数张量被简单地连接起来形成层的参数。杨和Hospedales
    [[2016a](#bib.bib161)] 的架构也明确区分了任务特定参数和共享参数，但采用了张量分解，这是一种在经典MTL文献中广泛使用的方法，Evgeniou
    和 Pontil [[2004](#bib.bib50)]，Argyriou 等 [[2008](#bib.bib10)]，Kumar 和 Daume III
    [[2012](#bib.bib79)]。张量分解在MTL中用于将多任务模型的参数张量表示为两个较小张量的乘积，一个在任务之间共享，一个任务特定，这强制执行一种与Sluice网络不同的共享/任务特定特征空间的划分。杨和Hospedales
    [[2016a](#bib.bib161)] 将这种方法扩展到深度学习设置中，以便在深度网络中的每一层学习共享结构。不幸的是，尚未对这种张量分解方法与其他交叉对话架构进行实证比较，也没有多少工作扩展杨和Hospedales
    [[2016a](#bib.bib161)] 对深度MTL的张量分解方法。
- en: 2.1.3 Prediction Distillation
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 预测蒸馏
- en: 'A major tenant and popular justification of MTL is that learned features from
    one task may be useful in performing another related task. Prediction distillation
    techniques are based on a natural extension of this principle: that the answers
    to one task may help learning of another. Vandenhende et al. [[2020](#bib.bib152)]
    provides a great motivating example of this phenomenon: In an MTL setup for jointly
    learning depth prediction and semantic segmentation, discontinuities in the depth
    map imply likely discontinuities in semantic segmentation labels, and vice versa.
    PAD-Net Xu et al. [[2018a](#bib.bib158)], Pattern-Affinitive Propagation Zhang
    et al. [[2019](#bib.bib172)], and MTI-Net Vandenhende et al. [[2020](#bib.bib152)]
    each take advantage of this phenomenon for the multi-task learning of computer
    vision tasks by making preliminary predictions for each task, then combining these
    predictions to produced final, refined outputs.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: MTL的一个主要信条和流行理由是，从一个任务中学到的特征可能对执行另一个相关任务有用。预测蒸馏技术基于这一原则的自然扩展：一个任务的答案可能有助于学习另一个任务。Vandenhende
    等 [[2020](#bib.bib152)] 提供了这一现象的一个很好的激励示例：在联合学习深度预测和语义分割的MTL设置中，深度图中的不连续性意味着语义分割标签中的可能不连续性，反之亦然。PAD-Net
    Xu 等 [[2018a](#bib.bib158)]、Pattern-Affinitive Propagation Zhang 等 [[2019](#bib.bib172)]
    和MTI-Net Vandenhende 等 [[2020](#bib.bib152)] 都利用了这一现象，通过对每个任务进行初步预测，然后将这些预测结合起来，生成最终的精炼输出，以进行计算机视觉任务的多任务学习。
- en: '![Refer to caption](img/665d75cbe7f25786a62660b8f995918b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/665d75cbe7f25786a62660b8f995918b.png)'
- en: 'Figure 5: PAD-Net architecture for prediction distillation Xu et al. [[2018a](#bib.bib158)].
    Preliminary predictions are made for four tasks, then these predictions are re-combined
    and used to compute final, refined predictions for two output tasks.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：预测蒸馏的PAD-Net架构，Xu 等 [[2018a](#bib.bib158)]。对四个任务进行初步预测，然后将这些预测重新组合，用于计算两个输出任务的最终精炼预测。
- en: 'PAD-Net Xu et al. [[2018a](#bib.bib158)] is the earliest of these works, introducing
    an architecture to combine preliminary predictions for depth prediction, scene
    parsing, surface normal estimation, and contour prediction to produce refined
    predictions for depth prediction and scene parsing, as pictured in figure [5](#S2.F5
    "Figure 5 ‣ 2.1.3 Prediction Distillation ‣ 2.1 Architectures for Computer Vision
    ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey"). The preliminary predictions are recombined using one of three novel
    variations of a multi-modal distillation module, either using naive feature concatenation,
    message passing, or attention-guided message passing. Pattern-Affinitive Propagation
    (PAP) Zhang et al. [[2019](#bib.bib172)] expands on this architecture by introducing
    an affinity learning layer which learns to represent pair-wise relationships of
    tasks and combines features from various tasks according to these relationships.
    PAP also does away with the extra auxiliary tasks of PAD-Net and instead produces
    both preliminary and final predictions for depth prediction, surface normal estimation,
    and semantic segmentation. Both of these methods, at the times of their publication,
    reached state of the art performance on at least one task from the NYU-v2 dataset
    Silberman et al. [[2012](#bib.bib137)].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'PAD-Net Xu et al. [[2018a](#bib.bib158)] 是这些工作的最早者，介绍了一种架构，将深度预测、场景解析、表面法线估计和轮廓预测的初步预测进行结合，以产生更精细的深度预测和场景解析预测，如图[5](#S2.F5
    "Figure 5 ‣ 2.1.3 Prediction Distillation ‣ 2.1 Architectures for Computer Vision
    ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey")所示。初步预测通过三种新型的多模态蒸馏模块的变体之一进行重新组合，分别是使用简单特征拼接、消息传递或注意力引导的消息传递。Pattern-Affinitive
    Propagation (PAP) Zhang et al. [[2019](#bib.bib172)] 通过引入一个亲和学习层来扩展这一架构，该层学习表示任务的成对关系，并根据这些关系组合来自不同任务的特征。PAP
    还取消了 PAD-Net 的额外辅助任务，而是为深度预测、表面法线估计和语义分割生成初步和最终预测。这两种方法在发表时，在 NYU-v2 数据集 Silberman
    et al. [[2012](#bib.bib137)] 上至少在一个任务上达到了最先进的性能。'
- en: 'This style of architecture is further extended by the recently proposed MTI-Net
    Vandenhende et al. [[2020](#bib.bib152)], which models task interactions at multiple
    scales of the receptive field. Specifically, the architecture consists of a backbone
    that extracts multi-scale features, and features from each scale are used to make
    preliminary task predictions. The initial predictions from the 1/32 scale are
    combined with 1/16 scale features to form the input for predictions at the 1/16
    scale, then the predictions from the 1/16 scale are used as input to make predictions
    at the 1/8 scale, etc. After predictions are made from each scale, the predictions
    are distilled between tasks and aggregated across scales to make the final refined
    task predictions. The motivation behind this multi-scale interaction network comes
    from the fact that one task’s features or ground-truth outputs may only be informative
    for learning another task at some (but not all) scales. The authors consider an
    example of adjacent cars: at the local level, when only considering small image
    patches, the depth discontinuity between cars suggests that there should be a
    change in the semantic labels across this discontinuity. At the global level,
    however, one can see that the objects surrounding the depth discontinuity have
    the same semantic label, which contradicts the supposed task interaction at the
    local level. This scenario suggests that multi-scale information should be considered
    when distilling predictions across tasks, and indeed this model does show a larger
    improvement over single-task counterparts than high-performing baselines.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最近提出的 MTI-Net Vandenhende et al. [[2020](#bib.bib152)] 进一步扩展了这种架构，它在感受野的多个尺度上建模任务交互。具体来说，该架构包含一个提取多尺度特征的主干，每个尺度的特征用于进行初步任务预测。来自
    1/32 尺度的初始预测与 1/16 尺度的特征结合，形成 1/16 尺度预测的输入，然后 1/16 尺度的预测作为输入进行 1/8 尺度的预测，依此类推。在从每个尺度生成预测后，预测在任务之间进行蒸馏，并在尺度间聚合，以生成最终的精细任务预测。这种多尺度交互网络的动机来自于这样一个事实：一个任务的特征或真实输出可能仅在某些（但不是所有）尺度上对学习另一个任务有用。作者考虑了相邻汽车的例子：在局部层面上，当仅考虑小的图像块时，汽车之间的深度不连续性表明应该在此不连续性处改变语义标签。然而，在全局层面上，可以看到深度不连续性周围的物体具有相同的语义标签，这与局部层面的任务交互假设相矛盾。这种情况表明，在任务之间蒸馏预测时应该考虑多尺度信息，确实，这种模型相对于单任务对比基线显示了更大的改进。
- en: 2.1.4 Task Routing
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 任务路由
- en: 'Despite their success, shared trunk and cross-talk architectures are somewhat
    rigid in their parameter sharing scheme. Strezoski et al. [[2019a](#bib.bib143)]
    presents an architecture which is more flexible, allowing for fine-grained parameter
    sharing between tasks that occurs at the feature level instead of the layer level.
    The novel component of this architecture is the Task Routing Layer which applies
    a task-specific binary mask to the output of a convolutional layer to which it
    is applied, zeroing out a subset of the computed features and effectively assigning
    a subnetwork to each task which overlaps with that of other tasks. The binary
    masks are not learned, instead they are randomly initialized at the beginning
    of training and fixed from that point on. Although this random initialization
    doesn’t allow for the possibility of a principled parameter sharing scheme between
    tasks, the user still has control over the degree of sharing between tasks through
    the use of a hyperparameter $\sigma$, known as the sharing ratio. $\sigma$ takes
    values between 0 and 1, specifying the proportion of units in each layer which
    are task-specific, and the random initialization of the binary masks in each layer
    are executed in a way to fit this constraint. The proposed architecture only requires
    a small increase in the number of parameters as the number of tasks increases,
    and experiments demonstrate superior performance over MTL baselines such as the
    Cross-Stitch network. Impressively, the Task Routing Layer allows for the network
    to scale up to handle up to 312 tasks simultaneously while maintaining decent
    performance. The Task Routing Layer is strongly related to the learned architectures
    Piggyback Mallya et al. [[2018](#bib.bib106)] and Sparse Sharing Architectures
    Sun et al. [[2019a](#bib.bib146)] (discussed in section [2.5.4](#S2.SS5.SSS4 "2.5.4
    Fine-Grained Sharing ‣ 2.5 Learned Architectures ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey")), though in these
    works the binary masks which assign a set of units to each task are learned.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们取得了成功，共享主干和交叉通信架构在参数共享方案上仍显得有些僵化。Strezoski 等人 [[2019a](#bib.bib143)] 提出了一个更灵活的架构，允许在特征层级而非层级层面上进行细粒度的参数共享。这个架构的创新组成部分是任务路由层，它对卷积层的输出应用任务特定的二进制掩码，将计算出的特征的一个子集置零，从而有效地为每个任务分配一个与其他任务重叠的子网络。二进制掩码不是通过学习获得的，而是在训练开始时随机初始化，并从那时起保持不变。虽然这种随机初始化不允许任务之间进行有原则的参数共享，但用户仍然可以通过使用超参数
    $\sigma$（称为共享比率）来控制任务之间的共享程度。$\sigma$ 的取值在 0 和 1 之间，指定每层中任务特定单元的比例，并且每层中二进制掩码的随机初始化是以适应这一约束的方式执行的。所提议的架构只需在任务数量增加时略微增加参数数量，实验表明其性能优于如
    Cross-Stitch 网络等 MTL 基线。令人印象深刻的是，任务路由层允许网络扩展到同时处理最多 312 个任务，同时保持良好的性能。任务路由层与学习到的架构
    Piggyback Mallya 等人 [[2018](#bib.bib106)] 和稀疏共享架构 Sun 等人 [[2019a](#bib.bib146)]（在[2.5.4](#S2.SS5.SSS4
    "2.5.4 细粒度共享 ‣ 2.5 学习到的架构 ‣ 2 多任务架构 ‣ 基于深度神经网络的多任务学习：一项调查")部分讨论）密切相关，尽管在这些工作中，分配给每个任务的一组单元的二进制掩码是通过学习获得的。
- en: 2.1.5 Single Tasking
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5 单任务处理
- en: 'Nearly every multi-task architecture for computer vision produces output for
    multiple tasks from the same given input, and each one we have discussed so far
    satisfies this condition. Maninis et al. [[2019](#bib.bib107)] is, to our knowledge,
    the only such method which handles a single task at once, but can be used for
    multiple tasks with multiple forward passes. The authors argue that, since the
    network only performs inference for a single task at a time, the network is better
    able to leverage task-specific information and disregard information useful for
    other tasks. This focusing is accomplished through the use of two different attention
    mechanisms: task-specific data-dependent modulation Perez et al. [[2018](#bib.bib121)]
    and task-specific Residual Adapter blocks Rebuffi et al. [[2018](#bib.bib126)].
    The network is also trained with an adversarial loss Liu et al. [[2017](#bib.bib92)]
    to encourage the gradients from each task to be indistinguishable. The idea of
    using an adversarial setup to encourage similar gradient directions between tasks
    has also been explored outside of the realm of computer vision, and is discussed
    further in section [3.4.1](#S3.SS4.SSS1 "3.4.1 Adversarial Gradient Modulation
    ‣ 3.4 Gradient Modulation ‣ 3 Optimization for Multi-Task Learning ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey").'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '几乎每种计算机视觉的多任务架构都从相同的输入中产生多个任务的输出，我们迄今讨论的每一种都满足这一条件。Maninis等人[[2019](#bib.bib107)]
    是我们所知的唯一一种一次处理单个任务的方法，但可以通过多个前向传递用于多个任务。作者认为，由于网络一次仅对单个任务进行推断，网络能够更好地利用任务特定的信息并忽略对其他任务有用的信息。这种聚焦是通过使用两种不同的注意力机制实现的：任务特定的数据依赖调制Perez等人[[2018](#bib.bib121)]和任务特定的Residual
    Adapter块Rebuffi等人[[2018](#bib.bib126)]。该网络还通过对抗性损失Liu等人[[2017](#bib.bib92)]进行训练，以鼓励来自每个任务的梯度不可区分。使用对抗性设置来鼓励任务之间相似梯度方向的想法也在计算机视觉领域之外得到探索，并在[3.4.1](#S3.SS4.SSS1
    "3.4.1 Adversarial Gradient Modulation ‣ 3.4 Gradient Modulation ‣ 3 Optimization
    for Multi-Task Learning ‣ Multi-Task Learning with Deep Neural Networks: A Survey")节中进一步讨论。'
- en: 2.2 Architectures for Natural Language Processing
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 自然语言处理的架构
- en: Natural language processing naturally lends itself well to MTL, due to the abundance
    of related questions one can ask about a given piece of text and the task-agnostic
    representations which are so often used in modern NLP techniques. The development
    in neural architectures for NLP has gone through phases in recent years, with
    traditional feed-forward architectures evolving into recurrent models, and recurrent
    models being succeeded by attention based architectures. These phases are reflected
    in the application of these NLP architectures for MTL.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理自然适合于多任务学习（MTL），因为可以对给定文本提出大量相关问题，而且现代NLP技术中常用的任务无关表示也是如此。近年来，NLP领域的神经网络架构经历了不同的阶段，从传统的前馈架构发展到递归模型，然后递归模型被基于注意力的架构所取代。这些阶段在这些NLP架构应用于MTL时有所体现。
- en: It should also be noted that many NLP techniques could be considered as multi-task
    in that they construct general representations which are task-agnostic (such as
    word embeddings), and under this interpretation a discussion of multi-task NLP
    would include a large number of methods which are better known as general NLP
    techniques. Here, for the sake of practicality, we restrict our discussion to
    mostly include techniques which explicitly learn multiple tasks simultaneously
    for the end goal of performing these tasks simultaneously.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 还应注意的是，许多NLP技术可以被认为是多任务的，因为它们构建了任务无关的一般表示（如词嵌入），在这种解释下，多任务NLP的讨论将包括许多更广为人知的一般NLP技术。这里，为了实用性，我们的讨论主要限制于那些明确同时学习多个任务以实现这些任务的技术。
- en: 2.2.1 Traditional Feed-Forward
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 传统前馈
- en: 'Collobert and Weston [[2008](#bib.bib33)], Collobert et al. [[2011](#bib.bib34)],
    Liu et al. [[2015a](#bib.bib95)] all use traditional feed-forward (non-attention
    based) architectures for multi-task NLP. Many of these architectures have a structural
    resemblance to the early shared architectures of computer vision: a shared, global
    feature extractor followed by task-specific output branches. In this case, however,
    the features are word representations. Collobert and Weston [[2008](#bib.bib33)]
    uses a shared lookup table layer to learn word representations, where the parameters
    of each word vector are directly learned through gradient descent. The remainder
    of the architecture is task-specific, and comprised of convolutions, max over
    time, fully connected layers, and a softmax output. The seminal work Collobert
    et al. [[2011](#bib.bib34)] is motivated by the general principles of MTL: representations
    which are shared across tasks generalize better, and sharing can improve performance
    on all tasks. Their architecture is similar to that of Collobert and Weston [[2008](#bib.bib33)],
    with lookup tables followed by sequences of convolutions and linear transformations.
    The main architectural difference is that the first hidden layer (whether it be
    linear or convolutional) following the lookup tables is shared between tasks.
    Following this trend, the architecture of Liu et al. [[2015a](#bib.bib95)] has
    a similar degree of sharing, and is pictured in figure [6](#S2.F6 "Figure 6 ‣
    2.2.1 Traditional Feed-Forward ‣ 2.2 Architectures for Natural Language Processing
    ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey"). In this case word vectors aren’t learned directly. Instead, the input
    sentence or document is converted into a bag-of-words representation, and hashed
    into letter 3-grams. These features are then fed into a shared linear projection
    followed by a tanh activation function, and then fed to task specific output branches.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 'Collobert 和 Weston [[2008](#bib.bib33)]、Collobert 等人 [[2011](#bib.bib34)]、Liu
    等人 [[2015a](#bib.bib95)] 都使用了传统的前馈（非注意力机制）架构来进行多任务自然语言处理。这些架构中的许多具有与早期计算机视觉的共享架构相似的结构特征：一个共享的、全局的特征提取器，后跟任务特定的输出分支。然而，在这种情况下，特征是词表示。Collobert
    和 Weston [[2008](#bib.bib33)] 使用了一个共享的查找表层来学习词表示，其中每个词向量的参数通过梯度下降直接学习。其余部分是任务特定的，包括卷积、时间上的最大值、全连接层和一个
    softmax 输出。开创性工作 Collobert 等人 [[2011](#bib.bib34)] 的动机来自于 MTL 的一般原则：跨任务共享的表示更具泛化能力，且共享可以提升所有任务的性能。他们的架构类似于
    Collobert 和 Weston [[2008](#bib.bib33)]，使用查找表，然后是卷积和线性变换序列。主要的架构差异在于，查找表之后的第一个隐藏层（无论是线性还是卷积）在任务间是共享的。沿着这一趋势，Liu
    等人 [[2015a](#bib.bib95)] 的架构具有类似的共享程度，如图 [6](#S2.F6 "Figure 6 ‣ 2.2.1 Traditional
    Feed-Forward ‣ 2.2 Architectures for Natural Language Processing ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey") 所示。在这种情况下，词向量不是直接学习的。而是将输入句子或文档转换为词袋表示，并哈希成字母
    3-grams。这些特征随后被输入到一个共享的线性投影层，经过 tanh 激活函数，再输入到任务特定的输出分支中。'
- en: '![Refer to caption](img/dd53620c9492dbf46725485c7d40eecf.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/dd53620c9492dbf46725485c7d40eecf.png)'
- en: 'Figure 6: Network architecture of Liu et al. [[2015a](#bib.bib95)]. The input
    is converted to a bag-of-words representation and hashed into letter 3-grams,
    followed by a shared linear transformation and nonlinear activation function.
    This shared representation is passed to task-specific outhead heads to compute
    final outputs for each task.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: Liu 等人 [[2015a](#bib.bib95)] 的网络架构。输入被转换为词袋表示并哈希成字母 3-grams，随后经过共享的线性变换和非线性激活函数。这一共享表示被传递给任务特定的输出头，以计算每个任务的最终输出。'
- en: 2.2.2 Recurrence
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 循环
- en: The introduction of modern recurrent neural networks for NLP yielded a new family
    of models for multi-task NLP, with novel recurrent architectures introduced in
    Luong et al. [[2015](#bib.bib104)], Liu et al. [[2016a](#bib.bib90), [b](#bib.bib91)],
    Dong et al. [[2015](#bib.bib43)]. Sequence to sequence learning Sutskever et al.
    [[2014](#bib.bib148)] was adapted for multi-task learning in Luong et al. [[2015](#bib.bib104)].
    In this work, the authors explore three variants of parameter sharing schemes
    for multi-task seq2seq models, which they name one-to-many, many-to-one, and many-to-many.
    In one-to-many, the encoder is shared through all tasks, and the decoder is task-specific.
    This is useful to handle sets of tasks which require differently formatted output,
    such as translating a piece of text into multiple target languages. In many-to-one,
    the encoder is task-specific, while the decoder is shared. This is an inversion
    of the usual parameter sharing scheme in which earlier layers are shared and feed
    into task-specific branches. The many-to-one variant is applicable when the set
    of tasks require output in the same format, such as in image captioning and machine
    translation into the same target language. Lastly, the authors explore the many-to-many
    variant, in which there are multiple shared or task-specific encoders and decoders.
    They use this variant, for example, to jointly train an english to german and
    a german to english translation system, with both an english and german encoder
    and decoder. The english encoder also feeds into the english decoder to perform
    an autoencoder reconstruction task, as does the german encoder. A similar sequence
    to sequence architecture for machine translation is proposed in Dong et al. [[2015](#bib.bib43)]
    with a focus on training a multi-task network to translate one source language
    into multiple target languages.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现代递归神经网络在自然语言处理中的引入产生了一系列新的多任务NLP模型，Luong等人[[2015](#bib.bib104)]、Liu等人[[2016a](#bib.bib90)、[b](#bib.bib91)]和Dong等人[[2015](#bib.bib43)]提出了新颖的递归架构。Sutskever等人[[2014](#bib.bib148)]提出的序列到序列学习被应用于Luong等人[[2015](#bib.bib104)]的多任务学习中。在这项工作中，作者探讨了三种多任务seq2seq模型的参数共享方案变体，分别命名为一对多、多对一和多对多。在一对多中，编码器在所有任务中共享，而解码器是任务特定的。这对于处理需要不同格式输出的任务集非常有用，例如将一段文本翻译成多种目标语言。在多对一中，编码器是任务特定的，而解码器是共享的。这是通常的参数共享方案的逆转，其中早期层共享并输入到任务特定的分支。多对一变体适用于任务集需要相同格式的输出的情况，例如图像描述和翻译到相同目标语言。最后，作者探讨了多对多变体，其中有多个共享或任务特定的编码器和解码器。例如，他们使用此变体来联合训练一个英德翻译系统和一个德英翻译系统，具有英语和德语编码器和解码器。英语编码器还输入到英语解码器以执行自编码器重建任务，德语编码器也是如此。Dong等人[[2015](#bib.bib43)]提出了一种类似的序列到序列架构，用于训练一个多任务网络，将一种源语言翻译成多种目标语言。
- en: 'Liu et al. [[2016a](#bib.bib90)] also explores several variants of recurrent
    multi-task architectures, though in the text classification regime instead of
    sequence to sequence learning. These parameter sharing schemes are generally more
    fine-grained than those described in Luong et al. [[2015](#bib.bib104)], with
    a focus on different methods to allow information flow betwen tasks. The authors
    explore three parameter sharing schemes: the Uniform-Layer, Coupled-Layer, and
    Shared-Layer architectures, which are shown in figure [7](#S2.F7 "Figure 7 ‣ 2.2.2
    Recurrence ‣ 2.2 Architectures for Natural Language Processing ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey"). In
    the Uniform-Layer architecture, each task has its own embedding layer, and all
    tasks share an embedding layer and an LSTM layer. Let $i$ be a task index, $t$
    be the recurrent timestep, and $x_{t}$ be the $t$-th word in an input sentence.
    Then the input to the shared LSTM layer for task $i$ on timestep $t$ is the concatenation
    of the $i$-th task specific embedding of $x_{t}$ with the shared embedding of
    $x_{t}$. For the Coupled-Layer model, each task has its own separate LSTM layer,
    but each task can read from the LSTM layers of the other tasks. More specifically,
    the memory content of the LSTM for a given task at timestep $t$ is modified to
    include a weighted sum of the hidden states of the LSTM layers of each task at
    timestep $t-1$, while preserving all other components of the LSTM. Finally, the
    Shared-Layer architecture allocates a separate LSTM layer for each task, as well
    as a shared bi-directional LSTM layer that feeds into the task-specific LSTMs.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '刘等人[[2016a](#bib.bib90)]还探讨了几种递归多任务架构的变体，尽管是在文本分类任务中，而不是序列到序列学习。这些参数共享方案通常比Luong等人[[2015](#bib.bib104)]所描述的更为细粒度，重点是允许任务之间的信息流动。作者探讨了三种参数共享方案：Uniform-Layer、Coupled-Layer和Shared-Layer架构，如图[7](#S2.F7
    "Figure 7 ‣ 2.2.2 Recurrence ‣ 2.2 Architectures for Natural Language Processing
    ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey")所示。在Uniform-Layer架构中，每个任务有其自己的嵌入层，所有任务共享一个嵌入层和一个LSTM层。设$i$为任务索引，$t$为递归时间步，$x_{t}$为输入句子的第$t$个单词。那么任务$i$在时间步$t$的共享LSTM层的输入是第$i$个任务特定的$x_{t}$的嵌入与$x_{t}$的共享嵌入的连接。对于Coupled-Layer模型，每个任务有其自己的独立LSTM层，但每个任务可以读取其他任务的LSTM层。更具体地说，给定任务在时间步$t$的LSTM的记忆内容被修改为包括每个任务在时间步$t-1$的LSTM层的隐藏状态的加权和，同时保持LSTM的所有其他组件。最后，Shared-Layer架构为每个任务分配一个独立的LSTM层，以及一个共享的双向LSTM层，该层馈送到任务特定的LSTM中。'
- en: '![Refer to caption](img/8d24b7bbe0d2fd3681af4acaf306200c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8d24b7bbe0d2fd3681af4acaf306200c.png)'
- en: 'Figure 7: From top to bottom: Uniform-Layer, Coupled-Layer, and Shared-Layer
    architectures of Liu et al. [[2016a](#bib.bib90)]. Each architecture presents
    a novel partition of recurrent architecture components into shared and task-specific
    modules.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：自上而下：刘等人[[2016a](#bib.bib90)]的Uniform-Layer、Coupled-Layer和Shared-Layer架构。每种架构都呈现了一种新颖的递归架构组件的共享和任务特定模块的划分。
- en: In addition to these recurrent architectures, Liu et al. [[2016b](#bib.bib91)]
    augments the LSTM architecture with a memory mechanism. To form a shared architecture,
    each task has its own LSTM parameters, but the memory is shared among all tasks.
    The memory mechanism is inspired by the memory enhanced LSTM (ME-LSTM) Sukhbaatar
    et al. [[2015](#bib.bib145)]. The novel contribution of Liu et al. [[2016b](#bib.bib91)]
    is in a fusion mechanism that allows the memory to be read from and written to
    jointly by all tasks. With this addition, the hidden state of each task’s LSTM
    is computed from a gated sum of the LSTM’s internal memory and the information
    held in the shared external memory. The authors also introduce a variant in which
    each task has its own private external memory, and the shared global external
    memory is read/written by each task-specific memory module.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些递归架构之外，刘等人[[2016b](#bib.bib91)]将LSTM架构与记忆机制进行了增强。为了形成共享架构，每个任务都有自己的LSTM参数，但记忆在所有任务之间共享。记忆机制受到Sukhbaatar等人[[2015](#bib.bib145)]提出的增强记忆LSTM（ME-LSTM）的启发。刘等人[[2016b](#bib.bib91)]的创新贡献在于一个融合机制，允许所有任务共同读取和写入记忆。通过这个附加机制，每个任务的LSTM的隐藏状态是由LSTM内部记忆和共享外部记忆中的信息的门控和计算得出的。作者还引入了一种变体，其中每个任务都有自己的私有外部记忆，而共享的全局外部记忆由每个任务特定的记忆模块读取/写入。
- en: 2.2.3 Cascaded Information
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 级联信息
- en: 'In all of the NLP architectures we have discussed so far, the sub-architectures
    corresponding to each task have been symmetric. In particular, the output branch
    of each task occurs at the maximum network depth for each task, meaning that supervision
    for the task-specific features of each task occurs at the same depth. Several
    works Søgaard and Goldberg [[2016](#bib.bib139)], Hashimoto et al. [[2016](#bib.bib59)],
    Sanh et al. [[2019](#bib.bib132)] propose supervising “lower-level" tasks at earlier
    layers so that the features learned for these tasks may be used by higher-level
    tasks. By doing this we form an explicit task hierarchy, and provide a direct
    way for information from one task to aid in the solution of another. We refer
    to this template for iterated inference and feature combination as cascaded information,
    with an example pictured in figure [8](#S2.F8 "Figure 8 ‣ 2.2.3 Cascaded Information
    ‣ 2.2 Architectures for Natural Language Processing ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们迄今讨论的所有 NLP 架构中，与每个任务对应的子架构都是对称的。特别是，每个任务的输出分支出现在每个任务的最大网络深度上，这意味着每个任务的任务特定特征的监督发生在相同的深度。几个研究工作
    Søgaard 和 Goldberg [[2016](#bib.bib139)]、Hashimoto 等人 [[2016](#bib.bib59)]、Sanh
    等人 [[2019](#bib.bib132)] 提出了在较早的层次上监督“较低级别”任务，以便这些任务学到的特征可以被更高级别的任务使用。通过这种方式，我们形成了一个明确的任务层次结构，并为一个任务的信息直接帮助解决另一个任务提供了一种途径。我们将这种迭代推理和特征组合的模板称为级联信息，图
    [8](#S2.F8 "Figure 8 ‣ 2.2.3 Cascaded Information ‣ 2.2 Architectures for Natural
    Language Processing ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep
    Neural Networks: A Survey") 中有一个示例。'
- en: 'Søgaard and Goldberg [[2016](#bib.bib139)] forms this hierarchy by choosing
    POS tagging as a low-level task to inform syntactic chunking and CCG supertagging.
    Their network architecture is made of a series of bi-directional RNN layers, and
    for each task $i$ there is an associated layer $\ell_{i}$ from which the task-specific
    classifier for task $i$ stems. In this case, the associated layer for POS tagging
    occurs earlier in the network than the associated layers of syntactic chunking
    and CCG supertagging, so that the learned POS features can inform the tasks of
    syntactic chunking and CCG supertagging. Not long after the publication of Søgaard
    and Goldberg [[2016](#bib.bib139)], Hashimoto et al. [[2016](#bib.bib59)] achieved
    a mix of SOTA and SOTA-competitive results on several language tasks by constructing
    a similarly supervised architecture with 5 tasks: POS tagging, chunking, dependency
    parsing, semantic relatedness, and textual entailment. The authors also replace
    the bi-directional RNN units of Søgaard and Goldberg [[2016](#bib.bib139)] with
    bi-directional LSTM units. Figure [8](#S2.F8 "Figure 8 ‣ 2.2.3 Cascaded Information
    ‣ 2.2 Architectures for Natural Language Processing ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey") shows their architecture.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 'Søgaard 和 Goldberg [[2016](#bib.bib139)] 通过选择词性标注作为低级任务来指导句法块分析和 CCG 超标签化，从而形成了这个层次结构。他们的网络架构由一系列双向
    RNN 层组成，对于每个任务 $i$，都有一个相关的层 $\ell_{i}$，任务 $i$ 的任务特定分类器即来源于该层。在这种情况下，词性标注的相关层出现在网络中比句法块分析和
    CCG 超标签化的相关层要早，从而使得学习到的词性特征可以为句法块分析和 CCG 超标签化任务提供信息。在 Søgaard 和 Goldberg [[2016](#bib.bib139)]
    发表之后不久，Hashimoto 等人 [[2016](#bib.bib59)] 通过构建一个类似的监督架构，完成了在多个语言任务上的 SOTA 及 SOTA
    竞争性结果，该架构包含 5 个任务：词性标注、块分析、依存句法分析、语义相关性和文本蕴涵。作者们还将 Søgaard 和 Goldberg [[2016](#bib.bib139)]
    的双向 RNN 单元替换为双向 LSTM 单元。图 [8](#S2.F8 "Figure 8 ‣ 2.2.3 Cascaded Information ‣
    2.2 Architectures for Natural Language Processing ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey") 显示了他们的架构。'
- en: '![Refer to caption](img/9dfaf2da26bc93ea28a08377d3d378c1.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9dfaf2da26bc93ea28a08377d3d378c1.png)'
- en: 'Figure 8: Various task supervision in various layers from Hashimoto et al.
    [[2016](#bib.bib59)]. Lower level tasks are supervised at earlier layers.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：Hashimoto 等人 [[2016](#bib.bib59)] 的不同任务在不同层次上的监督。较低级别的任务在较早的层次上受到监督。
- en: Besides the increase in the number of tasks, this method also introduces a regularization
    term to avoid training interference between the tasks. Each time a task’s dataset
    is sampled for training, the squared Euclidean distance between the pre-update
    parameters and the current model parameters is added to the loss function. This
    encourages the network parameters not to stray too far from the parameter configuration
    which was learned by training on a different task on the previous epoch.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了任务数量的增加，该方法还引入了正则化项，以避免任务之间的训练干扰。每次从任务数据集中抽样进行训练时，都会将更新前参数与当前模型参数之间的平方欧氏距离添加到损失函数中。这鼓励网络参数不要偏离前一轮训练中在不同任务上学习的参数配置。
- en: Following these two works, Sanh et al. [[2019](#bib.bib132)] introduces a similarly
    inspired model for a different set of tasks, achieving SOTA results for Named
    Entity Recognition, Entity Mention Detection and Relation Extraction. In order
    from lowest to highest, the task hierarchy in this work is NER, EMD, and coreference
    resolution/relation extraction (equally ranked as highest level).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两项工作之后，Sanh 等人 [[2019](#bib.bib132)] 引入了一个类似灵感的模型用于一组不同的任务，在命名实体识别、实体提及检测和关系抽取中取得了
    SOTA 结果。在这项工作中，任务层次结构从低到高依次为 NER、EMD 和共指解析/关系抽取（同样被认为是最高级别）。
- en: 2.2.4 Adversarial Feature Separation
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 对抗特征分离
- en: In a novel application of adversarial methods, Liu et al. [[2017](#bib.bib92)]
    introduces an adversarial learning framework for multi-task learning in order
    to distill learned features into task-specific and task-agnostic subspaces. Their
    architecture is comprised of a single shared LSTM layer and one task-specific
    LSTM layer per task. Once the input sentence from a task is passed through the
    shared LSTM layer and the task-specific LSTM layer, the two outputs are concatenated
    and used as the final features to perform inference on. However, the features
    produced by the shared LSTM layer are also fed into the task discriminator. The
    task discriminator is a linear transformation followed by a softmax layer that
    is trained to predict which task the original input sentence came from. The shared
    LSTM layer is then trained to jointly minimize the task loss along with the discriminator
    loss, so that the features produced by the shared LSTM do not contain any task-specific
    information. In addition, the shared features and the task specific features are
    encouraged to encode separate information with the use of an orthogonality penalty
    (similar to Ruder et al. [[2019](#bib.bib130)]) on the resulting features. More
    specifically, the orthogonality loss is defined as the squared Frobenius norm
    of the product of the task-specific features and the shared features. This loss
    is added to the overall training objective, in order to encourage the task-specific
    and the shared features to be orthogonal. These two auxiliary losses enforce the
    separation of task-specific and task-agnostic information in the shared network.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗方法的一个新颖应用中，Liu 等人 [[2017](#bib.bib92)] 引入了一个用于多任务学习的对抗学习框架，以将学习到的特征提炼为任务特定的和任务无关的子空间。他们的架构由一个共享的
    LSTM 层和每个任务一个任务特定的 LSTM 层组成。当一个任务的输入句子通过共享 LSTM 层和任务特定的 LSTM 层后，两个输出会被连接起来，并作为最终特征进行推理。然而，来自共享
    LSTM 层的特征也会被输入到任务判别器中。任务判别器是一个线性变换后接一个 softmax 层，训练用于预测原始输入句子来自哪个任务。然后，共享 LSTM
    层被训练以联合最小化任务损失和判别器损失，从而使共享 LSTM 产生的特征不包含任何任务特定信息。此外，利用正交惩罚（类似于 Ruder 等人 [[2019](#bib.bib130)])
    鼓励共享特征和任务特定特征编码不同的信息。更具体地说，正交损失定义为任务特定特征和共享特征的乘积的平方 Frobenius 范数。此损失被添加到总体训练目标中，以鼓励任务特定特征和共享特征之间的正交性。这两个辅助损失强制执行任务特定信息和任务无关信息在共享网络中的分离。
- en: 2.2.5 BERT for MTL
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5 BERT用于多任务学习
- en: 'Despite the popularity of the Bidirectional Encoder Representations from Transformers
    (BERT) Devlin et al. [[2018](#bib.bib41)], there have been surprisingly little
    applications of the text encoding method for MTL. Liu et al. [[2019b](#bib.bib96)]
    extends the work of Liu et al. [[2015a](#bib.bib95)] by adding shared BERT embedding
    layers into the architecture. The network architecture overall is quite similar
    to Liu et al. [[2019b](#bib.bib96)], the only difference being the addition of
    BERT contextual embedding layers following the input embedding vectors in figure
    [6](#S2.F6 "Figure 6 ‣ 2.2.1 Traditional Feed-Forward ‣ 2.2 Architectures for
    Natural Language Processing ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey"). This new MTL architecture, named MT-DNN,
    achieved SOTA performance on eight out of nine GLUE tasks Wang et al. [[2018](#bib.bib153)]
    at the time of its publication.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管双向编码器表示（BERT） Devlin et al. [[2018](#bib.bib41)] 受到了广泛关注，但文本编码方法在MTL中的应用却出奇地少。Liu
    et al. [[2019b](#bib.bib96)] 通过在架构中添加共享BERT嵌入层扩展了Liu et al. [[2015a](#bib.bib95)]
    的工作。总体上，网络架构与Liu et al. [[2019b](#bib.bib96)] 的工作非常相似，唯一的区别是添加了BERT上下文嵌入层，位于输入嵌入向量之后，如图
    [6](#S2.F6 "Figure 6 ‣ 2.2.1 Traditional Feed-Forward ‣ 2.2 Architectures for
    Natural Language Processing ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey") 所示。这种新的MTL架构，称为MT-DNN，在其发布时在九个GLUE任务中的八个上达到了SOTA表现
    Wang et al. [[2018](#bib.bib153)]。'
- en: 2.3 Architectures for Reinforcement Learning
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 强化学习的架构
- en: In recent years, many of the advances in reinforcement learning have focused
    on optimization and training methods Schulman et al. [[2017](#bib.bib133)], Haarnoja
    et al. [[2018](#bib.bib58)], Akkaya et al. [[2019](#bib.bib4)]. Since many RL
    problems don’t necessarily involve complex perception, such as working with words
    or pixels, the architectural demand isn’t as high for many RL problems. Because
    of this, many deep networks for RL are simple fully-connected, convolutional,
    or recurrent architectures. However, in the multi-task case, there are several
    instances of interesting works which leverage information between tasks to create
    improved architectures for RL.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，强化学习的许多进展集中在优化和训练方法上 Schulman et al. [[2017](#bib.bib133)]，Haarnoja et
    al. [[2018](#bib.bib58)]，Akkaya et al. [[2019](#bib.bib4)]。由于许多RL问题并不涉及复杂的感知，如处理单词或像素，许多RL问题对架构的要求并不高。因此，许多用于RL的深度网络是简单的全连接、卷积或递归架构。然而，在多任务情况下，有几个有趣的研究利用任务之间的信息创建了改进的RL架构。
- en: 2.3.1 Joint Task Training
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 联合任务训练
- en: 'Several works in RL have found that task performance can be improved by simply
    training for multiple tasks jointly, with or without parameter sharing. Pinto
    and Gupta [[2017](#bib.bib122)] uses a shared trunk architecture (shown in figure
    [9](#S2.F9 "Figure 9 ‣ 2.3.1 Joint Task Training ‣ 2.3 Architectures for Reinforcement
    Learning ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey") to jointly learn robotic grasping, pushing, and poking from pixels.
    The shared feature extractor consists of three convolutional layers, and these
    shared features are fed to three task-specific output branches. The grasping and
    poking output branches are made of three fully-connected layers each, and the
    pushing branch has one convolutional layer, followed by two fully-connected layers.
    This shared network is trained with a supervised loss which is an average of cross-entropy
    and squared Euclidean losses, one for each task. The network actions are parameterized
    in such a way to allow for supervised training. The authors find that this shared
    network trained with 2500 examples of both pushing and grasping outperforms a
    task-specific grasping network trained with 5000 examples. Zeng et al. [[2018](#bib.bib167)]
    also finds advantages by jointly training with robotic pushing and grasping, though
    their architecture does not employ any parameter sharing. The network is comprised
    of two separate fully convolutional Q-networks, one for pushing and one for grasping.
    The two networks are, however, given a joint training signal. The reward for a
    timestep $t$ is defined as follows: if a grasping action is chosen at timestep
    $t$ and the grasp is successful, the reward is 1\. If a pushing action is chosen
    at timestep $t$ and the action causes a sufficiently large change in the environment,
    then the reward is 0.5\. From this reward, there is no explicit encouragement
    of one task to aid another. But when both networks are jointly trained to maximize
    the same reward, the pushing network learns to push in a way that influences the
    environment to maximize the grasping reward. This joint training setup was shown
    to be much more sample efficient than baselines, making training on a physical
    robot feasible with only a few hours of training.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '在强化学习中，几项研究发现通过联合训练多个任务（无论是否共享参数）可以提高任务性能。Pinto和Gupta [[2017](#bib.bib122)]
    使用了一种共享的主干架构（如图 [9](#S2.F9 "Figure 9 ‣ 2.3.1 Joint Task Training ‣ 2.3 Architectures
    for Reinforcement Learning ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey") 所示），从像素中联合学习机器人抓取、推送和戳击。共享的特征提取器由三层卷积层组成，这些共享特征被送入三个任务特定的输出分支。抓取和戳击的输出分支由三层全连接层组成，而推送分支有一层卷积层，后接两层全连接层。这个共享网络通过一种监督损失进行训练，该损失是交叉熵和平方欧几里得损失的平均值，每个任务一个。网络动作的参数化方式允许进行监督训练。作者发现，使用2500个推送和抓取样本训练的共享网络比使用5000个样本训练的任务特定抓取网络表现更佳。Zeng等人
    [[2018](#bib.bib167)] 也发现通过机器人推送和抓取的联合训练具有优势，尽管他们的架构没有使用任何参数共享。该网络由两个独立的全卷积Q网络组成，一个用于推送，一个用于抓取。然而，这两个网络都接受了联合训练信号。时间步$t$的奖励定义如下：如果在时间步$t$选择了抓取动作并且抓取成功，奖励为1。如果在时间步$t$选择了推送动作并且该动作导致环境发生足够大的变化，则奖励为0.5。根据该奖励，没有明确的鼓励一个任务帮助另一个任务。但是，当两个网络被联合训练以最大化相同的奖励时，推送网络会学习以影响环境的方式进行推送，以最大化抓取奖励。这个联合训练设置被证明比基准方法更具样本效率，使得在物理机器人上进行训练只需几个小时的训练即可实现。'
- en: '![Refer to caption](img/c5ddc5ea7eb69968f8bf48451c27ddc4.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c5ddc5ea7eb69968f8bf48451c27ddc4.png)'
- en: 'Figure 9: Shared architecture for robotic grasping, pushing, and poking Pinto
    and Gupta [[2017](#bib.bib122)].'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：Pinto和Gupta [[2017](#bib.bib122)] 的机器人抓取、推送和戳击的共享架构。
- en: 2.3.2 Modular Policies
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 模块化策略
- en: 'There have been many similarities between the various parameter sharing schemes
    that we have discussed so far, but modular networks are present a novel family
    of parameter sharing methods which are totally different from the shared trunk
    or cross-talk architectures from sections [2.1.1](#S2.SS1.SSS1 "2.1.1 Shared Trunk
    ‣ 2.1 Architectures for Computer Vision ‣ 2 Multi-Task Architectures ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey") and [2.1.2](#S2.SS1.SSS2 "2.1.2
    Cross-Talk ‣ 2.1 Architectures for Computer Vision ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey"). In modular learning
    setups, each task’s network architecture is composed of a combination of smaller
    sub-networks, and these smaller sub-networks are combined in different ways for
    different tasks. Just as MTL is motivated by generality through shared representations,
    modular learning offers generality of computation through shared neural building
    blocks. The goal of these setups is to learn building blocks which are general
    enough to be useful as a part of the network architecture for multiple tasks.
    We discuss several other learned modular architectures in sections [2.5](#S2.SS5
    "2.5 Learned Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey") and [2.6](#S2.SS6 "2.6 Conditional Architectures
    ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey"), but here we only discuss those modular methods for which the parameters
    of the building blocks are learned and the configuration of building blocks for
    each task remains fixed. Discussion of modular methods with learned building block
    combinations can be found in the aforementioned sections.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的各种参数共享方案之间有很多相似之处，但模块化网络提出了一种全新的参数共享方法，与[2.1.1](#S2.SS1.SSS1 "2.1.1 共享主干
    ‣ 2.1 计算机视觉架构 ‣ 2 多任务架构 ‣ 深度神经网络的多任务学习：综述")和[2.1.2](#S2.SS1.SSS2 "2.1.2 交叉谈话 ‣
    2.1 计算机视觉架构 ‣ 2 多任务架构 ‣ 深度神经网络的多任务学习：综述")中的共享主干或交叉谈话架构完全不同。在模块化学习设置中，每个任务的网络架构由多个较小的子网络组合而成，这些较小的子网络以不同的方式组合以适应不同的任务。正如
    MTL 通过共享表示来推动通用性，模块化学习通过共享神经构建块提供计算的通用性。这些设置的目标是学习足够通用的构建块，以便作为多个任务网络架构的一部分。我们在
    [2.5](#S2.SS5 "2.5 学习的架构 ‣ 2 多任务架构 ‣ 深度神经网络的多任务学习：综述") 和 [2.6](#S2.SS6 "2.6 条件架构
    ‣ 2 多任务架构 ‣ 深度神经网络的多任务学习：综述") 中讨论了其他几种学习的模块化架构，但这里仅讨论那些构建块的参数被学习且每个任务的构建块配置保持不变的模块化方法。有关具有学习的构建块组合的模块化方法的讨论可以在上述章节中找到。
- en: 'Within weeks of each other, Heess et al. [[2016](#bib.bib61)] and Devin et al.
    [[2017](#bib.bib40)] both introduced modular neural network policies for multi-task
    learning across various robots. The architectures of each of these works are depicted
    in figures [10](#S2.F10 "Figure 10 ‣ 2.3.2 Modular Policies ‣ 2.3 Architectures
    for Reinforcement Learning ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey") and [11](#S2.F11 "Figure 11 ‣ 2.3.2 Modular
    Policies ‣ 2.3 Architectures for Reinforcement Learning ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey"), respectively.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在几周内，Heess 等人 [[2016](#bib.bib61)] 和 Devin 等人 [[2017](#bib.bib40)] 都介绍了用于多任务学习的模块化神经网络策略。这些工作的架构分别在图
    [10](#S2.F10 "图 10 ‣ 2.3.2 模块化策略 ‣ 2.3 强化学习架构 ‣ 2 多任务架构 ‣ 深度神经网络的多任务学习：综述") 和
    [11](#S2.F11 "图 11 ‣ 2.3.2 模块化策略 ‣ 2.3 强化学习架构 ‣ 2 多任务架构 ‣ 深度神经网络的多任务学习：综述") 中展示。
- en: '![Refer to caption](img/5f0fcd56211269c55366808c7edd719d.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5f0fcd56211269c55366808c7edd719d.png)'
- en: 'Figure 10: Shared modular architecture for locomotion with multiple robots
    Heess et al. [[2016](#bib.bib61)]. Note that the high-level module updates the
    modulated input to the low-level module at a different frequency than it itself
    receives input from the environment.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：多个机器人共享的模块化运动架构 Heess 等人 [[2016](#bib.bib61)]。请注意，高层模块以不同于其接收环境输入的频率来更新调制后的低层模块输入。
- en: '![Refer to caption](img/082f6230ad6f2645582a0a748afddd50.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/082f6230ad6f2645582a0a748afddd50.png)'
- en: 'Figure 11: Shared modular architecture for multi-task and multi-robot transfer
    Devin et al. [[2017](#bib.bib40)]. Each network is made of two modules, one robot
    module and one task module. Each robot module can be combined with a task module
    to form a network to perform each (task, robot) pair.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：Devin等人提出的多任务和多机器人转移的共享模块化架构[[2017](#bib.bib40)]。每个网络由两个模块组成，一个是机器人模块，一个是任务模块。每个机器人模块可以与任务模块组合，形成一个网络来执行每一个（任务，机器人）对。
- en: The task architecture of Heess et al. [[2016](#bib.bib61)] is made of two modules,
    a low-level “spinal" network and a high-level “cortical" network. The spinal network
    has access to proprioceptive information like muscle tension, and it chooses motor
    actions, while the cortical network has access to all observations and modulates
    inputs to the spinal network. It is important to note that the proprioceptive
    information given to the spinal network is always task-independent, so that the
    spinal network must learn task-independent representations. In their experiments,
    the low-level/spinal network is feed-forward, while the high-level/cortical network
    is recurrent. The combination of the division of labor between the two modules
    and the information hiding from the spinal network allows for a pre-trained spinal
    network to be deployed with a new cortical network to quickly solve a new task
    with the same robot body. The usage of the pre-trained spinal network allows for
    effective exploration in the robot body, despite the new task.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Heess等人[[2016](#bib.bib61)]的任务架构由两个模块组成，一个是低层的“脊髓”网络，一个是高层的“皮层”网络。脊髓网络可以访问本体感觉信息，如肌肉张力，并选择运动动作，而皮层网络可以访问所有观察数据并调节对脊髓网络的输入。需要注意的是，提供给脊髓网络的本体感觉信息总是与任务无关的，因此脊髓网络必须学习与任务无关的表征。在他们的实验中，低层/脊髓网络是前馈的，而高层/皮层网络是递归的。两个模块之间的劳动分工和脊髓网络的信息隐藏的组合使得预训练的脊髓网络能够与新的皮层网络配合，快速解决具有相同机器人身体的新任务。使用预训练的脊髓网络可以有效地在机器人身体中进行探索，尽管任务是新的。
- en: 'The architecture of Devin et al. [[2017](#bib.bib40)] is similarly inspired,
    but employs parameter sharing for the network controllers between different robots
    as well as between tasks. Each task and robot has its own network module. The
    network for each task/robot pair is composed of the corresponding task-specific
    module, followed by the corresponding robot-specific module, as shown in figure
    [11](#S2.F11 "Figure 11 ‣ 2.3.2 Modular Policies ‣ 2.3 Architectures for Reinforcement
    Learning ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey") Because each module is shared between tasks and robots, it is constrained
    to learn information which is general across its domains. The authors also show
    that the learned modules can be paired in combinations unseen during training,
    to instantiate a policy with zero-shot generalization capabilities. This method
    also gives partial information to the task module; each observation is decomposed
    into a task-specific portion and a robot-specific portion. The task-specific module
    only receives the task-specific observation as input, and the robot-specific module
    receives the robot-specific observation as well as the output of the task-specific
    module.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Devin等人[[2017](#bib.bib40)]的架构也受到类似的启发，但在不同机器人和任务之间的网络控制器中采用了参数共享。每个任务和机器人都有自己专属的网络模块。每个任务/机器人对的网络由相应的任务特定模块和相应的机器人特定模块组成，如图[11](#S2.F11
    "图 11 ‣ 2.3.2 模块化策略 ‣ 2.3 强化学习架构 ‣ 2 多任务架构 ‣ 多任务学习与深度神经网络：综述")所示。由于每个模块在任务和机器人之间共享，因此它被限制为学习其领域中的通用信息。作者还展示了学习到的模块可以在训练过程中未见过的组合中配对，从而实现零样本泛化能力的策略。这种方法还向任务模块提供了部分信息；每个观察值被分解为任务特定部分和机器人特定部分。任务特定模块仅接收任务特定观察值作为输入，而机器人特定模块接收机器人特定观察值以及任务特定模块的输出。
- en: 'Both of these architectures exhibit an interesting strategy for learning general
    representations across tasks: information hiding. We have so far discussed parameter
    sharing, adversarial methods, and orthogonality constraints as regularization
    strategies for multi-task methods. But the division of labor brought forth by
    the modularity in these two architectures allows for information to be restricted
    to certain modules in the network, forcing the modules missing this information
    to learn representations which are invariant to the omitted information. In this
    case, we obtain modules which are invariant to the task at hand.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种架构都展现了一种有趣的策略，用于学习跨任务的通用表示：信息隐藏。我们到目前为止讨论了参数共享、对抗方法和正交性约束作为多任务方法的正则化策略。但这两种架构的模块化带来的劳动分工使得信息被限制在网络的某些模块中，迫使缺少该信息的模块学习对遗漏信息不变的表示。在这种情况下，我们获得了对当前任务不变的模块。
- en: 'RL with Policy Sketches Andreas et al. [[2017](#bib.bib8)] is another template
    for policy modularity which was proposed soon after Heess et al. [[2016](#bib.bib61)]
    and Devin et al. [[2017](#bib.bib40)], in which the policy for a task is composed
    of several subpolicies, and each subpolicy is a neural network whose parameters
    are shared between tasks. The composition of subpolicies for each task is defined
    by a human-provided “policy sketch", which roughly outlines the steps to complete
    a task. For example, in the minecraft-inspired environment used for evaluation
    in the paper, the tasks “Make Planks" and “Make Sticks" may have the policy sketches
    (get wood, use workbench) and (get wood, use toolshed), respectively. In this
    case, the policies for these tasks would use the subpolicies $\pi_{\text{wood}}$,
    $\pi_{\text{bench}}$, and $\pi_{\text{shed}}$, with the compositions $\pi_{\text{planks}}=(\pi_{\text{wood}},\pi_{\text{bench}})$
    and $\pi_{\text{sticks}}=(\pi_{\text{wood}},\pi_{\text{shed}})$. The weak supervision
    provided by the policy sketches defines a sharing structure of subpolicies between
    tasks, which was shown to be more beneficial to learning than unsupervised option
    discovery. This process is similar to how the syntactical strcture of a question
    defines the composition of subpolicies in Neural Module Networks Andreas et al.
    [[2016](#bib.bib7)] (discussed in section [2.6](#S2.SS6 "2.6 Conditional Architectures
    ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey")), though in that example the composed architecture is dependent on
    each individual given question, while the composition of subpolicies remains fixed
    for each task with Policy Sketches. It is important to note that module composition
    takes two different forms with Policy Sketches and the architectures of Heess
    et al. [[2016](#bib.bib61)] and Devin et al. [[2017](#bib.bib40)]: subpolicies
    in Policy Sketches behave as in hierarchical reinforcement learning Kulkarni et al.
    [[2016](#bib.bib78)], where a subpolicy is chosen to act as the policy until some
    termination condition is met, as opposed to composition in the sense of function
    composition as in Heess et al. [[2016](#bib.bib61)] and Devin et al. [[2017](#bib.bib40)].'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'RL 与政策草图 Andreas 等人 [[2017](#bib.bib8)] 是另一种政策模块化的模板，该模板在 Heess 等人 [[2016](#bib.bib61)]
    和 Devin 等人 [[2017](#bib.bib40)] 提出之后不久被提出，其中任务的政策由若干子政策组成，每个子政策是一个神经网络，其参数在任务之间共享。每个任务的子政策组合由人为提供的“政策草图”定义，该草图大致概述了完成任务的步骤。例如，在论文中用于评估的以
    Minecraft 为灵感的环境中，任务“制作木板”和“制作木棍”可能具有政策草图（获取木材，使用工作台）和（获取木材，使用工具棚），分别。在这种情况下，这些任务的政策将使用子政策
    $\pi_{\text{wood}}$、$\pi_{\text{bench}}$ 和 $\pi_{\text{shed}}$，其组合为 $\pi_{\text{planks}}=(\pi_{\text{wood}},\pi_{\text{bench}})$
    和 $\pi_{\text{sticks}}=(\pi_{\text{wood}},\pi_{\text{shed}})$。政策草图提供的弱监督定义了任务之间子政策的共享结构，这被证明比无监督选项发现对学习更有益。这个过程类似于
    Neural Module Networks Andreas 等人 [[2016](#bib.bib7)] 中的问题的语法结构定义了子政策的组合（在 [2.6](#S2.SS6
    "2.6 Conditional Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey") 节讨论过），尽管在那个例子中，组合架构依赖于每个单独的问题，而在政策草图中子政策的组合对于每个任务保持不变。需要注意的是，模块组合在政策草图和
    Heess 等人 [[2016](#bib.bib61)] 以及 Devin 等人 [[2017](#bib.bib40)] 的架构中采取了两种不同的形式：政策草图中的子政策表现得像在层次化强化学习
    Kulkarni 等人 [[2016](#bib.bib78)] 中一样，其中子政策被选择作为政策，直到满足某个终止条件，而不是像 Heess 等人 [[2016](#bib.bib61)]
    和 Devin 等人 [[2017](#bib.bib40)] 中那样的函数组合形式。'
- en: 2.3.3 Multiple Auxiliary Tasks
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 多重辅助任务
- en: Jaderberg et al. [[2016](#bib.bib67)] introduces several unsupervised auxiliary
    tasks to be learned in conjuction with a main task, as an additional form of supervision.
    These auxiliary tasks encourage general representations in the usual sense for
    MTL, but they also help to decrease the sparsity of rewards in the original task.
    The architecture is a CNN-LSTM actor-critic with a shared trunk, and output branches
    for each auxiliary task that requires its own output. The auxiliary tasks themselves
    are called pixel control, feature control, and reward prediction. Pixel control
    shares parameters from the agent CNN and LSTM, and branches off into a task-specific
    branch that chooses its own actions. The actions are rewarded for causing maximal
    change in the pixel intesity of the pixels observed as a result of the chosen
    action. Feature control does not require an output, and instead the agent is rewarded
    for activating the hidden units of a given hidden layer of the agent network.
    Lastly, reward prediction uses the agent’s CNN to map three recent frames to a
    prediction of the reward on the next step. These auxiliary tasks do not require
    any supervision that isn’t provided by the environment dynamics and are general
    enough to apply to many different problem settings. Training an agent with these
    simple auxiliary tasks led to SOTA performance on the Arcade Learning Environment
    Bellemare et al. [[2013](#bib.bib14)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Jaderberg 等人 [[2016](#bib.bib67)] 引入了几种无监督的辅助任务，与主任务一起学习，作为一种额外的监督形式。这些辅助任务在通常意义上鼓励
    MTL 中的通用表示，但它们也有助于减少原任务中的奖励稀疏性。该架构是一个 CNN-LSTM 演员-评论家结构，具有共享的主干，并且为每个需要自己输出的辅助任务设置输出分支。辅助任务本身称为像素控制、特征控制和奖励预测。像素控制共享来自代理
    CNN 和 LSTM 的参数，并分支到一个任务特定的分支，该分支选择自己的动作。这些动作会因为导致所选动作导致的像素强度最大变化而获得奖励。特征控制不需要输出，相反，代理会因为激活代理网络中给定隐藏层的隐藏单元而获得奖励。最后，奖励预测利用代理的
    CNN 将三个最近的帧映射到下一步奖励的预测。这些辅助任务不需要环境动态以外的监督，并且足够通用以适用于许多不同的问题设置。使用这些简单的辅助任务训练代理导致了
    Arcade Learning Environment Bellemare 等人 [[2013](#bib.bib14)] 上的 SOTA 性能。
- en: 2.4 Multi-Modal Architectures
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 多模态架构
- en: 'In sections [2.1](#S2.SS1 "2.1 Architectures for Computer Vision ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey"), [2.2](#S2.SS2
    "2.2 Architectures for Natural Language Processing ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey"), and [2.3](#S2.SS3
    "2.3 Architectures for Reinforcement Learning ‣ 2 Multi-Task Architectures ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey"), we discussed the multi-task architectures
    which were specifically designed to handle data in one fixed domain. Here, we
    describe architectures to handle multiple tasks using data from multiple domains,
    which is usually some combination of visual and linguistic data. Multi-modal learning
    is an interesting extension of many of the motivating principles behind multi-task
    learning: sharing representations across domains decreases overfitting and increases
    data efficiency. In the multi-task single modality case, the representations are
    shared across tasks but within in a single modality. However, in the multi-task
    multi-modal case, representations are shared across tasks and across modes, providing
    another layer of abstraction through which the learned representations must generalize.
    This suggests that multi-task multi-modal learning may yield an increase in the
    benefits already exhibited by multi-task learning.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在章节 [2.1](#S2.SS1 "2.1 计算机视觉的架构 ‣ 2 多任务架构 ‣ 深度神经网络的多任务学习：综述")、[2.2](#S2.SS2
    "2.2 自然语言处理的架构 ‣ 2 多任务架构 ‣ 深度神经网络的多任务学习：综述") 和 [2.3](#S2.SS3 "2.3 强化学习的架构 ‣ 2
    多任务架构 ‣ 深度神经网络的多任务学习：综述") 中，我们讨论了专门设计来处理一个固定领域中数据的多任务架构。在这里，我们描述了处理来自多个领域的数据以应对多个任务的架构，这通常是视觉和语言数据的某种组合。多模态学习是多任务学习背后许多激励原则的有趣扩展：跨领域共享表示减少了过拟合并提高了数据效率。在多任务单模态情况下，表示在任务间共享，但仅在单一模态内。然而，在多任务多模态情况下，表示在任务和模式之间共享，通过这种方式，学习到的表示必须进行泛化。这表明多任务多模态学习可能会提高多任务学习已经表现出的好处。
- en: 'Nguyen and Okatani [[2019](#bib.bib117)] introduces an architecture for shared
    vision and language tasks by using dense co-attention layers Nguyen and Okatani
    [[2018](#bib.bib116)], in which tasks are organized into a hierarchy and low-level
    tasks are supervised at earlier layers in the network. Dense co-attention layers
    were developed for visual question answering, specifically for the integration
    of visual and linguistic information. This setup of task supervision is similar
    to the cascaded information architectures discussed in section [2.2.3](#S2.SS2.SSS3
    "2.2.3 Cascaded Information ‣ 2.2 Architectures for Natural Language Processing
    ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey"). However, instead of hand-designing a hierarchy of tasks, this method
    performs a search over the layers for each task in order to learn the task hierarchy.
    The architecture of Akhtar et al. [[2019](#bib.bib3)] handles visual, audio, and
    text input to classify emotion and sentiment in a video of a human speaker, using
    bi-directional GRU layers along with pairwise attention mechanisms for each pair
    of modes to learn a shared representation incorporating all modes of input.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '阮和冈谷 [[2019](#bib.bib117)] 通过使用密集共注意力层来引入用于共享视觉和语言任务的架构，阮和冈谷 [[2018](#bib.bib116)]
    中的任务被组织成一个层次结构，并且低级任务在网络的早期层中受到监督。密集共注意力层是为视觉问答开发的，特别是用于集成视觉和语言信息。这种任务监督设置类似于第
    [2.2.3](#S2.SS2.SSS3 "2.2.3 Cascaded Information ‣ 2.2 Architectures for Natural
    Language Processing ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep
    Neural Networks: A Survey") 节中讨论的级联信息架构。然而，与其手动设计任务层次结构不同，该方法在每个任务上进行层级搜索，以学习任务层次结构。阿赫塔尔等人
    [[2019](#bib.bib3)] 的架构处理视觉、音频和文本输入，以分类视频中的情感和情绪，使用双向GRU层以及每对模态的成对注意力机制来学习包含所有输入模态的共享表示。'
- en: 'Both Nguyen and Okatani [[2019](#bib.bib117)], Akhtar et al. [[2019](#bib.bib3)]
    are focused on a set of tasks which all share the same fixed set of modalities.
    Instead, Kaiser et al. [[2017](#bib.bib71)] and Pramanik et al. [[2019](#bib.bib124)]
    focus on building a “universal multi-modal multi-task model", in which a single
    model can handle multiple tasks with varying input domains. The architecture introduced
    in Kaiser et al. [[2017](#bib.bib71)] is comprised of an input encoder, an I/O
    mixer, and an autoregressive decoder. Each of these three blocks is made of a
    mix of convolutions, attention layers, and sparsely-gated mixture-of-experts layers.
    The authors also demonstrate that the large degree of sharing between tasks yields
    significantly increased performance for tasks with limited training data. Instead
    of aggregating mechanisms from various modes of deep learning, Pramanik et al.
    [[2019](#bib.bib124)] introduces an architecture called OmniNet with a spatio-temporal
    cache mechanism to learn dependencies across spatial dimensions of data as well
    as the temporal dimension. A diagram is shown in figure [12](#S2.F12 "Figure 12
    ‣ 2.4 Multi-Modal Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey"). Each input modality has a corresponding
    “peripheral" network, and the outputs of these networks are aggregated and fed
    into the Central Neural Processor, whose output is fed to task-specific output
    heads. The CNP has an encoder-decoder architecture with a spatial cache and a
    temporal cache. OmniNet reached SOTA-competitive performance on POS tagging, image
    captioning, visual question answering, and video activity recognition.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '阮和冈谷 [[2019](#bib.bib117)]、阿赫塔尔等人 [[2019](#bib.bib3)] 都关注于一组具有相同固定模态的任务。而凯瑟尔等人
    [[2017](#bib.bib71)] 和普拉马尼克等人 [[2019](#bib.bib124)] 则专注于构建一个“通用多模态多任务模型”，在该模型中，单一模型可以处理多个具有不同输入领域的任务。凯瑟尔等人
    [[2017](#bib.bib71)] 引入的架构包含一个输入编码器、一个I/O混合器和一个自回归解码器。这三个模块都是由卷积、注意力层和稀疏门控专家混合层的组合构成的。作者还展示了任务之间的大量共享能显著提升具有有限训练数据的任务的性能。普拉马尼克等人
    [[2019](#bib.bib124)] 介绍了一个名为OmniNet的架构，它具有一个时空缓存机制，用于学习数据空间维度及时间维度的依赖关系。图 [12](#S2.F12
    "Figure 12 ‣ 2.4 Multi-Modal Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey") 中展示了一个示意图。每个输入模态都有一个对应的“外围”网络，这些网络的输出被汇总并输入到中央神经处理器，中央神经处理器的输出再输入到任务特定的输出头。CNP
    具有一个带有空间缓存和时间缓存的编码器-解码器架构。OmniNet 在词性标注、图像描述、视觉问答和视频活动识别中达到了SOTA竞争性能。'
- en: '![Refer to caption](img/947207e85bb9673f8d0b01a8161d0964.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/947207e85bb9673f8d0b01a8161d0964.png)'
- en: 'Figure 12: OmniNet architecture proposed in Pramanik et al. [[2019](#bib.bib124)].
    Each modality has a separate network to handle inputs, and the aggregated outputs
    are processed by an encoder-decoder called the Central Neural Processor. The output
    of the CNP is then passed to several task-specific output heads.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：Pramanik等人提出的OmniNet架构[[2019](#bib.bib124)]。每种模态都有一个独立的网络来处理输入，汇总的输出由一个名为中央神经处理器的编码器-解码器处理。中央神经处理器的输出然后传递到几个特定任务的输出头。
- en: Most recently, Lu et al. [[2020](#bib.bib102)] introduces a multi-task model
    that handles 12 different datasets simultaneously, aptly named 12-in-1\. Their
    model achieves superior performance over the corresponding single-task models
    on 11 out of 12 of these tasks, and using multi-task training as a pre-training
    step leads to SOTA performance on 7 of these tasks. The architecture is based
    on the ViLBERT model Lu et al. [[2020](#bib.bib102)], and is trained using a mix
    of methods such as dynamic task scheduling, curriculum learning, and hyperparameter
    heuristics.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Lu等人[[2020](#bib.bib102)]提出了一个处理12个不同数据集的多任务模型，恰如其名为12-in-1。该模型在这12个任务中的11个任务上表现优于相应的单任务模型，并且将多任务训练作为预训练步骤在7个任务上实现了SOTA性能。该架构基于Lu等人[[2020](#bib.bib102)]的ViLBERT模型，训练使用了动态任务调度、课程学习和超参数启发式等多种方法。
- en: 2.5 Learned Architectures
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 学习到的架构
- en: 'As we have already seen in the preceding sections, there have been many developments
    in the design of shared architectures to emphasize the strengths of multi-task
    learning while mitigating the weaknesses. Another approach to architecture design
    for multi-task learning is to learn the architecture as well as the weights of
    the resulting model. Many of the following methods for learning shared architectures
    allow for the model to learn how parameters should be shared between tasks. With
    a varying parameter sharing scheme, the model can shift the overlap between tasks
    in such a way that similar tasks have a higher degree of sharing than unrelated
    tasks. This is one potential method for mitigating negative transfer between tasks:
    if two tasks exhibit negative transfer, the model may learn to keep the parameters
    for those tasks separate. Going further, it may be the case that two tasks exhibit
    positive transfer in some parts of the network, and negative transfer in others.
    In this case, designing a parameter sharing scheme by hand to accommodate various
    task similarites at different parts of the network becomes infeasible, especially
    as the number of tasks and the size of the network grows. Learned parameter sharing
    offers a way to facilitate adaptive sharing between tasks to a level of precision
    that isn’t realistic for hand designed shared architectures.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中已经看到的，已经有很多共享架构设计的发展，以突出多任务学习的优点，同时缓解其缺点。另一种多任务学习的架构设计方法是学习架构以及生成模型的权重。许多以下学习共享架构的方法允许模型学习参数在任务之间应如何共享。通过不同的参数共享方案，模型可以调整任务之间的重叠，以便类似的任务之间的共享程度高于无关的任务。这是一种缓解任务之间负迁移的潜在方法：如果两个任务表现出负迁移，模型可能会学习将这些任务的参数分开。进一步来说，可能存在两个任务在网络的某些部分表现出正迁移，在其他部分则表现出负迁移。在这种情况下，手动设计一个参数共享方案以适应网络不同部分的任务相似性变得不可行，特别是随着任务数量和网络规模的增长。学习到的参数共享提供了一种促进任务之间自适应共享的方式，这种精确度是手动设计的共享架构所无法实现的。
- en: 'We roughly categorize the methods for learned architectures into four groups:
    architecture search, branched sharing, modular sharing, and fine-grained sharing.
    The boundaries between these groups aren’t concrete, and they are often blurred,
    but we believe this is a useful way to broadly characterize the patterns in the
    recently developed methods. Branched sharing methods are a coarse-grained way
    to share parameters between tasks. Once the computation graphs for two tasks differ,
    they never rejoin (see figure [13](#S2.F13 "Figure 13 ‣ 2.5.2 Branched Sharing
    ‣ 2.5 Learned Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey")). Modular sharing represents a more fine-grained
    approach, in which a set of neural network modules is shared between tasks, where
    the architecture for each task is made by a task-specific combination of some
    or all of the modules, as in figure [16](#S2.F16 "Figure 16 ‣ 2.5.3 Modular Sharing
    ‣ 2.5 Learned Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey"). Lastly, the most fine-grained approach
    to parameter sharing is what we simply call fine-grained sharing, in which sharing
    decisions occur at the parameter level instead of the layer level, as shown in
    figure [17](#S2.F17 "Figure 17 ‣ 2.5.4 Fine-Grained Sharing ‣ 2.5 Learned Architectures
    ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey").'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '我们大致将学习到的架构方法分为四组：架构搜索、分支共享、模块共享和细粒度共享。这些组之间的界限并不明确，通常是模糊的，但我们认为这种分类方法对于广泛描述最近开发的方法中的模式是有用的。分支共享方法是一种粗粒度的任务间参数共享方式。一旦两个任务的计算图不同，它们就不会重新合并（参见图
    [13](#S2.F13 "Figure 13 ‣ 2.5.2 Branched Sharing ‣ 2.5 Learned Architectures ‣
    2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks: A
    Survey")）。模块共享代表了一种更细粒度的方法，其中一组神经网络模块在任务间共享，每个任务的架构是通过对一些或所有模块的任务特定组合来构建的，如图 [16](#S2.F16
    "Figure 16 ‣ 2.5.3 Modular Sharing ‣ 2.5 Learned Architectures ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey") 所示。最后，最细粒度的参数共享方法是我们简单称之为细粒度共享，其中共享决策发生在参数级别，而不是层级别，如图
    [17](#S2.F17 "Figure 17 ‣ 2.5.4 Fine-Grained Sharing ‣ 2.5 Learned Architectures
    ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey") 所示。'
- en: 2.5.1 Architecture Search
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.1 架构搜索
- en: Each of Wong and Gesmundo [[2017](#bib.bib156)], Liang et al. [[2018](#bib.bib85)],
    Gao et al. [[2020](#bib.bib54)] introduces a method for multi-task architecture
    search, but with completely different approaches. Wong and Gesmundo [[2017](#bib.bib156)]
    introduces the Multi-task Neural Model Search (MNMS) controller. This method doesn’t
    involve a single network which is shared between all tasks. Instead, the MNMS
    controller is trained simultaneously on all tasks to generate one individual architecture
    for each task. The method is an extension of Zoph and Le [[2016](#bib.bib176)],
    where an RNN controller iteratively makes architecture design choices, and is
    trained with reinforcement learning to maximize the expected performance of the
    resulting network. In the multi-task variant, the RNN also uses task embeddings,
    which are learned jointly with the MNMS controller, to condition architectural
    design choices on the nature of the task.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Wong 和 Gesmundo [[2017](#bib.bib156)]、Liang 等 [[2018](#bib.bib85)]、Gao 等 [[2020](#bib.bib54)]
    各自介绍了一种多任务架构搜索的方法，但采用了完全不同的策略。Wong 和 Gesmundo [[2017](#bib.bib156)] 介绍了多任务神经模型搜索（MNMS）控制器。该方法不涉及一个在所有任务之间共享的单一网络。相反，MNMS
    控制器在所有任务上同时训练，为每个任务生成一个独立的架构。该方法是 Zoph 和 Le [[2016](#bib.bib176)] 的扩展，其中 RNN 控制器迭代地进行架构设计选择，并通过强化学习进行训练，以最大化最终网络的预期性能。在多任务变体中，RNN
    还使用任务嵌入，这些嵌入与 MNMS 控制器共同学习，以便根据任务的性质对架构设计选择进行条件设置。
- en: 'On the other hand, Liang et al. [[2018](#bib.bib85)] introduces several variations
    of a multi-task neural architecture search algorithm that uses evolutionary strategies
    to learn neural network modules which can be reordered differently for various
    tasks. This method is an extension of the Soft Layer Ordering introduced in Meyerson
    and Miikkulainen [[2017](#bib.bib110)] (discussed in section [2.3.2](#S2.SS3.SSS2
    "2.3.2 Modular Policies ‣ 2.3 Architectures for Reinforcement Learning ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey")). Just
    as in Meyerson and Miikkulainen [[2017](#bib.bib110)], the method of Liang et al.
    [[2018](#bib.bib85)] involves learning neural network modules jointly with their
    ordering for various tasks. In the architecture search extension, the architecture
    of the modules is learned along with their routing for individual tasks. The most
    sophisticated variant of this algorithm is called Coevolution of Modules and Task
    Routing (CMTR), in which the CoDeepNEAT algorithm Miikkulainen et al. [[2019](#bib.bib112)]
    is used to evolve the architecture of the shared group modules in an outer loop,
    and the task specific routings of these modules are evolved in an inner loop.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，Liang 等人 [[2018](#bib.bib85)] 引入了几种多任务神经架构搜索算法的变体，这些算法使用进化策略来学习可以为不同任务重新排序的神经网络模块。这种方法是
    Meyerson 和 Miikkulainen [[2017](#bib.bib110)] 中引入的软层排序的扩展（讨论见[2.3.2](#S2.SS3.SSS2
    "2.3.2 Modular Policies ‣ 2.3 Architectures for Reinforcement Learning ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey")）。就像在
    Meyerson 和 Miikkulainen [[2017](#bib.bib110)] 中一样，Liang 等人 [[2018](#bib.bib85)]
    的方法涉及到针对不同任务的神经网络模块及其排序的联合学习。在架构搜索扩展中，模块的架构以及其针对各个任务的路由被一同学习。该算法最复杂的变体被称为模块与任务路由的共同进化（CMTR），在其中，CoDeepNEAT
    算法 Miikkulainen 等人 [[2019](#bib.bib112)] 被用来在外循环中进化共享组模块的架构，而这些模块的任务特定路由在内循环中进化。'
- en: 'Most recently, Gao et al. [[2020](#bib.bib54)] proposes MTL-NAS as a method
    for gradient-based architecture search in MTL. All architectures in this search
    space are made of a set of fixed-architecture single-task backbone networks, one
    for each task, and the search process operates over feature fusion operations
    between different layers of these single-task networks. The feature fusion operations
    are parameterized by NDDR (from NDDR-CNN Gao et al. [[2019](#bib.bib53)], see
    section [2.1.2](#S2.SS1.SSS2 "2.1.2 Cross-Talk ‣ 2.1 Architectures for Computer
    Vision ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey")), which is essentially a $1\times 1$ convolution acting on concatenations
    of feature maps from different tasks. This method also introduces a minimum entropy
    objective on the weights of the fusion operations, so that the search process
    converges to a discrete architecture during the architecture search phase, which
    diminishes the need for a discretization of a soft combination of architectures
    as in other NAS works Liu et al. [[2018](#bib.bib89)] and closes the performance
    gap between learned soft architectures and the final discretized version. The
    final learned architectures were shown to outperform common multi-task baselines
    on the NYU-v2 Silberman et al. [[2012](#bib.bib137)] and Taskonomy Zamir et al.
    [[2018](#bib.bib166)] datasets.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，Gao 等人 [[2020](#bib.bib54)] 提出了 MTL-NAS 作为一种在 MTL 中进行基于梯度的架构搜索的方法。该搜索空间中的所有架构都由一组固定架构的单任务骨干网络组成，每个任务一个，这个搜索过程在这些单任务网络的不同层之间的特征融合操作上进行。特征融合操作由
    NDDR 参数化（来自 NDDR-CNN Gao 等人 [[2019](#bib.bib53)]，见[2.1.2](#S2.SS1.SSS2 "2.1.2
    Cross-Talk ‣ 2.1 Architectures for Computer Vision ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey")），本质上是作用于来自不同任务的特征图连接的
    $1\times 1$ 卷积。这种方法还在融合操作的权重上引入了最小熵目标，从而使得搜索过程在架构搜索阶段收敛到离散架构，减少了与其他 NAS 工作中软组合架构离散化的需求
    Liu 等人 [[2018](#bib.bib89)]，并缩小了学习到的软架构与最终离散版本之间的性能差距。最终学习到的架构在 NYU-v2 Silberman
    等人 [[2012](#bib.bib137)] 和 Taskonomy Zamir 等人 [[2018](#bib.bib166)] 数据集上表现优于常见的多任务基准。'
- en: 2.5.2 Branched Sharing
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.2 分支共享
- en: 'Lu et al. [[2017](#bib.bib103)] is one of the earliest methods for learned
    parameter sharing in multi-task deep learning. The idea is to start with a network
    which is shared between all tasks up to task-specific output heads, then iteratively
    decouple parameters between tasks layer by layer, starting with the layer closest
    to the output heads, and moving to the earlier layers. A diagram of this process
    is shown in figure [13](#S2.F13 "Figure 13 ‣ 2.5.2 Branched Sharing ‣ 2.5 Learned
    Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural
    Networks: A Survey"). When a shared layer splits into multiple task specific layers,
    tasks are clustered based on an estimate of pairwise task affinity. These task
    affinities are computed according to the following principle: two tasks are likely
    related if the same input data is equally easy/difficult for the models corresponding
    to each task.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Lu 等人 [[2017](#bib.bib103)] 是多任务深度学习中最早的学习参数共享方法之一。其思路是从一个共享网络开始，该网络在所有任务中共享，直到任务特定的输出头，然后逐层迭代地解耦任务间的参数，从最接近输出头的层开始，逐步向早期层移动。这个过程的示意图如图
    [13](#S2.F13 "图 13 ‣ 2.5.2 分支共享 ‣ 2.5 学习架构 ‣ 2 多任务架构 ‣ 基于深度神经网络的多任务学习：综述") 所示。当一个共享层分裂成多个任务特定层时，任务根据任务对的相似度进行聚类。这些任务相似度是根据以下原则计算的：如果相同的输入数据对每个任务的模型同样容易/困难，那么这两个任务很可能相关。
- en: '![Refer to caption](img/87203ba212f7586ae96c6b00846a4af7.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/87203ba212f7586ae96c6b00846a4af7.png)'
- en: 'Figure 13: Learned branching architecture proposed in Lu et al. [[2017](#bib.bib103)].
    At the beginning of training, each task shares all layers of the network. As training
    goes on, less related tasks branch into clusters, so that only highly related
    tasks share as many parameters.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：Lu 等人 [[2017](#bib.bib103)] 提出的学习分支架构。在训练开始时，每个任务共享网络的所有层。随着训练的进行，相关性较低的任务会分支成集群，以便只有高度相关的任务共享尽可能多的参数。
- en: 'More recently, Vandenhende et al. [[2019](#bib.bib151)] proposes a similar
    method with a different criterion for task grouping. Instead of concurrent sample
    difficulty, this algorithm uses representation similarity analysis (RSA) Kriegeskorte
    [[2008](#bib.bib76)] as a measure of task affinity. RSA is built on the principle
    that similar tasks will rely on similar features of the input, and will therefore
    learn similar feature representations. The other important difference between
    these methods is that Vandenhende et al. [[2019](#bib.bib151)] computes the branching
    structure globally instead of greedily by layer. However, the search over all
    branching structures is computationally expensive, so the authors resort to a
    beam search strategy for computing the branching structure from the representation
    similarities across tasks in different parts of the network. This paper includes
    a direct comparison of the two methods, and the RSA-based variant is shown to
    be superior. RSA is also used in some methods to learn explicit task relationships,
    which are discussed in section [4](#S4 "4 Task Relationship Learning ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey").'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Vandenhende 等人 [[2019](#bib.bib151)] 提出了一个类似的方法，但任务分组的标准不同。该算法使用表示相似性分析（RSA）Kriegeskorte
    [[2008](#bib.bib76)] 作为任务亲和性的衡量标准，而不是并发样本难度。RSA 的基础原则是相似的任务会依赖于输入的相似特征，因此会学习到相似的特征表示。这些方法的另一个重要区别是，Vandenhende
    等人 [[2019](#bib.bib151)] 计算分支结构时是全局的，而不是逐层贪婪的。然而，对所有分支结构的搜索计算成本很高，因此作者采用了束搜索策略，根据网络中不同部分任务之间的表示相似性来计算分支结构。本文还直接比较了这两种方法，结果表明基于
    RSA 的变体更优。RSA 还用于一些方法中以学习显式任务关系，这些方法在章节 [4](#S4 "4 任务关系学习 ‣ 基于深度神经网络的多任务学习：综述")
    中讨论。
- en: 2.5.3 Modular Sharing
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.3 模块化共享
- en: 'The earliest work on modular parameter sharing in multi-task learning that
    we are aware of is PathNet Fernando et al. [[2017](#bib.bib51)]. A PathNet model
    is one large neural network which is used for multiple tasks, though different
    tasks have different computation pathways within the larger model. A diagram is
    shown in figure [14](#S2.F14 "Figure 14 ‣ 2.5.3 Modular Sharing ‣ 2.5 Learned
    Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural
    Networks: A Survey"). The pathway for each task is learned through a tournament
    selection genetic algorithm, in which many different candidate pathways compete
    and evolve towards an optimal subnetwork of the larger network. While this idea
    is mostly general and can be applied to various settings, such as multi-task learning
    and meta-learning, the authors deploy this model for continual learning with two
    reinforcement learning tasks. The weights learned during training on the first
    task are fixed during training of the second task, during which new pathways through
    the network are evolved to complete the task at hand.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '我们所知的最早的多任务学习中的模块化参数共享工作是PathNet Fernando 等人 [[2017](#bib.bib51)]。PathNet模型是一个大型神经网络，用于多个任务，尽管不同任务在大型模型中有不同的计算路径。图中显示了[14](#S2.F14
    "Figure 14 ‣ 2.5.3 Modular Sharing ‣ 2.5 Learned Architectures ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey")。每个任务的路径通过锦标赛选择遗传算法学习，在此过程中，许多不同的候选路径竞争并进化为大型网络的最佳子网络。虽然这个概念大多是通用的，可以应用于各种环境，如多任务学习和元学习，但作者将此模型用于包含两个强化学习任务的持续学习。在第二个任务的训练过程中，第一次任务训练期间学到的权重保持不变，并在此期间进化出新的网络路径以完成当前任务。'
- en: '![Refer to caption](img/2d8a0b5971be34a3eb5acd2aecd594c4.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2d8a0b5971be34a3eb5acd2aecd594c4.png)'
- en: 'Figure 14: Example PathNet architecture Fernando et al. [[2017](#bib.bib51)].
    A large network is shared by many tasks, but each task only uses a subnetwork
    which is evolved through a tournament selection genetic algorithm.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：示例PathNet架构，Fernando 等人 [[2017](#bib.bib51)]。一个大型网络被多个任务共享，但每个任务仅使用一个子网络，该子网络通过锦标赛选择遗传算法进化。
- en: 'Soft Layer Ordering Meyerson and Miikkulainen [[2017](#bib.bib110)] and Modular
    Meta-Learning Alet et al. [[2018](#bib.bib5)] are two concurrent works of modular
    MTL, similar but with an important difference. Each of these methods learns a
    shared set of neural network modules which are combined in different ways for
    different tasks, with the hope that a network “building block" will learn generally
    applicable knowledge if it is used in various contexts within the different task
    networks. Soft Layer Ordering parameterizes a task network by computing a convex
    combination of each module’s output at each layer of the network, as shown in
    figure [15](#S2.F15 "Figure 15 ‣ 2.5.3 Modular Sharing ‣ 2.5 Learned Architectures
    ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey"). With this parameterization, each learned module can contribute to
    each level of depth in the network. In contrast, Modular Meta-Learning learns
    a computation graph over the modules, meaning that each step of the computation
    is a discrete composition of a small number of modules, instead of a soft combination
    of all of them. The difference in the parameterization of the computation graph
    between these methods leads to different optimization strategies, namely, the
    computation graph in Soft Layer Ordering architectures can be optimized with gradient
    descent jointly with the network weights, since the composition of the modules
    is a differentiable operation. In comparison, the computation graph in Modular
    Meta-Learning is a discrete structure, so gradient-based optimization methods
    cannot be used to learn the graph over modules for each task. Instead, the authors
    employ simulated annealing, a black box optimization method, to learn the computation
    graph. While this two-level optimization incurs computational cost, the discrete
    nature of the computation graph affords the ability to produce an inductive bias
    in the resulting model, which the soft sharing of layers does not exhibit. These
    methods represent two realizations of a broadly generalizable template that many
    other methods have employed: Learn individual network pieces, and learn how to
    combine them.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 'Soft Layer Ordering Meyerson 和 Miikkulainen [[2017](#bib.bib110)] 和 Modular
    Meta-Learning Alet 等人 [[2018](#bib.bib5)] 是两项并行的模块化 MTL 工作，虽然相似但有一个重要的区别。这些方法中的每一种都学习一组共享的神经网络模块，这些模块以不同的方式组合用于不同的任务，期望“构建块”网络能够在不同任务网络的各种上下文中学习到普遍适用的知识。Soft
    Layer Ordering 通过计算每个模块在网络每一层输出的凸组合来参数化任务网络，如图 [15](#S2.F15 "Figure 15 ‣ 2.5.3
    Modular Sharing ‣ 2.5 Learned Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey") 所示。通过这种参数化，每个学习到的模块可以对网络的每一层深度做出贡献。相比之下，Modular
    Meta-Learning 在模块上学习一个计算图，这意味着每一步计算是少量模块的离散组合，而不是所有模块的软组合。这些方法在计算图的参数化上的差异导致了不同的优化策略，即
    Soft Layer Ordering 架构中的计算图可以与网络权重一起通过梯度下降进行优化，因为模块的组合是一个可微分操作。相比之下，Modular Meta-Learning
    中的计算图是一个离散结构，因此不能使用基于梯度的优化方法来学习每个任务的模块图。相反，作者使用模拟退火，一种黑箱优化方法，来学习计算图。虽然这种两级优化会增加计算成本，但计算图的离散性质使得结果模型能够产生归纳偏置，而层的软共享则没有表现出这种特性。这些方法代表了许多其他方法采用的广泛可推广模板的两种实现：学习单独的网络片段，并学习如何将它们组合起来。'
- en: '![Refer to caption](img/95c269f7b4d8e2f2c38b6a74e8d2ab19.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/95c269f7b4d8e2f2c38b6a74e8d2ab19.png)'
- en: 'Figure 15: Soft Layer Ordering with three learned layers Meyerson and Miikkulainen
    [[2017](#bib.bib110)]. Each layer of the network is a linear combination of several
    network modules, and the weights of these combinations are task-specific.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：Soft Layer Ordering 使用三层学习层 Meyerson 和 Miikkulainen [[2017](#bib.bib110)]。网络的每一层是几个网络模块的线性组合，这些组合的权重是特定于任务的。
- en: The method of Chen et al. [[2018](#bib.bib28)] is another strategy in this spirit,
    and closely resembles NAS Zoph and Le [[2016](#bib.bib176)]. This paper proposes
    a method that fits the template described above, but the composition of modules
    is not directly parameterized and learned such as in Soft Layer Ordering and Modular
    Meta-Learning. Instead, this method trains an RNN controller to choose a layer
    from a fixed set of layers to iteratively build an architecture as a sequence
    of modules, and the module is again trained with reinforcement learning to maximize
    the expected performance of the constructed architecture. This method bears a
    strong resemblance to the previously discussed Multi-task Neural Model Search
    controller Wong and Gesmundo [[2017](#bib.bib156)], with the main difference being
    that the RNN controller used in Chen et al. [[2018](#bib.bib28)] simply chooses
    between a set of network modules, while the MNMS controller makes architectural
    design decisions.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人[[2018](#bib.bib28)]的方法是这一理念的另一种策略，与NAS Zoph和Le [[2016](#bib.bib176)]非常相似。本文提出了一种适应上述模板的方法，但模块的组成并不像软层排序和模块化元学习那样直接参数化和学习。相反，这种方法训练一个RNN控制器，从一个固定的层集合中选择一个层，逐步构建一个由模块组成的架构，然后通过强化学习再次训练模块，以最大化构建架构的预期性能。这种方法与之前讨论的多任务神经模型搜索控制器Wong和Gesmundo
    [[2017](#bib.bib156)]有很强的相似性，主要区别在于陈等人[[2018](#bib.bib28)]使用的RNN控制器仅在一组网络模块之间进行选择，而MNMS控制器则做出架构设计决策。
- en: 'Most recently, AdaShare Sun et al. [[2019b](#bib.bib147)] is an algorithm for
    modular MTL in which each task architecture is comprised of a sequence of network
    layers. Each layer in the shared set is either included or omitted from the network
    for each task. An example is shown in figure [16](#S2.F16 "Figure 16 ‣ 2.5.3 Modular
    Sharing ‣ 2.5 Learned Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey"). Along with the weights of each
    layer, AdaShare learns an $N\times L$ array of binary values, where $N$ is the
    number of tasks, $L$ is the total number of shared layers, and the $(i,\ell)$-th
    element of the binary array denotes whether layer $\ell$ is included in the model
    of the $i$-th task. Since the output of a task’s network is not differentiable
    with respect to these binary values, the method adopts Gumbel-Softmax sampling
    Jang et al. [[2016](#bib.bib69)] to optimize these parameters with gradient descent
    jointly with the network weights. This strategy makes an interesting medium between
    Soft Layer Ordering Meyerson and Miikkulainen [[2017](#bib.bib110)] and Modular
    Meta-Learning Alet et al. [[2018](#bib.bib5)], in which each shared module is
    shared discretely instead of softly, but the computation graph can still be learned
    with gradient descent. AdaShare also employs several regularization terms to encourage
    sharing in the lower-level modules and sparsity in the resulting task-specific
    networks, which are discussed in section [3.2](#S3.SS2 "3.2 Regularization ‣ 3
    Optimization for Multi-Task Learning ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey").'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，AdaShare Sun等人[[2019b](#bib.bib147)]是一种模块化多任务学习的算法，其中每个任务架构由一系列网络层组成。共享集合中的每一层要么被包含在网络中，要么在每个任务中被省略。图[16](#S2.F16
    "图 16 ‣ 2.5.3 模块化共享 ‣ 2.5 学习的架构 ‣ 2 多任务架构 ‣ 基于深度神经网络的多任务学习：综述")展示了一个示例。除了每层的权重，AdaShare学习一个$N\times
    L$的二进制值数组，其中$N$是任务数量，$L$是共享层的总数，数组的$(i,\ell)$-th元素表示第$i$个任务的模型中是否包含层$\ell$。由于任务网络的输出对这些二进制值不可微分，该方法采用Gumbel-Softmax采样Jang等人[[2016](#bib.bib69)]与网络权重一起使用梯度下降优化这些参数。这种策略在软层排序Meyerson和Miikkulainen
    [[2017](#bib.bib110)]和模块化元学习Alet等人[[2018](#bib.bib5)]之间形成了一种有趣的中间状态，其中每个共享模块是离散共享的，而不是柔性共享，但计算图仍然可以通过梯度下降进行学习。AdaShare还采用了几个正则化项，以鼓励在较低级模块中的共享和在结果任务特定网络中的稀疏性，这些在[3.2](#S3.SS2
    "3.2 正则化 ‣ 3 多任务学习的优化 ‣ 基于深度神经网络的多任务学习：综述")节中讨论。
- en: '![Refer to caption](img/707af06e2ff52a68515e7d6705c3fe1b.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/707af06e2ff52a68515e7d6705c3fe1b.png)'
- en: 'Figure 16: A learned parameter sharing scheme with AdaShare Sun et al. [[2019b](#bib.bib147)].
    Each layer in the network is either included or ignored by each task, so that
    each task uses a subnetwork which is (likely) overlapping with other tasks.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：AdaShare Sun等人[[2019b](#bib.bib147)]的学习参数共享方案。网络中的每一层要么被每个任务包含，要么被忽略，从而使每个任务使用的子网络（可能）与其他任务重叠。
- en: 2.5.4 Fine-Grained Sharing
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.4 细粒度共享
- en: Fine-grained parameter sharing schemes are the most recently introduced MTL
    architecture type, and they allow for more flexible information flow between tasks
    than sharing at the layer or multi-layer level. Piggyback Mallya et al. [[2018](#bib.bib106)]
    is a method for adapting a pre-trained network on a related task by learning to
    mask out individual weights of the original network. This allows for the storage
    of a newly trained model with a storage cost of only one additional bit per parameter
    of the original model while preserving the original network function. Despite
    the fact that the network output is not differentiable with respect to these network
    masks, these network masks are optimized through gradient descent jointly with
    the network weights by using a continuous relaxation of the mask values as a noisy
    estimate of the binary mask values. This method of optimizing such mask values
    is justified in prior work on binarized neural networks Courbariaux et al. [[2015](#bib.bib36)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度参数共享方案是最近引入的 MTL 架构类型，它们允许比层级或多层级共享更灵活的信息流。Piggyback Mallya 等人 [[2018](#bib.bib106)]
    是一种通过学习掩蔽原始网络的单个权重来调整预训练网络以适应相关任务的方法。这允许以仅增加原始模型每个参数一个附加位的存储成本来存储新训练的模型，同时保持原始网络的功能。尽管网络输出对这些网络掩膜不是可微的，但这些网络掩膜通过梯度下降与网络权重一起优化，使用掩膜值的连续松弛作为二值掩膜值的噪声估计。优化这种掩膜值的方法在之前的二值神经网络研究中得到了证明
    Courbariaux 等人 [[2015](#bib.bib36)]。
- en: Newell et al. [[2019](#bib.bib115)] and Bragman et al. [[2019](#bib.bib22)]
    are concurrent works that each propose a parameter sharing scheme for multi-task
    CNNs in which sharing occurs at the filter level. For each convolutional layer
    of a multi-task network, the method of Newell et al. [[2019](#bib.bib115)] learns
    a binary valued $N\times C$ array $M$, where $N$ is again the number of tasks
    and $C$ is the number of feature channels in a given layer of the network. The
    $(i,c)$-th element of $M$ denotes whether the model of the $i$-th task should
    include the $c$-th feature map in the considered layer. Instead of optimizing
    this binary valued array with a Gumbel-Softmax Jang et al. [[2016](#bib.bib69)]
    distribution, the authors do not learn these values directly. Rather, the method
    learns a real-valued matrix $P$ of size $N\times N$, with values in the range
    $[0,1]$, where the $(i,j)$-th element of $P$ represents the proportion of feature
    channels which are shared by both the models for task $i$ and task $j$. In this
    way, the relationships between tasks are learned directly, and an array $M$ which
    satisfies $P=\frac{1}{C}M^{T}M$ is sampled after each new value of $P$ is computed.
    With this parameterization of $M$, the network architecture isn’t directly learned,
    but is instead sampled so that the learned task affinity matrix dictates the amount
    of overlap between task parameters. This task affinity matrix, $P$, is learned
    through evoluationary strategies. Bragman et al. [[2019](#bib.bib22)] proposes
    Stochastic Filter Groups (SFGs), in which the assignment of a convolutional filter
    to task-specific or shared is learned through variational inference. More specifically,
    SFGs are trained by learning the posterior distribution over the possible assignment
    of convolutional filters to task-specific or shared roles. As far as we know,
    SFGs are the only probabilistic approach to multi-task architecture learning.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Newell 等人 [[2019](#bib.bib115)] 和 Bragman 等人 [[2019](#bib.bib22)] 是同时提出了一种多任务
    CNN 的参数共享方案，其中共享发生在滤波器层面。对于多任务网络的每个卷积层，Newell 等人 [[2019](#bib.bib115)] 的方法学习一个二值的
    $N\times C$ 数组 $M$，其中 $N$ 再次表示任务的数量，$C$ 是网络中给定层的特征通道数量。$M$ 的 $(i,c)$-th 元素表示第
    $i$ 个任务的模型是否应该包含第 $c$ 个特征图。作者并未直接通过 Gumbel-Softmax Jang 等人 [[2016](#bib.bib69)]
    分布优化这个二值数组，而是学习一个大小为 $N\times N$ 的实值矩阵 $P$，其值范围在 $[0,1]$ 之间，其中 $P$ 的 $(i,j)$-th
    元素表示任务 $i$ 和任务 $j$ 的模型共享的特征通道的比例。通过这种方式，任务之间的关系被直接学习，并且在计算每个新值的 $P$ 之后，样本中得到一个满足
    $P=\frac{1}{C}M^{T}M$ 的数组 $M$。通过这种 $M$ 的参数化，网络结构不是直接学习的，而是通过采样使得学习到的任务亲和矩阵决定任务参数之间的重叠量。这个任务亲和矩阵
    $P$ 是通过进化策略学习的。Bragman 等人 [[2019](#bib.bib22)] 提出了随机滤波器组（SFGs），其中卷积滤波器分配到任务特定或共享的方式是通过变分推断学习的。更具体地说，SFGs
    通过学习卷积滤波器到任务特定或共享角色的可能分配的后验分布进行训练。据我们所知，SFGs 是唯一的概率性多任务架构学习方法。
- en: 'Sun et al. [[2019a](#bib.bib146)] introduces an algorithm for learning a fine-grained
    parameter sharing scheme by extracting sparse subnetworks of a single fully shared
    model. From a randomly initialized, overparameterized network, the authors employ
    Iterative Magnitude Pruning (IMP) Frankle and Carbin [[2018](#bib.bib52)] to extract
    a sparse subnetwork from the larger network for each individual task. IMP prunes
    a network by training for a small number of epochs, then removing the weights
    which have the smallest magnitude until a desired level of sparsity is reached.
    Given a reasonable level of sparsity, the extracted subnetworks for each task
    will overlap and exhibit fine-grained parameter sharing between tasks. A diagram
    is shown in figure [17](#S2.F17 "Figure 17 ‣ 2.5.4 Fine-Grained Sharing ‣ 2.5
    Learned Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with
    Deep Neural Networks: A Survey"). It is important to note that the degree of overlap
    between the extracted subnetworks of two tasks is not necessarily correlated with
    the relatedness of those two tasks, which suggests the need for a fine-grained
    parameter sharing scheme which incorporates information of task affinity to construct
    appropriate sharing mechanisms between tasks.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 'Sun 等人 [[2019a](#bib.bib146)] 介绍了一种通过提取单一完全共享模型的稀疏子网络来学习细粒度参数共享方案的算法。从一个随机初始化的过参数网络开始，作者使用
    Iterative Magnitude Pruning (IMP) Frankle 和 Carbin [[2018](#bib.bib52)] 从更大的网络中提取每个个别任务的稀疏子网络。IMP
    通过训练少量的训练周期来修剪网络，然后移除那些具有最小幅度的权重，直到达到所需的稀疏级别。考虑到合理的稀疏级别，每个任务的提取子网络将重叠并表现出任务之间的细粒度参数共享。图
    [17](#S2.F17 "Figure 17 ‣ 2.5.4 Fine-Grained Sharing ‣ 2.5 Learned Architectures
    ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey") 显示了这种情况。重要的是要注意，两个任务的提取子网络之间的重叠程度并不一定与这两个任务的相关性相关，这表明需要一个细粒度的参数共享方案，该方案结合了任务亲和力的信息以构建适当的任务共享机制。'
- en: '![Refer to caption](img/30d7d2800422bd29e5ed97e9efbdaa3d.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/30d7d2800422bd29e5ed97e9efbdaa3d.png)'
- en: 'Figure 17: Learned fine-grained sharing architecture from Sun et al. [[2019a](#bib.bib146)].
    Each task has a sparse subnetwork which may or may not overlap with that of other
    tasks. Each subnetwork is extracted using Iterative Magnitude Pruning Frankle
    and Carbin [[2018](#bib.bib52)] on the entire randomly initialized network before
    training.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：从 Sun 等人[[2019a](#bib.bib146)]的研究中学习到的细粒度共享架构。每个任务都有一个稀疏子网络，这些子网络可能会或可能不会与其他任务的子网络重叠。每个子网络都是使用
    Iterative Magnitude Pruning Frankle 和 Carbin [[2018](#bib.bib52)] 在整个随机初始化网络上提取的，然后再进行训练。
- en: 2.6 Conditional Architectures
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 条件架构
- en: Conditional or adaptive computation Bengio et al. [[2013](#bib.bib17)] is a
    method in which parts of a neural network architecture are selected for execution
    depending on the input to the network. Conditional computation is used in many
    areas outside of multi-task learning, such as to decrease model computational
    cost and in hierarchical reinforcement learning Kulkarni et al. [[2016](#bib.bib78)].
    In the multi-task case, a conditional architecture is dynamic between inputs as
    well as between tasks, though the components of these dynamically instantiated
    architectures are shared, which encourages these components to be generalizable
    between various inputs and tasks.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 条件或自适应计算 Bengio 等人 [[2013](#bib.bib17)] 是一种根据网络输入选择神经网络架构部分进行执行的方法。条件计算被应用于许多多任务学习以外的领域，如减少模型计算成本和在分层强化学习
    Kulkarni 等人 [[2016](#bib.bib78)] 中。在多任务情况下，条件架构在输入之间以及任务之间都是动态的，尽管这些动态实例化架构的组件是共享的，这鼓励这些组件在不同输入和任务之间具有可泛化性。
- en: 'Neural Module Networks Andreas et al. [[2016](#bib.bib7)] are an early work
    of conditional computation which were specifically designed for visual question
    answering. This method leverages the compositional structure of questions in natural
    language to train and deploy modules specifically catered for the individual parts
    of a question. The structure of a given question is determined by a non-neural
    semantic parser, specifically the Stanford Parser Klein and Manning [[2003](#bib.bib75)].
    The output of the parser is used to determine the compositional pieces of the
    question and the relationships between them, and the corresponding neural modules
    are used to dynamically instantiate a model for the given question. This process
    is shown in figure [18](#S2.F18 "Figure 18 ‣ 2.6 Conditional Architectures ‣ 2
    Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey").
    While this work paved the way for future methods of conditional computation, it
    is lakcing in the sense that the composition of modules is not learned. Therefore,
    the role of each module and combination of modules is fixed and cannot be improved.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '神经模块网络 Andreas 等人 [[2016](#bib.bib7)] 是条件计算的早期工作，专为视觉问答设计。该方法利用自然语言中问题的组合结构来训练和部署专门针对问题各个部分的模块。给定问题的结构由非神经语义解析器确定，特别是斯坦福解析器
    Klein 和 Manning [[2003](#bib.bib75)]。解析器的输出用于确定问题的组合部分及其之间的关系，并使用相应的神经模块动态实例化给定问题的模型。这个过程如图
    [18](#S2.F18 "Figure 18 ‣ 2.6 Conditional Architectures ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey") 所示。虽然这项工作为未来的条件计算方法铺平了道路，但它的不足之处在于模块的组合没有被学习。因此，每个模块及模块组合的角色是固定的，无法改进。'
- en: '![Refer to caption](img/7bbda7fc6d53c53cc7cd4c2d8ab79fcd.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7bbda7fc6d53c53cc7cd4c2d8ab79fcd.png)'
- en: 'Figure 18: Example Neural Module Network execution Andreas et al. [[2016](#bib.bib7)].
    The semantic structure of a given question is used to dynamically instantiate
    a network made of modules that correspond to the elements of the question.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：神经模块网络执行示例 Andreas 等人 [[2016](#bib.bib7)]。给定问题的语义结构用于动态实例化一个由对应于问题元素的模块组成的网络。
- en: 'Routing Networks Rosenbaum et al. [[2017](#bib.bib127)] and the Compositional
    Recursive Learner (CRL) Chang et al. [[2018](#bib.bib25)] are more recent related
    works of conditional computation in which the composition of modules is learned
    in addition to the weights of the modules themselves. A Routing Network is comprised
    of a router and a set of neural network modules. Given a piece of input data,
    the router iteratively chooses a module from the set of network modules to apply
    to the input for a fixed number of iterations; this process is shown in figure
    [19](#S2.F19 "Figure 19 ‣ 2.6 Conditional Architectures ‣ 2 Multi-Task Architectures
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey"). The router can also
    choose a “pass" action instead of a module, which simply continues to the next
    iteration of routing. The module weights can be learned directly through backpropagation,
    and the router weights are learned with reinforcement learning to maximize the
    performance of the dynamically instantiated networks on their inputs. The Compositional
    Recursive Learner of Chang et al. [[2018](#bib.bib25)] is similar, though with
    some key differences. Given a piece of input data, the CRL also iteratively chooses
    a network module from a fixed set of modules through which to route the input.
    In the case of the CRL, any task specific information (such as a task ID) is intentionally
    hidden from the network modules, to ensure that the modules learn task-agnostic
    and therefore generalizable information. CRL is also trained with reinforcement
    learning on a curriculum, to encourage the re-use of modules learned on easier
    problems.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 路由网络 Rosenbaum 等人 [[2017](#bib.bib127)] 和组合递归学习器（CRL）Chang 等人 [[2018](#bib.bib25)]
    是更近期的条件计算相关工作，其中除了学习模块本身的权重外，还学习模块的组合。路由网络由一个路由器和一组神经网络模块组成。给定一段输入数据，路由器迭代地从网络模块集合中选择一个模块来应用于输入，直到固定次数的迭代；这一过程如图
    [19](#S2.F19 "图 19 ‣ 2.6 条件架构 ‣ 2 多任务架构 ‣ 深度神经网络多任务学习：综述") 所示。路由器还可以选择一个“传递”操作，而不是模块，这样便继续到下一个路由迭代。模块权重可以通过反向传播直接学习，而路由器权重则通过强化学习进行学习，以最大化动态实例化网络在其输入上的表现。Chang
    等人 [[2018](#bib.bib25)] 的组合递归学习器也类似，但有一些关键差异。给定一段输入数据，CRL 也会从固定的模块集合中迭代地选择一个网络模块来路由输入。在
    CRL 的情况下，任何任务特定的信息（例如任务 ID）都故意隐藏在网络模块之外，以确保模块学习任务无关的且因此具有泛化性的知识。CRL 也通过强化学习在课程上进行训练，以鼓励重用在更简单问题上学到的模块。
- en: '![Refer to caption](img/2ccd175456c7a91a68e317a481fc118c.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ccd175456c7a91a68e317a481fc118c.png)'
- en: 'Figure 19: Example Routing Network execution Rosenbaum et al. [[2017](#bib.bib127)].
    The router iteratively chooses a layer to apply to the input to dynamically instantiate
    a network for each input.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：示例路由网络执行 Rosenbaum 等人 [[2017](#bib.bib127)]。路由器迭代地选择一个层级来应用于输入，以动态地实例化每个输入的网络。
- en: Ahn et al. [[2019](#bib.bib2)] introduces a very similar architecture in which
    layers of varying configuration and scale are chosen from a larger backbone network
    through which to route the input. The router (called the selector network in this
    variant) is again trained with reinforcement learning.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Ahn 等人 [[2019](#bib.bib2)] 引入了一个非常相似的架构，其中从一个更大的骨干网络中选择具有不同配置和规模的层来路由输入。路由器（在此变体中称为选择网络）再次通过强化学习进行训练。
- en: The architecture of Kirsch et al. [[2018](#bib.bib74)] is similarly inspired
    to Routing Networks and the CRL, but takes a local view of routing rather than
    a global one. In Routing Networks and the CRL, any of the network modules can
    be placed into an instantiated network at any depth. In contrast, Kirsch et al.
    [[2018](#bib.bib74)] proposes a conditional architecture in which routing decisions
    are made only within layers of the network. The architecture is made of a series
    of modular layers, each having $m$ network modules. When a layer is to be applied
    to an input, the input is passed through a controller, which selects $k$ modules
    from the set of $m$ modules belonging to the layer. The layer input is then individually
    passed through each of the $k$ selected modules, and the results are added or
    concatenated to form the output of the layer. The controllers in these modular
    layers are trained not with reinforcement learning but with variational methods,
    where the module choice is treated as a latent variable. The authors argue that
    the architectural differences in their model from past works on conditional computation
    diminish the occurence of module collapse, a well-known weakness of conditional
    models. When module collapse occurs, the router selects only a small number of
    modules from the available set, while the remaining modules remain mostly unused,
    and the resulting models do not exhibit modularity.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Kirsch 等人 [[2018](#bib.bib74)] 的架构同样受到路由网络和 CRL 的启发，但它采取了局部视角而非全局视角。在路由网络和 CRL
    中，任何网络模块可以放入任意深度的实例化网络中。相比之下，Kirsch 等人 [[2018](#bib.bib74)] 提出了一个条件架构，其中路由决策仅在网络层内进行。该架构由一系列模块化层组成，每层有
    $m$ 个网络模块。当层应用于输入时，输入会通过一个控制器，控制器从属于该层的 $m$ 个模块中选择 $k$ 个模块。然后，这一层的输入会分别通过 $k$
    个选定的模块，结果会被加和或连接，形成层的输出。这些模块化层中的控制器不是通过强化学习训练的，而是通过变分方法训练的，其中模块选择被视为潜在变量。作者认为，他们模型中的架构差异相比于过去的条件计算工作减少了模块崩溃的发生，这是条件模型的一个著名弱点。当模块崩溃发生时，路由器仅从可用的模块集中选择少量模块，而其余模块大部分未被使用，
    resulting models do not exhibit modularity.
- en: 'Most recently, Soft Modularization Yang et al. [[2020](#bib.bib160)] is another
    conditional approach, which can be seen as a soft relaxation of Routing Networks.
    Soft Modularization uses both a router network and a policy network composed of
    $L$ layers, each with $m$ modules. Instead of making a discrete decision and choosing
    one module at each step of computation, as Routing Networks do, the input to each
    module is a linear combination of the outputs of modules from the previous layer.
    Specifically, the router network takes as input an observation and the corresponding
    task index, and outputs an $m\times m$ matrix of linear combination weights for
    each layer after the first, so that the element in the $i$-th row and $j$-th column
    of the weight matrix for layer $\ell$ denotes the weight of module $i$ from layer
    $\ell-1$ in the input to module $j$ from layer $\ell$. The soft relaxation from
    Routing Networks eliminates the need to train the router separately from the policy,
    and instead the entire network can be trained end-to-end. This architecture is
    also related to Soft Layer Ordering Meyerson and Miikkulainen [[2017](#bib.bib110)]
    (see section [2.3.2](#S2.SS3.SSS2 "2.3.2 Modular Policies ‣ 2.3 Architectures
    for Reinforcement Learning ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey")), though with Soft Modularization the linear
    combination weights aren’t directly learned, instead they are dynamically computed
    by a separate network (the router network) at each step of computation. When combined
    with Soft Actor-Critic Haarnoja et al. [[2018](#bib.bib58)], the Soft Modularization
    agent reaches 60% success rate on MT50 from the Meta-World benchmark Yu et al.
    [[2019](#bib.bib164)].'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，Soft Modularization Yang et al. [[2020](#bib.bib160)] 是另一种条件方法，可以视为对路由网络的软放松。Soft
    Modularization 使用一个由 $L$ 层组成的路由网络和一个策略网络，每层包含 $m$ 个模块。与路由网络在每一步计算中做出离散决策并选择一个模块不同，Soft
    Modularization 将每个模块的输入设为上一层模块输出的线性组合。具体而言，路由网络接收一个观察值和相应的任务索引作为输入，并输出一个 $m\times
    m$ 的线性组合权重矩阵，该矩阵用于每一层的计算，除了第一层之外。权重矩阵中第 $\ell$ 层的第 $i$ 行第 $j$ 列的元素表示来自第 $\ell-1$
    层的模块 $i$ 对第 $\ell$ 层模块 $j$ 输入的权重。与路由网络的软放松消除了将路由器与策略分开训练的需要，整个网络可以端到端训练。该架构也与
    Soft Layer Ordering Meyerson 和 Miikkulainen [[2017](#bib.bib110)] （见[2.3.2](#S2.SS3.SSS2
    "2.3.2 Modular Policies ‣ 2.3 Architectures for Reinforcement Learning ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey")）相关，尽管在
    Soft Modularization 中，线性组合权重并不是直接学习的，而是由一个独立的网络（路由网络）在每一步计算中动态计算的。当与 Soft Actor-Critic
    Haarnoja et al. [[2018](#bib.bib58)] 结合使用时，Soft Modularization 代理在 Meta-World
    基准测试 Yu et al. [[2019](#bib.bib164)] 上达到了 60% 的成功率。'
- en: A thorough discussion of the strengths and weaknesses of routing based approaches
    can be found in Rosenbaum et al. [[2019](#bib.bib128)] and Ramachandran and Le
    [[2019](#bib.bib125)].
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于路由的方法的优缺点的详细讨论可以参见 Rosenbaum et al. [[2019](#bib.bib128)] 和 Ramachandran
    和 Le [[2019](#bib.bib125)]。
- en: 3 Optimization for Multi-Task Learning
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 多任务学习的优化
- en: With MTL architecture design as the modern generalization of hard parameter
    sharing on one side, MTL optimization is the broader version of soft parameter
    sharing on the other. Soft parameter sharing is a way to regularize model parameters
    by penalizing the distance from model parameters to corresponding parameters of
    a model for a different, but related task. While MTL optimization methods do include
    regularization strategies that penalize parameter distance, many other regularization
    strategies are being actively developed. When the challenge of negative transfer
    is viewed through an optimization lens, new methods for dealing with negative
    transfer - aside from various parameter sharing schemes - begin to appear.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: MTL 架构设计是硬参数共享的现代泛化的一方面，而 MTL 优化则是软参数共享的更广泛版本。软参数共享是一种通过惩罚模型参数与其他相关任务模型参数之间的距离来正则化模型参数的方法。虽然
    MTL 优化方法确实包括惩罚参数距离的正则化策略，但许多其他正则化策略也在积极开发中。当从优化的角度看待负迁移问题时，除了各种参数共享方案外，处理负迁移的新方法开始出现。
- en: 'We partition the existing MTL optimization methods into six distinct groups:
    loss weighting, regularization, gradient modulation, task scheduling, multi-objective
    optimization, and knowledge distillation. Just as in previous sections of this
    review, the boundaries between these groups are not always concrete. Certain methods
    may be interpreted as existing in more than one of these groups, but we believe
    that this partition is useful for conceptualizing the various directions of research
    in MTL optimization.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将现有的多任务学习（MTL）优化方法划分为六个不同的组：损失加权、正则化、梯度调节、任务调度、多目标优化和知识蒸馏。正如本综述的前几部分所述，这些组之间的界限并不总是明确的。某些方法可能被解读为属于多个组，但我们认为这种划分对于概念化
    MTL 优化的各种研究方向是有用的。
- en: 3.1 Loss Weighting
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 损失加权
- en: A very common approach to ease multi-task optimization is to balance the individual
    loss functions for different tasks. When a model is to be trained on more than
    one task, the various task-specific loss functions must be combined into a single
    aggregated loss function which the model is trained to minimize. A natural question
    to ask then, is how to exactly to combine multiple loss functions into one that
    is suitable for MTL. Most of the methods we describe here parameterize the aggregated
    loss function as a weighted sum of the task-specific loss functions, and the contribution
    of each method is in the computation these weights. Gong et al. [[2019](#bib.bib55)]
    contains an empirical comparison of existing loss weighting methods.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解多任务优化的一种非常常见的方法是平衡不同任务的个体损失函数。当一个模型需要在多个任务上进行训练时，必须将各种任务特定的损失函数合并为一个单一的聚合损失函数，并使模型在训练过程中最小化该损失函数。一个自然的问题是如何将多个损失函数准确地组合成一个适用于
    MTL 的函数。我们在这里描述的大多数方法将聚合损失函数参数化为任务特定损失函数的加权和，每种方法的贡献在于计算这些权重。Gong 等人 [[2019](#bib.bib55)]
    包含了现有损失加权方法的实证比较。
- en: It should be noted that there are several related works Xu et al. [[2018b](#bib.bib159)],
    Du et al. [[2018](#bib.bib45)] which introduce methods for weighting the loss
    of auxiliary tasks relative to a main task loss. While these methods are interesting
    and potentially useful for MTL, they were designed for a setting that lies outside
    MTL, namely one in which there is a main task accompanied by one or more auxiliary
    tasks.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意到，还有一些相关工作如 Xu 等人 [[2018b](#bib.bib159)]，Du 等人 [[2018](#bib.bib45)] 引入了相对于主任务损失的辅助任务损失加权方法。尽管这些方法有趣且可能对
    MTL 有用，但它们是为一种不同于 MTL 的设置设计的，即一个主任务伴随一个或多个辅助任务的设置。
- en: 3.1.1 Weighting by Uncertainty
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 通过不确定性加权
- en: One of the earliest methods for learning loss weights is Kendall et al. [[2017](#bib.bib72)].
    In this work, the authors treat the multi-task network as a probabilistic model,
    and derive a weighted multi-task loss function by maximizing the likelihood of
    the ground truth output. For the case of training on $N$ simultaneous regression
    tasks, the distribution computed by the network output for task $i$ is the Gaussian
    $\mathcal{N}(f_{i}(x),\sigma_{i}^{2})$, where $f_{i}(x)$ is the network output
    for task $i$ and $\sigma_{i}$ is a learned parameter which signifies the task-dependent
    (or homoscedastic) uncertainty for task $i$. The resulting loss function to jointly
    maximize the likelihood of each such distribution is
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 学习损失权重的最早方法之一是 Kendall 等人 [[2017](#bib.bib72)]。在这项工作中，作者将多任务网络视为一个概率模型，并通过最大化真实输出的似然性来推导加权多任务损失函数。在对
    $N$ 个同时进行的回归任务进行训练的情况下，网络输出为任务 $i$ 计算的分布是高斯分布 $\mathcal{N}(f_{i}(x),\sigma_{i}^{2})$，其中
    $f_{i}(x)$ 是任务 $i$ 的网络输出，而 $\sigma_{i}$ 是一个学习得到的参数，表示任务 $i$ 的任务相关（或同方差）不确定性。结果损失函数是为了联合最大化每个这样的分布的似然性。
- en: '|  | $\sum_{i}\frac{1}{2\sigma^{2}}\&#124;y_{i}-f_{i}(x)\&#124;^{2}+\text{log}~{}\sigma_{i}$
    |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{i}\frac{1}{2\sigma^{2}}\&#124;y_{i}-f_{i}(x)\&#124;^{2}+\text{log}~{}\sigma_{i}$
    |  |'
- en: Here $y_{i}$ is the ground-truth label for task $i$. From this derived loss
    function, we can see that each task’s loss is weighted by the inverse of it’s
    task-dependent uncertainty, so that tasks with less uncertainty will be given
    more weight. Also, each task’s loss is regularized by $\text{log}~{}\sigma_{i}$,
    so that the optimization process isn’t incentivized to follow the degenerate strategy
    of increasing the $\sigma_{i}$’s indefinitely. A very similar formula arises for
    training on classification tasks, and this method for weighting task losses is
    the main contribution of this work. Interestingly, the models trained with this
    method of weighting were shown to outperform identical models which were trained
    with the best performing constant loss weights.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $y_{i}$ 是任务 $i$ 的真实标签。从这个派生的损失函数中，我们可以看到，每个任务的损失由其任务相关不确定性的倒数加权，因此不确定性较小的任务将获得更多的权重。此外，每个任务的损失由
    $\text{log}~{}\sigma_{i}$ 进行正则化，以防优化过程被激励去无限制地增加 $\sigma_{i}$。一个非常类似的公式出现在分类任务的训练中，这种任务损失加权的方法是本文的主要贡献。有趣的是，用这种加权方法训练的模型被证明优于那些用表现最佳的常数损失权重训练的相同模型。
- en: 3.1.2 Weighting by Learning Speed
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 按学习速度加权
- en: Following Kendall et al. [[2017](#bib.bib72)], several methods of weighting
    multi-task loss functions were introduced which weigh a task’s loss by the learning
    speed on that task Chen et al. [[2017](#bib.bib30)], Liu et al. [[2019](#bib.bib93)],
    Zheng et al. [[2018](#bib.bib174)], Liu et al. [[2019a](#bib.bib94)], with slightly
    varying approaches. The majority of these methods increase a task’s loss weight
    when the learning speed for that task is low, in order to balance learning between
    tasks, though this is not the case for all methods discussed here.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Kendall 等人 [[2017](#bib.bib72)] 的方法，提出了几种加权多任务损失函数的方法，这些方法通过该任务的学习速度来加权任务的损失
    Chen 等人 [[2017](#bib.bib30)]，刘等人 [[2019](#bib.bib93)]，郑等人 [[2018](#bib.bib174)]，刘等人
    [[2019a](#bib.bib94)]，方法略有不同。这些方法中的大多数在任务的学习速度较低时会增加任务的损失权重，以便在任务之间平衡学习，尽管并非所有讨论的方法都这样做。
- en: 'Liu et al. [[2019](#bib.bib93)] and Liu et al. [[2019a](#bib.bib94)] each explicitly
    set a task’s loss weight using a ratio of the current loss to a previous loss.
    Let $\mathcal{L}_{i}(t)$ be the loss for task $i$ at timestep $t$, and let $N$
    be the number of tasks. Dynamic Weight Averaging Liu et al. [[2019](#bib.bib93)]
    sets the task weights in the following way:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人 [[2019](#bib.bib93)] 和刘等人 [[2019a](#bib.bib94)] 明确地通过当前损失与先前损失的比率来设置任务的损失权重。设
    $\mathcal{L}_{i}(t)$ 为任务 $i$ 在时间步 $t$ 的损失，$N$ 为任务的数量。动态权重平均（DWA）刘等人 [[2019](#bib.bib93)]
    通过以下方式设置任务权重：
- en: '|  | $\lambda_{i}(t)=\frac{N~{}\text{exp}(r_{i}(t-1)/T)}{\sum_{j}\text{exp}(r_{j}(t-1)/T)}$
    |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lambda_{i}(t)=\frac{N~{}\text{exp}(r_{i}(t-1)/T)}{\sum_{j}\text{exp}(r_{j}(t-1)/T)}$
    |  |'
- en: where $r_{i}(t-1)=\mathcal{L}_{i}(t-1)/\mathcal{L}_{i}(t-2)$ and $T$ is a temperature
    hyperparameter. In other words, the loss weight vector is a softmax over the ratios
    of successive loss values over the last two training steps for each task, multipled
    by the number of tasks. Similarly, Loss Balanced Task Weighting Liu et al. [[2019a](#bib.bib94)]
    sets
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r_{i}(t-1)=\mathcal{L}_{i}(t-1)/\mathcal{L}_{i}(t-2)$ 和 $T$ 是温度超参数。换句话说，损失权重向量是对每个任务在最后两个训练步骤中的连续损失值比率的
    softmax 计算，乘以任务数量。同样，LBTW 刘等人 [[2019a](#bib.bib94)] 设置了
- en: '|  | $\lambda_{i}(t)=\left(\frac{\mathcal{L}_{i}(t)}{\mathcal{L}_{i}(0)}\right)^{\alpha}$
    |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lambda_{i}(t)=\left(\frac{\mathcal{L}_{i}(t)}{\mathcal{L}_{i}(0)}\right)^{\alpha}$
    |  |'
- en: where $\alpha$ is a hyperparameter. Notice that LBTW measures learning speed
    as the ratio of the current loss to the initial loss, while DWA measures it as
    the ratio of the losses from the last two training steps. LBTW also does not normalize
    the weight values to sum to a fixed value.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是一个超参数。请注意，LBTW 通过当前损失与初始损失的比率来衡量学习速度，而 DWA 则通过最后两次训练步骤的损失比率来衡量。LBTW
    也不会将权重值归一化为固定值。
- en: GradNorm Chen et al. [[2017](#bib.bib30)] is similarly inspired to these two
    methods, but doesn’t compute loss weights explicitly. Instead, the weights are
    optimized to minimize an auxiliary loss which measures the difference between
    each task’s gradient and a desired task gradient based on the average task loss
    gradient and the learning speed of each task. To define this auxiliary loss, we
    first must define $G_{i}(t)=\|\nabla_{\theta}\lambda_{i}(t)\mathcal{L}_{i}(t)\|_{2}$
    (weighted gradient for task $i$), $\bar{G}(t)$ as the average of all such $G_{i}(t)$,
    $\tilde{\mathcal{L}}_{i}(t)=\mathcal{L}_{i}(t)/\mathcal{L}_{i}(0)$ (learning speed
    for task $k$), and $r_{i}(t)=\tilde{\mathcal{L}}_{i}(t)/\mathbb{E}_{j}[\tilde{\mathcal{L}}_{j}(t)]$
    (relative learning speed for task $i$). Then the auxiliary loss is defined as
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: GradNorm Chen 等人[[2017](#bib.bib30)] 同样受到这两种方法的启发，但没有明确计算损失权重。相反，权重被优化以最小化一个辅助损失，该损失衡量每个任务的梯度与基于平均任务损失梯度和每个任务学习速度的期望任务梯度之间的差异。为了定义这个辅助损失，我们首先必须定义
    $G_{i}(t)=\|\nabla_{\theta}\lambda_{i}(t)\mathcal{L}_{i}(t)\|_{2}$（任务 $i$ 的加权梯度），$\bar{G}(t)$
    作为所有 $G_{i}(t)$ 的平均值，$\tilde{\mathcal{L}}_{i}(t)=\mathcal{L}_{i}(t)/\mathcal{L}_{i}(0)$（任务
    $k$ 的学习速度），和 $r_{i}(t)=\tilde{\mathcal{L}}_{i}(t)/\mathbb{E}_{j}[\tilde{\mathcal{L}}_{j}(t)]$（任务
    $i$ 的相对学习速度）。然后，辅助损失定义为
- en: '|  | $\mathcal{L}_{\text{grad}}(\lambda(t))=\sum_{j}\&#124;G_{j}(t)-\bar{G}(t)\times[r_{i}(t)]^{\alpha}\&#124;_{1}$
    |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{grad}}(\lambda(t))=\sum_{j}\&#124;G_{j}(t)-\bar{G}(t)\times[r_{i}(t)]^{\alpha}\&#124;_{1}$
    |  |'
- en: where $\alpha$ is again a hyperparameter. By optimizing the task weights $\lambda_{i}(t)$
    to minimize $\mathcal{L}_{\text{grad}}$, the weights are shifted so that tasks
    with a higher learning speed yield gradients with smaller magnitude, and tasks
    with a lower learning speed yield gradients with a larger magnitude. It should
    be noted that this separate optimization adds some compute cost, though the authors
    only apply GradNorm to the last layer of shared weights in the network in order
    to minimize the added cost. Even with this restriction, GradNorm outperforms baselines.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 再次是一个超参数。通过优化任务权重 $\lambda_{i}(t)$ 以最小化 $\mathcal{L}_{\text{grad}}$，权重被调整，使得学习速度较高的任务生成的梯度幅度较小，而学习速度较低的任务生成的梯度幅度较大。需要注意的是，这种单独优化增加了一些计算成本，尽管作者仅将
    GradNorm 应用于网络中共享权重的最后一层，以最小化增加的成本。即便如此，GradNorm 仍然优于基线方法。
- en: 'Notice that all of the methods introduced so far in [3.1.2](#S3.SS1.SSS2 "3.1.2
    Weighting by Learning Speed ‣ 3.1 Loss Weighting ‣ 3 Optimization for Multi-Task
    Learning ‣ Multi-Task Learning with Deep Neural Networks: A Survey") increase
    the weight of a given task’s loss when learning on that task is slower than other
    tasks. In comparison, Zheng et al. [[2018](#bib.bib174)] assigns a loss weight
    to a task which decreases as learning speed increases, assigning a weight of zero
    if the loss increased on the previous training step. More specifically, the weight
    for task $i$ on timestep $t$ is defined in the following way: Let $\mathcal{L}_{i}(t)$
    be the loss from task $i$ on timestep $t$, let $\tilde{\mathcal{L}}_{i}(t)=\alpha\mathcal{L}_{i}(t)+(1-\alpha)\tilde{\mathcal{L}}_{i}(t-1)$,
    and $p_{i}(t)=\text{min}(\tilde{\mathcal{L}}_{i}(t),\tilde{\mathcal{L}}_{i}(t-1))/\tilde{\mathcal{L}}_{i}(t-1)$
    where $\alpha$ is a hyperparameter. Then the weight for task $i$ on timestep $t$
    is set to'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到到目前为止介绍的所有方法在[3.1.2](#S3.SS1.SSS2 "3.1.2 按学习速度加权 ‣ 3.1 损失加权 ‣ 3 多任务学习优化 ‣
    基于深度神经网络的多任务学习：综述")中，当某个任务的学习速度慢于其他任务时，会增加该任务损失的权重。相比之下，郑等人[[2018](#bib.bib174)]为一个任务分配的损失权重会随着学习速度的提高而减少，如果上一个训练步骤中的损失增加，则分配零权重。更具体地，任务
    $i$ 在时间步 $t$ 的权重定义如下：设 $\mathcal{L}_{i}(t)$ 为任务 $i$ 在时间步 $t$ 的损失，设 $\tilde{\mathcal{L}}_{i}(t)=\alpha\mathcal{L}_{i}(t)+(1-\alpha)\tilde{\mathcal{L}}_{i}(t-1)$，且
    $p_{i}(t)=\text{min}(\tilde{\mathcal{L}}_{i}(t),\tilde{\mathcal{L}}_{i}(t-1))/\tilde{\mathcal{L}}_{i}(t-1)$，其中
    $\alpha$ 是一个超参数。然后任务 $i$ 在时间步 $t$ 的权重被设置为
- en: '|  | $\lambda_{i}(t)=-(1-p_{i}(t))^{\gamma}\text{log}(p_{i}(t))$ |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lambda_{i}(t)=-(1-p_{i}(t))^{\gamma}\text{log}(p_{i}(t))$ |  |'
- en: similar to the Focal Loss Lin et al. [[2017](#bib.bib87)]. The rationale behind
    this strategy is that if the loss for task $i$ has increased (i.e. $p_{i}(t)=1$),
    there may be a local minimum in the loss landscape of that task. By assigning
    a weight for this task to zero, training steps will only depend on gradients from
    tasks whose loss is still decreasing, and gradient descent will (hopefully) escape
    from the task-specific local minimum in the landscape of the task whose loss has
    just increased.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Focal Loss Lin等人[[2017](#bib.bib87)]，这种策略的原理是，如果任务$i$的损失增加（即$p_{i}(t)=1$），则该任务的损失函数可能存在局部最小值。通过将该任务的权重设置为零，训练步骤将仅依赖于损失仍在减少的任务的梯度，从而使梯度下降（希望）能摆脱损失刚刚增加的任务的局部最小值。
- en: 3.1.3 Weighting by Performance
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 通过性能加权
- en: Weighting task’s losses by performance is similar to weighting by learning speed.
    These two categories are distinguished by the fact that learning speed can be
    thought of as the rate of change of performance. Given that there are numerous
    works which introduced methods for weighting by learning speed, there are surprisingly
    few methods for weighting by performance. To our knowledge, the only such works
    are Dynamic Task Prioritization Guo et al. [[2018](#bib.bib57)] and the implicit
    schedule in Jean et al. [[2019](#bib.bib70)].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 通过性能加权任务损失类似于通过学习速度加权。这两类方法的区别在于，学习速度可以被认为是性能变化的速率。尽管有很多工作提出了基于学习速度的加权方法，但基于性能的加权方法却出奇地少。根据我们的了解，唯一的此类工作是动态任务优先级
    Guo等人[[2018](#bib.bib57)]和Jean等人[[2019](#bib.bib70)]中的隐式调度。
- en: 'Dynamic Task Prioritization Guo et al. [[2018](#bib.bib57)] was inspired by
    the non-neural MTL work Self-Paced Multi-Task Learning Li et al. [[2016](#bib.bib84)].
    Dynamic Task Prioritization (or DTP) prioritizes difficult tasks and examples
    by assigning weights both at the task level and the example level. DTP employs
    the Focal Loss Lin et al. [[2017](#bib.bib87)] to weigh examples within a task
    and performance metrics such as classification accuracy to weigh tasks themselves,
    where both the example and task level of weights emphasize difficult data over
    easy data. These are the distinguishing factors of this work: usage of performance
    metrics other than the loss function to weigh tasks, and loss weighting at both
    the example and the task level.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 动态任务优先级 Guo等人[[2018](#bib.bib57)] 的灵感来源于非神经MTL工作自适应多任务学习 Li等人[[2016](#bib.bib84)]。动态任务优先级（或DTP）通过在任务级别和示例级别分配权重来优先考虑困难任务和示例。DTP使用Focal
    Loss Lin等人[[2017](#bib.bib87)]来加权任务内的示例，以及使用分类准确率等性能指标来加权任务本身，其中示例和任务级别的权重都强调困难数据相较于简单数据。这些是这项工作的区分因素：使用除损失函数之外的性能指标来加权任务，以及在示例和任务级别上加权损失。
- en: 'The method for loss weighting introduced in Jean et al. [[2019](#bib.bib70)]
    is deemed an implicit task schedule, in reference to the connection between loss
    weighting and task scheduling (see section [3.3](#S3.SS3 "3.3 Task Scheduling
    ‣ 3 Optimization for Multi-Task Learning ‣ Multi-Task Learning with Deep Neural
    Networks: A Survey")). In this work, the $i$-th task is assigned the weight'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jean等人[[2019](#bib.bib70)]提出的损失加权方法被认为是一种隐式任务调度，参考了损失加权与任务调度之间的关系（见[3.3](#S3.SS3
    "3.3 Task Scheduling ‣ 3 Optimization for Multi-Task Learning ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey")章节）。在这项工作中，第$i$个任务被分配的权重为'
- en: '|  | $\lambda_{i}=1+(\text{sign}(\bar{S}-S_{i}))~{}\text{min}(\gamma,(\text{max}_{j}~{}S_{j})^{\alpha}&#124;\bar{S}-S_{i}&#124;^{\beta})$
    |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lambda_{i}=1+(\text{sign}(\bar{S}-S_{i}))~{}\text{min}(\gamma,(\text{max}_{j}~{}S_{j})^{\alpha}&#124;\bar{S}-S_{i}&#124;^{\beta})$
    |  |'
- en: 'where $S_{i}$ is the ratio of the current validation performance to a target
    validation performance, $\bar{S}$ is the average over all $S_{i}$’s, $\gamma$
    is a hyperparameter which limits the difference between task weights, $\alpha$
    is a hyperparameter that adjusts how quickly the weights deviate from uniformity,
    and $\beta$ is a hyperparameter that adjusts the emphasis on deviations of a task’s
    score from the mean score. While the formula to compute the loss weights in this
    implicit schedule looks quite different from the focal loss, they have the same
    intention: focus on tasks with poor performance. Interestingly, this work also
    includes a discussion of the difference between scaling learning rates and scaling
    gradients (there is no difference for vanilla SGD), which is an often overlooked
    but important detail of choosing loss coefficients.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S_{i}$ 是当前验证性能与目标验证性能的比率，$\bar{S}$ 是所有 $S_{i}$ 的平均值，$\gamma$ 是限制任务权重差异的超参数，$\alpha$
    是调整权重偏离均匀性的超参数，$\beta$ 是调整任务得分偏离均值得分强调的超参数。虽然在这种隐式调度中计算损失权重的公式看起来与焦点损失有所不同，但它们有相同的意图：关注表现较差的任务。有趣的是，这项工作还讨论了缩放学习率和缩放梯度之间的差异（对于普通的
    SGD 来说没有差异），这是选择损失系数时一个常被忽视但重要的细节。
- en: 3.1.4 Weighting by Reward Magnitude
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 根据回报幅度加权
- en: It is a well known issue in multi-task learning that a difference in the scale
    of loss functions between tasks can cause imbalanced learning dynamics when training
    jointly on such tasks. For example, consider an MTL setting with two tasks, $T_{1}$
    and $T_{2}$, both classification tasks. Suppose that the loss $\mathcal{L}_{1}$
    for task $T_{1}$ is a standard cross-entropy loss, and the loss $\mathcal{L}_{2}$
    for $T_{2}$ is equal to the standard cross-entropy loss multiplied by a constant
    factor of 1000\. It is clear in this case that the gradient for the joint task
    loss $\mathcal{L}=\mathcal{L}_{1}+\mathcal{L}_{2}$ will be mostly dependent on
    the network’s performance on $T_{2}$ and very little on that of $T_{1}$, so that
    the multi-task learning will actually be focused mostly on $T_{2}$. While this
    is a somewhat contrived example, the same principle applies to MTL settings in
    the wild, where the scale of loss functions may differ greatly. One approach to
    tackle this issue is to compute task loss weights based on the magnitude of each
    task’s loss function.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在多任务学习中，一个众所周知的问题是任务之间损失函数的尺度差异会导致在联合训练时学习动态不平衡。例如，考虑一个有两个任务的多任务学习设置，$T_{1}$
    和 $T_{2}$，这两个任务都是分类任务。假设任务 $T_{1}$ 的损失 $\mathcal{L}_{1}$ 是标准的交叉熵损失，而任务 $T_{2}$
    的损失 $\mathcal{L}_{2}$ 等于标准的交叉熵损失乘以一个常数因子 1000。很明显，在这种情况下，联合任务损失 $\mathcal{L}=\mathcal{L}_{1}+\mathcal{L}_{2}$
    的梯度将主要依赖于网络在 $T_{2}$ 上的表现，而很少依赖于 $T_{1}$ 的表现，从而使得多任务学习实际上主要集中在 $T_{2}$ 上。虽然这是一个有些人为的例子，但同样的原则适用于实际中的多任务学习设置，其中损失函数的尺度可能大相径庭。解决这一问题的一种方法是根据每个任务的损失函数的大小计算任务损失权重。
- en: Hessel et al. [[2018](#bib.bib62)] uses PopArt normalization van Hasselt et al.
    [[2016](#bib.bib150)] to perform loss weighting for multi-task deep reinforcement
    learning. The authors derive a scale-invariant update rule for actor-critic methods,
    then extend it to a multi-task setting. The main idea is to keep a running estimate
    of the mean and standard deviation of the return from each timestep, then replace
    the returns with the normalized versions. The REINFORCE Williams [[1992](#bib.bib155)]
    algorithm uses an update rule in which the gradient of the objective with respect
    to the policy parameter is
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Hessel 等人 [[2018](#bib.bib62)] 使用 PopArt 标准化 van Hasselt 等人 [[2016](#bib.bib150)]
    来为多任务深度强化学习执行损失加权。作者推导出了一种尺度不变的更新规则用于演员-评论家方法，然后将其扩展到多任务设置中。主要思路是保持每个时间步的回报均值和标准差的动态估计，然后用归一化后的回报替代原始回报。REINFORCE
    Williams [[1992](#bib.bib155)] 算法使用了一种更新规则，其中相对于策略参数的目标梯度为
- en: '|  | $(R(t)-v(s_{t}))\nabla_{\theta}\text{log}~{}\pi(a_{t}&#124;s_{t})$ |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $(R(t)-v(s_{t}))\nabla_{\theta}\text{log}~{}\pi(a_{t}&#124;s_{t})$ |  |'
- en: where $R(t)$ is the return from step $t$, $v$ is the value function, $\theta$
    are the parameters of the policy $\pi$, and $s_{t}$ and $a_{t}$ are the state
    and action at step $t$. This work replaces this update rule with
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R(t)$ 是第 $t$ 步的回报，$v$ 是价值函数，$\theta$ 是策略 $\pi$ 的参数，$s_{t}$ 和 $a_{t}$ 是第
    $t$ 步的状态和动作。此工作将更新规则替换为
- en: '|  | $\left(\frac{R_{i}(t)-\mu_{i}}{\sigma_{i}}-\tilde{v}_{i}(s_{t})\right)\nabla_{\theta}\text{log}~{}\pi(a_{t}&#124;s_{t})$
    |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left(\frac{R_{i}(t)-\mu_{i}}{\sigma_{i}}-\tilde{v}_{i}(s_{t})\right)\nabla_{\theta}\text{log}~{}\pi(a_{t}&#124;s_{t})$
    |  |'
- en: where $R_{i}(t)$ is the return on step $t$ for task $i$, $\mu_{i}$ and $\sigma_{i}$
    are running estimates of the mean and standard deviation of $R_{i}(t)$, and $\tilde{v}_{i}$
    is a normalized value function for task $i$. A similar replacement is made for
    the update rule of the value function, and the details can be found in Hessel
    et al. [[2018](#bib.bib62)]. This normalization constrains the reward function
    from each task to have a similar contribution to the update, so that the relative
    importance of each task is agnostic to the scale of the reward functions. Applying
    PopArt to multi-task deep reinforcement learning significantly improves performance
    of agents trained with IMPALA Espeholt et al. [[2018](#bib.bib49)] on the DeepMind
    Lab Beattie et al. [[2016](#bib.bib13)] collection of tasks.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R_{i}(t)$ 是任务 $i$ 在步骤 $t$ 的回报，$\mu_{i}$ 和 $\sigma_{i}$ 是 $R_{i}(t)$ 的均值和标准差的运行估计，$\tilde{v}_{i}$
    是任务 $i$ 的归一化值函数。值函数的更新规则也进行了类似的替换，详细信息可以在 Hessel 等人 [[2018](#bib.bib62)] 中找到。这种归一化将每个任务的奖励函数约束为对更新有类似的贡献，从而使每个任务的相对重要性不受奖励函数规模的影响。将
    PopArt 应用于多任务深度强化学习显著提高了使用 IMPALA Espeholt 等人 [[2018](#bib.bib49)] 训练的代理在 DeepMind
    Lab Beattie 等人 [[2016](#bib.bib13)] 任务集合上的表现。
- en: 3.1.5 Geometric Mean of Losses
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 损失的几何平均
- en: While most MTL methods model the network loss as a weighted average of individual
    task losses, Chennupati et al. [[2019](#bib.bib31)] proposes to compute the geometric
    mean of task losses as an alternative. The authors claim that using a geometric
    mean facilitates balanced training of all tasks, and that this loss function handles
    differences in learning speeds of various tasks better than the traditional weighted
    average loss function. However, there is no rigorous evidence to support these
    claims. The results presented in their work show that models trained with the
    geometric outperform baselines, but there has been little work done on analyzing
    the specific properties of optimization using these loss functions in the MTL
    setting, which may be an interesting direction for future research.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数 MTL 方法将网络损失建模为各个任务损失的加权平均，但 Chennupati 等人 [[2019](#bib.bib31)] 提出了将任务损失的几何平均作为替代方案。作者声称，使用几何平均有助于所有任务的平衡训练，并且该损失函数比传统的加权平均损失函数更好地处理了各种任务的学习速度差异。然而，目前没有严格的证据来支持这些说法。他们工作的结果显示，使用几何平均的模型优于基线模型，但对这些损失函数在
    MTL 设置中的优化特性分析的研究较少，这可能是未来研究的一个有趣方向。
- en: 3.2 Regularization
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 正则化
- en: Regularization has long played in important role in multi-task learning, mostly
    in the form of soft parameter sharing. Soft parameter sharing is one of two popular
    techniques for MTL (the other being hard parameter sharing) in which parameters
    aren’t shared between task models, but instead the $L_{2}$ distance between the
    parameters of task models is added to the training objective, in order to encourage
    similar model parameters between different tasks. Soft parameter sharing is simple
    to implement and has been employed extensively in MTL methods.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化在多任务学习中长期发挥着重要作用，主要以软参数共享的形式存在。软参数共享是 MTL 的两种流行技术之一（另一种是硬参数共享），在这种技术中，任务模型之间的参数不会共享，而是将任务模型参数之间的
    $L_{2}$ 距离添加到训练目标中，以鼓励不同任务之间的模型参数相似。软参数共享实现简单，并在 MTL 方法中得到了广泛应用。
- en: Duong et al. [[2015](#bib.bib46)] is a well-known method which uses soft parameter
    sharing instead of hard parameter sharing. The authors employ the architecture
    of Chen and Manning [[2014](#bib.bib27)] for dependency parsing in multiple languages,
    but train separate copies of the same network, one for each language. Only a small
    fraction of the parameters in the network are softly shared between tasks, namely
    the layers which transform the embedded POS tags and the embedded arc labels.
    The use of soft parameter sharing across models for different languages was shown
    to greatly increase performance in the small data setting.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Duong 等人 [[2015](#bib.bib46)] 是一个著名的方法，它使用软参数共享而不是硬参数共享。作者采用 Chen 和 Manning
    [[2014](#bib.bib27)] 的架构进行多语言依赖解析，但为每种语言训练相同网络的单独副本。在网络中，只有一小部分参数在任务之间以软方式共享，即转换嵌入的
    POS 标签和嵌入的弧标签的层。跨模型的软参数共享在小数据设置下显著提高了性能。
- en: An interesting variant of soft parameter sharing is introduced in Yang and Hospedales
    [[2016b](#bib.bib162)], in which the $L_{2}$ distance between parameter vectors
    is replaced by the tensor trace norm of the tensor formed by stacking corresponding
    parameter vectors from different tasks. The trace norm of a matrix is the sum
    of the singular values of that matrix, and it can be thought of as a convex relaxation
    of rank, i.e. the number of non-zero singular values. Therefore, minimizing the
    trace norm of a matrix is a good surrogate for minimizing the rank of that matrix.
    By extending the trace norm from matrices to tensors and minimizing the tensor
    trace norm of stacked parameter vectors, this method encourages the learning of
    parameter vectors across tasks which are similar, just as in traditional soft
    parameter sharing. In this case, however, similarity is measured by the existence
    of linear dependencies between parameter vectors (i.e. low tensor rank), instead
    of $L_{2}$ distance. The authors interpret the resulting trace norms after training
    as a measure of sharing strength between corresponding layers in different task
    models (low trace norm means stronger sharing), and interestingly, the sharing
    strength was found to decrease with layer depth. This conincides with the common
    intuition in MTL that representations in earlier layers should be less task-dependent
    than those of deeper layers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Yang 和 Hospedales [[2016b](#bib.bib162)] 引入了一种有趣的软参数共享变体，其中将参数向量之间的 $L_{2}$
    距离替换为由不同任务的相应参数向量堆叠形成的张量的迹范数。矩阵的迹范数是该矩阵奇异值的和，可以被认为是秩的凸放松，即非零奇异值的数量。因此，最小化矩阵的迹范数是最小化该矩阵秩的良好替代方案。通过将迹范数从矩阵扩展到张量，并最小化堆叠参数向量的张量迹范数，这种方法鼓励在任务之间学习相似的参数向量，就像传统的软参数共享一样。然而，在这种情况下，相似性是通过参数向量之间线性依赖的存在（即低张量秩）来衡量的，而不是
    $L_{2}$ 距离。作者将训练后的迹范数解释为不同任务模型中对应层之间共享强度的度量（低迹范数意味着更强的共享），有趣的是，发现共享强度随着层的深度增加而减少。这与
    MTL 中的常见直觉相符，即较早层的表示应该比深层的表示对任务的依赖性较小。
- en: 'Besides soft parameter sharing, MTL models can also be regularized by placing
    prior distributions on the network parameters. Multilinear Relationship Networks
    (MRNs) Long et al. [[2017](#bib.bib99)] do exactly this by imposing a tensor normal
    distribution as a prior over the parameters in task-specific layers of multi-task
    models. A tensor normal distribution is essentially a multivariate normal distribution
    with the extra assumption that the covariance matrix can be decomposed into the
    Kronecker product of $K$ covariance matrices, where $K$ is the order of a tensor
    following this distribution. Each of these covariance matrices represents the
    covariance between rows of various matricizations of a tensor following the distribution.
    To impose this distribution on the parameter tensor of a multi-task network, the
    covariance is constructed as the Kronecker product of three covariance matrices:
    a covariance matrix representing the relationships between features, another indicating
    the relationships between classification classes, and the last modeling the relationships
    between tasks. It is this construction that allows the model to learn relationships
    between tasks, as its name suggests. At the time of its publication, MRNs reached
    state of the art performance on three different MTL benchmarks.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 除了软参数共享，MTL 模型还可以通过对网络参数施加先验分布来进行正则化。Long 等人（Multilinear Relationship Networks
    (MRNs)）[[2017](#bib.bib99)] 正是通过在多任务模型的任务特定层的参数上施加张量正态分布作为先验来实现这一点。张量正态分布本质上是一个多元正态分布，额外假设协方差矩阵可以分解为
    $K$ 个协方差矩阵的 Kronecker 乘积，其中 $K$ 是遵循该分布的张量的阶数。这些协方差矩阵中的每一个代表了遵循该分布的张量的不同矩阵化形式之间的协方差。为了将这一分布施加到多任务网络的参数张量上，协方差被构造为三个协方差矩阵的
    Kronecker 乘积：一个表示特征之间关系的协方差矩阵，一个表示分类类别之间关系的协方差矩阵，以及一个表示任务之间关系的协方差矩阵。正是这种构造使得模型能够学习任务之间的关系，正如其名称所示。在其发布时，MRNs
    在三个不同的 MTL 基准测试上达到了最先进的性能。
- en: Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) Lee et al. [[2018](#bib.bib83)]
    is a method of regularizing deep multi-task neural networks by introducing an
    autoencoder term to the objective function. This auxiliary task involves reconstructing
    the features from the second to last layer of a network from the network output,
    so that each of the task predictions is used to construct the features for all
    other tasks, a task which was proposed by Asymmetric Multi-Task Learning Lee et al.
    [[2016](#bib.bib82)]. The motivation behind these methods is to allow for information
    to flow from tasks which the model does well to tasks which the model does poorly,
    but not the other way around, hence the “asymmetric" in the names.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) Lee等 [[2018](#bib.bib83)]
    是一种通过在目标函数中引入自编码器项来正则化深度多任务神经网络的方法。这一辅助任务涉及从网络输出中重建倒数第二层的特征，使每个任务预测用于构建所有其他任务的特征，这是Asymmetric
    Multi-Task Learning Lee等 [[2016](#bib.bib82)] 提出的任务。这些方法的动机是允许信息从模型表现良好的任务流向模型表现不佳的任务，而不是反向流动，因此名称中带有“asymmetric”（不对称）。
- en: 'AdaShare Sun et al. [[2019b](#bib.bib147)] (architecture discussed in section
    [2.5.3](#S2.SS5.SSS3 "2.5.3 Modular Sharing ‣ 2.5 Learned Architectures ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey")) introduces
    a novel regularization scheme for MTL methods by regularizing sharing parameters
    instead of module parameters. AdaShare uses a set of neural network blocks which
    are shared between many tasks, though not all blocks are used by every task. The
    sharing parameters of this architecture encode the usage of blocks by different
    tasks, as shown in figure [16](#S2.F16 "Figure 16 ‣ 2.5.3 Modular Sharing ‣ 2.5
    Learned Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with
    Deep Neural Networks: A Survey"). AdaShare regularizes these sharing parameters
    $\alpha_{i}$ instead of the network weights. Specifically, the training objective
    includes two auxiliary terms'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 'AdaShare Sun等 [[2019b](#bib.bib147)] (架构在[2.5.3](#S2.SS5.SSS3 "2.5.3 Modular
    Sharing ‣ 2.5 Learned Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey")节中讨论) 引入了一种新颖的MTL方法正则化方案，通过正则化共享参数而不是模块参数。AdaShare使用了一组在多个任务之间共享的神经网络块，但并非所有块都被每个任务使用。这种架构的共享参数编码了不同任务对块的使用，如图[16](#S2.F16
    "Figure 16 ‣ 2.5.3 Modular Sharing ‣ 2.5 Learned Architectures ‣ 2 Multi-Task
    Architectures ‣ Multi-Task Learning with Deep Neural Networks: A Survey")所示。AdaShare正则化这些共享参数$\alpha_{i}$而不是网络权重。具体来说，训练目标包括两个辅助项'
- en: '|  | $\mathcal{L}_{sparsity}=\sum_{\ell\leq L,i\leq N}\text{log}~{}\alpha_{i,\ell}\qquad\mathcal{L}_{sharing}=\sum_{i,j\leq
    N}\sum_{\ell\leq L}\frac{L-\ell}{L}\&#124;\alpha_{i,\ell}-\alpha_{j,\ell}\&#124;_{1}$
    |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{sparsity}=\sum_{\ell\leq L,i\leq N}\text{log}~{}\alpha_{i,\ell}\qquad\mathcal{L}_{sharing}=\sum_{i,j\leq
    N}\sum_{\ell\leq L}\frac{L-\ell}{L}\&#124;\alpha_{i,\ell}-\alpha_{j,\ell}\&#124;_{1}$
    |  |'
- en: where $N$ is the number of tasks and $L$ is the number of blocks. $\mathcal{L}_{sparsity}$
    encourages each task’s network to be sparse, while $\mathcal{L}_{sharing}$ encourages
    similarity in the sharing parameters of different tasks. Notice that the coefficient
    $\frac{L-\ell}{L}$ in the definition of $\mathcal{L}_{sharing}$ linearly decreases
    the importance of sharing in deeper layers, which follows the observation in Yang
    and Hospedales [[2016b](#bib.bib162)] that more sharing should occur in earlier
    layers, though in this case it is explicitly encouraged. Both of these regularization
    terms are very general, and could potentially be applied to Soft Layer Ordering
    Meyerson and Miikkulainen [[2017](#bib.bib110)], Modular Meta Learning Alet et al.
    [[2018](#bib.bib5)], Stochastic Filter Groups Bragman et al. [[2019](#bib.bib22)],
    and many other architectures which learn what to share between tasks.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$N$是任务的数量，$L$是块的数量。$\mathcal{L}_{sparsity}$鼓励每个任务的网络保持稀疏，而$\mathcal{L}_{sharing}$则鼓励不同任务共享参数的相似性。注意，$\mathcal{L}_{sharing}$定义中的系数$\frac{L-\ell}{L}$线性减少了在更深层中的共享重要性，这符合Yang和Hospedales
    [[2016b](#bib.bib162)] 的观察，即更多的共享应发生在较早的层中，尽管在这种情况下这是显式鼓励的。这两个正则化项非常通用，可能会应用于Soft
    Layer Ordering Meyerson和Miikkulainen [[2017](#bib.bib110)]，Modular Meta Learning
    Alet等 [[2018](#bib.bib5)]，Stochastic Filter Groups Bragman等 [[2019](#bib.bib22)]，以及其他许多学习任务间共享内容的架构中。
- en: 'A related type of regularization is introduced for conditional computation
    models, specifically Routing Networks (discussed in section [2.6](#S2.SS6 "2.6
    Conditional Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with
    Deep Neural Networks: A Survey")), in Cases et al. [[2019](#bib.bib24)]. In this
    case, the regularizer is shaping the decisions of module selection between tasks
    by encouraging diversity of choices made by the router, but it takes a slightly
    different form compared to the AdaShare regularization, due to the differences
    between Routing Networks and the AdaShare architecture. Specifically, Routing
    Networks iteratively construct a network layer by layer, choosing between the
    set of all layers at each step, so that any permutation (with repetitions) of
    layers can be combined into a network by the router. In particular, this means
    that the router can ignore many or most layers and only utilize a few layers per
    task, which is a well known occurence in training Routing Networks called module
    collapse Rosenbaum et al. [[2019](#bib.bib128)]. Module collapse causes a waste
    of network components as well as a decrease in modularity. The issue of module
    collapse is addressed by the regularization technique used in Cases et al. [[2019](#bib.bib24)],
    which rewards diversity of choices made by the router, so that no layer is ignored.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '在Cases等人[[2019](#bib.bib24)]中，为条件计算模型，特别是路由网络（在[2.6](#S2.SS6 "2.6 Conditional
    Architectures ‣ 2 Multi-Task Architectures ‣ Multi-Task Learning with Deep Neural
    Networks: A Survey")节中讨论），引入了一种相关的正则化方法。在这种情况下，正则化器通过鼓励路由器的选择多样性来塑造任务间模块选择的决策，但由于路由网络和AdaShare架构的差异，其形式与AdaShare正则化略有不同。具体而言，路由网络逐层迭代构建网络，在每一步选择所有层的集合，这样任何层的排列（带有重复）都可以通过路由器组合成一个网络。特别是，这意味着路由器可以忽略许多或大多数层，每个任务只使用少量层，这是训练路由网络中一个已知的现象，称为模块崩溃（Rosenbaum等人[[2019](#bib.bib128)]）。模块崩溃导致网络组件的浪费以及模块化的减少。Cases等人[[2019](#bib.bib24)]中使用的正则化技术解决了模块崩溃的问题，奖励路由器做出的选择多样性，从而避免忽略任何层。'
- en: Finally, Maximum Roaming Pascal et al. [[2020](#bib.bib120)] is a multi-task
    regularization method that can be thought of as a variant of Dropout Srivastava
    et al. [[2014](#bib.bib141)] specifically made for MTL networks. While most MTL
    methods partition parameters between tasks in either a fixed or principally learned
    way, Maximum Roaming randomly varies the parameter partitioning during training,
    under the constraint that each parameter must be assigned to a maximal number
    of tasks. Despite the unorthodoxy of this idea, it is empirically demonstrated
    to be beneficial for performance on the CelebA Liu et al. [[2015b](#bib.bib98)],
    CityScapes Cordts et al. [[2016](#bib.bib35)], and NYU-v2 Silberman et al. [[2012](#bib.bib137)]
    datasets. The intuitive explanation of the benefit of roaming is that it allows
    for each unit to learn from all tasks throughout training without the constraint
    that each parameter is always shared between all tasks, which is a likely contributor
    to negative transfer. Intuition aside, this phenomenon is not yet rigorously understood
    and further work is certainly needed to fully utilize the potential gains.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Maximum Roaming Pascal等人[[2020](#bib.bib120)]是一种多任务正则化方法，可以被看作是专门为MTL网络设计的Dropout
    Srivastava等人[[2014](#bib.bib141)]的变体。虽然大多数MTL方法以固定或主要学习的方式在任务之间划分参数，但Maximum Roaming在训练过程中随机变化参数的划分，前提是每个参数必须分配给最大数量的任务。尽管这个想法不寻常，但在CelebA
    Liu等人[[2015b](#bib.bib98)]、CityScapes Cordts等人[[2016](#bib.bib35)]和NYU-v2 Silberman等人[[2012](#bib.bib137)]数据集上的实验表明它对性能有益。漫游的直观解释是，它允许每个单元在训练过程中从所有任务中学习，而不是每个参数始终在所有任务之间共享，这可能是负迁移的一个原因。直观感受虽然重要，但这种现象尚未被严格理解，进一步的研究无疑是充分利用潜在收益的必要条件。
- en: 3.3 Task Scheduling
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 任务调度
- en: Task scheduling is the process of choosing which task or tasks to train on at
    each training step. Most MTL models make this decision in a very simple way, either
    training on all tasks at each step or randomly sampling a subset of tasks to train
    on, though there is some variation in these simple task schedulers. For example,
    when training on a single task at each update step in supervised learning settings,
    it is common to employ either uniform task sampling Dong et al. [[2015](#bib.bib43)],
    where each task has the same probability of being chosen, or proportional task
    sampling Sanh et al. [[2019](#bib.bib132)], in which the probability of choosing
    a task is proportional to the size of its dataset. Despite the fact that most
    methods use these baseline task schedulers, it is a well known fact that optimized
    task scheduling can significantly improve model performance Bengio et al. [[2009](#bib.bib16)].
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 任务调度是选择在每个训练步骤中训练哪个任务或哪些任务的过程。大多数MTL模型以非常简单的方式做出这个决定，要么在每一步训练所有任务，要么随机抽取一个任务子集进行训练，尽管这些简单的任务调度器之间存在一些变异。例如，在监督学习设置中，每次更新步骤只训练一个任务时，通常会使用均匀任务采样Dong等人[[2015](#bib.bib43)]，即每个任务被选择的概率相同，或按比例任务采样Sanh等人[[2019](#bib.bib132)]，其中选择一个任务的概率与其数据集的大小成比例。尽管大多数方法使用这些基线任务调度器，但优化的任务调度可以显著提高模型性能，这是一项众所周知的事实Bengio等人[[2009](#bib.bib16)]。
- en: 'It is important to note that the problem of scheduling tasks is strongly tied
    to the problem of weighting task losses. To see this, consider an MTL setting
    with two tasks, $T_{1}$ and $T_{2}$, with corresponding loss functions $\mathcal{L}_{1}$
    and $\mathcal{L}_{2}$, and consider two separate training setups for this setting.
    In the first setup, the model is trained by minimizing the joint loss $\mathcal{L}_{1}+2\mathcal{L}_{2}$,
    and each training batch holds an equal amount of data from both tasks. In the
    second setup, each training batch either holds data exclusively from $T_{1}$ or
    $T_{2}$, where the chances of the batch containing $T_{1}$ data and $T_{2}$ data
    are $1/3$ and $2/3$, respectively. If a batch is from $T_{1}$, the training step
    will minimize $\mathcal{L}_{1}$, and if a batch is from $T_{2}$, the step will
    minimize $\mathcal{L}_{2}$. It isn’t hard to intuit that these setups will, on
    average, lead to similar results. The training processes won’t be numerically
    equivalent, but each setup jointly optimizes for the tasks in a way that prioritizes
    $T_{2}$ twice as much as $T_{1}$. Loss weighting can be seen as a continuous relaxation
    of task scheduling, so that many task schedulers can easily be adapted to loss
    weighting methods, and vice versa. However, most works adhere to the conventions
    of their subfield and only use one of these two framings: multi-task computer
    vision methods frequently use loss weighting Dai et al. [[2016](#bib.bib37)],
    Misra et al. [[2016](#bib.bib113)], Ruder et al. [[2019](#bib.bib130)], while
    multi-task NLP methods often employ task scheduling Liu et al. [[2015a](#bib.bib95)],
    Luong et al. [[2015](#bib.bib104)], Liu et al. [[2019b](#bib.bib96)].'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，任务调度问题与任务损失加权问题紧密相关。为了解释这一点，考虑一个有两个任务$T_{1}$和$T_{2}$的MTL设置，它们的损失函数分别为$\mathcal{L}_{1}$和$\mathcal{L}_{2}$，并考虑该设置的两种独立训练方式。在第一种设置中，模型通过最小化联合损失$\mathcal{L}_{1}+2\mathcal{L}_{2}$来训练，每个训练批次包含来自两个任务的相等数据量。在第二种设置中，每个训练批次只包含来自$T_{1}$或$T_{2}$的数据，其中包含$T_{1}$数据和$T_{2}$数据的批次的几率分别为$1/3$和$2/3$。如果一个批次来自$T_{1}$，训练步骤将最小化$\mathcal{L}_{1}$，如果批次来自$T_{2}$，步骤将最小化$\mathcal{L}_{2}$。可以直观地理解，这些设置平均会导致类似的结果。训练过程不会在数值上等效，但每种设置共同优化任务的方式是将$T_{2}$的优先级提高到$T_{1}$的两倍。损失加权可以看作是任务调度的连续放松，因此许多任务调度器可以很容易地适应损失加权方法，反之亦然。然而，大多数工作遵循其子领域的惯例，仅使用这两种框架之一：多任务计算机视觉方法经常使用损失加权Dai等人[[2016](#bib.bib37)]，Misra等人[[2016](#bib.bib113)]，Ruder等人[[2019](#bib.bib130)]，而多任务NLP方法通常采用任务调度Liu等人[[2015a](#bib.bib95)]，Luong等人[[2015](#bib.bib104)]，Liu等人[[2019b](#bib.bib96)]。
- en: '![Refer to caption](img/3a20c8689e048a6bb460542f76efa79b.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3a20c8689e048a6bb460542f76efa79b.png)'
- en: 'Figure 20: Task scheduling visualization from Sharma et al. [[2017](#bib.bib135)].
    A meta task-decider is trained to sample tasks with a training signal that encourages
    tasks with worse relative performance to be chosen more frequently.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：Sharma等人提出的任务调度可视化[[2017](#bib.bib135)]。一个元任务决策者经过训练，以一种训练信号来鼓励选择表现相对较差的任务。
- en: 'Sharma et al. [[2017](#bib.bib135)] proposes a method for task scheduling in
    multi-task RL which is based on active learning, with three different variants.
    The common idea behind these three variants is to assign task scheduling probabilities
    based on relative performance to a target level: the further the model is from
    the target performance on a given task, the more likely it is that the task will
    be scheduled. This is akin to the loss weighting methods that increase the loss
    weight of a task that exhibits slow learning. Figure [20](#S3.F20 "Figure 20 ‣
    3.3 Task Scheduling ‣ 3 Optimization for Multi-Task Learning ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey") shows a visualization of the task scheduling
    process. The difference between the three variants is in the implementation of
    the “meta task-decider", the component which computes task sampling probabilities.
    In one way or another, each variant uses the values $m_{i}=1-a_{i}/b_{i}$ to compute
    these probabilities, where $i$ is a task index, $b_{i}$ is the target performance
    for task $i$, and $a_{i}$ is the current model performance for task $i$. Notice
    that $m_{i}$ is a measure of the difference between the current model perfomance
    and the baseline performance for task $i$. The first variant, A5C, doesn’t learn
    a sampling distribution, but instead computes a softmax over all $m_{i}$’s to
    construct the sampling distriubtion over tasks. The second variant, UA4C, treats
    the task sampling problem as a non-stationary multi-armed bandit problem in which
    the reward for the meta task-decider when picking task $i$ is $m_{i}$. This way,
    the agent is rewarded for choosing tasks which are furthest from their respective
    target performances. Lastly, the third variant, EA4C, treats the sequence of task
    sampling decisions as a reinforcement learning problem, so that the meta task-decider
    can learn to choose sequences of tasks which help the agent to learn over time.
    In this case, the reward for the meta task-decider when choosing task $i$ is'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Sharma 等人 [[2017](#bib.bib135)] 提出了一个多任务强化学习中的任务调度方法，该方法基于主动学习，具有三种不同的变体。这三种变体的共同思想是根据相对于目标水平的相对性能分配任务调度概率：模型在给定任务上的表现离目标表现越远，任务被调度的可能性就越大。这类似于增加学习缓慢任务的损失权重的方法。图
    [20](#S3.F20 "图 20 ‣ 3.3 任务调度 ‣ 3 多任务学习的优化 ‣ 使用深度神经网络的多任务学习：调查") 显示了任务调度过程的可视化。这三种变体之间的区别在于“元任务决策器”的实现，该组件计算任务抽样概率。无论是哪种变体，都使用值
    $m_{i}=1-a_{i}/b_{i}$ 来计算这些概率，其中 $i$ 是任务索引，$b_{i}$ 是任务 $i$ 的目标性能，$a_{i}$ 是任务 $i$
    的当前模型性能。注意，$m_{i}$ 是当前模型性能与任务 $i$ 的基线性能之间差异的度量。第一个变体 A5C 不学习抽样分布，而是对所有 $m_{i}$
    进行 softmax 运算，以构建任务上的抽样分布。第二个变体 UA4C 将任务抽样问题视为非平稳的多臂老虎机问题，其中选择任务 $i$ 时元任务决策器的奖励为
    $m_{i}$。这样，代理在选择离各自目标性能最远的任务时会获得奖励。最后，第三个变体 EA4C 将任务抽样决策序列视为强化学习问题，从而使元任务决策器可以学习选择有助于代理随着时间推移学习的任务序列。在这种情况下，选择任务
    $i$ 时元任务决策器的奖励为
- en: '|  | $\lambda m_{i}+(1-\lambda)\left(\frac{1}{3}\sum_{j\in\mathbb{L}}(1-m_{j})\right)$
    |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lambda m_{i}+(1-\lambda)\left(\frac{1}{3}\sum_{j\in\mathbb{L}}(1-m_{j})\right)$
    |  |'
- en: where $\mathbb{L}$ is the task indices of the three tasks with the worst current
    performance and $\lambda$ is a hyperparameter. This reward function then incentivizes
    the meta task-decider to choose tasks which are furthest from their target performance
    while simultaneously choosing tasks which ensure that the performance on the worst
    tasks are still improving. Agents trained with these three variants vastly outperform
    an identical agent with uniform sampling probability on various collections of
    Atari games ranging in size from 6 games to 21 games.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{L}$ 是当前表现最差的三个任务的任务索引，$\lambda$ 是一个超参数。这个奖励函数激励元任务决策器选择离目标性能最远的任务，同时选择确保最差任务的表现仍在改善的任务。在各种
    Atari 游戏集合中，使用这三种变体训练的代理在表现上远远超过了具有均匀抽样概率的相同代理，这些游戏的数量从 6 个到 21 个不等。
- en: The A5C variant of the algorithm of Sharma et al. [[2017](#bib.bib135)] is very
    similar to a more recently proposed method for task scheduling Jean et al. [[2019](#bib.bib70)].
    In this work, each task is assigned an unnormalized score
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Sharma 等人 [[2017](#bib.bib135)] 的 A5C 变体与 Jean 等人 [[2019](#bib.bib70)] 最近提出的任务调度方法非常相似。在这项工作中，每个任务被分配一个未归一化的评分
- en: '|  | $\lambda_{i}=\frac{1}{\text{min}(1,\frac{a_{i}}{b_{i}})^{\alpha}+\epsilon}$
    |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lambda_{i}=\frac{1}{\text{min}(1,\frac{a_{i}}{b_{i}})^{\alpha}+\epsilon}$
    |  |'
- en: 'where $a_{i}$ and $b_{i}$ are defined similarly as above, and $\alpha$ and
    $\epsilon$ are hyperparameters. The unnormalized scores are simply divided by
    their sums to obtain the task sampling probabilities. The novel portion of this
    method is the inclusion of $\epsilon$ for numerical stability and $\alpha$ to
    control the degree of over and under sampling of tasks. Jean et al. [[2019](#bib.bib70)]
    also provides a discussion of task scheduling vs. loss weighting, in which loss
    weighting is referred to as “implicit task scheduling", as well as a loss weighting
    method which is discussed in section [3.1.3](#S3.SS1.SSS3 "3.1.3 Weighting by
    Performance ‣ 3.1 Loss Weighting ‣ 3 Optimization for Multi-Task Learning ‣ Multi-Task
    Learning with Deep Neural Networks: A Survey").'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $a_{i}$ 和 $b_{i}$ 的定义与上述相似，$\alpha$ 和 $\epsilon$ 是超参数。未经归一化的分数通过其总和进行除法，以获得任务采样概率。该方法的新颖之处在于引入了
    $\epsilon$ 以提高数值稳定性，并使用 $\alpha$ 来控制任务的过度采样和欠采样程度。Jean 等人 [[2019](#bib.bib70)]
    还讨论了任务调度与损失加权的比较，其中损失加权被称为“隐式任务调度”，并介绍了一种损失加权方法，该方法在 [3.1.3](#S3.SS1.SSS3 "3.1.3
    Weighting by Performance ‣ 3.1 Loss Weighting ‣ 3 Optimization for Multi-Task
    Learning ‣ Multi-Task Learning with Deep Neural Networks: A Survey") 节中进行了讨论。'
- en: 3.4 Gradient Modulation
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 梯度调制
- en: One of the main challenges in MTL is negative transfer, when the joint training
    of tasks hurts learning instead of helping it. From an optimization perspective,
    negative transfer manifests as the presence of conflicting task gradients. When
    two tasks have gradients which point in opposing directions, following the gradient
    for one task will decrease the performance on the other task, and following the
    average of the two gradients means that neither task sees the same improvement
    it would in a single-task training setting. Among many other approaches to alleviate
    the conflict in learning dynamics between different tasks, explicit gradient modulation
    has arisen as a potential solution. The methods presented here work by modifying
    training gradients, either through the use of adversarial methods or by simply
    replacing gradient vectors when conflicts arise.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习（MTL）中的主要挑战之一是负迁移，即任务的联合训练对学习产生负面影响，而不是帮助它。从优化的角度来看，负迁移表现为存在冲突的任务梯度。当两个任务的梯度方向相反时，跟随一个任务的梯度会降低另一个任务的性能，而跟随两个梯度的平均值则意味着两个任务都无法获得在单任务训练环境下的同样改进。在许多缓解不同任务之间学习动态冲突的方法中，显式梯度调制已经成为一种潜在的解决方案。这里介绍的方法通过修改训练梯度来工作，要么通过使用对抗方法，要么在冲突发生时简单地替换梯度向量。
- en: 3.4.1 Adversarial Gradient Modulation
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 对抗梯度调制
- en: 'If a multi-task model is training on a collection of related tasks, then ideally
    the gradients from these tasks should point in similar directions. Gradient Adversarial
    Training (GREAT) Sinha et al. [[2018](#bib.bib138)] explicitly enforces this condition
    by including an adversarial loss term that encourages gradients from different
    sources to have statistically indistinguishable distributions. GREAT is a general
    framework which can be applied for adversarial defense and knowledge distillation
    (and likely other settings) besides multi-task learning. In the MTL setup, the
    model is augmented with an auxiliary discriminator network which attempts to classify
    the tasks corresponding to gradients of the task decoders, as pictured in figure
    [21](#S3.F21 "Figure 21 ‣ 3.4.1 Adversarial Gradient Modulation ‣ 3.4 Gradient
    Modulation ‣ 3 Optimization for Multi-Task Learning ‣ Multi-Task Learning with
    Deep Neural Networks: A Survey"). During the backward pass, the gradients are
    modified by Gradient Alignment Layers (GALs) through element-wise scaling to minimize
    the performance of the auxiliary network in distinguishing between the task gradients.
    A similar adversarial setup to enforce gradient similarity between tasks is used
    in Maninis et al. [[2019](#bib.bib107)].'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '如果多任务模型正在训练一组相关任务，那么理想情况下，这些任务的梯度应该指向相似的方向。梯度对抗训练（GREAT）Sinha 等人 [[2018](#bib.bib138)]
    通过包含一个对抗损失项明确强制这一条件，鼓励来自不同来源的梯度具有统计上不可区分的分布。GREAT 是一个通用框架，除了多任务学习，还可以应用于对抗防御和知识蒸馏（以及其他设置）。在
    MTL 设置中，模型配备了一个辅助判别网络，试图对任务解码器的梯度进行分类，如图 [21](#S3.F21 "Figure 21 ‣ 3.4.1 Adversarial
    Gradient Modulation ‣ 3.4 Gradient Modulation ‣ 3 Optimization for Multi-Task
    Learning ‣ Multi-Task Learning with Deep Neural Networks: A Survey") 所示。在反向传递过程中，梯度通过梯度对齐层（GALs）进行逐元素缩放，以最小化辅助网络在区分任务梯度时的性能。Maninis
    等人 [[2019](#bib.bib107)] 也使用类似的对抗设置来强制任务之间的梯度相似性。'
- en: '![Refer to caption](img/31e529932c28ec7590272adb7e6a08d3.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/31e529932c28ec7590272adb7e6a08d3.png)'
- en: 'Figure 21: Multi-task GREAT model Sinha et al. [[2018](#bib.bib138)]. An auxiliary
    network takes a gradient vector for a single task’s loss and tries to classify
    which task the gradient vector came from. The network gradients are then modulated
    to minimize the performance of the auxiliary network, to enforce the condition
    that gradients from different task functions have statistically indistinguishable
    distributions.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：多任务 GREAT 模型 Sinha 等人 [[2018](#bib.bib138)]。一个辅助网络接收单个任务损失的梯度向量，并尝试分类该梯度向量来自哪个任务。然后，网络梯度被调制，以最小化辅助网络的性能，强制不同任务函数的梯度具有统计上不可区分的分布。
- en: While the motivation for the model is intuitively plausible, the premise of
    adversarial training isn’t rigorously justified. Just because two tasks are related,
    how can we be sure that their gradient distributions should be identical? Furthermore,
    it seems likely that the distribution of a task’s gradients will change throughout
    training, so how likely can it be that each task’s gradient distributions move
    together? The existence of negative transfer in the first place tells us that
    similar tasks do not necessarily have aligning gradients. Can we actually alleviate
    negative transfer by enforcing gradients to be similar, even when the original
    gradients of the loss function are conflicting? Without theoretical justification,
    we can’t be sure of the answers to these questions. Nevertheless, the experiments
    presented in this work show that GREAT does increase the performance of multi-task
    models, and that it outperforms other multi-task optimization baselines such as
    GradNorm. The nature of both the premise and the results of this model are still
    unanswered questions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型的动机直观上是合理的，但对抗训练的前提并没有得到严格的证明。仅仅因为两个任务相关，我们怎么能确保它们的梯度分布应该是相同的？此外，任务梯度的分布在训练过程中可能会变化，那么每个任务的梯度分布一起变化的可能性有多大？首先存在负迁移就告诉我们，类似的任务不一定具有一致的梯度。即使损失函数的原始梯度冲突，我们是否可以通过强制梯度相似来实际缓解负迁移？没有理论依据，我们无法确定这些问题的答案。然而，这项工作中展示的实验表明，GREAT
    确实提高了多任务模型的性能，并且优于其他多任务优化基准，如 GradNorm。这种模型的前提和结果的本质仍然是未解之谜。
- en: 3.4.2 Gradient Replacement
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 梯度替换
- en: An entirely different approach to gradient modulation is explored in Lopez-Paz
    and Ranzato [[2017](#bib.bib100)], Chaudhry et al. [[2018](#bib.bib26)], Yu et al.
    [[2020](#bib.bib165)]. The main idea behind these three works is to replace a
    task gradient vector which conflicts with another by a modified version which
    has no conflicts. This idea is broad, but the implementations of each of these
    works are similar at heart and we will present a rigorous definition of each.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Lopez-Paz 和 Ranzato [[2017](#bib.bib100)]、Chaudhry 等人 [[2018](#bib.bib26)]、Yu
    等人 [[2020](#bib.bib165)] 的研究中探讨了梯度调制的完全不同的方法。这三项工作的主要思想是用一个没有冲突的修改版替换与另一个任务冲突的任务梯度向量。这个想法很广泛，但这些工作的实现本质上是相似的，我们将对每个进行严格的定义。
- en: 'Lopez-Paz and Ranzato [[2017](#bib.bib100)] introduces Gradient Episodic Memory
    (GEM) for continual learning, a problem formulation in which a model learns multiple
    tasks sequentially, instead of simultaneously as in MTL. GEM keeps an episodic
    memory of training examples from past learned tasks, and enforces the following
    constraint at each update step $t$ when training on task $i$:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Lopez-Paz 和 Ranzato [[2017](#bib.bib100)] 介绍了用于持续学习的梯度情景记忆（GEM），这是一个模型按顺序学习多个任务的定义问题，而不是像多任务学习（MTL）那样同时进行。GEM
    保留了来自过去学习任务的训练样本的情景记忆，并在每次更新步骤 $t$ 训练任务 $i$ 时，强制执行以下约束：
- en: '|  | $\forall j<i:G_{i}(t)^{T}G_{j}(t)\geq 0$ |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\forall j<i:G_{i}(t)^{T}G_{j}(t)\geq 0$ |  |'
- en: 'where $G_{i}(t)$ is the gradient vector for task $i$ (the current task) and
    $G_{j}(t)$ is the gradient of the loss on the data in episodic memory from task
    $j$, at training step $t$. The condition that the dot product between two gradient
    vectors is non-negative is equivalent to the condition that the angle between
    the two gradient vectors is less than 90 degrees, so that they don’t point in
    opposing directions. If this condition isn’t met for some $j$, then $G_{i}(t)$
    is replaced by $\tilde{G}_{i}(t)$, the solution to the following optimization
    problem:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G_{i}(t)$ 是任务 $i$（当前任务）的梯度向量，而 $G_{j}(t)$ 是任务 $j$ 的数据在 episodic memory 中的损失梯度，在训练步骤
    $t$。两个梯度向量之间的点积为非负的条件等同于两个梯度向量之间的夹角小于 90 度，从而它们不指向相反的方向。如果这个条件对于某些 $j$ 不满足，则 $G_{i}(t)$
    被替换为 $\tilde{G}_{i}(t)$，即以下优化问题的解：
- en: '|  | minimize: | $\displaystyle\frac{1}{2}\&#124;G_{i}(t)-\tilde{G}_{i}(t)\&#124;^{2}$
    |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | 最小化： | $\displaystyle\frac{1}{2}\|G_{i}(t)-\tilde{G}_{i}(t)\|^{2}$ |  |'
- en: '|  | subject to: | $\displaystyle\forall j<i:\tilde{G}_{i}(t)^{T}G_{j}(t)\geq
    0$ |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | 约束条件： | $\displaystyle\forall j<i:\tilde{G}_{i}(t)^{T}G_{j}(t)\geq 0$
    |  |'
- en: This quadratic optimization problem can be solved efficiently by instead solving
    the dual and recovering the corresponding value of $\tilde{G}_{i}(t)$. Even so,
    GEM introduces significant increase in computation time compared to traditional
    training. Averaged GEM (A-GEM) Chaudhry et al. [[2018](#bib.bib26)] was proposed
    to alleviate the computation burden. The authors point out that it is much more
    efficient to relax the GEM constraint to
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个二次优化问题可以通过解决对偶问题并恢复相应的 $\tilde{G}_{i}(t)$ 值来有效地解决。即便如此，与传统训练相比，GEM 引入了显著的计算时间增加。Averaged
    GEM (A-GEM) Chaudhry 等人 [[2018](#bib.bib26)] 被提出以减轻计算负担。作者指出，放宽 GEM 约束为
- en: '|  | $G_{i}(t)^{T}G_{\text{avg}}(t)\geq 0$ |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | $G_{i}(t)^{T}G_{\text{avg}}(t)\geq 0$ |  |'
- en: 'where $G_{\text{avg}}(t)=\frac{1}{i-1}\sum_{j<i}G_{j}(t)$. In other words,
    instead of requiring the new gradient to be non-conflicting with the task gradient
    of each previous task, A-GEM only requires that the new gradient be non-conflicting
    with the average of the previous tasks’ gradients. By doing this, the modified
    optimization problem has the following closed form solution:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G_{\text{avg}}(t)=\frac{1}{i-1}\sum_{j<i}G_{j}(t)$。换句话说，A-GEM 只要求新梯度与之前任务梯度的平均值不冲突，而不是要求新梯度与每个先前任务的梯度不冲突。通过这样做，修改后的优化问题有以下封闭形式的解：
- en: '|  | $\tilde{G}_{i}(t)=G_{i}(t)-\frac{G_{i}(t)^{T}G_{\text{avg}}(t)}{G_{\text{avg}}(t)^{T}G_{\text{avg}}(t)}G_{\text{avg}}(t)$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{G}_{i}(t)=G_{i}(t)-\frac{G_{i}(t)^{T}G_{\text{avg}}(t)}{G_{\text{avg}}(t)^{T}G_{\text{avg}}(t)}G_{\text{avg}}(t)$
    |  |'
- en: This slight relaxation of the constraints yields a huge improvement in computation
    time while maintaining the performance of GEM.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这种约束的轻微放宽在保持 GEM 性能的同时，大大提高了计算时间。
- en: 'This exact update rule is adapted for the MTL setting in Yu et al. [[2020](#bib.bib165)]
    with a method named PCGrad. Besides the theoretical analysis in the paper, the
    PCGrad algorithm itself is near identical to A-GEM. The main difference is due
    to the difference in problem formulations: PCGrad is meant to be used when learning
    multiple tasks simultaneously, so multiple gradient vectors must be checked for
    conflicts with others at each update step. When combined with Soft Actor-Critic
    Haarnoja et al. [[2018](#bib.bib58)], PCGrad is able to successfully complete
    70% of the tasks in the MT50 benchmark of the Meta-World environment Yu et al.
    [[2019](#bib.bib164)], a challenging, recently proposed environment for multi-task
    and meta-learning with robotic manipulation tasks.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这个具体的更新规则在 Yu 等人 [[2020](#bib.bib165)] 的 MTL 设置中被适配，方法名为 PCGrad。除了论文中的理论分析外，PCGrad
    算法本身与 A-GEM 基本相同。主要区别在于问题公式的不同：PCGrad 旨在用于同时学习多个任务，因此在每次更新步骤中必须检查多个梯度向量是否与其他梯度向量冲突。当与
    Soft Actor-Critic Haarnoja 等人 [[2018](#bib.bib58)] 结合使用时，PCGrad 能够成功完成 Meta-World
    环境 MT50 基准中的 70% 任务，这是一种具有挑战性的、最近提出的多任务和元学习环境，涉及机器人操作任务。
- en: The success of gradient modulation methods demonstrate that minimizing the presence
    of conflicting gradients between tasks is an effective way to decrease negative
    transfer. Continuing to develop such methods may be an important part of MTL optimization
    in future research.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度调制方法的成功表明，最小化任务间冲突梯度的存在是一种减少负转移的有效方法。继续开发此类方法可能是未来 MTL 优化研究中的重要部分。
- en: 3.5 Knowledge Distillation
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 知识蒸馏
- en: Originally introduced for compressing large ensembles of non-neural machine
    learning models into a single model Bucila et al. [[2006](#bib.bib23)], knowledge
    distillation has found many applications outside of its originally intended domain.
    In MTL, the most common use of knowledge distillation is to instill a single multi-task
    “student" network with the knowledge of many individual single-task “teacher"
    networks. Interestingly, the performance of the student network has been shown
    to surpass that of the teacher networks in some domains, making knowledge distillation
    a desirable method not just for saving memory, but also for increasing performance.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最初为了将大量非神经网络机器学习模型压缩成一个模型而提出的知识蒸馏方法（Bucila et al. [[2006](#bib.bib23)]），如今已经在其最初意图之外的许多应用中找到了用武之地。在多任务学习（MTL）中，知识蒸馏最常见的应用是将一个多任务“学生”网络灌输多个单任务“教师”网络的知识。有趣的是，研究表明，学生网络在某些领域的表现已经超越了教师网络，使得知识蒸馏不仅仅是为了节省内存的理想方法，也是提升性能的有力手段。
- en: 'The first applications of policy distillation for multi-task learning came
    from two separate papers at the same time (uploaded to arXiv on the exact same
    day!), namely Policy Distillation Rusu et al. [[2015](#bib.bib131)] and Actor-Mimic
    Parisotto et al. [[2015](#bib.bib119)]. Both of these methods are designed for
    reinforcement learning, and follow roughly the same template: For each task in
    a collection of tasks, use reinforcement learning to train a task specific policy
    to convergence, and after training, use supervised learning to train a single
    student policy to mimic the outputs of the task-specific teacher policies, such
    as with a mean-square error or cross-entropy loss. Additionally, Actor-Mimic includes
    a feature regression objective, where each teacher network has a corresponding
    feature prediction network which attempts to predict the hidden activations of
    the teacher network from the hidden activations of the student network. The gradients
    of this objective are propagated through the student network, so that the student
    network is trained to compute features which contain the same information as each
    teacher network. Actor-Mimic was also shown to demonstrate impressive transfer
    performance. Transfer to new tasks was performed by removing the last layer of
    the distilled student policy and using the weights as the initialization for a
    single-task policy. The transferred policies were able to learn some new tasks
    faster than policies trained from scratch, though occasionally this transfer would
    slow down learning on new tasks. Also, both of these papers show similar results
    for the student network in the Atari domain: the distilled student network either
    matches or outperforms the single-task teachers. This is somewhat surprising,
    given that the student network is not trained to maximize in-game reward, it is
    only trained to mimic the behavior of the teacher networks.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习中政策蒸馏的首次应用来自于两篇同时发布的独立论文（在同一天上传到arXiv！），即政策蒸馏（Policy Distillation Rusu
    et al. [[2015](#bib.bib131)]）和Actor-Mimic（Parisotto et al. [[2015](#bib.bib119)]）。这两种方法都是为强化学习设计的，并且大致遵循相同的模板：对于任务集合中的每个任务，使用强化学习训练一个任务特定的策略直至收敛，然后在训练后，使用监督学习训练一个单一的学生策略来模拟任务特定的教师策略的输出，例如使用均方误差或交叉熵损失。此外，Actor-Mimic包含一个特征回归目标，其中每个教师网络都有一个相应的特征预测网络，试图从学生网络的隐藏激活中预测教师网络的隐藏激活。这个目标的梯度通过学生网络传播，使学生网络被训练成计算包含与每个教师网络相同信息的特征。Actor-Mimic还展示了令人印象深刻的迁移性能。通过移除蒸馏学生策略的最后一层，并使用权重作为单任务策略的初始化来进行新任务的迁移。这些迁移策略能够比从头训练的策略更快地学习一些新任务，尽管有时这种迁移会减缓新任务的学习。此外，这两篇论文在Atari领域对学生网络的结果显示类似：蒸馏学生网络的表现与单任务教师网络相当或更优。这一点有些令人惊讶，因为学生网络并没有被训练去最大化游戏内奖励，它只是被训练去模拟教师网络的行为。
- en: A common intuitive explanation for the phenomenon of student networks outperforming
    their teachers is that the student networks are provided with a more rich training
    signal than the teacher. For example, in classification, each single-task network
    is provided with a ground truth label for each input in the form of a one-hot
    vector. Meanwhile, the student’s training signal will be a “softer" version of
    this, namely the teachers’ output, a dense vector which may contain information
    about similarity of classes to the ground-truth class and other information not
    found in the ground truth one-hot vector.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的直观解释是，学生网络的表现优于教师网络，因为学生网络获得了比教师网络更丰富的训练信号。例如，在分类任务中，每个单任务网络为每个输入提供一个以一热向量形式呈现的真实标签。同时，学生的训练信号是这种标签的“软化”版本，即教师的输出，这是一个密集的向量，可能包含关于类别相似性和其他在真实标签一热向量中找不到的信息。
- en: '![Refer to caption](img/68b23ffaabb623573c136443b6295301.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/68b23ffaabb623573c136443b6295301.png)'
- en: 'Figure 22: Two architectures from the Distral framework for RL Teh et al. [[2017](#bib.bib149)].
    On the left is an architecture which employs both of the main ideas behind Distral:
    KL-regularization of single-task policies with the multi-task policy and a two-column
    policy for each task, where one column is shared between all tasks. On the right
    is an architecture which only employs KL-regularization of the single-task policies.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22：Distral 框架中的两种架构，来自 RL Teh 等人 [[2017](#bib.bib149)]。左侧是采用了 Distral 的两个主要思想的架构：对单任务策略进行
    KL 正则化与多任务策略以及每个任务的双列策略，其中一列在所有任务之间共享。右侧是只采用单任务策略 KL 正则化的架构。
- en: 'It is interesting to note that most knowledge distillation algorithms (these
    two included) have an asymmetric information flow between student and teacher,
    namely that information travels from teacher to student, but not the other way
    around. This observation raises the question: should the single-task teacher networks
    receive information from the distilled multi-task student network? This isn’t
    possible with the methods discussed so far, since the teacher networks are done
    training before the student network starts it. On the other hand, the Distral
    framework for multi-task reinforcement learning Teh et al. [[2017](#bib.bib149)]
    provides a setting which accomplishes exactly this symmetric information flow
    between student and teacher. Distral is a very general framework which leads to
    several different loss functions and architectures, though each variant is driven
    by one or both of two main ideas: The single-task policies are regularized by
    minimizing the KL-divergence between single-task policies and the shared multi-task
    policy as a part of the training objective, and the policies for each task are
    formed by adding the output of the corresponding single-task policy with the output
    of the shared multi-task policy. Two of the resulting architecture variants are
    pictured in figure [22](#S3.F22 "Figure 22 ‣ 3.5 Knowledge Distillation ‣ 3 Optimization
    for Multi-Task Learning ‣ Multi-Task Learning with Deep Neural Networks: A Survey").
    The details of each variation and the motivation behind the design choices can
    be found in the original work. It should be noted, though, that the lines between
    different approaches to multi-task RL being to blur when considering Distral.
    This framework does not use knowledge distillation in the same sense as Policy
    Distillation and Actor-Mimic, since the shared multi-task network isn’t explicitly
    trained to mimic the outputs of the single-task network.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，大多数知识蒸馏算法（包括这两种）在学生和教师之间存在不对称的信息流，即信息从教师流向学生，但反之则不然。这一观察引发了一个问题：单任务教师网络是否应该从蒸馏的多任务学生网络中接收信息？由于教师网络在学生网络开始训练之前已经完成训练，因此目前讨论的方法无法实现这一点。另一方面，Distral
    框架为多任务强化学习提供了一个设定，正好实现了学生和教师之间对称的信息流。Distral 是一个非常通用的框架，导致了几个不同的损失函数和架构，尽管每个变体都受到两个主要思想之一或两者的驱动：单任务策略通过最小化单任务策略与共享多任务策略之间的KL散度来进行正则化，并且每个任务的策略通过将相应单任务策略的输出与共享多任务策略的输出相加来形成。图
    [22](#S3.F22 "图 22 ‣ 3.5 知识蒸馏 ‣ 3 多任务学习优化 ‣ 深度神经网络的多任务学习：综述") 展示了两种结果架构变体。每种变体的详细信息和设计选择背后的动机可以在原始工作中找到。然而，值得注意的是，在考虑
    Distral 时，不同的多任务强化学习方法之间的界限变得模糊。这个框架并不像政策蒸馏和 Actor-Mimic 那样使用知识蒸馏，因为共享的多任务网络并不是明确地训练来模仿单任务网络的输出。
- en: Most recently, knowledge distillation was applied to multi-task NLP with MT-DNN
    ensembles Liu et al. [[2019c](#bib.bib97)] and Born-Again Multi-tasking networks
    (BAM) Clark et al. [[2019](#bib.bib32)]. Both works mainly use the original template
    for multi-task knowledge distillation, but the authors of BAM also introduce a
    training trick to help student networks surpass their teachers which they name
    teacher annealing. For model input $x$, ground truth label $y$, and teacher output
    $f_{T}(x)$, the usual target output for the student on a given example $x$ is
    $f_{T}(x)$. With teacher annealing, the student’s target output is replaced by
    $\lambda y+(1-\lambda)f_{T}(x)$, where $\lambda$ is linearly annealed from 0 to
    1 throughout student training. This way, by the end of the student training process,
    the student is trying to output the ground truth labels for each input and is
    no longer trying to mimic the teacher, so that the student isn’t inherently limited
    by the teacher’s weaknesses. Ablation studies in this work show that teacher annealing
    does improve student performance on the GLUE benchmark.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，知识蒸馏被应用于多任务自然语言处理（NLP），例如 MT-DNN 集成（Liu et al. [[2019c](#bib.bib97)]）和再生多任务网络（BAM）（Clark
    et al. [[2019](#bib.bib32)]）。这两项工作主要使用原始模板进行多任务知识蒸馏，但 BAM 的作者还引入了一种训练技巧，帮助学生网络超越其教师，称之为教师退火（teacher
    annealing）。对于模型输入 $x$、真实标签 $y$ 和教师输出 $f_{T}(x)$，通常学生在给定示例 $x$ 上的目标输出是 $f_{T}(x)$。使用教师退火时，学生的目标输出被替换为
    $\lambda y+(1-\lambda)f_{T}(x)$，其中 $\lambda$ 在学生训练过程中线性退火，从 0 变到 1。这样，到学生训练过程结束时，学生会尝试输出每个输入的真实标签，而不再试图模仿教师，从而避免了学生固有的教师弱点限制。这项工作中的消融研究表明，教师退火确实改善了学生在
    GLUE 基准测试中的表现。
- en: 3.6 Multi-Objective Optimization
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 多目标优化
- en: The need to optimize for multiple - possibly conflicting - loss functions is
    a fundamental difficulty of MTL. The standard formulation of machine learning
    involves the optimization of a single loss function, so that the methods created
    to solve such problems only consider a single loss function. As we’ve seen so
    far, most MTL methods circumvent this challenge by combining many loss functions
    into one using a weighted average, though this fix isn’t perfect. The map from
    a tuple of loss values $(\mathcal{L}_{1}(t),\mathcal{L}_{2}(t),...,\mathcal{L}_{N}(t))$
    to their weighted average $\sum_{i}\lambda_{i}\mathcal{L}_{i}(t)$ isn’t an injective
    mapping, meaning that some information is lost when we transform a collection
    of loss functions into a single weighted loss function ¹¹1It is a well known fact
    in mathematical analysis that there is no continuous injective mapping from $\mathbb{R}^{d}$
    to $\mathbb{R}$ for $d\geq 2$, so unfortunately there is no immediate candidate
    for a multi-task loss function which is superior to the weighted average in the
    sense of injectivity.. Constructing this weighted average also necessitates a
    choice of weights, which is prone to error. Using multi-objective optimization
    for MTL is an alternative optimization method which doesn’t suffer from these
    weaknesses.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 优化多个 - 可能相互冲突的 - 损失函数是多任务学习（MTL）的一个根本困难。机器学习的标准公式涉及优化单个损失函数，因此为解决此类问题而创建的方法仅考虑单个损失函数。正如我们迄今所见，大多数
    MTL 方法通过使用加权平均将多个损失函数组合成一个来规避这一挑战，尽管这种修复并不完美。从损失值元组 $(\mathcal{L}_{1}(t),\mathcal{L}_{2}(t),...,\mathcal{L}_{N}(t))$
    到其加权平均 $\sum_{i}\lambda_{i}\mathcal{L}_{i}(t)$ 的映射并不是单射映射，这意味着在将一组损失函数转换为单个加权损失函数时会丢失一些信息¹¹1
    在数学分析中，一个众所周知的事实是，从 $\mathbb{R}^{d}$ 到 $\mathbb{R}$ 的连续单射映射对于 $d\geq 2$ 是不存在的，因此不幸的是，没有立即的多任务损失函数候选者在单射性方面优于加权平均。
    构建这种加权平均还需要选择权重，这容易出错。使用多目标优化进行 MTL 是一种不受这些缺点影响的替代优化方法。
- en: 'Multi-objective optimization is exactly the process of optimizing several objective
    functions simultaneously. Notice that in a multi-objective problem, there is not
    necessarily a solution which is a global minimum for all objective functions,
    meaning that typically there are no globally optimal solutions for multi-objective
    optimization problems. Instead, we consider solutions which are Pareto optimal.
    Pareto optimal solutions to a multi-objective problem are those for which the
    performance for any of the objectives can only be improved by worsening performance
    on another objective. In other words, Pareto optimal solutions represent the best
    feasible options for solving a multi-objective optimization problem, up to a trade-off
    between objectives. The set of Pareto optimal solutions to a multi-objective optimization
    problem is called the Pareto frontier, and is visualized in figure [23](#S3.F23
    "Figure 23 ‣ 3.6 Multi-Objective Optimization ‣ 3 Optimization for Multi-Task
    Learning ‣ Multi-Task Learning with Deep Neural Networks: A Survey").'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标优化正是同时优化几个目标函数的过程。请注意，在多目标问题中，并不一定存在一个解，它是所有目标函数的全局最小值，这意味着通常情况下，多目标优化问题并没有全局最优解。相反，我们考虑的是帕累托最优解。对于多目标问题，帕累托最优解是这样的解，对于其中任何一个目标的性能都只能通过恶化另一个目标的性能来改进。换句话说，帕累托最优解代表了解决多目标优化问题的最佳可行选择，需要在目标之间进行权衡。多目标优化问题的帕累托最优解集被称为帕累托前沿，并在图[23](#S3.F23
    "图23 ‣ 3.6 多目标优化 ‣ 3 多任务学习的优化 ‣ 使用深度神经网络的多任务学习：一项调查")中可视化。
- en: '![Refer to caption](img/b1a2f6aa8e0b5c322882589cd2fa627e.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b1a2f6aa8e0b5c322882589cd2fa627e.png)'
- en: 'Figure 23: Visualization of Pareto optimal solutions for a two-objective optimization
    problem Dréo [[2006](#bib.bib44)]. The Pareto frontier is made of all points along
    the red curve.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '图23: Dréo提出的双目标优化问题帕累托最优解的可视化[[2006](#bib.bib44)]。帕累托前沿由红色曲线上的所有点组成。'
- en: 'Despite being a natural fit for MTL and a well-studied problem Miettinen [[1998](#bib.bib111)],
    it has only recently been applied to multi-task problems. Sener and Koltun [[2018](#bib.bib134)]
    brought gradient-based multi-objective optimization algorithms to the field of
    deep multi-task learning by extending the well known Multiple Gradient Descent
    Algorithm (MGDA) Désidéri [[2012](#bib.bib48)] to a form that scales well to the
    high dimensionality of deep learning problems. This is accomplished by minimizing
    an upper bound to the MGDA loss, and doing so incurs only a small amount of computational
    overhead compared to traditional MTL. Pareto Multi-Task Learning Lin et al. [[2019](#bib.bib88)]
    takes this extension one step further by generalizing the algorithm proposed in
    Sener and Koltun [[2018](#bib.bib134)] in order to compute multiple Pareto optimal
    solutions. Since no Pareto optimal solution is a priori superior to any other,
    a set of Pareto optimal solutions which is representative of the Pareto frontier
    is more flexible and likely more useful than a single solution. Pareto MTL works
    by decomposing the multi-objective optimization problem into multiple subproblems,
    each with varying preferences between objectives. Interestingly, the authors show
    that Pareto MTL and the algorithm presented in Sener and Koltun [[2018](#bib.bib134)]
    can actually be formulated as methods to compute adaptive loss weights, similar
    to those discussed in section [3.1](#S3.SS1 "3.1 Loss Weighting ‣ 3 Optimization
    for Multi-Task Learning ‣ Multi-Task Learning with Deep Neural Networks: A Survey").
    This is somewhat counterintuitive, since multi-objective optimization methods
    are intended to be of a fundamentally different nature than methods which optimize
    a weighted average over loss functions. With further exploration, the surprising
    connection between these two directions could potentially lead to a better understanding
    of existing multi-task optimization methods.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 TRL 是 MTL 的自然适配和一个研究充分的问题 Miettinen [[1998](#bib.bib111)]，但它直到最近才被应用于多任务问题。Sener
    和 Koltun [[2018](#bib.bib134)] 将基于梯度的多目标优化算法引入深度多任务学习领域，通过将著名的多梯度下降算法（MGDA）Désidéri
    [[2012](#bib.bib48)] 扩展到适合深度学习问题的高维度形式。这是通过最小化 MGDA 损失的上界来完成的，相比于传统的 MTL，这样做仅会产生少量的计算开销。Pareto
    多任务学习 Lin 等 [[2019](#bib.bib88)] 将这一扩展更进一步，通过对 Sener 和 Koltun [[2018](#bib.bib134)]
    提出的算法进行泛化，以计算多个 Pareto 最优解。由于没有哪个 Pareto 最优解是事先优于其他解的，代表 Pareto 边界的一组 Pareto 最优解比单个解更具灵活性和实用性。Pareto
    MTL 通过将多目标优化问题分解为多个子问题来工作，每个子问题在目标之间有不同的偏好。有趣的是，作者展示了 Pareto MTL 和 Sener 和 Koltun
    [[2018](#bib.bib134)] 提出的算法实际上可以被表述为计算自适应损失权重的方法，这与第 [3.1](#S3.SS1 "3.1 损失加权 ‣
    3 多任务学习的优化 ‣ 使用深度神经网络的多任务学习：综述") 节讨论的方法类似。这有些反直觉，因为多目标优化方法本质上与优化损失函数加权平均的方法具有不同的性质。通过进一步探索，这两个方向之间的惊人联系可能会带来对现有多任务优化方法的更好理解。
- en: 4 Task Relationship Learning
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 任务关系学习
- en: 'We have now discussed MTL architectures and optimization methods, completing
    a broader analogue of the popular dichotomy specified by hard and soft parameter
    sharing. However, there is a lesser known third wheel to this pair of approaches:
    task relationship learning. Task relationship learning (or TRL) is a separate
    approach that doesn’t quite fit into either architecture design or optimization,
    and is more specific to MTL. The goal of TRL is to learn an explicit representation
    of tasks or relationships between tasks, such as clustering tasks into groups
    by similarity, and leveraging the learned task relationships to improve learning
    on the tasks at hand.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经讨论了 MTL（多任务学习）架构和优化方法，完成了对由硬参数共享和软参数共享指定的流行二分法的更广泛的类比。然而，这对方法中还有一个鲜为人知的第三方：任务关系学习。任务关系学习（或
    TRL）是一种独立的方法，它并不完全适合于架构设计或优化，更特定于 MTL。TRL 的目标是学习任务或任务之间关系的明确表示，例如通过相似性将任务聚类到组中，并利用学到的任务关系来提高对当前任务的学习效果。
- en: In this section we discuss three research directions within TRL. The first is
    grouping tasks, where the goal is to partition a collection of tasks into groups
    such that simultaneous training of tasks in a group is beneficial. The second
    is learning transfer relationships, which includes methods that attempt to analyze
    and understand when transferring knowledge from one task to another is beneficial
    for learning. Finally, we discuss task embedding methods, which learn an embedding
    space for tasks themselves.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论了TR的三个研究方向。第一个是任务分组，其目标是将任务集合划分为若干组，使得组内任务的同时训练具有优势。第二个是学习转移关系，包括试图分析和理解将知识从一个任务转移到另一个任务何时对学习有益的方法。最后，我们讨论了任务嵌入方法，这些方法为任务本身学习嵌入空间。
- en: 4.1 Grouping Tasks
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 任务分组
- en: 'As a solution to negative transfer, many MTL methods are designed to adaptively
    share information between related tasks and separate information from tasks which
    might hurt each other’s learning. The papers discussed here use task grouping
    as an alternative solution: if two tasks exhibit negative transfer, simply separate
    their learning from the start. However, doing so requires significant computation
    time for trial and error in training networks jointly for various sets of tasks,
    and there are currently very few methods which can accurately determine the joint
    learning dynamics of groups of tasks without this kind of brute force trial and
    error.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决负迁移的方案，许多MTL方法设计了自适应地在相关任务之间共享信息，并将可能影响彼此学习的任务信息分开。这里讨论的论文使用任务分组作为替代方案：如果两个任务表现出负迁移，则从一开始就将它们的学习分开。然而，这样做需要大量的计算时间来进行试错，以在各种任务组合上共同训练网络，目前很少有方法可以准确确定任务组的联合学习动态，而不需要这种蛮力试错的方法。
- en: Two early concurrent works of learning to group tasks are Alonso and Plank [[2016](#bib.bib6)]
    and Bingel and Søgaard [[2017](#bib.bib19)]. Both of these papers are empirical
    studies analyzing the effectiveness of various task groupings in MTL for natural
    language processing, with a focus on choosing one or two auxiliary tasks (such
    as POS tagging, syntactic chunking, and word counting) to help learning on a main
    task (such as named entity recognition and semantic frame detection) by training
    a multi-task network on many combinations of tasks. In these studies, a single-task
    network is trained for each individual main task, and its performance is compared
    to the performance of a multi-task network trained on the main task jointly with
    one or two auxiliary tasks.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 关于任务分组的早期并行研究有 Alonso 和 Plank [[2016](#bib.bib6)] 和 Bingel 和 Søgaard [[2017](#bib.bib19)]。这两篇论文都是实证研究，分析了在自然语言处理的多任务学习（MTL）中，不同任务分组的有效性，重点是选择一个或两个辅助任务（如词性标注、句法块划分和词频统计）来帮助主任务（如命名实体识别和语义框架检测）的学习，通过在多个任务组合上训练多任务网络。在这些研究中，为每个单独的主任务训练了一个单任务网络，并将其性能与一个多任务网络的性能进行比较，该多任务网络在主任务与一个或两个辅助任务共同训练。
- en: Alonso and Plank [[2016](#bib.bib6)] trains 1440 task combinations, each with
    a main task and one or two auxiliary tasks, and finds that performance on the
    main task improves the most with auxiliary tasks whose label distributions have
    high entropy and low kurtosis. This is consistent with the findings of Bingel
    and Søgaard [[2017](#bib.bib19)], in which 90 pairs of tasks (one main, one auxiliary)
    are trained. Using the results of these training runs as data, this work trains
    a logistic regression model to predict whether an auxiliary task will help or
    hurt main task performance based on features from the datasets and learning curves
    of the two tasks. They also find that entropy of the auxiliary label distribution
    is highly correlated with improvement on the main task, though the features most
    highly correlated with main task improvement are the gradients of the main task
    learning curve when trained on its own. Specifically, if the learning curve of
    a task (when trained in a single-task setup) begins to plateau during the first
    20% to 30% of training, including an auxiliary task in training is likely to improve
    the performance on the main task. The authors speculate that this may be because
    a main task whose learning curve has an early plateau is likely to be stuck in
    a non-optimal local minimum, and the inclusion of an auxiliary task helps the
    optimization process to escape this minimum. Somewhat surprising is their finding
    that the difference in sizes of the main and auxiliary task dataset was not found
    to be indicative of the main task performance gain when including the auxiliary
    task. Despite the fact that these studies don’t treat all tasks identically, as
    is usually the case with MTL, the conditions they find which imply positive transfer
    between tasks are general enough that they may be useful in the multi-task setting.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Alonso和Plank [[2016](#bib.bib6)] 训练了1440个任务组合，每个组合包含一个主要任务和一个或两个辅助任务，并发现，具有高熵和低峰度的辅助任务标签分布对主要任务的性能提升最大。这与Bingel和Søgaard
    [[2017](#bib.bib19)] 的发现一致，他们训练了90对任务（一个主要，一个辅助）。利用这些训练结果作为数据，这项工作训练了一个逻辑回归模型，以预测辅助任务是否会帮助或损害主要任务的性能，基于两个任务的数据集和学习曲线的特征。他们还发现，辅助标签分布的熵与主要任务的改进高度相关，尽管与主要任务改进最相关的特征是主要任务在单独训练时的学习曲线的梯度。具体来说，如果一个任务的学习曲线（在单任务设置中训练时）在训练的前20%到30%期间开始趋于平稳，则在训练中包含一个辅助任务可能会改善主要任务的性能。作者推测，这可能是因为学习曲线早期平稳的主要任务可能被困在一个次优的局部最小值中，而包含辅助任务有助于优化过程逃离这一最小值。令人稍感意外的是，他们发现主要和辅助任务数据集的大小差异并未被发现与包含辅助任务时主要任务性能的提升有关。尽管这些研究没有像通常的MTL那样将所有任务视为相同，但他们发现的这些条件在任务之间暗示正向迁移的条件足够通用，可能在多任务设置中会有所帮助。
- en: An empirical study on the joint training of computer vision tasks is performed
    in Doersch and Zisserman [[2017](#bib.bib42)], with a focus on self-supervised
    tasks, namely relative position regression, colorization, motion segmentation,
    and exemplar matching. This study is less in-depth, as it is not the sole focus
    of the paper, but the authors come to the interesting conclusion that multi-task
    training always improved performance compared to the single-task baselines. This
    fact is very surprising given the inconsistency of improvement that MTL usually
    affords over single-task training. The consistent improvement may be due to the
    relationships between the tasks or the nature of their self-supervised labels,
    but none of these answers are certain.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在Doersch和Zisserman [[2017](#bib.bib42)] 的一项关于计算机视觉任务联合训练的实证研究中，重点关注自监督任务，即相对位置回归、着色、运动分割和示例匹配。由于这些任务不是论文的唯一焦点，这项研究的深入程度较低，但作者得出了一个有趣的结论，即多任务训练相比于单任务基准总是能提高性能。这一点令人非常惊讶，因为多任务学习（MTL）通常对比单任务训练的改进效果不一致。性能的一致提升可能与任务之间的关系或它们自监督标签的性质有关，但这些答案都不确定。
- en: '![Refer to caption](img/cbd33de1d0d03002edfce2ea201a188b.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cbd33de1d0d03002edfce2ea201a188b.png)'
- en: 'Figure 24: An example partitioning of a group of tasks into clusters with positive
    transfer Standley et al. [[2019](#bib.bib142)].'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图24：将一组任务分割成具有正向迁移的簇的示例，Standley等 [[2019](#bib.bib142)]。
- en: 'Adjacent to these empirical studies to analyze multi-task task relationships
    is a principled method for learning these relationships online during training
    without trial-and-error task grouping, called Selective Sharing Strezoski et al.
    [[2019b](#bib.bib144)]. Selective sharing uses a shared trunk architecture to
    handle multiple tasks, and clusters tasks into groups based on the similarity
    of their gradient vectors throughout training. This clustering is motivated by
    the fact that the task-specific branches are all initialized with identical parameters,
    so that the similarity between task gradients is indicative of the similarity
    of tasks. As the clusters of tasks are updated throughout training, the task branches
    of the network are merged so that tasks which are clustered together share parameters,
    and this process continues until the clusters stop changing. Aside from the obvious
    benefit of decreased computation cost compared to large scale empirical studies
    to determine groups of tasks, this method uses learned task features to understand
    the relationships between tasks, which is a powerful and inexpensive approach
    to TRL that is also employed in Kriegeskorte [[2008](#bib.bib76)], Song et al.
    [[2019](#bib.bib140)] (see section [4.2](#S4.SS2 "4.2 Transfer Relationships ‣
    4 Task Relationship Learning ‣ Multi-Task Learning with Deep Neural Networks:
    A Survey") for further discussion). It should be noted, however, that their model
    is based on an assumption which breaks down more and more during training. It
    may be true that gradient vectors are indicative of task similarity at the beginning
    of training, when parameters across tasks are still relatively similar. But as
    training continues and model parameters get further apart, similarity between
    task gradients becomes less and less representative of the similarity between
    tasks, and this signal will devolve into noise with a sufficiently non-convex
    loss landscape. Still, the approach is empirically shown to be effective for computing
    task relationships with proper configuration.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '除了这些用于分析多任务关系的实证研究之外，还有一种原则性方法可以在训练过程中在线学习这些关系，而无需试错式的任务分组，这种方法称为选择性共享（Selective
    Sharing）Strezoski 等人 [[2019b](#bib.bib144)]。选择性共享使用共享主干架构来处理多个任务，并在训练过程中根据梯度向量的相似性将任务聚类成组。这种聚类的动机是任务特定的分支都以相同的参数初始化，因此任务梯度之间的相似性反映了任务之间的相似性。随着训练过程中的任务簇更新，网络的任务分支被合并，以便聚类在一起的任务共享参数，这一过程会持续进行，直到聚类不再发生变化。除了与大规模实证研究相比显著降低计算成本的明显好处之外，这种方法还利用学习到的任务特征来理解任务之间的关系，这是一种强大且廉价的任务关系学习（TRL）方法，也被
    Kriegeskorte [[2008](#bib.bib76)]、Song 等人 [[2019](#bib.bib140)] 采用（详细讨论见 [4.2](#S4.SS2
    "4.2 Transfer Relationships ‣ 4 Task Relationship Learning ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey)）。然而，需要注意的是，他们的模型基于一个假设，该假设在训练过程中逐渐失效。虽然在训练开始时梯度向量可能确实能反映任务相似性，因为此时任务间的参数还相对相似。但随着训练的继续，模型参数逐渐分离，任务梯度之间的相似性越来越不能代表任务之间的相似性，这种信号在足够非凸的损失景观下会退化为噪声。不过，这种方法在适当配置下在计算任务关系方面被实证证明是有效的。'
- en: 'Most recently, Standley et al. [[2019](#bib.bib142)] includes an in-depth empirical
    study of task grouping with the Taskonomy dataset Zamir et al. [[2018](#bib.bib166)]
    and a method to partition a group of tasks into clusters which each exhibit positive
    transfer between their respective tasks. Such a partitioning of tasks is pictured
    in figure [24](#S4.F24 "Figure 24 ‣ 4.1 Grouping Tasks ‣ 4 Task Relationship Learning
    ‣ Multi-Task Learning with Deep Neural Networks: A Survey"). Using four different
    training settings with varying amounts of training data and network sizes to train
    each pair of tasks within groups of five tasks, the authors find several interesting
    trends with a more thorough analysis than the previous studies. First, there were
    mixed results on whether or not multi-task training improved over the single-task
    baselines, with many multi-task networks performing worse than the single-task
    counterparts. Next, the performance gain from single-task to multi-task training
    varies wildly with the training setting, implying that the effectiveness of MTL
    is not as dependent on the relationship of the tasks themselves as we might have
    once thought. Surprisingly, the study also finds no correlation between the multi-task
    affinity and the transfer affinity between tasks, which again shows that there
    are many more factors behind joint task learning dynamics (in both multi-task
    and transfer learning) than just the nature of the tasks in consideration. To
    find a partition of a group of tasks into clusters with desirable learning dynamics,
    this work uses both approximations of the performance of multi-task networks at
    convergence and a branch-and-bound algorithm that uses these approximations to
    select a set of multi-task networks to collectively perform all tasks. Using this
    method of grouping tasks, the resulting multi-task networks consistently outperform
    the single task baselines, which is a vast improvement over the multi-task setups
    from the empirical study in which every single pair of tasks is trained jointly.
    To our knowledge, this is the only computational framework for deciding which
    tasks to train together in multi-task learning that allows for more than two tasks
    to be trained jointly.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，Standley 等人 [[2019](#bib.bib142)] 包含了对 Taskonomy 数据集的任务分组进行深入实证研究，Zamir
    等人 [[2018](#bib.bib166)] 提出了将一组任务划分为每个任务之间表现出正向迁移的簇的方法。任务的这种划分如图 [24](#S4.F24
    "Figure 24 ‣ 4.1 Grouping Tasks ‣ 4 Task Relationship Learning ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey") 所示。通过四种不同的训练设置，使用不同数量的训练数据和网络规模对每对任务进行训练，作者发现了一些有趣的趋势，比之前的研究分析得更为透彻。首先，关于多任务训练是否优于单任务基线的结果混杂，多任务网络中的许多表现不如单任务对应网络。接下来，从单任务到多任务训练的性能提升在不同训练设置下差异很大，这表明
    MTL 的有效性并不像我们曾经认为的那样依赖于任务之间的关系。令人惊讶的是，该研究还发现多任务亲和力与任务之间的迁移亲和力之间没有相关性，这再次表明，在联合任务学习动态（包括多任务和迁移学习）中，除了考虑任务的本质之外，还存在许多其他因素。为了找到具有理想学习动态的任务簇划分，该研究使用了多任务网络在收敛时性能的近似值，并采用了一个分支限界算法，利用这些近似值选择一组多任务网络以共同执行所有任务。使用这种任务分组方法，得到的多任务网络在性能上始终优于单任务基线，这比在实证研究中每对任务都一起训练的多任务设置有了很大改进。据我们所知，这是唯一允许超过两个任务联合训练的多任务学习计算框架。'
- en: 4.2 Transfer Relationships
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 转移关系
- en: Learning transfer relationships between tasks in MTL is related to the problem
    of learning to group tasks for joint learning, though they don’t always correlate,
    as noted above. However, unlike learning tasks simultaneously, transfer learning
    already plays an important role in the wider deep learning research effort; most
    natural language processing and computer vision models start not from scratch,
    but transferring a pre-trained model to use on a new task. Be that as it may,
    research into methods that can explicitly learn transfer relationships between
    tasks is only somewhat recent. With the large existing applicability of transfer
    learning today, these methods have the potential to make a strong impact on the
    larger research community.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在多任务学习（MTL）中，任务之间的迁移关系与联合学习的任务分组问题有关，尽管正如上面所提到的那样，它们并不总是相关的。然而，与同时学习任务不同的是，迁移学习在更广泛的深度学习研究中已经扮演了重要角色；大多数自然语言处理和计算机视觉模型并非从零开始，而是将预训练模型迁移到新任务中。尽管如此，能够明确学习任务之间迁移关系的方法研究仅仅是最近的事。鉴于迁移学习今天的大规模应用，这些方法有潜力对更广泛的研究社区产生重大影响。
- en: 'The first (and certainly most well known) work which attempted to learn transfer
    affinities between tasks is Taskonomy Zamir et al. [[2018](#bib.bib166)]. Aside
    from the Taskonomy dataset with 4 million images labeled for 26 tasks, this paper
    introduces a computational method to automatically construct a taxonomy of visual
    tasks based on transfer relationships between tasks. To do this, a single-task
    network is trained on each individual task, then transfer relationships are computed
    by answering the following question for each pair of tasks: How well can we perform
    task $i$ by training a decoder on top of a feature extractor which was trained
    on task $j$? This is a bit of a simplification, as the actual training setup involves
    transferring from multiple source tasks to a single target task, but the main
    idea is the same. Once the transfer affinities are computed, the problem of constructing
    a task taxonomy is characterized as choosing the ideal source task or tasks for
    each target task in a way that satisfies a budget on the number of source tasks.
    The motivation here is to limit the number of tasks which have access to the full
    amount of supervised data (these are the source tasks), and to learn the remainder
    of tasks by transferring from the source tasks, with only a small amount of training
    data to train the decoder on top of the transferred feature extractor. The problem
    of choosing the ideal set of source tasks and which source tasks to use for each
    target task (given the task transfer affinities) is encoded as a Boolean Integer
    Programming problem. The solution can be represented as a directed graph in which
    the nodes are tasks, and the presence of an edge from task $i$ to task $j$ means
    that task $i$ is included in the set of source tasks for task $j$. Some resulting
    taxonomies for varying supervision budgets and transfer order (maximum number
    of source tasks for each target task) are shown in figure [25](#S4.F25 "Figure
    25 ‣ 4.2 Transfer Relationships ‣ 4 Task Relationship Learning ‣ Multi-Task Learning
    with Deep Neural Networks: A Survey"). Taskonomy is the first large scale empirical
    study to analyze task transfer relationships and compute an explicit hierarchy
    of tasks based on their transfer relationships, and by doing so they are able
    to compute optimal transfer policies for learning a group of related tasks with
    limited supervision. However, their method of doing so is extremely expensive,
    since it involves training for a huge number of combinations of source/target
    tasks. The entire process of constructing task taxonomies took 47,886 GPU hours.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '尝试学习任务间的迁移亲和力的第一个（且无疑是最知名的）工作是 Taskonomy Zamir 等人 [[2018](#bib.bib166)]。除了有
    400 万张标注了 26 个任务的图像的 Taskonomy 数据集外，这篇论文还引入了一种计算方法，自动构建基于任务间迁移关系的视觉任务分类法。为此，首先在每个单独任务上训练一个单任务网络，然后通过回答以下问题来计算迁移关系：我们通过在任务
    $j$ 上训练的特征提取器上训练解码器，能在多大程度上完成任务 $i$？这是一种简化，因为实际的训练设置涉及从多个源任务转移到一个目标任务，但主要思想是一样的。一旦计算出迁移亲和力，构建任务分类法的问题就被表述为为每个目标任务选择理想的源任务或任务集合，以满足源任务数量的预算。这里的动机是限制访问完整监督数据量的任务数量（这些是源任务），并通过从源任务迁移来学习其余任务，仅使用少量训练数据在转移的特征提取器上训练解码器。选择理想源任务集合及为每个目标任务使用哪些源任务（给定任务迁移亲和力）的问题被编码为布尔整数规划问题。解决方案可以表示为一个有向图，其中节点是任务，任务
    $i$ 到任务 $j$ 的边表示任务 $i$ 被包括在任务 $j$ 的源任务集合中。图 [25](#S4.F25 "Figure 25 ‣ 4.2 Transfer
    Relationships ‣ 4 Task Relationship Learning ‣ Multi-Task Learning with Deep Neural
    Networks: A Survey") 显示了不同监督预算和迁移顺序（每个目标任务的最大源任务数量）下的部分结果分类法。Taskonomy 是第一个大规模的实证研究，分析任务迁移关系并根据这些迁移关系计算任务的显式层级，通过这样做，他们能够计算出在有限监督条件下学习一组相关任务的最优迁移策略。然而，他们的方法非常昂贵，因为涉及到大量的源任务/目标任务组合的训练。构建任务分类法的整个过程花费了
    47,886 GPU 小时。'
- en: '![Refer to caption](img/e148affc72c9e60ebf3cf7af46ded8f6.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e148affc72c9e60ebf3cf7af46ded8f6.png)'
- en: 'Figure 25: Task taxonomies for a collection of computer vision tasks as computed
    in Taskonomy Zamir et al. [[2018](#bib.bib166)]. An edge from task $i$ to task
    $j$ denotes that task $i$ is an ideal source task to perform transfer learning
    on task $j$.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 25：Taskonomy Zamir 等人 [[2018](#bib.bib166)] 计算的计算机视觉任务集合的任务分类法。任务 $i$ 到任务
    $j$ 的边表示任务 $i$ 是执行任务 $j$ 迁移学习的理想源任务。
- en: A similarly inspired but much more efficient method for learning task transfer
    relationships is introduced in Dwivedi and Roig [[2019](#bib.bib47)], which uses
    Representation Similarity Analysis (RSA) Kriegeskorte [[2008](#bib.bib76)] to
    compute a measure of similarity between tasks. RSA is a commonly used tool in
    computational neuroscience to quantitatively compare measures of neural activity,
    and it has been adopted for analyzing neural network activations by the deep learning
    community in recent years Vandenhende et al. [[2019](#bib.bib151)]. The underlying
    assumption behind the RSA transfer model in Dwivedi and Roig [[2019](#bib.bib47)]
    is that if two tasks would exhibit positive transfer, then single-task networks
    trained on each of them will tend to learn similar representations, and so RSA
    will be an accurate measure of the transfer affinities of the task at hand. Because
    RSA only involves comparing the representations of different networks, there is
    no need to actually perform any transfer learning between each pair of tasks,
    making the RSA transfer model orders of magnitude faster than Taskonomy. Furthermore,
    the authors find that the computed task affinities from RSA are nearly independent
    of the size of the models used to train on the tasks, so that the computation
    of the task relationships can be done with very small models in order to cut computation
    cost even more.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Dwivedi 和 Roig [[2019](#bib.bib47)] 介绍了一种类似灵感但更高效的任务转移关系学习方法，该方法使用表示相似性分析（RSA）Kriegeskorte
    [[2008](#bib.bib76)] 来计算任务之间的相似度。RSA 是计算神经科学中常用的工具，用于定量比较神经活动的度量，近年来也被深度学习社区采用于分析神经网络激活
    Vandenhende 等人 [[2019](#bib.bib151)]。Dwivedi 和 Roig [[2019](#bib.bib47)] 中的 RSA
    转移模型的基本假设是，如果两个任务会表现出正转移，那么在每个任务上训练的单任务网络将倾向于学习相似的表示，因此 RSA 将是任务转移亲和力的准确度量。由于
    RSA 只涉及比较不同网络的表示，因此不需要实际执行任务对之间的转移学习，使得 RSA 转移模型比 Taskonomy 快了数量级。此外，作者发现，从 RSA
    计算出的任务亲和力几乎独立于用于训练任务的模型大小，因此可以使用非常小的模型来计算任务关系，从而进一步降低计算成本。
- en: 'Most recently, Song et al. [[2019](#bib.bib140)] follows a similar approach
    as the RSA transfer model: compare the similarity of single-task networks to compute
    task transfer affinities, instead of actually performing transfer learning. Instead
    of comparing the networks’ learned representations, the method presented in Song
    et al. [[2019](#bib.bib140)] compares their attribution maps on the same input
    data. An attribution map is a scoring over the individual units of a network’s
    input which represents the relevance of each unit to the network’s output. In
    computer vision, for example, an attribution map assigns a relevance score to
    each pixel in the input. Just as the RSA transfer model assumes that tasks with
    positive transfer will learn similar representations, the attribution map transfer
    model assumes that such tasks will pay attention to the same parts of an input.
    This approach shows similar results as the RSA transfer model: orders of magnitude
    speedup compared to Taskonomy without degradation of the results. Unfortunately,
    this work doesn’t include any direct comparison with the RSA transfer model, so
    there is no evidence of superiority of either model.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Song 等人 [[2019](#bib.bib140)] 采用了一种类似于 RSA 转移模型的方法：比较单任务网络的相似性来计算任务转移亲和力，而不是实际执行转移学习。与比较网络学习的表示不同，Song
    等人 [[2019](#bib.bib140)] 提出的该方法比较了在相同输入数据上的归因图。归因图是对网络输入的各个单元进行评分，表示每个单元对网络输出的相关性。例如，在计算机视觉中，归因图为输入中的每个像素分配一个相关性评分。正如
    RSA 转移模型假设具有正转移的任务会学习到相似的表示一样，归因图转移模型假设这些任务会关注输入的相同部分。这种方法与 RSA 转移模型表现出类似的结果：与
    Taskonomy 相比，速度提高了数量级而结果没有退化。不幸的是，这项工作没有包括与 RSA 转移模型的直接比较，因此没有证据表明哪种模型优越。
- en: 'The existing methods for learning task transfer relationships are all very
    recent, and there is much more work to be done in this area. One interesting thing
    to note is the manner in which the RSA and attribution map transfer models Kriegeskorte
    [[2008](#bib.bib76)], Song et al. [[2019](#bib.bib140)] achieve efficiency while
    computing nontrivial information. To summarize succinctly, these models use the
    network to train the network. Both methods leverage information learned by the
    single-task networks (either intermediate representations or relevance scoring)
    in order to inform training downstream. Taskonomy, on the other hand, trains extra
    networks to do what these two methods did without any extra training. It goes
    to show that the rich information learned by deep networks isn’t only useful for
    the network’s forward pass. In general, even outside of MTL, this information
    can and should be leveraged to further inform model training: Use the network
    to train the network.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的任务迁移关系学习方法都是非常新的，仍然有很多工作需要在这一领域完成。值得注意的一点是RSA和归因映射转移模型Kriegeskorte [[2008](#bib.bib76)]、Song等人[[2019](#bib.bib140)]在计算非平凡信息时如何实现高效。简而言之，这些模型利用网络来训练网络。这两种方法都利用了单任务网络（无论是中间表示还是相关性评分）学到的信息来指导下游训练。另一方面，Taskonomy通过训练额外的网络来完成这两种方法所做的事情，而不需要额外的训练。这表明深度网络学习到的丰富信息不仅对网络的前向传递有用。一般来说，即使在MTL之外，这些信息也可以并且应该用于进一步指导模型训练：使用网络来训练网络。
- en: 4.3 Task Embeddings
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 任务嵌入
- en: Although they are mostly used for meta-learning, task embeddings are a very
    general form of learning task relationships, and are strongly related to the methods
    we have so far discussed in this section. Even with this strong tie between models,
    there is a significant lack of methods in MTL which utilize task embeddings. This
    shouldn’t come as a surprise, though. Task embeddings find their most use in situations
    where a new task is given after already learning a number of tasks from the same
    distribution, and this new task must be localized with respect to tasks already
    learned. If the set of tasks for a model to learn is fixed - as is the case with
    MTL - why should one assign a vector representation to each task? Still, we feel
    that the connection to TRL is important, so we provide a brief summary of several
    task embedding methods in the meta-learning literature.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管任务嵌入大多用于元学习，但它们是一种非常通用的学习任务关系的形式，与我们在本节中讨论的方法有着密切的联系。即便模型之间有如此强的联系，MTL（多任务学习）中仍然缺乏利用任务嵌入的方法。不过，这也不应感到意外。任务嵌入在已有多个相同分布的任务后，当给出一个新任务时最为有用，此时必须将新任务相对于已学任务进行本地化。如果模型要学习的任务集合是固定的——如MTL的情况——那么为什么要给每个任务分配一个向量表示呢？尽管如此，我们认为与TRL（任务关系学习）的联系很重要，因此我们提供了元学习文献中几种任务嵌入方法的简要总结。
- en: James et al. [[2018](#bib.bib68)] uses metric learning to construct a task embedding
    for imitation learning of various robotic manipulation tasks. This model, named
    TecNet, is comprised of an embedding network and a control network. The embedding
    network produces a vector representation of a task given many examples from that
    task, while the control network takes an observation and a task representation
    as input to produce an action. Instead of computing a task embedding from expert
    demonstrations, Achille et al. [[2019](#bib.bib1)] constructs them from the Fisher
    Information Matrix of a pre-trained network. Lastly, Lan et al. [[2019](#bib.bib81)]
    trains a shared policy for meta-reinforcement learning which is conditioned on
    task embeddings. These task embeddings are the outputs of a task encoder which
    is trained to output embeddings based on experience from each task.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: James等人[[2018](#bib.bib68)]使用度量学习来构建用于各种机器人操控任务的模仿学习的任务嵌入。这个名为TecNet的模型由一个嵌入网络和一个控制网络组成。嵌入网络根据来自该任务的许多示例生成任务的向量表示，而控制网络则将观测值和任务表示作为输入，以生成动作。Achille等人[[2019](#bib.bib1)]则从预训练网络的Fisher信息矩阵中构建任务嵌入，而不是从专家演示中计算任务嵌入。最后，Lan等人[[2019](#bib.bib81)]训练了一种共享策略用于元强化学习，该策略以任务嵌入为条件。这些任务嵌入是任务编码器的输出，任务编码器被训练为根据每个任务的经验输出嵌入。
- en: 5 Multi-Task Benchmarks
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 多任务基准
- en: In this section, we give a short overview of commonly used benchmarks in various
    domains of multi-task learning, including benchmarks for computer vision, natural
    language processing, reinforcement learning, and multi-modal problems. It should
    be noted that, while there are a few benchmarks specifically designed for multi-task
    learning (such as Taskonomy Zamir et al. [[2018](#bib.bib166)] and Meta-World
    Yu et al. [[2019](#bib.bib164)]), these are few and far between. Most MTL methods
    are evaluated in multi-task settings which use generic benchmarks that include
    supervision for multiple tasks, such as NYU-v2 Silberman et al. [[2012](#bib.bib137)].
    Lastly, the benchmarks discussed here aren’t an exhaustive list, just a highlight
    of some of the most commonly used MTL benchmarks. The benchmarks within each domain
    are sorted by release date starting with the earliest.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要概述了多任务学习领域中常用的基准，包括计算机视觉、自然语言处理、强化学习和多模态问题的基准。需要注意的是，尽管有少数基准是专门为多任务学习设计的（如Taskonomy
    Zamir et al. [[2018](#bib.bib166)]和Meta-World Yu et al. [[2019](#bib.bib164)]），但这些基准相对较少。大多数MTL方法是在多任务设置中评估的，这些设置使用包括多个任务监督的通用基准，例如NYU-v2
    Silberman et al. [[2012](#bib.bib137)]。最后，这里讨论的基准并不是详尽无遗的列表，只是一些最常用的MTL基准的亮点。每个领域中的基准按发布日期排序，从最早的开始。
- en: 5.1 Computer Vision Benchmarks
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 计算机视觉基准
- en: •
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: NYU-v2 Silberman et al. [[2012](#bib.bib137)] is a dataset of RGB-depth images
    from 464 indoor scenes with 1449 densely labeled images and over 400,000 unlabeled
    images. The labeled images are labeled for instance segmentation, semantic segmentation,
    and scene classification, and all images contain depth values for each pixel.
    All images are frames extracted from video sequences.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NYU-v2 Silberman et al. [[2012](#bib.bib137)] 是一个包含464个室内场景的RGB-深度图像数据集，拥有1449张密集标注的图像和40万多张未标注的图像。标注图像的标签包括实例分割、语义分割和场景分类，所有图像都包含每个像素的深度值。所有图像均为从视频序列中提取的帧。
- en: •
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MS-COCO Lin et al. [[2014](#bib.bib86)] contains 328,000 images of natural scenes
    with a total of 2.5 million object instances spanning 91 object types. The images
    contain labels for image classification, semantic segmentation, and instance segmentation.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MS-COCO Lin et al. [[2014](#bib.bib86)] 包含328,000张自然场景图像，总计2.5百万个对象实例，涵盖91种对象类型。这些图像包含图像分类、语义分割和实例分割的标签。
- en: •
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CelebA Liu et al. [[2015b](#bib.bib98)] has 200,000 images of celebrity faces,
    with 20 images of 10,000 different people. Each image is labeled with 40 face
    attributes and five keypoints, for a total of 8 million facial attribute labels.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CelebA Liu et al. [[2015b](#bib.bib98)] 拥有200,000张名人面孔图像，每个图像包含10,000个不同人的20张图像。每张图像都标注了40个面部属性和五个关键点，总共8百万个面部属性标签。
- en: •
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: OmniGlot Lake et al. [[2015](#bib.bib80)] contains images of characters, unlike
    many of the other popular natural image benchmarks. The dataset contains images
    of 1623 characters from 50 different alphabets, operating in a low-data regime.
    Omniglot was designed with a focus on few-shot learning and meta-learning in image
    classification and generative modeling.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: OmniGlot Lake et al. [[2015](#bib.bib80)] 包含字符图像，与许多其他流行的自然图像基准不同。该数据集包含来自50种不同字母表的1623个字符图像，运行于低数据环境下。Omniglot设计时专注于图像分类和生成建模中的少样本学习和元学习。
- en: •
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CityScapes Cordts et al. [[2016](#bib.bib35)] is comprised of video frames shot
    in the streets of 50 urban cities. The densely labeled subset of the dataset contains
    5000 images with pixel-level annotations, while 20000 other images are coarsely
    labeled. The images are labeled for image classification, semantic segmentation,
    and instance segmentation.
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CityScapes Cordts et al. [[2016](#bib.bib35)] 包含拍摄于50个城市街道的视频帧。数据集中密集标注的子集包含5000张带有像素级注释的图像，而其他20000张图像则是粗略标注的。这些图像标注了图像分类、语义分割和实例分割。
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Taskonomy Zamir et al. [[2018](#bib.bib166)] may be the only large scale computer
    vision dataset specifically intended for research with multi-task learning. The
    dataset consists of 4 million images of indoor scenes from 600 different buildings,
    and each image is annotated for 26 different visual tasks, including 2D, 2.5D,
    and 3D tasks.
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Taskonomy Zamir et al. [[2018](#bib.bib166)] 可能是唯一一个专门用于多任务学习研究的大规模计算机视觉数据集。该数据集包含来自600栋不同建筑的400万张室内场景图像，每张图像都被标注了26个不同的视觉任务，包括2D、2.5D和3D任务。
- en: 5.2 Natural Language Processing Benchmarks
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 自然语言处理基准
- en: Unless otherwise specified, it can be assumed that the text within a corpus
    is English.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则可以假定语料库中的文本是英文的。
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Penn Treebank Marcus et al. [[1993](#bib.bib108)] is a corpus of text consisting
    of 4.5 million words. The text is aggregated from multiple sources including scientific
    abstracts, news stories, book chapters, computer manuals, and more, and contains
    Part-of-Speech tags and syntactical structure annotations.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Penn Treebank Marcus等人[[1993](#bib.bib108)]是一个包含450万单词的文本语料库。文本汇集自多个来源，包括科学摘要、新闻报道、书籍章节、计算机手册等，包含词性标注和句法结构注释。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: OntoNotes 5.0 Weischedel et al. [[2013](#bib.bib154)] is a multi-lingual corpus
    of Arabic, English, and Chinese text with 2.9 million words total, labeled for
    syntax and predicate argument structure, coreference resolution, and word sense
    disambiguation. The text sources consist of written news, broadcast news, web
    data, and more.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: OntoNotes 5.0 Weischedel等人[[2013](#bib.bib154)]是一个多语言语料库，包括阿拉伯语、英语和中文，共2.9百万个单词，标注了语法和谓词论元结构、共指消解和词义消歧。文本来源包括书面新闻、广播新闻、网络数据等。
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: WMT 14 Bojar et al. [[2014](#bib.bib20)] is a dataset from the 2014 Workshop
    on Statistical Machine Translation, with parallel corpuses of many language pairs,
    including French-English, German-English, Hindi-English, Russian-English, and
    Czech-English. These corpuses vary in size between 90 million total English sentences
    and 1 million total Hindi sentences.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: WMT 14 Bojar等人[[2014](#bib.bib20)]是2014年统计机器翻译研讨会的数据集，包含许多语言对的平行语料库，包括法语-英语、德语-英语、印地语-英语、俄语-英语和捷克语-英语。这些语料库的大小从总计9000万英语句子到100万印地语句子不等。
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Stanford Natural Language Inference Bowman et al. [[2015](#bib.bib21)] contains
    570,000 sentence pairs, where each pair contains a label describing their relationship
    as either neutral, entailment, or contradiction. The dataset was acquired through
    Amazon Turk, with the instructions displaying a captioned image and asking for
    one alternative caption, one caption that may be correct, and one caption that
    is certainly incorrect.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Stanford自然语言推理Bowman等人[[2015](#bib.bib21)]包含570,000对句子，每对句子都有一个标签，描述它们之间的关系为中立、蕴涵或矛盾。该数据集通过Amazon
    Turk获得，指示显示了一张带有说明的图片，并要求提供一个替代说明，一个可能正确的说明，以及一个明确错误的说明。
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SciTail Khot et al. [[2018](#bib.bib73)] is a textual entailment dataset consisting
    of scientific statements. The corpus was constructed by converting multiple choice
    questions on science exams (and web data) into entailed and non-entailed pairs,
    for a total of 27,000 total examples.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SciTail Khot等人[[2018](#bib.bib73)]是一个由科学陈述组成的文本蕴涵数据集。该语料库通过将科学考试中的多项选择题（以及网络数据）转换为蕴涵和非蕴涵对，共计27,000个示例。
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GLUE Wang et al. [[2018](#bib.bib153)] consists of nine NLP tasks accompanied
    with data from previously existing NLP corpuses. The benchmark is intended to
    be used to evaluate general language understanding models that can handle all
    or multiple tasks simulataneously. Some tasks are intentionally provided with
    small amounts of training data to encourage information sharing between tasks.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GLUE Wang等人[[2018](#bib.bib153)]包含九个NLP任务，并附有来自先前存在的NLP语料库的数据。该基准旨在评估可以同时处理所有或多个任务的通用语言理解模型。某些任务故意提供少量训练数据，以鼓励任务之间的信息共享。
- en: •
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'decaNLP McCann et al. [[2018](#bib.bib109)] is a collection of ten NLP tasks
    which are all posed as question answering. This is a new approach to NLP benchmarking:
    instead of the task being specified by explicit constraints on the input/output,
    each task is given to the model with a description in natural language. Each example
    is a 3-tuple of question, context, and answer.'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: decaNLP McCann等人[[2018](#bib.bib109)]是一个包含十个自然语言处理任务的集合，所有任务都被表述为问答形式。这是一种新的NLP基准测试方法：与其通过对输入/输出的明确约束来指定任务，不如通过自然语言描述将每个任务提供给模型。每个示例是一个由问题、上下文和答案组成的三元组。
- en: 5.3 Reinforcement Learning Benchmarks
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 强化学习基准
- en: •
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Arcade Learning Environment Bellemare et al. [[2013](#bib.bib14)] (or ALE) is
    a diverse collection of hundreds of Atari 2600 games, where observations are given
    to the agent as raw pixels. These games were originally designed to be a challenge
    for the human video game player, so they present a challenge for modern RL agents
    in aspects such as exploration and learning with sparse rewards.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Arcade Learning Environment Bellemare等人[[2013](#bib.bib14)]（或ALE）是一个多样化的Atari
    2600游戏集合，观察结果以原始像素形式提供给代理。这些游戏最初设计是为了挑战人类玩家，因此在探索和稀疏奖励学习等方面对现代强化学习代理提出了挑战。
- en: •
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DeepMind Lab Beattie et al. [[2016](#bib.bib13)] is a 3D first person game platform
    which requires the agent to make actions from raw pixels. DeepMind Lab offers
    the ability to customize environments through the observations, termination conditions,
    reward functions, and more. The 3D nature of the environment makes for a challenge
    not just in strategic decision making but also in perception.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DeepMind Lab Beattie et al. [[2016](#bib.bib13)] 是一个3D第一人称游戏平台，需要代理从原始像素中进行操作。DeepMind
    Lab 提供了通过观察、终止条件、奖励函数等自定义环境的能力。环境的3D性质不仅对战略决策提出挑战，还对感知能力提出挑战。
- en: •
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Meta-World Yu et al. [[2019](#bib.bib164)] is a collection of robotic manipulation
    tasks designed to encourage research in multi-task learning and meta-learning.
    The collection consists of 50 tasks for a simulated Sawyer robotic arm, each task
    with its own parametric variations, such as goal position. The multi-task benchmarks
    within Meta-World are MT10 and MT50, which consist of simultaneously learning
    10 and 50 tasks, respectively, while the meta-learning benchmarks are ML10 and
    ML45, which consist of learning on 10 and 45 tasks before being asked to quickly
    adapt to new unseen tasks.
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Meta-World Yu et al. [[2019](#bib.bib164)] 是一个机器人操作任务的集合，旨在鼓励多任务学习和元学习的研究。该集合包含50个针对模拟Sawyer机器人手臂的任务，每个任务都有其参数变化，如目标位置。Meta-World中的多任务基准是MT10和MT50，分别包括同时学习10个和50个任务，而元学习基准是ML10和ML45，分别包括在学习10个和45个任务后，快速适应新未见过的任务。
- en: 5.4 Multi-Modal Benchmarks
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 多模态基准
- en: •
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Flickr30K Captions Young et al. [[2014](#bib.bib163)] is a collection of 30,000
    photographs obtained from the image hosting website Flickr, with over 150,000
    corresponding captions.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Flickr30K Captions Young et al. [[2014](#bib.bib163)] 是一个包含30,000张来自图像托管网站Flickr的照片的集合，附有超过150,000个对应的描述。
- en: •
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MS-COCO Captions Chen et al. [[2015](#bib.bib29)] contains over 1.5 million
    captions of more than 300,000 photos from the MS-COCO Lin et al. [[2014](#bib.bib86)]
    dataset. These captions were collected using Amazon Mechanical Turk.
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MS-COCO Captions Chen et al. [[2015](#bib.bib29)] 包含超过150万条来自MS-COCO Lin et
    al. [[2014](#bib.bib86)] 数据集的300,000多张照片的描述。这些描述是通过Amazon Mechanical Turk收集的。
- en: •
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Visual Genome Krishna et al. [[2017](#bib.bib77)] is made of over 100,000 densely
    annotated images with a focus on a grounding connection between visual and linguistic
    concepts. Each image contains over 40 regions which each have their own description,
    17 (on average) question-answer pairs per image, an average of 21 object annotations
    per image, attribute labels per object, and relationship annotations between objects.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Visual Genome Krishna et al. [[2017](#bib.bib77)] 由超过100,000张密集标注的图像组成，重点关注视觉和语言概念之间的基础连接。每张图像包含超过40个区域，每个区域都有其描述，每张图像平均有17对问答对，每张图像平均有21个对象标注、每个对象的属性标签以及对象之间的关系标注。
- en: •
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Flickr30K Entities Plummer et al. [[2015](#bib.bib123)] augments the Flickr30K
    dataset with 276,000 annotated bounding boxes and 244,000 coreference chains.
    The coreference chains identify when references to objects in different captions
    of the same image are referring to the same object.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Flickr30K Entities Plummer et al. [[2015](#bib.bib123)] 增强了Flickr30K数据集，提供了276,000个标注的边界框和244,000个共指链。共指链识别在相同图像的不同描述中对相同对象的引用。
- en: •
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GuessWhat?! De Vries et al. [[2017](#bib.bib38)] is not just a dataset, but
    a dialogue-based guessing game in which a questioner asks an oracle about an unknown
    object pictured in a given image. The paper includes a collection of 150,000 games
    played by humans, with 800,000 visual question answer pairs on 66,000 images.
    The intention of GuessWhat?! is to introduce a task which bridges visual question
    answering with dialogue.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GuessWhat?! De Vries et al. [[2017](#bib.bib38)] 不仅仅是一个数据集，而是一个基于对话的猜谜游戏，其中提问者向一个神谕者询问关于给定图像中未知对象的问题。论文包括150,000个由人类参与的游戏，涵盖66,000张图像上的800,000对视觉问答对。GuessWhat?!
    的目的是引入一个将视觉问答与对话桥接起来的任务。
- en: •
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: VQA 2.0 Goyal et al. [[2017](#bib.bib56)] is a visual question answering dataset
    constructed by balancing the VQA Antol et al. [[2015](#bib.bib9)] dataset with
    a focus on the visual aspect of visual question answering. The paper points out
    that a model can reach decent performance on many VQA benchmarks based only on
    scene regularities and the question at hand while ignoring the visual input. VQA
    2.0 is balanced in the sense that every question is accompanied by two images
    that lead to different answers, so that a successful model must pay attention
    to the visual content of a given image.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VQA 2.0 Goyal et al. [[2017](#bib.bib56)] 是一个视觉问答数据集，通过平衡 VQA Antol et al. [[2015](#bib.bib9)]
    数据集，专注于视觉问答的视觉方面。论文指出，一个模型可以仅基于场景规律和手头的问题在许多 VQA 基准测试中取得不错的表现，而忽略视觉输入。VQA 2.0
    的平衡在于每个问题都伴随两个导致不同答案的图像，因此一个成功的模型必须关注给定图像的视觉内容。
- en: •
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GQA Hudson and Manning [[2019](#bib.bib65)] is another visual question answering
    dataset, constructed by leveraging scene graphs to create 22 million questions
    for their corresponding images. The programmatic construction of the dataset allowed
    for each question to be accompanied by a functional program which characterizes
    the question semantics, and for the distribution of answers to be tuned to minimize
    bias.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GQA Hudson 和 Manning [[2019](#bib.bib65)] 是另一个视觉问答数据集，通过利用场景图为其对应的图像创建了 2200
    万个问题。数据集的程序化构建使得每个问题都伴随一个功能程序，该程序描述了问题语义，并且答案的分布可以进行调整以最小化偏差。
- en: 6 Conclusion
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We have presented a review of the field of multi-task learning, covering the
    three broad directions of architecture design, optimization techniques, and task
    relationship learning. Currently, key techniques for the construction of multi-task
    neural networks include shared feature extractors with task-specific decoders,
    varying parameter sharing schemes in existing network architectures, sharing and
    recombination of neural network modules, learning what to share, and fine-grained
    parameter sharing. The most prominent directions within optimization are per-task
    loss weighting, such as by uncertainty or learning speed, regularization with
    $L_{2}$ and trace norms, gradient modulation and replacement to avoid conflicting
    gradients between tasks, and multi-objective optimization. Finally, several methods
    have been proposed to learn relationships between tasks, such as large-scale empirical
    studies to determine which tasks exhibit positive learning dynamics when learned
    simultaneously, comparing representations of networks to determine task similarity,
    and learning task embeddings.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了多任务学习领域的综述，涵盖了架构设计、优化技术和任务关系学习这三个广泛方向。目前，多任务神经网络构建的关键技术包括具有任务特定解码器的共享特征提取器、现有网络架构中的不同参数共享方案、神经网络模块的共享和重组、学习共享内容以及细粒度参数共享。优化方面最突出的方向包括每任务损失加权，如通过不确定性或学习速度、$L_{2}$
    和迹范数的正则化、梯度调制和替换以避免任务间的冲突梯度以及多目标优化。最后，提出了几种学习任务关系的方法，如大规模实证研究以确定哪些任务在同时学习时表现出积极的学习动态、比较网络的表示以确定任务相似性以及学习任务嵌入。
- en: 'Despite the progress the community has made so far to develop multi-task learning
    for deep networks, there is one direction of research that has had less development
    than others, and that we have not discussed at all so far: theory. This shouldn’t
    come as a surprise, given that this is also true of deep learning in general.
    Still, many non-neural multi-task learning methods are motivated and justified
    by strong theory Baxter [[2000](#bib.bib12)], Ben-David and Borbely [[2008](#bib.bib15)],
    Zhang [[2015](#bib.bib169)], Lounici et al. [[2009](#bib.bib101)], but aside from
    a small pool of recent work Shui et al. [[2019](#bib.bib136)], Ndirango and Lee
    [[2019](#bib.bib114)], D’Eramo et al. [[2020](#bib.bib39)], Wu et al. [[2020](#bib.bib157)],
    Zhang et al. [[2020](#bib.bib168)], Bettgenhäuser et al. [[2020](#bib.bib18)],
    there is a lack of theoretical understanding of MTL with deep neural networks.
    This is an important area to promote a deeper understanding of the field as a
    whole, and we hope to see more development in this direction in the coming years.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管社区在深度网络的多任务学习方面取得了进展，但有一个研究方向的发展较少，至今我们尚未讨论过：理论。这一点并不意外，因为深度学习整体上也存在这种情况。尽管如此，许多非神经多任务学习方法是由强有力的理论
    Baxter [[2000](#bib.bib12)], Ben-David 和 Borbely [[2008](#bib.bib15)], Zhang [[2015](#bib.bib169)],
    Lounici 等人 [[2009](#bib.bib101)] 激励和证明的，但除了少数几篇最近的工作 Shui 等人 [[2019](#bib.bib136)],
    Ndirango 和 Lee [[2019](#bib.bib114)], D’Eramo 等人 [[2020](#bib.bib39)], Wu 等人 [[2020](#bib.bib157)],
    Zhang 等人 [[2020](#bib.bib168)], Bettgenhäuser 等人 [[2020](#bib.bib18)]，对于深度神经网络的多任务学习理论理解仍然不足。这是一个重要领域，有助于更深入地理解整个领域，我们希望在未来几年看到更多的进展。
- en: We believe that the development of multi-task learning (and the related fields
    of meta-learning, transfer learning, and continuous/lifelong learning) is an important
    step towards developing artificial intelligence with more human-like qualities.
    In order to build machines that can learn as quickly and robustly as humans, we
    must create techniques for learning general underlying concepts which are applicable
    between tasks and applying these concepts to new and unfamiliar situations. Building
    systems that truly exhibit these qualities will require approaches from many different
    directions, likely including many that researchers haven’t yet discovered. The
    field has come a long way, but continued effort from the research community is
    needed to fully achieve the potential of multi-task methods.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为多任务学习（以及相关的元学习、迁移学习和持续/终身学习领域）的发展是向具有更类似人类特质的人工智能迈出的重要一步。为了构建能够像人类一样快速而稳健地学习的机器，我们必须创建能够在任务之间应用的通用基础概念，并将这些概念应用于新的和不熟悉的情况。真正展现这些特质的系统的构建将需要来自许多不同方向的方法，包括许多研究人员尚未发现的方法。该领域已取得长足进展，但研究社区需要持续努力，以充分发挥多任务方法的潜力。
- en: References
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Achille et al. [2019] Alessandro Achille, Michael Lam, Rahul Tewari, Avinash
    Ravichandran, Subhransu Maji, Charless C Fowlkes, Stefano Soatto, and Pietro Perona.
    Task2vec: Task embedding for meta-learning. In *Proceedings of the IEEE International
    Conference on Computer Vision*, pages 6430–6439, 2019.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achille 等人 [2019] Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran,
    Subhransu Maji, Charless C Fowlkes, Stefano Soatto 和 Pietro Perona. Task2vec：用于元学习的任务嵌入。发表于*IEEE国际计算机视觉会议论文集*，页码
    6430–6439，2019年。
- en: Ahn et al. [2019] Chanho Ahn, Eunwoo Kim, and Songhwai Oh. Deep elastic networks
    with model selection for multi-task learning. In *Proceedings of the IEEE International
    Conference on Computer Vision*, pages 6529–6538, 2019.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn 等人 [2019] Chanho Ahn, Eunwoo Kim 和 Songhwai Oh. 带有模型选择的深度弹性网络用于多任务学习。发表于*IEEE国际计算机视觉会议论文集*，页码
    6529–6538，2019年。
- en: Akhtar et al. [2019] Md Shad Akhtar, Dushyant Singh Chauhan, Deepanway Ghosal,
    Soujanya Poria, Asif Ekbal, and Pushpak Bhattacharyya. Multi-task learning for
    multi-modal emotion recognition and sentiment analysis. *arXiv preprint arXiv:1905.05812*,
    2019.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akhtar 等人 [2019] Md Shad Akhtar, Dushyant Singh Chauhan, Deepanway Ghosal, Soujanya
    Poria, Asif Ekbal 和 Pushpak Bhattacharyya. 多任务学习用于多模态情感识别和情感分析。*arXiv 预印本 arXiv:1905.05812*，2019年。
- en: Akkaya et al. [2019] Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz
    Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell,
    Raphael Ribas, et al. Solving rubik’s cube with a robot hand. *arXiv preprint
    arXiv:1910.07113*, 2019.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akkaya 等人 [2019] Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin,
    Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael
    Ribas 等人。用机器人手解决魔方。*arXiv 预印本 arXiv:1910.07113*，2019年。
- en: Alet et al. [2018] Ferran Alet, Tomás Lozano-Pérez, and Leslie P Kaelbling.
    Modular meta-learning. *arXiv preprint arXiv:1806.10166*, 2018.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alet等人[2018] Ferran Alet，Tomás Lozano-Pérez和Leslie P Kaelbling.模块化元学习。*arXiv预印本arXiv：1806.10166*，2018年。
- en: Alonso and Plank [2016] Héctor Martínez Alonso and Barbara Plank. When is multitask
    learning effective? semantic sequence prediction under varying data conditions.
    *arXiv preprint arXiv:1612.02251*, 2016.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alonso和Plank[2016] Héctor Martínez Alonso和Barbara Plank.多任务学习何时有效？在不同数据条件下的语义序列预测。*arXiv预印本arXiv：1612.02251*，2016年。
- en: Andreas et al. [2016] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan
    Klein. Neural module networks. In *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, pages 39–48, 2016.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andreas等人[2016] Jacob Andreas，Marcus Rohrbach，Trevor Darrell和Dan Klein.神经模块化网络。在*IEEE计算机视觉与模式识别会议文集*中，第39-48页，2016年。
- en: Andreas et al. [2017] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask
    reinforcement learning with policy sketches. In *International Conference on Machine
    Learning*, pages 166–175, 2017.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andreas等人[2017] Jacob Andreas，Dan Klein和Sergey Levine。带有策略草图的模块化多任务强化学习。在*机器学习国际会议*中，第166-175页，2017年。
- en: 'Antol et al. [2015] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
    Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question
    answering. In *Proceedings of the IEEE international conference on computer vision*,
    pages 2425–2433, 2015.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Antol等人[2015] Stanislaw Antol，Aishwarya Agrawal，Jiasen Lu，Margaret Mitchell，Dhruv
    Batra，C Lawrence Zitnick和Devi Parikh. Vqa：视觉问题回答。在*IEEE国际计算机视觉会议文集*中，第2425-2433页，2015年。
- en: 'Argyriou et al. [2008] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano
    Pontil. Convex multi-task feature learning. *Machine Learning*, 73(3):243–272,
    January 2008. doi: 10.1007/s10994-007-5040-8. URL [https://doi.org/10.1007/s10994-007-5040-8](https://doi.org/10.1007/s10994-007-5040-8).'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argyriou等人[2008] Andreas Argyriou，Theodoros Evgeniou和Massimiliano Pontil.凸多任务特征学习。*机器学习*，73(3)
    ：243–272，2008年1月。doi：10.1007/s10994-007-5040-8。URL [https://doi.org/10.1007/s10994-007-5040-8]
    (https://doi.org/10.1007/s10994-007-5040-8)。
- en: 'Baltrusaitis et al. [2019] Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe
    Morency. Multimodal machine learning: A survey and taxonomy. *IEEE Trans. Pattern
    Anal. Mach. Intell.*, 41(2):423–443, February 2019. ISSN 0162-8828. doi: 10.1109/TPAMI.2018.2798607.
    URL [https://doi.org/10.1109/TPAMI.2018.2798607](https://doi.org/10.1109/TPAMI.2018.2798607).'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baltrusaitis等人[2019] Tadas Baltrusaitis，Chaitanya Ahuja和Louis-Philippe Morency。多模式机器学习：综述与分类。*IEEE
    Trans。Pattern Anal。Mach。Intell。*，41(2) ：423–443，2019年2月。ISSN 0162-8828。doi：10.1109/TPAMI.2018.2798607。URL
    [https://doi.org/10.1109/TPAMI.2018.2798607] (https://doi.org/10.1109/TPAMI.2018.2798607)。
- en: Baxter [2000] Jonathan Baxter. A model of inductive bias learning. *Journal
    of artificial intelligence research*, 12:149–198, 2000.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baxter[2000] Jonathan Baxter.归纳偏见学习模型。*人工智能研究杂志*，12 ：149–198，2000年。
- en: Beattie et al. [2016] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward,
    Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés,
    Amir Sadik, et al. Deepmind lab. *arXiv preprint arXiv:1612.03801*, 2016.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beattie等人[2016] Charles Beattie，Joel Z Leibo，Denis Teplyashin，Tom Ward，Marcus
    Wainwright，Heinrich Küttler，Andrew Lefrancq，Simon Green，Víctor Valdés，Amir Sadik等。
    DeepMind实验室。*arXiv预印本arXiv：1612.03801*，2016年。
- en: 'Bellemare et al. [2013] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael
    Bowling. The arcade learning environment: An evaluation platform for general agents.
    *Journal of Artificial Intelligence Research*, 47:253–279, 2013.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare等人[2013] Marc G Bellemare，Yavar Naddaf，Joel Veness和 Michael Bowling.街机学习环境：通用代理的评估平台。*人工智能研究杂志*，第47页：253–279，2013年。
- en: 'Ben-David and Borbely [2008] Shai Ben-David and Reba Schuller Borbely. A notion
    of task relatedness yielding provable multiple-task learning guarantees. *Mach.
    Learn.*, 73(3):273–287, December 2008. ISSN 0885-6125. doi: 10.1007/s10994-007-5043-5.
    URL [https://doi.org/10.1007/s10994-007-5043-5](https://doi.org/10.1007/s10994-007-5043-5).'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben-David和Borbely[2008] Shai Ben-David和Reba Schuller Borbely.关于任务相关性的概念，提供可证明的多任务学习保证。*机器学习*，73(3):273–287，2008年12月。ISSN
    0885-6125。doi：10.1007/s10994-007-5043-5。URL [https://doi.org/10.1007/s10994-007-5043-5](https://doi.org/10.1007/s10994-007-5043-5)。
- en: 'Bengio et al. [2009] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and
    Jason Weston. Curriculum learning. In *Proceedings of the 26th Annual International
    Conference on Machine Learning*, ICML ’09, page 41–48, New York, NY, USA, 2009.
    Association for Computing Machinery. ISBN 9781605585161. doi: 10.1145/1553374.1553380.
    URL [https://doi.org/10.1145/1553374.1553380](https://doi.org/10.1145/1553374.1553380).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bengio 等 [2009] Yoshua Bengio, Jérôme Louradour, Ronan Collobert 和 Jason Weston.
    课程学习。收录于*第26届国际机器学习大会论文集*，ICML ’09，第 41–48 页，美国纽约，2009年。计算机协会。ISBN 9781605585161。doi:
    10.1145/1553374.1553380。网址 [https://doi.org/10.1145/1553374.1553380](https://doi.org/10.1145/1553374.1553380)。'
- en: Bengio et al. [2013] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating
    or propagating gradients through stochastic neurons for conditional computation.
    *arXiv preprint arXiv:1308.3432*, 2013.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等 [2013] Yoshua Bengio, Nicholas Léonard 和 Aaron Courville. 通过随机神经元估计或传播梯度以进行条件计算。*arXiv
    预印本 arXiv:1308.3432*，2013年。
- en: Bettgenhäuser et al. [2020] Gabriele Bettgenhäuser, Michael A Hedderich, and
    Dietrich Klakow. Learning functions to study the benefit of multitask learning.
    *arXiv preprint arXiv:2006.05561*, 2020.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bettgenhäuser 等 [2020] Gabriele Bettgenhäuser, Michael A Hedderich 和 Dietrich
    Klakow. 学习函数以研究多任务学习的好处。*arXiv 预印本 arXiv:2006.05561*，2020年。
- en: Bingel and Søgaard [2017] Joachim Bingel and Anders Søgaard. Identifying beneficial
    task relations for multi-task learning in deep neural networks. *arXiv preprint
    arXiv:1702.08303*, 2017.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bingel 和 Søgaard [2017] Joachim Bingel 和 Anders Søgaard. 识别深度神经网络中多任务学习的有益任务关系。*arXiv
    预印本 arXiv:1702.08303*，2017年。
- en: 'Bojar et al. [2014] Ondřej Bojar, Christian Buck, Christian Federmann, Barry
    Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post,
    Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleš Tamchyna. Findings of
    the 2014 workshop on statistical machine translation. In *Proceedings of the Ninth
    Workshop on Statistical Machine Translation*, pages 12–58, Baltimore, Maryland,
    USA, June 2014\. Association for Computational Linguistics. doi: 10.3115/v1/W14-3302.
    URL [https://www.aclweb.org/anthology/W14-3302](https://www.aclweb.org/anthology/W14-3302).'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bojar 等 [2014] Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow,
    Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve
    Saint-Amand, Radu Soricut, Lucia Specia 和 Aleš Tamchyna. 2014年统计机器翻译研讨会的发现。收录于*第九届统计机器翻译研讨会论文集*，第
    12–58 页，美国马里兰州巴尔的摩，2014年6月。计算语言学协会。doi: 10.3115/v1/W14-3302。网址 [https://www.aclweb.org/anthology/W14-3302](https://www.aclweb.org/anthology/W14-3302)。'
- en: 'Bowman et al. [2015] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and
    Christopher D. Manning. A large annotated corpus for learning natural language
    inference. In *Proceedings of the 2015 Conference on Empirical Methods in Natural
    Language Processing*, pages 632–642, Lisbon, Portugal, September 2015\. Association
    for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL [https://www.aclweb.org/anthology/D15-1075](https://www.aclweb.org/anthology/D15-1075).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bowman 等 [2015] Samuel R. Bowman, Gabor Angeli, Christopher Potts 和 Christopher
    D. Manning. 用于学习自然语言推理的大型注释语料库。收录于*2015年自然语言处理实证方法会议论文集*，第 632–642 页，葡萄牙里斯本，2015年9月。计算语言学协会。doi:
    10.18653/v1/D15-1075。网址 [https://www.aclweb.org/anthology/D15-1075](https://www.aclweb.org/anthology/D15-1075)。'
- en: 'Bragman et al. [2019] Felix JS Bragman, Ryutaro Tanno, Sebastien Ourselin,
    Daniel C Alexander, and Jorge Cardoso. Stochastic filter groups for multi-task
    cnns: Learning specialist and generalist convolution kernels. In *Proceedings
    of the IEEE International Conference on Computer Vision*, pages 1385–1394, 2019.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bragman 等 [2019] Felix JS Bragman, Ryutaro Tanno, Sebastien Ourselin, Daniel
    C Alexander 和 Jorge Cardoso. 多任务卷积神经网络的随机滤波器组：学习专用和通用卷积核。收录于*IEEE 国际计算机视觉会议论文集*，第
    1385–1394 页，2019年。
- en: 'Bucila et al. [2006] Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil.
    Model compression. In *Proceedings of the 12th ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining*, KDD ’06, page 535–541, New York, NY,
    USA, 2006\. Association for Computing Machinery. ISBN 1595933395. doi: 10.1145/1150402.1150464.
    URL [https://doi.org/10.1145/1150402.1150464](https://doi.org/10.1145/1150402.1150464).'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bucila 等 [2006] Cristian Bucila, Rich Caruana 和 Alexandru Niculescu-Mizil.
    模型压缩。收录于*第12届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集*，KDD ’06，第 535–541 页，美国纽约，2006年。计算机协会。ISBN
    1595933395。doi: 10.1145/1150402.1150464。网址 [https://doi.org/10.1145/1150402.1150464](https://doi.org/10.1145/1150402.1150464)。'
- en: 'Cases et al. [2019] Ignacio Cases, Clemens Rosenbaum, Matthew Riemer, Atticus
    Geiger, Tim Klinger, Alex Tamkin, Olivia Li, Sandhini Agarwal, Joshua D. Greene,
    Dan Jurafsky, Christopher Potts, and Lauri Karttunen. Recursive routing networks:
    Learning to compose modules for language understanding. In *Proceedings of the
    2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    3631–3648, Minneapolis, Minnesota, June 2019\. Association for Computational Linguistics.
    doi: 10.18653/v1/N19-1365. URL [https://www.aclweb.org/anthology/N19-1365](https://www.aclweb.org/anthology/N19-1365).'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cases et al. [2019] Ignacio Cases, Clemens Rosenbaum, Matthew Riemer, Atticus
    Geiger, Tim Klinger, Alex Tamkin, Olivia Li, Sandhini Agarwal, Joshua D. Greene,
    Dan Jurafsky, Christopher Potts, 和 Lauri Karttunen. 递归路由网络: 学习为语言理解组合模块。在 *2019年北美计算语言学学会会议：人类语言技术（长篇和短篇论文）*，第3631–3648页，明尼阿波利斯，明尼苏达州，2019年6月。计算语言学学会。doi:
    10.18653/v1/N19-1365。URL [https://www.aclweb.org/anthology/N19-1365](https://www.aclweb.org/anthology/N19-1365)。'
- en: Chang et al. [2018] Michael B Chang, Abhishek Gupta, Sergey Levine, and Thomas L
    Griffiths. Automatically composing representation transformations as a means for
    generalization. *arXiv preprint arXiv:1807.04640*, 2018.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang et al. [2018] Michael B Chang, Abhishek Gupta, Sergey Levine, 和 Thomas
    L Griffiths. 自动组合表示变换以实现泛化。*arXiv 预印本 arXiv:1807.04640*，2018。
- en: Chaudhry et al. [2018] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach,
    and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. *arXiv preprint
    arXiv:1812.00420*, 2018.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaudhry et al. [2018] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach,
    和 Mohamed Elhoseiny. 高效的终身学习与 a-gem。*arXiv 预印本 arXiv:1812.00420*，2018。
- en: 'Chen and Manning [2014] Danqi Chen and Christopher Manning. A fast and accurate
    dependency parser using neural networks. In *Proceedings of the 2014 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*, pages 740–750, Doha,
    Qatar, October 2014\. Association for Computational Linguistics. doi: 10.3115/v1/D14-1082.
    URL [https://www.aclweb.org/anthology/D14-1082](https://www.aclweb.org/anthology/D14-1082).'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen and Manning [2014] Danqi Chen 和 Christopher Manning. 使用神经网络的快速准确的依赖解析器。在
    *2014年自然语言处理实证方法会议（EMNLP）*，第740–750页，多哈，卡塔尔，2014年10月。计算语言学学会。doi: 10.3115/v1/D14-1082。URL
    [https://www.aclweb.org/anthology/D14-1082](https://www.aclweb.org/anthology/D14-1082)。'
- en: Chen et al. [2018] Junkun Chen, Kaiyu Chen, Xinchi Chen, Xipeng Qiu, and Xuanjing
    Huang. Exploring shared structures and hierarchies for multiple nlp tasks. *arXiv
    preprint arXiv:1808.07658*, 2018.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2018] Junkun Chen, Kaiyu Chen, Xinchi Chen, Xipeng Qiu, 和 Xuanjing
    Huang. 探索多种自然语言处理任务中的共享结构和层级。*arXiv 预印本 arXiv:1808.07658*，2018。
- en: 'Chen et al. [2015] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam,
    Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco captions:
    Data collection and evaluation server. *arXiv preprint arXiv:1504.00325*, 2015.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2015] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam,
    Saurabh Gupta, Piotr Dollár, 和 C Lawrence Zitnick. Microsoft coco captions: 数据收集和评估服务器。*arXiv
    预印本 arXiv:1504.00325*，2015。'
- en: 'Chen et al. [2017] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew
    Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep
    multitask networks, 2017.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2017] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, 和 Andrew Rabinovich.
    Gradnorm: 深度多任务网络中用于自适应损失平衡的梯度归一化, 2017。'
- en: 'Chennupati et al. [2019] Sumanth Chennupati, Ganesh Sistu, Senthil Yogamani,
    and Samir A Rawashdeh. Multinet++: Multi-stream feature aggregation and geometric
    loss strategy for multi-task learning, 2019.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chennupati et al. [2019] Sumanth Chennupati, Ganesh Sistu, Senthil Yogamani,
    和 Samir A Rawashdeh. Multinet++: 多流特征聚合和几何损失策略用于多任务学习, 2019。'
- en: Clark et al. [2019] Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D
    Manning, and Quoc V Le. Bam! born-again multi-task networks for natural language
    understanding. *arXiv preprint arXiv:1907.04829*, 2019.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. [2019] Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher
    D Manning, 和 Quoc V Le. Bam! 重生的多任务网络用于自然语言理解。*arXiv 预印本 arXiv:1907.04829*，2019。
- en: 'Collobert and Weston [2008] Ronan Collobert and Jason Weston. A unified architecture
    for natural language processing: Deep neural networks with multitask learning.
    In *Proceedings of the 25th International Conference on Machine Learning*, ICML
    ’08, page 160–167, New York, NY, USA, 2008\. Association for Computing Machinery.
    ISBN 9781605582054. doi: 10.1145/1390156.1390177. URL [https://doi.org/10.1145/1390156.1390177](https://doi.org/10.1145/1390156.1390177).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Collobert and Weston [2008] Ronan Collobert 和 Jason Weston. 自然语言处理的统一架构: 带有多任务学习的深度神经网络.
    在*第25届国际机器学习大会论文集*，ICML ’08，第160–167页，纽约，NY，USA，2008年。计算机协会。ISBN 9781605582054。doi:
    10.1145/1390156.1390177。网址 [https://doi.org/10.1145/1390156.1390177](https://doi.org/10.1145/1390156.1390177)。'
- en: Collobert et al. [2011] Ronan Collobert, Jason Weston, Léon Bottou, Michael
    Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost)
    from scratch. *Journal of machine learning research*, 12(Aug):2493–2537, 2011.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collobert et al. [2011] Ronan Collobert, Jason Weston, Léon Bottou, Michael
    Karlen, Koray Kavukcuoglu, 和 Pavel Kuksa. 从零开始的自然语言处理. *机器学习研究期刊*，12(Aug):2493–2537，2011年。
- en: Cordts et al. [2016] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,
    Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
    The cityscapes dataset for semantic urban scene understanding. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pages 3213–3223,
    2016.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cordts et al. [2016] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,
    Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, 和 Bernt Schiele.
    用于语义城市场景理解的Cityscapes数据集. 在*IEEE计算机视觉与模式识别会议论文集*，第3213–3223页，2016年。
- en: 'Courbariaux et al. [2015] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
    David. Binaryconnect: Training deep neural networks with binary weights during
    propagations. In *Advances in neural information processing systems*, pages 3123–3131,
    2015.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Courbariaux et al. [2015] Matthieu Courbariaux, Yoshua Bengio, 和 Jean-Pierre
    David. Binaryconnect: 在传播过程中用二进制权重训练深度神经网络. 在*神经信息处理系统进展*，第3123–3131页，2015年。'
- en: Dai et al. [2016] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware semantic
    segmentation via multi-task network cascades. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, pages 3150–3158, 2016.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai et al. [2016] Jifeng Dai, Kaiming He, 和 Jian Sun. 通过多任务网络级联进行实例感知的语义分割.
    在*IEEE计算机视觉与模式识别会议论文集*，第3150–3158页，2016年。
- en: De Vries et al. [2017] Harm De Vries, Florian Strub, Sarath Chandar, Olivier
    Pietquin, Hugo Larochelle, and Aaron Courville. Guesswhat?! visual object discovery
    through multi-modal dialogue. In *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, pages 5503–5512, 2017.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Vries et al. [2017] Harm De Vries, Florian Strub, Sarath Chandar, Olivier
    Pietquin, Hugo Larochelle, 和 Aaron Courville. Guesswhat？！通过多模态对话进行视觉对象发现. 在*IEEE计算机视觉与模式识别会议论文集*，第5503–5512页，2017年。
- en: D’Eramo et al. [2020] Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello
    Restelli, and Jan Peters. Sharing knowledge in multi-task deep reinforcement learning.
    In *International Conference on Learning Representations*, 2020. URL [https://openreview.net/forum?id=rkgpv2VFvr](https://openreview.net/forum?id=rkgpv2VFvr).
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D’Eramo et al. [2020] Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello
    Restelli, 和 Jan Peters. 在多任务深度强化学习中共享知识. 在*国际学习表征会议*，2020年。网址 [https://openreview.net/forum?id=rkgpv2VFvr](https://openreview.net/forum?id=rkgpv2VFvr)。
- en: Devin et al. [2017] Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel,
    and Sergey Levine. Learning modular neural network policies for multi-task and
    multi-robot transfer. In *2017 IEEE International Conference on Robotics and Automation
    (ICRA)*, pages 2169–2176\. IEEE, 2017.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devin et al. [2017] Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel,
    和 Sergey Levine. 学习用于多任务和多机器人迁移的模块化神经网络策略. 在*2017 IEEE国际机器人与自动化会议 (ICRA)*，第2169–2176页。IEEE，2017年。
- en: 'Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    Bert: 用于语言理解的深度双向变换器预训练. *arXiv预印本 arXiv:1810.04805*，2018年。'
- en: Doersch and Zisserman [2017] Carl Doersch and Andrew Zisserman. Multi-task self-supervised
    visual learning. In *Proceedings of the IEEE International Conference on Computer
    Vision*, pages 2051–2060, 2017.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doersch and Zisserman [2017] Carl Doersch 和 Andrew Zisserman. 多任务自监督视觉学习. 在*IEEE国际计算机视觉会议论文集*，第2051–2060页，2017年。
- en: 'Dong et al. [2015] Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang.
    Multi-task learning for multiple language translation. In *Proceedings of the
    53rd Annual Meeting of the Association for Computational Linguistics and the 7th
    International Joint Conference on Natural Language Processing (Volume 1: Long
    Papers)*, pages 1723–1732, Beijing, China, July 2015\. Association for Computational
    Linguistics. doi: 10.3115/v1/P15-1166. URL [https://www.aclweb.org/anthology/P15-1166](https://www.aclweb.org/anthology/P15-1166).'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong 等人 [2015] Daxiang Dong、Hua Wu、Wei He、Dianhai Yu 和 Haifeng Wang。多任务学习用于多语言翻译。在
    *第53届计算语言学协会年会及第7届国际联合自然语言处理会议（第1卷：长文）*，第1723–1732页，北京，中国，2015年7月。计算语言学协会。doi:
    10.3115/v1/P15-1166。网址 [https://www.aclweb.org/anthology/P15-1166](https://www.aclweb.org/anthology/P15-1166)。'
- en: Dréo [2006] Johann Dréo. Pareto front. *Wikipedia Commons*, 2006. URL [https://commons.wikimedia.org/wiki/File:Front_pareto.svg](https://commons.wikimedia.org/wiki/File:Front_pareto.svg).
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dréo [2006] Johann Dréo。帕累托前沿。*维基百科共享资源*，2006。网址 [https://commons.wikimedia.org/wiki/File:Front_pareto.svg](https://commons.wikimedia.org/wiki/File:Front_pareto.svg)。
- en: Du et al. [2018] Yunshu Du, Wojciech M. Czarnecki, Siddhant M. Jayakumar, Razvan
    Pascanu, and Balaji Lakshminarayanan. Adapting auxiliary losses using gradient
    similarity, 2018.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人 [2018] Yunshu Du、Wojciech M. Czarnecki、Siddhant M. Jayakumar、Razvan Pascanu
    和 Balaji Lakshminarayanan。使用梯度相似性调整辅助损失，2018。
- en: 'Duong et al. [2015] Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. Low
    resource dependency parsing: Cross-lingual parameter sharing in a neural network
    parser. In *Proceedings of the 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing
    (Volume 2: Short Papers)*, pages 845–850, Beijing, China, July 2015\. Association
    for Computational Linguistics. doi: 10.3115/v1/P15-2139. URL [https://www.aclweb.org/anthology/P15-2139](https://www.aclweb.org/anthology/P15-2139).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duong 等人 [2015] Long Duong、Trevor Cohn、Steven Bird 和 Paul Cook。低资源依赖解析：神经网络解析器中的跨语言参数共享。在
    *第53届计算语言学协会年会及第7届国际联合自然语言处理会议（第2卷：短文）*，第845–850页，北京，中国，2015年7月。计算语言学协会。doi: 10.3115/v1/P15-2139。网址
    [https://www.aclweb.org/anthology/P15-2139](https://www.aclweb.org/anthology/P15-2139)。'
- en: Dwivedi and Roig [2019] Kshitij Dwivedi and Gemma Roig. Representation similarity
    analysis for efficient task taxonomy & transfer learning. In *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, pages 12387–12396,
    2019.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dwivedi 和 Roig [2019] Kshitij Dwivedi 和 Gemma Roig。高效任务分类与迁移学习的表示相似性分析。在 *IEEE计算机视觉与模式识别会议*，第12387–12396页，2019年。
- en: 'Désidéri [2012] Jean-Antoine Désidéri. Multiple-gradient descent algorithm
    (mgda) for multiobjective optimization. *Comptes Rendus Mathematique*, 350(5):313
    – 318, 2012. ISSN 1631-073X. doi: https://doi.org/10.1016/j.crma.2012.03.014.
    URL [http://www.sciencedirect.com/science/article/pii/S1631073X12000738](http://www.sciencedirect.com/science/article/pii/S1631073X12000738).'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Désidéri [2012] Jean-Antoine Désidéri。多目标优化的多梯度下降算法 (mgda)。*数学汇刊*，350(5)：313
    – 318，2012年。ISSN 1631-073X。doi: https://doi.org/10.1016/j.crma.2012.03.014。网址
    [http://www.sciencedirect.com/science/article/pii/S1631073X12000738](http://www.sciencedirect.com/science/article/pii/S1631073X12000738)。'
- en: 'Espeholt et al. [2018] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan,
    Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning,
    et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner
    architectures. *arXiv preprint arXiv:1802.01561*, 2018.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Espeholt 等人 [2018] Lasse Espeholt、Hubert Soyer、Remi Munos、Karen Simonyan、Volodymir
    Mnih、Tom Ward、Yotam Doron、Vlad Firoiu、Tim Harley、Iain Dunning 等人。Impala：具有重要性加权的演员-学习者架构的可扩展分布式深度强化学习。*arXiv
    预印本 arXiv:1802.01561*，2018。
- en: 'Evgeniou and Pontil [2004] Theodoros Evgeniou and Massimiliano Pontil. Regularized
    multi–task learning. In *Proceedings of the Tenth ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining*, KDD ’04, page 109–117, New York, NY,
    USA, 2004\. Association for Computing Machinery. ISBN 1581138881. doi: 10.1145/1014052.1014067.
    URL [https://doi.org/10.1145/1014052.1014067](https://doi.org/10.1145/1014052.1014067).'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Evgeniou 和 Pontil [2004] Theodoros Evgeniou 和 Massimiliano Pontil。正则化多任务学习。在
    *第十届 ACM SIGKDD 国际知识发现与数据挖掘会议*，KDD ’04，第109–117页，纽约，NY，美国，2004年。计算机协会。ISBN 1581138881。doi:
    10.1145/1014052.1014067。网址 [https://doi.org/10.1145/1014052.1014067](https://doi.org/10.1145/1014052.1014067)。'
- en: 'Fernando et al. [2017] Chrisantha Fernando, Dylan Banarse, Charles Blundell,
    Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet:
    Evolution channels gradient descent in super neural networks. *arXiv preprint
    arXiv:1701.08734*, 2017.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fernando等人[2017] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori
    Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, 和 Daan Wierstra. Pathnet: 在超神经网络中进化引导梯度下降。*arXiv预印本
    arXiv:1701.08734*，2017年。'
- en: 'Frankle and Carbin [2018] Jonathan Frankle and Michael Carbin. The lottery
    ticket hypothesis: Finding sparse, trainable neural networks. *arXiv preprint
    arXiv:1803.03635*, 2018.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle和Carbin[2018] Jonathan Frankle 和 Michael Carbin. 彩票票据假设：发现稀疏的、可训练的神经网络。*arXiv预印本
    arXiv:1803.03635*，2018年。
- en: 'Gao et al. [2019] Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan L Yuille.
    Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative
    dimensionality reduction. In *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, pages 3205–3214, 2019.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao等人[2019] Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, 和 Alan L Yuille. NDDR-CNN:
    通过神经判别降维在多任务CNN中进行逐层特征融合。在*IEEE计算机视觉与模式识别会议论文集*，第3205–3214页，2019年。'
- en: 'Gao et al. [2020] Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, and
    Wei Liu. Mtl-nas: Task-agnostic neural architecture search towards general-purpose
    multi-task learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pages 11543–11552, 2020.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao等人[2020] Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, 和 Wei Liu.
    MTL-NAS: 面向通用多任务学习的任务无关神经架构搜索。在*IEEE/CVF计算机视觉与模式识别会议论文集*，第11543–11552页，2020年。'
- en: Gong et al. [2019] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchintala,
    Suchismita Padhy, Anthony Ndirango, Gokce Keskin, and Oguz H. Elibol. A comparison
    of loss weighting strategies for multi task learning in deep neural networks,
    2019.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong等人[2019] Ting Gong, Tyler Lee, Cory Stephenson, Venkata Renduchintala, Suchismita
    Padhy, Anthony Ndirango, Gokce Keskin, 和 Oguz H. Elibol. 深度神经网络中多任务学习的损失加权策略比较，2019年。
- en: 'Goyal et al. [2017] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra,
    and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding
    in visual question answering. In *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, pages 6904–6913, 2017.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal等人[2017] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, 和 Devi
    Parikh. 让VQA中的V变得重要：提升图像理解在视觉问答中的作用。在*IEEE计算机视觉与模式识别会议论文集*，第6904–6913页，2017年。
- en: Guo et al. [2018] Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and
    Li Fei-Fei. Dynamic task prioritization for multitask learning. In Vittorio Ferrari,
    Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, *Computer Vision
    – ECCV 2018*, pages 282–299, Cham, 2018\. Springer International Publishing. ISBN
    978-3-030-01270-0.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等人[2018] Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, 和 Li Fei-Fei.
    多任务学习的动态任务优先级。在Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, 和 Yair
    Weiss编辑的*计算机视觉 – ECCV 2018*，第282–299页，Cham，2018年。Springer International Publishing。ISBN
    978-3-030-01270-0。
- en: 'Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey
    Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning
    with a stochastic actor. *arXiv preprint arXiv:1801.01290*, 2018.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haarnoja等人[2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, 和 Sergey Levine.
    Soft Actor-Critic：带有随机Actor的离策略最大熵深度强化学习。*arXiv预印本 arXiv:1801.01290*，2018年。
- en: 'Hashimoto et al. [2016] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka,
    and Richard Socher. A joint many-task model: Growing a neural network for multiple
    nlp tasks. *arXiv preprint arXiv:1611.01587*, 2016.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hashimoto等人[2016] Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, 和 Richard
    Socher. 联合多任务模型：为多个NLP任务构建神经网络。*arXiv预印本 arXiv:1611.01587*，2016年。
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2016.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等人[2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 深度残差学习用于图像识别。在*IEEE计算机视觉与模式识别会议
    (CVPR)*，2016年6月。
- en: Heess et al. [2016] Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap,
    Martin Riedmiller, and David Silver. Learning and transfer of modulated locomotor
    controllers. *arXiv preprint arXiv:1610.05182*, 2016.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heess等人[2016] Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin
    Riedmiller, 和 David Silver. 调制运动控制器的学习与迁移。*arXiv预印本 arXiv:1610.05182*，2016年。
- en: Hessel et al. [2018] Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki,
    Simon Schmitt, and Hado van Hasselt. Multi-task deep reinforcement learning with
    popart, 2018.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hessel等人[2018] Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki,
    Simon Schmitt, 和 Hado van Hasselt. 使用PopArt的多任务深度强化学习，2018年。
- en: 'Hospedales et al. [2020] Timothy Hospedales, Antreas Antoniou, Paul Micaelli,
    and Amos Storkey. Meta-learning in neural networks: A survey. *arXiv preprint
    arXiv:2004.05439*, 2020.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hospedales 等人 [2020] Timothy Hospedales、Antreas Antoniou、Paul Micaelli 和 Amos
    Storkey. 神经网络中的元学习：综述。*arXiv 预印本 arXiv:2004.05439*，2020 年。
- en: Hu et al. [2018] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2018.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2018] Jie Hu、Li Shen 和 Gang Sun. 压缩与激励网络。发表于 *IEEE 计算机视觉与模式识别会议论文集 (CVPR)*，2018
    年 6 月。
- en: 'Hudson and Manning [2019] Drew A Hudson and Christopher D Manning. Gqa: A new
    dataset for real-world visual reasoning and compositional question answering.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    pages 6700–6709, 2019.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hudson 和 Manning [2019] Drew A Hudson 和 Christopher D Manning. Gqa: 一个用于现实世界视觉推理和组合问答的新数据集。发表于
    *IEEE 计算机视觉与模式识别会议论文集*，页码 6700–6709，2019 年。'
- en: 'Ioffe and Szegedy [2015] Sergey Ioffe and Christian Szegedy. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. *arXiv
    preprint arXiv:1502.03167*, 2015.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe 和 Szegedy [2015] Sergey Ioffe 和 Christian Szegedy. 批量归一化：通过减少内部协方差偏移加速深度网络训练。*arXiv
    预印本 arXiv:1502.03167*，2015 年。
- en: Jaderberg et al. [2016] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki,
    Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning
    with unsupervised auxiliary tasks. *arXiv preprint arXiv:1611.05397*, 2016.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等人 [2016] Max Jaderberg、Volodymyr Mnih、Wojciech Marian Czarnecki、Tom
    Schaul、Joel Z Leibo、David Silver 和 Koray Kavukcuoglu. 具有无监督辅助任务的强化学习。*arXiv 预印本
    arXiv:1611.05397*，2016 年。
- en: James et al. [2018] Stephen James, Michael Bloesch, and Andrew J Davison. Task-embedded
    control networks for few-shot imitation learning. *arXiv preprint arXiv:1810.03237*,
    2018.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: James 等人 [2018] Stephen James、Michael Bloesch 和 Andrew J Davison. 用于少样本模仿学习的任务嵌入控制网络。*arXiv
    预印本 arXiv:1810.03237*，2018 年。
- en: Jang et al. [2016] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization
    with gumbel-softmax. *arXiv preprint arXiv:1611.01144*, 2016.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang 等人 [2016] Eric Jang、Shixiang Gu 和 Ben Poole. 使用 Gumbel-Softmax 的类别重参数化。*arXiv
    预印本 arXiv:1611.01144*，2016 年。
- en: Jean et al. [2019] Sébastien Jean, Orhan Firat, and Melvin Johnson. Adaptive
    scheduling for multi-task learning. *arXiv preprint arXiv:1909.06434*, 2019.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jean 等人 [2019] Sébastien Jean、Orhan Firat 和 Melvin Johnson. 用于多任务学习的自适应调度。*arXiv
    预印本 arXiv:1909.06434*，2019 年。
- en: Kaiser et al. [2017] Lukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani,
    Niki Parmar, Llion Jones, and Jakob Uszkoreit. One model to learn them all. *arXiv
    preprint arXiv:1706.05137*, 2017.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaiser 等人 [2017] Lukasz Kaiser、Aidan N Gomez、Noam Shazeer、Ashish Vaswani、Niki
    Parmar、Llion Jones 和 Jakob Uszkoreit. 学习所有模型的单一模型。*arXiv 预印本 arXiv:1706.05137*，2017
    年。
- en: Kendall et al. [2017] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task
    learning using uncertainty to weigh losses for scene geometry and semantics, 2017.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall 等人 [2017] Alex Kendall、Yarin Gal 和 Roberto Cipolla. 使用不确定性来加权场景几何和语义的多任务学习，2017
    年。
- en: 'Khot et al. [2018] Tushar Khot, A. Sabharwal, and Peter Clark. Scitail: A textual
    entailment dataset from science question answering. In *AAAI*, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khot 等人 [2018] Tushar Khot、A. Sabharwal 和 Peter Clark. Scitail: 一个来自科学问答的文本蕴涵数据集。发表于
    *AAAI*，2018 年。'
- en: 'Kirsch et al. [2018] Louis Kirsch, Julius Kunze, and David Barber. Modular
    networks: Learning to decompose neural computation. In *Advances in Neural Information
    Processing Systems*, pages 2408–2418, 2018.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirsch 等人 [2018] Louis Kirsch、Julius Kunze 和 David Barber. 模块化网络：学习分解神经计算。发表于
    *神经信息处理系统进展*，第 2408–2418 页，2018 年。
- en: 'Klein and Manning [2003] Dan Klein and Christopher D. Manning. Accurate unlexicalized
    parsing. In *Proceedings of the 41st Annual Meeting on Association for Computational
    Linguistics - Volume 1*, ACL ’03, page 423–430, USA, 2003. Association for Computational
    Linguistics. doi: 10.3115/1075096.1075150. URL [https://doi.org/10.3115/1075096.1075150](https://doi.org/10.3115/1075096.1075150).'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Klein 和 Manning [2003] Dan Klein 和 Christopher D. Manning. 准确的非词汇化解析。发表于 *第
    41 届年度计算语言学协会会议 - 第 1 卷*，ACL ’03，第 423–430 页，美国，2003 年。计算语言学协会。doi: 10.3115/1075096.1075150.
    网址 [https://doi.org/10.3115/1075096.1075150](https://doi.org/10.3115/1075096.1075150)。'
- en: 'Kriegeskorte [2008] Nikolaus Kriegeskorte. Representational similarity analysis
    – connecting the branches of systems neuroscience. *Frontiers in Systems Neuroscience*,
    2008. doi: 10.3389/neuro.06.004.2008. URL [https://doi.org/10.3389/neuro.06.004.2008](https://doi.org/10.3389/neuro.06.004.2008).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kriegeskorte [2008] Nikolaus Kriegeskorte. 表征相似性分析 – 连接系统神经科学的分支。*系统神经科学前沿*，2008
    年。doi: 10.3389/neuro.06.004.2008. 网址 [https://doi.org/10.3389/neuro.06.004.2008](https://doi.org/10.3389/neuro.06.004.2008)。'
- en: 'Krishna et al. [2017] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
    Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A
    Shamma, et al. Visual genome: Connecting language and vision using crowdsourced
    dense image annotations. *International journal of computer vision*, 123(1):32–73,
    2017.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Krishna 等人 [2017] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji
    Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma
    等人。Visual genome: 利用众包的密集图像注释连接语言和视觉。*国际计算机视觉期刊*，123(1):32–73，2017年。'
- en: 'Kulkarni et al. [2016] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi,
    and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal
    abstraction and intrinsic motivation. In *Advances in neural information processing
    systems*, pages 3675–3683, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni 等人 [2016] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi 和 Josh
    Tenenbaum。分层深度强化学习：整合时间抽象和内在动机。见于 *神经信息处理系统进展*，页码 3675–3683，2016年。
- en: Kumar and Daume III [2012] Abhishek Kumar and Hal Daume III. Learning task grouping
    and overlap in multi-task learning. *arXiv preprint arXiv:1206.6417*, 2012.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 和 Daume III [2012] Abhishek Kumar 和 Hal Daume III。多任务学习中的任务分组和重叠学习。*arXiv
    预印本 arXiv:1206.6417*，2012年。
- en: 'Lake et al. [2015] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum.
    Human-level concept learning through probabilistic program induction. *Science*,
    350(6266):1332–1338, 2015. ISSN 0036-8075. doi: 10.1126/science.aab3050. URL [https://science.sciencemag.org/content/350/6266/1332](https://science.sciencemag.org/content/350/6266/1332).'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lake 等人 [2015] Brenden M. Lake, Ruslan Salakhutdinov 和 Joshua B. Tenenbaum。通过概率程序归纳进行人类水平的概念学习。*科学*，350(6266):1332–1338，2015年。ISSN
    0036-8075。doi: 10.1126/science.aab3050。网址 [https://science.sciencemag.org/content/350/6266/1332](https://science.sciencemag.org/content/350/6266/1332)。'
- en: Lan et al. [2019] Lin Lan, Zhenguo Li, Xiaohong Guan, and Pinghui Wang. Meta
    reinforcement learning with task embedding and shared policy. *arXiv preprint
    arXiv:1905.06527*, 2019.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan 等人 [2019] Lin Lan, Zhenguo Li, Xiaohong Guan 和 Pinghui Wang。带有任务嵌入和共享策略的元强化学习。*arXiv
    预印本 arXiv:1905.06527*，2019年。
- en: Lee et al. [2016] Giwoong Lee, Eunho Yang, and Sung Hwang. Asymmetric multi-task
    learning based on task relatedness and loss. In Maria Florina Balcan and Kilian Q.
    Weinberger, editors, *Proceedings of The 33rd International Conference on Machine
    Learning*, volume 48 of *Proceedings of Machine Learning Research*, pages 230–238,
    New York, New York, USA, 20–22 Jun 2016\. PMLR. URL [http://proceedings.mlr.press/v48/leeb16.html](http://proceedings.mlr.press/v48/leeb16.html).
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2016] Giwoong Lee, Eunho Yang 和 Sung Hwang。基于任务相关性和损失的非对称多任务学习。见于 Maria
    Florina Balcan 和 Kilian Q. Weinberger 编辑的 *第33届国际机器学习大会论文集*，*机器学习研究论文集*第48卷，页码
    230–238，美国纽约，2016年6月20–22日。PMLR。网址 [http://proceedings.mlr.press/v48/leeb16.html](http://proceedings.mlr.press/v48/leeb16.html)。
- en: Lee et al. [2018] Hae Beom Lee, Eunho Yang, and Sung Ju Hwang. Deep asymmetric
    multi-task feature learning. In *International Conference on Machine Learning*,
    pages 2956–2964, 2018.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2018] Hae Beom Lee, Eunho Yang 和 Sung Ju Hwang。深度非对称多任务特征学习。见于 *国际机器学习会议*，页码
    2956–2964，2018年。
- en: Li et al. [2016] Changsheng Li, Junchi Yan, Fan Wei, Weishan Dong, Qingshan
    Liu, and Hongyuan Zha. Self-paced multi-task learning. *arXiv preprint arXiv:1604.01474*,
    2016.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2016] Changsheng Li, Junchi Yan, Fan Wei, Weishan Dong, Qingshan Liu
    和 Hongyuan Zha。自适应多任务学习。*arXiv 预印本 arXiv:1604.01474*，2016年。
- en: Liang et al. [2018] Jason Liang, Elliot Meyerson, and Risto Miikkulainen. Evolutionary
    architecture search for deep multitask networks. In *Proceedings of the Genetic
    and Evolutionary Computation Conference*, pages 466–473, 2018.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 [2018] Jason Liang, Elliot Meyerson 和 Risto Miikkulainen。深度多任务网络的进化架构搜索。见于
    *遗传与进化计算会议论文集*，页码 466–473，2018年。
- en: 'Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco:
    Common objects in context. In *European conference on computer vision*, pages
    740–755. Springer, 2014.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro
    Perona, Deva Ramanan, Piotr Dollár, 和 C Lawrence Zitnick。Microsoft coco: 背景中的常见物体。见于
    *欧洲计算机视觉会议*，页码 740–755。Springer，2014年。'
- en: Lin et al. [2017] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
    Piotr Dollár. Focal loss for dense object detection. In *Proceedings of the IEEE
    international conference on computer vision*, pages 2980–2988, 2017.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 [2017] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He 和 Piotr Dollár。用于密集物体检测的焦点损失。见于
    *IEEE国际计算机视觉会议论文集*，页码 2980–2988，2017年。
- en: Lin et al. [2019] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam
    Kwong. Pareto multi-task learning. In *Advances in Neural Information Processing
    Systems*, pages 12060–12070, 2019.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. [2019] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, 和 Sam Kwong.
    帕累托多任务学习。见 *神经信息处理系统进展*，第12060–12070页，2019年。
- en: 'Liu et al. [2018] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable
    architecture search. *arXiv preprint arXiv:1806.09055*, 2018.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2018] Hanxiao Liu, Karen Simonyan, 和 Yiming Yang. Darts：可微分架构搜索。*arXiv预印本
    arXiv:1806.09055*，2018年。
- en: Liu et al. [2016a] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Recurrent neural
    network for text classification with multi-task learning. *arXiv preprint arXiv:1605.05101*,
    2016a.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2016a] Pengfei Liu, Xipeng Qiu, 和 Xuanjing Huang. 使用多任务学习的循环神经网络进行文本分类。*arXiv预印本
    arXiv:1605.05101*，2016a年。
- en: Liu et al. [2016b] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Deep multi-task
    learning with shared memory. *arXiv preprint arXiv:1609.07222*, 2016b.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2016b] Pengfei Liu, Xipeng Qiu, 和 Xuanjing Huang. 使用共享内存的深度多任务学习。*arXiv预印本
    arXiv:1609.07222*，2016b年。
- en: Liu et al. [2017] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task
    learning for text classification. *arXiv preprint arXiv:1704.05742*, 2017.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2017] Pengfei Liu, Xipeng Qiu, 和 Xuanjing Huang. 对抗性多任务学习用于文本分类。*arXiv预印本
    arXiv:1704.05742*，2017年。
- en: Liu et al. [2019] S. Liu, E. Johns, and A. J. Davison. End-to-end multi-task
    learning with attention. In *2019 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*, pages 1871–1880, 2019.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2019] S. Liu, E. Johns, 和 A. J. Davison. 端到端的多任务学习与注意力机制。见 *2019
    IEEE/CVF计算机视觉与模式识别大会（CVPR）*，第1871–1880页，2019年。
- en: Liu et al. [2019a] Shengchao Liu, Yingyu Liang, and Anthony Gitter. Loss-balanced
    task weighting to reduce negative transfer in multi-task learning. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 33, pages 9977–9978,
    2019a.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2019a] Shengchao Liu, Yingyu Liang, 和 Anthony Gitter. 平衡损失的任务加权以减少多任务学习中的负迁移。见
    *AAAI人工智能会议论文集*，第33卷，第9977–9978页，2019a年。
- en: 'Liu et al. [2015a] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin
    Duh, and Ye-yi Wang. Representation learning using multi-task deep neural networks
    for semantic classification and information retrieval. In *Proceedings of the
    2015 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 912–921, Denver, Colorado, May–June
    2015a. Association for Computational Linguistics. doi: 10.3115/v1/N15-1092. URL
    [https://www.aclweb.org/anthology/N15-1092](https://www.aclweb.org/anthology/N15-1092).'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2015a] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin
    Duh, 和 Ye-yi Wang. 使用多任务深度神经网络进行表示学习以实现语义分类和信息检索。见 *2015年北美计算语言学协会年会：人类语言技术会议论文集*，第912–921页，美国科罗拉多州丹佛市，2015年5–6月。计算语言学协会。doi:
    10.3115/v1/N15-1092。网址 [https://www.aclweb.org/anthology/N15-1092](https://www.aclweb.org/anthology/N15-1092)。'
- en: Liu et al. [2019b] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
    Multi-task deep neural networks for natural language understanding. *arXiv preprint
    arXiv:1901.11504*, 2019b.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2019b] Xiaodong Liu, Pengcheng He, Weizhu Chen, 和 Jianfeng Gao.
    用于自然语言理解的多任务深度神经网络。*arXiv预印本 arXiv:1901.11504*，2019b年。
- en: Liu et al. [2019c] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
    Improving multi-task deep neural networks via knowledge distillation for natural
    language understanding. *arXiv preprint arXiv:1904.09482*, 2019c.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2019c] Xiaodong Liu, Pengcheng He, Weizhu Chen, 和 Jianfeng Gao.
    通过知识蒸馏提高多任务深度神经网络用于自然语言理解。*arXiv预印本 arXiv:1904.09482*，2019c年。
- en: Liu et al. [2015b] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep
    learning face attributes in the wild. In *Proceedings of the IEEE international
    conference on computer vision*, pages 3730–3738, 2015b.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2015b] Ziwei Liu, Ping Luo, Xiaogang Wang, 和 Xiaoou Tang. 在实际环境中深度学习面部属性。见
    *IEEE国际计算机视觉会议论文集*，第3730–3738页，2015b年。
- en: Long et al. [2017] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and S Yu Philip.
    Learning multiple tasks with multilinear relationship networks. In *Advances in
    neural information processing systems*, pages 1594–1603, 2017.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. [2017] Mingsheng Long, Zhangjie Cao, Jianmin Wang, 和 S Yu Philip.
    学习具有多线性关系网络的多个任务。见 *神经信息处理系统进展*，第1594–1603页，2017年。
- en: Lopez-Paz and Ranzato [2017] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient
    episodic memory for continual learning. In *Advances in neural information processing
    systems*, pages 6467–6476, 2017.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lopez-Paz 和 Ranzato [2017] David Lopez-Paz 和 Marc’Aurelio Ranzato. 连续学习的梯度情景记忆。见
    *神经信息处理系统进展*，第6467–6476页，2017年。
- en: Lounici et al. [2009] Karim Lounici, Massimiliano Pontil, Alexandre B Tsybakov,
    and Sara Van De Geer. Taking advantage of sparsity in multi-task learning. *arXiv
    preprint arXiv:0903.1468*, 2009.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lounici et al. [2009] Karim Lounici, Massimiliano Pontil, Alexandre B Tsybakov,
    和 Sara Van De Geer. 在多任务学习中利用稀疏性。*arXiv 预印本 arXiv:0903.1468*，2009年。
- en: 'Lu et al. [2020] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh,
    and Stefan Lee. 12-in-1: Multi-task vision and language representation learning.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 10437–10446, 2020.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. [2020] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, 和
    Stefan Lee. 12合1：多任务视觉与语言表示学习。发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，第10437–10446页，2020年。
- en: Lu et al. [2017] Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi,
    and Rogerio Feris. Fully-adaptive feature sharing in multi-task networks with
    applications in person attribute classification. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, pages 5334–5343, 2017.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. [2017] Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi,
    和 Rogerio Feris. 在多任务网络中进行完全自适应特征共享，应用于人物属性分类。发表于*IEEE计算机视觉与模式识别会议论文集*，第5334–5343页，2017年。
- en: Luong et al. [2015] Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals,
    and Lukasz Kaiser. Multi-task sequence to sequence learning. *arXiv preprint arXiv:1511.06114*,
    2015.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luong et al. [2015] Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals,
    和 Lukasz Kaiser. 多任务序列到序列学习。*arXiv 预印本 arXiv:1511.06114*，2015年。
- en: 'Ma et al. [2018] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and
    Ed H. Chi. Modeling task relationships in multi-task learning with multi-gate
    mixture-of-experts. In *Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining*, KDD ’18, page 1930–1939, New York, NY,
    USA, 2018\. Association for Computing Machinery. ISBN 9781450355520. doi: 10.1145/3219819.3220007.
    URL [https://doi.org/10.1145/3219819.3220007](https://doi.org/10.1145/3219819.3220007).'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma et al. [2018] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, 和
    Ed H. Chi. 使用多门控混合专家建模多任务学习中的任务关系。发表于*第24届ACM SIGKDD国际知识发现与数据挖掘大会论文集*，KDD ’18，第1930–1939页，美国纽约，2018年。计算机协会。ISBN
    9781450355520。doi: 10.1145/3219819.3220007。网址 [https://doi.org/10.1145/3219819.3220007](https://doi.org/10.1145/3219819.3220007)。'
- en: 'Mallya et al. [2018] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback:
    Adapting a single network to multiple tasks by learning to mask weights. In *Proceedings
    of the European Conference on Computer Vision (ECCV)*, pages 67–82, 2018.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mallya et al. [2018] Arun Mallya, Dillon Davis, 和 Svetlana Lazebnik. Piggyback：通过学习遮罩权重将单一网络适应于多任务。发表于*欧洲计算机视觉会议（ECCV）论文集*，第67–82页，2018年。
- en: Maninis et al. [2019] Kevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas
    Kokkinos. Attentive single-tasking of multiple tasks. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, pages 1851–1860, 2019.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maninis et al. [2019] Kevis-Kokitsi Maninis, Ilija Radosavovic, 和 Iasonas Kokkinos.
    多任务的注意机制单任务处理。发表于*IEEE计算机视觉与模式识别会议论文集*，第1851–1860页，2019年。
- en: 'Marcus et al. [1993] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
    Building a large annotated corpus of English: The Penn Treebank. *Computational
    Linguistics*, 19(2):313–330, 1993. URL [https://www.aclweb.org/anthology/J93-2004](https://www.aclweb.org/anthology/J93-2004).'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marcus et al. [1993] Mitchell P. Marcus, Beatrice Santorini, 和 Mary Ann Marcinkiewicz.
    建立一个大规模的英语注释语料库：Penn Treebank。*计算语言学*，19(2)：313–330，1993年。网址 [https://www.aclweb.org/anthology/J93-2004](https://www.aclweb.org/anthology/J93-2004)。
- en: 'McCann et al. [2018] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and
    Richard Socher. The natural language decathlon: Multitask learning as question
    answering. *arXiv preprint arXiv:1806.08730*, 2018.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCann et al. [2018] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, 和 Richard
    Socher. 自然语言十项全能：将多任务学习应用于问答。*arXiv 预印本 arXiv:1806.08730*，2018年。
- en: 'Meyerson and Miikkulainen [2017] Elliot Meyerson and Risto Miikkulainen. Beyond
    shared hierarchies: Deep multitask learning through soft layer ordering. *arXiv
    preprint arXiv:1711.00108*, 2017.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meyerson and Miikkulainen [2017] Elliot Meyerson 和 Risto Miikkulainen. 超越共享层次结构：通过软层排序进行深度多任务学习。*arXiv
    预印本 arXiv:1711.00108*，2017年。
- en: Miettinen [1998] Kaisa Miettinen. *Nonlinear multiobjective optimization*, volume 12
    of *International series in operations research and management science*. Kluwer,
    1998. ISBN 978-0-7923-8278-2.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miettinen [1998] Kaisa Miettinen. *非线性多目标优化*，国际系列运筹学与管理科学第12卷。Kluwer，1998年。ISBN
    978-0-7923-8278-2。
- en: Miikkulainen et al. [2019] Risto Miikkulainen, Jason Liang, Elliot Meyerson,
    Aditya Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak
    Navruzyan, Nigel Duffy, et al. Evolving deep neural networks. In *Artificial Intelligence
    in the Age of Neural Networks and Brain Computing*, pages 293–312\. Elsevier,
    2019.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miikkulainen等人[2019] Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya
    Rawal, Daniel Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan,
    Nigel Duffy等人. 进化深度神经网络。在*神经网络与脑计算时代的人工智能*，页293–312。Elsevier，2019年。
- en: Misra et al. [2016] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial
    Hebert. Cross-stitch networks for multi-task learning. In *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, pages 3994–4003,
    2016.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Misra等人[2016] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta和Martial Hebert.
    用于多任务学习的交叉缝合网络。在*IEEE计算机视觉与模式识别会议论文集*，页3994–4003，2016年。
- en: 'Ndirango and Lee [2019] Anthony Ndirango and Tyler Lee. Generalization in multitask
    deep neural classifiers: a statistical physics approach. In *Advances in Neural
    Information Processing Systems*, pages 15862–15871, 2019.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ndirango和Lee[2019] Anthony Ndirango和Tyler Lee. 多任务深度神经分类器中的泛化：一种统计物理方法。在*神经信息处理系统进展*，页15862–15871，2019年。
- en: Newell et al. [2019] Alejandro Newell, Lu Jiang, Chong Wang, Li-Jia Li, and
    Jia Deng. Feature partitioning for efficient multi-task architectures. *arXiv
    preprint arXiv:1908.04339*, 2019.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newell等人[2019] Alejandro Newell, Lu Jiang, Chong Wang, Li-Jia Li和Jia Deng. 高效多任务架构的特征划分。*arXiv预印本
    arXiv:1908.04339*，2019年。
- en: Nguyen and Okatani [2018] Duy-Kien Nguyen and Takayuki Okatani. Improved fusion
    of visual and language representations by dense symmetric co-attention for visual
    question answering. In *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, pages 6087–6096, 2018.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen和Okatani[2018] Duy-Kien Nguyen和Takayuki Okatani. 通过密集对称共同注意力改进视觉和语言表示的融合，用于视觉问答。在*IEEE计算机视觉与模式识别会议论文集*，页6087–6096，2018年。
- en: Nguyen and Okatani [2019] Duy-Kien Nguyen and Takayuki Okatani. Multi-task learning
    of hierarchical vision-language representation. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, pages 10492–10501, 2019.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen和Okatani[2019] Duy-Kien Nguyen和Takayuki Okatani. 分层视觉-语言表示的多任务学习。在*IEEE计算机视觉与模式识别会议论文集*，页10492–10501，2019年。
- en: 'Parisi et al. [2019] German I Parisi, Ronald Kemker, Jose L Part, Christopher
    Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A
    review. *Neural Networks*, 2019.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisi等人[2019] 德国I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan和Stefan
    Wermter. 使用神经网络的持续终身学习：综述。*神经网络*，2019年。
- en: 'Parisotto et al. [2015] Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov.
    Actor-mimic: Deep multitask and transfer reinforcement learning. *arXiv preprint
    arXiv:1511.06342*, 2015.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisotto等人[2015] Emilio Parisotto, Jimmy Lei Ba和Ruslan Salakhutdinov. Actor-mimic：深度多任务和迁移强化学习。*arXiv预印本
    arXiv:1511.06342*，2015年。
- en: Pascal et al. [2020] Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet,
    and Maria A Zuluaga. Maximum roaming multi-task learning. *arXiv preprint arXiv:2006.09762*,
    2020.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pascal等人[2020] Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet和Maria
    A Zuluaga. 最大漫游多任务学习。*arXiv预印本 arXiv:2006.09762*，2020年。
- en: 'Perez et al. [2018] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin,
    and Aaron Courville. Film: Visual reasoning with a general conditioning layer.
    In *Thirty-Second AAAI Conference on Artificial Intelligence*, 2018.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez等人[2018] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin和Aaron
    Courville. Film：具有通用条件层的视觉推理。在*第32届AAAI人工智能会议*，2018年。
- en: 'Pinto and Gupta [2017] Lerrel Pinto and Abhinav Gupta. Learning to push by
    grasping: Using multiple tasks for effective learning. In *2017 IEEE International
    Conference on Robotics and Automation (ICRA)*, pages 2161–2168\. IEEE, 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinto和Gupta[2017] Lerrel Pinto和Abhinav Gupta. 通过抓取学习推动：使用多个任务进行有效学习。在*2017年IEEE国际机器人与自动化会议（ICRA）*，页2161–2168。IEEE，2017年。
- en: 'Plummer et al. [2015] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C
    Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting
    region-to-phrase correspondences for richer image-to-sentence models. In *Proceedings
    of the IEEE international conference on computer vision*, pages 2641–2649, 2015.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plummer等人[2015] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo,
    Julia Hockenmaier和Svetlana Lazebnik. Flickr30k实体：收集区域到短语的对应关系，以丰富图像到句子的模型。在*IEEE国际计算机视觉会议论文集*，页2641–2649，2015年。
- en: 'Pramanik et al. [2019] Subhojeet Pramanik, Priyanka Agrawal, and Aman Hussain.
    Omninet: A unified architecture for multi-modal multi-task learning. *arXiv preprint
    arXiv:1907.07804*, 2019.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pramanik 等人 [2019] Subhojeet Pramanik, Priyanka Agrawal, 和 Aman Hussain. Omninet:
    一种统一的多模态多任务学习架构。*arXiv 预印本 arXiv:1907.07804*，2019 年。'
- en: Ramachandran and Le [2019] Prajit Ramachandran and Quoc V. Le. Diversity and
    depth in per-example routing models. In *International Conference on Learning
    Representations*, 2019. URL [https://openreview.net/forum?id=BkxWJnC9tX](https://openreview.net/forum?id=BkxWJnC9tX).
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandran 和 Le [2019] Prajit Ramachandran 和 Quoc V. Le. 每例路由模型中的多样性和深度。在
    *International Conference on Learning Representations*，2019 年。网址 [https://openreview.net/forum?id=BkxWJnC9tX](https://openreview.net/forum?id=BkxWJnC9tX)。
- en: Rebuffi et al. [2018] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
    Efficient parametrization of multi-domain deep neural networks. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 8119–8127,
    2018.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rebuffi 等人 [2018] Sylvestre-Alvise Rebuffi, Hakan Bilen, 和 Andrea Vedaldi. 多领域深度神经网络的高效参数化。在
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*，第
    8119–8127 页，2018 年。
- en: 'Rosenbaum et al. [2017] Clemens Rosenbaum, Tim Klinger, and Matthew Riemer.
    Routing networks: Adaptive selection of non-linear functions for multi-task learning.
    *arXiv preprint arXiv:1711.01239*, 2017.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosenbaum 等人 [2017] Clemens Rosenbaum, Tim Klinger, 和 Matthew Riemer. 路由网络：为多任务学习自适应选择非线性函数。*arXiv
    预印本 arXiv:1711.01239*，2017 年。
- en: Rosenbaum et al. [2019] Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, and
    Tim Klinger. Routing networks and the challenges of modular and compositional
    computation. *arXiv preprint arXiv:1904.12774*, 2019.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosenbaum 等人 [2019] Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, 和 Tim
    Klinger. 路由网络和模块化及组合计算的挑战。*arXiv 预印本 arXiv:1904.12774*，2019 年。
- en: Ruder [2017] Sebastian Ruder. An overview of multi-task learning in deep neural
    networks. *arXiv preprint arXiv:1706.05098*, 2017.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruder [2017] Sebastian Ruder. 深度神经网络中的多任务学习概述。*arXiv 预印本 arXiv:1706.05098*，2017
    年。
- en: Ruder et al. [2019] Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and
    Anders Søgaard. Latent multi-task architecture learning. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, volume 33, pages 4822–4829, 2019.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruder 等人 [2019] Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, 和 Anders
    Søgaard. 潜在的多任务架构学习。在 *Proceedings of the AAAI Conference on Artificial Intelligence*，第
    33 卷，第 4822–4829 页，2019 年。
- en: Rusu et al. [2015] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre,
    Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray
    Kavukcuoglu, and Raia Hadsell. Policy distillation. *arXiv preprint arXiv:1511.06295*,
    2015.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rusu 等人 [2015] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume
    Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu,
    和 Raia Hadsell. 策略蒸馏。*arXiv 预印本 arXiv:1511.06295*，2015 年。
- en: Sanh et al. [2019] Victor Sanh, Thomas Wolf, and Sebastian Ruder. A hierarchical
    multi-task approach for learning embeddings from semantic tasks. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 33, pages 6949–6956,
    2019.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等人 [2019] Victor Sanh, Thomas Wolf, 和 Sebastian Ruder. 一种层次化的多任务方法，用于从语义任务中学习嵌入。在
    *Proceedings of the AAAI Conference on Artificial Intelligence*，第 33 卷，第 6949–6956
    页，2019 年。
- en: Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv preprint
    arXiv:1707.06347*, 2017.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等人 [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,
    和 Oleg Klimov. 近端策略优化算法。*arXiv 预印本 arXiv:1707.06347*，2017 年。
- en: Sener and Koltun [2018] Ozan Sener and Vladlen Koltun. Multi-task learning as
    multi-objective optimization. In *Advances in Neural Information Processing Systems*,
    pages 527–538, 2018.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sener 和 Koltun [2018] Ozan Sener 和 Vladlen Koltun. 多任务学习作为多目标优化。在 *Advances
    in Neural Information Processing Systems*，第 527–538 页，2018 年。
- en: Sharma et al. [2017] Sahil Sharma, Ashutosh Jha, Parikshit Hegde, and Balaraman
    Ravindran. Learning to multi-task by active sampling. *arXiv preprint arXiv:1702.06053*,
    2017.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma 等人 [2017] Sahil Sharma, Ashutosh Jha, Parikshit Hegde, 和 Balaraman Ravindran.
    通过主动采样学习多任务。*arXiv 预印本 arXiv:1702.06053*，2017 年。
- en: Shui et al. [2019] Changjian Shui, Mahdieh Abbasi, Louis-Émile Robitaille, Boyu
    Wang, and Christian Gagné. A principled approach for learning task similarity
    in multitask learning. *arXiv preprint arXiv:1903.09109*, 2019.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shui 等人 [2019] Changjian Shui, Mahdieh Abbasi, Louis-Émile Robitaille, Boyu
    Wang, 和 Christian Gagné. 一种学习多任务学习中任务相似性的原则性方法。*arXiv 预印本 arXiv:1903.09109*，2019
    年。
- en: Silberman et al. [2012] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
    Fergus. Indoor segmentation and support inference from rgbd images. In *Computer
    Vision – ECCV 2012*, pages 746–760, Berlin, Heidelberg, 2012\. Springer Berlin
    Heidelberg. ISBN 978-3-642-33715-4.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silberman 等人 [2012] Nathan Silberman, Derek Hoiem, Pushmeet Kohli 和 Rob Fergus。从
    rgbd 图像中进行室内分割和支持推断。载于*计算机视觉 – ECCV 2012*，第746–760页，柏林，海德堡，2012年。Springer Berlin
    Heidelberg。ISBN 978-3-642-33715-4。
- en: Sinha et al. [2018] Ayan Sinha, Zhao Chen, Vijay Badrinarayanan, and Andrew
    Rabinovich. Gradient adversarial training of neural networks. *arXiv preprint
    arXiv:1806.08028*, 2018.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinha 等人 [2018] Ayan Sinha, Zhao Chen, Vijay Badrinarayanan 和 Andrew Rabinovich。神经网络的梯度对抗训练。*arXiv
    预印本 arXiv:1806.08028*，2018年。
- en: 'Søgaard and Goldberg [2016] Anders Søgaard and Yoav Goldberg. Deep multi-task
    learning with low level tasks supervised at lower layers. In *Proceedings of the
    54th Annual Meeting of the Association for Computational Linguistics (Volume 2:
    Short Papers)*, pages 231–235, Berlin, Germany, August 2016\. Association for
    Computational Linguistics. doi: 10.18653/v1/P16-2038. URL [https://www.aclweb.org/anthology/P16-2038](https://www.aclweb.org/anthology/P16-2038).'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Søgaard 和 Goldberg [2016] Anders Søgaard 和 Yoav Goldberg。通过在较低层进行低级任务的监督进行深度多任务学习。载于*第54届计算语言学协会年会论文集（第2卷：短论文）*，第231–235页，德国柏林，2016年8月。计算语言学协会。doi:
    10.18653/v1/P16-2038。网址 [https://www.aclweb.org/anthology/P16-2038](https://www.aclweb.org/anthology/P16-2038)。'
- en: Song et al. [2019] Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen, and Mingli
    Song. Deep model transferability from attribution maps. In *Advances in Neural
    Information Processing Systems*, pages 6182–6192, 2019.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 [2019] Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen 和 Mingli Song。从归因图中深度模型的可迁移性。载于*神经信息处理系统进展*，第6182–6192页，2019年。
- en: 'Srivastava et al. [2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
    Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural
    networks from overfitting. *Journal of Machine Learning Research*, 15(56):1929–1958,
    2014. URL [http://jmlr.org/papers/v15/srivastava14a.html](http://jmlr.org/papers/v15/srivastava14a.html).'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Srivastava 等人 [2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
    Sutskever 和 Ruslan Salakhutdinov。Dropout: 防止神经网络过拟合的简单方法。*机器学习研究期刊*，15(56):1929–1958，2014年。网址
    [http://jmlr.org/papers/v15/srivastava14a.html](http://jmlr.org/papers/v15/srivastava14a.html)。'
- en: Standley et al. [2019] Trevor Standley, Amir R Zamir, Dawn Chen, Leonidas Guibas,
    Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in
    multi-task learning? *arXiv preprint arXiv:1905.07553*, 2019.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Standley 等人 [2019] Trevor Standley, Amir R Zamir, Dawn Chen, Leonidas Guibas,
    Jitendra Malik 和 Silvio Savarese。多任务学习中哪些任务应一起学习？*arXiv 预印本 arXiv:1905.07553*，2019年。
- en: Strezoski et al. [2019a] Gjorgji Strezoski, Nanne van Noord, and Marcel Worring.
    Many task learning with task routing. In *Proceedings of the IEEE International
    Conference on Computer Vision*, pages 1375–1384, 2019a.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strezoski 等人 [2019a] Gjorgji Strezoski, Nanne van Noord 和 Marcel Worring。具有任务路由的多任务学习。载于*IEEE
    国际计算机视觉大会论文集*，第1375–1384页，2019年。
- en: Strezoski et al. [2019b] Gjorgji Strezoski, Nanne van Noord, and Marcel Worring.
    Learning task relatedness in multi-task learning for images in context. In *Proceedings
    of the 2019 on International Conference on Multimedia Retrieval*, pages 78–86,
    2019b.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strezoski 等人 [2019b] Gjorgji Strezoski, Nanne van Noord 和 Marcel Worring。在上下文中学习图像的任务相关性。载于*2019
    年国际多媒体检索会议论文集*，第78–86页，2019年。
- en: Sukhbaatar et al. [2015] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
    End-to-end memory networks. In *Advances in neural information processing systems*,
    pages 2440–2448, 2015.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sukhbaatar 等人 [2015] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus 等。端到端记忆网络。载于*神经信息处理系统进展*，第2440–2448页，2015年。
- en: Sun et al. [2019a] Tianxiang Sun, Yunfan Shao, Xiaonan Li, Pengfei Liu, Hang
    Yan, Xipeng Qiu, and Xuanjing Huang. Learning sparse sharing architectures for
    multiple tasks. *arXiv preprint arXiv:1911.05034*, 2019a.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2019a] Tianxiang Sun, Yunfan Shao, Xiaonan Li, Pengfei Liu, Hang Yan,
    Xipeng Qiu 和 Xuanjing Huang。学习稀疏共享架构以应对多个任务。*arXiv 预印本 arXiv:1911.05034*，2019年。
- en: 'Sun et al. [2019b] Ximeng Sun, Rameswar Panda, and Rogerio Feris. Adashare:
    Learning what to share for efficient deep multi-task learning. *arXiv preprint
    arXiv:1911.12423*, 2019b.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人 [2019b] Ximeng Sun, Rameswar Panda 和 Rogerio Feris。Adashare: 学习共享内容以提高深度多任务学习的效率。*arXiv
    预印本 arXiv:1911.12423*，2019年。'
- en: Sutskever et al. [2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence
    to sequence learning with neural networks. In *Advances in neural information
    processing systems*, pages 3104–3112, 2014.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等人 [2014] Ilya Sutskever, Oriol Vinyals 和 Quoc V Le。使用神经网络的序列到序列学习。载于*神经信息处理系统进展*，第3104–3112页，2014年。
- en: 'Teh et al. [2017] Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James
    Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust
    multitask reinforcement learning. In *Advances in Neural Information Processing
    Systems*, pages 4496–4506, 2017.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Teh et al. [2017] 叶·特、维克多·巴普斯特、沃伊切赫·M·查尔内茨基、约翰·全、詹姆斯·柯克帕特里克、雷亚·哈德塞尔、尼古拉斯·赫斯和拉兹万·帕斯卡努。Distral：稳健的多任务强化学习。在
    *神经信息处理系统进展* 中，第4496–4506页，2017年。
- en: van Hasselt et al. [2016] Hado P van Hasselt, Arthur Guez, Matteo Hessel, Volodymyr
    Mnih, and David Silver. Learning values across many orders of magnitude. In *Advances
    in Neural Information Processing Systems*, pages 4287–4295, 2016.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Hasselt et al. [2016] 哈多·P·范·哈斯尔特、亚瑟·格兹、马泰奥·赫瑟尔、弗拉基米尔·穆尼赫和大卫·银。学习跨越多个数量级的值。在
    *神经信息处理系统进展* 中，第4287–4295页，2016年。
- en: 'Vandenhende et al. [2019] Simon Vandenhende, Stamatios Georgoulis, Bert De Brabandere,
    and Luc Van Gool. Branched multi-task networks: deciding what layers to share.
    *arXiv preprint arXiv:1904.02920*, 2019.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vandenhende et al. [2019] 西蒙·范登亨德、斯塔马蒂奥斯·乔治奥利斯、伯特·德·布拉班德雷和卢克·范·戈尔。分支多任务网络：决定共享哪些层。*arXiv
    预印本 arXiv:1904.02920*，2019年。
- en: 'Vandenhende et al. [2020] Simon Vandenhende, Stamatios Georgoulis, and Luc
    Van Gool. Mti-net: Multi-scale task interaction networks for multi-task learning.
    *arXiv preprint arXiv:2001.06902*, 2020.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vandenhende et al. [2020] 西蒙·范登亨德、斯塔马蒂奥斯·乔治奥利斯和卢克·范·戈尔。Mti-net：用于多任务学习的多尺度任务交互网络。*arXiv
    预印本 arXiv:2001.06902*，2020年。
- en: 'Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding. *arXiv preprint arXiv:1804.07461*, 2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2018] 亚历克斯·王、阿曼普里特·辛格、朱利安·迈克尔、费利克斯·希尔、奥默·利维和塞缪尔·R·鲍曼。Glue：自然语言理解的多任务基准和分析平台。*arXiv
    预印本 arXiv:1804.07461*，2018年。
- en: Weischedel et al. [2013] Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard
    Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Zue, Ann Taylor, Jeff Kaufman, Michelle
    Franchini, Mohammed El-Bachouti, Robert Belvin, and Ann Houston. Ontonotes release
    5.0. *Linguistic Data Consortium*, 2013.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weischedel et al. [2013] 拉尔夫·韦斯切德尔、玛莎·帕尔默、米切尔·马库斯、爱德华·霍维、萨米尔·普拉丹、兰斯·拉姆肖、阮文祖、安·泰勒、杰夫·考夫曼、米歇尔·弗兰基尼、穆罕默德·埃尔-巴赫提和罗伯特·贝尔文。Ontonotes
    5.0 版本。*语言数据联盟*，2013年。
- en: 'Williams [1992] Ronald J. Williams. Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. *Machine Learning*, 8(3-4):229–256,
    May 1992. doi: 10.1007/bf00992696. URL [https://doi.org/10.1007/bf00992696](https://doi.org/10.1007/bf00992696).'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Williams [1992] 罗纳德·J·威廉姆斯。连接主义强化学习的简单统计梯度跟踪算法。*机器学习*，8(3-4)：229–256，1992年5月。doi:
    10.1007/bf00992696。网址 [https://doi.org/10.1007/bf00992696](https://doi.org/10.1007/bf00992696)。'
- en: Wong and Gesmundo [2017] Catherine Wong and Andrea Gesmundo. Transfer learning
    to learn with multitask neural model search. *arXiv preprint arXiv:1710.10776*,
    2017.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong 和 Gesmundo [2017] 凯瑟琳·黄和安德里亚·盖斯蒙多。通过多任务神经模型搜索进行迁移学习。*arXiv 预印本 arXiv:1710.10776*，2017年。
- en: Wu et al. [2020] Sen Wu, Hongyang R Zhang, and Christopher Ré. Understanding
    and improving information transfer in multi-task learning. *arXiv preprint arXiv:2005.00944*,
    2020.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2020] 森·吴、洪阳·R·张和克里斯托弗·瑞。理解和改善多任务学习中的信息传递。*arXiv 预印本 arXiv:2005.00944*，2020年。
- en: 'Xu et al. [2018a] Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe. Pad-net:
    Multi-tasks guided prediction-and-distillation network for simultaneous depth
    estimation and scene parsing. In *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, pages 675–684, 2018a.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2018a] 邓·徐、万里·欧阳、肖岗·王和尼库·塞贝。Pad-net：用于深度估计和场景解析的多任务引导预测和蒸馏网络。在 *IEEE
    计算机视觉与模式识别会议论文集* 中，第675–684页，2018年。
- en: Xu et al. [2018b] Yichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu, and Jianfeng
    Gao. Multi-task learning with sample re-weighting for machine reading comprehension,
    2018b.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2018b] 逸聪·徐、肖冬·刘、叶龙·沈、静静·刘和剑锋·高。用于机器阅读理解的多任务学习与样本重加权，2018年。
- en: Yang et al. [2020] Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task
    reinforcement learning with soft modularization. *arXiv preprint arXiv:2003.13661*,
    2020.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2020] 瑞汉·杨、华哲·徐、易武和肖龙·王。带有软模块化的多任务强化学习。*arXiv 预印本 arXiv:2003.13661*，2020年。
- en: 'Yang and Hospedales [2016a] Yongxin Yang and Timothy Hospedales. Deep multi-task
    representation learning: A tensor factorisation approach. *arXiv preprint arXiv:1605.06391*,
    2016a.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 和 Hospedales [2016a] 永新·杨和蒂莫西·霍斯佩戴斯。深度多任务表示学习：一种张量分解方法。*arXiv 预印本 arXiv:1605.06391*，2016年。
- en: Yang and Hospedales [2016b] Yongxin Yang and Timothy M Hospedales. Trace norm
    regularised deep multi-task learning. *arXiv preprint arXiv:1606.04038*, 2016b.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 和 Hospedales [2016b] Yongxin Yang 和 Timothy M Hospedales. 跟踪范数正则化的深度多任务学习。*arXiv预印本
    arXiv:1606.04038*，2016b年。
- en: 'Young et al. [2014] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
    From image descriptions to visual denotations: New similarity metrics for semantic
    inference over event descriptions. *Transactions of the Association for Computational
    Linguistics*, 2:67–78, 2014. doi: 10.1162/tacl_a_00166. URL [https://www.aclweb.org/anthology/Q14-1006](https://www.aclweb.org/anthology/Q14-1006).'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Young et al. [2014] Peter Young, Alice Lai, Micah Hodosh, 和 Julia Hockenmaier.
    从图像描述到视觉指称：事件描述的语义推断新相似性度量。*计算语言学协会会刊*，2:67–78，2014年。doi: 10.1162/tacl_a_00166。网址
    [https://www.aclweb.org/anthology/Q14-1006](https://www.aclweb.org/anthology/Q14-1006)。'
- en: 'Yu et al. [2019] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol
    Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation
    for multi-task and meta reinforcement learning. *arXiv preprint arXiv:1910.10897*,
    2019.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. [2019] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol
    Hausman, Chelsea Finn, 和 Sergey Levine. Meta-world：多任务和元强化学习的基准和评估。*arXiv预印本 arXiv:1910.10897*，2019年。
- en: Yu et al. [2020] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol
    Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. *arXiv preprint
    arXiv:2001.06782*, 2020.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. [2020] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol
    Hausman, 和 Chelsea Finn. 多任务学习的梯度手术。*arXiv预印本 arXiv:2001.06782*，2020年。
- en: 'Zamir et al. [2018] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas,
    Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pages 3712–3722, 2018.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zamir et al. [2018] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas,
    Jitendra Malik, 和 Silvio Savarese. Taskonomy：任务迁移学习的解缠。见于*IEEE计算机视觉与模式识别会议论文集*，第3712–3722页，2018年。
- en: Zeng et al. [2018] Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto
    Rodriguez, and Thomas Funkhouser. Learning synergies between pushing and grasping
    with self-supervised deep reinforcement learning. In *2018 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, pages 4238–4245\. IEEE,
    2018.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng et al. [2018] Andy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto
    Rodriguez, 和 Thomas Funkhouser. 使用自监督深度强化学习学习推拉动作之间的协同效应。见于*2018 IEEE/RSJ国际智能机器人与系统会议（IROS）*，第4238–4245页。IEEE，2018年。
- en: Zhang et al. [2020] Amy Zhang, Shagun Sodhani, Khimya Khetarpal, and Joelle
    Pineau. Multi-task reinforcement learning as a hidden-parameter block mdp. *arXiv
    preprint arXiv:2007.07206*, 2020.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2020] Amy Zhang, Shagun Sodhani, Khimya Khetarpal, 和 Joelle Pineau.
    将多任务强化学习视为隐藏参数块MDP。*arXiv预印本 arXiv:2007.07206*，2020年。
- en: Zhang [2015] Yu Zhang. Multi-task learning and algorithmic stability. In *Proceedings
    of the Twenty-Ninth AAAI Conference on Artificial Intelligence*, pages 3181–3187,
    2015.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang [2015] Yu Zhang. 多任务学习与算法稳定性。见于*第二十九届AAAI人工智能会议论文集*，第3181–3187页，2015年。
- en: Zhang and Yang [2017] Yu Zhang and Qiang Yang. A survey on multi-task learning.
    *arXiv preprint arXiv:1707.08114*, 2017.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Yang [2017] Yu Zhang 和 Qiang Yang. 关于多任务学习的调查。*arXiv预印本 arXiv:1707.08114*，2017年。
- en: Zhang et al. [2014] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang.
    Facial landmark detection by deep multi-task learning. In David Fleet, Tomas Pajdla,
    Bernt Schiele, and Tinne Tuytelaars, editors, *Computer Vision – ECCV 2014*, pages
    94–108, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10599-4.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2014] Zhanpeng Zhang, Ping Luo, Chen Change Loy, 和 Xiaoou Tang.
    通过深度多任务学习进行面部标志点检测。见于David Fleet, Tomas Pajdla, Bernt Schiele, 和 Tinne Tuytelaars（编辑），*计算机视觉
    – ECCV 2014*，第94–108页，Cham，2014年。Springer国际出版社。ISBN 978-3-319-10599-4。
- en: Zhang et al. [2019] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe,
    and Jian Yang. Pattern-affinitive propagation across depth, surface normal and
    semantic segmentation. In *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, pages 4106–4115, 2019.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2019] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe,
    和 Jian Yang. 跨深度、表面法线和语义分割的模式亲和传播。见于*IEEE计算机视觉与模式识别会议论文集*，第4106–4115页，2019年。
- en: Zhao et al. [2018] Xiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang,
    and Ying Wu. A modulation module for multi-task learning with applications in
    image retrieval. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and
    Yair Weiss, editors, *Computer Vision – ECCV 2018*, pages 415–432, Cham, 2018\.
    Springer International Publishing. ISBN 978-3-030-01246-5.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等[2018] 赵向云、李浩翔、沈小辉、梁晓丹和吴颖。在Vittorio Ferrari、Martial Hebert、Cristian Sminchisescu和Yair
    Weiss主编的*计算机视觉 – ECCV 2018*中，第415–432页，Cham，2018年。Springer国际出版公司。ISBN 978-3-030-01246-5。
- en: Zheng et al. [2018] Feng Zheng, Cheng Deng, Xing Sun, Xinyang Jiang, Xiaowei
    Guo, Zongqiao Yu, Feiyue Huang, and Rongrong Ji. Pyramidal person re-identification
    via multi-loss dynamic training, 2018.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等[2018] 郑丰、邓城、孙兴、姜新阳、郭晓伟、余宗桥、黄飞跃和季荣荣。通过多损失动态训练的金字塔式行人重识别，2018年。
- en: Zhuang et al. [2019] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun
    Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning.
    *arXiv preprint arXiv:1911.02685*, 2019.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang等[2019] 庄福贞、齐智远、段科宇、席东博、朱永春、朱恒述、熊晖和何青。关于迁移学习的全面综述。*arXiv预印本 arXiv:1911.02685*，2019年。
- en: Zoph and Le [2016] Barret Zoph and Quoc V Le. Neural architecture search with
    reinforcement learning. *arXiv preprint arXiv:1611.01578*, 2016.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoph和Le [2016] Barret Zoph和Quoc V Le。使用强化学习进行神经网络架构搜索。*arXiv预印本 arXiv:1611.01578*，2016年。
