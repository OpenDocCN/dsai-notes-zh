- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:07:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1810.07862] Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1810.07862] 深度强化学习在通信与网络中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1810.07862](https://ar5iv.labs.arxiv.org/html/1810.07862)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1810.07862](https://ar5iv.labs.arxiv.org/html/1810.07862)
- en: 'Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习在通信与网络中的应用：综述
- en: 'Nguyen Cong Luong, Dinh Thai Hoang, Member, IEEE, Shimin Gong, Member, IEEE,
    Dusit Niyato, Fellow, IEEE, Ping Wang, Senior Member, IEEE, Ying-Chang Liang,
    Fellow, IEEE, Dong In Kim, Senior Member, IEEE N. C. Luong and D. Niyato are with
    School of Computer Science and Engineering, Nanyang Technological University,
    Singapore. E-mails: clnguyen@ntu.edu.sg, dniyato@ntu.edu.sg.D. T. Hoang is with
    the Faculty of Engineering and Information Technology, University of Technology
    Sydney, Australia. E-mail: hoang.dinh@uts.edu.au.S. Gong is with the Shenzhen
    Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055,
    China. E-mail: sm.gong@siat.ac.cn.P. Wang is with Department of Electrical Engineering
    & Computer Science, York University, Canada. E-mail: pingw@yorku.ca.Y.-C. Liang
    is with Center for Intelligent Networking and Communications (CINC), with University
    of Electronic Science and Technology of China, Chengdu, China. E-mail: liangyc@ieee.org.D. I. Kim
    is with School of Information and Communication Engineering, Sungkyunkwan University,
    Korea. Email: dikim@skku.ac.kr.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 阮公龙、丁泰晃、IEEE会员、龚世民、IEEE会员、杜斯特·尼亚托、IEEE院士、王平、IEEE高级会员、梁颖昌、IEEE院士、金东仁、IEEE高级会员
    N. C. Luong 和 D. Niyato 隶属于新加坡南洋理工大学计算机科学与工程学院。电子邮件：clnguyen@ntu.edu.sg, dniyato@ntu.edu.sg。D.
    T. Hoang 隶属于澳大利亚悉尼科技大学工程与信息技术学院。电子邮件：hoang.dinh@uts.edu.au。S. Gong 隶属于中国科学院深圳先进技术研究院，中国深圳
    518055。电子邮件：sm.gong@siat.ac.cn。P. Wang 隶属于加拿大约克大学电气工程与计算机科学系。电子邮件：pingw@yorku.ca。Y.-C.
    Liang 隶属于中国电子科技大学智能网络与通信中心，成都，中国。电子邮件：liangyc@ieee.org。D. I. Kim 隶属于韩国成均馆大学信息与通信工程学院。电子邮件：dikim@skku.ac.kr。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper presents a comprehensive literature review on applications of deep
    reinforcement learning in communications and networking. Modern networks, e.g.,
    Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become more
    decentralized and autonomous. In such networks, network entities need to make
    decisions locally to maximize the network performance under uncertainty of network
    environment. Reinforcement learning has been efficiently used to enable the network
    entities to obtain the optimal policy including, e.g., decisions or actions, given
    their states when the state and action spaces are small. However, in complex and
    large-scale networks, the state and action spaces are usually large, and the reinforcement
    learning may not be able to find the optimal policy in reasonable time. Therefore,
    deep reinforcement learning, a combination of reinforcement learning with deep
    learning, has been developed to overcome the shortcomings. In this survey, we
    first give a tutorial of deep reinforcement learning from fundamental concepts
    to advanced models. Then, we review deep reinforcement learning approaches proposed
    to address emerging issues in communications and networking. The issues include
    dynamic network access, data rate control, wireless caching, data offloading,
    network security, and connectivity preservation which are all important to next
    generation networks such as 5G and beyond. Furthermore, we present applications
    of deep reinforcement learning for traffic routing, resource sharing, and data
    collection. Finally, we highlight important challenges, open issues, and future
    research directions of applying deep reinforcement learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对深度强化学习在通信和网络中的应用进行了全面的文献综述。现代网络，如物联网（IoT）和无人机（UAV）网络，变得更加去中心化和自主。在这样的网络中，网络实体需要在网络环境不确定的情况下，局部做出决策以最大化网络性能。强化学习已被有效应用于使网络实体在状态和动作空间较小时，获取包括决策或行动在内的最优策略。然而，在复杂的大规模网络中，状态和动作空间通常较大，强化学习可能无法在合理时间内找到最优策略。因此，深度强化学习作为强化学习与深度学习的结合，已被开发以克服这些不足。在这项调查中，我们首先从基础概念到高级模型，对深度强化学习进行了一次教程。然后，我们回顾了为解决通信和网络中新兴问题而提出的深度强化学习方法。这些问题包括动态网络接入、数据速率控制、无线缓存、数据卸载、网络安全和连接性保护，这些都是
    5G 及其之后的下一代网络中非常重要的。进一步地，我们展示了深度强化学习在流量路由、资源共享和数据收集中的应用。最后，我们突出了应用深度强化学习的重要挑战、未解问题和未来的研究方向。
- en: Keywords- Deep reinforcement learning, deep Q-learning, networking, communications,
    spectrum access, rate control, security, caching, data offloading, data collection.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词 - 深度强化学习，深度 Q 学习，网络，通信，频谱接入，速率控制，安全性，缓存，数据卸载，数据收集。
- en: I Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Reinforcement learning [[1](#bib.bib1)] is one of the most important research
    directions of machine learning which has significant impacts to the development
    of Artificial Intelligence (AI) over the last 20 years. Reinforcement learning
    is a learning process in which an agent can periodically make decisions, observe
    the results, and then automatically adjust its strategy to achieve the optimal
    policy. However, this learning process, even though proved to converge, takes
    a lot of time to reach the best policy as it has to explore and gain knowledge
    of an entire system, making it unsuitable and inapplicable to large-scale networks.
    Consequently, applications of reinforcement learning are very limited in practice.
    Recently, deep learning [[2](#bib.bib2)] has been introduced as a new breakthrough
    technique. It can overcome the limitations of reinforcement learning, and thus
    open a new era for the development of reinforcement learning, namely *Deep Reinforcement
    Learning* (DRL). The DRL embraces the advantage of Deep Neural Networks (DNNs)
    to train the learning process, thereby improving the learning speed and the performance
    of reinforcement learning algorithms. As a result, DRL has been adopted in a numerous
    applications of reinforcement learning in practice such as robotics, computer
    vision, speech recognition, and natural language processing [[2](#bib.bib2)].
    One of the most famous applications of DRL is AlphaGo [[3](#bib.bib3)], the first
    computer program which can beat a human professional without handicaps on a full-sized
    19$\times$19 board.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习[[1](#bib.bib1)]是机器学习中最重要的研究方向之一，对过去20年人工智能（AI）的发展产生了重大影响。强化学习是一个学习过程，其中一个智能体可以定期做出决策，观察结果，然后自动调整其策略以实现**最佳策略**。然而，尽管这个学习过程已被证明会收敛，但由于必须探索并了解整个系统，因此需要大量时间才能达到**最佳策略**，使其不适用于大规模网络。因此，强化学习在实际应用中的应用非常有限。最近，深度学习[[2](#bib.bib2)]被引入作为一种新的突破性技术。它可以克服强化学习的局限性，从而为强化学习的发展开启了一个新的时代，即*深度强化学习*（DRL）。DRL利用深度神经网络（DNNs）的优势来训练学习过程，从而提高强化学习算法的学习速度和性能。因此，DRL已被广泛应用于实际中的各种强化学习应用，如机器人技术、计算机视觉、语音识别和自然语言处理[[2](#bib.bib2)]。DRL最著名的应用之一是AlphaGo[[3](#bib.bib3)]，这是第一个能够在19$\times$19的棋盘上击败没有任何让步的人类职业选手的计算机程序。
- en: 'In the areas of communications and networking, DRL has been recently used as
    an emerging tool to effectively address various problems and challenges. In particular,
    modern networks such as Internet of Things (IoT), Heterogeneous Networks (HetNets),
    and Unmanned Aerial Vehicle (UAV) network become more decentralized, ad-hoc, and
    autonomous in nature. Network entities such as IoT devices, mobile users, and
    UAVs need to make local and autonomous decisions, e.g., spectrum access, data
    rate selection, transmit power control, and base station association, to achieve
    the goals of different networks including, e.g., throughput maximization and energy
    consumption minimization. Under uncertain and stochastic environments, most of
    the decision-making problems can be modeled by a so-called Markov Decision Process
    (MDP) [[4](#bib.bib4)]. Dynamic programming [[5](#bib.bib5)], [[6](#bib.bib6)]
    and other algorithms such as value iteration, as well as reinforcement learning
    techniques can be adopted to solve the MDP. However, the modern networks are large-scale
    and complicated, and thus the computational complexity of the techniques rapidly
    becomes unmanageable. As a result, DRL has been developing to be an alternative
    solution to overcome the challenge. In general, the DRL approaches provide the
    following advantages:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在通信和网络领域，DRL最近被作为一种新兴工具来有效解决各种问题和挑战。特别是，现代网络如物联网（IoT）、异构网络（HetNets）和无人机（UAV）网络变得更加去中心化、临时性和自主性。网络实体如物联网设备、移动用户和无人机需要做出本地和自主的决策，例如频谱访问、数据速率选择、发射功率控制和基站关联，以实现不同网络的目标，包括例如吞吐量最大化和能耗最小化。在不确定和随机的环境下，大多数决策问题可以通过所谓的马尔可夫决策过程（MDP）[[4](#bib.bib4)]来建模。动态规划[[5](#bib.bib5)]、[[6](#bib.bib6)]和其他算法如价值迭代，以及强化学习技术可以用于解决MDP。然而，现代网络规模庞大且复杂，因此这些技术的计算复杂度迅速变得难以管理。因此，DRL正在发展成为克服这一挑战的替代解决方案。一般而言，DRL方法提供了以下优点：
- en: •
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DRL can obtain the solution of sophisticated network optimizations. Thus, it
    enables network controllers, e.g., base stations, in modern networks to solve
    non-convex and complex problems, e.g., joint user association, computation, and
    transmission schedule, to achieve the optimal solutions without complete and accurate
    network information.
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DRL可以获得复杂网络优化的解决方案。因此，它使现代网络中的网络控制器，例如基站，能够解决非凸和复杂的问题，例如联合用户关联、计算和传输调度，从而在没有完整和准确的网络信息的情况下实现最佳解决方案。
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DRL allows network entities to learn and build knowledge about the communication
    and networking environment. Thus, by using DRL, the network entities, e.g., a
    mobile user, can learn optimal policies, e.g., base station selection, channel
    selection, handover decision, caching and offloading decisions, without knowing
    channel model and mobility pattern.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DRL允许网络实体学习并建立关于通信和网络环境的知识。因此，通过使用DRL，网络实体，例如移动用户，可以学习最佳策略，例如基站选择、频道选择、切换决策、缓存和卸载决策，而无需了解频道模型和移动模式。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DRL provides autonomous decision-making. With the DRL approaches, network entities
    can make observation and obtain the best policy locally with minimum or without
    information exchange among each other. This not only reduces communication overheads
    but also improves security and robustness of the networks.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DRL提供自主决策。通过DRL方法，网络实体可以进行观察，并在最小或没有信息交换的情况下，在本地获得最佳策略。这不仅减少了通信开销，还提高了网络的安全性和鲁棒性。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DRL improves significantly the learning speed, especially in the problems with
    large state and action spaces. Thus, in large-scale networks, e.g., IoT systems
    with thousands of devices, DRL allows network controller or IoT gateways to control
    dynamically user association, spectrum access, and transmit power for a massive
    number of IoT devices and mobile users.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DRL显著提高了学习速度，特别是在状态和动作空间较大的问题中。因此，在大规模网络中，例如具有数千个设备的物联网系统，DRL允许网络控制器或物联网网关动态控制用户关联、频谱接入和大量物联网设备及移动用户的发射功率。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Several other problems in communications and networking such as cyber-physical
    attacks, interference management, and data offloading can be modeled as games,
    e.g., the non-cooperative game. DRL has been recently used as an efficient tool
    to solve the games, e.g., finding the Nash equilibrium, without the complete information.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在通信和网络中的其他几个问题，例如网络物理攻击、干扰管理和数据卸载，可以被建模为博弈，如非合作博弈。DRL最近被用作解决这些博弈的有效工具，例如，找到纳什均衡，而无需完整的信息。
- en: 'Although there are some surveys related to DRL, they do not focus on communications
    and networking. For example, the surveys of applications of DRL for computer vision
    and natural language processing can be found in [[7](#bib.bib7)] and[[8](#bib.bib8)].
    Also, there are surveys related to the use of only “deep learning” for networking.
    For example, the survey of machine learning for wireless networks is given in
    [[9](#bib.bib9)], but it does not focus on the DRL approaches. To the best of
    our knowledge, there is no survey specifically discussing the applications of
    DRL in communications and networking. This motivates us to deliver the survey
    with the tutorial of DRL and the comprehensive literature review on the applications
    of DRL to address issues in communications and networking. For convenience, the
    related works in this survey are classified based on issues in communications
    and networking as shown in Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Applications
    of Deep Reinforcement Learning in Communications and Networking: A Survey"). The
    major issues include network access, data rate control, wireless caching, data
    offloading, network security, connectivity preservation, traffic routing, and
    data collection. Also, the percentages of DRL related works for different networks
    and different issues in the networks are shown in Figs. [1](#S1.F1 "Figure 1 ‣
    I Introduction ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")(a) and [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Applications
    of Deep Reinforcement Learning in Communications and Networking: A Survey")(b),
    respectively. From the figures, we observe that the majority of the related works
    are for the cellular networks. Also, the related works to the wireless caching
    and offloading have received more attention than the other issues.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有一些与 DRL 相关的调查，但它们并未专注于通信和网络。例如，关于 DRL 在计算机视觉和自然语言处理中的应用的调查可以在 [[7](#bib.bib7)]
    和 [[8](#bib.bib8)] 中找到。同时，也有仅使用“深度学习”进行网络研究的调查。例如，关于无线网络机器学习的调查见 [[9](#bib.bib9)]，但它并未关注
    DRL 方法。据我们所知，目前尚无专门讨论 DRL 在通信和网络中应用的调查。这促使我们进行 DRL 教程和 DRL 在通信和网络中应用的综合文献综述的调查。为方便起见，本调查中的相关工作根据通信和网络中的问题进行分类，如图
    [2](#S1.F2 "图 2 ‣ I 引言 ‣ 深度强化学习在通信和网络中的应用：综述") 所示。主要问题包括网络接入、数据速率控制、无线缓存、数据卸载、网络安全、连接性保护、流量路由和数据收集。此外，DRL
    相关工作在不同网络和不同问题中的百分比分别如图 [1](#S1.F1 "图 1 ‣ I 引言 ‣ 深度强化学习在通信和网络中的应用：综述")(a) 和 [1](#S1.F1
    "图 1 ‣ I 引言 ‣ 深度强化学习在通信和网络中的应用：综述")(b) 所示。从图中可以看出，大多数相关工作集中在蜂窝网络上。同时，与无线缓存和卸载相关的工作比其他问题获得了更多关注。
- en: '![Refer to caption](img/d251dbc4d76c342dc33ff3f912b9a01f.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d251dbc4d76c342dc33ff3f912b9a01f.png)'
- en: (a)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/fdaef3e72e3b30684c37d74841d9a0a5.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fdaef3e72e3b30684c37d74841d9a0a5.png)'
- en: (b)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 1: Percentages of related work for (a) different networks and (b) different
    issues in the networks.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：不同网络的相关工作的百分比（a）和网络中不同问题的百分比（b）。
- en: 'TABLE I: List of abbreviations'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：缩略词列表
- en: '| Abbreviation | Description |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 缩略词 | 描述 |'
- en: '| ANN/APF | Artificial Neural Network/Artificial Potential Field |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| ANN/APF | 人工神经网络/人工势场 |'
- en: '| A3C | Asynchronous Advantage Actor- Critic |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| A3C | 异步优势演员-评论家 |'
- en: '| CRN | Cognitive Radio Network |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| CRN | 认知无线电网络 |'
- en: '| CNN | Convolutional Neural Network |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 卷积神经网络 |'
- en: '| DRL/DQL | Deep Reinforcement Learning/Deep Q-Learning |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| DRL/DQL | 深度强化学习/深度 Q 学习 |'
- en: '| DNN | Deep Neural Network |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| DNN | 深度神经网络 |'
- en: '| DQN/DDQN/DRQN | Deep Q-Network/Double DQN/Deep Recurrent Q-Learning |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| DQN/DDQN/DRQN | 深度 Q 网络/双重 DQN/深度递归 Q 学习 |'
- en: '| DASH | Dynamic Adaptive Streaming over HTTP |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| DASH | 动态自适应 HTTP 流媒体 |'
- en: '| DoS | Denial-of-Service |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| DoS | 拒绝服务 |'
- en: '| ESN | Echo State Network |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| ESN | 回声状态网络 |'
- en: '| FNN/RNN | Feedforward Neural Network/Recurrent Neural Network |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| FNN/RNN | 前馈神经网络/递归神经网络 |'
- en: '| FSMC | Finite-State Markov Channel |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| FSMC | 有限状态马尔可夫通道 |'
- en: '| HVFT | High Volume Flexible Time |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| HVFT | 高容量灵活时间 |'
- en: '| ITS | Intelligent Transportation System |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| ITS | 智能交通系统 |'
- en: '| LSM/LSTM | Liquid State Machine/Long Short-Term Memory |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| LSM/LSTM | 液体状态机/长短期记忆 |'
- en: '| MEC | Mobile Edge Computing |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| MEC | 移动边缘计算 |'
- en: '| MDP/POMDP | Markov Decision Process/Partially Observable MDP |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| MDP/POMDP | 马尔可夫决策过程/部分可观察马尔可夫决策过程 |'
- en: '| NFSP | Neural Fictitious Self-Play |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| NFSP | 神经虚拟自博弈 |'
- en: '| NFV | Network Function Virtualization |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| NFV | 网络功能虚拟化 |'
- en: '| RDPG | Recurrent Deterministic Policy Gradient |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| RDPG | 递归确定性策略梯度 |'
- en: '| RCNN | Recursive Convolutional Neural Network |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| RCNN | 递归卷积神经网络 |'
- en: '| RRH/BBU | Remote Radio Head/BaseBand Unit |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| RRH/BBU | 遥远无线电头/基带单元 |'
- en: '| RSSI | Received Signal Strength Indicators |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| RSSI | 接收信号强度指示器 |'
- en: '| SPD | Sequential Prisoner’s Dilemma |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| SPD | 顺序囚徒困境 |'
- en: '| SBS/BS | Small Base Station/Base Station |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| SBS/BS | 小型基站/基站 |'
- en: '| SDN | Software-Defined Network |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| SDN | 软件定义网络 |'
- en: '| SU/PU | Secondary User/Primary User |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| SU/PU | 次级用户/主用户 |'
- en: '| UDN/UAN | Ultra-Density Network/Underwater Acoustic Network |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| UDN/UAN | 超密集网络/水下声学网络 |'
- en: '| UAV | Unmanned Aerial Vehicle |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| UAV | 无人驾驶飞行器 |'
- en: '| VANET/V2V | Vehicular Ad hoc Network/Vehicle-to-Vehicle |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| VANET/V2V | 车载自组织网络/车辆到车辆 |'
- en: '![Refer to caption](img/feb9df8e689a1c909c8df987c4aee146.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/feb9df8e689a1c909c8df987c4aee146.png)'
- en: 'Figure 2: A taxonomy of the applications of deep reinforcement learning for
    communications and networking.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：深度强化学习在通信和网络中的应用分类。
- en: 'The rest of this paper is organized as follows. Section [II](#S2 "II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey") presents the introduction of reinforcement learning
    and discusses DRL techniques as well as their extensions. Section [III](#S3 "III
    Network Access and Rate Control ‣ Applications of Deep Reinforcement Learning
    in Communications and Networking: A Survey") reviews the applications of DRL for
    dynamic network access and adaptive data rate control. Section [IV](#S4 "IV Caching
    and Offloading ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey") discusses the applications of DRL for wireless caching
    and data offloading. Section [V](#S5 "V Network Security and Connectivity Preservation
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey") presents DRL related works for network security and connectivity preservation.
    Section [VI](#S6 "VI Miscellaneous Issues ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey") considers how to use DRL
    to deal with other issues in communications and networking. Important challenges,
    open issues, and future research directions are outlined in Section [VII](#S7
    "VII Challenges, Open Issues, and Future Research Directions ‣ Applications of
    Deep Reinforcement Learning in Communications and Networking: A Survey"). Section [VIII](#S8
    "VIII Conclusions ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey") concludes the paper. The list of abbreviations commonly
    appeared in this paper is given in Table [I](#S1.T1 "TABLE I ‣ I Introduction
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey"). Note that DRL consists of two different algorithms which are Deep
    Q-Learning (DQL) and policy gradients [[10](#bib.bib10)]. In particular, DQL is
    mostly used for the DRL related works. Therefore, in the rest of the paper, we
    use “DRL” and “DQL” interchangeably to refer to the DRL algorithms.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。第[II](#S2 "II 深度强化学习：概述 ‣ 深度强化学习在通信和网络中的应用：综述")节介绍了强化学习，并讨论了DRL技术及其扩展。第[III](#S3
    "III 网络访问和速率控制 ‣ 深度强化学习在通信和网络中的应用：综述")节回顾了DRL在动态网络访问和自适应数据速率控制中的应用。第[IV](#S4 "IV
    缓存和卸载 ‣ 深度强化学习在通信和网络中的应用：综述")节讨论了DRL在无线缓存和数据卸载中的应用。第[V](#S5 "V 网络安全和连接保持 ‣ 深度强化学习在通信和网络中的应用：综述")节介绍了与网络安全和连接保持相关的DRL工作。第[VI](#S6
    "VI 其他问题 ‣ 深度强化学习在通信和网络中的应用：综述")节考虑了如何利用DRL处理通信和网络中的其他问题。第[VII](#S7 "VII 挑战、开放问题和未来研究方向
    ‣ 深度强化学习在通信和网络中的应用：综述")节概述了重要挑战、开放问题和未来研究方向。第[VIII](#S8 "VIII 结论 ‣ 深度强化学习在通信和网络中的应用：综述")节总结了本文内容。表[I](#S1.T1
    "TABLE I ‣ I 介绍 ‣ 深度强化学习在通信和网络中的应用：综述")中列出了本文常见的缩略语。请注意，DRL包含两种不同的算法，即深度Q学习（DQL）和策略梯度[[10](#bib.bib10)]。特别地，DQL主要用于与DRL相关的工作。因此，在本文剩余部分，我们将“DRL”和“DQL”交替使用，以指代DRL算法。
- en: 'II Deep Reinforcement Learning: An Overview'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 深度强化学习：概述
- en: In this section, we first present fundamental knowledge of Markov decision processes,
    reinforcement learning, and deep learning techniques which are important branches
    of machine learning theory. We then discuss DRL technique that can capitalize
    on the capability of the deep learning to improve efficiency and performance in
    terms of the learning rate for reinforcement learning algorithms. Afterward, advanced
    DRL models and their extensions are reviewed.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先介绍马尔可夫决策过程、强化学习和深度学习技术的基本知识，这些都是机器学习理论的重要分支。接着，我们讨论能够利用深度学习能力来提高强化学习算法学习效率和性能的深度强化学习技术（DRL）。随后，我们回顾了高级DRL模型及其扩展。
- en: II-A Markov Decision Processes
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 马尔可夫决策过程
- en: MDP [[4](#bib.bib4)] is a discrete time stochastic control process. MDP provides
    a mathematical framework for modeling decision-making problems in which outcomes
    are partly random and under control of a decision maker or an agent. MDPs are
    useful for studying optimization problems which can be solved by dynamic programming
    and reinforcement learning techniques. Typically, an MDP is defined by a tuple
    $(\mathcal{S},\mathcal{A},p,r)$ where $\mathcal{S}$ is a finite set of states,
    $\mathcal{A}$ is a finite set of actions, $p$ is a transition probability from
    state $s$ to state $s^{\prime}$ after action $a$ is executed, and $r$ is the immediate
    reward obtained after action $a$ is performed. We denote $\pi$ as a “policy” which
    is a mapping from a state to an action. The goal of an MDP is to find an optimal
    policy to maximize the reward function. An MDP can be finite or infinite time
    horizon. For an infinite time horizon MDP, we aim to find an optimal policy $\pi^{*}$
    to maximize the expected total reward defined by $\overset{\infty}{\underset{t=0}{\sum}}\gamma
    r_{t}(s_{t},a_{t})$, where $a_{t}=\pi^{*}(s_{t})$, and $\gamma\in[0,1]$ is the
    discount factor.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: MDP [[4](#bib.bib4)] 是一种离散时间随机控制过程。MDP提供了一个数学框架，用于建模决策问题，其中结果既有随机性又在决策者或代理的控制之下。MDP对于研究可以通过动态规划和强化学习技术解决的优化问题非常有用。通常，MDP由元组$(\mathcal{S},\mathcal{A},p,r)$定义，其中$\mathcal{S}$是状态的有限集合，$\mathcal{A}$是动作的有限集合，$p$是动作$a$执行后从状态$s$到状态$s^{\prime}$的转移概率，$r$是执行动作$a$后获得的即时奖励。我们用$\pi$表示“策略”，即从状态映射到动作。MDP的目标是找到一个最优策略，以最大化奖励函数。MDP可以是有限时间或无限时间范围。对于无限时间范围的MDP，我们的目标是找到一个最优策略$\pi^{*}$，以最大化期望的总奖励，该奖励由$\overset{\infty}{\underset{t=0}{\sum}}\gamma
    r_{t}(s_{t},a_{t})$定义，其中$a_{t}=\pi^{*}(s_{t})$，$\gamma\in[0,1]$是折扣因子。
- en: II-A1 Partially Observable Markov Decision Process
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 部分可观察马尔可夫决策过程
- en: In MDPs, we assume that the system state is fully observable by the agent. However,
    in many cases, the agent only can observe a part of the system state, and thus
    Partially Observable Markov Decision Processes (POMDPs) [[11](#bib.bib11)] can
    be used to model the decision-making problems. A typical POMDP model is defined
    by a 6-tuple $(\mathcal{S},\mathcal{A},p,r,\Omega,\mathcal{O})$, where $\mathcal{S},\mathcal{A},p,r$
    are defined the same as in the MDP model, $\Omega$ and $\mathcal{O}$ are defined
    as the set of observations and observation probabilities, respectively. At each
    time step, the system is at state $s\in\mathcal{S}$. Then, the agent takes an
    action $a\in\mathcal{A}$ and the system transits to state $s^{\prime}\in\mathcal{S}$.
    At the same time, the agent has a new observation $o\in\Omega$ with probability
    $\mathcal{O}(o|s,a,s^{\prime})$. Finally, the agent receives an immediate reward
    $r$ that is equal to $r(s,a)$ in the MDP. Similar to the MDP model, the agent
    in POMDP also aims to find the optimal policy $\pi^{*}$ in order to maximize its
    expected long-term discounted reward $\overset{\infty}{\underset{t=0}{\sum}}\gamma
    r_{t}(s_{t},\pi^{*}(s_{t}))$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDP中，我们假设系统状态是完全可观察的。然而，在许多情况下，代理只能观察系统状态的一部分，因此可以使用部分可观察马尔可夫决策过程（POMDPs）[[11](#bib.bib11)]来建模决策问题。一个典型的POMDP模型由6元组$(\mathcal{S},\mathcal{A},p,r,\Omega,\mathcal{O})$定义，其中$\mathcal{S},\mathcal{A},p,r$的定义与MDP模型中相同，$\Omega$和$\mathcal{O}$分别定义为观察集合和观察概率。每个时间步，系统处于状态$s\in\mathcal{S}$。然后，代理采取一个动作$a\in\mathcal{A}$，系统转移到状态$s^{\prime}\in\mathcal{S}$。与此同时，代理以概率$\mathcal{O}(o|s,a,s^{\prime})$获得一个新的观察$o\in\Omega$。最后，代理获得一个即时奖励$r$，该奖励等于MDP中的$r(s,a)$。类似于MDP模型，POMDP中的代理也旨在找到最佳策略$\pi^{*}$，以最大化其期望的长期折扣奖励$\overset{\infty}{\underset{t=0}{\sum}}\gamma
    r_{t}(s_{t},\pi^{*}(s_{t}))$。
- en: II-A2 Markov Games
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 马尔可夫博弈
- en: In game theory, a Markov game, or a stochastic game [[12](#bib.bib12)], is a
    dynamic game with probabilistic transitions played by multiple players, i.e.,
    agents. A typical Markov game model is defined by a tuple $(\mathcal{I},\mathcal{S},\{\mathcal{A}^{i}\}_{i\in\mathcal{I}},p,\{r^{i}\}_{i\in\mathcal{I}})$,
    where
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在博弈论中，马尔可夫博弈或随机博弈[[12](#bib.bib12)]是一个由多个玩家即代理参与的具有概率转移的动态博弈。一个典型的马尔可夫博弈模型由一个元组$(\mathcal{I},\mathcal{S},\{\mathcal{A}^{i}\}_{i\in\mathcal{I}},p,\{r^{i}\}_{i\in\mathcal{I}})$定义，其中
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathcal{I}\triangleq\{1,\ldots,i,\ldots,I\}$ is a set of agents,
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathcal{I}\triangleq\{1,\ldots,i,\ldots,I\}$ 是代理集合，
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathcal{S}\triangleq\{\mathcal{S}^{1},\ldots,\mathcal{S}^{i},\ldots,\mathcal{S}^{I}\}$
    is the global state space of the all agents with $\mathcal{S}^{i}$ being the state
    space of agent $i$,
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathcal{S}\triangleq\{\mathcal{S}^{1},\ldots,\mathcal{S}^{i},\ldots,\mathcal{S}^{I}\}$
    是所有代理的全局状态空间，其中$\mathcal{S}^{i}$是代理$i$的状态空间，
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\{\mathcal{A}^{i}\}_{i\in\mathcal{I}}$ are sets of action spaces of the agents
    with $\mathcal{A}^{i}$ being the action space of agent $i$,
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\{\mathcal{A}^{i}\}_{i\in\mathcal{I}}$ 是代理的动作空间集合，其中$\mathcal{A}^{i}$是代理$i$的动作空间。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $p\triangleq\mathcal{S}\times\mathcal{A}^{1}\times\cdots\times\mathcal{A}^{I}\rightarrow[0,1]$
    is the transition probability function of the system.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $p\triangleq\mathcal{S}\times\mathcal{A}^{1}\times\cdots\times\mathcal{A}^{I}\rightarrow[0,1]$
    是系统的转移概率函数。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\{r^{i}\}_{i\in\mathcal{I}}$ are payoff functions of the agents with
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\{r^{i}\}_{i\in\mathcal{I}}$ 是代理的收益函数，
- en: $r^{i}\triangleq\mathcal{S}\times\mathcal{A}^{1}\times\cdots\times\mathcal{A}^{I}\rightarrow\mathbb{R}$,
    i.e., the payoff of agent $i$ obtained after all actions of the agents are executed.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $r^{i}\triangleq\mathcal{S}\times\mathcal{A}^{1}\times\cdots\times\mathcal{A}^{I}\rightarrow\mathbb{R}$，即代理$i$在所有代理的动作执行后获得的收益。
- en: In a Markov game, the agents start at some initial state $s_{0}\in\mathcal{S}$.
    After observing the current state, all the agents simultaneously select their
    actions $a=\{a^{1},\ldots,a^{I}\}$ and they will receive their corresponding rewards
    together with their own new observations. At the same time, the system will transit
    to a new state $s^{\prime}\in\mathcal{S}$ with probability $p(s^{\prime}|s,a)$.
    The procedure is repeated at the new state and continues for a finite or infinite
    number of stages. In this game, all the agents try to find their optimal policies
    to maximize their own expected long-term average rewards, i.e., $\overset{\infty}{\underset{t=0}{\sum}}\gamma_{i}r^{i}_{t}(s_{t},\pi_{i}^{*}(s_{t}))$,
    $\forall i$. The set of all optimal policies of this game, i.e., $\{\pi^{*}_{1},\ldots,\pi^{*}_{I}\}$
    is known to be the equilibrium of this game. If there is a finite number of players
    and the sets of states and actions are finite, then the Markov game always has
    a Nash equilibrium [[13](#bib.bib13)] under a finite number of stages. The same
    is true for Markov games with infinite stages, but the total payoff of agents
    is the discounted sum [[13](#bib.bib13)].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在马尔可夫博弈中，代理从某个初始状态$s_{0}\in\mathcal{S}$开始。观察当前状态后，所有代理同时选择他们的动作$a=\{a^{1},\ldots,a^{I}\}$，并将获得相应的奖励和各自的新的观察。同时，系统将以概率$p(s^{\prime}|s,a)$转移到新的状态$s^{\prime}\in\mathcal{S}$。该过程在新状态下重复进行，并持续进行有限或无限数量的阶段。在此博弈中，所有代理尝试找到其*最优策略*以最大化其预期的长期平均奖励，即$\overset{\infty}{\underset{t=0}{\sum}}\gamma_{i}r^{i}_{t}(s_{t},\pi_{i}^{*}(s_{t}))$，$\forall
    i$。此博弈的所有最优策略集合，即$\{\pi^{*}_{1},\ldots,\pi^{*}_{I}\}$，被称为此博弈的*均衡*。如果玩家数量有限且状态和动作集合都是有限的，则马尔可夫博弈在有限阶段内总是具有纳什均衡[[13](#bib.bib13)]。对于具有无限阶段的马尔可夫博弈也是如此，但代理的总收益是折扣总和[[13](#bib.bib13)]。
- en: II-B Reinforcement Learning
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 强化学习
- en: 'Reinforcement learning, an important branch of machine learning, is an effective
    tool and widely used in the literature to address MDPs [[1](#bib.bib1)]. In a
    reinforcement learning process, an agent can learn its optimal policy through
    interaction with its environment. In particular, the agent first observes its
    current state, and then takes an action, and receives its immediate reward together
    with its new state as illustrated in Fig. [3](#S2.F3 "Figure 3 ‣ II-B Reinforcement
    Learning ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep
    Reinforcement Learning in Communications and Networking: A Survey")(a). The observed
    information, i.e., the immediate reward and new state, is used to adjust the agent’s
    policy, and this process will be repeated until the agent’s policy approaches
    to the optimal policy. In reinforcement learning, $Q$-learning is the most effective
    method and widely used in the literature. In the following, we will discuss the
    $Q$-learning algorithm and its extensions for advanced MDP models.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '强化学习是机器学习的一个重要分支，是解决MDP问题的有效工具，并在文献中广泛使用 [[1](#bib.bib1)]。在强化学习过程中，智能体通过与环境的互动来学习其最优策略。具体来说，智能体首先观察其当前状态，然后采取一个行动，并接收到即时奖励和新状态，如图
    [3](#S2.F3 "Figure 3 ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning:
    An Overview ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey")(a) 所示。观察到的信息，即即时奖励和新状态，被用来调整智能体的策略，这个过程会一直重复，直到智能体的策略接近最优策略。在强化学习中，$Q$-学习是最有效且在文献中广泛使用的方法。接下来，我们将讨论
    $Q$-学习算法及其在高级MDP模型中的扩展。'
- en: <math   alttext="\begin{array}[]{ccc}\epsfbox{RL}\hfil&amp;\epsfbox{ANN}\hfil&amp;\epsfbox{DeepQLearning}\hfil\\
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <math   alttext="\begin{array}[]{ccc}\epsfbox{RL}\hfil&amp;\epsfbox{ANN}\hfil&amp;\epsfbox{DeepQLearning}\hfil\\
- en: \text{(a)}&amp;\text{(b)}&amp;\text{(c)}\end{array}" display="inline"><semantics
    ><mtable columnspacing="5pt" rowspacing="0pt" ><mtr  ><mtd ><mtext >(a)</mtext></mtd><mtd
    ><mtext  >(b)</mtext></mtd><mtd ><mtext >(c)</mtext></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow  ><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><ci ><mtext  >(a)</mtext></ci><ci ><mtext >(b)</mtext></ci><ci ><mtext  >(c)</mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{ccc}\epsfbox{RL}\hfil&\epsfbox{ANN}\hfil&\epsfbox{DeepQLearning}\hfil\\
    \text{(a)}&\text{(b)}&\text{(c)}\end{array}</annotation></semantics></math>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \text{(a)}&amp;\text{(b)}&amp;\text{(c)}\end{array}" display="inline"><semantics
    ><mtable columnspacing="5pt" rowspacing="0pt" ><mtr  ><mtd ><mtext >(a)</mtext></mtd><mtd
    ><mtext  >(b)</mtext></mtd><mtd ><mtext >(c)</mtext></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix ><matrixrow  ><cerror ><csymbol cd="ambiguous"
    >missing-subexpression</csymbol></cerror><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
    ><csymbol cd="ambiguous" >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><ci ><mtext  >(a)</mtext></ci><ci ><mtext >(b)</mtext></ci><ci ><mtext  >(c)</mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{ccc}\epsfbox{RL}\hfil&\epsfbox{ANN}\hfil&\epsfbox{DeepQLearning}\hfil\\
    \text{(a)}&\text{(b)}&\text{(c)}\end{array}</annotation></semantics></math>
- en: 'Figure 3: (a) Reinforcement learning, (b) Artificial neural network, and (c)
    Deep Q-learning.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图3： (a) 强化学习，(b) 人工神经网络，和 (c) 深度 Q-学习。
- en: II-B1 $Q$-Learning Algorithm
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 $Q$-学习算法
- en: 'In an MDP, we aim to find an optimal policy $\pi^{*}:\mathcal{S}\rightarrow\mathcal{A}$
    for the agent to minimize the overall cost for the system. Accordingly, we first
    define value function $\mathcal{V}^{\pi}:\mathcal{S}\rightarrow\mathbb{R}$ that
    represents the expected value obtained by following policy $\pi$ from each state
    $s\in\mathcal{S}$. The value function $\mathcal{V}$ for policy $\pi$ quantifies
    the goodness of the policy through an infinite horizon and discounted MDP that
    can be expressed as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDP（马尔可夫决策过程）中，我们的目标是为智能体找到一个最优策略 $\pi^{*}:\mathcal{S}\rightarrow\mathcal{A}$，以最小化系统的整体成本。因此，我们首先定义价值函数
    $\mathcal{V}^{\pi}:\mathcal{S}\rightarrow\mathbb{R}$，它表示从每个状态 $s\in\mathcal{S}$
    开始，遵循策略 $\pi$ 所获得的期望值。策略 $\pi$ 的价值函数 $\mathcal{V}$ 通过无限视野和折扣的MDP来量化策略的优劣，其表达式如下：
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)$ | $\displaystyle=\mathbb{E}_{\pi}\Big{[}\sum_{t=0}^{\infty}\gamma
    r_{t}(s_{t},a_{t})&#124;s_{0}=s\Big{]}$ |  | (1) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{V}^{\pi}(s)$ | $\displaystyle=\mathbb{E}_{\pi}\Big{[}\sum_{t=0}^{\infty}\gamma
    r_{t}(s_{t},a_{t})&#124;s_{0}=s\Big{]}$ |  | (1) |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\pi}\Big{[}r_{t}(s_{t},a_{t})+\gamma\mathcal{V}^{\pi}(s_{t+1})&#124;s_{0}=s\Big{]}.$
    |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\pi}\Big{[}r_{t}(s_{t},a_{t})+\gamma\mathcal{V}^{\pi}(s_{t+1})&#124;s_{0}=s\Big{]}.$
    |  |'
- en: Since we aim to find the optimal policy $\pi^{*}$, an optimal action at each
    state can be found through the optimal value function expressed by $\mathcal{V}^{*}(s)=\underset{a_{t}}{\max}\Big{\{}\mathbb{E}_{\pi}\big{[}r_{t}(s_{t},a_{t})+\gamma\mathcal{V}^{\pi}(s_{t+1})\big{]}\Big{\}}$
    .
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的目标是找到最优策略 $\pi^{*}$，因此可以通过最优值函数 $\mathcal{V}^{*}(s)=\underset{a_{t}}{\max}\Big{\{}\mathbb{E}_{\pi}\big{[}r_{t}(s_{t},a_{t})+\gamma\mathcal{V}^{\pi}(s_{t+1})\big{]}\Big{\}}$
    来找到每个状态下的最优动作。
- en: 'If we denote $\mathcal{Q}^{*}(s,a)\triangleq r_{t}(s_{t},a_{t})+\gamma\mathbb{E}_{\pi}\big{[}\mathcal{V}^{\pi}(s_{t+1})\big{]}$
    as the optimal $Q$-function for all state-action pairs, then the optimal value
    function can be written by $\mathcal{V}^{*}(s)=\underset{a}{\max}\big{\{}\mathcal{Q}^{*}(s,a)\big{\}}$.
    Now, the problem is reduced to find optimal values of $Q$-function, i.e., $\mathcal{Q}^{*}(s,a)$,
    for all state-action pairs, and this can be done through iterative processes.
    In particular, the $Q$-function is updated according to the following rule:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 $\mathcal{Q}^{*}(s,a)\triangleq r_{t}(s_{t},a_{t})+\gamma\mathbb{E}_{\pi}\big{[}\mathcal{V}^{\pi}(s_{t+1})\big{]}$
    表示为所有状态-动作对的最优 $Q$-函数，那么最优值函数可以写成 $\mathcal{V}^{*}(s)=\underset{a}{\max}\big{\{}\mathcal{Q}^{*}(s,a)\big{\}}$。现在，问题简化为通过迭代过程找到所有状态-动作对的
    $Q$-函数的最优值，即 $\mathcal{Q}^{*}(s,a)$。特别地，$Q$-函数根据以下规则更新：
- en: '|  | $\displaystyle\mathcal{Q}_{t+1}(s,a)=$ | $\displaystyle\mathcal{Q}_{t}(s,a)+$
    |  | (2) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{Q}_{t+1}(s,a)=$ | $\displaystyle\mathcal{Q}_{t}(s,a)+$
    |  | (2) |'
- en: '|  |  | $\displaystyle\alpha_{t}\Big{[}r_{t}(s,a)+\gamma\max_{a^{\prime}}\mathcal{Q}_{t}(s,a^{\prime})-\mathcal{Q}_{t}(s,a)\Big{]}.$
    |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\alpha_{t}\Big{[}r_{t}(s,a)+\gamma\max_{a^{\prime}}\mathcal{Q}_{t}(s,a^{\prime})-\mathcal{Q}_{t}(s,a)\Big{]}.$
    |  |'
- en: 'The core idea behind this update is to find the Temporal Difference (TD) between
    the predicted $\mathcal{Q}$-value, i.e., $r_{t}(s,a)+\gamma\underset{a^{\prime}}{\max}\mathcal{Q}_{t}(s,a^{\prime})$
    and its current value, i.e., $\mathcal{Q}_{t}(s,a)$. In ([2](#S2.E2 "In II-B1
    𝑄-Learning Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning:
    An Overview ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey")), the learning rate $\alpha_{t}$ is used to determine the
    impact of new information to the existing $\mathcal{Q}$-value. The learning rate
    can be chosen to be a constant, or it can be adjusted dynamically during the learning
    process. However, it must satisfy Assumption [1](#Thmassumption1 "Assumption 1\.
    ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey") to guarantee the convergence for the $Q$-learning algorithm.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '这个更新的核心思想是找到预测的 $\mathcal{Q}$-值，即 $r_{t}(s,a)+\gamma\underset{a^{\prime}}{\max}\mathcal{Q}_{t}(s,a^{\prime})$
    和其当前值，即 $\mathcal{Q}_{t}(s,a)$ 之间的时间差 (TD)。在 ([2](#S2.E2 "In II-B1 𝑄-Learning
    Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")) 中，学习率 $\alpha_{t}$ 用于确定新信息对现有 $\mathcal{Q}$-值的影响。学习率可以选择为常量，也可以在学习过程中动态调整。然而，它必须满足假设 [1](#Thmassumption1
    "Assumption 1\. ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement Learning ‣ II
    Deep Reinforcement Learning: An Overview ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey") 以保证 $Q$-学习算法的收敛性。'
- en: Assumption 1.
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 1。
- en: 'The step size $\alpha_{t}$ is deterministic, nonnegative and satisfies the
    following conditions: $\alpha_{t}\in[0,1]$, $\overset{\infty}{\underset{t=0}{\sum}}\alpha_{t}=\infty$,
    and $\phantom{5}\overset{\infty}{\underset{t=0}{\sum}}(\alpha_{t})^{2}<\infty$
    .'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 步长 $\alpha_{t}$ 是确定的、非负的，并且满足以下条件：$\alpha_{t}\in[0,1]$，$\overset{\infty}{\underset{t=0}{\sum}}\alpha_{t}=\infty$，并且
    $\phantom{5}\overset{\infty}{\underset{t=0}{\sum}}(\alpha_{t})^{2}<\infty$ 。
- en: 'The step size adaptation $\alpha_{t}=\frac{1}{t}$ is one of the most common
    examples used in reinforcement learning. More discussions for selecting an appropriate
    step size can be found in [[14](#bib.bib14)]. The details of the $Q$-learning
    algorithm are then provided in Algorithm [1](#alg1 "Algorithm 1 ‣ II-B1 𝑄-Learning
    Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '步长自适应 $\alpha_{t}=\frac{1}{t}$ 是强化学习中最常见的例子之一。有关选择合适步长的更多讨论可在 [[14](#bib.bib14)]
    中找到。然后在算法 [1](#alg1 "Algorithm 1 ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement
    Learning ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep
    Reinforcement Learning in Communications and Networking: A Survey") 中提供了 $Q$-学习算法的详细信息。'
- en: Algorithm 1 The $Q$-learning algorithm
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 $Q$-学习算法
- en: 'Input: For each state-action pair $(s,a)$, initialize the table entry $\mathcal{Q}(s,a)$
    arbitrarily, e.g., to zero. Observe the current state $s$, initialize a value
    for the learning rate $\alpha$ and the discount factor $\gamma$.  for $t:=1$ to
    $T$ do     From the current state-action pair $(s,a)$, execute action $a$ and
    obtain the immediate reward $r$ and a new state $s^{\prime}$.     Select an action
    $a^{\prime}$ based on the state $s^{\prime}$ and then update the table entry for
    $\mathcal{Q}(s,a)$ as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: 对于每个状态-动作对$(s,a)$，将表项$\mathcal{Q}(s,a)$初始化为任意值，例如，初始化为零。观察当前状态$s$，为学习率$\alpha$和折扣因子$\gamma$初始化一个值。  for $t:=1$到$T$ do     从当前状态-动作对$(s,a)$，执行动作$a$并获得即时奖励$r$和新状态$s^{\prime}$。     根据状态$s^{\prime}$选择一个动作$a^{\prime}$，然后更新$\mathcal{Q}(s,a)$的表项如下:'
- en: '|  | $\displaystyle\mathcal{Q}_{t+1}(s,a)$ | $\displaystyle\leftarrow\mathcal{Q}_{t}(s,a)+\alpha_{t}\Big{[}r_{t}(s,a)+$
    |  | (3) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{Q}_{t+1}(s,a)$ | $\displaystyle\leftarrow\mathcal{Q}_{t}(s,a)+\alpha_{t}\Big{[}r_{t}(s,a)+$
    |  | (3) |'
- en: '|  |  | $\displaystyle\gamma\max_{a^{\prime}}\mathcal{Q}_{t}(s^{\prime},a^{\prime})-\mathcal{Q}_{t}(s,a)\Big{]}$
    |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\gamma\max_{a^{\prime}}\mathcal{Q}_{t}(s^{\prime},a^{\prime})-\mathcal{Q}_{t}(s,a)\Big{]}$
    |  |'
- en: 'Replace $s\leftarrow s^{\prime}$.  end for  Output: $\pi^{*}(s)=\arg\max_{a}\mathcal{Q}^{*}(s,a)$.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '替换$s\leftarrow s^{\prime}$。  end for  输出: $\pi^{*}(s)=\arg\max_{a}\mathcal{Q}^{*}(s,a)$。'
- en: 'Once either all $\mathcal{Q}$-values converge or a certain number of iterations
    is reached, the algorithm will terminate. The algorithm then yields the optimal
    policy indicating an action to be taken at each state such that $\mathcal{Q}^{*}(s,a)$
    is maximized for all states in the state space, i.e., $\pi^{*}(s)=\arg\underset{a}{\max}\mathcal{Q}^{*}(s,a)$.
    Under the assumption of the step size (i.e., Assumption [1](#Thmassumption1 "Assumption
    1\. ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")), it is proved in [[15](#bib.bib15)] that the $Q$-learning
    algorithm converges to the optimum action-values with probability one.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦所有$\mathcal{Q}$-值收敛或达到一定次数的迭代，算法将终止。然后，算法将生成一个最优策略，指示在每个状态下采取的行动，以使所有状态下的$\mathcal{Q}^{*}(s,a)$达到最大，即$\pi^{*}(s)=\arg\underset{a}{\max}\mathcal{Q}^{*}(s,a)$。在假设步长的条件下（即，假设 [1](#Thmassumption1
    "Assumption 1\. ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement Learning ‣ II
    Deep Reinforcement Learning: An Overview ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey")），在 [[15](#bib.bib15)]中证明了$Q$-学习算法以概率一收敛到最优动作值。'
- en: 'II-B2 SARSA: An Online Q-Learning Algorithm'
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'II-B2 SARSA: 一种在线Q-学习算法'
- en: 'Although the $Q$-learning algorithm can find the optimal policy for the agent
    without requiring knowledge about the environment, this algorithm works in an
    offline fashion. In particular, Algorithm [1](#alg1 "Algorithm 1 ‣ II-B1 𝑄-Learning
    Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey") can obtain the optimal policy only after all $\mathcal{Q}$-values converge.
    Therefore, this section presents an alternative online learning algorithm, i.e.,
    the SARSA algorithm, which allows the agent to approach the optimal policy in
    an online fashion.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管$Q$-学习算法可以在无需了解环境的情况下找到代理的最优策略，但该算法是离线工作的。特别是，算法 [1](#alg1 "Algorithm 1 ‣
    II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")仅在所有$\mathcal{Q}$-值收敛之后才能获得最优策略。因此，本节介绍了一种替代的在线学习算法，即SARSA算法，它允许代理以在线方式接近最优策略。'
- en: Different from the $Q$-learning algorithm, the SARSA algorithm is an online
    algorithm which allows the agent to choose optimal actions at each time step in
    a real-time fashion without waiting until the algorithm converges. In the $Q$-learning
    algorithm, the policy is updated according to the maximum reward of available
    actions regardless of which policy is applied, i.e., an off-policy method. In
    contrast, the SARSA algorithm interacts with the environment and updates the policy
    directly from the actions taken, i.e., an on-policy method. Note that the SARSA
    algorithm updates $\mathcal{Q}$-values from the quintuple $\mathcal{Q}(s,a,r,s^{\prime},a^{\prime})$.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与 $Q$-学习算法不同，SARSA 算法是一种在线算法，它允许智能体在每个时间步实时选择最优动作，而不必等到算法收敛。在 $Q$-学习算法中，策略根据可用动作的最大奖励进行更新，而不考虑应用了哪种策略，即一种离策略方法。相比之下，SARSA
    算法通过与环境交互并直接从所采取的动作中更新策略，即一种在策略方法。请注意，SARSA 算法从五元组 $\mathcal{Q}(s,a,r,s^{\prime},a^{\prime})$
    更新 $\mathcal{Q}$ 值。
- en: II-B3 Q-Learning for Markov Games
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B3 马尔可夫游戏的 Q-学习
- en: 'To apply Q-learning algorithm to the Markov game context, we first define the
    $Q$-function for agent $i$ by $\mathcal{Q}_{i}(s,a^{i},\textbf{a}^{-i})$, where
    $\textbf{a}^{-i}\triangleq\{a^{1},\ldots,a^{i-1},a^{i+1},\ldots,a^{I}\}$ denotes
    the set of actions of all agents except $i$. Then, the Nash $Q$-function of agent
    $i$ is defined by:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将 Q-学习算法应用于马尔可夫游戏环境，我们首先通过 $\mathcal{Q}_{i}(s,a^{i},\textbf{a}^{-i})$ 定义智能体
    $i$ 的 $Q$-函数，其中 $\textbf{a}^{-i}\triangleq\{a^{1},\ldots,a^{i-1},a^{i+1},\ldots,a^{I}\}$
    表示所有智能体的动作集合（除 $i$ 外）。然后，智能体 $i$ 的纳什 $Q$-函数定义为：
- en: '|  | $\displaystyle\mathcal{Q}^{*}_{i}(s,a^{i},\textbf{a}^{-i})$ | $\displaystyle=r^{i}(s,a^{i},\textbf{a}^{-i})+$
    |  | (4) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{Q}^{*}_{i}(s,a^{i},\textbf{a}^{-i})$ | $\displaystyle=r^{i}(s,a^{i},\textbf{a}^{-i})+$
    |  | (4) |'
- en: '|  |  | $\displaystyle\beta\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}&#124;s,a^{i},\textbf{a}^{-i})\mathcal{V}^{i}(s^{\prime},\pi_{1}^{*},\ldots,\pi_{I}^{*}),$
    |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\beta\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}\mid
    s,a^{i},\textbf{a}^{-i})\mathcal{V}^{i}(s^{\prime},\pi_{1}^{*},\ldots,\pi_{I}^{*}),$
    |  |'
- en: where $(\pi_{1}^{*},\ldots,\pi_{I}^{*})$ is the joint Nash equilibrium strategy,
    $r^{i}(s,a^{i},\textbf{a}^{-i})$ is agent $i$’s immediate reward in state $s$
    under the joint action $(a^{i},\textbf{a}^{-i})$, and $\mathcal{V}^{i}(s^{\prime},\pi_{1}^{*},\ldots,\pi_{I}^{*})$
    is the total discounted reward over an infinite time horizon starting from state
    $s^{\prime}$ given that all the agents follow the equilibrium strategies.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(\pi_{1}^{*},\ldots,\pi_{I}^{*})$ 是联合纳什均衡策略，$r^{i}(s,a^{i},\textbf{a}^{-i})$
    是在状态 $s$ 下，联合动作 $(a^{i},\textbf{a}^{-i})$ 的智能体 $i$ 的即时奖励，而 $\mathcal{V}^{i}(s^{\prime},\pi_{1}^{*},\ldots,\pi_{I}^{*})$
    是在所有智能体遵循均衡策略的情况下，从状态 $s^{\prime}$ 开始的无限时间范围内的总折扣奖励。
- en: 'In [[13](#bib.bib13)], the authors propose a multi-agent Q-learning algorithm
    for general-sum Markov games which allows the agents to perform updates based
    on assuming Nash equilibrium behavior over the current Q-values. In particular,
    agent $i$ will learn its $Q$-values by forming an arbitrary guess from starting
    time of the game. At each time step $t$, agent $i$ observes the current state
    and takes an action $a^{i}$. Then, it observes its immediate reward $r^{i}$, actions
    taken by others $\textbf{a}^{-i}$, others’ immediate rewards, and the new system
    state $s^{\prime}$. After that, agent $i$ calculates a Nash equilibrium $(\pi_{1}(s^{\prime}),\ldots,\pi_{I}(s^{\prime}))$
    for the state game $(\mathcal{Q}_{1}^{t}(s^{\prime}),\ldots,\mathcal{Q}_{I}^{t}(s^{\prime}))$,
    and updates its $Q$-values according to:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[13](#bib.bib13)] 中，作者提出了一种用于一般总和马尔可夫游戏的多智能体 Q-学习算法，该算法允许智能体基于当前 Q 值假设纳什均衡行为来执行更新。特别地，智能体
    $i$ 将通过从游戏开始时形成一个任意猜测来学习其 $Q$ 值。在每个时间步 $t$，智能体 $i$ 观察当前状态并采取动作 $a^{i}$。然后，它观察其即时奖励
    $r^{i}$、其他智能体所采取的动作 $\textbf{a}^{-i}$、其他智能体的即时奖励以及新的系统状态 $s^{\prime}$。之后，智能体 $i$
    为状态游戏 $(\mathcal{Q}_{1}^{t}(s^{\prime}),\ldots,\mathcal{Q}_{I}^{t}(s^{\prime}))$
    计算一个纳什均衡 $(\pi_{1}(s^{\prime}),\ldots,\pi_{I}(s^{\prime}))$，并根据以下公式更新其 $Q$ 值：
- en: '|  | $\mathcal{Q}_{i}^{t+1}(s,a^{i},\textbf{a}^{-i})=(1-\alpha_{t})\mathcal{Q}_{i}^{t}(s,a^{i},\textbf{a}^{-i})+\alpha_{t}[r_{t}^{i}+\gamma\mathscr{N}_{t}^{i}(s^{\prime})],$
    |  | (5) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{Q}_{i}^{t+1}(s,a^{i},\textbf{a}^{-i})=(1-\alpha_{t})\mathcal{Q}_{i}^{t}(s,a^{i},\textbf{a}^{-i})+\alpha_{t}[r_{t}^{i}+\gamma\mathscr{N}_{t}^{i}(s^{\prime})],$
    |  | (5) |'
- en: where $\alpha_{t}\in(0,1)$ is the learning rate and $\mathscr{N}_{t}^{i}(s^{\prime})\triangleq\mathcal{Q}_{i}^{t}(s^{\prime})\times\pi_{1}(s^{\prime})\times\cdots\times\pi_{I}(s^{\prime})$.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha_{t}\in(0,1)$ 是学习率，$\mathscr{N}_{t}^{i}(s^{\prime})\triangleq\mathcal{Q}_{i}^{t}(s^{\prime})\times\pi_{1}(s^{\prime})\times\cdots\times\pi_{I}(s^{\prime})$。
- en: 'In order to calculate the Nash equilibrium, agent $i$ needs to know $(\mathcal{Q}_{1}^{t}(s^{\prime}),\ldots,\mathcal{Q}_{I}^{t}(s^{\prime}))$.
    However, the information about other agents’ $\mathcal{Q}$-values is not given,
    and thus agent $i$ must learn this information too. To do so, agent $i$ will set
    estimations about others’ $\mathcal{Q}$-values at the beginning of the game, e.g.,
    $\mathcal{Q}_{0}^{j}(s,a^{i},\textbf{a}^{-i})=0,\forall j,s$. As the game proceeds,
    agent $i$ observes other agents’ immediate rewards and previous actions. That
    information can then be used to update agent $i$’s conjectures on other agents’
    $Q$-functions. Agent $i$ updates its beliefs about agent $j$’s $Q$-function, according
    to the same updating rule in ([5](#S2.E5 "In II-B3 Q-Learning for Markov Games
    ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning: An Overview ‣
    Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")). Then, the authors prove that under some highly restrictive assumptions
    on the form of the state games during learning, the proposed multi-agent $Q$-learning
    algorithm is guaranteed to be converged.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '为了计算纳什均衡，代理 $i$ 需要知道 $(\mathcal{Q}_{1}^{t}(s^{\prime}),\ldots,\mathcal{Q}_{I}^{t}(s^{\prime}))$。然而，关于其他代理的
    $\mathcal{Q}$-值的信息没有给出，因此代理 $i$ 也必须学习这些信息。为此，代理 $i$ 会在游戏开始时对其他代理的 $\mathcal{Q}$-值做出估计，例如，$\mathcal{Q}_{0}^{j}(s,a^{i},\textbf{a}^{-i})=0,\forall
    j,s$。随着游戏的进行，代理 $i$ 观察其他代理的即时奖励和之前的动作。然后可以使用这些信息来更新代理 $i$ 对其他代理 $Q$-函数的猜测。代理 $i$
    根据在([5](#S2.E5 "In II-B3 Q-Learning for Markov Games ‣ II-B Reinforcement Learning
    ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey"))中的相同更新规则更新其对代理 $j$ 的 $Q$-函数的信念。随后，作者证明在学习过程中对状态游戏形式的一些高度限制性假设下，所提出的多代理
    $Q$-学习算法是保证收敛的。'
- en: II-C Deep Learning
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 深度学习
- en: Deep learning [[2](#bib.bib2)] is composed of a set of algorithms and techniques
    that attempt to find important features of data and to model its high-level abstractions.
    The main goal of deep learning is to avoid manual description of a data structure
    (like hand-written features) by automatic learning from the data. Its name refers
    to the fact that typically any neural network with two or more hidden layers is
    called DNN. Most deep learning models are based on an Artificial Neural Network
    (ANN), even though they can also include propositional formulas or latent variables
    organized layer-wise in deep generative models such as the nodes in Deep Belief
    Networks and Deep Boltzmann Machines.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习[[2](#bib.bib2)]由一组算法和技术组成，旨在寻找数据的重要特征并建模其高级抽象。深度学习的主要目标是通过从数据中自动学习来避免对数据结构的手动描述（如手工编写的特征）。其名称指的是通常任何具有两个或更多隐藏层的神经网络都被称为DNN。大多数深度学习模型基于人工神经网络（ANN），尽管它们也可以包括命题公式或以层次方式组织的潜在变量，如深度信念网络和深度玻尔兹曼机中的节点。
- en: 'An ANN is a computational nonlinear model based on the neural structure of
    the brain that is able to learn to perform tasks such as classification, prediction,
    decision-making, and visualization. An ANN consists of artificial neurons and
    is organized into three interconnected layers: input, hidden, and output as illustrated
    in Fig. [3](#S2.F3 "Figure 3 ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")(b). The input layer contains input neurons that send
    information to the hidden layer. The hidden layer sends data to the output layer.
    Every neuron has weighted inputs (synapses), an activation function (defines the
    output given an input), and one output. Synapses are the adjustable parameters
    that convert a neural network to a parameterized system.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '人工神经网络（ANN）是一种基于大脑神经结构的计算非线性模型，能够学习执行分类、预测、决策和可视化等任务。人工神经网络由人工神经元组成，并组织成三个互连的层：输入层、隐藏层和输出层，如图[3](#S2.F3
    "Figure 3 ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")(b)所示。输入层包含将信息传送到隐藏层的输入神经元。隐藏层将数据传送到输出层。每个神经元具有加权输入（突触）、一个激活函数（定义给定输入的输出）和一个输出。突触是将神经网络转换为参数化系统的可调参数。'
- en: During the training phase, ANNs use backpropagation as an effective learning
    algorithm to compute quickly a gradient descent with respect to the weights. Backpropagation
    is a special case of automatic differentiation. In the context of learning, backpropagation
    is commonly used by the gradient descent optimization algorithm to adjust the
    weights of neurons by calculating the gradient of the loss function. This technique
    is also sometimes called backward propagation of errors, because the error is
    calculated at the output and distributed back through the network layers.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，ANN 使用反向传播作为有效的学习算法，以便快速计算相对于权重的梯度下降。反向传播是自动微分的一种特殊情况。在学习的背景下，反向传播通常由梯度下降优化算法使用，以通过计算损失函数的梯度来调整神经元的权重。这种技术有时也称为误差的反向传播，因为误差在输出处计算，并通过网络层分发回去。
- en: 'A DNN is defined as an ANN with multiple hidden layers. There are two typical
    DNN models, i.e., Feedforward Neural Network (FNN) and Recurrent Neural Network
    (RNN). In the FNN, the information moves in only one direction, i.e., from the
    input nodes, through the hidden nodes and to the output nodes, and there are no
    cycles or loops in the network as shown in Fig. [4](#S2.F4 "Figure 4 ‣ II-C Deep
    Learning ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep
    Reinforcement Learning in Communications and Networking: A Survey"). In FNNs,
    Convolutional Neural Network (CNN) is the most well known model with a wide range
    of applications especially in image and speech recognition. The CNN contains one
    or more convolutional layers, pooling or fully connected, and uses a variation
    of multilayer perceptrons discussed above. Convolutional layers use a convolution
    operation to the input passing the result to the next layer. This operation allows
    the network to be deeper with much fewer parameters.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: DNN 被定义为具有多个隐藏层的 ANN。典型的 DNN 模型有两个，即前馈神经网络（FNN）和递归神经网络（RNN）。在 FNN 中，信息仅向一个方向流动，即从输入节点，通过隐藏节点到输出节点，并且网络中没有周期或循环，如图
    [4](#S2.F4 "图 4 ‣ II-C 深度学习 ‣ II 深度强化学习概述 ‣ 深度强化学习在通信和网络中的应用：综述") 所示。在 FNN 中，卷积神经网络（CNN）是最知名的模型，具有广泛的应用，特别是在图像和语音识别方面。CNN
    包含一个或多个卷积层、池化层或全连接层，并使用上述讨论的多层感知机的变体。卷积层对输入进行卷积操作，将结果传递到下一层。这种操作使得网络可以更深，同时具有更少的参数。
- en: '![Refer to caption](img/35562a59856188e83faf82a64b063247.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/35562a59856188e83faf82a64b063247.png)'
- en: 'Figure 4: RNN vs CNN.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：RNN 与 CNN。
- en: Unlike FNNs, the RNN is a variant of a recursive artificial neural network in
    which connections between neurons make directed cycles. It means that an output
    depends not only on its immediate inputs, but also on the previous further step’s
    neuron state. The RNNs are designed to utilize sequential data, when the current
    step has some relation with the previous steps. This makes the RNNs ideal for
    applications with a time component, e.g., time-series data, and natural language
    processing. However, all RNNs have feedback loops in the recurrent layer. This
    lets RNNs maintain information in memory over time. Nevertheless, it can be difficult
    to train standard RNNs to solve problems that require learning long-term temporal
    dependencies. The reason is that the gradient of the loss function decays exponentially
    with time, which is called the vanishing gradient problem. Thus, Long Short-Term
    Memory (LSTM) is often used in RNNs to address this issue. The LSTM is designed
    to model temporal sequences and their long-range dependencies are more accurate
    than conventional RNNs. The LSTM does not use an activation function within its
    recurrent components, the stored values are not modified, and the gradient does
    not tend to vanish during training. Usually, LSTM units are implemented in “blocks”
    with several units. These blocks have three or four “gates”, e.g., input gate,
    forget gate, output gate, that control information flow drawing on the logistic
    function.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与FNNs不同，RNN是一种递归人工神经网络的变体，其中神经元之间的连接形成了有向循环。这意味着输出不仅依赖于其即时输入，还依赖于之前更远一步的神经元状态。RNNs旨在利用序列数据，当当前步骤与之前的步骤有关时，这使得RNNs非常适合有时间成分的应用，例如时间序列数据和自然语言处理。然而，所有RNNs在递归层中都有反馈循环。这使得RNNs能够在时间上保持信息。然而，训练标准RNNs以解决需要学习长期时间依赖的问题可能会很困难。原因在于损失函数的梯度随着时间呈指数衰减，这被称为梯度消失问题。因此，长短期记忆（LSTM）通常在RNNs中使用来解决这个问题。LSTM旨在建模时间序列，其长期依赖性比传统RNNs更准确。LSTM在其递归组件中不使用激活函数，存储的值不会被修改，而且在训练过程中梯度不会消失。通常，LSTM单元以“块”的形式实现，每个块包含几个单元。这些块有三个或四个“门”，例如输入门、遗忘门、输出门，这些门利用逻辑函数控制信息流。
- en: II-D Deep $Q$-Learning
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 深度$Q$-学习
- en: 'The $Q$-learning algorithm can efficiently obtain an optimal policy when the
    state space and action space are small. However, in practice, with complicated
    system models, these spaces are usually large. As a result, the $Q$-learning algorithm
    may not be able to find the optimal policy. Thus, Deep $Q$-Learning (DQL) algorithm
    is introduced to overcome this shortcoming. Intuitively, the DQL algorithm implements
    a Deep $Q$-Network (DQN), i.e., a DNN, instead of the $Q$-table to derive an approximate
    value of $Q^{*}(s,a)$ as shown in Fig. [3](#S2.F3 "Figure 3 ‣ II-B Reinforcement
    Learning ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep
    Reinforcement Learning in Communications and Networking: A Survey")(c).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '$Q$-学习算法在状态空间和动作空间较小的情况下可以有效地获得最佳策略。然而，在实际应用中，复杂的系统模型通常会使这些空间变得很大。因此，$Q$-学习算法可能无法找到最佳策略。因此，引入了深度$Q$-学习（DQL）算法来克服这一缺陷。直观上，DQL算法实现了深度$Q$-网络（DQN），即使用DNN代替$Q$-表来推导$Q^{*}(s,a)$的近似值，如图[3](#S2.F3
    "Figure 3 ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")(c)所示。'
- en: As stated in [[16](#bib.bib16)], the average reward obtained by reinforcement
    learning algorithms may not be stable or even diverge when a nonlinear function
    approximator is used. This stems from the fact that a small change of $\mathcal{Q}$-values
    may greatly affect the policy. Thus, the data distribution and the correlations
    between the $\mathcal{Q}$-values and the target values $R+\gamma\max_{a^{\prime}}\mathcal{Q}(s^{\prime},a^{\prime})$
    are varied. To address this issue, two mechanisms, i.e., experience replay and
    target $Q$-network, can be used.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如[[16](#bib.bib16)]所述，当使用非线性函数逼近器时，强化学习算法获得的平均奖励可能不稳定，甚至可能发散。这源于$\mathcal{Q}$-值的小变化可能会极大地影响策略。因此，数据分布以及$\mathcal{Q}$-值与目标值$R+\gamma\max_{a^{\prime}}\mathcal{Q}(s^{\prime},a^{\prime})$之间的相关性是变化的。为了解决这个问题，可以使用两种机制，即经验回放和目标$Q$-网络。
- en: •
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Experience replay mechanism*: The algorithm first initializes a replay memory
    $\mathbf{D}$, i.e., the memory pool, with transitions $(s_{t},a_{t},r_{t},s_{t+1})$,
    i.e., experiences, generated randomly, e.g., through using $\epsilon$-greedy policy.
    Then, the algorithm randomly selects samples, i.e., minibatches, of transitions
    from $\mathbf{D}$ to train the DNN. The Q-values obtained by the trained DNN will
    be used to obtain new experiences, i.e., transitions, and these experiences will
    be then stored in the memory pool $\mathbf{D}$. This mechanism allows the DNN
    trained more efficiently by using both old and new experiences. In addition, by
    using the experience replay, the transitions are more independent and identically
    distributed, and thus the correlations between observations can be removed.'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*经验回放机制*：算法首先初始化一个回放记忆 $\mathbf{D}$，即记忆池，其中包含随机生成的转移 $(s_{t},a_{t},r_{t},s_{t+1})$，即经验，例如，通过使用
    $\epsilon$-贪婪策略。然后，算法从 $\mathbf{D}$ 中随机选择样本，即小批量转移，以训练 DNN。经过训练的 DNN 获得的 Q 值将用于获取新经验，即转移，这些经验随后将存储在记忆池
    $\mathbf{D}$ 中。该机制使得 DNN 能够通过使用旧经验和新经验来更高效地训练。此外，通过使用经验回放，转移更加独立且同分布，从而去除观察之间的相关性。'
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fixed target $Q$-network: In the training process, the $\mathcal{Q}$-value
    will be shifted. Thus, the value estimations can be out of control if a constantly
    shifting set of values is used to update the $Q$-network. This leads to the destabilization
    of the algorithm. To address this issue, the target $Q$-network is used to update
    frequently but slowly the primary $Q$-networks’ values. In this way, the correlations
    between the target and estimated $\mathcal{Q}$-values are significantly reduced,
    thereby stabilizing the algorithm.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 固定目标 $Q$-网络：在训练过程中，$\mathcal{Q}$-值会发生偏移。因此，如果使用一组不断变化的值来更新 $Q$-网络，价值估计可能会失控，这会导致算法的不稳定。为了解决这个问题，使用目标
    $Q$-网络来频繁但缓慢地更新主要 $Q$-网络的值。这样，目标和估计的 $\mathcal{Q}$-值之间的相关性显著减少，从而稳定了算法。
- en: 'The DQL algorithm with experience replay and fixed target $Q$-network is presented
    in Algorithm [2](#alg2 "Algorithm 2 ‣ II-D Deep Q-Learning ‣ II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey"). DQL inherits and promotes advantages of both reinforcement
    and deep learning techniques, and thus it has a wide range of applications in
    practice such as game development [[3](#bib.bib3)], transportation [[17](#bib.bib17)],
    and robotics [[18](#bib.bib18)].'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '带有经验回放和固定目标 $Q$-网络的 DQL 算法在算法 [2](#alg2 "Algorithm 2 ‣ II-D Deep Q-Learning
    ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey") 中提出。DQL 继承并提升了强化学习和深度学习技术的优势，因此在实践中具有广泛的应用，如游戏开发
    [[3](#bib.bib3)]、运输 [[17](#bib.bib17)] 和机器人 [[18](#bib.bib18)]。'
- en: Algorithm 2 The DQL Algorithm with Experience Replay and Fixed Target $Q$-Network
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 带有经验回放和固定目标 $Q$-网络的 DQL 算法
- en: '1:  Initialize replay memory $\mathbf{D}$.2:  Initialize the $Q$-network $\mathbf{Q}$
    with random weights $\boldsymbol{\theta}$.3:  Initialize the target $Q$-network
    $\hat{\mathbf{Q}}$ with random weights $\boldsymbol{\theta^{\prime}}$.4:  for episode=1
    to T do5:     With probability $\epsilon$ select a random action $a_{t}$, otherwise
    select $a_{t}=\arg\max\mathcal{Q}^{*}(s_{t},a_{t},\boldsymbol{\theta})$.6:     Perform
    action $a_{t}$ and observe immediate reward $r_{t}$ and next state $s_{t+1}$.7:     Store
    transition $(s_{t},a_{t},r_{t},s_{t+1})$ in $\mathbf{D}$.8:     Select randomly
    samples c$(s_{j},a_{j},r_{j},s_{j+1})$ from $\mathbf{D}$.9:     The weights of
    the neural network then are optimized by using stochastic gradient descent with
    respect to the network parameter $\boldsymbol{\theta}$ to minimize the loss:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 初始化回放记忆 $\mathbf{D}$。2: 用随机权重 $\boldsymbol{\theta}$ 初始化 $Q$-网络 $\mathbf{Q}$。3:
    用随机权重 $\boldsymbol{\theta^{\prime}}$ 初始化目标 $Q$-网络 $\hat{\mathbf{Q}}$。4: 对于 episode=1
    到 T 执行5: 以概率 $\epsilon$ 选择随机动作 $a_{t}$，否则选择 $a_{t}=\arg\max\mathcal{Q}^{*}(s_{t},a_{t},\boldsymbol{\theta})$。6:
    执行动作 $a_{t}$ 并观察即时奖励 $r_{t}$ 和下一个状态 $s_{t+1}$。7: 将转移 $(s_{t},a_{t},r_{t},s_{t+1})$
    存储在 $\mathbf{D}$ 中。8: 从 $\mathbf{D}$ 中随机选择样本 c$(s_{j},a_{j},r_{j},s_{j+1})$。9:
    然后使用随机梯度下降优化神经网络的权重，以最小化损失：'
- en: '|  | $\Big{[}r_{j}+\gamma\max_{a_{j+1}}\hat{\mathcal{Q}}(s_{j+1},a_{j+1};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta})\Big{]}^{2}.$
    |  | (6) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Big{[}r_{j}+\gamma\max_{a_{j+1}}\hat{\mathcal{Q}}(s_{j+1},a_{j+1};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta})\Big{]}^{2}.$
    |  | (6) |'
- en: 10:     Reset $\hat{\mathbf{Q}}=\mathbf{Q}$ after every a fixed number of steps.11:  end for
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 10:     每经过固定步数后，重置$\hat{\mathbf{Q}}=\mathbf{Q}$。11:  结束 for
- en: II-E Advanced Deep $Q$-Learning Models
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E 高级深度 $Q$-学习模型
- en: II-E1 Double Deep $Q$-Learning
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-E1 双重深度 $Q$-学习
- en: 'In some stochastic environments, the $Q$-learning algorithm performs poorly
    due to the large over-estimations of action values [[19](#bib.bib19)]. These over-estimations
    result from a positive bias that is introduced because $Q$-learning uses the maximum
    action value as an approximation for the maximum expected action value as shown
    in Eq. ([3](#S2.E3 "In 4 ‣ Algorithm 1 ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement
    Learning ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep
    Reinforcement Learning in Communications and Networking: A Survey")). The reason
    is that the same samples are used to decide which action is the best, i.e., with
    highest expected reward, and the same samples are also used to estimate that action-value.
    Thus, to overcome the over-estimation problem of the $Q$-learning algorithm, the
    authors in [[20](#bib.bib20)] introduce a solution using two $Q$-value functions,
    i.e., $\mathcal{Q}_{1}$ and $\mathcal{Q}_{2}$, to simultaneously select and evaluate
    action values through the loss function as follows: $\Big{[}r_{j}+\gamma\mathcal{Q}_{2}\Big{(}s_{j+1},\arg\underset{a_{j+1}}{\max}\mathcal{Q}_{1}\big{(}s_{j+1},a_{j+1};\boldsymbol{\theta}_{1}\big{)};\boldsymbol{\theta}_{2}\Big{)}-\mathcal{Q}_{1}(s_{j},a_{j};\boldsymbol{\theta}_{1})\Big{]}^{2}$.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '在一些随机环境中，由于对动作值的过度估计，$Q$-学习算法表现不佳[[19](#bib.bib19)]。这些过度估计是由于$Q$-学习使用最大动作值作为最大期望动作值的近似值而引入的正偏差，如公式（[3](#S2.E3
    "In 4 ‣ Algorithm 1 ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement Learning
    ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey")）所示。原因是相同的样本被用来决定哪个动作是最好的，即具有最高的期望回报，同时也用来估计该动作值。因此，为了克服$Q$-学习算法的过度估计问题，作者在[[20](#bib.bib20)]中引入了一种使用两个$Q$-值函数的解决方案，即$\mathcal{Q}_{1}$和$\mathcal{Q}_{2}$，通过以下损失函数同时选择和评估动作值：$\Big{[}r_{j}+\gamma\mathcal{Q}_{2}\Big{(}s_{j+1},\arg\underset{a_{j+1}}{\max}\mathcal{Q}_{1}\big{(}s_{j+1},a_{j+1};\boldsymbol{\theta}_{1}\big{)};\boldsymbol{\theta}_{2}\Big{)}-\mathcal{Q}_{1}(s_{j},a_{j};\boldsymbol{\theta}_{1})\Big{]}^{2}$。'
- en: 'Note that the selection of an action, in the $\arg\max$, is still due to the
    online weights $\boldsymbol{\theta}_{1}$. This means that, as in $Q$-learning,
    we are still estimating the value of the greedy policy according to the current
    values, as defined by $\boldsymbol{\theta}_{1}$. However, the second set of weights
    $\boldsymbol{\theta}_{2}$ is used to evaluate fairly the value of this policy.
    This second set of weights can be updated symmetrically by switching the roles
    of $\boldsymbol{\theta}_{1}$ and $\boldsymbol{\theta}_{2}$. Inspired by this idea,
    the authors in [[20](#bib.bib20)] then develop Double Deep $Q$-Learning (DDQL)
    model [[21](#bib.bib21)] using a Double Deep Q-Network (DDQN) with the loss function
    updated as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在$\arg\max$中选择一个动作仍然是由于在线权重$\boldsymbol{\theta}_{1}$。这意味着，和$Q$-学习一样，我们仍然根据当前的值（由$\boldsymbol{\theta}_{1}$定义）来估计贪婪策略的价值。然而，第二组权重$\boldsymbol{\theta}_{2}$用于公平地评估该策略的价值。第二组权重可以通过交换$\boldsymbol{\theta}_{1}$和$\boldsymbol{\theta}_{2}$的角色来对称更新。受此启发，作者在[[20](#bib.bib20)]中发展了双重深度$Q$-学习（DDQL）模型[[21](#bib.bib21)]，使用了一个双重深度Q网络（DDQN），并且损失函数更新如下：
- en: '|  | $\displaystyle\Big{[}r_{j}$ | $\displaystyle+\gamma\hat{\mathcal{Q}}\Big{(}s_{j+1},\arg\max_{a_{j+1}}\mathcal{Q}\big{(}s_{j+1},a_{j+1};\boldsymbol{\theta}\big{)};\boldsymbol{\theta^{\prime}}\Big{)}$
    |  | (7) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Big{[}r_{j}$ | $\displaystyle+\gamma\hat{\mathcal{Q}}\Big{(}s_{j+1},\arg\max_{a_{j+1}}\mathcal{Q}\big{(}s_{j+1},a_{j+1};\boldsymbol{\theta}\big{)};\boldsymbol{\theta^{\prime}}\Big{)}$
    |  | (7) |'
- en: '|  |  | $\displaystyle-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta})\Big{]}^{2}.$
    |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta})\Big{]}^{2}.$
    |  |'
- en: 'Unlike double $Q$-learning, the weights of the second network $\boldsymbol{\theta_{2}}$
    are replaced with the weights of the target networks $\boldsymbol{\theta^{\prime}}$
    for the evaluation of the current greedy policy as shown in Eq. ([7](#S2.E7 "In
    II-E1 Double Deep 𝑄-Learning ‣ II-E Advanced Deep Q-Learning Models ‣ II Deep
    Reinforcement Learning: An Overview ‣ Applications of Deep Reinforcement Learning
    in Communications and Networking: A Survey")). The update to the target network
    stays unchanged from DQN, and remains a periodic copy of the online network. Due
    to the effectiveness of DDQL, there are some applications of DDQL introduced recently
    to address dynamic spectrum access problems in multichannel wireless networks [[22](#bib.bib22)]
    and resource allocation in heterogeneous networks [[23](#bib.bib23)].'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '与双重$Q$-学习不同，第二个网络$\boldsymbol{\theta_{2}}$的权重在评估当前贪婪策略时被替换为目标网络$\boldsymbol{\theta^{\prime}}$的权重，如公式([7](#S2.E7
    "In II-E1 Double Deep 𝑄-Learning ‣ II-E Advanced Deep Q-Learning Models ‣ II Deep
    Reinforcement Learning: An Overview ‣ Applications of Deep Reinforcement Learning
    in Communications and Networking: A Survey"))所示。目标网络的更新保持不变，与DQN相同，仍然是在线网络的周期性副本。由于DDQL的有效性，最近有一些DDQL应用于解决多信道无线网络中的动态频谱接入问题[[22](#bib.bib22)]和异构网络中的资源分配问题[[23](#bib.bib23)]。'
- en: II-E2 Deep $Q$-Learning with Prioritized Experience Replay
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-E2 带有优先经验回放的深度$Q$-学习
- en: 'Experience replay mechanism allows the reinforcement learning agent to remember
    and reuse experiences, i.e., transitions, from the past. In particular, transitions
    are uniformly sampled from the replay memory $\mathbf{D}$. However, this approach
    simply replays transitions at the same frequency as that the agent was originally
    experienced, regardless of their significance. Therefore, the authors in [[24](#bib.bib24)]
    develop a framework for prioritizing experiences, so as to replay important transitions
    more frequently, and therefore learn more efficiently. Ideally, we want to sample
    more frequently those transitions from which there is much to learn. As a proxy
    for learning potential, the proposed Prioritized Experience Replay (PER) [[24](#bib.bib24)]
    samples transitions with probability $p_{t}$ relative to the last encountered
    absolute error defined as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放机制允许强化学习代理记住并重用过去的经验，即转移。特别是，从重放记忆$\mathbf{D}$中均匀采样转移。然而，这种方法仅以与代理最初经历的频率相同的频率回放转移，而不考虑其重要性。因此，作者在[[24](#bib.bib24)]中开发了一个优先级经验框架，以便更频繁地回放重要转移，从而更高效地学习。理想情况下，我们希望更频繁地采样那些需要大量学习的转移。作为学习潜力的代理，提出的优先经验回放（PER）[[24](#bib.bib24)]以概率$p_{t}$采样转移，相对于最后遇到的绝对误差定义如下：
- en: '|  | $p_{t}\varpropto\Big{&#124;}r_{j}+\gamma\max_{a^{\prime}}\hat{\mathcal{Q}}(s_{j+1},a^{\prime};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta)}\Big{&#124;}^{\omega},$
    |  | (8) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{t}\varpropto\Big{&#124;}r_{j}+\gamma\max_{a^{\prime}}\hat{\mathcal{Q}}(s_{j+1},a^{\prime};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta)}\Big{&#124;}^{\omega},$
    |  | (8) |'
- en: where $\omega$ is a hyper-parameter that determines the shape of the distribution.
    New transitions are inserted into the replay buffer with maximum priority, providing
    a bias towards recent transitions. Note that stochastic transitions may also be
    favoured, even when there is little left to learn about them. Through real experiments
    on many Atari games, the authors demonstrate that DQL with PER outperforms DQL
    with uniform replay on 41 out of 49 games. However, this solution is only appropriate
    to implement when we can find and define the important experiences in the replay
    memory $\mathbf{D}$.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\omega$是一个超参数，决定了分布的形状。新的转移以最大优先级插入到重放缓冲区中，偏向于最近的转移。请注意，即使对这些转移了解不多，随机转移也可能受到青睐。通过在多个Atari游戏中的实际实验，作者证明了带有优先经验回放（PER）的DQL在49个游戏中的41个游戏中优于均匀重放的DQL。然而，只有在我们能够找到并定义重放记忆$\mathbf{D}$中的重要经验时，这种解决方案才适合实施。
- en: II-E3 Dueling Deep $Q$-Learning
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-E3 对抗深度$Q$-学习
- en: 'The $Q$-values, i.e., $\mathcal{Q}(s,a)$, used in the $Q$-learning algorithm,
    i.e., Algorithm [1](#alg1 "Algorithm 1 ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement
    Learning ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep
    Reinforcement Learning in Communications and Networking: A Survey"), are to express
    how good it is to take a certain action at a given state. The value of an action
    $a$ at a given state $s$ can actually be decomposed into two fundamental values.
    The first value is the state-value function, i.e., $\mathscr{V}(s)$, to estimate
    the importance of being in a particular state $s$. The second value is the action-value
    function, i.e., $\mathscr{A}(a)$, to estimate the importance of selecting an action
    $a$ compared with other actions. As a result, the $Q$-value function can be expressed
    by two fundamental value functions as follows: $\mathcal{Q}(s,a)=\mathscr{V}(s)+\mathscr{A}(a)$.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在$Q$-学习算法中使用的$Q$-值，即$\mathcal{Q}(s,a)$，用于表示在给定状态下采取某个动作的好坏。给定状态$s$下动作$a$的值实际上可以分解为两个基本值。第一个值是状态值函数，即$\mathscr{V}(s)$，用于估计处于特定状态$s$的重要性。第二个值是动作值函数，即$\mathscr{A}(a)$，用于估计选择某个动作$a$与其他动作相比的重要性。因此，$Q$-值函数可以用两个基本值函数表示，如下所示：$\mathcal{Q}(s,a)=\mathscr{V}(s)+\mathscr{A}(a)$。
- en: 'Stemming from the fact that in many MDPs, it is unnecessary to estimate both
    values, i.e., action and state values of Q-function $\mathcal{Q}(s,a)$, at the
    same time. For example, in many racing games, moving left or right matters if
    and only if the agent meets the obstacles or enemies. Inspired by this idea, the
    authors in [[25](#bib.bib25)] introduce an idea of using two streams, i.e., two
    sequences, of fully connected layers instead of using a single sequence with fully
    connected layers for the DQN. The two streams are constructed such that they are
    able to provide separate estimations on the action and state value functions,
    i.e., $\mathscr{V}(s)$ and $\mathscr{A}(a)$. Finally, the two streams are combined
    to generate a single output $\mathcal{Q}(s,a)$ as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在许多MDP中，同时估计Q函数$\mathcal{Q}(s,a)$的动作值和状态值是没有必要的。例如，在许多赛车游戏中，只有当代理遇到障碍物或敌人时，向左或向右移动才会有意义。受到这一思想的启发，作者在[[25](#bib.bib25)]中提出了使用两个流，即两个序列的全连接层，而不是使用一个序列的全连接层来替代DQN。两个流被构建为能够分别提供动作值和状态值函数的估计，即$\mathscr{V}(s)$和$\mathscr{A}(a)$。最后，这两个流被组合生成一个单一的输出$\mathcal{Q}(s,a)$，具体如下：
- en: '|  | $\displaystyle\mathcal{Q}(s,a;\boldsymbol{\alpha},\boldsymbol{\beta})=\mathscr{V}(s;\boldsymbol{\beta})+\Big{(}\mathscr{A}(s,a;\boldsymbol{\alpha})-\frac{\sum_{a^{\prime}}\mathscr{A}(s,a^{\prime};\boldsymbol{\alpha})}{&#124;\mathcal{A}&#124;}\Big{)},$
    |  | (9) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{Q}(s,a;\boldsymbol{\alpha},\boldsymbol{\beta})=\mathscr{V}(s;\boldsymbol{\beta})+\Big{(}\mathscr{A}(s,a;\boldsymbol{\alpha})-\frac{\sum_{a^{\prime}}\mathscr{A}(s,a^{\prime};\boldsymbol{\alpha})}{&#124;\mathcal{A}&#124;}\Big{)},$
    |  | (9) |'
- en: 'where $\boldsymbol{\beta}$ and $\boldsymbol{\alpha}$ are the parameters of
    the two streams $\mathscr{V}(s;\boldsymbol{\beta})$ and $\mathscr{A}(s,a^{\prime};\boldsymbol{\alpha})$,
    respectively. Here, $|\mathcal{A}|$ is the total number of actions in the action
    space $\mathcal{A}$. Then, the loss function is derived in the similar way to ([6](#S2.E6
    "In 9 ‣ Algorithm 2 ‣ II-D Deep Q-Learning ‣ II Deep Reinforcement Learning: An
    Overview ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")) as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$\boldsymbol{\beta}$和$\boldsymbol{\alpha}$分别是两个流$\mathscr{V}(s;\boldsymbol{\beta})$和$\mathscr{A}(s,a^{\prime};\boldsymbol{\alpha})$的参数。这里，$|\mathcal{A}|$是动作空间$\mathcal{A}$中的动作总数。然后，损失函数的推导方式类似于([6](#S2.E6
    "In 9 ‣ Algorithm 2 ‣ II-D Deep Q-Learning ‣ II Deep Reinforcement Learning: An
    Overview ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey"))，具体如下：'
- en: $\Big{[}r_{j}+\gamma\underset{a_{j+1}}{\max}\hat{\mathcal{Q}}(s_{j+1},a_{j+1};\boldsymbol{\alpha^{\prime}},\boldsymbol{\beta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\alpha},\boldsymbol{\beta})\Big{]}^{2}$.
    Through the simulation, the authors show that the proposed dueling DQN can outperform
    DDQN [[21](#bib.bib21)] in 50 out of 57 learned Atari games. However, the proposed
    dueling architecture only clearly benefits for MDPs with large action spaces.
    For small state spaces, the performance of dueling DQL is even not as good as
    that of double DQL as shown in simulation results in [[25](#bib.bib25)].
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: $\Big{[}r_{j}+\gamma\underset{a_{j+1}}{\max}\hat{\mathcal{Q}}(s_{j+1},a_{j+1};\boldsymbol{\alpha^{\prime}},\boldsymbol{\beta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\alpha},\boldsymbol{\beta})\Big{]}^{2}$.
    通过模拟，作者展示了所提出的对抗DQN在57个学习的Atari游戏中有50个优于DDQN。然而，所提出的对抗架构仅对具有大动作空间的MDP显著有利。对于小状态空间，正如[[25](#bib.bib25)]中的模拟结果所示，对抗DQL的表现甚至不如双重DQL。
- en: II-E4 Asynchronous Multi-step Deep Q-Learning
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-E4 异步多步深度Q学习
- en: 'Most of the $Q$-learning methods such as DQL and dueling DQL rely on the experience
    replay method. However, such kind of method has several drawbacks. For example,
    it uses more memory and computation resources per real interaction, and it requires
    off-policy learning algorithms that can update from data generated by an older
    policy. This limits the applications of DQL. Therefore, the authors in [[26](#bib.bib26)]
    introduce a method using multiple agents to train the DNN in parallel. In particular,
    the authors propose a training procedure which utilizes asynchronous gradient
    decent updates from multiple agents at once. Instead of training one single agent
    that interacts with its environment, multiple agents are interacting with their
    own version of the environment simultaneously. After a certain amount of timesteps,
    accumulated gradient updates from an agent are applied to a global model, i.e.,
    the DNN. These updates are asynchronous and lock free. In addition, to tradeoff
    between bias and variance in the policy gradient, the authors adopt $n$-step updates
    method [[1](#bib.bib1)] to update the reward function. In particular, the truncated
    $n$-step reward function can be defined by $r_{t}^{(n)}=\underset{k=0}{\overset{n-1}{\sum}}\gamma^{(k)}r_{t+k+1}$.
    Thus, the alternative loss for each agent will be derived by:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数$Q$-学习方法，如DQL和对抗DQL，依赖于经验回放方法。然而，这种方法有几个缺点。例如，它在每次实际交互时需要更多的内存和计算资源，并且需要能够从旧策略生成的数据中进行更新的离线学习算法。这限制了DQL的应用。因此，文献[[26](#bib.bib26)]中的作者引入了一种利用多个代理并行训练DNN的方法。特别地，作者提出了一种训练过程，利用来自多个代理的异步梯度下降更新。不同于训练一个单独的代理与环境互动，多个代理同时与各自版本的环境互动。在一定的时间步后，来自一个代理的累积梯度更新应用于全局模型，即DNN。这些更新是异步的且无锁的。此外，为了在策略梯度中权衡偏差和方差，作者采用了$n$-步更新方法[[1](#bib.bib1)]来更新奖励函数。特别地，截断的$n$-步奖励函数可以定义为$r_{t}^{(n)}=\underset{k=0}{\overset{n-1}{\sum}}\gamma^{(k)}r_{t+k+1}$。因此，每个代理的替代损失将通过以下公式推导：
- en: '|  | $\Big{[}r_{j}^{(n)}+\gamma_{j}^{(n)}\max_{a^{\prime}}\hat{\mathcal{Q}}(s_{j+n},a^{\prime};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta})\Big{]}^{2}.$
    |  | (10) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Big{[}r_{j}^{(n)}+\gamma_{j}^{(n)}\max_{a^{\prime}}\hat{\mathcal{Q}}(s_{j+n},a^{\prime};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta})\Big{]}^{2}.$
    |  | (10) |'
- en: The effects of training speed and quality of the proposed asynchronous DQL with
    multi-step learning are analyzed for various reinforcement learning methods, e.g.,
    1-step $Q$-learning, 1-step SARSA, and n-step $Q$-learning. They show that asynchronous
    updates have a stabilizing effect on policy and value updates. Also, the proposed
    method outperforms the current state-of-the-art algorithms on the Atari games
    while training for half of the time on a single multi-core CPU instead of a GPU.
    As a result, some recent applications of asynchronous DQL have been developed
    for handover control problems in wireless systems [[27](#bib.bib27)]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于各种强化学习方法，例如1步$Q$-学习、1步SARSA和n步$Q$-学习，分析了所提出的异步DQL与多步学习的训练速度和质量的效果。结果表明，异步更新对策略和值更新具有稳定作用。此外，所提出的方法在Atari游戏上优于当前最先进的算法，同时在单个多核CPU上训练时间为GPU的一半。因此，最近开发了一些异步DQL的应用于无线系统中的切换控制问题[[27](#bib.bib27)]。
- en: II-E5 Distributional Deep Q-learning
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-E5 分布式深度Q学习
- en: All aforementioned methods use the Bellman equation to approximate the expected
    value of future rewards. However, if the environment is stochastic in nature and
    the future rewards follow multimodal distribution, choosing actions based on expected
    value may not lead to the optimal outcome. For example, we know that the expected
    transmission time of a packet in a wireless network is 20 minutes. However, this
    information may not be so meaningful because it may overestimate the transmission
    time most of the time. For example, the expected transmission time is calculated
    based on the normal transmissions (without collisions) and the interference transmissions
    (with collisions). Although the interference transmissions are very rare to happen,
    but it takes a lot of time. Then, the estimation about the expected transmission
    is overestimated most of the time. This makes estimations not useful for the DQL
    algorithms.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有方法都使用贝尔曼方程来近似未来奖励的期望值。然而，如果环境本质上是随机的，且未来奖励遵循多模态分布，则基于期望值选择动作可能不会导致*最佳结果*。例如，我们知道无线网络中数据包的预计传输时间是20分钟。然而，这个信息可能不太有意义，因为它可能大多数时候会高估传输时间。例如，预计传输时间是基于正常传输（没有冲突）和干扰传输（有冲突）来计算的。虽然干扰传输很少发生，但它需要很长时间。因此，对预计传输的估算大多数时候是被高估的。这使得估算对于DQL算法并不实用。
- en: 'Thus, the authors in [[28](#bib.bib28)] introduce a solution using distributional
    reinforcement learning to update $Q$-value function based on its distribution
    rather than its expectation. In particular, let $\mathcal{Z}(s,a)$ be the return
    obtained by starting from state $s$, executing action $a$, and following the current
    policy, then $\mathcal{Q}(s,a)=\mathbb{E}[\mathcal{Z}(s,a)]$. Here, $\mathcal{Z}$
    represents the distribution of future rewards, which is no longer a scalar quantity
    like $Q$-values. Then we obtain the distributional version of Bellman equation
    as follows: $\mathcal{Z}(s,a)=r+\gamma\mathcal{Z}(s^{\prime},a^{\prime})$. For
    example, if we use the DQN and extract an experience $(s,a,r,s^{\prime})$ from
    the replay buffer, then the sample of the target distribution is $\mathcal{Z}(s,a)=r+\gamma\mathcal{Z}(s^{\prime},a^{*})$
    with $a^{*}=\arg\underset{a^{\prime}}{\max}\mathcal{Q}(s,a^{\prime})$. Although
    the proposed distributional deep Q-learning is demonstrated to outperform the
    conventional DQL [[16](#bib.bib16)] on many Atari 2600 Games (45 out of 57 games),
    its performance relies much on the distribution function $\mathcal{Z}$. If $\mathcal{Z}$
    is well defined, the performance of distributional deep $Q$-learning is much more
    significant than that of the DQL. Otherwise, its performance is even worse than
    that of the DQL.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作者在[[28](#bib.bib28)]中引入了一种解决方案，使用分布式强化学习来基于其分布而非期望值更新$Q$值函数。具体地，令$\mathcal{Z}(s,a)$表示从状态$s$开始，执行动作$a$并遵循当前策略所获得的回报，那么$\mathcal{Q}(s,a)=\mathbb{E}[\mathcal{Z}(s,a)]$。这里，$\mathcal{Z}$表示未来奖励的分布，不再像$Q$值那样是一个标量。然后我们得到贝尔曼方程的分布式版本如下：$\mathcal{Z}(s,a)=r+\gamma\mathcal{Z}(s^{\prime},a^{\prime})$。例如，如果我们使用DQN并从重放缓冲区提取经验$(s,a,r,s^{\prime})$，那么目标分布的样本是$\mathcal{Z}(s,a)=r+\gamma\mathcal{Z}(s^{\prime},a^{*})$，其中$a^{*}=\arg\underset{a^{\prime}}{\max}\mathcal{Q}(s,a^{\prime})$。尽管提出的分布式深度Q学习在许多Atari
    2600游戏（57个游戏中的45个）中优于传统DQL[[16](#bib.bib16)]，但其性能很大程度上依赖于分布函数$\mathcal{Z}$。如果$\mathcal{Z}$定义良好，则分布式深度$Q$学习的性能显著优于DQL。否则，其性能甚至会比DQL更差。
- en: II-E6 Deep $Q$-learning with Noisy Nets
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-E6 使用噪声网络的深度$Q$学习
- en: 'In [[29](#bib.bib29)], the authors introduce Noisy Net, a type of neural network
    whose bias and weights are iteratively perturbed during training by a parametric
    function of the noise. This network basically adds the Gaussian noise to the last
    (fully-connected) layers of the network. The parameters of this noise can be adjusted
    by the model during training, which allows the agent to decide when and in what
    proportion it wants to introduce the uncertainty to its weights. In particular,
    to implement the noisy network, we first replace the $\epsilon$-greedy policy
    by a randomized action-value function. Then, the fully connected layers of the
    value network are parameterized as a noisy network, where the parameters are drawn
    from the noisy network parameter distribution after every replay step. For replay,
    the current noisy network parameter sample is held fixed across the batch. Since
    the DQL takes one step of optimization for every action step, the noisy network
    parameters are re-sampled before every action. After that, the loss function can
    be updated as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[29](#bib.bib29)] 中，作者引入了 Noisy Net，这是一种在训练过程中通过噪声的参数化函数迭代扰动其偏置和权重的神经网络。该网络基本上在网络的最后（全连接）层添加了高斯噪声。这些噪声的参数可以在训练过程中由模型调整，从而允许智能体决定何时以及以何种比例将不确定性引入到其权重中。具体来说，为了实现噪声网络，我们首先用随机化动作值函数替换了
    $\epsilon$-贪婪策略。然后，将值网络的全连接层参数化为噪声网络，其中参数在每次重放步骤后从噪声网络参数分布中抽取。对于重放，当前的噪声网络参数样本在整个批次中保持不变。由于
    DQL 每执行一步动作就进行一次优化，因此噪声网络参数在每次动作之前都会重新抽样。之后，损失函数可以更新如下：
- en: '|  | $\mathcal{L}=\mathbb{E}\Big{[}\mathbb{E}_{(s,a,r,s^{\prime})\thicksim\mathbf{D}}\big{[}r+\gamma\max_{a^{\prime}\in\mathcal{A}}\hat{\mathcal{Q}}(s^{\prime},a^{\prime},\epsilon^{\prime};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s,a,\epsilon;\boldsymbol{\theta})\big{]}\Big{]},$
    |  | (11) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\mathbb{E}\Big{[}\mathbb{E}_{(s,a,r,s^{\prime})\thicksim\mathbf{D}}\big{[}r+\gamma\max_{a^{\prime}\in\mathcal{A}}\hat{\mathcal{Q}}(s^{\prime},a^{\prime},\epsilon^{\prime};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s,a,\epsilon;\boldsymbol{\theta})\big{]}\Big{]},$
    |  | (11) |'
- en: where the outer and inner expectations are with respect to distributions of
    the noise variables $\epsilon$ and $\epsilon^{\prime}$ for the noisy value functions
    $\hat{\mathcal{Q}}(s^{\prime},a^{\prime},\epsilon^{\prime};\boldsymbol{\theta^{\prime}})$
    and $\mathcal{Q}(s,a,\epsilon;\boldsymbol{\theta})$, respectively.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，外层和内层期望分别是关于噪声变量 $\epsilon$ 和 $\epsilon^{\prime}$ 的分布，对于噪声值函数 $\hat{\mathcal{Q}}(s^{\prime},a^{\prime},\epsilon^{\prime};\boldsymbol{\theta^{\prime}})$
    和 $\mathcal{Q}(s,a,\epsilon;\boldsymbol{\theta})$。
- en: Through experimental results, the authors demonstrate that by adding the Gaussian
    noise layer to the DNN, the performance of conventional DQL [[16](#bib.bib16)],
    dueling DQL [[25](#bib.bib25)], and asynchronous DQL [[26](#bib.bib26)] can be
    significantly improved for a wide range of Atari games. However, the impact of
    noise to the performance of the deep DQL algorithms is still under debating in
    the literature, and thus analysis on the impact of noise layer requires further
    investigations.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实验结果，作者展示了通过向深度神经网络（DNN）中添加高斯噪声层，可以显著提高传统深度 Q 学习（DQL）[[16](#bib.bib16)]、对抗深度
    Q 学习（Dueling DQL）[[25](#bib.bib25)] 和异步深度 Q 学习（Asynchronous DQL）[[26](#bib.bib26)]
    在各种 Atari 游戏中的表现。然而，噪声对深度 Q 学习算法性能的影响仍在文献中讨论中，因此噪声层的影响分析需要进一步研究。
- en: 'TABLE II: Performance comparison among DQL algorithms'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：DQL 算法之间的性能比较
- en: '| DQL Algorithms | No Operations | Human Starts | Publish | Developer |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| DQL 算法 | 无操作 | 人工起始 | 发布 | 开发者 |'
- en: '| DQL | 79% | 68% | Nature 2015 [[16](#bib.bib16)] | Google DeepMind |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| DQL | 79% | 68% | Nature 2015 [[16](#bib.bib16)] | Google DeepMind |'
- en: '| DDQL | 117% | 110% | AAAI 2016 [[21](#bib.bib21)] | Google DeepMind |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| DDQL | 117% | 110% | AAAI 2016 [[21](#bib.bib21)] | Google DeepMind |'
- en: '| Prioritized DDQL | 140% | 128% | ICLR 2015 [[24](#bib.bib24)] | Google DeepMind
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 优先级 DDQL | 140% | 128% | ICLR 2015 [[24](#bib.bib24)] | Google DeepMind |'
- en: '| Dueling DDQL | 151% | 117% | ICML 2016 [[25](#bib.bib25)] | Google DeepMind
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 对抗 DDQL | 151% | 117% | ICML 2016 [[25](#bib.bib25)] | Google DeepMind |'
- en: '| Asynchronous DQL | - | 116% | ICML 2016 [[26](#bib.bib26)] | Google DeepMind
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 异步 DQL | - | 116% | ICML 2016 [[26](#bib.bib26)] | Google DeepMind |'
- en: '| Distributional DQL | 164% | 125% | ICML 2017 [[28](#bib.bib28)] | Google
    DeepMind |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 分布式 DQL | 164% | 125% | ICML 2017 [[28](#bib.bib28)] | Google DeepMind |'
- en: '| Noisy Nets DQL | 118% | 102% | ICLR 2018 [[29](#bib.bib29)] | Google DeepMind
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 噪声网络 DQL | 118% | 102% | ICLR 2018 [[29](#bib.bib29)] | Google DeepMind |'
- en: '| Rainbow | 223% | 153% | AAAI 2018 [[30](#bib.bib30)] | Google DeepMind |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 彩虹 | 223% | 153% | AAAI 2018 [[30](#bib.bib30)] | Google DeepMind |'
- en: II-E7 Rainbow Deep $Q$-learning
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-E7 彩虹深度 $Q$-学习
- en: In [[30](#bib.bib30)], the authors propose a solution which integrates all advantages
    of seven aforementioned solutions (including DQL) into a single learning agent,
    called Rainbow DQL. In particular, this algorithm first defines the loss function
    based on the asynchronous multi-step and distributional DQL. Then, the authors
    combine the multi-step distributional loss with double $Q$-learning by using the
    greedy action in $s_{t+n}$ selected according to the $Q$-network as the bootstrap
    action $a^{*}_{t+n}$, and evaluate the action by using the target network.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[30](#bib.bib30)]中，作者提出了一种将七种前述解决方案（包括 DQL）的所有优点集成到一个学习代理中的解决方案，称为 Rainbow
    DQL。特别是，该算法首先基于异步多步和分布式 DQL 定义损失函数。然后，作者通过使用根据 $Q$-网络选择的 $s_{t+n}$ 中的贪婪动作作为引导动作
    $a^{*}_{t+n}$，将多步分布式损失与双重 $Q$-学习结合起来，并通过目标网络评估该动作。
- en: In standard proportional prioritized replay [[24](#bib.bib24)] technique, the
    absolute TD-error is used to prioritize the transitions. Here, TD-error at a time
    slot is the error in the estimate made at the time slot. However, in the proposed
    Rainbow DQL algorithm, all distributional Rainbow variants prioritize transitions
    by the Kullbeck-Leibler (KL) loss because this loss may be more robust to noisy
    stochastic environment. Alternatively, the dueling architecture of DNNs is presented
    in [[25](#bib.bib25)]. Finally, the Noisy Net layer [[30](#bib.bib30)] is used
    to replace all linear layers in order to reduce the number of independent noise
    variables. Through simulation, the authors show that this is the most advanced
    technique which outperforms almost all current DQL algorithms in the literature
    over 57 Atari 2600 games.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的比例优先回放[[24](#bib.bib24)]技术中，绝对 TD-误差用于优先排序转移。在这里，时间段的 TD-误差是对该时间段做出的估计的误差。然而，在提议的
    Rainbow DQL 算法中，所有分布式 Rainbow 变体通过 Kullbeck-Leibler (KL) 损失来优先排序转移，因为这种损失可能对噪声随机环境更加稳健。另一种方法是使用[[25](#bib.bib25)]中提出的
    DNN 对抗架构。最后，使用 Noisy Net 层[[30](#bib.bib30)]来替换所有线性层，以减少独立噪声变量的数量。通过模拟，作者展示了这是最先进的技术，优于文献中几乎所有现有的
    DQL 算法，在 57 款 Atari 2600 游戏中表现出色。
- en: 'In Table [II](#S2.T2 "TABLE II ‣ II-E6 Deep 𝑄-learning with Noisy Nets ‣ II-E
    Advanced Deep Q-Learning Models ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey"), we summarize the DQL algorithms and their performance under the parameter
    settings used in [[30](#bib.bib30)]. As observed in Table [II](#S2.T2 "TABLE II
    ‣ II-E6 Deep 𝑄-learning with Noisy Nets ‣ II-E Advanced Deep Q-Learning Models
    ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey"), all of the DQL algorithms
    have been developed by Google DeepMind based on the original work in [[16](#bib.bib16)].
    So far, through experimental results on Atari 2600 games, the Rainbow DQL presents
    very impressive results over all other DQL algorithms. However, more experiments
    need to be further conducted in different domains to confirm the real efficiency
    of the Rainbow DQL algorithm.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[II](#S2.T2 "TABLE II ‣ II-E6 Deep 𝑄-learning with Noisy Nets ‣ II-E Advanced
    Deep Q-Learning Models ‣ II Deep Reinforcement Learning: An Overview ‣ Applications
    of Deep Reinforcement Learning in Communications and Networking: A Survey")中，我们总结了
    DQL 算法及其在[[30](#bib.bib30)]中使用的参数设置下的性能。如表[II](#S2.T2 "TABLE II ‣ II-E6 Deep 𝑄-learning
    with Noisy Nets ‣ II-E Advanced Deep Q-Learning Models ‣ II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")所示，所有 DQL 算法均由 Google DeepMind 基于[[16](#bib.bib16)]中的原始工作开发。截至目前，通过
    Atari 2600 游戏的实验结果，Rainbow DQL 在所有其他 DQL 算法中表现出非常令人印象深刻的结果。然而，需要在不同领域进一步进行更多实验，以确认
    Rainbow DQL 算法的实际效率。'
- en: II-F Deep Q-Learning for Extensions of MDPs
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-F 深度 Q-学习用于 MDP 扩展
- en: II-F1 Deep Deterministic Policy Gradient Q-Learning for Continuous Action
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-F1 深度确定性策略梯度 Q-学习用于连续动作
- en: Although DQL algorithm can solve problems with high-dimensional state spaces,
    it can only handle discrete and low-dimensional action spaces. However, systems
    in many applications have continuous, i.e., real values, and high dimensional
    action spaces. The DQL algorithms cannot be straightforwardly applied to continuous
    actions since they rely on choosing the best action that maximizes the $Q$-value
    function. In particular, a full search in a continuous action space to find the
    optimal action is often infeasible.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DQL 算法可以解决具有高维状态空间的问题，但它只能处理离散和低维动作空间。然而，许多应用中的系统具有连续的，即实值的高维动作空间。由于 DQL
    算法依赖于选择最大化 $Q$ 值函数的最佳动作，因此不能直接应用于连续动作。特别是，在连续动作空间中进行全面搜索以找到最佳动作通常是不切实际的。
- en: 'In [[31](#bib.bib31)], the authors introduce a model-free off-policy actor-critic
    algorithm using deep function approximators that can learn policies in high-dimensional,
    continuous action spaces. The key idea is based on the deterministic policy gradient
    (DPG) algorithm proposed in [[32](#bib.bib32)]. In particular, the DPG algorithm
    maintains a parameterized actor function $\mu(s;\boldsymbol{\theta}^{\mu})$ with
    parameter vector $\boldsymbol{\theta}$ which specifies the current policy by deterministically
    mapping states to a specific action. The critic $Q(s,a)$ is learned by using the
    Bellman equation as in $Q$-learning. The actor is updated by applying the chain
    rule to the expected return from the start distribution $J$ with respect to the
    actor parameters as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[31](#bib.bib31)]中，作者引入了一种无模型的脱策略演员-评论家算法，该算法使用深度函数逼近器，能够在高维连续动作空间中学习策略。其核心思想基于[[32](#bib.bib32)]中提出的确定性策略梯度（DPG）算法。特别地，DPG算法维护一个参数化的演员函数
    $\mu(s;\boldsymbol{\theta}^{\mu})$，其参数向量 $\boldsymbol{\theta}$ 指定了当前策略，通过确定性地将状态映射到特定动作。评论家
    $Q(s,a)$ 使用与 $Q$-学习相同的贝尔曼方程进行学习。演员通过对起始分布 $J$ 的期望回报应用链式法则来更新演员参数，如下所示：
- en: '|  |  | $\displaystyle\nabla_{\boldsymbol{\theta}^{\mu}}J\thickapprox\mathbb{E}_{s_{t}\thicksim\rho^{\beta}}\big{[}\nabla_{\boldsymbol{\theta}^{\mu}}Q(s,a;\boldsymbol{\theta}^{Q})&#124;_{s=s_{t},a=\mu(s_{t}&#124;\boldsymbol{\theta}^{\mu})}\big{]}$
    |  | (12) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\nabla_{\boldsymbol{\theta}^{\mu}}J\thickapprox\mathbb{E}_{s_{t}\thicksim\rho^{\beta}}\big{[}\nabla_{\boldsymbol{\theta}^{\mu}}Q(s,a;\boldsymbol{\theta}^{Q})&#124;_{s=s_{t},a=\mu(s_{t}&#124;\boldsymbol{\theta}^{\mu})}\big{]}$
    |  | (12) |'
- en: '|  |  | $\displaystyle\thickapprox\mathbb{E}_{s_{t}\thicksim\rho^{\beta}}\Big{[}\nabla_{a}Q(s,a;\boldsymbol{\theta}^{Q})&#124;_{s=s_{t},a=\mu(s_{t})}\nabla_{\boldsymbol{\theta}{\mu}}\mu(s;\boldsymbol{\theta}^{\mu})&#124;_{s=s_{t}}\Big{]}.$
    |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\thickapprox\mathbb{E}_{s_{t}\thicksim\rho^{\beta}}\Big{[}\nabla_{a}Q(s,a;\boldsymbol{\theta}^{Q})&#124;_{s=s_{t},a=\mu(s_{t})}\nabla_{\boldsymbol{\theta}{\mu}}\mu(s;\boldsymbol{\theta}^{\mu})&#124;_{s=s_{t}}\Big{]}.$
    |  |'
- en: 'Based on this update rule, the authors then introduce Deep DPG (DDPG) algorithm
    which can learn competitive policies by using low-dimensional observations (e.g.
    cartesian coordinates or joint angles) under the same hyper-parameters and network
    structure. The detail of the DDPG algorithm is presented in [3](#alg3 "Algorithm
    3 ‣ II-F1 Deep Deterministic Policy Gradient Q-Learning for Continuous Action
    ‣ II-F Deep Q-Learning for Extensions of MDPs ‣ II Deep Reinforcement Learning:
    An Overview ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey"). The algorithm makes a copy of the actor and critic networks
    $Q^{\prime}(s,a;\boldsymbol{\theta}^{Q^{\prime}})$ and $\mu^{\prime}(s;\boldsymbol{\theta}^{\mu^{\prime}})$,
    respectively, to calculate the target values. The weights of these target networks
    are then updated with slowly tracking on the learned networks, i.e., $\boldsymbol{\theta}^{\prime}\leftarrow\tau\boldsymbol{\theta}+(1-\tau)\boldsymbol{\theta}^{\prime}$
    with $\tau\ll 1$. This means that the target values are constrained to change
    slowly, greatly improving the stability of learning. Note that a major challenge
    of learning in continuous action spaces is exploration. Therefore, in Algorithm [3](#alg3
    "Algorithm 3 ‣ II-F1 Deep Deterministic Policy Gradient Q-Learning for Continuous
    Action ‣ II-F Deep Q-Learning for Extensions of MDPs ‣ II Deep Reinforcement Learning:
    An Overview ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey"), an exploration policy $\mu^{\prime}$ is constructed by
    adding noise sampled from a noise process $\mathcal{N}$ to the actor policy.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个更新规则，作者引入了深度 DPG（DDPG）算法，该算法可以在相同的超参数和网络结构下，通过使用低维观测（例如笛卡尔坐标或关节角度）来学习竞争性策略。DDPG
    算法的详细信息见[3](#alg3 "算法 3 ‣ II-F1 深度确定性策略梯度 Q-学习用于连续动作 ‣ II-F 深度 Q-学习用于 MDP 扩展 ‣
    II 深度强化学习：概述 ‣ 深度强化学习在通信和网络中的应用：综述")。该算法分别对演员和评论员网络 $Q^{\prime}(s,a;\boldsymbol{\theta}^{Q^{\prime}})$
    和 $\mu^{\prime}(s;\boldsymbol{\theta}^{\mu^{\prime}})$ 进行复制，以计算目标值。这些目标网络的权重随后通过缓慢追踪学习到的网络来更新，即
    $\boldsymbol{\theta}^{\prime}\leftarrow\tau\boldsymbol{\theta}+(1-\tau)\boldsymbol{\theta}^{\prime}$，其中
    $\tau\ll 1$。这意味着目标值的变化受到限制，极大地提高了学习的稳定性。需要注意的是，连续动作空间中的主要挑战是探索。因此，在算法[3](#alg3
    "算法 3 ‣ II-F1 深度确定性策略梯度 Q-学习用于连续动作 ‣ II-F 深度 Q-学习用于 MDP 扩展 ‣ II 深度强化学习：概述 ‣ 深度强化学习在通信和网络中的应用：综述")中，通过将从噪声过程
    $\mathcal{N}$ 中采样的噪声添加到演员策略来构造探索策略 $\mu^{\prime}$。
- en: Algorithm 3 DDPG algorithm
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 DDPG 算法
- en: '1:  Randomly initialize critic network $Q(s,a;\boldsymbol{\theta}^{Q})$ and
    actor $\mu(s;\boldsymbol{\theta}^{\mu})$ with weights $\boldsymbol{\theta}^{Q}$
    and $\boldsymbol{\theta}^{\mu}$, respectively.2:  Initialize target network $Q^{\prime}$
    and $\mu^{\prime}$ with weights $\boldsymbol{\theta}^{Q^{\prime}}\leftarrow\boldsymbol{\theta}^{Q}$,
    and $\boldsymbol{\theta}^{\mu^{\prime}}\leftarrow\boldsymbol{\theta}^{\mu}$, respectively.3:  Initialize
    replay memory $\mathbf{D}$.4:  for episode=1 to M do5:     Initialize a random
    process $N$ for action exploration6:     Receive initial observation state $s_{1}$7:     for t=1
    to T do8:        Select action $a_{t}=\mu(s_{t};\boldsymbol{\theta}^{\mu})+\mathcal{N}_{t}$
    according to the current policy and exploration noise.9:        Execute action
    $a_{t}$ and observe reward $r_{t}$ and new state $s_{t+1}$.10:        Store transition
    $(s_{t},a_{t},r_{t},s_{t+1})$ in $\mathbf{D}$.11:        Sample a random mini-batch
    of $N$ transitions $(s_{i},a_{i},r_{i},s_{i+1})$ from $\mathbf{D}$.12:        Set
    $y_{i}=r_{i}+\gamma\mathcal{Q}^{{}^{\prime}}\big{(}s_{i+1},\mu^{{}^{\prime}}(s_{i+1};\boldsymbol{\theta}^{\mu^{\prime}});\boldsymbol{\theta}^{Q^{\prime}}\big{)}$.13:        Update
    critic by minimizing the loss: $L=\frac{1}{N}\sum_{i}(y_{i}-\mathcal{Q}\big{(}s_{i},a_{i};\boldsymbol{\theta}^{Q})\big{)}^{2}$14:        Update
    the actor policy by using the sampled policy gradient: $\nabla_{\boldsymbol{\theta}^{\mu}}J\thickapprox\frac{1}{N}\sum_{i}\nabla_{a}\mathcal{Q}(s,a;\boldsymbol{\theta}^{Q})|_{s=s_{i},a=\mu(s_{i})}$
    $\nabla_{\boldsymbol{\theta}^{\mu}}\mu(s|\boldsymbol{\theta}^{\mu})|_{s=s_{i}}$15:        Update
    the target networks:$\boldsymbol{\theta}^{Q^{\prime}}\leftarrow\tau\boldsymbol{\theta}^{Q}+(1-\tau)\boldsymbol{\theta}^{Q^{\prime}}$$\boldsymbol{\theta}^{\mu^{\prime}}\leftarrow\tau\boldsymbol{\theta}^{\mu}+(1-\tau)\boldsymbol{\theta}^{\mu^{\prime}}$16:     end for17:  end for'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 随机初始化评论网络 $Q(s,a;\boldsymbol{\theta}^{Q})$ 和演员 $\mu(s;\boldsymbol{\theta}^{\mu})$，权重分别为
    $\boldsymbol{\theta}^{Q}$ 和 $\boldsymbol{\theta}^{\mu}$。2: 初始化目标网络 $Q^{\prime}$
    和 $\mu^{\prime}$，权重分别为 $\boldsymbol{\theta}^{Q^{\prime}}\leftarrow\boldsymbol{\theta}^{Q}$
    和 $\boldsymbol{\theta}^{\mu^{\prime}}\leftarrow\boldsymbol{\theta}^{\mu}$。3: 初始化回放记忆
    $\mathbf{D}$。4: 对于每个 episode=1 到 M 执行5:     初始化一个随机过程 $N$ 以进行动作探索6:     接收初始观察状态
    $s_{1}$7:     对于 t=1 到 T 执行8:         根据当前策略和探索噪声选择动作 $a_{t}=\mu(s_{t};\boldsymbol{\theta}^{\mu})+\mathcal{N}_{t}$。9:         执行动作
    $a_{t}$ 并观察奖励 $r_{t}$ 和新状态 $s_{t+1}$。10:        将转换 $(s_{t},a_{t},r_{t},s_{t+1})$
    存储在 $\mathbf{D}$ 中。11:        从 $\mathbf{D}$ 中随机抽取一个小批量的 $N$ 个转换 $(s_{i},a_{i},r_{i},s_{i+1})$。12:        设置
    $y_{i}=r_{i}+\gamma\mathcal{Q}^{{}^{\prime}}\big{(}s_{i+1},\mu^{{}^{\prime}}(s_{i+1};\boldsymbol{\theta}^{\mu^{\prime}});\boldsymbol{\theta}^{Q^{\prime}}\big{)}$。13:        通过最小化损失来更新评论者：$L=\frac{1}{N}\sum_{i}(y_{i}-\mathcal{Q}\big{(}s_{i},a_{i};\boldsymbol{\theta}^{Q})\big{)}^{2}$14:        使用抽样的策略梯度更新演员策略：$\nabla_{\boldsymbol{\theta}^{\mu}}J\thickapprox\frac{1}{N}\sum_{i}\nabla_{a}\mathcal{Q}(s,a;\boldsymbol{\theta}^{Q})|_{s=s_{i},a=\mu(s_{i})}$
    $\nabla_{\boldsymbol{\theta}^{\mu}}\mu(s|\boldsymbol{\theta}^{\mu})|_{s=s_{i}}$15:        更新目标网络：$\boldsymbol{\theta}^{Q^{\prime}}\leftarrow\tau\boldsymbol{\theta}^{Q}+(1-\tau)\boldsymbol{\theta}^{Q^{\prime}}$$\boldsymbol{\theta}^{\mu^{\prime}}\leftarrow\tau\boldsymbol{\theta}^{\mu}+(1-\tau)\boldsymbol{\theta}^{\mu^{\prime}}$16:     结束
    for 循环17: 结束 for 循环'
- en: II-F2 Deep Recurrent Q-Learning for POMDPs
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-F2 深度递归 Q 学习用于 POMDPs
- en: To tackle problems with partially observable environments by deep reinforcement
    learning, the authors in [[33](#bib.bib33)] propose a framework called Deep Recurrent
    Q-Learning (DRQN) in which an LSTM layer was used to replace the first post-convolutional
    fully-connected layer of the conventional DQN. The recurrent structure is able
    to integrate an arbitrarily long history to better estimate the current state
    instead of utilizing a fixed-length history as in DQNs. Thus, DRQNs estimate the
    function $\mathcal{Q}(o_{t},h_{t-1};\boldsymbol{\theta})$ instead of $\mathcal{Q}(s_{t},a_{t});\boldsymbol{\theta})$,
    where $\boldsymbol{\theta}$ denotes the parameters of entire network, $h_{t-1}$
    denotes the output of the LSTM layer at the previous step, i.e., $h_{t}=LSTM(h_{t-1},o_{t})$.
    DRQN matches DQN’s performance on standard MDP problems and outperforms DQN in
    partially observable domains. Regarding the training process, DRQN only considers
    the convolutional features of the observation history instead of explicitly incorporating
    the actions. Through the experiments, the authors demonstrate that DRQN is capable
    of handling partial observability, and recurrency confers benefits when the quality
    of observations changes during evaluation time.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过深度强化学习解决部分可观测环境中的问题，[[33](#bib.bib33)]中的作者提出了一个框架，称为深度递归Q学习（DRQN），其中使用LSTM层替换了传统DQN中的第一个卷积后的全连接层。递归结构能够整合任意长度的历史，以更好地估计当前状态，而不是像DQN中那样使用固定长度的历史。因此，DRQN估计函数$\mathcal{Q}(o_{t},h_{t-1};\boldsymbol{\theta})$，而不是$\mathcal{Q}(s_{t},a_{t});\boldsymbol{\theta})$，其中$\boldsymbol{\theta}$表示整个网络的参数，$h_{t-1}$表示上一步LSTM层的输出，即$h_{t}=LSTM(h_{t-1},o_{t})$。DRQN在标准MDP问题上与DQN的性能相匹配，并且在部分可观测领域中优于DQN。关于训练过程，DRQN仅考虑观测历史的卷积特征，而不是显式地结合动作。通过实验，作者证明DRQN能够处理部分可观测性，当观察质量在评估过程中发生变化时，递归性带来好处。
- en: II-F3 Deep SARSA Learning
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-F3 深度SARSA学习
- en: 'In [[34](#bib.bib34)], the authors introduce a DQL technique based on SARSA
    learning to help the agent determine optimal policies in an online fashion. As
    shown in Algorithm [4](#alg4 "Algorithm 4 ‣ II-F3 Deep SARSA Learning ‣ II-F Deep
    Q-Learning for Extensions of MDPs ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey"), given the current state $s$, a CNN is used to obtain the current state-action
    value $\mathcal{Q}(s,a)$. Then, the current action $a$ is selected by the $\epsilon$-greedy
    algorithm. After that, the immediate reward $r$ and the next state $s^{\prime}$
    can be observed. In order to estimate the current $\mathcal{Q}(s,a)$, the next
    state-action value $\mathcal{Q}(s^{\prime},a^{\prime})$ is obtained. Here, when
    the next state $s^{\prime}$ is used as the input of the CNN, $\mathcal{Q}(s^{\prime},a^{\prime})$
    can be obtained as the output. Then, a label vector related to $\mathcal{Q}(s,a)$
    is defined as $\mathcal{Q}(s^{\prime},a^{\prime})$ which represents the target
    vector. The two vectors only have one different component, i.e., $r+\gamma\mathcal{Q}(s^{\prime},a^{\prime})\rightarrow\mathcal{Q}(s,a)$.
    It should be noted that during the training phase, the next action $a^{\prime}$
    for estimating the current state-action value is never greedy. On the contrary,
    there is a small probability that a random action is chosen for exploration.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[34](#bib.bib34)]中，作者介绍了一种基于SARSA学习的DQL技术，以帮助智能体以在线方式确定最优策略。如算法[4](#alg4
    "Algorithm 4 ‣ II-F3 Deep SARSA Learning ‣ II-F Deep Q-Learning for Extensions
    of MDPs ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey")所示，给定当前状态$s$，使用CNN来获得当前状态-动作值$\mathcal{Q}(s,a)$。然后，通过$\epsilon$-贪婪算法选择当前动作$a$。接着，可以观察到即时奖励$r$和下一个状态$s^{\prime}$。为了估计当前的$\mathcal{Q}(s,a)$，获取下一个状态-动作值$\mathcal{Q}(s^{\prime},a^{\prime})$。在这里，当下一个状态$s^{\prime}$作为CNN的输入时，$\mathcal{Q}(s^{\prime},a^{\prime})$可以作为输出获得。然后，定义一个与$\mathcal{Q}(s,a)$相关的标签向量为$\mathcal{Q}(s^{\prime},a^{\prime})$，它表示目标向量。两个向量仅有一个不同的分量，即$r+\gamma\mathcal{Q}(s^{\prime},a^{\prime})\rightarrow\mathcal{Q}(s,a)$。需要注意的是，在训练阶段，用于估计当前状态-动作值的下一个动作$a^{\prime}$永远不会是贪婪的。相反，有小概率选择一个随机动作进行探索。'
- en: Algorithm 4 Deep SARSA learning algorithm
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 算法4 深度SARSA学习算法
- en: '1:  Initialize data stack $\mathbf{D}$ with size of $N$2:  Initialize parameters
    $\boldsymbol{\theta}$ of the CNN3:  for episode=1 to M do4:     Initialize state
    $s_{1}$ and pre-process state $\phi_{1}=\phi(s_{1})$5:     Select $a_{1}$ by the
    $\epsilon$-greedy method6:     for t=1 to T do7:        Take action $a_{t}$, observe
    $r_{t}$ and next state $s_{t+1}$8:        $\phi_{t+1}=\phi(s_{t+1})$9:        Store
    data $(\phi_{t},a_{t},r_{t},\phi_{t+1})$ into stack $\mathbf{D}$10:        Sample
    data from stack $\mathbf{D}$11:        Select action $a^{\prime}$ by the $\epsilon$-greedy
    method12:        if episode terminates at step $j+1$ then13:           Set $y_{j}=r_{j}$14:        else15:           set
    $y_{j}=r_{j}+\mathcal{Q}(\phi_{t+1},a^{\prime};\boldsymbol{\theta})$16:        end if17:        Minimize
    the loss function: $(y_{j}-\mathcal{Q}(\phi_{t},a^{\prime};\boldsymbol{\theta}))^{2}$18:        Update
    $a_{t}\leftarrow a^{\prime}$19:     end for20:  end for'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 初始化数据堆栈 $\mathbf{D}$，大小为 $N$2: 初始化CNN的参数 $\boldsymbol{\theta}$3:  对于 episode=1
    到 M 执行4:    初始化状态 $s_{1}$ 并预处理状态 $\phi_{1}=\phi(s_{1})$5:    使用 $\epsilon$-贪婪方法选择
    $a_{1}$6:    对于 t=1 到 T 执行7:      执行动作 $a_{t}$，观察 $r_{t}$ 和下一个状态 $s_{t+1}$8:     
    $\phi_{t+1}=\phi(s_{t+1})$9:      将数据 $(\phi_{t},a_{t},r_{t},\phi_{t+1})$ 存储到堆栈
    $\mathbf{D}$10:      从堆栈 $\mathbf{D}$ 中抽取样本11:      使用 $\epsilon$-贪婪方法选择动作 $a^{\prime}$12:
         如果 episode 在第 $j+1$ 步终止，则13:         设定 $y_{j}=r_{j}$14:      否则15:        
    设置 $y_{j}=r_{j}+\mathcal{Q}(\phi_{t+1},a^{\prime};\boldsymbol{\theta})$16:     
    结束如果17:      最小化损失函数: $(y_{j}-\mathcal{Q}(\phi_{t},a^{\prime};\boldsymbol{\theta}))^{2}$18:
         更新 $a_{t}\leftarrow a^{\prime}$19:    结束对于20:  结束对于'
- en: II-F4 Deep $Q$-Learning for Markov Games
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-F4 深度 $Q$-学习在马尔可夫游戏中的应用
- en: In [[35](#bib.bib35)], the authors introduce the general notion of sequential
    prisoner’s dilemma (SPD) to model real world prisoner’s dilemma (PD) problems.
    Since SPD is more complicated than PD, existing approaches addressing learning
    in matrix PD games cannot be directly applied in SPD. Thus, the authors propose
    a multi-agent DRL approach for mutual cooperation in SDP games. The deep multi-agent
    reinforcement learning towards mutual cooperation consists of two phases, i.e.,
    offline and online phases. The offline phase generates policies with varying cooperation
    degrees. Since the number of policies with different cooperation degrees is infinite,
    it is computationally infeasible to train all the policies from scratch. To address
    this issue, the algorithm first trains representative policies using actor-critic
    until it converges, i.e., cooperation and defection baseline policy. Second, the
    algorithm synthesizes the full range of policies from the above baseline policies.
    Another task is to detect effectively the cooperation degree of the opponent.
    The algorithm divides this task into two steps. First, the algorithm trains an
    LSTM-based cooperation degree detection network offline, which will be then used
    for real-time detection during the online phase. In the online phase, the agent
    plays against the opponents by reciprocating with a policy of a slightly higher
    cooperation degree than that of the opponent. On one hand, intuitively the algorithm
    is cooperation-oriented and seeks for mutual cooperation whenever possible. On
    the other hand, the algorithm is also robust against selfish exploitation and
    resorts to defection strategy to avoid being exploited whenever necessary.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[35](#bib.bib35)]中，作者引入了序列囚徒困境（SPD）这一概念来建模现实世界中的囚徒困境（PD）问题。由于SPD比PD更复杂，因此现有的处理矩阵PD游戏中学习的方法不能直接应用于SPD。因此，作者提出了一种针对SDP游戏中互惠合作的多智能体深度强化学习（DRL）方法。面向互惠合作的深度多智能体强化学习包括两个阶段，即离线阶段和在线阶段。离线阶段生成具有不同合作度的策略。由于具有不同合作度的策略数量是无限的，因此从头开始训练所有策略在计算上是不切实际的。为了解决这个问题，该算法首先使用演员-评论家方法训练代表性策略，直到收敛，即合作和背叛基准策略。其次，算法从上述基准策略中综合生成全范围的策略。另一个任务是有效检测对手的合作度。该算法将这一任务分为两个步骤。首先，算法离线训练一个基于LSTM的合作度检测网络，然后在在线阶段用于实时检测。在在线阶段，代理通过采用稍高合作度的策略来对抗对手。一方面，算法直观上是面向合作的，并在可能的情况下寻求互惠合作。另一方面，算法也能抵抗自私的利用，并在必要时采用背叛策略以避免被利用。
- en: Unlike [[35](#bib.bib35)] which considers a repeated normal form game with complete
    information, in [[36](#bib.bib36)], the authors introduce an application of DRL
    for extensive form games with imperfect information. In particular, the authors
    in [[36](#bib.bib36)] introduce Neural Fictitious Self-Play (NFSP), a DRL method
    for learning approximate Nash equilibria of imperfect-information games. NFSP
    combines FSP with neural network function approximation. An NFSP agent has two
    neural networks. The first network is trained by reinforcement learning from memorized
    experience of play against fellow agents. This network learns an approximate best
    response to the historical behaviour of other agents. The second network is trained
    by supervised learning from memorized experience of the agent’s own behaviour.
    This network learns a model that averages over the agent’s own historical strategies.
    The agent behaves according to a mixture of its average strategy and best response
    strategy.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与[[35](#bib.bib35)]所考虑的具有完全信息的重复标准形式游戏不同，在[[36](#bib.bib36)]中，作者介绍了DRL在不完全信息的扩展形式游戏中的应用。特别地，[[36](#bib.bib36)]中的作者介绍了神经虚拟自我博弈（NFSP），这是一种用于学习不完全信息游戏的近似纳什均衡的DRL方法。NFSP将FSP与神经网络函数逼近相结合。一个NFSP代理有两个神经网络。第一个网络通过强化学习从记忆中的对抗行为经验中进行训练，学习对其他代理的历史行为的近似最佳反应。第二个网络通过监督学习从代理自身行为的记忆经验中进行训练，学习一个平均代理历史策略的模型。代理的行为是其平均策略和最佳反应策略的混合。
- en: In the NSFP, all players of the game are controlled by separate NFSP agents
    that learn from simultaneous play against each other, i.e., self-play. An NFSP
    agent interacts with its fellow agents and memorizes its experience of game transitions
    and its own best response behaviour in two memories, $\mathcal{M}_{RL}$ and $\mathcal{M}_{SL}$.
    NFSP treats these memories as two distinct datasets suitable for DRL and supervised
    classification, respectively. The agent trains a neural network, $Q(s,a;\boldsymbol{\theta}^{Q})$,
    to predict action values from data in $\mathcal{M}_{RL}$ using off-policy reinforcement
    learning. The resulting network defines the agent’s approximate best response
    strategy, $\beta=\epsilon$-greedy($Q$), which selects a random action with probability
    $\epsilon$ and otherwise chooses the action that maximizes the predicted action
    values. The agent trains a separate neural network $\Pi(s,a;\boldsymbol{\theta}^{\Pi})$
    to imitate its own past best response behavior by using supervised classification
    on the data in $\mathcal{M}_{SL}$. NFSP also makes use of two technical innovations
    in order to ensure the stability of the resulting algorithm as well as to enable
    simultaneous self-play learning. Through experimental results, the authors show
    that the NFSP can converge to approximate Nash equilibria in a small poker game.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在NSFP中，游戏中的所有玩家都由各自的NFSP代理控制，这些代理通过相互对抗的自我博弈进行学习。一个NFSP代理与其其他代理进行互动，并将游戏过渡的经验和自身最佳反应行为记忆在两个记忆库中，即$\mathcal{M}_{RL}$和$\mathcal{M}_{SL}$。NFSP将这些记忆视为两个适用于DRL和监督分类的不同数据集。代理通过离线策略强化学习，从$\mathcal{M}_{RL}$中的数据中预测动作值，并训练一个神经网络$Q(s,a;\boldsymbol{\theta}^{Q})$。得到的网络定义了代理的近似最佳反应策略$\beta=\epsilon$-贪婪($Q$)，它以概率$\epsilon$选择随机动作，否则选择最大化预测动作值的动作。代理通过对$\mathcal{M}_{SL}$中的数据进行监督分类，训练另一个神经网络$\Pi(s,a;\boldsymbol{\theta}^{\Pi})$，以模仿自身过去的最佳反应行为。NFSP还利用了两项技术创新，以确保所得到的算法的稳定性，并实现同时的自我博弈学习。通过实验结果，作者展示了NFSP能够在小型扑克游戏中收敛到近似的纳什均衡。
- en: 'Summary: In this section, we have presented the basics of reinforcement learning,
    deep learning, and DQL. Furthermore, we have discussed various advanced DQL techniques
    and their extensions. Different DQL techniques can be used to solve different
    problems in different network scenarios. In the next sections, we review DQL related
    works for various problems in communications and networking.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：在本节中，我们介绍了强化学习、深度学习和DQL的基本知识。此外，我们讨论了各种先进的DQL技术及其扩展。不同的DQL技术可以用于解决不同网络场景中的不同问题。在接下来的章节中，我们将回顾与通信和网络中的各种问题相关的DQL工作。
- en: III Network Access and Rate Control
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 网络接入与速率控制
- en: 'Modern networks such as IoT become more decentralized and ad-hoc in nature.
    In such networks, entities such as sensors and mobile users need to make independent
    decisions, e.g., channel and base station selections, to achieve their own goals,
    e.g., throughput maximization. However, this is challenging due to the dynamic
    and the uncertainty of network status. Learning algorithms such as DQL allow to
    learn and build knowledge about the networks that are used to enable the network
    entities to make their optimal decisions. In this section, we review the applications
    of DQL for the following issues:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现代网络如物联网（IoT）变得更加去中心化和临时。在这样的网络中，像传感器和移动用户这样的实体需要做出独立的决策，例如频道和基站选择，以实现其目标，例如吞吐量最大化。然而，由于网络状态的动态性和不确定性，这是一项具有挑战性的任务。学习算法如
    DQL 可以用来学习和建立关于网络的知识，从而使网络实体能够做出最优决策。在本节中，我们回顾了 DQL 在以下问题中的应用：
- en: •
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dynamic spectrum access: Dynamic spectrum access allows users to locally select
    channels to maximize their throughput. However, the users may not have full observations
    of the system, e.g., channel states. Thus, DQL can be used as an effective tool
    for dynamic spectrum access.'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动态频谱访问：动态频谱访问允许用户本地选择频道以最大化其吞吐量。然而，用户可能无法完全观察系统，例如频道状态。因此，DQL 可以作为动态频谱访问的有效工具。
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Joint user association and spectrum access: User association is implemented
    to determine which user to be assigned to which Base Station (BS). The joint user
    association and spectrum access problems are studied in [[37](#bib.bib37)] and
    [[38](#bib.bib38)]. However, the problems are typically combinatorial and non-convex
    which require nearly complete and accurate network information to obtain the optimal
    strategy. DQL is able to provide distributed solutions which can be effectively
    used for the problems.'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联合用户关联和频谱访问：用户关联是为了确定将哪个用户分配给哪个基站（BS）。联合用户关联和频谱访问问题在[[37](#bib.bib37)]和[[38](#bib.bib38)]中进行了研究。然而，这些问题通常是组合的和非凸的，需要几乎完整和准确的网络信息来获得最优策略。DQL
    能够提供分布式解决方案，可有效用于这些问题。
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adaptive rate control: This refers to bitrate/data rate control in dynamic
    and unpredictable environments such as Dynamic Adaptive Streaming over HTTP (DASH).
    Such a system allows clients or users to independently choose video segments with
    different bitrates to download. The client’s objective is to maximize its Quality
    of Experience (QoE). DQL can be adopted to effectively solve the problem instead
    of dynamic programming which has high complexity and demands complete information.'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自适应速率控制：这指的是在动态和不可预测的环境中，如动态自适应流媒体（DASH），的比特率/数据速率控制。这样的系统允许客户端或用户独立选择不同比特率的视频片段进行下载。客户端的目标是最大化其体验质量（QoE）。DQL
    可以被采用来有效解决这个问题，而不是复杂性高且需要完整信息的动态编程。
- en: III-A Network Access
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 网络访问
- en: This section discusses how to use DQL to solve the spectrum access and user
    association in networks.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了如何使用 DQL 解决网络中的频谱访问和用户关联问题。
- en: III-A1 Dynamic Spectrum Access
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 动态频谱访问
- en: The authors in [[39](#bib.bib39)] propose a dynamic channel access scheme of
    a sensor based on the DQL for IoT. At each time slot, the sensor selects one of
    $M$ channels for transmitting its packet. The channel state is either in low interference,
    i.e., successful transmission, or in high interference, i.e., transmission failure.
    Since the sensor only knows the channel state after selecting the channel, the
    sensor’s optimization decision problem can be formulated as a POMDP. In particular,
    the action of sensor is to select one of $M$ channels. The sensor receives a positive
    reward “+1” if the selected channel is in low interference, and a negative reward
    “-1” otherwise. The objective is to find an optimal policy which maximizes the
    sensor’s the expected accumulated discounted reward over time slots. A DQN¹¹1Remind
    that DQN is the core of the DQL algorithms. using FNN with experience replay [[40](#bib.bib40)]
    is then adopted to find the optimal policy. The input of the DQN is a state of
    the sensor which is the combination of actions and observations, i.e., the rewards,
    in the past time slots. The output includes Q-values corresponding to the actions.
    To balance the exploration of the current best Q-value with the exploration of
    the better one, the $\epsilon$-greedy policy is adopted for the action selection
    mechanism. The simulation results based on real data from [[41](#bib.bib41)] show
    that the proposed scheme can achieve the average accumulated reward close to the
    myopic policy [[42](#bib.bib42)] without a full knowledge of the system.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在[[39](#bib.bib39)]中提出了一种基于DQL的传感器动态信道访问方案。在每个时间槽，传感器选择$M$个信道中的一个用于传输其数据包。信道状态可以是低干扰，即成功传输，或高干扰，即传输失败。由于传感器只能在选择信道后知道信道状态，因此传感器的优化决策问题可以被表述为POMDP。特别地，传感器的动作是选择$M$个信道中的一个。如果所选信道处于低干扰状态，传感器会收到正奖励“+1”，否则则收到负奖励“-1”。目标是找到一个能够最大化传感器在时间槽上期望累计折扣奖励的最佳策略。然后采用使用FNN和经验回放的DQN¹¹1提醒DQN是DQL算法的核心来找到最佳策略。DQN的输入是传感器的状态，该状态是过去时间槽中动作和观测，即奖励的组合。输出包括与动作对应的Q值。为了平衡当前最佳Q值的探索与更好Q值的探索，采用$\epsilon$-贪心策略作为动作选择机制。基于[[41](#bib.bib41)]的实际数据的仿真结果表明，所提出的方案可以在没有系统完整知识的情况下实现接近短视策略[[42](#bib.bib42)]的平均累计奖励。
- en: '[[39](#bib.bib39)] can be considered to be a pioneer work using the DQL for
    the channel access. However, the DQL keeps following the learned policy over time
    slots and stops learning a suitable policy. Actual IoT environments are dynamic,
    and the DQN in the DQL needs to be re-trained. An adaptive DQL scheme is proposed
    in [[43](#bib.bib43)] which evaluates the accumulated reward of the current policy
    for every period. When the reward is reduced by a given threshold, the DQN is
    re-trained to find a new good policy. The simulation results [[43](#bib.bib43)]
    show that when the states of the channels change, the adaptive DQL scheme can
    detect the change and start re-learning to obtain the high reward.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[[39](#bib.bib39)]可以被视为使用DQL进行信道访问的开创性工作。然而，DQL在时间槽上持续跟随学习到的策略，并停止学习合适的策略。实际的物联网环境是动态的，DQL中的DQN需要重新训练。在[[43](#bib.bib43)]中提出了一种自适应DQL方案，该方案评估当前策略的累计奖励。当奖励减少到给定阈值时，DQN会重新训练以找到一个新的良好策略。仿真结果[[43](#bib.bib43)]显示，当信道状态发生变化时，自适应DQL方案可以检测到变化并开始重新学习以获得高奖励。'
- en: '![Refer to caption](img/227cde3560faff5b3756c0c3109f5119.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/227cde3560faff5b3756c0c3109f5119.png)'
- en: 'Figure 5: Joint channel selection and packet forwarding in IoT.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：物联网中的联合信道选择和数据包转发。
- en: 'The models in [[39](#bib.bib39)] and [[43](#bib.bib43)] are constrained to
    only one sensor. Consider a multi-sensor scenario, the authors in [[44](#bib.bib44)]
    address the joint channel selection and packet forwarding using the DQL. The model
    is shown in Fig. [5](#S3.F5 "Figure 5 ‣ III-A1 Dynamic Spectrum Access ‣ III-A
    Network Access ‣ III Network Access and Rate Control ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey") in which one sensor as a
    relay forwards packets received from its neighboring sensors to the sink. The
    sensor is equipped with a buffer to store the received packets. At each time slot,
    the sensor selects a set of channels for the packet forwarding so as to maximize
    its utility, i.e., the ratio of the number of transmitted packets to the transmit
    power. Similar to [[39](#bib.bib39)], the sensor’s problem can be formulated as
    an MDP. The action is to select a set of channels, the number of packets transmitted
    on the channels, and a modulation mode. To avoid packet loss, the state is defined
    as the combination of the buffer state and channel state. The MDP is then solved
    by the DQL in which the input is the state and the output is the action selection.
    The DQL uses the stacked autoencoder to reduce the massive calculation and storage
    in the Q-learning phase. The sensor’s utility function is proved to be bounded
    which can guarantee the convergence of the algorithm. As shown in the simulation
    results, the proposed scheme can converge after a certain number of iterations.
    Also, the proposed scheme significantly improves the system utility compared with
    the random action selection scheme. However, as the packet arrival rate increases,
    the system utility of the proposed scheme decreases since the sensor needs to
    consume more power to transmit all packets.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[[39](#bib.bib39)]和[[43](#bib.bib43)]中的模型仅限于一个传感器。考虑到多传感器场景，[[44](#bib.bib44)]的作者利用DQL解决了联合信道选择和数据包转发的问题。模型如图[5](#S3.F5
    "Figure 5 ‣ III-A1 Dynamic Spectrum Access ‣ III-A Network Access ‣ III Network
    Access and Rate Control ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")所示，其中一个传感器作为中继，将其邻近传感器收到的数据包转发到汇聚点。传感器配备了一个缓冲区来存储收到的数据包。在每个时间槽，传感器选择一组信道进行数据包转发，以最大化其效用，即传输的数据包数量与传输功率的比率。类似于[[39](#bib.bib39)]，传感器的问题可以被表述为MDP。动作是选择一组信道、在信道上传输的数据包数量以及调制模式。为避免数据包丢失，状态被定义为缓冲区状态和信道状态的组合。然后通过DQL解决MDP，其中输入是状态，输出是动作选择。DQL使用堆叠自编码器来减少Q学习阶段的大量计算和存储。传感器的效用函数被证明是有界的，这可以保证算法的收敛。如模拟结果所示，所提出的方案在经过一定数量的迭代后可以收敛。同时，与随机动作选择方案相比，所提出的方案显著提高了系统效用。然而，随着数据包到达率的增加，所提出方案的系统效用下降，因为传感器需要消耗更多的功率来传输所有的数据包。'
- en: Consuming more power leads to poor sensor’s performance due to its energy constraint,
    i.e., a shorter IoT system lifetime. The channel access problem in the energy
    harvesting-enabled IoT system is investigated in [[45](#bib.bib45)]. The model
    consists of one BS and energy harvesting-based sensors. The BS as a controller
    allocates channels to the sensors. However, the uncertainty of ambient energy
    availability at the sensors may make the channel allocation inefficient. For example,
    the channel allocated to the sensor with low available energy may not be fully
    utilized since the sensor cannot communicate later.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 消耗更多的功率会导致传感器性能差，因为其能源受限，即更短的物联网系统寿命。[[45](#bib.bib45)]中研究了能量收集支持的物联网系统中的信道访问问题。该模型由一个基站和基于能量收集的传感器组成。基站作为控制器为传感器分配信道。然而，传感器环境能量的可用性不确定性可能使信道分配效率低下。例如，分配给能量较低的传感器的信道可能无法充分利用，因为该传感器无法后续进行通信。
- en: Therefore, the BS’s problem is to predict the sensors’ battery states and select
    sensors for the channel access so as to maximize the total rate. Since the sensors
    are distributed randomly over a geographical area, the complete statistical knowledge
    of the system dynamics, e.g., the battery states and channel states, may not be
    available. Thus, the DQL is used to solve the problem of the BS, i.e., the agent.
    The DQL uses a DQN consisting of two LSTM-based neural network layers. The first
    layer generates the predicted battery states of sensors, and the second layer
    uses the predicted states along with Channel State Information (CSI) to determine
    the channel access policy. The state space consists of (i) channel access scheduling
    history, (ii) the history of predicted battery information, (iii) the history
    of the true battery information, and (iv) the current CSI of the sensors. The
    action space contains all sets of sensors to be selected for the channel access,
    and the reward is the difference between the total rate and the prediction error.
    As shown in the simulation results, the proposed scheme outperforms the myopic
    policy [[42](#bib.bib42)] in terms of total rate. Moreover, the battery prediction
    error obtained from the proposed scheme is close to zero.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基站的问题是预测传感器的电池状态，并选择传感器进行频道访问，以最大化总速率。由于传感器在地理区域内随机分布，可能无法获得系统动态的完整统计知识，例如电池状态和频道状态。因此，使用DQL来解决基站的问题，即代理。DQL使用一个由两个基于LSTM的神经网络层组成的DQN。第一层生成传感器的预测电池状态，第二层利用预测状态和频道状态信息（CSI）来确定频道访问策略。状态空间包括（i）频道访问调度历史，（ii）预测电池信息的历史，（iii）真实电池信息的历史，以及（iv）当前的传感器CSI。动作空间包含所有可选的传感器集合，用于频道访问，奖励是总速率与预测误差之间的差异。如仿真结果所示，所提出的方案在总速率方面优于短视策略[[42](#bib.bib42)]。此外，所提出方案得到的电池预测误差接近零。
- en: The above schemes, e.g., [[39](#bib.bib39)] and [[45](#bib.bib45)], focus on
    the rate maximization. In IoT systems such as Vehicle-to-Vehicle (V2V) communications,
    latency also needs to be considered due to the mobility of V2V transmitters/receivers
    and vital applications in the traffic safety. One of the problems of each V2V
    transmitter is to select a channel and a transmit power level to maximize its
    capacity under a latency constraint. Given the decentralized network, a DQN is
    adopted to make optimal decisions as proposed in [[46](#bib.bib46)]. The model
    consists of V2V transmitters, i.e., agents, which share a set of channels. The
    actions of each V2V transmitter include choosing channels and transmit power levels.
    The reward is a function of the V2V transmitter’s capacity and latency. The state
    observed by the V2V transmitter consists of (i) the instantaneous CSI of the corresponding
    V2V link, (ii) the interference to the V2V link in the previous time slot, (iii)
    the channels selected by the V2V transmitter’ neighbors in the previous time slot,
    and (iv) the remaining time to meet the latency constraint. The state is also
    an input of the DQN. The output includes Q-values corresponding to the actions.
    As shown in the simulation results, by dynamically adjusting the power and channel
    selection when V2V links are likely to violate the latency constraint, the proposed
    scheme has more V2V transmitters meeting the latency constraint compared with
    the random channel allocation.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方案，例如[[39](#bib.bib39)]和[[45](#bib.bib45)]，侧重于速率最大化。在物联网系统中，如车对车（V2V）通信，由于V2V发射机/接收机的移动性以及交通安全中的关键应用，还需要考虑延迟。每个V2V发射机的问题是选择一个频道和传输功率级别，以在延迟约束下最大化其容量。鉴于去中心化网络，采用了DQN来做出最优决策，如[[46](#bib.bib46)]中所提。该模型由V2V发射机组成，即代理，它们共享一组频道。每个V2V发射机的动作包括选择频道和传输功率级别。奖励是V2V发射机容量和延迟的函数。V2V发射机观察到的状态包括（i）相应V2V链路的瞬时CSI，（ii）上一个时间槽中对V2V链路的干扰，（iii）上一个时间槽中V2V发射机邻居选择的频道，以及（iv）满足延迟约束的剩余时间。状态也是DQN的输入。输出包括对应于动作的Q值。如仿真结果所示，通过动态调整功率和频道选择，当V2V链路可能违反延迟约束时，所提出的方案使得更多V2V发射机满足延迟约束，相比于随机频道分配，效果更佳。
- en: To reduce spectrum cost, the above IoT systems often use unlicensed channels.
    However, this may cause the interference to existing networks, e.g., WLANs. The
    authors in [[47](#bib.bib47)] propose to use the DQN to jointly address the dynamic
    channel access and interference management. The model consists of Small Base Stations
    (SBSs) which share unlicensed channels in an LTE network. At each time slot, the
    SBS selects one of channels for transmitting its packet. However, there may be
    WLAN traffics on the selected channel, and thus the SBS accesses the selected
    channel with a probability. The actions of the SBS include pairs of channel selection
    and channel access probability. The problem of the SBS is to determine an action
    vector so as to maximize its total throughput, i.e., its utility, over all channels
    and time slots. The resource allocation problem can be formulated as a non-cooperative
    game, and the DQN using LSTM can be adopted to solve the game. The input of the
    DQN is the history traffic of the SBSs and the WLAN on the channels. The output
    includes predicted action vectors of the SBSs. The utility function of each SBS
    is proved to be convex, and thus the DQN-based algorithm converges to a Nash equilibrium
    of the game. The simulation results based on real traffic data from [[48](#bib.bib48)]
    show that the proposed scheme can improve the average throughput up to 28% compared
    with the standard Q-learning [[15](#bib.bib15)]. Moreover, deploying more SBSs
    in the LTE network does not allow more airtime fraction for the network. This
    implies that the proposed scheme can avoid causing performance degradation to
    the WLAN. However, the proposed scheme requires synchronization between the SBSs
    and the WLAN which is challenging in real networks.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低频谱成本，上述物联网系统通常使用未授权频道。然而，这可能会对现有网络（如WLAN）造成干扰。文献[[47](#bib.bib47)]提出使用DQN来共同解决动态频道访问和干扰管理的问题。该模型由在LTE网络中共享未授权频道的小型基站（SBSs）组成。在每个时间槽中，SBS选择一个频道来传输数据包。然而，所选频道上可能存在WLAN流量，因此SBS以一定概率访问所选频道。SBS的动作包括频道选择和频道访问概率的组合。SBS的问题是确定一个动作向量，以便最大化其总吞吐量，即其效用，涵盖所有频道和时间槽。资源分配问题可以被形式化为一个非合作博弈，DQN结合LSTM可以用来解决这个博弈。DQN的输入是SBS和WLAN在频道上的历史流量。输出包括SBS的预测动作向量。每个SBS的效用函数被证明是凸的，因此基于DQN的算法收敛到博弈的纳什均衡。基于文献[[48](#bib.bib48)]的实际流量数据的仿真结果显示，所提方案相比标准Q-learning
    [[15](#bib.bib15)]能将平均吞吐量提高至28%。此外，在LTE网络中部署更多的SBS不会增加网络的空中时间比例。这意味着所提方案可以避免对WLAN性能造成降级。然而，该方案需要SBS和WLAN之间的同步，这在实际网络中具有挑战性。
- en: In the same cellular network context, the authors in [[22](#bib.bib22)] address
    the dynamic spectrum access problem for multiple users sharing $K$ channels. At
    a time slot, the user selects a channel with a certain attempt probability or
    chooses not to transmit at all. The state is the history of the user’s actions
    and its local observations, and the user’s strategy is mapping from the history
    to an attempt probability. The problem of the user is to find a vector of the
    strategies, i.e., the policy, over time slots to maximize its expected accumulated
    discounted data rate of the user.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同的蜂窝网络背景下，文献[[22](#bib.bib22)]解决了多个用户共享$K$个频道的动态频谱访问问题。在一个时间槽中，用户以一定的尝试概率选择一个频道或选择不进行传输。状态是用户行动的历史记录及其本地观察，用户的策略是从历史记录映射到尝试概率。用户的问题是找到一个策略向量，即政策，以便在时间槽上最大化其预期累计折扣数据速率。
- en: The above problem is solved by training a DQN. The input of the DQN includes
    past actions and the corresponding observations. The output includes estimated
    Q-values of the actions. To avoid the overestimation in the Q-learning, the DDQN [[20](#bib.bib20)]
    is used. Moreover, the dueling DQN [[49](#bib.bib49)] is employed to improve the
    estimated Q-value. The DQN is then offline trained at a base station. Similar
    to [[47](#bib.bib47)], the multichannel random access is modeled as a non-cooperative
    game. As proved in [[22](#bib.bib22)], the game has a subgame perfect equilibrium.
    Note that some users can keep increasing their attempt probability to increase
    their rates. This makes the equilibrium point inefficient, and thus the strategy
    space of the users is restricted to avoid the situation. The simulation results
    show that the proposed scheme can achieve twice the channel throughput compared
    with the slotted-Aloha [[50](#bib.bib50)]. The reason is that in the proposed
    scheme, each user only learns from its local observation without an online coordination
    or carrier sensing. However, the proposed scheme requires the central unit which
    may raise the message exchanges as the training is frequently updated.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 上述问题是通过训练 DQN（深度 Q 网络）解决的。DQN 的输入包括过去的动作和相应的观察值。输出包括对这些动作的 Q 值估计。为了避免 Q 学习中的过度估计，使用了
    DDQN [[20](#bib.bib20)]。此外，采用了对抗 DQN [[49](#bib.bib49)] 以改进 Q 值估计。然后，DQN 在基站处进行离线训练。类似于
    [[47](#bib.bib47)]，多通道随机接入被建模为一个非合作博弈。如 [[22](#bib.bib22)] 所证明，该博弈具有子博弈完美均衡。注意到一些用户可以不断增加他们的尝试概率以提高他们的速率。这使得均衡点变得低效，因此用户的策略空间被限制以避免这种情况。模拟结果显示，所提出的方案可以实现比时隙
    Aloha [[50](#bib.bib50)] 高出两倍的信道吞吐量。原因在于，在所提出的方案中，每个用户仅从其本地观察中学习，而不需要在线协调或载波感知。然而，所提出的方案需要中央单元，这可能会增加消息交换，因为训练是频繁更新的。
- en: In the aforementioned models, the number of users is fixed in all time slots,
    and the arrival of new users is not considered. The authors in [[51](#bib.bib51)]
    address the channel allocation to new arrival users in a multibeam satellite system.
    The multibeam satellite system generates a geographical footprint subdivided into
    multiple beams which provide services to ground User Terminals (UTs). The system
    has a set of channels. If there exist available channels, the system allocates
    a channel to the new arrived UT, i.e., the new service is satisfied. Otherwise,
    the service is blocked. The system’s problem is to find a channel allocation decision
    to minimize the total service blocking probability of the new UT over time slots
    without causing the interference to the current UTs.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述模型中，用户数量在所有时间槽中是固定的，并且没有考虑新用户的到来。[[51](#bib.bib51)] 的作者处理了多波束卫星系统中新到达用户的信道分配问题。多波束卫星系统生成一个地理覆盖区域，该区域被细分为多个波束，这些波束向地面用户终端
    (UTs) 提供服务。该系统具有一组信道。如果存在可用的信道，系统将分配一个信道给新到达的 UT，即满足新的服务。否则，服务将被阻塞。系统的问题是找到一种信道分配决策，以在不干扰当前
    UT 的情况下最小化新 UT 在时间槽中的总服务阻塞概率。
- en: The system’s problem can be viewed as a temporal correlated sequential decision-making
    optimization problem which is effectively solved by the DQN. Here, the satellite
    system is the agent. The action is an index indicating which channel is allocated
    to the new arrived UT. The reward is positive when the new service is satisfied
    and is negative when the service is blocked. The state includes the set of current
    UTs, the current channel allocation matrix, and the new arrived UT. Note that
    the state has the spatial correlation feature due to the co-channel interference,
    and thus it can be represented in an image-like fashion, i.e., an image tensor.
    Therefore, the DQN adopts the CNN to extract useful features of the state. The
    simulation results show that the proposed DQN algorithm converges after a certain
    number of training steps. Also, by allocating available channels to the new arrived
    UTs, the proposed scheme can improve the system traffic up to 24.4% compared with
    the fixed channel allocation scheme. However, as the number of current UTs increases,
    the number of available channels is low or even zero. Therefore, the dynamic channel
    allocation decisions of the proposed scheme become meaningless, and the performance
    difference between the two schemes becomes insignificant. For the future work,
    a joint channel and power allocation algorithm based on the DQL can be investigated.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 系统问题可以看作是一个时间相关的顺序决策优化问题，DQN可以有效地解决这个问题。在这里，卫星系统是代理。动作是一个索引，指示将哪个频道分配给新到达的UT。当新服务得到满足时，奖励是正的；当服务被阻塞时，奖励是负的。状态包括当前UT的集合、当前频道分配矩阵以及新到达的UT。注意，由于同频道干扰，状态具有空间相关特征，因此可以以类似图像的方式表示，即图像张量。因此，DQN采用CNN来提取状态的有用特征。模拟结果表明，所提出的DQN算法在一定数量的训练步骤后收敛。此外，通过将可用频道分配给新到达的UT，与固定频道分配方案相比，所提出的方案可以将系统流量提高多达24.4%。然而，随着当前UT数量的增加，可用频道数量变低甚至为零。因此，所提出方案的动态频道分配决策变得毫无意义，两种方案之间的性能差异变得微不足道。对于未来的工作，可以研究基于DQL的联合频道和功率分配算法。
- en: III-A2 Joint User Association and Spectrum Access
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 联合用户关联与频谱接入
- en: The joint user association and spectrum access problems are typically non-convex.
    DQL is able to provide distributed solutions, and thus it can be effectively used
    to solve the problems without requiring complete and accurate network information.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 联合用户关联和频谱接入问题通常是非凸的。DQL能够提供分布式解决方案，因此可以有效地用于解决这些问题，而无需完整和准确的网络信息。
- en: The authors in [[23](#bib.bib23)] consider a HetNet which consists of multiple
    users and BSs including macro base stations and femto base stations. The BSs share
    a set of orthogonal channels, and the users are randomly located in the network.
    The problem of each user is to select one BS and a channel to maximize its data
    rate while guaranteeing that the Signal-to-Interference-plus-Noise Ratio (SINR)
    of the user is higher than a minimum Qualtiy of Service (QoS) requirement. The
    DQL is adopted to solve the problem in which each user is an agent, and its state
    is a vector including QoS states of all users, i.e., the global state. Here, the
    QoS state of the user refers to whether its SINR exceeds the minimum QoS requirement
    or not. At each time slot, the user takes an action. If the QoS is satisfied,
    the user receives utility as its immediate reward. Otherwise, it receives a negative
    reward, i.e., an action selection cost. Note that the cumulative reward of one
    user depends on actions of other users, then the user’s problem can be defined
    as an MDP. Similar to [[22](#bib.bib22)], the DDQN and the dueling DQN are used
    to learn the optimal policy, i.e., the joint BS and channel selections, for the
    user to maximize its cumulative reward. The simulation results from [[23](#bib.bib23)]
    show that the proposed scheme outperforms the Q-learning implemented in [[15](#bib.bib15)]
    in terms of convergence speed and system capacity.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[23](#bib.bib23)]中，作者考虑了一个包含多个用户和基站（包括宏基站和微基站）的异构网络（HetNet）。基站共享一组正交信道，用户在网络中随机分布。每个用户的问题是选择一个基站和一个信道，以最大化其数据速率，同时确保用户的信号干扰加噪声比（SINR）高于最低服务质量（QoS）要求。采用深度Q学习（DQL）来解决这个问题，其中每个用户是一个代理，其状态是包括所有用户QoS状态的向量，即全局状态。在这里，用户的QoS状态指的是其SINR是否超过最低QoS要求。在每个时间槽中，用户采取一个行动。如果QoS得到满足，用户将获得即时奖励作为效用。否则，用户将获得负奖励，即行动选择成本。注意，一个用户的累计奖励取决于其他用户的行动，因此用户的问题可以定义为一个马尔可夫决策过程（MDP）。类似于[[22](#bib.bib22)]，DDQN和对抗DQN用于学习最优策略，即联合基站和信道选择，以最大化用户的累计奖励。[[23](#bib.bib23)]的仿真结果显示，所提出的方案在收敛速度和系统容量方面优于[[15](#bib.bib15)]中实现的Q学习。
- en: The scheme proposed in [[23](#bib.bib23)] is considered to be the first work
    using the DQL for the joint user association and spectrum access problem. Inspired
    by this work, the authors in [[52](#bib.bib52)] propose to use the DQL for a joint
    user association, spectrum access, and content caching problem. The network model
    is an LTE network which consists of UAVs serving ground users. The UAVs are equipped
    with storage units and can act as cached-enabled LTE-BSs. The UAVs are able to
    access both licensed and unlicensed bands in the network. The UAVs are controlled
    by a cloud-based server, and the transmissions from the cloud to the UAVs are
    implemented by using the licensed cellular band. The problem of each UAV is to
    determine (i) its optimal user association, (ii) the bandwidth allocation indicators
    on the licensed band, (iii) the time slot indicators on the unlicensed band, and
    (iv) a set of popular contents that the users can request to maximize the number
    of users with stable queue, i.e., users satisfied with content transmission delay.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[[23](#bib.bib23)]中提出的方案被认为是首个使用DQL解决联合用户关联和频谱接入问题的研究。受到此工作的启发，[[52](#bib.bib52)]的作者提出使用DQL解决联合用户关联、频谱接入和内容缓存问题。网络模型是一个由无人机（UAV）服务地面用户的LTE网络。无人机配备有存储单元，可以作为缓存功能的LTE基站。无人机能够访问网络中的许可和非许可频段。无人机由基于云的服务器控制，云到无人机的传输通过使用许可的蜂窝频段来实现。每个无人机的问题是确定（i）其最优用户关联，（ii）许可频段上的带宽分配指标，（iii）非许可频段上的时间槽指标，以及（iv）一组用户可以请求的热门内容，以最大化具有稳定队列的用户数量，即对内容传输延迟感到满意的用户。'
- en: The UAV’s problem is combinatorial and non-convex, and the DQL can be used to
    solve it. The UAVs do not know the users’ content requests, and thus the Liquid
    State Machine approach (LSM) [[53](#bib.bib53)] is adopted to predict the content
    request distribution of the users and to perform resource allocation. In particular,
    predicting the content request distribution is implemented at the cloud based
    on an LSM-based prediction algorithm. Then, given the request distributions, each
    UAV as an agent uses an LSM-based learning algorithm to find its optimal users
    association. Specifically, the input of the LSM-based learning algorithm consists
    of actions, i.e., UAV-user association schemes, that other UAVs take, and the
    output includes the expected numbers of users with stable queues corresponding
    to actions that the UAV can take. After the user association is done, the optimal
    content caching is determined based on the results of [[54](#bib.bib54), Theorem
    2], and the optimal spectrum allocation is done by using linear programming. Based
    on the Gordon’s Theorem [[55](#bib.bib55)], the proposed DQL is proved to converge
    with probability one. The simulation results using content request data from [[56](#bib.bib56)]
    show that the proposed DQL can converge in around 400 iterations. Compared with
    the Q-learning, the proposed DQN improves the convergence time up to 33% . Moreover,
    the proposed DQL significantly improves the number of users with stable queues
    up to 50% compared with the Q-learning without cache. In fact, energy efficiency
    is also important for the UAVs, and thus applying the DQL for a joint user association,
    spectrum access, and power allocation problem needs to be investigated.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机（UAV）面临的问题是组合性和非凸性的，可以使用深度 Q 学习（DQL）来解决。无人机无法知道用户的内容请求，因此采用液态状态机方法（LSM）[[53](#bib.bib53)]来预测用户的内容请求分布并进行资源分配。具体来说，内容请求分布的预测在云端基于
    LSM 的预测算法进行实现。然后，基于请求分布，每个无人机作为代理使用基于 LSM 的学习算法来找到其最优用户关联。具体而言，LSM 基于的学习算法的输入包括其他无人机采取的动作，即无人机-用户关联方案，输出包括与无人机可以采取的动作对应的稳定队列的用户预期数量。在用户关联完成后，基于[[54](#bib.bib54),
    定理 2]的结果确定最优内容缓存，并通过线性规划完成最优频谱分配。基于戈登定理[[55](#bib.bib55)]，提出的 DQL 被证明以概率 1 收敛。使用[[56](#bib.bib56)]中的内容请求数据进行的仿真结果显示，提出的
    DQL 可以在大约 400 次迭代内收敛。与 Q 学习相比，提出的 DQN 将收敛时间提高了 33%。此外，与不使用缓存的 Q 学习相比，提出的 DQL 显著提高了稳定队列用户的数量，达到
    50%。实际上，能效对于无人机来说也很重要，因此需要研究将 DQL 应用于联合用户关联、频谱访问和功率分配问题。
- en: III-B Adaptive Rate Control
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 自适应速率控制
- en: '![Refer to caption](img/4b7c81765a2c8c18dbfe075ff3e16099.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4b7c81765a2c8c18dbfe075ff3e16099.png)'
- en: 'Figure 6: A dynamic adaptive streaming system based on HTTP standard.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：基于 HTTP 标准的动态自适应流媒体系统。
- en: 'Dynamic Adaptive Streaming over HTTP (DASH) becomes the dominant standard for
    video streaming [[57](#bib.bib57)]. DASH is able to leverage existing content
    delivery network infrastructure and is compatible with a multitude of client-side
    applications. A general DASH system is shown in Fig. [6](#S3.F6 "Figure 6 ‣ III-B
    Adaptive Rate Control ‣ III Network Access and Rate Control ‣ Applications of
    Deep Reinforcement Learning in Communications and Networking: A Survey") in which
    the videos are stored in servers as multiple segments, i.e., chunks. Each segment
    is encoded at different compression levels to generate representations with different
    bitrates, i.e., different video visual quality. At each time slot, the client
    chooses a representation, i.e., a segment with a certain bitrate, to download.
    The client’s problem is to find an optimal policy which maximizes its QoE such
    as maximizing average bitrate and minimizing rebuffering, i.e., the time which
    the video playout freezes.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 'HTTP 动态自适应流媒体（DASH）已成为视频流媒体的主流标准 [[57](#bib.bib57)]。DASH 能够利用现有的内容分发网络基础设施，并且与多种客户端应用兼容。图[6](#S3.F6
    "Figure 6 ‣ III-B Adaptive Rate Control ‣ III Network Access and Rate Control
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")展示了一个一般的 DASH 系统，其中视频以多个片段，即块的形式存储在服务器中。每个片段在不同的压缩级别下进行编码，以生成具有不同比特率的表示，即不同的视频视觉质量。在每个时间段，客户端选择一个表示，即具有一定比特率的片段进行下载。客户端面临的问题是找到一个最优策略，以最大化其用户体验（QoE），例如最大化平均比特率和最小化缓冲，即视频播放暂停的时间。'
- en: As presented in [[58](#bib.bib58)], the above problem can be modeled as an MDP
    in which the agent is the client and the action is choosing a representation to
    download. To maximize the QoE, the reward is defined as a function of (i) visual
    quality of the video, (ii) video quality stability, (iii) rebuffering event, and
    (iv) buffer state. Given the reward formulation, the state of the client should
    include (i) the video quality of the last downloaded segment, (ii) the current
    buffer state, (iii) the rebuffering time, and (iv) the channel capacities experienced
    during downloading of segments in the past time slots. The MDP can be solved by
    using dynamic programming, but the computational complexity rapidly becomes unmanageable
    as the size of the problem increases. Thus, the authors in [[58](#bib.bib58)]
    adopt the DQL to solve the problem. Similar to [[45](#bib.bib45)], the LSTM networks
    are used in which the input is the state of the client, and the output includes
    Q-values corresponding to the client’s possible actions. To improve the performance
    of the standard LSTM, peephole connections are added into the LSTM networks. The
    simulation results based on dataset from [[59](#bib.bib59)] show that the proposed
    DQL algorithm can converge much faster than Q-learning. Moreover, the proposed
    DQL improves the video quality and reduces the rebuffering since it is able to
    dynamically manage the buffer by considering the buffer state and channel capacity.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如[[58](#bib.bib58)]所述，上述问题可以建模为一个MDP，其中代理是客户端，动作是选择要下载的表示。为了最大化QoE，奖励被定义为（i）视频的视觉质量，（ii）视频质量稳定性，（iii）重缓冲事件，以及（iv）缓冲状态的函数。根据奖励的定义，客户端的状态应包括（i）最后下载的片段的视频质量，（ii）当前的缓冲状态，（iii）重缓冲时间，以及（iv）在过去时间段下载片段时经历的频道容量。MDP可以通过动态规划求解，但随着问题规模的增加，计算复杂度迅速变得不可管理。因此，[[58](#bib.bib58)]中的作者采用了DQL来解决这个问题。类似于[[45](#bib.bib45)]，使用LSTM网络，其中输入是客户端的状态，输出包括与客户端可能的动作相对应的Q值。为了提高标准LSTM的性能，在LSTM网络中添加了窥视连接。基于[[59](#bib.bib59)]的数据集的仿真结果表明，提出的DQL算法可以比Q学习更快地收敛。此外，提出的DQL改善了视频质量并减少了重缓冲，因为它能够通过考虑缓冲状态和频道容量来动态管理缓冲。
- en: 'The network model and the optimization problem in [[58](#bib.bib58)] are also
    found in [[60](#bib.bib60)]. However, different from [[58](#bib.bib58)], the authors
    in [[60](#bib.bib60)] adopt the Asynchronous Advantage Actor- Critic (A3C) method [[26](#bib.bib26)]
    for the DQL to further enhance and speed up the training. As presented in Section [II-F1](#S2.SS6.SSS1
    "II-F1 Deep Deterministic Policy Gradient Q-Learning for Continuous Action ‣ II-F
    Deep Q-Learning for Extensions of MDPs ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey"), A3C includes two neural networks, namely, actor network and critic
    network. The actor network is to choose bitrates for the client, and the critic
    network helps train the actor network. For the actor network, the input is the
    client’s state, and the output is a policy, i.e., a probability distribution over
    possible actions given states that the client can take. Here, the action is choosing
    the next representation, i.e., the next segment with a certain bitrate, to download.
    For the critic network, the input is the client’s state, and the output is the
    expected total reward when following the policy obtained from the actor network.
    The simulation results based on the mobile dataset from [[61](#bib.bib61)] show
    that the proposed DQL can improve the average QoE up to 25% compared with the
    bitrate control scheme [[62](#bib.bib62)]. Also, by having sufficient buffer to
    handle the network’s throughput fluctuations, the proposed DQL reduces the rebuffering
    around 32.8% compared with the baseline scheme.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '网络模型和优化问题在[[58](#bib.bib58)]中有所讨论，在[[60](#bib.bib60)]中也可以找到。然而，与[[58](#bib.bib58)]不同的是，[[60](#bib.bib60)]的作者采用了**异步优势演员-评论家（A3C）方法**[[26](#bib.bib26)]来增强和加速DQL的训练。如在[II-F1](#S2.SS6.SSS1
    "II-F1 Deep Deterministic Policy Gradient Q-Learning for Continuous Action ‣ II-F
    Deep Q-Learning for Extensions of MDPs ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")节中所示，A3C包含两个神经网络，即演员网络和评论家网络。演员网络用于为客户端选择比特率，评论家网络帮助训练演员网络。对于演员网络，输入是客户端的状态，输出是策略，即在给定状态下客户端可以采取的可能动作的概率分布。这里，动作是选择下一个表示，即带有特定比特率的下一个片段进行下载。对于评论家网络，输入是客户端的状态，输出是根据演员网络获得的策略所期望的总奖励。基于[[61](#bib.bib61)]的移动数据集的仿真结果显示，所提出的DQL相比比特率控制方案[[62](#bib.bib62)]可以提高平均QoE达25%。此外，通过拥有足够的缓冲区来处理网络吞吐量波动，所提出的DQL与基准方案相比，减少了约32.8%的重新缓冲。'
- en: In practice, the DQL algorithm proposed in [[60](#bib.bib60)] can be easily
    deployed in a multi-client network since A3C is able to support parallel training
    for multiple agents. Accordingly, each client, i.e., an agent, is configured to
    observe its reward. Then, the client sends a tuple including its state, action,
    and reward to a server. The server uses the actor-critic algorithm to update its
    actor network model. The server then pushes the newest model to the agent. This
    update process can happen asynchronously among all agents which improves quality
    and speeds up the training. Although the parallel training scheme may incur a
    Round-Trip Time (RTT) between the clients and the server, the simulation results
    in [[60](#bib.bib60)] show that the RTT between the clients and the server reduces
    the average QoE by only 3.5%. The performance degradation is small, and thus the
    proposed DQL can be implemented in real network systems.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，[[60](#bib.bib60)]中提出的DQL算法可以很容易地在多客户端网络中部署，因为A3C能够支持多个代理的并行训练。因此，每个客户端，即一个代理，都被配置为观察其奖励。然后，客户端将包含其状态、动作和奖励的元组发送到服务器。服务器使用演员-评论家算法来更新其演员网络模型。然后，服务器将最新的模型推送到代理。这个更新过程可以在所有代理之间异步发生，这样可以提高质量并加速训练。尽管并行训练方案可能会导致客户端与服务器之间的往返时间（RTT），但[[60](#bib.bib60)]中的仿真结果显示，客户端与服务器之间的RTT仅使平均QoE降低了3.5%。性能下降很小，因此所提出的DQL可以在实际网络系统中实施。
- en: In [[58](#bib.bib58)] and [[60](#bib.bib60)], the input of the DQL, i.e., the
    client’s state, includes the video quality of the last downloaded video segment.
    The video segment is raw which may cause “state explosion” to the state space [[63](#bib.bib63)].
    To reduce the state space and to improve the QoE, the authors in [[63](#bib.bib63)]
    propose to use a video quality prediction network. The prediction network extracts
    useful features from the raw video segments using CNN and RNN. Then, the output
    of the prediction network, i.e., the predicted video quality, is used as one of
    the inputs of the DQL which is proposed in [[60](#bib.bib60)]. The simulation
    results based on the broadband dataset from [[64](#bib.bib64)] show that the proposed
    DQL can improve the average QoE up to 25% compared with the Google Hangout, i.e.,
    a communication platform developed by Google. Moreover, the proposed DQL can reduce
    the average latency of video transmission around 45% due to the small state space.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[58](#bib.bib58)]和[[60](#bib.bib60)]中，DQL的输入，即客户端的状态，包括最后下载的视频片段的画质。视频片段是原始的，这可能导致状态空间的“状态爆炸”[[63](#bib.bib63)]。为了减少状态空间并提高QoE，[[63](#bib.bib63)]中的作者建议使用视频质量预测网络。预测网络利用CNN和RNN从原始视频片段中提取有用的特征。然后，预测网络的输出，即预测的视频质量，被用作DQL的输入之一，这在[[60](#bib.bib60)]中提出。基于[[64](#bib.bib64)]的宽带数据集的仿真结果显示，提出的DQL与Google
    Hangout（即Google开发的通信平台）相比，可以将平均QoE提高多达25%。此外，由于状态空间较小，提出的DQL还可以将视频传输的平均延迟减少约45%。
- en: Apart from the DASH systems, the DQL can be effectively used for the rate control
    in High Volume Flexible Time (HVFT) applications. HVFT applications use cellular
    networks to deliver IoT traffic. The HVFT applications have a large volume of
    traffic, and the traffic scheduling, e.g., data rate control, in the HVFT applications
    is necessary. One common approach is to assign static priority classes per traffic
    type, and then traffic scheduling is based on its priority class. However, such
    an approach does not evolve to accommodate new traffic classes. Thus, learning
    methods such as DQL should be used to provide adaptive rate control mechanisms
    as proposed in [[65](#bib.bib65)]. The network model is a single cell including
    one BS as a central controller and multiple mobile users. The problem at the BS
    is to find a proper policy, i.e., data rate for the users, to maximize the amount
    of transmitted HVFT traffic while minimizing performance degradation to existing
    data traffics. It is shown in [[65](#bib.bib65)] that the problem can be formulated
    as an MDP. The agent is the BS, and the state includes the current network state
    and the useful features extracted from network states in the past time slots.
    The network state at a time slot includes (i) the congestion metric, i.e., the
    cell’s traffic load, at the time slot, (ii) the total number of network connections,
    and (iii) the cell efficiency, i.e., the cell quality. The action that the BS
    takes is a combination of the traffic rate for the users. To achieve the BS’ objective,
    the reward is defined as a function of (i) the sum of HVFT traffic, (ii) traffic
    loss to existing applications due to the presence of the HVFT traffic, and (iii)
    the amount of bytes served below desired minimum throughput. The DQL using the
    actor and critic networks with LSTM is then adopted. By using the real network
    data collected in Melbourne, the simulation results show that the proposed DQL
    increases the HVFT traffic up to 2 times compared with the heuristic control scheme.
    However, how the proposed scheme reduces the traffic loss is not shown.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 除了DASH系统外，DQL还可以有效用于高容量灵活时间（HVFT）应用中的速率控制。HVFT应用利用蜂窝网络来传输物联网流量。HVFT应用有大量的流量，且在HVFT应用中需要进行流量调度，例如数据速率控制。一种常见的方法是为每种流量类型分配静态优先级类别，然后根据优先级类别进行流量调度。然而，这种方法不能适应新的流量类别。因此，应使用诸如DQL之类的学习方法，以提供自适应速率控制机制，如[[65](#bib.bib65)]所提议的那样。网络模型是一个单一的小区，包括一个基站（BS）作为中央控制器和多个移动用户。BS面临的问题是找到合适的策略，即为用户设定的数据速率，以最大化传输的HVFT流量，同时将对现有数据流量的性能降级最小化。[[65](#bib.bib65)]中显示，该问题可以被表述为一个MDP。代理是BS，状态包括当前网络状态和从过去时间段中提取的有用特征。一个时间段内的网络状态包括（i）该时间段的拥塞指标，即小区的流量负载，（ii）网络连接的总数，以及（iii）小区效率，即小区质量。BS采取的行动是用户流量速率的组合。为了实现BS的目标，奖励被定义为（i）HVFT流量的总和，（ii）由于HVFT流量存在而导致对现有应用的流量损失，以及（iii）低于期望最小吞吐量的服务字节量。然后采用了使用LSTM的演员和评论家网络的DQL。通过使用在墨尔本收集的真实网络数据，模拟结果表明，所提出的DQL将HVFT流量提高了最多2倍，相比之下，启发式控制方案的表现较差。然而，提出的方案如何减少流量损失尚未显示。
- en: In the aforementioned approaches, the maximum number of objectives is constrained,
    e.g., to 3 in [[66](#bib.bib66)]. The authors in [[67](#bib.bib67)] show that
    the DQL can be used for the rate control to achieve multiple objectives in complex
    communication systems. The network model is a future space communication system
    which is expected to operate in unpredictable environments, e.g., orbital dynamics,
    atmospheric and space weather, and dynamic channels. In the system, the transmitter
    needs to be configured with several transmit parameters, e.g., symbol rate and
    encoding rate, to achieve multiple conflict objectives, e.g., low Bit Error Rate
    (BER), throughput improvement, power and spectral efficiency. The adaptive coding
    and modulation schemes, i.e., [[68](#bib.bib68)], can be used. However, the methods
    allow to achieve only limited number objectives. Learning algorithms such as the
    DQL can be thus used. The agent is the transmitter in the system. The action is
    a combination of (i) symbol rate, (ii) energy per symbol, (iii) modulation mode,
    (iv) number of bits per symbol, and (v) encoding rate. The objective is to maximize
    the system performance. Thus, the reward is defined as a fitness function of performance
    parameters including (i) BER estimated at the receiver, (ii) throughput, (iii)
    spectral efficiency, (iv) power consumption, and (v) transmit power efficiency.
    The state is the system performance measured by the transmitter, and thus the
    state is the reward. To achieve multiple objectives, the DQL is implemented by
    using a set of multiple neural networks in parallel. The input of the DQL is the
    current state and the channel conditions, and the output is the predicted action.
    The neural networks are trained by using the Levenberg-Marquardt backpropagation
    algorithm [[69](#bib.bib69)]. The simulation results show that the proposed DQL
    can achieve the fitness score, i.e., the weighted sum of different objectives,
    close to the ideal, i.e., the exhaustive search approach. This implies that the
    DQL is able to select near-optimal actions and learn the relationship between
    rewards and actions given dynamic channel conditions.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方法中，目标数量受到限制，例如在[[66](#bib.bib66)]中限制为3个。[[67](#bib.bib67)]中的作者展示了DQL可以用于速率控制，以在复杂通信系统中实现多个目标。网络模型是一个未来的空间通信系统，预计将在不可预测的环境中运行，例如轨道动力学、大气和空间天气以及动态信道。在系统中，发射器需要配置多个发射参数，例如符号率和编码率，以实现多个冲突目标，例如低比特错误率（BER）、吞吐量提升、功率和频谱效率。可以使用自适应编码和调制方案，即[[68](#bib.bib68)]。然而，这些方法只能实现有限数量的目标。因此，可以使用诸如DQL的学习算法。智能体是系统中的发射器。行动是以下几种的组合：（i）符号率，（ii）每符号能量，（iii）调制模式，（iv）每符号比特数，以及（v）编码率。目标是最大化系统性能。因此，奖励被定义为性能参数的适应度函数，包括（i）在接收器处估计的BER，（ii）吞吐量，（iii）频谱效率，（iv）功耗，以及（v）发射功率效率。状态是由发射器测量的系统性能，因此状态就是奖励。为了实现多个目标，DQL通过使用一组多个神经网络并行实现。DQL的输入是当前状态和信道条件，输出是预测的动作。神经网络使用Levenberg-Marquardt反向传播算法进行训练[[69](#bib.bib69)]。仿真结果显示，提出的DQL可以使适应度分数，即不同目标的加权和，接近理想值，即穷举搜索方法。这表明DQL能够选择接近最优的动作，并学习在动态信道条件下奖励与动作之间的关系。
- en: 'Summary: This section reviews applications of DQL for the dynamic network access
    and adaptive rate control. The reviewed approaches are summarized along with the
    references in Table [III](#S3.T3 "TABLE III ‣ III-B Adaptive Rate Control ‣ III
    Network Access and Rate Control ‣ Applications of Deep Reinforcement Learning
    in Communications and Networking: A Survey"). We observe that the problems are
    mostly modeled as an MDP. Moreover, DQL approaches for the IoT and DASH systems
    receive more attentions than other networks. Future networks, e.g., 5G networks,
    involve multiple network entities with multiple conflicting objectives, e.g.,
    provider’s revenue versus users’ utility maximization. This poses a number of
    challenges to the traditional resource management mechanisms that deserve in-depth
    investigation. In the next section, we review the adoption of DQL for the emerging
    services, i.e., offloading and caching.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '总结：本节回顾了 DQL 在动态网络访问和自适应速率控制中的应用。回顾的方式以及参考文献总结见表 [III](#S3.T3 "TABLE III ‣
    III-B Adaptive Rate Control ‣ III Network Access and Rate Control ‣ Applications
    of Deep Reinforcement Learning in Communications and Networking: A Survey")。我们观察到问题大多被建模为
    MDP。此外，针对物联网（IoT）和 DASH 系统的 DQL 方法比其他网络受到更多关注。未来网络，例如 5G 网络，涉及多个具有相互冲突目标的网络实体，例如提供者的收入与用户效用最大化。这对传统资源管理机制提出了诸多挑战，值得深入研究。在下一节中，我们将回顾
    DQL 在新兴服务（即卸载和缓存）中的应用。'
- en: 'TABLE III: A summary of approaches using DQL for network access and adaptive
    rate control.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：使用 DQL 进行网络访问和自适应速率控制的方法总结。
- en: '| Issues | Ref. | Model | Learning algorithms | Agent | States | Actions |
    Rewards | Networks |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 参考文献 | 模型 | 学习算法 | 代理 | 状态 | 行动 | 奖励 | 网络 |'
- en: '| Network access | [[39](#bib.bib39)] | POMDP | DQN using FNN | Sensor | Past
    channel selections and observations | Channel selection | Score +1 or -1 | IoT
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 网络访问 | [[39](#bib.bib39)] | POMDP | 使用 FNN 的 DQN | 传感器 | 过去的信道选择和观察 | 信道选择
    | 分数 +1 或 -1 | 物联网 |'
- en: '|  | [[44](#bib.bib44)] | MDP | DQN using FNN | Sensor | Current buffer state
    and channel state | Channel, packets, and modulation mode selection | Ratio of
    number of transmitted packets to transmit power | IoT |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | [[44](#bib.bib44)] | MDP | 使用 FNN 的 DQN | 传感器 | 当前缓冲区状态和信道状态 | 信道、数据包和调制方式选择
    | 传输数据包数量与传输功率的比率 | 物联网 |'
- en: '|  | [[45](#bib.bib45)] | MDP | DQN with LSTM | Base station | Channel access
    history, predicted and true battery information history, and current CSI | Sensor
    selection for channel access | Total rate and prediction error | IoT |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | [[45](#bib.bib45)] | MDP | 带 LSTM 的 DQN | 基站 | 信道访问历史、预测和真实电池信息历史以及当前
    CSI | 信道访问的传感器选择 | 总速率和预测误差 | 物联网 |'
- en: '|  | [[46](#bib.bib46)] | MDP | DQN with LSTM | V2V transmitter | Current CSI,
    past interference, past channel selections, and remaining time to meet the latency
    constraints | Channel and transmit power selection | Capacity and latency | IoT
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | [[46](#bib.bib46)] | MDP | 带 LSTM 的 DQN | V2V 发射器 | 当前 CSI、过去的干扰、过去的信道选择和满足延迟约束的剩余时间
    | 信道和传输功率选择 | 容量和延迟 | 物联网'
- en: '|  | [[47](#bib.bib47)] | Game | DQN with LSTM | Small base station | Traffic
    history of small base stations and the WLAN | Channel selection and channel access
    probability | Throughput | LTE network |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | [[47](#bib.bib47)] | 游戏 | 带 LSTM 的 DQN | 小型基站 | 小型基站和 WLAN 的流量历史 | 信道选择和信道访问概率
    | 吞吐量 | LTE 网络 |'
- en: '|  | [[22](#bib.bib22)] | Game | DDQN and dueling DQN | Mobile user | Past
    channel selections and observations | Channel selection | Data rate | CRN |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | [[22](#bib.bib22)] | 游戏 | DDQN 和对抗 DQN | 移动用户 | 过去的信道选择和观察 | 信道选择 | 数据速率
    | CRN |'
- en: '|  | [[51](#bib.bib51)] | MDP | DQN with CNN | Satellite system | Current user
    terminals, channel allocation matrix, and the new arrival user | Channel selection
    | Score +1 or -1 | Satellite system |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | [[51](#bib.bib51)] | MDP | 带 CNN 的 DQN | 卫星系统 | 当前用户终端、信道分配矩阵以及新到达用户 |
    信道选择 | 分数 +1 或 -1 | 卫星系统 |'
- en: '|  | [[23](#bib.bib23)] | MDP | DDQN and dueling DQN | Mobile user | QoS states
    | Base station and channel selection | Utility | HetNet |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | [[23](#bib.bib23)] | MDP | DDQN 和对抗 DQN | 移动用户 | QoS 状态 | 基站和信道选择 | 效用
    | HetNet |'
- en: '|  | [[52](#bib.bib52)] | Game | DQN with LSM | UAV | Content request distribution
    | Base station selection | Users with stable queues | LTE network |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | [[52](#bib.bib52)] | 游戏 | 带 LSM 的 DQN | 无人机 | 内容请求分布 | 基站选择 | 队列稳定的用户
    | LTE 网络 |'
- en: '| Rate control | [[58](#bib.bib58)] | MDP | DQN with LSTM and peephole connections
    | Client | Last segment quality, current buffer state, rebuffering time, and channel
    capacities | Bitrate selection for segment | Video quality, rebuffering even,
    and buffer state | DASH system |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 速率控制 | [[58](#bib.bib58)] | MDP | 使用 LSTM 和窥视连接的 DQN | 客户端 | 最后一个片段的质量、当前缓冲区状态、重新缓冲时间和频道容量
    | 分段的比特率选择 | 视频质量、重新缓冲事件和缓冲区状态 | DASH 系统 |'
- en: '|  | [[60](#bib.bib60)] | MDP | DQN with A3C | Client | Last segment quality,
    current buffer state, rebuffering time, and channel capacities | Bitrate selection
    for segment | Video quality, rebuffering even, and buffer state | DASH system
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | [[60](#bib.bib60)] | MDP | 使用 A3C 的 DQN | 客户端 | 最后一个片段的质量、当前缓冲区状态、重新缓冲时间和频道容量
    | 分段的比特率选择 | 视频质量、重新缓冲事件和缓冲区状态 | DASH 系统 |'
- en: '|  | [[63](#bib.bib63)] | MDP | DQN with CNN and RNN | Client | Predicted video
    quality, current buffer state, rebuffering time, and channel capacities | Bitrate
    selection for segment | Video quality, rebuffering even, and buffer state | DASH
    system |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | [[63](#bib.bib63)] | MDP | 使用 CNN 和 RNN 的 DQN | 客户端 | 预测的视频质量、当前缓冲区状态、重新缓冲时间和频道容量
    | 分段的比特率选择 | 视频质量、重新缓冲事件和缓冲区状态 | DASH 系统 |'
- en: '|  | [[65](#bib.bib65)] | MDP | DQN using A3C and LSTM | Base station | Congestion
    metric, current network connections, and cell efficiency | Traffic rate decisions
    for mobile users | HVFT traffic, traffic loss to existing applications, and the
    amount of served bytes | HVFT application |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | [[65](#bib.bib65)] | MDP | 使用 A3C 和 LSTM 的 DQN | 基站 | 拥塞指标、当前网络连接和小区效率
    | 移动用户的流量速率决策 | HVFT 流量、现有应用的流量损失和服务字节量 | HVFT 应用 |'
- en: '|  | [[67](#bib.bib67)] | MDP | DQN using FNN | Base station | Measurements
    of BER, throughput, spectral efficiency, power consumption, and transmit power
    efficiency | Symbol rate, energy per symbol, modulation mode, number of bits per
    symbol, and encoding rate | Same as the state | Space communication system |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | [[67](#bib.bib67)] | MDP | 使用 FNN 的 DQN | 基站 | BER、吞吐量、频谱效率、功耗和发射功率效率的测量
    | 符号率、每符号能量、调制模式、每符号位数和编码速率 | 与状态相同 | 空间通信系统 |'
- en: IV Caching and Offloading
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 缓存和卸载
- en: As one of the key features of information-centric networking, in-network caching
    can efficiently reduce duplicated content transmissions. The studies on wireless
    caching has shown that access delays, energy consumption, and the total amount
    of traffic can be reduced significantly by caching contents in wireless devices.
    Big data analytics [[70](#bib.bib70)] also demonstrate that with limited cache
    size, proactive caching at network edge nodes can achieve 100% user satisfaction
    while offloading 98% of the backhaul traffic. Joint content caching and offloading
    can address the gap between the mobile users’ large data demands and the limited
    capacities in data storage and processing. This motivates the study on Mobile
    Edge Computing (MEC). By deploying both computational resources and caching capabilities
    close to end users, MEC significantly improves energy efficiency and QoS for applications
    that require intensive computations and low latency. A unified study on caching,
    offloading, networking, and transmission control in MEC scenarios involves very
    complicated system analysis because of strong couplings among mobile users with
    heterogeneities in application demand, QoS provisioning, mobility pattern, radio
    access interface, and wireless resources. A learning-based and model-free approach
    becomes a promising candidate to manage huge state space and optimization variables,
    especially by using DNNs. In this section, we review the modeling and optimization
    of caching and offloading policies in wireless networks by leveraging the DRL
    framework.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 作为信息中心网络的关键特性之一，网络内缓存可以有效减少重复内容传输。关于无线缓存的研究表明，通过在无线设备中缓存内容，可以显著减少访问延迟、能耗和总流量。大数据分析[[70](#bib.bib70)]也展示了，在有限缓存大小的情况下，网络边缘节点的主动缓存可以实现
    100% 的用户满意度，同时卸载 98% 的回程流量。联合内容缓存和卸载可以弥合移动用户的大数据需求与数据存储和处理能力的有限性之间的差距。这激发了对移动边缘计算（MEC）的研究。通过在离终端用户较近的地方部署计算资源和缓存能力，MEC
    显著提高了需要密集计算和低延迟的应用的能效和 QoS。对 MEC 场景中缓存、卸载、网络和传输控制的统一研究涉及非常复杂的系统分析，因为移动用户之间在应用需求、QoS
    提供、移动模式、无线接入接口和无线资源方面的异质性带来了强耦合。基于学习的无模型方法成为了管理庞大状态空间和优化变量的有希望的选择，特别是通过使用 DNN。
    在本节中，我们回顾了通过利用 DRL 框架对无线网络中的缓存和卸载策略进行建模和优化。
- en: IV-A Wireless Proactive Caching
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 无线主动缓存
- en: Wireless proactive caching has attracted great attentions from both academia
    and industry. Statistically, a few popular contents are usually requested by many
    users during a short time span, which accounts for most of the traffic load. Therefore,
    proactively caching popular contents can avoid the heavy traffic burden of the
    backhaul links. In particular, this technique aims at pre-caching the contents
    from the remote content servers at the edge devices or BSs that are close to the
    end users. If the requested contents are already cached locally, the BS can directly
    serve the end users with small delay. Otherwise, the BS requests these contents
    from the original content server and updates the local cache based on the caching
    policy, which is one of the main design problem for wireless proactive caching.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 无线主动缓存引起了学术界和工业界的广泛关注。从统计上看，一些热门内容通常在短时间内被许多用户请求，这占据了大部分的流量负载。因此，主动缓存热门内容可以避免回程链路的沉重流量负担。特别是，这项技术旨在将内容从远程内容服务器预缓存到靠近终端用户的边缘设备或基站（BS）。如果请求的内容已经在本地缓存中，则基站可以以较小的延迟直接为终端用户提供服务。否则，基站从原始内容服务器请求这些内容，并根据缓存策略更新本地缓存，这是无线主动缓存的主要设计问题之一。
- en: IV-A1 QoS-Aware Caching
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 QoS 感知缓存
- en: 'Content popularity is the key factor used to solve the content caching problem.
    With a large number of contents and their time-varying popularities, DQL is an
    attractive strategy to tackle this problem with high-dimensional state and action
    spaces. The authors in [[70](#bib.bib70)] present a DQL scheme to improve the
    caching performance. The system model consists of a single BS with a fixed cache
    size. For each request, the BS as an agent makes a decision on whether or not
    to store the currently requested content in the cache. If the new content is kept,
    the BS determines which local content will be replaced. The state is the feature
    space of the cached contents and the currently requested content. The feature
    space consists of the total number of requests for each content in a specific
    short-, medium-, and long-term. There are two types of actions: (i) to find a
    pair of contents and exchange the cache states of the two contents and (ii) to
    keep the cache states of the contents unchanged. The aim of the BS is to maximize
    the long-term cache hit rate, i.e., reward.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 内容受欢迎程度是解决内容缓存问题的关键因素。由于内容数量庞大且其受欢迎程度随时间变化，DQL 是解决具有高维状态和动作空间的这个问题的一个有吸引力的策略。[[70](#bib.bib70)]
    中的作者提出了一种 DQL 方案来提高缓存性能。系统模型由一个具有固定缓存大小的单一基站组成。对于每个请求，基站作为代理决定是否将当前请求的内容存储在缓存中。如果新内容被保留，基站决定将替换哪些本地内容。状态是缓存内容和当前请求内容的特征空间。特征空间包括在特定的短期、中期和长期内每个内容的总请求数量。有两种类型的动作：（i）找到一对内容并交换这两个内容的缓存状态；（ii）保持内容的缓存状态不变。基站的目标是最大化长期缓存命中率，即奖励。
- en: 'The DQL scheme in [[70](#bib.bib70)] trains the policy by using the DDPG method [[71](#bib.bib71)]
    and employs Wolpertinger architecture [[72](#bib.bib72)] to reduce the size of
    the action space and avoid missing an optimal policy. The Wolpertinger architecture
    consists of three main parts: an actor network, K-Nearest Neighbors (K-NN), and
    a critic network. The actor network is to avoid a large action space. The critic
    network is to correct the decision made by the actor network. The DDPG method
    is applied to update both critic and actor networks. K-NN can help to explore
    a set of actions to avoid poor decisions. The actor and critic networks are then
    implemented by using FNNs. The simulation results show that the proposed DQL scheme
    outperforms the first-in first-out scheme in terms of long-term cache hit rate.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: DQL 方案在 [[70](#bib.bib70)] 中通过使用 DDPG 方法 [[71](#bib.bib71)] 进行策略训练，并采用 Wolpertinger
    架构 [[72](#bib.bib72)] 来减少动作空间的大小，以避免遗漏最佳策略。Wolpertinger 架构由三个主要部分组成：一个演员网络、K 最近邻
    (K-NN) 和一个评论家网络。演员网络用于避免大规模的动作空间。评论家网络用于纠正演员网络所做的决策。DDPG 方法用于更新评论家和演员网络。K-NN 可以帮助探索一组动作，以避免糟糕的决策。然后，演员和评论家网络通过使用
    FNN 实现。仿真结果表明，所提出的 DQL 方案在长期缓存命中率方面优于先进先出方案。
- en: Maximizing the long-term cache hit rate in [[70](#bib.bib70)] implies that the
    cache stores the most popular contents. In a dynamic environment, contents stored
    in a cache have to be replaced according to the users’ dynamic requests. An optimization
    of the placement or replacement of cached contents is studied in [[73](#bib.bib73)]
    by a deep learning method. The optimization algorithm is trained by a DNN in advance
    and then used for real-time caching or scheduling with minimum delay. The authors
    in [[74](#bib.bib74)] propose an optimal caching policy to learn the cache expiration
    times, i.e., Time-To-Live (TTL), for dynamically changing requests in content
    delivery networks. The system includes a cloud database server and multiple mobile
    devices that can issue queries and update entries in a single database. The query
    results can be cached for a specified time interval at server-controlled caches.
    All cached queries will become invalid if one of the cached records has been updated.
    A large TTL will strain cache capacities while a small TTL increases latencies
    significantly if the database server is physically remote.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化长期缓存命中率的意思是缓存中存储的是最受欢迎的内容。在动态环境中，缓存中的内容需要根据用户的动态请求进行替换。[[70](#bib.bib70)]中的研究通过深度学习方法对缓存内容的放置或替换进行了优化。优化算法通过DNN（深度神经网络）提前训练，然后用于实时缓存或调度，以实现最小延迟。[[74](#bib.bib74)]中的作者提出了一种优化缓存策略，以学习内容分发网络中动态变化请求的缓存过期时间，即生存时间（TTL）。该系统包括一个云数据库服务器和多个可以发出查询并更新单一数据库条目的移动设备。查询结果可以在服务器控制的缓存中缓存指定的时间间隔。如果缓存记录之一已被更新，所有缓存的查询将变得无效。较大的TTL将会使缓存容量紧张，而较小的TTL则会显著增加延迟，尤其是当数据库服务器物理上远离时。
- en: Unlike the DDPG approach used in [[70](#bib.bib70)], the authors in [[74](#bib.bib74)]
    propose to utilize Normalized Advantage Functions (NAFs) for continuous DQL scheme
    to learn optimal cache expiration duration. The key problem in continuous DQL
    is to select an action maximizing the Q-function, while avoiding performing a
    costly numerical optimization at each step. The use of NAFs obviates a second
    actor network that needs to be trained separately. Instead, a single neural network
    is used to output both a value function and an advantage term. The DQL agent at
    the cloud database uses an encoding of a query itself and the query miss rates
    as the system states, which allows for an easier generalization. The system reward
    is linearly proportional to the current load, i.e., the number of cached queries
    divided by the total capacity. This reward function can encourage longer TTLs
    when fewer queries are cached, and shorter TTLs when the load is close to the
    system capacity. Considering incomplete measurements for rewards and next-states
    at run-time, the authors introduce the Delayed Experience Injection (DEI) approach
    that allows the DQL agent to keep track of incomplete transitions when measurements
    are not immediately available. The authors evaluate the learning algorithm by
    Yahoo! cloud serving benchmark with customized web workloads [[75](#bib.bib75)].
    The simulation results verify that the learning approach based on NAFs and DEI
    outperforms a statistical estimator.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 与[[70](#bib.bib70)]中使用的DDPG方法不同，[[74](#bib.bib74)]中的作者提议利用归一化优势函数（NAFs）来进行连续DQL（深度Q学习）方案，以学习最优的缓存过期时间。连续DQL中的关键问题是选择一个最大化Q函数的动作，同时避免在每一步执行代价高昂的数值优化。使用NAFs可以避免需要单独训练的第二个演员网络。相反，使用单个神经网络来输出价值函数和优势项。云数据库中的DQL代理使用查询本身和查询未命中率的编码作为系统状态，从而使得泛化更为容易。系统奖励与当前负载成线性关系，即缓存查询的数量除以总容量。当缓存的查询较少时，这种奖励函数可以鼓励较长的TTL，而当负载接近系统容量时，则鼓励较短的TTL。考虑到运行时奖励和下一状态的不完全测量，作者引入了延迟经验注入（DEI）方法，该方法允许DQL代理在测量结果尚不可用时跟踪不完整的转换。作者通过Yahoo!云服务基准测试与定制的网页工作负载[[75](#bib.bib75)]评估了学习算法。模拟结果验证了基于NAFs和DEI的学习方法优于统计估计器。
- en: IV-A2 Joint Caching and Transmission Control
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 联合缓存和传输控制
- en: The caching policies determine where to store and retrieve the requested content
    efficiently, e.g., by learning the contents’ popularities [[70](#bib.bib70)] and
    cache expiration time [[74](#bib.bib74)]. Another important aspect of caching
    design is the transmission control of the content delivery from caches to end
    users, especially for wireless systems with dynamic channel conditions. To avoid
    mutual interference in multi-user wireless networks, the transmission control
    decides which cached contents can be transmitted concurrently as well as the most
    appropriate control parameters, e.g., transmit power, precoding, data rate, and
    channel allocation. Hence, the joint design of caching and transmission control
    is required to enable efficient content delivery in multi-user wireless networks.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存策略决定了如何高效地存储和检索请求的内容，例如，通过学习内容的受欢迎程度[[70](#bib.bib70)]和缓存过期时间[[74](#bib.bib74)]。缓存设计的另一个重要方面是内容从缓存到终端用户的传输控制，特别是对于具有动态信道条件的无线系统。为了避免多用户无线网络中的相互干扰，传输控制决定哪些缓存内容可以同时传输，以及最合适的控制参数，例如传输功率、预编码、数据速率和信道分配。因此，需要联合设计缓存和传输控制，以实现多用户无线网络中的高效内容传递。
- en: The authors in [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78)] propose
    a DQL framework to address the joint caching and interference alignment to tackle
    mutual interference in multi-user wireless networks. The authors consider an MIMO
    system with limited backhaul capacity and the caches at the transmitter. The precoding
    design for interference alignment requires the global CSI at each transmitter.
    A central scheduler is responsible for collecting CSI and cache status from each
    user via the backhaul, scheduling the users’ transmission, and optimizing the
    resource allocation. By enabling content caching at individual transmitters, we
    can decrease the demand for data transfer and thus save more backhaul capacity
    for real-time CSI update and sharing. Using the DQL-based approach at the central
    scheduler can reduce the explicit demand for CSI and the computational complexity
    in matrix optimization, especially with time-varying channel conditions. The DQL
    agent implements the DNN to approximate the Q-function with experience replay
    in training. To make the learning process more stable, the target Q-network parameter
    is updated by the Q-network for every a few time instants. The collected information
    is assembled into a system state and sent to the DQL agent, which feeds back an
    optimal action for the current time instant. The action indicates which users
    to be active, and the resource allocation among active users. The system reward
    represents the total throughput of multiple users. An extended work of [[76](#bib.bib76)]
    and [[77](#bib.bib77)] with a similar DQL framework is presented in [[78](#bib.bib78)],
    in which a CNN-based DQN is adopted and evaluated in a more practical conditions
    with imperfect or delayed CSI. Simulation results show that the performance of
    the MIMO system is significantly improved in terms of the total throughput and
    energy efficiency.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们在[[76](#bib.bib76)、[77](#bib.bib77)、[78](#bib.bib78)]中提出了一个DQL框架，旨在解决联合缓存和干扰对齐，以应对多用户无线网络中的相互干扰。作者们考虑了一个具有有限回传容量和发射机缓存的MIMO系统。干扰对齐的预编码设计需要每个发射机的全局CSI。一个中央调度器负责通过回传收集每个用户的CSI和缓存状态，安排用户的传输，并优化资源分配。通过在各个发射机启用内容缓存，我们可以减少数据传输的需求，从而节省更多的回传容量用于实时CSI更新和共享。使用基于DQL的方法的中央调度器可以减少对CSI的明确需求和矩阵优化中的计算复杂性，特别是在时变信道条件下。DQL代理实现DNN，通过经验回放进行训练，以近似Q函数。为了使学习过程更稳定，目标Q网络参数会在每隔几个时间瞬间由Q网络更新。收集的信息被汇总成系统状态并发送到DQL代理，DQL代理反馈当前时间瞬间的最优动作。该动作指示哪些用户需要激活，以及在激活用户之间的资源分配。系统奖励表示多个用户的总吞吐量。在[[78](#bib.bib78)]中，[[76](#bib.bib76)]和[[77](#bib.bib77)]的扩展工作采用了类似的DQL框架，其中引入了基于CNN的DQN，并在具有不完善或延迟CSI的更实际条件下进行了评估。仿真结果表明，MIMO系统在总吞吐量和能效方面的性能显著提高。
- en: Interference management is an important requirement of wireless systems. The
    application-related QoS or user experience is also an essential metric. Different
    from [[76](#bib.bib76), [78](#bib.bib78), [77](#bib.bib77)], the authors in [[79](#bib.bib79)]
    propose a DQL approach to maximize Quality of Experience (QoE) of IoT devices
    by jointly optimizing the cache allocation and transmission rate in content-centric
    wireless networks. The system state is specified by the nodes’ caching conditions,
    e.g., the service information and cached contents, as well as the transmission
    rates of the cached contents. The aim of the DQL agent is to minimize continuously
    the network cost or maximize the QoE. The proposed DQL framework is further enhanced
    with the use of PER and DDQN. PER replays important transitions more frequently
    so that DQN can learn from samples more efficiently. The use of DDQN can stabilize
    the learning by providing two value functions in separated neural networks. This
    avoids an overestimation of the DQN with the increasing number of actions. These
    two neural networks are not completely decoupled as the target network is a periodic
    copy of estimation network. A discrete simulator ccnSim [[80](#bib.bib80)] is
    used to model the caching behavior in various graph structures. The output data
    trace of the simulator is then imported to Matlab and used to evaluate the learning
    algorithm. The simulation results show that the DQL framework by using PER and
    DDQN outperforms the standard penetration test scheme in terms of QoE.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 干扰管理是无线系统的重要要求。与应用相关的服务质量（QoS）或用户体验也是一个关键指标。不同于[[76](#bib.bib76), [78](#bib.bib78),
    [77](#bib.bib77)]，[[79](#bib.bib79)]中的作者提出了一种DQL方法，通过在内容中心无线网络中联合优化缓存分配和传输速率，来最大化物联网设备的体验质量（QoE）。系统状态由节点的缓存条件定义，例如服务信息和缓存内容，以及缓存内容的传输速率。DQL代理的目标是不断最小化网络成本或最大化QoE。所提出的DQL框架通过使用PER和DDQN进一步增强。PER更频繁地重放重要的过渡，以便DQN可以更有效地从样本中学习。使用DDQN可以通过在分开的神经网络中提供两个值函数来稳定学习，这样可以避免随着动作数量的增加而对DQN的高估。这两个神经网络并未完全解耦，因为目标网络是估计网络的周期性副本。离散模拟器ccnSim[[80](#bib.bib80)]被用来模拟各种图结构中的缓存行为。模拟器的输出数据跟踪然后被导入到Matlab中，并用来评估学习算法。模拟结果显示，使用PER和DDQN的DQL框架在QoE方面优于标准穿透测试方案。
- en: The QoE can be used to characterize the users’ perception of Virtual Reality
    (VR) services. The authors in [[81](#bib.bib81)] address the joint content caching
    and transmission strategy in a wireless VR network, where UAVs capture videos
    on live games and transmit them to small-cell BSs servicing the VR users. Millimeter
    wave (mmWave) downlink backhaul links are used for VR content transmission from
    the UAVs to BSs. The BSs can also cache the popular contents that may be requested
    frequently by end users. The joint content caching and transmission problem is
    formulated as an optimization to maximize the users’ reliability, i.e., the probability
    that the content transmission delay satisfies the instantaneous delay target.
    The maximization involves the control of transmission format, users’ association,
    the set and format of cached contents. A DQL framework combining the Liquid State
    Machine (LSM) and Echo State Network (ESN) is proposed for each BS to find the
    optimal transmission and caching strategies. As a randomly generated spiking neural
    network [[82](#bib.bib82)], LSM can store information about the network environment
    over time and adjust the users’ association policy, cached contents and formats
    according to the users’ content requests. It has been used in [[83](#bib.bib83)]
    to predict the users’ content request distribution while having only limited information
    regarding the network and different users. Conventional LSM uses FNNs as the output
    function, which demands high complexity in training due to the computation of
    gradients for all of the neurons. Conversely, the proposed DQL framework uses
    an ESN as the output function, which uses historical information to find the relationship
    between the users’ reliability, caching, and content transmission. It also has
    a lower complexity in training and a better memory for network information. Simulation
    results show that the proposed DQL framework can yield 25.4% gain in terms of
    users’ reliability compared to the baseline Q-learning.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 用户体验（QoE）可以用来描述用户对虚拟现实（VR）服务的感知。文献[[81](#bib.bib81)]中讨论了无线VR网络中的联合内容缓存和传输策略，其中无人机在实时游戏中捕捉视频并将其传输到服务VR用户的小区基站（BS）。毫米波（mmWave）下行回传链路用于从无人机到基站的VR内容传输。基站还可以缓存可能被最终用户频繁请求的热门内容。联合内容缓存和传输问题被制定为一个优化问题，目的是最大化用户的可靠性，即内容传输延迟满足瞬时延迟目标的概率。最大化涉及到传输格式、用户关联、缓存内容的集合和格式的控制。为每个基站提出了一种结合了液态状态机（LSM）和回声状态网络（ESN）的深度Q学习（DQL）框架，以找到最佳的传输和缓存策略。作为一种随机生成的脉冲神经网络[[82](#bib.bib82)]，LSM能够存储有关网络环境的信息，并根据用户的内容请求调整用户关联策略、缓存内容和格式。它已被用于[[83](#bib.bib83)]中预测用户的内容请求分布，同时仅有有限的网络和用户信息。传统的LSM使用前馈神经网络（FNNs）作为输出函数，这要求高复杂度的训练，因为需要计算所有神经元的梯度。相反，提出的DQL框架使用回声状态网络（ESN）作为输出函数，利用历史信息来寻找用户的可靠性、缓存和内容传输之间的关系。它在训练中具有较低的复杂度，并且对网络信息有更好的记忆。仿真结果表明，与基线Q学习相比，提出的DQL框架在用户可靠性方面可以获得25.4%的增益。
- en: IV-A3 Joint Caching, Networking, and Computation
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 联合缓存、网络和计算
- en: Caching and transmission control will become more involved in a HetNet that
    integrates different communication technologies, e.g., cellular system, device-to-device
    network, vehicular network, and networked UAVs, to support various application
    demands. The network heterogeneity raises the problem of complicated system design
    that needs to address challenging issues such as mutual interference, differentiated
    QoS provisioning, and resource allocation, hopefully in a unified framework. Obviously
    this demands a joint optimization far beyond the extent of joint caching and transmission
    control.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存和传输控制在一个集成了不同通信技术的异构网络（如蜂窝系统、设备对设备网络、车载网络以及网络化无人机）中将变得更加复杂，以支持各种应用需求。网络的异质性带来了复杂的系统设计问题，需要在统一框架内解决诸如相互干扰、差异化服务质量（QoS）提供以及资源分配等挑战性问题。显然，这要求进行联合优化，远超出联合缓存和传输控制的范围。
- en: Accordingly, the authors in [[84](#bib.bib84)] propose a DQL framework for energy-efficient
    resource allocation in green wireless networks, jointly considering the couplings
    among networking, in-network caching and computation. The system consists of a
    Software-Defined Network (SDN) with multiple virtual networks and mobile users
    requesting for video on-demand files that require a certain amount of computational
    resource at either the content server or at local devices. In each virtual network,
    an authorized user issues a request to download files from a set of available
    SBSs in its neighborhood area. The wireless channels between each mobile user
    and the SBSs are characterized as Finite-State Markov Channels (FSMC). The states
    are the available cache capacity at the SBSs, the channel conditions between mobile
    users and SBSs, the computational capability of the content servers and mobile
    users. The DQL agent at each SBS decides an association between each mobile user
    and SBS, where to perform the computational task, and how to schedule the transmissions
    of SBSs to deliver the required data. The objective is to minimize the total energy
    consumption of the system from data caching, wireless transmission, and computation.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，[[84](#bib.bib84)]中的作者提出了一个用于绿色无线网络中节能资源分配的DQL框架，综合考虑了网络、网络缓存和计算之间的耦合。系统由一个具有多个虚拟网络的软件定义网络（SDN）组成，移动用户请求按需视频文件，这些文件在内容服务器或本地设备上需要一定的计算资源。在每个虚拟网络中，一个授权用户向其邻域内的一组可用SBS发出下载文件的请求。每个移动用户和SBS之间的无线信道被描述为有限状态马尔可夫信道（FSMC）。状态包括SBS的可用缓存容量、移动用户与SBS之间的信道条件、内容服务器和移动用户的计算能力。每个SBS上的DQL代理决定每个移动用户与SBS之间的关联，计算任务的执行位置，以及如何安排SBS的传输以传送所需数据。目标是最小化系统在数据缓存、无线传输和计算中的总能耗。
- en: The DQL scheme proposed in [[84](#bib.bib84)] has been applied to improve the
    performance of Vehicular Ad doc NETworks (VANETs) in [[85](#bib.bib85), [86](#bib.bib86),
    [87](#bib.bib87)]. The network model includes multiple BSs, Road Side Units (RSUs),
    MEC servers, and content servers. All devices are controlled by a mobile virtual
    network operator. The vehicles request for video contents that can be cached at
    the BSs or retrieved from remote content servers. The authors in [[85](#bib.bib85)]
    formulate the resource allocation problem as a joint optimization of caching,
    networking, and computing, e.g., compressing and encoding operations of the video
    contents. The system states include the CSI from each BS, the computational capability,
    and cache size of each MEC/content server. The network operator feeds the system
    state to the FNN-based DQN and gets the optimal policy that determines the resource
    allocation for each vehicle. To exploit spatial correlations in learning, the
    authors in [[86](#bib.bib86)] enhance Q-learning by using CNNs in DQN. This makes
    it possible to extract high-level features from raw input data. Two schemes have
    been introduced in [[87](#bib.bib87)] to improve stability and performance of
    the ordinary DQN method. Firstly, DDQN is designed to avoid over-estimation of
    Q-value in ordinary DQN. Hence, the action can be decoupled from the target Q-value
    generation. This makes the training process faster and more reliable. Secondly,
    the dueling DQN approach is also integrated in the design with the intuition that
    it is not always necessary to estimate the reward by taking some action. The state-action
    Q-value in dueling DQN is decomposed into one value function representing the
    reward in the current state, and the advantage function that measures the relative
    importance of a certain action compared with other actions. The enhanced DQL agent
    combining these two schemes can achieve better performance and faster training
    speed.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[84](#bib.bib84)]中提出的DQL方案已经被应用于提升车辆自组织网络（VANETs）的性能，如[[85](#bib.bib85)，[86](#bib.bib86)，[87](#bib.bib87)]所示。网络模型包括多个基站（BSs）、路边单元（RSUs）、边缘计算（MEC）服务器和内容服务器。所有设备由移动虚拟网络运营商控制。车辆请求的视频内容可以缓存于基站或从远程内容服务器中检索。[[85](#bib.bib85)]中的作者将资源分配问题表述为缓存、网络和计算的联合优化，例如视频内容的压缩和编码操作。系统状态包括来自每个基站的CSI、每个MEC/内容服务器的计算能力和缓存大小。网络运营商将系统状态输入到基于FNN的DQN中，以获取确定每辆车资源分配的最佳策略。为了在学习中利用空间相关性，[[86](#bib.bib86)]中的作者通过在DQN中使用CNN来增强Q学习。这使得从原始输入数据中提取高级特征成为可能。[[87](#bib.bib87)]中介绍了两种方案以提升普通DQN方法的稳定性和性能。首先，DDQN旨在避免普通DQN中Q值的过度估计。因此，动作可以与目标Q值生成解耦。这使得训练过程更快且更可靠。其次，决斗DQN方法也被整合进设计中，其直觉是并不总是需要通过采取某些动作来估计奖励。决斗DQN中的状态-动作Q值被分解为一个表示当前状态下奖励的价值函数，以及一个衡量某动作相对于其他动作相对重要性的优势函数。结合这两种方案的增强DQL代理可以实现更好的性能和更快的训练速度。
- en: Considering the huge action space and high complexity with the vehicle’s mobility
    and service delay deadline $T_{d}$, a multi-time scale DQN framework is proposed
    in [[88](#bib.bib88)] to minimize the system cost by the joint design of communication,
    caching and computing in VANET. The policy design accounts for limited storage
    capacities and computational resources at the vehicles and the RSUs. The small
    timescale DQN is for every time slot and aims to maximize the exact immediate
    reward. Additionally, the large timescale DQN is designed for every $T_{d}$ time
    slots within the service delay deadline, and used to estimate the reward considering
    the vehicle’s mobility in a large timescale.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到车辆的移动性和服务延迟截止时间$T_{d}$带来的巨大动作空间和高复杂度，[[88](#bib.bib88)]中提出了一种多时间尺度DQN框架，通过在VANET中对通信、缓存和计算的联合设计来最小化系统成本。策略设计考虑了车辆和路边单元的有限存储容量和计算资源。小时间尺度DQN用于每个时间槽，旨在最大化精确的即时奖励。此外，大时间尺度DQN被设计用于服务延迟截止时间内的每$T_{d}$时间槽，用于估计考虑车辆移动性的奖励。
- en: The aforementioned DQL framework for VANETs, e.g., [[86](#bib.bib86), [85](#bib.bib85),
    [87](#bib.bib87)], has also been generalized to smart city applications in [[89](#bib.bib89)],
    which necessitates dynamic orchestration of networking, caching, and computation
    to meet different servicing requirements. Through Network Function Virtualization
    (NFV) [[90](#bib.bib90)], the physical wireless network in smart cities can be
    divided logically into several virtual ones by the network operator, which is
    responsible for network slicing and resource scheduling, as well as allocation
    of caching and computing capacities. The use cases in smart cities are presented
    in [[91](#bib.bib91), [92](#bib.bib92)], which apply the generalized DQL framework
    to improve the security and efficiency for trust-based data exchange, sharing,
    and delivery in mobile social networks through the resource allocation and optimization
    of MEC allocation, caching, and D2D (Device-to-Device) networking.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的用于VANETs的DQL框架，例如[[86](#bib.bib86), [85](#bib.bib85), [87](#bib.bib87)]，也已被推广到智能城市应用中 [[89](#bib.bib89)]，这需要动态协调网络、缓存和计算以满足不同的服务需求。通过网络功能虚拟化（NFV）[[90](#bib.bib90)]，智能城市中的物理无线网络可以由网络运营商逻辑上划分为多个虚拟网络，负责网络切片和资源调度，以及缓存和计算能力的分配。智能城市中的应用案例展示在 [[91](#bib.bib91),
    [92](#bib.bib92)]，这些应用利用推广的DQL框架，通过资源分配和优化MEC分配、缓存以及D2D（设备对设备）网络，提升基于信任的数据交换、共享和传递的安全性和效率。
- en: '![Refer to caption](img/f10303b4ec79a63c805db54b8fd1e356.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f10303b4ec79a63c805db54b8fd1e356.png)'
- en: 'Figure 7: Joint caching, networking, and transmission control to optimize cache
    hit rate [[70](#bib.bib70)], cache expiration time [[74](#bib.bib74)], interference
    alignment [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78)], Quality of Experience [[79](#bib.bib79),
    [81](#bib.bib81)], energy efficiency [[84](#bib.bib84)], resource allocation [[85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87)], traffic latency, or redundancy [[89](#bib.bib89),
    [91](#bib.bib91)].'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：联合缓存、网络和传输控制以优化缓存命中率 [[70](#bib.bib70)]、缓存过期时间 [[74](#bib.bib74)]、干扰对齐 [[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78)]、体验质量 [[79](#bib.bib79), [81](#bib.bib81)]、能效 [[84](#bib.bib84)]、资源分配 [[85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87)]、流量延迟或冗余 [[89](#bib.bib89), [91](#bib.bib91)]。
- en: IV-B Data and Computation Offloading
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 数据与计算卸载
- en: With limited computation, memory and power supplies, IoT devices such as sensors,
    wearable devices, and handheld devices become the bottleneck to support advanced
    applications such as interactive online gaming and face recognition. To address
    such a challenge, IoT devices can offload the computational tasks to nearby MEC
    servers, integrated with the BSs, Access Points (APs), and even neighboring Mobile
    Users (MUs). As a result, data and computation offloading can potentially reduce
    the processing delay, save the battery energy, and even enhance security for computation-intensive
    IoT applications. However, the critical problem in the computation offloading
    is to determine the offloading rate, i.e., the amount of computational workload,
    and choose the MEC server from all available servers. If the chosen MEC server
    experiences heavy workloads and degraded channel conditions, it may take even
    longer time for the IoT devices to offload data and receive the results from the
    MEC server. Hence, the design of an offloading policy has to take into account
    the time-varying channel conditions, user mobility, energy supply, computation
    workload and the computational capabilities of different MEC servers.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算、内存和电力供应有限，传感器、可穿戴设备和手持设备等物联网设备成为支持互动在线游戏和面部识别等先进应用的瓶颈。为应对这一挑战，物联网设备可以将计算任务卸载到附近的MEC服务器，这些服务器集成在基站、接入点（AP）甚至邻近的移动用户（MU）中。因此，数据和计算卸载可以潜在地减少处理延迟，节省电池能量，甚至增强计算密集型物联网应用的安全性。然而，计算卸载中的关键问题是确定卸载率，即计算工作负载的数量，并从所有可用服务器中选择MEC服务器。如果选择的MEC服务器遇到重负载和恶化的信道条件，物联网设备卸载数据并从MEC服务器接收结果的时间可能会更长。因此，卸载策略的设计必须考虑到时变的信道条件、用户移动性、能量供应、计算工作负载和不同MEC服务器的计算能力。
- en: 'The authors in [[93](#bib.bib93)] focus on minimizing the mobile user’s cost
    and energy consumption by offloading cellular traffic to WLAN. Each mobile user
    can either access the cellular network, or the complimentary WLAN as illustrated
    in Fig. [8](#S4.F8 "Figure 8 ‣ IV-B Data and Computation Offloading ‣ IV Caching
    and Offloading ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")(a), but with different monetary costs. The mobile user
    also has to pay a penalty if the data transmission does not finish before the
    deadline. The mobile user’s data offloading decision can be modeled as an MDP.
    The system state includes the mobile user’s location and the remaining file size
    of all data flows. The mobile user will choose to transmit data through either
    WLAN or cellular network, and decide how to allocate channel capacities to concurrent
    flows. Without knowing the mobility pattern in advance, the DQL is proposed for
    each mobile user to learn the optimal offloading policy from past experiences.
    CNNs are employed in the DQL to predict a continuous value of the mobile user’s
    remaining data. Simulation results reveal that the DQN-based scheme generally
    outperforms the dynamic programming algorithm for the MDP. The reason is that
    the DQN can learn from experience while the dynamic programming algorithm cannot
    obtain the optimal policy with incorrect transition probability.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '参考文献[[93](#bib.bib93)]中的作者专注于通过将蜂窝流量卸载到WLAN来最小化移动用户的成本和能量消耗。每个移动用户可以选择访问蜂窝网络或补充的WLAN，如图[8](#S4.F8
    "Figure 8 ‣ IV-B Data and Computation Offloading ‣ IV Caching and Offloading ‣
    Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")(a)所示，但它们的货币成本不同。如果数据传输未能在截止时间前完成，移动用户还需支付罚款。移动用户的数据卸载决策可以建模为一个MDP。系统状态包括移动用户的位置以及所有数据流的剩余文件大小。移动用户将选择通过WLAN或蜂窝网络传输数据，并决定如何分配信道容量给并发流。在事先不知道移动模式的情况下，DQL被提出用于让每个移动用户从过去的经验中学习最佳卸载策略。DQL中使用CNN来预测移动用户剩余数据的连续值。仿真结果显示，基于DQN的方案通常优于MDP的动态规划算法。原因在于DQN可以从经验中学习，而动态规划算法则无法在过渡概率不正确的情况下获得最佳策略。'
- en: 'The allocation of limited computational resources at the MEC server is critical
    for cost and energy minimization. The authors in [[94](#bib.bib94)] consider an
    MEC-enabled cellular system, in which multiple mobile users can offload their
    computational tasks via wireless channels to one MEC server, co-located with the
    cellular BS as shown in Fig. [8](#S4.F8 "Figure 8 ‣ IV-B Data and Computation
    Offloading ‣ IV Caching and Offloading ‣ Applications of Deep Reinforcement Learning
    in Communications and Networking: A Survey")(b). Each mobile user has a computational-intensive
    task, characterized by the required computational resources, CPU cycles, and the
    maximum tolerable delay. The capacity of the MEC server is limited to accommodate
    all mobile users’ task loads. The bandwidth sharing between different mobile users’
    offloading also affects the overall delay performance and energy consumptions.
    The DQL is used to minimize the cost of delay and power consumptions for all mobile
    users, by jointly optimizing the offloading decision and computational resource
    allocation. The system states include the sum of cost of the entire system and
    the available computational capacity of the MEC server. The action of BS is to
    determine the resource allocation and offloading decision for each mobile user.
    To limit the size of action space, a pre-classification step is proposed to check
    the mobile users’ feasible set of actions.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 'MEC服务器上有限计算资源的分配对于成本和能量的最小化至关重要。参考文献[[94](#bib.bib94)]中的作者考虑了一个MEC支持的蜂窝系统，其中多个移动用户可以通过无线信道将其计算任务卸载到一个与蜂窝基站共址的MEC服务器，如图[8](#S4.F8
    "Figure 8 ‣ IV-B Data and Computation Offloading ‣ IV Caching and Offloading ‣
    Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")(b)所示。每个移动用户都有一个计算密集型任务，其特征包括所需的计算资源、CPU周期和最大可容忍延迟。MEC服务器的容量有限，无法容纳所有移动用户的任务负载。不同移动用户的卸载带宽共享也会影响整体延迟性能和能量消耗。DQL用于通过联合优化卸载决策和计算资源分配，来最小化所有移动用户的延迟成本和功耗。系统状态包括整个系统的成本总和以及MEC服务器的可用计算容量。基站的行动是确定每个移动用户的资源分配和卸载决策。为了限制动作空间的大小，提出了一个预分类步骤来检查移动用户的可行动作集。'
- en: 'In contrast to [[94](#bib.bib94)], multiple BSs in an ultra-dense network is
    considered in [[95](#bib.bib95)] and [[96](#bib.bib96)], as shown in Fig. [8](#S4.F8
    "Figure 8 ‣ IV-B Data and Computation Offloading ‣ IV Caching and Offloading ‣
    Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")(c), with the objective of minimizing the long-term cost of delay in
    computation offloading. All computational tasks are offloaded to the shared MEC
    server via different BSs. Besides the allocation of computational resources and
    transmission control, the offloading policy also has to optimize the association
    between mobile users and the BSs. With dynamic network conditions, the mobile
    users’ decision-making can be formulated as an MDP. The system states are the
    channel conditions between the mobile user and the BSs, the states of energy and
    task queues. The cost function is defined as a weighted sum of the execution delay,
    the handover delay and the computational task dropping cost. The authors in [[96](#bib.bib96)]
    firstly propose a DDQN-based DQL algorithm to learn the optimal offloading policy
    without knowing the network dynamics. By leveraging the additive structure of
    the utility function, the Q-function decomposition combined with the DDQN further
    leads to a novel online SARSA-based DRL algorithm. Numerical experiments show
    that the new algorithm achieves a significant improvement in computation offloading
    performance compared with the baseline policies, e.g., the DQN-based DQL algorithm
    and some heuristic offloading strategies without learning. The high density of
    SBSs can relieve the data offloading pressure in peak traffic hours but consume
    a large amount of energy in off-peak time. Therefore, the authors in [[97](#bib.bib97)],
    [[98](#bib.bib98)], and [[99](#bib.bib99)] propose a DQL-based strategy for controlling
    the (de)activation of different SBSs to minimize the energy consumption without
    compromising the quality of provisioning. In particular, in [[97](#bib.bib97)],
    the on/off decision framework uses a DQL scheme to approximate both the policy
    and value functions in an actor-critic method. The reward of the DQL agent is
    defined as a cost function relating to energy consumption, QoS degradation, and
    the switching cost of SBSs. The DDPG approach is also employed together with an
    action refinement scheme to expedite the training process. Through extensive numerical
    simulations, the proposed scheme is shown to greatly outperform other baseline
    methods in terms of both energy and computational efficiency.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '与[[94](#bib.bib94)]相比，[[95](#bib.bib95)]和[[96](#bib.bib96)]中考虑了超密集网络中的多个基站，如图[8](#S4.F8
    "Figure 8 ‣ IV-B Data and Computation Offloading ‣ IV Caching and Offloading ‣
    Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")(c)所示，其目标是最小化计算卸载的长期延迟成本。所有计算任务都通过不同的基站卸载到共享的MEC服务器。除了计算资源和传输控制的分配外，卸载策略还必须优化移动用户与基站之间的关联。考虑到动态网络条件，移动用户的决策可以被形式化为一个MDP。系统状态包括移动用户与基站之间的信道条件、能量状态和任务队列状态。成本函数被定义为执行延迟、切换延迟和计算任务丢弃成本的加权和。[[96](#bib.bib96)]中的作者首先提出了一种基于DDQN的DQL算法，以在不知晓网络动态的情况下学习最优的卸载策略。通过利用效用函数的加法结构，Q函数分解与DDQN的结合进一步导致了一种新颖的基于在线SARSA的DRL算法。数值实验表明，新算法在计算卸载性能方面比基线策略，如基于DQN的DQL算法和一些无学习的启发式卸载策略，取得了显著的改进。高密度的SBS可以在高峰流量时减轻数据卸载压力，但在非高峰时间消耗大量能量。因此，[[97](#bib.bib97)]、[[98](#bib.bib98)]和[[99](#bib.bib99)]中的作者提出了一种基于DQL的策略，用于控制不同SBS的（停用）激活，以在不影响服务质量的情况下最小化能源消耗。特别地，在[[97](#bib.bib97)]中，开/关决策框架使用DQL方案来近似演员-评论家方法中的策略和价值函数。DQL代理的奖励被定义为与能量消耗、QoS降级和SBS切换成本相关的成本函数。DDPG方法也被采用，并结合一个动作精炼方案来加速训练过程。通过大量的数值模拟，所提出的方案在能源和计算效率方面都大大优于其他基线方法。'
- en: With a similar model to that in [[96](#bib.bib96)], computation offloading finds
    a proper application for cloud-based malware detection in [[100](#bib.bib100)].
    A review of the threat models and the RL-based solutions for security and privacy
    protection in mobile offloading and caching are discussed in [[101](#bib.bib101)].
    With limited energy supply, computational resources, and channel capacity, mobile
    users cannot always update the local malware database and process all application
    data in time and thus are vulnerable to zero-day attacks [[102](#bib.bib102)].
    By leveraging the remote MEC server, all mobile users can offload their application
    data and detection tasks via different BSs to the MEC/security server with larger
    and more sophisticated malware database, more computational capabilities, and
    powerful security services. This can be modeled by a dynamic malware detection
    game in which multiple mobile users interact with each other in resource competition,
    e.g., the allocation of wireless channel capacities and the computational capabilities
    of the MEC/security server. A DQL scheme is proposed for each mobile user to learn
    its offloading data rate to the MEC/security server. The system states include
    the channel state and the size of application traces. The objective is to optimize
    the detection accuracy of the security server, which is defined as a concave function
    in the total amount of malware samples. The Q-value is estimated by using a CNN
    in the DQL framework. The authors also propose the hotbooting Q-learning technique
    that provides a better initialization for Q-learning by exploiting the offloading
    experiences in similar scenarios. It can save exploration time at the initial
    stage and accelerate the learning speed compared with a standard Q-learning algorithm
    with all-zero initialization of the Q-value [[103](#bib.bib103)]. The proposed
    DQL scheme not only improves the detection speed and accuracy, but also increases
    the mobile users’ battery life. The simulation results reveal that compared with
    the hotbooting Q-learning, the DQL-based malware detection has the faster learning
    rate, the higher accuracy, and the shorter detection delay.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [[96](#bib.bib96)]中类似的模型相似，计算卸载为基于云的恶意软件检测找到了一种合适的应用场景，见于 [[100](#bib.bib100)]。对移动卸载和缓存中的威胁模型以及基于RL的安全和隐私保护解决方案进行了回顾，详见 [[101](#bib.bib101)]。由于能源供应、计算资源和频道容量有限，移动用户不能总是及时更新本地恶意软件数据库并处理所有应用数据，因此容易受到零日攻击 [[102](#bib.bib102)]。通过利用远程MEC服务器，所有移动用户可以通过不同的BS将其应用数据和检测任务卸载到拥有更大、更复杂恶意软件数据库、更多计算能力和强大安全服务的MEC/安全服务器。这可以通过一个动态恶意软件检测游戏来建模，其中多个移动用户在资源竞争中相互作用，例如，分配无线频道容量和MEC/安全服务器的计算能力。为每个移动用户提出了一个DQL方案，以学习其向MEC/安全服务器卸载数据的速率。系统状态包括频道状态和应用跟踪的大小。目标是优化安全服务器的检测准确性，这被定义为总恶意软件样本数量的凹函数。Q值通过DQL框架中的CNN进行估计。作者还提出了热启动Q学习技术，该技术通过利用类似场景中的卸载经验，为Q学习提供了更好的初始化。与具有全零Q值初始化的标准Q学习算法相比，它可以节省初始阶段的探索时间并加快学习速度 [[103](#bib.bib103)]。提出的DQL方案不仅提高了检测速度和准确性，还增加了移动用户的电池寿命。仿真结果表明，与热启动Q学习相比，基于DQL的恶意软件检测具有更快的学习速度、更高的准确性和更短的检测延迟。
- en: 'Multiple MEC servers have been considered in [[104](#bib.bib104), [105](#bib.bib105)],
    as illustrated in Fig. [8](#S4.F8 "Figure 8 ‣ IV-B Data and Computation Offloading
    ‣ IV Caching and Offloading ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")(d). The authors in [[104](#bib.bib104)] aim to design
    optimal offloading policy for IoT devices with energy harvesting capabilities.
    The system consists of multiple MEC servers, such as BSs and APs, with different
    capabilities in computation and communications. The IoT devices are equipped with
    energy storage and energy harvesters. They can execute computational tasks locally
    and offload the tasks to the MEC servers. The IoT device’s offloading decision
    can be formulated as an MDP. The system states include the battery status, the
    channel capacity, and the predicted amount of harvested energy in the future.
    The IoT device evaluates the reward based on the overall delay, energy consumption,
    the task drop loss and the data sharing gains in each time slot. Similar to [[100](#bib.bib100)],
    the authors in [[104](#bib.bib104)] enhance Q-learning by the hotbooting technique
    to save the random exploration time at the beginning of learning. The authors
    also propose a fast DQL offloading scheme that uses hotbooting to initialize the
    CNN and accelerates the learning speed. The authors in [[105](#bib.bib105)] view
    the MEC-enabled BSs as different physical machines constituting a part of the
    cloud resources. The cloud optimizes the MUs’ computation offloading to different
    virtual machines residing on the physical machines. A two-layered DQL algorithm
    is proposed for the offloading problem to maximize the utilization of cloud resources.
    The system state relates to the waiting time of each computational task and the
    number of virtual machines. The first layer is implemented by a CNN-based DQL
    framework to estimate an optimal cluster for each computational task. Different
    clusters of physical machines are generated based on the K-NN algorithm. The second
    layer determines the optimal serving physical machine within the cluster by Q-learning
    method.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [[104](#bib.bib104), [105](#bib.bib105)] 中考虑了多个 MEC 服务器，如图 [8](#S4.F8 "Figure
    8 ‣ IV-B Data and Computation Offloading ‣ IV Caching and Offloading ‣ Applications
    of Deep Reinforcement Learning in Communications and Networking: A Survey")(d)
    所示。[[104](#bib.bib104)] 中的作者旨在为具备能量收集能力的物联网设备设计最佳的卸载策略。该系统由多个 MEC 服务器组成，如基站 (BSs)
    和接入点 (APs)，它们在计算和通信方面具有不同的能力。物联网设备配备了能量存储和能量收集器。它们可以在本地执行计算任务，并将任务卸载到 MEC 服务器上。物联网设备的卸载决策可以被表述为一个
    MDP。系统状态包括电池状态、信道容量以及未来预测的收集能量量。物联网设备基于总体延迟、能量消耗、任务丢失和每个时间段的数据共享收益来评估奖励。类似于 [[100](#bib.bib100)]，[[104](#bib.bib104)]
    中的作者通过热启动技术增强 Q 学习，以节省学习开始时的随机探索时间。作者还提出了一种快速 DQL 卸载方案，该方案使用热启动来初始化 CNN 并加快学习速度。[[105](#bib.bib105)]
    中的作者将启用 MEC 的基站视为构成云资源的一部分的不同物理机器。云优化 MUs 的计算卸载到驻留在物理机器上的不同虚拟机。为卸载问题提出了一种两层 DQL
    算法，以最大化云资源的利用率。系统状态与每个计算任务的等待时间和虚拟机数量有关。第一层由基于 CNN 的 DQL 框架实现，以估计每个计算任务的最佳集群。基于
    K-NN 算法生成不同的物理机器集群。第二层通过 Q 学习方法确定集群内的最佳服务物理机器。'
- en: '![Refer to caption](img/a21b14fa5bfaf174d7a4626767736c40.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a21b14fa5bfaf174d7a4626767736c40.png)'
- en: 'Figure 8: Data/computation offloading models in cellular networks: (a) Offloading
    cellular traffic to WLAN [[93](#bib.bib93)], (b) Offloading to a single MEC-enabled
    BS [[94](#bib.bib94)], (c) Offloading to one shared MEC server via multiple BSs [[95](#bib.bib95),
    [96](#bib.bib96), [100](#bib.bib100)], (d) Offloading to multiple MEC-enabled
    BSs [[104](#bib.bib104), [105](#bib.bib105)] and mobile cloudlets [[106](#bib.bib106),
    [107](#bib.bib107)].'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：蜂窝网络中的数据/计算卸载模型：(a) 将蜂窝流量卸载到 WLAN [[93](#bib.bib93)]，(b) 卸载到单个 MEC 启用的基站
    [[94](#bib.bib94)]，(c) 通过多个基站卸载到一个共享的 MEC 服务器 [[95](#bib.bib95), [96](#bib.bib96),
    [100](#bib.bib100)]，(d) 卸载到多个 MEC 启用的基站 [[104](#bib.bib104), [105](#bib.bib105)]
    和移动云小组 [[106](#bib.bib106), [107](#bib.bib107)]。
- en: 'The aforementioned works all focus on data or computation offloading in cellular
    system via BSs to remote MEC servers, e.g., [[93](#bib.bib93), [94](#bib.bib94),
    [95](#bib.bib95), [96](#bib.bib96), [105](#bib.bib105), [100](#bib.bib100), [104](#bib.bib104)].
    In [[107](#bib.bib107)] and [[106](#bib.bib106)], the authors study QoS-aware
    computation offloading in an ad-hoc mobile network. By making a certain payment,
    the mobile user can offload its computational tasks to nearby mobile users constituting
    a mobile cloudlet, as shown in Fig. [8](#S4.F8 "Figure 8 ‣ IV-B Data and Computation
    Offloading ‣ IV Caching and Offloading ‣ Applications of Deep Reinforcement Learning
    in Communications and Networking: A Survey")(d). Each mobile user has a first-in-first-out
    queue with limited buffer size to store the arriving tasks arriving as a Poisson
    process. The mobile user selects nearby cloudlets within D2D communication range
    for task offloading. The offloading decision depends on the states including the
    number of remaining tasks, the quality of the links between mobile users and the
    cloudlet, and the availability of the cloudlet’s resources. The objective is to
    maximize a composite utility function, subject to the mobile user’s QoS requirements,
    e.g., energy consumption and processing delay. The utility function is firstly
    an increasing function of the total number of tasks that have been processed either
    locally or remotely by the cloudlets. It is also related to the user’s benefit
    such as energy efficiency and payment for task offloading. This problem can be
    formulated as an MDP, which can be solved by linear programming and Q-learning
    approaches, depending on the availability of information about the state transition
    probabilities. This work is further enhanced by leveraging DNN or DQN to learn
    the decision strategy more efficiently. A similar model is studied in [[108](#bib.bib108)],
    where the computation offloading is formulated as an MDP to minimize the cost
    of computation offloading. The solution to the MDP can be used to train a DNN
    by supervised learning. The well-trained DNN is then applied to unseen network
    conditions for real-time decision-making. Simulation results show that the use
    of deep supervised learning achieves significant performance gain in offloading
    accuracy and cost saving.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '上述研究均集中于通过基站将数据或计算任务卸载到远程 MEC 服务器的蜂窝系统，例如 [[93](#bib.bib93), [94](#bib.bib94),
    [95](#bib.bib95), [96](#bib.bib96), [105](#bib.bib105), [100](#bib.bib100), [104](#bib.bib104)]。在
    [[107](#bib.bib107)] 和 [[106](#bib.bib106)] 中，作者研究了在自组织移动网络中的 QoS 关注计算卸载。通过支付一定费用，移动用户可以将其计算任务卸载到附近的移动用户组成的移动云集群中，如图
    [8](#S4.F8 "Figure 8 ‣ IV-B Data and Computation Offloading ‣ IV Caching and Offloading
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")(d) 所示。每个移动用户都有一个先进先出队列，其缓冲区大小有限，用于存储作为泊松过程到达的任务。移动用户选择在 D2D 通信范围内的附近云集群进行任务卸载。卸载决策依赖于状态，包括剩余任务的数量、移动用户与云集群之间的链路质量以及云集群资源的可用性。目标是最大化一个复合效用函数，同时满足移动用户的
    QoS 要求，例如能耗和处理延迟。效用函数首先是处理过的任务总数的递增函数，无论是本地还是远程由云集群处理。它还与用户的收益相关，如能效和任务卸载的费用。这个问题可以被表述为一个
    MDP，可以通过线性规划和 Q 学习方法解决，具体取决于对状态转移概率的信息的可用性。此研究进一步利用 DNN 或 DQN 更高效地学习决策策略。类似的模型在
    [[108](#bib.bib108)] 中也被研究，其中计算卸载被表述为一个 MDP，以最小化计算卸载的成本。MDP 的解决方案可以用于通过监督学习训练
    DNN。经过良好训练的 DNN 随后应用于未知的网络条件下进行实时决策。模拟结果显示，使用深度监督学习在卸载准确性和成本节省方面取得了显著的性能提升。'
- en: Data and computation offloading is also used in fog computing. The mobile application
    demanding a set of data and computational resources can be hosted in a container,
    e.g., virtual machine of a fog node. With user’s mobility, the container has to
    be migrated or offloaded to other nodes and dynamically consolidated. With the
    container migration, some nodes with low resource utilization can be switched
    off to reduce power consumption. The authors in [[109](#bib.bib109)] model the
    container migration as a multi-dimensional MDP, which is solved by the DQL. The
    system states consist of the delay, the power consumption and the migration cost.
    The action includes the selection policy that selects the containers to be emigrated
    from each source node, and the allocation policy that determines the destination
    node of each container. The action space can be optimized for more efficient exploration
    by dividing fog nodes into under-utilization, normal-utilization, and over-utilization
    groups. By powering off under-utilization nodes, all their containers will be
    migrated to other nodes to reduce power consumption. The training process is also
    optimized by using DDQN and PER which assigns different priorities to the transitions
    in experience memory. This helps the DQL agent at each fog node to perform better
    in terms of faster learning speed and more stability. Simulation results reveal
    that the DQL scheme achieves fast decision-making and outperforms the existing
    baseline approaches significantly in terms of delay, power consumption, and migration
    cost.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 数据和计算卸载也被用于雾计算中。需要一组数据和计算资源的移动应用程序可以托管在容器中，例如，雾节点的虚拟机。随着用户的移动，容器必须迁移或卸载到其他节点，并动态整合。通过容器迁移，一些资源利用率低的节点可以被关闭，以减少功耗。作者在[[109](#bib.bib109)]中将容器迁移建模为多维马尔可夫决策过程（MDP），并通过深度Q学习（DQL）进行求解。系统状态包括延迟、功耗和迁移成本。动作包括选择策略，该策略选择从每个源节点迁移的容器，以及分配策略，该策略确定每个容器的目的节点。通过将雾节点划分为资源不足、正常利用和资源过度利用组，可以优化动作空间以实现更高效的探索。通过关闭资源不足的节点，所有容器将迁移到其他节点以减少功耗。训练过程也通过使用双深度Q网络（DDQN）和优先经验回放（PER）进行优化，后者为经验记忆中的转移分配不同的优先级。这有助于每个雾节点的DQL代理在学习速度更快和稳定性更高方面表现更好。仿真结果表明，DQL方案实现了快速决策，并在延迟、功耗和迁移成本方面显著优于现有基准方法。
- en: 'Summary: This section reviews the applications of the DQL for wireless caching
    and data/computation offloading, which are inherently coupled with networking
    and allocation of channel capacity, computational resources, and caching capabilities,
    etc. We observe that the DQL framework for caching is typically centralized and
    mostly implemented at the network controller, e.g., the BS, service provider,
    and central scheduler, which is more powerful in information collection and cross-layer
    policy design. On the contrary, the end users have more control over their offloading
    decisions, and hence we observe more popular implementation of the DQL agent at
    local devices, e.g., mobile users, IoT devices, and fog nodes. Though an orchestration
    of networking, caching, data and computation offloading in one unified DQL framework
    is promising for network performance maximization, we face many challenges in
    designing highly-stable and fast-convergent learning algorithms, due to excessive
    delay and unsynchronized information collection from different network entities.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：本节回顾了DQL在无线缓存和数据/计算卸载中的应用，这些应用本质上与网络和信道容量、计算资源和缓存能力等的分配密切相关。我们观察到，DQL框架在缓存中的应用通常是集中式的，大多在网络控制器（如基站、服务提供商和中央调度器）上实现，这在信息收集和跨层策略设计方面更为强大。相反，最终用户对他们的卸载决策拥有更多控制权，因此我们观察到DQL代理在本地设备（如移动用户、物联网设备和雾节点）上的实现更为普遍。尽管将网络、缓存、数据和计算卸载整合到一个统一的DQL框架中对网络性能最大化具有潜力，但由于不同网络实体的延迟过高和信息收集不同步，我们在设计高度稳定且收敛速度快的学习算法方面面临许多挑战。
- en: 'TABLE IV: A summary of approaches using DQL for caching and offloading.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：使用DQL进行缓存和卸载的方案汇总。
- en: '| Issues | Ref. | Model | Learning algorithms | Agent | States | Actions |
    Rewards | Networks |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 参考文献 | 模型 | 学习算法 | 代理 | 状态 | 动作 | 奖励 | 网络 |'
- en: '| Wireless proactive caching | [[70](#bib.bib70)] | MDP | DQN using actor-critic,
    DDPG | Base station | Cached contents and requested content | Replace selected
    content or not | Cache hit rate |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 无线主动缓存 | [[70](#bib.bib70)] | MDP | 使用演员-评论家、DDPG的DQN | 基站 | 缓存内容和请求内容 |
    是否替换选定内容 | 缓存命中率 |  |'
- en: '| (score 1 or 0) | CRN |  |  |  |  |  |  |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| (评分 1 或 0) | CRN |  |  |  |  |  |  |  |'
- en: '|  | [[84](#bib.bib84)] | MDP | DQN using FNN | Base station | Channel states
    and computational capabilities | User association, computational unit, content
    delivery | Energy consumption | CRN |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | [[84](#bib.bib84)] | MDP | 使用FNN的DQN | 基站 | 信道状态和计算能力 | 用户关联、计算单元、内容传输
    | 能耗 | CRN |'
- en: '|  | [[74](#bib.bib74)] | MDP | DQN using NAFs | Cloud database | Encoding
    of a query, query cache miss rate | Cache expiration times | Cache hit rates,
    CDN utilization | Cloud database |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | [[74](#bib.bib74)] | MDP | 使用NAFs的DQN | 云数据库 | 查询的编码，查询缓存缺失率 | 缓存过期时间
    | 缓存命中率，CDN利用率 | 云数据库 |'
- en: '|  | [[76](#bib.bib76)][[77](#bib.bib77)] | MDP | DQN using FNN | Central scheduler
    | Channel coefficients, cache state | Active users and resource allocation | Network
    throughput | MU MIMO system |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | [[76](#bib.bib76)][[77](#bib.bib77)] | MDP | 使用FNN的DQN | 中央调度器 | 信道系数，缓存状态
    | 活跃用户和资源分配 | 网络吞吐量 | MU MIMO系统'
- en: '|  | [[78](#bib.bib78)] | MDP | DQN using CNN | Central scheduler | Channel
    coefficients, cache state | Active users and resource allocation | Network throughput
    | MU MIMO system |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | [[78](#bib.bib78)] | MDP | 使用CNN的DQN | 中央调度器 | 信道系数，缓存状态 | 活跃用户和资源分配 |
    网络吞吐量 | MU MIMO系统 |'
- en: '|  | [[79](#bib.bib79)] | MDP | DDQN | Service provider | Conditions of cache
    nodes, transmission rates of content chunks | The content chunks to cache and
    to remove | Network cost, QoE | Content centric IoT |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | [[79](#bib.bib79)] | MDP | DDQN | 服务提供商 | 缓存节点的状态，内容块的传输速率 | 要缓存和移除的内容块
    | 网络成本，QoE | 以内容为中心的物联网 |'
- en: '|  | [[81](#bib.bib81)] | MDP | DQN using LSM and ESN | Base station | Historical
    content request | User association, cached contents and formats | Reliability
    | Cellular system |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | [[81](#bib.bib81)] | MDP | 使用LSM和ESN的DQN | 基站 | 历史内容请求 | 用户关联、缓存内容和格式
    | 可靠性 | 蜂窝系统 |'
- en: '|  | [[86](#bib.bib86)][[89](#bib.bib89)] | MDP | DQN using CNN | Service provider
    | Available BS, MEC, and cache | User association, caching, and offloading | Composite
    revenue | Vehicular ad hoc network |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | [[86](#bib.bib86)][[89](#bib.bib89)] | MDP | 使用CNN的DQN | 服务提供商 | 可用的基站、MEC和缓存
    | 用户关联、缓存和卸载 | 综合收益 | 车载自组网 |'
- en: '|  | [[85](#bib.bib85)] | MDP | DQN using FNN | Service provider | Available
    BS, MEC, and cache | User association, caching, and offloading | Composite revenue
    | Vehicular ad hoc network |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | [[85](#bib.bib85)] | MDP | 使用FNN的DQN | 服务提供商 | 可用的基站、MEC和缓存 | 用户关联、缓存和卸载
    | 综合收益 | 车载自组网 |'
- en: '|  | [[87](#bib.bib87)] | MDP | DDQN and dueling DQN | Service provider | Available
    BS, MEC, and cache | User association, caching, and offloading | Composite revenue
    | Vehicular ad hoc network |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  | [[87](#bib.bib87)] | MDP | DDQN和对抗DQN | 服务提供商 | 可用的基站、MEC和缓存 | 用户关联、缓存和卸载
    | 综合收益 | 车载自组网 |'
- en: '|  | [[91](#bib.bib91)] | MDP | DQN using CNN | Base station | Channel state,
    computational capability, content/version indicator, and the trust value | User
    association, caching, and offloading | Revenue | Mobile social network |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | [[91](#bib.bib91)] | MDP | 使用CNN的DQN | 基站 | 信道状态、计算能力、内容/版本指示器和信任值 | 用户关联、缓存和卸载
    | 收益 | 移动社交网络 |'
- en: '| Data and computation offloading | [[93](#bib.bib93)] | MDP | DQN using CNN
    | Mobile user | User’s location and remaining file size | Idle, transmit via WLAN
    or cellular network | Total data rate | Cellular system |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 数据和计算卸载 | [[93](#bib.bib93)] | MDP | 使用CNN的DQN | 移动用户 | 用户的位置和剩余文件大小 | 空闲、通过WLAN或蜂窝网络传输
    | 总数据速率 | 蜂窝系统 |'
- en: '|  | [[94](#bib.bib94)] | MDP | DQN using FNN | Base station | Sum of cost
    and computational capacity of the MEC server | Offloading decision and resource
    allocation | Sum of cost of delay and energy consumption | Cellular system |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  | [[94](#bib.bib94)] | MDP | 使用FNN的DQN | 基站 | MEC服务器的成本和计算能力之和 | 卸载决策和资源分配
    | 延迟成本和能耗成本之和 | 蜂窝系统 |'
- en: '|  | [[95](#bib.bib95)] | MDP | DQN using FNN | Mobile user | Channel qualities,
    states of energy and task queues | Offloading and resource allocation | Long term
    cost function | Cellular system |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | [[95](#bib.bib95)] | MDP | 使用FNN的DQN | 移动用户 | 信道质量、能量状态和任务队列状态 | 卸载和资源分配
    | 长期成本函数 | 蜂窝系统 |'
- en: '|  | [[96](#bib.bib96)] | MDP | DDQN, SARSA | Mobile user | Channel qualities,
    states of energy and task queues | Offloading decision and computational resource
    allocation | Long term cost function | Cellular system |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | [[96](#bib.bib96)] | MDP | DDQN，SARSA | 移动用户 | 信道质量、能量状态和任务队列状态 | 卸载决策和计算资源分配
    | 长期成本函数 | 蜂窝系统 |'
- en: '|  | [[100](#bib.bib100)] | Game | DQN using CNN, hotbooting Q-learning | Mobile
    user | Channel states, size of App traces | Offloading rate | Utility related
    to detection accuracy, response speed, and the transmission cost | Cellular system
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | [[100](#bib.bib100)] | 游戏 | 使用 CNN 的 DQN，热启动 Q 学习 | 移动用户 | 信道状态、应用程序轨迹大小
    | 卸载速率 | 与检测准确性、响应速度和传输成本相关的效用 | 蜂窝系统 |'
- en: '|  | [[109](#bib.bib109)] | MDP | DDQN | Fog node | Delay, container’s location
    and resource allocation | Container’s next location | Composite utility related
    to delay, power consumption, and migration cost | Fog computing |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  | [[109](#bib.bib109)] | MDP | DDQN | 边缘节点 | 延迟、容器位置和资源分配 | 容器的下一个位置 | 与延迟、功耗和迁移成本相关的综合效用
    | 边缘计算 |'
- en: V Network Security and Connectivity Preservation
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 网络安全和连接性保持
- en: 'Future networks become more decentralized and ad-hoc in nature which are vulnerable
    to various attacks such as Denial-of-Service (DoS) and cyber-physical attack.
    Recently, the DQL has been used as an effective solution to avoid and prevent
    the attacks. In this section, we review the applications of DQL in addressing
    the following security issues:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的网络变得更加去中心化和临时，这使其易受各种攻击，如拒绝服务（DoS）攻击和网络物理攻击。最近，DQL 已被作为一种有效的解决方案来避免和防止攻击。在本节中，我们回顾了
    DQL 在解决以下安全问题中的应用。
- en: •
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Jamming attack: In the jamming attack, attackers as jammers transmit Radio
    Frequency (RF) jamming signals with high power to cause interference to the legitimate
    communication channels, thus reducing the SINR at legitimate receivers. Anti-jamming
    techniques such as the frequency hopping [[110](#bib.bib110)] and user mobility,
    i.e., moving out from the heavy jamming area, have been commonly used. However,
    without being aware of the radio channel model and the jamming methods, it is
    challenging for the users to choose an appropriate frequency channel as well as
    to determine how to leave and avoid the attack. DQL enables the users to learn
    an optimal policy based on their past observations, and thus DQL can be used to
    address the above challenge.'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 干扰攻击：在干扰攻击中，攻击者作为干扰者传输高功率的射频（RF）干扰信号，导致合法通信信道受到干扰，从而降低合法接收者的信号干扰噪声比。抗干扰技术如频率跳变
    [[110](#bib.bib110)] 和用户移动，即离开严重干扰区域，已经被广泛使用。然而，在不了解无线电信道模型和干扰方法的情况下，用户选择合适的频率信道以及确定如何离开并避免攻击是具有挑战性的。DQL
    使用户能够基于其过去的观察学习最佳策略，因此 DQL 可以用于解决上述挑战。
- en: •
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cyber-physical attack: The cyber-physical attack is an integrity attack in
    which an attacker manipulates data to alter control signals in the system. This
    attack often happens in autonomous systems such as Intelligent Transportation
    Systems (ITSs) and increases the risk of accidents to Autonomous Vehicles (AVs).
    The DQL allows the AVs to learn optimal actions based on their time-varying observations
    of the attacker’ activities. Thus, the DQL can be used to achieve robust and dynamic
    control of the AV to the attacks.'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网络物理攻击：网络物理攻击是一种完整性攻击，其中攻击者操控数据以改变系统中的控制信号。这种攻击通常发生在智能交通系统（ITSs）等自主系统中，并增加了对自主车辆（AVs）事故的风险。DQL
    允许 AVs 根据对攻击者活动的时间变化观察学习最佳行动。因此，DQL 可以用于实现对攻击的鲁棒和动态控制。
- en: •
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Connectivity preserving: This refers to maintaining the connectivity among
    the robots, e.g., UAVs, to support the communication and exchange of information
    among them. The system and network environment is generally dynamic and complex,
    and thus the DQL which allows each robot to make dynamic decisions based on its
    state can be effectively used to preserve the connectivity in the system.'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 连接性保持：这指的是维护机器人之间的连接性，例如无人机，以支持它们之间的通信和信息交换。系统和网络环境通常是动态和复杂的，因此允许每个机器人根据其状态做出动态决策的
    DQL 可以有效地用来保持系统中的连接性。
- en: V-A Network Security
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 网络安全
- en: This section discusses the applications of DQL to address the jamming attack
    and the cyber-physical attack.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了 DQL 在应对干扰攻击和网络物理攻击中的应用。
- en: V-A1 Jamming Attack
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 干扰攻击
- en: '![Refer to caption](img/0cd6213775e8ecc76f10e8c2dff3242d.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0cd6213775e8ecc76f10e8c2dff3242d.png)'
- en: 'Figure 9: Jamming attack in cognitive radio network [[111](#bib.bib111)].'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：认知无线电网络中的干扰攻击 [[111](#bib.bib111)]。
- en: 'A pioneer work using the DQL for the anti-jamming is [[111](#bib.bib111)].
    The network model is a Cognitive Radio Network (CRN) as shown in Fig. [9](#S5.F9
    "Figure 9 ‣ V-A1 Jamming Attack ‣ V-A Network Security ‣ V Network Security and
    Connectivity Preservation ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey") which consists of one Secondary User (SU), multiple
    Primary Users (PUs), and multiple jammers. The network has a set of frequency
    channels for hopping. At each time slot, each jammer can arbitrarily select one
    of the channels to send its jamming signal, and the SU, i.e., the agent, needs
    to choose a proper action based on the SU’s current state. The action is (i) selecting
    one of the channels to send its signals or (ii) leaving the area to connect to
    another BS. The jammers are assumed to avoid causing interference to the PUs,
    and thus the SU’s current state consists of the number of PUs and the discretized
    SINR of the SU signal at the last time slot. The objective of the SU is to maximize
    its expected discounted utility over time slots. Note that when the SU chooses
    to leave the area to connect to another BS, it spends a mobility cost. Thus, the
    utility is defined as a function of the SINR of the SU signal and the mobility
    cost. Since the number of frequency channels may be large that results in a large
    action set, the CNN is used for the DQL to quickly learn the optimal policy. As
    shown in the simulation results, the proposed DQL has a faster convergence speed
    than that of the Q-learning algorithm. Moreover, considering the scenario with
    two jammers, the proposed DQL outperforms the frequency-hopping method in terms
    of the SINR and the mobility cost.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '使用DQL进行抗干扰的开创性工作是[[111](#bib.bib111)]。网络模型是一个认知无线电网络（CRN），如图[9](#S5.F9 "Figure
    9 ‣ V-A1 Jamming Attack ‣ V-A Network Security ‣ V Network Security and Connectivity
    Preservation ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey")所示，包括一个次级用户（SU）、多个主要用户（PU）和多个干扰源。网络有一组用于跳频的频率频道。在每个时间槽中，每个干扰源可以任意选择一个频道发送其干扰信号，而SU，即代理，需要根据SU的当前状态选择适当的行动。行动包括（i）选择一个频道发送信号或（ii）离开该区域连接到另一个基站。假设干扰源避免对PU造成干扰，因此SU的当前状态包括PU的数量和SU信号在上一个时间槽的离散化SINR。SU的目标是最大化其在时间槽上的期望折扣效用。注意，当SU选择离开该区域连接到另一个基站时，会花费移动成本。因此，效用被定义为SU信号的SINR和移动成本的函数。由于频率频道数量可能很大，从而导致行动集很大，因此使用CNN进行DQL以快速学习最优策略。如模拟结果所示，所提出的DQL比Q学习算法具有更快的收敛速度。此外，在两个干扰源的场景中，所提出的DQL在SINR和移动成本方面优于频率跳跃方法。'
- en: The model in [[111](#bib.bib111)] is constrained to two jammers. As the number
    of jammers in the network increases, the proposed scheme may not be effective.
    The reason is that it becomes hard for the SU to find good actions when the number
    of jammed channels increases. An appropriate solution, as proposed in  [[112](#bib.bib112)],
    allows the receiver of the SU to leave its current location. Since the leaving
    incurs the mobility cost, the receiver, i.e., the agent, needs an optimal policy,
    i.e., staying at or leaving the current location, to maximize its utility. In
    this scenario, the DQL based on CNN can be used for the receiver to find the optimal
    action to maximize its expected utility. Here, the utility and state of the receiver
    are essentially defined similarly to that of the agent in [[111](#bib.bib111)].
    In particular, the state includes the discretized SINR of the signal measured
    by the receiver at the last time slot.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在[[111](#bib.bib111)]中被限制为两个干扰源。随着网络中干扰源数量的增加，所提出的方案可能效果不佳。原因是当干扰频道数量增加时，SU很难找到好的行动。一个合适的解决方案，如在[[112](#bib.bib112)]中提出的，允许SU的接收机离开当前的位置。由于离开会产生移动成本，因此接收机，即代理，需要一个最优策略，即留在当前的位置或离开当前的位置，以最大化其效用。在这种情况下，可以使用基于CNN的DQL来帮助接收机找到最优行动以最大化其期望效用。在这里，接收机的效用和状态基本上与[[111](#bib.bib111)]中代理的定义相似。特别地，状态包括接收机在上一个时间槽测量到的离散化SINR信号。
- en: The above approaches, i.e., in [[111](#bib.bib111)] and [[112](#bib.bib112)],
    define states of the agents based on raw SINR values of the signals. In practical
    wireless environments, the number of SINR values may be large and even infinite.
    Moreover, the raw SINR can be inaccurate and noisy. To cope with the challenge
    of the infinite number of states, the DQL can use a recursive Convolutional Neural
    Network (RCNN) as proposed in [[113](#bib.bib113)]. By using the pre-processing
    layer and recursive convolution layers, the RCNN is able to remove noise from
    the network environment and extract useful features of the SINR, i.e., discrete
    spectrum sample values greater than a noise threshold, thus reducing the computational
    complexity. The network model and the problem formulation considered in [[113](#bib.bib113)]
    are similar to those in [[111](#bib.bib111)]. However, instead of directly using
    the raw SINR, the state of the SU is the extracted features of the SINR. Also,
    the action of the SU includes only frequency-hopping decision. The simulation
    results show that the proposed DQL based on the RCNN can converge in both fixed
    and dynamic jamming scenarios while the Q-learning cannot converge in the dynamic
    jamming one. Furthermore, the proposed DQL can achieve the average throughput
    close to that of the optimal scheme, i.e., an anti-jamming scheme with completely
    known jamming actions.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法，即[[111](#bib.bib111)]和[[112](#bib.bib112)]，基于信号的原始SINR值定义了代理的状态。在实际无线环境中，SINR值的数量可能非常大，甚至是无限的。此外，原始SINR可能不准确且有噪声。为应对无限状态数量的挑战，DQL可以使用递归卷积神经网络（RCNN），如[[113](#bib.bib113)]中所提。通过使用预处理层和递归卷积层，RCNN能够去除网络环境中的噪声并提取SINR的有用特征，即大于噪声阈值的离散谱样本值，从而减少计算复杂度。[[113](#bib.bib113)]中考虑的网络模型和问题表述与[[111](#bib.bib111)]中的类似。然而，不是直接使用原始SINR，SU的状态是提取的SINR特征。此外，SU的动作仅包括频率跳跃决策。模拟结果显示，基于RCNN的DQL在固定和动态干扰场景中均能收敛，而Q学习在动态干扰场景中无法收敛。此外，所提DQL的平均吞吐量接近于最优方案，即完全已知干扰行为的抗干扰方案。
- en: Instead of finding the frequency-hopping decisions, the authors in [[114](#bib.bib114)]
    propose the use of DQL to find an optimal power control policy for the anti-jamming.
    The model is an IoT network including IoT devices and one jammer. The jammer can
    observe the communications of the transmitter and chooses a jamming strategy to
    reduce the SINR at the receiver. Thus, the transmitter chooses an action, i.e.,
    transmit power level, to maximize its utility. Here, the utility is the difference
    between the SINR and the energy consumption cost due to the transmission. Note
    that choosing the transmit power impacts the future jamming strategy, and thus
    the interaction between the transmitter and the jammer can be formulated as an
    MDP. The transmitter is the agent, and the state is SINR measured at its receiver
    at the last time slot. The DQN using the CNN is then adopted to find an optimal
    power control policy for the transmitter to maximize its expected accumulated
    discounted reward, i.e., the utility, over time slots. The simulation results
    show that the proposed DQL can improve the utility of the transmitter up to 17.7%
    compared with the Q-learning scheme. Also, the proposed DQL reduces the utility
    of the jammer around 18.1% compared with the Q-learning scheme.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 为了寻找最佳的抗干扰功率控制策略，[[114](#bib.bib114)]中的作者提出使用DQL来替代频率跳跃决策。该模型包括IoT设备和一个干扰源。干扰源可以观察发射机的通信并选择一种干扰策略以降低接收机的SINR。因此，发射机选择一个动作，即发射功率级别，以最大化其效用。在这里，效用是SINR与由于传输造成的能量消耗成本之间的差异。需要注意的是，选择发射功率会影响未来的干扰策略，因此发射机与干扰源之间的互动可以被表述为MDP。发射机是代理，状态是上一个时间槽的接收机测得的SINR。然后采用使用CNN的DQN来为发射机找到一个最佳的功率控制策略，以最大化其期望的累计折扣奖励，即效用。模拟结果显示，所提DQL可以将发射机的效用提高多达17.7%，相比于Q学习方案。此外，所提DQL将干扰源的效用降低了大约18.1%，相比于Q学习方案。
- en: '![Refer to caption](img/7dca9c8fb69c8a571ee0afd8168a77ad.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7dca9c8fb69c8a571ee0afd8168a77ad.png)'
- en: 'Figure 10: Anti-jamming scheme based on UAV [[115](#bib.bib115)].'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '图10: 基于UAV的抗干扰方案[[115](#bib.bib115)]。'
- en: 'To prevent the jammer’s observations of communications, the transmitter can
    change its communication strategy, e.g., by using relays that are far from the
    jamming area. The relays can be UAVs as proposed in [[115](#bib.bib115)]. The
    model consists of one UAV, i.e., a relay, one jammer, one mobile user and its
    serving BS (see Fig. [10](#S5.F10 "Figure 10 ‣ V-A1 Jamming Attack ‣ V-A Network
    Security ‣ V Network Security and Connectivity Preservation ‣ Applications of
    Deep Reinforcement Learning in Communications and Networking: A Survey")). The
    mobile user transmits messages to its server via the serving BS. In the case that
    the serving BS is heavily jammed, the UAV helps the mobile user to relay the messages
    to the server through a backup BS. In particular, depending on the SINR and Bit
    Error Rate (BER) values sent from the serving BS, the UAV as an agent decides
    the relay power level to maximize its utility, i.e., the difference between the
    SINR and the relay cost. The relay power level can be considered to be the UAV’s
    actions, and the SINR and BER are its states. As such, the next state observed
    by the UAV is independent of all the past states and actions. The problem is formulated
    as an MDP. To quickly achieve the optimal relay policy for the UAV, the DQL based
    on CNN is then adopted. The simulation results in [[115](#bib.bib115)] show that
    the proposed DQL scheme takes only 200 time slots to converge to the optimal policy,
    which is 83.3% less than that of the relay scheme based on Q-learning [[116](#bib.bib116)].
    Moreover, the proposed DQL scheme reduces the BER of the user by 46.6% compared
    with the hill climbing-based UAV relay scheme [[117](#bib.bib117)].'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '为了防止干扰器对通信的观察，发射器可以改变其通信策略，例如，使用远离干扰区域的中继。中继可以是无人机，如在[[115](#bib.bib115)]中提出的那样。该模型包括一个无人机，即一个中继，一个干扰器，一个移动用户及其服务基站（见图[10](#S5.F10
    "Figure 10 ‣ V-A1 Jamming Attack ‣ V-A Network Security ‣ V Network Security and
    Connectivity Preservation ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")）。移动用户通过服务基站向其服务器传输消息。如果服务基站受到严重干扰，无人机会帮助移动用户通过备用基站中继消息给服务器。具体来说，根据从服务基站发送的SINR和比特误码率（BER）值，无人机作为一个代理决定中继功率水平以最大化其效用，即SINR与中继成本之间的差异。中继功率水平可以视为无人机的动作，SINR和BER则为其状态。因此，无人机观察到的下一个状态与所有过去的状态和动作无关。该问题被形式化为一个MDP。为了快速实现无人机的最优中继策略，随后采用了基于CNN的DQL。[[115](#bib.bib115)]中的模拟结果显示，所提出的DQL方案仅需200个时间槽即可收敛到最优策略，比基于Q学习的中继方案减少了83.3%。此外，与基于爬山算法的无人机中继方案[[117](#bib.bib117)]相比，所提出的DQL方案将用户的BER降低了46.6%。'
- en: 'The scheme proposed in [[115](#bib.bib115)] assumes that the relay UAV is sufficiently
    far from the jamming area. However, as illustrated in Fig. [10](#S5.F10 "Figure
    10 ‣ V-A1 Jamming Attack ‣ V-A Network Security ‣ V Network Security and Connectivity
    Preservation ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey"), the attacker can use a compromised UAV close to the relay
    UAV to launch the jamming attack to the relay UAV. In such a scenario, the authors
    in [[118](#bib.bib118)] show that the DQL can still be used to address the attack.
    The system model is based on physical layer security and consists of one UAV and
    one attacker. The attacker is assumed to be “smarter” than that in the model in [[115](#bib.bib115)].
    This means that the attacker can observe channels that the UAV uses to communicate
    with the BS in the past time slots and then chooses jamming power levels on the
    target channels. Therefore, the UAV needs to find a power allocation policy, i.e.,
    transmit power levels on the channels, to maximize the secrecy capacity of the
    UAV-BS communication. Similar to [[115](#bib.bib115)], the DQL based on CNN is
    used which enables the UAV to choose its actions, i.e., transmit power levels
    on the channels, based on its state, i.e., the attacker’s jamming power level
    in the last time slot. The reward is the difference between the secrecy capacity
    of the UAV and BS and the energy consumption cost.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[[115](#bib.bib115)]中提出的方案假设中继无人机远离干扰区域。然而，如图[10](#S5.F10 "Figure 10 ‣ V-A1
    Jamming Attack ‣ V-A Network Security ‣ V Network Security and Connectivity Preservation
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")所示，攻击者可以利用靠近中继无人机的被攻陷无人机对中继无人机发起干扰攻击。在这种情况下，[[118](#bib.bib118)]的作者展示了DQL仍然可以用来应对这种攻击。该系统模型基于物理层安全，包含一个无人机和一个攻击者。假设攻击者比[[115](#bib.bib115)]模型中的攻击者“更聪明”。这意味着攻击者可以观察无人机在过去时间段与基站通信的信道，然后选择目标信道的干扰功率水平。因此，无人机需要找到一个功率分配策略，即在信道上的发射功率水平，以最大化无人机-基站通信的保密容量。类似于[[115](#bib.bib115)]，使用基于CNN的DQL，这使得无人机可以根据其状态，即上一个时间段的攻击者干扰功率水平，选择其行动，即信道上的发射功率水平。奖励是无人机和基站保密容量与能源消耗成本之间的差异。'
- en: The simulation results in [[118](#bib.bib118)] show that the proposed DQL can
    improve the UAV’s utility up to 13% compared with the baseline scheme [[119](#bib.bib119)]
    which uses the Win or Learn Faster-Policy Hill Climbing (WoLF-PHC) to prevent
    the attack. Also, the safe rate of the UAV, i.e., the probability that the UAV
    is attacked, obtained by the proposed DQL is 7% higher than that of the baseline.
    However, the proposed DQL is applied only to a single-UAV system. For the future
    work, scenarios with multiple UAVs need to be considered. In such a scenario,
    more computational overhead is expected and multi-agent DQL algorithms can be
    applied.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[[118](#bib.bib118)]中的仿真结果显示，提出的DQL可以将无人机的效用提高多达13%，相比基线方案[[119](#bib.bib119)]，该方案使用Win
    or Learn Faster-Policy Hill Climbing (WoLF-PHC)来防止攻击。此外，提出的DQL获得的无人机安全率，即无人机被攻击的概率，比基线高出7%。然而，提出的DQL仅适用于单一无人机系统。未来的工作中需要考虑多无人机的场景。在这种情况下，预计会有更多的计算开销，并且可以应用多智能体DQL算法。'
- en: V-A2 Cyber-Physical Attack
  id: totrans-335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 网络物理攻击
- en: In autonomous systems such as ITSs, the attacker can seek to inject faulty data
    to information transmitted from the sensors to the AVs. The AVs which receive
    the injected information may inaccurately estimate the safe spacing among them.
    This increases the risk of AV accidents. Vehicular communication security algorithms,
    e.g., [[120](#bib.bib120)], can be used to minimize the spacing deviation. However,
    the attacker’s actions in these algorithms are assumed to be stable which may
    not be applicable in practical systems. The DQL that enables the AVs to learn
    optimal actions based on their time-varying observations of the attacker’ actions
    can be thus used.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在如ITS这样的自主系统中，攻击者可以试图向传感器到自动驾驶车辆（AVs）传输的信息中注入错误数据。接收到注入信息的AVs可能会不准确地估算它们之间的安全间距，这增加了AV事故的风险。可以使用车辆通信安全算法，例如[[120](#bib.bib120)]，来最小化间距偏差。然而，这些算法中的攻击者行为假设是稳定的，这可能在实际系统中不适用。因此，可以使用DQL，使AVs能够根据其对攻击者行为的时间变化观察学习最佳行动。
- en: '![Refer to caption](img/8c9737ddbf646022c024c845ceba6936.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8c9737ddbf646022c024c845ceba6936.png)'
- en: 'Figure 11: Car-following model with cyber-physical attack.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：带有网络物理攻击的跟车模型。
- en: 'The first work using the DQL for the cyber-physical attack in an ITS can be
    found in [[121](#bib.bib121)]. The system is a car-following model [[122](#bib.bib122)]
    of the General Motors as shown in Fig. [11](#S5.F11 "Figure 11 ‣ V-A2 Cyber-Physical
    Attack ‣ V-A Network Security ‣ V Network Security and Connectivity Preservation
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey"). In the model, each AV updates its speed based on measurement information
    received from the closest road smart sensors. The attacker attempts to inject
    faulty data to the measurement information. However, the attacker cannot inject
    the measurements of different sensors equally due to its resource constraint.
    Thus, the AV can choose less-faulty measurements by selecting a vector of measurement
    weights. The objective of the attacker is to maximize the deviation, i.e., the
    utility, from the safe spacing between the AV and its nearby AV while that of
    the AV is to minimize the deviation. The interaction between the attacker and
    the AV can be modeled as a zero-sum game. The authors in [[121](#bib.bib121)]
    show that the DQL can be used to find the equilibrium strategies. In particular,
    the action of the AV is to choose a weight vector. Its state includes the past
    actions, i.e., the weight vectors, and the past deviation values. Since the actions
    and deviations have continuous values, the state space is infinite. Thus, LSTM
    units that are able to extract useful features are adopted for the DQL to reduce
    the state space. The simulation results show that by using the past actions and
    deviations for learning the attacker’s action, the proposed DQL scheme can guarantee
    a lower steady-state deviation than the Kalmar filter-based scheme [[120](#bib.bib120)].
    Moreover, by using the LSTM units, the results show that the proposed DQL scheme
    can converge much faster than the baseline scheme.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '使用深度 Q 学习（DQL）来应对 ITS 中的网络物理攻击的首个研究可以在[[121](#bib.bib121)]中找到。该系统是通用汽车（General
    Motors）的跟车模型[[122](#bib.bib122)]，如图[11](#S5.F11 "Figure 11 ‣ V-A2 Cyber-Physical
    Attack ‣ V-A Network Security ‣ V Network Security and Connectivity Preservation
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")所示。在该模型中，每辆自动驾驶车（AV）根据从最近的道路智能传感器接收到的测量信息更新其速度。攻击者试图向测量信息中注入错误数据。然而，由于资源限制，攻击者不能均等地注入不同传感器的测量值。因此，AV
    可以通过选择测量权重向量来选择较少错误的测量值。攻击者的目标是最大化偏差，即与其附近的 AV 之间安全间距的效用，而 AV 的目标是最小化偏差。攻击者与 AV
    之间的互动可以建模为一个零和博弈。[[121](#bib.bib121)]中的作者展示了 DQL 可以用来找到均衡策略。具体来说，AV 的动作是选择一个权重向量。它的状态包括过去的动作，即权重向量，以及过去的偏差值。由于动作和偏差具有连续值，因此状态空间是无限的。因此，采用了能够提取有用特征的
    LSTM 单元来减少状态空间。模拟结果显示，通过利用过去的动作和偏差来学习攻击者的行为，所提出的 DQL 方案可以保证比基于 Kalmar 滤波器的方案[[120](#bib.bib120)]具有更低的稳态偏差。此外，通过使用
    LSTM 单元，结果表明所提出的 DQL 方案比基线方案收敛得更快。'
- en: 'Another work that uses the LSTM to extract useful features from the measurement
    information to detect the cyber-physical attack is proposed in [[123](#bib.bib123)].
    The model is an IoT system including a cloud and a set of IoT devices. The IoT
    devices generate signals and transmit the signals to the cloud (see Fig. [12](#S5.F12
    "Figure 12 ‣ V-A2 Cyber-Physical Attack ‣ V-A Network Security ‣ V Network Security
    and Connectivity Preservation ‣ Applications of Deep Reinforcement Learning in
    Communications and Networking: A Survey")). The cloud uses the received signals
    for estimation and control of the IoT devices’ operation. An attacker can launch
    the cyber-physical attack by manipulating the IoT devices’ output signals that
    causes control errors at the cloud and degrades the performance of the IoT system.
    To detect the attack, the cloud uses LSTM units to extract stochastic features
    or fingerprints such as flatness, skewness, and kurtosis, of the IoT devices’
    signals. The cloud sends the fingerprints back to the IoT devices, and the IoT
    devices embed, i.e., watermark, the fingerprints inside the signals. The cloud
    uses the fingerprints to authenticate the IoT devices’ signals to detect the attack.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '另一项利用LSTM从测量信息中提取有用特征以检测网络物理攻击的工作被提出于[[123](#bib.bib123)]。该模型是一个包括云平台和一组物联网设备的物联网系统。物联网设备生成信号并将信号传输到云平台（见图[12](#S5.F12
    "Figure 12 ‣ V-A2 Cyber-Physical Attack ‣ V-A Network Security ‣ V Network Security
    and Connectivity Preservation ‣ Applications of Deep Reinforcement Learning in
    Communications and Networking: A Survey")）。云平台利用接收到的信号对物联网设备的操作进行估计和控制。攻击者可以通过操控物联网设备的输出信号发起网络物理攻击，这会在云平台上造成控制错误，降低物联网系统的性能。为了检测攻击，云平台使用LSTM单元提取物联网设备信号的随机特征或指纹，如平坦度、偏度和峰度。云平台将指纹发送回物联网设备，物联网设备将指纹嵌入，即水印，信号中。云平台利用这些指纹对物联网设备的信号进行身份验证，以检测攻击。'
- en: '![Refer to caption](img/c355e211fe48250cd1c54f43d026ad50.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c355e211fe48250cd1c54f43d026ad50.png)'
- en: 'Figure 12: Cyber-physical detection in IoT systems using DQL.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：使用DQL在物联网系统中的网络物理检测。
- en: 'The algorithm proposed in [[123](#bib.bib123)] is also called dynamic watermarking
    [[124](#bib.bib124)] which is able to detect the cyber-physical attack and to
    prevent eavesdropping attacks. However, the algorithm requires large computational
    resources at the cloud for the IoT device signal authentication. Consequently,
    the cloud can only authenticate a limited number of vulnerable IoT devices. The
    cloud can choose the vulnerable IoT devices by observing their security status.
    However, this can be impractical since the IoT devices may not report their security
    status. Thus, the authors in [[125](#bib.bib125)] propose to use the DQL that
    enables the cloud to decide which IoT devices to authenticate with the incomplete
    information. Since IoT devices with more valuable data are likely to be attacked,
    the reward is defined as a function of data values of IoT devices. The cloud’s
    state includes attack actions of the attacker on the IoT devices in the past time
    slots. The actions of the attacker on the IoT devices can be obtained by using
    the dynamic watermarking algorithm in [[123](#bib.bib123)] (see Fig. [12](#S5.F12
    "Figure 12 ‣ V-A2 Cyber-Physical Attack ‣ V-A Network Security ‣ V Network Security
    and Connectivity Preservation ‣ Applications of Deep Reinforcement Learning in
    Communications and Networking: A Survey")). The DQL then uses an LSTM unit to
    find the optimal policy. The input of the LSTM unit is the state of the cloud,
    and the output includes probabilities of attacking the IoT devices. By using a
    real dataset from the accelerometers, the simulation results show that the proposed
    DQL can improve the cloud’s utility up to 30% compared with the case in which
    the cloud chooses the IoT devices with equal probability.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[[123](#bib.bib123)]中提出的算法也称为动态水印技术[[124](#bib.bib124)]，能够检测网络物理攻击并防止窃听攻击。然而，该算法在云端进行物联网设备信号认证时需要大量计算资源。因此，云端只能认证有限数量的易受攻击的物联网设备。云端可以通过观察其安全状态来选择易受攻击的物联网设备。然而，由于物联网设备可能不会报告其安全状态，这种方法可能不切实际。因此，[[125](#bib.bib125)]中的作者提出使用DQL，使云端能够在信息不完整的情况下决定要认证哪些物联网设备。由于数据更有价值的物联网设备更可能遭到攻击，因此奖励被定义为物联网设备数据值的函数。云端的状态包括攻击者在过去时间段内对物联网设备的攻击行为。攻击者对物联网设备的行为可以通过使用[[123](#bib.bib123)]中的动态水印算法获得（见图[12](#S5.F12
    "图 12 ‣ V-A2 网络物理攻击 ‣ V-A 网络安全 ‣ V 网络安全与连通性保护 ‣ 深度强化学习在通信和网络中的应用：综述")）。然后，DQL使用LSTM单元来寻找最佳策略。LSTM单元的输入是云端的状态，输出包括攻击物联网设备的概率。通过使用来自加速度计的真实数据集，仿真结果表明，与云端以相等概率选择物联网设备的情况相比，提出的DQL可以将云端的效用提高多达30%。'
- en: V-B Connectivity Preservation
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 连通性保护
- en: 'Multi-robot systems such as multi-UAV cooperative networks have been widely
    applied in many fields such as military, e.g., enemy detecting. In the cooperative
    multi-robot system, the connectivity among the robots, e.g., UAVs in Fig [13](#S5.F13
    "Figure 13 ‣ V-B Connectivity Preservation ‣ V Network Security and Connectivity
    Preservation ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey"), is required to enable the communication and exchange of
    information. To tackle the connectivity preservation problem, the Artificial Potential
    Field (APF) algorithm [[126](#bib.bib126)] has been used. However, the algorithm
    cannot be directly adopted when the robots are undertaking missions in dynamic
    and complex environments. The DQL which allows each robot to make dynamic decisions
    based on its own state can be effectively applied to preserve the connectivity
    in the multi-robot system. Such an approach is proposed in [[127](#bib.bib127)].'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 多机器人系统，如多无人机协作网络，已广泛应用于许多领域，如军事领域（例如敌人检测）。在协作的多机器人系统中，机器人之间的连通性（例如图[13](#S5.F13
    "图 13 ‣ V-B 连通性保护 ‣ V 网络安全与连通性保护 ‣ 深度强化学习在通信和网络中的应用：综述")中的无人机）是实现通信和信息交换所必需的。为了解决连通性保护问题，已使用人工势场（APF）算法[[126](#bib.bib126)]。然而，当机器人在动态和复杂的环境中执行任务时，该算法无法直接采用。DQL允许每个机器人根据自身状态做出动态决策，这可以有效地应用于多机器人系统中的连通性保护。这种方法在[[127](#bib.bib127)]中被提出。
- en: '![Refer to caption](img/6b52aa2f0a7b0e02e5361aabe3b8a9fe.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6b52aa2f0a7b0e02e5361aabe3b8a9fe.png)'
- en: 'Figure 13: Connectivity preservation of a multi-UAV network.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：多无人机网络的连通性保护。
- en: 'The model in [[127](#bib.bib127)] consists of two robots or UAVs, i.e., one
    leader robot and one follower robot. In the model, a central control, i.e., a
    ground BS, adjusts the velocity of the follower such that the follower stays in
    the communication range of the leader at all time (see Fig [13](#S5.F13 "Figure
    13 ‣ V-B Connectivity Preservation ‣ V Network Security and Connectivity Preservation
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")). The connectivity preservation problem can be thus formulated as an
    MDP. The agent is the BS, and the states are the relative position and the velocity
    of the leader with respect to the follower. The action space consists of possible
    velocity values of the follower. Taking an action returns a reward which is +1
    if the follower is in the range of the leader, and -1 otherwise. A DQN using FNN
    is used which enables the BS to learn an optimal policy to maximize the expected
    discounted cumulative reward. The input of the DQN includes the states of the
    two robots, and the output is the action space of the follower. The simulation
    results show that the proposed scheme can achieve better connectivity between
    the two robots than that of the APF method. However, a general scenario with more
    than one leader and one follower needs to be investigated.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '该模型在[[127](#bib.bib127)]中由两个机器人或无人机组成，即一个领导机器人和一个跟随机器人。在模型中，一个中央控制，即一个地面基站（BS），调整跟随机器人的速度，以确保跟随机器人始终处于领导机器人的通信范围内（见图[13](#S5.F13
    "Figure 13 ‣ V-B Connectivity Preservation ‣ V Network Security and Connectivity
    Preservation ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey")）。因此，连通性保护问题可以被表述为一个MDP。代理是BS，状态是领导者相对于跟随者的位置和速度。动作空间包括跟随者可能的速度值。采取一个动作会返回一个奖励，如果跟随者在领导者的范围内，则奖励为+1，否则为-1。采用了使用FNN的DQN，这使得BS能够学习一个最佳策略，以最大化期望的折扣累积奖励。DQN的输入包括两个机器人的状态，输出是跟随者的动作空间。仿真结果表明，提出的方案比APF方法能够实现更好的机器人间的连通性。然而，仍需研究具有多个领导者和多个跟随者的一般场景。'
- en: Considering the general scenario, the authors in [[128](#bib.bib128)] address
    the connectivity preservation between multiple leaders and multiple followers.
    The robot system is definitely connected if any two robots are connected via a
    direct link or multi-hop link. To express the connectivity in such a robot system,
    the authors introduce the concept of algebraic connectivity[[129](#bib.bib129)]
    which is the second smallest eigenvalue of a Laplacian matrix. The robot system
    is connected if the algebraic connectivity of the system is positive. Thus, the
    problem is to adjust the velocity of the followers such that the algebraic connectivity
    is positive over time slots. This problem can be formulated as an MDP in which
    the agent is the ground BS, the state is a combination of the states of all robots,
    the action is a set of possible velocity values for the followers. The reward
    is +1 if the algebraic connectivity of the system increases or holds, and becomes
    a penalty of -1 if the algebraic connectivity decreases. Similar to [[127](#bib.bib127)],
    a DQN is adopted. Due to the large action space of the followers, the actor-critic
    neural network [[26](#bib.bib26)] is used. The simulation results show that the
    followers always follow the motion of the leaders even if the leaders’ trajectory
    dynamically changes. However, the proposed DQN requires more time to converge
    than that in [[127](#bib.bib127)] because of the presence of more followers.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到一般场景，作者在[[128](#bib.bib128)]中解决了多个领导者和多个跟随者之间的连通性保护问题。机器人系统如果任何两个机器人通过直接链接或多跳链接相连，则系统被认为是连接的。为了表示这种机器人系统中的连通性，作者引入了代数连通性[[129](#bib.bib129)]的概念，它是拉普拉斯矩阵的第二小特征值。系统如果代数连通性为正，则系统是连接的。因此，问题在于调整跟随者的速度，以使代数连通性在时间段内为正。这个问题可以被表述为一个MDP，其中代理是地面BS，状态是所有机器人的状态的组合，动作是跟随者可能的速度值集合。奖励是+1如果系统的代数连通性增加或保持，如果代数连通性减少，则惩罚为-1。与[[127](#bib.bib127)]类似，采用了DQN。由于跟随者的动作空间较大，因此使用了演员-评论家神经网络[[26](#bib.bib26)]。仿真结果显示，即使领导者的轨迹动态变化，跟随者也始终跟随领导者的运动。然而，由于跟随者数量更多，提出的DQN需要比[[127](#bib.bib127)]更多的时间才能收敛。
- en: The proposed schemes in [[127](#bib.bib127)] and [[128](#bib.bib128)] do not
    consider a minimum distance between the leaders and followers. The leaders and
    followers can collide with each other if the distance between them is too short.
    Thus, the BS needs to guarantee the minimum distance between them. One solution
    is to have the minimum distance in the reward as proposed in [[130](#bib.bib130)].
    In particular, if the leader is too close to its follower, the reward of the system
    is penalized regarding the minimum distance. The DQL algorithm proposed in [[128](#bib.bib128)]
    is then used such that the BS learns proper actions, e.g., turning left and right,
    to maximize the cumulative reward.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 文献[[127](#bib.bib127)]和[[128](#bib.bib128)]中的方案未考虑领队与跟随者之间的最小距离。如果它们之间的距离过短，领队和跟随者可能会发生碰撞。因此，基站需要保证它们之间的最小距离。一种解决方案是将最小距离纳入奖励中，如文献[[130](#bib.bib130)]中所提议的那样。特别是，如果领队离其跟随者过近，系统的奖励会因最小距离而受到惩罚。然后使用文献[[128](#bib.bib128)]中提出的DQL算法，以便基站学习适当的动作，例如左转和右转，以最大化累计奖励。
- en: When BSs are densely deployed, the UAVs or mobile users need to trigger a frequent
    handover to preserve the connectivity. The frequent handover increases communication
    overhead and energy consumption of the mobile users, and interrupts data flows.
    Thus, it is essential to maintain an appropriate handover rate. The authors in [[27](#bib.bib27)]
    address the handover decision problem in an ultra-density network. The network
    model consists of multiple mobile users, SBSs, and one central controller. At
    each time slot, the user needs to decide its serving SBS. The handover decision
    process can be modeled as an MDP, and the DQL is adopted to find an optimal handover
    policy for each user to minimize the number of handover occurrences while ensuring
    certain throughput. The state of the user, i.e., the agent, includes reference
    signal quality received from candidate SBSs and the last action of the user. The
    reward is defined as the difference between the data rate of the user and its
    energy consumption for the handover process. Given a high density of users, the
    DQL using A3C and LSTM is adopted to find the optimal policy in short training
    time. The simulation results show that the proposed DQL can achieve better throughput
    and lower handover rate than those of the upper confidence bandit algorithm [[131](#bib.bib131)]
    with similar training time.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 当基站（BS）密集部署时，无人机（UAV）或移动用户需要频繁触发切换以保持连接。频繁的切换增加了通信开销和移动用户的能耗，并中断了数据流。因此，维持适当的切换率至关重要。文献[[27](#bib.bib27)]中的作者讨论了超密度网络中的切换决策问题。网络模型包括多个移动用户、微型基站（SBS）和一个中央控制器。在每个时间槽中，用户需要决定其服务的SBS。切换决策过程可以建模为一个MDP（马尔可夫决策过程），并采用DQL（深度Q学习）来为每个用户找到一个最优的切换策略，以最小化切换发生的次数，同时确保一定的吞吐量。用户的状态，即代理，包括从候选SBS接收到的参考信号质量以及用户的上一个动作。奖励定义为用户的数据速率与其切换过程能耗之间的差值。鉴于用户的高密度，使用A3C（异步优势
    Actor-Critic）和LSTM（长短期记忆网络）的DQL被采用，以在短时间内找到最优策略。仿真结果显示，所提出的DQL在相似的训练时间内能够实现比上置信带算法[[131](#bib.bib131)]更好的吞吐量和更低的切换率。
- en: To enhance the reliability of the communication between the SBSs and the mobile
    users, the SBSs should be able to handle network faults and failure automatically
    as self-healing. The DQL can be applied as proposed in [[132](#bib.bib132)] to
    make optimal parameter adjustments based on the observation of the network performance.
    The model is the 5G network including one MBS. The MBS as an agent needs to handle
    network faults such as transmit diversity faults and antenna azimuth change, e.g.,
    because of wind. These faults are represented as the MBS’s state that is the number
    of active alarms. Based on the alarms, the MBS can take actions including (i)
    enabling the transmit diversity and (ii) setting the antenna azimuth to default
    value. The reward that the MBS receives is the scores, e.g., -1, 0, and +1, depending
    on the number of faults happening. The DQL is used to learn the optimal policy.
    The simulation results show that the proposed DQL can achieve network throughput
    close to that of the oracle-based self-healing, i.e., the upper-performance bound,
    but incurs less fault message passing overhead.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强 SBS 与移动用户之间通信的可靠性，SBS 应能够自动处理网络故障和故障，作为自愈功能。可以根据 [[132](#bib.bib132)] 提出的方案应用
    DQL 来基于对网络性能的观察进行最佳参数调整。模型是包括一个 MBS 的 5G 网络。作为代理的 MBS 需要处理网络故障，例如传输多样性故障和天线方位角变化，例如由于风引起的。这些故障被表示为
    MBS 的状态，即活动警报的数量。根据警报，MBS 可以采取的行动包括（i）启用传输多样性和（ii）将天线方位角设置为默认值。MBS 接收的奖励是分数，例如
    -1、0 和 +1，具体取决于发生的故障数量。DQL 被用来学习最佳策略。仿真结果显示，提出的 DQL 可以实现接近于基于预言机的自愈网络吞吐量，即上限性能，但故障消息传递开销更少。
- en: 'TABLE V: A summary of approaches using DQL for network security and connectivity
    preservation.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：使用 DQL 进行网络安全和连接保持的方法总结。
- en: '| Issues | Ref. | Model | Learning algorithms | Agent | States | Actions |
    Rewards | Networks |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: 问题 | 参考文献 | 模型 | 学习算法 | 代理 | 状态 | 行动 | 奖励 | 网络 |
- en: '| Network security | [[111](#bib.bib111)] | Game | DQN using CNN | Secondary
    user | Number of PUs and signal SINR | Channel selection and leaving decision
    | SINR and mobility cost | CRN |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 网络安全 | [[111](#bib.bib111)] | 游戏 | 使用 CNN 的 DQN | 次级用户 | PU 的数量和信号 SINR |
    通道选择和离开决策 | SINR 和移动成本 | CRN |'
- en: '|  | [[112](#bib.bib112)] | Game | DQN using CNN | Receiving transducer | Signal
    SINR | Staying and leaving decisions | SINR and mobility cost | Underwater acoustic
    network |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|  | [[112](#bib.bib112)] | 游戏 | 使用 CNN 的 DQN | 接收传感器 | 信号 SINR | 停留和离开决策 |
    SINR 和移动成本 | 水下声学网络 |'
- en: '|  | [[113](#bib.bib113)] | MDP | DQN using RCNN | SU | Signal SINR | Channel
    selection | SINR and mobility cost | CRN |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | [[113](#bib.bib113)] | MDP | 使用 RCNN 的 DQN | SU | 信号 SINR | 通道选择 | SINR
    和移动成本 | CRN |'
- en: '|  | [[114](#bib.bib114)] | MDP | DQN using CNN | Transmit IoT device | Signal
    SINR | Channel selection | SINR and energy consumption cost | IoT |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  | [[114](#bib.bib114)] | MDP | 使用 CNN 的 DQN | 发射 IoT 设备 | 信号 SINR | 通道选择
    | SINR 和能量消耗成本 | IoT |'
- en: '|  | [[115](#bib.bib115)] | MDP | DQN using CNN | Relay UAV | Signal SINR and
    BER | Relay power | SINR and relay cost | UAV |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | [[115](#bib.bib115)] | MDP | 使用 CNN 的 DQN | 中继 UAV | 信号 SINR 和 BER | 中继功率
    | SINR 和中继成本 | UAV |'
- en: '|  | [[118](#bib.bib118)] | MDP | DQN using CNN | Transmit UAV | Jamming power
    | Transmit power | Secrecy capacity and energy consumption cost | UAV |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | [[118](#bib.bib118)] | MDP | 使用 CNN 的 DQN | 发射 UAV | 干扰功率 | 发射功率 | 保密能力和能量消耗成本
    | UAV |'
- en: '|  | [[121](#bib.bib121)] | Game | DQN using LSTM units | Autonomous vehicle
    | Deviation values | Measurement weight selection | Safe spacing deviation | ITS
    |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  | [[121](#bib.bib121)] | 游戏 | 使用 LSTM 单元的 DQN | 自动驾驶车辆 | 偏差值 | 测量权重选择 |
    安全间距偏差 | ITS |'
- en: '|  | [[125](#bib.bib125)] | Game | DQN using LSTM units | Cloud | Attack actions
    on IoT devices | IoT device set selection | IoT devices’ data values | IoT |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | [[125](#bib.bib125)] | 游戏 | 使用 LSTM 单元的 DQN | 云 | 对 IoT 设备的攻击行为 | IoT
    设备集合选择 | IoT 设备的数据值 | IoT |'
- en: '| Connectivity preservation | [[127](#bib.bib127)] | MDP | DQN using FNN |
    Ground base station | Relative positions and the velocity of robots | Velocity
    decision | Sore +1 and -1 | Robot system |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: 连接保持 | [[127](#bib.bib127)] | MDP | 使用 FNN 的 DQN | 地面基站 | 机器人相对位置和速度 | 速度决策
    | 分数 +1 和 -1 | 机器人系统 |
- en: '|  | [[128](#bib.bib128)] | MDP | DQN using A3C | Ground base station | Relative
    positions and the velocity of robots | Velocity decision | Sore +1 and -1 | Robot
    system |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  | [[128](#bib.bib128)] | MDP | 使用 A3C 的 DQN | 地面基站 | 机器人相对位置和速度 | 速度决策 |
    分数 +1 和 -1 | 机器人系统 |'
- en: '|  | [[130](#bib.bib130)] | POMDP | DQN using A3C | Ground base station | Information
    of distances among robots | Turning left and turning right decisions | Sore +1
    and -1 | Robot system |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | [[130](#bib.bib130)] | POMDP | 使用 A3C 的 DQN | 地面基站 | 机器人之间的距离信息 | 左转和右转决策
    | 分数 +1 和 -1 | 机器人系统 |'
- en: '|  | [[27](#bib.bib27)] | MDP | DQN using A3C and LSTM | Mobile users | Reference
    signal received quality and the last action | Serving SBS selection | Data rate
    and energy consumption | Ultra-dense network |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  | [[27](#bib.bib27)] | MDP | 使用 A3C 和 LSTM 的 DQN | 移动用户 | 参考信号接收质量和最后的动作
    | 服务 SBS 选择 | 数据速率和能耗 | 超密集网络 |'
- en: '|  | [[132](#bib.bib132)] | MDP | DQN using CNN | MBS | The number of active
    alarms | Enabling transmit diversity and changing antenna azimuth | Score -1,
    0, +1, and +5 | Self-organization network |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  | [[132](#bib.bib132)] | MDP | 使用 CNN 的 DQN | MBS | 活跃警报数量 | 启用传输多样性和改变天线方位角
    | 分数 -1、0、+1 和 +5 | 自组织网络 |'
- en: 'Summary: This section reviews applications of DQL for the network security
    and connectivity preservation. The reviewed approaches are summarized along with
    the references in Table [V](#S5.T5 "TABLE V ‣ V-B Connectivity Preservation ‣
    V Network Security and Connectivity Preservation ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey"). We observe that the CNN
    is mostly used for the DQL to enhance the network security. Moreover, DQL approaches
    for the anonymous system such as robot systems and ITS receive more attentions
    than other networks. However, the applications of DQL for the cyber-physical security
    are relatively few and need to be investigated.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '总结：本节回顾了深度 Q 学习（DQL）在网络安全和连接性保护方面的应用。所回顾的方法及其参考文献在表[V](#S5.T5 "TABLE V ‣ V-B
    Connectivity Preservation ‣ V Network Security and Connectivity Preservation ‣
    Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")中进行了总结。我们观察到 CNN 在 DQL 中主要用于增强网络安全。此外，用于匿名系统的 DQL 方法，如机器人系统和智能交通系统（ITS），比其他网络受到更多关注。然而，DQL
    在网络物理安全方面的应用相对较少，需要进一步研究。'
- en: VI Miscellaneous Issues
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 杂项问题
- en: This section reviews applications of DRL to solve some other issues in communications
    and networking. The issues include (i) traffic engineering and routing, (ii) resource
    sharing and scheduling, and (iii) data collection.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了深度强化学习（DRL）在解决通信和网络中的其他问题的应用。这些问题包括 (i) 流量工程和路由，(ii) 资源共享和调度，以及 (iii) 数据收集。
- en: VI-A Traffic Engineering and Routing
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 流量工程和路由
- en: Traffic Engineering (TE) in communication networks refers to Network Utility
    Maximization (NUM) by optimizing a path to forward the data traffic, given a set
    of network flows from source to destination nodes. Traditional NUM problems are
    mostly model-based. However, with the advances of wireless communication technologies,
    the network environment becomes more complicated and dynamic, which makes it hard
    to model, predict, and control. The recent development of DQL methods provides
    a feasible and efficient way to design experience-driven and model-free schemes
    that can learn and adapt to the dynamic wireless network from past observations.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 通信网络中的流量工程（TE）是通过优化路径以转发数据流量来实现网络效用最大化（NUM），前提是给定从源节点到目标节点的一组网络流。传统的 NUM 问题大多基于模型。然而，随着无线通信技术的进步，网络环境变得更加复杂和动态，这使得建模、预测和控制变得困难。近期
    DQL 方法的发展提供了一种可行且高效的方式来设计以经验驱动和无模型的方案，这些方案可以通过过去的观察学习和适应动态无线网络。
- en: Routing optimization is one of the major control problems in traffic engineering.
    The authors in [[133](#bib.bib133)] present the first attempt to use the DQL for
    the routing optimization. Through the interaction with the network environment,
    the DQL agent at the network controller determines the paths for all source-destination
    pairs. The system state is represented by the bandwidth request between each source-destination
    pair, and the reward is a function of the mean network delay. The DQL agent leverages
    the actor-critic method for solving the routing problem that minimizes the network
    delay, by adapting routing configurations automatically to current traffic conditions.
    The DQL agent is trained using the traffic information generated by a gravity
    model [[134](#bib.bib134)]. The routing solution is then evaluated by OMNet+ discrete
    event simulator [[135](#bib.bib135)]. The well-trained DQL agent can produce a
    near-optimal routing configuration in a single step and thus the agent is agile
    for real-time network control. The proposed approach is attractive as the traditional
    optimization-based techniques require a large number of steps to produce a new
    configuration. The authors in [[136](#bib.bib136)] consider a similar network
    model with multiple end-to-end communication sessions. Each source-destination
    pair has a set of candidate paths that can transport the traffic load. Experimental
    results show that the conventional DDPG method does not work well for the continuous
    control problem in [[136](#bib.bib136)]. One possible explanation is that DDPG
    utilizes uniform sampling for experience replay, which ignores different significance
    of the transition samples.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 路由优化是交通工程中的一个主要控制问题。文献[[133](#bib.bib133)]中的作者首次尝试使用DQL进行路由优化。通过与网络环境的互动，网络控制器中的DQL代理确定所有源-目的地对的路径。系统状态由每个源-目的地对之间的带宽请求表示，奖励则是网络平均延迟的函数。DQL代理利用演员-评论家方法解决最小化网络延迟的路由问题，通过自动调整路由配置以适应当前流量条件。DQL代理使用由引力模型生成的流量信息进行训练[[134](#bib.bib134)]。然后通过OMNet+离散事件模拟器[[135](#bib.bib135)]对路由解决方案进行评估。经过良好训练的DQL代理可以在单一步骤中生成接近最优的路由配置，因此该代理在实时网络控制中非常灵活。该方法具有吸引力，因为传统的基于优化的技术需要大量步骤才能生成新的配置。文献[[136](#bib.bib136)]中的作者考虑了一个具有多个端到端通信会话的类似网络模型。每个源-目的地对都有一组候选路径可以传输流量负载。实验结果表明，传统的DDPG方法在[[136](#bib.bib136)]中的连续控制问题上表现不佳。一个可能的解释是，DDPG利用统一采样进行经验回放，这忽略了转移样本的不同重要性。
- en: The authors in [[136](#bib.bib136)] also combine two new techniques to optimize
    DDPG particularly for traffic engineering problems, i.e., TE-aware exploration
    and actor-critic-based PER methods. The TE-aware exploration leverages the shortest
    path algorithm and NUM-based solution as the baseline during exploration. The
    PER method is conventionally used in DQL, e.g., [[79](#bib.bib79)] and [[109](#bib.bib109)],
    while the authors in [[136](#bib.bib136)] integrate the PER method with the actor-critic
    framework for the first time. The proposed scheme assigns different priorities
    to transitions in the experience replay. Based on the priority, the proposed scheme
    samples the transitions in each epoch. The system state consists of throughput
    and delay performance of each communication session. The action specifies the
    amount of traffic load going through each of the paths. By learning the dynamics
    of network environment, the DQL agent aims to maximize the total utility of all
    the communication sessions, which is defined based on end-to-end throughput and
    delay [[137](#bib.bib137)]. Packet-level simulations using NS-3 [[138](#bib.bib138)],
    tested on well-known network topologies as well as random topologies generated
    by BRITE [[139](#bib.bib139)], reveal that the proposed DQL scheme significantly
    reduces the end-to-end delay and improves the network utility, compared with the
    baseline schemes including DDPG and the NUM-based solutions.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 文献[[136](#bib.bib136)]中的作者还结合了两种新技术来特别优化DDPG用于流量工程问题，即TE感知探索和基于演员-评论员的PER方法。TE感知探索利用最短路径算法和基于NUM的解决方案作为探索过程中的基线。PER方法通常用于DQL中，例如[[79](#bib.bib79)]和[[109](#bib.bib109)]，而文献[[136](#bib.bib136)]中的作者首次将PER方法与演员-评论员框架结合。所提出的方案为经验回放中的转换分配了不同的优先级。根据优先级，提出的方案在每个时期采样转换。系统状态包括每个通信会话的吞吐量和延迟性能。行动指定了经过每条路径的流量负载。通过学习网络环境的动态，DQL代理旨在最大化所有通信会话的总效用，该效用基于端到端吞吐量和延迟[[137](#bib.bib137)]。使用NS-3进行的数据包级模拟[[138](#bib.bib138)]，在著名的网络拓扑以及由BRITE[[139](#bib.bib139)]生成的随机拓扑上进行测试，结果表明，所提出的DQL方案显著减少了端到端延迟，并改善了网络效用，相比于包括DDPG和基于NUM的解决方案在内的基线方案。
- en: 'The networking and routing optimization become more complicated in the UAV-based
    wireless communications. The authors in [[130](#bib.bib130)] model autonomous
    navigation of one single UAV in a large-scale unknown complex environment as a
    POMDP, which can be solved by actor-critic-based DRL method. The system state
    includes its distances and orientation angles to nearby obstacles, the distance
    and angle between its present position and the destination. The UAV’s action is
    to turn left or right or keep ahead. The reward is composed of four parts: an
    exponential penalty term if it is too close to any obstacles, a linear penalty
    term to encourage minimum time delay, the transition and direction rewards if
    the UAV is getting close to the target position in a proper direction. Instead
    of using conventional DDPG for continuous control, the Recurrent Deterministic
    Policy Gradient (RDPG) is proposed for the POMDP by approximating the actor and
    critic using RNNs. Considering that RDPG is not suitable for learning using memory
    replay, the authors in [[130](#bib.bib130)] propose the fast-RDPG method by utilizing
    the actor-critic framework with function approximation [[140](#bib.bib140)]. The
    proposed method derives policy update for POMDP by directly maximizing the expected
    long-term accumulated discounted reward.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在无人机无线通信中，网络和路由优化变得更加复杂。文献[[130](#bib.bib130)]中的作者将单个无人机在大规模未知复杂环境中的自主导航建模为POMDP，这可以通过基于演员-评论员的DRL方法来解决。系统状态包括无人机与附近障碍物的距离和方向角、当前位置与目的地之间的距离和角度。无人机的动作是向左转、向右转或保持前进。奖励由四部分组成：如果无人机距离任何障碍物过近，则有一个指数惩罚项；鼓励最小时间延迟的线性惩罚项；如果无人机在正确方向上接近目标位置，则有过渡和方向奖励。与传统的DDPG方法相比，提出了使用RNN来近似演员和评论员的递归确定性策略梯度（RDPG）方法。考虑到RDPG不适合使用记忆回放进行学习，文献[[130](#bib.bib130)]中的作者通过利用函数逼近的演员-评论员框架提出了fast-RDPG方法。该方法通过直接最大化预期的长期累积折扣奖励来推导POMDP的策略更新。
- en: Path planning for multiple UAVs connected via cellular systems is studied in [[141](#bib.bib141)]
    and [[142](#bib.bib142)]. Each UAV aims to achieve a tradeoff between maximizing
    energy efficiency and minimizing both latency and interference caused to the ground
    network along its path. The network state observable by each UAV includes its
    distances and orientation angles to cellular BSs, the orientation angle to its
    destination, and the horizontal coordinates of all UAVs. The action of each UAV
    includes an optimal path, transmit power, and cell association along its path.
    The interaction among UAVs is cast as a dynamic game and solved by a multi-agent
    DRL framework. The use of ESN in the DRL framework allows each UAV to retain previous
    memory states and make a decision for unseen network states, based on the reward
    obtained from previous states. ESN is a new type of RNNs with feedback connections,
    consisting of the input, recurrent, and output weight matrices. ESN training is
    typically quick and computationally efficient compared with other RNNs. Deep ESNs
    can exploit the advantages of a hierarchical temporal feature representation at
    different levels of abstraction, hence disentangling the difficulties in modeling
    complex tasks. Simulation results show that the proposed scheme improves the tradeoff
    between energy efficiency, wireless latency, and the interference caused to the
    ground network. Results also show that each UAV’s altitude is a function of the
    ground network density and the UAV’s objective function is an important factor
    in achieving the UAV’s target.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[141](#bib.bib141)]和[[142](#bib.bib142)]中研究了通过蜂窝系统连接的多个无人机的路径规划。每个无人机的目标是实现最大化能源效率与最小化沿路径对地面网络造成的延迟和干扰之间的折衷。每个无人机可观测的网络状态包括其与蜂窝基站的距离和方位角、到目的地的方位角，以及所有无人机的水平坐标。每个无人机的行动包括最优路径、发射功率和路径上的小区关联。无人机之间的互动被看作是一个动态博弈，并通过多智能体DRL框架解决。DRL框架中使用ESN使得每个无人机能够保留之前的记忆状态，并根据从之前状态获得的奖励，对未见过的网络状态做出决策。ESN是一种具有反馈连接的新型RNN，包括输入、递归和输出权重矩阵。与其他RNN相比，ESN训练通常较快且计算效率高。深度ESN可以利用不同抽象层次的层次化时间特征表示的优势，从而解开建模复杂任务的难题。仿真结果显示，所提出的方案改善了能源效率、无线延迟与对地面网络造成的干扰之间的折衷。结果还显示，每个无人机的高度是地面网络密度的函数，而无人机的目标函数在实现无人机目标中是一个重要因素。
- en: Besides networked UAVs, vehicle-to-infrastructure also constitutes an important
    part and provides rich application implications in 5G ecosystem. The authors in [[143](#bib.bib143)]
    adopt the DQL to achieve an optimal control policy in communication-based train
    control system, which is supported by bidirectional train-ground communications.
    The control problem aims to optimize the handoff decision and train control policy,
    i.e., accelerate or decelerate, based on the states of stochastic channel conditions
    and real-time information including train position, speed, measured SNR from APs,
    and handoff indicator. The objective of the DQL agent is to minimize a weighted
    combination of operation profile tracking error and energy consumption.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 除了网络化的无人机外，车对基础设施通信也构成了5G生态系统中的重要部分，并提供了丰富的应用意义。在[[143](#bib.bib143)]中，作者采用DQL在基于通信的列车控制系统中实现了最优控制策略，该系统支持双向列车与地面的通信。控制问题旨在基于随机信道条件和实时信息（包括列车位置、速度、从AP测量的信噪比以及切换指示）优化切换决策和列车控制策略，即加速或减速。DQL智能体的目标是最小化操作配置跟踪误差和能耗的加权组合。
- en: VI-B Resource Sharing and Scheduling
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 资源共享与调度
- en: System capacity is one of the most important performance metrics in wireless
    communication networks. System capacity enhancements can be based on the optimization
    of resource sharing and scheduling among multiple wireless nodes. The integration
    of DRL into 5G systems would revolutionize the resource sharing and scheduling
    schemes from model-based to model-free approaches and meet various application
    demands by learning from the network environment.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 系统容量是无线通信网络中最重要的性能指标之一。系统容量的提升可以基于多个无线节点之间资源共享和调度的优化。将DRL集成到5G系统中将使资源共享和调度方案从基于模型的方法转变为无模型的方法，并通过从网络环境中学习来满足各种应用需求。
- en: The authors in [[144](#bib.bib144)] study the user scheduling in a multi-user
    massive MIMO system. User scheduling is responsible for allocating resource blocks
    to BSs and mobile users, taking into account the channel conditions and QoS requirements.
    Based on this user scheduling strategy, a DRL-based coverage and capacity optimization
    is proposed to obtain dynamically the scheduling parameters and a unified threshold
    of QoS metric. The performance indicators are calculated as the average spectrum
    efficiency of all the users. The system state is an indicator of the average spectrum
    efficiency. The action of the scheduler is a set of scheduling parameters to maximize
    the reward as a function of the average spectrum efficiency. The DRL scheme uses
    policy gradient method to learn a policy function (instead of a Q-function) directly
    from trajectories generated by the current policy. The policy network is trained
    with a variant of the REINFORCE algorithm [[140](#bib.bib140)]. The simulation
    results in [[144](#bib.bib144)] show that compared with the optimization-based
    algorithms that suffer from incomplete network information, the policy gradient
    method achieves much better performance in terms of network coverage and capacity.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们在[[144](#bib.bib144)]研究了多用户大规模MIMO系统中的用户调度。用户调度负责将资源块分配给基站和移动用户，考虑到信道条件和QoS要求。基于这种用户调度策略，提出了一种基于DRL的覆盖和容量优化方法，以动态获取调度参数和统一的QoS度量阈值。性能指标计算为所有用户的平均频谱效率。系统状态是平均频谱效率的指标。调度器的动作是一组调度参数，旨在最大化作为平均频谱效率函数的奖励。DRL方案使用策略梯度方法直接从当前策略生成的轨迹中学习策略函数（而非Q函数）。策略网络使用REINFORCE算法的变体进行训练[[140](#bib.bib140)]。[[144](#bib.bib144)]中的仿真结果表明，与因网络信息不完整而表现不佳的优化算法相比，策略梯度方法在网络覆盖和容量方面取得了更好的性能。
- en: In [[145](#bib.bib145)], the authors focus on dynamic resource allocation in
    a cloud radio access network and present a DQL-based framework to minimize the
    total power consumption while fulfilling mobile users’ QoS requirements. The system
    model contains multiple Remote Radio Heads (RRHs) connected to a cloud BaseBand
    Unit (BBU). The information of RRHs can be shared in a centralized manner. The
    system state contains information about the mobile users’ demands and the RRHs’
    working states, e.g., active or sleep. According to the system state and the result
    of last execution, the DQL agent at the BBU decides whether to turn on or off
    certain RRH(s), and how to allocate beamforming weight for each active RRH. The
    objective is to minimize the total expected power consumption. The authors propose
    a two-step decision framework to reduce the size of action space. In the first
    step, the DQL agent determines the set of active RRHs by Q-learning and DNNs.
    In the second step, the BBU derives the optimal resource allocation for the active
    RRHs by solving a convex optimization problem. Through the combination of DQL
    and optimization techniques, the proposed framework results in a relatively small
    action space and low online computational complexity. Simulation results show
    that the framework achieves significant power savings while satisfying user demands
    and is robust in highly dynamic network environment. The aforementioned works
    mostly focus on simulations and numerical comparisons. With one step further,
    the authors in [[146](#bib.bib146)] implement a multi-objective DQL framework
    as the radio-resource-allocation controller for space communications. The implementation
    uses modular software architecture to encourage re-use and easy modification for
    different algorithms, which is integrated into the real space-ground system developed
    by NASA Glenn Research Center.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[145](#bib.bib145)]中，作者关注于云无线接入网络中的动态资源分配，并提出了一种基于DQL的框架，以在满足移动用户的QoS要求的同时最小化总功耗。系统模型包含多个远程无线电头（RRHs），这些RRHs连接到一个云基带单元（BBU）。RRHs的信息可以以集中方式共享。系统状态包含有关移动用户需求和RRHs工作状态的信息，例如，活动或休眠。根据系统状态和上次执行的结果，BBU中的DQL代理决定是否开启或关闭某些RRH，以及如何为每个活动RRH分配波束赋形权重。目标是最小化总预期功耗。作者提出了一个两步决策框架，以减少动作空间的规模。在第一步中，DQL代理通过Q学习和DNN确定活动RRHs的集合。在第二步中，BBU通过解决一个凸优化问题来得出活动RRHs的最佳资源分配。通过DQL和优化技术的结合，所提出的框架实现了相对较小的动作空间和较低的在线计算复杂度。仿真结果表明，该框架在满足用户需求的同时实现了显著的功耗节省，并且在高度动态的网络环境中具有鲁棒性。上述工作主要集中在仿真和数值比较上。进一步发展中，作者在[[146](#bib.bib146)]中实现了一个多目标DQL框架作为空间通信的无线资源分配控制器。该实现使用了模块化的软件架构，以促进不同算法的重用和易于修改，并集成到由NASA格伦研究中心开发的实际空间-地面系统中。
- en: In emerging and future wireless networks, BSs are deployed with a high density,
    and thus the interference among the BSs must be considered. The authors in [[147](#bib.bib147)]
    propose to use a DQL scheme which allows the BSs to learn their optimal power
    control policy. In the proposed scheme, each BS is an agent, the action is choosing
    power levels, and the state includes interference that the BS caused to its neighbors
    in the last time slot. The objective is to maximize the BS’s data rate. The DQN
    using FNN is then adopted to implement the DQL algorithm. For the future work,
    a joint power control and channel selection can be considered.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在新兴和未来的无线网络中，基站（BSs）的部署密度很高，因此必须考虑基站之间的干扰。[[147](#bib.bib147)]中的作者提出使用DQL方案，使基站能够学习其最佳功率控制策略。在该方案中，每个基站是一个代理，动作是选择功率级别，状态包括基站在上一个时间槽中对其邻居造成的干扰。目标是最大化基站的数据速率。然后采用使用FNN的DQN来实现DQL算法。未来的工作可以考虑联合功率控制和信道选择。
- en: Network slicing [[148](#bib.bib148)] and NFV [[90](#bib.bib90)] are two emerging
    concepts for resource allocation in the 5G ecosystem to provide cost-effective
    services with better performance. The network infrastructure, e.g., cache, computation,
    and radio resources, is comparatively static while the upper-layer Virtualized
    Network Functions (VNFs) are dynamic to support time-varying application-specific
    service requests. The concept of network slicing is to divide the network resources
    into multi-layer slices, managed by different service renderers independently
    with minimal conflicts. The concept of Service Function Chaining (SFC) is to orchestrate
    different VNFs to provide required functionalities and QoS provisioning.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 网络切片 [[148](#bib.bib148)] 和 NFV [[90](#bib.bib90)] 是在 5G 生态系统中用于资源分配的两个新兴概念，旨在提供成本效益高且性能更好的服务。网络基础设施，例如缓存、计算和无线电资源，相对静态，而上层的虚拟化网络功能（VNFs）则是动态的，以支持时间变化的应用特定服务请求。网络切片的概念是将网络资源划分为多层切片，由不同的服务提供者独立管理，减少冲突。服务功能链（SFC）的概念是协调不同的
    VNFs 以提供所需的功能和服务质量（QoS）保障。
- en: The authors in [[149](#bib.bib149)] propose a DQL scheme for QoS/QoE-aware SFC
    in NFV-enabled 5G systems. Typical QoS metrics are bandwidth, delay, throughput,
    etc. The evaluation of QoE normally involves the end-user’s participation in rating
    the service based on direct user perception. The authors quantify QoE by measurable
    QoS metrics without end-user involvements, according to the Weber-Fechner Law
    (WFL) [[150](#bib.bib150)] and exponential interdependency of QoE and QoS hypothesis [[151](#bib.bib151)].
    These two principles actually define nonlinear relationship between QoE and QoS.
    The system state represents the network environment including network topology,
    QoS/QoE status of the VNF instances, and the QoS requirements of the SFC request.
    The DQL agent selects a certain direct successive VNF instance as an action. The
    reward is a composite function of the QoE gain, the QoS constraint penalty, and
    the OPEX penalty. A DQL based on CNNs is implemented to approximate the action-value
    function. The authors in [[152](#bib.bib152)] review the application of a DQL
    framework in two typical resource management scenarios using network slicing.
    For radio resource slicing, the authors simulate a scenario containing one single
    BS with different types of services. The reward can be defined as a weighted sum
    of spectrum efficiency and QoE. For priority-based core network slicing, the authors
    simulate a scenario with 3 SFCs demanding different computational resources and
    waiting time. The reward is the sum of waiting time in different SFCs. Simulation
    results in both scenarios show that the DQL framework could exploit more implicit
    relationship between user activities and resource allocation in resource constrained
    scenarios, and enhance the effectiveness and agility for network slicing.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在 [[149](#bib.bib149)] 中提出了一种用于 NFV 支持的 5G 系统中的 QoS/QoE 认知 SFC 的 DQL 方案。典型的
    QoS 指标包括带宽、延迟、吞吐量等。QoE 的评估通常涉及最终用户根据直接的用户感知来评分。作者通过可测量的 QoS 指标来量化 QoE，而无需最终用户的参与，这基于
    Weber-Fechner 法则 (WFL) [[150](#bib.bib150)] 和 QoE 与 QoS 假设的指数相互依赖 [[151](#bib.bib151)]。这两个原则实际上定义了
    QoE 与 QoS 之间的非线性关系。系统状态表示网络环境，包括网络拓扑、VNF 实例的 QoS/QoE 状态以及 SFC 请求的 QoS 要求。DQL 代理选择某个直接连续的
    VNF 实例作为行动。奖励是 QoE 增益、QoS 约束罚款和运营支出（OPEX）罚款的复合函数。实现了基于 CNN 的 DQL 以近似行动价值函数。作者在
    [[152](#bib.bib152)] 中回顾了在两个典型的资源管理场景中使用网络切片的 DQL 框架的应用。对于无线电资源切片，作者模拟了一个包含一个单一基站和不同类型服务的场景。奖励可以定义为频谱效率和
    QoE 的加权和。对于基于优先级的核心网络切片，作者模拟了一个包含 3 个 SFC 需求不同计算资源和等待时间的场景。奖励是不同 SFC 中等待时间的总和。两个场景中的仿真结果表明，DQL
    框架可以在资源受限的场景中挖掘用户活动和资源分配之间的更多隐含关系，并增强网络切片的有效性和灵活性。
- en: Resource allocation and scheduling problems are also important for computer
    clusters or database systems. This usually leads to an online decision-making
    problem depending on the information of workload and environment. The authors
    in [[153](#bib.bib153)] propose a DRL-based solution, DeepRM, by employing policy
    gradient methods [[140](#bib.bib140)] to manage resources in computer systems
    directly from experience. The same policy gradient method is also used in [[144](#bib.bib144)]
    for user scheduling and resource management in wireless systems. DeepRM is a multi-resource
    cluster scheduler that learns to optimize various objectives such as minimizing
    average job slowdown or completion time. The system state is the current allocation
    of cluster resources and the resource profiles of jobs in the queue. The action
    of the scheduler is to decide how to schedule the pending jobs. By simulations
    with synthetic dataset, DeepRM is shown to perform comparably or better than state-of-the-art
    heuristics, e.g., Shortest-Job-First (SJF). It adapts to different conditions
    and converges quickly, without any prior knowledge of system behavior. In [[154](#bib.bib154)],
    the authors use the actor-critic method to address the scheduling problem in a
    general-purpose distributed data stream processing systems, which deal with processing
    of continuous data flow in real time or near-real-time. The system model contains
    multiple threads, processes, and machines. The system state consists of the current
    scheduling decision and the workload of each data source. The scheduling problem
    is to assign each thread to a process of a machine. The agent at the scheduler
    determines the assignment of each thread, with the objective of minimizing the
    average processing time. The DRL framework includes three components, i.e., an
    actor network, an optimizer producing a K-NN set of the actor network’s output
    action, and the critic network predicting the Q-value for each action in the set.
    The action is selected from the K-NN set with the maximum Q-value. The use of
    optimizer may avoid unstable learning and divergence problems in conventional
    actor-critic methods [[155](#bib.bib155)].
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 资源分配和调度问题在计算机集群或数据库系统中也很重要。这通常会导致一个在线决策问题，依赖于工作负载和环境的信息。在 [[153](#bib.bib153)]中，作者提出了一种基于深度强化学习（DRL）的解决方案，DeepRM，利用策略梯度方法 [[140](#bib.bib140)]直接从经验中管理计算机系统中的资源。相同的策略梯度方法也被用于 [[144](#bib.bib144)]中，用于无线系统中的用户调度和资源管理。DeepRM是一个多资源集群调度器，它学习优化各种目标，如最小化平均作业减慢或完成时间。系统状态是集群资源的当前分配情况以及队列中作业的资源配置。调度器的动作是决定如何安排待处理的作业。通过对合成数据集的模拟，DeepRM的表现与最先进的启发式算法（例如，最短作业优先（SJF））相当或更好。它能够适应不同的条件，并且在没有任何系统行为先验知识的情况下快速收敛。在 [[154](#bib.bib154)]中，作者使用演员-评论员方法来解决通用分布式数据流处理系统中的调度问题，该系统处理实时或接近实时的连续数据流。系统模型包含多个线程、进程和机器。系统状态包括当前调度决策和每个数据源的工作负载。调度问题是将每个线程分配给某台机器的一个进程。调度器中的代理决定每个线程的分配，目标是最小化平均处理时间。DRL框架包括三个组件，即演员网络、一个生成演员网络输出动作的K-NN集的优化器，以及预测每个动作Q值的评论员网络。动作从具有最大Q值的K-NN集中选择。使用优化器可以避免传统演员-评论员方法中的不稳定学习和发散问题 [[155](#bib.bib155)]。
- en: VI-C Power Control and Data Collection
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 电力控制与数据收集
- en: With the prevalence of IoT and smart mobile devices, mobile crowdsensing becomes
    a cost-effective solution for network information collection to support more intelligent
    operations of wireless systems. The authors in [[156](#bib.bib156)] consider spectrum
    sensing and power control in non-cooperative cognitive radio networks. There is
    no information exchange between PUs and SUs. As such, the SU outsources the sensing
    task to a set of spatially distributed sensing devices to collect information
    about the PU’s power control strategy. The SU’s power control can be formulated
    as an MDP. The system state is determined by the Received Signal Strength (RSS)
    at individual sensing devices. The SU chooses its transmit power from the set
    of pre-specified power levels based on the current state. A reward is obtained
    if both primary and SUs can fulfill their SNR requirements. Considering the randomness
    in RSS measurements, the authors propose a DQL scheme for the SU to learn and
    adjust its transmit power. The DQL is then implemented by a DQN by using FNN.
    The simulation results show that the proposed DQL scheme is able to converge to
    a close-to-optimal solution.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 随着物联网和智能移动设备的普及，移动众包感知成为一种具有成本效益的网络信息收集解决方案，以支持无线系统的更智能操作。作者们在[[156](#bib.bib156)]中考虑了非合作认知无线电网络中的频谱感知和功率控制。在这些网络中，主用户（PU）和次用户（SU）之间没有信息交换。因此，SU将感知任务外包给一组空间分布的感知设备，以收集有关PU功率控制策略的信息。SU的功率控制可以被建模为一个马尔可夫决策过程（MDP）。系统状态由各个感知设备的接收信号强度（RSS）决定。SU根据当前状态从预定的功率水平集合中选择其发射功率。如果主用户和次用户都能满足其信噪比（SNR）要求，则会获得奖励。考虑到RSS测量中的随机性，作者提出了一种DQL方案，使SU能够学习并调整其发射功率。DQL方案通过使用前馈神经网络（FNN）的DQN实现。仿真结果表明，所提出的DQL方案能够收敛到接近最优的解决方案。
- en: The authors in [[157](#bib.bib157)] leverage the DQL framework for sensing and
    control problems in a Wireless Sensor and Actor Network (WSAN), which is a group
    of wireless devices with the ability to sense events and to perform actions based
    on the sensed data shared by all sensors. The system state includes processing
    power, mobility abilities, and functionalities of the actors and sensors. The
    mobile actor can choose its moving direction, networking, sensing and actuation
    policies to maximize the number of connected actor nodes and the number of sensing
    events.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们在[[157](#bib.bib157)]中利用DQL框架来处理无线传感器与执行器网络（WSAN）中的感知和控制问题，这是一组具有感知事件和根据所有传感器共享的感知数据执行动作能力的无线设备。系统状态包括执行器和传感器的处理能力、移动能力和功能。移动执行器可以选择其移动方向、网络、感知和执行策略，以最大化连接的执行器节点数量和感知事件的数量。
- en: The authors in [[158](#bib.bib158)] focus on mobile crowdsensing paradigm, where
    data inference is incorporated to reduce sensing costs while maintaining the quality
    of sensing. The target sensing area is split into a set of cells. The objective
    of a sensing task is to collect data (e.g., temperature, air quality) in all the
    cells. A DQL-based cell selection mechanism is proposed for the mobile sensors
    to decide which cell is a better choice to perform sensing tasks. The system state
    includes the selection matrices for a few past decision epochs. The reward function
    is determined by the sensing quality and cost in the chosen cells. To extract
    temporal correlations in learning, the authors propose the DRQN that uses LSTM
    layers in DQL to capture the hidden patterns in state transitions. Considering
    inter-data correlations, the authors use the transfer learning method to reduce
    the amount of data in training. That is, the cell selection strategy learned for
    one task can benefit another correlated task. Hence, the parameters of DRQN can
    be initialized by another DRQN with rich training data. Simulations are conducted
    based on two real-life datasets collected from sensor networks, i.e., the Sensor-Scope
    dataset [[159](#bib.bib159)] in the EPFL campus and the U-Air dataset of air quality
    readings in Beijing [[160](#bib.bib160)]. The experiments verify that DRQN reduces
    up to 15% of the sensed cells with the same inference quality guarantee. The authors
    in [[161](#bib.bib161)] combine UAV and unmanned vehicle in mobile crowdsensing
    for smart city applications. The UAV cruises in the above of the target region
    for city-level data collection. Meanwhile, the unmanned vehicle carrying mobile
    charging stations moves on the ground and can charge the UAV at a preset charging
    point.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们在[[158](#bib.bib158)]中专注于移动众包感知范式，其中结合了数据推断以降低感知成本，同时保持感知质量。目标感知区域被划分为一组单元。感知任务的目标是收集所有单元中的数据（例如，温度、空气质量）。为移动传感器提出了一种基于DQL的单元选择机制，以决定哪个单元是执行感知任务的更好选择。系统状态包括过去几个决策时刻的选择矩阵。奖励函数由所选单元中的感知质量和成本决定。为了提取学习中的时间相关性，作者提出了DRQN，该方法在DQL中使用LSTM层来捕捉状态转移中的隐藏模式。考虑到数据间的相关性，作者使用了迁移学习方法来减少训练中的数据量。也就是说，为一个任务学习的单元选择策略可以惠及另一个相关任务。因此，DRQN的参数可以通过具有丰富训练数据的另一个DRQN进行初始化。基于两个来自传感器网络的实际数据集进行了模拟，即EPFL校园的Sensor-Scope数据集[[159](#bib.bib159)]和北京空气质量读数的U-Air数据集[[160](#bib.bib160)]。实验验证了DRQN在保证相同推断质量的情况下可以减少多达15%的感知单元。作者们在[[161](#bib.bib161)]中将无人机和无人驾驶车辆结合用于智能城市应用的移动众包感知。无人机在目标区域上方巡航进行城市级数据收集。同时，无人驾驶车辆携带移动充电站在地面移动，并可以在预设的充电点为无人机充电。
- en: The target region is divided into multiple subregions and each subregion has
    a different sample priority. The authors in [[161](#bib.bib161)] propose a DQL-based
    control framework for the unmanned vehicle to schedule its data collection, constrained
    by limited energy supply. The system state includes information about the sample
    priority of each subregion, the location of charging point, and the moving trace
    of the UAV and unmanned vehicle. The UAV and unmanned vehicle can choose the moving
    direction. The DQL framework utilizes CNNs for extracting the correlation of adjacent
    subregions, which can increase the convergence speed in training. The DQL algorithm
    can be enhanced by using a feasible control solution as the baseline during exploration.
    The PER method is also used in DQL to assign higher priorities to important transitions
    so that the DQL agent can learn from samples more efficiently. The proposed scheme
    is evaluated by using real dataset of taxi traces in Rome [[162](#bib.bib162)].
    Simulation results reveal that the proposed DQL algorithm can obtain the highest
    data collection rate compared with the MDP and other heuristic baselines.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 目标区域被划分为多个子区域，每个子区域具有不同的样本优先级。作者们在[[161](#bib.bib161)]中提出了一种基于DQL的控制框架，用于调度无人驾驶车辆的数据收集，受限于有限的能源供应。系统状态包括每个子区域的样本优先级信息、充电点的位置以及无人机和无人驾驶车辆的移动轨迹。无人机和无人驾驶车辆可以选择移动方向。DQL框架利用CNN提取相邻子区域的相关性，这可以提高训练中的收敛速度。通过在探索过程中使用可行的控制解决方案作为基准，DQL算法可以得到增强。PER方法也用于DQL中，以便为重要的转移分配更高的优先级，从而使DQL代理可以更有效地从样本中学习。提出的方案通过使用罗马出租车轨迹的真实数据集[[162](#bib.bib162)]进行了评估。模拟结果揭示，提出的DQL算法与MDP和其他启发式基线相比，可以获得最高的数据收集率。
- en: Mobile crowdsensing is vulnerable to faked sensing attacks, as selfish users
    may report faked sensing results to save their sensing costs and avoid compromising
    their privacy. The authors in [[163](#bib.bib163)] formulate the interactions
    between the server and a number of crowdsensing users as a Stackelberg game. The
    server is the leader that sets and broadcasts its payment policy for different
    sensing accuracy. In particular, the higher payment is set for more sensing accuracy.
    Based on the server’s sensing policy, each user as a follower then chooses its
    sensing effort and thus the sensing accuracy to receive the payment. The payment
    motivates the users to put in sensing efforts and thus the payment decision process
    can be modeled as an MDP. In a dynamic network, the server uses the DQL to derive
    the optimal payment to maximize its utility, based on the system state consisting
    of the previous sensing quality and the payment policy. The DQL uses a deep CNN
    to accelerate the learning process and improve the crowdsensing performance against
    selfish users. Simulation results show that the DQL-based scheme produces a higher
    sensing quality, lower attack rate, and higher utility of the server, exceeding
    those of both the Q-learning and the random payment strategies.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 移动众包感知易受到伪造感知攻击的威胁，因为自私的用户可能会报告伪造的感知结果以节省感知成本并避免泄露隐私。文献[[163](#bib.bib163)]中的作者将服务器与多个众包感知用户之间的互动形式化为一个斯塔克尔伯格博弈。服务器是领导者，设定并广播其针对不同感知准确度的支付政策。具体来说，支付金额与感知准确度成正比，即支付金额越高，感知准确度越高。根据服务器的感知政策，每个用户作为追随者，然后选择其感知努力，从而决定感知准确度以获取支付。支付激励用户投入感知努力，因此支付决策过程可以建模为一个马尔可夫决策过程（MDP）。在动态网络中，服务器使用深度Q学习（DQL）来推导出最优支付，以最大化其效用，这基于由前期感知质量和支付政策组成的系统状态。DQL使用深度卷积神经网络（CNN）加速学习过程，并提高了对抗自私用户的众包感知性能。仿真结果表明，基于DQL的方案产生了更高的感知质量、更低的攻击率和更高的服务器效用，超过了Q学习和随机支付策略。
- en: Social networking is an important component of smart city applications. The
    authors in [[164](#bib.bib164)] aim to extract useful information by observing
    and analyzing the users’ behaviors in social networking. One of the main difficulties
    is that the social behaviors are usually fuzzy and divergent. The authors model
    pervasive social networking as a monopolistically competitive market, which contains
    different users as data providers selling information at a certain price. Given
    the market model, the DQL can be used to estimate the users’ behavior patterns
    and find the market equilibrium. Considering the costly deep learning structure,
    the authors in [[164](#bib.bib164)] propose a Decentralized DRL (DDRL) framework
    that decomposes the costly deep component from the RL algorithms at individual
    users. The deep component can be a feature extractor integrated with the network
    infrastructure and provide mutual knowledge for all individuals. Multiple RL agents
    can purchase the most desirable data from the mutual knowledge. The authors combine
    well-known RL algorithms, i.e., Q-learning and learning automata, to estimate
    users’ patterns which are described by vectors of probabilities representing the
    users’ preferences or altitudes to different information. In social networking
    and smart city applications with human involvement, there can be both labeled
    and unlabeled data and hence a semi-supervised DRL framework can be designed,
    by combining the strengths of DNNs and statistical modeling to improve the performance
    and accuracy in learning. Then, the authors in [[165](#bib.bib165)] introduce
    the semi-supervised DRL framework that utilizes variational auto-encoders [[166](#bib.bib166)]
    as an inference engine to infer the classification of unlabeled data. As a case
    study, the proposed DRL framework is customized to provide indoor localization
    based on the RSS from Bluetooth devices. The positioning environment contains
    a set of positions. Each position is associated with the set of RSS values from
    the set of anchor devices with known positions. The system state includes a vector
    of RSS values, the current location, and the distance to the target. The DQL agent,
    i.e., the positioning algorithm itself, chooses a moving direction to minimize
    the error distance to the target point. Simulations tested on real-world dataset
    show an improvement of 23% in terms of the error distance to the target compared
    with the supervised DRL scheme.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 社交网络是智能城市应用中的一个重要组成部分。[[164](#bib.bib164)]中的作者旨在通过观察和分析用户在社交网络中的行为来提取有用的信息。主要的困难之一是社交行为通常是模糊且多样的。作者将普遍存在的社交网络建模为一个垄断竞争市场，其中不同的用户作为数据提供者以一定的价格出售信息。根据市场模型，可以使用DQL来估计用户的行为模式并找到市场均衡。考虑到深度学习结构的高成本，[[164](#bib.bib164)]中的作者提出了一种去中心化的DRL（DDRL）框架，将深度组件从单个用户的RL算法中分解出来。深度组件可以是一个集成到网络基础设施中的特征提取器，并为所有个体提供互相知识。多个RL代理可以从这些互相知识中购买最理想的数据。作者将著名的RL算法，如Q-learning和学习自动机结合起来，以估计用户的模式，这些模式由表示用户对不同信息的偏好或态度的概率向量描述。在涉及人类参与的社交网络和智能城市应用中，可能存在标记数据和未标记数据，因此可以设计一个半监督DRL框架，通过结合DNN和统计建模的优势来提高学习的性能和准确性。然后，[[165](#bib.bib165)]中的作者介绍了一种半监督DRL框架，该框架利用变分自编码器[[166](#bib.bib166)]作为推断引擎来推断未标记数据的分类。作为案例研究，所提出的DRL框架被定制为基于蓝牙设备的RSS提供室内定位。定位环境包含一组位置。每个位置与一组来自具有已知位置的锚设备的RSS值相关联。系统状态包括RSS值向量、当前位置以及到目标的距离。DQL代理，即定位算法本身，选择移动方向以最小化到目标点的误差距离。基于实际数据集的仿真测试显示，相比于监督DRL方案，目标误差距离提高了23%。
- en: 'Summary: In this section, we review miscellaneous uses of DRL in wireless and
    networked systems. DRL provides a flexible tool in rich and diversified applications,
    conventionally involving dynamic system modeling and multi-agent interactions.
    All these imply a huge space of state transitions and actions. These approaches
    are summarized along with the references in Table [VI](#S6.T6 "TABLE VI ‣ VI-C
    Power Control and Data Collection ‣ VI Miscellaneous Issues ‣ Applications of
    Deep Reinforcement Learning in Communications and Networking: A Survey"). We observe
    that the NUM problems in 5G ecosystem for traffic engineering and resource allocation
    face very diversified control variables, including discrete indicators, e.g.,
    for BS (de)activation, user/cell association, and path selection, as well as continuous
    variables such as bandwidth allocation, transmit power, and beamforming optimization.
    Hence, both DQL and policy gradient methods are used extensively for discrete
    and continuous control problems, respectively.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '总结：在本节中，我们回顾了 DRL 在无线和网络系统中的各种用途。DRL 提供了一种灵活的工具，广泛应用于动态系统建模和多智能体互动等领域。这些应用暗示了状态转换和动作的巨大空间。这些方法及其参考文献在表 [VI](#S6.T6
    "TABLE VI ‣ VI-C Power Control and Data Collection ‣ VI Miscellaneous Issues ‣
    Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey") 中进行了总结。我们观察到，在 5G 生态系统中，交通工程和资源分配的问题面临非常多样化的控制变量，包括离散指标，例如 BS 的 (停)激活、用户/小区关联和路径选择，以及连续变量如带宽分配、发射功率和波束赋形优化。因此，DQL
    和策略梯度方法分别在离散和连续控制问题中得到了广泛应用。'
- en: 'TABLE VI: A summary of applications of DQL for traffic engineering, resource
    scheduling, and data collection.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：DQL 在交通工程、资源调度和数据收集中的应用总结。
- en: '| Issues | Ref. | Model | Learning algorithms | Agent | States | Actions |
    Rewards | Scenarios |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 参考文献 | 模型 | 学习算法 | 代理 | 状态 | 动作 | 奖励 | 场景 |'
- en: '| Traffic engineering and routing | [[133](#bib.bib133)] | MDP | DQN using
    actor-critic networks | Network controller | Bandwidth request of each node pair
    | Traffic load split on different paths | Mean network delay | 5G network |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 交通工程和路由 | [[133](#bib.bib133)] | MDP | 使用 actor-critic 网络的 DQN | 网络控制器 |
    每对节点的带宽请求 | 在不同路径上的流量负载分配 | 平均网络延迟 | 5G 网络 |'
- en: '|  | [[136](#bib.bib136)] | NUM | DQN using actor-critic networks | Network
    controller | Throughput and delay performance | Traffic load split on different
    paths | $\alpha$-fairness utility | 5G network |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  | [[136](#bib.bib136)] | NUM | 使用 actor-critic 网络的 DQN | 网络控制器 | 吞吐量和延迟性能
    | 在不同路径上的流量负载分配 | $\alpha$-公平性效用 | 5G 网络 |'
- en: '|  | [[130](#bib.bib130)] | POMDP | DQN using actor-critic networks | UAV |
    Local sensory information, e.g., distances and angles | Turn left or right | Composite
    reward | UAV navigation |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | [[130](#bib.bib130)] | POMDP | 使用 actor-critic 网络的 DQN | 无人机 | 本地感测信息，如距离和角度
    | 向左或向右转 | 复合奖励 | 无人机导航 |'
- en: '|  | [[141](#bib.bib141)] [[142](#bib.bib142)] | Game | DQN using ESN | UAV
    | Coordinates, distances, and orientation angles | Path, transmit power, and cell
    association | Weighted sum of energy efficiency, latency, and interference | Cellular-connected
    UAVs |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|  | [[141](#bib.bib141)] [[142](#bib.bib142)] | 游戏 | 使用 ESN 的 DQN | 无人机 |
    坐标、距离和方向角 | 路径、发射功率和小区关联 | 能效、延迟和干扰的加权总和 | 蜂窝连接的无人机 |'
- en: '|  | [[143](#bib.bib143)] | MDP | DQN using FNN | Train scheduler | Channel
    conditions, train position, speed, SNR, and handoff indicator | Making handoff
    of connection, or accelerate or decelerate the train | Tracking error and energy
    consumption | Vehicle-to-infrastructure system |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | [[143](#bib.bib143)] | MDP | 使用 FNN 的 DQN | 列车调度器 | 信道条件、列车位置、速度、SNR 和切换指示符
    | 进行连接切换或加速/减速列车 | 跟踪误差和能耗 | 车到基础设施系统 |'
- en: '| Resource sharing and scheduling | [[145](#bib.bib145)] | MDP | DQN using
    FNN | Cloud baseband unit | MUs’ demands and the RRHs’ working states | Turn on
    or off certain RRH(s), and beamforming allocation | Expected power consumption
    | Cloud RAN |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 资源共享和调度 | [[145](#bib.bib145)] | MDP | 使用 FNN 的 DQN | 云基带单元 | MUs 的需求和 RRHs
    的工作状态 | 打开或关闭某些 RRH(s) 和波束赋形分配 | 预期功耗 | 云 RAN |'
- en: '|  | [[149](#bib.bib149)] | MDP | DQN with CNN | Network controller | Network
    topology, QoS/QoE status, and the QoS requirements | Successive VNF instance |
    Composite function of QoE gain, QoS constraints penalty, and OPEX penalty | Cellular
    system |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  | [[149](#bib.bib149)] | MDP | 使用 CNN 的 DQN | 网络控制器 | 网络拓扑、QoS/QoE 状态和 QoS
    要求 | 连续的 VNF 实例 | QoE 增益、QoS 约束惩罚和 OPEX 惩罚的复合函数 | 蜂窝系统 |'
- en: '|  | [[152](#bib.bib152)] | MDP | DQN using FNN | Network controller | The
    number of arrived packets/the priority and time-stamp of flows | Bandwidth/SFC
    allocation | Weighted sum of spectrum efficiency and QoE/waiting time in SFCs
    | 5G network |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|  | [[152](#bib.bib152)] | MDP | 使用 FNN 的 DQN | 网络控制器 | 到达的数据包数量/流的优先级和时间戳
    | 带宽/SFC 分配 | 频谱效率和 QoE/在 SFC 中的等待时间的加权总和 | 5G 网络 |'
- en: '|  | [[154](#bib.bib154)] | MDP | DQN using actor-critic networks | Central
    scheduler | Current scheduling decision and the workload | Assignment of each
    thread | Average processing time | Distributed stream data processing |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | [[154](#bib.bib154)] | MDP | 使用演员-评论家网络的 DQN | 中央调度器 | 当前调度决策和工作负载 | 每个线程的分配
    | 平均处理时间 | 分布式流数据处理 |'
- en: '| Data collection | [[156](#bib.bib156)] | MDP | DQN using FNN | Secondary
    user | Received signal strength at individual sensors | Transmit power | Fixed
    reward if QoS satisfied | CRN |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 数据收集 | [[156](#bib.bib156)] | MDP | 使用 FNN 的 DQN | 次级用户 | 各传感器的接收信号强度 | 传输功率
    | 如果满足 QoS 则固定奖励 | CRN |'
- en: '|  | [[158](#bib.bib158)] | MDP | DRQN, LSTM, transfer learning | Mobile sensors
    | Cell selection matrices | Next cell for sensing | A function of the sensing
    quality and cost | WSN |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | [[158](#bib.bib158)] | MDP | DRQN、LSTM、迁移学习 | 移动传感器 | 小区选择矩阵 | 下一个感测小区
    | 感测质量和成本的函数 | WSN |'
- en: '|  | [[161](#bib.bib161)] | MDP | DQN using CNN | UAV and unmanned vehicle
    | Subregions’ sample priority, charging point’s location, and trace of the UAV
    and unmanned vehicle | Moving direction of the UAV and unmanned vehicle | Fixed
    reward related to subregions’ sample priority | UAV and vehicle |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | [[161](#bib.bib161)] | MDP | 使用 CNN 的 DQN | UAV 和无人车 | 子区域样本优先级、充电点位置和
    UAV 与无人车的轨迹 | UAV 和无人车的移动方向 | 与子区域样本优先级相关的固定奖励 | UAV 和车辆 |'
- en: '|  | [[163](#bib.bib163)] | Game | DQN using CNN | Crowdsensing server | Previous
    sensing quality and payment policy | Current payment policy | Utility | Mobile
    crowdsensing |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  | [[163](#bib.bib163)] | 游戏 | 使用 CNN 的 DQN | 群体感知服务器 | 之前的感知质量和支付政策 | 当前支付政策
    | 效用 | 移动群体感知 |'
- en: '|  | [[164](#bib.bib164)] | Game | DDQN | Mobile users | Current preferences
    | Positive or negative altitude | Reward or profit | Mobile social network |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | [[164](#bib.bib164)] | 游戏 | DDQN | 移动用户 | 当前偏好 | 正负高度 | 奖励或利润 | 移动社交网络
    |'
- en: VII Challenges, Open Issues, and Future Research Directions
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 挑战、开放问题和未来研究方向
- en: Different approaches reviewed in this survey evidently show that DRL can effectively
    address various emerging issues in communications and networking. There are existing
    challenges, open issues, and new research directions which are discussed as follows.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查中回顾的不同方法明显显示，DRL 可以有效地解决通信和网络中的各种新兴问题。以下讨论了现有的挑战、开放问题和新的研究方向。
- en: VII-A Challenges
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 挑战
- en: VII-A1 State Determination in Density Networks
  id: totrans-413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-A1 密度网络中的状态确定
- en: The DRL approaches, e.g., [[23](#bib.bib23)], allow the users to find an optimal
    access policy without having complete and/or accurate network information. However,
    the DRL approaches often require the users to report their local states at every
    time slot. To observe the local state, the user needs to monitor Received Signal
    Strength Indicators (RSSIs) from its neighboring BSs, and then it temporarily
    connects to the BS with the maximum RSSI. However, the future networks will deploy
    a high density of the BSs, and the RSSIs from different BSs may not be different.
    Thus, it is challenging for the users to determine the temporary BS [[167](#bib.bib167)].
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 方法，例如 [[23](#bib.bib23)]，允许用户在没有完整和/或准确的网络信息的情况下找到最优访问策略。然而，DRL 方法通常要求用户在每个时间段报告其本地状态。为了观察本地状态，用户需要监控来自其邻近基站的接收信号强度指示器
    (RSSI)，然后暂时连接到 RSSI 最大的基站。然而，未来的网络将部署高密度的基站，不同基站的 RSSI 可能没有区别。因此，用户在确定临时基站时面临挑战
    [[167](#bib.bib167)]。
- en: VII-A2 Knowledge of Jammers’ Channel Information
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-A2 干扰源的频道信息知识
- en: The DRL approach for wireless security as proposed in [[118](#bib.bib118)] enables
    the UAV to find optimal transmit power levels to maximize the security capacity
    of the UAV and the BS. However, to formulate the reward of the UAV, a perfect
    knowledge of channel information of the jammers is required. This is challenging
    and even impossible in practice.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[118](#bib.bib118)] 提出的无线安全 DRL 方法使 UAV 能够找到最优的传输功率水平，以最大化 UAV 和基站的安全容量。然而，要制定
    UAV 的奖励，需要对干扰源的频道信息有完美的了解。这在实际中具有挑战性，甚至是不可能的。
- en: VII-A3 Multi-agent DRL in Dynamic HetNets
  id: totrans-417
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-A3 动态异构网络中的多智能体 DRL
- en: Most of the existing works focus on the customizations of DRL framework for
    individual network entities, based on locally observed or exchanged network information.
    Hopefully, the network environment is relatively static to ensure convergent learning
    results and stable policies. This requirement may be challenged in a dynamic heterogenous
    5G network, which consists of hierarchically nested IoT devices/networks with
    fast changing service requirements and networking conditions. In such a situation,
    the DQL agents for individual entities have to be light-weighted and agile to
    the change of network conditions. This implies a reduce to the state and action
    spaces in learning, which however may compromise the performance of the convergent
    policy. The interactions among multiple agents also complicate the network environment
    and cause a considerable increase to the state space, which inevitably slows down
    the learning algorithms.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的大多数工作集中于针对个别网络实体的DRL框架定制，基于本地观察或交换的网络信息。希望网络环境相对静态，以确保收敛的学习结果和稳定的策略。然而，这一要求可能会在动态异构的5G网络中受到挑战，该网络由具有快速变化服务需求和网络条件的分层嵌套IoT设备/网络组成。在这种情况下，针对个别实体的DQL代理必须具有轻量化并且对网络条件变化具有敏捷性。这意味着学习中的状态和动作空间会减少，但这可能会影响收敛策略的性能。多个代理之间的交互也使网络环境变得复杂，并导致状态空间显著增加，这不可避免地减慢了学习算法的速度。
- en: VII-A4 Training and Performance Evaluation of DRL Framework
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-A4 DRL框架的训练和性能评估
- en: The DRL framework requires large amounts of data for both training and performance
    evaluation. In wireless systems, such data is not easily accessible as we rarely
    have referential data pools as other deep learning scenarios, e.g., computer vision.
    Most of the existing works rely on simulated dataset, which undermines the confidence
    of the DRL framework in practical system. The simulated data set is usually generated
    by a specific stochastic model, which is a simplification of the real system and
    may overlook the hidden patterns. Hence, a more effective way for generating simulation
    data is required to ensure that the training and performance evaluation of the
    DRL framework are more consistent with practical system.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: DRL框架需要大量的数据用于训练和性能评估。在无线系统中，这些数据并不容易获取，因为我们很少有像计算机视觉等其他深度学习场景那样的参考数据池。现有的大多数工作依赖于模拟数据集，这削弱了DRL框架在实际系统中的信心。模拟数据集通常由特定的随机模型生成，这是一种对真实系统的简化，可能忽略了隐藏的模式。因此，需要一种更有效的生成模拟数据的方法，以确保DRL框架的训练和性能评估与实际系统更为一致。
- en: VII-B Open Issues
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 开放问题
- en: VII-B1 Distributed DRL Framework in Wireless Networks
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B1 无线网络中的分布式DRL框架
- en: The DRL framework requires large amounts of training for DNNs. This may be implemented
    at a centralized network controller, which has sufficient computational capacity
    and the capability for information collection. However, for massive end users
    with limited capabilities, it becomes a meaningful task to design distributed
    implementation for the DRL framework that decomposes resource-demanding basic
    functionalities, e.g., information collection, sharing, and DNN training, from
    reinforcement learning algorithms at individual devices. The basic functionalities
    can be integrated with the network controller. It remains an open issue for the
    design of network infrastructure that supports these common functionalities for
    distributed DRL. The overhead of information exchange between end users and network
    controller also has to be well controlled.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: DRL框架需要大量的DNN训练。这可以在一个集中式网络控制器上实现，该控制器具有足够的计算能力和信息收集能力。然而，对于具有有限能力的大量终端用户来说，设计一个分布式实施DRL框架变得很有意义，该框架将资源密集型的基本功能（如信息收集、共享和DNN训练）从各个设备上的强化学习算法中解耦。基本功能可以与网络控制器集成。支持这些通用功能的网络基础设施设计仍然是一个开放问题。终端用户和网络控制器之间的信息交换开销也必须得到良好的控制。
- en: VII-B2 Balance between Information Quality and Learning Performance
  id: totrans-424
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-B2 信息质量与学习性能之间的平衡
- en: The majority of the existing works consider the orchestration of networking,
    transmission control, offloading, and caching decisions in one DRL framework to
    derive the optimal policy, e.g., [[84](#bib.bib84), [86](#bib.bib86), [85](#bib.bib85),
    [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [91](#bib.bib91), [92](#bib.bib92)].
    However, from a practical viewpoint, the network system will have to pay substantially
    increasing cost for information gathering. The cost is incurred from large delay,
    pre-processing of asynchronous information, excessive energy consumption, reduced
    learning speed, etc. Hence, an open issue is to find the optimal balance between
    information quality and learning performance so that the DQL agent does not consume
    too much resources only to achieve insignificantly marginal increase in the learning
    performance.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有工作将网络编排、传输控制、卸载和缓存决策纳入一个DRL框架，以得出最佳策略，例如[[84](#bib.bib84)、[86](#bib.bib86)、[85](#bib.bib85)、[87](#bib.bib87)、[88](#bib.bib88)、[89](#bib.bib89)、[91](#bib.bib91)、[92](#bib.bib92)]。然而，从实际角度来看，网络系统将不得不为信息收集支付显著增加的成本。这些成本来源于大的延迟、异步信息的预处理、过度的能量消耗、学习速度的降低等。因此，一个待解决的问题是找到信息质量和学习性能之间的最佳平衡，以确保DQL代理不会消耗过多资源仅仅为了在学习性能上获得微不足道的提升。
- en: VII-C Future Research Directions
  id: totrans-426
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 未来研究方向
- en: VII-C1 DRL for Channel Estimation in Wireless Systems
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-C1 用于无线系统信道估计的DRL
- en: Massive MIMO will be deployed for 5G to achieve high-speed communications at
    Gbps. For this, the channel estimation is the prerequisite for realizing massive
    MIMO. However, in a large-scale heterogeneous cellular network foreseen for 5G
    or beyond, the required channel estimation is very challenging. Thus, DRL will
    play an important role in acquiring the channel estimates with regard to dynamic
    time-varying wireless channels.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模MIMO将用于5G，以实现Gbps级别的高速通信。为此，信道估计是实现大规模MIMO的前提。然而，在为5G或更高版本预期的大规模异构蜂窝网络中，所需的信道估计是非常具有挑战性的。因此，DRL将在获取动态时间变化无线信道的信道估计方面发挥重要作用。
- en: Also, we expect that the combination of Wireless Power Transfer (WPT) and Mobile
    Crowd Sensing (MCS), namely Wireless-Powered Crowd Sensing (WPCS) will be a promising
    technique for the emerging IoT services. To this end, a higher power transfer
    efficiency of WPT is very critical to enable the deployment of WPCS in low-power
    wide area network. A "large-scale array antenna based WPT" will achieve this goal
    of higher WPT efficiency, but the channel estimation should be performed with
    minimal power consumption at a sensor node. This is because of that the sensor
    must operate with self-powering via WPT from the dedicated energy source, e.g.,
    power beacon, Wi-Fi or small-cell access point, and/or ambient RF sources, e.g.,
    TV tower, Wi-Fi AP and cellular BS. In this regard, the channel estimation based
    on the receive power measurements at the sensor node is one viable solution, because
    the receive power can be measured by the passive-circuit power meter with negligible
    power consumption. DRL can be used for the time-varying wireless channels with
    temporal correlations over time by taking the receive power measurements from
    the sensor node as the input for DRL, which will enable the channel estimation
    for WPT efficiently.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们预计无线电力传输（WPT）与移动人群感知（MCS）的结合，即无线电力驱动的众包感知（WPCS），将成为新兴物联网服务的一项有前景的技术。为此，提高WPT的功率传输效率对于在低功耗广域网中部署WPCS至关重要。"大规模阵列天线基础的WPT"将实现这一较高WPT效率的目标，但信道估计应在传感器节点的最小功耗下进行。这是因为传感器必须通过来自专用能源源（例如，功率信标、Wi-Fi或小区接入点）和/或环境RF源（例如，电视塔、Wi-Fi
    AP和蜂窝基站）的WPT进行自供电。因此，基于传感器节点接收功率测量的信道估计是一种可行的解决方案，因为接收功率可以通过消耗极少电力的被动电路功率计测量。DRL可以利用从传感器节点获得的接收功率测量作为DRL的输入，以处理具有时间相关性的动态无线信道，从而高效地进行WPT的信道估计。
- en: VII-C2 DRL for Crowdsensing Service Optimization
  id: totrans-430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-C2 用于众包服务优化的DRL
- en: In MCS, mobile users contribute sensing data to a crowdsensing service provider
    and receive an incentive in return. However, due to limited resources, e.g., bandwidth
    and energy, the mobile user has to decide on whether and how much data to be uploaded
    to the provider. Likewise, the provider aiming to maximize its profit has to determine
    the amount of incentive to be given. The provider’s decision depends on the actions
    of the mobile users. For example, with many mobile users contributing data to
    the crowdsensing service provider, the provider can lower the incentive. Due to
    a large state space of a large number of users and dynamic environment, DRL can
    be applied to obtain an optimal crowdsensing policy similar to [[168](#bib.bib168)].
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在MCS中，移动用户将感知数据贡献给众包感知服务提供商，并获得相应的奖励。然而，由于资源有限，例如带宽和能量，移动用户必须决定是否以及上传多少数据给提供商。同样，旨在最大化利润的提供商必须确定奖励的金额。提供商的决策依赖于移动用户的行为。例如，当许多移动用户向众包感知服务提供商贡献数据时，提供商可以降低奖励。由于用户数量庞大和环境动态变化，DRL可以用于获得类似于 [[168](#bib.bib168)]的最佳众包感知策略。
- en: VII-C3 DRL for Cryptocurrency Management in Wireless Networks
  id: totrans-432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-C3 DRL在无线网络中的加密货币管理
- en: Pricing and economic models have been widely applied to wireless networks [[169](#bib.bib169)], [[170](#bib.bib170)].
    For example, wireless users pay money to access radio resources or mobile services.
    Alternatively, the users can receive money if they contribute to the networks,
    e.g., offering a relay or cache function. However, using real money and cash in
    such scenarios faces many issues related to accounting, security, and privacy.
    Recently, the concept of cryptocurrency based on the blockchain technology has
    been introduced and adopted in wireless networks, e.g., [[171](#bib.bib171)],
    which has been shown to be a secure and efficient solution. However, the value
    of cryptocurrency, i.e., token or coin, can be highly dynamic depending on many
    market factors. The wireless users possessing the tokens can decide to keep or
    spend the tokens, e.g., for radio resource access and service usage or exchange
    into real money. In the random cryptocurrency market environment, DRL can be applied
    to achieve the maximum long-term reward of the cryptocurrency management for wireless
    users as in [[172](#bib.bib172)].
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 定价和经济模型已广泛应用于无线网络 [[169](#bib.bib169)]， [[170](#bib.bib170)]。例如，无线用户支付费用以访问无线资源或移动服务。或者，用户可以通过贡献网络，例如提供中继或缓存功能，获得报酬。然而，在这种情况下使用真实货币会面临许多与会计、安全和隐私相关的问题。最近，基于区块链技术的加密货币概念已被引入并应用于无线网络，例如 [[171](#bib.bib171)]，这被证明是一种安全且高效的解决方案。然而，加密货币的价值，即代币或硬币，可能会因市场因素而高度动态。拥有代币的无线用户可以决定保留或花费代币，例如用于无线资源访问和服务使用或兑换成真实货币。在随机的加密货币市场环境中，DRL可以应用于实现无线用户加密货币管理的最大长期回报，如在 [[172](#bib.bib172)]中所示。
- en: VII-C4 DRL for Auction
  id: totrans-434
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VII-C4 DRL在拍卖中的应用
- en: An auction has been effectively used for radio resource management, e.g., spectrum
    allocation [[173](#bib.bib173)]. However, obtaining the solution of the auction,
    e.g., a winner determination problem, can be complicated and intractable when
    the number of participants, i.e., bidders and sellers, become very large. Such
    a scenario is typical in next-generation wireless networks such as 5G highly-dense
    heterogeneous networks. DRL appears to be an efficient approach for solving different
    types of auctions such as in [[174](#bib.bib174)].
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 拍卖已被有效用于无线资源管理，例如频谱分配 [[173](#bib.bib173)]。然而，当参与者数量，即竞标者和卖家，变得非常庞大时，拍卖的解决方案，例如赢家确定问题，可能变得复杂且难以处理。这种情况在下一代无线网络中，如5G高密度异构网络中非常典型。DRL似乎是一种解决不同类型拍卖的有效方法，如在 [[174](#bib.bib174)]中所示。
- en: VIII Conclusions
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 结论
- en: This paper has presented a comprehensive survey of the applications of deep
    reinforcement learning to communications and networking. First, we have presented
    an overview of reinforcement learning, deep learning, and deep reinforcement learning.
    Then, we have introduced various deep reinforcement learning techniques and their
    extensions. Afterwards, we have provided detailed reviews, analyses, and comparisons
    of the deep reinforcement learning to solve different issues in communications
    and networking. The issues include dynamic network access, data rate control,
    wireless caching, data offloading, network security, connectivity preservation,
    traffic routing, and data collection. Finally, we have outlined important challenges,
    open issues as well as future research directions.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文提供了深度强化学习在通信和网络领域应用的综合综述。首先，我们介绍了强化学习、深度学习和深度强化学习的概述。接着，我们介绍了各种深度强化学习技术及其扩展。随后，我们对深度强化学习在解决通信和网络中的不同问题方面进行了详细的评审、分析和比较。这些问题包括动态网络访问、数据速率控制、无线缓存、数据卸载、网络安全、连接性维护、流量路由和数据收集。最后，我们概述了重要的挑战、未解问题以及未来的研究方向。
- en: References
  id: totrans-438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. S. Sutton and A. G. Barto, *Reinforcement learning: An introduction*.   MIT
    press Cambridge, 1998.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. S. Sutton 和 A. G. Barto, *强化学习：导论*.  MIT press Cambridge, 1998.'
- en: '[2] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, *Deep learning*.   MIT
    press Cambridge, 2016.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] I. Goodfellow, Y. Bengio, A. Courville, 和 Y. Bengio, *深度学习*.  MIT press
    Cambridge, 2016.'
- en: '[3] (2016, Jan.) Google achieves ai “breakthrough” by beating go champion.
    BBC. [Online]. Available: https://www.bbc.com/news/technology-35420579'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] (2016年1月) Google通过击败围棋冠军实现AI“突破”。BBC. [在线]. 可用链接： https://www.bbc.com/news/technology-35420579'
- en: '[4] M. L. Puterman, *Markov decision processes: discrete stochastic dynamic
    programming*.   John Wiley & Sons, 2014.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. L. Puterman, *马尔可夫决策过程：离散随机动态规划*.  John Wiley & Sons, 2014.'
- en: '[5] D. P. Bertsekas, D. P. Bertsekas, D. P. Bertsekas, and D. P. Bertsekas,
    *Dynamic programming and optimal control*.   Athena scientific Belmont, MA, 2005,
    vol. 1, no. 3.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. P. Bertsekas, D. P. Bertsekas, D. P. Bertsekas, 和 D. P. Bertsekas, *动态规划与最优控制*.  Athena
    scientific Belmont, MA, 2005, 第1卷，第3期.'
- en: '[6] R. Bellman, *Dynamic programming*.   Mineola, NY: Courier Corporation,
    2013.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] R. Bellman, *动态规划*.  Mineola, NY: Courier Corporation, 2013.'
- en: '[7] Y. Li, “Deep reinforcement learning: An overview,” *arXiv preprint arXiv:1701.07274*,
    2017.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Li, “深度强化学习：概述,” *arXiv预印本 arXiv:1701.07274*, 2017.'
- en: '[8] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “A brief
    survey of deep reinforcement learning,” *IEEE Signal Processing Magazine*, vol. 34,
    no. 6, pp. 26–38, Nov. 2017.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] K. Arulkumaran, M. P. Deisenroth, M. Brundage, 和 A. A. Bharath, “深度强化学习的简要综述,”
    *IEEE信号处理杂志*, 第34卷，第6期, 第26–38页, 2017年11月.'
- en: '[9] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Machine learning
    for wireless networks with artificial intelligence: A tutorial on neural networks,”
    *arXiv preprint arXiv:1710.02913*, 2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] M. Chen, U. Challita, W. Saad, C. Yin, 和 M. Debbah, “人工智能下的无线网络中的机器学习：神经网络教程,”
    *arXiv预印本 arXiv:1710.02913*, 2017.'
- en: '[10] R. Lowe, Y. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I. Mordatch, “Multi-agent
    actor-critic for mixed cooperative-competitive environments,” in *Advances in
    Neural Information Processing Systems*, 2017, pp. 6379–6390.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] R. Lowe, Y. Wu, A. Tamar, J. Harb, O. P. Abbeel, 和 I. Mordatch, “多智能体演员-评论家在混合合作-竞争环境中的应用,”
    收录于 *神经信息处理系统进展*, 2017, 第6379–6390页.'
- en: '[11] G. E. Monahan, “State of the art-a survey of partially observable markov
    decision processes: theory, models, and algorithms,” *Management Science*, vol. 28,
    no. 1, pp. 1–16, 1982.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] G. E. Monahan, “前沿状态——部分可观察马尔可夫决策过程的综述：理论、模型与算法,” *管理科学*, 第28卷，第1期, 第1–16页,
    1982.'
- en: '[12] L. S. Shapley, “Stochastic games,” *Proceedings of the national academy
    of sciences*, vol. 39, no. 10, pp. 1095–1100, 1953.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] L. S. Shapley, “随机游戏,” *国家科学院学报*, 第39卷，第10期, 第1095–1100页, 1953.'
- en: '[13] J. Hu and M. P. Wellman, “Nash q-learning for general-sum stochastic games,”
    *Journal of machine learning research*, vol. 4, no. Nov, pp. 1039–1069, 2003.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Hu 和 M. P. Wellman, “针对一般和博弈博弈的纳什Q学习,” *机器学习研究杂志*, 第4卷，第11月期, 第1039–1069页,
    2003.'
- en: '[14] W. C. Dabney, “Adaptive step-sizes for reinforcement learning,” Ph.D.
    dissertation, 2014.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] W. C. Dabney, “强化学习的自适应步长,” 博士学位论文, 2014.'
- en: '[15] C. J. Watkins and P. Dayan, “Q-learning,” *Machine learning*, vol. 8,
    no. 3-4, pp. 279–292, 1992.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] C. J. Watkins 和 P. Dayan, “Q-learning,” *机器学习*, 第8卷，第3-4期, 第279–292页,
    1992.'
- en: '[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, p.
    529, 2015.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *等*，“通过深度强化学习实现人类水平的控制”，*自然*，第518卷，第7540期，第529页，2015年。'
- en: '[17] Y. Lin, X. Dai, L. Li, and F.-Y. Wang, “An efficient deep reinforcement
    learning model for urban traffic control,” *arXiv preprint arXiv:1808.01876*,
    2018.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Lin, X. Dai, L. Li, 和 F.-Y. Wang，“用于城市交通控制的高效深度强化学习模型”，*arXiv 预印本 arXiv:1808.01876*，2018年。'
- en: '[18] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement learning
    for robotic manipulation with asynchronous off-policy updates,” in *IEEE International
    Conference on Robotics and Automation (ICRA)*, 2017, pp. 3389–3396.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Gu, E. Holly, T. Lillicrap, 和 S. Levine，“用于机器人操作的深度强化学习与异步离策略更新”，发表于*IEEE国际机器人与自动化会议
    (ICRA)*，2017年，第3389-3396页。'
- en: '[19] S. Thrun and A. Schwartz, “Issues in using function approximation for
    reinforcement learning,” in *Proceedings of Connectionist Models Summer School
    Hillsdale, NJ. Lawrence Erlbaum*, 1993.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. Thrun 和 A. Schwartz，“使用函数逼近进行强化学习的问题”，发表于*连接主义模型暑期学校论文集，NJ. 劳伦斯·厄尔巴姆*，1993年。'
- en: '[20] H. V. Hasselt, “Double q-learning,” in *Advances in Neural Information
    Processing Systems*, 2010, pp. 2613–2621.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] H. V. Hasselt，“双重Q学习”，发表于*神经信息处理系统进展*，2010年，第2613-2621页。'
- en: '[21] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double q-learning.” in *AAAI*, vol. 2, Phoenix, AZ, Feb. 2016, pp. 2094–2100.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] H. Van Hasselt, A. Guez, 和 D. Silver，“使用双重Q学习的深度强化学习”，发表于*AAAI*，第2卷，凤凰城，AZ，2016年2月，第2094-2100页。'
- en: '[22] O. Naparstek and K. Cohen, “Deep multi-user reinforcement learning for
    dynamic spectrum access in multichannel wireless networks,” *arXiv preprint arXiv:1704.02613*,
    2017.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] O. Naparstek 和 K. Cohen，“多用户深度强化学习在多频道无线网络中的动态频谱访问”，*arXiv 预印本 arXiv:1704.02613*，2017年。'
- en: '[23] N. Zhao, Y.-C. Liang, D. Niyato, Y. Pei, M. Wu, and Y. Jiang, “Deep reinforcement
    learning for user association and resource allocation in heterogeneous networks,”
    in *IEEE GLOBECOM*, Abu Dhabi, UAE, Dec. 2018, pp. 1–6.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] N. Zhao, Y.-C. Liang, D. Niyato, Y. Pei, M. Wu, 和 Y. Jiang，“用于异构网络中的用户关联和资源分配的深度强化学习”，发表于*IEEE全球通信大会*，阿布扎比，阿联酋，2018年12月，第1-6页。'
- en: '[24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” *arXiv preprint arXiv:1511.05952*, 2015.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. Schaul, J. Quan, I. Antonoglou, 和 D. Silver，“优先经验回放”，*arXiv 预印本 arXiv:1511.05952*，2015年。'
- en: '[25] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas,
    “Dueling network architectures for deep reinforcement learning,” in *International
    Conference on Machine Learning*, New York, NY, Jun. 2016.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, 和 N. De Freitas，“用于深度强化学习的对抗网络架构”，发表于*国际机器学习会议*，纽约，NY，2016年6月。'
- en: '[26] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International Conference on Machine Learning*, New York City, New York, Jun.
    2016, pp. 1928–1937.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D.
    Silver, 和 K. Kavukcuoglu，“深度强化学习的异步方法”，发表于*国际机器学习会议*，纽约市，纽约，2016年6月，第1928-1937页。'
- en: '[27] Z. Wang, Y. Xu, L. Li, H. Tian, and S. Cui, “Handover control in wireless
    systems via asynchronous multi-user deep reinforcement learning,” *arXiv preprint
    arXiv:1801.02077*, 2018.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Z. Wang, Y. Xu, L. Li, H. Tian, 和 S. Cui，“通过异步多用户深度强化学习进行无线系统中的切换控制”，*arXiv
    预印本 arXiv:1801.02077*，2018年。'
- en: '[28] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective
    on reinforcement learning,” *arXiv preprint arXiv:1707.06887*, 2017.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] M. G. Bellemare, W. Dabney, 和 R. Munos，“对强化学习的分布视角”，*arXiv 预印本 arXiv:1707.06887*，2017年。'
- en: '[29] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih,
    R. Munos, D. Hassabis, O. Pietquin *et al.*, “Noisy networks for exploration,”
    in *International Conference on Learning Representations*, 2018.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V.
    Mnih, R. Munos, D. Hassabis, O. Pietquin *等*，“用于探索的噪声网络”，发表于*国际表示学习会议*，2018年。'
- en: '[30] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow: Combining improvements in
    deep reinforcement learning,” in *The Thirty-Second AAAI Conference on Artificial
    Intelligence*, 2018.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar, 和 D. Silver，“彩虹：结合深度强化学习中的改进”，发表于*第三十二届美国人工智能协会会议*，2018年。'
- en: '[31] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    San Juan, Puerto Rico, USA, May 2016.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, 和 D. Wierstra，“使用深度强化学习进行连续控制，” 圣胡安，波多黎各，美国，2016年5月。'
- en: '[32] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
    “Deterministic policy gradient algorithms,” in *ICML*, 2014.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, 和 M. Riedmiller，“确定性策略梯度算法，”
    在 *ICML*，2014年。'
- en: '[33] M. Hausknecht and P. Stone, “Deep recurrent q-learning for partially observable
    mdps,” *CoRR, abs/1507.06527*, vol. 7, no. 1, 2015.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] M. Hausknecht 和 P. Stone，“用于部分可观察马尔可夫决策过程的深度递归Q学习，” *CoRR, abs/1507.06527*，第7卷，第1期，2015年。'
- en: '[34] D. Zhao, H. Wang, K. Shao, and Y. Zhu, “Deep reinforcement learning with
    experience replay based on sarsa,” in *IEEE Symposium Series on Computational
    Intelligence (SSCI)*, 2016, pp. 1–6.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] D. Zhao, H. Wang, K. Shao, 和 Y. Zhu，“基于SARSA的深度强化学习与经验回放，” 在 *IEEE 计算智能研讨会系列
    (SSCI)*，2016年，第1–6页。'
- en: '[35] W. Wang, J. Hao, Y. Wang, and M. Taylor, “Towards cooperation in sequential
    prisoner’s dilemmas: a deep multiagent reinforcement learning approach,” *arXiv
    preprint arXiv:1803.00162*, 2018.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] W. Wang, J. Hao, Y. Wang, 和 M. Taylor，“在序列囚徒困境中的合作：一种深度多智能体强化学习方法，” *arXiv
    预印本 arXiv:1803.00162*，2018年。'
- en: '[36] J. Heinrich and D. Silver, “Deep reinforcement learning from self-play
    in imperfect-information games,” *arXiv preprint arXiv:1603.01121*, 2016.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Heinrich 和 D. Silver，“从自我博弈中学习的深度强化学习：在不完全信息游戏中的应用，” *arXiv 预印本 arXiv:1603.01121*，2016年。'
- en: '[37] D. Fooladivanda and C. Rosenberg, “Joint resource allocation and user
    association for heterogeneous wireless cellular networks,” *IEEE Transactions
    on Wireless Communications*, vol. 12, no. 1, pp. 248–257, 2013.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] D. Fooladivanda 和 C. Rosenberg，“异构无线蜂窝网络中的联合资源分配与用户关联，” *IEEE 无线通讯汇刊*，第12卷，第1期，第248–257页，2013年。'
- en: '[38] Y. Lin, W. Bao, W. Yu, and B. Liang, “Optimizing user association and
    spectrum allocation in hetnets: A utility perspective,” *IEEE Journal on Selected
    Areas in Communications*, vol. 33, no. 6, pp. 1025–1039, 2015.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Lin, W. Bao, W. Yu, 和 B. Liang，“在异构网络中优化用户关联和频谱分配：一种效用视角，” *IEEE 选定领域通讯杂志*，第33卷，第6期，第1025–1039页，2015年。'
- en: '[39] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, “Deep reinforcement
    learning for dynamic multichannel access,” in *International Conference on Computing,
    Networking and Communications (ICNC)*, 2017.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Wang, H. Liu, P. H. Gomes, 和 B. Krishnamachari，“动态多信道访问的深度强化学习，” 在
    *国际计算、网络与通信会议 (ICNC)*，2017年。'
- en: '[40] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    and M. A. Riedmiller, “Playing atari with deep reinforcement learning,” *CoRR*,
    vol. abs/1312.5602, 2013.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    和 M. A. Riedmiller，“用深度强化学习玩Atari游戏，” *CoRR*，第abs/1312.5602卷，2013年。'
- en: '[41] R. Govindan. Tutornet: A low power wireless iot testbed. [Online]. Available:
    http://anrg.usc.edu/www/tutornet/'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] R. Govindan. Tutornet: 一个低功耗无线物联网测试平台。 [在线]. 可用: http://anrg.usc.edu/www/tutornet/'
- en: '[42] Q. Zhao, B. Krishnamachari, and K. Liu, “On myopic sensing for multi-channel
    opportunistic access: structure, optimality, and performance,” *IEEE Transactions
    on Wireless Communications*, vol. 7, no. 12, pp. 5431–5440, December 2008.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Q. Zhao, B. Krishnamachari, 和 K. Liu，“关于多信道机会访问的短视感知：结构、最优性和性能，” *IEEE
    无线通讯汇刊*，第7卷，第12期，第5431–5440页，2008年12月。'
- en: '[43] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, “Deep reinforcement
    learning for dynamic multichannel access in wireless networks,” *IEEE Transactions
    on Cognitive Communications and Networking*, to appear.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] S. Wang, H. Liu, P. H. Gomes, 和 B. Krishnamachari，“用于无线网络中的动态多信道访问的深度强化学习，”
    *IEEE 认知通信与网络汇刊*，待发表。'
- en: '[44] J. Zhu, Y. Song, D. Jiang, and H. Song, “A new deep-q-learning-based transmission
    scheduling mechanism for the cognitive internet of things,” *IEEE Internet of
    Things Journal*, 2017.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] J. Zhu, Y. Song, D. Jiang, 和 H. Song，“一种基于深度Q学习的新型传输调度机制用于认知物联网，” *IEEE
    物联网杂志*，2017年。'
- en: '[45] M. Chu, H. Li, X. Liao, and S. Cui, “Reinforcement learning based multi-access
    control and battery prediction with energy harvesting in iot systems,” *arXiv
    preprint arXiv:1805.05929*, 2018.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] M. Chu, H. Li, X. Liao, 和 S. Cui，“基于强化学习的多访问控制与能量收集的电池预测在物联网系统中的应用，” *arXiv
    预印本 arXiv:1805.05929*，2018年。'
- en: '[46] H. Ye and G. Y. Li, “Deep reinforcement learning for resource allocation
    in v2v communications,” *arXiv preprint arXiv:1711.00968*, 2017.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] H. Ye 和 G. Y. Li，“用于V2V通信的资源分配的深度强化学习，” *arXiv 预印本 arXiv:1711.00968*，2017年。'
- en: '[47] U. Challita, L. Dong, and W. Saad, “Proactive resource management in lte-u
    systems: A deep learning perspective,” *arXiv preprint arXiv:1702.07031*, 2017.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] U. 查利塔，L. 董 和 W. 萨阿德，“在 LTE-U 系统中的前瞻性资源管理：深度学习视角，” *arXiv 预印本 arXiv:1702.07031*，2017
    年。'
- en: '[48] M. Balazinska and P. Castro. (2003) Ibm watson research center. [Online].
    Available: https://crawdad.org/ibm/watson/20030219'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] M. 巴拉辛斯卡 和 P. 卡斯特罗。（2003）IBM Watson 研究中心。[在线]. 可用: https://crawdad.org/ibm/watson/20030219'
- en: '[49] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas,
    “Dueling network architectures for deep reinforcement learning,” 2015.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Z. 王，T. 沙尔，M. 赫塞尔，H. 范·哈瑟尔特，M. 兰克托 和 N. 德·弗雷塔斯，“用于深度强化学习的对决网络架构，” 2015
    年。'
- en: '[50] H. Li, “Multiagent learning for aloha-like spectrum access in cognitive
    radio systems,” *EURASIP Journal on Wireless Communications and Networking*, vol.
    2010, no. 1, pp. 1–15, May 2010.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] H. 李，“在认知无线电系统中用于类似 ALOHA 的多智能体学习，” *EURASIP 无线通信与网络期刊*，第 2010 卷，第 1 期，第
    1–15 页，2010 年 5 月。'
- en: '[51] S. Liu, X. Hu, and W. Wang, “Deep reinforcement learning based dynamic
    channel allocation algorithm in multibeam satellite systems,” *IEEE ACCESS*, vol. 6,
    pp. 15 733–15 742, 2018.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] S. 刘，X. 胡 和 W. 王，“基于深度强化学习的多波束卫星系统动态信道分配算法，” *IEEE ACCESS*，第 6 卷，第 15 733–15 742
    页，2018 年。'
- en: '[52] M. Chen, W. Saad, and C. Yin, “Liquid state machine learning for resource
    allocation in a network of cache-enabled lte-u uavs,” in *IEEE GLOBECOM*, 2017,
    pp. 1–6.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. 陈，W. 萨阿德 和 C. 尹，“液态状态机器学习在缓存启用的 LTE-U UAV 网络中的资源分配，”收录于 *IEEE GLOBECOM*，2017
    年，第 1–6 页。'
- en: '[53] W. Maass, “Liquid state machines: motivation, theory, and applications,”
    in *Computability in context: computation and logic in the real world*.   World
    Scientific, 2011, pp. 275–296.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] W. 马斯，“液态状态机器：动机、理论和应用，”收录于 *Computability in context: computation and
    logic in the real world*。   世界科学出版社，2011 年，第 275–296 页。'
- en: '[54] M. Chen, M. Mozaffari, W. Saad, C. Yin, M. Debbah, and C. S. Hong, “Caching
    in the sky: Proactive deployment of cache-enabled unmanned aerial vehicles for
    optimized quality-of-experience,” *IEEE Journal on Selected Areas in Communications*,
    vol. 35, no. 5, pp. 1046–1061, 2017.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] M. 陈，M. 莫扎法里，W. 萨阿德，C. 尹，M. 德巴 和 C. S. 洪，“天上的缓存：前瞻性部署缓存启用的无人机以优化体验质量，”
    *IEEE 选定领域通信期刊*，第 35 卷，第 5 期，第 1046–1061 页，2017 年。'
- en: '[55] I. Szita, V. Gyenes, and A. Lőrincz, “Reinforcement learning with echo
    state networks,” in *International Conference on Artificial Neural Networks*.   Springer,
    2006, pp. 830–839.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] I. 斯基塔，V. 盖内斯 和 A. 洛伦茨，“带有回声状态网络的强化学习，”收录于 *国际人工神经网络会议*。   斯普林格，2006 年，第
    830–839 页。'
- en: '[56] Tyouku of china network video index. [Online]. Available: http://index.youku.com/'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 中国网络视频指数 Tyouku。[在线]. 可用: http://index.youku.com/'
- en: '[57] T. Stockhammer, “Dynamic adaptive streaming over http–: standards and
    design principles,” in *Proceedings of the second annual ACM conference on Multimedia
    systems*.   ACM, 2011, pp. 133–144.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] T. 斯托克哈默，“HTTP 动态自适应流媒体：标准和设计原则，”收录于 *第二届 ACM 多媒体系统会议论文集*。   ACM，2011
    年，第 133–144 页。'
- en: '[58] M. Gadaleta, F. Chiariotti, M. Rossi, and A. Zanella, “D-dash: A deep
    q-learning framework for dash video streaming,” *IEEE Transactions on Cognitive
    Communications and Networking*, vol. 3, no. 4, pp. 703–718, 2017.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] M. 加达莱塔，F. 基亚里奥蒂，M. 罗西 和 A. 扎内拉，“D-dash：用于 DASH 视频流媒体的深度 Q 学习框架，” *IEEE
    认知通信与网络期刊*，第 3 卷，第 4 期，第 703–718 页，2017 年。'
- en: '[59] J. Klaue, B. Rathke, and A. Wolisz, “Evalvid–a framework for video transmission
    and quality evaluation,” in *International conference on modelling techniques
    and tools for computer performance evaluation*, 2003, pp. 255–272.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] J. 克劳，B. 拉斯克 和 A. 沃利斯，“Evalvid–一个视频传输和质量评估框架，”收录于 *计算机性能评估建模技术与工具国际会议*，2003
    年，第 255–272 页。'
- en: '[60] H. Mao, R. Netravali, and M. Alizadeh, “Neural adaptive video streaming
    with pensieve,” in *Proceedings of the Conference of the ACM Special Interest
    Group on Data Communication*.   ACM, 2017, pp. 197–210.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] H. 毛，R. 内特拉瓦利 和 M. 阿利扎德，“带有 Pensieve 的神经自适应视频流媒体，”收录于 *ACM 特别兴趣小组数据通信会议论文集*。   ACM，2017
    年，第 197–210 页。'
- en: '[61] H. Riiser, P. Vigmostad, C. Griwodz, and P. Halvorsen, “Commute path bandwidth
    traces from 3g networks: analysis and applications,” in *Proceedings of the 4th
    ACM Multimedia Systems Conference*.   ACM, 2013, pp. 114–118.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] H. 里瑟，P. 维格莫斯塔德，C. 格里沃兹 和 P. 哈尔沃森，“来自 3G 网络的通勤路径带宽轨迹：分析和应用，”收录于 *第 4 届
    ACM 多媒体系统会议论文集*。   ACM，2013 年，第 114–118 页。'
- en: '[62] X. Yin, A. Jindal, V. Sekar, and B. Sinopoli, “A control-theoretic approach
    for dynamic adaptive video streaming over http,” in *ACM SIGCOMM Computer Communication
    Review*, vol. 45, no. 4.   ACM, 2015, pp. 325–338.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] X. Yin, A. Jindal, V. Sekar, 和 B. Sinopoli, “基于控制论的动态自适应 HTTP 视频流传输,”
    in *ACM SIGCOMM Computer Communication Review*, vol. 45, no. 4.   ACM, 2015, pp.
    325–338.'
- en: '[63] T. Huang, R.-X. Zhang, C. Zhou, and L. Sun, “Qarc: Video quality aware
    rate control for real-time video streaming based on deep reinforcement learning,”
    *arXiv preprint arXiv:1805.02482*, 2018.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] T. Huang, R.-X. Zhang, C. Zhou, 和 L. Sun, “Qarc: 基于深度强化学习的实时视频流视频质量感知率控制,”
    *arXiv preprint arXiv:1805.02482*, 2018.'
- en: '[64] (2016) Measuring fixed broadband report. [Online]. Available: https://www.fcc.gov/reports-research/reports/measuring-broadband-america/raw-data-measuring-broadband-america-2016'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] (2016) 定宽带报告. [在线]. 可访问: https://www.fcc.gov/reports-research/reports/measuring-broadband-america/raw-data-measuring-broadband-america-2016'
- en: '[65] S. Chinchali, P. Hu, T. Chu, M. Sharma, M. Bansal, R. Misra, M. Pavone,
    and K. Sachin, “Cellular network traffic scheduling with deep reinforcement learning,”
    in *National Conference on Artificial Intelligence (AAAI)*, 2018.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. Chinchali, P. Hu, T. Chu, M. Sharma, M. Bansal, R. Misra, M. Pavone,
    和 K. Sachin, “使用深度强化学习的蜂窝网络流量调度,” in *National Conference on Artificial Intelligence
    (AAAI)*, 2018.'
- en: '[66] Z. Zhang, Y. Zheng, M. Hua, Y. Huang, and L. Yang, “Cache-enabled dynamic
    rate allocation via deep self-transfer reinforcement learning,” *arXiv preprint
    arXiv:1803.11334*, 2018.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Z. Zhang, Y. Zheng, M. Hua, Y. Huang, 和 L. Yang, “通过深度自传输强化学习实现缓存启用的动态速率分配,”
    *arXiv preprint arXiv:1803.11334*, 2018.'
- en: '[67] P. V. R. Ferreira, R. Paffenroth, A. M. Wyglinski, T. M. Hackett, S. G.
    Bilén, R. C. Reinhart, and D. J. Mortensen, “Multi-objective reinforcement learning
    for cognitive satellite communications using deep neural network ensembles,” *IEEE
    Journal on Selected Areas in Communications*, 2018.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] P. V. R. Ferreira, R. Paffenroth, A. M. Wyglinski, T. M. Hackett, S. G.
    Bilén, R. C. Reinhart, 和 D. J. Mortensen, “使用深度神经网络簇的多目标强化学习在认知卫星通信中的应用,” *IEEE
    Journal on Selected Areas in Communications*, 2018.'
- en: '[68] D. Tarchi, G. E. Corazza, and A. Vanelli-Coralli, “Adaptive coding and
    modulation techniques for next generation hand-held mobile satellite communications,”
    in *IEE ICC*, 2013, pp. 4504–4508.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] D. Tarchi, G. E. Corazza, 和 A. Vanelli-Coralli, “面向下一代手持移动卫星通信的自适应编码和调制技术,”
    in *IEE ICC*, 2013, pp. 4504–4508.'
- en: '[69] M. T. Hagan and M. B. Menhaj, “Training feedforward networks with the
    marquardt algorithm,” *IEEE transactions on Neural Networks*, vol. 5, no. 6, pp.
    989–993, 1994.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M. T. Hagan 和 M. B. Menhaj, “使用 Marquardt 算法训练前馈神经网络,” *IEEE transactions
    on Neural Networks*, vol. 5, no. 6, pp. 989–993, 1994.'
- en: '[70] C. Zhong, M. C. Gursoy, and S. Velipasalar, “A deep reinforcement learning-based
    framework for content caching,” *arXiv preprint arXiv:1712.08132*, 2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] C. Zhong, M. C. Gursoy, 和 S. Velipasalar, “基于深度强化学习的内容缓存框架,” *arXiv preprint
    arXiv:1712.08132*, 2017.'
- en: '[71] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    *CoRR*, vol. abs/1509.02971, 2015\. [Online]. Available: http://arxiv.org/abs/1509.02971'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, 和 D. Wierstra, “使用深度强化学习进行连续控制,” *CoRR*, vol. abs/1509.02971, 2015\.
    [在线]. 可访问: http://arxiv.org/abs/1509.02971'
- en: '[72] G. Dulac-Arnold, R. Evans, P. Sunehag, and B. Coppin, “Reinforcement learning
    in large discrete action spaces,” *CoRR*, vol. abs/1512.07679, 2015\. [Online].
    Available: http://arxiv.org/abs/1512.07679'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] G. Dulac-Arnold, R. Evans, P. Sunehag, 和 B. Coppin, “大离散动作空间中的强化学习,” *CoRR*,
    vol. abs/1512.07679, 2015\. [在线]. 可访问: http://arxiv.org/abs/1512.07679'
- en: '[73] L. Lei, L. You, G. Dai, T. X. Vu, D. Yuan, and S. Chatzinotas, “A deep
    learning approach for optimizing content delivering in cache-enabled HetNet,”
    in *Int’l Sym. Wireless Commun. Systems (ISWCS)*, Aug. 2017, pp. 449–453.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] L. Lei, L. You, G. Dai, T. X. Vu, D. Yuan, 和 S. Chatzinotas, “一种用于缓存启用的多接入网络内容传递优化的深度学习方法,”
    in *Int’l Sym. Wireless Commun. Systems (ISWCS)*, Aug. 2017, pp. 449–453.'
- en: '[74] M. Schaarschmidt, F. Gessert, V. Dalibard, and E. Yoneki, “Learning runtime
    parameters in computer systems with delayed experience injection,” *arXiv preprint
    arXiv:1610.09903*, 2016.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] M. Schaarschmidt, F. Gessert, V. Dalibard, 和 E. Yoneki, “使用延迟经验注入学习计算系统的运行时参数,”
    *arXiv preprint arXiv:1610.09903*, 2016.'
- en: '[75] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears, “Benchmarking
    cloud serving systems with YCSB,” in *proc. 1st ACM Sym. Cloud Comput.*, 2010,
    pp. 143–154.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, 和 R. Sears, “使用
    YCSB 来评估云服务系统,” in *proc. 1st ACM Sym. Cloud Comput.*, 2010, pp. 143–154.'
- en: '[76] Y. He and S. Hu, “Cache-enabled wireless networks with opportunistic interference
    alignment,” *arXiv preprint arXiv:1706.09024*, 2017.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. He 和 S. Hu, “缓存启用的无线网络与机会干扰对齐，” *arXiv 预印本 arXiv:1706.09024*，2017。'
- en: '[77] Y. He, C. Liang, F. R. Yu, N. Zhao, and H. Yin, “Optimization of cache-enabled
    opportunistic interference alignment wireless networks: A big data deep reinforcement
    learning approach,” in *IEEE ICC*, 2017, pp. 1–6.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Y. He, C. Liang, F. R. Yu, N. Zhao, 和 H. Yin, “缓存启用机会干扰对齐无线网络优化：一种大数据深度强化学习方法，”
    在 *IEEE ICC*，2017，页码 1–6。'
- en: '[78] Y. He, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, V. C. Leung, and Y. Zhang,
    “Deep-reinforcement-learning-based optimization for cache-enabled opportunistic
    interference alignment wireless networks,” *IEEE Transactions on Vehicular Technology*,
    vol. 66, no. 11, pp. 10 433–10 445, 2017.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Y. He, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, V. C. Leung, 和 Y. Zhang, “基于深度强化学习的缓存启用机会干扰对齐无线网络优化，”
    *IEEE 车辆技术汇刊*，第 66 卷，第 11 期，页码 10 433–10 445，2017。'
- en: '[79] X. He, K. Wang, H. Huang, T. Miyazaki, Y. Wang, and S. Guo, “Green resource
    allocation based on deep reinforcement learning in content-centric iot,” *IEEE
    Transactions on Emerging Topics in Computing*, to appear.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] X. He, K. Wang, H. Huang, T. Miyazaki, Y. Wang, 和 S. Guo, “基于深度强化学习的内容中心物联网中的绿色资源分配，”
    *IEEE 计算新兴话题汇刊*，待刊。'
- en: '[80] Q. Wu, Z. Li, and G. Xie, “CodingCache: Multipath-aware CCN cache with
    network coding,” in *proc. ACM SIGCOMM Workshop on Information-centric Networking*,
    2013, pp. 41–42.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Q. Wu, Z. Li, 和 G. Xie, “CodingCache：具有网络编码的多路径感知 CCN 缓存，” 在 *ACM SIGCOMM
    信息中心网络研讨会论文集*，2013，页码 41–42。'
- en: '[81] M. Chen, W. Saad, and C. Yin, “Echo-liquid state deep learning for 360
    content transmission and caching in wireless vr networks with cellular-connected
    uavs,” *arXiv preprint arXiv:1804.03284*, 2018.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Chen, W. Saad, 和 C. Yin, “用于无线 VR 网络中的 360 内容传输和缓存的回声液态深度学习，结合蜂窝连接的无人机，”
    *arXiv 预印本 arXiv:1804.03284*，2018。'
- en: '[82] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Machine learning
    for wireless networks with artificial intelligence: A tutorial on neural networks,”
    *CoRR*, vol. abs/1710.02913, 2017\. [Online]. Available: http://arxiv.org/abs/1710.02913'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] M. Chen, U. Challita, W. Saad, C. Yin, 和 M. Debbah, “人工智能下的无线网络机器学习：神经网络教程，”
    *CoRR*，第 abs/1710.02913 卷，2017。 [在线]. 可用： http://arxiv.org/abs/1710.02913'
- en: '[83] M. Chen, W. Saad, and C. Yin, “Liquid state machine learning for resource
    allocation in a network of cache-enabled LTE-U UAVs,” in *IEEE GLOBECOM*, Dec.
    2017.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] M. Chen, W. Saad, 和 C. Yin, “用于缓存启用 LTE-U 无人机网络资源分配的液态状态机器学习，” 在 *IEEE
    GLOBECOM*，2017 年 12 月。'
- en: '[84] Y. He, Z. Zhang, and Y. Zhang, “A big data deep reinforcement learning
    approach to next generation green wireless networks,” in *IEEE GLOBECOM*, 2017,
    pp. 1–6.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. He, Z. Zhang, 和 Y. Zhang, “面向下一代绿色无线网络的大数据深度强化学习方法，” 在 *IEEE GLOBECOM*，2017，页码
    1–6。'
- en: '[85] Y. He, C. Liang, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, and Y. Zhang, “Resource
    allocation in software-defined and information-centric vehicular networks with
    mobile edge computing,” in *IEEE Vehicular Technology Conference*, 2017, pp. 1–5.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Y. He, C. Liang, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, 和 Y. Zhang, “结合移动边缘计算的软件定义和信息中心车载网络中的资源分配，”
    在 *IEEE 车辆技术会议*，2017，页码 1–5。'
- en: '[86] Y. He, F. R. Yu, N. Zhao, H. Yin, and A. Boukerche, “Deep reinforcement
    learning (drl)-based resource management in software-defined and virtualized vehicular
    ad hoc networks,” in *Proceedings of the 6th ACM Symposium on Development and
    Analysis of Intelligent Vehicular Networks and Applications*, 2017, pp. 47–54.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Y. He, F. R. Yu, N. Zhao, H. Yin, 和 A. Boukerche, “基于深度强化学习 (DRL) 的软件定义和虚拟化车载自组网资源管理，”
    在 *第六届 ACM 智能车载网络与应用开发与分析研讨会论文集*，2017，页码 47–54。'
- en: '[87] Y. He, N. Zhao, and H. Yin, “Integrated networking, caching, and computing
    for connected vehicles: A deep reinforcement learning approach,” *IEEE Transactions
    on Vehicular Technology*, vol. 67, no. 1, pp. 44–55, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Y. He, N. Zhao, 和 H. Yin, “连接车辆的网络、缓存和计算一体化：一种深度强化学习方法，” *IEEE 车辆技术汇刊*，第
    67 卷，第 1 期，页码 44–55，2018。'
- en: '[88] T. L. Thanh and R. Q. Hu, “Mobility-aware edge caching and computing framework
    in vehicle networks: A deep reinforcement learning,” *IEEE Transactions on Vehicular
    Technology*, to appear.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] T. L. Thanh 和 R. Q. Hu, “车载网络中的移动感知边缘缓存和计算框架：一种深度强化学习方法，” *IEEE 车辆技术汇刊*，待刊。'
- en: '[89] Y. He, F. R. Yu, N. Zhao, V. C. Leung, and H. Yin, “Software-defined networks
    with mobile edge computing and caching for smart cities: A big data deep reinforcement
    learning approach,” *IEEE Communications Magazine*, vol. 55, no. 12, pp. 31–37,
    2017.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Y. He, F. R. Yu, N. Zhao, V. C. Leung, 和 H. Yin，“具有移动边缘计算和缓存的智能城市中的软件定义网络：一种大数据深度强化学习方法，”
    *IEEE Communications Magazine*，第 55 卷，第 12 期，第 31–37 页，2017。'
- en: '[90] B. Han, V. Gopalakrishnan, L. Ji, and S. Lee, “Network function virtualization:
    Challenges and opportunities for innovations,” *IEEE Communications Magazine*,
    vol. 53, no. 2, pp. 90–97, 2015.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] B. Han, V. Gopalakrishnan, L. Ji, 和 S. Lee，“网络功能虚拟化：创新的挑战和机遇，” *IEEE Communications
    Magazine*，第 53 卷，第 2 期，第 90–97 页，2015。'
- en: '[91] Y. He, F. R. Yu, N. Zhao, and H. Yin, “Secure social networks in 5g systems
    with mobile edge computing, caching and device-to-device (d2d) communications,”
    *IEEE Wireless Communications*, vol. 25, no. 3, pp. 103–109, Jun. 2018.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Y. He, F. R. Yu, N. Zhao, 和 H. Yin，“在 5G 系统中具有移动边缘计算、缓存和设备对设备 (d2d) 通信的安全社交网络，”
    *IEEE Wireless Communications*，第 25 卷，第 3 期，第 103–109 页，2018年6月。'
- en: '[92] Y. He, C. Liang, F. R. Yu, and Z. Han, “Trust-based social networks with
    computing, caching and communications: A deep reinforcement learning approach,”
    *IEEE Transactions on Network Science and Engineering*, to appear.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Y. He, C. Liang, F. R. Yu, 和 Z. Han，“具有计算、缓存和通信的基于信任的社交网络：一种深度强化学习方法，”
    *IEEE Transactions on Network Science and Engineering*，待出版。'
- en: '[93] C. Zhang, Z. Liu, B. Gu, K. Yamori, and Y. Tanaka, “A deep reinforcement
    learning based approach for cost-and energy-aware multi-flow mobile data offloading,”
    *IEICE Transactions on Communications*, pp. 2017–2025.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] C. Zhang, Z. Liu, B. Gu, K. Yamori, 和 Y. Tanaka，“一种基于深度强化学习的成本和能源感知多流移动数据卸载方法，”
    *IEICE Transactions on Communications*，第 2017–2025 页。'
- en: '[94] L. Ji, G. Hui, L. Tiejun, and L. Yueming, “Deep reinforcement learning
    based computation offloading and resource allocation for mec,” in *IEEE WCNC*,
    2018, pp. 1–5.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] L. Ji, G. Hui, L. Tiejun, 和 L. Yueming，“基于深度强化学习的 MEC 计算卸载和资源分配，” 在 *IEEE
    WCNC*，2018，第 1–5 页。'
- en: '[95] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Performance optimization
    in mobile-edge computing via deep reinforcement learning,” *arXiv preprint arXiv:1804.00514*,
    2018.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, 和 M. Bennis，“通过深度强化学习优化移动边缘计算中的性能，”
    *arXiv preprint arXiv:1804.00514*，2018。'
- en: '[96] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Optimized computation
    offloading performance in virtual edge computing systems via deep reinforcement
    learning,” *arXiv preprint arXiv:1805.06146*, 2018.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, 和 M. Bennis，“通过深度强化学习优化虚拟边缘计算系统中的计算卸载性能，”
    *arXiv preprint arXiv:1805.06146*，2018。'
- en: '[97] J. Ye and Y.-J. A. Zhang, “DRAG: Deep reinforcement learning based base
    station activation in heterogeneous networks,” *arXiv:1809.02159*, Sep. 2018.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. Ye 和 Y.-J. A. Zhang，“DRAG：基于深度强化学习的异构网络基站激活，” *arXiv:1809.02159*，2018年9月。'
- en: '[98] H. Li, H. Gao, T. Lv, and Y. Lu, “Deep q-learning based dynamic resource
    allocation for self-powered ultra-dense networks,” in *IEEE ICC (ICC Workshops)*,
    2018, pp. 1–6.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] H. Li, H. Gao, T. Lv, 和 Y. Lu，“基于深度 Q 学习的自供电超密集网络的动态资源分配，” 在 *IEEE ICC
    (ICC Workshops)*，2018，第 1–6 页。'
- en: '[99] J. Liu, B. Krishnamachari, S. Zhou, and Z. Niu, “Deepnap: Data-driven
    base station sleeping operations through deep reinforcement learning,” *IEEE Internet
    of Things Journal*, 2018.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] J. Liu, B. Krishnamachari, S. Zhou, 和 Z. Niu，“Deepnap：通过深度强化学习驱动的数据驱动基站休眠操作，”
    *IEEE Internet of Things Journal*，2018。'
- en: '[100] X. Wan, G. Sheng, Y. Li, L. Xiao, and X. Du, “Reinforcement learning
    based mobile offloading for cloud-based malware detection,” in *IEEE GLOBECOM*,
    2017, pp. 1–6.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Wan, G. Sheng, Y. Li, L. Xiao, 和 X. Du，“基于强化学习的移动卸载用于云端恶意软件检测，” 在
    *IEEE GLOBECOM*，2017，第 1–6 页。'
- en: '[101] L. Xiao, X. Wan, C. Dai, X. Du, X. Chen, and M. Guizani, “Security in
    mobile edge caching with reinforcement learning,” *CoRR*, vol. abs/1801.05915,
    2018\. [Online]. Available: http://arxiv.org/abs/1801.05915'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] L. Xiao, X. Wan, C. Dai, X. Du, X. Chen, 和 M. Guizani，“基于强化学习的移动边缘缓存安全性，”
    *CoRR*，第 abs/1801.05915 卷，2018。 [在线] 可用： http://arxiv.org/abs/1801.05915'
- en: '[102] A. S. Shamili, C. Bauckhage, and T. Alpcan, “Malware detection on mobile
    devices using distributed machine learning,” in *proc. Int’l Conf. Pattern Recognition*,
    Aug. 2010, pp. 4348–4351.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] A. S. Shamili, C. Bauckhage, 和 T. Alpcan，“使用分布式机器学习的移动设备恶意软件检测，” 在 *proc.
    Int’l Conf. Pattern Recognition*，2010年8月，第 4348–4351 页。'
- en: '[103] Y. Li, J. Liu, Q. Li, and L. Xiao, “Mobile cloud offloading for malware
    detections with learning,” in *IEEE INFOCOM Workshops*, Apr. 2015, pp. 197–201.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y. Li, J. Liu, Q. Li, 和 L. Xiao，“具有学习的移动云卸载用于恶意软件检测，” 在 *IEEE INFOCOM
    Workshops*，2015年4月，第 197–201 页。'
- en: '[104] M. Min, D. Xu, L. Xiao, Y. Tang, and D. Wu, “Learning-based computation
    offloading for iot devices with energy harvesting,” *arXiv preprint arXiv:1712.08768*,
    2017.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] M. Min, D. Xu, L. Xiao, Y. Tang, 和 D. Wu, “基于学习的计算卸载用于具有能量收集的物联网设备，”
    *arXiv预印本 arXiv:1712.08768*，2017年。'
- en: '[105] L. Quan, Z. Wang, and F. Ren, “A novel two-layered reinforcement learning
    for task offloading with tradeoff between physical machine utilization rate and
    delay,” *Future Internet*, vol. 10, no. 7, 2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] L. Quan, Z. Wang, 和 F. Ren, “一种新颖的双层强化学习用于任务卸载，权衡物理机器利用率和延迟，” *未来互联网*，第10卷，第7期，2018年。'
- en: '[106] D. V. Le and C. Tham, “Quality of service aware computation offloading
    in an ad-hoc mobile cloud,” *IEEE Transactions on Vehicular Technology*, pp. 1–1,
    2018.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] D. V. Le 和 C. Tham, “自适应移动云中的服务质量感知计算卸载，” *IEEE车辆技术汇刊*，页码1–1，2018年。'
- en: '[107] D. V. Le and C.-K. Tham, “A deep reinforcement learning based offloading
    scheme in ad-hoc mobile clouds,” in *Proceedings of IEEE INFOCOM IECCO Workshop*,
    Honolulu, USA., apr 2018.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] D. V. Le 和 C.-K. Tham, “基于深度强化学习的卸载方案在自组移动云中的应用，” 见 *IEEE INFOCOM IECCO研讨会论文集*，美国檀香山，2018年4月。'
- en: '[108] S. Yu, X. Wang, and R. Langar, “Computation offloading for mobile edge
    computing: A deep learning approach,” in *IEEE PIMRC*, Oct. 2017.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] S. Yu, X. Wang, 和 R. Langar, “移动边缘计算的计算卸载：一种深度学习方法，” 见 *IEEE PIMRC*，2017年10月。'
- en: '[109] Z. Tang, X. Zhou, F. Zhang, W. Jia, and W. Zhao, “Migration modeling
    and learning algorithms for containers in fog computing,” *IEEE Transactions on
    Services Computing*, 2018.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Z. Tang, X. Zhou, F. Zhang, W. Jia, 和 W. Zhao, “雾计算中容器的迁移建模与学习算法，” *IEEE服务计算汇刊*，2018年。'
- en: '[110] P. Popovski, H. Yomo, and R. Prasad, “Strategies for adaptive frequency
    hopping in the unlicensed bands,” *IEEE Wireless Communications*, vol. 13, no. 6,
    pp. 60–67, 2006.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] P. Popovski, H. Yomo, 和 R. Prasad, “在非授权频段中的自适应跳频策略，” *IEEE无线通讯*，第13卷，第6期，页码60–67，2006年。'
- en: '[111] G. Han, L. Xiao, and H. V. Poor, “Two-dimensional anti-jamming communication
    based on deep reinforcement learning,” in *Proceedings of the 42nd IEEE International
    Conference on Acoustics, Speech and Signal Processing,*, 2017.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] G. Han, L. Xiao, 和 H. V. Poor, “基于深度强化学习的二维抗干扰通信，” 见 *第42届IEEE国际声学、语音与信号处理会议论文集*，2017年。'
- en: '[112] L. Xiao, D. Jiang, X. Wan, W. Su, and Y. Tang, “Anti-jamming underwater
    transmission with mobility and learning,” *IEEE Communications Letters*, 2018.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] L. Xiao, D. Jiang, X. Wan, W. Su, 和 Y. Tang, “具有移动性和学习的抗干扰水下传输，” *IEEE通讯快报*，2018年。'
- en: '[113] X. Liu, Y. Xu, L. Jia, Q. Wu, and A. Anpalagan, “Anti-jamming communications
    using spectrum waterfall: A deep reinforcement learning approach,” *IEEE Communications
    Letters*, 2018.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] X. Liu, Y. Xu, L. Jia, Q. Wu, 和 A. Anpalagan, “使用频谱瀑布的抗干扰通信：一种深度强化学习方法，”
    *IEEE通讯快报*，2018年。'
- en: '[114] Y. Chen, Y. Li, D. Xu, and L. Xiao, “Dqn-based power control for iot
    transmission against jamming,” in *IEEE 87th Vehicular Technology Conference (VTC
    Spring)*, 2018, pp. 1–5.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Y. Chen, Y. Li, D. Xu, 和 L. Xiao, “基于DQN的物联网传输抗干扰功率控制，” 见 *IEEE第87届车辆技术会议（VTC春季）*，2018年，页码1–5。'
- en: '[115] X. Lu, L. Xiao, and C. Dai, “Uav-aided 5g communications with deep reinforcement
    learning against jamming,” *arXiv preprint arXiv:1805.06628*, 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] X. Lu, L. Xiao, 和 C. Dai, “基于深度强化学习的无人机辅助5G通信抗干扰，” *arXiv预印本 arXiv:1805.06628*，2018年。'
- en: '[116] L. Xiao, X. Lu, D. Xu, Y. Tang, L. Wang, and W. Zhuang, “Uav relay in
    vanets against smart jamming with reinforcement learning,” *IEEE Transactions
    on Vehicular Technology*, vol. 67, no. 5, pp. 4087–4097, 2018.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] L. Xiao, X. Lu, D. Xu, Y. Tang, L. Wang, 和 W. Zhuang, “无人机中继在车载自组网中的抗智能干扰的强化学习，”
    *IEEE车辆技术汇刊*，第67卷，第5期，页码4087–4097，2018年。'
- en: '[117] S. Lv, L. Xiao, Q. Hu, X. Wang, C. Hu, and L. Sun, “Anti-jamming power
    control game in unmanned aerial vehicle networks,” in *IEEE GLOBECOM*, 2017, pp.
    1–6.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] S. Lv, L. Xiao, Q. Hu, X. Wang, C. Hu, 和 L. Sun, “无人机网络中的抗干扰功率控制博弈，”
    见 *IEEE GLOBECOM*，2017年，页码1–6。'
- en: '[118] L. Xiao, C. Xie, M. Min, and W. Zhuang, “User-centric view of unmanned
    aerial vehicle transmission against smart attacks,” *IEEE Transactions on Vehicular
    Technology*, vol. 67, no. 4, pp. 3420–3430, 2018.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] L. Xiao, C. Xie, M. Min, 和 W. Zhuang, “针对智能攻击的用户中心视角无人机传输，” *IEEE车辆技术汇刊*，第67卷，第4期，页码3420–3430，2018年。'
- en: '[119] M. Bowling and M. Veloso, “Multiagent learning using a variable learning
    rate,” *Artificial Intelligence*, vol. 136, no. 2, pp. 215–250, 2002.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] M. Bowling 和 M. Veloso, “使用可变学习率的多智能体学习，” *人工智能*，第136卷，第2期，页码215–250，2002年。'
- en: '[120] Y. Chen, S. Kar, and J. M. Moura, “Cyber-physical attacks with control
    objectives,” *IEEE Transactions on Automatic Control*, vol. 63, no. 5, pp. 1418–1425,
    2018.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Y. Chen, S. Kar, 和 J. M. Moura，“具有控制目标的网络物理攻击”，*IEEE 自动控制汇刊*，第63卷，第5期，第1418–1425页，2018年。'
- en: '[121] A. Ferdowsi, U. Challita, W. Saad, and N. B. Mandayam, “Robust deep reinforcement
    learning for security and safety in autonomous vehicle systems,” *arXiv preprint
    arXiv:1805.00983*, 2018.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] A. Ferdowsi, U. Challita, W. Saad, 和 N. B. Mandayam，“用于自主车辆系统中的安全和防护的鲁棒深度强化学习”，*arXiv
    预印本 arXiv:1805.00983*，2018年。'
- en: '[122] M. Brackstone and M. McDonald, “Car-following: a historical review,”
    *Transportation Research Part F: Traffic Psychology and Behaviour*, vol. 2, no. 4,
    pp. 181–196, 1999.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] M. Brackstone 和 M. McDonald，“跟车行为：历史回顾”，*交通研究 F 部分：交通心理学与行为*，第2卷，第4期，第181–196页，1999年。'
- en: '[123] A. Ferdowsi and W. Saad, “Deep learning-based dynamic watermarking for
    secure signal authentication in the internet of things,” in *IEEE ICC*, 2018,
    pp. 1–6.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] A. Ferdowsi 和 W. Saad，“基于深度学习的动态水印技术用于物联网中的安全信号认证”，发表于 *IEEE ICC*，2018年，第1–6页。'
- en: '[124] B. Satchidanandan and P. R. Kumar, “Dynamic watermarking: Active defense
    of networked cyber–physical systems,” *Proceedings of the IEEE*, vol. 105, no. 2,
    pp. 219–240, 2017.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] B. Satchidanandan 和 P. R. Kumar，“动态水印：网络化网络物理系统的主动防御”，*IEEE 汇刊*，第105卷，第2期，第219–240页，2017年。'
- en: '[125] A. Ferdowsi and W. Saad, “Deep learning for signal authentication and
    security in massive internet of things systems,” *arXiv preprint arXiv:1803.00916*,
    2018.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] A. Ferdowsi 和 W. Saad，“用于大规模物联网系统的信号认证和安全的深度学习”，*arXiv 预印本 arXiv:1803.00916*，2018年。'
- en: '[126] P. Vadakkepat, K. C. Tan, and W. Ming-Liang, “Evolutionary artificial
    potential fields and their application in real time robot path planning,” in *Proceedings
    of the 2000 Congress on Evolutionary Computation*, vol. 1, 2000, pp. 256–263.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] P. Vadakkepat, K. C. Tan, 和 W. Ming-Liang，“进化人工势场及其在实时机器人路径规划中的应用”，发表于
    *2000年进化计算大会论文集*，第1卷，2000年，第256–263页。'
- en: '[127] W. Huang, Y. Wang, and X. Yi, “Deep q-learning to preserve connectivity
    in multi-robot systems,” in *Proceedings of the 9th International Conference on
    Signal Processing Systems*.   ACM, 2017, pp. 45–50.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] W. Huang, Y. Wang, 和 X. Yi，“深度 Q 学习在多机器人系统中保持连通性”，发表于 *第九届国际信号处理系统会议论文集*。ACM，2017年，第45–50页。'
- en: '[128] W. Huang, Y. Wang, and X. Yi, “A deep reinforcement learning approach
    to preserve connectivity for multi-robot systems,” in *International Congress
    on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)*,
    2017, pp. 1–7.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] W. Huang, Y. Wang, 和 X. Yi，“一种深度强化学习方法用于多机器人系统中的连通性保持”，发表于 *国际图像与信号处理、生物医学工程与信息学大会
    (CISP-BMEI)*，2017年，第1–7页。'
- en: '[129] H. A. Poonawala, A. C. Satici, H. Eckert, and M. W. Spong, “Collision-free
    formation control with decentralized connectivity preservation for nonholonomic-wheeled
    mobile robots,” *IEEE Transactions on control of Network Systems*, vol. 2, no. 2,
    pp. 122–130, 2015.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] H. A. Poonawala, A. C. Satici, H. Eckert, 和 M. W. Spong，“无碰撞队形控制与非完整轮式移动机器人去中心化连通性保持”，*IEEE
    网络系统控制汇刊*，第2卷，第2期，第122–130页，2015年。'
- en: '[130] C. Wang, J. Wang, X. Zhang, and X. Zhang, “Autonomous navigation of uav
    in large-scale unknown complex environment with deep reinforcement learning,”
    in *IEEE GlobalSIP*, 2017, pp. 858–862.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] C. Wang, J. Wang, X. Zhang, 和 X. Zhang，“利用深度强化学习在大规模未知复杂环境中实现无人机自主导航”，发表于
    *IEEE GlobalSIP*，2017年，第858–862页。'
- en: '[131] C. Shen, C. Tekin, and M. van der Schaar, “A non-stochastic learning
    approach to energy efficient mobility management,” *IEEE Journal on Selected Areas
    in Communications*, vol. 34, no. 12, pp. 3854–3868, 2016.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] C. Shen, C. Tekin, 和 M. van der Schaar，“一种非随机学习方法用于节能移动管理”，*IEEE 选择领域通信期刊*，第34卷，第12期，第3854–3868页，2016年。'
- en: '[132] M. Faris and E. Brian, “Deep q-learning for self-organizing networks
    fault management and radio performance improvement,” *https://arxiv.org/abs/1707.02329*,
    2018.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] M. Faris 和 E. Brian，“用于自组织网络故障管理和无线性能改善的深度 Q 学习”，*https://arxiv.org/abs/1707.02329*，2018年。'
- en: '[133] G. Stampa, M. Arias, D. Sanchez-Charles, V. Muntes-Mulero, and A. Cabellos,
    “A deep-reinforcement learning approach for software-defined networking routing
    optimization,” *arXiv preprint arXiv:1709.07080*, 2017.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] G. Stampa, M. Arias, D. Sanchez-Charles, V. Muntes-Mulero, 和 A. Cabellos，“一种深度强化学习方法用于软件定义网络的路由优化”，*arXiv
    预印本 arXiv:1709.07080*，2017年。'
- en: '[134] M. Roughan, “Simplifying the synthesis of internet traffic matrices,”
    *ACM SIGCOMM Computer Communication Review*, vol. 35, no. 5, pp. 93–96, 2015\.
    [Online]. Available: http://arxiv.org/abs/1710.02913'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] M. Roughan, “简化互联网流量矩阵的合成，” *ACM SIGCOMM 计算机通信评论*，第 35 卷，第 5 期，第 93–96
    页，2015年。 [在线]. 可用: http://arxiv.org/abs/1710.02913'
- en: '[135] A. Varga and R. Hornig, “An overview of the OMNeT++ simulation environment,”
    in *proc. Int’l Conf. Simulation Tools and Techniques for Communications, Networks
    and Systems & Workshops*, 2008.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] A. Varga 和 R. Hornig, “OMNeT++ 仿真环境概述，” 在 *国际通信、网络和系统及研讨会仿真工具和技术会议论文集*，2008年。'
- en: '[136] Z. Xu, J. Tang, J. Meng, W. Zhang, Y. Wang, C. H. Liu, and D. Yang, “Experience-driven
    networking: A deep reinforcement learning based approach,” *arXiv preprint arXiv:1801.05757*,
    2018.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Z. Xu, J. Tang, J. Meng, W. Zhang, Y. Wang, C. H. Liu, 和 D. Yang, “经验驱动的网络：一种基于深度强化学习的方法，”
    *arXiv 预印本 arXiv:1801.05757*，2018年。'
- en: '[137] K. Winstein and H. Balakrishnan, “TCP ex Machina: Computer-generated
    congestion control,” in *ACM SIGCOMM*, 2013, pp. 123–134.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] K. Winstein 和 H. Balakrishnan, “TCP ex Machina：计算机生成的拥塞控制，” 在 *ACM SIGCOMM*，2013年，第
    123–134 页。'
- en: '[138] R. G.F. and H. T.R., *Modeling and Tools for Network Simulation*.   Springer,
    Berlin, Heidelberg, 2010, ch. The ns-3 Network Simulator.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] R. G.F. 和 H. T.R., *网络模拟的建模与工具*。Springer, 柏林，海德堡，2010年，第 ns-3 网络模拟器章。'
- en: '[139] A. Medina, A. Lakhina, I. Matta, and J. Byers, “BRITE: an approach to
    universal topology generation,” in *IEEE MASCOTS*, Aug. 2001, pp. 346–353.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] A. Medina, A. Lakhina, I. Matta, 和 J. Byers, “BRITE：一种通用拓扑生成方法，” 在 *IEEE
    MASCOTS*，2001年8月，第 346–353 页。'
- en: '[140] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradient
    methods for reinforcement learning with function approximation,” in *proc. 12th
    Int’l Conf. Neural Inform. Process. Syst.*, 1999, pp. 1057–1063.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] R. S. Sutton, D. McAllester, S. Singh, 和 Y. Mansour, “具有函数逼近的强化学习策略梯度方法，”
    在 *第12届国际神经信息处理系统会议论文集*，1999年，第 1057–1063 页。'
- en: '[141] U. Challita, W. Saad, and C. Bettstetter, “Deep reinforcement learning
    for interference-aware path planning of cellular connected uavs,” in *IEEE ICC*,
    Kansas City, MO, May 2018, pp. 1–6.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] U. Challita, W. Saad, 和 C. Bettstetter, “用于干扰感知的蜂窝连接无人机路径规划的深度强化学习，”
    在 *IEEE ICC*，密苏里州堪萨斯城，2018年5月，第 1–6 页。'
- en: '[142] U. Challita, W. Saad, and C. Bettstetter, “Cellular-connected uavs over
    5g: Deep reinforcement learning for interference management,” *arXiv preprint
    arXiv:1801.05500*, 2018.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] U. Challita, W. Saad, 和 C. Bettstetter, “5G下的蜂窝连接无人机：用于干扰管理的深度强化学习，”
    *arXiv 预印本 arXiv:1801.05500*，2018年。'
- en: '[143] L. Zhu, Y. He, F. R. Yu, B. Ning, T. Tang, and N. Zhao, “Communication-based
    train control system performance optimization using deep reinforcement learning,”
    *IEEE Transactions on Vehicular Technology*, vol. 66, no. 12, pp. 10 705–10 717,
    2017.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] L. Zhu, Y. He, F. R. Yu, B. Ning, T. Tang, 和 N. Zhao, “基于通信的列车控制系统性能优化：深度强化学习的应用，”
    *IEEE 车辆技术学报*，第 66 卷，第 12 期，第 10,705–10,717 页，2017年。'
- en: '[144] Y. Yang, Y. Li, K. Li, S. Zhao, R. Chen, J. Wang, and S. Ci, “Decco:
    Deep-learning enabled coverage and capacity optimization for massive mimo systems,”
    *IEEE Access*, to appear.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Y. Yang, Y. Li, K. Li, S. Zhao, R. Chen, J. Wang, 和 S. Ci, “Decco：针对大规模
    MIMO 系统的深度学习驱动的覆盖和容量优化，” *IEEE Access*，即将发表。'
- en: '[145] Z. Xu, Y. Wang, J. Tang, J. Wang, and M. C. Gursoy, “A deep reinforcement
    learning based framework for power-efficient resource allocation in cloud rans,”
    in *IEEE ICC*, 2017, pp. 1–6.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Z. Xu, Y. Wang, J. Tang, J. Wang, 和 M. C. Gursoy, “基于深度强化学习的云 RAN 中节能资源分配框架，”
    在 *IEEE ICC*，2017年，第 1–6 页。'
- en: '[146] T. M. Hackett, S. G. Bilén, P. V. R. Ferreira, A. M. Wyglinski, and R. C.
    Reinhart, “Implementation of a space communications cognitive engine,” in *Cognitive
    Communications for Aerospace Applications Workshop (CCAA)*, 2017, pp. 1–7.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] T. M. Hackett, S. G. Bilén, P. V. R. Ferreira, A. M. Wyglinski, 和 R.
    C. Reinhart, “空间通信认知引擎的实现，” 在 *航空航天应用认知通信研讨会 (CCAA)*，2017年，第 1–7 页。'
- en: '[147] Y. S. Nasir and D. Guo, “Deep reinforcement learning for distributed
    dynamic power allocation in wireless networks,” *arXiv preprint arXiv:1808.00490*,
    2018.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Y. S. Nasir 和 D. Guo, “用于无线网络的分布式动态功率分配的深度强化学习，” *arXiv 预印本 arXiv:1808.00490*，2018年。'
- en: '[148] X. Foukas, G. Patounas, A. Elmokashfi, and M. K. Marina, “Network slicing
    in 5g: Survey and challenges,” *IEEE Communications Magazine*, vol. 55, no. 5,
    pp. 94–100, 2017.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] X. Foukas, G. Patounas, A. Elmokashfi, 和 M. K. Marina, “5G中的网络切片：调查与挑战，”
    *IEEE 通信杂志*，第 55 卷，第 5 期，第 94–100 页，2017年。'
- en: '[149] X. Chen, Z. Li, Y. Zhang, R. Long, H. Yu, X. Du, and M. Guizani, “Reinforcement
    learning based qos/qoe-aware service function chaining in software-driven 5g slices,”
    *arXiv preprint arXiv:1804.02099*, 2018.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] X. Chen, Z. Li, Y. Zhang, R. Long, H. Yu, X. Du, 和 M. Guizani, “基于强化学习的
    QoS/QoE 感知服务功能链在软件驱动的 5G 切片中的应用，” *arXiv 预印本 arXiv:1804.02099*, 2018年。'
- en: '[150] P. Reichl, S. Egger, R. Schatz, and A. D’Alconzo, “The logarithmic nature
    of qoe and the role of the weber-fechner law in qoe assessment,” in *IEEE ICC*,
    Cape Town, South Africa, May 2010, pp. 1–5.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] P. Reichl, S. Egger, R. Schatz, 和 A. D’Alconzo, “qoe 的对数性质及 Weber-Fechner
    定律在 qoe 评估中的作用，” 收录于 *IEEE ICC*，南非开普敦，2010年5月，第1–5页。'
- en: '[151] M. Fiedler, T. Hossfeld, and P. Tran-Gia, “A generic quantitative relationship
    between quality of experience and quality of service,” *IEEE Network*, vol. 24,
    no. 2, pp. 36–41, 2014.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] M. Fiedler, T. Hossfeld, 和 P. Tran-Gia, “体验质量与服务质量之间的通用定量关系，” *IEEE Network*，第24卷，第2期，第36–41页，2014年。'
- en: '[152] Z. Zhao, R. Li, Q. Sun, Y. Yang, X. Chen, M. Zhao, H. Zhang *et al.*,
    “Deep reinforcement learning for network slicing,” *arXiv preprint arXiv:1805.06591*,
    2018.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Z. Zhao, R. Li, Q. Sun, Y. Yang, X. Chen, M. Zhao, H. Zhang *等*，“网络切片的深度强化学习，”
    *arXiv 预印本 arXiv:1805.06591*, 2018年。'
- en: '[153] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, “Resource management
    with deep reinforcement learning,” in *Proceedings of the 15th ACM Workshop on
    Hot Topics in Networks*, 2016, pp. 50–56.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] H. Mao, M. Alizadeh, I. Menache, 和 S. Kandula, “基于深度强化学习的资源管理，” 收录于 *第15届
    ACM 网络热点问题研讨会论文集*，2016年，第50–56页。'
- en: '[154] T. Li, Z. Xu, J. Tang, and Y. Wang, “Model-free control for distributed
    stream data processing using deep reinforcement learning,” *Proceedings of the
    VLDB Endowment*, vol. 11, no. 6, pp. 705–718, 2018.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] T. Li, Z. Xu, J. Tang, 和 Y. Wang, “使用深度强化学习的无模型控制用于分布式流数据处理，” *VLDB 论文集*，第11卷，第6期，第705–718页，2018年。'
- en: '[155] G. D. Arnold, R. Evans, H. v. Hasselt, P. Sunehag, T. Lillicrap, J. Hunt,
    T. Mann, T. Weber, T. Degris, and B. Coppin, “Deep reinforcement learning in large
    discrete action spaces,” *arXiv: 1512.07679*, 2016.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] G. D. Arnold, R. Evans, H. v. Hasselt, P. Sunehag, T. Lillicrap, J. Hunt,
    T. Mann, T. Weber, T. Degris, 和 B. Coppin, “大规模离散动作空间中的深度强化学习，” *arXiv: 1512.07679*,
    2016年。'
- en: '[156] X. Li, J. Fang, W. Cheng, H. Duan, Z. Chen, and H. Li, “Intelligent power
    control for spectrum sharing in cognitive radios: A deep reinforcement learning
    approach,” *arXiv preprint arXiv:1712.07365*, 2017.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] X. Li, J. Fang, W. Cheng, H. Duan, Z. Chen, 和 H. Li, “认知无线电中的智能功率控制：一种深度强化学习方法，”
    *arXiv 预印本 arXiv:1712.07365*, 2017年。'
- en: '[157] T. Oda, R. Obukata, M. Ikeda, L. Barolli, and M. Takizawa, “Design and
    implementation of a simulation system based on deep q-network for mobile actor
    node control in wireless sensor and actor networks,” in *International Conference
    on Advanced Information Networking and Applications Workshops (WAINA)*, 2017,
    pp. 195–200.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] T. Oda, R. Obukata, M. Ikeda, L. Barolli, 和 M. Takizawa, “基于深度 Q 网络的无线传感器和行为者网络中移动行为者节点控制的仿真系统设计与实现，”
    收录于 *国际先进信息网络与应用研讨会 (WAINA)*，2017年，第195–200页。'
- en: '[158] L. Wang, W. Liu, D. Zhang, Y. Wang, E. Wang, and Y. Yang, “Cell selection
    with deep reinforcement learning in sparse mobile crowdsensing,” *arXiv preprint
    arXiv:1804.07047*, 2018.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] L. Wang, W. Liu, D. Zhang, Y. Wang, E. Wang, 和 Y. Yang, “稀疏移动 crowdsensing
    中的深度强化学习的细胞选择，” *arXiv 预印本 arXiv:1804.07047*, 2018年。'
- en: '[159] F. Ingelrest, G. Barrenetxea, G. Schaefer, M. Vetterli, O. Couach, and
    M. Parlange., “SensorScope: Application-specific sensor network for environmental
    monitoring,” *ACM Transactions on Sensor Networks*, vol. 6, no. 2, pp. 1–32, 2010.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] F. Ingelrest, G. Barrenetxea, G. Schaefer, M. Vetterli, O. Couach, 和
    M. Parlange., “SensorScope: 专用传感器网络用于环境监测，” *ACM 传感器网络事务*，第6卷，第2期，第1–32页，2010年。'
- en: '[160] Y. Zheng, F. Liu, and H. P. Hsieh, “U-Air: when urban air quality inference
    meets big data,” in *ACM SIGKDD Int’l Conf. Knowledge Discovery and Data Mining*,
    2013.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Y. Zheng, F. Liu, 和 H. P. Hsieh, “U-Air: 当城市空气质量推断遇到大数据，” 收录于 *ACM SIGKDD
    国际会议 知识发现与数据挖掘*，2013年。'
- en: '[161] B. Zhang, C. H. Liu, J. Tang, Z. Xu, J. Ma, and W. Wang, “Learning-based
    energy-efficient data collection by unmanned vehicles in smart cities,” *IEEE
    Transactions on Industrial Informatics*, vol. 14, no. 4, pp. 1666–1676, 2018.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] B. Zhang, C. H. Liu, J. Tang, Z. Xu, J. Ma, 和 W. Wang, “基于学习的智能城市无人驾驶车辆的节能数据收集，”
    *IEEE 工业信息学汇刊*，第14卷，第4期，第1666–1676页，2018年。'
- en: '[162] L. Bracciale, M. Bonola, P. Loreti, G. Bianchi, R. Amici, and A. Rabuffi.
    (2014, Jul.) CRAWDAD dataset roma/taxi (v. 2014-07-17). [Online]. Available: http://crawdad.org/roma/taxi/20140717'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] L. Bracciale, M. Bonola, P. Loreti, G. Bianchi, R. Amici, 和 A. Rabuffi.
    (2014年7月) CRAWDAD 数据集 roma/taxi (v. 2014-07-17)。 [在线]. 可用: http://crawdad.org/roma/taxi/20140717'
- en: '[163] L. Xiao, Y. Li, G. Han, H. Dai, and H. V. Poor, “A secure mobile crowdsensing
    game with deep reinforcement learning,” *IEEE Transactions on Information Forensics
    and Security*, vol. 13, no. 1, pp. 35–47, Jan. 2018.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] L. Xiao, Y. Li, G. Han, H. Dai, 和 H. V. Poor, “带有深度强化学习的安全移动众包游戏，” *IEEE信息取证与安全汇刊*，第13卷，第1期，第35–47页，2018年1月。'
- en: '[164] Y. Zhang, B. Song, and P. Zhang, “Social behavior study under pervasive
    social networking based on decentralized deep reinforcement learning,” *Journal
    of Network and Computer Applications*, vol. 86, pp. 72–81, 2017.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Y. Zhang, B. Song, 和 P. Zhang, “基于去中心化深度强化学习的普遍社交网络下的社会行为研究，” *网络与计算机应用杂志*，第86卷，第72–81页，2017年。'
- en: '[165] M. Mohammadi, A. Al-Fuqaha, M. Guizani, and J.-S. Oh, “Semisupervised
    deep reinforcement learning in support of iot and smart city services,” *IEEE
    Internet of Things Journal*, vol. 5, no. 2, pp. 624–635, 2018.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] M. Mohammadi, A. Al-Fuqaha, M. Guizani, 和 J.-S. Oh, “支持物联网和智能城市服务的半监督深度强化学习，”
    *IEEE物联网杂志*，第5卷，第2期，第624–635页，2018年。'
- en: '[166] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, “Semi-supervised
    learning with deep generative models,” in *Advances in Neural Information Processing
    Systems*, 2014, pp. 3581–3589.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] D. P. Kingma, S. Mohamed, D. J. Rezende, 和 M. Welling, “深度生成模型的半监督学习，”
    见 *神经信息处理系统进展*，2014年，第3581–3589页。'
- en: '[167] G. Cao, Z. Lu, X. Wen, T. Lei, and Z. Hu, “Aif: An artificial intelligence
    framework for smart wireless network management,” *IEEE Communications Letters*,
    vol. 22, no. 2, pp. 400–403, 2018.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] G. Cao, Z. Lu, X. Wen, T. Lei, 和 Z. Hu, “AIF：用于智能无线网络管理的人工智能框架，” *IEEE通信快报*，第22卷，第2期，第400–403页，2018年。'
- en: '[168] Y. Zhan, Y. Xia, J. Zhang, T. Li, and Y. Wang, “Crowdsensing game with
    demand uncertainties: A deep reinforcement learning approach,” submitted.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Y. Zhan, Y. Xia, J. Zhang, T. Li, 和 Y. Wang, “具有需求不确定性的众包游戏：一种深度强化学习方法，”
    已提交。'
- en: '[169] N. C. Luong, P. Wang, D. Niyato, Y. Wen, and Z. Han, “Resource management
    in cloud networking using economic analysis and pricing models: a survey,” *IEEE
    Communications Surveys & Tutorials*, vol. 19, no. 2, pp. 954–1001, Jan. 2017.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] N. C. Luong, P. Wang, D. Niyato, Y. Wen, 和 Z. Han, “使用经济分析和定价模型的云网络资源管理：综述，”
    *IEEE通信调查与教程*，第19卷，第2期，第954–1001页，2017年1月。'
- en: '[170] N. C. Luong, D. T. Hoang, P. Wang, D. Niyato, D. I. Kim, and Z. Han,
    “Data collection and wireless communication in internet of things (iot) using
    economic analysis and pricing models: A survey,” *IEEE Communications Surveys
    & Tutorials*, vol. 18, no. 4, pp. 2546–2590, Jun. 2016.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] N. C. Luong, D. T. Hoang, P. Wang, D. Niyato, D. I. Kim, 和 Z. Han, “使用经济分析和定价模型的数据收集与无线通信：综述，”
    *IEEE通信调查与教程*，第18卷，第4期，第2546–2590页，2016年6月。'
- en: '[171] F. Shi, Z. Qin, and J. A. McCann, “Oppay: Design and implementation of
    a payment system for opportunistic data services,” in *IEEE International Conference
    on Distributed Computing Systems*, Atlanta, GA, Jul. 2017, pp. 1618–1628.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] F. Shi, Z. Qin, 和 J. A. McCann, “Oppay：机会数据服务支付系统的设计与实现，” 见 *IEEE国际分布式计算系统会议*，乔治亚州亚特兰大，2017年7月，第1618–1628页。'
- en: '[172] Z. Jiang and J. Liang, “Cryptocurrency portfolio management with deep
    reinforcement learning,” in *Intelligent Systems Conference (IntelliSys)*, 2017,
    pp. 905–913.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Z. Jiang 和 J. Liang, “使用深度强化学习的加密货币投资组合管理，” 见 *智能系统会议 (IntelliSys)*，2017年，第905–913页。'
- en: '[173] N. C. Luong, P. Wang, D. Niyato, Y.-C. Liang, F. Hou, and Z. Han, “Applications
    of economic and pricing models for resource management in 5g wireless networks:
    A survey,” *IEEE Communications Surveys and Tutorials*, to appear.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] N. C. Luong, P. Wang, D. Niyato, Y.-C. Liang, F. Hou, 和 Z. Han, “5G无线网络资源管理的经济与定价模型应用：综述，”
    *IEEE通信调查与教程*，待发表。'
- en: '[174] J. Zhao, G. Qiu, Z. Guan, W. Zhao, and X. He, “Deep reinforcement learning
    for sponsored search real-time bidding,” *arXiv preprint arXiv:1803.00259*, 2018.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] J. Zhao, G. Qiu, Z. Guan, W. Zhao, 和 X. He, “用于赞助搜索实时竞价的深度强化学习，” *arXiv预印本
    arXiv:1803.00259*，2018年。'
