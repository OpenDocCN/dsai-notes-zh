- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:59:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:59:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2008.07267] A Survey of Active Learning for Text Classification using Deep
    Neural Networks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2008.07267] 使用深度神经网络的文本分类主动学习调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2008.07267](https://ar5iv.labs.arxiv.org/html/2008.07267)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2008.07267](https://ar5iv.labs.arxiv.org/html/2008.07267)
- en: \ProvidesForestLibrary
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \ProvidesForestLibrary
- en: edges \forestset declare dimen=fork sep0.5em, forked edge/.style= edge=rotate/.pgfmath=grow(),
    edge path’=(!u.parent anchor) – ++(\forestoptionfork sep,0) |- (.child anchor),
    , forked edges/.style= for tree=parent anchor=children, for descendants=child
    anchor=parent,forked edge
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: edges \forestset declare dimen=fork sep0.5em, forked edge/.style= edge=rotate/.pgfmath=grow(),
    edge path’=(!u.parent anchor) – ++(\forestoptionfork sep,0) |- (.child anchor),
    , forked edges/.style= for tree=parent anchor=children, for descendants=child
    anchor=parent,forked edge
- en: A Survey of Active Learning for Text Classification using Deep Neural Networks
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度神经网络的文本分类主动学习调查
- en: Christopher Schröder Natural Language Processing Group, University of Leipzig
    Andreas Niekler Natural Language Processing Group, University of Leipzig
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Christopher Schröder 自然语言处理组，莱比锡大学 Andreas Niekler 自然语言处理组，莱比锡大学
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Natural language processing (NLP) and neural networks (NNs) have both undergone
    significant changes in recent years. For active learning (AL) purposes, NNs are,
    however, less commonly used – despite their current popularity. By using the superior
    text classification performance of NNs for AL, we can either increase a model’s
    performance using the same amount of data or reduce the data and therefore the
    required annotation efforts while keeping the same performance. We review AL for
    text classification using deep neural networks (DNNs) and elaborate on two main
    causes which used to hinder the adoption: (a) the inability of NNs to provide
    reliable uncertainty estimates, on which the most commonly used query strategies
    rely, and (b) the challenge of training DNNs on small data. To investigate the
    former, we construct a taxonomy of query strategies, which distinguishes between
    data-based, model-based, and prediction-based instance selection, and investigate
    the prevalence of these classes in recent research. Moreover, we review recent
    NN-based advances in NLP like word embeddings or language models in the context
    of (D)NNs, survey the current state-of-the-art at the intersection of AL, text
    classification, and DNNs and relate recent advances in NLP to AL. Finally, we
    analyze recent work in AL for text classification, connect the respective query
    strategies to the taxonomy, and outline commonalities and shortcomings. As a result,
    we highlight gaps in current research and present open research questions.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）和神经网络（NNs）近年来都发生了重大变化。然而，尽管神经网络目前非常流行，它们在主动学习（AL）中的应用却不那么常见。通过利用神经网络在文本分类中的优越表现，我们可以在使用相同数据量的情况下提高模型的性能，或者减少数据量，从而减少所需的标注工作量，同时保持相同的性能。我们回顾了深度神经网络（DNNs）在文本分类中的主动学习，并详细讨论了两个主要的阻碍因素：（a）神经网络无法提供可靠的不确定性估计，而大多数常用的查询策略依赖于此；（b）在小数据集上训练深度神经网络的挑战。为了解决前者，我们构建了一个查询策略的分类法，区分了基于数据、基于模型和基于预测的实例选择，并调查了这些类别在最近研究中的普及情况。此外，我们还回顾了在（D）NNs背景下，神经网络在NLP中的最新进展，如词嵌入或语言模型，并调查了主动学习、文本分类和深度神经网络的当前前沿状态，并将NLP中的最新进展与主动学习关联起来。最后，我们分析了最近在文本分类中的主动学习研究，将相关的查询策略与分类法联系起来，概述了共同点和不足之处。因此，我们强调了当前研究中的空白，并提出了开放的研究问题。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Data is the fuel of machine learning applications and therefore has been steadily
    increasing in value. In many settings an abundant amount of unlabeled data is
    produced, but in order to use such data in supervised machine learning, one has
    no choice but to provide labels. This usually entails a manual labeling process,
    which is often non-trivial and can even require a domain expert, e.g., in patent
    classification [[52](#bib.bibx52), [23](#bib.bibx23)], or clinical text classification
    [[75](#bib.bibx75), [24](#bib.bibx24), [28](#bib.bibx28)]. Moreover, this is time-consuming
    and rapidly increases monetary costs, thereby quickly rendering this approach
    infeasible. Even if an expert is available, it is often impossible to label each
    datum due to the vast size of modern datasets. This especially impedes the field
    of Natural Language Processing (NLP), in which both the dataset and the amount
    of text within each document can be huge, resulting in unbearable amounts of annotation
    efforts for human experts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是机器学习应用的燃料，因此其价值不断增加。在许多设置中，会产生大量的未标记数据，但为了在监督学习中使用这些数据，必须提供标签。这通常需要手动标记过程，这通常并非简单，并且可能需要领域专家，例如在专利分类[[52](#bib.bibx52),
    [23](#bib.bibx23)]或临床文本分类[[75](#bib.bibx75), [24](#bib.bibx24), [28](#bib.bibx28)]中。此外，这一过程耗时且迅速增加财务成本，从而迅速使这种方法变得不可行。即使有专家，也常常无法标记每个数据点，因为现代数据集的规模庞大。这尤其阻碍了自然语言处理（NLP）领域，在该领域中，数据集和每个文档中的文本量都可能非常庞大，导致对人工专家而言的注释工作量无法承受。
- en: 'Active Learning (AL) aims to reduce the amount of data annotated by the human
    expert. It is an iterative cyclic process between an oracle (usually the human
    annotator) and an active learner. In contrast to passive learning, in which the
    data is simply fed to the algorithm, the active learner chooses which samples
    are to be labeled next. The labeling itself, however, is done by a human expert,
    the so-called human in the loop. Having received new labels, the active learner
    trains a new model and the process starts from the beginning. Using the term active
    learner, we refer to the composition of a model, a query strategy, and a stopping
    criterion. In this work the model is w.l.o.g. a text classification model, the
    query strategy decides which instances should be labeled next, and the stopping
    criterion defines when to stop the AL loop. According to [[85](#bib.bibx85)] there
    are three main scenarios for AL: (1) Pool-based, in which the learner has access
    to the closed set of unlabeled instances, called the pool; (2) stream-based, where
    the learner receives one instance at a time and has the options to keep it, or
    to discard; (3) membership query synthesis, in which the learner creates new artificial
    instances to be labeled. If the pool-based scenario operates not on a single instance,
    but on a batch of instances, this is called batch-mode AL [[85](#bib.bibx85)].
    Throughout this work we assume a pool-based batch-mode scenario because in a text
    classification setting the dataset is usually a closed set, and the batch-wise
    operation reduces the number of retraining operations, which cause waiting periods
    for the user.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习（AL）的目标是减少由人工专家注释的数据量。这是一个在oracle（通常是人工注释者）和主动学习者之间的迭代循环过程。与被动学习相比，被动学习只是将数据提供给算法，主动学习者则选择下一步要标记的样本。然而，标记本身是由人工专家完成的，即所谓的“人机环节”。在获得新标签后，主动学习者训练一个新模型，过程从头开始。使用“主动学习者”一词，我们指的是模型、查询策略和停止标准的组合。在这项工作中，模型是文本分类模型，查询策略决定了哪些实例应该被标记，停止标准定义了何时停止AL循环。根据[[85](#bib.bibx85)]，AL有三种主要场景：（1）基于池的，学习者可以访问一组封闭的未标记实例，称为池；（2）基于流的，学习者一次接收一个实例，并有选择保留或丢弃；（3）成员查询合成，学习者创建新的人工实例进行标记。如果基于池的场景不是对单个实例操作，而是对一批实例操作，这被称为批处理模式AL
    [[85](#bib.bibx85)]。在这项工作中，我们假设一个基于池的批处理模式场景，因为在文本分类设置中，数据集通常是封闭的，并且批量操作减少了重训练操作的数量，从而减少了用户等待时间。
- en: The underlying idea of AL is that few representative instances can be used as
    surrogate for the full dataset. Not only does a smaller subset of the data reduce
    the computational costs, but also it has been shown that AL can even increase
    the quality of the resulting model compared to learning on the full dataset [[83](#bib.bibx83),
    [24](#bib.bibx24)]. As a consequence, AL has been used in many NLP tasks, e.g.
    text classification [[95](#bib.bibx95), [39](#bib.bibx39)], named entity recognition
    [[88](#bib.bibx88), [94](#bib.bibx94), [89](#bib.bibx89)], or machine translation
    [[35](#bib.bibx35)] and is still an active area of research.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: AL的基本思想是，少量具有代表性的实例可以用作完整数据集的替代品。较小的数据子集不仅降低了计算成本，而且研究表明，AL甚至可以提高模型的质量，相较于在完整数据集上学习[[83](#bib.bibx83),
    [24](#bib.bibx24)]。因此，AL已被广泛应用于许多NLP任务，如文本分类[[95](#bib.bibx95), [39](#bib.bibx39)]、命名实体识别[[88](#bib.bibx88),
    [94](#bib.bibx94), [89](#bib.bibx89)]或机器翻译[[35](#bib.bibx35)]，并且仍然是一个活跃的研究领域。
- en: In recent years, deep learning (DL) approaches have dominated most NLP tasks’
    state-of-the-art results. This can be attributed to advances in neural networks
    (NNs), above all Convolutional Neural Networks (CNN; [[48](#bib.bibx48)]) and
    (Bidirectional-)Long Short-Term Memory (LSTM; [[38](#bib.bibx38), [31](#bib.bibx31)]),
    which were eventually adopted into the NLP domain, and to the advances of using
    word embeddings [[66](#bib.bibx66), [65](#bib.bibx65), [74](#bib.bibx74)] and
    contextualized word embeddings [[76](#bib.bibx76), [20](#bib.bibx20)]. Both NN
    architectures and text representations have raised the state-of-the-art results
    in the field of text classification considerably (e.g., [[103](#bib.bibx103),
    [41](#bib.bibx41), [102](#bib.bibx102)]). If these improvements were transferrable
    to AL, this would result in a huge increase in efficiency. For the AL practitioner,
    this either means achieving the same performance using fewer samples, or having
    an increase in performance using the same amount of data. Another favorable development
    is that transfer learning, especially the paradigm of fine-tuning pre-trained
    language models (LMs), has become popular in NLP. In the context of AL this helps
    especially in the small data scenario, in which a pre-trained model can be leveraged
    to train a model by fine-tuning using only little data, which would otherwise
    be infeasible. Finally, by operating on sub-word units LMs also handle out-of-vocabulary
    tokens, which is an advantage over many traditional methods.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习（DL）方法在大多数NLP任务中主导了最先进的结果。这可以归因于神经网络（NNs）的进展，特别是卷积神经网络（CNN；[[48](#bib.bibx48)]）和（双向）长短期记忆（LSTM；[[38](#bib.bibx38),
    [31](#bib.bibx31)]），它们最终被引入到NLP领域，以及词嵌入[[66](#bib.bibx66), [65](#bib.bibx65),
    [74](#bib.bibx74)]和上下文化词嵌入[[76](#bib.bibx76), [20](#bib.bibx20)]的进展。NN架构和文本表示显著提高了文本分类领域的最先进结果（例如，[[103](#bib.bibx103),
    [41](#bib.bibx41), [102](#bib.bibx102)]）。如果这些改进可以转移到AL中，将会极大地提高效率。对于AL从业者来说，这意味着使用更少的样本实现相同的性能，或者在使用相同的数据量时提高性能。另一个有利的发展是迁移学习，特别是微调预训练语言模型（LMs）的范式，已在NLP中变得流行。在AL的背景下，这在小数据场景中尤为重要，在这种情况下，可以利用预训练模型，通过微调仅使用少量数据来训练模型，否则这将是不可行的。最后，通过在子词单元上操作，LMs还能够处理词汇表外的标记，这相对于许多传统方法是一个优势。
- en: 'Resulting from these advances, existing AL surveys have become both incomplete
    in some parts and outdated in others: They lack comparison against the current
    state of the art models, do not provide results for more recent large-scale datasets,
    and most importantly, they are lacking the aforementioned advances in NNs and
    text representations. Surprisingly, despite the current popularity of NNs, there
    is only little research about NN-based active learning in the context of NLP,
    and even less thereof in the context of text classification (see Section [3.2](#S3.SS2
    "3.2 Neural-Network-Based Active Learning ‣ 3 Active Learning ‣ A Survey of Active
    Learning for Text Classification using Deep Neural Networks") and Section [4.2](#S4.SS2
    "4.2 Text Classification for Active Learning ‣ 4 Active Learning for Text Classification
    ‣ A Survey of Active Learning for Text Classification using Deep Neural Networks")
    for a detailed summary). We suspect this is due to the following reasons: (1)
    Many DL models are known to require large amounts of data [[103](#bib.bibx103)],
    which is in strong contrast to AL aiming at requiring as little data as possible
    (2) there is a whole AL scenario based on artificial data generation, which unfortunately
    is a lot more challenging for text in contrast to for example images, for which
    data augmentation is commonly used in classification tasks [[100](#bib.bibx100)];
    (3) NNs are lacking uncertainty information regarding their predictions (as explained
    in Section [3.2](#S3.SS2 "3.2 Neural-Network-Based Active Learning ‣ 3 Active
    Learning ‣ A Survey of Active Learning for Text Classification using Deep Neural
    Networks")), which complicates the use of a whole prominent class of query strategies.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些进展导致现有的AL调查在某些部分变得不完整，在其他部分则过时：它们缺乏与当前最先进模型的比较，没有为更近期的大规模数据集提供结果，并且最重要的是，缺乏前述的NNs和文本表示的进展。令人惊讶的是，尽管NNs目前非常流行，但关于基于NN的主动学习在NLP背景下的研究仍然很少，在文本分类的背景下更少（详见第[3.2节](#S3.SS2
    "3.2 Neural-Network-Based Active Learning ‣ 3 Active Learning ‣ A Survey of Active
    Learning for Text Classification using Deep Neural Networks")和第[4.2节](#S4.SS2
    "4.2 Text Classification for Active Learning ‣ 4 Active Learning for Text Classification
    ‣ A Survey of Active Learning for Text Classification using Deep Neural Networks")）。我们怀疑这主要是由于以下原因：（1）许多DL模型已知需要大量数据[[103](#bib.bibx103)]，这与AL旨在尽可能少使用数据的目标形成鲜明对比；（2）存在基于人工数据生成的AL场景，这对文本来说比例如图像中的数据增强要困难得多[[100](#bib.bibx100)]；（3）NNs缺乏关于其预测的不确定性信息（如第[3.2节](#S3.SS2
    "3.2 Neural-Network-Based Active Learning ‣ 3 Active Learning ‣ A Survey of Active
    Learning for Text Classification using Deep Neural Networks")所述），这使得使用一整类显著的查询策略变得复杂。
- en: 'This survey aims at summarizing the existing approaches of (D)NN-based AL for
    text classification. Our main contributions are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查旨在总结现有的基于（D）NN的文本分类AL方法。我们的主要贡献如下：
- en: '1.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We provide a taxonomy of query strategies and classify strategies relevant for
    AL for text classification.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了查询策略的分类，并将与文本分类相关的AL策略进行分类。
- en: '2.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We survey existing work at the intersection of AL, text classification, and
    (D)NNs.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们调查了AL、文本分类和（D）NNs的交集中的现有工作。
- en: '3.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Recent advances in text classification are summarized and related to the AL
    process. It is then investigated, if and to what degree they have been adopted
    for AL.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对文本分类的最新进展进行总结，并与AL过程相关联。然后调查这些进展是否以及在何种程度上被应用于AL。
- en: '4.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: The experimental setup of previous research is collectively analyzed regarding
    datasets, models, and query strategies in order to identify recent trends, commonalities,
    and shortcomings in the experiments.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前研究的实验设置从数据集、模型和查询策略方面进行综合分析，以识别实验中的近期趋势、共同点和不足之处。
- en: '5.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: We identify research gaps and outline future research directions.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们识别研究中的空白并概述未来的研究方向。
- en: Thereby we provide a comprehensive survey of recent advances in NN-based active
    text classification. Having reviewed these recent advances, we illuminate areas
    that either need re-evaluation, or have not yet been evaluated in a more recent
    context. As a final result, we develop research questions outlining the scope
    of future research.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们提供了关于基于NN的主动文本分类的最新进展的综合调查。在回顾这些最新进展后，我们阐明了需要重新评估的领域或在更近期背景下尚未评估的领域。最终结果是，我们提出了研究问题，勾勒出未来研究的范围。
- en: 2 Related Work
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: '[[85](#bib.bibx85)] provides a general active learning survey, summarizing
    the prevalent AL scenario types and query strategies. They present variations
    of the basic AL setup like variable labeling costs or alternative query types,
    and most notably, they discuss empirical and theoretical research investigating
    the effectiveness of AL: They mention research suggesting that AL is effective
    in practice and has increasingly gained adoption in real world applications. However,
    it is pointed out that empirical research also reported cases in which AL performed
    worse than passive learning and that the theoretical analysis of AL is incomplete.
    Finally, relations to related research areas are illustrated, thereby connecting
    AL among others to reinforcement learning and semi-supervised learning.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[[85](#bib.bibx85)]提供了一般性的主动学习调查，总结了流行的AL场景类型和查询策略。他们展示了基本AL设置的变体，如可变标记成本或替代查询类型，最值得注意的是，他们讨论了研究AL效果的实证和理论研究：他们提到研究表明，AL在实践中是有效的，并且在实际应用中越来越受到采用。然而，也指出实证研究还报告了AL表现不如被动学习的情况，并且AL的理论分析是不完整的。最后，相关研究领域的关系被阐明，从而将AL与其他领域如强化学习和半监督学习联系起来。'
- en: The survey of [[25](#bib.bibx25)] is focused around a thorough analysis of uncertainty-based
    query strategies, which are categorized into a taxonomy. This taxonomy differentiates
    at the topmost level between the uncertainty of i.i.d. instances and instance
    correlation. The latter is a superset of the former and intends to reduce redundancy
    among instances by considering feature, label, and structure correlation when
    querying. Moreover, they perform an algorithmic analysis for each query strategy
    and order the strategies by their respective time complexity, highlighting the
    increased complexity for correlation-based strategies.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[[25](#bib.bibx25)]的调查集中在对基于不确定性的查询策略的深入分析，这些策略被分类到一个分类法中。该分类法在最顶层区分了i.i.d.实例的不确定性和实例相关性。后者是前者的超集，旨在通过在查询时考虑特征、标签和结构的相关性来减少实例之间的冗余。此外，他们对每种查询策略进行了算法分析，并根据各自的时间复杂度对策略进行排序，突出了基于相关性的策略的复杂性增加。'
- en: 'Another general survey covering a wide range of topics was conducted by [[1](#bib.bibx1)].
    They provide a flat categorization of query strategies, which is quite different
    from the taxonomy of [[25](#bib.bibx25)] and divides them into the following three
    categories: (1) “heterogenity-based”, which sample instances by their prediction
    uncertainty or dissimilarity compared to existing labeled instances, (2) “performance-based”’,
    which select instances based on a predicted change of the model loss, and (3)
    “representativeness-based”, which select data points to reflect a larger set in
    terms of their properties, usually achieved by the means of distribution density
    [[1](#bib.bibx1)]. Similarly to [[85](#bib.bibx85)], they present and discuss
    many non-standard variations of the active learning scenario.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项涵盖广泛主题的综合调查由[[1](#bib.bibx1)]进行。他们提供了查询策略的平面分类，这与[[25](#bib.bibx25)]的分类法有所不同，并将其分为以下三类：（1）“基于异质性”的，按照预测的不确定性或与现有标记实例的不相似性来采样实例，（2）“基于性能”的，依据模型损失的预测变化来选择实例，以及（3）“基于代表性”的，选择数据点以反映在其属性上的更大集合，通常通过分布密度来实现[[1](#bib.bibx1)]。与[[85](#bib.bibx85)]类似，他们展示并讨论了许多非标准的主动学习场景变体。
- en: An NLP-focused active learning survey was performed by [[71](#bib.bibx71)].
    This work’s main contribution is a survey of disagreement-based query strategies,
    which use the disagreement among multiple classifiers to select instances. Moreover,
    Olsson reviews practical considerations, e.g., selecting an initial seed set,
    deciding between stream-based and pool-based scenario, and deciding when to terminate
    the learning process.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一项针对NLP的主动学习调查由[[71](#bib.bibx71)]进行。该工作的主要贡献是对基于分歧的查询策略的综述，这些策略利用多个分类器之间的分歧来选择实例。此外，Olsson还回顾了实际考虑因素，例如选择初始种子集、决定流式和池式场景之间的选择，以及决定何时终止学习过程。
- en: Although some NN-based applications are mentioned, none of the above surveys
    covers NN-based AL in depth. Besides, none is recent enough to cover NN-architectures,
    which have only recently been adapted successfully to text classification problems
    like e.g., KimCNN [[48](#bib.bibx48)]. The same holds true for recent advances
    in NLP such as word embeddings, contextualized language models (explained in Section
    [4.1](#S4.SS1 "4.1 Recent Advances in Text Classification ‣ 4 Active Learning
    for Text Classification ‣ A Survey of Active Learning for Text Classification
    using Deep Neural Networks")), or resulting advances in text classification (discussed
    in Section [4.1](#S4.SS1.SSS0.Px2 "Neural-Network-Based Text Classification ‣
    4.1 Recent Advances in Text Classification ‣ 4 Active Learning for Text Classification
    ‣ A Survey of Active Learning for Text Classification using Deep Neural Networks")
    and Section [4.2](#S4.SS2 "4.2 Text Classification for Active Learning ‣ 4 Active
    Learning for Text Classification ‣ A Survey of Active Learning for Text Classification
    using Deep Neural Networks")). We intend to fill these gaps in the remainder of
    this survey.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提到了一些基于神经网络的应用，但上述调查中没有一个深入涵盖基于神经网络的主动学习。此外，没有一个调查足够新颖，以涵盖神经网络架构，这些架构最近才成功地应用于文本分类问题，例如，KimCNN
    [[48](#bib.bibx48)]。最近在自然语言处理中的进展，如词嵌入、上下文化语言模型（在第[4.1](#S4.SS1 "4.1 Recent Advances
    in Text Classification ‣ 4 Active Learning for Text Classification ‣ A Survey
    of Active Learning for Text Classification using Deep Neural Networks")节中解释）或由此产生的文本分类进展（在第[4.1](#S4.SS1.SSS0.Px2
    "Neural-Network-Based Text Classification ‣ 4.1 Recent Advances in Text Classification
    ‣ 4 Active Learning for Text Classification ‣ A Survey of Active Learning for
    Text Classification using Deep Neural Networks")节和第[4.2](#S4.SS2 "4.2 Text Classification
    for Active Learning ‣ 4 Active Learning for Text Classification ‣ A Survey of
    Active Learning for Text Classification using Deep Neural Networks")节中讨论）也同样适用。我们打算在本调查的其余部分填补这些空白。
- en: 3 Active Learning
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 主动学习
- en: 'The goal of AL is to create a model using as few labeled instances as possible,
    i.e. minimizing the interactions between the oracle and the active learner. The
    AL process (illustrated in Figure [1](#S3.F1 "Figure 1 ‣ 3 Active Learning ‣ A
    Survey of Active Learning for Text Classification using Deep Neural Networks"))
    is as follows: The oracle requests unlabeled instances from the active learner
    (query, see Figure [1](#S3.F1 "Figure 1 ‣ 3 Active Learning ‣ A Survey of Active
    Learning for Text Classification using Deep Neural Networks"): step 1), which
    are then selected by the active learner (based on the selected query strategy)
    and passed to the oracle (see Figure [1](#S3.F1 "Figure 1 ‣ 3 Active Learning
    ‣ A Survey of Active Learning for Text Classification using Deep Neural Networks"):
    step 2). Subsequently, these instances are labeled by the oracle and returned
    to the active learner (update, see Figure [1](#S3.F1 "Figure 1 ‣ 3 Active Learning
    ‣ A Survey of Active Learning for Text Classification using Deep Neural Networks"):
    step 3). After each update step the active learner’s model is retrained, which
    makes this operation at least as expensive as a training of the underlying model.
    This process is repeated until a stopping criterion is met (e.g., a maximum number
    of iterations or a minimum threshold of change in classification accuracy).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习的目标是使用尽可能少的标记实例来创建模型，即最小化oracle和主动学习者之间的交互。主动学习过程（如图[1](#S3.F1 "Figure 1
    ‣ 3 Active Learning ‣ A Survey of Active Learning for Text Classification using
    Deep Neural Networks")所示）如下：oracle从主动学习者那里请求未标记的实例（查询，见图[1](#S3.F1 "Figure 1 ‣
    3 Active Learning ‣ A Survey of Active Learning for Text Classification using
    Deep Neural Networks")：步骤1），这些实例由主动学习者（基于所选查询策略）选择，并传递给oracle（见图[1](#S3.F1 "Figure
    1 ‣ 3 Active Learning ‣ A Survey of Active Learning for Text Classification using
    Deep Neural Networks")：步骤2）。随后，这些实例由oracle进行标记并返回给主动学习者（更新，见图[1](#S3.F1 "Figure
    1 ‣ 3 Active Learning ‣ A Survey of Active Learning for Text Classification using
    Deep Neural Networks")：步骤3）。每次更新步骤后，主动学习者的模型都会被重新训练，这使得此操作至少与底层模型的训练一样昂贵。这个过程会一直重复，直到满足停止标准（例如，最大迭代次数或分类准确率的最小变化阈值）。
- en: '![Refer to caption](img/1aac2ccb961c305e2598f48b904bb257.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1aac2ccb961c305e2598f48b904bb257.png)'
- en: 'Figure 1: An overview of the AL process: Model, query strategy, and (optionally
    a) stopping criterion are the key components of an active learner. The main loop
    is as follows: First the oracle queries the active learner, which returns a fixed
    amount of unlabeled instances. Then, for all selected unlabeled unstances are
    assigned labels by the oracle. This process is repeated until the oracle stops,
    or a predefined stopping criterion is met.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：主动学习过程概述：模型、查询策略和（可选的）停止标准是主动学习者的关键组成部分。主要循环如下：首先，oracle 询问主动学习者，主动学习者返回固定数量的未标记实例。然后，oracle
    为所有选定的未标记实例分配标签。此过程重复进行，直到 oracle 停止或满足预定义的停止标准。
- en: The most important component for AL is the query strategy. In the introduction
    we claimed that a large fraction of query strategies are uncertainty-based. To
    analyze this we provide a taxonomy of query strategies in the following section
    and highlight the parts in which uncertainty is involved. For a general and more
    detailed introduction on AL refer to the surveys of [[85](#bib.bibx85)] and [[1](#bib.bibx1)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于主动学习，最重要的组件是查询策略。在介绍中我们声明了大量查询策略是基于不确定性的。为了分析这一点，我们在接下来的部分提供了查询策略的分类法，并突出了涉及不确定性的部分。有关主动学习的全面和详细介绍，请参阅[[85](#bib.bibx85)]和[[1](#bib.bibx1)]的综述。
- en: 3.1 Query Strategies
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 查询策略
- en: In Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Query Strategies ‣ 3 Active Learning ‣
    A Survey of Active Learning for Text Classification using Deep Neural Networks")
    we classify the most common AL query strategies based on a strategy’s input information,
    which denotes the numeric value(s) a strategy operates on. In our taxonomy the
    input information can be either random or one of data, model, and prediction.
    These categories are ordered by increasing complexity and are not mutually exclusive.
    Obviously, the model is a function of the data, as well as the prediction is a
    function of model and data, and moreover, in many cases a strategy use multiple
    of these criteria. In such cases we assign the query strategy to the most specific
    category (i.e. prediction-based precedes model-based, which in turn precedes data-based).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[2](#S3.F2 "图 2 ‣ 3.1 查询策略 ‣ 3 主动学习 ‣ 基于深度神经网络的文本分类主动学习综述")中，我们根据策略的输入信息对最常见的主动学习查询策略进行分类，该信息表示策略操作的数值。在我们的分类法中，输入信息可以是随机的，也可以是数据、模型和预测之一。这些类别按复杂性递增排序，并且不是相互排斥的。显然，模型是数据的函数，预测是模型和数据的函数，并且在许多情况下，一个策略使用这些标准的多个组合。在这种情况下，我们将查询策略分配到最具体的类别（即预测基础优先于模型基础，而模型基础又优先于数据基础）。
- en: '{forest}'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '{森林}'
- en: for tree= grow’=0, draw , forked edges, [query strategies, no edge, root [random,
    onode_dashed [,phantom] ] [data-based, onode [data uncertainty, onode [discriminative
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对树= grow’=0, 绘制, 分叉边缘, [查询策略, 无边缘, 根节点 [随机, onode_dashed [,phantom] ] [基于数据,
    onode [数据不确定性, onode [区分性
- en: '[[34](#bib.bibx34)], tier=leaf, tnode] ] [representativeness, onode [clustering,
    onode [flat'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[[34](#bib.bibx34)], 层级=叶子, tnode] ] [代表性, onode [聚类, onode [平面'
- en: '[[101](#bib.bibx101), [68](#bib.bibx68)], tier=leaf, tnode] [hierarchical'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[[101](#bib.bibx101), [68](#bib.bibx68)], 层级=叶子, tnode] [层次'
- en: '[[18](#bib.bibx18), [77](#bib.bibx77)], tier=leaf, tnode] ] [set construction,
    onode [core-set'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[[18](#bib.bibx18), [77](#bib.bibx77)], 层级=叶子, tnode] ] [集合构造, onode [核心集'
- en: '[[84](#bib.bibx84), [78](#bib.bibx78)], tier=leaf, tnode] ] ] ] [model-based,
    onode [model uncertainty, onode [UNC-IE'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[[84](#bib.bibx84), [78](#bib.bibx78)], 层级=叶子, tnode] ] ] ] [基于模型, onode [模型不确定性,
    onode [UNC-IE'
- en: '[[87](#bib.bibx87)], tier=leaf, tnode] ] [expected parameter'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[[87](#bib.bibx87)], 层级=叶子, tnode] ] [期望参数'
- en: change, onode [expected gradient length
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 变化, onode [期望梯度长度
- en: '[[86](#bib.bibx86), [104](#bib.bibx104)], tier=leaf, tnode] [expected weight
    change'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[[86](#bib.bibx86), [104](#bib.bibx104)], 层级=叶子, tnode] [期望权重变化'
- en: '[[96](#bib.bibx96)], tier=leaf, tnode] ] [adversarial, tnode [DFAL'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[[96](#bib.bibx96)], 层级=叶子, tnode] ] [对抗性, tnode [DFAL'
- en: '[[22](#bib.bibx22)], tier=leaf, tnode] ] ] [prediction-based, name=prediction_based,
    onode [prediction'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[[22](#bib.bibx22)], 层级=叶子, tnode] ] ] [基于预测, name=prediction_based, onode
    [预测'
- en: uncertainty, name=prediction_uncertainty, tnode [probabilistic, onode [uncertainty
    sampling [[55](#bib.bibx55)], tier=leaf,tnode] ] [margin-based, onode [version
    space
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性, name=prediction_uncertainty, tnode [概率, onode [不确定性采样 [[55](#bib.bibx55)],
    层级=叶子,tnode] ] [基于边际, onode [版本空间
- en: '[[95](#bib.bibx95)], tier=leaf,tnode] [closest to hyperplane'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[[95](#bib.bibx95)], 层级=叶子,tnode] [最接近超平面'
- en: '[[83](#bib.bibx83)], tier=leaf,tnode] ] [entropy, name=entropy, onode [BALD'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[[83](#bib.bibx83)], 层级=叶子,tnode] ] [熵, name=entropy, onode [BALD'
- en: '[[40](#bib.bibx40)], tier=leaf,tnode] ] ] [discriminative, onode [DAL'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[[40](#bib.bibx40)], 层级=叶子,tnode] ] ] [区分性, onode [DAL'
- en: '[[29](#bib.bibx29)], tier=leaf , tnode] ] [expected prediction change'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[[29](#bib.bibx29)], tier=leaf , tnode] ] [预期预测变化'
- en: ', onode [expected error reduction'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ', onode [预期错误减少'
- en: '[[81](#bib.bibx81)], name=lastnode, tier=leaf, tnode] ] [disagreement, name=disagreement,
    onode_dashed [,phantom, tier=leaf] ] ] ] \draw[zlevel,-] (disagreement.east) -|
    (entropy.south); \draw[zlevel,-] (entropy.north) |- (prediction_uncertainty.east);
    currentboundingbox.south)coordinates1); \draw[decorate,decoration=brace,amplitude=1em,mirror]
    ([yshift=-3pt]prediction_based.south west|-s1) – node[below=1em] class ([yshift=-3pt,xshift=-1pt]prediction_based.south
    east|-s1); \draw[decorate,decoration=brace,amplitude=1em,mirror] ([yshift=-3pt,xshift=1pt]prediction_based.south
    east|-s1) – node[below=1em] subclass(es) ([yshift=-3pt,xshift=-1pt]lastnode.south
    west|-s1); \draw[decorate,decoration=brace,amplitude=1em,mirror] ([yshift=-3pt,xshift=1pt]lastnode.south
    west|-s1) – node[below=1em] example ([yshift=-3pt,xshift=-1pt]lastnode.south east|-s1);'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[[81](#bib.bibx81)], name=lastnode, tier=leaf, tnode] ] [disagreement, name=disagreement,
    onode_dashed [,phantom, tier=leaf] ] ] ] \draw[zlevel,-] (disagreement.east) -|
    (entropy.south); \draw[zlevel,-] (entropy.north) |- (prediction_uncertainty.east);
    currentboundingbox.south)coordinates1); \draw[decorate,decoration=brace,amplitude=1em,mirror]
    ([yshift=-3pt]prediction_based.south west|-s1) – node[below=1em] class ([yshift=-3pt,xshift=-1pt]prediction_based.south
    east|-s1); \draw[decorate,decoration=brace,amplitude=1em,mirror] ([yshift=-3pt,xshift=1pt]prediction_based.south
    east|-s1) – node[below=1em] subclass(es) ([yshift=-3pt,xshift=-1pt]lastnode.south
    west|-s1); \draw[decorate,decoration=brace,amplitude=1em,mirror] ([yshift=-3pt,xshift=1pt]lastnode.south
    west|-s1) – node[below=1em] example ([yshift=-3pt,xshift=-1pt]lastnode.south east|-s1);'
- en: 'Figure 2: A taxonomy of query strategies for AL. The key distinction is at
    the first level, where the query strategies are categorized by their access to
    different kinds of input information. From the second to the penultimate level
    we form coherent subclasses, and the final level shows examples for the respective
    class. This taxonomy is not exhaustive due to the abundance of existing query
    strategies, and it is biased towards query strategies in NLP.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：AL查询策略的分类。关键区别在于第一层，在这一层，查询策略根据其访问不同类型输入信息的能力进行分类。从第二层到倒数第二层，我们形成一致的子类，最后一层展示了各自类别的示例。由于现有查询策略的丰富性，这个分类并不详尽，并且偏向于NLP中的查询策略。
- en: Random
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机
- en: Randomness has traditionally been used as a baseline for many tasks. In this
    case, random sampling selects instances at random and is a strong baseline for
    AL instance selection [[55](#bib.bibx55), [83](#bib.bibx83), [81](#bib.bibx81)].
    It often performs competitive to more sophisticated strategies, especially when
    the labeled pool has grown larger [[84](#bib.bibx84), [22](#bib.bibx22)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 随机性传统上被用作许多任务的基准。在这种情况下，随机抽样是以随机方式选择实例的，并且是AL实例选择的强基准[[55](#bib.bibx55), [83](#bib.bibx83),
    [81](#bib.bibx81)]。它的表现通常与更复杂的策略相竞争，特别是当标记池变得更大时[[84](#bib.bibx84), [22](#bib.bibx22)]。
- en: Data-based
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于数据
- en: Data-based strategies have the lowest level of knowledge, i.e. they only operate
    on the raw input data and optionally the labels of the labeled pool. We categorize
    them further into (1) strategies relying on data-uncertainty, which may use information
    about the data distribution, label distribution, and label correlation, and (2)
    representativeness, which tries to geometrically compress a set of points, by
    using fewer representative instances to represent the properties of the entirety.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于数据的策略具有最低的知识水平，即它们只处理原始输入数据和（可选的）标记池的标签。我们进一步将它们分类为（1）依赖于数据不确定性的策略，这些策略可能使用有关数据分布、标签分布和标签相关性的信息，以及（2）代表性策略，这些策略尝试通过使用较少的代表性实例来表示整个数据集的属性。
- en: Model-based
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于模型
- en: The class of model-based strategies has knowledge about both the data and the
    model. These strategies query instances based on measure provided by the model
    given an instance. An example for this would be a measure of confidence for the
    model’s explanation of the given instance [[26](#bib.bibx26)], for example, how
    reliable the model rates encountered features. This can also be an expected quantity,
    for example in terms of the gradient’s magnitude [[86](#bib.bibx86)]. While predictions
    from the model can still be obtained, we impose the restriction that the target
    metric must be an (observed or expected) quantity of the model, excluding the
    final prediction. Model-based uncertainty is a noteworthy subclass here, which
    operates using the uncertainty of a model’s weights [[26](#bib.bibx26)]. [[87](#bib.bibx87)]
    describe a similar class, in which the uncertainty stems from not finding enough
    evidence in the training data, i.e. failing to separate classes at training time.
    They refer to this kind of uncertainty as insufficient evidence uncertainty.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 模型基于的策略了解数据和模型。这些策略基于模型为给定实例提供的度量来查询实例。例如，这可以是模型对给定实例的解释的置信度度量[[26](#bib.bibx26)]，例如，模型如何评估遇到的特征的可靠性。这也可以是一个期望的量，例如梯度的大小[[86](#bib.bibx86)]。虽然仍可以获得模型的预测，但我们规定目标度量必须是模型的（观察到的或期望的）量，不包括最终预测。模型基于的不确定性是一个值得注意的子类，它使用模型权重的不确定性进行操作[[26](#bib.bibx26)]。[[87](#bib.bibx87)]
    描述了一个类似的类别，其中不确定性源于在训练数据中找不到足够的证据，即在训练时无法分离类。他们将这种不确定性称为证据不足不确定性。
- en: Prediction-based
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于预测
- en: Prediction-based strategies select instances by scoring their prediction output.
    The most prominent members of this class are prediction-uncertainty-based and
    disagreement-based approaches. [[87](#bib.bibx87)] denote prediction-based uncertainty
    by conflicting-evidence uncertainty, which they, contrary to this work, count
    as another form of model-based uncertainty. There is sometimes only a thin line
    between the concepts of model-based und prediction-based uncertainty. Roughly
    speaking, prediction-based uncertainty corresponds in a classification setting
    to inter-class uncertainty, as opposed to model-based uncertainty, which corresponds
    to intra-class uncertainty. In literature, uncertainty sampling [[55](#bib.bibx55)]
    usually refers to prediction-based uncertainty, unless otherwise specified.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于预测的策略通过评分其预测输出来选择实例。这一类中最突出的成员是基于预测不确定性和基于分歧的方法。[[87](#bib.bibx87)] 将预测不确定性标记为冲突证据不确定性，而他们与本研究相反地，将其视为另一种形式的基于模型的不确定性。在模型基于和预测基于不确定性之间，有时只有一线之隔。大致而言，在分类设置中，预测基于的不确定性对应于类间不确定性，而模型基于的不确定性对应于类内不确定性。在文献中，不确定性采样[[55](#bib.bibx55)]
    通常指的是预测基于的不确定性，除非另有说明。
- en: Ensembles
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集成
- en: 'When a query strategy combines the output of multiple other strategies, this
    is called an ensemble. We only classify the concept of ensemble strategies within
    the taxonomy (see disagreement-based subclass in Figure [2](#S3.F2 "Figure 2 ‣
    3.1 Query Strategies ‣ 3 Active Learning ‣ A Survey of Active Learning for Text
    Classification using Deep Neural Networks")) without going into detail due to
    several reasons: (1) Ensembles are again composed of primitive query strategies,
    which can be classified using our taxonomy. (2) Ensembles can be hybrids, i.e.
    they can be a mixture of different classes of query strategies. Moreover, the
    output of an ensemble is usually a function of the disagreement among the single
    classifiers, which is already covered in previous surveys of [[71](#bib.bibx71)]
    and [[25](#bib.bibx25)].'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当查询策略结合多个其他策略的输出时，这称为集成。由于多种原因，我们仅在分类法中对集成策略进行分类（见图 [2](#S3.F2 "Figure 2 ‣ 3.1
    Query Strategies ‣ 3 Active Learning ‣ A Survey of Active Learning for Text Classification
    using Deep Neural Networks)），而不详细讨论： (1) 集成策略再次由原始查询策略组成，这些策略可以使用我们的分类法进行分类。 (2)
    集成策略可以是混合型的，即它们可以是不同类别查询策略的混合。此外，集成的输出通常是单个分类器之间分歧的函数，这在之前的调查中已经涵盖[[71](#bib.bibx71)]
    和 [[25](#bib.bibx25)]。
- en: 'We are not the first to provide a classification of query strategies: [[1](#bib.bibx1)]
    provide an alternative classification, which divides the query strategies into
    heterogenity-based models, performance-based models, and representativeness-based
    models. Heterogenity-based models try to sample diverse data points, w.r.t the
    current labeled pool. This class includes among others uncertainty sampling and
    ensembles, i.e. no distinction is made between ensembles and single-model strategies.
    Performance-based models aim to sample data targeting an increase of the models
    performance, for example a reduction in the model’s error. This intersects with
    our model-based class, however, it lacks strategies which focus on a change of
    parameters (e.g., expected gradient length [[86](#bib.bibx86)]) as opposed to
    changes in a metric. Lastly, representativeness-based strategies sample instances
    so that the distribution of the subsample is as similar as possible to the training
    set. Although similar to our data-based class, they always assume the existence
    of a model, which is not the case for data-based strategies.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不是第一个提供查询策略分类的人：[[1](#bib.bibx1)] 提供了一种替代分类，将查询策略分为基于异质性的模型、基于性能的模型和基于代表性的模型。基于异质性的模型试图对当前标记池中的数据点进行多样化采样。此类模型包括不确定性采样和集成方法，即没有区分集成方法和单模型策略。基于性能的模型旨在采样以提高模型的性能，例如减少模型的误差。这与我们的模型驱动类交叉，但它缺乏关注参数变化（例如，预期梯度长度[[86](#bib.bibx86)]）的策略，与度量变化不同。最后，基于代表性的策略采样实例，使子样本的分布尽可能类似于训练集。虽然与我们的数据驱动类相似，但它们总是假设存在一个模型，而数据驱动策略则没有这种假设。
- en: '[[25](#bib.bibx25)] separate query strategies into uncertainty-based and diversity-based
    classes. Uncertainty-based strategies assume the i.i.d. distribution of instances;
    they compute a separate score for each instance, which is the basis for the instance
    selection. Diversity-based strategies are a superset thereof and additionally
    consider correlation amongst instances. Thereby they characterize uncertainty
    and correlation as critical components for query strategies. This classification
    successfully distinguishes query strategies by considering exclusively uncertainty
    and correlation. However, it is less transparent in terms of the input information,
    which our taxonomy highlights. Nevertheless, correlation is a factor orthogonal
    to our taxonomy and can be added as an additional criterion.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[[25](#bib.bibx25)] 将查询策略分为基于不确定性和基于多样性的类。基于不确定性的策略假设实例是独立同分布的；它们为每个实例计算一个单独的分数，这成为实例选择的基础。基于多样性的策略是其超集，并且额外考虑实例之间的相关性。因此，它们将不确定性和相关性作为查询策略的关键组成部分。这种分类通过仅考虑不确定性和相关性成功地区分了查询策略。然而，它在输入信息方面不如我们的分类体系透明。尽管如此，相关性是与我们的分类体系正交的因素，可以作为附加标准添加。'
- en: After creating our taxonomy, we discovered a recent categorization of uncertainty
    in deep learning [[26](#bib.bibx26)], which distinguishes between data-, model-,
    and predictive-uncertainty, similar to the taxonomy’s first level (data-, model-,
    prediction-based query strategies). Although this classification comes naturally
    from the data’s degree of processing, we emphasize that we are not the first to
    come up with this abstraction.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建我们的分类体系后，我们发现了深度学习中对不确定性的最近分类[[26](#bib.bibx26)]，它区分了数据不确定性、模型不确定性和预测不确定性，这与分类体系的第一级（数据驱动、模型驱动、基于预测的查询策略）相似。尽管这种分类自然地源于数据处理的程度，我们强调我们并不是首个提出这种抽象概念的。
- en: By using the input information as decisive criterion, this taxonomy provides
    an information-oriented view on query strategies. It highlights in which parts
    and how uncertainty has been involed in existing query strategies. Uncertainty
    in terms of NNs is, however, is known to be challenging as described in Section
    [3.2](#S3.SS2 "3.2 Neural-Network-Based Active Learning ‣ 3 Active Learning ‣
    A Survey of Active Learning for Text Classification using Deep Neural Networks").
    Moreover, we use the taxonomy to categorize recent work in AL for text classification
    in Section [4.3](#S4.SS3 "4.3 Commonalities and Limitations of Previous Experiments
    ‣ 4 Active Learning for Text Classification ‣ A Survey of Active Learning for
    Text Classification using Deep Neural Networks").
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用输入信息作为决定性标准，这一分类法提供了一种信息导向的查询策略视角。它突出显示了不确定性在现有查询策略中的哪些部分以及如何涉及。然而，正如第[3.2](#S3.SS2
    "3.2 Neural-Network-Based Active Learning ‣ 3 Active Learning ‣ A Survey of Active
    Learning for Text Classification using Deep Neural Networks")节所述，神经网络中的不确定性被认为是具有挑战性的。此外，我们使用这一分类法来对第[4.3](#S4.SS3
    "4.3 Commonalities and Limitations of Previous Experiments ‣ 4 Active Learning
    for Text Classification ‣ A Survey of Active Learning for Text Classification
    using Deep Neural Networks")节中的文本分类主动学习的最新工作进行分类。
- en: 3.2 Neural-Network-Based Active Learning
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于神经网络的主动学习
- en: 'In this section we investigate the question, why neural networks are not more
    prevalent in AL applications. This can be attributed to two central topics: Uncertainty
    estimation in NNs, and the contrast of NNs requiring between big data and AL dealing
    with small data. We examine these issues from a NN perspective, alleviating the
    NLP focus.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们探讨了神经网络在主动学习（AL）应用中为何不更为普及的问题。这可以归因于两个核心主题：神经网络中的不确定性估计，以及神经网络对大数据的需求与主动学习处理小数据之间的对比。我们从神经网络的角度来审视这些问题，减轻了自然语言处理（NLP）的关注。
- en: Previous Work
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 先前的工作
- en: Early research in NN-based AL can be divided into uncertainty-based [[16](#bib.bibx16)],
    and ensemble-based [[50](#bib.bibx50), [63](#bib.bibx63)] strategies. The former
    often use prediction entropy [[62](#bib.bibx62), [81](#bib.bibx81)] as measure
    of uncertainty, while the latter utilize the disagreement among the single classifiers.
    [[86](#bib.bibx86)] proposed the expected gradient length (EGL) query strategy,
    which selects instances by the expected change in the model’s weights. [[104](#bib.bibx104)]
    were first to use a CNN for AL. They proposed a variant of the expected gradient
    length strategy [[86](#bib.bibx86)], in which they select instances that are expected
    to result in the largest change in embedding space, thereby training highly discriminative
    representations. [[84](#bib.bibx84)] observed uncertainty-based query strategies
    not to be effective for CNN-based batch-mode AL, and proposed core-set selection,
    which samples a small subset to represent the full dataset. [[5](#bib.bibx5)]
    proposed BADGE, a query strategy for DNNs, which uses k-means++ seeding [[4](#bib.bibx4)]
    on the gradients of the final layer, in order to query by uncertainty and diversity.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的基于神经网络的主动学习研究可以分为基于不确定性的[[16](#bib.bibx16)]和基于集成的[[50](#bib.bibx50), [63](#bib.bibx63)]策略。前者通常使用预测熵[[62](#bib.bibx62),
    [81](#bib.bibx81)]作为不确定性的度量，而后者利用单个分类器之间的不一致[[86](#bib.bibx86)]提出了期望梯度长度（EGL）查询策略，该策略通过模型权重的预期变化来选择实例。[[104](#bib.bibx104)]首次将卷积神经网络（CNN）用于主动学习。他们提出了一种期望梯度长度策略的变体[[86](#bib.bibx86)]，其中选择预计会在嵌入空间中导致最大变化的实例，从而训练高度可区分的表示。[[84](#bib.bibx84)]观察到基于不确定性的查询策略对于基于CNN的批量模式主动学习并不有效，并提出了核心集合选择，该方法从小的子集中抽样以代表整个数据集。[[5](#bib.bibx5)]提出了BADGE，一种针对深度神经网络（DNNs）的查询策略，使用k-means++初始化[[4](#bib.bibx4)]在最终层的梯度上，以便通过不确定性和多样性进行查询。
- en: 'Finally, Generative Adversarial Networks (GANs; [[30](#bib.bibx30)]) have also
    been applied successfully for AL tasks: [[106](#bib.bibx106)] use GANs for query
    synthesis of images within an active learner using an SVM model. The instances
    are synthesized so that they would be classified with high uncertainty. The authors
    report this approach to outperform random sampling, pool-based uncertainty sampling
    using an SVM [[95](#bib.bibx95)], and in some cases passive learning, while having
    the weakness to generate too similar instances. The approach itself is neither
    pure NN-based, nor does it belong to the pool-based scenario, however, it is the
    first reported use of GANs for AL. [[22](#bib.bibx22)] use adversarial attacks
    to find instances that cross the decision boundary with the aim to increase the
    model robustness. They train two CNN architectures and report results superior
    to the core-set [[84](#bib.bibx84)] strategy on image classification tasks. It
    is obvious that GANs inherently belong to the membership query synthesis scenario.
    Therefore their performance correlates with the quality of artificial data synthesis,
    i.e. they are usually not that effective for NLP tasks. This has already been
    recognized and first improvements towards a better text generation have been made
    [[105](#bib.bibx105)].'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，生成对抗网络（GANs; [[30](#bib.bibx30)]）也已成功应用于主动学习（AL）任务：[[106](#bib.bibx106)]
    使用GANs对图像进行查询合成，利用SVM模型的主动学习器。实例经过合成，使其被分类时具有较高的不确定性。作者报告称，这种方法优于随机抽样、基于池的不确定性抽样（使用SVM
    [[95](#bib.bibx95)]），在某些情况下优于被动学习，但缺点是生成的实例过于相似。该方法本身既不是纯粹的神经网络（NN）方法，也不属于基于池的场景，然而，这是首次报告将GANs用于主动学习。[[22](#bib.bibx22)]
    使用对抗攻击找到跨越决策边界的实例，以提高模型的鲁棒性。他们训练了两个CNN架构，并报告其在图像分类任务上优于核心集 [[84](#bib.bibx84)]
    策略。显然，GANs 本质上属于成员查询合成场景。因此，它们的性能与人工数据合成的质量相关，即通常在自然语言处理（NLP）任务中效果不佳。这一点已经被认识到，并且已经对更好的文本生成进行了初步改进
    [[105](#bib.bibx105)]。
- en: Uncertainty in Neural Networks
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络中的不确定性
- en: One of the earliest and in many variations adopted class of strategies is uncertainty
    sampling [[83](#bib.bibx83), [95](#bib.bibx95)]. Unfortunately, this widely-used
    concept is not straightforward to apply for NNs, as they do not provide an inherent
    indicator of uncertainty. In the past, this has been tackled among others by ensembling
    [[50](#bib.bibx50), [36](#bib.bibx36), [12](#bib.bibx12)], or by learning error
    estimates [[70](#bib.bibx70)]. More recent approaches furthermore use Bayesian
    extensions [[11](#bib.bibx11)], obtain uncertainty estimations using dropout [[91](#bib.bibx91),
    [27](#bib.bibx27)], or use probabilistic NNs to estimate predictive uncertainty
    [[51](#bib.bibx51)]. However, ensemble and Bayesian approaches quickly become
    infeasible on larger datasets, and NN architectures are generally known to be
    overconfident in their predictions [[33](#bib.bibx33), [51](#bib.bibx51)]. Consequently,
    uncertainty in NNs is only insufficiently solved and therefore still remains a
    highly relevant research area.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最早采用的策略之一是**不确定性抽样** [[83](#bib.bibx83), [95](#bib.bibx95)]。不幸的是，这一广泛使用的概念在应用于神经网络（NNs）时并不简单，因为它们不提供固有的不确定性指示。过去，这一问题通过集成
    [[50](#bib.bibx50), [36](#bib.bibx36), [12](#bib.bibx12)] 或学习误差估计 [[70](#bib.bibx70)]
    等方法得到解决。最近的方法还使用贝叶斯扩展 [[11](#bib.bibx11)]，通过丢弃法 [[91](#bib.bibx91), [27](#bib.bibx27)]
    获得不确定性估计，或使用概率神经网络（NNs）来估计预测不确定性 [[51](#bib.bibx51)]。然而，集成和贝叶斯方法在较大数据集上很快变得不可行，NN架构通常被认为对其预测过于自信
    [[33](#bib.bibx33), [51](#bib.bibx51)]。因此，NN中的不确定性仍然未得到充分解决，因此仍然是一个高度相关的研究领域。
- en: Contrasting Paradigms
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对比范式
- en: DNNs are known to excel in particularly at large-scale datasets, but often having
    large amounts of data available is a strict requirement to perform well at all
    (e.g., [[103](#bib.bibx103)]). AL on the other hand tries to minimize the labeled
    data. The small labeled datasets can be a problem for DNNs, since they are known
    to overfit on small datasets (e.g., [[93](#bib.bibx93), [100](#bib.bibx100)]),
    which results in bad generalization performance on the test set. Moreover, DNNs
    often offer little advantage over shallow models when they are trained using small
    datasets [[89](#bib.bibx89)], thereby lacking justification for their higher computational
    costs. On the other hand we clearly cannot require AL to label more data, since
    this would defeat its purpose. Therefore there has been research on dealing with
    (D)NNs using small datasets, however, it is only a scarce amount, especially in
    relation to the large volume of NN literature in general. Handling small datasets
    is mostly circumvented by using pre-training [[37](#bib.bibx37), [97](#bib.bibx97)]
    or other transfer learning approaches [[13](#bib.bibx13), [8](#bib.bibx8), [97](#bib.bibx97)].
    Finally, the search for optimal hyperparameters is often neglected and instead
    the hyperparameters of related work are used, which are optimized for large datasets,
    if at all.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: DNNs（深度神经网络）在特别大规模的数据集上表现优异，但通常需要大量的数据才能表现良好（例如，[[103](#bib.bibx103)]）。另一方面，主动学习（AL）试图减少标记数据。小型标记数据集对于DNNs可能是个问题，因为它们在小型数据集上容易过拟合（例如，[[93](#bib.bibx93)，[100](#bib.bibx100)]），导致测试集上的泛化性能差。此外，当使用小型数据集训练时，DNNs通常对浅层模型没有明显优势[[89](#bib.bibx89)]，因此其较高的计算成本缺乏合理性。另一方面，我们显然不能要求AL标记更多的数据，因为这会违背其目的。因此，已有研究探讨了如何使用小型数据集处理（D）NNs，然而这类研究仍然稀少，尤其是在大量神经网络文献的背景下。处理小型数据集大多通过使用预训练[[37](#bib.bibx37)，[97](#bib.bibx97)]或其他迁移学习方法[[13](#bib.bibx13)，[8](#bib.bibx8)，[97](#bib.bibx97)]来规避。最后，优化超参数的搜索往往被忽视，取而代之的是使用相关工作的超参数，这些超参数如果有的话，通常是针对大数据集优化的。
- en: 4 Active Learning for Text Classification
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 主动学习用于文本分类
- en: In Sections [4.1](#S4.SS1 "4.1 Recent Advances in Text Classification ‣ 4 Active
    Learning for Text Classification ‣ A Survey of Active Learning for Text Classification
    using Deep Neural Networks") and [4.2](#S4.SS2 "4.2 Text Classification for Active
    Learning ‣ 4 Active Learning for Text Classification ‣ A Survey of Active Learning
    for Text Classification using Deep Neural Networks") we first summarize recent
    methods in text classification and NNs. We elaborate on each method’s importance
    in the context of AL, and analyze its adoption by recent research where applicable.
    For insufficiently adopted methods, we present how they could advance AL for text
    classification. Most importantly, we present an overview of recent experiments
    in AL for text classification and analyze commonalities and shortcomings.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在[4.1](#S4.SS1 "4.1 文本分类的最新进展 ‣ 4 主动学习用于文本分类 ‣ 深度神经网络在文本分类中的主动学习综述")和[4.2](#S4.SS2
    "4.2 主动学习的文本分类 ‣ 4 主动学习用于文本分类 ‣ 深度神经网络在文本分类中的主动学习综述")节中，我们首先总结了文本分类和神经网络的最新方法。我们详细阐述了每种方法在AL背景下的重要性，并分析了其在最近研究中的采用情况。对于采用不足的方法，我们展示了它们如何推动文本分类的AL。最重要的是，我们提供了最近在AL用于文本分类的实验概述，并分析了其中的共性和不足。
- en: 4.1 Recent Advances in Text Classification
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 文本分类的最新进展
- en: Representations
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 表示
- en: 'Traditional methods use the bag-of-words (BoW) representation, which are sparse
    and high-dimensional. However, with the introduction of word embeddings like word2vec
    [[66](#bib.bibx66), [65](#bib.bibx65)], GloVe [[74](#bib.bibx74)], or fastText
    [[46](#bib.bibx46)], word embeddings have replaced BoW representations in many
    cases. This is due to several reasons: (1) They represent semantic relations in
    vectors space and avoid the problem of mismatching features as for example due
    to synonymy; (2) incorporating word embeddings resulted in superior performance
    for many downstream tasks [[66](#bib.bibx66), [74](#bib.bibx74), [46](#bib.bibx46)];
    (3) unlike bag-of-words, word vectors are dense, low-dimensional representations,
    which makes them applicable to a wider range of algorithms – especially in the
    context of NNs which favor fixed-size inputs. Various approaches have been presented
    in order to obtain similar fixed size representations for word sequences, i.e.
    sentences, paragraphs or documents [[53](#bib.bibx53)].'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法使用词袋（BoW）表示，这种表示方式是稀疏且高维的。然而，随着像 word2vec [[66](#bib.bibx66), [65](#bib.bibx65)]、GloVe
    [[74](#bib.bibx74)] 或 fastText [[46](#bib.bibx46)] 等词嵌入的引入，词嵌入在许多情况下已经取代了 BoW
    表示。这是由于几个原因：(1) 词嵌入在向量空间中表示语义关系，避免了由于同义词等原因导致的特征不匹配问题；(2) 融入词嵌入后，许多下游任务的性能得到了提升
    [[66](#bib.bibx66), [74](#bib.bibx74), [46](#bib.bibx46)]；(3) 与词袋模型不同，词向量是密集的、低维的表示，这使得它们适用于更广泛的算法，特别是在神经网络（NNs）等固定大小输入的背景下。为获得类似的固定大小表示（例如句子、段落或文档）已经提出了各种方法
    [[53](#bib.bibx53)]。
- en: Word embeddings are representations, which provide exactly one vector per word
    and in consequence one meaning as well. This makes them also unaware of the current
    word’s context and therefore makes them unable to detect and handle ambiguities.
    Unlike word embeddings, language models (LMs) compute the word vector using the
    word and the surrounding context [[76](#bib.bibx76)]. This results in a contextualized
    representation, which inherits the advantages of word embeddings, and at the same
    time allows for context-specific representation (in contrast to static embeddings)
    [[76](#bib.bibx76)]. ELMo was the first LM to gain wide adoption and surpassed
    state of the art models on several NLP tasks [[76](#bib.bibx76)]. Shortly thereafter,
    BERT [[20](#bib.bibx20)] was introduced and provided bidirectional pre-training-based
    language modelling. The process to create a BERT-based model consists of a pre-training
    and a fine-tuning step as opposed to ELMO’s direct feature-based approach in which
    contextualized vectors are obtained from the pre-trained model and used directly
    as features [[20](#bib.bibx20)]. By masking, i.e. randomly removing a fraction
    of tokens during training, the training was adapted to predict the masked words.
    This made the bidrectional training possible, which would otherwise be obstructed
    because a word could "see itself" when computing its probability of occurrence
    given a context [[20](#bib.bibx20)]. Following this, XLNet [[102](#bib.bibx102)]
    introduced a similar approach of pre-training and fine-tuning using an autoregressive
    language model, however, it overcame BERT’s limitation as it does not rely on
    masking data during pre-training [[102](#bib.bibx102)], and moreover, successfully
    manages to integrate the recent TransformerXL architecture [[17](#bib.bibx17)].
    Since then, a variety of LMs have been published, which further optimize the pre-training
    of previous LM architectures (e.g., RoBERTa [[59](#bib.bibx59)] and ELECTRA [[15](#bib.bibx15)]),
    or distill the knowledge into a smaller model (e.g., DistilBERT [[82](#bib.bibx82)]).
    Similarly to word embeddings, there are approaches to use LMs in order to obtain
    sentence representations from LMs [[80](#bib.bibx80)].
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是表示方法，为每个单词提供一个向量，因此也只有一个含义。这使得它们无法感知当前单词的上下文，因此无法检测和处理歧义。与词嵌入不同，语言模型（LMs）通过使用单词和周围的上下文来计算词向量[[76](#bib.bibx76)]。这产生了一个上下文化的表示，它继承了词嵌入的优点，同时允许特定于上下文的表示（与静态嵌入相对）[[76](#bib.bibx76)]。ELMo
    是第一个广泛采用的语言模型，在多个 NLP 任务上超越了最先进的模型[[76](#bib.bibx76)]。不久之后，BERT [[20](#bib.bibx20)]
    被引入，并提供了基于双向预训练的语言建模。创建基于 BERT 的模型的过程包括预训练和微调步骤，与 ELMo 的直接特征基础方法不同，在该方法中，上下文化的向量从预训练模型中获得并直接用作特征[[20](#bib.bibx20)]。通过掩蔽，即在训练过程中随机移除一部分标记，训练被调整为预测被掩蔽的词。这使得双向训练成为可能，否则由于一个单词在计算其给定上下文的发生概率时可能会“看到自己”而受到阻碍[[20](#bib.bibx20)]。随后，XLNet
    [[102](#bib.bibx102)] 介绍了一种类似的预训练和微调方法，使用自回归语言模型，然而，它克服了 BERT 的限制，因为它在预训练期间不依赖于掩蔽数据[[102](#bib.bibx102)]，此外，还成功整合了最近的
    TransformerXL 架构[[17](#bib.bibx17)]。此后，发布了各种语言模型，它们进一步优化了之前语言模型架构的预训练（例如，RoBERTa
    [[59](#bib.bibx59)] 和 ELECTRA [[15](#bib.bibx15)]），或将知识提炼到一个更小的模型中（例如，DistilBERT
    [[82](#bib.bibx82)]）。与词嵌入类似，也有使用语言模型来获得句子表示的方法[[80](#bib.bibx80)]。
- en: All mentioned representations offer a richer expressiveness than traditional
    BoW representations and therefore are well-suited for active learning purposes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所有提到的表示方法比传统的 BoW 表示提供了更丰富的表达能力，因此非常适合主动学习目的。
- en: Neural-Network-Based Text Classification
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络基础的文本分类
- en: A well-known CNN architecture presented by [[48](#bib.bibx48)] (KimCNN) operates
    on pre-trained word vectors and achieved state of the art results at the time
    using only a simple but elegant architecture. The investigated CNN setups did
    not require much hyperparameter tuning and confirmed the effectiveness of dropout
    [[91](#bib.bibx91)] as a regularizer for CNN-based text classification.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的 CNN 架构由 [[48](#bib.bibx48)]（KimCNN）提出，基于预训练的词向量，并且当时仅使用简单而优雅的架构就达到了最先进的结果。研究的
    CNN 设置不需要太多的超参数调整，并且确认了 dropout [[91](#bib.bibx91)] 作为 CNN 基础文本分类的正则化器的有效性。
- en: The word embeddings of fastText [[46](#bib.bibx46)] differ from other word embeddings
    in the sense that the approach is (1) supervised and (2) specifically designed
    for text classification. Being a shallow neural network, it is still very efficient,
    while still obtaining performances comparable to deep learning approaches at that
    time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: fastText [[46](#bib.bibx46)] 的词嵌入与其他词嵌入的不同之处在于，该方法是 (1) 监督的，并且 (2) 专门为文本分类设计的。作为一个浅层神经网络，它仍然非常高效，同时在当时能够获得与深度学习方法相当的性能。
- en: '[[41](#bib.bibx41)] developed Universal Language Model Fine-tuning (ULMFiT),
    a LM transfer learning method using the AWD-LSTM architecture [[64](#bib.bibx64)],
    which outperformed the state of the art on several text classification datasets
    when trained on only $100$ labeled examples, and thereby achieved results significantly
    superior to more sophisticated architectures of previous work. Context-specific
    LMs like BERT [[20](#bib.bibx20)] and XLNet [[102](#bib.bibx102)] yield a context-dependent
    vector for each token, thereby strongly improving NN-based text classification
    [[20](#bib.bibx20), [102](#bib.bibx102), [92](#bib.bibx92)]. State of the art
    in NN-based text classification is LM-based fine-tuning with XLNet, which has
    a slight edge over BERT in terms of test error rate [[102](#bib.bibx102), [92](#bib.bibx92)].
    ULMFiT follows closely thereafter, and KimCNN is still a strong contender. Notably,
    ULMFiT, BERT and XLNet all perform transfer learning, which aims to transfer knowledge
    from one model to another [[79](#bib.bibx79), [13](#bib.bibx13)], thereby massively
    reducing the required amounts of data.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[[41](#bib.bibx41)] 开发了通用语言模型微调（ULMFiT），这是一种使用 AWD-LSTM 架构的语言模型迁移学习方法 [[64](#bib.bibx64)]，在仅用
    $100$ 个标注示例进行训练时，表现超越了多个文本分类数据集上的最先进技术，从而取得了显著优于先前更复杂架构的结果。像 BERT [[20](#bib.bibx20)]
    和 XLNet [[102](#bib.bibx102)] 这样的上下文特定语言模型为每个标记生成一个上下文相关的向量，从而显著提高了基于神经网络的文本分类
    [[20](#bib.bibx20), [102](#bib.bibx102), [92](#bib.bibx92)]。基于神经网络的文本分类领域的最先进技术是基于语言模型的
    XLNet 微调，其在测试错误率上相较于 BERT 稍有优势 [[102](#bib.bibx102), [92](#bib.bibx92)]。ULMFiT
    紧随其后，而 KimCNN 仍然是一个强有力的竞争者。值得注意的是，ULMFiT、BERT 和 XLNet 都执行迁移学习，旨在将知识从一个模型转移到另一个模型
    [[79](#bib.bibx79), [13](#bib.bibx13)]，从而大大减少了所需的数据量。'
- en: 4.2 Text Classification for Active Learning
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主动学习中的文本分类
- en: Traditional AL for text classification heavily relied on query strategies based
    on prediction-uncertainty [[55](#bib.bibx55)] and ensembling [[58](#bib.bibx58)].
    Common model choices included support vector machines (SVMs; [[95](#bib.bibx95)]),
    naive bayes [[69](#bib.bibx69)], logistic regression [[39](#bib.bibx39)] and neural
    networks [[50](#bib.bibx50)]. To the best of our knowledge, no previous survey
    covered traditional AL for text classification, however, ensembling-based AL for
    NLP has been covered in depth by [[71](#bib.bibx71)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的主动学习（AL）文本分类方法严重依赖于基于预测不确定性的查询策略 [[55](#bib.bibx55)] 和集成 [[58](#bib.bibx58)]。常见的模型选择包括支持向量机（SVMs;
    [[95](#bib.bibx95)]）、朴素贝叶斯 [[69](#bib.bibx69)]、逻辑回归 [[39](#bib.bibx39)] 和神经网络
    [[50](#bib.bibx50)]。据我们所知，之前的调查没有涵盖传统的主动学习文本分类，然而，基于集成的主动学习在自然语言处理领域已由 [[71](#bib.bibx71)]
    深入探讨。
- en: 'Regarding modern NN-based AL for text classification, the relevant models are
    primarily CNN- and LSTM-based deep architectures: [[104](#bib.bibx104)] claim
    to be the first to consider AL for text classification using DNNs. They use CNNs
    and contribute a query strategy, which selects the instances based on the expected
    change of the word embeddings and the model’s uncertainty given the instance,
    thereby learning discriminative embeddings for text classification. [[2](#bib.bibx2)]
    evaluated SVM, LSTM and gated recurrent unit (GRU; [[14](#bib.bibx14)]) models,
    and reported that the latter two significantly outperformed the SVM baseline on
    the Chinese news dataset ThucNews. [[61](#bib.bibx61)] investigated the performance
    of different text representations in a pool-based AL scenario. They compared frequency-based
    text representations, word embeddings and transformer-based representations used
    as input features for a SVM-based AL and different query strategies, in which
    transformer-based representations yielded consistently higher scores. [[78](#bib.bibx78)]
    investigate sampling bias and apply active text classification on the large scale
    text corpora of [[103](#bib.bibx103)]. They demonstrate FastText.zip [[47](#bib.bibx47)]
    with (entropy-based) uncertainty sampling to be a strong baseline, which is competitive
    compared to recent approaches in active text classification. Moreover, they use
    this strategy to obtain a surrogate dataset (comprising from 5% to 40% of the
    total data) on which a LSTM-based LM is trained using ULMFiT [[41](#bib.bibx41)],
    reaching accuracy levels close to a training on the full dataset. Unlike past
    publications, they report this uncertainty-based strategy to be effective, robust,
    and at the same time computationally cheap. This is the most relevant work in
    terms of the intersection between text classification, NNs and DL.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 关于现代基于神经网络的主动学习（AL）在文本分类中的应用，相关模型主要是基于CNN和LSTM的深度架构：[[104](#bib.bibx104)] 号称是首个考虑使用DNN进行文本分类的主动学习研究。他们使用CNN，并提出了一种查询策略，该策略基于单词嵌入的预期变化和给定实例的模型不确定性来选择实例，从而学习出用于文本分类的判别性嵌入。[[2](#bib.bibx2)]
    评估了SVM、LSTM和门控递归单元（GRU；[[14](#bib.bibx14)]）模型，并报告称后两者在中文新闻数据集ThucNews上的表现显著优于SVM基线。[[61](#bib.bibx61)]
    研究了不同文本表示在池化型主动学习场景中的表现。他们比较了频率基于的文本表示、单词嵌入和基于变换器的表示，这些表示被用作SVM基于主动学习的输入特征，其中基于变换器的表示
    consistently yielded higher scores。[[78](#bib.bibx78)] 调查了采样偏差，并在[[103](#bib.bibx103)]的大规模文本语料库上应用了主动文本分类。他们展示了使用（基于熵的）不确定性采样的FastText.zip
    [[47](#bib.bibx47)] 是一个强有力的基线，与近期的主动文本分类方法相竞争。此外，他们利用这一策略获得了一个替代数据集（占总数据的5%到40%），在此数据集上使用ULMFiT
    [[41](#bib.bibx41)] 训练了一个基于LSTM的语言模型，达到了接近于在完整数据集上训练的准确度水平。与过去的出版物不同，他们报告了这种基于不确定性的策略是有效的、稳健的，同时计算成本低。这是文本分类、神经网络和深度学习交集中的最相关工作。
- en: '| Publication | Datasets | Model(s) | Query Strategy Class(es) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 出版物 | 数据集 | 模型 | 查询策略类别 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| [[44](#bib.bibx44)] | 20N, R21, RV2, SPM | NB, SVM, kNN | 1\. Prediction
    uncertainty (LC) 2\. Prediction uncertainty (CTH)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '| [[44](#bib.bibx44)] | 20N, R21, RV2, SPM | NB, SVM, kNN | 1\. 预测不确定性（LC）
    2\. 预测不确定性（CTH）'
- en: 3\. Prediction uncertainty (disagreement) |
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 预测不确定性（分歧） |
- en: '| [[104](#bib.bibx104)] | CR, MR, SJ, MRL, MUR, DR | CNN | 1\. Model uncertainty
    (EGL) 2\. Prediction Uncertainty (entropy) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| [[104](#bib.bibx104)] | CR, MR, SJ, MRL, MUR, DR | CNN | 1\. 模型不确定性（EGL）
    2\. 预测不确定性（熵） |'
- en: '| [[10](#bib.bibx10)] | RMA | SVM | 1\. Prediction uncertainty (CTH) 2\. Prediction
    uncertainty (disagreement) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| [[10](#bib.bibx10)] | RMA | SVM | 1\. 预测不确定性（CTH） 2\. 预测不确定性（分歧） |'
- en: '| [[90](#bib.bibx90)] | TQA, MR | SVM, CNN, BiLSTM | Prediction uncertainty
    (disagreement) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| [[90](#bib.bibx90)] | TQA, MR | SVM, CNN, BiLSTM | 预测不确定性（分歧） |'
- en: '| [[60](#bib.bibx60)] | MR, SJ, TQA, CR | SVM, CNN, BiLSTM | 1\. Prediction
    uncertainty (entropy) 2\. Prediction uncertainty (disagreement) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| [[60](#bib.bibx60)] | MR, SJ, TQA, CR | SVM, CNN, BiLSTM | 1\. 预测不确定性（熵）
    2\. 预测不确定性（分歧） |'
- en: '| [[78](#bib.bibx78)] | SGN, DBP, YHA, YRP, YRF, AGN, ARP, ARF | FTZ, ULMFiT
    | Prediction uncertainty (entropy) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| [[78](#bib.bibx78)] | SGN, DBP, YHA, YRP, YRF, AGN, ARP, ARF | FTZ, ULMFiT
    | 预测不确定性（熵） |'
- en: '| [[61](#bib.bibx61)] | MRL, MDS, BAG, G13, ACR, SJ, AGN, DBP | SVM | 1\. Prediction
    uncertainty (CTH) 2\. Prediction uncertainty (disagreement)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '| [[61](#bib.bibx61)] | MRL, MDS, BAG, G13, ACR, SJ, AGN, DBP | SVM | 1\. 预测不确定性（CTH）
    2\. 预测不确定性（分歧）'
- en: 3\. Data-based (EGAL)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 基于数据（EGAL）
- en: 4\. Data-based (density) |
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 基于数据（密度） |
- en: 'Table 1: An overview of recent work on AL for text classification. We referred
    to the datasets using short keys, which can be looked up in Table LABEL:tab:datasets
    in the Appendix. Models: Naive Bayes (NB), Support Vector Machine (SVM), k-Nearest
    Neighbours (kNN), Convolutional Neural Network (CNN), [Bidirectional] Long Short-Term
    Memory ([Bi]LSTM), FastText.zip (FTZ), Univeral Language Model Fine-Tuning (ULMFiT).
    Query strategies: Least confidence (LC), Closest-to-hyperplane (CTH), expected
    gradient length (EGL). Random selection baselines were omitted.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：最近文本分类主动学习工作的概述。我们使用简短的键引用数据集，这些键可以在附录中的表 LABEL:tab:datasets 中查找。模型：朴素贝叶斯（NB）、支持向量机（SVM）、k-最近邻（kNN）、卷积神经网络（CNN）、[双向]
    长短期记忆网络（[Bi]LSTM）、FastText.zip（FTZ）、通用语言模型微调（ULMFiT）。查询策略：最少置信度（LC）、最近超平面（CTH）、期望梯度长度（EGL）。随机选择的基线被省略。
- en: 4.3 Commonalities and Limitations of Previous Experiments
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 先前实验的共同点和局限性
- en: Table [1](#S4.T1 "Table 1 ‣ 4.2 Text Classification for Active Learning ‣ 4
    Active Learning for Text Classification ‣ A Survey of Active Learning for Text
    Classification using Deep Neural Networks") shows the most recent AL for text
    classification experiments, all of them more recent than the surveys of [[85](#bib.bibx85)]
    and [[71](#bib.bibx71)]. For each publication we list the utilized datasets, models,
    and classes of query strategies (with respect to the taxonomy in Section [3.1](#S3.SS1
    "3.1 Query Strategies ‣ 3 Active Learning ‣ A Survey of Active Learning for Text
    Classification using Deep Neural Networks")). We present this table in order to
    get insights about the recently preferred classification models and query strategy
    classes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表[1](#S4.T1 "表 1 ‣ 4.2 主动学习的文本分类 ‣ 4 主动学习在文本分类中的应用 ‣ 基于深度神经网络的主动学习文本分类综述")展示了最近的文本分类主动学习实验，这些实验都比[[85](#bib.bibx85)]和[[71](#bib.bibx71)]的综述更新。对于每个出版物，我们列出了所使用的数据集、模型和查询策略的类别（根据[3.1](#S3.SS1
    "3.1 查询策略 ‣ 3 主动学习 ‣ 基于深度神经网络的主动学习文本分类综述")节中的分类法）。我们呈现这张表是为了获得对最近偏好的分类模型和查询策略类别的见解。
- en: 'We can draw multiple conclusions from Table [1](#S4.T1 "Table 1 ‣ 4.2 Text
    Classification for Active Learning ‣ 4 Active Learning for Text Classification
    ‣ A Survey of Active Learning for Text Classification using Deep Neural Networks"):
    It is obvious that a significant majority of these query strategies belong to
    the class of prediction-based query strategies, more specifically to the prediction-uncertainty
    and disagreement-based sub-classes. In addition to that, we can identify several
    shortcomings: First, in many experiments two or more standard datasets are evaluated,
    but very often there is little to no intersection between the experiments in terms
    of their datasets. As a result we lose comparability against previous research.
    For recent research, this can seen in Table [1](#S4.T1 "Table 1 ‣ 4.2 Text Classification
    for Active Learning ‣ 4 Active Learning for Text Classification ‣ A Survey of
    Active Learning for Text Classification using Deep Neural Networks"), where the
    only larger intersections is between the works of [[104](#bib.bibx104)] and [[60](#bib.bibx60)].
    [[90](#bib.bibx90)] provide at least some comparability against [[104](#bib.bibx104)]
    and [[60](#bib.bibx60)] through one dataset each. Additionally, RMA [[3](#bib.bibx3)]
    is a subset of R21 [[54](#bib.bibx54)], which are used by [[10](#bib.bibx10)]
    and [[44](#bib.bibx44)], so they might be comparable to some degree. [[78](#bib.bibx78)]
    are the only ones to evaluate on the more recent large-scale text classification
    datasets [[103](#bib.bibx103)], and although these datasets are more realistic
    in terms of their size, the authors omitted the classic datasets, so it is difficult
    to relate their contributions to previous work. Moreover, as a result of this,
    we do not know if and to what degree past experiments generalize to DNNs [[78](#bib.bibx78)].'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格[1](#S4.T1 "Table 1 ‣ 4.2 Text Classification for Active Learning ‣ 4 Active
    Learning for Text Classification ‣ A Survey of Active Learning for Text Classification
    using Deep Neural Networks")中我们可以得出多个结论：显而易见，这些查询策略中的绝大多数属于基于预测的查询策略类别，更具体地说，属于预测不确定性和分歧基准的子类。此外，我们还可以识别出几个不足之处：首先，在许多实验中评估了两个或更多的标准数据集，但实验之间的数据集几乎没有交集。因此，我们失去了与先前研究的可比性。在最近的研究中，这可以从表格[1](#S4.T1
    "Table 1 ‣ 4.2 Text Classification for Active Learning ‣ 4 Active Learning for
    Text Classification ‣ A Survey of Active Learning for Text Classification using
    Deep Neural Networks")中看到，唯一的大交集是[[104](#bib.bibx104)]和[[60](#bib.bibx60)]之间。[[90](#bib.bibx90)]通过各自的数据集至少提供了与[[104](#bib.bibx104)]和[[60](#bib.bibx60)]的一些可比性。此外，RMA[[3](#bib.bibx3)]是R21[[54](#bib.bibx54)]的一个子集，这些数据集被[[10](#bib.bibx10)]和[[44](#bib.bibx44)]使用，因此它们可能在某种程度上是可以比较的。[[78](#bib.bibx78)]是唯一在更新的大规模文本分类数据集[[103](#bib.bibx103)]上进行评估的研究，尽管这些数据集在规模上更为真实，但作者遗漏了经典数据集，因此很难将其贡献与以前的工作相关联。此外，由于此原因，我们不知道过去的实验是否以及在多大程度上能够推广到DNNs[[78](#bib.bibx78)]。
- en: 'Finally, it is not clear if recent (D)NNs benefit from the same query strategies,
    i.e. past findings may not apply to modern NN architectures: [[78](#bib.bibx78)]
    identified contradicting statements in recent literature about the effectiveness
    of using prediction uncertainty in combination with NNs. They achieved competitive
    results using a FastText.zip (FTZ) model and a prediction uncertainty query strategy,
    which proved to be very effective while requiring only a small amount of data,
    despite all reported weaknesses concering NNs and uncertainty estimates.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，目前尚不清楚最近的（D）NNs是否受益于相同的查询策略，即过去的发现可能不适用于现代NN架构：[[78](#bib.bibx78)]在最近的文献中发现了关于结合使用预测不确定性的NNs的有效性的矛盾说法。他们使用FastText.zip（FTZ）模型和预测不确定性查询策略取得了竞争力的结果，尽管存在所有报告的关于NNs和不确定性估计的缺陷，这种方法仍然被证明非常有效，并且只需要少量的数据。
- en: 5 Open Research Questions
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5个开放的研究问题
- en: Uncertainty Estimates in Neural Networks
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络中的不确定性估计
- en: In Section [3](#S3 "3 Active Learning ‣ A Survey of Active Learning for Text
    Classification using Deep Neural Networks") it was illustrated that uncertainty-based
    strategies have been used successfully in combination with non-NN models, and
    in Section [4.3](#S4.SS3 "4.3 Commonalities and Limitations of Previous Experiments
    ‣ 4 Active Learning for Text Classification ‣ A Survey of Active Learning for
    Text Classification using Deep Neural Networks") it was shown that they also account
    for the largest fraction of query strategies in recent NN-based AL. Unfortunately,
    uncertainty in NNs is still challenging due to inaccurate uncertainty estimates,
    or limited scalability (as described in Section [3.2](#S3.SS2.SSS0.Px2 "Uncertainty
    in Neural Networks ‣ 3.2 Neural-Network-Based Active Learning ‣ 3 Active Learning
    ‣ A Survey of Active Learning for Text Classification using Deep Neural Networks")).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Representations
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As outlined in Section [4.1](#S4.SS1 "4.1 Recent Advances in Text Classification
    ‣ 4 Active Learning for Text Classification ‣ A Survey of Active Learning for
    Text Classification using Deep Neural Networks"), the use of text representations
    in NLP has shifted from bag-of-words to static and contextualized word embeddings.
    These representations evidentially provide many advantages like disambiguation
    capabilities, non-sparse vectors, and an increase in performance for many tasks.
    Although there have been some applications [[104](#bib.bibx104), [78](#bib.bibx78),
    [61](#bib.bibx61)], there is no AL-specific systematic evaluation to compare word
    embeddings and LMs using NNs. Moreover, they are currently only scarcely used,
    which hints at either a slow adoption, or some non-investigated practical issues.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Small Data DNNs
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DL approaches are usually applied in the context of large datasets. AL, however,
    necessarily intends to keep the (labeled) dataset as small as possible. In Section
    [3](#S3 "3 Active Learning ‣ A Survey of Active Learning for Text Classification
    using Deep Neural Networks") we outlined why small datasets can be challenging
    for DNNs, and as a direct consequence as well for DNN-based AL. Using pre-trained
    language models, this problem is alleviated to some degree because fine-tuning
    allows training models using considerably smaller datasets. Nonetheless, it is
    to be investigated how little data is still necessary to successfully fine-tune
    a model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Comparable Evaluations
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Section [4.3](#S4.SS3 "4.3 Commonalities and Limitations of Previous Experiments
    ‣ 4 Active Learning for Text Classification ‣ A Survey of Active Learning for
    Text Classification using Deep Neural Networks") we provided an overview of the
    most common AL strategies for text classification. Unfortunately, the combinations
    of datasets used in the experiments are often completely disjoint, e.g. [[90](#bib.bibx90)],
    [[60](#bib.bibx60)], and [[78](#bib.bibx78)]. As a consequence, comparability
    is decreased or even lost, especially between more recent and past work. Comparibility
    is, however, crucial to verify if past insights regarding shallow NN-based AL
    still apply in context of DNN-based AL [[78](#bib.bibx78)].
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[4.3](#S4.SS3 "4.3 之前实验的共性与局限性 ‣ 4 活跃学习用于文本分类 ‣ 深度神经网络下的文本分类活跃学习综述")节中，我们提供了文本分类中最常见的活跃学习（AL）策略的概述。不幸的是，实验中使用的数据集组合通常完全不同，例如[[90](#bib.bibx90)]、[[60](#bib.bibx60)]和[[78](#bib.bibx78)]。因此，尤其是在较新和较旧的工作之间，可比性降低甚至丧失。然而，可比性对于验证过去关于浅层神经网络（NN）基于AL的见解在深度神经网络（DNN）基于AL的背景下是否仍然适用至关重要[[78](#bib.bibx78)]。
- en: Learning to Learn
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学习如何学习
- en: There is an abundance of query strategies to choose from, which we have (non-exhaustively)
    categorized in Section [3.1](#S3.SS1 "3.1 Query Strategies ‣ 3 Active Learning
    ‣ A Survey of Active Learning for Text Classification using Deep Neural Networks").
    This introduces the problem of choosing the optimal strategy. The right choice
    depends on many factors like data, model, or task, and can even vary between different
    iterations during the AL process. As a result, learning to learn (or meta-learning)
    has become popular and can be used to learn the optimal selection [[42](#bib.bibx42)],
    or even learn query strategies as a whole [[6](#bib.bibx6), [49](#bib.bibx49)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 可选择的查询策略种类繁多，我们在第[3.1](#S3.SS1 "3.1 查询策略 ‣ 3 活跃学习 ‣ 深度神经网络下的文本分类活跃学习综述")节中进行了（非详尽的）分类。这引出了选择最佳策略的问题。正确的选择依赖于数据、模型或任务等多个因素，甚至在活跃学习过程的不同迭代之间也可能有所不同。因此，学习如何学习（或元学习）变得越来越流行，可以用来学习最佳选择[[42](#bib.bibx42)]，甚至学习查询策略作为整体[[6](#bib.bibx6),
    [49](#bib.bibx49)]。
- en: 6 Conclusions
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this survey, we investigated (D)NN-based AL for text classification and inspected
    factors obstructing its adoption. We created a taxonomy, distinguishing query
    strategies by their reliance on data-based, model-based, and prediction-based
    input information. We analyzed query strategies used in AL for text classification
    and categorized them into the respective taxonomy classes. We presented the intersection
    between AL, text classification and DNNs, which is to the best of our knowledge
    the first survey of this topic. Furthermore, we reviewed (D)NN-based AL, identified
    current challenges and state of the art, and pointed out that it is both underresearched
    and often lacks comparability. In addition to that, we presented relevant recent
    advances in NLP, related them to AL, and showed gaps and limitations for their
    application. One of our main findings is that uncertainty-based query strategies
    are still the most widely used class, regardless of whether the analysis is restricted
    to NNs. LM-based representations offer finer-grained context-specific representations
    while also handling out-of-vocabulary words. Moreover, we find fine-tuning-based
    transfer learning alleviates the small data problem to some degree but lacks adoption.
    Most important DNNs are known for their strong performance on many tasks and first
    adoptions in AL have shown promising results [[104](#bib.bibx104), [90](#bib.bibx90)].
    All these gains would be highly desirable for AL. Therefore improving the adoption
    of DNNs in AL is crucial, especially since the expected increases in performance
    could be either used to improve the classification results while using the same
    amount of data or to increase the efficiency of the labeling process by reducing
    the data and therefore the labeling efforts. Based on these findings we identify
    research directions for future work in order to further advance (D)NN-based AL.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项综述中，我们探讨了基于（D）NN的主动学习（AL）在文本分类中的应用，并检查了阻碍其采纳的因素。我们创建了一个分类体系，通过数据驱动、模型驱动和预测驱动的信息区分查询策略。我们分析了在文本分类中使用的主动学习查询策略，并将其归入相应的分类体系中。我们展示了AL、文本分类和DNN之间的交集，据我们所知，这是该主题的首个综述。此外，我们回顾了基于（D）NN的AL，确定了当前的挑战和最新进展，并指出该领域研究不足且往往缺乏可比性。此外，我们介绍了NLP领域的相关最新进展，将其与AL关联，并展示了应用中的差距和局限性。我们的主要发现之一是，不确定性驱动的查询策略仍然是最广泛使用的类别，无论分析是否限于神经网络。基于语言模型（LM）的表示提供了更细粒度的上下文特定表示，同时处理了词汇外单词。此外，我们发现基于微调的迁移学习在一定程度上缓解了小数据问题，但采用率较低。最重要的是，DNN在许多任务上表现强劲，AL中的首次应用已显示出有希望的结果
    [[104](#bib.bibx104), [90](#bib.bibx90)]。所有这些进展对于AL来说都非常渴望。因此，提高DNN在AL中的应用至关重要，特别是因为预期的性能提升可以用于在相同数据量下改善分类结果，或通过减少数据从而减少标注工作来提高标注过程的效率。基于这些发现，我们确定了未来工作的研究方向，以进一步推进（D）NN基于的AL。
- en: Acknowledgements
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Gerhard Heyer for his valuable feedback on the manuscript, Lydia Müller
    for fruitful discussions about the taxonomy and advice thereon, and Janos Borst
    for sharing his thoughts on recent advances in language models. This research
    was partially funded by the Development Bank of Saxony (SAB) under project number
    100335729.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢Gerhard Heyer对手稿的宝贵反馈，感谢Lydia Müller就分类体系进行的富有成效的讨论和建议，感谢Janos Borst分享他对语言模型最新进展的看法。此项研究部分由萨克森州发展银行（SAB）资助，项目编号100335729。
- en: References
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Charu C. Aggarwal et al. “Active Learning: A Survey” In *Data Classification:
    Algorithms and Applications* CRC Press, 2014, pp. 571–606'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Charu C. Aggarwal 等. “主动学习：综述” 见 *数据分类：算法与应用* CRC出版社, 2014年, 第571–606页'
- en: '[2] Bang An, Wenjun Wu and Huimin Han “Deep Active Learning for Text Classification”
    In *Proceedings of the 2nd International Conference on Vision, Image and Signal
    Processing - ICVISP 2018* Las Vegas, NV, USA: ACM Press, 2018, pp. 1–6'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Bang An, Wenjun Wu 和 Huimin Han “文本分类的深度主动学习” 见 *第2届国际视觉、图像与信号处理会议论文集 -
    ICVISP 2018* 拉斯维加斯, NV, USA: ACM出版社, 2018年，第1–6页'
- en: '[3] Chidanand Apté, Fred Damerau and Sholom M. Weiss “Towards Language Independent
    Automated Learning of Text Categorization Models” In *Proceedings of the 17th
    Annual International ACM SIGIR Conference on Research and Development in Information
    Retrieval*, SIGIR ’94, 1994, pp. 23–30'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Chidanand Apté, Fred Damerau 和 Sholom M. Weiss “面向语言无关的文本分类模型自动学习” 见 *第17届国际ACM
    SIGIR信息检索研究与开发会议论文集*，SIGIR ’94，1994年，第23–30页'
- en: '[4] David Arthur and Sergei Vassilvitskii “K-Means++: The Advantages of Careful
    Seeding” In *Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete
    Algorithms*, SODA ’07 New Orleans, Louisiana: Society for IndustrialApplied Mathematics,
    2007, pp. 1027–1035'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Jordan T. Ash et al. “Deep Batch Active Learning by Diverse, Uncertain
    Gradient Lower Bounds” arXiv: 1906.03671 In *arXiv:1906.03671 [cs, stat]*, 2019
    URL: [http://arxiv.org/abs/1906.03671](http://arxiv.org/abs/1906.03671)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Philip Bachman, Alessandro Sordoni and Adam Trischler “Learning Algorithms
    for Active Learning” In *Proceedings of the 34th International Conference on Machine
    Learning* 70, ICML’17 JMLR.org, 2017, pp. 301–310'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Mark Belford, Brian Mac Namee and Derek Greene “Stability of Topic Modeling
    via Matrix Factorization” In *Expert Systems with Applications* 91.C USA: Pergamon
    Press, Inc., 2018, pp. 159–169'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Yoshua Bengio “Deep Learning of Representations for Unsupervised and Transfer
    Learning” In *Proceedings of the 2011 International Conference on Unsupervised
    and Transfer Learning Workshop* 27, UTLW’11, 2011, pp. 17–36'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] John Blitzer, Mark Dredze and Fernando Pereira “Biographies, Bollywood,
    Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification” In *Proceedings
    of the 45th Annual Meeting of the Association of Computational Linguistics*, 2007,
    pp. 440–447'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Michael Bloodgood “Support Vector Machine Active Learning Algorithms with
    Query-by-Committee Versus Closest-to-Hyperplane Selection” In *2018 IEEE 12th
    International Conference on Semantic Computing (ICSC)*, 2018, pp. 148–155'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu and Daan Wierstra
    “Weight Uncertainty in Neural Networks” In *Proceedings of the 32nd International
    Conference on International Conference on Machine Learning - Volume 37*, ICML’15
    JMLR.org, 2015, pp. 1613–1622'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] John Carney, Padraig Cunningham and Umesh Bhagwan “Confidence and prediction
    intervals for neural network ensembles” In *Proceedings of the International Joint
    Conference Neural Networks, IJCNN* IEEE, 1999, pp. 1215–1218'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Rich Caruana “Learning Many Related Tasks at the Same Time with Backpropagation”
    In *Advances in Neural Information Processing Systems 7* MIT Press, 1995, pp.
    657–664'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau and Yoshua Bengio
    “On the Properties of Neural Machine Translation: Encoder–Decoder Approaches”
    In *Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure
    in Statistical Translation* Association for Computational Linguistics, 2014, pp.
    103–111'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Kevin Clark, Minh-Thang Luong and Quoc V. Le “ELECTRA: Pre-training Text
    Encoders as Discriminators Rather Than Generators” In *arXiv preprint arXiv:2003.10555*,
    2020'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] David Cohn, Les Atlas and Richard Ladner “Improving Generalization with
    Active Learning” In *Machine Learning* 15.2, 1994, pp. 201–221'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Zihang Dai et al. “Transformer-XL: Attentive Language Models beyond a
    Fixed-Length Context” In *Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics* Association for Computational Linguistics, 2019,
    pp. 2978–2988'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Zihang Dai 等 “Transformer-XL：超越固定长度上下文的注意力语言模型” 载于 *第57届计算语言学协会年会论文集*
    计算语言学协会, 2019, pp. 2978–2988'
- en: '[18] Sanjoy Dasgupta and Daniel Hsu “Hierarchical Sampling for Active Learning”
    In *Proceedings of the 25th International Conference on Machine Learning*, ICML
    ’08 Helsinki, Finland: Association for Computing Machinery, 2008, pp. 208–215'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Sanjoy Dasgupta 和 Daniel Hsu “主动学习的层次采样” 载于 *第25届国际机器学习会议论文集*，ICML ’08
    赫尔辛基, 芬兰: 计算机协会, 2008, pp. 208–215'
- en: '[19] Sarah Jane Delany, Pádraig Cunningham, Alexey Tsymbal and Lorcan Coyle
    “A case-based technique for tracking concept drift in spam filtering” In *Knowledge-Based
    Systems* 18.4, 2005, pp. 187–195'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Sarah Jane Delany, Pádraig Cunningham, Alexey Tsymbal 和 Lorcan Coyle “一种基于案例的技术用于跟踪垃圾邮件过滤中的概念漂移”
    载于 *基于知识的系统* 18.4, 2005, pp. 187–195'
- en: '[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova “BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding” In
    *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)* Association for Computational Linguistics, 2019, pp. 4171–4186'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova “BERT：深度双向变换器的预训练用于语言理解”
    载于 *2019年北美计算语言学协会会议：人类语言技术卷（长篇和短篇论文）* 计算语言学协会, 2019, pp. 4171–4186'
- en: '[21] Xiaowen Ding, Bing Liu and Philip S. Yu “A Holistic Lexicon-Based Approach
    to Opinion Mining” In *Proceedings of the 2008 International Conference on Web
    Search and Data Mining*, WSDM ’08 Palo Alto, California, USA: Association for
    Computing Machinery, 2008, pp. 231–240'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Xiaowen Ding, Bing Liu 和 Philip S. Yu “一种整体词典基础的方法用于情感分析” 载于 *2008年国际网络搜索与数据挖掘会议论文集*，WSDM
    ’08 帕洛阿尔托，加州, USA: 计算机协会, 2008, pp. 231–240'
- en: '[22] Melanie Ducoffe and Frederic Precioso “Adversarial Active Learning for
    Deep Networks: a Margin Based Approach” In *arXiv preprint arXiv:1802.09841*,
    2018 URL: [http://arxiv.org/abs/1802.09841](http://arxiv.org/abs/1802.09841)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Melanie Ducoffe 和 Frederic Precioso “针对深度网络的对抗性主动学习：一种基于边际的方法” 载于 *arXiv预印本
    arXiv:1802.09841*，2018 网址: [http://arxiv.org/abs/1802.09841](http://arxiv.org/abs/1802.09841)'
- en: '[23] C.. Fall, A. Törcsvári, K. Benzineb and G. Karetka “Automated Categorization
    in the International Patent Classification” In *ACM SIGIR Forum* 37.1 New York,
    NY, USA: Association for Computing Machinery, 2003, pp. 10–25'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] C.. Fall, A. Törcsvári, K. Benzineb 和 G. Karetka “国际专利分类中的自动化分类” 载于 *ACM
    SIGIR论坛* 37.1 纽约, NY, USA: 计算机协会, 2003, pp. 10–25'
- en: '[24] Rosa L. Figueroa et al. “Active learning for clinical text classification:
    is it better than random sampling?” In *Journal of the American Medical Informatics
    Association* 19.5, 2012, pp. 809–816'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Rosa L. Figueroa 等 “临床文本分类的主动学习：它是否优于随机抽样？” 载于 *美国医学信息学协会期刊* 19.5, 2012,
    pp. 809–816'
- en: '[25] Yifan Fu, Xingquan Zhu and Bin Li “A survey on instance selection for
    active learning” In *Knowledge and Information Systems* 35.2, 2013, pp. 249–283'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Yifan Fu, Xingquan Zhu 和 Bin Li “主动学习的实例选择调查” 载于 *知识与信息系统* 35.2, 2013,
    pp. 249–283'
- en: '[26] Yarin Gal “Uncertainty in Deep Learning”, 2016'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Yarin Gal “深度学习中的不确定性”，2016'
- en: '[27] Yarin Gal and Zoubin Ghahramani “Dropout as a Bayesian Approximation:
    Representing Model Uncertainty in Deep Learning” In *Proceedings of the 33rd International
    Conference on International Conference on Machine Learning* 48, ICML’16 New York,
    NY, USA: JMLR.org, 2016, pp. 1050–1059'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Yarin Gal 和 Zoubin Ghahramani “Dropout作为贝叶斯近似：在深度学习中表示模型不确定性” 载于 *第33届国际机器学习大会论文集*
    48, ICML’16 纽约, NY, USA: JMLR.org, 2016, pp. 1050–1059'
- en: '[28] Vijay Garla, Caroline Taylor and Cynthia Brandt “Semi-supervised clinical
    text classification with Laplacian SVMs: An application to cancer case management”
    In *Journal of Biomedical Informatics* 46.5, 2013, pp. 869–875'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Vijay Garla, Caroline Taylor 和 Cynthia Brandt “使用拉普拉斯支持向量机的半监督临床文本分类：癌症案例管理的应用”
    载于 *生物医学信息学期刊* 46.5, 2013, pp. 869–875'
- en: '[29] Daniel Gissin and Shai Shalev-Shwartz “Discriminative Active Learning”
    arXiv: 1907.06347 In *arXiv preprint arXiv:1907.06347*, 2019 URL: [http://arxiv.org/abs/1907.06347](http://arxiv.org/abs/1907.06347)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Daniel Gissin 和 Shai Shalev-Shwartz “区分性主动学习” arXiv: 1907.06347 载于 *arXiv预印本
    arXiv:1907.06347*，2019 网址: [http://arxiv.org/abs/1907.06347](http://arxiv.org/abs/1907.06347)'
- en: '[30] Ian J. Goodfellow et al. “Generative Adversarial Nets” In *Proceedings
    of the 27th International Conference on Neural Information Processing Systems
    - Volume 2*, NIPS’14 Cambridge, MA, USA: MIT Press, 2014, pp. 2672–2680'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Ian J. Goodfellow 等 “生成对抗网络” 见于 *第27届国际神经信息处理系统大会 - 第2卷*，NIPS’14 剑桥，MA，美国：MIT出版社，2014年，第2672–2680页'
- en: '[31] Alex Graves and Jürgen Schmidhuber “Framewise phoneme classification with
    bidirectional LSTM and other neural network architectures” In *Neural networks*
    18.5, 2005, pp. 602–610'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Alex Graves 和 Jürgen Schmidhuber “使用双向 LSTM 和其他神经网络架构进行逐帧音素分类” 见于 *神经网络*
    18.5，2005年，第602–610页'
- en: '[32] Antonio Gulli “AG’s Corpus of News Articles” Online; visited on 02/11/2020,
    [http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html),
    2005'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Antonio Gulli “AG 的新闻文章语料库” 在线访问；访问于 2020年2月11日，[http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)，2005'
- en: '[33] Chuan Guo, Geoff Pleiss, Yu Sun and Kilian Q. Weinberger “On Calibration
    of Modern Neural Networks” In *Proceedings of the 34th International Conference
    on Machine Learning* 70, ICML’17 JMLR.org, 2017, pp. 1321–1330'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Chuan Guo, Geoff Pleiss, Yu Sun 和 Kilian Q. Weinberger “现代神经网络的校准” 见于
    *第34届国际机器学习大会* 70，ICML’17 JMLR.org，2017年，第1321–1330页'
- en: '[34] Yuhong Guo and Dale Schuurmans “Discriminative Batch Mode Active Learning”
    In *Proceedings of the 20th International Conference on Neural Information Processing
    Systems*, NIPS’07 Vancouver, British Columbia, Canada: Curran Associates Inc.,
    2007, pp. 593–600'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Yuhong Guo 和 Dale Schuurmans “区分批量模式主动学习” 见于 *第20届国际神经信息处理系统大会*，NIPS’07
    温哥华，卑诗省，加拿大：Curran Associates Inc.，2007年，第593–600页'
- en: '[35] Gholamreza Haffari and Anoop Sarkar “Active Learning for Multilingual
    Statistical Machine Translation” In *Proceedings of the Joint Conference of the
    47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural
    Language Processing of the AFNLP* 1, ACL ’09 Suntec, Singapore: Association for
    Computational Linguistics, 2009, pp. 181–189'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Gholamreza Haffari 和 Anoop Sarkar “多语言统计机器翻译的主动学习” 见于 *第47届ACL年会和第4届AFNLP国际自然语言处理联合会议*
    1，ACL ’09 新加坡：计算语言学协会，2009年，第181–189页'
- en: '[36] Tom Heskes “Practical Confidence and Prediction Intervals” In *Proceedings
    of the 9th International Conference on Neural Information Processing Systems*,
    NIPS’96 MIT Press, 1996, pp. 176–182'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Tom Heskes “实际置信度和预测区间” 见于 *第9届国际神经信息处理系统大会*，NIPS’96 MIT出版社，1996年，第176–182页'
- en: '[37] Geoffrey Hinton and Ruslan Salakhutdinov “Reducing the dimensionality
    of data with neural networks” In *Science* 313.5786 American Association for the
    Advancement of Science, 2006, pp. 504–507'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Geoffrey Hinton 和 Ruslan Salakhutdinov “使用神经网络减少数据的维度” 见于 *科学* 313.5786
    美国科学促进会，2006年，第504–507页'
- en: '[38] Sepp Hochreiter and Jürgen Schmidhuber “Long Short-Term Memory” In *Neural
    Computation* 9.8 MIT Press, 1997, pp. 1735–1780 URL: [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Sepp Hochreiter 和 Jürgen Schmidhuber “长短期记忆” 见于 *神经计算* 9.8 MIT出版社，1997年，第1735–1780页
    URL: [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)'
- en: '[39] Steven C.. Hoi, Rong Jin and Michael R. Lyu “Large-Scale Text Categorization
    by Batch Mode Active Learning” In *Proceedings of the 15th International Conference
    on World Wide Web*, WWW ’06 Edinburgh, Scotland: Association for Computing Machinery,
    2006, pp. 633–642'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Steven C.. Hoi, Rong Jin 和 Michael R. Lyu “通过批量模式主动学习的大规模文本分类” 见于 *第15届国际万维网大会*，WWW
    ’06 爱丁堡，苏格兰：计算机协会，2006年，第633–642页'
- en: '[40] Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani and Máté Lengyel “Bayesian
    Active Learning for Classification and Preference Learning” arXiv: 1112.5745 In
    *arXiv:1112.5745 [cs, stat]*, 2011 URL: [http://arxiv.org/abs/1112.5745](http://arxiv.org/abs/1112.5745)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani 和 Máté Lengyel “用于分类和偏好学习的贝叶斯主动学习”
    arXiv: 1112.5745 见于 *arXiv:1112.5745 [cs, stat]*，2011年 URL: [http://arxiv.org/abs/1112.5745](http://arxiv.org/abs/1112.5745)'
- en: '[41] Jeremy Howard and Sebastian Ruder “Universal Language Model Fine-tuning
    for Text Classification” In *Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)* Association for Computational
    Linguistics, 2018, pp. 328–339'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Jeremy Howard 和 Sebastian Ruder “用于文本分类的通用语言模型微调” 见于 *第56届计算语言学协会年会（第1卷：长篇论文）*
    计算语言学协会，2018年，第328–339页'
- en: '[42] Wei-Ning Hsu and Hsuan-Tien Lin “Active Learning by Learning” In *Proceedings
    of the Twenty-Ninth AAAI Conference on Artificial Intelligence*, AAAI’15 AAAI
    Press, 2015, pp. 2659–2665'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Wei-Ning Hsu 和 Hsuan-Tien Lin “通过学习实现主动学习” 载于 *第29届人工智能协会会议论文集*，AAAI’15
    AAAI Press，2015年，页码2659–2665'
- en: '[43] Minqing Hu and Bing Liu “Mining and Summarizing Customer Reviews” In *Proceedings
    of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining*, KDD ’04 Seattle, WA, USA: Association for Computing Machinery, 2004,
    pp. 168–177'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Minqing Hu 和 Bing Liu “挖掘和总结客户评论” 载于 *第十届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，KDD
    ’04 西雅图，WA，美国：计算机协会，2004年，页码168–177'
- en: '[44] Rong Hu, Brian Mac Namee and Sarah Jane Delany “Active Learning for Text
    Classification with Reusability” In *Expert Systems with Applications* 45.C USA:
    Pergamon Press, Inc., 2016, pp. 438–449'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Rong Hu, Brian Mac Namee 和 Sarah Jane Delany “具有可重用性的文本分类主动学习” 载于 *应用专家系统*
    45.C 美国：Pergamon Press, Inc.，2016年，页码438–449'
- en: '[45] Thorsten Joachims “A Probabilistic Analysis of the Rocchio Algorithm with
    TFIDF for Text Categorization” In *Proceedings of the Fourteenth International
    Conference on Machine Learning*, ICML ’97, 1997, pp. 143–151'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Thorsten Joachims “TFIDF 和 Rocchio 算法在文本分类中的概率分析” 载于 *第十四届国际机器学习会议论文集*，ICML
    ’97，1997年，页码143–151'
- en: '[46] Armand Joulin, Edouard Grave, Piotr Bojanowski and Tomas Mikolov “Bag
    of Tricks for Efficient Text Classification” In *Proceedings of the 15th Conference
    of the European Chapter of the Association for Computational Linguistics: Volume
    2, Short Papers* Association for Computational Linguistics, 2017, pp. 427–431'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Armand Joulin, Edouard Grave, Piotr Bojanowski 和 Tomas Mikolov “高效文本分类的技巧集”
    载于 *第15届欧洲计算语言学协会会议：第二卷，简短论文* 计算语言学协会，2017年，页码427–431'
- en: '[47] Armand Joulin et al. “FastText.zip: Compressing text classification models”
    arXiv: 1612.03651 In *arXiv:1612.03651 [cs]*, 2016'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Armand Joulin 等 “FastText.zip: 压缩文本分类模型” arXiv: 1612.03651 载于 *arXiv:1612.03651
    [cs]*，2016年'
- en: '[48] Yoon Kim “Convolutional Neural Networks for Sentence Classification” In
    *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
    (EMNLP)* Association for Computational Linguistics, 2014, pp. 1746–1751 URL: [https://www.aclweb.org/anthology/D14-1181](https://www.aclweb.org/anthology/D14-1181)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Yoon Kim “用于句子分类的卷积神经网络” 载于 *2014年自然语言处理实证方法会议（EMNLP）论文集* 计算语言学协会，2014年，页码1746–1751
    URL: [https://www.aclweb.org/anthology/D14-1181](https://www.aclweb.org/anthology/D14-1181)'
- en: '[49] Ksenia Konyushkova, Sznitman Raphael and Pascal Fua “Learning Active Learning
    from Data” In *Proceedings of the 31st International Conference on Neural Information
    Processing Systems*, NIPS’17 Curran Associates Inc., 2017, pp. 4228–4238'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Ksenia Konyushkova, Sznitman Raphael 和 Pascal Fua “从数据中学习主动学习” 载于 *第31届国际神经信息处理系统会议论文集*，NIPS’17
    Curran Associates Inc.，2017年，页码4228–4238'
- en: '[50] Anders Krogh and Jesper Vedelsby “Neural Network Ensembles, Cross Validation
    and Active Learning” In *Proceedings of the 7th International Conference on Neural
    Information Processing Systems*, NIPS’94 MIT Press, 1994, pp. 231–238'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Anders Krogh 和 Jesper Vedelsby “神经网络集成、交叉验证和主动学习” 载于 *第7届国际神经信息处理系统会议论文集*，NIPS’94
    MIT Press，1994年，页码231–238'
- en: '[51] Balaji Lakshminarayanan, Alexander Pritzel and Charles Blundell “Simple
    and Scalable Predictive Uncertainty Estimation Using Deep Ensembles” In *Proceedings
    of the 31st International Conference on Neural Information Processing Systems*,
    NIPS’17 Long Beach, California, USA: Curran Associates Inc., 2017, pp. 6405–6416'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Balaji Lakshminarayanan, Alexander Pritzel 和 Charles Blundell “使用深度集成进行简单且可扩展的预测不确定性估计”
    载于 *第31届国际神经信息处理系统会议论文集*，NIPS’17 长滩，加利福尼亚，美国：Curran Associates Inc.，2017年，页码6405–6416'
- en: '[52] Leah S. Larkey “A patent search and classification system” In *Proceedings
    of the fourth ACM conference on Digital libraries - DL ’99* ACM Press, 1999, pp.
    179–187'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Leah S. Larkey “一种专利搜索和分类系统” 载于 *第四届ACM数字图书馆会议论文集 - DL ’99* ACM Press，1999年，页码179–187'
- en: '[53] Quoc Le and Tomas Mikolov “Distributed Representations of Sentences and
    Documents” In *Proceedings of the 31st International Conference on International
    Conference on Machine Learning* 32, ICML’14 JMLR.org, 2014, pp. 1188–1196'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Quoc Le 和 Tomas Mikolov “句子和文档的分布式表示” 载于 *第31届国际机器学习会议论文集* 32，ICML’14
    JMLR.org，2014年，页码1188–1196'
- en: '[54] David D. Lewis “Reuters-21578 corpus” Online. Visited on 02/14/2020, [http://www.daviddlewis.com/resources/testcollections/reuters21578/](http://www.daviddlewis.com/resources/testcollections/reuters21578/),
    1997'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] David D. Lewis and William A. Gale “A Sequential Algorithm for Training
    Text Classifiers” In *Proceedings of the 17th Annual International ACM SIGIR Conference
    on Research and Development in Information Retrieval*, SIGIR ’94 Springer, 1994,
    pp. 3–12'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] David D. Lewis, Yiming Yang, Tony G. Rose and Fan Li “RCV1: A New Benchmark
    Collection for Text Categorization Research” In *J. Mach. Learn. Res.* 5 JMLR.org,
    2004, pp. 361–397'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Xin Li and Dan Roth “Learning Question Classifiers” In *Proceedings of
    the 19th International Conference on Computational Linguistics* 1, COLING ’02
    Taipei, Taiwan: Association for Computational Linguistics, 2002, pp. 1–7 DOI:
    [10.3115/1072228.1072378](https://dx.doi.org/10.3115/1072228.1072378)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Ray Liere and Prasad Tadepalli “Active Learning with Committees for Text
    Categorization” In *Proceedings of the Fourteenth National Conference on Artificial
    Intelligence and Ninth Conference on Innovative Applications of Artificial Intelligence*,
    AAAI’97/IAAI’97 AAAI Press, 1997, pp. 591–596'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Yinhan Liu et al. “RoBERTa: A Robustly Optimized BERT Pretraining Approach”
    arXiv: 1907.11692 In *arXiv:1907.11692 [cs]*, 2019 URL: [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] David Lowell, Zachary C. Lipton and Byron C. Wallace “Practical Obstacles
    to Deploying Active Learning” In *Proceedings of the 2019 Conference on Empirical
    Methods in Natural Language Processing and the 9th International Joint Conference
    on Natural Language Processing (EMNLP-IJCNLP)* Association for Computational Linguistics,
    2019, pp. 21–30'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Jinghui Lu and Brian MacNamee “Investigating the Effectiveness of Representations
    Based on Pretrained Transformer-based Language Models in Active Learning for Labelling
    Text Datasets” In *arXiv preprint arXiv:2004.13138*, 2020'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] David JC MacKay “The Evidence Framework Applied to Classification Networks”
    In *Neural Computation* 4.5 MIT Press, 1992, pp. 720–736'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Prem Melville and Raymond J. Mooney “Diverse Ensembles for Active Learning”
    In *Proceedings of the Twenty-First International Conference on Machine Learning*,
    ICML ’04 Banff, Alberta, Canada: Association for Computing Machinery, 2004, pp.
    584–591'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Stephen Merity, Nitish Shirish Keskar and Richard Socher “Regularizing
    and optimizing LSTM language models” In *arXiv preprint arXiv:1708.02182*, 2017'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Tomas Mikolov et al. “Distributed Representations of Words and Phrases
    and Their Compositionality” In *Proceedings of the 26th International Conference
    on Neural Information Processing Systems* 2, NIPS’13 Red Hook, NY, USA: Curran
    Associates Inc., 2013, pp. 3111–3119'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Tomas Mikolov, Kai Chen, Greg Corrado and Jeffrey Dean “Efficient Estimation
    of Word Representations in Vector Space” In *1st International Conference on Learning
    Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop
    Track Proceedings*, 2013'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Arjun Mukherjee and Bing Liu “Improving Gender Classification of Blog
    Authors” In *Proceedings of the 2010 Conference on Empirical Methods in Natural
    Language Processing*, EMNLP ’10 Cambridge, Massachusetts: Association for Computational
    Linguistics, 2010, pp. 207–217'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Hieu T. Nguyen and Arnold Smeulders “Active Learning Using Pre-Clustering”
    In *Proceedings of the Twenty-First International Conference on Machine Learning*,
    ICML ’04 New York, NY, USA: Association for Computing Machinery, 2004, pp. 623–630
    DOI: [10.1145/1015330.1015349](https://dx.doi.org/10.1145/1015330.1015349)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Kamal Nigam, Andrew McCallum, Sebastian Thrun and Tom M. Mitchell “Text
    Classification from Labeled and Unlabeled Documents using EM” In *Machine Learning*
    39, 2000, pp. 103–134'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] David A. Nix and Andreas S. Weigend “Learning Local Error Bars for Nonlinear
    Regression” In *Advances in Neural Information Processing Systems 7*, NIPS’94
    MIT Press, 1995, pp. 489–496'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Fredrik Olsson “A literature survey of active machine learning in the
    context of natural language processing”, SICS Technical Report Swedish Institute
    of Computer Science, 2009, pp. 59'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Bo Pang and Lillian Lee “A Sentimental Education: Sentiment Analysis Using
    Subjectivity Summarization Based on Minimum Cuts” In *Proceedings of the 42nd
    Annual Meeting on Association for Computational Linguistics*, ACL ’04 USA: Association
    for Computational Linguistics, 2004, pp. 271–278'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Bo Pang and Lillian Lee “Seeing Stars: Exploiting Class Relationships
    for Sentiment Categorization with Respect to Rating Scales” In *Proceedings of
    the 43rd Annual Meeting on Association for Computational Linguistics*, ACL ’05
    Ann Arbor, Michigan: Association for Computational Linguistics, 2005, pp. 115–124'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Jeffrey Pennington, Richard Socher and Christopher Manning “GloVe: Global
    Vectors for Word Representation” In *Proceedings of the 2014 Conference on Empirical
    Methods in Natural Language Processing (EMNLP)* Association for Computational
    Linguistics, 2014, pp. 1532–1543'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] John P. Pestian et al. “A shared task involving multi-label classification
    of clinical free text” In *Proceedings of the Workshop on BioNLP 2007: Biological,
    Translational, and Clinical Language Processing*, BioNLP ’07 Prague, Czech Republic:
    Association for Computational Linguistics, 2007, pp. 97–104 DOI: [10.3115/1572392.1572411](https://dx.doi.org/10.3115/1572392.1572411)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Matthew E. Peters et al. “Deep contextualized word representations” arXiv:
    1802.05365 In *arXiv:1802.05365 [cs]*, 2018 URL: [http://arxiv.org/abs/1802.05365](http://arxiv.org/abs/1802.05365)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Forough Poursabzi-Sangdeh, Jordan Boyd-Graber, Leah Findlater and Kevin
    Seppi “ALTO: Active Learning with Topic Overviews for Speeding Label Induction
    and Document Labeling” In *Proceedings of the 54th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)* Association for Computational
    Linguistics, 2016, pp. 1158–1169'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Ameya Prabhu, Charles Dognin and Maneesh Singh “Sampling Bias in Deep
    Active Classification: An Empirical Study” In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)* Association for
    Computational Linguistics, 2019, pp. 4058–4068'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Lorien Y. Pratt, Jack Mostow, Candace A. Kamm and Ace A. Kamm “Direct
    Transfer of Learned Information Among Neural Networks.” In *AAAI* 91, 1991, pp.
    584–589'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Nils Reimers and Iryna Gurevych “Sentence-BERT: Sentence Embeddings using
    Siamese BERT-Networks” In *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP)* Association for Computational Linguistics,
    2019, pp. 3982–3992'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Nicholas Roy and Andrew McCallum “Toward Optimal Active Learning through
    Sampling Estimation of Error Reduction” In *Proceedings of the Eighteenth International
    Conference on Machine Learning*, ICML ’01 Morgan Kaufmann Publishers Inc., 2001,
    pp. 441–448'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Victor Sanh, Lysandre Debut, Julien Chaumond and Thomas Wolf “DistilBERT,
    a distilled version of BERT: smaller, faster, cheaper and lighter” In *arXiv preprint
    arXiv:1910.01108*, 2020'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Greg Schohn and David Cohn “Less is More: Active Learning with Support
    Vector Machines” In *Proceedings of the Seventeenth International Conference on
    Machine Learning (ICML 2000)* Morgan Kaufmann, 2000, pp. 839–846'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Ozan Sener and Silvio Savarese “Active Learning for Convolutional Neural
    Networks: A Core-Set Approach” In *6th International Conference on Learning Representations,
    ICLR 2018, Conference Track Proceedings*, 2018'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Burr Settles “Active Learning Literature Survey”, 2010'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Burr Settles, Mark Craven and Soumya Ray “Multiple-Instance Active Learning”
    In *Proceedings of the 20th International Conference on Neural Information Processing
    Systems*, NIPS’07 Vancouver, British Columbia, Canada: Curran Associates Inc.,
    2007, pp. 1289–1296'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Manali Sharma and Mustafa Bilgic “Evidence-Based Uncertainty Sampling
    for Active Learning” In *Data Mining and Knowledge Discovery* 31.1 Kluwer Academic
    Publishers, 2017, pp. 164–202'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Dan Shen et al. “Multi-Criteria-Based Active Learning for Named Entity
    Recognition” In *Proceedings of the 42nd Annual Meeting on Association for Computational
    Linguistics*, ACL ’04 USA: Association for Computational Linguistics, 2004, pp.
    589–596'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Yanyao Shen et al. “Deep Active Learning for Named Entity Recognition”
    In *arXiv preprint arXiv:1707.05928*, 2018 URL: [http://arxiv.org/abs/1707.05928](http://arxiv.org/abs/1707.05928)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Aditya Siddhant and Zachary C. Lipton “Deep Bayesian Active Learning for
    Natural Language Processing: Results of a Large-Scale Empirical Study” In *Proceedings
    of the 2018 Conference on Empirical Methods in Natural Language Processing* Association
    for Computational Linguistics, 2018, pp. 2904–2909'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Nitish Srivastava et al. “Dropout: A Simple Way to Prevent Neural Networks
    from Overfitting” In *The Journal of Machine Learning Research* 15.1 JMLR.org,
    2014, pp. 1929–1958'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Chi Sun, Xipeng Qiu, Yige Xu and Xuanjing Huang “How to Fine-Tune BERT
    for Text Classification?” In *arXiv preprint arXiv:1905.05583*, 2020'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Luke Taylor and Geoff Nitschke “Improving Deep Learning with Generic Data
    Augmentation” In *2018 IEEE Symposium Series on Computational Intelligence (SSCI)*,
    2018, pp. 1542–1547'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Katrin Tomanek and Udo Hahn “Reducing Class Imbalance during Active Learning
    for Named Entity Annotation” In *Proceedings of the Fifth International Conference
    on Knowledge Capture*, K-CAP ’09 Association for Computing Machinery, 2009, pp.
    105–112'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Simon Tong and Daphne Koller “Support Vector Machine Active Learning with
    Applications to Text Classification” In *Journal of Machine Learning Research*
    2, 2001, pp. 45–66'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Alexander Vezhnevets, Joachim M. Buhmann and Vittorio Ferrari “Active
    learning for semantic segmentation with expected change” In *2012 IEEE Conference
    on Computer Vision and Pattern Recognition*, 2012, pp. 3162–3169'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Raimar Wagner et al. “Learning convolutional neural networks from few
    samples” In *The 2013 International Joint Conference on Neural Networks (IJCNN)*
    IEEE, 2013, pp. 1–7 URL: [http://ieeexplore.ieee.org/document/6706969/](http://ieeexplore.ieee.org/document/6706969/)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Byron C. Wallace et al. “A large-scale quantitative analysis of latent
    factors and sentiment in online doctor reviews” In *Journal of the American Medical
    Informatics Association* 21.6, 2014, pp. 1098–1103'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Canhui Wang, Min Zhang, Shaoping Ma and Liyun Ru “Automatic Online News
    Issue Construction in Web Environment” In *Proceedings of the 17th International
    Conference on World Wide Web*, WWW ’08 Beijing, China: Association for Computing
    Machinery, 2008, pp. 457–466'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Jason Wang and Luis Perez “The Effectiveness of Data Augmentation in
    Image Classification using Deep Learning” In *arXiv preprint arXiv:1712.04621*,
    2017, pp. 11'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Zhao Xu et al. “Representative sampling for text classification using
    support vector machines” In *Proceedings of the 25th European Conference on IR
    Research*, ECIR’03 Pisa, Italy: Springer-Verlag, 2003, pp. 393–407'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Zhilin Yang et al. “XLNet: Generalized Autoregressive Pretraining for
    Language Understanding” In *Advances in Neural Information Processing Systems
    32* Curran Associates, Inc., 2019, pp. 5753–5763'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Xiang Zhang, Junbo Zhao and Yann LeCun “Character-Level Convolutional
    Networks for Text Classification” In *Proceedings of the 28th International Conference
    on Neural Information Processing Systems* 1, NIPS’15 Montreal, Canada: MIT Press,
    2015, pp. 649–657'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Ye Zhang, Matthew Lease and Byron C. Wallace “Active Discriminative Text
    Representation Learning” In *Proceedings of the Thirty-First AAAI Conference on
    Artificial Intelligence*, AAAI’17 AAAI Press, 2017, pp. 3386–3392'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Yizhe Zhang et al. “Adversarial Feature Matching for Text Generation”
    In *Proceedings of the 34th International Conference on Machine Learning* 70,
    ICML’17 JMLR.org, 2017, pp. 4006–4015'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Jia-Jie Zhu and José Bento “Generative Adversarial Active Learning” In
    *arXiv preprint arXiv:1702.07956*, 2017'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Datasets
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following table provides additional information about the datasets which
    were referred to in Section [4.2](#S4.SS2 "4.2 Text Classification for Active
    Learning ‣ 4 Active Learning for Text Classification ‣ A Survey of Active Learning
    for Text Classification using Deep Neural Networks").
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: A collection of widely-used text classification datasets. The column
    "Type" denotes the classification setting (B = binary, MC = multi-class, ML =
    multi-class multi-label). The columns "#Train" and "#Test" show the size of the
    train and of the test set. In the case that no predefined splits were available
    "#Train" represents the full dataset’s size. Each dataset was assigned a short
    id (first column), which we use in the paper for reference.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '(*): documents, (**) labels reduced to positive/negative, (***) 20news-bydate
    with duplicates removed'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '| Id | Name | Type | Publication | #Train | #Test |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| TQA | TREC QA | MC | [[57](#bib.bibx57)] | 5,500 | 500 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| CR | Customer Reviews | MC | [[43](#bib.bibx43)] | ^*315 | - |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| ACR | Additional Customer Reviews | MC | [[21](#bib.bibx21)] | ^*325 | -
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| MDS | Multi-Domain Sentiment | B | [[9](#bib.bibx9)] | ^(**)8,000 | - |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| BAG | Blog Author Gender | B | [[67](#bib.bibx67)] | 3,100 | - |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| G13 | Guardian 2013 | MC | [[7](#bib.bibx7)] | 6,520 | - |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| MR | Movie Reviews | B | [[73](#bib.bibx73)] | 10,662 | - |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| MRL | Movie Reviews Long | B | [[72](#bib.bibx72)] | 2,000 | - |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| MUR | Music Review | B | [[9](#bib.bibx9)] | 2,000 | - |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| DR | Doctor Reviews | MC | [[98](#bib.bibx98)] | 58,110 | - |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| SJ | Subjectivity | B | [[72](#bib.bibx72)] | 10,000 | - |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '| 20N | 20newsgroups | MC | [[45](#bib.bibx45)] | ^(***)18,846 | - |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
- en: '| R21 | Reuters-21578 | ML | [[54](#bib.bibx54)] | 21578 | - |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| RMA | Reuters ModApté | ML | [[3](#bib.bibx3)] | 9,603 | 3,299 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: '| RV2 | RCV1-V2 | ML | [[56](#bib.bibx56)] | 23,149 | 781,265 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '| SPM | Spam | B | [[19](#bib.bibx19)] | 1,000 | - |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| AGN | AG News | MC | [[32](#bib.bibx32)] [[103](#bib.bibx103)] | 120,000
    | 7,600 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| SGN | Sogou News | MC | [[99](#bib.bibx99)] | 450,000 | 60,000 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| DBP | DBPedia | MC | [[103](#bib.bibx103)] | 560,000 | 70,000 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| YRP | Yelp Review Polarity | B | [[103](#bib.bibx103)] | 560,000 | 38,000
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| YRP | Yelp 评论极性 | B | [[103](#bib.bibx103)] | 560,000 | 38,000 |'
- en: '| YRF | Yelp Review Full | MC | [[103](#bib.bibx103)] | 650,000 | 50,000 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| YRF | Yelp 评论完整 | MC | [[103](#bib.bibx103)] | 650,000 | 50,000 |'
- en: '| YAH | Yahoo! Answers | MC | [[103](#bib.bibx103)] | 1,400,000 | 60,000 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| YAH | 雅虎回答 | MC | [[103](#bib.bibx103)] | 1,400,000 | 60,000 |'
- en: '| ARP | Amazon Review Polarity | B | [[103](#bib.bibx103)] | 3,600,000 | 40,000
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| ARP | 亚马逊评论极性 | B | [[103](#bib.bibx103)] | 3,600,000 | 40,000 |'
- en: '| ARF | Amazon Review Full | MC | [[103](#bib.bibx103)] | 3,000,000 | 650,000
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| ARF | 亚马逊评论完整 | MC | [[103](#bib.bibx103)] | 3,000,000 | 650,000 |'
