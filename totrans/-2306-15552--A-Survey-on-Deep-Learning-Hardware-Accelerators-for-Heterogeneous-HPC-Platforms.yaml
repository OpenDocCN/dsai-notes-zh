- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:38:38'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2306.15552] A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.15552](https://ar5iv.labs.arxiv.org/html/2306.15552)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \SetWatermarkText
  prefs: []
  type: TYPE_NORMAL
- en: Preprint version \SetWatermarkScale0.6 \SetWatermarkColor[gray]0.8 \forestsetqtree/.style=for
    tree=parent anchor=south, child anchor=north,align=center,inner sep=0pt
  prefs: []
  type: TYPE_NORMAL
- en: A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cristina Silvano [cristina.silvano@polimi.it](mailto:cristina.silvano@polimi.it)
    ,  Daniele Ielmini [daniele.ielmini@polimi.it](mailto:daniele.ielmini@polimi.it)
    ,  Fabrizio Ferrandi [fabrizio.ferrandi@polimi.it](mailto:fabrizio.ferrandi@polimi.it)
    ,  Leandro Fiorin [leandro.fiorin@polimi.it](mailto:leandro.fiorin@polimi.it)
    ,  Serena Curzel [serena.curzel@polimi.it](mailto:serena.curzel@polimi.it) Politecnico
    di MilanoItaly ,  Luca Benini [luca.benini@unibo.it](mailto:luca.benini@unibo.it)
    ,  Francesco Conti [f.conti@unibo.it](mailto:f.conti@unibo.it) ,  Angelo Garofalo
    [angelo.garofalo@unibo.it](mailto:angelo.garofalo@unibo.it) Università di BolognaItaly
    ,  Cristian Zambelli [cristian.zambelli@unife.it](mailto:cristian.zambelli@unife.it)
    ,  Enrico Calore [enrico.calore@fe.infn.it](mailto:enrico.calore@fe.infn.it) , 
    Sebastiano Fabio Schifano [sebastiano.fabio.schifano@unife.it](mailto:sebastiano.fabio.schifano@unife.it)
    Università degli Studi di FerraraItaly ,  Maurizio Palesi [maurizio.palesi@unict.it](mailto:maurizio.palesi@unict.it)
    ,  Giuseppe Ascia [giuseppe.ascia@unict.it](mailto:giuseppe.ascia@unict.it) , 
    Davide Patti [davide.patti@unict.it](mailto:davide.patti@unict.it) Università
    degli Studi di CataniaItaly ,  Stefania Perri [s.perri@unical.it](mailto:s.perri@unical.it)
    Università degli Studi della CalabriaItaly ,  Nicola Petra [nicola.petra@unina.it](mailto:nicola.petra@unina.it)
    ,  Davide De Caro [dadecaro@unina.it](mailto:dadecaro@unina.it) Università degli
    Studi di Napoli Federico IIItaly ,  Luciano Lavagno [luciano.lavagno@polito.it](mailto:luciano.lavagno@polito.it)
    ,  Teodoro Urso [teodoro.urso@polito.it](mailto:teodoro.urso@polito.it) Politecnico
    di TorinoItaly ,  Valeria Cardellini [cardellini@ing.uniroma2.it](mailto:cardellini@ing.uniroma2.it)
    ,  Gian Carlo Cardarilli [g.cardarilli@uniroma2.it](mailto:g.cardarilli@uniroma2.it)
    Università di Roma “Tor Vergata”Italy  and  Robert Birke [robert.birke@unito.it](mailto:robert.birke@unito.it)
    Università degli Studi di TorinoItaly(2023)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent trends in deep learning (DL) imposed hardware accelerators as the most
    viable solution for several classes of high-performance computing (HPC) applications
    such as image classification, computer vision, and speech recognition. This survey
    summarizes and classifies the most recent advances in designing DL accelerators
    suitable to reach the performance requirements of HPC applications. In particular,
    it highlights the most advanced approaches to support deep learning accelerations
    including not only GPU and TPU-based accelerators but also design-specific hardware
    accelerators such as FPGA-based and ASIC-based accelerators, Neural Processing
    Units, open hardware RISC-V-based accelerators and co-processors. The survey also
    describes accelerators based on emerging memory technologies and computing paradigms,
    such as 3D-stacked Processor-In-Memory, non-volatile memories (mainly, Resistive
    RAM and Phase Change Memories) to implement in-memory computing, Neuromorphic
    Processing Units, and accelerators based on Multi-Chip Modules. The survey classifies
    the most influential architectures and technologies proposed in the last years,
    with the purpose of offering the reader a comprehensive perspective in the rapidly
    evolving field of deep learning. Finally, it provides some insights into future
    challenges in DL accelerators such as quantum accelerators and photonics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hardware Accelerators, High-Performance Computing, Deep Learning, Deep Neural
    Networks, Emerging Memory Technologies.^†^†copyright: acmcopyright^†^†journalyear:
    2023^†^†doi: XXXXXXX.XXXXXXX^†^†journal: CSUR^†^†ccs: Computer systems organization Architectures^†^†ccs:
    Hardware Reconfigurable logic and FPGAs^†^†ccs: Hardware Emerging technologies^†^†ccs:
    Hardware Very large scale integration design^†^†ccs: Hardware Power and energy^†^†ccs:
    Computing methodologies Machine learning'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the advent of the Exascale era, we have witnessed the convergence between
    High-Performance Computing (HPC) and Artificial Intelligence (AI). The ever-increasing
    computing power of HPC systems and their ability to manage large amounts of data
    made the development of more and more sophisticated machine learning (ML) techniques
    possible. Deep Learning (DL) is a subset of ML and uses artificial Deep Neural
    Networks (DNNs) with multiple layers of artificial neurons to attempt to mimic
    human brain behavior by learning from large amounts of data. Thanks to technological
    and architectural improvements, not only an increasing number of parallel high-end
    processors but also co-processors such as graphics processing units (GPUs) and
    vector/tensor computing units have been integrated into the nodes of HPC systems.
    This supercomputing power enabled the speed up of the automatic training phase
    of DNN models and their subsequent inference phase in the target application scenarios.
    The introduction of the pioneering AlexNet ([Krizhevsky2012,](#bib.bib151) ) at
    the ImageNet challenge in 2012 was enabled by GPU computing, outlining the value
    of acceleration during the training and inference phase. Since then, a multitude
    of DNN models have been developed for various tasks including image recognition
    and classification, Natural Language Processing (NLP), and Generative AI. These
    applications require specialized *hardware accelerators*, to efficiently handle
    the heavy computational demands of DNN algorithms. DL accelerators are currently
    in use in several types of computing systems spanning from ultra-low-power and
    resource-constrained devices on the edge up to servers, HPC infrastructures, and
    data centers.
  prefs: []
  type: TYPE_NORMAL
- en: Scope of the survey. This survey is an attempt to provide an extensive overview
    of the most influential architectures to accelerate DL for high-performance applications.
    The survey highlights various approaches that support DL acceleration including
    GPU-based accelerators, Tensor Processor Units, FPGA-based accelerators, and ASIC-based
    accelerators, such as Neural Processing Units and co-processors on the open-hardware
    RISC-V architecture. The survey also includes accelerators based on emerging technologies
    and computing paradigms, such as 3D-stacked PIM, emerging non-volatile memories
    such as the Resistive switching Random Access Memory (RRAM) and the Phase Change
    Memory (PCM), Neuromorphic Processing Units, and Multi-Chip Modules.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we have reviewed the research on DL accelerators from the past two
    decades, covering a significant time span of literature in this field. We have
    described and referenced about 230 works proposed for DL acceleration. Being DL
    acceleration such a prolific and rapidly evolving field, we do not claim to cover
    exhaustively all the research works appeared so far, but we focused on the most
    influential contributions. Moreover, this survey can be leveraged as a connecting
    point for some previous surveys on accelerators on the AI and DL field  ([chen2020engineering,](#bib.bib40)
    ; [Hassanpour2022,](#bib.bib108) ; [gao2023acm,](#bib.bib82) ; [rathi2023acm,](#bib.bib231)
    ) and other surveys focused on some more specific aspects of DL, such as the architecture-oriented
    optimization of sparse matrices  ([reuther_hpec22,](#bib.bib232) ) and the Neural
    Architecture Search  ([Chitty2022ACMSUR,](#bib.bib46) ).
  prefs: []
  type: TYPE_NORMAL
- en: Organization of the survey. The survey is structured in different categories
    and sub-categories belonging to the areas of computer architecture and hardware
    design. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"), the
    proposed classification is based on several representative features of the accelerators,
    in order to highlight their similarities and differences. To this aim, we organized
    the material in a way that all research papers corresponding to multiple types
    of classifications are cited under each classification. For example, let us consider
    the work $W$, which primarily belongs to the sub-category $X$ where it makes its
    primary contribution. According to our classification policy, this work could
    be cited again in another sub-category $Y$, where it makes its secondary contribution.
    Moreover, under each classification, we have selectively chosen the most notable
    and influential works and, for each work, we focused on its innovative contributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main paper is structured as follows: Section [2](#S2 "2\. Deep Learning
    Background ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms") provides a brief background on dominant DL topologies, while Section [3](#S3
    "3\. GPU- and TPU-based accelerators ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms") reviews the most significant acceleration solutions
    based on GPUs and TPUs. Section [4](#S4 "4\. Hardware Accelerators ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms") introduces
    three types of hardware-based accelerators: FPGA-based, ASIC-based, and accelerators
    based on the open-hardware RISC-V Instruction Set Architecture (ISA). Section [5](#S5
    "5\. Accelerators based on Emerging Paradigms and Technologies ‣ A Survey on Deep
    Learning Hardware Accelerators for Heterogeneous HPC Platforms") describes DL
    accelerators based on emerging computing paradigms and technologies. A final discussion
    on future trends in DL accelerators can be found in Section $6$. The survey is
    complemented by a supplementary document (referenced as Appendix A) including
    some basic definitions and in-depth technological aspects. To conclude, we hope
    this survey could be useful for a wide range of readers, including computer architects,
    hardware developers, HPC engineers, researchers, and technical professionals.
    A major effort was spent to use a clear and concise technical writing style: we
    hope this effort could be useful in particular to the young generations of master’s
    and Ph.D. students. To facilitate the reading, a list of acronyms is reported
    in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ A Survey on Deep Learning Hardware
    Accelerators for Heterogeneous HPC Platforms").'
  prefs: []
  type: TYPE_NORMAL
- en: \forestset
  prefs: []
  type: TYPE_NORMAL
- en: dir tree/.style= for tree= parent anchor=south west, child anchor=west, anchor=mid
    west, inner ysep=-3.5pt, grow’=0, align=left, edge path= [draw, \forestoptionedge]
    (!u.parent anchor) ++(1em,0) —- (.child anchor)\forestoptionedge label; , if n
    children=0 delay= prepend=[,phantom, calign with current] , fit=rectangle, before
    computing xy= l=2em ,
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: dir tree [Survey Organization [§ [2](#S2 "2\. Deep Learning Background ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms") DL Background
    ] [§ [3](#S3 "3\. GPU- and TPU-based accelerators ‣ A Survey on Deep Learning
    Hardware Accelerators for Heterogeneous HPC Platforms") GPU- & TPU-based Accelerators
    [GPU-based Accelerators ] [TPU-based Accelerators ] ] [§ [4](#S4 "4\. Hardware
    Accelerators ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms") Hardware Accelerators [Reconfigurable Hardware Accelerators ]
    [ASIC-based Accelerators [Accelerating Arithmetic Data-paths] [Neural Processing
    Units] [Single-chip NPU] ] [Accelerators based on RISC-V [ISA extensions for (Deep)
    Learning] [Vector Co-processors] [Memory-coupled Neural Processing Units (NPUs)]
    ] ] ]
  prefs: []
  type: TYPE_NORMAL
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: dir tree [ [§ [5](#S5 "5\. Accelerators based on Emerging Paradigms and Technologies
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
    Emerging Paradigms and Technologies [Accelerators for Sparse Matrices ] [3D-stacked
    Processing-in-memory [3-D Stacked PIM Solutions] ] [In-Memory computing][Full-digital
    Neuromorphic Accelerators] [Multi-Chip Modules ] ] [§ [6](#S6 "6\. Conclusions
    and Open Challenges ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms") Conclusions & Open Challenges] ]
  prefs: []
  type: TYPE_NORMAL
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. Organization of the survey
  prefs: []
  type: TYPE_NORMAL
- en: Table 1. List of acronyms
  prefs: []
  type: TYPE_NORMAL
- en: '| Acronym | Acronym | Acronym |'
  prefs: []
  type: TYPE_TB
- en: '| AI: Artificial Intelligence | ASIC: Application Specific Integrated Circuit
    | BRAM: Block Random Access Memory |'
  prefs: []
  type: TYPE_TB
- en: '| CMOS: Complementary Metal Oxide Semiconductor | CNN: Convolutional Neural
    Network | CPU: Central Processing Unit |'
  prefs: []
  type: TYPE_TB
- en: '| DL: Deep Learning | DP: Double Precision | DNN: Deep Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| DRAM: Dynamic Random Access Memory | EDA: Electronic Design Automation |
    FLOPS: Floating Point Operations per Second |'
  prefs: []
  type: TYPE_TB
- en: '| FMA: Fused Multiply-Add | FPGA: Field-Programmable Gate Array | GEMM: General
    Matrix Multiply |'
  prefs: []
  type: TYPE_TB
- en: '| GP-GPU: General-Purpose Graphics Processing Unit | GPU: Graphics Processing
    Unit | HBM: High Bandwidth Memory |'
  prefs: []
  type: TYPE_TB
- en: '| HDL: Hardware Description Language | HLS: High Level Synthesis | HMC: Hybrid
    Memory Cube |'
  prefs: []
  type: TYPE_TB
- en: '| HPC: High-Performance Computing | MLP: Multi-Layer Perceptron | NPU: Neural
    Processing Unit |'
  prefs: []
  type: TYPE_TB
- en: '| IMC: In-Memory Computing | IoT: Internet of Things | ISA: Instruction Set
    Architecture |'
  prefs: []
  type: TYPE_TB
- en: '| MCM: Multi-Chip Module | ML: Machine Learning | NDP: Near Data Processing
    |'
  prefs: []
  type: TYPE_TB
- en: '| NN: Neural Network | NoC: Network on Chip | PCM: Phase Change Memory |'
  prefs: []
  type: TYPE_TB
- en: '| PCU: Programmable Computing Unit | PIM: Processing In-Memory | PULP: Parallel
    Ultra Low Power |'
  prefs: []
  type: TYPE_TB
- en: '| QC: Quantum Computing | QML: Quantum Machine Learning | QNN: Quantized Neural
    Network |'
  prefs: []
  type: TYPE_TB
- en: '| QPU: Quantum Processing Unit | RAM: Random Access Memory | RRAM: Resistive
    RAM |'
  prefs: []
  type: TYPE_TB
- en: '| RISC: Reduced Instruction Set Computer | RNN: Recurrent Neural Network |
    SoC: System on Chip |'
  prefs: []
  type: TYPE_TB
- en: '| SP: Single Precision | SIMD: Single Instruction Multiple Data | SIMT: Single
    Instruction Multiple Thread |'
  prefs: []
  type: TYPE_TB
- en: '| SNN: Spiking Neural Network | SRAM: Static Random Access Memory | TPU: Tensor
    Processing Unit |'
  prefs: []
  type: TYPE_TB
- en: '| TNN: Ternary Neural Network | VPU: Vector Processing Unit | VRAM: Video Random
    Access Memory |'
  prefs: []
  type: TYPE_TB
- en: 2\. Deep Learning Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep Learning ([lecun2015,](#bib.bib158) ; [Schmidhuber_2015,](#bib.bib238)
    ) is a subset of ML methods that can automatically discover the representations
    needed for feature detection or classification from large data sets, by employing
    multiple layers of processing to extract progressively higher-level features.
    The most recent works in literature clearly show that two main DL topologies have
    emerged as dominant: Deep Neural Networks and Transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Concerning DNNs, there are three types of DNNs mostly used today: Multi-Layer
    Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Recurrent Neural
    Networks (RNNs). MLPs ([Rosenblatt1957,](#bib.bib234) ) are feed-forward ANNs
    composed of a series of fully connected layers, where each layer is a set of nonlinear
    functions of a weighted sum of all outputs of the previous one. On the contrary,
    in a CNN ([Lecun1998,](#bib.bib159) ), a convolutional layer extracts the simple
    features from the inputs by executing convolution operations. Each layer is a
    set of nonlinear functions of weighted sums of different subsets of outputs from
    the previous layer, with each subset sharing the same weights. Each convolutional
    layer in the model can capture a different high-level representation of input
    data, allowing the system to automatically extract the features of the inputs
    to complete a specific task, e.g., image classification, face authentication,
    and image semantic segmentation. Finally, RNNs ([Schmidhuber_2015,](#bib.bib238)
    ) address the time-series problem of sequential input data. Each RNN layer is
    a collection of nonlinear functions of weighted sums of the outputs of the previous
    layer and the previous state, calculated when processing the previous samples
    and stored in the RNN’s internal memory. RNN models are widely used in NLP for
    natural language modeling, word embedding, and machine translation. More details
    on concepts and terminology related to DNNs are provided in Sec. A.1 of Appendix
    A.'
  prefs: []
  type: TYPE_NORMAL
- en: Each type of DNN is especially effective for a specific subset of cognitive
    applications. Depending on the target application, and the resource constraints
    of the computing system, different DNN models have been deployed. Besides DNNs,
    Transformer-based models ([vaswaniAttentionAllYou2017,](#bib.bib276) ) recently
    captured a great deal of attention. Transformers were originally proposed for
    NLP ([vaswaniAttentionAllYou2017,](#bib.bib276) ), and are designed to recognize
    long-distance dependencies between data by *attention layers*, where the weights
    used to linearly transform input data are computed dynamically based on the input
    data itself. While DNNs use convolutional layers to perform “local” operations
    on small portions of the input, Transformers use attention layers to perform “global”
    operations on the whole input. Although quite different, DNNs and Transformers
    share many underlying principles (such as gradient descent training, and reliance
    on linear algebra), and many of the DL-dedicated architectures described in this
    survey address both types of topologies.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. GPU- and TPU-based accelerators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1\. GPU-based accelerators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When designing a DL architecture, the decision to include GPUs relies on expected
    benefits in terms of memory bandwidth, datasets size, and optimization of long-running
    tasks. The performance of GPU accelerators could be compared in different ways.
    As a first approximation, their theoretical peak performance and memory bandwidth
    could be used. Anyhow several other architectural characteristics could affect
    the final performance of actual algorithm implementation. To get a better overview
    of their expected performance, running a specific workload, it could be preferable
    to use reference benchmarks, possibly made of representative sets of commonly
    used algorithms implementations. For this reason, different benchmarks have been
    developed, each of them able to test the obtainable performance concerning a given
    workload characteristic, or a given set of application kernels. In the context
    of ML, one of the most used benchmarks is MLPerf ([mlperf,](#bib.bib182) ), which
    has a specific set of training phase tasks ([mlperf-training,](#bib.bib181) ).
    Its results on two different systems, embedding the latest GPU architecture and
    its predecessor (i.e., NVIDIA Hopper and Ampere) are shown in Table [2](#S3.T2
    "Table 2 ‣ 3.1\. GPU-based accelerators ‣ 3\. GPU- and TPU-based accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"),
    highlighting on average an approximate $2\times$ factor of performance improvement.
    Different vendors, like AMD and Intel, have also developed GP-GPU architectures
    mostly oriented to HPC and more recently to AI computing. Yet the terminology
    used by different vendors is not the same, they share most of the hardware details.
    For example, AMD names Compute Unit which NVIDIA calls Streaming Multiprocessor,
    and Intel calls Compute Slice or Execution-Unite (EU). Further, NVIDIA names Warp
    the set of instructions scheduled and executed at each cycle, while AMD uses the
    term Wavefront, and Intel uses the term EU-Thread. Concerning the execution model,
    NVIDIA uses the Single Instruction Multiple Thread (SIMT), while AMD and Intel
    use the Single Instruction Multiple Data (SIMD) ([khairy2019,](#bib.bib141) ).
    In Table [3](#S3.T3 "Table 3 ‣ 3.1\. GPU-based accelerators ‣ 3\. GPU- and TPU-based
    accelerators ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms"), we report the main hardware features of the three most recent
    GP-GPU architectures developed by NVIDIA H100 ([h100,](#bib.bib12) ), AMD ([mi250x,](#bib.bib9)
    ) and Intel ([arc770,](#bib.bib119) ). We compare the peak performance related
    to the 32-bit single- and 64-bit double-precision, and the peak performance achieved
    using half-precision.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2. MLPerf Training v2.1 Benchmark Results (minutes)
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ImageNet | KiTS19 | OpenImages | COCO | LibriSpeech | Wikipedia | Go |'
  prefs: []
  type: TYPE_TB
- en: '|  | ResNet | 3D U-Net | RetinaNet | Mask R-CNN | RNN-T | BERT | Minigo |'
  prefs: []
  type: TYPE_TB
- en: '| 8 $\times$ A100 | 30.8 | 25.6 | 89.1 | 43.1 | 32.5 | 24.2 | 161.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 $\times$ H100 | 14.7 | 13.1 | 38.0 | 20.3 | 18.2 | 6.4 | 174.6 |'
  prefs: []
  type: TYPE_TB
- en: Table 3. Selected hardware features of most recent GP-GPU systems developed
    by NVIDIA, AMD, and Intel
  prefs: []
  type: TYPE_NORMAL
- en: '| Model (Vendor) | H100 (NVIDIA) | Instinct MI250X (AMD) | Arc 770 (Intel)
    |'
  prefs: []
  type: TYPE_TB
- en: '| #physical-cores | 132 | 220 | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| #logical-cores | 16896 | 14080 | 4096 |'
  prefs: []
  type: TYPE_TB
- en: '| Clock (GHz) | 1.6 | 1.7 | 2.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Peak perf. DP (TF) | 30 | 47.9 | 4.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Peak perf. SP (TF) | 60 | 95.8 | 19.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Peak perf. FP16 (TF) | 120 | 383 | 39.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Max Memory (GB) | 80 HBM2e | 128GB HBM2e | 16GB GDDR6 |'
  prefs: []
  type: TYPE_TB
- en: '| Mem BW (TB/s) | 2.0 | 3.2 | 0.56 |'
  prefs: []
  type: TYPE_TB
- en: '| TDP Power (Watt) | 350 | 560 | 225 |'
  prefs: []
  type: TYPE_TB
- en: Over the last years, NVIDIA deployed the DGX ([dgx,](#bib.bib200) ) line of
    server and workstation platforms specialized in using GPUs to accelerate DL applications.
    The DGX systems are based on high-performance commodity CPUs, and a set of GPUs
    interconnected using a motherboard-integrated network based on high-speed NVLink ([nvlink,](#bib.bib201)
    ) technology developed by NVIDIA. The hardware features of such architectures
    are described in detail in Sec. A.2 of Appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. TPU-based accelerators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tensor Processing Units (TPUs) dedicated to training and inference have been
    proposed very early after the emergence of the first large CNN-based applications.
    This is due to the observation that these workloads are dominated by linear algebra
    kernels that can be refactored as matrix multiplications (particularly if performed
    in batches) and that their acceleration is particularly desirable for high-margin
    applications in data centers. More recently, the emergence of exponentially larger
    models with each passing year (e.g., the GPT-2, GPT-3, GPT-4 Transformer-based
    large language models) required a continuous investment in higher-performance
    training architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Google showcased the first TPU ([Jouppi_2017,](#bib.bib132) ; [jouppiMotivationEvaluationFirst2018,](#bib.bib134)
    ) at ISCA in 2017, but according to the original paper the first deployment occurred
    in 2015 – just three years after the “AlexNet revolution”. Their last TPU v4 implementation
    outperforms the previous TPU v3 by 2.1x and improves performance/Watt by 2.7x
    ([jouppi2023isca,](#bib.bib133) ). In 2019, Habana Labs and Intel proposed Goya
    and Gaudi as microarchitectures for the acceleration of inference ([medina2019hotchips,](#bib.bib183)
    ). While Google and Intel rely on a mixture of in-house designs and GPUs, the
    other main data center providers typically relied on NVIDIA GPUs to serve DL workloads.
    Starting from the Volta architecture ([choquetteVoltaPerformanceProgrammability2018,](#bib.bib48)
    ) and continuing with Ampere ([choquetteA100DatacenterGPU2021,](#bib.bib49) )
    and Hopper ([choquetteNVIDIAHopperH1002023,](#bib.bib47) ; [elsterNvidiaHopperGPU2022,](#bib.bib70)
    ), NVIDIA has embedded inside the GPU Streaming Multiprocessors the counterpart
    of smaller TPUs, i.e., TensorCores.
  prefs: []
  type: TYPE_NORMAL
- en: GraphCore Colossus Mk1 and Mk2 IPUs ([knowlesGraphcore2021,](#bib.bib148) ;
    [jiaDissectingGraphcoreIPU2019,](#bib.bib126) ) target the GNNs, DNNs, and Transformers
    training employing a tiled many-core architecture of relatively simple processors.
    GraphCore focuses on a high power- and cost-efficient memory hierarchy that does
    not rely on high-bandwidth off-chip HBM, but on cheaper DRAM chips combined with
    a large amount of on-chip SRAM (in the order of 1 GiB per chip). According to
    GraphCore, this design achieves $\sim$2$\times$ the energy efficiency of an NVIDIA
    Ampere GPU and $\sim$3$\times$ that of a Google TPUv3 on sustained workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Concerning academic and research-proposed architectures, IBM Research focused
    on introducing techniques to reduce the precision of data formats used for training ([agrawal20219,](#bib.bib2)
    ; [venkataramaniRaPiDAIAccelerator2021,](#bib.bib279) ), introducing Hybrid-FP8
    formats in training ASICs and tensor processors. A similar effort is performed
    by the authors of Cambricon-Q ([zhaoCambriconQHybridArchitecture2021,](#bib.bib307)
    ), which also introduce further improvements to exploit the statistical properties
    of tensors to minimize bandwidth consumption and maximize efficiency. Finally,
    Gemmini ([gonzalez16mm106GOPS2021,](#bib.bib92) ; [gencGemminiEnablingSystematic2021,](#bib.bib87)
    ) and RedMulE ([tortorellaRedMulECompactFP162022a,](#bib.bib266) ; [tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265)
    ) are efforts to introduce tensor processor hardware IPs (respectively, generated
    from a template and hand-tuned) that can be integrated inside System-on-Chips,
    similarly to what NVIDIA does with TensorCores.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Hardware Accelerators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typical HPC workloads, like genomics, astrophysics, finance, and cyber security,
    require the elaboration of massive amounts of data and they can take advantage
    of DL methods with results that can surpass human ability ([Bengio_2009,](#bib.bib16)
    ; [Schmidhuber_2015,](#bib.bib238) ; [Goodfellow_2016,](#bib.bib93) ; [SPAGNOLO20201,](#bib.bib252)
    ). However, an ever-increasing computing power, a rapid change of the data analysis
    approaches, and the introduction of novel computational paradigms are needed.
    DL models rely on remarkable computational complexities that can be efficiently
    supported, without renouncing a good trade-off between speed, energy efficiency,
    design effort, and cost, by optimized hardware platforms that are able to provide
    high levels of parallelism and a considerable amount of memory resources. These
    platforms can be developed using CPUs, GPUs, FPGAs, CGRAs, and ASICs ([Goodfellow_2016,](#bib.bib93)
    ; [DHILLESWARARAO_2022,](#bib.bib63) ; [Sze_2017,](#bib.bib261) ; [Machupalli_2022,](#bib.bib176)
    ; [Du_2018,](#bib.bib67) ; [Wang_2019,](#bib.bib289) ; [Leibo2019ACMSUR,](#bib.bib171)
    ). CPUs may have higher cache size and higher on-chip bandwidth than GPUs and
    reconfigurable architectures, but they show a limited ability to process large
    amounts of data in parallel. On the other hand, with their high throughput and
    parallelism, GPUs are extremely efficient in terms of performance, but, as a drawback,
    they consume a lot of power and are much more expensive than their counterparts.
    Heterogeneous computing platforms based on modern FPGAs achieve moderate speed
    and consume less energy compared to GPUs, despite limited computing and memory
    resources ([Qadeer2015,](#bib.bib225) ; [Sze_2017,](#bib.bib261) ; [DHILLESWARARAO_2022,](#bib.bib63)
    ). Conversely, ASICs take longer design times, require higher design efforts,
    and do not offer flexibility, but they provide optimum computational speed and
    power consumption. A good trade-off between speed, power consumption, and design
    effort is offered by CGRAs that exhibit near-ASIC energy efficiency and performances
    with near-FPGA reconfigurability levels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/608526b17cd73d3a5f950458d9c127c2.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/681b55d873e3b349a21a3a8cc52037b7.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/049e980a46ac167faa6b2894bc30b5da.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2. Dataflows in DL accelerators: (a) Weights stationary; (b) Output
    stationary; (c) Input stationary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Independently on the technology, a common problem in the design of accelerators
    is the energy and latency cost of accessing the off-chip DRAM memory, in particular
    considering the significant amount of data that the target HPC applications need
    to process. As sketched in Figure [2](#S4.F2 "Figure 2 ‣ 4\. Hardware Accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"),
    different data reuse and stationary strategies can be exploited to reduce the
    number of accesses, each strategy offers a certain benefit ([Park2015,](#bib.bib213)
    ; [Peemen2013,](#bib.bib217) ; [Sankaradas2009,](#bib.bib236) ; [Sriam2011,](#bib.bib256)
    ; [Cavigelli2017,](#bib.bib34) ; [Gupta2015,](#bib.bib99) ). In weight stationary
    dataflow, convolutional weights are fixed and stored in the local memory of the
    Processing Elements (PEs) and reused on input activations uploaded step-by-step
    from the external DRAM. Conversely, in output stationary dataflow, partial outputs
    from the PEs are stored locally and reused step-by-step until the computation
    is completed. Then, just the final results are moved to the external DRAM. An
    efficient alternative is input stationary dataflow: in this case, input activations
    are stored in the PEs’ local memory, while weights are uploaded from the external
    DRAM and sent to the PEs. Another approach common to many accelerators is the
    introduction of *quantization* to reduce the data type width. Quantization represents
    an open challenge in designing DL models and many recent studies addressed this
    topic ([quant2021,](#bib.bib88) ; [Liu2022,](#bib.bib173) ). Integer or fixed-point
    data formats are generally preferred over the more computationally-intensive floating-point
    ones. This guarantees better memory occupation, lower computational cost, and
    higher robustness of the model ([jin2022fnet,](#bib.bib129) ). Extreme quantization
    techniques that use only one bit for the data stored (Binary Neural Networks ([BNN2020,](#bib.bib227)
    )) are widely used for very large networks but to get a comparable accuracy they
    require 2-11$\times$ the number of parameters and operations ([Umuroglu2017,](#bib.bib270)
    ), making them not suitable for complex problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Reconfigurable Hardware Accelerators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FPGAs and CGRAs are highly sought-after solutions to hardware accelerate a wide
    range of applications, including DL. The main feature of such reconfigurable platforms
    is the ability to support different computational requirements by repurposing
    the underlying hardware accelerators at deploy-time and also at runtime. More
    details on FPGA technologies and related EDA frameworks are respectively provided
    in Sections A.3 and A.4 of Appendix A. Several FPGA-based hardware accelerators
    for DL are structured as heterogeneous embedded systems ([Ma2018,](#bib.bib174)
    ; [Yazdanbakhsh2018,](#bib.bib298) ; [Aimar2019,](#bib.bib4) ; [Perri2020,](#bib.bib219)
    ; [Li2021,](#bib.bib164) ) that mainly consist of a general-purpose processor,
    for running the software workload; a computational module, designed to speed up
    common DL operators, like convolutions ([Qiu2016,](#bib.bib229) ; [Venieris2019,](#bib.bib277)
    ), de-convolutions ([Chang2020,](#bib.bib35) ; [Sestito2021,](#bib.bib240) ),
    pooling, fully connected operations, activation, and softmax functions ([Spagnolo2022_1,](#bib.bib253)
    ; [Spagnolo2022_2,](#bib.bib254) ); and a memory hierarchy to optimize data movement
    to/from the external DRAM to store data to be processed and computational results.
    A typical approach to accelerate convolutions consists of a systolic array architecture
    (SA), a regular pattern that can be easily replicated ([Xuechao2017,](#bib.bib292)
    ). Each PE in the array is a SIMD vector accumulation module where inputs and
    weights are supplied at each cycle by shifting them from the horizontally and
    vertically adjacent PEs (Figure [3(a)](#S4.F3.sf1 "In Figure 3 ‣ 4.1\. Reconfigurable
    Hardware Accelerators ‣ 4\. Hardware Accelerators ‣ A Survey on Deep Learning
    Hardware Accelerators for Heterogeneous HPC Platforms")). The use of pipelined
    groups of PEs with short local communication and regular architecture enables
    a high clock frequency and limited global data transfer (Figure [3(b)](#S4.F3.sf2
    "In Figure 3 ‣ 4.1\. Reconfigurable Hardware Accelerators ‣ 4\. Hardware Accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/800c4452f6812b62722462a610689803.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2046a6a03193da9f7f71e2233bc9ed13.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3. FPGA accelerators: (a) Systolic array accelerator; (b) Pipelined
    dataflow accelerator.'
  prefs: []
  type: TYPE_NORMAL
- en: Although FPGAs have traditionally been proposed as accelerators for edge applications,
    they are starting to be adopted also in data centers. Microsoft’s Project Brainwave
    ([Brainwave,](#bib.bib76) ) uses several FPGA boards to accelerate the execution
    of RNNs in the cloud, exploiting the reconfigurability to adapt the platform to
    different DL models. One way to face the limitations imposed by the capability
    of FPGAs to effectively map very large DL models is to use a deeply pipelined
    multi-FPGA design. Recent studies focus on optimizing this type of architecture
    and maximizing the overall throughput ([Zhang2016,](#bib.bib303) ; [Rahman2017,](#bib.bib230)
    ; [Junnan2019,](#bib.bib242) ). In these application contexts, CGRAs represent
    an alternative to FPGAs, providing reconfigurability with coarser-grained functional
    units. They are based on an array of PEs, performing the basic arithmetic, logic,
    and memory operations at the word level and using a small register file as temporary
    data storage. Neighboring PEs are connected through reconfigurable routing that
    allows transferring intermediate results of the computations towards the proper
    neighbors for the next computational step. CGRAs can represent a powerful solution
    to accelerate dense linear algebra applications, such as ML, image processing,
    and computer vision ([amber2022vlsi,](#bib.bib30) ; [tangram,](#bib.bib84) ).
    Thanks to parallel computing and time-multiplexing, CGRAs can efficiently support
    and combine spatial and temporal computational models. Furthermore, they are flexible
    enough for specific domains, and their interconnections, being not as complex
    as those present on FPGAs, provide remarkable advantages in terms of speed, energy
    efficiency, and resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. ASIC-based Accelerators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1\. Accelerating Arithmetic Data-paths
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Performance achievable with ASIC accelerators for the inference of DL models
    is mainly dependent on the structure of the arithmetic data path. At its core,
    DL systems perform several finite impulse response operations over a large set
    of data. Hence the accelerator can be optimized by exploiting techniques used
    for the efficient implementation of the underlying arithmetic operations. As shown
    in Figure  [4](#S4.F4 "Figure 4 ‣ 4.2.1\. Accelerating Arithmetic Data-paths ‣
    4.2\. ASIC-based Accelerators ‣ 4\. Hardware Accelerators ‣ A Survey on Deep Learning
    Hardware Accelerators for Heterogeneous HPC Platforms"), three main types of optimization
    can be performed on the arithmetic data path. The first optimization approach
    takes into account that convolution is one of the main operations performed in
    a DL system. As demonstrated in ([Cheng_2004,](#bib.bib43) ; [Tsao_2012,](#bib.bib269)
    ), mono-dimensional convolutions can be efficiently computed by performing a reduced
    number of multiplications, thus improving the trade-off between the throughput
    of the circuit and the number of hardware resources needed for its implementation.
    This technique has been further developed in ([Wang_2018,](#bib.bib287) ; [Cheng_2020,](#bib.bib44)
    ; [Wang_2022,](#bib.bib285) ), where it is applied to multi-dimensional convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: The multiplication itself can be implemented with optimized circuits. In ([MinJou_1999,](#bib.bib131)
    ; [Petra_2011,](#bib.bib220) ; [Frustaci2020,](#bib.bib80) ) the area and power
    dissipation of the multiplier circuit is reduced by discarding part of the partial
    products used to compute the result. These circuits trade off precision and circuit
    complexity to improve speed and power consumption. This approach is often referred
    to as *approximate computing paradigm*, providing a way to approximate the design
    at the cost of an acceptable accuracy loss. Approximate computing techniques proposed
    in ([Kulkarni_2011,](#bib.bib152) ; [Zervakis_2016,](#bib.bib302) ) provide a
    reduced complexity multiplier by modifying the way the partial products are computed.
    In ([Zacharelos_2022,](#bib.bib300) ), a recursive approach is proposed, in which
    the multiplier is decomposed into small approximate units. In the approach proposed
    in ([Esposito_2017,](#bib.bib72) ), the approximation is implemented in the way
    the partial products are summed. Finally, the approximate computing paradigm can
    also be implemented in the 4-2 compressors ([Ahmadinejad_2019,](#bib.bib3) ; [Yang_2015,](#bib.bib297)
    ; [Ha_2018,](#bib.bib101) ; [Strollo_2020,](#bib.bib259) ; [Park_2021,](#bib.bib208)
    ; [Kong_2021,](#bib.bib149) ) that represent the atomic blocks used for the compression
    of the partial products.
  prefs: []
  type: TYPE_NORMAL
- en: Different from the previous works, the segmentation method aims at reducing
    the bit-width of the multiplicands. The approaches in ([Hashemi_2015,](#bib.bib107)
    ; [Vahdat_2019,](#bib.bib273) ) describe a dynamic segmentation method in which
    the segment is selected starting from the leading one of the multiplicand binary
    representation. On the contrary, the paper in ([Narayanamoorthy_2015,](#bib.bib193)
    ) proposes a static segmentation method, which reduces the complexity of the selection
    mechanism by choosing between two segments with a fixed number of bits. The paper
    in ([Strollo_2022,](#bib.bib260) ) improves the accuracy of the static segmentation
    method multipliers by reducing the maximum approximation error, whereas in ([Li_2021,](#bib.bib166)
    ) the authors propose a hybrid approach in which a static stage is cascaded to
    a dynamic one.
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Arithmetic
    Data-path [Convolution Optimization [ Cheng ([Cheng_2004,](#bib.bib43) )
  prefs: []
  type: TYPE_NORMAL
- en: Tsao ([Tsao_2012,](#bib.bib269) )
  prefs: []
  type: TYPE_NORMAL
- en: Wang ([Wang_2018,](#bib.bib287) )
  prefs: []
  type: TYPE_NORMAL
- en: Cheng ([Cheng_2020,](#bib.bib44) )
  prefs: []
  type: TYPE_NORMAL
- en: Wang ([Wang_2022,](#bib.bib285) )
  prefs: []
  type: TYPE_NORMAL
- en: '] ], [Approximate Computing [Multiplication [ Jou ([MinJou_1999,](#bib.bib131)
    ), Petra ([Petra_2011,](#bib.bib220) )'
  prefs: []
  type: TYPE_NORMAL
- en: Kulkarni ([Kulkarni_2011,](#bib.bib152) ), Zervakis ([Zervakis_2016,](#bib.bib302)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Zacharelos ([Zacharelos_2022,](#bib.bib300) ), Esposito ([Esposito_2017,](#bib.bib72)
    )
  prefs: []
  type: TYPE_NORMAL
- en: '] ], [4-2 compressors [ Ahmadinejad ([Ahmadinejad_2019,](#bib.bib3) ), Yang
    ([Yang_2015,](#bib.bib297) )'
  prefs: []
  type: TYPE_NORMAL
- en: Ha ([Ha_2018,](#bib.bib101) ), Strollo ([Strollo_2020,](#bib.bib259) )
  prefs: []
  type: TYPE_NORMAL
- en: Park ([Park_2021,](#bib.bib208) ), Kong ([Kong_2021,](#bib.bib149) )
  prefs: []
  type: TYPE_NORMAL
- en: '] ] ], [Segmentation Methods [ Hashemi ([Hashemi_2015,](#bib.bib107) )'
  prefs: []
  type: TYPE_NORMAL
- en: Vahdat ([Vahdat_2019,](#bib.bib273) )
  prefs: []
  type: TYPE_NORMAL
- en: Narayanamoorthy ([Narayanamoorthy_2015,](#bib.bib193) )
  prefs: []
  type: TYPE_NORMAL
- en: Strollo ([Strollo_2022,](#bib.bib260) )
  prefs: []
  type: TYPE_NORMAL
- en: Li ([Li_2021,](#bib.bib166) )
  prefs: []
  type: TYPE_NORMAL
- en: '] ] ]'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4. Taxonomy of the data-path architectures described in Section [4.2.1](#S4.SS2.SSS1
    "4.2.1\. Accelerating Arithmetic Data-paths ‣ 4.2\. ASIC-based Accelerators ‣
    4\. Hardware Accelerators ‣ A Survey on Deep Learning Hardware Accelerators for
    Heterogeneous HPC Platforms")
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Neural Processing Units (NPUs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The purpose of an NPU is to accelerate the performance and improve the energy
    efficiency of specific tasks offloaded from the CPU ([song2019isscc,](#bib.bib248)
    ) ([hung2021jsscc,](#bib.bib115) ). In particular, NPUs are designed to accommodate
    a reasonable amount of multiply/accumulate (MAC) units, which are the PEs devised
    in the convolutional and fully-connected layers of DNNs ([Chen_2017,](#bib.bib41)
    ; [desoli17isscc,](#bib.bib62) ).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4. Summary of NPU accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: '| NPU | Process | Area [mm²] | Supply voltage [V] | Max. Freq. [MHz] | PP [TOPS]
    | Max EE [TOPS/W] | Max AE [TOPS/mm²] |'
  prefs: []
  type: TYPE_TB
- en: '| Samsung ([song2019isscc,](#bib.bib248) ) | 8 nm | 5.5 | 0.8 | 933 | 6.9 |
    3.4 | 1.25 |'
  prefs: []
  type: TYPE_TB
- en: '| UM+NVIDIA ([zhang2019vlsi,](#bib.bib304) ) | 16 nm | 2.4 | 0.8 | 480 | -
    | 3.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MediaTek ([lin2020isscc,](#bib.bib168) ) | 7 nm | 3.04 | 0.825 | 880 | 3.6
    | 6.55 | 1.18 |'
  prefs: []
  type: TYPE_TB
- en: '| Alibaba ([jiao2020isscc,](#bib.bib128) ) | 12 nm | 709 | - | 700 | 825 |
    499 | 1.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Samsung ([park2021isscc,](#bib.bib211) ) | 5 nm | 5.46 | 0.9 | 1196 | 29.4
    | 13.6 | 2.69 |'
  prefs: []
  type: TYPE_TB
- en: '| Samsung ([park2022isscc,](#bib.bib212) ) | 4 nm | 4.74 | 1 | 1197 | 39.3
    | 11.59 | 6.9 |'
  prefs: []
  type: TYPE_TB
- en: Each PE contains a synaptic weight buffer and the MAC units to perform the computation
    of a neuron, namely, multiplication, accumulation, and an activation function
    (e.g., sigmoid). A PE can be realized entirely with a full-CMOS design or by using
    emerging non-volatile memories such as RRAM and PCM to perform in situ matrix-vector
    multiplication as in the RENO chip ([liu2015dac,](#bib.bib172) ) or as in the
    MAC units proposed in ([xue2019isscc,](#bib.bib296) ; [narayanan2021ted,](#bib.bib194)
    ). The advantage of these architectures is that only the input and final output
    are digital; the intermediate results are all analog and are coordinated by analog
    routers. Data converters (DACs and ADCs) are required only when transferring data
    between the NPU and the CPU with an advantage in terms of energy efficiency (the
    work in ([xue2019isscc,](#bib.bib296) ) reports an energy efficiency of 53.17
    TOPS/W), although there are insufficient experimental data to support this evidence
    in comparison with full-digital NPUs. In Table [4](#S4.T4 "Table 4 ‣ 4.2.2\. Neural
    Processing Units (NPUs) ‣ 4.2\. ASIC-based Accelerators ‣ 4\. Hardware Accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"),
    we reported the main features of several full-digital NPUs designs by also highlighting
    their Peak Performance (PP), Energy Efficiency (EE), and Area Efficiency (AE).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3\. Single-chip NPUs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the DNN landscape, single-chip domain-specific accelerators achieved great
    success in both cloud and edge scenarios. These custom architectures offer better
    performance and energy efficiency concerning CPUs/GPUs thanks to an optimized
    data flow (or data reuse pattern) that reduces off-chip memory accesses, while
    improving the system efficiency ([chen2020engineering,](#bib.bib40) ). The DianNao
    series represents a full digital stand-alone DNN accelerator that introduces a
    customized design to minimize the memory transfer latency and enhance the system
    efficiency. DaDianNao ([chen2016commacm,](#bib.bib39) ) targets the datacenter
    scenario and integrates a large on-chip embedded dynamic random access memory
    (eDRAM) to avoid the long main memory access time. The same principle applies
    to the embedded scenario. ShiDianNao ([chen2016commacm,](#bib.bib39) ) is a DNN
    accelerator dedicated to CNN applications. Using a weight-sharing strategy, its
    footprint is much smaller than the previous design. It is possible to map all
    of the CNN parameters onto a small on-chip static random access memory (SRAM)
    when the CNN model is small. In this way, ShiDianNao avoids expensive off-chip
    DRAM access time and achieves 60 times more energy efficiency compared to DianNao.
    Furthermore, domain-specific instruction set architectures (ISAs) have been proposed
    to support a wide range of NN applications. Cambricon ([zhang2016micro,](#bib.bib305)
    ) and EIE ([han2016isca,](#bib.bib103) ) are examples of architectures that integrate
    scalar, vector, matrix, logical, data transfer, and control instructions. Their
    ISA considers data parallelism and the use of customized vector/matrix instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Eyeriss is another notable accelerator ([Chen_2017,](#bib.bib41) ) that can
    support high throughput inference and optimize system-level energy efficiency,
    also including off-chip DRAMs. The main features of Eyeriss are a spatial architecture
    based on an array of 168 PEs that creates a four-level memory hierarchy, a dataflow
    that reconfigures the spatial architecture to map the computation of a given CNN
    and optimize towards the best energy efficiency, a network-on-chip (NoC) architecture
    that uses both multi-cast and point-to-point single-cycle data delivery, and run-length
    compression (RLC) and PE data gating that exploit the statistics of zero data
    in CNNs to further improve EE.
  prefs: []
  type: TYPE_NORMAL
- en: In ([desoli17isscc,](#bib.bib62) ), STMicroelectronics introduced the Orlando
    system-on-chip, a 28nm FDSOI-based CNN accelerator integrating an SRAM-based architecture
    with low-power features and adaptive circuitry to support a wide voltage range.
    Such a DNN processor provides an energy-efficient set of convolutional accelerators
    supporting kernel compression, an on-chip reconfigurable data-transfer fabric,
    a power-efficient array of DSPs to support complete real-world computer vision
    applications, an ARM-based host subsystem with peripherals, a range of high-speed
    I/O interfaces, and a chip-to-chip multilink to pair multiple accelerators together.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5. Summary of single-chip digital DNN accelerators
  prefs: []
  type: TYPE_NORMAL
- en: '| Accelerator | Technology | Application | Area [mm²] | Power [mW] | Performance
    [GOPS] | EE [GOPS/W] |'
  prefs: []
  type: TYPE_TB
- en: '| DaDianNao ([chen2016commacm,](#bib.bib39) ) | 28 nm | DNN | 67.7 | 15970
    | 5585 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ShiDianNao ([chen2016commacm,](#bib.bib39) ) | 65 nm | CNN | 4.86 | 320 |
    194 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Cambricon ([zhang2016micro,](#bib.bib305) ) | 65 nm | CNN | 6.38 | 954 |
    1111.92 | - |'
  prefs: []
  type: TYPE_TB
- en: '| EIE ([han2016isca,](#bib.bib103) ) | 28 nm | CNN+LSTM | 63.8 | 2360 | 1.6
    | 177.78 |'
  prefs: []
  type: TYPE_TB
- en: '| Eyeriss ([Chen_2017,](#bib.bib41) ) | 65 nm | CNN | 16 | 450 | 33.6 | 74.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| STM ([desoli17isscc,](#bib.bib62) ) | 28 nm | CNN | 34.8 | 39 | 750 | 2900
    |'
  prefs: []
  type: TYPE_TB
- en: '| IBM ([oh2020vlsic,](#bib.bib202) ) | 14 nm | CNN+LSTM+RNN | 9.84 | - | 3000
    | 1100 |'
  prefs: []
  type: TYPE_TB
- en: '| IBM ([lee2022jsscc,](#bib.bib160) ) | 7 nm | CNN+RNN | 19.6 | - | 16300 |
    3580 |'
  prefs: []
  type: TYPE_TB
- en: IBM presented in ([oh2020vlsic,](#bib.bib202) ) a processor core for AI training
    and inference tasks applicable to a broad range of neural networks (such as CNN,
    LSTM, and RNN). High compute efficiency is achieved for robust FP16 training via
    efficient heterogeneous 2-D systolic array-SIMD compute engines that leverage
    DLFloat16 FPUs. A modular dual-corelet architecture with a shared scratchpad memory
    and a software-controlled network/memory interface enables scalability to many-core
    SoCs for scale-out paradigms. In 2022, IBM also presented a 7-nm four-core mixed-precision
    AI chip ([lee2022jsscc,](#bib.bib160) ) that demonstrates leading-edge power efficiency
    for low-precision training and inference without model accuracy degradation. The
    chip is based on a high-bandwidth ring interconnect to enable efficient data transfers,
    while workload-aware power management with clock frequency throttling maximizes
    the application performance within a given power envelope.
  prefs: []
  type: TYPE_NORMAL
- en: Qualcomm presented an AI core that is a scalar 4-way VLIW architecture that
    includes vector/tensor units and lower precision to enable high-performance inference
    ([karam2021hotchip,](#bib.bib36) ). The design uses a 7 nm technology and is sought
    to be integrated into the AI 100 SoC to reach up to 149 TOPS with a power efficiency
    of 12.37 TOPS/W.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Accelerators based on open-hardware RISC-V
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RISC-V is an open-source, modular instruction set architecture (ISA) that is
    gaining popularity in computer architecture research due to its flexibility and
    suitability for integration with acceleration capabilities for DL. The RISC-V
    ISA is designed with a small, simple core that can be extended with optional instruction
    set extensions (ISEs) to support various application domains. RISC-V offers several
    advantages for DL acceleration research. First, the modular nature of the ISA
    allows researchers to easily integrate acceleration capabilities as ISEs, which
    can be customized to suit the specific needs of different DL models. Second, RISC-V
    supports a set of standard interfaces, such as AXI4, that can be used to interface
    with external acceleration units integrated on the same System-on-Chip at various
    levels of coupling. This makes it easy to integrate specialized DL hardware accelerators
    into RISC-V-based systems. Moreover, the defining feature of the RISC-V ISA is
    its openness, meaning that anybody can design a RISC-V implementation without
    paying royalties or needing a particular license. Thanks to this non-technical
    advantage against other ISAs (such. as ARM, x86), RISC-V has gained significant
    attention from academia and emerging startups.
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [RISC-V for
    Deep Learning [Specialized ISA & SIMD [ Dustin ([ottaviDustin16CoresParallel2023,](#bib.bib205)
    ), Marsellus ([conti22124TOPS2023,](#bib.bib53) )
  prefs: []
  type: TYPE_NORMAL
- en: Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64) ), Manticore ([zarubaManticore4096CoreRISCV2021,](#bib.bib301)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Celerity ([davidsonCelerityOpenSource511Core2018,](#bib.bib56) ), Tenstorrent ([vasiljevicComputeSubstrateSoftware2021,](#bib.bib275)
    )
  prefs: []
  type: TYPE_NORMAL
- en: exSDOTP ([bertacciniMiniFloatNNExSdotpISA2022,](#bib.bib17) ), Esperanto ([ditzelAcceleratingMLRecommendation2022,](#bib.bib65)
    )
  prefs: []
  type: TYPE_NORMAL
- en: CNC ([chenEightCoreRISCVProcessor2022,](#bib.bib38) ), Cococcioni et al. ([cococcioniLightweightPositPorcessing2021,](#bib.bib51)
    )
  prefs: []
  type: TYPE_NORMAL
- en: PERCIVAL ([mallasenPERCIVAL2022,](#bib.bib178) ), Wang et al. ([wangWinogradBasedConvolution2021,](#bib.bib288)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Tang et al. ([tangGeneralPurposeGraphConvolutionNeuralAccelerator2022,](#bib.bib264)
    ), RISC-VTF ([JiaoRISCVExtensionTransformer2021,](#bib.bib127) )
  prefs: []
  type: TYPE_NORMAL
- en: Paulin et al. ([paulinRNNBasedRadioResource2021,](#bib.bib215) )
  prefs: []
  type: TYPE_NORMAL
- en: '] ], [Vector Co-Processor [ Lee et al. ([lee201445nm,](#bib.bib162) ), AVA ([lazoAdaptableRegisterFile2022,](#bib.bib157)
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Spatz ([cavalcanteSpatzCompactVector2022,](#bib.bib33) ), Vitruvius+ ([minerviniVitruviusAreaEfficientRISCV2023,](#bib.bib186)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Ara ([cavalcanteAra1GHzScalable2020,](#bib.bib32) ), Perotti et al. ([perottiNewAraVector2022,](#bib.bib218)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Xuantie ([chenXuantie910CommercialMultiCore2020,](#bib.bib37) ), Arrow ([assirArrowRISCVVector2021,](#bib.bib15)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Ventana ([ventanaProduct,](#bib.bib280) )
  prefs: []
  type: TYPE_NORMAL
- en: '] ], [Memory-Coupled NPUs [L1-Coupled [ Darkside ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86)
    ), Marsellus ([conti22124TOPS2023,](#bib.bib53) )'
  prefs: []
  type: TYPE_NORMAL
- en: Garofalo et al. ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85) ),
    Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235) )
  prefs: []
  type: TYPE_NORMAL
- en: RedMulE ([tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265) ; [tortorellaRedMulECompactFP162022a,](#bib.bib266)
    ), Archimedes ([prasadSpecializationMeetsFlexibility,](#bib.bib222) )
  prefs: []
  type: TYPE_NORMAL
- en: Bruschi et al. ([bruschiEndtoEndDNNInference2022,](#bib.bib26) ), GAP9 ([gap9Product,](#bib.bib94)
    )
  prefs: []
  type: TYPE_NORMAL
- en: '] ], [L2-Coupled [ SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136) ), SamurAI ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187)
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Gemmini ([gencGemminiEnablingSystematic2021,](#bib.bib87) ), DIANA ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112)
    )
  prefs: []
  type: TYPE_NORMAL
- en: TinyVers ([jainTinyVersTinyVersatile2023,](#bib.bib121) ), Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Simba ([shao_micro19,](#bib.bib243) ), Lee et al. ([lee64TOPSEnergyEfficientTensor2022,](#bib.bib161)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Tambe et al. ([tambe2212nm182023,](#bib.bib263) )
  prefs: []
  type: TYPE_NORMAL
- en: '] ], [L3-Coupled [ Gonzalez et al. ([gonzalez16mm106GOPS2021,](#bib.bib92)
    )'
  prefs: []
  type: TYPE_NORMAL
- en: ESP ([jia12nmAgileDesignedSoC2022,](#bib.bib124) )
  prefs: []
  type: TYPE_NORMAL
- en: '] ], ] ]'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5. Taxonomy of RISC-V based acceleration units discussed in Section [4.3](#S4.SS3
    "4.3\. Accelerators based on open-hardware RISC-V ‣ 4\. Hardware Accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
  prefs: []
  type: TYPE_NORMAL
- en: Figure [5](#S4.F5 "Figure 5 ‣ 4.3\. Accelerators based on open-hardware RISC-V
    ‣ 4\. Hardware Accelerators ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms") reports a synthetic taxonomy of the RISC-V-based
    architectures for DL acceleration discussed in this survey.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1\. RISC-V ISA extensions for (Deep) Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: (Deep) neural networks are often limited by the computing and memory resources
    used by their large number of weights. Weight compression via alternative or quantized
    number representations is often adopted to speed up and optimize the performance
    of neural network models. Works in ([cococcioniLightweightPositPorcessing2021,](#bib.bib51)
    ; [mallasenPERCIVAL2022,](#bib.bib178) ) propose ISA extensions for *posit* numbers
    which can be used to do weight compression. Posit is an alternative representation
    to the standard IEEE floating point format for real numbers. Posit numbers need
    fewer bits to obtain the same precision or dynamic range of IEEE floats allowing
    them to store more weights in a same-sized memory. For example, the work in ([cococcioniLightweightPositPorcessing2021,](#bib.bib51)
    ) provides an efficient conversion between 8- or 16-bit posits and 32-bit IEEE
    floats or fixed point formats with little loss in precision leading to a 10x speedup
    in inference time. Other works directly address the compute-intensive parts of
    different neural networks, in particular CNNs, GCNs, and transformers. ([wangWinogradBasedConvolution2021,](#bib.bib288)
    ) proposes a new Winograd-based convolution instruction to speed up the time-consuming
    convolutional layers in CNNs. The matrix convolution between the CNN kernel and
    the input data cannot be executed efficiently using the standard RISC-V instructions.
    The proposed extension enables to compute a convolution producing a $2\times 2$
    output using a $3\times 3$ kernel on a $4\times 4$ input in a single instruction
    using 19 clock cycles instead of multiple instructions totaling 140 cycles using
    the standard RISC-V ISA. ([tangGeneralPurposeGraphConvolutionNeuralAccelerator2022,](#bib.bib264)
    ) addresses the computational bottlenecks of GCNs by designing a set of general-purpose
    instructions for GCNs that mitigate the compute inefficiencies in aggregating
    and combining feature vectors. As such the authors combine the software programmability
    given by the RISC-V ISA with the compute efficiency of GCN accelerators. Similarly,
    ([JiaoRISCVExtensionTransformer2021,](#bib.bib127) ) focuses on transformer models.
    Notably, the extension comprises instructions to accelerate the well-known ReLU
    activation and softmax functions. Paulin et al. ([paulinRNNBasedRadioResource2021,](#bib.bib215)
    ) performs a similar task but focuses on RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Following the trend to use aggressive quantization schemes to accelerate inference
    of DNNs, many ISA extensions focus on low-bit-width arithmetics to implement Quantized
    Neural Networks (QNNs), often combined with multi-core parallel execution to further
    boost performance and efficiency. Several developments augment the PULP RI5CY
    core used in Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235) ) to improve its energy
    efficiency on QNNs. Marsellus ([conti22124TOPS2023,](#bib.bib53) ) (16 cores)
    and Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64) ) (8 cores) use Xpulpnn,
    an ISA extension for low-bitwidth (2/4-bit) integer dot-products used to accelerate
    symmetric precision QNNs. Dustin ([ottaviDustin16CoresParallel2023,](#bib.bib205)
    ) (16 cores) also exploits a similar concept, but it also introduces a lockstep
    mechanism to operate all the cores in a SIMD fashion, further increasing their
    efficiency. Manticore ([zarubaManticore4096CoreRISCV2021,](#bib.bib301) ), Celerity ([davidsonCelerityOpenSource511Core2018,](#bib.bib56)
    ), Esperanto ([ditzelAcceleratingMLRecommendation2022,](#bib.bib65) ) and Tenstorrent ([vasiljevicComputeSubstrateSoftware2021,](#bib.bib275)
    ) exploit ISA extensions for faster RISC-V based DL workload execution in the
    context of many-core architectures where a large number of very simple cores cooperate.
    As these architectures are targeted at server-based training as well as inference,
    they typically focus on floating point multiply-accumulate and dot-product operations,
    such as exSDOTP ([bertacciniMiniFloatNNExSdotpISA2022,](#bib.bib17) ).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2\. RISC-V Vector Co-processors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vector co-processors represent a sort of natural architectural target for Deep
    Learning-oriented RISC-V acceleration. Commercial RISC-V vector processors mainly
    targeted at HPC markets, such as Xuantie ([chenXuantie910CommercialMultiCore2020,](#bib.bib37)
    ), and Ventana ([ventanaProduct,](#bib.bib280) ), have recently started appearing.
    In addition, vector co-processors explicitly tailored for DL, like Spatz ([cavalcanteSpatzCompactVector2022,](#bib.bib33)
    ) and Arrow ([assirArrowRISCVVector2021,](#bib.bib15) ), have been developed.
    The former in particular focuses only on a subset of the V extension and on 32-bit
    data, capturing better opportunities for energy efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3\. RISC-V Memory-coupled Neural Processing Units (NPUs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Concerning the tightest kind of memory coupling, at L1, most proposals in the
    state-of-the-art are based on the Parallel Ultra-Low Power (PULP) template, and
    devote significant effort to enabling fast communication between RISC-V cores
    and accelerators. Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235) ) is a prototype
    system coupling 9 RI5CY cores with a quantized DNN convolutional NPU sharing directly
    L1 scratchpad memory with the cores. GreenWaves Technologies GAP9 ([gap9Product,](#bib.bib94)
    ) is a commercial product, targeted at the wearables market, that follows the
    same line with many architectural improvements and a redesigned AI NPU for QNNs
    – leading to the product achieving the best performance and energy efficiency
    in the public TinyMLPerf Tiny 1.0 challenge as of this writing ([tinyMLPerf2023,](#bib.bib189)
    ). Archimedes ([prasadSpecializationMeetsFlexibility,](#bib.bib222) ) proposes
    a large AI hardware accelerator for performance-hungry Extended Reality applications.
    RedMulE ([tortorellaRedMulECompactFP162022a,](#bib.bib266) ; [tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265)
    ), integrated into the Darkside prototype ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86)
    ), follows the same principles but focusing on floating-point computation to support
    training as well as inference. Garofalo et al. ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85)
    ) and Bruschi et al. ([bruschiEndtoEndDNNInference2022,](#bib.bib26) ) integrate
    in-memory-computing PCM-based NPUs, the latter simulating a system scaled up to
    match the size of server-class hardware.
  prefs: []
  type: TYPE_NORMAL
- en: <svg  class="ltx_picture" height="604.86" overflow="visible"
    version="1.1" width="1746.04"><g transform="translate(0,604.86) matrix(1 0 0 -1
    0 0) translate(236.2,0) translate(0,4.86)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    clip-path="url(#pgfcp1)"><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 99.52 40.47)"><foreignobject width="17.71" height="11.41" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{1}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 369.08 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{2}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 638.64 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{3}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 908.2 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{4}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1177.76 40.47)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{5}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 610.1 22.44)"><foreignobject
    width="75.18" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Power
    [mW]</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 73.37 71.19)"><foreignobject width="17.71" height="11.41" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{1}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 188.73)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{2}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 306.27)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{3}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 423.8)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{4}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 73.37 541.34)"><foreignobject
    width="17.71" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\displaystyle{10^{5}}$</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(0.0 1.0 -1.0 0.0 64.36 285.61)"><foreignobject
    width="75.85" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Perf
    [GOPS]</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 24.6 188.26)"><foreignobject width="381.86" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 441.15 146.66)"><foreignobject width="415.69" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Dustin ([ottaviDustin16CoresParallel2023,](#bib.bib205)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 375.58 96.6)"><foreignobject width="511.55" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Darkside ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 -86.21 150.18)"><foreignobject width="469.31" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">SamurAI ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 413.33 283.01)"><foreignobject width="364.84" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Marsellus ([conti22124TOPS2023,](#bib.bib53)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 436.56 317.68)"><foreignobject width="558.5" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Garofalo et al. ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 421.59 244.85)"><foreignobject width="472.19" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">DIANA ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 200.68 113.65)"><foreignobject width="418.22" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">TinyVers ([jainTinyVersTinyVersatile2023,](#bib.bib121)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 309.84 187.65)"><foreignobject width="416.11" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 306.53 130.65)"><foreignobject width="366.6" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235)
    )</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 -236.2 241.25)"><foreignobject width="702.27" height="44.74" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">RedMulE ([tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265)
    ; [tortorellaRedMulECompactFP162022a,](#bib.bib266) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 666.57 255.79)"><foreignobject
    width="428.68" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Spatz
    ([cavalcanteSpatzCompactVector2022,](#bib.bib33) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 470.24 124.65)"><foreignobject
    width="467.42" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Manticore
    ([zarubaManticore4096CoreRISCV2021,](#bib.bib301) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 567.49 110.51)"><foreignobject
    width="510.05" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Vitruvius+
    ([minerviniVitruviusAreaEfficientRISCV2023,](#bib.bib186) )</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 631.65 144.81)"><foreignobject
    width="396.14" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Ara
    ([cavalcanteAra1GHzScalable2020,](#bib.bib32) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 509.63 86.8)"><foreignobject
    width="412.46" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Perotti
    et al. ([perottiNewAraVector2022,](#bib.bib218) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 -66.37 329.09)"><foreignobject
    width="468.73" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Archimedes
    ([prasadSpecializationMeetsFlexibility,](#bib.bib222) )</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 674.59 532.16)"><foreignobject
    width="463.39" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Lee
    et al. ([lee64TOPSEnergyEfficientTensor2022,](#bib.bib161) )</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 778.65 507.27)"><foreignobject
    width="418.26" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Axelera
    AI ([ward-foxtonAxeleraDemosAI,](#bib.bib290) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 19.79 303.78)"><foreignobject
    width="379.83" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Tambe
    et al. ([tambe2212nm182023,](#bib.bib263) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 508.79 226.33)"><foreignobject
    width="482.87" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">exSDOTP
    ([bertacciniMiniFloatNNExSdotpISA2022,](#bib.bib17) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1009.36 557.91)"><foreignobject
    width="500.48" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Esperanto
    ([ditzelAcceleratingMLRecommendation2022,](#bib.bib65) )</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 789.97 472.79)"><foreignobject
    width="477.72" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Bruschi
    et al. ([bruschiEndtoEndDNNInference2022,](#bib.bib26) )</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 579.83 188.19)"><foreignobject
    width="425.26" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CNC
    ([chenEightCoreRISCVProcessor2022,](#bib.bib38) )</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1210.69 445.45)"><foreignobject
    width="54.23" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Maturity</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 423.87)"><foreignobject
    width="39.97" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Silicon</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 402.58)"><foreignobject
    width="63.34" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Pre-silicon</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 381.29)"><foreignobject
    width="64.96" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Simulation</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1210.69 359.12)"><foreignobject
    width="65.53" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Data
    Type</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 1241.25 337.55)"><foreignobject width="32.29" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">FP64</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 316.26)"><foreignobject
    width="32.29" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FP16</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 294.97)"><foreignobject
    width="25.37" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FP8</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 273.68)"><foreignobject
    width="25.37" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">FP4</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 252.39)"><foreignobject
    width="39.2" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">INT32</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 231.1)"><foreignobject
    width="32.29" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">INT8</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 209.81)"><foreignobject
    width="81.1" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">INT2
    x INT8</foreignobject></g><g stroke="#262626" fill="#262626" transform="matrix(1.0
    0.0 0.0 1.0 1241.25 188.52)"><foreignobject width="81.1" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">INT2 x INT4</foreignobject></g><g stroke="#262626"
    fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 167.23)"><foreignobject
    width="32.29" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">INT2</foreignobject></g><g
    stroke="#262626" fill="#262626" transform="matrix(1.0 0.0 0.0 1.0 1241.25 145.93)"><foreignobject
    width="42.66" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Analog</foreignobject></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6. Performance and power consumption of prototypes of several State-of-the-Art
    Deep Learning acceleration architectures discussed in Section [4.3](#S4.SS3 "4.3\.
    Accelerators based on open-hardware RISC-V ‣ 4\. Hardware Accelerators ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms").
  prefs: []
  type: TYPE_NORMAL
- en: Table 6. Summary of RISC-V Deep Learning acceleration architectures
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Accelerator | Tech [nm] | Area [mm2] | Freq [MHz] | Voltage [V]
    | Power [mW] | Perf [GOPS] | Eff [GOPS/W] | # MAC units | Data Type | Maturity
    |'
  prefs: []
  type: TYPE_TB
- en: '| ISA | Dustin ([ottaviDustin16CoresParallel2023,](#bib.bib205) ) | 65nm |
    10 | 205 | 1.2 | 156 | 33.6 | 215 | 128 | INT2 x INT4 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Kraken (RISC-V cores) ([dimauroKrakenDirectEvent2022,](#bib.bib64) ) | 22nm
    | 9 | 330 | 0.8 | 300 | 75 | 750 | 128 | INT2 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Manticore ([zarubaManticore4096CoreRISCV2021,](#bib.bib301) ) | 22nm | 888
    | 500 | 0.6 | 200 | 25 | 188 | 24 | FP64 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Celerity ([davidsonCelerityOpenSource511Core2018,](#bib.bib56) ) | 16nm |
    25 | 1050 | - | 1900 | - | - | 496 | INT32 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Tenstorrent ([vasiljevicComputeSubstrateSoftware2021,](#bib.bib275) ) | 12nm
    | 477 | - | - | - | 92000 | - | - | FP16 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| exSDOTP ([bertacciniMiniFloatNNExSdotpISA2022,](#bib.bib17) ) | 12nm | 0.52
    | 1260 | 0.8 | 278 | 160 | 575 | 16 | FP8 | Pre-silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Esperanto ([ditzelAcceleratingMLRecommendation2022,](#bib.bib65) ) | 7nm
    | 570 | 1000 | - | 20000 | 139000 | 6.95 | 69632 | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| CNC ([chenEightCoreRISCVProcessor2022,](#bib.bib38) ) | 4nm | 1.92 | 1150
    | 0.85 | 510 | 75.8 | 149 | 512 | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Vector | Lee et al. ([lee64TOPSEnergyEfficientTensor2022,](#bib.bib161) )
    | 14nm | 181 | 2000 | 0.8 | 60000 | 64000 | 1450 | 16384 | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| AVA ([lazoAdaptableRegisterFile2022,](#bib.bib157) ) | 22nm | 3.9 | - | -
    | - | - | - | - | FP64 | Pre-silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Spatz ([cavalcanteSpatzCompactVector2022,](#bib.bib33) ) | 22nm | 20 | 594
    | 0.8 | 1070 | 285 | 266 | 256 | INT32 | Pre-silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Vitruvius+ ([minerviniVitruviusAreaEfficientRISCV2023,](#bib.bib186) ) |
    22nm | 1.3 | 1400 | 0.8 | 459 | 21.7 | 47.3 | 8 | FP64 | Pre-silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Ara ([cavalcanteAra1GHzScalable2020,](#bib.bib32) ) | 22nm | 10735 kGE |
    1040 | 0.8 | 794 | 32.4 | 40.8 | 16 | FP64 | Pre-silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Perotti et al. ([perottiNewAraVector2022,](#bib.bib218) ) | 22nm | 0.81 |
    1340 | 0.8 | 280 | 10.4 | 37.1 | 4 | FP64 | Pre-silicon |'
  prefs: []
  type: TYPE_TB
- en: '| L1 NPU | Darkside ([garofaloDARKSIDEHeterogeneousRISCV2022,](#bib.bib86)
    ) | 65nm | 3.85 | 200 | 1.2 | 89.1 | 12.6 | 152 | 32 | FP16 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Marsellus (NPU) ([conti22124TOPS2023,](#bib.bib53) ) | 22nm | 18.7 | 420
    | 0.8 | 123 | 637 | 7600 | 10368 1–bit | INT2 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Garofalo et al. ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85)
    ) | 22nm | 30 | 500 | 0.8 | 150 | 958 | 6390 | 36 (DW) | INT8 | Pre-silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Vega ([rossiVegaTenCoreSoC2022,](#bib.bib235) ) | 22nm | 12 | 450 | 0.8 |
    49.4 | 32.2 | 651 | 27 | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| RedMulE ([tortorellaRedMuleMixedPrecisionMatrixMatrix2023,](#bib.bib265)
    ; [tortorellaRedMulECompactFP162022a,](#bib.bib266) ) | 22nm | 0.73 | 613 | 0.8
    | 193 | 117 | 608 | 96 | FP16 | Pre-silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Archimedes ([prasadSpecializationMeetsFlexibility,](#bib.bib222) ) | 22nm
    | 3.38 | 270 | 0.65 | 112 | 1198 | 10.6 | 5184 | INT2 x INT8 | Pre-silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Bruschi et al. ([bruschiEndtoEndDNNInference2022,](#bib.bib26) ) | 5nm |
    480 | - | - | 3070 | 20000 | 6500 | 3.35$\times$10⁷ | Analog | Simulation |'
  prefs: []
  type: TYPE_TB
- en: '| L2 NPU | SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136) ) | 65nm | 4.47 |
    400 | 1 | 116 | 75.9 | 655 | 100 | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| SamurAI ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187) ) | 28nm | 4.52
    | 350 | 0.9 | 94.7 | 36 | 380 | 64 | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Gemmini ([gencGemminiEnablingSystematic2021,](#bib.bib87) ) | 22nm | 1.03
    | 1000 | - | - | - | - | 256 | INT8 | Pre-silicon |'
  prefs: []
  type: TYPE_TB
- en: '| DIANA (digital) ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112) ) | 22nm
    | 10.24 | 280 | 0.8 | 132 | 230 | 1740 | 256 | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| DIANA (analog) ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112) ) | 22nm
    | 10.24 | 350 | 0.8 | 132 | 18100 | 176000 | 256 | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| TinyVers ([jainTinyVersTinyVersatile2023,](#bib.bib121) ) | 22nm | 6.25 |
    150 | 0.8 | 20 | 17.6 | 863 | 64 | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Simba ([shao_micro19,](#bib.bib243) ) | 16nm | 6 | 161 | 0.42 | - | - | 9100
    | 1024 | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Axelera AI ([ward-foxtonAxeleraDemosAI,](#bib.bib290) ) | 12nm | 9 | 800
    | - | 2787 | 39300 | 14100 | - | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| Tambe et al. ([tambe2212nm182023,](#bib.bib263) ) | 12nm | 4.59 | 717 | 1
    | 111 | 734 | 6612 | - | FP4 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| L3 NPU | Gonzalez et al. ([gonzalez16mm106GOPS2021,](#bib.bib92) ) | 22nm
    | 16 | 961 | - | - | - | 106.1 | 256 | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: '| ESP ([jia12nmAgileDesignedSoC2022,](#bib.bib124) ) | 12nm | 21.6 | 1520 |
    1 | 1830 | - | - | 3x NVDLA | INT8 | Silicon |'
  prefs: []
  type: TYPE_TB
- en: Moving the shared memory from L1 to L2/L3, there are other NPU solutions in
    the state-of-the-art. In these architectures, RISC-V open-source cores are flexible
    and easy to be integrated. Systems exploiting this template include, for example,
    SNCPU ([juSystolicNeuralCPU2023,](#bib.bib136) ), which builds a hybrid system
    that can act as either a set of 10 RISC-V cores or be reconfigured in a systolic
    NPU. Gonzalez et al. ([gonzalez16mm106GOPS2021,](#bib.bib92) ) and Genc et al. ([gencGemminiEnablingSystematic2021,](#bib.bib87)
    ) exploit a systolic array generation tool, Gemmini, to generate systolic arrays
    to accelerate DNNs and coupled with a RISC-V core by exploiting a shared L3 or
    L2 memory, respectively. Simba ([shao_micro19,](#bib.bib243) ) follows a similar
    template and is also meant to be scaled towards server-grade performance using
    the integration of chiplets on multi-chip modules. ESP ([giriESP4MLPlatformBasedDesign2020,](#bib.bib89)
    ; [giriAcceleratorIntegrationOpenSource2021,](#bib.bib90) ) and Tambe et al. ([tambe2212nm182023,](#bib.bib263)
    ) also focus on integrating hardware accelerators and NPUs in large-scale Network-on-Chips
    using RISC-V cores as computing engines. On the other end of the spectrum, SamurAI ([miro-panadesSamurAIVersatileIoT2022,](#bib.bib187)
    ), TinyVers ([jainTinyVersTinyVersatile2023,](#bib.bib121) ), and DIANA ([houshmandDIANAEndtoEndHybrid2023,](#bib.bib112)
    ) build up AI-IoT systems composed of a microcontroller and L2-coupled NPUs (in
    the case of DIANA both an analog SRAM-IMC-based accelerator and a conventional
    digital one). Kraken ([dimauroKrakenDirectEvent2022,](#bib.bib64) ) couples the
    aforementioned RISC-V ISA-extended cluster with specialized L2-coupled Spiking
    Neural Network (SNN) and Ternary Neural Network (TNN) accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [6](#S4.F6 "Figure 6 ‣ 4.3.3\. RISC-V Memory-coupled Neural Processing
    Units (NPUs) ‣ 4.3\. Accelerators based on open-hardware RISC-V ‣ 4\. Hardware
    Accelerators ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms") summarizes performance, power, technological maturity, and data
    types used in the architectures discussed in this section. RISC-V-based solutions
    occupy essentially the full spectrum of DL architectures ranging from 10 mW microcontrollers
    up to 100 W SoCs meant to be integrated as part of HPC systems. So far, most of
    the research has focused on the lower end of this spectrum, striving for the best
    energy efficiency. We can observe how efficiency is strongly correlated with architectural
    techniques yielding accuracy (e.g., data bit-width reduction & quantization).
    Table [6](#S4.T6 "Table 6 ‣ 4.3.3\. RISC-V Memory-coupled Neural Processing Units
    (NPUs) ‣ 4.3\. Accelerators based on open-hardware RISC-V ‣ 4\. Hardware Accelerators
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
    summarizes all quantitative information available on the discussed architectures;
    whenever multiple operating points have been reported for a single architecture,
    the table reports always the highest performance (for consistency, this applies
    to both performance and energy efficiency values).
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Accelerators based on Emerging Paradigms and Technologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1\. Accelerators for Sparse Matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section overviews accelerator architectures purposely designed to take
    advantage of computations suitable to sparse matrices. Definitions, storage formats
    appropriate for sparse matrices, and their impacts on the computational complexity
    of DL models are discussed in Appendix A.5.
  prefs: []
  type: TYPE_NORMAL
- en: Some accelerators (e.g., Cnvlutin, Cambricon-X, Eyeriss) handle only one-sided
    sparsity, which stems either from zero-valued activations or network pruning,
    thus achieving only a partial reduction in compute and data reduction. On the
    other hand, other accelerators (e.g., SCNN, SparTen, Eyeriss v2) target two-sided
    sparsity, which originates from network pruning and zero-valued activations. In
    addition to the different approaches to exploiting sparsity, these architectures
    also employ distinct dataflows to execute the DNN layers. Due to the complexity
    of the logic, existing hardware accelerators for sparse processing are typically
    limited to a specific layer type (e.g., fully-connected layers, convolutional
    layers).
  prefs: []
  type: TYPE_NORMAL
- en: Eyeriss ([Chen_2017,](#bib.bib41) ) targets CNN acceleration by storing in DRAM
    only nonzero-valued activations in Compressed Sparse Columns (CSC) format and
    by skipping zero-valued activations (by means of gating the datapath switching
    and memory accesses) to save energy. Eyeriss v2 ([eyerissv2:2019,](#bib.bib42)
    ), which targets DNNs on mobile devices, also supports sparse network models.
    It utilizes the CSC format to store weights and activations, which are kept compressed
    not only in memory but also during processing. To improve flexibility, it uses
    a hierarchical mesh for the PEs interconnections. By means of these optimizations,
    Eyeriss v2 is significantly faster and more energy-efficient than the original
    Eyeriss.
  prefs: []
  type: TYPE_NORMAL
- en: Cnvlutin ([cnvlutin:isca2016,](#bib.bib6) ), which also targets CNN acceleration,
    uses hierarchical data-parallel units, skips computation cycles for zero-valued
    activations and employs a co-designed data storage format based on Compressed
    Sparse Rows (CSR) to compress the activations in DRAM. However, it does not consider
    the sparsity of the weights. On the contrary, Cambricon-X architecture ([zhang2016micro,](#bib.bib305)
    ) exploits the sparsity of CNNs by enabling the PEs to store the compressed weights
    in CSR format for asynchronous computation. However, it does not exploit activation
    sparsity. EIE ([han2016isca,](#bib.bib103) ), besides compressing the weights
    through a variant of CSC sparse matrix representation and skipping zero-valued
    activations, employs a scalable array of PEs, each storing a partition of the
    DNN in SRAM that allows obtaining significant energy savings with respect to DRAM.
    NullHop ([Aimar2019,](#bib.bib4) ) is a CNN accelerator architecture that applies
    the Compressed Image Size (CIS) format to the weights and skips the null activations,
    similarly to EIE. Sparse CNN (SCNN) ([SCNN:isca2017,](#bib.bib207) ) is an accelerator
    architecture for inference in CNNs. It employs a cluster of asynchronous PEs comprising
    several multipliers and accumulators. SCNN exploits sparsity in both weights and
    activations, which are stored in the classic CSR representation. It employs a
    Cartesian product-based computation architecture that maximizes the reuse of weights
    and activations within the cluster of PEs; the values are delivered to an array
    of multipliers, and the resulting scattered products are summed using a dedicated
    interconnection mesh. By exploiting two-sided sparsity, SCNN improves performance
    and energy over dense architectures. SparTen ([sparten:micro2019,](#bib.bib91)
    ) is based on SCNN ([SCNN:isca2017,](#bib.bib207) ). It addresses some considerable
    overheads of SCNN in performing the sparse vector-vector dot product by improving
    the distribution of the operations to the multipliers and allows using any convolutional
    stride (not being limited to unit-stride convolutions as SCNN). It also addresses
    unbalanced sparsity distribution across the PEs employing an offline software
    scheme. The PermDNN architecture ([perm:micro2018,](#bib.bib60) ) addresses the
    generation and execution of hardware-friendly structured sparse DNN models using
    permuted diagonal matrices. In this way, it does not incur load imbalance which
    is caused by the irregularity of unstructured sparse DNN models.
  prefs: []
  type: TYPE_NORMAL
- en: SqueezeFlow ([squeezeflow:2019,](#bib.bib165) ) is an accelerator architecture
    that exploits the sparsity of CNN models. To reduce hardware complexity, it exploits
    concise convolution rules to benefit from the reduction of computation and memory
    accesses as well as the acceleration of existing dense CNN architectures without
    intrusive PE modifications. The Run Length Compression (RLC) format is used to
    compress activations and weights. A different strategy is pursued by the Unique
    Weight CNN (UCNN) accelerator ([ucnn:isca2018,](#bib.bib110) ), which proposes
    a generalization of the sparsity problem. Rather than considering only the repetition
    of zero-valued weights, UCNN exploits repeated weights with any value by reusing
    CNN sub-computations and reducing the model size in memory. SIGMA ([sigma:2020,](#bib.bib226)
    ) is an accelerator for DNN training, which is characterized by a flexible and
    scalable architecture that offers high utilization of its PEs regardless of kernel
    shape (i.e., matrices of arbitrary dimensions) and sparsity pattern. It targets
    the acceleration of GEMMs with unstructured sparsity. Bit-Tactical ([bittactical:2019,](#bib.bib59)
    ) is a DNN accelerator where the responsibility for exploiting weight sparsity
    is shared between a static scheduling middleware and a co-designed hardware front-end,
    with a lightweight sparse shuffling network which comprises two multiplexers per
    activation input. Unlike SIGMA and other accelerators, Bit-tactical leverages
    scheduling in software to align inputs and weights.
  prefs: []
  type: TYPE_NORMAL
- en: Flexagon ([flexagon:aplos2023,](#bib.bib191) ) is a reconfigurable accelerator
    capable of performing sparse-sparse matrix multiplication computation by using
    the particular data flow that best matches each case.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the design of specialized hardware accelerators to exploit model sparsity,
    a parallel trend is to use GPU architectures. Pruned sparse models with unstructured
    sparse patterns introduce irregular memory accesses that are unfriendly on commodity
    GPU architectures. The first direction to tackle this issue on commodity DNN accelerators
    is at the software layer, using pruning algorithms that enforce a particular sparsity
    pattern, such as tile sparsity ([guo:sc2020,](#bib.bib97) ), on the model that
    allows leveraging existing GEMM accelerators. A second direction to leverage the
    sparsity in DNN models on GPUs is to introduce new architectural support, such
    as Sparse Tensor Cores ([sparsetensorcore:2021,](#bib.bib188) ). The NVIDIA Ampere
    architecture introduces this Sparse Tensor Core design with a fixed 50% weight
    pruning target and achieves a better accuracy and performance trade-off. However,
    sparsity from activations, which are dynamic and unpredictable, is challenging
    to leverage on GPUs. Indeed, the current Sparse Tensor Core is only able to take
    advantage of weight sparsity but not activation sparsity. Reconfigurability appears
    to be a keyword for the design of new sparse accelerators because some network
    models exhibit dynamic sparsity ([fedus:jmlr2022,](#bib.bib73) ), where the position
    of non-zero elements changes over time.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Emerging 3D-stacked Processing-in-memory Technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Two main 3D stacked memory standards have been recently proposed: the Hybrid
    Memory Cube (HMC) and the High Bandwidth Memory (HBM) that provide highly parallel
    access to the memory which is well suited to the highly parallel architecture
    of the DNN accelerators. The processing elements of 3D stacked DNN accelerators
    can be embedded in the logic die or in the memory dies, reducing significantly
    the latency of accessing data in main memory, and improving the energy efficiency
    of the system. However, as detailed in Sec. A.6 of Appendix A, there are some
    challenges and limitations to be taken into account when using this technology
    ([Kim2022,](#bib.bib143) ).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d069eb841a255166d30cd9e7fba1914e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b5ba532313e955c6955f772ba8be642.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7. (a) Neurocube architecture. (b) HBM-PIM architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. State-of-the-art on 3D-stacked Processor-in-memory solutions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table 7. Summary of 3D-staked Processing-in-memory DNN accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: '| PIM | Year | Integration Level | 3D Mem. Tech. | Functions | Data Type |
    Tech. Node | Performance [GOPs/s] | Power [W] | Maturity |'
  prefs: []
  type: TYPE_TB
- en: '| Neurocube([Kim2016,](#bib.bib142) ) | 2016 | Logic die | HMC | MAC | 16-bit
    fixed point | 15nm | 132 | 3.4 + HMC | Layout |'
  prefs: []
  type: TYPE_TB
- en: '| Tetris([Gao2017,](#bib.bib83) ) | 2017 | Logic die | HMC | ALU/MAC | 16-bit
    fixed point | 45nm | - | 8.42 | Simulation |'
  prefs: []
  type: TYPE_TB
- en: '| NeuralHMC([Min2019,](#bib.bib185) ) | 2019 | Logic die | HMC | MAC | 32-bit
    floating point | - | - | - | Simulation |'
  prefs: []
  type: TYPE_TB
- en: '| VIMA([Cordeiro2021,](#bib.bib54) ) | 2021 | Logic die | HMC | ALU/MULT/DIV
    | 32-bit integer/floating point | - | - | 3.2 + HMC | Simulation |'
  prefs: []
  type: TYPE_TB
- en: '| Newton([He2020,](#bib.bib109) ) | 2020 | Bank | HBM | MAC | bfloat16 | -
    | - | - | Simulation |'
  prefs: []
  type: TYPE_TB
- en: '| HBM-PIM([Kwon2021,](#bib.bib154) ) | 2020 | Bank | HBM | ALU/MAC | 16-bit
    floating point | 20nm | 1200 | - | Silicon |'
  prefs: []
  type: TYPE_TB
- en: From Table [7](#S5.T7 "Table 7 ‣ 5.2.1\. State-of-the-art on 3D-stacked Processor-in-memory
    solutions ‣ 5.2\. Emerging 3D-stacked Processing-in-memory Technologies ‣ 5\.
    Accelerators based on Emerging Paradigms and Technologies ‣ A Survey on Deep Learning
    Hardware Accelerators for Heterogeneous HPC Platforms"), we can distinguish two
    different approaches when integrating digital PEs in a 3D stacked memory architecture
    ([Kim2022,](#bib.bib143) ). The first approach, most commonly found in the literature,
    embeds the computing into the logic die of the memory block (logic die-level PIM).
    In the second approach (bank-level PIM), the processing logic is integrated into
    each DRAM die at the level of the memory banks, after the column decoder and selector
    blocks.
  prefs: []
  type: TYPE_NORMAL
- en: A first example of a 3D PIM implementation is Neurocube ([Kim2016,](#bib.bib142)
    ) that, as shown in Figure [7(a)](#S5.F7.sf1 "In Figure 7 ‣ 5.2\. Emerging 3D-stacked
    Processing-in-memory Technologies ‣ 5\. Accelerators based on Emerging Paradigms
    and Technologies ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms"), is embedded into the logic die of an HMC, and consists of a cluster
    of PEs connected by a 2D mesh Network-on-Chip (NoC). The PE is composed of a row
    of multiply accumulator (MAC) units, a cache memory, a temporal buffer, and a
    memory module for storing shared synaptic weights. Each PE is associated with
    a single memory vault and can operate independently and communicate through the
    TSVs and the vault controller. A host communicates with the Neurocube through
    the external links of the HMC to configure the Neurocube for different neural
    network architectures. Each vault controller in the HMC has an associated programmable
    neuro sequence generator (PNG), i.e., a programmable state machine that controls
    the data movements required for neural computation. Neurocube implements an output
    stationary dataflow, meaning that each MAC from a PE is responsible for the computations
    of a different output neuron at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to Neurocube, Tetris ([Gao2017,](#bib.bib83) ) uses an HMC memory
    stack organized into 16 vaults. Each vault is associated with a PE, connected
    to the vault controller, and composed of a systolic array of 14 $\times$14 PEs
    and a small SRAM buffer, shared among the PEs. A 2D mesh NoC connects all the
    PEs. Differently from previous accelerator approaches, the dimension of the buffers
    in the logic layer is reduced and optimized to take into account the lower cost
    of accessing the DRAM layers, as well as the area constraints of the 3D package.
    Each PE has a register file and a MAC to locally store the inputs/weights and
    perform computations. Tetris implements a row stationary dataflow that maps 1D
    convolutions onto a single PE and utilizes the PE register file for local data
    reuse. A 2D convolution is orchestrated on the 2D array interconnect so that the
    data propagation among PEs remains local. In ([Gao2017,](#bib.bib83) ), an optimal
    scheduling is discussed to maximize on-chip reuse of weights and/or activations,
    and resource utilization. However, a programming model is not presented.
  prefs: []
  type: TYPE_NORMAL
- en: NeuralHMC ([Min2019,](#bib.bib185) ) adopts the same systolic architecture and
    row-stationary dataflow discussed in Tetris. However, the authors introduce the
    use of a weight-sharing pipelined MAC design to lower the cost of accessing weight
    data, by reducing the original 32 bits floating points weights to a 5 or 8 bits
    cluster index, saving memory consumption. Moreover, they discuss a series of mechanisms
    to reduce and optimize packet scheduling and on-chip communication in multi-HMC
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in ([Cordeiro2021,](#bib.bib54) ) study the benefits of migrating
    ML kernels on near-data processing (NDP) architecture capable of large-vector
    operations. The work derives from previous work of the same authors ([Alves2016,](#bib.bib7)
    ), where they introduced the HIVE architecture, which extends the HMC ISA for
    performing common vector operations directly inside the HMC, avoiding contention
    on the interconnections as well as cache pollution. The newly introduced Vector-In-Memory
    Architecture (VIMA) supports all ARM NEON Integer and Floating-point instructions
    and operates over vectors of 8 KB of data by fetching data over the 32 channels
    (vaults) of the HMC in parallel. The authors extend and use an NDP intrinsics
    library that supports the validation of NDP architectures based on large vectors,
    provides insights, and show the benefits of migrating ML algorithms to vector-based
    NDP architectures. Their simulated results show a significant speed-up and energy
    reduction with respect to an x86 baseline. Several accelerators employing the
    second PIM approach can be found in the literature. Newton ([He2020,](#bib.bib109)
    ) proposes a fixed data flow accelerator that computes matrix-vector multiplication
    effectively. It employs only MAC units, buffers, and a DRAM-like command interface
    for the host CPU to issue commands to the PIM compute, avoiding the overhead and
    granularity issues of offloading-based accelerators, e.g., the delay in the launch
    of the kernel and the switching between the PIM/non-PIM operational modes. To
    reduce the output vector write traffic with minimal output buffering, Newton employs
    an unusually-wide interleaved layout (DRAM row-wide). In Newton, input/output
    vectors have high reuse while the matrix has no reuse.
  prefs: []
  type: TYPE_NORMAL
- en: HBM-PIM ([Kwon2021,](#bib.bib154) ) implements a function-in-memory DRAM (FIMDRAM)
    that integrates a 16-wide SIMD engine within the memory banks exploits bank-level
    parallelism to provide 4 $\times$ higher processing bandwidth than an off-chip
    memory solution (Figure [7(b)](#S5.F7.sf2 "In Figure 7 ‣ 5.2\. Emerging 3D-stacked
    Processing-in-memory Technologies ‣ 5\. Accelerators based on Emerging Paradigms
    and Technologies ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms")). In their design, half of the cell array in each bank of the
    HBM was removed and replaced by a programmable computing unit (PCU), placed adjacent
    to the cell array to utilize bank-level parallelism. Each PCU is shared among
    two banks, and there are 8 PCUs per pseudo-channel. The PCU is divided into a
    register group, an execution unit, a decoding unit for parsing instructions needed
    to perform operations, and interface units to control data flow. The register
    group consists of a command-register file for instruction memory (CRF), a general-purpose
    register file for weight and accumulation (GRF), and a scalar register file to
    store constants for MAC operations (SRF). The PIM controller is integrated to
    support the programmability of the PCU and, similarly to Newton, the seamless
    integration with the host by determining the switching between the PIM/non-PIM
    operational modes. If the PIM mode is asserted, the PCUs execute the instructions
    pre-stored in the CRF, incrementing the program counter every time a DRAM’s read
    command is issued. 3D-stacked PIM has been also proposed for accelerating applications
    loosely related to DNNs. We present a brief overview of these accelerators in
    Sec. A.6 of the Appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. In-memory computing accelerators based on emerging memories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In-memory computing (IMC) has been proposed to break both the memory and the
    compute wall in data-driven AI workloads, using either SRAM or emerging memory
    technologies (such as PCM and RRAM described in Sec. A.7 of Appendix A) offering
    different trade-offs when used as an integrated computing device at the system
    level. Full-digital IMC designs offer a fast path for the integration of the next
    generation of neural processing systems like in NPUs. An example of IMC architecture
    has recently been proposed by STMicroelectronics in ([desoli2023isscc,](#bib.bib61)
    ), where a scalable and design time parametric NPU for edge AI relying on digital
    SRAM IMC has been manufactured in 18 nm FDSOI technology achieving an end-to-end
    system-level energy efficiency of 77 TOPS/W and an area efficiency of 13.6 TOPS/mm².
    This IMC-NPU is the evolution of the Orlando system-on-chip proposed in ([desoli17isscc,](#bib.bib62)
    ). Another digital IMC design is NeuroCIM ([kim2022vlsi,](#bib.bib145) ), an energy-efficient
    processor with four key features achieving 310.4 TOPS/W: Most significant bit
    (MSB) Word Skipping to reduce the BL activity; early stopping to enable lower
    bitline activity; mixed-mode firing for multi-macro aggregation; voltage folding
    to extend the dynamic range. A first example of an RRAM-based accelerator is the
    ISAAC ([shafiee2016isca,](#bib.bib241) ) tile-based architecture that proposes
    a pipeline design for CNN processing, which combines the data encoding and the
    processing steps within in situ multiply and accumulate units (IMA). In the first
    pipeline step, data are fetched from a chip eDRAM to the computation tile. The
    data format in ISAAC is fixed 16-bit. In computation, in each cycle, 1 bit is
    input to the IMA, and the computation result from the IMA is converted to digital
    format, thus requiring 16 clock cycles to process the input. The nonlinear activation
    is then applied, and the results are written back to eDRAM. Tiled computation
    is widely used from RRAM to improve the throughput. The PipeLayer ([song2017hpca,](#bib.bib249)
    ) architecture introduces intra-layer parallelism and an inter-layer pipeline
    for tiled architecture, using duplicates of processing units featuring the same
    weights to process multiple data in parallel. RRAM-based accelerators have been
    also designed for RNNs. In ([wan2022nature,](#bib.bib284) ), all the decomposed
    operations were formulated into in-situ MACs to provide high throughput. Further,
    designs like PRIME ([chi2016isca,](#bib.bib45) ) takes part of the RRAM memory
    arrays to serve as the accelerator instead of adding an extra processing unit
    for computation. This can be considered as an architecture that is borderline
    between NPUs and stand-alone. However, ([cao2022tcomp,](#bib.bib29) ) noted that
    existing PIM RRAM accelerators suffer from frequent and energy-intensive analog-to-digital
    (A/D) conversions, severely limiting their performance. To this extent, they presented
    a new architecture to efficiently accelerate DL tasks by minimizing the required
    A/D conversions with analog accumulation and neural-approximated peripheral circuits.
    Using a new dataflow they remarkably reduced the required A/D conversions for
    matrix-vector multiplications by extending shift-and-add (S+A) operations into
    the analog domain before the final quantizations. A summary of the technological
    features in major RRAM accelerators can be found in ([smagulova2023pieee,](#bib.bib246)
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: The first PCM-based silicon demonstrator for DNN inference is Hermes, which
    appeared in 2021 ([khaddam2022hermes,](#bib.bib140) ). The IMC accelerator consists
    of a 256x256 PCM cross-bar and optimized ADC circuitry to reduce the read-out
    latency and energy penalty. The SoC is implemented in 14nm technology, showing
    10.5 TOPS/W energy efficiency and performance density of 1.59 TOPS/mm2 on inference
    tasks of multi-layer perceptrons and ResNet-9 models trained on MNIST and CIFAR-10
    datasets, with comparable accuracy as the software baseline. The same 256x256
    PCM cross-bar has been integrated into a scaled-up mixed-signal architecture that
    targets the inference of long short-term memory (LSTM) and ResNet-based neural
    networks ([gallo202264,](#bib.bib81) ). The chip, implemented in the same 14nm
    technology, consists of 64 analog cores interconnected via an on-chip communication
    network and complemented with digital logic to execute activation functions, normalization,
    and other kernels than Matrix-Vector Multiplications (MVMs). The accelerator achieves
    a peak throughput of 63.1 TOPS with an energy efficiency of 9.76 TOPS for 8-bit
    input/8-bit output MVM operations.
  prefs: []
  type: TYPE_NORMAL
- en: Besides silicon stand-alone demonstrators, the PCM technology is evaluated from
    a broader perspective in heterogeneous architectures that target different classes
    of devices, from IoT end nodes to many-core HPC systems. Such studies aim to highlight
    and overcome the system-level challenges that arise when PCM technology is integrated
    into more complex mixed-signal systems. For example, Garofalo et al. ([garofaloHeterogeneousInMemoryComputing2022,](#bib.bib85)
    ) analyze the limited flexibility of AIMC cores that can only sustain MVM-oriented
    workloads, but they are inefficient to execute low-reuse kernels and other ancillary
    functions such as batch-normalization and activation functions. To better balance
    Amdahl’s effects that show up on the execution of end-to-end DNN inference workloads,
    they propose as a solution an analog-digital edge system that complements the
    computing capabilities of PCM-based accelerators with the flexibility of general-purpose
    cores. The architecture, benchmarked on a real-world MobileNetV2 model, demonstrates
    significant advantages over purely digital solutions. Bruschi et al. ([bruschiEndtoEndDNNInference2022,](#bib.bib26)
    ) leave the edge domain to study the potentiality of PCM-based AIMC in much more
    powerful HPC many-core systems. The work presents a general-purpose chiplet-oriented
    architecture of 512 processing clusters, each composed of RISC-V cores for digital
    computations and nvAIMC cores for analog-amenable operations, such as 2D convolutions.
    This system is benchmarked on a ResNet18 DNN model, achieving 20.2 TOPS and 6.5
    TOPS/W.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8. Summary of IMC accelerators based on RRAM and PCM memories
  prefs: []
  type: TYPE_NORMAL
- en: '| Accelerator | Technology | Process | Application | Area [mm²] | Power [mW]
    | Performance [GOPS] | EE [GOPS/W] | AE [GOPS/mm²] |'
  prefs: []
  type: TYPE_TB
- en: '| ISAAC ([shafiee2016isca,](#bib.bib241) ) | RRAM+CMOS | 32 nm | CNN | 85.4
    | 65800 | - | 380.7 | 466.8 |'
  prefs: []
  type: TYPE_TB
- en: '| PipeLayer ([song2017hpca,](#bib.bib249) ) | RRAM+CMOS | - | CNN | 82.63 |
    - | - | 140 | 1485 |'
  prefs: []
  type: TYPE_TB
- en: '| NeuralPIM ([cao2022tcomp,](#bib.bib29) ) | RRAM+CMOS | 32 nm | CNN+RNN |
    86.4 | 67700 | - | 2040.6 | 1904 |'
  prefs: []
  type: TYPE_TB
- en: '| PRIME ([chi2016isca,](#bib.bib45) ) | RRAM+CMOS | 65 nm | MLP+CNN | - | -
    | - | 2100 | 1230 |'
  prefs: []
  type: TYPE_TB
- en: '| NeuRRAM ([wan2022nature,](#bib.bib284) ) | RRAM+CMOS | 130 nm | CNN+RNN+RBN
    | 159 | 49.7 | 2135 | 43000 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Hermes ([khaddam2022hermes,](#bib.bib140) ) | PCM+CMOS | 14 nm | MLP+CNN+LSTM
    | - | - | - | 10500 | 1590 |'
  prefs: []
  type: TYPE_TB
- en: 5.4\. Full-digital Neuromorphic Accelerators
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neuromorphic computing aims at a paradigm shift from Von Neumann-based architectures
    to distributed and co-integrated memory and PEs, the granularity at which this
    paradigm shift is achieved in digital implementations strongly varies between
    a distributed Von Neumann or full-custom approach, from high to low processing
    and memory separation ([frenkel2021arxiv,](#bib.bib77) ). Neuromorphic chip architectures
    enable the hardware implementation of spiking neural networks (SNNs) ([rathi2023acm,](#bib.bib231)
    ) and advanced bio-inspired computing systems that have the potential to achieve
    even higher energy efficiency with respect to DNN stand-alone accelerators described
    so far ([akopyan2015tcad,](#bib.bib5) ).
  prefs: []
  type: TYPE_NORMAL
- en: A first example of a digital architecture for SNN and neuroscience simulation
    acceleration is the SpiNNaker chip ([painkras2013jsscc,](#bib.bib206) ). It follows
    a distributed von-Neumann approach using a globally asynchronous locally synchronous
    (GALS) design for efficient handling of asynchronous spike data and is based on
    a 130 nm technology. SpiNNaker has been optimized for large-scale SNN experiments
    while keeping a high degree of flexibility. The evolution of the architecture
    using 22 nm technology embedding 4 ARM Cortex M4F cores out of the 152 per chip
    is planned for the final SpiNNaker 2 system ([liu2018frontiers,](#bib.bib170)
    ). The objective is to simulate two orders of magnitude more neurons per chip
    compared to ([painkras2013jsscc,](#bib.bib206) ). However, it has been demonstrated
    that GPU-based accelerators compare favorably to a SpiNNaker system when it comes
    to large SNN and cortical-scale simulations ([knight2018frontiers,](#bib.bib147)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Full-custom digital hardware leads to higher-density and more energy-efficient
    neuron and synapse integration for spiking neural networks (SNN) compared to the
    two formerly described accelerators ([frenkel2021arxiv,](#bib.bib77) ). All the
    accelerators reported in this paper benefit from moving the memory (generally
    SRAM elements) closer to computation. The 45 nm design in ([seo2011cicc,](#bib.bib239)
    ) is a small-scale architecture for SNN acceleration embedding 256 Leaky-Integration-Fire
    (LIF) neurons and up to 64k synapses based on the Stochastic Synaptic Time Dependant
    Plasticity (S-STDP) concept. It achieves reasonably high neuron and synapse densities,
    despite the use of a custom SRAM and given its energy-efficiency figures is an
    ideal choice, especially for edge computing scenarios. At the same integration
    scale, the ODIN chip embeds 256 neurons and 64k Spike Driven Synaptic Plasticity
    (SDSP)-based 4-bit synapses in a 28 nm CMOS process ([frenkel2018tbiocas,](#bib.bib78)
    ). A first attempt to scale up the NPU for SNN applications is represented by
    the 65 nm MorphIC chip, which is based on the ODIN core integrated into a quadcore
    design ([frenkel2019tbiocas,](#bib.bib79) ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Concerning large-scale neuromorphic platforms required for cognitive computing
    applications, there are currently two designs offered: the 28 nm IBM TrueNorth
    ([akopyan2015tcad,](#bib.bib5) ) and the 14 nm Intel Loihi ([davies2018micro,](#bib.bib57)
    ) neuromorphic chips. TrueNorth is a GALS design embedding as high as 1M neurons
    and 256M binary non-plastic synapses per chip, where neurons rely on a custom
    model that allows modifying their behaviors by combining up to three neurons ([cassidy2013ijcnn,](#bib.bib31)
    ). Loihi is a fully asynchronous design embedding up to 180k neurons and 114k
    (9- bit) to 1M (binary) synapses per chip. Neurons rely on a LIF model with a
    configurable number of compartments to which several functionalities such as axonal
    and refractory delays, spike latency, and threshold adaptation have been added.
    The spike-based plasticity rule used for synapses is programmable.'
  prefs: []
  type: TYPE_NORMAL
- en: In digital designs for neuromorphic chips, versatility can be obtained with
    a joint optimization comprising power and area efficiencies. This flexibility
    in optimizing between versatility and efficiency in digital designs is highlighted
    with platforms going from versatility-driven (e.g., SpiNNaker) to efficiency-driven
    (e.g., ODIN and MorphIC), through platforms aiming at a well-balanced trade-off
    on both sides (e.g., Loihi). Table [9](#S5.T9 "Table 9 ‣ 5.4\. Full-digital Neuromorphic
    Accelerators ‣ 5\. Accelerators based on Emerging Paradigms and Technologies ‣
    A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
    summarizes the main characteristics of the neuromorphic chips described so far
    with particular insight on the Energy per spike operation (SOP) that is seen as
    a primary benchmarking factor for these architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9. Summary of Neuromorphic chip characteristics based on ([frenkel2021arxiv,](#bib.bib77)
    )
  prefs: []
  type: TYPE_NORMAL
- en: '| Chip name | Technology | Cores | Core Area [mm²] | Neurons per core | Synapses
    per core | Weights storage | Supply Voltage [V] | Energy per SOP [J] |'
  prefs: []
  type: TYPE_TB
- en: '| SpiNNaker ([painkras2013jsscc,](#bib.bib206) ) | 0.13 $\mu$m | 18 | 3.75
    | 1000 | - | Off-chip | 1.2 | $>$11.3n/26.6n |'
  prefs: []
  type: TYPE_TB
- en: '| ([seo2011cicc,](#bib.bib239) ) | 45 nm SOI | 1 | 0.8 | 256 | 64k | 1-bit
    SRAM | 0.53 - 1.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ODIN ([frenkel2018tbiocas,](#bib.bib78) ) | 28 nm FDSOI | 1 | 0.086 | 256
    | 64k | (3+1)-bits (SRAM) | 0.55 - 1.0 | 8.4p/12.7p |'
  prefs: []
  type: TYPE_TB
- en: '| MorphIC ([frenkel2019tbiocas,](#bib.bib79) ) | 65 nm LP | 4 | 0.715 | 512
    | 528k | 1-bit (SRAM) | 0.8 - 1.2 | 30p/51p |'
  prefs: []
  type: TYPE_TB
- en: '| TrueNorth ([akopyan2015tcad,](#bib.bib5) ) | 28 nm | 4096 | 0.095 | 256 |
    64k | 1-bit (SRAM) | 0.7 - 1.05 | 26p |'
  prefs: []
  type: TYPE_TB
- en: '| Loihi ([davies2018micro,](#bib.bib57) ) | 14 nm FinFET | 128 | 0.4 | 1024
    | 1M | 1- to 9 bits (SRAM) | 0.5 - 1.25 | $>$23.6p | {forest}'
  prefs: []
  type: TYPE_NORMAL
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [NPU and
    Neuromorphic computing accelerators [MAC acceleration [Full-digital [Samsung ([song2019isscc,](#bib.bib248)
    ), UM+NVIDIA ([zhang2019vlsi,](#bib.bib304) )
  prefs: []
  type: TYPE_NORMAL
- en: MediaTek ([lin2020isscc,](#bib.bib168) ), Alibaba ([jiao2020isscc,](#bib.bib128)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Samsung ([park2021isscc,](#bib.bib211) ), Samsung ([park2022isscc,](#bib.bib212)
    )
  prefs: []
  type: TYPE_NORMAL
- en: '] ] [RRAM/PCM-based [ RENO ([liu2015dac,](#bib.bib172) )'
  prefs: []
  type: TYPE_NORMAL
- en: NTHU ([xue2019isscc,](#bib.bib296) )
  prefs: []
  type: TYPE_NORMAL
- en: IBM ([narayanan2021ted,](#bib.bib194) )
  prefs: []
  type: TYPE_NORMAL
- en: '] ] ], [Single chip NPUs [Full-digital [DaDianNao ([chen2016commacm,](#bib.bib39)
    ), ShiDianNao ([chen2016commacm,](#bib.bib39) )'
  prefs: []
  type: TYPE_NORMAL
- en: Cambricon ([zhang2016micro,](#bib.bib305) ), EIE ([han2016isca,](#bib.bib103)
    )
  prefs: []
  type: TYPE_NORMAL
- en: STM ([desoli17isscc,](#bib.bib62) ), IBM ([oh2020vlsic,](#bib.bib202) )
  prefs: []
  type: TYPE_NORMAL
- en: IBM ([lee2022jsscc,](#bib.bib160) )
  prefs: []
  type: TYPE_NORMAL
- en: '] ] [In-memory computing [STM ([desoli2023isscc,](#bib.bib61) ), ISAAC ([shafiee2016isca,](#bib.bib241)
    )'
  prefs: []
  type: TYPE_NORMAL
- en: PipeLayer ([song2017hpca,](#bib.bib249) ), NeuralPIM ([cao2022tcomp,](#bib.bib29)
    )
  prefs: []
  type: TYPE_NORMAL
- en: PRIME ([chi2016isca,](#bib.bib45) ), NeuRRAM ([wan2022nature,](#bib.bib284)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Hermes ([khaddam2022hermes,](#bib.bib140) )
  prefs: []
  type: TYPE_NORMAL
- en: '] ] ], [Neuromorphic chips [SpiNNaker ([painkras2013jsscc,](#bib.bib206) ),
    Seo et al. ([seo2011cicc,](#bib.bib239) )'
  prefs: []
  type: TYPE_NORMAL
- en: ODIN ([frenkel2018tbiocas,](#bib.bib78) ), MorphIC ([frenkel2019tbiocas,](#bib.bib79)
    )
  prefs: []
  type: TYPE_NORMAL
- en: TrueNorth ([akopyan2015tcad,](#bib.bib5) ), Loihi ([davies2018micro,](#bib.bib57)
    )
  prefs: []
  type: TYPE_NORMAL
- en: '] ], ]'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8. Taxonomy of neural accelerators discussed in Sections [4.2.2](#S4.SS2.SSS2
    "4.2.2\. Neural Processing Units (NPUs) ‣ 4.2\. ASIC-based Accelerators ‣ 4\.
    Hardware Accelerators ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms"), [4.2.3](#S4.SS2.SSS3 "4.2.3\. Single-chip NPUs ‣ 4.2\. ASIC-based
    Accelerators ‣ 4\. Hardware Accelerators ‣ A Survey on Deep Learning Hardware
    Accelerators for Heterogeneous HPC Platforms"), [5.3](#S5.SS3 "5.3\. In-memory
    computing accelerators based on emerging memories ‣ 5\. Accelerators based on
    Emerging Paradigms and Technologies ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms"), [5.4](#S5.SS4 "5.4\. Full-digital Neuromorphic
    Accelerators ‣ 5\. Accelerators based on Emerging Paradigms and Technologies ‣
    A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
  prefs: []
  type: TYPE_NORMAL
- en: 5.5\. Accelerators based on Multi-Chip Modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The alternate multichip-module (MCM) silicon interposer-based integration technology,
    described in Section A.8 of Appendix A, offers several advantages over single-chip
    designs. These advantages include increased functionality, reduced power consumption,
    higher performance, improved reliability, and cost savings.
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing MCM, designers can combine multiple chips and functionalities into
    a single package, resulting in a reduced overall footprint and cost. MCM-based
    designs can also be optimized for different power requirements, achieving higher
    power efficiency compared to single-chip designs. Additionally, MCM-based designs
    can be fine-tuned for different tasks, offering superior performance compared
    to single-chip counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: The use of redundancy in MCM-based designs improves fault tolerance, leading
    to improved reliability compared to single-chip designs. Furthermore, MCM-based
    designs can utilize off-the-shelf components and existing manufacturing processes
    and technology, contributing to cost savings in overall manufacturing.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [9](#S5.F9 "Figure 9 ‣ 5.5\. Accelerators based on Multi-Chip Modules
    ‣ 5\. Accelerators based on Emerging Paradigms and Technologies ‣ A Survey on
    Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms") illustrates
    a comprehensive taxonomy of the MCM-based designs explored in this survey. Specifically,
    Sec. A.8.2 of Appendix A discusses a collection of representative general-purpose
    MCM-based designs, while this section concentrates on MCM-based DNN accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: rounded/.style=ellipse,draw, squared/.style=rectangle,draw, qtree, [Accelerators
    based on Multi-Chip Modules [General Purpose [Passive Interposer [ Mounce et al.([mounce_ac16,](#bib.bib190)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Vijayaraghavan et al. ([vijayaraghavan_hpca17,](#bib.bib282) )
  prefs: []
  type: TYPE_NORMAL
- en: Arunkumar et al. ([arunkumar_isca17,](#bib.bib14) ), Lin et al. ([lin_vlsi19,](#bib.bib169)
    ) ] ], [Active Interposer [ Vivet et al.([vivet_jssc21,](#bib.bib283) )
  prefs: []
  type: TYPE_NORMAL
- en: Martinez et al. ([martinez_vlsi20,](#bib.bib179) ) ] ] ], [Domain Specific [
    Kwon et al.([kwon_iceic23,](#bib.bib153) ), Nurvitadhi et al. ([nurvitadhi_fccm19,](#bib.bib196)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Verhelst et al. ([verhelst2022ml,](#bib.bib281) ), Centaur ([hwang_isca20,](#bib.bib116)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Lan et al. ([lan_eptc21,](#bib.bib155) ), Lego ([xuan_isocc22,](#bib.bib295)
    )
  prefs: []
  type: TYPE_NORMAL
- en: Chimera ([prabhu2022chimera,](#bib.bib221) ), SWAP ([sharma2022swap,](#bib.bib244)
    )
  prefs: []
  type: TYPE_NORMAL
- en: SPRINT ([li2021sprint,](#bib.bib167) ), Simba ([zimmer_jssc20,](#bib.bib308)
    ; [shao_micro19,](#bib.bib243) ) ] ] ]
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9. Taxonomy of MCM based accelerators discussed in Section [5.5](#S5.SS5
    "5.5\. Accelerators based on Multi-Chip Modules ‣ 5\. Accelerators based on Emerging
    Paradigms and Technologies ‣ A Survey on Deep Learning Hardware Accelerators for
    Heterogeneous HPC Platforms")
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of DL, chiplet-based design is utilized to create hardware accelerator
    platforms that are both efficient and scalable. Designing AI processors for data
    explosion computing due to the physical limitations of semiconductors and high
    costs is challenging. In ([kwon_iceic23,](#bib.bib153) ) authors propose chiplet-based
    design as a viable solution to this problem. They outline various aspects of designing
    a chiplet AI processor, including incorporating NPU chiplets, HBM chiplets, and
    2.5D interposers, ensuring signal integrity for high-speed interconnections, power
    delivery network for chiplets, bonding reliability, thermal stability, and interchiplet
    data transfer on heterogeneous integration architecture. They conclude that chiplet-based
    design provides higher performance at a lower cost compared to IP-based design.
  prefs: []
  type: TYPE_NORMAL
- en: Data-intensive DL algorithms with strict latency constraints require balancing
    both data movement and compute capabilities. Thus, persistent approaches that
    keep the entire DL model on-chip are becoming the new norm for real-time services
    to avoid expensive off-chip memory accesses. In ([nurvitadhi_fccm19,](#bib.bib196)
    ) it is shown how the integration of FPGA with ASIC chiplets outperforms GPU-based
    platforms (NVIDIA Volta) in terms of latency by enhancing on-chip memory capacity
    and bandwidth and provides compute throughput matching the required bandwidth.
    Specifically, it is reported that the GPU and chiplet-based FPGA computing capabilities
    are 6% and 57% of their peak, respectively. In terms of delay and energy efficiency,
    the FPGA outperforms the GPU with a delay that is 1/16 and energy efficiency that
    is 34x better than the GPU’s peak performance.
  prefs: []
  type: TYPE_NORMAL
- en: In ([verhelst2022ml,](#bib.bib281) ) the authors investigate the recent multi-core
    trend in deep-learning accelerators evolution as a solution to further increase
    throughput and match the ever-growing computational demands. Chiplet integration
    is considered a promising implementation strategy for both homogeneous and heterogeneous
    multi-core accelerators. In ([hwang_isca20,](#bib.bib116) ) authors present a
    new chiplet-based hybrid sparse-dense accelerator called Centaur, which addresses
    memory-intensive embedding layers and compute-intensive multi-layer perceptron
    layers. The proposed accelerator demonstrates significant performance speedup
    and energy efficiency improvement compared to conventional approaches monolithic
    approaches. The trend towards developing high throughput and energy-efficient
    neural network hardware accelerators due to the growing complexity and dimension
    of neural network algorithms is analyzed in ([lan_eptc21,](#bib.bib155) ). The
    authors propose a chiplet-based architecture for a multi-core neuromorphic processor
    with a chip-package co-design flow. It is shown how the proposed design is reusable
    for different neuromorphic computing applications by scaling the number of chips
    in a package and by reusing existing IPs from different technology nodes with
    2.5D integration technology. The challenges of using modern DNN accelerators in
    multi-tenant DNN data centers are investigated in ([xuan_isocc22,](#bib.bib295)
    ). The MCM architecture is proposed as a promising approach to address this issue
    but highlights the challenge of distributing DNN model layers with different parameters
    across chiplets. Thus the authors present Lego MCM architecture with a dynamic
    scheduler that adapts to the size of DNN model layers and increases chiplet utilization.
    The results show that Lego MCM achieves a 1.51x speedup over a monolithic DNN
    accelerator. Chimera ([prabhu2022chimera,](#bib.bib221) ) is a non-volatile chip
    for DNN training and inference that does not require off-chip memory. Multiple
    Chimera accelerator chiplets can be combined in a multi-chip system to enable
    inference on models larger than the single-chip memory with only $5\%$ energy
    overhead. Chiplet-based PIM DNN hardware accelerators have also been proposed.
    SWAP ([sharma2022swap,](#bib.bib244) ) is a DNN inference accelerator based on
    the 2.5D integration of multiple resistive RAM chiplets that allows fabrication
    cost reductions. The authors also propose a design space exploration flow to optimize
    the interconnection Network-on-Package, minimizing inter-chiplet communications
    and enabling link pruning. Inter-chiplet communication remains one of the main
    challenges in multi-chiplet architectures. Authors in ([li2021sprint,](#bib.bib167)
    ) investigate photonic-based interconnects as an alternative to metallic-based
    inter-chiplet networks and propose a DNN inference accelerator namely SPRINT.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/22f709a77d6b9b2905a308d5d4e8b0e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10. Simba architecture ([shao_micro19,](#bib.bib243) ) from left to
    right: package with 36 chiplets, chiplet, and processing element.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as a representative chiplet-based DNN hardware accelerator, we report
    Simba ([zimmer_jssc20,](#bib.bib308) ; [shao_micro19,](#bib.bib243) ). Simba is
    a scalable DNN accelerator consisting of 36 chiplets connected in a mesh network
    on a multi-chip module using ground-referenced signaling. Simba enables flexible
    scaling for efficient inference on a wide range of DNNs, from mobile to data center
    domains. The prototype achieves high area efficiency, energy efficiency, and peak
    performance for both one-chiplet and 36-chiplet systems. Simba architecture is
    shown in Figure [10](#S5.F10 "Figure 10 ‣ 5.5\. Accelerators based on Multi-Chip
    Modules ‣ 5\. Accelerators based on Emerging Paradigms and Technologies ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"). It implements
    a tile-based architecture and adopts a hierarchical interconnect to efficiently
    connect different PEs. This hierarchical interconnect consists of a network-on-chip
    (NoC) that connects PEs on the same chiplet and a network-on-package (NoP) that
    connects chiplets together on the same package. Each Simba chiplet contains an
    array of PEs, a global PE, an NoP router, and a controller, all connected by a
    chiplet-level interconnect. Table [10](#S5.T10 "Table 10 ‣ 5.5\. Accelerators
    based on Multi-Chip Modules ‣ 5\. Accelerators based on Emerging Paradigms and
    Technologies ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms") presents a summary of the key characteristics of a representative
    subset of chiplet-based DNN accelerators that were reviewed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10. Summary of Chiplet-based DNN Accelerators.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Simba ([shao_micro19,](#bib.bib243) ) | Centaur ([hwang_isca20,](#bib.bib116)
    ) | Lego ([xuan_isocc22,](#bib.bib295) ) | Chimera ([prabhu2022chimera,](#bib.bib221)
    ) | SWAP ([sharma2022swap,](#bib.bib244) ) | SPRINT ([li2021sprint,](#bib.bib167)
    ) |'
  prefs: []
  type: TYPE_TB
- en: '| Technology | 16nm | FPGA | FPGA | 40nm | - | 28nm |'
  prefs: []
  type: TYPE_TB
- en: '| Area | 6 mm²* | - | 19016 LUT, 16916 FF 0.5 BRAM, 28 DSP† | 29.2 mm² | -
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| Power Efficiency | 9.1 TOPS/W** | - | - | 2.2 TOPS/W | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Throughput | 4–128 TOPS | 0.313 TOPS | - | 0.92 TOPS | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Frequency | 161 MHz–1.8 GHz | 200 MHz | - | 200 MHz | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Precisions | int8 | - | - | int8, fp16 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| On-chip Memory | 752 KiB* | - | - | 2.5 MB‡ | - | 128 KiB* |'
  prefs: []
  type: TYPE_TB
- en: '| Chiplet Bandwidth | 100 GB/s | - | - | 1.9 Gbps | - | 180 Gbps |'
  prefs: []
  type: TYPE_TB
- en: '| Processing Type | Near-Memory | Near-Memory | Near-Memory | Near-Memory |
    In-Memory | Near-Memory |'
  prefs: []
  type: TYPE_TB
- en: '| Sparsity | ✗ | ✓ | ✗ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Interconnect | Wired Mesh (GRS) | - | - | Wired App. specific‡‡ | Wired Pruned
    | Optical§ |'
  prefs: []
  type: TYPE_TB
- en: '| Applications | CNN Inference | Recommendation Inference | Multi-Tenant Inference
    | Inference Training | Multiple Applications | CNN Inference |'
  prefs: []
  type: TYPE_TB
- en: '| *One chiplet, **When operating at a minimum voltage of 0.42 V with a 161
    MHz PE frequency |'
  prefs: []
  type: TYPE_TB
- en: '| †16$\times$16 chiplet, ‡2 MB RRAM, 0.5 MB SRAM, ‡‡C2C links (77 pJ/bit, 1.9
    Gbits/s), §0.77 pJ/bit |'
  prefs: []
  type: TYPE_TB
- en: 6\. Conclusions and Open Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Deep Learning ecosystem based on advanced computer architectures and memory
    technologies spans from edge computing solutions to high-performance servers,
    supercomputers, and up to large data centers for data analytics. In this context,
    the main objective of this survey is to provide an overview of the leading computing
    platforms utilized for accelerating the execution and enhancing the efficiency
    of high-performance Deep Learning applications. More in detail, this survey includes
    GPU-based accelerators, Tensor Processor Units, FPGA-based accelerators, Neural
    Processing Units, and co-processors based on the open-hardware RISC-V architecture.
    The survey also describes accelerators based on emerging computing paradigms and
    technologies, such as 3D-stacked PIM, emerging non-volatile memories, NPUs, and
    Multi-Chip Modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before concluding this survey, we would like to introduce some open challenges
    on two promising technologies to further speed up DL workloads: Quantum Computing
    and Photonic Computing. There is a general agreement that quantum computers will
    not replace classical computing systems, but they will be used in combination
    with supercomputers to accelerate some hard-to-compute problems. Quantum computers
    will play the role of unconventional accelerators with the goal to outperform
    conventional supercomputers, thanks to the improved parallelism which enables
    the so-called *quantum speedup*. Governments, supercomputing centers, and companies
    around the world have also started to investigate $How$/$When$/$Where$ quantum
    processing units (QPUs) could fit into HPC infrastructures to speed up some heavy
    tasks, such as Deep Learning workloads. Emerging trends and commercial solutions
    related to *hybrid* quantum-classical supercomputers are described in  ([HPCWIRE2022,](#bib.bib113)
    ). To address this challenging trend, in October 2022, the EuroHPC Joint Undertaking
    initiative selected six supercomputing centers across the European Union to host
    quantum computers and simulators. IBM Research was the first provider to offer
    a cloud-based QC service. IBM Qiskit  ([QISKIT2023,](#bib.bib228) ) is an open-source
    SDK based on a library of quantum gates/circuits: Remote users can develop quantum
    programs and execute them on quantum simulators and cloud-based quantum processors.
    Cloud providers have also jumped into the quantum race. As an example, Amazon
    Braket ([BRAKET2023,](#bib.bib23) ) is a QC service based on different types of
    quantum systems and simulators, including the quantum annealer from D-Wave. On
    this trend, there is a general agreement that GPUs will play a key role in hybrid
    quantum-classical computing systems. GPU company NVIDIA offers CuQuantum DGX hardware
    appliance integrating a software container on a full-stack quantum circuit simulator:
    The system uses NVIDIA’s A100 GPUs to accelerate quantum simulation workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, a survey on QC technologies appeared in  ([GYONGYOSI201951,](#bib.bib100)
    ), while another survey on QC frameworks appeared in  ([upama2022,](#bib.bib271)
    ). More specifically, there is a promising research trend on *Quantum Machine
    Learning*  ([Biamonte2017,](#bib.bib18) ) which aims at developing quantum algorithms
    that outperform classical computing algorithms on machine learning tasks such
    as recommendation systems. More in detail, classical deep neural networks inspired
    the development of *Deep Quantum Learning* methods. The main advantage of these
    methods is that they do not require a large, general-purpose quantum computer.
    Quantum annealers, such as the D-Wave commercial solutions ([D-WAVE2023,](#bib.bib55)
    ), are well-suited for implementing deep quantum learners. Quantum annealers are
    special-purpose quantum processors that are significantly easier to construct
    and scale up than general-purpose quantum computers. Following this research trend,
    Google proposed TensorFlow Quantum (TFQ)  ([broughton2021,](#bib.bib24) ), an
    open-source quantum machine learning library for prototyping hybrid quantum-classical
    ML models.
  prefs: []
  type: TYPE_NORMAL
- en: The second challenging and promising research direction is represented by the
    use of Photonic Computing to further accelerate DL tasks on HPC infrastructures.
    Photonic Computing relies on the computation of electromagnetic waves typically
    via non-linear modulation and interference effects. It was originally introduced
    in the 1980s to address optical pattern recognition and optical Fourier transform
    processing ([ambs_2010,](#bib.bib8) ). Despite the potential advantages of processing
    parallelism and speed, optical computing has never translated into a commercial
    technology. Only recently, due to the emergence of data-intensive computing tasks,
    such as AI, optical computing has seen a renewed interest. There are two main
    advantages of optical computing, namely (i) the inherent speed of signal transmission,
    where light pulses can be transferred without the typical RC delays and IR drop
    of electrical interconnects, and (ii) the inherent parallelism, where multiple
    wavelengths, polarizations, and modes can be processed by the same hardware (e.g.,
    waveguides, interferometers, etc.), without interfering with each other. These
    properties can provide strong benefits to data-intensive computing tasks such
    as DL. Photonic computing represents a promising platform for accelerating AI.
    For instance, it has been estimated that photonic multiply-accumulate operations
    can show significant improvements over digital electronics in terms of energy
    efficiency ($>10^{2}$), speed ($>10^{3}$), and compute density ($>10^{2}$) ([nahmias_2020,](#bib.bib192)
    ). However, there are still many challenges to developing an industrially feasible
    photonic system. The main challenge is the area/energy inefficiency of processing
    across the mixed optical/electronic domain. Optical-electrical conversion and
    vice versa result in considerable overhead in terms of area and power consumption.
    To bridge this gap, the trend is developing silicon photonic integrated circuits
    (PICs) with increasing robustness, manufacturability, and scalability. Photonic
    computing essentially operates in the analog domain, thus accuracy is deeply affected
    by accumulated noise and imprecision of optical devices, such as electro-optic
    and phase change modulators. These challenges, similar to those arising in analog
    IMC, might be mitigated by hardware-aware training and system-level calibration
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A.1\. Deep Learning Background: Concepts and Terminology'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep Learning ([Schmidhuber14,](#bib.bib237) ; [lecun2015,](#bib.bib158) )
    is a subset of ML methods that use artificial DNNs for automatically discover
    the representations needed for feature detection or classification from large
    data sets, by employing multiple layers of processing to extract progressively
    higher-level features. DNNs mimic human brain functionalities, in which neurons
    are interconnected with each other to receive information, process it, and pass
    it to other neurons. As shown in Figure [11(a)](#A1.F11.sf1 "In Figure 11 ‣ A.1\.
    Deep Learning Background: Concepts and Terminology ‣ Appendix A Appendix ‣ A Survey
    on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms"), in a
    way similar to the brain’s neuron, the simple model of a perceptron (artificial
    neuron) receives information from a set of inputs and applies a nonlinear function
    F (activation function) on a weighted (W) sum of the inputs (X) ([Rosenblatt1957,](#bib.bib234)
    ). DNNs are composed of a number of layers of artificial neurons (hidden layers),
    organized between the input layer, which brings the initial data into the system,
    and the output layer, in which the desired predictions are obtained (see Figure [11(b)](#A1.F11.sf2
    "In Figure 11 ‣ A.1\. Deep Learning Background: Concepts and Terminology ‣ Appendix
    A Appendix ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms")). In feed-forward networks, the outputs of one layer become the
    inputs of the next layer in the model, while in recurrent networks, the output
    of a neuron can be the input of neurons in the same or previous layers. The term
    “deep” in DNNs refers to the use of a large number of layers, which results in
    more accurate models that capture complex patterns and concepts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4421338c1045af6ff67b67dd029821b6.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cf10d8a53a9bd2cf5a47a820eb6b68c2.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11. Model of a perceptron (artificial neuron) (a) and of a multi-layer
    DNN (b).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two phases in a DNN’s operation: training, and inference. In the
    *training* phase, the neural network model is fed on a curated data set so that
    it can “learn” everything it needs to about the type of data it will analyze.
    In the case of *supervised* learning, a large set of examples and their corresponding
    labels indicating the correct classification are passed as input to the DNN. A
    forward pass is executed, and the error against the correct labels is measured.
    Then, the error is used in the DNN’s backward pass to update the weights. This
    loop is performed repeatedly until the DNN model achieves the desired accuracy.
    In *unsupervised* learning, the DNN uses unlabeled data to create an encoded self-organization
    of weights and activations that captures patterns as probability densities. With
    *semi-supervised* learning, during training a small amount of labeled data is
    combined with a large amount of unlabeled data. In the *inference* phase, the
    trained DNN model is used to make predictions on unseen data. When it comes to
    deployment, the trained model is often modified and simplified to meet real-world
    power and performance requirements. The two phases present different computational
    characteristics. On the one hand, the training phase of a model is computationally
    expensive but usually performed only once. On the other hand, the trained model
    is used for predictions on multiple input data, often under strict latency and/or
    energy constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three general types of DNN are mostly used today: Multi-Layer Perceptrons (MLPs),
    Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs). MLPs
    ([Rosenblatt1957,](#bib.bib234) ) are feed-forward ANNs composed of a series of
    fully connected layers, where each layer is a set of nonlinear functions of a
    weighted sum of all outputs of the previous one. On the contrary, in a CNN ([Lecun1998,](#bib.bib159)
    ), a convolutional layer extracts the simple features from the inputs by executing
    convolution operations. Each layer is a set of nonlinear functions of weighted
    sums of different subsets of outputs from the previous layer, with each subset
    sharing the same weights. Each convolutional layer in the model can capture a
    different high-level representation of input data, allowing the system to automatically
    extract the features of the inputs to complete a specific task, e.g., image classification,
    face authentication, and image semantic segmentation. Finally, RNNs ([Schmidhuber14,](#bib.bib237)
    ) address the time-series problem of sequential input data. Each RNN layer is
    a collection of nonlinear functions of weighted sums of the outputs of the previous
    layer and the previous state, calculated when processing the previous samples
    and stored in the RNN’s internal memory. RNN models are widely used in Natural
    Language Processing (NLP) for natural language modeling, word embedding, and machine
    translation.'
  prefs: []
  type: TYPE_NORMAL
- en: On the one hand, DNNs such as AlexNet([Krizhevsky2012,](#bib.bib151) ) and the
    more recent GoogLeNet([Szegedy2015,](#bib.bib262) ) are composed of tens of layers,
    with millions of weights to be trained and used in every prediction, requiring
    tens to hundreds of megabytes (or even gigabytes) of memory for their storage.
    The calculation of the weighted sums requires a large number of data movements
    between the different levels of the memory hierarchy and the processing units,
    often posing a challenge to the available energy, memory bandwidth, and memory
    storage of the computing architecture. On the other hand, Tiny machine learning
    (TinyML) DNN models ([Warden2019,](#bib.bib291) ) have been investigated to run
    on small, battery-operated devices like microcontrollers, trading off prediction
    accuracy with respect to low-latency, low-power, and low-bandwidth model inference
    of sensor data on edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides DNNs, the other major category of DL algorithms is that of Transformer-based
    models ([vaswaniAttentionAllYou2017,](#bib.bib276) ), which have recently captured
    great attention. Transformers were originally proposed for NLP ([vaswaniAttentionAllYou2017,](#bib.bib276)
    ), and are designed to recognize long-distance dependencies between data by means
    of *attention* layers. In attention layers, the weights used to linearly transform
    the input data are computed dynamically based on the input data itself. Transformer
    models are flexible (e.g., they can also be used for vision tasks ([dosovitskiyImageWorth16x162021,](#bib.bib66)
    )) and, most importantly, empirical scaling laws ([kaplanScalingLawsNeural2020,](#bib.bib139)
    ) govern their expressiveness: larger transformers trained for more time on larger
    datasets deliver better performance in consistent and predictable way capabilities.
    This makes it possible to construct and pre-train larger and larger models (e.g.,
    GPT-4 ([openaiGPT4TechnicalReport2023,](#bib.bib204) ), PaLM ([anilPaLMTechnicalReport2023,](#bib.bib13)
    ), LLaMA ([touvronLLaMAOpenEfficient2023,](#bib.bib267) )) with hundreds of billions
    of trillions of parameters, which can be then used as *Foundation Models* ([bommasaniOpportunitiesRisksFoundation2022,](#bib.bib22)
    ) to be fine-tuned for more focused applications.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2\. Technology for GPU and TPU Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this sub-section, we review the basic features of NVIDIA GPU architectures
    to boost the performance of HPC and Deep Learning applications. GPUs are specific-purpose
    processors introduced to compute efficiently graphics-related tasks, such as 3D
    rendering. They became widely used since the nineties as co-processors, working
    alongside CPUs, to offload graphics-related computations. The introduction of
    programmable shaders into GPU architectures, increased their flexibility paving
    the way for their adoption to perform general-purpose computations. Despite being
    specifically designed for computer graphics, their highly-parallel architecture
    is well suited to tackle a wide range of applications. Consequently, in the early
    2000s, GPUs started to be used to accelerate data-parallel computations not necessarily
    related to graphics, which could benefit from their architecture as well. This
    practice is commonly referred to as General-Purpose computing on GPUs (GP-GPU)
    started to be increasingly popular in the early 2010s with the advent of the CUDA
    language. The technological development of the last ten years significantly increased
    the computing power of GPU devices, which, due to their highly parallel nature,
    are incidentally very well suited to accelerate neural network training algorithms.
    The availability of such computing power allowed more complex neural network models
    to become practically usable, fostering the development of DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: The impressive results obtainable with DNNs in the context of AI, followed by
    significant investments in this market sector, induced hardware manufacturers
    to modify GPU architectures in order to be even more optimized to compute such
    workloads, as an example implementing the support for lower-precision computations.
    This led to a de-facto co-design of GPU architectures and neural network algorithms
    implementations, which is nowadays significantly boosting the performance, accuracy,
    and energy efficiency of AI applications. The hardware architecture of a GPU is
    based on a multicore design of processing elements called Streaming Multiprocessors
    (SM). Each SM, in turn, includes a number of compute units, called CUDA-cores
    in NVIDIA jargon, to execute at each clock-cycle multiple warps, i.e. groups of
    32 operations called CUDA-threads processed by the Single Instruction Multiple
    Thread (SIMT) fashion. SIMT execution enables different threads of a group to
    take different branches (with a performance penalty). By varying CPU threads,
    context switches among active CUDA threads are very fast. Typically one CUDA-thread
    processes one element of the target data set. This helps to exploit the available
    parallelism of the algorithm and to hide the latency by swapping among threads
    waiting for data coming from memory and threads ready to run. This structure remained
    stable across generations, with several enhancements implemented in the most recent
    architectures making available more registers addressable to each CUDA thread.
    Considering each generation of NVIDIA architecture, some minor differences occurred.
    The C2050 and C2070 boards based on the Fermi processor architecture differ in
    the amount of available global memory. Both cards have a peak performance of $\approx
    1$ Tflops in single-precision (SP), and $\approx 500$ Gflops in double-precision
    (DP), and the peak memory bandwidth is $144$ GB/s.
  prefs: []
  type: TYPE_NORMAL
- en: The K20, K40, and K80 are boards based on the Kepler architecture. The K40 processor
    has more global memory than the K20 and slightly improves memory bandwidth and
    floating-point throughput, while the K80 has two enhanced Kepler GPUs with more
    registers and shared memory than K20/K40 and extended GPUBoost features. On the
    Kepler K20 and K40, the peak SP (DP) performance is $\approx 5$ Tflops ($\approx
    1.5$ Tflops), while on the K80 the aggregate performance of the two GPUs delivers
    a peak SP (DP) of $\approx 5.6$ Tflops ($\approx 1.9$ Tflops). The peak memory
    bandwidth is $250$ and $288$ GB/s respectively for the K20X and the K40 while
    on the K80 the aggregate peak is $480$ GB/s.
  prefs: []
  type: TYPE_NORMAL
- en: The P100 board is based on the Pascal architecture, engineered to tackle memory
    challenges using stacked memory, a technology that enables multiple layers of
    DRAM components to be integrated vertically on the package along with the GPU.
    The P100 is the first GPU accelerator to use High Bandwidth Memory 2 (HBM2) to
    provide greater bandwidth, more than twice the capacity, and higher energy efficiency,
    compared to off-package GDDR5 used in previous generations. The SXM-2 version
    of P100 board also integrates the NVLinks, NVIDIA’s new high-speed interconnect
    technology for GPU-accelerated computing significantly increasing performance
    for both GPU-to-GPU communications and for GPU access to system memory. The P100
    delivers a peak performance of $\approx 10.5$ Tflops SP and $\approx 5.3$ in DP,
    while the peak memory bandwidth has been increased to $732$ GB/s.
  prefs: []
  type: TYPE_NORMAL
- en: The Volta architecture has been developed and engineered for the convergence
    of HPC and AI. Key compute features of Tesla V100 include a new SM Architecture
    Optimized for Deep Learning, integrating Tensor Cores designed specifically for
    deep learning. Also, the Tesla V100 board integrates a second-generation NVLink
    supporting up to 6 links at 25 GB/s for a total of 300 GB/s, and1 6GB of HBM2
    memory subsystem delivering 900 GB/sec peak memory bandwidth provides 1.5x delivered
    memory bandwidth versus Pascal GP100\. Volta increases the computing throughput
    to 7.5 Tflops DP, and the memory bandwidth to 900 GB/s, respectively a factor
    1.4X and 1.2X w.r.t. the Pascal architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The Ampere architecture adds a powerful new generation of Tensor Core that boosts
    throughput over V100 for Deep Learning applications running 10x faster. The peak
    performance in DP has been increased to 9.7 TFlops, and to 19.5 TFlops using Tensor
    Core or single precision FP32 operations. The A100 40GB of high-speed HBM2 memory
    with a peak bandwidth of 1555 GB/sec, corresponding to a 73% increase compared
    to the Tesla V100\. It also supports a third-generation of NVIDIA NVLink with
    a data rate of 50 Gbit/sec per signal pair, nearly doubling the 25.78 Gbits/sec
    rate in V100.
  prefs: []
  type: TYPE_NORMAL
- en: The Hopper is the latest architecture developed by NVidia providing a new generation
    of streaming multiprocessors with several new features. Tensor Cores are up to
    6x faster chip-to-chip compared to A100, the memory subsystem is based on HBM3
    modules providing nearly a 2x bandwidth increase over the previous generation,
    and integrate a fourth-generation of NVlinks providing a 3x bandwidth increase.
    The peak performance is boosted up to 24 TFlops in DP, and 48 TFlops using FP64
    tensor core and FP32 operations. The H100 SXM5 GPU raises the bar considerably
    by supporting 80 GB (five stacks) of fast HBM3 memory, delivering over 3 TB/sec
    of memory bandwidth, effectively a 2x increase over the memory bandwidth of the
    A100 that was launched just two years ago. The PCIe H100 provides 80 GB of fast
    HBM2e with over 2 TB/sec of memory bandwidth. The H100 also introduces DPX instructions
    to accelerate the performance of Dynamic Programming algorithms. These new instructions
    provide support for advanced fused operands for the inner loop of many dynamic
    programming algorithms. This leads to dramatically faster times-to-solution in
    disease diagnosis, logistics routing optimizations, and even graph analytics.
    For a more complete description, we can refer to ([fermi,](#bib.bib197) ; [kepler,](#bib.bib198)
    ; [pascal,](#bib.bib106) ; [volta,](#bib.bib69) ; [Ampere,](#bib.bib150) ; [h100,](#bib.bib12)
    ), while the work in [Table 11](#A1.T11 "Table 11 ‣ A.2\. Technology for GPU and
    TPU Architectures ‣ Appendix A Appendix ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms") summarizes just a few relevant parameters of
    NVIDIA GPU architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11. Summary of hardware features of NVIDIA GPU architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Fermi | Kepler | Kepler | Kepler |  | Pascal | Volta | Ampere
    | Hopper |'
  prefs: []
  type: TYPE_TB
- en: '| GPU | GF100 | GK110 | GK110B | GK210 | $\times$ 2 | P100 | V100 | A100 |
    H100 |'
  prefs: []
  type: TYPE_TB
- en: '| Year | 2011 | 2012 | 2013 | 2014 |  | 2016 | 2017 | 2021 | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '| #SMs | 16 | 14 | 15 | 13 | $\times$ 2 | 56 | 80 | 108 | 132 |'
  prefs: []
  type: TYPE_TB
- en: '| #CUDA-cores | 448 | 2688 | 2880 | 2496 | $\times$ 2 | 3584 | 5120 | 6912
    | 16896 |'
  prefs: []
  type: TYPE_TB
- en: '| Base clock (MHz) | 1.15 | 735 | 745 | 562 |  | 1328 | 1370 | 1700 | 1600
    |'
  prefs: []
  type: TYPE_TB
- en: '| Base DP (Gflops) | 515 | 1310 | 1430 | 935 | $\times$ 2 | 4755 | 7000 | 9700
    | 30000 |'
  prefs: []
  type: TYPE_TB
- en: '| Total available memory (GB) | 3 | 6 | 12 | 12 | $\times$ 2 | 16 | 16 | 40
    | 80 |'
  prefs: []
  type: TYPE_TB
- en: '| Memory bus width (bit) | 384 | 384 | 384 | 384 | $\times$ 2 | 4096 | 4096
    | 5120 | 5120 |'
  prefs: []
  type: TYPE_TB
- en: '| Peak mem. BW (GB/s) | 144 | 250 | 288 | 240 | $\times$ 2 | 732 | 900 | 1555
    | 3072 |'
  prefs: []
  type: TYPE_TB
- en: The number of GPU modules available within the DGX ([dgx,](#bib.bib200) ) line
    of server and workstation platforms varies from 4 to 16 Tesla daughter cards integrated
    into the system using a version of the high-bandwidth SMX([smx,](#bib.bib224)
    ) socket solution. The DGX-1 server, the first of DGX line, was announced in 2016,
    and it was first based on 8 Pascal cards, after upgraded to Volta, interconneced
    by an NVLink mesh network. The Pascal-based DGX-1 delivered 170 TFlops using FP16
    half-precision processing, while the Volta-based upgrade increased this to 960
    TFlops using FP16 tensor computing. The DGX-2, the successor of DGX-1, was announced
    in 2018; it is based on 16 V100 32 GB GPU cards in a single unit interconnected
    by a NVSwitch ([nvlink,](#bib.bib201) ) for high-bandwidth GPU-to-GPU communications,
    and delivers nearly 2 PFlops using FP16 tensor processing, ans assemble a total
    of 512 GB of HBM2 memory. The DGX Station is a workstation designed as a deskside
    AI system that can operate completely independent without the typical infrastructure
    of a datacenter. The DGX Station is a tower chassis, and the first available was
    including four Testa V100 accelerators each with 16 GB of HBM2 memory, delivering
    an aggregate computing performance of nearly 500 TFops using FP16 tensor computing.
    The Ampere version of the DGX Station includes four A100 accelerators configured
    with either 40 or 80 GB of memory each, resulting either in 160 GB or 320 GB variants,
    and a peak FP16-tensor computing performance of approximately 1 PFlops. The DGX
    A100 server is the 3rd generation of DGX servers announced in 2002\. It includes
    8 A100 accelerators, and it is the first DGX server replacing the Intel Xeon CPUs
    with the AMD EPYC CPUs, delivering a peak FP16-tensor computing performance of
    approximately 2.5 PFlops. The DGX H100 Server has been announced in 2022, and
    it is the 4th generation of DGX servers. It includes 8 Hopper H100 cards delivering
    a total of 16 PFlops of FP16-tensor AI computing, and assembling a total of 640
    GB of HBM3 memory. The DGX SuperPod is a high-performance turnkey supercomputer
    solution based on DGX hardware, combining high-performance DGX compute nodes with
    fast storage and high bandwidth networking, that can be used as building-block
    to assemble large supercomputer systems. The Selene Supercomputer, installed at
    the Argonne National Laboratory, is one example of a DGX SuperPod-based system,
    built from 280 DGX A100 nodes. The new version of SuperPod based on H100 DGX can
    scale up to 32 nodes, for a total of 256 H100 GPUs and 64 x86 CPUs. This gives
    the complete SuperPod a total 20TB of HBM3 memory, 70.4 TB/s of bisection bandwidth,
    and up to 1 EFlop of FP8 and 500 PFlops of FP16 tensor AI computing. The Eos([eos,](#bib.bib199)
    ) supercomputer announced in March 2022, designed, built, and operated by Nvidia,
    is based on 18 H100 SuperPods, for a total of 576 DGX H100 systems. This allows
    Eos to deliver approximately 18 EFlops of FP8 and 9 EFLOPs of FP16 computing,
    making Eos the fastest AI supercomputer in the world. Table [Table 12](#A1.T12
    "Table 12 ‣ A.2\. Technology for GPU and TPU Architectures ‣ Appendix A Appendix
    ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")
    summarizes the computing performance of a few DGX systems. We report the peak
    computing performance using tensor FP16 operations relevant for AI applications,
    and the standard FP32 and FP64 relevant for many scientific applications.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12. Performance in TFlops of DGX based platforms; for H100 platforms,
    sparsity features are used.
  prefs: []
  type: TYPE_NORMAL
- en: '| Platform | #GPUs | FP16 Tensor | F32 | FP64 |'
  prefs: []
  type: TYPE_TB
- en: '| DGX1-P100 | 8x P100 | – | 85 | 42 |'
  prefs: []
  type: TYPE_TB
- en: '| DGX1-V100 | 8x V100 | 1000 | 124 | 62 |'
  prefs: []
  type: TYPE_TB
- en: '| DGX2 | 16x V100 | 2000 | 248 | 124 |'
  prefs: []
  type: TYPE_TB
- en: '| DGX-A100 Server | 8x A100 | 2496 | 154 | 77 |'
  prefs: []
  type: TYPE_TB
- en: '| DGX-H100 Server | 8x H100 | 16000 | 544 | 272 |'
  prefs: []
  type: TYPE_TB
- en: '| Supercomputer |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Selene | 2240x A100 | 698880 | 43120 | 21560 |'
  prefs: []
  type: TYPE_TB
- en: '| Eos | 4608x H100 | 9216000 | 313344 | 156672 |'
  prefs: []
  type: TYPE_TB
- en: 'The TPU presented in TPU ([Jouppi_2017,](#bib.bib132) ; [jouppiMotivationEvaluationFirst2018,](#bib.bib134)
    ) is centered on a large (256$\times$256) systolic array operating on signed or
    unsigned 8-bit integers and targeting exclusively data center inference applications;
    this is coupled with a large amount of on-chip SRAM for activations (24 MiB) and
    a high-bandwidth (30 GiB/s) dedicated path to off-chip L3 DRAM for weights. The
    next design iterations (TPUv2, TPUv3) ([jouppiDomainspecificSupercomputerTraining2020,](#bib.bib135)
    ) forced to move from an inference-oriented design to a more general engine tuned
    for both inference and training, employing the 16-bit BF16 floating-point format,
    more cores (2 per chip) using each one or two 4$\times$ smaller arrays than TPUv1
    (128$\times$128, to reduce under-usage inefficiencies). TPUv2/v3 also introduced
    high-bandwidth memory support, which results in more than 20$\times$ increase
    in the available off-chip memory bandwidth. Conversely, Goya ([medina2019hotchips,](#bib.bib183)
    ) relies on PCIe 4.0 to interface to a host processor and exploits a design that
    uses a heterogenous approach comprising of a large General Matrix Multiply (GMM)
    engine, TPUs, and a large shared DDR4 memory pool. Each TPU also incorporates
    its own local memory that can be either hardware-managed or fully software-managed,
    allowing the compiler to optimize the residency of data and reduce movement. Each
    of the individual TPUs is a VLIW design that has been optimized for AI applications.
    The TPU supports mixed-precision operations including 8-bit, 16-bit, and 32-bit
    SIMD vector operations for both integer and floating-point. Gaudi has an enhanced
    version of the TPUs and uses HBM global memories rather than the DDR used in Goya,
    increasing the support towards bfloat16 data types and by including more operations
    and functionalities dedicated to training operations. As a counterpart of smaller
    TPUs, NVIDIA TensorCores are small units, designed to perform a 4$\times$4$\times$4
    FP16 GEMM operation per cycle in Volta (doubled in Ampere and quadrupled in Hopper,
    adding also support for other data types). Performance is then obtained by parallelization:
    each Streaming Multiprocessor includes eight TensorCores controlled by 32 threads;
    and, depending on the specific chip, GPUs can contain tens of Streaming Multiprocessors.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3\. FPGA Technology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'FPGAs are semiconductor devices that provide a unique combination of flexibility
    and performance thanks to their fundamental building blocks, known as Configurable
    Logic Blocks (CLBs) or simply Logic Elements (LEs). They consist of look-up tables
    (LUTs) and flip-flops that can be used to implement arbitrary combinational and
    sequential bit-level operations, on the basis of user-defined tasks. Programmable
    interconnects provide the necessary routing resources to establish connections
    between different elements within the device and to facilitate the seamless flow
    of data and control signals. FPGAs provide various types of memory resources that
    can be utilized for different purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Block RAM (BRAM): specialized on-chip memory resource that offers dedicated
    storage for data or program code. BRAMs are characterized by their dual-port or
    true dual-port design, providing high bandwidth and low latency. Organized into
    fixed-sized blocks, BRAMs serve a variety of purposes including data buffering,
    cache memory, FIFO implementation, and storage for coefficients or tables utilized
    in digital signal processing applications. BRAMs play a crucial role in optimizing
    data access and manipulation within FPGA designs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distributed RAM: memory elements that are distributed across the logic fabric
    of an FPGA. Unlike BRAM, which is dedicated memory, distributed RAM is implemented
    using the LUTs present in the CLBs. These LUT-based memories offer smaller storage
    capacity compared to BRAM but are more flexible and can be used for smaller data
    sets or temporary storage within the FPGA.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configuration Memory: a specialized form of memory dedicated to storing the
    configuration data necessary to define the desired behavior of the FPGA. This
    memory contains the bitstream or configuration file, which is loaded into the
    FPGA upon startup. Configuration memory can be implemented using diverse technologies,
    such as SRAM-based or flash-based configurations, offering flexibility in how
    the FPGA is programmed and initialized.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Moreover, in order to meet the demands of emerging technologies and applications,
    FPGAs provide the designers with specialized macros, such as Digital Signal Processors
    (DSPs) and embedded multipliers, that can be exploited to enhance processing capabilities,
    improve power efficiency, and increase the flexibility of hardware accelerators
    for DL. The latter exploit FPGAs mostly to accelerate inference, while training
    is delegated to GPUs: this reflects the differences between the two phases, as
    training is only executed once and requires high throughput, while for inference,
    especially on edge devices, latency, and power consumption become critical ([guo2019dl,](#bib.bib98)
    ; [blaiech2019survey,](#bib.bib19) ). FPGAs are also often used as a prototyping
    platform to explore different architectures before committing to ASIC manufacturing
    ([Spagnolo2022_3,](#bib.bib251) ).'
  prefs: []
  type: TYPE_NORMAL
- en: A.4\. EDA Frameworks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementing hardware accelerators for ML algorithms, particularly DNNs, is
    a complex task that is rarely addressed through manual coding in low-level Hardware
    Description Languages (HDL). When Register Transfer Level (RTL) design is required
    to achieve high performance, templated components may be used ([Suda2016,](#bib.bib175)
    ). Instead, there are several electronic design automation (EDA) tools that bridge
    the gap between ML models and FPGAs/ASICs, allowing researchers to focus on developing
    the algorithms at a high level of abstraction ([venieris2018toolflows,](#bib.bib278)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Vitis AI, Xilinx’s development environment for AI inference ([VitisAI,](#bib.bib10)
    ), supports models developed in major frameworks such as PyTorch ([torch,](#bib.bib214)
    ), TensorFlow ([TF,](#bib.bib1) ) and Caffe ([Caffe,](#bib.bib125) ), and maps
    them on deep learning processor unit (DPU) cores present on modern Xilinx boards
    alongside the standard FPGA logic. The work in ([Ajili2022,](#bib.bib268) ) describes
    the implementation of DeepSense, a framework that includes CNN and RNN, with a
    focus on the choice of parameters to define DPUs used by Vitis AI; ([Vandendriessche2022,](#bib.bib274)
    ) performs a parametric study of the DPU architecture used by Vitis AI and examines
    the tradeoffs between the resources used and the clock frequency, as well as their
    impact on power consumption; ([Wang2021,](#bib.bib286) ) compares the FPGA implementation
    of YOLOv3 provided by Vitis AI with its GPU counterpart, showing higher throughput
    and lower power consumption; ([Ushiroyama2022,](#bib.bib272) ) evaluates the implementation
    of three different CNNs in terms of precision, power consumption, throughput,
    and design man-hours, and compares these figures with their GPU counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: High-Level Synthesis (HLS) plays a crucial role to automate the design of ML
    accelerators. HLS tools such as Vitis HLS ([vitishls,](#bib.bib294) ), Bambu ([Bambu,](#bib.bib74)
    ), Intel HLS Compiler ([IntelHLS2022,](#bib.bib120) ), Catapult ([CatapultHLS2022,](#bib.bib245)
    ), Stratus HLS ([StratusHLS2022,](#bib.bib27) ), or LegUp ([LegUpHLS2013,](#bib.bib28)
    ) provide users with a high level of abstraction where they can describe the desired
    functionality with a software programming language (C/C++/SystemC) and automatically
    obtain a corresponding high-performance HDL implementation. HLS thus boosts the
    productivity of hardware designers, who can benefit from faster design changes
    and functional verification. In fact, HLS allows generating accelerators for different
    platforms (e.g., larger or smaller FPGAs) without altering the C/C++ source code
    apart from a few design directives. This makes it possible to explore the design
    space and find the best implementation much faster than with HDL design. Note
    that code must be written with hardware knowledge in mind in order to meet given
    performance and resource usage results. Arbitrary software code, written for a
    CPU target, could achieve very low performance since it typically does not expose
    enough parallelism to exploit the spatial concurrency available on FPGA or ASIC.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also academic open-source HLS tools available, such as Bambu ([Bambu,](#bib.bib74)
    ). Bambu aids designers in the high-level synthesis of complex applications. It
    supports various C/C++ constructs and follows a software compilation-like flow.
    The tool consists of three phases: front-end, middle-end, and back-end. In the
    front end, the input code is parsed and translated into an intermediate representation.
    The middle-end performs target-independent analyses and optimizations. Lastly,
    the back-end synthesizes Verilog/VHDL code for simulation, logic synthesis, and
    implementation using external tools. Bambu offers a command-line interface and
    is particularly useful for designers seeking assistance in HLS and optimizing
    hardware designs efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to explore the acceleration of DNN inference on FPGAs, several frameworks
    and packages have been developed based on HLS. They can be divided into two categories:
    tools based on libraries of HLS templates, such as FINN ([FINN,](#bib.bib20) )
    and hls4ml ([hls4ml,](#bib.bib68) ), and tools that use a compiler-based approach,
    such as SODA ([sodaMICRO,](#bib.bib21) ) and ScaleHLS ([scalehls2022dac,](#bib.bib299)
    ). In ([Machura2022,](#bib.bib177) ), a comparison between a custom implementation
    of two DNNs written in SystemVerilog and an implementation using the Xilinx tools
    FINN and Vitis AI is presented; a comparison between FINN and Vitis AI is reported
    in ([Hamanaka2023,](#bib.bib102) ), where a ResNet model is implemented using
    a widely used set of configurations of FINN and Vitis AI. Both FINN and hls4ml
    use Vitis HLS as a backend; they parse a model exported from high-level ML frameworks
    and replace operators with C/C++ functions taken from a library of templates that
    already contains Vitis optimization directives. The HLS tool processes the C/C++
    code and produces a corresponding accelerator design. The library of templates
    is necessarily tied to a specific HLS tool, and it requires expert HLS developers
    to implement in advance the best version of all necessary ML operators for a pre-determined
    backend tool. On the other hand, SODA and ScaleHLS use a compiler infrastructure
    (MLIR, the Multi-Level Intermediate Representation from the LLVM project ([lattner2021mlir,](#bib.bib156)
    )) to progressively translate the input model through representations at different
    levels of abstraction, until they can be passed to the HLS tool as a C++ representation
    or an LLVM IR. This second approach exploits the existing MLIR infrastructure
    for machine learning, without requiring to create and maintain a library of operators.
    A hybrid RTL–HLS approach has been proposed in ([Guan2017,](#bib.bib96) ) to improve
    performance and development time for various DL algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: A.5\. Sparse Matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most famous definition of *sparse matrix* is attributed to James Wilkinson
    and dates back to more than 50 years ago ([davis2007,](#bib.bib58) ): any matrix
    with enough zeros that it pays to take advantage of them. A more recent and quantitative
    definition by Filippone et al. ([filippone2017sparse,](#bib.bib75) ) states that
    a matrix is *sparse* if its number of non-zero coefficients is $O(n)$, where $n$
    is the number of rows (columns) of the matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: Sparse matrices are usually stored in a compressed format to avoid redundant
    storage as well as a lot of useless calculations. That is, the storage format
    attempts to take advantage of the zeros by avoiding their explicit storage. The
    counterpart is that the traditional simple mapping between the index pair of each
    matrix coefficient and the position of the coefficient in memory is destroyed.
    Therefore, all sparse matrix storage formats are devised around rebuilding this
    mapping by using some auxiliary index information. This rebuilding has a non-negligible
    cost and impact on the matrix operations to be performed. Therefore, the performance
    of sparse matrix computations depends on the selected storage format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Widely used sparse matrix storage formats include COOrdinate (COO), Compressed
    Sparse Rows (CSR), and Compressed Sparse Columns (CSC) ([filippone2017sparse,](#bib.bib75)
    ). CSR is perhaps the most popular sparse matrix representation: It compresses
    the sparse matrix into three different arrays. The first array represents the
    non-zero values, the second contains the column indexes, and the third marks the
    boundaries of each row in the matrix. The above formats can be considered general-purpose,
    meaning that they can be used on most hardware with little or no changes. However,
    hardware-oriented formats become attractive when moving to special computing architectures
    such as accelerators. Many storage formats, such as ELLPACK, are specifically
    developed for vector machines to introduce a certain degree of regularity in the
    data structure to enable the efficient exploitation of vector instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: A factor that can drive the choice of the storage format is the sparsity pattern
    of the matrix that is, the pattern of non-zero entries contained in the matrix.
    Common sparsity patterns include unstructured (where nonzeros are randomly and
    irregularly scattered), diagonal (where nonzeros are restricted to a small number
    of matrix diagonals), and block sparse (either coarse-grain or fine-grain). Each
    of these sparsity patterns is best addressed using different formats. For instance,
    the diagonal format (DIA) is an appropriate representation for diagonal matrices.
  prefs: []
  type: TYPE_NORMAL
- en: DNN models are composed of large, dense matrices which are typically used in
    matrix multiplication and convolutions. In the last years, state-of-the-art DL
    models have dramatically increased in size, with hundreds of billions of parameters
    (e.g., large language models as GPT-3 require 175B parameters ([brown:2020,](#bib.bib25)
    )) and trillions of compute operations per input sample. In order to reduce DNN
    model sizes and computation requirements (including the energy footprint), pruning
    (i.e., setting to zero) of DNN weights has emerged as a particularly effective
    and promising technique. Pruning entails identifying unnecessary redundancy in
    DNN-trained model weights and zero out these nonessential weights ([han:nips2015,](#bib.bib105)
    ; [srinivas:2015,](#bib.bib255) ), thus allowing to discard of zero values from
    storage and computations. Therefore, pruning induces sparsity in the DL model,
    in which a large proportion (typically between 50%([park:arxiv2016,](#bib.bib209)
    ) to 90% ([hao:arxiv2015,](#bib.bib104) )) of the weights are zero. Pruning methods
    allow keeping model accuracy with little loss in model quality, thus achieving
    the same expressive power as dense model counterparts, while leading to models
    that are more efficient in terms of computing and storage resources demand.
  prefs: []
  type: TYPE_NORMAL
- en: The second factor that induces sparsity in DNN models is the ReLU (rectified
    linear unit) operator, which is frequently used as an activation function. Indeed,
    ReLU resets all the negative values in the matrices of the activations¹¹1The activations
    are the output values of an individual layer that are passed as inputs to the
    next layer. to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Because of network pruning and zero-valued activations, sparsity has become
    an active area of research in DNN models. These two techniques allow to reduce
    both the memory size and the memory accesses, the latter thanks to the removal
    of useless operations (i.e., multiply by zero), which also save processing power
    and energy consumption. As regards the memory size, the number of non-zero entries
    in the resulting sparse matrices can be reduced to 20-80% and 50-70% for weights
    and activations, respectively ([han:nips2015,](#bib.bib105) ; [SCNN:isca2017,](#bib.bib207)
    ). Sparse matrices can thus be stored using a compressed representation, thus
    leading to at least 2-3x memory size reduction. However, the main disadvantage
    of a sparse matrix is that the indexes become relative, which adds extra levels
    of indirection that add complexity and need to be carefully managed to avoid inefficiency.
  prefs: []
  type: TYPE_NORMAL
- en: As regards the sparsity pattern of DNN models, it can range from unstructured
    as a result of fine-grain pruning, which maintains model accuracy, to structured
    when coarse-grain pruning is applied to improve execution efficiency at the cost
    of downgrading the model accuracy ([han:nips2015,](#bib.bib105) ; [wen:nisp2016,](#bib.bib293)
    ). Randomly distributed nonzeros can lead to irregular memory accesses, that are
    unfriendly on commodity architectures, e.g., GPU, as well as to irregular computations
    that introduce conditional branches to utilize the sparsity. The latter are hardly
    applicable for accelerators, which are designed for fine-grained data or thread
    parallelism rather than flexible data path control. On the other hand, hardware-friendly
    structured sparsity can efficiently accelerate the DNN evaluation at the cost
    of model accuracy degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, sparsity is becoming ubiquitous in modern deep learning workloads
    (i.e., not only because of the application of compression techniques such as network
    pruning and zero-valued activations) due to the application of deep learning to
    graphs for modeling relations (in social networks, proteins, etc.) using highly
    sparse matrices, such as in Graph Neural Networks (GNNs).
  prefs: []
  type: TYPE_NORMAL
- en: The key computational kernel within most DL workloads is general matrix-matrix
    multiplications (GEMM) ([gao2023acm,](#bib.bib82) ). It appears frequently during
    both the forward pass (inference and training) and backward pass (training); for
    instance, experiments reported in ([sigma:2020,](#bib.bib226) ) show that GEMM
    comprises around 70% of the total compute cycles during training for Transformer
    and Google Neural Machine Translation workloads. Therefore, GEMM represents a
    primary target for hardware acceleration in order to speed up training and inference.
    However, GEMM in DL is characterized by sparsity of matrices, which arises from
    pruning as explained above, and non-square matrix dimensions, which arise from
    mini-batches and weight factorization ([park-facebook:arxiv2018,](#bib.bib210)
    ). A popular computational kernel for convolutional neural networks (CNNs) is
    a sparse vector-vector dot product. Sparse-dense matrix multiplication (SpMM)
    and sampled-dense matrix multiplication (SDDMM) are two of the most generic kernels
    in GNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial-architecture-based hardware accelerators that exploit sparsity have
    different architectures that allow to adapt the computation to sparse matrices.
    In the following, we review their main features.
  prefs: []
  type: TYPE_NORMAL
- en: A.6\. Emerging 3D-stacked Processing-in-memory Technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3D integration technologies ([Kada2015,](#bib.bib137) ) enable to stack as many
    as 16 or more 2D integrated circuits and interconnect them vertically using, for
    instance, through-silicon vias (TSVs), micro bumps, or Cu-Cu connections. In this
    way, a 3D circuit behaves as a single device achieving a smaller area footprint
    than conventional 2D circuits, while reducing power and latency in data transfer.
    In general, 3D integration is a term that includes such technologies as 3D wafer-level
    packaging (3DWLP) ([Soussan2008,](#bib.bib250) ), 2.5D and 3D interposer-based
    integration ([Lau2011,](#bib.bib52) ), 3D stacked ICs (3D-SICs), 3D heterogeneous
    integration, and 3D systems integration, as well as true monolithic 3D ICs ([Knechtel2017,](#bib.bib146)
    ; [Mathur2021,](#bib.bib180) ).
  prefs: []
  type: TYPE_NORMAL
- en: These technologies have been employed in the development of new memory devices,
    which stack layers of conventional 2D DRAM or other memory types (for instance,
    Non-Volatile Memory (NVM) based on ReRAM ([Clarke2015,](#bib.bib50) )) together
    with one or more optional layers of logic circuits. These logic layers are often
    implemented with different process technology and can include buffer circuitry,
    test logic, and processing elements. Compared to 2D memories, 3D stacking increases
    memory capacity and bandwidth, reduces access latency due to the shorter on-chip
    wiring interconnection and the use of wider buses, and potentially improves the
    performance and power efficiency of the system. In fact, 3D stacking of DRAM memory
    provides an order of magnitude higher bandwidth and up to $5\times$ better energy
    efficiency than conventional 2D solutions, making the technology an excellent
    option for meeting the requirements in terms of high throughput and low energy
    of DNN accelerators ([Hassanpour2022,](#bib.bib108) ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Two main 3D stacked memory standards have been recently proposed: the Hybrid
    Memory Cube (HMC) and the High Bandwidth Memory (HBM). 3D stacked processing-in-memory
    accelerator proposals modify the architecture of the 3D memory block inserting
    processing logic near the memory elements. Two approaches can be commonly found.
    In the first approach, the computing logic is embedded into the logic die (logic
    die-level processing-in-memory); in the second approach, the processing logic
    is integrated into each DRAM die at the level of memory banks, after the column
    decoder and selector blocks (bank-level processing-in-memory). We present in the
    following subsections the main characteristics of the two 3D stacked memory standards
    and overview the existing accelerators adopting them.'
  prefs: []
  type: TYPE_NORMAL
- en: A.6.1\. Hybrid Memory Cube
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Hybrid Memory Cube is a single package containing four to eight DRAM die
    and one logic die, all stacked together using thousands of TSVs, achieving a much
    desired high memory bandwidth ([Jeddeloh2012,](#bib.bib122) ; [HMC2014,](#bib.bib117)
    ). As shown in Figure [12(a)](#A1.F12.sf1 "In Figure 12 ‣ A.6.1\. Hybrid Memory
    Cube ‣ A.6\. Emerging 3D-stacked Processing-in-memory Technologies ‣ Appendix
    A Appendix ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms"), in a HMC, memory is organized vertically, and portions of each
    memory die are combined with the corresponding portions of the other memory dies
    and the logic die. This creates a 2D grid of vertical partitions, referred as
    vaults ([Pawlowski2011,](#bib.bib216) ). Each vault is functionally and operationally
    independent and includes in the logic layer a memory controller that manages all
    memory reference operations within that vault, as well as determining timing requirements
    and dealing with refresh operations, eliminating these functions from the host
    memory controller. The independence of each vault allows to exploit memory level
    parallelism, as multiple partitions in the DRAM dies can be accessed simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Commands and data are transmitted from and to the host across external I/O links
    consisting of up to four serial links, each with a default of 16 input lanes and
    16 output lanes for full duplex operation (HMC2 specifications ([HMC2014,](#bib.bib117)
    )). All in-band communication across a link is packetized. According to specifications,
    up to 320 GB/s effective bandwidth can be achieved by considering 30 Gb/s SerDes
    I/O interfaces, with a storage capacity, depending on the number of stacked layers,
    of 4GB and 8GB ([Micron2018,](#bib.bib184) ; [HMC2014,](#bib.bib117) ).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ef05c738b121efc555df73560b4756a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d4d072174d468e295c811693bb3b8feb.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12. (a) High-level architecture of the Hybrid Memory Cube. (b) Cut through
    image of a computing system with HBM.
  prefs: []
  type: TYPE_NORMAL
- en: A.6.2\. High Bandwidth Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The High Bandwidth Memory is a high-speed computer memory interface for 3D-stacked
    synchronous dynamic random-access memory (SDRAM) ([Joonyoung2014,](#bib.bib144)
    ; [Sohn2017,](#bib.bib247) ). Each memory module is composed by stacking up to
    eight DRAM dies and an optional base die including buffer circuitry and test logic.
    Dies are vertically interconnected by TSVs and microbumps, in a way similar to
    the HMC. As shown in Figure [12(b)](#A1.F12.sf2 "In Figure 12 ‣ A.6.1\. Hybrid
    Memory Cube ‣ A.6\. Emerging 3D-stacked Processing-in-memory Technologies ‣ Appendix
    A Appendix ‣ A Survey on Deep Learning Hardware Accelerators for Heterogeneous
    HPC Platforms"), the memory stack is often connected to the memory controller
    on the host (e.g., GPU or CPU) through purpose-built silicon chip, called interposer
    ([Lau2011,](#bib.bib52) ), which is effectively a miniature PCB that goes inside
    the package and decreases the memory paths by allowing the host and the memory
    to be physically close. However, as semiconductor device fabrication is significantly
    more expensive than printed circuit board manufacture, this adds cost to the final
    product. Alternatively, the memory die can be stacked directly on the host processor
    chip.
  prefs: []
  type: TYPE_NORMAL
- en: The HBM DRAM is tightly coupled to the host computer through a distributed interface,
    which is divided into independent channels. The HBM DRAM uses a wide-interface
    architecture to achieve high-speed, low-power operation. Each channel interface
    maintains a 128-bit (HMB2) or 64-bit (HMB3) data bus operating at double data
    rate (DDR). The latest version (2022) of the HBM (HBM3) supports up to 16 channels
    of 64 bits, with a total number of data pins equal to 1024, and with an overall
    package bandwidth of 600 GB/s ([JEDEC2023,](#bib.bib123) ; [Prickett2022,](#bib.bib223)
    ). Depending on the producer, the HBM stack consists of 8 or 12 16Gb DRAMs, with
    a total maximum memory capacity of 24 GB ([Robinson2022,](#bib.bib233) ).
  prefs: []
  type: TYPE_NORMAL
- en: 'A.6.3\. 3D stacked PIM: Accelerating applications loosely related to DNNs'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to the DNN accelerators described in Section 5.2, we briefly overview
    3D stacked accelerators for applications loosely related to DNNs. The use of PEs
    in the logic layer of an HMC is discussed in ([Oliveira2017,](#bib.bib203) ),
    to support the simulation of large networks on neurons. The proposed Neuron In-Memory
    (NIM) architecture is composed of 2,048 functional units, operating on integer
    and floating-point data, and a small register file with 8 × 16 registers of 32
    bits each per vault. Fast vector elements operation is also supported. When compared
    with traditional multi-core environments, NIM provides overall system acceleration
    and reduces overall energy consumption, taking advantages of the broad bandwidth
    available in 3D-stacked memory devices.
  prefs: []
  type: TYPE_NORMAL
- en: Millipede ([Nitin2018,](#bib.bib195) ) is an NDP architecture for Big data Machine
    Learning Analytics (BMLA) that implements its processors in the logic layer of
    3D-stacked memories. These processors have a local memory, register file, pipeline,
    cache, and prefetch buffers.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in ([Mathur2021,](#bib.bib180) ) explore the design trade-offs and
    thermal implications of 3D stacking in different configurations layers of SRAM
    buffers and systolic accelerators composed of MAC elements, while targeting Deep
    learning applications. The main memory (DRAM) is however not necessarily stacked
    with the rest of the system. Their simulations show that stacking PE array on
    top of the SRAM stack in a logic-over-memory fashion can not only achieve low
    energy but also mitigate the thermal impact of 3-D.
  prefs: []
  type: TYPE_NORMAL
- en: iPIM ([Gu2020,](#bib.bib95) ) uses a near-bank architecture for image processing.
    The control and the execution are decoupled to obtain a higher bank-level bandwidth
    and maximize the parallel execution of processing engines on the memory dies.
    Each vault contains a control core in the logic die, while the execution logic
    is placed in the memory die of each vault. Each control core manages intra/inter-vault
    data communication and executes instruction decoding with the support of the single-instruction-multiple-bank
    (SIMB) instruction set architecture (ISA).
  prefs: []
  type: TYPE_NORMAL
- en: Neurosensor ([Amir2018,](#bib.bib11) ) is a 3D CMOS image sensor system with
    an integrated convolutional neural network computation. The image sensor, read-out
    circuits, memory, and neural computation logic layers are integrated in a single
    stack. The DNN computation platform is an adaptation from Neurocube ([Kim2016,](#bib.bib142)
    ), and consists of a global controller, processing elements, a 2D mesh NoC connecting
    the PEs, and a programmable neurosequence generator for DRAM. The DNN computation
    is split between the sensor and the host, and the optimal task distribution depends
    on the processing capabilities of the sensor, the available amount of in-sensor
    memory for storing the synaptic weights, and the available bandwidth between the
    sensor and the host.
  prefs: []
  type: TYPE_NORMAL
- en: 'A.6.4\. 3D stacked PIM: Some considerations'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, the amount of processing elements that can be integrated into 3D stacked
    memories is limited by the size of the package. Moreover, the overall power dissipation
    of these elements is limited by thermal issues of 3D stacking, as an increase
    in the operation temperature would result in performance degradation from overheating
    ([Heyman2022,](#bib.bib111) ). Second, the stacking of multiple IC layers has
    a high manufacturing complexity, which leads to lower yield and problematic testability.
    Therefore, in order to support the adoption of this technology, proper cooling
    methods and better manufacturing solutions are required.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the above-mentioned technological challenges, embedding processing
    elements into the memory and moving the computation closer to it requires to rethink
    of the optimization of the system design in order to take into account the proximity
    of the processing logic to the main memory. Depending on the use case, this might
    involve the redesign of the on-chip buffers in the logic die, to support the lower
    latency and energy cost of the accesses to main memory, as well as the use of
    new approaches for representing, partitioning, and mapping the dataflow of the
    application in order to exploit the highly parallel system supported by the availability
    of multiple channels ([Hassanpour2022,](#bib.bib108) ).
  prefs: []
  type: TYPE_NORMAL
- en: A.7\. RRAM and PCM Technologies for IMC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides conventional CMOS designs, emerging non-volatile memories such as the
    RRAM and the PCM have been recently explored for integration in stand-alone DNN
    accelerators. The RRAM device structure (see Figure [13](#A1.F13 "Figure 13 ‣
    A.7\. RRAM and PCM Technologies for IMC ‣ Appendix A Appendix ‣ A Survey on Deep
    Learning Hardware Accelerators for Heterogeneous HPC Platforms")a) is a metal-insulator-metal
    (MIM) structure that consists of a top electrode (TE), a bottom electrode (BE),
    and a metal-oxide layer sandwiched between them. By applying a proper voltage
    across the electrodes and setting the maximum current flowing in the MIM stack
    (through a series transistor), an RRAM cell can modulate the shape of a conductive
    filament created in the metal-oxide layer. In PCM the active material is a chalcogenide
    phase change material, which can remain in either crystalline or amorphous states
    for long periods of time at moderately high temperatures. Starting from the amorphous
    state, the application of voltage pulses with relatively low amplitude causes
    the crystallization induced by Joule heating, whereas the application of pulses
    at higher amplitudes can lead to local melting and consequent amorphization. A
    typical PCM cell has a mushroom shape shown in Figure [13](#A1.F13 "Figure 13
    ‣ A.7\. RRAM and PCM Technologies for IMC ‣ Appendix A Appendix ‣ A Survey on
    Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")a, where
    the pillar-like bottom electrode confines heat and current, thus resulting in
    a hemispherical shape of the molten material. In both technologies, their resistance
    state can be tuned not only as a digital memory but also as a continuous analog
    memory with multiple states to perform in-memory computing ([ielmini2018nature,](#bib.bib118)
    ). This characteristic allows efficient matrix-vector multiplication when RRAM
    and PCM are arranged in crossbar structures (see Figure [13](#A1.F13 "Figure 13
    ‣ A.7\. RRAM and PCM Technologies for IMC ‣ Appendix A Appendix ‣ A Survey on
    Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms")b).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24f13a977ce342577b541310eb32cadf.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13. (a) RRAM and PCM devices structure and (b) their arrangement in a
    crossbar structure for matrix-vector multiplication. (c) Example of a stand-alone
    DNN accelerator (i.e., PRIME ([chi2016isca,](#bib.bib45) )) using RRAM crossbars
    for in situ MAC operations. Reprinted from ([lepri2023jeds,](#bib.bib163) ) and
    ([chen2020engineering,](#bib.bib40) ) under Creative Commons License.
  prefs: []
  type: TYPE_NORMAL
- en: A.8\. Alternative Integration Technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The semiconductor industry has grown significantly as a result of increased
    integration complexity, resulting in improved performance and cost-effectiveness
    of transistors. Unfortunately, the trend of increasing the number of transistors
    per die is slowing down, leading to a power-efficiency-driven design era known
    as *dark silicon* ([esmaeilzadeh_isca11,](#bib.bib71) ). While the number of transistors
    per die continues to increase, many foundries are struggling to achieve the targeted
    area scaling per transistor, and new process technologies are expected to slow
    down. The cost per transistor may no longer hold, resulting in yield challenges
    and additional wafer costs. Circuit designers and computer architects can no longer
    rely on the free availability of additional transistors and integration opportunities
    with each new process node, and non-recurring engineering costs have also increased
    due to fabrication and system complexity challenges ([shalf_ptrs19,](#bib.bib130)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternative integration technologies can provide cost reductions and increase
    the number of transistors per circuit. These technologies include die-level integration
    such as 3D die stacking with connections through micro-bumps or Through-Silicon
    Vias (TSVs) ([hu_micro18,](#bib.bib114) ), or through interposer-based 2.5D integration ([zhang_apr15,](#bib.bib306)
    ). By partitioning a monolithic SoC across multiple small dies, namely *chiplets*,
    (see Figure [14(a)](#A1.F14.sf1 "In Figure 14 ‣ A.8\. Alternative Integration
    Technologies ‣ Appendix A Appendix ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms")), yield per die can be improved and metal layer
    count can be reduced, which can lead to a lower total IC cost ([stow_iccad16,](#bib.bib257)
    ). In fact, larger chips cost more due to two main factors: geometry and manufacturing
    defects. Fewer larger chips can fit in a wafer, while defects in larger chips
    waste more silicon than defects in smaller chips ([ajaykumar_micro15,](#bib.bib138)
    ). Smaller chips can be packed more tightly, resulting in more chips that work.
    In general, making smaller chips results in a higher yield of functioning chips
    (see Figure [14(b)](#A1.F14.sf2 "In Figure 14 ‣ A.8\. Alternative Integration
    Technologies ‣ Appendix A Appendix ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/154ef7af12776ea2eda5d522df665c2a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1fabdd942b755fc2afde5fae758184a3.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14. Die-level integration through TSV-based 3D and interposer-based 2.5D
    technologies [14(a)](#A1.F14.sf1 "In Figure 14 ‣ A.8\. Alternative Integration
    Technologies ‣ Appendix A Appendix ‣ A Survey on Deep Learning Hardware Accelerators
    for Heterogeneous HPC Platforms") ([stow_iccad16,](#bib.bib257) ) and overall
    number of chips and impact on yield of an example defect distribution for two
    different chip sizes ([ajaykumar_micro15,](#bib.bib138) )
  prefs: []
  type: TYPE_NORMAL
- en: Die-level integration provides innovative integration strategies, like heterogeneous
    process integration between dies that can improve performance and reduce costs ([zhang_apr15,](#bib.bib306)
    ). Additionally, this technology can be exploited for the reuse of intellectual
    property to configure SoCs with different die combinations and reduce non-recurring
    overheads.
  prefs: []
  type: TYPE_NORMAL
- en: In multichip-module (MCM) silicon interposer-based integration, the interposer
    uses micro-bumps to connect the smaller chips, which have a higher density than
    traditional C4 bumps. The impedance across the interposer is the same as conventional
    on-chip interconnects. The only downside is the additional cost of the interposer.
    Vertical 3D chip stacking involves combining multiple chips with through-silicon
    vias (TSVs) for vertical interconnects. This technique has the potential to offer
    the highest bandwidth but it requires significant cost and overall process complexity
    as each die must be thinned and processed for TSVs. Overall, as 3D stacking is
    more expensive and complex, while also potentially causing thermal issues, we
    focus on MCM silicon interposer-based design in the following.
  prefs: []
  type: TYPE_NORMAL
- en: A.8.1\. MCM Silicon Interposer-based Design
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In 2.5D integration technology, an interposer is a substrate that connects
    multiple dies (chiplets) together. There are two types of interposers: passive
    interposers and active interposers ([stow_iccad17,](#bib.bib258) ). Passive interposers
    are simple substrates that connect multiple dies together without adding any active
    components. They mainly provide electrical connections, signal routing, and thermal
    management between the dies. On the other hand, active interposers contain active
    components such as transistors, capacitors, and inductors, in addition to the
    electrical connections and signal routing provided by passive interposers. Active
    interposers can perform some processing and signal conditioning functions between
    the dies.'
  prefs: []
  type: TYPE_NORMAL
- en: A.8.2\. General Purpose Chiplet-based Architectures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Chiplet-based designs are being utilized across a wide range of platforms, tailored
    to support various application contexts. The challenges of creating integrated
    SoCs for aerospace platforms in advanced semiconductor nodes are reported in ([mounce_ac16,](#bib.bib190)
    ) in which authors highlight the possibility of creating heterogeneous mixtures
    of chiplets, including different embodiments of processors, ultradense memory
    servers, field-programmable gate array clusters, and configurable analog and radiofrequency
    functional blocks. Further, some of the features necessary to support scalability
    and heterogeneity with multi-domain, hybrid architectures involving a mixture
    of semiconductor technologies and advanced packaging approaches are also outlined.
    In ([vijayaraghavan_hpca17,](#bib.bib282) ) a chiplet-based computing system for
    climate prediction is presented. It integrates a high-throughput and energy-efficient
    GPU chiplet, high-performance multi-core CPU chiplet, and large-capacity 3D memory.
    The system can achieve a bandwidth of 3 TB/s and power consumption of 160 W at
    1 GHz.
  prefs: []
  type: TYPE_NORMAL
- en: GPU platforms are also benefiting from chiplet-based integration. In ([arunkumar_isca17,](#bib.bib14)
    ) a single-chip multi-core GPU is broken down into multiple GPU chiplets to improve
    both performance and energy efficiency by increasing hardware resource utilization
    for both the GPU and DRAM chiplets, while also mitigating the dark silicon effect.
    Additionally, breaking the larger GPU into multiple smaller chiplets has resulted
    in improved wafer yield.
  prefs: []
  type: TYPE_NORMAL
- en: The design and implementation of a dual-chiplet Chip-on-Wafer-on-Substrate are
    presented in ([lin_vlsi19,](#bib.bib169) ), where each chiplet has four ARM Cortex-A72
    processors operating at 4 GHz. The on-die interconnect mesh bus operates above
    4 GHz at a 2 mm distance and the inter-chiplet connection features a scalable,
    power-efficient, high-bandwidth interface achieving 8 Gb/s/pin and 320 GB/s bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: The above work uses 2.5D integration technology based on a passive interposer.
    In ([vivet_jssc21,](#bib.bib283) ), the authors observe that current passive interposer
    solutions still lack flexibility and efficiency when it comes to long-distance
    communication, smooth integration of chiplets with incompatible interfaces, and
    easy integration of less-scalable analog functions, such as power management and
    system input/output signals (IOs). Thus, they present a CMOS Active Interposer
    that integrates power management and distributed interconnects, enabling a scalable
    cache-coherent memory hierarchy. The proposed platform integrates six chiplets
    onto the active interposer, offering a total of 96 cores.
  prefs: []
  type: TYPE_NORMAL
- en: The exploitation of active interposers as a way to address energy-efficiency
    and computing density issues in high-performance computing (HPC) for Exascale
    architectures is discussed in ([martinez_vlsi20,](#bib.bib179) ). The authors
    suggest that the integration of chiplets, active interposers, and FPGA can lead
    to dense, efficient, and modular computing nodes. They detail the ExaNoDe multi-chip-module
    which combines various components and demonstrate that multi-level integration
    allows for tight integration of hardware accelerators in a heterogeneous HPC compute
    node.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This work has been supported by the Spoke 1 on *Future HPC* of the Italian Research
    Center on High-Performance Computing, Big Data and Quantum Computing (ICSC) funded
    by MUR Mission 4 - Next Generation EU.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '(1) Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
    G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,
    Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,
    J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens,
    J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan,
    V., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and
    Zheng, X. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, 2015.
    Software available from tensorflow.org.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2) Agrawal, A., Lee, S. K., Silberman, J., Ziegler, M., Kang, M., Venkataramani,
    S., Cao, N., Fleischer, B., Guillorn, M., Cohen, M., et al. 9.1 A 7nm 4-core AI
    chip with 25.6 TFLOPS hybrid FP8 training, 102.4 TOPS INT4 inference and workload-aware
    throttling. In 2021 IEEE International Solid-State Circuits Conference (ISSCC)
    (Feb. 2021), vol. 64, IEEE, pp. 144–146.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (3) Ahmadinejad, M., Moaiyeri, M. H., and Sabetzadeh, F. Energy and area efficient
    imprecise compressors for approximate multiplication at nanoscale. AEU - International
    Journal of Electronics and Communications 110 (2019), 152859.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(4) Aimar, A., Mostafa, H., Calabrese, E., Rios-Navarro, A., Tapiador-Morales,
    R., Lungu, I.-A., Milde, M. B., Corradi, F., Linares-Barranco, A., Liu, S.-C.,
    and Delbruck, T. NullHop: A Flexible Convolutional Neural Network Accelerator
    Based on Sparse Representations of Feature Maps. IEEE Transactions on Neural Networks
    and Learning Systems 30, 3 (2019), 644–656.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(5) Akopyan, F., Sawada, J., Cassidy, A., Alvarez-Icaza, R., Arthur, J., Merolla,
    P., Imam, N., Nakamura, Y., Datta, P., Nam, G.-J., Taba, B., Beakes, M., Brezzo,
    B., Kuang, J. B., Manohar, R., Risk, W. P., Jackson, B., and Modha, D. S. TrueNorth:
    Design and Tool Flow of a 65 mW 1 Million Neuron Programmable Neurosynaptic Chip.
    IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
    34, 10 (2015), 1537–1557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(6) Albericio, J., Judd, P., Hetherington, T., Aamodt, T., Jerger, N. E., and
    Moshovos, A. Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing.
    In Proceedings of the 43rd International Symposium on Computer Architecture (2016),
    ISCA’16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (7) Alves, M. A. Z., Diener, M., Santos, P. C., and Carro, L. Large vector extensions
    inside the HMC. In 2016 Design, Automation & Test in Europe Conference & Exhibition
    (DATE) (2016), pp. 1249–1254.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(8) Ambs, P. Optical Computing: A 60-Year Adventure. Advances in Optical Technologies
    2010, 372652 (May 2010).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (9) AMD. AMD Instinct MI200 series accelerator, Jan 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (10) AMD-Xilinx. VitisAI develop environment, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (11) Amir, M. F., Ko, J. H., Na, T., Kim, D., and Mukhopadhyay, S. 3-D Stacked
    Image Sensor With Deep Neural Network Computation. IEEE Sensors Journal 18, 10
    (2018), 4187–4199.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (12) Andersch, M., Palmer, G., Krashinsky, R., Stam, N., Mehta, V., Brito, G.,
    and Ramaswamy, S. NVIDIA Hopper Architecture In-Depth, Mar 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (13) Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A.,
    Shakeri, S., Taropa, E., Bailey, P., Chen, Z., Chu, E., Clark, J. H., Shafey,
    L. E., Huang, Y., Meier-Hellstern, K., Mishra, G., Moreira, E., Omernick, M.,
    Robinson, K., Ruder, S., Tay, Y., Xiao, K., Xu, Y., Zhang, Y., Abrego, G. H.,
    Ahn, J., Austin, J., Barham, P., Botha, J., Bradbury, J., Brahma, S., Brooks,
    K., Catasta, M., Cheng, Y., Cherry, C., Choquette-Choo, C. A., Chowdhery, A.,
    Crepy, C., Dave, S., Dehghani, M., Dev, S., Devlin, J., Díaz, M., Du, N., Dyer,
    E., Feinberg, V., Feng, F., Fienber, V., Freitag, M., Garcia, X., Gehrmann, S.,
    Gonzalez, L., Gur-Ari, G., Hand, S., Hashemi, H., Hou, L., Howland, J., Hu, A.,
    Hui, J., Hurwitz, J., Isard, M., Ittycheriah, A., Jagielski, M., Jia, W., Kenealy,
    K., Krikun, M., Kudugunta, S., Lan, C., Lee, K., Lee, B., Li, E., Li, M., Li,
    W., Li, Y., Li, J., Lim, H., Lin, H., Liu, Z., Liu, F., Maggioni, M., Mahendru,
    A., Maynez, J., Misra, V., Moussalem, M., Nado, Z., Nham, J., Ni, E., Nystrom,
    A., Parrish, A., Pellat, M., Polacek, M., Polozov, A., Pope, R., Qiao, S., Reif,
    E., Richter, B., Riley, P., Ros, A. C., Roy, A., Saeta, B., Samuel, R., Shelby,
    R., Slone, A., Smilkov, D., So, D. R., Sohn, D., Tokumine, S., Valter, D., Vasudevan,
    V., Vodrahalli, K., Wang, X., Wang, P., Wang, Z., Wang, T., Wieting, J., Wu, Y.,
    Xu, K., Xu, Y., Xue, L., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng, C., Zhou,
    W., Zhou, D., Petrov, S., and Wu, Y. PaLM 2 Technical Report, May 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(14) Arunkumar, A., Bolotin, E., Cho, B., Milic, U., Ebrahimi, E., Villa, O.,
    Jaleel, A., Wu, C.-J., and Nellans, D. MCM-GPU: Multi-chip-module GPUs for continued
    performance scalability. In 2017 ACM/IEEE 44th Annual International Symposium
    on Computer Architecture (ISCA) (2017), pp. 320–332.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(15) Assir, I. A., Iskandarani, M. E., Sandid, H. R. A., and Saghir, M. A. R.
    Arrow: A RISC-V Vector Accelerator for Machine Learning Inference. CoRR abs/2107.07169,
    arXiv:2107.07169 (July 2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (16) Bengio, Y. Learning Deep Architectures for AI. Foundations and Trends in
    Machine Learning 2, 1 (2009), 1–127.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(17) Bertaccini, L., Paulin, G., Fischer, T., Mach, S., and Benini, L. MiniFloat-NN
    and ExSdotp: An ISA Extension and a Modular Open Hardware Unit for Low-Precision
    Training on RISC-V Cores. In 2022 IEEE 29th Symposium on Computer Arithmetic (ARITH)
    (Sept. 2022), pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (18) Biamonte, J., Wittek, P., Pancotti, N., Rebentrost, P., Wiebe, N., and
    Lloyd, S. Quantum machine learning. Nature 549, 7671 (sep 2017), 195–202.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (19) Blaiech, A. G., Khalifa, K. B., Valderrama, C., Fernandes, M. A., and Bedoui,
    M. H. A survey and taxonomy of FPGA-based deep learning accelerators. Journal
    of Systems Architecture 98 (2019), 331–345.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(20) Blott, M., Preußer, T. B., Fraser, N. J., Gambardella, G., O’brien, K.,
    Umuroglu, Y., et al. FINN-R: An end-to-end deep-learning framework for fast exploration
    of quantized neural networks. ACM Transactions on Reconfigurable Technology and
    Systems 11, 3 (2018), 1–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(21) Bohm Agostini, N., Curzel, S., Zhang, J. J., Limaye, A., Tan, C., Amatya,
    V., Minutoli, M., Castellana, V. G., Manzano, J., Brooks, D., Wei, G.-Y., and
    Tumeo, A. Bridging Python to Silicon: The SODA Toolchain. IEEE Micro 42, 5 (2022),
    78–88.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (22) Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx,
    S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E.,
    Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis,
    J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy,
    J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K.,
    Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J.,
    Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri,
    P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M.,
    Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec,
    J., Levent, I., Li, X. L., Li, X., Ma, T., Malik, A., Manning, C. D., Mirchandani,
    S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman,
    B., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou,
    I., Park, J. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich,
    R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa,
    S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W.,
    Tramèr, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga,
    M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L.,
    Zhou, K., and Liang, P. On the Opportunities and Risks of Foundation Models, July
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (23) Quantum Computing Service - Amazon Braket - AWS, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(24) Broughton, M., Verdon, G., McCourt, T., Martinez, A. J., Yoo, J. H., Isakov,
    S. V., Massey, P., Halavati, R., Niu, M. Y., Zlokapa, A., Peters, E., Lockwood,
    O., Skolik, A., Jerbi, S., Dunjko, V., Leib, M., Streif, M., Dollen, D. V., Chen,
    H., Cao, S., Wiersema, R., Huang, H.-Y., McClean, J. R., Babbush, R., Boixo, S.,
    Bacon, D., Ho, A. K., Neven, H., and Mohseni, M. TensorFlow Quantum: A Software
    Framework for Quantum Machine Learning, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (25) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J.,
    Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B.,
    Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei,
    D. Language Models are Few-Shot Learners. CoRR abs/2005.14165 (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (26) Bruschi, N., Tagliavini, G., Garofalo, A., Conti, F., Boybat, I., Benini,
    L., and Rossi, D. End-to-end DNN inference on a massively parallel analog in memory
    computing architecture. In Design, Automation & Test in Europe Conference & Exhibition,
    DATE (2023), IEEE, pp. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (27) Cadence. Stratus High-Level Synthesis, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(28) Canis, A., Choi, J., Aldham, M., Zhang, V., Kammoona, A., Czajkowski,
    T. S., Brown, S. D., and Anderson, J. H. LegUp: An open-source high-level synthesis
    tool for FPGA-based processor/accelerator systems. ACM Trans. Embed. Comput. Syst.
    13, 2 (2013), 24:1–24:27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(29) Cao, W., Zhao, Y., Boloor, A., Han, Y., Zhang, X., and Jiang, L. Neural-PIM:
    Efficient Processing-In-Memory With Neural Approximation of Peripherals. IEEE
    Transactions on Computers 71, 9 (2022), 2142–2155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(30) Carsello, A., Feng, K., Kong, T., Koul, K., Liu, Q., Melchert, J., Nyengele,
    G., Strange, M., Zhang, K., Nayak, A., Setter, J., Thomas, J., Sreedhar, K., Chen,
    P.-H., Bhagdikar, N., Myers, Z., D’Agostino, B., Joshi, P., Richardson, S., Bahr,
    R., Torng, C., Horowitz, M., and Raina, P. Amber: A 367 GOPS, 538 GOPS/W 16nm
    SoC with a Coarse-Grained Reconfigurable Array for Flexible Acceleration of Dense
    Linear Algebra. In 2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology
    and Circuits) (2022), pp. 70–71.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(31) Cassidy, A. S., Merolla, P., Arthur, J. V., Esser, S. K., Jackson, B.,
    Alvarez-Icaza, R., Datta, P., Sawada, J., Wong, T. M., Feldman, V., Amir, A.,
    Rubin, D. B.-D., Akopyan, F., McQuinn, E., Risk, W. P., and Modha, D. S. Cognitive
    computing building block: A versatile and efficient digital neuron model for neurosynaptic
    cores. In The 2013 International Joint Conference on Neural Networks (IJCNN) (2013),
    pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(32) Cavalcante, M., Schuiki, F., Zaruba, F., Schaffner, M., and Benini, L.
    Ara: A 1-GHz+ Scalable and Energy-Efficient RISC-V Vector Processor With Multiprecision
    Floating-Point Support in 22-Nm FD-SOI. IEEE Transactions on Very Large Scale
    Integration (VLSI) Systems 28, 2 (Feb. 2020), 530–543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(33) Cavalcante, M., Wüthrich, D., Perotti, M., Riedel, S., and Benini, L.
    Spatz: A Compact Vector Processing Unit for High-Performance and Energy-Efficient
    Shared-L1 Clusters. In 41st IEEE/ACM International Conference on Computer-Aided
    Design (San Diego California, Oct. 2022), ACM, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(34) Cavigelli, L., and Benini, L. Origami: A 803-GOp/s/W Convolutional Network
    Accelerator. IEEE Transactions on Circuits and Systems for Video Technology 27,
    11 (2017), 2461–2475.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (35) Chang, J.-W., Kang, K.-W., and Kang, S.-J. An Energy-Efficient FPGA-Based
    Deconvolutional Neural Networks Accelerator for Single Image Super-Resolution.
    IEEE Transactions on Circuits and Systems for Video Technology 30, 1 (2020), 281–295.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(36) Chatha, K. Qualcomm® Cloud Al 100 : 12TOPS/W Scalable, High Performance
    and Low Latency Deep Learning Inference Accelerator. In 2021 IEEE Hot Chips 33
    Symposium (HCS) (2021), pp. 1–19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(37) Chen, C., Xiang, X., Liu, C., Shang, Y., Guo, R., Liu, D., Lu, Y., Hao,
    Z., Luo, J., Chen, Z., Li, C., Pu, Y., Meng, J., Yan, X., Xie, Y., and Qi, X.
    Xuantie-910: A Commercial Multi-Core 12-Stage Pipeline Out-of-Order 64-Bit High
    Performance RISC-V Processor with Vector Extension : Industrial Product. In 2020
    ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA) (May
    2020), pp. 52–64.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (38) Chen, G. K., Knag, P. C., Tokunaga, C., and Krishnamurthy, R. K. An Eight-Core
    RISC-V Processor With Compute Near Last Level Cache in Intel 4 CMOS. IEEE Journal
    of Solid-State Circuits (2022), 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(39) Chen, Y., Chen, T., Xu, Z., Sun, N., and Temam, O. DianNao Family: Energy-Efficient
    Hardware Accelerators for Machine Learning. Commun. ACM 59, 11 (Oct 2016), 105–112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (40) Chen, Y., Xie, Y., Song, L., Chen, F., and Tang, T. A Survey of Accelerator
    Architectures for Deep Neural Networks. Engineering 6, 3 (2020), 264–274.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(41) Chen, Y.-H., Krishna, T., Emer, J. S., and Sze, V. Eyeriss: An Energy-Efficient
    Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE Journal
    of Solid-State Circuits 52, 1 (Jan. 2017), 127–138.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(42) Chen, Y.-H., Yang, T.-J., Emer, J., and Sze, V. Eyeriss v2: A Flexible
    Accelerator for Emerging Deep Neural Networks on Mobile Devices. IEEE Journal
    on Emerging and Selected Topics in Circuits and Systems 9, 2 (2019), 292–308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(43) Cheng, C., and Parhi, K. Hardware efficient fast parallel FIR filter structures
    based on iterated short convolution. IEEE Transactions on Circuits and Systems
    I: Regular Papers 51, 8 (2004), 1492–1500.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(44) Cheng, C., and Parhi, K. K. Fast 2D Convolution Algorithms for Convolutional
    Neural Networks. IEEE Transactions on Circuits and Systems I: Regular Papers 67,
    5 (2020), 1678–1691.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(45) Chi, P., Li, S., Xu, C., Zhang, T., Zhao, J., Liu, Y., Wang, Y., and Xie,
    Y. PRIME: A Novel Processing-in-Memory Architecture for Neural Network Computation
    in ReRAM-Based Main Memory. In 2016 ACM/IEEE 43rd Annual International Symposium
    on Computer Architecture (ISCA) (2016), pp. 27–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(46) Chitty-Venkata, K. T., and Somani, A. K. Neural Architecture Search Survey:
    A Hardware Perspective. ACM Comput. Surv. 55, 4 (nov 2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(47) Choquette, J. NVIDIA Hopper H100 GPU: Scaling Performance. IEEE Micro
    (2023), 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(48) Choquette, J., Giroux, O., and Foley, D. Volta: Performance and Programmability.
    IEEE Micro 38, 2 (Mar. 2018), 42–52.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (49) Choquette, J., Lee, E., Krashinsky, R., Balan, V., and Khailany, B. 3.2
    The A100 Datacenter GPU and Ampere Architecture. In 2021 IEEE International Solid-
    State Circuits Conference (ISSCC) (Feb. 2021), vol. 64, pp. 48–50.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(50) Clarke, P. Intel, Micron Launch “Bulk-Switching” ReRAM. [https://www.eetimes.com/intel-micron-launch-bulk-switching-reram/](https://www.eetimes.com/intel-micron-launch-bulk-switching-reram/),
    Jul. 2015. Accessed: 18/04/2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (51) Cococcioni, M., Rossi, F., Ruffaldi, E., and Saponara, S. A Lightweight
    Posit Processing Unit for RISC-V Processors in Deep Neural Network Applications.
    IEEE Transactions on Emerging Topics in Computing 10, 4 (2022), 1898–1908.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (52) Conference, A. . P. R. T., on Packaging, E., of Electronic, I., Photonic Systems,
    M., and 1, N. V., Eds. The Most Cost-Effective Integrator (TSV Interposer) for
    3D IC Integration System-in-Package (SiP) (07 2011), International Electronic
    Packaging Technical Conference and Exhibition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (53) Conti, F., Rossi, D., Paulin, G., Garofalo, A., Di Mauro, A., Rutishauer,
    G., marco Ottavi, G., Eggimann, M., Okuhara, H., Huard, V., Montfort, O., Jure,
    L., Exibard, N., Gouedo, P., Louvat, M., Botte, E., and Benini, L. 22.1 A 12.4TOPS/W
    @ 136GOPS AI-IoT System-on-Chip with 16 RISC-V, 2-to-8b Precision-Scalable DNN
    Acceleration and 30%-Boost Adaptive Body Biasing. In 2023 IEEE International Solid-
    State Circuits Conference (ISSCC) (Feb. 2023), pp. 21–23.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (54) Cordeiro, A. S., Santos, S. R. d., Moreira, F. B., Santos, P. C., Carro,
    L., and Alves, M. A. Z. Machine Learning Migration for Efficient Near-Data Processing.
    In 2021 29th Euromicro International Conference on Parallel, Distributed and Network-Based
    Processing (PDP) (2021), pp. 212–219.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (55) D-Wave Systems - The Practical Quantum Computing Company, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(56) Davidson, S., Xie, S., Torng, C., Al-Hawai, K., Rovinski, A., Ajayi, T.,
    Vega, L., Zhao, C., Zhao, R., Dai, S., Amarnath, A., Veluri, B., Gao, P., Rao,
    A., Liu, G., Gupta, R. K., Zhang, Z., Dreslinski, R., Batten, C., and Taylor,
    M. B. The Celerity Open-Source 511-Core RISC-V Tiered Accelerator Fabric: Fast
    Architectures and Design Methodologies for Fast Chips. IEEE Micro 38, 2 (Mar.
    2018), 30–41.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(57) Davies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y., Choday, S. H.,
    Dimou, G., Joshi, P., Imam, N., Jain, S., Liao, Y., Lin, C.-K., Lines, A., Liu,
    R., Mathaikutty, D., McCoy, S., Paul, A., Tse, J., Venkataramanan, G., Weng, Y.-H.,
    Wild, A., Yang, Y., and Wang, H. Loihi: A Neuromorphic Manycore Processor with
    On-Chip Learning. IEEE Micro 38, 1 (2018), 82–99.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (58) Davis, T. Wilkinson’s Sparse Matrix Definition. NA Digest 7, 12 (2007),
    379–401.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(59) Delmas Lascorz, A., Judd, P., Stuart, D. M., Poulos, Z., Mahmoud, M.,
    Sharify, S., Nikolic, M., Siu, K., and Moshovos, A. Bit-Tactical: A Software/Hardware
    Approach to Exploiting Value and Bit Sparsity in Neural Networks. In Twenty-Fourth
    International Conference on Architectural Support for Programming Languages and
    Operating Systems (2019), ASPLOS ’19, pp. 749–763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(60) Deng, C., Liao, S., Xie, Y., Parhi, K. K., Qian, X., and Yuan, B. PermDNN:
    Efficient Compressed DNN Architecture with Permuted Diagonal Matrices. In 51st
    Annual IEEE/ACM International Symposium on Microarchitecture (2018), MICRO-51,
    pp. 189–202.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (61) Desoli, G., Chawla, N., Boesch, T., Avodhyawasi, M., Rawat, H., Chawla,
    H., Abhijith, V., Zambotti, P., Sharma, A., Cappetta, C., Rossi, M., De Vita,
    A., and Girardi, F. A 40-310TOPS/W SRAM-Based All-Digital Up to 4b In-Memory Computing
    Multi-Tiled NN Accelerator in FD-SOI 18nm for Deep-Learning Edge Applications.
    In 2023 IEEE International Solid- State Circuits Conference (ISSCC) (2023), pp. 260–262.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (62) Desoli, G., Chawla, N., Boesch, T., Singh, S.-p., Guidetti, E., De Ambroggi,
    F., Majo, T., Zambotti, P., Ayodhyawasi, M., Singh, H., and Aggarwal, N. A 2.9TOPS/W
    deep convolutional neural network SoC in FD-SOI 28nm for intelligent embedded
    systems. In 2017 IEEE International Solid-State Circuits Conference (ISSCC) (2017),
    pp. 238–239.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(63) Dhilleswararao, P., Boppu, S., Manikandan, M. S., and Cenkeramaddi, L. R.
    Efficient Hardware Architectures for Accelerating Deep Neural Networks: Survey.
    IEEE Access 10 (2022), 131788–131828.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(64) Di Mauro, A., Scherer, M., Rossi, D., and Benini, L. Kraken: A Direct
    Event/Frame-Based Multi-sensor Fusion SoC for Ultra-Efficient Visual Processing
    in Nano-UAVs, Aug. 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (65) Ditzel, D. R., and team, t. E. Accelerating ML Recommendation With Over
    1,000 RISC-V/Tensor Processors on Esperanto’s ET-SoC-1 Chip. IEEE Micro 42, 3
    (May 2022), 31–38.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(66) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
    Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit,
    J., and Houlsby, N. An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale, June 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(67) Du, L., Du, Y., Li, Y., and Chang, M.-C. F. A Reconfigurable Streaming
    Deep Convolutional Neural Network Accelerator for Internet of Things. IEEE Transactions
    on Circuits and Systems I: Regular Papers PP (07 2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (68) Duarte, J., et al. Fast inference of deep neural networks in FPGAs for
    particle physics. JINST 13, 07 (2018), P07027.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(69) Durant, L., Giroux, O., Harris, M., and Stam, N. Inside Volta: The World’s
    Most Advanced Data Center GPU, May 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (70) Elster, A. C., and Haugdahl, T. A. Nvidia Hopper GPU and Grace CPU Highlights.
    Computing in Science & Engineering 24, 2 (Mar. 2022), 95–100.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (71) Esmaeilzadeh, H., Blem, E., Amant, R. S., Sankaralingam, K., and Burger,
    D. Dark silicon and the end of multicore scaling. In 2011 38th Annual International
    Symposium on Computer Architecture (ISCA) (2011), pp. 365–376.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (72) Esposito, D., Strollo, A. G. M., and Alioto, M. Low-power approximate MAC
    unit. In 2017 13th Conference on Ph.D. Research in Microelectronics and Electronics
    (PRIME) (2017), pp. 81–84.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(73) Fedus, W., Zoph, B., and Shazeer, N. Switch Transformers: Scaling to Trillion
    Parameter Models with Simple and Efficient Sparsity. Journal of Machine Learning
    Research 23, 120 (2022), 1–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(74) Ferrandi, F., Castellana, V. G., Curzel, S., Fezzardi, P., Fiorito, M.,
    Lattuada, M., Minutoli, M., Pilato, C., and Tumeo, A. Invited: Bambu: an Open-Source
    Research Framework for the High-Level Synthesis of Complex Applications. In 2021
    58th ACM/IEEE Design Automation Conference (DAC) (2021), pp. 1327–1330.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (75) Filippone, S., Cardellini, V., Barbieri, D., and Fanfarillo, A. Sparse
    Matrix-Vector Multiplication on GPGPUs. ACM Trans. Math. Softw. 43, 4 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (76) Fowers, J., Ovtcharov, K., Papamichael, M., Massengill, T., Liu, M., Lo,
    D., Alkalay, S., Haselman, M., Adams, L., Ghandi, M., Heil, S., Patel, P., Sapek,
    A., Weisz, G., Woods, L., Lanka, S., Reinhardt, S. K., Caulfield, A. M., Chung,
    E. S., and Burger, D. A Configurable Cloud-Scale DNN Processor for Real-Time AI.
    In ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)
    (2018), pp. 1–14.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(77) Frenkel, C., Bol, D., and Indiveri, G. Bottom-Up and Top-Down Neural Processing
    Systems Design: Neuromorphic Intelligence as the Convergence of Natural and Artificial
    Intelligence. CoRR abs/2106.01288 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (78) Frenkel, C., Lefebvre, M., Legat, J.-D., and Bol, D. A 0.086-mm² 12.7-pJ/SOP
    64k-Synapse 256-Neuron Online-Learning Digital Spiking Neuromorphic Processor
    in 28-nm CMOS. IEEE Transactions on Biomedical Circuits and Systems 13, 1 (2019),
    145–158.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(79) Frenkel, C., Legat, J.-D., and Bol, D. MorphIC: A 65-nm 738k-Synapse/mm²
    Quad-Core Binary-Weight Digital Neuromorphic Processor With Stochastic Spike-Driven
    Online Learning. IEEE Transactions on Biomedical Circuits and Systems 13, 5 (2019),
    999–1010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(80) Frustaci, F., Perri, S., Corsonello, P., and Alioto, M. Approximate multipliers
    with dynamic truncation for energy reduction via graceful quality degradation.
    IEEE Transactions on Circuits and Systems II: Express Briefs 67, 12 (2020), 3427–3431.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (81) Gallo, M. L., Khaddam-Aljameh, R., Stanisavljevic, M., Vasilopoulos, A.,
    Kersting, B., Dazzi, M., Karunaratne, G., Braendli, M., Singh, A., Mueller, S. M.,
    et al. A 64-core mixed-signal in-memory compute chip based on phase-change memory
    for deep neural network inference. arXiv preprint arXiv:2212.02872 (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (82) Gao, J., Ji, W., Chang, F., Han, S., Wei, B., Liu, Z., and Wang, Y. A Systematic
    Survey of General Sparse Matrix-Matrix Multiplication. ACM Comput. Surv. 55, 12
    (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(83) Gao, M., Pu, J., Yang, X., Horowitz, M., and Kozyrakis, C. TETRIS: Scalable
    and Efficient Neural Network Acceleration with 3D Memory. SIGARCH Comput. Archit.
    News 45, 1 (2017), 751–764.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(84) Gao, M., Yang, X., Pu, J., Horowitz, M., and Kozyrakis, C. TANGRAM: Optimized
    Coarse-Grained Dataflow for Scalable NN Accelerators. In Twenty-Fourth International
    Conference on Architectural Support for Programming Languages and Operating Systems
    (New York, NY, USA, 2019), ASPLOS ’19, Association for Computing Machinery, p. 807–820.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (85) Garofalo, A., Ottavi, G., Conti, F., Karunaratne, G., Boybat, I., Benini,
    L., and Rossi, D. A Heterogeneous In-Memory Computing Cluster for Flexible End-to-End
    Inference of Real-World Deep Neural Networks. IEEE Journal on Emerging and Selected
    Topics in Circuits and Systems 12, 2 (June 2022), 422–435.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(86) Garofalo, A., Tortorella, Y., Perotti, M., Valente, L., Nadalini, A.,
    Benini, L., Rossi, D., and Conti, F. DARKSIDE: A Heterogeneous RISC-V Compute
    Cluster for Extreme-Edge On-Chip DNN Inference and Training. IEEE Open Journal
    of the Solid-State Circuits Society 2 (2022), 231–243.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(87) Genc, H., Kim, S., Amid, A., Haj-Ali, A., Iyer, V., Prakash, P., Zhao,
    J., Grubb, D., Liew, H., Mao, H., Ou, A., Schmidt, C., Steffl, S., Wright, J.,
    Stoica, I., Ragan-Kelley, J., Asanovic, K., Nikolic, B., and Shao, Y. S. Gemmini:
    Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration.
    In 2021 58th ACM/IEEE Design Automation Conference (DAC) (Dec. 2021), pp. 769–774.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (88) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K.
    A Survey of Quantization Methods for Efficient Neural Network Inference, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(89) Giri, D., Chiu, K.-L., Di Guglielmo, G., Mantovani, P., and Carloni, L. P.
    ESP4ML: Platform-Based Design of Systems-on-Chip for Embedded Machine Learning.
    In 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE) (Mar.
    2020), pp. 1049–1054.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (90) Giri, D., Chiu, K.-L., Eichler, G., Mantovani, P., and Carloni, L. P. Accelerator
    Integration for Open-Source SoC Design. IEEE Micro 41, 4 (July 2021), 8–14.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(91) Gondimalla, A., Chesnut, N., Thottethodi, M., and Vijaykumar, T. N. SparTen:
    A Sparse Tensor Accelerator for Convolutional Neural Networks. In 52nd Annual
    IEEE/ACM International Symposium on Microarchitecture (2019), MICRO’52, pp. 151–165.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (92) Gonzalez, A., Zhao, J., Korpan, B., Genc, H., Schmidt, C., Wright, J.,
    Biswas, A., Amid, A., Sheikh, F., Sorokin, A., Kale, S., Yalamanchi, M., Yarlagadda,
    R., Flannigan, M., Abramowitz, L., Alon, E., Shao, Y. S., Asanovic, K., and Nikolic,
    B. A 16mm ² 106.1 GOPS/W Heterogeneous RISC-V Multi-Core Multi-Accelerator SoC
    in Low-Power 22nm FinFET. In ESSCIRC 2021 - IEEE 47th European Solid State Circuits
    Conference (ESSCIRC) (Grenoble, France, Sept. 2021), IEEE, pp. 259–262.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (93) Goodfellow, I., Bengio, Y., and Courville, A. Deep Learning. MIT Press,
    2016. [http://www.deeplearningbook.org](http://www.deeplearningbook.org).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(94) GreenWaves Technologies GAP9 Processor. [https://greenwaves-technologies.com/gap9_processor/](https://greenwaves-technologies.com/gap9_processor/),
    2023. Accessed: 2023-04-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(95) Gu, P., Xie, X., Ding, Y., Chen, G., Zhang, W., Niu, D., and Xie, Y. iPIM:
    Programmable In-Memory Image Processing Accelerator Using Near-Bank Architecture.
    In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture
    (ISCA) (2020), pp. 804–817.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(96) Guan, Y., Liang, H., Xu, N., Wang, W., Shi, S., Chen, X., Sun, G., Zhang,
    W., and Cong, J. FP-DNN: An Automated Framework for Mapping Deep Neural Networks
    onto FPGAs with RTL-HLS Hybrid Templates. In 2017 IEEE 25th Annual International
    Symposium on Field-Programmable Custom Computing Machines (FCCM) (2017), pp. 152–159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (97) Guo, C., Hsueh, B. Y., Leng, J., Qiu, Y., Guan, Y., Wang, Z., Jia, X.,
    Li, X., Guo, M., and Zhu, Y. Accelerating Sparse DNN Models without Hardware-Support
    via Tile-Wise Sparsity. In International Conference for High Performance Computing,
    Networking, Storage and Analysis (2020), SC’20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (98) Guo, K., Zeng, S., Yu, J., Wang, Y., and Yang, H. [DL] A survey of FPGA-based
    neural network inference accelerators. ACM Transactions on Reconfigurable Technology
    and Systems (TRETS) 12, 1 (2019), 1–26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (99) Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. Deep Learning
    with Limited Numerical Precision. In 32nd International Conference on Machine
    Learning (07-09 Jul 2015), vol. 37 of Proceedings of Machine Learning Research,
    pp. 1737–1746.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (100) Gyongyosi, L., and Imre, S. A Survey on quantum computing technology.
    Computer Science Review 31 (2019), 51–71.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (101) Ha, M., and Lee, S. Multipliers With Approximate 4–2 Compressors and Error
    Recovery Modules. IEEE Embedded Systems Letters 10, 1 (2018), 6–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (102) Hamanaka, F., Odan, T., Kise, K., and Chu, T. V. An Exploration of State-of-the-Art
    Automation Frameworks for FPGA-Based DNN Acceleration. IEEE Access 11 (2023),
    5701–5713.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(103) Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and Dally,
    W. J. EIE: Efficient Inference Engine on Compressed Deep Neural Network. In 43rd
    International Symposium on Computer Architecture (2016), ISCA’16, p. 243–254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(104) Han, S., Mao, H., and Dally, W. J. Deep Compression: Compressing Deep
    Neural Networks with Pruning, Trained Quantization and Huffman Coding. CoRR abs/1510.00149
    (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (105) Han, S., Pool, J., Tran, J., and Dally, W. Learning both Weights and Connections
    for Efficient Neural Network. In Advances in Neural Information Processing Systems
    (2015), vol. 28, Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(106) Harris, M. Inside Pascal: NVIDIA’s Newest Computing Platform, Apr. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(107) Hashemi, S., Bahar, R. I., and Reda, S. DRUM: A Dynamic Range Unbiased
    Multiplier for approximate applications. In 2015 IEEE/ACM International Conference
    on Computer-Aided Design (ICCAD) (2015), pp. 418–425.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (108) Hassanpour, M., Riera, M., and González, A. A Survey of Near-Data Processing
    Architectures for Neural Networks. Machine Learning and Knowledge Extraction 4,
    1 (2022), 66–102.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(109) He, M., Song, C., Kim, I., Jeong, C., Kim, S., Park, I., Thottethodi,
    M., and Vijaykumar, T. N. Newton: A DRAM-maker’s Accelerator-in-Memory (AiM) Architecture
    for Machine Learning. In 2020 53rd Annual IEEE/ACM International Symposium on
    Microarchitecture (MICRO) (2020), pp. 372–385.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(110) Hegde, K., Yu, J., Agrawal, R., Yan, M., Pellauer, M., and Fletcher,
    C. W. UCNN: Exploiting Computational Reuse in Deep Neural Networks via Weight
    Repetition. In 45th Annual International Symposium on Computer Architecture (2018),
    ISCA’18, p. 674–687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(111) Heyman, K. DRAM Thermal Issues Reach Crisis Point. [https://semiengineering.com/dram-thermal-issues-reach-crisis-point/](https://semiengineering.com/dram-thermal-issues-reach-crisis-point/),
    Jun. 2022. Accessed: 18/04/2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(112) Houshmand, P., Sarda, G. M., Jain, V., Ueyoshi, K., Papistas, I. A.,
    Shi, M., Zheng, Q., Bhattacharjee, D., Mallik, A., Debacker, P., Verkest, D.,
    and Verhelst, M. DIANA: An End-to-End Hybrid DIgital and ANAlog Neural Network
    SoC for the Edge. IEEE Journal of Solid-State Circuits 58, 1 (Jan. 2023), 203–215.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (113) Quantum computers emerging as accelerators in HPC, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (114) Hu, X., Stow, D., and Xie, Y. Die Stacking Is Happening. IEEE Micro 38,
    1 (2018), 22–28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (115) Hung, J.-M., Jhang, C.-J., Wu, P.-C., Chiu, Y.-C., and Chang, M.-F. Challenges
    and Trends of Nonvolatile In-Memory-Computation Circuits for AI Edge Devices.
    IEEE Open Journal of the Solid-State Circuits Society 1 (2021), 171–183.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(116) Hwang, R., Kim, T., Kwon, Y., and Rhu, M. Centaur: A Chiplet-based, Hybrid
    Sparse-Dense Accelerator for Personalized Recommendations. In 2020 ACM/IEEE 47th
    Annual International Symposium on Computer Architecture (ISCA) (Los Alamitos,
    CA, USA, jun 2020), IEEE Computer Society, pp. 968–981.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (117) Hybrid Memory Cube Consortium. Hybrid Memory Cube specification 2.1, Nov.
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (118) Ielmini, D., and Wong, H.-S. P. In-memory computing with resistive switching
    devices. Nature Electronics 1, 6 (Jun 2018), 333–343.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (119) Intel. Intel Arc A770 Graphics 16GB, Jul 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (120) Intel. Intel High Level Synthesis Compiler Reference Manual, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(121) Jain, V., Giraldo, S., Roose, J. D., Mei, L., Boons, B., and Verhelst,
    M. TinyVers: A Tiny Versatile System-on-Chip With State-Retentive eMRAM for ML
    Inference at the Extreme Edge. IEEE Journal of Solid-State Circuits (2023), 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (122) Jeddeloh, J., and Keeth, B. Hybrid memory cube new DRAM architecture increases
    density and performance. In 2012 Symposium on VLSI Technology (VLSIT) (2012),
    pp. 87–88.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (123) JEDEC Solid State Technology Association. High Bandwidth Memory DRAM (HBM3),
    JESD238A, Jan. 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (124) Jia, T., Mantovani, P., Dos Santos, M. C., Giri, D., Zuckerman, J., Loscalzo,
    E. J., Cochet, M., Swaminathan, K., Tombesi, G., Zhang, J. J., Chandramoorthy,
    N., Wellman, J.-D., Tien, K., Carloni, L., Shepard, K., Brooks, D., Wei, G.-Y.,
    and Bose, P. A 12nm Agile-Designed SoC for Swarm-Based Perception with Heterogeneous
    IP Blocks, a Reconfigurable Memory Hierarchy, and an 800MHz Multi-Plane NoC. In
    ESSCIRC 2022- IEEE 48th European Solid State Circuits Conference (ESSCIRC) (Sept.
    2022), pp. 269–272.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(125) Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick,
    R., Guadarrama, S., and Darrell, T. Caffe: Convolutional Architecture for Fast
    Feature Embedding. arXiv preprint arXiv:1408.5093 (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (126) Jia, Z., Tillman, B., Maggioni, M., and Scarpazza, D. P. Dissecting the
    Graphcore IPU Architecture via Microbenchmarking, Dec. 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(127) Jiao, Q., Hu, W., Liu, F., and Dong, Y. RISC-VTF: RISC-V Based Extended
    Instruction Set for Transformer. In 2021 IEEE International Conference on Systems,
    Man, and Cybernetics (SMC) (2021), pp. 1565–1570.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (128) Jiao, Y., Han, L., Jin, R., Su, Y.-J., Ho, C., Yin, L., Li, Y., Chen,
    L., Chen, Z., Liu, L., He, Z., Yan, Y., He, J., Mao, J., Zai, X., Wu, X., Zhou,
    Y., Gu, M., Zhu, G., Zhong, R., Lee, W., Chen, P., Chen, Y., Li, W., Xiao, D.,
    Yan, Q., Zhuang, M., Chen, J., Tian, Y., Lin, Y., Wu, W., Li, H., and Dou, Z.
    A 12nm Programmable Convolution-Efficient Neural-Processing-Unit Chip Achieving
    825TOPS. In 2020 IEEE International Solid- State Circuits Conference - (ISSCC)
    (2020), pp. 136–140.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(129) Jin, Q., Ren, J., Zhuang, R., Hanumante, S., Li, Z., Chen, Z., Wang,
    Y., Yang, K., and Tulyakov, S. F8Net: Fixed-Point 8-bit Only Multiplication for
    Network Quantization. In International Conference on Learning Representations
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (130) John, S. The future of computing beyond Moore’s Law. Phil. Trans. R. Soc.
    (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(131) Jou, J. M., Kuang, S. R., and Chen, R. D. Design of low-error fixed-width
    multipliers for DSP applications. IEEE Transactions on Circuits and Systems II:
    Analog and Digital Signal Processing 46, 6 (1999), 836–842.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (132) Jouppi, N., Borchers, A., Boyle, R., Cantin, P.-l., Chao, C., Clark, C.,
    Coriell, J., Daley, M., Dau, M., Dean, J., Gelb, B., Young, C., Ghaemmaghami,
    T., Gottipati, R., Gulland, W., Hagmann, R., Ho, C., Hogberg, D., Hu, J., and
    Boden, N. In-Datacenter Performance Analysis of a Tensor Processing Unit. In 44th
    Annual International Symposium on Computer Architecture (06 2017), Association
    for Computing Machinery, pp. 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(133) Jouppi, N., Kurian, G., Li, S., Ma, P., Nagarajan, R., Nai, L., Patil,
    N., Subramanian, S., Swing, A., Towles, B., Young, C., Zhou, X., Zhou, Z., and
    Patterson, D. A. TPU v4: An Optically Reconfigurable Supercomputer for Machine
    Learning with Hardware Support for Embeddings. In 50th Annual International Symposium
    on Computer Architecture (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (134) Jouppi, N., Young, C., Patil, N., and Patterson, D. Motivation for and
    Evaluation of the First Tensor Processing Unit. IEEE Micro 38, 3 (May 2018), 10–19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (135) Jouppi, N. P., Yoon, D. H., Kurian, G., Li, S., Patil, N., Laudon, J.,
    Young, C., and Patterson, D. A Domain-Specific Supercomputer for Training Deep
    Neural Networks. Communications of the ACM 63, 7 (June 2020), 67–78.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (136) Ju, Y., and Gu, J. A Systolic Neural CPU Processor Combining Deep Learning
    and General-Purpose Computing With Enhanced Data Locality and End-to-End Performance.
    IEEE Journal of Solid-State Circuits 58, 1 (Jan. 2023), 216–226.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (137) Kada, M. Research and Development History of Three-Dimensional Integration
    Technology. Springer International Publishing, Cham, 2015, pp. 1–23.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (138) Kannan, A., Jerger, N. E., and Loh, G. H. Enabling interposer-based disintegration
    of multi-core processors. In 2015 48th Annual IEEE/ACM International Symposium
    on Microarchitecture (MICRO) (2015), pp. 546–558.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (139) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child,
    R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling Laws for Neural Language
    Models, Jan. 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (140) Khaddam-Aljameh, R., Stanisavljevic, M., Mas, J. F., Karunaratne, G.,
    Brändli, M., Liu, F., Singh, A., Müller, S. M., Egger, U., Petropoulos, A., et al.
    HERMES-core—A 1.59-TOPS/mm 2 PCM on 14-nm CMOS in-memory compute core using 300-ps/LSB
    linearized CCO-based ADCs. IEEE Journal of Solid-State Circuits 57, 4 (2022),
    1027–1038.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (141) Khairy, M., Wassal, A. G., and Zahran, M. A survey of architectural approaches
    for improving GPGPU performance, programmability and heterogeneity. Journal of
    Parallel and Distributed Computing 127 (2019), 65–88.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(142) Kim, D., Kung, J., Chai, S., Yalamanchili, S., and Mukhopadhyay, S. Neurocube:
    A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory.
    In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture
    (ISCA) (2016), pp. 380–392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (143) Kim, D., Yu, C., Xie, S., Chen, Y., Kim, J.-Y., Kim, B., Kulkarni, J. P.,
    and Kim, T. T.-H. An Overview of Processing-in-Memory Circuits for Artificial
    Intelligence and Machine Learning. IEEE Journal on Emerging and Selected Topics
    in Circuits and Systems 12, 2 (2022), 338–353.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(144) Kim, J., and Kim, Y. HBM: Memory solution for bandwidth-hungry processors.
    In 2014 IEEE Hot Chips 26 Symposium (HCS) (2014), pp. 1–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(145) Kim, S., Kim, S., Um, S., Kim, S., Kim, K., and Yoo, H.-J. Neuro-CIM:
    A 310.4 TOPS/W Neuromorphic Computing-in-Memory Processor with Low WL/BL activity
    and Digital-Analog Mixed-mode Neuron Firing. In 2022 IEEE Symposium on VLSI Technology
    and Circuits (VLSI Technology and Circuits) (2022), pp. 38–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(146) Knechtel, J., Sinanoglu, O., Elfadel, I. A. M., Lienig, J., and Sze,
    C. C. N. Large-Scale 3D Chips: Challenges and Solutions for Design Automation,
    Testing, and Trustworthy Integration. IPSJ Transactions on System and LSI Design
    Methodology 10 (2017), 45–62.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (147) Knight, J. C., and Nowotny, T. GPUs Outperform Current HPC and Neuromorphic
    Solutions in Terms of Speed and Energy When Simulating a Highly-Connected Cortical
    Model. Frontiers in Neuroscience 12 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (148) Knowles, S. Graphcore. In 2021 IEEE Hot Chips 33 Symposium (HCS) (Aug.
    2021), pp. 1–25.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (149) Kong, T., and Li, S. Design and Analysis of Approximate 4–2 Compressors
    for High-Accuracy Multipliers. IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems 29, 10 (2021), 1771–1781.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (150) Krashinsky, R., Giroux, O., Jones, S., Stam, N., and Ramaswamy, S. NVIDIA
    Ampere Architecture In-Depth, May 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (151) Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet Classification
    with Deep Convolutional Neural Networks. In Advances in Neural Information Processing
    Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds.
    Curran Associates, Inc., 2012, pp. 1097–1105.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (152) Kulkarni, P., Gupta, P., and Ercegovac, M. Trading Accuracy for Power
    with an Underdesigned Multiplier Architecture. In 2011 24th Internatioal Conference
    on VLSI Design (2011), pp. 346–351.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (153) Kwon, Y., Han, J., Cho, Y. P., Kim, J., Chung, J., Choi, J., Park, S.,
    Kim, I., Kwon, H., Kim, J., Kim, H., Jeon, W., Jeon, Y., Cho, M., and Choi, M.
    Chiplet Heterogeneous-Integration AI Processor. In 2023 International Conference
    on Electronics, Information, and Communication (ICEIC) (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (154) Kwon, Y.-C., Lee, S. H., Lee, J., Kwon, S.-H., Ryu, J. M., Son, J.-P.,
    Seongil, O., Yu, H.-S., Lee, H., Kim, S. Y., Cho, Y., Kim, J. G., Choi, J., Shin,
    H.-S., Kim, J., Phuah, B., Kim, H., Song, M. J., Choi, A., Kim, D., Kim, S., Kim,
    E.-B., Wang, D., Kang, S., Ro, Y., Seo, S., Song, J., Youn, J., Sohn, K., and
    Kim, N. S. 25.4 A 20nm 6GB Function-In-Memory DRAM, Based on HBM2 with a 1.2TFLOPS
    Programmable Computing Unit Using Bank-Level Parallelism, for Machine Learning
    Applications. In 2021 IEEE International Solid- State Circuits Conference (ISSCC)
    (2021), vol. 64, pp. 350–352.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (155) Lan, J., Nambiar, V. P., Sabapathy, R., Rotaru, M. D., and Do, A. T. Chiplet-based
    Architecture Design for Multi-Core Neuromorphic Processor. In 2021 IEEE 23rd Electronics
    Packaging Technology Conference (EPTC) (2021), pp. 410–412.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(156) Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar,
    J., Riddle, R., Shpeisman, T., Vasilache, N., and Zinenko, O. MLIR: Scaling Compiler
    Infrastructure for Domain Specific Computation. In IEEE/ACM International Symposium
    on Code Generation and Optimization (CGO) (2021), pp. 2–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (157) Lazo, C. R., Reggiani, E., Morales, C. R., Figueras Bagué, R., Villa Vargas,
    L. A., Ramírez Salinas, M. A., Cortés, M. V., Sabri Ünsal, O., and Cristal, A.
    Adaptable Register File Organization for Vector Processors. In 2022 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA) (Apr. 2022), pp. 786–799.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (158) LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Nature 521, 7553
    (2015), 436.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (159) Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning
    applied to document recognition. Proceedings of the IEEE 86, 11 (1998), 2278–2324.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (160) Lee, S. K., Agrawal, A., Silberman, J., Ziegler, M., Kang, M., Venkataramani,
    S., Cao, N., Fleischer, B., Guillorn, M., Cohen, M., Mueller, S. M., Oh, J., Lutz,
    M., Jung, J., Koswatta, S., Zhou, C., Zalani, V., Kar, M., Bonanno, J., Casatuta,
    R., Chen, C.-Y., Choi, J., Haynie, H., Herbert, A., Jain, R., Kim, K.-H., Li,
    Y., Ren, Z., Rider, S., Schaal, M., Schelm, K., Scheuermann, M. R., Sun, X., Tran,
    H., Wang, N., Wang, W., Zhang, X., Shah, V., Curran, B., Srinivasan, V., Lu, P.-F.,
    Shukla, S., Gopalakrishnan, K., and Chang, L. A 7-nm Four-Core Mixed-Precision
    AI Chip With 26.2-TFLOPS Hybrid-FP8 Training, 104.9-TOPS INT4 Inference, and Workload-Aware
    Throttling. IEEE Journal of Solid-State Circuits 57, 1 (2022), 182–197.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (161) Lee, S. M., Kim, H., Yeon, J., Lee, J., Choi, Y., Kim, M., Park, C., Jang,
    K., Kim, Y., Kim, Y., Lee, C., Han, H., Kim, W. E., Tang, R., and Baek, J. H.
    A 64-TOPS Energy-Efficient Tensor Accelerator in 14nm With Reconfigurable Fetch
    Network and Processing Fusion for Maximal Data Reuse. IEEE Open Journal of the
    Solid-State Circuits Society 2 (2022), 219–230.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (162) Lee, Y., Waterman, A., Avizienis, R., Cook, H., Sun, C., Stojanović, V.,
    and Asanović, K. A 45nm 1.3 GHz 16.7 double-precision GFLOPS/W RISC-V processor
    with vector accelerators. In ESSCIRC 2014-40th European Solid State Circuits Conference
    (ESSCIRC) (2014), IEEE, pp. 199–202.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (163) Lepri, N., Glukhov, A., Cattaneo, L., Farronato, M., Mannocci, P., and
    Ielmini, D. In-memory computing for machine learning and deep learning. IEEE Journal
    of the Electron Devices Society (2023), 1–1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(164) Li, G., Liu, Z., Li, F., and Cheng, J. Block Convolution: Towards Memory-Efficient
    Inference of Large-Scale CNNs on FPGA. CoRR abs/2105.08937 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(165) Li, J., Jiang, S., Gong, S., Wu, J., Yan, J., Yan, G., and Li, X. SqueezeFlow:
    A Sparse CNN Accelerator Exploiting Concise Convolution Rules. IEEE Transactions
    on Computers 68, 11 (2019), 1663–1677.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (166) Li, L., Hammad, I., and El-Sankary, K. Dual segmentation approximate multiplier.
    Electronics Letters 57, 19 (2021), 718–720.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(167) Li, Y., Louri, A., and Karanth, A. SPRINT: A high-performance, energy-efficient,
    and scalable chiplet-based accelerator with photonic interconnects for CNN inference.
    IEEE Transactions on Parallel and Distributed Systems 33, 10 (2021), 2332–2345.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (168) Lin, C.-H., Cheng, C.-C., Tsai, Y.-M., Hung, S.-J., Kuo, Y.-T., Wang,
    P. H., Tsung, P.-K., Hsu, J.-Y., Lai, W.-C., Liu, C.-H., Wang, S.-Y., Kuo, C.-H.,
    Chang, C.-Y., Lee, M.-H., Lin, T.-Y., and Chen, C.-C. A 3.4-to-13.3TOPS/W 3.6TOPS
    Dual-Core Deep-Learning Accelerator for Versatile AI Applications in 7nm 5G Smartphone
    SoC. In 2020 IEEE International Solid- State Circuits Conference - (ISSCC) (2020),
    pp. 134–136.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (169) Lin, M.-S., Huang, T.-C., Tsai, C.-C., Tam, K.-H., Hsieh, C.-H., Chen,
    T., Huang, W.-H., Hu, J., Chen, Y.-C., Goel, S. K., Fu, C.-M., Rusu, S., Li, C.-C.,
    Yang, S.-Y., Wong, M., Yang, S.-C., and Lee, F. A 7nm 4GHz Arm-core-based CoWoS
    Chiplet Design for High Performance Computing. In 2019 Symposium on VLSI Circuits
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (170) Liu, C., Bellec, G., Vogginger, B., Kappel, D., Partzsch, J., Neumärker,
    F., Höppner, S., Maass, W., Furber, S. B., Legenstein, R., and Mayr, C. G. Memory-Efficient
    Deep Learning on a SpiNNaker 2 Prototype. Frontiers in Neuroscience 12 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(171) Liu, L., Zhu, J., Li, Z., Lu, Y., Deng, Y., Han, J., Yin, S., and Wei,
    S. A Survey of Coarse-Grained Reconfigurable Architecture and Design: Taxonomy,
    Challenges, and Applications. ACM Comput. Surv. 52, 6 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(172) Liu, X., Mao, M., Liu, B., Li, H., Chen, Y., Li, B., Wang, Y., Jiang,
    H., Barnell, M., Wu, Q., and Yang, J. RENO: A high-efficient reconfigurable neuromorphic
    computing accelerator design. In 2015 52nd ACM/EDAC/IEEE Design Automation Conference
    (DAC) (2015), pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(173) Liu, Z., Cheng, K.-T., Huang, D., Xing, E., and Shen, Z. Nonuniform-to-Uniform
    Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation.
    In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
    (2022), pp. 4932–4942.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (174) Ma, Y., Cao, Y., Vrudhula, S., and Seo, J.-s. Optimizing the Convolution
    Operation to Accelerate Deep Neural Networks on FPGA. IEEE Transactions on Very
    Large Scale Integration (VLSI) Systems 26, 7 (2018), 1354–1367.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (175) Ma, Y., Suda, N., Cao, Y., Seo, J.-s., and Vrudhula, S. Scalable and modularized
    RTL compilation of Convolutional Neural Networks onto FPGA. In 2016 26th International
    Conference on Field Programmable Logic and Applications (FPL) (2016), pp. 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (176) Machupalli, R., Hossain, M., and Mandal, M. Review of ASIC accelerators
    for deep neural network. Microprocessors and Microsystems 89 (2022), 104441.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (177) Machura, M., Danilowicz, M., and Kryjak, T. Embedded Object Detection
    with Custom LittleNet, FINN and Vitis AI DCNN Accelerators. Journal of Low Power
    Electronics and Applications 12, 2 (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(178) Mallasén, D., Murillo, R., Barrio, A. A. D., Botella, G., Piñuel, L.,
    and Prieto-Matias, M. PERCIVAL: Open-Source Posit RISC-V Core With Quire Capability.
    IEEE Transactions on Emerging Topics in Computing 10, 3 (2022), 1241–1252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(179) Martinez, P. Y., Beilliard, Y., Godard, M., Danovitch, D., Drouin, D.,
    Charbonnier, J., Coudrain, P., Garnier, A., Lattard, D., Vivet, P., Cheramy, S.,
    Guthmuller, E., Tortolero, C. F., Mengue, V., Durupt, J., Philippe, A., and Dutoit,
    D. ExaNoDe: Combined Integration of Chiplets on Active Interposer with Bare Dice
    in a Multi-Chip-Module for Heterogeneous and Scalable High Performance Compute
    Nodes. In 2020 IEEE Symposium on VLSI Technology (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (180) Mathur, R., Kumar, A. K. A., John, L., and Kulkarni, J. P. Thermal-Aware
    Design Space Exploration of 3-D Systolic ML Accelerators. IEEE Journal on Exploratory
    Solid-State Computational Devices and Circuits 7, 1 (2021), 70–78.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (181) Mattson, P., Cheng, C., Diamos, G., Coleman, C., Micikevicius, P., Patterson,
    D., Tang, H., Wei, G.-Y., Bailis, P., Bittorf, V., et al. Mlperf training benchmark.
    Proceedings of Machine Learning and Systems 2 (2020), 336–349.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(182) Mattson, P., Reddi, V. J., Cheng, C., Coleman, C., Diamos, G., Kanter,
    D., Micikevicius, P., Patterson, D., Schmuelling, G., Tang, H., Wei, G.-Y., and
    Wu, C.-J. MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance.
    IEEE Micro 40, 2 (2020), 8–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (183) Medina, E. [Habana Labs presentation]. In 2019 IEEE Hot Chips 31 Symposium
    (HCS) (2019), pp. 1–29.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (184) Micron. Hybrid Memory Cube – HMC Gen2 HMC Memory Features, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(185) Min, C., Mao, J., Li, H., and Chen, Y. NeuralHMC: An Efficient HMC-Based
    Accelerator for Deep Neural Networks. In 24th Asia and South Pacific Design Automation
    Conference (2019), ACM, p. 394–399.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(186) Minervini, F., Palomar, O., Unsal, O., Reggiani, E., Quiroga, J., Marimon,
    J., Rojas, C., Figueras, R., Ruiz, A., Gonzalez, A., Mendoza, J., Vargas, I.,
    Hernandez, C., Cabre, J., Khoirunisya, L., Bouhali, M., Pavon, J., Moll, F., Olivieri,
    M., Kovac, M., Kovac, M., Dragic, L., Valero, M., and Cristal, A. Vitruvius+:
    An Area-Efficient RISC-V Decoupled Vector Coprocessor for High Performance Computing
    Applications. ACM Transactions on Architecture and Code Optimization 20, 2 (Mar.
    2023), 28:1–28:25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(187) Miro-Panades, I., Tain, B., Christmann, J.-F., Coriat, D., Lemaire, R.,
    Jany, C., Martineau, B., Chaix, F., Waltener, G., Pluchart, E., Noel, J.-P., Makosiej,
    A., Montoya, M., Bacles-Min, S., Briand, D., Philippe, J.-M., Thonnart, Y., Valentian,
    A., Heitzmann, F., and Clermidy, F. SamurAI: A Versatile IoT Node With Event-Driven
    Wake-Up and Embedded ML Acceleration. IEEE Journal of Solid-State Circuits (2022),
    1–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (188) Mishra, A. K., Latorre, J. A., Pool, J., Stosic, D., Stosic, D., Venkatesh,
    G., Yu, C., and Micikevicius, P. Accelerating Sparse Deep Neural Networks. CoRR
    abs/2104.08378 (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(189) MLPerf Inference Tiny v1.0 Results. [https://mlcommons.org/en/inference-tiny-10/](https://mlcommons.org/en/inference-tiny-10/),
    2023. Accessed: 2023-04-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (190) Mounce, G., Lyke, J., Horan, S., Powell, W., Doyle, R., and Some, R. Chiplet
    based approach for heterogeneous processing and packaging architectures. In 2016
    IEEE Aerospace Conference (2016), pp. 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(191) Muñoz Martínez, F., Garg, R., Pellauer, M., Abellán, J. L., Acacio, M. E.,
    and Krishna, T. Flexagon: A Multi-Dataflow Sparse-Sparse Matrix Multiplication
    Accelerator for Efficient DNN Processing. In 28th ACM International Conference
    on Architectural Support for Programming Languages and Operating Systems, Volume
    3 (2023), ASPLOS 2023, pp. 252–265.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (192) Nahmias, M. A., de Lima, T. F., Tait, A. N., Peng, H.-T., Shastri, B. J.,
    and Prucnal, P. R. Photonic Multiply-Accumulate Operations for Neural Networks.
    IEEE Journal of Selected Topics in Quantum Electronics 26, 1 (2020), 1–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (193) Narayanamoorthy, S., Moghaddam, H. A., Liu, Z., Park, T., and Kim, N. S.
    Energy-Efficient Approximate Multiplication for Digital Signal Processing and
    Classification Applications. IEEE Transactions on Very Large Scale Integration
    (VLSI) Systems 23, 6 (2015), 1180–1184.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (194) Narayanan, P., Ambrogio, S., Okazaki, A., Hosokawa, K., Tsai, H., Nomura,
    A., Yasuda, T., Mackin, C., Lewis, S. C., Friz, A., Ishii, M., Kohda, Y., Mori,
    H., Spoon, K., Khaddam-Aljameh, R., Saulnier, N., Bergendahl, M., Demarest, J.,
    Brew, K. W., Chan, V., Choi, S., Ok, I., Ahsan, I., Lie, F. L., Haensch, W., Narayanan,
    V., and Burr, G. W. Fully On-Chip MAC at 14 nm Enabled by Accurate Row-Wise Programming
    of PCM-Based Weights and Parallel Vector-Transport in Duration-Format. IEEE Transactions
    on Electron Devices 68, 12 (2021), 6629–6636.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(195) Nitin, ., Thottethodi, M., and Vijaykumar, T. N. Millipede: Die-Stacked
    Memory Optimizations for Big Data Machine Learning Analytics. In 2018 IEEE International
    Parallel and Distributed Processing Symposium (IPDPS) (2018), pp. 160–171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(196) Nurvitadhi, E., Kwon, D., Jafari, A., Boutros, A., Sim, J., Tomson, P.,
    Sumbul, H., Chen, G., Knag, P., Kumar, R., Krishnamurthy, R., Gribok, S., Pasca,
    B., Langhammer, M., Marr, D., and Dasu, A. Why Compete When You Can Work Together:
    FPGA-ASIC Integration for Persistent RNNs. In 2019 IEEE 27th Annual International
    Symposium on Field-Programmable Custom Computing Machines (FCCM) (2019), pp. 199–207.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (197) NVIDIA. Fermi, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (198) NVIDIA. Kepler GK110, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (199) NVidia. NVIDIA Announces DGX H100 Systems – World’s Most Advanced Enterprise
    AI Infrastructure, Mar. 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (200) NVidia. NVIDIA DGX Platform The best of NVIDIA AI—all in one place, May
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (201) NVidia. NVLink and NVSwitch, May 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (202) Oh, J., Lee, S. K., Kang, M., Ziegler, M., Silberman, J., Agrawal, A.,
    Venkataramani, S., Fleischer, B., Guillorn, M., Choi, J., Wang, W., Mueller, S.,
    Ben-Yehuda, S., Bonanno, J., Cao, N., Casatuta, R., Chen, C.-Y., Cohen, M., Erez,
    O., Fox, T., Gristede, G., Haynie, H., Ivanov, V., Koswatta, S., Lo, S.-H., Lutz,
    M., Maier, G., Mesh, A., Nustov, Y., Rider, S., Schaal, M., Scheuermann, M., Sun,
    X., Wang, N., Yee, F., Zhou, C., Shah, V., Curran, B., Srinivasan, V., Lu, P.-F.,
    Shukla, S., Gopalakrishnan, K., and Chang, L. A 3.0 TFLOPS 0.62V Scalable Processor
    Core for High Compute Utilization AI Training and Inference. In 2020 IEEE Symposium
    on VLSI Circuits (2020), pp. 1–2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(203) Oliveira, G. F., Santos, P. C., Alves, M. A. Z., and Carro, L. NIM: An
    HMC-Based Machine for Neuron Computation. In Applied Reconfigurable Computing
    (Cham, 2017), S. Wong, A. C. Beck, K. Bertels, and L. Carro, Eds., Springer International
    Publishing, pp. 28–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (204) OpenAI. GPT-4 Technical Report, Mar. 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(205) Ottavi, G., Garofalo, A., Tagliavini, G., Conti, F., Mauro, A. D., Benini,
    L., and Rossi, D. Dustin: A 16-Cores Parallel Ultra-Low-Power Cluster With 2b-to-32b
    Fully Flexible Bit-Precision and Vector Lockstep Execution Mode. IEEE Transactions
    on Circuits and Systems I: Regular Papers (2023), 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(206) Painkras, E., Plana, L. A., Garside, J., Temple, S., Galluppi, F., Patterson,
    C., Lester, D. R., Brown, A. D., and Furber, S. B. SpiNNaker: A 1-W 18-Core System-on-Chip
    for Massively-Parallel Neural Network Simulation. IEEE Journal of Solid-State
    Circuits 48, 8 (2013), 1943–1953.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(207) Parashar, A., Rhu, M., Mukkara, A., Puglielli, A., Venkatesan, R., Khailany,
    B., Emer, J., Keckler, S. W., and Dally, W. J. SCNN: An Accelerator for Compressed-Sparse
    Convolutional Neural Networks. In Proceedings of the 2017 ACM/IEEE 44th Annual
    International Symposium on Computer Architecture (2017), ISCA’17, pp. 27–40.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(208) Park, G., Kung, J., and Lee, Y. Design and Analysis of Approximate Compressors
    for Balanced Error Accumulation in MAC Operator. IEEE Transactions on Circuits
    and Systems I: Regular Papers 68, 7 (2021), 2950–2961.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(209) Park, J., Li, S. R., Wen, W., Li, H., Chen, Y., and Dubey, P. Holistic
    SparseCNN: Forging the Trident of Accuracy, Speed, and Size. CoRR abs/1608.01409
    (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(210) Park, J., Naumov, M., Basu, P., Deng, S., Kalaiah, A., Khudia, D. S.,
    Law, J., Malani, P., Malevich, A., Satish, N., Pino, J. M., Schatz, M., Sidorov,
    A., Sivakumar, V., Tulloch, A., Wang, X., Wu, Y., Yuen, H., Diril, U., Dzhulgakov,
    D., Hazelwood, K. M., Jia, B., Jia, Y., Qiao, L., Rao, V., Rotem, N., Yoo, S.,
    and Smelyanskiy, M. Deep Learning Inference in Facebook Data Centers: Characterization,
    Performance Optimizations and Hardware Implications. CoRR abs/1811.09886 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (211) Park, J.-S., Jang, J.-W., Lee, H., Lee, D., Lee, S., Jung, H., Lee, S.,
    Kwon, S., Jeong, K., Song, J.-H., Lim, S., and Kang, I. A 6K-MAC Feature-Map-Sparsity-Aware
    Neural Processing Unit in 5nm Flagship Mobile SoC. In 2021 IEEE International
    Solid- State Circuits Conference (ISSCC) (2021), vol. 64, pp. 152–154.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (212) Park, J.-S., Park, C., Kwon, S., Kim, H.-S., Jeon, T., Kang, Y., Lee,
    H., Lee, D., Kim, J., Lee, Y., Park, S., Jang, J.-W., Ha, S., Kim, M., Bang, J.,
    Lim, S. H., and Kang, I. A Multi-Mode 8K-MAC HW-Utilization-Aware Neural Processing
    Unit with a Unified Multi-Precision Datapath in 4nm Flagship Mobile SoC. In 2022
    IEEE International Solid- State Circuits Conference (ISSCC) (2022), vol. 65, pp. 246–248.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (213) Park, S.-W., Park, J., Bong, K., Shin, D., Lee, J., Choi, S., and Yoo,
    H.-J. An Energy-Efficient and Scalable Deep Learning/Inference Processor With
    Tetra-Parallel MIMD Architecture for Big Data Applications. IEEE Transactions
    on Biomedical Circuits and Systems 9, 6 (2015), 838–848.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(214) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G.,
    Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang,
    E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
    Bai, J., and Chintala, S. PyTorch: An Imperative Style, High-Performance Deep
    Learning Library. In Advances in Neural Information Processing Systems 32. Curran
    Associates, Inc., 2019, pp. 8024–8035.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (215) Paulin, G., Andri, R., Conti, F., and Benini, L. RNN-Based Radio Resource
    Management on Multicore RISC-V Accelerator Architectures. IEEE Transactions on
    Very Large Scale Integration (VLSI) Systems 29, 9 (Sept. 2021), 1624–1637.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (216) Pawlowski, J. T. Hybrid memory cube (HMC). In 2011 IEEE Hot Chips 23 Symposium
    (HCS) (2011), pp. 1–24.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (217) Peemen, M., Setio, A. A. A., Mesman, B., and Corporaal, H. Memory-centric
    accelerator design for Convolutional Neural Networks. In 2013 IEEE 31st International
    Conference on Computer Design (ICCD) (2013), pp. 13–19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(218) Perotti, M., Cavalcante, M., Wistoff, N., Andri, R., Cavigelli, L., and
    Benini, L. A “New Ara” for Vector Computing: An Open Source Highly Efficient RISC-V
    V 1.0 Vector Processor Design. In 2022 IEEE 33rd International Conference on Application-specific
    Systems, Architectures and Processors (ASAP) (July 2022), pp. 43–51.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (219) Perri, S., Sestito, C., Spagnolo, F., and Corsonello, P. Efficient Deconvolution
    Architecture for Heterogeneous Systems-on-Chip. Journal of Imaging 6, 9 (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(220) Petra, N., De Caro, D., Garofalo, V., Napoli, E., and Strollo, A. G. M.
    Design of Fixed-Width Multipliers With Linear Compensation Function. IEEE Transactions
    on Circuits and Systems I: Regular Papers 58, 5 (2011), 947–960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(221) Prabhu, K., Gural, A., Khan, Z. F., Radway, R. M., Giordano, M., Koul,
    K., Doshi, R., Kustin, J. W., Liu, T., Lopes, G. B., et al. CHIMERA: A 0.92-TOPS,
    2.2-TOPS/W edge AI accelerator with 2-MByte on-chip foundry resistive RAM for
    efficient training and inference. IEEE Journal of Solid-State Circuits 57, 4 (2022),
    1013–1026.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(222) Prasad, A., Benini, L., and Conti, F. Specialization Meets Flexibility:
    A Heterogeneous Architecture for High-Efficiency, High-flexibility AR/VR Processing.
    In Proceedings of the 2023 Design Automation Conference (DAC 2023), to Appear
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(223) Prickett, M. The HBM3 roadmap is just getting started. [https://www.nextplatform.com/2022/04/06/the-hbm3-roadmap-is-just-getting-started/](https://www.nextplatform.com/2022/04/06/the-hbm3-roadmap-is-just-getting-started/),
    Apr. 2022. Accessed: 18/04/2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(224) Proud, M. ACHIEVING MAXIMUM COMPUTE THROUGHPUT: PCIE VS. SXM2, May 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(225) Qadeer, W., Hameed, R., Shacham, O., Venkatesan, P., Kozyrakis, C., and
    Horowitz, M. Convolution engine: balancing efficiency and flexibility in specialized
    computing. Communications of the ACM 58, 4 (2015), 85–93.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(226) Qin, E., Samajdar, A., Kwon, H., Nadella, V., Srinivasan, S., Das, D.,
    Kaul, B., and Krishna, T. SIGMA: A Sparse and Irregular GEMM Accelerator with
    Flexible Interconnects for DNN Training. In 2020 IEEE International Symposium
    on High Performance Computer Architecture (2020), HPCA’20, pp. 58–70.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(227) Qin, H., Gong, R., Liu, X., Bai, X., Song, J., and Sebe, N. Binary neural
    networks: A survey. Pattern Recognition 105 (2020), 107281.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (228) IBM Qiskit Simulator, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (229) Qiu, J., Wang, J., Yao, S., Guo, K., Li, B., Zhou, E., Yu, J., Tang, T.,
    Xu, N., Song, S., Wang, Y., and Yang, H. Going Deeper with Embedded FPGA Platform
    for Convolutional Neural Network. 2016 ACM/SIGDA International Symposium on Field-Programmable
    Gate Arrays (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (230) Rahman, A., Oh, S., Lee, J., and Choi, K. Design space exploration of
    FPGA accelerators for convolutional neural networks. In Design, Automation & Test
    in Europe Conference & Exhibition (DATE) (2017), pp. 1147–1152.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(231) Rathi, N., Chakraborty, I., Kosta, A., Sengupta, A., Ankit, A., Panda,
    P., and Roy, K. Exploring Neuromorphic Computing Based on Spiking Neural Networks:
    Algorithms to Hardware. ACM Comput. Surv. 55, 12 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (232) Reuther, A., Michaleas, P., Jones, M., Gadepally, V., Samsi, S., and Kepner,
    J. AI and ML Accelerator Survey and Trends. In 2022 IEEE High Performance Extreme
    Computing Conference (HPEC) (2022), pp. 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(233) Robinson, C. NVIDIA H100 Hopper Details at HC34 as it Waits for Next-Gen
    CPUs. [https://www.servethehome.com/nvidia-h100-hopper-details-at-hc34-as-it-waits-for-next-gen-cpus/](https://www.servethehome.com/nvidia-h100-hopper-details-at-hc34-as-it-waits-for-next-gen-cpus/),
    Aug. 2022. Accessed: 18/04/2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (234) Rosenblatt, F. The perceptron - A perceiving and recognizing automaton.
    Tech. Rep. 85-460-1, Cornell Aeronautical Laboratory, Ithaca, New York, January
    1957.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(235) Rossi, D., Conti, F., Eggiman, M., Mauro, A. D., Tagliavini, G., Mach,
    S., Guermandi, M., Pullini, A., Loi, I., Chen, J., Flamand, E., and Benini, L.
    Vega: A Ten-Core SoC for IoT Endnodes With DNN Acceleration and Cognitive Wake-Up
    From MRAM-Based State-Retentive Sleep Mode. IEEE Journal of Solid-State Circuits
    57, 1 (Jan. 2022), 127–139.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (236) Sankaradas, M., Jakkula, V., Cadambi, S., Chakradhar, S., Durdanovic,
    I., Cosatto, E., and Graf, H. P. A Massively Parallel Coprocessor for Convolutional
    Neural Networks. In 20th IEEE International Conference on Application-Specific
    Systems, Architectures and Processors (USA, 2009), IEEE Computer Society, p. 53–60.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(237) Schmidhuber, J. Deep Learning in Neural Networks: An Overview. CoRR abs/1404.7828
    (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(238) Schmidhuber, J. Deep learning in neural networks: An overview. Neural
    Networks 61 (2015), 85–117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (239) Seo, J.-s., Brezzo, B., Liu, Y., Parker, B. D., Esser, S. K., Montoye,
    R. K., Rajendran, B., Tierno, J. A., Chang, L., Modha, D. S., and Friedman, D. J.
    A 45nm CMOS neuromorphic chip with a scalable architecture for learning in networks
    of spiking neurons. In 2011 IEEE Custom Integrated Circuits Conference (CICC)
    (2011), pp. 1–4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (240) Sestito, C., Spagnolo, F., and Perri, S. Design of Flexible Hardware Accelerators
    for Image Convolutions and Transposed Convolutions. Journal of Imaging 7, 10 (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(241) Shafiee, A., Nag, A., Muralimanohar, N., Balasubramonian, R., Strachan,
    J. P., Hu, M., Williams, R. S., and Srikumar, V. ISAAC: A Convolutional Neural
    Network Accelerator with in-Situ Analog Arithmetic in Crossbars. In 43rd International
    Symposium on Computer Architecture (2016), p. 14–26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (242) Shan, J., Casu, M. R., Cortadella, J., Lavagno, L., and Lazarescu, M. T.
    Exact and Heuristic Allocation of MuIti-kernel Applications to Multi-FPGA Platforms.
    In 2019 56th ACM/IEEE Design Automation Conference (DAC) (2019), pp. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(243) Shao, Y. S., Clemons, J., Venkatesan, R., Zimmer, B., Fojtik, M., Jiang,
    N., Keller, B., Klinefelter, A., Pinckney, N., Raina, P., Tell, S. G., Zhang,
    Y., Dally, W. J., Emer, J., Gray, C. T., Khailany, B., and Keckler, S. W. Simba:
    Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture. In
    52nd Annual IEEE/ACM International Symposium on Microarchitecture (Oct. 2019),
    Association for Computing Machinery, pp. 14–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(244) Sharma, H., Mandal, S. K., Doppa, J. R., Ogras, U. Y., and Pande, P. P.
    SWAP: A Server-Scale Communication-Aware Chiplet-Based Manycore PIM Accelerator.
    IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
    41, 11 (2022), 4145–4156.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (245) Siemens. Catapult C++/Systemc Synthesis, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (246) Smagulova, K., Fouda, M. E., Kurdahi, F., Salama, K. N., and Eltawil,
    A. Resistive Neural Hardware Accelerators. Proceedings of the IEEE 111, 5 (2023),
    500–527.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (247) Sohn, K., Yun, W.-J., Oh, R., Oh, C.-S., Seo, S.-Y., Park, M.-S., Shin,
    D.-H., Jung, W.-C., Shin, S.-H., Ryu, J.-M., Yu, H.-S., Jung, J.-H., Lee, H.,
    Kang, S.-Y., Sohn, Y.-S., Choi, J.-H., Bae, Y.-C., Jang, S.-J., and Jin, G. A
    1.2 V 20 nm 307 GB/s HBM DRAM With At-Speed Wafer-Level IO Test Scheme and Adaptive
    Refresh Considering Temperature Distribution. IEEE Journal of Solid-State Circuits
    52, 1 (2017), 250–260.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (248) Song, J., Cho, Y., Park, J.-S., Jang, J.-W., Lee, S., Song, J.-H., Lee,
    J.-G., and Kang, I. An 11.5TOPS/W 1024-MAC Butterfly Structure Dual-Core Sparsity-Aware
    Neural Processing Unit in 8nm Flagship Mobile SoC. In 2019 IEEE International
    Solid- State Circuits Conference - (ISSCC) (2019), pp. 130–132.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(249) Song, L., Qian, X., Li, H., and Chen, Y. PipeLayer: A Pipelined ReRAM-Based
    Accelerator for Deep Learning. In 2017 IEEE International Symposium on High Performance
    Computer Architecture (HPCA) (2017), pp. 541–552.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(250) Soussan, P., Sabuncuoglu Tezcan, D., Iker, F., Ruythooren, W., Swinnen,
    B., Majeed, B., and Beyne, E. 3D Wafer Level Packaging: Processes and Materials
    for Trough Silicon Vias & Thin Die Embedding. In MRS Online Proceedings Library
    (01 2008), vol. 1112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (251) Spagnolo, F., Corsonello, P., Frustaci, F., and Perri, S. Design of a
    Low-Power Super-Resolution Architecture for Virtual Reality Wearable Devices.
    IEEE Sensors Journal 23, 8 (2023), 9009–9016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (252) Spagnolo, F., Perri, S., and Corsonello, P. Design of a real-time face
    detection architecture for heterogeneous systems-on-chips. Integration 74 (2020),
    1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(253) Spagnolo, F., Perri, S., and Corsonello, P. Aggressive Approximation
    of the SoftMax Function for Power-Efficient Hardware Implementations. IEEE Transactions
    on Circuits and Systems II: Express Briefs 69, 3 (2022), 1652–1656.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (254) Spagnolo, F., Perri, S., and Corsonello, P. Approximate Down-Sampling
    Strategy for Power-Constrained Intelligent Systems. IEEE Access 10 (2022), 7073–7081.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (255) Srinivas, S., and Babu, R. V. Data-free Parameter Pruning for Deep Neural
    Networks. CoRR abs/1507.06149 (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (256) Sriram, V., Cox, D., Tsoi, K., and Luk, W. Towards an embedded biologically-inspired
    machine vision processor. In 2010 International Conference on Field-Programmable
    Technology (01 2011), pp. 273–278.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (257) Stow, D., Akgun, I., Barnes, R., Gu, P., and Xie, Y. Cost analysis and
    cost-driven IP reuse methodology for SoC design based on 2.5D/3D integration.
    In 2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD) (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (258) Stow, D., Xie, Y., Siddiqua, T., and Loh, G. H. Cost-effective design
    of scalable high-performance systems using active and passive interposers. In
    2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD) (2017),
    pp. 728–735.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(259) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., and Di Meo, G.
    Comparison and Extension of Approximate 4-2 Compressors for Low-Power Approximate
    Multipliers. IEEE Transactions on Circuits and Systems I: Regular Papers 67, 9
    (2020), 3021–3034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(260) Strollo, A. G. M., Napoli, E., De Caro, D., Petra, N., Saggese, G., and
    Di Meo, G. Approximate Multipliers Using Static Segmentation: Error Analysis and
    Improvements. IEEE Transactions on Circuits and Systems I: Regular Papers 69,
    6 (2022), 2449–2462.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(261) Sze, V., Chen, Y. H., Yang, T. J., and Emer, J. S. Efficient Processing
    of Deep Neural Networks: A Tutorial and Survey. Proceedings of the IEEE 105, 12
    (2017), 2295–2329.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (262) Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan,
    D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In 2015
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015), pp. 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (263) Tambe, T., Zhang, J., Hooper, C., Jia, T., Whatmough, P. N., Zuckerman,
    J., Santos, M. C. D., Loscalzo, E. J., Giri, D., Shepard, K., Carloni, L., Rush,
    A., Brooks, D., and Wei, G.-Y. 22.9 A 12nm 18.1TFLOPs/W Sparse Transformer Processor
    with Entropy-Based Early Exit, Mixed-Precision Predication and Fine-Grained Power
    Management. In 2023 IEEE International Solid- State Circuits Conference (ISSCC)
    (San Francisco, CA, USA, Feb. 2023), IEEE, pp. 342–344.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(264) Tang, W., and Zhang, P. GPGCN: A General-Purpose Graph Convolution Neural
    Network Accelerator Based on RISC-V ISA Extension. Electronics 11, 22 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(265) Tortorella, Y., Bertaccini, L., Benini, L., Rossi, D., and Conti, F.
    RedMule: A Mixed-Precision Matrix-Matrix Operation Engine for Flexible and Energy-Efficient
    On-Chip Linear Algebra and TinyML Training Acceleration. CoRR abs/2301.03904,
    arXiv:2301.03904 (Jan. 2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(266) Tortorella, Y., Bertaccini, L., Rossi, D., Benini, L., and Conti, F.
    RedMulE: A Compact FP16 Matrix-Multiplication Accelerator for Adaptive Deep Learning
    on RISC-V-based Ultra-Low-Power SoCs. In 2022 Conference & Exhibition on Design,
    Automation & Test in Europe (May 2022), European Design and Automation Association,
    pp. 1099–1102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(267) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., and Lample, G. LLaMA: Open and Efficient Foundation Language Models,
    Feb. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(268) Trabelsi Ajili, M., and Hara-Azumi, Y. Multimodal Neural Network Acceleration
    on a Hybrid CPU-FPGA Architecture: A Case Study. IEEE Access 10 (2022), 9603–9617.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(269) Tsao, Y.-C., and Choi, K. Area-Efficient VLSI Implementation for Parallel
    Linear-Phase FIR Digital Filters of Odd Length Based on Fast FIR Algorithm. IEEE
    Transactions on Circuits and Systems II: Express Briefs 59, 6 (2012), 371–375.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(270) Umuroglu, Y., Fraser, N. J., Gambardella, G., Blott, M., Leong, P., Jahre,
    M., and Vissers, K. FINN: A Framework for Fast, Scalable Binarized Neural Network
    Inference. In 2017 ACM/SIGDA International Symposium on Field-Programmable Gate
    Arrays (2017), ACM, p. 65–74.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(271) Upama, P. B., Faruk, M. J. H., Nazim, M., Masum, M., Shahriar, H., Uddin,
    G., Barzanjeh, S., Ahamed, S. I., and Rahman, A. Evolution of Quantum Computing:
    A Systematic Survey on the Use of Quantum Computing Tools, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (272) Ushiroyama, A., Watanabe, M., Watanabe, N., and Nagoya, A. Convolutional
    neural network implementations using Vitis AI. In 2022 IEEE 12th Annual Computing
    and Communication Workshop and Conference (CCWC) (2022), pp. 0365–0371.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(273) Vahdat, S., Kamal, M., Afzali-Kusha, A., and Pedram, M. TOSAM: An Energy-Efficient
    Truncation- and Rounding-Based Scalable Approximate Multiplier. IEEE Transactions
    on Very Large Scale Integration (VLSI) Systems 27, 5 (2019), 1161–1173.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (274) Vandendriessche, J., Da Silva, B., and Touhafi, A. Frequency Evaluation
    of the Xilinx DPU Towards Energy Efficiency. In IECON 2022 – 48th Annual Conference
    of the IEEE Industrial Electronics Society (2022), pp. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (275) Vasiljevic, J., Bajic, L., Capalija, D., Sokorac, S., Ignjatovic, D.,
    Bajic, L., Trajkovic, M., Hamer, I., Matosevic, I., Cejkov, A., Aydonat, U., Zhou,
    T., Gilani, S. Z., Paiva, A., Chu, J., Maksimovic, D., Chin, S. A., Moudallal,
    Z., Rakhmati, A., Nijjar, S., Bhullar, A., Drazic, B., Lee, C., Sun, J., Kwong,
    K.-M., Connolly, J., Dooley, M., Farooq, H., Chen, J. Y. T., Walker, M., Dabiri,
    K., Mabee, K., Lal, R. S., Rajatheva, N., Retnamma, R., Karodi, S., Rosen, D.,
    Munoz, E., Lewycky, A., Knezevic, A., Kim, R., Rui, A., Drouillard, A., and Thompson,
    D. Compute Substrate for Software 2.0. IEEE Micro 41, 2 (Mar. 2021), 50–55.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (276) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., Kaiser, Ł., and Polosukhin, I. Attention is All you Need. In Advances in
    Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio,
    H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran Associates,
    Inc., 2017, pp. 5998–6008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(277) Venieris, S. I., and Bouganis, C.-S. fpgaConvNet: Mapping Regular and
    Irregular Convolutional Neural Networks on FPGAs. IEEE Transactions on Neural
    Networks and Learning Systems 30, 2 (2019), 326–342.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(278) Venieris, S. I., Kouris, A., and Bouganis, C.-S. Toolflows for mapping
    convolutional neural networks on FPGAs: A survey and future directions. ACM Computing
    Surveys (CSUR) 51, 3 (2018), 1–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(279) Venkataramani, S., Srinivasan, V., Wang, W., Sen, S., Zhang, J., Agrawal,
    A., Kar, M., Jain, S., Mannari, A., Tran, H., Li, Y., Ogawa, E., Ishizaki, K.,
    Inoue, H., Schaal, M., Serrano, M., Choi, J., Sun, X., Wang, N., Chen, C.-Y.,
    Allain, A., Bonano, J., Cao, N., Casatuta, R., Cohen, M., Fleischer, B., Guillorn,
    M., Haynie, H., Jung, J., Kang, M., Kim, K.-h., Koswatta, S., Lee, S., Lutz, M.,
    Mueller, S., Oh, J., Ranjan, A., Ren, Z., Rider, S., Schelm, K., Scheuermann,
    M., Silberman, J., Yang, J., Zalani, V., Zhang, X., Zhou, C., Ziegler, M., Shah,
    V., Ohara, M., Lu, P.-F., Curran, B., Shukla, S., Chang, L., and Gopalakrishnan,
    K. RaPiD: AI Accelerator for Ultra-low Precision Training and Inference. In 2021
    ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) (June
    2021), pp. 153–166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(280) Ventana Micro. [https://www.ventanamicro.com/](https://www.ventanamicro.com/),
    2023. Accessed: 2023-04-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(281) Verhelst, M., Shi, M., and Mei, L. ML Processors Are Going Multi-Core:
    A performance dream or a scheduling nightmare? IEEE Solid-State Circuits Magazine
    14, 4 (2022), 18–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (282) Vijayaraghavan, T., Eckert, Y., Loh, G. H., Schulte, M. J., Ignatowski,
    M., Beckmann, B. M., Brantley, W. C., Greathouse, J. L., Huang, W., Karunanithi,
    A., Kayiran, O., Meswani, M., Paul, I., Poremba, M., Raasch, S., Reinhardt, S. K.,
    Sadowski, G., and Sridharan, V. Design and Analysis of an APU for Exascale Computing.
    In 2017 IEEE International Symposium on High Performance Computer Architecture
    (HPCA) (2017), pp. 85–96.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(283) Vivet, P., Guthmuller, E., Thonnart, Y., Pillonnet, G., Fuguet, C., Miro-Panades,
    I., Moritz, G., Durupt, J., Bernard, C., Varreau, D., Pontes, J., Thuries, S.,
    Coriat, D., Harrand, M., Dutoit, D., Lattard, D., Arnaud, L., Charbonnier, J.,
    Coudrain, P., Garnier, A., Berger, F., Gueugnot, A., Greiner, A., Meunier, Q. L.,
    Farcy, A., Arriordaz, A., Chéramy, S., and Clermidy, F. IntAct: A 96-Core Processor
    With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects
    and Integrated Power Management. IEEE Journal of Solid-State Circuits 56, 1 (2021),
    79–97.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (284) Wan, W., Kubendran, R., Schaefer, C., Eryilmaz, S. B., Zhang, W., Wu,
    D., Deiss, S., Raina, P., Qian, H., Gao, B., Joshi, S., Wu, H., Wong, H.-S. P.,
    and Cauwenberghs, G. A compute-in-memory chip based on resistive random-access
    memory. Nature 608, 7923 (Aug 2022), 504–512.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(285) Wang, H., Xu, W., Zhang, Z., You, X., and Zhang, C. An Efficient Stochastic
    Convolution Architecture Based on Fast FIR Algorithm. IEEE Transactions on Circuits
    and Systems II: Express Briefs 69, 3 (2022), 984–988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (286) Wang, J., and Gu, S. FPGA Implementation of Object Detection Accelerator
    Based on Vitis-AI. In 2021 11th International Conference on Information Science
    and Technology (ICIST) (2021), pp. 571–577.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(287) Wang, J., Lin, J., and Wang, Z. Efficient Hardware Architectures for
    Deep Convolutional Neural Network. IEEE Transactions on Circuits and Systems I:
    Regular Papers 65, 6 (2018), 1941–1953.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (288) Wang, S., Zhu, J., Wang, Q., He, C., and Ye, T. T. Customized Instruction
    on RISC-V for Winograd-Based Convolution Acceleration. In 2021 IEEE 32nd International
    Conference on Application-specific Systems, Architectures and Processors (ASAP)
    (2021), pp. 65–68.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(289) Wang, Y., Lin, J., and Wang, Z. FPAP: A Folded Architecture for Energy-Quality
    Scalable Convolutional Neural Networks. IEEE Transactions on Circuits and Systems
    I: Regular Papers 66 (2019), 288–301.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (290) Ward-Foxton, S. Axelera Demos AI Test Chip After Taping Out in Four Months,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (291) Warden, P., and Situnayake, D. TinyML. O’Reilly Media, Inc., 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (292) Wei, X., Yu, C. H., Zhang, P., Chen, Y., Wang, Y., Hu, H., Liang, Y.,
    and Cong, J. Automated systolic array architecture synthesis for high throughput
    CNN inference on FPGAs. In 2017 54th ACM/EDAC/IEEE Design Automation Conference
    (DAC) (2017), pp. 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (293) Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H. Learning Structured Sparsity
    in Deep Neural Networks. In Proceedings of the 30th International Conference on
    Neural Information Processing Systems (Red Hook, NY, USA, 2016), NIPS’16, Curran
    Associates Inc., pp. 2082–2090.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (294) Xilinx Inc. Vitis High-Level Synthesis User Guide, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(295) Xuan, Z. Y., Lee, C.-J., and Yeh, T. T. Lego: Dynamic Tensor-Splitting
    Multi-Tenant DNN Models on Multi-Chip-Module Architecture. In 2022 19th International
    SoC Design Conference (ISOCC) (2022), pp. 173–174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (296) Xue, C.-X., Chen, W.-H., Liu, J.-S., Li, J.-F., Lin, W.-Y., Lin, W.-E.,
    Wang, J.-H., Wei, W.-C., Chang, T.-W., Chang, T.-C., Huang, T.-Y., Kao, H.-Y.,
    Wei, S.-Y., Chiu, Y.-C., Lee, C.-Y., Lo, C.-C., King, Y.-C., Lin, C.-J., Liu,
    R.-S., Hsieh, C.-C., Tang, K.-T., and Chang, M.-F. A 1Mb Multibit ReRAM Computing-In-Memory
    Macro with 14.6ns Parallel MAC Computing Time for CNN Based AI Edge Processors.
    In 2019 IEEE International Solid- State Circuits Conference - (ISSCC) (2019),
    pp. 388–390.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (297) Yang, Z., Han, J., and Lombardi, F. Approximate compressors for error-resilient
    multiplier design. In 2015 IEEE International Symposium on Defect and Fault Tolerance
    in VLSI and Nanotechnology Systems (DFTS) (2015), pp. 183–186.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(298) Yazdanbakhsh, A., Samadi, K., Kim, N. S., Esmaeilzadeh, H., Falahati,
    H., and Wolfe, P. J. GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial
    Networks. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture
    (ISCA) (2018), pp. 650–661.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(299) Ye, H., Jun, H., Jeong, H., Neuendorffer, S., and Chen, D. ScaleHLS:
    A Scalable High-Level Synthesis Framework with Multi-Level Transformations and
    Optimizations. In Proceedings of the 59th ACM/IEEE Design Automation Conference
    (DAC) (2022), pp. 1355–1358.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (300) Zacharelos, E., Nunziata, I., Saggese, G., Strollo, A. G., and Napoli,
    E. Approximate Recursive Multipliers Using Low Power Building Blocks. IEEE Transactions
    on Emerging Topics in Computing 10, 3 (2022), 1315–1330.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(301) Zaruba, F., Schuiki, F., and Benini, L. Manticore: A 4096-Core RISC-V
    Chiplet Architecture for Ultraefficient Floating-Point Computing. IEEE Micro 41,
    2 (Mar. 2021), 36–42.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (302) Zervakis, G., Tsoumanis, K., Xydis, S., Soudris, D., and Pekmestzi, K.
    Design-Efficient Approximate Multiplication Circuits Through Partial Product Perforation.
    IEEE Transactions on Very Large Scale Integration (VLSI) Systems 24, 10 (2016),
    3105–3117.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (303) Zhang, C., Wu, D., Sun, J., Sun, G., Luo, G., and Cong, J. Energy-Efficient
    CNN Implementation on a Deeply Pipelined FPGA Cluster. In 2016 International Symposium
    on Low Power Electronics and Design (2016), ACM, p. 326–331.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(304) Zhang, J.-F., Lee, C.-E., Liu, C., Shao, Y. S., Keckler, S. W., and Zhang,
    Z. SNAP: A 1.67 — 21.55TOPS/W Sparse Neural Acceleration Processor for Unstructured
    Sparse Deep Neural Network Inference in 16nm CMOS. In 2019 Symposium on VLSI Circuits
    (2019), pp. C306–C307.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(305) Zhang, S., Du, Z., Zhang, L., Lan, H., Liu, S., Li, L., Guo, Q., Chen,
    T., and Chen, Y. Cambricon-X: An accelerator for sparse neural networks. In 2016
    49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO) (2016),
    MICRO-49, pp. 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (306) Zhang, X., Lin, J. K., Wickramanayaka, S., Zhang, S., Weerasekera, R.,
    Dutta, R., Chang, K. F., Chui, K.-J., Li, H. Y., Wee Ho, D. S., Ding, L., Katti,
    G., Bhattacharya, S., and Kwong, D.-L. Heterogeneous 2.5D integration on through
    silicon interposer. Applied Physics Reviews 2, 2 (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(307) Zhao, Y., Liu, C., Du, Z., Guo, Q., Hu, X., Zhuang, Y., Zhang, Z., Song,
    X., Li, W., Zhang, X., Li, L., Xu, Z., and Chen, T. Cambricon-Q: A Hybrid Architecture
    for Efficient Training. In 2021 ACM/IEEE 48th Annual International Symposium on
    Computer Architecture (ISCA) (June 2021), pp. 706–719.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (308) Zimmer, B., Venkatesan, R., Shao, Y. S., Clemons, J., Fojtik, M., Jiang,
    N., Keller, B., Klinefelter, A., Pinckney, N., Raina, P., Tell, S. G., Zhang,
    Y., Dally, W. J., Emer, J. S., Gray, C. T., Keckler, S. W., and Khailany, B. A
    0.32–128 TOPS, Scalable Multi-Chip-Module-Based Deep Neural Network Inference
    Accelerator With Ground-Referenced Signaling in 16 nm. IEEE Journal of Solid-State
    Circuits 55, 4 (2020), 920–932.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
