- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:56:56'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:56:56'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2101.11282] Deep Learning for Instance Retrieval: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2101.11282] 深度学习实例检索：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2101.11282](https://ar5iv.labs.arxiv.org/html/2101.11282)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2101.11282](https://ar5iv.labs.arxiv.org/html/2101.11282)
- en: 'Deep Learning for Instance Retrieval: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习实例检索：综述
- en: Wei Chen, Yu Liu, Weiping Wang, Erwin M. Bakker, Theodoros Georgiou,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 魏晨，刘宇，王伟平，厄尔温·M·巴克，西奥多罗斯·乔治欧，
- en: 'Paul Fieguth, Li Liu, and Michael S. Lew W. Chen is with Academy of Advanced
    Technology Research of Hunan, Changsha, China Y. Liu is with DUTRU International
    School of Information Science and Engineering, Dalian University of Technology,
    China.W. Wang is with Academy of Advanced Technology Research of Hunan, Changsha.E.
    Bakker, T. Georgiou, and M. Lew are with Leiden Institute of Advanced Computer
    Science, Leiden University, the Netherlands.P. Fieguth is with the Department
    of Systems Design Engineering, University of Waterloo, Canada.L. Liu is with Academy
    of Advanced Technology Research of Hunan, Changsha, China, and also with Center
    for Machine Vision and Signal Analysis, University of Oulu, Finland.Corresponding
    author: Li Liu, li.liu@oulu.fi, dreamliu2010@gmail.comThis work was supported
    by China Scholarship Council (No. 201703170183), the Academy of Finland under
    grant 331883, Infotech Project FRAGES, and the National Natural Science Foundation
    of China under Grant 61872379, 62022091, 61825305, and 62102061.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 保罗·费古特，刘莉，和迈克尔·S·刘。魏晨来自中国长沙湖南省先进技术研究院；刘宇来自中国大连理工大学DUTRU国际信息科学与工程学院。王伟平来自中国长沙湖南省先进技术研究院。厄尔温·巴克、西奥多罗斯·乔治欧和迈克尔·刘来自荷兰莱顿大学莱顿先进计算机科学研究所。保罗·费古特来自加拿大滑铁卢大学系统设计工程系。刘莉来自中国长沙湖南省先进技术研究院，同时也是芬兰奥卢大学机器视觉与信号分析中心的成员。通讯作者：刘莉，li.liu@oulu.fi，dreamliu2010@gmail.com。此项工作得到了中国国家留学基金委员会（编号：201703170183）、芬兰高级研究院资助（资助号：331883）、信息技术项目FRAGES以及中国国家自然科学基金（资助号：61872379，62022091，61825305和62102061）的支持。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years a vast amount of visual content has been generated and shared
    from many fields, such as social media platforms, medical imaging, and robotics.
    This abundance of content creation and sharing has introduced new challenges,
    particularly that of searching databases for similar content — Content Based Image
    Retrieval (CBIR) — a long-established research area in which improved efficiency
    and accuracy are needed for real-time retrieval. Artificial intelligence has made
    progress in CBIR and has significantly facilitated the process of instance search.
    In this survey we review recent instance retrieval works that are developed based
    on deep learning algorithms and techniques, with the survey organized by deep
    feature extraction, feature embedding and aggregation methods, and network fine-tuning
    strategies. Our survey considers a wide variety of recent methods, whereby we
    identify milestone work, reveal connections among various methods and present
    the commonly used benchmarks, evaluation results, common challenges, and propose
    promising future directions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，来自多个领域的大量视觉内容被生成和分享，例如社交媒体平台、医学成像和机器人技术。这种内容创建和分享的丰富性带来了新的挑战，特别是在搜索数据库中的类似内容方面——内容基础图像检索（CBIR）——这是一个需要提高实时检索效率和准确性的长期研究领域。人工智能在CBIR方面取得了进展，并显著促进了实例搜索的过程。在本次调查中，我们回顾了基于深度学习算法和技术开发的最新实例检索工作，调查内容按深度特征提取、特征嵌入和聚合方法以及网络微调策略进行组织。我们的调查考虑了各种最新方法，通过识别重要的工作，揭示不同方法之间的联系，介绍常用的基准、评估结果、常见挑战，并提出了有前景的未来方向。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Content based image retrieval, Instance retrieval, Deep learning, Convolutional
    neural networks, Literature survey
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内容的图像检索，实例检索，深度学习，卷积神经网络，文献综述
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Content Based Image Retrieval (CBIR) is the problem of searching for relevant
    images in an image gallery by analyzing the visual content (colors, textures,
    shapes, objects *etc.*), given a query image [[1](#bib.bib1)],[[2](#bib.bib2)].
    CBIR has been a longstanding research topic in the fields of computer vision and
    multimedia [[1](#bib.bib1)],[[2](#bib.bib2)]. With the exponential growth of image
    data, the development of appropriate information systems that efficiently manage
    such large image collections is of utmost importance, with image searching being
    one of the most indispensable techniques. Thus there is a nearly endless potential
    for applications of CBIR, such as person/vehicle reidentification [[3](#bib.bib3)],[[4](#bib.bib4)],
    landmark retrieval [[5](#bib.bib5)], remote sensing [[6](#bib.bib6)], online product
    searching [[7](#bib.bib7)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内容的图像检索（CBIR）是通过分析视觉内容（颜色、纹理、形状、物体*等*）在图像库中搜索相关图像的问题，给定一个查询图像 [[1](#bib.bib1)],[[2](#bib.bib2)]。CBIR
    一直是计算机视觉和多媒体领域的一个长期研究课题 [[1](#bib.bib1)],[[2](#bib.bib2)]。随着图像数据的指数增长，开发适当的信息系统来有效管理如此庞大的图像集合至关重要，其中图像搜索是最不可或缺的技术之一。因此，CBIR
    的应用潜力几乎是无限的，如人员/车辆重新识别 [[3](#bib.bib3)],[[4](#bib.bib4)]，地标检索 [[5](#bib.bib5)]，遥感
    [[6](#bib.bib6)]，在线产品搜索 [[7](#bib.bib7)]。
- en: 'Generally, CBIR methods can be grouped into two different tasks [[8](#bib.bib8)],[[9](#bib.bib9)]:
    Category level Image Retrieval (CIR) and Instance level Image Retrieval (IIR).
    The goal of CIR is to find an arbitrary image representative of the same category
    as the query (e.g., dogs, cars) [[10](#bib.bib10)],[[11](#bib.bib11)]. By contrast,
    in the IIR task, a query image of a particular instance (e.g., the Eiffel Tower,
    my neighbor’s dog) is given and the goal is to find images containing the same
    instance that may be captured under different conditions like different imaging
    distances, viewing angles, backgrounds, illuminations, and weather conditions
    (reidentifying exemplars of the same instance) [[12](#bib.bib12)],[[13](#bib.bib13)].
    The focus of this survey is the IIR task¹¹1If not further specified, “image retrieval”,
    ”IIR”, and “instance retrieval” are considered equivalent and will be used interchangeably..'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，内容基于图像检索（CBIR）方法可以分为两个不同的任务 [[8](#bib.bib8)],[[9](#bib.bib9)]: 分类级图像检索（CIR）和实例级图像检索（IIR）。CIR
    的目标是找到一个代表与查询相同类别的任意图像（例如，狗，汽车） [[10](#bib.bib10)],[[11](#bib.bib11)]。相比之下，在 IIR
    任务中，给定一个特定实例的查询图像（例如，埃菲尔铁塔，我邻居的狗），目标是找到包含相同实例的图像，这些图像可能是在不同条件下拍摄的，比如不同的成像距离、视角、背景、照明和天气条件（重新识别相同实例的样本）
    [[12](#bib.bib12)],[[13](#bib.bib13)]。本调查的重点是 IIR 任务¹¹1如果没有进一步说明，“图像检索”，“IIR”和“实例检索”被视为等效的，并将互换使用。。'
- en: In many real world applications, IIR is usually to find a desired image requiring
    a search among thousands, millions, or even billions of images. Hence, searching
    efficiently is as critical as searching accurately, to which continued efforts
    have been devoted [[12](#bib.bib12)],[[14](#bib.bib14)],[[15](#bib.bib15)]. To
    enable accurate and efficient retrieval over a large-scale image collection, *developing
    compact yet discriminative feature representations* is at the core of IIR.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实应用中，IIR 通常需要在数千、数百万甚至数十亿的图像中找到所需的图像。因此，高效搜索与准确搜索一样关键，为此做出了持续的努力 [[12](#bib.bib12)],[[14](#bib.bib14)],[[15](#bib.bib15)]。为了在大规模图像集合中实现准确和高效的检索，*开发紧凑而具有区分性的特征表示*
    是 IIR 的核心。
- en: 'TABLE I: A summary and comparison of the primary surveys in the field of image
    retrieval.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 图像检索领域主要调查的总结与比较。'
- en: '|        Title | Year | Published in | Main Content |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|        标题 | 年份 | 发表期刊 | 主要内容 |'
- en: '|      Content-Based Image Retrieval at the End of the Early Years [[1](#bib.bib1)]
    | 2000 | TPAMI | This paper discusses the steps for image retrieval systems, including
    image processing, feature extraction, user interaction, and similarity evaluation.
         |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|      早期年份末的基于内容的图像检索 [[1](#bib.bib1)] | 2000 | TPAMI | 本文讨论了图像检索系统的步骤，包括图像处理、特征提取、用户交互和相似性评估。
         |'
- en: '|      Image Search from Thousands to Billions in 20 Years [[16](#bib.bib16)]
    | 2013 | TOMM | This paper gives a good presentation of image search achievements
    from 1970 to 2013, but the methods are not deep learning-based.      |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|      从数千到数十亿的图像搜索 [[16](#bib.bib16)] | 2013 | TOMM | 本文很好地展示了从 1970 年到 2013
    年的图像搜索成就，但这些方法并非基于深度学习。      |'
- en: '|      Deep Learning for Content-Based Image Retrieval: A Comprehensive Study
    [[17](#bib.bib17)] | 2014 | ACM MM | This paper introduces supervised metric learning
    methods for fine-tuning AlexNet. Details of instance-based image retrieval are
    limited.      |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|      《基于内容的图像检索中的深度学习：全面研究》[[17](#bib.bib17)] | 2014 | ACM MM | 本文介绍了用于微调AlexNet的监督度量学习方法。关于基于实例的图像检索的细节有限。'
- en: '|      Semantic Content-based Image Retrieval: A Comprehensive Study [[18](#bib.bib18)]
    | 2015 | JVCI | This paper presents a comprehensive study about CBIR using traditional
    methods; deep learning is introduced as a section with limited details.      |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|      《基于语义的内容图像检索：全面研究》[[18](#bib.bib18)] | 2015 | JVCI | 本文对使用传统方法的CBIR进行了全面研究；引入了深度学习作为一个部分，但细节有限。
         |'
- en: '|      Socializing the Semantic Gap: A Compa- rative Survey on Image Tag Assignment,
    Refinement, and Retrieval [[19](#bib.bib19)] | 2016 | CSUR | A taxonomy is introduced
    to structure the growing literature of image retrieval. Deep learning methods
    for feature learning is introduced as future work.      |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|      《社交语义差距：图像标签分配、细化和检索的比较调查》[[19](#bib.bib19)] | 2016 | CSUR | 介绍了一种分类法来结构化日益增长的图像检索文献。将深度学习方法用于特征学习作为未来工作介绍。
         |'
- en: '|      Recent Advance in Content based Image Retrieval: A Literature Survey
    [[20](#bib.bib20)] | 2017 | arXiv | This survey presents image retrieval from
    2003 to 2016\. Neural networks are introduced in a section and mainly discussed
    as a future direction.      |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|      《基于内容的图像检索的最新进展：文献综述》[[20](#bib.bib20)] | 2017 | arXiv | 本调查呈现了2003年至2016年的图像检索研究。一个部分介绍了神经网络，并主要讨论了未来的发展方向。
         |'
- en: '|      Information Fusion in Content-based Image Retrieval: A Comprehensive
    Overview [[21](#bib.bib21)] | 2017 | Information Fusion | This paper presents
    information fusion strategies in CBIR. Deep convolutional networks for feature
    learning are introduced briefly but not covered thoroughly.      |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|      《基于内容的图像检索中的信息融合：全面概述》[[21](#bib.bib21)] | 2017 | Information Fusion
    | 本文介绍了CBIR中的信息融合策略。简要介绍了用于特征学习的深度卷积网络，但未做深入覆盖。      |'
- en: '|      A Survey on Learning to Hash [[22](#bib.bib22)] | 2018 | TPAMI | This
    paper focuses on hash learning algorithms and introduces the similarity-preserving
    methods and discusses their relationships.      |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|      《学习哈希的调查》[[22](#bib.bib22)] | 2018 | TPAMI | 本文关注于哈希学习算法，并介绍了相似性保留方法及其关系。
         |'
- en: '|      SIFT Meets CNN: A Decade Survey of Instance Retrieval [[13](#bib.bib13)]
    | 2018 | TPAMI | This paper presents a comprehensive review of instance retrieval
    based on SIFT and CNN methods.      |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|      《SIFT与CNN的结合：实例检索的十年综述》[[13](#bib.bib13)] | 2018 | TPAMI | 本文对基于SIFT和CNN的方法的实例检索进行了全面回顾。
         |'
- en: '|      Deep Learning for Instance Retrieval: A Survey | 2021 | Ours | Our survey
    focuses on deep learning methods. We expand the review with indepth details on
    IIR, including methods of feature extraction, feature embedding and aggregation,
    and network fine-tuning.      |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|      《用于实例检索的深度学习：调查》 | 2021 | Ours | 我们的调查专注于深度学习方法。我们扩展了对IIR的回顾，深入探讨了特征提取、特征嵌入和聚合以及网络微调的方法。
         |'
- en: '|        |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|        |  |'
- en: During the past two decades, startling progress has been witnessed in image
    representation which mainly consists of two important periods, i.e., feature engineering
    and deep learning. In the feature engineering era, the field was dominated by
    various milestone handcrafted image representations like SIFT [[23](#bib.bib23)]
    and Bag of visual Words (BoW) [[24](#bib.bib24)]. The deep learning era was reignited
    in 2012 when a Deep Convolutional Neural Network (DCNN) referred as “AlexNet”
    [[25](#bib.bib25)] won the first place in the ImageNet classification contest
    with a breakthrough reduction in classification error rate. Since then, the dominating
    role of SIFT like local descriptors has been replaced by data driven Deep Neural
    Networks (DNNs) which can learn powerful feature representations with multiple
    levels of abstraction directly from data. During the past decade, DNNs have set
    the state of the art in various classical computer vision tasks, including image
    classification [[25](#bib.bib25)],[[26](#bib.bib26)], object detection [[27](#bib.bib27)],
    semantic segmentation [[28](#bib.bib28)], and image retrieval [[17](#bib.bib17)].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去二十年中，图像表示领域取得了令人瞩目的进展，主要包括两个重要时期，即特征工程和深度学习。在特征工程时代，领域由各种里程碑式的手工图像表示主导，如SIFT
    [[23](#bib.bib23)]和视觉词袋（BoW）[[24](#bib.bib24)]。深度学习时代在2012年重新点燃，当时被称为“AlexNet”[[25](#bib.bib25)]的深度卷积神经网络（DCNN）在ImageNet分类比赛中以突破性的分类错误率降低赢得了第一名。从那时起，像SIFT这样的局部描述符的主导地位被数据驱动的深度神经网络（DNNs）取代，后者可以直接从数据中学习具有多个抽象层次的强大特征表示。在过去十年中，DNNs在各种经典计算机视觉任务中设立了最新的技术水平，包括图像分类[[25](#bib.bib25)],[[26](#bib.bib26)]，目标检测[[27](#bib.bib27)]，语义分割[[28](#bib.bib28)]，和图像检索[[17](#bib.bib17)]。
- en: 'Given this period of rapid evolution, the goal of this paper is to provide
    a comprehensive survey of the recent achievements in IIR. In comparison with existing
    excellent surveys on traditional image retrieval [[13](#bib.bib13)],[[18](#bib.bib18)],[[19](#bib.bib19)],[[20](#bib.bib20)],
    as contrasted in Table [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Deep Learning for
    Instance Retrieval: A Survey"), our focus in this paper is reviewing deep learning
    based methods for IIR, particularly on issues of retrieval accuracy and efficiency.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这一时期的快速演变，本文的目标是提供对IIR近期成就的全面综述。与现有关于传统图像检索的优秀综述[[13](#bib.bib13)],[[18](#bib.bib18)],[[19](#bib.bib19)],[[20](#bib.bib20)]相比，如表[I](#S1.T1
    "表 I ‣ 1 引言 ‣ 实例检索的深度学习：综述")所示，我们在本文中的重点是回顾基于深度学习的IIR方法，特别是检索准确性和效率的问题。
- en: 1.1 Summary of Progress since 2012
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 2012年以来的进展总结
- en: 'After the highly successful image classification implementation of AlexNet [[25](#bib.bib25)],
    significant exploration of DCNNs for instance retrieval tasks was undertaken and
    representative methods are shown in Figure [1](#S1.F1 "Figure 1 ‣ 1.1 Summary
    of Progress since 2012 ‣ 1 Introduction ‣ Deep Learning for Instance Retrieval:
    A Survey"). Building on these methods, more recent progress for IIR can be achieved
    from the perspectives of off-the-shelf models and fine-tuned models, which form
    the basis for this survey.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在AlexNet的图像分类实现取得巨大成功之后[[25](#bib.bib25)]，对DCNN在实例检索任务中的探索取得了显著进展，代表性方法如图[1](#S1.F1
    "图 1 ‣ 1.1 2012年以来的进展总结 ‣ 1 引言 ‣ 实例检索的深度学习：综述")所示。在这些方法的基础上，基于现成模型和微调模型的IIR（实例检索）最新进展可以得到，这也构成了本综述的基础。
- en: Off-the-shelf models, based on DCNNs with fixed parameters [[29](#bib.bib29)],[[30](#bib.bib30)],[[31](#bib.bib31)],
    extract features at image scales or patch scales, which correspond to single-pass
    and multiple-pass schemes, respectively. These methods focus on effective feature
    use, for which researchers have proposed embedding and aggregation methods, such
    as R-MAC [[32](#bib.bib32)], CroW [[15](#bib.bib15)], and SPoC [[12](#bib.bib12)]
    to promote the discriminativity of the extracted features. Fine-tuned models,
    based on DCNNs in which parameters are updated [[29](#bib.bib29)], are more popular
    since deep networks themselves have been investigated extensively. To learn better
    retrieval features, researchers have proposed to improve the network architectures
    and/or update the pre-stored parameters [[31](#bib.bib31)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于DCNNs的现成模型，参数固定[[29](#bib.bib29)],[[30](#bib.bib30)],[[31](#bib.bib31)]，在图像尺度或补丁尺度上提取特征，这对应于单次和多次方案。这些方法侧重于有效利用特征，为此，研究人员提出了嵌入和聚合方法，如R-MAC
    [[32](#bib.bib32)]、CroW [[15](#bib.bib15)]和SPoC [[12](#bib.bib12)]，以提高提取特征的辨别能力。基于DCNNs的微调模型，其中参数会更新[[29](#bib.bib29)]，由于深度网络本身已经得到广泛研究，因此更受欢迎。为了学习更好的检索特征，研究人员提出了改进网络架构和/或更新预存参数的建议[[31](#bib.bib31)]。
- en: 'This survey will explore recent progress in detail in the context of the three
    following themes:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查将详细探讨以下三个主题的最新进展：
- en: '![Refer to caption](img/42693c608f3f80b6cfc75b891e370b76.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/42693c608f3f80b6cfc75b891e370b76.png)'
- en: 'Figure 1: Representative methods in IIR. Off-the-shelf models have model parameters
    which are not further updated or tuned when extracting retrieval features. For
    single-pass schemes, the key step is the feature embedding and aggregation to
    promote the discriminativity of the extracted image-level activations [[15](#bib.bib15)],[[33](#bib.bib33)],[[34](#bib.bib34)],[[35](#bib.bib35)],
    whereas for multiple-pass schemes the goal is to extract instance features at
    region scales and eliminate image clutter as much as possible [[30](#bib.bib30)],[[36](#bib.bib36)],[[37](#bib.bib37)],[[38](#bib.bib38)].
    In contrast, for fine-tuned models, the model parameters are tuned towards the
    retrieval task and address the issue of domain shifts. For supervised fine-tuning,
    the key step lies in the design of objective functions and sample sampling strategies
    [[39](#bib.bib39)],[[40](#bib.bib40)],[[41](#bib.bib41)],[[42](#bib.bib42)],[[43](#bib.bib43)],
    while the success of unsupervised fine-tuning is to mine the relevance among training
    samples [[44](#bib.bib44)],[[45](#bib.bib45)],[[46](#bib.bib46)],[[47](#bib.bib47)],[[48](#bib.bib48)].
    See Sections [3](#S3 "3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning
    for Instance Retrieval: A Survey") and [4](#S4 "4 Retrieval via Learning DCNN
    Representations ‣ Deep Learning for Instance Retrieval: A Survey") for details.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '图1：IIR中的代表性方法。现成模型在提取检索特征时，其模型参数不会进一步更新或调整。对于单次方案，关键步骤是特征嵌入和聚合，以提高提取的图像级激活的辨别能力[[15](#bib.bib15)],[[33](#bib.bib33)],[[34](#bib.bib34)],[[35](#bib.bib35)]，而对于多次方案，目标是在区域尺度上提取实例特征，并尽可能消除图像杂乱[[30](#bib.bib30)],[[36](#bib.bib36)],[[37](#bib.bib37)],[[38](#bib.bib38)]。相比之下，对于微调模型，模型参数会根据检索任务进行调整，并解决领域迁移问题。对于监督微调，关键步骤在于目标函数的设计和样本采样策略[[39](#bib.bib39)],[[40](#bib.bib40)],[[41](#bib.bib41)],[[42](#bib.bib42)],[[43](#bib.bib43)]，而无监督微调的成功在于挖掘训练样本之间的相关性[[44](#bib.bib44)],[[45](#bib.bib45)],[[46](#bib.bib46)],[[47](#bib.bib47)],[[48](#bib.bib48)]。有关详细信息，请参见第[3](#S3
    "3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey")节和第[4](#S4 "4 Retrieval via Learning DCNN Representations ‣ Deep Learning
    for Instance Retrieval: A Survey")节。'
- en: '(1) *Deep Feature Extraction* (Section [3.1](#S3.SS1 "3.1 Deep Feature Extraction
    ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey"))'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*(深度特征提取)*（第[3.1节](#S3.SS1 "3.1 Deep Feature Extraction ‣ 3 Retrieval with
    Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey")）'
- en: One key step in IIR is to make the descriptors as semantic-aware [[26](#bib.bib26)],[[49](#bib.bib49)]
    as possible. For this, some recent work focus on the input data of DCNNs, thereby
    instance features can be extracted from the whole image, e.g., CroW [[15](#bib.bib15)],
    VLAD-CNN [[50](#bib.bib50)] or from image patches, e.g., MOP-CNN [[30](#bib.bib30)],
    FAemb [[38](#bib.bib38)]. For instance, evaluated on the Holidays dataset [[51](#bib.bib51)],
    patch-level input scheme can improve mAP by 8.29% compared to the results (70.53%)
    achieved using image-level input [[30](#bib.bib30)]. Others focus on exploring
    different feature extractors, e.g., one layer of a given DCNN, to get the output
    activations. Initially, fully-connected layers are usually chosen to extract features
    [[52](#bib.bib52)],[[53](#bib.bib53)], and then convolutional layers are popularly
    used [[12](#bib.bib12)],[[32](#bib.bib32)]. Afterwards, some work leverage the
    complementarity of different extractors to explore layer-level fusion, such as
    MoF [[36](#bib.bib36)], and model-level fusion, such as DeepIndex [[52](#bib.bib52)]
    to promote the retrieval performance.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: IIR中的一个关键步骤是使描述符尽可能具有语义意识[[26](#bib.bib26)],[[49](#bib.bib49)]。为此，一些最新的研究集中在DCNN的输入数据上，从而可以从整张图像中提取实例特征，例如，CroW
    [[15](#bib.bib15)]、VLAD-CNN [[50](#bib.bib50)]，或者从图像补丁中提取，例如，MOP-CNN [[30](#bib.bib30)]、FAemb
    [[38](#bib.bib38)]。例如，在Holidays数据集上进行评估[[51](#bib.bib51)]，补丁级输入方案相比于使用图像级输入[[30](#bib.bib30)]得到的结果（70.53%），可以将mAP提高8.29%。其他研究则专注于探索不同的特征提取器，例如，给定DCNN的一层，以获得输出激活。最初，通常选择全连接层来提取特征[[52](#bib.bib52)],[[53](#bib.bib53)]，然后流行地使用卷积层[[12](#bib.bib12)],[[32](#bib.bib32)]。之后，一些工作利用不同提取器的互补性来探索层级融合，例如MoF
    [[36](#bib.bib36)]，以及模型级融合，例如DeepIndex [[52](#bib.bib52)]，以提升检索性能。
- en: '(2) *Feature Embedding and Aggregation* (Section [3.2](#S3.SS2 "3.2 Feature
    Embedding and Aggregation ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep
    Learning for Instance Retrieval: A Survey"))'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '(2) *特征嵌入和聚合*（第[3.2](#S3.SS2 "3.2 Feature Embedding and Aggregation ‣ 3 Retrieval
    with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey")节）'
- en: Recent work revisit the classical embedding and aggregation methods and apply
    on deep features. Most work have a preference towards mapping individual vector
    from convolutional layer [[24](#bib.bib24)],[[54](#bib.bib54)],[[55](#bib.bib55)]
    and then aggregating into a global feature. The mapping process can be realized
    by using a pre-trained codebook (i.e., built separately), such as VLAD-CNN [[50](#bib.bib50)],
    DeepIndex [[52](#bib.bib52)] or learned as parameters during training (built simultaneously),
    such as NetVLAD [[43](#bib.bib43)], GeM-DSM [[56](#bib.bib56)]. Some work aggregate
    local features into a global one by direct pooling [[21](#bib.bib21)] or sophisticated
    pooling-based methods [[12](#bib.bib12)] without the aggregation operation, such
    as R-MAC [[32](#bib.bib32)].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究回顾了经典的嵌入和聚合方法，并将其应用于深度特征。大多数工作倾向于将卷积层的单个向量映射[[24](#bib.bib24)],[[54](#bib.bib54)],[[55](#bib.bib55)]，然后聚合成全局特征。映射过程可以通过使用预训练的代码本（即单独构建的）来实现，例如VLAD-CNN
    [[50](#bib.bib50)]、DeepIndex [[52](#bib.bib52)]，或者作为训练过程中的参数进行学习（同时构建），例如NetVLAD
    [[43](#bib.bib43)]、GeM-DSM [[56](#bib.bib56)]。一些工作通过直接池化[[21](#bib.bib21)]或复杂的池化方法[[12](#bib.bib12)]将局部特征聚合为全局特征，而不进行聚合操作，例如R-MAC
    [[32](#bib.bib32)]。
- en: '(3) *Network Fine-tuning for Learning Representations* (Section [4](#S4 "4
    Retrieval via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval:
    A Survey"))'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '(3) *网络微调以学习表征*（第[4](#S4 "4 Retrieval via Learning DCNN Representations ‣ Deep
    Learning for Instance Retrieval: A Survey")节）'
- en: 'DCNNs pretrained on source datasets for image classification are influenced
    by domain shifts when performing retrieval tasks on new datasets. Thus, it is
    necessary to fine-tune deep networks to the specific domain [[39](#bib.bib39)],
    by using supervised or unsupervised fine-tuning methods. As depicted in Figure
    [1](#S1.F1 "Figure 1 ‣ 1.1 Summary of Progress since 2012 ‣ 1 Introduction ‣ Deep
    Learning for Instance Retrieval: A Survey"), recent supervised fine-tuning methods
    focus on designing objective functions (e.g., Listwise loss [[57](#bib.bib57)])
    and sample sampling strategies, such as NetVLAD [[43](#bib.bib43)], Triplet Network
    [[42](#bib.bib42)]. Unsupervised methods focus on mining the relevance among training
    samples by using clustering, such as SfM-GeM [[46](#bib.bib46)] or manifold learning,
    such as AILIR [[58](#bib.bib58)]. Recently, convolution-free models that only
    rely on transformer layers have shown competitive performance and been used as
    a powerful alternative to DCNNs, such as IRT [[59](#bib.bib59)], reranking Transformers
    [[60](#bib.bib60)].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在源数据集上预训练的DCNNs在对新数据集执行检索任务时会受到领域偏移的影响。因此，有必要通过使用有监督或无监督的微调方法，将深度网络微调到特定领域[[39](#bib.bib39)]。如图[1](#S1.F1
    "图 1 ‣ 1.1 2012年以来进展总结 ‣ 1 引言 ‣ 实例检索的深度学习：调查")所示，最近的有监督微调方法集中在设计目标函数（例如，Listwise损失[[57](#bib.bib57)]）和样本采样策略上，如NetVLAD
    [[43](#bib.bib43)]、Triplet Network [[42](#bib.bib42)]。无监督方法则集中在通过聚类（如SfM-GeM [[46](#bib.bib46)]）或流形学习（如AILIR
    [[58](#bib.bib58)]）挖掘训练样本之间的相关性。最近，依赖于变压器层的卷积-free模型表现出竞争力，并作为DCNNs的强大替代品，如IRT
    [[59](#bib.bib59)]、重新排序变压器[[60](#bib.bib60)]。
- en: 1.2 Key Challenges
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 关键挑战
- en: 'The goal of each of the preceding three themes is to address the competing
    objectives of *accuracy* and *efficiency*, with both objectives continuing to
    present challenges:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以上三种主题的目标是解决*准确性*和*效率*这两个相互竞争的目标，这两个目标仍然持续存在挑战。
- en: 'A) Accuracy related challenges depend on the input data, the feature extractor,
    and the way in which the extracted features are processed:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: A) 与准确性相关的挑战取决于输入数据、特征提取器以及提取的特征处理方式：
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Invariance challenge: The instance in an image may be rotated, translated,
    or scaled differently, so the final features are affected by these transformations
    and retrieval accuracy may be degraded [[30](#bib.bib30)]. It is necessary to
    incorporate invariance into the IIR pipeline [[61](#bib.bib61)],[[62](#bib.bib62)].'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**不变性挑战**：图像中的实例可能会被旋转、平移或缩放，因此最终特征会受到这些变换的影响，检索准确性可能会降低[[30](#bib.bib30)]。需要在IIR流程中融入不变性[[61](#bib.bib61)]，[[62](#bib.bib62)]。'
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Distraction challenge: IIR systems may need to focus on only a certain object,
    or even only a small portion. DCNNs may be affected by image clutter or background,
    so multiple-pass schemes have been examined where region proposals are studied
    before feature extraction.'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**干扰挑战**：IIR系统可能需要只关注某个特定对象，甚至只是一个小部分。深度卷积神经网络（DCNNs）可能会受到图像杂乱或背景的影响，因此已经研究了多次通过的方案，在特征提取之前研究区域提议。'
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Discriminativity challenge: Deep features for IIR are needed to be as discriminative
    as possible to distinguish instances with small differences, leading to many explorations
    in feature processing. These include feature embedding and aggregation methods,
    to promote feature discriminativity; and attention mechanisms, to highlight the
    most relevant regions within the extracted features or to enable deep networks
    to focus on regions of interest.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**区分度挑战**：深度特征在IIR（实例检索）中需要尽可能具有高区分度，以便区分具有细微差异的实例，这导致了许多特征处理方面的探索。这些探索包括特征嵌入和聚合方法，以促进特征的区分度；以及注意力机制，以突出提取特征中的最相关区域或使深度网络能够关注感兴趣的区域。'
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-tune challenge: DCNNs can be fine-tuned as powerful extractors to capture
    fine semantic distinctions among instances. These explorations have offered improved
    accuracy, however the area remains a major challenge.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**微调挑战**：DCNNs可以作为强大的提取器进行微调，以捕捉实例之间的细微语义差异。这些探索提供了改进的准确性，然而这一领域仍然是一个主要挑战。'
- en: '![Refer to caption](img/91478a14e663be98042e00092a09a215.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/91478a14e663be98042e00092a09a215.png)'
- en: 'Figure 2: General framework of IIR, which includes feature extraction on image
    or image patches, followed by feature embedding and aggregation methods to improve
    feature discriminativity. Feature matching can be performed by using global features
    (initial filtering) or use local features to rerank the top-ranked images matched
    by global features.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：IIR的一般框架，包括对图像或图像补丁的特征提取，随后进行特征嵌入和聚合方法以提高特征的区分性。特征匹配可以通过使用全局特征（初步筛选）或使用局部特征对全局特征匹配的前排名图像进行重新排序来进行。
- en: B) Efficiency related challenges are important, especially for large-scale datasets
    [[63](#bib.bib63)]. Retrieval systems should respond quickly when given a query
    image. Deep features are high-dimensional and contain semantic-aware information
    to support higher accuracy, yet is often at the expense of efficiency.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: B) 效率相关的挑战非常重要，特别是在大规模数据集的情况下[[63](#bib.bib63)]。检索系统在接收到查询图像时应迅速响应。深度特征是高维的，包含支持更高准确率的语义感知信息，但往往以效率为代价。
- en: On the one hand, the efficiency is related to the format of features, i.e.,
    real valued or binary. Hash codes have advantages in storage and searching [[22](#bib.bib22)],[[39](#bib.bib39)],
    however for hashing methods one needs to carefully consider the loss function
    design [[64](#bib.bib64)],[[65](#bib.bib65)], to obtain optimal codes for high
    retrieval accuracy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，效率与特征的格式有关，即实值或二值。哈希码在存储和搜索方面具有优势[[22](#bib.bib22)],[[39](#bib.bib39)]，但对于哈希方法，需要仔细考虑损失函数设计[[64](#bib.bib64)],[[65](#bib.bib65)]，以获得用于高检索准确度的最佳代码。
- en: On the other hand, efficiency is also related to the mechanism of feature matching.
    For example, instead of time-consuming cross-matching between local features,
    one can choose to use global features to perform an initial ranking and then a
    post-step re-ranking via the features of top-ranked images.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，效率也与特征匹配的机制有关。例如，可以选择使用全局特征进行初步排序，然后通过前排名图像的特征进行后续排序，而不是耗时的局部特征交叉匹配。
- en: 2 General Framework of IIR
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 一般框架的IIR
- en: 'Figure [2](#S1.F2 "Figure 2 ‣ 1.2 Key Challenges ‣ 1 Introduction ‣ Deep Learning
    for Instance Retrieval: A Survey") offers an overview of the general framework
    for deep-learning-based IIR, involving three main stages.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S1.F2 "图2 ‣ 1.2 关键挑战 ‣ 1 引言 ‣ 深度学习实例检索：综述")提供了基于深度学习的IIR的一般框架概述，包括三个主要阶段。
- en: '1) Deep feature extraction: (Section [3.1](#S3.SS1 "3.1 Deep Feature Extraction
    ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey"))'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 深度特征提取：（第[3.1节](#S3.SS1 "3.1 深度特征提取 ‣ 3 使用现成的DCNN模型进行检索 ‣ 深度学习实例检索：综述")）
- en: 'Feature extraction is the first step of IIR and can be realized in a single-pass
    or multiple-pass way. Single-pass methods take as input the whole image, whereas
    multiple-pass methods depend on region extraction, as depicted in Figure [4](#S3.F4
    "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3
    Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey").'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取是IIR的第一步，可以通过单次或多次方式实现。单次方法以整个图像作为输入，而多次方法依赖于区域提取，如图[4](#S3.F4 "图4 ‣ 3.1.1
    网络前向传递方案 ‣ 3.1 深度特征提取 ‣ 3 使用现成的DCNN模型进行检索 ‣ 深度学习实例检索：综述")所示。
- en: 'The activations from fully-connected layers of a given DCNN can be used as
    retrieval features whether based on a whole image or on patches. The tensors from
    convolutional layers can be used when further processed by sophisticated pooling,
    as shown in Figure [2](#S1.F2 "Figure 2 ‣ 1.2 Key Challenges ‣ 1 Introduction
    ‣ Deep Learning for Instance Retrieval: A Survey"). Different layers of the same
    deep network can be combined as a more powerful extractor [[36](#bib.bib36)],[[66](#bib.bib66)].
    Furthermore, it is possible to fuse the activations from layers of different models
    [[67](#bib.bib67)],[[68](#bib.bib68)]. Feature extraction is the step to produce
    vanilla network activations (i.e., 3D tensors or a single vector), these activations,
    in most cases, are needed to be further processed.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从给定的DCNN的全连接层的激活可以用作检索特征，无论是基于整个图像还是图像补丁。当通过复杂的池化进一步处理卷积层的张量时，可以使用这些张量，如图[2](#S1.F2
    "图2 ‣ 1.2 关键挑战 ‣ 1 引言 ‣ 深度学习实例检索：综述")所示。相同深度网络的不同层可以结合成更强大的提取器[[36](#bib.bib36)],[[66](#bib.bib66)]。此外，也可以融合不同模型层的激活[[67](#bib.bib67)],[[68](#bib.bib68)]。特征提取是生成基础网络激活（即3D张量或单一向量）的步骤，这些激活在大多数情况下需要进一步处理。
- en: '2) Embedding and aggregation: (Section [3.2](#S3.SS2 "3.2 Feature Embedding
    and Aggregation ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for
    Instance Retrieval: A Survey"))'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 嵌入和聚合：（见第 [3.2](#S3.SS2 "3.2 特征嵌入和聚合 ‣ 3 使用现成 DCNN 模型的检索 ‣ 实例检索的深度学习：综述")
    节）
- en: Feature embedding and aggregation are two essential steps to produce global
    or local features. Feature embedding maps individual local features into higher-dimensional
    space whereas feature aggregation summarizes the multiple mapped vectors or all
    individual features into a global vector. Global features may come from pooling
    convolutional feature maps directly [[69](#bib.bib69)],[[70](#bib.bib70)] or using
    some sophisticated weighting methods [[12](#bib.bib12)],[[15](#bib.bib15)] (i.e.,
    both without feature embedding). Feature embedding method using a pre-generated
    codebook can be performed to encode individual convolutional vectors and then
    aggregated [[24](#bib.bib24)],[[54](#bib.bib54)],[[55](#bib.bib55)]. For local
    features, the well-embedded representations for all regions of interest are stored
    individually and used for cross-matching in the reranking stage without aggregation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 特征嵌入和聚合是生成全局或局部特征的两个关键步骤。特征嵌入将单个局部特征映射到更高维空间，而特征聚合将多个映射向量或所有单个特征汇总到一个全局向量中。全局特征可能直接来自池化卷积特征图
    [[69](#bib.bib69)],[[70](#bib.bib70)]，或使用一些复杂的加权方法 [[12](#bib.bib12)],[[15](#bib.bib15)]（即，都不进行特征嵌入）。使用预生成代码本的特征嵌入方法可以对单个卷积向量进行编码，然后进行聚合
    [[24](#bib.bib24)],[[54](#bib.bib54)],[[55](#bib.bib55)]。对于局部特征，所有感兴趣区域的良好嵌入表示被单独存储，并在重新排名阶段用于交叉匹配而不进行聚合。
- en: '3) Feature matching:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 特征匹配：
- en: Feature matching is a process to measure the feature similarity between images
    and then return a ranked list. Global matches can be computed efficiently via
    such as Euclidean distance. For local features [[5](#bib.bib5)],[[71](#bib.bib71)],
    the image similarity is usually evaluated by summarizing the similarities across
    local features, using classical RANSAC [[72](#bib.bib72)] or more recent variations
    [[73](#bib.bib73)],[[74](#bib.bib74)]. Storing local features separately and then
    estimating their similarity individually lead to additional memory and search
    costs [[71](#bib.bib71)],[[74](#bib.bib74)], therefore in most cases local features
    are used to re-rank the initial ranking image matched by global features [[32](#bib.bib32)],[[64](#bib.bib64)],[[71](#bib.bib71)],[[75](#bib.bib75)].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 特征匹配是一个测量图像之间特征相似性的过程，然后返回一个排名列表。全局匹配可以通过欧几里得距离等方法有效计算。对于局部特征 [[5](#bib.bib5)],[[71](#bib.bib71)]，图像相似性通常通过总结局部特征之间的相似性来评估，使用经典的
    RANSAC [[72](#bib.bib72)] 或者更近期的变体 [[73](#bib.bib73)],[[74](#bib.bib74)]。单独存储局部特征然后单独估计其相似性会导致额外的内存和搜索成本
    [[71](#bib.bib71)],[[74](#bib.bib74)]，因此在大多数情况下，局部特征用于重新排名由全局特征匹配的初始排名图像 [[32](#bib.bib32)],[[64](#bib.bib64)],[[71](#bib.bib71)],[[75](#bib.bib75)]。
- en: 'The three preceding stages for IIR rely on DCNNs as backbone architectures.
    In almost all cases, pre-stored parameters in these backbones can be fine-tuned
    (Section [4](#S4 "4 Retrieval via Learning DCNN Representations ‣ Deep Learning
    for Instance Retrieval: A Survey")) to be better suited for instance retrieval
    and to contribute to better performance.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的三个阶段依赖于 DCNN 作为主干架构。在几乎所有情况下，这些主干中的预存参数可以被微调（见第 [4](#S4 "4 通过学习 DCNN 表示的检索
    ‣ 实例检索的深度学习：综述") 节），以更好地适应实例检索，并提高性能。
- en: 'The detailed categorization of the material of the following sections is shown
    in Figure [3](#S3.F3 "Figure 3 ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣
    Deep Learning for Instance Retrieval: A Survey").'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分材料的详细分类见图 [3](#S3.F3 "图 3 ‣ 3 使用现成 DCNN 模型的检索 ‣ 实例检索的深度学习：综述")。
- en: 3 Retrieval with Off-the-Shelf DCNN Models
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 使用现成 DCNN 模型的检索
- en: Because of their size, DCNNs need to be trained, initially for classification
    tasks, on exceptionally large-scale datasets and be able to recognize images from
    different classes. One possible scheme, then, is that DCNNs effectively trained
    for classification directly serve as off-the-shelf feature detectors for the image
    retrieval task, the topic of this survey. That is, one can propose to undertake
    image retrieval on the basis of DCNNs, trained for classification and with their
    pre-trained parameters frozen.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其规模，DCNNs 需要在极其大规模的数据集上进行训练，最初用于分类任务，并能够识别来自不同类别的图像。因此，一个可能的方案是，经过有效分类训练的
    DCNNs 可以直接作为现成的特征检测器用于图像检索任务，这是本调查的主题。也就是说，可以提议在基于已经训练好的 DCNNs 进行图像检索，并保持其预训练参数不变。
- en: '<svg   height="490.88" overflow="visible" version="1.1" width="795.57"><g transform="translate(0,490.88)
    matrix(1 0 0 -1 0 0) translate(15.47,0) translate(0,213.95)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -11.78 264.95)" fill="#000000"
    stroke="#000000"><foreignobject width="316.22" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Deep Learning for Instance-level Image Retrieval
    (overall survey)</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 19.71
    249.2)" fill="#000000" stroke="#000000"><foreignobject width="268.53" height="11.07"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Retrieval with Off-the-Shelf
    DCNN Models (Section [3](#S3 "3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep
    Learning for Instance Retrieval: A Survey"))</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 51.21 233.45)" fill="#000000" stroke="#000000"><foreignobject width="179.48"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Deep Feature
    Extraction (Section [3.1](#S3.SS1 "3.1 Deep Feature Extraction ‣ 3 Retrieval with
    Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey"))</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 217.7)" fill="#000000" stroke="#000000"><foreignobject
    width="203.8" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Network
    Feedforward Scheme (Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Network Feedforward Scheme
    ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep
    Learning for Instance Retrieval: A Survey"))</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 114.2 201.95)" fill="#000000" stroke="#000000"><foreignobject width="518.43"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Single Feedforward
    Pass: MAC [[69](#bib.bib69)], R-MAC [[32](#bib.bib32)]</foreignobject></g> <g
    transform="matrix(1.0 0.0 0.0 1.0 114.2 186.21)" fill="#000000" stroke="#000000"><foreignobject
    width="485.1" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Multiple
    Feedforward Pass: SPM [[52](#bib.bib52)], RPNs [[42](#bib.bib42)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 82.71 170.46)" fill="#000000" stroke="#000000"><foreignobject
    width="174.68" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Deep
    Feature Selection (Section [3.1.2](#S3.SS1.SSS2 "3.1.2 Deep Feature Selection
    ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep
    Learning for Instance Retrieval: A Survey"))</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 114.2 154.71)" fill="#000000" stroke="#000000"><foreignobject width="338.12"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Fully-connected
    Layer: Neural codes [[39](#bib.bib39)]</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 114.2 138.96)" fill="#000000" stroke="#000000"><foreignobject width="514.49"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Convolutional
    Layer: SPoC [[12](#bib.bib12)], CroW [[15](#bib.bib15)]</foreignobject></g> <g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 123.21)" fill="#000000" stroke="#000000"><foreignobject
    width="179.7" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Feature
    Fusion Strategy (Section [3.1.3](#S3.SS1.SSS3 "3.1.3 Feature Fusion Strategies
    ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep
    Learning for Instance Retrieval: A Survey"))</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 114.2 107.47)" fill="#000000" stroke="#000000"><foreignobject width="434.02"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Layer-level
    Fusion: MoF [[36](#bib.bib36)], MOP [[30](#bib.bib30)]</foreignobject></g> <g
    transform="matrix(1.0 0.0 0.0 1.0 114.2 91.72)" fill="#000000" stroke="#000000"><foreignobject
    width="331.87" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Model-level
    Fusion: ConvNet fusion [[49](#bib.bib49)]</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 51.21 75.97)" fill="#000000" stroke="#000000"><foreignobject width="242.67"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Feature
    Embedding and Aggregation (Section [3.2](#S3.SS2 "3.2 Feature Embedding and Aggregation
    ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey"))</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 82.71 60.22)"
    fill="#000000" stroke="#000000"><foreignobject width="210.65" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Matching with Global Features (Section [3.2.1](#S3.SS2.SSS1
    "3.2.1 Matching with Global Features ‣ 3.2 Feature Embedding and Aggregation ‣
    3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey"))</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 82.71 44.48)"
    fill="#000000" stroke="#000000"><foreignobject width="204.88" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Matching with Local Features (Section [3.2.2](#S3.SS2.SSS2
    "3.2.2 Matching with Local Features ‣ 3.2 Feature Embedding and Aggregation ‣
    3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey"))</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 82.71 28.73)"
    fill="#000000" stroke="#000000"><foreignobject width="167.64" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Attention Mechanism (Section [3.2.3](#S3.SS2.SSS3
    "3.2.3 Attention Mechanism ‣ 3.2 Feature Embedding and Aggregation ‣ 3 Retrieval
    with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey"))</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 114.2 16.44)" fill="#000000" stroke="#000000"><foreignobject
    width="662.21" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Non-parameteric:
    CroW [[15](#bib.bib15)], SPoC [[12](#bib.bib12)], SWVF [[76](#bib.bib76)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 114.2 0.69)" fill="#000000" stroke="#000000"><foreignobject
    width="632.14" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Parameteric:
    CRN [[77](#bib.bib77)], DeepFixNet+SAM [[78](#bib.bib78)],[[79](#bib.bib79)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 82.71 -18.52)" fill="#000000" stroke="#000000"><foreignobject
    width="174.72" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Deep
    Hash Embedding (Section [3.2.4](#S3.SS2.SSS4 "3.2.4 Hashing Embedding ‣ 3.2 Feature
    Embedding and Aggregation ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep
    Learning for Instance Retrieval: A Survey"))</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 114.2 -34.26)" fill="#000000" stroke="#000000"><foreignobject width="295.99"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Supervised
    Hashing: SSDH [[80](#bib.bib80)]</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 114.2 -50.01)" fill="#000000" stroke="#000000"><foreignobject width="477.81"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Unsupervised
    Hashing: DeepBit [[65](#bib.bib65)], DSTH [[81](#bib.bib81)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 19.71 -65.76)" fill="#000000" stroke="#000000"><foreignobject
    width="299.4" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Fine-Tuning
    for Learning DCNN Representations (Section [4](#S4 "4 Retrieval via Learning DCNN
    Representations ‣ Deep Learning for Instance Retrieval: A Survey"))</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 51.21 -81.51)" fill="#000000" stroke="#000000"><foreignobject
    width="175.52" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Supervised
    Fine-tuning (Section [4.1](#S4.SS1 "4.1 Supervised Fine-tuning ‣ 4 Retrieval via
    Learning DCNN Representations ‣ Deep Learning for Instance Retrieval: A Survey"))</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 82.71 -97.25)" fill="#000000" stroke="#000000"><foreignobject
    width="223.94" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Fine-tuning
    via classification loss (Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Fine-tuning via Classification
    Loss ‣ 4.1 Supervised Fine-tuning ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey"))</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 82.71 -113)" fill="#000000" stroke="#000000"><foreignobject width="242.39"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Fine-tuning
    via pairwise ranking loss (Section [4.1.2](#S4.SS1.SSS2 "4.1.2 Fine-tuning via
    Pairwise Ranking Loss ‣ 4.1 Supervised Fine-tuning ‣ 4 Retrieval via Learning
    DCNN Representations ‣ Deep Learning for Instance Retrieval: A Survey"))</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 114.2 -128.75)" fill="#000000" stroke="#000000"><foreignobject
    width="332.37" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Transformation
    Matrix: Non-metric [[40](#bib.bib40)]</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 114.2 -144.5)" fill="#000000" stroke="#000000"><foreignobject width="229.08"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Siamese
    Networks [[41](#bib.bib41)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0
    1.0 114.2 -160.25)" fill="#000000" stroke="#000000"><foreignobject width="218.19"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Triplet
    Networks [[82](#bib.bib82)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0
    1.0 51.21 -175.99)" fill="#000000" stroke="#000000"><foreignobject width="184.8"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Unsupervised
    Fine-tuning (Section [4.2](#S4.SS2 "4.2 Unsupervised Fine-tuning ‣ 4 Retrieval
    via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval: A Survey"))</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 -191.74)" fill="#000000" stroke="#000000"><foreignobject
    width="378.09" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Manifold
    Learning Samples Mining: Diffusion Net [[47](#bib.bib47)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 82.71 -207.49)" fill="#000000" stroke="#000000"><foreignobject
    width="507.1" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Mining
    Samples by Clustering: SfM-GeM [[44](#bib.bib44)],[[46](#bib.bib46)]</foreignobject></g></g></g></svg>'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="490.88" overflow="visible" version="1.1" width="795.57"><g transform="translate(0,490.88)
    matrix(1 0 0 -1 0 0) translate(15.47,0) translate(0,213.95)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -11.78 264.95)" fill="#000000"
    stroke="#000000"><foreignobject width="316.22" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">实例级图像检索的深度学习（整体调查）</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 19.71 249.2)" fill="#000000" stroke="#000000"><foreignobject width="268.53"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">使用现成的 DCNN
    模型进行检索（第 [3](#S3 "3 使用现成的 DCNN 模型进行检索 ‣ 实例级图像检索的深度学习：一项调查") 节）</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 51.21 233.45)" fill="#000000" stroke="#000000"><foreignobject
    width="179.48" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">深度特征提取（第
    [3.1](#S3.SS1 "3.1 深度特征提取 ‣ 3 使用现成的 DCNN 模型进行检索 ‣ 实例级图像检索的深度学习：一项调查") 节）</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 82.71 217.7)" fill="#000000" stroke="#000000"><foreignobject
    width="203.8" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">网络前馈方案（第
    [3.1.1](#S3.SS1.SSS1 "3.1.1 网络前馈方案 ‣ 3.1 深度特征提取 ‣ 3 使用现成的 DCNN 模型进行检索 ‣ 实例级图像检索的深度学习：一项调查")
    节）</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 114.2 201.95)" fill="#000000"
    stroke="#000000"><foreignobject width="518.43" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">单次前馈传递：MAC [[69](#bib.bib69)], R-MAC [[32](#bib.bib32)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 114.2 186.21)" fill="#000000" stroke="#000000"><foreignobject
    width="485.1" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">多次前馈传递：SPM
    [[52](#bib.bib52)], RPNs [[42](#bib.bib42)]</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 82.71 170.46)" fill="#000000" stroke="#000000"><foreignobject width="174.68"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">深度特征选择（第
    [3.1.2](#S3.SS1.SSS2 "3.1.2 深度特征选择 ‣ 3.1 深度特征提取 ‣ 3 使用现成的 DCNN 模型进行检索 ‣ 实例级图像检索的深度学习：一项调查")
    节）</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 114.2 154.71)" fill="#000000"
    stroke="#000000"><foreignobject width="338.12" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">全连接层：神经编码 [[39](#bib.bib39)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 114.2 138.96)" fill="#000000" stroke="#000000"><foreignobject
    width="514.49" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">卷积层：SPoC
    [[12](#bib.bib12)], CroW [[15](#bib.bib15)]</foreignobject></g> <g transform="matrix(1.0
    0.0 0.0 1.0 82.71 123.21)" fill="#000000" stroke="#000000"><foreignobject width="179.7"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">特征融合策略（第
    [3.1.3](#S3.SS1.SSS3 "3.1.3 特征融合策略 ‣ 3.1 深度特征提取 ‣ 3 使用现成的 DCNN 模型进行检索 ‣ 实例级图像检索的深度学习：一项调查")
    节）</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0 114.2 107.47)" fill="#000000"
    stroke="#000000"><foreignobject width="434.02" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">层级融合：MoF [[36](#bib.bib36)], MOP [[30](#bib.bib30)]</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 114.2 91.72)" fill="#000000" stroke="#000000"><foreignobject
    width="331.87" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">模型级融合：ConvNet
    融合 [[49](#bib.bib49)]</foreignobject></g> <g transform="matrix(1.0 0.0 0.0 1.0
    51.21 75.97)" fill="#000000" stroke="#000000"><foreignobject width="242.67" height="11.07"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">特征嵌入与聚合（第 [3.2](#S3.SS2
    "3.2 特征嵌入与聚合 ‣ 3 使用现成的 DCNN 模型进行检索 ‣ 实例级图像检索的深度学习：一项调查") 节）</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 82.71 60.22)" fill="#000000" stroke="#000000"><foreignobject
    width="210.65" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">全局特征匹配（第
    [3.2.1](#S3.SS2.SSS1 "3.2.1 全局特征匹配 ‣ 3.2 特征嵌入与聚合 ‣ 3 使用现成的 DCNN 模型进行检索 ‣ 实例级图像检索的深度学习：一项调查")
    节）</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 82.71 44.48)" fill="#000000"
    stroke="#000000"><foreignobject width="204.88" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">局部特征匹配（第 [3.2.2](#S3.SS2.SSS2 "3.2.2 局部特征匹配
    ‣ 3.2 特征嵌入与聚合 ‣ 3 使用现成的 DCNN 模型
- en: 'Figure 3: This survey is organized around three key themes in instance-level
    image retrieval, shown in bold.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：本综述围绕实例级图像检索的三个关键主题组织，以粗体显示。
- en: 'There are limitations to this approach, most fundamentally that there is a
    model-transfer or domain-shift challenge between tasks [[13](#bib.bib13)],[[31](#bib.bib31)],[[83](#bib.bib83)],
    meaning that models trained for classification do not necessarily extract features
    well suited to image retrieval. In particular, a classification decision can be
    successful as long as features remain within classification boundaries, however
    features from such models may show insufficient capacity for retrieval where feature
    matching itself is more important than classification. This section will survey
    the strategies which have been developed to improve the quality of feature representations,
    particularly based on feature extraction / fusion (Section [3.1](#S3.SS1 "3.1
    Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning
    for Instance Retrieval: A Survey")) and feature embedding / aggregation (Section
    [3.2](#S3.SS2 "3.2 Feature Embedding and Aggregation ‣ 3 Retrieval with Off-the-Shelf
    DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey")).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法存在局限性，最根本的是任务之间存在模型转移或领域迁移挑战[[13](#bib.bib13)],[[31](#bib.bib31)],[[83](#bib.bib83)]，这意味着为分类训练的模型不一定能提取适合图像检索的特征。特别是，只要特征保持在分类边界内，分类决策可能会成功，但这些模型提取的特征在检索中可能表现出不足的能力，因为特征匹配比分类更为重要。本节将调查为提高特征表示质量而开发的策略，特别是基于特征提取/融合（第[3.1节](#S3.SS1
    "3.1 深度特征提取 ‣ 3 使用现成的 DCNN 模型进行检索 ‣ 实例检索的深度学习：综述")）和特征嵌入/聚合（第[3.2节](#S3.SS2 "3.2
    特征嵌入和聚合 ‣ 3 使用现成的 DCNN 模型进行检索 ‣ 实例检索的深度学习：综述")）的方法。
- en: 3.1 Deep Feature Extraction
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 深度特征提取
- en: Feature extraction is about the mechanism by which retrieval features can be
    extracted from off-the-shelf DCNNs. For an input image $x$ and a network $f(\cdot;\bm{\theta})$,
    we denote its features from a convolutional layer as $\bm{A}:=f_{conv}(x)\in\mathbb{R}^{H\times
    W\times C}$ with height $H$, width $W$, and channels $C$ while that from a fully-connected
    layer as $\bm{B}:=f_{fc}(x)\in\mathbb{R}^{D\times 1}$ with the dimensional $D$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取涉及从现成的 DCNN 中提取检索特征的机制。对于输入图像 $x$ 和网络 $f(\cdot;\bm{\theta})$，我们将来自卷积层的特征表示为
    $\bm{A}:=f_{conv}(x)\in\mathbb{R}^{H\times W\times C}$，其中 $H$ 是高度，$W$ 是宽度，$C$
    是通道数，而来自全连接层的特征表示为 $\bm{B}:=f_{fc}(x)\in\mathbb{R}^{D\times 1}$，其中 $D$ 是维度。
- en: 3.1.1 Network Feedforward Scheme
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 网络前馈方案
- en: Network feedforward schemes focus on how images are fed into a DCNN, which includes
    single-pass and multiple-pass.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 网络前馈方案集中于如何将图像输入到 DCNN 中，包括单次传递和多次传递。
- en: '*a. Single Feedforward Pass Methods*.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*a. 单次前馈传递方法*。'
- en: Single feedforward pass methods take the whole image and feed it into an off-the-shelf
    model to extract features. The approach is relatively efficient since the input
    image is fed only once. For these methods, both the fully-connected layer and
    last convolutional layer can be used as feature extractors [[84](#bib.bib84)].
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 单次前馈传递方法将整个图像输入到现成模型中以提取特征。由于输入图像仅传递一次，这种方法相对高效。对于这些方法，全连接层和最后的卷积层均可用作特征提取器[[84](#bib.bib84)]。
- en: Early network-based IIR work focused on leveraging DCNNs as a fixed extractor
    to obtain global features, especially based on the fully-connected layers [[29](#bib.bib29)],[[39](#bib.bib39)],
    requiring close to zero engineering effort. However, extracting features in this
    way affects retrieval accuracy since the extracted features may include background
    information or activations for irrelevant objects.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 早期基于网络的 IIR 工作集中于利用 DCNN 作为固定提取器来获得全局特征，特别是基于全连接层[[29](#bib.bib29)],[[39](#bib.bib39)]，几乎无需工程努力。然而，这种方式提取的特征可能包含背景信息或与目标无关的激活，从而影响检索准确性。
- en: The key to single-pass schemes is to embed and aggregate features to improve
    their discriminativity, such that features of two related images (i.e., including
    the same object) are more similar than these of two unrelated images [[12](#bib.bib12)].
    For this purpose, it is possible to first map the features $\bm{B}$ into a high-dimensional
    space and then to aggregate them into a final global feature [[30](#bib.bib30)].
    Another direction is to treat regions in convolutional features $\bm{A}$ as different
    sub-vectors, such that a combination of sub-vectors of all feature maps are used
    to represent the input image [[15](#bib.bib15)],[[32](#bib.bib32)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 单次处理方案的关键在于嵌入和聚合特征，以提高其辨别能力，使得两个相关图像（即，包括相同对象）的特征比两个不相关图像的特征更相似[[12](#bib.bib12)]。为此，可以首先将特征$\bm{B}$映射到高维空间，然后将其聚合为最终的全局特征[[30](#bib.bib30)]。另一种方向是将卷积特征$\bm{A}$中的区域视为不同的子向量，从而利用所有特征图的子向量组合来表示输入图像[[15](#bib.bib15)],[[32](#bib.bib32)]。
- en: '*b. Multiple Feedforward Pass Methods*.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*b. 多次前馈传递方法*。'
- en: 'Compared to single-pass schemes, multiple-pass methods are more time-consuming
    [[13](#bib.bib13)] because several patches are generated and then fed into the
    network, multiple-pass schemes are more helpful for addressing the “invariance
    challenges and “distraction challenges” in Section [1.2](#S1.SS2 "1.2 Key Challenges
    ‣ 1 Introduction ‣ Deep Learning for Instance Retrieval: A Survey"). Local patches
    at multiple scales become more robust for image translation, scaling and rotation
    [[30](#bib.bib30)],[[61](#bib.bib61)]. Also, these patches are helpful to filter
    several irrelevant background information.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '相比于单次处理方案，多次处理方法更耗时[[13](#bib.bib13)]，因为需要生成多个补丁并将其输入网络。多次处理方案对于解决第[1.2节](#S1.SS2
    "1.2 Key Challenges ‣ 1 Introduction ‣ Deep Learning for Instance Retrieval: A
    Survey")中的“不变性挑战”和“干扰挑战”更为有效。多尺度的局部补丁在图像转换、缩放和旋转方面变得更加稳健[[30](#bib.bib30)],[[61](#bib.bib61)]。此外，这些补丁有助于过滤掉一些无关的背景信息。'
- en: 'The representations are usually produced from two stages: patch detection and
    patch description. Multi-scale image patches are obtained using sliding windows
    [[37](#bib.bib37)],[[38](#bib.bib38)] or spatial pyramid model (SPM) [[52](#bib.bib52)],[[85](#bib.bib85)],[[86](#bib.bib86)],
    as shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1
    Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning
    for Instance Retrieval: A Survey"). For example, Zheng et al. [[86](#bib.bib86)]
    partition an image by using SPM and extract features at increasing scales, thus
    enabling the integration of global, regional, local contextual information.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '表征通常来自两个阶段：补丁检测和补丁描述。多尺度图像补丁是通过滑动窗口[[37](#bib.bib37)],[[38](#bib.bib38)]或空间金字塔模型（SPM）[[52](#bib.bib52)],[[85](#bib.bib85)],[[86](#bib.bib86)]获得的，如图[4](#S3.F4
    "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3
    Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey")所示。例如，Zheng等[[86](#bib.bib86)]通过使用SPM对图像进行分割，并在不断增加的尺度下提取特征，从而实现了全局、区域、局部上下文信息的整合。'
- en: Patch detection methods lack retrieval efficiency since irrelevant patches are
    also detected [[32](#bib.bib32)]. For example, Cao et al. [[87](#bib.bib87)] propose
    to merge image patches into larger regions with different hyper-parameters, where
    the hyper-parameter selection is viewed as an optimization problem to maximize
    the similarity between query and candidate features.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 补丁检测方法缺乏检索效率，因为也会检测到无关的补丁[[32](#bib.bib32)]。例如，Cao等[[87](#bib.bib87)]提出将图像补丁合并为具有不同超参数的较大区域，其中超参数选择被视为一个优化问题，以最大化查询和候选特征之间的相似性。
- en: '![Refer to caption](img/8db381115eb58d74b58a535c481eb3cf.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8db381115eb58d74b58a535c481eb3cf.png)'
- en: 'Figure 4: Image patch generation schemes: (a) Sliding windows [[37](#bib.bib37)],[[38](#bib.bib38)];
    (b) Spatial pyramid modeling [[85](#bib.bib85)]; (c) Dense sampling [[30](#bib.bib30)],[[36](#bib.bib36)];
    (d) Region proposals from region proposal networks [[27](#bib.bib27)],[[42](#bib.bib42)].'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：图像补丁生成方案：(a) 滑动窗口[[37](#bib.bib37)],[[38](#bib.bib38)]; (b) 空间金字塔建模[[85](#bib.bib85)];
    (c) 密集采样[[30](#bib.bib30)],[[36](#bib.bib36)]; (d) 来自区域提议网络的区域提议[[27](#bib.bib27)],[[42](#bib.bib42)]。
- en: Instead of generating multi-scale image patches randomly or densely, region
    proposal methods introduce a degree of purpose. Region proposals can be generated
    using object detectors, such as selective search [[61](#bib.bib61)], edge boxes
    [[88](#bib.bib88)],[[89](#bib.bib89)], and BING [[90](#bib.bib90)]. For example,
    Yu et al. [[89](#bib.bib89)] propose fuzzy object matching (FOM) for instance
    search in which the fuzzy objects are generated from 300 object proposals and
    then clustered to filter out overlapping proposals. Region proposals can also
    be learned using such as region proposal networks (RPNs) [[27](#bib.bib27)],[[42](#bib.bib42)]
    and convolutional kernel networks (CKNs) [[91](#bib.bib91)], and then to apply
    these networks into end-to-end fine-tuning for learning similarity [[92](#bib.bib92)].
    This usually requires the datasets provide well-localized bounding boxes as supervision,
    e.g., the datasets INSTRE [[93](#bib.bib93)], Oxford-5k [[94](#bib.bib94)], Paris-6k
    [[95](#bib.bib95)], GLD-v2 variant [[74](#bib.bib74)]. Also, in the off-the-shelf
    scenarios, the way that using the bounding boxes to crop the query images and
    use as input the DCNNs has been shown to provide better retrieval performance
    since only the information relevant to the instance is extracted [[92](#bib.bib92)],[[32](#bib.bib32)].
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与其随机或密集地生成多尺度图像补丁，不如采用区域提议方法引入一定的目的性。可以使用目标检测器生成区域提议，如选择性搜索 [[61](#bib.bib61)]、边缘框
    [[88](#bib.bib88)],[[89](#bib.bib89)] 和 BING [[90](#bib.bib90)]。例如，Yu 等 [[89](#bib.bib89)]
    提出了模糊对象匹配 (FOM) 方法用于实例搜索，其中模糊对象从 300 个对象提议中生成，并且经过聚类以筛选出重叠的提议。区域提议还可以通过区域提议网络
    (RPNs) [[27](#bib.bib27)],[[42](#bib.bib42)] 和卷积核网络 (CKNs) [[91](#bib.bib91)]
    进行学习，然后将这些网络应用于端到端的微调以学习相似性 [[92](#bib.bib92)]。这通常需要数据集提供良好的定位边界框作为监督，例如，数据集 INSTRE
    [[93](#bib.bib93)]、Oxford-5k [[94](#bib.bib94)]、Paris-6k [[95](#bib.bib95)]、GLD-v2
    变体 [[74](#bib.bib74)]。此外，在现成的场景中，使用边界框裁剪查询图像并作为输入 DCNNs 的方法已被证明提供了更好的检索性能，因为只提取了与实例相关的信息
    [[92](#bib.bib92)],[[32](#bib.bib32)]。
- en: 3.1.2 Deep Feature Selection
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 深度特征选择
- en: Feature selection decides the receptive field of the extracted features, i.e.,
    global-level from fully-connected layers and regional-level from convolutional
    layers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择决定了提取特征的接收场，即从全连接层的全局级别和从卷积层的区域级别。
- en: '*a. Extracted from Fully-connected Layers*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*a. 从全连接层提取*'
- en: 'It is straightforward to select a fully-connected layer as a global feature
    extractor [[29](#bib.bib29)],[[30](#bib.bib30)],[[39](#bib.bib39)]. With PCA dimensionality
    reduction and normalization [[29](#bib.bib29)] image similarity can be measured.
    Extracting features $\bm{B}$ from fully-connected layer leads to two obvious limitations
    for IIR: including irrelevant information, and a lack of local geometric invariance
    [[30](#bib.bib30)].'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择全连接层作为全局特征提取器是直接的 [[29](#bib.bib29)],[[30](#bib.bib30)],[[39](#bib.bib39)]。通过
    PCA 降维和归一化 [[29](#bib.bib29)] 可以测量图像相似度。从全连接层提取特征 $\bm{B}$ 对 IIR 有两个明显的限制：包括无关信息，以及缺乏局部几何不变性
    [[30](#bib.bib30)]。
- en: With regards to the first limitation, image-level global descriptors may include
    irrelevant patterns or background clutter, especially when a target instance is
    only a small portion of an image. It may then be more reasonable to extract region-level
    features at finer scales, i.e., using multiple passes [[30](#bib.bib30)],[[61](#bib.bib61)],[[64](#bib.bib64)].
    For the second limitation, an alternative is to extract multi-scale features on
    a convolutional layer [[69](#bib.bib69)],[[62](#bib.bib62)]. Further, it makes
    the global features incompatible with techniques such as spatial verification
    and re-ranking. Several methods then choose to leverage intermediate convolutional
    layers [[12](#bib.bib12)],[[30](#bib.bib30)],[[50](#bib.bib50)],[[69](#bib.bib69)].
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 关于第一个限制，图像级全局描述符可能包括无关的模式或背景杂乱，特别是当目标实例只是图像的一小部分时。此时，更合理的做法是在更细尺度上提取区域级特征，即使用多次
    [[30](#bib.bib30)],[[61](#bib.bib61)],[[64](#bib.bib64)]。对于第二个限制，另一种方法是在卷积层上提取多尺度特征
    [[69](#bib.bib69)],[[62](#bib.bib62)]。此外，这使得全局特征与空间验证和重新排序等技术不兼容。因此，几种方法选择利用中间卷积层
    [[12](#bib.bib12)],[[30](#bib.bib30)],[[50](#bib.bib50)],[[69](#bib.bib69)]。
- en: '*b. Extracted from Convolutional Layers*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*b. 从卷积层提取*'
- en: The neurons in a convolutional layer are connected only to a local region of
    the input image, and this smaller receptive field ensures that the produced features
    $\bm{A}$, usually from the last layer, preserve more local structural information
    [[96](#bib.bib96)],[[97](#bib.bib97)] and are more robust to image transformations
    [[12](#bib.bib12)] thereby address the “invariance challenge”. For instance, Razavian
    et al. [[69](#bib.bib69)] extract multi-scale features on the last convolutional
    layer and Morère et al. [[62](#bib.bib62)] incorporate a series of nested pooling
    layers into CNN. Both of them provide higher feature invariance. Thus, many image
    retrieval methods use convolutional layers as feature extractors  [[33](#bib.bib33)],[[50](#bib.bib50)],[[69](#bib.bib69)],[[98](#bib.bib98)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层中的神经元仅连接到输入图像的局部区域，这种较小的感受野确保了所产生的特征 $\bm{A}$（通常来自最后一层）保留了更多的局部结构信息 [[96](#bib.bib96)]，[[97](#bib.bib97)]，并且对图像变换更具鲁棒性
    [[12](#bib.bib12)]，从而解决了“不可变性挑战”。例如，Razavian 等人 [[69](#bib.bib69)] 在最后的卷积层上提取多尺度特征，而
    Morère 等人 [[62](#bib.bib62)] 将一系列嵌套的池化层纳入 CNN。两者都提供了更高的特征不可变性。因此，许多图像检索方法使用卷积层作为特征提取器
    [[33](#bib.bib33)]，[[50](#bib.bib50)]，[[69](#bib.bib69)]，[[98](#bib.bib98)]。
- en: 'Sum/average and max pooling are two simple aggregation methods to produce global
    features [[69](#bib.bib69)]. For a pooled layer, the last convolutional layer
    usually yields superior accuracy over other shallower or later fully-connected
    layers [[97](#bib.bib97)]. There is no other operation on the feature maps before
    pooling, so we illustrate these methods as “direct pooling” in Figure [2](#S1.F2
    "Figure 2 ‣ 1.2 Key Challenges ‣ 1 Introduction ‣ Deep Learning for Instance Retrieval:
    A Survey").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 求和/平均和最大池化是产生全局特征的两种简单聚合方法 [[69](#bib.bib69)]。对于一个池化层，最后的卷积层通常比其他较浅的或后来的全连接层提供更优的准确性
    [[97](#bib.bib97)]。在池化之前，对特征图没有其他操作，因此我们在图 [2](#S1.F2 "图 2 ‣ 1.2 关键挑战 ‣ 1 介绍 ‣
    深度学习实例检索：综述") 中将这些方法标记为“直接池化”。
- en: '![Refer to caption](img/fd428966514bdeab8b9bfa709934147d.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fd428966514bdeab8b9bfa709934147d.png)'
- en: 'Figure 5: Representative methods in single-pass methods, focusing on convolutional
    feature tensor $\bm{A}$. We denote the entry in $\bm{A}$ corresponding to channel
    $c$, at spatial location ($i,j$) as $A_{ijc}$: MAC [[69](#bib.bib69)], R-MAC [[32](#bib.bib32)],
    SPoC with the per-channel Gaussian weighting $\alpha^{\prime}_{ij}A_{ij}$ where
    $\alpha^{\prime}_{ij}=\exp\left\{-{\textstyle\frac{\left(i-\frac{H}{2}\right)^{2}+\left(j-\frac{W}{2}\right)^{2}}{2\sigma^{2}}}\right\}$
    [[12](#bib.bib12)], CroW with $\alpha^{\prime\prime}$ computed by summing all
    $C$ feature maps at location ($i,j$) and $\beta$ computed by summing the $H\times
    W$-array at each feature map $c$ [[15](#bib.bib15)], GeM with channel-wise powers
    operation [[46](#bib.bib46)], and CAM+CroW by performing $M_{ij}^{(l)}=\sum_{c=1}^{C}\omega_{lc}A_{ijc}$
    where $\omega_{lc}$ are weights activated by $l$-th class [[33](#bib.bib33)].'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：单次方法中的代表性方法，重点关注卷积特征张量 $\bm{A}$。我们将 $\bm{A}$ 中对应通道 $c$、空间位置（$i,j$）的条目记作
    $A_{ijc}$：MAC [[69](#bib.bib69)]，R-MAC [[32](#bib.bib32)]，SPoC 使用每通道的高斯加权 $\alpha^{\prime}_{ij}A_{ij}$
    其中 $\alpha^{\prime}_{ij}=\exp\left\{-{\textstyle\frac{\left(i-\frac{H}{2}\right)^{2}+\left(j-\frac{W}{2}\right)^{2}}{2\sigma^{2}}}\right\}$
    [[12](#bib.bib12)]，CroW 使用通过在位置（$i,j$）对所有 $C$ 特征图进行求和计算的 $\alpha^{\prime\prime}$
    和通过对每个特征图 $c$ 的 $H\times W$ 数组进行求和计算的 $\beta$ [[15](#bib.bib15)]，GeM 使用通道-wise
    功率操作 [[46](#bib.bib46)]，以及通过执行 $M_{ij}^{(l)}=\sum_{c=1}^{C}\omega_{lc}A_{ijc}$
    其中 $\omega_{lc}$ 是由 $l$-th 类激活的权重 [[33](#bib.bib33)] 来实现 CAM+CroW。
- en: 'Instead of direct pooling, many sophisticated aggregation methods have been
    explored, such as channel-wise or spatial-wise feature weighting on the convolutional
    feature maps [[62](#bib.bib62)],[[99](#bib.bib99)],[[100](#bib.bib100)]. These
    aggregation methods aim to highlight feature importance [[15](#bib.bib15)] or
    reduce the undesirable influence of bursty descriptors of some regions [[34](#bib.bib34)],[[101](#bib.bib101)].
    For clarity, we illustrate the representative strategies in Figure [5](#S3.F5
    "Figure 5 ‣ 3.1.2 Deep Feature Selection ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval
    with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey").
    Note that these feature aggregation methods are usually performed before channel-wise
    sum/max pooling and does not embed features into a higher dimensional space.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '许多复杂的聚合方法已经被探索，比如在卷积特征图上进行通道级或空间级特征加权[[62](#bib.bib62)],[[99](#bib.bib99)],[[100](#bib.bib100)]。这些聚合方法旨在突出特征的重要性[[15](#bib.bib15)]或减少某些区域的突发描述符的负面影响[[34](#bib.bib34)],[[101](#bib.bib101)]。为了清晰起见，我们在图[5](#S3.F5
    "Figure 5 ‣ 3.1.2 Deep Feature Selection ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval
    with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey")中展示了代表性的策略。请注意，这些特征聚合方法通常在通道级求和/最大池化之前进行，并且不会将特征嵌入到更高维的空间中。'
- en: One rationale behind using convolutional features is that each such vector can
    act as a “dense SIFT” feature [[12](#bib.bib12)] since each vector corresponds
    to a region in the input image. Inspired by this perception, many works leverage
    embedding methods (e.g., BoW) used for SIFT features [[23](#bib.bib23)] on the
    regional feature vectors and then aggregate them (e.g., by sum pooling) into a
    global descriptor. Feature embedding methods address the discriminativity challenge
    via mapping individual features into a high-dimensional space and make them distinguishable
    [[34](#bib.bib34)]. Feature embedding is followed by PCA to reduce feature dimensionality
    and whitening to down-weight co-occurrence between features.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积特征的一个理由是，每个这样的向量可以充当“密集SIFT”特征[[12](#bib.bib12)]，因为每个向量对应于输入图像中的一个区域。受这种观念的启发，许多工作利用用于SIFT特征的嵌入方法（例如，BoW）[[23](#bib.bib23)]来处理区域特征向量，然后将它们聚合（例如，通过求和池化）成一个全局描述符。特征嵌入方法通过将个体特征映射到高维空间来解决区分性挑战，使其可区分[[34](#bib.bib34)]。特征嵌入之后进行PCA以减少特征维度，并进行白化以降低特征之间的共现权重。
- en: 3.1.3 Feature Fusion Strategies
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 特征融合策略
- en: Fusion studies the complementarity of different features which includes layer-level
    and model-level fusion explorations.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 融合研究不同特征的互补性，包括层级融合和模型级融合探索。
- en: '*a. Layer-level Fusion*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*a. 层级融合*'
- en: With layer-level fusion it is possible to fuse multiple fully-connected layers
    in a deep network [[52](#bib.bib52)],[[66](#bib.bib66)]. For instance, Liu et
    al. [[52](#bib.bib52)] introduce DeepIndex to incorporate multiple global features
    from different fully connected layers. The activation from the first fully-connected
    layer is taken as column indexing, and that from the second layer serves as row
    indexing. Similarly, it is also possible to fuse the activations from multiple
    convolutional layers. For instance, Li et al. [[99](#bib.bib99)] apply the R-MAC
    encoding scheme on five convolutional layers of VGG-16 and then concatenate them
    into a multi-scale feature vector.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过层级融合，可以融合深度网络中的多个全连接层[[52](#bib.bib52)],[[66](#bib.bib66)]。例如，Liu等人[[52](#bib.bib52)]提出了DeepIndex来整合来自不同全连接层的多个全局特征。第一个全连接层的激活被用作列索引，第二个层的激活用作行索引。同样，也可以融合来自多个卷积层的激活。例如，Li等人[[99](#bib.bib99)]在VGG-16的五个卷积层上应用了R-MAC编码方案，然后将它们串联成一个多尺度特征向量。
- en: Features from fully-connected layers retain global high-level semantics, whereas
    features from convolutional layers can present local low- and intermediate-level
    cues. Global and local features therefore complement each other when measuring
    semantic similarity and can, to some extent, guarantee retrieval performance [[102](#bib.bib102)],[[103](#bib.bib103)].
    Such features can be concatenated directly [[71](#bib.bib71)],[[102](#bib.bib102)],
    with convolutional features normally filtered by sliding windows or region proposal
    nets. Direct concatenation can also be replaced by other advanced methods, such
    as orthogonal operations [[103](#bib.bib103)] or pooling-based methods, such as
    Multi-layer Orderless Fusion (MOF) of Li et al. [[36](#bib.bib36)], which is inspired
    by Multi-layer Orderless Pooling (MOP) [[30](#bib.bib30)]. However local features
    cannot play a decisive role in distinguishing subtle feature differences if global
    and local features are treated identically. Yu et al. [[102](#bib.bib102)] use
    a mapping function to assert local features in refining the return ranking lists,
    via an exponential mapping function for tapping the complementary strengths of
    convolutional and fully-connected layers. Similarly, Liu et al. [[4](#bib.bib4)]
    design two sub-networks on top of convolutional layers to obtain global and local
    features and then learn to fuse these features, thereby adaptively adjusting the
    fusion weights. Instead of directly fusing the layer activations, Zhang et al.
    [[104](#bib.bib104)] fuse the index matrices which are generated based on the
    two feature types extracted from the same CNN, a feature fusion which has low
    computational complexity.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接层的特征保留了全球高层次的语义，而卷积层的特征则能呈现局部低层次和中层次的线索。因此，在测量语义相似性时，全球和局部特征相互补充，并且在一定程度上能保证检索性能[[102](#bib.bib102)],[[103](#bib.bib103)]。这些特征可以直接进行连接[[71](#bib.bib71)],[[102](#bib.bib102)]，卷积特征通常通过滑动窗口或区域提议网络进行过滤。直接连接也可以被其他先进的方法替代，如正交操作[[103](#bib.bib103)]或基于池化的方法，如Li等人的多层无序融合（MOF）[[36](#bib.bib36)]，该方法受到多层无序池化（MOP）[[30](#bib.bib30)]的启发。然而，如果全球和局部特征被同等对待，局部特征在区分微妙的特征差异方面无法发挥决定性作用。Yu等人[[102](#bib.bib102)]使用映射函数在精细化返回排名列表时断言局部特征，通过指数映射函数利用卷积层和完全连接层的互补优势。类似地，Liu等人[[4](#bib.bib4)]在卷积层之上设计了两个子网络，以获取全球和局部特征，然后学习融合这些特征，从而自适应调整融合权重。Zhang等人[[104](#bib.bib104)]则融合了基于相同CNN提取的两种特征类型生成的索引矩阵，这种特征融合具有较低的计算复杂度。
- en: It is worth considering which layer combinations are better for fusion given
    their differences and complementarity. Yu et al. [[102](#bib.bib102)] compare
    the performance of different combinations between fully-connected and convolutional
    layers on the Oxford 5k, Holiday, and UKBench datasets. The results show that
    the combinations including the first fully-connected layer always perform better.
    Li et al. [[36](#bib.bib36)] demonstrate that fusing convolutional and fully-connected
    layers outperforms the fusion of only convolutional layers. Fusing two convolutional
    layers with one fully-connected layer achieves the best performance on the Holiday
    and UKBench datasets.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑不同层的组合对于融合的效果非常重要，鉴于它们的差异性和互补性。Yu等人[[102](#bib.bib102)] 比较了完全连接层和卷积层在Oxford
    5k、Holiday和UKBench数据集上的不同组合的表现。结果显示，包括第一个完全连接层的组合总是表现更好。Li等人[[36](#bib.bib36)]
    证明了融合卷积层和完全连接层优于仅融合卷积层。将两个卷积层与一个完全连接层融合在Holiday和UKBench数据集上取得了最佳表现。
- en: '*b. Model-level Fusion*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*b. 模型级融合*'
- en: It is possible to combine features from different models; such a fusion focuses
    more on model complementarity, with methods categorized into *intra-model* and
    *inter-model*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 将不同模型的特征进行融合是可能的；这种融合更关注模型的互补性，方法被分为*模型内部*和*模型间*。
- en: Intra-model fusion suggests multiple deep models having similar or highly compatible
    structures, while inter-model fusion involves models with differing structures.
    For example, Simonyan et al. [[49](#bib.bib49)] introduce a ConvNet intra-model
    fusion strategy to improve the feature learning capacity of VGG where VGG-16 and
    VGG-19 are fused. To attend to different parts of an image object, Wang et al.
    [[105](#bib.bib105)] realize the multi-feature fusion by selecting all convolutional
    layers of VGG-16 to extract image representations, which is demonstrated to be
    more robust than using only single-layer features.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 模型内融合建议多个深度模型具有相似或高度兼容的结构，而模型间融合涉及结构不同的模型。例如，Simonyan 等人[[49](#bib.bib49)]引入了一种
    ConvNet 模型内融合策略，以提升 VGG 的特征学习能力，其中 VGG-16 和 VGG-19 被融合在一起。为了关注图像对象的不同部分，Wang 等人[[105](#bib.bib105)]
    通过选择 VGG-16 的所有卷积层来提取图像表示，从而实现多特征融合，这被证明比仅使用单层特征更为稳健。
- en: Inter-model fusion is a way to bridge different features given the fact that
    different deep networks have different receptive fields [[52](#bib.bib52)],[[68](#bib.bib68)],[[79](#bib.bib79)],[[97](#bib.bib97)].
    For instance, a two-stream attention network [[79](#bib.bib79)] is introduced
    to implement image retrieval where the main network for semantic prediction is
    VGG-16 while an auxiliary network is used for predicting attention maps. Similarly,
    considering the importance and necessity of inter-model fusion to bridge the gap
    between mid-level and high-level features, Liu et al. [[52](#bib.bib52)] and Zheng
    et al. [[97](#bib.bib97)] combine VGG-19 and AlexNet to learn combined features,
    while Ozaki et al. [[68](#bib.bib68)] concatenate descriptors from six different
    models.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 模型间融合是一种桥接不同特征的方法，因为不同的深度网络具有不同的感受野[[52](#bib.bib52)],[[68](#bib.bib68)],[[79](#bib.bib79)],[[97](#bib.bib97)]。例如，引入了一个双流注意力网络[[79](#bib.bib79)]来实现图像检索，其中主要网络用于语义预测的是
    VGG-16，而辅助网络用于预测注意力图。类似地，考虑到模型间融合在弥合中级特征和高级特征之间差距的重要性和必要性，Liu 等人[[52](#bib.bib52)]和
    Zheng 等人[[97](#bib.bib97)]结合了 VGG-19 和 AlexNet 来学习组合特征，而 Ozaki 等人[[68](#bib.bib68)]则将来自六个不同模型的描述符连接在一起。
- en: Inter-model and intra-model fusion are relevant to model selection. There are
    some strategies to determine how to combine the features from two models. It is
    straightforward to fuse all features from the candidate models and then learn
    a metric based on the concatenated features [[52](#bib.bib52)],[[79](#bib.bib79)],
    which is a kind of “*early fusion*” strategy. Alternatively, it is also possible
    to learn optimal metrics separately for the features from each model, and then
    to combine these metrics for final retrieval ranking [[36](#bib.bib36)],[[106](#bib.bib106)],
    which is a kind of “*late fusion*” strategy.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 模型间融合和模型内融合与模型选择有关。确定如何结合两个模型的特征有一些策略。直接将候选模型的所有特征融合在一起，然后基于连接后的特征学习一个度量[[52](#bib.bib52)],[[79](#bib.bib79)]，这是一种“*早期融合*”策略。另一种选择是分别为每个模型的特征学习最佳度量，然后将这些度量结合起来进行最终的检索排序[[36](#bib.bib36)],[[106](#bib.bib106)]，这是一种“*晚期融合*”策略。
- en: Discussion. Layer-level fusion and model-level fusion are conditioned on the
    fact that the associated layers or networks have different feature description
    capacities. For these fusion strategies, the key question is what features are
    the best to be combined? Some explorations have been made on the basis of off-the-shelf
    models, such as Xuan et al. [[107](#bib.bib107)], who illustrates the effect of
    combining different numbers of features and different sizes within the ensemble.
    Chen et al. [[108](#bib.bib108)] analyze the performance of embedded features
    from off-the-shelf image classification and object detection models with respect
    to image retrieval.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论。层级融合和模型级融合基于相关层或网络具有不同特征描述能力的事实。对于这些融合策略，关键问题是哪些特征组合效果最佳？在现成模型的基础上已进行了一些探索，例如
    Xuan 等人[[107](#bib.bib107)]，他们展示了组合不同数量和不同尺寸特征的效果。Chen 等人[[108](#bib.bib108)]分析了现成的图像分类和目标检测模型中嵌入特征在图像检索中的表现。
- en: 3.2 Feature Embedding and Aggregation
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 特征嵌入和聚合
- en: The primary aim of feature embedding and aggregation is to further promote feature
    discriminativity, targeting for the “discriminativity challenge”, and obtain final
    global and/or local features for retrieving specific instances.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 特征嵌入和聚合的主要目的是进一步提升特征的区分性，以应对“区分性挑战”，并获得最终的全局和/或局部特征以检索特定实例。
- en: 3.2.1 Matching with Global Features
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 与全局特征的匹配
- en: Global features can be extracted from fully-connected layers, followed by dimensionality
    reduction and normalization [[29](#bib.bib29)],[[39](#bib.bib39)]. They are easy
    to implement and there is no further aggregation process. Gong et al. [[30](#bib.bib30)]
    extract fully-connected activations for local image patches at three scale levels
    and embed patch-level activations individually using VLAD. Thus, the final concatenated
    features significantly tackle the invariance challenge caused by image rotations.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从全连接层中提取全局特征，接着进行降维和归一化[[29](#bib.bib29)],[[39](#bib.bib39)]。它们易于实现且无需进一步的聚合过程。Gong
    等人[[30](#bib.bib30)]在三个尺度级别上提取了局部图像块的全连接激活，并使用VLAD单独嵌入图块级别的激活。因此，最终的连接特征显著解决了由于图像旋转引起的不变性挑战。
- en: Convolutional features can also be aggregated into compact a global feature.
    Simple aggregation methods are sum/average or max pooling [[69](#bib.bib69)],[[70](#bib.bib70)].
    Sum/average pooling is less discriminative, because it takes into account all
    activated convolutional outputs, thereby weakening the effect of highly activated
    features [[34](#bib.bib34)]. As a result, max pooling is particularly well suited
    for sparse features having a low probability of being active, however max pooling
    may be inferior to sum/average pooling when image features are whitened [[12](#bib.bib12)].
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积特征还可以聚合成紧凑的全局特征。简单的聚合方法包括求和/平均或最大池化[[69](#bib.bib69)],[[70](#bib.bib70)]。求和/平均池化的判别能力较差，因为它考虑了所有激活的卷积输出，从而削弱了高激活特征的效果[[34](#bib.bib34)]。因此，最大池化特别适用于具有低激活概率的稀疏特征，但当图像特征被去白化时，最大池化可能不如求和/平均池化[[12](#bib.bib12)]。
- en: 'Figure [5](#S3.F5 "Figure 5 ‣ 3.1.2 Deep Feature Selection ‣ 3.1 Deep Feature
    Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance
    Retrieval: A Survey") illustrates sophisticated feature aggregation methods using
    channel-wise or spatial-wise weighting [[12](#bib.bib12)],[[62](#bib.bib62)].
    For example, Babenko et al. [[12](#bib.bib12)] propose sum-pooling convolutional
    features (SPoC) to obtain compact descriptors weighted by $\alpha^{\prime}$ with
    a Gaussian center prior. Similarly, it is possible to treat regions in feature
    maps as different sub-vectors [[32](#bib.bib32)],[[69](#bib.bib69)],[[97](#bib.bib97)],
    thus combinations of $R$ sub-vectors are used to represent the input image, such
    as R-MAC [[32](#bib.bib32)]. Since convolutional features may include repetitive
    patterns and each vector may correspond to identical regions, the resulting descriptors
    may be bursty, which makes the final aggregated global feature less distinguishable.
    As a solution, Pang et al. [[101](#bib.bib101)] leverage heat diffusion to weigh
    convolutional features at the aggregation stage, and reduce the undesirable influence
    of burstiness.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S3.F5 "Figure 5 ‣ 3.1.2 Deep Feature Selection ‣ 3.1 Deep Feature Extraction
    ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey")展示了使用通道或空间加权的复杂特征聚合方法[[12](#bib.bib12)],[[62](#bib.bib62)]。例如，Babenko
    等人[[12](#bib.bib12)]提出了求和池化卷积特征（SPoC），以获得由$\alpha^{\prime}$加权并具有高斯中心先验的紧凑描述符。类似地，可以将特征图中的区域视为不同的子向量[[32](#bib.bib32)],[[69](#bib.bib69)],[[97](#bib.bib97)]，因此使用$R$个子向量的组合来表示输入图像，如R-MAC[[32](#bib.bib32)]。由于卷积特征可能包含重复的模式，每个向量可能对应相同的区域，因此生成的描述符可能会有爆发性，这使得最终聚合的全局特征较难区分。作为解决方案，Pang
    等人[[101](#bib.bib101)]利用热扩散在聚合阶段对卷积特征进行加权，减少了爆发性的负面影响。'
- en: Convolutional features have an interpretation as descriptors of local regions,
    thus many works leverage embedding methods, including BoW, VLAD, and FV, to encode
    regional feature vectors and then aggregate them into a global descriptor. Note
    that BoW and VLAD can be extended by using other metrics, such as a Hamming distance
    [[109](#bib.bib109)]. Here we briefly describe the principle of Euclidean embeddings.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积特征可以解释为局部区域的描述符，因此许多工作利用嵌入方法，包括BoW、VLAD和FV，来编码区域特征向量，然后将其聚合成全局描述符。需要注意的是，BoW和VLAD可以通过使用其他度量，如汉明距离[[109](#bib.bib109)]，进行扩展。在这里，我们简要描述了欧几里得嵌入的原理。
- en: BoW [[24](#bib.bib24)] is a widely used feature embedding which leads to a sparse
    vector of occurrence. Let $\bm{a}=\left\{a_{1},a_{2},...,a_{R}\right\}$ be a set
    of $R$ local features, each of dimensionality $d$. BoW requires a pre-defined
    codebook $\bm{c}=\left\{c_{1},c_{2},...,c_{K}\right\}$ with $K$ centroids, usually
    learned offline, to cluster these local descriptors, and maps each descriptor
    $a_{t}$ to the nearest centroid $c_{k}$. For each centroid, one can count and
    normalize the number of occurrences as
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: BoW [[24](#bib.bib24)] 是一种广泛使用的特征嵌入方法，它会导致稀疏的出现向量。设 $\bm{a}=\left\{a_{1},a_{2},...,a_{R}\right\}$
    为一组 $R$ 个局部特征，每个特征的维度为 $d$。BoW 需要一个预定义的代码本 $\bm{c}=\left\{c_{1},c_{2},...,c_{K}\right\}$，其中包含
    $K$ 个中心点，通常是离线学习的，用于将这些局部描述符进行聚类，并将每个描述符 $a_{t}$ 映射到最近的中心点 $c_{k}$。对于每个中心点，可以计算并规范化出现次数，如下所示：
- en: '|  | $g(c_{k})=\frac{1}{R}\sum_{r=1}^{R}\phi(a_{r},c_{k})$ |  | (1) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $g(c_{k})=\frac{1}{R}\sum_{r=1}^{R}\phi(a_{r},c_{k})$ |  | (1) |'
- en: '|  | <math   alttext="\phi(a_{r},c_{k})=\left\{\begin{array}[]{ll}1&amp;\textrm{if
    $c_{k}$ is the closest codeword for $a_{r}$ }\\ 0&amp;\textrm{otherwise}\\'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\phi(a_{r},c_{k})=\left\{\begin{array}[]{ll}1&amp;\textrm{如果
    $c_{k}$ 是 $a_{r}$ 的最近代码字 }\\ 0&amp;\textrm{否则}\\'
- en: \end{array}\right." display="block"><semantics ><mrow ><mrow  ><mi >ϕ</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub  ><mi
    >a</mi><mi >r</mi></msub><mo >,</mo><msub  ><mi >c</mi><mi >k</mi></msub><mo stretchy="false"
    >)</mo></mrow></mrow><mo  >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  columnalign="left" ><mn  >1</mn></mtd><mtd
    columnalign="left"  ><mrow ><mtext >if </mtext><msub ><mi >c</mi><mi >k</mi></msub><mtext
    > is the closest codeword for </mtext><msub ><mi >a</mi><mi >r</mi></msub></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mn  >0</mn></mtd><mtd columnalign="left"  ><mtext
    >otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content"
    ><apply  ><apply ><ci >italic-ϕ</ci><interval closure="open" ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><ci >𝑟</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑐</ci><ci  >𝑘</ci></apply></interval></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><matrix  ><matrixrow ><cn type="integer"
    >1</cn><ci  ><mrow ><mtext >if </mtext><msub ><mi >c</mi><mi >k</mi></msub><mtext
    > is the closest codeword for </mtext><msub ><mi >a</mi><mi >r</mi></msub></mrow></ci></matrixrow><matrixrow
    ><cn type="integer" >0</cn><ci  ><mtext >otherwise</mtext></ci></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\phi(a_{r},c_{k})=\left\{\begin{array}[]{ll}1&\textrm{if
    $c_{k}$ is the closest codeword for $a_{r}$ }\\ 0&\textrm{otherwise}\\ \end{array}\right.</annotation></semantics></math>
    |  | (2) |
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\right." display="block"><semantics ><mrow ><mrow  ><mi >ϕ</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub  ><mi
    >a</mi><mi >r</mi></msub><mo >,</mo><msub  ><mi >c</mi><mi >k</mi></msub><mo stretchy="false"
    >)</mo></mrow></mrow><mo  >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  columnalign="left" ><mn  >1</mn></mtd><mtd
    columnalign="left"  ><mrow ><mtext >如果 </mtext><msub ><mi >c</mi><mi >k</mi></msub><mtext
    > 是 </mtext><msub ><mi >a</mi><mi >r</mi></msub></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mn  >0</mn></mtd><mtd columnalign="left"  ><mtext >否则</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci >italic-ϕ</ci><interval closure="open"
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑎</ci><ci >𝑟</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑐</ci><ci  >𝑘</ci></apply></interval></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><matrix  ><matrixrow ><cn type="integer"
    >1</cn><ci  ><mrow ><mtext >如果 </mtext><msub ><mi >c</mi><mi >k</mi></msub><mtext
    > 是 </mtext><msub ><mi >a</mi><mi >r</mi></msub></mrow></ci></matrixrow><matrixrow
    ><cn type="integer" >0</cn><ci  ><mtext >否则</mtext></ci></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\phi(a_{r},c_{k})=\left\{\begin{array}[]{ll}1&\textrm{如果
    $c_{k}$ 是 $a_{r}$ 的最近代码字 }\\ 0&\textrm{否则}\\ \end{array}\right.</annotation></semantics></math>
    |  | (2) |
- en: 'Thus BoW considers the number of descriptors belonging to each $c_{k}$ (*i.e.*
    0-order feature statistics), so the BoW representation is the concatenation of
    all mapped vectors:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，BoW 考虑了每个 $c_{k}$ 的描述符数量（*即* 0阶特征统计），所以 BoW 表示是所有映射向量的连接：
- en: '|  | $G_{{}_{BoW}}(\bm{a})=\left[\begin{array}[]{ccc}g(c_{1}),\cdots,g(c_{k}),\cdots,g(c_{K})\end{array}\right]\rm^{\top}$
    |  | (3) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | $G_{{}_{BoW}}(\bm{a})=\left[\begin{array}[]{ccc}g(c_{1}),\cdots,g(c_{k}),\cdots,g(c_{K})\end{array}\right]\rm^{\top}$
    |  | (3) |'
- en: BoW is simple to implement the encoding of local descriptors, such as convolutional
    feature maps [[36](#bib.bib36)],[[84](#bib.bib84)] or fully-connected activations
    [[86](#bib.bib86)],[[104](#bib.bib104)], or to encode regional descriptors [[110](#bib.bib110)],[[111](#bib.bib111)].
    Mukherjee et al. [[111](#bib.bib111)] extract image patches based on information
    entropy and feed into a pre-trained VGG-16, then use BoW to embed and aggregate
    the patch-level descriptors from a fully-connected layer. Embedded BoW vectors
    are typically high-dimensional and sparse, so not well suited to large-scale datasets
    in terms of the mentioned efficiency challenge.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: BoW简单实现了局部描述符的编码，例如卷积特征图[[36](#bib.bib36)],[[84](#bib.bib84)]或全连接激活[[86](#bib.bib86)],[[104](#bib.bib104)]，也可以对区域描述符进行编码[[110](#bib.bib110)],[[111](#bib.bib111)]。Mukherjee等人[[111](#bib.bib111)]基于信息熵提取图像补丁，并将其输入到预训练的VGG-16中，然后使用BoW嵌入并聚合来自全连接层的补丁级描述符。嵌入的BoW向量通常是高维且稀疏的，因此在提到的效率挑战方面不适合大规模数据集。
- en: 'VLAD [[54](#bib.bib54)] stores the sum of residuals for each visual word. Similar
    to BoW, it generates $K$ visual word centroids, then each feature $a_{r}$ is assigned
    to its nearest visual centroid $c_{k}$:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: VLAD [[54](#bib.bib54)] 存储每个视觉词的残差和。类似于BoW，它生成$K$个视觉词质心，然后每个特征$a_{r}$被分配给其最近的视觉质心$c_{k}$：
- en: '|  | $g(c_{k})=\frac{1}{R}\sum_{r=1}^{R}\phi(a_{r},c_{k})(a_{r}-c_{k})$ |  |
    (4) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $g(c_{k})=\frac{1}{R}\sum_{r=1}^{R}\phi(a_{r},c_{k})(a_{r}-c_{k})$ |  |
    (4) |'
- en: The VLAD representation is stacked by the residuals for all centroids, with
    dimension ($d\times K$), i.e.,
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: VLAD表示通过所有质心的残差进行堆叠，维度为($d\times K$)，即，
- en: '|  | $G_{{}_{VLAD}}(\bm{a})\!=\!\left[\begin{array}[]{ccc}\cdots,g(c_{k})\rm^{\top},\cdots\end{array}\right]\rm^{\top}.$
    |  | (5) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $G_{{}_{VLAD}}(\bm{a})\!=\!\left[\begin{array}[]{ccc}\cdots,g(c_{k})\rm^{\top},\cdots\end{array}\right]\rm^{\top}.$
    |  | (5) |'
- en: 'VLAD captures first-order feature statistics, i.e., ($a_{r}-c_{k}$). Similar
    to BoW, the performance of VLAD is affected by the number of clusters: more centroids
    produce larger vectors that are harder to index. For instance-level image retrieval,
    Gong et al. [[30](#bib.bib30)] concatenate the activations of a fully-connected
    layer with VLAD applied to image-level and patch-level inputs [[112](#bib.bib112)].
    Ng et al. [[50](#bib.bib50)] replace BoW [[24](#bib.bib24)] with VLAD [[54](#bib.bib54)],
    and are the first to encode local features into VLAD representations. This idea
    inspired another milestone work [[43](#bib.bib43)] where, for the first time,
    VLAD is plugged into the last convolutional layer, which allows end-to-end training
    via back-propagation.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: VLAD捕捉一阶特征统计信息，即($a_{r}-c_{k}$)。类似于BoW，VLAD的性能受集群数量的影响：更多的质心生成更大的向量，这些向量更难以索引。例如，对于实例级图像检索，Gong等人[[30](#bib.bib30)]将全连接层的激活与应用于图像级和补丁级输入的VLAD进行串联[[112](#bib.bib112)]。Ng等人[[50](#bib.bib50)]用VLAD
    [[54](#bib.bib54)]替换了BoW [[24](#bib.bib24)]，并首次将局部特征编码到VLAD表示中。这一思想启发了另一项里程碑式的工作[[43](#bib.bib43)]，该工作首次将VLAD插入到最后的卷积层中，从而实现了通过反向传播进行端到端的训练。
- en: FV [[55](#bib.bib55)] extends BoW by encoding the first and second order statistics.
    FV clusters the set of local descriptors by a Gaussian Mixture Model (GMM) with
    $K$ components to generate a dictionary $\bm{c}=\left\{\mu_{k};\Sigma_{k};w_{k}\right\}_{k=1}^{K}$
    made up of mean / covariance / weight triples [[113](#bib.bib113)], where the
    covariance may be simplified by keeping only its diagonal elements. For each local
    feature $a_{r}$, a GMM is given by
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: FV [[55](#bib.bib55)] 通过编码一阶和二阶统计信息扩展了BoW。FV通过高斯混合模型（GMM）对局部描述符集进行聚类，使用$K$个组件生成一个字典$\bm{c}=\left\{\mu_{k};\Sigma_{k};w_{k}\right\}_{k=1}^{K}$，由均值/协方差/权重三元组组成[[113](#bib.bib113)]，其中协方差可以通过仅保留对角元素来简化。对于每个局部特征$a_{r}$，GMM表示为
- en: '|  | $\displaystyle\gamma_{k}(a_{r})=w_{k}\times p_{k}(a_{r})/\Big{(}\sum_{k=1}^{K}w_{k}p_{k}(a_{r})\Big{)}\quad
    s.t.\sum_{k=1}^{K}w_{k}=1$ |  | (6) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\gamma_{k}(a_{r})=w_{k}\times p_{k}(a_{r})/\Big{(}\sum_{k=1}^{K}w_{k}p_{k}(a_{r})\Big{)}\quad
    s.t.\sum_{k=1}^{K}w_{k}=1$ |  | (6) |'
- en: where $p_{k}(a_{r})=\mathcal{N}(a_{r},\mu_{k},\sigma_{k}^{2})$. All local features
    are assigned into each component $k$ in the dictionary, which is computed as
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{k}(a_{r})=\mathcal{N}(a_{r},\mu_{k},\sigma_{k}^{2})$。所有局部特征被分配到字典中的每个组件$k$，其计算公式为
- en: '|  |  | $\displaystyle g_{w_{k}}=\frac{1}{R\sqrt{w_{k}}}\sum_{r=1}^{R}\Big{(}\gamma_{k}(a_{r})-w_{k}\Big{)}$
    |  | (7) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle g_{w_{k}}=\frac{1}{R\sqrt{w_{k}}}\sum_{r=1}^{R}\Big{(}\gamma_{k}(a_{r})-w_{k}\Big{)}$
    |  | (7) |'
- en: '|  |  | $\displaystyle g_{u_{k}}=\frac{\gamma_{k}(a_{r})}{R\sqrt{w_{k}}}\sum_{r=1}^{R}\left(\frac{a_{r}-\mu_{k}}{\sigma_{k}}\right),$
    |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle g_{u_{k}}=\frac{\gamma_{k}(a_{r})}{R\sqrt{w_{k}}}\sum_{r=1}^{R}\left(\frac{a_{r}-\mu_{k}}{\sigma_{k}}\right),$
    |  |'
- en: '|  |  | $\displaystyle g_{\sigma_{k}^{2}}=\frac{\gamma_{k}(a_{r})}{R\sqrt{2w_{k}}}\sum_{r=1}^{R}\left[{\left(\frac{a_{r}-\mu_{k}}{\sigma_{k}}\right)}^{2}-1\right]$
    |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle g_{\sigma_{k}^{2}}=\frac{\gamma_{k}(a_{r})}{R\sqrt{2w_{k}}}\sum_{r=1}^{R}\left[{\left(\frac{a_{r}-\mu_{k}}{\sigma_{k}}\right)}^{2}-1\right]$
    |  |'
- en: 'The FV representation is produced by concatenating vectors from the $K$ components:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: FV 表示是通过将来自 $K$ 组件的向量连接起来生成的：
- en: '|  | $\displaystyle\!\!\!G_{{}_{FV}}(\bm{a})\!=\!\left[\begin{array}[]{ccc}g_{w_{1}},\cdots,g_{w_{K}},g_{u_{1}},\cdots,g_{u_{K}},g_{\sigma_{1}^{2}},\cdots,g_{\sigma_{K}^{2}}\end{array}\right]\rm^{\top}$
    |  | (8) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\!\!\!G_{{}_{FV}}(\bm{a})\!=\!\left[\begin{array}[]{ccc}g_{w_{1}},\cdots,g_{w_{K}},g_{u_{1}},\cdots,g_{u_{K}},g_{\sigma_{1}^{2}},\cdots,g_{\sigma_{K}^{2}}\end{array}\right]\rm^{\top}$
    |  | (8) |'
- en: The FV representation defines a kernel from a generative process and captures
    more statistics than BoW and VLAD. FV vectors do not increase the computational
    cost significantly but require more memory. Applying FV without memory controls
    may lead to suboptimal performance [[114](#bib.bib114)].
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: FV 表示通过生成过程定义了一个核，并捕捉了比 BoW 和 VLAD 更多的统计量。FV 向量不会显著增加计算成本，但需要更多的内存。如果在没有内存控制的情况下应用
    FV 可能会导致次优性能 [[114](#bib.bib114)]。
- en: 'Discussion. Traditionally, pooling-based aggregation methods (e.g., in Figure
    [5](#S3.F5 "Figure 5 ‣ 3.1.2 Deep Feature Selection ‣ 3.1 Deep Feature Extraction
    ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey")) are directly plugged into deep networks and then the whole model is
    used end-to-end. The three embedding methods (BoW, VLAD, FV) are initially trained
    with large pre-defined vocabularies [[52](#bib.bib52)],[[115](#bib.bib115)]. One
    needs to pay attention on their properties before choosing an embedding: BoW and
    VLAD are computed in the rigid Euclidean space where performance is closely related
    to the number of centroids, whereas FV can capture higher-order statistics and
    improves the effectiveness of feature embedding at the expense of a higher memory
    cost. Further, although vocabularies are usually built separately and pre-trained
    before encoding deep features, it is necessary to integrate the training of networks
    and the learning of vocabulary parameters into a unified framework so as to guarantee
    training and testing efficiency. For example, VLAD is integrated into deep networks
    where each spatial column feature is used to construct clusters via k-means [[50](#bib.bib50)].
    This idea led to NetVLAD [[43](#bib.bib43)], where deep networks are fine-tuned
    with the VLAD vectors. The FV method is also combined with deep networks for retrieval
    tasks [[34](#bib.bib34)],[[41](#bib.bib41)].'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '讨论。传统上，基于池化的聚合方法（例如，见图 [5](#S3.F5 "Figure 5 ‣ 3.1.2 Deep Feature Selection
    ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep
    Learning for Instance Retrieval: A Survey")）直接插入深度网络中，然后整个模型以端到端的方式使用。这三种嵌入方法（BoW、VLAD、FV）最初是用大量预定义的词汇表进行训练的
    [[52](#bib.bib52)], [[115](#bib.bib115)]。在选择嵌入方法时需要注意它们的特性：BoW 和 VLAD 是在刚性的欧几里得空间中计算的，其性能与质心的数量密切相关，而
    FV 可以捕捉更高阶的统计量，并在更高的内存成本下提高特征嵌入的有效性。此外，尽管词汇表通常是在编码深度特征之前单独构建和预训练的，但有必要将网络的训练和词汇参数的学习集成到一个统一的框架中，以确保训练和测试的效率。例如，VLAD
    被集成到深度网络中，其中每个空间列特征用于通过 k-means [[50](#bib.bib50)] 构建簇。这个想法促成了 NetVLAD [[43](#bib.bib43)]，其中深度网络通过
    VLAD 向量进行微调。FV 方法也与深度网络结合用于检索任务 [[34](#bib.bib34)], [[41](#bib.bib41)]。'
- en: 3.2.2 Matching with Local Features
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 使用局部特征进行匹配
- en: Although matching with global features has high efficiency for both feature
    extraction and similarity computation, global features are not compatible with
    spatial verification and correspondence estimation, which are important procedures
    for instance-level retrieval tasks, motivating work on matching with local features.
    In terms of the matching process, global features are matched only once while
    local feature matching is evaluated by summarizing the similarity across all individual
    local features (i.e., many-to-many matching).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用全局特征在特征提取和相似度计算中具有高效率，但全局特征与空间验证和对应估计不兼容，而这些是实例级检索任务的重要步骤，这推动了对局部特征匹配的研究。在匹配过程中，全局特征只匹配一次，而局部特征匹配则通过总结所有单个局部特征的相似度来评估（即多对多匹配）。
- en: One important aspect of local features is to detect the keypoints for an instance
    within an image, and then to describe the detected keypoints as a set of local
    descriptors. Inspired by [[116](#bib.bib116)], the common strategies of this whole
    procedure for IIR can be categorized as detect-then-describe and describe-then-detect.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 局部特征的一个重要方面是检测图像中一个实例的关键点，然后将检测到的关键点描述为一组局部描述符。受到[[116](#bib.bib116)]的启发，这一整套程序的常见策略可以归纳为检测-再描述和描述-再检测。
- en: 'In terms of detect-then-describe, we regard the descriptors around keypoints
    as local features, similar to [[29](#bib.bib29)],[[42](#bib.bib42)]. Coarse regions
    can be detected, for example, by using the methods depicted in Figure [4](#S3.F4
    "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3
    Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey"), and regions of interest in an image can be detected by using region
    proposal networks (RPNs) [[27](#bib.bib27)],[[74](#bib.bib74)]. The extracted
    coarse regions around the keypoints are fed into a DCNN, followed by feature description.
    Traditional detectors can also be used to detect fine regions around a keypoint.
    For instance, Zheng et al. [[86](#bib.bib86)] employ the popular Hessian-Affine
    detector [[117](#bib.bib117)] to get an affine-invariant local region. Paulin
    et al. [[112](#bib.bib112)] and Mishchuk et al. [[110](#bib.bib110)] detect regions
    using the Hessian-Affine detector and feed into patch-convolutional kernel networks
    (Patch-CKNs) [[91](#bib.bib91)]. Note that it becomes more convenient for the
    case where bounding boxes annotations have been provided by datasets (see Section
    [5.1](#S5.SS1 "5.1 Datasets ‣ 5 State of the Art Performance ‣ Deep Learning for
    Instance Retrieval: A Survey")), and then the image regions can be cropped directly
    for further reranking [[92](#bib.bib92)].'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '就检测-再描述而言，我们将关键点周围的描述符视为局部特征，类似于[[29](#bib.bib29)],[[42](#bib.bib42)]。例如，可以通过使用图[4](#S3.F4
    "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3
    Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey")中所示的方法来检测粗略区域，图像中的感兴趣区域可以通过区域提议网络（RPNs）[[27](#bib.bib27)],[[74](#bib.bib74)]来检测。将关键点周围提取的粗略区域输入到DCNN中，随后进行特征描述。传统检测器也可以用于检测关键点周围的精细区域。例如，Zheng等人[[86](#bib.bib86)]使用流行的Hessian-Affine检测器[[117](#bib.bib117)]来获得一个仿射不变的局部区域。Paulin等人[[112](#bib.bib112)]和Mishchuk等人[[110](#bib.bib110)]使用Hessian-Affine检测器检测区域并输入到补丁卷积核网络（Patch-CKNs）[[91](#bib.bib91)]中。值得注意的是，对于已由数据集提供边界框注释的情况（见第[5.1](#S5.SS1
    "5.1 Datasets ‣ 5 State of the Art Performance ‣ Deep Learning for Instance Retrieval:
    A Survey")节），直接裁剪图像区域以便进一步重排序会更加方便[[92](#bib.bib92)]。'
- en: Rather than performing keypoint detection early on, it is possible to postpone
    the detection stage on the convolutional feature maps, i.e., describe-then-detect.
    One can select regions on the convolutional feature maps to obtain a set of local
    features [[32](#bib.bib32)],[[37](#bib.bib37)],[[84](#bib.bib84)]; the local maxima
    of the feature maps are then detected as keypoints [[56](#bib.bib56)]. A similar
    strategy is also used in network fine-tuning [[5](#bib.bib5)],[[60](#bib.bib60)],[[71](#bib.bib71)],[[74](#bib.bib74)],[[118](#bib.bib118)],
    where the keypoints on the convolutional feature maps can be selected based on
    attention scores predicted by an attention network [[71](#bib.bib71)],[[5](#bib.bib5)],
    or based on single-head and multi-head attention modules in transformers [[60](#bib.bib60)],[[59](#bib.bib59)].
    This approach to keypoint selection is better for achieving computational efficiency.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与其早期执行关键点检测，不如将检测阶段推迟到卷积特征图上，即描述-再检测。可以在卷积特征图上选择区域，以获得一组局部特征[[32](#bib.bib32)],[[37](#bib.bib37)],[[84](#bib.bib84)]；然后检测特征图的局部极大值作为关键点[[56](#bib.bib56)]。类似的策略也用于网络微调[[5](#bib.bib5)],[[60](#bib.bib60)],[[71](#bib.bib71)],[[74](#bib.bib74)],[[118](#bib.bib118)]，其中可以基于注意力网络[[71](#bib.bib71)],[[5](#bib.bib5)]预测的注意力分数，或基于变换器中的单头和多头注意力模块[[60](#bib.bib60)],[[59](#bib.bib59)]来选择卷积特征图上的关键点。这种关键点选择方法有助于提高计算效率。
- en: After keypoint detection and description, a large number of local features are
    used in the matching stage to perform instance-level retrieval, and the image
    similarity is evaluated by matching across all local features. Local matching
    techniques include spatial verification and selective match kernels (SMK) [[73](#bib.bib73)].
    Spatial verification assumes object instances are rigid so that local matches
    between images can be estimated as an affine transformation using RANdom SAmple
    Consensus (RANSAC) [[72](#bib.bib72)]. One limitation of RANSAC is its high computational
    complexity of estimating the transformation model when all local descriptors are
    considered; instead, it is possible to apply RANSAC to a small number of top-ranked
    local descriptors, such as those selected by approximate nearest neighbor [[5](#bib.bib5)].
    SMK weighs the contributions of individual matches with a non-linear selective
    function, but is still memory intensive. Its extension, the Aggregated Selective
    Match Kernel (ASMK), focuses more on aggregating similarities between local features
    without explicitly modeling the geometric alignment, which can produce a more
    compact representation [[73](#bib.bib73)],[[118](#bib.bib118)]. Recently, Teichmann
    et al. [[74](#bib.bib74)] introduced Regional Aggregated Selective Match Kernel
    (R-ASMK) to combine information from detected regions, boosting image retrieval
    accuracy compared to the ASMK.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键点检测和描述之后，大量局部特征在匹配阶段用于执行实例级别的检索，并通过匹配所有局部特征来评估图像相似性。局部匹配技术包括空间验证和选择性匹配核（SMK）[[73](#bib.bib73)]。空间验证假设物体实例是刚性的，因此可以使用随机采样一致性（RANSAC）[[72](#bib.bib72)]将图像之间的局部匹配估计为一个仿射变换。RANSAC
    的一个局限性是当考虑所有局部描述符时，估计变换模型的计算复杂度较高；相反，可以将 RANSAC 应用于少量的高排名局部描述符，例如那些由近似最近邻选择的描述符[[5](#bib.bib5)]。SMK
    使用非线性选择函数来权衡各个匹配的贡献，但仍然消耗大量内存。其扩展，聚合选择性匹配核（ASMK），更侧重于聚合局部特征之间的相似性，而不明确建模几何对齐，这可以生成更紧凑的表示[[73](#bib.bib73)],[[118](#bib.bib118)]。最近，Teichmann
    等[[74](#bib.bib74)]引入了区域聚合选择性匹配核（R-ASMK），通过结合检测到的区域信息，提高了图像检索的准确性，相较于 ASMK。
- en: 'Discussion. Using local descriptors to perform instance retrieval tasks has
    two limitations. First, the local descriptors for an image are stored individually
    and independently, which is memory-intensive, and not well-suited for large-scale
    scenarios. Second, estimating the similarity between the query and database images
    depends on cross-matching all local descriptor pairs, which incurs additional
    searching cost and then a low retrieval efficiency. Therefore, most instance retrieval
    systems using local features follow a two-stage paradigm: initial filtering and
    re-ranking [[64](#bib.bib64)],[[71](#bib.bib71)],[[75](#bib.bib75)],[[92](#bib.bib92)],[[119](#bib.bib119)],
    as in Figure [2](#S1.F2 "Figure 2 ‣ 1.2 Key Challenges ‣ 1 Introduction ‣ Deep
    Learning for Instance Retrieval: A Survey"). The initial filtering stage is to
    employ a global descriptor to select a set of candidate matching images, thereby
    reducing the solution space; the re-ranking stage is to use local descriptors
    to re-rank the top-ranked images from the global descriptor.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论。使用局部描述符执行实例检索任务有两个局限性。首先，图像的局部描述符是单独且独立存储的，这在内存上消耗较大，不适合大规模场景。其次，查询图像与数据库图像之间的相似性估计依赖于所有局部描述符对的交叉匹配，这会带来额外的搜索成本，从而导致检索效率低下。因此，大多数使用局部特征的实例检索系统遵循两阶段范式：初步筛选和重新排序[[64](#bib.bib64)],[[71](#bib.bib71)],[[75](#bib.bib75)],[[92](#bib.bib92)],[[119](#bib.bib119)]，如图[2](#S1.F2
    "图 2 ‣ 1.2 关键挑战 ‣ 1 引言 ‣ 深度学习用于实例检索：综述")所示。初步筛选阶段是使用全局描述符选择一组候选匹配图像，从而减少解空间；重新排序阶段则是使用局部描述符对来自全局描述符的高排名图像进行重新排序。
- en: 3.2.3 Attention Mechanism
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 注意机制
- en: 'Attention mechanism can be regarded as a kind of feature aggregation, whose
    aim is to highlight the most relevant feature parts. It can effectively address
    the “distraction challenge” and also promote feature discriminativity [[98](#bib.bib98)],
    realized by computing an attention map. Approaches to obtaining attention maps
    can be categorized into non-parametric and parametric groups, as shown in Figure [6](#S3.F6
    "Figure 6 ‣ 3.2.4 Hashing Embedding ‣ 3.2 Feature Embedding and Aggregation ‣
    3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey"), where the main difference is whether the importance weights in the
    attention map are learnable.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '注意力机制可以看作是一种特征聚合，其目的是突出最相关的特征部分。它能有效解决“分心挑战”，并且通过计算注意力图来提升特征的区分性[[98](#bib.bib98)]。获取注意力图的方法可以分为非参数和参数两大类，如图[6](#S3.F6
    "Figure 6 ‣ 3.2.4 Hashing Embedding ‣ 3.2 Feature Embedding and Aggregation ‣
    3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey")所示，其主要区别在于注意力图中的重要性权重是否可学习。'
- en: 'Non-parametric weighting is a straightforward method to highlight feature importance,
    and the corresponding attention maps can be obtained by channel-wise or spatial-wise
    pooling, as in Figure [6](#S3.F6 "Figure 6 ‣ 3.2.4 Hashing Embedding ‣ 3.2 Feature
    Embedding and Aggregation ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep
    Learning for Instance Retrieval: A Survey") (a,b). For spatial-wise pooling, Kalantidis
    et al. [[15](#bib.bib15)] propose an effective CroW method to weight and pool
    feature maps, which concentrate on weighting activations at different spatial
    locations, without considering the relations between these activations. In contrast,
    Ng et al. [[96](#bib.bib96)] explore the correlations among activations at different
    spatial locations on the convolutional feature maps.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '非参数加权是一种直观的方法，用于突出特征的重要性，相应的注意力图可以通过通道级或空间级池化获得，如图[6](#S3.F6 "Figure 6 ‣ 3.2.4
    Hashing Embedding ‣ 3.2 Feature Embedding and Aggregation ‣ 3 Retrieval with Off-the-Shelf
    DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey")（a,b）。对于空间级池化，Kalantidis
    等人[[15](#bib.bib15)] 提出了一个有效的 CroW 方法来加权和池化特征图，该方法集中于加权不同空间位置的激活，而不考虑这些激活之间的关系。相比之下，Ng
    等人[[96](#bib.bib96)] 探索了卷积特征图上不同空间位置激活之间的相关性。'
- en: Channel-wise weighting methods are also popular non-parametric attention mechanisms
    [[35](#bib.bib35)],[[100](#bib.bib100)]. Xu et al. [[35](#bib.bib35)] rank the
    weighted feature maps to build “probabilistic proposals” to select regional features.
    Jimenez et al. [[33](#bib.bib33)] combine CroW and R-MAC to propose Classes Activation
    Maps (CAM) to weigh the feature map per class. Xiang et al. [[100](#bib.bib100)]
    employ a Gram matrix to analyze the correlations between different channels and
    then obtain channel sensitivity information to tune the importance of each feature
    map. Channel-wise and spatial-wise weighting methods are usually integrated into
    a deep model to highlight feature importance [[15](#bib.bib15)],[[105](#bib.bib105)].
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通道级加权方法也是流行的非参数注意力机制[[35](#bib.bib35)], [[100](#bib.bib100)]。Xu 等人[[35](#bib.bib35)]
    对加权特征图进行排序，以建立“概率提议”来选择区域特征。Jimenez 等人[[33](#bib.bib33)] 结合 CroW 和 R-MAC 提出了类激活图（CAM），用于对每个类别的特征图进行加权。Xiang
    等人[[100](#bib.bib100)] 使用 Gram 矩阵分析不同通道之间的相关性，从而获得通道敏感性信息，以调整每个特征图的重要性。通道级和空间级加权方法通常集成到深度模型中，以突出特征的重要性[[15](#bib.bib15)],
    [[105](#bib.bib105)]。
- en: 'Parametric attention maps, shown in Figure [6](#S3.F6 "Figure 6 ‣ 3.2.4 Hashing
    Embedding ‣ 3.2 Feature Embedding and Aggregation ‣ 3 Retrieval with Off-the-Shelf
    DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey") (c,d), can be learned
    via deep networks, where the input can be either image patches or feature maps
    [[77](#bib.bib77)],[[100](#bib.bib100)],[[103](#bib.bib103)], approaches which
    are commonly used in supervised metric learning [[98](#bib.bib98)]. Kim et al.
    [[77](#bib.bib77)] make the first attempt to propose a shallow network (CRN) to
    take as input the feature maps of convolutional layers and outputs a weighted
    mask indicating the importance of spatial regions in the feature maps. The resulting
    mask modulates feature aggregation to create a global representation of the input
    image. Noh et al. [[5](#bib.bib5)] design a 2-layer CNN with a softplus output
    layer to compute scores which indicate the importance of different image regions.
    Inspired by R-MAC, Kim et al. [[120](#bib.bib120)] employ a pre-trained ResNet101
    to train a context-aware attention network using multi-scale feature maps.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化注意力图，如图 [6](#S3.F6 "图 6 ‣ 3.2.4 Hashing Embedding ‣ 3.2 特征嵌入与聚合 ‣ 3 使用现成的
    DCNN 模型进行检索 ‣ 深度学习实例检索：综述") (c,d) 所示，可以通过深度网络进行学习，其中输入可以是图像块或特征图 [[77](#bib.bib77)],[[100](#bib.bib100)],[[103](#bib.bib103)]，这些方法通常用于监督度量学习
    [[98](#bib.bib98)]。Kim 等 [[77](#bib.bib77)] 首次尝试提出一种浅层网络（CRN），以卷积层的特征图作为输入，并输出一个加权掩模，表示特征图中空间区域的重要性。结果掩模调节特征聚合，以创建输入图像的全局表示。Noh
    等 [[5](#bib.bib5)] 设计了一个具有 softplus 输出层的 2 层 CNN 来计算分数，指示不同图像区域的重要性。受到 R-MAC 的启发，Kim
    等 [[120](#bib.bib120)] 使用预训练的 ResNet101 训练一个上下文感知的注意力网络，利用多尺度特征图。
- en: Apart from using feature maps as inputs, a whole image can be used to learn
    feature importance, for which specific networks are needed [[78](#bib.bib78)],[[79](#bib.bib79)],[[121](#bib.bib121)].
    Mohedano [[78](#bib.bib78)] explores different saliency models, including DeepFixNet
    and Saliency Attentive Model. Yang et al. [[79](#bib.bib79)] and Wei et al. [[121](#bib.bib121)]
    introduce a two-stream network for image retrieval in which the auxiliary stream,
    DeepFixNet, is used specifically for predicting attention maps, which are then
    fused with the feature maps produced by the main network. For image retrieval,
    attention mechanisms can be combined with supervised metric learning [[96](#bib.bib96)].
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用特征图作为输入外，还可以使用整个图像来学习特征的重要性，这需要特定的网络 [[78](#bib.bib78)],[[79](#bib.bib79)],[[121](#bib.bib121)]。Mohedano
    [[78](#bib.bib78)] 探索了不同的显著性模型，包括 DeepFixNet 和 Saliency Attentive Model。Yang 等
    [[79](#bib.bib79)] 和 Wei 等 [[121](#bib.bib121)] 引入了一种双流网络用于图像检索，其中辅助流 DeepFixNet
    专门用于预测注意力图，然后与主网络生成的特征图融合。在图像检索中，注意力机制可以与监督度量学习 [[96](#bib.bib96)] 结合。
- en: 3.2.4 Hashing Embedding
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 哈希嵌入
- en: Real-valued features extracted by deep networks are typically high-dimensional,
    and therefore are not well-suited to retrieval efficiency. As a result, there
    is significant motivation to transform deep features into more compact codes.
    Since their computational and storage efficiency are beneficial for the “efficiency
    challenge”, hashing algorithms have been widely used for global [[62](#bib.bib62)],[[80](#bib.bib80)]
    and local descriptors [[64](#bib.bib64)],[[85](#bib.bib85)],[[86](#bib.bib86)].
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深度网络提取的实值特征通常是高维的，因此不适合检索效率。因此，有很大的动机将深度特征转换为更紧凑的编码。由于其计算和存储效率对“效率挑战”有利，哈希算法已广泛用于全局
    [[62](#bib.bib62)],[[80](#bib.bib80)] 和局部描述符 [[64](#bib.bib64)],[[85](#bib.bib85)],[[86](#bib.bib86)]。
- en: '![Refer to caption](img/8d4020b24414c00022ae220e8a76e3c9.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8d4020b24414c00022ae220e8a76e3c9.png)'
- en: 'Figure 6: Illustration of attention mechanisms. (a)-(b) Non-parametric schemes:
    The attention is based on convolutional feature $\bm{A}$. Channel-wise attention
    in (a) produces a $C$-dimensional importance vector $\beta_{1}$ [[15](#bib.bib15)],[[35](#bib.bib35)];
    Spatial-wise attention in (b) computes a 2-dimensional attention map $\alpha$
    [[15](#bib.bib15)],[[33](#bib.bib33)],[[96](#bib.bib96)]. (c)-(d) Parametric schemes:
    The attention weights are learned by a trainable network. In (c), $\beta_{2}$
    are provided by a sub-network with parameters $\theta_{\gamma}$ [[71](#bib.bib71)],[[77](#bib.bib77)],[[98](#bib.bib98)],[[120](#bib.bib120)].
    In (d), the attention maps, as a tensor, are predicted by some auxiliary saliency
    extraction models from the input image directly [[78](#bib.bib78)],[[79](#bib.bib79)],[[121](#bib.bib121)].'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：注意力机制的示意图。 (a)-(b) 非参数方案：注意力基于卷积特征$\bm{A}$。 (a)中的通道注意力产生了一个$C$维的重要性向量$\beta_{1}$
    [[15](#bib.bib15)],[[35](#bib.bib35)]; (b)中的空间注意力计算了一个二维注意力图$\alpha$ [[15](#bib.bib15)],[[33](#bib.bib33)],[[96](#bib.bib96)]。
    (c)-(d) 参数方案：注意力权重由可训练网络学习。在(c)中，$\beta_{2}$由一个具有参数$\theta_{\gamma}$的子网络提供[[71](#bib.bib71)],[[77](#bib.bib77)],[[98](#bib.bib98)],[[120](#bib.bib120)]。在(d)中，作为张量的注意力图由一些辅助显著性提取模型直接从输入图像中预测[[78](#bib.bib78)],[[79](#bib.bib79)],[[121](#bib.bib121)]。
- en: Hash functions can be plugged as a layer into deep networks, so that hash codes
    and deep networks can be simultaneously trained and optimized, either supervised
    [[80](#bib.bib80)] or unsupervised [[65](#bib.bib65)]. During hash function training,
    the hash codes of originally similar images are embedded as closely as possible,
    and the hash codes of dissimilar images are as separated as possible. $d$-dim
    hash codes from a hash function $h(\cdot)$ for an image $x$ can be formulated
    as $b_{x}=h(x)=h\big{(}f(x;\bm{\theta})\big{)}\in\{+1,-1\}^{d}$. Because hash
    codes are non-differentiable their optimization is difficult, so $h(\cdot)$ can
    be relaxed to be differentiable by using tanh or sigmoid functions [[22](#bib.bib22)].
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希函数可以作为一个层嵌入到深度网络中，从而使哈希码和深度网络可以同时进行训练和优化，无论是有监督[[80](#bib.bib80)]还是无监督[[65](#bib.bib65)]。在哈希函数训练过程中，将原本相似的图像的哈希码尽可能嵌入得很紧密，而将不相似图像的哈希码尽可能分开。来自哈希函数$h(\cdot)$的$d$-dim哈希码对于图像$x$可以被公式化为$b_{x}=h(x)=h\big{(}f(x;\bm{\theta})\big{)}\in\{+1,-1\}^{d}$。由于哈希码是不可微分的，它们的优化是困难的，因此可以通过使用tanh或sigmoid函数[[22](#bib.bib22)]使得$h(\cdot)$放松为可微分的。
- en: When binarizing real-valued features, it is crucial to preserve image similarity
    and to improve hash code quality [[22](#bib.bib22)]. These two aspects are at
    the heart of hashing algorithms to maximize retrieval accuracy.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在将实值特征二值化时，保持图像相似性和提高哈希码质量至关重要[[22](#bib.bib22)]。这两个方面是哈希算法的核心，以最大化检索准确性。
- en: '*a. Hash Functions to Preserve Image Similarity*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*a. 哈希函数用于保持图像相似性*'
- en: Preserving similarity seeks to minimize the inconsistencies between real-valued
    features and corresponding hash codes, for which a variety of strategies have
    been adopted.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 保持相似性旨在最小化实值特征与相应哈希码之间的不一致性，为此采用了多种策略。
- en: Loss functions can significantly influence similarity preservation, which includes
    both supervised and unsupervised methods. With class labels available, many loss
    functions are designed to learn hash codes in a Hamming space. As a straightforward
    method, one can optimize either the difference between matrices computed from
    the binary codes and their supervision labels [[62](#bib.bib62)],[[81](#bib.bib81)]
    or the difference between the hash codes and real-valued deep features [[64](#bib.bib64)],[[65](#bib.bib65)].
    Song et al. [[64](#bib.bib64)] propose to learn hash codes for regional features
    in which each local feature is converted to a set of binary codes by multiplying
    a hash function and the raw RoI features, then the differences between RoI features
    and hash codes are characterized by an L[2] loss. Do et al. [[122](#bib.bib122)]
    regularize hash codes with a reconstruction loss, which ensure that codes can
    be reconstructed to their inputs so that similar/dissimilar inputs are mapped
    to similar/dissimilar hash codes. Lin et al. [[65](#bib.bib65)] learn hash codes
    and address the “invariance challenge” by introducing an objective function which
    characterize the difference between the binary codes which are computed from the
    original image and the geometric transformed one.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数可以显著影响相似性保留，这包括监督和无监督方法。在有类别标签的情况下，许多损失函数旨在学习哈希码在汉明空间中的表示。作为一种直接的方法，可以优化从二进制码和其监督标签计算的矩阵之间的差异[[62](#bib.bib62)],[[81](#bib.bib81)]，或者哈希码与实际深度特征之间的差异[[64](#bib.bib64)],[[65](#bib.bib65)]。Song等人[[64](#bib.bib64)]提出为区域特征学习哈希码，其中每个局部特征通过乘以哈希函数和原始RoI特征转换为一组二进制码，然后通过L[2]损失来表征RoI特征和哈希码之间的差异。Do等人[[122](#bib.bib122)]用重建损失对哈希码进行正则化，确保代码能够重建到其输入，从而使相似/不相似的输入被映射到相似/不相似的哈希码。Lin等人[[65](#bib.bib65)]学习哈希码并通过引入目标函数来解决“不可变性挑战”，该目标函数表征了从原始图像和几何变换图像计算的二进制码之间的差异。
- en: '*b. Improving Hash Function Quality*'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*b. 改善哈希函数质量*'
- en: A good hash function seeks to have binary codes uniformly distributed; that
    is, maximally filling and using the hash code space, normally on the basis of
    bit uncorrelation and bit balance [[22](#bib.bib22)],[[65](#bib.bib65)]. Bit uncorrelation
    implies that different bits are as independent as possible, so that a given set
    of bits can aggregate more information within a given code length [[65](#bib.bib65)].
    Bit balance means that each bit should have a 50% chance of being +1 or -1, thereby
    maximizing code variance and information [[22](#bib.bib22)]. Morère et al. [[62](#bib.bib62)]
    use the uniform distribution $U$(0,1) to build a regularization term to make hash
    codes distribute evenly where the codes are learned by a Restricted Boltzmann
    Machine layer. Likewise, Lin et al. [[65](#bib.bib65)] optimize the mean of learned
    hash codes to be close to 0.5 to prevent any bit bias towards zero or one.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的哈希函数旨在使二进制码均匀分布；也就是说，最大程度地填充和使用哈希码空间，通常基于位的无关性和位平衡[[22](#bib.bib22)],[[65](#bib.bib65)]。位的无关性意味着不同的位应尽可能独立，这样在给定的码长内，一组位可以聚合更多的信息[[65](#bib.bib65)]。位平衡意味着每个位应有50%的机会为+1或-1，从而最大化码的方差和信息[[22](#bib.bib22)]。Morère等人[[62](#bib.bib62)]使用均匀分布$U$(0,1)建立正则化项，以使哈希码分布均匀，其中代码由限制玻尔兹曼机层学习。同样，Lin等人[[65](#bib.bib65)]优化学习的哈希码的均值，使其接近0.5，以防止任何位偏向于零或一。
- en: 4 Retrieval via Learning DCNN Representations
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 通过学习DCNN表示进行检索
- en: 'The off-the-shelf DCNNs pre-trained on source datasets for classification are
    quite robust to inter-class variability. However, in most cases, deep features
    extracted based on off-the-shelf models may not be sufficient for accurate retrieval,
    even with the strategies discussed in Section [3](#S3 "3 Retrieval with Off-the-Shelf
    DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey"). In order for models
    to be more effective for retrieval, a common practice is network fine-tuning,
    i.e., updating the pre-stored parameters [[31](#bib.bib31)]. Fine-tuning methods
    have been studied extensively to learn better features, whose primary aim is to
    address the “fine-tune challenge”. A standard dataset with clear and well-defined
    ground-truth labels is indispensable for the supervised fine-tuning and subsequently
    pair-wise supervisory information is incorporated into ranking loss to update
    networks by regularizing on retrieval representations, otherwise it is necessary
    to develop unsupervised fine-tuned methods. After network fine-tuning, features
    can be organized as global or local to perform retrieval.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '在源数据集上进行分类预训练的现成DCNNs对类间变异性相当鲁棒。然而，在大多数情况下，即使采用了第[3](#S3 "3 Retrieval with
    Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey")节中讨论的策略，基于现成模型提取的深度特征也可能不足以进行准确的检索。为了使模型在检索中更有效，常见的做法是网络微调，即更新预存的参数
    [[31](#bib.bib31)]。微调方法已经被广泛研究，以学习更好的特征，其主要目标是解决“微调挑战”。一个具有清晰且明确的真实标签的标准数据集对于监督微调至关重要，随后将成对的监督信息纳入排名损失，通过对检索表示进行正则化来更新网络，否则需要开发无监督的微调方法。网络微调后，特征可以组织为全局或局部以进行检索。'
- en: 'For the most feature strategies we presented in Section [3](#S3 "3 Retrieval
    with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey"),
    including feature extraction, feature embedding and feature aggregation. Note
    that fine-tuning does not contradict or render irrelevant these feature processing
    methods; indeed, these strategies are complementary and can be equivalently incorporated
    as part of network fine-tuning. To this end, this section will survey the strategies
    which have been developed, based on the patch-level, image-level, or class-level
    supervision, to fine-tune deep networks for better instance retrieval.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在第[3](#S3 "3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for
    Instance Retrieval: A Survey")节中介绍的大多数特征策略，包括特征提取、特征嵌入和特征聚合。请注意，微调并不与这些特征处理方法相矛盾或使其无关;
    实际上，这些策略是互补的，并可以等效地作为网络微调的一部分纳入其中。为此，本节将调查基于补丁级、图像级或类别级监督开发的策略，以微调深度网络以实现更好的实例检索。'
- en: '![Refer to caption](img/3b0549641473a6e43a864c562d5184fa.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3b0549641473a6e43a864c562d5184fa.png)'
- en: 'Figure 7: Schemes of supervised fine-tuning. Anchor, positive, and negative
    images are indicated by $x_{a}$, $x_{p}$, $x_{n}$, respectively. (a) classification
    loss [[39](#bib.bib39)]; (b) similarity learning by using a transformation matrix
    [[40](#bib.bib40)]; (c) Siamese loss [[46](#bib.bib46)],[[41](#bib.bib41)],[[123](#bib.bib123)],[[124](#bib.bib124)];
    (d) triplet loss [[42](#bib.bib42)]; (e) an attention block into DCNNs to highlight
    regions [[64](#bib.bib64)]; (f) combining classification loss and pairwise ranking
    loss [[100](#bib.bib100)],[[125](#bib.bib125)]; (g) region proposal networks (RPNs)
    to locate the RoI and highlight specific regions or instances [[92](#bib.bib92)];
    (h) inserting the RPNs of (g) into DCNNs, such that the RPNs extract regions or
    instances at the convolutional layer [[42](#bib.bib42)],[[82](#bib.bib82)].'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：监督微调的方案。锚点、正样本和负样本分别由$x_{a}$、$x_{p}$、$x_{n}$表示。(a) 分类损失 [[39](#bib.bib39)];
    (b) 使用变换矩阵进行相似性学习 [[40](#bib.bib40)]; (c) Siamese损失 [[46](#bib.bib46)],[ [41](#bib.bib41)],[
    [123](#bib.bib123)],[ [124](#bib.bib124)]; (d) 三元组损失 [[42](#bib.bib42)]; (e) 将注意力块引入DCNN以突出区域
    [[64](#bib.bib64)]; (f) 结合分类损失和成对排名损失 [[100](#bib.bib100)],[ [125](#bib.bib125)];
    (g) 区域提议网络（RPNs）用于定位RoI并突出特定区域或实例 [[92](#bib.bib92)]; (h) 将（g）的RPNs插入DCNN中，使RPNs在卷积层提取区域或实例
    [[42](#bib.bib42)],[ [82](#bib.bib82)]。
- en: 4.1 Supervised Fine-tuning
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 监督微调
- en: The way to realize supervised fine-tuning can be determined by the given class
    labels or pairwise supervisory information.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 实现监督微调的方法可以通过给定的类别标签或成对的监督信息来确定。
- en: 4.1.1 Fine-tuning via Classification Loss
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 通过分类损失进行微调
- en: 'When class labels of a new dataset are available (e.g., INSTRE [[93](#bib.bib93)],
    GLDv2 [[5](#bib.bib5)],[[63](#bib.bib63)]), it is preferable to begin with a previously-trained
    DCNN, trained on a separate dataset, with the backbone DCNN typically chosen from
    one of AlexNet, VGG, GoogLeNet, or ResNet. The DCNN can then be fine-tuned, as
    shown in Figure [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey") (a), by optimizing its parameters
    on the basis of a cross entropy loss'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '当新数据集的类标签可用时（例如，INSTRE [[93](#bib.bib93)]，GLDv2 [[5](#bib.bib5)]，[[63](#bib.bib63)]），最好从一个先前在不同数据集上训练的DCNN开始，骨干DCNN通常从AlexNet、VGG、GoogLeNet或ResNet中选择。然后，可以通过如图[7](#S4.F7
    "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning for
    Instance Retrieval: A Survey") (a)所示的方法对DCNN进行微调，优化其参数，基于交叉熵损失。'
- en: '|  | $\mathcal{L}_{CE}(\hat{p_{i}},y_{i})=-\!\sum^{c}_{i}\!\big{(}y_{i}\!\times\!log(\hat{p}_{i})\big{)}$
    |  | (9) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{CE}(\hat{p_{i}},y_{i})=-\!\sum^{c}_{i}\!\big{(}y_{i}\!\times\!log(\hat{p}_{i})\big{)}$
    |  | (9) |'
- en: Here $y_{i}$ and $\hat{p}_{i}$ are the ground-truth labels and the predicted
    logits, respectively, and $c$ is the total number of categories. The milestone
    work in such fine-tuning is [[39](#bib.bib39)], in which AlexNet is re-trained
    on the Landmarks dataset. According to the class labels, the image-level features
    are required to compute the logits. Thus, the descriptors extracted from local
    regions on convolutional feature maps [[5](#bib.bib5)],[[71](#bib.bib71)] or image
    patch inputs [[74](#bib.bib74)] are further needed to be aggregated.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $y_{i}$ 和 $\hat{p}_{i}$ 分别是真实标签和预测logits，而 $c$ 是类别总数。在这种微调中的里程碑工作是[[39](#bib.bib39)]，其中AlexNet在Landmarks数据集上重新训练。根据类别标签，需要图像级特征来计算logits。因此，需要进一步聚合从卷积特征图[[5](#bib.bib5)]，[[71](#bib.bib71)]或图像补丁输入[[74](#bib.bib74)]中提取的描述符。
- en: A classification-based fine-tuning method enables to enforce higher similarity
    for intra-class samples and diversity for inter-class samples. Cao et al. [[71](#bib.bib71)]
    employ the ArcFace loss [[126](#bib.bib126)], which uses the margin-adjusted cosine
    similarity in the form of softmax loss, to induce smaller intra-class variance
    and show excellent results for instance retrieval. Recently, Boudiaf et al. [[127](#bib.bib127)]
    claim that cross entropy loss can minimize intra-class distances while maximizing
    inter-class distances. Cross entropy loss is, in essence, maximizing a common
    mutual information between the retrieval features and the ground-truth labels.
    Therefore, it can be regarded as an upper bound on a new pairwise loss, which
    has a structure similar to various pairwise ranking losses, of which representatives
    are introduced below.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分类的微调方法可以增强类内样本的相似性，并为类间样本提供多样性。Cao等人[[71](#bib.bib71)]使用ArcFace损失[[126](#bib.bib126)]，它以softmax损失的形式使用边际调整的余弦相似度，以减少类内方差，并在实例检索中表现出色。最近，Boudiaf等人[[127](#bib.bib127)]声称交叉熵损失可以在最大化类间距离的同时最小化类内距离。交叉熵损失本质上是在最大化检索特征与真实标签之间的共同互信息。因此，它可以被视为一种新的对偶损失的上界，该损失具有类似于各种对偶排序损失的结构，下面将介绍一些代表性的例子。
- en: 4.1.2 Fine-tuning via Pairwise Ranking Loss
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 通过对偶排序损失进行微调
- en: 'With affinity information (e.g., samples from the same group) indicating similar
    and dissimilar pairs, fine-tuning methods based on pairwise ranking loss learn
    an optimal metric which minimizes or maximizes the distance of pairs to maintain
    their similarity. Network fine-tuning via ranking loss involves two types of information
    [[17](#bib.bib17)]:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通过相似度信息（例如，同一组中的样本）表示相似和不相似的对，基于对偶排序损失的微调方法学习一个最佳度量，该度量最小化或最大化对之间的距离以保持它们的相似性。通过排序损失进行网络微调涉及两种类型的信息[[17](#bib.bib17)]：
- en: '1.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'A pair-wise constraint, corresponding to a Siamese network as in Figure [7](#S4.F7
    "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning for
    Instance Retrieval: A Survey") (c), in which input images are paired with either
    a positive or negative sample;'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '一对一约束，对应于如图[7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey") (c)所示的Siamese网络，其中输入图像与正样本或负样本配对；'
- en: '2.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'A triplet constraint, associated with triplet networks as in Figure [7](#S4.F7
    "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning for
    Instance Retrieval: A Survey") (e), in which anchor images are paired with both
    similar and dissimilar samples [[17](#bib.bib17)].'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '一个三元组约束，关联于图 [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey") (e) 中的三元组网络，其中锚点图像与相似和不相似的样本配对
    [[17](#bib.bib17)]。'
- en: 'These pairwise ranking loss based methods are categorized into globally supervised
    approaches (Figure [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey") (c,d)) and locally supervised
    approaches (Figure [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey") (g,h)), where the former ones
    learn a metric on global features by satisfying all constraints, whereas the latter
    ones focus on local areas by only satisfying the given local constraints (*e.g.*
    region proposals).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '基于对偶排名损失的方法分为全局监督方法（图 [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN
    Representations ‣ Deep Learning for Instance Retrieval: A Survey") (c,d)）和局部监督方法（图
    [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning
    for Instance Retrieval: A Survey") (g,h)），前者通过满足所有约束来学习全局特征上的度量，而后者则通过仅满足给定的局部约束（*例如*，区域提议）来关注局部区域。'
- en: To be specific, consider a triplet set $X\!\!=\!\!\{(x_{a},x_{p},x_{n})\}$ in
    a mini-batch, where $(x_{a},x_{p})$ indicates a similar pair and $(x_{a},x_{n})$
    a dissimilar pair. Features $f(x;\bm{\theta})$ of one image are extracted by a
    network $f(\cdot)$ with parameters $\bm{\theta}$, for which we can represent the
    affinity information for each similar or dissimilar pair as
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，考虑一个三元组集 $X\!\!=\!\!\{(x_{a},x_{p},x_{n})\}$ 在一个小批量中，其中 $(x_{a},x_{p})$
    表示相似对，而 $(x_{a},x_{n})$ 表示不相似对。通过网络 $f(\cdot)$ 和参数 $\bm{\theta}$ 提取一个图像的特征 $f(x;\bm{\theta})$，对于每对相似或不相似的对，我们可以表示其亲和信息为
- en: '|  | $\displaystyle\mathcal{D}_{ij}=\mathcal{D}(x_{i},x_{j})=&#124;&#124;f(x_{i};\bm{\theta})-f(x_{j};\bm{\theta})&#124;&#124;_{2}^{2}$
    |  | (10) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{D}_{ij}=\mathcal{D}(x_{i},x_{j})=&#124;&#124;f(x_{i};\bm{\theta})-f(x_{j};\bm{\theta})&#124;&#124;_{2}^{2}$
    |  | (10) |'
- en: '*a. Refining with Transformation Matrix*.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*a. 使用变换矩阵进行优化*。'
- en: 'Learning the similarity among input samples can be implemented by optimizing
    the weights of a linear transformation matrix [[40](#bib.bib40)]. It transforms
    the concatenated feature pairs into a common latent space using a transformation
    matrix $\bm{W}\!\!\in\!\!\mathbb{R}^{2d\times 1}$, where $d$ is the final feature
    dimension. The similarity score of these pairs are predicted via a sub-network
    $\mathcal{S}_{W}(x_{i},x_{j})=f_{W}(f(x_{i};\bm{\theta})\cup f(x_{j};\bm{\theta});\bm{W})$
    [[40](#bib.bib40)]. In other words, the sub-network $f_{W}$ predicts how similar
    the feature pairs are. Given the affinity information of feature pairs $\mathcal{S}_{ij}=\mathcal{S}(x_{i},x_{j})\!\in\!\{0,1\}$,
    the binary labels 0 and 1 indicate the similar (positive) or dissimilar (negative)
    pairs, respectively. The training of function $f_{W}$ can be achieved by using
    a regression loss:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通过优化线性变换矩阵的权重来学习输入样本之间的相似性 [[40](#bib.bib40)]。它使用变换矩阵 $\bm{W}\!\!\in\!\!\mathbb{R}^{2d\times
    1}$ 将连接的特征对变换到一个共同的潜在空间，其中 $d$ 是最终的特征维度。这些对的相似性得分通过子网络 $\mathcal{S}_{W}(x_{i},x_{j})=f_{W}(f(x_{i};\bm{\theta})\cup
    f(x_{j};\bm{\theta});\bm{W})$ 进行预测 [[40](#bib.bib40)]。换句话说，子网络 $f_{W}$ 预测特征对的相似度。给定特征对的亲和信息
    $\mathcal{S}_{ij}=\mathcal{S}(x_{i},x_{j})\!\in\!\{0,1\}$，二进制标签 0 和 1 分别表示相似（正）对或不相似（负）对。函数
    $f_{W}$ 的训练可以通过使用回归损失来实现：
- en: '|  | $\displaystyle\!\!\!\mathcal{L}_{W}(x_{i},x_{j})=$ | $\displaystyle&#124;\mathcal{S}_{W}(x_{i},x_{j})-\mathcal{S}_{ij}\big{(}\text{sim}(x_{i},x_{j})+m\big{)}-$
    |  | (11) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\!\!\!\mathcal{L}_{W}(x_{i},x_{j})=$ | $\displaystyle&#124;\mathcal{S}_{W}(x_{i},x_{j})-\mathcal{S}_{ij}\big{(}\text{sim}(x_{i},x_{j})+m\big{)}-$
    |  | (11) |'
- en: '|  |  | $\displaystyle(1-\mathcal{S}_{ij})\big{(}\text{sim}(x_{i},x_{j})-m\big{)}&#124;$
    |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle(1-\mathcal{S}_{ij})\big{(}\text{sim}(x_{i},x_{j})-m\big{)}&#124;$
    |  |'
- en: 'where $\text{sim}(x_{i},x_{j})$ can be the cosine function for guiding the
    training of $\bm{W}$ and $m$ is a margin. By optimizing the regression loss and
    updating $\bm{W}$, deep networks maximize the similarity of similar pairs and
    minimize that of dissimilar pairs. It is worth noting that the pre-stored parameters
    in the deep models are frozen when optimizing $\bm{W}$. The pipeline of this approach
    is depicted in Figure [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey") (b).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$\text{sim}(x_{i},x_{j})$可以是用于指导$\bm{W}$训练的余弦函数，而$m$是一个边际。通过优化回归损失并更新$\bm{W}$，深度网络最大化相似对的相似度，并最小化不相似对的相似度。值得注意的是，在优化$\bm{W}$时，深度模型中预存的参数是被冻结的。这种方法的流程如图[7](#S4.F7
    "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning for
    Instance Retrieval: A Survey")（b）所示。'
- en: '*b. Fine-tuning with Siamese Networks*.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*b. 使用Siamese网络进行微调*。'
- en: 'Siamese networks represent important options in implementing metric learning
    for fine-tuning, as in Figure [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning
    DCNN Representations ‣ Deep Learning for Instance Retrieval: A Survey") (c) and
    Figure [8](#S4.F8 "Figure 8 ‣ 4.1.2 Fine-tuning via Pairwise Ranking Loss ‣ 4.1
    Supervised Fine-tuning ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep
    Learning for Instance Retrieval: A Survey") (a). It is a structure composed of
    two branches that share the same weights across layers. Siamese networks are trained
    on paired data, consisting of an image pair $(x_{i},x_{j})$ such that $\mathcal{S}(x_{i},x_{j})\!\in\!\{0,1\}$.
    A Siamese loss is formulated as'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 'Siamese网络在实现用于微调的度量学习中是重要的选择，如图[7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning
    DCNN Representations ‣ Deep Learning for Instance Retrieval: A Survey")（c）和图[8](#S4.F8
    "Figure 8 ‣ 4.1.2 Fine-tuning via Pairwise Ranking Loss ‣ 4.1 Supervised Fine-tuning
    ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval:
    A Survey")（a）。它是一种由两个分支组成的结构，这两个分支在层间共享相同的权重。Siamese网络在配对数据上进行训练，数据由一个图像对$(x_{i},x_{j})$组成，使得$\mathcal{S}(x_{i},x_{j})\!\in\!\{0,1\}$。Siamese损失被公式化为'
- en: '|  | $\displaystyle\mathcal{L}_{Siam}(x_{i},x_{j})$ | $\displaystyle=\frac{1}{2}\mathcal{S}(x_{i},x_{j})\mathcal{D}(x_{i},x_{j})\;+$
    |  | (12) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{Siam}(x_{i},x_{j})$ | $\displaystyle=\frac{1}{2}\mathcal{S}(x_{i},x_{j})\mathcal{D}(x_{i},x_{j})\;+$
    |  | (12) |'
- en: '|  |  | $\displaystyle\frac{1}{2}\big{(}1-\mathcal{S}(x_{i},x_{j})\big{)}\max\big{(}0,\;m-\mathcal{D}(x_{i},x_{j})\big{)}$
    |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\frac{1}{2}\big{(}1-\mathcal{S}(x_{i},x_{j})\big{)}\max\big{(}0,\;m-\mathcal{D}(x_{i},x_{j})\big{)}$
    |  |'
- en: 'Siamese loss has recently been reaffirmed as a very effective metric in category-level
    image retrieval, outperforming many more sophisticated losses if implemented carefully
    [[123](#bib.bib123)]. Enabled by the standard Siamese network, this objective
    function is used to learn the similarity between semantically relevant samples
    under different scenarios [[46](#bib.bib46)],[[124](#bib.bib124)]. For example,
    Radenović et al. [[46](#bib.bib46)] employ a Siamese network on matching and non-matching
    global feature pairs which are aggregated by GeM-based pooling. The deep network
    fine-tuned by the Siamese loss generalizes better and converges at higher retrieval
    performance. Ong et al. [[41](#bib.bib41)] leverage the Siamese network to learn
    image features which are then fed into the Fisher Vector model for further encoding.
    Siamese networks can also be applied to hashing learning in which the Euclidean
    distance $\mathcal{D}(\cdot)$ in Eq. [12](#S4.E12 "In 4.1.2 Fine-tuning via Pairwise
    Ranking Loss ‣ 4.1 Supervised Fine-tuning ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey") is computed for binary codes
    [[128](#bib.bib128)].'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Siamese损失最近被重新确认在类别级图像检索中非常有效，如果实现得当，比许多更复杂的损失方法表现更好[[123](#bib.bib123)]。借助标准的Siamese网络，该目标函数用于学习在不同场景下语义相关样本之间的相似度[[46](#bib.bib46)],[[124](#bib.bib124)]。例如，Radenović等人[[46](#bib.bib46)]使用Siamese网络对匹配和非匹配的全局特征对进行匹配，这些特征对通过基于GeM的池化进行聚合。通过Siamese损失微调的深度网络具有更好的泛化能力，并在检索性能上收敛得更高。Ong等人[[41](#bib.bib41)]利用Siamese网络学习图像特征，然后将这些特征输入Fisher
    Vector模型以进一步编码。Siamese网络也可以应用于哈希学习，其中在二进制编码中计算欧几里得距离$\mathcal{D}(\cdot)$ [[128](#bib.bib128)]。
- en: An implicit drawback of the Siamese loss is that it may penalize similar image
    pairs even if the margin between these pairs is small or zero [[128](#bib.bib128)],
    if the constraint is too strong and unbalanced. At the same time, it is hard to
    map the features of similar pairs to the same point when images contain complex
    contents or scenes. To tackle this limitation, Cao et al. [[129](#bib.bib129)]
    adopt a double-margin Siamese loss [[128](#bib.bib128)] to relax the penalty for
    similar pairs by setting a margin $m_{1}$ instead of zero, in which case the original
    single-margin Siamese loss is re-formulated as
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Siamese 损失的一个隐性缺陷是，如果约束过强且不平衡，即使相似图像对之间的边距很小或为零，也可能会惩罚这些相似图像对 [[128](#bib.bib128)]。同时，当图像包含复杂内容或场景时，很难将相似对的特征映射到相同的点。为了解决这一局限性，Cao
    等人 [[129](#bib.bib129)] 采用了双边距 Siamese 损失 [[128](#bib.bib128)] 通过设置边距 $m_{1}$
    来放宽对相似对的惩罚，而不是零，在这种情况下，原始单边距 Siamese 损失被重新制定为
- en: '|  | $\displaystyle\!\!\!\!\mathcal{L}_{\mathcal{D}\_Siam}(x_{i},x_{j})$ |
    $\displaystyle=\frac{1}{2}\mathcal{S}(x_{i},x_{j})\max\big{(}0,\mathcal{D}(x_{i},x_{j})-m_{1}\big{)}+$
    |  | (13) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\!\!\!\!\mathcal{L}_{\mathcal{D}\_Siam}(x_{i},x_{j})$ |
    $\displaystyle=\frac{1}{2}\mathcal{S}(x_{i},x_{j})\max\big{(}0,\mathcal{D}(x_{i},x_{j})-m_{1}\big{)}+$
    |  | (13) |'
- en: '|  |  | $\displaystyle\!\!\frac{1}{2}\big{(}1-\mathcal{S}(x_{i},x_{j})\big{)}\max\big{(}0,m_{2}-\mathcal{D}(x_{i},x_{j})\big{)}$
    |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\!\!\frac{1}{2}\big{(}1-\mathcal{S}(x_{i},x_{j})\big{)}\max\big{(}0,m_{2}-\mathcal{D}(x_{i},x_{j})\big{)}$
    |  |'
- en: 'where $m_{1}\!\!>\!\!0$ and $m_{2}\!\!>\!\!0$ are the margins affecting the
    similar and dissimilar pairs, respectively, as in Figure [8](#S4.F8 "Figure 8
    ‣ 4.1.2 Fine-tuning via Pairwise Ranking Loss ‣ 4.1 Supervised Fine-tuning ‣ 4
    Retrieval via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval:
    A Survey") (b), meaning that the double margin Siamese loss only applies a contrastive
    force when the distance of a similar pair is larger than $m_{1}$. The mAP metric
    of retrieval is improved when using the double margin Siamese loss [[128](#bib.bib128)].'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $m_{1}\!\!>\!\!0$ 和 $m_{2}\!\!>\!\!0$ 分别是影响相似对和不相似对的边距，如图 [8](#S4.F8 "Figure
    8 ‣ 4.1.2 Fine-tuning via Pairwise Ranking Loss ‣ 4.1 Supervised Fine-tuning ‣
    4 Retrieval via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval:
    A Survey") (b) 所示，意味着双边距 Siamese 损失仅在相似对的距离大于 $m_{1}$ 时施加对比力。使用双边距 Siamese 损失时，检索的
    mAP 指标得到了提升 [[128](#bib.bib128)]。'
- en: More recently, transformers have been trained under the regularization of cross
    entropy [[60](#bib.bib60)] and Siamese loss [[59](#bib.bib59)] for instance-level
    retrieval and achieved competitive performance, positioning it as an alternative
    to convolutional architectures. As observed by [[59](#bib.bib59)], the transformer-based
    architecture is less impacted than convolutional networks by feature collapse
    since each input feature is projected to different sub-spaces before the multi-headed
    attention. Moreover, the transformer backbone operates as a learned aggregation
    operator, thereby avoiding the design of sophisticated feature aggregation methods.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，transformers 在交叉熵 [[60](#bib.bib60)] 和 Siamese 损失 [[59](#bib.bib59)] 的正则化下进行了训练，以实现实例级检索，并取得了有竞争力的性能，使其成为卷积架构的替代方案。如
    [[59](#bib.bib59)] 所观察到的，基于 transformer 的架构比卷积网络更不容易受到特征崩溃的影响，因为每个输入特征在多头注意力之前会被投影到不同的子空间。此外，transformer
    主干作为一个学习的聚合操作符，从而避免了复杂特征聚合方法的设计。
- en: '![Refer to caption](img/6a11d029ea79055f1858a9aee28886d4.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6a11d029ea79055f1858a9aee28886d4.png)'
- en: 'Figure 8: Illustrations of different losses for network fine-tuning. The same
    shape with different colors denotes images that include the same instance. (a)-(c)
    have been introduced in the text [[46](#bib.bib46)],[[128](#bib.bib128)],[[42](#bib.bib42)].
    (d) Listwise AP loss considers a mini-batch of $N$ features simultaneously and
    directly optimizes the Average-Precision computed from these features [[57](#bib.bib57)],[[130](#bib.bib130)].'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：网络微调中不同损失的示意图。相同形状的不同颜色表示包含相同实例的图像。(a)-(c) 已在文本中介绍 [[46](#bib.bib46)],[[128](#bib.bib128)],[[42](#bib.bib42)]。(d)
    Listwise AP 损失同时考虑一个 $N$ 特征的小批量，并直接优化从这些特征计算出的平均精度 [[57](#bib.bib57)],[[130](#bib.bib130)]。
- en: '*c. Fine-tuning with Triplet Networks*.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*c. 使用三元组网络的微调*。'
- en: 'Triplet networks optimize similar and dissimilar pairs simultaneously. As shown
    in Figure [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey") (d) and Figure [8](#S4.F8 "Figure
    8 ‣ 4.1.2 Fine-tuning via Pairwise Ranking Loss ‣ 4.1 Supervised Fine-tuning ‣
    4 Retrieval via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval:
    A Survey") (c), the plain triplet networks adopt a ranking loss for training:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '三元组网络同时优化相似和不相似的样本对。如图 [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN
    Representations ‣ Deep Learning for Instance Retrieval: A Survey") (d) 和图 [8](#S4.F8
    "Figure 8 ‣ 4.1.2 Fine-tuning via Pairwise Ranking Loss ‣ 4.1 Supervised Fine-tuning
    ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval:
    A Survey") (c) 所示，普通的三元组网络采用排序损失进行训练：'
- en: '|  | $\displaystyle\mathcal{L}_{Triplet}(x_{a},x_{p},x_{n})=\max\big{(}0,m+\mathcal{D}(x_{a},x_{p})-\mathcal{D}(x_{a},x_{n})\big{)}$
    |  | (14) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{Triplet}(x_{a},x_{p},x_{n})=\max\big{(}0,m+\mathcal{D}(x_{a},x_{p})-\mathcal{D}(x_{a},x_{n})\big{)}$
    |  | (14) |'
- en: which indicates that the distance of an anchor-negative pair $\mathcal{D}(x_{a},x_{n})$
    should be larger than that of an anchor-positive pair $\mathcal{D}(x_{a},x_{p})$
    by a certain margin $m$.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，锚点-负样本对 $\mathcal{D}(x_{a},x_{n})$ 的距离应该比锚点-正样本对 $\mathcal{D}(x_{a},x_{p})$
    的距离大一个特定的间隔 $m$。
- en: 'Given the datasets that provide bounding box annotations, such as INSTRE, Oxford-5k,
    Paris-6k, and their variants, the bounding box annotations are used as patch-level
    supervision to train a region detector which enables the final DCNNs to locate
    specific regions or objects. As an example, region proposal networks (RPNs) [[27](#bib.bib27)]
    is fine-tuned and subsequently plugged into DCNNs and trained end-to-end [[92](#bib.bib92)],
    as shown in Figure [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey") (g). RPNs yield the regressed
    bounding box coordinates of objects and are trained by the multi-class classification
    loss. Once fine-tuned, RPNs can produce regional features for each detected region
    by RoI pooling and perform better instance search.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '给定提供边界框标注的数据集，例如 INSTRE、Oxford-5k、Paris-6k 及其变体，这些边界框标注被用作补丁级别的监督，以训练区域检测器，从而使最终的
    DCNN 能够定位特定的区域或物体。例如，区域建议网络 (RPNs) [[27](#bib.bib27)] 被微调后接入 DCNNs，并进行端到端训练 [[92](#bib.bib92)]，如图
    [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning
    for Instance Retrieval: A Survey") (g) 所示。RPNs 产生回归的边界框坐标，并通过多类别分类损失进行训练。微调后，RPNs
    能够通过 RoI 池化为每个检测到的区域生成区域特征，并执行更好的实例搜索。'
- en: 'Further, local supervised metric learning has been explored based on the fact
    that RPNs [[27](#bib.bib27)] enable deep models to learn regional features for
    particular instance objects [[125](#bib.bib125)],[[64](#bib.bib64)],[[42](#bib.bib42)],[[82](#bib.bib82)].
    RPNs used in the triplet formulation are shown in Figure [7](#S4.F7 "Figure 7
    ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval:
    A Survey") (h). Firstly, regression loss (RPNs loss) is used to minimize the regressed
    bounding box relative to ground-truth. Then, the regional features for all detected
    RoIs are aggregated into a global one and L2-normalized for the triplet loss.
    Note that, in some cases, jointly training an RPN loss and triplet loss leads
    to unstable results, a problem addressed in [[42](#bib.bib42)] by first training
    a CNN to produce R-MAC using a rigid grid, after which the parameters in convolutional
    layers are fixed and RPNs are trained to replace the rigid grid.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，基于 RPNs [[27](#bib.bib27)] 使深度模型学习特定实例对象的区域特征的事实，已探索了局部监督度量学习 [[125](#bib.bib125)],[[64](#bib.bib64)],[[42](#bib.bib42)],[[82](#bib.bib82)]。在图
    [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning
    for Instance Retrieval: A Survey") (h) 中显示了用于三元组公式的 RPNs。首先，使用回归损失 (RPNs 损失) 来最小化回归的边界框相对于真实值的误差。然后，将所有检测到的
    RoI 的区域特征聚合为一个全局特征，并进行 L2 标准化以用于三元组损失。请注意，在某些情况下，联合训练 RPN 损失和三元组损失会导致结果不稳定，这一问题在
    [[42](#bib.bib42)] 中得到了处理，该方法首先训练 CNN 以使用刚性网格生成 R-MAC，然后固定卷积层中的参数，并训练 RPNs 来替代刚性网格。'
- en: 'Attention mechanisms can also be combined with metric learning for fine-tuning
    [[64](#bib.bib64)], as in Figure [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning
    DCNN Representations ‣ Deep Learning for Instance Retrieval: A Survey") (e), where
    the attention module is typically end-to-end trainable and takes as input the
    convolutional feature maps. Song et al. [[64](#bib.bib64)] introduce a convolutional
    attention layer to explore spatial-semantic information, highlighting regions
    in images to significantly improve the discrimination for image retrieval.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '注意力机制也可以与度量学习相结合进行微调[[64](#bib.bib64)]，如图 [7](#S4.F7 "Figure 7 ‣ 4 Retrieval
    via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval: A Survey")
    (e) 所示，其中注意力模块通常是端到端可训练的，并以卷积特征图作为输入。Song 等人[[64](#bib.bib64)] 引入了一种卷积注意力层以探索空间-语义信息，突出图像中的区域，从而显著提高图像检索的区分能力。'
- en: 'Recent studies [[100](#bib.bib100)],[[125](#bib.bib125)] have jointly optimized
    the triplet loss and classification loss to further improve network capacity,
    as shown in Figure [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey") (f). The overall joint function
    is'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '最近的研究[[100](#bib.bib100)],[[125](#bib.bib125)] 共同优化了三元组损失和分类损失，以进一步提高网络容量，如图
    [7](#S4.F7 "Figure 7 ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning
    for Instance Retrieval: A Survey") (f) 所示。整体的联合函数是'
- en: '|  | $\displaystyle\!\!\!\mathcal{L}_{Joint}=$ | $\displaystyle\lambda_{1}\!\cdot\!\mathcal{L}_{Triplet}(x_{i,a},x_{i,p},x_{i,n})\!+\!\lambda_{2}\!\cdot\!\mathcal{L}_{CE}(\hat{p_{i}},y_{i})$
    |  | (15) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\!\!\!\mathcal{L}_{Joint}=$ | $\displaystyle\lambda_{1}\!\cdot\!\mathcal{L}_{Triplet}(x_{i,a},x_{i,p},x_{i,n})\!+\!\lambda_{2}\!\cdot\!\mathcal{L}_{CE}(\hat{p_{i}},y_{i})$
    |  | (15) |'
- en: 'where the cross entropy loss (CE loss) $\mathcal{L}_{CE}$ is defined in Eq.
    ([9](#S4.E9 "In 4.1.1 Fine-tuning via Classification Loss ‣ 4.1 Supervised Fine-tuning
    ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval:
    A Survey")) and the triplet loss $\mathcal{L}_{Triplet}$ in Eq. ([14](#S4.E14
    "In 4.1.2 Fine-tuning via Pairwise Ranking Loss ‣ 4.1 Supervised Fine-tuning ‣
    4 Retrieval via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval:
    A Survey")). $\lambda_{1}$ and $\lambda_{2}$ are hyper-parameters tuning the tradeoff
    between the two loss functions.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '其中交叉熵损失（CE损失）$\mathcal{L}_{CE}$ 在 Eq. ([9](#S4.E9 "In 4.1.1 Fine-tuning via
    Classification Loss ‣ 4.1 Supervised Fine-tuning ‣ 4 Retrieval via Learning DCNN
    Representations ‣ Deep Learning for Instance Retrieval: A Survey")) 中定义，三元组损失
    $\mathcal{L}_{Triplet}$ 在 Eq. ([14](#S4.E14 "In 4.1.2 Fine-tuning via Pairwise
    Ranking Loss ‣ 4.1 Supervised Fine-tuning ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey")) 中定义。$\lambda_{1}$ 和 $\lambda_{2}$
    是调节两种损失函数之间权衡的超参数。'
- en: 4.1.3 Discussion
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 讨论
- en: In some cases, pairwise ranking loss cannot effectively learn the variations
    between samples and still suffers from a weaker generalization capability if the
    training set is not ordered correctly. Therefore, pairwise ranking loss requires
    careful sample mining and weighting strategies to obtain the most informative
    pairs, especially when considering mini-batches. The hard negative mining strategy
    is commonly used [[43](#bib.bib43)],[[46](#bib.bib46)],[[118](#bib.bib118)], however
    further sophisticated mining strategies have recently been developed. Mishchuk
    et al. [[110](#bib.bib110)] calculate a pair-wise distance matrix on all mini-batch
    samples to select two closest negative and one anchor-positive pair to form a
    triplet. Instead of traversing all possible two-tuple or three-tuple combinations,
    it is possible to consider all positive samples in one cluster and negative samples
    together. Liu et al. [[4](#bib.bib4)] introduce a group-group loss to decrease
    the intra-group distance and increase the inter-group distance. Considering all
    samples may be beneficial for stabilizing optimization and promoting generalization
    due to a larger data diversity, however the extra computational cost remains an
    issue to be addressed.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，成对排序损失不能有效学习样本之间的变化，并且如果训练集未正确排序，仍会遭受较弱的泛化能力。因此，成对排序损失需要仔细的样本挖掘和加权策略以获取最具信息量的对，特别是在考虑小批量时。硬负样本挖掘策略通常被使用[[43](#bib.bib43)],[[46](#bib.bib46)],[[118](#bib.bib118)]，然而最近也开发了更复杂的挖掘策略。Mishchuk
    等人[[110](#bib.bib110)] 在所有小批量样本上计算成对距离矩阵，以选择两个最接近的负样本和一个锚点正样本对来形成三元组。与其遍历所有可能的二元组或三元组组合，不如考虑同一簇中的所有正样本和负样本。Liu
    等人[[4](#bib.bib4)] 引入了一种组-组损失，以减少组内距离并增加组间距离。考虑所有样本可能有助于稳定优化和促进泛化，因为数据多样性更大，但额外的计算成本仍然是一个需要解决的问题。
- en: Substantial research has been devoted to pair-wise ranking loss, while cross
    entropy loss, mainly used for classification, has been largely overlooked. Recently,
    Boudiaf et al. [[127](#bib.bib127)] claim that cross entropy loss can match and
    even surpass the pair-wise ranking loss when carefully tuned on fine-grained category-level
    retrieval tasks. In fact, the greatest improvements have come from enhanced training
    schemes (e.g., data augmentation, learning rate polices, batch normalization freeze)
    rather than intrinsic properties of pairwise ranking loss. Further, although several
    sophisticated ranking losses have been explored and validated for category-level
    retrieval, Musgrave et al. [[123](#bib.bib123)] revisited these losses and found
    that most of them perform on par to vanilla Siamese loss and triplet loss, so
    there is merit to consider these losses also for instance-level image retrieval
    tasks.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 大量研究已投入到成对排序损失（pair-wise ranking loss）中，而主要用于分类的交叉熵损失（cross entropy loss）则在很大程度上被忽视了。最近，Boudiaf
    等人 [[127](#bib.bib127)] 声称，当在细粒度类别级检索任务中进行仔细调整时，交叉熵损失可以与成对排序损失相匹配甚至超越它。事实上，最大的改进来自于增强的训练方案（例如，数据增强、学习率策略、批量归一化冻结），而不是成对排序损失的内在属性。此外，虽然已经探索和验证了几种复杂的排序损失用于类别级检索，但
    Musgrave 等人 [[123](#bib.bib123)] 重新审视了这些损失，并发现它们中的大多数在性能上与基础的Siamese损失和三元组损失相当，因此，考虑将这些损失应用于实例级图像检索任务也是有意义的。
- en: Both cross entropy loss and pair-wise ranking loss regularize on the embedded
    features and the corresponding labels so as to maximize their mutual information
    [[127](#bib.bib127)]. Their effectiveness is not guaranteed to give retrieval
    results that also optimize mAP [[57](#bib.bib57)]. To tackle this limitation one
    can directly optimize the average precision (AP) metric using the listwise AP
    loss,
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失和成对排序损失都对嵌入特征及其对应标签进行正则化，以最大化它们的互信息 [[127](#bib.bib127)]。它们的有效性并不能保证获得同时优化
    mAP [[57](#bib.bib57)] 的检索结果。为了解决这一限制，可以直接使用列表式 AP 损失来优化平均精度（AP）指标，
- en: '|  | $\displaystyle\!\!\!\mathcal{L}_{mAP}=1-\frac{1}{N}\sum^{N}_{i=1}{\rm
    AP}(x_{i}^{\top}X_{N},Y_{i})$ |  | (16) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\!\!\!\mathcal{L}_{mAP}=1-\frac{1}{N}\sum^{N}_{i=1}{\rm
    AP}(x_{i}^{\top}X_{N},Y_{i})$ |  | (16) |'
- en: which optimizes the global ranking of thousands of images simultaneously, instead
    of only a few images at a time. Here $Y_{i}$ is the binary label to evaluate the
    relevance between batch images. $X_{N}=\{x_{1},x_{2},...x_{j},...,x_{N}\}$ denotes
    the features of all images, where each $x_{i}$ is used as a potential query to
    rank the remaining batch images. Each similarity score $x_{i}^{\top}x_{j}$ can
    be measured by a cosine function.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 该损失同时优化成千上万图像的全局排序，而不仅仅是一次几个图像。这里 $Y_{i}$ 是用来评估批量图像之间相关性的二进制标签。$X_{N}=\{x_{1},x_{2},...x_{j},...,x_{N}\}$
    表示所有图像的特征，其中每个 $x_{i}$ 被用作潜在的查询来对剩余的批量图像进行排序。每个相似度分数 $x_{i}^{\top}x_{j}$ 可以通过余弦函数来度量。
- en: 'It is demonstrated that training with AP-based loss improves retrieval performance
    [[57](#bib.bib57)],[[130](#bib.bib130)]. However average precision, as a metric,
    is normally non-differentiable. To directly optimize the AP loss during back-propagation,
    the key is that the indicator function for AP computing needs to be relaxed using
    methods such as triangular kernel-based soft assignment [[57](#bib.bib57)] or
    sigmoid function [[130](#bib.bib130)], as shown in Figure [8](#S4.F8 "Figure 8
    ‣ 4.1.2 Fine-tuning via Pairwise Ranking Loss ‣ 4.1 Supervised Fine-tuning ‣ 4
    Retrieval via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval:
    A Survey") (d).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '研究表明，使用基于 AP 的损失进行训练可以改善检索性能 [[57](#bib.bib57)],[[130](#bib.bib130)]。然而，作为一种指标，平均精度通常是不可微分的。为了在反向传播过程中直接优化
    AP 损失，关键在于使用如三角核软分配 [[57](#bib.bib57)] 或 sigmoid 函数 [[130](#bib.bib130)] 等方法放宽
    AP 计算的指示函数，如图 [8](#S4.F8 "Figure 8 ‣ 4.1.2 Fine-tuning via Pairwise Ranking Loss
    ‣ 4.1 Supervised Fine-tuning ‣ 4 Retrieval via Learning DCNN Representations ‣
    Deep Learning for Instance Retrieval: A Survey") (d) 所示。'
- en: 4.2 Unsupervised Fine-tuning
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 无监督微调
- en: Supervised network fine-tuning becomes infeasible when there is insufficient
    supervisory information, normally because of cost or unavailability. Therefore
    unsupervised fine-tuning methods for image retrieval are quite necessary, but
    less studied [[131](#bib.bib131)]. For unsupervised fine-tuning, two directions
    are to mine relevance among features via manifold learning, and via clustering
    techniques, each discussed below.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当监督信息不足时，监督网络的微调变得不可行，通常是由于成本或不可用性。因此，针对图像检索的无监督微调方法是非常必要的，但研究较少 [[131](#bib.bib131)]。对于无监督微调，有两个方向：通过流形学习挖掘特征之间的相关性，以及通过聚类技术，每个方向将在下文中讨论。
- en: 4.2.1 Mining Samples with Manifold Learning
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 利用流形学习挖掘样本
- en: 'Manifold learning focuses on capturing intrinsic correlations on a manifold
    structure to mine or deduce relevance, as illustrated in Figure [9](#S4.F9 "Figure
    9 ‣ 4.2.1 Mining Samples with Manifold Learning ‣ 4.2 Unsupervised Fine-tuning
    ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval:
    A Survey"). Initial similarities between the extracted global features [[132](#bib.bib132)]
    or local features [[14](#bib.bib14)],[[133](#bib.bib133)] are used to construct
    an affinity matrix, which is then re-evaluated and updated using manifold learning
    [[134](#bib.bib134)]. According to the manifold similarity in the updated affinity
    matrix, positive and hard negative samples are selected for metric learning using
    pairwise ranking loss based functions such as pair loss [[42](#bib.bib42)],[[133](#bib.bib133)]
    or triplet loss [[131](#bib.bib131)],[[135](#bib.bib135)]. Note that this is different
    from the aforementioned methods for pairwise ranking loss based fine-tuning methods,
    where the hard positive and negative samples are explicitly selected from an ordered
    dataset according to the given affinity information.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 流形学习侧重于捕捉流形结构上的内在相关性，以挖掘或推断相关性，如图[9](#S4.F9 "图9 ‣ 4.2.1 利用流形学习挖掘样本 ‣ 4.2 无监督微调
    ‣ 4 通过学习DCNN表示进行检索 ‣ 用于实例检索的深度学习：一项调查")所示。提取的全局特征[[132](#bib.bib132)]或局部特征[[14](#bib.bib14)],[[133](#bib.bib133)]之间的初始相似度用于构建一个亲和性矩阵，然后使用流形学习[[134](#bib.bib134)]对其进行重新评估和更新。根据更新后的亲和性矩阵中的流形相似性，通过基于成对排序损失的函数，如对偶损失[[42](#bib.bib42)],[[133](#bib.bib133)]或三元损失[[131](#bib.bib131)],[[135](#bib.bib135)]，选取出正样本和困难负样本进行度量学习。需要注意，这与前述的基于成对排序损失的微调方法不同，那里的硬正负样本是根据给定的亲和性信息从有序数据集中明确选取出来的。
- en: 'It is important to capture the geometry of the manifold of deep features, generally
    involving two steps [[134](#bib.bib134)], known as diffusion. First, the affinity
    matrix (Figure [9](#S4.F9 "Figure 9 ‣ 4.2.1 Mining Samples with Manifold Learning
    ‣ 4.2 Unsupervised Fine-tuning ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey")) is interpreted as a weighted
    kNN graph, where each vector is represented by a node, and edges are defined by
    the pairwise affinities of two connected nodes. Then, the pairwise affinities
    are re-evaluated in the context of all other elements by diffusing the similarity
    values through the graph [[48](#bib.bib48)],[[131](#bib.bib131)],[[133](#bib.bib133)],[[135](#bib.bib135)],
    with recent strategies proposed such as regularized diffusion (RDP) [[58](#bib.bib58)]
    and regional diffusion [[133](#bib.bib133)]. For more details on diffusion methods
    refer to survey [[134](#bib.bib134)].'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉深层特征的流形几何形状很重要，通常包括两个步骤[[134](#bib.bib134)]，称为扩散。首先，将亲和性矩阵（图[9](#S4.F9 "图9
    ‣ 4.2.1 利用流形学习挖掘样本 ‣ 4.2 无监督微调 ‣ 4 通过学习DCNN表示进行检索 ‣ 用于实例检索的深度学习：一项调查")）解释为加权kNN图，其中每个向量都由一个节点代表，并且边由两个相连节点的成对亲和性定义。然后，通过将相似性值通过图进行扩散[[48](#bib.bib48)],[[131](#bib.bib131)],[[133](#bib.bib133)],[[135](#bib.bib135)]来重新评估所有其他元素的成对亲和性，最近提出的策略包括正则化扩散（RDP）[[58](#bib.bib58)]和区域扩散[[133](#bib.bib133)]。有关扩散方法的更多细节，请参考调查[[134](#bib.bib134)]。
- en: 'Most algorithms follow the two steps of [[134](#bib.bib134)]; the differences
    among methods lie primarily in three aspects:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数算法遵循[[134](#bib.bib134)]中的两个步骤；方法之间的差异主要在于三个方面：
- en: '1.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Similarity initialization, which affects the subsequent kNN graph construction
    in an affinity matrix. Usually, an inner product [[48](#bib.bib48)] or Euclidean
    distance [[45](#bib.bib45)] is directly computed for the affinities. A Gaussian
    kernel function can be used [[134](#bib.bib134)],[[135](#bib.bib135)], or consider
    regional similarity from image patches [[133](#bib.bib133)].
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相似性初始化，这影响着亲和性矩阵中后续kNN图的构建。通常，内积[[48](#bib.bib48)]或欧氏距离[[45](#bib.bib45)]直接计算出亲和性。可以使用高斯核函数[[134](#bib.bib134)],[[135](#bib.bib135)]，或者考虑从图像块中提取的区域相似性[[133](#bib.bib133)]。
- en: '2.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Transition matrix definition, a row-stochastic matrix [[134](#bib.bib134)],
    determines the probabilities of transiting from one node to another in the graph.
    These probabilities are proportional to the affinities between nodes, which can
    be measured by Geodesic distance (*e.g.* the summation of weights of relevant
    edges).
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过渡矩阵定义，一个行随机矩阵[[134](#bib.bib134)]，决定了在图中从一个节点过渡到另一个节点的概率。这些概率与节点之间的亲和力成比例，可以通过测量节点间的测地距离（*例如*相关边的权重之和）来衡量。
- en: '3.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Iteration scheme, to re-valuate and update the values in the affinity matrix
    by the manifold similarity until some convergence is achieved. Most algorithms
    are iteration-based [[131](#bib.bib131)],[[134](#bib.bib134)], as illustrated
    in Figure [9](#S4.F9 "Figure 9 ‣ 4.2.1 Mining Samples with Manifold Learning ‣
    4.2 Unsupervised Fine-tuning ‣ 4 Retrieval via Learning DCNN Representations ‣
    Deep Learning for Instance Retrieval: A Survey").'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '迭代方案，通过流形相似度重新评估和更新亲和矩阵中的值，直到达到某种收敛性。大多数算法都是基于迭代的 [[131](#bib.bib131)]，[[134](#bib.bib134)]，如图
    [9](#S4.F9 "Figure 9 ‣ 4.2.1 Mining Samples with Manifold Learning ‣ 4.2 Unsupervised
    Fine-tuning ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning for
    Instance Retrieval: A Survey") 所示。'
- en: Diffusion process algorithms are indispensable for unsupervised fine-tuning.
    Better image similarity is guaranteed when it is improved based on initialization
    (*e.g.* regional similarity [[133](#bib.bib133)] or higher order information [[45](#bib.bib45)]).
    Diffusion is normally iterative and is computationally demanding [[135](#bib.bib135)],
    a limitation which cannot meet the efficiency requirements of image retrieval.
    To reduce the computational complexity, Bai et al. [[58](#bib.bib58)] propose
    a regularized diffusion process, facilitated by an efficient iteration-based solver.
    Zhao et al. [[135](#bib.bib135)] regard the diffusion process as a non-linear
    kernel mapping function, which is then modelled by a deep neural network. Other
    studies replace the diffusion process on a kNN graph with a diffusion network
    [[47](#bib.bib47)], which is derived from graph convolution networks [[136](#bib.bib136)],
    an end-to-end trainable framework which allows efficient computation during training
    and testing.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散过程算法对于无监督微调至关重要。当基于初始化进行改进时（*例如* 区域相似度 [[133](#bib.bib133)] 或高阶信息 [[45](#bib.bib45)]），可以保证更好的图像相似度。扩散过程通常是迭代的，并且计算量大
    [[135](#bib.bib135)]，这一限制无法满足图像检索的效率要求。为了降低计算复杂度，白等人 [[58](#bib.bib58)] 提出了通过高效的基于迭代的求解器来实现正则化扩散过程。赵等人
    [[135](#bib.bib135)] 将扩散过程视为一种非线性核映射函数，然后由深度神经网络进行建模。其他研究则用扩散网络 [[47](#bib.bib47)]
    替代了kNN图上的扩散过程，该网络源自图卷积网络 [[136](#bib.bib136)]，这是一个端到端可训练的框架，允许在训练和测试过程中高效计算。
- en: '![Refer to caption](img/e1914c628f731f15d27551e04a101631.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e1914c628f731f15d27551e04a101631.png)'
- en: 'Figure 9: Paradigm of manifold learning for unsupervised metric learning, based
    on triplet loss [[131](#bib.bib131)],[[135](#bib.bib135)].'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：基于三元组损失的无监督度量学习流形学习范式 [[131](#bib.bib131)]，[[135](#bib.bib135)]。
- en: Once the manifold space is learned, samples are mined by computing geodesic
    distances based on the Floyd-Warshall algorithm or by comparing the set difference
    [[131](#bib.bib131)]. The selected samples are fed into deep networks to perform
    fine-tuning.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦学习了流形空间，样本通过基于Floyd-Warshall算法计算测地距离或通过比较集合差异来挖掘 [[131](#bib.bib131)]。所选样本被送入深度网络中进行微调。
- en: 4.2.2 Mining Samples by Clustering
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 通过聚类挖掘样本
- en: Clustering is used to explore proximity information that has been studied in
    instance-level retrieval [[44](#bib.bib44)],[[137](#bib.bib137)],[[138](#bib.bib138)],[[139](#bib.bib139)],[[140](#bib.bib140)].
    The rationale behind these methods is that samples in a cluster are likely to
    satisfy a degree of similarity.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类用于探索已在实例级检索中研究的邻近信息 [[44](#bib.bib44)]，[[137](#bib.bib137)]，[[138](#bib.bib138)]，[[139](#bib.bib139)]，[[140](#bib.bib140)]。这些方法的基本原理是，同一簇中的样本很可能满足一定程度的相似性。
- en: 'One class of methods for clustering deep features is via k-means. Given $k$
    cluster centroids, during each training epoch a deep network alternates between
    two steps: first, a soft assignment between the feature representations and the
    cluster centroids; second, the cluster centroids are refined and, at the same
    time, the deep network is updated by learning from current high confidence assignments
    using a certain regularization. These two steps are repeated until a convergence
    criterion is met, at which point the cluster assignments are used as pseudo-labels
    [[138](#bib.bib138)],[[139](#bib.bib139)]. Alternatively, the pseudo-labels can
    be calculated from the samples in a cluster, e.g., the mean values. For example,
    Tzelepi et al. [[137](#bib.bib137)] compute $k$ nearest feature representations
    with respect to a query feature and then compute their mean vectors, which is
    used as a target for the query feature. In this case, fine-tuning is performed
    by minimizing the squared distance between each query feature and the mean of
    its $k$ nearest features. Liu et al. [[81](#bib.bib81)] propose a self-taught
    hashing algorithm using a kNN graph construction to generate pseudo labels that
    are used to analyze and guide network training. Shen et al. [[141](#bib.bib141)]
    and Radenović et al. [[44](#bib.bib44)],[[46](#bib.bib46)] use Structure-from-Motion
    (SfM) for each image cluster to explore sample reconstructions to select images
    for triplet loss. Clustering methods depend on the Euclidean distance, making
    it difficult to reveal the intrinsic relationship between objects.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一类对深度特征进行聚类的方法是通过 k-means。给定 $k$ 个聚类中心，在每个训练周期中，深度网络在两个步骤之间交替进行：首先，特征表示与聚类中心之间进行软分配；其次，聚类中心被精细化，同时，深度网络通过从当前高置信度分配中学习并使用一定的正则化进行更新。这两个步骤重复进行，直到满足收敛标准，此时聚类分配被用作伪标签
    [[138](#bib.bib138)],[[139](#bib.bib139)]。另外，伪标签也可以通过计算聚类中的样本得出，例如，均值。例如，Tzelepi
    等人 [[137](#bib.bib137)] 计算相对于查询特征的 $k$ 个最近特征表示，然后计算它们的均值向量，作为查询特征的目标。在这种情况下，通过最小化每个查询特征与其
    $k$ 个最近特征均值之间的平方距离来进行微调。Liu 等人 [[81](#bib.bib81)] 提出了一种自学习哈希算法，利用 kNN 图构建生成伪标签，以分析和指导网络训练。Shen
    等人 [[141](#bib.bib141)] 和 Radenović 等人 [[44](#bib.bib44)],[[46](#bib.bib46)] 使用
    Structure-from-Motion (SfM) 对每个图像聚类进行样本重建，以选择用于三元组损失的图像。聚类方法依赖于欧几里得距离，这使得揭示物体之间的内在关系变得困难。
- en: There are further techniques for instance retrieval, such as by using AutoEncoder
    [[122](#bib.bib122)],[[142](#bib.bib142)], generative adversarial networks (GANs)
    [[143](#bib.bib143)], convolutional kernel networks [[112](#bib.bib112)],[[144](#bib.bib144)],
    and graph convolutional networks [[47](#bib.bib47)]. For these methods, they focus
    on devising novel unsupervised frameworks to realize unsupervised learning, instead
    of iterative similarity diffusion or cluster refinement on feature space. For
    example, instead of performing iterative traversal on a set of nearest neighbors
    defined by kNN graph, Liu et al. [[47](#bib.bib47)] employ graph convolutional
    networks [[136](#bib.bib136)] to directly encode the neighbor information into
    image descriptors and then train the deep models to learn a new feature space.
    This method is demonstrated to significantly improve retrieval accuracy while
    maintaining efficiency. GANs are also explored, for the first time, for instance-level
    retrieval in an unsupervised fashion [[143](#bib.bib143)]. The generator retrieves
    images that contain similar instances as a given image, while the discriminator
    judges whether the retrieved images have the specified instance which appeared
    in the query image. During training, the discriminator and the generator play
    a min-max game via an adversarial reward which is computed based on the cosine
    distance between the query image and the images retrieved by the generator.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些进一步的实例检索技术，例如使用 AutoEncoder [[122](#bib.bib122)],[[142](#bib.bib142)]、生成对抗网络
    (GANs) [[143](#bib.bib143)]、卷积核网络 [[112](#bib.bib112)],[[144](#bib.bib144)] 和图卷积网络
    [[47](#bib.bib47)]。这些方法专注于设计新的无监督框架以实现无监督学习，而不是在特征空间上进行迭代相似性扩散或聚类优化。例如，Liu 等人
    [[47](#bib.bib47)] 采用图卷积网络 [[136](#bib.bib136)] 直接将邻居信息编码到图像描述符中，然后训练深度模型以学习新的特征空间，从而显著提高了检索准确性，同时保持了效率。GANs
    也首次被探索用于无监督的实例级检索 [[143](#bib.bib143)]。生成器检索包含与给定图像相似实例的图像，而判别器判断检索到的图像是否包含查询图像中出现的特定实例。在训练过程中，判别器和生成器通过基于查询图像与生成器检索图像之间的余弦距离计算的对抗奖励进行最小-最大博弈。
- en: 5 State of the Art Performance
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 最先进的性能
- en: 5.1 Datasets
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据集
- en: 'To demonstrate the effectiveness of methods, we choose the following commonly-used
    datasets for performance comparison:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示方法的有效性，我们选择了以下常用数据集进行性能比较：
- en: UKBench (UKB) [[145](#bib.bib145)] consists of 10,200 images of objects. This
    dataset has 2,550 groups of images, each group having four images of the same
    object from different viewpoints or illumination conditions, which can be regarded
    as a kind of class-level supervision information. All images can be used as a
    query.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: UKBench (UKB) [[145](#bib.bib145)] 包含 10,200 张物体图像。该数据集有 2,550 组图像，每组包含四张来自不同视角或光照条件下的相同物体图像，这可以视为一种类级别的监督信息。所有图像都可以用作查询。
- en: Holidays [[51](#bib.bib51)] consists of 1,491 images collected from personal
    holiday albums. Most images are scene-related. The dataset comprises 500 groups
    of similar images with a single query image for each group. The dataset also provides
    position information of the interest regions for each image.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Holidays [[51](#bib.bib51)] 包含 1,491 张来自个人假期相册的图像。大多数图像与场景相关。数据集包括 500 组相似图像，每组有一个查询图像。数据集还提供了每张图像的兴趣区域的位置信息。
- en: Oxford-5k [[94](#bib.bib94)] consists of 5,062 images for 11 Oxford buildings.
    Each building is associated with five hand-drawn bounding box queries. According
    to the relevance level, each image of the same building is assigned a label Good
    (i.e., positive), OK (i.e., positive), Junk, or Bad (i.e., negative). Junk images
    can be discarded or regarded as negative examples [[146](#bib.bib146)],[[54](#bib.bib54)].
    To build a tuple for each given query, one can select a positive example whose
    label corresponds to Good or OK in the same category, and select one negative
    example from each of the remaining building categories. Furthermore, an additional
    disjoint set of 100,000 distractor images is added to obtain Oxford-105k.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Oxford-5k [[94](#bib.bib94)] 包含 5,062 张关于 11 个牛津建筑物的图像。每栋建筑物都关联有五个手工绘制的边界框查询。根据相关性级别，每张同一建筑物的图像被标记为
    Good（即正面）、OK（即正面）、Junk 或 Bad（即负面）。Junk 图像可以被丢弃或视为负面样本 [[146](#bib.bib146)],[[54](#bib.bib54)]。为了构建每个给定查询的元组，可以选择一个标签为
    Good 或 OK 的正面样本，并从剩余的建筑物类别中选择一个负面样本。此外，还添加了一个额外的不相交的 100,000 张干扰图像集合，形成了 Oxford-105k。
- en: Paris-6k [[95](#bib.bib95)] includes 6,412 images and is categorized into 12
    groups by architecture. The supervision information can be used like that of Oxford-5k.
    Likewise, an additional disjoint set of 100,000 distractor images is added to
    obtain Paris-106k.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Paris-6k [[95](#bib.bib95)] 包含6,412张图像，并按建筑类型分类为12组。监督信息可以像使用Oxford-5k那样使用。同样，额外增加了一个包含100,000张干扰图像的不重叠集合，形成Paris-106k。
- en: 'INSTRE [[93](#bib.bib93)] consists of 28,543 images from 250 different object
    classes, including three disjoint subsets²²2https://github.com/imatge-upc/salbow:
    INSTRE-S1, INSTRE-S2, INSTRE-M. INSTRE dataset has bounding box annotations, providing
    single-labelled and double-labelled class information for single- and multiple-object
    retrieval, respectively. One can use the class information to build a tuple, with
    two positive examples from the same class and one negative from one of the remaining
    classes. The performance evaluation on INSTRE in our experiments follows the protocol
    in [[133](#bib.bib133)].'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 'INSTRE [[93](#bib.bib93)] 包含来自250个不同对象类别的28,543张图像，包括三个不重叠的子集²²2https://github.com/imatge-upc/salbow:
    INSTRE-S1, INSTRE-S2, INSTRE-M。INSTRE数据集具有边界框注释，提供单标签和双标签类别信息，分别用于单对象和多对象检索。可以使用类别信息构建一个样本，包括来自同一类别的两个正例和来自剩余类别中的一个负例。我们实验中对INSTRE的性能评估遵循了[[133](#bib.bib133)]中的协议。'
- en: 'Google Landmarks Dataset (GLD) [[5](#bib.bib5)],[[63](#bib.bib63)] consists
    of GLD-v1 and GLD-v2\. GLD-v2 is mainly recommended to use and it has the advantage
    of stability where all images have permissive licenses [[71](#bib.bib71)]. GLD-v2
    is divided into three subsets: (i) 118k query images with ground-truth annotations,
    (ii) 4.1M training images of 203k landmarks with labels, and (iii) 762k index
    images of 101k landmarks. Due to its large scale, GLD-v2 provides class-level
    ground-truth which can be used to build training tuples. Due to its image diversity,
    it may produce clutter images for each landmark so it is necessary to introduce
    pre-processing methods to select the more relevant images [[147](#bib.bib147)].
    Finally, the training set is cleaned by removing these clutters, consisting of
    a subset “GLD-v2-clean” containing 1.6M images of 81k landmarks. Since Google
    landmarks dataset stills lack bounding box for objects of interest, Teichmann
    et al. [[74](#bib.bib74)] provide a new dataset of landmark bounding boxes, based
    on GLD. This patch-level supervision information can help locate the most relevant
    regions.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Google Landmarks Dataset (GLD) [[5](#bib.bib5)],[[63](#bib.bib63)] 包含GLD-v1和GLD-v2。主要推荐使用GLD-v2，它具有稳定性，所有图像都有宽松的许可证[[71](#bib.bib71)]。GLD-v2分为三个子集：(i)
    118k带有真实注释的查询图像，(ii) 4.1M带标签的203k地标的训练图像，以及(iii) 762k带有101k地标的索引图像。由于其大规模，GLD-v2提供了类级别的真实标签，可用于构建训练样本。由于图像多样性，它可能会产生每个地标的杂乱图像，因此有必要引入预处理方法以选择更相关的图像[[147](#bib.bib147)]。最后，训练集通过去除这些杂乱图像进行清理，得到一个包含1.6M张81k地标图像的子集“GLD-v2-clean”。由于Google地标数据集仍缺乏感兴趣对象的边界框，Teichmann等人[[74](#bib.bib74)]基于GLD提供了一个新的地标边界框数据集。这种补丁级别的监督信息可以帮助定位最相关的区域。
- en: Note that, additional queries and distractor images have been added into Oxford-5k
    and Paris-6k, producing the Revisited Oxford ($\mathcal{R}$Oxford) and Revisited
    Paris ($\mathcal{R}$Paris) datasets where each image of the same building is assigned
    a label Easy, Hard, Unclear, or Negative [[146](#bib.bib146)]. Different label
    combinations are used as positive according to the difficulty level of different
    setups. During testing, if there are no positive images for a query, then that
    query is excluded from the evaluation. For details, we refer the reader to [[146](#bib.bib146)].
    We undertake partial comparisons under the hard evaluation protocol on these revisited
    datasets.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Oxford-5k和Paris-6k中已添加额外的查询和干扰图像，产生了Revisited Oxford ($\mathcal{R}$Oxford)
    和 Revisited Paris ($\mathcal{R}$Paris)数据集，其中每张同一建筑的图像被分配了标签Easy, Hard, Unclear或Negative
    [[146](#bib.bib146)]。根据不同设置的难度级别，使用不同的标签组合作为正例。在测试期间，如果查询没有正例图像，则该查询将从评估中排除。详细信息，请参见[[146](#bib.bib146)]。我们在这些修订数据集上采用了困难评估协议进行部分比较。
- en: 5.2 Evaluation Metrics
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 评估指标
- en: Average precision (AP) refers to the coverage area under the precision-recall
    (PR) curve. A larger AP implies a higher PR curve and better retrieval accuracy.
    AP can be calculated as $AP=\frac{1}{R}\sum_{k=1}^{N}P(k)\cdot rel(k)$, where
    $R$ denotes the number of relevant results for the query image from the total
    number $N$ of images. $P(k)$ is the precision of the top $k$ retrieved images,
    and $rel(k)$ is an indicator function equal to 1 if the item within rank $k$ is
    a relevant image and 0 otherwise. Mean average precision (mAP) is adopted for
    the evaluation over all query images, i.e., $mAP=\frac{1}{Q}{\sum_{q=1}^{Q}}AP(q)$,
    where $Q$ is the number of query images.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度（AP）指的是精度-召回（PR）曲线下的覆盖面积。较大的AP意味着更高的PR曲线和更好的检索准确率。AP可以计算为$AP=\frac{1}{R}\sum_{k=1}^{N}P(k)\cdot
    rel(k)$，其中$R$表示查询图像的相关结果数量，$N$是图像总数。$P(k)$是前$k$张检索图像的精度，而$rel(k)$是一个指示函数，当排名第$k$的项是相关图像时等于1，否则为0。均值平均精度（mAP）用于对所有查询图像进行评估，即$mAP=\frac{1}{Q}{\sum_{q=1}^{Q}}AP(q)$，其中$Q$是查询图像的数量。
- en: The N-S score is a metric used for UKBench [[145](#bib.bib145)]; the N-S score
    is the average for the top-4 precision over the dataset.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: N-S分数是一种用于UKBench的指标[[145](#bib.bib145)]；N-S分数是数据集上前4名精度的平均值。
- en: 5.3 Performance Comparison and Analysis
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 性能比较与分析
- en: <svg   height="248.21" overflow="visible" version="1.1" width="306.95"><g transform="translate(0,248.21)
    matrix(1 0 0 -1 0 0) translate(32.84,0) translate(0,22.95)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(0.7071 0.7071 -0.7071
    0.7071 -13.97 -18.38)" fill="#000000" stroke="#000000"><foreignobject width="19.37"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2014</foreignobject></g><g
    transform="matrix(0.7071 0.7071 -0.7071 0.7071 31 -18.38)" fill="#000000" stroke="#000000"><foreignobject
    width="19.37" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2015</foreignobject></g><g
    transform="matrix(0.7071 0.7071 -0.7071 0.7071 75.97 -18.38)" fill="#000000" stroke="#000000"><foreignobject
    width="19.37" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2016</foreignobject></g><g
    transform="matrix(0.7071 0.7071 -0.7071 0.7071 120.94 -18.38)" fill="#000000"
    stroke="#000000"><foreignobject width="19.37" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2017</foreignobject></g><g transform="matrix(0.7071
    0.7071 -0.7071 0.7071 165.91 -18.38)" fill="#000000" stroke="#000000"><foreignobject
    width="19.37" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2018</foreignobject></g><g
    transform="matrix(0.7071 0.7071 -0.7071 0.7071 210.88 -18.38)" fill="#000000"
    stroke="#000000"><foreignobject width="19.37" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2019</foreignobject></g><g transform="matrix(0.7071
    0.7071 -0.7071 0.7071 255.85 -18.38)" fill="#000000" stroke="#000000"><foreignobject
    width="19.37" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2020</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 22.94)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$80$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 75.07)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$85$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 127.21)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$90$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 179.34)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$95$</foreignobject></g><g
    clip-path="url(#pgfcp1)"><g stroke-width="0.4pt"><g transform="matrix(1.0 0.0
    0.0 1.0 3.51 36.51)" fill="#000000" stroke="#000000"><foreignobject width="101.11"
    height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">80.18 [[30](#bib.bib30)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -9.09 132.77)" fill="#000000" stroke="#000000"><foreignobject
    width="108.12" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">89.7
    [[37](#bib.bib37)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 81.63
    159.2)" fill="#000000" stroke="#000000"><foreignobject width="98.24" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">94.2 [[42](#bib.bib42)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 126.6 173.8)" fill="#000000" stroke="#000000"><foreignobject
    width="105.53" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">95.13
    [[148](#bib.bib148)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 167.64
    178.2)" fill="#000000" stroke="#000000"><foreignobject width="109.79" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">95.7 [[58](#bib.bib58)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 208.67 177.39)" fill="#000000" stroke="#000000"><foreignobject
    width="105.78" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">95.5
    [[149](#bib.bib149)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 241.83
    166.62)" fill="#000000" stroke="#000000"><foreignobject width="101.3" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">94.0 [[150](#bib.bib150)]</foreignobject></g></g><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 3.51 145.99)" fill="#000000"
    stroke="#000000"><foreignobject width="95.01" height="6.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">91.1 [[29](#bib.bib29)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 32.73 149.01)" fill="#000000" stroke="#000000"><foreignobject
    width="127.65" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">91.3
    [[12](#bib.bib12)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 81.63
    200.9)" fill="#000000" stroke="#000000"><foreignobject width="113.29" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">96.3 [[83](#bib.bib83)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 167.64 218.86)" fill="#000000" stroke="#000000"><foreignobject
    width="107.26" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">98.1
    [[119](#bib.bib119)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 241.83
    216.67)" fill="#000000" stroke="#000000"><foreignobject width="102.28" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">98.8 [[151](#bib.bib151)]</foreignobject></g></g><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 3.51 5.23)" fill="#000000"
    stroke="#000000"><foreignobject width="96.88" height="6.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">78.34 [[17](#bib.bib17)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -9.09 78.03)" fill="#000000" stroke="#000000"><foreignobject
    width="108.12" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">84.4
    [[37](#bib.bib37)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 81.63
    109.15)" fill="#000000" stroke="#000000"><foreignobject width="113.62" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">88.95 [[86](#bib.bib86)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 167.64 194.88)" fill="#000000" stroke="#000000"><foreignobject
    width="93.11" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">95.8
    [[132](#bib.bib132)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 208.67
    199.28)" fill="#000000" stroke="#000000"><foreignobject width="112.25" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">96.2 [[152](#bib.bib152)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 241.83 198.94)" fill="#000000" stroke="#000000"><foreignobject
    width="101.3" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">96.2
    [[150](#bib.bib150)]</foreignobject></g></g><g stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 3.51 102.2)" fill="#000000" stroke="#000000"><foreignobject width="96.88"
    height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">86.83 [[17](#bib.bib17)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 36.66 100.81)" fill="#000000" stroke="#000000"><foreignobject
    width="113.29" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">86.5
    [[32](#bib.bib32)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 81.63
    182.14)" fill="#000000" stroke="#000000"><foreignobject width="96.4" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">95.8 [[153](#bib.bib153)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 126.6 196.73)" fill="#000000" stroke="#000000"><foreignobject
    width="107.28" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">96.0
    [[46](#bib.bib46)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 167.64
    207.39)" fill="#000000" stroke="#000000"><foreignobject width="93.11" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">97.0 [[132](#bib.bib132)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 208.67 212.84)" fill="#000000" stroke="#000000"><foreignobject
    width="112.25" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">97.8
    [[152](#bib.bib152)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 241.83
    208.33)" fill="#000000" stroke="#000000"><foreignobject width="101.3" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">97.4 [[150](#bib.bib150)]</foreignobject></g></g></g><g
    transform="matrix(0.0 1.0 -1.0 0.0 -22.35 93.31)" fill="#000000" stroke="#000000"><foreignobject
    width="37.53" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">mAP(%)</foreignobject></g><g
    fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 178.16 9.35)"><g
    transform="matrix(1 0 0 -1 0 48.005)"><g transform="matrix(1 0 0 1 0 6.86)"><g
    transform="matrix(1 0 0 -1 28.61 0) translate(20.96,0) matrix(1.0 0.0 0.0 1.0
    -18.19 -2.64)" fill="#000000" stroke="#000000"><foreignobject width="36.65" height="8.61"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Holidays</foreignobject></g></g><g
    transform="matrix(1 0 0 1 0 20.57)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 26.42 0) translate(23.15,0) matrix(1.0 0.0 0.0 1.0
    -20.38 -2.64)" fill="#000000" stroke="#000000"><foreignobject width="40.76" height="6.73"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">UKBench</foreignobject></g></g><g
    transform="matrix(1 0 0 1 0 34.29)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 25.39 0) translate(24.17,0) matrix(1.0 0.0 0.0 1.0
    -21.4 -2.64)" fill="#000000" stroke="#000000"><foreignobject width="42.81" height="6.73"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Oxford-5k</foreignobject></g></g><g
    transform="matrix(1 0 0 1 0 48)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 29.47 0) translate(20.09,0) matrix(1.0 0.0 0.0 1.0
    -17.33 -2.64)" fill="#000000" stroke="#000000"><foreignobject width="34.65" height="6.73"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Paris-6k</foreignobject></g></g></g></g></g></g></svg><svg
    height="247.38" overflow="visible" version="1.1" width="290.53"><g transform="translate(0,247.38)
    matrix(1 0 0 -1 0 0) translate(16.42,0) translate(0,22.95)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(0.7071 0.7071 -0.7071
    0.7071 75.97 -18.38)" fill="#000000" stroke="#000000"><foreignobject width="19.37"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2018</foreignobject></g><g
    transform="matrix(0.7071 0.7071 -0.7071 0.7071 165.91 -18.38)" fill="#000000"
    stroke="#000000"><foreignobject width="19.37" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2019</foreignobject></g><g transform="matrix(0.7071
    0.7071 -0.7071 0.7071 255.85 -18.38)" fill="#000000" stroke="#000000"><foreignobject
    width="19.37" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2020</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 10.89)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$20$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 66.93)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$40$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 122.97)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$60$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 179.01)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$80$</foreignobject></g><g
    clip-path="url(#pgfcp2)"><g stroke-width="0.4pt"><g transform="matrix(1.0 0.0
    0.0 1.0 81.63 102.14)" fill="#000000" stroke="#000000"><foreignobject width="124.24"
    height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">54.8 [[146](#bib.bib146)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 171.57 154.54)" fill="#000000" stroke="#000000"><foreignobject
    width="106.7" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">65.8
    [[48](#bib.bib48)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 241.83
    139.06)" fill="#000000" stroke="#000000"><foreignobject width="102.07" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">64.0 [[71](#bib.bib71)]</foreignobject></g></g><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 81.63 210.58)" fill="#000000"
    stroke="#000000"><foreignobject width="124.24" height="6.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">84.0 [[146](#bib.bib146)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 171.57 186.76)" fill="#000000" stroke="#000000"><foreignobject
    width="95.15" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">85.3
    [[47](#bib.bib47)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 237.89
    189.76)" fill="#000000" stroke="#000000"><foreignobject width="127.65" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">80.4 [[154](#bib.bib154)]</foreignobject></g></g><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 1.54 161.58)" fill="#000000"
    stroke="#000000"><foreignobject width="105.76" height="6.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">80.0 [[133](#bib.bib133)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 81.63 174.15)" fill="#000000" stroke="#000000"><foreignobject
    width="93.11" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">80.5
    [[132](#bib.bib132)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 183.38
    204.18)" fill="#000000" stroke="#000000"><foreignobject width="95.15" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">92.4 [[47](#bib.bib47)]</foreignobject></g></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 241.83 38.18)"><foreignobject width="102.07" height="6.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">26.8 [[71](#bib.bib71)]</foreignobject></g></g><g
    fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 29.38 9.56)"><g
    transform="matrix(1 0 0 -1 0 47.835)"><g transform="matrix(1 0 0 1 0 6.86)"><g
    transform="matrix(1 0 0 -1 25.39 0) translate(27.74,0) matrix(1.0 0.0 0.0 1.0
    -24.97 -2.64)" fill="#000000" stroke="#000000"><foreignobject width="14.66" height="6.62"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathcal{R}$Oxford-5k</foreignobject></g></g><g
    transform="matrix(1 0 0 1 0 20.57)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 29.47 0) translate(23.66,0) matrix(1.0 0.0 0.0 1.0
    -20.89 -2.64)" fill="#000000" stroke="#000000"><foreignobject width="13.72" height="6.62"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathcal{R}$Paris-6k</foreignobject></g></g><g
    transform="matrix(1 0 0 1 0 34.23)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 31.93 0) translate(21.2,0) matrix(1.0 0.0 0.0 1.0 -18.43
    -2.58)" fill="#000000" stroke="#000000"><foreignobject width="36.86" height="6.62"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">INSTRE</foreignobject></g></g><g
    transform="matrix(1 0 0 1 0 47.83)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 33.24 0) translate(19.89,0) matrix(1.0 0.0 0.0 1.0
    -17.12 -2.58)" fill="#000000" stroke="#000000"><foreignobject width="34.24" height="6.62"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GLD-v2</foreignobject></g></g></g></g></g></g></svg>
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`<svg height="248.21" overflow="visible" version="1.1" width="306.95"><g transform="translate(0,248.21)
    matrix(1 0 0 -1 0 0) translate(32.84,0) translate(0,22.95)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(0.7071 0.7071 -0.7071
    0.7071 -13.97 -18.38)" fill="#000000" stroke="#000000"><foreignobject width="19.37"
    height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2014</foreignobject></g><g
    transform="matrix(0.7071 0.7071 -0.7071 0.7071 31 -18.38)" fill="#000000" stroke="#000000"><foreignobject
    width="19.37" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2015</foreignobject></g><g
    transform="matrix(0.7071 0.7071 -0.7071 0.7071 75.97 -18.38)" fill="#000000" stroke="#000000"><foreignobject
    width="19.37" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2016</foreignobject></g><g
    transform="matrix(0.7071 0.7071 -0.7071 0.7071 120.94 -18.38)" fill="#000000"
    stroke="#000000"><foreignobject width="19.37" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2017</foreignobject></g><g transform="matrix(0.7071
    0.7071 -0.7071 0.7071 165.91 -18.38)" fill="#000000" stroke="#000000"><foreignobject
    width="19.37" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2018</foreignobject></g><g
    transform="matrix(0.7071 0.7071 -0.7071 0.7071 210.88 -18.38)" fill="#000000"
    stroke="#000000"><foreignobject width="19.37" height="6.24" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2019</foreignobject></g><g transform="matrix(0.7071
    0.7071 -0.7071 0.7071 255.85 -18.38)" fill="#000000" stroke="#000000"><foreignobject
    width="19.37" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2020</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 22.94)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$80$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 75.07)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$85$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 127.21)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$90$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.19 179.34)" fill="#000000" stroke="#000000"><foreignobject
    width="9.69" height="6.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$95$</foreignobject></g><g
    clip-path="url(#pgfcp1)"><g stroke-width="0.4pt"><g transform="matrix(1.0 0.0
    0.0 1.0 3.51 36.51)" fill="#000000" stroke="#000000"><foreignobject width="101.11"
    height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">80.18 [[30](#bib.bib30)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -9.09 132.77)" fill="#000000" stroke="#000000"><foreignobject
    width="108.12" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">89.7
    [[37](#bib.bib37)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 81.63
    159.2)" fill="#000000" stroke="#000000"><foreignobject width="98.24" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">94.2 [[42](#bib.bib42)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 126.6 173.8)" fill="#000000" stroke="#000000"><foreignobject
    width="105.53" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">95.13
    [[148](#bib.bib148)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 167.64
    178.2)" fill="#000000" stroke="#000000"><foreignobject width="109.79" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">95.7 [[58](#bib.bib58)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 208.67 177.39)" fill="#000000" stroke="#000000"><foreignobject
    width="105.78" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">95.5
    [[149](#bib.bib149)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 241.83
    166.62)" fill="#000000" stroke="#000000"><foreignobject width="101.3" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">94.0 [[150](#bib.bib150)]</foreignobject></g></g><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 3.51 145.99)" fill="#000000"
    stroke="#000000"><foreignobject width="95.01" height="6.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">91.1 [[29](#bib.bib29)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 32.73 149.01)" fill="#000000" stroke="#000000"><foreignobject
    width="127.65" height="6.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">91.3
    [[12](#bib.bib12)]</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 81.63
    200.9)" fill="#000000" stroke="#000000"><foreignobject width="113.29" height="6.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">96.3 [[83](#bib.bib83)]</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 167.64 218.86)" fill="#000000" stroke="#000000"><foreignobject
    width="107.26" height="6.92" transform="matrix(1'
- en: 'Figure 10: Performance improved from 2014 to 2020.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：性能从 2014 年到 2020 年有所提升。
- en: 'Overview. Figure [10](#S5.F10 "Figure 10 ‣ 5.3 Performance Comparison and Analysis
    ‣ 5 State of the Art Performance ‣ Deep Learning for Instance Retrieval: A Survey")
    summarizes the performance over 6 datasets from 2014 to 2020\. Early on, the powerful
    feature extraction of DCNNs led to rapid improvements. Subsequent key ideas have
    been to extract instance features at the region level to reduce image clutter
    [[30](#bib.bib30)], and to improve feature discriminativity by using methods including
    feature fusion [[86](#bib.bib86)],[[148](#bib.bib148)],[[151](#bib.bib151)], feature
    aggregation [[32](#bib.bib32)],[[69](#bib.bib69)], and feature embedding [[86](#bib.bib86)].
    Fine-tuning is an important strategy to improve performance by tuning deep networks
    specific for learning instance features [[58](#bib.bib58)],[[150](#bib.bib150)].
    For instance, the accuracy increases steadily from 78.34% [[17](#bib.bib17)] to
    96.2% [[152](#bib.bib152)] on the Oxford-5k dataset when manifold learning is
    used to fine-tune deep networks. The mAP on $\mathcal{R}$Paris-6k and $\mathcal{R}$Oxford-5k
    is smaller than Paris-6k and Oxford-5k, leaving room for improvement.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '概述。图 [10](#S5.F10 "Figure 10 ‣ 5.3 Performance Comparison and Analysis ‣ 5
    State of the Art Performance ‣ Deep Learning for Instance Retrieval: A Survey")
    总结了 2014 年到 2020 年间在 6 个数据集上的表现。早期，DCNN 强大的特征提取能力带来了迅速的改进。随后的关键思想是通过在区域级别提取实例特征以减少图像杂乱
    [[30](#bib.bib30)]，并通过使用特征融合 [[86](#bib.bib86)], [[148](#bib.bib148)], [[151](#bib.bib151)]、特征聚合
    [[32](#bib.bib32)], [[69](#bib.bib69)] 和特征嵌入 [[86](#bib.bib86)] 等方法提高特征的区分性。微调是通过调整特定于实例特征学习的深度网络来提高性能的重要策略
    [[58](#bib.bib58)], [[150](#bib.bib150)]。例如，当流形学习用于微调深度网络时，Oxford-5k 数据集的准确率从
    78.34% [[17](#bib.bib17)] 稳步上升至 96.2% [[152](#bib.bib152)]。$\mathcal{R}$Paris-6k
    和 $\mathcal{R}$Oxford-5k 的 mAP 小于 Paris-6k 和 Oxford-5k，仍有改进空间。'
- en: 'We report results using off-the-shelf models (Table [II](#S5.T2 "TABLE II ‣
    5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance ‣ Deep
    Learning for Instance Retrieval: A Survey")) and fine-tuning networks (Table [III](#S5.T3
    "TABLE III ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey")). In Table [II](#S5.T2 "TABLE
    II ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey"), single-pass and multiple-pass
    are analyzed, while supervised and unsupervised fine-tuning are compared in Table
    [III](#S5.T3 "TABLE III ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of
    the Art Performance ‣ Deep Learning for Instance Retrieval: A Survey"). Since
    there are many aspects that vary across the different methods, making them not
    directly comparable, we mainly draw some general claims or trends based on the
    collected results.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '我们报告了使用现成模型的结果（表 [II](#S5.T2 "TABLE II ‣ 5.3 Performance Comparison and Analysis
    ‣ 5 State of the Art Performance ‣ Deep Learning for Instance Retrieval: A Survey")）和微调网络的结果（表
    [III](#S5.T3 "TABLE III ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of
    the Art Performance ‣ Deep Learning for Instance Retrieval: A Survey")）。在表 [II](#S5.T2
    "TABLE II ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey") 中，分析了单次和多次通行的结果，而表 [III](#S5.T3
    "TABLE III ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey") 中比较了监督和无监督的微调。由于不同方法在许多方面有所不同，导致它们不可直接比较，我们主要基于收集到的结果提出一些总体结论或趋势。'
- en: 'TABLE II: Performance evaluation of off-the-shelf DCNN models. “$\bullet$”
    indicates that the models or layers are combined to learn features; “PCA[w] indicates
    PCA with whitening on the extracted features to improve robustness; “MP” means
    Max Pooling; “SP” means Sum Pooling. The CNN-M network with “$\ast$” has an architecture
    similar to that of AlexNet. “-” means that the results were not reported.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：现成 DCNN 模型的性能评估。“$\bullet$” 表示模型或层被组合以学习特征；“PCA[w]” 表示对提取的特征进行白化的 PCA，以提高鲁棒性；“MP”
    表示最大池化；“SP” 表示求和池化。带有“$\ast$”的 CNN-M 网络具有与 AlexNet 相似的架构。“-” 表示结果未报告。
- en: '|       Type | Method | Backbone DCNN | Output Layer | Embed. Aggre. | Feat.
    Dim | Holidays | UKB | Oxford5k (+100k) | Paris6k (+100k) | Brief Conclusions
    and Highlights     |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|       类型 | 方法 | 基础 DCNN | 输出层 | 嵌入聚合 | 特征维度 | 假日 | UKB | Oxford5k (+100k)
    | Paris6k (+100k) | 简要结论和亮点     |'
- en: '|       Single-pass | Neural codes [[39](#bib.bib39)] | AlexNet | FC6 | PCA
    | $128$ | $74.7$ | $3.42$ (N-S) | $43.3$ (38.6) | $-$ | Compressed neural codes
    of different layers are explored. AlexNet is also fine-tuned for retrieval.    
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|       单次传递 | Neural codes [[39](#bib.bib39)] | AlexNet | FC6 | PCA | $128$
    | $74.7$ | $3.42$ (N-S) | $43.3$ (38.6) | $-$ | 探索了不同层的压缩神经编码。AlexNet 也经过了检索的微调。
        |'
- en: '|   | SPoC [[12](#bib.bib12)] | VGG16 | Conv5 | SPoC + PCA[w] | 256 | 80.2
    | $3.65$ (N-S) | $58.9$ (57.8) | $-$ | Exploring Gassian weighting scheme i.e.,
    the centering prior, to improve the discrimination of features.     |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|   | SPoC [[12](#bib.bib12)] | VGG16 | Conv5 | SPoC + PCA[w] | 256 | 80.2
    | $3.65$ (N-S) | $58.9$ (57.8) | $-$ | 探索了高斯加权方案，即中心先验，以提高特征的区分度。     |'
- en: '|   | CroW [[15](#bib.bib15)] | VGG16 | Conv5 | CroW + PCA[w] | 256 | $85.1$
    | $-$ | $68.4$ (63.7) | $76.5$ (69.1) | The spatialwise and channelwise weighting
    mechanisms are utilized to highlight crucial convolutional features.     |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|   | CroW [[15](#bib.bib15)] | VGG16 | Conv5 | CroW + PCA[w] | 256 | $85.1$
    | $-$ | $68.4$ (63.7) | $76.5$ (69.1) | 利用空间和通道加权机制突出关键的卷积特征。     |'
- en: '|   | R-MAC [[32](#bib.bib32)] | VGG16 | Conv5 | R-MAC + PCA[w] | 512 | $-$
    | $-$ | $66.9$ (61.6) | $-$ (75.7) | Sliding windows with different scales on
    convolutional feature maps to encode multiple image regions.     |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|   | R-MAC [[32](#bib.bib32)] | VGG16 | Conv5 | R-MAC + PCA[w] | 512 | $-$
    | $-$ | $66.9$ (61.6) | $-$ (75.7) | 在卷积特征图上使用不同尺度的滑动窗口来编码多个图像区域。     |'
- en: '|   | Multi-layer CNN [[102](#bib.bib102)] | VGG16 | FC6 $\bullet$ Conv4$\sim$5
    | SP | 4096 | 91.4 | $3.68$ (N-S) | $61.5$ ($-$) | $-$ | Layer-level feature fusion
    and the complementary properties of different layers are explored.     |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|   | 多层 CNN [[102](#bib.bib102)] | VGG16 | FC6 $\bullet$ Conv4$\sim$5 | SP
    | 4096 | 91.4 | $3.68$ (N-S) | $61.5$ ($-$) | $-$ | 探索了层级特征融合及不同层的互补特性。     |'
- en: '|   | BLCF [[84](#bib.bib84)] | VGG16 | Conv5 | BoW + PCA[w] | 25k | $-$ |
    $-$ | $73.9$ (59.3) | $82.0$ (64.8) | Both global features and local features
    are explored, demonstrating that local features have higher accuracy.     |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|   | BLCF [[84](#bib.bib84)] | VGG16 | Conv5 | BoW + PCA[w] | 25k | $-$ |
    $-$ | $73.9$ (59.3) | $82.0$ (64.8) | 探索了全局特征和局部特征，展示了局部特征的更高准确性。     |'
- en: '|       Multiple-pass | OLDFP [[61](#bib.bib61)] | AlexNet | FC6 | MP + PCA[w]
    | 512 | 88.5 | $3.81$ (N-S) | $60.7$ ($-$) | $66.2$ ($-$) | Exploring the impact
    of proposal number. Patches are extracted by RPNs (see Figure [4](#S3.F4 "Figure
    4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval
    with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey")
    (d)) and the features are encoded in an orderless way.     |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|       多次传递 | OLDFP [[61](#bib.bib61)] | AlexNet | FC6 | MP + PCA[w] | 512
    | 88.5 | $3.81$ (N-S) | $60.7$ ($-$) | $66.2$ ($-$) | 探索了提案数量的影响。通过 RPNs 提取补丁（见图
    [4](#S3.F4 "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction
    ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey") (d)），并以无序方式编码特征。     |'
- en: '|   | MOP-CNN [[30](#bib.bib30)] | AlexNet | FC7 | VLAD + PCA[w] | 2048 | 80.2
    | $-$ | $-$ | $-$ | Image patches are extracted densely, as shown in Figure [4](#S3.F4
    "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3
    Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey") (c). Multi-scale patch features are further embedded into VLAD descriptors.
        |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|   | MOP-CNN [[30](#bib.bib30)] | AlexNet | FC7 | VLAD + PCA[w] | 2048 | 80.2
    | $-$ | $-$ | $-$ | 图像补丁被密集提取，如图 [4](#S3.F4 "Figure 4 ‣ 3.1.1 Network Feedforward
    Scheme ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models
    ‣ Deep Learning for Instance Retrieval: A Survey") (c) 所示。多尺度补丁特征进一步嵌入到 VLAD 描述符中。
        |'
- en: '|   | CNNaug-ss [[29](#bib.bib29)] | Overfeat [[155](#bib.bib155)] | FC | PCA[w]
    | 15k | 84.3 | $91.1$ (mAP) | $68.0$ ($-$) | $79.5$ ($-$) | Image patches are
    extracted densely, as shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.1.1 Network Feedforward
    Scheme ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models
    ‣ Deep Learning for Instance Retrieval: A Survey") (c). Image regions at different
    locations with different sizes are included.     |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|   | CNNaug-ss [[29](#bib.bib29)] | Overfeat [[155](#bib.bib155)] | FC | PCA[w]
    | 15k | 84.3 | $91.1$ (mAP) | $68.0$ ($-$) | $79.5$ ($-$) | 图像补丁被密集提取，如图 [4](#S3.F4
    "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3
    Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey") (c) 所示。包含了不同位置和不同大小的图像区域。     |'
- en: '|   | MOF [[36](#bib.bib36)] | CNN-M^∗ [[156](#bib.bib156)] | FC7 $\bullet$
    Conv | SP or MP + BoW | 20k | 76.8 | $3.00$ (N-S) | $-$ | $-$ | Exploring layer-level
    fusion scheme. Image patches are extracted using spatial pyramid modeling, as
    shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1
    Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning
    for Instance Retrieval: A Survey") (b).     |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|   | MOF [[36](#bib.bib36)] | CNN-M^∗ [[156](#bib.bib156)] | FC7 $\bullet$
    Conv | SP or MP + BoW | 20k | 76.8 | $3.00$ (N-S) | $-$ | $-$ | 探索层级融合方案。图像块通过空间金字塔建模提取，如图[4](#S3.F4
    "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3
    Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey") (b)所示。     |'
- en: '|   | Multi-scale CNN [[69](#bib.bib69)] | VGG16 | Conv5 | SP or MP + PCA[w]
    | 32k | 89.6 | $95.1$ (mAP) | $84.3$ ($-$) | $87.9$ ($-$) | Image patches are
    extracted in a dense manner, as shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.1.1 Network
    Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf
    DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey") (c). Geometric
    invariance is considered when aggregating patch features.     |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|   | Multi-scale CNN [[69](#bib.bib69)] | VGG16 | Conv5 | SP or MP + PCA[w]
    | 32k | 89.6 | $95.1$ (mAP) | $84.3$ ($-$) | $87.9$ ($-$) | 图像块以密集方式提取，如图[4](#S3.F4
    "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3
    Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey") (c)所示。在聚合块特征时考虑了几何不变性。'
- en: '|   | LDD [[115](#bib.bib115)] | VGG19 | Conv5 | BoW + PCA[w] | 500k | 84.6
    | $-$ | $83.3$ ($-$) | $87.2$ ($-$) | Image patches are obtained using a uniform
    square mesh, as shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.1.1 Network Feedforward
    Scheme ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models
    ‣ Deep Learning for Instance Retrieval: A Survey") (a). Patch features are encoded
    into BoW descriptors.     |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|   | LDD [[115](#bib.bib115)] | VGG19 | Conv5 | BoW + PCA[w] | 500k | 84.6
    | $-$ | $83.3$ ($-$) | $87.2$ ($-$) | 图像块通过均匀的方形网格获得，如图[4](#S3.F4 "Figure 4 ‣
    3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with
    Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey") (a)所示。块特征被编码为BoW描述符。
        |'
- en: '|   | DeepIndex [[52](#bib.bib52)] | AlexNet $\bullet$ VGG19 | FC6-7 $\bullet$
    FC17-18 | BoW + PCA | 512 | 81.7 | $3.32$ (N-S) | $-$ | $75.4$ ($-$) | Exploring
    layer-level and model-level fusion methods. Image patches are extracted using
    spatial pyramid modeling, as shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.1.1 Network
    Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf
    DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey") (b).     |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|   | DeepIndex [[52](#bib.bib52)] | AlexNet $\bullet$ VGG19 | FC6-7 $\bullet$
    FC17-18 | BoW + PCA | 512 | 81.7 | $3.32$ (N-S) | $-$ | $75.4$ ($-$) | 探索了层级和模型级融合方法。图像块通过空间金字塔建模提取，如图[4](#S3.F4
    "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3
    Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey") (b)所示。     |'
- en: '|   | CCS [[53](#bib.bib53)] | GoogLeNet | Conv | VLAD + PCA[w] | 128 | 84.1
    | $3.81$ (N-S) | $64.8$ ($-$) | $76.8$ ($-$) | Object proposals are extracted
    by RPNs, as shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.1.1 Network Feedforward Scheme
    ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep
    Learning for Instance Retrieval: A Survey") (d). Object level and point level
    feature concatenation schemes are explored.     |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|   | CCS [[53](#bib.bib53)] | GoogLeNet | Conv | VLAD + PCA[w] | 128 | 84.1
    | $3.81$ (N-S) | $64.8$ ($-$) | $76.8$ ($-$) | 物体提议由RPNs提取，如图[4](#S3.F4 "Figure
    4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval
    with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey")
    (d)所示。探讨了物体级和点级特征拼接方案。     |'
- en: '|       |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|       |  |  |  |  |  |  |  |  |  |  |'
- en: 'TABLE III: Performance evaluation of methods in which DCNN models are fine-tuned,
    in a supervised or an unsupervised manner. “CE Loss” means the models are fine-tuned
    using the classification-based loss function in the form of Eq. [9](#S4.E9 "In
    4.1.1 Fine-tuning via Classification Loss ‣ 4.1 Supervised Fine-tuning ‣ 4 Retrieval
    via Learning DCNN Representations ‣ Deep Learning for Instance Retrieval: A Survey").
    “Siamese Loss” is in the form of Eq. [12](#S4.E12 "In 4.1.2 Fine-tuning via Pairwise
    Ranking Loss ‣ 4.1 Supervised Fine-tuning ‣ 4 Retrieval via Learning DCNN Representations
    ‣ Deep Learning for Instance Retrieval: A Survey"). “Regression Loss” is in the
    form of Eq. [11](#S4.E11 "In 4.1.2 Fine-tuning via Pairwise Ranking Loss ‣ 4.1
    Supervised Fine-tuning ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep
    Learning for Instance Retrieval: A Survey"). “Triplet Loss” is in the form of
    Eq. [14](#S4.E14 "In 4.1.2 Fine-tuning via Pairwise Ranking Loss ‣ 4.1 Supervised
    Fine-tuning ‣ 4 Retrieval via Learning DCNN Representations ‣ Deep Learning for
    Instance Retrieval: A Survey").'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：在监督或无监督方式下对DCNN模型进行微调的方法的性能评估。 “CE Loss”表示模型通过分类基础的损失函数（形式见Eq. [9](#S4.E9
    "在4.1.1 通过分类损失进行微调 ‣ 4.1 监督微调 ‣ 4 检索通过学习DCNN表示 ‣ 实例检索的深度学习：综述")）进行微调。 “Siamese
    Loss”是形式见Eq. [12](#S4.E12 "在4.1.2 通过成对排名损失进行微调 ‣ 4.1 监督微调 ‣ 4 检索通过学习DCNN表示 ‣ 实例检索的深度学习：综述")。
    “Regression Loss”是形式见Eq. [11](#S4.E11 "在4.1.2 通过成对排名损失进行微调 ‣ 4.1 监督微调 ‣ 4 检索通过学习DCNN表示
    ‣ 实例检索的深度学习：综述")。 “Triplet Loss”是形式见Eq. [14](#S4.E14 "在4.1.2 通过成对排名损失进行微调 ‣ 4.1
    监督微调 ‣ 4 检索通过学习DCNN表示 ‣ 实例检索的深度学习：综述")。
- en: '|       Type | Method | Backbone DCNN | Training set | Output Layer | Embed.
    Aggre. | Loss Function | Feat. Dim | Holidays | UKB | Oxford5k (+100k) | Paris6k
    (+100k) | Brief Conclusions and Highlights     |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|       类型 | 方法 | 基础DCNN | 训练集 | 输出层 | 嵌入聚合 | 损失函数 | 特征维度 | Holidays | UKB
    | Oxford5k (+100k) | Paris6k (+100k) | 简要结论和亮点     |'
- en: '|       Supervised Fine-tuning | Neural codes [[39](#bib.bib39)] | AlexNet
    | Landmarks [[39](#bib.bib39)] | FC6 | PCA | CE Loss | 128 | 78.9 | $3.29$ (N-S)
    | $55.7$ (52.3) | $-$ | The first work which fine-tunes deep networks for IIR.
    Compressed neural codes and different layers are explored.     |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|       监督微调 | Neural codes [[39](#bib.bib39)] | AlexNet | Landmarks [[39](#bib.bib39)]
    | FC6 | PCA | CE Loss | 128 | 78.9 | $3.29$ (N-S) | $55.7$ (52.3) | $-$ | 首次对深度网络进行IIR的微调。探索了压缩神经编码和不同层。
        |'
- en: '|   | Nonmetric [[40](#bib.bib40)] | VGG16 | Landmarks | Conv5 | PCA[w] | Regression
    Loss | 512 | $-$ | $-$ | $88.2$ (82.1) | $88.2$ (82.9) | Similarity learning of
    similar and dissimilar pairs is performed by a neural network, optimized using
    regression loss.     |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|   | Nonmetric [[40](#bib.bib40)] | VGG16 | Landmarks | Conv5 | PCA[w] | Regression
    Loss | 512 | $-$ | $-$ | $88.2$ (82.1) | $88.2$ (82.9) | 通过神经网络进行相似和不相似对的相似性学习，使用回归损失进行优化。
        |'
- en: '|   | SIAM-FV [[41](#bib.bib41)] | VGG16 | Landmarks | Conv5 | FV + PCA[w]
    | Siamese Loss | 512 | $-$ | $-$ | $81.5$ (76.6) | $82.4$ ($-$) | Fisher Vector
    is integrated on top of VGG and is trained with VGG simultaneously.     |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|   | SIAM-FV [[41](#bib.bib41)] | VGG16 | Landmarks | Conv5 | FV + PCA[w]
    | Siamese Loss | 512 | $-$ | $-$ | $81.5$ (76.6) | $82.4$ ($-$) | Fisher Vector被集成在VGG之上，并与VGG同时训练。
        |'
- en: '|   | Faster R-CNN [[92](#bib.bib92)] | VGG16 | Oxford5k Paris6k | Conv5 |
    MP / SP | Regression Loss | 512 | $-$ | $-$ | $75.1$ $(-)$ | $80.7$ ($-$) | RPN
    is fine-tuned, based on bounding box coordinates and class scores for specific
    region query which is region-targeted.     |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|   | Faster R-CNN [[92](#bib.bib92)] | VGG16 | Oxford5k Paris6k | Conv5 |
    MP / SP | Regression Loss | 512 | $-$ | $-$ | $75.1$ $(-)$ | $80.7$ ($-$) | RPN经过微调，基于边界框坐标和类得分进行特定区域查询，该查询是针对区域的。
        |'
- en: '|   | SIFT-CNN [[124](#bib.bib124)] | VGG16 | Holidays UKB | Conv5 | SP | Siamese
    Loss | 512 | 88.4 | $3.91$ (N-S) | $-$ | $-$ | SIFT features are used as supervisory
    information for mining positive and negative samples.     |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|   | SIFT-CNN [[124](#bib.bib124)] | VGG16 | Holidays UKB | Conv5 | SP | Siamese
    Loss | 512 | 88.4 | $3.91$ (N-S) | $-$ | $-$ | SIFT特征被用作监督信息来挖掘正负样本。     |'
- en: '|   | Quartet-Net [[129](#bib.bib129)] | VGG16 | GeoPair [[129](#bib.bib129)]
    | FC6 | PCA | Siamese Loss | 128 | 71.2 | $87.5$ (mAP) | $48.5$ ($-$) | $48.8$
    ($-$) | Quartet-net learning is explored to improve feature discrimination where
    double-margin contrastive loss is used.     |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|   | Quartet-Net [[129](#bib.bib129)] | VGG16 | GeoPair [[129](#bib.bib129)]
    | FC6 | PCA | Siamese Loss | 128 | 71.2 | $87.5$ (mAP) | $48.5$ ($-$) | $48.8$
    ($-$) | 探索了Quartet-net学习以提高特征辨别力，使用了双边距对比损失。     |'
- en: '|   | NetVLAD [[43](#bib.bib43)] | VGG16 | Tokyo Time Machine | VLAD Layer
    | PCA[w] | Triplet Loss | 256 | 79.9 | $-$ | $62.5$ ($-$) | $72.0$ ($-$) | VLAD
    is integrated at the last convolutional layer of VGG16 network as a plugged layer.
        |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|   | NetVLAD [[43](#bib.bib43)] | VGG16 | Tokyo Time Machine | VLAD Layer
    | PCA[w] | Triplet Loss | 256 | 79.9 | $-$ | $62.5$ ($-$) | $72.0$ ($-$) | VLAD作为插拔层集成在VGG16网络的最后卷积层。
        |'
- en: '|   | Deep Retrieval [[82](#bib.bib82)] | ResNet-101 | Landmarks | Conv5 Block
    | MP + PCA[w] | Triplet Loss | 2048 | 90.3 | $-$ | $86.1$ (82.8) | $94.5$ (90.6)
    | Dataset is cleaned automatically. Features are encoded by R-MAC. RPN is used
    to extract the most relevant regions.     |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|   | Deep Retrieval [[82](#bib.bib82)] | ResNet-101 | Landmarks | Conv5 Block
    | MP + PCA[w] | Triplet Loss | 2048 | 90.3 | $-$ | $86.1$ (82.8) | $94.5$ (90.6)
    | 数据集自动清理。特征通过R-MAC编码。RPN用于提取最相关的区域。     |'
- en: '|   | DELF [[5](#bib.bib5)] | ResNet-101 | GLDv1 | Conv4 Block | Attention
    + PCA[w] | CE Loss | 2048 | $-$ | $-$ | $83.8$ (82.6) | $85.0$ (81.7) | Exploring
    the FCN to extract region-level features and construct feature pyramids of different
    sizes.     |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|   | DELF [[5](#bib.bib5)] | ResNet-101 | GLDv1 | Conv4 Block | Attention
    + PCA[w] | CE Loss | 2048 | $-$ | $-$ | $83.8$ (82.6) | $85.0$ (81.7) | 探索FCN以提取区域级特征，并构建不同尺寸的特征金字塔。
        |'
- en: '|       Unsupervised Fine-tuning | MoM [[131](#bib.bib131)] | VGG16 | Flickr
    7M | Conv5 | MP + PCA[w] | Siamese Loss | 64 | 87.5 | $-$ | $78.2$ (72.6) | $85.1$
    (78.0) | Exploring manifold learning for mining dis/similar samples. Features
    are tested globally and regionally.     |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|       无监督微调 | MoM [[131](#bib.bib131)] | VGG16 | Flickr 7M | Conv5 | MP +
    PCA[w] | Siamese Loss | 64 | 87.5 | $-$ | $78.2$ (72.6) | $85.1$ (78.0) | 探索流形学习以挖掘相似/不相似样本。特征在全球和局部进行测试。
        |'
- en: '|   | GeM [[46](#bib.bib46)] | VGG16 | Flickr 7M | Conv5 | GeM Pooling | Siamese
    Loss | 512 | 83.1 | $-$ | $82.0$ (76.9) | $79.7$ (72.6) | Fine-tuning CNNs on
    an unordered dataset. Samples are selected from an automated 3D reconstruction
    system.     |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|   | GeM [[46](#bib.bib46)] | VGG16 | Flickr 7M | Conv5 | GeM Pooling | Siamese
    Loss | 512 | 83.1 | $-$ | $82.0$ (76.9) | $79.7$ (72.6) | 在无序数据集上微调CNN。样本从自动化3D重建系统中选择。
        |'
- en: '|   | SfM-CNN [[44](#bib.bib44)] | VGG16 | Flickr 7M | Conv5 | PCA[w] | Siamese
    Loss | 512 | 82.5 | $-$ | $77.0$ (69.2) | $83.8$ (76.4) | Employing Structure-from-Motion
    to select positive and negative samples from unordered images.     |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|   | SfM-CNN [[44](#bib.bib44)] | VGG16 | Flickr 7M | Conv5 | PCA[w] | Siamese
    Loss | 512 | 82.5 | $-$ | $77.0$ (69.2) | $83.8$ (76.4) | 使用从运动结构中选择正负样本，从无序图像中选择。
        |'
- en: '|   | MDP-CNN [[135](#bib.bib135)] | ResNet-101 | Landmarks | Conv5 Block |
    SP | Triplet Loss | 2048 | $-$ | $-$ | $85.4$ (85.1) | $96.3$ (94.7) | Exploring
    global feature structure by modeling the manifold learning to select positive
    and negative pairs.     |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|   | MDP-CNN [[135](#bib.bib135)] | ResNet-101 | Landmarks | Conv5 Block |
    SP | Triplet Loss | 2048 | $-$ | $-$ | $85.4$ (85.1) | $96.3$ (94.7) | 通过建模流形学习探索全局特征结构，以选择正负对。
        |'
- en: '|   | IME-CNN [[45](#bib.bib45)] | ResNet-101 | Oxford105k Paris106k | IME
    Layer | MP | Regression Loss | 2048 | $-$ | $-$ | $92.0$ (87.2) | $96.6$ (93.3)
    | Graph-based manifold learning is explored to mine the matching and non-matching
    pairs in unordered datasets.     |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|   | IME-CNN [[45](#bib.bib45)] | ResNet-101 | Oxford105k Paris106k | IME
    Layer | MP | Regression Loss | 2048 | $-$ | $-$ | $92.0$ (87.2) | $96.6$ (93.3)
    | 探索基于图的流形学习，以挖掘无序数据集中匹配和非匹配对。     |'
- en: '|       |  |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|       |  |  |  |  |  |  |  |  |  |  |  |  |'
- en: 'Evaluation for single feedforward pass. In general, we observe that fully-connected
    layers used as feature extractors may give a lower accuracy (e.g., 74.7% on Holidays
    in [[39](#bib.bib39)]), compared to using convolutional layers in Table [II](#S5.T2
    "TABLE II ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey"). For the case where the same
    VGG net is used, the way to embed or aggregate features is critical. The methods
    shown in Figure [5](#S3.F5 "Figure 5 ‣ 3.1.2 Deep Feature Selection ‣ 3.1 Deep
    Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning
    for Instance Retrieval: A Survey") improve the discrimination of convolutional
    feature maps and perform differently in Table [II](#S5.T2 "TABLE II ‣ 5.3 Performance
    Comparison and Analysis ‣ 5 State of the Art Performance ‣ Deep Learning for Instance
    Retrieval: A Survey"), 66.9% of R-MAC [[95](#bib.bib95)] and 58.9% of SPoC [[12](#bib.bib12)]
    on Oxford-5k, differences which we see as critical factors for further analysis.
    If embedded by a BoW model, the results are competitive on Oxford-5k and Paris-6k
    (73.9% and 82.0%, respectively), while its codebook size is 25k, which may affect
    retrieval efficiency. Moreover, layer-level feature fusion improves retrieval
    accuracy. Yu et al. [[102](#bib.bib102)] combine three layers (mAP of 91.4% on
    Holidays), outperforming the performance of non-fusion method [[12](#bib.bib12)]
    (mAP of 80.2%).'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '对于单次前馈过程的评估。一般来说，我们观察到，作为特征提取器使用的全连接层可能会给出较低的准确率（例如，在Holidays上的74.7%），相比于使用卷积层，如表[II](#S5.T2
    "TABLE II ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey")所示。对于使用相同的VGG网络的情况，嵌入或聚合特征的方法至关重要。图[5](#S3.F5
    "Figure 5 ‣ 3.1.2 Deep Feature Selection ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval
    with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval: A Survey")中展示的方法提高了卷积特征图的区分度，并且在表[II](#S5.T2
    "TABLE II ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey")中的表现有所不同，R-MAC在Oxford-5k上的准确率为66.9%[[95](#bib.bib95)]，SPoC的准确率为58.9%[[12](#bib.bib12)]，这些差异被视为进一步分析的关键因素。如果使用BoW模型进行嵌入，则在Oxford-5k和Paris-6k上的结果具有竞争力（分别为73.9%和82.0%），但其代码簿大小为25k，这可能影响检索效率。此外，层级特征融合提高了检索准确性。Yu等人[[102](#bib.bib102)]结合了三个层（Holidays上的mAP为91.4%），优于非融合方法[[12](#bib.bib12)]（mAP为80.2%）。'
- en: 'Evaluation for multiple feedforward pass. Results for the methods of Figure
    [4](#S3.F4 "Figure 4 ‣ 3.1.1 Network Feedforward Scheme ‣ 3.1 Deep Feature Extraction
    ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep Learning for Instance Retrieval:
    A Survey") are reported in Table [II](#S5.T2 "TABLE II ‣ 5.3 Performance Comparison
    and Analysis ‣ 5 State of the Art Performance ‣ Deep Learning for Instance Retrieval:
    A Survey"). Among them, extracting image patches densely using VGG [[49](#bib.bib49)]
    has the highest performance on the 4 datasets [[29](#bib.bib29)], and rigid grid
    with BoW encoding [[115](#bib.bib115)] is competitive (mAP of 87.2% on Paris-6k).
    These two methods consider more patches, even background information, when used
    for feature extraction. Instead of generating patches densely, region proposals
    and spatial pyramid modeling introduce a degree of purpose and efficiency in processing
    image objects. Spatial information is better maintained using multiple-pass schemes
    than with single-pass. For example, a shallower network (AlexNet) and region proposal
    networks in [[61](#bib.bib61)] have a UKBench N-Score of 3.81, higher than using
    deeper networks [[12](#bib.bib12)],[[39](#bib.bib39)],[[102](#bib.bib102)]. Besides
    feeding image patches into the same network, model-level fusion also exploits
    complementary spatial information to improve accuracy. For instance, as reported
    in [[52](#bib.bib52)], which combines AlexNet and VGG, the results on Holidays
    (81.7% of mAP) and UKBench (3.32 of N-Score) are better than these in [[36](#bib.bib36)]
    (76.75% and 3.00, respectively).'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对多个前向传递的评估。图[4](#S3.F4 "图 4 ‣ 3.1.1 网络前向方案 ‣ 3.1 深度特征提取 ‣ 3 用现成的DCNN模型检索 ‣ 深度学习实例检索：调查")中的方法结果报告在表[II](#S5.T2
    "表 II ‣ 5.3 性能比较与分析 ‣ 5 最先进的性能 ‣ 深度学习实例检索：调查")中。其中，使用VGG[[49](#bib.bib49)]进行密集图像块提取在4个数据集[[29](#bib.bib29)]上表现最佳，且使用BoW编码的刚性网格[[115](#bib.bib115)]具有竞争力（在Paris-6k上的mAP为87.2%）。这两种方法在特征提取时考虑了更多的图像块，甚至是背景信息。与生成密集图像块不同，区域提案和空间金字塔建模在处理图像对象时引入了一定的目的性和效率。使用多次传递方案比单次传递更好地保持空间信息。例如，在[[61](#bib.bib61)]中的较浅网络（AlexNet）和区域提案网络的UKBench
    N-Score为3.81，高于使用较深网络[[12](#bib.bib12)],[[39](#bib.bib39)],[[102](#bib.bib102)]的结果。除了将图像块输入同一网络，模型级融合还利用互补的空间信息来提高准确性。例如，如[[52](#bib.bib52)]所报告的，结合了AlexNet和VGG，在Holidays（mAP为81.7%）和UKBench（N-Score为3.32）的结果优于[[36](#bib.bib36)]中的结果（分别为76.75%和3.00）。
- en: 'Evaluation for supervised fine-tuning. Compared to off-the-shelf models, fine-tuning
    deep networks usually improves accuracy, see Table [III](#S5.T3 "TABLE III ‣ 5.3
    Performance Comparison and Analysis ‣ 5 State of the Art Performance ‣ Deep Learning
    for Instance Retrieval: A Survey"). For instance, the result on Oxford-5k [[32](#bib.bib32)]
    by using a pre-trained VGG is improved from 66.9% to 81.5% in [[41](#bib.bib41)]
    when a single-margin Siamese loss is used. Similar trends can be also observed
    on the Paris-6k dataset. For classification-based fine-tuning, its performance
    may be improved by using powerful DCNNs and feature enhancement methods such as
    the attention mechanism in [[5](#bib.bib5)], with an mAP increased from 55.7%
    in [[39](#bib.bib39)] to 83.8% in [[5](#bib.bib5)] on Oxford-5k. As for pairwise
    ranking loss fine-tuning, in some cases the loss used for fine-tuning is essential
    for performance improvement. For example, RPN is re-trained using regression loss
    on Oxford-5k and Paris-6k (75.1% and 80.7%, respectively) [[92](#bib.bib92)].
    Its results are lower than the results from [[40](#bib.bib40)] (88.2% and 88.2%,
    respectively) where a transformation matrix is used to learn visual similarity.
    However, when RPN is trained by using triplet loss such as [[82](#bib.bib82)],
    the effectiveness of retrieval is improved significantly where the results are
    86.1% (on Oxford-5k) and 94.5% (on Paris-6k). Feature embedding methods are important
    for retrieval accuracy; Ong et al. [[41](#bib.bib41)] embedded Conv5 feature maps
    by Fisher Vector and achieved an mAP of 81.5% on Oxford-5k, while embedding feature
    maps by using VLAD achieves an mAP of 62.5% on this dataset [[43](#bib.bib43)],[[44](#bib.bib44)].'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '监督微调的评估。与现成模型相比，微调深度网络通常会提高准确性，参见表格 [III](#S5.T3 "TABLE III ‣ 5.3 Performance
    Comparison and Analysis ‣ 5 State of the Art Performance ‣ Deep Learning for Instance
    Retrieval: A Survey")。例如，使用预训练的 VGG 在 Oxford-5k 上的结果从 [[32](#bib.bib32)] 的 66.9%
    提高到了 [[41](#bib.bib41)] 的 81.5%，当使用单边距 Siamese 损失时。在 Paris-6k 数据集中也可以观察到类似的趋势。对于基于分类的微调，通过使用强大的
    DCNN 和特征增强方法（如 [[5](#bib.bib5)] 中的注意力机制），其性能可能会得到提升，在 Oxford-5k 上的 mAP 从 [[39](#bib.bib39)]
    的 55.7% 提高到 [[5](#bib.bib5)] 的 83.8%。至于配对排名损失微调，在某些情况下，微调所使用的损失对性能改进至关重要。例如，RPN
    在 Oxford-5k 和 Paris-6k 上使用回归损失重新训练（分别为 75.1% 和 80.7%） [[92](#bib.bib92)]。其结果低于
    [[40](#bib.bib40)]（分别为 88.2% 和 88.2%），其中使用变换矩阵来学习视觉相似性。然而，当使用如 [[82](#bib.bib82)]
    的三元组损失训练 RPN 时，检索效果显著提高，结果为 86.1%（在 Oxford-5k 上）和 94.5%（在 Paris-6k 上）。特征嵌入方法对检索准确性至关重要；Ong
    等人 [[41](#bib.bib41)] 通过 Fisher 向量嵌入 Conv5 特征图，并在 Oxford-5k 上实现了 81.5% 的 mAP，而使用
    VLAD 嵌入特征图在该数据集上实现了 62.5% 的 mAP [[43](#bib.bib43)], [[44](#bib.bib44)]。'
- en: 'Evaluation for unsupervised fine-tuning. Compared to supervised fine-tuning,
    unsupervised fine-tuning methods are relatively less explored. The difficulty
    for unsupervised fine-tuning is to mine sample relevance without ground-truth
    labels. In general, unsupervised fine-tuning methods should be expected to have
    lower performance than supervised. For instance, supervised fine-tuning using
    Siamese loss [[124](#bib.bib124)] achieves an mAP 88.4% on Holidays, while unsupervised
    fine-tuning using the same loss function in [[44](#bib.bib44)],[[46](#bib.bib46)],[[131](#bib.bib131)]
    achieves 82.5%, 83.1%, and 87.5%, respectively. However, unsupervised fine-tuning
    methods can achieve a similar accuracy, even outperform the supervised fine-tuning,
    if a suitable feature embedding method is used. For instance, Zhao et al. [[135](#bib.bib135)]
    explore global feature structure modeling the manifold learning, producing an
    mAP of 85.4% (on Oxford-5k) and 96.3% (on Paris-6k), which is similar to supervised
    results [[82](#bib.bib82)] of 86.1% (on Oxford-5k) and 94.5% (on Paris-6k). As
    another example, the precision of ResNet-101 fine-tuned by cross entropy loss
    achieves 83.8% on Oxford-5k [[5](#bib.bib5)], while the precision is further improved
    to 92.0% when an IME layer is used to embed features and fine-tuned in an unsupervised
    way [[45](#bib.bib45)]. Note that fine-tuning strategies are related to the type
    of the target retrieval datasets. As demonstrated in Table [IV](#S5.T4 "TABLE
    IV ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey") and [[71](#bib.bib71)], fine-tuning
    on different datasets may produce a different final retrieval performance.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '无监督微调的评估。与监督微调相比，无监督微调方法相对较少被探索。无监督微调的难点在于在没有真实标签的情况下挖掘样本的相关性。通常，无监督微调方法的性能应低于监督微调。例如，使用Siamese损失的监督微调[[124](#bib.bib124)]在Holidays数据集上取得了88.4%的mAP，而在[[44](#bib.bib44)],
    [[46](#bib.bib46)], [[131](#bib.bib131)]中使用相同损失函数的无监督微调分别达到了82.5%、83.1%和87.5%。然而，如果使用适当的特征嵌入方法，无监督微调方法可以达到类似的准确度，甚至超过监督微调。例如，Zhao等人[[135](#bib.bib135)]探讨了全球特征结构建模流形学习，在Oxford-5k上取得了85.4%的mAP，在Paris-6k上取得了96.3%，这些结果与监督结果[[82](#bib.bib82)]在Oxford-5k上取得的86.1%和在Paris-6k上取得的94.5%相似。另一个例子是，ResNet-101通过交叉熵损失进行微调在Oxford-5k上取得了83.8%的精度，而使用IME层嵌入特征并以无监督方式进行微调时，精度进一步提高到92.0%[[45](#bib.bib45)]。需要注意的是，微调策略与目标检索数据集的类型相关。如表[IV](#S5.T4
    "TABLE IV ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey")和[[71](#bib.bib71)]所示，在不同的数据集上微调可能会产生不同的最终检索性能。'
- en: 'Network depth. We compare the efficacy of DCNNs by depth, following the fine-tuning
    protocols³³3https://github.com/filipradenovic/cnnimageretrieval-pytorch in [[46](#bib.bib46)].
    For fair comparisons, all convolutional features from these backbone DCNNs are
    aggregated by MAC [[69](#bib.bib69)], and fine-tuned by using the same loss function
    with the same learning rate, thus the adopted methods are the same except for
    the DCNN depth. We use the default feature dimension (*i.e.* AlexNet (256), VGG
    (512), GoogLeNet (1024), ResNet-50/101 (2048)). The results are reported in Figure
    [11](#S5.F11 "Figure 11 ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of
    the Art Performance ‣ Deep Learning for Instance Retrieval: A Survey") (a). We
    observe that the deeper networks consistently lead to better accuracy due to extracting
    more discriminative features.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '网络深度。我们按照深度比较了DCNNs的效果，遵循[[46](#bib.bib46)]中的微调协议³³3https://github.com/filipradenovic/cnnimageretrieval-pytorch。为了公平比较，从这些主干DCNNs中提取的所有卷积特征都通过MAC[[69](#bib.bib69)]进行聚合，并使用相同的损失函数和学习率进行微调，因此采用的方法除了DCNN深度外都是相同的。我们使用默认的特征维度（*即*
    AlexNet（256）、VGG（512）、GoogLeNet（1024）、ResNet-50/101（2048））。结果如图[11](#S5.F11 "Figure
    11 ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey")（a）所示。我们观察到，较深的网络由于提取了更多的判别特征，通常能带来更好的准确度。'
- en: 'Feature aggregation methods. The methods of embedding convolutional feature
    maps were illustrated in Figure [5](#S3.F5 "Figure 5 ‣ 3.1.2 Deep Feature Selection
    ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep
    Learning for Instance Retrieval: A Survey"). We use the off-the-shelf VGG (without
    updating parameters) on the Oxford and Paris datasets. The results are reported
    in Figure [11](#S5.F11 "Figure 11 ‣ 5.3 Performance Comparison and Analysis ‣
    5 State of the Art Performance ‣ Deep Learning for Instance Retrieval: A Survey")
    (b). We observe that the different ways to aggregate the same off-the-shelf DCNN
    leads to differences in retrieval performance. These reported results provide
    a reference for feature aggregation when one uses convolutional layers for performing
    retrieval tasks.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 特征聚合方法。嵌入卷积特征图的方法如图 [5](#S3.F5 "图 5 ‣ 3.1.2 深度特征选择 ‣ 3.1 深度特征提取 ‣ 3 使用现成的 DCNN
    模型进行检索 ‣ 实例检索的深度学习：综述") 所示。我们在牛津和巴黎数据集上使用现成的 VGG（不更新参数）。结果见图 [11](#S5.F11 "图 11
    ‣ 5.3 性能比较与分析 ‣ 5 最先进性能 ‣ 实例检索的深度学习：综述") (b)。我们观察到，对相同的现成 DCNN 进行不同方式的聚合会导致检索性能的差异。这些报告的结果为在进行检索任务时使用卷积层进行特征聚合提供了参考。
- en: 'Global feature dimension. We add fully-connected layers on the top of pooled
    convolutional features of ResNet-50 to obtain global descriptors with their dimensions
    varying from 32 to 8192\. The results of 5 datasets are shown in Figure [11](#S5.F11
    "Figure 11 ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey") (c). It is expected that higher-dimension
    features usually capture more semantics and are helpful for retrieval. The performance
    tends to be stable when the dimension is very large.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 全局特征维度。我们在 ResNet-50 的汇聚卷积特征顶部添加了全连接层，以获得维度从 32 到 8192 的全局描述符。5 个数据集的结果见图 [11](#S5.F11
    "图 11 ‣ 5.3 性能比较与分析 ‣ 5 最先进性能 ‣ 实例检索的深度学习：综述") (c)。预计更高维度的特征通常能够捕捉更多的语义，对检索有帮助。当维度非常大时，性能趋于稳定。
- en: 'Number of image regions. We compare the retrieval performance when different
    number of regions are fed and other components are kept the same, as depicted
    in Figure [11](#S5.F11 "Figure 11 ‣ 5.3 Performance Comparison and Analysis ‣
    5 State of the Art Performance ‣ Deep Learning for Instance Retrieval: A Survey")
    (d). Convolutional features of each region are pooled as 2048-dim regional features
    by MAC and then aggregated into a global one. Note that the final memory requirement
    is identical for the case that a holistic image is used as input (i.e., regarded
    as the case where only one region is used). Regional inputs on an image are extracted
    with a 40% overlap of neighboring regions and the number varying from 1 to 41\.
    For Oxford-5k, the best result is given by the case where 9 image regions are
    used. For the rest datasets, 3 image regions give the best results. Finally, more
    regions extracted from one image decline the retrieval mAP. A reason is that features
    of background or irrelevant regions have also been aggregated, and negatively
    affect the performance.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图像区域数量。当输入不同数量的区域且其他组件保持不变时，我们比较了检索性能，如图 [11](#S5.F11 "图 11 ‣ 5.3 性能比较与分析 ‣
    5 最先进性能 ‣ 实例检索的深度学习：综述") (d) 所示。每个区域的卷积特征通过 MAC 汇聚成 2048 维区域特征，然后汇聚成一个全局特征。注意，对于将整体图像作为输入的情况（即视为只使用一个区域的情况），最终的内存需求是相同的。图像上的区域输入以
    40% 的重叠提取，数量从 1 到 41\. 对于 Oxford-5k，使用 9 个图像区域的结果最佳。对于其他数据集，3 个图像区域给出最佳结果。最后，从一张图像中提取的更多区域会降低检索
    mAP。原因之一是背景或无关区域的特征也被聚合，负面影响了性能。
- en: 'TABLE IV: Evaluations of training sets and retrieval reranking. Numerical results
    are cited from [[71](#bib.bib71)].'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：训练集和检索重新排序的评估。数值结果引用自 [[71](#bib.bib71)]。
- en: '| Conditions | Global | Local reranking | Training set | $\mathcal{R}$Oxf |
    $\mathcal{R}$Par | GLD-v2 testing |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 条件 | 全局 | 局部重新排序 | 训练集 | $\mathcal{R}$Oxf | $\mathcal{R}$Par | GLD-v2 测试
    |'
- en: '| GLD-v1 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| GLD-v1 |'
- en: '&#124; GLD-v2 &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GLD-v2 &#124;'
- en: '&#124; -clean &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -清洁 &#124;'
- en: '|'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ResNet -50 | Case 1 | ✓ |  ✗ | ✓ |  ✗ | 45.1 | 63.4 | 20.4 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 | 案例 1 | ✓ |  ✗ | ✓ |  ✗ | 45.1 | 63.4 | 20.4 |'
- en: '| Case 2 | ✓ |  ✗ |  ✗ | ✓ | 51.0 | 71.5 | 24.1 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 案例 2 | ✓ |  ✗ |  ✗ | ✓ | 51.0 | 71.5 | 24.1 |'
- en: '| Case 3 | ✓ | ✓ | ✓ |  ✗ | 54.2 | 64.9 | 22.3 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 案例 3 | ✓ | ✓ | ✓ |  ✗ | 54.2 | 64.9 | 22.3 |'
- en: '| Case 4 | ✓ | ✓ |  ✗ | ✓ | 57.9 | 71.0 | 24.3 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 案例 4 | ✓ | ✓ |  ✗ | ✓ | 57.9 | 71.0 | 24.3 |'
- en: '| ResNet -101 | Case 5 | ✓ |  ✗ | ✓ |  ✗ | 51.2 | 64.7 | 21.7 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-101 | 案例 5 | ✓ |  ✗ | ✓ |  ✗ | 51.2 | 64.7 | 21.7 |'
- en: '| Case 6 | ✓ |  ✗ |  ✗ | ✓ | 55.6 | 72.4 | 26.0 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 案例 6 | ✓ |  ✗ |  ✗ | ✓ | 55.6 | 72.4 | 26.0 |'
- en: '| Case 7 | ✓ | ✓ | ✓ |  ✗ | 59.3 | 65.5 | 24.3 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 案例 7 | ✓ | ✓ | ✓ |  ✗ | 59.3 | 65.5 | 24.3 |'
- en: '| Case 8 | ✓ | ✓ |  ✗ | ✓ | 64.0 | 72.8 | 26.8 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 案例 8 | ✓ | ✓ |  ✗ | ✓ | 64.0 | 72.8 | 26.8 |'
- en: 'Fine-tuning datasets and retrieval reranking. We compare performance on $\mathcal{R}$Oxford-5k,
    $\mathcal{R}$Paris-6k, and GLD-v2, aiming at comparing the role of different fine-tuning
    training sets and the effectiveness of retrieval reranking. Table [IV](#S5.T4
    "TABLE IV ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the Art Performance
    ‣ Deep Learning for Instance Retrieval: A Survey") lists 8 experimental scenarios
    using two network backbones, as in [[71](#bib.bib71)].'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '微调数据集和检索重新排序。我们比较了在 $\mathcal{R}$Oxford-5k、$\mathcal{R}$Paris-6k 和 GLD-v2 上的性能，旨在比较不同微调训练集的作用以及检索重新排序的有效性。表
    [IV](#S5.T4 "TABLE IV ‣ 5.3 Performance Comparison and Analysis ‣ 5 State of the
    Art Performance ‣ Deep Learning for Instance Retrieval: A Survey") 列出了使用两个网络骨干的
    8 种实验场景，如 [[71](#bib.bib71)] 中所示。'
- en: Since GLD-v2 provides class-level ground-truth, its including images show large
    context diversity and may pose challenges to the network fine-tuning. Thus, the
    pre-processing steps, as proposed in [[147](#bib.bib147)],[[63](#bib.bib63)],
    are necessary to select the more coherent images, referring to the GLD-v2-clean
    subset. As a result, when using the global features only, this cleaned version
    of the training set improves the performance, as observed in Cases 1/5 and Cases
    2/6 for ResNet-50/ResNet-101, respectively. As an important postprocessing strategy,
    reranking further boosts the accuracy after the initial filtering step by using
    global features.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GLD-v2 提供了类别级的真实值，其包含的图像显示了较大的上下文多样性，可能对网络微调提出挑战。因此，如 [[147](#bib.bib147)],
    [[63](#bib.bib63)] 中提出的预处理步骤是必要的，以选择更一致的图像，参考 GLD-v2-clean 子集。因此，仅使用全局特征时，经过清理的训练集提高了性能，这在
    ResNet-50/ResNet-101 的案例 1/5 和案例 2/6 中得到了观察。作为重要的后处理策略，重新排序在初步过滤步骤后使用全局特征进一步提高了准确性。
- en: '![Refer to caption](img/ef7a311726994b602fa17b301a05adde.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ef7a311726994b602fa17b301a05adde.png)'
- en: 'Figure 11: (a) The effectiveness of different DCNNs; (b) Comparison of the
    feature aggregation methods in Figure [5](#S3.F5 "Figure 5 ‣ 3.1.2 Deep Feature
    Selection ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN
    Models ‣ Deep Learning for Instance Retrieval: A Survey"); (c) The impact of global
    feature dimension by using ResNet-50; (d) Performance comparison when aggregating
    different numbers of image regions.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: (a) 不同 DCNN 的有效性；(b) 图 [5](#S3.F5 "Figure 5 ‣ 3.1.2 Deep Feature Selection
    ‣ 3.1 Deep Feature Extraction ‣ 3 Retrieval with Off-the-Shelf DCNN Models ‣ Deep
    Learning for Instance Retrieval: A Survey") 中特征聚合方法的比较；(c) 使用 ResNet-50 时全局特征维度的影响；(d)
    聚合不同数量图像区域时的性能比较。'
- en: 6 Conclusions and Outlooks
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与展望
- en: 'As a comprehensive yet timely survey on instance retrieval using deep learning,
    this paper has discussed the main challenges, presented a taxonomy of recent developments
    according to their roles in implementing instance retrieval, highlighted the recent
    representative methods and analyzed their merits and demerits, discussed the datasets,
    evaluation protocols, and SOTA performance. Nowadays the exponentially increasing
    amount of image and video data due to surveillance, e-commerce, medical images,
    handheld devices, robotics, etc. , offers an endless potential for applications
    of instance retrieval. Although significant progress has been made, as discussed
    in Section [1.2](#S1.SS2 "1.2 Key Challenges ‣ 1 Introduction ‣ Deep Learning
    for Instance Retrieval: A Survey"), the main challenges in instance retrieval
    have not been fully addressed. Below we identify a number of promising directions
    for future research.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '作为关于使用深度学习进行实例检索的全面且及时的综述，本文讨论了主要挑战，按照在实现实例检索中的角色对最近的发展进行了分类，突出展示了近期的代表性方法并分析了其优缺点，讨论了数据集、评估协议和
    SOTA 性能。如今，由于监控、电子商务、医学图像、手持设备、机器人等因素，图像和视频数据量呈指数级增长，为实例检索应用提供了无限潜力。尽管取得了显著进展，但正如在第
    [1.2](#S1.SS2 "1.2 Key Challenges ‣ 1 Introduction ‣ Deep Learning for Instance
    Retrieval: A Survey") 节中讨论的，实例检索中的主要挑战尚未完全解决。以下我们确定了一些未来研究的有前景的方向。'
- en: '(1) Accurate and robust feature representations. One of the main challenges
    in instance retrieval is the large intra-class variations due to changes in viewpoint,
    scale, illumination, weather condition and background clutter *etc.*, as we discussed
    in Section [1.2](#S1.SS2 "1.2 Key Challenges ‣ 1 Introduction ‣ Deep Learning
    for Instance Retrieval: A Survey"). However, DCNN representations have very little
    invariance, even though trained with lots of augmented data [[146](#bib.bib146)].
    Fortunately, before deep learning, with instance retrieval there are lots of important
    ideas in handling such intra-class variations like local interest point detectors
    and local invariant descriptors. Therefore, it is worth enabling DCNN to learn
    more accurate and robust representations via leveraging such traditional wisdom
    to design better DCNNs. In addition, unlike most objects in existing benchmarks
    which are rigid, planar and textured, textureless objects, 3D objects, transparent
    objects, reflective surfaces, etc. are still very hard to find.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '(1) 准确而鲁棒的特征表示。实例检索中的主要挑战之一是由于视角、尺度、光照、天气条件和背景杂乱*等*因素导致的大量类内变异，正如我们在第[1.2](#S1.SS2
    "1.2 Key Challenges ‣ 1 Introduction ‣ Deep Learning for Instance Retrieval: A
    Survey")节中讨论的。然而，尽管DCNN表示经过了大量的数据增强训练[[146](#bib.bib146)]，其不变性仍然很少。幸运的是，在深度学习之前，实例检索中有很多处理这种类内变异的重要思路，如局部兴趣点检测器和局部不变描述符。因此，值得通过利用这些传统智慧来设计更好的DCNNs，使DCNN能够学习到更准确和鲁棒的表示。此外，与大多数现有基准中的物体（它们是刚性的、平面的和有纹理的）不同，缺乏纹理的物体、3D物体、透明物体、反射表面等仍然很难找到。'
- en: In addition, pursuing accuracy alone is not sufficient, as instance retrieval
    systems should be able to resist potential adversarial attacks. Recently, deep
    networks have been proven to be fooled rather easily by adversarial examples [[157](#bib.bib157)],
    i.e., images added with intentionally designed yet nearly imperceptible perturbations,
    which raises serious safety and robustness concerns. However, adversarial robustness
    in instance retrieval [[157](#bib.bib157)],[[158](#bib.bib158)] has received very
    little attention, and should merit further effort.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，仅仅追求准确性是不够的，因为实例检索系统还应能抵御潜在的对抗攻击。最近，深度网络已被证明容易被对抗样本[[157](#bib.bib157)]欺骗，即加入了故意设计但几乎不可察觉的扰动的图像，这引发了严重的安全性和鲁棒性问题。然而，实例检索中的对抗鲁棒性[[157](#bib.bib157)],
    [[158](#bib.bib158)]几乎没有受到关注，值得进一步努力。
- en: (2) Compact and efficient deep representations. In instance retrieval, searching
    efficiently is as critical as searching accurately, especially for the pervasive
    mobile or wearable devices with very limited computing resources. However, existing
    methods adopt large scale, energy hungry DCNNs that are very difficult to be deployed
    in mobile devices. Hence, there has been pressing needs to develop compact, efficient,
    yet reusable deep representations tailored to the resource limited devices, like
    using binary neural networks [[64](#bib.bib64)],[[85](#bib.bib85)],[[86](#bib.bib86)].
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 紧凑而高效的深度表示。在实例检索中，高效的搜索与准确的搜索同样关键，尤其是在计算资源非常有限的普遍移动或可穿戴设备上。然而，现有方法采用了大规模、能耗巨大的DCNNs，这些网络在移动设备上的部署非常困难。因此，迫切需要开发紧凑、高效且可重复使用的深度表示，针对资源有限的设备，例如使用二进制神经网络[[64](#bib.bib64)],
    [[85](#bib.bib85)], [[86](#bib.bib86)]。
- en: (3) Learning with fewer labels. Deep learning require a large amount of high-quality
    labeled dataset to achieve high accuracy. The presence of labels errors or the
    limited amount of labeled data can greatly degrade DCNN’s accuracy. However, collecting
    massive amounts of accurately labeled data is costly. In practical scenarios,
    datasets like GLDv2 [[5](#bib.bib5)],[[63](#bib.bib63)] have long-tailed distributions,
    and noisy labels. Thus, to address such limitations, few shot learning [[159](#bib.bib159)],
    self-supervised learning [[160](#bib.bib160)], imbalanced learning [[63](#bib.bib63)],
    noisy label aware learning [[161](#bib.bib161)] etc. should be paid more attention
    in instance retrieval in the future.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 用较少标签进行学习。深度学习需要大量高质量的标记数据集以达到高准确性。标签错误的存在或标记数据量的有限会严重降低DCNN的准确性。然而，收集大量准确标记的数据是昂贵的。在实际场景中，像GLDv2[[5](#bib.bib5)],
    [[63](#bib.bib63)]这样的数据集具有长尾分布和噪声标签。因此，为了应对这些限制，未来实例检索中应更多关注少样本学习[[159](#bib.bib159)],
    自监督学习[[160](#bib.bib160)], 不平衡学习[[63](#bib.bib63)], 噪声标签感知学习[[161](#bib.bib161)]等。
- en: (4) Continual learning for instance retrieval. In specific, the current IIR
    methods make restrictive assumptions, such as the training data being enough and
    stationary, retraining from scratch being possible when new data becomes available,
    which is problematic in realistic conditions. Our living world is continuously
    varying, and in general data distributions are often non-stationary, new data
    may be added, and previously unseen classes may be encountered. Thus, continual
    learning plays a vital role in continuously updating the IIR systems. The key
    issues are how to retain and utilize the previously learned knowledge, how to
    update the retrieval system as new images becomes available, and how to learn
    and improve over time.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 实例检索的持续学习。具体来说，目前的IIR方法做出了一些限制性假设，如训练数据足够且静态、新数据可用时可以从头开始重新训练，这在现实条件下是有问题的。我们的生活世界不断变化，数据分布通常是非静态的，可能会添加新数据，可能会遇到之前未见过的类别。因此，持续学习在不断更新IIR系统中发挥着至关重要的作用。关键问题是如何保留和利用之前学到的知识，如何在新图像可用时更新检索系统，以及如何随着时间的推移进行学习和改进。
- en: (5) Privacy-aware instance retrieval. Most IIR systems concentrate on improving
    the accuracy or efficiency performance, and the higher performance might come
    at the cost of users’ privacy. Therefore, in some cases, such as personalized
    search systems, the privacy protection problem is also an important issue to be
    considered. Deep models should be privacy-aware and protect users’ personalized
    searching experience to avoid their worries about using such IIR systems.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 隐私感知实例检索。大多数IIR系统集中于提高准确性或效率性能，而更高的性能可能以用户隐私为代价。因此，在某些情况下，如个性化搜索系统，隐私保护问题也是需要考虑的重要问题。深度模型应具有隐私意识，保护用户的个性化搜索体验，以避免他们对使用此类IIR系统的担忧。
- en: (6) Video instance retrieval. Searching a specific instance in an image cannot
    always meet the requirements in some scenarios such as the video surveillance
    system in the field of searching criminals. Currently, with the rapid growth of
    video data, retrieving a certain object, place, or action in videos has become
    more and more important and highly necessary in the future. For video instance
    retrieval, 3D-CNNs models need to be built to learn video’s spatio-temporal representations
    to compute the semantic similarity of instances.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 视频实例检索。在某些场景下，例如寻找犯罪分子的监控系统中，无法仅通过在图像中搜索特定实例来满足要求。目前，随着视频数据的快速增长，在视频中检索某个对象、地点或动作变得越来越重要，并且在未来高度必要。对于视频实例检索，需要建立3D-CNN模型，以学习视频的时空表示，以计算实例的语义相似性。
- en: References
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. W. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain, “Content-based
    image retrieval at the end of the early years,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, vol. 22, no. 12, pp. 1349–1380, 2000.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. W. Smeulders, M. Worring, S. Santini, A. Gupta, 和 R. Jain，“早期阶段结束时的内容基础图像检索，”*IEEE
    Trans. Pattern Anal. Mach. Intell.*，第22卷，第12期，第1349–1380页，2000年。'
- en: '[2] M. S. Lew, N. Sebe, C. Djeraba, and R. Jain, “Content-based multimedia
    information retrieval: State of the art and challenges,” *ACM Trans. Multimedia
    Comput. Commun. Appl.*, vol. 2, no. 1, pp. 1–19, 2006.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. S. Lew, N. Sebe, C. Djeraba, 和 R. Jain，“基于内容的多媒体信息检索：现状与挑战，”*ACM Trans.
    Multimedia Comput. Commun. Appl.*，第2卷，第1期，第1–19页，2006年。'
- en: '[3] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, “Scalable person
    re-identification: A benchmark,” in *ICCV*, 2015, pp. 1116–1124.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, 和 Q. Tian，“可扩展的人物再识别：基准测试，”发表于*ICCV*，2015年，第1116–1124页。'
- en: '[4] X. Liu, S. Zhang, X. Wang, R. Hong, and Q. Tian, “Group-group loss-based
    global-regional feature learning for vehicle re-identification,” *IEEE Trans.
    Image Process.*, vol. 29, pp. 2638–2652, 2019.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] X. Liu, S. Zhang, X. Wang, R. Hong, 和 Q. Tian，“基于组间损失的全球-区域特征学习用于车辆再识别，”*IEEE
    Trans. Image Process.*，第29卷，第2638–2652页，2019年。'
- en: '[5] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han, “Largescale image retrieval
    with attentive deep local features,” in *ICCV*, 2017.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] H. Noh, A. Araujo, J. Sim, T. Weyand, 和 B. Han，“使用注意力深度局部特征的大规模图像检索，”发表于*ICCV*，2017年。'
- en: '[6] U. Chaudhuri, B. Banerjee, and A. Bhattacharya, “Siamese graph convolutional
    network for content based remote sensing image retrieval,” *Comput. Vis. Image
    Underst.*, vol. 184, pp. 22–30, 2019.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] U. Chaudhuri, B. Banerjee, 和 A. Bhattacharya，“基于内容的遥感图像检索的Siamese图卷积网络，”*Comput.
    Vis. Image Underst.*，第184卷，第22–30页，2019年。'
- en: '[7] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang, “Deepfashion: Powering robust
    clothes recognition and retrieval with rich annotations,” in *CVPR*, 2016, pp.
    1096–1104.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Z. Liu, P. Luo, S. Qiu, X. Wang, 和 X. Tang，“Deepfashion: 通过丰富的注释增强服装识别和检索的鲁棒性，”发表于
    *CVPR*，2016，第1096–1104页。'
- en: '[8] A. Gordo and D. Larlus, “Beyond instance-level image retrieval: Leveraging
    captions to learn a global visual representation for semantic retrieval,” in *CVPR*,
    2017, pp. 6589–6598.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Gordo 和 D. Larlus，“超越实例级图像检索：利用标题学习用于语义检索的全局视觉表示，”发表于 *CVPR*，2017，第6589–6598页。'
- en: '[9] B. Barz and J. Denzler, “Content-based image retrieval and the semantic
    gap in the deep learning era,” in *ICPR*, 2021, pp. 245–260.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] B. Barz 和 J. Denzler，“基于内容的图像检索和深度学习时代的语义差距，”发表于 *ICPR*，2021，第245–260页。'
- en: '[10] X. Wang, X. Han, W. Huang, D. Dong, and M. R. Scott, “Multi-similarity
    loss with general pair weighting for deep metric learning,” in *CVPR*, 2019, pp.
    5022–5030.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] X. Wang, X. Han, W. Huang, D. Dong 和 M. R. Scott，“具有一般配对加权的多相似度损失用于深度度量学习，”发表于
    *CVPR*，2019，第5022–5030页。'
- en: '[11] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen,
    and Y. Wu, “Learning fine-grained image similarity with deep ranking,” in *CVPR*,
    2014, pp. 1386–1393.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen
    和 Y. Wu，“通过深度排序学习细粒度图像相似性，”发表于 *CVPR*，2014，第1386–1393页。'
- en: '[12] A. Babenko and V. Lempitsky, “Aggregating local deep features for image
    retrieval,” in *ICCV*, 2015, pp. 1269–1277.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Babenko 和 V. Lempitsky，“用于图像检索的局部深度特征聚合，”发表于 *ICCV*，2015，第1269–1277页。'
- en: '[13] L. Zheng, Y. Yang, and Q. Tian, “SIFT meets CNN: A decade survey of instance
    retrieval,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 40, no. 5, pp. 1224–1244,
    2018.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] L. Zheng, Y. Yang 和 Q. Tian，“SIFT遇上CNN：实例检索的十年综述，” *IEEE Trans. Pattern
    Anal. Mach. Intell.*，第40卷，第5期，第1224–1244页，2018年。'
- en: '[14] W. Zhang, C.-W. Ngo, and X. Cao, “Hyperlink-aware object retrieval,” *IEEE
    Trans. Image Process.*, vol. 25, no. 9, pp. 4186–4198, 2016.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] W. Zhang, C.-W. Ngo 和 X. Cao，“超链接感知对象检索，” *IEEE Trans. Image Process.*，第25卷，第9期，第4186–4198页，2016年。'
- en: '[15] Y. Kalantidis, C. Mellina, and S. Osindero, “Cross-dimensional weighting
    for aggregated deep convolutional features,” in *ECCV*, 2016.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Kalantidis, C. Mellina 和 S. Osindero，“用于聚合深度卷积特征的跨维加权，”发表于 *ECCV*，2016年。'
- en: '[16] L. Zhang and Y. Rui, “Image search from thousands to billions in 20 years,”
    *ACM Trans. Multimedia Comput. Commun. Appl.*, vol. 9, no. 1s, p. 36, 2013.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] L. Zhang 和 Y. Rui，“图像搜索从数千到数十亿年：20年的历程，” *ACM Trans. Multimedia Comput.
    Commun. Appl.*，第9卷，第1s期，第36页，2013年。'
- en: '[17] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y. Zhang, and J. Li, “Deep
    learning for content-based image retrieval: A comprehensive study,” in *ACM MM*,
    2014, pp. 157–166.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y. Zhang 和 J. Li，“基于内容的图像检索的深度学习：全面研究，”发表于
    *ACM MM*，2014，第157–166页。'
- en: '[18] A. Alzu’bi, A. Amira, and N. Ramzan, “Semantic content-based image retrieval:
    A comprehensive study,” *J. Vis. Commun. Image Represent.*, vol. 32, pp. 20–54,
    2015.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Alzu’bi, A. Amira 和 N. Ramzan，“基于语义内容的图像检索：综合研究，” *J. Vis. Commun.
    Image Represent.*，第32卷，第20–54页，2015年。'
- en: '[19] X. Li, T. Uricchio, L. Ballan, M. Bertini, C. G. Snoek, and A. D. Bimbo,
    “Socializing the semantic gap: A comparative survey on image tag assignment, refinement,
    and retrieval,” *ACM Comput. Surv. (CSUR)*, vol. 49, no. 1, pp. 1–39, 2016.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] X. Li, T. Uricchio, L. Ballan, M. Bertini, C. G. Snoek 和 A. D. Bimbo，“社交化语义差距：图像标签分配、精细化和检索的比较综述，”
    *ACM Comput. Surv. (CSUR)*，第49卷，第1期，第1–39页，2016年。'
- en: '[20] W. Zhou, H. Li, and Q. Tian, “Recent advance in content-based image retrieval:
    A literature survey,” *arXiv preprint arXiv:1706.06064*, 2017.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] W. Zhou, H. Li 和 Q. Tian，“基于内容的图像检索的最新进展：文献综述，” *arXiv preprint arXiv:1706.06064*，2017年。'
- en: '[21] L. Piras and G. Giacinto, “Information fusion in content based image retrieval:
    A comprehensive overview,” *Inf. Fusion*, vol. 37, pp. 50–60, 2017.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] L. Piras 和 G. Giacinto，“基于内容的图像检索中的信息融合：全面概述，” *Inf. Fusion*，第37卷，第50–60页，2017年。'
- en: '[22] J. Wang, T. Zhang, N. Sebe, H. T. Shen *et al.*, “A survey on learning
    to hash,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 40, no. 4, pp. 769–790,
    2018.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Wang, T. Zhang, N. Sebe, H. T. Shen *等*，“哈希学习综述，” *IEEE Trans. Pattern
    Anal. Mach. Intell.*，第40卷，第4期，第769–790页，2018年。'
- en: '[23] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    *Int. J. Comput. Vis.*, vol. 60, no. 2, pp. 91–110, 2004.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] D. G. Lowe，“来自尺度不变关键点的独特图像特征，” *Int. J. Comput. Vis.*，第60卷，第2期，第91–110页，2004年。'
- en: '[24] J. Sivic and A. Zisserman, “Video google: A text retrieval approach to
    object matching in videos,” in *CVPR*, 2003, pp. 1470–1477.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Sivic 和 A. Zisserman，“视频谷歌：一种文本检索方法用于视频中的对象匹配，”发表于 *CVPR*，2003，第1470–1477页。'
- en: '[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NeurIPS*, 2012.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Krizhevsky, I. Sutskever 和 G. E. Hinton， “使用深度卷积神经网络进行 Imagenet 分类”，发表于
    *NeurIPS*，2012年。'
- en: '[26] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] K. He, X. Zhang, S. Ren 和 J. Sun， “深度残差学习用于图像识别”，发表于 *CVPR*，2016年，第770–778页。'
- en: '[27] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time
    object detection with region proposal networks,” in *NeurIPS*, 2015, pp. 91–99.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Ren, K. He, R. Girshick 和 J. Sun， “Faster R-CNN：使用区域提议网络实现实时目标检测”，发表于
    *NeurIPS*，2015年，第91–99页。'
- en: '[28] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz, and
    D. Terzopoulos, “Image segmentation using deep learning: A survey,” *IEEE Trans.
    Pattern Anal. Mach. Intell.*, 2021.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz 和 D.
    Terzopoulos， “使用深度学习的图像分割：综述”，*IEEE Trans. Pattern Anal. Mach. Intell.*，2021年。'
- en: '[29] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, “CNN features
    off-the-shelf: an astounding baseline for recognition,” in *CVPR workshops*, 2014,
    pp. 806–813.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] A. Sharif Razavian, H. Azizpour, J. Sullivan 和 S. Carlsson， “CNN 特征现成可用：一个惊人的识别基准”，发表于
    *CVPR 工作坊*，2014年，第806–813页。'
- en: '[30] Y. Gong, L. Wang, R. Guo, and S. Lazebnik, “Multi-scale orderless pooling
    of deep convolutional activation features,” in *ECCV*, 2014.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Gong, L. Wang, R. Guo 和 S. Lazebnik， “深度卷积激活特征的多尺度无序池化”，发表于 *ECCV*，2014年。'
- en: '[31] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
    features in deep neural networks?” in *NeurIPS*, 2014, pp. 3320–3328.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Yosinski, J. Clune, Y. Bengio 和 H. Lipson， “深度神经网络中的特征可转移性如何？”发表于 *NeurIPS*，2014年，第3320–3328页。'
- en: '[32] G. Tolias, R. Sicre, and H. Jégou, “Particular object retrieval with integral
    max-pooling of CNN activations,” in *ICLR*, 2015, pp. 1–12.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] G. Tolias, R. Sicre 和 H. Jégou， “基于 CNN 激活的积分最大池化的特定对象检索”，发表于 *ICLR*，2015年，第1–12页。'
- en: '[33] A. Jiménez, J. M. Alvarez, and X. Giró Nieto, “Class-weighted convolutional
    features for visual instance search,” in *BMVC*, 2017, pp. 1–12.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. Jiménez, J. M. Alvarez 和 X. Giró Nieto， “用于视觉实例搜索的类加权卷积特征”，发表于 *BMVC*，2017年，第1–12页。'
- en: '[34] T.-T. Do, T. Hoang, D.-K. L. Tan, H. Le, T. V. Nguyen, and N.-M. Cheung,
    “From selective deep convolutional features to compact binary representations
    for image retrieval,” *ACM Trans. Multimedia Comput. Commun. Appl.*, vol. 15,
    no. 2, pp. 1–22, 2018.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] T.-T. Do, T. Hoang, D.-K. L. Tan, H. Le, T. V. Nguyen 和 N.-M. Cheung，
    “从选择性深度卷积特征到图像检索的紧凑二进制表示”，*ACM Trans. Multimedia Comput. Commun. Appl.*，第15卷，第2期，第1–22页，2018年。'
- en: '[35] J. Xu, C. Wang, C. Qi, C. Shi, and B. Xiao, “Unsupervised part-based weighting
    aggregation of deep convolutional features for image retrieval,” in *AAAI*, 2018,
    pp. 7436–7443.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J. Xu, C. Wang, C. Qi, C. Shi 和 B. Xiao， “基于无监督的部分加权聚合深度卷积特征用于图像检索”，发表于
    *AAAI*，2018年，第7436–7443页。'
- en: '[36] Y. Li, X. Kong, L. Zheng, and Q. Tian, “Exploiting hierarchical activations
    of neural network for image retrieval,” in *ACM MM*, 2016.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Y. Li, X. Kong, L. Zheng 和 Q. Tian， “利用神经网络的层次激活进行图像检索”，发表于 *ACM MM*，2016年。'
- en: '[37] A. Sharif Razavian, J. Sullivan, A. Maki, and S. Carlsson, “A baseline
    for visual instance retrieval with deep convolutional networks,” in *ICLR*, 2015.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] A. Sharif Razavian, J. Sullivan, A. Maki 和 S. Carlsson， “基于深度卷积网络的视觉实例检索基准”，发表于
    *ICLR*，2015年。'
- en: '[38] T.-T. Do and N.-M. Cheung, “Embedding based on function approximation
    for large scale image search,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 40,
    no. 3, pp. 626–638, 2017.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] T.-T. Do 和 N.-M. Cheung， “基于函数近似的大规模图像检索嵌入”，*IEEE Trans. Pattern Anal.
    Mach. Intell.*，第40卷，第3期，第626–638页，2017年。'
- en: '[39] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky, “Neural codes
    for image retrieval,” in *ECCV*, 2014, pp. 584–599.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] A. Babenko, A. Slesarev, A. Chigorin 和 V. Lempitsky， “图像检索的神经编码”，发表于 *ECCV*，2014年，第584–599页。'
- en: '[40] N. Garcia and G. Vogiatzis, “Learning non-metric visual similarity for
    image retrieval,” *Image Vis. Comput.*, vol. 82, pp. 18–25, 2019.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] N. Garcia 和 G. Vogiatzis， “学习非度量视觉相似性用于图像检索”，*Image Vis. Comput.*，第82卷，第18–25页，2019年。'
- en: '[41] E.-J. Ong, S. Husain, and M. Bober, “Siamese network of deep fisher-vector
    descriptors for image retrieval,” *arXiv preprint arXiv:1702.00338*, 2017.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] E.-J. Ong, S. Husain 和 M. Bober， “深度 Fisher 向量描述符的 Siamese 网络用于图像检索”，*arXiv
    预印本 arXiv:1702.00338*，2017年。'
- en: '[42] A. Gordo, J. Almazán, J. Revaud, and D. Larlus, “Deep image retrieval:
    Learning global representations for image search,” in *ECCV*, 2016.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. Gordo, J. Almazán, J. Revaud 和 D. Larlus， “深度图像检索：学习用于图像搜索的全局表示”，发表于
    *ECCV*，2016年。'
- en: '[43] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, “NetVLAD:
    CNN architecture for weakly supervised place recognition,” in *CVPR*, 2015, pp.
    5297–5307.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, 和 J. Sivic, “NetVLAD:
    用于弱监督位置识别的 CNN 架构，” *CVPR*，2015年，第5297–5307页。'
- en: '[44] F. Radenović, G. Tolias, and O. Chum, “CNN image retrieval learns from
    BoW: Unsupervised fine-tuning with hard examples,” in *ECCV*, 2016, pp. 3–20.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] F. Radenović, G. Tolias, 和 O. Chum, “CNN 图像检索学习自 BoW: 无监督的困难样本微调，” *ECCV*，2016年，第3–20页。'
- en: '[45] J. Xu, C. Wang, C. Qi, C. Shi, and B. Xiao, “Iterative manifold embedding
    layer learned by incomplete data for large-scale image retrieval,” *IEEE Trans.
    Multimedia*, vol. 21, no. 6, pp. 1551–1562, 2017.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] J. Xu, C. Wang, C. Qi, C. Shi, 和 B. Xiao, “通过不完整数据学习的迭代流形嵌入层用于大规模图像检索，”
    *IEEE Trans. Multimedia*，第21卷，第6期，第1551–1562页，2017年。'
- en: '[46] F. Radenović, G. Tolias, and O. Chum, “Fine-tuning CNN image retrieval
    with no human annotation,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 41,
    no. 7, pp. 1655–1668, 2018.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] F. Radenović, G. Tolias, 和 O. Chum, “无需人工注释的 CNN 图像检索微调，” *IEEE Trans.
    Pattern Anal. Mach. Intell.*，第41卷，第7期，第1655–1668页，2018年。'
- en: '[47] C. Liu, G. Yu, M. Volkovs, C. Chang, H. Rai, J. Ma, and S. K. Gorti, “Guided
    similarity separation for image retrieval,” in *NeurIPS*, 2019.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] C. Liu, G. Yu, M. Volkovs, C. Chang, H. Rai, J. Ma, 和 S. K. Gorti, “引导相似性分离用于图像检索，”
    *NeurIPS*，2019年。'
- en: '[48] C. Chang, G. Yu, C. Liu, and M. Volkovs, “Explore-exploit graph traversal
    for image retrieval,” in *CVPR*, 2019, pp. 9423–9431.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] C. Chang, G. Yu, C. Liu, 和 M. Volkovs, “用于图像检索的探索-利用图遍历，” *CVPR*，2019年，第9423–9431页。'
- en: '[49] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深的卷积网络，” *arXiv preprint arXiv:1409.1556*，2014年。'
- en: '[50] J. Yue-Hei Ng, F. Yang, and L. S. Davis, “Exploiting local features from
    deep networks for image retrieval,” in *CVPR workshops*, 2015.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Yue-Hei Ng, F. Yang, 和 L. S. Davis, “利用深度网络中的局部特征进行图像检索，” *CVPR workshops*，2015年。'
- en: '[51] H. Jegou, M. Douze, and C. Schmid, “Hamming embedding and weak geometric
    consistency for large scale image search,” in *ECCV*, 2008.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] H. Jegou, M. Douze, 和 C. Schmid, “哈明嵌入和弱几何一致性用于大规模图像搜索，” *ECCV*，2008年。'
- en: '[52] Y. Liu, Y. Guo, S. Wu, and M. S. Lew, “Deepindex for accurate and efficient
    image retrieval,” in *ICMR*, 2015, pp. 43–50.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Liu, Y. Guo, S. Wu, 和 M. S. Lew, “用于准确和高效图像检索的 Deepindex，” *ICMR*，2015年，第43–50页。'
- en: '[53] K. Yan, Y. Wang, D. Liang, T. Huang, and Y. Tian, “CNN vs. SIFT for image
    retrieval: Alternative or complementary?” in *ACM MM*, 2016.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] K. Yan, Y. Wang, D. Liang, T. Huang, 和 Y. Tian, “CNN 与 SIFT 在图像检索中的比较:
    替代还是互补？” *ACM MM*，2016年。'
- en: '[54] H. Jégou, M. Douze, C. Schmid, and P. Pérez, “Aggregating local descriptors
    into a compact image representation,” in *CVPR*, 2010.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] H. Jégou, M. Douze, C. Schmid, 和 P. Pérez, “将局部描述符聚合成紧凑的图像表示，” *CVPR*，2010年。'
- en: '[55] F. Perronnin and C. Dance, “Fisher kernels on visual vocabularies for
    image categorization,” in *CVPR*, 2007, pp. 1–8.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] F. Perronnin 和 C. Dance, “用于图像分类的视觉词汇上的费舍尔核，” *CVPR*，2007年，第1–8页。'
- en: '[56] O. Siméoni, Y. Avrithis, and O. Chum, “Local features and visual words
    emerge in activations,” in *CVPR*, 2019, pp. 11 651–11 660.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] O. Siméoni, Y. Avrithis, 和 O. Chum, “局部特征和视觉词汇在激活中出现，” *CVPR*，2019年，第11 651–11 660页。'
- en: '[57] J. Revaud, J. Almazán, R. S. Rezende, and C. R. d. Souza, “Learning with
    average precision: Training image retrieval with a listwise loss,” in *ICCV*,
    2019, pp. 5107–5116.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] J. Revaud, J. Almazán, R. S. Rezende, 和 C. R. d. Souza, “通过平均精度学习: 使用列表损失训练图像检索，”
    *ICCV*，2019年，第5107–5116页。'
- en: '[58] B. Song, X. Bai, Q. Tian, and L. J. Latecki, “Regularized diffusion process
    on bidirectional context for object retrieval,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, vol. 41, no. 5, pp. 1213–1226, 2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] B. Song, X. Bai, Q. Tian, 和 L. J. Latecki, “双向上下文的正则化扩散过程用于对象检索，” *IEEE
    Trans. Pattern Anal. Mach. Intell.*，第41卷，第5期，第1213–1226页，2018年。'
- en: '[59] A. El-Nouby, N. Neverova, I. Laptev, and H. Jégou, “Training vision transformers
    for image retrieval,” *arXiv preprint arXiv:2102.05644*, 2021.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] A. El-Nouby, N. Neverova, I. Laptev, 和 H. Jégou, “训练视觉变换器用于图像检索，” *arXiv
    preprint arXiv:2102.05644*，2021年。'
- en: '[60] F. Tan, J. Yuan, and V. Ordonez, “Instance-level image retrieval using
    reranking transformers,” *ICCV*, 2021.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] F. Tan, J. Yuan, 和 V. Ordonez, “使用重新排序变换器的实例级图像检索，” *ICCV*，2021年。'
- en: '[61] K. Reddy Mopuri and R. Venkatesh Babu, “Object level deep feature pooling
    for compact image representation,” in *CVPR Workshops*, 2015.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] K. Reddy Mopuri 和 R. Venkatesh Babu, “用于紧凑图像表示的对象级深度特征池化，” *CVPR Workshops*，2015年。'
- en: '[62] O. Morere, J. Lin, A. Veillard, L.-Y. Duan, V. Chandrasekhar, and T. Poggio,
    “Nested invariance pooling and RBM hashing for image instance retrieval,” in *ICMR*,
    2017, pp. 260–268.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] O. Morere, J. Lin, A. Veillard, L.-Y. Duan, V. Chandrasekhar, 和 T. Poggio,
    “嵌套不变池化和RBM哈希用于图像实例检索,” 发表在 *ICMR*, 2017年, 页码260–268。'
- en: '[63] T. Weyand, A. Araujo, B. Cao, and J. Sim, “Google landmarks dataset v2-a
    large-scale benchmark for instance-level recognition and retrieval,” in *CVPR*,
    2020, pp. 2575–2584.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] T. Weyand, A. Araujo, B. Cao, 和 J. Sim, “Google地标数据集v2——用于实例级识别和检索的大规模基准,”
    发表在 *CVPR*, 2020年, 页码2575–2584。'
- en: '[64] J. Song, T. He, L. Gao, X. Xu, and H. T. Shen, “Deep region hashing for
    efficient large-scale instance search from images,” in *AAAI*, 2018.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. Song, T. He, L. Gao, X. Xu, 和 H. T. Shen, “深度区域哈希用于高效的大规模实例搜索,” 发表在
    *AAAI*, 2018年。'
- en: '[65] K. Lin, J. Lu, C.-S. Chen, J. Zhou, and M.-T. Sun, “Unsupervised deep
    learning of compact binary descriptors,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    2018.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] K. Lin, J. Lu, C.-S. Chen, J. Zhou, 和 M.-T. Sun, “无监督深度学习的紧凑二进制描述符,” *IEEE
    Trans. Pattern Anal. Mach. Intell.*, 2018年。'
- en: '[66] W. Zhou, H. Li, J. Sun, and Q. Tian, “Collaborative index embedding for
    image retrieval,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 40, no. 5, pp.
    1154–1166, 2017.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] W. Zhou, H. Li, J. Sun, 和 Q. Tian, “图像检索的协同索引嵌入,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, 第40卷，第5期，页码1154–1166, 2017年。'
- en: '[67] H. Liu, Y. Tian, Y. Yang, L. Pang, and T. Huang, “Deep relative distance
    learning: Tell the difference between similar vehicles,” in *CVPR*, 2016.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] H. Liu, Y. Tian, Y. Yang, L. Pang, 和 T. Huang, “深度相对距离学习：区分相似车辆,” 发表在
    *CVPR*, 2016年。'
- en: '[68] K. Ozaki and S. Yokoo, “Large-scale landmark retrieval/recognition under
    a noisy and diverse dataset,” in *CVPR Workshop*, 2019.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] K. Ozaki 和 S. Yokoo, “在噪声和多样化数据集下的大规模地标检索/识别,” 发表在 *CVPR Workshop*, 2019年。'
- en: '[69] A. S. Razavian, J. Sullivan, S. Carlsson, and A. Maki, “Visual instance
    retrieval with deep convolutional networks,” *ITE Trans. Media Technol. Appl.*,
    vol. 4, no. 3, pp. 251–258, 2016.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. S. Razavian, J. Sullivan, S. Carlsson, 和 A. Maki, “使用深度卷积网络的视觉实例检索,”
    *ITE Trans. Media Technol. Appl.*, 第4卷，第3期，页码251–258, 2016年。'
- en: '[70] S. Pang, J. Xue, J. Zhu, L. Zhu, and Q. Tian, “Unifying sum and weighted
    aggregations for efficient yet effective image representation computation,” *IEEE
    Trans. Image Process.*, vol. 28, no. 2, pp. 841–852, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] S. Pang, J. Xue, J. Zhu, L. Zhu, 和 Q. Tian, “统一求和和加权聚合以实现高效而有效的图像表示计算,”
    *IEEE Trans. Image Process.*, 第28卷，第2期，页码841–852, 2018年。'
- en: '[71] B. Cao, A. Araujo, and J. Sim, “Unifying deep local and global features
    for efficient image search,” in *ECCV*, 2020, pp. 726–743.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] B. Cao, A. Araujo, 和 J. Sim, “统一深度局部和全局特征以实现高效的图像搜索,” 发表在 *ECCV*, 2020年,
    页码726–743。'
- en: '[72] M. A. Fischler and R. C. Bolles, “Random sample consensus: a paradigm
    for model fitting with applications to image analysis and automated cartography,”
    *Communications of the ACM*, vol. 24, no. 6, pp. 381–395, 1981.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. A. Fischler 和 R. C. Bolles, “随机样本共识：模型拟合的范式及其在图像分析和自动制图中的应用,” *Communications
    of the ACM*, 第24卷，第6期，页码381–395, 1981年。'
- en: '[73] G. Tolias, Y. Avrithis, and H. Jégou, “Image search with selective match
    kernels: aggregation across single and multiple images,” *Int. J. Comput. Vis.*,
    vol. 116, no. 3, pp. 247–261, 2016.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] G. Tolias, Y. Avrithis, 和 H. Jégou, “使用选择性匹配核的图像搜索：单图像和多图像的聚合,” *Int.
    J. Comput. Vis.*, 第116卷，第3期，页码247–261, 2016年。'
- en: '[74] M. Teichmann, A. Araujo, M. Zhu, and J. Sim, “Detect-to-retrieve: Efficient
    regional aggregation for image search,” in *CVPR*, 2019.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] M. Teichmann, A. Araujo, M. Zhu, 和 J. Sim, “检测-检索：图像搜索的高效区域聚合,” 发表在 *CVPR*,
    2019年。'
- en: '[75] S. Pang, J. Ma, J. Zhu, J. Xue, and Q. Tian, “Improving object retrieval
    quality by integration of similarity propagation and query expansion,” *IEEE Trans.
    Multimedia*, vol. 21, no. 3, pp. 760–770, 2018.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] S. Pang, J. Ma, J. Zhu, J. Xue, 和 Q. Tian, “通过相似性传播和查询扩展的结合来提高物体检索质量,”
    *IEEE Trans. Multimedia*, 第21卷，第3期，页码760–770, 2018年。'
- en: '[76] C. Qi, C. Shi, J. Xu, C. Wang, and B. Xiao, “Spatial weighted fisher vector
    for image retrieval,” in *ICME*, 2017, pp. 463–468.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] C. Qi, C. Shi, J. Xu, C. Wang, 和 B. Xiao, “用于图像检索的空间加权费舍尔向量,” 发表在 *ICME*,
    2017年, 页码463–468。'
- en: '[77] H. J. Kim, E. Dunn, and J.-M. Frahm, “Learned contextual feature reweighting
    for image geo-localization,” in *CVPR*, 2017, pp. 3251–3260.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] H. J. Kim, E. Dunn, 和 J.-M. Frahm, “用于图像地理定位的学习上下文特征重加权,” 发表在 *CVPR*,
    2017年, 页码3251–3260。'
- en: '[78] E. Mohedano, K. McGuinness, X. Giró-i Nieto, and N. E. O’Connor, “Saliency
    weighted convolutional features for instance search,” in *CBMI*, 2018, pp. 1–6.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] E. Mohedano, K. McGuinness, X. Giró-i Nieto, 和 N. E. O’Connor, “用于实例搜索的显著性加权卷积特征,”
    发表在 *CBMI*, 2018年, 页码1–6。'
- en: '[79] F. Yang, J. Li, S. Wei, Q. Zheng, T. Liu, and Y. Zhao, “Two-stream attentive
    CNNs for image retrieval,” in *ACM MM*, 2017, pp. 1513–1521.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] F. Yang, J. Li, S. Wei, Q. Zheng, T. Liu, 和 Y. Zhao，“用于图像检索的双流注意力CNN”，发表于
    *ACM MM*，2017年，第1513–1521页。'
- en: '[80] H.-F. Yang, K. Lin, and C.-S. Chen, “Supervised learning of semantics-preserving
    hash via deep convolutional neural networks,” *IEEE Trans. Pattern Anal. Mach.
    Intell.*, vol. 40, no. 2, pp. 437–451, 2018.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] H.-F. Yang, K. Lin, 和 C.-S. Chen，“通过深度卷积神经网络的语义保留哈希的监督学习”，*IEEE Trans.
    Pattern Anal. Mach. Intell.*，第40卷，第2期，第437–451页，2018年。'
- en: '[81] Y. Liu, J. Song, K. Zhou, L. Yan, L. Liu, F. Zou, and L. Shao, “Deep self-taught
    hashing for image retrieval,” *IEEE Trans Cybern.*, vol. 49, no. 6, pp. 2229–2241,
    2018.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Y. Liu, J. Song, K. Zhou, L. Yan, L. Liu, F. Zou, 和 L. Shao，“用于图像检索的深度自学习哈希”，*IEEE
    Trans Cybern.*，第49卷，第6期，第2229–2241页，2018年。'
- en: '[82] A. Gordo, J. Almazan, J. Revaud, and D. Larlus, “End-to-end learning of
    deep visual representations for image retrieval,” *Int. J. Comput. Vis.*, vol.
    124, no. 2, pp. 237–254, 2017.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] A. Gordo, J. Almazan, J. Revaud, 和 D. Larlus，“用于图像检索的深度视觉表示的端到端学习”，*Int.
    J. Comput. Vis.*，第124卷，第2期，第237–254页，2017年。'
- en: '[83] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, and S. Carlsson, “Factors
    of transferability for a generic convnet representation,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, vol. 38, no. 9, pp. 1790–1802, 2016.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, 和 S. Carlsson，“通用卷积网络表示的迁移因素”，*IEEE
    Trans. Pattern Anal. Mach. Intell.*，第38卷，第9期，第1790–1802页，2016年。'
- en: '[84] E. Mohedano, K. McGuinness, N. E. O’Connor, A. Salvador, F. Marqués, and
    X. Giro-i Nieto, “Bags of local convolutional features for scalable instance search,”
    in *ICMR*, 2016, pp. 327–331.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] E. Mohedano, K. McGuinness, N. E. O’Connor, A. Salvador, F. Marqués, 和
    X. Giro-i Nieto，“用于可扩展实例搜索的局部卷积特征袋”，发表于 *ICMR*，2016年，第327–331页。'
- en: '[85] W. Zhao, H. Luo, J. Peng, and J. Fan, “Spatial pyramid deep hashing for
    large-scale image retrieval,” *Neurocomputing*, vol. 243, pp. 166–173, 2017.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] W. Zhao, H. Luo, J. Peng, 和 J. Fan，“大规模图像检索的空间金字塔深度哈希”，*Neurocomputing*，第243卷，第166–173页，2017年。'
- en: '[86] L. Zheng, S. Wang, J. Wang, and Q. Tian, “Accurate image search with multi-scale
    contextual evidences,” *Int. J. Comput. Vis.*, vol. 120, no. 1, pp. 1–13, 2016.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] L. Zheng, S. Wang, J. Wang, 和 Q. Tian，“具有多尺度上下文证据的准确图像搜索”，*Int. J. Comput.
    Vis.*，第120卷，第1期，第1–13页，2016年。'
- en: '[87] J. Cao, L. Liu, P. Wang, Z. Huang, C. Shen, and H. T. Shen, “Where to
    focus: Query adaptive matching for instance retrieval using convolutional feature
    maps,” *arXiv preprint arXiv:1606.06811*, 2016.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] J. Cao, L. Liu, P. Wang, Z. Huang, C. Shen, 和 H. T. Shen，“聚焦在哪里：使用卷积特征图的查询自适应匹配进行实例检索”，*arXiv预印本
    arXiv:1606.06811*，2016年。'
- en: '[88] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals from
    edges,” in *ECCV*, 2014, pp. 391–405.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] C. L. Zitnick 和 P. Dollár，“边缘框：从边缘定位对象提议”，发表于 *ECCV*，2014年，第391–405页。'
- en: '[89] T. Yu, Y. Wu, S. D. Bhattacharjee, and J. Yuan, “Efficient object instance
    search using fuzzy objects matching.” in *AAAI*, 2017, pp. 4320–4326.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] T. Yu, Y. Wu, S. D. Bhattacharjee, 和 J. Yuan，“使用模糊对象匹配的高效对象实例搜索”，发表于 *AAAI*，2017年，第4320–4326页。'
- en: '[90] S. Sun, W. Zhou, Q. Tian, and H. Li, “Scalable object retrieval with compact
    image representation from generic object regions,” *ACM Trans. Multimedia Comput.
    Commun. Appl.*, vol. 12, no. 2, pp. 1–21, 2015.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] S. Sun, W. Zhou, Q. Tian, 和 H. Li，“具有紧凑图像表示的可扩展对象检索来自通用对象区域”，*ACM Trans.
    Multimedia Comput. Commun. Appl.*，第12卷，第2期，第1–21页，2015年。'
- en: '[91] J. Mairal, P. Koniusz, Z. Harchaoui, and C. Schmid, “Convolutional kernel
    networks,” in *NeurIPS*, 2014, pp. 2627–2635.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] J. Mairal, P. Koniusz, Z. Harchaoui, 和 C. Schmid，“卷积核网络”，发表于 *NeurIPS*，2014年，第2627–2635页。'
- en: '[92] A. Salvador, X. Giró-i Nieto, F. Marqués, and S. Satoh, “Faster R-CNN
    features for instance search,” in *CVPR Workshops*, 2016, pp. 9–16.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] A. Salvador, X. Giró-i Nieto, F. Marqués, 和 S. Satoh，“用于实例搜索的更快R-CNN特征”，发表于
    *CVPR Workshops*，2016年，第9–16页。'
- en: '[93] S. Wang and S. Jiang, “INSTRE: a new benchmark for instance-level object
    retrieval and recognition,” *ACM Trans. Multimedia Comput. Commun. Appl.*, vol. 11,
    no. 3, pp. 37:1–37:21, 2015.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. Wang 和 S. Jiang，“INSTRE：用于实例级对象检索和识别的新基准”，*ACM Trans. Multimedia Comput.
    Commun. Appl.*，第11卷，第3期，第37:1–37:21页，2015年。'
- en: '[94] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Object retrieval
    with large vocabularies and fast spatial matching,” in *CVPR*, 2007, pp. 1–8.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] J. Philbin, O. Chum, M. Isard, J. Sivic, 和 A. Zisserman，“具有大词汇量和快速空间匹配的对象检索”，发表于
    *CVPR*，2007年，第1–8页。'
- en: '[95] ——, “Lost in quantization: Improving particular object retrieval in large
    scale image databases,” in *CVPR*, 2008, pp. 1–8.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] ——，“在量化中迷失：提高大型图像数据库中特定对象检索的效果”，发表于 *CVPR*，2008年，第1–8页。'
- en: '[96] T. Ng, V. Balntas, Y. Tian, and K. Mikolajczyk, “SOLAR: Second-order loss
    and attention for image retrieval,” in *ECCV*, 2020, pp. 253–270.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] T. Ng, V. Balntas, Y. Tian, 和 K. Mikolajczyk， “SOLAR: 二阶损失和注意力用于图像检索，”
    在 *ECCV*，2020 年，页 253–270。'
- en: '[97] L. Zheng, Y. Zhao, S. Wang, J. Wang, and Q. Tian, “Good practice in CNN
    feature transfer,” *CoRR*, vol. abs/1604.00133, 2016.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] L. Zheng, Y. Zhao, S. Wang, J. Wang, 和 Q. Tian， “CNN 特征迁移中的最佳实践，” *CoRR*，卷
    abs/1604.00133，2016 年。'
- en: '[98] Y. Lou, Y. Bai, S. Wang, and L.-Y. Duan, “Multi-scale context attention
    network for image retrieval,” in *ACM MM*, 2018, pp. 1128–1136.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Y. Lou, Y. Bai, S. Wang, 和 L.-Y. Duan， “用于图像检索的多尺度上下文注意网络，” 在 *ACM MM*，2018
    年，页 1128–1136。'
- en: '[99] Y. Li, Y. Xu, J. Wang, Z. Miao, and Y. Zhang, “MS-RMAC: Multiscale regional
    maximum activation of convolutions for image retrieval,” *IEEE Signal Process
    Lett.*, vol. 24, no. 5, pp. 609–613, 2017.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Y. Li, Y. Xu, J. Wang, Z. Miao, 和 Y. Zhang， “MS-RMAC: 用于图像检索的多尺度区域最大激活卷积，”
    *IEEE Signal Process Lett.*，卷 24，第 5 期，页 609–613，2017 年。'
- en: '[100] X. Xiang, Z. Wang, Z. Zhao, and F. Su, “Multiple saliency and channel
    sensitivity network for aggregated convolutional feature,” in *AAAI*, vol. 33,
    no. 01, 2019, pp. 9013–9020.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Xiang, Z. Wang, Z. Zhao, 和 F. Su， “用于聚合卷积特征的多重显著性和通道敏感网络，” 在 *AAAI*，卷
    33，第 01 期，2019 年，页 9013–9020。'
- en: '[101] S. Pang, J. Ma, J. Xue, J. Zhu, and V. Ordonez, “Deep feature aggregation
    and image re-ranking with heat diffusion for image retrieval,” *IEEE Trans. Multimedia*,
    vol. 21, no. 6, pp. 1513–1523, 2018.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. Pang, J. Ma, J. Xue, J. Zhu, 和 V. Ordonez， “深度特征聚合和图像重新排序与热扩散结合用于图像检索，”
    *IEEE Trans. Multimedia*，卷 21，第 6 期，页 1513–1523，2018 年。'
- en: '[102] W. Yu, K. Yang, H. Yao, X. Sun, and P. Xu, “Exploiting the complementary
    strengths of multi-layer CNN features for image retrieval,” *Neurocomputing*,
    vol. 237, pp. 235–241, 2017.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] W. Yu, K. Yang, H. Yao, X. Sun, 和 P. Xu， “利用多层 CNN 特征的互补优势进行图像检索，” *Neurocomputing*，卷
    237，页 235–241，2017 年。'
- en: '[103] M. Yang, D. He, M. Fan, B. Shi, X. Xue, F. Li, E. Ding, and J. Huang,
    “Dolg: Single-stage image retrieval with deep orthogonal fusion of local and global
    features,” in *ICCV*, 2021, pp. 11 772–11 781.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] M. Yang, D. He, M. Fan, B. Shi, X. Xue, F. Li, E. Ding, 和 J. Huang， “Dolg:
    单阶段图像检索通过深度正交融合局部和全局特征，” 在 *ICCV*，2021 年，页 11 772–11 781。'
- en: '[104] Z. Zhang, Y. Xie, W. Zhang, and Q. Tian, “Effective image retrieval via
    multilinear multi-index fusion,” *IEEE Trans. Multimedia*, vol. 21, no. 11, pp.
    2878–2890, 2019.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Z. Zhang, Y. Xie, W. Zhang, 和 Q. Tian， “通过多线性多索引融合实现有效的图像检索，” *IEEE Trans.
    Multimedia*，卷 21，第 11 期，页 2878–2890，2019 年。'
- en: '[105] Q. Wang, J. Lai, Z. Yang, K. Xu, P. Kan, W. Liu, and L. Lei, “Improving
    cross-dimensional weighting pooling with multi-scale feature fusion for image
    retrieval,” *Neurocomputing*, vol. 363, pp. 17–26, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Q. Wang, J. Lai, Z. Yang, K. Xu, P. Kan, W. Liu, 和 L. Lei， “通过多尺度特征融合改进跨维加权池化用于图像检索，”
    *Neurocomputing*，卷 363，页 17–26，2019 年。'
- en: '[106] L. Zheng, S. Wang, L. Tian, F. He, Z. Liu, and Q. Tian, “Query-adaptive
    late fusion for image search and person re-identification,” in *CVPR*, 2015, pp.
    1741–1750.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] L. Zheng, S. Wang, L. Tian, F. He, Z. Liu, 和 Q. Tian， “查询自适应晚期融合用于图像搜索和人物重识别，”
    在 *CVPR*，2015 年，页 1741–1750。'
- en: '[107] H. Xuan, R. Souvenir, and R. Pless, “Deep randomized ensembles for metric
    learning,” in *ECCV*, 2018, pp. 723–734.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] H. Xuan, R. Souvenir, 和 R. Pless， “用于度量学习的深度随机集成，” 在 *ECCV*，2018 年，页
    723–734。'
- en: '[108] B.-C. Chen, L. S. Davis, and S.-N. Lim, “An analysis of object embeddings
    for image retrieval,” *arXiv preprint arXiv:1905.11903*.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] B.-C. Chen, L. S. Davis, 和 S.-N. Lim， “图像检索的对象嵌入分析，” *arXiv 预印本 arXiv:1905.11903*。'
- en: '[109] F. Wang, W.-L. Zhao, C.-W. Ngo, and B. Merialdo, “A hamming embedding
    kernel with informative bag-of-visual words for video semantic indexing,” *ACM
    Trans. Multimedia Comput. Commun. Appl.*, vol. 10, no. 3, pp. 1–20, 2014.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] F. Wang, W.-L. Zhao, C.-W. Ngo, 和 B. Merialdo， “带有信息视觉词袋的哈明嵌入核用于视频语义索引，”
    *ACM Trans. Multimedia Comput. Commun. Appl.*，卷 10，第 3 期，页 1–20，2014 年。'
- en: '[110] A. Mishchuk, D. Mishkin, F. Radenović, and J. Matas, “Working hard to
    know your neighbor’s margins: Local descriptor learning loss,” in *NeurIPS*, 2017,
    pp. 4827–4838.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] A. Mishchuk, D. Mishkin, F. Radenović, 和 J. Matas， “努力了解你邻居的边界：局部描述符学习损失，”
    在 *NeurIPS*，2017 年，页 4827–4838。'
- en: '[111] A. Mukherjee, J. Sil, A. Sahu, and A. S. Chowdhury, “A bag of constrained
    informative deep visual words for image retrieval,” *Pattern Recognition Letters*,
    vol. 129, pp. 158–165, 2020.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] A. Mukherjee, J. Sil, A. Sahu, 和 A. S. Chowdhury， “用于图像检索的受约束的信息深度视觉词袋，”
    *Pattern Recognition Letters*，卷 129，页 158–165，2020 年。'
- en: '[112] M. Paulin, M. Douze, Z. Harchaoui, J. Mairal, F. Perronin, and C. Schmid,
    “Local convolutional features with unsupervised training for image retrieval,”
    in *ICCV*, 2015, pp. 91–99.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] M. Paulin, M. Douze, Z. Harchaoui, J. Mairal, F. Perronin, 和 C. Schmid，
    “用于图像检索的无监督训练的局部卷积特征，” 在 *ICCV*，2015 年，页 91–99。'
- en: '[113] H. Jegou, F. Perronnin, M. Douze, J. Sánchez, P. Perez, and C. Schmid,
    “Aggregating local image descriptors into compact codes,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, vol. 34, no. 9, pp. 1704–1716, 2012.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] H. Jegou, F. Perronnin, M. Douze, J. Sánchez, P. Perez, 和 C. Schmid,
    “将局部图像描述符聚合为紧凑编码”，*IEEE Trans. Pattern Anal. Mach. Intell.*，第34卷，第9期，页1704–1716，2012年。'
- en: '[114] J. Sánchez, F. Perronnin, T. Mensink, and J. Verbeek, “Image classification
    with the fisher vector: Theory and practice,” *Int. J. Comput. Vis.*, vol. 105,
    no. 3, pp. 222–245, 2013.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] J. Sánchez, F. Perronnin, T. Mensink, 和 J. Verbeek, “使用费舍尔向量进行图像分类：理论与实践”，*Int.
    J. Comput. Vis.*，第105卷，第3期，页222–245，2013年。'
- en: '[115] J. Cao, Z. Huang, and H. T. Shen, “Local deep descriptors in bag-of-words
    for image retrieval,” in *ACM MM*, 2017, pp. 52–58.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. Cao, Z. Huang, 和 H. T. Shen, “在词袋模型中用于图像检索的局部深度描述符”，见 *ACM MM*，2017年，页52–58。'
- en: '[116] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, and
    T. Sattler, “D2-net: A trainable cnn for joint description and detection of local
    features,” in *CVPR*, 2019, pp. 8092–8101.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, 和
    T. Sattler, “D2-net：用于局部特征联合描述和检测的可训练CNN”，见 *CVPR*，2019年，页8092–8101。'
- en: '[117] K. Mikolajczyk and C. Schmid, “Scale & affine invariant interest point
    detectors,” *Int. J. Comput. Vis.*, vol. 60, no. 1, pp. 63–86, 2004.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] K. Mikolajczyk 和 C. Schmid, “尺度和仿射不变的兴趣点检测器”，*Int. J. Comput. Vis.*，第60卷，第1期，页63–86，2004年。'
- en: '[118] G. Tolias, T. Jenicek, and O. Chum, “Learning and aggregating deep local
    descriptors for instance-level recognition,” in *ECCV*, 2020.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] G. Tolias, T. Jenicek, 和 O. Chum, “学习和聚合深度局部描述符用于实例级识别”，见 *ECCV*，2020年。'
- en: '[119] J. Yang, J. Liang, H. Shen, K. Wang, P. L. Rosin, and M.-H. Yang, “Dynamic
    match kernel with deep convolutional features for image retrieval,” *IEEE Trans.
    Image Process.*, vol. 27, no. 11, pp. 5288–5302, 2018.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] J. Yang, J. Liang, H. Shen, K. Wang, P. L. Rosin, 和 M.-H. Yang, “带有深度卷积特征的动态匹配核用于图像检索”，*IEEE
    Trans. Image Process.*，第27卷，第11期，页5288–5302，2018年。'
- en: '[120] J. Kim and S.-E. Yoon, “Regional attention based deep feature for image
    retrieval,” in *BMVC*, 2018, pp. 209–223.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] J. Kim 和 S.-E. Yoon, “基于区域注意力的深度特征用于图像检索”，见 *BMVC*，2018年，页209–223。'
- en: '[121] S. Wei, L. Liao, J. Li, Q. Zheng, F. Yang, and Y. Zhao, “Saliency inside:
    Learning attentive CNNs for content-based image retrieval,” *IEEE Trans. Image
    Process.*, vol. 28, no. 9, pp. 4580–4593, 2019.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] S. Wei, L. Liao, J. Li, Q. Zheng, F. Yang, 和 Y. Zhao, “内部显著性：学习注意力CNN用于基于内容的图像检索”，*IEEE
    Trans. Image Process.*，第28卷，第9期，页4580–4593，2019年。'
- en: '[122] T.-T. Do, D.-K. Le Tan, T. T. Pham, and N.-M. Cheung, “Simultaneous feature
    aggregating and hashing for large-scale image search,” in *CVPR*, 2017, pp. 6618–6627.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] T.-T. Do, D.-K. Le Tan, T. T. Pham, 和 N.-M. Cheung, “同时特征聚合和哈希用于大规模图像搜索”，见
    *CVPR*，2017年，页6618–6627。'
- en: '[123] K. Musgrave, S. Belongie, and S.-N. Lim, “A metric learning reality check,”
    in *ECCV*, 2020, pp. 681–699.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] K. Musgrave, S. Belongie, 和 S.-N. Lim, “度量学习现实检验”，见 *ECCV*，2020年，页681–699。'
- en: '[124] Y. Lv, W. Zhou, Q. Tian, S. Sun, and H. Li, “Retrieval oriented deep
    feature learning with complementary supervision mining,” *IEEE Trans. Image Process.*,
    vol. 27, no. 10, pp. 4945–4957, 2018.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Y. Lv, W. Zhou, Q. Tian, S. Sun, 和 H. Li, “检索导向的深度特征学习与互补监督挖掘”，*IEEE
    Trans. Image Process.*，第27卷，第10期，页4945–4957，2018年。'
- en: '[125] W. Min, S. Mei, Z. Li, and S. Jiang, “A two-stage triplet network training
    framework for image retrieval,” *IEEE Trans. Multimedia*, vol. 22, no. 12, pp.
    3128–3138, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] W. Min, S. Mei, Z. Li, 和 S. Jiang, “用于图像检索的双阶段三元组网络训练框架”，*IEEE Trans.
    Multimedia*，第22卷，第12期，页3128–3138，2020年。'
- en: '[126] J. Deng, J. Guo, and S. Zafeiriou, “Arcface: Additive angular margin
    loss for deep face recognition,” *CVPR*, pp. 4685–4694, 2019.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Deng, J. Guo, 和 S. Zafeiriou, “Arcface：用于深度人脸识别的附加角度边际损失”，*CVPR*，页4685–4694，2019年。'
- en: '[127] M. Boudiaf, J. Rony, I. M. Ziko, E. Granger, M. Pedersoli, P. Piantanida,
    and I. B. Ayed, “A unifying mutual information view of metric learning: cross-entropy
    vs. pairwise losses,” in *ECCV*, 2020, pp. 548–564.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] M. Boudiaf, J. Rony, I. M. Ziko, E. Granger, M. Pedersoli, P. Piantanida,
    和 I. B. Ayed, “度量学习的统一互信息视图：交叉熵与成对损失”，见 *ECCV*，2020年，页548–564。'
- en: '[128] J. Lin, O. Morere, A. Veillard, L.-Y. Duan, H. Goh, and V. Chandrasekhar,
    “Deephash for image instance retrieval: Getting regularization, depth and fine-tuning
    right,” in *ICMR*, 2017, pp. 133–141.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] J. Lin, O. Morere, A. Veillard, L.-Y. Duan, H. Goh, 和 V. Chandrasekhar,
    “Deephash用于图像实例检索：正确获取正则化、深度和微调”，见 *ICMR*，2017年，页133–141。'
- en: '[129] J. Cao, Z. Huang, P. Wang, C. Li, X. Sun, and H. T. Shen, “Quartet-net
    learning for visual instance retrieval,” in *ACM MM*, 2016, pp. 456–460.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] J. Cao, Z. Huang, P. Wang, C. Li, X. Sun, 和 H. T. Shen, “Quartet-net学习用于视觉实例检索”，见
    *ACM MM*，2016年，页456–460。'
- en: '[130] A. Brown, W. Xie, V. Kalogeiton, and A. Zisserman, “Smooth-AP: Smoothing
    the path towards large-scale image retrieval,” in *ECCV*, 2020, pp. 677–694.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] A. Brown, W. Xie, V. Kalogeiton, 和 A. Zisserman, “Smooth-AP：平滑通往大规模图像检索的路径，”发表于
    *ECCV*，2020年，页码677–694。'
- en: '[131] A. Iscen, G. Tolias, Y. Avrithis, and O. Chum, “Mining on manifolds:
    Metric learning without labels,” in *CVPR*, 2018, pp. 7642–7651.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] A. Iscen, G. Tolias, Y. Avrithis, 和 O. Chum, “流形上的挖掘：无标签的度量学习，”发表于 *CVPR*，2018年，页码7642–7651。'
- en: '[132] A. Iscen, Y. Avrithis, G. Tolias, T. Furon, and O. Chum, “Fast spectral
    ranking for similarity search,” in *CVPR*, 2018, pp. 7632–7641.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] A. Iscen, Y. Avrithis, G. Tolias, T. Furon, 和 O. Chum, “相似性搜索的快速谱排名，”发表于
    *CVPR*，2018年，页码7632–7641。'
- en: '[133] A. Iscen, G. Tolias, Y. Avrithis, T. Furon, and O. Chum, “Efficient diffusion
    on region manifolds: Recovering small objects with compact CNN representations,”
    in *CVPR*, 2017, pp. 2077–2086.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] A. Iscen, G. Tolias, Y. Avrithis, T. Furon, 和 O. Chum, “区域流形上的高效扩散：用紧凑CNN表示恢复小物体，”发表于
    *CVPR*，2017年，页码2077–2086。'
- en: '[134] M. Donoser and H. Bischof, “Diffusion processes for retrieval revisited,”
    in *CVPR*, 2013, pp. 1320–1327.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] M. Donoser 和 H. Bischof, “检索中的扩散过程重访，”发表于 *CVPR*，2013年，页码1320–1327。'
- en: '[135] Y. Zhao, L. Wang, L. Zhou, Y. Shi, and Y. Gao, “Modelling diffusion process
    by deep neural networks for image retrieval,” in *BMVC*, 2018.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Y. Zhao, L. Wang, L. Zhou, Y. Shi, 和 Y. Gao, “通过深度神经网络建模扩散过程用于图像检索，”发表于
    *BMVC*，2018年。'
- en: '[136] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *ICLR*, 2017.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] T. N. Kipf 和 M. Welling, “图卷积网络的半监督分类，”发表于 *ICLR*，2017年。'
- en: '[137] T. Maria and T. Anastasios, “Deep convolutional image retrieval: A general
    framework,” *Signal Process. Image Commun.*, vol. 63, pp. 30–43, 2018.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] T. Maria 和 T. Anastasios, “深度卷积图像检索：一个通用框架，”*Signal Process. Image Commun.*，第63卷，页码30–43，2018年。'
- en: '[138] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for
    unsupervised learning of visual features,” in *ECCV*, 2018, pp. 132–149.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] M. Caron, P. Bojanowski, A. Joulin, 和 M. Douze, “用于无监督视觉特征学习的深度聚类，”发表于
    *ECCV*，2018年，页码132–149。'
- en: '[139] W. Jiang, Y. Wu, C. Jing, T. Yu, and Y. Jia, “Unsupervised deep quantization
    for object instance search,” *Neurocomputing*, vol. 362, pp. 60–71, 2019.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] W. Jiang, Y. Wu, C. Jing, T. Yu, 和 Y. Jia, “无监督深度量化用于目标实例搜索，”*Neurocomputing*，第362卷，页码60–71，2019年。'
- en: '[140] B. Ke, J. Shao, Z. Huang, and H. T. Shen, “Feature reconstruction by
    laplacian eigenmaps for efficient instance search,” in *ICMR*, 2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] B. Ke, J. Shao, Z. Huang, 和 H. T. Shen, “通过拉普拉斯特征图重建进行高效实例搜索，”发表于 *ICMR*，2018年。'
- en: '[141] T. Shen, Z. Luo, L. Zhou, R. Zhang, S. Zhu, T. Fang, and L. Quan, “Matchable
    image retrieval by learning from surface reconstruction,” in *ACCV*, 2018, pp.
    415–431.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] T. Shen, Z. Luo, L. Zhou, R. Zhang, S. Zhu, T. Fang, 和 L. Quan, “通过学习表面重建进行可匹配图像检索，”发表于
    *ACCV*，2018年，页码415–431。'
- en: '[142] J. Hu, R. Ji, H. Liu, S. Zhang, C. Deng, and Q. Tian, “Towards visual
    feature translation,” in *CVPR*, 2019, pp. 3004–3013.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] J. Hu, R. Ji, H. Liu, S. Zhang, C. Deng, 和 Q. Tian, “迈向视觉特征转换，”发表于 *CVPR*，2019年，页码3004–3013。'
- en: '[143] C. Bai, H. Li, J. Zhang, L. Huang, and L. Zhang, “Unsupervised adversarial
    instance-level image retrieval,” *IEEE Trans. Multimedia*, 2021.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] C. Bai, H. Li, J. Zhang, L. Huang, 和 L. Zhang, “无监督对抗性实例级图像检索，”*IEEE
    Trans. Multimedia*，2021年。'
- en: '[144] M. Paulin, J. Mairal, M. Douze, Z. Harchaoui, F. Perronnin, and C. Schmid,
    “Convolutional patch representations for image retrieval: an unsupervised approach,”
    *Int. J. Comput. Vis.*, vol. 121, no. 1, pp. 149–168, 2017.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] M. Paulin, J. Mairal, M. Douze, Z. Harchaoui, F. Perronnin, 和 C. Schmid,
    “用于图像检索的卷积补丁表示：一种无监督方法，”*Int. J. Comput. Vis.*，第121卷，第1期，页码149–168，2017年。'
- en: '[145] D. Nister and H. Stewenius, “Scalable recognition with a vocabulary tree,”
    in *CVPR*, 2006, pp. 2161–2168.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] D. Nister 和 H. Stewenius, “使用词汇树的可扩展识别，”发表于 *CVPR*，2006年，页码2161–2168。'
- en: '[146] F. Radenovic, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum, “Revisiting
    oxford and paris: Large-scale image retrieval benchmarking,” in *CVPR*, 2018.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] F. Radenovic, A. Iscen, G. Tolias, Y. Avrithis, 和 O. Chum, “重访牛津和巴黎：大规模图像检索基准测试，”发表于
    *CVPR*，2018年。'
- en: '[147] S. Yokoo, K. Ozaki, E. Simo-Serra, and S. Iizuka, “Two-stage discriminative
    re-ranking for large-scale landmark retrieval,” in *CVPR Workshops*, 2020, pp.
    1012–1013.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] S. Yokoo, K. Ozaki, E. Simo-Serra, 和 S. Iizuka, “大规模地标检索的双阶段判别重排序，”发表于
    *CVPR Workshops*，2020年，页码1012–1013。'
- en: '[148] A. Alzu’bi, A. Amira, and N. Ramzan, “Content-based image retrieval with
    compact deep convolutional features,” *Neurocomputing*, vol. 249, pp. 95–105,
    2017.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] A. Alzu’bi, A. Amira, 和 N. Ramzan, “基于内容的图像检索与紧凑深度卷积特征，”*Neurocomputing*，第249卷，页码95–105，2017年。'
- en: '[149] S. S. Husain and M. Bober, “REMAP: Multi-layer entropy-guided pooling
    of dense CNN features for image retrieval,” *IEEE Trans. Image Process.*, vol. 28,
    no. 10, pp. 5201–5213, 2019.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] S. S. Husain 和 M. Bober, “REMAP: 多层熵引导的密集CNN特征池化用于图像检索，” *IEEE Trans.
    Image Process.*, vol. 28, no. 10, pp. 5201–5213, 2019.'
- en: '[150] L. T. Alemu and M. Pelillo, “Multi-feature fusion for image retrieval
    using constrained dominant sets,” *Image Vis Comput*, vol. 94, p. 103862, 2020.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] L. T. Alemu 和 M. Pelillo, “使用约束主导集的图像检索的多特征融合，” *Image Vis Comput*, vol.
    94, p. 103862, 2020.'
- en: '[151] L. P. Valem and D. C. G. Pedronette, “Graph-based selective rank fusion
    for unsupervised image retrieval,” *Pattern Recognit Lett*, 2020.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] L. P. Valem 和 D. C. G. Pedronette, “基于图的选择性排名融合用于无监督图像检索，” *Pattern Recognit
    Lett*, 2020.'
- en: '[152] F. Yang, R. Hinami, Y. Matsui, S. Ly, and S. Satoh, “Efficient image
    retrieval via decoupling diffusion into online and offline processing,” in *AAAI*,
    vol. 33, 2019, pp. 9087–9094.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] F. Yang, R. Hinami, Y. Matsui, S. Ly 和 S. Satoh, “通过将扩散解耦为在线和离线处理实现高效图像检索，”
    在 *AAAI*, vol. 33, 2019, pp. 9087–9094.'
- en: '[153] H.-F. Yang, K. Lin, and C.-S. Chen, “Cross-batch reference learning for
    deep classification and retrieval,” in *ACM MM*, 2016, pp. 1237–1246.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] H.-F. Yang, K. Lin 和 C.-S. Chen, “用于深度分类和检索的跨批次参考学习，” 在 *ACM MM*, 2016,
    pp. 1237–1246.'
- en: '[154] J. Ouyang, W. Zhou, M. Wang, Q. Tian, and H. Li, “Collaborative image
    relevance learning for visual re-ranking,” *IEEE Trans. Multimedia*, 2020.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] J. Ouyang, W. Zhou, M. Wang, Q. Tian 和 H. Li, “用于视觉重排序的协同图像相关学习，” *IEEE
    Trans. Multimedia*, 2020.'
- en: '[155] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun,
    “Overfeat: Integrated recognition, localization and detection using convolutional
    networks,” in *ICLR*, 2014.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus 和 Y. LeCun, “Overfeat:
    使用卷积网络进行集成识别、定位和检测，” 在 *ICLR*, 2014.'
- en: '[156] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the
    devil in the details: Delving deep into convolutional nets,” in *BMVC*, 2014.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] K. Chatfield, K. Simonyan, A. Vedaldi 和 A. Zisserman, “细节中的魔鬼归来：*深入探讨*卷积网络，”
    在 *BMVC*, 2014.'
- en: '[157] J. Li, R. Ji, H. Liu, X. Hong, Y. Gao, and Q. Tian, “Universal perturbation
    attack against image retrieval,” in *ICCV*, 2019, pp. 4899–4908.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] J. Li, R. Ji, H. Liu, X. Hong, Y. Gao 和 Q. Tian, “针对图像检索的通用扰动攻击，” 在 *ICCV*,
    2019, pp. 4899–4908.'
- en: '[158] G. Tolias, F. Radenovic, and O. Chum, “Targeted mismatch adversarial
    attack: Query with a flower to retrieve the tower,” in *ICCV*, 2019.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] G. Tolias, F. Radenovic 和 O. Chum, “有针对性的错配对抗攻击：用花查询以检索塔，” 在 *ICCV*,
    2019.'
- en: '[159] E. Triantafillou, R. Zemel, and R. Urtasun, “Few-shot learning through
    an information retrieval lens,” *NeurIPS*, vol. 30, 2017.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] E. Triantafillou, R. Zemel 和 R. Urtasun, “通过信息检索视角进行少样本学习，” *NeurIPS*,
    vol. 30, 2017.'
- en: '[160] Z. Deng, Y. Zhong, S. Guo, and W. Huang, “InsCLR: Improving instance
    retrieval with self-supervision,” in *AAAI*, 2022.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Z. Deng, Y. Zhong, S. Guo 和 W. Huang, “InsCLR: 通过自监督提高实例检索，” 在 *AAAI*,
    2022.'
- en: '[161] S. Ibrahimi, A. Sors, R. S. de Rezende, and S. Clinchant, “Learning with
    label noise for image retrieval by selecting interactions,” in *WACV*, 2022, pp.
    2181–2190.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] S. Ibrahimi, A. Sors, R. S. de Rezende 和 S. Clinchant, “通过选择交互学习带标签噪声的图像检索，”
    在 *WACV*, 2022, pp. 2181–2190.'
- en: '| ![[Uncaptioned image]](img/5f78e12ace98fa5920bda1a3c1634264.png) | Wei Chen
    received the Ph.D. degree at Leiden University, in 2021\. Before starting with
    PhD study in Leiden University, he received his Master degree from the National
    University of Defense Technology, China, in 2016\. His research interest focuses
    on cross-modal retrieval with deep learning methods, and also in the context of
    incremental learning. He has published papers in international conferences and
    journal including CVPR, ACM MM, PR, Neurocomputing, and IEEE TMM etc. |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/5f78e12ace98fa5920bda1a3c1634264.png) | 魏晨于2021年在莱顿大学获得博士学位。在开始莱顿大学的博士研究之前，他于2016年获得了中国国防科技大学的硕士学位。他的研究兴趣集中在使用深度学习方法进行跨模态检索，以及增量学习的背景下。他已在国际会议和期刊上发表了论文，包括CVPR、ACM
    MM、PR、Neurocomputing和IEEE TMM等。 |'
- en: '| ![[Uncaptioned image]](img/4e17ae3790a634b0b823a82d3630a52a.png) | Yu Liu
    is currently an Associate Professor in International School of Information Science
    and Engineering at Dalian University of Technology, China. He was a post-doctoral
    researcher in the PSI group of KU Leuven, Belgium. In 2018, he received the PhD
    degree from Leiden University, Netherlands. His research interests include object
    recognition and retrieval in the context of continual learning and zero-shot learning.
    He has co-organized several workshops at ICCV, CVPR and ECCV, respectively. He
    has published papers in CVPR, ICCV, ECCV, ACM MM, IEEE TIP, etc, and received
    the best paper award at MMM2017. |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/4e17ae3790a634b0b823a82d3630a52a.png) | Yu Liu 目前是中国大连理工大学国际信息科学与工程学院的副教授。他曾在比利时
    KU Leuven 的 PSI 组担任博士后研究员。2018年，他获得了荷兰莱顿大学的博士学位。他的研究兴趣包括在持续学习和零样本学习背景下的物体识别与检索。他曾在
    ICCV、CVPR 和 ECCV 等会议上共同组织了多个研讨会，并在 CVPR、ICCV、ECCV、ACM MM、IEEE TIP 等期刊上发表了论文，并在
    MMM2017 获得了最佳论文奖。 |'
- en: '| ![[Uncaptioned image]](img/18b01a05979f98534537127efbdb9ba6.png) | Weiping
    Wang received the Ph.D. degree in systems engineering from the National University
    of Defense Technology, Changsha, China. He is a Professor at Academy of Advanced
    Technology Research of Hunan. He has more than 200 papers published on journals
    and conferences including IEEE Transactions multimedia, Transactions on Vehicular
    Technology, etc. His current research interests focus on intelligent decision
    experimentation, multi-modal knowledge extraction and knowledge integrated computation.
    |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/18b01a05979f98534537127efbdb9ba6.png) | Weiping Wang 获得了中国国防科技大学的系统工程博士学位。他是湖南省先进技术研究院的教授。他在包括
    IEEE Transactions multimedia、Transactions on Vehicular Technology 等期刊和会议上发表了200多篇论文。他目前的研究兴趣集中在智能决策实验、多模态知识提取和知识综合计算上。
    |'
- en: '| ![[Uncaptioned image]](img/a583dba6c16eff067f922f79b0892833.png) | Erwin
    M. Bakker is co-director of the LIACS Media Lab at Leiden University. He has published
    widely in the fields of image retrieval, audio analysis and retrieval and bioinformatics.
    He was closely involved with the start of the International Conference on Image
    and Video Retrieval (CIVR). Moreover, he regularly serves as a program committee
    member or organizing committee member for scientific multimedia and human-computer
    interaction conferences and workshops. |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/a583dba6c16eff067f922f79b0892833.png) | Erwin M. Bakker 是莱顿大学
    LIACS 媒体实验室的联合主任。他在图像检索、音频分析与检索以及生物信息学领域有广泛的出版物。他曾密切参与国际图像与视频检索会议（CIVR）的创办。此外，他还经常担任科学多媒体和人机交互会议及研讨会的程序委员会或组织委员会成员。
    |'
- en: '| ![[Uncaptioned image]](img/5403b8269ad9a001b290869015fec1f7.png) | Theodoros
    Georgiou received the Ph.D. degree at Leiden University, in 2021\. His research
    interest focuses on deep learning methods applied on higher than two dimensional
    data. Before starting with PhD study in Leiden University, he received his Master
    degree from the Leiden University in 2016\. He has published papers in international
    conferences and journal including ICPR, CBMI, WCCI and IJMIR. |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/5403b8269ad9a001b290869015fec1f7.png) | Theodoros Georgiou
    于2021年在莱顿大学获得博士学位。他的研究兴趣集中于应用于高于二维数据的深度学习方法。在开始在莱顿大学攻读博士学位之前，他于2016年获得了莱顿大学的硕士学位。他在国际会议和期刊上发表了包括
    ICPR、CBMI、WCCI 和 IJMIR 在内的多篇论文。 |'
- en: '| ![[Uncaptioned image]](img/3f8e5e056ea138adc3e47ec1e46a9ca6.png) | Paul Fieguth
    is co-director of the Vision & Image Processing Group in Systems Design Engineering
    at the University of Waterloo, where he is Professor and Associate Dean. He received
    the Ph.D. degree from the Massachusetts Institute of Technology, Cambridge, in
    1995, and has held visiting appointments at the University of Heidelberg in Germany,
    at INRIA/Sophia in France, at the Cambridge Research Laboratory in Boston, at
    Oxford University and the Rutherford Appleton Laboratory in England. His research
    interests include statistical signal and image processing, hierarchical algorithms,
    data fusion, machine learning, and the interdisciplinary applications of such
    methods. He has published textbooks on Statistical Image Processing and on Complex
    Systems theory. |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '![[未标注的图片]](img/3f8e5e056ea138adc3e47ec1e46a9ca6.png) | 保罗·费古斯特是滑铁卢大学系统设计工程系视觉与图像处理组的联合主任，他在该校担任教授和副院长。他于1995年获得麻省理工学院的博士学位，并曾在德国海德堡大学、法国INRIA/Sophia、波士顿剑桥研究实验室、牛津大学以及英国拉瑟福德·阿普尔顿实验室担任访问职位。他的研究兴趣包括统计信号和图像处理、层次算法、数据融合、机器学习以及这些方法的跨学科应用。他曾出版过关于统计图像处理和复杂系统理论的教材。'
- en: '| ![[Uncaptioned image]](img/25ef2bd7a67a83bcdf1a360f92d18aee.png) | Li Liu
    received the Ph.D. degree in information and communication engineering from the
    National University of Defense Technology, China, in 2012\. She is currently a
    Professor with the College of System Engineering. She has held visiting appointments
    at the University of Waterloo, Canada, at the Chinese University of Hong Kong,
    and at the University of Oulu, Finland. Her current research interests include
    computer vision, pattern recognition and machine learning. Her papers have currently
    over 7800 citations in Google Scholar. |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '![[未标注的图片]](img/25ef2bd7a67a83bcdf1a360f92d18aee.png) | 刘力于2012年获得中国国防科技大学信息与通信工程博士学位。她目前是系统工程学院的教授，并曾在加拿大滑铁卢大学、中国香港中文大学和芬兰奥卢大学担任访问职位。她目前的研究兴趣包括计算机视觉、模式识别和机器学习。她的论文在Google
    Scholar上已有超过7800次引用。'
- en: '| ![[Uncaptioned image]](img/c8d22eeac6a65e7974fa0a28d65bb1a9.png) | Michael
    S. Lew is the head of the Deep Learning and Computer Vision Research Group and
    a full Professor at LIACS, Leiden University. He has published over a dozen books
    and 190 peer-reviewed scientific articles in the areas of image retrieval, computer
    vision, and deep learning. Notably, he had the most cited paper in the ACM Transactions
    on Multimedia and one of the top 10 most cited articles in the history (out of
    more than 16,000 articles) of the ACM SIGMM. He was also a founding member of
    the advisory committee for the TRECVID video retrieval evaluation project, chair
    of the steering committee for the ACM International Conference on Multimedia Retrieval
    and a member of the ACM SIGMM Executive Committee. |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '![[未标注的图片]](img/c8d22eeac6a65e7974fa0a28d65bb1a9.png) | 迈克尔·S·刘易斯是深度学习与计算机视觉研究组的负责人，同时也是莱顿大学LIACS的教授。他已出版了十多本书籍和190篇同行评审的科学文章，领域涵盖图像检索、计算机视觉和深度学习。特别是，他在ACM
    Transactions on Multimedia中发表的论文被引用最多，并且在ACM SIGMM历史上（超过16000篇文章）位列前10篇最被引用的文章之一。他还曾是TRECVID视频检索评估项目的顾问委员会创始成员，ACM国际多媒体检索会议的指导委员会主席，以及ACM
    SIGMM执行委员会的成员。'
