- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:40:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:40:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2304.05055] A Comprehensive Survey on Deep Graph Representation Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2304.05055] 关于深度图表示学习的综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2304.05055](https://ar5iv.labs.arxiv.org/html/2304.05055)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2304.05055](https://ar5iv.labs.arxiv.org/html/2304.05055)
- en: A Comprehensive Survey on Deep Graph Representation Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于深度图表示学习的综合调查
- en: Wei Ju [juwei@pku.edu.cn](mailto:juwei@pku.edu.cn) ,  Zheng Fang [fang˙z@pku.edu.cn](mailto:fang%CB%99z@pku.edu.cn)
    ,  Yiyang Gu [yiyanggu@pku.edu.cn](mailto:yiyanggu@pku.edu.cn) ,  Zequn Liu [zequnliu@pku.edu.cn](mailto:zequnliu@pku.edu.cn)
    ,  Qingqing Long [qingqinglong@pku.edu.cn](mailto:qingqinglong@pku.edu.cn) Peking
    UniversityBeijingChina100871 ,  Ziyue Qiao [ziyuejoe@gmail.com](mailto:ziyuejoe@gmail.com)
    The Hong Kong University of Science and TechnologyGuangzhouChina511453 ,  Yifang
    Qin [qinyifang@pku.edu.cn](mailto:qinyifang@pku.edu.cn) ,  Jianhao Shen [jhshen@pku.edu.cn](mailto:jhshen@pku.edu.cn)
    ,  Fang Sun [fts@pku.edu.cn](mailto:fts@pku.edu.cn) Peking UniversityBeijingChina100871
    ,  Zhiping Xiao [patricia.xiao@cs.ucla.edu](mailto:patricia.xiao@cs.ucla.edu)
    University of California, Los AngelesUSA90095 ,  Junwei Yang [yjwtheonly@pku.edu.cn](mailto:yjwtheonly@pku.edu.cn)
    ,  Jingyang Yuan [yuanjy@pku.edu.cn](mailto:yuanjy@pku.edu.cn) ,  Yusheng Zhao
    [yusheng.zhao@stu.pku.edu.cn](mailto:yusheng.zhao@stu.pku.edu.cn) Peking UniversityBeijingChina100871
    ,  Xiao Luo [xiaoluo@cs.ucla.edu](mailto:xiaoluo@cs.ucla.edu) University of California,
    Los AngelesUSA90095  and  Ming Zhang [mzhang˙cs@pku.edu.cn](mailto:mzhang%CB%99cs@pku.edu.cn)
    Peking UniversityBeijingChina100871(2023)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**魏菊** [juwei@pku.edu.cn](mailto:juwei@pku.edu.cn) 、**郑芳** [fang˙z@pku.edu.cn](mailto:fang%CB%99z@pku.edu.cn)
    、**屹扬·顾** [yiyanggu@pku.edu.cn](mailto:yiyanggu@pku.edu.cn) 、**泽群·刘** [zequnliu@pku.edu.cn](mailto:zequnliu@pku.edu.cn)
    、**青青·龙** [qingqinglong@pku.edu.cn](mailto:qingqinglong@pku.edu.cn) 北京大学，北京，中国
    100871，**紫月·乔** [ziyuejoe@gmail.com](mailto:ziyuejoe@gmail.com) 香港科技大学，广州，中国 511453，**义芳·秦**
    [qinyifang@pku.edu.cn](mailto:qinyifang@pku.edu.cn) 、**建浩·沈** [jhshen@pku.edu.cn](mailto:jhshen@pku.edu.cn)
    、**方孙** [fts@pku.edu.cn](mailto:fts@pku.edu.cn) 北京大学，北京，中国 100871，**志平·肖** [patricia.xiao@cs.ucla.edu](mailto:patricia.xiao@cs.ucla.edu)
    加州大学洛杉矶分校，美国 90095，**俊伟·杨** [yjwtheonly@pku.edu.cn](mailto:yjwtheonly@pku.edu.cn)
    、**靖洋·袁** [yuanjy@pku.edu.cn](mailto:yuanjy@pku.edu.cn) 、**宇晟·赵** [yusheng.zhao@stu.pku.edu.cn](mailto:yusheng.zhao@stu.pku.edu.cn)
    北京大学，北京，中国 100871，**肖洛** [xiaoluo@cs.ucla.edu](mailto:xiaoluo@cs.ucla.edu) 加州大学洛杉矶分校，美国
    90095 和 **明张** [mzhang˙cs@pku.edu.cn](mailto:mzhang%CB%99cs@pku.edu.cn) 北京大学，北京，中国
    100871 (2023)'
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Abstract.
- en: 'Graph representation learning aims to effectively encode high-dimensional sparse
    graph-structured data into low-dimensional dense vectors, which is a fundamental
    task that has been widely studied in a range of fields, including machine learning
    and data mining. Classic graph embedding methods follow the basic idea that the
    embedding vectors of interconnected nodes in the graph can still maintain a relatively
    close distance, thereby preserving the structural information between the nodes
    in the graph. However, this is sub-optimal due to: (i) traditional methods have
    limited model capacity which limits the learning performance; (ii) existing techniques
    typically rely on unsupervised learning strategies and fail to couple with the
    latest learning paradigms; (iii) representation learning and downstream tasks
    are dependent on each other which should be jointly enhanced. With the remarkable
    success of deep learning, deep graph representation learning has shown great potential
    and advantages over shallow (traditional) methods, there exist a large number
    of deep graph representation learning techniques have been proposed in the past
    decade, especially graph neural networks. In this survey, we conduct a comprehensive
    survey on current deep graph representation learning algorithms by proposing a
    new taxonomy of existing state-of-the-art literature. Specifically, we systematically
    summarize the essential components of graph representation learning and categorize
    existing approaches by the ways of graph neural network architectures and the
    most recent advanced learning paradigms. Moreover, this survey also provides the
    practical and promising applications of deep graph representation learning. Last
    but not least, we state new perspectives and suggest challenging directions which
    deserve further investigations in the future.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图表示学习旨在将高维稀疏图结构数据有效编码为低维密集向量，这是一项基础任务，已在包括机器学习和数据挖掘在内的多个领域广泛研究。经典的图嵌入方法遵循这样的基本思想：图中互联节点的嵌入向量仍能保持相对接近的距离，从而保留图中节点之间的结构信息。然而，这种方法并非最优，原因有：
    (i) 传统方法的模型容量有限，限制了学习性能； (ii) 现有技术通常依赖于无监督学习策略，未能与最新的学习范式结合； (iii) 表示学习和下游任务相互依赖，应该共同增强。随着深度学习的显著成功，深度图表示学习在相较于浅层（传统）方法上展示了巨大的潜力和优势，过去十年中提出了大量的深度图表示学习技术，尤其是图神经网络。在本综述中，我们通过提出一种新的现有前沿文献分类法，对当前的深度图表示学习算法进行全面综述。具体而言，我们系统总结了图表示学习的基本组成部分，并根据图神经网络架构和最新的先进学习范式对现有方法进行了分类。此外，本综述还提供了深度图表示学习的实际和有前景的应用。最后但同样重要的是，我们阐述了新的观点并建议了值得在未来进一步研究的挑战方向。
- en: 'Deep Learning on Graphs, Graph Representation Learning, Graph Neural Network,
    Survey^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†doi: XXXXXXX.XXXXXXX^†^†journal:
    JACM^†^†ccs: Computing methodologies Neural networks^†^†ccs: Computing methodologies Learning
    latent representations'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在图上的应用，图表示学习，图神经网络，综述^†^†版权：acmcopyright^†^†期刊年份：2023^†^†doi：XXXXXXX.XXXXXXX^†^†期刊：JACM^†^†ccs：计算方法 神经网络^†^†ccs：计算方法 学习潜在表示
- en: 1\. Introduction By Wei Ju
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 引言 作者：魏瑜
- en: Graphs have recently emerged as a powerful tool for representing a variety of
    structured and complex data, including social networks, traffic networks, information
    systems, knowledge graphs, protein-protein interaction networks, and physical
    interaction networks. As a kind of general form of data organization, graph structures
    are capable of naturally expressing the intrinsic relationship of these data,
    and thus can characterize plenty of non-Euclidean structures that are crucial
    in a variety of disciplines and domains due to their flexible adaptability. For
    example, to encode a social network as a graph, nodes on the graph are used to
    represent individual users, and edges are used to represent the relationship between
    two individuals, such as friends. In the field of biology, nodes can be used to
    represent proteins, and edges can be used to represent biological interactions
    between various proteins, such as the dynamic interactions between proteins. Thus,
    by analyzing and mining the graph-structured data, we can understand the deep
    meaning hidden behind the data, and further discover valuable knowledge, so as
    to benefit society and human beings.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，图作为一种强大的工具出现，用于表示各种结构化和复杂的数据，包括社交网络、交通网络、信息系统、知识图谱、蛋白质-蛋白质相互作用网络和物理交互网络。作为一种通用的数据组织形式，图结构能够自然地表达这些数据的内在关系，从而能够刻画许多由于其灵活适应性而在各种学科和领域中至关重要的非欧几里得结构。例如，为了将社交网络编码为图，图上的节点用于表示单个用户，边用于表示两个个体之间的关系，如朋友。在生物学领域，节点可以用于表示蛋白质，边可以用于表示各种蛋白质之间的生物学相互作用，如蛋白质之间的动态相互作用。因此，通过分析和挖掘图结构数据，我们可以理解隐藏在数据背后的深层含义，并进一步发现有价值的知识，从而造福社会和人类。
- en: In the last decade years, a wide range of machine learning algorithms have been
    developed for graph-structured data learning. Among them, traditional graph kernel
    methods (Gärtner et al., [2003](#bib.bib108); Kashima et al., [2003](#bib.bib178);
    Shervashidze et al., [2011a](#bib.bib315), [2009](#bib.bib317)) usually break
    down graphs into different atomic substructures and then use kernel functions
    to measure the similarity between all pairs of them. Although graph kernels could
    provide a perspective on modeling graph topology, these approaches often generate
    substructures or feature representations based on given hand-crafted criteria.
    These rules are rather heuristic, prone to suffer from high computational complexity,
    and therefore have weak scalability and subpar performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，已经开发出广泛的机器学习算法用于图结构数据学习。其中，传统的图核方法（Gärtner 等人，[2003](#bib.bib108)；Kashima
    等人，[2003](#bib.bib178)；Shervashidze 等人，[2011a](#bib.bib315)，[2009](#bib.bib317)）通常将图分解为不同的原子子结构，然后使用核函数来衡量它们之间的相似性。虽然图核可以提供建模图拓扑的视角，但这些方法通常基于给定的手工制作标准生成子结构或特征表示。这些规则相当启发式，容易受到高计算复杂度的影响，因此具有较弱的可扩展性和较差的性能。
- en: 'In the past few years, graph embedding algorithms (Ahmed et al., [2013](#bib.bib3);
    Perozzi et al., [2014](#bib.bib277); Tang et al., [2015b](#bib.bib345); Grover
    and Leskovec, [2016](#bib.bib124); Tang et al., [2015a](#bib.bib344); Wang et al.,
    [2016](#bib.bib360)) have ever-increasing emerged, which attempt to encode the
    structural information of the graph (usually a high-dimensional sparse matrix)
    and map it into a low-dimensional dense vector embedding to preserve the topology
    information and attribute information in the embedding space as much as possible,
    so that the learned graph embeddings can be naturally integrated into traditional
    machine learning algorithms. Compared to previous works which use feature engineering
    in the pre-processing phase to extract graph structural features, current graph
    embedding algorithms are conducted in a data-driven way leveraging machine learning
    algorithms (such as neural networks) to encode the structural information of the
    graph. Specifically, existing graph embedding methods can be categorized into
    the following main groups: (i) matrix factorization based methods (Ahmed et al.,
    [2013](#bib.bib3); Ou et al., [2016](#bib.bib269); Cao et al., [2015](#bib.bib37))
    that factorize the matrix to learn node embedding which preserves the graph property;
    (ii) deep learning based methods (Perozzi et al., [2014](#bib.bib277); Tang et al.,
    [2015b](#bib.bib345); Grover and Leskovec, [2016](#bib.bib124); Wang et al., [2016](#bib.bib360))
    that apply deep learning techniques specifically designed for graph-structured
    data; (iii) edge reconstruction based methods (Tang et al., [2015a](#bib.bib344);
    Man et al., [2016](#bib.bib254); Liu et al., [2016](#bib.bib230)) that either
    maximizes edge reconstruction probability or minimizes edge reconstruction loss.
    Generally, these methods typically depend on shallow architectures, and fail to
    exploit the potential and capacity of deep neural networks, resulting in sub-optimal
    representation quality and learning performance.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，图嵌入算法（Ahmed et al., [2013](#bib.bib3); Perozzi et al., [2014](#bib.bib277);
    Tang et al., [2015b](#bib.bib345); Grover and Leskovec, [2016](#bib.bib124); Tang
    et al., [2015a](#bib.bib344); Wang et al., [2016](#bib.bib360)）不断涌现，这些算法试图将图的结构信息（通常是高维稀疏矩阵）编码并映射到低维密集向量嵌入中，以尽可能保留嵌入空间中的拓扑信息和属性信息，从而使学习到的图嵌入可以自然地融入传统的机器学习算法中。与以前在预处理阶段使用特征工程来提取图结构特征的工作相比，当前的图嵌入算法采用数据驱动的方法，利用机器学习算法（如神经网络）来编码图的结构信息。具体来说，现有的图嵌入方法可以分为以下主要组别：（i）基于矩阵分解的方法（Ahmed
    et al., [2013](#bib.bib3); Ou et al., [2016](#bib.bib269); Cao et al., [2015](#bib.bib37)），通过矩阵分解学习节点嵌入，保留图的属性；（ii）基于深度学习的方法（Perozzi
    et al., [2014](#bib.bib277); Tang et al., [2015b](#bib.bib345); Grover and Leskovec,
    [2016](#bib.bib124); Wang et al., [2016](#bib.bib360)），应用专为图结构数据设计的深度学习技术；（iii）基于边重构的方法（Tang
    et al., [2015a](#bib.bib344); Man et al., [2016](#bib.bib254); Liu et al., [2016](#bib.bib230)），通过最大化边重构概率或最小化边重构损失。总体而言，这些方法通常依赖于浅层架构，未能充分利用深度神经网络的潜力和能力，导致表示质量和学习性能不尽如人意。
- en: Inspired by the recent remarkable success of deep neural networks, a range of
    deep learning algorithms has been developed for graph-structured data learning.
    The core of these methods is to generate effective node and graph representations
    using graph neural networks (GNNs), followed by a goal-oriented learning paradigm.
    In this way, the derived representations can be adaptively coupled with a variety
    of downstream tasks and applications. Following this line of thought, in this
    paper, we propose a new taxonomy to classify the existing graph representation
    learning algorithms, i.e., graph neural network architectures, learning paradigms,
    and various promising applications, as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1\.
    Introduction By Wei Ju ‣ A Comprehensive Survey on Deep Graph Representation Learning").
    Specifically, for the architectures of GNNs, we investigate the studies on graph
    convolutions, graph kernel neural networks, graph pooling, and graph transformer.
    For the learning paradigms, we explore three advanced types namely supervised/semi-supervised
    learning on graphs, graph self-supervised learning, and graph structure learning.
    To demonstrate the effectiveness of the learned graph representations, we provide
    several promising applications to build tight connections between representation
    learning and downstream tasks, such as social analysis, molecular property prediction
    and generation, recommender systems, and traffic analysis. Last but not least,
    we present some perspectives for thought and suggest challenging directions that
    deserve further study in the future.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 受到最近深度神经网络显著成功的启发，已经开发出一系列用于图结构数据学习的深度学习算法。这些方法的核心是使用图神经网络（GNNs）生成有效的节点和图表示，随后采用目标导向的学习范式。通过这种方式，所生成的表示可以与各种下游任务和应用自适应地结合。遵循这一思路，在本文中，我们提出了一种新的分类法来对现有的图表示学习算法进行分类，即图神经网络架构、学习范式以及各种有前途的应用，如图 [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction By Wei Ju ‣ A Comprehensive Survey on Deep Graph
    Representation Learning")所示。具体而言，对于GNNs的架构，我们研究了图卷积、图核神经网络、图池化和图变换器的相关研究。对于学习范式，我们探讨了三种先进类型，即图上的监督/半监督学习、图自监督学习和图结构学习。为了展示学习到的图表示的有效性，我们提供了几个有前景的应用，以建立表示学习与下游任务之间的紧密联系，如社会分析、分子属性预测与生成、推荐系统和交通分析。最后但同样重要的是，我们提出了一些思考角度，并建议了一些值得未来进一步研究的挑战方向。
- en: Differences between this survey and existing ones. Up to now, there exist some
    other overview papers focusing on different perspectives of graph representation
    learning(Wu et al., [2020](#bib.bib388); Zhou et al., [2020](#bib.bib464); Zhang
    et al., [2020a](#bib.bib447); Chami et al., [2022](#bib.bib41); Bacciu et al.,
    [2020](#bib.bib13); Xia et al., [2021](#bib.bib391); Zhou et al., [2022b](#bib.bib465);
    Khoshraftar and An, [2022](#bib.bib180); Chen et al., [2022b](#bib.bib44), [2020f](#bib.bib48))
    that are closely related to ours. However, there are very few comprehensive reviews
    have summarized deep graph representation learning simultaneously from the perspective
    of diverse GNN architectures and corresponding up-to-date learning paradigms.
    Therefore, we here clearly state their distinctions from our survey as follows.
    There have been several surveys on classic graph embedding(Goyal and Ferrara,
    [2018](#bib.bib120); Cai et al., [2018](#bib.bib33)), these works categorize graph
    embedding methods based on different training objectives. Wang et al. (Wang et al.,
    [2022a](#bib.bib367)) goes further and provides a comprehensive review of existing
    heterogeneous graph embedding approaches. With the rapid development of deep learning,
    there are a handful of surveys along this line. For example, Wu et al. (Wu et al.,
    [2020](#bib.bib388)) and Zhang et al. (Zhang et al., [2020a](#bib.bib447)) mainly
    focus on several classical and representative GNN architectures without exploring
    deep graph representation learning from a view of the most recent advanced learning
    paradigms such as graph self-supervised learning and graph structure learning.
    Xia et al. (Xia et al., [2021](#bib.bib391)) and Chami et al. (Chami et al., [2022](#bib.bib41))
    jointly summarize the studies of graph embeddings and GNNs. Zhou et al. (Zhou
    et al., [2020](#bib.bib464)) explores different types of computational modules
    for GNNs. One recent survey under review (Khoshraftar and An, [2022](#bib.bib180))
    categorizes the existing works in graph representation learning from both static
    and dynamic graphs. However, these taxonomies emphasize the basic GNN methods
    but pay insufficient attention to the learning paradigms, and provide few discussions
    of the most promising applications, such as recommender systems and molecular
    property prediction and generation. To the best of our knowledge, the most relevant
    survey published formally is (Zhou et al., [2022b](#bib.bib465)), which presents
    a review of GNN architectures and roughly discusses the corresponding applications.
    Nevertheless, this survey merely covers methods up to the year of 2020, missing
    the latest developments in the past two years.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查与现有调查的区别。到目前为止，已经存在一些其他的综述论文，关注图表示学习的不同视角（Wu et al., [2020](#bib.bib388);
    Zhou et al., [2020](#bib.bib464); Zhang et al., [2020a](#bib.bib447); Chami et
    al., [2022](#bib.bib41); Bacciu et al., [2020](#bib.bib13); Xia et al., [2021](#bib.bib391);
    Zhou et al., [2022b](#bib.bib465); Khoshraftar and An, [2022](#bib.bib180); Chen
    et al., [2022b](#bib.bib44), [2020f](#bib.bib48))，这些论文与我们的研究密切相关。然而，很少有全面的综述同时从不同的GNN架构和最新的学习范式的角度总结深度图表示学习。因此，我们在此明确阐述与我们的调查的区别。已经有几篇关于经典图嵌入的调查（Goyal
    and Ferrara, [2018](#bib.bib120); Cai et al., [2018](#bib.bib33)），这些工作基于不同的训练目标对图嵌入方法进行了分类。Wang
    et al.（Wang et al., [2022a](#bib.bib367)）进一步提供了现有异质图嵌入方法的综合综述。随着深度学习的快速发展，也有少数综述沿着这条路线展开。例如，Wu
    et al.（Wu et al., [2020](#bib.bib388)）和Zhang et al.（Zhang et al., [2020a](#bib.bib447)）主要关注几个经典且具有代表性的GNN架构，但没有从最新的学习范式（如图自监督学习和图结构学习）角度探讨深度图表示学习。Xia
    et al.（Xia et al., [2021](#bib.bib391)）和Chami et al.（Chami et al., [2022](#bib.bib41)）联合总结了图嵌入和GNN的研究。Zhou
    et al.（Zhou et al., [2020](#bib.bib464)）探讨了GNN的不同计算模块。近期正在审阅的一篇综述（Khoshraftar
    and An, [2022](#bib.bib180)）将现有的图表示学习工作从静态图和动态图两个方面进行分类。然而，这些分类强调了基本的GNN方法，但对学习范式关注不够，并且对最有前景的应用，如推荐系统和分子属性预测与生成，讨论较少。据我们所知，最相关的正式发布的综述是（Zhou
    et al., [2022b](#bib.bib465)），该综述回顾了GNN架构，并粗略讨论了相应的应用。然而，这篇综述仅覆盖了2020年之前的方法，遗漏了过去两年的最新发展。
- en: Therefore, it is highly desired to summarize the representative GNN methods,
    the most recent advanced learning paradigms, and promising applications into one
    unified and comprehensive framework. Moreover, we strongly believe this survey
    with a new taxonomy of literature and more than 400 studies will strengthen future
    research on deep graph representation learning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，非常希望将代表性的GNN方法、最新的先进学习范式和有前景的应用总结成一个统一且全面的框架。此外，我们坚信这项调查通过新的文献分类法和400多项研究将加强对深度图表示学习的未来研究。
- en: 'Contribution of this survey. The goal of this survey is to systematically review
    the literature on the advances of deep graph representation learning and discuss
    further directions. It aims to help the researchers and practitioners who are
    interested in this area, and support them in understanding the panorama and the
    latest developments of deep graph representation learning. The key contributions
    of this survey are summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的贡献。该调查的目标是系统地回顾深度图表示学习的进展，并讨论进一步的方向。旨在帮助对该领域感兴趣的研究人员和从业者，并支持他们理解深度图表示学习的全景和最新发展。本调查的关键贡献总结如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Systematic Taxonomy. We propose a systematic taxonomy to organize the existing
    deep graph representation learning approaches based on the ways of GNN architectures
    and the most recent advanced learning paradigms via providing some representative
    branches of methods. Moreover, several promising applications are presented to
    illustrate the superiority and potential of graph representation learning.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系统分类法。我们提出了一种系统分类法，通过提供一些代表性的方法分支来组织现有的深度图表示学习方法，基于GNN架构的方式和最新的先进学习范式。此外，展示了几个有前景的应用，以说明图表示学习的优越性和潜力。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Comprehensive Review. For each branch of this survey, we review the essential
    components and provide detailed descriptions of representative algorithms, and
    systematically summarize the characteristics to make the overview comparison.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合综述。对于本次调查的每个分支，我们回顾了基本组成部分，并提供了代表性算法的详细描述，并系统总结了特征，以便进行概述比较。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future Directions. Based on the properties of existing deep graph representation
    learning algorithms, we discuss the limitations and challenges of current methods
    and propose the potential as well as promising research directions deserving of
    future investigations.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来方向。基于现有深度图表示学习算法的特性，我们讨论了当前方法的局限性和挑战，并提出了值得未来研究的潜力和有前景的研究方向。
- en: '![Refer to caption](img/3c7c2defb2f50ff994b6f754fe621c36.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/3c7c2defb2f50ff994b6f754fe621c36.png)'
- en: Figure 1\. The architecture of this paper.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 本文的架构。
- en: 2\. Background By Wei Ju
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景 作者：魏炬
- en: In this section, we first briefly introduce some definitions in deep graph representation
    learning that need to be clarified, then we explain the reasons why we need graph
    representation learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先简要介绍一些需要澄清的深度图表示学习中的定义，然后解释我们为什么需要图表示学习。
- en: 2.1\. Problem Definition
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 问题定义
- en: 'Definition: Graph. Given a graph $G=(V,E,\mathbf{X})$, where $V=\{v_{1},\cdots,v_{|V|}\}$
    is the set of nodes, $E=\{e_{1},\cdots,e_{|V|}\}$ is the set of edges, and the
    edge $e=(v_{i},v_{j})\in E$ represent the connection relationship between nodes
    $v_{i}$ and $v_{j}$ in the graph. $\mathbf{X}\in\mathbb{R}^{|V|\times M}$ is the
    node feature matrix with $M$ being the dimension of each node feature. The adjacency
    matrix of a graph can be defined as $\mathbf{A}\in\mathbb{R}^{|V|\times|V|}$,
    where $\mathbf{A}_{ij}=1$ if $(v_{i},v_{j})\in E$, otherwise $\mathbf{A}_{ij}=0$.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：图。给定一个图$G=(V,E,\mathbf{X})$，其中$V=\{v_{1},\cdots,v_{|V|}\}$是节点集合，$E=\{e_{1},\cdots,e_{|V|}\}$是边集合，边$e=(v_{i},v_{j})\in
    E$表示图中节点$v_{i}$和$v_{j}$之间的连接关系。$\mathbf{X}\in\mathbb{R}^{|V|\times M}$是节点特征矩阵，其中$M$是每个节点特征的维度。图的邻接矩阵可以定义为$\mathbf{A}\in\mathbb{R}^{|V|\times|V|}$，其中$\mathbf{A}_{ij}=1$如果$(v_{i},v_{j})\in
    E$，否则$\mathbf{A}_{ij}=0$。
- en: The adjacency matrix can be regarded as the structural representation of the
    graph-structured data, in which each row of the adjacency matrix $\mathbf{A}$
    represents the connection relationship between the corresponding node of the row
    and all other nodes, which can be regarded as a discrete representation of the
    node. However, in real-life circumstances, the adjacency matrix $\mathbf{A}$ corresponding
    to $G$ is a highly sparse matrix, and if $\mathbf{A}$ is used directly as node
    representations, it will be seriously affected by computational efficiency. The
    storage space of the adjacency matrix $\mathbf{A}$ is $|V|\times|V|$, which is
    usually unacceptable when the total number of nodes grows to the order of millions.
    At the same time, the value of most dimensions in the node representation is 0\.
    The sparsity will make subsequent machine learning tasks very difficult.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 邻接矩阵可以看作是图结构数据的结构表示，其中邻接矩阵 $\mathbf{A}$ 的每一行表示该行对应节点与所有其他节点之间的连接关系，这可以看作是节点的离散表示。然而，在实际情况下，$G$
    对应的邻接矩阵 $\mathbf{A}$ 是一个高度稀疏的矩阵，如果直接将 $\mathbf{A}$ 用作节点表示，它将严重影响计算效率。邻接矩阵 $\mathbf{A}$
    的存储空间为 $|V|\times|V|$，当节点总数增长到百万级别时，这通常是不可接受的。同时，节点表示中大多数维度的值为 0\。这种稀疏性会使后续的机器学习任务非常困难。
- en: '*Graph representation learning* is a bridge between the original input data
    and the task objectives in the graph. The fundamental idea of the graph representation
    learning algorithm is first to learn the embedded representations of nodes or
    the entire graph from the input graph structure data and then apply these embedded
    representations to downstream related tasks, such as node classification, graph
    classification, link prediction, community detection, and visualization, etc.
    Specifically, it aims to learn low-dimensional, dense distributed embedding representations
    for nodes in the graph. Formally, the goal of graph representation learning is
    to learn its embedding vector representation $R_{v}\in\mathbb{R}^{d}$ for each
    node $v\in V$, where the dimension $d$ of the vector is much smaller than the
    total number of nodes $|V|$ in the graph.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*图表示学习* 是原始输入数据与图中的任务目标之间的桥梁。图表示学习算法的基本思想是首先从输入图结构数据中学习节点或整个图的嵌入表示，然后将这些嵌入表示应用于下游相关任务，如节点分类、图分类、链接预测、社区检测和可视化等。具体而言，它的目标是为图中的节点学习低维的、密集分布的嵌入表示。正式地说，图表示学习的目标是为每个节点
    $v\in V$ 学习其嵌入向量表示 $R_{v}\in\mathbb{R}^{d}$，其中向量的维度 $d$ 远小于图中节点的总数 $|V|$。'
- en: 2.2\. Why study deep graph representation learning
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 为什么研究深度图表示学习
- en: 'With the rapid development of deep learning techniques, deep neural networks
    such as convolutional neural networks and recurrent neural networks have made
    breakthroughs in the fields of computer vision, natural language processing, and
    speech recognition. They can well abstract the semantic information of images,
    natural languages, and speeches. However, current deep learning techniques fail
    to handle more complex and irregular graph-structured data. To effectively analyze
    and model this kind of non-Euclidean structure data, many graph representation
    learning algorithms have emerged in recent years, including graph embedding and
    graph neural networks. At present, compared with Euclidean-style data such as
    images, natural language, and speech, graph-structured data is high-dimensional,
    complex, and irregular. Therefore, the graph representation learning algorithm
    is a rather powerful tool for studying graph-structured data. To encode complex
    graph-structured data, deep graph representation learning needs to meet several
    characteristics: (1) *topological properties*: Graph representations need to capture
    the complex topological information of the graph, such as the relationship between
    nodes and nodes, and other substructure information, such as subgraphs, motif,
    etc. (2) *feature attributes*: It is necessary for graph representations to describe
    high-dimensional attribute features in the graph, including the attributes of
    nodes and edges themselves. (3) *scalability*: Because different real graph data
    have different characteristics, graph representation learning algorithms should
    be able to efficiently learn its embedding representation on different graph structure
    data, making it universal and transferable.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习技术的快速发展，卷积神经网络和递归神经网络等深度神经网络在计算机视觉、自然语言处理和语音识别等领域取得了突破性进展。它们能够很好地抽象图像、自然语言和语音的语义信息。然而，当前的深度学习技术无法处理更复杂和不规则的图结构数据。为了有效分析和建模这种非欧几里得结构数据，近年来出现了许多图表示学习算法，包括图嵌入和图神经网络。目前，与图像、自然语言和语音等欧几里得数据相比，图结构数据具有高维、复杂和不规则的特点。因此，图表示学习算法是研究图结构数据的一个相当强大的工具。为了编码复杂的图结构数据，深度图表示学习需要满足几个特性：（1）*拓扑属性*：图表示需要捕捉图的复杂拓扑信息，例如节点与节点之间的关系，以及其他子结构信息，如子图、模式等。（2）*特征属性*：图表示需要描述图中的高维属性特征，包括节点和边本身的属性。（3）*可扩展性*：由于不同的实际图数据具有不同的特征，图表示学习算法应能够高效地学习其在不同图结构数据上的嵌入表示，使其具有通用性和可迁移性。
- en: 3\. Graph Convolutions By Yusheng Zhao
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 图卷积 由 Yusheng Zhao
- en: 'Graph convolutions have become the basic building blocks in many deep graph
    representation learning algorithms and graph neural networks developed recently.
    In this section, we provide a comprehensive review of graph convolutions, which
    generally fall into two categories: spectral graph convolutions and spatial graph
    convolutions. Based on the solid mathematical foundations of Graph Signal Processing
    (GSP)(Shuman et al., [2013a](#bib.bib321); Sandryhaila and Moura, [2013](#bib.bib304);
    Hammond et al., [2011](#bib.bib130)), spectral graph convolutions seek to capture
    the patterns of the graph in the frequency domain. On the other hand, spatial
    graph convolutions inherit the idea of message passing from Recurrent Graph Neural
    Networks (RecGNNs), and they compute node features by aggregating the features
    of their neighbors. Thus, the computation graph of a node is derived from the
    local graph structure around it, and the graph topology is naturally incorporated
    into the way node features are computed. In this section, we first introduce spectral
    graph convolutions and then spatial graph convolutions, followed by a brief summary.
    In Table [1](#S3.T1 "Table 1 ‣ 3\. Graph Convolutions By Yusheng Zhao ‣ A Comprehensive
    Survey on Deep Graph Representation Learning"), we summarize a number of graph
    convolutions proposed in recent years.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积已经成为许多深度图表示学习算法和最近开发的图神经网络的基本构建块。在这一部分，我们对图卷积进行全面回顾，这些卷积通常分为两类：谱图卷积和空间图卷积。基于图信号处理（GSP）的坚实数学基础（Shuman
    et al., [2013a](#bib.bib321); Sandryhaila and Moura, [2013](#bib.bib304); Hammond
    et al., [2011](#bib.bib130)），谱图卷积试图捕捉图在频域中的模式。另一方面，空间图卷积继承了递归图神经网络（RecGNNs）的消息传递思想，它们通过聚合邻居的特征来计算节点特征。因此，节点的计算图是从其周围的局部图结构中派生出来的，图的拓扑自然地融入了节点特征计算的方式。在这一部分，我们首先介绍谱图卷积，然后介绍空间图卷积，最后做一个简要总结。在表 [1](#S3.T1
    "Table 1 ‣ 3\. Graph Convolutions By Yusheng Zhao ‣ A Comprehensive Survey on
    Deep Graph Representation Learning")中，我们总结了近年来提出的一些图卷积方法。
- en: Table 1\. Summary of graph convolution methods.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 图卷积方法的总结。
- en: '| Method | Category | Aggregation | Time Complexity |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类别 | 聚合 | 时间复杂度 |'
- en: '| Spectral CNN (Bruna et al., [2013](#bib.bib30)) | Spectral Graph Convolution
    | - | $O(n^{3})$ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 谱CNN (Bruna et al., [2013](#bib.bib30)) | 谱图卷积 | - | $O(n^{3})$ |'
- en: '| Henaff et al. (Henaff et al., [2015](#bib.bib138)) | Spectral Graph Convolution
    | - | $O(n^{3})$ |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Henaff et al. (Henaff et al., [2015](#bib.bib138)) | 谱图卷积 | - | $O(n^{3})$'
- en: '| ChebNet (Defferrard et al., [2016](#bib.bib67)) | Spectral Graph Convolution
    | - | $O(m)$ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| ChebNet (Defferrard et al., [2016](#bib.bib67)) | 谱图卷积 | - | $O(m)$ |'
- en: '| GCN (Kipf and Welling, [2016a](#bib.bib183)) | Spectral / Spatial | Weighted
    Average | $O(m)$ |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| GCN (Kipf and Welling, [2016a](#bib.bib183)) | 谱 / 空间 | 加权平均 | $O(m)$ |'
- en: '| CayleyNet (Levie et al., [2018](#bib.bib204)) | Spectral Graph Convolution
    | - | $O(m)$ |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| CayleyNet (Levie et al., [2018](#bib.bib204)) | 谱图卷积 | - | $O(m)$ |'
- en: '| GraphSAGE (Hamilton et al., [2017](#bib.bib129)) | Spatial Graph Convolution
    | General | $O(m)$ |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| GraphSAGE (Hamilton et al., [2017](#bib.bib129)) | 空间图卷积 | 一般 | $O(m)$ |'
- en: '| GAT (Veličković et al., [2017](#bib.bib352)) | Spatial Graph Convolution
    | Attentive | $O(m)$ |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| GAT (Veličković et al., [2017](#bib.bib352)) | 空间图卷积 | 注意力 | $O(m)$ |'
- en: '| DGCNN (Wang et al., [2019d](#bib.bib373)) | Spatial Graph Convolution | General
    | $O(m)$ |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| DGCNN (Wang et al., [2019d](#bib.bib373)) | 空间图卷积 | 一般 | $O(m)$ |'
- en: '| LanzcosNet (Liao et al., [2018](#bib.bib225)) | Spectral Graph Convolution
    | - | $O(n^{2})$ |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| LanzcosNet (Liao et al., [2018](#bib.bib225)) | 谱图卷积 | - | $O(n^{2})$ |'
- en: '| SGC (Wu et al., [2019b](#bib.bib384)) | Spatial Graph Convolution | Weighted
    Average | $O(m)$ |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| SGC (Wu et al., [2019b](#bib.bib384)) | 空间图卷积 | 加权平均 | $O(m)$ |'
- en: '| GWNN (Xu et al., [2019b](#bib.bib400)) | Spectral Graph Convolution | - |
    $O(m)$ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| GWNN (Xu et al., [2019b](#bib.bib400)) | 谱图卷积 | - | $O(m)$ |'
- en: '| GIN (Xu et al., [2018a](#bib.bib404)) | Spatial Graph Convolution | Sum |
    $O(m)$ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| GIN (Xu et al., [2018a](#bib.bib404)) | 空间图卷积 | 求和 | $O(m)$ |'
- en: '| GraphAIR (Hu et al., [2020d](#bib.bib144)) | Spatial Graph Convolution |
    Sum | $O(m)$ |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| GraphAIR (Hu et al., [2020d](#bib.bib144)) | 空间图卷积 | 求和 | $O(m)$ |'
- en: '| PNA (Corso et al., [2020](#bib.bib65)) | Spatial Graph Convolution | Multiple
    | $O(m)$ |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| PNA (Corso et al., [2020](#bib.bib65)) | 空间图卷积 | 多重 | $O(m)$ |'
- en: '| S²GC (Zhu and Koniusz, [2021](#bib.bib467)) | Spectral Graph Convolution
    | - | $O(m)$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| S²GC (Zhu and Koniusz, [2021](#bib.bib467)) | 谱图卷积 | - | $O(m)$ |'
- en: '| GNNML3 (Balcilar et al., [2021](#bib.bib16)) | Spatial / Spectral | - | $O(m)$
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| GNNML3 (Balcilar et al., [2021](#bib.bib16)) | 空间 / 谱 | - | $O(m)$ |'
- en: '| MSGNN (He et al., [2022](#bib.bib136)) | Spectral Graph Convolution | - |
    $O(m)$ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| MSGNN（He 等，[2022](#bib.bib136)） | 谱图卷积 | - | $O(m)$ |'
- en: '| EGC (Tailor et al., [2022](#bib.bib341)) | Spatial Graph Convolution | General
    | $O(m)$ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| EGC（Tailor 等，[2022](#bib.bib341)） | 空间图卷积 | 一般 | $O(m)$ |'
- en: 3.1\. Spectral Graph Convolutions
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 谱图卷积
- en: With the success of Convolutional Neural Networks (CNNs) in computer vision (Krizhevsky
    et al., [2017](#bib.bib196)), efforts have been made to transfer the idea of convolution
    to the graph domain. However, this is not an easy task because of the non-Euclidean
    nature of graphical data. Graph signal processing (GSP) (Shuman et al., [2013a](#bib.bib321);
    Sandryhaila and Moura, [2013](#bib.bib304); Hammond et al., [2011](#bib.bib130))
    defines the Fourier Transform on graphs and thus provide solid theoretical foundation
    of spectral graph convolutions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 随着卷积神经网络（CNNs）在计算机视觉中的成功（Krizhevsky 等，[2017](#bib.bib196)），已经有人努力将卷积思想转移到图形领域。然而，由于图数据的非欧几里得特性，这不是一项容易的任务。图信号处理（GSP）（Shuman
    等，[2013a](#bib.bib321)；Sandryhaila 和 Moura，[2013](#bib.bib304)；Hammond 等，[2011](#bib.bib130)）在图上定义了傅里叶变换，从而提供了谱图卷积的坚实理论基础。
- en: 'In graph signal processing, a graph signal refers to a set of scalars associated
    to every nodes in the graph, *i.e.* $f(v),~{}\forall v\in V$, and it can be written
    in the $n$-dimensional vector form $\mathbf{x}\in\mathbb{R}^{n}$, where $n$ is
    the number of nodes in the graph. Another core concept of graph signal processing
    is the symmetric normalized graph Laplacian matrix (or simply, the graph Laplacian),
    defined as $\mathbf{L}=\mathbf{I}-\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$,
    where $\mathbf{I}$ is the identity matrix, $\mathbf{D}$ is the degree matrix (*i.e.*
    a diagonal matrix $\mathbf{D}_{ii}=\sum_{j}\mathbf{A}_{ij}$), and $\mathbf{A}$
    is the adjacency matrix. In the typical setting of graph signal processing, the
    graph $G$ is undirected. Therefore, $\mathbf{L}$ is real symmetric and positive
    semi-definite. This guarantees the eigen decomposition of the graph Laplacian:
    $\mathbf{L}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{T}$, where $\mathbf{U}=[\mathbf{u}_{0},\mathbf{u}_{1},...,\mathbf{u}_{n-1}]$
    is the eigenvectors of the graph Laplacian and the diagonal elements of $\mathbf{\Lambda}=\text{diag}(\lambda_{0},\lambda_{1},...,\lambda_{n-1})$
    are the eigenvalues. With this, the Graph Fourier Transform (GFT) of a graph signal
    $\mathbf{x}$ is defined as $\tilde{\mathbf{x}}=\mathbf{U}^{T}\mathbf{x}$, where
    $\tilde{\mathbf{x}}$ is the graph frequencies of $\mathbf{x}$. Correspondingly,
    the Inverse Graph Fourier Transform can be written as $\mathbf{x}=\mathbf{U}\tilde{\mathbf{x}}$.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在图信号处理中，图信号指的是与图中每个节点相关联的一组标量，*即* $f(v),~{}\forall v\in V$，并且可以写成 $n$-维向量形式
    $\mathbf{x}\in\mathbb{R}^{n}$，其中 $n$ 是图中的节点数。图信号处理的另一个核心概念是对称归一化图拉普拉斯矩阵（简称图拉普拉斯矩阵），定义为
    $\mathbf{L}=\mathbf{I}-\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$，其中 $\mathbf{I}$
    是单位矩阵，$\mathbf{D}$ 是度矩阵（*即* 对角矩阵 $\mathbf{D}_{ii}=\sum_{j}\mathbf{A}_{ij}$），$\mathbf{A}$
    是邻接矩阵。在图信号处理的典型设置中，图 $G$ 是无向的。因此，$\mathbf{L}$ 是实对称的并且是半正定的。这保证了图拉普拉斯矩阵的特征分解：$\mathbf{L}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{T}$，其中
    $\mathbf{U}=[\mathbf{u}_{0},\mathbf{u}_{1},...,\mathbf{u}_{n-1}]$ 是图拉普拉斯矩阵的特征向量，而
    $\mathbf{\Lambda}=\text{diag}(\lambda_{0},\lambda_{1},...,\lambda_{n-1})$ 的对角元素是特征值。由此，图傅里叶变换（GFT）定义为
    $\tilde{\mathbf{x}}=\mathbf{U}^{T}\mathbf{x}$，其中 $\tilde{\mathbf{x}}$ 是 $\mathbf{x}$
    的图频率。相应地，逆图傅里叶变换可以写作 $\mathbf{x}=\mathbf{U}\tilde{\mathbf{x}}$。
- en: 'With GFT and the Convolution Theorem, the graph convolution of a graph signal
    $\mathbf{x}$ and a filter $\mathbf{g}$ can be defined as $\mathbf{g}*_{G}\mathbf{x}=\mathbf{U}(\mathbf{U}^{T}\mathbf{g}\odot\mathbf{U}^{T}\mathbf{x})$.
    To simplify this, let $\mathbf{g}_{\theta}=\text{diag}(\mathbf{U}^{T}g)$, the
    graph convolution can be written as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 GFT 和卷积定理，图信号 $\mathbf{x}$ 和滤波器 $\mathbf{g}$ 的图卷积可以定义为 $\mathbf{g}*_{G}\mathbf{x}=\mathbf{U}(\mathbf{U}^{T}\mathbf{g}\odot\mathbf{U}^{T}\mathbf{x})$。为了简化，设
    $\mathbf{g}_{\theta}=\text{diag}(\mathbf{U}^{T}g)$，图卷积可以写为：
- en: '| (1) |  | $\mathbf{g}*_{G}\mathbf{x}=\mathbf{U}\mathbf{g}_{\theta}\mathbf{U}^{T}\mathbf{x},$
    |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\mathbf{g}*_{G}\mathbf{x}=\mathbf{U}\mathbf{g}_{\theta}\mathbf{U}^{T}\mathbf{x},$
    |  |'
- en: which is the general form of most spectral graph convolutions. The key of spectral
    graph convolutions is to parameterize and learn the filter $\mathbf{g}_{\theta}$.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是大多数谱图卷积的一般形式。谱图卷积的关键是对滤波器 $\mathbf{g}_{\theta}$ 进行参数化和学习。
- en: Bruna et al. (Bruna et al., [2013](#bib.bib30)) propose Spectral Convolutional
    Neural Network (Spectral CNN) that sets graph filter as a learnable diagonal matrix
    $\mathbf{W}$. The convolution operation can be written as $\mathbf{y}=\mathbf{U}\mathbf{W}\mathbf{U}^{T}\mathbf{x}$.
    In practice, multi-channel signals and activation functions are common, and the
    graph convolution can be written as
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Bruna 等人（Bruna et al., [2013](#bib.bib30)）提出了谱卷积神经网络（Spectral CNN），该网络将图滤波器设置为可学习的对角矩阵
    $\mathbf{W}$。卷积操作可以写成 $\mathbf{y}=\mathbf{U}\mathbf{W}\mathbf{U}^{T}\mathbf{x}$。实际上，多通道信号和激活函数是常见的，图卷积可以写成
- en: '| (2) |  | $\mathbf{Y}_{:,j}=\sigma\left(\mathbf{U}\sum_{i=1}^{c_{in}}\mathbf{W}_{i,j}\mathbf{U}^{T}\mathbf{X}_{:,i}\right),~{}j=1,2,...,c_{out},$
    |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\mathbf{Y}_{:,j}=\sigma\left(\mathbf{U}\sum_{i=1}^{c_{in}}\mathbf{W}_{i,j}\mathbf{U}^{T}\mathbf{X}_{:,i}\right),~{}j=1,2,...,c_{out},$
    |  |'
- en: where $c_{in}$ is the number of input channel, $c_{out}$ is the number of output
    channel, $\mathbf{X}$ is a $n\times c_{in}$ matrix representing the input signal,
    $\mathbf{Y}$ is a $n\times c_{out}$ matrix denoting the output signal, $\mathbf{W}_{i,j}$
    is a parameterized diagonal matrix, and $\sigma(\cdot)$ is the activation function.
    For mathematical convenience we sometimes use single-channel version of graph
    convolutions omitting activation functions, and the multi-channel versions are
    similar to Eq. [2](#S3.E2 "In 3.1\. Spectral Graph Convolutions ‣ 3\. Graph Convolutions
    By Yusheng Zhao ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c_{in}$ 是输入通道数，$c_{out}$ 是输出通道数，$\mathbf{X}$ 是一个 $n\times c_{in}$ 矩阵，表示输入信号，$\mathbf{Y}$
    是一个 $n\times c_{out}$ 矩阵，表示输出信号，$\mathbf{W}_{i,j}$ 是一个参数化的对角矩阵，$\sigma(\cdot)$
    是激活函数。为了数学方便，我们有时使用单通道版本的图卷积，省略激活函数，多通道版本类似于公式 [2](#S3.E2 "In 3.1\. Spectral Graph
    Convolutions ‣ 3\. Graph Convolutions By Yusheng Zhao ‣ A Comprehensive Survey
    on Deep Graph Representation Learning")。
- en: 'Spectral CNN has several limitations. Firstly, the filters are basis-dependent,
    which means that they cannot generalize across graphs. Secondly, the algorithm
    requires eigen decomposition, which is computationally expensive. Thirdly, it
    has no guarantee of spatial localization of filters. To make filters spatially
    localized, Henaff et al. (Henaff et al., [2015](#bib.bib138)) propose to use a
    smooth spectral transfer function $\Theta(\mathbf{\Lambda})$ to parameterize the
    filter, and the convolution operation can be written as:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 谱 CNN 存在一些局限性。首先，滤波器依赖于基，这意味着它们无法在图之间泛化。其次，该算法需要特征分解，这在计算上是昂贵的。第三，它无法保证滤波器的空间定位。为了使滤波器具有空间定位性，Henaff
    等人（Henaff et al., [2015](#bib.bib138)）提出使用平滑的谱传递函数 $\Theta(\mathbf{\Lambda})$
    来参数化滤波器，卷积操作可以写成：
- en: '| (3) |  | $\mathbf{y}=\mathbf{U}F(\mathbf{\Lambda})\mathbf{U}^{T}\mathbf{x}.$
    |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\mathbf{y}=\mathbf{U}F(\mathbf{\Lambda})\mathbf{U}^{T}\mathbf{x}.$
    |  |'
- en: Chebyshev Spectral Convolutional Neural Network (ChebNet) (Defferrard et al.,
    [2016](#bib.bib67)) extends this idea by using truncated Chebyshev polynomials
    to approximate the spectral transfer function. The Chebyshev polynomial is defined
    as $T_{0}(x)=1,~{}T_{1}(x)=x,~{}T_{k}(x)=2xT_{k-1}(x)-T_{k-2}(x)$, and the spectral
    transfer function $F(\mathbf{\Lambda})$ is approximated to the order of $K-1$
    as
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 切比雪夫谱卷积神经网络（ChebNet）（Defferrard et al., [2016](#bib.bib67)）通过使用截断的切比雪夫多项式来逼近谱传递函数，从而扩展了这一思想。切比雪夫多项式定义为
    $T_{0}(x)=1,~{}T_{1}(x)=x,~{}T_{k}(x)=2xT_{k-1}(x)-T_{k-2}(x)$，谱传递函数 $F(\mathbf{\Lambda})$
    逼近到 $K-1$ 的阶数为
- en: '| (4) |  | $F(\mathbf{\Lambda})=\sum_{k=0}^{K-1}\theta_{k}T_{k}(\tilde{\mathbf{\Lambda}}),$
    |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $F(\mathbf{\Lambda})=\sum_{k=0}^{K-1}\theta_{k}T_{k}(\tilde{\mathbf{\Lambda}}),$
    |  |'
- en: 'where the model parameters $\theta_{k},~{}k\in\{0,1,...,K-1\}$ are the Chebyshev
    coefficients, and $\tilde{\mathbf{\Lambda}}=2\mathbf{\Lambda}/\lambda_{max}-\mathbf{I}$
    is a diagonal matrix of scaled eigenvalues. Thus, the graph convolution can be
    written as:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中模型参数 $\theta_{k},~{}k\in\{0,1,...,K-1\}$ 是切比雪夫系数，$\tilde{\mathbf{\Lambda}}=2\mathbf{\Lambda}/\lambda_{max}-\mathbf{I}$
    是缩放特征值的对角矩阵。因此，图卷积可以写成：
- en: '| (5) |  | $\mathbf{g}*_{G}\mathbf{x}=\mathbf{U}F(\mathbf{\Lambda})\mathbf{U}^{T}\mathbf{x}=\mathbf{U}\sum_{k=0}^{K-1}\theta_{k}T_{k}(\tilde{\mathbf{\Lambda}})\mathbf{U}^{T}\mathbf{x}=\sum_{k=0}^{K-1}\theta_{k}T_{k}(\tilde{\mathbf{L}})\mathbf{x},$
    |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\mathbf{g}*_{G}\mathbf{x}=\mathbf{U}F(\mathbf{\Lambda})\mathbf{U}^{T}\mathbf{x}=\mathbf{U}\sum_{k=0}^{K-1}\theta_{k}T_{k}(\tilde{\mathbf{\Lambda}})\mathbf{U}^{T}\mathbf{x}=\sum_{k=0}^{K-1}\theta_{k}T_{k}(\tilde{\mathbf{L}})\mathbf{x},$
    |  |'
- en: where $\tilde{\mathbf{L}}=2\mathbf{L}/\lambda_{max}-\mathbf{I}$.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{\mathbf{L}}=2\mathbf{L}/\lambda_{max}-\mathbf{I}$。
- en: 'Graph Convolutional Network (GCN) (Kipf and Welling, [2016a](#bib.bib183))
    is proposed as the localized first-order approximation of ChebNet. Assuming $K=2$
    and $\lambda_{max}=2$, Eq. [5](#S3.E5 "In 3.1\. Spectral Graph Convolutions ‣
    3\. Graph Convolutions By Yusheng Zhao ‣ A Comprehensive Survey on Deep Graph
    Representation Learning") can be simplified as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积网络（GCN）（Kipf and Welling, [2016a](#bib.bib183)）被提出作为 ChebNet 的局部一阶近似。假设 $K=2$
    和 $\lambda_{max}=2$，则 Eq. [5](#S3.E5 "In 3.1\. Spectral Graph Convolutions ‣ 3\.
    Graph Convolutions By Yusheng Zhao ‣ A Comprehensive Survey on Deep Graph Representation
    Learning") 可以简化为：
- en: '| (6) |  | $\mathbf{g}*_{G}\mathbf{x}=\theta_{0}\mathbf{x}+\theta_{1}(\mathbf{L}-\mathbf{I})\mathbf{x}=\theta_{0}\mathbf{x}-\theta_{1}\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}\mathbf{x}.$
    |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\mathbf{g}*_{G}\mathbf{x}=\theta_{0}\mathbf{x}+\theta_{1}(\mathbf{L}-\mathbf{I})\mathbf{x}=\theta_{0}\mathbf{x}-\theta_{1}\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}\mathbf{x}.$
    |  |'
- en: 'To further constraint the number of parameters, we assume $\theta=\theta_{0}=-\theta_{1}$,
    which gives a simpler form of graph convolution:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步约束参数的数量，我们假设 $\theta=\theta_{0}=-\theta_{1}$，这给出了更简单的图卷积形式：
- en: '| (7) |  | $\mathbf{g}*_{\mathcal{G}}\mathbf{x}=\theta(\mathbf{I}+\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2})\mathbf{x}.$
    |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\mathbf{g}*_{\mathcal{G}}\mathbf{x}=\theta(\mathbf{I}+\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2})\mathbf{x}.$
    |  |'
- en: As $\mathbf{I}+\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$ now has the eigenvalues
    in the range of $[0,2]$ and repeatedly multiplying this matrix can lead to numerical
    instabilities, GCN empirically proposes a renormalization trick to solve this
    problem by using $\mathbf{\tilde{D}}^{-1/2}\mathbf{\tilde{A}}\mathbf{\tilde{D}}^{-1/2}$
    instead, where $\mathbf{\tilde{A}}=\mathbf{A}+\mathbf{I}$ and $\mathbf{\tilde{D}}_{ii}=\sum_{i}\mathbf{\tilde{A}}_{ij}$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\mathbf{I}+\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$ 现在的特征值范围在 $[0,2]$
    之间，并且反复乘以该矩阵可能会导致数值不稳定，GCN 实证上提出了一种重新归一化技巧来解决这个问题，使用 $\mathbf{\tilde{D}}^{-1/2}\mathbf{\tilde{A}}\mathbf{\tilde{D}}^{-1/2}$，其中
    $\mathbf{\tilde{A}}=\mathbf{A}+\mathbf{I}$ 和 $\mathbf{\tilde{D}}_{ii}=\sum_{i}\mathbf{\tilde{A}}_{ij}$。
- en: 'Allowing multi-channel signals and adding activation functions, the following
    formula is more commonly seen in literature:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 允许多通道信号并添加激活函数，以下公式在文献中更为常见：
- en: '| (8) |  | $\mathbf{Y}=\sigma((\mathbf{\tilde{D}}^{-1/2}\mathbf{\tilde{A}}\mathbf{\tilde{D}}^{-1/2})\mathbf{X}\mathbf{\Theta}),$
    |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\mathbf{Y}=\sigma((\mathbf{\tilde{D}}^{-1/2}\mathbf{\tilde{A}}\mathbf{\tilde{D}}^{-1/2})\mathbf{X}\mathbf{\Theta}),$
    |  |'
- en: where $\mathbf{X}$, $\mathbf{Y}$ have the same shape as in Eq. [2](#S3.E2 "In
    3.1\. Spectral Graph Convolutions ‣ 3\. Graph Convolutions By Yusheng Zhao ‣ A
    Comprehensive Survey on Deep Graph Representation Learning") and $\mathbf{\Theta}$
    is a $c_{in}\times c_{out}$ matrix as model’s parameters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{X}$ 和 $\mathbf{Y}$ 的形状与 Eq. [2](#S3.E2 "In 3.1\. Spectral Graph
    Convolutions ‣ 3\. Graph Convolutions By Yusheng Zhao ‣ A Comprehensive Survey
    on Deep Graph Representation Learning") 中相同，$\mathbf{\Theta}$ 是一个 $c_{in}\times
    c_{out}$ 矩阵，作为模型的参数。
- en: Apart from the aforementioned methods, other spectral graph convolutions have
    been proposed. Levie et al. (Levie et al., [2018](#bib.bib204)) propose CayleyNets
    that utilize Cayley Polynomials to equip the filters with the ability to detect
    narrow frequency bands. Liao et al. (Liao et al., [2018](#bib.bib225)) propose
    LanczosNets that employs Lanczos algorithm to construct low-rank approximation
    of graph Laplacian in order to improve the computation efficiency of graph convolutions.
    The proposed model is able to efficiently utilizes the multi-scale information
    in the graph data. Instead of using Graph Fourier Transform, Xu et al. (Xu et al.,
    [2019b](#bib.bib400)) propose Graph Wavelet Neural Network (GWNN) that use graph
    wavelet transform to avoid matrix eigendecomposition. Moreover, graph wavelets
    are sparse and localized, which provide good interpretations for the convolution
    operation. Zhu et al. (Zhu and Koniusz, [2021](#bib.bib467)) derive a Simple Spectral
    Graph Convolution (S²GC) from a modified Markov Diffusion Kernel, which achieves
    a trade-off between low-pass and high-pass filter bands.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的方法，其他的谱图卷积也已被提出。Levie 等人（Levie et al., [2018](#bib.bib204)）提出了利用 Cayley
    多项式的 CayleyNets，使得滤波器能够检测狭窄的频带。Liao 等人（Liao et al., [2018](#bib.bib225)）提出了 LanczosNets，该方法使用
    Lanczos 算法构造图拉普拉斯算子的低秩近似，以提高图卷积的计算效率。所提模型能够有效地利用图数据中的多尺度信息。Xu 等人（Xu et al., [2019b](#bib.bib400)）提出了图小波神经网络（GWNN），该网络使用图小波变换来避免矩阵特征分解。此外，图小波是稀疏和局部化的，这为卷积操作提供了良好的解释。Zhu
    等人（Zhu and Koniusz, [2021](#bib.bib467)）从修改后的 Markov 扩散核推导出了简单谱图卷积（S²GC），该方法在低通和高通滤波器带之间实现了权衡。
- en: 3.2\. Spatial Graph Convolutions
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 空间图卷积
- en: Inspired by the convolution on Euclidean data (*e.g.* images and texts), which
    applies data transformation on a small region, spatial graph convolutions compute
    the central node’s feature via transforming and aggregating its neighbors’ features.
    In this way, the graph structure is naturally embedded in the computation graph
    of node features. Moreover, the idea of sending one node’s feature to another
    node is similar to the message passing used in recurrent graph neural networks.
    In the following, we will introduce several seminal spatial graph convolutions
    as well as some recently proposed promising methods.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 受欧几里得数据（*例如* 图像和文本）上的卷积启发，该卷积在小区域内应用数据变换，空间图卷积通过变换和聚合邻居的特征来计算中心节点的特征。通过这种方式，图结构自然地嵌入到节点特征的计算图中。此外，将一个节点的特征传递到另一个节点的想法类似于在递归图神经网络中使用的消息传递。接下来，我们将介绍几个开创性的空间图卷积方法以及一些最近提出的有前景的方法。
- en: 'Spatial graph convolutions generally follows a three-step paradigm: message
    generation, feature aggregation and feature update. This can be mathematically
    written as:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 空间图卷积通常遵循一个三步骤范式：消息生成、特征聚合和特征更新。数学上可以写作：
- en: '| (9) |  | $\mathbf{y}_{i}=\text{UPDATE}\left(\mathbf{x}_{i},\text{AGGREGATE}\left(\{\text{MESSAGE}\left(\mathbf{x}_{i},\mathbf{x}_{j},\mathbf{e}_{ij}\right),~{}j\in\mathcal{N}(i)\}\right)\right),$
    |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\mathbf{y}_{i}=\text{UPDATE}\left(\mathbf{x}_{i},\text{AGGREGATE}\left(\{\text{MESSAGE}\left(\mathbf{x}_{i},\mathbf{x}_{j},\mathbf{e}_{ij}\right),~{}j\in\mathcal{N}(i)\}\right)\right),$
    |  |'
- en: where $\mathbf{x}_{i}$ and $\mathbf{y}_{i}$ is the input and output feature
    vector of node $i$, $\mathbf{e}_{ij}$ is the feature vector of the edge (or more
    generally, the relationship) between node $i$ and its neighbor node $j$, and $\mathcal{N}(i)$
    denotes the neighbor of node $i$, which could be more generally defined.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbf{x}_{i}$ 和 $\mathbf{y}_{i}$ 是节点 $i$ 的输入和输出特征向量，$\mathbf{e}_{ij}$
    是节点 $i$ 和其邻居节点 $j$ 之间的边（或更一般地说，关系）的特征向量，$\mathcal{N}(i)$ 表示节点 $i$ 的邻居，可以更一般地定义。
- en: 'In the previous subsection, we show the spectral interpretation of GCN (Kipf
    and Welling, [2016a](#bib.bib183)). The model also has its spatial interpretation,
    which can be mathematically written as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们展示了 GCN 的频谱解释（Kipf 和 Welling，[2016a](#bib.bib183)）。该模型也有其空间解释，数学上可以写作：
- en: '| (10) |  | $\mathbf{y}_{i}=\mathbf{\Theta}^{T}\sum_{j\in\mathcal{N}(i)\cup{i}}\frac{1}{\sqrt{\hat{d}_{i}\hat{d}_{j}}}\mathbf{x}_{j},$
    |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\mathbf{y}_{i}=\mathbf{\Theta}^{T}\sum_{j\in\mathcal{N}(i)\cup{i}}\frac{1}{\sqrt{\hat{d}_{i}\hat{d}_{j}}}\mathbf{x}_{j},$
    |  |'
- en: where $\hat{d}_{i}$ and $\hat{d}_{j}$ is the $i$-th and $j$-th row sums of $\hat{\mathbf{A}}$
    in Eq. [8](#S3.E8 "In 3.1\. Spectral Graph Convolutions ‣ 3\. Graph Convolutions
    By Yusheng Zhao ‣ A Comprehensive Survey on Deep Graph Representation Learning").
    For each node, the model takes a weighted sum of its neighbors’ features as well
    as its own feature and applies a linear transformation to obtain the result. In
    practice, multiple GCN layers are often stack together with non-linear functions
    after convolution to encode complex and hierarchical features. Nonetheless, Wu
    et al. (Wu et al., [2019b](#bib.bib384)) show that the model still achieves competitive
    results without non-linearity.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\hat{d}_{i}$ 和 $\hat{d}_{j}$ 是 $\hat{\mathbf{A}}$ 中第 $i$ 行和第 $j$ 行的和，如公式 [8](#S3.E8
    "在 3.1\. 频谱图卷积 ‣ 3\. 由 Yusheng Zhao 提出的图卷积 ‣ 深度图表示学习的全面调查") 所示。对于每个节点，模型对其邻居的特征以及自身的特征进行加权求和，并应用线性变换以获得结果。在实际应用中，多个
    GCN 层通常会与卷积后的非线性函数堆叠在一起，以编码复杂的层次特征。然而，Wu 等人（Wu et al., [2019b](#bib.bib384)）展示了该模型在没有非线性的情况下仍能取得具有竞争力的结果。
- en: Although GCN as well as other spectral graph convolutions achieve competitive
    results on a number of benchmarks, these methods assume the presence of all nodes
    in the graph and fall in the category of transductive learning. Hamilton et al. (Hamilton
    et al., [2017](#bib.bib129)) propose GraphSAGE that performs graph convolutions
    in inductive settings, when there are new nodes during inference (*e.g.* newcomers
    in the social network). For each node, the model samples its $K$-hop neighbors
    and uses $K$ graph convolutions to aggregate their features hierarchically. Furthermore,
    the use of sampling also reduces the computation when a node has too many neighbors.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 GCN 以及其他谱图卷积在多个基准测试中取得了竞争性的结果，这些方法假设图中存在所有节点，并且属于传导学习范畴。Hamilton 等人（Hamilton
    et al., [2017](#bib.bib129)）提出了 GraphSAGE，它在归纳设置中执行图卷积，当推理过程中出现新节点时（*例如* 社交网络中的新用户）。对于每个节点，模型会对其
    $K$ 跳邻居进行采样，并使用 $K$ 次图卷积来分层聚合它们的特征。此外，采样的使用也减少了节点拥有过多邻居时的计算量。
- en: 'The attention mechanism has been successfully used in natural language processing (Vaswani
    et al., [2017](#bib.bib351)), computer vision (Liu et al., [2021](#bib.bib236))
    and multi-modal tasks (Zhao et al., [2022a](#bib.bib456); Chen et al., [2021b](#bib.bib50);
    He et al., [2021](#bib.bib134); Yu et al., [2019](#bib.bib428)). Graph Attention
    Networks (GAT) (Veličković et al., [2017](#bib.bib352)) introduce the idea of
    attention to graphs. The attention mechanism uses an adaptive, feature-dependent
    weight (*i.e.* attention coefficient) to aggregate a set of features, which can
    be mathematically written as:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制已成功应用于自然语言处理（Vaswani et al., [2017](#bib.bib351)）、计算机视觉（Liu et al., [2021](#bib.bib236)）和多模态任务（Zhao
    et al., [2022a](#bib.bib456); Chen et al., [2021b](#bib.bib50); He et al., [2021](#bib.bib134);
    Yu et al., [2019](#bib.bib428)）。图注意力网络（GAT）（Veličković et al., [2017](#bib.bib352)）将注意力思想引入图中。注意力机制使用自适应的、特征依赖的权重（*即*
    注意力系数）来聚合一组特征，数学表达式为：
- en: '| (11) |  | $\alpha_{i,j}=\frac{\exp\left({\text{LeakyReLU}\left(\mathbf{a}^{T}[\mathbf{\Theta}\mathbf{x}_{i}&#124;&#124;\mathbf{\Theta}\mathbf{x}_{j}]\right)}\right)}{\sum_{k\in\mathcal{N}(i)\cup\{i\}}\exp\left({\text{LeakyReLU}\left(\mathbf{a}^{T}[\mathbf{\Theta}\mathbf{x}_{i}&#124;&#124;\mathbf{\Theta}\mathbf{x}_{j}]\right)}\right)},$
    |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\alpha_{i,j}=\frac{\exp\left({\text{LeakyReLU}\left(\mathbf{a}^{T}[\mathbf{\Theta}\mathbf{x}_{i}&#124;&#124;\mathbf{\Theta}\mathbf{x}_{j}]\right)}\right)}{\sum_{k\in\mathcal{N}(i)\cup\{i\}}\exp\left({\text{LeakyReLU}\left(\mathbf{a}^{T}[\mathbf{\Theta}\mathbf{x}_{i}&#124;&#124;\mathbf{\Theta}\mathbf{x}_{j}]\right)}\right)},$
    |  |'
- en: 'where $\alpha_{i,j}$ is the attention coefficient, $\mathbf{a}$ and $\mathbf{\Theta}$
    are model parameters, and $[\cdot||\cdot]$ means concatenation. After the $\alpha$s
    are obtained, the new features are computed as a weighted sum of input node features,
    which is:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha_{i,j}$ 是注意力系数，$\mathbf{a}$ 和 $\mathbf{\Theta}$ 是模型参数，$[\cdot||\cdot]$
    表示拼接。在获得 $\alpha$s 后，新特征被计算为输入节点特征的加权和，其表达式为：
- en: '| (12) |  | $\mathbf{y}_{i}=\alpha_{i,i}\mathbf{\Theta}\mathbf{x}_{i}+\sum_{j\in\mathcal{N}(i)}\alpha_{i,j}\mathbf{\Theta}\mathbf{x}_{j}.$
    |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\mathbf{y}_{i}=\alpha_{i,i}\mathbf{\Theta}\mathbf{x}_{i}+\sum_{j\in\mathcal{N}(i)}\alpha_{i,j}\mathbf{\Theta}\mathbf{x}_{j}.$
    |  |'
- en: 'Xu et al. (Xu et al., [2018a](#bib.bib404)) explores the representational limitations
    of graph neural networks. What they discover is that message passing networks
    like GCN (Kipf and Welling, [2016a](#bib.bib183)) and GraphSAGE (Hamilton et al.,
    [2017](#bib.bib129)) are incapable of distinguishing certain graph structures.
    To improve the representational power of graph neural networks, they propose Graph
    Isomorphism Network (GIN) that gives an adjustable weight to the central node
    feature, which can be mathematically written as:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人（Xu et al., [2018a](#bib.bib404)）探讨了图神经网络的表示能力限制。他们发现像 GCN（Kipf 和 Welling,
    [2016a](#bib.bib183)）和 GraphSAGE（Hamilton et al., [2017](#bib.bib129)）这样的消息传递网络无法区分某些图结构。为了提高图神经网络的表示能力，他们提出了图同构网络（GIN），为中心节点特征赋予可调权重，数学表达式为：
- en: '| (13) |  | $\mathbf{y}_{i}=\text{MLP}\left((1+\epsilon)\mathbf{x}_{i}+\sum_{j\in\mathcal{N}(i)}\mathbf{x}_{j}\right),$
    |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\mathbf{y}_{i}=\text{MLP}\left((1+\epsilon)\mathbf{x}_{i}+\sum_{j\in\mathcal{N}(i)}\mathbf{x}_{j}\right),$
    |  |'
- en: where $\epsilon$ is a learnable parameter.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\epsilon$ 是一个可学习的参数。
- en: More recently, efforts have been made to improve the representational power
    of graph neural networks. For example, Hu et al. (Hu et al., [2020d](#bib.bib144))
    propose GraphAIR that explicitly models the neighborhood interaction to better
    capture complex non-linear features. Specifically, they use Hadamard product between
    pairs of nodes in the neighborhood to model the quadratic terms of neighborhood
    interaction. Balcilar et al. (Balcilar et al., [2021](#bib.bib16)) propose GNNML3
    that break the limits of first-order Weisfeiler-Lehman test (1-WL) and reach third-order
    WL test (3-WL) experimentally. They also show that Hadamard product is required
    for the model to have more representational power than first-order Weisfeiler-Lehman
    test. Other elements in spatial graph convolutions are widely studied. For example,
    Corso et al. (Corso et al., [2020](#bib.bib65)) explore the aggregation operation
    in GNN and propose Principal Neighbourhood Aggregation (PNA) that uses multiple
    aggregators with degree-scalers. Tailor et al. (Tailor et al., [2022](#bib.bib341))
    explores the anisotropism and isotropism in the message passing process of graph
    neural networks, and propose Efficient Graph Convolution (EGC) that achieves promising
    results with reduced memory consumption due to isotropism.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，已经做出了一些努力以提高图神经网络的表现力。例如，Hu 等人（Hu et al., [2020d](#bib.bib144)）提出了 GraphAIR，该方法显式建模邻域交互，以更好地捕捉复杂的非线性特征。具体来说，他们使用邻域中节点对之间的
    Hadamard 积来建模邻域交互的二次项。Balcilar 等人（Balcilar et al., [2021](#bib.bib16)）提出了 GNNML3，实验上突破了一级
    Weisfeiler-Lehman 测试（1-WL）的限制，达到了三级 WL 测试（3-WL）。他们还表明，Hadamard 积是使模型具有比一级 Weisfeiler-Lehman
    测试更高表现力的必要条件。空间图卷积中的其他元素也得到了广泛研究。例如，Corso 等人（Corso et al., [2020](#bib.bib65)）探索了
    GNN 中的聚合操作，并提出了 Principal Neighbourhood Aggregation（PNA），该方法使用多个带有度量缩放因子的聚合器。Tailor
    等人（Tailor et al., [2022](#bib.bib341)）探讨了图神经网络消息传递过程中的各向异性和各向同性，并提出了 Efficient
    Graph Convolution（EGC），由于各向同性，该方法在减少内存消耗的同时取得了有希望的结果。
- en: 3.3\. Summary
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 总结
- en: 'This section introduces graph convolutions and we provide the summary as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了图卷积，并提供如下总结：
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Techniques. Graph convolutions mainly fall into two types, *i.e.* spectral graph
    convolutions and spatial graph convolutions. Spectral graph convolutions have
    solid mathematical foundations of Graph Signal Processing and therefore their
    operations have theoretical interpretations. Spatial graph convolutions are inspired
    by Recurrent Graph Neural Networks and their computation is simple and straightforward,
    as their computation graph is derived from the local graph structure. Generally,
    spatial graph convolutions are more common in applications.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术。图卷积主要分为两种类型，*即* 谱图卷积和空间图卷积。谱图卷积具有坚实的图信号处理数学基础，因此其操作有理论解释。空间图卷积受到递归图神经网络的启发，其计算简单直接，因为其计算图源自局部图结构。一般而言，空间图卷积在应用中更为常见。
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Challenges and Limitations. Despite the great success of graph convolutions,
    their performance is unsatisfactory in more complicated applications. On the one
    hand, the performance of graph convolutions rely heavily on the construction of
    the graph. Different construction of the graph might result in different performance
    of graph convolutions. On the other hand, graph convolutions are prone to over-smoothing
    when constructing very deep neural networks.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战与局限性。尽管图卷积取得了巨大成功，但在更复杂的应用中其性能仍不尽如人意。一方面，图卷积的性能在很大程度上依赖于图的构建。不同的图构建可能导致图卷积性能的差异。另一方面，图卷积在构建非常深的神经网络时容易出现过度平滑的现象。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future Works. In the future, we expect that more powerful graph convolutions
    are developed to mitigate the problem of over-smoothing and we also hope that
    techniques and methodologies in Graph Structure Learning (GSL) can help learn
    more meaningful graph structure to benefit the performance of graph convolutions.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来工作。未来，我们期望开发出更强大的图卷积方法，以缓解过度平滑的问题，同时也希望图结构学习（GSL）中的技术和方法能够帮助学习到更有意义的图结构，从而提升图卷积的性能。
- en: 4\. Graph Kernel Neural Networks By Qingqing Long
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 图核神经网络 由 Qingqing Long
- en: Graph kernels (GKs) are historically the most widely used technique on graph
    analyzing and representation tasks (Shawe-Taylor et al., [2004](#bib.bib314);
    Gärtner et al., [2003](#bib.bib108); Zhou et al., [2020](#bib.bib464)). However,
    traditional graph kernels rely on hand-crafted patterns or domain knowledge on
    specific tasks(Kriege et al., [2020](#bib.bib195); Shervashidze et al., [2009](#bib.bib317)).
    Over the years, an amount of research has been conducted on graph kernel neural
    networks (GKNNs), which has yielded promising results. Researchers have explored
    various aspects of GKNNs, including their theoretical foundations, algorithmic
    design, and practical applications. These efforts have led to the development
    of a wide range of GKNN-based models and methods that can be used for graph analysis
    and representation tasks, such as node classification, link prediction, and graph
    clustering (Chen et al., [2020c](#bib.bib45); Long et al., [2021a](#bib.bib238),
    [b](#bib.bib239)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图核（GKs）在图分析和表示任务中历史上是最广泛使用的技术（Shawe-Taylor 等，[2004](#bib.bib314)；Gärtner 等，[2003](#bib.bib108)；Zhou
    等，[2020](#bib.bib464)）。然而，传统的图核依赖于手工制作的模式或特定任务的领域知识（Kriege 等，[2020](#bib.bib195)；Shervashidze
    等，[2009](#bib.bib317)）。多年来，关于图核神经网络（GKNN）的研究已经取得了令人鼓舞的结果。研究人员探讨了 GKNN 的各个方面，包括其理论基础、算法设计和实际应用。这些努力导致了多种基于
    GKNN 的模型和方法的开发，这些模型和方法可以用于图分析和表示任务，如节点分类、链接预测和图聚类（Chen 等，[2020c](#bib.bib45)；Long
    等，[2021a](#bib.bib238)，[b](#bib.bib239)）。
- en: The success of GKNNs can be attributed to their ability to leverage the strengths
    of both graph kernels and neural networks. By using kernel functions to measure
    similarity between graphs, GKNNs can capture the structural properties of graphs,
    while the use of neural networks enables them to learn more complex and abstract
    representations of graphs. This combination of techniques allows GKNNs to achieve
    state-of-the-art performance on a wide range of graph-related tasks.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: GKNN 的成功可以归因于其利用图核和神经网络的优势。通过使用核函数来测量图之间的相似性，GKNN 可以捕捉图的结构属性，而神经网络的使用使其能够学习更复杂和抽象的图表示。这种技术的结合使
    GKNN 能在广泛的图相关任务中达到最先进的性能。
- en: In this section, we begin with introducing the most representative traditional
    graph kernels. Then we summarize the basic framework for combining GNNs and graph
    kernels. Finally, we categorize the popular graph kernel Neural networks into
    several categories and compare their differences.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍最具代表性的传统图核。然后，我们总结了结合 GNN 和图核的基本框架。最后，我们将流行的图核神经网络分为几类，并比较它们的差异。
- en: 4.1\. Graph Kernels
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 图核
- en: Graph kernels generally evaluate pairwise similarity between nodes or graphs
    by decomposing them into basic structural units. Random walks (Kang et al., [2012](#bib.bib176)),
    subtrees (Shervashidze et al., [2011b](#bib.bib316)), shortest paths (Borgwardt
    and Kriegel, [2005](#bib.bib25)) and graphlets (Shervashidze et al., [2009](#bib.bib317))
    are representative categories.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图核通常通过将节点或图分解为基本结构单元来评估节点或图之间的成对相似性。随机游走（Kang 等，[2012](#bib.bib176)）、子树（Shervashidze
    等，[2011b](#bib.bib316)）、最短路径（Borgwardt 和 Kriegel，[2005](#bib.bib25)）和图小物件（Shervashidze
    等，[2009](#bib.bib317)）是代表性的类别。
- en: 'Given two graphs $G_{1}=(V_{1},E_{1},X_{1})$ and $G_{2}=(V_{2},E_{2},X_{2})$,
    a graph kernel function $K(G_{1},G_{2})$ measures the similarity between $G_{1}$
    and $G_{2}$ through the following formula:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个图 $G_{1}=(V_{1},E_{1},X_{1})$ 和 $G_{2}=(V_{2},E_{2},X_{2})$，图核函数 $K(G_{1},G_{2})$
    通过以下公式来测量 $G_{1}$ 和 $G_{2}$ 之间的相似性：
- en: '| (14) |  | $K(G_{1},G_{2})=\sum_{u_{1}\in V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{base}\left(l_{G_{1}}(u_{1}),l_{G_{2}}(u_{2})\right),$
    |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $K(G_{1},G_{2})=\sum_{u_{1}\in V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{base}\left(l_{G_{1}}(u_{1}),l_{G_{2}}(u_{2})\right),$
    |  |'
- en: 'where $l_{G}(u)$ denotes a set of local substructures centered at node $u$
    in graph $G$, and $\kappa_{base}$ is a base kernel measuring the similarity between
    the two sets of substructures. For simplicity, we may rewrite Eqn. [14](#S4.E14
    "In 4.1\. Graph Kernels ‣ 4\. Graph Kernel Neural Networks By Qingqing Long ‣
    A Comprehensive Survey on Deep Graph Representation Learning") as:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l_{G}(u)$ 表示以节点 $u$ 为中心的图 $G$ 中的一组局部子结构，$\kappa_{base}$ 是一个基本核，用于测量两组子结构之间的相似性。为简便起见，我们可以将公式
    [14](#S4.E14 "在 4.1\. 图核 ‣ 4\. 图核神经网络由 Qingqing Long ‣ 关于深度图表示学习的综合调查") 重写为：
- en: '| (15) |  | $K(G_{1},G_{2})=\sum_{u_{1}\in V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{base}(u_{1},u_{2}),$
    |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $K(G_{1},G_{2})=\sum_{u_{1}\in V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{base}(u_{1},u_{2}),$
    |  |'
- en: the uppercase letter $K(G_{1},G_{2})$ is denoted as graph kernels, $\kappa(u_{1},u_{2})$
    is denoted as node kernels, and lowercase $k(x,y)$ is denoted as general kernel
    functions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 大写字母 $K(G_{1},G_{2})$ 表示图核，$\kappa(u_{1},u_{2})$ 表示节点核，小写字母 $k(x,y)$ 表示一般的核函数。
- en: The kernel mapping of a kernel $\psi$ maps a data point into its corresponding
    Reproducing Kernel Hilbert Space (RKHS) $\mathcal{H}$. Specifically, given a kernel
    $k_{*}(\cdot,\cdot)$, its kernel mapping $\psi_{*}$ can be formulized as,
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 核 $\psi$ 的核映射将数据点映射到其对应的再生核希尔伯特空间（RKHS）$\mathcal{H}$ 中。具体来说，给定一个核 $k_{*}(\cdot,\cdot)$，其核映射
    $\psi_{*}$ 可以表示为，
- en: '| (16) |  | $\forall x_{1},x_{2},k_{*}(x_{1},x_{2})=\langle\psi_{*}(x_{1}),\psi_{*}(x_{2})\rangle_{\mathcal{H}_{*}},$
    |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\forall x_{1},x_{2},k_{*}(x_{1},x_{2})=\langle\psi_{*}(x_{1}),\psi_{*}(x_{2})\rangle_{\mathcal{H}_{*}},$
    |  |'
- en: where $\mathcal{H}_{*}$ is the RKHS of $k_{*}(\cdot,\cdot)$.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{H}_{*}$ 是 $k_{*}(\cdot,\cdot)$ 的 RKHS。
- en: We introduce several representative and popular used graph kernels below.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面介绍几个具有代表性和广泛使用的图核。
- en: Walk and Path Kernels. A $l$-walk kernel $K_{walk}^{(l)}$ compares all length
    $l$ walks starting from each node in two graphs $G_{1},G_{2}$,
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤和路径核。一个 $l$-步核 $K_{walk}^{(l)}$ 比较两个图 $G_{1}$ 和 $G_{2}$ 中每个节点出发的所有长度为 $l$
    的路径，
- en: '| (17) |  | $\displaystyle\kappa_{walk}^{(l)}(u_{1},u_{2})$ | $\displaystyle=\sum_{w_{1}\in\mathcal{W}^{l}(G_{1},u_{1})}\sum_{w_{2}\in\mathcal{W}^{l}(G_{2},u_{2})}\delta(X_{1}(w_{1}),X_{2}(w_{2})),$
    |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $\displaystyle\kappa_{walk}^{(l)}(u_{1},u_{2})$ | $\displaystyle=\sum_{w_{1}\in\mathcal{W}^{l}(G_{1},u_{1})}\sum_{w_{2}\in\mathcal{W}^{l}(G_{2},u_{2})}\delta(X_{1}(w_{1}),X_{2}(w_{2})),$
    |  |'
- en: '|  | $\displaystyle K_{walk}^{(l)}(G_{1},G_{2})$ | $\displaystyle=\sum_{u_{1}\in
    V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{walk}^{(l)}(u_{1},u_{2}).$ |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K_{walk}^{(l)}(G_{1},G_{2})$ | $\displaystyle=\sum_{u_{1}\in
    V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{walk}^{(l)}(u_{1},u_{2}).$ |  |'
- en: Substituting $\mathcal{W}$ with $\mathcal{P}$ is able to get the $l$-path kernel.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将 $\mathcal{W}$ 替换为 $\mathcal{P}$ 可以得到 $l$-路径核。
- en: Subtree Kernels. The WL subtree kernel is the most popular one in subtree kernels.
    It is a finite-depth kernel variant of the 1-WL test. The WL subtree kernel with
    depth $l$, $K_{WL}^{(l)}$ compares all subtrees with depth $\leq l$ rooted at
    each node.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 子树核。WL 子树核是子树核中最流行的一个。它是 1-WL 测试的有限深度核变体。深度为 $l$ 的 WL 子树核 $K_{WL}^{(l)}$ 比较每个节点处深度
    $\leq l$ 的所有子树。
- en: '| (18) |  | $\displaystyle\kappa_{subtree}^{(i)}(u_{1},u_{2})$ | $\displaystyle=\sum_{t_{1}\in\mathcal{T}^{i}(G_{1},u_{2})}\sum_{t_{2}\in\mathcal{T}^{i}(G_{2},u_{2})}\delta(t_{1},t_{2}),$
    |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $\displaystyle\kappa_{subtree}^{(i)}(u_{1},u_{2})$ | $\displaystyle=\sum_{t_{1}\in\mathcal{T}^{i}(G_{1},u_{2})}\sum_{t_{2}\in\mathcal{T}^{i}(G_{2},u_{2})}\delta(t_{1},t_{2}),$
    |  |'
- en: '|  | $\displaystyle K_{subtree}^{(i)}(G_{1},G_{2})$ | $\displaystyle=\sum_{u_{1}\in
    V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{subtree}^{(i)}(u_{1},u_{2}),$ |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K_{subtree}^{(i)}(G_{1},G_{2})$ | $\displaystyle=\sum_{u_{1}\in
    V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{subtree}^{(i)}(u_{1},u_{2}),$ |  |'
- en: '|  | $\displaystyle K_{WL}^{(l)}(G_{1},G_{2})$ | $\displaystyle=\sum_{i=0}^{l}K_{subtree}^{(i)}(G_{1},G_{2}),$
    |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K_{WL}^{(l)}(G_{1},G_{2})$ | $\displaystyle=\sum_{i=0}^{l}K_{subtree}^{(i)}(G_{1},G_{2}),$
    |  |'
- en: where $t\in\mathcal{T}^{(i)}(G,u)$ denotes a subtree of depth $i$ rooted at
    $u$ in $G$.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t\in\mathcal{T}^{(i)}(G,u)$ 表示 $G$ 中以 $u$ 为根的深度为 $i$ 的子树。
- en: 4.2\. General Framework of GKNNs
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. GKNNs 的一般框架
- en: In this section, we summarize the general framework of GKNNs. For the first
    step, a kernel that measures similarities of heterogeneous features from heterogeneous
    nodes and edges $(u_{1},e_{\cdot,u_{2}})$ and $(u_{2},e_{\cdot,u_{2}})$ should
    be defined. Take the inner product of neighbor tensors as an example, its neighbor
    kernel is defined as follows,
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了 GKNNs 的一般框架。第一步，应定义一个度量异质特征相似性的核，这些特征来自异质节点和边 $(u_{1},e_{\cdot,u_{2}})$
    和 $(u_{2},e_{\cdot,u_{2}})$。以邻居张量的内积为例，其邻居核定义如下，
- en: '|  | $\kappa((u_{1},e_{\cdot,u_{1}}),(u_{2},e_{\cdot,u_{2}}))=\langle f(u_{1}),f(u_{2})\rangle\cdot\langle
    f(e_{\cdot,u_{1}}),f(e_{\cdot,u_{2}})\rangle.$ |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\kappa((u_{1},e_{\cdot,u_{1}}),(u_{2},e_{\cdot,u_{2}}))=\langle f(u_{1}),f(u_{2})\rangle\cdot\langle
    f(e_{\cdot,u_{1}}),f(e_{\cdot,u_{2}})\rangle.$ |  |'
- en: Based on the neighbor kernel, a kernel with two $l$-hop neighborhoods for central
    node $u_{1}$ and $u_{2}$ can be defined as $K^{(l)}(u_{1},u_{2})=$
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 基于邻居核，中央节点 $u_{1}$ 和 $u_{2}$ 的一个具有两个 $l$-跳邻域的核可以定义为 $K^{(l)}(u_{1},u_{2})=$
- en: '| (19) |  | $\left\{\begin{aligned} &amp;\langle f(u_{1}),f(u_{2})\rangle&amp;\
    l=0\\ &amp;\langle f(u_{1}),f(u_{2})\rangle\cdot\sum_{v_{1}\in N(u_{1})}\sum_{v_{2}\in
    N(u_{2})}K^{(l-1)}(v_{1},v_{2})\cdot\langle f(e_{\cdot,v_{1}}),f(e_{\cdot,v_{2}})\rangle&amp;\
    l>0\end{aligned}\right.,$ |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $\left\{\begin{aligned} &\langle f(u_{1}),f(u_{2})\rangle&\ l=0\\
    &\langle f(u_{1}),f(u_{2})\rangle\cdot\sum_{v_{1}\in N(u_{1})}\sum_{v_{2}\in N(u_{2})}K^{(l-1)}(v_{1},v_{2})\cdot\langle
    f(e_{\cdot,v_{1}}),f(e_{\cdot,v_{2}})\rangle&\ l>0\end{aligned}\right.,$ |  |'
- en: By regarding the lower-hop kernel $\kappa^{(l-1)}(u_{1},u_{2})$, as the inner
    product of the $(l-1)$-th hidden representations of $u_{1}$ and $u_{2}$. Furthermore,
    by recursively applying the neighborhood kernel, the $l$-hop graph kernel can
    be derived as
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将较低跳跃核 $\kappa^{(l-1)}(u_{1},u_{2})$ 视为 $u_{1}$ 和 $u_{2}$ 的 $(l-1)$-th 隐藏表示的内积。此外，通过递归应用邻域核，可以推导出
    $l$-跳图核如下
- en: '| (20) |  | $K^{l}(G_{1},G_{2})=\sum_{\boldsymbol{w}_{1}\in\mathcal{W}^{l}(G_{1})}\sum_{\boldsymbol{w}_{2}\in\mathcal{W}^{l}(G_{2})}\left(\prod_{i=0}^{l-1}\langle
    f(\boldsymbol{w}_{1}^{(i)}),f(\boldsymbol{w}_{2}^{(i)})\rangle\times\prod_{i=0}^{l-2}\langle
    f(e_{\boldsymbol{w}_{1}^{(i)},\boldsymbol{w}_{1}^{(i+1)}}),f(e_{\boldsymbol{w}_{2}^{(i)},\boldsymbol{w}_{2}^{(i+1)}})\rangle\right),$
    |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $K^{l}(G_{1},G_{2})=\sum_{\boldsymbol{w}_{1}\in\mathcal{W}^{l}(G_{1})}\sum_{\boldsymbol{w}_{2}\in\mathcal{W}^{l}(G_{2})}\left(\prod_{i=0}^{l-1}\langle
    f(\boldsymbol{w}_{1}^{(i)}),f(\boldsymbol{w}_{2}^{(i)})\rangle\times\prod_{i=0}^{l-2}\langle
    f(e_{\boldsymbol{w}_{1}^{(i)},\boldsymbol{w}_{1}^{(i+1)}}),f(e_{\boldsymbol{w}_{2}^{(i)},\boldsymbol{w}_{2}^{(i+1)}})\rangle\right),$
    |  |'
- en: where $\mathcal{W}^{l}(G)$ denotes the set of all walk sequences with length
    $l$ in graph $G$, and $\boldsymbol{w}_{1}^{(i)}$ denotes the $i$-th node in sequence
    $\boldsymbol{w}_{1}$.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{W}^{l}(G)$ 表示图 $G$ 中所有长度为 $l$ 的步长序列的集合，$\boldsymbol{w}_{1}^{(i)}$
    表示序列 $\boldsymbol{w}_{1}$ 中的第 $i$ 个节点。
- en: As shown in Eqn. ([16](#S4.E16 "In 4.1\. Graph Kernels ‣ 4\. Graph Kernel Neural
    Networks By Qingqing Long ‣ A Comprehensive Survey on Deep Graph Representation
    Learning")), kernel methods implicitly perform projections from original data
    spaces to their RKHS $\mathcal{H}$. Hence, as GNNs also project nodes or graphs
    into vector spaces, connections have been established between GKs and GNNs through
    the kernel mappings. And several works conducted research on the connections (Lei
    et al., [2017](#bib.bib203); Williams and Seeger, [2001](#bib.bib382)), and found
    some foundation conclusions. Take the basic rule introduced in (Lei et al., [2017](#bib.bib203))
    as an example, the proposed graph kernel in Eqn. ([14](#S4.E14 "In 4.1\. Graph
    Kernels ‣ 4\. Graph Kernel Neural Networks By Qingqing Long ‣ A Comprehensive
    Survey on Deep Graph Representation Learning")) can be derived as the general
    formulas,
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如公式 ([16](#S4.E16 "In 4.1\. Graph Kernels ‣ 4\. Graph Kernel Neural Networks
    By Qingqing Long ‣ A Comprehensive Survey on Deep Graph Representation Learning"))
    所示，核方法隐式地将原始数据空间投影到它们的 RKHS $\mathcal{H}$ 中。因此，既然 GNNs 也将节点或图投影到向量空间，通过核映射已建立了
    GKs 和 GNNs 之间的联系。一些研究工作（Lei 等人，[2017](#bib.bib203)；Williams 和 Seeger，[2001](#bib.bib382)）也探讨了这些联系，并得出了一些基础结论。以
    (Lei 等人，[2017](#bib.bib203)) 中介绍的基本规则为例，公式 ([14](#S4.E14 "In 4.1\. Graph Kernels
    ‣ 4\. Graph Kernel Neural Networks By Qingqing Long ‣ A Comprehensive Survey on
    Deep Graph Representation Learning")) 中提出的图核可以推导为一般公式，
- en: '| (21) |  | $\displaystyle h^{(0)}(v)=$ | $\displaystyle\boldsymbol{W}^{(0)}_{t_{V}(v)}f(v),$
    |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $\displaystyle h^{(0)}(v)=$ | $\displaystyle\boldsymbol{W}^{(0)}_{t_{V}(v)}f(v),$
    |  |'
- en: '|  | $\displaystyle h^{(l)}(v)=$ | $\displaystyle\boldsymbol{W}^{(l)}_{t_{V}(v)}f(v)\odot\sum_{u\in
    N(v)}(\boldsymbol{U}_{t_{V}(v)}^{(l)}h^{(l-1)}(u)\odot\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}f(e_{u,v})),\qquad
    1<l\leq L,$ |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{(l)}(v)=$ | $\displaystyle\boldsymbol{W}^{(l)}_{t_{V}(v)}f(v)\odot\sum_{u\in
    N(v)}(\boldsymbol{U}_{t_{V}(v)}^{(l)}h^{(l-1)}(u)\odot\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}f(e_{u,v})),\qquad
    1<l\leq L,$ |  |'
- en: where $\odot$ is the element-wise product and $h^{(l)}(v)$ is the cell state
    vector of node v. The parameter matrices $\boldsymbol{W}^{(l)}_{t_{V}(v)}$, $\boldsymbol{U}_{t_{V}(v)}^{(l)}$
    and $\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}$ are learnable parameters related to
    types of nodes and edges.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\odot$ 是逐元素乘积，$h^{(l)}(v)$ 是节点 v 的细胞状态向量。参数矩阵 $\boldsymbol{W}^{(l)}_{t_{V}(v)}$、$\boldsymbol{U}_{t_{V}(v)}^{(l)}$
    和 $\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}$ 是与节点和边类型相关的可学习参数。
- en: Then mean embeddings of all nodes are usually used to represent the graph-level
    embedding, let $|G_{i}|$ denote the number of nodes in the $i$-th graph, then
    the graph-level embeddings are generated as,
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，所有节点的平均嵌入通常用于表示图级别的嵌入，令 $|G_{i}|$ 表示第 $i$ 个图中的节点数量，则图级别的嵌入生成如下，
- en: '| (22) |  | $\Phi(G_{i})=\sum_{u\in G_{i}}\frac{1}{&#124;G_{i}&#124;}h^{(L)}(v).$
    |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $\Phi(G_{i})=\sum_{u\in G_{i}}\frac{1}{\vert G_{i} \vert}h^{(L)}(v).$
    |  |'
- en: For semi-supervised multiclass classification, the cross entropy is used as
    the objective function over all training examples (Cao et al., [2016](#bib.bib38),
    [2015](#bib.bib37)),
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于半监督多类分类，交叉熵被用作所有训练样本的目标函数 (Cao 等，[2016](#bib.bib38)，[2015](#bib.bib37))，
- en: '| (23) |  | $\mathcal{L}=-\sum_{l\in\mathcal{Y}_{L}}\sum_{f=1}^{F}Y_{lf}\ln{Z_{lf}},$
    |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| (23) |  | $\mathcal{L}=-\sum_{l\in\mathcal{Y}_{L}}\sum_{f=1}^{F}Y_{lf}\ln{Z_{lf}},$
    |  |'
- en: where $\mathcal{Y}_{L}$ is the set of node indices which have labels in node
    classification tasks, or set of graph indices in graph classification tasks. $Z_{lf}$
    denotes the prediction of labels, which are outputs of a linear layer with an
    activation function, inputing $h^{(l)}(v)$ in node classification task, and $\Phi(G_{i})$
    in graph classification task.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{Y}_{L}$ 是节点分类任务中具有标签的节点索引集合，或图分类任务中图索引的集合。$Z_{lf}$ 表示标签的预测，它们是具有激活函数的线性层的输出，输入为节点分类任务中的
    $h^{(l)}(v)$ 和图分类任务中的 $\Phi(G_{i})$。
- en: 4.3\. Popular Variants of GKNNs
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. GKNNs 的流行变体
- en: We summarize the popular variants of GKNNs and compare their differences in
    Table [2](#S4.T2 "Table 2 ‣ 4.3\. Popular Variants of GKNNs ‣ 4\. Graph Kernel
    Neural Networks By Qingqing Long ‣ A Comprehensive Survey on Deep Graph Representation
    Learning"). Specifically, we conclude their basic graph kernels, whether designed
    for heterogeneous graphs, and experimental datasets, etc. As the original designed
    graph-kernel based GNNs have high complexity, they usually by acceleration strategies,
    such as sampling strategies, simplification and approximation, etc. In this section,
    We select four typical and popular GKNNs to introduce their well-designed graph
    kernels and corresponding GNN frameworks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了 GKNNs 的流行变体，并在表 [2](#S4.T2 "Table 2 ‣ 4.3\. Popular Variants of GKNNs
    ‣ 4\. Graph Kernel Neural Networks By Qingqing Long ‣ A Comprehensive Survey on
    Deep Graph Representation Learning") 中比较了它们的差异。具体来说，我们总结了它们的基本图核，是否设计用于异构图，以及实验数据集等。由于原始设计的基于图核的
    GNNs 具有高复杂性，它们通常通过加速策略，如采样策略、简化和近似等，来进行加速。在本节中，我们选择了四个典型且流行的 GKNNs 来介绍它们精心设计的图核和相应的
    GNN 框架。
- en: Table 2\. Summary of popular GKNNs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2. 流行 GKNNs 的总结。
- en: '| Method | Type | Related GK | Adaptive | Datasets |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类型 | 相关 GK | 自适应 | 数据集 |'
- en: '| $k$-GNN  (Morris et al., [2019](#bib.bib265)) | Isomorphic | WL Subtree |
    ✓ | Biochemical Network |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| $k$-GNN (Morris 等，[2019](#bib.bib265)) | 同构 | WL 子树 | ✓ | 生物化学网络 |'
- en: '| RetGK (Zhang et al., [2018c](#bib.bib450)) | Isomorphic | Random Walk | ✓
    | Biochemical Network, Social Network |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| RetGK (Zhang 等，[2018c](#bib.bib450)) | 同构 | 随机游走 | ✓ | 生物化学网络，社交网络 |'
- en: '| GNTK (Du et al., [2019](#bib.bib78)) | Isomorphic | Neural Tangent Kernel
    |  | Biochemical Network, Social Network |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| GNTK (Du 等，[2019](#bib.bib78)) | 同构 | 神经切线核 |  | 生物化学网络，社交网络 |'
- en: '| DDGK (Al-Rfou et al., [2019](#bib.bib5)) | Isomorphic | Random Walk |  |
    Biochemical Network |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| DDGK (Al-Rfou 等，[2019](#bib.bib5)) | 同构 | 随机游走 |  | 生物化学网络 |'
- en: '| GCKN (Chen et al., [2020c](#bib.bib45)) | Isomorphic | Random Walk | ✓ |
    Biochemical Network, Social Network |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| GCKN (Chen 等，[2020c](#bib.bib45)) | 同构 | 随机游走 | ✓ | 生物化学网络，社交网络 |'
- en: '| GSKN (Long et al., [2021a](#bib.bib238)) | Isomorphic | Random Walk, Anonymous
    Walk | ✓ | Biochemical Network, Social Network |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| GSKN (Long 等，[2021a](#bib.bib238)) | 同构 | 随机游走，无名游走 | ✓ | 生物化学网络，社交网络 |'
- en: '| GCN-LASE (Li et al., [2019c](#bib.bib223)) | Heterogeneous | Random Walk
    | - | Social Network Academic Network |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| GCN-LASE (Li 等，[2019c](#bib.bib223)) | 异构 | 随机游走 | - | 社交网络 学术网络 |'
- en: '| HGK-GNN (Long et al., [2021b](#bib.bib239)) | Heterogeneous | Random Walk
    | ✓ | Social Network Academic Network |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| HGK-GNN (Long 等，[2021b](#bib.bib239)) | 异构 | 随机游走 | ✓ | 社交网络 学术网络 |'
- en: $k$-dimensional Graph Neural Networks (Morris et al., [2019](#bib.bib265)) ($k$-GNN)
    is the pioneer of GKNNs, it incorporates the WL-subtree graph kernel and graph
    neural networks. For better scalability, the paper considers a set-based version
    of the $k$-WL. Let $h^{(l)}_{k}(s)$ and $h^{(l)}_{k,L}(s)$ denote the global and
    local hidden representation for node $s$ in the $l$-th layer respectively. $k$-GNN
    defined the end-to-end hierarchical trainable framework as,
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: $k$-维图神经网络 (Morris 等，[2019](#bib.bib265)) ($k$-GNN) 是 GKNNs 的先锋，它结合了 WL-子树图核和图神经网络。为了更好的可扩展性，论文考虑了
    $k$-WL 的基于集合的版本。令 $h^{(l)}_{k}(s)$ 和 $h^{(l)}_{k,L}(s)$ 分别表示 $l$-层中节点 $s$ 的全局和局部隐藏表示。$k$-GNN
    定义了端到端的分层可训练框架，如下所示，
- en: '| (24) |  | $\displaystyle h^{(0)}_{k}(s)=$ | $\displaystyle\sigma\left(\left[h^{iso}(s),\sum_{u\in
    s}h^{(T_{k}-1)}(u)\right]\cdot\boldsymbol{W}^{(0)}_{k-1}\right),$ |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| (24) |  | $\displaystyle h^{(0)}_{k}(s)=$ | $\displaystyle\sigma\left(\left[h^{iso}(s),\sum_{u\in
    s}h^{(T_{k}-1)}(u)\right]\cdot\boldsymbol{W}^{(0)}_{k-1}\right),$ |  |'
- en: '|  | $\displaystyle h^{(l)}_{k}(s)=$ | $\displaystyle\sigma\left(h^{(l-1)}_{k}\cdot\boldsymbol{W}_{1}^{(l)}+\sum_{u\in
    N_{L}(s)\cup N_{G}(s)}h^{(l-1)}_{k}(u)\cdot\boldsymbol{W}_{2}^{(l)}\right),\qquad
    1<l\leq L,$ |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{(l)}_{k}(s)=$ | $\displaystyle\sigma\left(h^{(l-1)}_{k}\cdot\boldsymbol{W}_{1}^{(l)}+\sum_{u\in
    N_{L}(s)\cup N_{G}(s)}h^{(l-1)}_{k}(u)\cdot\boldsymbol{W}_{2}^{(l)}\right),\qquad
    1<l\leq L,$ |  |'
- en: '|  | $\displaystyle h^{(l)}_{k,L}(s)=$ | $\displaystyle\sigma\left(h^{(l-1)}_{k,L}(s)\cdot\boldsymbol{W}_{1}^{(l)}+\sum_{u\in
    N_{L}(s)}h^{(l-1)}_{k,L}(u)\cdot\boldsymbol{W}_{2}^{(l)}\right),\qquad 1<l\leq
    L.$ |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{(l)}_{k,L}(s)=$ | $\displaystyle\sigma\left(h^{(l-1)}_{k,L}(s)\cdot\boldsymbol{W}_{1}^{(l)}+\sum_{u\in
    N_{L}(s)}h^{(l-1)}_{k,L}(u)\cdot\boldsymbol{W}_{2}^{(l)}\right),\qquad 1<l\leq
    L.$ |  |'
- en: where $h^{iso}(s)$ is the one-hot encoding of the isomorphism type of $G[s]$,
    $N_{L}(s)$ is the local neighborhood, $N_{G}(s)$ is the global neighborhood, $h^{(l)}_{k,L}(s)$
    is designed for better scalability and running speed, and $h^{(l)}_{k}(s)$ has
    better performance due to its larger neighbor sets.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$h^{iso}(s)$是$G[s]$同构类型的一次性编码，$N_{L}(s)$是局部邻域，$N_{G}(s)$是全局邻域，$h^{(l)}_{k,L}(s)$旨在提高可扩展性和运行速度，而$h^{(l)}_{k}(s)$由于其更大的邻域集合具有更好的性能。
- en: Graph Convolutional Kernel Networks (Chen et al., [2020c](#bib.bib45)) (GCKN).
    GCKN is a representative random walk and path-based GKNN. It The Gaussian kernel
    $k$ can be written as,
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积核网络（Chen et al., [2020c](#bib.bib45)）（GCKN）。GCKN是一种代表性的随机游走和路径基础的GKNN。高斯核$k$可以写成：
- en: '| (25) |  | $k(z_{1},z_{2})=e^{-\frac{\alpha_{1}}{2}&#124;&#124;z_{1}-z_{2}&#124;&#124;^{2}}=e^{\alpha(z_{1}^{T}z_{2}-k-1)}=\sigma(z_{1}^{T}z_{2}),$
    |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $k(z_{1},z_{2})=e^{-\frac{\alpha_{1}}{2}&#124;&#124;z_{1}-z_{2}&#124;&#124;^{2}}=e^{\alpha(z_{1}^{T}z_{2}-k-1)}=\sigma(z_{1}^{T}z_{2}),$
    |  |'
- en: then the GNN architecture can be written as
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，GNN架构可以写作：
- en: '| (26) |  | $\displaystyle h^{(l)}(u)=$ | $\displaystyle\sum_{z\in}K(z_{1},z_{2})\cdot\sigma\left(Z^{T}h^{(l-1)}(p)\right)$
    |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| (26) |  | $\displaystyle h^{(l)}(u)=$ | $\displaystyle\sum_{z\in}K(z_{1},z_{2})\cdot\sigma\left(Z^{T}h^{(l-1)}(p)\right)$
    |  |'
- en: '|  | $\displaystyle=$ | $\displaystyle\sigma\left(ZZ^{T}\right)^{-\frac{1}{2}}\cdot\sum_{p\in\mathcal{P}_{k}(G,u)}\sigma\left(Z^{T}h^{(l-1)}(p)\right),$
    |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ | $\displaystyle\sigma\left(ZZ^{T}\right)^{-\frac{1}{2}}\cdot\sum_{p\in\mathcal{P}_{k}(G,u)}\sigma\left(Z^{T}h^{(l-1)}(p)\right),$
    |  |'
- en: where $Z$ is the matrix of prototype path attributes.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Z$是原型路径属性的矩阵。
- en: Furthermore, the paper analysis the relationship between GCKN and the WL-subtree
    based $k$-GNN. The Theorem 1 in paper(Chen et al., [2020c](#bib.bib45)) shows
    that WL-subtree based GKNNs can be seen as a special case in GCKN.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，论文分析了GCKN与基于WL子树的$k$-GNN之间的关系。论文中的定理1（Chen et al., [2020c](#bib.bib45)）表明，基于WL子树的GKNN可以看作GCKN中的一个特例。
- en: Graph Neural Tangent Kernel (Du et al., [2019](#bib.bib78)) (GNTK). Different
    from the above two works, GNTK proposed a new class of graph kernels. GNTK is
    a general recipe which translates a GNN architecture to its corresponding GNTK.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经切线核（Du et al., [2019](#bib.bib78)）（GNTK）。与上述两个工作不同，GNTK提出了一类新的图核。GNTK是一种通用的方法，将GNN架构转换为相应的GNTK。
- en: The neighborhood aggregation operation in GNTK is defined as,
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: GNTK中的邻域聚合操作定义为：
- en: '| (27) |  | $\displaystyle\left[\sum_{(0)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle c_{u}c_{u^{{}^{\prime}}}\sum_{u\in N(v)\cup\{u\}}\sum_{v^{{}^{\prime}}\in
    N(u^{{}^{\prime}})\cup\{u^{{}^{\prime}}\}}\left[\sum_{R}^{(l-1)}(G,G^{{}^{\prime}})\right]_{vv^{{}^{\prime}}},$
    |  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| (27) |  | $\displaystyle\left[\sum_{(0)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle c_{u}c_{u^{{}^{\prime}}}\sum_{u\in N(v)\cup\{u\}}\sum_{v^{{}^{\prime}}\in
    N(u^{{}^{\prime}})\cup\{u^{{}^{\prime}}\}}\left[\sum_{R}^{(l-1)}(G,G^{{}^{\prime}})\right]_{vv^{{}^{\prime}}},$
    |  |'
- en: '|  | $\displaystyle\left[\Theta_{(0)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle c_{u}c_{u^{{}^{\prime}}}\sum_{u\in N(v)\cup\{u\}}\sum_{v^{{}^{\prime}}\in
    N(u^{{}^{\prime}})\cup\{u^{{}^{\prime}}\}}\left[\Theta_{R}^{(l-1)}(G,G^{{}^{\prime}})\right]_{vv^{{}^{\prime}}},$
    |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left[\Theta_{(0)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle c_{u}c_{u^{{}^{\prime}}}\sum_{u\in N(v)\cup\{u\}}\sum_{v^{{}^{\prime}}\in
    N(u^{{}^{\prime}})\cup\{u^{{}^{\prime}}\}}\left[\Theta_{R}^{(l-1)}(G,G^{{}^{\prime}})\right]_{vv^{{}^{\prime}}},$
    |  |'
- en: where $\sum_{R}^{(0)}(G,G^{{}^{\prime}})$ and $\Theta_{R}^{(0)}(G,G^{{}^{\prime}})$
    are defined as $\sum^{(0)}(G,G^{{}^{\prime}})$. Then GNTK performed $R$ transformations
    to xx.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\sum_{R}^{(0)}(G,G^{{}^{\prime}})$和$\Theta_{R}^{(0)}(G,G^{{}^{\prime}})$定义为$\sum^{(0)}(G,G^{{}^{\prime}})$。然后，GNTK对xx进行了$R$次变换。
- en: '| (28) |  | $\left[A_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=\left(\begin{aligned}
    &amp;\left[\sum_{(r-1)}^{(l)}(G,G)\right]_{uu^{{}^{\prime}}}&amp;,\ &amp;\left[\sum_{(r-1)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}\\
    &amp;\left[\sum_{(r-1)}^{(l)}(G^{{}^{\prime}},G)\right]_{uu^{{}^{\prime}}}&amp;,\
    &amp;\left[\sum_{(r-1)}^{(l)}(G^{{}^{\prime}},G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}\end{aligned}\right).$
    |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| (28) |  | $\left[A_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=\left(\begin{aligned}
    &\left[\sum_{(r-1)}^{(l)}(G,G)\right]_{uu^{{}^{\prime}}},&\ &\left[\sum_{(r-1)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}\\
    &\left[\sum_{(r-1)}^{(l)}(G^{{}^{\prime}},G)\right]_{uu^{{}^{\prime}}},&\ &\left[\sum_{(r-1)}^{(l)}(G^{{}^{\prime}},G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}\end{aligned}\right).$
    |  |'
- en: '| (29) |  | $\displaystyle\left[\sum_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle\mathbb{E}_{(a,b)\sim\mathcal{N}(0,[A_{(r)}^{(l)}(G,G^{{}^{\prime}})]_{uu^{{}^{\prime}}})}[\sigma(a)\cdot\sigma(b)],$
    |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| (29) |  | $\displaystyle\left[\sum_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle\mathbb{E}_{(a,b)\sim\mathcal{N}(0,[A_{(r)}^{(l)}(G,G^{{}^{\prime}})]_{uu^{{}^{\prime}}})}[\sigma(a)\cdot\sigma(b)],$
    |  |'
- en: '|  | $\displaystyle\left[\sum_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle\mathbb{E}_{(a,b)\sim\mathcal{N}(0,[A_{(r)}^{(l)}(G,G^{{}^{\prime}})]_{uu^{{}^{\prime}}})}[\sigma(a)\cdot\sigma(b)],$
    |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left[\sum_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle\mathbb{E}_{(a,b)\sim\mathcal{N}(0,[A_{(r)}^{(l)}(G,G^{{}^{\prime}})]_{uu^{{}^{\prime}}})}[\sigma(a)\cdot\sigma(b)],$
    |  |'
- en: then the $r$ order can be calculated as,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以计算$r$阶为，
- en: '| (30) |  | $\left[\Theta_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=\left[\Theta_{(r-1)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}\left[\sum{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}+\left[\sum{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}.$
    |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| (30) |  | $\left[\Theta_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=\left[\Theta_{(r-1)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}\left[\sum{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}+\left[\sum{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}.$
    |  |'
- en: Finally, GNTK calculates the final output as
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，GNTK 计算最终输出为
- en: '| (31) |  | $\Theta(G,G^{{}^{\prime}})=\sum_{u\in V,u^{{}^{\prime}}\in V^{{}^{\prime}}}\left[\sum_{l=0}^{L}\Theta_{(R)}^{(l)}(G,G^{{}^{\prime}})\right]_{u,u^{{}^{\prime}}}.$
    |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| (31) |  | $\Theta(G,G^{{}^{\prime}})=\sum_{u\in V,u^{{}^{\prime}}\in V^{{}^{\prime}}}\left[\sum_{l=0}^{L}\Theta_{(R)}^{(l)}(G,G^{{}^{\prime}})\right]_{u,u^{{}^{\prime}}}.$
    |  |'
- en: Heterogeneous Graph Kernel based Graph Neural Network (Long et al., [2021b](#bib.bib239))
    (HGK-GNN). HGK-GNN first proposed GKNN for heterogeneous graphs. It adopted $\langle
    f(u_{1}),f(u_{2})\rangle_{M}$ as graph kernel based on the Mahalanobis Distance
    to build connections among heterogeneous nodes and edges,
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基于异质图核的图神经网络（Long et al., [2021b](#bib.bib239)）（HGK-GNN）。HGK-GNN 首次提出了针对异质图的
    GKNN。它采用了 $\langle f(u_{1}),f(u_{2})\rangle_{M}$ 作为基于马氏距离的图核，以建立异质节点和边之间的连接，
- en: '|  | $\langle f(u_{1}),f(u_{2})\rangle_{M_{1}}=f(u_{1})^{T}\boldsymbol{M}_{1}f(u_{2}),$
    |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $\langle f(u_{1}),f(u_{2})\rangle_{M_{1}}=f(u_{1})^{T}\boldsymbol{M}_{1}f(u_{2}),$
    |  |'
- en: '|  | $\langle f(e_{\cdot,u_{1}}),f(e_{\cdot,u_{2}})\rangle_{M_{2}}=f(e_{\cdot,u_{1}})^{T}\boldsymbol{M}_{2}f(e_{\cdot,u_{2}}).$
    |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $\langle f(e_{\cdot,u_{1}}),f(e_{\cdot,u_{2}})\rangle_{M_{2}}=f(e_{\cdot,u_{1}})^{T}\boldsymbol{M}_{2}f(e_{\cdot,u_{2}}).$
    |  |'
- en: Following the route introduced in (Lei et al., [2017](#bib.bib203)) , the corresponding
    neural network architecture of heterogeneous graph kernel can be derived as
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 按照（Lei et al., [2017](#bib.bib203)）中介绍的路线，可以推导出异质图核的对应神经网络架构为
- en: '| (32) |  | $\displaystyle h^{(0)}(v)=$ | $\displaystyle\boldsymbol{W}^{(0)}_{t_{V}(v)}f(v),$
    |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| (32) |  | $\displaystyle h^{(0)}(v)=$ | $\displaystyle\boldsymbol{W}^{(0)}_{t_{V}(v)}f(v),$
    |  |'
- en: '|  | $\displaystyle h^{(l)}(v)=$ | $\displaystyle\boldsymbol{W}^{(l)}_{t_{V}(v)}f(v)\odot\sum_{u\in
    N(v)}(\boldsymbol{U}_{t_{V}(v)}^{(l)}h^{(l-1)}(u)\odot\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}f(e_{u,v})),\qquad
    1<l\leq L,$ |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{(l)}(v)=$ | $\displaystyle\boldsymbol{W}^{(l)}_{t_{V}(v)}f(v)\odot\sum_{u\in
    N(v)}(\boldsymbol{U}_{t_{V}(v)}^{(l)}h^{(l-1)}(u)\odot\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}f(e_{u,v})),\qquad
    1<l\leq L,$ |  |'
- en: where $h^{(l)}(v)$ is the cell state vector of node v, and $\boldsymbol{W}^{(l)}_{t_{V}(v)}$,
    $\boldsymbol{U}_{t_{V}(v)}^{(l)}$ $\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}$ are
    learnable parameters.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h^{(l)}(v)$ 是节点 v 的细胞状态向量，$\boldsymbol{W}^{(l)}_{t_{V}(v)}$、$\boldsymbol{U}_{t_{V}(v)}^{(l)}$
    和 $\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}$ 是可学习的参数。
- en: 4.4\. Summary
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 总结
- en: 'This section introduces graph kernel neural networks. We provide the summary
    as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了图核神经网络。我们提供了以下总结：
- en: •
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Techniques. Graph kernel neural networks (GKNNs) are a recent popular research
    area that combines the advantages of graph kernels and GNNs to learn more effective
    graph representations. Researchers have studied GKNNs in various aspects, such
    as theoretical foundations, algorithmic design, and practical applications. As
    a result, a wide range of GKNN-based models and methods have been developed for
    graph analysis and representation tasks, including node classification, link prediction,
    and graph clustering.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术。图核神经网络（GKNNs）是一个最近流行的研究领域，它结合了图核和GNNs的优势，以学习更有效的图表示。研究人员在理论基础、算法设计和实际应用等方面研究了GKNNs。因此，已经开发出了一系列基于GKNN的模型和方法用于图分析和表示任务，包括节点分类、链路预测和图聚类。
- en: •
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Challenges and Limitations. Although GKNNs have shown great potential in graph-related
    tasks, they also have several limitations that need to be addressed. Scalability
    is a significant challenge, particularly when dealing with large-scale graphs
    and networks. As the size of the graph increases, the computational cost of GKNNs
    grows exponentially, which can limit their ability to handle large and complex
    real-world applications.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战和限制。尽管GKNNs在图相关任务中显示出巨大的潜力，但它们也存在一些需要解决的限制。可扩展性是一个重要挑战，尤其是在处理大规模图和网络时。随着图的规模增加，GKNNs的计算成本呈指数增长，这可能限制了它们处理大规模和复杂实际应用的能力。
- en: •
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future Works. For future works, we expect the GKNNs can integrate more domain-specific
    knowledge into the designed kernels. Domain-specific knowledge has been shown
    to significantly improve the performance of many applications, such as drug discovery,
    knowledge graph-based information retrieval systems, and molecular analysis (Wang
    et al., [2019e](#bib.bib361); Feinberg et al., [2018](#bib.bib91)). Incorporating
    domain-specific knowledge into GKNNs can enhance their ability to handle complex
    and diverse data structures, leading to more accurate and interpretable models.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来工作。对于未来的工作，我们期望GKNNs能将更多的领域特定知识整合到设计的内核中。领域特定知识已经显示出显著提高许多应用的性能，如药物发现、基于知识图谱的信息检索系统和分子分析（Wang
    et al., [2019e](#bib.bib361); Feinberg et al., [2018](#bib.bib91)）。将领域特定知识融入GKNNs可以增强它们处理复杂和多样化数据结构的能力，从而生成更准确和可解释的模型。
- en: 5\. Graph Pooling By Yiyang Gu
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 由Yiyang Gu提供的图池化
- en: 'When it comes to graph-level tasks, such as graph classification and graph
    regression, graph pooling is an essential component for generating the whole graph
    representation from the learned node embeddings. To ensure isomorphic graphs have
    the same representation, the graph pooling operations should be invariant to the
    permutations of nodes. In this section, we give a systematic review of existing
    graph pooling algorithms and generally classify them into two categories: global
    pooling algorithms and hierarchical pooling algorithms. The global pooling algorithms
    aggregate the node embeddings to the final graph representation directly, while
    the hierarchical pooling algorithms reduce the graph size and generate the immediate
    representations gradually to capture the hierarchical structure and characteristics
    of the input graph. A summary is provided in Table [3](#S5.T3 "Table 3 ‣ 5.1\.
    Global Pooling ‣ 5\. Graph Pooling By Yiyang Gu ‣ A Comprehensive Survey on Deep
    Graph Representation Learning").'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到图级任务，如图分类和图回归时，图池化是从学习到的节点嵌入生成整个图表示的一个重要组件。为了确保同构图具有相同的表示，图池化操作应对节点的排列保持不变。在这一部分，我们对现有的图池化算法进行了系统的回顾，并将其一般性地分为两类：全局池化算法和层次池化算法。全局池化算法直接将节点嵌入聚合到最终的图表示中，而层次池化算法则通过逐步减少图的大小和生成即时表示来捕捉输入图的层次结构和特征。总结见表[3](#S5.T3
    "表 3 ‣ 5.1\. 全局池化 ‣ 5\. 由Yiyang Gu提供的图池化 ‣ 关于深度图表示学习的综合调查")。
- en: 5.1\. Global Pooling
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 全局池化
- en: Global pooling operations generate a holistic graph representation from the
    learned node embeddings in one step, which are referred to as readout functions
    in some literature (Xu et al., [2018a](#bib.bib404); Corso et al., [2020](#bib.bib65))
    as well. Several simple permutation-invariant operations, such as mean, sum, and
    max, are widely employed on the node embeddings to output the graph-level representation.
    To enhance the adaptiveness of global pooling operators, GGS-NN (Li et al., [2016](#bib.bib222))
    introduces a soft attention mechanism to evaluate the importance of nodes for
    a particular task and then takes a weighted sum of the node embeddings. SortPool
    (Zhang et al., [2018a](#bib.bib438)) exploits Weisfeiler-Lehman methods (Weisfeiler
    and Leman, [1968](#bib.bib379)) to sort nodes based on their structural positions
    in the graph topology, and produces the graph representation from the sorted node
    embeddings by traditional convolutional neural networks. Recently, Lee et al.
    propose a structural-semantic pooling method, SSRead (Lee et al., [2021](#bib.bib200)),
    which first aligns nodes and learnable structural prototypes semantically, and
    then aggregates node representations in groups based on matching structural prototypes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 全局池化操作通过一步生成从学习到的节点嵌入中获得的整体图表示，这在一些文献中被称为读出函数（Xu 等，[2018a](#bib.bib404)；Corso
    等，[2020](#bib.bib65)）。几种简单的置换不变操作，如均值、和以及最大值，广泛应用于节点嵌入上以输出图级别的表示。为了增强全局池化操作符的适应性，GGS-NN（Li
    等，[2016](#bib.bib222)）引入了软注意力机制，以评估节点对特定任务的重要性，然后对节点嵌入进行加权求和。SortPool（Zhang 等，[2018a](#bib.bib438)）利用Weisfeiler-Lehman方法（Weisfeiler
    和 Leman，[1968](#bib.bib379)）根据节点在图拓扑中的结构位置对节点进行排序，然后通过传统卷积神经网络从排序后的节点嵌入中生成图表示。最近，Lee
    等提出了一种结构-语义池化方法SSRead（Lee 等，[2021](#bib.bib200)），该方法首先在语义上对齐节点和可学习的结构原型，然后基于匹配的结构原型对节点表示进行分组聚合。
- en: Table 3\. Summary of graph pooling methods.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 图池化方法总结。
- en: '| Method | Type | TopK-based | Cluster-based | Attention Mechanism |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类型 | 基于TopK | 基于聚类 | 注意力机制 |'
- en: '| Mean/Sum/Max | Global |  |  |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 均值/和/最大值 | 全局 |  |  |  |'
- en: '| Pooling |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 池化 |'
- en: '| GGS-NN (Li et al., [2016](#bib.bib222)) | Global |  |  | ✓ |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| GGS-NN (Li 等，[2016](#bib.bib222)) | 全局 |  |  | ✓ |'
- en: '| SortPool (Zhang et al., [2018a](#bib.bib438)) | Global |  |  |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| SortPool (Zhang 等，[2018a](#bib.bib438)) | 全局 |  |  |  |'
- en: '| SSRead (Lee et al., [2021](#bib.bib200)) | Global |  |  |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SSRead (Lee 等，[2021](#bib.bib200)) | 全局 |  |  |  |'
- en: '| gPool (Gao and Ji, [2019](#bib.bib105)) | Hierarchical | ✓ |  |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| gPool (Gao 和 Ji，[2019](#bib.bib105)) | 层次型 | ✓ |  |  |'
- en: '| SAGPool (Lee et al., [2019](#bib.bib202)) | Hierarchical | ✓ |  | ✓ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| SAGPool (Lee 等，[2019](#bib.bib202)) | 层次型 | ✓ |  | ✓ |'
- en: '| HGP-SL (Zhang et al., [2019a](#bib.bib446)) | Hierarchical | ✓ |  | ✓ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| HGP-SL (Zhang 等，[2019a](#bib.bib446)) | 层次型 | ✓ |  | ✓ |'
- en: '| TAPool (Gao et al., [2021](#bib.bib106)) | Hierarchical | ✓ |  |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| TAPool (Gao 等，[2021](#bib.bib106)) | 层次型 | ✓ |  |  |'
- en: '| DiffPool (Ying et al., [2018](#bib.bib418)) | Hierarchical |  | ✓ |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| DiffPool (Ying 等，[2018](#bib.bib418)) | 层次型 |  | ✓ |  |'
- en: '| MinCutPool (Bianchi et al., [2020](#bib.bib23)) | Hierarchical |  | ✓ |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| MinCutPool (Bianchi 等，[2020](#bib.bib23)) | 层次型 |  | ✓ |  |'
- en: '| SEP (Wu et al., [2022](#bib.bib385)) | Hierarchical |  | ✓ |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| SEP (Wu 等，[2022](#bib.bib385)) | 层次型 |  | ✓ |  |'
- en: '| ASAP (Ranjan et al., [2020](#bib.bib293)) | Hierarchical | ✓ | ✓ | ✓ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ASAP (Ranjan 等，[2020](#bib.bib293)) | 层次型 | ✓ | ✓ | ✓ |'
- en: '| MuchPool (Du et al., [2021](#bib.bib77)) | Hierarchical | ✓ | ✓ |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| MuchPool (Du 等，[2021](#bib.bib77)) | 层次型 | ✓ | ✓ |  |'
- en: 5.2\. Hierarchical Pooling
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 层次型池化
- en: Different from global pooling methods, hierarchical pooling methods coarsen
    the graph gradually, to preserve the structural information of the graph better.
    To adaptively coarse the graph and learn optimal hierarchical representations
    according to a particular task, many learnable hierarchical pooling operators
    are proposed in the past few years, which can be integrated with multifarious
    graph convolution layers. There are two common approaches to coarsening the graph,
    one is selecting important nodes and dropping the others by TopK selection, and
    the other one is merging nodes and generating the coarsened graph by clustering
    methods. We call the former TopK-based pooling, and the latter cluster-based pooling
    in this survey. In addition, some works combine these two types of pooling methods,
    which will be reviewed in the hybrid pooling section.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 与全局池化方法不同，分层池化方法逐渐粗化图，以更好地保留图的结构信息。为了根据特定任务自适应地粗化图并学习最佳的分层表示，近年来提出了许多可学习的分层池化算子，这些算子可以与各种图卷积层集成。粗化图有两种常见的方法，一种是通过
    TopK 选择重要节点并丢弃其他节点，另一种是通过聚类方法合并节点并生成粗化图。在本调查中，我们称前者为基于 TopK 的池化，称后者为基于聚类的池化。此外，一些工作结合了这两种池化方法，这将在混合池化部分进行回顾。
- en: 5.2.1\. TopK-based Pooling.
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 基于 TopK 的池化。
- en: Typically, TopK-based pooling methods first learn a scoring function to evaluate
    the importance of nodes of the original graph. Based on importance score $\mathbf{Z}\in\mathbb{R}^{|V|\times
    1}$ generated, they select the top $K$ nodes out of all nodes,
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，基于 TopK 的池化方法首先学习一个评分函数来评估原始图中节点的重要性。基于生成的重要性分数 $\mathbf{Z}\in\mathbb{R}^{|V|\times
    1}$，它们从所有节点中选择前 $K$ 个节点，
- en: '| (33) |  | $idx=\operatorname{TOP}_{k}\left(\mathbf{Z}\right),$ |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| (33) |  | $idx=\operatorname{TOP}_{k}\left(\mathbf{Z}\right),$ |  |'
- en: where $idx$ denotes the index of the top $K$ nodes. Based on these selected
    nodes, most methods directly employ the induced subgraph as the pooled graph,
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $idx$ 表示前 $K$ 个节点的索引。基于这些选定的节点，大多数方法直接将诱导子图作为池化图，
- en: '| (34) |  | $\mathbf{A}_{\text{pool}}=\mathbf{A}_{idx,idx},$ |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| (34) |  | $\mathbf{A}_{\text{pool}}=\mathbf{A}_{idx,idx},$ |  |'
- en: where $A_{idx,idx}$ denotes the adjacency matrix indexed by the selected rows
    and columns. Moreover, to make the scoring function learnable, they further use
    score $Z$ as a gate for the selected node features,
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A_{idx,idx}$ 表示由选定行和列索引的邻接矩阵。此外，为了使评分函数可学习，它们进一步使用分数 $Z$ 作为选定节点特征的门控，
- en: '| (35) |  | $\mathbf{X}_{\text{pool}}=\mathbf{X}_{idx,:}\odot\mathbf{Z}_{idx},$
    |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| (35) |  | $\mathbf{X}_{\text{pool}}=\mathbf{X}_{idx,:}\odot\mathbf{Z}_{idx},$
    |  |'
- en: where $X_{idx,:}$ denotes the feature matrix indexed by the selected nodes,
    and $\odot$ denotes the broadcasted element-wise product. With the help of the
    gate mechanism, the scoring function can be trained by back-propagation, to adaptively
    evaluate the importance of nodes according to a certain task. Several representative
    TopK-based pooling methods are reviewed detailly in the following.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X_{idx,:}$ 表示由选定节点索引的特征矩阵，$\odot$ 表示广播的逐元素乘积。借助门控机制，评分函数可以通过反向传播训练，以根据某个任务自适应地评估节点的重要性。以下将详细回顾几个具有代表性的基于
    TopK 的池化方法。
- en: gPool (Gao and Ji, [2019](#bib.bib105)). gPool is one of the first works to
    select the most important node subset from the original graph to construct the
    coarsened graph by Top K operation. The key idea of gPool is to evaluate the importance
    of all nodes by learning a projection vector $\mathbf{p}$, which projects node
    features to a scalar score,
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: gPool（Gao 和 Ji，[2019](#bib.bib105)）。gPool 是最早通过 Top K 操作从原始图中选择最重要的节点子集以构建粗化图的工作之一。gPool
    的关键思想是通过学习投影向量 $\mathbf{p}$ 来评估所有节点的重要性，该向量将节点特征投影到一个标量分数上，
- en: '| (36) |  | $\mathbf{Z}_{j}=\mathbf{X}_{j,:}\mathbf{p}/\&#124;\mathbf{p}\&#124;,$
    |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| (36) |  | $\mathbf{Z}_{j}=\mathbf{X}_{j,:}\mathbf{p}/\|\mathbf{p}\|,$ |  |'
- en: and then select nodes with K-highest scores to form the pooled graph.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 然后选择具有 K 高分的节点以形成池化图。
- en: Self-Attention Graph Pooling (SAGPool)  (Lee et al., [2019](#bib.bib202)). Unlike
    gPool (Gao and Ji, [2019](#bib.bib105)), which only uses node features to generate
    projection scores, SAGPool captures both graph topology and node features to obtain
    self-attention scores by graph convolution. The various formulas of graph convolution
    can be employed to compute the self-attention score $\mathbf{Z}$,
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力图池化（SAGPool）（Lee 等，[2019](#bib.bib202)）。与仅使用节点特征生成投影分数的 gPool（Gao 和 Ji，[2019](#bib.bib105)）不同，SAGPool
    捕捉图的拓扑结构和节点特征，通过图卷积获得自注意力分数。可以使用各种图卷积公式来计算自注意力分数 $\mathbf{Z}$，
- en: '| (37) |  | $\mathbf{Z}=\sigma(\operatorname{GNN}(\mathbf{X},\mathbf{A})),$
    |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| (37) |  | $\mathbf{Z}=\sigma(\operatorname{GNN}(\mathbf{X},\mathbf{A})),$
    |  |'
- en: where $\sigma$ denotes the activation function, and $\operatorname{GNN}$ denotes
    various graph convolutional layers or stacks of them, whose output dimension is
    one.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma$ 表示激活函数，$\operatorname{GNN}$ 表示各种图卷积层或其堆叠，其输出维度为一。
- en: Hierarchical Graph Pooling with Structure Learning (HGP-SL)  (Zhang et al.,
    [2019a](#bib.bib446)) . HGP-SL evaluates the importance score of a node according
    to the information it contains given its neighbors. It supposes that a node which
    can be easily represented by its neighborhood carries relatively little information.
    Specifically, the importance score can be defined by the Manhattan distance between
    the original node representation and the reconstructed one aggregated from its
    neighbors’ representation,
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 层次图池化与结构学习（HGP-SL）（Zhang 等，[2019a](#bib.bib446)）。HGP-SL 根据节点在其邻居给定的信息评估节点的重要性分数。它假设一个可以通过其邻域容易表示的节点包含的信息相对较少。具体而言，重要性分数可以通过原始节点表示和从其邻居表示聚合的重建节点表示之间的曼哈顿距离来定义，
- en: '| (38) |  | $\mathbf{Z}=\left\&#124;\left(\mathbf{I}-\mathbf{D}^{-1}\mathbf{A}\right)\mathbf{X}\right\&#124;_{1},$
    |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| (38) |  | $\mathbf{Z}=\left\&#124;\left(\mathbf{I}-\mathbf{D}^{-1}\mathbf{A}\right)\mathbf{X}\right\&#124;_{1},$
    |  |'
- en: where $\mathbf{I}$ denotes the identity matrix, $\mathbf{D}$ denotes the diagonal
    degree matrix of $\mathbf{A}$, and $\|\cdot\|_{1}$ means $\ell_{1}$ norm. Furthermore,
    to reduce the loss of topology information, HGP-SL leverages structure learning
    to learn a refined graph topology for the reserved nodes. Specifically, it utilizes
    the attention mechanism to compute the similarity of two nodes as the weight of
    the reconstructed edge,
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{I}$ 表示单位矩阵，$\mathbf{D}$ 表示 $\mathbf{A}$ 的对角度矩阵，$\|\cdot\|_{1}$ 表示
    $\ell_{1}$ 范数。此外，为了减少拓扑信息的损失，HGP-SL 利用结构学习为保留的节点学习精炼的图拓扑。具体而言，它利用注意力机制计算两个节点的相似性作为重建边的权重，
- en: '| (39) |  | $\widetilde{\boldsymbol{A}}^{\text{pool}}_{ij}=\operatorname{sparsemax}\left(\sigma\left(\mathbf{w}\left[\boldsymbol{X}^{\text{pool}}_{i,:}\&#124;\boldsymbol{X}^{\text{pool}}_{j,:}\right]^{\top}\right)+\lambda\cdot\boldsymbol{A}^{\text{pool}}_{ij}\right),$
    |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| (39) |  | $\widetilde{\boldsymbol{A}}^{\text{pool}}_{ij}=\operatorname{sparsemax}\left(\sigma\left(\mathbf{w}\left[\boldsymbol{X}^{\text{pool}}_{i,:}\&#124;\boldsymbol{X}^{\text{pool}}_{j,:}\right]^{\top}\right)+\lambda\cdot\boldsymbol{A}^{\text{pool}}_{ij}\right),$
    |  |'
- en: where $\widetilde{\boldsymbol{A}}^{\text{pool}}$ denotes the refined adjacency
    matrix of the pooled graph, $\operatorname{sparsemax}(\cdot)$ truncates the values
    below a threshold to zeros, $\mathbf{w}$ denotes a learnable weight vector, and
    $\lambda$ is a weight parameter between the original edges and the reconstructed
    edges. These reconstructed edges may capture the underlying relationship between
    nodes disconnected due to node dropping.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\widetilde{\boldsymbol{A}}^{\text{pool}}$ 表示池化图的精炼邻接矩阵，$\operatorname{sparsemax}(\cdot)$
    将低于阈值的值截断为零，$\mathbf{w}$ 表示一个可学习的权重向量，$\lambda$ 是原始边和重建边之间的权重参数。这些重建的边可能捕捉由于节点丢弃而导致的节点间潜在关系。
- en: Topology-Aware Graph Pooling (TAPool)  (Gao et al., [2021](#bib.bib106)). TAPool
    takes both the local and global significance of a node into account. On the one
    hand, it utilizes the average similarity between a node and its neighbors to evaluate
    its local importance,
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑感知图池化（TAPool）（Gao 等，[2021](#bib.bib106)）。TAPool 同时考虑节点的局部和全局重要性。一方面，它利用节点与其邻居之间的平均相似性来评估其局部重要性，
- en: '| (40) |  | $\hat{\mathbf{R}}=\left(\mathbf{X}\mathbf{X}^{T}\right)\odot\left(\mathbf{D}^{-1}\mathbf{A}\right),\mathbf{Z}^{l}=\operatorname{softmax}\left(\frac{1}{n}\hat{\mathbf{R}}\mathbf{1}_{n}\right),$
    |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| (40) |  | $\hat{\mathbf{R}}=\left(\mathbf{X}\mathbf{X}^{T}\right)\odot\left(\mathbf{D}^{-1}\mathbf{A}\right),\mathbf{Z}^{l}=\operatorname{softmax}\left(\frac{1}{n}\hat{\mathbf{R}}\mathbf{1}_{n}\right),$
    |  |'
- en: where $\hat{\mathbf{R}}$ denotes the localized similarity matrix, and $\mathbf{Z}^{l}$
    denotes the local importance score. On the other hand, it measures the global
    importance of a node according to the significance of its one-hop neighborhood
    in the whole graph,
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\hat{\mathbf{R}}$ 表示局部相似度矩阵，而 $\mathbf{Z}^{l}$ 表示局部重要性得分。另一方面，它根据节点在整个图中的单跳邻域的重要性来衡量节点的全局重要性，
- en: '| (41) |  | $\hat{\mathbf{X}}=\mathbf{D}^{-1}\mathbf{A}\mathbf{X},\mathbf{Z}^{g}=\operatorname{softmax}\left(\hat{\mathbf{X}}\mathbf{p}\right),$
    |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| (41) |  | $\hat{\mathbf{X}}=\mathbf{D}^{-1}\mathbf{A}\mathbf{X},\mathbf{Z}^{g}=\operatorname{softmax}\left(\hat{\mathbf{X}}\mathbf{p}\right),$
    |  |'
- en: where $\mathbf{p}$ is a learnable and globally shared projector vector, similar
    to the aforementioned gPool (Gao and Ji, [2019](#bib.bib105)). However, $\hat{\mathbf{X}}$
    here further aggregates the features from the neighborhood, which enables the
    global importance score $\mathbf{Z}^{g}$ to capture more topology information
    such as salient subgraphs. Moreover, TAPool encourages connectivity in the coarsened
    graph with the help of a degree-based connectivity term, then obtaining the final
    importance score $\mathbf{Z}=\mathbf{Z}^{l}+\mathbf{Z}^{g}+\lambda\mathbf{D}/|V|$,
    where $\lambda$ is a trade-off hyperparameter.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbf{p}$ 是一个可学习的全局共享投影向量，类似于前述的 gPool（Gao 和 Ji，[2019](#bib.bib105)）。然而，这里的
    $\hat{\mathbf{X}}$ 进一步聚合了来自邻域的特征，这使得全局重要性得分 $\mathbf{Z}^{g}$ 能够捕捉更多的拓扑信息，如显著子图。此外，TAPool
    通过度数基础的连接项鼓励在粗化图中的连接，然后得到最终的重要性得分 $\mathbf{Z}=\mathbf{Z}^{l}+\mathbf{Z}^{g}+\lambda\mathbf{D}/|V|$，其中
    $\lambda$ 是一个权衡超参数。
- en: 5.2.2\. Cluster-based Pooling.
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 基于聚类的池化。
- en: Pooling the graph by clustering and merging nodes is the main concept behind
    cluster-based pooling methods. Typically, they allocate nodes to a collection
    of clusters by learning a cluster assignment matrix $\mathbf{S}\in\mathbb{R}^{|V|\times
    K}$, where $K$ is the number of the clusters. After that, they merge the nodes
    within each cluster to generate a new node in the pooled graph. The feature (embedding)
    matrix of the new nodes can be obtained by aggregating the features (embeddings)
    of nodes within the clusters, according to the cluster assignment matrix,
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过聚类和合并节点对图进行池化是基于聚类的池化方法的主要概念。通常，它们通过学习一个聚类分配矩阵 $\mathbf{S}\in\mathbb{R}^{|V|\times
    K}$ 将节点分配到一组集群中，其中 $K$ 是集群的数量。之后，它们将每个集群中的节点合并，以生成池化图中的新节点。新节点的特征（嵌入）矩阵可以通过聚合集群内节点的特征（嵌入）来获得，根据聚类分配矩阵，
- en: '| (42) |  | $\mathbf{X}^{\text{pool}}=\mathbf{S}^{T}\mathbf{X}.$ |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| (42) |  | $\mathbf{X}^{\text{pool}}=\mathbf{S}^{T}\mathbf{X}.$ |  |'
- en: While the adjacency matrix of the pooled graph can be generated by calculating
    the connectivity strength between each pair of clusters,
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，池化图的邻接矩阵可以通过计算每对集群之间的连接强度来生成，
- en: '| (43) |  | $\mathbf{A}^{\text{pool}}=\mathbf{S}^{T}\mathbf{A}\mathbf{S}.$
    |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| (43) |  | $\mathbf{A}^{\text{pool}}=\mathbf{S}^{T}\mathbf{A}\mathbf{S}.$
    |  |'
- en: Then, we review several representative cluster-based pooling methods in detail.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们详细回顾几种具有代表性的基于聚类的池化方法。
- en: DiffPool (Ying et al., [2018](#bib.bib418)). DiffPool is one of the first and
    classic works to hierarchically pool the graph by graph clustering. Specifically,
    it uses an embedding GNN to generate embeddings of nodes, and a pooling GNN to
    generate the cluster assignment matrix,
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: DiffPool（Ying 等，[2018](#bib.bib418)）。DiffPool 是最早且经典的图层次池化方法之一。具体来说，它使用嵌入 GNN
    生成节点的嵌入，以及一个池化 GNN 生成聚类分配矩阵，
- en: '| (44) |  | $\hat{\mathbf{X}}=\operatorname{GNN}_{\text{embed}}\left(\mathbf{X},\mathbf{A}\right),\mathbf{S}=\operatorname{softmax}\left(\operatorname{GNN}_{\text{pool}}\left(\mathbf{X},\mathbf{A}\right)\right),\mathbf{X}^{\text{pool}}=\mathbf{S}^{T}\hat{\mathbf{X}}.$
    |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| (44) |  | $\hat{\mathbf{X}}=\operatorname{GNN}_{\text{embed}}\left(\mathbf{X},\mathbf{A}\right),\mathbf{S}=\operatorname{softmax}\left(\operatorname{GNN}_{\text{pool}}\left(\mathbf{X},\mathbf{A}\right)\right),\mathbf{X}^{\text{pool}}=\mathbf{S}^{T}\hat{\mathbf{X}}.$
    |  |'
- en: Besides, DiffPool leverages an auxiliary link prediction objective $L_{\mathrm{LP}}=\left\|\mathbf{A},\mathbf{S}\mathbf{S}^{T}\right\|_{F}$
    to encourage the adjacent nodes to be in the same cluster and avoid fake local
    minima, where $\|\cdot\|_{F}$ is the Frobenius norm. And it utilizes an entropy
    regularization term $L_{\mathrm{E}}=\frac{1}{|V|}\sum_{i=1}^{|V|}H\left(\mathbf{S}_{i}\right)$
    to impel clear cluster assignments, where $H(\cdot)$ represents the entropy.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DiffPool 利用辅助链接预测目标 $L_{\mathrm{LP}}=\left\|\mathbf{A},\mathbf{S}\mathbf{S}^{T}\right\|_{F}$
    来鼓励相邻节点在同一聚类中，并避免假局部最小值，其中 $\|\cdot\|_{F}$ 是 Frobenius 范数。它还利用熵正则化项 $L_{\mathrm{E}}=\frac{1}{|V|}\sum_{i=1}^{|V|}H\left(\mathbf{S}_{i}\right)$
    来推动清晰的聚类分配，其中 $H(\cdot)$ 代表熵。
- en: Graph Pooling with Spectral Clustering (MinCutPool)  (Bianchi et al., [2020](#bib.bib23)).
    MinCutPool takes advantage of the properties of spectral clustering (SC) to provide
    a better inductive bias and avoid degenerate cluster assignments. It learns to
    cluster like SC by optimizing the MinCut loss,
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 基于谱聚类的图池化（MinCutPool）（Bianchi 等，[2020](#bib.bib23)）。MinCutPool 利用谱聚类（SC）的属性提供更好的归纳偏差，并避免退化的聚类分配。它通过优化
    MinCut 损失来学习像 SC 一样的聚类。
- en: '| (45) |  | $L_{c}=-\frac{\operatorname{Tr}\left(\mathbf{S}^{T}{\mathbf{A}}\mathbf{S}\right)}{\operatorname{Tr}\left(\mathbf{S}^{T}{\mathbf{D}}\mathbf{S}\right)}.$
    |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| (45) |  | $L_{c}=-\frac{\operatorname{Tr}\left(\mathbf{S}^{T}{\mathbf{A}}\mathbf{S}\right)}{\operatorname{Tr}\left(\mathbf{S}^{T}{\mathbf{D}}\mathbf{S}\right)}.$
    |  |'
- en: In addition, it utilizes an orthogonality loss $L_{o}=\left\|\frac{\mathbf{S}^{T}\mathbf{S}}{\left\|\mathbf{S}^{T}\mathbf{S}\right\|_{F}}-\frac{\mathbf{I}_{K}}{\sqrt{K}}\right\|_{F}$
    to encourage orthogonal and uniform cluster assignments, and prevent the bad minima
    of $L_{c}$, where $K$ is the number of the clusters. When performing a specific
    task, it can optimize the weighted sum of the unsupervised loss $L_{u}=L_{c}+L_{o}$
    and a task-specific loss to find the optimal balance between the theoretical prior
    and the task objective.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它利用正交损失 $L_{o}=\left\|\frac{\mathbf{S}^{T}\mathbf{S}}{\left\|\mathbf{S}^{T}\mathbf{S}\right\|_{F}}-\frac{\mathbf{I}_{K}}{\sqrt{K}}\right\|_{F}$
    来鼓励正交和均匀的聚类分配，并防止 $L_{c}$ 的不良最小值，其中 $K$ 是聚类的数量。在执行特定任务时，它可以优化无监督损失 $L_{u}=L_{c}+L_{o}$
    和任务特定损失的加权和，以找到理论先验与任务目标之间的最佳平衡。
- en: Structural Entropy Guided Graph Pooling (SEP)  (Wu et al., [2022](#bib.bib385)).
    In order to lessen the local structural harm and suboptimal performance caused
    by separate pooling layers and predesigned pooling ratios, SEP leverages the concept
    of structural entropy to generate the global and hierarchical cluster assignments
    at once. Specifically, SEP treats the nodes of a given graph as the leaf nodes
    of a coding tree and exploits the hierarchical layers of the coding tree to capture
    the hierarchical structure of the graph. The optimal code tree $T$ can be obtained
    by minimizing the structural entropy (Li and Pan, [2016](#bib.bib205)),
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 结构熵引导图池化（SEP）（Wu 等，[2022](#bib.bib385)）。为了减少由独立池化层和预设池化比例引起的局部结构损害和次优性能，SEP
    利用结构熵的概念一次性生成全局和层次聚类分配。具体而言，SEP 将给定图的节点视为编码树的叶节点，并利用编码树的层次结构来捕捉图的层次结构。通过最小化结构熵（Li
    和 Pan，[2016](#bib.bib205)），可以获得最优的代码树 $T$。
- en: '| (46) |  | $\mathcal{H}^{T}(G)=-\sum_{v_{i}\in T}\frac{g(P_{v_{i}})}{\operatorname{vol}(V)}\log\frac{\operatorname{vol}\left(P_{v_{i}}\right)}{\operatorname{vol}\left(P_{v_{i}^{+}}\right)},$
    |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| (46) |  | $\mathcal{H}^{T}(G)=-\sum_{v_{i}\in T}\frac{g(P_{v_{i}})}{\operatorname{vol}(V)}\log\frac{\operatorname{vol}\left(P_{v_{i}}\right)}{\operatorname{vol}\left(P_{v_{i}^{+}}\right)},$
    |  |'
- en: where $v_{i}^{+}$ represents the father node of node $v_{i}$, $P_{v_{i}}$ denotes
    the partition of leaf nodes which are descendants of $v_{i}$ in the coding tree
    $T$, $g(P_{v_{i}})$ denotes the number of edges that have a terminal in the $P_{v_{i}}$,
    and $\operatorname{vol}(\cdot)$ denotes the total degrees of leaf nodes in the
    given partition. Then, the cluster assignment matrix for each pooling layer can
    be derived from the edges of each layer in the coding tree. With the help of the
    one-step joint assignments generation based on structural entropy, it can not
    only make the best use of the hierarchical relationships of pooling layers, but
    also reduce the structural noise in the original graph.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$v_{i}^{+}$ 表示节点 $v_{i}$ 的父节点，$P_{v_{i}}$ 表示编码树 $T$ 中 $v_{i}$ 的叶子节点的分区，$g(P_{v_{i}})$
    表示在 $P_{v_{i}}$ 中具有终端的边数，$\operatorname{vol}(\cdot)$ 表示给定分区中叶子节点的总度数。然后，每个池化层的簇分配矩阵可以从编码树中每层的边缘推导出来。借助基于结构熵的单步联合分配生成，它不仅能够充分利用池化层的层次关系，还能减少原始图中的结构噪声。
- en: 5.2.3\. Hybrid pooling.
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 混合池化。
- en: Hybrid pooling methods combine TopK-based pooling methods and cluster-based
    pooling methods, to exert the advantages of the two methods and overcome their
    respective limitations. Here, we review two representative hybrid pooling methods,
    Adaptive Structure Aware Pooling  (Ranjan et al., [2020](#bib.bib293)) and Multi-channel
    Pooling  (Du et al., [2021](#bib.bib77)).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 混合池化方法结合了基于TopK的池化方法和基于聚类的池化方法，充分发挥这两种方法的优点并克服它们各自的局限性。在这里，我们回顾了两种代表性的混合池化方法，即自适应结构感知池化（Ranjan等，[2020](#bib.bib293)）和多通道池化（Du等，[2021](#bib.bib77)）。
- en: Adaptive Structure Aware Pooling (ASAP)  (Ranjan et al., [2020](#bib.bib293)).
    Considering that TopK-based pooling methods are not good at capturing the connectivity
    of the coarsened graph, while cluster-based pooling methods fail to be employed
    for large graphs because of the dense assignment matrix, ASAP organically combines
    the two types of pooling methods to overcome the above limitations. Specifically,
    it regards the $h$-hop ego-network $c_{h}(v_{i})$ of each node $v_{i}$ as a cluster.
    Such local clustering enables the cluster assignment matrix to be sparse. Then,
    a new self-attention mechanism Master2Token is used to learn the cluster assignment
    matrix $\mathbf{S}$ and the cluster representations,
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应结构感知池化（ASAP）（Ranjan等，[2020](#bib.bib293)）。考虑到基于TopK的池化方法在捕捉粗化图的连通性方面表现不佳，而基于聚类的池化方法由于密集的分配矩阵无法应用于大图，ASAP
    有机地结合了这两种类型的池化方法以克服上述局限性。具体而言，它将每个节点 $v_{i}$ 的 $h$-跳自我网络 $c_{h}(v_{i})$ 视为一个簇。这种局部聚类使得簇分配矩阵变得稀疏。然后，使用一种新的自注意力机制
    Master2Token 来学习簇分配矩阵 $\mathbf{S}$ 和簇表示。
- en: '| (47) |  | $\mathbf{m}_{i}=\max_{v_{j}\in c_{h}\left(v_{i}\right)}\left(\mathbf{X}_{j}^{\prime}\right),\mathbf{S}_{j,i}=\operatorname{softmax}\left(\mathbf{w}^{T}\sigma\left(\mathbf{W}\mathbf{m}_{i}\&#124;\mathbf{X}_{j}^{\prime}\right)\right),\mathbf{X}_{i}^{c}=\sum_{j=1}^{\left&#124;c_{h}\left(v_{i}\right)\right&#124;}\mathbf{S}_{j,i}\mathbf{X}_{j},$
    |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| (47) |  | $\mathbf{m}_{i}=\max_{v_{j}\in c_{h}\left(v_{i}\right)}\left(\mathbf{X}_{j}^{\prime}\right),\mathbf{S}_{j,i}=\operatorname{softmax}\left(\mathbf{w}^{T}\sigma\left(\mathbf{W}\mathbf{m}_{i}\&#124;\mathbf{X}_{j}^{\prime}\right)\right),\mathbf{X}_{i}^{c}=\sum_{j=1}^{\left&#124;c_{h}\left(v_{i}\right)\right&#124;}\mathbf{S}_{j,i}\mathbf{X}_{j},$
    |  |'
- en: where $\mathbf{X}^{\prime}$ is the node embedding matrix after passing GCN,
    $\mathbf{w}$ and $\mathbf{W}$ denote the trainable vector and matrix respectively,
    and $\mathbf{X}_{i}^{c}$ denotes the representation of the cluster $c_{h}(v_{i})$.
    Next, it utilizes the graph convolution and TopK selection to choose the top $K$
    clusters, whose centers are treated as the nodes of the pooled graph. The adjacency
    matrix of the pooled graph can be calculated like common cluster-based pooling
    methods ([43](#S5.E43 "In 5.2.2\. Cluster-based Pooling. ‣ 5.2\. Hierarchical
    Pooling ‣ 5\. Graph Pooling By Yiyang Gu ‣ A Comprehensive Survey on Deep Graph
    Representation Learning")), preserving the connectivity of the original graph
    well.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbf{X}^{\prime}$ 是经过GCN后的节点嵌入矩阵，$\mathbf{w}$ 和 $\mathbf{W}$ 分别表示可训练的向量和矩阵，$\mathbf{X}_{i}^{c}$
    表示簇 $c_{h}(v_{i})$ 的表示。接下来，它利用图卷积和TopK选择来选择前 $K$ 个簇，其中心被视为池化图的节点。池化图的邻接矩阵可以像常见的基于聚类的池化方法一样计算（[43](#S5.E43
    "在 5.2.2\. 基于聚类的池化. ‣ 5.2\. 层次池化 ‣ 5\. 图池化由Yiyang Gu提供 ‣ 深度图表示学习的全面调查")），很好地保留了原始图的连通性。
- en: Multi-channel Pooling (MuchPool)  (Du et al., [2021](#bib.bib77)). The key idea
    of MuchPool is to capture both the local and global structure of a given graph
    by combining the TopK-based pooling methods and the cluster-based pooling methods.
    MuchPool has two pooling channels based on TopK selection to yield two fine-grained
    pooled graphs, whose selection criteria are node degrees and projected scores
    of node features respectively, so that both the local topology and the node features
    are considered. Besides, it leverages a channel based on graph clustering to obtain
    a coarse-grained pooled graph, which captures the global and hierarchical structure
    of the input graph. To better integrate the information of different channels,
    a cross-channel convolution is proposed, which fuses the node embeddings of the
    fine-grained pooled graph $\mathbf{X}^{\text{fine}}$ and the coarse-grained pooled
    graph $\mathbf{X}^{\text{coarse}}$ with the help of the cluster assignments $\mathbf{S}$
    of the cluster-based pooling channel,
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 多通道池化（MuchPool） (Du 等， [2021](#bib.bib77))。MuchPool 的关键思想是通过结合基于 TopK 的池化方法和基于聚类的池化方法，捕捉给定图的局部和全局结构。MuchPool
    有两个基于 TopK 选择的池化通道，以产生两个细粒度池化图，其选择标准分别是节点度数和节点特征的投影得分，从而考虑了局部拓扑和节点特征。此外，它利用基于图聚类的通道来获得粗粒度池化图，从而捕捉输入图的全局和层次结构。为了更好地整合不同通道的信息，提出了一种交叉通道卷积，它融合了细粒度池化图
    $\mathbf{X}^{\text{fine}}$ 和粗粒度池化图 $\mathbf{X}^{\text{coarse}}$ 的节点嵌入，借助于基于聚类的池化通道的聚类分配
    $\mathbf{S}$。
- en: '| (48) |  | $\widetilde{\mathbf{X}}^{\text{fine}}=\sigma\left(\left[\mathbf{X}^{\text{fine}}+\mathbf{S}\mathbf{X}^{\text{coarse}}\right]\cdot\mathbf{W}\right),$
    |  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| (48) |  | $\widetilde{\mathbf{X}}^{\text{fine}}=\sigma\left(\left[\mathbf{X}^{\text{fine}}+\mathbf{S}\mathbf{X}^{\text{coarse}}\right]\cdot\mathbf{W}\right),$
    |  |'
- en: where $\mathbf{W}$ denotes the learnable weights. Finally, it merges the node
    embeddings and the adjacency matrices of the two fine-grained pooled graphs to
    obtain the eventually pooled graph.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{W}$ 表示可学习的权重。最后，它将两个细粒度池化图的节点嵌入和邻接矩阵合并以获得最终池化图。
- en: 5.3\. Summary
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 总结
- en: 'This section introduces graph pooling methods for graph-level representation
    learning. We provide the summary as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了图级表示学习的图池化方法。我们提供的总结如下：
- en: •
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Techniques. Graph pooling methods play a vital role in generating an entire
    graph representation by aggregating node embeddings. There are mainly two categories
    of graph pooling methods: global pooling methods and hierarchical pooling methods.
    While global pooling methods directly aggregate node embeddings in one step, hierarchical
    pooling methods gradually coarsen a graph to capture hierarchical structure characteristics
    of the graph based on TopK selection, clustering methods, or hybrid methods.'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术。图池化方法在通过聚合节点嵌入生成整个图表示方面发挥着至关重要的作用。图池化方法主要分为两类：全局池化方法和层次池化方法。全局池化方法直接在一步中聚合节点嵌入，而层次池化方法则逐渐粗化图，以基于
    TopK 选择、聚类方法或混合方法捕捉图的层次结构特征。
- en: •
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Challenges and Limitations. Despite the great success of graph pooling methods
    for learning the whole graph representation, there remain several challenges and
    limitations unsolved: 1) For hierarchical pooling, most cluster-based methods
    involve the dense assignment matrix, which limits their application to large graphs,
    while TopK-based methods are not good at capturing structure information of the
    graph and may lose information due to node dropping. 2) Most graph pooling methods
    are designed for simple attributed graphs, while pooling algorithms tailored to
    other types of graphs, like dynamic graphs and heterogeneous graphs, are largely
    under-explored.'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战与局限性。尽管图池化方法在学习整个图表示方面取得了巨大成功，但仍存在若干未解决的挑战和局限性：1) 对于层次池化，大多数基于聚类的方法涉及密集分配矩阵，这限制了它们在大图中的应用，而基于
    TopK 的方法不擅长捕捉图的结构信息，可能由于节点丢失而导致信息丢失。2) 大多数图池化方法是针对简单属性图设计的，而针对其他类型图（如动态图和异构图）的池化算法尚未得到充分探索。
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future Works. In the future, we expect that more hybrid or other pooling methods
    can be studied to capture the graph structure information sufficiently as well
    as be efficient for large graphs. In realistic scenarios, there are various types
    of graphs involving dynamic, heterogeneous, or spatial-temporal information. It
    is promising to design graph pooling methods specifically for these graphs, which
    can be beneficial to more real-world applications, such as traffic analysis and
    recommendation systems.
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来的工作。未来，我们期望能研究更多混合或其他池化方法，以充分捕捉图形结构信息，同时对大型图形保持高效。在实际场景中，有各种类型的图形涉及动态、异构或时空信息。为这些图形设计特定的图形池化方法具有前景，这可以有利于更多实际应用，如交通分析和推荐系统。
- en: 6\. Graph Transformer By Junwei Yang
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6. 图形变换器 由 Junwei Yang
- en: Though GNNs based on the message-passing paradigm have achieved impressive performance
    on multiple well-known tasks (Gilmer et al., [2017](#bib.bib116); Xu et al., [2018a](#bib.bib404);
    Wang et al., [2019c](#bib.bib364); Li et al., [2020](#bib.bib206)), they still
    face some intrinsic problems due to the iterative neighbor-aggregation operation.
    Many previous works have demonstrated the two major defects of message-passing
    GNNs, which are known as the over-smoothing and long-distance modeling problem.
    And there are lots of explanatory works trying to mine insights from these two
    issues. The over-smoothing problem can be explained in terms of various GNNs focusing
    only on low-frequency information (Bo et al., [2021](#bib.bib24)), mixing information
    between different kinds of nodes destroying model performance (Chen et al., [2020d](#bib.bib46)),
    GCN is equivalent to Laplacian smoothing (Li et al., [2018a](#bib.bib214)), isotropic
    aggregation among neighbors leading to the same influence distribution as random
    walk (Xu et al., [2018b](#bib.bib405)), etc. The inability to model long-distance
    dependencies of GNNs is partially due to the over-smoothing problem, because in
    the context of conventional neighbor-aggregation GNNs, node information can be
    passed over long distances only through multiple GNN layers. Recently, Alon et
    al. (Alon and Yahav, [2020](#bib.bib7)) find that this problem may also be caused
    by over-squashing, which means the exponential growth of computation paths with
    increasing distance. Though the two basic performance bottlenecks can be tackled
    with elaborate message passing and aggregation strategies, representational power
    of GNNs is inherently bounded by the Weisfeiler-Lehman isomorphism hierarchy (Morris
    et al., [2019](#bib.bib265)). Worse still, most GNNs (Kipf and Welling, [2016a](#bib.bib183);
    Veličković et al., [2017](#bib.bib352); Gilmer et al., [2017](#bib.bib116)) are
    bounded by the simplest first-order Weisfeiler-Lehman test (1-WL). Some efforts
    have been dedicated to break this limitation, such as hypergraph-based (Feng et al.,
    [2019](#bib.bib94); Huang and Yang, [2021](#bib.bib150)), path-based (Cai and
    Lam, [2020](#bib.bib32); Ying et al., [2021](#bib.bib416)), and k-WL-based (Balcilar
    et al., [2021](#bib.bib16); Morris et al., [2019](#bib.bib265)) approaches.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于消息传递范式的GNN在多个著名任务上取得了令人印象深刻的成绩（Gilmer 等，[2017](#bib.bib116)；Xu 等，[2018a](#bib.bib404)；Wang
    等，[2019c](#bib.bib364)；Li 等，[2020](#bib.bib206)），但由于迭代邻居聚合操作，它们仍然面临一些内在问题。许多早期工作展示了消息传递GNN的两个主要缺陷，即过度平滑和长距离建模问题。还有许多解释性工作尝试从这两个问题中挖掘见解。过度平滑问题可以通过多种GNN仅关注低频信息（Bo
    等，[2021](#bib.bib24)）、不同类型节点之间信息混合破坏模型性能（Chen 等，[2020d](#bib.bib46)）、GCN等同于拉普拉斯平滑（Li
    等，[2018a](#bib.bib214)）、邻居之间的各向同性聚合导致与随机游走相同的影响分布（Xu 等，[2018b](#bib.bib405)）等方面来解释。GNN无法建模长距离依赖问题部分是由于过度平滑问题，因为在传统邻居聚合GNN的背景下，节点信息只能通过多个GNN层传递长距离。最近，Alon
    等（Alon 和 Yahav，[2020](#bib.bib7)）发现这个问题也可能是由于过度压缩，即随着距离增加计算路径的指数增长。尽管可以通过精心设计的消息传递和聚合策略解决这两个基本性能瓶颈，但GNN的表示能力本质上受限于Weisfeiler-Lehman同构层次（Morris
    等，[2019](#bib.bib265)）。更糟糕的是，大多数GNN（Kipf 和 Welling，[2016a](#bib.bib183)；Veličković
    等，[2017](#bib.bib352)；Gilmer 等，[2017](#bib.bib116)）受限于最简单的一阶Weisfeiler-Lehman测试（1-WL）。一些努力致力于打破这一限制，如基于超图的方法（Feng
    等，[2019](#bib.bib94)；Huang 和 Yang，[2021](#bib.bib150)）、基于路径的方法（Cai 和 Lam，[2020](#bib.bib32)；Ying
    等，[2021](#bib.bib416)）以及基于k-WL的方法（Balcilar 等，[2021](#bib.bib16)；Morris 等，[2019](#bib.bib265)）。
- en: Among many attempts to solve these fundamental problems, an essential one is
    the adaptation of Transformer (Vaswani et al., [2017](#bib.bib351)) for graph
    representation learning. Transformers, both the vanilla version and several variants,
    have been adopted with impressive results in various deep learning fields including
    NLP (Vaswani et al., [2017](#bib.bib351); Devlin et al., [2018](#bib.bib71)),
    CV (Carion et al., [2020](#bib.bib39); Zhu et al., [2020a](#bib.bib470)), etc.
    Recently, Transformer also shows powerful graph modeling abilities in many researches
    (Dwivedi and Bresson, [2020](#bib.bib82); Kreuzer et al., [2021](#bib.bib194);
    Ying et al., [2021](#bib.bib416); Wu et al., [2021](#bib.bib387); Chen et al.,
    [2022a](#bib.bib47)). And extensive empirical results show that some chronic shortcomings
    in conventional GNNs can be easily overcome in Transformer-based approaches. This
    section gives an overview of the current progress on this kind of methods.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多尝试解决这些基本问题的工作中，一个重要的方法是将 Transformer (Vaswani et al., [2017](#bib.bib351))
    适用于图表示学习。Transformer，无论是原始版本还是几种变体，都在包括 NLP (Vaswani et al., [2017](#bib.bib351);
    Devlin et al., [2018](#bib.bib71))、CV (Carion et al., [2020](#bib.bib39); Zhu
    et al., [2020a](#bib.bib470)) 等多个深度学习领域取得了令人印象深刻的成果。最近，Transformer 在许多研究中也展示了强大的图建模能力
    (Dwivedi and Bresson, [2020](#bib.bib82); Kreuzer et al., [2021](#bib.bib194);
    Ying et al., [2021](#bib.bib416); Wu et al., [2021](#bib.bib387); Chen et al.,
    [2022a](#bib.bib47))。广泛的实证结果表明，传统 GNN 的一些长期缺陷在基于 Transformer 的方法中可以轻松克服。本节概述了这类方法的当前进展。
- en: 6.1\. Transformer
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. Transformer
- en: Transformer (Vaswani et al., [2017](#bib.bib351)) was firstly applied to model
    machine translation, but two of the key mechanisms adopted in this work, attention
    operation and positional encoding, are highly compatible with the graph modeling
    problem.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer (Vaswani et al., [2017](#bib.bib351)) 最初被应用于机器翻译建模，但该工作中采用的两个关键机制，注意力操作和位置编码，与图建模问题高度兼容。
- en: 'To be specific, we denote the input of attention layer in Transformer as $\mathbf{X}=[\mathbf{x}_{0},\mathbf{x}_{1},\ldots,\mathbf{x}_{n-1}]$,
    $\mathbf{x}_{i}\in\mathbb{R}^{d}$, where $n$ is the length of input sequence and
    $d$ is the dimension of each input embedding $\mathbf{x}_{i}$. Then the core operation
    of calculating new embedding $\hat{\mathbf{x}}_{i}$ for each $\mathbf{x}_{i}$
    in attention layer can be streamlined as:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们将 Transformer 中注意力层的输入表示为 $\mathbf{X}=[\mathbf{x}_{0},\mathbf{x}_{1},\ldots,\mathbf{x}_{n-1}]$，其中
    $\mathbf{x}_{i}\in\mathbb{R}^{d}$，$n$ 是输入序列的长度，$d$ 是每个输入嵌入 $\mathbf{x}_{i}$ 的维度。然后，计算注意力层中每个
    $\mathbf{x}_{i}$ 的新嵌入 $\hat{\mathbf{x}}_{i}$ 的核心操作可以简化为：
- en: '| (49) |  | <math   alttext="\begin{gathered}\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{NORM}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\mathcal{Q}^{h}(\mathbf{x}_{i})^{\mathrm{T}}\mathcal{K}^{h}(\mathbf{x}_{k})),\\
    \mathbf{x}_{i}^{h}\ =\mathop{\sum}_{\mathbf{x}_{j}\in\mathbf{X}}{\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})\mathcal{V}^{h}(\mathbf{x}_{j}}),\\'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '| (49) |  | <math   alttext="\begin{gathered}\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{NORM}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\mathcal{Q}^{h}(\mathbf{x}_{i})^{\mathrm{T}}\mathcal{K}^{h}(\mathbf{x}_{k})),\\
    \mathbf{x}_{i}^{h}\ =\mathop{\sum}_{\mathbf{x}_{j}\in\mathbf{X}}{\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})\mathcal{V}^{h}(\mathbf{x}_{j}}),\\'
- en: \hat{\mathbf{x}}_{i}=\text{MERGE}(\mathbf{x}_{i}^{1},\mathbf{x}_{i}^{2},\ldots,\mathbf{x}_{i}^{H}),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd ><mrow ><mrow ><mrow ><msup ><mi mathvariant="normal"  >s</mi><mi >h</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi  >𝐱</mi><mi
    >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >j</mi></msub><mo stretchy="false"
    >)</mo></mrow></mrow><mo >=</mo><mrow ><msub ><mtext >NORM</mtext><mi >j</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><munder
    ><mo lspace="0em" movablelimits="false" rspace="0.167em" >∥</mo><mrow ><msub ><mi
    >𝐱</mi><mi >k</mi></msub><mo >∈</mo><mi >𝐗</mi></mrow></munder><mrow ><msup ><mi
    >𝒬</mi><mi >h</mi></msup><mo lspace="0em" rspace="0em" >​</mo><msup ><mrow ><mo
    stretchy="false" >(</mo><msub ><mi >𝐱</mi><mi >i</mi></msub><mo stretchy="false"
    >)</mo></mrow><mi mathvariant="normal" >T</mi></msup><mo lspace="0em" rspace="0em"
    >​</mo><msup ><mi >𝒦</mi><mi >h</mi></msup><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub ><mi >𝐱</mi><mi >k</mi></msub><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd ><mrow ><mrow ><msubsup ><mi  >𝐱</mi><mi
    >i</mi><mi >h</mi></msubsup><mo lspace="0.778em" rspace="0.111em"  >=</mo><mrow
    ><munder ><mo largeop="false" movablelimits="false"  >∑</mo><mrow ><msub ><mi
    >𝐱</mi><mi >j</mi></msub><mo >∈</mo><mi >𝐗</mi></mrow></munder><mrow ><msup ><mi
    mathvariant="normal" >s</mi><mi >h</mi></msup><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub ><mi >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub
    ><mi >𝐱</mi><mi >j</mi></msub><mo stretchy="false" >)</mo></mrow><mo lspace="0em"
    rspace="0em" >​</mo><msup ><mi >𝒱</mi><mi >h</mi></msup><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi >𝐱</mi><mi >j</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mrow ><msub ><mover accent="true" ><mi  >𝐱</mi><mo >^</mo></mover><mi
    >i</mi></msub><mo >=</mo><mrow ><mtext  >MERGE</mtext><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >𝐱</mi><mi >i</mi><mn
    >1</mn></msubsup><mo >,</mo><msubsup ><mi >𝐱</mi><mi >i</mi><mn >2</mn></msubsup><mo
    >,</mo><mi mathvariant="normal"  >…</mi><mo >,</mo><msubsup ><mi >𝐱</mi><mi >i</mi><mi
    >H</mi></msubsup><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >s</ci><ci
    >ℎ</ci></apply><interval closure="open" ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐱</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐱</ci><ci >𝑗</ci></apply></interval></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci ><mtext >NORM</mtext></ci><ci >𝑗</ci></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >∥</ci><apply ><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝐱</ci><ci >𝑘</ci></apply><ci >𝐗</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝒬</ci><ci >ℎ</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐱</ci><ci >𝑖</ci></apply><ci >T</ci></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝒦</ci><ci >ℎ</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐱</ci><ci >𝑘</ci></apply></apply></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐱</ci><ci >𝑖</ci></apply><ci >ℎ</ci></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐱</ci><ci >𝑗</ci></apply><ci >𝐗</ci></apply></apply><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >s</ci><ci >ℎ</ci></apply><interval closure="open"
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐱</ci><ci >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐱</ci><ci >𝑗</ci></apply></interval><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝒱</ci><ci >ℎ</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐱</ci><ci >𝑗</ci></apply></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><ci >^</ci><ci >𝐱</ci></apply><ci
    >𝑖</ci></apply><apply ><ci ><mtext >MERGE</mtext></ci><vector ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐱</ci><ci >𝑖</ci></apply><cn type="integer" >1</cn></apply><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐱</ci><ci >𝑖</ci></apply><cn type="integer"  >2</cn></apply><ci >…</ci><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐱</ci><ci >𝑖</ci></apply><ci >𝐻</ci></apply></vector></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{gathered}\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{NORM}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\mathcal{Q}^{h}(\mathbf{x}_{i})^{\mathrm{T}}\mathcal{K}^{h}(\mathbf{x}_{k})),\\
    \mathbf{x}_{i}^{h}\ =\mathop{\sum}_{\mathbf{x}_{j}\in\mathbf{X}}{\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})\mathcal{V}^{h}(\mathbf{x}_{j}}),\\
    \hat{\mathbf{x}}_{i}=\text{MERGE}(\mathbf{x}_{i}^{1},\mathbf{x}_{i}^{2},\ldots,\mathbf{x}_{i}^{H}),\end{gathered}</annotation></semantics></math>
    |  |
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: \hat{\mathbf{x}}_{i}=\text{合并}(\mathbf{x}_{i}^{1},\mathbf{x}_{i}^{2},\ldots,\mathbf{x}_{i}^{H})，\\
    \mathbf{x}_{i}^{h}\ =\mathop{\sum}_{\mathbf{x}_{j}\in\mathbf{X}}{\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})\mathcal{V}^{h}(\mathbf{x}_{j}})，\\
    \hat{\mathbf{x}}_{i}=\text{合并}(\mathbf{x}_{i}^{1},\mathbf{x}_{i}^{2},\ldots,\mathbf{x}_{i}^{H})。
- en: 'where $h\in\{0,1,\ldots,H-1\}$ represents the attention head number. $\mathcal{Q}^{h}$,
    $\mathcal{K}^{h}$ and $\mathcal{V}^{h}$ are projection functions mapping a vector
    to the query space, key space and value space respectively. $\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})$
    is score function measuring the similarity between $\mathbf{x}_{i}$ and $\mathbf{x}_{j}$.
    NORM is the normalization operation ensuring $\sum_{\mathbf{x}_{j}\in\mathbf{X}}\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})\equiv
    1$ to propel the stability of the output generated by a stack of attention layers,
    it is usually performed as scaled softmax: $\text{NORM}(\cdot)=\text{SoftMax}(\cdot/\sqrt{d})$.
    And MERGE function is designed to combine the information extracted from multiple
    attention heads. Here, we omit further implementation details that do not affect
    our understanding of attention operation.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h\in\{0,1,\ldots,H-1\}$ 表示注意力头数。$\mathcal{Q}^{h}$、$\mathcal{K}^{h}$ 和 $\mathcal{V}^{h}$
    是将向量映射到查询空间、键空间和值空间的投影函数。$\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})$ 是度量 $\mathbf{x}_{i}$
    和 $\mathbf{x}_{j}$ 相似性的评分函数。NORM 是归一化操作，确保 $\sum_{\mathbf{x}_{j}\in\mathbf{X}}\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})\equiv
    1$，以推动由注意力层堆叠生成的输出的稳定性，通常作为缩放 softmax 执行：$\text{NORM}(\cdot)=\text{SoftMax}(\cdot/\sqrt{d})$。MERGE
    函数旨在结合从多个注意力头提取的信息。这里，我们省略了不影响我们对注意力操作理解的进一步实现细节。
- en: 'The attention process cannot encode the position information of each $\mathbf{x}_{i}$,
    which is essential in machine translation problem. So the positional encoding
    is introduced to remedy this deficiency, and it’s calculated as:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力过程不能编码每个 $\mathbf{x}_{i}$ 的位置信息，这在机器翻译问题中至关重要。因此，引入了位置编码来弥补这一缺陷，其计算公式为：
- en: '| (50) |  | $\begin{gathered}\mathbf{X}^{pos}_{i,2j}=\sin(i/10000^{2j/d}),\
    \mathbf{X}^{pos}_{i,2j+1}=\cos(i/10000^{2j/d}),\end{gathered}$ |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| (50) |  | $\begin{gathered}\mathbf{X}^{pos}_{i,2j}=\sin(i/10000^{2j/d}),\
    \mathbf{X}^{pos}_{i,2j+1}=\cos(i/10000^{2j/d}),\end{gathered}$ |  |'
- en: where $i$ is the position and $j$ is the dimension. The positional encoding
    is added to the input before it is fed to the Transformer.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $i$ 是位置，$j$ 是维度。位置编码在输入被送入 Transformer 之前会被加到输入中。
- en: Table 4\. A Summary of Graph Transformer.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 图 Transformer 的总结。
- en: '| Method | Technique | Capacity |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 技术 | 容量 |'
- en: '| Attention Modification | Encoding Enhancement | Heterogeneous | Long Distance
    | ¿ 1-WL |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 注意力修改 | 编码增强 | 异质 | 长距离 | ¿ 1-WL |'
- en: '| GGT (Dwivedi and Bresson, [2020](#bib.bib82)) | ✓ | ✓ |  | structure only
    | ✓ |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| GGT (Dwivedi 和 Bresson, [2020](#bib.bib82)) | ✓ | ✓ |  | 仅结构 | ✓ |'
- en: '| GTSA (Kreuzer et al., [2021](#bib.bib194)) | ✓ | ✓ |  | ✓ | ✓ |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| GTSA (Kreuzer 等, [2021](#bib.bib194)) | ✓ | ✓ |  | ✓ | ✓ |'
- en: '| HGT (Hu et al., [2020a](#bib.bib148)) | ✓ |  | ✓ |  |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| HGT (Hu 等, [2020a](#bib.bib148)) | ✓ |  | ✓ |  |  |'
- en: '| G2SHGT (Yao et al., [2020](#bib.bib414)) | ✓ |  | ✓ | ✓ |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| G2SHGT (Yao 等, [2020](#bib.bib414)) | ✓ |  | ✓ | ✓ |  |'
- en: '| GRUGT (Cai and Lam, [2020](#bib.bib32)) | ✓ |  |  | ✓ | ✓ |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| GRUGT (Cai 和 Lam, [2020](#bib.bib32)) | ✓ |  |  | ✓ | ✓ |'
- en: '| Graphormer (Ying et al., [2021](#bib.bib416)) | ✓ | ✓ |  | ✓ | ✓ |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| Graphormer (Ying 等, [2021](#bib.bib416)) | ✓ | ✓ |  | ✓ | ✓ |'
- en: '| GSGT (Hussain et al., [2021](#bib.bib154)) |  | ✓ |  | ✓ | ✓ |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| GSGT (Hussain 等, [2021](#bib.bib154)) |  | ✓ |  | ✓ | ✓ |'
- en: '| Graph-BERT (Zhang et al., [2020b](#bib.bib437)) |  | ✓ |  | ✓ | ✓ |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| Graph-BERT (Zhang 等, [2020b](#bib.bib437)) |  | ✓ |  | ✓ | ✓ |'
- en: '| LRGT (Wu et al., [2021](#bib.bib387)) |  | ✓ |  | ✓ |  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| LRGT (Wu 等, [2021](#bib.bib387)) |  | ✓ |  | ✓ |  |'
- en: '| SAT (Chen et al., [2022a](#bib.bib47)) |  | ✓ |  | ✓ | ✓ |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| SAT (Chen 等, [2022a](#bib.bib47)) |  | ✓ |  | ✓ | ✓ |'
- en: 6.2\. Overview
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 概述
- en: From the simplified process shown in Equation. [49](#S6.E49 "In 6.1\. Transformer
    ‣ 6\. Graph Transformer By Junwei Yang ‣ A Comprehensive Survey on Deep Graph
    Representation Learning"), we can see that the core of the attention operation
    is to accomplish information transfer based on the similarity between the source
    and the target to be updated. It’s quite similar to the message-passing process
    on a fully-connected graph. However, direct application of this architecture to
    arbitrary graphs does not make use of structural information, so it may lead to
    poor performance when graph topology is important. On the other hand, the definition
    of positional encoding in graphs is not a trivial problem because the order or
    coordinates of graph nodes are underdefined.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 从公式 [49](#S6.E49 "在 6.1\. Transformer ‣ 6\. Graph Transformer By Junwei Yang
    ‣ 深度图表示学习综合调查") 中展示的简化过程，我们可以看到注意力操作的核心是基于源和目标之间的相似性来完成信息传递。这与完全连接图上的消息传递过程非常相似。然而，将这种架构直接应用于任意图形时，并没有利用结构信息，因此当图形拓扑很重要时可能会导致性能较差。另一方面，图中位置编码的定义并不是一个简单的问题，因为图节点的顺序或坐标定义不充分。
- en: According to these two challenges, Transformer-based methods for graph representation
    learning can be classified into two major categories, one considering graph structure
    during attention process, and the other encoding the topological information of
    the graph into initial node features. We name the first one as Attention Modification
    and the second one as Encoding Enhancement. A summarization is provided in Table
    [4](#S6.T4 "Table 4 ‣ 6.1\. Transformer ‣ 6\. Graph Transformer By Junwei Yang
    ‣ A Comprehensive Survey on Deep Graph Representation Learning"). In the following
    discussion, if both methods are used in one paper, we will list them in different
    subsections, and we will ignore the multi-head trick in attention operation.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这两个挑战，基于 Transformer 的图表示学习方法可以分为两大类，一类在注意力过程中考虑图结构，另一类将图的拓扑信息编码到初始节点特征中。我们将第一类称为注意力修改，将第二类称为编码增强。总结见表
    [4](#S6.T4 "表 4 ‣ 6.1\. Transformer ‣ 6\. 图 Transformer 作者：Junwei Yang ‣ 深度图表示学习的全面调查")。在接下来的讨论中，如果一篇论文中同时使用了这两种方法，我们将它们列在不同的子部分，并且忽略注意力操作中的多头技巧。
- en: 6.3\. Attention Modification
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 注意力修改
- en: This group of works attempt to modify the full attention operation to capture
    structure information.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这组工作尝试修改全注意力操作以捕获结构信息。
- en: 'The most prevalent approach is changing the score function, which is denoted
    as $\mathrm{s}(\cdot,\cdot)$ in Equation [49](#S6.E49 "In 6.1\. Transformer ‣
    6\. Graph Transformer By Junwei Yang ‣ A Comprehensive Survey on Deep Graph Representation
    Learning"). GGT (Dwivedi and Bresson, [2020](#bib.bib82)) constrains each node
    feature can only attend to neighbours and enables model to represent edge feature
    information by rewrite $\mathrm{s}(\cdot,\cdot)$ as:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的方法是改变评分函数，在方程 [49](#S6.E49 "在 6.1\. Transformer ‣ 6\. 图 Transformer 作者：Junwei
    Yang ‣ 深度图表示学习的全面调查")中表示为 $\mathrm{s}(\cdot,\cdot)$。GGT (Dwivedi 和 Bresson，[2020](#bib.bib82))
    限制每个节点特征只能关注邻居，并通过重写 $\mathrm{s}(\cdot,\cdot)$ 来使模型能够表示边特征信息：
- en: '| (51) |  | <math   alttext="\begin{gathered}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &amp;(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}\mathbf{x}_{j}\odot\mathbf{W}^{E}\mathbf{e}_{ji}),&amp;\left<j,i\right>\in
    E\\ &amp;-\infty,&amp;\text{otherwise}\end{aligned}\right.,\\'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '| (51) |  | <math alttext="\begin{gathered}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &amp;(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}\mathbf{x}_{j}\odot\mathbf{W}^{E}\mathbf{e}_{ji}),&amp;\left<j,i\right>\in
    E\\ &amp;-\infty,&amp;\text{otherwise}\end{aligned}\right.,\\'
- en: \mathrm{s}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{SoftMax}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{k})),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd ><mrow ><msub ><mover accent="true" ><mi mathvariant="normal" >s</mi><mo
    >~</mo></mover><mn >1</mn></msub><mrow ><mo stretchy="false" >(</mo><msub ><mi
    >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >j</mi></msub><mo stretchy="false"
    >)</mo></mrow><mo >=</mo><mrow ><mo >{</mo><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd  columnalign="left" ><mrow ><mrow ><msup ><mrow ><mo
    stretchy="false" >(</mo><mrow ><msup ><mi >𝐖</mi><mi >Q</mi></msup><mo lspace="0em"
    rspace="0em" >​</mo><msub ><mi >𝐱</mi><mi >i</mi></msub></mrow><mo stretchy="false"
    >)</mo></mrow><mi mathvariant="normal" >T</mi></msup><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mrow ><mrow ><msup ><mi >𝐖</mi><mi
    >K</mi></msup><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >𝐱</mi><mi >j</mi></msub></mrow><mo
    lspace="0.222em" rspace="0.222em" >⊙</mo><msup ><mi >𝐖</mi><mi >E</mi></msup></mrow><mo
    lspace="0em" rspace="0em" >​</mo><msub ><mi >𝐞</mi><mrow ><mi >j</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >i</mi></mrow></msub></mrow><mo stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="right" ><mrow ><mrow ><mo >⟨</mo><mi >j</mi><mo
    >,</mo><mi >i</mi><mo >⟩</mo></mrow><mo >∈</mo><mi >E</mi></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><mrow ><mo >−</mo><mi mathvariant="normal" >∞</mi></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="right" ><mtext >otherwise</mtext></mtd></mtr></mtable><mo
    >,</mo></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow ><mrow ><mrow ><msub ><mi mathvariant="normal"
    >s</mi><mn >1</mn></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >j</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow ><msub ><mtext >SoftMax</mtext><mi
    >j</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><munder ><mo lspace="0em" movablelimits="false" rspace="0.167em"
    >∥</mo><mrow ><msub ><mi >𝐱</mi><mi >k</mi></msub><mo >∈</mo><mi >𝐗</mi></mrow></munder><mrow
    ><msub ><mover accent="true" ><mi mathvariant="normal" >s</mi><mo >~</mo></mover><mn
    >1</mn></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >k</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" >\begin{gathered}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}\mathbf{x}_{j}\odot\mathbf{W}^{E}\mathbf{e}_{ji}),&\left<j,i\right>\in
    E\\ &-\infty,&\text{otherwise}\end{aligned}\right.,\\ \mathrm{s}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{SoftMax}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{k})),\end{gathered}</annotation></semantics></math>
    |  |
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{s}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{SoftMax}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{k})),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd ><mrow ><msub ><mover accent="true" ><mi mathvariant="normal" >s</mi><mo
    >~</mo></mover><mn >1</mn></msub><mrow ><mo stretchy="false" >(</mo><msub ><mi
    >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >j</mi></msub><mo stretchy="false"
    >)</mo></mrow><mo >=</mo><mrow ><mo >{</mo><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd  columnalign="left" ><mrow ><mrow ><msup ><mrow ><mo
    stretchy="false" >(</mo><mrow ><msup ><mi >𝐖</mi><mi >Q</mi></msup><mo lspace="0em"
    rspace="0em" >​</mo><msub ><mi >𝐱</mi><mi >i</mi></msub></mrow><mo stretchy="false"
    >)</mo></mrow><mi mathvariant="normal" >T</mi></msup><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mrow ><mrow ><msup ><mi >𝐖</mi><mi
    >K</mi></msup><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >𝐱</mi><mi >j</mi></msub></mrow><mo
    lspace="0.222em" rspace="0.222em" >⊙</mo><msup ><mi >𝐖</mi><mi >E</mi></msup></mrow><mo
    lspace="0em" rspace="0em" >​</mo><msub ><mi >𝐞</mi><mrow ><mi >j</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >i</mi></mrow></msub></mrow><mo stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="right" ><mrow ><mrow ><mo >⟨</mo><mi >j</mi><mo
    >,</mo><mi >i</mi><mo >⟩</mo></mrow><mo >∈</mo><mi >E</mi></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><mrow ><mo >−</mo><mi mathvariant="normal" >∞</mi></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="right" ><mtext >otherwise</mtext></mtd></mtr></mtable><mo
    >,</mo></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow ><mrow ><mrow ><msub ><mi mathvariant="normal"
    >s</mi><mn >1</mn></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >j</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow ><msub ><mtext >SoftMax</mtext><mi
    >j</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><munder ><mo lspace="0em" movablelimits="false" rspace="0.167em"
    >∥</mo><mrow ><msub ><mi >𝐱</mi><mi >k</mi></msub><mo >∈</mo><mi >𝐗</mi></mrow></munder><mrow
    ><msub ><mover accent="true" ><mi mathvariant="normal" >s</mi><mo >~</mo></mover><mn
    >1</mn></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >k</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" >\begin{gathered}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}\mathbf{x}_{j}\odot\mathbf{W}^{E}\mathbf{e}_{ji}),&\left<j,i\right>\in
    E\\ &-\infty,&\text{otherwise}\end{aligned}\right.,\\ \mathrm{s}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{SoftMax}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{k})),\end{gathered}</annotation></semantics></math>
    |  |
- en: 'where $\odot$ is Hadamard product and $\mathbf{W}^{Q,K,E}$ represents trainable
    parameter matrix. This approach is not efficient yet to model long-distance dependencies
    since only 1st-neighbors are considered. Though it adopts Laplacian eigenvectors
    to gather global information (cf. Section [6.4](#S6.SS4 "6.4\. Encoding Enhancement
    ‣ 6\. Graph Transformer By Junwei Yang ‣ A Comprehensive Survey on Deep Graph
    Representation Learning")), but only long-distance structure information is remedied
    while the node and edge features are not. GTSA (Kreuzer et al., [2021](#bib.bib194))
    improves this approach by combining the original graph and the full graph. Specifically,
    it extends $\mathrm{s}_{1}(\cdot,\cdot)$ to:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\odot$ 是 Hadamard 乘积，$\mathbf{W}^{Q,K,E}$ 表示可训练的参数矩阵。由于仅考虑了第一层邻居，这种方法在建模远距离依赖关系方面效率较低。尽管它采用了拉普拉斯特征向量来收集全局信息（参见第
    [6.4](#S6.SS4 "6.4\. 编码增强 ‣ 6\. 图转换器 by Junwei Yang ‣ 深度图表示学习的全面综述") 节），但仅修正了远距离结构信息，而节点和边特征没有得到处理。GTSA（Kreuzer
    等，[2021](#bib.bib194)）通过结合原始图和完整图改进了这种方法。具体来说，它将 $\mathrm{s}_{1}(\cdot,\cdot)$
    扩展为：
- en: '| (52) |  | <math   alttext="\begin{gathered}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &amp;(\mathbf{W}^{Q}_{1}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{1}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{1}\mathbf{e}_{ji}),&amp;\left<j,i\right>\in
    E\\ &amp;(\mathbf{W}^{Q}_{0}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{0}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{0}\mathbf{e}_{ji}),&amp;\text{otherwise}\end{aligned}\right.,\\'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '| (52) |  | <math   alttext="\begin{gathered}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &amp;(\mathbf{W}^{Q}_{1}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{1}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{1}\mathbf{e}_{ji}),&amp;\left<j,i\right>\in
    E\\ &amp;(\mathbf{W}^{Q}_{0}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{0}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{0}\mathbf{e}_{ji}),&amp;\text{otherwise}\end{aligned}\right.,\\'
- en: \mathrm{s}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned} &amp;\frac{1}{1+\lambda}\text{SoftMax}_{j}(\mathop{\&#124;}_{\left<k,i\right>\in
    E}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{k})),&amp;\left<j,i\right>\in
    E\\
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: \mathrm{s}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned} &amp;\frac{1}{1+\lambda}\text{SoftMax}_{j}(\mathop{\&#124;}_{\left<k,i\right>\in
    E}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{k})),&amp;\left<j,i\right>\in
    E\\
- en: '&amp;\frac{\lambda}{1+\lambda}\text{SoftMax}_{j}(\mathop{\&#124;}_{\left<k,i\right>\not\in
    E}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{k})),&amp;\text{otherwise}\end{aligned}\right.,\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd ><mrow ><msub ><mover accent="true" ><mi mathvariant="normal" >s</mi><mo
    >~</mo></mover><mn >2</mn></msub><mrow ><mo stretchy="false" >(</mo><msub ><mi
    >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >j</mi></msub><mo stretchy="false"
    >)</mo></mrow><mo >=</mo><mrow ><mo >{</mo><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd  columnalign="left" ><mrow ><mrow ><msup ><mrow ><mo
    stretchy="false" >(</mo><mrow ><msubsup ><mi >𝐖</mi><mn >1</mn><mi >Q</mi></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><msub ><mi >𝐱</mi><mi >i</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow><mi mathvariant="normal" >T</mi></msup><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mrow ><mrow ><msubsup
    ><mi >𝐖</mi><mn >1</mn><mi >K</mi></msubsup><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >𝐱</mi><mi >j</mi></msub></mrow><mo lspace="0.222em" rspace="0.222em" >⊙</mo><msubsup
    ><mi >𝐖</mi><mn >1</mn><mi >E</mi></msubsup></mrow><mo lspace="0em" rspace="0em"
    >​</mo><msub ><mi >𝐞</mi><mrow ><mi >j</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >i</mi></mrow></msub></mrow><mo stretchy="false" >)</mo></mrow></mrow><mo >,</mo></mrow></mtd><mtd
    columnalign="right" ><mrow ><mrow ><mo >⟨</mo><mi >j</mi><mo >,</mo><mi >i</mi><mo
    >⟩</mo></mrow><mo >∈</mo><mi >E</mi></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mrow ><msup ><mrow ><mo stretchy="false" >(</mo><mrow ><msubsup ><mi
    >𝐖</mi><mn >0</mn><mi >Q</mi></msubsup><mo lspace="0em" rspace="0em" >​</mo><msub
    ><mi >𝐱</mi><mi >i</mi></msub></mrow><mo stretchy="false" >)</mo></mrow><mi mathvariant="normal"
    >T</mi></msup><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mrow ><mrow ><msubsup ><mi >𝐖</mi><mn >0</mn><mi >K</mi></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><msub ><mi >𝐱</mi><mi >j</mi></msub></mrow><mo
    lspace="0.222em" rspace="0.222em" >⊙</mo><msubsup ><mi >𝐖</mi><mn >0</mn><mi >E</mi></msubsup></mrow><mo
    lspace="0em" rspace="0em" >​</mo><msub ><mi >𝐞</mi><mrow ><mi >j</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >i</mi></mrow></msub></mrow><mo stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="right" ><mtext >otherwise</mtext></mtd></mtr></mtable><mo
    >,</mo></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow ><msub ><mi mathvariant="normal"
    >s</mi><mn >2</mn></msub><mrow ><mo stretchy="false" >(</mo><msub ><mi >𝐱</mi><mi
    >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >j</mi></msub><mo stretchy="false"
    >)</mo></mrow><mo >=</mo><mrow ><mo >{</mo><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd columnalign="left" ><mrow ><mrow ><mfrac ><mn >1</mn><mrow
    ><mn >1</mn><mo >+</mo><mi >λ</mi></mrow></mfrac><mo lspace="0em" rspace="0em"
    >​</mo><msub ><mtext >SoftMax</mtext><mi >j</mi></msub><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><munder ><mo lspace="0em" movablelimits="false"
    rspace="0.167em" >∥</mo><mrow ><mrow ><mo >⟨</mo><mi >k</mi><mo >,</mo><mi >i</mi><mo
    >⟩</mo></mrow><mo >∈</mo><mi >E</mi></mrow></munder><mrow ><msub ><mover accent="true"
    ><mi mathvariant="normal" >s</mi><mo >~</mo></mover><mn >2</mn></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi >𝐱</mi><mi >i</mi></msub><mo
    >,</mo><msub ><mi >𝐱</mi><mi >k</mi></msub><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo >,</mo></mrow></mtd><mtd columnalign="right"
    ><mrow ><mrow ><mo >⟨</mo><mi >j</mi><mo >,</mo><mi >i</mi><mo >⟩</mo></mrow><mo
    >∈</mo><mi >E</mi></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow ><mrow
    ><mfrac ><mi >λ</mi><mrow ><mn >1</mn><mo >+</mo><mi >λ</mi></mrow></mfrac><mo
    lspace="0em" rspace="0em" >​</mo><msub ><mtext >SoftMax</mtext><mi >j</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><munder
    ><mo lspace="0em" movablelimits="false" rspace="0.167em" >∥</mo><mrow ><mrow ><mo
    >⟨</mo><mi >k</mi><mo >,</mo><mi >i</mi><mo >⟩</mo></mrow><mo >∉</mo><mi >E</mi></mrow></munder><mrow
    ><msub ><mover accent="true" ><mi mathvariant="normal" >s</mi><mo >~</mo></mover><mn
    >2</mn></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >k</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="right" ><mtext >otherwise</mtext></mtd></mtr></mtable><mo
    >,</mo></mrow></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex"
    >\begin{gathered}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &(\mathbf{W}^{Q}_{1}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{1}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{1}\mathbf{e}_{ji}),&\left<j,i\right>\in
    E\\ &(\mathbf{W}^{Q}_{0}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{0}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{0}\mathbf{e}_{ji}),&\text{otherwise}\end{aligned}\right.,\\
    \mathrm{s}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned} &\frac{1}{1+\lambda}\text{SoftMax}_{j}(\mathop{\&#124;}_{\left<k,i\right>\in
    E}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{k})),&\left<j,i\right>\in
    E\\ &\frac{\lambda}{1+\lambda}\text{SoftMax}_{j}(\mathop{\&#124;}_{\left<k,i\right>\not\in
    E}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{k})),&\text{otherwise}\end{aligned}\right.,\end{gathered}</annotation></semantics></math>
    |  |'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: \(\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &(\mathbf{W}^{Q}_{1}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{1}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{1}\mathbf{e}_{ji}),&\left<j,i\right>\in
    E\\ &(\mathbf{W}^{Q}_{0}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{0}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{0}\mathbf{e}_{ji}),&\text{其他情况}\end{aligned}\right.,\\
    \mathrm{s}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned} &\frac{1}{1+\lambda}\text{SoftMax}_{j}(\mathop{\&#124;}_{\left<k,i\right>\in
    E}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{k})),&\left<j,i\right>\in
    E\\ &\frac{\lambda}{1+\lambda}\text{SoftMax}_{j}(\mathop{\&#124;}_{\left<k,i\right>\not\in
    E}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{k})),&\text{其他情况}\end{aligned}\right.,\)
- en: where $\lambda$ is a hyperparameter representing the strength of full connection.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是一个超参数，表示全连接的强度。
- en: Some works try to reduce information-mixing problems (Chen et al., [2020d](#bib.bib46))
    in heterogeneous graph. HGT (Hu et al., [2020a](#bib.bib148)) disentangles the
    attention of different node type and edge type by adopting additional attention
    heads. It defines $\mathbf{W}_{Q,K,V}^{\tau(v)}$ for each node type $\tau(v)$
    and $\mathbf{W}_{E}^{\phi(e)}$ for each edge type $\phi(e)$, $\tau(\cdot)$ and
    $\phi(\cdot)$ are type indicating function. G2SHGT (Yao et al., [2020](#bib.bib414))
    defines four types of subgraphs, fully-connected, connected, default and reverse,
    to capture global, undirected, forward and backward information respectively.
    And each subgraph is homogeneous, so it can reduce interactions between different
    classes.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究试图减少异质图中的信息混合问题 (Chen 等, [2020d](#bib.bib46))。HGT (Hu 等, [2020a](#bib.bib148))
    通过采用额外的注意力头来解开不同节点类型和边类型的注意力。它为每个节点类型 $\tau(v)$ 定义 $\mathbf{W}_{Q,K,V}^{\tau(v)}$
    和为每个边类型 $\phi(e)$ 定义 $\mathbf{W}_{E}^{\phi(e)}$，$\tau(\cdot)$ 和 $\phi(\cdot)$
    是类型指示函数。G2SHGT (Yao 等, [2020](#bib.bib414)) 定义了四种类型的子图：完全连接、连接、默认和反向，分别捕捉全局、无向、前向和后向信息。每个子图都是同质的，因此可以减少不同类别之间的交互。
- en: 'Path features between nodes are always treated as inductive bias added to the
    original score function. Let $\text{SP}_{ij}=(e_{1},e_{2},\ldots,e_{N})$ denote
    the shortest path between node pair $(v_{i},v_{j})$. GRUGT (Cai and Lam, [2020](#bib.bib32))
    uses GRU (Chung et al., [2014](#bib.bib62)) to encode forward and backward features
    as: $\mathbf{r}_{ij}=\text{GRU}(\text{SP}_{ij})$, $\mathbf{r}_{ji}=\text{GRU}(\text{SP}_{ji})$.
    Then, the final attention score is calculated by adding up four components:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 节点之间的路径特征通常被视为对原始得分函数的归纳偏差。设 $\text{SP}_{ij}=(e_{1},e_{2},\ldots,e_{N})$ 表示节点对
    $(v_{i},v_{j})$ 之间的最短路径。GRUGT (Cai 和 Lam, [2020](#bib.bib32)) 使用 GRU (Chung 等,
    [2014](#bib.bib62)) 来编码前向和后向特征：$\mathbf{r}_{ij}=\text{GRU}(\text{SP}_{ij})$, $\mathbf{r}_{ji}=\text{GRU}(\text{SP}_{ji})$。然后，最终的注意力得分通过四个组件的加和计算得出：
- en: '| (53) |  | $\begin{gathered}\tilde{\mathrm{s}}_{3}(\mathbf{x}_{i},\mathbf{x}_{j})=(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{x}_{j}+(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{r}_{ji}+(\mathbf{W}^{Q}\mathbf{r}_{ij})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{x}_{j}+(\mathbf{W}^{Q}\mathbf{r}_{ij})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{r}_{ji},\end{gathered}$
    |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| (53) |  | $\begin{gathered}\tilde{\mathrm{s}}_{3}(\mathbf{x}_{i},\mathbf{x}_{j})=(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{x}_{j}+(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{r}_{ji}+(\mathbf{W}^{Q}\mathbf{r}_{ij})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{x}_{j}+(\mathbf{W}^{Q}\mathbf{r}_{ij})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{r}_{ji},\end{gathered}$
    |  |'
- en: 'from front to back, which represent content-based score, source-dependent bias,
    target-dependent bias and universal bias respectively. Graphormer (Ying et al.,
    [2021](#bib.bib416)) uses both path length and path embedding to introduce structural
    bias as:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 从前到后，分别表示基于内容的得分、源依赖偏差、目标依赖偏差和通用偏差。Graphormer (Ying 等, [2021](#bib.bib416))
    使用路径长度和路径嵌入来引入结构偏差，如下所示：
- en: '| (54) |  | <math   alttext="\begin{gathered}\tilde{\mathrm{s}}_{4}(\mathbf{x}_{i},\mathbf{x}_{j})=(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{x}_{j}/\sqrt{d}+b_{N}+c_{ij},\\
    c_{ij}=\frac{1}{N}\sum_{k=1}^{N}(\mathbf{e}_{k})^{\mathrm{T}}\mathbf{w}^{E}_{k},\\'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '| (54) |  | <math   alttext="\begin{gathered}\tilde{\mathrm{s}}_{4}(\mathbf{x}_{i},\mathbf{x}_{j})=(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{x}_{j}/\sqrt{d}+b_{N}+c_{ij},\\
    c_{ij}=\frac{1}{N}\sum_{k=1}^{N}(\mathbf{e}_{k})^{\mathrm{T}}\mathbf{w}^{E}_{k},\\'
- en: \mathrm{s}_{4}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{SoftMax}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\tilde{\mathrm{s}}_{4}(\mathbf{x}_{i},\mathbf{x}_{k})),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd ><mrow ><mrow ><mrow ><msub ><mover accent="true"  ><mi mathvariant="normal"  >s</mi><mo
    >~</mo></mover><mn >4</mn></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><msub ><mi  >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub
    ><mi >𝐱</mi><mi >j</mi></msub><mo stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow
    ><mrow ><mrow ><msup ><mrow ><mo stretchy="false" >(</mo><mrow ><msup ><mi >𝐖</mi><mi
    >Q</mi></msup><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >𝐱</mi><mi >i</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow><mi mathvariant="normal" >T</mi></msup><mo lspace="0em"
    rspace="0em" >​</mo><msup ><mi >𝐖</mi><mi >K</mi></msup><mo lspace="0em" rspace="0em"
    >​</mo><msub ><mi >𝐱</mi><mi >j</mi></msub></mrow><mo >/</mo><msqrt ><mi >d</mi></msqrt></mrow><mo
    >+</mo><msub ><mi >b</mi><mi >N</mi></msub><mo >+</mo><msub ><mi >c</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow></msub></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd ><mrow ><mrow ><msub ><mi  >c</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow></msub><mo >=</mo><mrow
    ><mfrac ><mn >1</mn><mi >N</mi></mfrac><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><munderover ><mo movablelimits="false" rspace="0em" >∑</mo><mrow ><mi >k</mi><mo
    >=</mo><mn >1</mn></mrow><mi >N</mi></munderover><mrow ><msup ><mrow ><mo stretchy="false"
    >(</mo><msub ><mi >𝐞</mi><mi >k</mi></msub><mo stretchy="false" >)</mo></mrow><mi
    mathvariant="normal"  >T</mi></msup><mo lspace="0em" rspace="0em" >​</mo><msubsup
    ><mi >𝐰</mi><mi >k</mi><mi >E</mi></msubsup></mrow></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mrow ><mrow ><msub ><mi mathvariant="normal"  >s</mi><mn >4</mn></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi
    >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >j</mi></msub><mo stretchy="false"
    >)</mo></mrow></mrow><mo >=</mo><mrow ><msub ><mtext >SoftMax</mtext><mi >j</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><munder
    ><mo lspace="0em" movablelimits="false" rspace="0.167em" >∥</mo><mrow ><msub ><mi
    >𝐱</mi><mi >k</mi></msub><mo >∈</mo><mi >𝐗</mi></mrow></munder><mrow ><msub ><mover
    accent="true" ><mi mathvariant="normal" >s</mi><mo >~</mo></mover><mn >4</mn></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msub ><mi
    >𝐱</mi><mi >i</mi></msub><mo >,</mo><msub ><mi >𝐱</mi><mi >k</mi></msub><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" ><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><apply ><ci >~</ci><ci  >s</ci></apply><cn
    type="integer"  >4</cn></apply><interval closure="open" ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐱</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐱</ci><ci >𝑗</ci></apply></interval></apply><apply ><apply
    ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐖</ci><ci >𝑄</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐱</ci><ci >𝑖</ci></apply></apply><ci
    >T</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐖</ci><ci
    >𝐾</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐱</ci><ci
    >𝑗</ci></apply></apply><apply ><ci >𝑑</ci></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑏</ci><ci >𝑁</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑐</ci><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑐</ci><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply
    ><apply ><cn type="integer" >1</cn><ci >𝑁</ci></apply><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><ci >𝑘</ci><cn type="integer" >1</cn></apply></apply><ci >𝑁</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐞</ci><ci >𝑘</ci></apply><ci >T</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐰</ci><ci >𝐸</ci></apply><ci >𝑘</ci></apply></apply></apply></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >s</ci><cn type="integer"
    >4</cn></apply><interval closure="open" ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐱</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐱</ci><ci >𝑗</ci></apply></interval></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci ><mtext >SoftMax</mtext></ci><ci >𝑗</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >∥</ci><apply ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐱</ci><ci >𝑘</ci></apply><ci
    >𝐗</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><ci >~</ci><ci >s</ci></apply><cn type="integer" >4</cn></apply><interval closure="open"
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐱</ci><ci >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐱</ci><ci >𝑘</ci></apply></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{gathered}\tilde{\mathrm{s}}_{4}(\mathbf{x}_{i},\mathbf{x}_{j})=(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{x}_{j}/\sqrt{d}+b_{N}+c_{ij},\\
    c_{ij}=\frac{1}{N}\sum_{k=1}^{N}(\mathbf{e}_{k})^{\mathrm{T}}\mathbf{w}^{E}_{k},\\
    \mathrm{s}_{4}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{SoftMax}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\tilde{\mathrm{s}}_{4}(\mathbf{x}_{i},\mathbf{x}_{k})),\end{gathered}</annotation></semantics></math>
    |  |
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: where $b_{N}$ is a trainable scalar indexed by $N$, the length of $\text{SP}_{ij}$
    . $\mathbf{e}_{k}$ is the embedding of the the edge $e_{k}$, and $\mathbf{w}^{E}_{k}\in\mathbb{R}^{d}$
    is the $k$-th edge parameter. If $\text{SP}_{ij}$ does not exist, then $b_{N}$
    and $c_{ij}$ are set to be special value.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b_{N}$ 是一个由 $N$ 索引的可训练标量，$\text{SP}_{ij}$ 的长度。$\mathbf{e}_{k}$ 是边 $e_{k}$
    的嵌入，$\mathbf{w}^{E}_{k}\in\mathbb{R}^{d}$ 是第 $k$ 条边的参数。如果 $\text{SP}_{ij}$ 不存在，则
    $b_{N}$ 和 $c_{ij}$ 被设置为特殊值。
- en: 6.4\. Encoding Enhancement
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 编码增强
- en: This kind of methods intend to enhance initial node representations to enable
    Transformer to encode structure information. They can be further divided into
    two categories, position-analogy methods and structure-aware methods.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这类方法旨在增强初始节点表示，以使 Transformer 能够编码结构信息。它们可以进一步分为两类，位置类比方法和结构感知方法。
- en: 6.4.1\. Position-analogy methods
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1\. 位置类比方法
- en: In Euclidean space, the Laplacian operator corresponds to the divergence of
    the gradient, whose eigenfunctions are sine/cosine functions. For graph, the Laplacian
    operator is Laplacian matrix, whose eigenvectors can be considered as eigenfunctions.
    Hence, inspired by Equation [50](#S6.E50 "In 6.1\. Transformer ‣ 6\. Graph Transformer
    By Junwei Yang ‣ A Comprehensive Survey on Deep Graph Representation Learning"),
    position-analogy methods utilize Laplacian eigenvectors to simulate positional
    encoding $\mathbf{X}^{pos}$ as they are the equivalents of sine/cosine functions.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在欧几里得空间中，拉普拉斯算子对应于梯度的散度，其特征函数是正弦/余弦函数。对于图，拉普拉斯算子是拉普拉斯矩阵，其特征向量可以视为特征函数。因此，受到方程
    [50](#S6.E50 "在 6.1\. Transformer ‣ 6\. Graph Transformer 由 Junwei Yang ‣ 深度图表示学习的综合调查")
    启发，位置类比方法利用拉普拉斯特征向量来模拟位置编码 $\mathbf{X}^{pos}$，因为它们相当于正弦/余弦函数。
- en: 'Laplacian eigenvectors can be calculated through the eigendecomposition of
    normalized graph Laplacian matrix $\tilde{\mathbf{L}}$:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯特征向量可以通过归一化图拉普拉斯矩阵 $\tilde{\mathbf{L}}$ 的特征分解来计算：
- en: '| (55) |  | $\tilde{\mathbf{L}}\triangleq\mathbf{I}-\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\text{T}},$
    |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| (55) |  | $\tilde{\mathbf{L}}\triangleq\mathbf{I}-\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\text{T}},$
    |  |'
- en: 'where $\mathbf{A}$ is the adjacency matrix, $\mathbf{D}$ is the degree matrix,
    $\mathbf{U}=[\mathbf{u}_{1},\mathbf{u}_{2},\ldots,\mathbf{u}_{n-1}]$ are eigenvectors
    and $\mathbf{\Lambda}=diag(\lambda_{0},\lambda_{1},\ldots,\lambda_{n-1})$ are
    eigenvalues. With $\mathbf{U}$ and $\mathbf{\Lambda}$, GGT (Dwivedi and Bresson,
    [2020](#bib.bib82)) uses eigenvectors of the k smallest non-trivial eigenvalues
    to denote the intermediate embedding $\mathbf{X}^{mid}\in\mathbb{R}^{n\times k}$,
    and maps it to d-dimensional space and gets the position encoding $\mathbf{X}^{pos}\in\mathbb{R}^{n\times
    d}$. This process can be formulized as:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{A}$ 是邻接矩阵，$\mathbf{D}$ 是度矩阵，$\mathbf{U}=[\mathbf{u}_{1},\mathbf{u}_{2},\ldots,\mathbf{u}_{n-1}]$
    是特征向量，$\mathbf{\Lambda}=diag(\lambda_{0},\lambda_{1},\ldots,\lambda_{n-1})$ 是特征值。利用
    $\mathbf{U}$ 和 $\mathbf{\Lambda}$，GGT (Dwivedi 和 Bresson, [2020](#bib.bib82))
    使用 $k$ 个最小非平凡特征值的特征向量来表示中间嵌入 $\mathbf{X}^{mid}\in\mathbb{R}^{n\times k}$，并将其映射到
    $d$ 维空间中，从而得到位置编码 $\mathbf{X}^{pos}\in\mathbb{R}^{n\times d}$。这个过程可以被形式化为：
- en: '| (56) |  | <math   alttext="\begin{gathered}index=\text{argmin}_{k}(\{\lambda_{i}&#124;0\leq
    i<n\wedge\lambda_{i}>0\}),\\ \mathbf{X}^{mid}=[\mathbf{u}_{index_{0}},\mathbf{u}_{index_{1}},\ldots,\mathbf{u}_{index_{k-1}}]^{\text{T}},\\'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '| (56) |  | <math alttext="\begin{gathered}index=\text{argmin}_{k}(\{\lambda_{i}&#124;0\leq
    i<n\wedge\lambda_{i}>0\}),\\ \mathbf{X}^{mid}=[\mathbf{u}_{index_{0}},\mathbf{u}_{index_{1}},\ldots,\mathbf{u}_{index_{k-1}}]^{\text{T}},\\'
- en: \mathbf{X}^{pos}=\mathbf{X}^{mid}\mathbf{W}^{k\times d},\end{gathered}" display="block"><semantics
    ><mtable displaystyle="true" rowspacing="0pt" ><mtr ><mtd ><mrow ><mrow ><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >x</mi></mrow><mo >=</mo><mrow ><msub ><mtext  >argmin</mtext><mi
    >k</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mo stretchy="false" >{</mo><msub ><mi >λ</mi><mi >i</mi></msub><mo
    lspace="0em" rspace="0em" >&#124;</mo><mrow ><mn >0</mn><mo >≤</mo><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo fence="true" rspace="0em"
    ><</mo><mrow ><mi >n</mi><mo >∧</mo><msub ><mi >λ</mi><mi >i</mi></msub></mrow><mo
    fence="true" lspace="0em" >></mo></mrow><mo lspace="0em" rspace="0em" >​</mo><mn
    >0</mn></mrow></mrow><mo stretchy="false" >}</mo></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd ><mrow ><mrow ><msup ><mi  >𝐗</mi><mrow ><mi
    >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >d</mi></mrow></msup><mo >=</mo><msup ><mrow ><mo stretchy="false"
    >[</mo><msub ><mi >𝐮</mi><mrow ><mi  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >e</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >x</mi><mn >0</mn></msub></mrow></msub><mo
    >,</mo><msub ><mi >𝐮</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >n</mi><mo lspace="0em" rspace="0em" >​</mo><mi >d</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >e</mi><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >x</mi><mn >1</mn></msub></mrow></msub><mo
    >,</mo><mi mathvariant="normal"  >…</mi><mo >,</mo><msub ><mi >𝐮</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >x</mi><mrow ><mi >k</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow></msub><mo
    stretchy="false" >]</mo></mrow><mtext >T</mtext></msup></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mrow ><msup ><mi >𝐗</mi><mrow ><mi >p</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >s</mi></mrow></msup><mo
    >=</mo><mrow ><msup ><mi  >𝐗</mi><mrow ><mi >m</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >d</mi></mrow></msup><mo
    lspace="0em" rspace="0em" >​</mo><msup ><mi >𝐖</mi><mrow ><mi >k</mi><mo lspace="0.222em"
    rspace="0.222em" >×</mo><mi >d</mi></mrow></msup></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><ci >𝑖</ci><ci >𝑛</ci><ci  >𝑑</ci><ci >𝑒</ci><ci >𝑥</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci ><mtext >argmin</mtext></ci><ci
    >𝑘</ci></apply><apply ><csymbol cd="latexml" >conditional-set</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜆</ci><ci >𝑖</ci></apply><apply
    ><cn type="integer" >0</cn><apply ><ci >𝑖</ci><apply ><csymbol cd="latexml" >expectation</csymbol><apply
    ><ci >𝑛</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜆</ci><ci
    >𝑖</ci></apply></apply></apply><cn type="integer"  >0</cn></apply></apply></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐗</ci><apply ><ci >𝑚</ci><ci >𝑖</ci><ci
    >𝑑</ci></apply></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><list
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐮</ci><apply ><ci >𝑖</ci><ci
    >𝑛</ci><ci >𝑑</ci><ci >𝑒</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><cn type="integer" >0</cn></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐮</ci><apply ><ci >𝑖</ci><ci >𝑛</ci><ci >𝑑</ci><ci >𝑒</ci><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><cn type="integer" >1</cn></apply></apply></apply><ci
    >…</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐮</ci><apply ><ci
    >𝑖</ci><ci >𝑛</ci><ci >𝑑</ci><ci >𝑒</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><apply ><ci >𝑘</ci><cn type="integer"  >1</cn></apply></apply></apply></apply></list><ci
    ><mtext mathsize="70%"  >T</mtext></ci></apply></apply><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐗</ci><apply ><ci >𝑝</ci><ci >𝑜</ci><ci
    >𝑠</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐗</ci><apply ><ci >𝑚</ci><ci >𝑖</ci><ci >𝑑</ci></apply></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐖</ci><apply ><ci >𝑘</ci><ci >𝑑</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{gathered}index=\text{argmin}_{k}(\{\lambda_{i}&#124;0\leq
    i<n\wedge\lambda_{i}>0\}),\\ \mathbf{X}^{mid}=[\mathbf{u}_{index_{0}},\mathbf{u}_{index_{1}},\ldots,\mathbf{u}_{index_{k-1}}]^{\text{T}},\\
    \mathbf{X}^{pos}=\mathbf{X}^{mid}\mathbf{W}^{k\times d},\end{gathered}</annotation></semantics></math>
    |  |
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: \mathbf{X}^{pos}=\mathbf{X}^{mid}\mathbf{W}^{k\times d},\end{gathered}" display="block"><semantics
    ><mtable displaystyle="true" rowspacing="0pt" ><mtr ><mtd ><mrow ><mrow ><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >x</mi></mrow><mo >=</mo><mrow ><msub ><mtext  >argmin</mtext><mi
    >k</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mo stretchy="false" >{</mo><msub ><mi >λ</mi><mi >i</mi></msub><mo
    lspace="0em" rspace="0em" >&#124;</mo><mrow ><mn >0</mn><mo >≤</mo><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo fence="true" rspace="0em"
    ><</mo><mrow ><mi >n</mi><mo >∧</mo><msub ><mi >λ</mi><mi >i</mi></msub></mrow><mo
    fence="true" lspace="0em" >></mo></mrow><mo lspace="0em" rspace="0em" >​</mo><mn
    >0</mn></mrow></mrow><mo stretchy="false" >}</mo></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd ><mrow ><mrow ><msup ><mi  >𝐗</mi><mrow ><mi
    >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >d</mi></mrow></msup><mo >=</mo><msup ><mrow ><mo stretchy="false"
    >[</mo><msub ><mi >𝐮</mi><mrow ><mi  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >e</mi><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >x</mi><mn >0</mn></msub></mrow></msub><mo
    >,</mo><msub ><mi >𝐮</mi><mrow ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >n</mi><mo lspace="0em" rspace="0em" >​</mo><mi >d</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >e</mi><mo lspace="0em" rspace="0em" >​</mo><msub ><mi >x</mi><mn >1</mn></msub></mrow></msub><mo
    >,</mo><mi mathvariant="normal"  >…</mi><mo >,</mo><msub ><mi >𝐮</mi><mrow ><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >x</mi><mrow ><mi >k</mi><mo >−</mo><mn >1</mn></mrow></msub></mrow></msub><mo
    stretchy="false" >]</mo></mrow><mtext >T</mtext></msup></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mrow ><msup ><mi >𝐗</mi><mrow ><mi >p</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >s</mi></mrow></msup><mo
    >=</mo><mrow ><msup ><mi  >𝐗</mi><mrow ><mi >m</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >d</mi></mrow></msup><mo
    lspace="0em" rspace="0em" >​</mo><msup ><mi >𝐖</mi><mrow ><mi >k</mi><mo lspace="0.222em"
    rspace="0.222em" >×</mo><mi >d</mi></mrow></msup></mrow></mrow><mo >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><ci >𝑖</ci><ci >𝑛</ci><ci  >𝑑</ci><ci >𝑒</ci><ci >𝑥</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci ><mtext >argmin</mtext></ci><ci
    >𝑘</ci></apply><apply ><csymbol cd="latexml" >conditional-set</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜆</ci><ci >𝑖</ci></apply><apply
    ><cn type="integer" >0</cn><apply ><ci >𝑖</ci><apply ><csymbol cd="latexml" >expectation</csymbol><apply
    ><ci >𝑛</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜆</ci><ci
    >𝑖</ci></apply></apply></apply><cn type="integer"  >0</cn></apply></apply></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐗</ci><apply ><ci >𝑚</ci><ci >𝑖</ci><ci
    >𝑑</ci></apply></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><list
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐮</ci><apply ><ci >𝑖</ci><ci
    >𝑛</ci><ci >𝑑</ci><ci >𝑒</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><cn type="integer" >0</cn></apply></apply></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐮</ci><apply ><ci >𝑖</ci><ci >𝑛</ci><ci >𝑑</ci><ci >𝑒</ci><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><cn type="integer" >1</cn></apply></apply></apply><ci
    >…</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐮</ci><apply ><ci
    >𝑖</ci><ci >𝑛</ci><ci >𝑑</ci><ci >𝑒</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><apply ><ci >𝑘</ci><cn type="integer"  >1</cn></apply></apply></apply></apply></list><ci
    ><mtext mathsize="70%"  >T</mtext></ci></apply></apply><apply ><apply ><csymbol
- en: 'where $index$ is the subscript of the selected eigenvectors. GTSA (Kreuzer
    et al., [2021](#bib.bib194)) puts eigenvector $\mathbf{u}_{i}$ on the frequency
    axis at $\lambda_{i}$ and uses sequence modeling methods to generate positional
    encoding. Specificly, it extends $\mathbf{X}^{mid}$ in Equation [56](#S6.E56 "In
    6.4.1\. Position-analogy methods ‣ 6.4\. Encoding Enhancement ‣ 6\. Graph Transformer
    By Junwei Yang ‣ A Comprehensive Survey on Deep Graph Representation Learning")
    to $\tilde{\mathbf{X}}^{mid}\in\mathbb{R}^{n\times k\times 2}$ by concatenating
    each value in eigenvectors with corresponding eigenvalue, and then positional
    encoding $\mathbf{X}^{pos}\in\mathbb{R}^{n\times d}$ are generated as:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $index$ 是选择的特征向量的下标。GTSA (Kreuzer et al., [2021](#bib.bib194)) 将特征向量 $\mathbf{u}_{i}$
    放置在频率轴上，位于 $\lambda_{i}$ 并使用序列建模方法生成位置编码。具体而言，它通过将特征向量中的每个值与对应的特征值连接，扩展 $\mathbf{X}^{mid}$（方程
    [56](#S6.E56 "在 6.4.1\. Position-analogy methods ‣ 6.4\. Encoding Enhancement
    ‣ 6\. Graph Transformer By Junwei Yang ‣ A Comprehensive Survey on Deep Graph
    Representation Learning")）为 $\tilde{\mathbf{X}}^{mid}\in\mathbb{R}^{n\times k\times
    2}$，然后生成位置编码 $\mathbf{X}^{pos}\in\mathbb{R}^{n\times d}$，表示为：
- en: '| (57) |  | $\begin{gathered}\mathbf{X}^{input}=\tilde{\mathbf{X}}^{mid}\mathbf{W}^{2\times
    d},\\ \mathbf{X}^{pos}=\text{SumPooling}(\text{Transformer}(\mathbf{X}^{input}),\text{dim}=1).\end{gathered}$
    |  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| (57) |  | $\begin{gathered}\mathbf{X}^{input}=\tilde{\mathbf{X}}^{mid}\mathbf{W}^{2\times
    d},\\ \mathbf{X}^{pos}=\text{SumPooling}(\text{Transformer}(\mathbf{X}^{input}),\text{dim}=1).\end{gathered}$
    |  |'
- en: Here, $\mathbf{X}_{input}\in\mathbb{R}^{n\times k\times d}$ is equivalent to
    the input matrix in sequence modeling problem with shape $(batch\_size,length,dim)$,
    and can be naturally processed by Transformer. Since the Laplacian eigenvectors
    can be complex-valued for directed graph, GSGT (Hussain et al., [2021](#bib.bib154))
    proposes to utilize SVD of adjacency matrix $\mathbf{A}$, which is denoted as
    $\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\text{T}}$, and uses the largest
    $k$ singular values $\mathbf{\Sigma}_{k}$ and associated left and right singular
    vectors $\mathbf{U}_{k}$ and $\mathbf{V}_{k}^{\text{T}}$ to output $\mathbf{X}^{pos}$
    as $\mathbf{X}^{pos}=[\mathbf{U}_{k}\mathbf{\Sigma}_{k}^{1/2}\|\mathbf{V}_{k}\mathbf{\Sigma}_{k}^{1/2}]$,
    where $\|$ is the concatenation operation. All these methods above randomly flip
    the signs of eigenvectors or singular vectors during the training phase to promote
    the invariance of the models to the sign ambiguity.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\mathbf{X}_{input}\in\mathbb{R}^{n\times k\times d}$ 相当于序列建模问题中的输入矩阵，其形状为
    $(batch\_size,length,dim)$，可以自然地通过 Transformer 处理。由于拉普拉斯特征向量对于有向图可能是复数值的，GSGT
    (Hussain et al., [2021](#bib.bib154)) 提出利用邻接矩阵 $\mathbf{A}$ 的 SVD，其表示为 $\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\text{T}}$，并使用最大的
    $k$ 个奇异值 $\mathbf{\Sigma}_{k}$ 及相关的左奇异向量和右奇异向量 $\mathbf{U}_{k}$ 和 $\mathbf{V}_{k}^{\text{T}}$
    来输出 $\mathbf{X}^{pos}$，表示为 $\mathbf{X}^{pos}=[\mathbf{U}_{k}\mathbf{\Sigma}_{k}^{1/2}\|\mathbf{V}_{k}\mathbf{\Sigma}_{k}^{1/2}]$，其中
    $\|$ 是连接操作。上述所有方法在训练阶段随机翻转特征向量或奇异向量的符号，以促进模型对符号模糊性的不可变性。
- en: 6.4.2\. Structure-aware methods
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2\. 结构感知方法
- en: In contrast to position-analogy methods, structure-aware methods do not attempt
    to mathematically rigorously simulate sequence positional encoding. They use some
    additional mechanisms to directly calculate structure related encoding.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 与位置类比方法相比，结构感知方法不试图在数学上严格模拟序列位置编码。它们使用一些额外机制直接计算与结构相关的编码。
- en: 'Some approaches compute extra encoding $\mathbf{X}^{add}$ and add it to the
    initial node representation. Graphormer (Ying et al., [2021](#bib.bib416)) proposes
    to leverage node centrality as additional signal to address the importance of
    each node. Concretely, $\mathbf{x}_{i}^{add}$ is determined by the indegree $\text{deg}_{i}^{-}$
    and outdegree $\text{deg}_{i}^{+}$:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法计算额外的编码 $\mathbf{X}^{add}$ 并将其添加到初始节点表示中。Graphormer (Ying et al., [2021](#bib.bib416))
    提出利用节点中心性作为额外信号来处理每个节点的重要性。具体而言，$\mathbf{x}_{i}^{add}$ 由入度 $\text{deg}_{i}^{-}$
    和出度 $\text{deg}_{i}^{+}$ 确定：
- en: '| (58) |  | $\mathbf{x}_{i}^{add}=\mathcal{P}^{-}(\text{deg}_{i}^{-})+\mathcal{P}^{+}(\text{deg}_{i}^{+}),$
    |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| (58) |  | $\mathbf{x}_{i}^{add}=\mathcal{P}^{-}(\text{deg}_{i}^{-})+\mathcal{P}^{+}(\text{deg}_{i}^{+}),$
    |  |'
- en: 'where $\mathcal{P}^{-}$ and $\mathcal{P}^{+}$ are learnable embedding function.
    Graph-BERT (Zhang et al., [2020b](#bib.bib437)) employs Weisfeiler-Lehman algorithm
    to label node $v_{i}$ to a number $\text{WL}(v_{i})\in\mathbb{N}$ and defines
    $\mathbf{x}_{i}^{add}$ as:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{P}^{-}$ 和 $\mathcal{P}^{+}$ 是可学习的嵌入函数。Graph-BERT (Zhang et al.,
    [2020b](#bib.bib437)) 使用 Weisfeiler-Lehman 算法将节点 $v_{i}$ 标记为一个数字 $\text{WL}(v_{i})\in\mathbb{N}$
    并定义 $\mathbf{x}_{i}^{add}$ 为：
- en: '| (59) |  | $\begin{gathered}\mathbf{x}_{i,2j}^{add}=\sin(\text{WL}(v_{i})/10000^{2j/d}),\
    \mathbf{x}_{i,2j+1}^{add}=\cos(\text{WL}(v_{i})/10000^{2j/d}).\end{gathered}$
    |  |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| (59) |  | $\begin{gathered}\mathbf{x}_{i,2j}^{add}=\sin(\text{WL}(v_{i})/10000^{2j/d}),\
    \mathbf{x}_{i,2j+1}^{add}=\cos(\text{WL}(v_{i})/10000^{2j/d}).\end{gathered}$
    |  |'
- en: 'The other approaches try to leverage GNNs to initialize inputs to Transformer.
    LRGT (Wu et al., [2021](#bib.bib387)) applies GNN to get intermediate vectors
    as $\mathbf{X}^{\prime}=\text{GNN}(\mathbf{X})$, and passes the concatenation
    of $\mathbf{X}^{\prime}$ and a special vector $\mathbf{x}_{\text{CLS}}$ to Transformer
    layer as: $\hat{\mathbf{X}}=\text{Transformer}([\mathbf{X}^{\prime}\|\mathbf{x}_{\text{CLS}}])$.
    Then $\hat{\mathbf{x}}_{\text{CLS}}$ can be used as the representation of the
    entire graph for downstream tasks. This method cannot break 1-WL bottleneck because
    it uses GCN (Kipf and Welling, [2016a](#bib.bib183)) and GIN (Xu et al., [2018a](#bib.bib404))
    as graph encoder in the first step, which are intrinsically limited by 1-WL test.
    SAT (Chen et al., [2022a](#bib.bib47)) improves this deficiency by using subgraph-GNN
    NGNN (Zhang and Li, [2021](#bib.bib439)) for initialization, and achieves outstanding
    performance.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法尝试利用 GNN 来初始化 Transformer 的输入。LRGT (Wu et al., [2021](#bib.bib387)) 应用 GNN
    以获得中间向量 $\mathbf{X}^{\prime}=\text{GNN}(\mathbf{X})$，并将 $\mathbf{X}^{\prime}$
    和一个特殊向量 $\mathbf{x}_{\text{CLS}}$ 的拼接传递给 Transformer 层，如下所示：$\hat{\mathbf{X}}=\text{Transformer}([\mathbf{X}^{\prime}\|\mathbf{x}_{\text{CLS}}])$。然后
    $\hat{\mathbf{x}}_{\text{CLS}}$ 可以作为下游任务中整个图的表示。由于在第一步中使用了 GCN (Kipf 和 Welling,
    [2016a](#bib.bib183)) 和 GIN (Xu et al., [2018a](#bib.bib404)) 作为图编码器，这些方法在本质上受到
    1-WL 测试的限制，因此无法突破 1-WL 瓶颈。SAT (Chen et al., [2022a](#bib.bib47)) 通过使用子图-GNN NGNN
    (Zhang 和 Li, [2021](#bib.bib439)) 进行初始化，改善了这一缺陷，并取得了优异的性能。
- en: 6.5\. Summary
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5\. 总结
- en: 'This section introduces Transformer-based approaches for graph representation
    learning and we provide the summary as follows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了基于 Transformer 的图表示学习方法，我们提供如下总结：
- en: •
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Techniques. Graph Transformer methods modify two fundamental techniques in Transformer,
    attention operation and positional encoding, to enhance its ability to encode
    graph data. Typically, they introduce fully-connected attention to model long-distance
    relationship, utilize shortest path and Laplacian eigenvectors to break 1-WL bottleneck,
    and separate points and edges belonging to different classes to avoid over-mixing
    problem.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术。图 Transformer 方法修改了 Transformer 中的两个基本技术，注意力操作和位置编码，以增强其编码图数据的能力。通常，它们引入全连接注意力以建模长距离关系，利用最短路径和拉普拉斯特征向量来突破
    1-WL 瓶颈，并将不同类别的点和边分开，以避免过度混合问题。
- en: •
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Challenges and Limitations. Though Graph Transformers achieve encouraging performance,
    they still face two major challenges. The first challenge is the computational
    cost of the quadratic attention mechanism and shortest path calculation. These
    operations require significant computing resources and can be a bottleneck, particularly
    for large graphs. The second is the reliance of Transformer-based models on large
    amounts of data for stable performance. It poses a challenge when dealing with
    problems that lack sufficient data, especially for few-shot and zero-shot settings.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战与限制。尽管图 Transformer 实现了令人鼓舞的性能，但仍面临两个主要挑战。第一个挑战是二次注意力机制和最短路径计算的计算成本。这些操作需要大量的计算资源，并且对于大规模图来说可能成为瓶颈。第二个挑战是基于
    Transformer 的模型对大量数据的依赖，以保证稳定的性能。当处理数据不足的问题时，特别是在少样本和零样本设置中，这会成为一个挑战。
- en: •
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future Works. We expect efficiency improvement for Graph Transformer should
    be further explored. Additionally, there are some works using pre-training and
    fine-tuning framework to balance performance and complexity in downstream tasks
    (Ying et al., [2021](#bib.bib416)), this may be a promising solution to address
    the aforementioned two challenges.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来工作。我们期望图 Transformer 的效率提升应进一步探索。此外，还有一些工作使用预训练和微调框架来平衡下游任务中的性能和复杂性 (Ying
    et al., [2021](#bib.bib416))，这可能是解决上述两个挑战的一个有前景的解决方案。
- en: 7\. Semi-supervised Learning on Graphs By Xiao Luo
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 由 Xiao Luo 撰写的图上的半监督学习
- en: We have investigated various architectures of graph neural networks in which
    the parameters should be tuned by a learning objective. The most prevalent optimization
    approach is supervised learning on graph data. Due to the label deficiency, semi-supervised
    learning has attracted increasing attention in the data mining community. In detail,
    these methods attempt to combine graph representation learning with current semi-supervised
    techniques including pseudo-labeling, consistency learning, knowledge distillation
    and active learning. These works can be further subdivided into node-level representation
    learning and graph-level representation learning. We would introduce both parts
    in detail as in Sec. [7.1](#S7.SS1 "7.1\. Node Representation Learning ‣ 7\. Semi-supervised
    Learning on Graphs By Xiao Luo ‣ A Comprehensive Survey on Deep Graph Representation
    Learning") and Sec. [7.2](#S7.SS2 "7.2\. Graph Representation Learning ‣ 7\. Semi-supervised
    Learning on Graphs By Xiao Luo ‣ A Comprehensive Survey on Deep Graph Representation
    Learning"), respectively. A summarization is provided in Table [5](#S7.T5 "Table
    5 ‣ 7.1\. Node Representation Learning ‣ 7\. Semi-supervised Learning on Graphs
    By Xiao Luo ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了各种图神经网络架构，其中参数应通过学习目标进行调整。最常见的优化方法是对图数据进行监督学习。由于标签不足，半监督学习在数据挖掘领域受到了越来越多的关注。具体而言，这些方法尝试将图表示学习与当前的半监督技术结合，包括伪标签、**一致性学习**、**知识蒸馏**和**主动学习**。这些工作可以进一步细分为节点级表示学习和图级表示学习。我们将详细介绍这两个部分，分别在
    Sec. [7.1](#S7.SS1 "7.1\. 节点表示学习 ‣ 7\. 图上半监督学习 作者：肖罗 ‣ 深度图表示学习综合调查") 和 Sec. [7.2](#S7.SS2
    "7.2\. 图表示学习 ‣ 7\. 图上半监督学习 作者：肖罗 ‣ 深度图表示学习综合调查") 中。表 [5](#S7.T5 "表 5 ‣ 7.1\. 节点表示学习
    ‣ 7\. 图上半监督学习 作者：肖罗 ‣ 深度图表示学习综合调查") 提供了总结。
- en: 7.1\. Node Representation Learning
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 节点表示学习
- en: Typically, node representation learning follows the concept of transductive
    learning, which has the access to test unlabeled data. We first review the simplest
    loss objective, i.e., node-level supervised loss. This loss exploits the ground
    truth of labeled nodes on graphs. The standard cross-entropy is usually adopted
    for optimization. In formulation,
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，节点表示学习遵循**传导学习**的概念，这种方法可以访问测试中的未标记数据。我们首先回顾最简单的损失目标，即节点级监督损失。该损失利用了图上标记节点的真实标签。标准的交叉熵通常用于优化。在公式中，
- en: '| (60) |  | $\mathcal{L}_{NSL}=-\frac{1}{&#124;\mathcal{Y}^{L}&#124;}\sum_{i\in\mathcal{Y}^{L}}\mathbf{y}_{i}^{T}\log\mathbf{p}_{i},$
    |  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| (60) |  | $\mathcal{L}_{NSL}=-\frac{1}{&#124;\mathcal{Y}^{L}&#124;}\sum_{i\in\mathcal{Y}^{L}}\mathbf{y}_{i}^{T}\log\mathbf{p}_{i},$
    |  |'
- en: where $\mathcal{Y}^{L}$ denotes the set of labeled nodes. Additionally, there
    are a variety of unlabeled nodes that can be used to offer semantic information.
    To fully utilize these nodes, a range of methods attempt to combine semi-supervised
    approaches with graph neural networks. Pseudo-labeling (Lee et al., [2013](#bib.bib201))
    is a fundamental semi-supervised technique that uses the classifier to produce
    the label distribution of unlabeled examples and then adds appropriately labeled
    examples to the training set (Li et al., [2022d](#bib.bib213); Zhou et al., [2019](#bib.bib466)).
    Another line of semi-supervised learning is consistency regularization (Laine
    and Aila, [2016](#bib.bib198)) that requires two examples to have identical predictions
    under perturbation. This regularization is based on the assumption that each instance
    has a distinct label that is resistant to random perturbations (Feng et al., [2020](#bib.bib93);
    Park et al., [2021](#bib.bib272)). Then, we show several representative works
    in detail.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{Y}^{L}$ 表示标记节点的集合。此外，还有各种未标记节点可以用来提供语义信息。为了充分利用这些节点，一系列方法尝试将半监督方法与图神经网络结合。伪标签（Lee
    et al., [2013](#bib.bib201)）是一个基本的半监督技术，它利用分类器生成未标记样本的标签分布，然后将适当标记的样本添加到训练集中（Li
    et al., [2022d](#bib.bib213); Zhou et al., [2019](#bib.bib466)）。另一种半监督学习方法是**一致性正则化**（Laine
    和 Aila, [2016](#bib.bib198)），要求两个样本在扰动下具有相同的预测。这种正则化基于每个实例具有抗随机扰动的独特标签的假设（Feng
    et al., [2020](#bib.bib93); Park et al., [2021](#bib.bib272)）。接下来，我们将详细展示几个具有代表性的工作。
- en: Table 5\. A Summary of Methods for Semi-supervised Learning on Graphs. Contrastive
    learning can be considered as a specific kind of consistency learning.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 图上半监督学习方法总结。对比学习可以被视为一种特定的**一致性学习**。
- en: '|  | Approach | Pseudo-labeling | Consistency Learning | Knowledge Distillation
    | Active Learning |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 伪标签 | 一致性学习 | 知识蒸馏 | 主动学习 |'
- en: '| Node-level | CoGNet (Li et al., [2022d](#bib.bib213)) | ✓ |  |  |  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 节点级别 | CoGNet (Li et al., [2022d](#bib.bib213)) | ✓ |  |  |  |'
- en: '| DSGCN (Zhou et al., [2019](#bib.bib466)) | ✓ |  |  |  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| DSGCN (Zhou et al., [2019](#bib.bib466)) | ✓ |  |  |  |'
- en: '| GRAND (Feng et al., [2020](#bib.bib93)) |  | ✓ |  |  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| GRAND (Feng et al., [2020](#bib.bib93)) |  | ✓ |  |  |'
- en: '| AugGCR (Park et al., [2021](#bib.bib272)) |  | ✓ |  |  |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| AugGCR (Park et al., [2021](#bib.bib272)) |  | ✓ |  |  |'
- en: '| Graph-level | SEAL (Li et al., [2019b](#bib.bib212)) | ✓ |  |  | ✓ |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 图级别 | SEAL (Li et al., [2019b](#bib.bib212)) | ✓ |  |  | ✓ |'
- en: '| InfoGraph (Sun et al., [2020a](#bib.bib336)) |  | ✓ | ✓ |  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| InfoGraph (Sun et al., [2020a](#bib.bib336)) |  | ✓ | ✓ |  |'
- en: '| DSGC (Yang et al., [2022a](#bib.bib409)) |  | ✓ |  |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| DSGC (Yang et al., [2022a](#bib.bib409)) |  | ✓ |  |  |'
- en: '| ASGN (Hao et al., [2020](#bib.bib132)) |  |  | ✓ | ✓ |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| ASGN (Hao et al., [2020](#bib.bib132)) |  |  | ✓ | ✓ |'
- en: '| TGNN (Ju et al., [2022b](#bib.bib173)) |  | ✓ |  |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| TGNN (Ju et al., [2022b](#bib.bib173)) |  | ✓ |  |  |'
- en: '| KGNN (Ju et al., [2022d](#bib.bib175)) | ✓ |  |  |  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| KGNN (Ju et al., [2022d](#bib.bib175)) | ✓ |  |  |  |'
- en: '| HGMI (Li et al., [2022a](#bib.bib210)) | ✓ | ✓ |  |  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| HGMI (Li et al., [2022a](#bib.bib210)) | ✓ | ✓ |  |  |'
- en: '| ASGNN (Xie et al., [2022b](#bib.bib396)) | ✓ |  |  | ✓ |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| ASGNN (Xie et al., [2022b](#bib.bib396)) | ✓ |  |  | ✓ |'
- en: '| DualGraph (Luo et al., [2022a](#bib.bib243)) | ✓ | ✓ |  |  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| DualGraph (Luo et al., [2022a](#bib.bib243)) | ✓ | ✓ |  |  |'
- en: '| GLA (Yue et al., [2022](#bib.bib429)) |  | ✓ |  |  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| GLA (Yue et al., [2022](#bib.bib429)) |  | ✓ |  |  |'
- en: '| SS (Xie et al., [2022a](#bib.bib395)) | ✓ |  |  |  |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| SS (Xie et al., [2022a](#bib.bib395)) | ✓ |  |  |  |'
- en: 'Cooperative Graph Neural Networks (Li et al., [2022d](#bib.bib213)) (CoGNet).
    CoGNet is a representative pseudo-label-based GNN approach for semi-supervised
    node classification. It employs two GNN classifiers to jointly annotate unlabeled
    nodes. In particular, it calculates the confidence of each node as follows:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 协同图神经网络 (Li et al., [2022d](#bib.bib213))（CoGNet）。CoGNet 是一种基于伪标签的 GNN 方法，适用于半监督节点分类。它使用两个
    GNN 分类器共同标注未标记节点。特别地，它计算每个节点的置信度，如下所示：
- en: '| (61) |  | $CV(\mathbf{p}_{i})=\mathbf{p}_{i}^{T}\log\mathbf{p}_{i},$ |  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| (61) |  | $CV(\mathbf{p}_{i})=\mathbf{p}_{i}^{T}\log\mathbf{p}_{i},$ |  |'
- en: 'where $\mathbf{p}_{i}$ denotes the output label distribution. Then it selects
    the pseudo-labels with high confidence generated from one model to supervise the
    optimization of the other model. In particular, the objective for unlabeled nodes
    is written as follows:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{p}_{i}$ 表示输出标签分布。然后，它从一个模型生成的高置信度伪标签中选择以监督另一个模型的优化。特别是，对于未标记节点的目标如下：
- en: '| (62) |  | $\mathcal{L}_{CoGNet}=\sum_{i\in\mathcal{V}^{U}}\mathbf{1}_{CV(\mathbf{p}_{i})>\tau}\hat{\mathbf{y}}^{T}_{i}log\mathbf{q}_{i},$
    |  |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| (62) |  | $\mathcal{L}_{CoGNet}=\sum_{i\in\mathcal{V}^{U}}\mathbf{1}_{CV(\mathbf{p}_{i})>\tau}\hat{\mathbf{y}}^{T}_{i}log\mathbf{q}_{i},$
    |  |'
- en: where $\hat{\mathbf{y}}_{i}$ denotes the one-hot formulation of the pseudo-label
    $\hat{y}_{i}=argmax\mathbf{p}_{i}$ and $\mathbf{q}_{i}$ denotes the label distribution
    predicted by the other classifier. $\tau$ is a pre-defined temperature coefficient.
    This cross supervision has been demonstrated effective in (Chen et al., [2021c](#bib.bib52);
    Luo et al., [2021c](#bib.bib245)) to prevent the provision of biased pseudo-labels.
    Moreover, it employs GNNExplainer (Ying et al., [2019](#bib.bib417)) to provide
    additional information from a dual perspective. Here it measures the minimal subgraphs
    where GNN classifiers can still generate the same prediction. In this way, CoGNet
    can illustrate the entire optimization process to enhance our understanding.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{\mathbf{y}}_{i}$ 表示伪标签 $\hat{y}_{i}=argmax\mathbf{p}_{i}$ 的独热编码形式，$\mathbf{q}_{i}$
    表示由其他分类器预测的标签分布。$\tau$ 是预定义的温度系数。这种交叉监督已在 (Chen et al., [2021c](#bib.bib52); Luo
    et al., [2021c](#bib.bib245)) 中证明有效，以防止提供偏差的伪标签。此外，它采用 GNNExplainer (Ying et al.,
    [2019](#bib.bib417)) 从双重视角提供额外信息。这里它测量了 GNN 分类器仍能生成相同预测的最小子图。这样，CoGNet 可以展示整个优化过程，以增强我们的理解。
- en: 'Dynamic Self-training Graph Neural Network (Zhou et al., [2019](#bib.bib466))
    (DSGCN). DSGCN develops an adaptive manner to utilize reliable pseudo-labels for
    unlabeled nodes. In particular, it allocates smaller weights to samples with lower
    confidence with the additional consideration of class balance. The weight is formulated
    as:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 动态自训练图神经网络 (Zhou et al., [2019](#bib.bib466))（DSGCN）。DSGCN 采用适应性方法利用可靠的伪标签进行未标记节点的训练。特别是，它对置信度较低的样本分配较小的权重，并额外考虑类别平衡。权重的公式为：
- en: '| (63) |  | $\omega_{i}=\frac{1}{n_{c^{i}}}\max\left(\operatorname{RELU}\left(\mathbf{p}_{i}-\beta\cdot\mathbf{1}\right)\right),$
    |  |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| (63) |  | $\omega_{i}=\frac{1}{n_{c^{i}}}\max\left(\operatorname{RELU}\left(\mathbf{p}_{i}-\beta\cdot\mathbf{1}\right)\right),$
    |  |'
- en: where $n_{c^{i}}$ denotes the number of unlabeled samples assigned to the class
    $c^{i}$. This technique will decrease the impact of wrong pseudo-labels during
    iterative training.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n_{c^{i}}$ 表示分配给类 $c^{i}$ 的未标记样本的数量。这种技术将减少迭代训练过程中错误伪标签的影响。
- en: 'Graph Random Neural Networks (Feng et al., [2020](#bib.bib93)) (GRAND). GRAND
    is a representative consistency learning-based method. It first adds a variety
    of perturbations to the input graph to generate a list of graph views. Each graph
    view $G^{r}$ is sent to a GNN classifier to produce a prediction matrix $\mathbf{P}^{r}=[\mathbf{p}_{1}^{r},\cdots,\mathbf{p}_{N}^{r}]$.
    Then it summarizes these matrices as follows:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图随机神经网络 (Feng et al., [2020](#bib.bib93)) (GRAND)。GRAND 是一种代表性的一致性学习方法。它首先对输入图添加各种扰动，以生成图视图列表。每个图视图
    $G^{r}$ 被送到一个 GNN 分类器，以产生预测矩阵 $\mathbf{P}^{r}=[\mathbf{p}_{1}^{r},\cdots,\mathbf{p}_{N}^{r}]$。然后，它按如下方式总结这些矩阵：
- en: '| (64) |  | $\mathbf{P}=\frac{1}{R}\mathbf{P}^{r}.$ |  |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| (64) |  | $\mathbf{P}=\frac{1}{R}\mathbf{P}^{r}.$ |  |'
- en: 'To provide more discriminative information and ensure that the matrix is row-normalized,
    GRAND sharpens the summarized label matrix into $\mathbf{P}^{SA}$ as:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更多的区分信息并确保矩阵是行归一化的，GRAND 将总结的标签矩阵锐化为 $\mathbf{P}^{SA}$，如下所示：
- en: '| (65) |  | $\mathbf{P}^{SA}_{ij}=\frac{\mathbf{P}_{ij}^{1/T}}{\sum_{j^{\prime}=0}\mathbf{P}_{ij^{\prime}}^{1/T}},$
    |  |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| (65) |  | $\mathbf{P}^{SA}_{ij}=\frac{\mathbf{P}_{ij}^{1/T}}{\sum_{j^{\prime}=0}\mathbf{P}_{ij^{\prime}}^{1/T}},$
    |  |'
- en: 'where $T$ is a given temperature parameter. Finally, consistency learning is
    performed by comparing the sharpened summarized matrix with the matrix of each
    graph view. Formally, the objective is:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $T$ 是给定的温度参数。最后，通过将锐化的总结矩阵与每个图视图的矩阵进行比较，执行一致性学习。形式上，目标是：
- en: '| (66) |  | $\mathcal{L}_{GRAND}=\frac{1}{R}\sum_{r=1}^{R}\sum_{i\in V}&#124;&#124;\mathbf{P}^{SA}_{i}-\mathbf{P}_{i}&#124;&#124;,$
    |  |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| (66) |  | $\mathcal{L}_{GRAND}=\frac{1}{R}\sum_{r=1}^{R}\sum_{i\in V}||\mathbf{P}^{SA}_{i}-\mathbf{P}_{i}||,$
    |  |'
- en: here $\mathcal{L}_{GRAND}$ serves as a regularization which is combined with
    the standard supervised loss.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{GRAND}$ 作为一种正则化，与标准监督损失结合使用。
- en: Augmentation for GNNs with the Consistency Regularization (Park et al., [2021](#bib.bib272))
    (AugGCR). AugGCR begins with the generation of augmented graphs by random dropout
    and mixup of different order features. To enhance the model generalization, it
    borrows the idea of meta-learning to partition the training data, which improves
    the quality of graph augmentation. In addition, it utilizes consistency regularization
    to enhance the semi-supervised node classification.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 带有一致性正则化的 GNN 增强 (Park et al., [2021](#bib.bib272)) (AugGCR)。AugGCR 首先通过随机丢弃和混合不同顺序的特征来生成增强图。为了增强模型的泛化能力，它借鉴了元学习的思想来划分训练数据，从而提高图增强的质量。此外，它利用一致性正则化来增强半监督节点分类。
- en: 7.2\. Graph Representation Learning
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 图表示学习
- en: 'The objective of graph classification is to predict the property of the whole
    graph example. Assuming that the training set comprises $N^{l}$ and $N^{u}$ graph
    samples $\mathcal{G}^{l}=\{G^{1},\cdots,G^{N^{l}}\}$ and $\mathcal{G}^{u}=\{G^{N^{l}+1},\cdots,G^{N^{l}+N^{u}}\}$,
    the graph-level supervised loss for labeled data can be expressed as follows:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图分类的目标是预测整个图示例的属性。假设训练集包括 $N^{l}$ 和 $N^{u}$ 个图样本 $\mathcal{G}^{l}=\{G^{1},\cdots,G^{N^{l}}\}$
    和 $\mathcal{G}^{u}=\{G^{N^{l}+1},\cdots,G^{N^{l}+N^{u}}\}$，则标记数据的图级监督损失可以表示如下：
- en: '| (67) |  | $\mathcal{L}_{GSL}=-\frac{1}{\left&#124;\mathcal{G}^{u}\right&#124;}\sum_{G_{j}\in\mathcal{G}^{L}}{\mathbf{y}^{j}}^{T}log\mathbf{p}^{j},$
    |  |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| (67) |  | $\mathcal{L}_{GSL}=-\frac{1}{\left|\mathcal{G}^{u}\right|}\sum_{G_{j}\in\mathcal{G}^{L}}{\mathbf{y}^{j}}^{T}log\mathbf{p}^{j},$
    |  |'
- en: where $\mathbf{y}^{j}$ denotes the one-hot label vector for the $j$-th sample
    while $\mathbf{p}^{j}$ denotes the predicted distribution of $G^{j}$. When $N^{u}=0$,
    this objective can be utilized to optimize supervised methods. However, due to
    the shortage of labels in graph data, supervised methods cannot reach exceptional
    performance in real-world applications (Hao et al., [2020](#bib.bib132)). To tackle
    this, semi-supervised graph classification has been developed extensively. These
    approaches can be categorized into pseudo-labeling-based methods, knowledge distillation-based
    methods and contrastive learning-based methods. Pseudo-labeling methods annotate
    graph instances and utilize well-classified graph examples to update the training
    set (Li et al., [2019b](#bib.bib212), [2022a](#bib.bib210)). Knowledge distillation-based
    methods usually utilize a teacher-student architecture, where the teacher model
    conducts graph representation learning without label information to extract generalized
    knowledge while the student model focuses on the downstream task. Due to the restricted
    number of labeled instances, the student model transfers knowledge from the teacher
    model to prevent overfitting (Sun et al., [2020a](#bib.bib336); Hao et al., [2020](#bib.bib132)).
    Another line of this topic is to utilize graph contrastive learning, which is
    frequently used in unsupervised learning. Typically, these methods extract topological
    information from two perspectives (i.e., different perturbation strategies and
    graph encoders), and maximize the similarity of their representations compared
    with those from other examples (Ju et al., [2022b](#bib.bib173); Luo et al., [2022a](#bib.bib243);
    Ju et al., [2022a](#bib.bib172)). Active learning, as a prevalent technique to
    improve the efficiency of data annotation, has also been utilized for semi-supervised
    methods (Hao et al., [2020](#bib.bib132); Xie et al., [2022b](#bib.bib396)). Then,
    we review these methods in detail.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbf{y}^{j}$ 表示第 $j$ 个样本的一热标签向量，而 $\mathbf{p}^{j}$ 表示 $G^{j}$ 的预测分布。当
    $N^{u}=0$ 时，该目标可以用于优化监督方法。然而，由于图数据标签的短缺，监督方法在实际应用中无法达到卓越的表现（Hao et al., [2020](#bib.bib132)）。为了解决这个问题，半监督图分类已被广泛发展。这些方法可以分为伪标签方法、知识蒸馏方法和对比学习方法。伪标签方法对图实例进行标注，并利用分类良好的图示例更新训练集（Li
    et al., [2019b](#bib.bib212), [2022a](#bib.bib210)）。知识蒸馏方法通常利用教师-学生架构，其中教师模型在没有标签信息的情况下进行图表示学习，以提取一般化知识，而学生模型则专注于下游任务。由于标注实例的数量有限，学生模型从教师模型中转移知识，以防止过拟合（Sun
    et al., [2020a](#bib.bib336); Hao et al., [2020](#bib.bib132)）。另一个方向是利用图对比学习，这在无监督学习中被广泛使用。通常，这些方法从两个视角（即不同的扰动策略和图编码器）提取拓扑信息，并最大化其表示的相似性，与来自其他示例的表示进行对比（Ju
    et al., [2022b](#bib.bib173); Luo et al., [2022a](#bib.bib243); Ju et al., [2022a](#bib.bib172)）。主动学习作为提高数据标注效率的常见技术，也已被用于半监督方法（Hao
    et al., [2020](#bib.bib132); Xie et al., [2022b](#bib.bib396)）。接下来，我们将详细回顾这些方法。
- en: SEmi-supervised grAph cLassification (Li et al., [2019b](#bib.bib212)) (SEAL).
    SEAL treats each graph example as a node in a hierarchical graph. It builds two
    graph classifiers which generate graph representations and conduct semi-supervised
    graph classification respectively. SEAL employs a self-attention module to encode
    each graph into a graph-level representation, and then conducts message passing
    from a graph level for final classification. SEAL can also be combined with cautious
    iteration and active iteration. The former merely utilizes partial graph samples
    to optimize the parameters in the first classifier due to the potential erroneous
    pseudo-labels. The second combines active learning with the model, which increases
    the annotation efficiency in semi-supervised scenarios.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督图分类（Li et al., [2019b](#bib.bib212)）（SEAL）。SEAL 将每个图示例视为层次图中的一个节点。它构建了两个图分类器，分别生成图表示并进行半监督图分类。SEAL
    采用自注意力模块将每个图编码成图级表示，然后从图级进行消息传递以进行最终分类。SEAL 还可以与谨慎迭代和主动迭代相结合。前者仅利用部分图样本来优化第一个分类器中的参数，以避免潜在的错误伪标签。后者将主动学习与模型结合，提高半监督场景中的标注效率。
- en: 'InfoGraph (Sun et al., [2020a](#bib.bib336)). Infograph is the first contrastive
    learning-based method. It maximizes the similarity between summarized graph representations
    and their node representations. In particular, it generates node representations
    using the message passing mechanism and summarizes these node representations
    into a graph representation. Let $\Phi(\cdot,\cdot)$ denote a discriminator to
    distinguish whether a node belongs to the graph, and we have:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGraph（Sun 等人，[2020a](#bib.bib336)）。InfoGraph 是第一种基于对比学习的方法。它最大化了总结图表示和其节点表示之间的相似度。特别地，它使用消息传递机制生成节点表示，并将这些节点表示总结为图表示。设
    $\Phi(\cdot,\cdot)$ 表示一个判别器，用于区分节点是否属于图，公式如下：
- en: '| (68) |  | $\mathcal{L}_{InfoGraph}=\sum_{j=1}^{&#124;\mathcal{G}^{l}&#124;+&#124;\mathcal{G}^{u}&#124;}\sum_{i\in\mathcal{G}_{j}}\left[-\operatorname{sp}\left(-\Phi\left(\mathbf{h}_{i}^{j},\mathbf{z}^{j}\right)\right)\right]-\frac{1}{&#124;N_{i}^{j}&#124;}\sum_{i^{\prime
    j^{\prime}}\in N_{i}^{j}}\left[\operatorname{sp}\left(\Phi\left(\mathbf{h}_{i^{\prime}}^{j^{\prime}},\mathbf{z}^{j}\right)\right)\right],$
    |  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| (68) |  | $\mathcal{L}_{InfoGraph}=\sum_{j=1}^{&#124;\mathcal{G}^{l}&#124;+&#124;\mathcal{G}^{u}&#124;}\sum_{i\in\mathcal{G}_{j}}\left[-\operatorname{sp}\left(-\Phi\left(\mathbf{h}_{i}^{j},\mathbf{z}^{j}\right)\right)\right]-\frac{1}{&#124;N_{i}^{j}&#124;}\sum_{i^{\prime
    j^{\prime}}\in N_{i}^{j}}\left[\operatorname{sp}\left(\Phi\left(\mathbf{h}_{i^{\prime}}^{j^{\prime}},\mathbf{z}^{j}\right)\right)\right],$
    |  |'
- en: where $\operatorname{sp}(\cdot)$ denotes the softplus function. $N_{i}^{j}$
    denotes the negative node set where nodes are not in $G^{j}$. This mutual information
    maximization formulation is originally developed for unsupervised learning and
    it can be simply extended for semi-supervised graph classification. In particular,
    InfoGraph utilizes a teacher-student architecture that compares the representation
    across the teacher and student networks. The contrastive learning objective serves
    as a regularization by combining with supervised loss.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\operatorname{sp}(\cdot)$ 表示 softplus 函数。$N_{i}^{j}$ 表示负节点集，即不在 $G^{j}$
    中的节点。该互信息最大化公式最初是为无监督学习开发的，可以简单地扩展到半监督图分类。特别地，InfoGraph 利用教师-学生架构来比较教师网络和学生网络之间的表示。对比学习目标作为正则化，通过与监督损失结合来发挥作用。
- en: 'Dual Space Graph Contrastive Learning (Yang et al., [2022a](#bib.bib409)) (DSGC).
    DSGC is a representative contrastive learning-based method. It utilizes two graph
    encoders. The first is a standard GNN encoder in the Euclidean space and the second
    is the hyperbolic GNN encoder. The hyperbolic GNN encoder first converts graph
    embeddings into hyperbolic space and then measures the distance based on the length
    of geodesics. DSGC compares graph embeddings in the Euclidean space and hyperbolic
    space. Assuming the two GNNs are named as $f_{1}(\cdot)$ and $f_{2}(\cdot)$, the
    positive pair is denoted as:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 双空间图对比学习（Yang 等人，[2022a](#bib.bib409)）（DSGC）。DSGC 是一种代表性的基于对比学习的方法。它利用两个图编码器。第一个是欧几里得空间中的标准
    GNN 编码器，第二个是超曲面 GNN 编码器。超曲面 GNN 编码器首先将图嵌入转换为超曲面空间，然后基于测地线的长度来衡量距离。DSGC 比较了欧几里得空间和超曲面空间中的图嵌入。假设两个
    GNN 命名为 $f_{1}(\cdot)$ 和 $f_{2}(\cdot)$，正对的表示为：
- en: '| (69) |  | $\begin{array}[]{c}\mathbf{z}^{j}_{E\rightarrow H}=\exp_{\mathbf{o}}^{c}(f_{1}(G^{j})),\\
    \mathbf{z}^{j}_{H}=\exp_{\mathbf{o}}^{c}\left(f_{2}(G^{j})\right).\end{array}$
    |  |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| (69) |  | $\begin{array}[]{c}\mathbf{z}^{j}_{E\rightarrow H}=\exp_{\mathbf{o}}^{c}(f_{1}(G^{j})),\\
    \mathbf{z}^{j}_{H}=\exp_{\mathbf{o}}^{c}\left(f_{2}(G^{j})\right).\end{array}$
    |  |'
- en: Then it selects one labeled sample and $N_{B}$ unlabeled sample $G^{j}$ for
    graph contrastive learning in the hyperbolic space. In formulation,
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它选择一个标记样本和 $N_{B}$ 个未标记样本 $G^{j}$ 进行超曲面空间中的图对比学习。在公式中，
- en: '| (70) |  | $\displaystyle\mathcal{L}_{DSGC}$ | $\displaystyle=-\log\frac{\mathrm{e}^{d^{H}\left(\mathbf{h}^{i}_{H},\mathbf{z}^{i}_{E\rightarrow
    H}\right)/\tau}}{\mathbf{e}^{d^{H}\left(\mathbf{z}^{i}_{H},\mathbf{z}^{i}_{E\rightarrow
    H}\right)/\tau}+\sum_{i=1}^{N}\mathrm{e}^{d_{\mathbb{D}}\left(\mathbf{z}^{i}_{E\rightarrow
    H},\mathbf{z}^{j}_{H}\right)/\tau}}$ |  |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| (70) |  | $\displaystyle\mathcal{L}_{DSGC}$ | $\displaystyle=-\log\frac{\mathrm{e}^{d^{H}\left(\mathbf{h}^{i}_{H},\mathbf{z}^{i}_{E\rightarrow
    H}\right)/\tau}}{\mathbf{e}^{d^{H}\left(\mathbf{z}^{i}_{H},\mathbf{z}^{i}_{E\rightarrow
    H}\right)/\tau}+\sum_{i=1}^{N}\mathrm{e}^{d_{\mathbb{D}}\left(\mathbf{z}^{i}_{E\rightarrow
    H},\mathbf{z}^{j}_{H}\right)/\tau}}$ |  |'
- en: '|  |  | $\displaystyle-\frac{\lambda_{u}}{N}\sum_{i=1}^{N}\log\frac{\mathrm{e}^{d_{\mathbb{D}}^{u}\left(\mathbf{z}^{j}_{H},\mathbf{z}^{j}_{E\rightarrow
    H}\right)/\tau}}{\mathrm{e}^{d_{\mathbb{D}}^{u}\left(\mathbf{z}^{j}_{H},\mathbf{z}^{j}_{E\rightarrow
    H}\right)/\tau}+\mathrm{e}^{d_{\mathbb{D}}\left(\mathbf{z}^{i}_{H},\mathbf{z}^{j}_{E\rightarrow
    H}\right)/\tau}},$ |  |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\frac{\lambda_{u}}{N}\sum_{i=1}^{N}\log\frac{\mathrm{e}^{d_{\mathbb{D}}^{u}\left(\mathbf{z}^{j}_{H},\mathbf{z}^{j}_{E\rightarrow
    H}\right)/\tau}}{\mathrm{e}^{d_{\mathbb{D}}^{u}\left(\mathbf{z}^{j}_{H},\mathbf{z}^{j}_{E\rightarrow
    H}\right)/\tau}+\mathrm{e}^{d_{\mathbb{D}}\left(\mathbf{z}^{i}_{H},\mathbf{z}^{j}_{E\rightarrow
    H}\right)/\tau}},$ |  |'
- en: where $\mathbf{z}^{i}_{E\rightarrow H}$ and $\mathbf{z}^{i}_{H}$ denote the
    embeddings for labeled graph sample $G^{i}$ and $d^{H}(\cdot)$ denotes a distance
    metric in the hyperbolic space. This contrastive learning objective maximizes
    the similarity between embeddings learned from two encoders compared with other
    samples. Finally, the contrastive learning objective can be combined with the
    supervised loss to achieve effective semi-supervised contrastive learning.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{z}^{i}_{E\rightarrow H}$ 和 $\mathbf{z}^{i}_{H}$ 表示标记图样本 $G^{i}$
    的嵌入，$d^{H}(\cdot)$ 表示超曲面空间中的距离度量。这个对比学习目标最大化从两个编码器学习的嵌入与其他样本的相似性。最后，对比学习目标可以与监督损失结合，实现有效的半监督对比学习。
- en: Active Semi-supervised Graph Neural Network (Hao et al., [2020](#bib.bib132))
    (ASGN). ASGN utilizes a teacher-student architecture with the teacher model focusing
    on representation learning and the student model targeting at molecular property
    prediction. In the teacher model, ASGN first employs a message passing neural
    network to learn node representations under the reconstruction task and then borrows
    the idea of balanced clustering to learn graph-level representations in a self-supervised
    fashion. In the student model, ASGN utilizes label information to monitor the
    model training based on the weights of the teacher model. In addition, active
    learning is also used to minimize the annotation cost while maintaining sufficient
    performance. Typically, the teacher model seeks to provide discriminative graph-level
    representations without labels, which transfer knowledge to the student model
    to overcome the potential overfitting in the presence of label scarcity.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 主动半监督图神经网络 (Hao 等, [2020](#bib.bib132)) (ASGN)。ASGN 利用教师-学生架构，其中教师模型专注于表征学习，而学生模型则针对分子属性预测。在教师模型中，ASGN
    首先使用消息传递神经网络在重建任务下学习节点表征，然后借鉴平衡聚类的思想以自监督方式学习图级表征。在学生模型中，ASGN 利用标签信息根据教师模型的权重来监控模型训练。此外，主动学习还用于在保持足够性能的同时最小化标注成本。通常，教师模型旨在提供没有标签的区分性图级表征，将知识传递给学生模型，以克服标签稀缺下可能出现的过拟合问题。
- en: Twin Graph Neural Networks (Ju et al., [2022b](#bib.bib173)) (TGNN). TGNN also
    uses two graph neural networks to give different perspectives to learn graph representations.
    Differently, it adopts a graph kernel neural network to learn graph-level representations
    in virtue of random walk kernels. Rather than directly enforcing representation
    from two modules to be similar, TGNN exchanges information by contrasting the
    similarity structure of the two modules. In particular, it constructs a list of
    anchor graphs, $G^{a_{1}},G^{a_{2}},\cdots,G^{a_{M}}$, and utilizes two graph
    encoders to produce their embeddings, i.e., $\left\{z^{a_{m}}\right\}_{m=1}^{M}$,
    $\left\{w^{a_{m}}\right\}_{m=1}^{M}$. Then it calculates the similarity distribution
    between each unlabeled graph and anchor graphs for two modules. Formally,
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 双重图神经网络 (Ju 等, [2022b](#bib.bib173)) (TGNN)。TGNN 同样使用两个图神经网络来从不同角度学习图表征。不同的是，它采用图内核神经网络通过随机游走内核来学习图级表征。TGNN
    通过对比两个模块的相似性结构来交换信息，而不是直接强制两个模块的表征相似。特别地，它构造了一组锚点图 $G^{a_{1}},G^{a_{2}},\cdots,G^{a_{M}}$，并利用两个图编码器生成它们的嵌入，即
    $\left\{z^{a_{m}}\right\}_{m=1}^{M}$, $\left\{w^{a_{m}}\right\}_{m=1}^{M}$。然后，它计算每个未标记图与锚点图在两个模块中的相似性分布。形式上，
- en: '| (71) |  | $p_{m}^{j}=\frac{\exp\left(\cos\left(z^{j},z^{a_{m}}\right)/\tau\right)}{\sum_{m^{\prime}=1}^{M}\exp\left(\cos\left(z^{j},z^{a_{m^{\prime}}}\right)/\tau\right)},$
    |  |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| (71) |  | $p_{m}^{j}=\frac{\exp\left(\cos\left(z^{j},z^{a_{m}}\right)/\tau\right)}{\sum_{m^{\prime}=1}^{M}\exp\left(\cos\left(z^{j},z^{a_{m^{\prime}}}\right)/\tau\right)},$
    |  |'
- en: '| (72) |  | $q_{m}^{j}=\frac{\exp\left(\cos\left(\mathbf{w}^{j},\mathbf{w}^{a_{m}}\right)/\tau\right)}{\sum_{m^{\prime}=1}^{M}\exp\left(\cos\left(\mathbf{w}^{j},\mathbf{w}^{a_{m^{\prime}}}\right)/\tau\right)}.$
    |  |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| (72) |  | $q_{m}^{j}=\frac{\exp\left(\cos\left(\mathbf{w}^{j},\mathbf{w}^{a_{m}}\right)/\tau\right)}{\sum_{m^{\prime}=1}^{M}\exp\left(\cos\left(\mathbf{w}^{j},\mathbf{w}^{a_{m^{\prime}}}\right)/\tau\right)}.$
    |  |'
- en: 'Then, TGNN minimizes the distance between distributions from different modules
    as follows:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，TGNN 最小化不同模块之间的分布距离，如下所示：
- en: '| (73) |  | $\mathcal{L}_{TGNN}=\frac{1}{\left&#124;\mathcal{G}^{U}\right&#124;}\sum_{G^{j}\in\mathcal{G}^{u}}\frac{1}{2}\left(D_{\mathrm{KL}}\left(\mathbf{p}^{j}\&#124;\mathbf{q}^{j}\right)+D_{\mathrm{KL}}\left(\mathbf{q}^{j}\&#124;\mathbf{p}^{j}\right)\right),$
    |  |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| (73) |  | $\mathcal{L}_{TGNN}=\frac{1}{\left|\mathcal{G}^{U}\right|}\sum_{G^{j}\in\mathcal{G}^{u}}\frac{1}{2}\left(D_{\mathrm{KL}}\left(\mathbf{p}^{j}\|\mathbf{q}^{j}\right)+D_{\mathrm{KL}}\left(\mathbf{q}^{j}\|\mathbf{p}^{j}\right)\right),$
    |  |'
- en: which serves as a regularization term to combine with the supervised loss.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 这作为正则化项与监督损失结合使用。
- en: 7.3\. Summary
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3. 总结
- en: 'This section introduces semi-supervised learning for graph representation learning
    and we provide the summary as follows:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了图表示学习的半监督学习，并提供了如下总结：
- en: •
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Techniques. Classic node classification aims to conduct transductive learning
    on graphs with access to unlabeled data, which is a natural semi-supervised problem.
    Semi-supervised graph classification aims to relieve the requirement of abundant
    labeled graphs. Here, a variety of semi-supervised methods have been put forward
    to achieve better performance under the label scarcity. Typically, they try to
    integrate semi-supervised techniques such as active learning, pseudo-labeling,
    consistency learning, and consistency learning with graph representation learning.
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术。经典的节点分类旨在对带有未标记数据的图进行传递学习，这是一种自然的半监督问题。半监督图分类旨在减少对大量标记图的需求。在这里，提出了各种半监督方法，以在标签稀缺的情况下实现更好的性能。通常，它们尝试将半监督技术如主动学习、伪标签生成、一致性学习以及与图表示学习相结合的一致性学习进行整合。
- en: •
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Challenges and Limitations. Despite their great success, the performance of
    these methods is still unsatisfactory, especially in graph-level representation
    learning. For example, DSGC can only achieve an accuracy of 57% in a binary classification
    dataset REDDIT-BINARY. Even worse, label scarcity often accompanies by unbalanced
    datasets and potential domain shifts, which provides more challenges from real-world
    applications.
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战与限制。尽管这些方法取得了很大的成功，但它们的性能仍然不尽如人意，特别是在图级别的表示学习中。例如，DSGC 在二分类数据集 REDDIT-BINARY
    中的准确率仅为 57%。更糟糕的是，标签稀缺通常伴随着不平衡的数据集和潜在的领域转移，这为实际应用带来了更多挑战。
- en: •
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future Works. In the future, we expect that these methods can be applied to
    different problems such as molecular property predictions. There are also works
    to extend graph representation learning in more realistic scenarios like few-shot
    learning (Ma et al., [2020b](#bib.bib250); Chauhan et al., [2020](#bib.bib42)).
    A higher accuracy is always anticipated for more advanced and effective semi-supervised
    techniques.
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来工作。未来，我们希望这些方法能够应用于分子性质预测等不同问题。还有一些工作将图表示学习扩展到更现实的场景，如少样本学习 (Ma et al., [2020b](#bib.bib250);
    Chauhan et al., [2020](#bib.bib42))。对更先进和有效的半监督技术，始终期待更高的准确率。
- en: 8\. Graph Self-supervised Learning By Jingyang Yuan
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8. 图自监督学习 由袁静扬
- en: Besides supervised or semi-supervised methods, self-supervised learning (SSL)
    also has shown its powerful capability in data mining and representation embedding
    recent years. In this section we investigated Graph Neural Networks based on SSL,
    and provided a detailed introduction to a few typical models. Graph SSL methods
    usually has an unified pipeline, which includes pretext tasks and downstream tasks.
    Pretext tasks help model encoder to learn better representation, as a premise
    of better performance in downstream tasks. So a delicate design of pretext task
    is crucial for Graph SSL. We would firstly introduce overall framework of Graph
    SSL in Section [8.1](#S8.SS1 "8.1\. Overall framework ‣ 8\. Graph Self-supervised
    Learning By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph Representation
    Learning"), then introduce the two kind of pretext task design, generation-based
    methods and contrast-Based methods respectively in Section [8.2](#S8.SS2 "8.2\.
    Generation-based pretext task design ‣ 8\. Graph Self-supervised Learning By Jingyang
    Yuan ‣ A Comprehensive Survey on Deep Graph Representation Learning") and [8.3](#S8.SS3
    "8.3\. Contrast-Based pretext task design ‣ 8\. Graph Self-supervised Learning
    By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph Representation Learning").
    A summarization is provided in Table [6](#S8.T6 "Table 6 ‣ 8\. Graph Self-supervised
    Learning By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph Representation
    Learning").
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 除了监督或半监督方法，近年来自监督学习（SSL）也展示了其在数据挖掘和表示嵌入方面的强大能力。在本节中，我们研究了基于SSL的图神经网络，并详细介绍了几种典型的模型。图SSL方法通常具有统一的流程，包括预任务和下游任务。预任务帮助模型编码器学习更好的表示，这是下游任务表现更好的前提。因此，预任务的精细设计对于图SSL至关重要。我们将在第[8.1节](#S8.SS1
    "8.1\. 总体框架 ‣ 8\. 图自监督学习 Jingyang Yuan ‣ 深度图表示学习综述")中首先介绍图SSL的总体框架，然后在第[8.2节](#S8.SS2
    "8.2\. 基于生成的预任务设计 ‣ 8\. 图自监督学习 Jingyang Yuan ‣ 深度图表示学习综述")和[8.3节](#S8.SS3 "8.3\.
    基于对比的预任务设计 ‣ 8\. 图自监督学习 Jingyang Yuan ‣ 深度图表示学习综述")中分别介绍两种预任务设计：基于生成的方法和基于对比的方法。总结见表[6](#S8.T6
    "表 6 ‣ 8\. 图自监督学习 Jingyang Yuan ‣ 深度图表示学习综述")。
- en: Table 6\. A Summary of Methods for Self-supervised Learning on Graphs. ”PT”,
    ”CT” and ”UFE” mean ”Pre-training”, ”Collaborative Train” and ”Unsupervised Feature
    Extracting” respectively.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. 自监督学习在图上的方法总结。“PT”、“CT”和“UFE”分别表示“预训练”、“协作训练”和“无监督特征提取”。
- en: '|  | Approach | Augmentation Scheme | Training Scheme | Generation Target |
    Objective Function |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 增强方案 | 训练方案 | 生成目标 | 目标函数 |'
- en: '| Generation-based | Graph Completion (You et al., [2020](#bib.bib420)) | Feature
    Mask | PT/CT | Node Feature | - |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 基于生成的 | 图补全 (You et al., [2020](#bib.bib420)) | 特征掩码 | PT/CT | 节点特征 | - |'
- en: '| AttributeMask (Jin et al., [2020a](#bib.bib167)) | Feature Mask | PT/CT |
    PCA Node Feature | - |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| AttributeMask (Jin et al., [2020a](#bib.bib167)) | 特征掩码 | PT/CT | PCA 节点特征
    | - |'
- en: '| AttrMasking (Hu et al., [2019](#bib.bib146)) | Feature Mask | PT | Node/Edge
    Feature | - |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| AttrMasking (Hu et al., [2019](#bib.bib146)) | 特征掩码 | PT | 节点/边特征 | - |'
- en: '| MGAE (Wang et al., [2017](#bib.bib359)) | No Augmentation | CT | Node Feature
    | - |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| MGAE (Wang et al., [2017](#bib.bib359)) | 无增强 | CT | 节点特征 | - |'
- en: '| GAE (Kipf and Welling, [2016b](#bib.bib184)) | Feature Noise | UFE | Adjacency
    Matrix | - |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| GAE (Kipf and Welling, [2016b](#bib.bib184)) | 特征噪声 | UFE | 邻接矩阵 | - |'
- en: '| Contrast-based | DeepWalk (Perozzi et al., [2014](#bib.bib277)) | Random
    Walk | UFE | - | SkipGram |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 基于对比的 | DeepWalk (Perozzi et al., [2014](#bib.bib277)) | 随机游走 | UFE | - |
    SkipGram |'
- en: '| LINE (Tang et al., [2015b](#bib.bib345)) | Random Walk | UFE | - | Jensen-Shannon
    |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| LINE (Tang et al., [2015b](#bib.bib345)) | 随机游走 | UFE | - | Jensen-Shannon
    |'
- en: '| GCC (Qiu et al., [2020a](#bib.bib287)) | Random Walk | PT/URL | - | InfoNCE
    |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| GCC (Qiu et al., [2020a](#bib.bib287)) | 随机游走 | PT/URL | - | InfoNCE |'
- en: '| SimGCL (Yu et al., [2022](#bib.bib423)) | Embedding Noise | UFE | - | InfoNCE
    |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| SimGCL (Yu et al., [2022](#bib.bib423)) | 嵌入噪声 | UFE | - | InfoNCE |'
- en: '| SimGRACE (Xia et al., [2022b](#bib.bib392)) | Model Noise | UFE | - | InfoNCE
    |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| SimGRACE (Xia et al., [2022b](#bib.bib392)) | 模型噪声 | UFE | - | InfoNCE |'
- en: '| GCA (Zhu et al., [2021](#bib.bib472)) | Feature Masking & Strcture Adjustment
    | URL | - | InfoNCE |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| GCA (Zhu et al., [2021](#bib.bib472)) | 特征掩码与结构调整 | URL | - | InfoNCE |'
- en: '| BGRL (Grill et al., [2020](#bib.bib121)) | Feature Masking & Strcture Adjustment
    | URL | - | BYOL |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| BGRL (Grill et al., [2020](#bib.bib121)) | 特征掩码与结构调整 | URL | - | BYOL |'
- en: 8.1\. Overall framework
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 总体框架
- en: 'Consider a featured graph $\mathcal{G}$, we denote a graph encoder $f$ to learn
    representation of graph, and a pretext decoder $g$ with specific architecture
    in different pretext tasks. Then the pretext self-supervised learning loss can
    be formulated as:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个特征图 $\mathcal{G}$，我们引入一个图编码器 $f$ 来学习图的表示，并且在不同的预任务中使用具有特定架构的预任务解码器 $g$。然后，预任务自监督学习损失可以表示为：
- en: '| (74) |  | $\mathcal{L}_{total}=E_{\mathcal{G}\sim\mathcal{D}}[\mathcal{L}_{ssl}(g,f,\mathcal{G})],$
    |  |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| (74) |  | $\mathcal{L}_{total}=E_{\mathcal{G}\sim\mathcal{D}}[\mathcal{L}_{ssl}(g,f,\mathcal{G})],$
    |  |'
- en: 'where $\mathcal{D}$ denotes the distribution of featured graph $\mathcal{G}$.
    By minimizing $\mathcal{L}_{overall}$, we can learn encoder $f$ with capacity
    to produce high-quality embedding. As for downstream tasks, we denote a graph
    decoder $d$ which transforms the output of graph encoder $f$ into model prediction.
    The loss of downstream tasks can be formulated as:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}$ 表示特征图 $\mathcal{G}$ 的分布。通过最小化 $\mathcal{L}_{overall}$，我们可以学习具有生成高质量嵌入能力的编码器
    $f$。对于下游任务，我们引入一个图解码器 $d$，将图编码器 $f$ 的输出转换为模型预测。下游任务的损失可以表示为：
- en: '| (75) |  | $\mathcal{L}_{sup}=\mathcal{L}_{sup}(d,f,\mathcal{G};y),$ |  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| (75) |  | $\mathcal{L}_{sup}=\mathcal{L}_{sup}(d,f,\mathcal{G};y),$ |  |'
- en: 'where $y$ is the ground truth in downstream tasks. We can obvious that $\mathcal{L}_{sup}$
    is a typical supervised loss. To ensure the model achieve wise graph representation
    extraction and optimistic prediction performance, $\mathcal{L}_{ssl}$ and $\mathcal{L}_{sup}$
    have to be minimized simultaneously. We introduce 3 different ways to minimize
    the two loss function:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y$ 是下游任务中的真实值。可以明显看出，$\mathcal{L}_{sup}$ 是典型的有监督损失。为了确保模型实现有效的图表示提取和乐观的预测性能，$\mathcal{L}_{ssl}$
    和 $\mathcal{L}_{sup}$ 必须同时最小化。我们介绍了 3 种不同的方法来最小化这两个损失函数：
- en: 'Pre-training. This strategy has two steps. In pre-training step, the $\mathcal{L}_{ssl}$
    is minimized to get $g^{*}$ and $f^{*}$:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练。这一策略分为两个步骤。在预训练步骤中，最小化 $\mathcal{L}_{ssl}$ 以获得 $g^{*}$ 和 $f^{*}$：
- en: '| (76) |  | $g^{*},f^{*}=\underset{g,f}{\arg\min}\mathcal{L}_{ssl}(g,f,\mathcal{D}).$
    |  |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| (76) |  | $g^{*},f^{*}=\underset{g,f}{\arg\min}\mathcal{L}_{ssl}(g,f,\mathcal{D}).$
    |  |'
- en: Then the parameter of $f^{*}$ is kept to continue training in pretext supervised
    learning progress. The supervised loss is minimized to get final parameters of
    $f$ and $d$.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 然后保持 $f^{*}$ 的参数，继续在预任务监督学习过程中训练。监督损失被最小化以获得最终的 $f$ 和 $d$ 参数。
- en: '| (77) |  | $\underset{d,f}{\min}\mathcal{L}_{ssl}(d,f&#124;_{f_{0}=f^{*}},\mathcal{G};y).$
    |  |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| (77) |  | $\underset{d,f}{\min}\mathcal{L}_{ssl}(d,f&#124;_{f_{0}=f^{*}},\mathcal{G};y).$
    |  |'
- en: 'Collaborative Train. In this strategy, $\mathcal{L}_{ssl}$ and $\mathcal{L}_{sup}$
    are optimized simultaneously. A hyperparameter $\alpha$ is used to balance the
    contribution of pretext task loss and downstream task loss. The overall minimization
    strategy is like traditional supervised strategy with a pretext task regularization:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 协同训练。在此策略中，$\mathcal{L}_{ssl}$ 和 $\mathcal{L}_{sup}$ 被同时优化。使用超参数 $\alpha$ 来平衡预任务损失和下游任务损失的贡献。整体最小化策略类似于传统的有监督策略，但增加了预任务正则化：
- en: '| (78) |  | $\underset{g,f,d}{\min}[\mathcal{L}_{ssl}(g,f,\mathcal{G})+\alpha\mathcal{L}_{sup}(d,f,\mathcal{G};y)].$
    |  |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| (78) |  | $\underset{g,f,d}{\min}[\mathcal{L}_{ssl}(g,f,\mathcal{G})+\alpha\mathcal{L}_{sup}(d,f,\mathcal{G};y)].$
    |  |'
- en: 'Unsupervised Feature Extracting. This strategy is similar to Pre-training and
    Fine-tuning strategy in first step to minimize pretext task loss $\mathcal{L}_{ssl}$
    and get $f^{*}$. However, when minimizing downstream loss $\mathcal{L}_{sup}$,
    the encoder $f^{*}$ is fixed. Also, the training graph data are on same dataset,
    which differs from Pre-training and Fine-tuning strategy. The formulation is defined
    as:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督特征提取。这一策略类似于预训练和微调策略，在第一步中最小化预任务损失 $\mathcal{L}_{ssl}$ 以获得 $f^{*}$。然而，在最小化下游损失
    $\mathcal{L}_{sup}$ 时，编码器 $f^{*}$ 是固定的。此外，训练图数据在相同数据集上，这与预训练和微调策略不同。公式定义为：
- en: '| (79) |  | $g^{*},f^{*}=\underset{g,f}{\arg\min}\mathcal{L}_{ssl}(g,f,\mathcal{D}),\\
    $ |  |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| (79) |  | $g^{*},f^{*}=\underset{g,f}{\arg\min}\mathcal{L}_{ssl}(g,f,\mathcal{D}),\\
    $ |  |'
- en: '| (80) |  | $\underset{d}{\min}\mathcal{L}_{sup}(d,f^{*},\mathcal{G};y).\\
    $ |  |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| (80) |  | $\underset{d}{\min}\mathcal{L}_{sup}(d,f^{*},\mathcal{G};y).\\
    $ |  |'
- en: 8.2\. Generation-based pretext task design
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 基于生成的预任务设计
- en: 'If a model with encoder-decoder structure can reproduce certain graph feature
    from incomplete or perturbed graph, it indicated the encoder has the ability to
    extract useful graph representation. This motivation derived from Autoencoder (Hinton
    and Salakhutdinov, [2006](#bib.bib140)) which originally learns on image dataset.
    In such a case, Equation [76](#S8.E76 "In 8.1\. Overall framework ‣ 8\. Graph
    Self-supervised Learning By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph
    Representation Learning") can be rewritten as:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个具有编码器-解码器结构的模型可以从不完整或扰动的图中重建出某些图特征，这表明编码器具备提取有用图表示的能力。这种动机源自自编码器（Hinton
    and Salakhutdinov, [2006](#bib.bib140)），最初是在图像数据集上进行学习的。在这种情况下，方程 [76](#S8.E76
    "In 8.1\. Overall framework ‣ 8\. Graph Self-supervised Learning By Jingyang Yuan
    ‣ A Comprehensive Survey on Deep Graph Representation Learning") 可以被改写为：
- en: '| (81) |  | $\underset{g,f}{\min}\mathcal{L}_{ssl}(g(f(\hat{\mathcal{G}})),\mathcal{G}),$
    |  |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| (81) |  | $\underset{g,f}{\min}\mathcal{L}_{ssl}(g(f(\hat{\mathcal{G}})),\mathcal{G}),$
    |  |'
- en: 'where $f(\cdot)$ and $g(\cdot)$ stand for the representation encoder and rebuilding
    decoder. However, for graph dataset, feature information and structure information
    are both important composition suitable to be rebuilt. So generation-based pretext
    can be divided into two categories: feature rebuilding and structure rebuilding.
    We introduce several outstanding models in followed part.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(\cdot)$ 和 $g(\cdot)$ 代表表示编码器和重建解码器。然而，对于图数据集，特征信息和结构信息都是重要的组成部分，适合进行重建。因此，基于生成的预训练任务可以分为两个类别：特征重建和结构重建。我们在接下来的部分介绍了几个杰出的模型。
- en: Graph Completion (You et al., [2020](#bib.bib420)) is one of a representative
    method about feature rebuilding. They mask some node features to generate an incomplete
    graph. Then the pretext task is set as predicting the removed node features. As
    shown in Equation [82](#S8.E82 "In 8.2\. Generation-based pretext task design
    ‣ 8\. Graph Self-supervised Learning By Jingyang Yuan ‣ A Comprehensive Survey
    on Deep Graph Representation Learning"), this method can be formulated as a special
    case of Equation [82](#S8.E82 "In 8.2\. Generation-based pretext task design ‣
    8\. Graph Self-supervised Learning By Jingyang Yuan ‣ A Comprehensive Survey on
    Deep Graph Representation Learning"), letting $\mathcal{\hat{G}}=(A,\hat{X})$
    and replacing $\mathcal{G}\xrightarrow{}X$. The loss function is often Mean Squared
    Error or Cross Entropy, depending on the feature is continuous or binary.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 图补全（You et al., [2020](#bib.bib420)）是关于特征重建的一个代表性方法。他们掩盖了一些节点特征以生成一个不完整的图。然后，预训练任务设置为预测被移除的节点特征。如方程
    [82](#S8.E82 "In 8.2\. Generation-based pretext task design ‣ 8\. Graph Self-supervised
    Learning By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph Representation
    Learning") 所示，这种方法可以被表述为方程 [82](#S8.E82 "In 8.2\. Generation-based pretext task
    design ‣ 8\. Graph Self-supervised Learning By Jingyang Yuan ‣ A Comprehensive
    Survey on Deep Graph Representation Learning") 的特例，令 $\mathcal{\hat{G}}=(A,\hat{X})$
    并替换 $\mathcal{G}\xrightarrow{}X$。损失函数通常是均方误差或交叉熵，这取决于特征是连续的还是二进制的。
- en: '| (82) |  | $\underset{g,f}{\min}\ \mathbf{MSE}(g(f(\hat{\mathcal{G}})),\mathbf{X}).$
    |  |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| (82) |  | $\underset{g,f}{\min}\ \mathbf{MSE}(g(f(\hat{\mathcal{G}})),\mathbf{X}).$
    |  |'
- en: Other works make some changes about feature setting. For example, AttrMasking (Hu
    et al., [2019](#bib.bib146)) aims to rebuild both node representation and edge
    representation, AttributeMask (Jin et al., [2020a](#bib.bib167)) preprocess $X$
    firstly by PCA to reduce the complexity of rebuilding features.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 其他工作对特征设置做了一些改动。例如，AttrMasking（Hu et al., [2019](#bib.bib146)）旨在重建节点表示和边表示，而
    AttributeMask（Jin et al., [2020a](#bib.bib167)）首先通过 PCA 预处理 $X$ 以减少重建特征的复杂性。
- en: In the other hand, MGAE (Wang et al., [2017](#bib.bib359)) modify the original
    graph by adding noise in node representation, motivated by denoising autoencoder (Vincent
    et al., [2010](#bib.bib354)). As shown in Equation [82](#S8.E82 "In 8.2\. Generation-based
    pretext task design ‣ 8\. Graph Self-supervised Learning By Jingyang Yuan ‣ A
    Comprehensive Survey on Deep Graph Representation Learning"), we can also consider
    MGAE as an implement of Equation [76](#S8.E76 "In 8.1\. Overall framework ‣ 8\.
    Graph Self-supervised Learning By Jingyang Yuan ‣ A Comprehensive Survey on Deep
    Graph Representation Learning") where $\mathcal{\hat{G}}=(A,\hat{X})$ and $\mathcal{G}\xrightarrow{}X$.
    $\hat{X}$ stands for perturbed node representation. Since the noise are independent
    and random, the encoder are more robust to feature input.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，MGAE（Wang 等，[2017](#bib.bib359)）通过在节点表示中添加噪声来修改原始图，这一方法的灵感来源于去噪自编码器（Vincent
    等，[2010](#bib.bib354)）。如方程 [82](#S8.E82 "在 8.2\. 基于生成的前置任务设计 ‣ 8\. 图的自监督学习 by
    Jingyang Yuan ‣ 关于深度图表示学习的全面调查") 所示，我们也可以将 MGAE 视为方程 [76](#S8.E76 "在 8.1\. 总体框架
    ‣ 8\. 图的自监督学习 by Jingyang Yuan ‣ 关于深度图表示学习的全面调查") 的实现，其中 $\mathcal{\hat{G}}=(A,\hat{X})$
    和 $\mathcal{G}\xrightarrow{}X$。$\hat{X}$ 代表扰动后的节点表示。由于噪声是独立且随机的，编码器对特征输入更具鲁棒性。
- en: '| (83) |  | $\underset{g,f}{\min}\ \mathbf{BCE}(g(f(\hat{\mathcal{G}})),\mathbf{A}).$
    |  |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| (83) |  | $\underset{g,f}{\min}\ \mathbf{BCE}(g(f(\hat{\mathcal{G}})),\mathbf{A}).$
    |  |'
- en: As for structure rebuilding methods, GAE (Kipf and Welling, [2016b](#bib.bib184))
    is the simplest instance, which can be regard as an implement of Equation [76](#S8.E76
    "In 8.1\. Overall framework ‣ 8\. Graph Self-supervised Learning By Jingyang Yuan
    ‣ A Comprehensive Survey on Deep Graph Representation Learning") where $\mathcal{\hat{G}}=\mathcal{G}$
    and $\mathcal{G}\xrightarrow{}A$. A is the adjacency matrix of graph. Similar
    with feature rebuilding method, GAE compresses raw node representation vectors
    into low-dimensional embedding with its encoder. Then the adjacency matrix is
    rebuilt by computing node embedding similarity. Loss function is set to error
    between ground-truth adjacency matrix and the recovered one, to help model rebuild
    correct graph structure.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 对于结构重建方法，GAE（Kipf 和 Welling，[2016b](#bib.bib184)）是最简单的实例，它可以视为方程 [76](#S8.E76
    "在 8.1\. 总体框架 ‣ 8\. 图的自监督学习 by Jingyang Yuan ‣ 关于深度图表示学习的全面调查") 的实现，其中 $\mathcal{\hat{G}}=\mathcal{G}$
    和 $\mathcal{G}\xrightarrow{}A$。A 是图的邻接矩阵。与特征重建方法类似，GAE 将原始节点表示向量压缩成低维嵌入，然后通过计算节点嵌入相似性重建邻接矩阵。损失函数设置为真实邻接矩阵与重建矩阵之间的误差，以帮助模型重建正确的图结构。
- en: 8.3\. Contrast-Based pretext task design
  id: totrans-457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 基于对比的前置任务设计
- en: The mutual information maximization principle, which implement self-supervising
    by predicting the similarity between the two augmented views, forms the foundation
    of contrast-based approaches. Since mutual information represent the degree of
    correlation between two samples, we can maximize it in augmented pairs and minimize
    it in random-selected pairs.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 互信息最大化原则，通过预测两个增强视图之间的相似性来实现自监督，是基于对比的方法的基础。由于互信息表示两个样本之间的相关程度，我们可以在增强对中最大化它，并在随机选择的对中最小化它。
- en: The contrast-based graph SSL taxonomy can be formulated as Equation [84](#S8.E84
    "In 8.3\. Contrast-Based pretext task design ‣ 8\. Graph Self-supervised Learning
    By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph Representation Learning").
    The discriminator that calculates similarity of sample pairs is indicated by pretext
    decoder $g$. $\mathcal{G}^{(1)}$ and $\mathcal{G}^{(2)}$ are two variants of $G$
    that have been augmented. Since graph contrastive learning methods differ from
    each other in 1) view generation, 2) MI estimation method we introduce this methodology
    in these 2 perspectives.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对比的图 SSL 分类可以表述为方程 [84](#S8.E84 "在 8.3\. 基于对比的前置任务设计 ‣ 8\. 图的自监督学习 by Jingyang
    Yuan ‣ 关于深度图表示学习的全面调查")。计算样本对相似性的判别器由前置解码器 $g$ 表示。$\mathcal{G}^{(1)}$ 和 $\mathcal{G}^{(2)}$
    是两个经过增强的 $G$ 的变体。由于图对比学习方法在 1）视图生成，2）MI 估计方法方面有所不同，我们从这两个角度介绍这种方法论。
- en: '| (84) |  | $\underset{g,f}{\min}\mathcal{L}_{ssl}(g[f(\hat{\mathcal{G}}^{(1)}),f(\hat{\mathcal{G}}^{(2)})]).$
    |  |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| (84) |  | $\underset{g,f}{\min}\mathcal{L}_{ssl}(g[f(\hat{\mathcal{G}}^{(1)}),f(\hat{\mathcal{G}}^{(2)})]).$
    |  |'
- en: 8.3.1\. View generation.
  id: totrans-461
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.1\. 视图生成。
- en: Traditional pipeline of contrastive learning-based models is first augmenting
    the graph by well-crafted empirical methods, and then maximizing the consistency
    between different augmentations. Following methods in computer vision domain and
    considering non-Euclidean structure of graph data, typical graph augmentation
    methods aim to modify graph topologically or representatively.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对比学习的模型传统流程是首先通过精心设计的经验方法增强图，然后最大化不同增强之间的一致性。借鉴计算机视觉领域的方法并考虑图数据的非欧几里得结构，典型的图增强方法旨在从拓扑或代表性上修改图。
- en: 'Given graph $\mathcal{G}=(A,X)$, the topologically augmentation methods usually
    modify the adjacency matrix $A$, which can be formulated as:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 给定图 $\mathcal{G}=(A,X)$，拓扑增强方法通常修改邻接矩阵 $A$，这可以表述为：
- en: '| (85) |  | $\hat{A}=\mathscr{T}_{A}(A),$ |  |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| (85) |  | $\hat{A}=\mathscr{T}_{A}(A),$ |  |'
- en: 'where $\mathscr{T}_{A}(\cdot)$ is the transform function of adjacency matrix.
    Topology augmentation methods has many variants, in which the most popular one
    is edge modification, given by $\mathscr{T}_{A}(A)=P\circ A+Q\circ(1-A)$. $P$
    and $Q$ are two matrix representing edge dropping and adding respectively. Another
    method, graph diffusion, connect nodes with their k-hop neighhors with specific
    weight, defined as: $\mathscr{T}_{A}(A)=\sum^{\infty}_{k=0}\alpha_{k}T^{k}$. where
    $\alpha$ and $T$ are coefficient and transition matrix. Graph diffusion method
    can integrate broad topological information with local structure.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathscr{T}_{A}(\cdot)$ 是邻接矩阵的变换函数。拓扑增强方法有许多变体，其中最受欢迎的是边缘修改，表示为 $\mathscr{T}_{A}(A)=P\circ
    A+Q\circ(1-A)$。$P$ 和 $Q$ 是分别表示边缘删除和添加的两个矩阵。另一种方法，图扩散，用特定权重将节点与其 k 跳邻居连接，定义为：$\mathscr{T}_{A}(A)=\sum^{\infty}_{k=0}\alpha_{k}T^{k}$。其中
    $\alpha$ 和 $T$ 是系数和转移矩阵。图扩散方法可以整合广泛的拓扑信息与局部结构。
- en: 'In the other hand, the representative augmentation modify the node representation
    directly, which can be formulated as:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，代表性增强直接修改节点表示，这可以表述为：
- en: '| (86) |  | $\hat{X}=\mathscr{T}_{X}(X),$ |  |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| (86) |  | $\hat{X}=\mathscr{T}_{X}(X),$ |  |'
- en: usually $\mathscr{T}_{X}(\cdot)$ can be a simple masking operater, a.k.a. $\mathscr{T}_{X}(X)=M\circ
    X$ and $M\in\{0,1\}^{N\times D}$. Based on such mask strategy, some method propose
    ways to improve performance. GCA (Zhu et al., [2021](#bib.bib472)) preserves critical
    nodes while giving less significant nodes a larger masking probability, where
    significance is determined by node centrality.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 通常 $\mathscr{T}_{X}(\cdot)$ 可以是一个简单的掩蔽操作符，即 $\mathscr{T}_{X}(X)=M\circ X$ 和
    $M\in\{0,1\}^{N\times D}$。基于这种掩蔽策略，一些方法提出了提高性能的方法。GCA（Zhu et al., [2021](#bib.bib472)）保留了关键节点，同时给较不显著的节点更大的掩蔽概率，其中显著性由节点中心性决定。
- en: 'As introduced before, the paradigm of augmentation has been prove to be effective
    in contrastive learning view generation. However, given the variety of graph data,
    it is challenging to maintain semantics properly during augmentations. In order
    to preserve valuable nature in specific graph dataset, There are currently three
    mainly-used methods: picking by trial-and-errors, trying laborious search or seeking
    domain-specific information as guidance (Luo et al., [2022b](#bib.bib244); Ju
    et al., [2023a](#bib.bib170)). It is clear that such complicated augmentation
    methods constrain the effectiveness and widespread application of graph contrastive
    learning. So many newest works question the necessity of augmentation and seek
    other contrastive views generation methods.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，增强范式已被证明在对比学习视图生成中有效。然而，鉴于图数据的多样性，在增强过程中保持语义正确是一项挑战。为了在特定图数据集中保留有价值的特性，目前主要有三种使用方法：通过试错选择、尝试繁琐搜索或寻求领域特定信息作为指导（Luo
    et al., [2022b](#bib.bib244); Ju et al., [2023a](#bib.bib170)）。显然，这些复杂的增强方法限制了图对比学习的效果和广泛应用。因此，许多最新的研究质疑增强的必要性，并寻求其他对比视图生成方法。
- en: 'SimGCL (Yu et al., [2022](#bib.bib423)) is one of outstanding works challenging
    the effectiveness of graph augmentation. The author find that noise can be a substitution
    to augmentation to produce graph views in specific task such as recommendation.
    After doing ablation study about augmentation and InfoNCE (Xie et al., [2022c](#bib.bib398)),
    they find that the InfoNCE loss, not the augmentation of the graph, is what makes
    the difference. It can be further explained by the importance of distribution
    uniformity. The contrastive learning enhance model representation ability by intensifying
    two characteristics: The alignment of features from positive samples and the uniformity
    of the normalized feature distribution. SimGCL directly add random noises to node
    embeddings as augmentation, to control the uniformity of the representation distribution
    in a more effective way:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: SimGCL （Yu 等， [2022](#bib.bib423)）是挑战图增强有效性的杰出工作之一。作者发现噪声可以作为增强的替代品，以在特定任务中（如推荐）生成图视图。经过关于增强和
    InfoNCE （Xie 等， [2022c](#bib.bib398)）的消融研究，他们发现 InfoNCE 损失，而不是图的增强，是造成差异的原因。这可以通过分布均匀性的重要性进一步解释。对比学习通过强化两个特征来提升模型表示能力：正样本特征的对齐和标准化特征分布的均匀性。SimGCL
    直接将随机噪声添加到节点嵌入中作为增强，以更有效地控制表示分布的均匀性：
- en: '| (87) |  | $\displaystyle\textbf{e}^{(1)}_{i}=\textbf{e}_{i}+\epsilon^{(1)}*\mathbf{\tau}^{(1)}_{i}$
    | $\displaystyle,\quad\textbf{e}^{(2)}_{i}=\textbf{e}_{i}+\epsilon^{(2)}*\mathbf{\tau}^{(2)}_{i},$
    |  |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| (87) |  | $\displaystyle\textbf{e}^{(1)}_{i}=\textbf{e}_{i}+\epsilon^{(1)}*\mathbf{\tau}^{(1)}_{i}$
    | $\displaystyle,\quad\textbf{e}^{(2)}_{i}=\textbf{e}_{i}+\epsilon^{(2)}*\mathbf{\tau}^{(2)}_{i},$
    |  |'
- en: '|  | $\displaystyle\epsilon$ | $\displaystyle\sim\mathcal{N}(0,\sigma^{2}),$
    |  |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\epsilon$ | $\displaystyle\sim\mathcal{N}(0,\sigma^{2}),$
    |  |'
- en: where $\textbf{e}_{i}$ is a node representation in embedding space, $\mathbf{\tau}^{(1)}_{i}$
    and $\mathbf{\tau}^{(2)}_{i}$ are two random sampled unit vector. The experiment
    results indicate that SimGCL performs better than its graph augmentation-based
    competitors and means, while training time is significantly decreased.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{e}_{i}$ 是嵌入空间中的节点表示，$\mathbf{\tau}^{(1)}_{i}$ 和 $\mathbf{\tau}^{(2)}_{i}$
    是两个随机采样的单位向量。实验结果表明，SimGCL 的表现优于其基于图增强的竞争者，同时训练时间显著减少。
- en: 'SimGRACE (Xia et al., [2022b](#bib.bib392)) is another graph contrastive learning
    framework without data augmentation. Motivated by the observation that despite
    encoder disrupted, graph data can effectively maintain their semantics, SimGRACE
    take GNN with its modified version as encoder to produce two contrastive embedding
    views by the same graph input. For GNN encoder $f(\cdot;\theta)$, the two contrastive
    embedding views $\textbf{e},\textbf{e}^{\prime}$ can be computed by:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: SimGRACE （Xia 等， [2022b](#bib.bib392)）是另一种无数据增强的图对比学习框架。受到以下观察的启发：尽管编码器被干扰，图数据仍能有效保持其语义，SimGRACE
    采用 GNN 及其修改版本作为编码器，通过相同的图输入生成两个对比嵌入视图。对于 GNN 编码器 $f(\cdot;\theta)$，两个对比嵌入视图 $\textbf{e},\textbf{e}^{\prime}$
    可以通过以下方式计算：
- en: '| (88) |  | $\displaystyle\textbf{e}^{(1)}=f(\mathcal{G};\theta),$ | $\displaystyle\textbf{e}^{(2)}=f(\mathcal{G};\theta+\epsilon\cdot\Delta\theta),$
    |  |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| (88) |  | $\displaystyle\textbf{e}^{(1)}=f(\mathcal{G};\theta),$ | $\displaystyle\textbf{e}^{(2)}=f(\mathcal{G};\theta+\epsilon\cdot\Delta\theta),$
    |  |'
- en: '|  | $\displaystyle\Delta\theta_{l}$ | $\displaystyle\sim\mathcal{N}(0,\sigma_{l}^{2}),$
    |  |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta\theta_{l}$ | $\displaystyle\sim\mathcal{N}(0,\sigma_{l}^{2}),$
    |  |'
- en: where $\Delta\theta_{l}$ represents GNN parameter perturbation $\Delta\theta$
    in the $l$th layer. SimGRACE can improve alignment and uniformity simutanously,
    proving its capacity of producing high-quality embedding.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Delta\theta_{l}$ 表示 $l$ 层中 GNN 参数的扰动 $\Delta\theta$。SimGRACE 可以同时提高对齐性和均匀性，证明其生成高质量嵌入的能力。
- en: 8.3.2\. MI estimation method.
  id: totrans-478
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.2\. MI 估计方法。
- en: 'The mutual information $I(x,y)$ measures the information that x and y share,
    given a pair of random variables $(x,y)$. As discussed before, mutual information
    is a significant component of contrast-based method by formulating the loss function.
    Mathematically rigorous MI is defined on the probability space, we can formulate
    mutual information between a pair of instances $(x_{i},x_{j})$ as:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 互信息 $I(x,y)$ 衡量了给定一对随机变量 $(x,y)$ 的信息共享。如前所述，互信息是基于对比的方法中通过公式化损失函数来显著的组成部分。从数学上严格的
    MI 定义在概率空间上，我们可以将一对实例 $(x_{i},x_{j})$ 之间的互信息公式化为：
- en: '| (89) |  | $\displaystyle I(x,y)$ | $\displaystyle=D_{KL}(p(x,y)&#124;&#124;p(x)p(y))$
    |  |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| (89) |  | $\displaystyle I(x,y)$ | $\displaystyle=D_{KL}(p(x,y)&#124;&#124;p(x)p(y))$
    |  |'
- en: '|  |  | $\displaystyle=E_{p(x,y)}[\log{\frac{p(x,y)}{p(x)p(y)}}].$ |  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=E_{p(x,y)}[\log{\frac{p(x,y)}{p(x)p(y)}}].$ |  |'
- en: 'However, directly compute Equation [89](#S8.E89 "In 8.3.2\. MI estimation method.
    ‣ 8.3\. Contrast-Based pretext task design ‣ 8\. Graph Self-supervised Learning
    By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph Representation Learning")
    is quiet difficult, so we introduce several different types of estimation for
    MI:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直接计算方程 [89](#S8.E89 "在 8.3.2\. 互信息估计方法。 ‣ 8.3\. 基于对比的预文本任务设计 ‣ 8\. 自监督学习图形
    by Jingyang Yuan ‣ 深度图表示学习的全面调查") 是相当困难的，因此我们介绍了几种不同类型的互信息估计：
- en: 'InfoNCE. Noise-contrastive estimator is a widely used lower bound MI estimatior.
    Given a positive sample $y$ and several negative sample $y^{\prime}_{i}$, a noise-contrastive
    estimator can be fomulated as (Zhu et al., [2020b](#bib.bib471))(Qiu et al., [2020a](#bib.bib287)):'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: InfoNCE。噪声对比估计器是广泛使用的下界互信息估计器。给定一个正样本 $y$ 和几个负样本 $y^{\prime}_{i}$，一个噪声对比估计器可以被表述为
    (Zhu 等， [2020b](#bib.bib471))(Qiu 等， [2020a](#bib.bib287))：
- en: '| (90) |  | $\mathcal{L}=-I(x,y)=-E_{p(x,y)}[\log{\frac{e^{g(x,y)}}{e^{g(x,y)}+\sum_{i}{e^{g(x,y^{\prime}_{i})}}}}],$
    |  |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| (90) |  | $\mathcal{L}=-I(x,y)=-E_{p(x,y)}[\log{\frac{e^{g(x,y)}}{e^{g(x,y)}+\sum_{i}{e^{g(x,y^{\prime}_{i})}}}}],$
    |  |'
- en: usually the kernal function $g(\cdot)$ can be cosine similarity or dot product.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，核函数 $g(\cdot)$ 可以是余弦相似度或点积。
- en: 'Triplet Loss. Intuitively, we can push the similarity between positive samples
    and negative samples differ by a certain distance. So we can define the loss function
    in the following manner (Jiao et al., [2020](#bib.bib163)):'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 三元组损失。从直观上看，我们可以推动正样本之间的相似性和负样本之间的相似性相差一定的距离。因此，我们可以按以下方式定义损失函数 (Jiao 等， [2020](#bib.bib163))：
- en: '| (91) |  | $\mathcal{L}=E_{p(x,y)}[{\max(g(x,y)-g(x,y^{\prime})+\epsilon,0)}],$
    |  |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| (91) |  | $\mathcal{L}=E_{p(x,y)}[{\max(g(x,y)-g(x,y^{\prime})+\epsilon,0)}],$
    |  |'
- en: where $\epsilon$ is a hyperparameter. This function is straightforward and easy
    to compute.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\epsilon$ 是一个超参数。这个函数是直接的，且易于计算。
- en: 'BYOL Loss. Estimation without negative samples is investigated by BYOL (Grill
    et al., [2020](#bib.bib121)). The estimator is Asymmetrical structured:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL 损失。BYOL（Grill 等， [2020](#bib.bib121)）研究了在没有负样本的情况下的估计。估计器是非对称结构的：
- en: '| (92) |  | $\mathcal{L}=E_{p(x,y)}[2-2\frac{g(x)\cdot y}{\&#124;g(x)\&#124;\&#124;y\&#124;}],$
    |  |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| (92) |  | $\mathcal{L}=E_{p(x,y)}[2-2\frac{g(x)\cdot y}{\&#124;g(x)\&#124;\&#124;y\&#124;}],$
    |  |'
- en: note that encoder $g$ should keep the dimension of input and output the same.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，编码器 $g$ 应保持输入和输出的维度相同。
- en: 8.4\. Summary
  id: totrans-492
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4\. 摘要
- en: 'This section introduces graph self-supervised learning and we provide the summary
    as follows:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了图形自监督学习，并提供了如下摘要：
- en: •
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Techniques. Differ from classic supervised learning and semi-supervised learning,
    self-supervised learning increase model generalization ability and robustness,
    whilst decrease label reliance. Graph SSL utilize pretext tasks to extract inherent
    information in representation distribution. Typical Graph SSL methods can be devide
    into generation-based and contrast-based. Generation-based methods learns an encoder
    with ability to reconstruct graph as precise as possible, motivated by Autoencoder.
    Contrast-based methods attract significant interests recently, they learns an
    encoder to minimizing mutual information between relevant instance and maximizing
    mutaul information between unrelated instance.
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术。与经典的监督学习和半监督学习不同，自监督学习提高了模型的泛化能力和鲁棒性，同时减少了对标签的依赖。图形自监督学习利用预文本任务来提取表示分布中的内在信息。典型的图形自监督学习方法可以分为基于生成的和基于对比的。基于生成的方法学习一个编码器，其能力是尽可能精确地重构图，受到自编码器的启发。基于对比的方法最近引起了广泛的关注，它们学习一个编码器以最小化相关实例之间的互信息，并最大化不相关实例之间的互信息。
- en: •
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Challenges and Limitations. Although graph SSL has achieved superior performance
    in many tasks, its theoretical basis is not so solid. Many well-known methods
    are just validated through experiments without explaining theoretical or coming
    up with mathematical proof. It is imperative to establish a strong theoretical
    foundation for graph SSL.
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战与局限性。尽管图形自监督学习在许多任务中取得了优越的性能，但其理论基础并不那么扎实。许多著名的方法只是通过实验验证，而没有理论解释或数学证明。必须建立强大的理论基础来支持图形自监督学习。
- en: •
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future Works. In the future we expect more graph ssl methods designed essentially
    by theoretical proof, without dedicate designed augment process or pretext tasks
    by intuition. This will bring us more definite mathematical properties and less
    ambiguous empirical sense. Also, graphs are a prevalent form of data representation
    across diverse domains, yet obtaining manual labels can be prohibitively expensive.
    Expanding the applications of graph SSL to broader fields is a promising avenue
    for future research.
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来工作。在未来，我们期望更多的图SSL方法能够通过理论证明来设计，而不是依赖直观设计的增强过程或前置任务。这将为我们带来更明确的数学性质和更少的模糊经验感。同时，图是一种在不同领域广泛应用的数据表示形式，但获取人工标签可能非常昂贵。将图SSL的应用扩展到更广泛的领域是未来研究的一个有前景的方向。
- en: 9\. Graph Structure Learning By Jianhao Shen
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 图结构学习 由贾恩浩·申
- en: Graph structure determines how node features propagate and affect each other,
    playing a crucial role in graph representation learning. In some scenarios the
    provided graph is incomplete, noisy, or even has no structure information at all.
    Recent research also finds that graph adversarial attacks (i.e., modifying a small
    number of node features or edges), can degrade learned representations significantly.
    These issues motivate graph structure learning (GSL), which aims to learn a new
    graph structure to produce optimal graph representations. According to how edge
    connectivity is modeled, there are three different approaches in GSL, namely metric-based
    approaches, model-based approaches, and direct approaches. Besides edge modeling,
    regularization is also a common trick to make the learned graph satisfy some desired
    properties. We first present the basic framework and regularization methods for
    GSL in Sec. [9.1](#S9.SS1 "9.1\. Overall Framework ‣ 9\. Graph Structure Learning
    By Jianhao Shen ‣ A Comprehensive Survey on Deep Graph Representation Learning")
    and Sec. [9.2](#S9.SS2 "9.2\. Regularization ‣ 9\. Graph Structure Learning By
    Jianhao Shen ‣ A Comprehensive Survey on Deep Graph Representation Learning"),
    respectively, and then introduce different categories of GSL in Sec. [9.3](#S9.SS3
    "9.3\. Metric-based Methods ‣ 9\. Graph Structure Learning By Jianhao Shen ‣ A
    Comprehensive Survey on Deep Graph Representation Learning"), [9.4](#S9.SS4 "9.4\.
    Model-based Methods ‣ 9\. Graph Structure Learning By Jianhao Shen ‣ A Comprehensive
    Survey on Deep Graph Representation Learning") and [9.5](#S9.SS5 "9.5\. Direct
    Methods ‣ 9\. Graph Structure Learning By Jianhao Shen ‣ A Comprehensive Survey
    on Deep Graph Representation Learning"). We summarize GSL approaches in Table
    [7](#S9.T7 "Table 7 ‣ 9.1\. Overall Framework ‣ 9\. Graph Structure Learning By
    Jianhao Shen ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图结构决定了节点特征如何传播和相互影响，在图表示学习中发挥着至关重要的作用。在某些场景中，提供的图可能是不完整的、嘈杂的，甚至完全没有结构信息。近期研究还发现，图对抗攻击（即修改少量节点特征或边缘）会显著降低学习到的表示。这些问题激发了图结构学习（GSL）的研究，其目标是学习新的图结构以产生最优的图表示。根据边缘连通性的建模方式，GSL有三种不同的方法，即基于度量的方法、基于模型的方法和直接方法。除了边缘建模，正则化也是一种常见的技巧，使学习到的图满足一些期望的属性。我们首先在第[9.1](#S9.SS1
    "9.1\. Overall Framework ‣ 9\. Graph Structure Learning By Jianhao Shen ‣ A Comprehensive
    Survey on Deep Graph Representation Learning")节和第[9.2](#S9.SS2 "9.2\. Regularization
    ‣ 9\. Graph Structure Learning By Jianhao Shen ‣ A Comprehensive Survey on Deep
    Graph Representation Learning")节中介绍GSL的基本框架和正则化方法，然后在第[9.3](#S9.SS3 "9.3\. Metric-based
    Methods ‣ 9\. Graph Structure Learning By Jianhao Shen ‣ A Comprehensive Survey
    on Deep Graph Representation Learning")节、第[9.4](#S9.SS4 "9.4\. Model-based Methods
    ‣ 9\. Graph Structure Learning By Jianhao Shen ‣ A Comprehensive Survey on Deep
    Graph Representation Learning")节和第[9.5](#S9.SS5 "9.5\. Direct Methods ‣ 9\. Graph
    Structure Learning By Jianhao Shen ‣ A Comprehensive Survey on Deep Graph Representation
    Learning")节中介绍GSL的不同类别。我们在表[7](#S9.T7 "Table 7 ‣ 9.1\. Overall Framework ‣ 9\.
    Graph Structure Learning By Jianhao Shen ‣ A Comprehensive Survey on Deep Graph
    Representation Learning")中总结了GSL方法。
- en: 9.1\. Overall Framework
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1\. 总体框架
- en: We denote a graph by $\mathcal{G}=(\mathbf{A},\mathbf{X})$, where $\mathbf{A}\in\mathbb{R}^{N\times
    N}$ is the adjacency matrix and $\mathbf{X}\in\mathbb{R}^{N\times M}$ is the node
    feature matrix with $M$ being the dimension of each node feature. A graph encoder
    $f_{\theta}$ learns to represent the graph based on node features and graph structure
    for task-specific objective $\mathcal{L}_{t}(f_{\theta}(\mathbf{A},\mathbf{X}))$.
    In the GSL setting, there is also a graph structure learner which aims to build
    a new graph adjacency matrix $\mathbf{A}^{*}$ to optimize the learned representation.
    Besides the task-specific objective, a regularization term can be added to constrain
    the learned structure. So the overall objective function of GSL can be formulated
    as
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 $\mathcal{G}=(\mathbf{A},\mathbf{X})$ 表示一个图，其中 $\mathbf{A}\in\mathbb{R}^{N\times
    N}$ 是邻接矩阵，$\mathbf{X}\in\mathbb{R}^{N\times M}$ 是节点特征矩阵，$M$ 是每个节点特征的维度。图编码器 $f_{\theta}$
    基于节点特征和图结构来表示图，用于任务特定目标 $\mathcal{L}_{t}(f_{\theta}(\mathbf{A},\mathbf{X}))$。在
    GSL 设置中，还有一个图结构学习器，旨在构建一个新的图邻接矩阵 $\mathbf{A}^{*}$ 以优化学习到的表示。除了任务特定目标，还可以添加正则化项以约束学习到的结构。因此，GSL
    的总体目标函数可以表示为
- en: '| (93) |  | $\min_{\theta,\mathbf{A}^{*}}\mathcal{L}=\mathcal{L}_{t}(f_{\theta}(\mathbf{A}^{*},\mathbf{X}))+\lambda\mathcal{L}_{r}(\mathbf{A}^{*},\mathbf{A},\mathbf{X}),$
    |  |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| (93) |  | $\min_{\theta,\mathbf{A}^{*}}\mathcal{L}=\mathcal{L}_{t}(f_{\theta}(\mathbf{A}^{*},\mathbf{X}))+\lambda\mathcal{L}_{r}(\mathbf{A}^{*},\mathbf{A},\mathbf{X}),$
    |  |'
- en: where $\mathcal{L}_{t}$ is the task-specific objective, $\mathcal{L}_{r}$ is
    the regularization term and $\lambda$ is a hyperparameter for the weight of regularization.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{t}$ 是任务特定目标，$\mathcal{L}_{r}$ 是正则化项，$\lambda$ 是正则化权重的超参数。
- en: Table 7\. Summary of graph structure learning methods.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7. 图结构学习方法总结
- en: '|  | Method | Structure Learning | Regularization |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 结构学习 | 正则化 |'
- en: '|  | Sparsity | Low-rank | Smoothness |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '|  | 稀疏性 | 低秩 | 平滑性 |'
- en: '| Metric-based | AGCN (Li et al., [2018b](#bib.bib215)) | Mahalanobis distance
    |  |  |  |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 基于度量 | AGCN (Li et al., [2018b](#bib.bib215)) | 马氏距离 |  |  |  |'
- en: '| GRCN (Yu et al., [2021b](#bib.bib422)) | Inner product | ✓ |  |  |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| GRCN (Yu et al., [2021b](#bib.bib422)) | 内积 | ✓ |  |  |'
- en: '| CAGCN (Zhu et al., [2020c](#bib.bib473)) | Inner product | ✓ |  |  |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| CAGCN (Zhu et al., [2020c](#bib.bib473)) | 内积 | ✓ |  |  |'
- en: '| GNNGUARD (Zhang and Zitnik, [2020](#bib.bib441)) | Cosine similarity |  |  |  |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| GNNGUARD (Zhang and Zitnik, [2020](#bib.bib441)) | 余弦相似度 |  |  |  |'
- en: '| IDGL (Chen et al., [2020g](#bib.bib53)) | Cosine similarity | ✓ | ✓ | ✓ |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| IDGL (Chen et al., [2020g](#bib.bib53)) | 余弦相似度 | ✓ | ✓ | ✓ |'
- en: '| HGSL (Zhao et al., [2021b](#bib.bib452)) | Cosine similarity | ✓ |  |  |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| HGSL (Zhao et al., [2021b](#bib.bib452)) | 余弦相似度 | ✓ |  |  |'
- en: '| GDC (Gasteiger et al., [2019](#bib.bib109)) | Graph diffusion | ✓ |  |  |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| GDC (Gasteiger et al., [2019](#bib.bib109)) | 图扩散 | ✓ |  |  |'
- en: '| Model-based | GLN (Pilco and Rivera, [2019](#bib.bib278)) | Recurrent blocks
    |  |  |  |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 基于模型 | GLN (Pilco and Rivera, [2019](#bib.bib278)) | 递归块 |  |  |  |'
- en: '| GLCN (Jiang et al., [2019](#bib.bib160)) | One-layer neural network | ✓ |  |
    ✓ |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| GLCN (Jiang et al., [2019](#bib.bib160)) | 一层神经网络 | ✓ |  | ✓ |'
- en: '| NeuralSparse (Zheng et al., [2020](#bib.bib458)) | Multi-layer neural network
    | ✓ |  |  |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| NeuralSparse (Zheng et al., [2020](#bib.bib458)) | 多层神经网络 | ✓ |  |  |'
- en: '| GAT (Veličković et al., [2017](#bib.bib352)) | Self-attention |  |  |  |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| GAT (Veličković et al., [2017](#bib.bib352)) | 自注意力 |  |  |  |'
- en: '| GaAN (Zhang et al., [2018b](#bib.bib436)) | Gated attention |  |  |  |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| GaAN (Zhang et al., [2018b](#bib.bib436)) | 门控注意力 |  |  |  |'
- en: '| hGAO (Gao and Ji, [2019](#bib.bib105)) | Hard attention | ✓ |  |  |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| hGAO (Gao and Ji, [2019](#bib.bib105)) | 硬注意力 | ✓ |  |  |'
- en: '| VIB-GSL (Sun et al., [2022](#bib.bib338)) | Dot-product attention | ✓ |  |  |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| VIB-GSL (Sun et al., [2022](#bib.bib338)) | 点积注意力 | ✓ |  |  |'
- en: '| MAGNA (Wang et al., [2020c](#bib.bib366)) | Graph attention diffusion |  |  |  |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| MAGNA (Wang et al., [2020c](#bib.bib366)) | 图注意力扩散 |  |  |  |'
- en: '| Direct | GLNN (Gao et al., [2020](#bib.bib107)) | MAP estimation | ✓ |  |
    ✓ |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| 直接 | GLNN (Gao et al., [2020](#bib.bib107)) | MAP估计 | ✓ |  | ✓ |'
- en: '| GSML (Wan and Kokel, [2021](#bib.bib358)) | Bilevel optimization | ✓ |  |  |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| GSML (Wan and Kokel, [2021](#bib.bib358)) | 双层优化 | ✓ |  |  |'
- en: '| BGCNN (Zhang et al., [2019c](#bib.bib443)) | Bayesion optimization |  |  |  |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| BGCNN (Zhang et al., [2019c](#bib.bib443)) | 贝叶斯优化 |  |  |  |'
- en: '| VGCN (Elinas et al., [2020](#bib.bib84)) | Stochastic variational inference
    |  |  |  |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| VGCN (Elinas et al., [2020](#bib.bib84)) | 随机变分推断 |  |  |  |'
- en: 9.2\. Regularization
  id: totrans-528
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2. 正则化
- en: The goal of regularization is to constrain the learned graph to satisfy some
    properties by adding some penalties to the learned structure. The most common
    properties used in GSL are sparsity, low lank, and smoothness.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的目标是通过向学习到的结构添加一些惩罚，约束学习到的图满足一些特性。GSL 中最常用的特性包括稀疏性、低秩和光滑性。
- en: 9.2.1\. Sparsity
  id: totrans-530
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1\. 稀疏性
- en: 'Noise or adversarial attacks will introduce redundant edges into graphs and
    degrade the quality of graph representation. An effective technique to remove
    unnecessary edges is sparsity regularization, i.e., adding a penalty on the number
    of nonzero entries of the adjacency matrix ($\ell_{0}$-norm):'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声或对抗攻击会在图中引入冗余边，降低图表示的质量。去除不必要边的一种有效技术是稀疏性正则化，即对邻接矩阵的非零条目数量添加惩罚（$\ell_{0}$-范数）：
- en: '| (94) |  | $\mathcal{L}_{sp}=\lVert\mathbf{A}\rVert_{0},$ |  |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| (94) |  | $\mathcal{L}_{sp}=\lVert\mathbf{A}\rVert_{0},$ |  |'
- en: however, $\ell_{0}$-norm is not differentiable so optimizing it is difficult,
    and in many cases $\ell_{1}$-norm is used instead as a convex relaxation. Other
    methods to impose sparsity include pruning and discretization. These processes
    are also called postprocessing since they usually happen after the adjacency matrix
    is learned. Pruning removes part of the edges according to some criteria. For
    example, edges with weights lower than a threshold, or those not in the top-K
    edges of nodes or graph. Discretization is applied to generate graph structure
    by sampling from some distribution. Compared to directly learning edge weights,
    sampling enjoys the advantage to control the generated graph, but has issues during
    optimizing since sampling itself is discrete and hard to optimize. Reparameterization
    and Gumbel-softmax are two useful techniques to overcome such issue, and are widely
    adopted in GSL.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，$\ell_{0}$-范数不可导，因此优化它很困难。在许多情况下，$\ell_{1}$-范数作为凸松弛方法被使用。施加稀疏性的其他方法包括剪枝和离散化。这些过程也称为后处理，因为它们通常发生在邻接矩阵学习之后。剪枝根据某些标准去除部分边。例如，权重低于阈值的边，或那些不在节点或图的前-K
    边中。离散化通过从某些分布中采样生成图结构。与直接学习边权重相比，采样具有控制生成图的优势，但在优化过程中存在问题，因为采样本身是离散的且难以优化。重新参数化和
    Gumbel-softmax 是解决这些问题的两种有用技术，并在 GSL 中得到广泛采用。
- en: 9.2.2\. Low Rank
  id: totrans-534
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.2\. 低秩
- en: 'In real-world graphs, similar nodes are likely to group together and form communities,
    which should lead to a low-rank adjacency matrix. Recent work also finds that
    adversarial attacks tend to increase the rank of the adjacency matrix quickly.
    Therefore, low rank regularization is also a useful tool to make graph representation
    learning more robust:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的图中，相似的节点很可能会聚集在一起形成社区，这应导致一个低秩的邻接矩阵。最近的研究还发现，对抗攻击倾向于迅速增加邻接矩阵的秩。因此，低秩正则化也是使图表示学习更加鲁棒的有用工具：
- en: '| (95) |  | $\mathcal{L}_{lr}=Rank(\mathbf{A}).$ |  |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| (95) |  | $\mathcal{L}_{lr}=Rank(\mathbf{A}).$ |  |'
- en: 'It is hard to minimize matrix rank directly. A common technique is to optimize
    the nuclear norm, which is a convex envelope of the matrix rank:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 直接最小化矩阵秩是困难的。一种常见技术是优化核范数，核范数是矩阵秩的一个凸包：
- en: '| (96) |  | $\mathcal{L}_{nc}=\lVert\mathbf{A}\rVert_{*}=\sum_{i}^{N}\sigma_{i},$
    |  |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| (96) |  | $\mathcal{L}_{nc}=\lVert\mathbf{A}\rVert_{*}=\sum_{i}^{N}\sigma_{i},$
    |  |'
- en: where $\sigma_{i}$ are singular values of $\mathbf{A}$. Entezari et al. replaces
    the learned adjacency matrix with rank-r approximation by singular value decomposition
    (SVD) to achieve robust graph learning against adversarial attacks.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma_{i}$ 是 $\mathbf{A}$ 的奇异值。Entezari 等人通过奇异值分解（SVD）将学习到的邻接矩阵替换为秩-r 近似，从而实现对抗攻击下的鲁棒图学习。
- en: 9.2.3\. Smoothness
  id: totrans-540
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.3\. 平滑性
- en: 'A common assumption is that connected nodes share similar features, or in other
    words, the graph is “smooth” as the difference between local neighbors is small.
    The following metric is a natural way to measure graph smoothness:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的假设是连接的节点具有相似的特征，换句话说，图是“光滑的”，因为局部邻居之间的差异很小。以下度量是衡量图光滑性的自然方法：
- en: '| (97) |  | $\mathcal{L}_{sm}=\frac{1}{2}\sum_{i,j=1}^{N}A_{ij}(x_{i}-x_{j})^{2}=tr(\mathbf{X}^{\top}\mathbf{(D-A)X})=tr(\mathbf{X}^{\top}\mathbf{LX}),$
    |  |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| (97) |  | $\mathcal{L}_{sm}=\frac{1}{2}\sum_{i,j=1}^{N}A_{ij}(x_{i}-x_{j})^{2}=tr(\mathbf{X}^{\top}\mathbf{(D-A)X})=tr(\mathbf{X}^{\top}\mathbf{LX}),$
    |  |'
- en: where $\mathbf{D}$ is the degree matrix of $\mathbf{A}$ and $\mathbf{L}=\mathbf{D-A}$
    is called graph Laplacian. A variant is to use the normalized graph Laplacian
    $\widehat{\mathbf{L}}=\mathbf{D}^{-\frac{1}{2}}\mathbf{LD}^{-\frac{1}{2}}$.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{D}$ 是 $\mathbf{A}$ 的度矩阵，而 $\mathbf{L}=\mathbf{D-A}$ 被称为图拉普拉斯算子。一种变体是使用标准化的图拉普拉斯算子
    $\widehat{\mathbf{L}}=\mathbf{D}^{-\frac{1}{2}}\mathbf{LD}^{-\frac{1}{2}}$。
- en: 9.3\. Metric-based Methods
  id: totrans-544
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3\. 基于度量的方法
- en: Metric-based methods measure the similarity between nodes as the edge weights.
    They follow the basic assumption that similar nodes tend to have connection with
    each other. We show some representative works
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 基于度量的方法将节点之间的相似性度量为边权重。它们遵循一个基本假设，即相似的节点往往会彼此连接。我们展示了一些代表性的工作。
- en: 'Adaptive Graph Convolutional Neural Networks (Li et al., [2018b](#bib.bib215))
    (AGCN). AGCN learns a task-driven adaptive graph during training to enable a more
    generalized and flexible graph representation model. After parameterizing the
    distance metric between nodes, AGCN is able to adapt graph topology to the given
    task. It proposes generalized Mahalanobis distance between two nodes with the
    following formula:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应图卷积神经网络 (Li et al., [2018b](#bib.bib215)) (AGCN)。AGCN 在训练过程中学习任务驱动的自适应图，以实现更通用和灵活的图表示模型。在对节点之间的距离度量进行参数化后，AGCN
    能够根据给定任务适应图拓扑。它提出了两个节点之间的广义马哈拉诺比斯距离，其公式如下：
- en: '| (98) |  | $\mathbb{D}(x_{i},x_{j})=\sqrt{(x_{i}-x_{j})^{\top}M(x_{i}-x_{j})},$
    |  |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| (98) |  | $\mathbb{D}(x_{i},x_{j})=\sqrt{(x_{i}-x_{j})^{\top}M(x_{i}-x_{j})},$
    |  |'
- en: 'where $M=W_{d}W_{d}^{\top}$ and $W_{d}$ is the trainable weights to minimize
    task-specific objective. Then the Gaussian kernel is used to obtain the adjacency
    matrix:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M=W_{d}W_{d}^{\top}$，$W_{d}$ 是用于最小化任务特定目标的可训练权重。然后使用高斯核函数来获得邻接矩阵：
- en: '| (99) |  | $\displaystyle\mathbb{G}_{ij}$ | $\displaystyle=\exp(-\mathbb{D}(x_{i},x_{j})/(2\sigma^{2})),$
    |  |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| (99) |  | $\displaystyle\mathbb{G}_{ij}$ | $\displaystyle=\exp(-\mathbb{D}(x_{i},x_{j})/(2\sigma^{2})),$
    |  |'
- en: '| (100) |  | $\displaystyle\hat{A}$ | $\displaystyle=normalize(\mathbb{G}).$
    |  |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| (100) |  | $\displaystyle\hat{A}$ | $\displaystyle=normalize(\mathbb{G}).$
    |  |'
- en: Graph-Revised Convolutional Network (Yu et al., [2021b](#bib.bib422)) (GRCN).
    GRCN uses a graph revision module to predict missing edges and revise edge weights
    through joint optimization on downstream tasks. It first learns the node embedding
    with GCN and then calculates pair-wise node similarity with the dot product as
    the kernel function.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 图修正卷积网络 (Yu et al., [2021b](#bib.bib422)) (GRCN)。GRCN 使用图修正模块来预测缺失的边，并通过下游任务上的联合优化来修正边权重。它首先通过
    GCN 学习节点嵌入，然后使用点积作为核函数计算成对节点的相似性。
- en: '| (101) |  | $\displaystyle Z$ | $\displaystyle=GCN_{g}(A,X),$ |  |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| (101) |  | $\displaystyle Z$ | $\displaystyle=GCN_{g}(A,X),$ |  |'
- en: '| (102) |  | $\displaystyle S_{ij}$ | $\displaystyle=\left\langle z_{i},z_{j}\right\rangle.$
    |  |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| (102) |  | $\displaystyle S_{ij}$ | $\displaystyle=\left\langle z_{i},z_{j}\right\rangle.$
    |  |'
- en: 'The revised adjacency matrix is the residual summation of the original adjacency
    matrix $\hat{A}=A+S$. GRCN also applies a sparsification technique on the similarity
    matrix $S$ to reduce computation cost:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 修正后的邻接矩阵是原始邻接矩阵的残差总和 $\hat{A}=A+S$。GRCN 还对相似性矩阵 $S$ 应用了稀疏化技术，以减少计算成本：
- en: '| (103) |  | $S^{(K)}_{ij}=\left\{\begin{aligned} S_{ij},&amp;~{}~{}S_{ij}\in
    topK(S_{i})\\ 0,&amp;~{}~{}S_{ij}\notin topK(S_{i})\end{aligned}\right..$ |  |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| (103) |  | $S^{(K)}_{ij}=\left\{\begin{aligned} S_{ij},&amp;~{}~{}S_{ij}\in
    topK(S_{i})\\ 0,&amp;~{}~{}S_{ij}\notin topK(S_{i})\end{aligned}\right..$ |  |'
- en: Threshold pruning is also a common strategy for sparsification. For example,
    CAGCN (Zhu et al., [2020c](#bib.bib473)) also uses dot product to measure node
    similarity, and refines the graph structure by removing edges between nodes whose
    similarity is less than a threshold $\tau_{r}$ and adding edges between nodes
    whose similarity is greater than another threshold $\tau_{a}$.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值剪枝也是一种常见的稀疏化策略。例如，CAGCN (Zhu et al., [2020c](#bib.bib473)) 也使用点积来度量节点相似性，并通过删除相似性小于阈值
    $\tau_{r}$ 的节点之间的边以及添加相似性大于另一个阈值 $\tau_{a}$ 的节点之间的边来优化图结构。
- en: Defending Graph Neural Networks against Adversarial Attacks
  id: totrans-557
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 防御图神经网络对抗攻击
- en: '(Zhang and Zitnik, [2020](#bib.bib441)) (GNNGuard). GNNGuard measures similarity
    between a node $u$ and its neighbor $v$ in the $k$-th layer by cosine similarity
    and normalizes node similarity at the node level within the neighborhood as follows:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: (Zhang and Zitnik, [2020](#bib.bib441)) (GNNGuard)。GNNGuard 通过余弦相似性来度量第 $k$
    层中节点 $u$ 与其邻居 $v$ 之间的相似性，并在邻域内按节点级别对节点相似性进行标准化，如下所示：
- en: '| (104) |  | $\displaystyle s_{uv}^{k}$ | $\displaystyle=\frac{h_{u}^{k}\odot
    h_{v}^{k}}{\&#124;h_{u}^{k}\&#124;_{2}\&#124;h_{v}^{k}\&#124;_{2}},$ |  |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| (104) |  | $\displaystyle s_{uv}^{k}$ | $\displaystyle=\frac{h_{u}^{k}\odot
    h_{v}^{k}}{\&#124;h_{u}^{k}\&#124;_{2}\&#124;h_{v}^{k}\&#124;_{2}},$ |  |'
- en: '| (105) |  | $\displaystyle\alpha_{uv}^{k}$ | $\displaystyle=\left\{\begin{aligned}
    &amp;s_{uv}^{k}/\sum\nolimits_{v\in\mathcal{N}_{u}}s_{uv}^{k}\times\hat{N}_{u}^{k}/(\hat{N}_{u}^{k}+1),&amp;~{}~{}if~{}~{}u\neq
    v\\ &amp;1/(\hat{N}_{u}^{k}+1),&amp;~{}~{}if~{}~{}u=v\end{aligned}\right.,$ |  |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| (105) |  | $\displaystyle\alpha_{uv}^{k}$ | $\displaystyle=\left\{\begin{aligned}
    &amp;s_{uv}^{k}/\sum\nolimits_{v\in\mathcal{N}_{u}}s_{uv}^{k}\times\hat{N}_{u}^{k}/(\hat{N}_{u}^{k}+1),&amp;~{}~{}if~{}~{}u\neq
    v\\ &amp;1/(\hat{N}_{u}^{k}+1),&amp;~{}~{}if~{}~{}u=v\end{aligned}\right.,$ |  |'
- en: where $\mathcal{N}_{u}$ denotes the neighborhood of node $u$ and $\hat{N}_{u}^{k}=\sum\nolimits_{v\in\mathcal{N}_{u}}\|s_{uv}^{k}\|_{0}$.
    To stabilize GNN training, it also proposes a layer-wise graph memory by keeping
    part of the information from the previous layer in the current layer. Similar
    to GNNGuard, IDGL (Chen et al., [2020g](#bib.bib53)) uses multi-head cosine similarity
    and mask edges with node similarity smaller than a non-negative threshold, and
    HGSL (Zhao et al., [2021b](#bib.bib452)) generalizes this idea to heterogeneous
    graphs.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{N}_{u}$表示节点$u$的邻域，$\hat{N}_{u}^{k}=\sum\nolimits_{v\in\mathcal{N}_{u}}\|s_{uv}^{k}\|_{0}$。为了稳定GNN训练，它还提出了一个逐层图记忆，通过在当前层中保留来自上一层的部分信息。类似于GNNGuard，IDGL（Chen
    等人，[2020g](#bib.bib53)）使用多头余弦相似度，并用节点相似度小于非负阈值的边进行掩码，而HGSL（Zhao 等人，[2021b](#bib.bib452)）将这一思想推广到异质图。
- en: Graph Diffusion Convolution
  id: totrans-562
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图扩散卷积
- en: '(Gasteiger et al., [2019](#bib.bib109)) (GDC). GDC replaces the original adjacency
    matrix with generalized graph diffusion matrix $\mathbf{S}$:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: (Gasteiger 等人，[2019](#bib.bib109)) (GDC)。GDC用广义图扩散矩阵$\mathbf{S}$替换了原始的邻接矩阵：
- en: '| (106) |  | $\mathbf{S}=\sum_{k=0}^{\infty}\theta_{k}\mathbf{T}^{k},$ |  |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| (106) |  | $\mathbf{S}=\sum_{k=0}^{\infty}\theta_{k}\mathbf{T}^{k},$ |  |'
- en: where $\theta_{k}$ is the weighting coefficient and $\mathbf{T}$ is the generalized
    transition matrix. To ensure convergence, GDC further requires that $\sum_{k=0}^{\infty}\theta_{k}=1$
    and the eigenvalues of $\mathbf{T}$ lie in $[0,1]$. The random walk transition
    matrix $\mathbf{T}_{rw}=\mathbf{AD}^{-1}$ and the symmetric transition matrix
    $\mathbf{T}_{sym}=\mathbf{D}^{-1/2}\mathbf{AD}^{-1/2}$ are two examples. This
    new graph structure allows graph convolution to aggregate information from a larger
    neighborhood. The graph diffusion acts as a smoothing operator to filter out underlying
    noise. However, in most cases graph diffusion will result in a dense adjacency
    matrix $S$, so sparsification technology like top-k filtering and threshold filtering
    will be applied to graph diffusion. Following GDC, there are some other graph
    diffusion proposed. For example, AdaCAD (Lim et al., [2021](#bib.bib226)) proposes
    Class-Attentive Diffusion, which further considers node features and aggregates
    nodes probably of the same class among K-hop neighbors. Adaptive diffusion convolution (Zhao
    et al., [2021a](#bib.bib451)) (ADC) learns the optimal neighborhood size via optimizing
    a bi-level problem.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\theta_{k}$是权重系数，$\mathbf{T}$是广义转移矩阵。为了确保收敛，GDC进一步要求$\sum_{k=0}^{\infty}\theta_{k}=1$，并且$\mathbf{T}$的特征值位于$[0,1]$。随机游走转移矩阵$\mathbf{T}_{rw}=\mathbf{AD}^{-1}$和对称转移矩阵$\mathbf{T}_{sym}=\mathbf{D}^{-1/2}\mathbf{AD}^{-1/2}$是两个例子。这种新的图结构允许图卷积从更大的邻域中聚合信息。图扩散作为一个平滑算子，能够滤除潜在的噪声。然而，在大多数情况下，图扩散会导致一个稠密的邻接矩阵$S$，因此像top-k过滤和阈值过滤这样的稀疏化技术将应用于图扩散。继GDC之后，还有一些其他的图扩散方法被提出。例如，AdaCAD（Lim
    等人，[2021](#bib.bib226)）提出了Class-Attentive Diffusion，进一步考虑了节点特征，并在K-hop邻域中聚合可能属于同一类别的节点。自适应扩散卷积（Zhao
    等人，[2021a](#bib.bib451)）(ADC)通过优化双层问题来学习最优的邻域大小。
- en: 9.4\. Model-based Methods
  id: totrans-566
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4\. 基于模型的方法
- en: Model-based methods parameterize edge weights with more complex models like
    deep neural networks. Compared to metric-based methods, model-based methods offer
    greater flexibility and expressive power.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的方法使用更复杂的模型如深度神经网络对边权进行参数化。与基于度量的方法相比，基于模型的方法提供了更大的灵活性和表达能力。
- en: 'Graph Learning Network (Pilco and Rivera, [2019](#bib.bib278)) (GLN). GLN proposes
    a recurrent block to first produce intermediate node embeddings and then merge
    them with adjacency information as the output of this layer to predict the adjacency
    matrix for the next layer. Specifically, it uses convolutional graph operations
    to extract node features, and creates a local-context embedding based on node
    features and the current adjacency matrix:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 图学习网络（Pilco 和 Rivera，[2019](#bib.bib278)）(GLN)。GLN 提出了一个递归块，首先生成中间节点嵌入，然后将其与邻接信息合并，作为该层的输出以预测下一层的邻接矩阵。具体而言，它使用卷积图操作来提取节点特征，并基于节点特征和当前邻接矩阵创建本地上下文嵌入：
- en: '| (107) |  | $\displaystyle H_{int}^{(l)}=\sum_{i=1}^{k}\sigma_{l}(\tau(A^{(l)})H^{(l)}W_{i}^{(l)}),$
    |  |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| (107) |  | $\displaystyle H_{int}^{(l)}=\sum_{i=1}^{k}\sigma_{l}(\tau(A^{(l)})H^{(l)}W_{i}^{(l)}),$
    |  |'
- en: '| (108) |  | $\displaystyle H_{local}^{(l)}=\sigma_{l}(\tau(A^{(l)})H_{int}^{(l)}U^{(l)}),$
    |  |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| (108) |  | $\displaystyle H_{local}^{(l)}=\sigma_{l}(\tau(A^{(l)})H_{int}^{(l)}U^{(l)}),$
    |  |'
- en: 'where $W_{i}^{(l)}$ and $U^{(l)}$ are the learnable weights. GLN then predicts
    the next adjacency matrix as follows:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{i}^{(l)}$ 和 $U^{(l)}$ 是可学习的权重。GLN 然后通过以下方式预测下一邻接矩阵：
- en: '| (109) |  | $A^{(l+1)}=\sigma_{l}(M^{(l)}\alpha_{l}(H_{local}^{(l)}){M^{(l)}}^{\top}).$
    |  |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| (109) |  | $A^{(l+1)}=\sigma_{l}(M^{(l)}\alpha_{l}(H_{local}^{(l)}){M^{(l)}}^{\top}).$
    |  |'
- en: Similarly, GLCN (Jiang et al., [2019](#bib.bib160)) models graph structure with
    a softmax layer over the inner product between the difference of node features
    and a learnable vector. NeuralSparse (Zheng et al., [2020](#bib.bib458)) uses
    a multi-layer neural network to generate a learnable distribution from which a
    sparse graph structure is sampled. PTDNet (Luo et al., [2021a](#bib.bib241)) prunes
    graph edges with a multi-layer neural network and penalizes the number of non-zero
    elements to encourage sparsity.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，GLCN（Jiang 等，[2019](#bib.bib160)）通过对节点特征差异和一个可学习向量之间的内积进行 softmax 层建模图结构。NeuralSparse（Zheng
    等，[2020](#bib.bib458)）使用多层神经网络生成一个可学习的分布，从中采样稀疏图结构。PTDNet（Luo 等，[2021a](#bib.bib241)）使用多层神经网络修剪图边，并惩罚非零元素的数量以鼓励稀疏性。
- en: 'Graph Attention Networks (Veličković et al., [2017](#bib.bib352)) (GAT). Besides
    constructing a new graph to guide the massage passing and aggregation process
    of GNNs, many recent researchers also leverage the attention mechanism to adaptively
    model the relationship between nodes. GAT is the first work to introduce the self-attention
    strategy into graph learning. In each attention layer, the attention weight between
    two nodes is calculated as the Softmax output on the combination of linear and
    non-linear transform of node features:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 图注意力网络（Veličković 等， [2017](#bib.bib352)）(GAT)。除了构造新的图来指导图神经网络（GNN）的信息传递和聚合过程外，许多近期的研究者还利用注意力机制自适应地建模节点之间的关系。GAT
    是首个将自注意力策略引入图学习的工作。在每个注意力层中，两个节点之间的注意力权重是通过对节点特征的线性和非线性变换组合进行 Softmax 输出计算的：
- en: '| (110) |  | $\displaystyle e_{ij}$ | $\displaystyle=a(\mathbf{W}\vec{h}_{i},\mathbf{W}\vec{h}_{j}),$
    |  |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| (110) |  | $\displaystyle e_{ij}$ | $\displaystyle=a(\mathbf{W}\vec{h}_{i},\mathbf{W}\vec{h}_{j}),$
    |  |'
- en: '| (111) |  | $\displaystyle\alpha_{ij}$ | $\displaystyle=\frac{exp(e_{ij})}{\sum_{k\in\mathcal{N}_{i}}exp(e_{ik})},$
    |  |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| (111) |  | $\displaystyle\alpha_{ij}$ | $\displaystyle=\frac{exp(e_{ij})}{\sum_{k\in\mathcal{N}_{i}}exp(e_{ik})},$
    |  |'
- en: 'where $\mathcal{N}_{i}$ denotes the neighborhood of node $i$,$\mathbf{W}$ is
    learnable linear transform and $a$ is pre-defined attention function. In the original
    implementation of GAT, $a$ is a single-layer neural network with $\mathrm{LeakyReLU}$:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{N}_{i}$ 表示节点 $i$ 的邻域，$\mathbf{W}$ 是可学习的线性变换，$a$ 是预定义的注意力函数。在 GAT
    的原始实现中，$a$ 是一个具有 $\mathrm{LeakyReLU}$ 的单层神经网络：
- en: '| (112) |  | $a(\mathbf{W}\vec{h}_{i},\mathbf{W}\vec{h}_{j})=\mathrm{LeakyReLU}(\vec{\mathrm{a}}^{\top}[\mathbf{W}\vec{h}_{i}&#124;&#124;\mathbf{W}\vec{h}_{j}]).$
    |  |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| (112) |  | $a(\mathbf{W}\vec{h}_{i},\mathbf{W}\vec{h}_{j})=\mathrm{LeakyReLU}(\vec{\mathrm{a}}^{\top}[\mathbf{W}\vec{h}_{i}&#124;&#124;\mathbf{W}\vec{h}_{j}]).$
    |  |'
- en: 'The attention weights are then used to guide the message-passing phase of GNNs:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力权重随后用于指导 GNN 的信息传递阶段：
- en: '| (113) |  | $\vec{h}^{\prime}_{i}=\sigma(\sum_{j\in\mathcal{N}_{i}}\alpha_{ij}\mathbf{W}\vec{h}_{j}),$
    |  |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| (113) |  | $\vec{h}^{\prime}_{i}=\sigma(\sum_{j\in\mathcal{N}_{i}}\alpha_{ij}\mathbf{W}\vec{h}_{j}),$
    |  |'
- en: where $\sigma$ is a nonlinear function. It is beneficial to concatenate multiple
    heads of attention together to get a more stable and generalizable model, so-called
    multi-head attention. The attention mechanism serves as a soft graph structure
    learner which captures important connections within node neighborhoods. Following
    GAT, many recent works propose more effective and efficient graph attention operators
    to improve performance. GaAN (Zhang et al., [2018b](#bib.bib436)) adds a soft
    gate at each attention head to adjust its importance. MAGNA (Wang et al., [2020c](#bib.bib366))
    proposes a novel graph attention diffusion layer to incorporate multi-hop information.
    One drawback of graph attention is that the time and space complexities are both
    $O(N^{3})$. hGAO (Gao and Ji, [2019](#bib.bib105)) performs hard graph attention
    by limiting node attention to its neighborhood. VIB-GSL (Sun et al., [2022](#bib.bib338))
    adopts the information bottleneck principle to guide feature masking in order
    to drop task-irrelevant information and preserve actionable information for the
    downstream task.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\sigma$是一个非线性函数。将多个注意力头串联在一起，以获得一个更稳定和更具泛化能力的模型是有益的，这就是所谓的多头注意力。注意力机制作为一个软图结构学习器，捕捉节点邻域内的重要连接。继GAT之后，许多近期的工作提出了更有效和更高效的图注意力操作符，以提高性能。GaAN（Zhang等，[2018b](#bib.bib436)）在每个注意力头上添加了一个软门控来调整其重要性。MAGNA（Wang等，[2020c](#bib.bib366)）提出了一种新型的图注意力扩散层，以整合多跳信息。图注意力的一个缺点是时间和空间复杂度都是$O(N^{3})$。hGAO（Gao和Ji，[2019](#bib.bib105)）通过将节点注意力限制在其邻域内来执行硬图注意力。VIB-GSL（Sun等，[2022](#bib.bib338)）采用信息瓶颈原理来指导特征掩蔽，以去除与任务无关的信息并保留下游任务所需的可操作信息。
- en: 9.5\. Direct Methods
  id: totrans-582
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5\. 直接方法
- en: Direct methods treat edge weights as free learnable parameters. These methods
    enjoy more flexibility but are also more difficult to train. The optimization
    is usually carried out in an alternating way, i.e., iteratively updating the adjacency
    matrix $\mathbf{A}$ and the GNN encoder parameters $\theta$.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 直接方法将边权视为自由可学习的参数。这些方法具有更大的灵活性，但训练起来也更困难。优化通常以交替的方式进行，即迭代更新邻接矩阵$\mathbf{A}$和GNN编码器参数$\theta$。
- en: 'GLNN (Gao et al., [2020](#bib.bib107)). GLNN uses MAP estimation to learn an
    optimal adjacency matrix for a joint objective function including sparsity and
    smoothness. Specifically, it targets at finding the most probable adjacency matrix
    $\hat{A}$ given graph node features $x$:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: GLNN（Gao等，[2020](#bib.bib107)）。GLNN使用MAP估计来学习一个最优的邻接矩阵，以实现包括稀疏性和光滑性的联合目标函数。具体来说，它的目标是找到在给定图节点特征$x$的情况下最可能的邻接矩阵$\hat{A}$：
- en: '| (114) |  | $\tilde{A}_{MAP}(x)=\mathop{\mathrm{argmax}}\limits_{\hat{A}}f(x&#124;\hat{A})g(\hat{A}),$
    |  |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| (114) |  | $\tilde{A}_{MAP}(x)=\mathop{\mathrm{argmax}}\limits_{\hat{A}}f(x&#124;\hat{A})g(\hat{A}),$
    |  |'
- en: 'where $f(x|\hat{A})$ measures the likelihood of observing $x$ given $\hat{A}$,
    and $g(\hat{A})$ is the prior distribution of $\hat{A}$. GLNN uses sparsity and
    property constraint as prior, and define the likelihood function $f$ as:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$f(x|\hat{A})$衡量在给定$\hat{A}$的情况下观察到$x$的可能性，而$g(\hat{A})$是$\hat{A}$的先验分布。GLNN将稀疏性和属性约束作为先验，并将可能性函数$f$定义为：
- en: '| (115) |  | $\displaystyle f(x&#124;\hat{A})$ | $\displaystyle=exp(-\lambda_{0}x^{\top}Lx)$
    |  |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| (115) |  | $\displaystyle f(x&#124;\hat{A})$ | $\displaystyle=exp(-\lambda_{0}x^{\top}Lx)$
    |  |'
- en: '| (116) |  |  | $\displaystyle=exp(-\lambda_{0}x^{\top}(I-\hat{A})x),$ |  |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| (116) |  |  | $\displaystyle=exp(-\lambda_{0}x^{\top}(I-\hat{A})x),$ |  |'
- en: where $\lambda_{0}$ is a parameter. This likelihood imposed smoothness assumption
    on the learned graph structure. Some other works also model the adjacency matrix
    in a probabilistic manner. Bayesian GCNN (Zhang et al., [2019c](#bib.bib443))
    adopts a Bayesian framework and treats the observed graph as a realization from
    a family of random graphs. Then it estimates the posterior probablity of labels
    given the observed graph adjacency matrix and features with Monte Carlo approximation.
    VGCN (Elinas et al., [2020](#bib.bib84)) follows a similar formulation and estimates
    the graph posterior through stochastic variational inference.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\lambda_{0}$是一个参数。这种可能性赋予了学习到的图结构平滑性假设。其他一些研究也以概率的方式对邻接矩阵进行建模。贝叶斯GCNN（Zhang等，[2019c](#bib.bib443)）采用了贝叶斯框架，将观察到的图视为从随机图族中的一个实现。然后，它通过蒙特卡洛近似估计在给定观察到的图邻接矩阵和特征下标签的后验概率。VGCN（Elinas等，[2020](#bib.bib84)）遵循类似的公式，并通过随机变分推断估计图的后验。
- en: 'Graph Sparsification via Meta-Learning (Wan and Kokel, [2021](#bib.bib358))
    (GSML). GSML formulates GSL as a meta-leanring problem and uses bi-level optimization
    to find the optimal graph structure. The goal is to find a sparse graph structure
    which leads to high node classification accuracy at the same time given labeled
    and unlabeled nodes. To achieves this, GSML makes the inner optimization as training
    on the node classification task, and targets the outer optimization at the sparsity
    of the graph structure, which formulates the following bi-level optimization problem:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 图稀疏化通过元学习（Wan 和 Kokel，[2021](#bib.bib358)）（GSML）。GSML将GSL表述为一个元学习问题，并使用双层优化来寻找最优图结构。目标是在给定标记和未标记节点的情况下，找到一个稀疏的图结构，从而实现高节点分类准确率。为了实现这一点，GSML将内层优化设为节点分类任务的训练，并将外层优化目标定为图结构的稀疏性，形成以下双层优化问题：
- en: '| (117) |  | $\displaystyle\hat{G}^{*}$ | $\displaystyle=\mathop{\mathrm{min}}\limits_{\hat{G}\in\Phi(G)}L_{sps}(f_{\theta^{*}}(\hat{G}),Y_{U}),$
    |  |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| (117) |  | $\displaystyle\hat{G}^{*}$ | $\displaystyle=\mathop{\mathrm{min}}\limits_{\hat{G}\in\Phi(G)}L_{sps}(f_{\theta^{*}}(\hat{G}),Y_{U}),$
    |  |'
- en: '| (118) |  | $\displaystyle s.t.~{}$ | $\displaystyle~{}\theta^{*}=\mathop{\mathrm{argmin}}\limits_{\theta}L_{train}(f_{\theta}(\hat{G}),Y_{L}).$
    |  |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| (118) |  | $\displaystyle s.t.~{}$ | $\displaystyle~{}\theta^{*}=\mathop{\mathrm{argmin}}\limits_{\theta}L_{train}(f_{\theta}(\hat{G}),Y_{L}).$
    |  |'
- en: In this bi-level optimization problem, $\hat{G}\in\Phi(G)$ are the meta-parameters
    and optimized directly without parameterization. Similarly, LSD-GNN (Franceschi
    et al., [2019](#bib.bib98)) also uses bi-level optimization. It models graph structure
    with a probability distribution over graph and reformulates the bi-level program
    in terms of the continuous distribution parameters.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个双层优化问题中，$\hat{G}\in\Phi(G)$ 是元参数，直接进行优化而不进行参数化。同样，LSD-GNN（Franceschi 等， [2019](#bib.bib98)）也使用双层优化。它通过对图的概率分布来建模图结构，并以连续分布参数的形式重新表述双层程序。
- en: 9.6\. Summary
  id: totrans-594
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6\. 总结
- en: 'This section and we provide the summary as follows:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了以下总结：
- en: •
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Techniques. GSL aims to learn an optimized graph structure for better graph
    representations. It is also used for more robust graph representation against
    adversarial attacks. According to the way of edge modeling, we categorize GSL
    into three groups: metric-based methods, model-based methods, and direct methods.
    Regularization is also a commonly used principle to make the learned graph structure
    satisfy specific properties including sparsity, low-rank and smoothness.'
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术。GSL旨在学习一个优化的图结构，以获得更好的图表示。它还用于提高图表示对对抗攻击的鲁棒性。根据边建模的方法，我们将GSL分为三类：基于度量的方法、基于模型的方法和直接方法。正则化也是一种常用的原则，用于使学习到的图结构满足特定属性，包括稀疏性、低秩和光滑性。
- en: •
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Challenges and Limitations. Since there is no way to access the groundtruth
    or optimal graph structure as training data, the learning objective of GSL is
    either indirect (e.g., performance on downstream tasks) or manually designed (e.g.,
    sparsity and smoothness). Therefore, the optimization of GSL is difficult and
    the performance is not satisfying. In addition, many GSL methods are based on
    homophily assumption, i.e., similar nodes are more likely to connect with each
    other. However, many other types of connection exist in the real-world which impose
    great challenges for GSL.
  id: totrans-599
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战与局限性。由于无法获取真实的或最优的图结构作为训练数据，GSL的学习目标要么是间接的（例如，下游任务的性能），要么是手动设计的（例如，稀疏性和光滑性）。因此，GSL的优化是困难的，性能也不尽如人意。此外，许多GSL方法基于同质性假设，即相似的节点更可能彼此连接。然而，现实世界中存在许多其他类型的连接，这对GSL提出了巨大挑战。
- en: •
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future Works. In the future we expect more efficient and generalizable GSL methods
    to be applied to large-scale and heterogeneous graphs. Most existing GSL methods
    focus on pair-wise node similarities and thus struggle to scale to large graphs.
    Besides, they often learn homogeneous graph structure, but in many scenarios graphs
    are heterogeneous.
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来工作。在未来，我们期望更多高效且具有广泛适应性的GSL方法能应用于大规模和异质图。现有的大多数GSL方法集中在节点对相似性上，因此难以扩展到大规模图。此外，它们通常学习同质图结构，但在许多情况下，图是异质的。
- en: 10\. Social Analysis By Ziyue Qiao
  id: totrans-602
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10\. 社会分析 作者：赵子悦
- en: In the real world, there usually exist complex relations and interactions between
    people and multiple entities. Taking people, concrete things, and abstract concepts
    in society as nodes and taking the diverse, changeable, and large-scale connections
    between data as links, we can form massive and complex social information as social
    networks (Tabassum et al., [2018](#bib.bib340); Camacho et al., [2020](#bib.bib34)).
    Compared with traditional data structures such as texts and forms, modeling social
    data as graphs have many benefits. Especially with the arrival of the ”big data”
    era, more and more heterogeneous information are interconnected and integrated,
    and it is difficult and uneconomical to model this information with a traditional
    data structure. The graph is an effective implementation for information integration,
    as it can naturally incorporate different types of objects and their interactions
    from heterogeneous data sources (Shi et al., [2016](#bib.bib318); Moscato and
    Sperlì, [2021](#bib.bib266)). A summarization of social analysis applications
    is provided in Table [8](#S10.T8 "Table 8 ‣ 10\. Social Analysis By Ziyue Qiao
    ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，人们与多个实体之间通常存在复杂的关系和互动。将人、具体事物和社会中的抽象概念视为节点，将数据之间多样、可变和大规模的连接视为边，我们可以形成庞大而复杂的社会信息作为社交网络（Tabassum
    等，[2018](#bib.bib340); Camacho 等，[2020](#bib.bib34)）。与传统的数据结构如文本和表单相比，将社会数据建模为图具有许多优点。特别是随着“数据大时代”的到来，越来越多的异构信息被互联和整合，使用传统数据结构对这些信息建模变得困难且不经济。图是信息整合的有效实现，因为它可以自然地融入来自异构数据源的不同类型的对象及其互动（Shi
    等，[2016](#bib.bib318); Moscato 和 Sperlì，[2021](#bib.bib266)）。社交分析应用的总结见表[8](#S10.T8
    "Table 8 ‣ 10\. Social Analysis By Ziyue Qiao ‣ A Comprehensive Survey on Deep
    Graph Representation Learning")。
- en: Table 8\. A summarization of social analysis applications
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8\. 社会分析应用的总结
- en: '| Social networks | Node type | Edge type | Applications | References |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| 社交网络 | 节点类型 | 边缘类型 | 应用 | 参考文献 |'
- en: '| Academic Social Network | Author, Publication, Venue, Organization, Keyword
    | Authorship, Co-Author, Advisor-advisee, Citing, Cited, Co-Citing, Publishing
    | Classification/ Clustering | Paper/author classification (Dong et al., [2017](#bib.bib75);
    Wang et al., [2019b](#bib.bib369); Zhang et al., [2019d](#bib.bib433); Qiao et al.,
    [2020a](#bib.bib284)), name disambiguation (Zhang et al., [2018e](#bib.bib445);
    Qiao et al., [2019](#bib.bib282); Chen et al., [2020h](#bib.bib43)) |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| 学术社交网络 | 作者、出版物、场所、组织、关键词 | 作者身份、合著者、导师-学生、引用、被引、共同引用、出版 | 分类/聚类 | 论文/作者分类（Dong
    等，[2017](#bib.bib75); Wang 等，[2019b](#bib.bib369); Zhang 等，[2019d](#bib.bib433);
    Qiao 等，[2020a](#bib.bib284)），姓名消歧（Zhang 等，[2018e](#bib.bib445); Qiao 等，[2019](#bib.bib282);
    Chen 等，[2020h](#bib.bib43)） |'
- en: '| Relationship prediction | Co-authorship  (Chuan et al., [2018](#bib.bib60);
    Cho and Yu, [2018](#bib.bib57)), citation relationship  (Yu et al., [2012](#bib.bib426);
    Jiang et al., [2018](#bib.bib162); Wang et al., [2020e](#bib.bib363)), advisor-advisee
    relationship (Liu et al., [2019](#bib.bib229); Zhao et al., [2018](#bib.bib457))
    |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| 关系预测 | 合著关系（Chuan 等，[2018](#bib.bib60); Cho 和 Yu，[2018](#bib.bib57)），引用关系（Yu
    等，[2012](#bib.bib426); Jiang 等，[2018](#bib.bib162); Wang 等，[2020e](#bib.bib363)），导师-学生关系（Liu
    等，[2019](#bib.bib229); Zhao 等，[2018](#bib.bib457)） |'
- en: '| Recommen- dation | Collaborator recommendation (Liu et al., [2018c](#bib.bib237);
    Kong et al., [2017](#bib.bib188), [2016](#bib.bib189)), paper recommendation (Bai
    et al., [2019](#bib.bib15); Sugiyama and Kan, [2010](#bib.bib335)), venue recommendation (Yu
    et al., [2018](#bib.bib425); Margaris et al., [2019](#bib.bib258)) |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| 推荐 | 合作伙伴推荐（Liu 等，[2018c](#bib.bib237); Kong 等，[2017](#bib.bib188), [2016](#bib.bib189)），论文推荐（Bai
    等，[2019](#bib.bib15); Sugiyama 和 Kan，[2010](#bib.bib335)），场所推荐（Yu 等，[2018](#bib.bib425);
    Margaris 等，[2019](#bib.bib258)） |'
- en: '| Academic Media Network | User, Blog, Article, Image, Video | Following, Like,
    Unlike, Clicked, Viewed, Commented, Reposted | Anomaly detection | Malicious attacks (Sun
    et al., [2020c](#bib.bib339); Liu et al., [2018b](#bib.bib235)), emergency detection (Bian
    et al., [2020](#bib.bib22)), and robot discovery (Feng et al., [2021](#bib.bib92))
    |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| 学术媒体网络 | 用户、博客、文章、图片、视频 | 关注、喜欢、不喜欢、点击、浏览、评论、转发 | 异常检测 | 恶意攻击（Sun 等，[2020c](#bib.bib339);
    Liu 等，[2018b](#bib.bib235)），紧急检测（Bian 等，[2020](#bib.bib22)），以及机器人发现（Feng 等，[2021](#bib.bib92)）
    |'
- en: '| Sentiment analysis | Customer feedback (Rosa et al., [2018](#bib.bib299);
    Zhang et al., [2014](#bib.bib442)), public events (Unankard et al., [2014](#bib.bib350);
    Manguri et al., [2020](#bib.bib255)) |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | 客户反馈 （Rosa 等，[2018](#bib.bib299)；Zhang 等，[2014](#bib.bib442)），公共事件 （Unankard
    等，[2014](#bib.bib350)；Manguri 等，[2020](#bib.bib255)） |'
- en: '| Influence analysis | Important node finding (Domingos and Richardson, [2001](#bib.bib74);
    Richardson and Domingos, [2002](#bib.bib296)), information diffusion modeling (Panagopoulos
    et al., [2020](#bib.bib270); Keikha et al., [2020](#bib.bib179); Zhang et al.,
    [2022a](#bib.bib432); Kumar et al., [2022](#bib.bib197)) |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| 影响分析 | 重要节点发现 （Domingos 和 Richardson，[2001](#bib.bib74)；Richardson 和 Domingos，[2002](#bib.bib296)），信息扩散建模 （Panagopoulos
    等，[2020](#bib.bib270)；Keikha 等，[2020](#bib.bib179)；Zhang 等，[2022a](#bib.bib432)；Kumar
    等，[2022](#bib.bib197)） |'
- en: '| Location-based Social Network | Restaurant, Cinema, Mall, Parking | Friendship,
    Check-in | POI recommendation | Spatial/temporal influence (Si et al., [2019](#bib.bib323);
    Wang et al., [2022e](#bib.bib377); Zhao et al., [2020](#bib.bib454)), social relationship (Xu
    et al., [2021a](#bib.bib401)), textual information (Xu et al., [2021b](#bib.bib403))
    |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| 基于位置的社交网络 | 餐馆、电影院、商场、停车场 | 友谊、签到 | POI 推荐 | 空间/时间影响 （Si 等，[2019](#bib.bib323)；Wang
    等，[2022e](#bib.bib377)；Zhao 等，[2020](#bib.bib454)），社交关系 （Xu 等，[2021a](#bib.bib401)），文本信息 （Xu
    等，[2021b](#bib.bib403)） |'
- en: '| Urban computing | Traffic congestion prediction (Jiang and Luo, [2022](#bib.bib161);
    Xiong et al., [2018](#bib.bib399)), urban mobility analysis (Yildirimoglu and
    Kim, [2018](#bib.bib415); Cao et al., [2021b](#bib.bib36)), event detection (Yu
    et al., [2021a](#bib.bib424); Sofuoglu and Aviyente, [2022](#bib.bib327)) |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| 城市计算 | 交通拥堵预测 （Jiang 和 Luo，[2022](#bib.bib161)；Xiong 等，[2018](#bib.bib399)），城市流动性分析 （Yildirimoglu
    和 Kim，[2018](#bib.bib415)；Cao 等，[2021b](#bib.bib36)），事件检测 （Yu 等，[2021a](#bib.bib424)；Sofuoglu
    和 Aviyente，[2022](#bib.bib327)） |'
- en: 10.1\. Concepts of Social Networks
  id: totrans-614
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1\. 社交网络的概念
- en: 'A social network is usually composed of multiple types of nodes, link relationships,
    and node attributes, which inherently include rich structural and semantic information.
    Specifically, a social network can be homogeneous or heterogeneous and directed
    or undirected in different scenarios. Without loss of generality, we define the
    social network as a directed heterogeneous graph $G=\{V,E,\mathcal{T},\mathcal{R}\}$,
    where $V=\{n_{i}\}^{|V|}_{i=1}$ is the node set, $E=\{e_{i}\}^{|E|}_{i=1}$ is
    the edge set, $\mathcal{T}=\{t_{i}\}^{|\mathcal{T}|}_{i=1}$ is the node type set,
    and $\mathcal{R}=\{r_{i}\}^{|\mathcal{R}|}_{i=1}$ is the edge type set. Each node
    $n_{i}\in V$ is associated with a node type mapping: $\phi_{n}(n_{i})=t_{j}:V\longrightarrow\mathcal{T}$
    and each edge $e_{i}\in E$ is associated with a node type mapping: $\phi_{e}(e_{i})=r_{j}:E\longrightarrow\mathcal{R}$.
    A node $n_{i}$ may have a feature set, where the feature space is specific for
    the node type. An edge $e_{i}$ is also represented by node pairs $(n_{j},n_{k})$
    at both ends and can be directed or undirected with relation-type-specific attributes.
    If $|\mathcal{T}|=1$ and $|\mathcal{R}|=1$, the social network is a homogeneous
    graph; otherwise, it is a heterogeneous graph.'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 社交网络通常由多种类型的节点、链接关系和节点属性组成，固有地包含丰富的结构和语义信息。具体来说，社交网络可以在不同场景下是同质的或异质的、定向的或非定向的。一般而言，我们将社交网络定义为一个定向异质图
    $G=\{V,E,\mathcal{T},\mathcal{R}\}$，其中 $V=\{n_{i}\}^{|V|}_{i=1}$ 是节点集，$E=\{e_{i}\}^{|E|}_{i=1}$
    是边集，$\mathcal{T}=\{t_{i}\}^{|\mathcal{T}|}_{i=1}$ 是节点类型集，$\mathcal{R}=\{r_{i}\}^{|\mathcal{R}|}_{i=1}$
    是边类型集。每个节点 $n_{i}\in V$ 关联一个节点类型映射：$\phi_{n}(n_{i})=t_{j}:V\longrightarrow\mathcal{T}$，每条边
    $e_{i}\in E$ 关联一个边类型映射：$\phi_{e}(e_{i})=r_{j}:E\longrightarrow\mathcal{R}$。节点
    $n_{i}$ 可能有一个特征集，其中特征空间对于节点类型是特定的。边 $e_{i}$ 也由两端的节点对 $(n_{j},n_{k})$ 表示，可以是定向的或非定向的，并具有特定于关系类型的属性。如果
    $|\mathcal{T}|=1$ 且 $|\mathcal{R}|=1$，则社交网络是一个同质图；否则，它是一个异质图。
- en: Almost any data produced by social activities can be modeled as social networks,
    for example, the academic social network produced by academic activities such
    as collaboration and citation, the online social network produced by user following
    and followed on social media, and the location-based social network produced by
    human activities on different locations. Based on constructing social networks,
    researchers have new paths to data mining, knowledge discovery, and multiple application
    tasks on social data. Exploring social networks also brings new challenges. One
    of the critical challenges is how to succinctly represent the network from the
    massive and heterogeneous raw graph data, that is, how to learn continuous and
    low-dimensional social network representations, so as to researchers can efficiently
    perform advanced machine learning techniques on the social network data for multiple
    application tasks, such as analysis, clustering, prediction, and knowledge discovery.
    Thus, graph representation learning on the social network becomes the foundational
    technique for social analysis.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎任何由社会活动产生的数据都可以建模为社交网络，例如，由学术活动（如合作和引用）产生的学术社交网络，由社交媒体上用户关注和被关注产生的在线社交网络，以及由人类在不同地点的活动产生的位置基础社交网络。基于构建社交网络，研究人员可以在数据挖掘、知识发现和多个应用任务上开辟新的路径。探索社交网络也带来了新的挑战。一个关键挑战是如何从海量且异构的原始图数据中简洁地表示网络，即如何学习连续且低维的社交网络表示，以便研究人员能够高效地对社交网络数据进行高级机器学习技术应用于分析、聚类、预测和知识发现等多个应用任务。因此，社交网络的图表示学习成为社交分析的基础技术。
- en: 10.2\. Academic Social Network
  id: totrans-617
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2\. 学术社交网络
- en: Academic collaboration is a common and important behavior in academic society,
    and also a major way for scientists and researchers to innovate and break through
    scientific research, which leads to a complex social relationship between scholars.
    Also, the academic data generated by academic collaboration also contains a large
    number of interconnected entities with complex relationships (Kong et al., [2019](#bib.bib190)).
    Normally, in an academic social network, the node type set consists of Author,
    Publication, Venue, Organization, Keyword, etc., and the relation set consists
    of Authorship, Co-Author, Advisor-advisee, Citing, Cited, Co-Citing, Publishing,
    Co-Word, etc. Note that in most social networks, each relation type always connects
    two fixed node types with a fixed direction. For example, the relation Authorship
    points from the node type Author to Publication, and the Co-Author is an undirected
    relation between two nodes with type Author. Based on the node and relation types
    in an academic social network, one can divide it into multiple categories. For
    example, the co-author network with nodes of Author and relations of Co-Author,
    the citation network with nodes of Publication and relation of Citing, and the
    academic heterogeneous information graph with multiple academic node and relation
    types. Many research institutes and academic search engines, such as Aminer¹¹1https://www.aminer.cn/,
    DBLP²²2https://dblp.uni-trier.de/, Microsoft Academic Graph (MAG)³³3https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/,
    have provided open academic social network datasets for research purposes.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 学术合作在学术界是一种常见且重要的行为，同时也是科学家和研究人员进行创新和突破科学研究的主要方式，这导致了学者之间复杂的社会关系。此外，学术合作产生的学术数据也包含大量相互关联的实体以及复杂的关系（Kong
    等， [2019](#bib.bib190)）。通常，在学术社交网络中，节点类型集合包括作者（Author）、出版物（Publication）、会议（Venue）、组织（Organization）、关键词（Keyword）等，而关系集合包括作者身份（Authorship）、共同作者（Co-Author）、导师-学员（Advisor-advisee）、引用（Citing）、被引用（Cited）、共同引用（Co-Citing）、出版（Publishing）、共同词（Co-Word）等。请注意，在大多数社交网络中，每种关系类型通常将两个固定的节点类型通过固定的方向连接。例如，作者身份（Authorship）关系从节点类型作者（Author）指向出版物（Publication），而共同作者（Co-Author）是一种在两个作者类型节点之间的无向关系。基于学术社交网络中的节点和关系类型，可以将其划分为多个类别。例如，具有作者节点和共同作者关系的共同作者网络，具有出版物节点和引用关系的引用网络，以及具有多种学术节点和关系类型的学术异构信息图。许多研究机构和学术搜索引擎，如
    Aminer¹¹1https://www.aminer.cn/，DBLP²²2https://dblp.uni-trier.de/，微软学术图谱（MAG）³³3https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/，已经提供了用于研究目的的开放学术社交网络数据集。
- en: There are multiple applications of graph representation learning on the academic
    social network. Roughly, they can be divided into three categories–academic entity
    classification/clustering, academic relationship prediction, and academic resource
    recommendation.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 图表示学习在学术社交网络上有多种应用。大致来说，可以将其分为三类——学术实体分类/聚类、学术关系预测和学术资源推荐。
- en: •
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Academic entities usually belong to different classes of research areas. Research
    of academic entity classification and clustering aims to categorize these entities,
    such as papers and authors, into different classes (Dong et al., [2017](#bib.bib75);
    Wang et al., [2019b](#bib.bib369); Zhang et al., [2019d](#bib.bib433); Qiao et al.,
    [2020a](#bib.bib284)). In literature, academic networks such as Cora, CiteSeer,
    and Pubmed (Sen et al., [2008](#bib.bib313)) have become the most widely used
    benchmark datasets for examining the performance of graph representation learning
    models on paper classification. Also, the author name disambiguation problem (Zhang
    et al., [2018e](#bib.bib445); Qiao et al., [2019](#bib.bib282); Chen et al., [2020h](#bib.bib43))
    is also essentially a node clustering task on co-author networks and is usually
    solved by the graph representation learning technique.
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学术实体通常属于不同的研究领域类别。学术实体分类和聚类的研究旨在将这些实体，如论文和作者，分类到不同的类别中（董等，[2017](#bib.bib75)；王等，[2019b](#bib.bib369)；张等，[2019d](#bib.bib433)；乔等，[2020a](#bib.bib284)）。在文献中，学术网络如Cora、CiteSeer和Pubmed（沈等，[2008](#bib.bib313)）已成为最广泛使用的基准数据集，用于检验图表示学习模型在论文分类上的性能。此外，作者名称消歧义问题（张等，[2018e](#bib.bib445)；乔等，[2019](#bib.bib282)；陈等，[2020h](#bib.bib43)）本质上也是共作者网络上的节点聚类任务，通常通过图表示学习技术解决。
- en: •
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Academic relationship prediction represents the link prediction task on various
    academic relations. Typical applications are co-authorship prediction (Chuan et al.,
    [2018](#bib.bib60); Cho and Yu, [2018](#bib.bib57)) and citation relationship
    prediction (Yu et al., [2012](#bib.bib426); Jiang et al., [2018](#bib.bib162);
    Wang et al., [2020e](#bib.bib363)). Existing methods learn representations of
    authors and papers and use the similarity between two nodes to predict the link
    probability. Besides, some work (Liu et al., [2019](#bib.bib229); Zhao et al.,
    [2018](#bib.bib457)) studies the problem of advisor-advisee relationship prediction
    in the collaboration network.
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学术关系预测代表了各种学术关系的链接预测任务。典型的应用有共同作者预测（川等，[2018](#bib.bib60)；赵和于，[2018](#bib.bib57)）和引用关系预测（于等，[2012](#bib.bib426)；姜等，[2018](#bib.bib162)；王等，[2020e](#bib.bib363)）。现有方法学习作者和论文的表示，并使用两个节点之间的相似性来预测链接概率。此外，一些研究（刘等，[2019](#bib.bib229)；赵等，[2018](#bib.bib457)）研究了合作网络中的导师-学生关系预测问题。
- en: •
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Various academic recommendation systems have been introduced to retrieve academic
    resources for users from large amounts of academic data in recent years. For example,
    collaborator recommendation (Liu et al., [2018c](#bib.bib237); Kong et al., [2017](#bib.bib188),
    [2016](#bib.bib189)) benefit researchers by finding suitable collaborators under
    particular topics; paper recommendation (Bai et al., [2019](#bib.bib15); Sugiyama
    and Kan, [2010](#bib.bib335)) help researchers find relevant papers on given topics;
    venue recommendation (Yu et al., [2018](#bib.bib425); Margaris et al., [2019](#bib.bib258))
    help researchers choose appropriate venues when they submit papers.
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 近年来，已经引入了各种学术推荐系统，以从大量学术数据中为用户检索学术资源。例如，合作者推荐（刘等，[2018c](#bib.bib237)；孔等，[2017](#bib.bib188)，[2016](#bib.bib189)）通过在特定主题下寻找合适的合作者来惠及研究人员；论文推荐（白等，[2019](#bib.bib15)；杉山和关，[2010](#bib.bib335)）帮助研究人员找到相关的论文；场地推荐（于等，[2018](#bib.bib425)；玛尔加里斯等，[2019](#bib.bib258)）帮助研究人员在提交论文时选择合适的场地。
- en: 10.3\. Social Media Network
  id: totrans-626
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3\. 社交媒体网络
- en: With the development of the Internet in decades, various online social media
    have emerged in large numbers and greatly changed people’s traditional social
    models. People can establish friendships with others beyond the distance limit
    and share interests, hobbies, status, activities, and other information among
    friends. These abundant interactions on the Internet form large-scale complex
    social media networks, also named online social networks. Usually, in an academic
    social network, the node type set consists of User, Blog, Article, Image, Video,
    etc., and the relation type set consists of Following, Like, Unlike, Clicked,
    Viewed, Commented, Reposted, etc. The main property of a social media network
    is that it usually contains multi-mode information on the nodes, such as video,
    image, and text. Also, the relations are more complex and multiplex, including
    the explicit relations such as Like and Unlike and the implicit relations such
    as Clicked. The social media network can be categorized into multiple types based
    on their media categories. For example, the friendship network, the movie review
    network, and the music interacting network are extracted from different social
    media platforms. In a broad sense, the user-item networks in online shopping system
    can also be viewed as social media networks as they also exist on the Internet
    and contains rich interactions by people. There are many widely used data sources
    for social media network analysis, such as Twitter, Facebook, Weibo, YouTube,
    and Instagram.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 随着互联网的发展，数十年来，出现了大量各种在线社交媒体，极大地改变了人们的传统社交模式。人们可以与他人建立跨越距离限制的友谊，并在朋友之间分享兴趣、爱好、状态、活动及其他信息。这些丰富的互联网互动形成了大规模复杂的社交媒体网络，也称为在线社交网络。通常，在学术社交网络中，节点类型集包括用户、博客、文章、图片、视频等，关系类型集包括关注、喜欢、不喜欢、点击、查看、评论、转发等。社交媒体网络的主要特点是它通常包含多模态的节点信息，如视频、图片和文本。此外，关系更加复杂和多样，包括显性关系如喜欢和不喜欢，以及隐性关系如点击。社交媒体网络可以根据其媒体类别分为多种类型。例如，友谊网络、电影评论网络和音乐互动网络是从不同的社交媒体平台提取的。从广义上讲，在线购物系统中的用户-商品网络也可以视为社交媒体网络，因为它们也存在于互联网上并包含丰富的人际互动。社交媒体网络分析有许多广泛使用的数据源，如Twitter、Facebook、微博、YouTube和Instagram。
- en: The mainstream application research on social media networks via graph representation
    learning techniques mainly includes anomaly detection, sentiment analysis, and
    influence analysis.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体网络中主流的应用研究通过图表示学习技术主要包括异常检测、情感分析和影响力分析。
- en: •
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Anomaly detection aims to find strange or unusual patterns in social networks,
    which has a wide range of application scenarios, such as malicious attacks (Sun
    et al., [2020c](#bib.bib339); Liu et al., [2018b](#bib.bib235)), emergency detection (Bian
    et al., [2020](#bib.bib22)), and robot discovery (Feng et al., [2021](#bib.bib92))
    in social networks. Unsupervised anomaly detection usually learns a reconstructed
    graph to detect those nodes with higher reconstructed error as the anomaly nodes (Ahmed
    et al., [2021](#bib.bib4); Zhao et al., [2022b](#bib.bib453)); Supervised methods
    model the problem as a binary classification task on the learned graph representations (Meng
    et al., [2021](#bib.bib260); Zheng et al., [2019](#bib.bib459)).
  id: totrans-630
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 异常检测旨在发现社交网络中的异常或不寻常模式，具有广泛的应用场景，如恶意攻击（Sun et al., [2020c](#bib.bib339); Liu
    et al., [2018b](#bib.bib235)）、紧急检测（Bian et al., [2020](#bib.bib22)）和机器人发现（Feng
    et al., [2021](#bib.bib92)）等。无监督异常检测通常通过学习重构图来检测那些重构误差较高的节点作为异常节点（Ahmed et al.,
    [2021](#bib.bib4); Zhao et al., [2022b](#bib.bib453)）；监督方法则将问题建模为在学习到的图表示上的二分类任务（Meng
    et al., [2021](#bib.bib260); Zheng et al., [2019](#bib.bib459)）。
- en: •
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sentiment analysis, also named as opinion mining, is to mining the sentiment,
    opinions, and attitudes, which can help enterprises understand customer feedback
    on products (Rosa et al., [2018](#bib.bib299); Zhang et al., [2014](#bib.bib442))
    and help the government analyze the public emotion and make rapid response to
    public events (Unankard et al., [2014](#bib.bib350); Manguri et al., [2020](#bib.bib255)).
    The graph representation learning model is usually combined with RNN-based (Zhang
    et al., [2019b](#bib.bib431); Chen et al., [2020e](#bib.bib49)) or Transformer-based (AlBadani
    et al., [2022](#bib.bib6); Tang et al., [2020a](#bib.bib343)) text encoders to
    incorporate both the user relationship and textual semantic information.
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 情感分析，也称为意见挖掘，是挖掘情感、观点和态度的过程，这可以帮助企业了解客户对产品的反馈（Rosa et al., [2018](#bib.bib299);
    Zhang et al., [2014](#bib.bib442)），并帮助政府分析公众情绪，并对公共事件做出快速反应（Unankard et al., [2014](#bib.bib350);
    Manguri et al., [2020](#bib.bib255)）。图表示学习模型通常与基于RNN的（Zhang et al., [2019b](#bib.bib431);
    Chen et al., [2020e](#bib.bib49)）或基于Transformer的（AlBadani et al., [2022](#bib.bib6);
    Tang et al., [2020a](#bib.bib343)）文本编码器相结合，以结合用户关系和文本语义信息。
- en: •
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Influence analysis usually aims to find several nodes in a social network to
    initially spread information such as advertisements, so as to maximize the final
    spread of information (Domingos and Richardson, [2001](#bib.bib74); Richardson
    and Domingos, [2002](#bib.bib296)). The core challenge is to model the information
    diffusion process in the social network. Deep learning methods (Panagopoulos et al.,
    [2020](#bib.bib270); Keikha et al., [2020](#bib.bib179); Zhang et al., [2022a](#bib.bib432);
    Kumar et al., [2022](#bib.bib197)) usually leverage graph neural networks to learn
    node embeddings and diffusion probabilities between nodes.
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 影响力分析通常旨在找出社交网络中的几个节点，以最初传播信息，如广告，从而最大化信息的最终传播（Domingos and Richardson, [2001](#bib.bib74);
    Richardson and Domingos, [2002](#bib.bib296)）。核心挑战是建模社交网络中的信息扩散过程。深度学习方法（Panagopoulos
    et al., [2020](#bib.bib270); Keikha et al., [2020](#bib.bib179); Zhang et al.,
    [2022a](#bib.bib432); Kumar et al., [2022](#bib.bib197)）通常利用图神经网络来学习节点嵌入和节点之间的扩散概率。
- en: 10.4\. Location-based Social Network
  id: totrans-635
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4\. 基于位置的社交网络
- en: Locations are the fundamental information of human social activities. With the
    wide availability of mobile Internet and GPS positioning technology, people can
    easily acquire their precise locations and socialize with their friends by sharing
    their historical check-ins on the Internet. This opens up a new avenue of research
    on location-based social network analysis, which gathered significant attention
    from the user, business, and government perspectives. Usually, in a location-based
    social network, the node type set consists of User, and Location, also named Point
    of Interest(POI) in the recommendation scenario containing multiple categories
    such as Restaurant, Cinema, Mall, Parking, etc. The relation type set consists
    of Friendship, Check-in. Also, those node and relation types that exist in traditional
    social media networks can be included in a location-based social network. The
    difference with other social networks, the main location-based social networks
    are spatial and temporal, making the graph representation learning more challenging.
    For example, in a typical social network constructed for the POI recommendation,
    the user nodes are connected with each other by their friendship. The location
    nodes are connected by user nodes with the relations feature of timestamps. The
    location nodes also have a spatial relationship with each other and own have complex
    features, including categories, tags, check-in counts, number of users check-in,
    etc. There are many location-based social network datasets, such as Foursquare⁴⁴4https://foursquare.com/,
    Gowalla⁵⁵5https://www.gowalla.com/, and Waze⁶⁶6https://www.waze.com/live-map/.
    Also, many social media such as Twitter, Instagram, and Facebook can provide location
    information.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 地点是人类社交活动的基本信息。随着移动互联网和GPS定位技术的广泛应用，人们可以轻松获取精确位置，并通过在互联网上分享历史签到来与朋友社交。这为基于位置的社交网络分析开辟了新的研究方向，吸引了用户、商业和政府的广泛关注。通常，在基于位置的社交网络中，节点类型集包括用户和位置，也称为兴趣点（POI），在推荐场景中包含多个类别，如餐馆、电影院、购物中心、停车场等。关系类型集包括友谊、签到。此外，传统社交媒体网络中的那些节点和关系类型也可以包含在基于位置的社交网络中。与其他社交网络的不同之处在于，主要的基于位置的社交网络具有空间和时间特性，使得图表示学习更加具有挑战性。例如，在为POI推荐构建的典型社交网络中，用户节点通过友谊相互连接。位置节点通过具有时间戳特征的用户节点连接。位置节点之间也具有空间关系，并拥有复杂的特征，包括类别、标签、签到次数、用户签到数量等。有许多基于位置的社交网络数据集，如[Foursquare](https://foursquare.com/)、[Gowalla](https://www.gowalla.com/)和[Waze](https://www.waze.com/live-map/)。此外，许多社交媒体，如Twitter、Instagram和Facebook，也可以提供位置信息。
- en: 'The research of graph representation learning on location-based social networks
    can be divided into two categories: POI recommendation for business benefits and
    urban computing for public management.'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 图表示学习在基于位置的社交网络中的研究可以分为两类：为了商业利益的POI推荐和为了公共管理的城市计算。
- en: •
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: POI recommendation is one of the research hotspots in the field of location-based
    social networks and recommendation systems in recent years (Islam et al., [2022](#bib.bib158);
    Werneck et al., [2020](#bib.bib380); Ju et al., [2022c](#bib.bib174)), which aim
    to utilize historical check-ins of users and auxiliary information to recommend
    potential favor places for users from a large of location points. Existing researches
    mainly integrate four essential characteristics, including spatial influence,
    temporal influence (Si et al., [2019](#bib.bib323); Wang et al., [2022e](#bib.bib377);
    Zhao et al., [2020](#bib.bib454)), social relationship (Xu et al., [2021a](#bib.bib401)),
    and textual information (Xu et al., [2021b](#bib.bib403)).
  id: totrans-639
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: POI推荐是近年来基于位置的社交网络和推荐系统领域的研究热点之一（Islam等，[2022](#bib.bib158)；Werneck等，[2020](#bib.bib380)；Ju等，[2022c](#bib.bib174)），旨在利用用户的历史签到和辅助信息，从大量的地理点中推荐潜在的喜好场所给用户。现有研究主要整合了四个基本特征，包括空间影响、时间影响（Si等，[2019](#bib.bib323)；Wang等，[2022e](#bib.bib377)；Zhao等，[2020](#bib.bib454)）、社交关系（Xu等，[2021a](#bib.bib401)）和文本信息（Xu等，[2021b](#bib.bib403)）。
- en: •
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Urban computing is defined as a process of analysis of the large-scale connected
    urban data created from city activities of vehicles, human beings, and sensors (Paulos
    and Goodman, [2004](#bib.bib274); Paulos et al., [2004](#bib.bib273); Silva et al.,
    [2019](#bib.bib324)). Besides the local-based social network, the urban data also
    includes physical sensors, city infrastructure, traffic roads, and so on. Urban
    computing aims to improve the quality of public management and life quality of
    people living in city environments. Typical applications including traffic congestion
    prediction (Jiang and Luo, [2022](#bib.bib161); Xiong et al., [2018](#bib.bib399)),
    urban mobility analysis (Yildirimoglu and Kim, [2018](#bib.bib415); Cao et al.,
    [2021b](#bib.bib36)), event detection (Yu et al., [2021a](#bib.bib424); Sofuoglu
    and Aviyente, [2022](#bib.bib327)).
  id: totrans-641
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 城市计算被定义为对由车辆、人类和传感器的城市活动生成的大规模连接城市数据的分析过程（Paulos 和 Goodman，[2004](#bib.bib274)；Paulos
    等，[2004](#bib.bib273)；Silva 等，[2019](#bib.bib324)）。除了基于本地的社交网络，城市数据还包括物理传感器、城市基础设施、交通道路等。城市计算的目标是改善公共管理的质量和生活在城市环境中的人的生活质量。典型应用包括交通拥堵预测（Jiang
    和 Luo，[2022](#bib.bib161)；Xiong 等，[2018](#bib.bib399)），城市流动性分析（Yildirimoglu 和
    Kim，[2018](#bib.bib415)；Cao 等，[2021b](#bib.bib36)），事件检测（Yu 等，[2021a](#bib.bib424)；Sofuoglu
    和 Aviyente，[2022](#bib.bib327)）。
- en: 10.5\. Summary
  id: totrans-642
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5\. 总结
- en: 'This section introduces social analysis by graph representation learning and
    we provide the summary as follows:'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了通过图表示学习进行的社会分析，我们的总结如下：
- en: •
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Techniques. Social networks, generated by human social activities, such as communication,
    collaboration, and social interactions, typically involve massive and heterogeneous
    data, with different types of attributes and properties that can change over time.
    Thus, social network analysis is a field of study that explores the techniques
    to understand and analyze the complex attributes, heterogeneous structures, and
    dynamic information of social networks. Social network analysis typically learns
    low-dimensional graph representations that capture the essential properties and
    patterns of the social network data, which can be used for various downstream
    tasks, such as classification, clustering, link prediction, and recommendation.
  id: totrans-645
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术。由人类社交活动（如沟通、协作和社会互动）生成的社交网络通常涉及大量异质数据，这些数据具有不同类型的属性和特性，并且可能随着时间变化。因此，社交网络分析是一个研究领域，探索理解和分析社交网络复杂属性、异质结构和动态信息的技术。社交网络分析通常学习低维图表示，以捕捉社交网络数据的基本属性和模式，这些表示可以用于各种下游任务，如分类、聚类、链接预测和推荐。
- en: •
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Chanllenges and Limitations. Despite the structural heterogeneity in social
    networks (nodes and relations have different types), with the technological advances
    in social media, the node attributes have become more heterogeneous now, containing
    text, video, and images. Also, the large-scale problem is a pending issue in social
    network analysis. The data in the social network has increased exponentially in
    past decades, containing a high density of topological links and a large amount
    of node attribute information, which brings new challenges to the efficiency and
    effectiveness of traditional network representation learning on the social network.
    Lastly, social networks are often dynamic, which means the network information
    usually changes over time, and this temporal information plays a significant role
    in many downstream tasks, such as recommendations. This brings new challenges
    to representation learning on social networks in incorporating temporal information.
  id: totrans-647
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战和局限性。尽管社交网络中存在结构异质性（节点和关系有不同类型），但随着社交媒体技术的进步，节点属性现在变得更加异质，包含文本、视频和图像。此外，大规模问题仍然是社交网络分析中的悬而未决的问题。过去几十年中，社交网络中的数据呈指数级增长，包含了高密度的拓扑链接和大量的节点属性信息，这给传统的网络表示学习带来了新的挑战。最后，社交网络通常是动态的，这意味着网络信息通常随时间变化，这种时间信息在许多下游任务中（如推荐）扮演着重要角色。这给社交网络上的表示学习带来了在融入时间信息方面的新挑战。
- en: •
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future Works. Recently, multi-modal big pre-training models that can fuse information
    from different modalities have gained increasing attention (Qiao et al., [2022](#bib.bib283);
    Radford et al., [2021](#bib.bib290)). These models can obtain valuable information
    from a large amount of unlabeled data and transfer it to various downstream analysis
    tasks. Moreover, Transformer-based models have demonstrated better effectiveness
    than RNNs in capturing temporal information. In the future, there is potential
    for introducing multi-modal big pre-training models in social network analysis.
    Also, it is important to make the models more efficient for network information
    extraction and use lightweight techniques like knowledge distillation to further
    enhance the applicability of the models. These advancements can lead to more effective
    social network analysis and enable the development of more sophisticated applications
    in various domains.
  id: totrans-649
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来工作。最近，能够融合不同模态信息的多模态大预训练模型受到越来越多的关注（Qiao等，[2022](#bib.bib283)；Radford等，[2021](#bib.bib290)）。这些模型可以从大量未标记数据中获取有价值的信息，并将其转移到各种下游分析任务中。此外，基于Transformer的模型在捕捉时间信息方面比RNN表现出更好的效果。未来，有可能在社交网络分析中引入多模态大预训练模型。此外，还需要提高模型在网络信息提取方面的效率，并使用轻量化技术，如知识蒸馏，进一步增强模型的适用性。这些进展可以导致更有效的社交网络分析，并促进各种领域中更复杂应用的发展。
- en: 11\. Molecular Property Prediction By Zequn Liu
  id: totrans-650
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11\. 分子性质预测，作者：刘泽群
- en: Molecular Property Prediction is an essential task in computational drug discovery
    and cheminformatics. Traditional quantitative structure property/activity relationship
    (QSPR/QSAR) approaches are based on either SMILES or fingerprints (Mikolov et al.,
    [2013](#bib.bib263); Xu et al., [2017](#bib.bib407); Zhang et al., [2018d](#bib.bib440)),
    largely overlooking the topological features of the molecules. To address this
    problem, graph representation learning has been widely applied to molecular property
    prediction. A molecule can be represented as a graph where nodes stand for atoms
    and edges stand for atom-bonds (ABs). Graph-level molecular representations are
    learned via message passing mechanism to incorporate the topological information.
    The representations are then utilized for the molecular property prediction tasks.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 分子性质预测是计算药物发现和化学信息学中的一项重要任务。传统的定量结构性质/活性关系（QSPR/QSAR）方法基于SMILES或指纹（Mikolov等，[2013](#bib.bib263)；Xu等，[2017](#bib.bib407)；Zhang等，[2018d](#bib.bib440)），在很大程度上忽视了分子的拓扑特征。为了解决这个问题，图表示学习已经广泛应用于分子性质预测。一个分子可以表示为一个图，其中节点代表原子，边代表原子间的键（ABs）。通过消息传递机制学习图级分子表示，以融入拓扑信息。然后，这些表示被用于分子性质预测任务。
- en: Specifically, a molecule is denoted as a topological graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$,
    where $\mathcal{V}=\{v_{i}|i=1,\ldots,|\mathcal{G}|\}$ is the set of nodes representing
    atoms. A feature vector $\mathbf{x}_{i}$ is associated with each node $v_{i}$
    indicating its type such as Carbon, Nitrogen. $\mathcal{E}=\{e_{ij}|i,j=1,\ldots,|\mathcal{G}|\}$
    is the set of edges connecting two nodes (atoms) $v_{i}$ and $v_{j}$ representing
    atom bonds. Graph representation learning methods are used to obtain the molecular
    representation $\mathbf{h}_{\mathcal{G}}$. Then downstream classification or regression
    layers $f(\cdot)$ are applied to predict the probability of target property of
    each molecule $y=f(\mathbf{h}_{\mathcal{G}})$.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，一个分子表示为拓扑图$\mathcal{G}=(\mathcal{V},\mathcal{E})$，其中$\mathcal{V}=\{v_{i}|i=1,\ldots,|\mathcal{G}|\}$是表示原子的节点集合。每个节点$v_{i}$都关联一个特征向量$\mathbf{x}_{i}$，指示其类型，如碳、氮。$\mathcal{E}=\{e_{ij}|i,j=1,\ldots,|\mathcal{G}|\}$是连接两个节点（原子）$v_{i}$和$v_{j}$的边集合，表示原子键。图表示学习方法用于获得分子表示$\mathbf{h}_{\mathcal{G}}$。然后，将下游分类或回归层$f(\cdot)$应用于预测每个分子目标性质的概率$y=f(\mathbf{h}_{\mathcal{G}})$。
- en: In Section [11.1](#S11.SS1 "11.1\. Molecular Property Categorization ‣ 11\.
    Molecular Property Prediction By Zequn Liu ‣ A Comprehensive Survey on Deep Graph
    Representation Learning"), we introduce 4 types of molecular properties graph
    representation learning can be treated and their corresponding datasets. Section
    [11.2](#S11.SS2 "11.2\. Molecular Graph Representation Learning Backbones ‣ 11\.
    Molecular Property Prediction By Zequn Liu ‣ A Comprehensive Survey on Deep Graph
    Representation Learning") reviews the graph representation learning backbones
    applied to molecular property prediction. Strategies for training the molecular
    property prediction methods are listed in Section [11.3](#S11.SS3 "11.3\. Training
    strategies ‣ 11\. Molecular Property Prediction By Zequn Liu ‣ A Comprehensive
    Survey on Deep Graph Representation Learning").
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [11.1](#S11.SS1 "11.1\. Molecular Property Categorization ‣ 11\. Molecular
    Property Prediction By Zequn Liu ‣ A Comprehensive Survey on Deep Graph Representation
    Learning") 节中，我们介绍了 4 种分子性质图表示学习的方法及其对应的数据集。第 [11.2](#S11.SS2 "11.2\. Molecular
    Graph Representation Learning Backbones ‣ 11\. Molecular Property Prediction By
    Zequn Liu ‣ A Comprehensive Survey on Deep Graph Representation Learning") 节回顾了应用于分子性质预测的图表示学习框架。分子性质预测方法的训练策略列在第
    [11.3](#S11.SS3 "11.3\. Training strategies ‣ 11\. Molecular Property Prediction
    By Zequn Liu ‣ A Comprehensive Survey on Deep Graph Representation Learning")
    节中。
- en: 11.1\. Molecular Property Categorization
  id: totrans-654
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1\. 分子性质分类
- en: 'Plenty of molecular properties can be predicted by graph-based methods. We
    follow (Wieder et al., [2020](#bib.bib381)) to categorize them into 4 types: quantum
    chemistry, physicochemical properties, biophysics, and biological effect.'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分子性质可以通过基于图的方法进行预测。我们遵循 (Wieder et al., [2020](#bib.bib381)) 将它们分类为 4 种类型：量子化学、物理化学性质、生物物理性质和生物效应。
- en: Quantum chemistry is a branch of physical chemistry focused on the application
    of quantum mechanics to chemical systems, including conformation, partial charges
    and energies. QM7, QM8, QM9 (Wu et al., [2018](#bib.bib390)), COD (Ruddigkeit
    et al., [2012](#bib.bib300)) and CSD (Groom et al., [2016](#bib.bib123)) are datasets
    for quantum chemistry prediction.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 量子化学是物理化学的一个分支，专注于量子力学在化学系统中的应用，包括构象、部分电荷和能量。QM7、QM8、QM9 (Wu et al., [2018](#bib.bib390))、COD
    (Ruddigkeit et al., [2012](#bib.bib300)) 和 CSD (Groom et al., [2016](#bib.bib123))
    是用于量子化学预测的数据集。
- en: Physicochemical properties are the intrinsic physical and chemical characteristics
    of a substance, such as bioavailability, octanol solubility, aqueous solubility
    and hydrophobicity. ESOL, Lipophilicity and Freesolv (Wu et al., [2018](#bib.bib390))
    are datasets for physicochemical properties prediction.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 物理化学性质是物质的内在物理和化学特征，例如生物利用度、辛醇溶解度、 aqueous 溶解度和疏水性。ESOL、Lipophilicity 和 Freesolv
    (Wu et al., [2018](#bib.bib390)) 是物理化学性质预测的数据集。
- en: Biophysics properties are about the physical underpinnings of biomolecular phenomena,
    such as affinity, efficacy and activity. PDBbind (Wang et al., [2005](#bib.bib365)),
    MUV, and HIV (Wu et al., [2018](#bib.bib390)) are biophysics property prediction
    datasets.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 生物物理性质涉及生物分子现象的物理基础，例如亲和力、效能和活性。PDBbind (Wang et al., [2005](#bib.bib365))、MUV
    和 HIV (Wu et al., [2018](#bib.bib390)) 是生物物理性质预测数据集。
- en: Biological effect properties are generally defined as the response of an organism,
    a population, or a community to changes in its environment, such as side effects,
    toxicity and ADMET. Tox21, toxcast (Wu et al., [2018](#bib.bib390)) and PTC (Toivonen
    et al., [2003](#bib.bib349)) are biological effect prediction datasets.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 生物效应性质通常定义为生物体、群体或群落对环境变化的反应，例如副作用、毒性和 ADMET。Tox21、toxcast (Wu et al., [2018](#bib.bib390))
    和 PTC (Toivonen et al., [2003](#bib.bib349)) 是生物效应预测的数据集。
- en: Moleculenet (Wu et al., [2018](#bib.bib390)) is a widely-used benchmark dataset
    for molecule property prediction. It contains over 700,000 compounds tested on
    different properties. For each dataset, they provide a metric and a splitting
    pattern. Among the datasets, QM7, OM7b, QM8, QM9, ESOL, FreeSolv, Lipophilicity
    and PDBbind are regression tasks, using MAE or RMSE as evaluation metrics. Other
    tasks such as tox21 and toxcast are classification tasks, using AUC as evaluation
    metric.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: Moleculenet (Wu et al., [2018](#bib.bib390)) 是一个广泛使用的分子性质预测基准数据集。它包含超过 700,000
    种化合物，测试了不同的性质。每个数据集都提供了一个指标和一个划分模式。在这些数据集中，QM7、OM7b、QM8、QM9、ESOL、FreeSolv、Lipophilicity
    和 PDBbind 是回归任务，使用 MAE 或 RMSE 作为评价指标。其他任务如 tox21 和 toxcast 是分类任务，使用 AUC 作为评价指标。
- en: 11.2\. Molecular Graph Representation Learning Backbones
  id: totrans-661
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2\. 分子图表示学习框架
- en: Since node attributes and edge attributes are crucial to molecular representation,
    most works use GNN instead of traditional graph representation learning methods
    as backbones, since many GNN methods consider edge information. Existing GNNs
    designed for the general domain can be applied to molecular graph. Table [9](#S11.T9
    "Table 9 ‣ 11.2\. Molecular Graph Representation Learning Backbones ‣ 11\. Molecular
    Property Prediction By Zequn Liu ‣ A Comprehensive Survey on Deep Graph Representation
    Learning") summarizes the GNNs used for molecular property prediction and the
    types of properties they can be applied to predict.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 由于节点属性和边属性对分子表示至关重要，大多数研究使用 GNN 而非传统的图表示学习方法作为骨干，因为许多 GNN 方法考虑了边信息。现有的通用领域 GNN
    可以应用于分子图。表 [9](#S11.T9 "Table 9 ‣ 11.2\. Molecular Graph Representation Learning
    Backbones ‣ 11\. Molecular Property Prediction By Zequn Liu ‣ A Comprehensive
    Survey on Deep Graph Representation Learning") 总结了用于分子属性预测的 GNN 及其适用的属性类型。
- en: Table 9\. Summary of GNNs in molecular property prediction.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9\. GNN 在分子属性预测中的总结。
- en: '| Type | Spatial/Specrtal | Method | Application |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 空间/谱域 | 方法 | 应用 |'
- en: '| Reccurent GNN | - | R-GNN | Biological effect (Scarselli et al., [2008](#bib.bib307))
    |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| 循环 GNN | - | R-GNN | 生物效应 (Scarselli et al., [2008](#bib.bib307)) |'
- en: '| Reccurent GNN | - | GGNN | Quantum chemistry (Mansimov et al., [2019](#bib.bib256)),
    Biological effect (Withnall et al., [2020](#bib.bib383); Altae-Tran et al., [2017](#bib.bib8);
    Feinberg et al., [2018](#bib.bib91)) |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| 循环 GNN | - | GGNN | 量子化学 (Mansimov et al., [2019](#bib.bib256))，生物效应 (Withnall
    et al., [2020](#bib.bib383); Altae-Tran et al., [2017](#bib.bib8); Feinberg et
    al., [2018](#bib.bib91)) |'
- en: '| Reccurent GNN | - | IterRefLSTM | Biophysics (Altae-Tran et al., [2017](#bib.bib8)),
    Biological effect (Altae-Tran et al., [2017](#bib.bib8)) |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| 循环 GNN | - | IterRefLSTM | 生物物理学 (Altae-Tran et al., [2017](#bib.bib8))，生物效应
    (Altae-Tran et al., [2017](#bib.bib8)) |'
- en: '| Convolutional GNN | Spatial/Specrtal | GCN | Quantum chemistry (Yang et al.,
    [2019](#bib.bib410); Liao et al., [2018](#bib.bib225); Withnall et al., [2020](#bib.bib383)),
    pysicochemical properties (Ryu et al., [2018](#bib.bib302); Duvenaud et al., [2015](#bib.bib81);
    Coley et al., [2017](#bib.bib63)), Biophysics (Duvenaud et al., [2015](#bib.bib81);
    Yang et al., [2019](#bib.bib410); Bouritsas et al., [2022](#bib.bib26)) Biological
    effect (Li et al., [2017a](#bib.bib209); Wu et al., [2018](#bib.bib390)) |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| 卷积 GNN | 空间/谱域 | GCN | 量子化学 (Yang et al., [2019](#bib.bib410); Liao et al.,
    [2018](#bib.bib225); Withnall et al., [2020](#bib.bib383))，物理化学性质 (Ryu et al.,
    [2018](#bib.bib302); Duvenaud et al., [2015](#bib.bib81); Coley et al., [2017](#bib.bib63))，生物物理学
    (Duvenaud et al., [2015](#bib.bib81); Yang et al., [2019](#bib.bib410); Bouritsas
    et al., [2022](#bib.bib26))，生物效应 (Li et al., [2017a](#bib.bib209); Wu et al.,
    [2018](#bib.bib390)) |'
- en: '| Convolutional GNN | Specrtal | LanczosNet | Quantum chemistry (Liao et al.,
    [2018](#bib.bib225)) |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| 卷积 GNN | 谱域 | LanczosNet | 量子化学 (Liao et al., [2018](#bib.bib225)) |'
- en: '| Convolutional GNN | Specrtal | ChebNet | Physicochemical properties, Biophysics,
    Biological effect (Li et al., [2018b](#bib.bib215)) |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| 卷积 GNN | 谱域 | ChebNet | 物理化学性质，生物物理学，生物效应 (Li et al., [2018b](#bib.bib215))
    |'
- en: '| Convolutional GNN | Spatial | GraphSAGE | Physicochemical properties (Hu
    et al., [2019](#bib.bib146)), Biophysics (Errica et al., [2019](#bib.bib87); Liang
    et al., [2020](#bib.bib224); Chen et al., [2020a](#bib.bib56)), Biological effect
    (Hu et al., [2019](#bib.bib146); Ma et al., [2019](#bib.bib251)) |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| 卷积 GNN | 空间 | GraphSAGE | 物理化学性质 (Hu et al., [2019](#bib.bib146))，生物物理学 (Errica
    et al., [2019](#bib.bib87); Liang et al., [2020](#bib.bib224); Chen et al., [2020a](#bib.bib56))，生物效应
    (Hu et al., [2019](#bib.bib146); Ma et al., [2019](#bib.bib251)) |'
- en: '| Convolutional GNN | Spatial | GAT | Physicochemical properties (Hu et al.,
    [2019](#bib.bib146)), Biophysics (Chen et al., [2020a](#bib.bib56); Bouritsas
    et al., [2022](#bib.bib26)), Biological effect (Hu et al., [2019](#bib.bib146))
    |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| 卷积 GNN | 空间 | GAT | 物理化学性质 (Hu et al., [2019](#bib.bib146))，生物物理学 (Chen et
    al., [2020a](#bib.bib56); Bouritsas et al., [2022](#bib.bib26))，生物效应 (Hu et al.,
    [2019](#bib.bib146)) |'
- en: '| Convolutional GNN | Spatial | DGCNN | Biophysics (Chen et al., [2019](#bib.bib51)),
    Biological effect (Zhang et al., [2018a](#bib.bib438)) |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| 卷积 GNN | 空间 | DGCNN | 生物物理学 (Chen et al., [2019](#bib.bib51))，生物效应 (Zhang
    et al., [2018a](#bib.bib438)) |'
- en: '| Convolutional GNN | Spatial | GIN | Physicochemical properties (Hu et al.,
    [2019](#bib.bib146); Bouritsas et al., [2022](#bib.bib26)), Biophysics (Hu et al.,
    [2019](#bib.bib146), [2020c](#bib.bib145)), Biological effect (Hu et al., [2019](#bib.bib146))
    |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| 卷积 GNN | 空间 | GIN | 物理化学性质 (Hu et al., [2019](#bib.bib146); Bouritsas et
    al., [2022](#bib.bib26))，生物物理学 (Hu et al., [2019](#bib.bib146), [2020c](#bib.bib145))，生物效应
    (Hu et al., [2019](#bib.bib146)) |'
- en: '| Convolutional GNN | Spatial | MPNN | Physicochemical (Ma et al., [2020a](#bib.bib248))
    |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| 卷积 GNN | 空间 | MPNN | 物理化学 (Ma et al., [2020a](#bib.bib248)) |'
- en: '| Transformer | - | MAT | Physicochemical, Biophysics (Łukasz Maziarka et al.,
    [2020](#bib.bib475)) |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | - | MAT | Physicochemical, Biophysics (Łukasz Maziarka等人，[2020](#bib.bib475))
    |'
- en: Furthermore, many works customize their GNN structure by considering the chemical
    domain knowledge.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多研究会考虑化学领域的知识，定制他们的GNN结构。
- en: •
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'First, the chemical bonds are taken into consideration carefully. For example,
    Ma et al. (Ma et al., [2020a](#bib.bib248)) use an additional edge GNN to model
    the chemical bonds separately. Specifically, given an edge $(v,w)$, they formulate
    an Edge-based GNN as:'
  id: totrans-679
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，化学键被认真考虑。 例如，马等人（Ma等，[2020a](#bib.bib248)）使用额外的边缘GNN来单独模拟化学键。 具体来说，给定边缘$（v，w）$，他们构建了一个基于边缘的GNN：
- en: '| (119) |  | $\displaystyle{\mathbf{m}}_{vw}^{(k)}$ | $\displaystyle=\text{AGG}_{\text{edge}}(\{{\mathbf{h}}_{vw}^{(k-1)},{\mathbf{h}}_{uv}^{(k-1)},{\mathbf{x}}_{u}&#124;u\in\mathcal{N}_{v}\setminus
    w\}),\quad{\mathbf{h}}_{vw}^{(k)}=\text{MLP}_{\text{edge}}(\{{\mathbf{m}}_{vw}^{(k-1)},{\mathbf{h}}_{vw}^{(0)}\}),$
    |  |'
  id: totrans-680
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|（119）| | $ \displaystyle{\mathbf{m}}_{vw}^{(k)} $ | $ \displaystyle=\text{AGG}_{\text{edge}}(\{{\mathbf{h}}_{vw}^{(k-1)},{\mathbf{h}}_{uv}^{(k-1)},{\mathbf{x}}_{u}&#124;u\in\mathcal{N}_{v}\setminus
    w\}),\quad{\mathbf{h}}_{vw}^{(k)}=\text{MLP}_{\text{edge}}(\{{\mathbf{m}}_{vw}^{(k-1)},{\mathbf{h}}_{vw}^{(0)}\}),
    $ | |'
- en: where ${\mathbf{h}}_{vw}^{(0)}=\sigma({\mathbf{W}}_{\text{ein}}{\mathbf{e}}_{vw})$
    is the input state of the Edge-based GNN, ${\mathbf{W}}_{\text{ein}}\in\mathbb{R}^{d_{\text{hid}}\times
    d_{e}}$ is the input weight matrix. PotentialNet (Feinberg et al., [2018](#bib.bib91))
    further uses different message passing operations for different edge types.
  id: totrans-681
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中$ {\mathbf{h}}_{vw}^{(0)}=\sigma({\mathbf{W}}_{\text{ein}}{\mathbf{e}}_{vw})
    $是基于边缘的GNN的输入状态，$ {\mathbf{W}}_{\text{ein}}\in\mathbb{R}^{d_{\text{hid}}\times
    d_{e}} $是输入权重矩阵。 PotentialNet（Feinberg等人，[2018](#bib.bib91)）进一步针对不同的边缘类型使用不同的消息传递操作。
- en: •
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Second, motifs in molecular graphs play an important role in molecular property
    prediction. GSN (Bouritsas et al., [2022](#bib.bib26)) leverages substructure
    encoding to construct a topologically-aware message passing method. Each node
    $v$ updates its state $\mathbf{h}^{t}_{v}$ by combining its previous state with
    the aggregated messages:'
  id: totrans-683
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其次，分子图中的基序在分子性质预测中扮演重要角色。 GSN（Bouritsas等人，[2022](#bib.bib26)）利用亚结构编码来构建具有拓扑感知的消息传递方法。
    每个节点$ v $通过将其先前状态与聚合的消息相结合来更新其状态$ \mathbf{h}^{t}_{v} $：
- en: '| (120) |  | $\displaystyle\mathbf{h}^{t+1}_{v}$ | $\displaystyle=\mathrm{UP}^{t+1}\big{(}\mathbf{h}^{t}_{v},\mathbf{m}^{t+1}_{v}\big{)},$
    |  |'
  id: totrans-684
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|（120）| | $ \displaystyle\mathbf{h}^{t+1}_{v} $ | $ \displaystyle=\mathrm{UP}^{t+1}\big{(}\mathbf{h}^{t}_{v},\mathbf{m}^{t+1}_{v}\big{)},
    $ | |'
- en: '| (124) |  | $\displaystyle\mathbf{m}^{t+1}_{v}$ | <math alttext="\displaystyle=\left\{\begin{array}[]{l}M^{t+1}\bigg{(}\Lbag(\mathbf{h}^{t}_{v},\mathbf{h}^{t}_{u},\mathbf{x}^{V}_{v},\mathbf{x}^{V}_{u},\mathbf{e}_{u,v})\Rbag_{u\in\mathcal{N}(v)}\bigg{)}\
    (\textbf{GSN-v})\\ \quad\quad\quad\quad\quad\quad\text{ or }\\'
  id: totrans-685
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|（124）| | $ \displaystyle\mathbf{m}^{t+1}_{v} $ | <math alttext="\displaystyle=\left\{\begin{array}[]{l}M^{t+1}\bigg{(}\Lbag(\mathbf{h}^{t}_{v},\mathbf{h}^{t}_{u},\mathbf{x}^{V}_{v},\mathbf{x}^{V}_{u},\mathbf{e}_{u,v})\Rbag_{u\in\mathcal{N}(v)}\bigg{)}\
    (\textbf{GSN-v})\\ \quad\quad\quad\quad\quad\quad\text{ or }\\'
- en: M^{t+1}\bigg{(}\Lbag(\mathbf{h}^{t}_{v},\mathbf{h}^{t}_{u},\mathbf{x}^{E}_{u,v},\mathbf{e}_{u,v})\Rbag_{u\in\mathcal{N}(v)}\bigg{)}\
    (\textbf{GSN-e})\end{array}\right.," display="inline"><semantics ><mrow ><mrow
    ><mo  >=</mo><mrow ><mo >{</mo><mtable rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><mrow
    ><msup  ><mi >M</mi><mrow ><mi  >t</mi><mo >+</mo><mn >1</mn></mrow></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo maxsize="210%" minsize="210%" >(</mo><mrow  ><merror
    ><mtext >\Lbag</mtext></merror><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    stretchy="false" >(</mo><msubsup ><mi >𝐡</mi><mi >v</mi><mi >t</mi></msubsup><mo
    >,</mo><msubsup ><mi  >𝐡</mi><mi >u</mi><mi >t</mi></msubsup><mo >,</mo><msubsup
    ><mi >𝐱</mi><mi >v</mi><mi >V</mi></msubsup><mo >,</mo><msubsup ><mi  >𝐱</mi><mi
    >u</mi><mi >V</mi></msubsup><mo >,</mo><msub ><mi >𝐞</mi><mrow  ><mi >u</mi><mo
    >,</mo><mi  >v</mi></mrow></msub><mo stretchy="false"  >)</mo></mrow><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><merror class="ltx_ERROR undefined undefined"  ><mtext
    >\Rbag</mtext></merror><mrow ><mi >u</mi><mo  >∈</mo><mrow ><mi >𝒩</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mi >v</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></msub></mrow><mo maxsize="210%" minsize="210%"  >)</mo></mrow><mo
    lspace="0.500em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mtext
    >GSN-v</mtext><mo stretchy="false" >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mtext  > or </mtext></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow ><msup  ><mi
    >M</mi><mrow ><mi  >t</mi><mo >+</mo><mn >1</mn></mrow></msup><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo maxsize="210%" minsize="210%" >(</mo><mrow  ><merror
    ><mtext >\Lbag</mtext></merror><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    stretchy="false" >(</mo><msubsup ><mi >𝐡</mi><mi >v</mi><mi >t</mi></msubsup><mo
    >,</mo><msubsup ><mi  >𝐡</mi><mi >u</mi><mi >t</mi></msubsup><mo >,</mo><msubsup
    ><mi >𝐱</mi><mrow  ><mi >u</mi><mo >,</mo><mi  >v</mi></mrow><mi >E</mi></msubsup><mo
    >,</mo><msub ><mi  >𝐞</mi><mrow ><mi >u</mi><mo  >,</mo><mi >v</mi></mrow></msub><mo
    stretchy="false"  >)</mo></mrow><mo lspace="0em" rspace="0em"  >​</mo><msub ><merror
    class="ltx_ERROR undefined undefined"  ><mtext >\Rbag</mtext></merror><mrow ><mi
    >u</mi><mo  >∈</mo><mrow ><mi >𝒩</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><mi >v</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow></msub></mrow><mo
    maxsize="210%" minsize="210%"  >)</mo></mrow><mo lspace="0.500em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><mtext >GSN-e</mtext><mo stretchy="false" >)</mo></mrow></mrow></mtd></mtr></mtable></mrow></mrow><mo
    >,</mo></mrow><annotation-xml encoding="MathML-Content" ><apply ><csymbol cd="latexml"
    >absent</csymbol><apply ><csymbol cd="latexml" >cases</csymbol><matrix ><matrixrow  ><apply
    ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑀</ci><apply ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply ><ci ><merror ><mtext  >\Lbag</mtext></merror></ci><vector
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝐡</ci><ci >𝑡</ci></apply><ci >𝑣</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐡</ci><ci >𝑡</ci></apply><ci >𝑢</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐱</ci><ci >𝑉</ci></apply><ci
    >𝑣</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐱</ci><ci >𝑉</ci></apply><ci >𝑢</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐞</ci><list  ><ci >𝑢</ci><ci
    >𝑣</ci></list></apply></vector><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    ><merror ><mtext  >\Rbag</mtext></merror></ci><apply ><ci >𝑢</ci><apply  ><ci
    >𝒩</ci><ci >𝑣</ci></apply></apply></apply></apply><ci ><mtext >GSN-v</mtext></ci></apply></matrixrow><matrixrow
    ><ci  ><mtext > or </mtext></ci></matrixrow><matrixrow ><apply ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci >𝑀</ci><apply ><ci  >𝑡</ci><cn type="integer"  >1</cn></apply></apply><apply
    ><ci ><merror ><mtext  >\Lbag</mtext></merror></ci><vector ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐡</ci><ci >𝑡</ci></apply><ci >𝑣</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐡</ci><ci >𝑡</ci></apply><ci
    >𝑢</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐱</ci><ci >𝐸</ci></apply><list ><ci  >𝑢</ci><ci
    >𝑣</ci></list></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐞</ci><list ><ci >𝑢</ci><ci  >𝑣</ci></list></apply></vector><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci ><merror class="ltx_ERROR undefined undefined"  ><mtext
    >\Rbag</mtext></merror></ci><apply ><ci >𝑢</ci><apply  ><ci >𝒩</ci><ci >𝑣</ci></apply></apply></apply></apply><ci
    ><mtext >GSN-e</mtext></ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle=\left\{\begin{array}[]{l}M^{t+1}\bigg{(}\Lbag(\mathbf{h}^{t}_{v},\mathbf{h}^{t}_{u},\mathbf{x}^{V}_{v},\mathbf{x}^{V}_{u},\mathbf{e}_{u,v})\Rbag_{u\in\mathcal{N}(v)}\bigg{)}\
    (\textbf{GSN-v})\\ \quad\quad\quad\quad\quad\quad\text{ or }\\ M^{t+1}\bigg{(}\Lbag(\mathbf{h}^{t}_{v},\mathbf{h}^{t}_{u},\mathbf{x}^{E}_{u,v},\mathbf{e}_{u,v})\Rbag_{u\in\mathcal{N}(v)}\bigg{)}\
    (\textbf{GSN-e})\end{array}\right.,</annotation></semantics></math> |  |
  id: totrans-686
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: M^{t+1}\bigg{(}\Lbag(\mathbf{h}^{t}_{v},\mathbf{h}^{t}_{u},\mathbf{x}^{V}_{v},\mathbf{x}^{V}_{u},\mathbf{e}_{u,v})\Rbag_{u\in\mathcal{N}(v)}\bigg{)}\
    (\textbf{GSN-v})\\ \quad\quad\quad\quad\quad\quad\text{ 或 }\\ M^{t+1}\bigg{(}\Lbag(\mathbf{h}^{t}_{v},\mathbf{h}^{t}_{u},\mathbf{x}^{E}_{u,v},\mathbf{e}_{u,v})\Rbag_{u\in\mathcal{N}(v)}\bigg{)}\
    (\textbf{GSN-e})\end{array}\right.,
- en: where $\mathbf{x}^{V}_{v},\mathbf{x}^{V}_{u},\mathbf{x}^{E}_{u,v},\mathbf{e}_{u,v}$
    contains the substructure information associated with nodes and edges, $\Lbag\Rbag$
    denotes a multiset. Yu et al. (Yu and Gao, [2022](#bib.bib427)) constructs a heterogeneous
    graph using motifs and molecules. Motifs and molecules are both treated as nodes
    and the edges model the relationship between motifs and graphs, for example, if
    a graph contains a motif, there will be an edge between them. MGSSL (Zhang et al.,
    [2021a](#bib.bib448)) leverages a retrosynthesis-based algorithm BRICS and additional
    rules to find the motifs and combines motif layers with atom layers. It is a hierarchical
    framework jointly modeling atom-level information and motif-level information.
  id: totrans-687
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中$\mathbf{x}^{V}_{v},\mathbf{x}^{V}_{u},\mathbf{x}^{E}_{u,v},\mathbf{e}_{u,v}$包含了与节点和边相关的子结构信息，$\Lbag\Rbag$表示一个多重集。Yu等（Yu
    and Gao, [2022](#bib.bib427)）使用基元和分子构建了一个异质图。基元和分子都被视为节点，边则建模了基元与图之间的关系，例如，如果一个图包含一个基元，它们之间会有一条边。MGSSL（Zhang
    et al., [2021a](#bib.bib448)）利用基于逆合成的算法BRICS和附加规则来找到基元，并将基元层与原子层结合。它是一个层次化的框架，联合建模原子级别的信息和基元级别的信息。
- en: •
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Third, different feature modalities have been used to improve the molecular
    graph embedding. Lin et al. (Lin et al., [2022b](#bib.bib228)) combine SMILES
    modality and graph modality with contrastive learning. Zhu et al. (Zhu et al.,
    [2022b](#bib.bib469)) encode 2D molecular graph and 3D molecular conformation
    with a unified Transformer. It uses a unified model to learn 3D conformation generation
    given 2D graph and 2D graph generation given 3D conformation.
  id: totrans-689
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三，不同的特征模态已被用来改善分子图嵌入。Lin等（Lin et al., [2022b](#bib.bib228)）结合了SMILES模态和图模态，并使用对比学习。Zhu等（Zhu
    et al., [2022b](#bib.bib469)）使用统一的Transformer对2D分子图和3D分子构象进行编码。它使用统一的模型来学习在给定2D图的情况下生成3D构象，以及在给定3D构象的情况下生成2D图。
- en: •
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Finally, knowledge graph and literature can provide additional knowledge for
    molecular property prediction. Fang et al. (Fang et al., [2022](#bib.bib89)) introduce
    a chemical element knowledge graph to summarize microscopic associations between
    elements and augment the molecular graph based on the knowledge graph, and a knowledge-aware
    message passing network is used to encode the augmented graph. MuMo (Su et al.,
    [2022](#bib.bib334)) introduces biomedical literature to guide the molecular property
    prediction. It pretrains a GNN and a language model on paired data of molecules
    and literature mentions via contrastive learning:'
  id: totrans-691
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，知识图谱和文献可以为分子属性预测提供额外的知识。Fang等（Fang et al., [2022](#bib.bib89)）引入了一个化学元素知识图谱，以总结元素之间的微观关联，并基于知识图谱增强分子图，并使用一个知识感知消息传递网络来编码增强后的图。MuMo（Su
    et al., [2022](#bib.bib334)）引入生物医学文献来指导分子属性预测。它通过对比学习对分子和文献提及的配对数据进行GNN和语言模型的预训练：
- en: '| (125) |  | $\ell_{i}^{(\mathbf{z}_{i}^{G},\mathbf{z}_{i}^{T})}=-\log\frac{\exp{(\emph{sim}(\mathbf{z}_{i}^{G},\mathbf{z}_{i}^{T})/\tau)}}{\sum_{j=1}^{N}\exp{(\emph{sim}(\mathbf{z}_{i}^{G},\mathbf{z}_{j}^{T})/\tau})},$
    |  |'
  id: totrans-692
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (125) |  | $\ell_{i}^{(\mathbf{z}_{i}^{G},\mathbf{z}_{i}^{T})}=-\log\frac{\exp{(\emph{sim}(\mathbf{z}_{i}^{G},\mathbf{z}_{i}^{T})/\tau)}}{\sum_{j=1}^{N}\exp{(\emph{sim}(\mathbf{z}_{i}^{G},\mathbf{z}_{j}^{T})/\tau})},$
    |  |'
- en: where $\mathbf{z}_{i}^{G},\mathbf{z}_{i}^{T}$ are the representation of molecule
    and its corresponding literature.
  id: totrans-693
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中$\mathbf{z}_{i}^{G},\mathbf{z}_{i}^{T}$分别是分子及其对应文献的表示。
- en: 11.3\. Training strategies
  id: totrans-694
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3\. 训练策略
- en: 'Despite the encouraging performance achieved by GNNs, traditional supervised
    training scheme of GNNs faces a severe limitation: The scarcity of available molecules
    with desired properties. Although there are a large number of molecular graphs
    in public databases such as PubChem, labeled molecules are hard to acquire due
    to the high cost of wet-lab experiments and quantum chemistry calculations. Directly
    training GNNs on such limited molecules in a supervised way is prone to over-fitting
    and lack of generalization. To address this issue, few-shot learning and self-supervised
    learning are widely used in molecular property prediction.'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GNNs取得了令人鼓舞的性能，但GNNs的传统监督训练方案面临严重的限制：可用的具有期望属性的分子稀缺。尽管像PubChem这样的公共数据库中有大量的分子图，但由于湿实验和量子化学计算的高成本，标记分子很难获得。在如此有限的分子上以监督方式直接训练GNNs容易导致过拟合和缺乏泛化能力。为了解决这一问题，少样本学习和自监督学习在分子属性预测中被广泛使用。
- en: Few-shot learning. Few-shot learning aims at generalizing to a task with a small
    labeled data set. The prediction of each property is treated as a single task.
    Metric-based and optimization-based few-shot learning have been adopted for molecular
    property prediction. Metric-based few-shot learning is similar to nearest neighbors
    and kernel density estimation, which learns a metric or distance function over
    objects. IterRefLSTM (Altae-Tran et al., [2017](#bib.bib8)) leverages matching
    network (Vinyals et al., [2016](#bib.bib355)) as the few-shot learning framework,
    calculating the similarity between support samples and query samples. Optimization-based
    few-shot learning optimizes a meta-learner for parameter initialization which
    can be fast adapted to new tasks. Meta-MGNN (Guo et al., [2021](#bib.bib127))
    adopts MAML (Finn et al., [2017](#bib.bib95)) to train a parameter initialization
    to adapt to different tasks and use self-attentive task weights for each task.
    PAR (Wang et al., [2021](#bib.bib371)) also uses MAML framework and learns an
    adaptive relation graph among molecules for each task.
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习。少样本学习旨在对小型标注数据集中的任务进行泛化。每个属性的预测被视为一个单独的任务。基于度量和基于优化的少样本学习已被采用用于分子属性预测。基于度量的少样本学习类似于最近邻和核密度估计，它学习对象上的度量或距离函数。IterRefLSTM
    (Altae-Tran et al., [2017](#bib.bib8)) 利用匹配网络 (Vinyals et al., [2016](#bib.bib355))
    作为少样本学习框架，计算支持样本和查询样本之间的相似性。基于优化的少样本学习优化一个元学习器进行参数初始化，该学习器可以快速适应新任务。Meta-MGNN
    (Guo et al., [2021](#bib.bib127)) 采用 MAML (Finn et al., [2017](#bib.bib95)) 来训练一个参数初始化以适应不同任务，并为每个任务使用自注意力任务权重。PAR
    (Wang et al., [2021](#bib.bib371)) 也使用 MAML 框架，并学习每个任务中分子之间的自适应关系图。
- en: Self-supervised learning. Self-supervised learning can pre-train a GNN model
    with plenty of unlabeled molecular graphs and transfer it to specific molecular
    property prediction tasks. Self-supervised learning contains generative methods
    and predictive methods. Predictive methods design prediction tasks to capture
    the intrinsic data features. Pre-GNN (Hu et al., [2019](#bib.bib146)) exploits
    both node-level and graph-level prediction tasks including context prediction,
    attribute masking, graph-level property prediction and structural similarity prediction.
    MGSSL (Zhang et al., [2021a](#bib.bib448)) provides a motif-based generative pre-training
    framework making topology prediction and motif generation iteratively. Contrastive
    methods learn graph representations by pulling views from the same graph close
    and pushing views from different graphs apart. Different views of the same graph
    are constructed by graph augmentation or leveraging the 1D SMILES and 3D structure.
    MolCLR (Wang et al., [2022d](#bib.bib374)) augments molecular graphs by atom masking,
    bond deletion and subgraph removal and maximizes the agreement between the original
    molecular graph and augmented graphs. Fang et al. (Fang et al., [2022](#bib.bib89))
    uses a chemical knowledge graph to guide the graph augmentation. SMICLR (Pinheiro
    et al., [2022](#bib.bib280)) uses contrastive learning across SMILES and 2D molecular
    graphs. GeomGCL (Li et al., [2022e](#bib.bib216)) leverages graph contrastive
    learning to capture the geometry of the molecule across 2D and 3D views. Self-supervised
    learning can also be combined with few-shot learning to fully leverage the hierarchical
    information in the training set (Ju et al., [2023b](#bib.bib171)).
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习。自监督学习可以用大量未标记的分子图来预训练 GNN 模型，并将其转移到特定的分子属性预测任务中。自监督学习包括生成方法和预测方法。预测方法设计预测任务以捕捉数据的内在特征。Pre-GNN
    (Hu et al., [2019](#bib.bib146)) 利用节点级和图级预测任务，包括上下文预测、属性掩蔽、图级属性预测和结构相似性预测。MGSSL
    (Zhang et al., [2021a](#bib.bib448)) 提供了一个基于结构的生成预训练框架，通过迭代进行拓扑预测和结构生成。对比方法通过将同一图的视图拉近，将不同图的视图推开来学习图表示。同一图的不同视图通过图增强或利用
    1D SMILES 和 3D 结构来构造。MolCLR (Wang et al., [2022d](#bib.bib374)) 通过原子掩蔽、键删除和子图移除来增强分子图，并最大化原始分子图与增强图之间的一致性。Fang
    et al. (Fang et al., [2022](#bib.bib89)) 使用化学知识图来指导图增强。SMICLR (Pinheiro et al.,
    [2022](#bib.bib280)) 使用跨 SMILES 和 2D 分子图的对比学习。GeomGCL (Li et al., [2022e](#bib.bib216))
    利用图对比学习捕捉分子在 2D 和 3D 视图中的几何形状。自监督学习还可以与少样本学习相结合，以充分利用训练集中的层次信息 (Ju et al., [2023b](#bib.bib171))。
- en: 11.4\. Summary
  id: totrans-698
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4\. 总结
- en: 'This section introduces graph representation learning in molecular property
    prediction and we provide the summary as follows:'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了分子属性预测中的图表示学习，并提供了如下总结：
- en: •
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Techniques. For molecular property prediction, a molecule is represented as
    a graph whose nodes are atoms and edges are atom-bonds (ABs). GNNs such as GCN,
    GAT, and GraphSAGE are adopted to learn the graph-level representation. The representations
    are then fed into a classification or regression head for the molecular property
    prediction tasks. Many works guide the model structure design with medical domain
    knowledge including chemical bond features, motif features, different modalities
    of molecular representation, chemical knowledge graph and literature. Due to the
    scarcity of available molecules with desired properties, few-shot learning and
    contrastive learning are used to train molecular property prediction model, so
    that the model can leverage the information in large unlabeled dataset and can
    be adapted to new tasks with a few examples.
  id: totrans-701
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术。在分子属性预测中，分子被表示为一个图，其节点是原子，边是原子键（ABs）。采用 GNNs，如 GCN、GAT 和 GraphSAGE 来学习图级表示。然后将这些表示输入分类或回归头进行分子属性预测任务。许多工作利用医学领域知识来指导模型结构设计，包括化学键特征、基序特征、不同的分子表示模态、化学知识图谱和文献。由于可用具有期望属性的分子稀缺，采用少量学习和对比学习来训练分子属性预测模型，使得模型能够利用大规模未标记数据集中的信息，并能在少量示例下适应新任务。
- en: •
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Challenges and Limitations. Despite the great success of graph representation
    learning in molecular property prediction, the methods still have limitations:
    1) Few-shot molecular property prediction are not fully explored. 2) Most methods
    depend on training with labeled data, but neglect the chemical domain knowledge.'
  id: totrans-703
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战与局限。尽管图表示学习在分子属性预测中取得了巨大成功，但这些方法仍然存在局限性：1) 少量分子属性预测尚未完全探索。2) 大多数方法依赖于有标签数据的训练，但忽略了化学领域知识。
- en: •
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Future Works. In the future, we expect that: 1) More few-shot learning and
    zero-shot learning methods are studied for molecular property prediction to solve
    the data scarcity problem. 2) Heterogeneous data can be fused for molecular property
    prediction. There are a large amount of heterogeneous data about molecules such
    as knowledge graphs, molecule descriptions and property descriptions. They can
    be considered to assist molecular property prediction. 3) Chemical domain knowledge
    can be leveraged for the prediction model. For example, when we perform affinity
    prediction, we can consider molecular dynamics knowledge.'
  id: totrans-705
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来工作。未来，我们期望：1) 研究更多少量学习和零样本学习方法用于分子属性预测，以解决数据稀缺问题。2) 可以融合异质数据进行分子属性预测。关于分子的异质数据量很大，如知识图谱、分子描述和属性描述。这些可以考虑用于辅助分子属性预测。3)
    可以利用化学领域知识进行预测模型的提升。例如，在进行亲和力预测时，我们可以考虑分子动力学知识。
- en: 12\. Molecular Generation By Fang Sun
  id: totrans-706
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12\. 分子生成 作者：方孙
- en: Molecular generation is pivotal to drug discovery, where it serves a fundamental
    role in downstream tasks like molecular docking (Meng et al., [2011](#bib.bib261))
    and virtual screening (Walters et al., [1998](#bib.bib357)). The goal of molecular
    generation is to produce chemical structures that satisfy a specific molecular
    profile, e.g., novelty, binding affinity, and SA scores. Traditional methods have
    relied on 1D string formats like SMILES (Gómez-Bombarelli et al., [2018](#bib.bib117))
    and SELFIES (Krenn et al., [2020](#bib.bib193)). With the recent advances in graph
    representation learning, numerous graph-based methods have also emerged, where
    molecular graph $\mathcal{G}$ can naturally embody both 2D topology and 3D geometry.
    While recent literature reviews  (Meyers et al., [2021](#bib.bib262); Du et al.,
    [2022a](#bib.bib79)) have covered the general topics of molecular design, this
    chapter is dedicated to the applications of graph representation learning in the
    molecular generation task. Molecular generation is intrinsically a de novo task,
    where molecular structures are generated from scratch to navigate and sample from
    the vast chemical space. Therefore, this chapter does not discuss tasks that restrict
    chemical structures a priori, such as docking (Ganea et al., [2021](#bib.bib104);
    Stärk et al., [2022](#bib.bib333)) and conformation generation (Shi et al., [2021](#bib.bib319);
    Zhu et al., [2022a](#bib.bib468)).
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 分子生成对药物发现至关重要，在下游任务如分子对接 (Meng 等, [2011](#bib.bib261)) 和虚拟筛选 (Walters 等, [1998](#bib.bib357))
    中发挥了基础作用。分子生成的目标是生成满足特定分子特征的化学结构，例如新颖性、结合亲和力和 SA 分数。传统方法依赖于像 SMILES (Gómez-Bombarelli
    等, [2018](#bib.bib117)) 和 SELFIES (Krenn 等, [2020](#bib.bib193)) 这样的1D字符串格式。随着图表示学习的最新进展，出现了众多基于图的方法，其中分子图
    $\mathcal{G}$ 可以自然地体现2D拓扑和3D几何。尽管最近的文献综述 (Meyers 等, [2021](#bib.bib262); Du 等,
    [2022a](#bib.bib79)) 涵盖了分子设计的一般主题，本章专注于图表示学习在分子生成任务中的应用。分子生成本质上是一个从头开始的任务，其中分子结构从零开始生成，以探索和从广阔的化学空间中进行采样。因此，本章不讨论限制化学结构的任务，例如对接 (Ganea
    等, [2021](#bib.bib104); Stärk 等, [2022](#bib.bib333)) 和构象生成 (Shi 等, [2021](#bib.bib319);
    Zhu 等, [2022a](#bib.bib468))。
- en: 12.1\. Taxonomy for molecular featurization methods
  id: totrans-708
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1\. 分子特征化方法的分类
- en: This section categorizes the different methods to featurize molecules. The taxonomy
    presented here is unique to the task of molecular generation, owing to the various
    modalities of molecular entities, complex interactions with other bio-molecular
    systems and formal knowledge from the laws of chemistry and physics.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 本节对分子特征化的不同方法进行了分类。这里所呈现的分类法独特地适用于分子生成任务，这是由于分子实体的各种形式、与其他生物分子系统的复杂相互作用以及化学和物理定律中的正式知识。
- en: 2D topology vs. 3D geometry. Molecular data are multi-modal by nature. For one
    thing, a molecule can be unambiguously represented by its 2D topological graph
    $\mathcal{G}_{\mathrm{2D}}$, where atoms are nodes and bonds are edges. $\mathcal{G}_{\mathrm{2D}}$
    can be encoded by canonical MPNN models like GCN (Kipf and Welling, [2016a](#bib.bib183)),
    GAT (Veličković et al., [2017](#bib.bib352)), and R-GCN (Schlichtkrull et al.,
    [2018](#bib.bib308)), in ways similar to tasks like social networks and knowledge
    graphs. A typical example of this line of work is GCPN (You et al., [2018](#bib.bib419)),
    a graph convolutional policy network generating molecules with desired properties
    such as synthetic accessibility and drug-likeness.
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 2D拓扑与3D几何。分子数据天生具有多模态性。一方面，分子可以通过其2D拓扑图 $\mathcal{G}_{\mathrm{2D}}$ 明确表示，其中原子是节点，化学键是边。$\mathcal{G}_{\mathrm{2D}}$
    可以通过像 GCN (Kipf 和 Welling, [2016a](#bib.bib183))、GAT (Veličković 等, [2017](#bib.bib352))
    和 R-GCN (Schlichtkrull 等, [2018](#bib.bib308)) 这样的标准 MPNN 模型进行编码，类似于社交网络和知识图谱等任务。这一研究方向的一个典型例子是
    GCPN (You 等, [2018](#bib.bib419))，一个图卷积策略网络，用于生成具有合成可达性和药物相似性等所需属性的分子。
- en: For another, the 3D conformation of a molecule can be accurately depicted by
    its 3D geometric graph $\mathcal{G}_{\mathrm{3D}}$, which incorporates 3D atom
    coordinates. In 3D-GNNs like SchNet (Schütt et al., [2018](#bib.bib312)) and OrbNet (Qiao
    et al., [2020b](#bib.bib285)), $\mathcal{G}_{\mathrm{3D}}$ is organized into a
    $k$-NN graph or a radius graph according to the Euclidean distance between atoms.
    It is justifiable to approximate $\mathcal{G}_{\mathrm{3D}}$ as a 3D extension
    to $\mathcal{G}_{\mathrm{2D}}$, since covalent atoms are closest to each other
    in most cases. However, $\mathcal{G}_{\mathrm{3D}}$ can also find a more long-standing
    origin in the realm of computational chemistry  (Frisch et al., [2016](#bib.bib100)),
    where both covalent and non-covalent atomistic interactions are considered to
    optimize the potential surface and simulate molecular dynamics. Therefore, $\mathcal{G}_{\mathrm{3D}}$
    more realistically represents the molecular geometry, which makes a good fit for
    protein pocket binding and 3D-QSAR optimization (Verma et al., [2010](#bib.bib353)).
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，分子的三维构象可以通过其包含三维原子坐标的三维几何图$\mathcal{G}_{\mathrm{3D}}$准确描绘。在像SchNet（Schütt等，[2018](#bib.bib312)）和OrbNet（Qiao等，[2020b](#bib.bib285)）这样的3D-GNNs中，$\mathcal{G}_{\mathrm{3D}}$根据原子之间的欧几里得距离组织成$k$-NN图或半径图。将$\mathcal{G}_{\mathrm{3D}}$近似为$\mathcal{G}_{\mathrm{2D}}$的三维扩展是合理的，因为共价原子在大多数情况下彼此最接近。然而，$\mathcal{G}_{\mathrm{3D}}$在计算化学领域（Frisch等，[2016](#bib.bib100)）也有更久远的起源，其中考虑了共价和非共价的原子交互来优化势能面和模拟分子动力学。因此，$\mathcal{G}_{\mathrm{3D}}$更现实地表示了分子几何结构，这使其非常适合蛋白质口袋结合和三维-QSAR优化（Verma等，[2010](#bib.bib353)）。
- en: Molecules can rotate and translate, affecting their position in the 3D space.
    Therefore, it is ideal to encode these molecules with GNNs equivariant/invariant
    to roto-translations, which can be $\sim 10^{3}$ times more efficient than data
    augmentation (Geiger and Smidt, [2022](#bib.bib114)). Equivariant GNNs can be
    based on irreducible representation (Thomas et al., [2018](#bib.bib348); Anderson
    et al., [2019](#bib.bib9); Fuchs et al., [2020](#bib.bib103); Batzner et al.,
    [2021](#bib.bib19); Brandstetter et al., [2022](#bib.bib29)), regular representation (Finzi
    et al., [2020](#bib.bib96); Hutchinson et al., [2021](#bib.bib155)), or scalarization (Schütt
    et al., [2018](#bib.bib312); Klicpera et al., [2020](#bib.bib186); Liu et al.,
    [2022b](#bib.bib234); Köhler et al., [2020](#bib.bib187); Jing et al., [2021](#bib.bib169);
    Satorras et al., [2021](#bib.bib305); Huang et al., [2022](#bib.bib153); Schütt
    et al., [2021](#bib.bib311); Thölke and Fabritiis, [2022](#bib.bib347); Klicpera
    et al., [2021](#bib.bib185)), which are explained in more detail in Han et al. (Han
    et al., [2022](#bib.bib131)).
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 分子可以旋转和平移，影响它们在三维空间中的位置。因此，理想的做法是使用对旋转-平移等变/不变的GNNs来编码这些分子，这比数据增强要高效$\sim 10^{3}$倍（Geiger和Smidt，[2022](#bib.bib114)）。等变GNNs可以基于不可约表示（Thomas等，[2018](#bib.bib348)；Anderson等，[2019](#bib.bib9)；Fuchs等，[2020](#bib.bib103)；Batzner等，[2021](#bib.bib19)；Brandstetter等，[2022](#bib.bib29)），正规表示（Finzi等，[2020](#bib.bib96)；Hutchinson等，[2021](#bib.bib155)），或标量化（Schütt等，[2018](#bib.bib312)；Klicpera等，[2020](#bib.bib186)；Liu等，[2022b](#bib.bib234)；Köhler等，[2020](#bib.bib187)；Jing等，[2021](#bib.bib169)；Satorras等，[2021](#bib.bib305)；Huang等，[2022](#bib.bib153)；Schütt等，[2021](#bib.bib311)；Thölke和Fabritiis，[2022](#bib.bib347)；Klicpera等，[2021](#bib.bib185)），这些在Han等（Han等，[2022](#bib.bib131)）中有更详细的解释。
- en: Unbounded vs. binding-based. Earlier works have aimed to generate unbounded
    molecules in either 2D or 3D space, striving to learn good molecular representations
    through this task. In the 2D scenario, GraphNVP (Madhawa et al., [2019](#bib.bib252))
    first introduces a flow-based model to learn an invertible transformation between
    the 2D chemical space and the latent space. GraphAF (Shi et al., [2020](#bib.bib320))
    further adopts an autoregressive generation scheme to check the valence of the
    generated atoms and bonds. In the 3D scenario, G-SchNet  (Gebauer et al., [2019](#bib.bib112))
    first proposes to utilize $\mathcal{G}_{\mathrm{3D}}$ (instead of 3D density grids)
    as the generation backbone. It encodes $\mathcal{G}_{\mathrm{3D}}$ via SchNet,
    and uses an auxiliary token to generate atoms on the discretized 3D space autoregressively.
    G-SphereNet (Luo and Ji, [2022](#bib.bib246)) uses symmetry-invariant representations
    in a spherical coordinate system (SCS) to generate atoms in the continuous 3D
    space and preserve equivariance.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 无约束与基于绑定的对比。早期的工作旨在生成 2D 或 3D 空间中的无约束分子，努力通过这一任务学习良好的分子表示。在 2D 场景中，GraphNVP（Madhawa
    等， [2019](#bib.bib252)）首次引入流式模型，以学习 2D 化学空间与潜在空间之间的可逆变换。GraphAF（Shi 等， [2020](#bib.bib320)）进一步采用自回归生成方案来检查生成的原子和键的价态。在
    3D 场景中，G-SchNet（Gebauer 等， [2019](#bib.bib112)）首次提出利用 $\mathcal{G}_{\mathrm{3D}}$（而非
    3D 密度网格）作为生成的基础。它通过 SchNet 对 $\mathcal{G}_{\mathrm{3D}}$ 进行编码，并使用辅助令牌在离散的 3D 空间上自回归地生成原子。G-SphereNet（Luo
    和 Ji， [2022](#bib.bib246)）使用球坐标系统（SCS）中的对称不变表示生成连续 3D 空间中的原子，并保持等变性。
- en: Unbounded models adopt certain techniques to optimize specific properties of
    the generated molecules. GCPN and GraphAF use scores likes logP, QED, and chemical
    validity to tune the model via reinforcement learning. EDM (Hoogeboom et al.,
    [2022](#bib.bib143)) can generate 3D molecules with property $c$ by re-training
    the diffusion model with $c$’s feature vector concatenated to the E(n) equivariant
    dynamics function $\hat{\boldsymbol{\epsilon}}_{t}=\phi\left(\boldsymbol{z}_{t},[t,c]\right)$.
    cG-SchNet  (Gebauer et al., [2022](#bib.bib113)) adopts a conditioning network
    architecture to jointly target multiple electronic properties during conditional
    generation without the need to re-train the model. RetMol (Wang et al., [2022b](#bib.bib375))
    uses a retrieval-based model for controllable generation.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 无约束模型采用某些技术来优化生成分子的特定属性。GCPN 和 GraphAF 使用 logP、QED 和化学有效性等评分，通过强化学习来调整模型。EDM（Hoogeboom
    等， [2022](#bib.bib143)）通过将 $c$ 的特征向量与 E(n) 等变动力学函数 $\hat{\boldsymbol{\epsilon}}_{t}=\phi\left(\boldsymbol{z}_{t},[t,c]\right)$
    级联来重新训练扩散模型，从而生成具有属性 $c$ 的 3D 分子。cG-SchNet（Gebauer 等， [2022](#bib.bib113)）采用条件网络架构，在条件生成过程中联合目标多个电子属性，无需重新训练模型。RetMol（Wang
    等， [2022b](#bib.bib375)）使用基于检索的模型进行可控生成。
- en: On the other hand, binding-based methods generate drug-like molecules (aka.
    ligands) according to the binding site (aka. binding pocket) of a protein receptor.
    Drawing inspirations from the lock-and-key model for enzyme action  (Fischer,
    [1894](#bib.bib97)), works like LiGAN (Ragoza et al., [2022](#bib.bib291)) and
    DESERT (Long et al., [2022](#bib.bib240)) use 3D density grids to fit the density
    surface between the ligand and the receptor, encoded by 3D-CNNs. Meanwhile, a
    growing amount of literature has adopted $\mathcal{G}_{\mathrm{3D}}$ for representing
    ligand and receptor molecules, because $\mathcal{G}_{\mathrm{3D}}$ more accurately
    depicts molecular structures and atomistic interactions both within and between
    the ligand and the receptor. Representative works include 3D-SBDD (Luo et al.,
    [2021b](#bib.bib242)), GraphBP (Liu et al., [2022a](#bib.bib231)), Pocket2Mol (Peng
    et al., [2022](#bib.bib276)), and DiffSBDD  (Schneuing et al., [2022](#bib.bib310)).
    GraphBP shares a similar workflow with G-SphereNet, except that the receptor atoms
    are also incorporated into $\mathcal{G}_{\mathrm{3D}}$ to depict the 3D geometry
    at the binding pocket.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，基于结合的的方法根据蛋白质受体的结合位点（即结合口袋）生成类似药物的分子（即配体）。受到酶作用中的锁和钥匙模型的启发（Fischer，[1894](#bib.bib97)），像LiGAN（Ragoza等，[2022](#bib.bib291)）和DESERT（Long等，[2022](#bib.bib240)）等工作使用3D密度网格来拟合配体与受体之间的密度表面，这一过程通过3D-CNN编码。同时，越来越多的文献采用$\mathcal{G}_{\mathrm{3D}}$来表示配体和受体分子，因为$\mathcal{G}_{\mathrm{3D}}$更准确地描绘了分子结构和配体与受体之间的原子相互作用。代表性工作包括3D-SBDD（Luo等，[2021b](#bib.bib242)）、GraphBP（Liu等，[2022a](#bib.bib231)）、Pocket2Mol（Peng等，[2022](#bib.bib276)）和DiffSBDD（Schneuing等，[2022](#bib.bib310)）。GraphBP与G-SphereNet的工作流程相似，但受体原子也被纳入$\mathcal{G}_{\mathrm{3D}}$中，以描绘结合口袋的3D几何结构。
- en: Atom-based vs. fragment-based. Molecules are inherently hierarchical structures.
    At the atomistic level, molecules are represented by encoding atoms and bonds.
    At a coarser level, molecules can also be represented as molecular fragments like
    functional groups or chemical sub-structures. Both the composition and the geometry
    are fixed within a given fragment, e.g., the planar peptide-bond (–CO–NH–) structure.
    Fragment-based generation effectively reduces the degree of freedom (DOF) of chemical
    structures, and injects well-established knowledge about molecular patterns and
    reactivity. JT-VAE (Jin et al., [2018](#bib.bib166)) decomposes 2D molecular graph
    $\mathcal{G}_{\mathrm{2D}}$ into a junction-tree structure $\mathcal{T}$, which
    is further encoded via tree message-passing. DeepScaffold (Li et al., [2019a](#bib.bib217))
    expands the provided molecular scaffold into 3D molecules. L-Net (Li et al., [2021a](#bib.bib218))
    adopts a graph U-Net architecture and devises a custom three-level node clustering
    scheme for pooling and unpooling operations in molecular graphs. A number of works
    have also emerged lately for fragment-based generation in the binding-based setting,
    including FLAG (ZHANG et al., [[n. d.]](#bib.bib449)) and FragDiff (Peng et al.,
    [2023](#bib.bib275)). FLAG uses a regression-based approach to sequentially decide
    the type and torsion angle of the next fragment to be placed at the binding site,
    and finally optimizes the molecule conformation via a pseudo-force field. FragDiff
    also adopts a sequential generation process but uses a diffusion model to determine
    the type and pose of each fragment in one go.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 基于原子 vs. 基于片段。分子本质上是层级结构。在原子层面，分子通过编码原子和键来表示。在更粗略的层面上，分子也可以表示为像功能团或化学子结构这样的分子片段。无论是组成还是几何结构，在给定片段内都是固定的，例如平面肽键（–CO–NH–）结构。基于片段的生成有效地减少了化学结构的自由度（DOF），并注入了关于分子模式和反应性的成熟知识。JT-VAE（Jin等，[2018](#bib.bib166)）将2D分子图$\mathcal{G}_{\mathrm{2D}}$分解为一个连接树结构$\mathcal{T}$，进一步通过树消息传递编码。DeepScaffold（Li等，[2019a](#bib.bib217)）将提供的分子框架扩展为3D分子。L-Net（Li等，[2021a](#bib.bib218)）采用图U-Net架构，并设计了自定义的三层节点聚类方案用于分子图中的池化和反池化操作。最近也出现了许多基于片段的生成方法，包括FLAG（ZHANG等，[n. d.](#bib.bib449)）和FragDiff（Peng等，[2023](#bib.bib275)）。FLAG采用基于回归的方法，依次决定下一个片段的类型和扭转角度，并最终通过伪力场优化分子构象。FragDiff也采用了顺序生成过程，但使用扩散模型一次性确定每个片段的类型和姿态。
- en: 12.2\. Generative methods for molecular graphs
  id: totrans-717
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2\. 分子图的生成方法
- en: For a molecular graph generation process, the model first learns a latent distribution
    $P(Z|\mathcal{G})$ characterizing the input molecular graphs. A new molecular
    graph $\hat{\mathcal{G}}$ is then generated by sampling and decoding from this
    learned distribution. Various models have been adopted to generate molecular graphs,
    including generative adversarial network (GAN), variational auto-encoder (VAE),
    normalizing flow (NF), diffusion model (DM), and autoregressive model (AR).
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分子图生成过程，模型首先学习一个潜在分布 $P(Z|\mathcal{G})$，该分布描述输入分子图。然后，通过从这个学习到的分布中采样和解码来生成新的分子图
    $\hat{\mathcal{G}}$。已经采用了多种模型来生成分子图，包括生成对抗网络（GAN）、变分自编码器（VAE）、标准化流（NF）、扩散模型（DM）和自回归模型（AR）。
- en: Generative adversarial network (GAN). GAN (Goodfellow et al., [2020](#bib.bib118))
    is trained to discriminate real data $\boldsymbol{x}$ from generated generated
    data $\boldsymbol{z}$, with the training object formalized as
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）。GAN（Goodfellow 等，[2020](#bib.bib118)）的训练目标是区分真实数据 $\boldsymbol{x}$
    和生成的数据 $\boldsymbol{z}$，训练目标形式化为
- en: '| (126) |  | $\min_{G}\max_{D}\mathcal{L}(D,G)=\mathbb{E}_{\boldsymbol{x}\sim
    p_{\text{data }}}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z}\sim p(\boldsymbol{z})}[\log(1-D(G(\boldsymbol{z})))],$
    |  |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '| (126) |  | $\min_{G}\max_{D}\mathcal{L}(D,G)=\mathbb{E}_{\boldsymbol{x}\sim
    p_{\text{data }}}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z}\sim p(\boldsymbol{z})}[\log(1-D(G(\boldsymbol{z})))],$
    |  |'
- en: where $G(\cdot)$ is the generator function and $D(\cdot)$ is the discriminator
    function. For example, MolGAN (De Cao and Kipf, [2018](#bib.bib66)) encodes $\mathcal{G}_{\mathrm{2D}}$
    with R-GCN, trains $D$ and $G$ with improved W-GAN (Arjovsky et al., [2017](#bib.bib10)),
    and uses reinforcement learning to generate attributed molecules, where the score
    function is assigned from RDKit (Landrum et al., [2013](#bib.bib199)) and chemical
    validity.
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G(\cdot)$ 是生成函数，$D(\cdot)$ 是判别函数。例如，MolGAN（De Cao 和 Kipf，[2018](#bib.bib66)）用
    R-GCN 编码 $\mathcal{G}_{\mathrm{2D}}$，用改进的 W-GAN（Arjovsky 等，[2017](#bib.bib10)）训练
    $D$ 和 $G$，并使用强化学习生成带属性的分子，其中评分函数来源于 RDKit（Landrum 等，[2013](#bib.bib199)）和化学有效性。
- en: 'Varaitional auto-encoder (VAE). In VAE (Kingma and Welling, [2013](#bib.bib181)),
    the decoder parameterizes the conditional likelihood distribution $p_{\theta}(\boldsymbol{x}|\boldsymbol{z})$,
    and the encoder parameterizes an approximate posterior distribution $q_{\phi}(\boldsymbol{z}|\boldsymbol{x})\approx
    p_{\theta}(\boldsymbol{z}|\boldsymbol{x})$. The model is optimized by the evidence
    lower bound (ELBO), consisting of the reconstruction loss term and the distance
    loss term:'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）。在 VAE（Kingma 和 Welling，[2013](#bib.bib181)）中，解码器对条件似然分布 $p_{\theta}(\boldsymbol{x}|\boldsymbol{z})$
    进行参数化，编码器对近似后验分布 $q_{\phi}(\boldsymbol{z}|\boldsymbol{x})\approx p_{\theta}(\boldsymbol{z}|\boldsymbol{x})$
    进行参数化。模型通过证据下界（ELBO）进行优化，包括重构损失项和距离损失项：
- en: '| (127) |  | $\max_{\theta,\phi}\mathcal{L}_{\theta,\phi}(\boldsymbol{x}):=\mathbb{E}_{\boldsymbol{z}\sim
    q_{\phi}(\cdot&#124;\boldsymbol{x})}\left[\ln\frac{p_{\theta}(\boldsymbol{x},\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}&#124;\boldsymbol{x})}\right]=\ln
    p_{\theta}(\boldsymbol{x})-D_{\mathrm{KL}}\left(q_{\phi}(\cdot&#124;\boldsymbol{x})\&#124;p_{\theta}(\cdot&#124;\boldsymbol{x})\right).$
    |  |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
  zh: '| (127) |  | $\max_{\theta,\phi}\mathcal{L}_{\theta,\phi}(\boldsymbol{x}):=\mathbb{E}_{\boldsymbol{z}\sim
    q_{\phi}(\cdot| \boldsymbol{x})}\left[\ln\frac{p_{\theta}(\boldsymbol{x},\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}|
    \boldsymbol{x})}\right]=\ln p_{\theta}(\boldsymbol{x})-D_{\mathrm{KL}}\left(q_{\phi}(\cdot|
    \boldsymbol{x}) \| p_{\theta}(\cdot| \boldsymbol{x})\right).$ |  |'
- en: Maximizing ELBO is equivalent to simultaneously maximizing the log-likelihood
    of the observed data, and minimizing the divergence of the approximate posterior
    ${q_{\phi}(\cdot|x)}$ from the exact posterior ${p_{\theta}(\cdot|x)}$. Representative
    works along this thread include JT-VAE (Jin et al., [2018](#bib.bib166)), GraphVAE (Simonovsky
    and Komodakis, [2018](#bib.bib326)), and CGVAE (Liu et al., [2018a](#bib.bib232))
    for the 2D generation task, and 3DMolNet (Nesterov et al., [2020](#bib.bib268))
    for the 3D generation task.
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化 ELBO 等同于同时最大化观察数据的对数似然，并最小化近似后验 ${q_{\phi}(\cdot|x)}$ 与精确后验 ${p_{\theta}(\cdot|x)}$
    之间的散度。沿着这一方向的代表性工作包括用于 2D 生成任务的 JT-VAE（Jin 等，[2018](#bib.bib166)）、GraphVAE（Simonovsky
    和 Komodakis，[2018](#bib.bib326)）和 CGVAE（Liu 等，[2018a](#bib.bib232)），以及用于 3D 生成任务的
    3DMolNet（Nesterov 等，[2020](#bib.bib268)）。
- en: 'Autoregressive model (AR). Autoregressive model is an umbrella definition for
    any model that sequentially generates the components (atoms or fragments) of a
    molecule. ARs better capture the interdependency within the molecular structure
    and allows for explicit valency check. For each step in AR, the new component
    can be produced using different techniques:'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型（AR）。自回归模型是对任何顺序生成分子组成部分（原子或片段）的模型的总称。AR 更好地捕捉分子结构中的相互依赖关系，并允许显式的价键检查。对于
    AR 中的每一步，可以使用不同的技术来生成新的组成部分：
- en: •
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Regression/classification, such is the case with 3D-SBDD (Luo et al., [2021b](#bib.bib242)),
    Pocket2Mol (Peng et al., [2022](#bib.bib276)), etc.
  id: totrans-727
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回归/分类，例如 3D-SBDD (Luo et al., [2021b](#bib.bib242))、Pocket2Mol (Peng et al.,
    [2022](#bib.bib276)) 等。
- en: •
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reinforcement learning, such is the case with L-Net (Li et al., [2021a](#bib.bib218)),
    DeepLigBuilder (Li et al., [2021b](#bib.bib219)), etc.
  id: totrans-729
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 强化学习，例如 L-Net (Li et al., [2021a](#bib.bib218))、DeepLigBuilder (Li et al., [2021b](#bib.bib219))
    等。
- en: •
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Probabilistic models like normalizing flow and diffusion.
  id: totrans-731
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 像正规化流和扩散这样的概率模型。
- en: 'Normalizing flow (NF). Based on the change-of-variable theorem, NF (Rezende
    and Mohamed, [2015](#bib.bib295)) constructs an invertible mapping $f$ between
    a complex data distribution $\boldsymbol{x}\sim X$: and a normalized latent distribution
    $\boldsymbol{z}\sim Z$. Unlike VAE, which has juxtaposed parameters for encoder
    and decoder, the flow model uses the same set of parameter for encoding and encoding:
    reverse flow $f^{-1}$ for generation, and forward flow $f$ for training:'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 正规化流（NF）。基于变量变换定理，NF (Rezende and Mohamed, [2015](#bib.bib295)) 构建了一个复杂数据分布
    $\boldsymbol{x}\sim X$ 和一个标准化潜在分布 $\boldsymbol{z}\sim Z$ 之间的可逆映射 $f$。与 VAE 不同，VAE
    为编码器和解码器具有并列的参数，而流模型使用相同的参数集进行编码和解码：反向流 $f^{-1}$ 用于生成，正向流 $f$ 用于训练：
- en: '| (128) |  | $\displaystyle\max_{f}\log p(\boldsymbol{x})$ | $\displaystyle=\log
    p_{K}\left(\boldsymbol{z}_{K}\right)$ |  |'
  id: totrans-733
  prefs: []
  type: TYPE_TB
  zh: '| (128) |  | $\displaystyle\max_{f}\log p(\boldsymbol{x})$ | $\displaystyle=\log
    p_{K}\left(\boldsymbol{z}_{K}\right)$ |  |'
- en: '| (129) |  |  | $\displaystyle=\log p_{K-1}\left(\boldsymbol{z}_{K-1}\right)-\log\left&#124;\operatorname{det}\left(\frac{df_{K}\left(\boldsymbol{z}_{K-1}\right)}{d\boldsymbol{z}_{K-1}}\right)\right&#124;$
    |  |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
  zh: '| (129) |  |  | $\displaystyle=\log p_{K-1}\left(\boldsymbol{z}_{K-1}\right)-\log\left|\operatorname{det}\left(\frac{df_{K}\left(\boldsymbol{z}_{K-1}\right)}{d\boldsymbol{z}_{K-1}}\right)\right|$
    |  |'
- en: '| (130) |  |  | $\displaystyle=\ldots$ |  |'
  id: totrans-735
  prefs: []
  type: TYPE_TB
  zh: '| (130) |  |  | $\displaystyle=\ldots$ |  |'
- en: '| (131) |  |  | $\displaystyle=\log p_{0}\left(\boldsymbol{z}_{0}\right)-\sum_{i=1}^{K}\log\left&#124;\operatorname{det}\left(\frac{df_{i}\left(\boldsymbol{z}_{i-1}\right)}{d\boldsymbol{z}_{i-1}}\right)\right&#124;,$
    |  |'
  id: totrans-736
  prefs: []
  type: TYPE_TB
  zh: '| (131) |  |  | $\displaystyle=\log p_{0}\left(\boldsymbol{z}_{0}\right)-\sum_{i=1}^{K}\log\left|\operatorname{det}\left(\frac{df_{i}\left(\boldsymbol{z}_{i-1}\right)}{d\boldsymbol{z}_{i-1}}\right)\right|,$
    |  |'
- en: where $f=f_{K}\circ f_{K-1}\circ...\circ f_{1}$ is a composite of $K$ blocks
    of transformation. While GraphNVP (Madhawa et al., [2019](#bib.bib252)) generates
    the molecular graph with NF in one go, following works tend to use the autoregressive
    flow model, including GraphAF (Shi et al., [2020](#bib.bib320)), GraphDF (Luo
    et al., [2021d](#bib.bib247)), and GraphBP (Liu et al., [2022a](#bib.bib231)).
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f=f_{K}\circ f_{K-1}\circ...\circ f_{1}$ 是 $K$ 个变换块的复合体。虽然 GraphNVP (Madhawa
    et al., [2019](#bib.bib252)) 一次性生成分子图，但后续工作往往使用自回归流模型，包括 GraphAF (Shi et al.,
    [2020](#bib.bib320))、GraphDF (Luo et al., [2021d](#bib.bib247)) 和 GraphBP (Liu
    et al., [2022a](#bib.bib231))。
- en: 'Diffusion model (DM). Diffusion models (Sohl-Dickstein et al., [2015](#bib.bib328);
    Song and Ermon, [2019](#bib.bib331); Ho et al., [2020](#bib.bib141)) define a
    Markov chain of diffusion steps to slowly add random noise to data $\boldsymbol{x}_{0}\sim
    q(\boldsymbol{x})$:'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型（DM）。扩散模型 (Sohl-Dickstein et al., [2015](#bib.bib328); Song and Ermon, [2019](#bib.bib331);
    Ho et al., [2020](#bib.bib141)) 定义了一个扩散步骤的马尔可夫链，以慢慢向数据 $\boldsymbol{x}_{0}\sim
    q(\boldsymbol{x})$ 添加随机噪声：
- en: '| (132) |  | $\displaystyle q(\boldsymbol{x}_{t}&#124;\boldsymbol{x}_{t-1})$
    | $\displaystyle=\mathcal{N}(\boldsymbol{x}_{t};\sqrt{1-\beta_{t}}\boldsymbol{x}_{t-1},\beta_{t}\boldsymbol{I}),$
    |  |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '| (132) |  | $\displaystyle q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1})$ | $\displaystyle=\mathcal{N}(\boldsymbol{x}_{t};\sqrt{1-\beta_{t}}\boldsymbol{x}_{t-1},\beta_{t}\boldsymbol{I}),$
    |  |'
- en: '| (133) |  | $\displaystyle q(\boldsymbol{x}_{1:T}&#124;\boldsymbol{x}_{0})$
    | $\displaystyle=\prod^{T}_{t=1}q(\boldsymbol{x}_{t}&#124;\boldsymbol{x}_{t-1}).$
    |  |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '| (133) |  | $\displaystyle q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_{0})$ | $\displaystyle=\prod^{T}_{t=1}q(\boldsymbol{x}_{t}|\boldsymbol{x}_{t-1}).$
    |  |'
- en: 'They then learn to reverse the diffusion process to construct desired data
    samples from the noise:'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，他们学习逆转扩散过程，从噪声中构建所需的数据样本：
- en: '| (134) |  | $\displaystyle p_{\theta}(\boldsymbol{x}_{0:T})$ | $\displaystyle=p(\boldsymbol{x}_{T})\prod^{T}_{t=1}p_{\theta}(\boldsymbol{x}_{t-1}&#124;\boldsymbol{x}_{t}),$
    |  |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '| (134) |  | $\displaystyle p_{\theta}(\boldsymbol{x}_{0:T})$ | $\displaystyle=p(\boldsymbol{x}_{T})\prod^{T}_{t=1}p_{\theta}(\boldsymbol{x}_{t-1}&#124;\boldsymbol{x}_{t}),$
    |  |'
- en: '| (135) |  | $\displaystyle p_{\theta}(\boldsymbol{x}_{t-1}&#124;\boldsymbol{x}_{t})$
    | $\displaystyle=\mathcal{N}(\boldsymbol{x}_{t-1};\boldsymbol{\mu}_{\theta}(\boldsymbol{x}_{t},t),\boldsymbol{\Sigma}_{\theta}(\boldsymbol{x}_{t},t)),$
    |  |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '| (135) |  | $\displaystyle p_{\theta}(\boldsymbol{x}_{t-1}&#124;\boldsymbol{x}_{t})$
    | $\displaystyle=\mathcal{N}(\boldsymbol{x}_{t-1};\boldsymbol{\mu}_{\theta}(\boldsymbol{x}_{t},t),\boldsymbol{\Sigma}_{\theta}(\boldsymbol{x}_{t},t)),$
    |  |'
- en: while the models are trained using a variational lower bound. Diffusion models
    have been applied to generate unbounded 3D molecules in EDM (Hoogeboom et al.,
    [2022](#bib.bib143)), and binding-specific ligands in DiffSBDD (Schneuing et al.,
    [2022](#bib.bib310)) and DiffBP (Lin et al., [2022a](#bib.bib227)). Diffusion
    can also be applied to generate molecular fragments in autoregressive models,
    as is the case with FragDiff (Peng et al., [2023](#bib.bib275)).
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，这些模型是通过变分下界进行训练的。扩散模型已被应用于在 EDM (Hoogeboom et al., [2022](#bib.bib143)) 中生成无限制的
    3D 分子，以及在 DiffSBDD (Schneuing et al., [2022](#bib.bib310)) 和 DiffBP (Lin et al.,
    [2022a](#bib.bib227)) 中生成特定于结合的配体。扩散也可以应用于自回归模型中生成分子片段，例如 FragDiff (Peng et al.,
    [2023](#bib.bib275))。
- en: Table 10\. Summary of molecular generation models.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10\. 分子生成模型总结。
- en: '| Model | 2D/3D | Binding- based | Fragment- based | GNN Backbone | Generative
    Model |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 2D/3D | 基于绑定 | 基于片段 | GNN 主干 | 生成模型 |'
- en: '| GCPN (You et al., [2018](#bib.bib419)) | 2D |  |  | GCN (Kipf and Welling,
    [2016a](#bib.bib183)) | GAN |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '| GCPN (You et al., [2018](#bib.bib419)) | 2D |  |  | GCN (Kipf and Welling,
    [2016a](#bib.bib183)) | GAN |'
- en: '| MolGAN (De Cao and Kipf, [2018](#bib.bib66)) | 2D |  |  | R-GCN (Schlichtkrull
    et al., [2018](#bib.bib308)) | GAN |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
  zh: '| MolGAN (De Cao and Kipf, [2018](#bib.bib66)) | 2D |  |  | R-GCN (Schlichtkrull
    et al., [2018](#bib.bib308)) | GAN |'
- en: '| DEFactor (Assouel et al., [2018](#bib.bib12)) | 2D |  |  | GCN | GAN |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '| DEFactor (Assouel et al., [2018](#bib.bib12)) | 2D |  |  | GCN | GAN |'
- en: '| GraphVAE (Simonovsky and Komodakis, [2018](#bib.bib326)) | 2D |  |  | ECC (Simonovsky
    and Komodakis, [2017](#bib.bib325)) | VAE |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '| GraphVAE (Simonovsky and Komodakis, [2018](#bib.bib326)) | 2D |  |  | ECC (Simonovsky
    and Komodakis, [2017](#bib.bib325)) | VAE |'
- en: '| MDVAE (Du et al., [2022b](#bib.bib80)) | 2D |  |  | GGNN (Li et al., [2015](#bib.bib220))
    | VAE |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
  zh: '| MDVAE (Du et al., [2022b](#bib.bib80)) | 2D |  |  | GGNN (Li et al., [2015](#bib.bib220))
    | VAE |'
- en: '| JT-VAE (Jin et al., [2018](#bib.bib166)) | 2D |  | $\checkmark$ | MPNN (Gilmer
    et al., [2017](#bib.bib116)) | VAE |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
  zh: '| JT-VAE (Jin et al., [2018](#bib.bib166)) | 2D |  | $\checkmark$ | MPNN (Gilmer
    et al., [2017](#bib.bib116)) | VAE |'
- en: '| CGVAE (Liu et al., [2018a](#bib.bib232)) | 2D |  |  | GGNN | VAE |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '| CGVAE (Liu et al., [2018a](#bib.bib232)) | 2D |  |  | GGNN | VAE |'
- en: '| DeepScaffold (Li et al., [2019a](#bib.bib217)) | 2D |  | $\checkmark$ | GCN
    | VAE |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '| DeepScaffold (Li et al., [2019a](#bib.bib217)) | 2D |  | $\checkmark$ | GCN
    | VAE |'
- en: '| GraphNVP (Madhawa et al., [2019](#bib.bib252)) | 2D |  |  | R-GCN | NF |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
  zh: '| GraphNVP (Madhawa et al., [2019](#bib.bib252)) | 2D |  |  | R-GCN | NF |'
- en: '| MoFlow (Zang and Wang, [2020](#bib.bib430)) | 2D |  |  | R-GCN | NF |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '| MoFlow (Zang and Wang, [2020](#bib.bib430)) | 2D |  |  | R-GCN | NF |'
- en: '| GraphAF (Shi et al., [2020](#bib.bib320)) | 2D |  |  | R-GCN | NF + AR |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
  zh: '| GraphAF (Shi et al., [2020](#bib.bib320)) | 2D |  |  | R-GCN | NF + AR |'
- en: '| GraphDF (Luo et al., [2021d](#bib.bib247)) | 2D |  |  | R-GCN | NF + AR |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
  zh: '| GraphDF (Luo et al., [2021d](#bib.bib247)) | 2D |  |  | R-GCN | NF + AR |'
- en: '| L-Net (Li et al., [2021a](#bib.bib218)) | 3D |  | $\checkmark$ | g-U-Net (Gao
    and Ji, [2019](#bib.bib105)) | AR |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
  zh: '| L-Net (Li et al., [2021a](#bib.bib218)) | 3D |  | $\checkmark$ | g-U-Net (Gao
    and Ji, [2019](#bib.bib105)) | AR |'
- en: '| G-SchNet (Gebauer et al., [2019](#bib.bib112)) | 3D |  |  | SchNet (Schütt
    et al., [2018](#bib.bib312)) | AR |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
  zh: '| G-SchNet (Gebauer et al., [2019](#bib.bib112)) | 3D |  |  | SchNet (Schütt
    et al., [2018](#bib.bib312)) | AR |'
- en: '| GEN3D (Roney et al., [2021](#bib.bib298)) | 3D |  |  | EGNN (Satorras et al.,
    [2021](#bib.bib305)) | AR |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
  zh: '| GEN3D (Roney et al., [2021](#bib.bib298)) | 3D |  |  | EGNN (Satorras et al.,
    [2021](#bib.bib305)) | AR |'
- en: '| G-SphereNet (Luo and Ji, [2022](#bib.bib246)) | 3D |  |  | SphereNet (Liu
    et al., [2022b](#bib.bib234)) | NF + AR |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
  zh: '| G-SphereNet (Luo and Ji, [2022](#bib.bib246)) | 3D |  |  | SphereNet (Liu
    et al., [2022b](#bib.bib234)) | NF + AR |'
- en: '| EDM (Hoogeboom et al., [2022](#bib.bib143)) | 3D |  |  | EGNN | DM |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
  zh: '| EDM (Hoogeboom et al., [2022](#bib.bib143)) | 3D |  |  | EGNN | DM |'
- en: '| 3D-SBDD (Luo et al., [2021b](#bib.bib242)) | 3D | $\checkmark$ |  | SchNet
    | AR |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
  zh: '| 3D-SBDD (Luo et al., [2021b](#bib.bib242)) | 3D | $\checkmark$ |  | SchNet
    | AR |'
- en: '| Pocket2Mol (Peng et al., [2022](#bib.bib276)) | 3D | $\checkmark$ |  | GVP (Jing
    et al., [2020](#bib.bib168)) | AR |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
  zh: '| Pocket2Mol (Peng et al., [2022](#bib.bib276)) | 3D | $\checkmark$ |  | GVP (Jing
    et al., [2020](#bib.bib168)) | AR |'
- en: '| FLAG (ZHANG et al., [[n. d.]](#bib.bib449)) | 3D | $\checkmark$ | $\checkmark$
    | SchNet | AR |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
  zh: '| FLAG (ZHANG et al., [[n. d.]](#bib.bib449)) | 3D | $\checkmark$ | $\checkmark$
    | SchNet | AR |'
- en: '| GraphBP (Liu et al., [2022a](#bib.bib231)) | 3D | $\checkmark$ |  | SchNet
    | NF + AR |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
  zh: '| GraphBP (Liu 等，[2022a](#bib.bib231)) | 3D | $\checkmark$ |  | SchNet | NF
    + AR |'
- en: '| DiffBP (Lin et al., [2022a](#bib.bib227)) | 3D | $\checkmark$ |  | EGNN |
    DM |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '| DiffBP (Lin 等，[2022a](#bib.bib227)) | 3D | $\checkmark$ |  | EGNN | DM |'
- en: '| DiffSBDD (Schneuing et al., [2022](#bib.bib310)) | 3D | $\checkmark$ |  |
    EGNN | DM |'
  id: totrans-769
  prefs: []
  type: TYPE_TB
  zh: '| DiffSBDD (Schneuing 等，[2022](#bib.bib310)) | 3D | $\checkmark$ |  | EGNN
    | DM |'
- en: '| FragDiff (Peng et al., [2023](#bib.bib275)) | 2D + 3D | $\checkmark$ | $\checkmark$
    | MPNN | DM + AR |'
  id: totrans-770
  prefs: []
  type: TYPE_TB
  zh: '| FragDiff (Peng 等，[2023](#bib.bib275)) | 2D + 3D | $\checkmark$ | $\checkmark$
    | MPNN | DM + AR |'
- en: 12.3\. Summary and prospects
  id: totrans-771
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3\. 总结与展望
- en: We wrap up this chapter with Table [10](#S12.T10 "Table 10 ‣ 12.2\. Generative
    methods for molecular graphs ‣ 12\. Molecular Generation By Fang Sun ‣ A Comprehensive
    Survey on Deep Graph Representation Learning"), which profiles existing molecular
    generation models according to their taxonomy for molecular featurization, the
    GNN backbone, and the generative method. This chapter covers the critical topics
    of molecular generation, which also elicit valuable insights into the promising
    directions for future research. We summarize these important aspects as follows.
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过表格 [10](#S12.T10 "Table 10 ‣ 12.2\. Generative methods for molecular graphs
    ‣ 12\. Molecular Generation By Fang Sun ‣ A Comprehensive Survey on Deep Graph
    Representation Learning") 结束本章，该表格根据分子特征化的分类、GNN 主干和生成方法，对现有的分子生成模型进行了概述。本章涵盖了分子生成的关键主题，这些主题还提供了对未来研究有前景方向的宝贵见解。我们总结了以下这些重要方面。
- en: Techniques. Graph neural networks can be flexibly leveraged to encode molecular
    features on different representation levels and across different problem settings.
    Canonical GNNs like GCN (Kipf and Welling, [2016a](#bib.bib183)), GAT (Veličković
    et al., [2017](#bib.bib352)), and R-GCN (Schlichtkrull et al., [2018](#bib.bib308))
    have been widely adopted to model 2D molecular graphs, while 3D equivariant GNNs
    have also been effective in modeling 3D molecular graphs. In particular, this
    3D approach can be readily extended to binding-based scenarios, where the 3D geometry
    of the binding protein receptor is considered alongside the ligand geometry per
    se. Fragment-based models like JT-VAE (Jin et al., [2018](#bib.bib166)) and L-Net (Li
    et al., [2021a](#bib.bib218)) can also effectively capture the hierarchical molecular
    structure. Various generative methods have also been effectively incorporated
    into the molecular setting, including generative adversarial network (GAN), variational
    auto-encoder (VAE), autoregressive model (AR), normalizing flow (NF), and diffusion
    model (DM). These models have been able to generate valid 2D molecular topologies
    and realistic 3D molecular geometries, greatly accelerating the search for drug
    candidates.
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 技术。图神经网络可以灵活地用于编码不同表示级别和不同问题设置下的分子特征。像 GCN（Kipf 和 Welling，[2016a](#bib.bib183)）、GAT（Veličković
    等，[2017](#bib.bib352)）和 R-GCN（Schlichtkrull 等，[2018](#bib.bib308)）等经典 GNN 已被广泛应用于建模
    2D 分子图，而 3D 等变 GNN 在建模 3D 分子图方面也很有效。特别是，这种 3D 方法可以方便地扩展到基于结合的场景，其中考虑了结合蛋白质受体的
    3D 几何形状以及配体几何形状。基于片段的模型，如 JT-VAE（Jin 等，[2018](#bib.bib166)）和 L-Net（Li 等，[2021a](#bib.bib218)），也能有效捕捉分子结构的层次性。各种生成方法也已有效地融入分子设置中，包括生成对抗网络（GAN）、变分自编码器（VAE）、自回归模型（AR）、归一化流（NF）和扩散模型（DM）。这些模型能够生成有效的
    2D 分子拓扑和现实的 3D 分子几何形状，大大加快了药物候选物的搜索速度。
- en: Challenges and Limitations. While there has been an abundant supply of unlabelled
    molecular structural and geometric data (Irwin et al., [2012](#bib.bib156); Spackman
    et al., [2016](#bib.bib332); Francoeur et al., [2020](#bib.bib99)), the number
    of labeled molecular data over certain critical biochemical properties like toxicity (Gayvert
    et al., [2016](#bib.bib111)) and solubility (Delaney, [2004](#bib.bib68)) remain
    very limited. On the other hand, existing models have heavily relied on expert-crafted
    metrics to evaluate the quality of the generated molecules, such as QED and Vina (Eberhardt
    et al., [2021](#bib.bib83)), rather than actual wet lab experiments.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战与局限性。尽管有大量未标记的分子结构和几何数据（Irwin 等，[2012](#bib.bib156); Spackman 等，[2016](#bib.bib332);
    Francoeur 等，[2020](#bib.bib99)），但在某些关键生化特性如毒性（Gayvert 等，[2016](#bib.bib111)）和溶解度（Delaney，[2004](#bib.bib68)）上，标记数据仍然非常有限。另一方面，现有模型在评估生成分子质量时主要依赖于专家制定的指标，如
    QED 和 Vina（Eberhardt 等，[2021](#bib.bib83)），而非实际的湿实验。
- en: Future Works. Besides the structural and geometric attributes described in this
    chapter, an even more extensive array of data can be applied to aid molecular
    generation, including chemical reactions and medical ontology. These data can
    be organized into a heterogeneous knowledge graph to aid the extraction of high-quality
    molecular representations. Furthermore, high throughput experimentation (HTE)
    should be adopted to realistically evaluate the synthesizablity and druggability
    of the generated molecules in the wet lab. Concrete case studies, such as the
    design of potential inhibitors to SARS-CoV-2 (Li et al., [2021b](#bib.bib219)),
    would be even more encouraging, bringing new insights into leveraging these molecular
    generative models to facilitate the design and fabrication of potent and applicable
    drug molecules in the pharmaceutical industry.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 未来工作。除了本章描述的结构和几何属性外，还可以应用更广泛的数据来辅助分子生成，包括化学反应和医学本体。这些数据可以组织成一个异构知识图谱，以帮助提取高质量的分子表示。此外，应采用高通量实验
    (HTE) 来现实地评估生成分子的合成性和药物性。在药物行业中，具体的案例研究，例如针对 SARS-CoV-2 的潜在抑制剂设计 (Li et al., [2021b](#bib.bib219))，将更具鼓舞性，带来新的见解，利用这些分子生成模型促进有效且适用药物分子的设计和制造。
- en: 13\. Recommender Systems By Yifang Qin
  id: totrans-776
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13\. 推荐系统 作者：Yifang Qin
- en: The use of graph representation learning in recommender systems has been drawing
    increasing attention as one of the key strategies for addressing the issue of
    information overload. With their strong ability to capture high-order connectivity
    between graph nodes, deep graph representation learning has been shown to be beneficial
    in enhancing recommendation performance across a variety of recommendation scenarios.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 图表示学习在推荐系统中的应用受到越来越多的关注，成为解决信息过载问题的关键策略之一。由于其强大的能力来捕捉图节点之间的高阶连接，深度图表示学习已被证明有助于提升各种推荐场景中的推荐性能。
- en: 'Typical recommender systems take the observed interactions between users and
    items and their fixed features as input, and are intended for making proper predictions
    on which items a specific user is probably interested in. To formulate, given
    an user set $\mathcal{U}$, an item set $\mathcal{I}$ and the interaction matrix
    between users and items $X\in\{0,1\}^{\left|\mathcal{U}\right|\times\left|\mathcal{I}\right|}$,
    where $X_{u,v}$ indicates there is an observed interaction between user $u$ and
    item $i$. The target of GNNs on recommender systems is to learn representations
    $h_{u},h_{i}\in\mathbb{R}^{d}$ for given $u$ and $i$. The pereference score can
    further be calculated by a similarity function:'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的推荐系统将用户和项目之间的观察交互及其固定特征作为输入，旨在对特定用户可能感兴趣的项目进行适当预测。具体而言，给定用户集 $\mathcal{U}$、项目集
    $\mathcal{I}$ 以及用户和项目之间的交互矩阵 $X\in\{0,1\}^{\left|\mathcal{U}\right|\times\left|\mathcal{I}\right|}$，其中
    $X_{u,v}$ 表示用户 $u$ 和项目 $i$ 之间存在观察到的交互。GNN 在推荐系统中的目标是学习给定 $u$ 和 $i$ 的表示 $h_{u},h_{i}\in\mathbb{R}^{d}$。偏好评分可以进一步通过相似度函数计算：
- en: '| (136) |  | $\hat{x}_{u,i}=f(h_{u},h_{i}),$ |  |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '| (136) |  | $\hat{x}_{u,i}=f(h_{u},h_{i}),$ |  |'
- en: where $f(\cdot,\cdot)$ is the similarity function, e.g. inner product, consine
    similarity, multi-layer perceptrons that takes the representation of $u$ and $i$
    and calculate the pereference score $\hat{x}_{u,i}$.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(\cdot,\cdot)$ 是相似度函数，例如内积、余弦相似度、多层感知器，它接收 $u$ 和 $i$ 的表示并计算偏好评分 $\hat{x}_{u,i}$。
- en: When it comes to adapting graph representation learning in recommender systems,
    a key step is to construct graph-structured data from the interaction set $X$.
    Generally, a graph is represented as $\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$
    where $\mathcal{V},\mathcal{E}$ denotes the set of vertices and edges respectively.
    According the construction of $\mathcal{G}$, we can categorize the existing works
    as follows into three parts which are introduced in following subsections. A summary
    is provided in Table [11](#S13.T11 "Table 11 ‣ 13\. Recommender Systems By Yifang
    Qin ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 在将图表示学习应用于推荐系统时，一个关键步骤是从交互集 $X$ 中构建图结构化数据。通常，图被表示为 $\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$，其中
    $\mathcal{V}$ 和 $\mathcal{E}$ 分别表示顶点集合和边集合。根据 $\mathcal{G}$ 的构建方式，我们可以将现有工作分为以下三部分，详细介绍见以下小节。总结见表
    [11](#S13.T11 "Table 11 ‣ 13\. Recommender Systems By Yifang Qin ‣ A Comprehensive
    Survey on Deep Graph Representation Learning")。
- en: Table 11\. Summary of graph models for recommender systems.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11\. 推荐系统图模型汇总。
- en: '| Model | Recommendation Task | Graph Structure | Graph Encoder | Representation
    |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 推荐任务 | 图结构 | 图编码器 | 表示 |'
- en: '| GC-MC (Berg et al., [2017](#bib.bib21)) | Matrix Completion | User-Item Graph
    | GCN | Last-Layer |'
  id: totrans-784
  prefs: []
  type: TYPE_TB
  zh: '| GC-MC (Berg et al., [2017](#bib.bib21)) | 矩阵补全 | 用户-物品图 | GCN | 最后一层 |'
- en: '| NGCF (Wang et al., [2019a](#bib.bib368)) | Collaborative Filtering | User-Item
    Graph | GCN+Affinity | Concatenation |'
  id: totrans-785
  prefs: []
  type: TYPE_TB
  zh: '| NGCF (Wang et al., [2019a](#bib.bib368)) | 协同过滤 | 用户-物品图 | GCN+Affinity |
    拼接 |'
- en: '| MMGCN (Wei et al., [2019](#bib.bib378)) | Micro-Video | Multi-Modal Graph
    | GCN | Last-Layer |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '| MMGCN (Wei et al., [2019](#bib.bib378)) | 微视频 | 多模态图 | GCN | 最后一层 |'
- en: '| LightGCN (He et al., [2020](#bib.bib135)) | Collaborative Filtering | User-Item
    Graph | LGC | Mean-Pooling |'
  id: totrans-787
  prefs: []
  type: TYPE_TB
  zh: '| LightGCN (He et al., [2020](#bib.bib135)) | 协同过滤 | 用户-物品图 | LGC | 平均池化 |'
- en: '| DGCF (Wang et al., [2020b](#bib.bib370)) | Collaborative Filtering | User-Item
    Graph | Dynamic Routing | Mean-Pooling |'
  id: totrans-788
  prefs: []
  type: TYPE_TB
  zh: '| DGCF (Wang et al., [2020b](#bib.bib370)) | 协同过滤 | 用户-物品图 | 动态路由 | 平均池化 |'
- en: '| SR-GNN (Wu et al., [2019c](#bib.bib386)) | Session-based | Transition Graph
    | GGNN | Soft-Attention |'
  id: totrans-789
  prefs: []
  type: TYPE_TB
  zh: '| SR-GNN (Wu et al., [2019c](#bib.bib386)) | 会话型 | 过渡图 | GGNN | 软注意力 |'
- en: '| GC-SAN (Wu et al., [2019c](#bib.bib386); Xu et al., [2019b](#bib.bib400))
    | Session-based | Session Graph | GGNN | Self-Attention |'
  id: totrans-790
  prefs: []
  type: TYPE_TB
  zh: '| GC-SAN (Wu et al., [2019c](#bib.bib386); Xu et al., [2019b](#bib.bib400))
    | 会话型 | 会话图 | GGNN | 自注意力 |'
- en: '| FGNN (Qiu et al., [2019](#bib.bib288)) | Session-based | Session Graph |
    GAT | Last-Layer |'
  id: totrans-791
  prefs: []
  type: TYPE_TB
  zh: '| FGNN (Qiu et al., [2019](#bib.bib288)) | 会话型 | 会话图 | GAT | 最后一层 |'
- en: '| GAG (Qiu et al., [2020b](#bib.bib289)) | Session-based | Session Graph |
    GCN | Self-Attention |'
  id: totrans-792
  prefs: []
  type: TYPE_TB
  zh: '| GAG (Qiu et al., [2020b](#bib.bib289)) | 会话型 | 会话图 | GCN | 自注意力 |'
- en: '| GCE-GNN (Wang et al., [2020d](#bib.bib376)) | Session-based | Transition+Global
    | GAT | Sum-Pooling |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
  zh: '| GCE-GNN (Wang et al., [2020d](#bib.bib376)) | 会话型 | 过渡+全局 | GAT | 求和池化 |'
- en: '| HyperRec (Wang et al., [2020a](#bib.bib362)) | Sequence-based | Sequential
    HyperGraph | HGCN | Self-Attention |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
  zh: '| HyperRec (Wang et al., [2020a](#bib.bib362)) | 序列型 | 序列超图 | HGCN | 自注意力 |'
- en: '| DHCF (Ji et al., [2020](#bib.bib159)) | Collaborative Filtering | Dual HyperGraph
    | JHConv | Last-Layer |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
  zh: '| DHCF (Ji et al., [2020](#bib.bib159)) | 协同过滤 | 双重超图 | JHConv | 最后一层 |'
- en: '| MBHT (Yang et al., [2022b](#bib.bib412)) | Sequence-based | Learnable HyperGraph
    | Transformer | Cross-View Attention |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
  zh: '| MBHT (Yang et al., [2022b](#bib.bib412)) | 序列型 | 可学习超图 | Transformer | 交叉视图注意力
    |'
- en: '| HCCF (Xia et al., [2022a](#bib.bib393)) | Collaborative Filtering | Learnable
    HyperGraph | HGCN | Last-Layer |'
  id: totrans-797
  prefs: []
  type: TYPE_TB
  zh: '| HCCF (Xia et al., [2022a](#bib.bib393)) | 协同过滤 | 可学习超图 | HGCN | 最后一层 |'
- en: 13.1\. User-Item Bipartite Graph
  id: totrans-798
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1\. 用户-物品二部图
- en: 13.1.1\. Graph Construction
  id: totrans-799
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.1\. 图构建
- en: A undirected bipartite graph where the vertex set $\mathcal{V}=\mathcal{U}\cup\mathcal{I}$
    and the undirected edge set $\mathcal{E}=\{(u,i)|u\in\mathcal{U}\land i\in\mathcal{I}\}$.
    Under this case the graph adjacency can be directly obtained from the interaction
    matrix, thus the optimization target on the user-item bipartite graph is equivalent
    to collaborative filtering tasks such as MF (Koren et al., [2009](#bib.bib192))
    and SVD++ (Koren, [2008](#bib.bib191)).
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 一个无向二部图，其中顶点集 $\mathcal{V}=\mathcal{U}\cup\mathcal{I}$ 和无向边集 $\mathcal{E}=\{(u,i)|u\in\mathcal{U}\land
    i\in\mathcal{I}\}$。在这种情况下，图的邻接矩阵可以直接从交互矩阵中获得，因此用户-物品二部图上的优化目标等同于协同过滤任务，如MF (Koren
    et al., [2009](#bib.bib192)) 和 SVD++ (Koren, [2008](#bib.bib191))。
- en: There have been plenty of previous works that applied GNNs on the constructed
    user-item bipartite graphs. GC-MC (Berg et al., [2017](#bib.bib21)) firstly applies
    graph convolution networks to user-item recommendation and optimizes a graph autoencoder
    (GAE) to reconstruct interactions between users and items. NGCF (Wang et al.,
    [2019a](#bib.bib368)) introduces the concept of Collaborative Filtering (CF) into
    graph-based recommendations by modeling the affinity between neighboring nodes
    on the interaction graph. MMGCN (Wei et al., [2019](#bib.bib378)) extend graph-based
    recommendation to multi-modal scenarios by constructing different subgraphs for
    each modal. LightGCN (He et al., [2020](#bib.bib135)) improves NGCF by removing
    the non-linear activation functions and simplifying the message function. With
    the development of disentangled representation learning, there are works like
    DGCF (Wang et al., [2020b](#bib.bib370)) introduce disentangled graph representation
    learning to represent users and items from multiple disentangled perspective
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 之前已经有很多研究将 GNNs 应用到构建的用户-物品二分图上。GC-MC（Berg 等，[2017](#bib.bib21)）首次将图卷积网络应用于用户-物品推荐，并优化了图自编码器（GAE）以重建用户与物品之间的交互。NGCF（Wang
    等，[2019a](#bib.bib368)）通过建模交互图上邻近节点之间的亲和力，将协同过滤（CF）的概念引入基于图的推荐。MMGCN（Wei 等，[2019](#bib.bib378)）通过为每种模态构建不同的子图，将基于图的推荐扩展到多模态场景。LightGCN（He
    等，[2020](#bib.bib135)）通过去除非线性激活函数和简化消息函数来改进 NGCF。随着解缠结表示学习的发展，DGCF（Wang 等，[2020b](#bib.bib370)）等研究引入了解缠结图表示学习，以从多个解缠结的视角表示用户和物品。
- en: 13.1.2\. Graph Propagation Scheme
  id: totrans-802
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.2\. 图传播方案
- en: 'A common practice is to follow the traditional message-passing networks (MPNNs)
    and design the graph propagation method accordingly. GC-MC adopt vanilla GCNs
    to encode the user-item bipartite graph. NGCF enhance GCNs by considering the
    affinity between users and items. The message function of NGCF from node $j$ to
    $i$ is formulated as:'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的做法是遵循传统的消息传递网络（MPNNs）并相应地设计图传播方法。GC-MC 采用原始 GCNs 来编码用户-物品二分图。NGCF 通过考虑用户和物品之间的亲和力来增强
    GCNs。NGCF 从节点 $j$ 到 $i$ 的消息函数被表述为：
- en: '| (137) |  | $\begin{cases}m_{i\leftarrow j}=\frac{1}{\sqrt{&#124;\mathcal{N}_{i}&#124;&#124;\mathcal{N}_{j}&#124;}}(W_{1}e_{j}+W_{2}(e_{i}\odot
    e_{j}))\\ m_{i\leftarrow i}=W_{1}e_{i}\end{cases},$ |  |'
  id: totrans-804
  prefs: []
  type: TYPE_TB
  zh: '| (137) |  | $\begin{cases}m_{i\leftarrow j}=\frac{1}{\sqrt{&#124;\mathcal{N}_{i}&#124;&#124;\mathcal{N}_{j}&#124;}}(W_{1}e_{j}+W_{2}(e_{i}\odot
    e_{j}))\\ m_{i\leftarrow i}=W_{1}e_{i}\end{cases},$ |  |'
- en: 'where $W_{1},W_{2}$ are trainable parameters, $e_{i}$ represents $i$’s representation
    from previous layer. The matrix form can be further provided by:'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{1},W_{2}$ 是可训练参数，$e_{i}$ 代表来自前一层的 $i$ 的表示。矩阵形式可以进一步提供为：
- en: '| (138) |  | $E^{(l)}=\text{LeakyReLU}((\mathcal{L}+I)E^{(l-1)}W_{1}^{(l)}+\mathcal{L}E^{(l-1)}\odot
    E^{(l-1)}W_{2}^{(l)}),$ |  |'
  id: totrans-806
  prefs: []
  type: TYPE_TB
  zh: '| (138) |  | $E^{(l)}=\text{LeakyReLU}((\mathcal{L}+I)E^{(l-1)}W_{1}^{(l)}+\mathcal{L}E^{(l-1)}\odot
    E^{(l-1)}W_{2}^{(l)}),$ |  |'
- en: where $\mathcal{L}$ represents the Laplacian matrix of the user-item graph.
    The element-wise product in Eq. [138](#S13.E138 "In 13.1.2\. Graph Propagation
    Scheme ‣ 13.1\. User-Item Bipartite Graph ‣ 13\. Recommender Systems By Yifang
    Qin ‣ A Comprehensive Survey on Deep Graph Representation Learning") represents
    the affinity between connected nodes, containing the collaborative signals from
    interactions.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}$ 代表用户-物品图的拉普拉斯矩阵。方程 [138](#S13.E138 "In 13.1.2\. Graph Propagation
    Scheme ‣ 13.1\. User-Item Bipartite Graph ‣ 13\. Recommender Systems By Yifang
    Qin ‣ A Comprehensive Survey on Deep Graph Representation Learning") 中的元素乘积表示连接节点之间的亲和力，包含来自交互的协同信号。
- en: 'However, the notably heaviness and burdensome calculation of NGCF’s architecture
    hinders the model from making faster recommendations on larger graphs. LightGCN
    solves this issue by proposing Light Graph Convolution (LGC), which simplifies
    the convolution operation with:'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，NGCF 架构的显著重量和繁重计算阻碍了模型在较大图上进行更快的推荐。LightGCN 通过提出 Light Graph Convolution
    (LGC) 解决了这个问题，LGC 简化了卷积操作：
- en: '| (139) |  | $e_{i}^{(l+1)}=\sum_{j\in\mathcal{N}_{i}}\frac{1}{\sqrt{&#124;\mathcal{N}_{i}&#124;&#124;\mathcal{N}_{j}&#124;}}e_{j}^{(l)}.$
    |  |'
  id: totrans-809
  prefs: []
  type: TYPE_TB
  zh: '| (139) |  | $e_{i}^{(l+1)}=\sum_{j\in\mathcal{N}_{i}}\frac{1}{\sqrt{&#124;\mathcal{N}_{i}&#124;&#124;\mathcal{N}_{j}&#124;}}e_{j}^{(l)}.$
    |  |'
- en: 'When an interaction takes place, e.g. a user clickes a particular item, there
    could be multiple intentions behind the observed interaction. Thus it is necessary
    to consider the various disentangled intentions among users and items. DGCF proposes
    the cross-intent embedding propagation scheme on graph, inspired by the dynamic
    routing algorithm of capsule netwroks (Sabour et al., [2017](#bib.bib303)). To
    formulate, the propagation process maintains a set of routing logits $\tilde{S}_{k}(u,i)$
    for each user $u$. The weighted sum aggregator to get the representation of $u$
    can be defined as:'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生交互时，例如用户点击某个特定项时，观察到的交互可能背后有多种意图。因此，有必要考虑用户和项目之间各种解耦的意图。DGCF 提出了图上的跨意图嵌入传播方案，灵感来自胶囊网络的动态路由算法（Sabour
    等，[2017](#bib.bib303)）。为公式化，传播过程为每个用户 $u$ 维护一组路由 logits $\tilde{S}_{k}(u,i)$。加权求和聚合器用于获取
    $u$ 的表示，可以定义为：
- en: '| (140) |  | $u_{k}^{t}=\sum_{i\in\mathcal{N}_{u}}\mathcal{L}_{k}^{t}(u,i)\cdot
    i_{k}^{0}$ |  |'
  id: totrans-811
  prefs: []
  type: TYPE_TB
  zh: '| (140) |  | $u_{k}^{t}=\sum_{i\in\mathcal{N}_{u}}\mathcal{L}_{k}^{t}(u,i)\cdot
    i_{k}^{0}$ |  |'
- en: 'for $t$-th iteration, where $\mathcal{L}_{k}^{t}(u,i)$ denotes the Laplacian
    matrix of $S_{k}^{t}(u,i)$, formulated as:'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $t$-th 迭代，其中 $\mathcal{L}_{k}^{t}(u,i)$ 表示 $S_{k}^{t}(u,i)$ 的拉普拉斯矩阵，公式化为：
- en: '| (141) |  | $\mathcal{L}_{k}^{t}(u,i)=\frac{S_{k}^{t}}{\sqrt{[\sum_{i^{\prime}\in\mathcal{N}_{u}}S_{k}^{t}(u,i^{\prime})]\cdot[\sum_{u^{\prime}\in\mathcal{N}_{i}}S_{k}^{t}(u^{\prime},i)]}}.$
    |  |'
  id: totrans-813
  prefs: []
  type: TYPE_TB
  zh: '| (141) |  | $\mathcal{L}_{k}^{t}(u,i)=\frac{S_{k}^{t}}{\sqrt{[\sum_{i^{\prime}\in\mathcal{N}_{u}}S_{k}^{t}(u,i^{\prime})]\cdot[\sum_{u^{\prime}\in\mathcal{N}_{i}}S_{k}^{t}(u^{\prime},i)]}}.$
    |  |'
- en: 13.1.3\. Node Representations
  id: totrans-814
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.3\. 节点表示
- en: 'After the graph propagation module outputs node-level representations, there
    are multiple methods to leverage node representations for recommendation tasks.
    A plain solution is to apply a readout function on layer outputs like the concatenation
    operation used by NGCF:'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 在图传播模块输出节点级表示后，有多种方法可以利用节点表示进行推荐任务。一种简单的解决方案是对层输出应用读取函数，例如 NGCF 使用的拼接操作：
- en: '| (142) |  | $e^{*}=Concat(e^{(0)},...,e^{(L)})=e^{(0)}\&#124;...\&#124;e^{(L)}.$
    |  |'
  id: totrans-816
  prefs: []
  type: TYPE_TB
  zh: '| (142) |  | $e^{*}=Concat(e^{(0)},...,e^{(L)})=e^{(0)}\&#124;...\&#124;e^{(L)}.$
    |  |'
- en: 'However, the readout function among layers would neglects the relationship
    between target item and current user. A general solution is to use attention mechanism
    (Vaswani et al., [2017](#bib.bib351)) to reweight and aggregate the node representations.
    SR-GNN adapts soft-attention mechanism to model the item-item relationship:'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，层间的读取函数可能会忽略目标项和当前用户之间的关系。一种通用的解决方案是使用注意力机制（Vaswani 等，[2017](#bib.bib351)）来重新加权和聚合节点表示。SR-GNN
    采用软注意力机制来建模项-项关系：
- en: '| (143) |  | $\begin{split}\alpha_{i}&amp;=\textbf{q}^{T}\sigma(W_{1}e_{t}+W_{2}e_{i}+c),\\
    s_{g}&amp;=\sum_{i=1}^{n-1}\alpha_{i}e_{i},\end{split}$ |  |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
  zh: '| (143) |  | $\begin{split}\alpha_{i}&amp;=\textbf{q}^{T}\sigma(W_{1}e_{t}+W_{2}e_{i}+c),\\
    s_{g}&amp;=\sum_{i=1}^{n-1}\alpha_{i}e_{i},\end{split}$ |  |'
- en: where $\textbf{q},\ W_{1},\ W_{2}$ are trainable matrices.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textbf{q},\ W_{1},\ W_{2}$ 是可训练矩阵。
- en: 'Some methods focus on exploiting information from multiple graph structures.
    A common practice is contrastive learning, which maximize the mutual information
    between hidden representations from several views. HCCF leverage InfoNCE loss
    as the estimator of mutual information, given a pair of representation $z_{i},\Gamma_{i}$
    for node $i$, controlled by temperature parameter $\tau$:'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法集中在利用多个图结构的信息。一个常见的做法是对比学习，它最大化来自多个视角的隐藏表示之间的互信息。HCCF 利用 InfoNCE 损失作为互信息的估计器，给定节点
    $i$ 的一对表示 $z_{i},\Gamma_{i}$，由温度参数 $\tau$ 控制：
- en: '| (144) |  | $\mathcal{L}_{InfoNCE}(i)=-\log\frac{\exp(cosine(z_{i},\Gamma_{i}))/\tau}{\sum_{i^{\prime}\neq
    i}\exp(cosine(z_{i},\Gamma_{i^{\prime}}))/\tau}.$ |  |'
  id: totrans-821
  prefs: []
  type: TYPE_TB
  zh: '| (144) |  | $\mathcal{L}_{InfoNCE}(i)=-\log\frac{\exp(cosine(z_{i},\Gamma_{i}))/\tau}{\sum_{i^{\prime}\neq
    i}\exp(cosine(z_{i},\Gamma_{i^{\prime}}))/\tau}.$ |  |'
- en: Besides InfoNCE, there exists several other ways to combine node representations
    from different views. For instance, MBHT applies attention mechanism to fuse multiple
    semantics, DisenPOI adapt bayesian personalized ranking loss (BPR) (Rendle et al.,
    [2012](#bib.bib294)) as a soft estimator for contrastive learning, and KBGNN applies
    pair-wise similarities to ensure the consistency from two views.
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 InfoNCE，还有其他几种方法可以结合来自不同视角的节点表示。例如，MBHT 应用注意力机制融合多种语义，DisenPOI 采用贝叶斯个性化排序损失（BPR）（Rendle
    等，[2012](#bib.bib294)）作为对比学习的软估计器，而 KBGNN 应用成对相似性以确保两个视角的一致性。
- en: 13.2\. Transition Graph
  id: totrans-823
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2\. 转换图
- en: 13.2.1\. Transition Graph Construction
  id: totrans-824
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.2.1\. 转换图构建
- en: Since sequence-based recommendation (SR) is one of the fundamental problems
    in recomemnder systems, some researches focus on modeling the sequential information
    with GNNs. A commonly applied way is to construct transition graphs based on each
    given sequence according to the clicking sequence by a user. To formulate, given
    a user $u$’s clicking sequence $s_{u}=[i_{u,1},i_{u,2},...,i_{u,n}]$ containing
    $n$ items, noting that there could be duplicated items, the sequential graph is
    constructed via $\mathcal{G}_{s}=\{\text{SET}(s_{u}),\mathcal{E}\}$, where $\forall\left<i_{j},i_{k}\right>\in\mathcal{E}$
    indicates there exists a successive transition from $i_{j}$ to $i_{k}$. Since
    $\mathcal{G}_{s}$ are directed graphs, a widely used way to depict graph connectivity
    is by building the connection matrix $A_{s}\in\mathbb{R}^{n\times 2n}$. $A_{s}$
    is the combination of two adjacency matrices $A_{s}=[A_{s}^{(in)};A_{s}^{(out)}]$,
    which denotes the normalized node degrees of incoming and outgoing edges in the
    session graph respectively.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于序列的推荐（SR）是推荐系统中的一个基础问题，一些研究集中于用 GNNs 建模序列信息。一种常见的方法是根据用户的点击序列构建过渡图。具体来说，给定用户
    $u$ 的点击序列 $s_{u}=[i_{u,1},i_{u,2},...,i_{u,n}]$ 包含 $n$ 项，注意到可能有重复项，序列图通过 $\mathcal{G}_{s}=\{\text{SET}(s_{u}),\mathcal{E}\}$
    构建，其中 $\forall\left<i_{j},i_{k}\right>\in\mathcal{E}$ 表示存在从 $i_{j}$ 到 $i_{k}$
    的连续过渡。由于 $\mathcal{G}_{s}$ 是有向图，广泛使用的表示图连接性的方法是构建连接矩阵 $A_{s}\in\mathbb{R}^{n\times
    2n}$。$A_{s}$ 是两个邻接矩阵 $A_{s}=[A_{s}^{(in)};A_{s}^{(out)}]$ 的组合，分别表示会话图中入边和出边的归一化节点度数。
- en: The proposed transition graphs that obtain user behavior patterns has been demonstrated
    important to session-based recommendations (Li et al., [2017b](#bib.bib211); Liu
    et al., [2018d](#bib.bib233)). SR-GNN and GC-SAN (Wu et al., [2019c](#bib.bib386);
    Xu et al., [2019b](#bib.bib400)) propose to leverage transition graphs and applies
    attention-based GNNs to capture the sequential information for session-based recommendation.
    FGNN (Qiu et al., [2019](#bib.bib288)) formulates the recommendation within a
    session as a graph classification problem to predict next item for an anonymous
    user. GAG (Qiu et al., [2020b](#bib.bib289)) and GCE-GNN (Wang et al., [2020d](#bib.bib376))
    further extends the model to capture global embeddings among multiple session
    graphs.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的过渡图在获取用户行为模式方面被证明对基于会话的推荐非常重要（Li et al., [2017b](#bib.bib211); Liu et al.,
    [2018d](#bib.bib233)）。SR-GNN 和 GC-SAN（Wu et al., [2019c](#bib.bib386); Xu et al.,
    [2019b](#bib.bib400)）提议利用过渡图，并应用基于注意力的 GNNs 捕捉会话推荐的序列信息。FGNN（Qiu et al., [2019](#bib.bib288)）将会话内的推荐形式化为图分类问题，以预测匿名用户的下一个项目。GAG（Qiu
    et al., [2020b](#bib.bib289)）和 GCE-GNN（Wang et al., [2020d](#bib.bib376)）进一步扩展了模型，以捕捉多个会话图之间的全局嵌入。
- en: 13.2.2\. Session Graph Propagation
  id: totrans-827
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.2.2\. 会话图传播
- en: Since the session graphs are directed item graphs, there have been multiple
    session graph propagation methods to obtain node representations on session graphs.
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 由于会话图是有向的项目图，因此已有多种会话图传播方法用于获取会话图上的节点表示。
- en: 'SR-GNN leverages Gated Graph Neural Networks (GGNNs) to obtain sequential information
    from a given session graph adjacency $A_{s}=[A_{s}^{(in)};A_{s}^{(out)}]$ and
    item embedding set $\{e_{i}\}$:'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: SR-GNN 利用门控图神经网络（GGNNs）从给定的会话图邻接矩阵 $A_{s}=[A_{s}^{(in)};A_{s}^{(out)}]$ 和项目嵌入集
    $\{e_{i}\}$ 中获取序列信息：
- en: '| (145) |  | $\displaystyle a_{t}$ | $\displaystyle=A_{s}[e_{1},...,e_{t-1}]^{T}H+b,$
    |  |'
  id: totrans-830
  prefs: []
  type: TYPE_TB
  zh: '| (145) |  | $\displaystyle a_{t}$ | $\displaystyle=A_{s}[e_{1},...,e_{t-1}]^{T}H+b,$
    |  |'
- en: '| (146) |  | $\displaystyle z_{t}$ | $\displaystyle=\sigma(W_{z}a_{t}+U_{z}e_{t-1}),$
    |  |'
  id: totrans-831
  prefs: []
  type: TYPE_TB
  zh: '| (146) |  | $\displaystyle z_{t}$ | $\displaystyle=\sigma(W_{z}a_{t}+U_{z}e_{t-1}),$
    |  |'
- en: '| (147) |  | $\displaystyle r_{t}$ | $\displaystyle=\sigma(W_{r}a_{t}+U_{r}e_{t-1}),$
    |  |'
  id: totrans-832
  prefs: []
  type: TYPE_TB
  zh: '| (147) |  | $\displaystyle r_{t}$ | $\displaystyle=\sigma(W_{r}a_{t}+U_{r}e_{t-1}),$
    |  |'
- en: '| (148) |  | $\displaystyle\tilde{e_{t}}$ | $\displaystyle=\tanh(W_{o}a_{t}+U_{o}(r_{t}\odot
    e_{t-1})),$ |  |'
  id: totrans-833
  prefs: []
  type: TYPE_TB
  zh: '| (148) |  | $\displaystyle\tilde{e_{t}}$ | $\displaystyle=\tanh(W_{o}a_{t}+U_{o}(r_{t}\odot
    e_{t-1})),$ |  |'
- en: '| (149) |  | $\displaystyle e_{t}$ | $\displaystyle=(1-z_{t})\odot e_{t-1}+z_{t}\tilde{e_{t}},$
    |  |'
  id: totrans-834
  prefs: []
  type: TYPE_TB
  zh: '| (149) |  | $\displaystyle e_{t}$ | $\displaystyle=(1-z_{t})\odot e_{t-1}+z_{t}\tilde{e_{t}},$
    |  |'
- en: 'where $W$s and $U$s are trainable parameters. GC-SAN extend GGNN by calculating
    initial state $a_{t}$ separately to better exploit transition information:'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W$s 和 $U$s 是可训练的参数。GC-SAN 通过单独计算初始状态 $a_{t}$ 来扩展 GGNN，以更好地利用过渡信息：
- en: '| (150) |  | $a_{t}=Concat(A_{s}^{(in)}([e_{1},...,e_{t-1}W_{a}^{(in)}]+b^{(in)}),A_{s}^{(out)}([e_{1},...,e_{t-1}W_{a}^{(out)}]+b^{(out)})).$
    |  |'
  id: totrans-836
  prefs: []
  type: TYPE_TB
  zh: '| (150) |  | $a_{t}=Concat(A_{s}^{(in)}([e_{1},...,e_{t-1}W_{a}^{(in)}]+b^{(in)}),A_{s}^{(out)}([e_{1},...,e_{t-1}W_{a}^{(out)}]+b^{(out)})).$
    |  |'
- en: 13.3\. HyperGraph
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3\. 超图
- en: 13.3.1\. Hypergraph Topology Construction
  id: totrans-838
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.3.1\. 超图拓扑构建
- en: 'Motivated by the idea of modeling hyper-structures and high-order correlation
    among nodes, hypergraphs (Feng et al., [2019](#bib.bib94)) are proposed as extentions
    of the commonly used graph structures. For graph-based recommender systems, a
    common practice is to construct hyper structures among the original user-item
    bipartite graphs. To be specific, an incidence matrix of a graph with vertex set
    $\mathcal{V}$ is presented as a binary matrix $H\in\{0,1\}^{|\mathcal{V}|\times|\mathcal{E}|}$,
    where $\mathcal{E}$ represents the set of hyperedges. Each entry $h(v,e)$ of $H$
    depicts the connectivity between vertex $v$ and hyperedge $e$:'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 受到建模超结构和节点间高阶相关性思想的启发，超图（Feng等，[2019](#bib.bib94)）被提出作为常用图结构的扩展。对于基于图的推荐系统，常见的做法是构建原始用户-项目二分图中的超结构。具体来说，一个具有顶点集$\mathcal{V}$的图的关联矩阵表示为一个二进制矩阵$H\in\{0,1\}^{|\mathcal{V}|\times|\mathcal{E}|}$，其中$\mathcal{E}$表示超边的集合。矩阵$H$的每个条目$h(v,e)$描述了顶点$v$与超边$e$之间的连接性：
- en: '| (151) |  | $h(v,e)=\begin{cases}1\ if\ v\in e\\ 0\ if\ v\notin e\end{cases}.$
    |  |'
  id: totrans-840
  prefs: []
  type: TYPE_TB
  zh: '| (151) |  | $h(v,e)=\begin{cases}1\ 如果\ v\in e\\ 0\ 如果\ v\notin e\end{cases}.$
    |  |'
- en: Given the formulation of hypergraphs, the degrees of vertices and hyperedges
    of $H$ can then be defined with two diagnal matrices $D_{v}\in\mathbb{N}^{|\mathcal{V}|\times|\mathcal{V}|}$
    and $D_{e}\in\mathbb{N}^{|\mathcal{E}|\times|\mathcal{E}|}$, where
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 给定超图的公式，$H$的顶点和超边的度数可以通过两个对角矩阵$D_{v}\in\mathbb{N}^{|\mathcal{V}|\times|\mathcal{V}|}$和$D_{e}\in\mathbb{N}^{|\mathcal{E}|\times|\mathcal{E}|}$来定义，其中
- en: '| (152) |  | $D_{v}(i;i)=\sum_{e\in\mathcal{E}}h(v_{i},e),\ \ \ \ D_{e}(j;j)=\sum_{v\in\mathcal{V}}h(v,e_{j}).$
    |  |'
  id: totrans-842
  prefs: []
  type: TYPE_TB
  zh: '| (152) |  | $D_{v}(i;i)=\sum_{e\in\mathcal{E}}h(v_{i},e),\ \ \ \ D_{e}(j;j)=\sum_{v\in\mathcal{V}}h(v,e_{j}).$
    |  |'
- en: With the development of Hypergraph Neural Networks (HGNNs) (Feng et al., [2019](#bib.bib94);
    Zhou et al., [2006](#bib.bib461); Huang et al., [2015](#bib.bib152)) have shown
    to be capable of capturing the high-order connectivity between nodes. HyperRec
    (Wang et al., [2020a](#bib.bib362)) firstly attempts to leverage hypergraph structures
    for sequential recommendation by connecting items with hyperedges according to
    the interactions with users during different time periods. DHCF (Ji et al., [2020](#bib.bib159))
    proposes to construct hypergraphs for users and items respectively based on certain
    rules, to explicit capture the collaborative similarities via HGNNs. MBHT (Yang
    et al., [2022b](#bib.bib412)) combines hypergraphs with low-rank self-attention
    mechanism to capture the dynamic heterogeneous relationships between users and
    items. HCCF (Xia et al., [2022a](#bib.bib393)) uses the contrastive information
    between hypergraph and interaction graph to enhance the recommendation performance.
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 随着超图神经网络（HGNNs）的发展（Feng等， [2019](#bib.bib94)；Zhou等，[2006](#bib.bib461)；Huang等，[2015](#bib.bib152)）已显示能够捕捉节点之间的高阶连接性。HyperRec（Wang等，[2020a](#bib.bib362)）首次尝试利用超图结构进行序列推荐，通过根据用户在不同时间段的交互来连接项目与超边。DHCF（Ji等，[2020](#bib.bib159)）提出基于某些规则分别为用户和项目构建超图，以通过HGNNs显式捕捉协作相似性。MBHT（Yang等，[2022b](#bib.bib412)）将超图与低秩自注意力机制相结合，以捕捉用户和项目之间的动态异质关系。HCCF（Xia等，[2022a](#bib.bib393)）利用超图与交互图之间的对比信息来增强推荐性能。
- en: 13.3.2\. Hyper Graph Message Passing
  id: totrans-844
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.3.2\. 超图消息传递
- en: 'With the development of HGNNs, previous works have propose different variants
    of HGNN to better exploit hypergraph structures. A classic high-order hyper convolution
    process on a fixed hypergraph $\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$ with hyper
    adjacency $H$ is given by:'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 随着HGNNs的发展，之前的工作提出了不同的HGNN变体，以更好地利用超图结构。一个经典的高阶超卷积过程在一个固定的超图$\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$上，具有超邻接矩阵$H$，其定义为：
- en: '| (153) |  | $g\star X=D_{v}^{-1/2}HD_{e}^{-1}H^{T}D_{v}^{-1/2}X\Theta,$ |  |'
  id: totrans-846
  prefs: []
  type: TYPE_TB
  zh: '| (153) |  | $g\star X=D_{v}^{-1/2}HD_{e}^{-1}H^{T}D_{v}^{-1/2}X\Theta,$ |  |'
- en: 'where $D_{v},\ D_{e}$ are degree matrices of nodes and hyperedges, $\Theta$
    denotes the convolution kernel. For hyper adjacency matrix $H$, DHCF refers to
    a rule-based hyperstructure via k-order reachable rule, where nodes in the same
    hyperedge group are k-order reachable to each other:'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$D_{v},\ D_{e}$是节点和超边的度矩阵，$\Theta$表示卷积核。对于超邻接矩阵$H$，DHCF指的是通过k阶可达规则的基于规则的超结构，其中同一超边组中的节点是k阶可达的：
- en: '| (154) |  | $A_{u}^{k}=\min(1,\text{power}(A\cdot A^{T},k)),$ |  |'
  id: totrans-848
  prefs: []
  type: TYPE_TB
  zh: '| (154) |  | $A_{u}^{k}=\min(1,\text{power}(A\cdot A^{T},k)),$ |  |'
- en: 'where $A$ denotes the graph adjacency matrix. By considering the situations
    where $k=1,2$, the matrix formulation of the hyper connectivity of users and items
    are calculated with:'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 表示图的邻接矩阵。通过考虑 $k=1,2$ 的情况，计算了用户和项目的超连接矩阵表示：
- en: '| (155) |  | $\begin{cases}H_{u}=A\&#124;(A(A^{T}A))\\ H_{i}=A^{T}\&#124;(A^{T}(AA^{T}))\end{cases},$
    |  |'
  id: totrans-850
  prefs: []
  type: TYPE_TB
  zh: '| (155) |  | $\begin{cases}H_{u}=A\&#124;(A(A^{T}A))\\ H_{i}=A^{T}\&#124;(A^{T}(AA^{T}))\end{cases},$
    |  |'
- en: which depicts the dual hypergraphs for users and items.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 这描绘了用户和项目的双重超图。
- en: 'HCCF proposes to construct a learnable hypergraph to depict the global dependencies
    between nodes on the interaction graph. To be specific, the hyperstructure is
    factorized with two low-rank embedding matrices to achieve model efficiency:'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: HCCF 提出了构建一个可学习的超图，以描绘交互图中节点之间的全局依赖关系。具体而言，超结构通过两个低秩嵌入矩阵进行分解，以实现模型效率：
- en: '| (156) |  | $H_{u}=E_{u}\cdot W_{u},\ H_{v}=E_{v}\cdot W_{v}.$ |  |'
  id: totrans-853
  prefs: []
  type: TYPE_TB
  zh: '| (156) |  | $H_{u}=E_{u}\cdot W_{u},\ H_{v}=E_{v}\cdot W_{v}.$ |  |'
- en: 13.4\. Other Graphs
  id: totrans-854
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4\. 其他图
- en: Since there are a variety of recommendation scenarios, several tailored designed
    graph structures have been proposed accordingly, to better exploit the domain
    information from different scenarios. For instance, CKE (Zhang et al., [2016](#bib.bib434))
    and MKR (Wang et al., [2019e](#bib.bib361)) introduce Knowledge graphs to enhance
    graph recommendation. GSTN (Wang et al., [2022e](#bib.bib377)), KBGNN (Ju et al.,
    [2022c](#bib.bib174)) and DisenPOI (Qin et al., [2022](#bib.bib286)) propose to
    build geographical graphs based on the distance between Poit-of-Interests (POIs)
    to better model the locality of users’ visiting patterns. TGSRec (Fan et al.,
    [2021](#bib.bib88)) and DisenCTR (Wang et al., [2022c](#bib.bib372)) empower the
    user-item interaction graphs with temporal sampling between layers to obtain sequential
    information from static bipartite graphs.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 由于推荐场景的多样性，已经提出了几种量身定制的图结构，以更好地利用来自不同场景的领域信息。例如，CKE（Zhang 等，[2016](#bib.bib434)）和MKR（Wang
    等，[2019e](#bib.bib361)）引入了知识图谱来增强图推荐。GSTN（Wang 等，[2022e](#bib.bib377)），KBGNN（Ju
    等，[2022c](#bib.bib174)）和DisenPOI（Qin 等，[2022](#bib.bib286)）提出了基于兴趣点（POIs）之间距离的地理图，以更好地建模用户访问模式的局部性。TGSRec（Fan
    等，[2021](#bib.bib88)）和DisenCTR（Wang 等，[2022c](#bib.bib372)）通过在层间进行时间采样来增强用户-项目交互图，从静态二部图中获取序列信息。
- en: 13.5\. Summary
  id: totrans-856
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.5\. 总结
- en: 'This section introduces the application of different kinds of graph neural
    networks in recommender systems and can be summarized as follows:'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了不同类型图神经网络在推荐系统中的应用，可以总结如下：
- en: •
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Graph Constructions. There are multiple options for constructing graph-structured
    data for a variety of recommendation tasks. For instance, the user-item bipartite
    graphs reveal the high-order collaborative similarity between users and items,
    and the transition graph is suitable for encoding sequential information in clicking
    history. These diversified graph structures provide different views for node representation
    learning on users and items, and can be further used for downstream ranking tasks.
  id: totrans-859
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图构建。对于各种推荐任务，有多种选项用于构建图结构数据。例如，用户-项目二部图揭示了用户与项目之间的高阶协同相似性，而转移图适用于编码点击历史中的序列信息。这些多样化的图结构为用户和项目的节点表示学习提供了不同的视角，并且可以进一步用于下游的排名任务。
- en: •
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Challenges and Limitations. Though the superiority of graph-structured data
    and GNNs against traditional methods has been widely illustrated, there are still
    challenges unsolved. For example, the computational cost of graph methods are
    normally expensive and thus unacceptable in real-world applications. The data
    sparsity and cold-started issue in graph recommendation remains to be explored
    as well.
  id: totrans-861
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战与局限性。尽管图结构数据和GNN相比传统方法的优越性已被广泛阐明，但仍然存在未解决的挑战。例如，图方法的计算成本通常较高，因此在实际应用中不可接受。图推荐中的数据稀疏性和冷启动问题也仍待探索。
- en: •
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future Works. In the future, the efficient solution of applying GNNs in recommendation
    tasks is expected. There are also some attempts (Fan et al., [2021](#bib.bib88);
    Wang et al., [2022c](#bib.bib372)) on incorporating temporal information in graph
    representation learning for sequential recommendation tasks.
  id: totrans-863
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来工作。未来，期望在推荐任务中应用GNN的高效解决方案。此外，还有一些尝试（Fan 等，[2021](#bib.bib88)；Wang 等，[2022c](#bib.bib372)）在将时间信息融入图表示学习以进行序列推荐任务方面。
- en: 14\. Traffic Analysis By Zheng Fang
  id: totrans-864
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14\. 由郑芳进行的流量分析
- en: Intelligent Transportation Systems (ITS) are essential for safe, reliable, and
    efficient transportation in smart cities, serving the daily commuting and traveling
    needs of millions of people. To support ITS, advanced modeling and analysis techniques
    are necessary, and Graph Neural Networks (GNNs) are a promising tool for traffic
    analysis. GNNs can effectively model spatial correlations, making them well-suited
    for analyzing complex transportation networks. As such, GNNs have garnered significant
    interest in the traffic domain for their ability to provide insights into traffic
    patterns and behaviors.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 智能交通系统（ITS）对智能城市中的安全、可靠和高效交通至关重要，满足了数百万人的日常通勤和旅行需求。为了支持 ITS，需要先进的建模和分析技术，而图神经网络（GNNs）是交通分析中一种有前途的工具。GNNs
    可以有效地建模空间相关性，使其非常适合分析复杂的交通网络。因此，GNNs 因其提供对交通模式和行为的洞察而在交通领域引起了广泛关注。
- en: In this section, we first conclude the main GNN research directions in traffic
    domain, and then we summarize the typical graph construction processes in different
    traffic scenes and datasets. Finally, we list the classical GNN workflows for
    dealing with tasks in traffic networks. A summary is provided in Table [12](#S14.T12
    "Table 12 ‣ 14.1\. Research Directions in Traffic Domain ‣ 14\. Traffic Analysis
    By Zheng Fang ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先总结了交通领域主要的 GNN 研究方向，然后总结了不同交通场景和数据集中的典型图构建过程。最后，我们列出了处理交通网络任务的经典 GNN
    工作流程。总结见表 [12](#S14.T12 "Table 12 ‣ 14.1\. Research Directions in Traffic Domain
    ‣ 14\. Traffic Analysis By Zheng Fang ‣ A Comprehensive Survey on Deep Graph Representation
    Learning")。
- en: 14.1\. Research Directions in Traffic Domain
  id: totrans-867
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1\. 交通领域研究方向
- en: We summarize main GNN research directions in traffic domain as follows,
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了交通领域主要的 GNN 研究方向如下，
- en: •
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Traffic Flow Forecasting. Traffic flow forecasting plays an indispensable role
    in ITS (Ran and Boyce, [2012](#bib.bib292); Dimitrakopoulos and Demestichas, [2010](#bib.bib73)),
    which involves leveraging spatial-temporal data collected by various sensors to
    gain insights into future traffic patterns and behaviors. Classic methods, like
    autoregressive integrated moving average (ARIMA) (Box and Pierce, [1970](#bib.bib28)),
    support vector machine (SVM) (Hearst et al., [1998](#bib.bib137)) and recurrent
    neural networks (RNN) (Connor et al., [1994](#bib.bib64)) can only model time
    series separately without considering their spatial connections. To address this
    issue, graph neural networks (GNNs) have emerged as a powerful approach for traffic
    forecasting due to their strong ability of modeling complex graph-structured correlations
    (Jiang and Luo, [2022](#bib.bib161); Xie et al., [2020](#bib.bib394); Bui et al.,
    [2021](#bib.bib31)).
  id: totrans-870
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交通流量预测。交通流量预测在 ITS（Ran 和 Boyce，[2012](#bib.bib292)；Dimitrakopoulos 和 Demestichas，[2010](#bib.bib73)）中扮演着不可或缺的角色，它利用各种传感器收集的时空数据来洞察未来的交通模式和行为。经典方法，如自回归综合滑动平均（ARIMA）（Box
    和 Pierce，[1970](#bib.bib28)），支持向量机（SVM）（Hearst 等，[1998](#bib.bib137)）和递归神经网络（RNN）（Connor
    等，[1994](#bib.bib64)）只能单独建模时间序列，而不考虑它们的空间关联。为解决这一问题，图神经网络（GNNs）因其建模复杂图结构相关性的强大能力而成为交通预测的一种有力方法（Jiang
    和 Luo，[2022](#bib.bib161)；Xie 等，[2020](#bib.bib394)；Bui 等，[2021](#bib.bib31)）。
- en: •
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Trajectory Prediction. Trajectory prediction is a crucial task in various applications,
    such as autonomous driving and traffic surveillance, which aims to forecast future
    positions of agents in the traffic scene. However, accurately predicting trajectories
    can be challenging, as the behavior of an agent is influenced not only by its
    own motion but also by interactions with surrounding objects. To address this
    challenge, Graph Neural Networks (GNNs) have emerged as a promising tool for modeling
    complex interactions in trajectory prediction (Mohamed et al., [2020](#bib.bib264);
    Cao et al., [2021a](#bib.bib35); Zhou et al., [2021](#bib.bib463); Sun et al.,
    [2020b](#bib.bib337)). By representing the scene as a graph, where each node corresponds
    to an agent and the edges capture interactions between them, GNNs can effectively
    capture spatial dependencies and interactions between agents. This makes GNNs
    well-suited for predicting trajectories that accurately capture the behavior of
    agents in complex traffic scenes.
  id: totrans-872
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 轨迹预测。轨迹预测是各种应用中的关键任务，例如自动驾驶和交通监控，其目标是预测交通场景中代理的未来位置。然而，准确预测轨迹可能具有挑战性，因为代理的行为不仅受到其自身运动的影响，还受到与周围物体的互动影响。为了解决这一挑战，图神经网络（GNNs）作为一种建模复杂互动的有前途的工具在轨迹预测中出现（Mohamed
    et al., [2020](#bib.bib264); Cao et al., [2021a](#bib.bib35); Zhou et al., [2021](#bib.bib463);
    Sun et al., [2020b](#bib.bib337)）。通过将场景表示为图，其中每个节点对应于一个代理，边捕获它们之间的互动，GNNs可以有效地捕捉空间依赖关系和代理之间的互动。这使得GNNs非常适合预测准确捕捉代理在复杂交通场景中的行为的轨迹。
- en: •
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Traffic Anomaly Detection. Anomaly detection is an essential support for ITS.
    There are lots of traffic anomalies in daily transportation systems, for example,
    traffic accidents, extreme weather and unexpected situations. Handling these traffic
    anomalies timely can improve the service quality of public transportation. The
    main trouble of traffic anomaly detection is the highly twisted spatial-temporal
    characteristics of traffic data. The criteria and influence of traffic anomaly
    varies among locations and times. GNNs have been introduced and achieved success
    in this domain (Deng et al., [2022](#bib.bib70); Chen et al., [2021a](#bib.bib54);
    Zhang et al., [2022b](#bib.bib435); Deng and Hooi, [2021](#bib.bib69)).
  id: totrans-874
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交通异常检测。异常检测是智能交通系统（ITS）的重要支持。日常交通系统中存在大量的交通异常，例如交通事故、极端天气和突发情况。及时处理这些交通异常可以提高公共交通服务质量。交通异常检测的主要难点在于交通数据高度扭曲的时空特征。交通异常的标准和影响因地点和时间而异。图神经网络（GNNs）已被引入并在该领域取得了成功（Deng
    et al., [2022](#bib.bib70); Chen et al., [2021a](#bib.bib54); Zhang et al., [2022b](#bib.bib435);
    Deng and Hooi, [2021](#bib.bib69)）。
- en: •
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Others. Traffic demand prediction targets at estimating the future number of
    travelling at some location. It is of vital and practical significance in the
    resource scheduling for ITS. By using GNNs, the spatial dependencies of demands
    can be revealed (Yao et al., [2018](#bib.bib413); Yang et al., [2020](#bib.bib411)).
    What is more, urban vehicle emission analysis is also considered in recent work,
    which is closely related to environmental protection and gains increasing researcher
    attention (Xu et al., [2020](#bib.bib406)).
  id: totrans-876
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他。交通需求预测旨在估计某个地点未来的出行人数。这在智能交通系统的资源调度中具有重要的实际意义。通过使用GNNs，可以揭示需求的空间依赖关系（Yao
    et al., [2018](#bib.bib413); Yang et al., [2020](#bib.bib411)）。此外，最近的研究还考虑了城市车辆排放分析，这与环境保护密切相关，并受到越来越多研究人员的关注（Xu
    et al., [2020](#bib.bib406)）。
- en: Table 12\. Summary of graph models for traffic analysis.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12\. 交通分析的图模型汇总。
- en: '| Models | Tasks | Adjcency matrices | GNN types | Temporal modules |'
  id: totrans-878
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 任务 | 邻接矩阵 | GNN 类型 | 时序模块 |'
- en: '| STGCN(Yu et al., [2017](#bib.bib421)) | Traffic Flow Forecasting | Fixed
    Matrix | GCN | TCN |'
  id: totrans-879
  prefs: []
  type: TYPE_TB
  zh: '| STGCN (Yu et al., [2017](#bib.bib421)) | 交通流量预测 | 固定矩阵 | GCN | TCN |'
- en: '| DCRNN(Li et al., [2017c](#bib.bib221)) | Traffic Flow Forecasting | Fixed
    Matrix | ChebNet | RNN |'
  id: totrans-880
  prefs: []
  type: TYPE_TB
  zh: '| DCRNN (Li et al., [2017c](#bib.bib221)) | 交通流量预测 | 固定矩阵 | ChebNet | RNN |'
- en: '| AGCRN (Bai et al., [2020](#bib.bib14)) | Traffic Flow Forecasting | Dynamic
    Matrix | GCN | GRU |'
  id: totrans-881
  prefs: []
  type: TYPE_TB
  zh: '| AGCRN (Bai et al., [2020](#bib.bib14)) | 交通流量预测 | 动态矩阵 | GCN | GRU |'
- en: '| ASTGCN (Guo et al., [2019](#bib.bib126)) | Traffic Flow Forecasting | Fixed
    Matrix | GAT | Attention&TCN |'
  id: totrans-882
  prefs: []
  type: TYPE_TB
  zh: '| ASTGCN (Guo et al., [2019](#bib.bib126)) | 交通流量预测 | 固定矩阵 | GAT | 注意力&TCN
    |'
- en: '| STSGCN (Song et al., [2020a](#bib.bib329)) | Traffic Flow Forecasting | Dynamic
    Matrix | GCN | Cropping |'
  id: totrans-883
  prefs: []
  type: TYPE_TB
  zh: '| STSGCN (Song et al., [2020a](#bib.bib329)) | 交通流量预测 | 动态矩阵 | GCN | 裁剪 |'
- en: '| GraphWaveNet (Wu et al., [2019a](#bib.bib389)) | Traffic Flow Forecasting
    | Dynamic Matrix | GCN | Gated-TCN |'
  id: totrans-884
  prefs: []
  type: TYPE_TB
  zh: '| GraphWaveNet (Wu et al., [2019a](#bib.bib389)) | 交通流预测 | 动态矩阵 | GCN | Gated-TCN
    |'
- en: '| LSGCN (Huang et al., [2020](#bib.bib151)) | Traffic Flow Forecasting | Fixed
    Matrix | GAT | GLU |'
  id: totrans-885
  prefs: []
  type: TYPE_TB
  zh: '| LSGCN (Huang et al., [2020](#bib.bib151)) | 交通流预测 | 固定矩阵 | GAT | GLU |'
- en: '| GAC-Net (Song et al., [2020b](#bib.bib330)) | Traffic Flow Forecasting |
    Fixed Matrix | GAT | Gated-TCN |'
  id: totrans-886
  prefs: []
  type: TYPE_TB
  zh: '| GAC-Net (Song et al., [2020b](#bib.bib330)) | 交通流预测 | 固定矩阵 | GAT | Gated-TCN
    |'
- en: '| STGODE (Fang et al., [2021](#bib.bib90)) | Traffic Flow Forecasting | Fixed
    Matrix | Graph ODE | TCN |'
  id: totrans-887
  prefs: []
  type: TYPE_TB
  zh: '| STGODE (Fang et al., [2021](#bib.bib90)) | 交通流预测 | 固定矩阵 | 图 ODE | TCN |'
- en: '| STG-NCDE (Choi et al., [2022](#bib.bib58)) | Traffic Flow Forecasting | Dynamic
    Matrix | GCN | NCDE |'
  id: totrans-888
  prefs: []
  type: TYPE_TB
  zh: '| STG-NCDE (Choi et al., [2022](#bib.bib58)) | 交通流预测 | 动态矩阵 | GCN | NCDE |'
- en: '| MS-ASTN (Wang et al., [2020c](#bib.bib366)) | OD Flow Forecasting | OD Matrix
    | GCN | LSTM |'
  id: totrans-889
  prefs: []
  type: TYPE_TB
  zh: '| MS-ASTN (Wang et al., [2020c](#bib.bib366)) | OD 流量预测 | OD 矩阵 | GCN | LSTM
    |'
- en: '| Social-STGCNN (Mohamed et al., [2020](#bib.bib264)) | Trajectory Prediction
    | Fixed Matrix | GCN | TXP-CNN |'
  id: totrans-890
  prefs: []
  type: TYPE_TB
  zh: '| Social-STGCNN (Mohamed et al., [2020](#bib.bib264)) | 轨迹预测 | 固定矩阵 | GCN |
    TXP-CNN |'
- en: '| RSBG (Sun et al., [2020b](#bib.bib337)) | Trajectory Prediction | Dynamic
    Matrix | GCN | LSTM |'
  id: totrans-891
  prefs: []
  type: TYPE_TB
  zh: '| RSBG (Sun et al., [2020b](#bib.bib337)) | 轨迹预测 | 动态矩阵 | GCN | LSTM |'
- en: '| STGAN (Deng et al., [2022](#bib.bib70)) | Anomaly Detection | Fixed Matrix
    | GCN | GRU |'
  id: totrans-892
  prefs: []
  type: TYPE_TB
  zh: '| STGAN (Deng et al., [2022](#bib.bib70)) | 异常检测 | 固定矩阵 | GCN | GRU |'
- en: '| DMVST-VGNN (Jin et al., [2020b](#bib.bib165)) | Traffic Demand Prediction
    | Fixed Matrix | GAT | GLU |'
  id: totrans-893
  prefs: []
  type: TYPE_TB
  zh: '| DMVST-VGNN (Jin et al., [2020b](#bib.bib165)) | 交通需求预测 | 固定矩阵 | GAT | GLU
    |'
- en: '| ST-GRAT (Park et al., [2020](#bib.bib271)) | Traffic Speed Prediction | Fixed
    Matrix | GAT | Attention |'
  id: totrans-894
  prefs: []
  type: TYPE_TB
  zh: '| ST-GRAT (Park et al., [2020](#bib.bib271)) | 交通速度预测 | 固定矩阵 | GAT | Attention
    |'
- en: 14.2\. Traffic Graph Construction
  id: totrans-895
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2\. 交通图构建
- en: 14.2.1\. Traffic Graph
  id: totrans-896
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.1\. 交通图
- en: '. The traffic network is represented as a graph $\mathcal{G}=(V,E,A)$, where
    $V$ is the set of $N$ traffic nodes, $E$ is the set of edges, and $A\in\mathbb{R}^{N\times
    N}$ is an adjacency matrix representing the connectivity of $N$ nodes. In the
    traffic domain, $V$ usually represents a set of physical nodes, like traffic stations
    or traffic sensors. The features of nodes typically depend on the specific task.
    Take the traffic flow forecasting as an example. The features are the traffic
    flows, i.e., the historical time series of nodes. The traffic flow can be represented
    as a flow matrix $X\in\mathbb{R}^{N\times T}$, where $N$ is the number of traffic
    nodes and $T$ is the length of historical series, and $X_{nt}$ denotes the traffic
    flow of node $n$ at time $t$. The goal of traffic flow forecasting is to learn
    a mapping function $f$ to predict the traffic flow during future $T^{\prime}$
    steps given the historical $T$ step information, which can be formulated as following:'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: . 交通网络被表示为图 $\mathcal{G}=(V,E,A)$，其中 $V$ 是 $N$ 个交通节点的集合，$E$ 是边的集合，$A\in\mathbb{R}^{N\times
    N}$ 是表示 $N$ 个节点连接的邻接矩阵。在交通领域，$V$ 通常表示一组物理节点，如交通站点或交通传感器。节点的特征通常取决于具体任务。例如，在交通流预测中，特征是交通流量，即节点的历史时间序列。交通流量可以表示为流量矩阵
    $X\in\mathbb{R}^{N\times T}$，其中 $N$ 是交通节点的数量，$T$ 是历史序列的长度，$X_{nt}$ 表示时间 $t$ 时节点
    $n$ 的交通流量。交通流预测的目标是学习一个映射函数 $f$，以预测在未来 $T^{\prime}$ 步期间的交通流量，给定历史 $T$ 步信息，这可以表述如下：
- en: '| (157) |  | $\left[X_{:,t-T+1},X_{:,t-T+2},\cdots,X_{:,t};\mathcal{G}\right]\stackrel{{\scriptstyle
    f}}{{\longrightarrow}}\left[X_{:,t+1},X_{:,t+2},\cdots,X_{:,t+T^{\prime}}\right].$
    |  |'
  id: totrans-898
  prefs: []
  type: TYPE_TB
  zh: '| (157) |  | $\left[X_{:,t-T+1},X_{:,t-T+2},\cdots,X_{:,t};\mathcal{G}\right]\stackrel{{\scriptstyle
    f}}{{\longrightarrow}}\left[X_{:,t+1},X_{:,t+2},\cdots,X_{:,t+T^{\prime}}\right].$
    |  |'
- en: 14.2.2\. Graph Construction.
  id: totrans-899
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.2\. 图构建。
- en: Constructing a graph to describe the interactions among traffic nodes, i.e.,
    the design of the adjacency matrix $A$, is the key part of traffic analysis. The
    mainstream designs can be divided into two categories, fixed matrix and dynamic
    matrix.
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个描述交通节点之间交互的图，即邻接矩阵 $A$ 的设计，是交通分析的关键部分。主流设计可以分为两类，固定矩阵和动态矩阵。
- en: Fixed matrix. Lots of works assume that the correlations among traffic nodes
    are fixed and constant over time, and they design a fixed and pre-defined adjacency
    matrix to capture the spatial correlation. Here we list several common choices
    of fixed adjacency matrix.
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 固定矩阵。许多工作假设交通节点之间的相关性在时间上是固定且恒定的，他们设计了一个固定且预定义的邻接矩阵来捕捉空间相关性。这里列出了几种常见的固定邻接矩阵选择。
- en: The connectivity matrix is the most natural construction way. It relies on the
    support of road map data. The element of the connectivity matrix is defined as
    1 if two node are physically connected and 0 otherwise. This binary format is
    convenient to construct and easy to interpret.
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: 连通性矩阵是最自然的构造方式。它依赖于道路地图数据的支持。如果两个节点物理上连接，则连通性矩阵的元素定义为1，否则为0。这种二进制格式构建方便且易于解释。
- en: The distance-based matrix is also a common choice, which shows the connection
    between two nodes more precisely. The elements of the matrix are defined as the
    function of distance between two nodes (driving distance or geographical distance).
    A typical way is to use threshold Guassian function as follows,
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 基于距离的矩阵也是一个常见的选择，它更精确地显示了两个节点之间的连接。矩阵的元素定义为两个节点之间距离（驾驶距离或地理距离）的函数。一个典型的方法是使用阈值高斯函数，如下所示，
- en: '| (158) |  | <math   alttext="A_{ij}=\left\{\begin{array}[]{cr}\exp(-\frac{d_{ij}^{2}}{\sigma^{2}}),&amp;d_{ij}<\epsilon\\
    0,&amp;d_{ij}>\epsilon\\'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: '| (158) |  | <math alttext="A_{ij}=\left\{\begin{array}[]{cr}\exp(-\frac{d_{ij}^{2}}{\sigma^{2}}),&amp;d_{ij}<\epsilon\\
    0,&amp;d_{ij}>\epsilon\\'
- en: \end{array}\right.," display="block"><semantics ><mrow ><mrow  ><msub ><mi >A</mi><mrow  ><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow></msub><mo >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd ><mrow ><mrow  ><mi >exp</mi><mo >⁡</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mo >−</mo><mstyle displaystyle="false" ><mfrac ><msubsup ><mi >d</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow><mn >2</mn></msubsup><msup
    ><mi >σ</mi><mn >2</mn></msup></mfrac></mstyle></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="right"  ><mrow ><msub ><mi  >d</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow></msub><mo
    ><</mo><mi  >ϵ</mi></mrow></mtd></mtr><mtr ><mtd  ><mrow ><mn >0</mn><mo  >,</mo></mrow></mtd><mtd
    columnalign="right"  ><mrow ><msub ><mi  >d</mi><mrow ><mi >i</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >j</mi></mrow></msub><mo >></mo><mi  >ϵ</mi></mrow></mtd></mtr></mtable></mrow></mrow><mo
    >,</mo></mrow><annotation-xml encoding="MathML-Content" ><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝐴</ci><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><matrix ><matrixrow  ><apply ><apply ><apply
    ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑑</ci><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><cn type="integer"  >2</cn></apply><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝜎</ci><cn type="integer"  >2</cn></apply></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑑</ci><apply ><ci  >𝑖</ci><ci
    >𝑗</ci></apply></apply><ci >italic-ϵ</ci></apply></matrixrow><matrixrow ><cn type="integer"
    >0</cn><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑑</ci><apply
    ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><ci >italic-ϵ</ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >A_{ij}=\left\{\begin{array}[]{cr}\exp(-\frac{d_{ij}^{2}}{\sigma^{2}}),&d_{ij}<\epsilon\\
    0,&d_{ij}>\epsilon\\ \end{array}\right.,</annotation></semantics></math> |  |
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\right.," display="block"><semantics ><mrow ><mrow  ><msub ><mi >A</mi><mrow  ><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow></msub><mo >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"
    ><mtr  ><mtd ><mrow ><mrow  ><mi >exp</mi><mo >⁡</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mo >−</mo><mstyle displaystyle="false" ><mfrac ><msubsup ><mi >d</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow><mn >2</mn></msubsup><msup
    ><mi >σ</mi><mn >2</mn></msup></mfrac></mstyle></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="right"  ><mrow ><msub ><mi  >d</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow></msub><mo
    ><</mo><mi  >ϵ</mi></mrow></mtd></mtr><mtr ><mtd  ><mrow ><mn >0</mn><mo  >,</mo></mrow></mtd><mtd
    columnalign="right"  ><mrow ><msub ><mi  >d</mi><mrow ><mi >i</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >j</mi></mrow></msub><mo >></mo><mi  >ϵ</mi></mrow></mtd></mtr></mtable></mrow></mrow><mo
    >,</mo></mrow><annotation-xml encoding="MathML-Content" ><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝐴</ci><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><matrix ><matrixrow  ><apply ><apply ><apply
    ><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑑</ci><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><cn type="integer"  >2</cn></apply><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝜎</ci><cn type="integer"  >2</cn></apply></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑑</ci><apply ><ci  >𝑖</ci><ci
    >𝑗</ci></apply></apply><ci >italic-ϵ</ci></apply></matrixrow><matrixrow ><cn type="integer"
    >0</cn><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑑</ci><apply
    ><ci >𝑖</ci><ci >𝑗</ci></apply></apply><ci >italic-ϵ</ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >A_{ij}=\left\{\begin{array}[]{cr}\exp(-\frac{d_{ij}^{2}}{\sigma^{2}}),&d_{ij}<\epsilon\\
    0,&d_{ij}>\epsilon\\ \end{array}\right.,</annotation></semantics></math> |  |
- en: where $d_{ij}$ is the distance between node $i$ and $j$, and $\sigma$ and $\epsilon$
    are two hyperparameters to control the distribution and the sparsity of the matrix.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{ij}$ 是节点 $i$ 和 $j$ 之间的距离，$\sigma$ 和 $\epsilon$ 是控制矩阵分布和稀疏度的两个超参数。
- en: Another kind of fixed adjacency matrix is the similarity-based matrix. In fact,
    similarity matrix is not an adjacency matrix to some extent. It is constructed
    according to the similarity of two nodes, which means the neighbors in the similarity
    graph may be far way in the real world. There are various similarity metrics.
    For example, many works measure the similarity of two node by their functionality,
    e.g., the distribution of surrounding point of interests (POIs). The assumption
    behind is that nodes share similar functionality may share similar traffic patterns.
    We can also define the similarity through the historical flow patterns. To compute
    the similarity of two time series, a common practice is to use Dynamic Time Wrapping
    (DTW) algorithm (Müller, [2007](#bib.bib267)), which is superior to other metrics
    due to its sensitivity to shape similarity rather than point-wise similarity.
    Specifically, given two time series $X=(x_{1},x_{2},\cdots,x_{n})$ and $Y=(y_{1},y_{2},\cdots,y_{n})$,
    DTW is a dynamic programming algorithm defined as
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种固定邻接矩阵是基于相似度的矩阵。实际上，相似度矩阵在某种程度上不是邻接矩阵。它是根据两个节点的相似度构建的，这意味着相似图中的邻居在现实世界中可能很远。存在各种相似度度量。例如，许多研究通过功能性测量两个节点的相似度，如周围兴趣点（POIs）的分布。其假设是功能相似的节点可能具有相似的交通模式。我们还可以通过历史流量模式定义相似度。为了计算两个时间序列的相似度，常用的方法是使用动态时间规整（DTW）算法（Müller,
    [2007](#bib.bib267)），它因对形状相似度的敏感性而优于其他度量。具体而言，给定两个时间序列$X=(x_{1},x_{2},\cdots,x_{n})$和$Y=(y_{1},y_{2},\cdots,y_{n})$，DTW是一个动态规划算法定义为
- en: '| (159) |  | $D(i,j)=dist(x_{i},y_{j})+\min\left(D(i-1,j),D(i,j-1),D(i-1,j-1)\right),$
    |  |'
  id: totrans-908
  prefs: []
  type: TYPE_TB
  zh: '| (159) |  | $D(i,j)=dist(x_{i},y_{j})+\min\left(D(i-1,j),D(i,j-1),D(i-1,j-1)\right),$
    |  |'
- en: where $D(i,j)$ represents the shortest distance between subseries $X=(x_{1},x_{2},\cdots,x_{i})$
    and $Y=(y_{1},y_{2},\cdots,y_{j})$, and $dist(x_{i},y_{j})$ is some distance metric
    like absolute distance. As a result, $DTW(X,Y)=D(n,n)$ is set as the final distance
    between $X$ and $Y$, which better reflects the similarity of two time series compared
    to the Euclidean distance.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$D(i,j)$表示子序列$X=(x_{1},x_{2},\cdots,x_{i})$和$Y=(y_{1},y_{2},\cdots,y_{j})$之间的最短距离，$dist(x_{i},y_{j})$是某种距离度量，如绝对距离。因此，$DTW(X,Y)=D(n,n)$被设定为$X$和$Y$之间的最终距离，这比欧几里得距离更好地反映了两个时间序列的相似性。
- en: Dynamic matrix. The pre-defined matrix is sometimes unavailable and cannot reflect
    complete information of spatial correlations. The dynamic adaptive matrix is proposed
    to solve the issue. The dynamic matrix is learned from input data automatically.
    To achieve the best prediction performance, the dynamic matrix will manage to
    infer the hidden correlations among nodes, more than those physical connections.
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 动态矩阵。预定义矩阵有时不可用，无法反映空间相关性的完整信息。为解决此问题，提出了动态自适应矩阵。动态矩阵是从输入数据中自动学习的。为了获得最佳预测性能，动态矩阵会设法推断节点之间的隐藏相关性，而不仅仅是那些物理连接。
- en: A typical practice is learning adjacency matrix from node embeddings (Bai et al.,
    [2020](#bib.bib14)). Let $E_{A}\in\mathbb{R}^{N\times d}$ be a learnable node
    embedding dictionary, where each row of $E_{A}$ represents the embedding of a
    node, $N$ and $d$ denote the number of node and the dimension of embeddings respectively.
    The graph adjacency matrix is defined as the similarities among node embeddings,
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: 一种典型的实践是从节点嵌入学习邻接矩阵（Bai et al., [2020](#bib.bib14)）。设$E_{A}\in\mathbb{R}^{N\times
    d}$为一个可学习的节点嵌入字典，其中$E_{A}$的每一行表示一个节点的嵌入，$N$和$d$分别表示节点的数量和嵌入的维度。图的邻接矩阵定义为节点嵌入之间的相似度，
- en: '| (160) |  | $D^{-\frac{1}{2}}AD^{-\frac{1}{2}}=softmax\left(ReLU(E_{A}\cdot
    E_{A}^{T})\right),$ |  |'
  id: totrans-912
  prefs: []
  type: TYPE_TB
  zh: '| (160) |  | $D^{-\frac{1}{2}}AD^{-\frac{1}{2}}=softmax\left(ReLU(E_{A}\cdot
    E_{A}^{T})\right),$ |  |'
- en: where $softmax$ function is to perform row-normalization, and $D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$
    is the Laplacian matrix.
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$softmax$函数用于执行行归一化，$D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$是拉普拉斯矩阵。
- en: 14.3\. Typical GNN Frameworks in Traffic Domain
  id: totrans-914
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3\. 交通领域的典型GNN框架
- en: Here we list several representative GNN models for traffic analysis.
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出几个用于交通分析的代表性GNN模型。
- en: 14.3.1\. Spatial Temporal Graph Convolution Network (STGCN) (Yu et al., [2017](#bib.bib421)).
  id: totrans-916
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.1\. 空间时间图卷积网络（STGCN）（Yu et al., [2017](#bib.bib421)）。
- en: STGCN is a pioneering work in the spatial-temporal GNN domain. It utilizes graph
    convolution to capture spatial features, and deploys a gated causal convolution
    to extract temporal patterns. Specifically, the graph convolution and temporal
    convolution are defined as follows,
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: STGCN 是空间-时间 GNN 领域的开创性工作。它利用图卷积捕捉空间特征，并部署了门控因果卷积来提取时间模式。具体而言，图卷积和时间卷积定义如下，
- en: '| (161) |  | $\displaystyle\Theta*_{\mathcal{G}}x$ | $\displaystyle=\theta(I_{n}+D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x=\theta(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}})x,$
    |  |'
  id: totrans-918
  prefs: []
  type: TYPE_TB
  zh: '| (161) |  | $\displaystyle\Theta*_{\mathcal{G}}x$ | $\displaystyle=\theta(I_{n}+D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x=\theta(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}})x,$
    |  |'
- en: '| (162) |  | $\displaystyle\Gamma*_{\mathcal{T}}y$ | $\displaystyle=P\odot\sigma(Q),$
    |  |'
  id: totrans-919
  prefs: []
  type: TYPE_TB
  zh: '| (162) |  | $\displaystyle\Gamma*_{\mathcal{T}}y$ | $\displaystyle=P\odot\sigma(Q),$
    |  |'
- en: where $\Theta$ is the parameter of graph convolution, $P$ and $Q$ are the outputs
    of an 1-d convolution along the temporal dimension. The sigmoid gate $\sigma(Q)$
    controls how the states of $P$ are relevant for discovering hidden temporal patterns.
    In order to fuse features from both spatial and temporal dimension, the spatial
    convolution layer and the temporal convolution layer are combined to construct
    a spatial temporal block to jointly deal with graph-structured time series, and
    more blocks can be stacked to achieve a more scalable and complex model.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Theta$ 是图卷积的参数，$P$ 和 $Q$ 是沿时间维度的 1-d 卷积的输出。Sigmoid 门控 $\sigma(Q)$ 控制 $P$
    的状态如何与发现隐藏的时间模式相关。为了融合空间和时间维度的特征，空间卷积层和时间卷积层被组合在一起，构建了一个空间时间块，以共同处理图结构的时间序列，并且可以堆叠更多的块以实现更具扩展性和复杂性的模型。
- en: 14.3.2\. Diffusion Convolutional Recurrent Neural Network (DCRNN) (Li et al.,
    [2017c](#bib.bib221)).
  id: totrans-921
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.2. 扩散卷积递归神经网络（DCRNN）（Li 等，[2017c](#bib.bib221)）。
- en: 'DCRNN is a representative solution combining graph convolution networks with
    recurrent neural networks. It captures spatial dependencies by bidirectional random
    walks on the graph. The diffusion convolution operation on a graph is defined
    as:'
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: DCRNN 是结合图卷积网络和递归神经网络的代表性解决方案。它通过图上的双向随机游走来捕捉空间依赖性。图上的扩散卷积操作定义为：
- en: '| (163) |  | $X*_{\mathcal{G}}f_{\theta}=\sum_{k=0}^{K}\left(\theta_{k,1}(D_{O}^{-1}A)^{k}+\theta_{k,2}(D_{I}^{-1}A)^{k}\right)X,$
    |  |'
  id: totrans-923
  prefs: []
  type: TYPE_TB
  zh: '| (163) |  | $X*_{\mathcal{G}}f_{\theta}=\sum_{k=0}^{K}\left(\theta_{k,1}(D_{O}^{-1}A)^{k}+\theta_{k,2}(D_{I}^{-1}A)^{k}\right)X,$
    |  |'
- en: where $\theta$ are parameters for the convolution filter, and $D_{O}^{-1}A,D_{I}^{-1}A$
    represent the bidirectional diffusion processes respectively. In term of the temporal
    dependency, DCRNN utilizes Gated Recurrent Units (GRU), and replace the linear
    transformation in the GRU with the diffusion convolution as follows,
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta$ 是卷积滤波器的参数，$D_{O}^{-1}A,D_{I}^{-1}A$ 分别表示双向扩散过程。关于时间依赖性，DCRNN 利用门控递归单元（GRU），并将
    GRU 中的线性变换替换为扩散卷积，如下所示，
- en: '| (164) |  | $\displaystyle r^{(t)}$ | $\displaystyle=\sigma(\Theta_{r}*_{\mathcal{G}}[X^{(t)},H^{(t-1)}]+b_{r}),$
    |  |'
  id: totrans-925
  prefs: []
  type: TYPE_TB
  zh: '| (164) |  | $\displaystyle r^{(t)}$ | $\displaystyle=\sigma(\Theta_{r}*_{\mathcal{G}}[X^{(t)},H^{(t-1)}]+b_{r}),$
    |  |'
- en: '| (165) |  | $\displaystyle u^{(t)}$ | $\displaystyle=\sigma(\Theta_{u}*_{\mathcal{G}}[X^{(t)},H^{(t-1)}]+b_{u}),$
    |  |'
  id: totrans-926
  prefs: []
  type: TYPE_TB
  zh: '| (165) |  | $\displaystyle u^{(t)}$ | $\displaystyle=\sigma(\Theta_{u}*_{\mathcal{G}}[X^{(t)},H^{(t-1)}]+b_{u}),$
    |  |'
- en: '| (166) |  | $\displaystyle C^{(t)}$ | $\displaystyle=\tanh(\Theta_{C}*_{\mathcal{G}}[X^{(t)},(r^{(t)}\odot
    H^{(t-1)}]+b_{c}),$ |  |'
  id: totrans-927
  prefs: []
  type: TYPE_TB
  zh: '| (166) |  | $\displaystyle C^{(t)}$ | $\displaystyle=\tanh(\Theta_{C}*_{\mathcal{G}}[X^{(t)},(r^{(t)}\odot
    H^{(t-1)}]+b_{c}),$ |  |'
- en: '| (167) |  | $\displaystyle H^{(t)}$ | $\displaystyle=u^{(t)}\odot H^{(t-1)}+(1-u^{(t)})\odot
    C^{(t)},$ |  |'
  id: totrans-928
  prefs: []
  type: TYPE_TB
  zh: '| (167) |  | $\displaystyle H^{(t)}$ | $\displaystyle=u^{(t)}\odot H^{(t-1)}+(1-u^{(t)})\odot
    C^{(t)},$ |  |'
- en: where $X^{(t)},H^{(t)}$ denote the input and output at time $t$, $r^{(t)},u^{(t)}$
    are the reset and update gates respectively, and $\Theta_{r},\Theta_{u},\Theta_{C}$
    are parameters of convolution filters. Moreover, DCRNN employs a sequence to sequence
    architecture to predict future series. Both the encoder and the decoder are constructed
    with diffusion convolutional recurrent layers. The historical time series are
    fed into the encoder and the predictions are generated by the decoder. The scheduled
    sampling technique is utilized to solve the discrepancy problem between training
    and test distribution.
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X^{(t)},H^{(t)}$ 表示时间 $t$ 的输入和输出，$r^{(t)},u^{(t)}$ 分别是重置和更新门，而 $\Theta_{r},\Theta_{u},\Theta_{C}$
    是卷积滤波器的参数。此外，DCRNN 使用序列到序列的架构来预测未来的序列。编码器和解码器都由扩散卷积递归层构成。历史时间序列被输入到编码器中，预测结果由解码器生成。调度采样技术用于解决训练和测试分布之间的差异问题。
- en: 14.3.3\. Adaptive Graph Convolutional Recurrent Network (AGCRN) (Bai et al.,
    [2020](#bib.bib14)).
  id: totrans-930
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.3\. 自适应图卷积递归网络（AGCRN）（Bai 等， [2020](#bib.bib14)）。
- en: The focuses of AGCRN are two-fold. On the one hand, it argues that the temporal
    patterns are diversified and thus parameter-sharing for each node is inferior;
    on the other hand, it proposes that the pre-defined graph may be intuitive and
    incomplete for the specific prediction task. To mitigate the two issues, it designs
    a Node Adaptive Parameter Learning (NAPL) module to learn node-specific patterns
    for each traffic series, and a Data Adaptive Graph Generation (DAGG) module to
    infer the hidden correlations among nodes from data and to generate the graph
    during training. Specifically, the NAPL module is defined as follows,
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: AGCRN 的关注点有两个方面。一方面，它认为时间模式是多样化的，因此每个节点的参数共享效果较差；另一方面，它提出预定义的图可能对于特定的预测任务直观且不完整。为了解决这两个问题，它设计了一个节点自适应参数学习（NAPL）模块，以学习每个交通系列的节点特定模式，以及一个数据自适应图生成（DAGG）模块，以从数据中推断节点之间的隐藏相关性，并在训练过程中生成图。具体而言，NAPL
    模块定义如下，
- en: '| (168) |  | $\displaystyle Z=(I_{n}+D^{-\frac{1}{2}}AD^{-\frac{1}{2}})XE_{\mathcal{G}}W_{\mathcal{G}}+E_{\mathcal{G}}b_{\mathcal{G}},$
    |  |'
  id: totrans-932
  prefs: []
  type: TYPE_TB
  zh: '| (168) |  | $\displaystyle Z=(I_{n}+D^{-\frac{1}{2}}AD^{-\frac{1}{2}})XE_{\mathcal{G}}W_{\mathcal{G}}+E_{\mathcal{G}}b_{\mathcal{G}},$
    |  |'
- en: where $X\in\mathbb{R}^{N\times C}$ is the input feature, $E_{\mathcal{G}}\in\mathbb{R}^{N\times
    d}$ is a node embedding dictionary, $d$ is the embedding dimension ($d<<N$), $W_{\mathcal{G}}\in\mathbb{R}^{d\times
    C\times F}$ is a weight pool. The original parameter $\Theta$ in the graph convolution
    is replaced by the matrix production of $E_{\mathcal{G}}W_{\mathcal{G}}$, and
    the same operation is applied for the bias. This can help the model to capture
    node specific patterns from a pattern pool according the node embedding. The DAGG
    module has been introduced in [160](#S14.E160 "In 14.2.2\. Graph Construction.
    ‣ 14.2\. Traffic Graph Construction ‣ 14\. Traffic Analysis By Zheng Fang ‣ A
    Comprehensive Survey on Deep Graph Representation Learning"). The whole workflow
    of AGCRN is formulated as following,
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X\in\mathbb{R}^{N\times C}$ 是输入特征，$E_{\mathcal{G}}\in\mathbb{R}^{N\times
    d}$ 是节点嵌入字典，$d$ 是嵌入维度（$d<<N$），$W_{\mathcal{G}}\in\mathbb{R}^{d\times C\times F}$
    是权重池。图卷积中的原始参数 $\Theta$ 被 $E_{\mathcal{G}}W_{\mathcal{G}}$ 的矩阵乘积替代，偏置也进行相同操作。这有助于模型根据节点嵌入从模式池中捕获节点特定模式。DAGG
    模块在 [160](#S14.E160 "在 14.2.2\. 图构建。 ‣ 14.2\. 交通图构建 ‣ 14\. 交通分析 由郑方 ‣ 深度图表示学习的综合调查")
    中介绍。AGCRN 的整个工作流程如下，
- en: '| (169) |  | $\displaystyle\tilde{A}$ | $\displaystyle=softmax(ReLU(EE^{T})),$
    |  |'
  id: totrans-934
  prefs: []
  type: TYPE_TB
  zh: '| (169) |  | $\displaystyle\tilde{A}$ | $\displaystyle=softmax(ReLU(EE^{T})),$
    |  |'
- en: '| (170) |  | $\displaystyle z^{(t)}$ | $\displaystyle=\sigma(\tilde{A}[X^{(t)},H^{(t-1)}]EW_{z}+Eb_{z},$
    |  |'
  id: totrans-935
  prefs: []
  type: TYPE_TB
  zh: '| (170) |  | $\displaystyle z^{(t)}$ | $\displaystyle=\sigma(\tilde{A}[X^{(t)},H^{(t-1)}]EW_{z}+Eb_{z},$
    |  |'
- en: '| (171) |  | $\displaystyle r^{(t)}$ | $\displaystyle=\sigma(\tilde{A}[X^{(t)},H^{(t-1)}]EW_{r}+Eb_{r},$
    |  |'
  id: totrans-936
  prefs: []
  type: TYPE_TB
  zh: '| (171) |  | $\displaystyle r^{(t)}$ | $\displaystyle=\sigma(\tilde{A}[X^{(t)},H^{(t-1)}]EW_{r}+Eb_{r},$
    |  |'
- en: '| (172) |  | $\displaystyle\hat{H}^{(t)}$ | $\displaystyle=\tanh(\tilde{A}[X,r^{(t)}\odot
    H^{(t-1)}]EW_{h}+Eb_{h},$ |  |'
  id: totrans-937
  prefs: []
  type: TYPE_TB
  zh: '| (172) |  | $\displaystyle\hat{H}^{(t)}$ | $\displaystyle=\tanh(\tilde{A}[X,r^{(t)}\odot
    H^{(t-1)}]EW_{h}+Eb_{h},$ |  |'
- en: '| (173) |  | $\displaystyle H^{(t)}$ | $\displaystyle=z^{(t)}\odot H^{(t-1)}+(1-z^{(t)})\odot\hat{H}^{(t)}.$
    |  |'
  id: totrans-938
  prefs: []
  type: TYPE_TB
  zh: '| (173) |  | $\displaystyle H^{(t)}$ | $\displaystyle=z^{(t)}\odot H^{(t-1)}+(1-z^{(t)})\odot\hat{H}^{(t)}.$
    |  |'
- en: 14.3.4\. Attention Based Spatial-Temporal Graph Convolutional Networks (ASTGCN)
    (Guo et al., [2019](#bib.bib126)).
  id: totrans-939
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.3.4\. 基于注意力的时空图卷积网络（ASTGCN）（Guo 等， [2019](#bib.bib126)）。
- en: ASTGCN introduces two kinds of attention mechanisms into spatial temporal forecasting,
    i.e., spatial attention and temporal attention. The spatial attention is defined
    as following,
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: ASTGCN 引入了两种注意力机制用于时空预测，即空间注意力和时间注意力。空间注意力定义如下：
- en: '| (174) |  | $\displaystyle S=V_{S}\sigma\left((XW_{1})W_{2}(W_{3}X)^{T}+b_{S}\right),$
    |  |'
  id: totrans-941
  prefs: []
  type: TYPE_TB
  zh: '| (174) |  | $\displaystyle S=V_{S}\sigma\left((XW_{1})W_{2}(W_{3}X)^{T}+b_{S}\right),$
    |  |'
- en: '| (175) |  | $\displaystyle S_{i,j}^{\prime}=\frac{\exp(S_{i,j})}{\sum_{j=1}^{N}\exp(S_{i,j})},$
    |  |'
  id: totrans-942
  prefs: []
  type: TYPE_TB
  zh: '| (175) |  | $\displaystyle S_{i,j}^{\prime}=\frac{\exp(S_{i,j})}{\sum_{j=1}^{N}\exp(S_{i,j})},$
    |  |'
- en: 'where $S^{\prime}$ is the attention score, and $W_{1},W_{2},W3$ are learnable
    parameters. The similar construction is applied for temporal attention, where
    $X^{T}$ denotes transpose the spatial dimension and the temporal dimension. Besides
    the attention mechanism, ASTGCN also introduces multi-component fusion to enhance
    the prediction ability. The input of ASTGCN consists of three parts, the recent
    segments, the daily-periodic segments and the weekly-periodic segment. The three
    segments are processed by the main model independently and fused with learnable
    weights at last:'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S^{\prime}$ 是注意力得分，$W_{1},W_{2},W_{3}$ 是可学习的参数。类似的构建也适用于时间注意力，其中 $X^{T}$
    表示转置空间维度和时间维度。除了注意力机制，ASTGCN 还引入了多组件融合以增强预测能力。ASTGCN 的输入由三个部分组成，即近期段、日周期段和周周期段。这三个段由主模型独立处理，最后用可学习的权重融合：
- en: '| (177) |  | $Y=W_{h}\odot Y_{h}+W_{d}\odot Y_{d}+W_{w}\odot Y_{w},$ |  |'
  id: totrans-944
  prefs: []
  type: TYPE_TB
  zh: '| (177) |  | $Y=W_{h}\odot Y_{h}+W_{d}\odot Y_{d}+W_{w}\odot Y_{w},$ |  |'
- en: where $Y_{h},Y_{d},Y_{w}$ denotes the predictions of different segments respectively.
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Y_{h},Y_{d},Y_{w}$ 分别表示不同段落的预测值。
- en: 15\. Summary
  id: totrans-946
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15\. 总结
- en: 'This section introduces graph models for traffic analysis and we provide a
    summary as follows:'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了用于交通分析的图模型，我们提供了如下总结：
- en: •
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Techniques. Traffic analysis is a classical spatial temporal data mining task,
    and graph models play a vital role for extracting spatial correlations. Typical
    procedures include the graph construction, spatial dimension operations, temporal
    dimension operations and the information fusion. There are multiple implementations
    for each procedure, each implementation has its strengths and weaknesses. By combining
    different implementations, various kinds of traffic analysis models can be created.
    Choosing the right combination of procedures and implementations is critical for
    achieving accurate and reliable traffic analysis results.
  id: totrans-949
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术。交通分析是一个经典的时空数据挖掘任务，图模型在提取空间相关性中起着关键作用。典型的程序包括图构建、空间维度操作、时间维度操作和信息融合。每个程序都有多种实现，每种实现都有其优缺点。通过组合不同的实现，可以创建各种类型的交通分析模型。选择正确的程序和实现组合对获得准确和可靠的交通分析结果至关重要。
- en: •
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Challenges and Limitations. Despite the remarkable success of graph representation
    learning in traffic analysis, there are still several challenges that need to
    be addressed in current studies. Firstly, external data, such as weather and calendar
    information, are not well-utilized in current models, despite their close relation
    to traffic status. The challenge lies in how to effectively fuse heterogeneous
    data to improve traffic analysis accuracy. Secondly, the interpretability of models
    has been underexplored, which could hinder their deployment in real-world transportation
    systems. Interpretable models are crucial for building trust and understanding
    among stakeholders, and more research is needed to develop models that are both
    accurate and interpretable. Addressing these challenges will be critical for advancing
    the state-of-the-art in traffic analysis and ensuring the deployment of effective
    transportation systems.
  id: totrans-951
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 挑战与局限。尽管图表示学习在交通分析中取得了显著成功，但当前研究仍面临几个需要解决的挑战。首先，外部数据如天气和日历信息在当前模型中利用不充分，尽管这些数据与交通状态密切相关。挑战在于如何有效融合异质数据以提高交通分析的准确性。其次，模型的可解释性研究不足，这可能阻碍其在实际交通系统中的应用。可解释的模型对于建立利益相关者的信任和理解至关重要，需要更多研究来开发既准确又可解释的模型。解决这些挑战对推进交通分析的最先进技术和确保有效交通系统的部署至关重要。
- en: •
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Future Works. In the future, we anticipate that more data sources will be available
    for traffic analysis, enabling a more comprehensive understanding of real-world
    traffic scenes. From data collection to model design, there is still a lot of
    work to be done to fully leverage the potential of GNNs in traffic analysis. In
    addition, we expect to see more creative applications of GNN-based traffic analysis,
    such as designing traffic light control strategies, which can help to improve
    the efficiency and safety of transportation systems. To achieve these goals, it
    is necessary to continue advancing the development of GNN-based models and exploring
    new ways to fuse diverse data sources. Additionally, there is a need to enhance
    the interpretability of models and ensure their applicability to real-world transportation
    systems. We believe that these efforts will contribute to the continued success
    of traffic analysis and the development of intelligent transportation systems.
  id: totrans-953
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来工作。未来，我们预期将有更多的数据来源用于交通分析，从而更全面地理解现实世界的交通场景。从数据收集到模型设计，还有很多工作要做，以充分发挥GNN在交通分析中的潜力。此外，我们期望看到更多基于GNN的交通分析的创新应用，例如设计交通灯控制策略，这可以帮助提高交通系统的效率和安全性。为了实现这些目标，有必要继续推进GNN模型的发展，并探索融合多样化数据源的新方法。此外，还需要增强模型的可解释性，确保其在现实交通系统中的适用性。我们相信，这些努力将有助于交通分析的持续成功和智能交通系统的发展。
- en: 16\. Future Directions By Zhiping Xiao
  id: totrans-954
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16\. 未来方向，由肖智平提供
- en: In this section, we outline some prospective future directions of deep graph
    representation learning based on the above cornerstone, taxonomy and real-world
    applications. We also outline a few more directions closer to the theoretical
    side.
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们基于上述基石、分类法和现实世界的应用，概述了一些深度图表示学习的前景未来方向。我们还概述了一些更接近理论方面的方向。
- en: 16.1\. Application-Inspired Directions
  id: totrans-956
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.1\. 应用驱动的方向
- en: Since deep graph representation is widely used these years, many problems are
    solved while many others arose. While we observe many real-world applications,
    we conclude plenty of challenging problems that are not yet solved. Here in this
    subsection, we outline a few.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来深度图表示被广泛使用，解决了许多问题，同时也出现了许多新的问题。在观察到许多现实世界的应用时，我们总结了许多尚未解决的挑战性问题。在这一小节中，我们概述了其中的一些。
- en: 16.1.1\. Fairness in Graph Representation Learning
  id: totrans-958
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.1.1\. 图表示学习中的公平性
- en: One common aspect to care about is the fairness concern. Fairness, by definition,
    refers to that the protected features does not infect the outcome. In general,
    a data set’s fairness refers to that the protected features are not influencing
    the data distribution. A model’s fairness, on the other hand, refers to the concern
    that the output of our algorithms should not be affected by some certain protected
    features. The protected features can be race, gender, etc.
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 需要关注的一个共同方面是公平性问题。公平性，按定义，指的是受保护特征不影响结果。一般来说，数据集的公平性指的是受保护特征不会影响数据分布。另一方面，模型的公平性指的是我们的算法输出不应受某些受保护特征的影响。受保护特征可以是种族、性别等。
- en: To have some fair data, we might measure how frequently in a text corpus a female
    character is associated to leadership, versus how frequently a male is. To design
    a fair model, we require the results of the model keep the same once we make any
    change to the protected features. For instance, if we swap the gender feature
    of some data samples, the predicted outcomes remain the same.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现一些公平的数据，我们可能会衡量在文本语料库中女性角色与领导力的关联频率，与男性角色的频率进行比较。为了设计一个公平的模型，我们要求模型的结果在我们对受保护特征进行任何更改时保持不变。例如，如果我们交换一些数据样本的性别特征，预测结果仍然保持不变。
- en: Similar to the fairness challenge in many other fields of machine learning (Chouldechova
    and Roth, [2018](#bib.bib59); Mehrabi et al., [2021](#bib.bib259)), graph representation
    learning can easily suffer from bias from the data sets which inherit stereotypes
    from the real world, from the architecture of models, or from design decisions
    aiming at a higher performance on a particular task. As the graph representation
    becomes increasingly popular in recent years, the researchers are getting fairness
    into their sights (Ma et al., [2021](#bib.bib249); Dong et al., [2021](#bib.bib76)).
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于许多其他机器学习领域中的公平性挑战（Chouldechova and Roth, [2018](#bib.bib59); Mehrabi et al.,
    [2021](#bib.bib259)），图表示学习也容易受到数据集中的偏见的影响，这些数据集从现实世界继承了刻板印象，或者来自模型的架构，或来自旨在提高特定任务性能的设计决策。随着图表示在近年来越来越受欢迎，研究人员开始关注公平性问题（Ma
    et al., [2021](#bib.bib249); Dong et al., [2021](#bib.bib76)）。
- en: 16.1.2\. Robustness in Graph Representation Learning
  id: totrans-962
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.1.2\. 图表示学习中的鲁棒性
- en: Real world data is always noisy, containing many different kinds of disruptions,
    and not end up being perfect normal distribution. In the worst case, some noise
    can potentially avoid a model from learning the correct knowledge. Better robustness
    refers to the model has better chance of reaching a relatively good and stable
    outcome while input being changed. If a model is not robust enough, the performance
    can not be relied on. Therefore, robustness is another important yet challenging
    consideration in deep graph representation learning.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的数据总是噪声较多，包含多种不同的干扰，且最终并非完美的正态分布。在最坏的情况下，一些噪声可能会使模型无法学习到正确的知识。更好的鲁棒性意味着模型在输入发生变化时有更好的机会达到相对良好和稳定的结果。如果模型不够鲁棒，那么其性能是无法依赖的。因此，鲁棒性是深度图表示学习中的另一个重要且具有挑战性的考虑因素。
- en: Again, similar to many other machine learning approaches that aims at solving
    real-world problems (Carlini and Wagner, [2017](#bib.bib40)), improving the robustness
    of deep graph representation models is a nontrivial direction. Either enhancing
    the models’ robustness, or conducting adversarial attacks to challenge the robustness
    of graph representations, are promising direction to go for (Tang et al., [2020b](#bib.bib346);
    Geisler et al., [2021](#bib.bib115); Günnemann, [2022](#bib.bib125)).
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 再次类似于许多其他旨在解决实际问题的机器学习方法（Carlini and Wagner, [2017](#bib.bib40)），提高深度图表示模型的鲁棒性是一个非平凡的方向。无论是增强模型的鲁棒性，还是进行对抗攻击以挑战图表示的鲁棒性，都是值得探索的有前景的方向（Tang
    et al., [2020b](#bib.bib346); Geisler et al., [2021](#bib.bib115); Günnemann,
    [2022](#bib.bib125)）。
- en: Conducting adversarial attack on models are centered around manipulating the
    data input. Enhancing a model’s robustness usually works on introducing new tricks
    or new frameworks to the model, or even change the model’s architecture or other
    design details.
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型进行对抗攻击通常围绕着操控数据输入。增强模型的鲁棒性通常涉及引入新的技巧或新的框架到模型中，甚至改变模型的架构或其他设计细节。
- en: 16.1.3\. Adversarial Reprogramming
  id: totrans-966
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.1.3\. 对抗重编程
- en: With the emerge of pre-trained graph neural network models (Hu et al., [2019](#bib.bib146),
    [2020b](#bib.bib147); Qiu et al., [2020a](#bib.bib287)), introducing adversarial
    reprogramming (Elsayed et al., [2018](#bib.bib85); Zheng et al., [2021](#bib.bib460))
    into deep graph representation learning becomes another possibility as well. The
    major difference between adversarial reprogramming and adversarial attack lies
    in whether or not there is a particular target after putting some adversarial
    samples against the model. Adversarial attack requires some small modification
    on the input data samples. An adversarial attack is considered successful once
    the result is influenced. However, under the adversarial reprogramming settings,
    the task succeed if and only if the influenced results can be used for another
    desired task.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 随着预训练图神经网络模型的出现（Hu et al., [2019](#bib.bib146), [2020b](#bib.bib147); Qiu et
    al., [2020a](#bib.bib287)），将对抗重编程（Elsayed et al., [2018](#bib.bib85); Zheng et
    al., [2021](#bib.bib460)）引入深度图表示学习也成为了另一种可能性。对抗重编程和对抗攻击之间的主要区别在于，是否在对模型施加一些对抗样本之后有特定的目标。对抗攻击要求对输入数据样本进行一些小的修改。对抗攻击被认为成功的标准是结果受到影响。然而，在对抗重编程的设置下，只有当受影响的结果可以用于另一个期望的任务时，任务才算成功。
- en: This is to say, without changing the model’s inner structure or fine-tuning
    its parameters, we might be able to use some pre-trained graph models for some
    other tasks that were not planned to be solved by these models in the first place.
    In other deep learning fields, adversarial reprogramming problems are normally
    solved by having the input carefully encoded, and output cleverly mapped. On some
    graph data sets, such as chemical data sets and biology data sets, pretrained
    models are already available. Therefore, there is a possibility that adversarial
    reprogramming could be applied in the future.
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是说，在不改变模型内部结构或微调其参数的情况下，我们可能能够将一些预训练的图模型用于最初并未计划用这些模型解决的其他任务。在其他深度学习领域，对抗性重编程问题通常通过对输入进行仔细编码和对输出进行巧妙映射来解决。在某些图数据集上，如化学数据集和生物数据集，已经有预训练模型可用。因此，将来有可能将对抗性重编程应用于这些数据集。
- en: 16.1.4\. Generalizing to Out of Distribution Data
  id: totrans-969
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.1.4. 泛化到分布外数据
- en: In order to perform better on unobserved data set, in the ideal case, the representation
    we learn should better be able to generalize to some out-of-distribution (OOD)
    data. Being out-of-distribution is not identical to being mis-classified. The
    mis-classified samples are coming from the same distribution of the training data
    but the model fails to classify it correctly, while out-of-distribution refer
    to the case where the sample comes from a distribution other than the training
    data (Hendrycks and Gimpel, [2016](#bib.bib139)). Being able to generalize to
    out-of-distribution data will greatly enhance a model’s reliability in real life.
    And studying out-of-distribution generalized graph representation (Li et al.,
    [2022b](#bib.bib207)) is an opening field (Li et al., [2022c](#bib.bib208)). This
    is partly because of, currently, even the problem of detecting out-of-distribution
    data samples is not fully conquered yet (Hendrycks and Gimpel, [2016](#bib.bib139)).
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在未观察到的数据集上表现得更好，在理想情况下，我们学到的表示应该能够更好地泛化到一些分布外（OOD）数据。分布外并不等同于被错误分类。错误分类的样本来自与训练数据相同的分布，但模型无法正确分类，而分布外则指样本来自于与训练数据不同的分布（Hendrycks
    and Gimpel, [2016](#bib.bib139)）。能够泛化到分布外数据将大大增强模型在现实生活中的可靠性。而研究分布外泛化图表示（Li et
    al., [2022b](#bib.bib207)）是一个前沿领域（Li et al., [2022c](#bib.bib208)）。这部分是因为目前，即使是检测分布外数据样本的问题也尚未完全攻克（Hendrycks
    and Gimpel, [2016](#bib.bib139)）。
- en: In order to do something on the out-of-distribution data samples, we need to
    detect which samples belong to this type first. Detecting OOD samples itself is
    somewhat similar novelty detection, or outlier detection problems (Pimentel et al.,
    [2014](#bib.bib279)). Their major difference is whether or not a well-performed
    model conducting the original tasks remains part of our goal. Novelty detection
    cares only about figuring out who are the outliers; OOD detection requires our
    model to detect the outliers while keep the performance remain unharmed at the
    same time.
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在分布外数据样本上进行操作，我们需要首先检测哪些样本属于这种类型。检测OOD样本本身有点类似于新颖性检测或离群点检测问题（Pimentel et al.,
    [2014](#bib.bib279)）。它们的主要区别在于，是否一个表现良好的模型在执行原始任务时仍然是我们的目标的一部分。新颖性检测只关心找出谁是离群点；而OOD检测则要求我们的模型在发现离群点的同时保持性能不受损害。
- en: 16.1.5\. Interpretability in Graph Representation Learning
  id: totrans-972
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.1.5. 图表示学习中的可解释性
- en: Interpretability concern is another limitation that exists when researchers
    try to apply deep graph representation learning onto some of the emerging application
    fields. For instance, in the field of computational social science, researchers
    are urging for more efforts in integrating explanation and prediction together (Hofman
    et al., [2021](#bib.bib142)). So as drug discovery, being able to explain why
    such structure is chosen instead of another option, is very important (Jiménez-Luna
    et al., [2020](#bib.bib164)). Generally speaking, neural networks are completely
    in black-box mode to human knowledge without taking efforts to make them interpretable
    and explainable. Although more and more tasks are being handled by deep learning
    methods in many fields, the tool remains mysterious to most human beings. Even
    an expert of deep learning can not easily explain to you how the tasks are performed
    and what the model has learned from the data. This situation reduces the trustworthiness
    of the neural network models, prevent human from learning more from the models’
    results, and even limit the potential improvements of the models themselves, without
    sufficient feedback to human beings.
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性问题是当研究人员尝试将深度图表示学习应用于一些新兴应用领域时存在的另一个限制。例如，在计算社会科学领域，研究人员呼吁更多地将解释与预测结合起来（Hofman
    et al., [2021](#bib.bib142)）。在药物发现领域，能够解释为何选择这种结构而不是另一种选择是非常重要的（Jiménez-Luna et
    al., [2020](#bib.bib164)）。一般而言，神经网络对人类知识完全处于黑箱模式，没有努力使其具有解释性和可解释性。尽管越来越多的任务在许多领域由深度学习方法处理，但这个工具对大多数人类仍然神秘。即使是深度学习的专家也不能轻易解释任务是如何完成的，模型从数据中学到了什么。这种情况降低了神经网络模型的可信度，阻止了人类从模型结果中学到更多的知识，甚至限制了模型本身的潜在改进，因为没有足够的反馈给人类。
- en: Seeking for better interpretability is not only some personal interests of companies
    and researchers, in fact, as more and more ethics concern arose since more and
    more black-box decisions were made by AI algorithms, interpretability has become
    a legal requirement (Goodman and Flaxman, [2017](#bib.bib119)).
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 寻求更好的解释性不仅是公司和研究人员的个人兴趣，实际上，随着越来越多的黑箱决策由AI算法做出，解释性已成为法律要求（Goodman and Flaxman,
    [2017](#bib.bib119)）。
- en: Various approaches have been applied, serving for the goal of better interpretability (Zhang
    et al., [2021b](#bib.bib444)). There we find existing works that provide either
    ad-hoc explanation after the results come out, or those actively change the model
    structure to provide better explanations; explanation by providing similar examples,
    by highlighting some attributes to the input features, by making sense of some
    hidden layers and extract semantics from them, or by extracting logical rules;
    we also see local explanations that explain some particular samples, global explanations
    that explain the network as a whole, or hybrid. Most of those existing directions
    make sense in a graph representation learning setting.
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 已经应用了各种方法，旨在实现更好的解释性（Zhang et al., [2021b](#bib.bib444)）。我们发现现有的工作提供了结果出来后进行的临时解释，或那些主动改变模型结构以提供更好解释的工作；通过提供类似示例来解释，通过突出输入特征的一些属性，通过理解一些隐藏层并从中提取语义，或通过提取逻辑规则来解释；我们还看到局部解释解释一些特定样本，全球解释解释整个网络，或混合解释。这些现有方向中的大多数在图表示学习设置中是有意义的。
- en: Not a consensus has been reached on what are the best methods of making a model
    interpretable. Researchers are still actively exploring every possibility, and
    thus there are plenty of challenges and interesting topics in this direction.
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如何使模型具有解释性的最佳方法尚未达成共识。研究人员仍在积极探索各种可能性，因此在这个方向上存在大量挑战和有趣的课题。
- en: 16.1.6\. Causality in Graph Representation Learning
  id: totrans-977
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.1.6. 图表示学习中的因果关系
- en: In recent years, there are increasing research works focusing on combining causality
    and machine learning models (Madumal et al., [2020](#bib.bib253); Hu and Li, [2021](#bib.bib149);
    Richens et al., [2020](#bib.bib297)). It is widely believed that making good use
    of causality will help models gain higher performances. However, finding the right
    way to model causality in many real world scenarios remain super challenging.
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，越来越多的研究工作集中于将因果关系与机器学习模型结合起来（Madumal et al., [2020](#bib.bib253); Hu and
    Li, [2021](#bib.bib149); Richens et al., [2020](#bib.bib297)）。广泛认为，充分利用因果关系将有助于模型获得更高的性能。然而，在许多现实世界场景中找到正确的因果建模方法仍然非常具有挑战性。
- en: 'Something to note is that, the most common kind of graph that comes along the
    causal study, called “causal graph”, is not identical to the kind of graphs we
    are studying in deep graph representation learning. Causal graphs are the kind
    of graph whose nodes are factors and links represents causal relations. Up till
    now, they are among the most reliable tools for causal inference study. Traditionally,
    causal graphs are defined by human experts. Recent works has shown that neural
    networks can help with scalable causal graph generation (Xu et al., [2019a](#bib.bib402)).
    From this perspective, the story can be other-side around: beside using causal
    relations to enhance graph representation learning, it is also possible to use
    graph representation learning strategies to help with causal study.'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，最常见的因果研究图，称为“因果图”，与我们在深度图表示学习中研究的图类型并不相同。因果图是节点表示因子，链接表示因果关系的图。到目前为止，它们是因果推断研究中最可靠的工具之一。传统上，因果图由人类专家定义。最近的研究表明，神经网络可以帮助生成可扩展的因果图（Xu
    et al., [2019a](#bib.bib402)）。从这个角度来看，故事可以反过来：除了使用因果关系来增强图表示学习外，还可以利用图表示学习策略来辅助因果研究。
- en: 16.1.7\. Emerging Application Fields
  id: totrans-980
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.1.7\. 新兴应用领域
- en: Beside the above-mentioned directions solving existing challenges in the deep
    learning world, there are many emerging fields of application that naturally come
    along with the graph-structured data.
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述解决深度学习领域现有挑战的方向，还有许多新兴应用领域自然与图结构数据相关。
- en: For instance, social network analysis and drug discovery. Due to the nature
    of the data, social network interactions and drug molecule structures can be easily
    depicted as graph-structured data. Therefore, deep graph representation learning
    has much to do in these fields (Abbas, [2021](#bib.bib2); Zhu, [2022](#bib.bib474);
    Gaudelet et al., [2021](#bib.bib110)).
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，社交网络分析和药物发现。由于数据的性质，社交网络互动和药物分子结构可以很容易地描绘为图结构数据。因此，深度图表示学习在这些领域有很多应用（Abbas,
    [2021](#bib.bib2); Zhu, [2022](#bib.bib474); Gaudelet et al., [2021](#bib.bib110)）。
- en: 'Some basic problems on social network are easily solved using graph representation
    learning strategies. Those basic problems include node-classification, link-prediction,
    graph-classification, and so on. In practice, those problem settings could refer
    to real-world problems such as: ideology prediction, interaction prediction, analyzing
    a social group, etc. However, social network data typically has many unique features
    that could potentially stop the general-purposed models to perform well. For instance,
    social media data can be sparse, incomplete, and can be extremely imbalanced (Zhao
    et al., [2021c](#bib.bib455)). On the other hand, people have clear goals when
    studying social media data, such as controversy detection (Benslimane et al.,
    [2022](#bib.bib20)), rumor detection (Takahashi and Igata, [2012](#bib.bib342);
    Hamidian and Diab, [2019](#bib.bib128)), mis-information and dis-information detection (Di Domenico
    et al., [2021](#bib.bib72)), or studying the dynamics of the system (Kipf et al.,
    [2018](#bib.bib182)). There are still a lot of open quests to be conquered, where
    deep graph representation learning can help with.'
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: 社交网络上的一些基本问题可以通过图表示学习策略轻松解决。这些基本问题包括节点分类、链接预测、图分类等。在实际应用中，这些问题设置可以参考现实世界的问题，例如：意识形态预测、互动预测、社群分析等。然而，社交网络数据通常具有许多独特的特征，这些特征可能会阻碍通用模型的良好表现。例如，社交媒体数据可能稀疏、不完整，并且可能极度不平衡（Zhao
    et al., [2021c](#bib.bib455)）。另一方面，人们在研究社交媒体数据时有明确的目标，例如争议检测（Benslimane et al.,
    [2022](#bib.bib20)）、谣言检测（Takahashi and Igata, [2012](#bib.bib342); Hamidian and
    Diab, [2019](#bib.bib128)）、虚假信息和误导信息检测（Di Domenico et al., [2021](#bib.bib72)），或研究系统的动态（Kipf
    et al., [2018](#bib.bib182)）。仍然有许多未解的难题可以通过深度图表示学习来帮助解决。
- en: As for drug-discovery, researchers are having some interests in other perspective
    beside simply proposing a set of potentially functional structures, which is widely
    seen today. The other perspectives include having more interpretable results from
    the model’s proposals (Preuer et al., [2019](#bib.bib281); Jiménez-Luna et al.,
    [2020](#bib.bib164)), and to consider synthetic accessibility (Xie et al., [2021](#bib.bib397)).
    These directions are important, in answer to some doubt on AI from the society (Goodman
    and Flaxman, [2017](#bib.bib119)), as well as from the tradition of chemistry
    studies (Schneider et al., [2020](#bib.bib309)). Similar to the challenges we
    faced when combining social science and neural networks, chemical science would
    also prefer the black-box AI models to be interpretable instead. Some chemical
    scientists would also prefer AI tools to provide them with synthetic route instead
    of the targeting structure itself. In practice, proposing new molecule structures
    is usually not the bottleneck, but synthesising is. There are already some existing
    works focusing on conquering this problem (Empel and Koenigs, [2019](#bib.bib86);
    Ishida et al., [2022](#bib.bib157)). But so far there is a gap between chemical
    experiments and AI tools, indicating that there is still plenty of improvement
    to be made.
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 关于药物发现，研究人员对除了简单提出一组潜在功能结构的其他视角也表现出兴趣，这种情况在今天被广泛看到。其他视角包括从模型的提案中获得更具可解释性的结果（Preuer
    等，[2019](#bib.bib281)；Jiménez-Luna 等，[2020](#bib.bib164)），以及考虑合成可及性（Xie 等，[2021](#bib.bib397)）。这些方向非常重要，以回应社会对人工智能的某些疑虑（Goodman
    和 Flaxman，[2017](#bib.bib119)），以及化学研究传统（Schneider 等，[2020](#bib.bib309)）。类似于我们在结合社会科学和神经网络时所面临的挑战，化学科学也更希望黑箱AI模型能够被解释。一些化学科学家也希望AI工具提供合成路线，而不是目标结构本身。在实际操作中，提出新分子结构通常不是瓶颈，而是合成。已经有一些现有工作专注于解决这个问题（Empel
    和 Koenigs，[2019](#bib.bib86)；Ishida 等，[2022](#bib.bib157)）。但到目前为止，化学实验与AI工具之间仍存在差距，表明还有很多改进空间。
- en: Some chemistry researchers also found it useful to have material data better
    organized, given that the molecule structures are becoming increasingly complex,
    and massive amount of research papers are describing the material’s features from
    different aspects (Walsh et al., [2023](#bib.bib356)). This direction might be
    more closely related to knowledge base or even database systems. But in a way,
    given that the polymer structure is typically a node-link graph, graph representation
    learning might be able to help with deadling with such issues.
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: 一些化学研究人员发现，由于分子结构日益复杂且大量研究论文从不同角度描述了材料的特征（Walsh 等，[2023](#bib.bib356)），更好地组织材料数据变得十分有用。这个方向可能与知识库或甚至数据库系统更为相关。但在某种程度上，由于聚合物结构通常是节点-链接图，图表示学习可能有助于处理这些问题。
- en: 16.2\. Theory-Driven Directions
  id: totrans-986
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.2\. 理论驱动的方向
- en: Some other future directions dig into the root of graph theory. More specifically,
    focusing on some fundamental improvement on neural network structure design, or
    better ways of expressing the graph representations. These directions require
    background knowledge of their mathematical backgrounds. All in all, breakthroughs
    in these directions might not end up in immediate impact, but every study in these
    directions has the potential to change the entire field, sooner or later.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些未来方向深入探讨图论的根本问题。更具体地说，关注于神经网络结构设计的基础性改进，或更好的图表示方法。这些方向需要对其数学背景有一定了解。总的来说，这些方向的突破可能不会立即产生影响，但每项研究都有可能在早晚改变整个领域。
- en: 16.2.1\. Mathematical Proof of Feasibility
  id: totrans-988
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.2.1\. 可行性的数学证明
- en: It has been a long-lasting problem that most of the existing deep learning approaches
    lack mathematical proof of their learnability, bound, etc (Bouzerdoum and Pattison,
    [1993](#bib.bib27); Bartlett et al., [2017](#bib.bib17)). This problem relates
    to the difficulty of providing theoretical proof on a complicated structure like
    neural network (Grohs and Voigtlaender, [2021](#bib.bib122)).
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有深度学习方法缺乏数学可学习性证明、界限等问题一直存在（Bouzerdoum 和 Pattison，[1993](#bib.bib27)；Bartlett
    等，[2017](#bib.bib17)）。这个问题与提供复杂结构如神经网络的理论证明的难度有关（Grohs 和 Voigtlaender，[2021](#bib.bib122)）。
- en: 'Currently, most of the theoretical proof aims at figuring out theoretical bounds (Harvey
    et al., [2017](#bib.bib133); Bartlett et al., [2019](#bib.bib18); Karpinski and
    Macintyre, [1997](#bib.bib177)). There are multiple types of bounds with different
    problem settings. Such as: given a known architecture of the model, with input
    data satisfying particular normal distribution, prove that training will converge,
    and provide the estimated number of iterations. Most of these architectures being
    studied are simple, such as those made of multi-layer perceptron (MLP), or simply
    studying the updates of parameters in a single fully-connected layer.'
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数理论证明旨在找出理论界限（Harvey 等，[2017](#bib.bib133)；Bartlett 等，[2019](#bib.bib18)；Karpinski
    和 Macintyre，[1997](#bib.bib177)）。界限有多种类型，适用于不同的问题设置。例如：给定已知的模型架构，输入数据满足特定的正态分布，证明训练将收敛，并提供估计的迭代次数。大多数被研究的架构很简单，例如由多层感知器（MLP）构成的，或只是研究单个全连接层中的参数更新。
- en: In the field of deep graph representation learning, the neural network architectures
    are typically much more complex than MLPs. Graph neural networks (GNNs), since
    the very beginning (Defferrard et al., [2016](#bib.bib67); Kipf and Welling, [2016a](#bib.bib183)),
    involve a lot of approximation and simplification of mathematical theorems. Nowadays,
    most researchers rely heavily on the experimental results. No matter how wild
    an idea is, as long as it finally works out in an experiment, say, being able
    to converge and the results are acceptable, the design is acceptable. All these
    practice makes the entire field somewhat experiments-oriented or experience-oriented,
    while there remains a huge gap between the theoretical proof and the frontier
    of deep graph representation.
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度图表示学习领域，神经网络架构通常比多层感知器（MLPs）复杂得多。图神经网络（GNNs），自最初开始（Defferrard 等，[2016](#bib.bib67)；Kipf
    和 Welling，[2016a](#bib.bib183)），涉及大量数学定理的近似和简化。如今，大多数研究人员严重依赖实验结果。无论一个想法多么疯狂，只要它最终在实验中有效，例如能够收敛并且结果可接受，该设计就是可接受的。所有这些实践使整个领域在一定程度上变得实验导向或经验导向，同时理论证明与深度图表示前沿之间仍然存在巨大的差距。
- en: It will be more than beneficial to the whole field if some researchers can push
    forward these theoretical foundations. However, these problems are incredibly
    challenging.
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一些研究人员能够推进这些理论基础，将对整个领域大有裨益。然而，这些问题极其具有挑战性。
- en: 16.2.2\. Combining Spectral Graph Theory
  id: totrans-993
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.2.2\. 结合谱图理论
- en: Down to the theory foundations, the idea of graph neural networks (Shuman et al.,
    [2013b](#bib.bib322); Defferrard et al., [2016](#bib.bib67); Kipf and Welling,
    [2016a](#bib.bib183)) initially comes from spectral graph theory (Chung, [1997](#bib.bib61)).
    In recent years, many researchers are investigating possible improvement on graph
    representation learning strategies via utilizing spectral graph theory (Chen et al.,
    [2020b](#bib.bib55); Yang et al., [2021](#bib.bib408); MansourLakouraj et al.,
    [2022](#bib.bib257); He et al., [2022](#bib.bib136)). For example, graph Laplacian
    is closely related to many properties, such as the connectivity of a graph. By
    studying the properties of Laplacian, it is possible to provide proof on graph
    neural network models’ properties, and to propose better models with desired advantages,
    such as robustness (Fu et al., [2022](#bib.bib101); Runwal et al., [2022](#bib.bib301)).
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论基础上看，图神经网络的思想（Shuman 等，[2013b](#bib.bib322)；Defferrard 等，[2016](#bib.bib67)；Kipf
    和 Welling，[2016a](#bib.bib183)）最初来自谱图理论（Chung，[1997](#bib.bib61)）。近年来，许多研究人员通过利用谱图理论研究图表示学习策略的可能改进（Chen
    等，[2020b](#bib.bib55)；Yang 等，[2021](#bib.bib408)；MansourLakouraj 等，[2022](#bib.bib257)；He
    等，[2022](#bib.bib136)）。例如，图拉普拉斯与许多属性密切相关，如图的连通性。通过研究拉普拉斯的属性，可以对图神经网络模型的属性提供证明，并提出具有期望优势的更好模型，如鲁棒性（Fu
    等，[2022](#bib.bib101)；Runwal 等，[2022](#bib.bib301)）。
- en: Spectral graph theory provides a lot of useful insights into graph representation
    learning from a new perspective. There are a lot to be done in this direction.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: 谱图理论从一个新的角度提供了对图表示学习的许多有用见解。在这一方向上还有很多工作要做。
- en: 16.2.3\. From Graph to Manifolds
  id: totrans-996
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 16.2.3\. 从图到流形
- en: Many researchers are devoted to the direction of learning graph representation
    in non-Euclidean spaces (Asif et al., [2021](#bib.bib11); Saxena et al., [2020](#bib.bib306)).
    That is to say, to embed and to compute on some other spaces that are not Euclidean,
    such as, hyperbolic and spherical spaces.
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究人员致力于在非欧几里得空间中学习图表示的方向（Asif et al., [2021](#bib.bib11); Saxena et al., [2020](#bib.bib306)）。也就是说，将图嵌入并计算在一些非欧几里得空间中，例如双曲空间和球面空间。
- en: Theoretical reasoning and experimental results have shown certain advantages
    of working on manifolds instead of standard Euclidean space. It is believed that
    these advantages are brought by their abilities to capture complex correlations
    on the surface manifold (Zhou et al., [2022a](#bib.bib462)). Besides, researchers
    have shown that, by combining standard graph representation learning strategies
    and manifold assumptions, models work better on preserving and acquiring the locality
    and similarity relationships  (Fu and Liu, [2021](#bib.bib102)). Intuitively,
    sometimes two nodes’ embeddings are regarded way too similar in Euclidean space,
    but in non-Euclidean space they are easily distinguishable.
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 理论推理和实验结果表明，在流形上工作相较于标准欧几里得空间具有一定的优势。相信这些优势源于其捕捉表面流形上复杂关联的能力（Zhou et al., [2022a](#bib.bib462)）。此外，研究人员已表明，通过结合标准图表示学习策略和流形假设，模型在保持和获取局部性和相似性关系方面表现更好（Fu
    and Liu, [2021](#bib.bib102)）。直观地说，有时在欧几里得空间中，两个节点的嵌入被认为过于相似，但在非欧几里得空间中，它们容易区分。
- en: 17\. Conclusion By Wei Ju
  id: totrans-999
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17\. 结论 作者：魏炬
- en: 'In this survey, we present a comprehensive and up-to-date overview of deep
    graph representation learning. We present a novel taxonomy of existing algorithms
    categorized into GNN architectures, learning paradigms, and applications. Technically,
    we first summarize the ways of GNN architectures namely graph convolutions, graph
    kernel neural networks, graph pooling, and graph transformer. Based on the different
    training objectives, we present three types of the most recent advanced learning
    paradigms namely: supervised/semi-supervised learning on graphs, graph self-supervised
    learning, and graph structure learning. Then, we provide several promising applications
    to demonstrate the effectiveness of deep graph representation learning. Last but
    not least, we discuss the future directions in deep graph representation learning
    that have potential opportunities.'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次调查中，我们提供了对深度图表示学习的全面且最新的概述。我们提出了一种新颖的算法分类法，将现有算法分为 GNN 架构、学习范式和应用。技术上，我们首先总结了
    GNN 架构的方式，即图卷积、图核神经网络、图池化和图变换器。基于不同的训练目标，我们提出了三种最新的高级学习范式，即：图上的监督/半监督学习、图自监督学习和图结构学习。接着，我们提供了若干有前景的应用以展示深度图表示学习的有效性。最后，我们讨论了深度图表示学习中具有潜力的未来方向。
- en: Acknowledgements.
  id: totrans-1001
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: The authors are grateful to the anonymous reviewers for critically reading the
    manuscript and for giving important suggestions to improve their paper. This paper
    is partially supported by the National Key Research and Development Program of
    China with Grant No. 2018AAA0101902 and the National Natural Science Foundation
    of China (NSFC Grant No. 62276002).
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢匿名评审对稿件的批判性阅读及提出的重要建议，以改进论文。本论文部分得到中国国家重点研发计划资助（资助号：2018AAA0101902）及中国国家自然科学基金（NSFC
    资助号：62276002）。
- en: References
  id: totrans-1003
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Abbas (2021) Ash Mohammad Abbas. 2021. Social network analysis using deep learning:
    applications and schemes. *Social Network Analysis and Mining* 11, 1 (2021), 1–21.'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbas (2021) Ash Mohammad Abbas. 2021. 使用深度学习进行社交网络分析：应用和方案。*社会网络分析与挖掘* 11,
    1 (2021), 1–21。
- en: Ahmed et al. (2013) Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja
    Josifovski, and Alexander J Smola. 2013. Distributed large-scale natural graph
    factorization. In *Proceedings of the 22nd international conference on World Wide
    Web*. 37–48.
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmed et al. (2013) Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja
    Josifovski, and Alexander J Smola. 2013. 分布式大规模自然图因式分解。发表于 *第22届国际万维网大会论文集*。37–48。
- en: Ahmed et al. (2021) Imtiaz Ahmed, Travis Galoppo, Xia Hu, and Yu Ding. 2021.
    Graph regularized autoencoder and its application in unsupervised anomaly detection.
    *IEEE Transactions on Pattern Analysis and Machine Intelligence* 44, 8 (2021),
    4110–4124.
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmed et al. (2021) Imtiaz Ahmed, Travis Galoppo, Xia Hu, and Yu Ding. 2021.
    图正则化自编码器及其在无监督异常检测中的应用。*IEEE 计算机学会模式分析与机器智能学报* 44, 8 (2021), 4110–4124。
- en: 'Al-Rfou et al. (2019) Rami Al-Rfou, Bryan Perozzi, and Dustin Zelle. 2019.
    Ddgk: Learning graph representations for deep divergence graph kernels. In *The
    World Wide Web Conference*. 37–48.'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Al-Rfou 等 (2019) Rami Al-Rfou、Bryan Perozzi 和 Dustin Zelle。2019年。Ddgk：用于深度散度图内核的图表示学习。发表于
    *全球信息网会议*。37–48。
- en: AlBadani et al. (2022) Barakat AlBadani, Ronghua Shi, Jian Dong, Raeed Al-Sabri,
    and Oloulade Babatounde Moctard. 2022. Transformer-Based Graph Convolutional Network
    for Sentiment Analysis. *Applied Sciences* 12, 3 (2022), 1316.
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlBadani 等 (2022) Barakat AlBadani、Ronghua Shi、Jian Dong、Raeed Al-Sabri 和 Oloulade
    Babatounde Moctard。2022年。基于 Transformer 的图卷积网络用于情感分析。*应用科学* 12, 3 (2022), 1316。
- en: Alon and Yahav (2020) Uri Alon and Eran Yahav. 2020. On the bottleneck of graph
    neural networks and its practical implications. *arXiv preprint arXiv:2006.05205*
    (2020).
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alon 和 Yahav (2020) Uri Alon 和 Eran Yahav。2020年。图神经网络的瓶颈及其实际影响。*arXiv 预印本 arXiv:2006.05205*
    (2020)。
- en: Altae-Tran et al. (2017) Han Altae-Tran, Bharath Ramsundar, Aneesh S Pappu,
    and Vijay Pande. 2017. Low data drug discovery with one-shot learning. *ACS central
    science* 3, 4 (2017), 283–293.
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Altae-Tran 等 (2017) Han Altae-Tran、Bharath Ramsundar、Aneesh S Pappu 和 Vijay
    Pande。2017年。利用一次性学习进行低数据药物发现。*ACS 中央科学* 3, 4 (2017), 283–293。
- en: 'Anderson et al. (2019) Brandon Anderson, Truong Son Hy, and Risi Kondor. 2019.
    Cormorant: Covariant Molecular Neural Networks. In *NeurIPS*, Vol. 32. [https://proceedings.neurips.cc/paper/2019/file/03573b32b2746e6e8ca98b9123f2249b-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/03573b32b2746e6e8ca98b9123f2249b-Paper.pdf)'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson 等 (2019) Brandon Anderson、Truong Son Hy 和 Risi Kondor。2019年。Cormorant：协变分子神经网络。发表于
    *NeurIPS*，第32卷。 [https://proceedings.neurips.cc/paper/2019/file/03573b32b2746e6e8ca98b9123f2249b-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/03573b32b2746e6e8ca98b9123f2249b-Paper.pdf)
- en: Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017.
    Wasserstein generative adversarial networks. In *International conference on machine
    learning*. PMLR, 214–223.
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky 等 (2017) Martin Arjovsky、Soumith Chintala 和 Léon Bottou。2017年。Wasserstein
    生成对抗网络。发表于 *国际机器学习会议*。PMLR, 214–223。
- en: 'Asif et al. (2021) Nurul A Asif, Yeahia Sarker, Ripon K Chakrabortty, Michael J
    Ryan, Md Hafiz Ahamed, Dip K Saha, Faisal R Badal, Sajal K Das, Md Firoz Ali,
    Sumaya I Moyeen, et al. 2021. Graph neural network: A comprehensive review on
    non-euclidean space. *IEEE Access* 9 (2021), 60588–60606.'
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asif 等 (2021) Nurul A Asif、Yeahia Sarker、Ripon K Chakrabortty、Michael J Ryan、Md
    Hafiz Ahamed、Dip K Saha、Faisal R Badal、Sajal K Das、Md Firoz Ali、Sumaya I Moyeen
    等。2021年。图神经网络：对非欧几里得空间的全面综述。*IEEE Access* 9 (2021), 60588–60606。
- en: 'Assouel et al. (2018) Rim Assouel, Mohamed Ahmed, Marwin H Segler, Amir Saffari,
    and Yoshua Bengio. 2018. Defactor: Differentiable edge factorization-based probabilistic
    graph generation. *arXiv preprint arXiv:1811.09766* (2018).'
  id: totrans-1015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Assouel 等 (2018) Rim Assouel、Mohamed Ahmed、Marwin H Segler、Amir Saffari 和 Yoshua
    Bengio。2018年。Defactor：基于可微边因子分解的概率图生成。*arXiv 预印本 arXiv:1811.09766* (2018)。
- en: Bacciu et al. (2020) Davide Bacciu, Federico Errica, Alessio Micheli, and Marco
    Podda. 2020. A gentle introduction to deep learning for graphs. *Neural Networks*
    129 (2020), 203–221.
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bacciu 等 (2020) Davide Bacciu、Federico Errica、Alessio Micheli 和 Marco Podda。2020年。图的深度学习温和介绍。*神经网络*
    129 (2020), 203–221。
- en: Bai et al. (2020) Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020.
    Adaptive graph convolutional recurrent network for traffic forecasting. *Advances
    in neural information processing systems* 33 (2020), 17804–17815.
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等 (2020) Lei Bai、Lina Yao、Can Li、Xianzhi Wang 和 Can Wang。2020年。用于交通预测的自适应图卷积递归网络。*神经信息处理系统进展*
    33 (2020), 17804–17815。
- en: 'Bai et al. (2019) Xiaomei Bai, Mengyang Wang, Ivan Lee, Zhuo Yang, Xiangjie
    Kong, and Feng Xia. 2019. Scientific paper recommendation: A survey. *Ieee Access*
    7 (2019), 9324–9339.'
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等 (2019) Xiaomei Bai、Mengyang Wang、Ivan Lee、Zhuo Yang、Xiangjie Kong 和 Feng
    Xia。2019年。科学论文推荐：综述。*IEEE Access* 7 (2019), 9324–9339。
- en: Balcilar et al. (2021) Muhammet Balcilar, Pierre Héroux, Benoit Gauzere, Pascal
    Vasseur, Sébastien Adam, and Paul Honeine. 2021. Breaking the limits of message
    passing graph neural networks. In *International Conference on Machine Learning*.
    PMLR, 599–608.
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balcilar 等 (2021) Muhammet Balcilar、Pierre Héroux、Benoit Gauzere、Pascal Vasseur、Sébastien
    Adam 和 Paul Honeine。2021年。打破消息传递图神经网络的极限。发表于 *国际机器学习会议*。PMLR, 599–608。
- en: Bartlett et al. (2017) Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky.
    2017. Spectrally-normalized margin bounds for neural networks. *Advances in neural
    information processing systems* 30 (2017).
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartlett 等 (2017) Peter L Bartlett、Dylan J Foster 和 Matus J Telgarsky。2017年。神经网络的光谱归一化边界。*神经信息处理系统进展*
    30 (2017)。
- en: Bartlett et al. (2019) Peter L Bartlett, Nick Harvey, Christopher Liaw, and
    Abbas Mehrabian. 2019. Nearly-tight VC-dimension and pseudodimension bounds for
    piecewise linear neural networks. *The Journal of Machine Learning Research* 20,
    1 (2019), 2285–2301.
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartlett 等（2019）Peter L Bartlett, Nick Harvey, Christopher Liaw 和 Abbas Mehrabian。2019。分段线性神经网络的近紧
    VC 维度和伪维度界限。*机器学习研究杂志* 第 20 卷，第 1 期（2019），2285–2301 页。
- en: Batzner et al. (2021) Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger,
    Jonathan P. Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E. Smidt, and Boris
    Kozinsky. 2021. E(3)-Equivariant Graph Neural Networks for Data-Efficient and
    Accurate Interatomic Potentials. arXiv:2101.03164 [physics.comp-ph]
  id: totrans-1022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Batzner 等（2021）Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan
    P. Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E. Smidt 和 Boris Kozinsky。2021。E(3)
    等变图神经网络用于数据高效和准确的原子间势能。arXiv:2101.03164 [physics.comp-ph]
- en: Benslimane et al. (2022) Samy Benslimane, Jérôme Azé, Sandra Bringay, Maximilien
    Servajean, and Caroline Mollevi. 2022. A text and GNN based controversy detection
    method on social media. *World Wide Web* (2022), 1–27.
  id: totrans-1023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benslimane 等（2022）Samy Benslimane, Jérôme Azé, Sandra Bringay, Maximilien Servajean
    和 Caroline Mollevi。2022。基于文本和 GNN 的社交媒体争议检测方法。*万维网*（2022），1–27 页。
- en: Berg et al. (2017) Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017.
    Graph convolutional matrix completion. *arXiv preprint arXiv:1706.02263* (2017).
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berg 等（2017）Rianne van den Berg, Thomas N Kipf 和 Max Welling。2017。图卷积矩阵补全。*arXiv
    预印本 arXiv:1706.02263*（2017）。
- en: Bian et al. (2020) Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang,
    Yu Rong, and Junzhou Huang. 2020. Rumor detection on social media with bi-directional
    graph convolutional networks. In *Proceedings of the AAAI conference on artificial
    intelligence*, Vol. 34. 549–556.
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bian 等（2020）Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu
    Rong 和 Junzhou Huang。2020。使用双向图卷积网络进行社交媒体上的谣言检测。发表于 *AAAI 人工智能会议论文集*，第 34 卷，549–556
    页。
- en: Bianchi et al. (2020) Filippo Maria Bianchi, Daniele Grattarola, and Cesare
    Alippi. 2020. Spectral clustering with graph neural networks for graph pooling.
    In *International Conference on Machine Learning*. PMLR, 874–883.
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianchi 等（2020）Filippo Maria Bianchi, Daniele Grattarola 和 Cesare Alippi。2020。图神经网络的谱聚类与图池化。发表于
    *国际机器学习会议*。PMLR，874–883 页。
- en: Bo et al. (2021) Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. 2021. Beyond
    low-frequency information in graph convolutional networks. In *Proceedings of
    the AAAI Conference on Artificial Intelligence*, Vol. 35. 3950–3957.
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bo 等（2021）Deyu Bo, Xiao Wang, Chuan Shi 和 Huawei Shen。2021。图卷积网络中的低频信息超越。发表于
    *AAAI 人工智能会议论文集*，第 35 卷，3950–3957 页。
- en: Borgwardt and Kriegel (2005) Karsten M Borgwardt and Hans-Peter Kriegel. 2005.
    Shortest-path kernels on graphs. In *Fifth IEEE international conference on data
    mining (ICDM)*. IEEE, 8–pp.
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgwardt 和 Kriegel（2005）Karsten M Borgwardt 和 Hans-Peter Kriegel。2005。图上的最短路径核。发表于
    *第五届 IEEE 国际数据挖掘会议（ICDM）*。IEEE，8–页。
- en: Bouritsas et al. (2022) Giorgos Bouritsas, Fabrizio Frasca, Stefanos P Zafeiriou,
    and Michael Bronstein. 2022. Improving graph neural network expressivity via subgraph
    isomorphism counting. *IEEE Transactions on Pattern Analysis and Machine Intelligence*
    (2022).
  id: totrans-1029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bouritsas 等（2022）Giorgos Bouritsas, Fabrizio Frasca, Stefanos P Zafeiriou 和
    Michael Bronstein。2022。通过子图同构计数提高图神经网络的表达能力。*IEEE 模式分析与机器智能汇刊*（2022）。
- en: Bouzerdoum and Pattison (1993) Abdesselam Bouzerdoum and Tim R Pattison. 1993.
    Neural network for quadratic optimization with bound constraints. *IEEE transactions
    on neural networks* 4, 2 (1993), 293–304.
  id: totrans-1030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bouzerdoum 和 Pattison（1993）Abdesselam Bouzerdoum 和 Tim R Pattison。1993。用于带约束的二次优化的神经网络。*IEEE
    神经网络汇刊* 第 4 卷，第 2 期（1993），293–304 页。
- en: Box and Pierce (1970) George EP Box and David A Pierce. 1970. Distribution of
    residual autocorrelations in autoregressive-integrated moving average time series
    models. *Journal of the American statistical Association* 65, 332 (1970), 1509–1526.
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Box 和 Pierce（1970）George EP Box 和 David A Pierce。1970。自回归积分滑动平均时间序列模型中残差自相关分布。*美国统计协会杂志*
    第 65 卷，第 332 期（1970），1509–1526 页。
- en: Brandstetter et al. (2022) Johannes Brandstetter, Rob Hesselink, Elise van der
    Pol, Erik J Bekkers, and Max Welling. 2022. Geometric and Physical Quantities
    improve E(3) Equivariant Message Passing. In *ICLR*. [https://openreview.net/forum?id=_xwr8gOBeV1](https://openreview.net/forum?id=_xwr8gOBeV1)
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brandstetter 等（2022）Johannes Brandstetter, Rob Hesselink, Elise van der Pol,
    Erik J Bekkers 和 Max Welling。2022。几何和物理量提高 E(3) 等变信息传递。发表于 *ICLR*。[https://openreview.net/forum?id=_xwr8gOBeV1](https://openreview.net/forum?id=_xwr8gOBeV1)
- en: Bruna et al. (2013) Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun.
    2013. Spectral networks and locally connected networks on graphs. *arXiv preprint
    arXiv:1312.6203* (2013).
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruna等 (2013) Joan Bruna, Wojciech Zaremba, Arthur Szlam和Yann LeCun. 2013. 图上的谱网络和局部连接网络。*arXiv预印本
    arXiv:1312.6203* (2013)。
- en: 'Bui et al. (2021) Khac-Hoai Nam Bui, Jiho Cho, and Hongsuk Yi. 2021. Spatial-temporal
    graph neural network for traffic forecasting: An overview and open research issues.
    *Applied Intelligence* (2021), 1–12.'
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bui等 (2021) Khac-Hoai Nam Bui, Jiho Cho和Hongsuk Yi. 2021. 空间-时间图神经网络用于交通预测：概述与开放研究问题。*应用智能*
    (2021), 1–12。
- en: Cai and Lam (2020) Deng Cai and Wai Lam. 2020. Graph transformer for graph-to-sequence
    learning. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 34\. 7464–7471.
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai和Lam (2020) Deng Cai和Wai Lam. 2020. 图变换器用于图到序列的学习。在*AAAI人工智能会议论文集*，第34卷。7464–7471。
- en: 'Cai et al. (2018) Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang.
    2018. A comprehensive survey of graph embedding: Problems, techniques, and applications.
    *IEEE Transactions on Knowledge and Data Engineering* 30, 9 (2018), 1616–1637.'
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai等 (2018) Hongyun Cai, Vincent W Zheng和Kevin Chen-Chuan Chang. 2018. 图嵌入的综合调查：问题、技术和应用。*IEEE知识与数据工程学报*
    30, 9 (2018), 1616–1637。
- en: 'Camacho et al. (2020) David Camacho, Ángel Panizo-LLedot, Gema Bello-Orgaz,
    Antonio Gonzalez-Pardo, and Erik Cambria. 2020. The four dimensions of social
    network analysis: An overview of research methods, applications, and software
    tools. *Information Fusion* 63 (2020), 88–120.'
  id: totrans-1037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Camacho等 (2020) David Camacho, Ángel Panizo-LLedot, Gema Bello-Orgaz, Antonio
    Gonzalez-Pardo和Erik Cambria. 2020. 社会网络分析的四个维度：研究方法、应用和软件工具的概述。*信息融合* 63 (2020),
    88–120。
- en: Cao et al. (2021a) Defu Cao, Jiachen Li, Hengbo Ma, and Masayoshi Tomizuka.
    2021a. Spectral temporal graph neural network for trajectory prediction. In *2021
    IEEE International Conference on Robotics and Automation (ICRA)*. IEEE, 1839–1845.
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等 (2021a) Defu Cao, Jiachen Li, Hengbo Ma和Masayoshi Tomizuka. 2021a. 谱时间图神经网络用于轨迹预测。在*2021年IEEE国际机器人与自动化会议（ICRA）*。IEEE，1839–1845。
- en: Cao et al. (2021b) Jinzhou Cao, Qingquan Li, Wei Tu, Qili Gao, Rui Cao, and
    Chen Zhong. 2021b. Resolving urban mobility networks from individual travel graphs
    using massive-scale mobile phone tracking data. *Cities* 110 (2021), 103077.
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等 (2021b) Jinzhou Cao, Qingquan Li, Wei Tu, Qili Gao, Rui Cao和Chen Zhong.
    2021b. 从个体旅行图中解析城市移动网络，使用大规模手机跟踪数据。*城市* 110 (2021), 103077。
- en: 'Cao et al. (2015) Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning
    graph representations with global structural information. In *Proceedings of the
    24th ACM international on conference on information and knowledge management*.
    891–900.'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等 (2015) Shaosheng Cao, Wei Lu和Qiongkai Xu. 2015. Grarep：利用全局结构信息学习图表示。在*第24届ACM国际信息与知识管理会议论文集*。891–900。
- en: Cao et al. (2016) Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep neural
    networks for learning graph representations. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 30.
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等 (2016) Shaosheng Cao, Wei Lu和Qiongkai Xu. 2016. 深度神经网络用于学习图表示。在*AAAI人工智能会议论文集*，第30卷。
- en: 'Carion et al. (2020) Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
    Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection
    with transformers. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
    UK, August 23–28, 2020, Proceedings, Part I 16*. Springer, 213–229.'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carion等 (2020) Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier,
    Alexander Kirillov和Sergey Zagoruyko. 2020. 基于变换器的端到端目标检测。在*计算机视觉–ECCV 2020：第16届欧洲会议，英国格拉斯哥，2020年8月23日至28日，会议论文集，第16部分*。Springer，213–229。
- en: Carlini and Wagner (2017) Nicholas Carlini and David Wagner. 2017. Towards evaluating
    the robustness of neural networks. In *2017 ieee symposium on security and privacy
    (sp)*. Ieee, 39–57.
  id: totrans-1043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini和Wagner (2017) Nicholas Carlini和David Wagner. 2017. 评估神经网络的鲁棒性。In *2017
    IEEE安全与隐私研讨会（SP）*。IEEE，39–57。
- en: 'Chami et al. (2022) Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher
    Ré, and Kevin Murphy. 2022. Machine learning on graphs: A model and comprehensive
    taxonomy. *Journal of Machine Learning Research* 23, 89 (2022), 1–64.'
  id: totrans-1044
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chami等 (2022) Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher Ré和Kevin
    Murphy. 2022. 图上的机器学习：模型与全面分类。*机器学习研究期刊* 23, 89 (2022), 1–64。
- en: Chauhan et al. (2020) Jatin Chauhan, Deepak Nathani, and Manohar Kaul. 2020.
    Few-shot learning on graphs via super-classes based on graph spectral measures.
    *arXiv preprint arXiv:2002.12815* (2020).
  id: totrans-1045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chauhan等 (2020) Jatin Chauhan, Deepak Nathani和Manohar Kaul. 2020. 基于图谱测度的超类的图上少量学习。*arXiv预印本
    arXiv:2002.12815* (2020)。
- en: 'Chen et al. (2020h) Bo Chen, Jing Zhang, Jie Tang, Lingfan Cai, Zhaoyu Wang,
    Shu Zhao, Hong Chen, and Cuiping Li. 2020h. Conna: Addressing name disambiguation
    on the fly. *IEEE Transactions on Knowledge and Data Engineering* (2020).'
  id: totrans-1046
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2020h）博·陈、晶·张、杰·唐、凌凡·蔡、赵宇·王、舒·赵、洪·陈和崔平·李。2020h。Conna：实时解决姓名歧义。*IEEE知识与数据工程交易*（2020）。
- en: 'Chen et al. (2022b) Chaoqi Chen, Yushuang Wu, Qiyuan Dai, Hong-Yu Zhou, Mutian
    Xu, Sibei Yang, Xiaoguang Han, and Yizhou Yu. 2022b. A Survey on Graph Neural
    Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective.
    *arXiv preprint arXiv:2209.13232* (2022).'
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2022b）超奇·陈、雨霜·吴、启元·戴、洪宇·周、穆天·徐、思北·杨、晓光·韩和逸洲·余。2022b。计算机视觉中的图神经网络和图变换器综述：任务导向视角。*arXiv预印本
    arXiv:2209.13232*（2022）。
- en: Chen et al. (2020c) Dexiong Chen, Laurent Jacob, and Julien Mairal. 2020c. Convolutional
    Kernel Networks for Graph-Structured Data. *arXiv preprint arXiv:2003.05189* (2020).
  id: totrans-1048
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2020c）德修·陈、劳伦特·雅各布和朱利安·迈拉尔。2020c。图结构数据的卷积核网络。*arXiv预印本 arXiv:2003.05189*（2020）。
- en: Chen et al. (2020d) Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu
    Sun. 2020d. Measuring and relieving the over-smoothing problem for graph neural
    networks from the topological view. In *Proceedings of the AAAI conference on
    artificial intelligence*, Vol. 34\. 3438–3445.
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2020d）德力·陈、彦凯·林、伟·李、鹏·李、杰·周和旭·孙。2020d。从拓扑视角测量和缓解图神经网络的过平滑问题。载于*AAAI人工智能会议论文集*，第34卷。3438–3445。
- en: Chen et al. (2022a) Dexiong Chen, Leslie O’Bray, and Karsten Borgwardt. 2022a.
    Structure-aware transformer for graph representation learning. In *International
    Conference on Machine Learning*. PMLR, 3469–3489.
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2022a）德修·陈、莱斯利·奥布雷和卡斯滕·博尔格瓦特。2022a。结构感知变换器用于图表示学习。载于*国际机器学习大会*。PMLR，3469–3489。
- en: 'Chen et al. (2020f) Fenxiao Chen, Yun-Cheng Wang, Bin Wang, and C-C Jay Kuo.
    2020f. Graph representation learning: a survey. *APSIPA Transactions on Signal
    and Information Processing* 9 (2020), e15.'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2020f）芬晓·陈、云城·王、宾·王和C-C 杰伊·阮。2020f。图表示学习：综述。*APSIPA信号与信息处理交易* 9（2020），e15。
- en: Chen et al. (2020e) Guimin Chen, Yuanhe Tian, and Yan Song. 2020e. Joint aspect
    extraction and sentiment analysis with directional graph convolutional networks.
    In *Proceedings of the 28th international conference on computational linguistics*.
    272–279.
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2020e）桂敏·陈、袁和·田和闫·宋。2020e。带有方向图卷积网络的联合方面提取和情感分析。载于*第28届国际计算语言学大会论文集*，272–279。
- en: Chen et al. (2021b) Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan
    Laptev. 2021b. History aware multimodal transformer for vision-and-language navigation.
    *Advances in Neural Information Processing Systems* 34 (2021), 5834–5847.
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021b）施哲·陈、皮埃尔-路易斯·古赫、科尔德利亚·施密德和伊万·拉普捷夫。2021b。历史感知多模态变换器用于视觉与语言导航。*神经信息处理系统进展*
    34（2021），5834–5847。
- en: Chen et al. (2019) Ting Chen, Song Bian, and Yizhou Sun. 2019. Are powerful
    graph neural nets necessary? a dissection on graph classification. *arXiv preprint
    arXiv:1905.04579* (2019).
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019）廷·陈、宋·边和逸洲·孙。2019。强大的图神经网络是否必要？对图分类的解剖。*arXiv预印本 arXiv:1905.04579*（2019）。
- en: Chen et al. (2021c) Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang.
    2021c. Semi-supervised semantic segmentation with cross pseudo supervision. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    2613–2622.
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021c）肖康·陈、余辉·袁、刚·曾和晶东·王。2021c。具有交叉伪监督的半监督语义分割。载于*IEEE/CVF计算机视觉与模式识别会议论文集*，2613–2622。
- en: 'Chen et al. (2020g) Yu Chen, Lingfei Wu, and Mohammed Zaki. 2020g. Iterative
    deep graph learning for graph neural networks: Better and robust node embeddings.
    *Advances in neural information processing systems* 33 (2020), 19314–19326.'
  id: totrans-1056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2020g）余·陈、凌飞·吴和穆罕默德·扎基。2020g。用于图神经网络的迭代深度图学习：更好且稳健的节点嵌入。*神经信息处理系统进展* 33（2020），19314–19326。
- en: Chen et al. (2021a) Zekai Chen, Dingshuo Chen, Xiao Zhang, Zixuan Yuan, and
    Xiuzhen Cheng. 2021a. Learning graph structures with transformer for multivariate
    time series anomaly detection in iot. *IEEE Internet of Things Journal* (2021).
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021a）泽凯·陈、鼎硕·陈、肖·张、子轩·袁和秀珍·程。2021a。使用变换器学习图结构用于物联网中的多变量时间序列异常检测。*IEEE物联网期刊*（2021）。
- en: 'Chen et al. (2020b) Zhiqian Chen, Fanglan Chen, Lei Zhang, Taoran Ji, Kaiqun
    Fu, Liang Zhao, Feng Chen, Lingfei Wu, Charu Aggarwal, and Chang-Tien Lu. 2020b.
    Bridging the gap between spatial and spectral domains: A survey on graph neural
    networks. *arXiv preprint arXiv:2002.11867* (2020).'
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2020b) Zhiqian Chen, Fanglan Chen, Lei Zhang, Taoran Ji, Kaiqun Fu,
    Liang Zhao, Feng Chen, Lingfei Wu, Charu Aggarwal, 和 Chang-Tien Lu。2020b。弥合空间域和光谱域之间的差距：关于图神经网络的调查。*arXiv
    预印本 arXiv:2002.11867* (2020)。
- en: Chen et al. (2020a) Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna.
    2020a. Can graph neural networks count substructures? *Advances in neural information
    processing systems* 33 (2020), 10383–10395.
  id: totrans-1059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2020a) Zhengdao Chen, Lei Chen, Soledad Villar, 和 Joan Bruna。2020a。图神经网络能否计数子结构？*神经信息处理系统进展*
    33 (2020)，10383–10395。
- en: Cho and Yu (2018) Haeran Cho and Yi Yu. 2018. Link prediction for interdisciplinary
    collaboration via co-authorship network. *Social Network Analysis and Mining*
    8, 1 (2018), 1–12.
  id: totrans-1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 和 Yu (2018) Haeran Cho 和 Yi Yu。2018。通过合作网络进行跨学科合作的链接预测。*社会网络分析与挖掘* 8, 1
    (2018)，1–12。
- en: Choi et al. (2022) Jeongwhan Choi, Hwangyong Choi, Jeehyun Hwang, and Noseong
    Park. 2022. Graph neural controlled differential equations for traffic forecasting.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 36. 6367–6374.
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等 (2022) Jeongwhan Choi, Hwangyong Choi, Jeehyun Hwang, 和 Noseong Park。2022。用于交通预测的图神经控制微分方程。在
    *AAAI 人工智能会议论文集*，第 36 卷。6367–6374。
- en: Chouldechova and Roth (2018) Alexandra Chouldechova and Aaron Roth. 2018. The
    frontiers of fairness in machine learning. *arXiv preprint arXiv:1810.08810* (2018).
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chouldechova 和 Roth (2018) Alexandra Chouldechova 和 Aaron Roth。2018。机器学习中的公平性前沿。*arXiv
    预印本 arXiv:1810.08810* (2018)。
- en: Chuan et al. (2018) Pham Minh Chuan, Le Hoang Son, Mumtaz Ali, Tran Dinh Khang,
    Le Thanh Huong, and Nilanjan Dey. 2018. Link prediction in co-authorship networks
    based on hybrid content similarity metric. *Applied Intelligence* 48, 8 (2018),
    2470–2486.
  id: totrans-1063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chuan 等 (2018) Pham Minh Chuan, Le Hoang Son, Mumtaz Ali, Tran Dinh Khang, Le
    Thanh Huong, 和 Nilanjan Dey。2018。基于混合内容相似度度量的合作网络链接预测。*应用智能* 48, 8 (2018)，2470–2486。
- en: Chung (1997) Fan RK Chung. 1997. *Spectral graph theory*. Vol. 92. American
    Mathematical Soc.
  id: totrans-1064
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung (1997) Fan RK Chung。1997。*光谱图论*。第 92 卷。美国数学学会。
- en: Chung et al. (2014) Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua
    Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence
    modeling. *arXiv preprint arXiv:1412.3555* (2014).
  id: totrans-1065
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等 (2014) Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, 和 Yoshua Bengio。2014。门控递归神经网络在序列建模上的实证评估。*arXiv
    预印本 arXiv:1412.3555* (2014)。
- en: Coley et al. (2017) Connor W Coley, Regina Barzilay, William H Green, Tommi S
    Jaakkola, and Klavs F Jensen. 2017. Convolutional embedding of attributed molecular
    graphs for physical property prediction. *Journal of chemical information and
    modeling* 57, 8 (2017), 1757–1772.
  id: totrans-1066
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coley 等 (2017) Connor W Coley, Regina Barzilay, William H Green, Tommi S Jaakkola,
    和 Klavs F Jensen。2017。属性分子图的卷积嵌入用于物理性质预测。*化学信息与建模杂志* 57, 8 (2017)，1757–1772。
- en: Connor et al. (1994) Jerome T Connor, R Douglas Martin, and Les E Atlas. 1994.
    Recurrent neural networks and robust time series prediction. *IEEE transactions
    on neural networks* 5, 2 (1994), 240–254.
  id: totrans-1067
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Connor 等 (1994) Jerome T Connor, R Douglas Martin, 和 Les E Atlas。1994。递归神经网络与稳健时间序列预测。*IEEE
    神经网络交易* 5, 2 (1994)，240–254。
- en: Corso et al. (2020) Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro
    Liò, and Petar Veličković. 2020. Principal neighbourhood aggregation for graph
    nets. *Advances in Neural Information Processing Systems* 33 (2020), 13260–13271.
  id: totrans-1068
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Corso 等 (2020) Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò,
    和 Petar Veličković。2020。图网的主要邻域聚合。*神经信息处理系统进展* 33 (2020)，13260–13271。
- en: 'De Cao and Kipf (2018) Nicola De Cao and Thomas Kipf. 2018. MolGAN: An implicit
    generative model for small molecular graphs. *arXiv preprint arXiv:1805.11973*
    (2018).'
  id: totrans-1069
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Cao 和 Kipf (2018) Nicola De Cao 和 Thomas Kipf。2018。MolGAN：一种用于小分子图的隐式生成模型。*arXiv
    预印本 arXiv:1805.11973* (2018)。
- en: Defferrard et al. (2016) Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst.
    2016. Convolutional neural networks on graphs with fast localized spectral filtering.
    *Advances in neural information processing systems* 29 (2016).
  id: totrans-1070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Defferrard 等 (2016) Michaël Defferrard, Xavier Bresson, 和 Pierre Vandergheynst。2016。具有快速局部光谱滤波的图卷积神经网络。*神经信息处理系统进展*
    29 (2016)。
- en: 'Delaney (2004) John S Delaney. 2004. ESOL: estimating aqueous solubility directly
    from molecular structure. *Journal of chemical information and computer sciences*
    44, 3 (2004), 1000–1005.'
  id: totrans-1071
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delaney (2004) John S Delaney。2004。ESOL：直接从分子结构估计水相溶解度。*化学信息与计算科学杂志* 44, 3 (2004)，1000–1005。
- en: Deng and Hooi (2021) Ailin Deng and Bryan Hooi. 2021. Graph neural network-based
    anomaly detection in multivariate time series. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 35. 4027–4035.
  id: totrans-1072
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 和 Hooi (2021) Ailin Deng 和 Bryan Hooi. 2021. 基于图神经网络的多变量时间序列异常检测。在 *AAAI
    人工智能会议论文集*，第35卷。4027–4035。
- en: Deng et al. (2022) Leyan Deng, Defu Lian, Zhenya Huang, and Enhong Chen. 2022.
    Graph convolutional adversarial networks for spatiotemporal anomaly detection.
    *IEEE Transactions on Neural Networks and Learning Systems* 33, 6 (2022), 2416–2428.
  id: totrans-1073
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2022) Leyan Deng, Defu Lian, Zhenya Huang 和 Enhong Chen. 2022. 图卷积对抗网络用于时空异常检测。*IEEE
    神经网络与学习系统交易* 33, 6 (2022)，2416–2428。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-1074
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等 (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova.
    2018. Bert：深度双向变换器的预训练用于语言理解。*arXiv 预印本 arXiv:1810.04805* (2018)。
- en: 'Di Domenico et al. (2021) Giandomenico Di Domenico, Jason Sit, Alessio Ishizaka,
    and Daniel Nunan. 2021. Fake news, social media and marketing: A systematic review.
    *Journal of Business Research* 124 (2021), 329–341.'
  id: totrans-1075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Di Domenico 等 (2021) Giandomenico Di Domenico, Jason Sit, Alessio Ishizaka 和
    Daniel Nunan. 2021. 假新闻、社交媒体与营销：系统综述。*商业研究杂志* 124 (2021)，329–341。
- en: Dimitrakopoulos and Demestichas (2010) George Dimitrakopoulos and Panagiotis
    Demestichas. 2010. Intelligent transportation systems. *IEEE Vehicular Technology
    Magazine* 5, 1 (2010), 77–84.
  id: totrans-1076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dimitrakopoulos 和 Demestichas (2010) George Dimitrakopoulos 和 Panagiotis Demestichas.
    2010. 智能交通系统。*IEEE 车辆技术杂志* 5, 1 (2010)，77–84。
- en: Domingos and Richardson (2001) Pedro Domingos and Matt Richardson. 2001. Mining
    the network value of customers. In *Proceedings of the seventh ACM SIGKDD international
    conference on Knowledge discovery and data mining*. 57–66.
  id: totrans-1077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Domingos 和 Richardson (2001) Pedro Domingos 和 Matt Richardson. 2001. 开采客户网络价值。在
    *第七届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集*。57–66。
- en: 'Dong et al. (2017) Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017.
    metapath2vec: Scalable representation learning for heterogeneous networks. In
    *Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery
    and data mining*. 135–144.'
  id: totrans-1078
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等 (2017) Yuxiao Dong, Nitesh V Chawla 和 Ananthram Swami. 2017. metapath2vec：异质网络的可扩展表示学习。在
    *第23届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集*。135–144。
- en: 'Dong et al. (2021) Yushun Dong, Jian Kang, Hanghang Tong, and Jundong Li. 2021.
    Individual fairness for graph neural networks: A ranking based approach. In *Proceedings
    of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining*. 300–310.'
  id: totrans-1079
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等 (2021) Yushun Dong, Jian Kang, Hanghang Tong 和 Jundong Li. 2021. 图神经网络的个体公平性：一种基于排名的方法。在
    *第27届 ACM SIGKDD 知识发现与数据挖掘大会论文集*。300–310。
- en: Du et al. (2021) Jinlong Du, Senzhang Wang, Hao Miao, and Jiaqiang Zhang. 2021.
    Multi-Channel Pooling Graph Neural Networks. In *Proceedings of the Thirtieth
    International Joint Conference on Artificial Intelligence, IJCAI-21*, Zhi-Hua
    Zhou (Ed.). International Joint Conferences on Artificial Intelligence Organization,
    1442–1448. [https://doi.org/10.24963/ijcai.2021/199](https://doi.org/10.24963/ijcai.2021/199)
    Main Track.
  id: totrans-1080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等 (2021) Jinlong Du, Senzhang Wang, Hao Miao 和 Jiaqiang Zhang. 2021. 多通道池化图神经网络。在
    *第三十届国际人工智能联合会议，IJCAI-21*，Zhi-Hua Zhou (编)。国际人工智能联合会议组织，1442–1448。 [https://doi.org/10.24963/ijcai.2021/199](https://doi.org/10.24963/ijcai.2021/199)
    主会议。
- en: 'Du et al. (2019) Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas
    Poczos, Ruosong Wang, and Keyulu Xu. 2019. Graph neural tangent kernel: Fusing
    graph neural networks with graph kernels. In *Advances in Neural Information Processing
    Systems*. 5723–5733.'
  id: totrans-1081
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等 (2019) Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos,
    Ruosong Wang 和 Keyulu Xu. 2019. 图神经切线核：将图神经网络与图核融合。在 *神经信息处理系统进展*。5723–5733。
- en: 'Du et al. (2022a) Yuanqi Du, Tianfan Fu, Jimeng Sun, and Shengchao Liu. 2022a.
    MolGenSurvey: A Systematic Survey in Machine Learning Models for Molecule Design.
    *arXiv preprint arXiv:2203.14500* (2022).'
  id: totrans-1082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等 (2022a) Yuanqi Du, Tianfan Fu, Jimeng Sun 和 Shengchao Liu. 2022a. MolGenSurvey：分子设计机器学习模型的系统综述。*arXiv
    预印本 arXiv:2203.14500* (2022)。
- en: Du et al. (2022b) Yuanqi Du, Xiaojie Guo, Amarda Shehu, and Liang Zhao. 2022b.
    Interpretable molecular graph generation via monotonic constraints. In *Proceedings
    of the 2022 SIAM International Conference on Data Mining (SDM)*. SIAM, 73–81.
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等 (2022b) Yuanqi Du, Xiaojie Guo, Amarda Shehu 和 Liang Zhao. 2022b. 通过单调约束的可解释分子图生成。在
    *2022 SIAM 国际数据挖掘会议 (SDM)* 论文集。SIAM，73–81。
- en: Duvenaud et al. (2015) David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre,
    Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional
    networks on graphs for learning molecular fingerprints. *Advances in neural information
    processing systems* 28 (2015).
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duvenaud 等 (2015) David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael
    Bombarell, Timothy Hirzel, Alán Aspuru-Guzik 和 Ryan P Adams。2015。用于学习分子指纹的图卷积网络。*神经信息处理系统进展*
    28（2015年）。
- en: Dwivedi and Bresson (2020) Vijay Prakash Dwivedi and Xavier Bresson. 2020. A
    generalization of transformer networks to graphs. *arXiv preprint arXiv:2012.09699*
    (2020).
  id: totrans-1085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dwivedi 和 Bresson (2020) Vijay Prakash Dwivedi 和 Xavier Bresson。2020。对图的变换器网络的推广。*arXiv
    预印本 arXiv:2012.09699*（2020年）。
- en: 'Eberhardt et al. (2021) Jerome Eberhardt, Diogo Santos-Martins, Andreas F Tillack,
    and Stefano Forli. 2021. AutoDock Vina 1.2\. 0: New docking methods, expanded
    force field, and python bindings. *Journal of Chemical Information and Modeling*
    61, 8 (2021), 3891–3898.'
  id: totrans-1086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eberhardt 等 (2021) Jerome Eberhardt, Diogo Santos-Martins, Andreas F Tillack
    和 Stefano Forli。2021。AutoDock Vina 1.2.0：新的对接方法、扩展的力场和 Python 绑定。*化学信息与建模期刊* 61,
    8（2021年），3891–3898。
- en: Elinas et al. (2020) Pantelis Elinas, Edwin V Bonilla, and Louis Tiao. 2020.
    Variational inference for graph convolutional networks in the absence of graph
    data and adversarial settings. *Advances in Neural Information Processing Systems*
    33 (2020), 18648–18660.
  id: totrans-1087
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elinas 等 (2020) Pantelis Elinas, Edwin V Bonilla 和 Louis Tiao。2020。缺乏图数据和对抗性设置下的图卷积网络的变分推断。*神经信息处理系统进展*
    33（2020年），18648–18660。
- en: Elsayed et al. (2018) Gamaleldin F Elsayed, Ian Goodfellow, and Jascha Sohl-Dickstein.
    2018. Adversarial reprogramming of neural networks. *arXiv preprint arXiv:1806.11146*
    (2018).
  id: totrans-1088
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsayed 等 (2018) Gamaleldin F Elsayed, Ian Goodfellow 和 Jascha Sohl-Dickstein。2018。对抗性重编程神经网络。*arXiv
    预印本 arXiv:1806.11146*（2018年）。
- en: Empel and Koenigs (2019) Claire Empel and Rene M Koenigs. 2019. Artificial-Intelligence-Driven
    Organic Synthesis—En Route towards Autonomous Synthesis? *Angewandte Chemie International
    Edition* 58, 48 (2019), 17114–17116.
  id: totrans-1089
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Empel 和 Koenigs (2019) Claire Empel 和 Rene M Koenigs。2019。基于人工智能的有机合成——迈向自主合成？*应用化学国际版*
    58, 48（2019年），17114–17116。
- en: Errica et al. (2019) Federico Errica, Marco Podda, Davide Bacciu, and Alessio
    Micheli. 2019. A fair comparison of graph neural networks for graph classification.
    *arXiv preprint arXiv:1912.09893* (2019).
  id: totrans-1090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Errica 等 (2019) Federico Errica, Marco Podda, Davide Bacciu 和 Alessio Micheli。2019。图神经网络在图分类中的公平比较。*arXiv
    预印本 arXiv:1912.09893*（2019年）。
- en: Fan et al. (2021) Ziwei Fan, Zhiwei Liu, Jiawei Zhang, Yun Xiong, Lei Zheng,
    and Philip S Yu. 2021. Continuous-time sequential recommendation with temporal
    graph collaborative transformer. In *Proceedings of the 30th ACM International
    Conference on Information & Knowledge Management*. 433–442.
  id: totrans-1091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等 (2021) Ziwei Fan, Zhiwei Liu, Jiawei Zhang, Yun Xiong, Lei Zheng 和 Philip
    S Yu。2021。基于时序图协同变换器的连续时间序列推荐。在*第30届 ACM 国际信息与知识管理会议论文集*。433–442。
- en: Fang et al. (2022) Yin Fang, Qiang Zhang, Haihong Yang, Xiang Zhuang, Shumin
    Deng, Wen Zhang, Ming Qin, Zhuo Chen, Xiaohui Fan, and Huajun Chen. 2022. Molecular
    contrastive learning with chemical element knowledge graph. In *Proceedings of
    the AAAI Conference on Artificial Intelligence*, Vol. 36. 3968–3976.
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等 (2022) Yin Fang, Qiang Zhang, Haihong Yang, Xiang Zhuang, Shumin Deng,
    Wen Zhang, Ming Qin, Zhuo Chen, Xiaohui Fan 和 Huajun Chen。2022。基于化学元素知识图的分子对比学习。在*AAAI
    人工智能会议论文集*，第36卷。3968–3976。
- en: Fang et al. (2021) Zheng Fang, Qingqing Long, Guojie Song, and Kunqing Xie.
    2021. Spatial-temporal graph ode networks for traffic flow forecasting. In *Proceedings
    of the 27th ACM SIGKDD conference on knowledge discovery & data mining*. 364–373.
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等 (2021) Zheng Fang, Qingqing Long, Guojie Song 和 Kunqing Xie。2021。用于交通流量预测的时空图
    ODE 网络。在*第27届 ACM SIGKDD 知识发现与数据挖掘会议论文集*。364–373。
- en: Feinberg et al. (2018) Evan N Feinberg, Debnil Sur, Zhenqin Wu, Brooke E Husic,
    Huanghao Mai, Yang Li, Saisai Sun, Jianyi Yang, Bharath Ramsundar, and Vijay S
    Pande. 2018. PotentialNet for molecular property prediction. *ACS central science*
    4, 11 (2018), 1520–1530.
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feinberg 等 (2018) Evan N Feinberg, Debnil Sur, Zhenqin Wu, Brooke E Husic, Huanghao
    Mai, Yang Li, Saisai Sun, Jianyi Yang, Bharath Ramsundar 和 Vijay S Pande。2018。用于分子属性预测的
    PotentialNet。*ACS 中央科学* 4, 11（2018年），1520–1530。
- en: 'Feng et al. (2021) Shangbin Feng, Herun Wan, Ningnan Wang, and Minnan Luo.
    2021. BotRGCN: Twitter bot detection with relational graph convolutional networks.
    In *Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social
    Networks Analysis and Mining*. 236–239.'
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等 (2021) Shangbin Feng, Herun Wan, Ningnan Wang 和 Minnan Luo。2021。BotRGCN：基于关系图卷积网络的
    Twitter 机器人检测。在*2021年 IEEE/ACM 国际社交网络分析与挖掘会议论文集*。236–239。
- en: Feng et al. (2020) Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan,
    Qian Xu, Qiang Yang, Evgeny Kharlamov, and Jie Tang. 2020. Graph random neural
    networks for semi-supervised learning on graphs. (2020), 22092–22103.
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2020）Wenzheng Feng、Jie Zhang、Yuxiao Dong、Yu Han、Huanbo Luan、Qian Xu、Qiang
    Yang、Evgeny Kharlamov 和 Jie Tang. 2020. 图随机神经网络用于图上的半监督学习。（2020），22092–22103。
- en: Feng et al. (2019) Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue
    Gao. 2019. Hypergraph neural networks. In *Proceedings of the AAAI conference
    on artificial intelligence*, Vol. 33\. 3558–3565.
  id: totrans-1097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2019）Yifan Feng、Haoxuan You、Zizhao Zhang、Rongrong Ji 和 Yue Gao. 2019.
    超图神经网络。在 *AAAI 人工智能会议论文集* 第 33 卷。3558–3565。
- en: Finn et al. (2017) Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic
    meta-learning for fast adaptation of deep networks. In *International conference
    on machine learning*. PMLR, 1126–1135.
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn 等（2017）Chelsea Finn、Pieter Abbeel 和 Sergey Levine. 2017. 面向深度网络快速适应的模型无关元学习。在
    *国际机器学习会议*。PMLR，1126–1135。
- en: Finzi et al. (2020) Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon
    Wilson. 2020. Generalizing convolutional neural networks for equivariance to lie
    groups on arbitrary continuous data. In *ICML*.
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finzi 等（2020）Marc Finzi、Samuel Stanton、Pavel Izmailov 和 Andrew Gordon Wilson.
    2020. 为 Lie 群的等变性推广卷积神经网络，适用于任意连续数据。在 *ICML*。
- en: Fischer (1894) Emil Fischer. 1894. Einfluss der Configuration auf die Wirkung
    der Enzyme. *Berichte der deutschen chemischen Gesellschaft* 27, 3 (1894), 2985–2993.
  id: totrans-1100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fischer（1894）Emil Fischer. 1894. 配置对酶作用的影响。*德国化学学会报告* 27，第 3 期（1894），2985–2993。
- en: Franceschi et al. (2019) Luca Franceschi, Mathias Niepert, Massimiliano Pontil,
    and Xiao He. 2019. Learning discrete structures for graph neural networks. In
    *International conference on machine learning*. PMLR, 1972–1982.
  id: totrans-1101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Franceschi 等（2019）Luca Franceschi、Mathias Niepert、Massimiliano Pontil 和 Xiao
    He. 2019. 为图神经网络学习离散结构。在 *国际机器学习会议*。PMLR，1972–1982。
- en: Francoeur et al. (2020) Paul G Francoeur, Tomohide Masuda, Jocelyn Sunseri,
    Andrew Jia, Richard B Iovanisci, Ian Snyder, and David R Koes. 2020. Three-dimensional
    convolutional neural networks and a cross-docked data set for structure-based
    drug design. *Journal of Chemical Information and Modeling* 60, 9 (2020), 4200–4215.
  id: totrans-1102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Francoeur 等（2020）Paul G Francoeur、Tomohide Masuda、Jocelyn Sunseri、Andrew Jia、Richard
    B Iovanisci、Ian Snyder 和 David R Koes. 2020. 三维卷积神经网络和用于基于结构的药物设计的交叉对接数据集。*化学信息与建模期刊*
    60，第 9 期（2020），4200–4215。
- en: Frisch et al. (2016) MJ ea Frisch, GW Trucks, HB Schlegel, GE Scuseria, MA Robb,
    JR Cheeseman, G Scalmani, VPGA Barone, GA Petersson, HJRA Nakatsuji, et al. 2016.
    Gaussian 16.
  id: totrans-1103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frisch 等（2016）MJ 等 Frisch、GW Trucks、HB Schlegel、GE Scuseria、MA Robb、JR Cheeseman、G
    Scalmani、VPGA Barone、GA Petersson、HJRA Nakatsuji 等。2016. Gaussian 16。
- en: Fu et al. (2022) Guoji Fu, Peilin Zhao, and Yatao Bian. 2022. $p$-Laplacian
    Based Graph Neural Networks. In *International Conference on Machine Learning*.
    PMLR, 6878–6917.
  id: totrans-1104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2022）Guoji Fu、Peilin Zhao 和 Yatao Bian. 2022. $p$-Laplacian 基于图神经网络。在 *国际机器学习会议*。PMLR，6878–6917。
- en: 'Fu and Liu (2021) Sichao Fu and Weifeng Liu. 2021. Recent Advances of Manifold-based
    Graph Convolutional Networks for Remote Sensing Images Recognition. *Generalization
    With Deep Learning: For Improvement On Sensing Capability* (2021), 209–232.'
  id: totrans-1105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 和 Liu（2021）Sichao Fu 和 Weifeng Liu. 2021. 基于流形的图卷积网络在遥感图像识别中的最新进展。*深度学习中的泛化：提升感知能力*（2021），209–232。
- en: 'Fuchs et al. (2020) Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling.
    2020. SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks.
    In *NeurIPS*, Vol. 33. [https://proceedings.neurips.cc/paper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf)'
  id: totrans-1106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fuchs 等（2020）Fabian Fuchs、Daniel Worrall、Volker Fischer 和 Max Welling. 2020.
    SE(3)-变换器：3D 旋转-平移等变注意力网络。在 *NeurIPS* 第 33 卷。[https://proceedings.neurips.cc/paper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf)
- en: Ganea et al. (2021) Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao
    Bian, Regina Barzilay, Tommi Jaakkola, and Andreas Krause. 2021. Independent se
    (3)-equivariant models for end-to-end rigid protein docking. *arXiv preprint arXiv:2111.07786*
    (2021).
  id: totrans-1107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ganea 等（2021）Octavian-Eugen Ganea、Xinyuan Huang、Charlotte Bunne、Yatao Bian、Regina
    Barzilay、Tommi Jaakkola 和 Andreas Krause. 2021. 独立的 SE(3)-等变模型用于端到端的刚性蛋白质对接。*arXiv
    预印本 arXiv:2111.07786*（2021）。
- en: Gao and Ji (2019) Hongyang Gao and Shuiwang Ji. 2019. Graph u-nets. In *international
    conference on machine learning*. PMLR, 2083–2092.
  id: totrans-1108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 和 Ji（2019）Hongyang Gao 和 Shuiwang Ji. 2019. 图 u-nets。在 *国际机器学习会议*。PMLR，2083–2092。
- en: Gao et al. (2021) Hongyang Gao, Yi Liu, and Shuiwang Ji. 2021. Topology-aware
    graph pooling networks. *IEEE Transactions on Pattern Analysis and Machine Intelligence*
    43, 12 (2021), 4512–4518.
  id: totrans-1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2021）Hongyang Gao, Yi Liu 和 Shuiwang Ji. 2021. 具有拓扑感知的图池化网络。*IEEE模式分析与机器智能学报*
    43, 12 (2021), 4512–4518。
- en: Gao et al. (2020) Xiang Gao, Wei Hu, and Zongming Guo. 2020. Exploring structure-adaptive
    graph learning for robust semi-supervised classification. In *2020 ieee international
    conference on multimedia and expo (icme)*. IEEE, 1–6.
  id: totrans-1110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2020）Xiang Gao, Wei Hu 和 Zongming Guo. 2020. 探索结构自适应图学习以实现鲁棒的半监督分类。发表于
    *2020 IEEE 国际多媒体与博览会会议（ICME）*。IEEE, 1–6。
- en: 'Gärtner et al. (2003) Thomas Gärtner, Peter Flach, and Stefan Wrobel. 2003.
    On graph kernels: Hardness results and efficient alternatives. In *Proceedings
    of Computational Learning theory and kernel machines*. 129–143.'
  id: totrans-1111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gärtner 等（2003）Thomas Gärtner, Peter Flach 和 Stefan Wrobel. 2003. 关于图核：难度结果和高效替代方案。发表于
    *计算学习理论与核机器会议论文集*。129–143。
- en: Gasteiger et al. (2019) Johannes Gasteiger, Stefan Weißenberger, and Stephan
    Günnemann. 2019. Diffusion improves graph learning. *Advances in neural information
    processing systems* 32 (2019).
  id: totrans-1112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gasteiger 等（2019）Johannes Gasteiger, Stefan Weißenberger 和 Stephan Günnemann.
    2019. 扩散改进图学习。*神经信息处理系统进展* 32 (2019)。
- en: Gaudelet et al. (2021) Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman,
    Cristian Regep, Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts,
    Jian Tang, et al. 2021. Utilizing graph machine learning within drug discovery
    and development. *Briefings in bioinformatics* 22, 6 (2021), bbab159.
  id: totrans-1113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gaudelet 等（2021）Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman, Cristian
    Regep, Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts, Jian
    Tang 等. 2021. 在药物发现与开发中利用图机器学习。*生物信息学简报* 22, 6 (2021), bbab159。
- en: Gayvert et al. (2016) Kaitlyn M Gayvert, Neel S Madhukar, and Olivier Elemento.
    2016. A data-driven approach to predicting successes and failures of clinical
    trials. *Cell chemical biology* 23, 10 (2016), 1294–1301.
  id: totrans-1114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gayvert 等（2016）Kaitlyn M Gayvert, Neel S Madhukar 和 Olivier Elemento. 2016.
    预测临床试验成功与失败的数据驱动方法。*细胞化学生物学* 23, 10 (2016), 1294–1301。
- en: Gebauer et al. (2019) Niklas Gebauer, Michael Gastegger, and Kristof Schütt.
    2019. Symmetry-adapted generation of 3d point sets for the targeted discovery
    of molecules. *Advances in neural information processing systems* 32 (2019).
  id: totrans-1115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gebauer 等（2019）Niklas Gebauer, Michael Gastegger 和 Kristof Schütt. 2019. 对称适应性生成3D点集以目标分子发现。*神经信息处理系统进展*
    32 (2019)。
- en: Gebauer et al. (2022) Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann,
    Klaus-Robert Müller, and Kristof T Schütt. 2022. Inverse design of 3d molecular
    structures with conditional generative neural networks. *Nature communications*
    13, 1 (2022), 1–11.
  id: totrans-1116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gebauer 等（2022）Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert
    Müller 和 Kristof T Schütt. 2022. 使用条件生成神经网络的3D分子结构逆向设计。*自然通讯* 13, 1 (2022), 1–11。
- en: 'Geiger and Smidt (2022) Mario Geiger and Tess Smidt. 2022. e3nn: Euclidean
    neural networks. *arXiv preprint arXiv:2207.09453* (2022).'
  id: totrans-1117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Geiger 和 Smidt（2022）Mario Geiger 和 Tess Smidt. 2022. e3nn: 欧几里得神经网络。*arXiv
    预印本 arXiv:2207.09453* (2022)。'
- en: Geisler et al. (2021) Simon Geisler, Tobias Schmidt, Hakan Şirin, Daniel Zügner,
    Aleksandar Bojchevski, and Stephan Günnemann. 2021. Robustness of graph neural
    networks at scale. *Advances in Neural Information Processing Systems* 34 (2021),
    7637–7649.
  id: totrans-1118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geisler 等（2021）Simon Geisler, Tobias Schmidt, Hakan Şirin, Daniel Zügner, Aleksandar
    Bojchevski 和 Stephan Günnemann. 2021. 大规模图神经网络的鲁棒性。*神经信息处理系统进展* 34 (2021), 7637–7649。
- en: Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol
    Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry.
    In *International conference on machine learning*. PMLR, 1263–1272.
  id: totrans-1119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gilmer 等（2017）Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals
    和 George E Dahl. 2017. 用于量子化学的神经消息传递。发表于 *国际机器学习大会*。PMLR, 1263–1272。
- en: Gómez-Bombarelli et al. (2018) Rafael Gómez-Bombarelli, Jennifer N Wei, David
    Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla,
    Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik.
    2018. Automatic chemical design using a data-driven continuous representation
    of molecules. *ACS central science* 4, 2 (2018), 268–276.
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gómez-Bombarelli 等（2018）Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud,
    José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge
    Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams 和 Alán Aspuru-Guzik. 2018.
    使用数据驱动的分子连续表示进行自动化化学设计。*ACS 中央科学* 4, 2 (2018), 268–276。
- en: Goodfellow et al. (2020) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.
    Generative adversarial networks. *Commun. ACM* 63, 11 (2020), 139–144.
  id: totrans-1121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow等（2020）Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
    Warde-Farley, Sherjil Ozair, Aaron Courville和Yoshua Bengio。2020年。生成对抗网络。*Commun.
    ACM* 63, 11 (2020), 139–144。
- en: Goodman and Flaxman (2017) Bryce Goodman and Seth Flaxman. 2017. European Union
    regulations on algorithmic decision-making and a “right to explanation”. *AI magazine*
    38, 3 (2017), 50–57.
  id: totrans-1122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodman和Flaxman（2017）Bryce Goodman和Seth Flaxman。2017年。欧盟关于算法决策和“解释权”的法规。*AI
    magazine* 38, 3 (2017), 50–57。
- en: 'Goyal and Ferrara (2018) Palash Goyal and Emilio Ferrara. 2018. Graph embedding
    techniques, applications, and performance: A survey. *Knowledge-Based Systems*
    151 (2018), 78–94.'
  id: totrans-1123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal和Ferrara（2018）Palash Goyal和Emilio Ferrara。2018年。图嵌入技术、应用和性能：综述。*Knowledge-Based
    Systems* 151 (2018), 78–94。
- en: Grill et al. (2020) Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin
    Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires,
    Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a
    new approach to self-supervised learning. *Advances in neural information processing
    systems* 33 (2020), 21271–21284.
  id: totrans-1124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grill等（2020）Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec,
    Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan
    Guo, Mohammad Gheshlaghi Azar等。2020年。引导你自己的潜在—自监督学习的新方法。*Advances in neural information
    processing systems* 33 (2020), 21271–21284。
- en: Grohs and Voigtlaender (2021) Philipp Grohs and Felix Voigtlaender. 2021. Proof
    of the theory-to-practice gap in deep learning via sampling complexity bounds
    for neural network approximation spaces. *arXiv preprint arXiv:2104.02746* (2021).
  id: totrans-1125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grohs和Voigtlaender（2021）Philipp Grohs和Felix Voigtlaender。2021年。通过神经网络逼近空间的采样复杂性界限证明深度学习中的理论与实践差距。*arXiv预印本
    arXiv:2104.02746*（2021年）。
- en: 'Groom et al. (2016) Colin R Groom, Ian J Bruno, Matthew P Lightfoot, and Suzanna C
    Ward. 2016. The Cambridge structural database. *Acta Crystallographica Section
    B: Structural Science, Crystal Engineering and Materials* 72, 2 (2016), 171–179.'
  id: totrans-1126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Groom等（2016）Colin R Groom, Ian J Bruno, Matthew P Lightfoot和Suzanna C Ward。2016年。剑桥结构数据库。*Acta
    Crystallographica Section B: Structural Science, Crystal Engineering and Materials*
    72, 2 (2016), 171–179。'
- en: 'Grover and Leskovec (2016) Aditya Grover and Jure Leskovec. 2016. node2vec:
    Scalable feature learning for networks. In *Proceedings of the 22nd ACM SIGKDD
    international conference on Knowledge discovery and data mining*. 855–864.'
  id: totrans-1127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grover和Leskovec（2016）Aditya Grover和Jure Leskovec。2016年。node2vec：用于网络的可扩展特征学习。在*Proceedings
    of the 22nd ACM SIGKDD international conference on Knowledge discovery and data
    mining*。855–864。
- en: 'Günnemann (2022) Stephan Günnemann. 2022. Graph neural networks: Adversarial
    robustness. In *Graph Neural Networks: Foundations, Frontiers, and Applications*.
    Springer, 149–176.'
  id: totrans-1128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Günnemann（2022）Stephan Günnemann。2022年。图神经网络：对抗鲁棒性。在*Graph Neural Networks:
    Foundations, Frontiers, and Applications*。Springer，149–176。'
- en: Guo et al. (2019) Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu
    Wan. 2019. Attention based spatial-temporal graph convolutional networks for traffic
    flow forecasting. In *Proceedings of the AAAI conference on artificial intelligence*,
    Vol. 33\. 922–929.
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等（2019）Shengnan Guo, Youfang Lin, Ning Feng, Chao Song和Huaiyu Wan。2019年。基于注意力的时空图卷积网络用于交通流预测。在*Proceedings
    of the AAAI conference on artificial intelligence*，第33卷。922–929。
- en: Guo et al. (2021) Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest,
    Meng Jiang, and Nitesh V Chawla. 2021. Few-shot graph learning for molecular property
    prediction. In *Proceedings of the Web Conference 2021*. 2559–2567.
  id: totrans-1130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等（2021）Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang和Nitesh
    V Chawla。2021年。用于分子属性预测的少样本图学习。在*Proceedings of the Web Conference 2021*。2559–2567。
- en: Hamidian and Diab (2019) Sardar Hamidian and Mona T Diab. 2019. Rumor detection
    and classification for twitter data. *arXiv preprint arXiv:1912.08926* (2019).
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamidian和Diab（2019）Sardar Hamidian和Mona T Diab。2019年。推特数据的谣言检测与分类。*arXiv预印本
    arXiv:1912.08926*（2019年）。
- en: Hamilton et al. (2017) Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017.
    Inductive representation learning on large graphs. *Advances in neural information
    processing systems* 30 (2017).
  id: totrans-1132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamilton等（2017）Will Hamilton, Zhitao Ying和Jure Leskovec。2017年。在大图上的归纳表示学习。*Advances
    in neural information processing systems* 30 (2017)。
- en: Hammond et al. (2011) David K Hammond, Pierre Vandergheynst, and Rémi Gribonval.
    2011. Wavelets on graphs via spectral graph theory. *Applied and Computational
    Harmonic Analysis* 30, 2 (2011), 129–150.
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hammond等（2011）David K Hammond, Pierre Vandergheynst和Rémi Gribonval。2011年。通过谱图理论的图上的小波。*Applied
    and Computational Harmonic Analysis* 30, 2 (2011), 129–150。
- en: 'Han et al. (2022) Jiaqi Han, Yu Rong, Tingyang Xu, and Wenbing Huang. 2022.
    Geometrically equivariant graph neural networks: A survey. *arXiv preprint arXiv:2202.07230*
    (2022).'
  id: totrans-1134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等人（2022）Jiaqi Han, Yu Rong, Tingyang Xu, 和 Wenbing Huang. 2022. 几何等变图神经网络：综述。*arXiv
    预印本 arXiv:2202.07230*（2022）。
- en: 'Hao et al. (2020) Zhongkai Hao, Chengqiang Lu, Zhenya Huang, Hao Wang, Zheyuan
    Hu, Qi Liu, Enhong Chen, and Cheekong Lee. 2020. ASGN: An active semi-supervised
    graph neural network for molecular property prediction. In *Proceedings of the
    ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*. 731–752.'
  id: totrans-1135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hao 等人（2020）Zhongkai Hao, Chengqiang Lu, Zhenya Huang, Hao Wang, Zheyuan Hu,
    Qi Liu, Enhong Chen, 和 Cheekong Lee. 2020. ASGN: 一种用于分子属性预测的主动半监督图神经网络。发表于 *ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*。731–752。'
- en: Harvey et al. (2017) Nick Harvey, Christopher Liaw, and Abbas Mehrabian. 2017.
    Nearly-tight VC-dimension bounds for piecewise linear neural networks. In *Conference
    on learning theory*. PMLR, 1064–1068.
  id: totrans-1136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harvey 等人（2017）Nick Harvey, Christopher Liaw, 和 Abbas Mehrabian. 2017. 分段线性神经网络的近紧VC维界限。发表于
    *学习理论会议*。PMLR, 1064–1068。
- en: 'He et al. (2021) Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang,
    Aixi Zhang, and Si Liu. 2021. TransRefer3D: Entity-and-Relation Aware Transformer
    for Fine-Grained 3D Visual Grounding. In *Proceedings of the 29th ACM International
    Conference on Multimedia*. 2344–2352.'
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等人（2021）Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang,
    Aixi Zhang, 和 Si Liu. 2021. TransRefer3D: 面向实体和关系的Transformer用于细粒度三维视觉定位。发表于 *第29届ACM国际多媒体会议论文集*。2344–2352。'
- en: 'He et al. (2020) Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang,
    and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network
    for recommendation. In *Proceedings of the 43rd International ACM SIGIR conference
    on research and development in Information Retrieval*. 639–648.'
  id: totrans-1138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等人（2020）Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, 和 Meng
    Wang. 2020. Lightgcn: 简化和增强推荐的图卷积网络。发表于 *第43届国际ACM SIGIR信息检索研究与开发会议论文集*。639–648。'
- en: 'He et al. (2022) Yixuan He, Michael Permultter, Gesine Reinert, and Mihai Cucuringu.
    2022. MSGNN: A Spectral Graph Neural Network Based on a Novel Magnetic Signed
    Laplacian. *arXiv preprint arXiv:2209.00546* (2022).'
  id: totrans-1139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等人（2022）Yixuan He, Michael Permultter, Gesine Reinert, 和 Mihai Cucuringu.
    2022. MSGNN: 基于新型磁签名拉普拉斯的谱图神经网络。*arXiv 预印本 arXiv:2209.00546*（2022）。'
- en: Hearst et al. (1998) Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt,
    and Bernhard Scholkopf. 1998. Support vector machines. *IEEE Intelligent Systems
    and their applications* 13, 4 (1998), 18–28.
  id: totrans-1140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hearst 等人（1998）Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, 和 Bernhard
    Scholkopf. 1998. 支持向量机。*IEEE智能系统及其应用* 13, 4（1998），18–28。
- en: Henaff et al. (2015) Mikael Henaff, Joan Bruna, and Yann LeCun. 2015. Deep convolutional
    networks on graph-structured data. *arXiv preprint arXiv:1506.05163* (2015).
  id: totrans-1141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henaff 等人（2015）Mikael Henaff, Joan Bruna, 和 Yann LeCun. 2015. 图结构数据上的深度卷积网络。*arXiv
    预印本 arXiv:1506.05163*（2015）。
- en: Hendrycks and Gimpel (2016) Dan Hendrycks and Kevin Gimpel. 2016. A baseline
    for detecting misclassified and out-of-distribution examples in neural networks.
    *arXiv preprint arXiv:1610.02136* (2016).
  id: totrans-1142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 和 Gimpel（2016）Dan Hendrycks 和 Kevin Gimpel. 2016. 神经网络中检测误分类和分布外样本的基准。*arXiv
    预印本 arXiv:1610.02136*（2016）。
- en: Hinton and Salakhutdinov (2006) Geoffrey E Hinton and Ruslan R Salakhutdinov.
    2006. Reducing the dimensionality of data with neural networks. *science* 313,
    5786 (2006), 504–507.
  id: totrans-1143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 和 Salakhutdinov（2006）Geoffrey E Hinton 和 Ruslan R Salakhutdinov. 2006.
    用神经网络减少数据的维度。*科学* 313, 5786（2006），504–507。
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising
    diffusion probabilistic models. *Advances in Neural Information Processing Systems*
    33 (2020), 6840–6851.
  id: totrans-1144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等人（2020）Jonathan Ho, Ajay Jain, 和 Pieter Abbeel. 2020. 去噪扩散概率模型。*神经信息处理系统进展*
    33（2020），6840–6851。
- en: Hofman et al. (2021) Jake M Hofman, Duncan J Watts, Susan Athey, Filiz Garip,
    Thomas L Griffiths, Jon Kleinberg, Helen Margetts, Sendhil Mullainathan, Matthew J
    Salganik, Simine Vazire, et al. 2021. Integrating explanation and prediction in
    computational social science. *Nature* 595, 7866 (2021), 181–188.
  id: totrans-1145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofman 等人（2021）Jake M Hofman, Duncan J Watts, Susan Athey, Filiz Garip, Thomas
    L Griffiths, Jon Kleinberg, Helen Margetts, Sendhil Mullainathan, Matthew J Salganik,
    Simine Vazire 等. 2021. 在计算社会科学中整合解释与预测。*自然* 595, 7866（2021），181–188。
- en: Hoogeboom et al. (2022) Emiel Hoogeboom, Vıctor Garcia Satorras, Clément Vignac,
    and Max Welling. 2022. Equivariant diffusion for molecule generation in 3d. In
    *International Conference on Machine Learning*. PMLR, 8867–8887.
  id: totrans-1146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoogeboom 等人（2022）Emiel Hoogeboom, Víctor Garcia Satorras, Clément Vignac, 和
    Max Welling. 2022. 用于三维分子生成的等变扩散。发表于 *国际机器学习会议*。PMLR, 8867–8887。
- en: 'Hu et al. (2020d) Fenyu Hu, Yanqiao Zhu, Shu Wu, Weiran Huang, Liang Wang,
    and Tieniu Tan. 2020d. Graphair: Graph representation learning with neighborhood
    aggregation and interaction. *Pattern Recognition* 112 (2020), 107745.'
  id: totrans-1147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2020d) Fenyu Hu, Yanqiao Zhu, Shu Wu, Weiran Huang, Liang Wang,
    和 Tieniu Tan. 2020d. Graphair: 通过邻域聚合和交互进行图表示学习。*模式识别* 112 (2020), 107745。'
- en: 'Hu et al. (2020c) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu
    Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020c. Open graph benchmark:
    Datasets for machine learning on graphs. *Advances in neural information processing
    systems* 33 (2020), 22118–22133.'
  id: totrans-1148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2020c) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu
    Ren, Bowen Liu, Michele Catasta, 和 Jure Leskovec. 2020c. 开放图基准：用于图上的机器学习的数据集。*神经信息处理系统进展*
    33 (2020), 22118–22133。
- en: Hu et al. (2019) Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang,
    Vijay Pande, and Jure Leskovec. 2019. Strategies for pre-training graph neural
    networks. *arXiv preprint arXiv:1905.12265* (2019).
  id: totrans-1149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2019) Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang,
    Vijay Pande, 和 Jure Leskovec. 2019. 图神经网络的预训练策略。*arXiv预印本 arXiv:1905.12265* (2019)。
- en: 'Hu et al. (2020b) Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou
    Sun. 2020b. Gpt-gnn: Generative pre-training of graph neural networks. In *Proceedings
    of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining*. 1857–1867.'
  id: totrans-1150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2020b) Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, 和 Yizhou
    Sun. 2020b. Gpt-gnn: 图神经网络的生成预训练。发表于 *第26届ACM SIGKDD国际知识发现与数据挖掘会议录*，1857–1867。'
- en: Hu et al. (2020a) Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020a.
    Heterogeneous graph transformer. In *Proceedings of the web conference 2020*.
    2704–2710.
  id: totrans-1151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2020a) Ziniu Hu, Yuxiao Dong, Kuansan Wang, 和 Yizhou Sun. 2020a.
    异质图变换器。发表于 *2020年网络会议录*，2704–2710。
- en: Hu and Li (2021) Zhiting Hu and Li Erran Li. 2021. A causal lens for controllable
    text generation. *Advances in Neural Information Processing Systems* 34 (2021),
    24941–24955.
  id: totrans-1152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu and Li (2021) Zhiting Hu 和 Li Erran Li. 2021. 可控文本生成的因果视角。*神经信息处理系统进展* 34
    (2021), 24941–24955。
- en: 'Huang and Yang (2021) Jing Huang and Jie Yang. 2021. Unignn: a unified framework
    for graph and hypergraph neural networks. *arXiv preprint arXiv:2105.00956* (2021).'
  id: totrans-1153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang and Yang (2021) Jing Huang 和 Jie Yang. 2021. Unignn: 一个统一的图和超图神经网络框架。*arXiv预印本
    arXiv:2105.00956* (2021)。'
- en: 'Huang et al. (2020) Rongzhou Huang, Chuyin Huang, Yubao Liu, Genan Dai, and
    Weiyang Kong. 2020. LSGCN: Long Short-Term Traffic Prediction with Graph Convolutional
    Networks.. In *IJCAI*, Vol. 7\. 2355–2361.'
  id: totrans-1154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2020) Rongzhou Huang, Chuyin Huang, Yubao Liu, Genan Dai, 和 Weiyang
    Kong. 2020. LSGCN: 基于图卷积网络的长短期交通预测。发表于 *IJCAI*，第7卷，2355–2361。'
- en: Huang et al. (2015) Sheng Huang, Mohamed Elhoseiny, Ahmed Elgammal, and Dan
    Yang. 2015. Learning hypergraph-regularized attribute predictors. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. 409–417.
  id: totrans-1155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2015) Sheng Huang, Mohamed Elhoseiny, Ahmed Elgammal, 和 Dan Yang.
    2015. 学习超图正则化属性预测器。发表于 *IEEE计算机视觉与模式识别会议录*，409–417。
- en: Huang et al. (2022) Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun,
    and Junzhou Huang. 2022. Constrained Graph Mechanics Networks. In *ICLR*. [https://openreview.net/forum?id=SHbhHHfePhP](https://openreview.net/forum?id=SHbhHHfePhP)
  id: totrans-1156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2022) Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun,
    和 Junzhou Huang. 2022. 约束图力学网络。发表于 *ICLR*。 [https://openreview.net/forum?id=SHbhHHfePhP](https://openreview.net/forum?id=SHbhHHfePhP)
- en: 'Hussain et al. (2021) Md Shamim Hussain, Mohammed J Zaki, and Dharmashankar
    Subramanian. 2021. Edge-augmented graph transformers: Global self-attention is
    enough for graphs. *arXiv preprint arXiv:2108.03348* (2021).'
  id: totrans-1157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hussain et al. (2021) Md Shamim Hussain, Mohammed J Zaki, 和 Dharmashankar Subramanian.
    2021. 边缘增强图变换器：全球自注意力对于图足够了。*arXiv预印本 arXiv:2108.03348* (2021)。
- en: 'Hutchinson et al. (2021) Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi,
    Emilien Dupont, Yee Whye Teh, and Hyunjik Kim. 2021. Lietransformer: equivariant
    self-attention for lie groups. In *ICML*.'
  id: totrans-1158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hutchinson et al. (2021) Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi,
    Emilien Dupont, Yee Whye Teh, 和 Hyunjik Kim. 2021. Lietransformer: 对Lie群的等变自注意力。发表于
    *ICML*。'
- en: 'Irwin et al. (2012) John J Irwin, Teague Sterling, Michael M Mysinger, Erin S
    Bolstad, and Ryan G Coleman. 2012. ZINC: a free tool to discover chemistry for
    biology. *Journal of chemical information and modeling* 52, 7 (2012), 1757–1768.'
  id: totrans-1159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Irwin et al. (2012) John J Irwin, Teague Sterling, Michael M Mysinger, Erin
    S Bolstad, 和 Ryan G Coleman. 2012. ZINC: 一种用于发现生物学化学的免费工具。*化学信息与建模期刊* 52, 7 (2012),
    1757–1768。'
- en: Ishida et al. (2022) Shoichi Ishida, Kei Terayama, Ryosuke Kojima, Kiyosei Takasu,
    and Yasushi Okuno. 2022. Ai-driven synthetic route design incorporated with retrosynthesis
    knowledge. *Journal of chemical information and modeling* 62, 6 (2022), 1357–1367.
  id: totrans-1160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ishida et al. (2022) 石田正一、寺山圭、儿童小岛、久世高須以及奥野康。2022年。结合逆合成知识的人工智能驱动的合成路线设计。*化学信息与建模杂志*
    62卷，第6期（2022年），1357–1367页。
- en: Islam et al. (2022) Md Ashraful Islam, Mir Mahathir Mohammad, Sarkar Snigdha Sarathi
    Das, and Mohammed Eunus Ali. 2022. A survey on deep learning based Point-of-Interest
    (POI) recommendations. *Neurocomputing* 472 (2022), 306–325.
  id: totrans-1161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Islam et al. (2022) Md Ashraful Islam、Mir Mahathir Mohammad、Sarkar Snigdha Sarathi
    Das和Mohammed Eunus Ali。2022年。基于深度学习的兴趣点（POI）推荐综述。*神经计算* 472卷（2022年），306–325页。
- en: Ji et al. (2020) Shuyi Ji, Yifan Feng, Rongrong Ji, Xibin Zhao, Wanwan Tang,
    and Yue Gao. 2020. Dual channel hypergraph collaborative filtering. In *Proceedings
    of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining*. 2020–2029.
  id: totrans-1162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji et al. (2020) 纪淑仪、冯艺凡、纪荣荣、赵西宾、唐婉婉和高岳。2020年。双通道超图协同过滤。在*第26届ACM SIGKDD国际知识发现与数据挖掘会议论文集*上。2020–2029页。
- en: Jiang et al. (2019) Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, and Bin Luo.
    2019. Semi-supervised learning with graph learning-convolutional networks. In
    *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*.
    11313–11320.
  id: totrans-1163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2019) 姜博、张子言、林豆豆、唐金和罗宾。2019年。结合图学习卷积网络的半监督学习。在*IEEE/CVF计算机视觉与模式识别会议论文集*上。11313–11320页。
- en: 'Jiang and Luo (2022) Weiwei Jiang and Jiayun Luo. 2022. Graph neural network
    for traffic forecasting: A survey. *Expert Systems with Applications* (2022),
    117921.'
  id: totrans-1164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang and Luo (2022) 姜维维和罗佳云。2022年。用于交通预测的图神经网络：综述。*专家系统与应用*（2022年），117921页。
- en: Jiang et al. (2018) Zhuoren Jiang, Yue Yin, Liangcai Gao, Yao Lu, and Xiaozhong
    Liu. 2018. Cross-language citation recommendation via hierarchical representation
    learning on heterogeneous graph. In *The 41st International ACM SIGIR Conference
    on Research & Development in Information Retrieval*. 635–644.
  id: totrans-1165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2018) 蒋卓仁、尹悦、李昂才、姚璐和刘晓中。2018年。通过异构图的层次表示学习进行跨语言引用推荐。在*第41届国际ACM
    SIGIR信息检索研究与发展会议*上。635–644页。
- en: Jiao et al. (2020) Yizhu Jiao, Yun Xiong, Jiawei Zhang, Yao Zhang, Tianqi Zhang,
    and Yangyong Zhu. 2020. Sub-graph contrast for scalable self-supervised graph
    representation learning. In *2020 IEEE international conference on data mining
    (ICDM)*. IEEE, 222–231.
  id: totrans-1166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao et al. (2020) 角一竹、熊云、张佳伟、张姚、张天齐和朱阳永。2020年。用于可扩展自监督图表示学习的子图对比。在*2020年IEEE国际数据挖掘会议（ICDM）*上。IEEE，222–231页。
- en: Jiménez-Luna et al. (2020) José Jiménez-Luna, Francesca Grisoni, and Gisbert
    Schneider. 2020. Drug discovery with explainable artificial intelligence. *Nature
    Machine Intelligence* 2, 10 (2020), 573–584.
  id: totrans-1167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiménez-Luna et al. (2020) 何塞·吉门尼斯-卢纳、弗朗西斯卡·格里索尼和吉斯伯特·施奈德。2020年。利用可解释的人工智能进行药物发现。*自然机器智能*
    2卷，第10期（2020年），573–584页。
- en: Jin et al. (2020b) Guangyin Jin, Zhexu Xi, Hengyu Sha, Yanghe Feng, and Jincai
    Huang. 2020b. Deep multi-view spatiotemporal virtual graph neural network for
    significant citywide ride-hailing demand prediction. *arXiv preprint arXiv:2007.15189*
    (2020).
  id: totrans-1168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin et al. (2020b) 金光银、席哲旭、沙恒宇、冯杨赫和黄金才。2020b。用于显著城市范围内叫车需求预测的深度多视角时空虚拟图神经网络。*arXiv预印本
    arXiv:2007.15189*（2020年）。
- en: Jin et al. (2018) Wengong Jin, Regina Barzilay, and Tommi Jaakkola. 2018. Junction
    tree variational autoencoder for molecular graph generation. In *International
    conference on machine learning*. PMLR, 2323–2332.
  id: totrans-1169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin et al. (2018) 金文功、瑞吉娜·巴兹莱和汤米·贾卡拉。2018年。用于分子图生成的连接树变分自编码器。在*国际机器学习会议*上。PMLR，2323–2332页。
- en: 'Jin et al. (2020a) Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang,
    Zitao Liu, and Jiliang Tang. 2020a. Self-supervised learning on graphs: Deep insights
    and new direction. *arXiv preprint arXiv:2006.10141* (2020).'
  id: totrans-1170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin et al. (2020a) 金伟、泰勒·德尔、刘浩晨、王一奇、王苏航、刘子涛和唐继良。2020a。图上的自监督学习：深度见解与新方向。*arXiv预印本
    arXiv:2006.10141*（2020年）。
- en: Jing et al. (2020) Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL
    Townshend, and Ron Dror. 2020. Learning from protein structure with geometric
    vector perceptrons. *arXiv preprint arXiv:2009.01411* (2020).
  id: totrans-1171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jing et al. (2020) 景博文、斯蒂芬·艾斯曼、帕特里夏·苏里安娜、拉斐尔·JL·汤申德和罗恩·德罗。2020年。利用几何向量感知机从蛋白质结构中学习。*arXiv预印本
    arXiv:2009.01411*（2020年）。
- en: Jing et al. (2021) Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre
    Townshend, and Ron Dror. 2021. Learning from Protein Structure with Geometric
    Vector Perceptrons. In *ICLR*. [https://openreview.net/forum?id=1YLJDvSx6J4](https://openreview.net/forum?id=1YLJDvSx6J4)
  id: totrans-1172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jing 等人（2021）**鲍文·景**、**斯蒂芬·艾斯曼**、**帕特里夏·苏里亚纳**、**拉斐尔·约翰·拉马尔·汤申德** 和 **罗恩·德罗尔**。2021。从蛋白质结构中学习几何向量感知机。载于
    *ICLR*。 [https://openreview.net/forum?id=1YLJDvSx6J4](https://openreview.net/forum?id=1YLJDvSx6J4)
- en: Ju et al. (2023a) Wei Ju, Yiyang Gu, Xiao Luo, Yifan Wang, Haochen Yuan, Huasong
    Zhong, and Ming Zhang. 2023a. Unsupervised graph-level representation learning
    with hierarchical contrasts. *Neural Networks* 158 (2023), 359–368.
  id: totrans-1173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ju 等人（2023a）**魏瑜**、**顾奕阳**、**罗晓**、**王一帆**、**袁浩辰**、**钟华松** 和 **张铭**。2023a。具有层次对比的无监督图级表示学习。*神经网络*
    158（2023），359–368。
- en: Ju et al. (2023b) Wei Ju, Zequn Liu, Yifang Qin, Bin Feng, Chen Wang, Zhihui
    Guo, Xiao Luo, and Ming Zhang. 2023b. Few-shot molecular property prediction via
    Hierarchically Structured Learning on Relation Graphs. *Neural Networks* (2023).
  id: totrans-1174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ju 等人（2023b）**魏瑜**、**刘泽群**、**秦一芳**、**冯斌**、**王晨**、**郭志辉**、**罗晓** 和 **张铭**。2023b。通过在关系图上的分层结构学习进行少样本分子性质预测。*神经网络*（2023）。
- en: 'Ju et al. (2022a) Wei Ju, Xiao Luo, Zeyu Ma, Junwei Yang, Minghua Deng, and
    Ming Zhang. 2022a. GHNN: Graph Harmonic Neural Networks for semi-supervised graph-level
    classification. *Neural Networks* 151 (2022), 70–79.'
  id: totrans-1175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ju 等人（2022a）**魏瑜**、**罗晓**、**马泽宇**、**杨俊伟**、**邓明华** 和 **张铭**。2022a。GHNN：用于半监督图级分类的图谐波神经网络。*神经网络*
    151（2022），70–79。
- en: 'Ju et al. (2022b) Wei Ju, Xiao Luo, Meng Qu, Yifan Wang, Chong Chen, Minghua
    Deng, Xian-Sheng Hua, and Ming Zhang. 2022b. TGNN: A Joint Semi-supervised Framework
    for Graph-level Classification. In *Proceedings of the International Joint Conference
    on Artificial Intelligence*. 2122–2128.'
  id: totrans-1176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ju 等人（2022b）**魏瑜**、**罗晓**、**曲萌**、**王一帆**、**陈冲**、**邓明华**、**华显生** 和 **张铭**。2022b。TGNN：一种用于图级分类的联合半监督框架。载于
    *国际人工智能联合会议论文集*。2122–2128。
- en: Ju et al. (2022c) Wei Ju, Yifang Qin, Ziyue Qiao, Xiao Luo, Yifan Wang, Yanjie
    Fu, and Ming Zhang. 2022c. Kernel-based Substructure Exploration for Next POI
    Recommendation. *arXiv preprint arXiv:2210.03969* (2022).
  id: totrans-1177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ju 等人（2022c）**魏瑜**、**秦一芳**、**乔子越**、**罗晓**、**王一帆**、**傅彦杰** 和 **张铭**。2022c。基于核的子结构探索用于下一个
    POI 推荐。*arXiv 预印本 arXiv:2210.03969*（2022）。
- en: 'Ju et al. (2022d) Wei Ju, Junwei Yang, Meng Qu, Weiping Song, Jianhao Shen,
    and Ming Zhang. 2022d. KGNN: Harnessing Kernel-based Networks for Semi-supervised
    Graph Classification. In *Proceedings of the Fifteenth ACM International Conference
    on Web Search and Data Mining*. 421–429.'
  id: totrans-1178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ju 等人（2022d）**魏瑜**、**杨俊伟**、**曲萌**、**宋伟平**、**沈建浩** 和 **张铭**。2022d。KGNN：利用基于核的网络进行半监督图分类。载于
    *第十五届 ACM 国际网络搜索与数据挖掘会议论文集*。421–429。
- en: Kang et al. (2012) U Kang, Hanghang Tong, and Jimeng Sun. 2012. Fast random
    walk graph kernel. In *Proceedings of the SIAM international conference on data
    mining*. SIAM, 828–838.
  id: totrans-1179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 等人（2012）**U·康**、**童航航** 和 **孙继铭**。2012。快速随机游走图核。载于 *SIAM 数据挖掘国际会议论文集*。SIAM，828–838。
- en: Karpinski and Macintyre (1997) Marek Karpinski and Angus Macintyre. 1997. Polynomial
    bounds for VC dimension of sigmoidal and general Pfaffian neural networks. *J.
    Comput. System Sci.* 54, 1 (1997), 169–176.
  id: totrans-1180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpinski 和 Macintyre（1997）**马雷克·卡尔平斯基** 和 **安格斯·麦金泰尔**。1997。具有 sigmoid 和一般
    Pfaffian 神经网络的 VC 维度的多项式界限。*计算机系统科学杂志* 54，第 1 期（1997），169–176。
- en: Kashima et al. (2003) Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. 2003.
    Marginalized kernels between labeled graphs. In *Proceedings of international
    conference on machine learning*. 321–328.
  id: totrans-1181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kashima 等人（2003）**鹿岛久志**、**津田浩司** 和 **井口明弘**。2003。标记图之间的边缘化核。载于 *国际机器学习会议论文集*。321–328。
- en: Keikha et al. (2020) Mohammad Mehdi Keikha, Maseud Rahgozar, Masoud Asadpour,
    and Mohammad Faghih Abdollahi. 2020. Influence maximization across heterogeneous
    interconnected networks based on deep learning. *Expert Systems with Applications*
    140 (2020), 112905.
  id: totrans-1182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keikha 等人（2020）**穆罕默德·梅赫迪·凯赫卡**、**马苏德·拉赫戈扎尔**、**马苏德·阿萨德普尔** 和 **穆罕默德·法基赫·阿卜杜拉希**。2020。基于深度学习的异构互连网络中的影响力最大化。*应用专家系统*
    140（2020），112905。
- en: Khoshraftar and An (2022) Shima Khoshraftar and Aijun An. 2022. A Survey on
    Graph Representation Learning Methods. *arXiv preprint arXiv:2204.01855* (2022).
  id: totrans-1183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khoshraftar 和 An（2022）**希玛·科什拉夫塔** 和 **安爱军**。2022。图表示学习方法综述。*arXiv 预印本 arXiv:2204.01855*（2022）。
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding
    variational bayes. *arXiv preprint arXiv:1312.6114* (2013).
  id: totrans-1184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Welling（2013）**迪德里克·P·金马** 和 **马克斯·韦林**。2013。自编码变分贝叶斯。*arXiv 预印本 arXiv:1312.6114*（2013）。
- en: Kipf et al. (2018) Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling,
    and Richard Zemel. 2018. Neural relational inference for interacting systems.
    In *International Conference on Machine Learning*. PMLR, 2688–2697.
  id: totrans-1185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf et al. (2018) Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling 和
    Richard Zemel. 2018. 交互系统的神经关系推理。发表于*International Conference on Machine Learning*。PMLR,
    2688–2697.
- en: Kipf and Welling (2016a) Thomas N Kipf and Max Welling. 2016a. Semi-supervised
    classification with graph convolutional networks. *arXiv preprint arXiv:1609.02907*
    (2016).
  id: totrans-1186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf and Welling (2016a) Thomas N Kipf 和 Max Welling. 2016a. 基于图卷积网络的半监督分类。*arXiv
    预印本 arXiv:1609.02907* (2016).
- en: Kipf and Welling (2016b) Thomas N Kipf and Max Welling. 2016b. Variational graph
    auto-encoders. *arXiv preprint arXiv:1611.07308* (2016).
  id: totrans-1187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf and Welling (2016b) Thomas N Kipf 和 Max Welling. 2016b. 变分图自编码器。*arXiv
    预印本 arXiv:1611.07308* (2016).
- en: 'Klicpera et al. (2021) Johannes Klicpera, Florian Becker, and Stephan Günnemann.
    2021. GemNet: Universal Directional Graph Neural Networks for Molecules. In *NeurIPS*.
    [https://openreview.net/forum?id=HS_sOaxS9K-](https://openreview.net/forum?id=HS_sOaxS9K-)'
  id: totrans-1188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Klicpera et al. (2021) Johannes Klicpera, Florian Becker 和 Stephan Günnemann.
    2021. GemNet: 用于分子的通用方向图神经网络。发表于*NeurIPS*。 [https://openreview.net/forum?id=HS_sOaxS9K-](https://openreview.net/forum?id=HS_sOaxS9K-)'
- en: Klicpera et al. (2020) Johannes Klicpera, Janek Groß, and Stephan Günnemann.
    2020. Directional Message Passing for Molecular Graphs. In *ICLR*. [https://openreview.net/forum?id=B1eWbxStPH](https://openreview.net/forum?id=B1eWbxStPH)
  id: totrans-1189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klicpera et al. (2020) Johannes Klicpera, Janek Groß 和 Stephan Günnemann. 2020.
    分子图的方向消息传递。发表于*ICLR*。 [https://openreview.net/forum?id=B1eWbxStPH](https://openreview.net/forum?id=B1eWbxStPH)
- en: 'Köhler et al. (2020) Jonas Köhler, Leon Klein, and Frank Noe. 2020. Equivariant
    Flows: Exact Likelihood Generative Learning for Symmetric Densities. In *ICML*.
    [https://proceedings.mlr.press/v119/kohler20a.html](https://proceedings.mlr.press/v119/kohler20a.html)'
  id: totrans-1190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Köhler et al. (2020) Jonas Köhler, Leon Klein 和 Frank Noe. 2020. 等变流：对称密度的精确似然生成学习。发表于*ICML*。
    [https://proceedings.mlr.press/v119/kohler20a.html](https://proceedings.mlr.press/v119/kohler20a.html)
- en: Kong et al. (2017) Xiangjie Kong, Huizhen Jiang, Wei Wang, Teshome Megersa Bekele,
    Zhenzhen Xu, and Meng Wang. 2017. Exploring dynamic research interest and academic
    influence for scientific collaborator recommendation. *Scientometrics* 113, 1
    (2017), 369–385.
  id: totrans-1191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kong et al. (2017) Xiangjie Kong, Huizhen Jiang, Wei Wang, Teshome Megersa Bekele,
    Zhenzhen Xu 和 Meng Wang. 2017. 探索动态研究兴趣和学术影响力以进行科学合作者推荐。*Scientometrics* 113,
    1 (2017), 369–385.
- en: Kong et al. (2016) Xiangjie Kong, Huizhen Jiang, Zhuo Yang, Zhenzhen Xu, Feng
    Xia, and Amr Tolba. 2016. Exploiting publication contents and collaboration networks
    for collaborator recommendation. *PloS one* 11, 2 (2016), e0148492.
  id: totrans-1192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kong et al. (2016) Xiangjie Kong, Huizhen Jiang, Zhuo Yang, Zhenzhen Xu, Feng
    Xia 和 Amr Tolba. 2016. 利用出版内容和合作网络进行合作者推荐。*PloS one* 11, 2 (2016), e0148492.
- en: 'Kong et al. (2019) Xiangjie Kong, Yajie Shi, Shuo Yu, Jiaying Liu, and Feng
    Xia. 2019. Academic social networks: Modeling, analysis, mining and applications.
    *Journal of Network and Computer Applications* 132 (2019), 86–103.'
  id: totrans-1193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kong et al. (2019) Xiangjie Kong, Yajie Shi, Shuo Yu, Jiaying Liu 和 Feng Xia.
    2019. 学术社交网络：建模、分析、挖掘和应用。*Journal of Network and Computer Applications* 132 (2019),
    86–103.
- en: 'Koren (2008) Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted
    collaborative filtering model. In *Proceedings of the 14th ACM SIGKDD international
    conference on Knowledge discovery and data mining*. 426–434.'
  id: totrans-1194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koren (2008) Yehuda Koren. 2008. 分解与邻域相遇：一个多方面的协同过滤模型。发表于*Proceedings of the
    14th ACM SIGKDD international conference on Knowledge discovery and data mining*。426–434.
- en: Koren et al. (2009) Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix
    factorization techniques for recommender systems. *Computer* 42, 8 (2009), 30–37.
  id: totrans-1195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koren et al. (2009) Yehuda Koren, Robert Bell 和 Chris Volinsky. 2009. 推荐系统的矩阵分解技术。*Computer*
    42, 8 (2009), 30–37.
- en: 'Krenn et al. (2020) Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich,
    and Alan Aspuru-Guzik. 2020. Self-referencing embedded strings (SELFIES): A 100%
    robust molecular string representation. *Machine Learning: Science and Technology*
    1, 4 (2020), 045024.'
  id: totrans-1196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Krenn et al. (2020) Mario Krenn, Florian Häse, Akshat Kumar Nigam, Pascal Friederich
    和 Alan Aspuru-Guzik. 2020. 自引用嵌入字符串（SELFIES）：一种100%鲁棒的分子字符串表示。*Machine Learning:
    Science and Technology* 1, 4 (2020), 045024.'
- en: Kreuzer et al. (2021) Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent
    Létourneau, and Prudencio Tossou. 2021. Rethinking graph transformers with spectral
    attention. *Advances in Neural Information Processing Systems* 34 (2021), 21618–21629.
  id: totrans-1197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kreuzer et al. (2021) Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent
    Létourneau 和 Prudencio Tossou. 2021. 重新思考图变换器与谱注意力。*Advances in Neural Information
    Processing Systems* 34 (2021), 21618–21629.
- en: Kriege et al. (2020) Nils M Kriege, Fredrik D Johansson, and Christopher Morris.
    2020. A survey on graph kernels. *Applied Network Science* 5, 1 (2020), 1–42.
  id: totrans-1198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kriege 等 (2020) Nils M Kriege, Fredrik D Johansson, 和 Christopher Morris. 2020.
    图核的综述。*应用网络科学* 5, 1 (2020), 1–42。
- en: Krizhevsky et al. (2017) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2017. Imagenet classification with deep convolutional neural networks. *Commun.
    ACM* 60, 6 (2017), 84–90.
  id: totrans-1199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 (2017) Alex Krizhevsky, Ilya Sutskever, 和 Geoffrey E Hinton. 2017.
    使用深度卷积神经网络进行 Imagenet 分类。*Commun. ACM* 60, 6 (2017), 84–90。
- en: Kumar et al. (2022) Sanjay Kumar, Abhishek Mallik, Anavi Khetarpal, and BS Panda.
    2022. Influence maximization in social networks using graph embedding and graph
    neural network. *Information Sciences* 607 (2022), 1617–1636.
  id: totrans-1200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等 (2022) Sanjay Kumar, Abhishek Mallik, Anavi Khetarpal, 和 BS Panda. 2022.
    使用图嵌入和图神经网络进行社交网络中的影响力最大化。*信息科学* 607 (2022), 1617–1636。
- en: Laine and Aila (2016) Samuli Laine and Timo Aila. 2016. Temporal ensembling
    for semi-supervised learning. *arXiv preprint arXiv:1610.02242* (2016).
  id: totrans-1201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laine 和 Aila (2016) Samuli Laine 和 Timo Aila. 2016. 适用于半监督学习的时间集成。*arXiv 预印本
    arXiv:1610.02242* (2016)。
- en: 'Landrum et al. (2013) Greg Landrum et al. 2013. RDKit: A software suite for
    cheminformatics, computational chemistry, and predictive modeling. *Greg Landrum*
    (2013).'
  id: totrans-1202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Landrum 等 (2013) Greg Landrum 等. 2013. RDKit：用于化学信息学、计算化学和预测建模的软件套件。*Greg Landrum*
    (2013)。
- en: Lee et al. (2021) Dongha Lee, Su Kim, Seonghyeon Lee, Chanyoung Park, and Hwanjo
    Yu. 2021. Learnable structural semantic readout for graph classification. In *2021
    IEEE International Conference on Data Mining (ICDM)*. IEEE, 1180–1185.
  id: totrans-1203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2021) Dongha Lee, Su Kim, Seonghyeon Lee, Chanyoung Park, 和 Hwanjo Yu.
    2021. 可学习的结构语义读取用于图分类。发表于 *2021 IEEE 国际数据挖掘会议 (ICDM)*。IEEE, 1180–1185。
- en: 'Lee et al. (2013) Dong-Hyun Lee et al. 2013. Pseudo-label: The simple and efficient
    semi-supervised learning method for deep neural networks. In *Workshop on challenges
    in representation learning, ICML*, Vol. 3\. 896.'
  id: totrans-1204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2013) Dong-Hyun Lee 等. 2013. 伪标签：一种简单高效的深度神经网络半监督学习方法。发表于 *表征学习挑战工作坊，ICML*，第
    3 卷，896。
- en: Lee et al. (2019) Junhyun Lee, Inyeop Lee, and Jaewoo Kang. 2019. Self-attention
    graph pooling. In *International conference on machine learning*. PMLR, 3734–3743.
  id: totrans-1205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2019) Junhyun Lee, Inyeop Lee, 和 Jaewoo Kang. 2019. 自注意力图池化。发表于 *国际机器学习会议*。PMLR,
    3734–3743。
- en: Lei et al. (2017) Tao Lei, Wengong Jin, Regina Barzilay, and Tommi Jaakkola.
    2017. Deriving Neural Architectures from Sequence and Graph Kernels. In *International
    Conference on Machine Learning*. 2024–2033.
  id: totrans-1206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等 (2017) Tao Lei, Wengong Jin, Regina Barzilay, 和 Tommi Jaakkola. 2017.
    从序列和图核中推导神经架构。发表于 *国际机器学习会议*。2024–2033。
- en: 'Levie et al. (2018) Ron Levie, Federico Monti, Xavier Bresson, and Michael M
    Bronstein. 2018. Cayleynets: Graph convolutional neural networks with complex
    rational spectral filters. *IEEE Transactions on Signal Processing* 67, 1 (2018),
    97–109.'
  id: totrans-1207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levie 等 (2018) Ron Levie, Federico Monti, Xavier Bresson, 和 Michael M Bronstein.
    2018. Cayleynets：具有复数有理谱滤波器的图卷积神经网络。*IEEE 信号处理学报* 67, 1 (2018), 97–109。
- en: Li and Pan (2016) Angsheng Li and Yicheng Pan. 2016. Structural information
    and dynamical complexity of networks. *IEEE Transactions on Information Theory*
    62, 6 (2016), 3290–3339.
  id: totrans-1208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Pan (2016) Angsheng Li 和 Yicheng Pan. 2016. 网络的结构信息和动态复杂性。*IEEE 信息理论学报*
    62, 6 (2016), 3290–3339。
- en: 'Li et al. (2020) Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem.
    2020. Deepergcn: All you need to train deeper gcns. *arXiv preprint arXiv:2006.07739*
    (2020).'
  id: totrans-1209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2020) Guohao Li, Chenxin Xiong, Ali Thabet, 和 Bernard Ghanem. 2020. Deepergcn:
    训练更深 GCNS 的一切所需。*arXiv 预印本 arXiv:2006.07739* (2020)。'
- en: 'Li et al. (2022b) Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2022b.
    Ood-gnn: Out-of-distribution generalized graph neural network. *IEEE Transactions
    on Knowledge and Data Engineering* (2022).'
  id: totrans-1210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2022b) Haoyang Li, Xin Wang, Ziwei Zhang, 和 Wenwu Zhu. 2022b. Ood-gnn：分布外泛化图神经网络。*IEEE
    知识与数据工程学报* (2022)。
- en: 'Li et al. (2022c) Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2022c.
    Out-of-distribution generalization on graphs: A survey. *arXiv preprint arXiv:2202.07987*
    (2022).'
  id: totrans-1211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2022c) Haoyang Li, Xin Wang, Ziwei Zhang, 和 Wenwu Zhu. 2022c. 图上的分布外泛化：综述。*arXiv
    预印本 arXiv:2202.07987* (2022)。
- en: Li et al. (2017a) Junying Li, Deng Cai, and Xiaofei He. 2017a. Learning graph-level
    representation for drug discovery. *arXiv preprint arXiv:1709.03741* (2017).
  id: totrans-1212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2017a) Junying Li, Deng Cai, 和 Xiaofei He. 2017a. 药物发现的图级表示学习。*arXiv 预印本
    arXiv:1709.03741* (2017)。
- en: Li et al. (2022a) Jia Li, Yongfeng Huang, Heng Chang, and Yu Rong. 2022a. Semi-Supervised
    Hierarchical Graph Classification. *IEEE Transactions on Pattern Analysis and
    Machine Intelligence* (2022).
  id: totrans-1213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022a）佳李、永丰黄、恒常和余荣。2022a。半监督层次图分类。*IEEE模式分析与机器智能汇刊*（2022年）。
- en: Li et al. (2017b) Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian,
    and Jun Ma. 2017b. Neural attentive session-based recommendation. In *Proceedings
    of the 2017 ACM on Conference on Information and Knowledge Management*. 1419–1428.
  id: totrans-1214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2017b）静李、鹏杰任、竹敏陈、赵春任、陶联和君马。2017b。神经注意会话推荐。在*2017年ACM信息与知识管理会议论文集*。1419–1428。
- en: 'Li et al. (2019b) Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, and
    Junzhou Huang. 2019b. Semi-supervised graph classification: A hierarchical graph
    perspective. In *Proceedings of the Web Conference*. 972–982.'
  id: totrans-1215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2019b）佳李、余荣、洪城、海伦·孟、文冰黄和君洲黄。2019b。半监督图分类：一种层次图视角。在*网络会议论文集*。972–982。
- en: 'Li et al. (2022d) Peibo Li, Yixing Yang, Maurice Pagnucco, and Yang Song. 2022d.
    CoGNet: Cooperative Graph Neural Networks. In *Proceedings of the International
    Joint Conference on Neural Networks*.'
  id: totrans-1216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022d）沛博李、艺兴杨、莫里斯·帕尼科和杨宋。2022d。CoGNet：合作图神经网络。在*国际联合神经网络会议论文集*。
- en: Li et al. (2018a) Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018a. Deeper insights
    into graph convolutional networks for semi-supervised learning. In *Proceedings
    of the AAAI conference on artificial intelligence*, Vol. 32.
  id: totrans-1217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2018a）启麦李、志超韩和晓明吴。2018a。对半监督学习图卷积网络的更深刻见解。在*人工智能AAAI会议论文集*，第32卷。
- en: Li et al. (2018b) Ruoyu Li, Sheng Wang, Feiyun Zhu, and Junzhou Huang. 2018b.
    Adaptive graph convolutional neural networks. In *Proceedings of the AAAI conference
    on artificial intelligence*, Vol. 32.
  id: totrans-1218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2018b）鲁悠李、盛王、费云朱和君洲黄。2018b。自适应图卷积神经网络。在*人工智能AAAI会议论文集*，第32卷。
- en: 'Li et al. (2022e) Shuangli Li, Jingbo Zhou, Tong Xu, Dejing Dou, and Hui Xiong.
    2022e. Geomgcl: geometric graph contrastive learning for molecular property prediction.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 36. 4541–4549.'
  id: totrans-1219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022e）双力李、景博周、彤徐、德晶窦和辉熊。2022e。Geomgcl：用于分子属性预测的几何图对比学习。在*人工智能AAAI会议论文集*，第36卷。4541–4549。
- en: 'Li et al. (2019a) Yibo Li, Jianxing Hu, Yanxing Wang, Jielong Zhou, Liangren
    Zhang, and Zhenming Liu. 2019a. Deepscaffold: a comprehensive tool for scaffold-based
    de novo drug discovery using deep learning. *Journal of chemical information and
    modeling* 60, 1 (2019), 77–91.'
  id: totrans-1220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2019a）毅博李、建星胡、艳星王、杰龙周、良仁张和振明刘。2019a。Deepscaffold：一种基于支架的深度学习药物发现综合工具。*化学信息与建模期刊*
    60，1（2019），77–91。
- en: Li et al. (2021a) Yibo Li, Jianfeng Pei, and Luhua Lai. 2021a. Learning to design
    drug-like molecules in three-dimensional space using deep generative models. *arXiv
    preprint arXiv:2104.08474* (2021).
  id: totrans-1221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021a）毅博李、建锋裴和吕华赖。2021a。利用深度生成模型在三维空间中设计类药物分子。*arXiv 预印本 arXiv:2104.08474*（2021年）。
- en: Li et al. (2021b) Yibo Li, Jianfeng Pei, and Luhua Lai. 2021b. Structure-based
    de novo drug design using 3D deep generative models. *Chemical science* 12, 41
    (2021), 13664–13675.
  id: totrans-1222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021b）毅博李、建锋裴和吕华赖。2021b。基于结构的全新药物设计，使用3D深度生成模型。*化学科学* 12，41（2021），13664–13675。
- en: Li et al. (2015) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
    2015. Gated graph sequence neural networks. *arXiv preprint arXiv:1511.05493*
    (2015).
  id: totrans-1223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2015）宇佳李、丹尼尔·塔洛、马克·布罗克施密特和理查德·泽梅尔。2015。门控图序列神经网络。*arXiv 预印本 arXiv:1511.05493*（2015年）。
- en: 'Li et al. (2017c) Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2017c. Diffusion
    convolutional recurrent neural network: Data-driven traffic forecasting. *arXiv
    preprint arXiv:1707.01926* (2017).'
  id: totrans-1224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2017c）亚光李、玫瑰余、塞鲁斯·沙哈比和燕刘。2017c。扩散卷积递归神经网络：数据驱动的交通预测。*arXiv 预印本 arXiv:1707.01926*（2017年）。
- en: Li et al. (2016) Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow.
    2016. Gated Graph Sequence Neural Networks. In *Proceedings of ICLR’16*.
  id: totrans-1225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2016）宇佳李、理查德·泽梅尔、马克·布罗克施密特和丹尼尔·塔洛。2016。门控图序列神经网络。在*ICLR’16会议论文集*。
- en: 'Li et al. (2019c) Ziyao Li, Liang Zhang, and Guojie Song. 2019c. GCN-LASE:
    Towards Adequately Incorporating Link Attributes in Graph Convolutional Networks.
    In *Proceedings of the Twenty-Eighth International Joint Conference on Artificial
    Intelligence, IJCAI-19*. 2959–2965.'
  id: totrans-1226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2019c）子瑶李、梁张和国杰宋。2019c。GCN-LASE：在图卷积网络中充分考虑链接属性。 In *第二十八届国际联合人工智能会议论文集，IJCAI-19*。2959–2965。
- en: 'Liang et al. (2020) Yanyan Liang, Yanfeng Zhang, Dechao Gao, and Qian Xu. 2020.
    MxPool: Multiplex Pooling for Hierarchical Graph Representation Learning. *arXiv
    preprint arXiv:2004.06846* (2020).'
  id: totrans-1227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang 等 (2020) Yanyan Liang, Yanfeng Zhang, Dechao Gao, 和 Qian Xu. 2020. MxPool:
    用于分层图表示学习的多重池化。*arXiv 预印本 arXiv:2004.06846* (2020)。'
- en: 'Liao et al. (2018) Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard Zemel.
    2018. LanczosNet: Multi-Scale Deep Graph Convolutional Networks. In *International
    Conference on Learning Representations*.'
  id: totrans-1228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liao 等 (2018) Renjie Liao, Zhizhen Zhao, Raquel Urtasun, 和 Richard Zemel. 2018.
    LanczosNet: 多尺度深度图卷积网络。发表于 *国际学习表示会议*。'
- en: Lim et al. (2021) Jongin Lim, Daeho Um, Hyung Jin Chang, Dae Ung Jo, and Jin Young
    Choi. 2021. Class-attentive diffusion network for semi-supervised classification.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 35. 8601–8609.
  id: totrans-1229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lim 等 (2021) Jongin Lim, Daeho Um, Hyung Jin Chang, Dae Ung Jo, 和 Jin Young
    Choi. 2021. 用于半监督分类的类注意力扩散网络。发表于 *AAAI 人工智能会议论文集*，第35卷。8601–8609。
- en: 'Lin et al. (2022a) Haitao Lin, Yufei Huang, Meng Liu, Xuanjing Li, Shuiwang
    Ji, and Stan Z Li. 2022a. DiffBP: Generative Diffusion of 3D Molecules for Target
    Protein Binding. *arXiv preprint arXiv:2211.11214* (2022).'
  id: totrans-1230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 (2022a) Haitao Lin, Yufei Huang, Meng Liu, Xuanjing Li, Shuiwang Ji,
    和 Stan Z Li. 2022a. DiffBP: 用于靶蛋白结合的三维分子的生成扩散。*arXiv 预印本 arXiv:2211.11214* (2022)。'
- en: 'Lin et al. (2022b) Jiacheng Lin, Hanwen Xu, Addie Woicik, Jianzhu Ma, and Sheng
    Wang. 2022b. Pisces: A cross-modal contrastive learning approach to synergistic
    drug combination prediction. *bioRxiv* (2022). [https://doi.org/10.1101/2022.11.21.517439](https://doi.org/10.1101/2022.11.21.517439)'
  id: totrans-1231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 (2022b) Jiacheng Lin, Hanwen Xu, Addie Woicik, Jianzhu Ma, 和 Sheng Wang.
    2022b. Pisces: 一种用于协同药物组合预测的跨模态对比学习方法。*bioRxiv* (2022)。 [https://doi.org/10.1101/2022.11.21.517439](https://doi.org/10.1101/2022.11.21.517439)'
- en: 'Liu et al. (2019) Jiaying Liu, Feng Xia, Lei Wang, Bo Xu, Xiangjie Kong, Hanghang
    Tong, and Irwin King. 2019. Shifu2: A network representation learning based model
    for advisor-advisee relationship mining. *IEEE Transactions on Knowledge and Data
    Engineering* 33, 4 (2019), 1763–1777.'
  id: totrans-1232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2019) Jiaying Liu, Feng Xia, Lei Wang, Bo Xu, Xiangjie Kong, Hanghang
    Tong, 和 Irwin King. 2019. Shifu2: 基于网络表示学习的顾问-被顾问关系挖掘模型。*IEEE 知识与数据工程汇刊* 33, 4
    (2019), 1763–1777。'
- en: Liu et al. (2016) Li Liu, William K Cheung, Xin Li, and Lejian Liao. 2016. Aligning
    Users across Social Networks Using Network Embedding.. In *Ijcai*, Vol. 16. 1774–80.
  id: totrans-1233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2016) Li Liu, William K Cheung, Xin Li, 和 Lejian Liao. 2016. 使用网络嵌入对齐社交网络中的用户。发表于
    *Ijcai*，第16卷。1774–80。
- en: Liu et al. (2022a) Meng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, and Shuiwang
    Ji. 2022a. Generating 3D Molecules for Target Protein Binding. *arXiv preprint
    arXiv:2204.09410* (2022).
  id: totrans-1234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2022a) Meng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, 和 Shuiwang
    Ji. 2022a. 生成用于靶蛋白结合的三维分子。*arXiv 预印本 arXiv:2204.09410* (2022)。
- en: Liu et al. (2018a) Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander
    Gaunt. 2018a. Constrained graph variational autoencoders for molecule design.
    *Advances in neural information processing systems* 31 (2018).
  id: totrans-1235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2018a) Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, 和 Alexander Gaunt.
    2018a. 用于分子设计的约束图变分自编码器。*神经信息处理系统进展* 31 (2018)。
- en: 'Liu et al. (2018d) Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. 2018d.
    STAMP: short-term attention/memory priority model for session-based recommendation.
    In *Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery
    & data mining*. 1831–1839.'
  id: totrans-1236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2018d) Qiao Liu, Yifu Zeng, Refuoe Mokhosi, 和 Haibin Zhang. 2018d. STAMP:
    用于会话推荐的短期注意力/记忆优先模型。发表于 *第24届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*。1831–1839。'
- en: Liu et al. (2022b) Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora
    Oztekin, and Shuiwang Ji. 2022b. Spherical Message Passing for 3D Molecular Graphs.
    In *ICLR*. [https://openreview.net/forum?id=givsRXsOt9r](https://openreview.net/forum?id=givsRXsOt9r)
  id: totrans-1237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2022b) Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin,
    和 Shuiwang Ji. 2022b. 用于三维分子图的球面信息传递。发表于 *ICLR*。 [https://openreview.net/forum?id=givsRXsOt9r](https://openreview.net/forum?id=givsRXsOt9r)
- en: Liu et al. (2018b) Ziqi Liu, Chaochao Chen, Xinxing Yang, Jun Zhou, Xiaolong
    Li, and Le Song. 2018b. Heterogeneous graph neural networks for malicious account
    detection. In *Proceedings of the 27th ACM International Conference on Information
    and Knowledge Management*. 2077–2085.
  id: totrans-1238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2018b) Ziqi Liu, Chaochao Chen, Xinxing Yang, Jun Zhou, Xiaolong Li,
    和 Le Song. 2018b. 用于恶意账户检测的异质图神经网络。发表于 *第27届 ACM 国际信息与知识管理大会论文集*。2077–2085。
- en: 'Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,
    Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer
    using shifted windows. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*. 10012–10022.'
  id: totrans-1239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,
    Stephen Lin, 和 Baining Guo. 2021. Swin变换器：使用移位窗口的分层视觉变换器。在*IEEE/CVF国际计算机视觉会议论文集*。10012–10022。
- en: Liu et al. (2018c) Zheng Liu, Xing Xie, and Lei Chen. 2018c. Context-aware academic
    collaborator recommendation. In *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*. 1870–1879.
  id: totrans-1240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018c) Zheng Liu, Xing Xie, 和 Lei Chen. 2018c. 上下文感知的学术合作者推荐。在*第24届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*。1870–1879。
- en: Long et al. (2021a) Qingqing Long, Yilun Jin, Yi Wu, and Guojie Song. 2021a.
    Theoretically improving graph neural networks via anonymous walk graph kernels.
    In *Proceedings of the Web Conference 2021*. 1204–1214.
  id: totrans-1241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. (2021a) Qingqing Long, Yilun Jin, Yi Wu, 和 Guojie Song. 2021a. 通过匿名游走图核理论上改进图神经网络。在*2021年网络会议论文集*。1204–1214。
- en: 'Long et al. (2021b) Qingqing Long, Lingjun Xu, Zheng Fang, and Guojie Song.
    2021b. HGK-GNN: Heterogeneous Graph Kernel based Graph Neural Networks. In *Proceedings
    of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining*. 1129–1138.'
  id: totrans-1242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. (2021b) Qingqing Long, Lingjun Xu, Zheng Fang, 和 Guojie Song. 2021b.
    HGK-GNN：基于异质图核的图神经网络。在*第27届ACM SIGKDD知识发现与数据挖掘会议论文集*。1129–1138。
- en: Long et al. (2022) Siyu Long, Yi Zhou, Xinyu Dai, and Hao Zhou. 2022. Zero-Shot
    3D Drug Design by Sketching and Generating. *arXiv preprint arXiv:2209.13865*
    (2022).
  id: totrans-1243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. (2022) Siyu Long, Yi Zhou, Xinyu Dai, 和 Hao Zhou. 2022. 通过素描和生成进行零样本3D药物设计。*arXiv预印本
    arXiv:2209.13865* (2022)。
- en: 'Luo et al. (2021a) Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao
    Ni, Haifeng Chen, and Xiang Zhang. 2021a. Learning to drop: Robust graph neural
    network via topological denoising. In *Proceedings of the 14th ACM international
    conference on web search and data mining*. 779–787.'
  id: totrans-1244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2021a) Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni,
    Haifeng Chen, 和 Xiang Zhang. 2021a. 学会丢弃：通过拓扑去噪增强鲁棒性的图神经网络。在*第14届ACM国际网络搜索与数据挖掘会议论文集*。779–787。
- en: Luo et al. (2021b) Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. 2021b.
    A 3D generative model for structure-based drug design. *Advances in Neural Information
    Processing Systems* 34 (2021), 6229–6239.
  id: totrans-1245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2021b) Shitong Luo, Jiaqi Guan, Jianzhu Ma, 和 Jian Peng. 2021b.
    一种用于基于结构的药物设计的3D生成模型。*神经信息处理系统进展* 34 (2021), 6229–6239。
- en: 'Luo et al. (2022a) Xiao Luo, Wei Ju, Meng Qu, Chong Chen, Minghua Deng, Xian-Sheng
    Hua, and Ming Zhang. 2022a. DualGraph: Improving Semi-supervised Graph Classification
    via Dual Contrastive Learning. In *Proceedings of the IEEE International Conference
    on Data Engineering*. 699–712.'
  id: totrans-1246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2022a) Xiao Luo, Wei Ju, Meng Qu, Chong Chen, Minghua Deng, Xian-Sheng
    Hua, 和 Ming Zhang. 2022a. DualGraph：通过双重对比学习改进半监督图分类。在*IEEE国际数据工程会议论文集*。699–712。
- en: 'Luo et al. (2022b) Xiao Luo, Wei Ju, Meng Qu, Yiyang Gu, Chong Chen, Minghua
    Deng, Xian-Sheng Hua, and Ming Zhang. 2022b. Clear: Cluster-enhanced contrast
    for self-supervised graph representation learning. *IEEE Transactions on Neural
    Networks and Learning Systems* (2022).'
  id: totrans-1247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2022b) Xiao Luo, Wei Ju, Meng Qu, Yiyang Gu, Chong Chen, Minghua
    Deng, Xian-Sheng Hua, 和 Ming Zhang. 2022b. Clear：自监督图表示学习中的簇增强对比。*IEEE神经网络与学习系统汇刊*
    (2022)。
- en: 'Luo et al. (2021c) Xiao Luo, Daqing Wu, Zeyu Ma, Chong Chen, Minghua Deng,
    Jinwen Ma, Zhongming Jin, Jianqiang Huang, and Xian-Sheng Hua. 2021c. CIMON: Towards
    High-quality Hash Codes. In *Proceedings of the International Joint Conference
    on Artificial Intelligence*.'
  id: totrans-1248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2021c) Xiao Luo, Daqing Wu, Zeyu Ma, Chong Chen, Minghua Deng, Jinwen
    Ma, Zhongming Jin, Jianqiang Huang, 和 Xian-Sheng Hua. 2021c. CIMON：面向高质量哈希码。在*国际人工智能联合会议论文集*。
- en: Luo and Ji (2022) Youzhi Luo and Shuiwang Ji. 2022. An autoregressive flow model
    for 3d molecular geometry generation from scratch. In *International Conference
    on Learning Representations (ICLR)*.
  id: totrans-1249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo and Ji (2022) Youzhi Luo 和 Shuiwang Ji. 2022. 从头开始的3D分子几何生成的自回归流模型。在*国际学习表征会议
    (ICLR)*。
- en: 'Luo et al. (2021d) Youzhi Luo, Keqiang Yan, and Shuiwang Ji. 2021d. Graphdf:
    A discrete flow model for molecular graph generation. In *International Conference
    on Machine Learning*. PMLR, 7192–7203.'
  id: totrans-1250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2021d) Youzhi Luo, Keqiang Yan, 和 Shuiwang Ji. 2021d. Graphdf：用于分子图生成的离散流模型。在*国际机器学习会议*。PMLR,
    7192–7203。
- en: Ma et al. (2020a) Hehuan Ma, Yatao Bian, Yu Rong, Wenbing Huang, Tingyang Xu,
    Weiyang Xie, Geyan Ye, and Junzhou Huang. 2020a. Multi-view graph neural networks
    for molecular property prediction. *arXiv preprint arXiv:2005.13607* (2020).
  id: totrans-1251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2020a) Hehuan Ma, Yatao Bian, Yu Rong, Wenbing Huang, Tingyang Xu,
    Weiyang Xie, Geyan Ye, 和 Junzhou Huang. 2020a. 用于分子属性预测的多视角图神经网络。*arXiv预印本 arXiv:2005.13607*
    (2020)。
- en: Ma et al. (2021) Jiaqi Ma, Junwei Deng, and Qiaozhu Mei. 2021. Subgroup generalization
    and fairness of graph neural networks. *Advances in Neural Information Processing
    Systems* 34 (2021), 1048–1061.
  id: totrans-1252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2021) Jiaqi Ma, Junwei Deng, 和 Qiaozhu Mei. 2021. 图神经网络的子群泛化与公平性。*神经信息处理系统进展*
    34 (2021), 1048–1061。
- en: Ma et al. (2020b) Ning Ma, Jiajun Bu, Jieyu Yang, Zhen Zhang, Chengwei Yao,
    Zhi Yu, Sheng Zhou, and Xifeng Yan. 2020b. Adaptive-step graph meta-learner for
    few-shot graph classification. In *Proceedings of the 29th ACM International Conference
    on Information & Knowledge Management*. 1055–1064.
  id: totrans-1253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2020b) Ning Ma, Jiajun Bu, Jieyu Yang, Zhen Zhang, Chengwei Yao,
    Zhi Yu, Sheng Zhou, 和 Xifeng Yan. 2020b. 自适应步长图元学习器用于少样本图分类。收录于*第29届ACM国际信息与知识管理会议论文集*。1055–1064。
- en: Ma et al. (2019) Yao Ma, Suhang Wang, Charu C Aggarwal, and Jiliang Tang. 2019.
    Graph convolutional networks with eigenpooling. In *Proceedings of the 25th ACM
    SIGKDD international conference on knowledge discovery & data mining*. 723–731.
  id: totrans-1254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2019) Yao Ma, Suhang Wang, Charu C Aggarwal, 和 Jiliang Tang. 2019.
    带有特征池化的图卷积网络。收录于*第25届ACM SIGKDD国际知识发现与数据挖掘会议论文集*。723–731。
- en: 'Madhawa et al. (2019) Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago,
    and Motoki Abe. 2019. Graphnvp: An invertible flow model for generating molecular
    graphs. *arXiv preprint arXiv:1905.11600* (2019).'
  id: totrans-1255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madhawa et al. (2019) Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago,
    和 Motoki Abe. 2019. Graphnvp：一种用于生成分子图的可逆流模型。*arXiv预印本 arXiv:1905.11600* (2019)。
- en: Madumal et al. (2020) Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank
    Vetere. 2020. Explainable reinforcement learning through a causal lens. In *Proceedings
    of the AAAI conference on artificial intelligence*, Vol. 34. 2493–2500.
  id: totrans-1256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madumal et al. (2020) Prashan Madumal, Tim Miller, Liz Sonenberg, 和 Frank Vetere.
    2020. 通过因果视角进行可解释的强化学习。收录于*AAAI人工智能会议论文集*, Vol. 34. 2493–2500。
- en: Man et al. (2016) Tong Man, Huawei Shen, Shenghua Liu, Xiaolong Jin, and Xueqi
    Cheng. 2016. Predict anchor links across social networks via an embedding approach..
    In *Ijcai*, Vol. 16\. 1823–1829.
  id: totrans-1257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Man et al. (2016) Tong Man, Huawei Shen, Shenghua Liu, Xiaolong Jin, 和 Xueqi
    Cheng. 2016. 通过嵌入方法预测社交网络中的锚链接。收录于*Ijcai*, Vol. 16. 1823–1829。
- en: Manguri et al. (2020) Kamaran H Manguri, Rebaz N Ramadhan, and Pshko R Mohammed
    Amin. 2020. Twitter sentiment analysis on worldwide COVID-19 outbreaks. *Kurdistan
    Journal of Applied Research* (2020), 54–65.
  id: totrans-1258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manguri et al. (2020) Kamaran H Manguri, Rebaz N Ramadhan, 和 Pshko R Mohammed
    Amin. 2020. 对全球COVID-19爆发的Twitter情感分析。*库尔德斯坦应用研究期刊* (2020), 54–65。
- en: Mansimov et al. (2019) Elman Mansimov, Omar Mahmood, Seokho Kang, and Kyunghyun
    Cho. 2019. Molecular geometry prediction using a deep generative graph neural
    network. *Scientific reports* 9, 1 (2019), 1–13.
  id: totrans-1259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mansimov et al. (2019) Elman Mansimov, Omar Mahmood, Seokho Kang, 和 Kyunghyun
    Cho. 2019. 使用深度生成图神经网络进行分子几何预测。*科学报告* 9, 1 (2019), 1–13。
- en: MansourLakouraj et al. (2022) Mohammad MansourLakouraj, Mukesh Gautam, Hanif
    Livani, and Mohammed Benidris. 2022. A multi-rate sampling PMU-based event classification
    in active distribution grids with spectral graph neural network. *Electric Power
    Systems Research* 211 (2022), 108145.
  id: totrans-1260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MansourLakouraj et al. (2022) Mohammad MansourLakouraj, Mukesh Gautam, Hanif
    Livani, 和 Mohammed Benidris. 2022. 基于PMU的多速率采样事件分类在主动配电网中的应用与谱图神经网络。*电力系统研究* 211
    (2022), 108145。
- en: Margaris et al. (2019) Dionisis Margaris, Costas Vassilakis, and Dimitris Spiliotopoulos.
    2019. Handling uncertainty in social media textual information for improving venue
    recommendation formulation quality in social networks. *Social Network Analysis
    and Mining* 9, 1 (2019), 1–19.
  id: totrans-1261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Margaris et al. (2019) Dionisis Margaris, Costas Vassilakis, 和 Dimitris Spiliotopoulos.
    2019. 处理社交媒体文本信息中的不确定性，以提高社交网络中场馆推荐的制定质量。*社交网络分析与挖掘* 9, 1 (2019), 1–19。
- en: Mehrabi et al. (2021) Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina
    Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning.
    *ACM Computing Surveys (CSUR)* 54, 6 (2021), 1–35.
  id: totrans-1262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehrabi et al. (2021) Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina
    Lerman, 和 Aram Galstyan. 2021. 关于机器学习中的偏见和公平性的调查。*ACM Computing Surveys (CSUR)*
    54, 6 (2021), 1–35。
- en: Meng et al. (2021) Xuying Meng, Suhang Wang, Zhimin Liang, Di Yao, Jihua Zhou,
    and Yujun Zhang. 2021. Semi-supervised anomaly detection in dynamic communication
    networks. *Information Sciences* 571 (2021), 527–542.
  id: totrans-1263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等（2021）Xuying Meng、Suhang Wang、Zhimin Liang、Di Yao、Jihua Zhou 和 Yujun Zhang。2021年。在动态通信网络中的半监督异常检测。*信息科学*
    571（2021），527–542。
- en: 'Meng et al. (2011) Xuan-Yu Meng, Hong-Xing Zhang, Mihaly Mezei, and Meng Cui.
    2011. Molecular docking: a powerful approach for structure-based drug discovery.
    *Current computer-aided drug design* 7, 2 (2011), 146–157.'
  id: totrans-1264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等（2011）Xuan-Yu Meng、Hong-Xing Zhang、Mihaly Mezei 和 Meng Cui。2011年。分子对接：一种基于结构的药物发现的强大方法。*当前计算机辅助药物设计*
    7，2（2011），146–157。
- en: Meyers et al. (2021) Joshua Meyers, Benedek Fabian, and Nathan Brown. 2021.
    De novo molecular design and generative models. *Drug Discovery Today* 26, 11
    (2021), 2707–2715.
  id: totrans-1265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meyers 等（2021）Joshua Meyers、Benedek Fabian 和 Nathan Brown。2021年。全新分子设计和生成模型。*药物发现今日*
    26，11（2021），2707–2715。
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013. Distributed representations of words and phrases and their
    compositionality. *Advances in neural information processing systems* 26 (2013).
  id: totrans-1266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等（2013）Tomas Mikolov、Ilya Sutskever、Kai Chen、Greg S Corrado 和 Jeff Dean。2013年。词汇和短语的分布式表示及其组合性。*神经信息处理系统进展*
    26（2013）。
- en: 'Mohamed et al. (2020) Abduallah Mohamed, Kun Qian, Mohamed Elhoseiny, and Christian
    Claudel. 2020. Social-stgcnn: A social spatio-temporal graph convolutional neural
    network for human trajectory prediction. In *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*. 14424–14432.'
  id: totrans-1267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohamed 等（2020）Abduallah Mohamed、Kun Qian、Mohamed Elhoseiny 和 Christian Claudel。2020年。Social-stgcnn：一种用于人类轨迹预测的社交时空图卷积神经网络。载于
    *IEEE/CVF计算机视觉与模式识别会议论文集*。14424–14432。
- en: 'Morris et al. (2019) Christopher Morris, Martin Ritzert, Matthias Fey, William L
    Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. Weisfeiler
    and leman go neural: Higher-order graph neural networks. In *Proceedings of the
    AAAI conference on artificial intelligence*, Vol. 33. 4602–4609.'
  id: totrans-1268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Morris 等（2019）Christopher Morris、Martin Ritzert、Matthias Fey、William L Hamilton、Jan
    Eric Lenssen、Gaurav Rattan 和 Martin Grohe。2019年。Weisfeiler 和 Leman 变得神经化：高阶图神经网络。载于
    *AAAI人工智能会议论文集*，第33卷。4602–4609。
- en: Moscato and Sperlì (2021) Vincenzo Moscato and Giancarlo Sperlì. 2021. A survey
    about community detection over On-line Social and Heterogeneous Information Networks.
    *Knowledge-Based Systems* 224 (2021), 107112.
  id: totrans-1269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moscato 和 Sperlì（2021）Vincenzo Moscato 和 Giancarlo Sperlì。2021年。关于在线社交和异构信息网络中的社区检测的调查。*知识基础系统*
    224（2021），107112。
- en: Müller (2007) Meinard Müller. 2007. Dynamic time warping. *Information retrieval
    for music and motion* (2007), 69–84.
  id: totrans-1270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Müller（2007）Meinard Müller。2007年。动态时间规整。*音乐和运动的信息检索*（2007），69–84。
- en: 'Nesterov et al. (2020) Vitali Nesterov, Mario Wieser, and Volker Roth. 2020.
    3DMolNet: a generative network for molecular structures. *arXiv preprint arXiv:2010.06477*
    (2020).'
  id: totrans-1271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nesterov 等（2020）Vitali Nesterov、Mario Wieser 和 Volker Roth。2020年。3DMolNet：一种用于分子结构的生成网络。*arXiv预印本
    arXiv:2010.06477*（2020）。
- en: Ou et al. (2016) Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu.
    2016. Asymmetric transitivity preserving graph embedding. In *Proceedings of the
    22nd ACM SIGKDD international conference on Knowledge discovery and data mining*.
    1105–1114.
  id: totrans-1272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ou 等（2016）Mingdong Ou、Peng Cui、Jian Pei、Ziwei Zhang 和 Wenwu Zhu。2016年。非对称传递保留图嵌入。载于
    *第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集*。1105–1114。
- en: Panagopoulos et al. (2020) George Panagopoulos, Fragkiskos Malliaros, and Michalis
    Vazirgiannis. 2020. Multi-task learning for influence estimation and maximization.
    *IEEE Transactions on Knowledge and Data Engineering* (2020).
  id: totrans-1273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panagopoulos 等（2020）George Panagopoulos、Fragkiskos Malliaros 和 Michalis Vazirgiannis。2020年。用于影响估计和最大化的多任务学习。*IEEE知识与数据工程学报*（2020）。
- en: 'Park et al. (2020) Cheonbok Park, Chunggi Lee, Hyojin Bahng, Yunwon Tae, Seungmin
    Jin, Kihwan Kim, Sungahn Ko, and Jaegul Choo. 2020. ST-GRAT: A novel spatio-temporal
    graph attention networks for accurately forecasting dynamically changing road
    speed. In *Proceedings of the 29th ACM international conference on information
    & knowledge management*. 1215–1224.'
  id: totrans-1274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等（2020）Cheonbok Park、Chunggi Lee、Hyojin Bahng、Yunwon Tae、Seungmin Jin、Kihwan
    Kim、Sungahn Ko 和 Jaegul Choo。2020年。ST-GRAT：一种新颖的时空图注意力网络，用于准确预测动态变化的道路速度。载于 *第29届ACM国际信息与知识管理会议论文集*。1215–1224。
- en: Park et al. (2021) Hyeonjin Park, Seunghun Lee, Dasol Hwang, Jisu Jeong, Kyung-Min
    Kim, Jung-Woo Ha, and Hyunwoo J Kim. 2021. Learning Augmentation for GNNs With
    Consistency Regularization. *IEEE Access* 9 (2021), 127961–127972.
  id: totrans-1275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2021) Hyeonjin Park, Seunghun Lee, Dasol Hwang, Jisu Jeong, Kyung-Min
    Kim, Jung-Woo Ha, 和 Hyunwoo J Kim. 2021. 带有一致性正则化的GNN学习增强。 *IEEE Access* 9 (2021),
    127961–127972。
- en: Paulos et al. (2004) Eric Paulos, Ken Anderson, and Anthony Townsend. 2004.
    UbiComp in the urban frontier. (2004).
  id: totrans-1276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paulos et al. (2004) Eric Paulos, Ken Anderson, 和 Anthony Townsend. 2004. 城市前沿的普适计算。
    (2004)。
- en: 'Paulos and Goodman (2004) Eric Paulos and Elizabeth Goodman. 2004. The familiar
    stranger: anxiety, comfort, and play in public places. In *Proceedings of the
    SIGCHI conference on Human factors in computing systems*. 223–230.'
  id: totrans-1277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paulos and Goodman (2004) Eric Paulos 和 Elizabeth Goodman. 2004. 熟悉的陌生人：公共场所的焦虑、舒适和游戏。在
    *SIGCHI人机计算系统会议论文集*。223–230。
- en: Peng et al. (2023) Xingang Peng, Jiaqi Guan, Jian Peng, and Jianzhu Ma. 2023.
    Pocket-specific 3D Molecule Generation by Fragment-based Autoregressive Diffusion
    Models. [https://openreview.net/forum?id=HGsoe1wmRW5](https://openreview.net/forum?id=HGsoe1wmRW5)
  id: totrans-1278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2023) Xingang Peng, Jiaqi Guan, Jian Peng, 和 Jianzhu Ma. 2023.
    基于片段的自回归扩散模型进行口袋特异性3D分子生成。 [https://openreview.net/forum?id=HGsoe1wmRW5](https://openreview.net/forum?id=HGsoe1wmRW5)
- en: 'Peng et al. (2022) Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng,
    and Jianzhu Ma. 2022. Pocket2Mol: Efficient Molecular Sampling Based on 3D Protein
    Pockets. *arXiv preprint arXiv:2205.07249* (2022).'
  id: totrans-1279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng et al. (2022) Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng,
    和 Jianzhu Ma. 2022. Pocket2Mol: 基于3D蛋白质口袋的高效分子采样。*arXiv预印本 arXiv:2205.07249* (2022)。'
- en: 'Perozzi et al. (2014) Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014.
    Deepwalk: Online learning of social representations. In *Proceedings of the 20th
    ACM SIGKDD international conference on Knowledge discovery and data mining*. 701–710.'
  id: totrans-1280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perozzi et al. (2014) Bryan Perozzi, Rami Al-Rfou, 和 Steven Skiena. 2014. Deepwalk：社交表征的在线学习。在
    *第20届ACM SIGKDD国际知识发现与数据挖掘会议论文集*。701–710。
- en: 'Pilco and Rivera (2019) Darwin Saire Pilco and Adín Ramírez Rivera. 2019. Graph
    learning network: A structure learning algorithm. *arXiv preprint arXiv:1905.12665*
    (2019).'
  id: totrans-1281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pilco and Rivera (2019) Darwin Saire Pilco 和 Adín Ramírez Rivera. 2019. 图学习网络：一种结构学习算法。
    *arXiv预印本 arXiv:1905.12665* (2019)。
- en: Pimentel et al. (2014) Marco AF Pimentel, David A Clifton, Lei Clifton, and
    Lionel Tarassenko. 2014. A review of novelty detection. *Signal processing* 99
    (2014), 215–249.
  id: totrans-1282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pimentel et al. (2014) Marco AF Pimentel, David A Clifton, Lei Clifton, 和 Lionel
    Tarassenko. 2014. 新颖性检测综述。 *信号处理* 99 (2014), 215–249。
- en: 'Pinheiro et al. (2022) Gabriel A Pinheiro, Juarez LF Da Silva, and Marcos G
    Quiles. 2022. SMICLR: Contrastive Learning on Multiple Molecular Representations
    for Semisupervised and Unsupervised Representation Learning. *Journal of Chemical
    Information and Modeling* 62, 17 (2022), 3948–3960.'
  id: totrans-1283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinheiro et al. (2022) Gabriel A Pinheiro, Juarez LF Da Silva, 和 Marcos G Quiles.
    2022. SMICLR：用于半监督和无监督表示学习的多分子表示对比学习。 *化学信息与建模杂志* 62, 17 (2022), 3948–3960。
- en: 'Preuer et al. (2019) Kristina Preuer, Günter Klambauer, Friedrich Rippmann,
    Sepp Hochreiter, and Thomas Unterthiner. 2019. Interpretable deep learning in
    drug discovery. In *Explainable AI: Interpreting, Explaining and Visualizing Deep
    Learning*. Springer, 331–345.'
  id: totrans-1284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Preuer et al. (2019) Kristina Preuer, Günter Klambauer, Friedrich Rippmann,
    Sepp Hochreiter, 和 Thomas Unterthiner. 2019. 药物发现中的可解释深度学习。在 *可解释的AI：解释、阐述和可视化深度学习*。Springer,
    331–345。
- en: Qiao et al. (2019) Ziyue Qiao, Yi Du, Yanjie Fu, Pengfei Wang, and Yuanchun
    Zhou. 2019. Unsupervised author disambiguation using heterogeneous graph convolutional
    network embedding. In *2019 IEEE international conference on big data (Big Data)*.
    IEEE, 910–919.
  id: totrans-1285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao et al. (2019) Ziyue Qiao, Yi Du, Yanjie Fu, Pengfei Wang, 和 Yuanchun Zhou.
    2019. 使用异构图卷积网络嵌入的无监督作者消歧。在 *2019 IEEE国际大数据会议 (Big Data)*。IEEE, 910–919。
- en: 'Qiao et al. (2022) Ziyue Qiao, Yanjie Fu, Pengyang Wang, Meng Xiao, Zhiyuan
    Ning, Yi Du, and Yuanchun Zhou. 2022. RPT: Toward Transferable Model on Heterogeneous
    Researcher Data via Pre-Training. *IEEE Transactions on Big Data* (2022).'
  id: totrans-1286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao et al. (2022) Ziyue Qiao, Yanjie Fu, Pengyang Wang, Meng Xiao, Zhiyuan
    Ning, Yi Du, 和 Yuanchun Zhou. 2022. RPT：通过预训练实现异质研究者数据上的可迁移模型。 *IEEE大数据汇刊* (2022)。
- en: Qiao et al. (2020a) Ziyue Qiao, Pengyang Wang, Yanjie Fu, Yi Du, Pengfei Wang,
    and Yuanchun Zhou. 2020a. Tree structure-aware graph representation learning via
    integrated hierarchical aggregation and relational metric learning. In *2020 IEEE
    International Conference on Data Mining (ICDM)*. IEEE, 432–441.
  id: totrans-1287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao et al. (2020a) Ziyue Qiao, Pengyang Wang, Yanjie Fu, Yi Du, Pengfei Wang,
    和 Yuanchun Zhou. 2020a. 通过集成层次聚合和关系度量学习的树结构感知图表示学习。在 *2020 IEEE国际数据挖掘会议 (ICDM)*。IEEE,
    432–441。
- en: 'Qiao et al. (2020b) Zhuoran Qiao, Matthew Welborn, Animashree Anandkumar, Frederick R
    Manby, and Thomas F Miller III. 2020b. OrbNet: Deep learning for quantum chemistry
    using symmetry-adapted atomic-orbital features. *The Journal of chemical physics*
    153, 12 (2020), 124111.'
  id: totrans-1288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao 等 (2020b) Zhuoran Qiao, Matthew Welborn, Animashree Anandkumar, Frederick
    R Manby 和 Thomas F Miller III. 2020b. OrbNet：使用对称适应原子轨道特征的量子化学深度学习。*化学物理学报* 153，12
    (2020)，124111。
- en: 'Qin et al. (2022) Yifang Qin, Yifan Wang, Fang Sun, Wei Ju, Xuyang Hou, Zhe
    Wang, Jia Cheng, Jun Lei, and Ming Zhang. 2022. DisenPOI: Disentangling Sequential
    and Geographical Influence for Point-of-Interest Recommendation. *arXiv preprint
    arXiv:2210.16591* (2022).'
  id: totrans-1289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等 (2022) Yifang Qin, Yifan Wang, Fang Sun, Wei Ju, Xuyang Hou, Zhe Wang,
    Jia Cheng, Jun Lei 和 Ming Zhang. 2022. DisenPOI：解开点兴趣推荐中的顺序和地理影响。*arXiv预印本 arXiv:2210.16591*
    (2022)。
- en: 'Qiu et al. (2020a) Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia
    Yang, Ming Ding, Kuansan Wang, and Jie Tang. 2020a. Gcc: Graph contrastive coding
    for graph neural network pre-training. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 1150–1160.'
  id: totrans-1290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等 (2020a) Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang,
    Ming Ding, Kuansan Wang 和 Jie Tang. 2020a. Gcc：用于图神经网络预训练的图对比编码。在 *第26届ACM SIGKDD国际知识发现与数据挖掘会议论文集*。1150–1160。
- en: Qiu et al. (2019) Ruihong Qiu, Jingjing Li, Zi Huang, and Hongzhi Yin. 2019.
    Rethinking the item order in session-based recommendation with graph neural networks.
    In *Proceedings of the 28th ACM international conference on information and knowledge
    management*. 579–588.
  id: totrans-1291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等 (2019) Ruihong Qiu, Jingjing Li, Zi Huang 和 Hongzhi Yin. 2019. 使用图神经网络重新思考会话推荐中的项目顺序。在
    *第28届ACM国际信息与知识管理会议论文集*。579–588。
- en: 'Qiu et al. (2020b) Ruihong Qiu, Hongzhi Yin, Zi Huang, and Tong Chen. 2020b.
    Gag: Global attributed graph neural network for streaming session-based recommendation.
    In *Proceedings of the 43rd International ACM SIGIR Conference on Research and
    Development in Information Retrieval*. 669–678.'
  id: totrans-1292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等 (2020b) Ruihong Qiu, Hongzhi Yin, Zi Huang 和 Tong Chen. 2020b. Gag：用于流式会话推荐的全球属性图神经网络。在
    *第43届国际ACM SIGIR信息检索研究与开发会议论文集*。669–678。
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. 2021. Learning transferable visual models from natural language
    supervision. In *International Conference on Machine Learning*. PMLR, 8748–8763.
  id: totrans-1293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark 等. 2021. 从自然语言监督中学习可转移的视觉模型。在 *国际机器学习大会*。PMLR，8748–8763。
- en: Ragoza et al. (2022) Matthew Ragoza, Tomohide Masuda, and David Ryan Koes. 2022.
    Generating 3D molecules conditional on receptor binding sites with deep generative
    models. *Chemical science* 13, 9 (2022), 2701–2713.
  id: totrans-1294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ragoza 等 (2022) Matthew Ragoza, Tomohide Masuda 和 David Ryan Koes. 2022. 使用深度生成模型生成以受体结合位点为条件的3D分子。*化学科学*
    13，9 (2022)，2701–2713。
- en: 'Ran and Boyce (2012) Bin Ran and David Boyce. 2012. *Modeling dynamic transportation
    networks: an intelligent transportation system oriented approach*. Springer Science
    & Business Media.'
  id: totrans-1295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ran 和 Boyce (2012) Bin Ran 和 David Boyce. 2012. *动态交通网络建模：一种面向智能交通系统的方法*。Springer
    Science & Business Media。
- en: 'Ranjan et al. (2020) Ekagra Ranjan, Soumya Sanyal, and Partha Talukdar. 2020.
    Asap: Adaptive structure aware pooling for learning hierarchical graph representations.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 34\.
    5470–5477.'
  id: totrans-1296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ranjan 等 (2020) Ekagra Ranjan, Soumya Sanyal 和 Partha Talukdar. 2020. Asap：用于学习层次图表示的自适应结构感知池化。在
    *AAAI人工智能会议论文集*，第34卷。5470–5477。
- en: 'Rendle et al. (2012) Steffen Rendle, Christoph Freudenthaler, Zeno Gantner,
    and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit
    feedback. *arXiv preprint arXiv:1205.2618* (2012).'
  id: totrans-1297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rendle 等 (2012) Steffen Rendle, Christoph Freudenthaler, Zeno Gantner 和 Lars
    Schmidt-Thieme. 2012. BPR：来自隐式反馈的贝叶斯个性化排序。*arXiv预印本 arXiv:1205.2618* (2012)。
- en: Rezende and Mohamed (2015) Danilo Rezende and Shakir Mohamed. 2015. Variational
    inference with normalizing flows. In *International conference on machine learning*.
    PMLR, 1530–1538.
  id: totrans-1298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezende 和 Mohamed (2015) Danilo Rezende 和 Shakir Mohamed. 2015. 使用归一化流的变分推断。在
    *国际机器学习大会*。PMLR，1530–1538。
- en: Richardson and Domingos (2002) Matthew Richardson and Pedro Domingos. 2002.
    Mining knowledge-sharing sites for viral marketing. In *Proceedings of the eighth
    ACM SIGKDD international conference on Knowledge discovery and data mining*. 61–70.
  id: totrans-1299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richardson和Domingos（2002）Matthew Richardson 和 Pedro Domingos。2002年。挖掘知识共享网站进行病毒营销。在*第八届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*。61–70。
- en: Richens et al. (2020) Jonathan G Richens, Ciarán M Lee, and Saurabh Johri. 2020.
    Improving the accuracy of medical diagnosis with causal machine learning. *Nature
    communications* 11, 1 (2020), 1–9.
  id: totrans-1300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richens等（2020）Jonathan G Richens, Ciarán M Lee, 和 Saurabh Johri。2020年。通过因果机器学习提高医疗诊断的准确性。*自然通讯*
    11, 1（2020），1–9。
- en: Roney et al. (2021) James P Roney, Paul Maragakis, Peter Skopp, and David E
    Shaw. 2021. Generating Realistic 3D Molecules with an Equivariant Conditional
    Likelihood Model. (2021).
  id: totrans-1301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roney等（2021）James P Roney, Paul Maragakis, Peter Skopp, 和 David E Shaw。2021年。使用等变条件似然模型生成现实的3D分子。（2021）。
- en: Rosa et al. (2018) Renata Lopes Rosa, Gisele Maria Schwartz, Wilson Vicente
    Ruggiero, and Demóstenes Zegarra Rodríguez. 2018. A knowledge-based recommendation
    system that includes sentiment analysis and deep learning. *IEEE Transactions
    on Industrial Informatics* 15, 4 (2018), 2124–2135.
  id: totrans-1302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosa等（2018）Renata Lopes Rosa, Gisele Maria Schwartz, Wilson Vicente Ruggiero,
    和 Demóstenes Zegarra Rodríguez。2018年。一个包含情感分析和深度学习的基于知识的推荐系统。*IEEE工业信息学事务* 15,
    4（2018），2124–2135。
- en: Ruddigkeit et al. (2012) Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and
    Jean-Louis Reymond. 2012. Enumeration of 166 billion organic small molecules in
    the chemical universe database GDB-17. *Journal of chemical information and modeling*
    52, 11 (2012), 2864–2875.
  id: totrans-1303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruddigkeit等（2012）Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, 和 Jean-Louis
    Reymond。2012年。化学宇宙数据库GDB-17中1660亿种有机小分子的枚举。*化学信息与建模杂志* 52, 11（2012），2864–2875。
- en: Runwal et al. (2022) Bharat Runwal, Sandeep Kumar, et al. 2022. Robustifying
    GNN Via Weighted Laplacian. In *2022 IEEE International Conference on Signal Processing
    and Communications (SPCOM)*. IEEE, 1–5.
  id: totrans-1304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Runwal等（2022）Bharat Runwal, Sandeep Kumar等。2022年。通过加权拉普拉斯增强GNN的鲁棒性。在*2022 IEEE国际信号处理与通信会议（SPCOM）*。IEEE，1–5。
- en: Ryu et al. (2018) Seongok Ryu, Jaechang Lim, Seung Hwan Hong, and Woo Youn Kim.
    2018. Deeply learning molecular structure-property relationships using attention-and
    gate-augmented graph convolutional network. *arXiv preprint arXiv:1805.10988*
    (2018).
  id: totrans-1305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ryu等（2018）Seongok Ryu, Jaechang Lim, Seung Hwan Hong, 和 Woo Youn Kim。2018年。通过注意力和门控增强的图卷积网络深入学习分子结构-属性关系。*arXiv预印本
    arXiv:1805.10988*（2018）。
- en: Sabour et al. (2017) Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017.
    Dynamic routing between capsules. *Advances in neural information processing systems*
    30 (2017).
  id: totrans-1306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabour等（2017）Sara Sabour, Nicholas Frosst, 和 Geoffrey E Hinton。2017年。胶囊之间的动态路由。*神经信息处理系统进展*
    30（2017）。
- en: Sandryhaila and Moura (2013) Aliaksei Sandryhaila and José MF Moura. 2013. Discrete
    signal processing on graphs. *IEEE transactions on signal processing* 61, 7 (2013),
    1644–1656.
  id: totrans-1307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sandryhaila和Moura（2013）Aliaksei Sandryhaila 和 José MF Moura。2013年。图上的离散信号处理。*IEEE信号处理事务*
    61, 7（2013），1644–1656。
- en: Satorras et al. (2021) Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling.
    2021. E(n) Equivariant Graph Neural Networks. *arXiv preprint arXiv:2102.09844*
    (2021). arXiv:2102.09844 [cs.LG]
  id: totrans-1308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Satorras等（2021）Victor Garcia Satorras, Emiel Hoogeboom, 和 Max Welling。2021年。E(n)等变图神经网络。*arXiv预印本
    arXiv:2102.09844*（2021）。arXiv:2102.09844 [cs.LG]
- en: Saxena et al. (2020) Chandni Saxena, Tianyu Liu, and Irwin King. 2020. A Survey
    of Graph Curvature and Embedding in Non-Euclidean Spaces. In *International Conference
    on Neural Information Processing*. Springer, 127–139.
  id: totrans-1309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saxena等（2020）Chandni Saxena, Tianyu Liu, 和 Irwin King。2020年。图曲率和非欧几里得空间中的嵌入调查。在*国际神经信息处理会议*。Springer，127–139。
- en: Scarselli et al. (2008) Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
    Hagenbuchner, and Gabriele Monfardini. 2008. The graph neural network model. *IEEE
    transactions on neural networks* 20, 1 (2008), 61–80.
  id: totrans-1310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scarselli等（2008）Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner,
    和 Gabriele Monfardini。2008年。图神经网络模型。*IEEE神经网络事务* 20, 1（2008），61–80。
- en: Schlichtkrull et al. (2018) Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,
    Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data
    with graph convolutional networks. In *European semantic web conference*. Springer,
    593–607.
  id: totrans-1311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schlichtkrull等（2018）Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne
    Van Den Berg, Ivan Titov, 和 Max Welling。2018年。使用图卷积网络建模关系数据。在*欧洲语义网会议*。Springer，593–607。
- en: Schneider et al. (2020) Petra Schneider, W Patrick Walters, Alleyn T Plowright,
    Norman Sieroka, Jennifer Listgarten, Robert A Goodnow, Jasmin Fisher, Johanna M
    Jansen, José S Duca, Thomas S Rush, et al. 2020. Rethinking drug design in the
    artificial intelligence era. *Nature Reviews Drug Discovery* 19, 5 (2020), 353–364.
  id: totrans-1312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schneider 等人（2020）Petra Schneider, W Patrick Walters, Alleyn T Plowright, Norman
    Sieroka, Jennifer Listgarten, Robert A Goodnow, Jasmin Fisher, Johanna M Jansen,
    José S Duca, Thomas S Rush 等。2020。《在人工智能时代重新思考药物设计》。*自然药物发现评论* 19, 5 (2020), 353–364。
- en: Schneuing et al. (2022) Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb,
    Ilia Igashov, Weitao Du, Tom Blundell, Pietro Lió, Carla Gomes, Max Welling, et al.
    2022. Structure-based Drug Design with Equivariant Diffusion Models. *arXiv preprint
    arXiv:2210.13695* (2022).
  id: totrans-1313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schneuing 等人（2022）Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia
    Igashov, Weitao Du, Tom Blundell, Pietro Lió, Carla Gomes, Max Welling 等。2022。《基于结构的药物设计与等变扩散模型》。*arXiv
    预印本 arXiv:2210.13695* (2022)。
- en: Schütt et al. (2021) Kristof Schütt, Oliver Unke, and Michael Gastegger. 2021.
    Equivariant message passing for the prediction of tensorial properties and molecular
    spectra. In *ICML*. [https://proceedings.mlr.press/v139/schutt21a.html](https://proceedings.mlr.press/v139/schutt21a.html)
  id: totrans-1314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schütt 等人（2021）Kristof Schütt, Oliver Unke 和 Michael Gastegger。2021。《用于预测张量属性和分子光谱的等变信息传递》。在
    *ICML* 中。[https://proceedings.mlr.press/v139/schutt21a.html](https://proceedings.mlr.press/v139/schutt21a.html)
- en: Schütt et al. (2018) Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre
    Tkatchenko, and K-R Müller. 2018. Schnet–a deep learning architecture for molecules
    and materials. *The Journal of Chemical Physics* 148, 24 (2018), 241722.
  id: totrans-1315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schütt 等人（2018）Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre
    Tkatchenko 和 K-R Müller。2018。《Schnet–用于分子和材料的深度学习架构》。*化学物理学杂志* 148, 24 (2018),
    241722。
- en: Sen et al. (2008) Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor,
    Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classification in network
    data. *AI magazine* 29, 3 (2008), 93–93.
  id: totrans-1316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sen 等人（2008）Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian
    Galligher 和 Tina Eliassi-Rad。2008。《网络数据中的集体分类》。*AI 杂志* 29, 3 (2008), 93–93。
- en: Shawe-Taylor et al. (2004) John Shawe-Taylor, Nello Cristianini, et al. 2004.
    *Kernel methods for pattern analysis*. Cambridge university press.
  id: totrans-1317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shawe-Taylor 等人（2004）John Shawe-Taylor, Nello Cristianini 等。2004。*模式分析的核方法*。剑桥大学出版社。
- en: Shervashidze et al. (2011a) Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen,
    Kurt Mehlhorn, and Karsten M Borgwardt. 2011a. Weisfeiler-lehman graph kernels.
    *Journal of Machine Learning Research* 12, 9 (2011), 2539–2561.
  id: totrans-1318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shervashidze 等人（2011a）Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen,
    Kurt Mehlhorn 和 Karsten M Borgwardt。2011a。《Weisfeiler-Lehman 图核》。*机器学习研究期刊* 12,
    9 (2011), 2539–2561。
- en: Shervashidze et al. (2011b) Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen,
    Kurt Mehlhorn, and Karsten M Borgwardt. 2011b. Weisfeiler-lehman graph kernels.
    *Journal of Machine Learning Research* 12, 9 (2011).
  id: totrans-1319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shervashidze 等人（2011b）Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen,
    Kurt Mehlhorn 和 Karsten M Borgwardt。2011b。《Weisfeiler-Lehman 图核》。*机器学习研究期刊* 12,
    9 (2011)。
- en: Shervashidze et al. (2009) Nino Shervashidze, SVN Vishwanathan, Tobias Petri,
    Kurt Mehlhorn, and Karsten Borgwardt. 2009. Efficient graphlet kernels for large
    graph comparison. In *Proceedings of International Conference on Artificial Intelligence
    and Statistics*. 488–495.
  id: totrans-1320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shervashidze 等人（2009）Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt
    Mehlhorn 和 Karsten Borgwardt。2009。《用于大规模图比较的高效图小部件核》。在 *国际人工智能与统计会议论文集* 中。488–495。
- en: Shi et al. (2016) Chuan Shi, Yitong Li, Jiawei Zhang, Yizhou Sun, and S Yu Philip.
    2016. A survey of heterogeneous information network analysis. *IEEE Transactions
    on Knowledge and Data Engineering* 29, 1 (2016), 17–37.
  id: totrans-1321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2016）Chuan Shi, Yitong Li, Jiawei Zhang, Yizhou Sun 和 S Yu Philip。2016。《异构信息网络分析综述》。*IEEE
    知识与数据工程期刊* 29, 1 (2016), 17–37。
- en: Shi et al. (2021) Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. 2021. Learning
    gradient fields for molecular conformation generation. *Proceedings of the 38th
    International Conference on Machine Learning, ICML* 139 (2021), 9558–9568.
  id: totrans-1322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2021）Chence Shi, Shitong Luo, Minkai Xu 和 Jian Tang。2021。《用于分子构象生成的梯度场学习》。*第38届国际机器学习会议论文集，ICML*
    139 (2021), 9558–9568。
- en: 'Shi et al. (2020) Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming
    Zhang, and Jian Tang. 2020. Graphaf: a flow-based autoregressive model for molecular
    graph generation. *arXiv preprint arXiv:2001.09382* (2020).'
  id: totrans-1323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2020）Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang 和
    Jian Tang。2020。《Graphaf：一种基于流的自回归模型用于分子图生成》。*arXiv 预印本 arXiv:2001.09382* (2020)。
- en: 'Shuman et al. (2013a) David I Shuman, Sunil K Narang, Pascal Frossard, Antonio
    Ortega, and Pierre Vandergheynst. 2013a. The emerging field of signal processing
    on graphs: Extending high-dimensional data analysis to networks and other irregular
    domains. *IEEE signal processing magazine* 30, 3 (2013), 83–98.'
  id: totrans-1324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shuman 等 (2013a) David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega
    和 Pierre Vandergheynst. 2013a. 图上的信号处理新兴领域：将高维数据分析扩展到网络和其他不规则领域。*IEEE 信号处理杂志*
    30, 3 (2013), 83–98.
- en: 'Shuman et al. (2013b) David I Shuman, Sunil K Narang, Pascal Frossard, Antonio
    Ortega, and Pierre Vandergheynst. 2013b. The emerging field of signal processing
    on graphs: Extending high-dimensional data analysis to networks and other irregular
    domains. *IEEE signal processing magazine* 30, 3 (2013), 83–98.'
  id: totrans-1325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shuman 等 (2013b) David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega
    和 Pierre Vandergheynst. 2013b. 图上的信号处理新兴领域：将高维数据分析扩展到网络和其他不规则领域。*IEEE 信号处理杂志*
    30, 3 (2013), 83–98.
- en: Si et al. (2019) Yali Si, Fuzhi Zhang, and Wenyuan Liu. 2019. An adaptive point-of-interest
    recommendation method for location-based social networks based on user activity
    and spatial features. *Knowledge-Based Systems* 163 (2019), 267–282.
  id: totrans-1326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Si 等 (2019) Yali Si, Fuzhi Zhang 和 Wenyuan Liu. 2019. 一种基于用户活动和空间特征的自适应兴趣点推荐方法，用于基于位置的社交网络。*知识基础系统*
    163 (2019), 267–282.
- en: 'Silva et al. (2019) Thiago H Silva, Aline Carneiro Viana, Fabrício Benevenuto,
    Leandro Villas, Juliana Salles, Antonio Loureiro, and Daniele Quercia. 2019. Urban
    computing leveraging location-based social network data: a survey. *ACM Computing
    Surveys (CSUR)* 52, 1 (2019), 1–39.'
  id: totrans-1327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silva 等 (2019) Thiago H Silva, Aline Carneiro Viana, Fabrício Benevenuto, Leandro
    Villas, Juliana Salles, Antonio Loureiro 和 Daniele Quercia. 2019. 利用基于位置的社交网络数据进行城市计算：综述。*ACM
    计算调查 (CSUR)* 52, 1 (2019), 1–39.
- en: Simonovsky and Komodakis (2017) Martin Simonovsky and Nikos Komodakis. 2017.
    Dynamic edge-conditioned filters in convolutional neural networks on graphs. In
    *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    3693–3702.
  id: totrans-1328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonovsky 和 Komodakis (2017) Martin Simonovsky 和 Nikos Komodakis. 2017. 图上的卷积神经网络中的动态边缘条件滤波器。在
    *IEEE 计算机视觉与模式识别会议论文集* 中，3693–3702.
- en: 'Simonovsky and Komodakis (2018) Martin Simonovsky and Nikos Komodakis. 2018.
    Graphvae: Towards generation of small graphs using variational autoencoders. In
    *International conference on artificial neural networks*. Springer, 412–422.'
  id: totrans-1329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonovsky 和 Komodakis (2018) Martin Simonovsky 和 Nikos Komodakis. 2018. Graphvae：使用变分自编码器生成小图的研究。在
    *国际人工神经网络会议* 中。Springer, 412–422.
- en: 'Sofuoglu and Aviyente (2022) Seyyid Emre Sofuoglu and Selin Aviyente. 2022.
    Gloss: Tensor-based anomaly detection in spatiotemporal urban traffic data. *Signal
    Processing* 192 (2022), 108370.'
  id: totrans-1330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sofuoglu 和 Aviyente (2022) Seyyid Emre Sofuoglu 和 Selin Aviyente. 2022. Gloss:
    基于张量的时空城市交通数据异常检测。*信号处理* 192 (2022), 108370.'
- en: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *International Conference on Machine Learning*. PMLR, 2256–2265.
  id: totrans-1331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohl-Dickstein 等 (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan
    和 Surya Ganguli. 2015. 使用非平衡热力学的深度无监督学习。在 *国际机器学习会议* 中。PMLR, 2256–2265.
- en: 'Song et al. (2020a) Chao Song, Youfang Lin, Shengnan Guo, and Huaiyu Wan. 2020a.
    Spatial-temporal synchronous graph convolutional networks: A new framework for
    spatial-temporal network data forecasting. In *Proceedings of the AAAI conference
    on artificial intelligence*, Vol. 34\. 914–921.'
  id: totrans-1332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等 (2020a) Chao Song, Youfang Lin, Shengnan Guo 和 Huaiyu Wan. 2020a. 时空同步图卷积网络：用于时空网络数据预测的新框架。在
    *AAAI 人工智能会议论文集* 中，第34卷，914–921.
- en: 'Song et al. (2020b) Qingyu Song, RuiBo Ming, Jianming Hu, Haoyi Niu, and Mingyang
    Gao. 2020b. Graph attention convolutional network: Spatiotemporal modeling for
    urban traffic prediction. In *2020 IEEE 23rd International Conference on Intelligent
    Transportation Systems (ITSC)*. IEEE, 1–6.'
  id: totrans-1333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等 (2020b) Qingyu Song, RuiBo Ming, Jianming Hu, Haoyi Niu 和 Mingyang Gao.
    2020b. 图注意力卷积网络：城市交通预测的时空建模。在 *2020 IEEE 第23届智能交通系统国际会议 (ITSC)* 中。IEEE, 1–6.
- en: Song and Ermon (2019) Yang Song and Stefano Ermon. 2019. Generative modeling
    by estimating gradients of the data distribution. *Advances in Neural Information
    Processing Systems* 32 (2019).
  id: totrans-1334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 和 Ermon (2019) Yang Song 和 Stefano Ermon. 2019. 通过估计数据分布的梯度进行生成建模。*神经信息处理系统进展*
    32 (2019).
- en: Spackman et al. (2016) Peter R. Spackman, Dylan Jayatilaka, and Amir Karton.
    2016. Basis set convergence of CCSD(T) equilibrium geometries using a large and
    diverse set of molecular structures. *The Journal of Chemical Physics* 145, 10
    (2016), 104101. [https://doi.org/10.1063/1.4962168](https://doi.org/10.1063/1.4962168)
  id: totrans-1335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spackman 等人 (2016) Peter R. Spackman、Dylan Jayatilaka 和 Amir Karton。2016。使用大规模和多样的分子结构的
    CCSD(T) 平衡几何基础集收敛性。*化学物理学报* 145, 10 (2016), 104101。 [https://doi.org/10.1063/1.4962168](https://doi.org/10.1063/1.4962168)
- en: 'Stärk et al. (2022) Hannes Stärk, Octavian Ganea, Lagnajit Pattanaik, Regina
    Barzilay, and Tommi Jaakkola. 2022. Equibind: Geometric deep learning for drug
    binding structure prediction. In *International Conference on Machine Learning*.
    PMLR, 20503–20521.'
  id: totrans-1336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stärk 等人 (2022) Hannes Stärk、Octavian Ganea、Lagnajit Pattanaik、Regina Barzilay
    和 Tommi Jaakkola。2022。Equibind：药物结合结构预测的几何深度学习。发表于 *国际机器学习会议*。PMLR，20503–20521。
- en: Su et al. (2022) Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi
    Rao, Hao Sun, Zhiwu Lu, and Ji-Rong Wen. 2022. A molecular multimodal foundation
    model associating molecule graphs with natural language. *arXiv preprint arXiv:2209.05481*
    (2022).
  id: totrans-1337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等人 (2022) Bing Su、Dazhao Du、Zhao Yang、Yujie Zhou、Jiangmeng Li、Anyi Rao、Hao
    Sun、Zhiwu Lu 和 Ji-Rong Wen。2022。一个将分子图与自然语言关联的分子多模态基础模型。*arXiv 预印本 arXiv:2209.05481*
    (2022)。
- en: Sugiyama and Kan (2010) Kazunari Sugiyama and Min-Yen Kan. 2010. Scholarly paper
    recommendation via user’s recent research interests. In *Proceedings of the 10th
    annual joint conference on Digital libraries*. 29–38.
  id: totrans-1338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sugiyama 和 Kan (2010) Kazunari Sugiyama 和 Min-Yen Kan。2010。基于用户近期研究兴趣的学术论文推荐。发表于
    *第 10 届数字图书馆年会联合会议论文集*，29–38。
- en: 'Sun et al. (2020a) Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang.
    2020a. Infograph: Unsupervised and semi-supervised graph-level representation
    learning via mutual information maximization. In *Proceedings of the International
    Conference on Learning Representations*.'
  id: totrans-1339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 (2020a) Fan-Yun Sun、Jordan Hoffmann、Vikas Verma 和 Jian Tang。2020a。Infograph：通过互信息最大化的无监督和半监督图级表示学习。发表于
    *国际学习表征会议论文集*。
- en: Sun et al. (2020b) Jianhua Sun, Qinhong Jiang, and Cewu Lu. 2020b. Recursive
    social behavior graph for trajectory prediction. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 660–669.
  id: totrans-1340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 (2020b) Jianhua Sun、Qinhong Jiang 和 Cewu Lu。2020b。递归社交行为图用于轨迹预测。发表于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，660–669。
- en: Sun et al. (2022) Qingyun Sun, Jianxin Li, Hao Peng, Jia Wu, Xingcheng Fu, Cheng
    Ji, and S Yu Philip. 2022. Graph structure learning with variational information
    bottleneck. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 36. 4165–4174.
  id: totrans-1341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 (2022) Qingyun Sun、Jianxin Li、Hao Peng、Jia Wu、Xingcheng Fu、Cheng Ji 和
    S Yu Philip。2022。带有变分信息瓶颈的图结构学习。发表于 *AAAI 人工智能会议论文集*，第 36 卷，4165–4174。
- en: 'Sun et al. (2020c) Xiaoqing Sun, Zhiliang Wang, Jiahai Yang, and Xinran Liu.
    2020c. Deepdom: Malicious domain detection with scalable and heterogeneous graph
    convolutional networks. *Computers & Security* 99 (2020), 102057.'
  id: totrans-1342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 (2020c) Xiaoqing Sun、Zhiliang Wang、Jiahai Yang 和 Xinran Liu。2020c。Deepdom：使用可扩展和异质图卷积网络进行恶意域检测。*计算机与安全*
    99 (2020), 102057。
- en: 'Tabassum et al. (2018) Shazia Tabassum, Fabiola SF Pereira, Sofia Fernandes,
    and João Gama. 2018. Social network analysis: An overview. *Wiley Interdisciplinary
    Reviews: Data Mining and Knowledge Discovery* 8, 5 (2018), e1256.'
  id: totrans-1343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tabassum 等人 (2018) Shazia Tabassum、Fabiola SF Pereira、Sofia Fernandes 和 João
    Gama。2018。社交网络分析：概述。*Wiley 交叉学科评论：数据挖掘与知识发现* 8, 5 (2018), e1256。
- en: Tailor et al. (2022) Shyam A Tailor, Felix Opolka, Pietro Lio, and Nicholas Donald
    Lane. 2022. Do We Need Anisotropic Graph Neural Networks?. In *International Conference
    on Learning Representations*.
  id: totrans-1344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tailor 等人 (2022) Shyam A Tailor、Felix Opolka、Pietro Lio 和 Nicholas Donald Lane。2022。我们是否需要各向异性图神经网络？发表于
    *国际学习表征会议*。
- en: Takahashi and Igata (2012) Tetsuro Takahashi and Nobuyuki Igata. 2012. Rumor
    detection on twitter. In *The 6th International Conference on Soft Computing and
    Intelligent Systems, and The 13th International Symposium on Advanced Intelligence
    Systems*. IEEE, 452–457.
  id: totrans-1345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takahashi 和 Igata (2012) Tetsuro Takahashi 和 Nobuyuki Igata。2012。Twitter 上的谣言检测。发表于
    *第 6 届国际软计算与智能系统会议及第 13 届先进智能系统国际研讨会*。IEEE，452–457。
- en: Tang et al. (2020a) Hao Tang, Donghong Ji, Chenliang Li, and Qiji Zhou. 2020a.
    Dependency graph enhanced dual-transformer structure for aspect-based sentiment
    classification. In *Proceedings of the 58th annual meeting of the association
    for computational linguistics*. 6578–6588.
  id: totrans-1346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等人（2020a）Hao Tang, Donghong Ji, Chenliang Li, 和 Qiji Zhou. 2020a. 依赖图增强的双变换器结构用于基于方面的情感分类.
    见于 *第58届计算语言学协会年会论文集*。6578–6588。
- en: 'Tang et al. (2015a) Jian Tang, Meng Qu, and Qiaozhu Mei. 2015a. Pte: Predictive
    text embedding through large-scale heterogeneous text networks. In *Proceedings
    of the 21th ACM SIGKDD international conference on knowledge discovery and data
    mining*. 1165–1174.'
  id: totrans-1347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等人（2015a）Jian Tang, Meng Qu, 和 Qiaozhu Mei. 2015a. Pte：通过大规模异构文本网络进行预测文本嵌入.
    见于 *第21届ACM SIGKDD国际知识发现与数据挖掘会议论文集*。1165–1174。
- en: 'Tang et al. (2015b) Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan,
    and Qiaozhu Mei. 2015b. Line: Large-scale information network embedding. In *Proceedings
    of the 24th international conference on world wide web*. 1067–1077.'
  id: totrans-1348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等人（2015b）Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, 和 Qiaozhu
    Mei. 2015b. Line：大规模信息网络嵌入. 见于 *第24届国际万维网会议论文集*。1067–1077。
- en: Tang et al. (2020b) Xianfeng Tang, Yandong Li, Yiwei Sun, Huaxiu Yao, Prasenjit
    Mitra, and Suhang Wang. 2020b. Transferring robustness for graph neural network
    against poisoning attacks. In *Proceedings of the 13th international conference
    on web search and data mining*. 600–608.
  id: totrans-1349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等人（2020b）Xianfeng Tang, Yandong Li, Yiwei Sun, Huaxiu Yao, Prasenjit Mitra,
    和 Suhang Wang. 2020b. 针对图神经网络的毒化攻击的鲁棒性转移. 见于 *第13届国际网络搜索与数据挖掘会议论文集*。600–608。
- en: Thölke and Fabritiis (2022) Philipp Thölke and Gianni De Fabritiis. 2022. Equivariant
    Transformers for Neural Network based Molecular Potentials. In *ICLR*. [https://openreview.net/forum?id=zNHzqZ9wrRB](https://openreview.net/forum?id=zNHzqZ9wrRB)
  id: totrans-1350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thölke和Fabritiis（2022）Philipp Thölke 和 Gianni De Fabritiis. 2022. 等变变换器用于基于神经网络的分子势能.
    见于 *ICLR*。 [https://openreview.net/forum?id=zNHzqZ9wrRB](https://openreview.net/forum?id=zNHzqZ9wrRB)
- en: 'Thomas et al. (2018) Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang,
    Li Li, Kai Kohlhoff, and Patrick Riley. 2018. Tensor field networks: Rotation-and
    translation-equivariant neural networks for 3d point clouds. *arXiv preprint arXiv:1802.08219*
    (2018).'
  id: totrans-1351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thomas等人（2018）Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li
    Li, Kai Kohlhoff, 和 Patrick Riley. 2018. 张量场网络：用于3D点云的旋转和位移等变神经网络. *arXiv预印本 arXiv:1802.08219*
    (2018)。
- en: Toivonen et al. (2003) Hannu Toivonen, Ashwin Srinivasan, Ross D King, Stefan
    Kramer, and Christoph Helma. 2003. Statistical evaluation of the predictive toxicology
    challenge 2000–2001. *Bioinformatics* 19, 10 (2003), 1183–1193.
  id: totrans-1352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toivonen等人（2003）Hannu Toivonen, Ashwin Srinivasan, Ross D King, Stefan Kramer,
    和 Christoph Helma. 2003. 预测毒理学挑战2000-2001的统计评估. *生物信息学* 19, 10 (2003), 1183–1193。
- en: Unankard et al. (2014) Sayan Unankard, Xue Li, Mohamed Sharaf, Jiang Zhong,
    and Xueming Li. 2014. Predicting elections from social networks based on sub-event
    detection and sentiment analysis. In *International Conference on Web Information
    Systems Engineering*. Springer, 1–16.
  id: totrans-1353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unankard等人（2014）Sayan Unankard, Xue Li, Mohamed Sharaf, Jiang Zhong, 和 Xueming
    Li. 2014. 基于子事件检测和情感分析的社交网络选举预测. 见于 *国际网络信息系统工程会议*。Springer, 1–16。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems* 30 (2017).
  id: totrans-1354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等人（2017）Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. 注意力即一切. *神经信息处理系统进展*
    30 (2017)。
- en: Veličković et al. (2017) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks.
    *arXiv preprint arXiv:1710.10903* (2017).
  id: totrans-1355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veličković等人（2017）Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana
    Romero, Pietro Lio, 和 Yoshua Bengio. 2017. 图注意力网络. *arXiv预印本 arXiv:1710.10903*
    (2017)。
- en: Verma et al. (2010) Jitender Verma, Vijay M Khedkar, and Evans C Coutinho. 2010.
    3D-QSAR in drug design-a review. *Current topics in medicinal chemistry* 10, 1
    (2010), 95–115.
  id: totrans-1356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verma等人（2010）Jitender Verma, Vijay M Khedkar, 和 Evans C Coutinho. 2010. 药物设计中的3D-QSAR综述.
    *当前医学化学话题* 10, 1 (2010), 95–115。
- en: 'Vincent et al. (2010) Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua
    Bengio, Pierre-Antoine Manzagol, and Léon Bottou. 2010. Stacked denoising autoencoders:
    Learning useful representations in a deep network with a local denoising criterion.
    *Journal of machine learning research* 11, 12 (2010).'
  id: totrans-1357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent等人（2010）Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio,
    Pierre-Antoine Manzagol, 和 Léon Bottou. 2010. 堆叠去噪自编码器：在具有局部去噪标准的深度网络中学习有用的表示.
    *机器学习研究期刊* 11, 12 (2010)。
- en: Vinyals et al. (2016) Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan
    Wierstra, et al. 2016. Matching networks for one shot learning. *Advances in neural
    information processing systems* 29 (2016).
  id: totrans-1358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals et al. (2016) Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan
    Wierstra 等. 2016. 一次性学习的匹配网络。*神经信息处理系统进展* 29 (2016)。
- en: 'Walsh et al. (2023) Dylan J Walsh, Weizhong Zou, Ludwig Schneider, Reid Mello,
    Michael E Deagen, Joshua Mysona, Tzyy-Shyang Lin, Juan J de Pablo, Klavs F Jensen,
    Debra J Audus, et al. 2023. Community Resource for Innovation in Polymer Technology
    (CRIPT): A Scalable Polymer Material Data Structure.'
  id: totrans-1359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Walsh et al. (2023) Dylan J Walsh, Weizhong Zou, Ludwig Schneider, Reid Mello,
    Michael E Deagen, Joshua Mysona, Tzyy-Shyang Lin, Juan J de Pablo, Klavs F Jensen,
    Debra J Audus 等. 2023. 聚合物技术创新社区资源 (CRIPT)：一个可扩展的聚合物材料数据结构。
- en: Walters et al. (1998) W Patrick Walters, Matthew T Stahl, and Mark A Murcko.
    1998. Virtual screening—an overview. *Drug discovery today* 3, 4 (1998), 160–178.
  id: totrans-1360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Walters et al. (1998) W Patrick Walters, Matthew T Stahl, 和 Mark A Murcko. 1998.
    虚拟筛选—概述。*药物发现今日* 3, 4 (1998), 160–178。
- en: Wan and Kokel (2021) Guihong Wan and Harsha Kokel. 2021. Graph sparsification
    via meta-learning. *DLG@ AAAI* (2021).
  id: totrans-1361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan and Kokel (2021) Guihong Wan 和 Harsha Kokel. 2021. 通过元学习进行图稀疏化。*DLG@ AAAI*
    (2021)。
- en: 'Wang et al. (2017) Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing
    Jiang. 2017. Mgae: Marginalized graph autoencoder for graph clustering. In *Proceedings
    of the 2017 ACM on Conference on Information and Knowledge Management*. 889–898.'
  id: totrans-1362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2017) Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, 和 Jing
    Jiang. 2017. Mgae：用于图聚类的边际化图自编码器。见 *第2017届 ACM 信息与知识管理会议论文集*。889–898。
- en: Wang et al. (2016) Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep
    network embedding. In *Proceedings of the 22nd ACM SIGKDD international conference
    on Knowledge discovery and data mining*. 1225–1234.
  id: totrans-1363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2016) Daixin Wang, Peng Cui, 和 Wenwu Zhu. 2016. 结构化深度网络嵌入。见 *第22届
    ACM SIGKDD 国际知识发现与数据挖掘会议论文集*。1225–1234。
- en: Wang et al. (2019e) Hongwei Wang, Fuzheng Zhang, Miao Zhao, Wenjie Li, Xing
    Xie, and Minyi Guo. 2019e. Multi-task feature learning for knowledge graph enhanced
    recommendation. In *The world wide web conference*. 2000–2010.
  id: totrans-1364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019e) Hongwei Wang, Fuzheng Zhang, Miao Zhao, Wenjie Li, Xing
    Xie, 和 Minyi Guo. 2019e. 知识图谱增强推荐的多任务特征学习。见 *全球互联网大会*。2000–2010。
- en: Wang et al. (2020a) Jianling Wang, Kaize Ding, Liangjie Hong, Huan Liu, and
    James Caverlee. 2020a. Next-item recommendation with sequential hypergraphs. In
    *Proceedings of the 43rd international ACM SIGIR conference on research and development
    in information retrieval*. 1101–1110.
  id: totrans-1365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020a) Jianling Wang, Kaize Ding, Liangjie Hong, Huan Liu, 和 James
    Caverlee. 2020a. 基于序列超图的下一个项目推荐。见 *第43届国际 ACM SIGIR 信息检索研究与发展会议论文集*。1101–1110。
- en: Wang et al. (2020e) Jie Wang, Li Zhu, Tao Dai, and Yabin Wang. 2020e. Deep memory
    network with Bi-LSTM for personalized context-aware citation recommendation. *Neurocomputing*
    410 (2020), 103–113.
  id: totrans-1366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020e) Jie Wang, Li Zhu, Tao Dai, 和 Yabin Wang. 2020e. 基于双向 LSTM
    的深度记忆网络用于个性化上下文感知引用推荐。*神经计算* 410 (2020), 103–113。
- en: 'Wang et al. (2019c) Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X
    Chang, and Daniel Ritchie. 2019c. Planit: Planning and instantiating indoor scenes
    with relation graph and spatial prior networks. *ACM Transactions on Graphics
    (TOG)* 38, 4 (2019), 1–15.'
  id: totrans-1367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019c) Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel
    X Chang, 和 Daniel Ritchie. 2019c. Planit：使用关系图和空间先验网络进行室内场景的规划和实例化。*ACM 图形学交易*
    38, 4 (2019), 1–15。
- en: 'Wang et al. (2005) Renxiao Wang, Xueliang Fang, Yipin Lu, Chao-Yie Yang, and
    Shaomeng Wang. 2005. The PDBbind database: methodologies and updates. *Journal
    of medicinal chemistry* 48, 12 (2005), 4111–4119.'
  id: totrans-1368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2005) Renxiao Wang, Xueliang Fang, Yipin Lu, Chao-Yie Yang, 和 Shaomeng
    Wang. 2005. PDBbind 数据库：方法学和更新。*药物化学杂志* 48, 12 (2005), 4111–4119。
- en: Wang et al. (2020c) Senzhang Wang, Hao Miao, Hao Chen, and Zhiqiu Huang. 2020c.
    Multi-task adversarial spatial-temporal networks for crowd flow prediction. In
    *Proceedings of the 29th ACM international conference on information & knowledge
    management*. 1555–1564.
  id: totrans-1369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020c) Senzhang Wang, Hao Miao, Hao Chen, 和 Zhiqiu Huang. 2020c.
    多任务对抗空间-时间网络用于人群流量预测。见 *第29届 ACM 国际信息与知识管理会议论文集*。1555–1564。
- en: 'Wang et al. (2022a) Xiao Wang, Deyu Bo, Chuan Shi, Shaohua Fan, Yanfang Ye,
    and S Yu Philip. 2022a. A survey on heterogeneous graph embedding: methods, techniques,
    applications and sources. *IEEE Transactions on Big Data* (2022).'
  id: totrans-1370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022a) Xiao Wang, Deyu Bo, Chuan Shi, Shaohua Fan, Yanfang Ye,
    和 S Yu Philip. 2022a. 异质图嵌入综述：方法、技术、应用和来源。*IEEE 大数据交易* (2022)。
- en: Wang et al. (2019a) Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng
    Chua. 2019a. Neural graph collaborative filtering. In *Proceedings of the 42nd
    international ACM SIGIR conference on Research and development in Information
    Retrieval*. 165–174.
  id: totrans-1371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2019a）Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, 和 Tat-Seng Chua.
    2019a. 神经图协同过滤。载于 *第42届国际ACM SIGIR信息检索研究与开发会议论文集*。165–174。
- en: Wang et al. (2019b) Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng
    Cui, and Philip S Yu. 2019b. Heterogeneous graph attention network. In *The world
    wide web conference*. 2022–2032.
  id: totrans-1372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2019b）Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui,
    和 Philip S Yu. 2019b. 异构图注意力网络。载于 *全球网络会议*。2022–2032。
- en: Wang et al. (2020b) Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu,
    and Tat-Seng Chua. 2020b. Disentangled graph collaborative filtering. In *Proceedings
    of the 43rd international ACM SIGIR conference on research and development in
    information retrieval*. 1001–1010.
  id: totrans-1373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2020b）Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, 和 Tat-Seng
    Chua. 2020b. 解耦图协同过滤。载于 *第43届国际ACM SIGIR信息检索研究与开发会议论文集*。1001–1010。
- en: Wang et al. (2021) Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, and Dejing
    Dou. 2021. Property-aware relation networks for few-shot molecular property prediction.
    *Advances in Neural Information Processing Systems* 34 (2021), 17441–17454.
  id: totrans-1374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2021）Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, 和 Dejing Dou.
    2021. 具备属性感知的关系网络用于少样本分子属性预测。*神经信息处理系统进展* 34 (2021), 17441–17454。
- en: 'Wang et al. (2022c) Yifan Wang, Yifang Qin, Fang Sun, Bo Zhang, Xuyang Hou,
    Ke Hu, Jia Cheng, Jun Lei, and Ming Zhang. 2022c. DisenCTR: Dynamic Graph-based
    Disentangled Representation for Click-Through Rate Prediction. In *Proceedings
    of the 45th International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 2314–2318.'
  id: totrans-1375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2022c）Yifan Wang, Yifang Qin, Fang Sun, Bo Zhang, Xuyang Hou, Ke Hu,
    Jia Cheng, Jun Lei, 和 Ming Zhang. 2022c. DisenCTR：基于动态图的解耦表示用于点击率预测。载于 *第45届国际ACM
    SIGIR信息检索研究与开发会议论文集*。2314–2318。
- en: Wang et al. (2019d) Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M
    Bronstein, and Justin M Solomon. 2019d. Dynamic graph cnn for learning on point
    clouds. *Acm Transactions On Graphics (tog)* 38, 5 (2019), 1–12.
  id: totrans-1376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2019d）Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein,
    和 Justin M Solomon. 2019d. 用于点云学习的动态图CNN。*ACM图形学交易（TOG）* 38, 5 (2019), 1–12。
- en: Wang et al. (2022d) Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani.
    2022d. Molecular contrastive learning of representations via graph neural networks.
    *Nature Machine Intelligence* 4, 3 (2022), 279–287.
  id: totrans-1377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2022d）Yuyang Wang, Jianren Wang, Zhonglin Cao, 和 Amir Barati Farimani.
    2022d. 通过图神经网络的分子对比学习表示。*自然机器智能* 4, 3 (2022), 279–287。
- en: Wang et al. (2022b) Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard
    Baraniuk, and Anima Anandkumar. 2022b. Retrieval-based Controllable Molecule Generation.
    *arXiv preprint arXiv:2208.11126* (2022).
  id: totrans-1378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2022b）Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard Baraniuk,
    和 Anima Anandkumar. 2022b. 基于检索的可控分子生成。*arXiv预印本 arXiv:2208.11126* (2022)。
- en: Wang et al. (2020d) Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao,
    and Minghui Qiu. 2020d. Global context enhanced graph neural networks for session-based
    recommendation. In *Proceedings of the 43rd international ACM SIGIR conference
    on research and development in information retrieval*. 169–178.
  id: totrans-1379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2020d）Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, 和 Minghui
    Qiu. 2020d. 全球上下文增强的图神经网络用于基于会话的推荐。载于 *第43届国际ACM SIGIR信息检索研究与开发会议论文集*。169–178。
- en: Wang et al. (2022e) Zhaobo Wang, Yanmin Zhu, Qiaomei Zhang, Haobing Liu, Chunyang
    Wang, and Tong Liu. 2022e. Graph-Enhanced Spatial-Temporal Network for Next POI
    Recommendation. *ACM Transactions on Knowledge Discovery from Data (TKDD)* 16,
    6 (2022), 1–21.
  id: totrans-1380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2022e）Zhaobo Wang, Yanmin Zhu, Qiaomei Zhang, Haobing Liu, Chunyang
    Wang, 和 Tong Liu. 2022e. 图增强时空网络用于下一个POI推荐。*ACM数据知识发现交易* 16, 6 (2022), 1–21。
- en: 'Wei et al. (2019) Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang
    Hong, and Tat-Seng Chua. 2019. MMGCN: Multi-modal graph convolution network for
    personalized recommendation of micro-video. In *Proceedings of the 27th ACM International
    Conference on Multimedia*. 1437–1445.'
  id: totrans-1381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2019）Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong,
    和 Tat-Seng Chua. 2019. MMGCN：用于个性化微视频推荐的多模态图卷积网络。载于 *第27届ACM国际多媒体会议论文集*。1437–1445。
- en: Weisfeiler and Leman (1968) Boris Weisfeiler and Andrei Leman. 1968. The reduction
    of a graph to canonical form and the algebra which appears therein. *nti, Series*
    2, 9 (1968), 12–16.
  id: totrans-1382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weisfeiler and Leman (1968) 博里斯·韦斯费勒和安德烈·莱曼。1968年。图的规范形式及其中出现的代数。*nti，系列* 2卷，第9期（1968年），12–16页。
- en: Werneck et al. (2020) Heitor Werneck, Nícollas Silva, Matheus Carvalho Viana,
    Fernando Mourão, Adriano CM Pereira, and Leonardo Rocha. 2020. A survey on point-of-interest
    recommendation in location-based social networks. In *Proceedings of the Brazilian
    Symposium on Multimedia and the Web*. 185–192.
  id: totrans-1383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Werneck et al. (2020) 赫伊托尔·维尔内克、尼科拉斯·席尔瓦、马修斯·卡瓦略·维亚纳、费尔南多·穆朗、阿德里亚诺·CM·佩雷拉、以及莱昂纳多·罗查。2020年。基于位置的社交网络中兴趣点推荐的调查。在*巴西多媒体与网络研讨会论文集*中。185–192页。
- en: 'Wieder et al. (2020) Oliver Wieder, Stefan Kohlbacher, Mélaine Kuenemann, Arthur
    Garon, Pierre Ducrot, Thomas Seidel, and Thierry Langer. 2020. A compact review
    of molecular property prediction with graph neural networks. *Drug Discovery Today:
    Technologies* 37 (2020), 1–12.'
  id: totrans-1384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wieder et al. (2020) 奥利弗·维德尔、斯特凡·科尔巴赫、梅兰妮·库恩曼、阿瑟·加龙、皮埃尔·迪克罗、托马斯·赛德尔、以及蒂埃里·兰格。2020年。关于使用图神经网络进行分子属性预测的紧凑综述。*药物发现今日：技术*
    37卷（2020年），1–12页。
- en: Williams and Seeger (2001) Christopher KI Williams and Matthias Seeger. 2001.
    Using the Nyström method to speed up kernel machines. In *Advances in neural information
    processing systems*. 682–688.
  id: totrans-1385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams and Seeger (2001) 克里斯托弗·KI·威廉姆斯和马蒂亚斯·西格。2001年。使用Nyström方法加速核机器。在*神经信息处理系统进展*中。682–688页。
- en: Withnall et al. (2020) Michael Withnall, Edvard Lindelöf, Ola Engkvist, and
    Hongming Chen. 2020. Building attention and edge message passing neural networks
    for bioactivity and physical–chemical property prediction. *Journal of cheminformatics*
    12, 1 (2020), 1–18.
  id: totrans-1386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Withnall et al. (2020) 迈克尔·威斯诺尔、埃德瓦尔德·林德洛夫、奥拉·恩克维斯特、以及洪铭·陈。2020年。构建用于生物活性和物理-化学性质预测的注意力和边缘消息传递神经网络。*化学信息学期刊*
    12卷，第1期（2020年），1–18页。
- en: Wu et al. (2019b) Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao
    Yu, and Kilian Weinberger. 2019b. Simplifying graph convolutional networks. In
    *International conference on machine learning*. PMLR, 6861–6871.
  id: totrans-1387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2019b) 费利克斯·吴、阿毛里·苏扎、张天艺、克里斯托弗·费弗提、余涛、以及基利安·温伯格。2019b年。简化图卷积网络。在*机器学习国际会议*中。PMLR，6861–6871页。
- en: Wu et al. (2022) Junran Wu, Xueyuan Chen, Ke Xu, and Shangzhe Li. 2022. Structural
    entropy guided graph hierarchical pooling. In *International Conference on Machine
    Learning*. PMLR, 24017–24030.
  id: totrans-1388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2022) 吴俊然、陈雪原、徐柯、以及李尚哲。2022年。结构熵引导的图层次池化。在*机器学习国际会议*中。PMLR，24017–24030页。
- en: Wu et al. (2019c) Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and
    Tieniu Tan. 2019c. Session-based recommendation with graph neural networks. In
    *Proceedings of the AAAI conference on artificial intelligence*, Vol. 33. 346–353.
  id: totrans-1389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2019c) 伍舒、唐宇元、朱延乔、王亮、谢兴、以及谭天纽。2019c年。基于会话的图神经网络推荐。在*AAAI人工智能会议论文集*中，第33卷。346–353页。
- en: Wu et al. (2021) Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini,
    Joseph E Gonzalez, and Ion Stoica. 2021. Representing long-range context for graph
    neural networks with global attention. *Advances in Neural Information Processing
    Systems* 34 (2021), 13266–13279.
  id: totrans-1390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2021) 吴张浩、帕拉斯·贾恩、马修·赖特、阿扎利亚·米尔霍塞尼、约瑟夫·E·冈萨雷斯、以及伊昂·斯托伊卡。2021年。使用全局注意力表示图神经网络的长程上下文。*神经信息处理系统进展*
    34卷（2021年），13266–13279页。
- en: Wu et al. (2020) Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi
    Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks.
    *IEEE transactions on neural networks and learning systems* 32, 1 (2020), 4–24.
  id: totrans-1391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2020) 伍宗涵、潘世睿、陈凤文、龙国栋、张成启、以及**S Yu Philip**。2020年。图神经网络的综合调查。*IEEE神经网络与学习系统汇刊*
    32卷，第1期（2020年），4–24页。
- en: Wu et al. (2019a) Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi
    Zhang. 2019a. Graph wavenet for deep spatial-temporal graph modeling. In *Proceedings
    of the 28th International Joint Conference on Artificial Intelligence*. 1907–1913.
  id: totrans-1392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2019a) 伍宗涵、潘世睿、龙国栋、蒋静、以及张成启。2019a年。用于深度时空图建模的图WaveNet。在*第28届国际联合人工智能会议论文集*中。1907–1913页。
- en: 'Wu et al. (2018) Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes,
    Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. MoleculeNet:
    a benchmark for molecular machine learning. *Chemical science* 9, 2 (2018), 513–530.'
  id: totrans-1393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2018) 伍震琴、巴拉斯·拉姆苏达、埃文·N·费因伯格、约瑟夫·戈梅斯、卡勒布·杰尼斯、阿尼什·S·帕普、卡尔·莱斯温格、以及维贾伊·潘德。2018年。MoleculeNet：一个分子机器学习基准。*化学科学*
    9卷，第2期（2018年），513–530页。
- en: 'Xia et al. (2021) Feng Xia, Ke Sun, Shuo Yu, Abdul Aziz, Liangtian Wan, Shirui
    Pan, and Huan Liu. 2021. Graph learning: A survey. *IEEE Transactions on Artificial
    Intelligence* 2, 2 (2021), 109–127.'
  id: totrans-1394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. (2021) Feng Xia, Ke Sun, Shuo Yu, Abdul Aziz, Liangtian Wan, Shirui
    Pan, and Huan Liu. 2021. 图学习：综述。*IEEE人工智能学报* 2, 2 (2021)，109–127。
- en: 'Xia et al. (2022b) Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li.
    2022b. Simgrace: A simple framework for graph contrastive learning without data
    augmentation. In *Proceedings of the ACM Web Conference 2022*. 1070–1079.'
  id: totrans-1395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. (2022b) Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li.
    2022b. Simgrace：一种无数据增强的图对比学习简单框架。发表于 *ACM Web Conference 2022论文集*。1070–1079。
- en: Xia et al. (2022a) Lianghao Xia, Chao Huang, Yong Xu, Jiashu Zhao, Dawei Yin,
    and Jimmy Huang. 2022a. Hypergraph contrastive collaborative filtering. In *Proceedings
    of the 45th International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 70–79.
  id: totrans-1396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. (2022a) Lianghao Xia, Chao Huang, Yong Xu, Jiashu Zhao, Dawei Yin,
    and Jimmy Huang. 2022a. 超图对比协同过滤。发表于 *第45届国际ACM SIGIR信息检索研究与开发会议论文集*。70–79。
- en: 'Xie et al. (2020) Peng Xie, Tianrui Li, Jia Liu, Shengdong Du, Xin Yang, and
    Junbo Zhang. 2020. Urban flow prediction from spatiotemporal data using machine
    learning: A survey. *Information Fusion* 59 (2020), 1–12.'
  id: totrans-1397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2020) Peng Xie, Tianrui Li, Jia Liu, Shengdong Du, Xin Yang, and
    Junbo Zhang. 2020. 基于时空数据的城市流动预测：综述。*信息融合* 59 (2020)，1–12。
- en: Xie et al. (2022a) Yu Xie, Yanfeng Liang, Maoguo Gong, AK Qin, Yew-Soon Ong,
    and Tiantian He. 2022a. Semisupervised Graph Neural Networks for Graph Classification.
    *IEEE Transactions on Cybernetics* (2022).
  id: totrans-1398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2022a) Yu Xie, Yanfeng Liang, Maoguo Gong, AK Qin, Yew-Soon Ong,
    and Tiantian He. 2022a. 半监督图神经网络用于图分类。*IEEE Transactions on Cybernetics* (2022)。
- en: Xie et al. (2022b) Yu Xie, Shengze Lv, Yuhua Qian, Chao Wen, and Jiye Liang.
    2022b. Active and Semi-supervised Graph Neural Networks for Graph Classification.
    *IEEE Transactions on Big Data* (2022).
  id: totrans-1399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2022b) Yu Xie, Shengze Lv, Yuhua Qian, Chao Wen, and Jiye Liang.
    2022b. 主动和半监督图神经网络用于图分类。*IEEE大数据学报* (2022)。
- en: 'Xie et al. (2021) Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang,
    Yong Yu, and Lei Li. 2021. Mars: Markov molecular sampling for multi-objective
    drug discovery. *arXiv preprint arXiv:2103.10432* (2021).'
  id: totrans-1400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2021) Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang,
    Yong Yu, and Lei Li. 2021. Mars：用于多目标药物发现的马尔可夫分子采样。*arXiv预印本arXiv:2103.10432*
    (2021)。
- en: 'Xie et al. (2022c) Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and
    Shuiwang Ji. 2022c. Self-supervised learning of graph neural networks: A unified
    review. *IEEE transactions on pattern analysis and machine intelligence* (2022).'
  id: totrans-1401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2022c) Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and
    Shuiwang Ji. 2022c. 图神经网络的自监督学习：统一综述。*IEEE模式分析与机器智能学报* (2022)。
- en: 'Xiong et al. (2018) Haoyi Xiong, Amin Vahedian, Xun Zhou, Yanhua Li, and Jun
    Luo. 2018. Predicting traffic congestion propagation patterns: A propagation graph
    approach. In *Proceedings of the 11th ACM SIGSPATIAL International Workshop on
    Computational Transportation Science*. 60–69.'
  id: totrans-1402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong et al. (2018) Haoyi Xiong, Amin Vahedian, Xun Zhou, Yanhua Li, and Jun
    Luo. 2018. 预测交通拥堵传播模式：一种传播图方法。发表于 *第11届ACM SIGSPATIAL国际计算运输科学研讨会论文集*。60–69。
- en: Xu et al. (2019b) Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng.
    2019b. Graph Wavelet Neural Network. In *International Conference on Learning
    Representations*.
  id: totrans-1403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2019b) Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng.
    2019b. 图小波神经网络。发表于 *国际学习表征会议*。
- en: Xu et al. (2021a) Chonghuan Xu, Austin Shijun Ding, and Kaidi Zhao. 2021a. A
    novel POI recommendation method based on trust relationship and spatial–temporal
    factors. *Electronic Commerce Research and Applications* 48 (2021), 101060.
  id: totrans-1404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2021a) Chonghuan Xu, Austin Shijun Ding, and Kaidi Zhao. 2021a. 一种基于信任关系和时空因素的新型POI推荐方法。*电子商务研究与应用*
    48 (2021)，101060。
- en: Xu et al. (2019a) Chenxiao Xu, Hao Huang, and Shinjae Yoo. 2019a. Scalable causal
    graph learning through a deep neural network. In *Proceedings of the 28th ACM
    international conference on information and knowledge management*. 1853–1862.
  id: totrans-1405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2019a) Chenxiao Xu, Hao Huang, and Shinjae Yoo. 2019a. 通过深度神经网络进行可扩展的因果图学习。发表于
    *第28届ACM国际信息与知识管理会议论文集*。1853–1862。
- en: Xu et al. (2021b) Chonghuan Xu, Dongsheng Liu, and Xinyao Mei. 2021b. Exploring
    an efficient POI recommendation model based on user characteristics and spatial-temporal
    factors. *Mathematics* 9, 21 (2021), 2673.
  id: totrans-1406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2021b) Chonghuan Xu, Dongsheng Liu, and Xinyao Mei. 2021b. 基于用户特征和时空因素的高效POI推荐模型探索。*数学*
    9, 21 (2021)，2673。
- en: Xu et al. (2018a) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
    2018a. How Powerful are Graph Neural Networks?. In *International Conference on
    Learning Representations*.
  id: totrans-1407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xu等人（2018a）](https://www.example.org/xu_2018a) Keyulu Xu, Weihua Hu, Jure
    Leskovec和Stefanie Jegelka。2018a。图神经网络有多强大？。在*学习表示国际会议*上。'
- en: Xu et al. (2018b) Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
    Kawarabayashi, and Stefanie Jegelka. 2018b. Representation learning on graphs
    with jumping knowledge networks. In *International conference on machine learning*.
    PMLR, 5453–5462.
  id: totrans-1408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xu等人（2018b）](https://www.example.org/xu_2018b) Keyulu Xu, Chengtao Li, Yonglong
    Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi和Stefanie Jegelka。2018b。跳跃知识网络上的图表示学习。在*机器学习国际会议*上。PMLR,
    5453-5462。'
- en: Xu et al. (2020) Zhenyi Xu, Yu Kang, Yang Cao, and Zhijun Li. 2020. Spatiotemporal
    graph convolution multifusion network for urban vehicle emission prediction. *IEEE
    Transactions on Neural Networks and Learning Systems* 32, 8 (2020), 3342–3354.
  id: totrans-1409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xu等人（2020）](https://www.example.org/xu_2020) Zhenyi Xu, Yu Kang, Yang Cao和Zhijun
    Li。2020。城市车辆排放预测的时空图卷积多融合网络。*IEEE神经网络与学习系统交易* 32, 8 (2020), 3342-3354。'
- en: 'Xu et al. (2017) Zheng Xu, Sheng Wang, Feiyun Zhu, and Junzhou Huang. 2017.
    Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery.
    In *Proceedings of the 8th ACM international conference on bioinformatics, computational
    biology, and health informatics*. 285–294.'
  id: totrans-1410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xu等人（2017）](https://www.example.org/xu_2017) Zheng Xu, Sheng Wang, Feiyun
    Zhu和Junzhou Huang。2017。Seq2seq指纹：一种用于药物发现的无监督深度分子嵌入。在*生物信息学、计算生物学和健康信息学第8届国际会议论文集*上。285-294。'
- en: 'Yang et al. (2021) Chaoying Yang, Kaibo Zhou, and Jie Liu. 2021. SuperGraph:
    Spatial-temporal graph-based feature extraction for rotating machinery diagnosis.
    *IEEE Transactions on Industrial Electronics* 69, 4 (2021), 4167–4176.'
  id: totrans-1411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yang等人（2021）](https://www.example.org/yang_2021) Chaoying Yang, Kaibo Zhou和Jie
    Liu。2021。SuperGraph: 旋转机械诊断的空间-时间图特征提取。*IEEE工业电子交易* 69, 4 (2021), 4167-4176。'
- en: Yang et al. (2022a) Haoran Yang, Hongxu Chen, Shirui Pan, Lin Li, Philip S Yu,
    and Guandong Xu. 2022a. Dual Space Graph Contrastive Learning. In *Proceedings
    of the Web Conference*. 1238–1247.
  id: totrans-1412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yang等人（2022a）](https://www.example.org/yang_2022a) Haoran Yang, Hongxu Chen,
    Shirui Pan, Lin Li, Philip S Yu和Guandong Xu。2022a。双空间图对比学习。在*网络会议论文集*中。1238-1247。'
- en: Yang et al. (2019) Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp
    Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea,
    et al. 2019. Analyzing learned molecular representations for property prediction.
    *Journal of chemical information and modeling* 59, 8 (2019), 3370–3388.
  id: totrans-1413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yang等人（2019）](https://www.example.org/yang_2019) Kevin Yang, Kyle Swanson,
    Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy
    Hopper, Brian Kelley, Miriam Mathea等。2019。用于属性预测的学习分子表示分析。*化学信息学和建模杂志* 59, 8 (2019),
    3370-3388。'
- en: Yang et al. (2020) Yuanxuan Yang, Alison Heppenstall, Andy Turner, and Alexis
    Comber. 2020. Using graph structural information about flows to enhance short-term
    demand prediction in bike-sharing systems. *Computers, Environment and Urban Systems*
    83 (2020), 101521.
  id: totrans-1414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yang等人（2020）](https://www.example.org/yang_2020) Yuanxuan Yang, Alison Heppenstall,
    Andy Turner和Alexis Comber。2020。利用有关流动的图结构信息增强共享单车系统的短期需求预测。*计算机、环境和城市系统* 83 (2020),
    101521。'
- en: Yang et al. (2022b) Yuhao Yang, Chao Huang, Lianghao Xia, Yuxuan Liang, Yanwei
    Yu, and Chenliang Li. 2022b. Multi-Behavior Hypergraph-Enhanced Transformer for
    Sequential Recommendation. In *Proceedings of the 28th ACM SIGKDD Conference on
    Knowledge Discovery and Data Mining*. 2263–2274.
  id: totrans-1415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yang等人（2022b）](https://www.example.org/yang_2022b) Yuhao Yang, Chao Huang,
    Lianghao Xia, Yuxuan Liang, Yanwei Yu和Chenliang Li。2022b。用于顺序推荐的多行为超图增强变压器。在*第28届ACM
    SIGKDD知识发现与数据挖掘会议论文集*中。2263-2274。'
- en: Yao et al. (2018) Huaxiu Yao, Fei Wu, Jintao Ke, Xianfeng Tang, Yitian Jia,
    Siyu Lu, Pinghua Gong, Jieping Ye, and Zhenhui Li. 2018. Deep multi-view spatial-temporal
    network for taxi demand prediction. In *Proceedings of the AAAI conference on
    artificial intelligence*, Vol. 32.
  id: totrans-1416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yao等人（2018）](https://www.example.org/yao_2018) Huaxiu Yao, Fei Wu, Jintao
    Ke, Xianfeng Tang, Yitian Jia, Siyu Lu, Pinghua Gong, Jieping Ye和Zhenhui Li。2018。出租车需求预测的深度多视图时空网络。在*AAAI人工智能大会论文集*上。Vol.
    32。'
- en: Yao et al. (2020) Shaowei Yao, Tianming Wang, and Xiaojun Wan. 2020. Heterogeneous
    graph transformer for graph-to-sequence learning. In *Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics*. 7145–7154.
  id: totrans-1417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yao等人（2020）](https://www.example.org/yao_2020) Shaowei Yao, Tianming Wang和Xiaojun
    Wan。2020。用于图到序列学习的异质图变压器。在*计算语言学联合会年会论文集*上。7145-7154。'
- en: 'Yildirimoglu and Kim (2018) Mehmet Yildirimoglu and Jiwon Kim. 2018. Identification
    of communities in urban mobility networks using multi-layer graphs of network
    traffic. *Transportation Research Part C: Emerging Technologies* 89 (2018), 254–267.'
  id: totrans-1418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yildirimoglu and Kim (2018) Mehmet Yildirimoglu 和 Jiwon Kim. 2018. 使用网络流量的多层图识别城市移动网络中的社区。*运输研究C部分：新兴技术*
    89 (2018), 254–267。
- en: Ying et al. (2021) Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin
    Ke, Di He, Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform
    badly for graph representation? *Advances in Neural Information Processing Systems*
    34 (2021), 28877–28888.
  id: totrans-1419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ying et al. (2021) Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin
    Ke, Di He, Yanming Shen, 和 Tie-Yan Liu. 2021. 变压器在图表示任务中真的表现差吗？*神经信息处理系统进展* 34
    (2021), 28877–28888。
- en: 'Ying et al. (2019) Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik,
    and Jure Leskovec. 2019. Gnnexplainer: Generating explanations for graph neural
    networks. In *Proceedings of the Conference on Neural Information Processing Systems*.'
  id: totrans-1420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ying et al. (2019) Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik,
    和 Jure Leskovec. 2019. Gnnexplainer: 为图神经网络生成解释。发表于 *神经信息处理系统会议论文集*。'
- en: Ying et al. (2018) Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren,
    Will Hamilton, and Jure Leskovec. 2018. Hierarchical graph representation learning
    with differentiable pooling. *Advances in neural information processing systems*
    31 (2018).
  id: totrans-1421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ying et al. (2018) Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren,
    Will Hamilton, 和 Jure Leskovec. 2018. 具有可微池化的层次图表示学习。*神经信息处理系统进展* 31 (2018)。
- en: You et al. (2018) Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure
    Leskovec. 2018. Graph convolutional policy network for goal-directed molecular
    graph generation. *Advances in neural information processing systems* 31 (2018).
  id: totrans-1422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You et al. (2018) Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, 和 Jure Leskovec.
    2018. 目标导向的分子图生成的图卷积策略网络。*神经信息处理系统进展* 31 (2018)。
- en: You et al. (2020) Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen.
    2020. When does self-supervision help graph convolutional networks?. In *international
    conference on machine learning*. PMLR, 10871–10880.
  id: totrans-1423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You et al. (2020) Yuning You, Tianlong Chen, Zhangyang Wang, 和 Yang Shen. 2020.
    自我监督何时有助于图卷积网络？发表于 *国际机器学习会议*。PMLR，10871–10880。
- en: 'Yu et al. (2017) Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2017. Spatio-temporal
    graph convolutional networks: A deep learning framework for traffic forecasting.
    *arXiv preprint arXiv:1709.04875* (2017).'
  id: totrans-1424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2017) Bing Yu, Haoteng Yin, 和 Zhanxing Zhu. 2017. 时空图卷积网络：用于交通预测的深度学习框架。*arXiv预印本
    arXiv:1709.04875* (2017)。
- en: 'Yu et al. (2021b) Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and
    Yiming Yang. 2021b. Graph-revised convolutional network. In *Machine Learning
    and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent,
    Belgium, September 14–18, 2020, Proceedings, Part III*. Springer, 378–393.'
  id: totrans-1425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2021b) Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, 和 Yiming
    Yang. 2021b. 图修正卷积网络。发表于 *机器学习与知识发现数据库：欧洲会议，ECML PKDD 2020，比利时根特，2020年9月14-18日，论文集，第三部分*。Springer，378–393。
- en: Yu et al. (2022) Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and
    Quoc Viet Hung Nguyen. 2022. Are graph augmentations necessary? simple graph contrastive
    learning for recommendation. In *Proceedings of the 45th International ACM SIGIR
    Conference on Research and Development in Information Retrieval*. 1294–1303.
  id: totrans-1426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2022) Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, 和
    Quoc Viet Hung Nguyen. 2022. 图增强是否必要？简单的图对比学习用于推荐。发表于 *第45届国际ACM SIGIR信息检索研究与发展会议论文集*。1294–1303。
- en: Yu et al. (2021a) Le Yu, Bowen Du, Xiao Hu, Leilei Sun, Liangzhe Han, and Weifeng
    Lv. 2021a. Deep spatio-temporal graph convolutional network for traffic accident
    prediction. *Neurocomputing* 423 (2021), 135–147.
  id: totrans-1427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2021a) Le Yu, Bowen Du, Xiao Hu, Leilei Sun, Liangzhe Han, 和 Weifeng
    Lv. 2021a. 用于交通事故预测的深度时空图卷积网络。*神经计算* 423 (2021), 135–147。
- en: 'Yu et al. (2018) Shuo Yu, Jiaying Liu, Zhuo Yang, Zhen Chen, Huizhen Jiang,
    Amr Tolba, and Feng Xia. 2018. PAVE: Personalized Academic Venue recommendation
    Exploiting co-publication networks. *Journal of Network and Computer Applications*
    104 (2018), 38–47.'
  id: totrans-1428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu et al. (2018) Shuo Yu, Jiaying Liu, Zhuo Yang, Zhen Chen, Huizhen Jiang,
    Amr Tolba, 和 Feng Xia. 2018. PAVE: 个性化学术场所推荐，利用共同发表网络。*网络与计算机应用期刊* 104 (2018),
    38–47。'
- en: Yu et al. (2012) Xiao Yu, Quanquan Gu, Mianwei Zhou, and Jiawei Han. 2012. Citation
    prediction in heterogeneous bibliographic networks. In *Proceedings of the 2012
    SIAM international conference on data mining*. SIAM, 1119–1130.
  id: totrans-1429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2012) Xiao Yu, Quanquan Gu, Mianwei Zhou, 和 Jiawei Han. 2012. 异质文献网络中的引用预测。发表于
    *2012 SIAM国际数据挖掘会议论文集*。SIAM，1119–1130。
- en: Yu and Gao (2022) Zhaoning Yu and Hongyang Gao. 2022. Molecular representation
    learning via heterogeneous motif graph neural networks. In *International Conference
    on Machine Learning*. PMLR, 25581–25594.
  id: totrans-1430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 和 Gao (2022) Zhaoning Yu 和 Hongyang Gao. 2022. 通过异构模式图神经网络进行分子表示学习。在 *国际机器学习会议*
    上。PMLR，25581–25594。
- en: Yu et al. (2019) Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. 2019.
    Deep modular co-attention networks for visual question answering. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*. 6281–6290.
  id: totrans-1431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 (2019) Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, 和 Qi Tian. 2019. 用于视觉问答的深度模块化协同注意力网络。在
    *IEEE/CVF 计算机视觉与模式识别会议论文集*。6281–6290。
- en: Yue et al. (2022) Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu. 2022.
    Label-invariant Augmentation for Semi-Supervised Graph Classification. In *Proceedings
    of the Conference on Neural Information Processing Systems*.
  id: totrans-1432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue 等 (2022) Han Yue, Chunhui Zhang, Chuxu Zhang, 和 Hongfu Liu. 2022. 标签不变增强用于半监督图分类。在
    *神经信息处理系统会议论文集*。
- en: 'Zang and Wang (2020) Chengxi Zang and Fei Wang. 2020. MoFlow: an invertible
    flow model for generating molecular graphs. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 617–626.'
  id: totrans-1433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zang 和 Wang (2020) Chengxi Zang 和 Fei Wang. 2020. MoFlow: 一种用于生成分子图的可逆流模型。在
    *第26届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集*。617–626。'
- en: Zhang et al. (2019b) Chen Zhang, Qiuchi Li, and Dawei Song. 2019b. Aspect-based
    Sentiment Classification with Aspect-specific Graph Convolutional Networks. In
    *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
    and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*.
    4568–4578.
  id: totrans-1434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019b) Chen Zhang, Qiuchi Li, 和 Dawei Song. 2019b. 基于方面的情感分类与方面特定图卷积网络。在
    *2019 年自然语言处理实证方法会议和第 9 届国际联合自然语言处理会议 (EMNLP-IJCNLP)*。4568–4578。
- en: Zhang et al. (2022a) Cai Zhang, Weimin Li, Dingmei Wei, Yanxia Liu, and Zheng
    Li. 2022a. Network dynamic GCN influence maximization algorithm with leader fake
    labeling mechanism. *IEEE Transactions on Computational Social Systems* (2022).
  id: totrans-1435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022a) Cai Zhang, Weimin Li, Dingmei Wei, Yanxia Liu, 和 Zheng Li. 2022a.
    带有领导者伪标签机制的网络动态 GCN 影响力最大化算法。*IEEE 计算社会系统汇刊* (2022)。
- en: Zhang et al. (2019d) Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami,
    and Nitesh V Chawla. 2019d. Heterogeneous graph neural network. In *Proceedings
    of the 25th ACM SIGKDD international conference on knowledge discovery & data
    mining*. 793–803.
  id: totrans-1436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019d) Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, 和 Nitesh
    V Chawla. 2019d. 异构图神经网络。在 *第25届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集*。793–803。
- en: Zhang et al. (2016) Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie,
    and Wei-Ying Ma. 2016. Collaborative knowledge base embedding for recommender
    systems. In *Proceedings of the 22nd ACM SIGKDD international conference on knowledge
    discovery and data mining*. 353–362.
  id: totrans-1437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2016) Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, 和 Wei-Ying
    Ma. 2016. 用于推荐系统的协同知识库嵌入。在 *第22届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集*。353–362。
- en: Zhang et al. (2022b) Hengyuan Zhang, Suyao Zhao, Ruiheng Liu, Wenlong Wang,
    Yixin Hong, and Runjiu Hu. 2022b. Automatic traffic anomaly detection on the road
    network with spatial-temporal graph neural network representation learning. *Wireless
    Communications and Mobile Computing* 2022 (2022).
  id: totrans-1438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022b) Hengyuan Zhang, Suyao Zhao, Ruiheng Liu, Wenlong Wang, Yixin
    Hong, 和 Runjiu Hu. 2022b. 基于时空图神经网络表示学习的自动交通异常检测。*无线通信与移动计算* 2022 (2022)。
- en: 'Zhang et al. (2018b) Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin
    King, and Dit-Yan Yeung. 2018b. Gaan: Gated attention networks for learning on
    large and spatiotemporal graphs. *arXiv preprint arXiv:1803.07294* (2018).'
  id: totrans-1439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2018b) Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King,
    和 Dit-Yan Yeung. 2018b. Gaan: 用于大规模和时空图学习的门控注意力网络。*arXiv 预印本 arXiv:1803.07294*
    (2018)。'
- en: 'Zhang et al. (2020b) Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun.
    2020b. Graph-bert: Only attention is needed for learning graph representations.
    *arXiv preprint arXiv:2001.05140* (2020).'
  id: totrans-1440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2020b) Jiawei Zhang, Haopeng Zhang, Congying Xia, 和 Li Sun. 2020b.
    Graph-bert: 学习图表示只需要注意力。*arXiv 预印本 arXiv:2001.05140* (2020)。'
- en: Zhang et al. (2018a) Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen.
    2018a. An end-to-end deep learning architecture for graph classification. In *Proceedings
    of the AAAI conference on artificial intelligence*, Vol. 32.
  id: totrans-1441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2018a) Muhan Zhang, Zhicheng Cui, Marion Neumann, 和 Yixin Chen. 2018a.
    一种用于图分类的端到端深度学习架构。在 *AAAI 人工智能会议论文集*，第 32 卷。
- en: Zhang and Li (2021) Muhan Zhang and Pan Li. 2021. Nested graph neural networks.
    *Advances in Neural Information Processing Systems* 34 (2021), 15734–15747.
  id: totrans-1442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Li（2021）Muhan Zhang 和 Pan Li。2021。嵌套图神经网络。*神经信息处理系统进展* 34 (2021), 15734–15747。
- en: 'Zhang et al. (2018d) Xiaoyu Zhang, Sheng Wang, Feiyun Zhu, Zheng Xu, Yuhong
    Wang, and Junzhou Huang. 2018d. Seq3seq fingerprint: towards end-to-end semi-supervised
    deep drug discovery. In *Proceedings of the 2018 ACM International Conference
    on Bioinformatics, Computational Biology, and Health Informatics*. 404–413.'
  id: totrans-1443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2018d）Xiaoyu Zhang, Sheng Wang, Feiyun Zhu, Zheng Xu, Yuhong Wang, 和
    Junzhou Huang。2018d。Seq3seq 指纹：迈向端到端的半监督深度药物发现。在 *2018年 ACM 国际生物信息学、计算生物学和健康信息学会议论文集*。404–413。
- en: 'Zhang and Zitnik (2020) Xiang Zhang and Marinka Zitnik. 2020. Gnnguard: Defending
    graph neural networks against adversarial attacks. *Advances in neural information
    processing systems* 33 (2020), 9263–9275.'
  id: totrans-1444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Zitnik（2020）Xiang Zhang 和 Marinka Zitnik。2020。Gnnguard：防御图神经网络对抗攻击的研究。*神经信息处理系统进展*
    33 (2020), 9263–9275。
- en: Zhang et al. (2014) Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu,
    and Shaoping Ma. 2014. Explicit factor models for explainable recommendation based
    on phrase-level sentiment analysis. In *Proceedings of the 37th international
    ACM SIGIR conference on Research & development in information retrieval*. 83–92.
  id: totrans-1445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2014）Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu, 和 Shaoping
    Ma。2014。基于短语级情感分析的可解释推荐的显式因子模型。在 *第37届国际 ACM SIGIR 信息检索研究与开发会议论文集*。83–92。
- en: Zhang et al. (2019c) Yingxue Zhang, Soumyasundar Pal, Mark Coates, and Deniz
    Ustebay. 2019c. Bayesian graph convolutional neural networks for semi-supervised
    classification. In *Proceedings of the AAAI conference on artificial intelligence*,
    Vol. 33. 5829–5836.
  id: totrans-1446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2019c）Yingxue Zhang, Soumyasundar Pal, Mark Coates, 和 Deniz Ustebay。2019c。用于半监督分类的贝叶斯图卷积神经网络。在
    *AAAI 人工智能会议论文集*，第33卷。5829–5836。
- en: Zhang et al. (2021b) Yu Zhang, Peter Tiňo, Aleš Leonardis, and Ke Tang. 2021b.
    A survey on neural network interpretability. *IEEE Transactions on Emerging Topics
    in Computational Intelligence* (2021).
  id: totrans-1447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2021b）Yu Zhang, Peter Tiňo, Aleš Leonardis, 和 Ke Tang。2021b。神经网络可解释性的综述。*IEEE
    计算智能新兴主题汇刊*（2021）。
- en: 'Zhang et al. (2018e) Yutao Zhang, Fanjin Zhang, Peiran Yao, and Jie Tang. 2018e.
    Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop..
    In *Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery
    & data mining*. 1002–1011.'
  id: totrans-1448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2018e）Yutao Zhang, Fanjin Zhang, Peiran Yao, 和 Jie Tang。2018e。AMiner中的名称消歧：聚类、维护和人工干预。在
    *第24届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*。1002–1011。
- en: Zhang et al. (2019a) Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei
    Yao, Zhi Yu, and Can Wang. 2019a. Hierarchical graph pooling with structure learning.
    *arXiv preprint arXiv:1911.05954* (2019).
  id: totrans-1449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2019a）Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei
    Yao, Zhi Yu, 和 Can Wang。2019a。具有结构学习的层次化图池化。*arXiv 预印本 arXiv:1911.05954*（2019）。
- en: 'Zhang et al. (2020a) Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2020a. Deep learning
    on graphs: A survey. *IEEE Transactions on Knowledge and Data Engineering* (2020).'
  id: totrans-1450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020a）Ziwei Zhang, Peng Cui, 和 Wenwu Zhu。2020a。图上的深度学习：综述。*IEEE 知识与数据工程汇刊*（2020）。
- en: Zhang et al. (2021a) Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong
    Lee. 2021a. Motif-based graph self-supervised learning for molecular property
    prediction. *Advances in Neural Information Processing Systems* 34 (2021), 15870–15882.
  id: totrans-1451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2021a）Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, 和 Chee-Kong Lee。2021a。基于模体的图自监督学习用于分子属性预测。*神经信息处理系统进展*
    34 (2021), 15870–15882。
- en: ZHANG et al. ([n. d.]) ZAIXI ZHANG, Yaosen Min, Shuxin Zheng, and Qi Liu. [n. d.].
    Molecule Generation For Target Protein Binding with Structural Motifs. In *The
    Eleventh International Conference on Learning Representations*.
  id: totrans-1452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZHANG 等（[n. d.]）ZAIXI ZHANG, Yaosen Min, Shuxin Zheng, 和 Qi Liu。[n. d.]。基于结构模体的目标蛋白结合分子生成。在
    *第十一届国际学习表征会议*。
- en: 'Zhang et al. (2018c) Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, and
    Arye Nehorai. 2018c. Retgk: Graph kernels based on return probabilities of random
    walks. In *Advances in Neural Information Processing Systems*. 3964–3974.'
  id: totrans-1453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2018c）Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, 和 Arye Nehorai。2018c。Retgk：基于随机游走返回概率的图核。在
    *神经信息处理系统进展*。3964–3974。
- en: Zhao et al. (2021a) Jialin Zhao, Yuxiao Dong, Ming Ding, Evgeny Kharlamov, and
    Jie Tang. 2021a. Adaptive diffusion in graph neural networks. *Advances in Neural
    Information Processing Systems* 34 (2021), 23321–23333.
  id: totrans-1454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2021a) 赵佳林，董宇霄，丁明，叶甫根尼·哈尔拉莫夫和唐杰。2021a。图神经网络中的自适应扩散。*Advances in
    Neural Information Processing Systems* 第34卷（2021），23321–23333。
- en: Zhao et al. (2021b) Jianan Zhao, Xiao Wang, Chuan Shi, Binbin Hu, Guojie Song,
    and Yanfang Ye. 2021b. Heterogeneous graph structure learning for graph neural
    networks. In *Proceedings of the AAAI conference on artificial intelligence*,
    Vol. 35. 4697–4705.
  id: totrans-1455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2021b) 赵嘉南，王晓，施川，胡彬彬，宋国杰和叶艳芳。2021b。图神经网络的异质图结构学习。发表于 *Proceedings
    of the AAAI conference on artificial intelligence*，第35卷。4697–4705。
- en: Zhao et al. (2022b) Lingxiao Zhao, Saurabh Sawlani, Arvind Srinivasan, and Leman
    Akoglu. 2022b. Graph Anomaly Detection with Unsupervised GNNs. *arXiv preprint
    arXiv:2210.09535* (2022).
  id: totrans-1456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2022b) 赵灵霄，萨乌拉布·索瓦尔尼，阿尔文德·斯里尼瓦桑和莱曼·阿科格鲁。2022b。基于无监督GNN的图异常检测。*arXiv预印本
    arXiv:2210.09535*（2022）。
- en: 'Zhao et al. (2020) Pengpeng Zhao, Anjing Luo, Yanchi Liu, Fuzhen Zhuang, Jiajie
    Xu, Zhixu Li, Victor S Sheng, and Xiaofang Zhou. 2020. Where to go next: A spatio-temporal
    gated network for next poi recommendation. *IEEE Transactions on Knowledge and
    Data Engineering* (2020).'
  id: totrans-1457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2020) 赵鹏鹏，罗安静，刘燕池，庄辅仁，徐家杰，李志旭，维克托·S·盛和周晓芳。2020。下一步去哪：用于下一 POI 推荐的时空门控网络。*IEEE
    Transactions on Knowledge and Data Engineering*（2020）。
- en: 'Zhao et al. (2021c) Tianxiang Zhao, Xiang Zhang, and Suhang Wang. 2021c. Graphsmote:
    Imbalanced node classification on graphs with graph neural networks. In *Proceedings
    of the 14th ACM international conference on web search and data mining*. 833–841.'
  id: totrans-1458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2021c) 赵天翔，张翔和王苏航。2021c。Graphsmote：图神经网络上的不平衡节点分类。发表于 *Proceedings
    of the 14th ACM international conference on web search and data mining*。833–841。
- en: Zhao et al. (2022a) Yusheng Zhao, Jinyu Chen, Chen Gao, Wenguan Wang, Lirong
    Yang, Haibing Ren, Huaxia Xia, and Si Liu. 2022a. Target-Driven Structured Transformer
    Planner for Vision-Language Navigation. In *Proceedings of the 30th ACM International
    Conference on Multimedia*. 4194–4203.
  id: totrans-1459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2022a) 赵玉生，陈金玉，高晨，汪文冠，杨立荣，任海冰，夏华夏和刘思。2022a。目标驱动的结构化变换器规划器用于视觉语言导航。发表于
    *Proceedings of the 30th ACM International Conference on Multimedia*。4194–4203。
- en: Zhao et al. (2018) Zhongying Zhao, Wenqiang Liu, Yuhua Qian, Liqiang Nie, Yilong
    Yin, and Yong Zhang. 2018. Identifying advisor-advisee relationships from co-author
    networks via a novel deep model. *Information Sciences* 466 (2018), 258–269.
  id: totrans-1460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2018) 赵中颖，刘文强，钱玉华，聂立强，尹义龙和张勇。2018。通过新颖的深度模型从共同作者网络中识别导师-学生关系。*Information
    Sciences* 第466卷（2018），258–269。
- en: Zheng et al. (2020) Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao
    Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. 2020. Robust graph representation
    learning via neural sparsification. In *International Conference on Machine Learning*.
    PMLR, 11458–11468.
  id: totrans-1461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2020) 郑成，宗博，程伟，宋东晋，倪静超，余文超，陈海峰和王伟。2020。通过神经稀疏化进行鲁棒的图表示学习。发表于 *International
    Conference on Machine Learning*。PMLR，11458–11468。
- en: 'Zheng et al. (2019) Li Zheng, Zhenpeng Li, Jian Li, Zhao Li, and Jun Gao. 2019.
    AddGraph: Anomaly Detection in Dynamic Graph Using Attention-based Temporal GCN..
    In *IJCAI*. 4419–4425.'
  id: totrans-1462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2019) 李郑，李振鹏，李健，赵丽和高军。2019。AddGraph: 基于注意力的时间图卷积网络中的异常检测。发表于
    *IJCAI*，4419–4425。'
- en: Zheng et al. (2021) Yang Zheng, Xiaoyi Feng, Zhaoqiang Xia, Xiaoyue Jiang, Ambra
    Demontis, Maura Pintor, Battista Biggio, and Fabio Roli. 2021. Why Adversarial
    Reprogramming Works, When It Fails, and How to Tell the Difference. *arXiv preprint
    arXiv:2108.11673* (2021).
  id: totrans-1463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2021) 郑扬，冯晓怡，夏朝强，姜晓月，安布拉·德蒙蒂斯，毛拉·平托，巴蒂斯塔·比吉奥和法比奥·罗利。2021。对抗性重编程为何有效、何时失效以及如何区分。*arXiv预印本
    arXiv:2108.11673*（2021）。
- en: 'Zhou et al. (2006) Dengyong Zhou, Jiayuan Huang, and Bernhard Schölkopf. 2006.
    Learning with hypergraphs: Clustering, classification, and embedding. *Advances
    in neural information processing systems* 19 (2006).'
  id: totrans-1464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2006) 周登勇，黄家元和伯恩哈德·肖尔科普夫。2006。超图学习：聚类、分类和嵌入。*Advances in neural
    information processing systems* 第19卷（2006）。
- en: Zhou et al. (2022a) Fan Zhou, Rongfan Li, Qiang Gao, Goce Trajcevski, Kunpeng
    Zhang, and Ting Zhong. 2022a. Dynamic Manifold Learning for Land Deformation Forecasting.
    (2022).
  id: totrans-1465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2022a) 周凡，李荣范，高强，戈采夫·特拉耶夫斯基，张昆鹏和钟婷。2022a。用于土地变形预测的动态流形学习。（2022）。
- en: 'Zhou et al. (2021) Hao Zhou, Dongchun Ren, Huaxia Xia, Mingyu Fan, Xu Yang,
    and Hai Huang. 2021. AST-GNN: An attention-based spatio-temporal graph neural
    network for Interaction-aware pedestrian trajectory prediction. *Neurocomputing*
    445 (2021), 298–308.'
  id: totrans-1466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 (2021) Hao Zhou, Dongchun Ren, Huaxia Xia, Mingyu Fan, Xu Yang, 和 Hai
    Huang. 2021. AST-GNN：一种基于注意力的时空图神经网络，用于交互感知的行人轨迹预测. *Neurocomputing* 445 (2021),
    298–308.
- en: 'Zhou et al. (2020) Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng
    Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural
    networks: A review of methods and applications. *AI Open* 1 (2020), 57–81.'
  id: totrans-1467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 (2020) Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang,
    Zhiyuan Liu, Lifeng Wang, Changcheng Li, 和 Maosong Sun. 2020. 图神经网络：方法和应用综述. *AI
    Open* 1 (2020), 57–81.
- en: 'Zhou et al. (2022b) Yu Zhou, Haixia Zheng, Xin Huang, Shufeng Hao, Dengao Li,
    and Jumin Zhao. 2022b. Graph Neural Networks: Taxonomy, Advances, and Trends.
    *ACM Transactions on Intelligent Systems and Technology (TIST)* 13, 1 (2022),
    1–54.'
  id: totrans-1468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 (2022b) Yu Zhou, Haixia Zheng, Xin Huang, Shufeng Hao, Dengao Li, 和
    Jumin Zhao. 2022b. 图神经网络：分类、进展和趋势. *ACM智能系统与技术交易* 13, 1 (2022), 1–54.
- en: Zhou et al. (2019) Ziang Zhou, Shenzhong Zhang, and Zengfeng Huang. 2019. Dynamic
    self-training framework for graph convolutional networks. (2019).
  id: totrans-1469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 (2019) Ziang Zhou, Shenzhong Zhang, 和 Zengfeng Huang. 2019. 图卷积网络的动态自训练框架.
    (2019).
- en: Zhu and Koniusz (2021) Hao Zhu and Piotr Koniusz. 2021. Simple spectral graph
    convolution. In *International Conference on Learning Representations*.
  id: totrans-1470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 和 Koniusz (2021) Hao Zhu 和 Piotr Koniusz. 2021. 简单的谱图卷积. 在 *国际学习表征会议* 中.
- en: Zhu et al. (2022a) Jinhua Zhu, Yingce Xia, Chang Liu, Lijun Wu, Shufang Xie,
    Tong Wang, Yusong Wang, Wengang Zhou, Tao Qin, Houqiang Li, et al. 2022a. Direct
    Molecular Conformation Generation. *arXiv preprint arXiv:2202.01356* (2022).
  id: totrans-1471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2022a) Jinhua Zhu, Yingce Xia, Chang Liu, Lijun Wu, Shufang Xie, Tong
    Wang, Yusong Wang, Wengang Zhou, Tao Qin, Houqiang Li, 等人. 2022a. 直接分子构象生成. *arXiv预印本
    arXiv:2202.01356* (2022).
- en: Zhu et al. (2022b) Jinhua Zhu, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Wengang
    Zhou, Houqiang Li, and Tie-Yan Liu. 2022b. Unified 2d and 3d pre-training of molecular
    representations. In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge
    Discovery and Data Mining*. 2626–2636.
  id: totrans-1472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2022b) Jinhua Zhu, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Wengang
    Zhou, Houqiang Li, 和 Tie-Yan Liu. 2022b. 分子表示的统一2D和3D预训练. 在 *第28届ACM SIGKDD知识发现与数据挖掘大会论文集*
    中. 2626–2636.
- en: 'Zhu et al. (2020a) Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
    and Jifeng Dai. 2020a. Deformable detr: Deformable transformers for end-to-end
    object detection. *arXiv preprint arXiv:2010.04159* (2020).'
  id: totrans-1473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2020a) Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, 和 Jifeng
    Dai. 2020a. 可变形的detr：用于端到端目标检测的可变形变换器. *arXiv预印本 arXiv:2010.04159* (2020).
- en: Zhu et al. (2020b) Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang
    Wang. 2020b. Deep graph contrastive representation learning. *arXiv preprint arXiv:2006.04131*
    (2020).
  id: totrans-1474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2020b) Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, 和 Liang Wang.
    2020b. 深度图对比表示学习. *arXiv预印本 arXiv:2006.04131* (2020).
- en: Zhu et al. (2021) Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang
    Wang. 2021. Graph contrastive learning with adaptive augmentation. In *Proceedings
    of the Web Conference 2021*. 2069–2080.
  id: totrans-1475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2021) Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, 和 Liang Wang.
    2021. 带有自适应增强的图对比学习. 在 *2021年网络会议论文集* 中. 2069–2080.
- en: 'Zhu et al. (2020c) Yanqiao Zhu, Yichen Xu, Feng Yu, Shu Wu, and Liang Wang.
    2020c. CAGNN: Cluster-aware graph neural networks for unsupervised graph representation
    learning. *arXiv preprint arXiv:2009.01674* (2020).'
  id: totrans-1476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2020c) Yanqiao Zhu, Yichen Xu, Feng Yu, Shu Wu, 和 Liang Wang. 2020c.
    CAGNN：集群感知图神经网络，用于无监督图表示学习. *arXiv预印本 arXiv:2009.01674* (2020).
- en: Zhu (2022) Zhenyi Zhu. 2022. A Survey of GNN in Bioinformation Data. (2022).
  id: totrans-1477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu (2022) Zhenyi Zhu. 2022. 生物信息数据中的GNN调查. (2022).
- en: Łukasz Maziarka et al. (2020) Łukasz Maziarka, Tomasz Danel, Sławomir Mucha,
    Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrzębski. 2020. Molecule Attention
    Transformer. (2020). arXiv:2002.08264 [cs.LG]
  id: totrans-1478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Łukasz Maziarka 等人 (2020) Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof
    Rataj, Jacek Tabor, 和 Stanisław Jastrzębski. 2020. 分子注意力变换器. (2020). arXiv:2002.08264
    [cs.LG]
