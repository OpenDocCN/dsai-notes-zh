- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:40:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2304.05055] A Comprehensive Survey on Deep Graph Representation Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2304.05055](https://ar5iv.labs.arxiv.org/html/2304.05055)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Comprehensive Survey on Deep Graph Representation Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wei Ju [juwei@pku.edu.cn](mailto:juwei@pku.edu.cn) ,  Zheng Fang [fang˙z@pku.edu.cn](mailto:fang%CB%99z@pku.edu.cn)
    ,  Yiyang Gu [yiyanggu@pku.edu.cn](mailto:yiyanggu@pku.edu.cn) ,  Zequn Liu [zequnliu@pku.edu.cn](mailto:zequnliu@pku.edu.cn)
    ,  Qingqing Long [qingqinglong@pku.edu.cn](mailto:qingqinglong@pku.edu.cn) Peking
    UniversityBeijingChina100871 ,  Ziyue Qiao [ziyuejoe@gmail.com](mailto:ziyuejoe@gmail.com)
    The Hong Kong University of Science and TechnologyGuangzhouChina511453 ,  Yifang
    Qin [qinyifang@pku.edu.cn](mailto:qinyifang@pku.edu.cn) ,  Jianhao Shen [jhshen@pku.edu.cn](mailto:jhshen@pku.edu.cn)
    ,  Fang Sun [fts@pku.edu.cn](mailto:fts@pku.edu.cn) Peking UniversityBeijingChina100871
    ,  Zhiping Xiao [patricia.xiao@cs.ucla.edu](mailto:patricia.xiao@cs.ucla.edu)
    University of California, Los AngelesUSA90095 ,  Junwei Yang [yjwtheonly@pku.edu.cn](mailto:yjwtheonly@pku.edu.cn)
    ,  Jingyang Yuan [yuanjy@pku.edu.cn](mailto:yuanjy@pku.edu.cn) ,  Yusheng Zhao
    [yusheng.zhao@stu.pku.edu.cn](mailto:yusheng.zhao@stu.pku.edu.cn) Peking UniversityBeijingChina100871
    ,  Xiao Luo [xiaoluo@cs.ucla.edu](mailto:xiaoluo@cs.ucla.edu) University of California,
    Los AngelesUSA90095  and  Ming Zhang [mzhang˙cs@pku.edu.cn](mailto:mzhang%CB%99cs@pku.edu.cn)
    Peking UniversityBeijingChina100871(2023)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Graph representation learning aims to effectively encode high-dimensional sparse
    graph-structured data into low-dimensional dense vectors, which is a fundamental
    task that has been widely studied in a range of fields, including machine learning
    and data mining. Classic graph embedding methods follow the basic idea that the
    embedding vectors of interconnected nodes in the graph can still maintain a relatively
    close distance, thereby preserving the structural information between the nodes
    in the graph. However, this is sub-optimal due to: (i) traditional methods have
    limited model capacity which limits the learning performance; (ii) existing techniques
    typically rely on unsupervised learning strategies and fail to couple with the
    latest learning paradigms; (iii) representation learning and downstream tasks
    are dependent on each other which should be jointly enhanced. With the remarkable
    success of deep learning, deep graph representation learning has shown great potential
    and advantages over shallow (traditional) methods, there exist a large number
    of deep graph representation learning techniques have been proposed in the past
    decade, especially graph neural networks. In this survey, we conduct a comprehensive
    survey on current deep graph representation learning algorithms by proposing a
    new taxonomy of existing state-of-the-art literature. Specifically, we systematically
    summarize the essential components of graph representation learning and categorize
    existing approaches by the ways of graph neural network architectures and the
    most recent advanced learning paradigms. Moreover, this survey also provides the
    practical and promising applications of deep graph representation learning. Last
    but not least, we state new perspectives and suggest challenging directions which
    deserve further investigations in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning on Graphs, Graph Representation Learning, Graph Neural Network,
    Survey^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†doi: XXXXXXX.XXXXXXX^†^†journal:
    JACM^†^†ccs: Computing methodologies Neural networks^†^†ccs: Computing methodologies Learning
    latent representations'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction By Wei Ju
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graphs have recently emerged as a powerful tool for representing a variety of
    structured and complex data, including social networks, traffic networks, information
    systems, knowledge graphs, protein-protein interaction networks, and physical
    interaction networks. As a kind of general form of data organization, graph structures
    are capable of naturally expressing the intrinsic relationship of these data,
    and thus can characterize plenty of non-Euclidean structures that are crucial
    in a variety of disciplines and domains due to their flexible adaptability. For
    example, to encode a social network as a graph, nodes on the graph are used to
    represent individual users, and edges are used to represent the relationship between
    two individuals, such as friends. In the field of biology, nodes can be used to
    represent proteins, and edges can be used to represent biological interactions
    between various proteins, such as the dynamic interactions between proteins. Thus,
    by analyzing and mining the graph-structured data, we can understand the deep
    meaning hidden behind the data, and further discover valuable knowledge, so as
    to benefit society and human beings.
  prefs: []
  type: TYPE_NORMAL
- en: In the last decade years, a wide range of machine learning algorithms have been
    developed for graph-structured data learning. Among them, traditional graph kernel
    methods (Gärtner et al., [2003](#bib.bib108); Kashima et al., [2003](#bib.bib178);
    Shervashidze et al., [2011a](#bib.bib315), [2009](#bib.bib317)) usually break
    down graphs into different atomic substructures and then use kernel functions
    to measure the similarity between all pairs of them. Although graph kernels could
    provide a perspective on modeling graph topology, these approaches often generate
    substructures or feature representations based on given hand-crafted criteria.
    These rules are rather heuristic, prone to suffer from high computational complexity,
    and therefore have weak scalability and subpar performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the past few years, graph embedding algorithms (Ahmed et al., [2013](#bib.bib3);
    Perozzi et al., [2014](#bib.bib277); Tang et al., [2015b](#bib.bib345); Grover
    and Leskovec, [2016](#bib.bib124); Tang et al., [2015a](#bib.bib344); Wang et al.,
    [2016](#bib.bib360)) have ever-increasing emerged, which attempt to encode the
    structural information of the graph (usually a high-dimensional sparse matrix)
    and map it into a low-dimensional dense vector embedding to preserve the topology
    information and attribute information in the embedding space as much as possible,
    so that the learned graph embeddings can be naturally integrated into traditional
    machine learning algorithms. Compared to previous works which use feature engineering
    in the pre-processing phase to extract graph structural features, current graph
    embedding algorithms are conducted in a data-driven way leveraging machine learning
    algorithms (such as neural networks) to encode the structural information of the
    graph. Specifically, existing graph embedding methods can be categorized into
    the following main groups: (i) matrix factorization based methods (Ahmed et al.,
    [2013](#bib.bib3); Ou et al., [2016](#bib.bib269); Cao et al., [2015](#bib.bib37))
    that factorize the matrix to learn node embedding which preserves the graph property;
    (ii) deep learning based methods (Perozzi et al., [2014](#bib.bib277); Tang et al.,
    [2015b](#bib.bib345); Grover and Leskovec, [2016](#bib.bib124); Wang et al., [2016](#bib.bib360))
    that apply deep learning techniques specifically designed for graph-structured
    data; (iii) edge reconstruction based methods (Tang et al., [2015a](#bib.bib344);
    Man et al., [2016](#bib.bib254); Liu et al., [2016](#bib.bib230)) that either
    maximizes edge reconstruction probability or minimizes edge reconstruction loss.
    Generally, these methods typically depend on shallow architectures, and fail to
    exploit the potential and capacity of deep neural networks, resulting in sub-optimal
    representation quality and learning performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the recent remarkable success of deep neural networks, a range of
    deep learning algorithms has been developed for graph-structured data learning.
    The core of these methods is to generate effective node and graph representations
    using graph neural networks (GNNs), followed by a goal-oriented learning paradigm.
    In this way, the derived representations can be adaptively coupled with a variety
    of downstream tasks and applications. Following this line of thought, in this
    paper, we propose a new taxonomy to classify the existing graph representation
    learning algorithms, i.e., graph neural network architectures, learning paradigms,
    and various promising applications, as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1\.
    Introduction By Wei Ju ‣ A Comprehensive Survey on Deep Graph Representation Learning").
    Specifically, for the architectures of GNNs, we investigate the studies on graph
    convolutions, graph kernel neural networks, graph pooling, and graph transformer.
    For the learning paradigms, we explore three advanced types namely supervised/semi-supervised
    learning on graphs, graph self-supervised learning, and graph structure learning.
    To demonstrate the effectiveness of the learned graph representations, we provide
    several promising applications to build tight connections between representation
    learning and downstream tasks, such as social analysis, molecular property prediction
    and generation, recommender systems, and traffic analysis. Last but not least,
    we present some perspectives for thought and suggest challenging directions that
    deserve further study in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Differences between this survey and existing ones. Up to now, there exist some
    other overview papers focusing on different perspectives of graph representation
    learning(Wu et al., [2020](#bib.bib388); Zhou et al., [2020](#bib.bib464); Zhang
    et al., [2020a](#bib.bib447); Chami et al., [2022](#bib.bib41); Bacciu et al.,
    [2020](#bib.bib13); Xia et al., [2021](#bib.bib391); Zhou et al., [2022b](#bib.bib465);
    Khoshraftar and An, [2022](#bib.bib180); Chen et al., [2022b](#bib.bib44), [2020f](#bib.bib48))
    that are closely related to ours. However, there are very few comprehensive reviews
    have summarized deep graph representation learning simultaneously from the perspective
    of diverse GNN architectures and corresponding up-to-date learning paradigms.
    Therefore, we here clearly state their distinctions from our survey as follows.
    There have been several surveys on classic graph embedding(Goyal and Ferrara,
    [2018](#bib.bib120); Cai et al., [2018](#bib.bib33)), these works categorize graph
    embedding methods based on different training objectives. Wang et al. (Wang et al.,
    [2022a](#bib.bib367)) goes further and provides a comprehensive review of existing
    heterogeneous graph embedding approaches. With the rapid development of deep learning,
    there are a handful of surveys along this line. For example, Wu et al. (Wu et al.,
    [2020](#bib.bib388)) and Zhang et al. (Zhang et al., [2020a](#bib.bib447)) mainly
    focus on several classical and representative GNN architectures without exploring
    deep graph representation learning from a view of the most recent advanced learning
    paradigms such as graph self-supervised learning and graph structure learning.
    Xia et al. (Xia et al., [2021](#bib.bib391)) and Chami et al. (Chami et al., [2022](#bib.bib41))
    jointly summarize the studies of graph embeddings and GNNs. Zhou et al. (Zhou
    et al., [2020](#bib.bib464)) explores different types of computational modules
    for GNNs. One recent survey under review (Khoshraftar and An, [2022](#bib.bib180))
    categorizes the existing works in graph representation learning from both static
    and dynamic graphs. However, these taxonomies emphasize the basic GNN methods
    but pay insufficient attention to the learning paradigms, and provide few discussions
    of the most promising applications, such as recommender systems and molecular
    property prediction and generation. To the best of our knowledge, the most relevant
    survey published formally is (Zhou et al., [2022b](#bib.bib465)), which presents
    a review of GNN architectures and roughly discusses the corresponding applications.
    Nevertheless, this survey merely covers methods up to the year of 2020, missing
    the latest developments in the past two years.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is highly desired to summarize the representative GNN methods,
    the most recent advanced learning paradigms, and promising applications into one
    unified and comprehensive framework. Moreover, we strongly believe this survey
    with a new taxonomy of literature and more than 400 studies will strengthen future
    research on deep graph representation learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contribution of this survey. The goal of this survey is to systematically review
    the literature on the advances of deep graph representation learning and discuss
    further directions. It aims to help the researchers and practitioners who are
    interested in this area, and support them in understanding the panorama and the
    latest developments of deep graph representation learning. The key contributions
    of this survey are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systematic Taxonomy. We propose a systematic taxonomy to organize the existing
    deep graph representation learning approaches based on the ways of GNN architectures
    and the most recent advanced learning paradigms via providing some representative
    branches of methods. Moreover, several promising applications are presented to
    illustrate the superiority and potential of graph representation learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comprehensive Review. For each branch of this survey, we review the essential
    components and provide detailed descriptions of representative algorithms, and
    systematically summarize the characteristics to make the overview comparison.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Directions. Based on the properties of existing deep graph representation
    learning algorithms, we discuss the limitations and challenges of current methods
    and propose the potential as well as promising research directions deserving of
    future investigations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3c7c2defb2f50ff994b6f754fe621c36.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. The architecture of this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Background By Wei Ju
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first briefly introduce some definitions in deep graph representation
    learning that need to be clarified, then we explain the reasons why we need graph
    representation learning.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Problem Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Definition: Graph. Given a graph $G=(V,E,\mathbf{X})$, where $V=\{v_{1},\cdots,v_{|V|}\}$
    is the set of nodes, $E=\{e_{1},\cdots,e_{|V|}\}$ is the set of edges, and the
    edge $e=(v_{i},v_{j})\in E$ represent the connection relationship between nodes
    $v_{i}$ and $v_{j}$ in the graph. $\mathbf{X}\in\mathbb{R}^{|V|\times M}$ is the
    node feature matrix with $M$ being the dimension of each node feature. The adjacency
    matrix of a graph can be defined as $\mathbf{A}\in\mathbb{R}^{|V|\times|V|}$,
    where $\mathbf{A}_{ij}=1$ if $(v_{i},v_{j})\in E$, otherwise $\mathbf{A}_{ij}=0$.'
  prefs: []
  type: TYPE_NORMAL
- en: The adjacency matrix can be regarded as the structural representation of the
    graph-structured data, in which each row of the adjacency matrix $\mathbf{A}$
    represents the connection relationship between the corresponding node of the row
    and all other nodes, which can be regarded as a discrete representation of the
    node. However, in real-life circumstances, the adjacency matrix $\mathbf{A}$ corresponding
    to $G$ is a highly sparse matrix, and if $\mathbf{A}$ is used directly as node
    representations, it will be seriously affected by computational efficiency. The
    storage space of the adjacency matrix $\mathbf{A}$ is $|V|\times|V|$, which is
    usually unacceptable when the total number of nodes grows to the order of millions.
    At the same time, the value of most dimensions in the node representation is 0\.
    The sparsity will make subsequent machine learning tasks very difficult.
  prefs: []
  type: TYPE_NORMAL
- en: '*Graph representation learning* is a bridge between the original input data
    and the task objectives in the graph. The fundamental idea of the graph representation
    learning algorithm is first to learn the embedded representations of nodes or
    the entire graph from the input graph structure data and then apply these embedded
    representations to downstream related tasks, such as node classification, graph
    classification, link prediction, community detection, and visualization, etc.
    Specifically, it aims to learn low-dimensional, dense distributed embedding representations
    for nodes in the graph. Formally, the goal of graph representation learning is
    to learn its embedding vector representation $R_{v}\in\mathbb{R}^{d}$ for each
    node $v\in V$, where the dimension $d$ of the vector is much smaller than the
    total number of nodes $|V|$ in the graph.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Why study deep graph representation learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the rapid development of deep learning techniques, deep neural networks
    such as convolutional neural networks and recurrent neural networks have made
    breakthroughs in the fields of computer vision, natural language processing, and
    speech recognition. They can well abstract the semantic information of images,
    natural languages, and speeches. However, current deep learning techniques fail
    to handle more complex and irregular graph-structured data. To effectively analyze
    and model this kind of non-Euclidean structure data, many graph representation
    learning algorithms have emerged in recent years, including graph embedding and
    graph neural networks. At present, compared with Euclidean-style data such as
    images, natural language, and speech, graph-structured data is high-dimensional,
    complex, and irregular. Therefore, the graph representation learning algorithm
    is a rather powerful tool for studying graph-structured data. To encode complex
    graph-structured data, deep graph representation learning needs to meet several
    characteristics: (1) *topological properties*: Graph representations need to capture
    the complex topological information of the graph, such as the relationship between
    nodes and nodes, and other substructure information, such as subgraphs, motif,
    etc. (2) *feature attributes*: It is necessary for graph representations to describe
    high-dimensional attribute features in the graph, including the attributes of
    nodes and edges themselves. (3) *scalability*: Because different real graph data
    have different characteristics, graph representation learning algorithms should
    be able to efficiently learn its embedding representation on different graph structure
    data, making it universal and transferable.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Graph Convolutions By Yusheng Zhao
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Graph convolutions have become the basic building blocks in many deep graph
    representation learning algorithms and graph neural networks developed recently.
    In this section, we provide a comprehensive review of graph convolutions, which
    generally fall into two categories: spectral graph convolutions and spatial graph
    convolutions. Based on the solid mathematical foundations of Graph Signal Processing
    (GSP)(Shuman et al., [2013a](#bib.bib321); Sandryhaila and Moura, [2013](#bib.bib304);
    Hammond et al., [2011](#bib.bib130)), spectral graph convolutions seek to capture
    the patterns of the graph in the frequency domain. On the other hand, spatial
    graph convolutions inherit the idea of message passing from Recurrent Graph Neural
    Networks (RecGNNs), and they compute node features by aggregating the features
    of their neighbors. Thus, the computation graph of a node is derived from the
    local graph structure around it, and the graph topology is naturally incorporated
    into the way node features are computed. In this section, we first introduce spectral
    graph convolutions and then spatial graph convolutions, followed by a brief summary.
    In Table [1](#S3.T1 "Table 1 ‣ 3\. Graph Convolutions By Yusheng Zhao ‣ A Comprehensive
    Survey on Deep Graph Representation Learning"), we summarize a number of graph
    convolutions proposed in recent years.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Summary of graph convolution methods.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Category | Aggregation | Time Complexity |'
  prefs: []
  type: TYPE_TB
- en: '| Spectral CNN (Bruna et al., [2013](#bib.bib30)) | Spectral Graph Convolution
    | - | $O(n^{3})$ |'
  prefs: []
  type: TYPE_TB
- en: '| Henaff et al. (Henaff et al., [2015](#bib.bib138)) | Spectral Graph Convolution
    | - | $O(n^{3})$ |'
  prefs: []
  type: TYPE_TB
- en: '| ChebNet (Defferrard et al., [2016](#bib.bib67)) | Spectral Graph Convolution
    | - | $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| GCN (Kipf and Welling, [2016a](#bib.bib183)) | Spectral / Spatial | Weighted
    Average | $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| CayleyNet (Levie et al., [2018](#bib.bib204)) | Spectral Graph Convolution
    | - | $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| GraphSAGE (Hamilton et al., [2017](#bib.bib129)) | Spatial Graph Convolution
    | General | $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| GAT (Veličković et al., [2017](#bib.bib352)) | Spatial Graph Convolution
    | Attentive | $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| DGCNN (Wang et al., [2019d](#bib.bib373)) | Spatial Graph Convolution | General
    | $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| LanzcosNet (Liao et al., [2018](#bib.bib225)) | Spectral Graph Convolution
    | - | $O(n^{2})$ |'
  prefs: []
  type: TYPE_TB
- en: '| SGC (Wu et al., [2019b](#bib.bib384)) | Spatial Graph Convolution | Weighted
    Average | $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| GWNN (Xu et al., [2019b](#bib.bib400)) | Spectral Graph Convolution | - |
    $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| GIN (Xu et al., [2018a](#bib.bib404)) | Spatial Graph Convolution | Sum |
    $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| GraphAIR (Hu et al., [2020d](#bib.bib144)) | Spatial Graph Convolution |
    Sum | $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| PNA (Corso et al., [2020](#bib.bib65)) | Spatial Graph Convolution | Multiple
    | $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| S²GC (Zhu and Koniusz, [2021](#bib.bib467)) | Spectral Graph Convolution
    | - | $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| GNNML3 (Balcilar et al., [2021](#bib.bib16)) | Spatial / Spectral | - | $O(m)$
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSGNN (He et al., [2022](#bib.bib136)) | Spectral Graph Convolution | - |
    $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: '| EGC (Tailor et al., [2022](#bib.bib341)) | Spatial Graph Convolution | General
    | $O(m)$ |'
  prefs: []
  type: TYPE_TB
- en: 3.1\. Spectral Graph Convolutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the success of Convolutional Neural Networks (CNNs) in computer vision (Krizhevsky
    et al., [2017](#bib.bib196)), efforts have been made to transfer the idea of convolution
    to the graph domain. However, this is not an easy task because of the non-Euclidean
    nature of graphical data. Graph signal processing (GSP) (Shuman et al., [2013a](#bib.bib321);
    Sandryhaila and Moura, [2013](#bib.bib304); Hammond et al., [2011](#bib.bib130))
    defines the Fourier Transform on graphs and thus provide solid theoretical foundation
    of spectral graph convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In graph signal processing, a graph signal refers to a set of scalars associated
    to every nodes in the graph, *i.e.* $f(v),~{}\forall v\in V$, and it can be written
    in the $n$-dimensional vector form $\mathbf{x}\in\mathbb{R}^{n}$, where $n$ is
    the number of nodes in the graph. Another core concept of graph signal processing
    is the symmetric normalized graph Laplacian matrix (or simply, the graph Laplacian),
    defined as $\mathbf{L}=\mathbf{I}-\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$,
    where $\mathbf{I}$ is the identity matrix, $\mathbf{D}$ is the degree matrix (*i.e.*
    a diagonal matrix $\mathbf{D}_{ii}=\sum_{j}\mathbf{A}_{ij}$), and $\mathbf{A}$
    is the adjacency matrix. In the typical setting of graph signal processing, the
    graph $G$ is undirected. Therefore, $\mathbf{L}$ is real symmetric and positive
    semi-definite. This guarantees the eigen decomposition of the graph Laplacian:
    $\mathbf{L}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{T}$, where $\mathbf{U}=[\mathbf{u}_{0},\mathbf{u}_{1},...,\mathbf{u}_{n-1}]$
    is the eigenvectors of the graph Laplacian and the diagonal elements of $\mathbf{\Lambda}=\text{diag}(\lambda_{0},\lambda_{1},...,\lambda_{n-1})$
    are the eigenvalues. With this, the Graph Fourier Transform (GFT) of a graph signal
    $\mathbf{x}$ is defined as $\tilde{\mathbf{x}}=\mathbf{U}^{T}\mathbf{x}$, where
    $\tilde{\mathbf{x}}$ is the graph frequencies of $\mathbf{x}$. Correspondingly,
    the Inverse Graph Fourier Transform can be written as $\mathbf{x}=\mathbf{U}\tilde{\mathbf{x}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With GFT and the Convolution Theorem, the graph convolution of a graph signal
    $\mathbf{x}$ and a filter $\mathbf{g}$ can be defined as $\mathbf{g}*_{G}\mathbf{x}=\mathbf{U}(\mathbf{U}^{T}\mathbf{g}\odot\mathbf{U}^{T}\mathbf{x})$.
    To simplify this, let $\mathbf{g}_{\theta}=\text{diag}(\mathbf{U}^{T}g)$, the
    graph convolution can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\mathbf{g}*_{G}\mathbf{x}=\mathbf{U}\mathbf{g}_{\theta}\mathbf{U}^{T}\mathbf{x},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which is the general form of most spectral graph convolutions. The key of spectral
    graph convolutions is to parameterize and learn the filter $\mathbf{g}_{\theta}$.
  prefs: []
  type: TYPE_NORMAL
- en: Bruna et al. (Bruna et al., [2013](#bib.bib30)) propose Spectral Convolutional
    Neural Network (Spectral CNN) that sets graph filter as a learnable diagonal matrix
    $\mathbf{W}$. The convolution operation can be written as $\mathbf{y}=\mathbf{U}\mathbf{W}\mathbf{U}^{T}\mathbf{x}$.
    In practice, multi-channel signals and activation functions are common, and the
    graph convolution can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\mathbf{Y}_{:,j}=\sigma\left(\mathbf{U}\sum_{i=1}^{c_{in}}\mathbf{W}_{i,j}\mathbf{U}^{T}\mathbf{X}_{:,i}\right),~{}j=1,2,...,c_{out},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $c_{in}$ is the number of input channel, $c_{out}$ is the number of output
    channel, $\mathbf{X}$ is a $n\times c_{in}$ matrix representing the input signal,
    $\mathbf{Y}$ is a $n\times c_{out}$ matrix denoting the output signal, $\mathbf{W}_{i,j}$
    is a parameterized diagonal matrix, and $\sigma(\cdot)$ is the activation function.
    For mathematical convenience we sometimes use single-channel version of graph
    convolutions omitting activation functions, and the multi-channel versions are
    similar to Eq. [2](#S3.E2 "In 3.1\. Spectral Graph Convolutions ‣ 3\. Graph Convolutions
    By Yusheng Zhao ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 'Spectral CNN has several limitations. Firstly, the filters are basis-dependent,
    which means that they cannot generalize across graphs. Secondly, the algorithm
    requires eigen decomposition, which is computationally expensive. Thirdly, it
    has no guarantee of spatial localization of filters. To make filters spatially
    localized, Henaff et al. (Henaff et al., [2015](#bib.bib138)) propose to use a
    smooth spectral transfer function $\Theta(\mathbf{\Lambda})$ to parameterize the
    filter, and the convolution operation can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\mathbf{y}=\mathbf{U}F(\mathbf{\Lambda})\mathbf{U}^{T}\mathbf{x}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Chebyshev Spectral Convolutional Neural Network (ChebNet) (Defferrard et al.,
    [2016](#bib.bib67)) extends this idea by using truncated Chebyshev polynomials
    to approximate the spectral transfer function. The Chebyshev polynomial is defined
    as $T_{0}(x)=1,~{}T_{1}(x)=x,~{}T_{k}(x)=2xT_{k-1}(x)-T_{k-2}(x)$, and the spectral
    transfer function $F(\mathbf{\Lambda})$ is approximated to the order of $K-1$
    as
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $F(\mathbf{\Lambda})=\sum_{k=0}^{K-1}\theta_{k}T_{k}(\tilde{\mathbf{\Lambda}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the model parameters $\theta_{k},~{}k\in\{0,1,...,K-1\}$ are the Chebyshev
    coefficients, and $\tilde{\mathbf{\Lambda}}=2\mathbf{\Lambda}/\lambda_{max}-\mathbf{I}$
    is a diagonal matrix of scaled eigenvalues. Thus, the graph convolution can be
    written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\mathbf{g}*_{G}\mathbf{x}=\mathbf{U}F(\mathbf{\Lambda})\mathbf{U}^{T}\mathbf{x}=\mathbf{U}\sum_{k=0}^{K-1}\theta_{k}T_{k}(\tilde{\mathbf{\Lambda}})\mathbf{U}^{T}\mathbf{x}=\sum_{k=0}^{K-1}\theta_{k}T_{k}(\tilde{\mathbf{L}})\mathbf{x},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\tilde{\mathbf{L}}=2\mathbf{L}/\lambda_{max}-\mathbf{I}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph Convolutional Network (GCN) (Kipf and Welling, [2016a](#bib.bib183))
    is proposed as the localized first-order approximation of ChebNet. Assuming $K=2$
    and $\lambda_{max}=2$, Eq. [5](#S3.E5 "In 3.1\. Spectral Graph Convolutions ‣
    3\. Graph Convolutions By Yusheng Zhao ‣ A Comprehensive Survey on Deep Graph
    Representation Learning") can be simplified as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\mathbf{g}*_{G}\mathbf{x}=\theta_{0}\mathbf{x}+\theta_{1}(\mathbf{L}-\mathbf{I})\mathbf{x}=\theta_{0}\mathbf{x}-\theta_{1}\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}\mathbf{x}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'To further constraint the number of parameters, we assume $\theta=\theta_{0}=-\theta_{1}$,
    which gives a simpler form of graph convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\mathbf{g}*_{\mathcal{G}}\mathbf{x}=\theta(\mathbf{I}+\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2})\mathbf{x}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: As $\mathbf{I}+\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}$ now has the eigenvalues
    in the range of $[0,2]$ and repeatedly multiplying this matrix can lead to numerical
    instabilities, GCN empirically proposes a renormalization trick to solve this
    problem by using $\mathbf{\tilde{D}}^{-1/2}\mathbf{\tilde{A}}\mathbf{\tilde{D}}^{-1/2}$
    instead, where $\mathbf{\tilde{A}}=\mathbf{A}+\mathbf{I}$ and $\mathbf{\tilde{D}}_{ii}=\sum_{i}\mathbf{\tilde{A}}_{ij}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Allowing multi-channel signals and adding activation functions, the following
    formula is more commonly seen in literature:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $\mathbf{Y}=\sigma((\mathbf{\tilde{D}}^{-1/2}\mathbf{\tilde{A}}\mathbf{\tilde{D}}^{-1/2})\mathbf{X}\mathbf{\Theta}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{X}$, $\mathbf{Y}$ have the same shape as in Eq. [2](#S3.E2 "In
    3.1\. Spectral Graph Convolutions ‣ 3\. Graph Convolutions By Yusheng Zhao ‣ A
    Comprehensive Survey on Deep Graph Representation Learning") and $\mathbf{\Theta}$
    is a $c_{in}\times c_{out}$ matrix as model’s parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the aforementioned methods, other spectral graph convolutions have
    been proposed. Levie et al. (Levie et al., [2018](#bib.bib204)) propose CayleyNets
    that utilize Cayley Polynomials to equip the filters with the ability to detect
    narrow frequency bands. Liao et al. (Liao et al., [2018](#bib.bib225)) propose
    LanczosNets that employs Lanczos algorithm to construct low-rank approximation
    of graph Laplacian in order to improve the computation efficiency of graph convolutions.
    The proposed model is able to efficiently utilizes the multi-scale information
    in the graph data. Instead of using Graph Fourier Transform, Xu et al. (Xu et al.,
    [2019b](#bib.bib400)) propose Graph Wavelet Neural Network (GWNN) that use graph
    wavelet transform to avoid matrix eigendecomposition. Moreover, graph wavelets
    are sparse and localized, which provide good interpretations for the convolution
    operation. Zhu et al. (Zhu and Koniusz, [2021](#bib.bib467)) derive a Simple Spectral
    Graph Convolution (S²GC) from a modified Markov Diffusion Kernel, which achieves
    a trade-off between low-pass and high-pass filter bands.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Spatial Graph Convolutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inspired by the convolution on Euclidean data (*e.g.* images and texts), which
    applies data transformation on a small region, spatial graph convolutions compute
    the central node’s feature via transforming and aggregating its neighbors’ features.
    In this way, the graph structure is naturally embedded in the computation graph
    of node features. Moreover, the idea of sending one node’s feature to another
    node is similar to the message passing used in recurrent graph neural networks.
    In the following, we will introduce several seminal spatial graph convolutions
    as well as some recently proposed promising methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spatial graph convolutions generally follows a three-step paradigm: message
    generation, feature aggregation and feature update. This can be mathematically
    written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $\mathbf{y}_{i}=\text{UPDATE}\left(\mathbf{x}_{i},\text{AGGREGATE}\left(\{\text{MESSAGE}\left(\mathbf{x}_{i},\mathbf{x}_{j},\mathbf{e}_{ij}\right),~{}j\in\mathcal{N}(i)\}\right)\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{x}_{i}$ and $\mathbf{y}_{i}$ is the input and output feature
    vector of node $i$, $\mathbf{e}_{ij}$ is the feature vector of the edge (or more
    generally, the relationship) between node $i$ and its neighbor node $j$, and $\mathcal{N}(i)$
    denotes the neighbor of node $i$, which could be more generally defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous subsection, we show the spectral interpretation of GCN (Kipf
    and Welling, [2016a](#bib.bib183)). The model also has its spatial interpretation,
    which can be mathematically written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $\mathbf{y}_{i}=\mathbf{\Theta}^{T}\sum_{j\in\mathcal{N}(i)\cup{i}}\frac{1}{\sqrt{\hat{d}_{i}\hat{d}_{j}}}\mathbf{x}_{j},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\hat{d}_{i}$ and $\hat{d}_{j}$ is the $i$-th and $j$-th row sums of $\hat{\mathbf{A}}$
    in Eq. [8](#S3.E8 "In 3.1\. Spectral Graph Convolutions ‣ 3\. Graph Convolutions
    By Yusheng Zhao ‣ A Comprehensive Survey on Deep Graph Representation Learning").
    For each node, the model takes a weighted sum of its neighbors’ features as well
    as its own feature and applies a linear transformation to obtain the result. In
    practice, multiple GCN layers are often stack together with non-linear functions
    after convolution to encode complex and hierarchical features. Nonetheless, Wu
    et al. (Wu et al., [2019b](#bib.bib384)) show that the model still achieves competitive
    results without non-linearity.
  prefs: []
  type: TYPE_NORMAL
- en: Although GCN as well as other spectral graph convolutions achieve competitive
    results on a number of benchmarks, these methods assume the presence of all nodes
    in the graph and fall in the category of transductive learning. Hamilton et al. (Hamilton
    et al., [2017](#bib.bib129)) propose GraphSAGE that performs graph convolutions
    in inductive settings, when there are new nodes during inference (*e.g.* newcomers
    in the social network). For each node, the model samples its $K$-hop neighbors
    and uses $K$ graph convolutions to aggregate their features hierarchically. Furthermore,
    the use of sampling also reduces the computation when a node has too many neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The attention mechanism has been successfully used in natural language processing (Vaswani
    et al., [2017](#bib.bib351)), computer vision (Liu et al., [2021](#bib.bib236))
    and multi-modal tasks (Zhao et al., [2022a](#bib.bib456); Chen et al., [2021b](#bib.bib50);
    He et al., [2021](#bib.bib134); Yu et al., [2019](#bib.bib428)). Graph Attention
    Networks (GAT) (Veličković et al., [2017](#bib.bib352)) introduce the idea of
    attention to graphs. The attention mechanism uses an adaptive, feature-dependent
    weight (*i.e.* attention coefficient) to aggregate a set of features, which can
    be mathematically written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (11) |  | $\alpha_{i,j}=\frac{\exp\left({\text{LeakyReLU}\left(\mathbf{a}^{T}[\mathbf{\Theta}\mathbf{x}_{i}&#124;&#124;\mathbf{\Theta}\mathbf{x}_{j}]\right)}\right)}{\sum_{k\in\mathcal{N}(i)\cup\{i\}}\exp\left({\text{LeakyReLU}\left(\mathbf{a}^{T}[\mathbf{\Theta}\mathbf{x}_{i}&#124;&#124;\mathbf{\Theta}\mathbf{x}_{j}]\right)}\right)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\alpha_{i,j}$ is the attention coefficient, $\mathbf{a}$ and $\mathbf{\Theta}$
    are model parameters, and $[\cdot||\cdot]$ means concatenation. After the $\alpha$s
    are obtained, the new features are computed as a weighted sum of input node features,
    which is:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (12) |  | $\mathbf{y}_{i}=\alpha_{i,i}\mathbf{\Theta}\mathbf{x}_{i}+\sum_{j\in\mathcal{N}(i)}\alpha_{i,j}\mathbf{\Theta}\mathbf{x}_{j}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Xu et al. (Xu et al., [2018a](#bib.bib404)) explores the representational limitations
    of graph neural networks. What they discover is that message passing networks
    like GCN (Kipf and Welling, [2016a](#bib.bib183)) and GraphSAGE (Hamilton et al.,
    [2017](#bib.bib129)) are incapable of distinguishing certain graph structures.
    To improve the representational power of graph neural networks, they propose Graph
    Isomorphism Network (GIN) that gives an adjustable weight to the central node
    feature, which can be mathematically written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | $\mathbf{y}_{i}=\text{MLP}\left((1+\epsilon)\mathbf{x}_{i}+\sum_{j\in\mathcal{N}(i)}\mathbf{x}_{j}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\epsilon$ is a learnable parameter.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, efforts have been made to improve the representational power
    of graph neural networks. For example, Hu et al. (Hu et al., [2020d](#bib.bib144))
    propose GraphAIR that explicitly models the neighborhood interaction to better
    capture complex non-linear features. Specifically, they use Hadamard product between
    pairs of nodes in the neighborhood to model the quadratic terms of neighborhood
    interaction. Balcilar et al. (Balcilar et al., [2021](#bib.bib16)) propose GNNML3
    that break the limits of first-order Weisfeiler-Lehman test (1-WL) and reach third-order
    WL test (3-WL) experimentally. They also show that Hadamard product is required
    for the model to have more representational power than first-order Weisfeiler-Lehman
    test. Other elements in spatial graph convolutions are widely studied. For example,
    Corso et al. (Corso et al., [2020](#bib.bib65)) explore the aggregation operation
    in GNN and propose Principal Neighbourhood Aggregation (PNA) that uses multiple
    aggregators with degree-scalers. Tailor et al. (Tailor et al., [2022](#bib.bib341))
    explores the anisotropism and isotropism in the message passing process of graph
    neural networks, and propose Efficient Graph Convolution (EGC) that achieves promising
    results with reduced memory consumption due to isotropism.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section introduces graph convolutions and we provide the summary as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques. Graph convolutions mainly fall into two types, *i.e.* spectral graph
    convolutions and spatial graph convolutions. Spectral graph convolutions have
    solid mathematical foundations of Graph Signal Processing and therefore their
    operations have theoretical interpretations. Spatial graph convolutions are inspired
    by Recurrent Graph Neural Networks and their computation is simple and straightforward,
    as their computation graph is derived from the local graph structure. Generally,
    spatial graph convolutions are more common in applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and Limitations. Despite the great success of graph convolutions,
    their performance is unsatisfactory in more complicated applications. On the one
    hand, the performance of graph convolutions rely heavily on the construction of
    the graph. Different construction of the graph might result in different performance
    of graph convolutions. On the other hand, graph convolutions are prone to over-smoothing
    when constructing very deep neural networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Works. In the future, we expect that more powerful graph convolutions
    are developed to mitigate the problem of over-smoothing and we also hope that
    techniques and methodologies in Graph Structure Learning (GSL) can help learn
    more meaningful graph structure to benefit the performance of graph convolutions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4\. Graph Kernel Neural Networks By Qingqing Long
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graph kernels (GKs) are historically the most widely used technique on graph
    analyzing and representation tasks (Shawe-Taylor et al., [2004](#bib.bib314);
    Gärtner et al., [2003](#bib.bib108); Zhou et al., [2020](#bib.bib464)). However,
    traditional graph kernels rely on hand-crafted patterns or domain knowledge on
    specific tasks(Kriege et al., [2020](#bib.bib195); Shervashidze et al., [2009](#bib.bib317)).
    Over the years, an amount of research has been conducted on graph kernel neural
    networks (GKNNs), which has yielded promising results. Researchers have explored
    various aspects of GKNNs, including their theoretical foundations, algorithmic
    design, and practical applications. These efforts have led to the development
    of a wide range of GKNN-based models and methods that can be used for graph analysis
    and representation tasks, such as node classification, link prediction, and graph
    clustering (Chen et al., [2020c](#bib.bib45); Long et al., [2021a](#bib.bib238),
    [b](#bib.bib239)).
  prefs: []
  type: TYPE_NORMAL
- en: The success of GKNNs can be attributed to their ability to leverage the strengths
    of both graph kernels and neural networks. By using kernel functions to measure
    similarity between graphs, GKNNs can capture the structural properties of graphs,
    while the use of neural networks enables them to learn more complex and abstract
    representations of graphs. This combination of techniques allows GKNNs to achieve
    state-of-the-art performance on a wide range of graph-related tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we begin with introducing the most representative traditional
    graph kernels. Then we summarize the basic framework for combining GNNs and graph
    kernels. Finally, we categorize the popular graph kernel Neural networks into
    several categories and compare their differences.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Graph Kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph kernels generally evaluate pairwise similarity between nodes or graphs
    by decomposing them into basic structural units. Random walks (Kang et al., [2012](#bib.bib176)),
    subtrees (Shervashidze et al., [2011b](#bib.bib316)), shortest paths (Borgwardt
    and Kriegel, [2005](#bib.bib25)) and graphlets (Shervashidze et al., [2009](#bib.bib317))
    are representative categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given two graphs $G_{1}=(V_{1},E_{1},X_{1})$ and $G_{2}=(V_{2},E_{2},X_{2})$,
    a graph kernel function $K(G_{1},G_{2})$ measures the similarity between $G_{1}$
    and $G_{2}$ through the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (14) |  | $K(G_{1},G_{2})=\sum_{u_{1}\in V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{base}\left(l_{G_{1}}(u_{1}),l_{G_{2}}(u_{2})\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $l_{G}(u)$ denotes a set of local substructures centered at node $u$
    in graph $G$, and $\kappa_{base}$ is a base kernel measuring the similarity between
    the two sets of substructures. For simplicity, we may rewrite Eqn. [14](#S4.E14
    "In 4.1\. Graph Kernels ‣ 4\. Graph Kernel Neural Networks By Qingqing Long ‣
    A Comprehensive Survey on Deep Graph Representation Learning") as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (15) |  | $K(G_{1},G_{2})=\sum_{u_{1}\in V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{base}(u_{1},u_{2}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: the uppercase letter $K(G_{1},G_{2})$ is denoted as graph kernels, $\kappa(u_{1},u_{2})$
    is denoted as node kernels, and lowercase $k(x,y)$ is denoted as general kernel
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel mapping of a kernel $\psi$ maps a data point into its corresponding
    Reproducing Kernel Hilbert Space (RKHS) $\mathcal{H}$. Specifically, given a kernel
    $k_{*}(\cdot,\cdot)$, its kernel mapping $\psi_{*}$ can be formulized as,
  prefs: []
  type: TYPE_NORMAL
- en: '| (16) |  | $\forall x_{1},x_{2},k_{*}(x_{1},x_{2})=\langle\psi_{*}(x_{1}),\psi_{*}(x_{2})\rangle_{\mathcal{H}_{*}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{H}_{*}$ is the RKHS of $k_{*}(\cdot,\cdot)$.
  prefs: []
  type: TYPE_NORMAL
- en: We introduce several representative and popular used graph kernels below.
  prefs: []
  type: TYPE_NORMAL
- en: Walk and Path Kernels. A $l$-walk kernel $K_{walk}^{(l)}$ compares all length
    $l$ walks starting from each node in two graphs $G_{1},G_{2}$,
  prefs: []
  type: TYPE_NORMAL
- en: '| (17) |  | $\displaystyle\kappa_{walk}^{(l)}(u_{1},u_{2})$ | $\displaystyle=\sum_{w_{1}\in\mathcal{W}^{l}(G_{1},u_{1})}\sum_{w_{2}\in\mathcal{W}^{l}(G_{2},u_{2})}\delta(X_{1}(w_{1}),X_{2}(w_{2})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle K_{walk}^{(l)}(G_{1},G_{2})$ | $\displaystyle=\sum_{u_{1}\in
    V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{walk}^{(l)}(u_{1},u_{2}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Substituting $\mathcal{W}$ with $\mathcal{P}$ is able to get the $l$-path kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Subtree Kernels. The WL subtree kernel is the most popular one in subtree kernels.
    It is a finite-depth kernel variant of the 1-WL test. The WL subtree kernel with
    depth $l$, $K_{WL}^{(l)}$ compares all subtrees with depth $\leq l$ rooted at
    each node.
  prefs: []
  type: TYPE_NORMAL
- en: '| (18) |  | $\displaystyle\kappa_{subtree}^{(i)}(u_{1},u_{2})$ | $\displaystyle=\sum_{t_{1}\in\mathcal{T}^{i}(G_{1},u_{2})}\sum_{t_{2}\in\mathcal{T}^{i}(G_{2},u_{2})}\delta(t_{1},t_{2}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle K_{subtree}^{(i)}(G_{1},G_{2})$ | $\displaystyle=\sum_{u_{1}\in
    V_{1}}\sum_{u_{2}\in V_{2}}\kappa_{subtree}^{(i)}(u_{1},u_{2}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle K_{WL}^{(l)}(G_{1},G_{2})$ | $\displaystyle=\sum_{i=0}^{l}K_{subtree}^{(i)}(G_{1},G_{2}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $t\in\mathcal{T}^{(i)}(G,u)$ denotes a subtree of depth $i$ rooted at
    $u$ in $G$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. General Framework of GKNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we summarize the general framework of GKNNs. For the first
    step, a kernel that measures similarities of heterogeneous features from heterogeneous
    nodes and edges $(u_{1},e_{\cdot,u_{2}})$ and $(u_{2},e_{\cdot,u_{2}})$ should
    be defined. Take the inner product of neighbor tensors as an example, its neighbor
    kernel is defined as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\kappa((u_{1},e_{\cdot,u_{1}}),(u_{2},e_{\cdot,u_{2}}))=\langle f(u_{1}),f(u_{2})\rangle\cdot\langle
    f(e_{\cdot,u_{1}}),f(e_{\cdot,u_{2}})\rangle.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Based on the neighbor kernel, a kernel with two $l$-hop neighborhoods for central
    node $u_{1}$ and $u_{2}$ can be defined as $K^{(l)}(u_{1},u_{2})=$
  prefs: []
  type: TYPE_NORMAL
- en: '| (19) |  | $\left\{\begin{aligned} &amp;\langle f(u_{1}),f(u_{2})\rangle&amp;\
    l=0\\ &amp;\langle f(u_{1}),f(u_{2})\rangle\cdot\sum_{v_{1}\in N(u_{1})}\sum_{v_{2}\in
    N(u_{2})}K^{(l-1)}(v_{1},v_{2})\cdot\langle f(e_{\cdot,v_{1}}),f(e_{\cdot,v_{2}})\rangle&amp;\
    l>0\end{aligned}\right.,$ |  |'
  prefs: []
  type: TYPE_TB
- en: By regarding the lower-hop kernel $\kappa^{(l-1)}(u_{1},u_{2})$, as the inner
    product of the $(l-1)$-th hidden representations of $u_{1}$ and $u_{2}$. Furthermore,
    by recursively applying the neighborhood kernel, the $l$-hop graph kernel can
    be derived as
  prefs: []
  type: TYPE_NORMAL
- en: '| (20) |  | $K^{l}(G_{1},G_{2})=\sum_{\boldsymbol{w}_{1}\in\mathcal{W}^{l}(G_{1})}\sum_{\boldsymbol{w}_{2}\in\mathcal{W}^{l}(G_{2})}\left(\prod_{i=0}^{l-1}\langle
    f(\boldsymbol{w}_{1}^{(i)}),f(\boldsymbol{w}_{2}^{(i)})\rangle\times\prod_{i=0}^{l-2}\langle
    f(e_{\boldsymbol{w}_{1}^{(i)},\boldsymbol{w}_{1}^{(i+1)}}),f(e_{\boldsymbol{w}_{2}^{(i)},\boldsymbol{w}_{2}^{(i+1)}})\rangle\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{W}^{l}(G)$ denotes the set of all walk sequences with length
    $l$ in graph $G$, and $\boldsymbol{w}_{1}^{(i)}$ denotes the $i$-th node in sequence
    $\boldsymbol{w}_{1}$.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Eqn. ([16](#S4.E16 "In 4.1\. Graph Kernels ‣ 4\. Graph Kernel Neural
    Networks By Qingqing Long ‣ A Comprehensive Survey on Deep Graph Representation
    Learning")), kernel methods implicitly perform projections from original data
    spaces to their RKHS $\mathcal{H}$. Hence, as GNNs also project nodes or graphs
    into vector spaces, connections have been established between GKs and GNNs through
    the kernel mappings. And several works conducted research on the connections (Lei
    et al., [2017](#bib.bib203); Williams and Seeger, [2001](#bib.bib382)), and found
    some foundation conclusions. Take the basic rule introduced in (Lei et al., [2017](#bib.bib203))
    as an example, the proposed graph kernel in Eqn. ([14](#S4.E14 "In 4.1\. Graph
    Kernels ‣ 4\. Graph Kernel Neural Networks By Qingqing Long ‣ A Comprehensive
    Survey on Deep Graph Representation Learning")) can be derived as the general
    formulas,
  prefs: []
  type: TYPE_NORMAL
- en: '| (21) |  | $\displaystyle h^{(0)}(v)=$ | $\displaystyle\boldsymbol{W}^{(0)}_{t_{V}(v)}f(v),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{(l)}(v)=$ | $\displaystyle\boldsymbol{W}^{(l)}_{t_{V}(v)}f(v)\odot\sum_{u\in
    N(v)}(\boldsymbol{U}_{t_{V}(v)}^{(l)}h^{(l-1)}(u)\odot\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}f(e_{u,v})),\qquad
    1<l\leq L,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\odot$ is the element-wise product and $h^{(l)}(v)$ is the cell state
    vector of node v. The parameter matrices $\boldsymbol{W}^{(l)}_{t_{V}(v)}$, $\boldsymbol{U}_{t_{V}(v)}^{(l)}$
    and $\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}$ are learnable parameters related to
    types of nodes and edges.
  prefs: []
  type: TYPE_NORMAL
- en: Then mean embeddings of all nodes are usually used to represent the graph-level
    embedding, let $|G_{i}|$ denote the number of nodes in the $i$-th graph, then
    the graph-level embeddings are generated as,
  prefs: []
  type: TYPE_NORMAL
- en: '| (22) |  | $\Phi(G_{i})=\sum_{u\in G_{i}}\frac{1}{&#124;G_{i}&#124;}h^{(L)}(v).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: For semi-supervised multiclass classification, the cross entropy is used as
    the objective function over all training examples (Cao et al., [2016](#bib.bib38),
    [2015](#bib.bib37)),
  prefs: []
  type: TYPE_NORMAL
- en: '| (23) |  | $\mathcal{L}=-\sum_{l\in\mathcal{Y}_{L}}\sum_{f=1}^{F}Y_{lf}\ln{Z_{lf}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{Y}_{L}$ is the set of node indices which have labels in node
    classification tasks, or set of graph indices in graph classification tasks. $Z_{lf}$
    denotes the prediction of labels, which are outputs of a linear layer with an
    activation function, inputing $h^{(l)}(v)$ in node classification task, and $\Phi(G_{i})$
    in graph classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Popular Variants of GKNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We summarize the popular variants of GKNNs and compare their differences in
    Table [2](#S4.T2 "Table 2 ‣ 4.3\. Popular Variants of GKNNs ‣ 4\. Graph Kernel
    Neural Networks By Qingqing Long ‣ A Comprehensive Survey on Deep Graph Representation
    Learning"). Specifically, we conclude their basic graph kernels, whether designed
    for heterogeneous graphs, and experimental datasets, etc. As the original designed
    graph-kernel based GNNs have high complexity, they usually by acceleration strategies,
    such as sampling strategies, simplification and approximation, etc. In this section,
    We select four typical and popular GKNNs to introduce their well-designed graph
    kernels and corresponding GNN frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Summary of popular GKNNs.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Type | Related GK | Adaptive | Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| $k$-GNN  (Morris et al., [2019](#bib.bib265)) | Isomorphic | WL Subtree |
    ✓ | Biochemical Network |'
  prefs: []
  type: TYPE_TB
- en: '| RetGK (Zhang et al., [2018c](#bib.bib450)) | Isomorphic | Random Walk | ✓
    | Biochemical Network, Social Network |'
  prefs: []
  type: TYPE_TB
- en: '| GNTK (Du et al., [2019](#bib.bib78)) | Isomorphic | Neural Tangent Kernel
    |  | Biochemical Network, Social Network |'
  prefs: []
  type: TYPE_TB
- en: '| DDGK (Al-Rfou et al., [2019](#bib.bib5)) | Isomorphic | Random Walk |  |
    Biochemical Network |'
  prefs: []
  type: TYPE_TB
- en: '| GCKN (Chen et al., [2020c](#bib.bib45)) | Isomorphic | Random Walk | ✓ |
    Biochemical Network, Social Network |'
  prefs: []
  type: TYPE_TB
- en: '| GSKN (Long et al., [2021a](#bib.bib238)) | Isomorphic | Random Walk, Anonymous
    Walk | ✓ | Biochemical Network, Social Network |'
  prefs: []
  type: TYPE_TB
- en: '| GCN-LASE (Li et al., [2019c](#bib.bib223)) | Heterogeneous | Random Walk
    | - | Social Network Academic Network |'
  prefs: []
  type: TYPE_TB
- en: '| HGK-GNN (Long et al., [2021b](#bib.bib239)) | Heterogeneous | Random Walk
    | ✓ | Social Network Academic Network |'
  prefs: []
  type: TYPE_TB
- en: $k$-dimensional Graph Neural Networks (Morris et al., [2019](#bib.bib265)) ($k$-GNN)
    is the pioneer of GKNNs, it incorporates the WL-subtree graph kernel and graph
    neural networks. For better scalability, the paper considers a set-based version
    of the $k$-WL. Let $h^{(l)}_{k}(s)$ and $h^{(l)}_{k,L}(s)$ denote the global and
    local hidden representation for node $s$ in the $l$-th layer respectively. $k$-GNN
    defined the end-to-end hierarchical trainable framework as,
  prefs: []
  type: TYPE_NORMAL
- en: '| (24) |  | $\displaystyle h^{(0)}_{k}(s)=$ | $\displaystyle\sigma\left(\left[h^{iso}(s),\sum_{u\in
    s}h^{(T_{k}-1)}(u)\right]\cdot\boldsymbol{W}^{(0)}_{k-1}\right),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{(l)}_{k}(s)=$ | $\displaystyle\sigma\left(h^{(l-1)}_{k}\cdot\boldsymbol{W}_{1}^{(l)}+\sum_{u\in
    N_{L}(s)\cup N_{G}(s)}h^{(l-1)}_{k}(u)\cdot\boldsymbol{W}_{2}^{(l)}\right),\qquad
    1<l\leq L,$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{(l)}_{k,L}(s)=$ | $\displaystyle\sigma\left(h^{(l-1)}_{k,L}(s)\cdot\boldsymbol{W}_{1}^{(l)}+\sum_{u\in
    N_{L}(s)}h^{(l-1)}_{k,L}(u)\cdot\boldsymbol{W}_{2}^{(l)}\right),\qquad 1<l\leq
    L.$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $h^{iso}(s)$ is the one-hot encoding of the isomorphism type of $G[s]$,
    $N_{L}(s)$ is the local neighborhood, $N_{G}(s)$ is the global neighborhood, $h^{(l)}_{k,L}(s)$
    is designed for better scalability and running speed, and $h^{(l)}_{k}(s)$ has
    better performance due to its larger neighbor sets.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Convolutional Kernel Networks (Chen et al., [2020c](#bib.bib45)) (GCKN).
    GCKN is a representative random walk and path-based GKNN. It The Gaussian kernel
    $k$ can be written as,
  prefs: []
  type: TYPE_NORMAL
- en: '| (25) |  | $k(z_{1},z_{2})=e^{-\frac{\alpha_{1}}{2}&#124;&#124;z_{1}-z_{2}&#124;&#124;^{2}}=e^{\alpha(z_{1}^{T}z_{2}-k-1)}=\sigma(z_{1}^{T}z_{2}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: then the GNN architecture can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '| (26) |  | $\displaystyle h^{(l)}(u)=$ | $\displaystyle\sum_{z\in}K(z_{1},z_{2})\cdot\sigma\left(Z^{T}h^{(l-1)}(p)\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sigma\left(ZZ^{T}\right)^{-\frac{1}{2}}\cdot\sum_{p\in\mathcal{P}_{k}(G,u)}\sigma\left(Z^{T}h^{(l-1)}(p)\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $Z$ is the matrix of prototype path attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the paper analysis the relationship between GCKN and the WL-subtree
    based $k$-GNN. The Theorem 1 in paper(Chen et al., [2020c](#bib.bib45)) shows
    that WL-subtree based GKNNs can be seen as a special case in GCKN.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Neural Tangent Kernel (Du et al., [2019](#bib.bib78)) (GNTK). Different
    from the above two works, GNTK proposed a new class of graph kernels. GNTK is
    a general recipe which translates a GNN architecture to its corresponding GNTK.
  prefs: []
  type: TYPE_NORMAL
- en: The neighborhood aggregation operation in GNTK is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '| (27) |  | $\displaystyle\left[\sum_{(0)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle c_{u}c_{u^{{}^{\prime}}}\sum_{u\in N(v)\cup\{u\}}\sum_{v^{{}^{\prime}}\in
    N(u^{{}^{\prime}})\cup\{u^{{}^{\prime}}\}}\left[\sum_{R}^{(l-1)}(G,G^{{}^{\prime}})\right]_{vv^{{}^{\prime}}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\left[\Theta_{(0)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle c_{u}c_{u^{{}^{\prime}}}\sum_{u\in N(v)\cup\{u\}}\sum_{v^{{}^{\prime}}\in
    N(u^{{}^{\prime}})\cup\{u^{{}^{\prime}}\}}\left[\Theta_{R}^{(l-1)}(G,G^{{}^{\prime}})\right]_{vv^{{}^{\prime}}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\sum_{R}^{(0)}(G,G^{{}^{\prime}})$ and $\Theta_{R}^{(0)}(G,G^{{}^{\prime}})$
    are defined as $\sum^{(0)}(G,G^{{}^{\prime}})$. Then GNTK performed $R$ transformations
    to xx.
  prefs: []
  type: TYPE_NORMAL
- en: '| (28) |  | $\left[A_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=\left(\begin{aligned}
    &amp;\left[\sum_{(r-1)}^{(l)}(G,G)\right]_{uu^{{}^{\prime}}}&amp;,\ &amp;\left[\sum_{(r-1)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}\\
    &amp;\left[\sum_{(r-1)}^{(l)}(G^{{}^{\prime}},G)\right]_{uu^{{}^{\prime}}}&amp;,\
    &amp;\left[\sum_{(r-1)}^{(l)}(G^{{}^{\prime}},G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}\end{aligned}\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (29) |  | $\displaystyle\left[\sum_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle\mathbb{E}_{(a,b)\sim\mathcal{N}(0,[A_{(r)}^{(l)}(G,G^{{}^{\prime}})]_{uu^{{}^{\prime}}})}[\sigma(a)\cdot\sigma(b)],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\left[\sum_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=$
    | $\displaystyle\mathbb{E}_{(a,b)\sim\mathcal{N}(0,[A_{(r)}^{(l)}(G,G^{{}^{\prime}})]_{uu^{{}^{\prime}}})}[\sigma(a)\cdot\sigma(b)],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: then the $r$ order can be calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: '| (30) |  | $\left[\Theta_{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}=\left[\Theta_{(r-1)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}\left[\sum{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}+\left[\sum{(r)}^{(l)}(G,G^{{}^{\prime}})\right]_{uu^{{}^{\prime}}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Finally, GNTK calculates the final output as
  prefs: []
  type: TYPE_NORMAL
- en: '| (31) |  | $\Theta(G,G^{{}^{\prime}})=\sum_{u\in V,u^{{}^{\prime}}\in V^{{}^{\prime}}}\left[\sum_{l=0}^{L}\Theta_{(R)}^{(l)}(G,G^{{}^{\prime}})\right]_{u,u^{{}^{\prime}}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Heterogeneous Graph Kernel based Graph Neural Network (Long et al., [2021b](#bib.bib239))
    (HGK-GNN). HGK-GNN first proposed GKNN for heterogeneous graphs. It adopted $\langle
    f(u_{1}),f(u_{2})\rangle_{M}$ as graph kernel based on the Mahalanobis Distance
    to build connections among heterogeneous nodes and edges,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\langle f(u_{1}),f(u_{2})\rangle_{M_{1}}=f(u_{1})^{T}\boldsymbol{M}_{1}f(u_{2}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\langle f(e_{\cdot,u_{1}}),f(e_{\cdot,u_{2}})\rangle_{M_{2}}=f(e_{\cdot,u_{1}})^{T}\boldsymbol{M}_{2}f(e_{\cdot,u_{2}}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Following the route introduced in (Lei et al., [2017](#bib.bib203)) , the corresponding
    neural network architecture of heterogeneous graph kernel can be derived as
  prefs: []
  type: TYPE_NORMAL
- en: '| (32) |  | $\displaystyle h^{(0)}(v)=$ | $\displaystyle\boldsymbol{W}^{(0)}_{t_{V}(v)}f(v),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{(l)}(v)=$ | $\displaystyle\boldsymbol{W}^{(l)}_{t_{V}(v)}f(v)\odot\sum_{u\in
    N(v)}(\boldsymbol{U}_{t_{V}(v)}^{(l)}h^{(l-1)}(u)\odot\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}f(e_{u,v})),\qquad
    1<l\leq L,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $h^{(l)}(v)$ is the cell state vector of node v, and $\boldsymbol{W}^{(l)}_{t_{V}(v)}$,
    $\boldsymbol{U}_{t_{V}(v)}^{(l)}$ $\boldsymbol{U}_{t_{E}(e_{u,v})}^{(l)}$ are
    learnable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section introduces graph kernel neural networks. We provide the summary
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques. Graph kernel neural networks (GKNNs) are a recent popular research
    area that combines the advantages of graph kernels and GNNs to learn more effective
    graph representations. Researchers have studied GKNNs in various aspects, such
    as theoretical foundations, algorithmic design, and practical applications. As
    a result, a wide range of GKNN-based models and methods have been developed for
    graph analysis and representation tasks, including node classification, link prediction,
    and graph clustering.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and Limitations. Although GKNNs have shown great potential in graph-related
    tasks, they also have several limitations that need to be addressed. Scalability
    is a significant challenge, particularly when dealing with large-scale graphs
    and networks. As the size of the graph increases, the computational cost of GKNNs
    grows exponentially, which can limit their ability to handle large and complex
    real-world applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Works. For future works, we expect the GKNNs can integrate more domain-specific
    knowledge into the designed kernels. Domain-specific knowledge has been shown
    to significantly improve the performance of many applications, such as drug discovery,
    knowledge graph-based information retrieval systems, and molecular analysis (Wang
    et al., [2019e](#bib.bib361); Feinberg et al., [2018](#bib.bib91)). Incorporating
    domain-specific knowledge into GKNNs can enhance their ability to handle complex
    and diverse data structures, leading to more accurate and interpretable models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5\. Graph Pooling By Yiyang Gu
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When it comes to graph-level tasks, such as graph classification and graph
    regression, graph pooling is an essential component for generating the whole graph
    representation from the learned node embeddings. To ensure isomorphic graphs have
    the same representation, the graph pooling operations should be invariant to the
    permutations of nodes. In this section, we give a systematic review of existing
    graph pooling algorithms and generally classify them into two categories: global
    pooling algorithms and hierarchical pooling algorithms. The global pooling algorithms
    aggregate the node embeddings to the final graph representation directly, while
    the hierarchical pooling algorithms reduce the graph size and generate the immediate
    representations gradually to capture the hierarchical structure and characteristics
    of the input graph. A summary is provided in Table [3](#S5.T3 "Table 3 ‣ 5.1\.
    Global Pooling ‣ 5\. Graph Pooling By Yiyang Gu ‣ A Comprehensive Survey on Deep
    Graph Representation Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Global Pooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Global pooling operations generate a holistic graph representation from the
    learned node embeddings in one step, which are referred to as readout functions
    in some literature (Xu et al., [2018a](#bib.bib404); Corso et al., [2020](#bib.bib65))
    as well. Several simple permutation-invariant operations, such as mean, sum, and
    max, are widely employed on the node embeddings to output the graph-level representation.
    To enhance the adaptiveness of global pooling operators, GGS-NN (Li et al., [2016](#bib.bib222))
    introduces a soft attention mechanism to evaluate the importance of nodes for
    a particular task and then takes a weighted sum of the node embeddings. SortPool
    (Zhang et al., [2018a](#bib.bib438)) exploits Weisfeiler-Lehman methods (Weisfeiler
    and Leman, [1968](#bib.bib379)) to sort nodes based on their structural positions
    in the graph topology, and produces the graph representation from the sorted node
    embeddings by traditional convolutional neural networks. Recently, Lee et al.
    propose a structural-semantic pooling method, SSRead (Lee et al., [2021](#bib.bib200)),
    which first aligns nodes and learnable structural prototypes semantically, and
    then aggregates node representations in groups based on matching structural prototypes.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Summary of graph pooling methods.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Type | TopK-based | Cluster-based | Attention Mechanism |'
  prefs: []
  type: TYPE_TB
- en: '| Mean/Sum/Max | Global |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Pooling |'
  prefs: []
  type: TYPE_TB
- en: '| GGS-NN (Li et al., [2016](#bib.bib222)) | Global |  |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| SortPool (Zhang et al., [2018a](#bib.bib438)) | Global |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SSRead (Lee et al., [2021](#bib.bib200)) | Global |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| gPool (Gao and Ji, [2019](#bib.bib105)) | Hierarchical | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SAGPool (Lee et al., [2019](#bib.bib202)) | Hierarchical | ✓ |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| HGP-SL (Zhang et al., [2019a](#bib.bib446)) | Hierarchical | ✓ |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| TAPool (Gao et al., [2021](#bib.bib106)) | Hierarchical | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| DiffPool (Ying et al., [2018](#bib.bib418)) | Hierarchical |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| MinCutPool (Bianchi et al., [2020](#bib.bib23)) | Hierarchical |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| SEP (Wu et al., [2022](#bib.bib385)) | Hierarchical |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| ASAP (Ranjan et al., [2020](#bib.bib293)) | Hierarchical | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| MuchPool (Du et al., [2021](#bib.bib77)) | Hierarchical | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: 5.2\. Hierarchical Pooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different from global pooling methods, hierarchical pooling methods coarsen
    the graph gradually, to preserve the structural information of the graph better.
    To adaptively coarse the graph and learn optimal hierarchical representations
    according to a particular task, many learnable hierarchical pooling operators
    are proposed in the past few years, which can be integrated with multifarious
    graph convolution layers. There are two common approaches to coarsening the graph,
    one is selecting important nodes and dropping the others by TopK selection, and
    the other one is merging nodes and generating the coarsened graph by clustering
    methods. We call the former TopK-based pooling, and the latter cluster-based pooling
    in this survey. In addition, some works combine these two types of pooling methods,
    which will be reviewed in the hybrid pooling section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. TopK-based Pooling.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Typically, TopK-based pooling methods first learn a scoring function to evaluate
    the importance of nodes of the original graph. Based on importance score $\mathbf{Z}\in\mathbb{R}^{|V|\times
    1}$ generated, they select the top $K$ nodes out of all nodes,
  prefs: []
  type: TYPE_NORMAL
- en: '| (33) |  | $idx=\operatorname{TOP}_{k}\left(\mathbf{Z}\right),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $idx$ denotes the index of the top $K$ nodes. Based on these selected
    nodes, most methods directly employ the induced subgraph as the pooled graph,
  prefs: []
  type: TYPE_NORMAL
- en: '| (34) |  | $\mathbf{A}_{\text{pool}}=\mathbf{A}_{idx,idx},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $A_{idx,idx}$ denotes the adjacency matrix indexed by the selected rows
    and columns. Moreover, to make the scoring function learnable, they further use
    score $Z$ as a gate for the selected node features,
  prefs: []
  type: TYPE_NORMAL
- en: '| (35) |  | $\mathbf{X}_{\text{pool}}=\mathbf{X}_{idx,:}\odot\mathbf{Z}_{idx},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $X_{idx,:}$ denotes the feature matrix indexed by the selected nodes,
    and $\odot$ denotes the broadcasted element-wise product. With the help of the
    gate mechanism, the scoring function can be trained by back-propagation, to adaptively
    evaluate the importance of nodes according to a certain task. Several representative
    TopK-based pooling methods are reviewed detailly in the following.
  prefs: []
  type: TYPE_NORMAL
- en: gPool (Gao and Ji, [2019](#bib.bib105)). gPool is one of the first works to
    select the most important node subset from the original graph to construct the
    coarsened graph by Top K operation. The key idea of gPool is to evaluate the importance
    of all nodes by learning a projection vector $\mathbf{p}$, which projects node
    features to a scalar score,
  prefs: []
  type: TYPE_NORMAL
- en: '| (36) |  | $\mathbf{Z}_{j}=\mathbf{X}_{j,:}\mathbf{p}/\&#124;\mathbf{p}\&#124;,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and then select nodes with K-highest scores to form the pooled graph.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention Graph Pooling (SAGPool)  (Lee et al., [2019](#bib.bib202)). Unlike
    gPool (Gao and Ji, [2019](#bib.bib105)), which only uses node features to generate
    projection scores, SAGPool captures both graph topology and node features to obtain
    self-attention scores by graph convolution. The various formulas of graph convolution
    can be employed to compute the self-attention score $\mathbf{Z}$,
  prefs: []
  type: TYPE_NORMAL
- en: '| (37) |  | $\mathbf{Z}=\sigma(\operatorname{GNN}(\mathbf{X},\mathbf{A})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\sigma$ denotes the activation function, and $\operatorname{GNN}$ denotes
    various graph convolutional layers or stacks of them, whose output dimension is
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Graph Pooling with Structure Learning (HGP-SL)  (Zhang et al.,
    [2019a](#bib.bib446)) . HGP-SL evaluates the importance score of a node according
    to the information it contains given its neighbors. It supposes that a node which
    can be easily represented by its neighborhood carries relatively little information.
    Specifically, the importance score can be defined by the Manhattan distance between
    the original node representation and the reconstructed one aggregated from its
    neighbors’ representation,
  prefs: []
  type: TYPE_NORMAL
- en: '| (38) |  | $\mathbf{Z}=\left\&#124;\left(\mathbf{I}-\mathbf{D}^{-1}\mathbf{A}\right)\mathbf{X}\right\&#124;_{1},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{I}$ denotes the identity matrix, $\mathbf{D}$ denotes the diagonal
    degree matrix of $\mathbf{A}$, and $\|\cdot\|_{1}$ means $\ell_{1}$ norm. Furthermore,
    to reduce the loss of topology information, HGP-SL leverages structure learning
    to learn a refined graph topology for the reserved nodes. Specifically, it utilizes
    the attention mechanism to compute the similarity of two nodes as the weight of
    the reconstructed edge,
  prefs: []
  type: TYPE_NORMAL
- en: '| (39) |  | $\widetilde{\boldsymbol{A}}^{\text{pool}}_{ij}=\operatorname{sparsemax}\left(\sigma\left(\mathbf{w}\left[\boldsymbol{X}^{\text{pool}}_{i,:}\&#124;\boldsymbol{X}^{\text{pool}}_{j,:}\right]^{\top}\right)+\lambda\cdot\boldsymbol{A}^{\text{pool}}_{ij}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\widetilde{\boldsymbol{A}}^{\text{pool}}$ denotes the refined adjacency
    matrix of the pooled graph, $\operatorname{sparsemax}(\cdot)$ truncates the values
    below a threshold to zeros, $\mathbf{w}$ denotes a learnable weight vector, and
    $\lambda$ is a weight parameter between the original edges and the reconstructed
    edges. These reconstructed edges may capture the underlying relationship between
    nodes disconnected due to node dropping.
  prefs: []
  type: TYPE_NORMAL
- en: Topology-Aware Graph Pooling (TAPool)  (Gao et al., [2021](#bib.bib106)). TAPool
    takes both the local and global significance of a node into account. On the one
    hand, it utilizes the average similarity between a node and its neighbors to evaluate
    its local importance,
  prefs: []
  type: TYPE_NORMAL
- en: '| (40) |  | $\hat{\mathbf{R}}=\left(\mathbf{X}\mathbf{X}^{T}\right)\odot\left(\mathbf{D}^{-1}\mathbf{A}\right),\mathbf{Z}^{l}=\operatorname{softmax}\left(\frac{1}{n}\hat{\mathbf{R}}\mathbf{1}_{n}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\hat{\mathbf{R}}$ denotes the localized similarity matrix, and $\mathbf{Z}^{l}$
    denotes the local importance score. On the other hand, it measures the global
    importance of a node according to the significance of its one-hop neighborhood
    in the whole graph,
  prefs: []
  type: TYPE_NORMAL
- en: '| (41) |  | $\hat{\mathbf{X}}=\mathbf{D}^{-1}\mathbf{A}\mathbf{X},\mathbf{Z}^{g}=\operatorname{softmax}\left(\hat{\mathbf{X}}\mathbf{p}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{p}$ is a learnable and globally shared projector vector, similar
    to the aforementioned gPool (Gao and Ji, [2019](#bib.bib105)). However, $\hat{\mathbf{X}}$
    here further aggregates the features from the neighborhood, which enables the
    global importance score $\mathbf{Z}^{g}$ to capture more topology information
    such as salient subgraphs. Moreover, TAPool encourages connectivity in the coarsened
    graph with the help of a degree-based connectivity term, then obtaining the final
    importance score $\mathbf{Z}=\mathbf{Z}^{l}+\mathbf{Z}^{g}+\lambda\mathbf{D}/|V|$,
    where $\lambda$ is a trade-off hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2\. Cluster-based Pooling.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pooling the graph by clustering and merging nodes is the main concept behind
    cluster-based pooling methods. Typically, they allocate nodes to a collection
    of clusters by learning a cluster assignment matrix $\mathbf{S}\in\mathbb{R}^{|V|\times
    K}$, where $K$ is the number of the clusters. After that, they merge the nodes
    within each cluster to generate a new node in the pooled graph. The feature (embedding)
    matrix of the new nodes can be obtained by aggregating the features (embeddings)
    of nodes within the clusters, according to the cluster assignment matrix,
  prefs: []
  type: TYPE_NORMAL
- en: '| (42) |  | $\mathbf{X}^{\text{pool}}=\mathbf{S}^{T}\mathbf{X}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: While the adjacency matrix of the pooled graph can be generated by calculating
    the connectivity strength between each pair of clusters,
  prefs: []
  type: TYPE_NORMAL
- en: '| (43) |  | $\mathbf{A}^{\text{pool}}=\mathbf{S}^{T}\mathbf{A}\mathbf{S}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Then, we review several representative cluster-based pooling methods in detail.
  prefs: []
  type: TYPE_NORMAL
- en: DiffPool (Ying et al., [2018](#bib.bib418)). DiffPool is one of the first and
    classic works to hierarchically pool the graph by graph clustering. Specifically,
    it uses an embedding GNN to generate embeddings of nodes, and a pooling GNN to
    generate the cluster assignment matrix,
  prefs: []
  type: TYPE_NORMAL
- en: '| (44) |  | $\hat{\mathbf{X}}=\operatorname{GNN}_{\text{embed}}\left(\mathbf{X},\mathbf{A}\right),\mathbf{S}=\operatorname{softmax}\left(\operatorname{GNN}_{\text{pool}}\left(\mathbf{X},\mathbf{A}\right)\right),\mathbf{X}^{\text{pool}}=\mathbf{S}^{T}\hat{\mathbf{X}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Besides, DiffPool leverages an auxiliary link prediction objective $L_{\mathrm{LP}}=\left\|\mathbf{A},\mathbf{S}\mathbf{S}^{T}\right\|_{F}$
    to encourage the adjacent nodes to be in the same cluster and avoid fake local
    minima, where $\|\cdot\|_{F}$ is the Frobenius norm. And it utilizes an entropy
    regularization term $L_{\mathrm{E}}=\frac{1}{|V|}\sum_{i=1}^{|V|}H\left(\mathbf{S}_{i}\right)$
    to impel clear cluster assignments, where $H(\cdot)$ represents the entropy.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Pooling with Spectral Clustering (MinCutPool)  (Bianchi et al., [2020](#bib.bib23)).
    MinCutPool takes advantage of the properties of spectral clustering (SC) to provide
    a better inductive bias and avoid degenerate cluster assignments. It learns to
    cluster like SC by optimizing the MinCut loss,
  prefs: []
  type: TYPE_NORMAL
- en: '| (45) |  | $L_{c}=-\frac{\operatorname{Tr}\left(\mathbf{S}^{T}{\mathbf{A}}\mathbf{S}\right)}{\operatorname{Tr}\left(\mathbf{S}^{T}{\mathbf{D}}\mathbf{S}\right)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In addition, it utilizes an orthogonality loss $L_{o}=\left\|\frac{\mathbf{S}^{T}\mathbf{S}}{\left\|\mathbf{S}^{T}\mathbf{S}\right\|_{F}}-\frac{\mathbf{I}_{K}}{\sqrt{K}}\right\|_{F}$
    to encourage orthogonal and uniform cluster assignments, and prevent the bad minima
    of $L_{c}$, where $K$ is the number of the clusters. When performing a specific
    task, it can optimize the weighted sum of the unsupervised loss $L_{u}=L_{c}+L_{o}$
    and a task-specific loss to find the optimal balance between the theoretical prior
    and the task objective.
  prefs: []
  type: TYPE_NORMAL
- en: Structural Entropy Guided Graph Pooling (SEP)  (Wu et al., [2022](#bib.bib385)).
    In order to lessen the local structural harm and suboptimal performance caused
    by separate pooling layers and predesigned pooling ratios, SEP leverages the concept
    of structural entropy to generate the global and hierarchical cluster assignments
    at once. Specifically, SEP treats the nodes of a given graph as the leaf nodes
    of a coding tree and exploits the hierarchical layers of the coding tree to capture
    the hierarchical structure of the graph. The optimal code tree $T$ can be obtained
    by minimizing the structural entropy (Li and Pan, [2016](#bib.bib205)),
  prefs: []
  type: TYPE_NORMAL
- en: '| (46) |  | $\mathcal{H}^{T}(G)=-\sum_{v_{i}\in T}\frac{g(P_{v_{i}})}{\operatorname{vol}(V)}\log\frac{\operatorname{vol}\left(P_{v_{i}}\right)}{\operatorname{vol}\left(P_{v_{i}^{+}}\right)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $v_{i}^{+}$ represents the father node of node $v_{i}$, $P_{v_{i}}$ denotes
    the partition of leaf nodes which are descendants of $v_{i}$ in the coding tree
    $T$, $g(P_{v_{i}})$ denotes the number of edges that have a terminal in the $P_{v_{i}}$,
    and $\operatorname{vol}(\cdot)$ denotes the total degrees of leaf nodes in the
    given partition. Then, the cluster assignment matrix for each pooling layer can
    be derived from the edges of each layer in the coding tree. With the help of the
    one-step joint assignments generation based on structural entropy, it can not
    only make the best use of the hierarchical relationships of pooling layers, but
    also reduce the structural noise in the original graph.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3\. Hybrid pooling.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hybrid pooling methods combine TopK-based pooling methods and cluster-based
    pooling methods, to exert the advantages of the two methods and overcome their
    respective limitations. Here, we review two representative hybrid pooling methods,
    Adaptive Structure Aware Pooling  (Ranjan et al., [2020](#bib.bib293)) and Multi-channel
    Pooling  (Du et al., [2021](#bib.bib77)).
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive Structure Aware Pooling (ASAP)  (Ranjan et al., [2020](#bib.bib293)).
    Considering that TopK-based pooling methods are not good at capturing the connectivity
    of the coarsened graph, while cluster-based pooling methods fail to be employed
    for large graphs because of the dense assignment matrix, ASAP organically combines
    the two types of pooling methods to overcome the above limitations. Specifically,
    it regards the $h$-hop ego-network $c_{h}(v_{i})$ of each node $v_{i}$ as a cluster.
    Such local clustering enables the cluster assignment matrix to be sparse. Then,
    a new self-attention mechanism Master2Token is used to learn the cluster assignment
    matrix $\mathbf{S}$ and the cluster representations,
  prefs: []
  type: TYPE_NORMAL
- en: '| (47) |  | $\mathbf{m}_{i}=\max_{v_{j}\in c_{h}\left(v_{i}\right)}\left(\mathbf{X}_{j}^{\prime}\right),\mathbf{S}_{j,i}=\operatorname{softmax}\left(\mathbf{w}^{T}\sigma\left(\mathbf{W}\mathbf{m}_{i}\&#124;\mathbf{X}_{j}^{\prime}\right)\right),\mathbf{X}_{i}^{c}=\sum_{j=1}^{\left&#124;c_{h}\left(v_{i}\right)\right&#124;}\mathbf{S}_{j,i}\mathbf{X}_{j},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{X}^{\prime}$ is the node embedding matrix after passing GCN,
    $\mathbf{w}$ and $\mathbf{W}$ denote the trainable vector and matrix respectively,
    and $\mathbf{X}_{i}^{c}$ denotes the representation of the cluster $c_{h}(v_{i})$.
    Next, it utilizes the graph convolution and TopK selection to choose the top $K$
    clusters, whose centers are treated as the nodes of the pooled graph. The adjacency
    matrix of the pooled graph can be calculated like common cluster-based pooling
    methods ([43](#S5.E43 "In 5.2.2\. Cluster-based Pooling. ‣ 5.2\. Hierarchical
    Pooling ‣ 5\. Graph Pooling By Yiyang Gu ‣ A Comprehensive Survey on Deep Graph
    Representation Learning")), preserving the connectivity of the original graph
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-channel Pooling (MuchPool)  (Du et al., [2021](#bib.bib77)). The key idea
    of MuchPool is to capture both the local and global structure of a given graph
    by combining the TopK-based pooling methods and the cluster-based pooling methods.
    MuchPool has two pooling channels based on TopK selection to yield two fine-grained
    pooled graphs, whose selection criteria are node degrees and projected scores
    of node features respectively, so that both the local topology and the node features
    are considered. Besides, it leverages a channel based on graph clustering to obtain
    a coarse-grained pooled graph, which captures the global and hierarchical structure
    of the input graph. To better integrate the information of different channels,
    a cross-channel convolution is proposed, which fuses the node embeddings of the
    fine-grained pooled graph $\mathbf{X}^{\text{fine}}$ and the coarse-grained pooled
    graph $\mathbf{X}^{\text{coarse}}$ with the help of the cluster assignments $\mathbf{S}$
    of the cluster-based pooling channel,
  prefs: []
  type: TYPE_NORMAL
- en: '| (48) |  | $\widetilde{\mathbf{X}}^{\text{fine}}=\sigma\left(\left[\mathbf{X}^{\text{fine}}+\mathbf{S}\mathbf{X}^{\text{coarse}}\right]\cdot\mathbf{W}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{W}$ denotes the learnable weights. Finally, it merges the node
    embeddings and the adjacency matrices of the two fine-grained pooled graphs to
    obtain the eventually pooled graph.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section introduces graph pooling methods for graph-level representation
    learning. We provide the summary as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Techniques. Graph pooling methods play a vital role in generating an entire
    graph representation by aggregating node embeddings. There are mainly two categories
    of graph pooling methods: global pooling methods and hierarchical pooling methods.
    While global pooling methods directly aggregate node embeddings in one step, hierarchical
    pooling methods gradually coarsen a graph to capture hierarchical structure characteristics
    of the graph based on TopK selection, clustering methods, or hybrid methods.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Challenges and Limitations. Despite the great success of graph pooling methods
    for learning the whole graph representation, there remain several challenges and
    limitations unsolved: 1) For hierarchical pooling, most cluster-based methods
    involve the dense assignment matrix, which limits their application to large graphs,
    while TopK-based methods are not good at capturing structure information of the
    graph and may lose information due to node dropping. 2) Most graph pooling methods
    are designed for simple attributed graphs, while pooling algorithms tailored to
    other types of graphs, like dynamic graphs and heterogeneous graphs, are largely
    under-explored.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Works. In the future, we expect that more hybrid or other pooling methods
    can be studied to capture the graph structure information sufficiently as well
    as be efficient for large graphs. In realistic scenarios, there are various types
    of graphs involving dynamic, heterogeneous, or spatial-temporal information. It
    is promising to design graph pooling methods specifically for these graphs, which
    can be beneficial to more real-world applications, such as traffic analysis and
    recommendation systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6\. Graph Transformer By Junwei Yang
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Though GNNs based on the message-passing paradigm have achieved impressive performance
    on multiple well-known tasks (Gilmer et al., [2017](#bib.bib116); Xu et al., [2018a](#bib.bib404);
    Wang et al., [2019c](#bib.bib364); Li et al., [2020](#bib.bib206)), they still
    face some intrinsic problems due to the iterative neighbor-aggregation operation.
    Many previous works have demonstrated the two major defects of message-passing
    GNNs, which are known as the over-smoothing and long-distance modeling problem.
    And there are lots of explanatory works trying to mine insights from these two
    issues. The over-smoothing problem can be explained in terms of various GNNs focusing
    only on low-frequency information (Bo et al., [2021](#bib.bib24)), mixing information
    between different kinds of nodes destroying model performance (Chen et al., [2020d](#bib.bib46)),
    GCN is equivalent to Laplacian smoothing (Li et al., [2018a](#bib.bib214)), isotropic
    aggregation among neighbors leading to the same influence distribution as random
    walk (Xu et al., [2018b](#bib.bib405)), etc. The inability to model long-distance
    dependencies of GNNs is partially due to the over-smoothing problem, because in
    the context of conventional neighbor-aggregation GNNs, node information can be
    passed over long distances only through multiple GNN layers. Recently, Alon et
    al. (Alon and Yahav, [2020](#bib.bib7)) find that this problem may also be caused
    by over-squashing, which means the exponential growth of computation paths with
    increasing distance. Though the two basic performance bottlenecks can be tackled
    with elaborate message passing and aggregation strategies, representational power
    of GNNs is inherently bounded by the Weisfeiler-Lehman isomorphism hierarchy (Morris
    et al., [2019](#bib.bib265)). Worse still, most GNNs (Kipf and Welling, [2016a](#bib.bib183);
    Veličković et al., [2017](#bib.bib352); Gilmer et al., [2017](#bib.bib116)) are
    bounded by the simplest first-order Weisfeiler-Lehman test (1-WL). Some efforts
    have been dedicated to break this limitation, such as hypergraph-based (Feng et al.,
    [2019](#bib.bib94); Huang and Yang, [2021](#bib.bib150)), path-based (Cai and
    Lam, [2020](#bib.bib32); Ying et al., [2021](#bib.bib416)), and k-WL-based (Balcilar
    et al., [2021](#bib.bib16); Morris et al., [2019](#bib.bib265)) approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Among many attempts to solve these fundamental problems, an essential one is
    the adaptation of Transformer (Vaswani et al., [2017](#bib.bib351)) for graph
    representation learning. Transformers, both the vanilla version and several variants,
    have been adopted with impressive results in various deep learning fields including
    NLP (Vaswani et al., [2017](#bib.bib351); Devlin et al., [2018](#bib.bib71)),
    CV (Carion et al., [2020](#bib.bib39); Zhu et al., [2020a](#bib.bib470)), etc.
    Recently, Transformer also shows powerful graph modeling abilities in many researches
    (Dwivedi and Bresson, [2020](#bib.bib82); Kreuzer et al., [2021](#bib.bib194);
    Ying et al., [2021](#bib.bib416); Wu et al., [2021](#bib.bib387); Chen et al.,
    [2022a](#bib.bib47)). And extensive empirical results show that some chronic shortcomings
    in conventional GNNs can be easily overcome in Transformer-based approaches. This
    section gives an overview of the current progress on this kind of methods.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformer (Vaswani et al., [2017](#bib.bib351)) was firstly applied to model
    machine translation, but two of the key mechanisms adopted in this work, attention
    operation and positional encoding, are highly compatible with the graph modeling
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be specific, we denote the input of attention layer in Transformer as $\mathbf{X}=[\mathbf{x}_{0},\mathbf{x}_{1},\ldots,\mathbf{x}_{n-1}]$,
    $\mathbf{x}_{i}\in\mathbb{R}^{d}$, where $n$ is the length of input sequence and
    $d$ is the dimension of each input embedding $\mathbf{x}_{i}$. Then the core operation
    of calculating new embedding $\hat{\mathbf{x}}_{i}$ for each $\mathbf{x}_{i}$
    in attention layer can be streamlined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (49) |  | <math  class="ltx_Math" alttext="\begin{gathered}\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{NORM}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\mathcal{Q}^{h}(\mathbf{x}_{i})^{\mathrm{T}}\mathcal{K}^{h}(\mathbf{x}_{k})),\\
    \mathbf{x}_{i}^{h}\ =\mathop{\sum}_{\mathbf{x}_{j}\in\mathbf{X}}{\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})\mathcal{V}^{h}(\mathbf{x}_{j}}),\\'
  prefs: []
  type: TYPE_NORMAL
- en: \hat{\mathbf{x}}_{i}=\text{MERGE}(\mathbf{x}_{i}^{1},\mathbf{x}_{i}^{2},\ldots,\mathbf{x}_{i}^{H}),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd ><mrow
    ><mrow ><mrow
    ><msup ><mi
    mathvariant="normal"  >s</mi><mi
     >h</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub
    ><mi  >𝐱</mi><mi
     >i</mi></msub><mo
    >,</mo><msub ><mi
     >𝐱</mi><mi 
    >j</mi></msub><mo stretchy="false" >)</mo></mrow></mrow><mo
     >=</mo><mrow
    ><msub ><mtext
     >NORM</mtext><mi
     >j</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><munder ><mo
    lspace="0em" movablelimits="false" rspace="0.167em" 
    >∥</mo><mrow 
    ><msub 
    ><mi 
    >𝐱</mi><mi 
    >k</mi></msub><mo 
    >∈</mo><mi 
    >𝐗</mi></mrow></munder><mrow ><msup
    ><mi class="ltx_font_mathcaligraphic"
     >𝒬</mi><mi
     >h</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><msup
    ><mrow ><mo
    stretchy="false" >(</mo><msub ><mi
     >𝐱</mi><mi
     >i</mi></msub><mo
    stretchy="false" >)</mo></mrow><mi mathvariant="normal"
     >T</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><msup
    ><mi class="ltx_font_mathcaligraphic"
     >𝒦</mi><mi
     >h</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub
    ><mi 
    >𝐱</mi><mi 
    >k</mi></msub><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd
    ><mrow ><mrow ><msubsup
    ><mi  >𝐱</mi><mi
     >i</mi><mi
     >h</mi></msubsup><mo
    lspace="0.778em" rspace="0.111em"  >=</mo><mrow
    ><munder ><mo
    largeop="false" movablelimits="false"  >∑</mo><mrow
     ><msub 
    ><mi 
    >𝐱</mi><mi 
    >j</mi></msub><mo 
    >∈</mo><mi 
    >𝐗</mi></mrow></munder><mrow ><msup
    ><mi mathvariant="normal" 
    >s</mi><mi 
    >h</mi></msup><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo
    stretchy="false" >(</mo><msub ><mi
     >𝐱</mi><mi
     >i</mi></msub><mo
    >,</mo><msub ><mi
     >𝐱</mi><mi
     >j</mi></msub><mo
    stretchy="false" >)</mo></mrow><mo lspace="0em"
    rspace="0em" >​</mo><msup ><mi
    class="ltx_font_mathcaligraphic"  >𝒱</mi><mi
     >h</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub
    ><mi 
    >𝐱</mi><mi 
    >j</mi></msub><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd
    ><mrow ><mrow ><msub
    ><mover accent="true" 
    ><mi  >𝐱</mi><mo
     >^</mo></mover><mi
     >i</mi></msub><mo
     >=</mo><mrow
    ><mtext  >MERGE</mtext><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msubsup
    ><mi 
    >𝐱</mi><mi 
    >i</mi><mn 
    >1</mn></msubsup><mo >,</mo><msubsup
    ><mi 
    >𝐱</mi><mi 
    >i</mi><mn 
    >2</mn></msubsup><mo >,</mo><mi
    mathvariant="normal"  >…</mi><mo
    >,</mo><msubsup ><mi
     >𝐱</mi><mi
     >i</mi><mi
     >H</mi></msubsup><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol
    cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci
     >s</ci><ci 
    >ℎ</ci></apply><interval closure="open" ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci 
    >𝑖</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci 
    >𝑗</ci></apply></interval></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     ><mtext
     >NORM</mtext></ci><ci
     >𝑗</ci></apply><apply
    ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >∥</ci><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑘</ci></apply><ci
     >𝐗</ci></apply></apply><apply
    ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci
     >𝒬</ci><ci
     >ℎ</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous"
    >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑖</ci></apply><ci
     >T</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci
     >𝒦</ci><ci
     >ℎ</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑘</ci></apply></apply></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci 
    >𝑖</ci></apply><ci 
    >ℎ</ci></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑗</ci></apply><ci
     >𝐗</ci></apply></apply><apply
    ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci
     >s</ci><ci 
    >ℎ</ci></apply><interval closure="open" ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑗</ci></apply></interval><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci
     >𝒱</ci><ci
     >ℎ</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑗</ci></apply></apply></apply></apply><apply
    ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><apply
     ><ci 
    >^</ci><ci 
    >𝐱</ci></apply><ci 
    >𝑖</ci></apply><apply ><ci
     ><mtext 
    >MERGE</mtext></ci><vector ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci 
    >𝑖</ci></apply><cn type="integer" 
    >1</cn></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑖</ci></apply><cn
    type="integer"  >2</cn></apply><ci
     >…</ci><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑖</ci></apply><ci
     >𝐻</ci></apply></vector></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{gathered}\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{NORM}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\mathcal{Q}^{h}(\mathbf{x}_{i})^{\mathrm{T}}\mathcal{K}^{h}(\mathbf{x}_{k})),\\
    \mathbf{x}_{i}^{h}\ =\mathop{\sum}_{\mathbf{x}_{j}\in\mathbf{X}}{\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})\mathcal{V}^{h}(\mathbf{x}_{j}}),\\
    \hat{\mathbf{x}}_{i}=\text{MERGE}(\mathbf{x}_{i}^{1},\mathbf{x}_{i}^{2},\ldots,\mathbf{x}_{i}^{H}),\end{gathered}</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: 'where $h\in\{0,1,\ldots,H-1\}$ represents the attention head number. $\mathcal{Q}^{h}$,
    $\mathcal{K}^{h}$ and $\mathcal{V}^{h}$ are projection functions mapping a vector
    to the query space, key space and value space respectively. $\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})$
    is score function measuring the similarity between $\mathbf{x}_{i}$ and $\mathbf{x}_{j}$.
    NORM is the normalization operation ensuring $\sum_{\mathbf{x}_{j}\in\mathbf{X}}\mathrm{s}^{h}(\mathbf{x}_{i},\mathbf{x}_{j})\equiv
    1$ to propel the stability of the output generated by a stack of attention layers,
    it is usually performed as scaled softmax: $\text{NORM}(\cdot)=\text{SoftMax}(\cdot/\sqrt{d})$.
    And MERGE function is designed to combine the information extracted from multiple
    attention heads. Here, we omit further implementation details that do not affect
    our understanding of attention operation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The attention process cannot encode the position information of each $\mathbf{x}_{i}$,
    which is essential in machine translation problem. So the positional encoding
    is introduced to remedy this deficiency, and it’s calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (50) |  | $\begin{gathered}\mathbf{X}^{pos}_{i,2j}=\sin(i/10000^{2j/d}),\
    \mathbf{X}^{pos}_{i,2j+1}=\cos(i/10000^{2j/d}),\end{gathered}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $i$ is the position and $j$ is the dimension. The positional encoding
    is added to the input before it is fed to the Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. A Summary of Graph Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Technique | Capacity |'
  prefs: []
  type: TYPE_TB
- en: '| Attention Modification | Encoding Enhancement | Heterogeneous | Long Distance
    | ¿ 1-WL |'
  prefs: []
  type: TYPE_TB
- en: '| GGT (Dwivedi and Bresson, [2020](#bib.bib82)) | ✓ | ✓ |  | structure only
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| GTSA (Kreuzer et al., [2021](#bib.bib194)) | ✓ | ✓ |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| HGT (Hu et al., [2020a](#bib.bib148)) | ✓ |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| G2SHGT (Yao et al., [2020](#bib.bib414)) | ✓ |  | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| GRUGT (Cai and Lam, [2020](#bib.bib32)) | ✓ |  |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Graphormer (Ying et al., [2021](#bib.bib416)) | ✓ | ✓ |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| GSGT (Hussain et al., [2021](#bib.bib154)) |  | ✓ |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-BERT (Zhang et al., [2020b](#bib.bib437)) |  | ✓ |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| LRGT (Wu et al., [2021](#bib.bib387)) |  | ✓ |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| SAT (Chen et al., [2022a](#bib.bib47)) |  | ✓ |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 6.2\. Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the simplified process shown in Equation. [49](#S6.E49 "In 6.1\. Transformer
    ‣ 6\. Graph Transformer By Junwei Yang ‣ A Comprehensive Survey on Deep Graph
    Representation Learning"), we can see that the core of the attention operation
    is to accomplish information transfer based on the similarity between the source
    and the target to be updated. It’s quite similar to the message-passing process
    on a fully-connected graph. However, direct application of this architecture to
    arbitrary graphs does not make use of structural information, so it may lead to
    poor performance when graph topology is important. On the other hand, the definition
    of positional encoding in graphs is not a trivial problem because the order or
    coordinates of graph nodes are underdefined.
  prefs: []
  type: TYPE_NORMAL
- en: According to these two challenges, Transformer-based methods for graph representation
    learning can be classified into two major categories, one considering graph structure
    during attention process, and the other encoding the topological information of
    the graph into initial node features. We name the first one as Attention Modification
    and the second one as Encoding Enhancement. A summarization is provided in Table
    [4](#S6.T4 "Table 4 ‣ 6.1\. Transformer ‣ 6\. Graph Transformer By Junwei Yang
    ‣ A Comprehensive Survey on Deep Graph Representation Learning"). In the following
    discussion, if both methods are used in one paper, we will list them in different
    subsections, and we will ignore the multi-head trick in attention operation.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Attention Modification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This group of works attempt to modify the full attention operation to capture
    structure information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most prevalent approach is changing the score function, which is denoted
    as $\mathrm{s}(\cdot,\cdot)$ in Equation [49](#S6.E49 "In 6.1\. Transformer ‣
    6\. Graph Transformer By Junwei Yang ‣ A Comprehensive Survey on Deep Graph Representation
    Learning"). GGT (Dwivedi and Bresson, [2020](#bib.bib82)) constrains each node
    feature can only attend to neighbours and enables model to represent edge feature
    information by rewrite $\mathrm{s}(\cdot,\cdot)$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (51) |  | <math  class="ltx_math_unparsed" alttext="\begin{gathered}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &amp;(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}\mathbf{x}_{j}\odot\mathbf{W}^{E}\mathbf{e}_{ji}),&amp;\left<j,i\right>\in
    E\\ &amp;-\infty,&amp;\text{otherwise}\end{aligned}\right.,\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathrm{s}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{SoftMax}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{k})),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd ><mrow
    ><msub ><mover accent="true"
    ><mi mathvariant="normal" >s</mi><mo
    >~</mo></mover><mn >1</mn></msub><mrow
    ><mo stretchy="false" >(</mo><msub
    ><mi >𝐱</mi><mi >i</mi></msub><mo
    >,</mo><msub ><mi
    >𝐱</mi><mi >j</mi></msub><mo
    stretchy="false" >)</mo></mrow><mo >=</mo><mrow
    ><mo >{</mo><mtable
    columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd class="ltx_align_left" columnalign="left"
    ><mrow ><mrow
    ><msup ><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><msup ><mi
    >𝐖</mi><mi >Q</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><msub
    ><mi >𝐱</mi><mi
    >i</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow><mi
    mathvariant="normal" >T</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><mrow ><mrow
    ><msup ><mi
    >𝐖</mi><mi >K</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><msub
    ><mi >𝐱</mi><mi
    >j</mi></msub></mrow><mo
    lspace="0.222em" rspace="0.222em" >⊙</mo><msup
    ><mi >𝐖</mi><mi
    >E</mi></msup></mrow><mo
    lspace="0em" rspace="0em" >​</mo><msub
    ><mi >𝐞</mi><mrow
    ><mi >j</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi
    >i</mi></mrow></msub></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd class="ltx_align_right"
    columnalign="right" ><mrow ><mrow
    ><mo >⟨</mo><mi
    >j</mi><mo >,</mo><mi
    >i</mi><mo >⟩</mo></mrow><mo
    >∈</mo><mi >E</mi></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left"
    ><mrow ><mrow
    ><mo >−</mo><mi
    mathvariant="normal" >∞</mi></mrow><mo
    >,</mo></mrow></mtd><mtd class="ltx_align_right"
    columnalign="right" ><mtext >otherwise</mtext></mtd></mtr></mtable><mo
    >,</mo></mrow></mrow></mtd></mtr><mtr ><mtd
    ><mrow ><mrow ><mrow
    ><msub ><mi
    mathvariant="normal" >s</mi><mn >1</mn></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub
    ><mi >𝐱</mi><mi
    >i</mi></msub><mo >,</mo><msub
    ><mi >𝐱</mi><mi
    >j</mi></msub><mo stretchy="false" >)</mo></mrow></mrow><mo
    >=</mo><mrow ><msub
    ><mtext >SoftMax</mtext><mi
    >j</mi></msub><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><munder
    ><mo lspace="0em" movablelimits="false"
    rspace="0.167em" >∥</mo><mrow ><msub
    ><mi >𝐱</mi><mi
    >k</mi></msub><mo >∈</mo><mi
    >𝐗</mi></mrow></munder><mrow ><msub
    ><mover accent="true" ><mi
    mathvariant="normal" >s</mi><mo >~</mo></mover><mn
    >1</mn></msub><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo
    stretchy="false" >(</mo><msub ><mi
    >𝐱</mi><mi >i</mi></msub><mo
    >,</mo><msub ><mi
    >𝐱</mi><mi >k</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{gathered}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}\mathbf{x}_{j}\odot\mathbf{W}^{E}\mathbf{e}_{ji}),&\left<j,i\right>\in
    E\\ &-\infty,&\text{otherwise}\end{aligned}\right.,\\ \mathrm{s}_{1}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{SoftMax}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\tilde{\mathrm{s}}_{1}(\mathbf{x}_{i},\mathbf{x}_{k})),\end{gathered}</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: 'where $\odot$ is Hadamard product and $\mathbf{W}^{Q,K,E}$ represents trainable
    parameter matrix. This approach is not efficient yet to model long-distance dependencies
    since only 1st-neighbors are considered. Though it adopts Laplacian eigenvectors
    to gather global information (cf. Section [6.4](#S6.SS4 "6.4\. Encoding Enhancement
    ‣ 6\. Graph Transformer By Junwei Yang ‣ A Comprehensive Survey on Deep Graph
    Representation Learning")), but only long-distance structure information is remedied
    while the node and edge features are not. GTSA (Kreuzer et al., [2021](#bib.bib194))
    improves this approach by combining the original graph and the full graph. Specifically,
    it extends $\mathrm{s}_{1}(\cdot,\cdot)$ to:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (52) |  | <math  class="ltx_math_unparsed" alttext="\begin{gathered}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &amp;(\mathbf{W}^{Q}_{1}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{1}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{1}\mathbf{e}_{ji}),&amp;\left<j,i\right>\in
    E\\ &amp;(\mathbf{W}^{Q}_{0}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{0}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{0}\mathbf{e}_{ji}),&amp;\text{otherwise}\end{aligned}\right.,\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathrm{s}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned} &amp;\frac{1}{1+\lambda}\text{SoftMax}_{j}(\mathop{\&#124;}_{\left<k,i\right>\in
    E}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{k})),&amp;\left<j,i\right>\in
    E\\
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;\frac{\lambda}{1+\lambda}\text{SoftMax}_{j}(\mathop{\&#124;}_{\left<k,i\right>\not\in
    E}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{k})),&amp;\text{otherwise}\end{aligned}\right.,\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd ><mrow
    ><msub ><mover accent="true"
    ><mi mathvariant="normal" >s</mi><mo
    >~</mo></mover><mn >2</mn></msub><mrow
    ><mo stretchy="false" >(</mo><msub
    ><mi >𝐱</mi><mi >i</mi></msub><mo
    >,</mo><msub ><mi
    >𝐱</mi><mi >j</mi></msub><mo
    stretchy="false" >)</mo></mrow><mo >=</mo><mrow
    ><mo >{</mo><mtable
    columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd class="ltx_align_left" columnalign="left"
    ><mrow ><mrow
    ><msup ><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><msubsup ><mi
    >𝐖</mi><mn >1</mn><mi
    >Q</mi></msubsup><mo lspace="0em"
    rspace="0em" >​</mo><msub
    ><mi >𝐱</mi><mi
    >i</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow><mi
    mathvariant="normal" >T</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><mrow ><mrow
    ><msubsup ><mi
    >𝐖</mi><mn >1</mn><mi
    >K</mi></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><msub
    ><mi >𝐱</mi><mi
    >j</mi></msub></mrow><mo
    lspace="0.222em" rspace="0.222em" >⊙</mo><msubsup
    ><mi >𝐖</mi><mn
    >1</mn><mi >E</mi></msubsup></mrow><mo
    lspace="0em" rspace="0em" >​</mo><msub
    ><mi >𝐞</mi><mrow
    ><mi >j</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi
    >i</mi></mrow></msub></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd class="ltx_align_right"
    columnalign="right" ><mrow ><mrow
    ><mo >⟨</mo><mi
    >j</mi><mo >,</mo><mi
    >i</mi><mo >⟩</mo></mrow><mo
    >∈</mo><mi >E</mi></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left"
    ><mrow ><mrow
    ><msup ><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><msubsup ><mi
    >𝐖</mi><mn >0</mn><mi
    >Q</mi></msubsup><mo lspace="0em"
    rspace="0em" >​</mo><msub
    ><mi >𝐱</mi><mi
    >i</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow><mi
    mathvariant="normal" >T</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><mrow ><mrow
    ><msubsup ><mi
    >𝐖</mi><mn >0</mn><mi
    >K</mi></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><msub
    ><mi >𝐱</mi><mi
    >j</mi></msub></mrow><mo
    lspace="0.222em" rspace="0.222em" >⊙</mo><msubsup
    ><mi >𝐖</mi><mn
    >0</mn><mi >E</mi></msubsup></mrow><mo
    lspace="0em" rspace="0em" >​</mo><msub
    ><mi >𝐞</mi><mrow
    ><mi >j</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi
    >i</mi></mrow></msub></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd class="ltx_align_right"
    columnalign="right" ><mtext >otherwise</mtext></mtd></mtr></mtable><mo
    >,</mo></mrow></mrow></mtd></mtr><mtr ><mtd
    ><mrow ><msub ><mi
    mathvariant="normal" >s</mi><mn >2</mn></msub><mrow
    ><mo stretchy="false" >(</mo><msub
    ><mi >𝐱</mi><mi
    >i</mi></msub><mo >,</mo><msub
    ><mi >𝐱</mi><mi
    >j</mi></msub><mo stretchy="false" >)</mo></mrow><mo
    >=</mo><mrow ><mo
    >{</mo><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd
    class="ltx_align_left" columnalign="left" ><mrow
    ><mrow ><mfrac
    ><mn >1</mn><mrow
    ><mn >1</mn><mo
    >+</mo><mi >λ</mi></mrow></mfrac><mo
    lspace="0em" rspace="0em" >​</mo><msub
    ><mtext >SoftMax</mtext><mi
    >j</mi></msub><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow
    ><munder ><mo
    lspace="0em" movablelimits="false" rspace="0.167em" >∥</mo><mrow
    ><mrow ><mo
    >⟨</mo><mi >k</mi><mo
    >,</mo><mi >i</mi><mo
    >⟩</mo></mrow><mo >∈</mo><mi
    >E</mi></mrow></munder><mrow ><msub
    ><mover accent="true" ><mi
    mathvariant="normal" >s</mi><mo
    >~</mo></mover><mn >2</mn></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub
    ><mi >𝐱</mi><mi
    >i</mi></msub><mo >,</mo><msub
    ><mi >𝐱</mi><mi
    >k</mi></msub><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd class="ltx_align_right"
    columnalign="right" ><mrow ><mrow
    ><mo >⟨</mo><mi
    >j</mi><mo >,</mo><mi
    >i</mi><mo >⟩</mo></mrow><mo
    >∈</mo><mi >E</mi></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left"
    ><mrow ><mrow
    ><mfrac ><mi
    >λ</mi><mrow ><mn
    >1</mn><mo >+</mo><mi
    >λ</mi></mrow></mfrac><mo lspace="0em"
    rspace="0em" >​</mo><msub ><mtext
    >SoftMax</mtext><mi >j</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><munder ><mo
    lspace="0em" movablelimits="false" rspace="0.167em" >∥</mo><mrow
    ><mrow ><mo
    >⟨</mo><mi >k</mi><mo
    >,</mo><mi >i</mi><mo
    >⟩</mo></mrow><mo >∉</mo><mi
    >E</mi></mrow></munder><mrow ><msub
    ><mover accent="true" ><mi
    mathvariant="normal" >s</mi><mo
    >~</mo></mover><mn >2</mn></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub
    ><mi >𝐱</mi><mi
    >i</mi></msub><mo >,</mo><msub
    ><mi >𝐱</mi><mi
    >k</mi></msub><mo stretchy="false"
    >)</mo></mrow></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo
    >,</mo></mrow></mtd><mtd class="ltx_align_right"
    columnalign="right" ><mtext >otherwise</mtext></mtd></mtr></mtable><mo
    >,</mo></mrow></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{gathered}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned}
    &(\mathbf{W}^{Q}_{1}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{1}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{1}\mathbf{e}_{ji}),&\left<j,i\right>\in
    E\\ &(\mathbf{W}^{Q}_{0}\mathbf{x}_{i})^{\mathrm{T}}(\mathbf{W}^{K}_{0}\mathbf{x}_{j}\odot\mathbf{W}^{E}_{0}\mathbf{e}_{ji}),&\text{otherwise}\end{aligned}\right.,\\
    \mathrm{s}_{2}(\mathbf{x}_{i},\mathbf{x}_{j})=\left\{\begin{aligned} &\frac{1}{1+\lambda}\text{SoftMax}_{j}(\mathop{\&#124;}_{\left<k,i\right>\in
    E}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{k})),&\left<j,i\right>\in
    E\\ &\frac{\lambda}{1+\lambda}\text{SoftMax}_{j}(\mathop{\&#124;}_{\left<k,i\right>\not\in
    E}\tilde{\mathrm{s}}_{2}(\mathbf{x}_{i},\mathbf{x}_{k})),&\text{otherwise}\end{aligned}\right.,\end{gathered}</annotation></semantics></math>
    |  |'
  prefs: []
  type: TYPE_NORMAL
- en: where $\lambda$ is a hyperparameter representing the strength of full connection.
  prefs: []
  type: TYPE_NORMAL
- en: Some works try to reduce information-mixing problems (Chen et al., [2020d](#bib.bib46))
    in heterogeneous graph. HGT (Hu et al., [2020a](#bib.bib148)) disentangles the
    attention of different node type and edge type by adopting additional attention
    heads. It defines $\mathbf{W}_{Q,K,V}^{\tau(v)}$ for each node type $\tau(v)$
    and $\mathbf{W}_{E}^{\phi(e)}$ for each edge type $\phi(e)$, $\tau(\cdot)$ and
    $\phi(\cdot)$ are type indicating function. G2SHGT (Yao et al., [2020](#bib.bib414))
    defines four types of subgraphs, fully-connected, connected, default and reverse,
    to capture global, undirected, forward and backward information respectively.
    And each subgraph is homogeneous, so it can reduce interactions between different
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Path features between nodes are always treated as inductive bias added to the
    original score function. Let $\text{SP}_{ij}=(e_{1},e_{2},\ldots,e_{N})$ denote
    the shortest path between node pair $(v_{i},v_{j})$. GRUGT (Cai and Lam, [2020](#bib.bib32))
    uses GRU (Chung et al., [2014](#bib.bib62)) to encode forward and backward features
    as: $\mathbf{r}_{ij}=\text{GRU}(\text{SP}_{ij})$, $\mathbf{r}_{ji}=\text{GRU}(\text{SP}_{ji})$.
    Then, the final attention score is calculated by adding up four components:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (53) |  | $\begin{gathered}\tilde{\mathrm{s}}_{3}(\mathbf{x}_{i},\mathbf{x}_{j})=(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{x}_{j}+(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{r}_{ji}+(\mathbf{W}^{Q}\mathbf{r}_{ij})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{x}_{j}+(\mathbf{W}^{Q}\mathbf{r}_{ij})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{r}_{ji},\end{gathered}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'from front to back, which represent content-based score, source-dependent bias,
    target-dependent bias and universal bias respectively. Graphormer (Ying et al.,
    [2021](#bib.bib416)) uses both path length and path embedding to introduce structural
    bias as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (54) |  | <math  class="ltx_Math" alttext="\begin{gathered}\tilde{\mathrm{s}}_{4}(\mathbf{x}_{i},\mathbf{x}_{j})=(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{x}_{j}/\sqrt{d}+b_{N}+c_{ij},\\
    c_{ij}=\frac{1}{N}\sum_{k=1}^{N}(\mathbf{e}_{k})^{\mathrm{T}}\mathbf{w}^{E}_{k},\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathrm{s}_{4}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{SoftMax}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\tilde{\mathrm{s}}_{4}(\mathbf{x}_{i},\mathbf{x}_{k})),\end{gathered}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd ><mrow
    ><mrow ><mrow
    ><msub ><mover
    accent="true"  ><mi
    mathvariant="normal"  >s</mi><mo
     >~</mo></mover><mn
     >4</mn></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub
    ><mi  >𝐱</mi><mi
     >i</mi></msub><mo
    >,</mo><msub ><mi
     >𝐱</mi><mi 
    >j</mi></msub><mo stretchy="false" >)</mo></mrow></mrow><mo
     >=</mo><mrow
    ><mrow ><mrow
    ><msup ><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><msup ><mi
     >𝐖</mi><mi
     >Q</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><msub
    ><mi 
    >𝐱</mi><mi 
    >i</mi></msub></mrow><mo stretchy="false"
    >)</mo></mrow><mi mathvariant="normal" 
    >T</mi></msup><mo lspace="0em" rspace="0em"
    >​</mo><msup ><mi
     >𝐖</mi><mi
     >K</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><msub
    ><mi 
    >𝐱</mi><mi 
    >j</mi></msub></mrow><mo 
    >/</mo><msqrt 
    ><mi 
    >d</mi></msqrt></mrow><mo 
    >+</mo><msub ><mi
     >b</mi><mi
     >N</mi></msub><mo
     >+</mo><msub
    ><mi 
    >c</mi><mrow 
    ><mi 
    >i</mi><mo lspace="0em" rspace="0em"
     >​</mo><mi
     >j</mi></mrow></msub></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd
    ><mrow ><mrow ><msub
    ><mi  >c</mi><mrow
     ><mi 
    >i</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >j</mi></mrow></msub><mo 
    >=</mo><mrow ><mfrac
     ><mn 
    >1</mn><mi 
    >N</mi></mfrac><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><munderover
    ><mo movablelimits="false" rspace="0em"
     >∑</mo><mrow
     ><mi 
    >k</mi><mo 
    >=</mo><mn 
    >1</mn></mrow><mi 
    >N</mi></munderover><mrow ><msup
    ><mrow ><mo
    stretchy="false" >(</mo><msub ><mi
     >𝐞</mi><mi 
    >k</mi></msub><mo stretchy="false" >)</mo></mrow><mi
    mathvariant="normal"  >T</mi></msup><mo
    lspace="0em" rspace="0em" >​</mo><msubsup
    ><mi 
    >𝐰</mi><mi 
    >k</mi><mi 
    >E</mi></msubsup></mrow></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd
    ><mrow ><mrow ><mrow
    ><msub ><mi
    mathvariant="normal"  >s</mi><mn
     >4</mn></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub
    ><mi 
    >𝐱</mi><mi 
    >i</mi></msub><mo >,</mo><msub
    ><mi 
    >𝐱</mi><mi 
    >j</mi></msub><mo stretchy="false" >)</mo></mrow></mrow><mo
     >=</mo><mrow
    ><msub ><mtext
     >SoftMax</mtext><mi
     >j</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><munder ><mo
    lspace="0em" movablelimits="false" rspace="0.167em" 
    >∥</mo><mrow 
    ><msub 
    ><mi 
    >𝐱</mi><mi 
    >k</mi></msub><mo 
    >∈</mo><mi 
    >𝐗</mi></mrow></munder><mrow ><msub
    ><mover accent="true" 
    ><mi mathvariant="normal" 
    >s</mi><mo 
    >~</mo></mover><mn 
    >4</mn></msub><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo
    stretchy="false" >(</mo><msub ><mi
     >𝐱</mi><mi
     >i</mi></msub><mo
    >,</mo><msub ><mi
     >𝐱</mi><mi
     >k</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol
    cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply
     ><ci 
    >~</ci><ci  >s</ci></apply><cn
    type="integer"  >4</cn></apply><interval
    closure="open" ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci 
    >𝑖</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci 
    >𝑗</ci></apply></interval></apply><apply ><apply
    ><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci
     >𝐖</ci><ci
     >𝑄</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑖</ci></apply></apply><ci
     >T</ci></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci
     >𝐖</ci><ci
     >𝐾</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑗</ci></apply></apply><apply
     ><ci 
    >𝑑</ci></apply></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >𝑏</ci><ci
     >𝑁</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝑐</ci><apply
     ><ci
     >𝑖</ci><ci
     >𝑗</ci></apply></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >𝑐</ci><apply
     ><ci 
    >𝑖</ci><ci 
    >𝑗</ci></apply></apply><apply ><apply
     ><cn type="integer"
     >1</cn><ci
     >𝑁</ci></apply><apply
    ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply
     ><ci 
    >𝑘</ci><cn type="integer" 
    >1</cn></apply></apply><ci 
    >𝑁</ci></apply><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous"
    >subscript</csymbol><ci
     >𝐞</ci><ci 
    >𝑘</ci></apply><ci 
    >T</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci
     >𝐰</ci><ci
     >𝐸</ci></apply><ci
     >𝑘</ci></apply></apply></apply></apply></apply><apply
    ><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >s</ci><cn type="integer"
     >4</cn></apply><interval
    closure="open" ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci 
    >𝑖</ci></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >𝐱</ci><ci 
    >𝑗</ci></apply></interval></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     ><mtext
     >SoftMax</mtext></ci><ci
     >𝑗</ci></apply><apply
    ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >∥</ci><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑘</ci></apply><ci
     >𝐗</ci></apply></apply><apply
    ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><apply
     ><ci 
    >~</ci><ci 
    >s</ci></apply><cn type="integer" 
    >4</cn></apply><interval closure="open" ><apply
    ><csymbol cd="ambiguous"
    >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous"
    >subscript</csymbol><ci
     >𝐱</ci><ci
     >𝑘</ci></apply></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{gathered}\tilde{\mathrm{s}}_{4}(\mathbf{x}_{i},\mathbf{x}_{j})=(\mathbf{W}^{Q}\mathbf{x}_{i})^{\mathrm{T}}\mathbf{W}^{K}\mathbf{x}_{j}/\sqrt{d}+b_{N}+c_{ij},\\
    c_{ij}=\frac{1}{N}\sum_{k=1}^{N}(\mathbf{e}_{k})^{\mathrm{T}}\mathbf{w}^{E}_{k},\\
    \mathrm{s}_{4}(\mathbf{x}_{i},\mathbf{x}_{j})=\text{SoftMax}_{j}(\mathop{\&#124;}_{\mathbf{x}_{k}\in\mathbf{X}}\tilde{\mathrm{s}}_{4}(\mathbf{x}_{i},\mathbf{x}_{k})),\end{gathered}</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: where $b_{N}$ is a trainable scalar indexed by $N$, the length of $\text{SP}_{ij}$
    . $\mathbf{e}_{k}$ is the embedding of the the edge $e_{k}$, and $\mathbf{w}^{E}_{k}\in\mathbb{R}^{d}$
    is the $k$-th edge parameter. If $\text{SP}_{ij}$ does not exist, then $b_{N}$
    and $c_{ij}$ are set to be special value.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4\. Encoding Enhancement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This kind of methods intend to enhance initial node representations to enable
    Transformer to encode structure information. They can be further divided into
    two categories, position-analogy methods and structure-aware methods.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1\. Position-analogy methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Euclidean space, the Laplacian operator corresponds to the divergence of
    the gradient, whose eigenfunctions are sine/cosine functions. For graph, the Laplacian
    operator is Laplacian matrix, whose eigenvectors can be considered as eigenfunctions.
    Hence, inspired by Equation [50](#S6.E50 "In 6.1\. Transformer ‣ 6\. Graph Transformer
    By Junwei Yang ‣ A Comprehensive Survey on Deep Graph Representation Learning"),
    position-analogy methods utilize Laplacian eigenvectors to simulate positional
    encoding $\mathbf{X}^{pos}$ as they are the equivalents of sine/cosine functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Laplacian eigenvectors can be calculated through the eigendecomposition of
    normalized graph Laplacian matrix $\tilde{\mathbf{L}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (55) |  | $\tilde{\mathbf{L}}\triangleq\mathbf{I}-\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\text{T}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{A}$ is the adjacency matrix, $\mathbf{D}$ is the degree matrix,
    $\mathbf{U}=[\mathbf{u}_{1},\mathbf{u}_{2},\ldots,\mathbf{u}_{n-1}]$ are eigenvectors
    and $\mathbf{\Lambda}=diag(\lambda_{0},\lambda_{1},\ldots,\lambda_{n-1})$ are
    eigenvalues. With $\mathbf{U}$ and $\mathbf{\Lambda}$, GGT (Dwivedi and Bresson,
    [2020](#bib.bib82)) uses eigenvectors of the k smallest non-trivial eigenvalues
    to denote the intermediate embedding $\mathbf{X}^{mid}\in\mathbb{R}^{n\times k}$,
    and maps it to d-dimensional space and gets the position encoding $\mathbf{X}^{pos}\in\mathbb{R}^{n\times
    d}$. This process can be formulized as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (56) |  | <math  class="ltx_Math" alttext="\begin{gathered}index=\text{argmin}_{k}(\{\lambda_{i}&#124;0\leq
    i<n\wedge\lambda_{i}>0\}),\\ \mathbf{X}^{mid}=[\mathbf{u}_{index_{0}},\mathbf{u}_{index_{1}},\ldots,\mathbf{u}_{index_{k-1}}]^{\text{T}},\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{X}^{pos}=\mathbf{X}^{mid}\mathbf{W}^{k\times d},\end{gathered}" display="block"><semantics
    ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd ><mrow ><mrow
    ><mrow ><mi
     >i</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi 
    >n</mi><mo lspace="0em" rspace="0em" >​</mo><mi
     >d</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi 
    >e</mi><mo lspace="0em" rspace="0em" >​</mo><mi
     >x</mi></mrow><mo
     >=</mo><mrow ><msub
    ><mtext  >argmin</mtext><mi
     >k</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><mo stretchy="false" >{</mo><msub
    ><mi 
    >λ</mi><mi 
    >i</mi></msub><mo lspace="0em" rspace="0em"
    >&#124;</mo><mrow ><mn
     >0</mn><mo
     >≤</mo><mrow
    ><mi 
    >i</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo fence="true" rspace="0em"
    ><</mo><mrow ><mi
     >n</mi><mo
     >∧</mo><msub
    ><mi 
    >λ</mi><mi 
    >i</mi></msub></mrow><mo fence="true"
    lspace="0em" >></mo></mrow><mo lspace="0em" rspace="0em"
    >​</mo><mn 
    >0</mn></mrow></mrow><mo stretchy="false"
    >}</mo></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd
    ><mrow ><mrow ><msup
    ><mi  >𝐗</mi><mrow
     ><mi 
    >m</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >i</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >d</mi></mrow></msup><mo 
    >=</mo><msup ><mrow
    ><mo stretchy="false" >[</mo><msub
    ><mi 
    >𝐮</mi><mrow 
    ><mi  >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >n</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >d</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >e</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >x</mi><mn
     >0</mn></msub></mrow></msub><mo
    >,</mo><msub ><mi
     >𝐮</mi><mrow
     ><mi 
    >i</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >n</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >d</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >e</mi><mo lspace="0em" rspace="0em" 
    >​</mo><msub 
    ><mi 
    >x</mi><mn 
    >1</mn></msub></mrow></msub><mo >,</mo><mi
    mathvariant="normal"  >…</mi><mo
    >,</mo><msub ><mi
     >𝐮</mi><mrow
     ><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >n</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >d</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >e</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >x</mi><mrow
     ><mi
     >k</mi><mo
     >−</mo><mn
     >1</mn></mrow></msub></mrow></msub><mo
    stretchy="false" >]</mo></mrow><mtext 
    >T</mtext></msup></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mrow
    ><msup ><mi
     >𝐗</mi><mrow
     ><mi 
    >p</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >o</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >s</mi></mrow></msup><mo 
    >=</mo><mrow ><msup
    ><mi  >𝐗</mi><mrow
     ><mi 
    >m</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >i</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >d</mi></mrow></msup><mo lspace="0em"
    rspace="0em" >​</mo><msup ><mi
     >𝐖</mi><mrow
     ><mi 
    >k</mi><mo lspace="0.222em" rspace="0.222em"
     >×</mo><mi
     >d</mi></mrow></msup></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol
    cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><ci
     >𝑖</ci><ci 
    >𝑛</ci><ci  >𝑑</ci><ci
     >𝑒</ci><ci 
    >𝑥</ci></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     ><mtext 
    >argmin</mtext></ci><ci 
    >𝑘</ci></apply><apply ><csymbol
    cd="latexml" >conditional-set</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >𝜆</ci><ci
     >𝑖</ci></apply><apply
    ><cn type="integer" 
    >0</cn><apply ><ci
     >𝑖</ci><apply
    ><csymbol cd="latexml" >expectation</csymbol><apply
    ><ci 
    >𝑛</ci><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >𝜆</ci><ci
     >𝑖</ci></apply></apply></apply><cn
    type="integer"  >0</cn></apply></apply></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci
     >𝐗</ci><apply
     ><ci 
    >𝑚</ci><ci 
    >𝑖</ci><ci 
    >𝑑</ci></apply></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><list
    ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >𝐮</ci><apply
     ><ci 
    >𝑖</ci><ci 
    >𝑛</ci><ci 
    >𝑑</ci><ci 
    >𝑒</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑥</ci><cn type="integer" 
    >0</cn></apply></apply></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >𝐮</ci><apply
     ><ci 
    >𝑖</ci><ci 
    >𝑛</ci><ci 
    >𝑑</ci><ci 
    >𝑒</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑥</ci><cn type="integer" 
    >1</cn></apply></apply></apply><ci 
    >…</ci><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >𝐮</ci><apply
     ><ci
     >𝑖</ci><ci
     >𝑛</ci><ci
     >𝑑</ci><ci
     >𝑒</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑥</ci><apply
     ><ci
     >𝑘</ci><cn
    type="integer"  >1</cn></apply></apply></apply></apply></list><ci
     ><mtext
    mathsize="70%"  >T</mtext></ci></apply></apply><apply
    ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci
     >𝐗</ci><apply
     ><ci 
    >𝑝</ci><ci 
    >𝑜</ci><ci 
    >𝑠</ci></apply></apply><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci
     >𝐗</ci><apply
     ><ci 
    >𝑚</ci><ci 
    >𝑖</ci><ci 
    >𝑑</ci></apply></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci
     >𝐖</ci><apply
     ><ci 
    >𝑘</ci><ci 
    >𝑑</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{gathered}index=\text{argmin}_{k}(\{\lambda_{i}&#124;0\leq
    i<n\wedge\lambda_{i}>0\}),\\ \mathbf{X}^{mid}=[\mathbf{u}_{index_{0}},\mathbf{u}_{index_{1}},\ldots,\mathbf{u}_{index_{k-1}}]^{\text{T}},\\
    \mathbf{X}^{pos}=\mathbf{X}^{mid}\mathbf{W}^{k\times d},\end{gathered}</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: 'where $index$ is the subscript of the selected eigenvectors. GTSA (Kreuzer
    et al., [2021](#bib.bib194)) puts eigenvector $\mathbf{u}_{i}$ on the frequency
    axis at $\lambda_{i}$ and uses sequence modeling methods to generate positional
    encoding. Specificly, it extends $\mathbf{X}^{mid}$ in Equation [56](#S6.E56 "In
    6.4.1\. Position-analogy methods ‣ 6.4\. Encoding Enhancement ‣ 6\. Graph Transformer
    By Junwei Yang ‣ A Comprehensive Survey on Deep Graph Representation Learning")
    to $\tilde{\mathbf{X}}^{mid}\in\mathbb{R}^{n\times k\times 2}$ by concatenating
    each value in eigenvectors with corresponding eigenvalue, and then positional
    encoding $\mathbf{X}^{pos}\in\mathbb{R}^{n\times d}$ are generated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (57) |  | $\begin{gathered}\mathbf{X}^{input}=\tilde{\mathbf{X}}^{mid}\mathbf{W}^{2\times
    d},\\ \mathbf{X}^{pos}=\text{SumPooling}(\text{Transformer}(\mathbf{X}^{input}),\text{dim}=1).\end{gathered}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $\mathbf{X}_{input}\in\mathbb{R}^{n\times k\times d}$ is equivalent to
    the input matrix in sequence modeling problem with shape $(batch\_size,length,dim)$,
    and can be naturally processed by Transformer. Since the Laplacian eigenvectors
    can be complex-valued for directed graph, GSGT (Hussain et al., [2021](#bib.bib154))
    proposes to utilize SVD of adjacency matrix $\mathbf{A}$, which is denoted as
    $\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\text{T}}$, and uses the largest
    $k$ singular values $\mathbf{\Sigma}_{k}$ and associated left and right singular
    vectors $\mathbf{U}_{k}$ and $\mathbf{V}_{k}^{\text{T}}$ to output $\mathbf{X}^{pos}$
    as $\mathbf{X}^{pos}=[\mathbf{U}_{k}\mathbf{\Sigma}_{k}^{1/2}\|\mathbf{V}_{k}\mathbf{\Sigma}_{k}^{1/2}]$,
    where $\|$ is the concatenation operation. All these methods above randomly flip
    the signs of eigenvectors or singular vectors during the training phase to promote
    the invariance of the models to the sign ambiguity.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2\. Structure-aware methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In contrast to position-analogy methods, structure-aware methods do not attempt
    to mathematically rigorously simulate sequence positional encoding. They use some
    additional mechanisms to directly calculate structure related encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some approaches compute extra encoding $\mathbf{X}^{add}$ and add it to the
    initial node representation. Graphormer (Ying et al., [2021](#bib.bib416)) proposes
    to leverage node centrality as additional signal to address the importance of
    each node. Concretely, $\mathbf{x}_{i}^{add}$ is determined by the indegree $\text{deg}_{i}^{-}$
    and outdegree $\text{deg}_{i}^{+}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (58) |  | $\mathbf{x}_{i}^{add}=\mathcal{P}^{-}(\text{deg}_{i}^{-})+\mathcal{P}^{+}(\text{deg}_{i}^{+}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{P}^{-}$ and $\mathcal{P}^{+}$ are learnable embedding function.
    Graph-BERT (Zhang et al., [2020b](#bib.bib437)) employs Weisfeiler-Lehman algorithm
    to label node $v_{i}$ to a number $\text{WL}(v_{i})\in\mathbb{N}$ and defines
    $\mathbf{x}_{i}^{add}$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (59) |  | $\begin{gathered}\mathbf{x}_{i,2j}^{add}=\sin(\text{WL}(v_{i})/10000^{2j/d}),\
    \mathbf{x}_{i,2j+1}^{add}=\cos(\text{WL}(v_{i})/10000^{2j/d}).\end{gathered}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The other approaches try to leverage GNNs to initialize inputs to Transformer.
    LRGT (Wu et al., [2021](#bib.bib387)) applies GNN to get intermediate vectors
    as $\mathbf{X}^{\prime}=\text{GNN}(\mathbf{X})$, and passes the concatenation
    of $\mathbf{X}^{\prime}$ and a special vector $\mathbf{x}_{\text{CLS}}$ to Transformer
    layer as: $\hat{\mathbf{X}}=\text{Transformer}([\mathbf{X}^{\prime}\|\mathbf{x}_{\text{CLS}}])$.
    Then $\hat{\mathbf{x}}_{\text{CLS}}$ can be used as the representation of the
    entire graph for downstream tasks. This method cannot break 1-WL bottleneck because
    it uses GCN (Kipf and Welling, [2016a](#bib.bib183)) and GIN (Xu et al., [2018a](#bib.bib404))
    as graph encoder in the first step, which are intrinsically limited by 1-WL test.
    SAT (Chen et al., [2022a](#bib.bib47)) improves this deficiency by using subgraph-GNN
    NGNN (Zhang and Li, [2021](#bib.bib439)) for initialization, and achieves outstanding
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section introduces Transformer-based approaches for graph representation
    learning and we provide the summary as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques. Graph Transformer methods modify two fundamental techniques in Transformer,
    attention operation and positional encoding, to enhance its ability to encode
    graph data. Typically, they introduce fully-connected attention to model long-distance
    relationship, utilize shortest path and Laplacian eigenvectors to break 1-WL bottleneck,
    and separate points and edges belonging to different classes to avoid over-mixing
    problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and Limitations. Though Graph Transformers achieve encouraging performance,
    they still face two major challenges. The first challenge is the computational
    cost of the quadratic attention mechanism and shortest path calculation. These
    operations require significant computing resources and can be a bottleneck, particularly
    for large graphs. The second is the reliance of Transformer-based models on large
    amounts of data for stable performance. It poses a challenge when dealing with
    problems that lack sufficient data, especially for few-shot and zero-shot settings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Works. We expect efficiency improvement for Graph Transformer should
    be further explored. Additionally, there are some works using pre-training and
    fine-tuning framework to balance performance and complexity in downstream tasks
    (Ying et al., [2021](#bib.bib416)), this may be a promising solution to address
    the aforementioned two challenges.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 7\. Semi-supervised Learning on Graphs By Xiao Luo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have investigated various architectures of graph neural networks in which
    the parameters should be tuned by a learning objective. The most prevalent optimization
    approach is supervised learning on graph data. Due to the label deficiency, semi-supervised
    learning has attracted increasing attention in the data mining community. In detail,
    these methods attempt to combine graph representation learning with current semi-supervised
    techniques including pseudo-labeling, consistency learning, knowledge distillation
    and active learning. These works can be further subdivided into node-level representation
    learning and graph-level representation learning. We would introduce both parts
    in detail as in Sec. [7.1](#S7.SS1 "7.1\. Node Representation Learning ‣ 7\. Semi-supervised
    Learning on Graphs By Xiao Luo ‣ A Comprehensive Survey on Deep Graph Representation
    Learning") and Sec. [7.2](#S7.SS2 "7.2\. Graph Representation Learning ‣ 7\. Semi-supervised
    Learning on Graphs By Xiao Luo ‣ A Comprehensive Survey on Deep Graph Representation
    Learning"), respectively. A summarization is provided in Table [5](#S7.T5 "Table
    5 ‣ 7.1\. Node Representation Learning ‣ 7\. Semi-supervised Learning on Graphs
    By Xiao Luo ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. Node Representation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Typically, node representation learning follows the concept of transductive
    learning, which has the access to test unlabeled data. We first review the simplest
    loss objective, i.e., node-level supervised loss. This loss exploits the ground
    truth of labeled nodes on graphs. The standard cross-entropy is usually adopted
    for optimization. In formulation,
  prefs: []
  type: TYPE_NORMAL
- en: '| (60) |  | $\mathcal{L}_{NSL}=-\frac{1}{&#124;\mathcal{Y}^{L}&#124;}\sum_{i\in\mathcal{Y}^{L}}\mathbf{y}_{i}^{T}\log\mathbf{p}_{i},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{Y}^{L}$ denotes the set of labeled nodes. Additionally, there
    are a variety of unlabeled nodes that can be used to offer semantic information.
    To fully utilize these nodes, a range of methods attempt to combine semi-supervised
    approaches with graph neural networks. Pseudo-labeling (Lee et al., [2013](#bib.bib201))
    is a fundamental semi-supervised technique that uses the classifier to produce
    the label distribution of unlabeled examples and then adds appropriately labeled
    examples to the training set (Li et al., [2022d](#bib.bib213); Zhou et al., [2019](#bib.bib466)).
    Another line of semi-supervised learning is consistency regularization (Laine
    and Aila, [2016](#bib.bib198)) that requires two examples to have identical predictions
    under perturbation. This regularization is based on the assumption that each instance
    has a distinct label that is resistant to random perturbations (Feng et al., [2020](#bib.bib93);
    Park et al., [2021](#bib.bib272)). Then, we show several representative works
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5\. A Summary of Methods for Semi-supervised Learning on Graphs. Contrastive
    learning can be considered as a specific kind of consistency learning.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Approach | Pseudo-labeling | Consistency Learning | Knowledge Distillation
    | Active Learning |'
  prefs: []
  type: TYPE_TB
- en: '| Node-level | CoGNet (Li et al., [2022d](#bib.bib213)) | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| DSGCN (Zhou et al., [2019](#bib.bib466)) | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GRAND (Feng et al., [2020](#bib.bib93)) |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| AugGCR (Park et al., [2021](#bib.bib272)) |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-level | SEAL (Li et al., [2019b](#bib.bib212)) | ✓ |  |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| InfoGraph (Sun et al., [2020a](#bib.bib336)) |  | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| DSGC (Yang et al., [2022a](#bib.bib409)) |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ASGN (Hao et al., [2020](#bib.bib132)) |  |  | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| TGNN (Ju et al., [2022b](#bib.bib173)) |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| KGNN (Ju et al., [2022d](#bib.bib175)) | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| HGMI (Li et al., [2022a](#bib.bib210)) | ✓ | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ASGNN (Xie et al., [2022b](#bib.bib396)) | ✓ |  |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| DualGraph (Luo et al., [2022a](#bib.bib243)) | ✓ | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GLA (Yue et al., [2022](#bib.bib429)) |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SS (Xie et al., [2022a](#bib.bib395)) | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Cooperative Graph Neural Networks (Li et al., [2022d](#bib.bib213)) (CoGNet).
    CoGNet is a representative pseudo-label-based GNN approach for semi-supervised
    node classification. It employs two GNN classifiers to jointly annotate unlabeled
    nodes. In particular, it calculates the confidence of each node as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (61) |  | $CV(\mathbf{p}_{i})=\mathbf{p}_{i}^{T}\log\mathbf{p}_{i},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{p}_{i}$ denotes the output label distribution. Then it selects
    the pseudo-labels with high confidence generated from one model to supervise the
    optimization of the other model. In particular, the objective for unlabeled nodes
    is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (62) |  | $\mathcal{L}_{CoGNet}=\sum_{i\in\mathcal{V}^{U}}\mathbf{1}_{CV(\mathbf{p}_{i})>\tau}\hat{\mathbf{y}}^{T}_{i}log\mathbf{q}_{i},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\hat{\mathbf{y}}_{i}$ denotes the one-hot formulation of the pseudo-label
    $\hat{y}_{i}=argmax\mathbf{p}_{i}$ and $\mathbf{q}_{i}$ denotes the label distribution
    predicted by the other classifier. $\tau$ is a pre-defined temperature coefficient.
    This cross supervision has been demonstrated effective in (Chen et al., [2021c](#bib.bib52);
    Luo et al., [2021c](#bib.bib245)) to prevent the provision of biased pseudo-labels.
    Moreover, it employs GNNExplainer (Ying et al., [2019](#bib.bib417)) to provide
    additional information from a dual perspective. Here it measures the minimal subgraphs
    where GNN classifiers can still generate the same prediction. In this way, CoGNet
    can illustrate the entire optimization process to enhance our understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dynamic Self-training Graph Neural Network (Zhou et al., [2019](#bib.bib466))
    (DSGCN). DSGCN develops an adaptive manner to utilize reliable pseudo-labels for
    unlabeled nodes. In particular, it allocates smaller weights to samples with lower
    confidence with the additional consideration of class balance. The weight is formulated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (63) |  | $\omega_{i}=\frac{1}{n_{c^{i}}}\max\left(\operatorname{RELU}\left(\mathbf{p}_{i}-\beta\cdot\mathbf{1}\right)\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $n_{c^{i}}$ denotes the number of unlabeled samples assigned to the class
    $c^{i}$. This technique will decrease the impact of wrong pseudo-labels during
    iterative training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph Random Neural Networks (Feng et al., [2020](#bib.bib93)) (GRAND). GRAND
    is a representative consistency learning-based method. It first adds a variety
    of perturbations to the input graph to generate a list of graph views. Each graph
    view $G^{r}$ is sent to a GNN classifier to produce a prediction matrix $\mathbf{P}^{r}=[\mathbf{p}_{1}^{r},\cdots,\mathbf{p}_{N}^{r}]$.
    Then it summarizes these matrices as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (64) |  | $\mathbf{P}=\frac{1}{R}\mathbf{P}^{r}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'To provide more discriminative information and ensure that the matrix is row-normalized,
    GRAND sharpens the summarized label matrix into $\mathbf{P}^{SA}$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (65) |  | $\mathbf{P}^{SA}_{ij}=\frac{\mathbf{P}_{ij}^{1/T}}{\sum_{j^{\prime}=0}\mathbf{P}_{ij^{\prime}}^{1/T}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $T$ is a given temperature parameter. Finally, consistency learning is
    performed by comparing the sharpened summarized matrix with the matrix of each
    graph view. Formally, the objective is:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (66) |  | $\mathcal{L}_{GRAND}=\frac{1}{R}\sum_{r=1}^{R}\sum_{i\in V}&#124;&#124;\mathbf{P}^{SA}_{i}-\mathbf{P}_{i}&#124;&#124;,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: here $\mathcal{L}_{GRAND}$ serves as a regularization which is combined with
    the standard supervised loss.
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation for GNNs with the Consistency Regularization (Park et al., [2021](#bib.bib272))
    (AugGCR). AugGCR begins with the generation of augmented graphs by random dropout
    and mixup of different order features. To enhance the model generalization, it
    borrows the idea of meta-learning to partition the training data, which improves
    the quality of graph augmentation. In addition, it utilizes consistency regularization
    to enhance the semi-supervised node classification.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Graph Representation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The objective of graph classification is to predict the property of the whole
    graph example. Assuming that the training set comprises $N^{l}$ and $N^{u}$ graph
    samples $\mathcal{G}^{l}=\{G^{1},\cdots,G^{N^{l}}\}$ and $\mathcal{G}^{u}=\{G^{N^{l}+1},\cdots,G^{N^{l}+N^{u}}\}$,
    the graph-level supervised loss for labeled data can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (67) |  | $\mathcal{L}_{GSL}=-\frac{1}{\left&#124;\mathcal{G}^{u}\right&#124;}\sum_{G_{j}\in\mathcal{G}^{L}}{\mathbf{y}^{j}}^{T}log\mathbf{p}^{j},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{y}^{j}$ denotes the one-hot label vector for the $j$-th sample
    while $\mathbf{p}^{j}$ denotes the predicted distribution of $G^{j}$. When $N^{u}=0$,
    this objective can be utilized to optimize supervised methods. However, due to
    the shortage of labels in graph data, supervised methods cannot reach exceptional
    performance in real-world applications (Hao et al., [2020](#bib.bib132)). To tackle
    this, semi-supervised graph classification has been developed extensively. These
    approaches can be categorized into pseudo-labeling-based methods, knowledge distillation-based
    methods and contrastive learning-based methods. Pseudo-labeling methods annotate
    graph instances and utilize well-classified graph examples to update the training
    set (Li et al., [2019b](#bib.bib212), [2022a](#bib.bib210)). Knowledge distillation-based
    methods usually utilize a teacher-student architecture, where the teacher model
    conducts graph representation learning without label information to extract generalized
    knowledge while the student model focuses on the downstream task. Due to the restricted
    number of labeled instances, the student model transfers knowledge from the teacher
    model to prevent overfitting (Sun et al., [2020a](#bib.bib336); Hao et al., [2020](#bib.bib132)).
    Another line of this topic is to utilize graph contrastive learning, which is
    frequently used in unsupervised learning. Typically, these methods extract topological
    information from two perspectives (i.e., different perturbation strategies and
    graph encoders), and maximize the similarity of their representations compared
    with those from other examples (Ju et al., [2022b](#bib.bib173); Luo et al., [2022a](#bib.bib243);
    Ju et al., [2022a](#bib.bib172)). Active learning, as a prevalent technique to
    improve the efficiency of data annotation, has also been utilized for semi-supervised
    methods (Hao et al., [2020](#bib.bib132); Xie et al., [2022b](#bib.bib396)). Then,
    we review these methods in detail.
  prefs: []
  type: TYPE_NORMAL
- en: SEmi-supervised grAph cLassification (Li et al., [2019b](#bib.bib212)) (SEAL).
    SEAL treats each graph example as a node in a hierarchical graph. It builds two
    graph classifiers which generate graph representations and conduct semi-supervised
    graph classification respectively. SEAL employs a self-attention module to encode
    each graph into a graph-level representation, and then conducts message passing
    from a graph level for final classification. SEAL can also be combined with cautious
    iteration and active iteration. The former merely utilizes partial graph samples
    to optimize the parameters in the first classifier due to the potential erroneous
    pseudo-labels. The second combines active learning with the model, which increases
    the annotation efficiency in semi-supervised scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'InfoGraph (Sun et al., [2020a](#bib.bib336)). Infograph is the first contrastive
    learning-based method. It maximizes the similarity between summarized graph representations
    and their node representations. In particular, it generates node representations
    using the message passing mechanism and summarizes these node representations
    into a graph representation. Let $\Phi(\cdot,\cdot)$ denote a discriminator to
    distinguish whether a node belongs to the graph, and we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (68) |  | $\mathcal{L}_{InfoGraph}=\sum_{j=1}^{&#124;\mathcal{G}^{l}&#124;+&#124;\mathcal{G}^{u}&#124;}\sum_{i\in\mathcal{G}_{j}}\left[-\operatorname{sp}\left(-\Phi\left(\mathbf{h}_{i}^{j},\mathbf{z}^{j}\right)\right)\right]-\frac{1}{&#124;N_{i}^{j}&#124;}\sum_{i^{\prime
    j^{\prime}}\in N_{i}^{j}}\left[\operatorname{sp}\left(\Phi\left(\mathbf{h}_{i^{\prime}}^{j^{\prime}},\mathbf{z}^{j}\right)\right)\right],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\operatorname{sp}(\cdot)$ denotes the softplus function. $N_{i}^{j}$
    denotes the negative node set where nodes are not in $G^{j}$. This mutual information
    maximization formulation is originally developed for unsupervised learning and
    it can be simply extended for semi-supervised graph classification. In particular,
    InfoGraph utilizes a teacher-student architecture that compares the representation
    across the teacher and student networks. The contrastive learning objective serves
    as a regularization by combining with supervised loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dual Space Graph Contrastive Learning (Yang et al., [2022a](#bib.bib409)) (DSGC).
    DSGC is a representative contrastive learning-based method. It utilizes two graph
    encoders. The first is a standard GNN encoder in the Euclidean space and the second
    is the hyperbolic GNN encoder. The hyperbolic GNN encoder first converts graph
    embeddings into hyperbolic space and then measures the distance based on the length
    of geodesics. DSGC compares graph embeddings in the Euclidean space and hyperbolic
    space. Assuming the two GNNs are named as $f_{1}(\cdot)$ and $f_{2}(\cdot)$, the
    positive pair is denoted as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (69) |  | $\begin{array}[]{c}\mathbf{z}^{j}_{E\rightarrow H}=\exp_{\mathbf{o}}^{c}(f_{1}(G^{j})),\\
    \mathbf{z}^{j}_{H}=\exp_{\mathbf{o}}^{c}\left(f_{2}(G^{j})\right).\end{array}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Then it selects one labeled sample and $N_{B}$ unlabeled sample $G^{j}$ for
    graph contrastive learning in the hyperbolic space. In formulation,
  prefs: []
  type: TYPE_NORMAL
- en: '| (70) |  | $\displaystyle\mathcal{L}_{DSGC}$ | $\displaystyle=-\log\frac{\mathrm{e}^{d^{H}\left(\mathbf{h}^{i}_{H},\mathbf{z}^{i}_{E\rightarrow
    H}\right)/\tau}}{\mathbf{e}^{d^{H}\left(\mathbf{z}^{i}_{H},\mathbf{z}^{i}_{E\rightarrow
    H}\right)/\tau}+\sum_{i=1}^{N}\mathrm{e}^{d_{\mathbb{D}}\left(\mathbf{z}^{i}_{E\rightarrow
    H},\mathbf{z}^{j}_{H}\right)/\tau}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\frac{\lambda_{u}}{N}\sum_{i=1}^{N}\log\frac{\mathrm{e}^{d_{\mathbb{D}}^{u}\left(\mathbf{z}^{j}_{H},\mathbf{z}^{j}_{E\rightarrow
    H}\right)/\tau}}{\mathrm{e}^{d_{\mathbb{D}}^{u}\left(\mathbf{z}^{j}_{H},\mathbf{z}^{j}_{E\rightarrow
    H}\right)/\tau}+\mathrm{e}^{d_{\mathbb{D}}\left(\mathbf{z}^{i}_{H},\mathbf{z}^{j}_{E\rightarrow
    H}\right)/\tau}},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{z}^{i}_{E\rightarrow H}$ and $\mathbf{z}^{i}_{H}$ denote the
    embeddings for labeled graph sample $G^{i}$ and $d^{H}(\cdot)$ denotes a distance
    metric in the hyperbolic space. This contrastive learning objective maximizes
    the similarity between embeddings learned from two encoders compared with other
    samples. Finally, the contrastive learning objective can be combined with the
    supervised loss to achieve effective semi-supervised contrastive learning.
  prefs: []
  type: TYPE_NORMAL
- en: Active Semi-supervised Graph Neural Network (Hao et al., [2020](#bib.bib132))
    (ASGN). ASGN utilizes a teacher-student architecture with the teacher model focusing
    on representation learning and the student model targeting at molecular property
    prediction. In the teacher model, ASGN first employs a message passing neural
    network to learn node representations under the reconstruction task and then borrows
    the idea of balanced clustering to learn graph-level representations in a self-supervised
    fashion. In the student model, ASGN utilizes label information to monitor the
    model training based on the weights of the teacher model. In addition, active
    learning is also used to minimize the annotation cost while maintaining sufficient
    performance. Typically, the teacher model seeks to provide discriminative graph-level
    representations without labels, which transfer knowledge to the student model
    to overcome the potential overfitting in the presence of label scarcity.
  prefs: []
  type: TYPE_NORMAL
- en: Twin Graph Neural Networks (Ju et al., [2022b](#bib.bib173)) (TGNN). TGNN also
    uses two graph neural networks to give different perspectives to learn graph representations.
    Differently, it adopts a graph kernel neural network to learn graph-level representations
    in virtue of random walk kernels. Rather than directly enforcing representation
    from two modules to be similar, TGNN exchanges information by contrasting the
    similarity structure of the two modules. In particular, it constructs a list of
    anchor graphs, $G^{a_{1}},G^{a_{2}},\cdots,G^{a_{M}}$, and utilizes two graph
    encoders to produce their embeddings, i.e., $\left\{z^{a_{m}}\right\}_{m=1}^{M}$,
    $\left\{w^{a_{m}}\right\}_{m=1}^{M}$. Then it calculates the similarity distribution
    between each unlabeled graph and anchor graphs for two modules. Formally,
  prefs: []
  type: TYPE_NORMAL
- en: '| (71) |  | $p_{m}^{j}=\frac{\exp\left(\cos\left(z^{j},z^{a_{m}}\right)/\tau\right)}{\sum_{m^{\prime}=1}^{M}\exp\left(\cos\left(z^{j},z^{a_{m^{\prime}}}\right)/\tau\right)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (72) |  | $q_{m}^{j}=\frac{\exp\left(\cos\left(\mathbf{w}^{j},\mathbf{w}^{a_{m}}\right)/\tau\right)}{\sum_{m^{\prime}=1}^{M}\exp\left(\cos\left(\mathbf{w}^{j},\mathbf{w}^{a_{m^{\prime}}}\right)/\tau\right)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Then, TGNN minimizes the distance between distributions from different modules
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (73) |  | $\mathcal{L}_{TGNN}=\frac{1}{\left&#124;\mathcal{G}^{U}\right&#124;}\sum_{G^{j}\in\mathcal{G}^{u}}\frac{1}{2}\left(D_{\mathrm{KL}}\left(\mathbf{p}^{j}\&#124;\mathbf{q}^{j}\right)+D_{\mathrm{KL}}\left(\mathbf{q}^{j}\&#124;\mathbf{p}^{j}\right)\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which serves as a regularization term to combine with the supervised loss.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section introduces semi-supervised learning for graph representation learning
    and we provide the summary as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques. Classic node classification aims to conduct transductive learning
    on graphs with access to unlabeled data, which is a natural semi-supervised problem.
    Semi-supervised graph classification aims to relieve the requirement of abundant
    labeled graphs. Here, a variety of semi-supervised methods have been put forward
    to achieve better performance under the label scarcity. Typically, they try to
    integrate semi-supervised techniques such as active learning, pseudo-labeling,
    consistency learning, and consistency learning with graph representation learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and Limitations. Despite their great success, the performance of
    these methods is still unsatisfactory, especially in graph-level representation
    learning. For example, DSGC can only achieve an accuracy of 57% in a binary classification
    dataset REDDIT-BINARY. Even worse, label scarcity often accompanies by unbalanced
    datasets and potential domain shifts, which provides more challenges from real-world
    applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Works. In the future, we expect that these methods can be applied to
    different problems such as molecular property predictions. There are also works
    to extend graph representation learning in more realistic scenarios like few-shot
    learning (Ma et al., [2020b](#bib.bib250); Chauhan et al., [2020](#bib.bib42)).
    A higher accuracy is always anticipated for more advanced and effective semi-supervised
    techniques.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 8\. Graph Self-supervised Learning By Jingyang Yuan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides supervised or semi-supervised methods, self-supervised learning (SSL)
    also has shown its powerful capability in data mining and representation embedding
    recent years. In this section we investigated Graph Neural Networks based on SSL,
    and provided a detailed introduction to a few typical models. Graph SSL methods
    usually has an unified pipeline, which includes pretext tasks and downstream tasks.
    Pretext tasks help model encoder to learn better representation, as a premise
    of better performance in downstream tasks. So a delicate design of pretext task
    is crucial for Graph SSL. We would firstly introduce overall framework of Graph
    SSL in Section [8.1](#S8.SS1 "8.1\. Overall framework ‣ 8\. Graph Self-supervised
    Learning By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph Representation
    Learning"), then introduce the two kind of pretext task design, generation-based
    methods and contrast-Based methods respectively in Section [8.2](#S8.SS2 "8.2\.
    Generation-based pretext task design ‣ 8\. Graph Self-supervised Learning By Jingyang
    Yuan ‣ A Comprehensive Survey on Deep Graph Representation Learning") and [8.3](#S8.SS3
    "8.3\. Contrast-Based pretext task design ‣ 8\. Graph Self-supervised Learning
    By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph Representation Learning").
    A summarization is provided in Table [6](#S8.T6 "Table 6 ‣ 8\. Graph Self-supervised
    Learning By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph Representation
    Learning").
  prefs: []
  type: TYPE_NORMAL
- en: Table 6\. A Summary of Methods for Self-supervised Learning on Graphs. ”PT”,
    ”CT” and ”UFE” mean ”Pre-training”, ”Collaborative Train” and ”Unsupervised Feature
    Extracting” respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Approach | Augmentation Scheme | Training Scheme | Generation Target |
    Objective Function |'
  prefs: []
  type: TYPE_TB
- en: '| Generation-based | Graph Completion (You et al., [2020](#bib.bib420)) | Feature
    Mask | PT/CT | Node Feature | - |'
  prefs: []
  type: TYPE_TB
- en: '| AttributeMask (Jin et al., [2020a](#bib.bib167)) | Feature Mask | PT/CT |
    PCA Node Feature | - |'
  prefs: []
  type: TYPE_TB
- en: '| AttrMasking (Hu et al., [2019](#bib.bib146)) | Feature Mask | PT | Node/Edge
    Feature | - |'
  prefs: []
  type: TYPE_TB
- en: '| MGAE (Wang et al., [2017](#bib.bib359)) | No Augmentation | CT | Node Feature
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| GAE (Kipf and Welling, [2016b](#bib.bib184)) | Feature Noise | UFE | Adjacency
    Matrix | - |'
  prefs: []
  type: TYPE_TB
- en: '| Contrast-based | DeepWalk (Perozzi et al., [2014](#bib.bib277)) | Random
    Walk | UFE | - | SkipGram |'
  prefs: []
  type: TYPE_TB
- en: '| LINE (Tang et al., [2015b](#bib.bib345)) | Random Walk | UFE | - | Jensen-Shannon
    |'
  prefs: []
  type: TYPE_TB
- en: '| GCC (Qiu et al., [2020a](#bib.bib287)) | Random Walk | PT/URL | - | InfoNCE
    |'
  prefs: []
  type: TYPE_TB
- en: '| SimGCL (Yu et al., [2022](#bib.bib423)) | Embedding Noise | UFE | - | InfoNCE
    |'
  prefs: []
  type: TYPE_TB
- en: '| SimGRACE (Xia et al., [2022b](#bib.bib392)) | Model Noise | UFE | - | InfoNCE
    |'
  prefs: []
  type: TYPE_TB
- en: '| GCA (Zhu et al., [2021](#bib.bib472)) | Feature Masking & Strcture Adjustment
    | URL | - | InfoNCE |'
  prefs: []
  type: TYPE_TB
- en: '| BGRL (Grill et al., [2020](#bib.bib121)) | Feature Masking & Strcture Adjustment
    | URL | - | BYOL |'
  prefs: []
  type: TYPE_TB
- en: 8.1\. Overall framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider a featured graph $\mathcal{G}$, we denote a graph encoder $f$ to learn
    representation of graph, and a pretext decoder $g$ with specific architecture
    in different pretext tasks. Then the pretext self-supervised learning loss can
    be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (74) |  | $\mathcal{L}_{total}=E_{\mathcal{G}\sim\mathcal{D}}[\mathcal{L}_{ssl}(g,f,\mathcal{G})],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{D}$ denotes the distribution of featured graph $\mathcal{G}$.
    By minimizing $\mathcal{L}_{overall}$, we can learn encoder $f$ with capacity
    to produce high-quality embedding. As for downstream tasks, we denote a graph
    decoder $d$ which transforms the output of graph encoder $f$ into model prediction.
    The loss of downstream tasks can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (75) |  | $\mathcal{L}_{sup}=\mathcal{L}_{sup}(d,f,\mathcal{G};y),$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $y$ is the ground truth in downstream tasks. We can obvious that $\mathcal{L}_{sup}$
    is a typical supervised loss. To ensure the model achieve wise graph representation
    extraction and optimistic prediction performance, $\mathcal{L}_{ssl}$ and $\mathcal{L}_{sup}$
    have to be minimized simultaneously. We introduce 3 different ways to minimize
    the two loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-training. This strategy has two steps. In pre-training step, the $\mathcal{L}_{ssl}$
    is minimized to get $g^{*}$ and $f^{*}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (76) |  | $g^{*},f^{*}=\underset{g,f}{\arg\min}\mathcal{L}_{ssl}(g,f,\mathcal{D}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Then the parameter of $f^{*}$ is kept to continue training in pretext supervised
    learning progress. The supervised loss is minimized to get final parameters of
    $f$ and $d$.
  prefs: []
  type: TYPE_NORMAL
- en: '| (77) |  | $\underset{d,f}{\min}\mathcal{L}_{ssl}(d,f&#124;_{f_{0}=f^{*}},\mathcal{G};y).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Collaborative Train. In this strategy, $\mathcal{L}_{ssl}$ and $\mathcal{L}_{sup}$
    are optimized simultaneously. A hyperparameter $\alpha$ is used to balance the
    contribution of pretext task loss and downstream task loss. The overall minimization
    strategy is like traditional supervised strategy with a pretext task regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (78) |  | $\underset{g,f,d}{\min}[\mathcal{L}_{ssl}(g,f,\mathcal{G})+\alpha\mathcal{L}_{sup}(d,f,\mathcal{G};y)].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Unsupervised Feature Extracting. This strategy is similar to Pre-training and
    Fine-tuning strategy in first step to minimize pretext task loss $\mathcal{L}_{ssl}$
    and get $f^{*}$. However, when minimizing downstream loss $\mathcal{L}_{sup}$,
    the encoder $f^{*}$ is fixed. Also, the training graph data are on same dataset,
    which differs from Pre-training and Fine-tuning strategy. The formulation is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (79) |  | $g^{*},f^{*}=\underset{g,f}{\arg\min}\mathcal{L}_{ssl}(g,f,\mathcal{D}),\\
    $ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (80) |  | $\underset{d}{\min}\mathcal{L}_{sup}(d,f^{*},\mathcal{G};y).\\
    $ |  |'
  prefs: []
  type: TYPE_TB
- en: 8.2\. Generation-based pretext task design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If a model with encoder-decoder structure can reproduce certain graph feature
    from incomplete or perturbed graph, it indicated the encoder has the ability to
    extract useful graph representation. This motivation derived from Autoencoder (Hinton
    and Salakhutdinov, [2006](#bib.bib140)) which originally learns on image dataset.
    In such a case, Equation [76](#S8.E76 "In 8.1\. Overall framework ‣ 8\. Graph
    Self-supervised Learning By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph
    Representation Learning") can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (81) |  | $\underset{g,f}{\min}\mathcal{L}_{ssl}(g(f(\hat{\mathcal{G}})),\mathcal{G}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $f(\cdot)$ and $g(\cdot)$ stand for the representation encoder and rebuilding
    decoder. However, for graph dataset, feature information and structure information
    are both important composition suitable to be rebuilt. So generation-based pretext
    can be divided into two categories: feature rebuilding and structure rebuilding.
    We introduce several outstanding models in followed part.'
  prefs: []
  type: TYPE_NORMAL
- en: Graph Completion (You et al., [2020](#bib.bib420)) is one of a representative
    method about feature rebuilding. They mask some node features to generate an incomplete
    graph. Then the pretext task is set as predicting the removed node features. As
    shown in Equation [82](#S8.E82 "In 8.2\. Generation-based pretext task design
    ‣ 8\. Graph Self-supervised Learning By Jingyang Yuan ‣ A Comprehensive Survey
    on Deep Graph Representation Learning"), this method can be formulated as a special
    case of Equation [82](#S8.E82 "In 8.2\. Generation-based pretext task design ‣
    8\. Graph Self-supervised Learning By Jingyang Yuan ‣ A Comprehensive Survey on
    Deep Graph Representation Learning"), letting $\mathcal{\hat{G}}=(A,\hat{X})$
    and replacing $\mathcal{G}\xrightarrow{}X$. The loss function is often Mean Squared
    Error or Cross Entropy, depending on the feature is continuous or binary.
  prefs: []
  type: TYPE_NORMAL
- en: '| (82) |  | $\underset{g,f}{\min}\ \mathbf{MSE}(g(f(\hat{\mathcal{G}})),\mathbf{X}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Other works make some changes about feature setting. For example, AttrMasking (Hu
    et al., [2019](#bib.bib146)) aims to rebuild both node representation and edge
    representation, AttributeMask (Jin et al., [2020a](#bib.bib167)) preprocess $X$
    firstly by PCA to reduce the complexity of rebuilding features.
  prefs: []
  type: TYPE_NORMAL
- en: In the other hand, MGAE (Wang et al., [2017](#bib.bib359)) modify the original
    graph by adding noise in node representation, motivated by denoising autoencoder (Vincent
    et al., [2010](#bib.bib354)). As shown in Equation [82](#S8.E82 "In 8.2\. Generation-based
    pretext task design ‣ 8\. Graph Self-supervised Learning By Jingyang Yuan ‣ A
    Comprehensive Survey on Deep Graph Representation Learning"), we can also consider
    MGAE as an implement of Equation [76](#S8.E76 "In 8.1\. Overall framework ‣ 8\.
    Graph Self-supervised Learning By Jingyang Yuan ‣ A Comprehensive Survey on Deep
    Graph Representation Learning") where $\mathcal{\hat{G}}=(A,\hat{X})$ and $\mathcal{G}\xrightarrow{}X$.
    $\hat{X}$ stands for perturbed node representation. Since the noise are independent
    and random, the encoder are more robust to feature input.
  prefs: []
  type: TYPE_NORMAL
- en: '| (83) |  | $\underset{g,f}{\min}\ \mathbf{BCE}(g(f(\hat{\mathcal{G}})),\mathbf{A}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: As for structure rebuilding methods, GAE (Kipf and Welling, [2016b](#bib.bib184))
    is the simplest instance, which can be regard as an implement of Equation [76](#S8.E76
    "In 8.1\. Overall framework ‣ 8\. Graph Self-supervised Learning By Jingyang Yuan
    ‣ A Comprehensive Survey on Deep Graph Representation Learning") where $\mathcal{\hat{G}}=\mathcal{G}$
    and $\mathcal{G}\xrightarrow{}A$. A is the adjacency matrix of graph. Similar
    with feature rebuilding method, GAE compresses raw node representation vectors
    into low-dimensional embedding with its encoder. Then the adjacency matrix is
    rebuilt by computing node embedding similarity. Loss function is set to error
    between ground-truth adjacency matrix and the recovered one, to help model rebuild
    correct graph structure.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3\. Contrast-Based pretext task design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The mutual information maximization principle, which implement self-supervising
    by predicting the similarity between the two augmented views, forms the foundation
    of contrast-based approaches. Since mutual information represent the degree of
    correlation between two samples, we can maximize it in augmented pairs and minimize
    it in random-selected pairs.
  prefs: []
  type: TYPE_NORMAL
- en: The contrast-based graph SSL taxonomy can be formulated as Equation [84](#S8.E84
    "In 8.3\. Contrast-Based pretext task design ‣ 8\. Graph Self-supervised Learning
    By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph Representation Learning").
    The discriminator that calculates similarity of sample pairs is indicated by pretext
    decoder $g$. $\mathcal{G}^{(1)}$ and $\mathcal{G}^{(2)}$ are two variants of $G$
    that have been augmented. Since graph contrastive learning methods differ from
    each other in 1) view generation, 2) MI estimation method we introduce this methodology
    in these 2 perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: '| (84) |  | $\underset{g,f}{\min}\mathcal{L}_{ssl}(g[f(\hat{\mathcal{G}}^{(1)}),f(\hat{\mathcal{G}}^{(2)})]).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 8.3.1\. View generation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Traditional pipeline of contrastive learning-based models is first augmenting
    the graph by well-crafted empirical methods, and then maximizing the consistency
    between different augmentations. Following methods in computer vision domain and
    considering non-Euclidean structure of graph data, typical graph augmentation
    methods aim to modify graph topologically or representatively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given graph $\mathcal{G}=(A,X)$, the topologically augmentation methods usually
    modify the adjacency matrix $A$, which can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (85) |  | $\hat{A}=\mathscr{T}_{A}(A),$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathscr{T}_{A}(\cdot)$ is the transform function of adjacency matrix.
    Topology augmentation methods has many variants, in which the most popular one
    is edge modification, given by $\mathscr{T}_{A}(A)=P\circ A+Q\circ(1-A)$. $P$
    and $Q$ are two matrix representing edge dropping and adding respectively. Another
    method, graph diffusion, connect nodes with their k-hop neighhors with specific
    weight, defined as: $\mathscr{T}_{A}(A)=\sum^{\infty}_{k=0}\alpha_{k}T^{k}$. where
    $\alpha$ and $T$ are coefficient and transition matrix. Graph diffusion method
    can integrate broad topological information with local structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the other hand, the representative augmentation modify the node representation
    directly, which can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (86) |  | $\hat{X}=\mathscr{T}_{X}(X),$ |  |'
  prefs: []
  type: TYPE_TB
- en: usually $\mathscr{T}_{X}(\cdot)$ can be a simple masking operater, a.k.a. $\mathscr{T}_{X}(X)=M\circ
    X$ and $M\in\{0,1\}^{N\times D}$. Based on such mask strategy, some method propose
    ways to improve performance. GCA (Zhu et al., [2021](#bib.bib472)) preserves critical
    nodes while giving less significant nodes a larger masking probability, where
    significance is determined by node centrality.
  prefs: []
  type: TYPE_NORMAL
- en: 'As introduced before, the paradigm of augmentation has been prove to be effective
    in contrastive learning view generation. However, given the variety of graph data,
    it is challenging to maintain semantics properly during augmentations. In order
    to preserve valuable nature in specific graph dataset, There are currently three
    mainly-used methods: picking by trial-and-errors, trying laborious search or seeking
    domain-specific information as guidance (Luo et al., [2022b](#bib.bib244); Ju
    et al., [2023a](#bib.bib170)). It is clear that such complicated augmentation
    methods constrain the effectiveness and widespread application of graph contrastive
    learning. So many newest works question the necessity of augmentation and seek
    other contrastive views generation methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SimGCL (Yu et al., [2022](#bib.bib423)) is one of outstanding works challenging
    the effectiveness of graph augmentation. The author find that noise can be a substitution
    to augmentation to produce graph views in specific task such as recommendation.
    After doing ablation study about augmentation and InfoNCE (Xie et al., [2022c](#bib.bib398)),
    they find that the InfoNCE loss, not the augmentation of the graph, is what makes
    the difference. It can be further explained by the importance of distribution
    uniformity. The contrastive learning enhance model representation ability by intensifying
    two characteristics: The alignment of features from positive samples and the uniformity
    of the normalized feature distribution. SimGCL directly add random noises to node
    embeddings as augmentation, to control the uniformity of the representation distribution
    in a more effective way:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (87) |  | $\displaystyle\textbf{e}^{(1)}_{i}=\textbf{e}_{i}+\epsilon^{(1)}*\mathbf{\tau}^{(1)}_{i}$
    | $\displaystyle,\quad\textbf{e}^{(2)}_{i}=\textbf{e}_{i}+\epsilon^{(2)}*\mathbf{\tau}^{(2)}_{i},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\epsilon$ | $\displaystyle\sim\mathcal{N}(0,\sigma^{2}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\textbf{e}_{i}$ is a node representation in embedding space, $\mathbf{\tau}^{(1)}_{i}$
    and $\mathbf{\tau}^{(2)}_{i}$ are two random sampled unit vector. The experiment
    results indicate that SimGCL performs better than its graph augmentation-based
    competitors and means, while training time is significantly decreased.
  prefs: []
  type: TYPE_NORMAL
- en: 'SimGRACE (Xia et al., [2022b](#bib.bib392)) is another graph contrastive learning
    framework without data augmentation. Motivated by the observation that despite
    encoder disrupted, graph data can effectively maintain their semantics, SimGRACE
    take GNN with its modified version as encoder to produce two contrastive embedding
    views by the same graph input. For GNN encoder $f(\cdot;\theta)$, the two contrastive
    embedding views $\textbf{e},\textbf{e}^{\prime}$ can be computed by:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (88) |  | $\displaystyle\textbf{e}^{(1)}=f(\mathcal{G};\theta),$ | $\displaystyle\textbf{e}^{(2)}=f(\mathcal{G};\theta+\epsilon\cdot\Delta\theta),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\Delta\theta_{l}$ | $\displaystyle\sim\mathcal{N}(0,\sigma_{l}^{2}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\Delta\theta_{l}$ represents GNN parameter perturbation $\Delta\theta$
    in the $l$th layer. SimGRACE can improve alignment and uniformity simutanously,
    proving its capacity of producing high-quality embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2\. MI estimation method.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The mutual information $I(x,y)$ measures the information that x and y share,
    given a pair of random variables $(x,y)$. As discussed before, mutual information
    is a significant component of contrast-based method by formulating the loss function.
    Mathematically rigorous MI is defined on the probability space, we can formulate
    mutual information between a pair of instances $(x_{i},x_{j})$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (89) |  | $\displaystyle I(x,y)$ | $\displaystyle=D_{KL}(p(x,y)&#124;&#124;p(x)p(y))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=E_{p(x,y)}[\log{\frac{p(x,y)}{p(x)p(y)}}].$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'However, directly compute Equation [89](#S8.E89 "In 8.3.2\. MI estimation method.
    ‣ 8.3\. Contrast-Based pretext task design ‣ 8\. Graph Self-supervised Learning
    By Jingyang Yuan ‣ A Comprehensive Survey on Deep Graph Representation Learning")
    is quiet difficult, so we introduce several different types of estimation for
    MI:'
  prefs: []
  type: TYPE_NORMAL
- en: 'InfoNCE. Noise-contrastive estimator is a widely used lower bound MI estimatior.
    Given a positive sample $y$ and several negative sample $y^{\prime}_{i}$, a noise-contrastive
    estimator can be fomulated as (Zhu et al., [2020b](#bib.bib471))(Qiu et al., [2020a](#bib.bib287)):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (90) |  | $\mathcal{L}=-I(x,y)=-E_{p(x,y)}[\log{\frac{e^{g(x,y)}}{e^{g(x,y)}+\sum_{i}{e^{g(x,y^{\prime}_{i})}}}}],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: usually the kernal function $g(\cdot)$ can be cosine similarity or dot product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Triplet Loss. Intuitively, we can push the similarity between positive samples
    and negative samples differ by a certain distance. So we can define the loss function
    in the following manner (Jiao et al., [2020](#bib.bib163)):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (91) |  | $\mathcal{L}=E_{p(x,y)}[{\max(g(x,y)-g(x,y^{\prime})+\epsilon,0)}],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\epsilon$ is a hyperparameter. This function is straightforward and easy
    to compute.
  prefs: []
  type: TYPE_NORMAL
- en: 'BYOL Loss. Estimation without negative samples is investigated by BYOL (Grill
    et al., [2020](#bib.bib121)). The estimator is Asymmetrical structured:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (92) |  | $\mathcal{L}=E_{p(x,y)}[2-2\frac{g(x)\cdot y}{\&#124;g(x)\&#124;\&#124;y\&#124;}],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: note that encoder $g$ should keep the dimension of input and output the same.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section introduces graph self-supervised learning and we provide the summary
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques. Differ from classic supervised learning and semi-supervised learning,
    self-supervised learning increase model generalization ability and robustness,
    whilst decrease label reliance. Graph SSL utilize pretext tasks to extract inherent
    information in representation distribution. Typical Graph SSL methods can be devide
    into generation-based and contrast-based. Generation-based methods learns an encoder
    with ability to reconstruct graph as precise as possible, motivated by Autoencoder.
    Contrast-based methods attract significant interests recently, they learns an
    encoder to minimizing mutual information between relevant instance and maximizing
    mutaul information between unrelated instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and Limitations. Although graph SSL has achieved superior performance
    in many tasks, its theoretical basis is not so solid. Many well-known methods
    are just validated through experiments without explaining theoretical or coming
    up with mathematical proof. It is imperative to establish a strong theoretical
    foundation for graph SSL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Works. In the future we expect more graph ssl methods designed essentially
    by theoretical proof, without dedicate designed augment process or pretext tasks
    by intuition. This will bring us more definite mathematical properties and less
    ambiguous empirical sense. Also, graphs are a prevalent form of data representation
    across diverse domains, yet obtaining manual labels can be prohibitively expensive.
    Expanding the applications of graph SSL to broader fields is a promising avenue
    for future research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 9\. Graph Structure Learning By Jianhao Shen
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graph structure determines how node features propagate and affect each other,
    playing a crucial role in graph representation learning. In some scenarios the
    provided graph is incomplete, noisy, or even has no structure information at all.
    Recent research also finds that graph adversarial attacks (i.e., modifying a small
    number of node features or edges), can degrade learned representations significantly.
    These issues motivate graph structure learning (GSL), which aims to learn a new
    graph structure to produce optimal graph representations. According to how edge
    connectivity is modeled, there are three different approaches in GSL, namely metric-based
    approaches, model-based approaches, and direct approaches. Besides edge modeling,
    regularization is also a common trick to make the learned graph satisfy some desired
    properties. We first present the basic framework and regularization methods for
    GSL in Sec. [9.1](#S9.SS1 "9.1\. Overall Framework ‣ 9\. Graph Structure Learning
    By Jianhao Shen ‣ A Comprehensive Survey on Deep Graph Representation Learning")
    and Sec. [9.2](#S9.SS2 "9.2\. Regularization ‣ 9\. Graph Structure Learning By
    Jianhao Shen ‣ A Comprehensive Survey on Deep Graph Representation Learning"),
    respectively, and then introduce different categories of GSL in Sec. [9.3](#S9.SS3
    "9.3\. Metric-based Methods ‣ 9\. Graph Structure Learning By Jianhao Shen ‣ A
    Comprehensive Survey on Deep Graph Representation Learning"), [9.4](#S9.SS4 "9.4\.
    Model-based Methods ‣ 9\. Graph Structure Learning By Jianhao Shen ‣ A Comprehensive
    Survey on Deep Graph Representation Learning") and [9.5](#S9.SS5 "9.5\. Direct
    Methods ‣ 9\. Graph Structure Learning By Jianhao Shen ‣ A Comprehensive Survey
    on Deep Graph Representation Learning"). We summarize GSL approaches in Table
    [7](#S9.T7 "Table 7 ‣ 9.1\. Overall Framework ‣ 9\. Graph Structure Learning By
    Jianhao Shen ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 9.1\. Overall Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We denote a graph by $\mathcal{G}=(\mathbf{A},\mathbf{X})$, where $\mathbf{A}\in\mathbb{R}^{N\times
    N}$ is the adjacency matrix and $\mathbf{X}\in\mathbb{R}^{N\times M}$ is the node
    feature matrix with $M$ being the dimension of each node feature. A graph encoder
    $f_{\theta}$ learns to represent the graph based on node features and graph structure
    for task-specific objective $\mathcal{L}_{t}(f_{\theta}(\mathbf{A},\mathbf{X}))$.
    In the GSL setting, there is also a graph structure learner which aims to build
    a new graph adjacency matrix $\mathbf{A}^{*}$ to optimize the learned representation.
    Besides the task-specific objective, a regularization term can be added to constrain
    the learned structure. So the overall objective function of GSL can be formulated
    as
  prefs: []
  type: TYPE_NORMAL
- en: '| (93) |  | $\min_{\theta,\mathbf{A}^{*}}\mathcal{L}=\mathcal{L}_{t}(f_{\theta}(\mathbf{A}^{*},\mathbf{X}))+\lambda\mathcal{L}_{r}(\mathbf{A}^{*},\mathbf{A},\mathbf{X}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}_{t}$ is the task-specific objective, $\mathcal{L}_{r}$ is
    the regularization term and $\lambda$ is a hyperparameter for the weight of regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7\. Summary of graph structure learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Structure Learning | Regularization |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sparsity | Low-rank | Smoothness |'
  prefs: []
  type: TYPE_TB
- en: '| Metric-based | AGCN (Li et al., [2018b](#bib.bib215)) | Mahalanobis distance
    |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GRCN (Yu et al., [2021b](#bib.bib422)) | Inner product | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CAGCN (Zhu et al., [2020c](#bib.bib473)) | Inner product | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GNNGUARD (Zhang and Zitnik, [2020](#bib.bib441)) | Cosine similarity |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| IDGL (Chen et al., [2020g](#bib.bib53)) | Cosine similarity | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| HGSL (Zhao et al., [2021b](#bib.bib452)) | Cosine similarity | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GDC (Gasteiger et al., [2019](#bib.bib109)) | Graph diffusion | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Model-based | GLN (Pilco and Rivera, [2019](#bib.bib278)) | Recurrent blocks
    |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GLCN (Jiang et al., [2019](#bib.bib160)) | One-layer neural network | ✓ |  |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| NeuralSparse (Zheng et al., [2020](#bib.bib458)) | Multi-layer neural network
    | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GAT (Veličković et al., [2017](#bib.bib352)) | Self-attention |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GaAN (Zhang et al., [2018b](#bib.bib436)) | Gated attention |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| hGAO (Gao and Ji, [2019](#bib.bib105)) | Hard attention | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| VIB-GSL (Sun et al., [2022](#bib.bib338)) | Dot-product attention | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MAGNA (Wang et al., [2020c](#bib.bib366)) | Graph attention diffusion |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Direct | GLNN (Gao et al., [2020](#bib.bib107)) | MAP estimation | ✓ |  |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| GSML (Wan and Kokel, [2021](#bib.bib358)) | Bilevel optimization | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| BGCNN (Zhang et al., [2019c](#bib.bib443)) | Bayesion optimization |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| VGCN (Elinas et al., [2020](#bib.bib84)) | Stochastic variational inference
    |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 9.2\. Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of regularization is to constrain the learned graph to satisfy some
    properties by adding some penalties to the learned structure. The most common
    properties used in GSL are sparsity, low lank, and smoothness.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1\. Sparsity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Noise or adversarial attacks will introduce redundant edges into graphs and
    degrade the quality of graph representation. An effective technique to remove
    unnecessary edges is sparsity regularization, i.e., adding a penalty on the number
    of nonzero entries of the adjacency matrix ($\ell_{0}$-norm):'
  prefs: []
  type: TYPE_NORMAL
- en: '| (94) |  | $\mathcal{L}_{sp}=\lVert\mathbf{A}\rVert_{0},$ |  |'
  prefs: []
  type: TYPE_TB
- en: however, $\ell_{0}$-norm is not differentiable so optimizing it is difficult,
    and in many cases $\ell_{1}$-norm is used instead as a convex relaxation. Other
    methods to impose sparsity include pruning and discretization. These processes
    are also called postprocessing since they usually happen after the adjacency matrix
    is learned. Pruning removes part of the edges according to some criteria. For
    example, edges with weights lower than a threshold, or those not in the top-K
    edges of nodes or graph. Discretization is applied to generate graph structure
    by sampling from some distribution. Compared to directly learning edge weights,
    sampling enjoys the advantage to control the generated graph, but has issues during
    optimizing since sampling itself is discrete and hard to optimize. Reparameterization
    and Gumbel-softmax are two useful techniques to overcome such issue, and are widely
    adopted in GSL.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2\. Low Rank
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In real-world graphs, similar nodes are likely to group together and form communities,
    which should lead to a low-rank adjacency matrix. Recent work also finds that
    adversarial attacks tend to increase the rank of the adjacency matrix quickly.
    Therefore, low rank regularization is also a useful tool to make graph representation
    learning more robust:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (95) |  | $\mathcal{L}_{lr}=Rank(\mathbf{A}).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'It is hard to minimize matrix rank directly. A common technique is to optimize
    the nuclear norm, which is a convex envelope of the matrix rank:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (96) |  | $\mathcal{L}_{nc}=\lVert\mathbf{A}\rVert_{*}=\sum_{i}^{N}\sigma_{i},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\sigma_{i}$ are singular values of $\mathbf{A}$. Entezari et al. replaces
    the learned adjacency matrix with rank-r approximation by singular value decomposition
    (SVD) to achieve robust graph learning against adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.3\. Smoothness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A common assumption is that connected nodes share similar features, or in other
    words, the graph is “smooth” as the difference between local neighbors is small.
    The following metric is a natural way to measure graph smoothness:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (97) |  | $\mathcal{L}_{sm}=\frac{1}{2}\sum_{i,j=1}^{N}A_{ij}(x_{i}-x_{j})^{2}=tr(\mathbf{X}^{\top}\mathbf{(D-A)X})=tr(\mathbf{X}^{\top}\mathbf{LX}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{D}$ is the degree matrix of $\mathbf{A}$ and $\mathbf{L}=\mathbf{D-A}$
    is called graph Laplacian. A variant is to use the normalized graph Laplacian
    $\widehat{\mathbf{L}}=\mathbf{D}^{-\frac{1}{2}}\mathbf{LD}^{-\frac{1}{2}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3\. Metric-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Metric-based methods measure the similarity between nodes as the edge weights.
    They follow the basic assumption that similar nodes tend to have connection with
    each other. We show some representative works
  prefs: []
  type: TYPE_NORMAL
- en: 'Adaptive Graph Convolutional Neural Networks (Li et al., [2018b](#bib.bib215))
    (AGCN). AGCN learns a task-driven adaptive graph during training to enable a more
    generalized and flexible graph representation model. After parameterizing the
    distance metric between nodes, AGCN is able to adapt graph topology to the given
    task. It proposes generalized Mahalanobis distance between two nodes with the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (98) |  | $\mathbb{D}(x_{i},x_{j})=\sqrt{(x_{i}-x_{j})^{\top}M(x_{i}-x_{j})},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $M=W_{d}W_{d}^{\top}$ and $W_{d}$ is the trainable weights to minimize
    task-specific objective. Then the Gaussian kernel is used to obtain the adjacency
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (99) |  | $\displaystyle\mathbb{G}_{ij}$ | $\displaystyle=\exp(-\mathbb{D}(x_{i},x_{j})/(2\sigma^{2})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (100) |  | $\displaystyle\hat{A}$ | $\displaystyle=normalize(\mathbb{G}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Graph-Revised Convolutional Network (Yu et al., [2021b](#bib.bib422)) (GRCN).
    GRCN uses a graph revision module to predict missing edges and revise edge weights
    through joint optimization on downstream tasks. It first learns the node embedding
    with GCN and then calculates pair-wise node similarity with the dot product as
    the kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: '| (101) |  | $\displaystyle Z$ | $\displaystyle=GCN_{g}(A,X),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (102) |  | $\displaystyle S_{ij}$ | $\displaystyle=\left\langle z_{i},z_{j}\right\rangle.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The revised adjacency matrix is the residual summation of the original adjacency
    matrix $\hat{A}=A+S$. GRCN also applies a sparsification technique on the similarity
    matrix $S$ to reduce computation cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (103) |  | $S^{(K)}_{ij}=\left\{\begin{aligned} S_{ij},&amp;~{}~{}S_{ij}\in
    topK(S_{i})\\ 0,&amp;~{}~{}S_{ij}\notin topK(S_{i})\end{aligned}\right..$ |  |'
  prefs: []
  type: TYPE_TB
- en: Threshold pruning is also a common strategy for sparsification. For example,
    CAGCN (Zhu et al., [2020c](#bib.bib473)) also uses dot product to measure node
    similarity, and refines the graph structure by removing edges between nodes whose
    similarity is less than a threshold $\tau_{r}$ and adding edges between nodes
    whose similarity is greater than another threshold $\tau_{a}$.
  prefs: []
  type: TYPE_NORMAL
- en: Defending Graph Neural Networks against Adversarial Attacks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '(Zhang and Zitnik, [2020](#bib.bib441)) (GNNGuard). GNNGuard measures similarity
    between a node $u$ and its neighbor $v$ in the $k$-th layer by cosine similarity
    and normalizes node similarity at the node level within the neighborhood as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (104) |  | $\displaystyle s_{uv}^{k}$ | $\displaystyle=\frac{h_{u}^{k}\odot
    h_{v}^{k}}{\&#124;h_{u}^{k}\&#124;_{2}\&#124;h_{v}^{k}\&#124;_{2}},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (105) |  | $\displaystyle\alpha_{uv}^{k}$ | $\displaystyle=\left\{\begin{aligned}
    &amp;s_{uv}^{k}/\sum\nolimits_{v\in\mathcal{N}_{u}}s_{uv}^{k}\times\hat{N}_{u}^{k}/(\hat{N}_{u}^{k}+1),&amp;~{}~{}if~{}~{}u\neq
    v\\ &amp;1/(\hat{N}_{u}^{k}+1),&amp;~{}~{}if~{}~{}u=v\end{aligned}\right.,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{N}_{u}$ denotes the neighborhood of node $u$ and $\hat{N}_{u}^{k}=\sum\nolimits_{v\in\mathcal{N}_{u}}\|s_{uv}^{k}\|_{0}$.
    To stabilize GNN training, it also proposes a layer-wise graph memory by keeping
    part of the information from the previous layer in the current layer. Similar
    to GNNGuard, IDGL (Chen et al., [2020g](#bib.bib53)) uses multi-head cosine similarity
    and mask edges with node similarity smaller than a non-negative threshold, and
    HGSL (Zhao et al., [2021b](#bib.bib452)) generalizes this idea to heterogeneous
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Diffusion Convolution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '(Gasteiger et al., [2019](#bib.bib109)) (GDC). GDC replaces the original adjacency
    matrix with generalized graph diffusion matrix $\mathbf{S}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (106) |  | $\mathbf{S}=\sum_{k=0}^{\infty}\theta_{k}\mathbf{T}^{k},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\theta_{k}$ is the weighting coefficient and $\mathbf{T}$ is the generalized
    transition matrix. To ensure convergence, GDC further requires that $\sum_{k=0}^{\infty}\theta_{k}=1$
    and the eigenvalues of $\mathbf{T}$ lie in $[0,1]$. The random walk transition
    matrix $\mathbf{T}_{rw}=\mathbf{AD}^{-1}$ and the symmetric transition matrix
    $\mathbf{T}_{sym}=\mathbf{D}^{-1/2}\mathbf{AD}^{-1/2}$ are two examples. This
    new graph structure allows graph convolution to aggregate information from a larger
    neighborhood. The graph diffusion acts as a smoothing operator to filter out underlying
    noise. However, in most cases graph diffusion will result in a dense adjacency
    matrix $S$, so sparsification technology like top-k filtering and threshold filtering
    will be applied to graph diffusion. Following GDC, there are some other graph
    diffusion proposed. For example, AdaCAD (Lim et al., [2021](#bib.bib226)) proposes
    Class-Attentive Diffusion, which further considers node features and aggregates
    nodes probably of the same class among K-hop neighbors. Adaptive diffusion convolution (Zhao
    et al., [2021a](#bib.bib451)) (ADC) learns the optimal neighborhood size via optimizing
    a bi-level problem.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4\. Model-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model-based methods parameterize edge weights with more complex models like
    deep neural networks. Compared to metric-based methods, model-based methods offer
    greater flexibility and expressive power.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph Learning Network (Pilco and Rivera, [2019](#bib.bib278)) (GLN). GLN proposes
    a recurrent block to first produce intermediate node embeddings and then merge
    them with adjacency information as the output of this layer to predict the adjacency
    matrix for the next layer. Specifically, it uses convolutional graph operations
    to extract node features, and creates a local-context embedding based on node
    features and the current adjacency matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (107) |  | $\displaystyle H_{int}^{(l)}=\sum_{i=1}^{k}\sigma_{l}(\tau(A^{(l)})H^{(l)}W_{i}^{(l)}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (108) |  | $\displaystyle H_{local}^{(l)}=\sigma_{l}(\tau(A^{(l)})H_{int}^{(l)}U^{(l)}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $W_{i}^{(l)}$ and $U^{(l)}$ are the learnable weights. GLN then predicts
    the next adjacency matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (109) |  | $A^{(l+1)}=\sigma_{l}(M^{(l)}\alpha_{l}(H_{local}^{(l)}){M^{(l)}}^{\top}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Similarly, GLCN (Jiang et al., [2019](#bib.bib160)) models graph structure with
    a softmax layer over the inner product between the difference of node features
    and a learnable vector. NeuralSparse (Zheng et al., [2020](#bib.bib458)) uses
    a multi-layer neural network to generate a learnable distribution from which a
    sparse graph structure is sampled. PTDNet (Luo et al., [2021a](#bib.bib241)) prunes
    graph edges with a multi-layer neural network and penalizes the number of non-zero
    elements to encourage sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph Attention Networks (Veličković et al., [2017](#bib.bib352)) (GAT). Besides
    constructing a new graph to guide the massage passing and aggregation process
    of GNNs, many recent researchers also leverage the attention mechanism to adaptively
    model the relationship between nodes. GAT is the first work to introduce the self-attention
    strategy into graph learning. In each attention layer, the attention weight between
    two nodes is calculated as the Softmax output on the combination of linear and
    non-linear transform of node features:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (110) |  | $\displaystyle e_{ij}$ | $\displaystyle=a(\mathbf{W}\vec{h}_{i},\mathbf{W}\vec{h}_{j}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (111) |  | $\displaystyle\alpha_{ij}$ | $\displaystyle=\frac{exp(e_{ij})}{\sum_{k\in\mathcal{N}_{i}}exp(e_{ik})},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{N}_{i}$ denotes the neighborhood of node $i$,$\mathbf{W}$ is
    learnable linear transform and $a$ is pre-defined attention function. In the original
    implementation of GAT, $a$ is a single-layer neural network with $\mathrm{LeakyReLU}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (112) |  | $a(\mathbf{W}\vec{h}_{i},\mathbf{W}\vec{h}_{j})=\mathrm{LeakyReLU}(\vec{\mathrm{a}}^{\top}[\mathbf{W}\vec{h}_{i}&#124;&#124;\mathbf{W}\vec{h}_{j}]).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The attention weights are then used to guide the message-passing phase of GNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (113) |  | $\vec{h}^{\prime}_{i}=\sigma(\sum_{j\in\mathcal{N}_{i}}\alpha_{ij}\mathbf{W}\vec{h}_{j}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\sigma$ is a nonlinear function. It is beneficial to concatenate multiple
    heads of attention together to get a more stable and generalizable model, so-called
    multi-head attention. The attention mechanism serves as a soft graph structure
    learner which captures important connections within node neighborhoods. Following
    GAT, many recent works propose more effective and efficient graph attention operators
    to improve performance. GaAN (Zhang et al., [2018b](#bib.bib436)) adds a soft
    gate at each attention head to adjust its importance. MAGNA (Wang et al., [2020c](#bib.bib366))
    proposes a novel graph attention diffusion layer to incorporate multi-hop information.
    One drawback of graph attention is that the time and space complexities are both
    $O(N^{3})$. hGAO (Gao and Ji, [2019](#bib.bib105)) performs hard graph attention
    by limiting node attention to its neighborhood. VIB-GSL (Sun et al., [2022](#bib.bib338))
    adopts the information bottleneck principle to guide feature masking in order
    to drop task-irrelevant information and preserve actionable information for the
    downstream task.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5\. Direct Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Direct methods treat edge weights as free learnable parameters. These methods
    enjoy more flexibility but are also more difficult to train. The optimization
    is usually carried out in an alternating way, i.e., iteratively updating the adjacency
    matrix $\mathbf{A}$ and the GNN encoder parameters $\theta$.
  prefs: []
  type: TYPE_NORMAL
- en: 'GLNN (Gao et al., [2020](#bib.bib107)). GLNN uses MAP estimation to learn an
    optimal adjacency matrix for a joint objective function including sparsity and
    smoothness. Specifically, it targets at finding the most probable adjacency matrix
    $\hat{A}$ given graph node features $x$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (114) |  | $\tilde{A}_{MAP}(x)=\mathop{\mathrm{argmax}}\limits_{\hat{A}}f(x&#124;\hat{A})g(\hat{A}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $f(x|\hat{A})$ measures the likelihood of observing $x$ given $\hat{A}$,
    and $g(\hat{A})$ is the prior distribution of $\hat{A}$. GLNN uses sparsity and
    property constraint as prior, and define the likelihood function $f$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (115) |  | $\displaystyle f(x&#124;\hat{A})$ | $\displaystyle=exp(-\lambda_{0}x^{\top}Lx)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (116) |  |  | $\displaystyle=exp(-\lambda_{0}x^{\top}(I-\hat{A})x),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{0}$ is a parameter. This likelihood imposed smoothness assumption
    on the learned graph structure. Some other works also model the adjacency matrix
    in a probabilistic manner. Bayesian GCNN (Zhang et al., [2019c](#bib.bib443))
    adopts a Bayesian framework and treats the observed graph as a realization from
    a family of random graphs. Then it estimates the posterior probablity of labels
    given the observed graph adjacency matrix and features with Monte Carlo approximation.
    VGCN (Elinas et al., [2020](#bib.bib84)) follows a similar formulation and estimates
    the graph posterior through stochastic variational inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph Sparsification via Meta-Learning (Wan and Kokel, [2021](#bib.bib358))
    (GSML). GSML formulates GSL as a meta-leanring problem and uses bi-level optimization
    to find the optimal graph structure. The goal is to find a sparse graph structure
    which leads to high node classification accuracy at the same time given labeled
    and unlabeled nodes. To achieves this, GSML makes the inner optimization as training
    on the node classification task, and targets the outer optimization at the sparsity
    of the graph structure, which formulates the following bi-level optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (117) |  | $\displaystyle\hat{G}^{*}$ | $\displaystyle=\mathop{\mathrm{min}}\limits_{\hat{G}\in\Phi(G)}L_{sps}(f_{\theta^{*}}(\hat{G}),Y_{U}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (118) |  | $\displaystyle s.t.~{}$ | $\displaystyle~{}\theta^{*}=\mathop{\mathrm{argmin}}\limits_{\theta}L_{train}(f_{\theta}(\hat{G}),Y_{L}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In this bi-level optimization problem, $\hat{G}\in\Phi(G)$ are the meta-parameters
    and optimized directly without parameterization. Similarly, LSD-GNN (Franceschi
    et al., [2019](#bib.bib98)) also uses bi-level optimization. It models graph structure
    with a probability distribution over graph and reformulates the bi-level program
    in terms of the continuous distribution parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 9.6\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section and we provide the summary as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Techniques. GSL aims to learn an optimized graph structure for better graph
    representations. It is also used for more robust graph representation against
    adversarial attacks. According to the way of edge modeling, we categorize GSL
    into three groups: metric-based methods, model-based methods, and direct methods.
    Regularization is also a commonly used principle to make the learned graph structure
    satisfy specific properties including sparsity, low-rank and smoothness.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and Limitations. Since there is no way to access the groundtruth
    or optimal graph structure as training data, the learning objective of GSL is
    either indirect (e.g., performance on downstream tasks) or manually designed (e.g.,
    sparsity and smoothness). Therefore, the optimization of GSL is difficult and
    the performance is not satisfying. In addition, many GSL methods are based on
    homophily assumption, i.e., similar nodes are more likely to connect with each
    other. However, many other types of connection exist in the real-world which impose
    great challenges for GSL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Works. In the future we expect more efficient and generalizable GSL methods
    to be applied to large-scale and heterogeneous graphs. Most existing GSL methods
    focus on pair-wise node similarities and thus struggle to scale to large graphs.
    Besides, they often learn homogeneous graph structure, but in many scenarios graphs
    are heterogeneous.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 10\. Social Analysis By Ziyue Qiao
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the real world, there usually exist complex relations and interactions between
    people and multiple entities. Taking people, concrete things, and abstract concepts
    in society as nodes and taking the diverse, changeable, and large-scale connections
    between data as links, we can form massive and complex social information as social
    networks (Tabassum et al., [2018](#bib.bib340); Camacho et al., [2020](#bib.bib34)).
    Compared with traditional data structures such as texts and forms, modeling social
    data as graphs have many benefits. Especially with the arrival of the ”big data”
    era, more and more heterogeneous information are interconnected and integrated,
    and it is difficult and uneconomical to model this information with a traditional
    data structure. The graph is an effective implementation for information integration,
    as it can naturally incorporate different types of objects and their interactions
    from heterogeneous data sources (Shi et al., [2016](#bib.bib318); Moscato and
    Sperlì, [2021](#bib.bib266)). A summarization of social analysis applications
    is provided in Table [8](#S10.T8 "Table 8 ‣ 10\. Social Analysis By Ziyue Qiao
    ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  prefs: []
  type: TYPE_NORMAL
- en: Table 8\. A summarization of social analysis applications
  prefs: []
  type: TYPE_NORMAL
- en: '| Social networks | Node type | Edge type | Applications | References |'
  prefs: []
  type: TYPE_TB
- en: '| Academic Social Network | Author, Publication, Venue, Organization, Keyword
    | Authorship, Co-Author, Advisor-advisee, Citing, Cited, Co-Citing, Publishing
    | Classification/ Clustering | Paper/author classification (Dong et al., [2017](#bib.bib75);
    Wang et al., [2019b](#bib.bib369); Zhang et al., [2019d](#bib.bib433); Qiao et al.,
    [2020a](#bib.bib284)), name disambiguation (Zhang et al., [2018e](#bib.bib445);
    Qiao et al., [2019](#bib.bib282); Chen et al., [2020h](#bib.bib43)) |'
  prefs: []
  type: TYPE_TB
- en: '| Relationship prediction | Co-authorship  (Chuan et al., [2018](#bib.bib60);
    Cho and Yu, [2018](#bib.bib57)), citation relationship  (Yu et al., [2012](#bib.bib426);
    Jiang et al., [2018](#bib.bib162); Wang et al., [2020e](#bib.bib363)), advisor-advisee
    relationship (Liu et al., [2019](#bib.bib229); Zhao et al., [2018](#bib.bib457))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Recommen- dation | Collaborator recommendation (Liu et al., [2018c](#bib.bib237);
    Kong et al., [2017](#bib.bib188), [2016](#bib.bib189)), paper recommendation (Bai
    et al., [2019](#bib.bib15); Sugiyama and Kan, [2010](#bib.bib335)), venue recommendation (Yu
    et al., [2018](#bib.bib425); Margaris et al., [2019](#bib.bib258)) |'
  prefs: []
  type: TYPE_TB
- en: '| Academic Media Network | User, Blog, Article, Image, Video | Following, Like,
    Unlike, Clicked, Viewed, Commented, Reposted | Anomaly detection | Malicious attacks (Sun
    et al., [2020c](#bib.bib339); Liu et al., [2018b](#bib.bib235)), emergency detection (Bian
    et al., [2020](#bib.bib22)), and robot discovery (Feng et al., [2021](#bib.bib92))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | Customer feedback (Rosa et al., [2018](#bib.bib299);
    Zhang et al., [2014](#bib.bib442)), public events (Unankard et al., [2014](#bib.bib350);
    Manguri et al., [2020](#bib.bib255)) |'
  prefs: []
  type: TYPE_TB
- en: '| Influence analysis | Important node finding (Domingos and Richardson, [2001](#bib.bib74);
    Richardson and Domingos, [2002](#bib.bib296)), information diffusion modeling (Panagopoulos
    et al., [2020](#bib.bib270); Keikha et al., [2020](#bib.bib179); Zhang et al.,
    [2022a](#bib.bib432); Kumar et al., [2022](#bib.bib197)) |'
  prefs: []
  type: TYPE_TB
- en: '| Location-based Social Network | Restaurant, Cinema, Mall, Parking | Friendship,
    Check-in | POI recommendation | Spatial/temporal influence (Si et al., [2019](#bib.bib323);
    Wang et al., [2022e](#bib.bib377); Zhao et al., [2020](#bib.bib454)), social relationship (Xu
    et al., [2021a](#bib.bib401)), textual information (Xu et al., [2021b](#bib.bib403))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Urban computing | Traffic congestion prediction (Jiang and Luo, [2022](#bib.bib161);
    Xiong et al., [2018](#bib.bib399)), urban mobility analysis (Yildirimoglu and
    Kim, [2018](#bib.bib415); Cao et al., [2021b](#bib.bib36)), event detection (Yu
    et al., [2021a](#bib.bib424); Sofuoglu and Aviyente, [2022](#bib.bib327)) |'
  prefs: []
  type: TYPE_TB
- en: 10.1\. Concepts of Social Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A social network is usually composed of multiple types of nodes, link relationships,
    and node attributes, which inherently include rich structural and semantic information.
    Specifically, a social network can be homogeneous or heterogeneous and directed
    or undirected in different scenarios. Without loss of generality, we define the
    social network as a directed heterogeneous graph $G=\{V,E,\mathcal{T},\mathcal{R}\}$,
    where $V=\{n_{i}\}^{|V|}_{i=1}$ is the node set, $E=\{e_{i}\}^{|E|}_{i=1}$ is
    the edge set, $\mathcal{T}=\{t_{i}\}^{|\mathcal{T}|}_{i=1}$ is the node type set,
    and $\mathcal{R}=\{r_{i}\}^{|\mathcal{R}|}_{i=1}$ is the edge type set. Each node
    $n_{i}\in V$ is associated with a node type mapping: $\phi_{n}(n_{i})=t_{j}:V\longrightarrow\mathcal{T}$
    and each edge $e_{i}\in E$ is associated with a node type mapping: $\phi_{e}(e_{i})=r_{j}:E\longrightarrow\mathcal{R}$.
    A node $n_{i}$ may have a feature set, where the feature space is specific for
    the node type. An edge $e_{i}$ is also represented by node pairs $(n_{j},n_{k})$
    at both ends and can be directed or undirected with relation-type-specific attributes.
    If $|\mathcal{T}|=1$ and $|\mathcal{R}|=1$, the social network is a homogeneous
    graph; otherwise, it is a heterogeneous graph.'
  prefs: []
  type: TYPE_NORMAL
- en: Almost any data produced by social activities can be modeled as social networks,
    for example, the academic social network produced by academic activities such
    as collaboration and citation, the online social network produced by user following
    and followed on social media, and the location-based social network produced by
    human activities on different locations. Based on constructing social networks,
    researchers have new paths to data mining, knowledge discovery, and multiple application
    tasks on social data. Exploring social networks also brings new challenges. One
    of the critical challenges is how to succinctly represent the network from the
    massive and heterogeneous raw graph data, that is, how to learn continuous and
    low-dimensional social network representations, so as to researchers can efficiently
    perform advanced machine learning techniques on the social network data for multiple
    application tasks, such as analysis, clustering, prediction, and knowledge discovery.
    Thus, graph representation learning on the social network becomes the foundational
    technique for social analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2\. Academic Social Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Academic collaboration is a common and important behavior in academic society,
    and also a major way for scientists and researchers to innovate and break through
    scientific research, which leads to a complex social relationship between scholars.
    Also, the academic data generated by academic collaboration also contains a large
    number of interconnected entities with complex relationships (Kong et al., [2019](#bib.bib190)).
    Normally, in an academic social network, the node type set consists of Author,
    Publication, Venue, Organization, Keyword, etc., and the relation set consists
    of Authorship, Co-Author, Advisor-advisee, Citing, Cited, Co-Citing, Publishing,
    Co-Word, etc. Note that in most social networks, each relation type always connects
    two fixed node types with a fixed direction. For example, the relation Authorship
    points from the node type Author to Publication, and the Co-Author is an undirected
    relation between two nodes with type Author. Based on the node and relation types
    in an academic social network, one can divide it into multiple categories. For
    example, the co-author network with nodes of Author and relations of Co-Author,
    the citation network with nodes of Publication and relation of Citing, and the
    academic heterogeneous information graph with multiple academic node and relation
    types. Many research institutes and academic search engines, such as Aminer¹¹1https://www.aminer.cn/,
    DBLP²²2https://dblp.uni-trier.de/, Microsoft Academic Graph (MAG)³³3https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/,
    have provided open academic social network datasets for research purposes.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple applications of graph representation learning on the academic
    social network. Roughly, they can be divided into three categories–academic entity
    classification/clustering, academic relationship prediction, and academic resource
    recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Academic entities usually belong to different classes of research areas. Research
    of academic entity classification and clustering aims to categorize these entities,
    such as papers and authors, into different classes (Dong et al., [2017](#bib.bib75);
    Wang et al., [2019b](#bib.bib369); Zhang et al., [2019d](#bib.bib433); Qiao et al.,
    [2020a](#bib.bib284)). In literature, academic networks such as Cora, CiteSeer,
    and Pubmed (Sen et al., [2008](#bib.bib313)) have become the most widely used
    benchmark datasets for examining the performance of graph representation learning
    models on paper classification. Also, the author name disambiguation problem (Zhang
    et al., [2018e](#bib.bib445); Qiao et al., [2019](#bib.bib282); Chen et al., [2020h](#bib.bib43))
    is also essentially a node clustering task on co-author networks and is usually
    solved by the graph representation learning technique.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Academic relationship prediction represents the link prediction task on various
    academic relations. Typical applications are co-authorship prediction (Chuan et al.,
    [2018](#bib.bib60); Cho and Yu, [2018](#bib.bib57)) and citation relationship
    prediction (Yu et al., [2012](#bib.bib426); Jiang et al., [2018](#bib.bib162);
    Wang et al., [2020e](#bib.bib363)). Existing methods learn representations of
    authors and papers and use the similarity between two nodes to predict the link
    probability. Besides, some work (Liu et al., [2019](#bib.bib229); Zhao et al.,
    [2018](#bib.bib457)) studies the problem of advisor-advisee relationship prediction
    in the collaboration network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various academic recommendation systems have been introduced to retrieve academic
    resources for users from large amounts of academic data in recent years. For example,
    collaborator recommendation (Liu et al., [2018c](#bib.bib237); Kong et al., [2017](#bib.bib188),
    [2016](#bib.bib189)) benefit researchers by finding suitable collaborators under
    particular topics; paper recommendation (Bai et al., [2019](#bib.bib15); Sugiyama
    and Kan, [2010](#bib.bib335)) help researchers find relevant papers on given topics;
    venue recommendation (Yu et al., [2018](#bib.bib425); Margaris et al., [2019](#bib.bib258))
    help researchers choose appropriate venues when they submit papers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 10.3\. Social Media Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the development of the Internet in decades, various online social media
    have emerged in large numbers and greatly changed people’s traditional social
    models. People can establish friendships with others beyond the distance limit
    and share interests, hobbies, status, activities, and other information among
    friends. These abundant interactions on the Internet form large-scale complex
    social media networks, also named online social networks. Usually, in an academic
    social network, the node type set consists of User, Blog, Article, Image, Video,
    etc., and the relation type set consists of Following, Like, Unlike, Clicked,
    Viewed, Commented, Reposted, etc. The main property of a social media network
    is that it usually contains multi-mode information on the nodes, such as video,
    image, and text. Also, the relations are more complex and multiplex, including
    the explicit relations such as Like and Unlike and the implicit relations such
    as Clicked. The social media network can be categorized into multiple types based
    on their media categories. For example, the friendship network, the movie review
    network, and the music interacting network are extracted from different social
    media platforms. In a broad sense, the user-item networks in online shopping system
    can also be viewed as social media networks as they also exist on the Internet
    and contains rich interactions by people. There are many widely used data sources
    for social media network analysis, such as Twitter, Facebook, Weibo, YouTube,
    and Instagram.
  prefs: []
  type: TYPE_NORMAL
- en: The mainstream application research on social media networks via graph representation
    learning techniques mainly includes anomaly detection, sentiment analysis, and
    influence analysis.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection aims to find strange or unusual patterns in social networks,
    which has a wide range of application scenarios, such as malicious attacks (Sun
    et al., [2020c](#bib.bib339); Liu et al., [2018b](#bib.bib235)), emergency detection (Bian
    et al., [2020](#bib.bib22)), and robot discovery (Feng et al., [2021](#bib.bib92))
    in social networks. Unsupervised anomaly detection usually learns a reconstructed
    graph to detect those nodes with higher reconstructed error as the anomaly nodes (Ahmed
    et al., [2021](#bib.bib4); Zhao et al., [2022b](#bib.bib453)); Supervised methods
    model the problem as a binary classification task on the learned graph representations (Meng
    et al., [2021](#bib.bib260); Zheng et al., [2019](#bib.bib459)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis, also named as opinion mining, is to mining the sentiment,
    opinions, and attitudes, which can help enterprises understand customer feedback
    on products (Rosa et al., [2018](#bib.bib299); Zhang et al., [2014](#bib.bib442))
    and help the government analyze the public emotion and make rapid response to
    public events (Unankard et al., [2014](#bib.bib350); Manguri et al., [2020](#bib.bib255)).
    The graph representation learning model is usually combined with RNN-based (Zhang
    et al., [2019b](#bib.bib431); Chen et al., [2020e](#bib.bib49)) or Transformer-based (AlBadani
    et al., [2022](#bib.bib6); Tang et al., [2020a](#bib.bib343)) text encoders to
    incorporate both the user relationship and textual semantic information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Influence analysis usually aims to find several nodes in a social network to
    initially spread information such as advertisements, so as to maximize the final
    spread of information (Domingos and Richardson, [2001](#bib.bib74); Richardson
    and Domingos, [2002](#bib.bib296)). The core challenge is to model the information
    diffusion process in the social network. Deep learning methods (Panagopoulos et al.,
    [2020](#bib.bib270); Keikha et al., [2020](#bib.bib179); Zhang et al., [2022a](#bib.bib432);
    Kumar et al., [2022](#bib.bib197)) usually leverage graph neural networks to learn
    node embeddings and diffusion probabilities between nodes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 10.4\. Location-based Social Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Locations are the fundamental information of human social activities. With the
    wide availability of mobile Internet and GPS positioning technology, people can
    easily acquire their precise locations and socialize with their friends by sharing
    their historical check-ins on the Internet. This opens up a new avenue of research
    on location-based social network analysis, which gathered significant attention
    from the user, business, and government perspectives. Usually, in a location-based
    social network, the node type set consists of User, and Location, also named Point
    of Interest(POI) in the recommendation scenario containing multiple categories
    such as Restaurant, Cinema, Mall, Parking, etc. The relation type set consists
    of Friendship, Check-in. Also, those node and relation types that exist in traditional
    social media networks can be included in a location-based social network. The
    difference with other social networks, the main location-based social networks
    are spatial and temporal, making the graph representation learning more challenging.
    For example, in a typical social network constructed for the POI recommendation,
    the user nodes are connected with each other by their friendship. The location
    nodes are connected by user nodes with the relations feature of timestamps. The
    location nodes also have a spatial relationship with each other and own have complex
    features, including categories, tags, check-in counts, number of users check-in,
    etc. There are many location-based social network datasets, such as Foursquare⁴⁴4https://foursquare.com/,
    Gowalla⁵⁵5https://www.gowalla.com/, and Waze⁶⁶6https://www.waze.com/live-map/.
    Also, many social media such as Twitter, Instagram, and Facebook can provide location
    information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The research of graph representation learning on location-based social networks
    can be divided into two categories: POI recommendation for business benefits and
    urban computing for public management.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: POI recommendation is one of the research hotspots in the field of location-based
    social networks and recommendation systems in recent years (Islam et al., [2022](#bib.bib158);
    Werneck et al., [2020](#bib.bib380); Ju et al., [2022c](#bib.bib174)), which aim
    to utilize historical check-ins of users and auxiliary information to recommend
    potential favor places for users from a large of location points. Existing researches
    mainly integrate four essential characteristics, including spatial influence,
    temporal influence (Si et al., [2019](#bib.bib323); Wang et al., [2022e](#bib.bib377);
    Zhao et al., [2020](#bib.bib454)), social relationship (Xu et al., [2021a](#bib.bib401)),
    and textual information (Xu et al., [2021b](#bib.bib403)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urban computing is defined as a process of analysis of the large-scale connected
    urban data created from city activities of vehicles, human beings, and sensors (Paulos
    and Goodman, [2004](#bib.bib274); Paulos et al., [2004](#bib.bib273); Silva et al.,
    [2019](#bib.bib324)). Besides the local-based social network, the urban data also
    includes physical sensors, city infrastructure, traffic roads, and so on. Urban
    computing aims to improve the quality of public management and life quality of
    people living in city environments. Typical applications including traffic congestion
    prediction (Jiang and Luo, [2022](#bib.bib161); Xiong et al., [2018](#bib.bib399)),
    urban mobility analysis (Yildirimoglu and Kim, [2018](#bib.bib415); Cao et al.,
    [2021b](#bib.bib36)), event detection (Yu et al., [2021a](#bib.bib424); Sofuoglu
    and Aviyente, [2022](#bib.bib327)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 10.5\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section introduces social analysis by graph representation learning and
    we provide the summary as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques. Social networks, generated by human social activities, such as communication,
    collaboration, and social interactions, typically involve massive and heterogeneous
    data, with different types of attributes and properties that can change over time.
    Thus, social network analysis is a field of study that explores the techniques
    to understand and analyze the complex attributes, heterogeneous structures, and
    dynamic information of social networks. Social network analysis typically learns
    low-dimensional graph representations that capture the essential properties and
    patterns of the social network data, which can be used for various downstream
    tasks, such as classification, clustering, link prediction, and recommendation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chanllenges and Limitations. Despite the structural heterogeneity in social
    networks (nodes and relations have different types), with the technological advances
    in social media, the node attributes have become more heterogeneous now, containing
    text, video, and images. Also, the large-scale problem is a pending issue in social
    network analysis. The data in the social network has increased exponentially in
    past decades, containing a high density of topological links and a large amount
    of node attribute information, which brings new challenges to the efficiency and
    effectiveness of traditional network representation learning on the social network.
    Lastly, social networks are often dynamic, which means the network information
    usually changes over time, and this temporal information plays a significant role
    in many downstream tasks, such as recommendations. This brings new challenges
    to representation learning on social networks in incorporating temporal information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Works. Recently, multi-modal big pre-training models that can fuse information
    from different modalities have gained increasing attention (Qiao et al., [2022](#bib.bib283);
    Radford et al., [2021](#bib.bib290)). These models can obtain valuable information
    from a large amount of unlabeled data and transfer it to various downstream analysis
    tasks. Moreover, Transformer-based models have demonstrated better effectiveness
    than RNNs in capturing temporal information. In the future, there is potential
    for introducing multi-modal big pre-training models in social network analysis.
    Also, it is important to make the models more efficient for network information
    extraction and use lightweight techniques like knowledge distillation to further
    enhance the applicability of the models. These advancements can lead to more effective
    social network analysis and enable the development of more sophisticated applications
    in various domains.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 11\. Molecular Property Prediction By Zequn Liu
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Molecular Property Prediction is an essential task in computational drug discovery
    and cheminformatics. Traditional quantitative structure property/activity relationship
    (QSPR/QSAR) approaches are based on either SMILES or fingerprints (Mikolov et al.,
    [2013](#bib.bib263); Xu et al., [2017](#bib.bib407); Zhang et al., [2018d](#bib.bib440)),
    largely overlooking the topological features of the molecules. To address this
    problem, graph representation learning has been widely applied to molecular property
    prediction. A molecule can be represented as a graph where nodes stand for atoms
    and edges stand for atom-bonds (ABs). Graph-level molecular representations are
    learned via message passing mechanism to incorporate the topological information.
    The representations are then utilized for the molecular property prediction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, a molecule is denoted as a topological graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$,
    where $\mathcal{V}=\{v_{i}|i=1,\ldots,|\mathcal{G}|\}$ is the set of nodes representing
    atoms. A feature vector $\mathbf{x}_{i}$ is associated with each node $v_{i}$
    indicating its type such as Carbon, Nitrogen. $\mathcal{E}=\{e_{ij}|i,j=1,\ldots,|\mathcal{G}|\}$
    is the set of edges connecting two nodes (atoms) $v_{i}$ and $v_{j}$ representing
    atom bonds. Graph representation learning methods are used to obtain the molecular
    representation $\mathbf{h}_{\mathcal{G}}$. Then downstream classification or regression
    layers $f(\cdot)$ are applied to predict the probability of target property of
    each molecule $y=f(\mathbf{h}_{\mathcal{G}})$.
  prefs: []
  type: TYPE_NORMAL
- en: In Section [11.1](#S11.SS1 "11.1\. Molecular Property Categorization ‣ 11\.
    Molecular Property Prediction By Zequn Liu ‣ A Comprehensive Survey on Deep Graph
    Representation Learning"), we introduce 4 types of molecular properties graph
    representation learning can be treated and their corresponding datasets. Section
    [11.2](#S11.SS2 "11.2\. Molecular Graph Representation Learning Backbones ‣ 11\.
    Molecular Property Prediction By Zequn Liu ‣ A Comprehensive Survey on Deep Graph
    Representation Learning") reviews the graph representation learning backbones
    applied to molecular property prediction. Strategies for training the molecular
    property prediction methods are listed in Section [11.3](#S11.SS3 "11.3\. Training
    strategies ‣ 11\. Molecular Property Prediction By Zequn Liu ‣ A Comprehensive
    Survey on Deep Graph Representation Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 11.1\. Molecular Property Categorization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Plenty of molecular properties can be predicted by graph-based methods. We
    follow (Wieder et al., [2020](#bib.bib381)) to categorize them into 4 types: quantum
    chemistry, physicochemical properties, biophysics, and biological effect.'
  prefs: []
  type: TYPE_NORMAL
- en: Quantum chemistry is a branch of physical chemistry focused on the application
    of quantum mechanics to chemical systems, including conformation, partial charges
    and energies. QM7, QM8, QM9 (Wu et al., [2018](#bib.bib390)), COD (Ruddigkeit
    et al., [2012](#bib.bib300)) and CSD (Groom et al., [2016](#bib.bib123)) are datasets
    for quantum chemistry prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Physicochemical properties are the intrinsic physical and chemical characteristics
    of a substance, such as bioavailability, octanol solubility, aqueous solubility
    and hydrophobicity. ESOL, Lipophilicity and Freesolv (Wu et al., [2018](#bib.bib390))
    are datasets for physicochemical properties prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Biophysics properties are about the physical underpinnings of biomolecular phenomena,
    such as affinity, efficacy and activity. PDBbind (Wang et al., [2005](#bib.bib365)),
    MUV, and HIV (Wu et al., [2018](#bib.bib390)) are biophysics property prediction
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Biological effect properties are generally defined as the response of an organism,
    a population, or a community to changes in its environment, such as side effects,
    toxicity and ADMET. Tox21, toxcast (Wu et al., [2018](#bib.bib390)) and PTC (Toivonen
    et al., [2003](#bib.bib349)) are biological effect prediction datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Moleculenet (Wu et al., [2018](#bib.bib390)) is a widely-used benchmark dataset
    for molecule property prediction. It contains over 700,000 compounds tested on
    different properties. For each dataset, they provide a metric and a splitting
    pattern. Among the datasets, QM7, OM7b, QM8, QM9, ESOL, FreeSolv, Lipophilicity
    and PDBbind are regression tasks, using MAE or RMSE as evaluation metrics. Other
    tasks such as tox21 and toxcast are classification tasks, using AUC as evaluation
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2\. Molecular Graph Representation Learning Backbones
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since node attributes and edge attributes are crucial to molecular representation,
    most works use GNN instead of traditional graph representation learning methods
    as backbones, since many GNN methods consider edge information. Existing GNNs
    designed for the general domain can be applied to molecular graph. Table [9](#S11.T9
    "Table 9 ‣ 11.2\. Molecular Graph Representation Learning Backbones ‣ 11\. Molecular
    Property Prediction By Zequn Liu ‣ A Comprehensive Survey on Deep Graph Representation
    Learning") summarizes the GNNs used for molecular property prediction and the
    types of properties they can be applied to predict.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9\. Summary of GNNs in molecular property prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Spatial/Specrtal | Method | Application |'
  prefs: []
  type: TYPE_TB
- en: '| Reccurent GNN | - | R-GNN | Biological effect (Scarselli et al., [2008](#bib.bib307))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Reccurent GNN | - | GGNN | Quantum chemistry (Mansimov et al., [2019](#bib.bib256)),
    Biological effect (Withnall et al., [2020](#bib.bib383); Altae-Tran et al., [2017](#bib.bib8);
    Feinberg et al., [2018](#bib.bib91)) |'
  prefs: []
  type: TYPE_TB
- en: '| Reccurent GNN | - | IterRefLSTM | Biophysics (Altae-Tran et al., [2017](#bib.bib8)),
    Biological effect (Altae-Tran et al., [2017](#bib.bib8)) |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional GNN | Spatial/Specrtal | GCN | Quantum chemistry (Yang et al.,
    [2019](#bib.bib410); Liao et al., [2018](#bib.bib225); Withnall et al., [2020](#bib.bib383)),
    pysicochemical properties (Ryu et al., [2018](#bib.bib302); Duvenaud et al., [2015](#bib.bib81);
    Coley et al., [2017](#bib.bib63)), Biophysics (Duvenaud et al., [2015](#bib.bib81);
    Yang et al., [2019](#bib.bib410); Bouritsas et al., [2022](#bib.bib26)) Biological
    effect (Li et al., [2017a](#bib.bib209); Wu et al., [2018](#bib.bib390)) |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional GNN | Specrtal | LanczosNet | Quantum chemistry (Liao et al.,
    [2018](#bib.bib225)) |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional GNN | Specrtal | ChebNet | Physicochemical properties, Biophysics,
    Biological effect (Li et al., [2018b](#bib.bib215)) |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional GNN | Spatial | GraphSAGE | Physicochemical properties (Hu
    et al., [2019](#bib.bib146)), Biophysics (Errica et al., [2019](#bib.bib87); Liang
    et al., [2020](#bib.bib224); Chen et al., [2020a](#bib.bib56)), Biological effect
    (Hu et al., [2019](#bib.bib146); Ma et al., [2019](#bib.bib251)) |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional GNN | Spatial | GAT | Physicochemical properties (Hu et al.,
    [2019](#bib.bib146)), Biophysics (Chen et al., [2020a](#bib.bib56); Bouritsas
    et al., [2022](#bib.bib26)), Biological effect (Hu et al., [2019](#bib.bib146))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional GNN | Spatial | DGCNN | Biophysics (Chen et al., [2019](#bib.bib51)),
    Biological effect (Zhang et al., [2018a](#bib.bib438)) |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional GNN | Spatial | GIN | Physicochemical properties (Hu et al.,
    [2019](#bib.bib146); Bouritsas et al., [2022](#bib.bib26)), Biophysics (Hu et al.,
    [2019](#bib.bib146), [2020c](#bib.bib145)), Biological effect (Hu et al., [2019](#bib.bib146))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional GNN | Spatial | MPNN | Physicochemical (Ma et al., [2020a](#bib.bib248))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Transformer | - | MAT | Physicochemical, Biophysics (Łukasz Maziarka et al.,
    [2020](#bib.bib475)) |'
  prefs: []
  type: TYPE_TB
- en: Furthermore, many works customize their GNN structure by considering the chemical
    domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, the chemical bonds are taken into consideration carefully. For example,
    Ma et al. (Ma et al., [2020a](#bib.bib248)) use an additional edge GNN to model
    the chemical bonds separately. Specifically, given an edge $(v,w)$, they formulate
    an Edge-based GNN as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (119) |  | $\displaystyle{\mathbf{m}}_{vw}^{(k)}$ | $\displaystyle=\text{AGG}_{\text{edge}}(\{{\mathbf{h}}_{vw}^{(k-1)},{\mathbf{h}}_{uv}^{(k-1)},{\mathbf{x}}_{u}&#124;u\in\mathcal{N}_{v}\setminus
    w\}),\quad{\mathbf{h}}_{vw}^{(k)}=\text{MLP}_{\text{edge}}(\{{\mathbf{m}}_{vw}^{(k-1)},{\mathbf{h}}_{vw}^{(0)}\}),$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where ${\mathbf{h}}_{vw}^{(0)}=\sigma({\mathbf{W}}_{\text{ein}}{\mathbf{e}}_{vw})$
    is the input state of the Edge-based GNN, ${\mathbf{W}}_{\text{ein}}\in\mathbb{R}^{d_{\text{hid}}\times
    d_{e}}$ is the input weight matrix. PotentialNet (Feinberg et al., [2018](#bib.bib91))
    further uses different message passing operations for different edge types.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second, motifs in molecular graphs play an important role in molecular property
    prediction. GSN (Bouritsas et al., [2022](#bib.bib26)) leverages substructure
    encoding to construct a topologically-aware message passing method. Each node
    $v$ updates its state $\mathbf{h}^{t}_{v}$ by combining its previous state with
    the aggregated messages:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (120) |  | $\displaystyle\mathbf{h}^{t+1}_{v}$ | $\displaystyle=\mathrm{UP}^{t+1}\big{(}\mathbf{h}^{t}_{v},\mathbf{m}^{t+1}_{v}\big{)},$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| (124) |  | $\displaystyle\mathbf{m}^{t+1}_{v}$ | <math 
    class="ltx_Math" alttext="\displaystyle=\left\{\begin{array}[]{l}M^{t+1}\bigg{(}\Lbag(\mathbf{h}^{t}_{v},\mathbf{h}^{t}_{u},\mathbf{x}^{V}_{v},\mathbf{x}^{V}_{u},\mathbf{e}_{u,v})\Rbag_{u\in\mathcal{N}(v)}\bigg{)}\
    (\textbf{GSN-v})\\ \quad\quad\quad\quad\quad\quad\text{ or }\\'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: M^{t+1}\bigg{(}\Lbag(\mathbf{h}^{t}_{v},\mathbf{h}^{t}_{u},\mathbf{x}^{E}_{u,v},\mathbf{e}_{u,v})\Rbag_{u\in\mathcal{N}(v)}\bigg{)}\
    (\textbf{GSN-e})\end{array}\right.," display="inline"><semantics ><mrow
     ><mrow 
    ><mo  >=</mo><mrow
     ><mo 
    >{</mo><mtable rowspacing="0pt" 
    ><mtr  ><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><msup  ><mi
     >M</mi><mrow 
    ><mi  >t</mi><mo
     >+</mo><mn 
    >1</mn></mrow></msup><mo lspace="0em" rspace="0em"
     >​</mo><mrow 
    ><mo maxsize="210%" minsize="210%" 
    >(</mo><mrow  ><merror
    class="ltx_ERROR undefined undefined"  ><mtext
     >\Lbag</mtext></merror><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><msubsup
     ><mi 
    >𝐡</mi><mi 
    >v</mi><mi 
    >t</mi></msubsup><mo 
    >,</mo><msubsup 
    ><mi  >𝐡</mi><mi
     >u</mi><mi
     >t</mi></msubsup><mo
     >,</mo><msubsup
     ><mi 
    >𝐱</mi><mi 
    >v</mi><mi 
    >V</mi></msubsup><mo 
    >,</mo><msubsup 
    ><mi  >𝐱</mi><mi
     >u</mi><mi
     >V</mi></msubsup><mo
     >,</mo><msub
     ><mi 
    >𝐞</mi><mrow  ><mi
     >u</mi><mo 
    >,</mo><mi  >v</mi></mrow></msub><mo
    stretchy="false"  >)</mo></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><merror class="ltx_ERROR
    undefined undefined"  ><mtext
     >\Rbag</mtext></merror><mrow
     ><mi 
    >u</mi><mo  >∈</mo><mrow
     ><mi class="ltx_font_mathcaligraphic"
     >𝒩</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi 
    >v</mi><mo stretchy="false" 
    >)</mo></mrow></mrow></mrow></msub></mrow><mo maxsize="210%"
    minsize="210%"  >)</mo></mrow><mo
    lspace="0.500em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false" 
    >(</mo><mtext class="ltx_mathvariant_bold" 
    >GSN-v</mtext><mo stretchy="false" 
    >)</mo></mrow></mrow></mtd></mtr><mtr 
    ><mtd class="ltx_align_left" columnalign="left" 
    ><mtext  > or </mtext></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><msup  ><mi
     >M</mi><mrow 
    ><mi  >t</mi><mo
     >+</mo><mn 
    >1</mn></mrow></msup><mo lspace="0em" rspace="0em"
     >​</mo><mrow 
    ><mo maxsize="210%" minsize="210%" 
    >(</mo><mrow  ><merror
    class="ltx_ERROR undefined undefined"  ><mtext
     >\Lbag</mtext></merror><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><msubsup
     ><mi 
    >𝐡</mi><mi 
    >v</mi><mi 
    >t</mi></msubsup><mo 
    >,</mo><msubsup 
    ><mi  >𝐡</mi><mi
     >u</mi><mi
     >t</mi></msubsup><mo
     >,</mo><msubsup
     ><mi 
    >𝐱</mi><mrow  ><mi
     >u</mi><mo 
    >,</mo><mi  >v</mi></mrow><mi
     >E</mi></msubsup><mo
     >,</mo><msub 
    ><mi  >𝐞</mi><mrow
     ><mi 
    >u</mi><mo  >,</mo><mi
     >v</mi></mrow></msub><mo
    stretchy="false"  >)</mo></mrow><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><merror class="ltx_ERROR
    undefined undefined"  ><mtext
     >\Rbag</mtext></merror><mrow
     ><mi 
    >u</mi><mo  >∈</mo><mrow
     ><mi class="ltx_font_mathcaligraphic"
     >𝒩</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi 
    >v</mi><mo stretchy="false" 
    >)</mo></mrow></mrow></mrow></msub></mrow><mo maxsize="210%"
    minsize="210%"  >)</mo></mrow><mo
    lspace="0.500em" rspace="0em"  >​</mo><mrow
     ><mo stretchy="false" 
    >(</mo><mtext class="ltx_mathvariant_bold" 
    >GSN-e</mtext><mo stretchy="false" 
    >)</mo></mrow></mrow></mtd></mtr></mtable></mrow></mrow><mo
     >,</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply 
    ><csymbol cd="latexml" 
    >absent</csymbol><apply 
    ><csymbol cd="latexml" 
    >cases</csymbol><matrix 
    ><matrixrow  ><apply
     ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝑀</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply
     ><ci 
    ><merror class="ltx_ERROR undefined undefined" 
    ><mtext  >\Lbag</mtext></merror></ci><vector
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐡</ci><ci 
    >𝑡</ci></apply><ci 
    >𝑣</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐡</ci><ci 
    >𝑡</ci></apply><ci 
    >𝑢</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐱</ci><ci 
    >𝑉</ci></apply><ci 
    >𝑣</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐱</ci><ci 
    >𝑉</ci></apply><ci 
    >𝑢</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝐞</ci><list  ><ci
     >𝑢</ci><ci 
    >𝑣</ci></list></apply></vector><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    ><merror class="ltx_ERROR undefined undefined" 
    ><mtext  >\Rbag</mtext></merror></ci><apply
     ><ci 
    >𝑢</ci><apply  ><ci
     >𝒩</ci><ci 
    >𝑣</ci></apply></apply></apply></apply><ci 
    ><mtext class="ltx_mathvariant_bold" 
    >GSN-v</mtext></ci></apply></matrixrow><matrixrow 
    ><ci  ><mtext
     > or </mtext></ci></matrixrow><matrixrow
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝑀</ci><apply 
    ><ci  >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply
     ><ci 
    ><merror class="ltx_ERROR undefined undefined" 
    ><mtext  >\Lbag</mtext></merror></ci><vector
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐡</ci><ci 
    >𝑡</ci></apply><ci 
    >𝑣</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐡</ci><ci 
    >𝑡</ci></apply><ci 
    >𝑢</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐱</ci><ci 
    >𝐸</ci></apply><list 
    ><ci  >𝑢</ci><ci
     >𝑣</ci></list></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐞</ci><list
     ><ci 
    >𝑢</ci><ci  >𝑣</ci></list></apply></vector><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     ><merror class="ltx_ERROR
    undefined undefined"  ><mtext
     >\Rbag</mtext></merror></ci><apply
     ><ci 
    >𝑢</ci><apply  ><ci
     >𝒩</ci><ci 
    >𝑣</ci></apply></apply></apply></apply><ci 
    ><mtext class="ltx_mathvariant_bold" 
    >GSN-e</mtext></ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle=\left\{\begin{array}[]{l}M^{t+1}\bigg{(}\Lbag(\mathbf{h}^{t}_{v},\mathbf{h}^{t}_{u},\mathbf{x}^{V}_{v},\mathbf{x}^{V}_{u},\mathbf{e}_{u,v})\Rbag_{u\in\mathcal{N}(v)}\bigg{)}\
    (\textbf{GSN-v})\\ \quad\quad\quad\quad\quad\quad\text{ or }\\ M^{t+1}\bigg{(}\Lbag(\mathbf{h}^{t}_{v},\mathbf{h}^{t}_{u},\mathbf{x}^{E}_{u,v},\mathbf{e}_{u,v})\Rbag_{u\in\mathcal{N}(v)}\bigg{)}\
    (\textbf{GSN-e})\end{array}\right.,</annotation></semantics></math> |  |
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where $\mathbf{x}^{V}_{v},\mathbf{x}^{V}_{u},\mathbf{x}^{E}_{u,v},\mathbf{e}_{u,v}$
    contains the substructure information associated with nodes and edges, $\Lbag\Rbag$
    denotes a multiset. Yu et al. (Yu and Gao, [2022](#bib.bib427)) constructs a heterogeneous
    graph using motifs and molecules. Motifs and molecules are both treated as nodes
    and the edges model the relationship between motifs and graphs, for example, if
    a graph contains a motif, there will be an edge between them. MGSSL (Zhang et al.,
    [2021a](#bib.bib448)) leverages a retrosynthesis-based algorithm BRICS and additional
    rules to find the motifs and combines motif layers with atom layers. It is a hierarchical
    framework jointly modeling atom-level information and motif-level information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third, different feature modalities have been used to improve the molecular
    graph embedding. Lin et al. (Lin et al., [2022b](#bib.bib228)) combine SMILES
    modality and graph modality with contrastive learning. Zhu et al. (Zhu et al.,
    [2022b](#bib.bib469)) encode 2D molecular graph and 3D molecular conformation
    with a unified Transformer. It uses a unified model to learn 3D conformation generation
    given 2D graph and 2D graph generation given 3D conformation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, knowledge graph and literature can provide additional knowledge for
    molecular property prediction. Fang et al. (Fang et al., [2022](#bib.bib89)) introduce
    a chemical element knowledge graph to summarize microscopic associations between
    elements and augment the molecular graph based on the knowledge graph, and a knowledge-aware
    message passing network is used to encode the augmented graph. MuMo (Su et al.,
    [2022](#bib.bib334)) introduces biomedical literature to guide the molecular property
    prediction. It pretrains a GNN and a language model on paired data of molecules
    and literature mentions via contrastive learning:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (125) |  | $\ell_{i}^{(\mathbf{z}_{i}^{G},\mathbf{z}_{i}^{T})}=-\log\frac{\exp{(\emph{sim}(\mathbf{z}_{i}^{G},\mathbf{z}_{i}^{T})/\tau)}}{\sum_{j=1}^{N}\exp{(\emph{sim}(\mathbf{z}_{i}^{G},\mathbf{z}_{j}^{T})/\tau})},$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $\mathbf{z}_{i}^{G},\mathbf{z}_{i}^{T}$ are the representation of molecule
    and its corresponding literature.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 11.3\. Training strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Despite the encouraging performance achieved by GNNs, traditional supervised
    training scheme of GNNs faces a severe limitation: The scarcity of available molecules
    with desired properties. Although there are a large number of molecular graphs
    in public databases such as PubChem, labeled molecules are hard to acquire due
    to the high cost of wet-lab experiments and quantum chemistry calculations. Directly
    training GNNs on such limited molecules in a supervised way is prone to over-fitting
    and lack of generalization. To address this issue, few-shot learning and self-supervised
    learning are widely used in molecular property prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot learning. Few-shot learning aims at generalizing to a task with a small
    labeled data set. The prediction of each property is treated as a single task.
    Metric-based and optimization-based few-shot learning have been adopted for molecular
    property prediction. Metric-based few-shot learning is similar to nearest neighbors
    and kernel density estimation, which learns a metric or distance function over
    objects. IterRefLSTM (Altae-Tran et al., [2017](#bib.bib8)) leverages matching
    network (Vinyals et al., [2016](#bib.bib355)) as the few-shot learning framework,
    calculating the similarity between support samples and query samples. Optimization-based
    few-shot learning optimizes a meta-learner for parameter initialization which
    can be fast adapted to new tasks. Meta-MGNN (Guo et al., [2021](#bib.bib127))
    adopts MAML (Finn et al., [2017](#bib.bib95)) to train a parameter initialization
    to adapt to different tasks and use self-attentive task weights for each task.
    PAR (Wang et al., [2021](#bib.bib371)) also uses MAML framework and learns an
    adaptive relation graph among molecules for each task.
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised learning. Self-supervised learning can pre-train a GNN model
    with plenty of unlabeled molecular graphs and transfer it to specific molecular
    property prediction tasks. Self-supervised learning contains generative methods
    and predictive methods. Predictive methods design prediction tasks to capture
    the intrinsic data features. Pre-GNN (Hu et al., [2019](#bib.bib146)) exploits
    both node-level and graph-level prediction tasks including context prediction,
    attribute masking, graph-level property prediction and structural similarity prediction.
    MGSSL (Zhang et al., [2021a](#bib.bib448)) provides a motif-based generative pre-training
    framework making topology prediction and motif generation iteratively. Contrastive
    methods learn graph representations by pulling views from the same graph close
    and pushing views from different graphs apart. Different views of the same graph
    are constructed by graph augmentation or leveraging the 1D SMILES and 3D structure.
    MolCLR (Wang et al., [2022d](#bib.bib374)) augments molecular graphs by atom masking,
    bond deletion and subgraph removal and maximizes the agreement between the original
    molecular graph and augmented graphs. Fang et al. (Fang et al., [2022](#bib.bib89))
    uses a chemical knowledge graph to guide the graph augmentation. SMICLR (Pinheiro
    et al., [2022](#bib.bib280)) uses contrastive learning across SMILES and 2D molecular
    graphs. GeomGCL (Li et al., [2022e](#bib.bib216)) leverages graph contrastive
    learning to capture the geometry of the molecule across 2D and 3D views. Self-supervised
    learning can also be combined with few-shot learning to fully leverage the hierarchical
    information in the training set (Ju et al., [2023b](#bib.bib171)).
  prefs: []
  type: TYPE_NORMAL
- en: 11.4\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section introduces graph representation learning in molecular property
    prediction and we provide the summary as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques. For molecular property prediction, a molecule is represented as
    a graph whose nodes are atoms and edges are atom-bonds (ABs). GNNs such as GCN,
    GAT, and GraphSAGE are adopted to learn the graph-level representation. The representations
    are then fed into a classification or regression head for the molecular property
    prediction tasks. Many works guide the model structure design with medical domain
    knowledge including chemical bond features, motif features, different modalities
    of molecular representation, chemical knowledge graph and literature. Due to the
    scarcity of available molecules with desired properties, few-shot learning and
    contrastive learning are used to train molecular property prediction model, so
    that the model can leverage the information in large unlabeled dataset and can
    be adapted to new tasks with a few examples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Challenges and Limitations. Despite the great success of graph representation
    learning in molecular property prediction, the methods still have limitations:
    1) Few-shot molecular property prediction are not fully explored. 2) Most methods
    depend on training with labeled data, but neglect the chemical domain knowledge.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Future Works. In the future, we expect that: 1) More few-shot learning and
    zero-shot learning methods are studied for molecular property prediction to solve
    the data scarcity problem. 2) Heterogeneous data can be fused for molecular property
    prediction. There are a large amount of heterogeneous data about molecules such
    as knowledge graphs, molecule descriptions and property descriptions. They can
    be considered to assist molecular property prediction. 3) Chemical domain knowledge
    can be leveraged for the prediction model. For example, when we perform affinity
    prediction, we can consider molecular dynamics knowledge.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 12\. Molecular Generation By Fang Sun
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Molecular generation is pivotal to drug discovery, where it serves a fundamental
    role in downstream tasks like molecular docking (Meng et al., [2011](#bib.bib261))
    and virtual screening (Walters et al., [1998](#bib.bib357)). The goal of molecular
    generation is to produce chemical structures that satisfy a specific molecular
    profile, e.g., novelty, binding affinity, and SA scores. Traditional methods have
    relied on 1D string formats like SMILES (Gómez-Bombarelli et al., [2018](#bib.bib117))
    and SELFIES (Krenn et al., [2020](#bib.bib193)). With the recent advances in graph
    representation learning, numerous graph-based methods have also emerged, where
    molecular graph $\mathcal{G}$ can naturally embody both 2D topology and 3D geometry.
    While recent literature reviews  (Meyers et al., [2021](#bib.bib262); Du et al.,
    [2022a](#bib.bib79)) have covered the general topics of molecular design, this
    chapter is dedicated to the applications of graph representation learning in the
    molecular generation task. Molecular generation is intrinsically a de novo task,
    where molecular structures are generated from scratch to navigate and sample from
    the vast chemical space. Therefore, this chapter does not discuss tasks that restrict
    chemical structures a priori, such as docking (Ganea et al., [2021](#bib.bib104);
    Stärk et al., [2022](#bib.bib333)) and conformation generation (Shi et al., [2021](#bib.bib319);
    Zhu et al., [2022a](#bib.bib468)).
  prefs: []
  type: TYPE_NORMAL
- en: 12.1\. Taxonomy for molecular featurization methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section categorizes the different methods to featurize molecules. The taxonomy
    presented here is unique to the task of molecular generation, owing to the various
    modalities of molecular entities, complex interactions with other bio-molecular
    systems and formal knowledge from the laws of chemistry and physics.
  prefs: []
  type: TYPE_NORMAL
- en: 2D topology vs. 3D geometry. Molecular data are multi-modal by nature. For one
    thing, a molecule can be unambiguously represented by its 2D topological graph
    $\mathcal{G}_{\mathrm{2D}}$, where atoms are nodes and bonds are edges. $\mathcal{G}_{\mathrm{2D}}$
    can be encoded by canonical MPNN models like GCN (Kipf and Welling, [2016a](#bib.bib183)),
    GAT (Veličković et al., [2017](#bib.bib352)), and R-GCN (Schlichtkrull et al.,
    [2018](#bib.bib308)), in ways similar to tasks like social networks and knowledge
    graphs. A typical example of this line of work is GCPN (You et al., [2018](#bib.bib419)),
    a graph convolutional policy network generating molecules with desired properties
    such as synthetic accessibility and drug-likeness.
  prefs: []
  type: TYPE_NORMAL
- en: For another, the 3D conformation of a molecule can be accurately depicted by
    its 3D geometric graph $\mathcal{G}_{\mathrm{3D}}$, which incorporates 3D atom
    coordinates. In 3D-GNNs like SchNet (Schütt et al., [2018](#bib.bib312)) and OrbNet (Qiao
    et al., [2020b](#bib.bib285)), $\mathcal{G}_{\mathrm{3D}}$ is organized into a
    $k$-NN graph or a radius graph according to the Euclidean distance between atoms.
    It is justifiable to approximate $\mathcal{G}_{\mathrm{3D}}$ as a 3D extension
    to $\mathcal{G}_{\mathrm{2D}}$, since covalent atoms are closest to each other
    in most cases. However, $\mathcal{G}_{\mathrm{3D}}$ can also find a more long-standing
    origin in the realm of computational chemistry  (Frisch et al., [2016](#bib.bib100)),
    where both covalent and non-covalent atomistic interactions are considered to
    optimize the potential surface and simulate molecular dynamics. Therefore, $\mathcal{G}_{\mathrm{3D}}$
    more realistically represents the molecular geometry, which makes a good fit for
    protein pocket binding and 3D-QSAR optimization (Verma et al., [2010](#bib.bib353)).
  prefs: []
  type: TYPE_NORMAL
- en: Molecules can rotate and translate, affecting their position in the 3D space.
    Therefore, it is ideal to encode these molecules with GNNs equivariant/invariant
    to roto-translations, which can be $\sim 10^{3}$ times more efficient than data
    augmentation (Geiger and Smidt, [2022](#bib.bib114)). Equivariant GNNs can be
    based on irreducible representation (Thomas et al., [2018](#bib.bib348); Anderson
    et al., [2019](#bib.bib9); Fuchs et al., [2020](#bib.bib103); Batzner et al.,
    [2021](#bib.bib19); Brandstetter et al., [2022](#bib.bib29)), regular representation (Finzi
    et al., [2020](#bib.bib96); Hutchinson et al., [2021](#bib.bib155)), or scalarization (Schütt
    et al., [2018](#bib.bib312); Klicpera et al., [2020](#bib.bib186); Liu et al.,
    [2022b](#bib.bib234); Köhler et al., [2020](#bib.bib187); Jing et al., [2021](#bib.bib169);
    Satorras et al., [2021](#bib.bib305); Huang et al., [2022](#bib.bib153); Schütt
    et al., [2021](#bib.bib311); Thölke and Fabritiis, [2022](#bib.bib347); Klicpera
    et al., [2021](#bib.bib185)), which are explained in more detail in Han et al. (Han
    et al., [2022](#bib.bib131)).
  prefs: []
  type: TYPE_NORMAL
- en: Unbounded vs. binding-based. Earlier works have aimed to generate unbounded
    molecules in either 2D or 3D space, striving to learn good molecular representations
    through this task. In the 2D scenario, GraphNVP (Madhawa et al., [2019](#bib.bib252))
    first introduces a flow-based model to learn an invertible transformation between
    the 2D chemical space and the latent space. GraphAF (Shi et al., [2020](#bib.bib320))
    further adopts an autoregressive generation scheme to check the valence of the
    generated atoms and bonds. In the 3D scenario, G-SchNet  (Gebauer et al., [2019](#bib.bib112))
    first proposes to utilize $\mathcal{G}_{\mathrm{3D}}$ (instead of 3D density grids)
    as the generation backbone. It encodes $\mathcal{G}_{\mathrm{3D}}$ via SchNet,
    and uses an auxiliary token to generate atoms on the discretized 3D space autoregressively.
    G-SphereNet (Luo and Ji, [2022](#bib.bib246)) uses symmetry-invariant representations
    in a spherical coordinate system (SCS) to generate atoms in the continuous 3D
    space and preserve equivariance.
  prefs: []
  type: TYPE_NORMAL
- en: Unbounded models adopt certain techniques to optimize specific properties of
    the generated molecules. GCPN and GraphAF use scores likes logP, QED, and chemical
    validity to tune the model via reinforcement learning. EDM (Hoogeboom et al.,
    [2022](#bib.bib143)) can generate 3D molecules with property $c$ by re-training
    the diffusion model with $c$’s feature vector concatenated to the E(n) equivariant
    dynamics function $\hat{\boldsymbol{\epsilon}}_{t}=\phi\left(\boldsymbol{z}_{t},[t,c]\right)$.
    cG-SchNet  (Gebauer et al., [2022](#bib.bib113)) adopts a conditioning network
    architecture to jointly target multiple electronic properties during conditional
    generation without the need to re-train the model. RetMol (Wang et al., [2022b](#bib.bib375))
    uses a retrieval-based model for controllable generation.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, binding-based methods generate drug-like molecules (aka.
    ligands) according to the binding site (aka. binding pocket) of a protein receptor.
    Drawing inspirations from the lock-and-key model for enzyme action  (Fischer,
    [1894](#bib.bib97)), works like LiGAN (Ragoza et al., [2022](#bib.bib291)) and
    DESERT (Long et al., [2022](#bib.bib240)) use 3D density grids to fit the density
    surface between the ligand and the receptor, encoded by 3D-CNNs. Meanwhile, a
    growing amount of literature has adopted $\mathcal{G}_{\mathrm{3D}}$ for representing
    ligand and receptor molecules, because $\mathcal{G}_{\mathrm{3D}}$ more accurately
    depicts molecular structures and atomistic interactions both within and between
    the ligand and the receptor. Representative works include 3D-SBDD (Luo et al.,
    [2021b](#bib.bib242)), GraphBP (Liu et al., [2022a](#bib.bib231)), Pocket2Mol (Peng
    et al., [2022](#bib.bib276)), and DiffSBDD  (Schneuing et al., [2022](#bib.bib310)).
    GraphBP shares a similar workflow with G-SphereNet, except that the receptor atoms
    are also incorporated into $\mathcal{G}_{\mathrm{3D}}$ to depict the 3D geometry
    at the binding pocket.
  prefs: []
  type: TYPE_NORMAL
- en: Atom-based vs. fragment-based. Molecules are inherently hierarchical structures.
    At the atomistic level, molecules are represented by encoding atoms and bonds.
    At a coarser level, molecules can also be represented as molecular fragments like
    functional groups or chemical sub-structures. Both the composition and the geometry
    are fixed within a given fragment, e.g., the planar peptide-bond (–CO–NH–) structure.
    Fragment-based generation effectively reduces the degree of freedom (DOF) of chemical
    structures, and injects well-established knowledge about molecular patterns and
    reactivity. JT-VAE (Jin et al., [2018](#bib.bib166)) decomposes 2D molecular graph
    $\mathcal{G}_{\mathrm{2D}}$ into a junction-tree structure $\mathcal{T}$, which
    is further encoded via tree message-passing. DeepScaffold (Li et al., [2019a](#bib.bib217))
    expands the provided molecular scaffold into 3D molecules. L-Net (Li et al., [2021a](#bib.bib218))
    adopts a graph U-Net architecture and devises a custom three-level node clustering
    scheme for pooling and unpooling operations in molecular graphs. A number of works
    have also emerged lately for fragment-based generation in the binding-based setting,
    including FLAG (ZHANG et al., [[n. d.]](#bib.bib449)) and FragDiff (Peng et al.,
    [2023](#bib.bib275)). FLAG uses a regression-based approach to sequentially decide
    the type and torsion angle of the next fragment to be placed at the binding site,
    and finally optimizes the molecule conformation via a pseudo-force field. FragDiff
    also adopts a sequential generation process but uses a diffusion model to determine
    the type and pose of each fragment in one go.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2\. Generative methods for molecular graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a molecular graph generation process, the model first learns a latent distribution
    $P(Z|\mathcal{G})$ characterizing the input molecular graphs. A new molecular
    graph $\hat{\mathcal{G}}$ is then generated by sampling and decoding from this
    learned distribution. Various models have been adopted to generate molecular graphs,
    including generative adversarial network (GAN), variational auto-encoder (VAE),
    normalizing flow (NF), diffusion model (DM), and autoregressive model (AR).
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial network (GAN). GAN (Goodfellow et al., [2020](#bib.bib118))
    is trained to discriminate real data $\boldsymbol{x}$ from generated generated
    data $\boldsymbol{z}$, with the training object formalized as
  prefs: []
  type: TYPE_NORMAL
- en: '| (126) |  | $\min_{G}\max_{D}\mathcal{L}(D,G)=\mathbb{E}_{\boldsymbol{x}\sim
    p_{\text{data }}}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z}\sim p(\boldsymbol{z})}[\log(1-D(G(\boldsymbol{z})))],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $G(\cdot)$ is the generator function and $D(\cdot)$ is the discriminator
    function. For example, MolGAN (De Cao and Kipf, [2018](#bib.bib66)) encodes $\mathcal{G}_{\mathrm{2D}}$
    with R-GCN, trains $D$ and $G$ with improved W-GAN (Arjovsky et al., [2017](#bib.bib10)),
    and uses reinforcement learning to generate attributed molecules, where the score
    function is assigned from RDKit (Landrum et al., [2013](#bib.bib199)) and chemical
    validity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Varaitional auto-encoder (VAE). In VAE (Kingma and Welling, [2013](#bib.bib181)),
    the decoder parameterizes the conditional likelihood distribution $p_{\theta}(\boldsymbol{x}|\boldsymbol{z})$,
    and the encoder parameterizes an approximate posterior distribution $q_{\phi}(\boldsymbol{z}|\boldsymbol{x})\approx
    p_{\theta}(\boldsymbol{z}|\boldsymbol{x})$. The model is optimized by the evidence
    lower bound (ELBO), consisting of the reconstruction loss term and the distance
    loss term:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (127) |  | $\max_{\theta,\phi}\mathcal{L}_{\theta,\phi}(\boldsymbol{x}):=\mathbb{E}_{\boldsymbol{z}\sim
    q_{\phi}(\cdot&#124;\boldsymbol{x})}\left[\ln\frac{p_{\theta}(\boldsymbol{x},\boldsymbol{z})}{q_{\phi}(\boldsymbol{z}&#124;\boldsymbol{x})}\right]=\ln
    p_{\theta}(\boldsymbol{x})-D_{\mathrm{KL}}\left(q_{\phi}(\cdot&#124;\boldsymbol{x})\&#124;p_{\theta}(\cdot&#124;\boldsymbol{x})\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Maximizing ELBO is equivalent to simultaneously maximizing the log-likelihood
    of the observed data, and minimizing the divergence of the approximate posterior
    ${q_{\phi}(\cdot|x)}$ from the exact posterior ${p_{\theta}(\cdot|x)}$. Representative
    works along this thread include JT-VAE (Jin et al., [2018](#bib.bib166)), GraphVAE (Simonovsky
    and Komodakis, [2018](#bib.bib326)), and CGVAE (Liu et al., [2018a](#bib.bib232))
    for the 2D generation task, and 3DMolNet (Nesterov et al., [2020](#bib.bib268))
    for the 3D generation task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Autoregressive model (AR). Autoregressive model is an umbrella definition for
    any model that sequentially generates the components (atoms or fragments) of a
    molecule. ARs better capture the interdependency within the molecular structure
    and allows for explicit valency check. For each step in AR, the new component
    can be produced using different techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression/classification, such is the case with 3D-SBDD (Luo et al., [2021b](#bib.bib242)),
    Pocket2Mol (Peng et al., [2022](#bib.bib276)), etc.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning, such is the case with L-Net (Li et al., [2021a](#bib.bib218)),
    DeepLigBuilder (Li et al., [2021b](#bib.bib219)), etc.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probabilistic models like normalizing flow and diffusion.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Normalizing flow (NF). Based on the change-of-variable theorem, NF (Rezende
    and Mohamed, [2015](#bib.bib295)) constructs an invertible mapping $f$ between
    a complex data distribution $\boldsymbol{x}\sim X$: and a normalized latent distribution
    $\boldsymbol{z}\sim Z$. Unlike VAE, which has juxtaposed parameters for encoder
    and decoder, the flow model uses the same set of parameter for encoding and encoding:
    reverse flow $f^{-1}$ for generation, and forward flow $f$ for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (128) |  | $\displaystyle\max_{f}\log p(\boldsymbol{x})$ | $\displaystyle=\log
    p_{K}\left(\boldsymbol{z}_{K}\right)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (129) |  |  | $\displaystyle=\log p_{K-1}\left(\boldsymbol{z}_{K-1}\right)-\log\left&#124;\operatorname{det}\left(\frac{df_{K}\left(\boldsymbol{z}_{K-1}\right)}{d\boldsymbol{z}_{K-1}}\right)\right&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (130) |  |  | $\displaystyle=\ldots$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (131) |  |  | $\displaystyle=\log p_{0}\left(\boldsymbol{z}_{0}\right)-\sum_{i=1}^{K}\log\left&#124;\operatorname{det}\left(\frac{df_{i}\left(\boldsymbol{z}_{i-1}\right)}{d\boldsymbol{z}_{i-1}}\right)\right&#124;,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $f=f_{K}\circ f_{K-1}\circ...\circ f_{1}$ is a composite of $K$ blocks
    of transformation. While GraphNVP (Madhawa et al., [2019](#bib.bib252)) generates
    the molecular graph with NF in one go, following works tend to use the autoregressive
    flow model, including GraphAF (Shi et al., [2020](#bib.bib320)), GraphDF (Luo
    et al., [2021d](#bib.bib247)), and GraphBP (Liu et al., [2022a](#bib.bib231)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Diffusion model (DM). Diffusion models (Sohl-Dickstein et al., [2015](#bib.bib328);
    Song and Ermon, [2019](#bib.bib331); Ho et al., [2020](#bib.bib141)) define a
    Markov chain of diffusion steps to slowly add random noise to data $\boldsymbol{x}_{0}\sim
    q(\boldsymbol{x})$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (132) |  | $\displaystyle q(\boldsymbol{x}_{t}&#124;\boldsymbol{x}_{t-1})$
    | $\displaystyle=\mathcal{N}(\boldsymbol{x}_{t};\sqrt{1-\beta_{t}}\boldsymbol{x}_{t-1},\beta_{t}\boldsymbol{I}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (133) |  | $\displaystyle q(\boldsymbol{x}_{1:T}&#124;\boldsymbol{x}_{0})$
    | $\displaystyle=\prod^{T}_{t=1}q(\boldsymbol{x}_{t}&#124;\boldsymbol{x}_{t-1}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'They then learn to reverse the diffusion process to construct desired data
    samples from the noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (134) |  | $\displaystyle p_{\theta}(\boldsymbol{x}_{0:T})$ | $\displaystyle=p(\boldsymbol{x}_{T})\prod^{T}_{t=1}p_{\theta}(\boldsymbol{x}_{t-1}&#124;\boldsymbol{x}_{t}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (135) |  | $\displaystyle p_{\theta}(\boldsymbol{x}_{t-1}&#124;\boldsymbol{x}_{t})$
    | $\displaystyle=\mathcal{N}(\boldsymbol{x}_{t-1};\boldsymbol{\mu}_{\theta}(\boldsymbol{x}_{t},t),\boldsymbol{\Sigma}_{\theta}(\boldsymbol{x}_{t},t)),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: while the models are trained using a variational lower bound. Diffusion models
    have been applied to generate unbounded 3D molecules in EDM (Hoogeboom et al.,
    [2022](#bib.bib143)), and binding-specific ligands in DiffSBDD (Schneuing et al.,
    [2022](#bib.bib310)) and DiffBP (Lin et al., [2022a](#bib.bib227)). Diffusion
    can also be applied to generate molecular fragments in autoregressive models,
    as is the case with FragDiff (Peng et al., [2023](#bib.bib275)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 10\. Summary of molecular generation models.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | 2D/3D | Binding- based | Fragment- based | GNN Backbone | Generative
    Model |'
  prefs: []
  type: TYPE_TB
- en: '| GCPN (You et al., [2018](#bib.bib419)) | 2D |  |  | GCN (Kipf and Welling,
    [2016a](#bib.bib183)) | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| MolGAN (De Cao and Kipf, [2018](#bib.bib66)) | 2D |  |  | R-GCN (Schlichtkrull
    et al., [2018](#bib.bib308)) | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| DEFactor (Assouel et al., [2018](#bib.bib12)) | 2D |  |  | GCN | GAN |'
  prefs: []
  type: TYPE_TB
- en: '| GraphVAE (Simonovsky and Komodakis, [2018](#bib.bib326)) | 2D |  |  | ECC (Simonovsky
    and Komodakis, [2017](#bib.bib325)) | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| MDVAE (Du et al., [2022b](#bib.bib80)) | 2D |  |  | GGNN (Li et al., [2015](#bib.bib220))
    | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| JT-VAE (Jin et al., [2018](#bib.bib166)) | 2D |  | $\checkmark$ | MPNN (Gilmer
    et al., [2017](#bib.bib116)) | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| CGVAE (Liu et al., [2018a](#bib.bib232)) | 2D |  |  | GGNN | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| DeepScaffold (Li et al., [2019a](#bib.bib217)) | 2D |  | $\checkmark$ | GCN
    | VAE |'
  prefs: []
  type: TYPE_TB
- en: '| GraphNVP (Madhawa et al., [2019](#bib.bib252)) | 2D |  |  | R-GCN | NF |'
  prefs: []
  type: TYPE_TB
- en: '| MoFlow (Zang and Wang, [2020](#bib.bib430)) | 2D |  |  | R-GCN | NF |'
  prefs: []
  type: TYPE_TB
- en: '| GraphAF (Shi et al., [2020](#bib.bib320)) | 2D |  |  | R-GCN | NF + AR |'
  prefs: []
  type: TYPE_TB
- en: '| GraphDF (Luo et al., [2021d](#bib.bib247)) | 2D |  |  | R-GCN | NF + AR |'
  prefs: []
  type: TYPE_TB
- en: '| L-Net (Li et al., [2021a](#bib.bib218)) | 3D |  | $\checkmark$ | g-U-Net (Gao
    and Ji, [2019](#bib.bib105)) | AR |'
  prefs: []
  type: TYPE_TB
- en: '| G-SchNet (Gebauer et al., [2019](#bib.bib112)) | 3D |  |  | SchNet (Schütt
    et al., [2018](#bib.bib312)) | AR |'
  prefs: []
  type: TYPE_TB
- en: '| GEN3D (Roney et al., [2021](#bib.bib298)) | 3D |  |  | EGNN (Satorras et al.,
    [2021](#bib.bib305)) | AR |'
  prefs: []
  type: TYPE_TB
- en: '| G-SphereNet (Luo and Ji, [2022](#bib.bib246)) | 3D |  |  | SphereNet (Liu
    et al., [2022b](#bib.bib234)) | NF + AR |'
  prefs: []
  type: TYPE_TB
- en: '| EDM (Hoogeboom et al., [2022](#bib.bib143)) | 3D |  |  | EGNN | DM |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-SBDD (Luo et al., [2021b](#bib.bib242)) | 3D | $\checkmark$ |  | SchNet
    | AR |'
  prefs: []
  type: TYPE_TB
- en: '| Pocket2Mol (Peng et al., [2022](#bib.bib276)) | 3D | $\checkmark$ |  | GVP (Jing
    et al., [2020](#bib.bib168)) | AR |'
  prefs: []
  type: TYPE_TB
- en: '| FLAG (ZHANG et al., [[n. d.]](#bib.bib449)) | 3D | $\checkmark$ | $\checkmark$
    | SchNet | AR |'
  prefs: []
  type: TYPE_TB
- en: '| GraphBP (Liu et al., [2022a](#bib.bib231)) | 3D | $\checkmark$ |  | SchNet
    | NF + AR |'
  prefs: []
  type: TYPE_TB
- en: '| DiffBP (Lin et al., [2022a](#bib.bib227)) | 3D | $\checkmark$ |  | EGNN |
    DM |'
  prefs: []
  type: TYPE_TB
- en: '| DiffSBDD (Schneuing et al., [2022](#bib.bib310)) | 3D | $\checkmark$ |  |
    EGNN | DM |'
  prefs: []
  type: TYPE_TB
- en: '| FragDiff (Peng et al., [2023](#bib.bib275)) | 2D + 3D | $\checkmark$ | $\checkmark$
    | MPNN | DM + AR |'
  prefs: []
  type: TYPE_TB
- en: 12.3\. Summary and prospects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We wrap up this chapter with Table [10](#S12.T10 "Table 10 ‣ 12.2\. Generative
    methods for molecular graphs ‣ 12\. Molecular Generation By Fang Sun ‣ A Comprehensive
    Survey on Deep Graph Representation Learning"), which profiles existing molecular
    generation models according to their taxonomy for molecular featurization, the
    GNN backbone, and the generative method. This chapter covers the critical topics
    of molecular generation, which also elicit valuable insights into the promising
    directions for future research. We summarize these important aspects as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques. Graph neural networks can be flexibly leveraged to encode molecular
    features on different representation levels and across different problem settings.
    Canonical GNNs like GCN (Kipf and Welling, [2016a](#bib.bib183)), GAT (Veličković
    et al., [2017](#bib.bib352)), and R-GCN (Schlichtkrull et al., [2018](#bib.bib308))
    have been widely adopted to model 2D molecular graphs, while 3D equivariant GNNs
    have also been effective in modeling 3D molecular graphs. In particular, this
    3D approach can be readily extended to binding-based scenarios, where the 3D geometry
    of the binding protein receptor is considered alongside the ligand geometry per
    se. Fragment-based models like JT-VAE (Jin et al., [2018](#bib.bib166)) and L-Net (Li
    et al., [2021a](#bib.bib218)) can also effectively capture the hierarchical molecular
    structure. Various generative methods have also been effectively incorporated
    into the molecular setting, including generative adversarial network (GAN), variational
    auto-encoder (VAE), autoregressive model (AR), normalizing flow (NF), and diffusion
    model (DM). These models have been able to generate valid 2D molecular topologies
    and realistic 3D molecular geometries, greatly accelerating the search for drug
    candidates.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and Limitations. While there has been an abundant supply of unlabelled
    molecular structural and geometric data (Irwin et al., [2012](#bib.bib156); Spackman
    et al., [2016](#bib.bib332); Francoeur et al., [2020](#bib.bib99)), the number
    of labeled molecular data over certain critical biochemical properties like toxicity (Gayvert
    et al., [2016](#bib.bib111)) and solubility (Delaney, [2004](#bib.bib68)) remain
    very limited. On the other hand, existing models have heavily relied on expert-crafted
    metrics to evaluate the quality of the generated molecules, such as QED and Vina (Eberhardt
    et al., [2021](#bib.bib83)), rather than actual wet lab experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Future Works. Besides the structural and geometric attributes described in this
    chapter, an even more extensive array of data can be applied to aid molecular
    generation, including chemical reactions and medical ontology. These data can
    be organized into a heterogeneous knowledge graph to aid the extraction of high-quality
    molecular representations. Furthermore, high throughput experimentation (HTE)
    should be adopted to realistically evaluate the synthesizablity and druggability
    of the generated molecules in the wet lab. Concrete case studies, such as the
    design of potential inhibitors to SARS-CoV-2 (Li et al., [2021b](#bib.bib219)),
    would be even more encouraging, bringing new insights into leveraging these molecular
    generative models to facilitate the design and fabrication of potent and applicable
    drug molecules in the pharmaceutical industry.
  prefs: []
  type: TYPE_NORMAL
- en: 13\. Recommender Systems By Yifang Qin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The use of graph representation learning in recommender systems has been drawing
    increasing attention as one of the key strategies for addressing the issue of
    information overload. With their strong ability to capture high-order connectivity
    between graph nodes, deep graph representation learning has been shown to be beneficial
    in enhancing recommendation performance across a variety of recommendation scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical recommender systems take the observed interactions between users and
    items and their fixed features as input, and are intended for making proper predictions
    on which items a specific user is probably interested in. To formulate, given
    an user set $\mathcal{U}$, an item set $\mathcal{I}$ and the interaction matrix
    between users and items $X\in\{0,1\}^{\left|\mathcal{U}\right|\times\left|\mathcal{I}\right|}$,
    where $X_{u,v}$ indicates there is an observed interaction between user $u$ and
    item $i$. The target of GNNs on recommender systems is to learn representations
    $h_{u},h_{i}\in\mathbb{R}^{d}$ for given $u$ and $i$. The pereference score can
    further be calculated by a similarity function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (136) |  | $\hat{x}_{u,i}=f(h_{u},h_{i}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $f(\cdot,\cdot)$ is the similarity function, e.g. inner product, consine
    similarity, multi-layer perceptrons that takes the representation of $u$ and $i$
    and calculate the pereference score $\hat{x}_{u,i}$.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to adapting graph representation learning in recommender systems,
    a key step is to construct graph-structured data from the interaction set $X$.
    Generally, a graph is represented as $\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$
    where $\mathcal{V},\mathcal{E}$ denotes the set of vertices and edges respectively.
    According the construction of $\mathcal{G}$, we can categorize the existing works
    as follows into three parts which are introduced in following subsections. A summary
    is provided in Table [11](#S13.T11 "Table 11 ‣ 13\. Recommender Systems By Yifang
    Qin ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  prefs: []
  type: TYPE_NORMAL
- en: Table 11\. Summary of graph models for recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Recommendation Task | Graph Structure | Graph Encoder | Representation
    |'
  prefs: []
  type: TYPE_TB
- en: '| GC-MC (Berg et al., [2017](#bib.bib21)) | Matrix Completion | User-Item Graph
    | GCN | Last-Layer |'
  prefs: []
  type: TYPE_TB
- en: '| NGCF (Wang et al., [2019a](#bib.bib368)) | Collaborative Filtering | User-Item
    Graph | GCN+Affinity | Concatenation |'
  prefs: []
  type: TYPE_TB
- en: '| MMGCN (Wei et al., [2019](#bib.bib378)) | Micro-Video | Multi-Modal Graph
    | GCN | Last-Layer |'
  prefs: []
  type: TYPE_TB
- en: '| LightGCN (He et al., [2020](#bib.bib135)) | Collaborative Filtering | User-Item
    Graph | LGC | Mean-Pooling |'
  prefs: []
  type: TYPE_TB
- en: '| DGCF (Wang et al., [2020b](#bib.bib370)) | Collaborative Filtering | User-Item
    Graph | Dynamic Routing | Mean-Pooling |'
  prefs: []
  type: TYPE_TB
- en: '| SR-GNN (Wu et al., [2019c](#bib.bib386)) | Session-based | Transition Graph
    | GGNN | Soft-Attention |'
  prefs: []
  type: TYPE_TB
- en: '| GC-SAN (Wu et al., [2019c](#bib.bib386); Xu et al., [2019b](#bib.bib400))
    | Session-based | Session Graph | GGNN | Self-Attention |'
  prefs: []
  type: TYPE_TB
- en: '| FGNN (Qiu et al., [2019](#bib.bib288)) | Session-based | Session Graph |
    GAT | Last-Layer |'
  prefs: []
  type: TYPE_TB
- en: '| GAG (Qiu et al., [2020b](#bib.bib289)) | Session-based | Session Graph |
    GCN | Self-Attention |'
  prefs: []
  type: TYPE_TB
- en: '| GCE-GNN (Wang et al., [2020d](#bib.bib376)) | Session-based | Transition+Global
    | GAT | Sum-Pooling |'
  prefs: []
  type: TYPE_TB
- en: '| HyperRec (Wang et al., [2020a](#bib.bib362)) | Sequence-based | Sequential
    HyperGraph | HGCN | Self-Attention |'
  prefs: []
  type: TYPE_TB
- en: '| DHCF (Ji et al., [2020](#bib.bib159)) | Collaborative Filtering | Dual HyperGraph
    | JHConv | Last-Layer |'
  prefs: []
  type: TYPE_TB
- en: '| MBHT (Yang et al., [2022b](#bib.bib412)) | Sequence-based | Learnable HyperGraph
    | Transformer | Cross-View Attention |'
  prefs: []
  type: TYPE_TB
- en: '| HCCF (Xia et al., [2022a](#bib.bib393)) | Collaborative Filtering | Learnable
    HyperGraph | HGCN | Last-Layer |'
  prefs: []
  type: TYPE_TB
- en: 13.1\. User-Item Bipartite Graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 13.1.1\. Graph Construction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A undirected bipartite graph where the vertex set $\mathcal{V}=\mathcal{U}\cup\mathcal{I}$
    and the undirected edge set $\mathcal{E}=\{(u,i)|u\in\mathcal{U}\land i\in\mathcal{I}\}$.
    Under this case the graph adjacency can be directly obtained from the interaction
    matrix, thus the optimization target on the user-item bipartite graph is equivalent
    to collaborative filtering tasks such as MF (Koren et al., [2009](#bib.bib192))
    and SVD++ (Koren, [2008](#bib.bib191)).
  prefs: []
  type: TYPE_NORMAL
- en: There have been plenty of previous works that applied GNNs on the constructed
    user-item bipartite graphs. GC-MC (Berg et al., [2017](#bib.bib21)) firstly applies
    graph convolution networks to user-item recommendation and optimizes a graph autoencoder
    (GAE) to reconstruct interactions between users and items. NGCF (Wang et al.,
    [2019a](#bib.bib368)) introduces the concept of Collaborative Filtering (CF) into
    graph-based recommendations by modeling the affinity between neighboring nodes
    on the interaction graph. MMGCN (Wei et al., [2019](#bib.bib378)) extend graph-based
    recommendation to multi-modal scenarios by constructing different subgraphs for
    each modal. LightGCN (He et al., [2020](#bib.bib135)) improves NGCF by removing
    the non-linear activation functions and simplifying the message function. With
    the development of disentangled representation learning, there are works like
    DGCF (Wang et al., [2020b](#bib.bib370)) introduce disentangled graph representation
    learning to represent users and items from multiple disentangled perspective
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.2\. Graph Propagation Scheme
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A common practice is to follow the traditional message-passing networks (MPNNs)
    and design the graph propagation method accordingly. GC-MC adopt vanilla GCNs
    to encode the user-item bipartite graph. NGCF enhance GCNs by considering the
    affinity between users and items. The message function of NGCF from node $j$ to
    $i$ is formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (137) |  | $\begin{cases}m_{i\leftarrow j}=\frac{1}{\sqrt{&#124;\mathcal{N}_{i}&#124;&#124;\mathcal{N}_{j}&#124;}}(W_{1}e_{j}+W_{2}(e_{i}\odot
    e_{j}))\\ m_{i\leftarrow i}=W_{1}e_{i}\end{cases},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $W_{1},W_{2}$ are trainable parameters, $e_{i}$ represents $i$’s representation
    from previous layer. The matrix form can be further provided by:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (138) |  | $E^{(l)}=\text{LeakyReLU}((\mathcal{L}+I)E^{(l-1)}W_{1}^{(l)}+\mathcal{L}E^{(l-1)}\odot
    E^{(l-1)}W_{2}^{(l)}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}$ represents the Laplacian matrix of the user-item graph.
    The element-wise product in Eq. [138](#S13.E138 "In 13.1.2\. Graph Propagation
    Scheme ‣ 13.1\. User-Item Bipartite Graph ‣ 13\. Recommender Systems By Yifang
    Qin ‣ A Comprehensive Survey on Deep Graph Representation Learning") represents
    the affinity between connected nodes, containing the collaborative signals from
    interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the notably heaviness and burdensome calculation of NGCF’s architecture
    hinders the model from making faster recommendations on larger graphs. LightGCN
    solves this issue by proposing Light Graph Convolution (LGC), which simplifies
    the convolution operation with:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (139) |  | $e_{i}^{(l+1)}=\sum_{j\in\mathcal{N}_{i}}\frac{1}{\sqrt{&#124;\mathcal{N}_{i}&#124;&#124;\mathcal{N}_{j}&#124;}}e_{j}^{(l)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'When an interaction takes place, e.g. a user clickes a particular item, there
    could be multiple intentions behind the observed interaction. Thus it is necessary
    to consider the various disentangled intentions among users and items. DGCF proposes
    the cross-intent embedding propagation scheme on graph, inspired by the dynamic
    routing algorithm of capsule netwroks (Sabour et al., [2017](#bib.bib303)). To
    formulate, the propagation process maintains a set of routing logits $\tilde{S}_{k}(u,i)$
    for each user $u$. The weighted sum aggregator to get the representation of $u$
    can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (140) |  | $u_{k}^{t}=\sum_{i\in\mathcal{N}_{u}}\mathcal{L}_{k}^{t}(u,i)\cdot
    i_{k}^{0}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'for $t$-th iteration, where $\mathcal{L}_{k}^{t}(u,i)$ denotes the Laplacian
    matrix of $S_{k}^{t}(u,i)$, formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (141) |  | $\mathcal{L}_{k}^{t}(u,i)=\frac{S_{k}^{t}}{\sqrt{[\sum_{i^{\prime}\in\mathcal{N}_{u}}S_{k}^{t}(u,i^{\prime})]\cdot[\sum_{u^{\prime}\in\mathcal{N}_{i}}S_{k}^{t}(u^{\prime},i)]}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 13.1.3\. Node Representations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After the graph propagation module outputs node-level representations, there
    are multiple methods to leverage node representations for recommendation tasks.
    A plain solution is to apply a readout function on layer outputs like the concatenation
    operation used by NGCF:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (142) |  | $e^{*}=Concat(e^{(0)},...,e^{(L)})=e^{(0)}\&#124;...\&#124;e^{(L)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'However, the readout function among layers would neglects the relationship
    between target item and current user. A general solution is to use attention mechanism
    (Vaswani et al., [2017](#bib.bib351)) to reweight and aggregate the node representations.
    SR-GNN adapts soft-attention mechanism to model the item-item relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (143) |  | $\begin{split}\alpha_{i}&amp;=\textbf{q}^{T}\sigma(W_{1}e_{t}+W_{2}e_{i}+c),\\
    s_{g}&amp;=\sum_{i=1}^{n-1}\alpha_{i}e_{i},\end{split}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\textbf{q},\ W_{1},\ W_{2}$ are trainable matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some methods focus on exploiting information from multiple graph structures.
    A common practice is contrastive learning, which maximize the mutual information
    between hidden representations from several views. HCCF leverage InfoNCE loss
    as the estimator of mutual information, given a pair of representation $z_{i},\Gamma_{i}$
    for node $i$, controlled by temperature parameter $\tau$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (144) |  | $\mathcal{L}_{InfoNCE}(i)=-\log\frac{\exp(cosine(z_{i},\Gamma_{i}))/\tau}{\sum_{i^{\prime}\neq
    i}\exp(cosine(z_{i},\Gamma_{i^{\prime}}))/\tau}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Besides InfoNCE, there exists several other ways to combine node representations
    from different views. For instance, MBHT applies attention mechanism to fuse multiple
    semantics, DisenPOI adapt bayesian personalized ranking loss (BPR) (Rendle et al.,
    [2012](#bib.bib294)) as a soft estimator for contrastive learning, and KBGNN applies
    pair-wise similarities to ensure the consistency from two views.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2\. Transition Graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 13.2.1\. Transition Graph Construction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since sequence-based recommendation (SR) is one of the fundamental problems
    in recomemnder systems, some researches focus on modeling the sequential information
    with GNNs. A commonly applied way is to construct transition graphs based on each
    given sequence according to the clicking sequence by a user. To formulate, given
    a user $u$’s clicking sequence $s_{u}=[i_{u,1},i_{u,2},...,i_{u,n}]$ containing
    $n$ items, noting that there could be duplicated items, the sequential graph is
    constructed via $\mathcal{G}_{s}=\{\text{SET}(s_{u}),\mathcal{E}\}$, where $\forall\left<i_{j},i_{k}\right>\in\mathcal{E}$
    indicates there exists a successive transition from $i_{j}$ to $i_{k}$. Since
    $\mathcal{G}_{s}$ are directed graphs, a widely used way to depict graph connectivity
    is by building the connection matrix $A_{s}\in\mathbb{R}^{n\times 2n}$. $A_{s}$
    is the combination of two adjacency matrices $A_{s}=[A_{s}^{(in)};A_{s}^{(out)}]$,
    which denotes the normalized node degrees of incoming and outgoing edges in the
    session graph respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed transition graphs that obtain user behavior patterns has been demonstrated
    important to session-based recommendations (Li et al., [2017b](#bib.bib211); Liu
    et al., [2018d](#bib.bib233)). SR-GNN and GC-SAN (Wu et al., [2019c](#bib.bib386);
    Xu et al., [2019b](#bib.bib400)) propose to leverage transition graphs and applies
    attention-based GNNs to capture the sequential information for session-based recommendation.
    FGNN (Qiu et al., [2019](#bib.bib288)) formulates the recommendation within a
    session as a graph classification problem to predict next item for an anonymous
    user. GAG (Qiu et al., [2020b](#bib.bib289)) and GCE-GNN (Wang et al., [2020d](#bib.bib376))
    further extends the model to capture global embeddings among multiple session
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2.2\. Session Graph Propagation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since the session graphs are directed item graphs, there have been multiple
    session graph propagation methods to obtain node representations on session graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'SR-GNN leverages Gated Graph Neural Networks (GGNNs) to obtain sequential information
    from a given session graph adjacency $A_{s}=[A_{s}^{(in)};A_{s}^{(out)}]$ and
    item embedding set $\{e_{i}\}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (145) |  | $\displaystyle a_{t}$ | $\displaystyle=A_{s}[e_{1},...,e_{t-1}]^{T}H+b,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (146) |  | $\displaystyle z_{t}$ | $\displaystyle=\sigma(W_{z}a_{t}+U_{z}e_{t-1}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (147) |  | $\displaystyle r_{t}$ | $\displaystyle=\sigma(W_{r}a_{t}+U_{r}e_{t-1}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (148) |  | $\displaystyle\tilde{e_{t}}$ | $\displaystyle=\tanh(W_{o}a_{t}+U_{o}(r_{t}\odot
    e_{t-1})),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (149) |  | $\displaystyle e_{t}$ | $\displaystyle=(1-z_{t})\odot e_{t-1}+z_{t}\tilde{e_{t}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $W$s and $U$s are trainable parameters. GC-SAN extend GGNN by calculating
    initial state $a_{t}$ separately to better exploit transition information:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (150) |  | $a_{t}=Concat(A_{s}^{(in)}([e_{1},...,e_{t-1}W_{a}^{(in)}]+b^{(in)}),A_{s}^{(out)}([e_{1},...,e_{t-1}W_{a}^{(out)}]+b^{(out)})).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 13.3\. HyperGraph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 13.3.1\. Hypergraph Topology Construction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Motivated by the idea of modeling hyper-structures and high-order correlation
    among nodes, hypergraphs (Feng et al., [2019](#bib.bib94)) are proposed as extentions
    of the commonly used graph structures. For graph-based recommender systems, a
    common practice is to construct hyper structures among the original user-item
    bipartite graphs. To be specific, an incidence matrix of a graph with vertex set
    $\mathcal{V}$ is presented as a binary matrix $H\in\{0,1\}^{|\mathcal{V}|\times|\mathcal{E}|}$,
    where $\mathcal{E}$ represents the set of hyperedges. Each entry $h(v,e)$ of $H$
    depicts the connectivity between vertex $v$ and hyperedge $e$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (151) |  | $h(v,e)=\begin{cases}1\ if\ v\in e\\ 0\ if\ v\notin e\end{cases}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Given the formulation of hypergraphs, the degrees of vertices and hyperedges
    of $H$ can then be defined with two diagnal matrices $D_{v}\in\mathbb{N}^{|\mathcal{V}|\times|\mathcal{V}|}$
    and $D_{e}\in\mathbb{N}^{|\mathcal{E}|\times|\mathcal{E}|}$, where
  prefs: []
  type: TYPE_NORMAL
- en: '| (152) |  | $D_{v}(i;i)=\sum_{e\in\mathcal{E}}h(v_{i},e),\ \ \ \ D_{e}(j;j)=\sum_{v\in\mathcal{V}}h(v,e_{j}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: With the development of Hypergraph Neural Networks (HGNNs) (Feng et al., [2019](#bib.bib94);
    Zhou et al., [2006](#bib.bib461); Huang et al., [2015](#bib.bib152)) have shown
    to be capable of capturing the high-order connectivity between nodes. HyperRec
    (Wang et al., [2020a](#bib.bib362)) firstly attempts to leverage hypergraph structures
    for sequential recommendation by connecting items with hyperedges according to
    the interactions with users during different time periods. DHCF (Ji et al., [2020](#bib.bib159))
    proposes to construct hypergraphs for users and items respectively based on certain
    rules, to explicit capture the collaborative similarities via HGNNs. MBHT (Yang
    et al., [2022b](#bib.bib412)) combines hypergraphs with low-rank self-attention
    mechanism to capture the dynamic heterogeneous relationships between users and
    items. HCCF (Xia et al., [2022a](#bib.bib393)) uses the contrastive information
    between hypergraph and interaction graph to enhance the recommendation performance.
  prefs: []
  type: TYPE_NORMAL
- en: 13.3.2\. Hyper Graph Message Passing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With the development of HGNNs, previous works have propose different variants
    of HGNN to better exploit hypergraph structures. A classic high-order hyper convolution
    process on a fixed hypergraph $\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$ with hyper
    adjacency $H$ is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (153) |  | $g\star X=D_{v}^{-1/2}HD_{e}^{-1}H^{T}D_{v}^{-1/2}X\Theta,$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $D_{v},\ D_{e}$ are degree matrices of nodes and hyperedges, $\Theta$
    denotes the convolution kernel. For hyper adjacency matrix $H$, DHCF refers to
    a rule-based hyperstructure via k-order reachable rule, where nodes in the same
    hyperedge group are k-order reachable to each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (154) |  | $A_{u}^{k}=\min(1,\text{power}(A\cdot A^{T},k)),$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $A$ denotes the graph adjacency matrix. By considering the situations
    where $k=1,2$, the matrix formulation of the hyper connectivity of users and items
    are calculated with:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (155) |  | $\begin{cases}H_{u}=A\&#124;(A(A^{T}A))\\ H_{i}=A^{T}\&#124;(A^{T}(AA^{T}))\end{cases},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: which depicts the dual hypergraphs for users and items.
  prefs: []
  type: TYPE_NORMAL
- en: 'HCCF proposes to construct a learnable hypergraph to depict the global dependencies
    between nodes on the interaction graph. To be specific, the hyperstructure is
    factorized with two low-rank embedding matrices to achieve model efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (156) |  | $H_{u}=E_{u}\cdot W_{u},\ H_{v}=E_{v}\cdot W_{v}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 13.4\. Other Graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since there are a variety of recommendation scenarios, several tailored designed
    graph structures have been proposed accordingly, to better exploit the domain
    information from different scenarios. For instance, CKE (Zhang et al., [2016](#bib.bib434))
    and MKR (Wang et al., [2019e](#bib.bib361)) introduce Knowledge graphs to enhance
    graph recommendation. GSTN (Wang et al., [2022e](#bib.bib377)), KBGNN (Ju et al.,
    [2022c](#bib.bib174)) and DisenPOI (Qin et al., [2022](#bib.bib286)) propose to
    build geographical graphs based on the distance between Poit-of-Interests (POIs)
    to better model the locality of users’ visiting patterns. TGSRec (Fan et al.,
    [2021](#bib.bib88)) and DisenCTR (Wang et al., [2022c](#bib.bib372)) empower the
    user-item interaction graphs with temporal sampling between layers to obtain sequential
    information from static bipartite graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 13.5\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section introduces the application of different kinds of graph neural
    networks in recommender systems and can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph Constructions. There are multiple options for constructing graph-structured
    data for a variety of recommendation tasks. For instance, the user-item bipartite
    graphs reveal the high-order collaborative similarity between users and items,
    and the transition graph is suitable for encoding sequential information in clicking
    history. These diversified graph structures provide different views for node representation
    learning on users and items, and can be further used for downstream ranking tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and Limitations. Though the superiority of graph-structured data
    and GNNs against traditional methods has been widely illustrated, there are still
    challenges unsolved. For example, the computational cost of graph methods are
    normally expensive and thus unacceptable in real-world applications. The data
    sparsity and cold-started issue in graph recommendation remains to be explored
    as well.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Works. In the future, the efficient solution of applying GNNs in recommendation
    tasks is expected. There are also some attempts (Fan et al., [2021](#bib.bib88);
    Wang et al., [2022c](#bib.bib372)) on incorporating temporal information in graph
    representation learning for sequential recommendation tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 14\. Traffic Analysis By Zheng Fang
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Intelligent Transportation Systems (ITS) are essential for safe, reliable, and
    efficient transportation in smart cities, serving the daily commuting and traveling
    needs of millions of people. To support ITS, advanced modeling and analysis techniques
    are necessary, and Graph Neural Networks (GNNs) are a promising tool for traffic
    analysis. GNNs can effectively model spatial correlations, making them well-suited
    for analyzing complex transportation networks. As such, GNNs have garnered significant
    interest in the traffic domain for their ability to provide insights into traffic
    patterns and behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we first conclude the main GNN research directions in traffic
    domain, and then we summarize the typical graph construction processes in different
    traffic scenes and datasets. Finally, we list the classical GNN workflows for
    dealing with tasks in traffic networks. A summary is provided in Table [12](#S14.T12
    "Table 12 ‣ 14.1\. Research Directions in Traffic Domain ‣ 14\. Traffic Analysis
    By Zheng Fang ‣ A Comprehensive Survey on Deep Graph Representation Learning").
  prefs: []
  type: TYPE_NORMAL
- en: 14.1\. Research Directions in Traffic Domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We summarize main GNN research directions in traffic domain as follows,
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic Flow Forecasting. Traffic flow forecasting plays an indispensable role
    in ITS (Ran and Boyce, [2012](#bib.bib292); Dimitrakopoulos and Demestichas, [2010](#bib.bib73)),
    which involves leveraging spatial-temporal data collected by various sensors to
    gain insights into future traffic patterns and behaviors. Classic methods, like
    autoregressive integrated moving average (ARIMA) (Box and Pierce, [1970](#bib.bib28)),
    support vector machine (SVM) (Hearst et al., [1998](#bib.bib137)) and recurrent
    neural networks (RNN) (Connor et al., [1994](#bib.bib64)) can only model time
    series separately without considering their spatial connections. To address this
    issue, graph neural networks (GNNs) have emerged as a powerful approach for traffic
    forecasting due to their strong ability of modeling complex graph-structured correlations
    (Jiang and Luo, [2022](#bib.bib161); Xie et al., [2020](#bib.bib394); Bui et al.,
    [2021](#bib.bib31)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trajectory Prediction. Trajectory prediction is a crucial task in various applications,
    such as autonomous driving and traffic surveillance, which aims to forecast future
    positions of agents in the traffic scene. However, accurately predicting trajectories
    can be challenging, as the behavior of an agent is influenced not only by its
    own motion but also by interactions with surrounding objects. To address this
    challenge, Graph Neural Networks (GNNs) have emerged as a promising tool for modeling
    complex interactions in trajectory prediction (Mohamed et al., [2020](#bib.bib264);
    Cao et al., [2021a](#bib.bib35); Zhou et al., [2021](#bib.bib463); Sun et al.,
    [2020b](#bib.bib337)). By representing the scene as a graph, where each node corresponds
    to an agent and the edges capture interactions between them, GNNs can effectively
    capture spatial dependencies and interactions between agents. This makes GNNs
    well-suited for predicting trajectories that accurately capture the behavior of
    agents in complex traffic scenes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic Anomaly Detection. Anomaly detection is an essential support for ITS.
    There are lots of traffic anomalies in daily transportation systems, for example,
    traffic accidents, extreme weather and unexpected situations. Handling these traffic
    anomalies timely can improve the service quality of public transportation. The
    main trouble of traffic anomaly detection is the highly twisted spatial-temporal
    characteristics of traffic data. The criteria and influence of traffic anomaly
    varies among locations and times. GNNs have been introduced and achieved success
    in this domain (Deng et al., [2022](#bib.bib70); Chen et al., [2021a](#bib.bib54);
    Zhang et al., [2022b](#bib.bib435); Deng and Hooi, [2021](#bib.bib69)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Others. Traffic demand prediction targets at estimating the future number of
    travelling at some location. It is of vital and practical significance in the
    resource scheduling for ITS. By using GNNs, the spatial dependencies of demands
    can be revealed (Yao et al., [2018](#bib.bib413); Yang et al., [2020](#bib.bib411)).
    What is more, urban vehicle emission analysis is also considered in recent work,
    which is closely related to environmental protection and gains increasing researcher
    attention (Xu et al., [2020](#bib.bib406)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Table 12\. Summary of graph models for traffic analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Tasks | Adjcency matrices | GNN types | Temporal modules |'
  prefs: []
  type: TYPE_TB
- en: '| STGCN(Yu et al., [2017](#bib.bib421)) | Traffic Flow Forecasting | Fixed
    Matrix | GCN | TCN |'
  prefs: []
  type: TYPE_TB
- en: '| DCRNN(Li et al., [2017c](#bib.bib221)) | Traffic Flow Forecasting | Fixed
    Matrix | ChebNet | RNN |'
  prefs: []
  type: TYPE_TB
- en: '| AGCRN (Bai et al., [2020](#bib.bib14)) | Traffic Flow Forecasting | Dynamic
    Matrix | GCN | GRU |'
  prefs: []
  type: TYPE_TB
- en: '| ASTGCN (Guo et al., [2019](#bib.bib126)) | Traffic Flow Forecasting | Fixed
    Matrix | GAT | Attention&TCN |'
  prefs: []
  type: TYPE_TB
- en: '| STSGCN (Song et al., [2020a](#bib.bib329)) | Traffic Flow Forecasting | Dynamic
    Matrix | GCN | Cropping |'
  prefs: []
  type: TYPE_TB
- en: '| GraphWaveNet (Wu et al., [2019a](#bib.bib389)) | Traffic Flow Forecasting
    | Dynamic Matrix | GCN | Gated-TCN |'
  prefs: []
  type: TYPE_TB
- en: '| LSGCN (Huang et al., [2020](#bib.bib151)) | Traffic Flow Forecasting | Fixed
    Matrix | GAT | GLU |'
  prefs: []
  type: TYPE_TB
- en: '| GAC-Net (Song et al., [2020b](#bib.bib330)) | Traffic Flow Forecasting |
    Fixed Matrix | GAT | Gated-TCN |'
  prefs: []
  type: TYPE_TB
- en: '| STGODE (Fang et al., [2021](#bib.bib90)) | Traffic Flow Forecasting | Fixed
    Matrix | Graph ODE | TCN |'
  prefs: []
  type: TYPE_TB
- en: '| STG-NCDE (Choi et al., [2022](#bib.bib58)) | Traffic Flow Forecasting | Dynamic
    Matrix | GCN | NCDE |'
  prefs: []
  type: TYPE_TB
- en: '| MS-ASTN (Wang et al., [2020c](#bib.bib366)) | OD Flow Forecasting | OD Matrix
    | GCN | LSTM |'
  prefs: []
  type: TYPE_TB
- en: '| Social-STGCNN (Mohamed et al., [2020](#bib.bib264)) | Trajectory Prediction
    | Fixed Matrix | GCN | TXP-CNN |'
  prefs: []
  type: TYPE_TB
- en: '| RSBG (Sun et al., [2020b](#bib.bib337)) | Trajectory Prediction | Dynamic
    Matrix | GCN | LSTM |'
  prefs: []
  type: TYPE_TB
- en: '| STGAN (Deng et al., [2022](#bib.bib70)) | Anomaly Detection | Fixed Matrix
    | GCN | GRU |'
  prefs: []
  type: TYPE_TB
- en: '| DMVST-VGNN (Jin et al., [2020b](#bib.bib165)) | Traffic Demand Prediction
    | Fixed Matrix | GAT | GLU |'
  prefs: []
  type: TYPE_TB
- en: '| ST-GRAT (Park et al., [2020](#bib.bib271)) | Traffic Speed Prediction | Fixed
    Matrix | GAT | Attention |'
  prefs: []
  type: TYPE_TB
- en: 14.2\. Traffic Graph Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 14.2.1\. Traffic Graph
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '. The traffic network is represented as a graph $\mathcal{G}=(V,E,A)$, where
    $V$ is the set of $N$ traffic nodes, $E$ is the set of edges, and $A\in\mathbb{R}^{N\times
    N}$ is an adjacency matrix representing the connectivity of $N$ nodes. In the
    traffic domain, $V$ usually represents a set of physical nodes, like traffic stations
    or traffic sensors. The features of nodes typically depend on the specific task.
    Take the traffic flow forecasting as an example. The features are the traffic
    flows, i.e., the historical time series of nodes. The traffic flow can be represented
    as a flow matrix $X\in\mathbb{R}^{N\times T}$, where $N$ is the number of traffic
    nodes and $T$ is the length of historical series, and $X_{nt}$ denotes the traffic
    flow of node $n$ at time $t$. The goal of traffic flow forecasting is to learn
    a mapping function $f$ to predict the traffic flow during future $T^{\prime}$
    steps given the historical $T$ step information, which can be formulated as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (157) |  | $\left[X_{:,t-T+1},X_{:,t-T+2},\cdots,X_{:,t};\mathcal{G}\right]\stackrel{{\scriptstyle
    f}}{{\longrightarrow}}\left[X_{:,t+1},X_{:,t+2},\cdots,X_{:,t+T^{\prime}}\right].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 14.2.2\. Graph Construction.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Constructing a graph to describe the interactions among traffic nodes, i.e.,
    the design of the adjacency matrix $A$, is the key part of traffic analysis. The
    mainstream designs can be divided into two categories, fixed matrix and dynamic
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Fixed matrix. Lots of works assume that the correlations among traffic nodes
    are fixed and constant over time, and they design a fixed and pre-defined adjacency
    matrix to capture the spatial correlation. Here we list several common choices
    of fixed adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The connectivity matrix is the most natural construction way. It relies on the
    support of road map data. The element of the connectivity matrix is defined as
    1 if two node are physically connected and 0 otherwise. This binary format is
    convenient to construct and easy to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: The distance-based matrix is also a common choice, which shows the connection
    between two nodes more precisely. The elements of the matrix are defined as the
    function of distance between two nodes (driving distance or geographical distance).
    A typical way is to use threshold Guassian function as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '| (158) |  | <math  class="ltx_Math" alttext="A_{ij}=\left\{\begin{array}[]{cr}\exp(-\frac{d_{ij}^{2}}{\sigma^{2}}),&amp;d_{ij}<\epsilon\\
    0,&amp;d_{ij}>\epsilon\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{array}\right.," display="block"><semantics ><mrow 
    ><mrow  ><msub
     ><mi 
    >A</mi><mrow  ><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >j</mi></mrow></msub><mo
     >=</mo><mrow 
    ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" 
    ><mtr  ><mtd
     ><mrow 
    ><mrow  ><mi
     >exp</mi><mo 
    >⁡</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow 
    ><mo 
    >−</mo><mstyle displaystyle="false"
     ><mfrac
     ><msubsup
     ><mi
     >d</mi><mrow
     ><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >j</mi></mrow><mn
     >2</mn></msubsup><msup
     ><mi
     >σ</mi><mn
     >2</mn></msup></mfrac></mstyle></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >,</mo></mrow></mtd><mtd
    class="ltx_align_right" columnalign="right"  ><mrow
     ><msub 
    ><mi  >d</mi><mrow
     ><mi 
    >i</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >j</mi></mrow></msub><mo 
    ><</mo><mi  >ϵ</mi></mrow></mtd></mtr><mtr
     ><mtd  ><mrow
     ><mn 
    >0</mn><mo  >,</mo></mrow></mtd><mtd
    class="ltx_align_right" columnalign="right"  ><mrow
     ><msub 
    ><mi  >d</mi><mrow
     ><mi 
    >i</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >j</mi></mrow></msub><mo 
    >></mo><mi  >ϵ</mi></mrow></mtd></mtr></mtable></mrow></mrow><mo
     >,</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝐴</ci><apply
     ><ci 
    >𝑖</ci><ci 
    >𝑗</ci></apply></apply><apply 
    ><csymbol cd="latexml" 
    >cases</csymbol><matrix 
    ><matrixrow  ><apply
     ><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑑</ci><apply
     ><ci
     >𝑖</ci><ci
     >𝑗</ci></apply></apply><cn
    type="integer"  >2</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝜎</ci><cn
    type="integer"  >2</cn></apply></apply></apply></apply><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑑</ci><apply 
    ><ci  >𝑖</ci><ci
     >𝑗</ci></apply></apply><ci
     >italic-ϵ</ci></apply></matrixrow><matrixrow
     ><cn type="integer" 
    >0</cn><apply  ><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑑</ci><apply
     ><ci 
    >𝑖</ci><ci 
    >𝑗</ci></apply></apply><ci 
    >italic-ϵ</ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >A_{ij}=\left\{\begin{array}[]{cr}\exp(-\frac{d_{ij}^{2}}{\sigma^{2}}),&d_{ij}<\epsilon\\
    0,&d_{ij}>\epsilon\\ \end{array}\right.,</annotation></semantics></math> |  |
  prefs: []
  type: TYPE_NORMAL
- en: where $d_{ij}$ is the distance between node $i$ and $j$, and $\sigma$ and $\epsilon$
    are two hyperparameters to control the distribution and the sparsity of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Another kind of fixed adjacency matrix is the similarity-based matrix. In fact,
    similarity matrix is not an adjacency matrix to some extent. It is constructed
    according to the similarity of two nodes, which means the neighbors in the similarity
    graph may be far way in the real world. There are various similarity metrics.
    For example, many works measure the similarity of two node by their functionality,
    e.g., the distribution of surrounding point of interests (POIs). The assumption
    behind is that nodes share similar functionality may share similar traffic patterns.
    We can also define the similarity through the historical flow patterns. To compute
    the similarity of two time series, a common practice is to use Dynamic Time Wrapping
    (DTW) algorithm (Müller, [2007](#bib.bib267)), which is superior to other metrics
    due to its sensitivity to shape similarity rather than point-wise similarity.
    Specifically, given two time series $X=(x_{1},x_{2},\cdots,x_{n})$ and $Y=(y_{1},y_{2},\cdots,y_{n})$,
    DTW is a dynamic programming algorithm defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| (159) |  | $D(i,j)=dist(x_{i},y_{j})+\min\left(D(i-1,j),D(i,j-1),D(i-1,j-1)\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $D(i,j)$ represents the shortest distance between subseries $X=(x_{1},x_{2},\cdots,x_{i})$
    and $Y=(y_{1},y_{2},\cdots,y_{j})$, and $dist(x_{i},y_{j})$ is some distance metric
    like absolute distance. As a result, $DTW(X,Y)=D(n,n)$ is set as the final distance
    between $X$ and $Y$, which better reflects the similarity of two time series compared
    to the Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic matrix. The pre-defined matrix is sometimes unavailable and cannot reflect
    complete information of spatial correlations. The dynamic adaptive matrix is proposed
    to solve the issue. The dynamic matrix is learned from input data automatically.
    To achieve the best prediction performance, the dynamic matrix will manage to
    infer the hidden correlations among nodes, more than those physical connections.
  prefs: []
  type: TYPE_NORMAL
- en: A typical practice is learning adjacency matrix from node embeddings (Bai et al.,
    [2020](#bib.bib14)). Let $E_{A}\in\mathbb{R}^{N\times d}$ be a learnable node
    embedding dictionary, where each row of $E_{A}$ represents the embedding of a
    node, $N$ and $d$ denote the number of node and the dimension of embeddings respectively.
    The graph adjacency matrix is defined as the similarities among node embeddings,
  prefs: []
  type: TYPE_NORMAL
- en: '| (160) |  | $D^{-\frac{1}{2}}AD^{-\frac{1}{2}}=softmax\left(ReLU(E_{A}\cdot
    E_{A}^{T})\right),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $softmax$ function is to perform row-normalization, and $D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$
    is the Laplacian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3\. Typical GNN Frameworks in Traffic Domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we list several representative GNN models for traffic analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3.1\. Spatial Temporal Graph Convolution Network (STGCN) (Yu et al., [2017](#bib.bib421)).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: STGCN is a pioneering work in the spatial-temporal GNN domain. It utilizes graph
    convolution to capture spatial features, and deploys a gated causal convolution
    to extract temporal patterns. Specifically, the graph convolution and temporal
    convolution are defined as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '| (161) |  | $\displaystyle\Theta*_{\mathcal{G}}x$ | $\displaystyle=\theta(I_{n}+D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x=\theta(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}})x,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (162) |  | $\displaystyle\Gamma*_{\mathcal{T}}y$ | $\displaystyle=P\odot\sigma(Q),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\Theta$ is the parameter of graph convolution, $P$ and $Q$ are the outputs
    of an 1-d convolution along the temporal dimension. The sigmoid gate $\sigma(Q)$
    controls how the states of $P$ are relevant for discovering hidden temporal patterns.
    In order to fuse features from both spatial and temporal dimension, the spatial
    convolution layer and the temporal convolution layer are combined to construct
    a spatial temporal block to jointly deal with graph-structured time series, and
    more blocks can be stacked to achieve a more scalable and complex model.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3.2\. Diffusion Convolutional Recurrent Neural Network (DCRNN) (Li et al.,
    [2017c](#bib.bib221)).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'DCRNN is a representative solution combining graph convolution networks with
    recurrent neural networks. It captures spatial dependencies by bidirectional random
    walks on the graph. The diffusion convolution operation on a graph is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (163) |  | $X*_{\mathcal{G}}f_{\theta}=\sum_{k=0}^{K}\left(\theta_{k,1}(D_{O}^{-1}A)^{k}+\theta_{k,2}(D_{I}^{-1}A)^{k}\right)X,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\theta$ are parameters for the convolution filter, and $D_{O}^{-1}A,D_{I}^{-1}A$
    represent the bidirectional diffusion processes respectively. In term of the temporal
    dependency, DCRNN utilizes Gated Recurrent Units (GRU), and replace the linear
    transformation in the GRU with the diffusion convolution as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '| (164) |  | $\displaystyle r^{(t)}$ | $\displaystyle=\sigma(\Theta_{r}*_{\mathcal{G}}[X^{(t)},H^{(t-1)}]+b_{r}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (165) |  | $\displaystyle u^{(t)}$ | $\displaystyle=\sigma(\Theta_{u}*_{\mathcal{G}}[X^{(t)},H^{(t-1)}]+b_{u}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (166) |  | $\displaystyle C^{(t)}$ | $\displaystyle=\tanh(\Theta_{C}*_{\mathcal{G}}[X^{(t)},(r^{(t)}\odot
    H^{(t-1)}]+b_{c}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (167) |  | $\displaystyle H^{(t)}$ | $\displaystyle=u^{(t)}\odot H^{(t-1)}+(1-u^{(t)})\odot
    C^{(t)},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $X^{(t)},H^{(t)}$ denote the input and output at time $t$, $r^{(t)},u^{(t)}$
    are the reset and update gates respectively, and $\Theta_{r},\Theta_{u},\Theta_{C}$
    are parameters of convolution filters. Moreover, DCRNN employs a sequence to sequence
    architecture to predict future series. Both the encoder and the decoder are constructed
    with diffusion convolutional recurrent layers. The historical time series are
    fed into the encoder and the predictions are generated by the decoder. The scheduled
    sampling technique is utilized to solve the discrepancy problem between training
    and test distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 14.3.3\. Adaptive Graph Convolutional Recurrent Network (AGCRN) (Bai et al.,
    [2020](#bib.bib14)).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The focuses of AGCRN are two-fold. On the one hand, it argues that the temporal
    patterns are diversified and thus parameter-sharing for each node is inferior;
    on the other hand, it proposes that the pre-defined graph may be intuitive and
    incomplete for the specific prediction task. To mitigate the two issues, it designs
    a Node Adaptive Parameter Learning (NAPL) module to learn node-specific patterns
    for each traffic series, and a Data Adaptive Graph Generation (DAGG) module to
    infer the hidden correlations among nodes from data and to generate the graph
    during training. Specifically, the NAPL module is defined as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '| (168) |  | $\displaystyle Z=(I_{n}+D^{-\frac{1}{2}}AD^{-\frac{1}{2}})XE_{\mathcal{G}}W_{\mathcal{G}}+E_{\mathcal{G}}b_{\mathcal{G}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $X\in\mathbb{R}^{N\times C}$ is the input feature, $E_{\mathcal{G}}\in\mathbb{R}^{N\times
    d}$ is a node embedding dictionary, $d$ is the embedding dimension ($d<<N$), $W_{\mathcal{G}}\in\mathbb{R}^{d\times
    C\times F}$ is a weight pool. The original parameter $\Theta$ in the graph convolution
    is replaced by the matrix production of $E_{\mathcal{G}}W_{\mathcal{G}}$, and
    the same operation is applied for the bias. This can help the model to capture
    node specific patterns from a pattern pool according the node embedding. The DAGG
    module has been introduced in [160](#S14.E160 "In 14.2.2\. Graph Construction.
    ‣ 14.2\. Traffic Graph Construction ‣ 14\. Traffic Analysis By Zheng Fang ‣ A
    Comprehensive Survey on Deep Graph Representation Learning"). The whole workflow
    of AGCRN is formulated as following,
  prefs: []
  type: TYPE_NORMAL
- en: '| (169) |  | $\displaystyle\tilde{A}$ | $\displaystyle=softmax(ReLU(EE^{T})),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (170) |  | $\displaystyle z^{(t)}$ | $\displaystyle=\sigma(\tilde{A}[X^{(t)},H^{(t-1)}]EW_{z}+Eb_{z},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (171) |  | $\displaystyle r^{(t)}$ | $\displaystyle=\sigma(\tilde{A}[X^{(t)},H^{(t-1)}]EW_{r}+Eb_{r},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (172) |  | $\displaystyle\hat{H}^{(t)}$ | $\displaystyle=\tanh(\tilde{A}[X,r^{(t)}\odot
    H^{(t-1)}]EW_{h}+Eb_{h},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (173) |  | $\displaystyle H^{(t)}$ | $\displaystyle=z^{(t)}\odot H^{(t-1)}+(1-z^{(t)})\odot\hat{H}^{(t)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 14.3.4\. Attention Based Spatial-Temporal Graph Convolutional Networks (ASTGCN)
    (Guo et al., [2019](#bib.bib126)).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ASTGCN introduces two kinds of attention mechanisms into spatial temporal forecasting,
    i.e., spatial attention and temporal attention. The spatial attention is defined
    as following,
  prefs: []
  type: TYPE_NORMAL
- en: '| (174) |  | $\displaystyle S=V_{S}\sigma\left((XW_{1})W_{2}(W_{3}X)^{T}+b_{S}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (175) |  | $\displaystyle S_{i,j}^{\prime}=\frac{\exp(S_{i,j})}{\sum_{j=1}^{N}\exp(S_{i,j})},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $S^{\prime}$ is the attention score, and $W_{1},W_{2},W3$ are learnable
    parameters. The similar construction is applied for temporal attention, where
    $X^{T}$ denotes transpose the spatial dimension and the temporal dimension. Besides
    the attention mechanism, ASTGCN also introduces multi-component fusion to enhance
    the prediction ability. The input of ASTGCN consists of three parts, the recent
    segments, the daily-periodic segments and the weekly-periodic segment. The three
    segments are processed by the main model independently and fused with learnable
    weights at last:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (177) |  | $Y=W_{h}\odot Y_{h}+W_{d}\odot Y_{d}+W_{w}\odot Y_{w},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $Y_{h},Y_{d},Y_{w}$ denotes the predictions of different segments respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 15\. Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section introduces graph models for traffic analysis and we provide a
    summary as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques. Traffic analysis is a classical spatial temporal data mining task,
    and graph models play a vital role for extracting spatial correlations. Typical
    procedures include the graph construction, spatial dimension operations, temporal
    dimension operations and the information fusion. There are multiple implementations
    for each procedure, each implementation has its strengths and weaknesses. By combining
    different implementations, various kinds of traffic analysis models can be created.
    Choosing the right combination of procedures and implementations is critical for
    achieving accurate and reliable traffic analysis results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and Limitations. Despite the remarkable success of graph representation
    learning in traffic analysis, there are still several challenges that need to
    be addressed in current studies. Firstly, external data, such as weather and calendar
    information, are not well-utilized in current models, despite their close relation
    to traffic status. The challenge lies in how to effectively fuse heterogeneous
    data to improve traffic analysis accuracy. Secondly, the interpretability of models
    has been underexplored, which could hinder their deployment in real-world transportation
    systems. Interpretable models are crucial for building trust and understanding
    among stakeholders, and more research is needed to develop models that are both
    accurate and interpretable. Addressing these challenges will be critical for advancing
    the state-of-the-art in traffic analysis and ensuring the deployment of effective
    transportation systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future Works. In the future, we anticipate that more data sources will be available
    for traffic analysis, enabling a more comprehensive understanding of real-world
    traffic scenes. From data collection to model design, there is still a lot of
    work to be done to fully leverage the potential of GNNs in traffic analysis. In
    addition, we expect to see more creative applications of GNN-based traffic analysis,
    such as designing traffic light control strategies, which can help to improve
    the efficiency and safety of transportation systems. To achieve these goals, it
    is necessary to continue advancing the development of GNN-based models and exploring
    new ways to fuse diverse data sources. Additionally, there is a need to enhance
    the interpretability of models and ensure their applicability to real-world transportation
    systems. We believe that these efforts will contribute to the continued success
    of traffic analysis and the development of intelligent transportation systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 16\. Future Directions By Zhiping Xiao
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we outline some prospective future directions of deep graph
    representation learning based on the above cornerstone, taxonomy and real-world
    applications. We also outline a few more directions closer to the theoretical
    side.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1\. Application-Inspired Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since deep graph representation is widely used these years, many problems are
    solved while many others arose. While we observe many real-world applications,
    we conclude plenty of challenging problems that are not yet solved. Here in this
    subsection, we outline a few.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.1\. Fairness in Graph Representation Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One common aspect to care about is the fairness concern. Fairness, by definition,
    refers to that the protected features does not infect the outcome. In general,
    a data set’s fairness refers to that the protected features are not influencing
    the data distribution. A model’s fairness, on the other hand, refers to the concern
    that the output of our algorithms should not be affected by some certain protected
    features. The protected features can be race, gender, etc.
  prefs: []
  type: TYPE_NORMAL
- en: To have some fair data, we might measure how frequently in a text corpus a female
    character is associated to leadership, versus how frequently a male is. To design
    a fair model, we require the results of the model keep the same once we make any
    change to the protected features. For instance, if we swap the gender feature
    of some data samples, the predicted outcomes remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the fairness challenge in many other fields of machine learning (Chouldechova
    and Roth, [2018](#bib.bib59); Mehrabi et al., [2021](#bib.bib259)), graph representation
    learning can easily suffer from bias from the data sets which inherit stereotypes
    from the real world, from the architecture of models, or from design decisions
    aiming at a higher performance on a particular task. As the graph representation
    becomes increasingly popular in recent years, the researchers are getting fairness
    into their sights (Ma et al., [2021](#bib.bib249); Dong et al., [2021](#bib.bib76)).
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.2\. Robustness in Graph Representation Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Real world data is always noisy, containing many different kinds of disruptions,
    and not end up being perfect normal distribution. In the worst case, some noise
    can potentially avoid a model from learning the correct knowledge. Better robustness
    refers to the model has better chance of reaching a relatively good and stable
    outcome while input being changed. If a model is not robust enough, the performance
    can not be relied on. Therefore, robustness is another important yet challenging
    consideration in deep graph representation learning.
  prefs: []
  type: TYPE_NORMAL
- en: Again, similar to many other machine learning approaches that aims at solving
    real-world problems (Carlini and Wagner, [2017](#bib.bib40)), improving the robustness
    of deep graph representation models is a nontrivial direction. Either enhancing
    the models’ robustness, or conducting adversarial attacks to challenge the robustness
    of graph representations, are promising direction to go for (Tang et al., [2020b](#bib.bib346);
    Geisler et al., [2021](#bib.bib115); Günnemann, [2022](#bib.bib125)).
  prefs: []
  type: TYPE_NORMAL
- en: Conducting adversarial attack on models are centered around manipulating the
    data input. Enhancing a model’s robustness usually works on introducing new tricks
    or new frameworks to the model, or even change the model’s architecture or other
    design details.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.3\. Adversarial Reprogramming
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the emerge of pre-trained graph neural network models (Hu et al., [2019](#bib.bib146),
    [2020b](#bib.bib147); Qiu et al., [2020a](#bib.bib287)), introducing adversarial
    reprogramming (Elsayed et al., [2018](#bib.bib85); Zheng et al., [2021](#bib.bib460))
    into deep graph representation learning becomes another possibility as well. The
    major difference between adversarial reprogramming and adversarial attack lies
    in whether or not there is a particular target after putting some adversarial
    samples against the model. Adversarial attack requires some small modification
    on the input data samples. An adversarial attack is considered successful once
    the result is influenced. However, under the adversarial reprogramming settings,
    the task succeed if and only if the influenced results can be used for another
    desired task.
  prefs: []
  type: TYPE_NORMAL
- en: This is to say, without changing the model’s inner structure or fine-tuning
    its parameters, we might be able to use some pre-trained graph models for some
    other tasks that were not planned to be solved by these models in the first place.
    In other deep learning fields, adversarial reprogramming problems are normally
    solved by having the input carefully encoded, and output cleverly mapped. On some
    graph data sets, such as chemical data sets and biology data sets, pretrained
    models are already available. Therefore, there is a possibility that adversarial
    reprogramming could be applied in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.4\. Generalizing to Out of Distribution Data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to perform better on unobserved data set, in the ideal case, the representation
    we learn should better be able to generalize to some out-of-distribution (OOD)
    data. Being out-of-distribution is not identical to being mis-classified. The
    mis-classified samples are coming from the same distribution of the training data
    but the model fails to classify it correctly, while out-of-distribution refer
    to the case where the sample comes from a distribution other than the training
    data (Hendrycks and Gimpel, [2016](#bib.bib139)). Being able to generalize to
    out-of-distribution data will greatly enhance a model’s reliability in real life.
    And studying out-of-distribution generalized graph representation (Li et al.,
    [2022b](#bib.bib207)) is an opening field (Li et al., [2022c](#bib.bib208)). This
    is partly because of, currently, even the problem of detecting out-of-distribution
    data samples is not fully conquered yet (Hendrycks and Gimpel, [2016](#bib.bib139)).
  prefs: []
  type: TYPE_NORMAL
- en: In order to do something on the out-of-distribution data samples, we need to
    detect which samples belong to this type first. Detecting OOD samples itself is
    somewhat similar novelty detection, or outlier detection problems (Pimentel et al.,
    [2014](#bib.bib279)). Their major difference is whether or not a well-performed
    model conducting the original tasks remains part of our goal. Novelty detection
    cares only about figuring out who are the outliers; OOD detection requires our
    model to detect the outliers while keep the performance remain unharmed at the
    same time.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.5\. Interpretability in Graph Representation Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Interpretability concern is another limitation that exists when researchers
    try to apply deep graph representation learning onto some of the emerging application
    fields. For instance, in the field of computational social science, researchers
    are urging for more efforts in integrating explanation and prediction together (Hofman
    et al., [2021](#bib.bib142)). So as drug discovery, being able to explain why
    such structure is chosen instead of another option, is very important (Jiménez-Luna
    et al., [2020](#bib.bib164)). Generally speaking, neural networks are completely
    in black-box mode to human knowledge without taking efforts to make them interpretable
    and explainable. Although more and more tasks are being handled by deep learning
    methods in many fields, the tool remains mysterious to most human beings. Even
    an expert of deep learning can not easily explain to you how the tasks are performed
    and what the model has learned from the data. This situation reduces the trustworthiness
    of the neural network models, prevent human from learning more from the models’
    results, and even limit the potential improvements of the models themselves, without
    sufficient feedback to human beings.
  prefs: []
  type: TYPE_NORMAL
- en: Seeking for better interpretability is not only some personal interests of companies
    and researchers, in fact, as more and more ethics concern arose since more and
    more black-box decisions were made by AI algorithms, interpretability has become
    a legal requirement (Goodman and Flaxman, [2017](#bib.bib119)).
  prefs: []
  type: TYPE_NORMAL
- en: Various approaches have been applied, serving for the goal of better interpretability (Zhang
    et al., [2021b](#bib.bib444)). There we find existing works that provide either
    ad-hoc explanation after the results come out, or those actively change the model
    structure to provide better explanations; explanation by providing similar examples,
    by highlighting some attributes to the input features, by making sense of some
    hidden layers and extract semantics from them, or by extracting logical rules;
    we also see local explanations that explain some particular samples, global explanations
    that explain the network as a whole, or hybrid. Most of those existing directions
    make sense in a graph representation learning setting.
  prefs: []
  type: TYPE_NORMAL
- en: Not a consensus has been reached on what are the best methods of making a model
    interpretable. Researchers are still actively exploring every possibility, and
    thus there are plenty of challenges and interesting topics in this direction.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.6\. Causality in Graph Representation Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In recent years, there are increasing research works focusing on combining causality
    and machine learning models (Madumal et al., [2020](#bib.bib253); Hu and Li, [2021](#bib.bib149);
    Richens et al., [2020](#bib.bib297)). It is widely believed that making good use
    of causality will help models gain higher performances. However, finding the right
    way to model causality in many real world scenarios remain super challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 'Something to note is that, the most common kind of graph that comes along the
    causal study, called “causal graph”, is not identical to the kind of graphs we
    are studying in deep graph representation learning. Causal graphs are the kind
    of graph whose nodes are factors and links represents causal relations. Up till
    now, they are among the most reliable tools for causal inference study. Traditionally,
    causal graphs are defined by human experts. Recent works has shown that neural
    networks can help with scalable causal graph generation (Xu et al., [2019a](#bib.bib402)).
    From this perspective, the story can be other-side around: beside using causal
    relations to enhance graph representation learning, it is also possible to use
    graph representation learning strategies to help with causal study.'
  prefs: []
  type: TYPE_NORMAL
- en: 16.1.7\. Emerging Application Fields
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Beside the above-mentioned directions solving existing challenges in the deep
    learning world, there are many emerging fields of application that naturally come
    along with the graph-structured data.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, social network analysis and drug discovery. Due to the nature
    of the data, social network interactions and drug molecule structures can be easily
    depicted as graph-structured data. Therefore, deep graph representation learning
    has much to do in these fields (Abbas, [2021](#bib.bib2); Zhu, [2022](#bib.bib474);
    Gaudelet et al., [2021](#bib.bib110)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some basic problems on social network are easily solved using graph representation
    learning strategies. Those basic problems include node-classification, link-prediction,
    graph-classification, and so on. In practice, those problem settings could refer
    to real-world problems such as: ideology prediction, interaction prediction, analyzing
    a social group, etc. However, social network data typically has many unique features
    that could potentially stop the general-purposed models to perform well. For instance,
    social media data can be sparse, incomplete, and can be extremely imbalanced (Zhao
    et al., [2021c](#bib.bib455)). On the other hand, people have clear goals when
    studying social media data, such as controversy detection (Benslimane et al.,
    [2022](#bib.bib20)), rumor detection (Takahashi and Igata, [2012](#bib.bib342);
    Hamidian and Diab, [2019](#bib.bib128)), mis-information and dis-information detection (Di Domenico
    et al., [2021](#bib.bib72)), or studying the dynamics of the system (Kipf et al.,
    [2018](#bib.bib182)). There are still a lot of open quests to be conquered, where
    deep graph representation learning can help with.'
  prefs: []
  type: TYPE_NORMAL
- en: As for drug-discovery, researchers are having some interests in other perspective
    beside simply proposing a set of potentially functional structures, which is widely
    seen today. The other perspectives include having more interpretable results from
    the model’s proposals (Preuer et al., [2019](#bib.bib281); Jiménez-Luna et al.,
    [2020](#bib.bib164)), and to consider synthetic accessibility (Xie et al., [2021](#bib.bib397)).
    These directions are important, in answer to some doubt on AI from the society (Goodman
    and Flaxman, [2017](#bib.bib119)), as well as from the tradition of chemistry
    studies (Schneider et al., [2020](#bib.bib309)). Similar to the challenges we
    faced when combining social science and neural networks, chemical science would
    also prefer the black-box AI models to be interpretable instead. Some chemical
    scientists would also prefer AI tools to provide them with synthetic route instead
    of the targeting structure itself. In practice, proposing new molecule structures
    is usually not the bottleneck, but synthesising is. There are already some existing
    works focusing on conquering this problem (Empel and Koenigs, [2019](#bib.bib86);
    Ishida et al., [2022](#bib.bib157)). But so far there is a gap between chemical
    experiments and AI tools, indicating that there is still plenty of improvement
    to be made.
  prefs: []
  type: TYPE_NORMAL
- en: Some chemistry researchers also found it useful to have material data better
    organized, given that the molecule structures are becoming increasingly complex,
    and massive amount of research papers are describing the material’s features from
    different aspects (Walsh et al., [2023](#bib.bib356)). This direction might be
    more closely related to knowledge base or even database systems. But in a way,
    given that the polymer structure is typically a node-link graph, graph representation
    learning might be able to help with deadling with such issues.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2\. Theory-Driven Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some other future directions dig into the root of graph theory. More specifically,
    focusing on some fundamental improvement on neural network structure design, or
    better ways of expressing the graph representations. These directions require
    background knowledge of their mathematical backgrounds. All in all, breakthroughs
    in these directions might not end up in immediate impact, but every study in these
    directions has the potential to change the entire field, sooner or later.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2.1\. Mathematical Proof of Feasibility
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It has been a long-lasting problem that most of the existing deep learning approaches
    lack mathematical proof of their learnability, bound, etc (Bouzerdoum and Pattison,
    [1993](#bib.bib27); Bartlett et al., [2017](#bib.bib17)). This problem relates
    to the difficulty of providing theoretical proof on a complicated structure like
    neural network (Grohs and Voigtlaender, [2021](#bib.bib122)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, most of the theoretical proof aims at figuring out theoretical bounds (Harvey
    et al., [2017](#bib.bib133); Bartlett et al., [2019](#bib.bib18); Karpinski and
    Macintyre, [1997](#bib.bib177)). There are multiple types of bounds with different
    problem settings. Such as: given a known architecture of the model, with input
    data satisfying particular normal distribution, prove that training will converge,
    and provide the estimated number of iterations. Most of these architectures being
    studied are simple, such as those made of multi-layer perceptron (MLP), or simply
    studying the updates of parameters in a single fully-connected layer.'
  prefs: []
  type: TYPE_NORMAL
- en: In the field of deep graph representation learning, the neural network architectures
    are typically much more complex than MLPs. Graph neural networks (GNNs), since
    the very beginning (Defferrard et al., [2016](#bib.bib67); Kipf and Welling, [2016a](#bib.bib183)),
    involve a lot of approximation and simplification of mathematical theorems. Nowadays,
    most researchers rely heavily on the experimental results. No matter how wild
    an idea is, as long as it finally works out in an experiment, say, being able
    to converge and the results are acceptable, the design is acceptable. All these
    practice makes the entire field somewhat experiments-oriented or experience-oriented,
    while there remains a huge gap between the theoretical proof and the frontier
    of deep graph representation.
  prefs: []
  type: TYPE_NORMAL
- en: It will be more than beneficial to the whole field if some researchers can push
    forward these theoretical foundations. However, these problems are incredibly
    challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2.2\. Combining Spectral Graph Theory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Down to the theory foundations, the idea of graph neural networks (Shuman et al.,
    [2013b](#bib.bib322); Defferrard et al., [2016](#bib.bib67); Kipf and Welling,
    [2016a](#bib.bib183)) initially comes from spectral graph theory (Chung, [1997](#bib.bib61)).
    In recent years, many researchers are investigating possible improvement on graph
    representation learning strategies via utilizing spectral graph theory (Chen et al.,
    [2020b](#bib.bib55); Yang et al., [2021](#bib.bib408); MansourLakouraj et al.,
    [2022](#bib.bib257); He et al., [2022](#bib.bib136)). For example, graph Laplacian
    is closely related to many properties, such as the connectivity of a graph. By
    studying the properties of Laplacian, it is possible to provide proof on graph
    neural network models’ properties, and to propose better models with desired advantages,
    such as robustness (Fu et al., [2022](#bib.bib101); Runwal et al., [2022](#bib.bib301)).
  prefs: []
  type: TYPE_NORMAL
- en: Spectral graph theory provides a lot of useful insights into graph representation
    learning from a new perspective. There are a lot to be done in this direction.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2.3\. From Graph to Manifolds
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many researchers are devoted to the direction of learning graph representation
    in non-Euclidean spaces (Asif et al., [2021](#bib.bib11); Saxena et al., [2020](#bib.bib306)).
    That is to say, to embed and to compute on some other spaces that are not Euclidean,
    such as, hyperbolic and spherical spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical reasoning and experimental results have shown certain advantages
    of working on manifolds instead of standard Euclidean space. It is believed that
    these advantages are brought by their abilities to capture complex correlations
    on the surface manifold (Zhou et al., [2022a](#bib.bib462)). Besides, researchers
    have shown that, by combining standard graph representation learning strategies
    and manifold assumptions, models work better on preserving and acquiring the locality
    and similarity relationships  (Fu and Liu, [2021](#bib.bib102)). Intuitively,
    sometimes two nodes’ embeddings are regarded way too similar in Euclidean space,
    but in non-Euclidean space they are easily distinguishable.
  prefs: []
  type: TYPE_NORMAL
- en: 17\. Conclusion By Wei Ju
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this survey, we present a comprehensive and up-to-date overview of deep
    graph representation learning. We present a novel taxonomy of existing algorithms
    categorized into GNN architectures, learning paradigms, and applications. Technically,
    we first summarize the ways of GNN architectures namely graph convolutions, graph
    kernel neural networks, graph pooling, and graph transformer. Based on the different
    training objectives, we present three types of the most recent advanced learning
    paradigms namely: supervised/semi-supervised learning on graphs, graph self-supervised
    learning, and graph structure learning. Then, we provide several promising applications
    to demonstrate the effectiveness of deep graph representation learning. Last but
    not least, we discuss the future directions in deep graph representation learning
    that have potential opportunities.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The authors are grateful to the anonymous reviewers for critically reading the
    manuscript and for giving important suggestions to improve their paper. This paper
    is partially supported by the National Key Research and Development Program of
    China with Grant No. 2018AAA0101902 and the National Natural Science Foundation
    of China (NSFC Grant No. 62276002).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abbas (2021) Ash Mohammad Abbas. 2021. Social network analysis using deep learning:
    applications and schemes. *Social Network Analysis and Mining* 11, 1 (2021), 1–21.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahmed et al. (2013) Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja
    Josifovski, and Alexander J Smola. 2013. Distributed large-scale natural graph
    factorization. In *Proceedings of the 22nd international conference on World Wide
    Web*. 37–48.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahmed et al. (2021) Imtiaz Ahmed, Travis Galoppo, Xia Hu, and Yu Ding. 2021.
    Graph regularized autoencoder and its application in unsupervised anomaly detection.
    *IEEE Transactions on Pattern Analysis and Machine Intelligence* 44, 8 (2021),
    4110–4124.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Al-Rfou et al. (2019) Rami Al-Rfou, Bryan Perozzi, and Dustin Zelle. 2019.
    Ddgk: Learning graph representations for deep divergence graph kernels. In *The
    World Wide Web Conference*. 37–48.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AlBadani et al. (2022) Barakat AlBadani, Ronghua Shi, Jian Dong, Raeed Al-Sabri,
    and Oloulade Babatounde Moctard. 2022. Transformer-Based Graph Convolutional Network
    for Sentiment Analysis. *Applied Sciences* 12, 3 (2022), 1316.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alon and Yahav (2020) Uri Alon and Eran Yahav. 2020. On the bottleneck of graph
    neural networks and its practical implications. *arXiv preprint arXiv:2006.05205*
    (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Altae-Tran et al. (2017) Han Altae-Tran, Bharath Ramsundar, Aneesh S Pappu,
    and Vijay Pande. 2017. Low data drug discovery with one-shot learning. *ACS central
    science* 3, 4 (2017), 283–293.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anderson et al. (2019) Brandon Anderson, Truong Son Hy, and Risi Kondor. 2019.
    Cormorant: Covariant Molecular Neural Networks. In *NeurIPS*, Vol. 32. [https://proceedings.neurips.cc/paper/2019/file/03573b32b2746e6e8ca98b9123f2249b-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/03573b32b2746e6e8ca98b9123f2249b-Paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017.
    Wasserstein generative adversarial networks. In *International conference on machine
    learning*. PMLR, 214–223.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Asif et al. (2021) Nurul A Asif, Yeahia Sarker, Ripon K Chakrabortty, Michael J
    Ryan, Md Hafiz Ahamed, Dip K Saha, Faisal R Badal, Sajal K Das, Md Firoz Ali,
    Sumaya I Moyeen, et al. 2021. Graph neural network: A comprehensive review on
    non-euclidean space. *IEEE Access* 9 (2021), 60588–60606.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Assouel et al. (2018) Rim Assouel, Mohamed Ahmed, Marwin H Segler, Amir Saffari,
    and Yoshua Bengio. 2018. Defactor: Differentiable edge factorization-based probabilistic
    graph generation. *arXiv preprint arXiv:1811.09766* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bacciu et al. (2020) Davide Bacciu, Federico Errica, Alessio Micheli, and Marco
    Podda. 2020. A gentle introduction to deep learning for graphs. *Neural Networks*
    129 (2020), 203–221.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2020) Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020.
    Adaptive graph convolutional recurrent network for traffic forecasting. *Advances
    in neural information processing systems* 33 (2020), 17804–17815.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2019) Xiaomei Bai, Mengyang Wang, Ivan Lee, Zhuo Yang, Xiangjie
    Kong, and Feng Xia. 2019. Scientific paper recommendation: A survey. *Ieee Access*
    7 (2019), 9324–9339.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balcilar et al. (2021) Muhammet Balcilar, Pierre Héroux, Benoit Gauzere, Pascal
    Vasseur, Sébastien Adam, and Paul Honeine. 2021. Breaking the limits of message
    passing graph neural networks. In *International Conference on Machine Learning*.
    PMLR, 599–608.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bartlett et al. (2017) Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky.
    2017. Spectrally-normalized margin bounds for neural networks. *Advances in neural
    information processing systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bartlett et al. (2019) Peter L Bartlett, Nick Harvey, Christopher Liaw, and
    Abbas Mehrabian. 2019. Nearly-tight VC-dimension and pseudodimension bounds for
    piecewise linear neural networks. *The Journal of Machine Learning Research* 20,
    1 (2019), 2285–2301.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batzner et al. (2021) Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger,
    Jonathan P. Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E. Smidt, and Boris
    Kozinsky. 2021. E(3)-Equivariant Graph Neural Networks for Data-Efficient and
    Accurate Interatomic Potentials. arXiv:2101.03164 [physics.comp-ph]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benslimane et al. (2022) Samy Benslimane, Jérôme Azé, Sandra Bringay, Maximilien
    Servajean, and Caroline Mollevi. 2022. A text and GNN based controversy detection
    method on social media. *World Wide Web* (2022), 1–27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berg et al. (2017) Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017.
    Graph convolutional matrix completion. *arXiv preprint arXiv:1706.02263* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bian et al. (2020) Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang,
    Yu Rong, and Junzhou Huang. 2020. Rumor detection on social media with bi-directional
    graph convolutional networks. In *Proceedings of the AAAI conference on artificial
    intelligence*, Vol. 34. 549–556.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bianchi et al. (2020) Filippo Maria Bianchi, Daniele Grattarola, and Cesare
    Alippi. 2020. Spectral clustering with graph neural networks for graph pooling.
    In *International Conference on Machine Learning*. PMLR, 874–883.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bo et al. (2021) Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. 2021. Beyond
    low-frequency information in graph convolutional networks. In *Proceedings of
    the AAAI Conference on Artificial Intelligence*, Vol. 35. 3950–3957.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Borgwardt and Kriegel (2005) Karsten M Borgwardt and Hans-Peter Kriegel. 2005.
    Shortest-path kernels on graphs. In *Fifth IEEE international conference on data
    mining (ICDM)*. IEEE, 8–pp.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bouritsas et al. (2022) Giorgos Bouritsas, Fabrizio Frasca, Stefanos P Zafeiriou,
    and Michael Bronstein. 2022. Improving graph neural network expressivity via subgraph
    isomorphism counting. *IEEE Transactions on Pattern Analysis and Machine Intelligence*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bouzerdoum and Pattison (1993) Abdesselam Bouzerdoum and Tim R Pattison. 1993.
    Neural network for quadratic optimization with bound constraints. *IEEE transactions
    on neural networks* 4, 2 (1993), 293–304.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Box and Pierce (1970) George EP Box and David A Pierce. 1970. Distribution of
    residual autocorrelations in autoregressive-integrated moving average time series
    models. *Journal of the American statistical Association* 65, 332 (1970), 1509–1526.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brandstetter et al. (2022) Johannes Brandstetter, Rob Hesselink, Elise van der
    Pol, Erik J Bekkers, and Max Welling. 2022. Geometric and Physical Quantities
    improve E(3) Equivariant Message Passing. In *ICLR*. [https://openreview.net/forum?id=_xwr8gOBeV1](https://openreview.net/forum?id=_xwr8gOBeV1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bruna et al. (2013) Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun.
    2013. Spectral networks and locally connected networks on graphs. *arXiv preprint
    arXiv:1312.6203* (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bui et al. (2021) Khac-Hoai Nam Bui, Jiho Cho, and Hongsuk Yi. 2021. Spatial-temporal
    graph neural network for traffic forecasting: An overview and open research issues.
    *Applied Intelligence* (2021), 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai and Lam (2020) Deng Cai and Wai Lam. 2020. Graph transformer for graph-to-sequence
    learning. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 34\. 7464–7471.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2018) Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang.
    2018. A comprehensive survey of graph embedding: Problems, techniques, and applications.
    *IEEE Transactions on Knowledge and Data Engineering* 30, 9 (2018), 1616–1637.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Camacho et al. (2020) David Camacho, Ángel Panizo-LLedot, Gema Bello-Orgaz,
    Antonio Gonzalez-Pardo, and Erik Cambria. 2020. The four dimensions of social
    network analysis: An overview of research methods, applications, and software
    tools. *Information Fusion* 63 (2020), 88–120.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2021a) Defu Cao, Jiachen Li, Hengbo Ma, and Masayoshi Tomizuka.
    2021a. Spectral temporal graph neural network for trajectory prediction. In *2021
    IEEE International Conference on Robotics and Automation (ICRA)*. IEEE, 1839–1845.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2021b) Jinzhou Cao, Qingquan Li, Wei Tu, Qili Gao, Rui Cao, and
    Chen Zhong. 2021b. Resolving urban mobility networks from individual travel graphs
    using massive-scale mobile phone tracking data. *Cities* 110 (2021), 103077.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2015) Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. Grarep: Learning
    graph representations with global structural information. In *Proceedings of the
    24th ACM international on conference on information and knowledge management*.
    891–900.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2016) Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep neural
    networks for learning graph representations. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carion et al. (2020) Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
    Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection
    with transformers. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
    UK, August 23–28, 2020, Proceedings, Part I 16*. Springer, 213–229.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini and Wagner (2017) Nicholas Carlini and David Wagner. 2017. Towards evaluating
    the robustness of neural networks. In *2017 ieee symposium on security and privacy
    (sp)*. Ieee, 39–57.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chami et al. (2022) Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher
    Ré, and Kevin Murphy. 2022. Machine learning on graphs: A model and comprehensive
    taxonomy. *Journal of Machine Learning Research* 23, 89 (2022), 1–64.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chauhan et al. (2020) Jatin Chauhan, Deepak Nathani, and Manohar Kaul. 2020.
    Few-shot learning on graphs via super-classes based on graph spectral measures.
    *arXiv preprint arXiv:2002.12815* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020h) Bo Chen, Jing Zhang, Jie Tang, Lingfan Cai, Zhaoyu Wang,
    Shu Zhao, Hong Chen, and Cuiping Li. 2020h. Conna: Addressing name disambiguation
    on the fly. *IEEE Transactions on Knowledge and Data Engineering* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022b) Chaoqi Chen, Yushuang Wu, Qiyuan Dai, Hong-Yu Zhou, Mutian
    Xu, Sibei Yang, Xiaoguang Han, and Yizhou Yu. 2022b. A Survey on Graph Neural
    Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective.
    *arXiv preprint arXiv:2209.13232* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020c) Dexiong Chen, Laurent Jacob, and Julien Mairal. 2020c. Convolutional
    Kernel Networks for Graph-Structured Data. *arXiv preprint arXiv:2003.05189* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020d) Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu
    Sun. 2020d. Measuring and relieving the over-smoothing problem for graph neural
    networks from the topological view. In *Proceedings of the AAAI conference on
    artificial intelligence*, Vol. 34\. 3438–3445.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022a) Dexiong Chen, Leslie O’Bray, and Karsten Borgwardt. 2022a.
    Structure-aware transformer for graph representation learning. In *International
    Conference on Machine Learning*. PMLR, 3469–3489.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020f) Fenxiao Chen, Yun-Cheng Wang, Bin Wang, and C-C Jay Kuo.
    2020f. Graph representation learning: a survey. *APSIPA Transactions on Signal
    and Information Processing* 9 (2020), e15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020e) Guimin Chen, Yuanhe Tian, and Yan Song. 2020e. Joint aspect
    extraction and sentiment analysis with directional graph convolutional networks.
    In *Proceedings of the 28th international conference on computational linguistics*.
    272–279.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021b) Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan
    Laptev. 2021b. History aware multimodal transformer for vision-and-language navigation.
    *Advances in Neural Information Processing Systems* 34 (2021), 5834–5847.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019) Ting Chen, Song Bian, and Yizhou Sun. 2019. Are powerful
    graph neural nets necessary? a dissection on graph classification. *arXiv preprint
    arXiv:1905.04579* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021c) Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang.
    2021c. Semi-supervised semantic segmentation with cross pseudo supervision. In
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    2613–2622.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020g) Yu Chen, Lingfei Wu, and Mohammed Zaki. 2020g. Iterative
    deep graph learning for graph neural networks: Better and robust node embeddings.
    *Advances in neural information processing systems* 33 (2020), 19314–19326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021a) Zekai Chen, Dingshuo Chen, Xiao Zhang, Zixuan Yuan, and
    Xiuzhen Cheng. 2021a. Learning graph structures with transformer for multivariate
    time series anomaly detection in iot. *IEEE Internet of Things Journal* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020b) Zhiqian Chen, Fanglan Chen, Lei Zhang, Taoran Ji, Kaiqun
    Fu, Liang Zhao, Feng Chen, Lingfei Wu, Charu Aggarwal, and Chang-Tien Lu. 2020b.
    Bridging the gap between spatial and spectral domains: A survey on graph neural
    networks. *arXiv preprint arXiv:2002.11867* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020a) Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna.
    2020a. Can graph neural networks count substructures? *Advances in neural information
    processing systems* 33 (2020), 10383–10395.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho and Yu (2018) Haeran Cho and Yi Yu. 2018. Link prediction for interdisciplinary
    collaboration via co-authorship network. *Social Network Analysis and Mining*
    8, 1 (2018), 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choi et al. (2022) Jeongwhan Choi, Hwangyong Choi, Jeehyun Hwang, and Noseong
    Park. 2022. Graph neural controlled differential equations for traffic forecasting.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 36. 6367–6374.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chouldechova and Roth (2018) Alexandra Chouldechova and Aaron Roth. 2018. The
    frontiers of fairness in machine learning. *arXiv preprint arXiv:1810.08810* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chuan et al. (2018) Pham Minh Chuan, Le Hoang Son, Mumtaz Ali, Tran Dinh Khang,
    Le Thanh Huong, and Nilanjan Dey. 2018. Link prediction in co-authorship networks
    based on hybrid content similarity metric. *Applied Intelligence* 48, 8 (2018),
    2470–2486.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung (1997) Fan RK Chung. 1997. *Spectral graph theory*. Vol. 92. American
    Mathematical Soc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2014) Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua
    Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence
    modeling. *arXiv preprint arXiv:1412.3555* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coley et al. (2017) Connor W Coley, Regina Barzilay, William H Green, Tommi S
    Jaakkola, and Klavs F Jensen. 2017. Convolutional embedding of attributed molecular
    graphs for physical property prediction. *Journal of chemical information and
    modeling* 57, 8 (2017), 1757–1772.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connor et al. (1994) Jerome T Connor, R Douglas Martin, and Les E Atlas. 1994.
    Recurrent neural networks and robust time series prediction. *IEEE transactions
    on neural networks* 5, 2 (1994), 240–254.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Corso et al. (2020) Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro
    Liò, and Petar Veličković. 2020. Principal neighbourhood aggregation for graph
    nets. *Advances in Neural Information Processing Systems* 33 (2020), 13260–13271.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De Cao and Kipf (2018) Nicola De Cao and Thomas Kipf. 2018. MolGAN: An implicit
    generative model for small molecular graphs. *arXiv preprint arXiv:1805.11973*
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defferrard et al. (2016) Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst.
    2016. Convolutional neural networks on graphs with fast localized spectral filtering.
    *Advances in neural information processing systems* 29 (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Delaney (2004) John S Delaney. 2004. ESOL: estimating aqueous solubility directly
    from molecular structure. *Journal of chemical information and computer sciences*
    44, 3 (2004), 1000–1005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng and Hooi (2021) Ailin Deng and Bryan Hooi. 2021. Graph neural network-based
    anomaly detection in multivariate time series. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, Vol. 35. 4027–4035.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2022) Leyan Deng, Defu Lian, Zhenya Huang, and Enhong Chen. 2022.
    Graph convolutional adversarial networks for spatiotemporal anomaly detection.
    *IEEE Transactions on Neural Networks and Learning Systems* 33, 6 (2022), 2416–2428.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Di Domenico et al. (2021) Giandomenico Di Domenico, Jason Sit, Alessio Ishizaka,
    and Daniel Nunan. 2021. Fake news, social media and marketing: A systematic review.
    *Journal of Business Research* 124 (2021), 329–341.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimitrakopoulos and Demestichas (2010) George Dimitrakopoulos and Panagiotis
    Demestichas. 2010. Intelligent transportation systems. *IEEE Vehicular Technology
    Magazine* 5, 1 (2010), 77–84.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domingos and Richardson (2001) Pedro Domingos and Matt Richardson. 2001. Mining
    the network value of customers. In *Proceedings of the seventh ACM SIGKDD international
    conference on Knowledge discovery and data mining*. 57–66.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2017) Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017.
    metapath2vec: Scalable representation learning for heterogeneous networks. In
    *Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery
    and data mining*. 135–144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2021) Yushun Dong, Jian Kang, Hanghang Tong, and Jundong Li. 2021.
    Individual fairness for graph neural networks: A ranking based approach. In *Proceedings
    of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining*. 300–310.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2021) Jinlong Du, Senzhang Wang, Hao Miao, and Jiaqiang Zhang. 2021.
    Multi-Channel Pooling Graph Neural Networks. In *Proceedings of the Thirtieth
    International Joint Conference on Artificial Intelligence, IJCAI-21*, Zhi-Hua
    Zhou (Ed.). International Joint Conferences on Artificial Intelligence Organization,
    1442–1448. [https://doi.org/10.24963/ijcai.2021/199](https://doi.org/10.24963/ijcai.2021/199)
    Main Track.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2019) Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas
    Poczos, Ruosong Wang, and Keyulu Xu. 2019. Graph neural tangent kernel: Fusing
    graph neural networks with graph kernels. In *Advances in Neural Information Processing
    Systems*. 5723–5733.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2022a) Yuanqi Du, Tianfan Fu, Jimeng Sun, and Shengchao Liu. 2022a.
    MolGenSurvey: A Systematic Survey in Machine Learning Models for Molecule Design.
    *arXiv preprint arXiv:2203.14500* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2022b) Yuanqi Du, Xiaojie Guo, Amarda Shehu, and Liang Zhao. 2022b.
    Interpretable molecular graph generation via monotonic constraints. In *Proceedings
    of the 2022 SIAM International Conference on Data Mining (SDM)*. SIAM, 73–81.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duvenaud et al. (2015) David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre,
    Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional
    networks on graphs for learning molecular fingerprints. *Advances in neural information
    processing systems* 28 (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dwivedi and Bresson (2020) Vijay Prakash Dwivedi and Xavier Bresson. 2020. A
    generalization of transformer networks to graphs. *arXiv preprint arXiv:2012.09699*
    (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eberhardt et al. (2021) Jerome Eberhardt, Diogo Santos-Martins, Andreas F Tillack,
    and Stefano Forli. 2021. AutoDock Vina 1.2\. 0: New docking methods, expanded
    force field, and python bindings. *Journal of Chemical Information and Modeling*
    61, 8 (2021), 3891–3898.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elinas et al. (2020) Pantelis Elinas, Edwin V Bonilla, and Louis Tiao. 2020.
    Variational inference for graph convolutional networks in the absence of graph
    data and adversarial settings. *Advances in Neural Information Processing Systems*
    33 (2020), 18648–18660.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elsayed et al. (2018) Gamaleldin F Elsayed, Ian Goodfellow, and Jascha Sohl-Dickstein.
    2018. Adversarial reprogramming of neural networks. *arXiv preprint arXiv:1806.11146*
    (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Empel and Koenigs (2019) Claire Empel and Rene M Koenigs. 2019. Artificial-Intelligence-Driven
    Organic Synthesis—En Route towards Autonomous Synthesis? *Angewandte Chemie International
    Edition* 58, 48 (2019), 17114–17116.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Errica et al. (2019) Federico Errica, Marco Podda, Davide Bacciu, and Alessio
    Micheli. 2019. A fair comparison of graph neural networks for graph classification.
    *arXiv preprint arXiv:1912.09893* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2021) Ziwei Fan, Zhiwei Liu, Jiawei Zhang, Yun Xiong, Lei Zheng,
    and Philip S Yu. 2021. Continuous-time sequential recommendation with temporal
    graph collaborative transformer. In *Proceedings of the 30th ACM International
    Conference on Information & Knowledge Management*. 433–442.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2022) Yin Fang, Qiang Zhang, Haihong Yang, Xiang Zhuang, Shumin
    Deng, Wen Zhang, Ming Qin, Zhuo Chen, Xiaohui Fan, and Huajun Chen. 2022. Molecular
    contrastive learning with chemical element knowledge graph. In *Proceedings of
    the AAAI Conference on Artificial Intelligence*, Vol. 36. 3968–3976.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2021) Zheng Fang, Qingqing Long, Guojie Song, and Kunqing Xie.
    2021. Spatial-temporal graph ode networks for traffic flow forecasting. In *Proceedings
    of the 27th ACM SIGKDD conference on knowledge discovery & data mining*. 364–373.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feinberg et al. (2018) Evan N Feinberg, Debnil Sur, Zhenqin Wu, Brooke E Husic,
    Huanghao Mai, Yang Li, Saisai Sun, Jianyi Yang, Bharath Ramsundar, and Vijay S
    Pande. 2018. PotentialNet for molecular property prediction. *ACS central science*
    4, 11 (2018), 1520–1530.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2021) Shangbin Feng, Herun Wan, Ningnan Wang, and Minnan Luo.
    2021. BotRGCN: Twitter bot detection with relational graph convolutional networks.
    In *Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social
    Networks Analysis and Mining*. 236–239.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2020) Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan,
    Qian Xu, Qiang Yang, Evgeny Kharlamov, and Jie Tang. 2020. Graph random neural
    networks for semi-supervised learning on graphs. (2020), 22092–22103.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2019) Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue
    Gao. 2019. Hypergraph neural networks. In *Proceedings of the AAAI conference
    on artificial intelligence*, Vol. 33\. 3558–3565.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finn et al. (2017) Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic
    meta-learning for fast adaptation of deep networks. In *International conference
    on machine learning*. PMLR, 1126–1135.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finzi et al. (2020) Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon
    Wilson. 2020. Generalizing convolutional neural networks for equivariance to lie
    groups on arbitrary continuous data. In *ICML*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fischer (1894) Emil Fischer. 1894. Einfluss der Configuration auf die Wirkung
    der Enzyme. *Berichte der deutschen chemischen Gesellschaft* 27, 3 (1894), 2985–2993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Franceschi et al. (2019) Luca Franceschi, Mathias Niepert, Massimiliano Pontil,
    and Xiao He. 2019. Learning discrete structures for graph neural networks. In
    *International conference on machine learning*. PMLR, 1972–1982.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Francoeur et al. (2020) Paul G Francoeur, Tomohide Masuda, Jocelyn Sunseri,
    Andrew Jia, Richard B Iovanisci, Ian Snyder, and David R Koes. 2020. Three-dimensional
    convolutional neural networks and a cross-docked data set for structure-based
    drug design. *Journal of Chemical Information and Modeling* 60, 9 (2020), 4200–4215.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frisch et al. (2016) MJ ea Frisch, GW Trucks, HB Schlegel, GE Scuseria, MA Robb,
    JR Cheeseman, G Scalmani, VPGA Barone, GA Petersson, HJRA Nakatsuji, et al. 2016.
    Gaussian 16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2022) Guoji Fu, Peilin Zhao, and Yatao Bian. 2022. $p$-Laplacian
    Based Graph Neural Networks. In *International Conference on Machine Learning*.
    PMLR, 6878–6917.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu and Liu (2021) Sichao Fu and Weifeng Liu. 2021. Recent Advances of Manifold-based
    Graph Convolutional Networks for Remote Sensing Images Recognition. *Generalization
    With Deep Learning: For Improvement On Sensing Capability* (2021), 209–232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fuchs et al. (2020) Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling.
    2020. SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks.
    In *NeurIPS*, Vol. 33. [https://proceedings.neurips.cc/paper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ganea et al. (2021) Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao
    Bian, Regina Barzilay, Tommi Jaakkola, and Andreas Krause. 2021. Independent se
    (3)-equivariant models for end-to-end rigid protein docking. *arXiv preprint arXiv:2111.07786*
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao and Ji (2019) Hongyang Gao and Shuiwang Ji. 2019. Graph u-nets. In *international
    conference on machine learning*. PMLR, 2083–2092.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2021) Hongyang Gao, Yi Liu, and Shuiwang Ji. 2021. Topology-aware
    graph pooling networks. *IEEE Transactions on Pattern Analysis and Machine Intelligence*
    43, 12 (2021), 4512–4518.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2020) Xiang Gao, Wei Hu, and Zongming Guo. 2020. Exploring structure-adaptive
    graph learning for robust semi-supervised classification. In *2020 ieee international
    conference on multimedia and expo (icme)*. IEEE, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gärtner et al. (2003) Thomas Gärtner, Peter Flach, and Stefan Wrobel. 2003.
    On graph kernels: Hardness results and efficient alternatives. In *Proceedings
    of Computational Learning theory and kernel machines*. 129–143.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gasteiger et al. (2019) Johannes Gasteiger, Stefan Weißenberger, and Stephan
    Günnemann. 2019. Diffusion improves graph learning. *Advances in neural information
    processing systems* 32 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaudelet et al. (2021) Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman,
    Cristian Regep, Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts,
    Jian Tang, et al. 2021. Utilizing graph machine learning within drug discovery
    and development. *Briefings in bioinformatics* 22, 6 (2021), bbab159.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gayvert et al. (2016) Kaitlyn M Gayvert, Neel S Madhukar, and Olivier Elemento.
    2016. A data-driven approach to predicting successes and failures of clinical
    trials. *Cell chemical biology* 23, 10 (2016), 1294–1301.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gebauer et al. (2019) Niklas Gebauer, Michael Gastegger, and Kristof Schütt.
    2019. Symmetry-adapted generation of 3d point sets for the targeted discovery
    of molecules. *Advances in neural information processing systems* 32 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gebauer et al. (2022) Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann,
    Klaus-Robert Müller, and Kristof T Schütt. 2022. Inverse design of 3d molecular
    structures with conditional generative neural networks. *Nature communications*
    13, 1 (2022), 1–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geiger and Smidt (2022) Mario Geiger and Tess Smidt. 2022. e3nn: Euclidean
    neural networks. *arXiv preprint arXiv:2207.09453* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geisler et al. (2021) Simon Geisler, Tobias Schmidt, Hakan Şirin, Daniel Zügner,
    Aleksandar Bojchevski, and Stephan Günnemann. 2021. Robustness of graph neural
    networks at scale. *Advances in Neural Information Processing Systems* 34 (2021),
    7637–7649.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol
    Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry.
    In *International conference on machine learning*. PMLR, 1263–1272.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gómez-Bombarelli et al. (2018) Rafael Gómez-Bombarelli, Jennifer N Wei, David
    Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla,
    Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik.
    2018. Automatic chemical design using a data-driven continuous representation
    of molecules. *ACS central science* 4, 2 (2018), 268–276.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2020) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.
    Generative adversarial networks. *Commun. ACM* 63, 11 (2020), 139–144.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodman and Flaxman (2017) Bryce Goodman and Seth Flaxman. 2017. European Union
    regulations on algorithmic decision-making and a “right to explanation”. *AI magazine*
    38, 3 (2017), 50–57.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal and Ferrara (2018) Palash Goyal and Emilio Ferrara. 2018. Graph embedding
    techniques, applications, and performance: A survey. *Knowledge-Based Systems*
    151 (2018), 78–94.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grill et al. (2020) Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin
    Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires,
    Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a
    new approach to self-supervised learning. *Advances in neural information processing
    systems* 33 (2020), 21271–21284.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grohs and Voigtlaender (2021) Philipp Grohs and Felix Voigtlaender. 2021. Proof
    of the theory-to-practice gap in deep learning via sampling complexity bounds
    for neural network approximation spaces. *arXiv preprint arXiv:2104.02746* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Groom et al. (2016) Colin R Groom, Ian J Bruno, Matthew P Lightfoot, and Suzanna C
    Ward. 2016. The Cambridge structural database. *Acta Crystallographica Section
    B: Structural Science, Crystal Engineering and Materials* 72, 2 (2016), 171–179.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grover and Leskovec (2016) Aditya Grover and Jure Leskovec. 2016. node2vec:
    Scalable feature learning for networks. In *Proceedings of the 22nd ACM SIGKDD
    international conference on Knowledge discovery and data mining*. 855–864.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Günnemann (2022) Stephan Günnemann. 2022. Graph neural networks: Adversarial
    robustness. In *Graph Neural Networks: Foundations, Frontiers, and Applications*.
    Springer, 149–176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019) Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu
    Wan. 2019. Attention based spatial-temporal graph convolutional networks for traffic
    flow forecasting. In *Proceedings of the AAAI conference on artificial intelligence*,
    Vol. 33\. 922–929.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2021) Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest,
    Meng Jiang, and Nitesh V Chawla. 2021. Few-shot graph learning for molecular property
    prediction. In *Proceedings of the Web Conference 2021*. 2559–2567.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hamidian and Diab (2019) Sardar Hamidian and Mona T Diab. 2019. Rumor detection
    and classification for twitter data. *arXiv preprint arXiv:1912.08926* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hamilton et al. (2017) Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017.
    Inductive representation learning on large graphs. *Advances in neural information
    processing systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hammond et al. (2011) David K Hammond, Pierre Vandergheynst, and Rémi Gribonval.
    2011. Wavelets on graphs via spectral graph theory. *Applied and Computational
    Harmonic Analysis* 30, 2 (2011), 129–150.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2022) Jiaqi Han, Yu Rong, Tingyang Xu, and Wenbing Huang. 2022.
    Geometrically equivariant graph neural networks: A survey. *arXiv preprint arXiv:2202.07230*
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hao et al. (2020) Zhongkai Hao, Chengqiang Lu, Zhenya Huang, Hao Wang, Zheyuan
    Hu, Qi Liu, Enhong Chen, and Cheekong Lee. 2020. ASGN: An active semi-supervised
    graph neural network for molecular property prediction. In *Proceedings of the
    ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*. 731–752.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harvey et al. (2017) Nick Harvey, Christopher Liaw, and Abbas Mehrabian. 2017.
    Nearly-tight VC-dimension bounds for piecewise linear neural networks. In *Conference
    on learning theory*. PMLR, 1064–1068.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2021) Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang,
    Aixi Zhang, and Si Liu. 2021. TransRefer3D: Entity-and-Relation Aware Transformer
    for Fine-Grained 3D Visual Grounding. In *Proceedings of the 29th ACM International
    Conference on Multimedia*. 2344–2352.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2020) Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang,
    and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network
    for recommendation. In *Proceedings of the 43rd International ACM SIGIR conference
    on research and development in Information Retrieval*. 639–648.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2022) Yixuan He, Michael Permultter, Gesine Reinert, and Mihai Cucuringu.
    2022. MSGNN: A Spectral Graph Neural Network Based on a Novel Magnetic Signed
    Laplacian. *arXiv preprint arXiv:2209.00546* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hearst et al. (1998) Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt,
    and Bernhard Scholkopf. 1998. Support vector machines. *IEEE Intelligent Systems
    and their applications* 13, 4 (1998), 18–28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henaff et al. (2015) Mikael Henaff, Joan Bruna, and Yann LeCun. 2015. Deep convolutional
    networks on graph-structured data. *arXiv preprint arXiv:1506.05163* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks and Gimpel (2016) Dan Hendrycks and Kevin Gimpel. 2016. A baseline
    for detecting misclassified and out-of-distribution examples in neural networks.
    *arXiv preprint arXiv:1610.02136* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton and Salakhutdinov (2006) Geoffrey E Hinton and Ruslan R Salakhutdinov.
    2006. Reducing the dimensionality of data with neural networks. *science* 313,
    5786 (2006), 504–507.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising
    diffusion probabilistic models. *Advances in Neural Information Processing Systems*
    33 (2020), 6840–6851.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hofman et al. (2021) Jake M Hofman, Duncan J Watts, Susan Athey, Filiz Garip,
    Thomas L Griffiths, Jon Kleinberg, Helen Margetts, Sendhil Mullainathan, Matthew J
    Salganik, Simine Vazire, et al. 2021. Integrating explanation and prediction in
    computational social science. *Nature* 595, 7866 (2021), 181–188.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoogeboom et al. (2022) Emiel Hoogeboom, Vıctor Garcia Satorras, Clément Vignac,
    and Max Welling. 2022. Equivariant diffusion for molecule generation in 3d. In
    *International Conference on Machine Learning*. PMLR, 8867–8887.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2020d) Fenyu Hu, Yanqiao Zhu, Shu Wu, Weiran Huang, Liang Wang,
    and Tieniu Tan. 2020d. Graphair: Graph representation learning with neighborhood
    aggregation and interaction. *Pattern Recognition* 112 (2020), 107745.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2020c) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu
    Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020c. Open graph benchmark:
    Datasets for machine learning on graphs. *Advances in neural information processing
    systems* 33 (2020), 22118–22133.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2019) Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang,
    Vijay Pande, and Jure Leskovec. 2019. Strategies for pre-training graph neural
    networks. *arXiv preprint arXiv:1905.12265* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2020b) Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou
    Sun. 2020b. Gpt-gnn: Generative pre-training of graph neural networks. In *Proceedings
    of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining*. 1857–1867.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2020a) Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020a.
    Heterogeneous graph transformer. In *Proceedings of the web conference 2020*.
    2704–2710.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu and Li (2021) Zhiting Hu and Li Erran Li. 2021. A causal lens for controllable
    text generation. *Advances in Neural Information Processing Systems* 34 (2021),
    24941–24955.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang and Yang (2021) Jing Huang and Jie Yang. 2021. Unignn: a unified framework
    for graph and hypergraph neural networks. *arXiv preprint arXiv:2105.00956* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2020) Rongzhou Huang, Chuyin Huang, Yubao Liu, Genan Dai, and
    Weiyang Kong. 2020. LSGCN: Long Short-Term Traffic Prediction with Graph Convolutional
    Networks.. In *IJCAI*, Vol. 7\. 2355–2361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2015) Sheng Huang, Mohamed Elhoseiny, Ahmed Elgammal, and Dan
    Yang. 2015. Learning hypergraph-regularized attribute predictors. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. 409–417.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2022) Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun,
    and Junzhou Huang. 2022. Constrained Graph Mechanics Networks. In *ICLR*. [https://openreview.net/forum?id=SHbhHHfePhP](https://openreview.net/forum?id=SHbhHHfePhP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hussain et al. (2021) Md Shamim Hussain, Mohammed J Zaki, and Dharmashankar
    Subramanian. 2021. Edge-augmented graph transformers: Global self-attention is
    enough for graphs. *arXiv preprint arXiv:2108.03348* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hutchinson et al. (2021) Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi,
    Emilien Dupont, Yee Whye Teh, and Hyunjik Kim. 2021. Lietransformer: equivariant
    self-attention for lie groups. In *ICML*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Irwin et al. (2012) John J Irwin, Teague Sterling, Michael M Mysinger, Erin S
    Bolstad, and Ryan G Coleman. 2012. ZINC: a free tool to discover chemistry for
    biology. *Journal of chemical information and modeling* 52, 7 (2012), 1757–1768.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ishida et al. (2022) Shoichi Ishida, Kei Terayama, Ryosuke Kojima, Kiyosei Takasu,
    and Yasushi Okuno. 2022. Ai-driven synthetic route design incorporated with retrosynthesis
    knowledge. *Journal of chemical information and modeling* 62, 6 (2022), 1357–1367.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Islam et al. (2022) Md Ashraful Islam, Mir Mahathir Mohammad, Sarkar Snigdha Sarathi
    Das, and Mohammed Eunus Ali. 2022. A survey on deep learning based Point-of-Interest
    (POI) recommendations. *Neurocomputing* 472 (2022), 306–325.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2020) Shuyi Ji, Yifan Feng, Rongrong Ji, Xibin Zhao, Wanwan Tang,
    and Yue Gao. 2020. Dual channel hypergraph collaborative filtering. In *Proceedings
    of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining*. 2020–2029.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2019) Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, and Bin Luo.
    2019. Semi-supervised learning with graph learning-convolutional networks. In
    *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*.
    11313–11320.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang and Luo (2022) Weiwei Jiang and Jiayun Luo. 2022. Graph neural network
    for traffic forecasting: A survey. *Expert Systems with Applications* (2022),
    117921.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2018) Zhuoren Jiang, Yue Yin, Liangcai Gao, Yao Lu, and Xiaozhong
    Liu. 2018. Cross-language citation recommendation via hierarchical representation
    learning on heterogeneous graph. In *The 41st International ACM SIGIR Conference
    on Research & Development in Information Retrieval*. 635–644.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiao et al. (2020) Yizhu Jiao, Yun Xiong, Jiawei Zhang, Yao Zhang, Tianqi Zhang,
    and Yangyong Zhu. 2020. Sub-graph contrast for scalable self-supervised graph
    representation learning. In *2020 IEEE international conference on data mining
    (ICDM)*. IEEE, 222–231.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiménez-Luna et al. (2020) José Jiménez-Luna, Francesca Grisoni, and Gisbert
    Schneider. 2020. Drug discovery with explainable artificial intelligence. *Nature
    Machine Intelligence* 2, 10 (2020), 573–584.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2020b) Guangyin Jin, Zhexu Xi, Hengyu Sha, Yanghe Feng, and Jincai
    Huang. 2020b. Deep multi-view spatiotemporal virtual graph neural network for
    significant citywide ride-hailing demand prediction. *arXiv preprint arXiv:2007.15189*
    (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin et al. (2018) Wengong Jin, Regina Barzilay, and Tommi Jaakkola. 2018. Junction
    tree variational autoencoder for molecular graph generation. In *International
    conference on machine learning*. PMLR, 2323–2332.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. (2020a) Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang,
    Zitao Liu, and Jiliang Tang. 2020a. Self-supervised learning on graphs: Deep insights
    and new direction. *arXiv preprint arXiv:2006.10141* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jing et al. (2020) Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL
    Townshend, and Ron Dror. 2020. Learning from protein structure with geometric
    vector perceptrons. *arXiv preprint arXiv:2009.01411* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jing et al. (2021) Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre
    Townshend, and Ron Dror. 2021. Learning from Protein Structure with Geometric
    Vector Perceptrons. In *ICLR*. [https://openreview.net/forum?id=1YLJDvSx6J4](https://openreview.net/forum?id=1YLJDvSx6J4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ju et al. (2023a) Wei Ju, Yiyang Gu, Xiao Luo, Yifan Wang, Haochen Yuan, Huasong
    Zhong, and Ming Zhang. 2023a. Unsupervised graph-level representation learning
    with hierarchical contrasts. *Neural Networks* 158 (2023), 359–368.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ju et al. (2023b) Wei Ju, Zequn Liu, Yifang Qin, Bin Feng, Chen Wang, Zhihui
    Guo, Xiao Luo, and Ming Zhang. 2023b. Few-shot molecular property prediction via
    Hierarchically Structured Learning on Relation Graphs. *Neural Networks* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ju et al. (2022a) Wei Ju, Xiao Luo, Zeyu Ma, Junwei Yang, Minghua Deng, and
    Ming Zhang. 2022a. GHNN: Graph Harmonic Neural Networks for semi-supervised graph-level
    classification. *Neural Networks* 151 (2022), 70–79.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ju et al. (2022b) Wei Ju, Xiao Luo, Meng Qu, Yifan Wang, Chong Chen, Minghua
    Deng, Xian-Sheng Hua, and Ming Zhang. 2022b. TGNN: A Joint Semi-supervised Framework
    for Graph-level Classification. In *Proceedings of the International Joint Conference
    on Artificial Intelligence*. 2122–2128.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ju et al. (2022c) Wei Ju, Yifang Qin, Ziyue Qiao, Xiao Luo, Yifan Wang, Yanjie
    Fu, and Ming Zhang. 2022c. Kernel-based Substructure Exploration for Next POI
    Recommendation. *arXiv preprint arXiv:2210.03969* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ju et al. (2022d) Wei Ju, Junwei Yang, Meng Qu, Weiping Song, Jianhao Shen,
    and Ming Zhang. 2022d. KGNN: Harnessing Kernel-based Networks for Semi-supervised
    Graph Classification. In *Proceedings of the Fifteenth ACM International Conference
    on Web Search and Data Mining*. 421–429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. (2012) U Kang, Hanghang Tong, and Jimeng Sun. 2012. Fast random
    walk graph kernel. In *Proceedings of the SIAM international conference on data
    mining*. SIAM, 828–838.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpinski and Macintyre (1997) Marek Karpinski and Angus Macintyre. 1997. Polynomial
    bounds for VC dimension of sigmoidal and general Pfaffian neural networks. *J.
    Comput. System Sci.* 54, 1 (1997), 169–176.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kashima et al. (2003) Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. 2003.
    Marginalized kernels between labeled graphs. In *Proceedings of international
    conference on machine learning*. 321–328.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keikha et al. (2020) Mohammad Mehdi Keikha, Maseud Rahgozar, Masoud Asadpour,
    and Mohammad Faghih Abdollahi. 2020. Influence maximization across heterogeneous
    interconnected networks based on deep learning. *Expert Systems with Applications*
    140 (2020), 112905.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khoshraftar and An (2022) Shima Khoshraftar and Aijun An. 2022. A Survey on
    Graph Representation Learning Methods. *arXiv preprint arXiv:2204.01855* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding
    variational bayes. *arXiv preprint arXiv:1312.6114* (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf et al. (2018) Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling,
    and Richard Zemel. 2018. Neural relational inference for interacting systems.
    In *International Conference on Machine Learning*. PMLR, 2688–2697.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling (2016a) Thomas N Kipf and Max Welling. 2016a. Semi-supervised
    classification with graph convolutional networks. *arXiv preprint arXiv:1609.02907*
    (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling (2016b) Thomas N Kipf and Max Welling. 2016b. Variational graph
    auto-encoders. *arXiv preprint arXiv:1611.07308* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Klicpera et al. (2021) Johannes Klicpera, Florian Becker, and Stephan Günnemann.
    2021. GemNet: Universal Directional Graph Neural Networks for Molecules. In *NeurIPS*.
    [https://openreview.net/forum?id=HS_sOaxS9K-](https://openreview.net/forum?id=HS_sOaxS9K-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Klicpera et al. (2020) Johannes Klicpera, Janek Groß, and Stephan Günnemann.
    2020. Directional Message Passing for Molecular Graphs. In *ICLR*. [https://openreview.net/forum?id=B1eWbxStPH](https://openreview.net/forum?id=B1eWbxStPH)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Köhler et al. (2020) Jonas Köhler, Leon Klein, and Frank Noe. 2020. Equivariant
    Flows: Exact Likelihood Generative Learning for Symmetric Densities. In *ICML*.
    [https://proceedings.mlr.press/v119/kohler20a.html](https://proceedings.mlr.press/v119/kohler20a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kong et al. (2017) Xiangjie Kong, Huizhen Jiang, Wei Wang, Teshome Megersa Bekele,
    Zhenzhen Xu, and Meng Wang. 2017. Exploring dynamic research interest and academic
    influence for scientific collaborator recommendation. *Scientometrics* 113, 1
    (2017), 369–385.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kong et al. (2016) Xiangjie Kong, Huizhen Jiang, Zhuo Yang, Zhenzhen Xu, Feng
    Xia, and Amr Tolba. 2016. Exploiting publication contents and collaboration networks
    for collaborator recommendation. *PloS one* 11, 2 (2016), e0148492.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kong et al. (2019) Xiangjie Kong, Yajie Shi, Shuo Yu, Jiaying Liu, and Feng
    Xia. 2019. Academic social networks: Modeling, analysis, mining and applications.
    *Journal of Network and Computer Applications* 132 (2019), 86–103.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Koren (2008) Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted
    collaborative filtering model. In *Proceedings of the 14th ACM SIGKDD international
    conference on Knowledge discovery and data mining*. 426–434.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koren et al. (2009) Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix
    factorization techniques for recommender systems. *Computer* 42, 8 (2009), 30–37.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krenn et al. (2020) Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich,
    and Alan Aspuru-Guzik. 2020. Self-referencing embedded strings (SELFIES): A 100%
    robust molecular string representation. *Machine Learning: Science and Technology*
    1, 4 (2020), 045024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kreuzer et al. (2021) Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent
    Létourneau, and Prudencio Tossou. 2021. Rethinking graph transformers with spectral
    attention. *Advances in Neural Information Processing Systems* 34 (2021), 21618–21629.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kriege et al. (2020) Nils M Kriege, Fredrik D Johansson, and Christopher Morris.
    2020. A survey on graph kernels. *Applied Network Science* 5, 1 (2020), 1–42.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2017) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2017. Imagenet classification with deep convolutional neural networks. *Commun.
    ACM* 60, 6 (2017), 84–90.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2022) Sanjay Kumar, Abhishek Mallik, Anavi Khetarpal, and BS Panda.
    2022. Influence maximization in social networks using graph embedding and graph
    neural network. *Information Sciences* 607 (2022), 1617–1636.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laine and Aila (2016) Samuli Laine and Timo Aila. 2016. Temporal ensembling
    for semi-supervised learning. *arXiv preprint arXiv:1610.02242* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Landrum et al. (2013) Greg Landrum et al. 2013. RDKit: A software suite for
    cheminformatics, computational chemistry, and predictive modeling. *Greg Landrum*
    (2013).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2021) Dongha Lee, Su Kim, Seonghyeon Lee, Chanyoung Park, and Hwanjo
    Yu. 2021. Learnable structural semantic readout for graph classification. In *2021
    IEEE International Conference on Data Mining (ICDM)*. IEEE, 1180–1185.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2013) Dong-Hyun Lee et al. 2013. Pseudo-label: The simple and efficient
    semi-supervised learning method for deep neural networks. In *Workshop on challenges
    in representation learning, ICML*, Vol. 3\. 896.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2019) Junhyun Lee, Inyeop Lee, and Jaewoo Kang. 2019. Self-attention
    graph pooling. In *International conference on machine learning*. PMLR, 3734–3743.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2017) Tao Lei, Wengong Jin, Regina Barzilay, and Tommi Jaakkola.
    2017. Deriving Neural Architectures from Sequence and Graph Kernels. In *International
    Conference on Machine Learning*. 2024–2033.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levie et al. (2018) Ron Levie, Federico Monti, Xavier Bresson, and Michael M
    Bronstein. 2018. Cayleynets: Graph convolutional neural networks with complex
    rational spectral filters. *IEEE Transactions on Signal Processing* 67, 1 (2018),
    97–109.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Pan (2016) Angsheng Li and Yicheng Pan. 2016. Structural information
    and dynamical complexity of networks. *IEEE Transactions on Information Theory*
    62, 6 (2016), 3290–3339.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem.
    2020. Deepergcn: All you need to train deeper gcns. *arXiv preprint arXiv:2006.07739*
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022b) Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2022b.
    Ood-gnn: Out-of-distribution generalized graph neural network. *IEEE Transactions
    on Knowledge and Data Engineering* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022c) Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2022c.
    Out-of-distribution generalization on graphs: A survey. *arXiv preprint arXiv:2202.07987*
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017a) Junying Li, Deng Cai, and Xiaofei He. 2017a. Learning graph-level
    representation for drug discovery. *arXiv preprint arXiv:1709.03741* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022a) Jia Li, Yongfeng Huang, Heng Chang, and Yu Rong. 2022a. Semi-Supervised
    Hierarchical Graph Classification. *IEEE Transactions on Pattern Analysis and
    Machine Intelligence* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017b) Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian,
    and Jun Ma. 2017b. Neural attentive session-based recommendation. In *Proceedings
    of the 2017 ACM on Conference on Information and Knowledge Management*. 1419–1428.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019b) Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, and
    Junzhou Huang. 2019b. Semi-supervised graph classification: A hierarchical graph
    perspective. In *Proceedings of the Web Conference*. 972–982.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022d) Peibo Li, Yixing Yang, Maurice Pagnucco, and Yang Song. 2022d.
    CoGNet: Cooperative Graph Neural Networks. In *Proceedings of the International
    Joint Conference on Neural Networks*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2018a) Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018a. Deeper insights
    into graph convolutional networks for semi-supervised learning. In *Proceedings
    of the AAAI conference on artificial intelligence*, Vol. 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2018b) Ruoyu Li, Sheng Wang, Feiyun Zhu, and Junzhou Huang. 2018b.
    Adaptive graph convolutional neural networks. In *Proceedings of the AAAI conference
    on artificial intelligence*, Vol. 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022e) Shuangli Li, Jingbo Zhou, Tong Xu, Dejing Dou, and Hui Xiong.
    2022e. Geomgcl: geometric graph contrastive learning for molecular property prediction.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 36. 4541–4549.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019a) Yibo Li, Jianxing Hu, Yanxing Wang, Jielong Zhou, Liangren
    Zhang, and Zhenming Liu. 2019a. Deepscaffold: a comprehensive tool for scaffold-based
    de novo drug discovery using deep learning. *Journal of chemical information and
    modeling* 60, 1 (2019), 77–91.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021a) Yibo Li, Jianfeng Pei, and Luhua Lai. 2021a. Learning to design
    drug-like molecules in three-dimensional space using deep generative models. *arXiv
    preprint arXiv:2104.08474* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021b) Yibo Li, Jianfeng Pei, and Luhua Lai. 2021b. Structure-based
    de novo drug design using 3D deep generative models. *Chemical science* 12, 41
    (2021), 13664–13675.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2015) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
    2015. Gated graph sequence neural networks. *arXiv preprint arXiv:1511.05493*
    (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017c) Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2017c. Diffusion
    convolutional recurrent neural network: Data-driven traffic forecasting. *arXiv
    preprint arXiv:1707.01926* (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow.
    2016. Gated Graph Sequence Neural Networks. In *Proceedings of ICLR’16*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019c) Ziyao Li, Liang Zhang, and Guojie Song. 2019c. GCN-LASE:
    Towards Adequately Incorporating Link Attributes in Graph Convolutional Networks.
    In *Proceedings of the Twenty-Eighth International Joint Conference on Artificial
    Intelligence, IJCAI-19*. 2959–2965.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2020) Yanyan Liang, Yanfeng Zhang, Dechao Gao, and Qian Xu. 2020.
    MxPool: Multiplex Pooling for Hierarchical Graph Representation Learning. *arXiv
    preprint arXiv:2004.06846* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. (2018) Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard Zemel.
    2018. LanczosNet: Multi-Scale Deep Graph Convolutional Networks. In *International
    Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lim et al. (2021) Jongin Lim, Daeho Um, Hyung Jin Chang, Dae Ung Jo, and Jin Young
    Choi. 2021. Class-attentive diffusion network for semi-supervised classification.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 35. 8601–8609.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2022a) Haitao Lin, Yufei Huang, Meng Liu, Xuanjing Li, Shuiwang
    Ji, and Stan Z Li. 2022a. DiffBP: Generative Diffusion of 3D Molecules for Target
    Protein Binding. *arXiv preprint arXiv:2211.11214* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2022b) Jiacheng Lin, Hanwen Xu, Addie Woicik, Jianzhu Ma, and Sheng
    Wang. 2022b. Pisces: A cross-modal contrastive learning approach to synergistic
    drug combination prediction. *bioRxiv* (2022). [https://doi.org/10.1101/2022.11.21.517439](https://doi.org/10.1101/2022.11.21.517439)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Jiaying Liu, Feng Xia, Lei Wang, Bo Xu, Xiangjie Kong, Hanghang
    Tong, and Irwin King. 2019. Shifu2: A network representation learning based model
    for advisor-advisee relationship mining. *IEEE Transactions on Knowledge and Data
    Engineering* 33, 4 (2019), 1763–1777.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2016) Li Liu, William K Cheung, Xin Li, and Lejian Liao. 2016. Aligning
    Users across Social Networks Using Network Embedding.. In *Ijcai*, Vol. 16. 1774–80.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022a) Meng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, and Shuiwang
    Ji. 2022a. Generating 3D Molecules for Target Protein Binding. *arXiv preprint
    arXiv:2204.09410* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018a) Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander
    Gaunt. 2018a. Constrained graph variational autoencoders for molecule design.
    *Advances in neural information processing systems* 31 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018d) Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. 2018d.
    STAMP: short-term attention/memory priority model for session-based recommendation.
    In *Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery
    & data mining*. 1831–1839.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022b) Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora
    Oztekin, and Shuiwang Ji. 2022b. Spherical Message Passing for 3D Molecular Graphs.
    In *ICLR*. [https://openreview.net/forum?id=givsRXsOt9r](https://openreview.net/forum?id=givsRXsOt9r)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018b) Ziqi Liu, Chaochao Chen, Xinxing Yang, Jun Zhou, Xiaolong
    Li, and Le Song. 2018b. Heterogeneous graph neural networks for malicious account
    detection. In *Proceedings of the 27th ACM International Conference on Information
    and Knowledge Management*. 2077–2085.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,
    Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer
    using shifted windows. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*. 10012–10022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018c) Zheng Liu, Xing Xie, and Lei Chen. 2018c. Context-aware academic
    collaborator recommendation. In *Proceedings of the 24th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*. 1870–1879.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2021a) Qingqing Long, Yilun Jin, Yi Wu, and Guojie Song. 2021a.
    Theoretically improving graph neural networks via anonymous walk graph kernels.
    In *Proceedings of the Web Conference 2021*. 1204–1214.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Long et al. (2021b) Qingqing Long, Lingjun Xu, Zheng Fang, and Guojie Song.
    2021b. HGK-GNN: Heterogeneous Graph Kernel based Graph Neural Networks. In *Proceedings
    of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining*. 1129–1138.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2022) Siyu Long, Yi Zhou, Xinyu Dai, and Hao Zhou. 2022. Zero-Shot
    3D Drug Design by Sketching and Generating. *arXiv preprint arXiv:2209.13865*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2021a) Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao
    Ni, Haifeng Chen, and Xiang Zhang. 2021a. Learning to drop: Robust graph neural
    network via topological denoising. In *Proceedings of the 14th ACM international
    conference on web search and data mining*. 779–787.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2021b) Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. 2021b.
    A 3D generative model for structure-based drug design. *Advances in Neural Information
    Processing Systems* 34 (2021), 6229–6239.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2022a) Xiao Luo, Wei Ju, Meng Qu, Chong Chen, Minghua Deng, Xian-Sheng
    Hua, and Ming Zhang. 2022a. DualGraph: Improving Semi-supervised Graph Classification
    via Dual Contrastive Learning. In *Proceedings of the IEEE International Conference
    on Data Engineering*. 699–712.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2022b) Xiao Luo, Wei Ju, Meng Qu, Yiyang Gu, Chong Chen, Minghua
    Deng, Xian-Sheng Hua, and Ming Zhang. 2022b. Clear: Cluster-enhanced contrast
    for self-supervised graph representation learning. *IEEE Transactions on Neural
    Networks and Learning Systems* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2021c) Xiao Luo, Daqing Wu, Zeyu Ma, Chong Chen, Minghua Deng,
    Jinwen Ma, Zhongming Jin, Jianqiang Huang, and Xian-Sheng Hua. 2021c. CIMON: Towards
    High-quality Hash Codes. In *Proceedings of the International Joint Conference
    on Artificial Intelligence*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo and Ji (2022) Youzhi Luo and Shuiwang Ji. 2022. An autoregressive flow model
    for 3d molecular geometry generation from scratch. In *International Conference
    on Learning Representations (ICLR)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2021d) Youzhi Luo, Keqiang Yan, and Shuiwang Ji. 2021d. Graphdf:
    A discrete flow model for molecular graph generation. In *International Conference
    on Machine Learning*. PMLR, 7192–7203.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2020a) Hehuan Ma, Yatao Bian, Yu Rong, Wenbing Huang, Tingyang Xu,
    Weiyang Xie, Geyan Ye, and Junzhou Huang. 2020a. Multi-view graph neural networks
    for molecular property prediction. *arXiv preprint arXiv:2005.13607* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2021) Jiaqi Ma, Junwei Deng, and Qiaozhu Mei. 2021. Subgroup generalization
    and fairness of graph neural networks. *Advances in Neural Information Processing
    Systems* 34 (2021), 1048–1061.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2020b) Ning Ma, Jiajun Bu, Jieyu Yang, Zhen Zhang, Chengwei Yao,
    Zhi Yu, Sheng Zhou, and Xifeng Yan. 2020b. Adaptive-step graph meta-learner for
    few-shot graph classification. In *Proceedings of the 29th ACM International Conference
    on Information & Knowledge Management*. 1055–1064.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2019) Yao Ma, Suhang Wang, Charu C Aggarwal, and Jiliang Tang. 2019.
    Graph convolutional networks with eigenpooling. In *Proceedings of the 25th ACM
    SIGKDD international conference on knowledge discovery & data mining*. 723–731.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madhawa et al. (2019) Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago,
    and Motoki Abe. 2019. Graphnvp: An invertible flow model for generating molecular
    graphs. *arXiv preprint arXiv:1905.11600* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madumal et al. (2020) Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank
    Vetere. 2020. Explainable reinforcement learning through a causal lens. In *Proceedings
    of the AAAI conference on artificial intelligence*, Vol. 34. 2493–2500.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Man et al. (2016) Tong Man, Huawei Shen, Shenghua Liu, Xiaolong Jin, and Xueqi
    Cheng. 2016. Predict anchor links across social networks via an embedding approach..
    In *Ijcai*, Vol. 16\. 1823–1829.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manguri et al. (2020) Kamaran H Manguri, Rebaz N Ramadhan, and Pshko R Mohammed
    Amin. 2020. Twitter sentiment analysis on worldwide COVID-19 outbreaks. *Kurdistan
    Journal of Applied Research* (2020), 54–65.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mansimov et al. (2019) Elman Mansimov, Omar Mahmood, Seokho Kang, and Kyunghyun
    Cho. 2019. Molecular geometry prediction using a deep generative graph neural
    network. *Scientific reports* 9, 1 (2019), 1–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MansourLakouraj et al. (2022) Mohammad MansourLakouraj, Mukesh Gautam, Hanif
    Livani, and Mohammed Benidris. 2022. A multi-rate sampling PMU-based event classification
    in active distribution grids with spectral graph neural network. *Electric Power
    Systems Research* 211 (2022), 108145.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Margaris et al. (2019) Dionisis Margaris, Costas Vassilakis, and Dimitris Spiliotopoulos.
    2019. Handling uncertainty in social media textual information for improving venue
    recommendation formulation quality in social networks. *Social Network Analysis
    and Mining* 9, 1 (2019), 1–19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mehrabi et al. (2021) Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina
    Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning.
    *ACM Computing Surveys (CSUR)* 54, 6 (2021), 1–35.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meng et al. (2021) Xuying Meng, Suhang Wang, Zhimin Liang, Di Yao, Jihua Zhou,
    and Yujun Zhang. 2021. Semi-supervised anomaly detection in dynamic communication
    networks. *Information Sciences* 571 (2021), 527–542.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meng et al. (2011) Xuan-Yu Meng, Hong-Xing Zhang, Mihaly Mezei, and Meng Cui.
    2011. Molecular docking: a powerful approach for structure-based drug discovery.
    *Current computer-aided drug design* 7, 2 (2011), 146–157.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meyers et al. (2021) Joshua Meyers, Benedek Fabian, and Nathan Brown. 2021.
    De novo molecular design and generative models. *Drug Discovery Today* 26, 11
    (2021), 2707–2715.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013. Distributed representations of words and phrases and their
    compositionality. *Advances in neural information processing systems* 26 (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mohamed et al. (2020) Abduallah Mohamed, Kun Qian, Mohamed Elhoseiny, and Christian
    Claudel. 2020. Social-stgcnn: A social spatio-temporal graph convolutional neural
    network for human trajectory prediction. In *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*. 14424–14432.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Morris et al. (2019) Christopher Morris, Martin Ritzert, Matthias Fey, William L
    Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. 2019. Weisfeiler
    and leman go neural: Higher-order graph neural networks. In *Proceedings of the
    AAAI conference on artificial intelligence*, Vol. 33. 4602–4609.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moscato and Sperlì (2021) Vincenzo Moscato and Giancarlo Sperlì. 2021. A survey
    about community detection over On-line Social and Heterogeneous Information Networks.
    *Knowledge-Based Systems* 224 (2021), 107112.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Müller (2007) Meinard Müller. 2007. Dynamic time warping. *Information retrieval
    for music and motion* (2007), 69–84.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nesterov et al. (2020) Vitali Nesterov, Mario Wieser, and Volker Roth. 2020.
    3DMolNet: a generative network for molecular structures. *arXiv preprint arXiv:2010.06477*
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ou et al. (2016) Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu.
    2016. Asymmetric transitivity preserving graph embedding. In *Proceedings of the
    22nd ACM SIGKDD international conference on Knowledge discovery and data mining*.
    1105–1114.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Panagopoulos et al. (2020) George Panagopoulos, Fragkiskos Malliaros, and Michalis
    Vazirgiannis. 2020. Multi-task learning for influence estimation and maximization.
    *IEEE Transactions on Knowledge and Data Engineering* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2020) Cheonbok Park, Chunggi Lee, Hyojin Bahng, Yunwon Tae, Seungmin
    Jin, Kihwan Kim, Sungahn Ko, and Jaegul Choo. 2020. ST-GRAT: A novel spatio-temporal
    graph attention networks for accurately forecasting dynamically changing road
    speed. In *Proceedings of the 29th ACM international conference on information
    & knowledge management*. 1215–1224.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2021) Hyeonjin Park, Seunghun Lee, Dasol Hwang, Jisu Jeong, Kyung-Min
    Kim, Jung-Woo Ha, and Hyunwoo J Kim. 2021. Learning Augmentation for GNNs With
    Consistency Regularization. *IEEE Access* 9 (2021), 127961–127972.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paulos et al. (2004) Eric Paulos, Ken Anderson, and Anthony Townsend. 2004.
    UbiComp in the urban frontier. (2004).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paulos and Goodman (2004) Eric Paulos and Elizabeth Goodman. 2004. The familiar
    stranger: anxiety, comfort, and play in public places. In *Proceedings of the
    SIGCHI conference on Human factors in computing systems*. 223–230.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2023) Xingang Peng, Jiaqi Guan, Jian Peng, and Jianzhu Ma. 2023.
    Pocket-specific 3D Molecule Generation by Fragment-based Autoregressive Diffusion
    Models. [https://openreview.net/forum?id=HGsoe1wmRW5](https://openreview.net/forum?id=HGsoe1wmRW5)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2022) Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng,
    and Jianzhu Ma. 2022. Pocket2Mol: Efficient Molecular Sampling Based on 3D Protein
    Pockets. *arXiv preprint arXiv:2205.07249* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perozzi et al. (2014) Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014.
    Deepwalk: Online learning of social representations. In *Proceedings of the 20th
    ACM SIGKDD international conference on Knowledge discovery and data mining*. 701–710.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pilco and Rivera (2019) Darwin Saire Pilco and Adín Ramírez Rivera. 2019. Graph
    learning network: A structure learning algorithm. *arXiv preprint arXiv:1905.12665*
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pimentel et al. (2014) Marco AF Pimentel, David A Clifton, Lei Clifton, and
    Lionel Tarassenko. 2014. A review of novelty detection. *Signal processing* 99
    (2014), 215–249.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pinheiro et al. (2022) Gabriel A Pinheiro, Juarez LF Da Silva, and Marcos G
    Quiles. 2022. SMICLR: Contrastive Learning on Multiple Molecular Representations
    for Semisupervised and Unsupervised Representation Learning. *Journal of Chemical
    Information and Modeling* 62, 17 (2022), 3948–3960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Preuer et al. (2019) Kristina Preuer, Günter Klambauer, Friedrich Rippmann,
    Sepp Hochreiter, and Thomas Unterthiner. 2019. Interpretable deep learning in
    drug discovery. In *Explainable AI: Interpreting, Explaining and Visualizing Deep
    Learning*. Springer, 331–345.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiao et al. (2019) Ziyue Qiao, Yi Du, Yanjie Fu, Pengfei Wang, and Yuanchun
    Zhou. 2019. Unsupervised author disambiguation using heterogeneous graph convolutional
    network embedding. In *2019 IEEE international conference on big data (Big Data)*.
    IEEE, 910–919.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiao et al. (2022) Ziyue Qiao, Yanjie Fu, Pengyang Wang, Meng Xiao, Zhiyuan
    Ning, Yi Du, and Yuanchun Zhou. 2022. RPT: Toward Transferable Model on Heterogeneous
    Researcher Data via Pre-Training. *IEEE Transactions on Big Data* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiao et al. (2020a) Ziyue Qiao, Pengyang Wang, Yanjie Fu, Yi Du, Pengfei Wang,
    and Yuanchun Zhou. 2020a. Tree structure-aware graph representation learning via
    integrated hierarchical aggregation and relational metric learning. In *2020 IEEE
    International Conference on Data Mining (ICDM)*. IEEE, 432–441.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiao et al. (2020b) Zhuoran Qiao, Matthew Welborn, Animashree Anandkumar, Frederick R
    Manby, and Thomas F Miller III. 2020b. OrbNet: Deep learning for quantum chemistry
    using symmetry-adapted atomic-orbital features. *The Journal of chemical physics*
    153, 12 (2020), 124111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2022) Yifang Qin, Yifan Wang, Fang Sun, Wei Ju, Xuyang Hou, Zhe
    Wang, Jia Cheng, Jun Lei, and Ming Zhang. 2022. DisenPOI: Disentangling Sequential
    and Geographical Influence for Point-of-Interest Recommendation. *arXiv preprint
    arXiv:2210.16591* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. (2020a) Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia
    Yang, Ming Ding, Kuansan Wang, and Jie Tang. 2020a. Gcc: Graph contrastive coding
    for graph neural network pre-training. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 1150–1160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiu et al. (2019) Ruihong Qiu, Jingjing Li, Zi Huang, and Hongzhi Yin. 2019.
    Rethinking the item order in session-based recommendation with graph neural networks.
    In *Proceedings of the 28th ACM international conference on information and knowledge
    management*. 579–588.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. (2020b) Ruihong Qiu, Hongzhi Yin, Zi Huang, and Tong Chen. 2020b.
    Gag: Global attributed graph neural network for streaming session-based recommendation.
    In *Proceedings of the 43rd International ACM SIGIR Conference on Research and
    Development in Information Retrieval*. 669–678.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. 2021. Learning transferable visual models from natural language
    supervision. In *International Conference on Machine Learning*. PMLR, 8748–8763.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ragoza et al. (2022) Matthew Ragoza, Tomohide Masuda, and David Ryan Koes. 2022.
    Generating 3D molecules conditional on receptor binding sites with deep generative
    models. *Chemical science* 13, 9 (2022), 2701–2713.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ran and Boyce (2012) Bin Ran and David Boyce. 2012. *Modeling dynamic transportation
    networks: an intelligent transportation system oriented approach*. Springer Science
    & Business Media.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ranjan et al. (2020) Ekagra Ranjan, Soumya Sanyal, and Partha Talukdar. 2020.
    Asap: Adaptive structure aware pooling for learning hierarchical graph representations.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 34\.
    5470–5477.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rendle et al. (2012) Steffen Rendle, Christoph Freudenthaler, Zeno Gantner,
    and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit
    feedback. *arXiv preprint arXiv:1205.2618* (2012).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rezende and Mohamed (2015) Danilo Rezende and Shakir Mohamed. 2015. Variational
    inference with normalizing flows. In *International conference on machine learning*.
    PMLR, 1530–1538.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Richardson and Domingos (2002) Matthew Richardson and Pedro Domingos. 2002.
    Mining knowledge-sharing sites for viral marketing. In *Proceedings of the eighth
    ACM SIGKDD international conference on Knowledge discovery and data mining*. 61–70.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Richens et al. (2020) Jonathan G Richens, Ciarán M Lee, and Saurabh Johri. 2020.
    Improving the accuracy of medical diagnosis with causal machine learning. *Nature
    communications* 11, 1 (2020), 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roney et al. (2021) James P Roney, Paul Maragakis, Peter Skopp, and David E
    Shaw. 2021. Generating Realistic 3D Molecules with an Equivariant Conditional
    Likelihood Model. (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosa et al. (2018) Renata Lopes Rosa, Gisele Maria Schwartz, Wilson Vicente
    Ruggiero, and Demóstenes Zegarra Rodríguez. 2018. A knowledge-based recommendation
    system that includes sentiment analysis and deep learning. *IEEE Transactions
    on Industrial Informatics* 15, 4 (2018), 2124–2135.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruddigkeit et al. (2012) Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and
    Jean-Louis Reymond. 2012. Enumeration of 166 billion organic small molecules in
    the chemical universe database GDB-17. *Journal of chemical information and modeling*
    52, 11 (2012), 2864–2875.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runwal et al. (2022) Bharat Runwal, Sandeep Kumar, et al. 2022. Robustifying
    GNN Via Weighted Laplacian. In *2022 IEEE International Conference on Signal Processing
    and Communications (SPCOM)*. IEEE, 1–5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ryu et al. (2018) Seongok Ryu, Jaechang Lim, Seung Hwan Hong, and Woo Youn Kim.
    2018. Deeply learning molecular structure-property relationships using attention-and
    gate-augmented graph convolutional network. *arXiv preprint arXiv:1805.10988*
    (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sabour et al. (2017) Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017.
    Dynamic routing between capsules. *Advances in neural information processing systems*
    30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sandryhaila and Moura (2013) Aliaksei Sandryhaila and José MF Moura. 2013. Discrete
    signal processing on graphs. *IEEE transactions on signal processing* 61, 7 (2013),
    1644–1656.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Satorras et al. (2021) Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling.
    2021. E(n) Equivariant Graph Neural Networks. *arXiv preprint arXiv:2102.09844*
    (2021). arXiv:2102.09844 [cs.LG]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saxena et al. (2020) Chandni Saxena, Tianyu Liu, and Irwin King. 2020. A Survey
    of Graph Curvature and Embedding in Non-Euclidean Spaces. In *International Conference
    on Neural Information Processing*. Springer, 127–139.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scarselli et al. (2008) Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
    Hagenbuchner, and Gabriele Monfardini. 2008. The graph neural network model. *IEEE
    transactions on neural networks* 20, 1 (2008), 61–80.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schlichtkrull et al. (2018) Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,
    Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data
    with graph convolutional networks. In *European semantic web conference*. Springer,
    593–607.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schneider et al. (2020) Petra Schneider, W Patrick Walters, Alleyn T Plowright,
    Norman Sieroka, Jennifer Listgarten, Robert A Goodnow, Jasmin Fisher, Johanna M
    Jansen, José S Duca, Thomas S Rush, et al. 2020. Rethinking drug design in the
    artificial intelligence era. *Nature Reviews Drug Discovery* 19, 5 (2020), 353–364.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schneuing et al. (2022) Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb,
    Ilia Igashov, Weitao Du, Tom Blundell, Pietro Lió, Carla Gomes, Max Welling, et al.
    2022. Structure-based Drug Design with Equivariant Diffusion Models. *arXiv preprint
    arXiv:2210.13695* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schütt et al. (2021) Kristof Schütt, Oliver Unke, and Michael Gastegger. 2021.
    Equivariant message passing for the prediction of tensorial properties and molecular
    spectra. In *ICML*. [https://proceedings.mlr.press/v139/schutt21a.html](https://proceedings.mlr.press/v139/schutt21a.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schütt et al. (2018) Kristof T Schütt, Huziel E Sauceda, P-J Kindermans, Alexandre
    Tkatchenko, and K-R Müller. 2018. Schnet–a deep learning architecture for molecules
    and materials. *The Journal of Chemical Physics* 148, 24 (2018), 241722.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sen et al. (2008) Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor,
    Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classification in network
    data. *AI magazine* 29, 3 (2008), 93–93.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shawe-Taylor et al. (2004) John Shawe-Taylor, Nello Cristianini, et al. 2004.
    *Kernel methods for pattern analysis*. Cambridge university press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shervashidze et al. (2011a) Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen,
    Kurt Mehlhorn, and Karsten M Borgwardt. 2011a. Weisfeiler-lehman graph kernels.
    *Journal of Machine Learning Research* 12, 9 (2011), 2539–2561.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shervashidze et al. (2011b) Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen,
    Kurt Mehlhorn, and Karsten M Borgwardt. 2011b. Weisfeiler-lehman graph kernels.
    *Journal of Machine Learning Research* 12, 9 (2011).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shervashidze et al. (2009) Nino Shervashidze, SVN Vishwanathan, Tobias Petri,
    Kurt Mehlhorn, and Karsten Borgwardt. 2009. Efficient graphlet kernels for large
    graph comparison. In *Proceedings of International Conference on Artificial Intelligence
    and Statistics*. 488–495.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2016) Chuan Shi, Yitong Li, Jiawei Zhang, Yizhou Sun, and S Yu Philip.
    2016. A survey of heterogeneous information network analysis. *IEEE Transactions
    on Knowledge and Data Engineering* 29, 1 (2016), 17–37.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2021) Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. 2021. Learning
    gradient fields for molecular conformation generation. *Proceedings of the 38th
    International Conference on Machine Learning, ICML* 139 (2021), 9558–9568.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2020) Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming
    Zhang, and Jian Tang. 2020. Graphaf: a flow-based autoregressive model for molecular
    graph generation. *arXiv preprint arXiv:2001.09382* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shuman et al. (2013a) David I Shuman, Sunil K Narang, Pascal Frossard, Antonio
    Ortega, and Pierre Vandergheynst. 2013a. The emerging field of signal processing
    on graphs: Extending high-dimensional data analysis to networks and other irregular
    domains. *IEEE signal processing magazine* 30, 3 (2013), 83–98.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shuman et al. (2013b) David I Shuman, Sunil K Narang, Pascal Frossard, Antonio
    Ortega, and Pierre Vandergheynst. 2013b. The emerging field of signal processing
    on graphs: Extending high-dimensional data analysis to networks and other irregular
    domains. *IEEE signal processing magazine* 30, 3 (2013), 83–98.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Si et al. (2019) Yali Si, Fuzhi Zhang, and Wenyuan Liu. 2019. An adaptive point-of-interest
    recommendation method for location-based social networks based on user activity
    and spatial features. *Knowledge-Based Systems* 163 (2019), 267–282.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silva et al. (2019) Thiago H Silva, Aline Carneiro Viana, Fabrício Benevenuto,
    Leandro Villas, Juliana Salles, Antonio Loureiro, and Daniele Quercia. 2019. Urban
    computing leveraging location-based social network data: a survey. *ACM Computing
    Surveys (CSUR)* 52, 1 (2019), 1–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonovsky and Komodakis (2017) Martin Simonovsky and Nikos Komodakis. 2017.
    Dynamic edge-conditioned filters in convolutional neural networks on graphs. In
    *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    3693–3702.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonovsky and Komodakis (2018) Martin Simonovsky and Nikos Komodakis. 2018.
    Graphvae: Towards generation of small graphs using variational autoencoders. In
    *International conference on artificial neural networks*. Springer, 412–422.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sofuoglu and Aviyente (2022) Seyyid Emre Sofuoglu and Selin Aviyente. 2022.
    Gloss: Tensor-based anomaly detection in spatiotemporal urban traffic data. *Signal
    Processing* 192 (2022), 108370.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *International Conference on Machine Learning*. PMLR, 2256–2265.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2020a) Chao Song, Youfang Lin, Shengnan Guo, and Huaiyu Wan. 2020a.
    Spatial-temporal synchronous graph convolutional networks: A new framework for
    spatial-temporal network data forecasting. In *Proceedings of the AAAI conference
    on artificial intelligence*, Vol. 34\. 914–921.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2020b) Qingyu Song, RuiBo Ming, Jianming Hu, Haoyi Niu, and Mingyang
    Gao. 2020b. Graph attention convolutional network: Spatiotemporal modeling for
    urban traffic prediction. In *2020 IEEE 23rd International Conference on Intelligent
    Transportation Systems (ITSC)*. IEEE, 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song and Ermon (2019) Yang Song and Stefano Ermon. 2019. Generative modeling
    by estimating gradients of the data distribution. *Advances in Neural Information
    Processing Systems* 32 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spackman et al. (2016) Peter R. Spackman, Dylan Jayatilaka, and Amir Karton.
    2016. Basis set convergence of CCSD(T) equilibrium geometries using a large and
    diverse set of molecular structures. *The Journal of Chemical Physics* 145, 10
    (2016), 104101. [https://doi.org/10.1063/1.4962168](https://doi.org/10.1063/1.4962168)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stärk et al. (2022) Hannes Stärk, Octavian Ganea, Lagnajit Pattanaik, Regina
    Barzilay, and Tommi Jaakkola. 2022. Equibind: Geometric deep learning for drug
    binding structure prediction. In *International Conference on Machine Learning*.
    PMLR, 20503–20521.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2022) Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi
    Rao, Hao Sun, Zhiwu Lu, and Ji-Rong Wen. 2022. A molecular multimodal foundation
    model associating molecule graphs with natural language. *arXiv preprint arXiv:2209.05481*
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sugiyama and Kan (2010) Kazunari Sugiyama and Min-Yen Kan. 2010. Scholarly paper
    recommendation via user’s recent research interests. In *Proceedings of the 10th
    annual joint conference on Digital libraries*. 29–38.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2020a) Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang.
    2020a. Infograph: Unsupervised and semi-supervised graph-level representation
    learning via mutual information maximization. In *Proceedings of the International
    Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2020b) Jianhua Sun, Qinhong Jiang, and Cewu Lu. 2020b. Recursive
    social behavior graph for trajectory prediction. In *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*. 660–669.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022) Qingyun Sun, Jianxin Li, Hao Peng, Jia Wu, Xingcheng Fu, Cheng
    Ji, and S Yu Philip. 2022. Graph structure learning with variational information
    bottleneck. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    Vol. 36. 4165–4174.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2020c) Xiaoqing Sun, Zhiliang Wang, Jiahai Yang, and Xinran Liu.
    2020c. Deepdom: Malicious domain detection with scalable and heterogeneous graph
    convolutional networks. *Computers & Security* 99 (2020), 102057.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tabassum et al. (2018) Shazia Tabassum, Fabiola SF Pereira, Sofia Fernandes,
    and João Gama. 2018. Social network analysis: An overview. *Wiley Interdisciplinary
    Reviews: Data Mining and Knowledge Discovery* 8, 5 (2018), e1256.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tailor et al. (2022) Shyam A Tailor, Felix Opolka, Pietro Lio, and Nicholas Donald
    Lane. 2022. Do We Need Anisotropic Graph Neural Networks?. In *International Conference
    on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takahashi and Igata (2012) Tetsuro Takahashi and Nobuyuki Igata. 2012. Rumor
    detection on twitter. In *The 6th International Conference on Soft Computing and
    Intelligent Systems, and The 13th International Symposium on Advanced Intelligence
    Systems*. IEEE, 452–457.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2020a) Hao Tang, Donghong Ji, Chenliang Li, and Qiji Zhou. 2020a.
    Dependency graph enhanced dual-transformer structure for aspect-based sentiment
    classification. In *Proceedings of the 58th annual meeting of the association
    for computational linguistics*. 6578–6588.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2015a) Jian Tang, Meng Qu, and Qiaozhu Mei. 2015a. Pte: Predictive
    text embedding through large-scale heterogeneous text networks. In *Proceedings
    of the 21th ACM SIGKDD international conference on knowledge discovery and data
    mining*. 1165–1174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2015b) Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan,
    and Qiaozhu Mei. 2015b. Line: Large-scale information network embedding. In *Proceedings
    of the 24th international conference on world wide web*. 1067–1077.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2020b) Xianfeng Tang, Yandong Li, Yiwei Sun, Huaxiu Yao, Prasenjit
    Mitra, and Suhang Wang. 2020b. Transferring robustness for graph neural network
    against poisoning attacks. In *Proceedings of the 13th international conference
    on web search and data mining*. 600–608.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thölke and Fabritiis (2022) Philipp Thölke and Gianni De Fabritiis. 2022. Equivariant
    Transformers for Neural Network based Molecular Potentials. In *ICLR*. [https://openreview.net/forum?id=zNHzqZ9wrRB](https://openreview.net/forum?id=zNHzqZ9wrRB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thomas et al. (2018) Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang,
    Li Li, Kai Kohlhoff, and Patrick Riley. 2018. Tensor field networks: Rotation-and
    translation-equivariant neural networks for 3d point clouds. *arXiv preprint arXiv:1802.08219*
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toivonen et al. (2003) Hannu Toivonen, Ashwin Srinivasan, Ross D King, Stefan
    Kramer, and Christoph Helma. 2003. Statistical evaluation of the predictive toxicology
    challenge 2000–2001. *Bioinformatics* 19, 10 (2003), 1183–1193.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unankard et al. (2014) Sayan Unankard, Xue Li, Mohamed Sharaf, Jiang Zhong,
    and Xueming Li. 2014. Predicting elections from social networks based on sub-event
    detection and sentiment analysis. In *International Conference on Web Information
    Systems Engineering*. Springer, 1–16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veličković et al. (2017) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks.
    *arXiv preprint arXiv:1710.10903* (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verma et al. (2010) Jitender Verma, Vijay M Khedkar, and Evans C Coutinho. 2010.
    3D-QSAR in drug design-a review. *Current topics in medicinal chemistry* 10, 1
    (2010), 95–115.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vincent et al. (2010) Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua
    Bengio, Pierre-Antoine Manzagol, and Léon Bottou. 2010. Stacked denoising autoencoders:
    Learning useful representations in a deep network with a local denoising criterion.
    *Journal of machine learning research* 11, 12 (2010).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vinyals et al. (2016) Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan
    Wierstra, et al. 2016. Matching networks for one shot learning. *Advances in neural
    information processing systems* 29 (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Walsh et al. (2023) Dylan J Walsh, Weizhong Zou, Ludwig Schneider, Reid Mello,
    Michael E Deagen, Joshua Mysona, Tzyy-Shyang Lin, Juan J de Pablo, Klavs F Jensen,
    Debra J Audus, et al. 2023. Community Resource for Innovation in Polymer Technology
    (CRIPT): A Scalable Polymer Material Data Structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walters et al. (1998) W Patrick Walters, Matthew T Stahl, and Mark A Murcko.
    1998. Virtual screening—an overview. *Drug discovery today* 3, 4 (1998), 160–178.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan and Kokel (2021) Guihong Wan and Harsha Kokel. 2021. Graph sparsification
    via meta-learning. *DLG@ AAAI* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2017) Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing
    Jiang. 2017. Mgae: Marginalized graph autoencoder for graph clustering. In *Proceedings
    of the 2017 ACM on Conference on Information and Knowledge Management*. 889–898.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2016) Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep
    network embedding. In *Proceedings of the 22nd ACM SIGKDD international conference
    on Knowledge discovery and data mining*. 1225–1234.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019e) Hongwei Wang, Fuzheng Zhang, Miao Zhao, Wenjie Li, Xing
    Xie, and Minyi Guo. 2019e. Multi-task feature learning for knowledge graph enhanced
    recommendation. In *The world wide web conference*. 2000–2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020a) Jianling Wang, Kaize Ding, Liangjie Hong, Huan Liu, and
    James Caverlee. 2020a. Next-item recommendation with sequential hypergraphs. In
    *Proceedings of the 43rd international ACM SIGIR conference on research and development
    in information retrieval*. 1101–1110.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020e) Jie Wang, Li Zhu, Tao Dai, and Yabin Wang. 2020e. Deep memory
    network with Bi-LSTM for personalized context-aware citation recommendation. *Neurocomputing*
    410 (2020), 103–113.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019c) Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X
    Chang, and Daniel Ritchie. 2019c. Planit: Planning and instantiating indoor scenes
    with relation graph and spatial prior networks. *ACM Transactions on Graphics
    (TOG)* 38, 4 (2019), 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2005) Renxiao Wang, Xueliang Fang, Yipin Lu, Chao-Yie Yang, and
    Shaomeng Wang. 2005. The PDBbind database: methodologies and updates. *Journal
    of medicinal chemistry* 48, 12 (2005), 4111–4119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020c) Senzhang Wang, Hao Miao, Hao Chen, and Zhiqiu Huang. 2020c.
    Multi-task adversarial spatial-temporal networks for crowd flow prediction. In
    *Proceedings of the 29th ACM international conference on information & knowledge
    management*. 1555–1564.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022a) Xiao Wang, Deyu Bo, Chuan Shi, Shaohua Fan, Yanfang Ye,
    and S Yu Philip. 2022a. A survey on heterogeneous graph embedding: methods, techniques,
    applications and sources. *IEEE Transactions on Big Data* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019a) Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng
    Chua. 2019a. Neural graph collaborative filtering. In *Proceedings of the 42nd
    international ACM SIGIR conference on Research and development in Information
    Retrieval*. 165–174.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019b) Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng
    Cui, and Philip S Yu. 2019b. Heterogeneous graph attention network. In *The world
    wide web conference*. 2022–2032.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu,
    and Tat-Seng Chua. 2020b. Disentangled graph collaborative filtering. In *Proceedings
    of the 43rd international ACM SIGIR conference on research and development in
    information retrieval*. 1001–1010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, and Dejing
    Dou. 2021. Property-aware relation networks for few-shot molecular property prediction.
    *Advances in Neural Information Processing Systems* 34 (2021), 17441–17454.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022c) Yifan Wang, Yifang Qin, Fang Sun, Bo Zhang, Xuyang Hou,
    Ke Hu, Jia Cheng, Jun Lei, and Ming Zhang. 2022c. DisenCTR: Dynamic Graph-based
    Disentangled Representation for Click-Through Rate Prediction. In *Proceedings
    of the 45th International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 2314–2318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019d) Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M
    Bronstein, and Justin M Solomon. 2019d. Dynamic graph cnn for learning on point
    clouds. *Acm Transactions On Graphics (tog)* 38, 5 (2019), 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022d) Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani.
    2022d. Molecular contrastive learning of representations via graph neural networks.
    *Nature Machine Intelligence* 4, 3 (2022), 279–287.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022b) Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard
    Baraniuk, and Anima Anandkumar. 2022b. Retrieval-based Controllable Molecule Generation.
    *arXiv preprint arXiv:2208.11126* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020d) Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao,
    and Minghui Qiu. 2020d. Global context enhanced graph neural networks for session-based
    recommendation. In *Proceedings of the 43rd international ACM SIGIR conference
    on research and development in information retrieval*. 169–178.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022e) Zhaobo Wang, Yanmin Zhu, Qiaomei Zhang, Haobing Liu, Chunyang
    Wang, and Tong Liu. 2022e. Graph-Enhanced Spatial-Temporal Network for Next POI
    Recommendation. *ACM Transactions on Knowledge Discovery from Data (TKDD)* 16,
    6 (2022), 1–21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2019) Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang
    Hong, and Tat-Seng Chua. 2019. MMGCN: Multi-modal graph convolution network for
    personalized recommendation of micro-video. In *Proceedings of the 27th ACM International
    Conference on Multimedia*. 1437–1445.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weisfeiler and Leman (1968) Boris Weisfeiler and Andrei Leman. 1968. The reduction
    of a graph to canonical form and the algebra which appears therein. *nti, Series*
    2, 9 (1968), 12–16.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Werneck et al. (2020) Heitor Werneck, Nícollas Silva, Matheus Carvalho Viana,
    Fernando Mourão, Adriano CM Pereira, and Leonardo Rocha. 2020. A survey on point-of-interest
    recommendation in location-based social networks. In *Proceedings of the Brazilian
    Symposium on Multimedia and the Web*. 185–192.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wieder et al. (2020) Oliver Wieder, Stefan Kohlbacher, Mélaine Kuenemann, Arthur
    Garon, Pierre Ducrot, Thomas Seidel, and Thierry Langer. 2020. A compact review
    of molecular property prediction with graph neural networks. *Drug Discovery Today:
    Technologies* 37 (2020), 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams and Seeger (2001) Christopher KI Williams and Matthias Seeger. 2001.
    Using the Nyström method to speed up kernel machines. In *Advances in neural information
    processing systems*. 682–688.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Withnall et al. (2020) Michael Withnall, Edvard Lindelöf, Ola Engkvist, and
    Hongming Chen. 2020. Building attention and edge message passing neural networks
    for bioactivity and physical–chemical property prediction. *Journal of cheminformatics*
    12, 1 (2020), 1–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019b) Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao
    Yu, and Kilian Weinberger. 2019b. Simplifying graph convolutional networks. In
    *International conference on machine learning*. PMLR, 6861–6871.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022) Junran Wu, Xueyuan Chen, Ke Xu, and Shangzhe Li. 2022. Structural
    entropy guided graph hierarchical pooling. In *International Conference on Machine
    Learning*. PMLR, 24017–24030.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019c) Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and
    Tieniu Tan. 2019c. Session-based recommendation with graph neural networks. In
    *Proceedings of the AAAI conference on artificial intelligence*, Vol. 33. 346–353.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021) Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini,
    Joseph E Gonzalez, and Ion Stoica. 2021. Representing long-range context for graph
    neural networks with global attention. *Advances in Neural Information Processing
    Systems* 34 (2021), 13266–13279.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi
    Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks.
    *IEEE transactions on neural networks and learning systems* 32, 1 (2020), 4–24.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019a) Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi
    Zhang. 2019a. Graph wavenet for deep spatial-temporal graph modeling. In *Proceedings
    of the 28th International Joint Conference on Artificial Intelligence*. 1907–1913.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2018) Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes,
    Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. MoleculeNet:
    a benchmark for molecular machine learning. *Chemical science* 9, 2 (2018), 513–530.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. (2021) Feng Xia, Ke Sun, Shuo Yu, Abdul Aziz, Liangtian Wan, Shirui
    Pan, and Huan Liu. 2021. Graph learning: A survey. *IEEE Transactions on Artificial
    Intelligence* 2, 2 (2021), 109–127.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. (2022b) Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li.
    2022b. Simgrace: A simple framework for graph contrastive learning without data
    augmentation. In *Proceedings of the ACM Web Conference 2022*. 1070–1079.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xia et al. (2022a) Lianghao Xia, Chao Huang, Yong Xu, Jiashu Zhao, Dawei Yin,
    and Jimmy Huang. 2022a. Hypergraph contrastive collaborative filtering. In *Proceedings
    of the 45th International ACM SIGIR Conference on Research and Development in
    Information Retrieval*. 70–79.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2020) Peng Xie, Tianrui Li, Jia Liu, Shengdong Du, Xin Yang, and
    Junbo Zhang. 2020. Urban flow prediction from spatiotemporal data using machine
    learning: A survey. *Information Fusion* 59 (2020), 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2022a) Yu Xie, Yanfeng Liang, Maoguo Gong, AK Qin, Yew-Soon Ong,
    and Tiantian He. 2022a. Semisupervised Graph Neural Networks for Graph Classification.
    *IEEE Transactions on Cybernetics* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2022b) Yu Xie, Shengze Lv, Yuhua Qian, Chao Wen, and Jiye Liang.
    2022b. Active and Semi-supervised Graph Neural Networks for Graph Classification.
    *IEEE Transactions on Big Data* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2021) Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang,
    Yong Yu, and Lei Li. 2021. Mars: Markov molecular sampling for multi-objective
    drug discovery. *arXiv preprint arXiv:2103.10432* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2022c) Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and
    Shuiwang Ji. 2022c. Self-supervised learning of graph neural networks: A unified
    review. *IEEE transactions on pattern analysis and machine intelligence* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiong et al. (2018) Haoyi Xiong, Amin Vahedian, Xun Zhou, Yanhua Li, and Jun
    Luo. 2018. Predicting traffic congestion propagation patterns: A propagation graph
    approach. In *Proceedings of the 11th ACM SIGSPATIAL International Workshop on
    Computational Transportation Science*. 60–69.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019b) Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng.
    2019b. Graph Wavelet Neural Network. In *International Conference on Learning
    Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2021a) Chonghuan Xu, Austin Shijun Ding, and Kaidi Zhao. 2021a. A
    novel POI recommendation method based on trust relationship and spatial–temporal
    factors. *Electronic Commerce Research and Applications* 48 (2021), 101060.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019a) Chenxiao Xu, Hao Huang, and Shinjae Yoo. 2019a. Scalable causal
    graph learning through a deep neural network. In *Proceedings of the 28th ACM
    international conference on information and knowledge management*. 1853–1862.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2021b) Chonghuan Xu, Dongsheng Liu, and Xinyao Mei. 2021b. Exploring
    an efficient POI recommendation model based on user characteristics and spatial-temporal
    factors. *Mathematics* 9, 21 (2021), 2673.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2018a) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
    2018a. How Powerful are Graph Neural Networks?. In *International Conference on
    Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2018b) Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
    Kawarabayashi, and Stefanie Jegelka. 2018b. Representation learning on graphs
    with jumping knowledge networks. In *International conference on machine learning*.
    PMLR, 5453–5462.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020) Zhenyi Xu, Yu Kang, Yang Cao, and Zhijun Li. 2020. Spatiotemporal
    graph convolution multifusion network for urban vehicle emission prediction. *IEEE
    Transactions on Neural Networks and Learning Systems* 32, 8 (2020), 3342–3354.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2017) Zheng Xu, Sheng Wang, Feiyun Zhu, and Junzhou Huang. 2017.
    Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery.
    In *Proceedings of the 8th ACM international conference on bioinformatics, computational
    biology, and health informatics*. 285–294.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021) Chaoying Yang, Kaibo Zhou, and Jie Liu. 2021. SuperGraph:
    Spatial-temporal graph-based feature extraction for rotating machinery diagnosis.
    *IEEE Transactions on Industrial Electronics* 69, 4 (2021), 4167–4176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022a) Haoran Yang, Hongxu Chen, Shirui Pan, Lin Li, Philip S Yu,
    and Guandong Xu. 2022a. Dual Space Graph Contrastive Learning. In *Proceedings
    of the Web Conference*. 1238–1247.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2019) Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp
    Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea,
    et al. 2019. Analyzing learned molecular representations for property prediction.
    *Journal of chemical information and modeling* 59, 8 (2019), 3370–3388.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2020) Yuanxuan Yang, Alison Heppenstall, Andy Turner, and Alexis
    Comber. 2020. Using graph structural information about flows to enhance short-term
    demand prediction in bike-sharing systems. *Computers, Environment and Urban Systems*
    83 (2020), 101521.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022b) Yuhao Yang, Chao Huang, Lianghao Xia, Yuxuan Liang, Yanwei
    Yu, and Chenliang Li. 2022b. Multi-Behavior Hypergraph-Enhanced Transformer for
    Sequential Recommendation. In *Proceedings of the 28th ACM SIGKDD Conference on
    Knowledge Discovery and Data Mining*. 2263–2274.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. (2018) Huaxiu Yao, Fei Wu, Jintao Ke, Xianfeng Tang, Yitian Jia,
    Siyu Lu, Pinghua Gong, Jieping Ye, and Zhenhui Li. 2018. Deep multi-view spatial-temporal
    network for taxi demand prediction. In *Proceedings of the AAAI conference on
    artificial intelligence*, Vol. 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. (2020) Shaowei Yao, Tianming Wang, and Xiaojun Wan. 2020. Heterogeneous
    graph transformer for graph-to-sequence learning. In *Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics*. 7145–7154.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yildirimoglu and Kim (2018) Mehmet Yildirimoglu and Jiwon Kim. 2018. Identification
    of communities in urban mobility networks using multi-layer graphs of network
    traffic. *Transportation Research Part C: Emerging Technologies* 89 (2018), 254–267.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ying et al. (2021) Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin
    Ke, Di He, Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform
    badly for graph representation? *Advances in Neural Information Processing Systems*
    34 (2021), 28877–28888.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ying et al. (2019) Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik,
    and Jure Leskovec. 2019. Gnnexplainer: Generating explanations for graph neural
    networks. In *Proceedings of the Conference on Neural Information Processing Systems*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ying et al. (2018) Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren,
    Will Hamilton, and Jure Leskovec. 2018. Hierarchical graph representation learning
    with differentiable pooling. *Advances in neural information processing systems*
    31 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You et al. (2018) Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure
    Leskovec. 2018. Graph convolutional policy network for goal-directed molecular
    graph generation. *Advances in neural information processing systems* 31 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You et al. (2020) Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen.
    2020. When does self-supervision help graph convolutional networks?. In *international
    conference on machine learning*. PMLR, 10871–10880.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2017) Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2017. Spatio-temporal
    graph convolutional networks: A deep learning framework for traffic forecasting.
    *arXiv preprint arXiv:1709.04875* (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2021b) Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and
    Yiming Yang. 2021b. Graph-revised convolutional network. In *Machine Learning
    and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent,
    Belgium, September 14–18, 2020, Proceedings, Part III*. Springer, 378–393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2022) Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and
    Quoc Viet Hung Nguyen. 2022. Are graph augmentations necessary? simple graph contrastive
    learning for recommendation. In *Proceedings of the 45th International ACM SIGIR
    Conference on Research and Development in Information Retrieval*. 1294–1303.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2021a) Le Yu, Bowen Du, Xiao Hu, Leilei Sun, Liangzhe Han, and Weifeng
    Lv. 2021a. Deep spatio-temporal graph convolutional network for traffic accident
    prediction. *Neurocomputing* 423 (2021), 135–147.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Shuo Yu, Jiaying Liu, Zhuo Yang, Zhen Chen, Huizhen Jiang,
    Amr Tolba, and Feng Xia. 2018. PAVE: Personalized Academic Venue recommendation
    Exploiting co-publication networks. *Journal of Network and Computer Applications*
    104 (2018), 38–47.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2012) Xiao Yu, Quanquan Gu, Mianwei Zhou, and Jiawei Han. 2012. Citation
    prediction in heterogeneous bibliographic networks. In *Proceedings of the 2012
    SIAM international conference on data mining*. SIAM, 1119–1130.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu and Gao (2022) Zhaoning Yu and Hongyang Gao. 2022. Molecular representation
    learning via heterogeneous motif graph neural networks. In *International Conference
    on Machine Learning*. PMLR, 25581–25594.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2019) Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. 2019.
    Deep modular co-attention networks for visual question answering. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*. 6281–6290.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yue et al. (2022) Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu. 2022.
    Label-invariant Augmentation for Semi-Supervised Graph Classification. In *Proceedings
    of the Conference on Neural Information Processing Systems*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zang and Wang (2020) Chengxi Zang and Fei Wang. 2020. MoFlow: an invertible
    flow model for generating molecular graphs. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 617–626.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019b) Chen Zhang, Qiuchi Li, and Dawei Song. 2019b. Aspect-based
    Sentiment Classification with Aspect-specific Graph Convolutional Networks. In
    *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
    and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*.
    4568–4578.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022a) Cai Zhang, Weimin Li, Dingmei Wei, Yanxia Liu, and Zheng
    Li. 2022a. Network dynamic GCN influence maximization algorithm with leader fake
    labeling mechanism. *IEEE Transactions on Computational Social Systems* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019d) Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami,
    and Nitesh V Chawla. 2019d. Heterogeneous graph neural network. In *Proceedings
    of the 25th ACM SIGKDD international conference on knowledge discovery & data
    mining*. 793–803.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2016) Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie,
    and Wei-Ying Ma. 2016. Collaborative knowledge base embedding for recommender
    systems. In *Proceedings of the 22nd ACM SIGKDD international conference on knowledge
    discovery and data mining*. 353–362.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022b) Hengyuan Zhang, Suyao Zhao, Ruiheng Liu, Wenlong Wang,
    Yixin Hong, and Runjiu Hu. 2022b. Automatic traffic anomaly detection on the road
    network with spatial-temporal graph neural network representation learning. *Wireless
    Communications and Mobile Computing* 2022 (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018b) Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin
    King, and Dit-Yan Yeung. 2018b. Gaan: Gated attention networks for learning on
    large and spatiotemporal graphs. *arXiv preprint arXiv:1803.07294* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020b) Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun.
    2020b. Graph-bert: Only attention is needed for learning graph representations.
    *arXiv preprint arXiv:2001.05140* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018a) Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen.
    2018a. An end-to-end deep learning architecture for graph classification. In *Proceedings
    of the AAAI conference on artificial intelligence*, Vol. 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Li (2021) Muhan Zhang and Pan Li. 2021. Nested graph neural networks.
    *Advances in Neural Information Processing Systems* 34 (2021), 15734–15747.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018d) Xiaoyu Zhang, Sheng Wang, Feiyun Zhu, Zheng Xu, Yuhong
    Wang, and Junzhou Huang. 2018d. Seq3seq fingerprint: towards end-to-end semi-supervised
    deep drug discovery. In *Proceedings of the 2018 ACM International Conference
    on Bioinformatics, Computational Biology, and Health Informatics*. 404–413.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Zitnik (2020) Xiang Zhang and Marinka Zitnik. 2020. Gnnguard: Defending
    graph neural networks against adversarial attacks. *Advances in neural information
    processing systems* 33 (2020), 9263–9275.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2014) Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu,
    and Shaoping Ma. 2014. Explicit factor models for explainable recommendation based
    on phrase-level sentiment analysis. In *Proceedings of the 37th international
    ACM SIGIR conference on Research & development in information retrieval*. 83–92.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019c) Yingxue Zhang, Soumyasundar Pal, Mark Coates, and Deniz
    Ustebay. 2019c. Bayesian graph convolutional neural networks for semi-supervised
    classification. In *Proceedings of the AAAI conference on artificial intelligence*,
    Vol. 33. 5829–5836.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021b) Yu Zhang, Peter Tiňo, Aleš Leonardis, and Ke Tang. 2021b.
    A survey on neural network interpretability. *IEEE Transactions on Emerging Topics
    in Computational Intelligence* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018e) Yutao Zhang, Fanjin Zhang, Peiran Yao, and Jie Tang. 2018e.
    Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop..
    In *Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery
    & data mining*. 1002–1011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019a) Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei
    Yao, Zhi Yu, and Can Wang. 2019a. Hierarchical graph pooling with structure learning.
    *arXiv preprint arXiv:1911.05954* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020a) Ziwei Zhang, Peng Cui, and Wenwu Zhu. 2020a. Deep learning
    on graphs: A survey. *IEEE Transactions on Knowledge and Data Engineering* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021a) Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong
    Lee. 2021a. Motif-based graph self-supervised learning for molecular property
    prediction. *Advances in Neural Information Processing Systems* 34 (2021), 15870–15882.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZHANG et al. ([n. d.]) ZAIXI ZHANG, Yaosen Min, Shuxin Zheng, and Qi Liu. [n. d.].
    Molecule Generation For Target Protein Binding with Structural Motifs. In *The
    Eleventh International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018c) Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, and
    Arye Nehorai. 2018c. Retgk: Graph kernels based on return probabilities of random
    walks. In *Advances in Neural Information Processing Systems*. 3964–3974.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2021a) Jialin Zhao, Yuxiao Dong, Ming Ding, Evgeny Kharlamov, and
    Jie Tang. 2021a. Adaptive diffusion in graph neural networks. *Advances in Neural
    Information Processing Systems* 34 (2021), 23321–23333.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2021b) Jianan Zhao, Xiao Wang, Chuan Shi, Binbin Hu, Guojie Song,
    and Yanfang Ye. 2021b. Heterogeneous graph structure learning for graph neural
    networks. In *Proceedings of the AAAI conference on artificial intelligence*,
    Vol. 35. 4697–4705.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2022b) Lingxiao Zhao, Saurabh Sawlani, Arvind Srinivasan, and Leman
    Akoglu. 2022b. Graph Anomaly Detection with Unsupervised GNNs. *arXiv preprint
    arXiv:2210.09535* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020) Pengpeng Zhao, Anjing Luo, Yanchi Liu, Fuzhen Zhuang, Jiajie
    Xu, Zhixu Li, Victor S Sheng, and Xiaofang Zhou. 2020. Where to go next: A spatio-temporal
    gated network for next poi recommendation. *IEEE Transactions on Knowledge and
    Data Engineering* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021c) Tianxiang Zhao, Xiang Zhang, and Suhang Wang. 2021c. Graphsmote:
    Imbalanced node classification on graphs with graph neural networks. In *Proceedings
    of the 14th ACM international conference on web search and data mining*. 833–841.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2022a) Yusheng Zhao, Jinyu Chen, Chen Gao, Wenguan Wang, Lirong
    Yang, Haibing Ren, Huaxia Xia, and Si Liu. 2022a. Target-Driven Structured Transformer
    Planner for Vision-Language Navigation. In *Proceedings of the 30th ACM International
    Conference on Multimedia*. 4194–4203.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018) Zhongying Zhao, Wenqiang Liu, Yuhua Qian, Liqiang Nie, Yilong
    Yin, and Yong Zhang. 2018. Identifying advisor-advisee relationships from co-author
    networks via a novel deep model. *Information Sciences* 466 (2018), 258–269.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2020) Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao
    Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. 2020. Robust graph representation
    learning via neural sparsification. In *International Conference on Machine Learning*.
    PMLR, 11458–11468.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2019) Li Zheng, Zhenpeng Li, Jian Li, Zhao Li, and Jun Gao. 2019.
    AddGraph: Anomaly Detection in Dynamic Graph Using Attention-based Temporal GCN..
    In *IJCAI*. 4419–4425.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2021) Yang Zheng, Xiaoyi Feng, Zhaoqiang Xia, Xiaoyue Jiang, Ambra
    Demontis, Maura Pintor, Battista Biggio, and Fabio Roli. 2021. Why Adversarial
    Reprogramming Works, When It Fails, and How to Tell the Difference. *arXiv preprint
    arXiv:2108.11673* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2006) Dengyong Zhou, Jiayuan Huang, and Bernhard Schölkopf. 2006.
    Learning with hypergraphs: Clustering, classification, and embedding. *Advances
    in neural information processing systems* 19 (2006).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022a) Fan Zhou, Rongfan Li, Qiang Gao, Goce Trajcevski, Kunpeng
    Zhang, and Ting Zhong. 2022a. Dynamic Manifold Learning for Land Deformation Forecasting.
    (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2021) Hao Zhou, Dongchun Ren, Huaxia Xia, Mingyu Fan, Xu Yang,
    and Hai Huang. 2021. AST-GNN: An attention-based spatio-temporal graph neural
    network for Interaction-aware pedestrian trajectory prediction. *Neurocomputing*
    445 (2021), 298–308.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2020) Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng
    Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural
    networks: A review of methods and applications. *AI Open* 1 (2020), 57–81.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2022b) Yu Zhou, Haixia Zheng, Xin Huang, Shufeng Hao, Dengao Li,
    and Jumin Zhao. 2022b. Graph Neural Networks: Taxonomy, Advances, and Trends.
    *ACM Transactions on Intelligent Systems and Technology (TIST)* 13, 1 (2022),
    1–54.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2019) Ziang Zhou, Shenzhong Zhang, and Zengfeng Huang. 2019. Dynamic
    self-training framework for graph convolutional networks. (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu and Koniusz (2021) Hao Zhu and Piotr Koniusz. 2021. Simple spectral graph
    convolution. In *International Conference on Learning Representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2022a) Jinhua Zhu, Yingce Xia, Chang Liu, Lijun Wu, Shufang Xie,
    Tong Wang, Yusong Wang, Wengang Zhou, Tao Qin, Houqiang Li, et al. 2022a. Direct
    Molecular Conformation Generation. *arXiv preprint arXiv:2202.01356* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2022b) Jinhua Zhu, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Wengang
    Zhou, Houqiang Li, and Tie-Yan Liu. 2022b. Unified 2d and 3d pre-training of molecular
    representations. In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge
    Discovery and Data Mining*. 2626–2636.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020a) Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
    and Jifeng Dai. 2020a. Deformable detr: Deformable transformers for end-to-end
    object detection. *arXiv preprint arXiv:2010.04159* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2020b) Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang
    Wang. 2020b. Deep graph contrastive representation learning. *arXiv preprint arXiv:2006.04131*
    (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2021) Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang
    Wang. 2021. Graph contrastive learning with adaptive augmentation. In *Proceedings
    of the Web Conference 2021*. 2069–2080.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020c) Yanqiao Zhu, Yichen Xu, Feng Yu, Shu Wu, and Liang Wang.
    2020c. CAGNN: Cluster-aware graph neural networks for unsupervised graph representation
    learning. *arXiv preprint arXiv:2009.01674* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu (2022) Zhenyi Zhu. 2022. A Survey of GNN in Bioinformation Data. (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Łukasz Maziarka et al. (2020) Łukasz Maziarka, Tomasz Danel, Sławomir Mucha,
    Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrzębski. 2020. Molecule Attention
    Transformer. (2020). arXiv:2002.08264 [cs.LG]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
