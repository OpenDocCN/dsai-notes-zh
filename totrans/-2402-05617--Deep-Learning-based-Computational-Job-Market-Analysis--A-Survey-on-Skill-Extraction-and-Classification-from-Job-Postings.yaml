- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:34:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:34:34'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2402.05617] Deep Learning-based Computational Job Market Analysis: A Survey
    on Skill Extraction and Classification from Job Postings'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2402.05617] 基于深度学习的计算工作市场分析：关于从职位发布中提取和分类技能的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.05617](https://ar5iv.labs.arxiv.org/html/2402.05617)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.05617](https://ar5iv.labs.arxiv.org/html/2402.05617)
- en: 'Deep Learning-based Computational Job Market Analysis:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的计算工作市场分析：
- en: A Survey on Skill Extraction and Classification from Job Postings
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 关于从职位发布中提取和分类技能的调查
- en: Elena Senger^(1,3), Mike Zhang², Rob van der Goot², Barbara Plank^(1,2)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Elena Senger^(1,3), Mike Zhang², Rob van der Goot², Barbara Plank^(1,2)
- en: ¹MaiNLP, Center for Information and Language Processing, LMU Munich, Germany
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹MaiNLP, 信息与语言处理中心, 慕尼黑大学, 德国
- en: ²Department of Computer Science, IT University of Copenhagen, Denmark
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ²哥本哈根IT大学计算机科学系, 丹麦
- en: ³Fraunhofer Center for International Management and Knowledge Economy IMW, Germany
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ³弗劳恩霍夫国际管理与知识经济中心 IMW, 德国
- en: elena.senger@cis.lmu.de, {mikz, robv}@itu.dk, b.plank@lmu.de
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: elena.senger@cis.lmu.de, {mikz, robv}@itu.dk, b.plank@lmu.de
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent years have brought significant advances to Natural Language Processing
    (NLP), which enabled fast progress in the field of *computational job market analysis*.
    Core tasks in this application domain are *skill extraction and classification*
    from job postings. Because of its quick growth and its interdisciplinary nature,
    there is no exhaustive assessment of this emerging field. This survey aims to
    fill this gap by providing a comprehensive overview of deep learning methodologies,
    datasets, and terminologies specific to NLP-driven skill extraction and classification.
    Our comprehensive cataloging of publicly available datasets addresses the lack
    of consolidated information on dataset creation and characteristics. Finally,
    the focus on terminology addresses the current lack of consistent definitions
    for important concepts, such as hard and soft skills, and terms relating to skill
    extraction and classification.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，自然语言处理（NLP）取得了显著进展，使得*计算工作市场分析*领域得以快速发展。这个应用领域的核心任务是*从职位发布中提取和分类技能*。由于其快速增长和跨学科性质，目前尚无对这一新兴领域的全面评估。本次调查旨在通过提供关于NLP驱动的技能提取和分类的深度学习方法、数据集和术语的全面概述来填补这一空白。我们对公开可用数据集的全面目录弥补了有关数据集创建和特征的整合信息缺乏的问题。最后，对术语的关注解决了当前在硬技能和软技能等重要概念，以及与技能提取和分类相关术语的定义不一致的问题。
- en: 'Deep Learning-based Computational Job Market Analysis:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的计算工作市场分析：
- en: A Survey on Skill Extraction and Classification from Job Postings
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于从职位发布中提取和分类技能的调查
- en: Elena Senger^(1,3), Mike Zhang², Rob van der Goot², Barbara Plank^(1,2) ¹MaiNLP,
    Center for Information and Language Processing, LMU Munich, Germany ²Department
    of Computer Science, IT University of Copenhagen, Denmark ³Fraunhofer Center for
    International Management and Knowledge Economy IMW, Germany elena.senger@cis.lmu.de,
    {mikz, robv}@itu.dk, b.plank@lmu.de
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Elena Senger^(1,3), Mike Zhang², Rob van der Goot², Barbara Plank^(1,2) ¹MaiNLP,
    信息与语言处理中心, 慕尼黑大学, 德国 ²哥本哈根IT大学计算机科学系, 丹麦 ³弗劳恩霍夫国际管理与知识经济中心 IMW, 德国 elena.senger@cis.lmu.de,
    {mikz, robv}@itu.dk, b.plank@lmu.de
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Skill extraction and classification has recently been the subject of an increased
    amount of interest (Zhang et al., [2023](#bib.bib64); Clavié and Soulié, [2023](#bib.bib12)),
    which shows in a high number of publications, driven by the advances in natural
    language processing (NLP) technology. For instance, through large language models
    (LLMs) the low resource tasks of skill extraction can be approached by using synthetic
    training data (Clavié and Soulié, [2023](#bib.bib12); Decorte et al., [2023](#bib.bib17)).
    Surveys regarding skill extraction are emerging (Khaouja et al., [2021a](#bib.bib34);
    Papoutsoglou et al., [2019](#bib.bib47)), nevertheless, a comprehensive overview
    from an NLP perspective is still lacking—a gap we aim to fill in this survey.
    Our contributions are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 技能提取和分类最近受到越来越多的关注（Zhang et al., [2023](#bib.bib64); Clavié 和 Soulié, [2023](#bib.bib12)），这在大量的出版物中得到了体现，主要是由于自然语言处理（NLP）技术的进步。例如，通过大语言模型（LLMs），可以通过使用合成训练数据来处理低资源任务的技能提取（Clavié
    和 Soulié, [2023](#bib.bib12); Decorte et al., [2023](#bib.bib17)）。关于技能提取的调查正在出现（Khaouja
    et al., [2021a](#bib.bib34); Papoutsoglou et al., [2019](#bib.bib47)），尽管如此，从NLP的角度出发，仍然缺乏全面的概述——这是我们希望在本次调查中填补的空白。我们的贡献包括：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Firstly, we aim to address the lack of standardized terminology in the field,
    bringing clarity to terms like hard and soft skills, as well as phrases related
    to skill extraction and classification.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们旨在解决该领域中标准术语缺乏的问题，为硬技能和软技能等术语以及与技能提取和分类相关的短语带来清晰度。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Additionally, this survey is the first to examine various publicly accessible
    datasets and sheds light on their creation methodologies.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，本次调查首次检查了各种公开可访问的数据集，并揭示了它们的创建方法。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In contrast to prior surveys, we adopt an NLP-centric focus, with a deep dive
    into the latest advancements of neural methods for skill extraction and classification.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与以往的调查相比，我们采用了以 NLP 为中心的重点，深入探讨了神经方法在技能提取和分类方面的最新进展。
- en: While prior surveys exists, they focus typically on *Skill count* and *Topic
    modeling* methods for extracting skills. Skill count is performed manually or
    by matching n-grams with a skill base. Topic modeling is an unsupervised method
    utilizing word distributions to identify underlying topics in documents. Due to
    primary statistical basis and lack of defined skill spans or labels, topic modeling,
    as well as skill count, methods are not covered in this survey. For further details
    on skill count, see Khaouja et al. ([2021a](#bib.bib34)) and Ternikov ([2022](#bib.bib55)),
    and for topic modeling, please refer to Khaouja et al. ([2021a](#bib.bib34)),
    Ternikov ([2022](#bib.bib55)) and Ao et al. ([2023](#bib.bib2)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然已有先前的调查存在，但它们通常关注于*技能计数*和*主题建模*方法来提取技能。技能计数是通过手动操作或将 n-grams 与技能库匹配来进行的。主题建模是一种无监督方法，利用词分布来识别文档中的潜在主题。由于主要基于统计，并且缺乏定义的技能范围或标签，主题建模以及技能计数方法在本次调查中未被涵盖。有关技能计数的更多细节，请参阅
    Khaouja 等人 ([2021a](#bib.bib34)) 和 Ternikov ([2022](#bib.bib55))，有关主题建模，请参考 Khaouja
    等人 ([2021a](#bib.bib34))、Ternikov ([2022](#bib.bib55)) 和 Ao 等人 ([2023](#bib.bib2))。
- en: Research Methodology
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 研究方法论
- en: For our search strategy we used several academic databases including the ACL
    Anthology, Google Scholar, arXiv, IEEE, ACM, Science Direct, and Springer Link.
    The primary search terms were “skill extraction” and “job”. To refine the search,
    we added terms like “deep learning”, “machine learning”, or “natural language
    processing” to our query for Google Scholar and Science Direct databases. This
    yielded the inclusion of 26 publications on neural skill extraction from job postings
    (JPs) that were published before November 2023.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的搜索策略，我们使用了多个学术数据库，包括 ACL Anthology、Google Scholar、arXiv、IEEE、ACM、Science
    Direct 和 Springer Link。主要搜索词是“skill extraction”和“job”。为了进一步优化搜索，我们在 Google Scholar
    和 Science Direct 数据库中添加了“deep learning”、“machine learning”或“natural language processing”等术语。这使我们纳入了26篇关于从职位发布（JPs）中提取神经技能的文献，这些文献均在2023年11月之前发表。
- en: 2 Other Surveys
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 其他调查
- en: Previous surveys provide a foundation for our survey. Notable contributions
    include works from the social sciences, in particular, by Napierala and Kvetan
    ([2023](#bib.bib46)) in the “Handbook of Computational Social Science for Policy”
    (Chapter 13). It focuses on changing skills in a dynamic world from a social science
    perspective. Moreover, Papoutsoglou et al. ([2019](#bib.bib47)) focus on studies
    regarding the software engineering labor market. Besides JPs, they research other
    sources like social networks or Q&A sites. Lastly, the survey by Khaouja et al.
    ([2021a](#bib.bib34)) on skill identification from JPs is the closest to this
    survey. It overviews papers using methodologies such as skill counts, topic modeling,
    skill embeddings, and other machine learning-based methods. With this survey,
    we steer away from manual and topic modeling approaches to delve deeply into recent
    extraction methodologies and deep learning-based innovations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的调查为我们的调查提供了基础。值得注意的贡献包括来自社会科学领域的研究，特别是 Napierala 和 Kvetan ([2023](#bib.bib46))
    在《政策计算社会科学手册》（第13章）中的工作。它从社会科学的角度关注动态世界中的技能变化。此外，Papoutsoglou 等人 ([2019](#bib.bib47))
    关注于软件工程劳动力市场的研究。除了 JPs，他们还研究了其他来源，如社交网络或问答网站。最后，Khaouja 等人 ([2021a](#bib.bib34))
    关于从 JPs 中识别技能的调查最接近本次调查。它综述了使用技能计数、主题建模、技能嵌入和其他基于机器学习的方法的论文。在本次调查中，我们避免了手动和主题建模方法，深入探讨了最近的提取方法和基于深度学习的创新。
- en: 3 Skill-related Terminology
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 技能相关术语
- en: 'The terms skill extraction, identification (Li et al., [2023](#bib.bib39)),
    detection (Beauchemin et al., [2022](#bib.bib3)), standardization (Li et al.,
    [2023](#bib.bib39)) and classification are used differently, sometimes interchangeably,
    and describe the same or different tasks. We provide the following definition
    (See an example in Table [3](#A1.T3 "Table 3 ‣ A.1 Terminology Example ‣ Appendix
    A Appendix ‣ Deep Learning-based Computational Job Market Analysis: A Survey on
    Skill Extraction and Classification from Job Postings") in the Appendix):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '术语技能提取、识别（Li et al., [2023](#bib.bib39)）、检测（Beauchemin et al., [2022](#bib.bib3)）、标准化（Li
    et al., [2023](#bib.bib39)）和分类使用方式不同，有时可以互换，描述相同或不同的任务。我们提供以下定义（参见附录中表[3](#A1.T3
    "Table 3 ‣ A.1 Terminology Example ‣ Appendix A Appendix ‣ Deep Learning-based
    Computational Job Market Analysis: A Survey on Skill Extraction and Classification
    from Job Postings)中的示例）：'
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Skill Extraction ($E$): as a generic (parent) category for retrieving skill-related
    information. Skill extraction $E:\text{JP}\rightarrow(S)$, where $E$ maps a job
    posting (JP) to a set of skills $S$.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技能提取（$E$）：作为检索技能相关信息的通用（父类）类别。技能提取$E:\text{JP}\rightarrow(S)$，其中$E$将职位发布（JP）映射到一组技能$S$。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Skill Identification/Detection ($I$): as the process of extracting skills without
    any pre-defined labels. It can be represented as $I:\text{JP}\rightarrow S$, where
    skills, especially skill spans, are extracted from JPs. It can also be formalized
    as a classification problem, $I:\text{Span}\rightarrow\{0,1\}$, to determine whether
    a given span in a JP represents a skill (1) or not (0).'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技能识别/检测（$I$）：指在没有任何预定义标签的情况下提取技能的过程。它可以表示为$I:\text{JP}\rightarrow S$，其中技能，特别是技能跨度，从JPs中提取。它也可以形式化为一个分类问题，即$I:\text{Span}\rightarrow\{0,1\}$，以确定给定的JP跨度是否表示技能（1）或不表示技能（0）。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Skill Extraction with Coarse Labels ($E_{C}$): as identifying broader categories
    of skill spans. It is formalized as $E_{C}:\text{JP}\rightarrow\{SC_{1},SC_{2},\dots,SC_{n}\}$,
    where each $SC_{i}$ represents a skill span with a coarse label.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 粗标签技能提取（$E_{C}$）：指识别技能跨度的更广泛类别。它被形式化为$E_{C}:\text{JP}\rightarrow\{SC_{1},SC_{2},\dots,SC_{n}\}$，其中每个$SC_{i}$代表一个具有粗略标签的技能跨度。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Skill Standardization ($Std$): as the normalization process of skill terms,
    formalized as $Std:S\rightarrow S^{\prime}$, mapping an initial set of skills
    $S$ to a standardized set $S^{\prime}$.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技能标准化（$Std$）：作为技能术语的规范化过程，形式化为$Std:S\rightarrow S^{\prime}$，将初始技能集$S$映射到标准化的技能集$S^{\prime}$。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Direct Skill Classification ($C_{D}$): as mapping skills to a predefined skill
    base for assigning fine-grained labels. This process can be formalized as $C_{D}:S\rightarrow
    L$, where $C_{D}$ maps a set of already extracted skills $S$ to a set of fine-grained
    labels $L$.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接技能分类（$C_{D}$）：作为将技能映射到预定义技能库以分配细粒度标签的过程。这个过程可以形式化为$C_{D}:S\rightarrow L$，其中$C_{D}$将一组已经提取的技能$S$映射到一组细粒度标签$L$。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Skill Classification with Extraction ($C_{E}$): as mapping JPs to a predefined
    skill base for assigning fine-grained labels. This process can be formalized as
    $C_{E}:JP\rightarrow L$, where $C_{E}$ maps a set of already extracted skills
    $S$ entire JP or raw JP snippets to a set of fine-grained labels $L$.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技能分类与提取（$C_{E}$）：作为将JPs映射到预定义技能库以分配细粒度标签的过程。这个过程可以形式化为$C_{E}:JP\rightarrow L$，其中$C_{E}$将一组已经提取的技能$S$整个JP或原始JP片段映射到一组细粒度标签$L$。
- en: Given these definitions, the skill extraction step can happen at different levels
    of granularity (of the input). Some works extract skills per JP ($E_{JP}$, the
    overall document), per sentence ($E_{sentence}$) or per n-gram ($E_{n-gram}$).
    A skill span ($E_{span}$)is a continuous n-gram sequence that capture a skill.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些定义，技能提取步骤可以在不同的粒度级别上发生（输入的粒度）。一些工作在每个JP（$E_{JP}$，整体文档）、每个句子（$E_{sentence}$）或每个n-gram（$E_{n-gram}$）上提取技能。技能跨度（$E_{span}$）是一个连续的n-gram序列，捕捉一个技能。
- en: A skill base ($B$) is a knowledge base containing skill entities and terminology.
    A taxonomy is a hierarchically structured skill base, while ontologies provide
    a structure via relationships between concepts (Khaouja et al., [2021a](#bib.bib34)).
    Several works use the term “skill dictionary” for a skill base, most often referring
    to an unstructured skill base or a list of skills (Gugnani and Misra, [2020](#bib.bib27);
    Yao et al., [2022](#bib.bib60)). Two popular publicly-available skill bases, created
    by domain experts, and are frequently used and maintained are the European Skills,
    Competences, Qualifications and Occupations (ESCO; le Vrang et al., [2014](#bib.bib38))
    taxonomy and the US Occupational Information Network (O*NET; Council et al., [2010](#bib.bib15)).
    We refer to Khaouja et al. ([2021a](#bib.bib34)) for more examples of skill bases.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 技能库（$B$）是一个包含技能实体和术语的知识库。分类学是一个分层结构的技能库，而本体论通过概念之间的关系提供结构（Khaouja et al., [2021a](#bib.bib34)）。一些工作使用“技能词典”一词来指代技能库，这通常指的是一个非结构化的技能库或技能列表（Gugnani
    and Misra, [2020](#bib.bib27); Yao et al., [2022](#bib.bib60)）。两个由领域专家创建的流行的公开技能库，且经常使用和维护的是欧洲技能、能力、资格和职业（ESCO；le
    Vrang et al., [2014](#bib.bib38)）分类学和美国职业信息网络（O*NET；Council et al., [2010](#bib.bib15)）。更多技能库的例子请参考Khaouja
    et al. ([2021a](#bib.bib34))。
- en: 4 What are Skills? On Skill Definitions
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 什么是技能？关于技能定义
- en: Understanding the concept of a *skill* is pivotal in the field of skill extraction.
    In this section, we investigate several definitions of skills by various publications
    and institutions, aiming to identify commonalities and distinctions across different
    sources, which is crucial for establishing a common ground in this emerging field.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 理解*技能*的概念在技能提取领域至关重要。在这一部分，我们调查了各种出版物和机构对技能的多种定义，旨在识别不同来源之间的共性和差异，这对于在这个新兴领域建立共同基础非常重要。
- en: 'The concept of *skill* can be seen as one broad concept (Green et al., [2022](#bib.bib25);
    Wild et al., [2021](#bib.bib59); Fang et al., [2023](#bib.bib19)) or split into
    subclasses, with multiple possibilities for the split. In the latest version of
    the ESCO taxonomy the “skill pillar” is divided into four categories: “Transversal
    skills”, “Skills”,“ Knowledge” and “Language skills and knowledge”.¹¹1[https://esco.ec.europa.eu/en/classification/skill_main](https://esco.ec.europa.eu/en/classification/skill_main)
    O*NET is structured in six domains (Council et al., [2010](#bib.bib15)), the domain
    most fitting for skill extraction from JP is “Worker Requirements”. This domain
    entails four subcategories: basic skills, cross-functional skills, knowledge,
    and education.²²2 [https://www.onetcenter.org/content.html](https://www.onetcenter.org/content.html)
    But publications considered in this survey that define skills, mainly distinguish
    between hard and soft skills (Tamburri et al., [2020](#bib.bib54); Beauchemin
    et al., [2022](#bib.bib3); Sayfullina et al., [2018](#bib.bib50)), which is therefore
    also the separation used in this survey.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*技能*的概念可以被视为一个广泛的概念（Green et al., [2022](#bib.bib25); Wild et al., [2021](#bib.bib59);
    Fang et al., [2023](#bib.bib19)），也可以分为子类别，分割的可能性有多种。在最新版本的ESCO分类学中，“技能支柱”被分为四个类别：“横向技能”、“技能”、“知识”和“语言技能与知识”。¹¹1
    [https://esco.ec.europa.eu/en/classification/skill_main](https://esco.ec.europa.eu/en/classification/skill_main)
    O*NET 结构为六个领域（Council et al., [2010](#bib.bib15)），最适合从JP中提取技能的领域是“工人要求”。该领域包括四个子类别：基本技能、跨职能技能、知识和教育。²²2
    [https://www.onetcenter.org/content.html](https://www.onetcenter.org/content.html)
    但在本次调查中考虑的定义技能的出版物主要区分硬技能和软技能（Tamburri et al., [2020](#bib.bib54); Beauchemin
    et al., [2022](#bib.bib3); Sayfullina et al., [2018](#bib.bib50)），因此本次调查也使用这一划分。'
- en: Hard Skills
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 硬技能
- en: Tamburri et al. ([2020](#bib.bib54)) delineate hard skills as professional competencies,
    activities, or knowledge pertinent to organizational functions, processes, and
    roles, essential for the successful completion of specific tasks. This definition
    emphasizes the practicality and functionality of hard skills within a professional
    setting. Aligning with this, the study by Beauchemin et al. ([2022](#bib.bib3))
    views hard skills as task-oriented technical competencies, drawing upon Lyu and
    Liu ([2021](#bib.bib43)) to define them as formal technical abilities for performing
    certain tasks. Furthermore, Gugnani and Misra ([2020](#bib.bib27)) expand on this
    perspective by incorporating technological terminologies for skill identification
    and therefore integrating knowledge as a fundamental component of hard skills.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Tamburri 等人（[2020](#bib.bib54)）将硬技能定义为与组织职能、流程和角色相关的专业能力、活动或知识，这些都是成功完成特定任务所必需的。这个定义强调了硬技能在专业环境中的实用性和功能性。与此一致，Beauchemin
    等人（[2022](#bib.bib3)）的研究将硬技能视为面向任务的技术能力，借鉴了 Lyu 和 Liu（[2021](#bib.bib43)）的定义，将其定义为执行特定任务的正式技术能力。此外，Gugnani
    和 Misra（[2020](#bib.bib27)）通过引入技术术语来识别技能，扩展了这一观点，因此将知识整合为硬技能的一个基本组成部分。
- en: By incorporating knowledge as a component of hard skills, the definitions of
    hard skills and knowledge categories of O*NET and ESCO can be combined. O*NET’s
    definition of hard skills states that they are developed abilities that enable
    learning or knowledge acquisition, coupled with their definition of knowledge
    as “Organized sets of principles and facts applying in general domains”.³³3See
    footnote 2. This comprehensive definition underscores not only technical proficiency
    but also the ability to adapt and apply knowledge. Similarly, ESCO, referencing
    the European Qualifications Framework, defines skills as “the ability to apply
    knowledge and use know-how to complete tasks and solve problems”, while defining
    knowledge as “the outcome of the assimilation of information through learning”.⁴⁴4[https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/knowledge](https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/knowledge)
    and [https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/skill](https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/skill)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将知识作为硬技能的一个组成部分，硬技能的定义与 O*NET 和 ESCO 的知识类别可以结合起来。O*NET 对硬技能的定义表示，它们是使学习或知识获取成为可能的能力，并与其对知识的定义“组织原则和事实的集合，适用于一般领域”相结合。³³3见脚注2。这个全面的定义不仅强调了技术熟练度，还强调了适应和应用知识的能力。同样，ESCO
    参考了欧洲资格框架，将技能定义为“应用知识和使用技能以完成任务和解决问题的能力”，同时将知识定义为“通过学习吸收信息的结果”。⁴⁴4[https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/knowledge](https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/knowledge)
    和 [https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/skill](https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/skill)
- en: In conclusion, we define hard skills as a wide variety of professional abilities,
    ranging from measurable technical skills to the more general capacity for learning
    and effectively applying knowledge. They are quantifiable and teachable competencies,
    predominantly technical, yet intrinsically linked to the ability to adapt and
    apply them in diverse professional scenarios.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们将硬技能定义为各种专业能力，从可测量的技术技能到更一般的学习和有效应用知识的能力。它们是可量化和可教授的能力，主要是技术性的，但与适应和在不同专业场景中应用它们的能力密切相关。
- en: Soft Skills
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 软技能
- en: Sayfullina et al. ([2018](#bib.bib50)), referencing the Collins dictionary (HarperCollins
    Publishers, [2023](#bib.bib29)), views soft skills as innate, non-technical qualities
    highly sought after in employment, diverging from reliance on acquired knowledge.
    In a more social context, Tamburri et al. ([2020](#bib.bib54)) characterizes soft
    skills as encompassing personal, emotional, social, or intellectual aspects, further
    known as behavioral skills or competencies. Echoing this sentiment, Beauchemin
    et al. ([2022](#bib.bib3)), drawing from Lyu and Liu ([2021](#bib.bib43)), identifies
    soft skills as a variety of personal attributes and behaviors crucial for effective
    workplace interaction, collaboration, and adaptability.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'Sayfullina 等人 ([2018](#bib.bib50)) 参考 Collins 词典（HarperCollins Publishers，[2023](#bib.bib29)），将软技能视为与生俱来的非技术性特质，这些特质在就业中受到高度重视，与依赖获得的知识有所不同。在更为社会化的背景下，Tamburri
    等人 ([2020](#bib.bib54)) 将软技能定义为涵盖个人、情感、社会或智力方面的能力，进一步被称为行为技能或能力。与此观点相呼应，Beauchemin
    等人 ([2022](#bib.bib3)) 从 Lyu 和 Liu ([2021](#bib.bib43)) 的研究中得出结论，将软技能识别为一系列个人属性和行为，这些技能对于有效的工作场所互动、协作和适应性至关重要。  '
- en: Adding to these perspectives, ESCO characterizes soft skills as *transversal
    skills*, highlighting their wide applicability across various occupations and
    sectors and their fundamental role in individual growth.⁵⁵5[https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/transversal-knowledge-skills-and-competences](https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/transversal-knowledge-skills-and-competences)
    Similarly, O*NET classifies these skills under Cross-Functional Skills, defining
    them as developed capacities that enhance the performance of activities common
    across different jobs, encompassing areas like Social Skills and Complex Problem
    Solving Skills.⁶⁶6See footnote 2. Both sources underscore the universal relevance
    of soft skills.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，ESCO 将软技能描述为*横向技能*，突显了其在各类职业和领域中的广泛适用性及其在个人成长中的基础性作用。[https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/transversal-knowledge-skills-and-competences](https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/transversal-knowledge-skills-and-competences)
    同样，O*NET 将这些技能归类为跨职能技能，定义为提升各种工作中常见活动表现的能力，包括社会技能和复杂问题解决技能。[6See footnote 2.]
    两个来源都强调了软技能的普遍相关性。
- en: These previous definitions lead to our converged definition that soft skills
    cover a vast array of personal, social, and intellectual competencies, all of
    which are indispensable for successful interpersonal engagement and personal development
    in professional settings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '这些先前的定义促成了我们的汇聚定义，即软技能涵盖了广泛的个人、社会和智力能力，这些能力对于成功的人际交往和职业环境中的个人发展是不可或缺的。  '
- en: 5 Operationalization of Skill Definitions
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**技能定义的操作化**  '
- en: In this section, we explore various methodologies for operationalizing skill
    definitions in skill extraction and classification research.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们探讨了在技能提取和分类研究中操作化技能定义的各种方法。  '
- en: Using a Skill Base
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**使用技能库**  '
- en: 'By using a given skill base, a pre-defined definition of the concept of skills
    is provided by the authors of the skill base. Numerous studies employ established
    skill bases such as the ESCO taxonomy (Zhang et al., [2023](#bib.bib64), [2022b](#bib.bib62);
    Clavié and Soulié, [2023](#bib.bib12); Decorte et al., [2023](#bib.bib17), [2022](#bib.bib16))
    or O*NET (Gugnani and Misra, [2020](#bib.bib27)). However, it is often ambiguous
    whether these studies use all or only specific subcategories (Li et al., [2023](#bib.bib39);
    Decorte et al., [2022](#bib.bib16); Gugnani and Misra, [2020](#bib.bib27)). Some
    papers mention explicitly the use of all subclasses (Zhang et al., [2022b](#bib.bib62),
    [a](#bib.bib61); Gnehm et al., [2022a](#bib.bib21)) other times it can be inferred
    from the number of skill spans used (Clavié and Soulié, [2023](#bib.bib12); Decorte
    et al., [2023](#bib.bib17)). However, one should note that the interpretations
    of ESCO definitions differ based on the ESCO version and authors’ perspective.
    Zhang et al. ([2022a](#bib.bib61), [b](#bib.bib62)) used ESCO version 1.0 with
    a different soft skill category than discussed in Section [4](#S4 "4 What are
    Skills? On Skill Definitions ‣ Deep Learning-based Computational Job Market Analysis:
    A Survey on Skill Extraction and Classification from Job Postings") and implemented
    two labels: “knowledge” aligns with ESCO’s “Knowledge” category, and “Skills”
    as a fusion of the hard and soft skills. In contrast, Colombo et al. ([2019](#bib.bib13))
    using the same ESCO version, but treat soft skills separate from hard skills.
    Most of the publications used all subcategories as skills without differentiating
    (Clavié and Soulié, [2023](#bib.bib12); Gnehm et al., [2022a](#bib.bib21); Decorte
    et al., [2023](#bib.bib17)).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '利用给定的技能库，技能库的作者提供了概念的预定义定义。许多研究使用了既定的技能库，如 ESCO 分类法（Zhang 等人，[2023](#bib.bib64),
    [2022b](#bib.bib62); Clavié 和 Soulié，[2023](#bib.bib12); Decorte 等人，[2023](#bib.bib17),
    [2022](#bib.bib16)）或 O*NET（Gugnani 和 Misra，[2020](#bib.bib27)）。然而，这些研究是否使用了所有子类别或仅使用特定子类别常常不明确（Li
    等人，[2023](#bib.bib39); Decorte 等人，[2022](#bib.bib16); Gugnani 和 Misra，[2020](#bib.bib27)）。一些论文明确提到使用了所有子类（Zhang
    等人，[2022b](#bib.bib62), [a](#bib.bib61); Gnehm 等人，[2022a](#bib.bib21)），而其他情况下可以从使用的技能范围数量中推断出来（Clavié
    和 Soulié，[2023](#bib.bib12); Decorte 等人，[2023](#bib.bib17)）。然而，需要注意的是，ESCO 定义的解释会根据
    ESCO 版本和作者的视角有所不同。Zhang 等人（[2022a](#bib.bib61), [b](#bib.bib62)）使用了 ESCO 1.0 版本，其软技能类别与第[4](#S4
    "4 What are Skills? On Skill Definitions ‣ Deep Learning-based Computational Job
    Market Analysis: A Survey on Skill Extraction and Classification from Job Postings")节讨论的不同，并实现了两个标签：“知识”与
    ESCO 的“Knowledge”类别对齐，“技能”作为硬技能和软技能的融合。相比之下，Colombo 等人（[2019](#bib.bib13)）使用相同的
    ESCO 版本，但将软技能与硬技能分开处理。大多数出版物将所有子类别作为技能使用而不做区分（Clavié 和 Soulié，[2023](#bib.bib12);
    Gnehm 等人，[2022a](#bib.bib21); Decorte 等人，[2023](#bib.bib17)）。'
- en: Beyond these, there are other skill bases, such as the Russian professional
    standard in Botov et al. ([2019](#bib.bib8)) or the Chinese Occupation Classification
    Grand Dictionary used in Cao and Zhang ([2021](#bib.bib10)); Cao et al. ([2021](#bib.bib11)).
    Additionally, non-official skill bases exist, like the list of 1K soft skills
    in (Sayfullina et al., [2018](#bib.bib50)) or LinkedIn’s in-house taxonomy for
    skill extraction (Shi et al., [2020](#bib.bib52)). In general, for transparency
    and reproducibility, it is helpful to state which subset of fine-grained labels
    $L$ of the skill base ($B$) and which skill base version is used.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，还有其他技能库，例如 Botov 等人（[2019](#bib.bib8)）中的俄罗斯职业标准或 Cao 和 Zhang（[2021](#bib.bib10)）；Cao
    等人（[2021](#bib.bib11)）使用的中国职业分类大典。此外，还存在非官方的技能库，如 Sayfullina 等人（[2018](#bib.bib50)）中的
    1K 软技能列表或 LinkedIn 的内部分类法（Shi 等人，[2020](#bib.bib52)）。一般来说，为了保证透明性和可重复性，说明使用了技能库的哪个细化标签子集
    $L$ 和哪个技能库版本是很有帮助的。
- en: Leveraging Automated Tools
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 利用自动化工具
- en: Some studies leverage automated tools like AutoPhrase (Shang et al., [2018](#bib.bib51))
    or Microsoft Azure Analytics Service for NER for initial skill term detection,
    followed by manual verification and refinement (Yao et al., [2022](#bib.bib60);
    Kortum et al., [2022](#bib.bib36)). Also Vermeer et al. ([2022](#bib.bib56)) extract
    parts of their training data using an automated tool, while others are taken from
    a skill base.⁷⁷7[https://www.textkernel.com/de/](https://www.textkernel.com/de/)
    Lastly, Gugnani and Misra ([2020](#bib.bib27)) employ an IBM tool for skill identification,
    which forms a part of a larger skill identification framework.⁸⁸8[https://www.ibm.com/products/natural-language-understanding](https://www.ibm.com/products/natural-language-understanding)
    While some previous work did not apply manual verification (Gugnani and Misra,
    [2020](#bib.bib27); Vermeer et al., [2022](#bib.bib56)), we recommend it to reduce
    automation bias from the tool impacting the data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究利用自动化工具如 AutoPhrase (Shang et al., [2018](#bib.bib51)) 或 Microsoft Azure
    Analytics Service 进行初步技能术语检测，然后进行人工验证和改进 (Yao et al., [2022](#bib.bib60); Kortum
    et al., [2022](#bib.bib36))。此外，Vermeer et al. ([2022](#bib.bib56)) 使用自动化工具提取部分训练数据，而其他数据则来自技能库。⁷⁷7[https://www.textkernel.com/de/](https://www.textkernel.com/de/)
    最后，Gugnani 和 Misra ([2020](#bib.bib27)) 使用 IBM 工具进行技能识别，这属于更大技能识别框架的一部分。⁸⁸8[https://www.ibm.com/products/natural-language-understanding](https://www.ibm.com/products/natural-language-understanding)
    虽然一些早期工作未进行人工验证 (Gugnani 和 Misra, [2020](#bib.bib27); Vermeer et al., [2022](#bib.bib56))，但我们建议进行人工验证，以减少工具对数据的自动化偏差影响。
- en: Definition through Labeling
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通过标注定义
- en: 'Domain experts play a crucial role for labeling data and therefore impact how
    the definition of skills is put into work (Shi et al., [2020](#bib.bib52); Tamburri
    et al., [2020](#bib.bib54); Beauchemin et al., [2022](#bib.bib3)). Tamburri et al.
    ([2020](#bib.bib54)) additionally provide a codebook with skill definitions to
    address ambiguities. Shi et al. ([2020](#bib.bib52)) used next skills identified
    by hiring experts and skills common among successful applicants as training data.
    The study by Bhola et al. ([2020](#bib.bib5)) treat the companies filing the JPs
    as domain experts by using their labels (see also Section [6](#S6.SS0.SSS0.Px8
    "BHOLA ‣ 6 Data ‣ Deep Learning-based Computational Job Market Analysis: A Survey
    on Skill Extraction and Classification from Job Postings")). Besides domain experts,
    crowd workers and the people writing the guidelines for the workers oftentimes
    determine which terms are skills. Some studies do not mention who labels the data
    (Wild et al., [2021](#bib.bib59); Cao and Zhang, [2021](#bib.bib10); Botov et al.,
    [2019](#bib.bib8)). We suggest being clear about the labeling process and guidelines,
    making them public for transparency and re-use/standardization, and using domain
    experts if possible for accurate labeling.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 领域专家在标注数据中发挥着关键作用，因此影响技能定义的实施方式 (Shi et al., [2020](#bib.bib52); Tamburri et
    al., [2020](#bib.bib54); Beauchemin et al., [2022](#bib.bib3))。Tamburri et al.
    ([2020](#bib.bib54)) 还提供了一个技能定义的编码书，以解决歧义问题。Shi et al. ([2020](#bib.bib52)) 使用了由招聘专家识别的下一步技能和成功申请者中常见的技能作为训练数据。Bhola
    et al. ([2020](#bib.bib5)) 通过使用公司提交的职位发布标签，将这些公司视为领域专家 (参见第[6](#S6.SS0.SSS0.Px8
    "BHOLA ‣ 6 数据 ‣ 基于深度学习的计算工作市场分析：职位发布中技能提取和分类的调查")节)。除了领域专家外，众包工人和编写指南的人通常决定哪些术语是技能。一些研究没有提及谁标注了数据
    (Wild et al., [2021](#bib.bib59); Cao 和 Zhang, [2021](#bib.bib10); Botov et al.,
    [2019](#bib.bib8))。我们建议明确标注过程和指南，使其公开以便透明化和重用/标准化，并尽可能使用领域专家进行准确标注。
- en: 6 Data
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 数据
- en: 'In this section, we provide a comprehensive description of publicly available
    datasets, with an overview in Table [1](#S6.T1 "Table 1 ‣ 6 Data ‣ Deep Learning-based
    Computational Job Market Analysis: A Survey on Skill Extraction and Classification
    from Job Postings").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们提供了对公开可用数据集的全面描述，概述见表[1](#S6.T1 "表 1 ‣ 6 数据 ‣ 基于深度学习的计算工作市场分析：职位发布中技能提取和分类的调查")。
- en: '| Publication | Approach | Granularity | Skill type | Use case | Size | \faBook
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 发表 | 方法 | 粒度 | 技能类型 | 使用案例 | 大小 | \faBook |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| (Sayfullina et al., [2018](#bib.bib50)) | Crowdsourced | span-level | soft
    | $I$ | 7411 spans | \faRemove |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| (Sayfullina et al., [2018](#bib.bib50)) | 众包 | 范围级 | 软 | $I$ | 7411 范围 |
    \faRemove |'
- en: '| (Green et al., [2022](#bib.bib25)) | Crowdsourced | span-level | hard + soft
    | $E_{C}$ | 10,606 spans | \faCheck |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| (Green et al., [2022](#bib.bib25)) | 众包 | 范围级 | 硬 + 软 | $E_{C}$ | 10,606
    范围 | \faCheck |'
- en: '| (Beauchemin et al., [2022](#bib.bib3)) | Expert | span-level | soft | $E_{C}$
    | 47 JPs - 932 spans | \faRemove |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| (Beauchemin 等，[2022](#bib.bib3)) | 专家 | 范围级别 | 软技能 | $E_{C}$ | 47 JPs - 932
    范围 | \faRemove |'
- en: '| (Zhang et al., [2022a](#bib.bib61)) | Expert | span-level | hard + soft |
    $E_{C}$ | 265 JP - 9,633 spans | \faCheck |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| (Zhang 等，[2022a](#bib.bib61)) | 专家 | 范围级别 | 硬技能 + 软技能 | $E_{C}$ | 265 JP
    - 9,633 范围 | \faCheck |'
- en: '| (Zhang et al., [2022b](#bib.bib62)) | Expert | span-level | hard + soft |
    $E_{C}$+$C_{D}$ | 60 JP - 920 spans | \faCheck |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| (Zhang 等，[2022b](#bib.bib62)) | 专家 | 范围级别 | 硬技能 + 软技能 | $E_{C}$+$C_{D}$ |
    60 JP - 920 范围 | \faCheck |'
- en: '| (Decorte et al., [2022](#bib.bib16)) | Manual | span-level | hard + soft
    | $I$+$C_{D}$ | 1,618 spans | \faCheck |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| (Decorte 等，[2022](#bib.bib16)) | 手动 | 范围级别 | 硬技能 + 软技能 | $I$+$C_{D}$ | 1,618
    范围 | \faCheck |'
- en: '| (Gnehm et al., [2022b](#bib.bib22)) | Expert | span-level | hard + soft |
    $E_{C}$+$C_{D}$ | 10,995 spans | \faRemove |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| (Gnehm 等，[2022b](#bib.bib22)) | 专家 | 范围级别 | 硬技能 + 软技能 | $E_{C}$+$C_{D}$ |
    10,995 范围 | \faRemove |'
- en: '| (Bhola et al., [2020](#bib.bib5)) | Skill Inventory | document-level | unknown
    | $C_{E}$ | 20,298 JP | \faRemove |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| (Bhola 等，[2020](#bib.bib5)) | 技能清单 | 文档级别 | 未知 | $C_{E}$ | 20,298 JP | \faRemove
    |'
- en: 'Table 1: Overview of publicly-available labeled datasets. \faBook indicates
    if the authors used guidelines (not necessarily publicly available).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：公开可用的标注数据集概述。 \faBook 表示作者是否使用了指南（不一定是公开可用的）。
- en: SAYFULLINA
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SAYFULLINA
- en: by Sayfullina et al. ([2018](#bib.bib50)) is a dataset derived from a publicly
    available Kaggle dataset, containing JPs from within the UK and representing a
    variety of sectors.⁹⁹9[https://www.kaggle.com/datasets/airiddha/trainrev1/?select=Train_rev1.csv](https://www.kaggle.com/datasets/airiddha/trainrev1/?select=Train_rev1.csv)
    The authors retrieved soft skill spans by exact matching with a list of 1,072
    soft skills. Each identified span is accompanied by up to 10 surrounding words.
    Crowdsourcing was used to determine whether the highlighted skill belongs to a
    job applicant. To ensure reliability, the workers were tested on a small set of
    JPs and each snippet was evaluated by at least three workers. This process led
    to a dataset with high class imbalance due to more positive examples. To counter
    this, additional skill spans were added, including those usually not describing
    candidates (marked as negative) and those consistently labeled positive.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Sayfullina 等（[2018](#bib.bib50)）创建的数据集来源于一个公开可用的 Kaggle 数据集，包含来自英国的 JP，代表了各种行业。⁹⁹9[https://www.kaggle.com/datasets/airiddha/trainrev1/?select=Train_rev1.csv](https://www.kaggle.com/datasets/airiddha/trainrev1/?select=Train_rev1.csv)
    作者通过与1,072个软技能列表进行精确匹配来提取软技能范围。每个识别出的范围伴随最多10个周围的词。通过众包确定突出技能是否属于求职者。为了确保可靠性，工人在一小部分
    JP 上进行了测试，每个片段至少由三名工人评估。这一过程导致了一个具有高度类别不平衡的数据集，因为正面例子较多。为此，添加了额外的技能范围，包括那些通常不描述候选人的（标记为负面）和那些一致标记为正面的。
- en: GREEN
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GREEN
- en: by Green et al. ([2022](#bib.bib25)) uses the same Kaggle dataset as SAYFULLINA.
    The labeling was done via crowdsourcing, they did not use experts but only workers
    who passed a test were included, and encouraged to follow the guidelines. Apart
    from the “Skill” label capturing hard and soft skills, the labels “Occupation”,
    “Domain”, “Experience”, “Qualification”, and “None” are used in a BIO scheme.
    The authors reduced errors by label aggregation with a preference towards labels
    from higher-performing workers. Additionally, they reclassified specific “Experience”
    spans, as “Skill” spans, and manually split multi-term spans into separate spans.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Green 等（[2022](#bib.bib25)）使用了与 SAYFULLINA 相同的 Kaggle 数据集。标签是通过众包完成的，他们没有使用专家，而只包括通过测试的工人，并鼓励他们遵循指南。除了“技能”标签捕捉硬技能和软技能外，还在
    BIO 方案中使用了“职业”、“领域”、“经验”、“资格”和“无”标签。作者通过对标签进行聚合来减少错误，偏向于高性能工人的标签。此外，他们将特定的“经验”范围重新分类为“技能”范围，并手动将多词范围拆分为独立的范围。
- en: FIJO
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: FIJO
- en: 'by Beauchemin et al. ([2022](#bib.bib3)) was created in partnership with Canadian
    insurance companies, and consists of cleaned and de-identified French JPs published
    between 2009 and 2020\. The dataset focus on soft skills and includes 867 JPs
    with 47 annotated JPs, selected and annotated by a domain expert. The annotated
    spans are unevenly distributed across four classes: “Thoughts”, “Results”, “Relational”,
    and “Personal”.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Beauchemin 等（[2022](#bib.bib3)）与加拿大保险公司合作创建，包含2009年至2020年发布的清理和去标识化的法语 JPs。该数据集专注于软技能，包括867个JP，其中47个由领域专家选择和标注。标注的范围在“四个类别”中分布不均：
    “思维”、“结果”、“关系”和“个人”。
- en: SKILLSPAN
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SKILLSPAN
- en: 'by Zhang et al. ([2022a](#bib.bib61)) consists of the anonymized raw data and
    annotations of skill and knowledge spans from three JP datasets, one of which
    cannot be made publicly available due to its license. The available datasets are:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Zhang 等人 ([2022a](#bib.bib61)) 提供的内容包括三个 JP 数据集中的匿名原始数据和技能及知识范围的注释，其中一个由于许可问题无法公开。可用的数据集包括：
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'HOUSE: A static in-house dataset with different types of JPs from 2012-2020
    and'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'HOUSE: 一个静态的内部数据集，包含2012-2020年的不同类型的 JP'
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TECH: The StackOverflow JP platform, consisting mostly of technical jobs collected
    between June 2020 and September 2021.'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'TECH: StackOverflow JP 平台，主要包括2020年6月到2021年9月间收集的技术职位。'
- en: The development of the publicly available annotation guidelines involved an
    iterative process, starting with a few JPs and progressing through several rounds
    of annotation and refinement by three domain experts.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 公开的注释指南的开发经历了一个迭代过程，从少量 JP 开始，通过三位领域专家的多轮注释和改进进行推进。
- en: KOMPETENCER
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: KOMPETENCER
- en: 'by Zhang et al. ([2022b](#bib.bib62)) consists of Danish JPs with annotated
    skill and knowledge spans, see Table [4](#A1.T4 "Table 4 ‣ A.1 Terminology Example
    ‣ Appendix A Appendix ‣ Deep Learning-based Computational Job Market Analysis:
    A Survey on Skill Extraction and Classification from Job Postings") in the Appendix.
    The same skill definitions, guidelines, and metrics as in SKILLSPAN are used for
    annotation. This dataset can be used for skill extraction with coarse labels,
    but the authors have also added fine-grained annotations to evaluate a classification
    with the ESCO taxonomy. For fine-grained annotations, they query the ESCO API
    with the annotated spans and use Levenshtein distance to determine the relevance
    of each obtained label. Then, the quality of these distantly supervised labels
    is assessed through human evaluation. They also repeated this process for the
    English SKILLSPAN dataset but only manually checked a sample for calculating statistics.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '由 Zhang 等人 ([2022b](#bib.bib62)) 提供的 KOMPETENCER 包含丹麦的 JP，附有技能和知识范围的注释，详见附录中的表格[4](#A1.T4
    "Table 4 ‣ A.1 Terminology Example ‣ Appendix A Appendix ‣ Deep Learning-based
    Computational Job Market Analysis: A Survey on Skill Extraction and Classification
    from Job Postings")。注释使用了与 SKILLSPAN 相同的技能定义、指南和指标。该数据集可以用于粗略标签的技能提取，但作者还添加了细粒度的注释，以评估与
    ESCO 分类法的分类。对于细粒度注释，他们查询了 ESCO API 以获取注释的范围，并使用 Levenshtein 距离来确定每个获得标签的相关性。然后，通过人工评估来评估这些远程监督标签的质量。他们还重复了这个过程对英语
    SKILLSPAN 数据集进行了检查，但仅手动检查了样本以计算统计数据。'
- en: DECORTE
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DECORTE
- en: 'by Decorte et al. ([2022](#bib.bib16)) is a variant of the SKILLSPAN dataset
    with annotated ESCO labels. They used the identified skill without the skill and
    knowledge labels, but they can be recreated by matching the dataset with SKILLSPAN,
    see Table [4](#A1.T4 "Table 4 ‣ A.1 Terminology Example ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings") in the Appendix. Unlike in KOMPETENCER
    they manually matched the skills with fitting ESCO labels (if they exist) to create
    a gold standard.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '由 Decorte 等人 ([2022](#bib.bib16)) 提供的 DECORTE 是 SKILLSPAN 数据集的一个变体，附有标注的 ESCO
    标签。他们使用了识别出的技能，但没有技能和知识标签，不过可以通过将数据集与 SKILLSPAN 匹配来重新创建，详见附录中的表格[4](#A1.T4 "Table
    4 ‣ A.1 Terminology Example ‣ Appendix A Appendix ‣ Deep Learning-based Computational
    Job Market Analysis: A Survey on Skill Extraction and Classification from Job
    Postings")。与 KOMPETENCER 不同，他们手动将技能与适配的 ESCO 标签（如果存在）匹配，以创建一个黄金标准。'
- en: GNEHM-ICT
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GNEHM-ICT
- en: by Gnehm et al. ([2022b](#bib.bib22)) is a Swiss-German dataset where they annotated
    for Information and Communications Technology (ICT)-related entity recognition.
    These could be ICT tasks, technology stack, responsibilities, and so forth. The
    used dataset is a combination of two other Swiss datasets namely the Swiss Job
    Market Monitor and an online job ad dataset Gnehm and Clematide ([2020](#bib.bib23));
    Buchmann et al. ([2022](#bib.bib9)). There are around 25,000 sentences in the
    dataset.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Gnehm 等人 ([2022b](#bib.bib22)) 提供的 GNEHM-ICT 是一个瑞士-德国数据集，其中进行了信息和通信技术（ICT）相关实体识别的注释。这些可能包括
    ICT 任务、技术栈、职责等。使用的数据集是两个其他瑞士数据集的组合，即瑞士就业市场监测器和一个在线招聘广告数据集 Gnehm 和 Clematide ([2020](#bib.bib23));
    Buchmann 等人 ([2022](#bib.bib9))。数据集中约有25,000个句子。
- en: BHOLA
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: BHOLA
- en: by Bhola et al. ([2020](#bib.bib5)) was obtained from a government website^(10)^(10)10[https://www.mycareersfuture.gov.sg/](https://www.mycareersfuture.gov.sg/).
    in Singapore. The preprocessing steps for this English language dataset include
    converting text to lowercase and removing stop words and rarely used words. The
    companies filing the JPs added skill labels, which are mapped to the whole JP
    document. This makes the dataset suitable for performing multi-label classification
    by predicting a set of required skills for a given JP.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Bhola 等人 ([2020](#bib.bib5)) 获得，数据来源于新加坡的一个政府网站^(10)^(10)10[https://www.mycareersfuture.gov.sg/](https://www.mycareersfuture.gov.sg/)。该英语数据集的预处理步骤包括将文本转换为小写字母，去除停用词和不常用词。提交
    JPs 的公司添加了技能标签，这些标签被映射到整个 JP 文档。这使得该数据集适合通过预测给定 JP 所需的一组技能来进行多标签分类。
- en: 7 Methods
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 方法
- en: '| Paper | Model | Skill Type | Granularity | Use Case |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 模型 | 技能类型 | 粒度 | 用例 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| (Fang et al., [2023](#bib.bib19)) | Custom pre-trained LM | soft + hard |
    word-level | $E_{C}$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| (Fang等, [2023](#bib.bib19)) | 自定义预训练语言模型 | soft + hard | word-level | $E_{C}$
    |'
- en: '| (Goyal et al., [2023](#bib.bib24)) | FastText skip-gram, GNN | unknown |
    word-level | $C_{E}$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| (Goyal等, [2023](#bib.bib24)) | FastText skip-gram, GNN | unknown | word-level
    | $C_{E}$ |'
- en: '| (Clavié and Soulié, [2023](#bib.bib12)) | GPT-4 | soft + hard | span-level
    | $C_{E}$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| (Clavié和Soulié, [2023](#bib.bib12)) | GPT-4 | soft + hard | span-level |
    $C_{E}$ |'
- en: '| (Li et al., [2023](#bib.bib39)) | XMLC - LLM | soft + hard | document-level
    | $C_{E}$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| (李等, [2023](#bib.bib39)) | XMLC - LLM | soft + hard | document-level | $C_{E}$
    |'
- en: '| (Decorte et al., [2023](#bib.bib17)) | GPT-3.5 | soft + hard | sentence-level
    | $C_{E}$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| (Decorte等, [2023](#bib.bib17)) | GPT-3.5 | soft + hard | sentence-level |
    $C_{E}$ |'
- en: '| (Zhang et al., [2023](#bib.bib64)) | Multilingual XLM-R | soft + hard | span-level
    | $E_{C}$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| (张等, [2023](#bib.bib64)) | 多语言 XLM-R | soft + hard | span-level | $E_{C}$
    |'
- en: '| (Decorte et al., [2022](#bib.bib16)) | RoBERTa | soft + hard | sentence-level
    | $C_{E}$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| (Decorte等, [2022](#bib.bib16)) | RoBERTa | soft + hard | sentence-level |
    $C_{E}$ |'
- en: '| (Zhang et al., [2022c](#bib.bib63)) | RoBERTa, JobBERT | soft + hard | span-level
    | $C_{D}$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| (张等, [2022c](#bib.bib63)) | RoBERTa, JobBERT | soft + hard | span-level |
    $C_{D}$ |'
- en: '| (Gnehm et al., [2022a](#bib.bib21)) | JobBERT-de, SBERT | soft + hard | span-level
    | $E_{C}$ + $C_{D}$ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| (Gnehm等, [2022a](#bib.bib21)) | JobBERT-de, SBERT | soft + hard | span-level
    | $E_{C}$ + $C_{D}$ |'
- en: '| (Zhang et al., [2022b](#bib.bib62)) | BERTbase , DaBERT | soft + hard | span-level
    | $C_{E}$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| (张等, [2022b](#bib.bib62)) | BERTbase , DaBERT | soft + hard | span-level
    | $C_{E}$ |'
- en: '| (Beauchemin et al., [2022](#bib.bib3)) | Bi-LSTM, CamemBERT | soft | span-level
    | $E_{C}$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (Beauchemin等, [2022](#bib.bib3)) | Bi-LSTM, CamemBERT | soft | span-level
    | $E_{C}$ |'
- en: '| (Yao et al., [2022](#bib.bib60)) | BERT, word2vec | unknown | word-level
    | $I$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| (姚等, [2022](#bib.bib60)) | BERT, word2vec | unknown | word-level | $I$ |'
- en: '| (Anand et al., [2022](#bib.bib1)) | LaBSE model | soft + hard | title | $C_{E}$
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| (Anand等, [2022](#bib.bib1)) | LaBSE模型 | soft + hard | title | $C_{E}$ |'
- en: '| (Vermeer et al., [2022](#bib.bib56)) | RobBERT | soft + hard | document-level
    | $C_{E}$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| (Vermeer等, [2022](#bib.bib56)) | RobBERT | soft + hard | document-level |
    $C_{E}$ |'
- en: '| (Wild et al., [2021](#bib.bib59)) | BERT, spaCy | soft + hard | span-level
    | $I$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| (Wild等, [2021](#bib.bib59)) | BERT, spaCy | soft + hard | span-level | $I$
    |'
- en: '| (Khaouja et al., [2021b](#bib.bib35)) | Sent2vec, SBERT | soft + hard | sentence-level
    | $C_{E}$ |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| (Khaouja等, [2021b](#bib.bib35)) | Sent2vec, SBERT | soft + hard | sentence-level
    | $C_{E}$ |'
- en: '| (Cao et al., [2021](#bib.bib11)) | BERT-BiLSTM-CRF | soft + hard | span-level
    | $I$ |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| (曹等, [2021](#bib.bib11)) | BERT-BiLSTM-CRF | soft + hard | span-level | $I$
    |'
- en: '| (Cao and Zhang, [2021](#bib.bib10)) | BERT-BiLSTM-CRF | soft + hard | span-level
    | $I$ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| (曹和张, [2021](#bib.bib10)) | BERT-BiLSTM-CRF | soft + hard | span-level |
    $I$ |'
- en: '| (Li et al., [2020](#bib.bib40)) | Deep Averaging Network, FastText | unknown
    | span-level | $C_{E}$ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| (李等, [2020](#bib.bib40)) | Deep Averaging Network, FastText | unknown | span-level
    | $C_{E}$ |'
- en: '| (Tamburri et al., [2020](#bib.bib54)) | BERT Multilingual Cased | soft +
    hard | sentence-level | $I$ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| (Tamburri等, [2020](#bib.bib54)) | BERT 多语言 Cased | soft + hard | sentence-level
    | $I$ |'
- en: '| (Bhola et al., [2020](#bib.bib5)) | BERTbase | unknown | document-level |
    $C_{E}$ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| (Bhola等, [2020](#bib.bib5)) | BERTbase | unknown | document-level | $C_{E}$
    |'
- en: '| (Gugnani and Misra, [2020](#bib.bib27)) | Word2vec | soft + hard | span-level
    | $I$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| (Gugnani和Misra, [2020](#bib.bib27)) | Word2vec | soft + hard | span-level
    | $I$ |'
- en: '| (Botov et al., [2019](#bib.bib8)) | Word2vec | unknown | span-level | $C_{E}$
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| (Botov等, [2019](#bib.bib8)) | Word2vec | unknown | span-level | $C_{E}$ |'
- en: '| (Jia et al., [2018](#bib.bib32)) | LSTM | unknown | word-level | $I$ |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| (贾等, [2018](#bib.bib32)) | LSTM | unknown | word-level | $I$ |'
- en: '| (Sayfullina et al., [2018](#bib.bib50)) | CNN, LSTM, HAN | soft | span-level
    | $I$ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| (Sayfullina等, [2018](#bib.bib50)) | CNN, LSTM, HAN | soft | span-level |
    $I$ |'
- en: '| (Javed et al., [2017](#bib.bib31)) | Word2vec | soft + hard | span-level
    | $C_{E}$ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| (Javed et al., [2017](#bib.bib31)) | Word2vec | 软+硬 | 跨度级别 | $C_{E}$ |'
- en: 'Table 2: Publications regarding neural skill extraction and classification.
    The skill type was not always explicitly mentioned in some cases it’s derived
    from examples given in the paper.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：有关神经技能提取和分类的出版物。在某些情况下，技能类型并未明确提到，而是从论文中给出的示例推导出来的。
- en: 'In this section, we survey methods for skill extraction and classification.
    As in Section [3](#S3 "3 Skill-related Terminology ‣ Deep Learning-based Computational
    Job Market Analysis: A Survey on Skill Extraction and Classification from Job
    Postings") the goal of the extraction is to identify skill spans with ($E_{C}$)
    or without coarse labels ($I$). The classification section covers direct classification
    methods ($C_{D}$) and classification methods with extraction ($C_{E}$), both aim
    to retrieve fine-grained skill labels.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们调查了技能提取和分类的方法。与第[3](#S3 "3 Skill-related Terminology ‣ Deep Learning-based
    Computational Job Market Analysis: A Survey on Skill Extraction and Classification
    from Job Postings")节一样，提取的目标是识别带有（$E_{C}$）或不带有粗略标签（$I$）的技能跨度。分类部分涵盖了直接分类方法（$C_{D}$）和带有提取的分类方法（$C_{E}$），两者都旨在检索细粒度的技能标签。'
- en: 7.1 Skill Extraction
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 技能提取
- en: 'This chapter delineates the evolution of skill extraction methodologies, grouped
    into three categories: skill identification as span labeling, skill identification
    through binary classification, and skill extraction with coarse span labels. Starting
    with LSTM neural networks in 2018 the methods in all three sub-chapters used after
    the introduction of BERT (Devlin et al., [2019](#bib.bib18)) in 2019 heavily BERT
    and BERT-based models. Recent advancements continue to diversify the landscape,
    integrating a broader array of language models (LMs).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了技能提取方法的演变，分为三类：作为跨度标注的技能识别、通过二分类进行的技能识别以及带有粗略跨度标签的技能提取。从2018年的LSTM神经网络开始，所有三章的方法在2019年引入BERT（Devlin
    et al., [2019](#bib.bib18)）之后，广泛使用了BERT及其基于BERT的模型。最近的进展继续丰富这一领域，整合了更多的语言模型（LMs）。
- en: 7.1.1 Skill Identification as Span Labeling
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1 技能识别作为跨度标注
- en: In this category approach skill identification as a span labeling task. The
    primary objective is to accurately identify skill spans, encompassing both the
    identification of the relevant skill phrases and their precise boundaries. Jia
    et al. ([2018](#bib.bib32)) are the first to use sequence tagging for identifying
    skills from JPs in 2018\. The authors use a pre-trained LSTM neural network (Lample
    et al., [2016](#bib.bib37)) for identifying skill terms on the word-level. Tamburri
    et al. ([2020](#bib.bib54)) also employed binary classification, but at the sentence-level,
    using a Dutch JP dataset. Their best-performing model, BERT Multilingual Cased,
    was fine-tuned on expert-annotated JP sentences, suggesting potential improvement
    with more data and optimization. Further publications retrieve embeddings using
    a pre-trained BERT model (Wild et al., [2021](#bib.bib59); Cao and Zhang, [2021](#bib.bib10);
    Cao et al., [2021](#bib.bib11)). Notably, Cao et al. ([2021](#bib.bib11)) and
    Cao and Zhang ([2021](#bib.bib10)) combine BERT’s pre-trained vectors with a Bi-LSTM
    and a CRF layer for finer entity classification. This approach aligns with previous
    research demonstrating the efficacy of a CRF layer in NER tasks (Souza et al.,
    [2020](#bib.bib53)). In Zhang et al. ([2023](#bib.bib64)), they further built
    upon the domain-adaptive pre-training paradigm (Gururangan et al., [2020](#bib.bib28)).
    They make use of the ESCO taxonomy (le Vrang et al., [2014](#bib.bib38)) and integrate
    this in a multilingual XLM-R model Conneau et al. ([2020](#bib.bib14)), using
    this taxonomy-driven pre-training method, they introduce a new state-of-the-art
    for all skill identification benchmarks. For analysis, they show that performance
    increases especially for skills that are shorter in length, due to ESCO skills
    also being shorter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一类别中，将技能识别视为一个跨度标注任务。主要目标是准确识别技能跨度，包括相关技能短语的识别及其精确边界。贾等人（[2018](#bib.bib32)）是首批在2018年使用序列标注方法从职位描述中识别技能的作者。作者们使用了预训练的LSTM神经网络（Lample等人，[2016](#bib.bib37)）来识别词级别的技能术语。Tamburri等人（[2020](#bib.bib54)）也采用了二分类方法，但在句子级别，使用了荷兰职位描述数据集。他们表现最佳的模型BERT
    Multilingual Cased，在专家标注的职位描述句子上进行了微调，建议通过更多的数据和优化可以进一步改进。进一步的研究检索了使用预训练BERT模型的嵌入（Wild等人，[2021](#bib.bib59)；Cao和Zhang，[2021](#bib.bib10)；Cao等人，[2021](#bib.bib11)）。值得注意的是，Cao等人（[2021](#bib.bib11)）和Cao与Zhang（[2021](#bib.bib10)）将BERT的预训练向量与Bi-LSTM和CRF层结合，以进行更精细的实体分类。这种方法与之前的研究一致，展示了CRF层在命名实体识别任务中的有效性（Souza等人，[2020](#bib.bib53)）。在张等人（[2023](#bib.bib64)）的研究中，他们进一步建立了领域自适应预训练范式（Gururangan等人，[2020](#bib.bib28)）。他们利用ESCO分类法（le
    Vrang等人，[2014](#bib.bib38)），并将其整合到多语言XLM-R模型（Conneau等人，[2020](#bib.bib14)）中，通过这种基于分类法的预训练方法，他们在所有技能识别基准中引入了新的最先进水平。在分析中，他们表明，尤其是对于较短技能的表现有所提高，因为ESCO技能通常较短。
- en: 'In contrast to these single-model approaches, Gugnani and Misra ([2020](#bib.bib27))
    adopted a multi-faceted methodology to predict the relevance of identified skill
    spans. Their methodology encompassed four modules: using part-of-speech (PoS)
    tagging, parsing sentences with skill bases (O*NET, Hope, and Wikipedia), leveraging
    a ready-made sequence tagging solution, and employing a pre-trained word2vec model
    for final score determination through cosine similarity.^(11)^(11)11[https://www.ibm.com/products/natural-language-understanding](https://www.ibm.com/products/natural-language-understanding).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 与这些单模型方法相比，Gugnani和Misra（[2020](#bib.bib27)）采用了一种多方面的方法来预测已识别技能跨度的相关性。他们的方法包括四个模块：使用词性标注（PoS），解析具有技能基础的句子（O*NET、Hope和Wikipedia），利用现成的序列标注解决方案，以及使用预训练的word2vec模型通过余弦相似度确定最终得分。^(11)^(11)11[https://www.ibm.com/products/natural-language-understanding](https://www.ibm.com/products/natural-language-understanding)。
- en: 7.1.2 Skill Identification as binary Classification Task
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2 将技能识别视为二分类任务
- en: In this category, skill identification is framed as a binary classification
    task. The focus is on determining whether a given sequence either constitutes
    or contains a (specific) skill. The task in Sayfullina et al. ([2018](#bib.bib50))
    differs from the other publications. They extract skill spans by exact match and
    aim to decide whether skill spans refer to a candidate or something else, like
    a company. They experiment with various classifiers and input representations,
    such as Soft Skill Masking, Embedding, and Tagging, finding the LSTM classifier
    with skill tagging most effective on their dataset. Tamburri et al. ([2020](#bib.bib54))
    employed binary classification at the sentence-level to determine if it contains
    a skill. Their best-performing model, BERT Multilingual Cased, was fine-tuned
    on expert-annotated JP sentences using a Dutch JP dataset. Yao et al. ([2022](#bib.bib60))
    classify individual words as skill-related or not. They split JPs into individual
    words, analyzing each through character-level and word-level encoders, integrating
    linguistic features like POS tags and capitalization. Their initial training employs
    AutoPhrase (Shang et al., [2018](#bib.bib51)) for automatic skill term identification,
    followed by manual verification and expert-labeled samples. The model is further
    refined using Positive-Unlabeled learning, where the classifier’s predictions
    on unlabeled data help expand the skill base for continuous adaptation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一类别中，技能识别被框定为二分类任务。重点在于确定给定的序列是否构成或包含（特定的）技能。Sayfullina 等人（[2018](#bib.bib50)）的任务与其他出版物不同。他们通过精确匹配提取技能跨度，并旨在决定这些技能跨度是否指向一个候选人或其他事物，比如公司。他们尝试了各种分类器和输入表示方法，如软技能掩码、嵌入和标记，发现
    LSTM 分类器与技能标记在他们的数据集上效果最佳。Tamburri 等人（[2020](#bib.bib54)）在句子级别采用二分类方法，以确定句子是否包含技能。他们表现最好的模型，BERT
    Multilingual Cased，经过了在荷兰 JP 数据集上使用专家标注的 JP 句子进行的微调。Yao 等人（[2022](#bib.bib60)）将单词分类为与技能相关或不相关。他们将
    JP 拆分为单个单词，通过字符级和词级编码器分析每个单词，整合了如词性标签和大小写等语言特征。他们的初始训练采用了 AutoPhrase（Shang 等人，[2018](#bib.bib51)）进行自动技能术语识别，随后进行手动验证和专家标注样本。该模型进一步通过正负标记学习进行精炼，其中分类器对未标记数据的预测帮助扩展技能库，以实现持续适应。
- en: 7.1.3 Skill Extraction with Coarse Labels
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.3 使用粗略标签的技能提取
- en: This section explores advancements in skill extraction with coarse labels, where
    each publication extract spans from two to four different categories. The studies
    of Gnehm et al. ([2022a](#bib.bib21)) and Zhang et al. ([2022a](#bib.bib61)) both
    utilize sequence tagging-based models. Gnehm et al. ([2022a](#bib.bib21)) focusing
    on iterative training and annotation with jobBERT-de, a German LM tailored for
    JPs. Zhang et al. ([2022a](#bib.bib61)) compare BERT-based (Devlin et al., [2019](#bib.bib18))
    and SpanBERT-based (Joshi et al., [2020](#bib.bib33)) models, highlighting the
    importance of domain adaptation. On the other hand, Beauchemin et al. ([2022](#bib.bib3))
    and Fang et al. ([2023](#bib.bib19)) delve into the intricacies of training and
    optimizing LMs for skill extraction. Beauchemin et al. ([2022](#bib.bib3)) examine
    the sensitivity of Bi-LSTM and CamemBERT Martin et al. ([2020](#bib.bib44)) models
    to training data volume, with CamemBERT unfrozen yielding the highest mean token-wise
    accuracy. Fang et al. ([2023](#bib.bib19)) introduce RecruitPro, a specialized
    model for skill extraction from recruitment texts, employing innovative techniques
    for dealing with data noise and label imbalances. Collectively, these papers emphasize
    the need for tailored approaches and continuous innovation in model development.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了使用粗略标签进行技能提取的进展，其中每篇出版物的提取范围跨越了两个到四个不同的类别。Gnehm 等人（[2022a](#bib.bib21)）和
    Zhang 等人（[2022a](#bib.bib61)）的研究都利用了基于序列标注的模型。Gnehm 等人（[2022a](#bib.bib21)）专注于使用
    jobBERT-de（一种针对 JP 的德语语言模型）进行迭代训练和标注。Zhang 等人（[2022a](#bib.bib61)）比较了基于 BERT（Devlin
    等人，[2019](#bib.bib18)）和基于 SpanBERT（Joshi 等人，[2020](#bib.bib33)）的模型，强调了领域适应的重要性。另一方面，Beauchemin
    等人（[2022](#bib.bib3)）和 Fang 等人（[2023](#bib.bib19)）深入探讨了为技能提取训练和优化语言模型的复杂性。Beauchemin
    等人（[2022](#bib.bib3)）研究了 Bi-LSTM 和 CamemBERT Martin 等人（[2020](#bib.bib44)）模型对训练数据量的敏感性，其中
    CamemBERT 解冻后表现出最高的均值逐词准确率。Fang 等人（[2023](#bib.bib19)）提出了 RecruitPro，这是一种专门用于从招聘文本中提取技能的模型，采用了处理数据噪声和标签不平衡的创新技术。这些论文共同强调了模型开发中量体裁衣的方法和持续创新的必要性。
- en: 7.2 Skill Classification
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 技能分类
- en: While skill standardization can be achieved through classification, other methods
    such as clustering (Bernabé-Moreno et al., [2019](#bib.bib4); Lukauskas et al.,
    [2023](#bib.bib42)), matching n-grams based on string similarity (Boselli et al.,
    [2018](#bib.bib7)), or identifying semantically similar skills (Bernabé-Moreno
    et al., [2019](#bib.bib4); Colombo et al., [2019](#bib.bib13); Grüger and Schneider,
    [2019](#bib.bib26)) also lead to standardized skill spans. These methods simplify
    the variety and quantity of skill spans without assigning standardized labels.
    Transitioning from these methods, we now focus on skill classification, a crucial
    step for assigning standardized labels to effectively organize and understand
    skills. Most publications skip a traditional extraction and match the JPs directly
    to the skill base ($C_{E}$), which can be seen as skill extraction against a skill
    base. Exceptions are Gnehm et al. ([2022a](#bib.bib21)), which perform extraction
    of skill spans with coarse labels before the fine-grained classification step,
    and Zhang et al. ([2022b](#bib.bib62)) who rely on prior work for extraction and
    focus solely on the matching of skill spans to ESCO ($C_{D}$). We divide the publications
    by methodology into those that match based on semantic similarity and those using
    extreme multi-label classification to solve the matching task.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通过分类可以实现技能的标准化，但其他方法如聚类（Bernabé-Moreno 等，[2019](#bib.bib4)；Lukauskas 等，[2023](#bib.bib42)）、基于字符串相似性的
    n-gram 匹配（Boselli 等，[2018](#bib.bib7)），或者识别语义相似的技能（Bernabé-Moreno 等，[2019](#bib.bib4)；Colombo
    等，[2019](#bib.bib13)；Grüger 和 Schneider，[2019](#bib.bib26)）也能导致标准化的技能范围。这些方法简化了技能范围的多样性和数量，而无需分配标准化标签。从这些方法过渡，我们现在关注技能分类，这是为有效组织和理解技能分配标准化标签的关键步骤。大多数出版物跳过传统的提取步骤，直接将
    JPs 匹配到技能库（$C_{E}$），这可以看作是针对技能库的技能提取。例外的是 Gnehm 等人（[2022a](#bib.bib21)），他们在细粒度分类步骤之前进行粗略标签的技能范围提取，以及
    Zhang 等人（[2022b](#bib.bib62)），他们依赖于先前的工作进行提取，并仅关注技能范围与 ESCO（$C_{D}$）的匹配。我们将出版物按方法分为基于语义相似性的匹配和使用极端多标签分类解决匹配任务的两类。
- en: 7.2.1 Similarity-based Approaches
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1 基于相似性的的方法
- en: 'The publications with similarity-based approaches split the JPs into sentences
    or n-grams before matching them. All of the following publications use skill embedding
    methods, which can be seen as an advancement of the skill count methods (Section [1](#S1
    "1 Introduction ‣ Deep Learning-based Computational Job Market Analysis: A Survey
    on Skill Extraction and Classification from Job Postings")). The advances in text
    embeddings over time are reflected in the scope of the approaches. While Javed
    et al. ([2017](#bib.bib31)) and Botov et al. ([2019](#bib.bib8)) improve the matching
    using word2vec embeddings(Mikolov et al., [2013](#bib.bib45)), later Li et al.
    ([2020](#bib.bib40)) use FastText (Bojanowski et al., [2017](#bib.bib6)) leveraging
    sub-word information to handle out-of-vocabulary words and capture more detailed
    semantic and syntactic information. Khaouja et al. ([2021b](#bib.bib35)) compare
    using sent2vec trained on Wikipedia sentences, and SBERT (Reimers and Gurevych,
    [2019](#bib.bib48)) trained on millions of paraphrase sentences for embeddings.
    Moreover, Zhang et al. ([2022c](#bib.bib63)) uses LMs like RoBERTa and JobBERT
    to match n-grams from JP sentences with the ESCO taxonomy. They also experiment
    with context and frequency-aware embeddings. Gnehm et al. ([2022a](#bib.bib21))
    performed direct skill extraction using context-aware embeddings and the SBERT
    model similar to Zhang et al. ([2022c](#bib.bib63)), additionally they contextualize
    skill areas within spans and ontology terms using their hierarchical structure.
    The study explores techniques to enhance BERT model similarity, including in-domain
    pretraining, transformer-based sequential denoising auto-encoder (TSDAE; Wang
    et al., [2021](#bib.bib57)) for domain-specific terminology, and Siamese BERT
    Networks for training sentence embeddings (Reimers and Gurevych, [2019](#bib.bib48)).
    They further leverage MNR loss in Siamese networks (Henderson et al., [2017](#bib.bib30)),
    using ontology data to create positive text pairings for better label matching.
    SkillGPT Li et al. ([2023](#bib.bib39)) is the first tool to use an LLM for the
    matching task, they convert ESCO entries into structured documents, which are
    vectorized by the LM. Then, they summarize the input text, and use an embedding
    of the summary to retrieve the closest ESCO entries.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '使用基于相似性的研究将职位发布信息分割成句子或n-grams后再进行匹配。以下所有的研究都使用了技能嵌入方法，这可以被看作是技能计数方法的进步（见第[1](#S1
    "1 Introduction ‣ Deep Learning-based Computational Job Market Analysis: A Survey
    on Skill Extraction and Classification from Job Postings)节"）。随着时间推移，文本嵌入技术的进步体现在方法的范围上。虽然Javed等人（[2017](#bib.bib31)）和Botov等人（[2019](#bib.bib8)）使用word2vec嵌入（Mikolov等人，[2013](#bib.bib45)）改进了匹配，后来的Li等人（[2020](#bib.bib40)）则使用了FastText（Bojanowski等人，[2017](#bib.bib6)），通过利用子词信息来处理词汇外词汇，并捕捉更详细的语义和句法信息。Khaouja等人（[2021b](#bib.bib35)）比较了使用在维基百科句子上训练的sent2vec和在数百万句子上训练的SBERT（Reimers和Gurevych，[2019](#bib.bib48)）作为嵌入。此外，Zhang等人（[2022c](#bib.bib63)）使用了像RoBERTa和JobBERT这样的语言模型，将职位发布中的n-grams与ESCO分类法进行匹配。他们还实验了上下文和频率感知的嵌入。Gnehm等人（[2022a](#bib.bib21)）使用上下文感知的嵌入和与Zhang等人（[2022c](#bib.bib63)）类似的SBERT模型进行直接技能提取，此外，他们还使用层次结构在范围和本体术语中对技能领域进行上下文化。该研究探索了增强BERT模型相似性的技术，包括领域内预训练、基于变换器的序列去噪自编码器（TSDAE；Wang等人，[2021](#bib.bib57)）用于领域特定术语，以及用于训练句子嵌入的Siamese
    BERT网络（Reimers和Gurevych，[2019](#bib.bib48)）。他们进一步利用了Siamese网络中的MNR损失（Henderson等人，[2017](#bib.bib30)），使用本体数据创建正文本对以更好地匹配标签。SkillGPT
    Li等人（[2023](#bib.bib39)）是第一个使用LLM进行匹配任务的工具，他们将ESCO条目转换为结构化文档，并由语言模型向量化。然后，他们总结输入文本，并使用总结的嵌入来检索最接近的ESCO条目。'
- en: 7.2.2 Extreme Multi-label Classification Approaches
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2 极端多标签分类方法
- en: Bhola et al. ([2020](#bib.bib5)) were the first to formulate skill extraction
    against a skill base as an extreme multi-label classification (XMLC). They classify
    multiple skill labels per document using the labels of the BHOLA dataset (around
    2500 labels) as a skill base. Their BERT–XMLC framework, involves a Text Encoder
    that uses the pre-trained BERTbase model to convert JP texts into dense vector
    representations, a Bottleneck Layer that reduces overfitting by compressing these
    representations (Liu et al., [2017](#bib.bib41)) and subsequently a fully connected
    layer for multi-label classification of the skills. Enhancements include focusing
    on semantic skill label representation and skill co-occurrence, using bootstrapping
    to augment training data, and improve skill correlation capture. Their model outperformed
    XMLC baselines. Vermeer et al. ([2022](#bib.bib56)) adapted this approach for
    using RobBERT and additional linear layers, validating on BHOLA and a non-public
    Dutch dataset. Similarly, Anand et al. ([2022](#bib.bib1)) extended the model
    to predict skill importance using LaBSE-encoded Feng et al. ([2022](#bib.bib20))
    job titles, ranking skills from an in-house database based on a 0-1 scale of importance.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Bhola 等人 ([2020](#bib.bib5)) 首次将技能提取对技能库的极端多标签分类（XMLC）进行公式化。他们使用 BHOLA 数据集的标签（约
    2500 个标签）作为技能库，对每个文档进行多标签分类。他们的 BERT–XMLC 框架包括一个文本编码器，使用预训练的 BERTbase 模型将 JP 文本转换为密集的向量表示，一个瓶颈层通过压缩这些表示来减少过拟合（Liu
    等人，[2017](#bib.bib41)），然后是一个全连接层用于技能的多标签分类。改进包括关注语义技能标签表示和技能共现，使用自助法增强训练数据，并改进技能关联捕捉。他们的模型优于
    XMLC 基准。Vermeer 等人 ([2022](#bib.bib56)) 采用了这种方法，使用 RobBERT 和额外的线性层，在 BHOLA 和一个非公开的荷兰数据集上进行了验证。同样，Anand
    等人 ([2022](#bib.bib1)) 将模型扩展到使用 LaBSE 编码的 Feng 等人 ([2022](#bib.bib20)) 职位标题来预测技能重要性，根据
    0-1 重要性尺度从内部数据库中对技能进行排序。
- en: Subsequent publications have concentrated on XMLC for skill extraction and classification
    using the ESCO taxonomy with around 13000 labels. For a pure skill classification
    for already identified skill spans Zhang et al. ([2022b](#bib.bib62)) use distant
    supervision by querying the ESCO API for the fine-grained skill labels. For model
    training, they employ zero-shot cross-lingual transfer learning techniques using
    various BERT models and fine-tune them on Danish JPs. The effectiveness of the
    models is tested on an adapted version of SKILLSPAN and KOMPETENCER. The same
    year Decorte et al. ([2022](#bib.bib16)) addressed the XMLC task on the sentence-level,
    again using distant supervision with the ESCO taxonomy. They enhance binary skill
    classifier training with three negative sampling strategies, involving siblings
    in ESCO hierarchy, Levenshtein distance, and cosine similarity of RoBERTa-encoded
    skill names. Their model employs a frozen pre-trained RoBERTa with mean pooling
    for sentence representation, followed by separate binary classifiers for each
    skill, evaluated on DECORTE.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 随后的出版物集中于使用 ESCO 分类法进行技能提取和分类，标签约为 13000 个。对于已经识别的技能范围的纯技能分类，Zhang 等人 ([2022b](#bib.bib62))
    通过查询 ESCO API 来进行远程监督，以获得细粒度的技能标签。在模型训练中，他们采用零样本跨语言迁移学习技术，使用各种 BERT 模型，并在丹麦 JPs
    上进行微调。这些模型的有效性在适应版的 SKILLSPAN 和 KOMPETENCER 上进行了测试。同年，Decorte 等人 ([2022](#bib.bib16))
    针对句子级别的 XMLC 任务进行了研究，再次使用 ESCO 分类法进行远程监督。他们通过三种负采样策略来增强二分类技能分类器的训练，包括 ESCO 层次中的兄弟节点、Levenshtein
    距离和 RoBERTa 编码技能名称的余弦相似度。他们的模型采用了冻结的预训练 RoBERTa 进行句子表示的均值池化，然后是针对每个技能的单独二分类器，在
    DECORTE 上进行了评估。
- en: 'As for the similarity-based approaches, LLMs are prominent in recent XMLC approaches.
    Unlike Li et al. ([2023](#bib.bib39)), Decorte et al. ([2023](#bib.bib17)) use
    the LLM solely during training to reduce latency and enhance reproducibility.
    They create a synthetic training dataset using the LLM, then optimize a bi-encoder
    through contrastive training, to effectively represent both skill names and corresponding
    sentences in close proximity within the same space. This method outperforms the
    distance supervision baseline by Decorte et al. ([2022](#bib.bib16)) (see Table [5](#A1.T5
    "Table 5 ‣ A.3 Scores of Selected Models ‣ Appendix A Appendix ‣ Deep Learning-based
    Computational Job Market Analysis: A Survey on Skill Extraction and Classification
    from Job Postings")). Similarly, Clavié and Soulié ([2023](#bib.bib12)) treat
    the skill extraction and classification task as individual binary classification
    problems, using GPT-3.5 like Decorte et al. ([2023](#bib.bib17)) but generating
    more spans per skill for synthetic training. They propose two extraction methods:
    one using linear classifiers for each skill, employing hard negative sampling
    (Robinson et al., [2021](#bib.bib49)) for improved skill differentiation, and
    another based on similarity, utilizing E5-LARGE-V2 embeddings (Wang et al., [2022](#bib.bib58))
    for cosine similarity calculations between JP extracts and ESCO labels or synthetic
    sentences. Potential skills are then reranked using an LLM. In evaluations using
    the DECORTE dataset, their methods achieved high performance with GPT-4, though
    results with GPT-3.5 were lower than Decorte et al. ([2023](#bib.bib17)), see
    Table [5](#A1.T5 "Table 5 ‣ A.3 Scores of Selected Models ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings") in the Appendix.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '就基于相似度的方法而言，LLM 在近期的 XMLC 方法中非常突出。与 Li 等人 ([2023](#bib.bib39)) 不同，Decorte 等人
    ([2023](#bib.bib17)) 仅在训练过程中使用 LLM，以减少延迟并增强可重复性。他们使用 LLM 创建一个合成训练数据集，然后通过对比训练优化一个双编码器，有效地在同一空间中将技能名称和相应的句子表示得更接近。这种方法优于
    Decorte 等人 ([2022](#bib.bib16)) 的距离监督基线（见附录中的表格 [5](#A1.T5 "Table 5 ‣ A.3 Scores
    of Selected Models ‣ Appendix A Appendix ‣ Deep Learning-based Computational Job
    Market Analysis: A Survey on Skill Extraction and Classification from Job Postings")）。类似地，Clavié
    和 Soulié ([2023](#bib.bib12)) 将技能提取和分类任务视为单独的二分类问题，使用 GPT-3.5 如同 Decorte 等人 ([2023](#bib.bib17))，但为合成训练生成更多的跨度。他们提出了两种提取方法：一种是针对每个技能使用线性分类器，采用硬负样本采样（Robinson
    等人，[2021](#bib.bib49)）以改进技能区分；另一种是基于相似度，利用 E5-LARGE-V2 嵌入（Wang 等人，[2022](#bib.bib58)）进行
    JP 提取和 ESCO 标签或合成句子之间的余弦相似度计算。潜在技能随后通过 LLM 进行重新排序。在使用 DECORTE 数据集的评估中，他们的方法在 GPT-4
    上取得了高性能，尽管 GPT-3.5 的结果低于 Decorte 等人 ([2023](#bib.bib17))，详见附录中的表格 [5](#A1.T5 "Table
    5 ‣ A.3 Scores of Selected Models ‣ Appendix A Appendix ‣ Deep Learning-based
    Computational Job Market Analysis: A Survey on Skill Extraction and Classification
    from Job Postings")。'
- en: 'Goyal et al. ([2023](#bib.bib24)) present JobXMLC, a unique framework for the
    XMLC task, distinct from the prevailing methods. JobXMLC integrates a job-skill
    graph to represent job-skill interconnections, utilizes a GNN for multi-hop embeddings
    from the graph’s structure, and incorporates an extreme classification system
    with skill attention based on skill frequency in the dataset. The framework’s
    effectiveness is validated on the BHOLA and a proprietary StackOverflow dataset,
    see Table [5](#A1.T5 "Table 5 ‣ A.3 Scores of Selected Models ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings") in the Appendix.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 'Goyal 等人 ([2023](#bib.bib24)) 提出了 JobXMLC，这是一个独特的 XMLC 任务框架，与现有方法不同。 JobXMLC
    整合了一个工作-技能图来表示工作和技能之间的联系，利用 GNN 从图的结构中进行多跳嵌入，并结合了基于数据集中的技能频率的技能注意力极端分类系统。该框架的有效性在
    BHOLA 和一个专有的 StackOverflow 数据集上得到了验证，详见附录中的表格 [5](#A1.T5 "Table 5 ‣ A.3 Scores
    of Selected Models ‣ Appendix A Appendix ‣ Deep Learning-based Computational Job
    Market Analysis: A Survey on Skill Extraction and Classification from Job Postings")。'
- en: 8 Conclusions and Future Directions
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论和未来方向
- en: Recent publications indicate two emerging trends in skill extraction. Firstly,
    extracting skills against skill bases like ESCO is gaining popularity, facilitating
    cross-industry and regional comparisons. Secondly, LLMs are increasingly applied
    in skill extraction and classification, proving particularly advantageous due
    to the scarcity of training data in this domain.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的出版物表明了技能提取中的两个新兴趋势。首先，基于技能库（如 ESCO）提取技能正越来越受欢迎，这有助于跨行业和地区的比较。其次，LLM 在技能提取和分类中的应用越来越多，尤其由于该领域训练数据稀缺，LLM
    显得特别有利。
- en: Future research in skill extraction and classification could focus on emerging
    skills and the extraction of implicit skills. Methods like those by Javed et al.
    ([2017](#bib.bib31)) and Khaouja et al. ([2021b](#bib.bib35)) update skill bases
    with emerging technologies and frequently used keywords, but evaluating these
    remains difficult without a standard benchmark. The challenge of extracting implicit
    skills, not directly stated in job postings, is also gaining attention. Techniques
    include prompting LLMs to generate training data with implied skills (Clavié and
    Soulié, [2023](#bib.bib12)) and using complete sentences to encompass both explicit
    and implicit skills (Decorte et al., [2022](#bib.bib16), [2023](#bib.bib17)).
    However, these methods need thorough evaluation, presenting an open field for
    future exploration.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 未来在技能提取和分类方面的研究可以聚焦于新兴技能和隐性技能的提取。像 Javed 等人（[2017](#bib.bib31)）和 Khaouja 等人（[2021b](#bib.bib35)）的方法通过新兴技术和常用关键词更新技能库，但在没有标准基准的情况下评估这些方法仍然困难。隐性技能的提取（即在职位描述中未直接陈述的技能）也受到关注。技术包括引导大型语言模型生成隐性技能的训练数据（Clavié
    和 Soulié，[2023](#bib.bib12)）和使用完整句子涵盖显性和隐性技能（Decorte 等人，[2022](#bib.bib16)，[2023](#bib.bib17)）。然而，这些方法需要彻底评估，为未来探索提供了开放领域。
- en: Limitations
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: A limitation that should be considered is that only publications in the English
    language (although data was from multiple languages) were surveyed in this paper.
    Second, to allow for a deeper focus publications regarding topic modeling were
    excluded even if they used deep-learning-based methods.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的一个限制是，本论文仅调查了英文出版物（尽管数据来源于多种语言）。其次，为了更深入地聚焦于话题建模的出版物被排除，即使它们使用了基于深度学习的方法。
- en: Acknowledgements
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank the reviewers for their insightful feedback. ES acknowledges financial
    support with funds provided by the German Federal Ministry for Economic Affairs
    and Climate Action due to an enactment of the German Bundestag under grant 46SKD127X
    (GENESIS). MZ is supported by the Independent Research Fund Denmark (DFF) grant
    9131-00019B and BP is supported by ERC Consolidator Grant DIALECT 101043235.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢审稿人提供的有见地的反馈。ES 感谢德国联邦经济事务和气候行动部提供的资金支持，这些资金是根据德国联邦议院通过的第46SKD127X号（GENESIS）拨款提供的。MZ
    得到了丹麦独立研究基金（DFF）9131-00019B号资助的支持，BP 则得到了 ERC Consolidator Grant DIALECT 101043235
    的资助。
- en: References
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Anand et al. (2022) Sarthak Anand, Jens-Joris Decorte, and Niels Lowie. 2022.
    [Is it required? ranking the skills required for a job-title](http://arxiv.org/abs/2212.08553).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anand 等人（2022年）Sarthak Anand、Jens-Joris Decorte 和 Niels Lowie。2022年。[这是必需的吗？为职位名称排名所需的技能](http://arxiv.org/abs/2212.08553)。
- en: 'Ao et al. (2023) Ziqiao Ao, Gergely Horváth, Chunyuan Sheng, Yifan Song, and
    Yutong Sun. 2023. [Skill requirements in job advertisements: A comparison of skill-categorization
    methods based on wage regressions](https://doi.org/https://doi.org/10.1016/j.ipm.2022.103185).
    *Information Processing & Management*, 60(2):103185.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ao 等人（2023年）Ziqiao Ao、Gergely Horváth、Chunyuan Sheng、Yifan Song 和 Yutong Sun。2023年。[职位广告中的技能要求：基于工资回归的技能分类方法比较](https://doi.org/https://doi.org/10.1016/j.ipm.2022.103185)。*Information
    Processing & Management*，60(2):103185。
- en: 'Beauchemin et al. (2022) David Beauchemin, Julien Laumonier, Yvan Le Ster,
    and Marouane Yassine. 2022. ["fijo": a french insurance soft skill detection dataset](https://arxiv.org/abs/2204.05208).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beauchemin 等人（2022年）David Beauchemin、Julien Laumonier、Yvan Le Ster 和 Marouane
    Yassine。2022年。[“fijo”：一个法语保险软技能检测数据集](https://arxiv.org/abs/2204.05208)。
- en: 'Bernabé-Moreno et al. (2019) Juan Bernabé-Moreno, Álvaro Tejeda-Lorente, Julio
    Herce-Zelaya, Carlos Porcel, and Enrique Herrera-Viedma. 2019. [An automatic skills
    standardization method based on subject expert knowledge extraction and semantic
    matching](https://doi.org/https://doi.org/10.1016/j.procs.2019.12.060). *Procedia
    Computer Science*, 162:857–864. 7th International Conference on Information Technology
    and Quantitative Management (ITQM 2019): Information technology and quantitative
    management based on Artificial Intelligence.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bernabé-Moreno 等人（2019年）Juan Bernabé-Moreno、Álvaro Tejeda-Lorente、Julio Herce-Zelaya、Carlos
    Porcel 和 Enrique Herrera-Viedma。2019年。[一种基于主题专家知识提取和语义匹配的自动技能标准化方法](https://doi.org/https://doi.org/10.1016/j.procs.2019.12.060)。*Procedia
    Computer Science*，162:857–864。第七届国际信息技术与定量管理会议（ITQM 2019）：基于人工智能的信息技术与定量管理。
- en: 'Bhola et al. (2020) Akshay Bhola, Kishaloy Halder, Animesh Prasad, and Min-Yen
    Kan. 2020. [Retrieving skills from job descriptions: A language model based extreme
    multi-label classification framework](https://doi.org/10.18653/v1/2020.coling-main.513).
    In *Proceedings of the 28th International Conference on Computational Linguistics*,
    pages 5832–5842, Barcelona, Spain (Online). International Committee on Computational
    Linguistics.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhola等人（2020）Akshay Bhola、Kishaloy Halder、Animesh Prasad和Min-Yen Kan。2020年。[从职位描述中检索技能：基于语言模型的极端多标签分类框架](https://doi.org/10.18653/v1/2020.coling-main.513)。在*第28届国际计算语言学会议论文集*，第5832–5842页，西班牙巴塞罗那（线上）。国际计算语言学委员会。
- en: Bojanowski et al. (2017) Piotr Bojanowski, Edouard Grave, Armand Joulin, and
    Tomas Mikolov. 2017. [Enriching word vectors with subword information](https://doi.org/10.1162/tacl_a_00051).
    *Transactions of the Association for Computational Linguistics*, 5:135–146.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bojanowski等人（2017）Piotr Bojanowski、Edouard Grave、Armand Joulin和Tomas Mikolov。2017年。[通过子词信息丰富词向量](https://doi.org/10.1162/tacl_a_00051)。*计算语言学协会会刊*，5:135–146。
- en: Boselli et al. (2018) Roberto Boselli, Mirko Cesarini, Fabio Mercorio, and Mario
    Mezzanzanica. 2018. [Classifying online job advertisements through machine learning](https://doi.org/https://doi.org/10.1016/j.future.2018.03.035).
    *Future Generation Computer Systems*, 86:319–328.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boselli等人（2018）Roberto Boselli、Mirko Cesarini、Fabio Mercorio和Mario Mezzanzanica。2018年。[通过机器学习分类在线职位广告](https://doi.org/https://doi.org/10.1016/j.future.2018.03.035)。*未来一代计算机系统*，86:319–328。
- en: Botov et al. (2019) Dmitriy Botov, Julius Klenin, Andrey Melnikov, Yuri Dmitrin,
    Ivan Nikolaev, and Mikhail Vinel. 2019. [Mining labor market requirements using
    distributional semantic models and deep learning](https://doi.org/10.1007/978-3-030-20482-2_15).
    In *Business Information Systems - 22nd International Conference, BIS 2019, Seville,
    Spain, June 26-28, 2019, Proceedings, Part II*, volume 354 of *Lecture Notes in
    Business Information Processing*, pages 177–190\. Springer.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Botov等人（2019）Dmitriy Botov、Julius Klenin、Andrey Melnikov、Yuri Dmitrin、Ivan Nikolaev和Mikhail
    Vinel。2019年。[使用分布式语义模型和深度学习挖掘劳动市场需求](https://doi.org/10.1007/978-3-030-20482-2_15)。在*商业信息系统
    - 第22届国际会议，BIS 2019，西班牙塞维利亚，2019年6月26-28日，会议记录，第二部分*，第354卷的*商业信息处理讲义*，第177–190页。施普林格。
- en: 'Buchmann et al. (2022) Marlis Buchmann, Helen Buchs, Felix Busch, Simon Clematide,
    Ann-Sophie Gnehm, and Jan Müller. 2022. Swiss job market monitor: A rich source
    of demand-side micro data of the labour market. *European Sociological Review*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buchmann等人（2022）Marlis Buchmann、Helen Buchs、Felix Busch、Simon Clematide、Ann-Sophie
    Gnehm和Jan Müller。2022年。瑞士劳动力市场监测器：丰富的需求方微观数据来源。*欧洲社会学评论*。
- en: Cao and Zhang (2021) Lina Cao and Jian Zhang. 2021. [Skill requirements analysis
    for data analysts based on named entities recognition](https://doi.org/10.1109/ICBDIE52740.2021.00023).
    In *2021 2nd International Conference on Big Data and Informatization Education
    (ICBDIE)*, pages 64–68.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao和Zhang（2021）Lina Cao和Jian Zhang。2021年。[基于命名实体识别的数据分析师技能需求分析](https://doi.org/10.1109/ICBDIE52740.2021.00023)。在*2021年第二届国际大数据与信息化教育会议（ICBDIE）*，第64–68页。
- en: 'Cao et al. (2021) Lina Cao, Jian Zhang, Xinquan Ge, and Jindong Chen. 2021.
    [Occupational profiling driven by online job advertisements: Taking the data analysis
    and processing engineering technicians as an example](https://api.semanticscholar.org/CorpusID:235609409).
    *PLoS ONE*, 16.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等人（2021）Lina Cao、Jian Zhang、Xinquan Ge和Jindong Chen。2021年。[基于在线职位广告的职业画像：以数据分析与处理工程技术员为例](https://api.semanticscholar.org/CorpusID:235609409)。*PLoS
    ONE*，16。
- en: Clavié and Soulié (2023) Benjamin Clavié and Guillaume Soulié. 2023. [Large
    language models as batteries-included zero-shot esco skills matchers](https://arxiv.org/abs/2307.03539).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clavié和Soulié（2023）Benjamin Clavié和Guillaume Soulié。2023年。[大型语言模型作为内置零样本ESCO技能匹配器](https://arxiv.org/abs/2307.03539)。
- en: 'Colombo et al. (2019) Emilio Colombo, Fabio Mercorio, and Mario Mezzanzanica.
    2019. [Ai meets labor market: Exploring the link between automation and skills](https://doi.org/https://doi.org/10.1016/j.infoecopol.2019.05.003).
    *Information Economics and Policy*, 47:27–37. The Economics of Artificial Intelligence
    and Machine Learning.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colombo等人（2019）Emilio Colombo、Fabio Mercorio和Mario Mezzanzanica。2019年。[人工智能与劳动力市场的结合：探索自动化与技能之间的联系](https://doi.org/https://doi.org/10.1016/j.infoecopol.2019.05.003)。*信息经济与政策*，47:27–37。人工智能与机器学习的经济学。
- en: Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
    Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,
    and Veselin Stoyanov. 2020. [Unsupervised cross-lingual representation learning
    at scale](https://doi.org/10.18653/v1/2020.acl-main.747). In *Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics*, pages 8440–8451,
    Online. Association for Computational Linguistics.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conneau 等（2020）亚历克西斯·孔诺、卡尔提凯·坎德尔瓦尔、纳曼·戈亚尔、维什拉夫·乔杜里、吉约姆·温泽克、弗朗西斯科·古兹曼、爱德华·格拉夫、迈尔·奥特、卢克·泽特尔莫耶和维塞林·斯托扬诺夫。2020年。[大规模无监督跨语言表示学习](https://doi.org/10.18653/v1/2020.acl-main.747)。在*第58届计算语言学协会年会论文集*，第8440–8451页，在线。计算语言学协会。
- en: 'Council et al. (2010) National Research Council, Nancy Thomas Tippins, Margaret L
    Hilton, et al. 2010. *A database for a changing economy: Review of the Occupational
    Information Network (O* NET)*. National Academies Press.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国国家研究委员会等（2010）国家研究委员会、南希·托马斯·蒂平斯、玛格丽特·L·希尔顿等。2010年。*变化经济中的数据库：职业信息网络（O* NET）评审*。国家科学院出版社。
- en: Decorte et al. (2022) Jens-Joris Decorte, Jeroen Van Hautte, Johannes Deleu,
    Chris Develder, and Thomas Demeester. 2022. [Design of negative sampling strategies
    for distantly supervised skill extraction](https://ceur-ws.org/Vol-3218/RecSysHR2022-paper_4.pdf).
    In *Proceedings of the 2nd Workshop on Recommender Systems for Human Resources
    (RecSys-in-HR 2022)*, volume 3218, page 7\. CEUR.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 德科特等（2022）延斯-乔里斯·德科特、耶伦·范·豪特、约翰内斯·德勒、克里斯·德维尔德和托马斯·德梅斯特。2022年。[设计用于远程监督技能提取的负采样策略](https://ceur-ws.org/Vol-3218/RecSysHR2022-paper_4.pdf)。在*第2届人力资源推荐系统研讨会（RecSys-in-HR
    2022）论文集*，第3218卷，第7页。CEUR。
- en: Decorte et al. (2023) Jens-Joris Decorte, Severine Verlinden, Jeroen Van Hautte,
    Johannes Deleu, Chris Develder, and Thomas Demeester. 2023. [Extreme multi-label
    skill extraction training using large language models](https://arxiv.org/abs/2307.10778).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 德科特等（2023）延斯-乔里斯·德科特、塞维琳·费尔林登、耶伦·范·豪特、约翰内斯·德勒、克里斯·德维尔德和托马斯·德梅斯特。2023年。[使用大型语言模型进行极端多标签技能提取训练](https://arxiv.org/abs/2307.10778)。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: Pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/N19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 德夫林等（2019）雅各布·德夫林、明-魏·张、肯顿·李和克里斯蒂娜·图塔诺娃。2019年。[BERT：深度双向变换器的预训练用于语言理解](https://doi.org/10.18653/v1/N19-1423)。在*第2019届北美计算语言学协会会议：人类语言技术（长短篇论文卷）*，第4171–4186页，美国明尼阿波利斯。计算语言学协会。
- en: 'Fang et al. (2023) Chuyu Fang, Chuan Qin, Qi Zhang, Kaichun Yao, Jingshuai
    Zhang, Hengshu Zhu, Fuzhen Zhuang, and Hui Xiong. 2023. [Recruitpro: A pretrained
    language model with skill-aware prompt learning for intelligent recruitment](https://doi.org/10.1145/3580305.3599894).
    In *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*, KDD ’23, page 3991–4002, New York, NY, USA. Association for Computing
    Machinery.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方等（2023）方楚宇、秦川、张奇、姚凯春、张敬帅、朱恒树、庄福真和熊辉。2023年。[Recruitpro：一种具有技能感知提示学习的预训练语言模型，用于智能招聘](https://doi.org/10.1145/3580305.3599894)。在*第29届ACM
    SIGKDD知识发现与数据挖掘会议论文集*，KDD ’23，第3991–4002页，美国纽约。计算机协会。
- en: 'Feng et al. (2022) Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan,
    and Wei Wang. 2022. [Language-agnostic BERT sentence embedding](https://doi.org/10.18653/v1/2022.acl-long.62).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 878–891, Dublin, Ireland. Association
    for Computational Linguistics.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冯等（2022）冯方晓宇、杨银飞、丹尼尔·瑟尔、纳文·阿里瓦扎根和王伟。2022年。[语言无关的BERT句子嵌入](https://doi.org/10.18653/v1/2022.acl-long.62)。在*第60届计算语言学协会年会论文集（第1卷：长篇论文）*，第878–891页，爱尔兰都柏林。计算语言学协会。
- en: Gnehm et al. (2022a) Ann-sophie Gnehm, Eva Bühlmann, Helen Buchs, and Simon
    Clematide. 2022a. [Fine-grained extraction and classification of skill requirements
    in German-speaking job ads](https://aclanthology.org/2022.nlpcss-1.2). In *Proceedings
    of the Fifth Workshop on Natural Language Processing and Computational Social
    Science (NLP+CSS)*, pages 14–24, Abu Dhabi, UAE. Association for Computational
    Linguistics.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gnehm等（2022a）Ann-sophie Gnehm, Eva Bühlmann, Helen Buchs, 和 Simon Clematide。2022a。[德语招聘广告中技能要求的细粒度提取与分类](https://aclanthology.org/2022.nlpcss-1.2)。在*第五届自然语言处理与计算社会科学研讨会（NLP+CSS）论文集*中，页码14–24，阿布扎比，阿联酋。计算语言学协会。
- en: Gnehm et al. (2022b) Ann-Sophie Gnehm, Eva Bühlmann, and Simon Clematide. 2022b.
    [Evaluation of transfer learning and domain adaptation for analyzing German-speaking
    job advertisements](https://aclanthology.org/2022.lrec-1.414). In *Proceedings
    of the Thirteenth Language Resources and Evaluation Conference*, pages 3892–3901,
    Marseille, France. European Language Resources Association.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gnehm等（2022b）Ann-Sophie Gnehm, Eva Bühlmann, 和 Simon Clematide。2022b。[用于分析德语招聘广告的迁移学习和领域适应的评估](https://aclanthology.org/2022.lrec-1.414)。在*第十三届语言资源与评估会议论文集*中，页码3892–3901，法国马赛。欧洲语言资源协会。
- en: Gnehm and Clematide (2020) Ann-Sophie Gnehm and Simon Clematide. 2020. [Text
    zoning and classification for job advertisements in German, French and English](https://doi.org/10.18653/v1/2020.nlpcss-1.10).
    In *Proceedings of the Fourth Workshop on Natural Language Processing and Computational
    Social Science*, pages 83–93, Online. Association for Computational Linguistics.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gnehm和Clematide（2020）Ann-Sophie Gnehm 和 Simon Clematide。2020。[德语、法语和英语招聘广告的文本分区和分类](https://doi.org/10.18653/v1/2020.nlpcss-1.10)。在*第四届自然语言处理与计算社会科学研讨会论文集*中，页码83–93，在线。计算语言学协会。
- en: 'Goyal et al. (2023) Nidhi Goyal, Jushaan Kalra, Charu Sharma, Raghava Mutharaju,
    Niharika Sachdeva, and Ponnurangam Kumaraguru. 2023. [JobXMLC: EXtreme multi-label
    classification of job skills with graph neural networks](https://aclanthology.org/2023.findings-eacl.163).
    In *Findings of the Association for Computational Linguistics: EACL 2023*, pages
    2181–2191, Dubrovnik, Croatia. Association for Computational Linguistics.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal等（2023）Nidhi Goyal, Jushaan Kalra, Charu Sharma, Raghava Mutharaju, Niharika
    Sachdeva, 和 Ponnurangam Kumaraguru。2023。[JobXMLC：利用图神经网络对职位技能进行极端多标签分类](https://aclanthology.org/2023.findings-eacl.163)。在*计算语言学协会：EACL
    2023发现*中，页码2181–2191，克罗地亚杜布罗夫尼克。计算语言学协会。
- en: Green et al. (2022) Thomas Green, Diana Maynard, and Chenghua Lin. 2022. [Development
    of a benchmark corpus to support entity recognition in job descriptions](https://aclanthology.org/2022.lrec-1.128).
    In *Proceedings of the Thirteenth Language Resources and Evaluation Conference*,
    pages 1201–1208, Marseille, France. European Language Resources Association.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Green等（2022）Thomas Green, Diana Maynard, 和 Chenghua Lin。2022。[支持职位描述中实体识别的基准语料库的开发](https://aclanthology.org/2022.lrec-1.128)。在*第十三届语言资源与评估会议论文集*中，页码1201–1208，法国马赛。欧洲语言资源协会。
- en: Grüger and Schneider (2019) Joscha Grüger and Georg Schneider. 2019. [Automated
    analysis of job requirements for computer scientists in online job advertisements](https://doi.org/10.5220/0008068202260233).
    In *Proceedings of the 15th International Conference on Web Information Systems
    and Technologies*, WEBIST 2019, page 226–233, Setubal, PRT. SCITEPRESS - Science
    and Technology Publications, Lda.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grüger和Schneider（2019）Joscha Grüger 和 Georg Schneider。2019。[在线招聘广告中计算机科学家职位要求的自动化分析](https://doi.org/10.5220/0008068202260233)。在*第十五届国际网络信息系统与技术会议论文集*中，WEBIST
    2019，页码226–233，葡萄牙塞图巴尔。SCITEPRESS - 科学与技术出版公司。
- en: Gugnani and Misra (2020) Akshay Gugnani and Hemant Misra. 2020. [Implicit skills
    extraction using document embedding and its use in job recommendation](https://aaai.org/ojs/index.php/AAAI/article/view/7038).
    In *The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
    Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
    2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2020, New York, NY, USA, February 7-12, 2020*, pages 13286–13293\. AAAI Press.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gugnani和Misra（2020）Akshay Gugnani 和 Hemant Misra。2020。[使用文档嵌入进行隐性技能提取及其在职位推荐中的应用](https://aaai.org/ojs/index.php/AAAI/article/view/7038)。在*第三十四届AAAI人工智能大会，AAAI
    2020，第三十二届人工智能创新应用大会，IAAI 2020，第十届AAAI教育进展研讨会，EAAI 2020，美国纽约，2020年2月7-12日*中，页码13286–13293。AAAI出版社。
- en: 'Gururangan et al. (2020) Suchin Gururangan, Ana Marasović, Swabha Swayamdipta,
    Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. [Don’t stop pretraining:
    Adapt language models to domains and tasks](https://doi.org/10.18653/v1/2020.acl-main.740).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8342–8360, Online. Association for Computational Linguistics.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gururangan et al. (2020) Suchin Gururangan, Ana Marasović, Swabha Swayamdipta,
    Kyle Lo, Iz Beltagy, Doug Downey, 和 Noah A. Smith. 2020. [不要停止预训练：将语言模型适应领域和任务](https://doi.org/10.18653/v1/2020.acl-main.740)。在*第58届计算语言学协会年会论文集*中，第8342–8360页，在线。计算语言学协会。
- en: 'HarperCollins Publishers (2023) HarperCollins Publishers. 2023. Collins COBUILD
    Advanced Learner’s Dictionary: Soft Skills. [https://www.collinsdictionary.com/dictionary/english/soft-skills](https://www.collinsdictionary.com/dictionary/english/soft-skills).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HarperCollins Publishers (2023) HarperCollins Publishers. 2023. Collins COBUILD高级学习者词典：软技能。
    [https://www.collinsdictionary.com/dictionary/english/soft-skills](https://www.collinsdictionary.com/dictionary/english/soft-skills).
- en: Henderson et al. (2017) Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun hsuan
    Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil.
    2017. [Efficient natural language response suggestion for smart reply](http://arxiv.org/abs/1705.00652).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson et al. (2017) Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun hsuan
    Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, 和 Ray Kurzweil. 2017.
    [智能回复的高效自然语言响应建议](http://arxiv.org/abs/1705.00652).
- en: Javed et al. (2017) Faizan Javed, Phuong Hoang, Thomas Mahoney, and Matt McNair.
    2017. Large-scale occupational skills normalization for online recruitment. In
    *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 31, pages
    4627–4634.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Javed et al. (2017) Faizan Javed, Phuong Hoang, Thomas Mahoney, 和 Matt McNair.
    2017. 大规模职业技能规范化以支持在线招聘。在*AAAI人工智能会议论文集*中，第31卷，第4627–4634页.
- en: Jia et al. (2018) Shanshan Jia, Xiaoan Liu, Ping Zhao, Chang Liu, Lianying Sun,
    and Tao Peng. 2018. Representation of job-skill in artificial intelligence with
    knowledge graph analysis. In *2018 IEEE symposium on product compliance engineering-asia
    (ISPCE-CN)*, pages 1–6\. IEEE.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia et al. (2018) Shanshan Jia, Xiaoan Liu, Ping Zhao, Chang Liu, Lianying Sun,
    和 Tao Peng. 2018. 人工智能中的职业技能表示与知识图谱分析。在*2018 IEEE产品合规工程亚洲研讨会（ISPCE-CN）*中，第1–6页。IEEE.
- en: 'Joshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke
    Zettlemoyer, and Omer Levy. 2020. [SpanBERT: Improving pre-training by representing
    and predicting spans](https://doi.org/10.1162/tacl_a_00300). *Transactions of
    the Association for Computational Linguistics*, 8:64–77.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke
    Zettlemoyer, 和 Omer Levy. 2020. [SpanBERT：通过表示和预测跨度来改进预训练](https://doi.org/10.1162/tacl_a_00300)。*计算语言学协会会刊*,
    8:64–77.
- en: Khaouja et al. (2021a) Imane Khaouja, Ismail Kassou, and Mounir Ghogho. 2021a.
    [A survey on skill identification from online job ads](https://doi.org/10.1109/ACCESS.2021.3106120).
    *IEEE Access*, 9:118134–118153.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khaouja et al. (2021a) Imane Khaouja, Ismail Kassou, 和 Mounir Ghogho. 2021a.
    [从在线招聘广告中识别技能的调查](https://doi.org/10.1109/ACCESS.2021.3106120)。*IEEE Access*,
    9:118134–118153.
- en: Khaouja et al. (2021b) Imane Khaouja, Ghita Mezzour, and Ismail Kassou. 2021b.
    [Unsupervised skill identification from job ads](https://doi.org/10.1109/IRI51335.2021.00026).
    In *2021 IEEE 22nd International Conference on Information Reuse and Integration
    for Data Science (IRI)*, pages 147–151.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khaouja et al. (2021b) Imane Khaouja, Ghita Mezzour, 和 Ismail Kassou. 2021b.
    [从招聘广告中无监督技能识别](https://doi.org/10.1109/IRI51335.2021.00026)。在*2021 IEEE第22届信息重用与数据科学整合国际会议（IRI）*中，第147–151页.
- en: 'Kortum et al. (2022) Henrik Kortum, Jonas Rebstadt, and Oliver Thomas. 2022.
    [Dissection of AI job advertisements: A text mining-based analysis of employee
    skills in the disciplines computer vision and natural language processing](http://hdl.handle.net/10125/79973).
    In *55th Hawaii International Conference on System Sciences, HICSS 2022, Virtual
    Event / Maui, Hawaii, USA, January 4-7, 2022*, pages 1–10\. ScholarSpace.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kortum et al. (2022) Henrik Kortum, Jonas Rebstadt, 和 Oliver Thomas. 2022. [AI招聘广告的解剖：基于文本挖掘的计算机视觉和自然语言处理领域员工技能分析](http://hdl.handle.net/10125/79973)。在*第55届夏威夷国际系统科学会议，HICSS
    2022，虚拟活动 / 美国夏威夷毛伊岛，2022年1月4-7日*中，第1–10页。ScholarSpace.
- en: 'Lample et al. (2016) Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian,
    Kazuya Kawakami, and Chris Dyer. 2016. [Neural architectures for named entity
    recognition](https://doi.org/10.18653/v1/N16-1030). In *Proceedings of the 2016
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 260–270, San Diego, California.
    Association for Computational Linguistics.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lample 等（2016）Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya
    Kawakami 和 Chris Dyer。2016年。[命名实体识别的神经网络架构](https://doi.org/10.18653/v1/N16-1030)。在
    *2016年北美计算语言学协会：人类语言技术会议论文集*，页码 260–270，加利福尼亚州圣地亚哥。计算语言学协会。
- en: 'le Vrang et al. (2014) Martin le Vrang, Agis Papantoniou, Erika Pauwels, Pieter
    Fannes, Dominique Vandensteen, and Johan De Smedt. 2014. [Esco: Boosting job matching
    in europe with semantic interoperability](https://doi.org/10.1109/MC.2014.283).
    *Computer*, 47(10):57–64.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: le Vrang 等（2014）Martin le Vrang, Agis Papantoniou, Erika Pauwels, Pieter Fannes,
    Dominique Vandensteen 和 Johan De Smedt。2014年。[ESCO：通过语义互操作性提升欧洲的职位匹配](https://doi.org/10.1109/MC.2014.283)。*计算机*，47(10):57–64。
- en: 'Li et al. (2023) Nan Li, Bo Kang, and Tijl De Bie. 2023. [Skillgpt: a restful
    api service for skill extraction and standardization using a large language model](https://arxiv.org/abs/2304.11060).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023）Nan Li, Bo Kang 和 Tijl De Bie。2023年。[Skillgpt：用于技能提取和标准化的大型语言模型的RESTful
    API服务](https://arxiv.org/abs/2304.11060)。
- en: Li et al. (2020) Shan Li, Baoxu Shi, Jaewon Yang, Ji Yan, Shuai Wang, Fei Chen,
    and Qi He. 2020. [Deep job understanding at linkedin](https://doi.org/10.1145/3397271.3401403).
    In *Proceedings of the 43rd International ACM SIGIR conference on research and
    development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30,
    2020*, pages 2145–2148\. ACM.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）Shan Li, Baoxu Shi, Jaewon Yang, Ji Yan, Shuai Wang, Fei Chen 和 Qi
    He。2020年。[LinkedIn上的深度职位理解](https://doi.org/10.1145/3397271.3401403)。在 *第43届国际ACM
    SIGIR信息检索研究与开发大会论文集，SIGIR 2020，虚拟会议，中国，2020年7月25-30日*，页码 2145–2148。ACM。
- en: Liu et al. (2017) Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and Yiming Yang.
    2017. [Deep learning for extreme multi-label text classification](https://doi.org/10.1145/3077136.3080834).
    In *Proceedings of the 40th International ACM SIGIR Conference on Research and
    Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017*,
    pages 115–124\. ACM.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2017）Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu 和 Yiming Yang。2017年。[用于极端多标签文本分类的深度学习](https://doi.org/10.1145/3077136.3080834)。在
    *第40届国际ACM SIGIR信息检索研究与开发大会论文集，2017年8月7-11日，新宿，东京，日本*，页码 115–124。ACM。
- en: Lukauskas et al. (2023) Mantas Lukauskas, Viktorija Šarkauskaitė, Vaida Pilinkienė,
    Alina Stundziene, Andrius Grybauskas, and Jurgita Bruneckienė. 2023. [Enhancing
    skills demand understanding through job ad segmentation using nlp and clustering
    techniques](https://doi.org/10.3390/app13106119). *Applied Sciences*, 13.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lukauskas 等（2023）Mantas Lukauskas, Viktorija Šarkauskaitė, Vaida Pilinkienė,
    Alina Stundziene, Andrius Grybauskas 和 Jurgita Bruneckienė。2023年。[通过使用自然语言处理和聚类技术对招聘广告进行分割以增强技能需求理解](https://doi.org/10.3390/app13106119)。*应用科学*，13。
- en: 'Lyu and Liu (2021) Wenjing Lyu and Jin Liu. 2021. [Soft skills, hard skills:
    What matters most? evidence from job postings](https://doi.org/https://doi.org/10.1016/j.apenergy.2021.117307).
    *Applied Energy*, 300:117307.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyu 和 Liu（2021）Wenjing Lyu 和 Jin Liu。2021年。[软技能与硬技能：什么最重要？来自招聘信息的证据](https://doi.org/https://doi.org/10.1016/j.apenergy.2021.117307)。*应用能源*，300:117307。
- en: 'Martin et al. (2020) Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez,
    Yoann Dupont, Laurent Romary, Éric de la Clergerie, Djamé Seddah, and Benoît Sagot.
    2020. [CamemBERT: a tasty French language model](https://doi.org/10.18653/v1/2020.acl-main.645).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 7203–7219, Online. Association for Computational Linguistics.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martin 等（2020）Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann
    Dupont, Laurent Romary, Éric de la Clergerie, Djamé Seddah 和 Benoît Sagot。2020年。[CamemBERT：一款美味的法语语言模型](https://doi.org/10.18653/v1/2020.acl-main.645)。在
    *第58届计算语言学协会年会论文集*，页码 7203–7219，在线。计算语言学协会。
- en: Mikolov et al. (2013) Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013. [Efficient estimation of word representations in vector space](http://arxiv.org/abs/1301.3781).
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等（2013）Tomas Mikolov, Kai Chen, Greg Corrado 和 Jeffrey Dean。2013年。[在向量空间中高效估计词表示](http://arxiv.org/abs/1301.3781)。
- en: Napierala and Kvetan (2023) Joanna Napierala and Vladimir Kvetan. 2023. [*Changing
    Job Skills in a Changing World*](https://doi.org/10.1007/978-3-031-16624-2_13),
    pages 243–259\. Springer International Publishing, Cham.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Napierala 和 Kvetan（2023）Joanna Napierala 和 Vladimir Kvetan。2023年。[*在变化的世界中改变的职位技能*](https://doi.org/10.1007/978-3-031-16624-2_13)，页码243–259。Springer
    International Publishing，Cham。
- en: 'Papoutsoglou et al. (2019) Maria Papoutsoglou, Apostolos Ampatzoglou, Nikolaos
    Mittas, and Lefteris Angelis. 2019. [Extracting knowledge from on-line sources
    for software engineering labor market: A mapping study](https://doi.org/10.1109/ACCESS.2019.2949905).
    *IEEE Access*, 7:157595–157613.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papoutsoglou 等（2019）Maria Papoutsoglou、Apostolos Ampatzoglou、Nikolaos Mittas
    和 Lefteris Angelis。2019年。[从在线来源提取软件工程劳动市场的知识：一个映射研究](https://doi.org/10.1109/ACCESS.2019.2949905)。*IEEE
    Access*，7:157595–157613。
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. [Sentence-BERT:
    Sentence embeddings using Siamese BERT-networks](https://doi.org/10.18653/v1/D19-1410).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 3982–3992, Hong Kong, China. Association for Computational
    Linguistics.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers 和 Gurevych（2019）Nils Reimers 和 Iryna Gurevych。2019年。[Sentence-BERT：使用Siamese
    BERT网络的句子嵌入](https://doi.org/10.18653/v1/D19-1410)。在 *2019年自然语言处理经验方法会议及第9届国际自然语言处理联合会议（EMNLP-IJCNLP）*，页码3982–3992，香港，中国。计算语言学协会。
- en: Robinson et al. (2021) Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra,
    and Stefanie Jegelka. 2021. [Contrastive learning with hard negative samples](https://openreview.net/forum?id=CR1XOQ0UTh-).
    In *9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021*. OpenReview.net.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robinson 等（2021）Joshua David Robinson、Ching-Yao Chuang、Suvrit Sra 和 Stefanie
    Jegelka。2021年。[带有困难负样本的对比学习](https://openreview.net/forum?id=CR1XOQ0UTh-)。在 *第9届国际学习表示会议，ICLR
    2021，虚拟会议，奥地利，2021年5月3-7日*。OpenReview.net。
- en: Sayfullina et al. (2018) Luiza Sayfullina, Eric Malmi, and Juho Kannala. 2018.
    Learning representations for soft skill matching. In *Analysis of Images, Social
    Networks and Texts*, pages 141–152, Cham. Springer International Publishing.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sayfullina 等（2018）Luiza Sayfullina、Eric Malmi 和 Juho Kannala。2018年。为软技能匹配学习表示。在
    *图像、社交网络和文本分析*，页码141–152，Cham。Springer International Publishing。
- en: Shang et al. (2018) Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R.
    Voss, and Jiawei Han. 2018. [Automated phrase mining from massive text corpora](https://doi.org/10.1109/TKDE.2018.2812203).
    *IEEE Transactions on Knowledge and Data Engineering*, 30(10):1825–1837.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang 等（2018）Jingbo Shang、Jialu Liu、Meng Jiang、Xiang Ren、Clare R. Voss 和 Jiawei
    Han。2018年。[从大规模文本语料库中自动化短语挖掘](https://doi.org/10.1109/TKDE.2018.2812203)。*IEEE知识与数据工程汇刊*，30(10):1825–1837。
- en: 'Shi et al. (2020) Baoxu Shi, Jaewon Yang, Feng Guo, and Qi He. 2020. [Salience
    and market-aware skill extraction for job targeting](https://dl.acm.org/doi/10.1145/3394486.3403338).
    In *KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
    Virtual Event, CA, USA, August 23-27, 2020*, pages 2871–2879\. ACM.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等（2020）Baoxu Shi、Jaewon Yang、Feng Guo 和 Qi He。2020年。[面向市场的显著性和技能提取用于职位目标定位](https://dl.acm.org/doi/10.1145/3394486.3403338)。在
    *KDD ’20：第26届ACM SIGKDD知识发现与数据挖掘会议，虚拟会议，CA，美国，2020年8月23-27日*，页码2871–2879。ACM。
- en: Souza et al. (2020) Fábio Souza, Rodrigo Nogueira, and Roberto Lotufo. 2020.
    [Portuguese named entity recognition using bert-crf](http://arxiv.org/abs/1909.10649).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Souza 等（2020）Fábio Souza、Rodrigo Nogueira 和 Roberto Lotufo。2020年。[使用 bert-crf
    的葡萄牙语命名实体识别](http://arxiv.org/abs/1909.10649)。
- en: 'Tamburri et al. (2020) Damian A. Tamburri, Willem-Jan Van Den Heuvel, and Martin
    Garriga. 2020. [Dataops for societal intelligence: a data pipeline for labor market
    skills extraction and matching](https://doi.org/10.1109/IRI49571.2020.00063).
    In *2020 IEEE 21st International Conference on Information Reuse and Integration
    for Data Science (IRI)*, pages 391–394.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tamburri 等（2020）Damian A. Tamburri、Willem-Jan Van Den Heuvel 和 Martin Garriga。2020年。[社会智能的数据操作：用于劳动市场技能提取和匹配的数据管道](https://doi.org/10.1109/IRI49571.2020.00063)。在
    *2020 IEEE第21届信息重用与集成数据科学国际会议（IRI）*，页码391–394。
- en: 'Ternikov (2022) Andrei Ternikov. 2022. [Soft and hard skills identification:
    insights from it job advertisements in the cis region](https://doi.org/https://doi.org/10.7717/peerj-cs.946).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ternikov（2022）Andrei Ternikov。2022年。[软技能与硬技能识别：来自CIS地区IT职位广告的见解](https://doi.org/https://doi.org/10.7717/peerj-cs.946)。
- en: 'Vermeer et al. (2022) Ninande Vermeer, Vera Provatorova, David Graus, Thilina
    Rajapakse, and Sepideh Mesbah. 2022. Using robbert and extreme multi-label classification
    to extract implicit and explicit skills from dutch job descriptions. In *compjobs
    ’22: Computational Jobs Marketplace, Feb 25, 2022*. ACM.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vermeer等人（2022）宁南德·费尔梅尔、维拉·普罗瓦托罗娃、大卫·格劳斯、提利纳·拉贾帕克塞和塞皮德赫·梅斯巴。2022年。使用RobBERT和极端多标签分类从荷兰职位描述中提取隐性和显性技能。在*compjobs
    ’22：计算工作市场，2022年2月25日*。ACM。
- en: 'Wang et al. (2021) Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021. [TSDAE:
    Using transformer-based sequential denoising auto-encoderfor unsupervised sentence
    embedding learning](https://doi.org/10.18653/v1/2021.findings-emnlp.59). In *Findings
    of the Association for Computational Linguistics: EMNLP 2021*, pages 671–688,
    Punta Cana, Dominican Republic. Association for Computational Linguistics.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2021）柯欣·王、尼尔斯·雷默斯和伊琳娜·古列维奇。2021年。[TSDAE：使用基于变换器的序列去噪自编码器进行无监督句子嵌入学习](https://doi.org/10.18653/v1/2021.findings-emnlp.59)。在*计算语言学协会会议记录：EMNLP
    2021*，页码671–688，多米尼加共和国蓬塔卡纳。计算语言学协会。
- en: Wang et al. (2022) Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun
    Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. [Text embeddings by weakly-supervised
    contrastive pre-training](http://arxiv.org/abs/2212.03533).
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2022）梁旺、南杨、肖龙黄、彬兴焦、林俊杨、大新姜、兰根·马朱姆德和富如·魏。2022年。[通过弱监督对比预训练的文本嵌入](http://arxiv.org/abs/2212.03533)。
- en: Wild et al. (2021) Simon Wild, Soyhan Parlar, Thomas Hanne, and Rolf Dornberger.
    2021. [Naïve bayes and named entity recognition for requirements mining in job
    postings](https://doi.org/10.1109/ICNLP52887.2021.00032). In *2021 3rd International
    Conference on Natural Language Processing (ICNLP)*, pages 155–161.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wild等人（2021）西蒙·怀尔德、索扬·帕尔拉尔、托马斯·汉恩和罗尔夫·多恩贝格。2021年。[朴素贝叶斯和命名实体识别用于职位发布中的需求挖掘](https://doi.org/10.1109/ICNLP52887.2021.00032)。在*2021年第三届国际自然语言处理会议（ICNLP）*，页码155–161。
- en: Yao et al. (2022) Kaichun Yao, Jingshuai Zhang, Chuan Qin, Peng Wang, Hengshu
    Zhu, and Hui Xiong. 2022. [Knowledge enhanced person-job fit for talent recruitment](https://doi.org/10.1109/ICDE53745.2022.00325).
    In *2022 IEEE 38th International Conference on Data Engineering (ICDE)*, pages
    3467–3480.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等人（2022）开春姚、靖帅张、川秦、鹏王、恒书朱和慧雄。2022年。[知识增强的人岗匹配用于人才招聘](https://doi.org/10.1109/ICDE53745.2022.00325)。在*2022
    IEEE第38届国际数据工程会议（ICDE）*，页码3467–3480。
- en: 'Zhang et al. (2022a) Mike Zhang, Kristian Jensen, Sif Sonniks, and Barbara
    Plank. 2022a. [SkillSpan: Hard and soft skill extraction from English job postings](https://doi.org/10.18653/v1/2022.naacl-main.366).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 4962–4984,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2022a）迈克·张、克里斯蒂安·詹森、西夫·索尼克斯和芭芭拉·普兰克。2022a。[SkillSpan：从英语职位发布中提取硬技能和软技能](https://doi.org/10.18653/v1/2022.naacl-main.366)。在*2022年北美计算语言学协会会议：人类语言技术会议论文集*，页码4962–4984，美国西雅图。计算语言学协会。
- en: 'Zhang et al. (2022b) Mike Zhang, Kristian Nørgaard Jensen, and Barbara Plank.
    2022b. [Kompetencer: Fine-grained skill classification in Danish job postings
    via distant supervision and transfer learning](https://aclanthology.org/2022.lrec-1.46).
    In *Proceedings of the Thirteenth Language Resources and Evaluation Conference*,
    pages 436–447, Marseille, France. European Language Resources Association.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2022b）迈克·张、克里斯蒂安·诺尔加德·詹森和芭芭拉·普兰克。2022b。[Kompetencer：通过远程监督和迁移学习在丹麦职位发布中进行细粒度技能分类](https://aclanthology.org/2022.lrec-1.46)。在*第十三届语言资源与评估会议论文集*，页码436–447，法国马赛。欧洲语言资源协会。
- en: 'Zhang et al. (2022c) Mike Zhang, Kristian Nørgaard Jensen, Rob van der Goot,
    and Barbara Plank. 2022c. Skill extraction from job postings using weak supervision.
    In *RecSys in HR’22: The 2nd Workshop on Recommender Systems for Human Resources,
    in conjunction with the 16th ACM Conference on Recommender Systems, September
    18–23, 2022, Seattle, USA.* CEUR Workshop Proceedings.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2022c）迈克·张、克里斯蒂安·诺尔加德·詹森、罗布·范德·古特和芭芭拉·普兰克。2022c。使用弱监督从职位发布中提取技能。在*RecSys
    in HR’22：第二届人力资源推荐系统研讨会，与第16届ACM推荐系统会议联合举办，2022年9月18–23日，美国西雅图*。CEUR研讨会论文集。
- en: 'Zhang et al. (2023) Mike Zhang, Rob van der Goot, and Barbara Plank. 2023.
    [ESCOXLM-R: Multilingual taxonomy-driven pre-training for the job market domain](https://doi.org/10.18653/v1/2023.acl-long.662).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 11871–11890, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等人（2023）迈克·张，罗布·范·德·古特，芭芭拉·普朗克。2023年。[ESCOXLM-R: 多语言分类驱动的预训练用于职业市场领域](https://doi.org/10.18653/v1/2023.acl-long.662)。见于
    *第61届计算语言学协会年会论文集（第1卷：长篇论文）*，页码11871–11890，多伦多，加拿大。计算语言学协会。'
- en: Appendix A Appendix
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Terminology Example
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 术语示例
- en: 'In Table [3](#A1.T3 "Table 3 ‣ A.1 Terminology Example ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings"), we present an example sentence for better
    terminology understanding.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [3](#A1.T3 "表格 3 ‣ A.1 术语示例 ‣ 附录 A 附录 ‣ 基于深度学习的计算职业市场分析：从职位发布中技能提取和分类的调查")中，我们展示了一个示例句子，以便更好地理解术语。
- en: '|  | Familiar | with | building | tests | in | python |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | 熟悉 | 使用 | python | 构建 | 测试 |'
- en: '| $I$: | O | O | B | I | O | B |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| $I$: | O | O | B | I | O | B |'
- en: '| $E_{C}$: | O | O | B[skill] | I[skill] | O | B[knowl.] |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| $E_{C}$: | O | O | B[skill] | I[skill] | O | B[knowl.] |'
- en: '| $C_{D}/C_{E}$: | “Python (computer programming)”, “ plan ” |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| $C_{D}/C_{E}$: | “Python (计算机编程)”，“计划” |'
- en: '|  | “software testing” |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | “软件测试” |'
- en: 'Table 3: An example with annotations for the different tasks described in Section [3](#S3
    "3 Skill-related Terminology ‣ Deep Learning-based Computational Job Market Analysis:
    A Survey on Skill Extraction and Classification from Job Postings"). For skill
    classification ($C$), we used the ESCO taxonomy in this example, and for skill
    extraction with coarse labels ($E_{C}$) we follow the guidelines of SkillSpan Zhang
    et al. ([2022a](#bib.bib61))'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：展示了第 [3](#S3 "3 技能相关术语 ‣ 基于深度学习的计算职业市场分析：从职位发布中技能提取和分类的调查")节中描述的不同任务的注释示例。对于技能分类
    ($C$)，我们在此示例中使用了 ESCO 分类法，对于具有粗略标签的技能提取 ($E_{C}$)，我们遵循了 SkillSpan 张等人 ([2022a](#bib.bib61))
    的指导方针。
- en: '| Source | # Skill Spans | # Knowledge Spans |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 来源 | 技能跨度数量 | 知识跨度数量 |'
- en: '| --- | --- | --- |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| SKILLSPAN - HOUSE | 2,146 | 1,418 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| SKILLSPAN - HOUSE | 2,146 | 1,418 |'
- en: '| DECORTE - HOUSE | 509* | 210* |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| DECORTE - HOUSE | 509* | 210* |'
- en: '| SKILLSPAN - TECH | 2,241 | 3,828 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| SKILLSPAN - TECH | 2,241 | 3,828 |'
- en: '| DECORTE - TECH | 419 | 480* |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| DECORTE - TECH | 419 | 480* |'
- en: '| KOMPETENCER | 665 | 255 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| KOMPETENCER | 665 | 255 |'
- en: 'Table 4: Number of labeled spans. The star * indicates, that two values found
    in the Decorte HOUSE test dataset (tagged as knowledge) were actually from the
    Skillspan TECH dataset; eight values found in the Decorte TECH test dataset (four
    skill spans, four knowledge spans) were actually from the Skillspan HOUSE dataset.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4：标注跨度数量。星号 * 表示 Decorte HOUSE 测试数据集中发现的两个值（标记为知识）实际上来自 Skillspan TECH 数据集；在
    Decorte TECH 测试数据集中发现的八个值（四个技能跨度，四个知识跨度）实际上来自 Skillspan HOUSE 数据集。
- en: A.2 Number of Skill and Knowledge Spans
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 技能和知识跨度数量
- en: 'In Table [4](#A1.T4 "Table 4 ‣ A.1 Terminology Example ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings"), we show the number of labeled spans for
    skills and knowledge in the SKILLSPAN Zhang et al. ([2022a](#bib.bib61)), DECORTE Decorte
    et al. ([2022](#bib.bib16)), and KOMPETENCER Zhang et al. ([2022b](#bib.bib62))
    dataset.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [4](#A1.T4 "表格 4 ‣ A.1 术语示例 ‣ 附录 A 附录 ‣ 基于深度学习的计算职业市场分析：从职位发布中技能提取和分类的调查")中，我们展示了
    SKILLSPAN 张等人 ([2022a](#bib.bib61))、DECORTE Decorte 等人 ([2022](#bib.bib16)) 和
    KOMPETENCER 张等人 ([2022b](#bib.bib62)) 数据集中技能和知识的标注跨度数量。
- en: A.3 Scores of Selected Models
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 选定模型的得分
- en: 'In Table [5](#A1.T5 "Table 5 ‣ A.3 Scores of Selected Models ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings"), we display the scores of recent LMM-based
    approaches on the DECORTE Decorte et al. ([2022](#bib.bib16)) dataset for comparison.
    Furthermore, we show results of Zhang et al. ([2023](#bib.bib64)); Goyal et al.
    ([2023](#bib.bib24)) and (Bhola et al., [2020](#bib.bib5)) on the BHOLA Bhola
    et al. ([2020](#bib.bib5)) dataset.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [5](#A1.T5 "表格 5 ‣ A.3 选定模型的得分 ‣ 附录 A 附录 ‣ 基于深度学习的计算职业市场分析：从职位发布中技能提取和分类的调查")中，我们展示了
    DECORTE Decorte 等人 ([2022](#bib.bib16)) 数据集中最近基于 LMM 方法的得分以供比较。此外，我们还展示了张等人 ([2023](#bib.bib64))；戈亚尔等人
    ([2023](#bib.bib24)) 和（Bhola 等人，[2020](#bib.bib5)）在 BHOLA Bhola 等人 ([2020](#bib.bib5))
    数据集上的结果。
- en: '| Model | Source | HOUSE* | TECH* | BHOLA |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 来源 | HOUSE* | TECH* | BHOLA |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  |  | MRR | RP@5 | RP@10 | MRR | RP@5 | RP@10 | MRR | R@5 | R@10 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MRR | RP@5 | RP@10 | MRR | RP@5 | RP@10 | MRR | R@5 | R@10 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| $Classifier^{neg}$ | (Decorte et al., [2022](#bib.bib16)) | 0.299 | 30.82
    | 38.69 | 0.326 | 31.71 | 39.09 | N/A | N/A | N/A |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| $Classifier^{neg}$ | (Decorte 等, [2022](#bib.bib16)) | 0.299 | 30.82 | 38.69
    | 0.326 | 31.71 | 39.09 | 不适用 | 不适用 | 不适用 |'
- en: '| $GPTsentences^{aug}$ | (Decorte et al., [2023](#bib.bib17)) | 0.428 | 45.74
    | N/A | 0.529 | 54.62 | N/A | N/A | N/A | N/A |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| $GPTsentences^{aug}$ | (Decorte 等, [2023](#bib.bib17)) | 0.428 | 45.74 |
    不适用 | 0.529 | 54.62 | 不适用 | 不适用 | 不适用 | 不适用 |'
- en: '| $GPT3.5Re-ranking$ | (Clavié and Soulié, [2023](#bib.bib12)) | 0.427 | 43.57
    | 51.44 | 0.488 | 52.50 | 59.75 | N/A | N/A | N/A |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| $GPT3.5Re-ranking$ | (Clavié 和 Soulié, [2023](#bib.bib12)) | 0.427 | 43.57
    | 51.44 | 0.488 | 52.50 | 59.75 | 不适用 | 不适用 | 不适用 |'
- en: '| $GPT4Re-ranking$ | (Clavié and Soulié, [2023](#bib.bib12)) | 0.495 | 53.34
    | 61.02 | 0.537 | 61.50 | 68.94 | N/A | N/A | N/A |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| $GPT4Re-ranking$ | (Clavié 和 Soulié, [2023](#bib.bib12)) | 0.495 | 53.34
    | 61.02 | 0.537 | 61.50 | 68.94 | 不适用 | 不适用 | 不适用 |'
- en: '| $BERT\textendash XMLC+CAB$ | (Bhola et al., [2020](#bib.bib5)) | N/A | N/A
    | N/A | N/A | N/A | N/A | 0.9049 | 21.67 | 40.49 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| $BERT\textendash XMLC+CAB$ | (Bhola 等, [2020](#bib.bib5)) | 不适用 | 不适用 | 不适用
    | 不适用 | 不适用 | 不适用 | 0.9049 | 21.67 | 40.49 |'
- en: '| $JobXMLC$ | (Goyal et al., [2023](#bib.bib24)) | N/A | N/A | N/A | N/A |
    N/A | N/A | 0.90 | 18.29 | 32.33 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| $JobXMLC$ | (Goyal 等, [2023](#bib.bib24)) | 不适用 | 不适用 | 不适用 | 不适用 | 不适用 |
    不适用 | 0.90 | 18.29 | 32.33 |'
- en: '| $ESCOXML-R$ | (Zhang et al., [2023](#bib.bib64)) | N/A | N/A | N/A | N/A
    | N/A | N/A | 0.907 | N/A | N/A |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| $ESCOXML-R$ | (Zhang 等, [2023](#bib.bib64)) | 不适用 | 不适用 | 不适用 | 不适用 | 不适用
    | 不适用 | 0.907 | 不适用 | 不适用 |'
- en: 'Table 5: Scores of selected models on the benchmarking datasets DECORTE and
    BHOLA.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 选定模型在 DECORTE 和 BHOLA 基准数据集上的评分。'
