- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:34:34'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2402.05617] Deep Learning-based Computational Job Market Analysis: A Survey
    on Skill Extraction and Classification from Job Postings'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.05617](https://ar5iv.labs.arxiv.org/html/2402.05617)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning-based Computational Job Market Analysis:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Survey on Skill Extraction and Classification from Job Postings
  prefs: []
  type: TYPE_NORMAL
- en: Elena Senger^(1,3), Mike Zhang², Rob van der Goot², Barbara Plank^(1,2)
  prefs: []
  type: TYPE_NORMAL
- en: ¹MaiNLP, Center for Information and Language Processing, LMU Munich, Germany
  prefs: []
  type: TYPE_NORMAL
- en: ²Department of Computer Science, IT University of Copenhagen, Denmark
  prefs: []
  type: TYPE_NORMAL
- en: ³Fraunhofer Center for International Management and Knowledge Economy IMW, Germany
  prefs: []
  type: TYPE_NORMAL
- en: elena.senger@cis.lmu.de, {mikz, robv}@itu.dk, b.plank@lmu.de
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent years have brought significant advances to Natural Language Processing
    (NLP), which enabled fast progress in the field of *computational job market analysis*.
    Core tasks in this application domain are *skill extraction and classification*
    from job postings. Because of its quick growth and its interdisciplinary nature,
    there is no exhaustive assessment of this emerging field. This survey aims to
    fill this gap by providing a comprehensive overview of deep learning methodologies,
    datasets, and terminologies specific to NLP-driven skill extraction and classification.
    Our comprehensive cataloging of publicly available datasets addresses the lack
    of consolidated information on dataset creation and characteristics. Finally,
    the focus on terminology addresses the current lack of consistent definitions
    for important concepts, such as hard and soft skills, and terms relating to skill
    extraction and classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning-based Computational Job Market Analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: A Survey on Skill Extraction and Classification from Job Postings
  prefs: []
  type: TYPE_NORMAL
- en: Elena Senger^(1,3), Mike Zhang², Rob van der Goot², Barbara Plank^(1,2) ¹MaiNLP,
    Center for Information and Language Processing, LMU Munich, Germany ²Department
    of Computer Science, IT University of Copenhagen, Denmark ³Fraunhofer Center for
    International Management and Knowledge Economy IMW, Germany elena.senger@cis.lmu.de,
    {mikz, robv}@itu.dk, b.plank@lmu.de
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Skill extraction and classification has recently been the subject of an increased
    amount of interest (Zhang et al., [2023](#bib.bib64); Clavié and Soulié, [2023](#bib.bib12)),
    which shows in a high number of publications, driven by the advances in natural
    language processing (NLP) technology. For instance, through large language models
    (LLMs) the low resource tasks of skill extraction can be approached by using synthetic
    training data (Clavié and Soulié, [2023](#bib.bib12); Decorte et al., [2023](#bib.bib17)).
    Surveys regarding skill extraction are emerging (Khaouja et al., [2021a](#bib.bib34);
    Papoutsoglou et al., [2019](#bib.bib47)), nevertheless, a comprehensive overview
    from an NLP perspective is still lacking—a gap we aim to fill in this survey.
    Our contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Firstly, we aim to address the lack of standardized terminology in the field,
    bringing clarity to terms like hard and soft skills, as well as phrases related
    to skill extraction and classification.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, this survey is the first to examine various publicly accessible
    datasets and sheds light on their creation methodologies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In contrast to prior surveys, we adopt an NLP-centric focus, with a deep dive
    into the latest advancements of neural methods for skill extraction and classification.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While prior surveys exists, they focus typically on *Skill count* and *Topic
    modeling* methods for extracting skills. Skill count is performed manually or
    by matching n-grams with a skill base. Topic modeling is an unsupervised method
    utilizing word distributions to identify underlying topics in documents. Due to
    primary statistical basis and lack of defined skill spans or labels, topic modeling,
    as well as skill count, methods are not covered in this survey. For further details
    on skill count, see Khaouja et al. ([2021a](#bib.bib34)) and Ternikov ([2022](#bib.bib55)),
    and for topic modeling, please refer to Khaouja et al. ([2021a](#bib.bib34)),
    Ternikov ([2022](#bib.bib55)) and Ao et al. ([2023](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: Research Methodology
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For our search strategy we used several academic databases including the ACL
    Anthology, Google Scholar, arXiv, IEEE, ACM, Science Direct, and Springer Link.
    The primary search terms were “skill extraction” and “job”. To refine the search,
    we added terms like “deep learning”, “machine learning”, or “natural language
    processing” to our query for Google Scholar and Science Direct databases. This
    yielded the inclusion of 26 publications on neural skill extraction from job postings
    (JPs) that were published before November 2023.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Other Surveys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previous surveys provide a foundation for our survey. Notable contributions
    include works from the social sciences, in particular, by Napierala and Kvetan
    ([2023](#bib.bib46)) in the “Handbook of Computational Social Science for Policy”
    (Chapter 13). It focuses on changing skills in a dynamic world from a social science
    perspective. Moreover, Papoutsoglou et al. ([2019](#bib.bib47)) focus on studies
    regarding the software engineering labor market. Besides JPs, they research other
    sources like social networks or Q&A sites. Lastly, the survey by Khaouja et al.
    ([2021a](#bib.bib34)) on skill identification from JPs is the closest to this
    survey. It overviews papers using methodologies such as skill counts, topic modeling,
    skill embeddings, and other machine learning-based methods. With this survey,
    we steer away from manual and topic modeling approaches to delve deeply into recent
    extraction methodologies and deep learning-based innovations.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Skill-related Terminology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The terms skill extraction, identification (Li et al., [2023](#bib.bib39)),
    detection (Beauchemin et al., [2022](#bib.bib3)), standardization (Li et al.,
    [2023](#bib.bib39)) and classification are used differently, sometimes interchangeably,
    and describe the same or different tasks. We provide the following definition
    (See an example in Table [3](#A1.T3 "Table 3 ‣ A.1 Terminology Example ‣ Appendix
    A Appendix ‣ Deep Learning-based Computational Job Market Analysis: A Survey on
    Skill Extraction and Classification from Job Postings") in the Appendix):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Skill Extraction ($E$): as a generic (parent) category for retrieving skill-related
    information. Skill extraction $E:\text{JP}\rightarrow(S)$, where $E$ maps a job
    posting (JP) to a set of skills $S$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Skill Identification/Detection ($I$): as the process of extracting skills without
    any pre-defined labels. It can be represented as $I:\text{JP}\rightarrow S$, where
    skills, especially skill spans, are extracted from JPs. It can also be formalized
    as a classification problem, $I:\text{Span}\rightarrow\{0,1\}$, to determine whether
    a given span in a JP represents a skill (1) or not (0).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Skill Extraction with Coarse Labels ($E_{C}$): as identifying broader categories
    of skill spans. It is formalized as $E_{C}:\text{JP}\rightarrow\{SC_{1},SC_{2},\dots,SC_{n}\}$,
    where each $SC_{i}$ represents a skill span with a coarse label.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Skill Standardization ($Std$): as the normalization process of skill terms,
    formalized as $Std:S\rightarrow S^{\prime}$, mapping an initial set of skills
    $S$ to a standardized set $S^{\prime}$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Direct Skill Classification ($C_{D}$): as mapping skills to a predefined skill
    base for assigning fine-grained labels. This process can be formalized as $C_{D}:S\rightarrow
    L$, where $C_{D}$ maps a set of already extracted skills $S$ to a set of fine-grained
    labels $L$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Skill Classification with Extraction ($C_{E}$): as mapping JPs to a predefined
    skill base for assigning fine-grained labels. This process can be formalized as
    $C_{E}:JP\rightarrow L$, where $C_{E}$ maps a set of already extracted skills
    $S$ entire JP or raw JP snippets to a set of fine-grained labels $L$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given these definitions, the skill extraction step can happen at different levels
    of granularity (of the input). Some works extract skills per JP ($E_{JP}$, the
    overall document), per sentence ($E_{sentence}$) or per n-gram ($E_{n-gram}$).
    A skill span ($E_{span}$)is a continuous n-gram sequence that capture a skill.
  prefs: []
  type: TYPE_NORMAL
- en: A skill base ($B$) is a knowledge base containing skill entities and terminology.
    A taxonomy is a hierarchically structured skill base, while ontologies provide
    a structure via relationships between concepts (Khaouja et al., [2021a](#bib.bib34)).
    Several works use the term “skill dictionary” for a skill base, most often referring
    to an unstructured skill base or a list of skills (Gugnani and Misra, [2020](#bib.bib27);
    Yao et al., [2022](#bib.bib60)). Two popular publicly-available skill bases, created
    by domain experts, and are frequently used and maintained are the European Skills,
    Competences, Qualifications and Occupations (ESCO; le Vrang et al., [2014](#bib.bib38))
    taxonomy and the US Occupational Information Network (O*NET; Council et al., [2010](#bib.bib15)).
    We refer to Khaouja et al. ([2021a](#bib.bib34)) for more examples of skill bases.
  prefs: []
  type: TYPE_NORMAL
- en: 4 What are Skills? On Skill Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the concept of a *skill* is pivotal in the field of skill extraction.
    In this section, we investigate several definitions of skills by various publications
    and institutions, aiming to identify commonalities and distinctions across different
    sources, which is crucial for establishing a common ground in this emerging field.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of *skill* can be seen as one broad concept (Green et al., [2022](#bib.bib25);
    Wild et al., [2021](#bib.bib59); Fang et al., [2023](#bib.bib19)) or split into
    subclasses, with multiple possibilities for the split. In the latest version of
    the ESCO taxonomy the “skill pillar” is divided into four categories: “Transversal
    skills”, “Skills”,“ Knowledge” and “Language skills and knowledge”.¹¹1[https://esco.ec.europa.eu/en/classification/skill_main](https://esco.ec.europa.eu/en/classification/skill_main)
    O*NET is structured in six domains (Council et al., [2010](#bib.bib15)), the domain
    most fitting for skill extraction from JP is “Worker Requirements”. This domain
    entails four subcategories: basic skills, cross-functional skills, knowledge,
    and education.²²2 [https://www.onetcenter.org/content.html](https://www.onetcenter.org/content.html)
    But publications considered in this survey that define skills, mainly distinguish
    between hard and soft skills (Tamburri et al., [2020](#bib.bib54); Beauchemin
    et al., [2022](#bib.bib3); Sayfullina et al., [2018](#bib.bib50)), which is therefore
    also the separation used in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: Hard Skills
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Tamburri et al. ([2020](#bib.bib54)) delineate hard skills as professional competencies,
    activities, or knowledge pertinent to organizational functions, processes, and
    roles, essential for the successful completion of specific tasks. This definition
    emphasizes the practicality and functionality of hard skills within a professional
    setting. Aligning with this, the study by Beauchemin et al. ([2022](#bib.bib3))
    views hard skills as task-oriented technical competencies, drawing upon Lyu and
    Liu ([2021](#bib.bib43)) to define them as formal technical abilities for performing
    certain tasks. Furthermore, Gugnani and Misra ([2020](#bib.bib27)) expand on this
    perspective by incorporating technological terminologies for skill identification
    and therefore integrating knowledge as a fundamental component of hard skills.
  prefs: []
  type: TYPE_NORMAL
- en: By incorporating knowledge as a component of hard skills, the definitions of
    hard skills and knowledge categories of O*NET and ESCO can be combined. O*NET’s
    definition of hard skills states that they are developed abilities that enable
    learning or knowledge acquisition, coupled with their definition of knowledge
    as “Organized sets of principles and facts applying in general domains”.³³3See
    footnote 2. This comprehensive definition underscores not only technical proficiency
    but also the ability to adapt and apply knowledge. Similarly, ESCO, referencing
    the European Qualifications Framework, defines skills as “the ability to apply
    knowledge and use know-how to complete tasks and solve problems”, while defining
    knowledge as “the outcome of the assimilation of information through learning”.⁴⁴4[https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/knowledge](https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/knowledge)
    and [https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/skill](https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/skill)
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, we define hard skills as a wide variety of professional abilities,
    ranging from measurable technical skills to the more general capacity for learning
    and effectively applying knowledge. They are quantifiable and teachable competencies,
    predominantly technical, yet intrinsically linked to the ability to adapt and
    apply them in diverse professional scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Soft Skills
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sayfullina et al. ([2018](#bib.bib50)), referencing the Collins dictionary (HarperCollins
    Publishers, [2023](#bib.bib29)), views soft skills as innate, non-technical qualities
    highly sought after in employment, diverging from reliance on acquired knowledge.
    In a more social context, Tamburri et al. ([2020](#bib.bib54)) characterizes soft
    skills as encompassing personal, emotional, social, or intellectual aspects, further
    known as behavioral skills or competencies. Echoing this sentiment, Beauchemin
    et al. ([2022](#bib.bib3)), drawing from Lyu and Liu ([2021](#bib.bib43)), identifies
    soft skills as a variety of personal attributes and behaviors crucial for effective
    workplace interaction, collaboration, and adaptability.
  prefs: []
  type: TYPE_NORMAL
- en: Adding to these perspectives, ESCO characterizes soft skills as *transversal
    skills*, highlighting their wide applicability across various occupations and
    sectors and their fundamental role in individual growth.⁵⁵5[https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/transversal-knowledge-skills-and-competences](https://esco.ec.europa.eu/en/about-esco/escopedia/escopedia/transversal-knowledge-skills-and-competences)
    Similarly, O*NET classifies these skills under Cross-Functional Skills, defining
    them as developed capacities that enhance the performance of activities common
    across different jobs, encompassing areas like Social Skills and Complex Problem
    Solving Skills.⁶⁶6See footnote 2. Both sources underscore the universal relevance
    of soft skills.
  prefs: []
  type: TYPE_NORMAL
- en: These previous definitions lead to our converged definition that soft skills
    cover a vast array of personal, social, and intellectual competencies, all of
    which are indispensable for successful interpersonal engagement and personal development
    in professional settings.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Operationalization of Skill Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explore various methodologies for operationalizing skill
    definitions in skill extraction and classification research.
  prefs: []
  type: TYPE_NORMAL
- en: Using a Skill Base
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'By using a given skill base, a pre-defined definition of the concept of skills
    is provided by the authors of the skill base. Numerous studies employ established
    skill bases such as the ESCO taxonomy (Zhang et al., [2023](#bib.bib64), [2022b](#bib.bib62);
    Clavié and Soulié, [2023](#bib.bib12); Decorte et al., [2023](#bib.bib17), [2022](#bib.bib16))
    or O*NET (Gugnani and Misra, [2020](#bib.bib27)). However, it is often ambiguous
    whether these studies use all or only specific subcategories (Li et al., [2023](#bib.bib39);
    Decorte et al., [2022](#bib.bib16); Gugnani and Misra, [2020](#bib.bib27)). Some
    papers mention explicitly the use of all subclasses (Zhang et al., [2022b](#bib.bib62),
    [a](#bib.bib61); Gnehm et al., [2022a](#bib.bib21)) other times it can be inferred
    from the number of skill spans used (Clavié and Soulié, [2023](#bib.bib12); Decorte
    et al., [2023](#bib.bib17)). However, one should note that the interpretations
    of ESCO definitions differ based on the ESCO version and authors’ perspective.
    Zhang et al. ([2022a](#bib.bib61), [b](#bib.bib62)) used ESCO version 1.0 with
    a different soft skill category than discussed in Section [4](#S4 "4 What are
    Skills? On Skill Definitions ‣ Deep Learning-based Computational Job Market Analysis:
    A Survey on Skill Extraction and Classification from Job Postings") and implemented
    two labels: “knowledge” aligns with ESCO’s “Knowledge” category, and “Skills”
    as a fusion of the hard and soft skills. In contrast, Colombo et al. ([2019](#bib.bib13))
    using the same ESCO version, but treat soft skills separate from hard skills.
    Most of the publications used all subcategories as skills without differentiating
    (Clavié and Soulié, [2023](#bib.bib12); Gnehm et al., [2022a](#bib.bib21); Decorte
    et al., [2023](#bib.bib17)).'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond these, there are other skill bases, such as the Russian professional
    standard in Botov et al. ([2019](#bib.bib8)) or the Chinese Occupation Classification
    Grand Dictionary used in Cao and Zhang ([2021](#bib.bib10)); Cao et al. ([2021](#bib.bib11)).
    Additionally, non-official skill bases exist, like the list of 1K soft skills
    in (Sayfullina et al., [2018](#bib.bib50)) or LinkedIn’s in-house taxonomy for
    skill extraction (Shi et al., [2020](#bib.bib52)). In general, for transparency
    and reproducibility, it is helpful to state which subset of fine-grained labels
    $L$ of the skill base ($B$) and which skill base version is used.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging Automated Tools
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some studies leverage automated tools like AutoPhrase (Shang et al., [2018](#bib.bib51))
    or Microsoft Azure Analytics Service for NER for initial skill term detection,
    followed by manual verification and refinement (Yao et al., [2022](#bib.bib60);
    Kortum et al., [2022](#bib.bib36)). Also Vermeer et al. ([2022](#bib.bib56)) extract
    parts of their training data using an automated tool, while others are taken from
    a skill base.⁷⁷7[https://www.textkernel.com/de/](https://www.textkernel.com/de/)
    Lastly, Gugnani and Misra ([2020](#bib.bib27)) employ an IBM tool for skill identification,
    which forms a part of a larger skill identification framework.⁸⁸8[https://www.ibm.com/products/natural-language-understanding](https://www.ibm.com/products/natural-language-understanding)
    While some previous work did not apply manual verification (Gugnani and Misra,
    [2020](#bib.bib27); Vermeer et al., [2022](#bib.bib56)), we recommend it to reduce
    automation bias from the tool impacting the data.
  prefs: []
  type: TYPE_NORMAL
- en: Definition through Labeling
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Domain experts play a crucial role for labeling data and therefore impact how
    the definition of skills is put into work (Shi et al., [2020](#bib.bib52); Tamburri
    et al., [2020](#bib.bib54); Beauchemin et al., [2022](#bib.bib3)). Tamburri et al.
    ([2020](#bib.bib54)) additionally provide a codebook with skill definitions to
    address ambiguities. Shi et al. ([2020](#bib.bib52)) used next skills identified
    by hiring experts and skills common among successful applicants as training data.
    The study by Bhola et al. ([2020](#bib.bib5)) treat the companies filing the JPs
    as domain experts by using their labels (see also Section [6](#S6.SS0.SSS0.Px8
    "BHOLA ‣ 6 Data ‣ Deep Learning-based Computational Job Market Analysis: A Survey
    on Skill Extraction and Classification from Job Postings")). Besides domain experts,
    crowd workers and the people writing the guidelines for the workers oftentimes
    determine which terms are skills. Some studies do not mention who labels the data
    (Wild et al., [2021](#bib.bib59); Cao and Zhang, [2021](#bib.bib10); Botov et al.,
    [2019](#bib.bib8)). We suggest being clear about the labeling process and guidelines,
    making them public for transparency and re-use/standardization, and using domain
    experts if possible for accurate labeling.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we provide a comprehensive description of publicly available
    datasets, with an overview in Table [1](#S6.T1 "Table 1 ‣ 6 Data ‣ Deep Learning-based
    Computational Job Market Analysis: A Survey on Skill Extraction and Classification
    from Job Postings").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Publication | Approach | Granularity | Skill type | Use case | Size | \faBook
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (Sayfullina et al., [2018](#bib.bib50)) | Crowdsourced | span-level | soft
    | $I$ | 7411 spans | \faRemove |'
  prefs: []
  type: TYPE_TB
- en: '| (Green et al., [2022](#bib.bib25)) | Crowdsourced | span-level | hard + soft
    | $E_{C}$ | 10,606 spans | \faCheck |'
  prefs: []
  type: TYPE_TB
- en: '| (Beauchemin et al., [2022](#bib.bib3)) | Expert | span-level | soft | $E_{C}$
    | 47 JPs - 932 spans | \faRemove |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang et al., [2022a](#bib.bib61)) | Expert | span-level | hard + soft |
    $E_{C}$ | 265 JP - 9,633 spans | \faCheck |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang et al., [2022b](#bib.bib62)) | Expert | span-level | hard + soft |
    $E_{C}$+$C_{D}$ | 60 JP - 920 spans | \faCheck |'
  prefs: []
  type: TYPE_TB
- en: '| (Decorte et al., [2022](#bib.bib16)) | Manual | span-level | hard + soft
    | $I$+$C_{D}$ | 1,618 spans | \faCheck |'
  prefs: []
  type: TYPE_TB
- en: '| (Gnehm et al., [2022b](#bib.bib22)) | Expert | span-level | hard + soft |
    $E_{C}$+$C_{D}$ | 10,995 spans | \faRemove |'
  prefs: []
  type: TYPE_TB
- en: '| (Bhola et al., [2020](#bib.bib5)) | Skill Inventory | document-level | unknown
    | $C_{E}$ | 20,298 JP | \faRemove |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Overview of publicly-available labeled datasets. \faBook indicates
    if the authors used guidelines (not necessarily publicly available).'
  prefs: []
  type: TYPE_NORMAL
- en: SAYFULLINA
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: by Sayfullina et al. ([2018](#bib.bib50)) is a dataset derived from a publicly
    available Kaggle dataset, containing JPs from within the UK and representing a
    variety of sectors.⁹⁹9[https://www.kaggle.com/datasets/airiddha/trainrev1/?select=Train_rev1.csv](https://www.kaggle.com/datasets/airiddha/trainrev1/?select=Train_rev1.csv)
    The authors retrieved soft skill spans by exact matching with a list of 1,072
    soft skills. Each identified span is accompanied by up to 10 surrounding words.
    Crowdsourcing was used to determine whether the highlighted skill belongs to a
    job applicant. To ensure reliability, the workers were tested on a small set of
    JPs and each snippet was evaluated by at least three workers. This process led
    to a dataset with high class imbalance due to more positive examples. To counter
    this, additional skill spans were added, including those usually not describing
    candidates (marked as negative) and those consistently labeled positive.
  prefs: []
  type: TYPE_NORMAL
- en: GREEN
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: by Green et al. ([2022](#bib.bib25)) uses the same Kaggle dataset as SAYFULLINA.
    The labeling was done via crowdsourcing, they did not use experts but only workers
    who passed a test were included, and encouraged to follow the guidelines. Apart
    from the “Skill” label capturing hard and soft skills, the labels “Occupation”,
    “Domain”, “Experience”, “Qualification”, and “None” are used in a BIO scheme.
    The authors reduced errors by label aggregation with a preference towards labels
    from higher-performing workers. Additionally, they reclassified specific “Experience”
    spans, as “Skill” spans, and manually split multi-term spans into separate spans.
  prefs: []
  type: TYPE_NORMAL
- en: FIJO
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'by Beauchemin et al. ([2022](#bib.bib3)) was created in partnership with Canadian
    insurance companies, and consists of cleaned and de-identified French JPs published
    between 2009 and 2020\. The dataset focus on soft skills and includes 867 JPs
    with 47 annotated JPs, selected and annotated by a domain expert. The annotated
    spans are unevenly distributed across four classes: “Thoughts”, “Results”, “Relational”,
    and “Personal”.'
  prefs: []
  type: TYPE_NORMAL
- en: SKILLSPAN
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'by Zhang et al. ([2022a](#bib.bib61)) consists of the anonymized raw data and
    annotations of skill and knowledge spans from three JP datasets, one of which
    cannot be made publicly available due to its license. The available datasets are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HOUSE: A static in-house dataset with different types of JPs from 2012-2020
    and'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TECH: The StackOverflow JP platform, consisting mostly of technical jobs collected
    between June 2020 and September 2021.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The development of the publicly available annotation guidelines involved an
    iterative process, starting with a few JPs and progressing through several rounds
    of annotation and refinement by three domain experts.
  prefs: []
  type: TYPE_NORMAL
- en: KOMPETENCER
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'by Zhang et al. ([2022b](#bib.bib62)) consists of Danish JPs with annotated
    skill and knowledge spans, see Table [4](#A1.T4 "Table 4 ‣ A.1 Terminology Example
    ‣ Appendix A Appendix ‣ Deep Learning-based Computational Job Market Analysis:
    A Survey on Skill Extraction and Classification from Job Postings") in the Appendix.
    The same skill definitions, guidelines, and metrics as in SKILLSPAN are used for
    annotation. This dataset can be used for skill extraction with coarse labels,
    but the authors have also added fine-grained annotations to evaluate a classification
    with the ESCO taxonomy. For fine-grained annotations, they query the ESCO API
    with the annotated spans and use Levenshtein distance to determine the relevance
    of each obtained label. Then, the quality of these distantly supervised labels
    is assessed through human evaluation. They also repeated this process for the
    English SKILLSPAN dataset but only manually checked a sample for calculating statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: DECORTE
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'by Decorte et al. ([2022](#bib.bib16)) is a variant of the SKILLSPAN dataset
    with annotated ESCO labels. They used the identified skill without the skill and
    knowledge labels, but they can be recreated by matching the dataset with SKILLSPAN,
    see Table [4](#A1.T4 "Table 4 ‣ A.1 Terminology Example ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings") in the Appendix. Unlike in KOMPETENCER
    they manually matched the skills with fitting ESCO labels (if they exist) to create
    a gold standard.'
  prefs: []
  type: TYPE_NORMAL
- en: GNEHM-ICT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: by Gnehm et al. ([2022b](#bib.bib22)) is a Swiss-German dataset where they annotated
    for Information and Communications Technology (ICT)-related entity recognition.
    These could be ICT tasks, technology stack, responsibilities, and so forth. The
    used dataset is a combination of two other Swiss datasets namely the Swiss Job
    Market Monitor and an online job ad dataset Gnehm and Clematide ([2020](#bib.bib23));
    Buchmann et al. ([2022](#bib.bib9)). There are around 25,000 sentences in the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: BHOLA
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: by Bhola et al. ([2020](#bib.bib5)) was obtained from a government website^(10)^(10)10[https://www.mycareersfuture.gov.sg/](https://www.mycareersfuture.gov.sg/).
    in Singapore. The preprocessing steps for this English language dataset include
    converting text to lowercase and removing stop words and rarely used words. The
    companies filing the JPs added skill labels, which are mapped to the whole JP
    document. This makes the dataset suitable for performing multi-label classification
    by predicting a set of required skills for a given JP.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Paper | Model | Skill Type | Granularity | Use Case |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (Fang et al., [2023](#bib.bib19)) | Custom pre-trained LM | soft + hard |
    word-level | $E_{C}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Goyal et al., [2023](#bib.bib24)) | FastText skip-gram, GNN | unknown |
    word-level | $C_{E}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Clavié and Soulié, [2023](#bib.bib12)) | GPT-4 | soft + hard | span-level
    | $C_{E}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Li et al., [2023](#bib.bib39)) | XMLC - LLM | soft + hard | document-level
    | $C_{E}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Decorte et al., [2023](#bib.bib17)) | GPT-3.5 | soft + hard | sentence-level
    | $C_{E}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang et al., [2023](#bib.bib64)) | Multilingual XLM-R | soft + hard | span-level
    | $E_{C}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Decorte et al., [2022](#bib.bib16)) | RoBERTa | soft + hard | sentence-level
    | $C_{E}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang et al., [2022c](#bib.bib63)) | RoBERTa, JobBERT | soft + hard | span-level
    | $C_{D}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Gnehm et al., [2022a](#bib.bib21)) | JobBERT-de, SBERT | soft + hard | span-level
    | $E_{C}$ + $C_{D}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Zhang et al., [2022b](#bib.bib62)) | BERTbase , DaBERT | soft + hard | span-level
    | $C_{E}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Beauchemin et al., [2022](#bib.bib3)) | Bi-LSTM, CamemBERT | soft | span-level
    | $E_{C}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Yao et al., [2022](#bib.bib60)) | BERT, word2vec | unknown | word-level
    | $I$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Anand et al., [2022](#bib.bib1)) | LaBSE model | soft + hard | title | $C_{E}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| (Vermeer et al., [2022](#bib.bib56)) | RobBERT | soft + hard | document-level
    | $C_{E}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Wild et al., [2021](#bib.bib59)) | BERT, spaCy | soft + hard | span-level
    | $I$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Khaouja et al., [2021b](#bib.bib35)) | Sent2vec, SBERT | soft + hard | sentence-level
    | $C_{E}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Cao et al., [2021](#bib.bib11)) | BERT-BiLSTM-CRF | soft + hard | span-level
    | $I$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Cao and Zhang, [2021](#bib.bib10)) | BERT-BiLSTM-CRF | soft + hard | span-level
    | $I$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Li et al., [2020](#bib.bib40)) | Deep Averaging Network, FastText | unknown
    | span-level | $C_{E}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Tamburri et al., [2020](#bib.bib54)) | BERT Multilingual Cased | soft +
    hard | sentence-level | $I$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Bhola et al., [2020](#bib.bib5)) | BERTbase | unknown | document-level |
    $C_{E}$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Gugnani and Misra, [2020](#bib.bib27)) | Word2vec | soft + hard | span-level
    | $I$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Botov et al., [2019](#bib.bib8)) | Word2vec | unknown | span-level | $C_{E}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| (Jia et al., [2018](#bib.bib32)) | LSTM | unknown | word-level | $I$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Sayfullina et al., [2018](#bib.bib50)) | CNN, LSTM, HAN | soft | span-level
    | $I$ |'
  prefs: []
  type: TYPE_TB
- en: '| (Javed et al., [2017](#bib.bib31)) | Word2vec | soft + hard | span-level
    | $C_{E}$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Publications regarding neural skill extraction and classification.
    The skill type was not always explicitly mentioned in some cases it’s derived
    from examples given in the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we survey methods for skill extraction and classification.
    As in Section [3](#S3 "3 Skill-related Terminology ‣ Deep Learning-based Computational
    Job Market Analysis: A Survey on Skill Extraction and Classification from Job
    Postings") the goal of the extraction is to identify skill spans with ($E_{C}$)
    or without coarse labels ($I$). The classification section covers direct classification
    methods ($C_{D}$) and classification methods with extraction ($C_{E}$), both aim
    to retrieve fine-grained skill labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Skill Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This chapter delineates the evolution of skill extraction methodologies, grouped
    into three categories: skill identification as span labeling, skill identification
    through binary classification, and skill extraction with coarse span labels. Starting
    with LSTM neural networks in 2018 the methods in all three sub-chapters used after
    the introduction of BERT (Devlin et al., [2019](#bib.bib18)) in 2019 heavily BERT
    and BERT-based models. Recent advancements continue to diversify the landscape,
    integrating a broader array of language models (LMs).'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1 Skill Identification as Span Labeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this category approach skill identification as a span labeling task. The
    primary objective is to accurately identify skill spans, encompassing both the
    identification of the relevant skill phrases and their precise boundaries. Jia
    et al. ([2018](#bib.bib32)) are the first to use sequence tagging for identifying
    skills from JPs in 2018\. The authors use a pre-trained LSTM neural network (Lample
    et al., [2016](#bib.bib37)) for identifying skill terms on the word-level. Tamburri
    et al. ([2020](#bib.bib54)) also employed binary classification, but at the sentence-level,
    using a Dutch JP dataset. Their best-performing model, BERT Multilingual Cased,
    was fine-tuned on expert-annotated JP sentences, suggesting potential improvement
    with more data and optimization. Further publications retrieve embeddings using
    a pre-trained BERT model (Wild et al., [2021](#bib.bib59); Cao and Zhang, [2021](#bib.bib10);
    Cao et al., [2021](#bib.bib11)). Notably, Cao et al. ([2021](#bib.bib11)) and
    Cao and Zhang ([2021](#bib.bib10)) combine BERT’s pre-trained vectors with a Bi-LSTM
    and a CRF layer for finer entity classification. This approach aligns with previous
    research demonstrating the efficacy of a CRF layer in NER tasks (Souza et al.,
    [2020](#bib.bib53)). In Zhang et al. ([2023](#bib.bib64)), they further built
    upon the domain-adaptive pre-training paradigm (Gururangan et al., [2020](#bib.bib28)).
    They make use of the ESCO taxonomy (le Vrang et al., [2014](#bib.bib38)) and integrate
    this in a multilingual XLM-R model Conneau et al. ([2020](#bib.bib14)), using
    this taxonomy-driven pre-training method, they introduce a new state-of-the-art
    for all skill identification benchmarks. For analysis, they show that performance
    increases especially for skills that are shorter in length, due to ESCO skills
    also being shorter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to these single-model approaches, Gugnani and Misra ([2020](#bib.bib27))
    adopted a multi-faceted methodology to predict the relevance of identified skill
    spans. Their methodology encompassed four modules: using part-of-speech (PoS)
    tagging, parsing sentences with skill bases (O*NET, Hope, and Wikipedia), leveraging
    a ready-made sequence tagging solution, and employing a pre-trained word2vec model
    for final score determination through cosine similarity.^(11)^(11)11[https://www.ibm.com/products/natural-language-understanding](https://www.ibm.com/products/natural-language-understanding).'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2 Skill Identification as binary Classification Task
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this category, skill identification is framed as a binary classification
    task. The focus is on determining whether a given sequence either constitutes
    or contains a (specific) skill. The task in Sayfullina et al. ([2018](#bib.bib50))
    differs from the other publications. They extract skill spans by exact match and
    aim to decide whether skill spans refer to a candidate or something else, like
    a company. They experiment with various classifiers and input representations,
    such as Soft Skill Masking, Embedding, and Tagging, finding the LSTM classifier
    with skill tagging most effective on their dataset. Tamburri et al. ([2020](#bib.bib54))
    employed binary classification at the sentence-level to determine if it contains
    a skill. Their best-performing model, BERT Multilingual Cased, was fine-tuned
    on expert-annotated JP sentences using a Dutch JP dataset. Yao et al. ([2022](#bib.bib60))
    classify individual words as skill-related or not. They split JPs into individual
    words, analyzing each through character-level and word-level encoders, integrating
    linguistic features like POS tags and capitalization. Their initial training employs
    AutoPhrase (Shang et al., [2018](#bib.bib51)) for automatic skill term identification,
    followed by manual verification and expert-labeled samples. The model is further
    refined using Positive-Unlabeled learning, where the classifier’s predictions
    on unlabeled data help expand the skill base for continuous adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.3 Skill Extraction with Coarse Labels
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section explores advancements in skill extraction with coarse labels, where
    each publication extract spans from two to four different categories. The studies
    of Gnehm et al. ([2022a](#bib.bib21)) and Zhang et al. ([2022a](#bib.bib61)) both
    utilize sequence tagging-based models. Gnehm et al. ([2022a](#bib.bib21)) focusing
    on iterative training and annotation with jobBERT-de, a German LM tailored for
    JPs. Zhang et al. ([2022a](#bib.bib61)) compare BERT-based (Devlin et al., [2019](#bib.bib18))
    and SpanBERT-based (Joshi et al., [2020](#bib.bib33)) models, highlighting the
    importance of domain adaptation. On the other hand, Beauchemin et al. ([2022](#bib.bib3))
    and Fang et al. ([2023](#bib.bib19)) delve into the intricacies of training and
    optimizing LMs for skill extraction. Beauchemin et al. ([2022](#bib.bib3)) examine
    the sensitivity of Bi-LSTM and CamemBERT Martin et al. ([2020](#bib.bib44)) models
    to training data volume, with CamemBERT unfrozen yielding the highest mean token-wise
    accuracy. Fang et al. ([2023](#bib.bib19)) introduce RecruitPro, a specialized
    model for skill extraction from recruitment texts, employing innovative techniques
    for dealing with data noise and label imbalances. Collectively, these papers emphasize
    the need for tailored approaches and continuous innovation in model development.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Skill Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While skill standardization can be achieved through classification, other methods
    such as clustering (Bernabé-Moreno et al., [2019](#bib.bib4); Lukauskas et al.,
    [2023](#bib.bib42)), matching n-grams based on string similarity (Boselli et al.,
    [2018](#bib.bib7)), or identifying semantically similar skills (Bernabé-Moreno
    et al., [2019](#bib.bib4); Colombo et al., [2019](#bib.bib13); Grüger and Schneider,
    [2019](#bib.bib26)) also lead to standardized skill spans. These methods simplify
    the variety and quantity of skill spans without assigning standardized labels.
    Transitioning from these methods, we now focus on skill classification, a crucial
    step for assigning standardized labels to effectively organize and understand
    skills. Most publications skip a traditional extraction and match the JPs directly
    to the skill base ($C_{E}$), which can be seen as skill extraction against a skill
    base. Exceptions are Gnehm et al. ([2022a](#bib.bib21)), which perform extraction
    of skill spans with coarse labels before the fine-grained classification step,
    and Zhang et al. ([2022b](#bib.bib62)) who rely on prior work for extraction and
    focus solely on the matching of skill spans to ESCO ($C_{D}$). We divide the publications
    by methodology into those that match based on semantic similarity and those using
    extreme multi-label classification to solve the matching task.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1 Similarity-based Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The publications with similarity-based approaches split the JPs into sentences
    or n-grams before matching them. All of the following publications use skill embedding
    methods, which can be seen as an advancement of the skill count methods (Section [1](#S1
    "1 Introduction ‣ Deep Learning-based Computational Job Market Analysis: A Survey
    on Skill Extraction and Classification from Job Postings")). The advances in text
    embeddings over time are reflected in the scope of the approaches. While Javed
    et al. ([2017](#bib.bib31)) and Botov et al. ([2019](#bib.bib8)) improve the matching
    using word2vec embeddings(Mikolov et al., [2013](#bib.bib45)), later Li et al.
    ([2020](#bib.bib40)) use FastText (Bojanowski et al., [2017](#bib.bib6)) leveraging
    sub-word information to handle out-of-vocabulary words and capture more detailed
    semantic and syntactic information. Khaouja et al. ([2021b](#bib.bib35)) compare
    using sent2vec trained on Wikipedia sentences, and SBERT (Reimers and Gurevych,
    [2019](#bib.bib48)) trained on millions of paraphrase sentences for embeddings.
    Moreover, Zhang et al. ([2022c](#bib.bib63)) uses LMs like RoBERTa and JobBERT
    to match n-grams from JP sentences with the ESCO taxonomy. They also experiment
    with context and frequency-aware embeddings. Gnehm et al. ([2022a](#bib.bib21))
    performed direct skill extraction using context-aware embeddings and the SBERT
    model similar to Zhang et al. ([2022c](#bib.bib63)), additionally they contextualize
    skill areas within spans and ontology terms using their hierarchical structure.
    The study explores techniques to enhance BERT model similarity, including in-domain
    pretraining, transformer-based sequential denoising auto-encoder (TSDAE; Wang
    et al., [2021](#bib.bib57)) for domain-specific terminology, and Siamese BERT
    Networks for training sentence embeddings (Reimers and Gurevych, [2019](#bib.bib48)).
    They further leverage MNR loss in Siamese networks (Henderson et al., [2017](#bib.bib30)),
    using ontology data to create positive text pairings for better label matching.
    SkillGPT Li et al. ([2023](#bib.bib39)) is the first tool to use an LLM for the
    matching task, they convert ESCO entries into structured documents, which are
    vectorized by the LM. Then, they summarize the input text, and use an embedding
    of the summary to retrieve the closest ESCO entries.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2 Extreme Multi-label Classification Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Bhola et al. ([2020](#bib.bib5)) were the first to formulate skill extraction
    against a skill base as an extreme multi-label classification (XMLC). They classify
    multiple skill labels per document using the labels of the BHOLA dataset (around
    2500 labels) as a skill base. Their BERT–XMLC framework, involves a Text Encoder
    that uses the pre-trained BERTbase model to convert JP texts into dense vector
    representations, a Bottleneck Layer that reduces overfitting by compressing these
    representations (Liu et al., [2017](#bib.bib41)) and subsequently a fully connected
    layer for multi-label classification of the skills. Enhancements include focusing
    on semantic skill label representation and skill co-occurrence, using bootstrapping
    to augment training data, and improve skill correlation capture. Their model outperformed
    XMLC baselines. Vermeer et al. ([2022](#bib.bib56)) adapted this approach for
    using RobBERT and additional linear layers, validating on BHOLA and a non-public
    Dutch dataset. Similarly, Anand et al. ([2022](#bib.bib1)) extended the model
    to predict skill importance using LaBSE-encoded Feng et al. ([2022](#bib.bib20))
    job titles, ranking skills from an in-house database based on a 0-1 scale of importance.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequent publications have concentrated on XMLC for skill extraction and classification
    using the ESCO taxonomy with around 13000 labels. For a pure skill classification
    for already identified skill spans Zhang et al. ([2022b](#bib.bib62)) use distant
    supervision by querying the ESCO API for the fine-grained skill labels. For model
    training, they employ zero-shot cross-lingual transfer learning techniques using
    various BERT models and fine-tune them on Danish JPs. The effectiveness of the
    models is tested on an adapted version of SKILLSPAN and KOMPETENCER. The same
    year Decorte et al. ([2022](#bib.bib16)) addressed the XMLC task on the sentence-level,
    again using distant supervision with the ESCO taxonomy. They enhance binary skill
    classifier training with three negative sampling strategies, involving siblings
    in ESCO hierarchy, Levenshtein distance, and cosine similarity of RoBERTa-encoded
    skill names. Their model employs a frozen pre-trained RoBERTa with mean pooling
    for sentence representation, followed by separate binary classifiers for each
    skill, evaluated on DECORTE.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the similarity-based approaches, LLMs are prominent in recent XMLC approaches.
    Unlike Li et al. ([2023](#bib.bib39)), Decorte et al. ([2023](#bib.bib17)) use
    the LLM solely during training to reduce latency and enhance reproducibility.
    They create a synthetic training dataset using the LLM, then optimize a bi-encoder
    through contrastive training, to effectively represent both skill names and corresponding
    sentences in close proximity within the same space. This method outperforms the
    distance supervision baseline by Decorte et al. ([2022](#bib.bib16)) (see Table [5](#A1.T5
    "Table 5 ‣ A.3 Scores of Selected Models ‣ Appendix A Appendix ‣ Deep Learning-based
    Computational Job Market Analysis: A Survey on Skill Extraction and Classification
    from Job Postings")). Similarly, Clavié and Soulié ([2023](#bib.bib12)) treat
    the skill extraction and classification task as individual binary classification
    problems, using GPT-3.5 like Decorte et al. ([2023](#bib.bib17)) but generating
    more spans per skill for synthetic training. They propose two extraction methods:
    one using linear classifiers for each skill, employing hard negative sampling
    (Robinson et al., [2021](#bib.bib49)) for improved skill differentiation, and
    another based on similarity, utilizing E5-LARGE-V2 embeddings (Wang et al., [2022](#bib.bib58))
    for cosine similarity calculations between JP extracts and ESCO labels or synthetic
    sentences. Potential skills are then reranked using an LLM. In evaluations using
    the DECORTE dataset, their methods achieved high performance with GPT-4, though
    results with GPT-3.5 were lower than Decorte et al. ([2023](#bib.bib17)), see
    Table [5](#A1.T5 "Table 5 ‣ A.3 Scores of Selected Models ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings") in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Goyal et al. ([2023](#bib.bib24)) present JobXMLC, a unique framework for the
    XMLC task, distinct from the prevailing methods. JobXMLC integrates a job-skill
    graph to represent job-skill interconnections, utilizes a GNN for multi-hop embeddings
    from the graph’s structure, and incorporates an extreme classification system
    with skill attention based on skill frequency in the dataset. The framework’s
    effectiveness is validated on the BHOLA and a proprietary StackOverflow dataset,
    see Table [5](#A1.T5 "Table 5 ‣ A.3 Scores of Selected Models ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings") in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusions and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent publications indicate two emerging trends in skill extraction. Firstly,
    extracting skills against skill bases like ESCO is gaining popularity, facilitating
    cross-industry and regional comparisons. Secondly, LLMs are increasingly applied
    in skill extraction and classification, proving particularly advantageous due
    to the scarcity of training data in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Future research in skill extraction and classification could focus on emerging
    skills and the extraction of implicit skills. Methods like those by Javed et al.
    ([2017](#bib.bib31)) and Khaouja et al. ([2021b](#bib.bib35)) update skill bases
    with emerging technologies and frequently used keywords, but evaluating these
    remains difficult without a standard benchmark. The challenge of extracting implicit
    skills, not directly stated in job postings, is also gaining attention. Techniques
    include prompting LLMs to generate training data with implied skills (Clavié and
    Soulié, [2023](#bib.bib12)) and using complete sentences to encompass both explicit
    and implicit skills (Decorte et al., [2022](#bib.bib16), [2023](#bib.bib17)).
    However, these methods need thorough evaluation, presenting an open field for
    future exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A limitation that should be considered is that only publications in the English
    language (although data was from multiple languages) were surveyed in this paper.
    Second, to allow for a deeper focus publications regarding topic modeling were
    excluded even if they used deep-learning-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank the reviewers for their insightful feedback. ES acknowledges financial
    support with funds provided by the German Federal Ministry for Economic Affairs
    and Climate Action due to an enactment of the German Bundestag under grant 46SKD127X
    (GENESIS). MZ is supported by the Independent Research Fund Denmark (DFF) grant
    9131-00019B and BP is supported by ERC Consolidator Grant DIALECT 101043235.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anand et al. (2022) Sarthak Anand, Jens-Joris Decorte, and Niels Lowie. 2022.
    [Is it required? ranking the skills required for a job-title](http://arxiv.org/abs/2212.08553).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ao et al. (2023) Ziqiao Ao, Gergely Horváth, Chunyuan Sheng, Yifan Song, and
    Yutong Sun. 2023. [Skill requirements in job advertisements: A comparison of skill-categorization
    methods based on wage regressions](https://doi.org/https://doi.org/10.1016/j.ipm.2022.103185).
    *Information Processing & Management*, 60(2):103185.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beauchemin et al. (2022) David Beauchemin, Julien Laumonier, Yvan Le Ster,
    and Marouane Yassine. 2022. ["fijo": a french insurance soft skill detection dataset](https://arxiv.org/abs/2204.05208).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bernabé-Moreno et al. (2019) Juan Bernabé-Moreno, Álvaro Tejeda-Lorente, Julio
    Herce-Zelaya, Carlos Porcel, and Enrique Herrera-Viedma. 2019. [An automatic skills
    standardization method based on subject expert knowledge extraction and semantic
    matching](https://doi.org/https://doi.org/10.1016/j.procs.2019.12.060). *Procedia
    Computer Science*, 162:857–864. 7th International Conference on Information Technology
    and Quantitative Management (ITQM 2019): Information technology and quantitative
    management based on Artificial Intelligence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bhola et al. (2020) Akshay Bhola, Kishaloy Halder, Animesh Prasad, and Min-Yen
    Kan. 2020. [Retrieving skills from job descriptions: A language model based extreme
    multi-label classification framework](https://doi.org/10.18653/v1/2020.coling-main.513).
    In *Proceedings of the 28th International Conference on Computational Linguistics*,
    pages 5832–5842, Barcelona, Spain (Online). International Committee on Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bojanowski et al. (2017) Piotr Bojanowski, Edouard Grave, Armand Joulin, and
    Tomas Mikolov. 2017. [Enriching word vectors with subword information](https://doi.org/10.1162/tacl_a_00051).
    *Transactions of the Association for Computational Linguistics*, 5:135–146.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boselli et al. (2018) Roberto Boselli, Mirko Cesarini, Fabio Mercorio, and Mario
    Mezzanzanica. 2018. [Classifying online job advertisements through machine learning](https://doi.org/https://doi.org/10.1016/j.future.2018.03.035).
    *Future Generation Computer Systems*, 86:319–328.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Botov et al. (2019) Dmitriy Botov, Julius Klenin, Andrey Melnikov, Yuri Dmitrin,
    Ivan Nikolaev, and Mikhail Vinel. 2019. [Mining labor market requirements using
    distributional semantic models and deep learning](https://doi.org/10.1007/978-3-030-20482-2_15).
    In *Business Information Systems - 22nd International Conference, BIS 2019, Seville,
    Spain, June 26-28, 2019, Proceedings, Part II*, volume 354 of *Lecture Notes in
    Business Information Processing*, pages 177–190\. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Buchmann et al. (2022) Marlis Buchmann, Helen Buchs, Felix Busch, Simon Clematide,
    Ann-Sophie Gnehm, and Jan Müller. 2022. Swiss job market monitor: A rich source
    of demand-side micro data of the labour market. *European Sociological Review*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao and Zhang (2021) Lina Cao and Jian Zhang. 2021. [Skill requirements analysis
    for data analysts based on named entities recognition](https://doi.org/10.1109/ICBDIE52740.2021.00023).
    In *2021 2nd International Conference on Big Data and Informatization Education
    (ICBDIE)*, pages 64–68.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2021) Lina Cao, Jian Zhang, Xinquan Ge, and Jindong Chen. 2021.
    [Occupational profiling driven by online job advertisements: Taking the data analysis
    and processing engineering technicians as an example](https://api.semanticscholar.org/CorpusID:235609409).
    *PLoS ONE*, 16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clavié and Soulié (2023) Benjamin Clavié and Guillaume Soulié. 2023. [Large
    language models as batteries-included zero-shot esco skills matchers](https://arxiv.org/abs/2307.03539).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Colombo et al. (2019) Emilio Colombo, Fabio Mercorio, and Mario Mezzanzanica.
    2019. [Ai meets labor market: Exploring the link between automation and skills](https://doi.org/https://doi.org/10.1016/j.infoecopol.2019.05.003).
    *Information Economics and Policy*, 47:27–37. The Economics of Artificial Intelligence
    and Machine Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
    Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,
    and Veselin Stoyanov. 2020. [Unsupervised cross-lingual representation learning
    at scale](https://doi.org/10.18653/v1/2020.acl-main.747). In *Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics*, pages 8440–8451,
    Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Council et al. (2010) National Research Council, Nancy Thomas Tippins, Margaret L
    Hilton, et al. 2010. *A database for a changing economy: Review of the Occupational
    Information Network (O* NET)*. National Academies Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decorte et al. (2022) Jens-Joris Decorte, Jeroen Van Hautte, Johannes Deleu,
    Chris Develder, and Thomas Demeester. 2022. [Design of negative sampling strategies
    for distantly supervised skill extraction](https://ceur-ws.org/Vol-3218/RecSysHR2022-paper_4.pdf).
    In *Proceedings of the 2nd Workshop on Recommender Systems for Human Resources
    (RecSys-in-HR 2022)*, volume 3218, page 7\. CEUR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decorte et al. (2023) Jens-Joris Decorte, Severine Verlinden, Jeroen Van Hautte,
    Johannes Deleu, Chris Develder, and Thomas Demeester. 2023. [Extreme multi-label
    skill extraction training using large language models](https://arxiv.org/abs/2307.10778).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: Pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/N19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. (2023) Chuyu Fang, Chuan Qin, Qi Zhang, Kaichun Yao, Jingshuai
    Zhang, Hengshu Zhu, Fuzhen Zhuang, and Hui Xiong. 2023. [Recruitpro: A pretrained
    language model with skill-aware prompt learning for intelligent recruitment](https://doi.org/10.1145/3580305.3599894).
    In *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*, KDD ’23, page 3991–4002, New York, NY, USA. Association for Computing
    Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2022) Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan,
    and Wei Wang. 2022. [Language-agnostic BERT sentence embedding](https://doi.org/10.18653/v1/2022.acl-long.62).
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 878–891, Dublin, Ireland. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gnehm et al. (2022a) Ann-sophie Gnehm, Eva Bühlmann, Helen Buchs, and Simon
    Clematide. 2022a. [Fine-grained extraction and classification of skill requirements
    in German-speaking job ads](https://aclanthology.org/2022.nlpcss-1.2). In *Proceedings
    of the Fifth Workshop on Natural Language Processing and Computational Social
    Science (NLP+CSS)*, pages 14–24, Abu Dhabi, UAE. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gnehm et al. (2022b) Ann-Sophie Gnehm, Eva Bühlmann, and Simon Clematide. 2022b.
    [Evaluation of transfer learning and domain adaptation for analyzing German-speaking
    job advertisements](https://aclanthology.org/2022.lrec-1.414). In *Proceedings
    of the Thirteenth Language Resources and Evaluation Conference*, pages 3892–3901,
    Marseille, France. European Language Resources Association.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gnehm and Clematide (2020) Ann-Sophie Gnehm and Simon Clematide. 2020. [Text
    zoning and classification for job advertisements in German, French and English](https://doi.org/10.18653/v1/2020.nlpcss-1.10).
    In *Proceedings of the Fourth Workshop on Natural Language Processing and Computational
    Social Science*, pages 83–93, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal et al. (2023) Nidhi Goyal, Jushaan Kalra, Charu Sharma, Raghava Mutharaju,
    Niharika Sachdeva, and Ponnurangam Kumaraguru. 2023. [JobXMLC: EXtreme multi-label
    classification of job skills with graph neural networks](https://aclanthology.org/2023.findings-eacl.163).
    In *Findings of the Association for Computational Linguistics: EACL 2023*, pages
    2181–2191, Dubrovnik, Croatia. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Green et al. (2022) Thomas Green, Diana Maynard, and Chenghua Lin. 2022. [Development
    of a benchmark corpus to support entity recognition in job descriptions](https://aclanthology.org/2022.lrec-1.128).
    In *Proceedings of the Thirteenth Language Resources and Evaluation Conference*,
    pages 1201–1208, Marseille, France. European Language Resources Association.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grüger and Schneider (2019) Joscha Grüger and Georg Schneider. 2019. [Automated
    analysis of job requirements for computer scientists in online job advertisements](https://doi.org/10.5220/0008068202260233).
    In *Proceedings of the 15th International Conference on Web Information Systems
    and Technologies*, WEBIST 2019, page 226–233, Setubal, PRT. SCITEPRESS - Science
    and Technology Publications, Lda.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gugnani and Misra (2020) Akshay Gugnani and Hemant Misra. 2020. [Implicit skills
    extraction using document embedding and its use in job recommendation](https://aaai.org/ojs/index.php/AAAI/article/view/7038).
    In *The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
    Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
    2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2020, New York, NY, USA, February 7-12, 2020*, pages 13286–13293\. AAAI Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gururangan et al. (2020) Suchin Gururangan, Ana Marasović, Swabha Swayamdipta,
    Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. [Don’t stop pretraining:
    Adapt language models to domains and tasks](https://doi.org/10.18653/v1/2020.acl-main.740).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 8342–8360, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HarperCollins Publishers (2023) HarperCollins Publishers. 2023. Collins COBUILD
    Advanced Learner’s Dictionary: Soft Skills. [https://www.collinsdictionary.com/dictionary/english/soft-skills](https://www.collinsdictionary.com/dictionary/english/soft-skills).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henderson et al. (2017) Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun hsuan
    Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil.
    2017. [Efficient natural language response suggestion for smart reply](http://arxiv.org/abs/1705.00652).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Javed et al. (2017) Faizan Javed, Phuong Hoang, Thomas Mahoney, and Matt McNair.
    2017. Large-scale occupational skills normalization for online recruitment. In
    *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 31, pages
    4627–4634.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2018) Shanshan Jia, Xiaoan Liu, Ping Zhao, Chang Liu, Lianying Sun,
    and Tao Peng. 2018. Representation of job-skill in artificial intelligence with
    knowledge graph analysis. In *2018 IEEE symposium on product compliance engineering-asia
    (ISPCE-CN)*, pages 1–6\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke
    Zettlemoyer, and Omer Levy. 2020. [SpanBERT: Improving pre-training by representing
    and predicting spans](https://doi.org/10.1162/tacl_a_00300). *Transactions of
    the Association for Computational Linguistics*, 8:64–77.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khaouja et al. (2021a) Imane Khaouja, Ismail Kassou, and Mounir Ghogho. 2021a.
    [A survey on skill identification from online job ads](https://doi.org/10.1109/ACCESS.2021.3106120).
    *IEEE Access*, 9:118134–118153.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khaouja et al. (2021b) Imane Khaouja, Ghita Mezzour, and Ismail Kassou. 2021b.
    [Unsupervised skill identification from job ads](https://doi.org/10.1109/IRI51335.2021.00026).
    In *2021 IEEE 22nd International Conference on Information Reuse and Integration
    for Data Science (IRI)*, pages 147–151.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kortum et al. (2022) Henrik Kortum, Jonas Rebstadt, and Oliver Thomas. 2022.
    [Dissection of AI job advertisements: A text mining-based analysis of employee
    skills in the disciplines computer vision and natural language processing](http://hdl.handle.net/10125/79973).
    In *55th Hawaii International Conference on System Sciences, HICSS 2022, Virtual
    Event / Maui, Hawaii, USA, January 4-7, 2022*, pages 1–10\. ScholarSpace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lample et al. (2016) Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian,
    Kazuya Kawakami, and Chris Dyer. 2016. [Neural architectures for named entity
    recognition](https://doi.org/10.18653/v1/N16-1030). In *Proceedings of the 2016
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 260–270, San Diego, California.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'le Vrang et al. (2014) Martin le Vrang, Agis Papantoniou, Erika Pauwels, Pieter
    Fannes, Dominique Vandensteen, and Johan De Smedt. 2014. [Esco: Boosting job matching
    in europe with semantic interoperability](https://doi.org/10.1109/MC.2014.283).
    *Computer*, 47(10):57–64.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Nan Li, Bo Kang, and Tijl De Bie. 2023. [Skillgpt: a restful
    api service for skill extraction and standardization using a large language model](https://arxiv.org/abs/2304.11060).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Shan Li, Baoxu Shi, Jaewon Yang, Ji Yan, Shuai Wang, Fei Chen,
    and Qi He. 2020. [Deep job understanding at linkedin](https://doi.org/10.1145/3397271.3401403).
    In *Proceedings of the 43rd International ACM SIGIR conference on research and
    development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30,
    2020*, pages 2145–2148\. ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2017) Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and Yiming Yang.
    2017. [Deep learning for extreme multi-label text classification](https://doi.org/10.1145/3077136.3080834).
    In *Proceedings of the 40th International ACM SIGIR Conference on Research and
    Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017*,
    pages 115–124\. ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lukauskas et al. (2023) Mantas Lukauskas, Viktorija Šarkauskaitė, Vaida Pilinkienė,
    Alina Stundziene, Andrius Grybauskas, and Jurgita Bruneckienė. 2023. [Enhancing
    skills demand understanding through job ad segmentation using nlp and clustering
    techniques](https://doi.org/10.3390/app13106119). *Applied Sciences*, 13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lyu and Liu (2021) Wenjing Lyu and Jin Liu. 2021. [Soft skills, hard skills:
    What matters most? evidence from job postings](https://doi.org/https://doi.org/10.1016/j.apenergy.2021.117307).
    *Applied Energy*, 300:117307.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martin et al. (2020) Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez,
    Yoann Dupont, Laurent Romary, Éric de la Clergerie, Djamé Seddah, and Benoît Sagot.
    2020. [CamemBERT: a tasty French language model](https://doi.org/10.18653/v1/2020.acl-main.645).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 7203–7219, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013) Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013. [Efficient estimation of word representations in vector space](http://arxiv.org/abs/1301.3781).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Napierala and Kvetan (2023) Joanna Napierala and Vladimir Kvetan. 2023. [*Changing
    Job Skills in a Changing World*](https://doi.org/10.1007/978-3-031-16624-2_13),
    pages 243–259\. Springer International Publishing, Cham.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papoutsoglou et al. (2019) Maria Papoutsoglou, Apostolos Ampatzoglou, Nikolaos
    Mittas, and Lefteris Angelis. 2019. [Extracting knowledge from on-line sources
    for software engineering labor market: A mapping study](https://doi.org/10.1109/ACCESS.2019.2949905).
    *IEEE Access*, 7:157595–157613.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. [Sentence-BERT:
    Sentence embeddings using Siamese BERT-networks](https://doi.org/10.18653/v1/D19-1410).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 3982–3992, Hong Kong, China. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robinson et al. (2021) Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra,
    and Stefanie Jegelka. 2021. [Contrastive learning with hard negative samples](https://openreview.net/forum?id=CR1XOQ0UTh-).
    In *9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021*. OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sayfullina et al. (2018) Luiza Sayfullina, Eric Malmi, and Juho Kannala. 2018.
    Learning representations for soft skill matching. In *Analysis of Images, Social
    Networks and Texts*, pages 141–152, Cham. Springer International Publishing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shang et al. (2018) Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R.
    Voss, and Jiawei Han. 2018. [Automated phrase mining from massive text corpora](https://doi.org/10.1109/TKDE.2018.2812203).
    *IEEE Transactions on Knowledge and Data Engineering*, 30(10):1825–1837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2020) Baoxu Shi, Jaewon Yang, Feng Guo, and Qi He. 2020. [Salience
    and market-aware skill extraction for job targeting](https://dl.acm.org/doi/10.1145/3394486.3403338).
    In *KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
    Virtual Event, CA, USA, August 23-27, 2020*, pages 2871–2879\. ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Souza et al. (2020) Fábio Souza, Rodrigo Nogueira, and Roberto Lotufo. 2020.
    [Portuguese named entity recognition using bert-crf](http://arxiv.org/abs/1909.10649).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tamburri et al. (2020) Damian A. Tamburri, Willem-Jan Van Den Heuvel, and Martin
    Garriga. 2020. [Dataops for societal intelligence: a data pipeline for labor market
    skills extraction and matching](https://doi.org/10.1109/IRI49571.2020.00063).
    In *2020 IEEE 21st International Conference on Information Reuse and Integration
    for Data Science (IRI)*, pages 391–394.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ternikov (2022) Andrei Ternikov. 2022. [Soft and hard skills identification:
    insights from it job advertisements in the cis region](https://doi.org/https://doi.org/10.7717/peerj-cs.946).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vermeer et al. (2022) Ninande Vermeer, Vera Provatorova, David Graus, Thilina
    Rajapakse, and Sepideh Mesbah. 2022. Using robbert and extreme multi-label classification
    to extract implicit and explicit skills from dutch job descriptions. In *compjobs
    ’22: Computational Jobs Marketplace, Feb 25, 2022*. ACM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021. [TSDAE:
    Using transformer-based sequential denoising auto-encoderfor unsupervised sentence
    embedding learning](https://doi.org/10.18653/v1/2021.findings-emnlp.59). In *Findings
    of the Association for Computational Linguistics: EMNLP 2021*, pages 671–688,
    Punta Cana, Dominican Republic. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun
    Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. [Text embeddings by weakly-supervised
    contrastive pre-training](http://arxiv.org/abs/2212.03533).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wild et al. (2021) Simon Wild, Soyhan Parlar, Thomas Hanne, and Rolf Dornberger.
    2021. [Naïve bayes and named entity recognition for requirements mining in job
    postings](https://doi.org/10.1109/ICNLP52887.2021.00032). In *2021 3rd International
    Conference on Natural Language Processing (ICNLP)*, pages 155–161.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. (2022) Kaichun Yao, Jingshuai Zhang, Chuan Qin, Peng Wang, Hengshu
    Zhu, and Hui Xiong. 2022. [Knowledge enhanced person-job fit for talent recruitment](https://doi.org/10.1109/ICDE53745.2022.00325).
    In *2022 IEEE 38th International Conference on Data Engineering (ICDE)*, pages
    3467–3480.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022a) Mike Zhang, Kristian Jensen, Sif Sonniks, and Barbara
    Plank. 2022a. [SkillSpan: Hard and soft skill extraction from English job postings](https://doi.org/10.18653/v1/2022.naacl-main.366).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 4962–4984,
    Seattle, United States. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022b) Mike Zhang, Kristian Nørgaard Jensen, and Barbara Plank.
    2022b. [Kompetencer: Fine-grained skill classification in Danish job postings
    via distant supervision and transfer learning](https://aclanthology.org/2022.lrec-1.46).
    In *Proceedings of the Thirteenth Language Resources and Evaluation Conference*,
    pages 436–447, Marseille, France. European Language Resources Association.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022c) Mike Zhang, Kristian Nørgaard Jensen, Rob van der Goot,
    and Barbara Plank. 2022c. Skill extraction from job postings using weak supervision.
    In *RecSys in HR’22: The 2nd Workshop on Recommender Systems for Human Resources,
    in conjunction with the 16th ACM Conference on Recommender Systems, September
    18–23, 2022, Seattle, USA.* CEUR Workshop Proceedings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Mike Zhang, Rob van der Goot, and Barbara Plank. 2023.
    [ESCOXLM-R: Multilingual taxonomy-driven pre-training for the job market domain](https://doi.org/10.18653/v1/2023.acl-long.662).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 11871–11890, Toronto, Canada. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Terminology Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [3](#A1.T3 "Table 3 ‣ A.1 Terminology Example ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings"), we present an example sentence for better
    terminology understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Familiar | with | building | tests | in | python |'
  prefs: []
  type: TYPE_TB
- en: '| $I$: | O | O | B | I | O | B |'
  prefs: []
  type: TYPE_TB
- en: '| $E_{C}$: | O | O | B[skill] | I[skill] | O | B[knowl.] |'
  prefs: []
  type: TYPE_TB
- en: '| $C_{D}/C_{E}$: | “Python (computer programming)”, “ plan ” |'
  prefs: []
  type: TYPE_TB
- en: '|  | “software testing” |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: An example with annotations for the different tasks described in Section [3](#S3
    "3 Skill-related Terminology ‣ Deep Learning-based Computational Job Market Analysis:
    A Survey on Skill Extraction and Classification from Job Postings"). For skill
    classification ($C$), we used the ESCO taxonomy in this example, and for skill
    extraction with coarse labels ($E_{C}$) we follow the guidelines of SkillSpan Zhang
    et al. ([2022a](#bib.bib61))'
  prefs: []
  type: TYPE_NORMAL
- en: '| Source | # Skill Spans | # Knowledge Spans |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SKILLSPAN - HOUSE | 2,146 | 1,418 |'
  prefs: []
  type: TYPE_TB
- en: '| DECORTE - HOUSE | 509* | 210* |'
  prefs: []
  type: TYPE_TB
- en: '| SKILLSPAN - TECH | 2,241 | 3,828 |'
  prefs: []
  type: TYPE_TB
- en: '| DECORTE - TECH | 419 | 480* |'
  prefs: []
  type: TYPE_TB
- en: '| KOMPETENCER | 665 | 255 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Number of labeled spans. The star * indicates, that two values found
    in the Decorte HOUSE test dataset (tagged as knowledge) were actually from the
    Skillspan TECH dataset; eight values found in the Decorte TECH test dataset (four
    skill spans, four knowledge spans) were actually from the Skillspan HOUSE dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Number of Skill and Knowledge Spans
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [4](#A1.T4 "Table 4 ‣ A.1 Terminology Example ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings"), we show the number of labeled spans for
    skills and knowledge in the SKILLSPAN Zhang et al. ([2022a](#bib.bib61)), DECORTE Decorte
    et al. ([2022](#bib.bib16)), and KOMPETENCER Zhang et al. ([2022b](#bib.bib62))
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Scores of Selected Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [5](#A1.T5 "Table 5 ‣ A.3 Scores of Selected Models ‣ Appendix A Appendix
    ‣ Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction
    and Classification from Job Postings"), we display the scores of recent LMM-based
    approaches on the DECORTE Decorte et al. ([2022](#bib.bib16)) dataset for comparison.
    Furthermore, we show results of Zhang et al. ([2023](#bib.bib64)); Goyal et al.
    ([2023](#bib.bib24)) and (Bhola et al., [2020](#bib.bib5)) on the BHOLA Bhola
    et al. ([2020](#bib.bib5)) dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Source | HOUSE* | TECH* | BHOLA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | MRR | RP@5 | RP@10 | MRR | RP@5 | RP@10 | MRR | R@5 | R@10 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $Classifier^{neg}$ | (Decorte et al., [2022](#bib.bib16)) | 0.299 | 30.82
    | 38.69 | 0.326 | 31.71 | 39.09 | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| $GPTsentences^{aug}$ | (Decorte et al., [2023](#bib.bib17)) | 0.428 | 45.74
    | N/A | 0.529 | 54.62 | N/A | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| $GPT3.5Re-ranking$ | (Clavié and Soulié, [2023](#bib.bib12)) | 0.427 | 43.57
    | 51.44 | 0.488 | 52.50 | 59.75 | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| $GPT4Re-ranking$ | (Clavié and Soulié, [2023](#bib.bib12)) | 0.495 | 53.34
    | 61.02 | 0.537 | 61.50 | 68.94 | N/A | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| $BERT\textendash XMLC+CAB$ | (Bhola et al., [2020](#bib.bib5)) | N/A | N/A
    | N/A | N/A | N/A | N/A | 0.9049 | 21.67 | 40.49 |'
  prefs: []
  type: TYPE_TB
- en: '| $JobXMLC$ | (Goyal et al., [2023](#bib.bib24)) | N/A | N/A | N/A | N/A |
    N/A | N/A | 0.90 | 18.29 | 32.33 |'
  prefs: []
  type: TYPE_TB
- en: '| $ESCOXML-R$ | (Zhang et al., [2023](#bib.bib64)) | N/A | N/A | N/A | N/A
    | N/A | N/A | 0.907 | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Scores of selected models on the benchmarking datasets DECORTE and
    BHOLA.'
  prefs: []
  type: TYPE_NORMAL
