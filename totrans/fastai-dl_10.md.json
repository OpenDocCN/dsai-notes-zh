["```py\n**from** **fastai.text** **import** *\n**import** **html**\n```", "```py\nBOS = 'xbos'  *# beginning-of-sentence tag*\nFLD = 'xfld'  *# data field tag*PATH=Path('data/aclImdb/')\n```", "```py\nCLAS_PATH=Path('data/imdb_clas/')\nCLAS_PATH.mkdir(exist_ok=**True**)LM_PATH=Path('data/imdb_lm/')\nLM_PATH.mkdir(exist_ok=**True**)\n```", "```py\nCLASSES = ['neg', 'pos', 'unsup']**def** get_texts(path):\n    texts,labels = [],[]\n    **for** idx,label **in** enumerate(CLASSES):\n        **for** fname **in** (path/label).glob('*.*'):\n            texts.append(fname.open('r').read())\n            labels.append(idx)\n    **return** np.array(texts),np.array(labels)trn_texts,trn_labels = get_texts(PATH/'train')\nval_texts,val_labels = get_texts(PATH/'test')len(trn_texts),len(val_texts)*(75000, 25000)*\n```", "```py\ncol_names = ['labels','text']\n```", "```py\nnp.random.seed(42)\ntrn_idx = np.random.permutation(len(trn_texts))\nval_idx = np.random.permutation(len(val_texts))\n```", "```py\ntrn_texts = trn_texts[trn_idx]\nval_texts = val_texts[val_idx]trn_labels = trn_labels[trn_idx]\nval_labels = val_labels[val_idx]\n```", "```py\ndf_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, \n                      columns=col_names)\ndf_val = pd.DataFrame({'text':val_texts, 'labels':val_labels},\n                      columns=col_names)df_trn[df_trn['labels']!=2].to_csv(CLAS_PATH/'train.csv',\n                                   header=**False**, index=**False**)\ndf_val.to_csv(CLAS_PATH/'test.csv', header=**False**, index=**False**)(CLAS_PATH/'classes.txt').open('w')\n    .writelines(f'**{o}\\n**' **for** o **in** CLASSES)\n(CLAS_PATH/'classes.txt').open().readlines()*['neg\\n', 'pos\\n', 'unsup\\n']*\n```", "```py\ntrn_texts,val_texts = sklearn.model_selection.train_test_split(\n    np.concatenate([trn_texts,val_texts]), test_size=0.1)len(trn_texts), len(val_texts)*(90000, 10000)*\n```", "```py\ndf_trn = pd.DataFrame({'text':trn_texts, 'labels':\n                       [0]*len(trn_texts)}, columns=col_names)\ndf_val = pd.DataFrame({'text':val_texts, 'labels':\n                       [0]*len(val_texts)}, columns=col_names)df_trn.to_csv(LM_PATH/'train.csv', header=**False**, index=**False**)\ndf_val.to_csv(LM_PATH/'test.csv', header=**False**, index=**False**)\n```", "```py\nchunksize=24000\n```", "```py\nre1 = re.compile(r'  +')**def** fixup(x):\n   x = x.replace('#39;', \"'\").replace('amp;', '&')\n        .replace('#146;', \"'\").replace('nbsp;', ' ')\n        .replace('#36;', '$').replace('**\\\\**n', \"**\\n**\")\n        .replace('quot;', \"'\").replace('<br />', \"**\\n**\")\n        .replace('**\\\\**\"', '\"').replace('<unk>','u_n')\n        .replace(' @.@ ','.').replace(' @-@ ','-')\n        .replace('**\\\\**', ' **\\\\** ')\n    **return** re1.sub(' ', html.unescape(x))**def** get_texts(df, n_lbls=1):\n    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n    texts = f'**\\n{BOS}** **{FLD}** 1 ' + df[n_lbls].astype(str)\n    **for** i **in** range(n_lbls+1, len(df.columns)): \n        texts += f' **{FLD}** {i-n_lbls} ' + df[i].astype(str)\n    texts = texts.apply(fixup).values.astype(str) tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n    **return** tok, list(labels)\n```", "```py\n**def** get_all(df, n_lbls):\n    tok, labels = [], []\n    **for** i, r **in** enumerate(df):\n        print(i)\n        tok_, labels_ = get_texts(r, n_lbls)\n        tok += tok_;\n        labels += labels_\n    **return** tok, labels\n```", "```py\ndf_trn = pd.read_csv(LM_PATH/'train.csv', header=**None**, \n                     chunksize=chunksize)\ndf_val = pd.read_csv(LM_PATH/'test.csv', header=**None**, \n                     chunksize=chunksize)tok_trn, trn_labels = get_all(df_trn, 1)\ntok_val, val_labels = get_all(df_val, 1)*0\n1\n2\n3\n0*(LM_PATH/'tmp').mkdir(exist_ok=**True**)\n```", "```py\n' '.join(tok_trn[0])\n```", "```py\nnp.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)\nnp.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')\ntok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')\n```", "```py\nfreq = Counter(p **for** o **in** tok_trn **for** p **in** o)\nfreq.most_common(25)*[('the', 1207984),\n ('.', 991762),\n (',', 985975),\n ('and', 587317),\n ('a', 583569),\n ('of', 524362),\n ('to', 484813),\n ('is', 393574),\n ('it', 341627),\n ('in', 337461),\n ('i', 308563),\n ('this', 270705),\n ('that', 261447),\n ('\"', 236753),\n (\"'s\", 221112),\n ('-', 188249),\n ('was', 180235),\n ('\\n\\n', 178679),\n ('as', 165610),\n ('with', 159164),\n ('for', 158981),\n ('movie', 157676),\n ('but', 150203),\n ('film', 144108),\n ('you', 124114)]*\n```", "```py\nmax_vocab = 60000\nmin_freq = 2itos = [o **for** o,c **in** freq.most_common(max_vocab) **if** c>min_freq]\nitos.insert(0, '_pad_')\nitos.insert(0, '_unk_')\n```", "```py\nstoi = collections.defaultdict(**lambda**:0, \n                               {v:k **for** k,v **in** enumerate(itos)})\nlen(itos)*60002*\n```", "```py\ntrn_lm = np.array([[stoi[o] **for** o **in** p] **for** p **in** tok_trn])\nval_lm = np.array([[stoi[o] **for** o **in** p] **for** p **in** tok_val])\n```", "```py\nnp.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\nnp.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)\npickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))\n```", "```py\ntrn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')\nval_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')\nitos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))\n```", "```py\nvs=len(itos)\nvs,len(trn_lm)*(60002, 90000)*\n```", "```py\n# ! wget -nH -r -np -P {PATH} [http://files.fast.ai/models/wt103/](http://files.fast.ai/models/wt103/)\n```", "```py\nem_sz,nh,nl = 400,1150,3\n```", "```py\nPRE_PATH = PATH**/**'models'**/**'wt103'\nPRE_LM_PATH = PRE_PATH**/**'fwd_wt103.h5'\n```", "```py\nwgts = torch.load(PRE_LM_PATH, map_location=**lambda** storage, \n                  loc: storage)enc_wgts = to_np(wgts['0.encoder.weight'])\nrow_m = enc_wgts.mean(0)\n```", "```py\nitos2 = pickle.load((PRE_PATH**/**'itos_wt103.pkl').open('rb'))stoi2 = collections.defaultdict(**lambda**:**-**1, {v:k **for** k,v \n                                              **in** enumerate(itos2)})\n```", "```py\nnew_w = np.zeros((vs, em_sz), dtype=np.float32)\n**for** i,w **in** enumerate(itos):\n    r = stoi2[w]\n    new_w[i] = enc_wgts[r] **if** r**>**=0 **else** row_m\n```", "```py\nwgts['0.encoder.weight'] = T(new_w)\nwgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\nwgts['1.decoder.weight'] = T(np.copy(new_w))\n```", "```py\nwd=1e-7\nbptt=70\nbs=52\nopt_fn = partial(optim.Adam, betas=(0.8, 0.99))t = len(np.concatenate(trn_lm))\nt, t//64*(24998320, 390598)*\n```", "```py\ntrn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\nval_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\nmd = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, \n                       bptt=bptt)\n```", "```py\ndrops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])***0.7**learner= md.get_model(opt_fn, em_sz, nh, nl, \n    dropouti=drops[0], dropout=drops[1], wdrop=drops[2],\n    dropoute=drops[3], dropouth=drops[4])learner.metrics = [accuracy]\nlearner.freeze_to(-1)\n```", "```py\nlearner.model.load_state_dict(wgts)lr=1e-3\nlrs = lrlearner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)*epoch      trn_loss   val_loss   accuracy                     \n    0      4.398856   4.175343   0.28551**[4.175343, 0.2855095456305303]*learner.save('lm_last_ft')learner.load('lm_last_ft')learner.unfreeze()learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=**True**)learner.sched.plot()\n```", "```py\nlearner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)epoch      trn_loss   val_loss   accuracy                     \n    0      4.332359   4.120674   0.289563  \n    1      4.247177   4.067932   0.294281                     \n    2      4.175848   4.027153   0.298062                     \n    3      4.140306   4.001291   0.300798                     \n    4      4.112395   3.98392    0.302663                     \n    5      4.078948   3.971053   0.304059                     \n    6      4.06956    3.958152   0.305356                     \n    7      4.025542   3.951509   0.306309                     \n    8      4.019778   3.94065    0.30756                      \n    9      4.027846   3.931385   0.308232                     \n    10     3.98106    3.928427   0.309011                     \n    11     3.97106    3.920667   0.30989                      \n    12     3.941096   3.917029   0.310515                     \n    13     3.924818   3.91302    0.311015                     \n    14     3.923296   3.908476   0.311586[3.9084756, 0.3115861900150776]\n```", "```py\nlearner.save('lm1')\nlearner.save_encoder('lm1_enc')\n```", "```py\nlearner.sched.plot_loss()\n```", "```py\ndf_trn = pd.read_csv(CLAS_PATH/'train.csv', header=**None**, \n                     chunksize=chunksize)\ndf_val = pd.read_csv(CLAS_PATH/'test.csv', header=**None**, \n                     chunksize=chunksize)tok_trn, trn_labels = get_all(df_trn, 1)\ntok_val, val_labels = get_all(df_val, 1)*0\n1\n0\n1*(CLAS_PATH/'tmp').mkdir(exist_ok=**True**)np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)\nnp.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)\nnp.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')\ntok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')\n```", "```py\nitos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\nstoi = collections.defaultdict(**lambda**:0, {v:k **for** k,v **in** \n                                          enumerate(itos)})\nlen(itos)*60002*trn_clas = np.array([[stoi[o] **for** o **in** p] **for** p **in** tok_trn])\nval_clas = np.array([[stoi[o] **for** o **in** p] **for** p **in** tok_val])np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)\nnp.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)\n```", "```py\ntrn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\nval_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\nval_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))\n```", "```py\nbptt,em_sz,nh,nl = 70,400,1150,3\nvs = len(itos)\nopt_fn = partial(optim.Adam, betas=(0.8, 0.99))\nbs = 48min_lbl = trn_labels.min()\ntrn_labels -= min_lbl\nval_labels -= min_lbl\nc=int(trn_labels.max())+1\n```", "```py\ntrn_ds = TextDataset(trn_clas, trn_labels)\nval_ds = TextDataset(val_clas, val_labels)\n```", "```py\ntrn_samp = SortishSampler(trn_clas, key=**lambda** x: len(trn_clas[x]), \n                          bs=bs//2)\nval_samp = SortSampler(val_clas, key=**lambda** x: len(val_clas[x]))trn_dl = DataLoader(trn_ds, bs//2, transpose=**True**, num_workers=1,\n                    pad_idx=1, sampler=trn_samp)\nval_dl = DataLoader(val_ds, bs, transpose=**True**, num_workers=1, \n                    pad_idx=1, sampler=val_samp)\nmd = ModelData(PATH, trn_dl, val_dl)\n```", "```py\n *# part 1*\ndps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5m = get_rnn_classifer(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, \n                      n_layers=nl, pad_token=1,\n                      layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n                      dropouti=dps[0], wdrop=dps[1],        \n                      dropoute=dps[2], dropouth=dps[3])opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n```", "```py\nlearn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\nlearn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\nlearn.clip=25.\nlearn.metrics = [accuracy]\n```", "```py\nlr=3e-3\nlrm = 2.6\nlrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n```", "```py\nwd = 1e-7\nwd = 0\nlearn.load_encoder('lm2_enc')\n```", "```py\nlearn.freeze_to(-1)learn.lr_find(lrs/1000)\nlearn.sched.plot()learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))*epoch      trn_loss   val_loss   accuracy                      \n    0      0.365457   0.185553   0.928719**[0.18555279, 0.9287188090884525]*learn.save('clas_0')\nlearn.load('clas_0')\n```", "```py\nlearn.freeze_to(-2)learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))*epoch      trn_loss   val_loss   accuracy                      \n    0      0.340473   0.17319    0.933125**[0.17319041, 0.9331253991245995]*learn.save('clas_1')\nlearn.load('clas_1')learn.unfreeze()learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))epoch      trn_loss   val_loss   accuracy                      \n    0      0.337347   0.186812   0.930782  \n    1      0.284065   0.318038   0.932062                      \n    2      0.246721   0.156018   0.941747                      \n    3      0.252745   0.157223   0.944106                      \n    4      0.24023    0.159444   0.945393                      \n    5      0.210046   0.202856   0.942858                      \n    6      0.212139   0.149009   0.943746                      \n    7      0.21163    0.186739   0.946553                      \n    8      0.186233   0.1508     0.945218                      \n    9      0.176225   0.150472   0.947985                      \n    10     0.198024   0.146215   0.948345                      \n    11     0.20324    0.189206   0.948145                      \n    12     0.165159   0.151402   0.947745                      \n    13     0.165997   0.146615   0.947905[0.14661488, 0.9479046703071374]learn.sched.plot_loss()learn.save('clas_2')\n```", "```py\nif __name__ == '__main__': fire.Fire(train_clas)\n```"]