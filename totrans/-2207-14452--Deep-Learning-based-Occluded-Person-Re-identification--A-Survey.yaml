- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:44:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2207.14452] Deep Learning-based Occluded Person Re-identification: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2207.14452](https://ar5iv.labs.arxiv.org/html/2207.14452)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning-based Occluded Person Re-identification: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yunjie Peng, Saihui Hou, Chunshui Cao, Xu Liu,
  prefs: []
  type: TYPE_NORMAL
- en: 'Yongzhen Huang, Zhiqiang He1 1Corresponding author.Yunjie Peng is with the
    School of Computer Science and Technology, Beihang University, Beijing 100191,
    China (email: yunjiepeng@buaa.edu.cn).Saihui Hou and Yongzhen Huang are with the
    School of Artificial Intelligence, Beijing Normal University, Beijing 100875,
    China and also with Watrix Technology Limited Co. Ltd, Beijing, China (email:
    housaihui@bnu.edu.cn; huangyongzhen@bnu.edu.cn).Chunshui Cao and Xu Liu are with
    the Watrix Technology Limited Co. Ltd, Beijing, China (email: chunshui.cao@watrix.ai;
    xu.liu@watrix.ai).Zhiqiang He is with the School of Computer Science and Technology,
    Beihang University, Beijing 100191, China and the Lenovo Corporation, Beijing,
    China (email: zqhe1963@gmail.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Occluded person re-identification (Re-ID) aims at addressing the occlusion
    problem when retrieving the person of interest across multiple cameras. With the
    promotion of deep learning technology and the increasing demand for intelligent
    video surveillance, the frequent occlusion in real-world applications has made
    occluded person Re-ID draw considerable interest from researchers. A large number
    of occluded person Re-ID methods have been proposed while there are few surveys
    that focus on occlusion. To fill this gap and help boost future research, this
    paper provides a systematic survey of occluded person Re-ID. Through an in-depth
    analysis of the occlusion in person Re-ID, most existing methods are found to
    only consider part of the problems brought by occlusion. Therefore, we review
    occlusion-related person Re-ID methods from the perspective of issues and solutions.
    We summarize four issues caused by occlusion in person Re-ID, i.e., position misalignment,
    scale misalignment, noisy information, and missing information. The occlusion-related
    methods addressing different issues are then categorized and introduced accordingly.
    After that, we summarize and compare the performance of recent occluded person
    Re-ID methods on four popular datasets: Partial-ReID, Partial-iLIDS, Occluded-ReID,
    and Occluded-DukeMTMC. Finally, we provide insights on promising future research
    directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Occluded Person Re-identification, Partial Person Re-identification, Literature
    Survey, Deep Learning.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Person re-identification (Re-ID) retrieves persons of the same identity across
    different cameras [[1](#bib.bib1)]. With the expanding deployment of surveillance
    cameras and the increasing demand for public safety, person Re-ID which plays
    the fundamental role in intelligent surveillance has become a research hotspot
    in the computer vision community. In practice, the internal variations of a person
    (e.g., pose variations [[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)] and clothes
    changes [[5](#bib.bib5), [6](#bib.bib6)]), as well as the complex environments
    (e.g., illumination changes [[7](#bib.bib7), [8](#bib.bib8)], viewpoint variations [[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)], and occlusion [[12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14)]), bring significant challenges to person Re-ID. Among them,
    the occlusion which occurs frequently in real-world applications and affects the
    accuracy greatly has received considerable interest from researchers.
  prefs: []
  type: TYPE_NORMAL
- en: Occluded person re-identification [[15](#bib.bib15), [16](#bib.bib16), [14](#bib.bib14)]
    is proposed to address the occlusion problem for real-world person re-identification.
    Different from general person Re-ID approaches which assume the retrieval process
    with whole human body available, occluded person Re-ID highlights the scenario
    of pedestrian images occluded by various obstacles (e.g., cars, trees, and crowds)
    and focuses on retrieving persons of the same identity when given an occluded
    query.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d251e866dce637738c1c7a400478e7cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The taxonomy of occluded person Re-ID methods from the perspective
    of issues and solutions. Using the above taxonomy, it is easy to know the inherent
    challenges for occluded person Re-ID and have a general understanding of overall
    technical routes.'
  prefs: []
  type: TYPE_NORMAL
- en: With the advancement of deep learning, a large number of occluded person Re-ID
    methods have been proposed while there are few surveys that focus on occlusion.
    To fill this gap, this paper summarizes occlusion-related person Re-ID works and
    provides a systematic survey of occluded person Re-ID. Through an in-depth analysis
    of the occlusion in person re-identification, most existing methods are found
    to only consider part of the problems caused by occlusion. Therefore, we review
    occluded person Re-ID from the perspective of issues and solutions to facilitate
    the understanding of the latest approaches and inspire new ideas in the field.
  prefs: []
  type: TYPE_NORMAL
- en: 'The issues caused by occlusion for person Re-ID are carefully summarized from
    the whole process of person re-identification. Technically speaking, a practical
    person Re-ID system in video surveillance mainly consists of three stages [[17](#bib.bib17)]:
    pedestrian detection, trajectory tracking, and person retrieval. Although it is
    generally believed that the first two stages are independent computer vision tasks
    and most person Re-ID works focus on person retrieval, the occlusion will affect
    the whole process and bring great challenges to the final re-identification. In
    summary, four significant issues are considered for occluded person Re-ID in this
    paper: the position misalignment, the scale misalignment, the noisy information,
    and the missing information. Each issue is illustrated in Fig. [2](#S1.F2 "Figure
    2 ‣ I Introduction ‣ Deep Learning-based Occluded Person Re-identification: A
    Survey") and we briefly introduce each issue as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 1). *The position misalignment.* Generally, the detected human boxes are resized
    by height to obtain the same size of input data for person retrieval. In the case
    of occlusion, the detected box of the person contains only part of the human body
    while it undergoes the same alignment processing as that of a non-occluded person.
    The contents at the same position of the processed partial image and the processed
    holistic image are likely to be mismatched, resulting in the position misalignment
    issue. 2). *The scale misalignment.* Similar to the position misalignment, the
    scale misalignment also arises from the upstream data processing procedure. The
    occlusion may affect the height of the detected box and thus influence the resizing
    ratio in the data processing, resulting in the scale misalignment between a partial
    and a holistic image. 3). *The noisy information.* In the detected boxes of occluded
    pedestrians, the occlusion is inevitably included in whole or in part and brings
    the noisy information for person Re-ID. 4). *The missing information.* In the
    detected boxes of occluded pedestrians, the identity information of occluded regions
    is missing, resulting in the missing information issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper analyzes occlusion-related person Re-ID methods regarding the above-mentioned
    four issues and provides a multi-dimensional taxonomy to categorize solutions
    for each issue (see Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning-based
    Occluded Person Re-identification: A Survey")). Specifically, we mainly review
    published publications of deep learning-based occluded person Re-ID from top conferences
    and journals before June, 2022, and meanwhile we also introduce some methods from
    other conferences and journals as supplements. We discuss the issues brought by
    occlusion for person Re-ID and provide an in-depth analysis of how the issues
    are addressed in recent works with evaluation results summarized accordingly.
    Particularly, some person Re-ID methods are closely related to occlusion and we
    also summarize these methods to obtain a more comprehensive survey for occluded
    person Re-ID. The main contributions of this survey lie in three aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To fill the gap of the occluded person Re-ID survey, we review recent person
    re-identification methods for occlusion and provide a systematic survey from the
    perspective of issues and solutions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize and compare the performance of mainstream occluded person Re-ID
    approaches for researchers and industries to use based on their practical needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize and analyze the advantages and disadvantages of different types
    of solutions for occluded person Re-ID and provide insights on promising research
    directions in this field.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The rest of this paper is organized as follows. Section 2 presents a summary
    of previous surveys and elaborates on efforts made by this survey compared with
    others. Section 3 summarizes the common datasets and evaluation metrics of occluded
    person Re-ID. Section 4 provides an in-depth analysis of deep learning-based occluded
    person Re-ID methods from the perspective of issues and solutions. Section 5 compares
    the performance of various solutions and provides insights on promising research
    directions. Section 6 gives our conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/469d50fa3a831e3e17d421a9740a0318.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The position misalignment, scale misalignment, noisy information,
    and missing information issues caused by occlusion for person Re-ID.'
  prefs: []
  type: TYPE_NORMAL
- en: II Previous Surveys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous literature, there are some surveys that have also reviewed
    the field of person Re-ID. To obtain a more comprehensive comparison, we summarize
    the surveys of person Re-ID since 2012. The taxonomies of these surveys are listed
    in Table [I](#S2.T1 "TABLE I ‣ II Previous Surveys ‣ Deep Learning-based Occluded
    Person Re-identification: A Survey"). On the whole, previous surveys of person
    Re-ID can be roughly divided into traditional surveys [[18](#bib.bib18), [19](#bib.bib19)]
    and deep learning-based surveys [[17](#bib.bib17), [20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26),
    [27](#bib.bib27), [1](#bib.bib1), [28](#bib.bib28)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional surveys mainly review person Re-ID methods that manually design
    the feature extraction procedure and learn a better similarity measurement. Mazzon
    et al., [[18](#bib.bib18)] summarize four main phases for person Re-ID: multi-person
    detection, feature extraction, cross-camera calibration, and person association.
    Assuming that the first phase (i.e., multi-person detection) has been solved,
    methods of extracting color/texture/shape appearance features, grouping temporal
    information, conducting color/spatio-temporal cross-camera calibration, and distance/learning/optimization-based
    person association are reviewed accordingly. Apurva et al., [[19](#bib.bib19)]
    regard the tracking across multiple cameras as the open set matching problem and
    the identity retrieval as the close set matching problem. According to whether
    additional camera geometry/calibration information is available, methods are divided
    into contextual Re-ID and non-contextual Re-ID for open-set and close-set matching
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning-based surveys mainly summarize the person Re-ID methods using
    deep learning techniques from different perspectives. A few of these surveys [[17](#bib.bib17),
    [24](#bib.bib24)] also review traditional methods for the sake of completeness.
    In general, previous deep learning-based surveys have involved loss design [[20](#bib.bib20),
    [25](#bib.bib25)], technical means [[25](#bib.bib25), [28](#bib.bib28)], data
    augmentation [[20](#bib.bib20), [21](#bib.bib21)], image and video [[17](#bib.bib17),
    [1](#bib.bib1)], classification and verification [[20](#bib.bib20), [21](#bib.bib21),
    [23](#bib.bib23)], open-world and close-world [[22](#bib.bib22), [1](#bib.bib1)],
    multi-modality [[22](#bib.bib22), [27](#bib.bib27)], ranking optimization [[24](#bib.bib24),
    [1](#bib.bib1)], noisy annotation [[1](#bib.bib1)], unsupervised learning [[26](#bib.bib26)],
    and metric learning [[21](#bib.bib21), [24](#bib.bib24), [25](#bib.bib25), [28](#bib.bib28),
    [1](#bib.bib1)]. Despite such a number of surveys on person Re-ID, the occlusion
    problem has not drawn enough attention and only a few surveys pay attention to
    occluded person Re-ID. As far as we know, Ye et al.,  [[1](#bib.bib1)] have made
    a rough summary of occluded person Re-ID as a part of Noise-Robust Re-ID. Ming
    et al.,  [[28](#bib.bib28)] have included several occluded person Re-ID methods
    in local feature learning and sequence feature learning. Considering the practical
    importance of occlusion for person Re-ID, a systematic investigation for occluded
    person Re-ID is essential. Therefore, we provide an in-depth survey of issues
    and solutions involved in occlusion-related person Re-ID works to help boost future
    research.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: The Overview of Person Re-ID Surveys in Recent Years.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Surveys | Reference | Taxonomy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Traditional | 2012 PRL [[18](#bib.bib18)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Feature Extraction / Cross-camera Calibration / &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Person Association &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 IVC [[19](#bib.bib19)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Contextual Methods (camera geometry info; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; camera calibration.) / Non-contextual Methods &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (passive methods; active methods) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Deep learning-based | 2016 arXiv [[17](#bib.bib17)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image-based Methods / Video-based Methods &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 TCSVT [[22](#bib.bib22)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Person Verification / Application-driven Methods &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (raw data; practial procedure; efficiency) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 TPAMI [[24](#bib.bib24)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Feature Extraction / Metric Learning / Multi-shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ranking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 arXiv [[23](#bib.bib23)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Identification Task / Verification Task &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 IVC [[25](#bib.bib25)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Feature Learning / Model Architecture Design / &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Metric Learning / Loss Function &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 arXiv [[26](#bib.bib26)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Pseudo-label Estimation / Deep Feature &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Representation Learning / Camera-aware &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Invariance Learning / Unsupervised Domain &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Adaptation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 IJCAI [[29](#bib.bib29)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Low Resolution / Infrared / Sketch / Text &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 IJCAI [[27](#bib.bib27)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Deep Feature Representation Learning / Deep &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Metric Learning / Identity-driven detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 TPAMI [[1](#bib.bib1)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Closed-world Setting (deep feature representation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; learning; deep metric learning; ranking optimization) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; / Open-world Setting (heterogeneous data; raw &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; images or videos; unavailable or limited labels; &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; open-set; noisy annotation) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2022 IVC [[28](#bib.bib28)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Deep Metric Learning / Local Feature Learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; / Generative Adversarial Networks / Sequence &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Feature Learning / Graph Convolutional Networks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: III Datasets and Evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We review eight widely-used datasets for occluded person Re-ID, including 3
    image-based partial¹¹1The partial person Re-ID assumes that the visible region
    of occluded person image is manually cropped for identification. Re-ID datasets,
    4 image-based occluded²²2The occluded person Re-ID does not require the manually
    cropping process of occluded images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unless otherwise specified, occluded person Re-ID in this survey includes both
    partial person Re-ID and occluded person Re-ID. Re-ID datasets, and 1 video-based
    occluded person Re-ID dataset. The statistics of these datasets are summarized
    in Table [II](#S3.T2 "TABLE II ‣ III-A Datasets ‣ III Datasets and Evaluations
    ‣ Deep Learning-based Occluded Person Re-identification: A Survey") and the details
    of each dataset are reviewed as follows. Examples of partial/occluded person Re-ID
    datasets are shown in Fig. [3](#S3.F3 "Figure 3 ‣ III-A Datasets ‣ III Datasets
    and Evaluations ‣ Deep Learning-based Occluded Person Re-identification: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Partial-REID [[15](#bib.bib15)] is an image-based partial Re-ID dataset with
    a variety of viewpoints, backgrounds, and occlusion types. It contains 600 images
    of 60 people, with 5 full-body images and 5 partial images per person. The partial
    observation is generated by manually cropping the occluded region in occluded
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Partial-iLIDS [[30](#bib.bib30)] is an image-based simulated partial Re-ID dataset
    derived from iLIDS [[31](#bib.bib31)]. It is captured by multiple non-overlapping
    cameras in the airport and contains 238 images from 119 people, with 1 full-body
    image and 1 manually cropped non-occluded partial image per person.
  prefs: []
  type: TYPE_NORMAL
- en: p-CUHK03 [[32](#bib.bib32)] is an image-based partial Re-ID dataset constructed
    from CUHK03 [[33](#bib.bib33)]. It contains 1360 person identities captured in
    campus environment. In general, 1160 person identities are used as training set,
    100 person identities are used as validation set, and 100 person identities are
    used as test set. It selects 5 images with same view point from the raw dataset
    for each identity and generates 10 partial body probe images out of selected two
    images. The rest 3 images of each identity are used as full body gallery image.
  prefs: []
  type: TYPE_NORMAL
- en: P-ETHZ [[16](#bib.bib16)] is an image-based occluded person Re-ID dataset modified
    from ETHZ [[34](#bib.bib34)]. It has 3897 images of 85 person identities. Each
    identity has 1 to 30 full-body person images and occluded person images respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Occluded-REID [[16](#bib.bib16)] is an image-based occluded person Re-ID dataset
    captured by mobile cameras with different viewpoints and different types of severe
    occlusion. It consists of 2000 images of 200 people, with 5 full-body images and
    5 occluded images per person.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/046ca4ecfd337a309bd5427adffdc953.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Examples of partial/occluded person Re-ID datasets. Partial/occluded
    person images (the first row) and full-body person images (the second row).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Occluded/Partial Person Re-id Datasets. Occluded-dukemtmc and Occluded-dukemtmc-videoreid
    are Abbreviated as Occ-dukemtmc and Occ-dukemtmc-video Respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Training Set (ID/Image) | Test Set (ID/Image) |'
  prefs: []
  type: TYPE_TB
- en: '| Gallery | Query |'
  prefs: []
  type: TYPE_TB
- en: '| Partial-REID | - | 60/300 | 60/300 |'
  prefs: []
  type: TYPE_TB
- en: '| Partial-iLIDS | - | 119/119 | 119/119 |'
  prefs: []
  type: TYPE_TB
- en: '| p-CUHK03 | 1160/15080 | 100/300 | 100/1000 |'
  prefs: []
  type: TYPE_TB
- en: '| P-ETHZ | 43/ - | 42/ - | 42/ - |'
  prefs: []
  type: TYPE_TB
- en: '| Occluded-REID | - | 200/1000 | 200/1000 |'
  prefs: []
  type: TYPE_TB
- en: '| P-DukeMTMC-reID | 650/ - | 649/ - | 649/ - |'
  prefs: []
  type: TYPE_TB
- en: '| Occ-DukeMTMC | 702/15,618 | 1,110/17,661 | 519/2,210 |'
  prefs: []
  type: TYPE_TB
- en: '| Occ-DukeMTMC-Video | 702/292,343 | 1,110/281,114 | 661/39,526 |'
  prefs: []
  type: TYPE_TB
- en: P-DukeMTMC-reID [[16](#bib.bib16)] is an image-based occluded person Re-ID dataset
    modified from DukeMTMC-reID [[35](#bib.bib35)]. It has 24143 images of 1299 person
    identities and contains images with target persons occluded by different types
    of occlusion in public, e.g., people, cars, and guideboards. Each identity has
    both full-body images and occluded images.
  prefs: []
  type: TYPE_NORMAL
- en: Occluded-DukeMTMC [[36](#bib.bib36)] is an image-based occluded person Re-ID
    dataset built from DukeMTMC-reID [[35](#bib.bib35)]. It contains 15,618 images
    of 708 people for training while including 2,210 query images of 519 persons and
    17,661 gallery images of 1110 persons for testing. The 9% of the training set,
    the 100% of the query set, and the 10% of the gallery set are occluded images.
  prefs: []
  type: TYPE_NORMAL
- en: Occluded-DukeMTMC-VideoReID [[14](#bib.bib14)] is a video-based occluded Re-ID
    dataset reorganized from the DukeMTMC-VideoReID [[37](#bib.bib37)]. It includes
    large variety of obstacles, e.g., cars, trees, bicycles, and other persons. It
    contains 1,702 image sequences of 702 identities for training, 661 query image
    sequences of 661 identities and 2,636 gallery image sequences of 1110 identities
    for testing. More than 1/3 frames of each query sequence in the testing set contain
    occlusion.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The occluded person Re-ID evaluates the performance of a Re-ID system under
    the scenario of occlusion. Therefore, the settings of partial/occluded person
    Re-ID datasets are usually specially designed. In principle, the query images/videos
    for testing are all occluded samples. The evaluation focuses on whether the correct
    identities can be retrieved when only occluded queries are provided. To evaluate
    a Re-ID system, the Cumulative Matching Characteristics (CMC) curves and the mean
    Average Precision (mAP) are two widely used metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CMC curves calculate the probability that a correct match appears in the
    top-$k$ ranked retrieval results, $k\in\left\{1,2,3,...\right\}$. Specifically,
    the top-$k$ accuracy of the query $i$ is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S3.E1.m1.5" class="ltx_Math" alttext="{Acc_{k}^{i}}=\begin{cases}1,&amp;{\text{if
    the top-$k$ ranked gallery samples}}\\ &amp;{\text{contain the sample(s) of query
    $i$}};\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0,&amp;{\text{otherwise}}.\end{cases}" display="block"><semantics id="S3.E1.m1.5a"><mrow
    id="S3.E1.m1.5.6" xref="S3.E1.m1.5.6.cmml"><mrow id="S3.E1.m1.5.6.2" xref="S3.E1.m1.5.6.2.cmml"><mi
    mathsize="90%" id="S3.E1.m1.5.6.2.2" xref="S3.E1.m1.5.6.2.2.cmml">A</mi><mo lspace="0em"
    rspace="0em" id="S3.E1.m1.5.6.2.1" xref="S3.E1.m1.5.6.2.1.cmml">​</mo><mi mathsize="90%"
    id="S3.E1.m1.5.6.2.3" xref="S3.E1.m1.5.6.2.3.cmml">c</mi><mo lspace="0em" rspace="0em"
    id="S3.E1.m1.5.6.2.1a" xref="S3.E1.m1.5.6.2.1.cmml">​</mo><msubsup id="S3.E1.m1.5.6.2.4"
    xref="S3.E1.m1.5.6.2.4.cmml"><mi mathsize="90%" id="S3.E1.m1.5.6.2.4.2.2" xref="S3.E1.m1.5.6.2.4.2.2.cmml">c</mi><mi
    mathsize="90%" id="S3.E1.m1.5.6.2.4.2.3" xref="S3.E1.m1.5.6.2.4.2.3.cmml">k</mi><mi
    mathsize="90%" id="S3.E1.m1.5.6.2.4.3" xref="S3.E1.m1.5.6.2.4.3.cmml">i</mi></msubsup></mrow><mo
    mathsize="90%" id="S3.E1.m1.5.6.1" xref="S3.E1.m1.5.6.1.cmml">=</mo><mrow id="S3.E1.m1.5.5"
    xref="S3.E1.m1.5.6.3.1.cmml"><mo id="S3.E1.m1.5.5.6" xref="S3.E1.m1.5.6.3.1.1.cmml">{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E1.m1.5.5.5" xref="S3.E1.m1.5.6.3.1.cmml"><mtr
    id="S3.E1.m1.5.5.5a" xref="S3.E1.m1.5.6.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E1.m1.5.5.5b" xref="S3.E1.m1.5.6.3.1.cmml"><mrow id="S3.E1.m1.3.3.3.3.2.1.3"
    xref="S3.E1.m1.5.6.3.1.cmml"><mn mathsize="90%" id="S3.E1.m1.3.3.3.3.2.1.1" xref="S3.E1.m1.3.3.3.3.2.1.1.cmml">1</mn><mo
    mathsize="90%" id="S3.E1.m1.3.3.3.3.2.1.3.1" xref="S3.E1.m1.5.6.3.1.cmml">,</mo></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E1.m1.5.5.5c" xref="S3.E1.m1.5.6.3.1.cmml"><mrow
    id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1c.cmml"><mtext mathsize="90%"
    id="S3.E1.m1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1c.cmml">if the top-</mtext><mi
    mathsize="90%" id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.cmml">k</mi><mtext
    mathsize="90%" id="S3.E1.m1.1.1.1.1.1.1b" xref="S3.E1.m1.1.1.1.1.1.1c.cmml"> ranked
    gallery samples</mtext></mrow></mtd></mtr><mtr id="S3.E1.m1.5.5.5d" xref="S3.E1.m1.5.6.3.1.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S3.E1.m1.5.5.5f" xref="S3.E1.m1.5.6.3.1.cmml"><mrow
    id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.1b.cmml"><mrow id="S3.E1.m1.2.2.2.2.1.1.1.1"
    xref="S3.E1.m1.2.2.2.2.1.1.1.1b.cmml"><mtext mathsize="90%" id="S3.E1.m1.2.2.2.2.1.1.1.1a"
    xref="S3.E1.m1.2.2.2.2.1.1.1.1a.cmml">contain the sample(s) of query </mtext><mi
    mathsize="90%" id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.cmml">i</mi></mrow><mo
    mathsize="90%" id="S3.E1.m1.2.2.2.2.1.1.3.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1b.cmml">;</mo></mrow></mtd></mtr><mtr
    id="S3.E1.m1.5.5.5g" xref="S3.E1.m1.5.6.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S3.E1.m1.5.5.5h" xref="S3.E1.m1.5.6.3.1.cmml"><mrow id="S3.E1.m1.4.4.4.4.1.1.3"
    xref="S3.E1.m1.5.6.3.1.cmml"><mn mathsize="90%" id="S3.E1.m1.4.4.4.4.1.1.1" xref="S3.E1.m1.4.4.4.4.1.1.1.cmml">0</mn><mo
    mathsize="90%" id="S3.E1.m1.4.4.4.4.1.1.3.1" xref="S3.E1.m1.5.6.3.1.cmml">,</mo></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S3.E1.m1.5.5.5i" xref="S3.E1.m1.5.6.3.1.cmml"><mrow
    id="S3.E1.m1.5.5.5.5.2.1.3" xref="S3.E1.m1.5.5.5.5.2.1.1a.cmml"><mtext mathsize="90%"
    id="S3.E1.m1.5.5.5.5.2.1.1" xref="S3.E1.m1.5.5.5.5.2.1.1.cmml">otherwise</mtext><mo
    lspace="0em" mathsize="90%" id="S3.E1.m1.5.5.5.5.2.1.3.1" xref="S3.E1.m1.5.5.5.5.2.1.1a.cmml">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.6.cmml" xref="S3.E1.m1.5.6"><apply
    id="S3.E1.m1.5.6.2.cmml" xref="S3.E1.m1.5.6.2"><ci id="S3.E1.m1.5.6.2.2.cmml"
    xref="S3.E1.m1.5.6.2.2">𝐴</ci><ci id="S3.E1.m1.5.6.2.3.cmml" xref="S3.E1.m1.5.6.2.3">𝑐</ci><apply
    id="S3.E1.m1.5.6.2.4.cmml" xref="S3.E1.m1.5.6.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.5.6.2.4.1.cmml"
    xref="S3.E1.m1.5.6.2.4">superscript</csymbol><apply id="S3.E1.m1.5.6.2.4.2.cmml"
    xref="S3.E1.m1.5.6.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.5.6.2.4.2.1.cmml"
    xref="S3.E1.m1.5.6.2.4">subscript</csymbol><ci id="S3.E1.m1.5.6.2.4.2.2.cmml"
    xref="S3.E1.m1.5.6.2.4.2.2">𝑐</ci><ci id="S3.E1.m1.5.6.2.4.2.3.cmml" xref="S3.E1.m1.5.6.2.4.2.3">𝑘</ci></apply><ci
    id="S3.E1.m1.5.6.2.4.3.cmml" xref="S3.E1.m1.5.6.2.4.3">𝑖</ci></apply></apply><apply
    id="S3.E1.m1.5.6.3.1.cmml" xref="S3.E1.m1.5.5"><csymbol cd="latexml" id="S3.E1.m1.5.6.3.1.1.cmml"
    xref="S3.E1.m1.5.5.6">cases</csymbol><cn type="integer" id="S3.E1.m1.3.3.3.3.2.1.1.cmml"
    xref="S3.E1.m1.3.3.3.3.2.1.1">1</cn><ci id="S3.E1.m1.1.1.1.1.1.1c.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><mrow
    id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><mtext mathsize="90%"
    id="S3.E1.m1.1.1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1.1.1.1">if the top-</mtext><mi
    mathsize="90%" id="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.m1.1.1">k</mi><mtext
    mathsize="90%" id="S3.E1.m1.1.1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1.1.1.1"> ranked
    gallery samples</mtext></mrow></ci><ci id="S3.E1.m1.5.6.3.1.4a.cmml" xref="S3.E1.m1.5.5"><mtext
    class="ltx_mathvariant_italic" mathsize="90%" id="S3.E1.m1.5.6.3.1.4.cmml" xref="S3.E1.m1.5.5.6">otherwise</mtext></ci><ci
    id="S3.E1.m1.2.2.2.2.1.1.1.1b.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3"><mrow id="S3.E1.m1.2.2.2.2.1.1.1.1.cmml"
    xref="S3.E1.m1.2.2.2.2.1.1.3"><mtext mathsize="90%" id="S3.E1.m1.2.2.2.2.1.1.1.1a.cmml"
    xref="S3.E1.m1.2.2.2.2.1.1.1.1a">contain the sample(s) of query </mtext><mi mathsize="90%"
    id="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.m1.1.1">i</mi></mrow></ci><cn
    type="integer" id="S3.E1.m1.4.4.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1">0</cn><ci
    id="S3.E1.m1.5.5.5.5.2.1.1a.cmml" xref="S3.E1.m1.5.5.5.5.2.1.3"><mtext mathsize="90%"
    id="S3.E1.m1.5.5.5.5.2.1.1.cmml" xref="S3.E1.m1.5.5.5.5.2.1.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E1.m1.5c">{Acc_{k}^{i}}=\begin{cases}1,&{\text{if
    the top-$k$ ranked gallery samples}}\\ &{\text{contain the sample(s) of query
    $i$}};\\ 0,&{\text{otherwise}}.\end{cases}</annotation></semantics></math> |  |
    (1) |
  prefs: []
  type: TYPE_NORMAL
- en: 'Supposing there are $N$ queries in the test set, the CMC-$k$ (a.k.a., the rank-$k$
    accuracy) that calculates the probability of the top-$k$ accuracy for all queries
    is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{CMC-}k=\frac{1}{N}\sum_{i=1}^{N}Acc_{k}^{i}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Since only the first match is concerned in the calculation, the CMC curves are
    acceptable when there are only one ground truth for each query or when we care
    more about the ground truth match in the top positions of the rank list.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mAP measures the average retrieval performance that takes the order of
    all true matches in the ranked retrieval results into consideration. Specifically,
    the average precision (AP) of the query $i$ is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{AP}_{i}=\frac{1}{M_{i}}\sum_{j=1}^{M_{i}}\frac{j}{Rank_{j}}$ |  |
    (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $M_{i}$ denotes there are $M_{i}$ samples with identity $i$ in the gallery
    set and $Rank_{j}$ denotes the rank of the $j$-th ground truth in the retrieval
    gallery list for query $i$. Supposing there are $N$ queries in the test set, the
    mAP is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{mAP}=\frac{1}{N}\sum_{i=1}^{N}\text{AP}_{i}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Since the order of all true matches in the ranked retrieval results participates
    in the calculation of mAP, the mAP measures the average retrieval performance
    and is suitable for the gallery with multiple true matches.
  prefs: []
  type: TYPE_NORMAL
- en: On the whole, the mAP pays more attention to the ability of retrieval recall
    while the CMC curves focus on the ability of retrieving a true match in candidate
    lists with different sizes. Consequently, the CMC curves and the mAP always work
    together for the evaluation of a Re-ID system.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/acd9cf0d13fa63a3884f4d4344c8ee04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Examples of the four issues caused by occlusion in real-world applications:
    (*a*) position misalignment, (*b*) scale misalignment, (*c*) noisy information,
    and (*d*) missing information.'
  prefs: []
  type: TYPE_NORMAL
- en: IV Occluded Person Re-ID
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Occluded person re-identification (Re-ID) aims at addressing the occlusion
    problem when retrieving the person of interest across multiple cameras. In real-world
    applications of person Re-ID, a person may be occluded by a variety of obstacles
    (e.g., cars, trees, streetlights, and other persons) and the surveillance system
    often fails to capture the holistic person. A practical person Re-ID system in
    video surveillance generally includes three stages: pedestrian detection, trajectory
    tracking, and person retrieval. The occlusion will affect the whole process and
    bring great challenges to the final Re-ID.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, there are four issues to be considered when developing a solution
    for occluded person Re-ID. The first two issues are the position and the scale
    misalignments between partial and holistic images. This is mainly caused by the
    upstream data processing: the detected box of a partial person undergoes the same
    alignment processing as that of a holistic person to obtain a consistent input
    size, resulting in the position misalignment issue and the scale misalignment
    issue. The last two issues are the noisy information and the missing information
    caused by occlusion. In the detected boxes of occluded pedestrians, occlusion
    is inevitably included in whole or in part and the identity information of occluded
    regions is missing, resulting in the noisy information issue and the missing information
    issue. Each issue is shown in Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Deep
    Learning-based Occluded Person Re-identification: A Survey") and real-world examples
    of the four issues are presented in Fig. [4](#S3.F4 "Figure 4 ‣ III-B Evaluation
    Metrics ‣ III Datasets and Evaluations ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we analyze occlusion-related person Re-ID methods from the
    perspective of above-mentioned four issues. Corresponding solutions for each issue
    are summarized following the taxonomy illustrated in Fig. [1](#S1.F1 "Figure 1
    ‣ I Introduction ‣ Deep Learning-based Occluded Person Re-identification: A Survey").
    It should be noticed that some methods simultaneously address more than one issue
    and these methods will be introduced multiple times from different perspectives
    accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Position Misalignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep learning-based solutions to address the position misalignment issue caused
    by occlusion in person Re-ID can be roughly summarized into four categories: matching [[15](#bib.bib15),
    [38](#bib.bib38), [30](#bib.bib30), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)], auxiliary model for position [[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [42](#bib.bib42), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55)], additional supervision for position [[56](#bib.bib56),
    [57](#bib.bib57), [38](#bib.bib38), [58](#bib.bib58), [14](#bib.bib14), [59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61)], and attention mechanism for position [[32](#bib.bib32),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68)]. Firstly, the matching-based solutions take
    person Re-ID as a matching task and propose a variety of matching components,
    as well as the matching strategies, to address the position misalignment issue.
    Secondly, the auxiliary model-based solutions for person Re-ID rely on the position
    information provided by external models to boost performance. Thirdly, the additional
    supervision-based solutions for person Re-ID utilize extra information to guide
    the position-related learning process while being independent during the inference
    stage. Fourthly, the attention mechanism-based solutions for person Re-ID learn
    attention to address the position misalignment issue without any additional information.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A1 Matching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The main points of a matching-based method can probably be summarized as the
    matching component and the matching strategy (see Fig. [5](#S4.F5 "Figure 5 ‣
    IV-A1 Matching ‣ IV-A Position Misalignment ‣ IV Occluded Person Re-ID ‣ Deep
    Learning-based Occluded Person Re-identification: A Survey")). Diverse definitions
    of the matching component, as well as the corresponding matching strategies, have
    been proposed for addressing the position misalignment issue. On the whole, matching-based
    methods can be further grouped into sliding window matching [[15](#bib.bib15)],
    shortest path matching [[38](#bib.bib38)], reconstruction-based matching [[15](#bib.bib15),
    [30](#bib.bib30)], denoising matching [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)],
    graph-based matching [[42](#bib.bib42), [43](#bib.bib43)], and set-based matching [[44](#bib.bib44)].'
  prefs: []
  type: TYPE_NORMAL
- en: The sliding window matching [[15](#bib.bib15)] treats the partial probe image
    as a whole and slides it exhaustively over a full-body gallery image to match
    the most similar local region. The L1-norm distance between the partial image
    and its most similar match on a full-body image is employed for the measurement.
  prefs: []
  type: TYPE_NORMAL
- en: The shortest path matching [[38](#bib.bib38)] performs the matching by calculating
    the shortest path between two sets of local features and uses the matched local
    features to compute the similarity, explicitly accomplishing the position alignment.
  prefs: []
  type: TYPE_NORMAL
- en: The reconstruction-based matching [[15](#bib.bib15), [30](#bib.bib30)] assumes
    the identity information in an occluded image is a subset of that in a non-occluded
    image and thus the partial image can be reconstructed in whole or in part from
    a holistic image of the same identity. The patch-level reconstruction-based matching [[15](#bib.bib15)]
    decomposes the partial and the full-body images into regular grid patches as matching
    components. Each probe patch is matched to a set of gallery patches by optimizing
    the gallery patch selection for self reconstruction. The block-level reconstruction-based
    matching [[30](#bib.bib30)] defines $c\times c$ pixels on a feature map as an
    independent block for matching. It is assumed that each block of a partial probe
    image can be reconstructed from the sparse linear combination of blocks of a full-body
    gallery image with the same identity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f0b555976102256e79671db004ef1f94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The diagram of matching-based methods: constructing local matching
    elements and designing matching strategies to address the position misalignment,
    scale misalignment, and noise information issues.'
  prefs: []
  type: TYPE_NORMAL
- en: The denoising matching [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]
    proposes to focus on foreground visible human parts while discarding the noisy
    information during matching. Specifically, GASM [[39](#bib.bib39)] learns a saliency
    heatmap with the supervision of pose estimation and human segmentation to highlight
    foreground visible human parts. Guided by the saliency heatmap, the matching scores
    of spatial element-wise features in foreground visible human parts are assigned
    with large weights while that in background or occlusion regions are assigned
    with small weights, adaptively. Co-Attention [[40](#bib.bib40)] performs the matching
    between a partial and a holistic image under the guidance of body parsing masks.
    Particularly, the self-attention mechanism [[69](#bib.bib69)] is adopted in Co-Attention
    matching where the parsing mask of the partial image is viewed as the query, parsing
    masks and CNN features of the partial and the holistic images serve as the key
    and the value respectively. ASAN [[41](#bib.bib41)] proposes to replace the segmentation
    mask of holistic gallery images with the mask of the current target person image
    in each retrieval matching process. In this way, the feature extraction guided
    by ASAN is able to suppress the interference from useless parts.
  prefs: []
  type: TYPE_NORMAL
- en: The graph-based matching [[42](#bib.bib42), [43](#bib.bib43)] formulates the
    occluded person re-identification as the graph matching problem. HOReID [[42](#bib.bib42)]
    employs the key-point heatmaps to extract the semantic local features of an image
    as nodes of a graph. The graph convolutional network (GCN) with learnable adjacent
    matrices is designed to pass messages between nodes for capturing the high-order
    relation information. To measure the similarity between two graphs, node features
    of the two graphs are further processed with relevant information extracted from
    each other for learning topology information. Both the high-order relation information
    extracted by GCN and the topology information learned from each other are employed
    to compute the final similarity for the two graphs. The multi-granular hypergraph
    matching [[43](#bib.bib43)] designs multiple hypergraphs with different spatial
    and temporal granularities to address the misalignment and occlusion issues for
    video-based person re-identification. Different from the conventional graphs,
    the hypergraphs [[70](#bib.bib70)] are able to model the high-order dependency
    involving multiple nodes and are more suitable for modeling the multi-granular
    correlations in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: The set-based matching takes occluded person Re-ID as a set matching task without
    requiring explicit spatial alignment. Specifically, MoS [[44](#bib.bib44)] employs
    a CNN backbone to capture diverse visual patterns along the channel dimension
    as matching elements. And the Jaccard similarity coefficient is introduced as
    the metric to compute the similarity between pattern sets of person images. The
    minimization and maximization are used to approximate the operations of intersection
    and union of sets, and the ratio of intersection over union is calculated to measure
    the similarity between two sets.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A2 Auxiliary Model for Position
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To address the position misalignment issue caused by occlusion, some methods
    directly use the auxiliary information provided by external models for position
    alignment. According to the type of employed auxiliary models, these methods can
    be roughly summarized into pose-based [[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [42](#bib.bib42), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52)], parsing-based [[53](#bib.bib53)], and hybrid-based [[54](#bib.bib54),
    [55](#bib.bib55)] solutions.
  prefs: []
  type: TYPE_NORMAL
- en: The pose-based methods [[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [42](#bib.bib42), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52)] utilize the position information predicted
    by an external pose estimation model to address the position misalignment issue.
    PGFA [[36](#bib.bib36)], PDVM [[45](#bib.bib45)], and PMFB [[46](#bib.bib46)]
    generate heatmaps consisting of a 2D Gaussian centered on key-point locations
    to extract aligned pose features through a dot product with the CNN feature map.
    KBFM [[47](#bib.bib47)] utilizes shared visible keypoints between images to locate
    aligned rectangular regions for calculating the similarity. PVPM [[48](#bib.bib48)]
    uses the key-point heatmaps and the part affinity fields estimated by OpenPose [[71](#bib.bib71)]
    to generate part attention maps for extracting aligned local features. Similarly,
    PGMANet [[49](#bib.bib49)] generates heatmaps of key-points locations and employs
    them to calculate part attention masks on the CNN feature map. Based on the part
    features aggregated by part attention masks, PGMANet further computes the correlation
    among different part features to exploit the second-order information to enrich
    the feature extraction. HOReID [[42](#bib.bib42)] employs the key-point heatmaps
    to extract the semantic local features on a person image. The local features of
    an image are taken as nodes of a graph and HOReID designs a graph convolutional
    network with learnable adjacent matrices to exploit the more discriminative relation
    information among nodes for re-identification. Further, CTL [[50](#bib.bib50)]
    proposes to divide the human body into three granularities and uses key-point
    heatmaps to extract multi-scale part features as graph nodes. The cross-scale
    graph convolution and the 3D graph convolution are designed to capture the structural
    information and the hierarchical spatial-temporal dependencies for addressing
    the spatial misalignment issues in video person Re-ID. ACSAP [[51](#bib.bib51)]
    proposes to use the external pose information to guide the adversarial generation
    of aligned feature maps. PFD [[52](#bib.bib52)] generates local patch features
    and multiplies them with the processed keypoint heatmaps element-wisely to obtain
    the pose-guided features. Throught the measurement of set similarity, PFD performs
    the matching between local features and pose-guided features to disentangle the
    pose information from patch features for position alignment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/02cfafecf5140d83d7e7a66d47032acd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: (*a*) The diagram of pose-based methods: keypoint coordinates for
    addressing the position misalignment and confidence scores for excluding the noisy
    information. (*b*) The diagram of segmentation-based methods: part masks for position
    locating and noisy information excluding.'
  prefs: []
  type: TYPE_NORMAL
- en: The parsing-based methods directly take advantage of the parsing information
    generated from an external human parsing model to address the position misalignment
    issue. SPReID [[53](#bib.bib53)] trains an extra semantic parsing model on the
    human parsing dataset LIP [[72](#bib.bib72)] to predict probability maps associated
    to 5 pre-defined semantic regions of human body. These probability maps are then
    used to extract different semantic features through the weighted sum operation
    on the feature map generated by a modified Inception-V3 [[73](#bib.bib73)].
  prefs: []
  type: TYPE_NORMAL
- en: The hybrid-based methods [[54](#bib.bib54), [55](#bib.bib55)] employ two or
    more external models to provide auxiliary information for addressing the position
    misalignment issue. Specifically, SSP-ReID [[54](#bib.bib54)] exploits the capabilities
    of both clues, i.e., the saliency and the semantic parsing, to guide the CNN backbone
    to learn complementary representations for re-identification. The external off-the-shelf
    deep models [[72](#bib.bib72), [74](#bib.bib74)] are employed to generate the
    semantic parsing masks and the saliency masks. The element-wise product is applied
    between masks and a CNN feature map to obtain parsing features and saliency features
    for fusion. TSA [[55](#bib.bib55)] employs off-the-shelf HRNet [[75](#bib.bib75)]
    and DensePose [[76](#bib.bib76)] to provide extra key-points information and body
    parts information. Based on the key-points locations, the TSA divides the whole
    person into 5 regions and obtains region features on the CNN feature map through
    the soft region pooling. Based on the body part masks, the TSA extracts corresponding
    region features on the texture image produced by a texture generator. The region
    features guided by the key-points are then concatenated with the region features
    guided by the part masks accordingly to learn robust representations for re-identification.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A3 Additional Supervision for Position
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different from the auxiliary model-based solutions that rely on extra information
    in both training and test phases, the additional supervision-based solutions for
    position misalignment only use the extra information to guide the learning process
    and are independent during inference. According to the type of external information
    used, the additional supervision-based solutions for position misalignment can
    be coarsely summarized into pose-based [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38),
    [58](#bib.bib58), [14](#bib.bib14)], segmentation-based [[59](#bib.bib59), [60](#bib.bib60)],
    and hybrid-based [[61](#bib.bib61)] methods.
  prefs: []
  type: TYPE_NORMAL
- en: The pose-based methods [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38),
    [58](#bib.bib58), [14](#bib.bib14)] employ external pose information to guide
    the learning process for alleviating the position misalignment problem. Specifically,
    AACN [[56](#bib.bib56)] and DAReID [[38](#bib.bib38)] learn part attention maps
    to locate and extract part features with the ground truth built from external
    pose information. DSAG [[57](#bib.bib57)] and PGFL-KD [[58](#bib.bib58)] use features
    located by pose information to guide the feature learning process. DSAG constructs
    a set of densely semantically aligned part images with the external pose information
    provided by DensePose [[76](#bib.bib76)]. Taking the semantically aligned part
    images as the input, DSAG serves as a regulator to guide the feature learning
    on original images through the carefully designed fusion and loss. Similarly,
    PGFL-KD uses external keypoint heatmaps to extract semantically aligned features.
    The aligned features are then employed to regularize the global feature learning
    through knowledge distillation and interaction-based training. Differently, the
    RFCNet [[14](#bib.bib14)] trains an adaptive partition unit supervised by external
    pose information to split the CNN feature map into different regions and extract
    region features for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: The segmentation-based methods [[59](#bib.bib59), [60](#bib.bib60)] utilize
    extra segmentation masks provided by the segmentation model, e.g., human parsing
    model or scene segmentation model, to guide the learning process for addressing
    the position misalignment problem. Specifically, MMGA [[59](#bib.bib59)] learns
    to generate the whole-body, upper-body, and lower-body attention maps with the
    parsing labels estimated by JPPNet [[77](#bib.bib77)]. The learned attention maps
    are then employed to extract global and local features for re-identification.
    HPNet [[60](#bib.bib60)] introduces human parsing as an auxiliary task and employs
    the parsing masks to extract part-level features for addressing the position misalignment
    issue. The person Re-ID and the human parsing are learned in a multi-task manner
    where the pseudo parsing label are predicted by the scene segmentation model [[78](#bib.bib78)]
    trained on the COCO DensePose [[76](#bib.bib76)] dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The hybrid-based methods utilize more than one type of external information
    to guide the learning process. Specifically, FGSA [[61](#bib.bib61)] mines fine-grained
    local features with the supervision of both the pose information and the attribute
    information to address the position misalignment issue. FGSA designs a pose resolve
    net (pre-trained on MSCOCO [[79](#bib.bib79)]) to provide part confidence maps
    and part affinity fields of the key parts. These part maps are then used to extract
    part features on a CNN feature map through compact bilinear pooling. Given the
    extracted part features, FGSA treats the attribute recognition as multiple classification
    tasks and trains an intermediate model for attribute classification along with
    the person Re-ID. In this way, the attribute classification tasks guide the pose
    resolve net and the CNN backbone to learn more discriminated local information
    for re-identification.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A4 Attention Mechanism for Position
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The attention mechanism-based methods learn attention to address the position
    misalignment issue without any additional information (see Fig. [7](#S4.F7 "Figure
    7 ‣ IV-A4 Attention Mechanism for Position ‣ IV-A Position Misalignment ‣ IV Occluded
    Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification: A Survey")).
    According to the main point of the attention learning process, the attention mechanism-based
    solutions for position misalignment can be further grouped into cropping-based [[32](#bib.bib32)],
    clustering-based [[62](#bib.bib62)], self-supervised [[63](#bib.bib63), [64](#bib.bib64)],
    and constraint-based [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68)]
    methods. It should be noticed that some methods have also involved the attention
    mechanism but rely on external information provided by auxiliary models or use
    additional information for supervision. These methods are summarized in Auxiliary
    Model for position or Additional Supervision for Position accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: The cropping-based methods crop images into different local regions and learn
    attention to find the same local regions across different images for similarity
    calculation. Specifically, DPPR [[32](#bib.bib32)] crops 13 predefined partial
    regions on holistic images and designs an attention module conditioned on the
    partial probe image to assign the partial regions with larger attention weights
    if the same body parts are included.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad9b14b2ae74aeb4dbd62a98bd6626a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Examples of technical routes in attention mechanism-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: The clustering-based methods generate pseudo-labels from clustering to supervise
    the attention learning. Specifically, ISP [[62](#bib.bib62)] designs the cascaded
    clustering on CNN feature maps to gradually generate pixel-level pseudo-labels
    of human parts for part attention learning. Based on the assumption that the foreground
    pixels have higher responses than the background ones, all pixels of a feature
    map are first clustered into foreground or background according to the activation.
    Secondly, the foreground pixels are further clustered into different human parts
    according to the similarities between pixels. The clustered pixel-level pseudo-labels
    of human parts are then employed to guide the part attention learning for extracting
    local part features to address the position misalignment issue.
  prefs: []
  type: TYPE_NORMAL
- en: The self-supervised methods [[63](#bib.bib63), [64](#bib.bib64)] construct self-supervision
    to guide the attention learning. Given a holistic image, VPM [[63](#bib.bib63)]
    defines $m\times n$ rectangle regions and randomly crops a patch in which every
    pixel is assigned with the region label accordingly for self-supervision. VPM
    appends a region locator upon the extracted CNN feature map to discover different
    regions through the pixel-wise classification (attention) with the self-supervision
    constructed above. APN [[64](#bib.bib64)] randomly crops the holistic image into
    different partial types as the self-supervision for attention-based cropping type
    classification of a partial image. The holistic gallery images are cropped for
    person retrieval according to the predicted cropping type of the partial probe
    image to accomplish the position alignment.
  prefs: []
  type: TYPE_NORMAL
- en: The constraint-based methods [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68)] build constraints among different attention maps to help locate
    diverse body parts for addressing the position misalignment issue. The multiple
    spatial attentions in [[65](#bib.bib65)] employ a diversity regularization term
    on attention maps to ensure each attention focuses on different regions of the
    given image. SBPA [[66](#bib.bib66)] separates local attention maps through minimizing
    the L1-norm distance between the local attention and the masked global attention.
    PAT [[67](#bib.bib67)] maintains vectors of part prototypes to generate part-aware
    attention masks on contextual CNN features and designs the part diversity mechanism
    to help achieve diverse part discovery. Specifically, the part diversity mechanism
    minimizes the cosine similarity between every two part features to expand the
    discrepancy among different part features. MHSA-Net [[68](#bib.bib68)] proposes
    the feature diversity regularization term to encourage the diversity of local
    features captured by a multi-head self-attention mechanism. Specifically, in order
    to obtain diverse local features, the regularization term restricts the Gram matrix
    of local features to be close to an identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Scale Misalignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning-based methods [[80](#bib.bib80), [43](#bib.bib43), [30](#bib.bib30),
    [81](#bib.bib81), [66](#bib.bib66)] propose to construct pyramid features or multi-scale
    features for alleviating the scale misalignment issue in person Re-ID. Given a
    CNN feature map, the pyramid features are extracted from global to local while
    the multi-scale features maintain features of different receptive fields at the
    same position. On the whole, the core of both pyramid features and multi-scale
    features is to extract features of different scales to construct robust representations
    to scale variation.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B1 Pyramid Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The pyramid features [[80](#bib.bib80), [43](#bib.bib43)] are hierarchical and
    are extracted from global to local on the feature map. The pyramidal model in [[80](#bib.bib80)]
    horizontally slices the feature map into $n$ basic parts and builds corresponding
    branches for every $l\in\{1,2,...,n\}$ adjacent parts to obtain the pyramid features.
    MGH [[43](#bib.bib43)] hierarchically divides the feature map into $p\in\{1,2,4,8\}$
    horizontal strips and average pools each strip to obtain multi-granular spatial
    features.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B2 Multi-scale Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The multi-scale features [[30](#bib.bib30), [81](#bib.bib81), [66](#bib.bib66)]
    focus on restricted local regions and maintain features of different receptive
    fields at the same position. Specifically, DSR [[30](#bib.bib30)] average pools
    the square area of $s\times s$ pixels on a feature map to obtain the multi-scale
    block representations for alleviating the influence of scale mismatching, $s=\{1,2,3\}$.
    FPR [[81](#bib.bib81)] performs multiple max-pooling layers of different kernel
    sizes upon the feature map to capture diverse spatial features from small local
    regions to relatively large regions. SBPA [[66](#bib.bib66)] maintains features
    in different scales at each pixel through the scale-wise residual connection for
    addressing the scale misalignment issue.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Position and Scale Misalignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the literature, there are some methods [[82](#bib.bib82), [83](#bib.bib83),
    [40](#bib.bib40), [51](#bib.bib51), [84](#bib.bib84), [64](#bib.bib64)] that simultaneously
    address the position and scale misalignment issues through the transformation
    of partial or holistic images (see Fig. [8](#S4.F8 "Figure 8 ‣ IV-C Position and
    Scale Misalignment ‣ IV Occluded Person Re-ID ‣ Deep Learning-based Occluded Person
    Re-identification: A Survey") (*a*)). These methods are specifically summarized
    in this subsection for clarity.'
  prefs: []
  type: TYPE_NORMAL
- en: The partial image transformation [[82](#bib.bib82), [83](#bib.bib83), [40](#bib.bib40),
    [51](#bib.bib51)] aims to transform partial images to obtain the rectified results
    that are spatially aligned with holistic ones for re-identification. Specifically,
    APNet [[82](#bib.bib82)] designs a bounding box aligner (BBA) which predicts 4
    offset values (top, bottom, left, and right) to shift the detected bounding boxes
    to cover the estimated holistic body region. Without manual annotations, APNet
    is trained by constructing automatic data augmentation. PPCL [[83](#bib.bib83)]
    employs a gated transformation regression CNN module to predict the affine transformation
    coefficients between the partial and the holistic images for generating rectified
    partial images with proper scale and layout. The prediction of transformation
    coefficients is self-supervised by randomly cropping holistic images to simulate
    partial images with known transformation coefficients. The Image Rescaler (IR)
    in [[40](#bib.bib40)] predicts the 2D affine transformation parameters to transform
    a partial image into a desirable distortion-free image for addressing the spatial
    misalignment issue. Specifically, the partial image and the desirable distortion-free
    image are obtained by randomly cropping the holistic image and masking the uncropped
    regions, for self-supervision. Differently, ACSAP [[51](#bib.bib51)] designs a
    pose-guided generator that utilizes extra pose information of both partial and
    holistic images to guide the generation of aligned features for partial images.
    The pose-guided generator is adversarially learned by training a pose-guided discriminator,
    which aims to distinguish the authenticity of image features.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cad2655e9ac03bb02c21c0b60eb5c3a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: (*a*) The diagram of image transformation methods: addressing the
    position and scale misalignments simultaneously. (*b*) The diagram of attribute-based
    methods: associating the attribute annotations with person Re-ID.'
  prefs: []
  type: TYPE_NORMAL
- en: The holistic image transformation [[84](#bib.bib84), [64](#bib.bib64)] aims
    to find and transform the corresponding regions of holistic images to obtain the
    images that are spatially aligned with the query partial images for re-identification.
    Specifically, STNReID [[84](#bib.bib84)] utilizes the high-level CNN features
    of the partial and the holistic images to predict the 2D affine transformation
    parameters between them to transform the holistic image for matching with the
    partial image. In STNReID, the holistic images are randomly cropped to partial
    images for building the self-supervised training of 2D affine transformation parameters
    prediction. APN [[64](#bib.bib64)] predicts the cropping type of a partial probe
    image and crops the corresponding regions of holistic gallery images for person
    retrieval. In APN, the prediction of the cropping type is trained with the self-supervision
    constructed by randomly cropping the holistic image into different partial types.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Noisy Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The occlusion is inevitably included in the detected boxes of occluded pedestrians
    and therefore brings noisy information to person Re-ID (see Fig. [4](#S3.F4 "Figure
    4 ‣ III-B Evaluation Metrics ‣ III Datasets and Evaluations ‣ Deep Learning-based
    Occluded Person Re-identification: A Survey") (*c*)). This is especially problematic
    since the similar obstacles across different identities disturb the appearance-based
    similarity calculation, i.e., retrieval results easily contain negative images
    with similar obstacles [[85](#bib.bib85)]. Deep learning-based solutions for addressing
    the noisy information issue can be divided into auxiliary model for noise [[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48),
    [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87), [55](#bib.bib55), [40](#bib.bib40)],
    additional supervision for noise [[56](#bib.bib56), [38](#bib.bib38), [81](#bib.bib81),
    [60](#bib.bib60), [88](#bib.bib88), [39](#bib.bib39)], and attention mechanism
    for noise [[16](#bib.bib16), [63](#bib.bib63), [89](#bib.bib89), [82](#bib.bib82),
    [90](#bib.bib90), [91](#bib.bib91), [13](#bib.bib13), [92](#bib.bib92), [93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97)].'
  prefs: []
  type: TYPE_NORMAL
- en: IV-D1 Auxiliary Model for Noise
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The auxiliary model-based methods rely on external information provided by auxiliary
    models to help identify and suppress the noisy occlusion. According to the type
    of employed auxiliary models, these methods can be further divided into pose-based [[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48),
    [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87)] and parsing-based [[55](#bib.bib55),
    [40](#bib.bib40)].
  prefs: []
  type: TYPE_NORMAL
- en: The pose-based methods [[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46),
    [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48), [86](#bib.bib86), [52](#bib.bib52),
    [87](#bib.bib87)] exploits pose landmarks predicted by external pose estimation
    models to disentangle the useful information from the occlusion noise. Specifically,
    PGFA [[36](#bib.bib36)], PDVM [[45](#bib.bib45)], and PMFB [[46](#bib.bib46)]
    preset a threshold to filter out occluded invisible pose landmarks based on their
    prediction confidence. The visible pose landmarks are used to generate heatmaps
    for extracting non-occluded pose features. These methods also construct different
    local part features on the CNN feature map and employ the confidence of pose estimation
    to determine whether the corresponding parts are occluded or not. Only the shared
    visible parts between probe and gallery images are used in these methods to compute
    the similarity, explicitly avoiding the disturbance from occlusion. Similarly,
    ACSAP [[51](#bib.bib51)] utilizes the confidence of pose estimation to decide
    the visibility of horizontally partitioned parts and assigns the shared visible
    parts with larger weights in similarity metrics. KBFM [[47](#bib.bib47)] builds
    rectangular regions based on the shared visible keypoints between probe and gallery
    images to extract features for measuring the similarity. Moreover, PMFB employs
    the pose embeddings generated from visible landmarks as gates to adaptively recalibrate
    channel-wise feature responses based on the visible body parts. Given part features
    extracted with external pose information, PVPM [[48](#bib.bib48)] utilizes the
    characteristic of part correspondence between images of the same identity to mine
    correspondence scores as pseudo-labels for training a visibility predictor that
    estimates whether a part suffers from the occlusion. Considering that the provided
    external pose information may be sparse or noisy, LKWS [[86](#bib.bib86)] proposes
    to discretize the pose information to obtain robust visibility label of horizontally
    divided body parts. Utilizing the label of visibility, LKWS trains a visibility
    discriminator to help suppress the influence of invisible horizontal parts. Specifically,
    the visibility of each horizontal part is voted by all keypoints within the part
    region based on their prediction confidence scores and the preset threshold. PFD [[52](#bib.bib52)]
    assigns the keypoint heatmap of higher confidence score than the preset threshold
    with $label=1$ and the keypoint heatmap of lower confidence score with $label=0$.
    With the assigned labels, PFD divides the view feature set into high-confidence
    and low-confidence keypoint view feature sets. The high-confidence keypoint view
    features are employed in the inference stage to explicitly match visible body
    parts and automatically separate occlusion features. Moreover, PFD designs a Pose-guided
    push loss that encourages the difference between human body parts (i.e., the high-confidence
    features) and non-human body parts (i.e., the low-confidence features) to help
    focus on human body parts and alleviate the interference of occlusion. Differently,
    STAL [[87](#bib.bib87)] uses external pose landmarks to slice the video into multiple
    spatial-temporal units. To down-weight the possible occluded units, STAL designs
    a joint spatial-temporal attention module to evaluate the quality scores of each
    unit with carefully designed loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: The parsing-based methods [[55](#bib.bib55), [40](#bib.bib40)] employ parsing
    masks estimated by human parsing models to help suppress the noisy occlusion.
    Specifically, TSA [[55](#bib.bib55)] utilizes external parsing results estimated
    by DensePose [[76](#bib.bib76)] to provide the visible signal to guide the learning
    of visible regions, suppressing the invisible occluded regions. Co-Attention [[40](#bib.bib40)]
    employs parsing masks as the query in the self-attention mechanism to perform
    image matching on associated regions for alleviating the noisy information brought
    by occlusion.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D2 Additional Supervision for Noise
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The additional supervision-based methods employ extra information to guide the
    learning of suppressing the occlusion noise while being independent during inference.
    According to the type of the extra information employed, the additional supervision-based
    solutions can be further summarized into pose-based [[56](#bib.bib56), [38](#bib.bib38)],
    segmentation-based [[81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88)], attribute-based [[41](#bib.bib41)],
    and hybrid-based [[39](#bib.bib39)] methods.
  prefs: []
  type: TYPE_NORMAL
- en: The pose-based methods [[56](#bib.bib56), [38](#bib.bib38)] utilize additional
    pose information to supervise the learning of excluding noisy occlusion. Specifically,
    AACN [[56](#bib.bib56)] uses external pose information to supervise the part attention
    learning. Based on the intensities of each part attention map, AACN computes part
    visibility scores to measure the occlusion extent of each body part. Similarly,
    with the ground truth built from external pose information, DAReID [[38](#bib.bib38)]
    learns to predict upper and lower body masks to extract non-occluded part features.
    Moreover, DAReID regards the heatmap of upper or lower features with large high-activation
    areas as reliable. The significance of the reliable regions is enhanced to suppress
    the occlusion noise.
  prefs: []
  type: TYPE_NORMAL
- en: The segmentation-based methods [[81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88)]
    utilize extra segmentation masks provided by a segmentation model, e.g., the human
    parsing model or the scene segmentation model, to help address the noisy information
    issue. Specifically, FPR [[81](#bib.bib81)] employs the person mask obtained by
    CE2P [[98](#bib.bib98)] to supervise the generation of foreground probability
    maps, encouraging the feature extraction to concentrate more on clean human body
    parts to refine the similarity computation with less contamination from occlusion.
    HPNet [[60](#bib.bib60)] uses part labels provided by a COCO-trained human parsing
    model to learn human parsing and person re-identification in a multi-task manner.
    In HPNet, the predicted part probability maps are binarized with a threshold of
    0.5 to extract part-level features and determine the visibility of each part for
    alleviating the occlusion noise. SORN [[88](#bib.bib88)] trains a semantic branch
    with pseudo-labels predicted by external semantic segmentation model to generate
    foreground-background masks for extracting features from non-occluded areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'The attribute-based methods leverage semantic-level attribute annotations of
    person Re-ID datasets to help suppress the noisy information brought by occlusion
    (see Fig. [8](#S4.F8 "Figure 8 ‣ IV-C Position and Scale Misalignment ‣ IV Occluded
    Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification: A Survey")
    (*b*)). ASAN [[41](#bib.bib41)] employs the attribute information to guide the
    learning of occlusion-sensitive segmentation in a weakly supervised manner to
    extract non-occluded human body features.'
  prefs: []
  type: TYPE_NORMAL
- en: The hybrid-based methods employ more than one type of external information to
    guide the learning process for alleviating the noisy information issue. Specifically,
    GASM [[39](#bib.bib39)] trains a mask layer and a pose layer with the ground truth
    predicted by the semantic segmentation model PSPNet [[99](#bib.bib99)] and the
    pose estimation model CenterNet [[100](#bib.bib100)]. GASM then combines the mask
    heatmap predicted by the mask layer and the keypoint heatmaps estimated by the
    pose layer into a saliency map to extract salient features, explicitly excluding
    the noisy information brought by occlusion.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D3 Attention Mechanism for Noise
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The attention mechanism-based methods learn to generate the attention maps that
    assign smaller weights to occluded regions to address the noisy information issue,
    without the requirement of any extra information. According to the main idea of
    the attention learning process, the attention mechanism-based solutions for noise
    can be further grouped into data augmentation [[16](#bib.bib16), [63](#bib.bib63),
    [89](#bib.bib89), [82](#bib.bib82), [90](#bib.bib90), [91](#bib.bib91), [13](#bib.bib13),
    [92](#bib.bib92), [93](#bib.bib93)], query-guided [[94](#bib.bib94), [95](#bib.bib95)],
    drop-based [[96](#bib.bib96)], and relation-based [[97](#bib.bib97)] methods.
  prefs: []
  type: TYPE_NORMAL
- en: The data augmentation methods [[16](#bib.bib16), [63](#bib.bib63), [89](#bib.bib89),
    [82](#bib.bib82), [90](#bib.bib90), [91](#bib.bib91), [13](#bib.bib13), [92](#bib.bib92),
    [93](#bib.bib93)] generate artificial occlusion to train the network to focus
    on clean body parts and exclude the noisy occlusion. Specifically, AFPB [[16](#bib.bib16)]
    constructs an occlusion simulator where a random patch from the background of
    source images is used as an occlusion to cover a part of holistic persons. AFPB
    designs the multi-task losses that force the network to simultaneously identify
    the person and classify whether the sample is from the occluded data distribution.
    VPM [[63](#bib.bib63)] pre-defines $m\times n$ rectangle regions on an image and
    randomly crops partial pedestrian images from the holistic ones where every pixel
    in partial images is assigned with the region label accordingly for self-supervision.
    In VPM, a region locator is designed to generate probability maps that infer the
    location of each region. Through the sum operation over each probability map,
    VPM obtains the region visibility scores to suppress the occluded noisy regions.
    RE [[89](#bib.bib89)] proposes the operation of random erasing, which randomly
    selects a rectangle region in an image and erases its pixels with random values,
    to generate images with various levels of occlusion for training the model to
    extract non-occluded discriminative identity information. APNet [[82](#bib.bib82)]
    trains a part identifier with the self-supervision built from data augmentation
    to identify visible strip parts. The visible part features are then selected for
    similarity computation while the invisible part features on occluded or noisy
    regions are discarded. IGOAS [[90](#bib.bib90)] designs an incremental generative
    occlusion block that randomly generates a uniform occlusion mask from small to
    large on images in a batch, training the model more robust to occlusion through
    gradually learning harder occlusion. With the synthesized occlusion data and their
    corresponding occlusion masks, IGOAS focuses more on foreground information by
    suppressing the response of generated occlusion regions to zero. SSGR [[91](#bib.bib91)]
    employs the random erasing and the batch-constant erasing, which equally divides
    images into horizontal strips and randomly erases the same strip in a sub-batch,
    to simulate occlusion for training the disentangled non-local (DNL [[101](#bib.bib101)])
    attention network. OAMN [[13](#bib.bib13)] designs a novel occlusion augmentation
    scheme that crops a rectangular patch of a randomly chosen training image and
    scales the patch onto four pre-defined locations of the target image, producing
    diverse and precisely labeled occlusion. With the supervision of labeled occlusion
    data, the OAMN learns to generate spatial attention maps which precisely capture
    body parts regardless of the occlusion. DRL-Net [[92](#bib.bib92)] utilizes the
    obstacles appearing in the train set to synthesize more diverse and realistic
    occluded samples to guide the contrast feature learning for mitigating the interference
    of occlusion noises. Apart from this, DRL-Net designs a transformer that simultaneously
    maintains the ID-relevant and ID-irrelevant queries for disentangled representation
    learning. Differently, FED [[93](#bib.bib93)] considers the occlusion from not
    only the non-pedestrian obstacles but also the non-target pedestrians for augmentation.
    To simulate reasonable non-pedestrian occlusions, FED manually crops the patches
    of backgrounds and occlusion objects from training images, and pastes them on
    pedestrian images with carefully designed augmentation process. To synthesize
    non-target pedestrian occlusions, FED maintains a memory bank of the feature centers
    of different identities. The memory bank is then employed to search $K$-nearest
    features of different identities for the current pedestrian. Finally, FED diffuses
    the pedestrian representations with these memorized features and generates non-target
    pedestrian occlusions at the feature level for training.
  prefs: []
  type: TYPE_NORMAL
- en: The query-guided methods [[94](#bib.bib94), [95](#bib.bib95)] aims to generate
    consistent attentions between query and gallery images for addressing the noisy
    information issue. Specifically, CASN [[94](#bib.bib94)] designs an attention-driven
    siamese learning architecture which enforces the attention consistency among images
    of the same identity to deal with viewpoint variations, occlusion, and background
    clutter. PISNet [[95](#bib.bib95)] focuses on the crowded occlusion in which the
    detected bounding boxes may involve multiple people and include distractive information.
    Under the guidance of the query image (single-person), PISNet calculates the inner
    product of the query and gallery features to formulate the pixel-wise query-guided
    attention to enhance the feature of the target in the gallery image (multi-person).
  prefs: []
  type: TYPE_NORMAL
- en: The drop-based methods develop the training strategy based on dropping to guide
    the network to learn a more robust representation, alleviating the noisy information
    brought by occlusion. Specifically, CBDB-Net [[96](#bib.bib96)] uniformly partitions
    the feature map into strips and continuously drop each strip from top to bottom.
    Trained with drop-based incomplete features, the model is forced to learn a more
    robust person descriptor for re-identification.
  prefs: []
  type: TYPE_NORMAL
- en: The relation-based methods mine the relation among different regions to refine
    features, alleviating the interference of occlusion. Specifically, OCNet [[97](#bib.bib97)]
    predefines the global region, top region (i.e., $1/2$ top horizontal strip), bottom
    region (i.e., $1/2$ bottom horizontal strip), and center region (i.e., $1/3$ center
    vertical strip) on an image, and extracts four region features through the group
    convolution and the carefully designed attention mechanism. In OCNet, the relational
    adaptive module consisting of two fully connected shared layers is proposed to
    capture the relation between different region features. The relational weights
    are then used to refine region features to suppress the occluded or insignificant
    information.
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Missing Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The lack of identity information in occluded regions results in the missing
    information issue for person Re-ID (see Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ Deep Learning-based Occluded Person Re-identification: A Survey") (*d*)). There
    are mainly two ways to recover the missing information in occluded regions: spatial
    recovery [[102](#bib.bib102), [14](#bib.bib14)] and temporal recovery [[103](#bib.bib103),
    [102](#bib.bib102), [14](#bib.bib14)].'
  prefs: []
  type: TYPE_NORMAL
- en: IV-E1 Spatial Recovery
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The spatial recovery [[102](#bib.bib102), [14](#bib.bib14)] utilizes the spatial
    structure of a pedestrian image to infer the missing information. Specifically,
    VRSTC [[102](#bib.bib102)] designs an auto-encoder which takes a image masked
    with white pixels as input and generates the contents for the occluded white region.
    To improve the quality of generated contents of the occluded parts, VRSTC adopts
    a local and a global discriminator to adversarially judge the reality and the
    contextual consistency of the synthesized contents. RFCNet [[14](#bib.bib14)]
    exploits the long-range spatial contexts from non-occluded regions to predict
    the features of occluded regions, recovering the missing information at the feature
    level. Specifically, RFCNet estimates four keypoints to divide the feature map
    into 6 regions. In RFCNet, the encoder-decoder architecture is adopted, in which
    the encoder models the correlation between regions through clustering and the
    decoder utilizes the spatial correlation to recover occluded region features.
  prefs: []
  type: TYPE_NORMAL
- en: IV-E2 Temporal Recovery
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The temporal recovery [[103](#bib.bib103), [102](#bib.bib102), [14](#bib.bib14)]
    requires the continuous sequence of images (i.e., video-based person Re-ID) and
    utilizes the temporal information to recover the missing part in occluded regions.
    Assuming that the information at the same position in other frames can help recover
    the lost information in the current frame, the Refining Recurrent Unit (RRU) in [[103](#bib.bib103)]
    is designed to remove noise and recover missing activation regions by implicitly
    referring the appearance and motion information extracted from historical frames.
    VRSTC [[102](#bib.bib102)] proposes a differentiable temporal attention layer
    which employs the cosine similarity to determine where to attend from adjacent
    frames for recovering the contents of the occluded parts. RFCNet [[14](#bib.bib14)]
    employs a query-memory attention mechanism in which the current region is regarded
    as the query and the corresponding regions of remaining frames serve as the memory.
    The dot-product similarity [[104](#bib.bib104)] between the query and each item
    of the memory is employed for re-weighting all items in the memory to obtain the
    long-term temporal contexts to refine the query.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Performance Comparison on Partial-reid and Partial-ilids. S-ST is
    Short for Sing-ShoT, which Denotes that Each Identity Only Contains One Gallery
    Image in the Inference Stage.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Issues | Technical Routes | Methods | Publications | Partial-REID | Partial-iLIDS
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rank-1 | Rank-3 | S-ST | Rank-1 | Rank-3 |'
  prefs: []
  type: TYPE_TB
- en: '| Position Misalignment | Matching | AMC+SWM [[15](#bib.bib15)] | ICCV2015
    | 37.3 | 46.0 | ✓ | 21.0 | 32.8 |'
  prefs: []
  type: TYPE_TB
- en: '| DSR [[30](#bib.bib30)] | CVPR2018 | 50.7 | 70.0 | ✓ | 58.8 | 67.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DAReID [[38](#bib.bib38)] | KBS2021 | 68.1 | 79.5 | × | 76.7 | 85.3 |'
  prefs: []
  type: TYPE_TB
- en: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Co-Attention [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| ASAN [[41](#bib.bib41)] | TCSVT2021 | 86.8 | 93.5 | × | 81.7 | 88.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Auxiliary Model for Position | PDVM [[45](#bib.bib45)] | PRL2020 | 43.3 |
    - | × | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 68.0 | 80.0 | ✓ | 69.1 | 80.9 |'
  prefs: []
  type: TYPE_TB
- en: '| KBFM [[47](#bib.bib47)] | ICIP2020 | 69.7 | 82.2 | × | 64.1 | 73.9 |'
  prefs: []
  type: TYPE_TB
- en: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 72.5 | 83.0 | × | 70.6 | 81.3 |'
  prefs: []
  type: TYPE_TB
- en: '| TSA [[55](#bib.bib55)] | ACM MM2020 | 72.7 | 85.2 | ✓ | 73.9 | 84.7 |'
  prefs: []
  type: TYPE_TB
- en: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
  prefs: []
  type: TYPE_TB
- en: '| PVPM [[48](#bib.bib48)] | CVPR2020 | 78.3 | - | ✓ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| HOReID [[42](#bib.bib42)] | CVPR2020 | 85.3 | 91.0 | × | 72.6 | 86.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Additional Supervision for Position | DAReID [[38](#bib.bib38)] | KBS2021
    | 68.1 | 79.5 | × | 76.7 | 85.3 |'
  prefs: []
  type: TYPE_TB
- en: '| PGFL-KD [[58](#bib.bib58)] | MM2021 | 85.1 | 90.8 | × | 74.0 | 86.7 |'
  prefs: []
  type: TYPE_TB
- en: '| HPNet [[60](#bib.bib60)] | ICME2020 | 85.7 | - | ✓ | 68.9 | 80.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Attention Mechanism for Position | VPM [[63](#bib.bib63)] | CVPR2019 | 67.7
    | 81.9 | ✓ | 65.5 | 74.8 |'
  prefs: []
  type: TYPE_TB
- en: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MHSA-Net [[68](#bib.bib68)] | TNNLS2022 | 85.7 | 91.3 | × | 74.9 | 87.2 |'
  prefs: []
  type: TYPE_TB
- en: '| PAT [[67](#bib.bib67)] | CVPR2021 | 88.0 | 92.3 | × | 76.5 | 88.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Image Transformation | STNReID [[84](#bib.bib84)] | TMM2020 | 66.7 | 80.3
    | × | 54.6 | 71.3 |'
  prefs: []
  type: TYPE_TB
- en: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Co-Attention [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| PPCL [[83](#bib.bib83)] | CVPR2021 | 83.7 | 88.7 | × | 71.4 | 85.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Scale Misalignment | Multi-scale Features | DSR [[30](#bib.bib30)] | CVPR2018
    | 50.7 | 70.0 | ✓ | 58.8 | 67.2 |'
  prefs: []
  type: TYPE_TB
- en: '| FPR [[81](#bib.bib81)] | ICCV2019 | 81.0 | - | × | 68.1 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Image Transformation | STNReID [[84](#bib.bib84)] | TMM2020 | 66.7 | 80.3
    | × | 54.6 | 71.3 |'
  prefs: []
  type: TYPE_TB
- en: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Co-Attention [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| PPCL [[83](#bib.bib83)] | CVPR2021 | 83.7 | 88.7 | × | 71.4 | 85.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Noisy Information | Auxiliary Model for Noise | PDVM [[45](#bib.bib45)] |
    PRL2020 | 43.3 | - | × | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 68.0 | 80.0 | ✓ | 69.1 | 80.9 |'
  prefs: []
  type: TYPE_TB
- en: '| KBFM [[47](#bib.bib47)] | ICIP2020 | 69.7 | 82.2 | × | 64.1 | 73.9 |'
  prefs: []
  type: TYPE_TB
- en: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 72.5 | 83.0 | × | 70.6 | 81.3 |'
  prefs: []
  type: TYPE_TB
- en: '| TSA [[55](#bib.bib55)] | ACM MM2020 | 72.7 | 85.2 | ✓ | 73.9 | 84.7 |'
  prefs: []
  type: TYPE_TB
- en: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
  prefs: []
  type: TYPE_TB
- en: '| PVPM [[48](#bib.bib48)] | CVPR2020 | 78.3 | - | ✓ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Co-Attention [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| LKWS [[86](#bib.bib86)] | ICCV2021 | 85.7 | 93.7 | × | 80.7 | 88.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Additional Supervision for Noise | DAReID [[38](#bib.bib38)] | KBS2021 |
    68.1 | 79.5 | × | 76.7 | 85.3 |'
  prefs: []
  type: TYPE_TB
- en: '| FPR [[81](#bib.bib81)] | ICCV2019 | 81.0 | - | × | 68.1 | - |'
  prefs: []
  type: TYPE_TB
- en: '| HPNet [[60](#bib.bib60)] | ICME2020 | 85.7 | - | ✓ | 68.9 | 80.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Attention Mechanism for Noise | CBDB-Net [[96](#bib.bib96)] | TCSVT2021 |
    66.7 | 78.3 | × | 68.4 | 81.5 |'
  prefs: []
  type: TYPE_TB
- en: '| VPM [[63](#bib.bib63)] | CVPR2019 | 67.7 | 81.9 | ✓ | 65.5 | 74.8 |'
  prefs: []
  type: TYPE_TB
- en: '| FED [[93](#bib.bib93)] | CVPR2022 | 83.1 | - | × | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| OAMN [[13](#bib.bib13)] | ICCV2021 | 86.0 | - | × | 77.3 | - |'
  prefs: []
  type: TYPE_TB
- en: V Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we summarize and analyze the evaluation results of occluded
    person Re-ID methods based on the four issues discussed earlier. Following the
    proposed taxonomy, we aim to present potential factors that boost the performance
    of occluded person Re-ID to help facilitate future research.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Performance Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate the occluded person re-identification methods on four widely-used
    image-based datasets, i.e., Partial-REID [[15](#bib.bib15)], Partial-iLIDS [[30](#bib.bib30)],
    Occluded-DukeMTMC (Occ-DukeMTMC) [[36](#bib.bib36)], and Occluded-REID (Occ-ReID) [[16](#bib.bib16)].
    Details about the four datasets are illustrated in Sec. [III-A](#S3.SS1 "III-A
    Datasets ‣ III Datasets and Evaluations ‣ Deep Learning-based Occluded Person
    Re-identification: A Survey"). Since the Partial-REID, Partial-iLIDS, and Occluded-REID
    are small, methods are generally trained on the training set of Market-1501 [[105](#bib.bib105)]
    and tested on these three datasets for evaluation. The performance comparisons
    on two partial person Re-ID datasets and two occluded person Re-ID datasets are
    summarized in Table [III](#S4.T3 "TABLE III ‣ IV-E2 Temporal Recovery ‣ IV-E Missing
    Information ‣ IV Occluded Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey") and Table [IV](#S5.T4 "TABLE IV ‣ V-B Future Directions ‣ V Discussion
    ‣ Deep Learning-based Occluded Person Re-identification: A Survey") respectively.
    From the two tables we obtain the following three observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '1). For addressing the position misalignment issue and the noisy information
    issue, effective solutions are rich and diverse. To be specific, firstly, there
    is not a dominant technical route to accomplish the position alignment: On Partial-ReID,
    attention mechanism-based method PAT [[67](#bib.bib67)] has achieved the top rank-1
    accuracy; On Partial-iLIDS, matching-based method ASAN [[41](#bib.bib41)] has
    reached the best rank-1 and rank-3 accuracy; On Occluded-DukeMTMC, auxiliary model-based
    method PFD [[52](#bib.bib52)] has achieved the top rank-1 accuracy and the best
    mAP; On Occluded-ReID, additional supervision-based method HPNet [[60](#bib.bib60)]
    has obtained the best rank-1 accuracy. Secondly, the most promising technical
    route to suppress the noisy occlusion is difficult to judge: On Partial-ReID,
    the rank-1 accuracies of three technical routes, i.e., auxiliary model, additional
    mechanism, and attention mechanism for noise, are comparable; On Partial-iLIDS
    and Occluded-DukeMTMC, auxiliary model-based methods LKWS [[86](#bib.bib86)] and
    PFD [[52](#bib.bib52)] have reached the best rank-1 accuracy accordingly; On Occluded-ReID,
    additional supervision-based method HPNet [[60](#bib.bib60)] has achieved the
    top rank-1 accuracy. Although neither the position misalignment issue nor the
    noisy information issue has a dominant technical route, the advantages and disadvantages
    of different technical routes can be summarized (see Sec.[V-B](#S5.SS2 "V-B Future
    Directions ‣ V Discussion ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2). The scale misalignment issue and the missing information issue have drawn
    less attention in existing methods. Compared with the methods for addressing the
    position misalignment issue or the noisy information issue, the number of methods
    intended for the scale misalignment issue or the missing information issue is
    significantly smaller.
  prefs: []
  type: TYPE_NORMAL
- en: 3). There are a large number of methods that have considered more than one issue
    at the same time. For instance, PPCL [[83](#bib.bib83)] addresses the position
    and scale misalignment issues; PFD [[52](#bib.bib52)] focuses on the position
    misalignment and the noisy information issues; Co-Attention [[40](#bib.bib40)]
    takes the position misalignment, the scale misalignment, and the noisy information
    issues into consideration. However, there are none of the methods that have considered
    all the four issues caused by occlusion. The in-depth analysis of issues and solutions
    in this survey aims to fill this gap and help develop a more comprehensive solution
    for occluded person Re-ID.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Future Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Table [III](#S4.T3 "TABLE III ‣ IV-E2 Temporal Recovery ‣ IV-E
    Missing Information ‣ IV Occluded Person Re-ID ‣ Deep Learning-based Occluded
    Person Re-identification: A Survey") and Table [IV](#S5.T4 "TABLE IV ‣ V-B Future
    Directions ‣ V Discussion ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey"), there have been consistent improvements in different technical routes
    for addressing different issues over the past few years. Based on the analysis
    of issues and solutions, the following insights can be drawn for the future research
    of occluded person Re-ID.'
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of four significant issues caused by occlusion, the position
    misalignment and the noisy information issues have been widely studied while the
    scale misalignment and the missing information issues are rarely considered in
    existing methods. With the four issues summarized and analyzed in this survey,
    the deeper understanding of occluded person Re-ID can be obtained to contribute
    a more comprehensive solution and help inspire new ideas in the filed.
  prefs: []
  type: TYPE_NORMAL
- en: From the perspective of promising technical routes, it remains an open question
    since the evaluation results of state-of-the-art methods in different technical
    routes are comparable. In spite of this, we analyze and summarize the advantages
    and disadcantages of different technical routes as follows to help boost future
    research.
  prefs: []
  type: TYPE_NORMAL
- en: '1). *Matching.* The well-designed matching components and matching strategies
    greatly improves the performance of occluded person Re-ID. Local and scalable
    matching components with the corresponding matching strategy can help address
    the position misalignment, scale misalignment, and noisy information issues. Moreover,
    it can be easily integrated with other technical routes, e.g., the repeatedly
    reported methods Co-Attention [[40](#bib.bib40)] and ASAN [[41](#bib.bib41)] in
    Table [III](#S4.T3 "TABLE III ‣ IV-E2 Temporal Recovery ‣ IV-E Missing Information
    ‣ IV Occluded Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '2). *Auxiliary Model and Additional Supervision.* In general, there are mainly
    three types of extra information employed for occluded person Re-ID: poses, segments,
    and attributes. The position information, as well as their estimation confidence,
    provided by pose estimation or segmentation are used to help address the position
    misalignment and the noisy information issues respectively (see Fig. [6](#S4.F6
    "Figure 6 ‣ IV-A2 Auxiliary Model for Position ‣ IV-A Position Misalignment ‣
    IV Occluded Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey")). And the extra attribute information provided by person Re-ID datasets
    is generally utilized to formulate an extra task to help alleviate the issues
    brought by occlusion. The two technical routes bring a lot of benefits and convenience
    while they are dependent on the extra labels or external models.'
  prefs: []
  type: TYPE_NORMAL
- en: '3). *Attention Mechanism.* The attention mechanism has been widely studied
    in existing methods for its huge potential and flexibility. In recent three years,
    the methods [[106](#bib.bib106), [40](#bib.bib40), [92](#bib.bib92), [68](#bib.bib68),
    [93](#bib.bib93)] introduce the self-attention (Transformer) to occluded person
    Re-ID have made remarkable improvements on public datasets. Similar to matching-based
    technical route, the attention mechanism can also be integrated with other technical
    routes, e.g., APN [[64](#bib.bib64)] in Table [IV](#S5.T4 "TABLE IV ‣ V-B Future
    Directions ‣ V Discussion ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '4). *Image Transformation.* The partial (holistic) image is transformed to
    obtain the image of consistent contents with the holistic (partial) image, addressing
    the position and the scale misalignment issues simultaneously (see Fig. [8](#S4.F8
    "Figure 8 ‣ IV-C Position and Scale Misalignment ‣ IV Occluded Person Re-ID ‣
    Deep Learning-based Occluded Person Re-identification: A Survey") (*a*)). This
    technical route does make sense and is close to the ideal process while it requires
    more computation costs for the conditional image transformation in the inference
    stage. Furthermore, the image transformation has not achieved a satisfying result
    in the current stage and the performance of this technical route is a little bit
    lower than others.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Performance Comparison on Occ-dukemtmc and Occ-reid.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Issues | Technical Routes | Methods | Publications | Occ-DukeMTMC | Occ-ReID
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rank-1 | mAP | Rank-1 | mAP |'
  prefs: []
  type: TYPE_TB
- en: '| Position Misalignment | Matching | AMC+SWM [[15](#bib.bib15)] | ICCV2015
    | - | - | 31.1 | 27.3 |'
  prefs: []
  type: TYPE_TB
- en: '| GASM [[39](#bib.bib39)] | ECCV2020 | - | - | 74.5 | 65.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DSR [[30](#bib.bib30)] | CVPR2018 | 40.8 | 30.4 | 72.8 | 62.8 |'
  prefs: []
  type: TYPE_TB
- en: '| HOReID [[42](#bib.bib42)] | CVPR2020 | 55.1 | 43.8 | 80.3 | 70.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DAReID [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MoS [[44](#bib.bib44)] | AAAI2021 | 66.6 | 55.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Auxiliary Model for Position | PVPM [[48](#bib.bib48)] | CVPR2020 | - | -
    | 70.4 | 61.2 |'
  prefs: []
  type: TYPE_TB
- en: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 51.4 | 37.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PDVM [[45](#bib.bib45)] | PRL2020 | 53.0 | 38.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| HOReID [[42](#bib.bib42)] | CVPR2020 | 55.1 | 43.8 | 80.3 | 70.2 |'
  prefs: []
  type: TYPE_TB
- en: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 56.3 | 43.5 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PFD [[52](#bib.bib52)] | AAAI2022 | 69.5 | 61.8 | 81.5 | 83.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Additional Supervision for Position | HPNet [[60](#bib.bib60)] | ICME2020
    | - | - | 87.3 | 77.4 |'
  prefs: []
  type: TYPE_TB
- en: '| PGFL-KD [[58](#bib.bib58)] | MM2021 | 63.0 | 54.1 | 80.7 | 70.3 |'
  prefs: []
  type: TYPE_TB
- en: '| DAReID [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RFCNet [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Attention Mechanism for Position | MHSA-Net [[68](#bib.bib68)] | TNNLS2022
    | 59.7 | 44.8 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ISP [[62](#bib.bib62)] | ECCV2020 | 62.8 | 52.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PAT [[67](#bib.bib67)] | CVPR2021 | 64.5 | 53.6 | 81.6 | 72.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SBPA [[66](#bib.bib66)] | SPL2021 | 64.5 | 54.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Scale Misalignment | Multi-scale Features | FPR [[81](#bib.bib81)] | ICCV2019
    | - | - | 78.3 | 68.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DSR [[30](#bib.bib30)] | CVPR2018 | 40.8 | 30.4 | 72.8 | 62.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Noisy Information | Auxiliary Model for Noise | PVPM [[48](#bib.bib48)] |
    CVPR2020 | - | - | 70.4 | 61.2 |'
  prefs: []
  type: TYPE_TB
- en: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 51.4 | 37.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PDVM [[45](#bib.bib45)] | PRL2020 | 53.0 | 38.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 56.3 | 43.5 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| LKWS [[86](#bib.bib86)] | ICCV2021 | 62.2 | 46.3 | 81.0 | 71.0 |'
  prefs: []
  type: TYPE_TB
- en: '| PFD [[52](#bib.bib52)] | AAAI2022 | 69.5 | 61.8 | 81.5 | 83.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Additional Supervision for Noise | GASM [[39](#bib.bib39)] | ECCV2020 | -
    | - | 74.5 | 65.6 |'
  prefs: []
  type: TYPE_TB
- en: '| FPR [[81](#bib.bib81)] | ICCV2019 | - | - | 78.3 | 68.0 |'
  prefs: []
  type: TYPE_TB
- en: '| HPNet [[60](#bib.bib60)] | ICME2020 | - | - | 87.3 | 77.4 |'
  prefs: []
  type: TYPE_TB
- en: '| DAReID [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Attention Mechanism for Noise | IGOAS [[66](#bib.bib66)] | TIP2021 | 60.1
    | 49.4 | 81.1 | - |'
  prefs: []
  type: TYPE_TB
- en: '| OAMN [[13](#bib.bib13)] | ICCV2021 | 62.6 | 46.1 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| OCNet [[97](#bib.bib97)] | ICASSP2022 | 64.3 | 54.4 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DRL-Net [[92](#bib.bib92)] | TMM2021 | 65.8 | 53.9 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SSGR [[91](#bib.bib91)] | ICCV2021 | 65.8 | 57.2 | 78.5 | 72.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FED [[93](#bib.bib93)] | CVPR2022 | 68.1 | 56.4 | 86.3 | 79.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Missing Information | Spatial Recovery | RFCNet [[14](#bib.bib14)] | TPAMI2021
    | 63.9 | 54.5 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Temporal Recovery | RFCNet [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 |
    - | - |'
  prefs: []
  type: TYPE_TB
- en: VI Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This paper aims at providing a systematic survey of occluded person Re-ID to
    help promote future research. We first analyze and summarize four issues brought
    by occlusion in person Re-ID: the position misalignment, the scale misalignment,
    the noisy information, and the missing information. The published publications
    of deep learning-based occluded person Re-ID from top conferences and journals
    before June, 2022 are categorized and introduced accordingly. We provide the performance
    comparison of recent occluded person Re-ID methods on four popular datasets: Partial-ReID,
    Partial-iLIDS, Occluded-ReID, and Occluded-DukeMTMC. Based on the analysis of
    evaluation results, we finally discuss the promising future research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] M. Ye et al. Deep learning for person re-identification: A survey and outlook.
    IEEE Trans. Pattern Anal. Mach. Intell., 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] H. Zhao et al. Spindle net: Person re-identification with human body region
    guided feature decomposition and fusion. In CVPR, pp. 1077–1085, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] M. S. Sarfraz et al. A pose-sensitive embedding for person re-identification
    with expanded cross neighborhood re-ranking. In CVPR, pp. 420–429, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. Liu et al. Pose transferrable person re-identification. In CVPR, pp.
    4099–4108, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] X. Qian et al. Long-term cloth-changing person re-identification. In ACCV,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] P. Hong et al. Fine-grained shape-appearance mutual learning for cloth-changing
    person re-identification. In CVPR, pp. 10513–10522, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Y. Huang et al. Illumination-invariant person re-identification. In MM
    - Proc. ACM Int. Conf. Multimed., pp. 365–373, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] G. Zhang et al. Illumination unification for person re-identification.
    IEEE Trans. Circuits Syst. Video Technol., pp. 1–1, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] S. Karanam et al. Person re-identification with discriminatively trained
    viewpoint invariant dictionaries. In ICCV, pp. 4516–4524, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] X. Sun and L. Zheng. Dissecting person re-identification from the viewpoint
    of viewpoint. In CVPR, pp. 608–617, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] L. Wu et al. Cross-entropy adversarial view adaptation for person re-identification.
    IEEE Trans. Circuits Syst. Video Technol., 30(7):2081–2092, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] J. Miao et al. Identifying visible parts via pose estimation for occluded
    person re-identification. IEEE Trans. Neural Netw. Learn. Syst., 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] P. Chen et al. Occlude them all: Occlusion-aware attention network for
    occluded person re-id. In ICCV, pp. 11833–11842, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] R. Hou et al. Feature completion for occluded person re-identification.
    IEEE Trans. Pattern Anal. Mach. Intell., 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] W.-S. Zheng et al. Partial person re-identification. In ICCV, pp. 4678–4686,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Zhuo et al. Occluded person re-identification. In ICME, pp. 1–6\. IEEE,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] L. Zheng et al. Person re-identification: Past, present and future. arXiv
    preprint arXiv:1610.02984, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] R. Mazzon et al. Person re-identification in crowd. Pattern Recognit.
    Lett., 33(14):1828–1837, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] A. Bedagkar-Gala and S. K. Shah. A survey of approaches and trends in
    person re-identification. Image Vision Comput., 32(4):270–286, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] B. Lavi et al. Survey on deep learning techniques for person re-identification
    task. arXiv preprint arXiv:1807.05284, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] D. Wu et al. Deep learning-based methods for person re-identification:
    A comprehensive review. Neurocomputing, 337:354–371, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Q. Leng et al. A survey of open-world person re-identification. IEEE Trans.
    Circuits Syst. Video Technol., 30(4):1092–1108, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] B. Lavi et al. Survey on reliable deep learning-based person re-identification
    models: Are we there yet? arXiv preprint arXiv:2005.00355, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. karanam et al. A systematic evaluation and benchmark for person re-identification:
    Features, metrics, and datasets. IEEE Trans. Pattern Anal. Mach. Intell., 41(3):523–536,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] K. Islam. Person search: New paradigm of person re-identification: A survey
    and outlook of recent works. Image Vision Comput., 101:103970, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] X. Lin et al. Unsupervised person re-identification: A systematic survey
    of challenges and solutions. arXiv preprint arXiv:2109.06057, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] X. Lin et al. Person search challenges and solutions: A survey. In IJCAI
    Int. Joint Conf. Artif. Intell., pp. 1–10\. RMIT University, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Z. Ming et al. Deep learning-based person re-identification methods: A
    survey and outlook of recent works. Image Vision Comput., pp. 104394, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Z. Wang et al. Beyond intra-modality: a survey of heterogeneous person
    re-identification. In IJCAI Int. Joint Conf. Artif. Intell., pp. 4973–4980, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] L. He et al. Deep spatial feature reconstruction for partial person re-identification:
    Alignment-free approach. In CVPR, pp. 7073–7082, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] W.-S. Zheng et al. Person re-identification by probabilistic relative
    distance comparison. In CVPR 2011, pp. 649–656\. IEEE, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J. Kim and C. D. Yoo. Deep partial person re-identification via attention
    model. In ICIP, pp. 3425–3429\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] W. Li et al. Deepreid: Deep filter pairing neural network for person re-identification.
    In CVPR, pp. 152–159, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A. Ess et al. A mobile vision system for robust multi-person tracking.
    In CVPR, pp. 1–8\. IEEE, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Z. Zheng et al. Unlabeled samples generated by gan improve the person
    re-identification baseline in vitro. In ICCV, pp. 3754–3762, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Miao et al. Pose-guided feature alignment for occluded person re-identification.
    In ICCV, pp. 542–551, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Y. Wu et al. Exploit the unknown gradually: One-shot video-based person
    re-identification by stepwise learning. In CVPR, pp. 5177–5186, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Y. Xu et al. Dual attention-based method for occluded person re-identification.
    Knowledge-Based Systems, 212:106554, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] L. He and W. Liu. Guided saliency feature learning for person re-identification
    in crowded scenes. In ECCV, pp. 357–373\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] C.-S. Lin and Y.-C. F. Wang. Self-supervised bodymap-to-appearance co-attention
    for partial person re-identification. In ICIP, pp. 2299–2303\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] H. Jin et al. Occlusion-sensitive person re-identification via attribute-based
    shift attention. IEEE Trans. Circuits Syst. Video Technol., 32(4):2170–2185, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] G. Wang et al. High-order information matters: Learning relation and topology
    for occluded person re-identification. In CVPR, pp. 6449–6458, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Y. Yan et al. Learning multi-granular hypergraphs for video-based person
    re-identification. In CVPR, pp. 2899–2908, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] M. Jia et al. Matching on sets: Conquer occluded person re-identification
    without alignment. In AAAI Conf. Artif. Intell., pp. 1673–1681, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] S. Zhou et al. Depth occlusion perception feature analysis for person
    re-identification. Pattern Recognit. Lett., 138:617–623, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J. Miao et al. Identifying visible parts via pose estimation for occluded
    person re-identification. IEEE Trans. Neural Netw. Learn. Syst., 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] C. Han et al. Keypoint-based feature matching for partial person re-identification.
    In ICIP, pp. 226–230\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] S. Gao et al. Pose-guided visible part matching for occluded person reid.
    In CVPR, pp. 11744–11752, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Y. Zhai et al. Pgmanet: Pose-guided mixed attention network for occluded
    person re-identification. In IJCNN, pp. 1–8\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. Liu et al. Spatial-temporal correlation and topology learning for person
    re-identification in videos. In CVPR, pp. 4370–4379, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. He et al. Adversarial cross-scale alignment pursuit for seriously misaligned
    person re-identification. In ICIP, pp. 2373–2377\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] W. Tao et al. Pose-guided feature disentangling for occluded person re-identification
    based on transformer. AAAI Conf. Artif. Intell., 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. M. Kalayeh et al. Human semantic parsing for person re-identification.
    In CVPR, pp. 1062–1071, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] R. Quispe and H. Pedrini. Improved person re-identification based on saliency
    and semantic parsing with deep neural network models. Image Vision Comput., 92:103809,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] L. Gao et al. Texture semantically aligned with visibility-aware for partial
    person re-identification. In MM - Proc. ACM Int. Conf. Multimed., pp. 3771–3779,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] J. Xu et al. Attention-aware compositional network for person re-identification.
    In CVPR, pp. 2119–2128, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Z. Zhang et al. Densely semantically aligned person re-identification.
    In CVPR, pp. 667–676, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] K. Zheng et al. Pose-guided feature learning with knowledge distillation
    for occluded person re-identification. In MM - Proc. ACM Int. Conf. Multimed.,
    pp. 4537–4545, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] H. Cai et al. Multi-scale body-part mask guided attention for person re-identification.
    In CVPR Workshops, pp. 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] H. Huang et al. Human parsing based alignment with multi-task learning
    for occluded person re-identification. In ICME, pp. 1–6\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Q. Zhou et al. Fine-grained spatial alignment model for person re-identification
    with focal triplet loss. IEEE Trans. Image Process., 29:7578–7589, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] K. Zhu et al. Identity-guided human semantic parsing for person re-identification.
    In ECCV, pp. 346–363\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Y. Sun et al. Perceive where to focus: Learning visibility-aware part-level
    features for partial person re-identification. In CVPR, pp. 393–402, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] L. Huo et al. Attentive part-aware networks for partial person re-identification.
    In ICPR, pp. 3652–3659\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. Li et al. Diversity regularized spatiotemporal attention for video-based
    person re-identification. In CVPR, pp. 369–378, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] G. Wang et al. Self-guided body part alignment with relation transformers
    for occluded person re-identification. IEEE Signal Processing Letters, 28:1155–1159,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Y. Li et al. Diverse part discovery: Occluded person re-identification
    with part-aware transformer. In CVPR, pp. 2898–2907, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] H. Tan et al. Mhsa-net: Multihead self-attention network for occluded
    person re-identification. IEEE Trans. Neural Netw. Learn. Syst., 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. Vaswani et al. Attention is all you need. Advances in neural information
    processing systems, 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] J. Jiang et al. Dynamic hypergraph neural networks. In IJCAI Int. Joint
    Conf. Artif. Intell., pp. 2635–2641, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Z. Cao et al. Realtime multi-person 2d pose estimation using part affinity
    fields. In CVPR, pp. 7291–7299, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] K. Gong et al. Look into person: Self-supervised structure-sensitive learning
    and a new benchmark for human parsing. In CVPR, pp. 932–940, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] C. Szegedy et al. Rethinking the inception architecture for computer vision.
    In CVPR, pp. 2818–2826, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] X. Li et al. Deepsaliency: Multi-task deep neural network model for salient
    object detection. IEEE Trans. Image Process., 25(8):3919–3930, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] K. Sun et al. Deep high-resolution representation learning for human pose
    estimation. In CVPR, pp. 5693–5703, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] R. A. Güler et al. Densepose: Dense human pose estimation in the wild.
    In CVPR, pp. 7297–7306, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] X. Liang et al. Look into person: Joint body parsing & pose estimation
    network and a new benchmark. IEEE Trans. Pattern Anal. Mach. Intell., 41(4):871–885,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] J. Fu et al. Dual attention network for scene segmentation. In CVPR, pp.
    3146–3154, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] T.-Y. Lin et al. Microsoft coco: Common objects in context. In ECCV, pp.
    740–755\. Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] F. Zheng et al. Pyramidal person re-identification via multi-loss dynamic
    training. In CVPR, pp. 8514–8522, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] L. He et al. Foreground-aware pyramid reconstruction for alignment-free
    occluded person re-identification. In ICCV, pp. 8450–8459, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Y. Zhong et al. Robust partial matching for person search in the wild.
    In CVPR, pp. 6827–6835, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] T. He et al. Partial person re-identification with part-part correspondence
    learning. In CVPR, pp. 9105–9115, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] H. Luo et al. Stnreid: Deep convolutional networks with pairwise spatial
    transformer networks for partial person re-identification. IEEE Trans. Multimedia,
    22(11):2905–2913, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Yu et al. Neighbourhood-guided feature reconstruction for occluded
    person re-identification. arXiv preprint arXiv:2105.07345, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Yang et al. Learning to know where to see: A visibility-aware approach
    for occluded person re-identification. In ICCV, pp. 11885–11894, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] G. Chen et al. Spatial-temporal attention-aware learning for video-based
    person re-identification. IEEE Trans. Image Process., 28(9):4192–4205, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] X. Zhang et al. Semantic-aware occlusion-robust network for occluded person
    re-identification. IEEE Trans. Circuits Syst. Video Technol., 31(7):2764–2778,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Z. Zhong et al. Random erasing data augmentation. In AAAI Conf. Artif.
    Intell., volume 34, pp. 13001–13008, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] C. Zhao et al. Incremental generative occlusion adversarial suppression
    network for person reid. IEEE Trans. Image Process., 30:4212–4224, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] C. Yan et al. Occluded person re-identification with single-scale global
    representations. In ICCV, pp. 11875–11884, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] M. Jia et al. Learning disentangled representation implicitly via transformer
    for occluded person re-identification. IEEE Trans. Multimedia, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Z. Wang et al. Feature erasing and diffusion network for occluded person
    re-identification. In CVPR, pp. 4754–4763, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] M. Zheng et al. Re-identification with consistent attentive siamese networks.
    In CVPR, pp. 5735–5744, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] S. Zhao et al. Do not disturb me: Person re-identification under the interference
    of other pedestrians. In ECCV, pp. 647–663\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] H. Tan et al. Incomplete descriptor mining with elastic loss for person
    re-identification. IEEE Trans. Circuits Syst. Video Technol., 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] M. Kim et al. Occluded person re-identification via relational adaptive
    feature correction learning. In ICASSP IEEE Int Conf Acoust Speech Signal Process
    Proc, pp. 2719–2723\. IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] T. Ruan et al. Devil in the details: Towards accurate single and multiple
    human parsing. In AAAI Conf. Artif. Intell., volume 33, pp. 4814–4821, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] H. Zhao et al. Pyramid scene parsing network. In CVPR, pp. 2881–2890,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Zhou et al. Objects as points. arXiv preprint arXiv:1904.07850, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] M. Yin et al. Disentangled non-local neural networks. In ECCV, pp. 191–207\.
    Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] R. Hou et al. Vrstc: Occlusion-free video person re-identification. In
    CVPR, pp. 7183–7192, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y. Liu et al. Spatial and temporal mutual promotion for video-based person
    re-identification. In AAAI Conf. Artif. Intell., volume 33, pp. 8786–8793, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] H. Zhang et al. Self-attention generative adversarial networks. In ICML,
    pp. 7354–7363\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] L. Zheng et al. Scalable person re-identification: A benchmark. In ICCV,
    pp. 1116–1124, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] A. Dosovitskiy et al. An image is worth 16x16 words: Transformers for
    image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Yunjie Peng received her B.S. degree in the College of Computer and Information
    Science & College of Software from Southwest University, China, in 2018. She is
    currently a Ph.D. student in the School of Computer Science and Technology, Beihang
    University, China. Her research interests include gait recognition, perosn re-identification,
    computer vision, and machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| Saihui Hou received the B.E. and Ph.D. degrees from University of Science
    and Technology of China in 2014 and 2019, respectively. He is currently an Assistant
    Professor with School of Artificial Intelligence, Beijing Normal University. His
    research interests include computer vision and machine learning. He recently focuses
    on gait recognition which aims to identify different people according to the walking
    patterns. |'
  prefs: []
  type: TYPE_TB
- en: '| Chunshui Cao received the B.E. and Ph.D. degrees from University of Science
    and Technology of China in 2013 and 2018, respectively. During his Ph.D. study,
    he joined Center for Research on Intelligent Perception and Computing, National
    Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of
    Sciences. From 2018 to 2020, he worked as a Postdoctoral Fellow with PBC School
    of Finance, Tsinghua University. He is currently a Research Scientist with Watrix
    Technology Limited Co. Ltd. His research interests include pattern recognition,
    computer vision and machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| Xu Liu received the B.S. and Ph.D. degrees in control science and engineering
    from the University of Science and Technology of China (USTC), Hefei, China, in
    2013 and 2018, respectively. He is currently an algorithm researcher at Watrix
    Technology Limited Co. Ltd. His current research interests include gait recognition,
    object detection, segmentation and deep learning. |'
  prefs: []
  type: TYPE_TB
- en: '| Yongzhen Huang received the B.E. degree from Huazhong University of Science
    and Technology in 2006, and the Ph.D. degree from Institute of Automation, Chinese
    Academy of Sciences in 2011. He is currently an Associate Professor with School
    of Artificial Intelligence, Beijing Normal University. He has published one book
    and more than 80 papers at international journals and conferences such as TPAMI,
    IJCV, TIP, TSMCB, TMM, TCSVT, CVPR, ICCV, ECCV, NIPS, AAAI. His research interests
    include pattern recognition, computer vision and machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| Zhiqiang He is currently the Senior Vice President of Lenovo Company and
    President of Lenovo Capital and Incubator Group. This group is responsible for
    exploring external innovation as well as accelerating internal innovation for
    Lenovo Group, leveraging Lenovo global resources, power of capital, and entrepreneurship.
    Previously, he was the Chief Technology Officer and held various leadership positions
    in Lenovo, particularly in overseeing Lenovoâs Research & Technology initiatives
    and systems. He is a doctoral supervisor at the Institute of Computing Technology,
    Chinese Academy of Sciences and Beihang University. |'
  prefs: []
  type: TYPE_TB
