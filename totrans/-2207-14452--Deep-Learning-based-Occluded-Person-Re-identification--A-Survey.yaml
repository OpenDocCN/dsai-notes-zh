- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:44:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:44:58（日期：2024年09月06日 19:44:58）'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2207.14452] Deep Learning-based Occluded Person Re-identification: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2207.14452] Deep Learning-based Occluded Person Re-identification: A Survey（易曼：基于深度学习的遮挡人物再识别：一项调查研究）'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2207.14452](https://ar5iv.labs.arxiv.org/html/2207.14452)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2207.14452](https://ar5iv.labs.arxiv.org/html/2207.14452)
- en: 'Deep Learning-based Occluded Person Re-identification: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Deep Learning-based Occluded Person Re-identification: A Survey（基于深度学习的遮挡人物再识别：一项调查研究）'
- en: Yunjie Peng, Saihui Hou, Chunshui Cao, Xu Liu,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 云洁彭，赫某人，曹春水，刘旭，
- en: 'Yongzhen Huang, Zhiqiang He1 1Corresponding author.Yunjie Peng is with the
    School of Computer Science and Technology, Beihang University, Beijing 100191,
    China (email: yunjiepeng@buaa.edu.cn).Saihui Hou and Yongzhen Huang are with the
    School of Artificial Intelligence, Beijing Normal University, Beijing 100875,
    China and also with Watrix Technology Limited Co. Ltd, Beijing, China (email:
    housaihui@bnu.edu.cn; huangyongzhen@bnu.edu.cn).Chunshui Cao and Xu Liu are with
    the Watrix Technology Limited Co. Ltd, Beijing, China (email: chunshui.cao@watrix.ai;
    xu.liu@watrix.ai).Zhiqiang He is with the School of Computer Science and Technology,
    Beihang University, Beijing 100191, China and the Lenovo Corporation, Beijing,
    China (email: zqhe1963@gmail.com).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 云洁彭，张强何1 1通讯作者。云洁彭就职于北航计算机科学与技术学院，中国北京市，邮编100191（电子邮件：yunjiepeng@buaa.edu.cn）。赫某人和黄勇臻就职于北京师范大学人工智能学院，中国北京市，并且还在北京智慧科技有限公司任职（电子邮件：housaihui@bnu.edu.cn；huangyongzhen@bnu.edu.cn）。曹春水和刘旭就职于北京智慧科技有限公司，中国北京市（电子邮件：chunshui.cao@watrix.ai；xu.liu@watrix.ai）。张强何就职于北航计算机科学与技术学院，中国北京市，以及联想公司，中国北京市（电子邮件：zqhe1963@gmail.com）。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Abstract（摘要）
- en: 'Occluded person re-identification (Re-ID) aims at addressing the occlusion
    problem when retrieving the person of interest across multiple cameras. With the
    promotion of deep learning technology and the increasing demand for intelligent
    video surveillance, the frequent occlusion in real-world applications has made
    occluded person Re-ID draw considerable interest from researchers. A large number
    of occluded person Re-ID methods have been proposed while there are few surveys
    that focus on occlusion. To fill this gap and help boost future research, this
    paper provides a systematic survey of occluded person Re-ID. Through an in-depth
    analysis of the occlusion in person Re-ID, most existing methods are found to
    only consider part of the problems brought by occlusion. Therefore, we review
    occlusion-related person Re-ID methods from the perspective of issues and solutions.
    We summarize four issues caused by occlusion in person Re-ID, i.e., position misalignment,
    scale misalignment, noisy information, and missing information. The occlusion-related
    methods addressing different issues are then categorized and introduced accordingly.
    After that, we summarize and compare the performance of recent occluded person
    Re-ID methods on four popular datasets: Partial-ReID, Partial-iLIDS, Occluded-ReID,
    and Occluded-DukeMTMC. Finally, we provide insights on promising future research
    directions.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 遮挡人物再识别（Re-ID）旨在解决在多个摄像头中检索感兴趣人物时的遮挡问题。随着深度学习技术的推动和智能视频监控需求的增加，现实世界应用中的频繁遮挡使得遮挡人物Re-ID引起了研究人员的广泛关注。已经提出了大量的遮挡人物Re-ID方法，但关于遮挡的调查报告很少。为了填补这一空白并帮助推动未来的研究，本文对遮挡人物Re-ID进行了系统的调查。通过深入分析人物Re-ID中的遮挡，发现现有方法只考虑部分由遮挡带来的问题。因此，我们从问题和解决方法的角度对相关的遮挡人物Re-ID方法进行了回顾。我们总结了人物Re-ID中由遮挡引起的四个问题，即位置不对齐、尺度不对齐、噪声信息和丢失信息。然后，我们对解决不同问题的遮挡相关方法进行了分类和介绍。之后，我们总结并比较了近期遮挡人物Re-ID方法在四个常用数据集上的性能：Partial-ReID，Partial-iLIDS，Occluded-ReID和Occluded-DukeMTMC。最后，我们对未来有希望的研究方向提供了一些见解。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Index Terms:（索引术语）
- en: Occluded Person Re-identification, Partial Person Re-identification, Literature
    Survey, Deep Learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Occluded Person Re-identification, Partial Person Re-identification, Literature
    Survey, Deep Learning.
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I Introduction（导言）
- en: Person re-identification (Re-ID) retrieves persons of the same identity across
    different cameras [[1](#bib.bib1)]. With the expanding deployment of surveillance
    cameras and the increasing demand for public safety, person Re-ID which plays
    the fundamental role in intelligent surveillance has become a research hotspot
    in the computer vision community. In practice, the internal variations of a person
    (e.g., pose variations [[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)] and clothes
    changes [[5](#bib.bib5), [6](#bib.bib6)]), as well as the complex environments
    (e.g., illumination changes [[7](#bib.bib7), [8](#bib.bib8)], viewpoint variations [[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)], and occlusion [[12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14)]), bring significant challenges to person Re-ID. Among them,
    the occlusion which occurs frequently in real-world applications and affects the
    accuracy greatly has received considerable interest from researchers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人像重识别（Re-ID）在不同摄像头中检索相同身份的人员 [[1](#bib.bib1)]。随着监控摄像头的不断增加和对公共安全需求的增加，人像重识别作为智能监控中的基础角色，已成为计算机视觉领域的研究热点。在实际应用中，个人的内部变化（例如，姿态变化 [[2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4)] 和服装变化 [[5](#bib.bib5), [6](#bib.bib6)]），以及复杂的环境（例如，光照变化 [[7](#bib.bib7),
    [8](#bib.bib8)]，视角变化 [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)] 和遮挡 [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)]），给人像重识别带来了显著挑战。其中，遮挡在实际应用中频繁出现，并且对准确性影响很大，已经引起了研究人员的广泛关注。
- en: Occluded person re-identification [[15](#bib.bib15), [16](#bib.bib16), [14](#bib.bib14)]
    is proposed to address the occlusion problem for real-world person re-identification.
    Different from general person Re-ID approaches which assume the retrieval process
    with whole human body available, occluded person Re-ID highlights the scenario
    of pedestrian images occluded by various obstacles (e.g., cars, trees, and crowds)
    and focuses on retrieving persons of the same identity when given an occluded
    query.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 被遮挡人像重识别 [[15](#bib.bib15), [16](#bib.bib16), [14](#bib.bib14)]是为解决实际人像重识别中的遮挡问题而提出的。与假设检索过程中有完整人体存在的一般人像重识别方法不同，被遮挡人像重识别突出了被各种障碍物（例如汽车、树木和人群）遮挡的行人图像的场景，并专注于在给定被遮挡查询时检索相同身份的人员。
- en: '![Refer to caption](img/d251e866dce637738c1c7a400478e7cc.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d251e866dce637738c1c7a400478e7cc.png)'
- en: 'Figure 1: The taxonomy of occluded person Re-ID methods from the perspective
    of issues and solutions. Using the above taxonomy, it is easy to know the inherent
    challenges for occluded person Re-ID and have a general understanding of overall
    technical routes.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：从问题和解决方案的角度来看，被遮挡人像重识别方法的分类。使用上述分类，可以轻松了解被遮挡人像重识别的固有挑战，并对整体技术路线有一个大致了解。
- en: With the advancement of deep learning, a large number of occluded person Re-ID
    methods have been proposed while there are few surveys that focus on occlusion.
    To fill this gap, this paper summarizes occlusion-related person Re-ID works and
    provides a systematic survey of occluded person Re-ID. Through an in-depth analysis
    of the occlusion in person re-identification, most existing methods are found
    to only consider part of the problems caused by occlusion. Therefore, we review
    occluded person Re-ID from the perspective of issues and solutions to facilitate
    the understanding of the latest approaches and inspire new ideas in the field.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的进步，已经提出了大量的被遮挡人像重识别方法，但针对遮挡问题的综述却很少。为填补这一空白，本文总结了与遮挡相关的人像重识别工作，并提供了对被遮挡人像重识别的系统性综述。通过对人像重识别中的遮挡进行深入分析，发现大多数现有方法仅考虑了遮挡引发的一部分问题。因此，我们从问题和解决方案的角度回顾被遮挡人像重识别，以促进对最新方法的理解，并激发该领域的新思路。
- en: 'The issues caused by occlusion for person Re-ID are carefully summarized from
    the whole process of person re-identification. Technically speaking, a practical
    person Re-ID system in video surveillance mainly consists of three stages [[17](#bib.bib17)]:
    pedestrian detection, trajectory tracking, and person retrieval. Although it is
    generally believed that the first two stages are independent computer vision tasks
    and most person Re-ID works focus on person retrieval, the occlusion will affect
    the whole process and bring great challenges to the final re-identification. In
    summary, four significant issues are considered for occluded person Re-ID in this
    paper: the position misalignment, the scale misalignment, the noisy information,
    and the missing information. Each issue is illustrated in Fig. [2](#S1.F2 "Figure
    2 ‣ I Introduction ‣ Deep Learning-based Occluded Person Re-identification: A
    Survey") and we briefly introduce each issue as follows.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '由遮挡引起的行人重识别问题在整个行人重识别过程中进行了详细总结。从技术角度来看，视频监控中的实际行人重识别系统主要包括三个阶段[[17](#bib.bib17)]：行人检测、轨迹跟踪和行人检索。虽然通常认为前两个阶段是独立的计算机视觉任务，大多数行人重识别工作集中于行人检索，但遮挡会影响整个过程，并给最终重识别带来巨大挑战。总之，本文考虑了四个重要的遮挡行人重识别问题：位置错位、尺度错位、噪声信息和缺失信息。每个问题在图[2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey")中进行了说明，我们简要介绍每个问题如下。'
- en: 1). *The position misalignment.* Generally, the detected human boxes are resized
    by height to obtain the same size of input data for person retrieval. In the case
    of occlusion, the detected box of the person contains only part of the human body
    while it undergoes the same alignment processing as that of a non-occluded person.
    The contents at the same position of the processed partial image and the processed
    holistic image are likely to be mismatched, resulting in the position misalignment
    issue. 2). *The scale misalignment.* Similar to the position misalignment, the
    scale misalignment also arises from the upstream data processing procedure. The
    occlusion may affect the height of the detected box and thus influence the resizing
    ratio in the data processing, resulting in the scale misalignment between a partial
    and a holistic image. 3). *The noisy information.* In the detected boxes of occluded
    pedestrians, the occlusion is inevitably included in whole or in part and brings
    the noisy information for person Re-ID. 4). *The missing information.* In the
    detected boxes of occluded pedestrians, the identity information of occluded regions
    is missing, resulting in the missing information issue.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 1). *位置错位。* 通常，检测到的人体框会根据高度进行调整，以获得相同大小的输入数据进行行人检索。在遮挡情况下，检测到的人的框仅包含人体的一部分，而它经过的对齐处理与非遮挡行人相同。处理的部分图像和完整图像在相同位置的内容可能会不匹配，导致位置错位问题。2).
    *尺度错位。* 类似于位置错位，尺度错位也来源于上游数据处理过程。遮挡可能影响检测框的高度，从而影响数据处理中的调整比例，导致部分图像和整体图像之间的尺度错位。3).
    *噪声信息。* 在遮挡行人的检测框中，遮挡不可避免地全部或部分包含在内，带来噪声信息，影响行人重识别。4). *缺失信息。* 在遮挡行人的检测框中，遮挡区域的身份信息丢失，导致缺失信息问题。
- en: 'This paper analyzes occlusion-related person Re-ID methods regarding the above-mentioned
    four issues and provides a multi-dimensional taxonomy to categorize solutions
    for each issue (see Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning-based
    Occluded Person Re-identification: A Survey")). Specifically, we mainly review
    published publications of deep learning-based occluded person Re-ID from top conferences
    and journals before June, 2022, and meanwhile we also introduce some methods from
    other conferences and journals as supplements. We discuss the issues brought by
    occlusion for person Re-ID and provide an in-depth analysis of how the issues
    are addressed in recent works with evaluation results summarized accordingly.
    Particularly, some person Re-ID methods are closely related to occlusion and we
    also summarize these methods to obtain a more comprehensive survey for occluded
    person Re-ID. The main contributions of this survey lie in three aspects:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '本文分析了与遮挡相关的人脸 Re-ID 方法，针对上述四个问题提供了多维度的分类方法，对每个问题的解决方案进行分类（参见图 [1](#S1.F1 "Figure
    1 ‣ I Introduction ‣ Deep Learning-based Occluded Person Re-identification: A
    Survey")）。具体来说，我们主要回顾了在 2022 年 6 月之前的顶级会议和期刊上发表的基于深度学习的遮挡人脸 Re-ID 相关的出版物，同时也介绍了一些来自其他会议和期刊的方法作为补充。我们讨论了遮挡对人脸
    Re-ID 带来的问题，并对如何在近期工作中解决这些问题进行了深入分析，并总结了评估结果。特别地，一些人脸 Re-ID 方法与遮挡密切相关，我们也总结了这些方法，以获得对遮挡人脸
    Re-ID 更全面的调查。此调查的主要贡献有三方面：'
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To fill the gap of the occluded person Re-ID survey, we review recent person
    re-identification methods for occlusion and provide a systematic survey from the
    perspective of issues and solutions.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了填补遮挡人脸 Re-ID 调查的空白，我们回顾了近期的遮挡人脸重新识别方法，并从问题和解决方案的角度提供了系统的调查。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We summarize and compare the performance of mainstream occluded person Re-ID
    approaches for researchers and industries to use based on their practical needs.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们总结并比较了主流遮挡人脸 Re-ID 方法的性能，以供研究人员和工业界根据实际需求使用。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We summarize and analyze the advantages and disadvantages of different types
    of solutions for occluded person Re-ID and provide insights on promising research
    directions in this field.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们总结和分析了不同类型的遮挡人脸重新识别（Re-ID）解决方案的优缺点，并提供了该领域有前景的研究方向的见解。
- en: The rest of this paper is organized as follows. Section 2 presents a summary
    of previous surveys and elaborates on efforts made by this survey compared with
    others. Section 3 summarizes the common datasets and evaluation metrics of occluded
    person Re-ID. Section 4 provides an in-depth analysis of deep learning-based occluded
    person Re-ID methods from the perspective of issues and solutions. Section 5 compares
    the performance of various solutions and provides insights on promising research
    directions. Section 6 gives our conclusions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：第 2 节总结了先前的调查，并详细阐述了本调查与其他调查的比较。第 3 节总结了遮挡人脸 Re-ID 的常用数据集和评估指标。第
    4 节从问题和解决方案的角度对基于深度学习的遮挡人脸 Re-ID 方法进行了深入分析。第 5 节比较了各种解决方案的性能，并提供了对有前景的研究方向的见解。第
    6 节给出了我们的结论。
- en: '![Refer to caption](img/469d50fa3a831e3e17d421a9740a0318.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/469d50fa3a831e3e17d421a9740a0318.png)'
- en: 'Figure 2: The position misalignment, scale misalignment, noisy information,
    and missing information issues caused by occlusion for person Re-ID.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：遮挡对人脸 Re-ID 造成的位置错位、尺度错位、噪声信息和缺失信息问题。
- en: II Previous Surveys
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 先前的调查
- en: 'In the previous literature, there are some surveys that have also reviewed
    the field of person Re-ID. To obtain a more comprehensive comparison, we summarize
    the surveys of person Re-ID since 2012. The taxonomies of these surveys are listed
    in Table [I](#S2.T1 "TABLE I ‣ II Previous Surveys ‣ Deep Learning-based Occluded
    Person Re-identification: A Survey"). On the whole, previous surveys of person
    Re-ID can be roughly divided into traditional surveys [[18](#bib.bib18), [19](#bib.bib19)]
    and deep learning-based surveys [[17](#bib.bib17), [20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26),
    [27](#bib.bib27), [1](#bib.bib1), [28](#bib.bib28)].'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '在以往的文献中，也有一些调查回顾了人员重识别（Re-ID）领域。为了获得更全面的比较，我们总结了自2012年以来的人员重识别调查。这些调查的分类列在表格 [I](#S2.T1
    "TABLE I ‣ II Previous Surveys ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey")中。总体来说，以往的人员重识别调查可以大致分为传统调查[[18](#bib.bib18), [19](#bib.bib19)]和基于深度学习的调查[[17](#bib.bib17),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [1](#bib.bib1), [28](#bib.bib28)]。'
- en: 'Traditional surveys mainly review person Re-ID methods that manually design
    the feature extraction procedure and learn a better similarity measurement. Mazzon
    et al., [[18](#bib.bib18)] summarize four main phases for person Re-ID: multi-person
    detection, feature extraction, cross-camera calibration, and person association.
    Assuming that the first phase (i.e., multi-person detection) has been solved,
    methods of extracting color/texture/shape appearance features, grouping temporal
    information, conducting color/spatio-temporal cross-camera calibration, and distance/learning/optimization-based
    person association are reviewed accordingly. Apurva et al., [[19](#bib.bib19)]
    regard the tracking across multiple cameras as the open set matching problem and
    the identity retrieval as the close set matching problem. According to whether
    additional camera geometry/calibration information is available, methods are divided
    into contextual Re-ID and non-contextual Re-ID for open-set and close-set matching
    respectively.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 传统调查主要回顾了手动设计特征提取过程并学习更好的相似度测量的人员重识别方法。Mazzon 等人[[18](#bib.bib18)]总结了人员重识别的四个主要阶段：多目标检测、特征提取、跨摄像头标定和人员关联。假设第一个阶段（即多目标检测）已被解决，那么针对提取颜色/纹理/形状外观特征、分组时间信息、进行颜色/时空跨摄像头标定以及基于距离/学习/优化的人员关联的方法进行了相应的回顾。Apurva
    等人[[19](#bib.bib19)]将跨摄像头跟踪视为开放集匹配问题，将身份检索视为闭合集匹配问题。根据是否有额外的摄像头几何/标定信息，方法被分为上下文相关的重识别和非上下文相关的重识别，分别用于开放集和闭合集匹配。
- en: Deep learning-based surveys mainly summarize the person Re-ID methods using
    deep learning techniques from different perspectives. A few of these surveys [[17](#bib.bib17),
    [24](#bib.bib24)] also review traditional methods for the sake of completeness.
    In general, previous deep learning-based surveys have involved loss design [[20](#bib.bib20),
    [25](#bib.bib25)], technical means [[25](#bib.bib25), [28](#bib.bib28)], data
    augmentation [[20](#bib.bib20), [21](#bib.bib21)], image and video [[17](#bib.bib17),
    [1](#bib.bib1)], classification and verification [[20](#bib.bib20), [21](#bib.bib21),
    [23](#bib.bib23)], open-world and close-world [[22](#bib.bib22), [1](#bib.bib1)],
    multi-modality [[22](#bib.bib22), [27](#bib.bib27)], ranking optimization [[24](#bib.bib24),
    [1](#bib.bib1)], noisy annotation [[1](#bib.bib1)], unsupervised learning [[26](#bib.bib26)],
    and metric learning [[21](#bib.bib21), [24](#bib.bib24), [25](#bib.bib25), [28](#bib.bib28),
    [1](#bib.bib1)]. Despite such a number of surveys on person Re-ID, the occlusion
    problem has not drawn enough attention and only a few surveys pay attention to
    occluded person Re-ID. As far as we know, Ye et al.,  [[1](#bib.bib1)] have made
    a rough summary of occluded person Re-ID as a part of Noise-Robust Re-ID. Ming
    et al.,  [[28](#bib.bib28)] have included several occluded person Re-ID methods
    in local feature learning and sequence feature learning. Considering the practical
    importance of occlusion for person Re-ID, a systematic investigation for occluded
    person Re-ID is essential. Therefore, we provide an in-depth survey of issues
    and solutions involved in occlusion-related person Re-ID works to help boost future
    research.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的调查主要总结了从不同角度使用深度学习技术的行人再识别方法。其中一些调查 [[17](#bib.bib17), [24](#bib.bib24)] 也为了完整性回顾了传统方法。总体而言，以前的深度学习调查涉及了损失设计 [[20](#bib.bib20),
    [25](#bib.bib25)], 技术手段 [[25](#bib.bib25), [28](#bib.bib28)], 数据增强 [[20](#bib.bib20),
    [21](#bib.bib21)], 图像和视频 [[17](#bib.bib17), [1](#bib.bib1)], 分类和验证 [[20](#bib.bib20),
    [21](#bib.bib21), [23](#bib.bib23)], 开放世界和封闭世界 [[22](#bib.bib22), [1](#bib.bib1)],
    多模态 [[22](#bib.bib22), [27](#bib.bib27)], 排名优化 [[24](#bib.bib24), [1](#bib.bib1)],
    噪声注释 [[1](#bib.bib1)], 无监督学习 [[26](#bib.bib26)], 和度量学习 [[21](#bib.bib21), [24](#bib.bib24),
    [25](#bib.bib25), [28](#bib.bib28), [1](#bib.bib1)]。尽管有这么多关于行人再识别的调查，但遮挡问题尚未引起足够的关注，只有少数调查关注到遮挡行人再识别。据我们所知，Ye
    等人 [[1](#bib.bib1)] 对遮挡行人再识别做了粗略的总结，作为噪声鲁棒再识别的一部分。Ming 等人 [[28](#bib.bib28)] 将几个遮挡行人再识别方法纳入了局部特征学习和序列特征学习。考虑到遮挡对行人再识别的实际重要性，对遮挡行人再识别的系统性研究至关重要。因此，我们提供了对遮挡相关的行人再识别工作的深入调查，以促进未来的研究。
- en: 'TABLE I: The Overview of Person Re-ID Surveys in Recent Years.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 近年来行人再识别调查概述。'
- en: '| Surveys | Reference | Taxonomy |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 调查 | 参考文献 | 分类 |'
- en: '| --- | --- | --- |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Traditional | 2012 PRL [[18](#bib.bib18)] |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 传统 | 2012 PRL [[18](#bib.bib18)] |'
- en: '&#124; Feature Extraction / Cross-camera Calibration / &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征提取 / 跨相机校准 / &#124;'
- en: '&#124; Person Association &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行人关联 &#124;'
- en: '|'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2014 IVC [[19](#bib.bib19)] |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 2014 IVC [[19](#bib.bib19)] |'
- en: '&#124; Contextual Methods (camera geometry info; &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 上下文方法（相机几何信息； &#124;'
- en: '&#124; camera calibration.) / Non-contextual Methods &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相机校准。） / 非上下文方法 &#124;'
- en: '&#124; (passive methods; active methods) &#124;'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (被动方法；主动方法) &#124;'
- en: '|'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Deep learning-based | 2016 arXiv [[17](#bib.bib17)] |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 基于深度学习的 | 2016 arXiv [[17](#bib.bib17)] |'
- en: '&#124; Image-based Methods / Video-based Methods &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于图像的方法 / 基于视频的方法 &#124;'
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 TCSVT [[22](#bib.bib22)] |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 2019 TCSVT [[22](#bib.bib22)] |'
- en: '&#124; Person Verification / Application-driven Methods &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行人验证 / 应用驱动方法 &#124;'
- en: '&#124; (raw data; practial procedure; efficiency) &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (原始数据；实际程序；效率) &#124;'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2019 TPAMI [[24](#bib.bib24)] |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 2019 TPAMI [[24](#bib.bib24)] |'
- en: '&#124; Feature Extraction / Metric Learning / Multi-shot &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征提取 / 度量学习 / 多镜头 &#124;'
- en: '&#124; Ranking &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 排名 &#124;'
- en: '|'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2020 arXiv [[23](#bib.bib23)] |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 2020 arXiv [[23](#bib.bib23)] |'
- en: '&#124; Identification Task / Verification Task &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 识别任务 / 验证任务 &#124;'
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2020 IVC [[25](#bib.bib25)] |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 2020 IVC [[25](#bib.bib25)] |'
- en: '&#124; Feature Learning / Model Architecture Design / &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征学习 / 模型架构设计 / &#124;'
- en: '&#124; Metric Learning / Loss Function &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 度量学习 / 损失函数 &#124;'
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2021 arXiv [[26](#bib.bib26)] |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 2021 arXiv [[26](#bib.bib26)] |'
- en: '&#124; Pseudo-label Estimation / Deep Feature &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 伪标签估计 / 深度特征 &#124;'
- en: '&#124; Representation Learning / Camera-aware &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 表示学习 / 相机感知 &#124;'
- en: '&#124; Invariance Learning / Unsupervised Domain &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不变性学习 / 无监督领域 &#124;'
- en: '&#124; Adaptation &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 适应 &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2021 IJCAI [[29](#bib.bib29)] |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 2021 IJCAI [[29](#bib.bib29)] |'
- en: '&#124; Low Resolution / Infrared / Sketch / Text &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低分辨率 / 红外 / 草图 / 文本 &#124;'
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2021 IJCAI [[27](#bib.bib27)] |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 2021 IJCAI [[27](#bib.bib27)] |'
- en: '&#124; Deep Feature Representation Learning / Deep &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度特征表示学习 / 深度 &#124;'
- en: '&#124; Metric Learning / Identity-driven detection &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 度量学习 / 基于身份的检测 &#124;'
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2021 TPAMI [[1](#bib.bib1)] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 2021 TPAMI [[1](#bib.bib1)] |'
- en: '&#124; Closed-world Setting (deep feature representation &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 封闭世界设置（深度特征表示 &#124;'
- en: '&#124; learning; deep metric learning; ranking optimization) &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习；深度度量学习；排序优化) &#124;'
- en: '&#124; / Open-world Setting (heterogeneous data; raw &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; / 开放世界设置（异质数据；原始 &#124;'
- en: '&#124; images or videos; unavailable or limited labels; &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像或视频；不可用或有限标签； &#124;'
- en: '&#124; open-set; noisy annotation) &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开集；噪声标注) &#124;'
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2022 IVC [[28](#bib.bib28)] |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 2022 IVC [[28](#bib.bib28)] |'
- en: '&#124; Deep Metric Learning / Local Feature Learning &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度度量学习 / 局部特征学习 &#124;'
- en: '&#124; / Generative Adversarial Networks / Sequence &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; / 生成对抗网络 / 序列 &#124;'
- en: '&#124; Feature Learning / Graph Convolutional Networks &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征学习 / 图卷积网络 &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: III Datasets and Evaluations
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 数据集与评估
- en: III-A Datasets
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 数据集
- en: We review eight widely-used datasets for occluded person Re-ID, including 3
    image-based partial¹¹1The partial person Re-ID assumes that the visible region
    of occluded person image is manually cropped for identification. Re-ID datasets,
    4 image-based occluded²²2The occluded person Re-ID does not require the manually
    cropping process of occluded images.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了八个广泛使用的遮挡人重识别数据集，包括3个基于图像的部分¹¹1 部分人重识别假设遮挡人图像的可见区域经过手动裁剪以进行识别。重识别数据集，4个基于图像的遮挡²²2
    遮挡人重识别不需要手动裁剪遮挡图像的过程。
- en: 'Unless otherwise specified, occluded person Re-ID in this survey includes both
    partial person Re-ID and occluded person Re-ID. Re-ID datasets, and 1 video-based
    occluded person Re-ID dataset. The statistics of these datasets are summarized
    in Table [II](#S3.T2 "TABLE II ‣ III-A Datasets ‣ III Datasets and Evaluations
    ‣ Deep Learning-based Occluded Person Re-identification: A Survey") and the details
    of each dataset are reviewed as follows. Examples of partial/occluded person Re-ID
    datasets are shown in Fig. [3](#S3.F3 "Figure 3 ‣ III-A Datasets ‣ III Datasets
    and Evaluations ‣ Deep Learning-based Occluded Person Re-identification: A Survey").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，本调查中的遮挡人重识别包括部分人重识别和遮挡人重识别。重识别数据集包括1个基于视频的遮挡人重识别数据集。这些数据集的统计信息总结在表[II](#S3.T2
    "TABLE II ‣ III-A 数据集 ‣ III 数据集与评估 ‣ 基于深度学习的遮挡人重识别：综述")中，每个数据集的详细信息如下审查。部分/遮挡人重识别数据集的示例如图[3](#S3.F3
    "Figure 3 ‣ III-A 数据集 ‣ III 数据集与评估 ‣ 基于深度学习的遮挡人重识别：综述")所示。
- en: Partial-REID [[15](#bib.bib15)] is an image-based partial Re-ID dataset with
    a variety of viewpoints, backgrounds, and occlusion types. It contains 600 images
    of 60 people, with 5 full-body images and 5 partial images per person. The partial
    observation is generated by manually cropping the occluded region in occluded
    images.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Partial-REID [[15](#bib.bib15)] 是一个基于图像的部分重识别数据集，具有多种视角、背景和遮挡类型。它包含了60个人的600张图像，每人有5张全身图像和5张部分图像。部分观测是通过手动裁剪遮挡图像中的遮挡区域生成的。
- en: Partial-iLIDS [[30](#bib.bib30)] is an image-based simulated partial Re-ID dataset
    derived from iLIDS [[31](#bib.bib31)]. It is captured by multiple non-overlapping
    cameras in the airport and contains 238 images from 119 people, with 1 full-body
    image and 1 manually cropped non-occluded partial image per person.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Partial-iLIDS [[30](#bib.bib30)] 是一个基于图像的模拟部分重识别数据集，源自 iLIDS [[31](#bib.bib31)]。它由机场中的多个非重叠摄像头捕获，包含119人的238张图像，每人有1张全身图像和1张手动裁剪的非遮挡部分图像。
- en: p-CUHK03 [[32](#bib.bib32)] is an image-based partial Re-ID dataset constructed
    from CUHK03 [[33](#bib.bib33)]. It contains 1360 person identities captured in
    campus environment. In general, 1160 person identities are used as training set,
    100 person identities are used as validation set, and 100 person identities are
    used as test set. It selects 5 images with same view point from the raw dataset
    for each identity and generates 10 partial body probe images out of selected two
    images. The rest 3 images of each identity are used as full body gallery image.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: p-CUHK03 [[32](#bib.bib32)] 是一个基于图像的部分 Re-ID 数据集，构建自 CUHK03 [[33](#bib.bib33)]。它包含1360个在校园环境中捕获的人物身份。一般来说，1160个人物身份用作训练集，100个人物身份用作验证集，100个人物身份用作测试集。它从原始数据集中为每个身份选择5张相同视角的图像，并从中生成10张部分身体探测图像。其余3张图像用于全身图库图像。
- en: P-ETHZ [[16](#bib.bib16)] is an image-based occluded person Re-ID dataset modified
    from ETHZ [[34](#bib.bib34)]. It has 3897 images of 85 person identities. Each
    identity has 1 to 30 full-body person images and occluded person images respectively.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: P-ETHZ [[16](#bib.bib16)] 是一个基于图像的遮挡行人 Re-ID 数据集，修改自 ETHZ [[34](#bib.bib34)]。它包含3897张85个身份的图像。每个身份分别拥有1到30张全身图像和遮挡图像。
- en: Occluded-REID [[16](#bib.bib16)] is an image-based occluded person Re-ID dataset
    captured by mobile cameras with different viewpoints and different types of severe
    occlusion. It consists of 2000 images of 200 people, with 5 full-body images and
    5 occluded images per person.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Occluded-REID [[16](#bib.bib16)] 是一个基于图像的遮挡行人 Re-ID 数据集，由移动摄像机捕获，具有不同的视角和严重的遮挡类型。它包含2000张200人的图像，每人有5张全身图像和5张遮挡图像。
- en: '![Refer to caption](img/046ca4ecfd337a309bd5427adffdc953.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/046ca4ecfd337a309bd5427adffdc953.png)'
- en: 'Figure 3: Examples of partial/occluded person Re-ID datasets. Partial/occluded
    person images (the first row) and full-body person images (the second row).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：部分/遮挡行人 Re-ID 数据集的示例。部分/遮挡行人图像（第一行）和全身行人图像（第二行）。
- en: 'TABLE II: Occluded/Partial Person Re-id Datasets. Occluded-dukemtmc and Occluded-dukemtmc-videoreid
    are Abbreviated as Occ-dukemtmc and Occ-dukemtmc-video Respectively.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：遮挡/部分行人 Re-ID 数据集。Occluded-dukemtmc 和 Occluded-dukemtmc-videoreid 分别缩写为
    Occ-dukemtmc 和 Occ-dukemtmc-video。
- en: '| Dataset | Training Set (ID/Image) | Test Set (ID/Image) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 训练集（ID/图像） | 测试集（ID/图像） |'
- en: '| Gallery | Query |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 图库 | 查询 |'
- en: '| Partial-REID | - | 60/300 | 60/300 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Partial-REID | - | 60/300 | 60/300 |'
- en: '| Partial-iLIDS | - | 119/119 | 119/119 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Partial-iLIDS | - | 119/119 | 119/119 |'
- en: '| p-CUHK03 | 1160/15080 | 100/300 | 100/1000 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| p-CUHK03 | 1160/15080 | 100/300 | 100/1000 |'
- en: '| P-ETHZ | 43/ - | 42/ - | 42/ - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| P-ETHZ | 43/ - | 42/ - | 42/ - |'
- en: '| Occluded-REID | - | 200/1000 | 200/1000 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Occluded-REID | - | 200/1000 | 200/1000 |'
- en: '| P-DukeMTMC-reID | 650/ - | 649/ - | 649/ - |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| P-DukeMTMC-reID | 650/ - | 649/ - | 649/ - |'
- en: '| Occ-DukeMTMC | 702/15,618 | 1,110/17,661 | 519/2,210 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Occ-DukeMTMC | 702/15,618 | 1,110/17,661 | 519/2,210 |'
- en: '| Occ-DukeMTMC-Video | 702/292,343 | 1,110/281,114 | 661/39,526 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Occ-DukeMTMC-Video | 702/292,343 | 1,110/281,114 | 661/39,526 |'
- en: P-DukeMTMC-reID [[16](#bib.bib16)] is an image-based occluded person Re-ID dataset
    modified from DukeMTMC-reID [[35](#bib.bib35)]. It has 24143 images of 1299 person
    identities and contains images with target persons occluded by different types
    of occlusion in public, e.g., people, cars, and guideboards. Each identity has
    both full-body images and occluded images.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: P-DukeMTMC-reID [[16](#bib.bib16)] 是一个基于图像的遮挡行人Re-ID数据集，修改自DukeMTMC-reID [[35](#bib.bib35)]。它包含24143张1299个身份的图像，并且包含由公共场所不同类型的遮挡物（如人、车和指示牌）遮挡的目标人物图像。每个身份都有全身图像和遮挡图像。
- en: Occluded-DukeMTMC [[36](#bib.bib36)] is an image-based occluded person Re-ID
    dataset built from DukeMTMC-reID [[35](#bib.bib35)]. It contains 15,618 images
    of 708 people for training while including 2,210 query images of 519 persons and
    17,661 gallery images of 1110 persons for testing. The 9% of the training set,
    the 100% of the query set, and the 10% of the gallery set are occluded images.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Occluded-DukeMTMC [[36](#bib.bib36)] 是一个基于图像的遮挡行人Re-ID数据集，来源于DukeMTMC-reID [[35](#bib.bib35)]。它包含15,618张708人的图像用于训练，同时包括2,210张519人的查询图像和17,661张1110人的图库图像用于测试。训练集中9%的图像、查询集中100%的图像以及图库集中10%的图像是遮挡图像。
- en: Occluded-DukeMTMC-VideoReID [[14](#bib.bib14)] is a video-based occluded Re-ID
    dataset reorganized from the DukeMTMC-VideoReID [[37](#bib.bib37)]. It includes
    large variety of obstacles, e.g., cars, trees, bicycles, and other persons. It
    contains 1,702 image sequences of 702 identities for training, 661 query image
    sequences of 661 identities and 2,636 gallery image sequences of 1110 identities
    for testing. More than 1/3 frames of each query sequence in the testing set contain
    occlusion.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Occluded-DukeMTMC-VideoReID [[14](#bib.bib14)] 是一个基于视频的遮挡 Re-ID 数据集，重组自 DukeMTMC-VideoReID [[37](#bib.bib37)]。它包括各种障碍物，例如汽车、树木、自行车和其他人。数据集包含
    1,702 个图像序列用于训练，661 个查询图像序列用于 661 个身份的查询，以及 2,636 个图像序列用于 1110 个身份的测试。测试集中的每个查询序列的超过
    1/3 帧包含遮挡。
- en: III-B Evaluation Metrics
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 评估指标
- en: The occluded person Re-ID evaluates the performance of a Re-ID system under
    the scenario of occlusion. Therefore, the settings of partial/occluded person
    Re-ID datasets are usually specially designed. In principle, the query images/videos
    for testing are all occluded samples. The evaluation focuses on whether the correct
    identities can be retrieved when only occluded queries are provided. To evaluate
    a Re-ID system, the Cumulative Matching Characteristics (CMC) curves and the mean
    Average Precision (mAP) are two widely used metrics.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 遮挡人 Re-ID 评估 Re-ID 系统在遮挡场景下的性能。因此，部分/遮挡人 Re-ID 数据集的设置通常是特别设计的。原则上，测试用的查询图像/视频都是遮挡样本。评估侧重于当仅提供遮挡查询时是否能检索到正确的身份。为了评估
    Re-ID 系统，累计匹配特性（CMC）曲线和均值平均精度（mAP）是两种广泛使用的指标。
- en: 'The CMC curves calculate the probability that a correct match appears in the
    top-$k$ ranked retrieval results, $k\in\left\{1,2,3,...\right\}$. Specifically,
    the top-$k$ accuracy of the query $i$ is calculated as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: CMC 曲线计算正确匹配出现在前-$k$ 排名检索结果中的概率，其中 $k\in\left\{1,2,3,...\right\}$。具体来说，查询 $i$
    的前-$k$ 准确率计算为：
- en: '|  | <math   alttext="{Acc_{k}^{i}}=\begin{cases}1,&amp;{\text{if the top-$k$
    ranked gallery samples}}\\ &amp;{\text{contain the sample(s) of query $i$}};\\'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="{Acc_{k}^{i}}=\begin{cases}1,&amp;{\text{如果前-$k$ 排名的图库样本}}\\
    &amp;{\text{包含查询 $i$ 的样本}};\\'
- en: 0,&amp;{\text{otherwise}}.\end{cases}" display="block"><semantics ><mrow ><mrow  ><mi
    mathsize="90%"  >A</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"
    >c</mi><mo lspace="0em" rspace="0em" >​</mo><msubsup ><mi mathsize="90%"  >c</mi><mi
    mathsize="90%"  >k</mi><mi mathsize="90%"  >i</mi></msubsup></mrow><mo mathsize="90%"  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="left"  ><mrow ><mn mathsize="90%"  >1</mn><mo mathsize="90%"  >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mtext mathsize="90%" >if the top-</mtext><mi mathsize="90%"  >k</mi><mtext
    mathsize="90%"  > ranked gallery samples</mtext></mrow></mtd></mtr><mtr  ><mtd
    columnalign="left"  ><mrow ><mrow ><mtext mathsize="90%" >contain the sample(s)
    of query </mtext><mi mathsize="90%"  >i</mi></mrow><mo mathsize="90%"  >;</mo></mrow></mtd></mtr><mtr
    ><mtd columnalign="left"  ><mrow ><mn mathsize="90%"  >0</mn><mo mathsize="90%"  >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mtext mathsize="90%" >otherwise</mtext><mo lspace="0em"
    mathsize="90%"  >.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci >𝐴</ci><ci  >𝑐</ci><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑐</ci><ci  >𝑘</ci></apply><ci >𝑖</ci></apply></apply><apply ><csymbol cd="latexml"
    >cases</csymbol><cn type="integer" >1</cn><ci  ><mrow ><mtext mathsize="90%" >if
    the top-</mtext><mi mathsize="90%"  >k</mi><mtext mathsize="90%"  > ranked gallery
    samples</mtext></mrow></ci><ci  ><mtext mathsize="90%"  >otherwise</mtext></ci><ci
    ><mrow ><mtext mathsize="90%" >contain the sample(s) of query </mtext><mi mathsize="90%"
    >i</mi></mrow></ci><cn type="integer"  >0</cn><ci ><mtext mathsize="90%" >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >{Acc_{k}^{i}}=\begin{cases}1,&{\text{if the top-$k$
    ranked gallery samples}}\\ &{\text{contain the sample(s) of query $i$}};\\ 0,&{\text{otherwise}}.\end{cases}</annotation></semantics></math>
    |  | (1) |
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 0,&amp;{\text{否则}}.\end{cases}" display="block"><semantics ><mrow ><mrow  ><mi
    mathsize="90%"  >A</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathsize="90%"
    >c</mi><mo lspace="0em" rspace="0em" >​</mo><msubsup ><mi mathsize="90%"  >c</mi><mi
    mathsize="90%"  >k</mi><mi mathsize="90%"  >i</mi></msubsup></mrow><mo mathsize="90%"  >=</mo><mrow
    ><mo  >{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd columnalign="left"  ><mrow ><mn mathsize="90%"  >1</mn><mo mathsize="90%"  >,</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mtext mathsize="90%" >如果排名前-</mtext><mi mathsize="90%"  >k</mi><mtext
    mathsize="90%" >的画廊样本</mtext></mrow></mtd></mtr><mtr  ><mtd columnalign="left"  ><mrow
    ><mrow ><mtext mathsize="90%" >包含查询样本 </mtext><mi mathsize="90%"  >i</mi></mrow><mo
    mathsize="90%"  >;</mo></mrow></mtd></mtr><mtr ><mtd columnalign="left"  ><mrow
    ><mn mathsize="90%"  >0</mn><mo mathsize="90%"  >,</mo></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mtext mathsize="90%" >否则</mtext><mo lspace="0em" mathsize="90%"  >.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply ><ci >𝐴</ci><ci  >𝑐</ci><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑐</ci><ci  >𝑘</ci></apply><ci >𝑖</ci></apply></apply><apply ><csymbol cd="latexml"
    >cases</csymbol><cn type="integer" >1</cn><ci  ><mrow ><mtext mathsize="90%" >如果排名前-</mtext><mi
    mathsize="90%"  >k</mi><mtext mathsize="90%"  >的画廊样本</mtext></mrow></ci><ci  ><mtext
    mathsize="90%"  >否则</mtext></ci><ci ><mrow ><mtext mathsize="90%" >包含查询样本 </mtext><mi
    mathsize="90%" >i</mi></mrow></ci><cn type="integer"  >0</cn><ci ><mtext mathsize="90%"
    >否则</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex"
    >{Acc_{k}^{i}}=\begin{cases}1,&{\text{如果排名前-$k$ 的画廊样本}}\\ &{\text{包含查询样本 $i$}};\\
    0,&{\text{否则}}.\end{cases}</annotation></semantics></math> |  | (1) |
- en: 'Supposing there are $N$ queries in the test set, the CMC-$k$ (a.k.a., the rank-$k$
    accuracy) that calculates the probability of the top-$k$ accuracy for all queries
    is computed as:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 假设测试集中有 $N$ 个查询，CMC-$k$（即 rank-$k$ 精度）计算所有查询的前-$k$ 精度的概率。
- en: '|  | $\text{CMC-}k=\frac{1}{N}\sum_{i=1}^{N}Acc_{k}^{i}$ |  | (2) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{CMC-}k=\frac{1}{N}\sum_{i=1}^{N}Acc_{k}^{i}$ |  | (2) |'
- en: Since only the first match is concerned in the calculation, the CMC curves are
    acceptable when there are only one ground truth for each query or when we care
    more about the ground truth match in the top positions of the rank list.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算中仅考虑第一个匹配，当每个查询只有一个真实值或我们更关注排名列表顶部位置的真实匹配时，CMC 曲线是可以接受的。
- en: 'The mAP measures the average retrieval performance that takes the order of
    all true matches in the ranked retrieval results into consideration. Specifically,
    the average precision (AP) of the query $i$ is calculated as:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: mAP 衡量了考虑所有真实匹配在排名检索结果中顺序的平均检索性能。具体来说，查询 $i$ 的平均精度（AP）计算如下：
- en: '|  | $\text{AP}_{i}=\frac{1}{M_{i}}\sum_{j=1}^{M_{i}}\frac{j}{Rank_{j}}$ |  |
    (3) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{AP}_{i}=\frac{1}{M_{i}}\sum_{j=1}^{M_{i}}\frac{j}{Rank_{j}}$ |  |
    (3) |'
- en: 'where $M_{i}$ denotes there are $M_{i}$ samples with identity $i$ in the gallery
    set and $Rank_{j}$ denotes the rank of the $j$-th ground truth in the retrieval
    gallery list for query $i$. Supposing there are $N$ queries in the test set, the
    mAP is computed as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M_{i}$ 表示在图库集中身份为 $i$ 的样本数，$Rank_{j}$ 表示查询 $i$ 在检索图库列表中的第 $j$ 个真实匹配的排名。假设测试集中有
    $N$ 个查询，则 mAP 的计算公式为：
- en: '|  | $\text{mAP}=\frac{1}{N}\sum_{i=1}^{N}\text{AP}_{i}$ |  | (4) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{mAP}=\frac{1}{N}\sum_{i=1}^{N}\text{AP}_{i}$ |  | (4) |'
- en: Since the order of all true matches in the ranked retrieval results participates
    in the calculation of mAP, the mAP measures the average retrieval performance
    and is suitable for the gallery with multiple true matches.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于排名检索结果中的所有真实匹配的顺序参与了 mAP 的计算，mAP 测量了平均检索性能，并适用于具有多个真实匹配的图库。
- en: On the whole, the mAP pays more attention to the ability of retrieval recall
    while the CMC curves focus on the ability of retrieving a true match in candidate
    lists with different sizes. Consequently, the CMC curves and the mAP always work
    together for the evaluation of a Re-ID system.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，mAP 更关注于检索召回能力，而 CMC 曲线则侧重于在不同大小的候选列表中检索真实匹配的能力。因此，CMC 曲线和 mAP 通常会一起用于
    Re-ID 系统的评估。
- en: '![Refer to caption](img/acd9cf0d13fa63a3884f4d4344c8ee04.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/acd9cf0d13fa63a3884f4d4344c8ee04.png)'
- en: 'Figure 4: Examples of the four issues caused by occlusion in real-world applications:
    (*a*) position misalignment, (*b*) scale misalignment, (*c*) noisy information,
    and (*d*) missing information.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：实际应用中由于遮挡引起的四个问题的示例：(*a*) 位置错位，(*b*) 尺寸错位，(*c*) 噪声信息，以及 (*d*) 信息缺失。
- en: IV Occluded Person Re-ID
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 遮挡人员 Re-ID
- en: 'Occluded person re-identification (Re-ID) aims at addressing the occlusion
    problem when retrieving the person of interest across multiple cameras. In real-world
    applications of person Re-ID, a person may be occluded by a variety of obstacles
    (e.g., cars, trees, streetlights, and other persons) and the surveillance system
    often fails to capture the holistic person. A practical person Re-ID system in
    video surveillance generally includes three stages: pedestrian detection, trajectory
    tracking, and person retrieval. The occlusion will affect the whole process and
    bring great challenges to the final Re-ID.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 遮挡人员再识别（Re-ID）旨在解决在多个摄像头中检索目标人物时的遮挡问题。在人员 Re-ID 的实际应用中，人物可能会被各种障碍物（如汽车、树木、路灯和其他人）遮挡，监控系统往往无法捕捉到完整的人物。一个实际的视频监控人员
    Re-ID 系统通常包括三个阶段：行人检测、轨迹跟踪和人物检索。遮挡会影响整个过程，并给最终的 Re-ID 带来巨大的挑战。
- en: 'In general, there are four issues to be considered when developing a solution
    for occluded person Re-ID. The first two issues are the position and the scale
    misalignments between partial and holistic images. This is mainly caused by the
    upstream data processing: the detected box of a partial person undergoes the same
    alignment processing as that of a holistic person to obtain a consistent input
    size, resulting in the position misalignment issue and the scale misalignment
    issue. The last two issues are the noisy information and the missing information
    caused by occlusion. In the detected boxes of occluded pedestrians, occlusion
    is inevitably included in whole or in part and the identity information of occluded
    regions is missing, resulting in the noisy information issue and the missing information
    issue. Each issue is shown in Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Deep
    Learning-based Occluded Person Re-identification: A Survey") and real-world examples
    of the four issues are presented in Fig. [4](#S3.F4 "Figure 4 ‣ III-B Evaluation
    Metrics ‣ III Datasets and Evaluations ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey").'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，在开发遮挡人员 Re-ID 解决方案时，需要考虑四个问题。前两个问题是部分图像和整体图像之间的位置和尺寸错位。这主要是由于上游数据处理造成的：部分人的检测框经过与整体人相同的对齐处理以获得一致的输入尺寸，导致了位置错位问题和尺寸错位问题。最后两个问题是由遮挡引起的噪声信息和信息缺失。在遮挡行人的检测框中，遮挡不可避免地部分或全部包含在内，遮挡区域的身份信息缺失，从而导致噪声信息问题和信息缺失问题。每个问题在图 [2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey") 中显示，四个问题的实际示例呈现在图 [4](#S3.F4 "Figure 4 ‣ III-B Evaluation Metrics ‣
    III Datasets and Evaluations ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey") 中。'
- en: 'In this section, we analyze occlusion-related person Re-ID methods from the
    perspective of above-mentioned four issues. Corresponding solutions for each issue
    are summarized following the taxonomy illustrated in Fig. [1](#S1.F1 "Figure 1
    ‣ I Introduction ‣ Deep Learning-based Occluded Person Re-identification: A Survey").
    It should be noticed that some methods simultaneously address more than one issue
    and these methods will be introduced multiple times from different perspectives
    accordingly.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从上述四个问题的角度分析了与遮挡相关的行人重识别方法。针对每个问题的相应解决方案按照图 [1](#S1.F1 "图 1 ‣ I 引言 ‣
    基于深度学习的遮挡行人重识别：综述")中所示的分类法进行了总结。需要注意的是，有些方法同时解决多个问题，这些方法将根据不同的角度多次介绍。
- en: IV-A Position Misalignment
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 位置错位
- en: 'Deep learning-based solutions to address the position misalignment issue caused
    by occlusion in person Re-ID can be roughly summarized into four categories: matching [[15](#bib.bib15),
    [38](#bib.bib38), [30](#bib.bib30), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)], auxiliary model for position [[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [42](#bib.bib42), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55)], additional supervision for position [[56](#bib.bib56),
    [57](#bib.bib57), [38](#bib.bib38), [58](#bib.bib58), [14](#bib.bib14), [59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61)], and attention mechanism for position [[32](#bib.bib32),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68)]. Firstly, the matching-based solutions take
    person Re-ID as a matching task and propose a variety of matching components,
    as well as the matching strategies, to address the position misalignment issue.
    Secondly, the auxiliary model-based solutions for person Re-ID rely on the position
    information provided by external models to boost performance. Thirdly, the additional
    supervision-based solutions for person Re-ID utilize extra information to guide
    the position-related learning process while being independent during the inference
    stage. Fourthly, the attention mechanism-based solutions for person Re-ID learn
    attention to address the position misalignment issue without any additional information.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的解决方案用于解决遮挡引起的行人重识别中的位置错位问题，大致可以总结为四类：匹配 [[15](#bib.bib15), [38](#bib.bib38),
    [30](#bib.bib30), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)], 位置辅助模型 [[36](#bib.bib36), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [42](#bib.bib42),
    [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55)], 位置附加监督 [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38),
    [58](#bib.bib58), [14](#bib.bib14), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61)],
    和位置注意机制 [[32](#bib.bib32), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68)]。首先，基于匹配的解决方案将行人重识别视为匹配任务，并提出了多种匹配组件以及匹配策略，以解决位置错位问题。其次，基于辅助模型的行人重识别解决方案依赖外部模型提供的位置信息来提升性能。第三，基于附加监督的行人重识别解决方案利用额外的信息来引导位置相关的学习过程，同时在推理阶段保持独立。第四，基于注意机制的行人重识别解决方案通过学习注意机制解决位置错位问题，无需任何额外信息。
- en: IV-A1 Matching
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 匹配
- en: 'The main points of a matching-based method can probably be summarized as the
    matching component and the matching strategy (see Fig. [5](#S4.F5 "Figure 5 ‣
    IV-A1 Matching ‣ IV-A Position Misalignment ‣ IV Occluded Person Re-ID ‣ Deep
    Learning-based Occluded Person Re-identification: A Survey")). Diverse definitions
    of the matching component, as well as the corresponding matching strategies, have
    been proposed for addressing the position misalignment issue. On the whole, matching-based
    methods can be further grouped into sliding window matching [[15](#bib.bib15)],
    shortest path matching [[38](#bib.bib38)], reconstruction-based matching [[15](#bib.bib15),
    [30](#bib.bib30)], denoising matching [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)],
    graph-based matching [[42](#bib.bib42), [43](#bib.bib43)], and set-based matching [[44](#bib.bib44)].'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基于匹配的方法的主要要点可以总结为匹配组件和匹配策略（见图[5](#S4.F5 "图 5 ‣ IV-A1 匹配 ‣ IV-A 位置对齐问题 ‣ IV 遮挡人物重新识别
    ‣ 基于深度学习的遮挡人物重新识别综述")）。为解决位置对齐问题，已经提出了多种匹配组件的定义以及相应的匹配策略。总体而言，基于匹配的方法可以进一步分为滑动窗口匹配[[15](#bib.bib15)]、最短路径匹配[[38](#bib.bib38)]、基于重建的匹配[[15](#bib.bib15),
    [30](#bib.bib30)]、去噪匹配[[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]、基于图的匹配[[42](#bib.bib42),
    [43](#bib.bib43)]和基于集合的匹配[[44](#bib.bib44)]。
- en: The sliding window matching [[15](#bib.bib15)] treats the partial probe image
    as a whole and slides it exhaustively over a full-body gallery image to match
    the most similar local region. The L1-norm distance between the partial image
    and its most similar match on a full-body image is employed for the measurement.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口匹配[[15](#bib.bib15)]将部分探测图像视为一个整体，并在全身画廊图像上进行穷举滑动，以匹配最相似的局部区域。部分图像与全身图像上最相似匹配的L1范数距离用于测量。
- en: The shortest path matching [[38](#bib.bib38)] performs the matching by calculating
    the shortest path between two sets of local features and uses the matched local
    features to compute the similarity, explicitly accomplishing the position alignment.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最短路径匹配[[38](#bib.bib38)]通过计算两组局部特征之间的最短路径来进行匹配，并使用匹配的局部特征来计算相似度，明确实现位置对齐。
- en: The reconstruction-based matching [[15](#bib.bib15), [30](#bib.bib30)] assumes
    the identity information in an occluded image is a subset of that in a non-occluded
    image and thus the partial image can be reconstructed in whole or in part from
    a holistic image of the same identity. The patch-level reconstruction-based matching [[15](#bib.bib15)]
    decomposes the partial and the full-body images into regular grid patches as matching
    components. Each probe patch is matched to a set of gallery patches by optimizing
    the gallery patch selection for self reconstruction. The block-level reconstruction-based
    matching [[30](#bib.bib30)] defines $c\times c$ pixels on a feature map as an
    independent block for matching. It is assumed that each block of a partial probe
    image can be reconstructed from the sparse linear combination of blocks of a full-body
    gallery image with the same identity.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 基于重建的匹配[[15](#bib.bib15), [30](#bib.bib30)]假设遮挡图像中的身份信息是非遮挡图像中身份信息的一个子集，因此可以从相同身份的整体图像中完全或部分重建部分图像。基于补丁的重建匹配[[15](#bib.bib15)]将部分和全身图像分解为规则的网格补丁作为匹配组件。通过优化画廊补丁选择以进行自我重建，将每个探测补丁与一组画廊补丁进行匹配。基于块的重建匹配[[30](#bib.bib30)]将特征图上的$c\times
    c$像素定义为一个独立的块进行匹配。假设部分探测图像的每个块可以从相同身份的全身画廊图像的块的稀疏线性组合中重建。
- en: '![Refer to caption](img/f0b555976102256e79671db004ef1f94.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f0b555976102256e79671db004ef1f94.png)'
- en: 'Figure 5: The diagram of matching-based methods: constructing local matching
    elements and designing matching strategies to address the position misalignment,
    scale misalignment, and noise information issues.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于匹配的方法的示意图：构建局部匹配元素并设计匹配策略，以解决位置对齐、尺度对齐和噪声信息问题。
- en: The denoising matching [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]
    proposes to focus on foreground visible human parts while discarding the noisy
    information during matching. Specifically, GASM [[39](#bib.bib39)] learns a saliency
    heatmap with the supervision of pose estimation and human segmentation to highlight
    foreground visible human parts. Guided by the saliency heatmap, the matching scores
    of spatial element-wise features in foreground visible human parts are assigned
    with large weights while that in background or occlusion regions are assigned
    with small weights, adaptively. Co-Attention [[40](#bib.bib40)] performs the matching
    between a partial and a holistic image under the guidance of body parsing masks.
    Particularly, the self-attention mechanism [[69](#bib.bib69)] is adopted in Co-Attention
    matching where the parsing mask of the partial image is viewed as the query, parsing
    masks and CNN features of the partial and the holistic images serve as the key
    and the value respectively. ASAN [[41](#bib.bib41)] proposes to replace the segmentation
    mask of holistic gallery images with the mask of the current target person image
    in each retrieval matching process. In this way, the feature extraction guided
    by ASAN is able to suppress the interference from useless parts.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪匹配[[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41)]提出在匹配过程中关注前景可见的人体部位，同时忽略噪声信息。具体而言，GASM[[39](#bib.bib39)]通过姿态估计和人体分割的监督来学习显著性热图，以突出前景可见的人体部位。在显著性热图的指导下，前景可见人体部位的空间元素特征的匹配分数被赋予较大权重，而背景或遮挡区域的匹配分数则被赋予较小权重，进行自适应调整。Co-Attention[[40](#bib.bib40)]在身体解析掩码的指导下执行部分图像和整体图像之间的匹配。特别地，Co-Attention匹配中采用了自注意机制[[69](#bib.bib69)]，其中部分图像的解析掩码被视为查询，部分和整体图像的解析掩码以及CNN特征分别作为键和值。ASAN[[41](#bib.bib41)]建议在每次检索匹配过程中，将整体库图像的分割掩码替换为当前目标人物图像的掩码。这样，ASAN引导的特征提取能够抑制来自无用部分的干扰。
- en: The graph-based matching [[42](#bib.bib42), [43](#bib.bib43)] formulates the
    occluded person re-identification as the graph matching problem. HOReID [[42](#bib.bib42)]
    employs the key-point heatmaps to extract the semantic local features of an image
    as nodes of a graph. The graph convolutional network (GCN) with learnable adjacent
    matrices is designed to pass messages between nodes for capturing the high-order
    relation information. To measure the similarity between two graphs, node features
    of the two graphs are further processed with relevant information extracted from
    each other for learning topology information. Both the high-order relation information
    extracted by GCN and the topology information learned from each other are employed
    to compute the final similarity for the two graphs. The multi-granular hypergraph
    matching [[43](#bib.bib43)] designs multiple hypergraphs with different spatial
    and temporal granularities to address the misalignment and occlusion issues for
    video-based person re-identification. Different from the conventional graphs,
    the hypergraphs [[70](#bib.bib70)] are able to model the high-order dependency
    involving multiple nodes and are more suitable for modeling the multi-granular
    correlations in a sequence.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的匹配[[42](#bib.bib42), [43](#bib.bib43)]将遮挡人物的再识别问题表述为图匹配问题。HOReID[[42](#bib.bib42)]使用关键点热图将图像的语义局部特征提取为图的节点。图卷积网络（GCN）通过可学习的邻接矩阵设计，用于在节点之间传递信息，以捕捉高阶关系信息。为了测量两个图之间的相似性，两个图的节点特征进一步处理，通过提取的相关信息学习拓扑信息。GCN提取的高阶关系信息和相互学习的拓扑信息被用于计算两个图的最终相似性。多粒度超图匹配[[43](#bib.bib43)]设计了多个具有不同空间和时间粒度的超图，以解决视频基础人物再识别中的对齐和遮挡问题。与传统图不同，超图[[70](#bib.bib70)]能够建模涉及多个节点的高阶依赖关系，更适合于建模序列中的多粒度相关性。
- en: The set-based matching takes occluded person Re-ID as a set matching task without
    requiring explicit spatial alignment. Specifically, MoS [[44](#bib.bib44)] employs
    a CNN backbone to capture diverse visual patterns along the channel dimension
    as matching elements. And the Jaccard similarity coefficient is introduced as
    the metric to compute the similarity between pattern sets of person images. The
    minimization and maximization are used to approximate the operations of intersection
    and union of sets, and the ratio of intersection over union is calculated to measure
    the similarity between two sets.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 基于集合的匹配将遮挡人物重识别视为集合匹配任务，无需明确的空间对齐。具体来说，MoS [[44](#bib.bib44)] 使用CNN骨干网捕捉沿通道维度的多样视觉模式作为匹配元素。引入Jaccard相似系数作为计算人物图像模式集合之间相似度的度量。通过最小化和最大化来近似集合的交集和并集操作，并计算交集与并集的比率来测量两个集合之间的相似度。
- en: IV-A2 Auxiliary Model for Position
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 位置的辅助模型
- en: To address the position misalignment issue caused by occlusion, some methods
    directly use the auxiliary information provided by external models for position
    alignment. According to the type of employed auxiliary models, these methods can
    be roughly summarized into pose-based [[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [42](#bib.bib42), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52)], parsing-based [[53](#bib.bib53)], and hybrid-based [[54](#bib.bib54),
    [55](#bib.bib55)] solutions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决因遮挡引起的位置信号错位问题，一些方法直接利用外部模型提供的辅助信息进行位置对齐。根据所使用的辅助模型类型，这些方法可以大致总结为基于姿态的 [[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [42](#bib.bib42), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52)], 基于解析的 [[53](#bib.bib53)]，以及基于混合的 [[54](#bib.bib54),
    [55](#bib.bib55)] 解决方案。
- en: The pose-based methods [[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [42](#bib.bib42), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52)] utilize the position information predicted
    by an external pose estimation model to address the position misalignment issue.
    PGFA [[36](#bib.bib36)], PDVM [[45](#bib.bib45)], and PMFB [[46](#bib.bib46)]
    generate heatmaps consisting of a 2D Gaussian centered on key-point locations
    to extract aligned pose features through a dot product with the CNN feature map.
    KBFM [[47](#bib.bib47)] utilizes shared visible keypoints between images to locate
    aligned rectangular regions for calculating the similarity. PVPM [[48](#bib.bib48)]
    uses the key-point heatmaps and the part affinity fields estimated by OpenPose [[71](#bib.bib71)]
    to generate part attention maps for extracting aligned local features. Similarly,
    PGMANet [[49](#bib.bib49)] generates heatmaps of key-points locations and employs
    them to calculate part attention masks on the CNN feature map. Based on the part
    features aggregated by part attention masks, PGMANet further computes the correlation
    among different part features to exploit the second-order information to enrich
    the feature extraction. HOReID [[42](#bib.bib42)] employs the key-point heatmaps
    to extract the semantic local features on a person image. The local features of
    an image are taken as nodes of a graph and HOReID designs a graph convolutional
    network with learnable adjacent matrices to exploit the more discriminative relation
    information among nodes for re-identification. Further, CTL [[50](#bib.bib50)]
    proposes to divide the human body into three granularities and uses key-point
    heatmaps to extract multi-scale part features as graph nodes. The cross-scale
    graph convolution and the 3D graph convolution are designed to capture the structural
    information and the hierarchical spatial-temporal dependencies for addressing
    the spatial misalignment issues in video person Re-ID. ACSAP [[51](#bib.bib51)]
    proposes to use the external pose information to guide the adversarial generation
    of aligned feature maps. PFD [[52](#bib.bib52)] generates local patch features
    and multiplies them with the processed keypoint heatmaps element-wisely to obtain
    the pose-guided features. Throught the measurement of set similarity, PFD performs
    the matching between local features and pose-guided features to disentangle the
    pose information from patch features for position alignment.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 基于姿态的方法[[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [42](#bib.bib42), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52)]利用外部姿态估计模型预测的位置来解决位置错位问题。PGFA [[36](#bib.bib36)], PDVM [[45](#bib.bib45)],
    和 PMFB [[46](#bib.bib46)] 生成包含以关键点位置为中心的 2D 高斯的热图，通过与 CNN 特征图的点积来提取对齐的姿态特征。KBFM
    [[47](#bib.bib47)] 利用图像间共享的可见关键点来定位对齐的矩形区域以计算相似性。PVPM [[48](#bib.bib48)] 使用关键点热图和由
    OpenPose [[71](#bib.bib71)] 估计的部件亲和场来生成部件注意力图，以提取对齐的局部特征。类似地，PGMANet [[49](#bib.bib49)]
    生成关键点位置的热图，并利用它们在 CNN 特征图上计算部件注意力掩模。基于部件注意力掩模聚合的部件特征，PGMANet 进一步计算不同部件特征之间的相关性，以利用二阶信息来丰富特征提取。HOReID
    [[42](#bib.bib42)] 使用关键点热图提取个人图像上的语义局部特征。图像的局部特征被视为图的节点，HOReID 设计了一个具有可学习邻接矩阵的图卷积网络，以利用节点之间的更多区分信息进行再识别。此外，CTL
    [[50](#bib.bib50)] 提议将人体分为三种粒度，并使用关键点热图提取多尺度部件特征作为图节点。交叉尺度图卷积和 3D 图卷积旨在捕捉结构信息和层次空间-时间依赖关系，以解决视频人物再识别中的空间错位问题。ACSAP
    [[51](#bib.bib51)] 提议使用外部姿态信息来指导对齐特征图的对抗生成。PFD [[52](#bib.bib52)] 生成局部补丁特征，并将其与处理后的关键点热图逐元素相乘，以获得姿态引导的特征。通过集相似性的度量，PFD
    在局部特征和姿态引导特征之间执行匹配，以从补丁特征中解开姿态信息以实现位置对齐。
- en: '![Refer to caption](img/02cfafecf5140d83d7e7a66d47032acd.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/02cfafecf5140d83d7e7a66d47032acd.png)'
- en: 'Figure 6: (*a*) The diagram of pose-based methods: keypoint coordinates for
    addressing the position misalignment and confidence scores for excluding the noisy
    information. (*b*) The diagram of segmentation-based methods: part masks for position
    locating and noisy information excluding.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: (*a*) 基于姿态的方法示意图：解决位置错位的关键点坐标和用于排除噪声信息的置信度分数。(*b*) 基于分割的方法示意图：用于定位位置的部件掩模和排除噪声信息。'
- en: The parsing-based methods directly take advantage of the parsing information
    generated from an external human parsing model to address the position misalignment
    issue. SPReID [[53](#bib.bib53)] trains an extra semantic parsing model on the
    human parsing dataset LIP [[72](#bib.bib72)] to predict probability maps associated
    to 5 pre-defined semantic regions of human body. These probability maps are then
    used to extract different semantic features through the weighted sum operation
    on the feature map generated by a modified Inception-V3 [[73](#bib.bib73)].
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 基于解析的方法直接利用从外部人工解析模型生成的解析信息来解决位置对齐问题。SPReID [[53](#bib.bib53)] 在人工解析数据集 LIP
    [[72](#bib.bib72)] 上训练了一个额外的语义解析模型，以预测与人体的5个预定义语义区域相关的概率图。这些概率图随后用于通过对由修改后的 Inception-V3
    [[73](#bib.bib73)] 生成的特征图进行加权求和操作来提取不同的语义特征。
- en: The hybrid-based methods [[54](#bib.bib54), [55](#bib.bib55)] employ two or
    more external models to provide auxiliary information for addressing the position
    misalignment issue. Specifically, SSP-ReID [[54](#bib.bib54)] exploits the capabilities
    of both clues, i.e., the saliency and the semantic parsing, to guide the CNN backbone
    to learn complementary representations for re-identification. The external off-the-shelf
    deep models [[72](#bib.bib72), [74](#bib.bib74)] are employed to generate the
    semantic parsing masks and the saliency masks. The element-wise product is applied
    between masks and a CNN feature map to obtain parsing features and saliency features
    for fusion. TSA [[55](#bib.bib55)] employs off-the-shelf HRNet [[75](#bib.bib75)]
    and DensePose [[76](#bib.bib76)] to provide extra key-points information and body
    parts information. Based on the key-points locations, the TSA divides the whole
    person into 5 regions and obtains region features on the CNN feature map through
    the soft region pooling. Based on the body part masks, the TSA extracts corresponding
    region features on the texture image produced by a texture generator. The region
    features guided by the key-points are then concatenated with the region features
    guided by the part masks accordingly to learn robust representations for re-identification.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 基于混合的方法 [[54](#bib.bib54), [55](#bib.bib55)] 使用两个或更多外部模型提供辅助信息以解决位置对齐问题。具体而言，SSP-ReID
    [[54](#bib.bib54)] 利用两种线索，即显著性和语义解析，来引导 CNN 主干学习用于重新识别的互补表示。外部现成的深度模型 [[72](#bib.bib72),
    [74](#bib.bib74)] 用于生成语义解析掩模和显著性掩模。在掩模与 CNN 特征图之间应用逐元素乘积，以获取解析特征和显著性特征进行融合。TSA
    [[55](#bib.bib55)] 使用现成的 HRNet [[75](#bib.bib75)] 和 DensePose [[76](#bib.bib76)]
    提供额外的关键点信息和身体部位信息。基于关键点位置，TSA 将整个人体划分为5个区域，并通过软区域池化在 CNN 特征图上获得区域特征。基于身体部位掩模，TSA
    从纹理生成器生成的纹理图像中提取相应的区域特征。由关键点引导的区域特征与由部位掩模引导的区域特征进行连接，以学习用于重新识别的鲁棒表示。
- en: IV-A3 Additional Supervision for Position
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 位置的额外监督
- en: Different from the auxiliary model-based solutions that rely on extra information
    in both training and test phases, the additional supervision-based solutions for
    position misalignment only use the extra information to guide the learning process
    and are independent during inference. According to the type of external information
    used, the additional supervision-based solutions for position misalignment can
    be coarsely summarized into pose-based [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38),
    [58](#bib.bib58), [14](#bib.bib14)], segmentation-based [[59](#bib.bib59), [60](#bib.bib60)],
    and hybrid-based [[61](#bib.bib61)] methods.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 与在训练和测试阶段都依赖额外信息的辅助模型解决方案不同，位置对齐的额外监督基于解决方案仅在学习过程中使用额外信息进行指导，并且在推理阶段是独立的。根据使用的外部信息类型，位置对齐的额外监督基于解决方案可以粗略地总结为基于姿态
    [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38), [58](#bib.bib58), [14](#bib.bib14)]、基于分割
    [[59](#bib.bib59), [60](#bib.bib60)] 和基于混合 [[61](#bib.bib61)] 的方法。
- en: The pose-based methods [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38),
    [58](#bib.bib58), [14](#bib.bib14)] employ external pose information to guide
    the learning process for alleviating the position misalignment problem. Specifically,
    AACN [[56](#bib.bib56)] and DAReID [[38](#bib.bib38)] learn part attention maps
    to locate and extract part features with the ground truth built from external
    pose information. DSAG [[57](#bib.bib57)] and PGFL-KD [[58](#bib.bib58)] use features
    located by pose information to guide the feature learning process. DSAG constructs
    a set of densely semantically aligned part images with the external pose information
    provided by DensePose [[76](#bib.bib76)]. Taking the semantically aligned part
    images as the input, DSAG serves as a regulator to guide the feature learning
    on original images through the carefully designed fusion and loss. Similarly,
    PGFL-KD uses external keypoint heatmaps to extract semantically aligned features.
    The aligned features are then employed to regularize the global feature learning
    through knowledge distillation and interaction-based training. Differently, the
    RFCNet [[14](#bib.bib14)] trains an adaptive partition unit supervised by external
    pose information to split the CNN feature map into different regions and extract
    region features for further processing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 基于姿态的方法 [[56](#bib.bib56), [57](#bib.bib57), [38](#bib.bib38), [58](#bib.bib58),
    [14](#bib.bib14)] 利用外部姿态信息来引导学习过程，以缓解位置对齐问题。具体来说，AACN [[56](#bib.bib56)] 和 DAReID [[38](#bib.bib38)]
    通过外部姿态信息构建的地面真实数据来学习部件注意力图，从而定位和提取部件特征。DSAG [[57](#bib.bib57)] 和 PGFL-KD [[58](#bib.bib58)]
    使用由姿态信息定位的特征来引导特征学习过程。DSAG 使用 DensePose [[76](#bib.bib76)] 提供的外部姿态信息构建一组密集语义对齐的部件图像。将语义对齐的部件图像作为输入，DSAG
    作为调节器，通过精心设计的融合和损失来引导原始图像上的特征学习。同样，PGFL-KD 使用外部关键点热图来提取语义对齐的特征。然后，这些对齐的特征用于通过知识蒸馏和基于互动的训练来规范全局特征学习。不同的是，RFCNet [[14](#bib.bib14)]
    训练一个由外部姿态信息监督的自适应分区单元，将 CNN 特征图分割成不同的区域，并提取区域特征以进行进一步处理。
- en: The segmentation-based methods [[59](#bib.bib59), [60](#bib.bib60)] utilize
    extra segmentation masks provided by the segmentation model, e.g., human parsing
    model or scene segmentation model, to guide the learning process for addressing
    the position misalignment problem. Specifically, MMGA [[59](#bib.bib59)] learns
    to generate the whole-body, upper-body, and lower-body attention maps with the
    parsing labels estimated by JPPNet [[77](#bib.bib77)]. The learned attention maps
    are then employed to extract global and local features for re-identification.
    HPNet [[60](#bib.bib60)] introduces human parsing as an auxiliary task and employs
    the parsing masks to extract part-level features for addressing the position misalignment
    issue. The person Re-ID and the human parsing are learned in a multi-task manner
    where the pseudo parsing label are predicted by the scene segmentation model [[78](#bib.bib78)]
    trained on the COCO DensePose [[76](#bib.bib76)] dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分割的方法 [[59](#bib.bib59), [60](#bib.bib60)] 利用由分割模型提供的额外分割掩码，例如，人类解析模型或场景分割模型，来引导学习过程，以解决位置对齐问题。具体来说，MMGA [[59](#bib.bib59)]
    学习生成全身、上半身和下半身的注意力图，这些注意力图使用 JPPNet [[77](#bib.bib77)] 估计的解析标签。然后，学习到的注意力图用于提取全局和局部特征以进行再识别。HPNet [[60](#bib.bib60)]
    引入人类解析作为辅助任务，并利用解析掩码提取部件级特征，以解决位置对齐问题。个人 Re-ID 和人类解析以多任务方式进行学习，其中伪解析标签由在 COCO
    DensePose [[76](#bib.bib76)] 数据集上训练的场景分割模型 [[78](#bib.bib78)] 预测。
- en: The hybrid-based methods utilize more than one type of external information
    to guide the learning process. Specifically, FGSA [[61](#bib.bib61)] mines fine-grained
    local features with the supervision of both the pose information and the attribute
    information to address the position misalignment issue. FGSA designs a pose resolve
    net (pre-trained on MSCOCO [[79](#bib.bib79)]) to provide part confidence maps
    and part affinity fields of the key parts. These part maps are then used to extract
    part features on a CNN feature map through compact bilinear pooling. Given the
    extracted part features, FGSA treats the attribute recognition as multiple classification
    tasks and trains an intermediate model for attribute classification along with
    the person Re-ID. In this way, the attribute classification tasks guide the pose
    resolve net and the CNN backbone to learn more discriminated local information
    for re-identification.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 基于混合的方法利用多种类型的外部信息来指导学习过程。具体来说，FGSA [[61](#bib.bib61)] 在姿态信息和属性信息的监督下挖掘细粒度的局部特征，以解决位置错位问题。FGSA
    设计了一个姿态解析网络（在 MSCOCO [[79](#bib.bib79)] 上预训练），提供关键部位的部件置信度图和部件亲和场。这些部件图用于通过紧凑双线性池化在
    CNN 特征图上提取部件特征。给定提取的部件特征，FGSA 将属性识别视为多分类任务，并训练一个中间模型进行属性分类以及人物重新识别。通过这种方式，属性分类任务指导姿态解析网络和
    CNN 主干学习更具区分性的局部信息以便重新识别。
- en: IV-A4 Attention Mechanism for Position
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A4 用于位置的注意机制
- en: 'The attention mechanism-based methods learn attention to address the position
    misalignment issue without any additional information (see Fig. [7](#S4.F7 "Figure
    7 ‣ IV-A4 Attention Mechanism for Position ‣ IV-A Position Misalignment ‣ IV Occluded
    Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification: A Survey")).
    According to the main point of the attention learning process, the attention mechanism-based
    solutions for position misalignment can be further grouped into cropping-based [[32](#bib.bib32)],
    clustering-based [[62](#bib.bib62)], self-supervised [[63](#bib.bib63), [64](#bib.bib64)],
    and constraint-based [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68)]
    methods. It should be noticed that some methods have also involved the attention
    mechanism but rely on external information provided by auxiliary models or use
    additional information for supervision. These methods are summarized in Auxiliary
    Model for position or Additional Supervision for Position accordingly.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意机制的方法通过学习注意力来解决位置错位问题，而不需要任何额外信息（见图 [7](#S4.F7 "图 7 ‣ IV-A4 用于位置的注意机制 ‣
    IV-A 位置错位 ‣ IV 遮挡人物重新识别 ‣ 基于深度学习的遮挡人物重新识别：综述")）。根据注意学习过程的主要观点，基于注意机制的位置错位解决方案可以进一步分为基于裁剪的 [[32](#bib.bib32)]、基于聚类的 [[62](#bib.bib62)]、自监督的 [[63](#bib.bib63),
    [64](#bib.bib64)] 和基于约束的 [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68)] 方法。需要注意的是，一些方法虽然也涉及到注意机制，但依赖于辅助模型提供的外部信息或使用额外的信息进行监督。这些方法分别总结在位置的辅助模型或位置的额外监督中。
- en: The cropping-based methods crop images into different local regions and learn
    attention to find the same local regions across different images for similarity
    calculation. Specifically, DPPR [[32](#bib.bib32)] crops 13 predefined partial
    regions on holistic images and designs an attention module conditioned on the
    partial probe image to assign the partial regions with larger attention weights
    if the same body parts are included.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 基于裁剪的方法将图像裁剪为不同的局部区域，并学习注意力以在不同图像中找到相同的局部区域进行相似性计算。具体而言，DPPR [[32](#bib.bib32)]
    在整体图像上裁剪 13 个预定义的部分区域，并设计一个条件于部分探测图像的注意力模块，以在包含相同身体部位的情况下为这些部分区域分配更大的注意力权重。
- en: '![Refer to caption](img/ad9b14b2ae74aeb4dbd62a98bd6626a9.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad9b14b2ae74aeb4dbd62a98bd6626a9.png)'
- en: 'Figure 7: Examples of technical routes in attention mechanism-based methods.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：基于注意机制方法的技术路线示例。
- en: The clustering-based methods generate pseudo-labels from clustering to supervise
    the attention learning. Specifically, ISP [[62](#bib.bib62)] designs the cascaded
    clustering on CNN feature maps to gradually generate pixel-level pseudo-labels
    of human parts for part attention learning. Based on the assumption that the foreground
    pixels have higher responses than the background ones, all pixels of a feature
    map are first clustered into foreground or background according to the activation.
    Secondly, the foreground pixels are further clustered into different human parts
    according to the similarities between pixels. The clustered pixel-level pseudo-labels
    of human parts are then employed to guide the part attention learning for extracting
    local part features to address the position misalignment issue.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 基于聚类的方法通过聚类生成伪标签，以监督注意力学习。具体而言，ISP [[62](#bib.bib62)] 在 CNN 特征图上设计级联聚类，逐步生成用于部分注意力学习的像素级伪标签。基于前景像素响应高于背景像素的假设，首先将特征图的所有像素根据激活值聚类为前景或背景。其次，根据像素之间的相似性，将前景像素进一步聚类为不同的人体部分。然后，利用聚类而得到的像素级人体部分伪标签来指导部分注意力学习，提取局部部分特征以解决位置不匹配问题。
- en: The self-supervised methods [[63](#bib.bib63), [64](#bib.bib64)] construct self-supervision
    to guide the attention learning. Given a holistic image, VPM [[63](#bib.bib63)]
    defines $m\times n$ rectangle regions and randomly crops a patch in which every
    pixel is assigned with the region label accordingly for self-supervision. VPM
    appends a region locator upon the extracted CNN feature map to discover different
    regions through the pixel-wise classification (attention) with the self-supervision
    constructed above. APN [[64](#bib.bib64)] randomly crops the holistic image into
    different partial types as the self-supervision for attention-based cropping type
    classification of a partial image. The holistic gallery images are cropped for
    person retrieval according to the predicted cropping type of the partial probe
    image to accomplish the position alignment.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督方法 [[63](#bib.bib63), [64](#bib.bib64)] 构建自我监督以指导注意力学习。给定一个整体图像，VPM [[63](#bib.bib63)]
    定义 $m\times n$ 的矩形区域，并随机裁剪一个 patch，其中每个像素根据相应的区域标签进行自我监督。VPM 在提取的 CNN 特征图上添加了区域定位器，通过上述构建的自我监督进行像素级分类（注意力），以发现不同的区域。APN
    [[64](#bib.bib64)] 将整体图像随机裁剪为不同的部分类型，作为部分图像的自监督任务，用于基于注意力的部分裁剪类型分类。根据部分探测图像的预测裁剪类型，对整体样本图像进行裁剪以实现位置对齐。
- en: The constraint-based methods [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68)] build constraints among different attention maps to help locate
    diverse body parts for addressing the position misalignment issue. The multiple
    spatial attentions in [[65](#bib.bib65)] employ a diversity regularization term
    on attention maps to ensure each attention focuses on different regions of the
    given image. SBPA [[66](#bib.bib66)] separates local attention maps through minimizing
    the L1-norm distance between the local attention and the masked global attention.
    PAT [[67](#bib.bib67)] maintains vectors of part prototypes to generate part-aware
    attention masks on contextual CNN features and designs the part diversity mechanism
    to help achieve diverse part discovery. Specifically, the part diversity mechanism
    minimizes the cosine similarity between every two part features to expand the
    discrepancy among different part features. MHSA-Net [[68](#bib.bib68)] proposes
    the feature diversity regularization term to encourage the diversity of local
    features captured by a multi-head self-attention mechanism. Specifically, in order
    to obtain diverse local features, the regularization term restricts the Gram matrix
    of local features to be close to an identity matrix.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 基于约束的方法 [[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68)]
    在不同的注意力图之间建立约束，以帮助定位不同的身体部位，以解决位置不匹配问题。[[65](#bib.bib65)] 中的多个空间注意力对注意力图应用多样性正则化项，确保每个注意力都专注于给定图像的不同区域。SBPA
    [[66](#bib.bib66)] 通过最小化局部注意力与被屏蔽的全局注意力之间的 L1-范数距离来分离局部注意力图。PAT [[67](#bib.bib67)]
    维护部分原型向量，以在上下文 CNN 特征上生成部分感知注意力掩码，并设计部分多样性机制来帮助实现多样的部分发现。具体而言，部分多样性机制将所有部分特征之间的余弦相似度最小化，扩大不同部分特征之间的差异。MHSA-Net
    [[68](#bib.bib68)] 提出特征多样性正则化项，鼓励多头自注意机制捕捉多样的局部特征。具体而言，为了获得多样的局部特征，正则化项限制局部特征的
    Gram 矩阵接近恒等矩阵。
- en: IV-B Scale Misalignment
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 尺度错位
- en: Deep learning-based methods [[80](#bib.bib80), [43](#bib.bib43), [30](#bib.bib30),
    [81](#bib.bib81), [66](#bib.bib66)] propose to construct pyramid features or multi-scale
    features for alleviating the scale misalignment issue in person Re-ID. Given a
    CNN feature map, the pyramid features are extracted from global to local while
    the multi-scale features maintain features of different receptive fields at the
    same position. On the whole, the core of both pyramid features and multi-scale
    features is to extract features of different scales to construct robust representations
    to scale variation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的方法 [[80](#bib.bib80), [43](#bib.bib43), [30](#bib.bib30), [81](#bib.bib81),
    [66](#bib.bib66)] 提出构建金字塔特征或多尺度特征以缓解人脸重识别中的尺度错位问题。给定一个 CNN 特征图，金字塔特征从全局到局部提取，而多尺度特征在同一位置保持不同感受野的特征。总体而言，金字塔特征和多尺度特征的核心都是提取不同尺度的特征，以构建对尺度变化具有鲁棒性的表示。
- en: IV-B1 Pyramid Features
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 金字塔特征
- en: The pyramid features [[80](#bib.bib80), [43](#bib.bib43)] are hierarchical and
    are extracted from global to local on the feature map. The pyramidal model in [[80](#bib.bib80)]
    horizontally slices the feature map into $n$ basic parts and builds corresponding
    branches for every $l\in\{1,2,...,n\}$ adjacent parts to obtain the pyramid features.
    MGH [[43](#bib.bib43)] hierarchically divides the feature map into $p\in\{1,2,4,8\}$
    horizontal strips and average pools each strip to obtain multi-granular spatial
    features.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 金字塔特征 [[80](#bib.bib80), [43](#bib.bib43)] 是层次化的，并且从特征图的全局到局部提取。[[80](#bib.bib80)]
    中的金字塔模型水平切分特征图为 $n$ 个基本部分，并为每个 $l\in\{1,2,...,n\}$ 相邻部分建立相应的分支以获取金字塔特征。MGH [[43](#bib.bib43)]
    将特征图层次化地划分为 $p\in\{1,2,4,8\}$ 个水平条带，并对每个条带进行平均池化以获得多粒度空间特征。
- en: IV-B2 Multi-scale Features
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 多尺度特征
- en: The multi-scale features [[30](#bib.bib30), [81](#bib.bib81), [66](#bib.bib66)]
    focus on restricted local regions and maintain features of different receptive
    fields at the same position. Specifically, DSR [[30](#bib.bib30)] average pools
    the square area of $s\times s$ pixels on a feature map to obtain the multi-scale
    block representations for alleviating the influence of scale mismatching, $s=\{1,2,3\}$.
    FPR [[81](#bib.bib81)] performs multiple max-pooling layers of different kernel
    sizes upon the feature map to capture diverse spatial features from small local
    regions to relatively large regions. SBPA [[66](#bib.bib66)] maintains features
    in different scales at each pixel through the scale-wise residual connection for
    addressing the scale misalignment issue.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度特征 [[30](#bib.bib30), [81](#bib.bib81), [66](#bib.bib66)] 聚焦于限制的局部区域，并在同一位置保持不同感受野的特征。具体而言，DSR [[30](#bib.bib30)]
    对特征图上的 $s\times s$ 像素的正方形区域进行平均池化，以获取多尺度块表示，从而缓解尺度错位的影响，$s=\{1,2,3\}$。FPR [[81](#bib.bib81)]
    对特征图执行不同核大小的多次最大池化层，以捕捉从小的局部区域到相对较大区域的多样空间特征。SBPA [[66](#bib.bib66)] 通过尺度方向的残差连接在每个像素处维持不同尺度的特征，以解决尺度错位问题。
- en: IV-C Position and Scale Misalignment
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 位置与尺度错位
- en: 'In the literature, there are some methods [[82](#bib.bib82), [83](#bib.bib83),
    [40](#bib.bib40), [51](#bib.bib51), [84](#bib.bib84), [64](#bib.bib64)] that simultaneously
    address the position and scale misalignment issues through the transformation
    of partial or holistic images (see Fig. [8](#S4.F8 "Figure 8 ‣ IV-C Position and
    Scale Misalignment ‣ IV Occluded Person Re-ID ‣ Deep Learning-based Occluded Person
    Re-identification: A Survey") (*a*)). These methods are specifically summarized
    in this subsection for clarity.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '文献中，有一些方法 [[82](#bib.bib82), [83](#bib.bib83), [40](#bib.bib40), [51](#bib.bib51),
    [84](#bib.bib84), [64](#bib.bib64)] 通过部分或整体图像的变换同时解决位置和尺度错位问题（见图 [8](#S4.F8 "Figure
    8 ‣ IV-C Position and Scale Misalignment ‣ IV Occluded Person Re-ID ‣ Deep Learning-based
    Occluded Person Re-identification: A Survey") (*a*)）。这些方法在本小节中进行了专门总结，以便于理解。'
- en: The partial image transformation [[82](#bib.bib82), [83](#bib.bib83), [40](#bib.bib40),
    [51](#bib.bib51)] aims to transform partial images to obtain the rectified results
    that are spatially aligned with holistic ones for re-identification. Specifically,
    APNet [[82](#bib.bib82)] designs a bounding box aligner (BBA) which predicts 4
    offset values (top, bottom, left, and right) to shift the detected bounding boxes
    to cover the estimated holistic body region. Without manual annotations, APNet
    is trained by constructing automatic data augmentation. PPCL [[83](#bib.bib83)]
    employs a gated transformation regression CNN module to predict the affine transformation
    coefficients between the partial and the holistic images for generating rectified
    partial images with proper scale and layout. The prediction of transformation
    coefficients is self-supervised by randomly cropping holistic images to simulate
    partial images with known transformation coefficients. The Image Rescaler (IR)
    in [[40](#bib.bib40)] predicts the 2D affine transformation parameters to transform
    a partial image into a desirable distortion-free image for addressing the spatial
    misalignment issue. Specifically, the partial image and the desirable distortion-free
    image are obtained by randomly cropping the holistic image and masking the uncropped
    regions, for self-supervision. Differently, ACSAP [[51](#bib.bib51)] designs a
    pose-guided generator that utilizes extra pose information of both partial and
    holistic images to guide the generation of aligned features for partial images.
    The pose-guided generator is adversarially learned by training a pose-guided discriminator,
    which aims to distinguish the authenticity of image features.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 部分图像变换 [[82](#bib.bib82), [83](#bib.bib83), [40](#bib.bib40), [51](#bib.bib51)]
    旨在对部分图像进行变换，以获得与整体图像空间对齐的校正结果以用于重新识别。具体而言，APNet [[82](#bib.bib82)] 设计了一种边界框对齐器（BBA），预测4个偏移值（上、下、左、右）以将检测到的边界框移动到覆盖估计的整体身体区域。APNet
    通过构建自动数据增强进行训练，无需手动注释。PPCL [[83](#bib.bib83)] 采用了一个门控变换回归CNN模块来预测部分图像与整体图像之间的仿射变换系数，从而生成具有适当尺度和布局的校正部分图像。变换系数的预测通过随机裁剪整体图像来模拟具有已知变换系数的部分图像进行自监督。[[40](#bib.bib40)]
    中的图像重标定器（IR）预测2D仿射变换参数，以将部分图像变换为理想的无畸变图像，以解决空间不对齐问题。具体而言，部分图像和理想的无畸变图像通过随机裁剪整体图像并遮蔽未裁剪区域来获得，用于自监督。不同的是，ACSAP [[51](#bib.bib51)]
    设计了一个姿态引导生成器，利用部分图像和整体图像的额外姿态信息来引导对齐特征的生成。姿态引导生成器通过训练一个姿态引导判别器进行对抗学习，旨在区分图像特征的真实性。
- en: '![Refer to caption](img/cad2655e9ac03bb02c21c0b60eb5c3a8.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cad2655e9ac03bb02c21c0b60eb5c3a8.png)'
- en: 'Figure 8: (*a*) The diagram of image transformation methods: addressing the
    position and scale misalignments simultaneously. (*b*) The diagram of attribute-based
    methods: associating the attribute annotations with person Re-ID.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：(*a*) 图像变换方法示意图：同时处理位置和尺度的不匹配。 (*b*) 基于属性的方法示意图：将属性注释与人员重新识别（Re-ID）关联起来。
- en: The holistic image transformation [[84](#bib.bib84), [64](#bib.bib64)] aims
    to find and transform the corresponding regions of holistic images to obtain the
    images that are spatially aligned with the query partial images for re-identification.
    Specifically, STNReID [[84](#bib.bib84)] utilizes the high-level CNN features
    of the partial and the holistic images to predict the 2D affine transformation
    parameters between them to transform the holistic image for matching with the
    partial image. In STNReID, the holistic images are randomly cropped to partial
    images for building the self-supervised training of 2D affine transformation parameters
    prediction. APN [[64](#bib.bib64)] predicts the cropping type of a partial probe
    image and crops the corresponding regions of holistic gallery images for person
    retrieval. In APN, the prediction of the cropping type is trained with the self-supervision
    constructed by randomly cropping the holistic image into different partial types.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 整体图像变换 [[84](#bib.bib84), [64](#bib.bib64)] 旨在查找和变换整体图像的对应区域，以获得与查询部分图像空间对齐的图像以进行重新识别。具体而言，STNReID [[84](#bib.bib84)]
    利用部分图像和整体图像的高级CNN特征来预测它们之间的2D仿射变换参数，以变换整体图像以匹配部分图像。在STNReID中，整体图像被随机裁剪为部分图像，以建立2D仿射变换参数预测的自监督训练。APN [[64](#bib.bib64)]
    预测部分探测图像的裁剪类型，并裁剪整体图库图像的对应区域以进行人员检索。在APN中，裁剪类型的预测通过随机裁剪整体图像为不同部分类型的自监督进行训练。
- en: IV-D Noisy Information
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 噪声信息
- en: 'The occlusion is inevitably included in the detected boxes of occluded pedestrians
    and therefore brings noisy information to person Re-ID (see Fig. [4](#S3.F4 "Figure
    4 ‣ III-B Evaluation Metrics ‣ III Datasets and Evaluations ‣ Deep Learning-based
    Occluded Person Re-identification: A Survey") (*c*)). This is especially problematic
    since the similar obstacles across different identities disturb the appearance-based
    similarity calculation, i.e., retrieval results easily contain negative images
    with similar obstacles [[85](#bib.bib85)]. Deep learning-based solutions for addressing
    the noisy information issue can be divided into auxiliary model for noise [[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48),
    [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87), [55](#bib.bib55), [40](#bib.bib40)],
    additional supervision for noise [[56](#bib.bib56), [38](#bib.bib38), [81](#bib.bib81),
    [60](#bib.bib60), [88](#bib.bib88), [39](#bib.bib39)], and attention mechanism
    for noise [[16](#bib.bib16), [63](#bib.bib63), [89](#bib.bib89), [82](#bib.bib82),
    [90](#bib.bib90), [91](#bib.bib91), [13](#bib.bib13), [92](#bib.bib92), [93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97)].'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 遮挡不可避免地被包含在遮挡行人的检测框中，因此为行人重识别（见图[4](#S3.F4 "图 4 ‣ III-B 评估指标 ‣ III 数据集与评估 ‣
    基于深度学习的遮挡行人重识别：综述") (*c*)）带来了噪声信息。这尤其成问题，因为不同身份的相似障碍物扰乱了基于外观的相似性计算，即检索结果容易包含具有相似障碍物的负面图像[[85](#bib.bib85)]。解决噪声信息问题的基于深度学习的方法可以分为噪声辅助模型[[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48),
    [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87), [55](#bib.bib55), [40](#bib.bib40)]、额外监督[[56](#bib.bib56),
    [38](#bib.bib38), [81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88), [39](#bib.bib39)]，以及噪声注意机制[[16](#bib.bib16),
    [63](#bib.bib63), [89](#bib.bib89), [82](#bib.bib82), [90](#bib.bib90), [91](#bib.bib91),
    [13](#bib.bib13), [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95),
    [96](#bib.bib96), [97](#bib.bib97)]。
- en: IV-D1 Auxiliary Model for Noise
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D1 噪声辅助模型
- en: The auxiliary model-based methods rely on external information provided by auxiliary
    models to help identify and suppress the noisy occlusion. According to the type
    of employed auxiliary models, these methods can be further divided into pose-based [[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48),
    [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87)] and parsing-based [[55](#bib.bib55),
    [40](#bib.bib40)].
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 基于辅助模型的方法依赖于辅助模型提供的外部信息，以帮助识别和抑制噪声遮挡。根据所使用的辅助模型类型，这些方法可以进一步分为基于姿态的[[36](#bib.bib36),
    [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48),
    [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87)]和基于解析的[[55](#bib.bib55),
    [40](#bib.bib40)]。
- en: The pose-based methods [[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46),
    [51](#bib.bib51), [47](#bib.bib47), [48](#bib.bib48), [86](#bib.bib86), [52](#bib.bib52),
    [87](#bib.bib87)] exploits pose landmarks predicted by external pose estimation
    models to disentangle the useful information from the occlusion noise. Specifically,
    PGFA [[36](#bib.bib36)], PDVM [[45](#bib.bib45)], and PMFB [[46](#bib.bib46)]
    preset a threshold to filter out occluded invisible pose landmarks based on their
    prediction confidence. The visible pose landmarks are used to generate heatmaps
    for extracting non-occluded pose features. These methods also construct different
    local part features on the CNN feature map and employ the confidence of pose estimation
    to determine whether the corresponding parts are occluded or not. Only the shared
    visible parts between probe and gallery images are used in these methods to compute
    the similarity, explicitly avoiding the disturbance from occlusion. Similarly,
    ACSAP [[51](#bib.bib51)] utilizes the confidence of pose estimation to decide
    the visibility of horizontally partitioned parts and assigns the shared visible
    parts with larger weights in similarity metrics. KBFM [[47](#bib.bib47)] builds
    rectangular regions based on the shared visible keypoints between probe and gallery
    images to extract features for measuring the similarity. Moreover, PMFB employs
    the pose embeddings generated from visible landmarks as gates to adaptively recalibrate
    channel-wise feature responses based on the visible body parts. Given part features
    extracted with external pose information, PVPM [[48](#bib.bib48)] utilizes the
    characteristic of part correspondence between images of the same identity to mine
    correspondence scores as pseudo-labels for training a visibility predictor that
    estimates whether a part suffers from the occlusion. Considering that the provided
    external pose information may be sparse or noisy, LKWS [[86](#bib.bib86)] proposes
    to discretize the pose information to obtain robust visibility label of horizontally
    divided body parts. Utilizing the label of visibility, LKWS trains a visibility
    discriminator to help suppress the influence of invisible horizontal parts. Specifically,
    the visibility of each horizontal part is voted by all keypoints within the part
    region based on their prediction confidence scores and the preset threshold. PFD [[52](#bib.bib52)]
    assigns the keypoint heatmap of higher confidence score than the preset threshold
    with $label=1$ and the keypoint heatmap of lower confidence score with $label=0$.
    With the assigned labels, PFD divides the view feature set into high-confidence
    and low-confidence keypoint view feature sets. The high-confidence keypoint view
    features are employed in the inference stage to explicitly match visible body
    parts and automatically separate occlusion features. Moreover, PFD designs a Pose-guided
    push loss that encourages the difference between human body parts (i.e., the high-confidence
    features) and non-human body parts (i.e., the low-confidence features) to help
    focus on human body parts and alleviate the interference of occlusion. Differently,
    STAL [[87](#bib.bib87)] uses external pose landmarks to slice the video into multiple
    spatial-temporal units. To down-weight the possible occluded units, STAL designs
    a joint spatial-temporal attention module to evaluate the quality scores of each
    unit with carefully designed loss functions.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 基于姿态的方法[[36](#bib.bib36), [45](#bib.bib45), [46](#bib.bib46), [51](#bib.bib51),
    [47](#bib.bib47), [48](#bib.bib48), [86](#bib.bib86), [52](#bib.bib52), [87](#bib.bib87)]
    利用外部姿态估计模型预测的姿态标志来从遮挡噪声中分离出有用的信息。具体而言，PGFA [[36](#bib.bib36)]、PDVM [[45](#bib.bib45)]
    和 PMFB [[46](#bib.bib46)] 预设阈值来基于预测置信度过滤掉被遮挡的不可见姿态标志。可见的姿态标志用于生成热图，以提取非遮挡的姿态特征。这些方法还在
    CNN 特征图上构建不同的局部部分特征，并利用姿态估计的置信度来确定相应部分是否被遮挡。这些方法仅使用探测图像和库图像之间共享的可见部分来计算相似性，明确避免了遮挡带来的干扰。类似地，ACSAP [[51](#bib.bib51)]
    利用姿态估计的置信度来决定水平划分部分的可见性，并在相似性度量中为共享的可见部分分配更大的权重。KBFM [[47](#bib.bib47)] 基于探测图像和库图像之间共享的可见关键点构建矩形区域，以提取特征来测量相似性。此外，PMFB
    利用从可见标志生成的姿态嵌入作为门控，自适应地重新校准基于可见身体部位的通道级特征响应。给定通过外部姿态信息提取的部件特征，PVPM [[48](#bib.bib48)]
    利用相同身份图像之间部件对应的特性，挖掘对应分数作为伪标签，用于训练一个可见性预测器，以估计部件是否受到遮挡。考虑到提供的外部姿态信息可能是稀疏或嘈杂的，LKWS [[86](#bib.bib86)]
    提出将姿态信息离散化，以获得水平分割身体部位的鲁棒可见性标签。利用可见性标签，LKWS 训练一个可见性鉴别器，以帮助抑制不可见水平部分的影响。具体而言，基于预测置信度分数和预设阈值，所有关键点对每个水平部分的可见性进行投票。PFD [[52](#bib.bib52)]
    将置信度分数高于预设阈值的关键点热图分配 $label=1$，将置信度分数低于预设阈值的关键点热图分配 $label=0$。根据分配的标签，PFD 将视图特征集分为高置信度和低置信度的关键点视图特征集。高置信度的关键点视图特征在推断阶段用于显式匹配可见身体部位，并自动分离遮挡特征。此外，PFD
    设计了一个姿态引导的推送损失，鼓励人体部位（即高置信度特征）和非人体部位（即低置信度特征）之间的差异，以帮助专注于人体部位并缓解遮挡的干扰。不同的是，STAL [[87](#bib.bib87)]
    使用外部姿态标志将视频切割成多个时空单元。为了降低可能的遮挡单元的权重，STAL 设计了一个联合时空注意力模块，通过精心设计的损失函数来评估每个单元的质量分数。
- en: The parsing-based methods [[55](#bib.bib55), [40](#bib.bib40)] employ parsing
    masks estimated by human parsing models to help suppress the noisy occlusion.
    Specifically, TSA [[55](#bib.bib55)] utilizes external parsing results estimated
    by DensePose [[76](#bib.bib76)] to provide the visible signal to guide the learning
    of visible regions, suppressing the invisible occluded regions. Co-Attention [[40](#bib.bib40)]
    employs parsing masks as the query in the self-attention mechanism to perform
    image matching on associated regions for alleviating the noisy information brought
    by occlusion.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基于解析的方法[[55](#bib.bib55), [40](#bib.bib40)]采用由人工解析模型估计的解析掩模来帮助抑制噪声遮挡。具体来说，TSA[[55](#bib.bib55)]利用DensePose[[76](#bib.bib76)]估计的外部解析结果提供可见信号，以指导对可见区域的学习，抑制不可见的遮挡区域。Co-Attention[[40](#bib.bib40)]将解析掩模作为自注意力机制中的查询，以在相关区域进行图像匹配，从而减轻遮挡带来的噪声信息。
- en: IV-D2 Additional Supervision for Noise
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D2 噪声的额外监督
- en: The additional supervision-based methods employ extra information to guide the
    learning of suppressing the occlusion noise while being independent during inference.
    According to the type of the extra information employed, the additional supervision-based
    solutions can be further summarized into pose-based [[56](#bib.bib56), [38](#bib.bib38)],
    segmentation-based [[81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88)], attribute-based [[41](#bib.bib41)],
    and hybrid-based [[39](#bib.bib39)] methods.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 额外监督基方法通过使用额外信息来指导抑制遮挡噪声的学习，同时在推理过程中保持独立。根据所使用的额外信息的类型，额外监督基方法可以进一步总结为基于姿态的[[56](#bib.bib56),
    [38](#bib.bib38)], 基于分割的[[81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88)],
    基于属性的[[41](#bib.bib41)]，和混合基的[[39](#bib.bib39)]方法。
- en: The pose-based methods [[56](#bib.bib56), [38](#bib.bib38)] utilize additional
    pose information to supervise the learning of excluding noisy occlusion. Specifically,
    AACN [[56](#bib.bib56)] uses external pose information to supervise the part attention
    learning. Based on the intensities of each part attention map, AACN computes part
    visibility scores to measure the occlusion extent of each body part. Similarly,
    with the ground truth built from external pose information, DAReID [[38](#bib.bib38)]
    learns to predict upper and lower body masks to extract non-occluded part features.
    Moreover, DAReID regards the heatmap of upper or lower features with large high-activation
    areas as reliable. The significance of the reliable regions is enhanced to suppress
    the occlusion noise.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 基于姿态的方法[[56](#bib.bib56), [38](#bib.bib38)]利用额外的姿态信息来监督排除噪声遮挡的学习。具体来说，AACN[[56](#bib.bib56)]使用外部姿态信息来监督部分注意力的学习。根据每个部分注意力图的强度，AACN计算部分可见性分数以衡量每个身体部位的遮挡程度。类似地，利用从外部姿态信息构建的真实情况，DAReID[[38](#bib.bib38)]学习预测上半身和下半身掩模，以提取未被遮挡的部件特征。此外，DAReID将具有大面积高激活区域的上半身或下半身特征热图视为可靠。通过增强可靠区域的显著性来抑制遮挡噪声。
- en: The segmentation-based methods [[81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88)]
    utilize extra segmentation masks provided by a segmentation model, e.g., the human
    parsing model or the scene segmentation model, to help address the noisy information
    issue. Specifically, FPR [[81](#bib.bib81)] employs the person mask obtained by
    CE2P [[98](#bib.bib98)] to supervise the generation of foreground probability
    maps, encouraging the feature extraction to concentrate more on clean human body
    parts to refine the similarity computation with less contamination from occlusion.
    HPNet [[60](#bib.bib60)] uses part labels provided by a COCO-trained human parsing
    model to learn human parsing and person re-identification in a multi-task manner.
    In HPNet, the predicted part probability maps are binarized with a threshold of
    0.5 to extract part-level features and determine the visibility of each part for
    alleviating the occlusion noise. SORN [[88](#bib.bib88)] trains a semantic branch
    with pseudo-labels predicted by external semantic segmentation model to generate
    foreground-background masks for extracting features from non-occluded areas.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分割的方法 [[81](#bib.bib81), [60](#bib.bib60), [88](#bib.bib88)] 利用由分割模型（如人类解析模型或场景分割模型）提供的额外分割掩码，以帮助解决噪声信息问题。具体来说，FPR [[81](#bib.bib81)]
    使用由CE2P [[98](#bib.bib98)] 获得的人物掩码来监督前景概率图的生成，鼓励特征提取更多集中在干净的人体部位，从而减少遮挡带来的污染，精炼相似度计算。HPNet [[60](#bib.bib60)]
    使用由COCO训练的人体解析模型提供的部件标签，以多任务的方式学习人体解析和人物再识别。在HPNet中，预测的部件概率图通过0.5的阈值二值化，以提取部件级特征并确定每个部件的可见性，从而减轻遮挡噪声。SORN [[88](#bib.bib88)]
    训练一个语义分支，使用外部语义分割模型预测的伪标签生成前景-背景掩码，从非遮挡区域提取特征。
- en: 'The attribute-based methods leverage semantic-level attribute annotations of
    person Re-ID datasets to help suppress the noisy information brought by occlusion
    (see Fig. [8](#S4.F8 "Figure 8 ‣ IV-C Position and Scale Misalignment ‣ IV Occluded
    Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification: A Survey")
    (*b*)). ASAN [[41](#bib.bib41)] employs the attribute information to guide the
    learning of occlusion-sensitive segmentation in a weakly supervised manner to
    extract non-occluded human body features.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '基于属性的方法利用人物Re-ID数据集的语义级属性注释，帮助抑制由遮挡带来的噪声信息（见图 [8](#S4.F8 "Figure 8 ‣ IV-C Position
    and Scale Misalignment ‣ IV Occluded Person Re-ID ‣ Deep Learning-based Occluded
    Person Re-identification: A Survey") (*b*)）。ASAN [[41](#bib.bib41)] 使用属性信息来指导弱监督下的遮挡敏感分割学习，以提取未遮挡的人体特征。'
- en: The hybrid-based methods employ more than one type of external information to
    guide the learning process for alleviating the noisy information issue. Specifically,
    GASM [[39](#bib.bib39)] trains a mask layer and a pose layer with the ground truth
    predicted by the semantic segmentation model PSPNet [[99](#bib.bib99)] and the
    pose estimation model CenterNet [[100](#bib.bib100)]. GASM then combines the mask
    heatmap predicted by the mask layer and the keypoint heatmaps estimated by the
    pose layer into a saliency map to extract salient features, explicitly excluding
    the noisy information brought by occlusion.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 基于混合的方法采用多种外部信息来指导学习过程，以减轻噪声信息问题。具体来说，GASM [[39](#bib.bib39)] 训练一个掩码层和一个姿态层，利用语义分割模型PSPNet [[99](#bib.bib99)]
    和姿态估计模型CenterNet [[100](#bib.bib100)] 预测的真实标签。GASM 然后将掩码层预测的掩码热图和姿态层估计的关键点热图组合成一个显著性图，以提取显著特征，明确排除由遮挡带来的噪声信息。
- en: IV-D3 Attention Mechanism for Noise
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D3 噪声的注意力机制
- en: The attention mechanism-based methods learn to generate the attention maps that
    assign smaller weights to occluded regions to address the noisy information issue,
    without the requirement of any extra information. According to the main idea of
    the attention learning process, the attention mechanism-based solutions for noise
    can be further grouped into data augmentation [[16](#bib.bib16), [63](#bib.bib63),
    [89](#bib.bib89), [82](#bib.bib82), [90](#bib.bib90), [91](#bib.bib91), [13](#bib.bib13),
    [92](#bib.bib92), [93](#bib.bib93)], query-guided [[94](#bib.bib94), [95](#bib.bib95)],
    drop-based [[96](#bib.bib96)], and relation-based [[97](#bib.bib97)] methods.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力机制的方法学习生成注意力图，将较小的权重分配给遮挡区域，以解决噪声信息问题，无需任何额外信息。根据注意力学习过程的主要思想，基于注意力机制的噪声解决方案可以进一步分为数据增强 [[16](#bib.bib16),
    [63](#bib.bib63), [89](#bib.bib89), [82](#bib.bib82), [90](#bib.bib90), [91](#bib.bib91),
    [13](#bib.bib13), [92](#bib.bib92), [93](#bib.bib93)], 查询引导 [[94](#bib.bib94),
    [95](#bib.bib95)], 丢弃基 [[96](#bib.bib96)] 和关系基 [[97](#bib.bib97)] 方法。
- en: The data augmentation methods [[16](#bib.bib16), [63](#bib.bib63), [89](#bib.bib89),
    [82](#bib.bib82), [90](#bib.bib90), [91](#bib.bib91), [13](#bib.bib13), [92](#bib.bib92),
    [93](#bib.bib93)] generate artificial occlusion to train the network to focus
    on clean body parts and exclude the noisy occlusion. Specifically, AFPB [[16](#bib.bib16)]
    constructs an occlusion simulator where a random patch from the background of
    source images is used as an occlusion to cover a part of holistic persons. AFPB
    designs the multi-task losses that force the network to simultaneously identify
    the person and classify whether the sample is from the occluded data distribution.
    VPM [[63](#bib.bib63)] pre-defines $m\times n$ rectangle regions on an image and
    randomly crops partial pedestrian images from the holistic ones where every pixel
    in partial images is assigned with the region label accordingly for self-supervision.
    In VPM, a region locator is designed to generate probability maps that infer the
    location of each region. Through the sum operation over each probability map,
    VPM obtains the region visibility scores to suppress the occluded noisy regions.
    RE [[89](#bib.bib89)] proposes the operation of random erasing, which randomly
    selects a rectangle region in an image and erases its pixels with random values,
    to generate images with various levels of occlusion for training the model to
    extract non-occluded discriminative identity information. APNet [[82](#bib.bib82)]
    trains a part identifier with the self-supervision built from data augmentation
    to identify visible strip parts. The visible part features are then selected for
    similarity computation while the invisible part features on occluded or noisy
    regions are discarded. IGOAS [[90](#bib.bib90)] designs an incremental generative
    occlusion block that randomly generates a uniform occlusion mask from small to
    large on images in a batch, training the model more robust to occlusion through
    gradually learning harder occlusion. With the synthesized occlusion data and their
    corresponding occlusion masks, IGOAS focuses more on foreground information by
    suppressing the response of generated occlusion regions to zero. SSGR [[91](#bib.bib91)]
    employs the random erasing and the batch-constant erasing, which equally divides
    images into horizontal strips and randomly erases the same strip in a sub-batch,
    to simulate occlusion for training the disentangled non-local (DNL [[101](#bib.bib101)])
    attention network. OAMN [[13](#bib.bib13)] designs a novel occlusion augmentation
    scheme that crops a rectangular patch of a randomly chosen training image and
    scales the patch onto four pre-defined locations of the target image, producing
    diverse and precisely labeled occlusion. With the supervision of labeled occlusion
    data, the OAMN learns to generate spatial attention maps which precisely capture
    body parts regardless of the occlusion. DRL-Net [[92](#bib.bib92)] utilizes the
    obstacles appearing in the train set to synthesize more diverse and realistic
    occluded samples to guide the contrast feature learning for mitigating the interference
    of occlusion noises. Apart from this, DRL-Net designs a transformer that simultaneously
    maintains the ID-relevant and ID-irrelevant queries for disentangled representation
    learning. Differently, FED [[93](#bib.bib93)] considers the occlusion from not
    only the non-pedestrian obstacles but also the non-target pedestrians for augmentation.
    To simulate reasonable non-pedestrian occlusions, FED manually crops the patches
    of backgrounds and occlusion objects from training images, and pastes them on
    pedestrian images with carefully designed augmentation process. To synthesize
    non-target pedestrian occlusions, FED maintains a memory bank of the feature centers
    of different identities. The memory bank is then employed to search $K$-nearest
    features of different identities for the current pedestrian. Finally, FED diffuses
    the pedestrian representations with these memorized features and generates non-target
    pedestrian occlusions at the feature level for training.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强方法[[16](#bib.bib16), [63](#bib.bib63), [89](#bib.bib89), [82](#bib.bib82),
    [90](#bib.bib90), [91](#bib.bib91), [13](#bib.bib13), [92](#bib.bib92), [93](#bib.bib93)]
    生成人工遮挡，以训练网络关注干净的身体部位并排除噪声遮挡。具体而言，AFPB [[16](#bib.bib16)] 构建了一个遮挡模拟器，其中源图像的背景随机区域作为遮挡，覆盖整体人物的一部分。AFPB
    设计了多任务损失，迫使网络同时识别人物并分类样本是否来自遮挡数据分布。VPM [[63](#bib.bib63)] 在图像上预定义 $m\times n$
    矩形区域，并随机从整体图像中裁剪部分行人图像，每个部分图像的每个像素都相应分配了区域标签以进行自我监督。在 VPM 中，设计了一个区域定位器来生成概率图，推断每个区域的位置。通过对每个概率图进行求和操作，VPM
    获得了区域可见性评分，以抑制遮挡的噪声区域。RE [[89](#bib.bib89)] 提出了随机擦除操作，即随机选择图像中的一个矩形区域，并用随机值擦除其像素，以生成具有不同遮挡级别的图像，以训练模型提取未遮挡的判别性身份信息。APNet
    [[82](#bib.bib82)] 训练一个部分识别器，通过数据增强构建的自我监督来识别可见的条形部件。然后选择可见部件特征进行相似性计算，而遮挡或噪声区域上的不可见部件特征则被丢弃。IGOAS
    [[90](#bib.bib90)] 设计了一个增量生成遮挡块，从小到大随机生成均匀的遮挡掩码，并在一批图像上训练模型更强健地应对遮挡，通过逐步学习更难的遮挡。通过合成的遮挡数据及其相应的遮挡掩码，IGOAS
    通过将生成的遮挡区域的响应抑制为零，更加关注前景信息。SSGR [[91](#bib.bib91)] 采用随机擦除和批次恒定擦除，将图像均分为水平条带，并随机擦除子批次中的相同条带，以模拟遮挡，训练解耦的非本地（DNL
    [[101](#bib.bib101)]) 注意力网络。OAMN [[13](#bib.bib13)] 设计了一种新型的遮挡增强方案，裁剪随机选择的训练图像的矩形补丁，并将补丁缩放到目标图像的四个预定义位置，产生多样且精确标记的遮挡。在标记的遮挡数据监督下，OAMN
    学习生成空间注意图，这些图精确捕捉身体部位，无论遮挡情况如何。DRL-Net [[92](#bib.bib92)] 利用训练集中出现的障碍物合成更多样化和真实的遮挡样本，以引导对比特征学习，以减轻遮挡噪声的干扰。此外，DRL-Net
    设计了一个变压器，同时维护与 ID 相关和与 ID 无关的查询，以进行解耦表示学习。不同的是，FED [[93](#bib.bib93)] 在增强时不仅考虑非行人障碍物，还考虑非目标行人。为了模拟合理的非行人遮挡，FED
    手动裁剪训练图像中的背景和遮挡物体补丁，并将其粘贴到行人图像上，经过精心设计的增强过程。为了合成非目标行人遮挡，FED 维护不同身份特征中心的记忆库。然后利用记忆库搜索当前行人的
    $K$ 个最近特征。最后，FED 使用这些记忆特征扩散行人表示，并在特征级别生成非目标行人遮挡进行训练。
- en: The query-guided methods [[94](#bib.bib94), [95](#bib.bib95)] aims to generate
    consistent attentions between query and gallery images for addressing the noisy
    information issue. Specifically, CASN [[94](#bib.bib94)] designs an attention-driven
    siamese learning architecture which enforces the attention consistency among images
    of the same identity to deal with viewpoint variations, occlusion, and background
    clutter. PISNet [[95](#bib.bib95)] focuses on the crowded occlusion in which the
    detected bounding boxes may involve multiple people and include distractive information.
    Under the guidance of the query image (single-person), PISNet calculates the inner
    product of the query and gallery features to formulate the pixel-wise query-guided
    attention to enhance the feature of the target in the gallery image (multi-person).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 查询引导的方法 [[94](#bib.bib94), [95](#bib.bib95)] 旨在生成查询图像与图库图像之间的一致注意力，以解决噪声信息问题。具体来说，CASN [[94](#bib.bib94)]
    设计了一种基于注意力的孪生学习架构，该架构强制同一身份的图像之间保持注意力一致，以处理视角变化、遮挡和背景杂乱的问题。PISNet [[95](#bib.bib95)]
    专注于拥挤遮挡的情况，其中检测到的边界框可能涉及多个人并包含干扰信息。在查询图像（单人）的指导下，PISNet 计算查询和图库特征的内积，以制定像素级查询引导的注意力，从而增强图库图像（多人）中目标的特征。
- en: The drop-based methods develop the training strategy based on dropping to guide
    the network to learn a more robust representation, alleviating the noisy information
    brought by occlusion. Specifically, CBDB-Net [[96](#bib.bib96)] uniformly partitions
    the feature map into strips and continuously drop each strip from top to bottom.
    Trained with drop-based incomplete features, the model is forced to learn a more
    robust person descriptor for re-identification.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 基于丢弃的方法开发了基于丢弃的训练策略，以引导网络学习更鲁棒的表示，从而减轻遮挡带来的噪声信息。具体来说，CBDB-Net [[96](#bib.bib96)]
    将特征图均匀划分为条带，并从上到下连续丢弃每个条带。通过基于丢弃的不完整特征进行训练，模型被迫学习更鲁棒的人物描述符以用于重新识别。
- en: The relation-based methods mine the relation among different regions to refine
    features, alleviating the interference of occlusion. Specifically, OCNet [[97](#bib.bib97)]
    predefines the global region, top region (i.e., $1/2$ top horizontal strip), bottom
    region (i.e., $1/2$ bottom horizontal strip), and center region (i.e., $1/3$ center
    vertical strip) on an image, and extracts four region features through the group
    convolution and the carefully designed attention mechanism. In OCNet, the relational
    adaptive module consisting of two fully connected shared layers is proposed to
    capture the relation between different region features. The relational weights
    are then used to refine region features to suppress the occluded or insignificant
    information.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 基于关系的方法挖掘不同区域之间的关系以细化特征，从而减轻遮挡的干扰。具体来说，OCNet [[97](#bib.bib97)] 在图像上预定义了全局区域、上部区域（即
    $1/2$ 上部水平条带）、下部区域（即 $1/2$ 下部水平条带）和中心区域（即 $1/3$ 中部垂直条带），并通过分组卷积和精心设计的注意力机制提取四个区域特征。在
    OCNet 中，提出了由两个全连接共享层组成的关系自适应模块，以捕捉不同区域特征之间的关系。然后使用关系权重细化区域特征，以抑制遮挡或无关的信息。
- en: IV-E Missing Information
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 缺失信息
- en: 'The lack of identity information in occluded regions results in the missing
    information issue for person Re-ID (see Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction
    ‣ Deep Learning-based Occluded Person Re-identification: A Survey") (*d*)). There
    are mainly two ways to recover the missing information in occluded regions: spatial
    recovery [[102](#bib.bib102), [14](#bib.bib14)] and temporal recovery [[103](#bib.bib103),
    [102](#bib.bib102), [14](#bib.bib14)].'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '遮挡区域中缺乏身份信息会导致缺失信息的问题（见图 [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Deep Learning-based
    Occluded Person Re-identification: A Survey") (*d*)）。在遮挡区域中恢复缺失信息主要有两种方式：空间恢复 [[102](#bib.bib102),
    [14](#bib.bib14)] 和时间恢复 [[103](#bib.bib103), [102](#bib.bib102), [14](#bib.bib14)]。'
- en: IV-E1 Spatial Recovery
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-E1 空间恢复
- en: The spatial recovery [[102](#bib.bib102), [14](#bib.bib14)] utilizes the spatial
    structure of a pedestrian image to infer the missing information. Specifically,
    VRSTC [[102](#bib.bib102)] designs an auto-encoder which takes a image masked
    with white pixels as input and generates the contents for the occluded white region.
    To improve the quality of generated contents of the occluded parts, VRSTC adopts
    a local and a global discriminator to adversarially judge the reality and the
    contextual consistency of the synthesized contents. RFCNet [[14](#bib.bib14)]
    exploits the long-range spatial contexts from non-occluded regions to predict
    the features of occluded regions, recovering the missing information at the feature
    level. Specifically, RFCNet estimates four keypoints to divide the feature map
    into 6 regions. In RFCNet, the encoder-decoder architecture is adopted, in which
    the encoder models the correlation between regions through clustering and the
    decoder utilizes the spatial correlation to recover occluded region features.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 空间恢复 [[102](#bib.bib102), [14](#bib.bib14)] 利用行人图像的空间结构来推断缺失的信息。具体来说，VRSTC [[102](#bib.bib102)]
    设计了一个自编码器，该自编码器以被白色像素遮挡的图像作为输入，并生成遮挡白色区域的内容。为了提高遮挡部分生成内容的质量，VRSTC 采用了局部和全局鉴别器来对合成内容的真实性和上下文一致性进行对抗性判断。RFCNet
    [[14](#bib.bib14)] 利用非遮挡区域的长范围空间上下文来预测遮挡区域的特征，在特征级别恢复缺失的信息。具体来说，RFCNet 估计四个关键点，将特征图划分为
    6 个区域。在 RFCNet 中，采用了编码器-解码器架构，其中编码器通过聚类建模区域间的相关性，而解码器利用空间相关性来恢复遮挡区域特征。
- en: IV-E2 Temporal Recovery
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-E2 时间恢复
- en: The temporal recovery [[103](#bib.bib103), [102](#bib.bib102), [14](#bib.bib14)]
    requires the continuous sequence of images (i.e., video-based person Re-ID) and
    utilizes the temporal information to recover the missing part in occluded regions.
    Assuming that the information at the same position in other frames can help recover
    the lost information in the current frame, the Refining Recurrent Unit (RRU) in [[103](#bib.bib103)]
    is designed to remove noise and recover missing activation regions by implicitly
    referring the appearance and motion information extracted from historical frames.
    VRSTC [[102](#bib.bib102)] proposes a differentiable temporal attention layer
    which employs the cosine similarity to determine where to attend from adjacent
    frames for recovering the contents of the occluded parts. RFCNet [[14](#bib.bib14)]
    employs a query-memory attention mechanism in which the current region is regarded
    as the query and the corresponding regions of remaining frames serve as the memory.
    The dot-product similarity [[104](#bib.bib104)] between the query and each item
    of the memory is employed for re-weighting all items in the memory to obtain the
    long-term temporal contexts to refine the query.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 时间恢复 [[103](#bib.bib103), [102](#bib.bib102), [14](#bib.bib14)] 需要连续的图像序列（即基于视频的人物重识别）并利用时间信息恢复遮挡区域的缺失部分。假设其他帧相同位置的信息可以帮助恢复当前帧中的丢失信息，[[103](#bib.bib103)]
    中的 Refining Recurrent Unit (RRU) 被设计用来通过隐式参考从历史帧提取的外观和运动信息来去除噪声并恢复缺失的激活区域。VRSTC
    [[102](#bib.bib102)] 提出了一个可微分的时间注意力层，该层利用余弦相似性来确定从相邻帧中关注的位置，以恢复遮挡部分的内容。RFCNet
    [[14](#bib.bib14)] 采用了查询-记忆注意力机制，其中当前区域被视为查询，剩余帧的相应区域作为记忆。通过查询与记忆中每项的点积相似性 [[104](#bib.bib104)]
    对所有记忆项进行重新加权，以获得长期时间上下文，从而优化查询。
- en: 'TABLE III: Performance Comparison on Partial-reid and Partial-ilids. S-ST is
    Short for Sing-ShoT, which Denotes that Each Identity Only Contains One Gallery
    Image in the Inference Stage.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 部分重识别和部分iLIDS的性能比较。S-ST 是 Single-ShoT 的缩写，表示每个身份在推理阶段仅包含一个图库图像。'
- en: '| Issues | Technical Routes | Methods | Publications | Partial-REID | Partial-iLIDS
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 技术路线 | 方法 | 发表论文 | 部分-REID | 部分-iLIDS |'
- en: '| Rank-1 | Rank-3 | S-ST | Rank-1 | Rank-3 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Rank-1 | Rank-3 | S-ST | Rank-1 | Rank-3 |'
- en: '| Position Misalignment | Matching | AMC+SWM [[15](#bib.bib15)] | ICCV2015
    | 37.3 | 46.0 | ✓ | 21.0 | 32.8 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Position Misalignment | Matching | AMC+SWM [[15](#bib.bib15)] | ICCV2015
    | 37.3 | 46.0 | ✓ | 21.0 | 32.8 |'
- en: '| DSR [[30](#bib.bib30)] | CVPR2018 | 50.7 | 70.0 | ✓ | 58.8 | 67.2 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| DSR [[30](#bib.bib30)] | CVPR2018 | 50.7 | 70.0 | ✓ | 58.8 | 67.2 |'
- en: '| DAReID [[38](#bib.bib38)] | KBS2021 | 68.1 | 79.5 | × | 76.7 | 85.3 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| DAReID [[38](#bib.bib38)] | KBS2021 | 68.1 | 79.5 | × | 76.7 | 85.3 |'
- en: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
- en: '| Co-Attention [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Co-Attention [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2
    |'
- en: '| ASAN [[41](#bib.bib41)] | TCSVT2021 | 86.8 | 93.5 | × | 81.7 | 88.3 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| ASAN [[41](#bib.bib41)] | TCSVT2021 | 86.8 | 93.5 | × | 81.7 | 88.3 |'
- en: '| Auxiliary Model for Position | PDVM [[45](#bib.bib45)] | PRL2020 | 43.3 |
    - | × | - | - |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 位置的辅助模型 | PDVM [[45](#bib.bib45)] | PRL2020 | 43.3 | - | × | - | - |'
- en: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 68.0 | 80.0 | ✓ | 69.1 | 80.9 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 68.0 | 80.0 | ✓ | 69.1 | 80.9 |'
- en: '| KBFM [[47](#bib.bib47)] | ICIP2020 | 69.7 | 82.2 | × | 64.1 | 73.9 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| KBFM [[47](#bib.bib47)] | ICIP2020 | 69.7 | 82.2 | × | 64.1 | 73.9 |'
- en: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 72.5 | 83.0 | × | 70.6 | 81.3 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 72.5 | 83.0 | × | 70.6 | 81.3 |'
- en: '| TSA [[55](#bib.bib55)] | ACM MM2020 | 72.7 | 85.2 | ✓ | 73.9 | 84.7 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| TSA [[55](#bib.bib55)] | ACM MM2020 | 72.7 | 85.2 | ✓ | 73.9 | 84.7 |'
- en: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
- en: '| PVPM [[48](#bib.bib48)] | CVPR2020 | 78.3 | - | ✓ | - | - |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| PVPM [[48](#bib.bib48)] | CVPR2020 | 78.3 | - | ✓ | - | - |'
- en: '| HOReID [[42](#bib.bib42)] | CVPR2020 | 85.3 | 91.0 | × | 72.6 | 86.4 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| HOReID [[42](#bib.bib42)] | CVPR2020 | 85.3 | 91.0 | × | 72.6 | 86.4 |'
- en: '| Additional Supervision for Position | DAReID [[38](#bib.bib38)] | KBS2021
    | 68.1 | 79.5 | × | 76.7 | 85.3 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 位置的额外监督 | DAReID [[38](#bib.bib38)] | KBS2021 | 68.1 | 79.5 | × | 76.7 |
    85.3 |'
- en: '| PGFL-KD [[58](#bib.bib58)] | MM2021 | 85.1 | 90.8 | × | 74.0 | 86.7 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| PGFL-KD [[58](#bib.bib58)] | MM2021 | 85.1 | 90.8 | × | 74.0 | 86.7 |'
- en: '| HPNet [[60](#bib.bib60)] | ICME2020 | 85.7 | - | ✓ | 68.9 | 80.7 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| HPNet [[60](#bib.bib60)] | ICME2020 | 85.7 | - | ✓ | 68.9 | 80.7 |'
- en: '| Attention Mechanism for Position | VPM [[63](#bib.bib63)] | CVPR2019 | 67.7
    | 81.9 | ✓ | 65.5 | 74.8 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 位置的注意力机制 | VPM [[63](#bib.bib63)] | CVPR2019 | 67.7 | 81.9 | ✓ | 65.5 | 74.8
    |'
- en: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
- en: '| MHSA-Net [[68](#bib.bib68)] | TNNLS2022 | 85.7 | 91.3 | × | 74.9 | 87.2 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| MHSA-Net [[68](#bib.bib68)] | TNNLS2022 | 85.7 | 91.3 | × | 74.9 | 87.2 |'
- en: '| PAT [[67](#bib.bib67)] | CVPR2021 | 88.0 | 92.3 | × | 76.5 | 88.2 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| PAT [[67](#bib.bib67)] | CVPR2021 | 88.0 | 92.3 | × | 76.5 | 88.2 |'
- en: '| Image Transformation | STNReID [[84](#bib.bib84)] | TMM2020 | 66.7 | 80.3
    | × | 54.6 | 71.3 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 图像变换 | STNReID [[84](#bib.bib84)] | TMM2020 | 66.7 | 80.3 | × | 54.6 | 71.3
    |'
- en: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
- en: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
- en: '| Co-Attention [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 协同注意力机制 [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2 |'
- en: '| PPCL [[83](#bib.bib83)] | CVPR2021 | 83.7 | 88.7 | × | 71.4 | 85.7 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| PPCL [[83](#bib.bib83)] | CVPR2021 | 83.7 | 88.7 | × | 71.4 | 85.7 |'
- en: '| Scale Misalignment | Multi-scale Features | DSR [[30](#bib.bib30)] | CVPR2018
    | 50.7 | 70.0 | ✓ | 58.8 | 67.2 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 尺度不对齐 | 多尺度特征 | DSR [[30](#bib.bib30)] | CVPR2018 | 50.7 | 70.0 | ✓ | 58.8
    | 67.2 |'
- en: '| FPR [[81](#bib.bib81)] | ICCV2019 | 81.0 | - | × | 68.1 | - |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| FPR [[81](#bib.bib81)] | ICCV2019 | 81.0 | - | × | 68.1 | - |'
- en: '| Image Transformation | STNReID [[84](#bib.bib84)] | TMM2020 | 66.7 | 80.3
    | × | 54.6 | 71.3 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 图像变换 | STNReID [[84](#bib.bib84)] | TMM2020 | 66.7 | 80.3 | × | 54.6 | 71.3
    |'
- en: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| APN [[64](#bib.bib64)] | ICPR2021 | 71.8 | 85.5 | × | 66.4 | 76.5 |'
- en: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
- en: '| Co-Attention [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 协同注意力机制 [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2 |'
- en: '| PPCL [[83](#bib.bib83)] | CVPR2021 | 83.7 | 88.7 | × | 71.4 | 85.7 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| PPCL [[83](#bib.bib83)] | CVPR2021 | 83.7 | 88.7 | × | 71.4 | 85.7 |'
- en: '| Noisy Information | Auxiliary Model for Noise | PDVM [[45](#bib.bib45)] |
    PRL2020 | 43.3 | - | × | - | - |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 噪声信息 | 噪声的辅助模型 | PDVM [[45](#bib.bib45)] | PRL2020 | 43.3 | - | × | - | -
    |'
- en: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 68.0 | 80.0 | ✓ | 69.1 | 80.9 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 68.0 | 80.0 | ✓ | 69.1 | 80.9 |'
- en: '| KBFM [[47](#bib.bib47)] | ICIP2020 | 69.7 | 82.2 | × | 64.1 | 73.9 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| KBFM [[47](#bib.bib47)] | ICIP2020 | 69.7 | 82.2 | × | 64.1 | 73.9 |'
- en: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 72.5 | 83.0 | × | 70.6 | 81.3 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 72.5 | 83.0 | × | 70.6 | 81.3 |'
- en: '| TSA [[55](#bib.bib55)] | ACM MM2020 | 72.7 | 85.2 | ✓ | 73.9 | 84.7 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| TSA [[55](#bib.bib55)] | ACM MM2020 | 72.7 | 85.2 | ✓ | 73.9 | 84.7 |'
- en: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| ACSAP [[51](#bib.bib51)] | ICIP2021 | 77.0 | 83.7 | × | 76.5 | 87.4 |'
- en: '| PVPM [[48](#bib.bib48)] | CVPR2020 | 78.3 | - | ✓ | - | - |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| PVPM [[48](#bib.bib48)] | CVPR2020 | 78.3 | - | ✓ | - | - |'
- en: '| Co-Attention [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 协同注意力机制 [[40](#bib.bib40)] | ICIP2021 | 83.0 | 90.3 | × | 73.1 | 83.2 |'
- en: '| LKWS [[86](#bib.bib86)] | ICCV2021 | 85.7 | 93.7 | × | 80.7 | 88.2 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| LKWS [[86](#bib.bib86)] | ICCV2021 | 85.7 | 93.7 | × | 80.7 | 88.2 |'
- en: '| Additional Supervision for Noise | DAReID [[38](#bib.bib38)] | KBS2021 |
    68.1 | 79.5 | × | 76.7 | 85.3 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 噪声的额外监督 | DAReID [[38](#bib.bib38)] | KBS2021 | 68.1 | 79.5 | × | 76.7 |
    85.3 |'
- en: '| FPR [[81](#bib.bib81)] | ICCV2019 | 81.0 | - | × | 68.1 | - |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| FPR [[81](#bib.bib81)] | ICCV2019 | 81.0 | - | × | 68.1 | - |'
- en: '| HPNet [[60](#bib.bib60)] | ICME2020 | 85.7 | - | ✓ | 68.9 | 80.7 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| HPNet [[60](#bib.bib60)] | ICME2020 | 85.7 | - | ✓ | 68.9 | 80.7 |'
- en: '| Attention Mechanism for Noise | CBDB-Net [[96](#bib.bib96)] | TCSVT2021 |
    66.7 | 78.3 | × | 68.4 | 81.5 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 噪声的注意力机制 | CBDB-Net [[96](#bib.bib96)] | TCSVT2021 | 66.7 | 78.3 | × | 68.4
    | 81.5 |'
- en: '| VPM [[63](#bib.bib63)] | CVPR2019 | 67.7 | 81.9 | ✓ | 65.5 | 74.8 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| VPM [[63](#bib.bib63)] | CVPR2019 | 67.7 | 81.9 | ✓ | 65.5 | 74.8 |'
- en: '| FED [[93](#bib.bib93)] | CVPR2022 | 83.1 | - | × | - | - |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| FED [[93](#bib.bib93)] | CVPR2022 | 83.1 | - | × | - | - |'
- en: '| OAMN [[13](#bib.bib13)] | ICCV2021 | 86.0 | - | × | 77.3 | - |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| OAMN [[13](#bib.bib13)] | ICCV2021 | 86.0 | - | × | 77.3 | - |'
- en: V Discussion
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 讨论
- en: In this section, we summarize and analyze the evaluation results of occluded
    person Re-ID methods based on the four issues discussed earlier. Following the
    proposed taxonomy, we aim to present potential factors that boost the performance
    of occluded person Re-ID to help facilitate future research.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结和分析了基于前述四个问题的遮挡行人重识别方法的评估结果。根据提出的分类法，我们旨在提出可能提升遮挡行人重识别性能的因素，以促进未来的研究。
- en: V-A Performance Comparison
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 性能比较
- en: 'We evaluate the occluded person re-identification methods on four widely-used
    image-based datasets, i.e., Partial-REID [[15](#bib.bib15)], Partial-iLIDS [[30](#bib.bib30)],
    Occluded-DukeMTMC (Occ-DukeMTMC) [[36](#bib.bib36)], and Occluded-REID (Occ-ReID) [[16](#bib.bib16)].
    Details about the four datasets are illustrated in Sec. [III-A](#S3.SS1 "III-A
    Datasets ‣ III Datasets and Evaluations ‣ Deep Learning-based Occluded Person
    Re-identification: A Survey"). Since the Partial-REID, Partial-iLIDS, and Occluded-REID
    are small, methods are generally trained on the training set of Market-1501 [[105](#bib.bib105)]
    and tested on these three datasets for evaluation. The performance comparisons
    on two partial person Re-ID datasets and two occluded person Re-ID datasets are
    summarized in Table [III](#S4.T3 "TABLE III ‣ IV-E2 Temporal Recovery ‣ IV-E Missing
    Information ‣ IV Occluded Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey") and Table [IV](#S5.T4 "TABLE IV ‣ V-B Future Directions ‣ V Discussion
    ‣ Deep Learning-based Occluded Person Re-identification: A Survey") respectively.
    From the two tables we obtain the following three observations:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在四个广泛使用的基于图像的数据集上评估了遮挡行人重识别方法，即 Partial-REID [[15](#bib.bib15)]、Partial-iLIDS [[30](#bib.bib30)]、Occluded-DukeMTMC
    (Occ-DukeMTMC) [[36](#bib.bib36)] 和 Occluded-REID (Occ-ReID) [[16](#bib.bib16)]。关于这四个数据集的详细信息在第[III-A](#S3.SS1
    "III-A 数据集 ‣ III 数据集和评估 ‣ 基于深度学习的遮挡行人重识别：综述")节中进行了说明。由于 Partial-REID、Partial-iLIDS
    和 Occluded-REID 较小，方法通常在 Market-1501 [[105](#bib.bib105)] 的训练集上进行训练，并在这三个数据集上进行测试和评估。两个部分行人重识别数据集和两个遮挡行人重识别数据集的性能比较分别总结在表[III](#S4.T3
    "TABLE III ‣ IV-E2 时间恢复 ‣ IV-E 缺失信息 ‣ IV 遮挡行人重识别 ‣ 基于深度学习的遮挡行人重识别：综述")和表[IV](#S5.T4
    "TABLE IV ‣ V-B 未来方向 ‣ V 讨论 ‣ 基于深度学习的遮挡行人重识别：综述")中。从这两个表中我们得出以下三个观察结果：
- en: '1). For addressing the position misalignment issue and the noisy information
    issue, effective solutions are rich and diverse. To be specific, firstly, there
    is not a dominant technical route to accomplish the position alignment: On Partial-ReID,
    attention mechanism-based method PAT [[67](#bib.bib67)] has achieved the top rank-1
    accuracy; On Partial-iLIDS, matching-based method ASAN [[41](#bib.bib41)] has
    reached the best rank-1 and rank-3 accuracy; On Occluded-DukeMTMC, auxiliary model-based
    method PFD [[52](#bib.bib52)] has achieved the top rank-1 accuracy and the best
    mAP; On Occluded-ReID, additional supervision-based method HPNet [[60](#bib.bib60)]
    has obtained the best rank-1 accuracy. Secondly, the most promising technical
    route to suppress the noisy occlusion is difficult to judge: On Partial-ReID,
    the rank-1 accuracies of three technical routes, i.e., auxiliary model, additional
    mechanism, and attention mechanism for noise, are comparable; On Partial-iLIDS
    and Occluded-DukeMTMC, auxiliary model-based methods LKWS [[86](#bib.bib86)] and
    PFD [[52](#bib.bib52)] have reached the best rank-1 accuracy accordingly; On Occluded-ReID,
    additional supervision-based method HPNet [[60](#bib.bib60)] has achieved the
    top rank-1 accuracy. Although neither the position misalignment issue nor the
    noisy information issue has a dominant technical route, the advantages and disadvantages
    of different technical routes can be summarized (see Sec.[V-B](#S5.SS2 "V-B Future
    Directions ‣ V Discussion ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey")).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '1). 针对位置不对齐问题和噪声信息问题，有效的解决方案丰富多样。具体来说，首先，没有主导的技术路线来实现位置对齐：在Partial-ReID中，基于注意力机制的方法PAT
    [[67](#bib.bib67)] 达到了最佳的rank-1准确率；在Partial-iLIDS中，基于匹配的方法ASAN [[41](#bib.bib41)]
    达到了最佳的rank-1和rank-3准确率；在Occluded-DukeMTMC中，基于辅助模型的方法PFD [[52](#bib.bib52)] 实现了最佳的rank-1准确率和最佳的mAP；在Occluded-ReID中，基于额外监督的方法HPNet
    [[60](#bib.bib60)] 获得了最佳的rank-1准确率。其次，压制噪声遮挡的最有前景的技术路线难以判断：在Partial-ReID中，三种技术路线，即辅助模型、额外机制和噪声注意力机制的rank-1准确率相当；在Partial-iLIDS和Occluded-DukeMTMC中，基于辅助模型的方法LKWS
    [[86](#bib.bib86)] 和PFD [[52](#bib.bib52)] 分别达到了最佳的rank-1准确率；在Occluded-ReID中，基于额外监督的方法HPNet
    [[60](#bib.bib60)] 达到了最佳的rank-1准确率。虽然位置不对齐问题和噪声信息问题都没有主导的技术路线，但不同技术路线的优缺点可以总结（见Sec.[V-B](#S5.SS2
    "V-B Future Directions ‣ V Discussion ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey")）。'
- en: 2). The scale misalignment issue and the missing information issue have drawn
    less attention in existing methods. Compared with the methods for addressing the
    position misalignment issue or the noisy information issue, the number of methods
    intended for the scale misalignment issue or the missing information issue is
    significantly smaller.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 2). 在现有方法中，尺度不对齐问题和信息缺失问题引起的关注较少。与解决位置不对齐问题或噪声信息问题的方法相比，旨在解决尺度不对齐问题或信息缺失问题的方法数量明显较少。
- en: 3). There are a large number of methods that have considered more than one issue
    at the same time. For instance, PPCL [[83](#bib.bib83)] addresses the position
    and scale misalignment issues; PFD [[52](#bib.bib52)] focuses on the position
    misalignment and the noisy information issues; Co-Attention [[40](#bib.bib40)]
    takes the position misalignment, the scale misalignment, and the noisy information
    issues into consideration. However, there are none of the methods that have considered
    all the four issues caused by occlusion. The in-depth analysis of issues and solutions
    in this survey aims to fill this gap and help develop a more comprehensive solution
    for occluded person Re-ID.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 3). 有大量方法同时考虑了多个问题。例如，PPCL [[83](#bib.bib83)] 解决了位置和尺度不对齐问题；PFD [[52](#bib.bib52)]
    专注于位置不对齐和噪声信息问题；Co-Attention [[40](#bib.bib40)] 考虑了位置不对齐、尺度不对齐和噪声信息问题。然而，尚无方法同时考虑由遮挡引起的所有四个问题。本调查对这些问题及其解决方案的深入分析旨在填补这一空白，并帮助开发更全面的遮挡行人再识别解决方案。
- en: V-B Future Directions
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 未来方向
- en: 'As shown in Table [III](#S4.T3 "TABLE III ‣ IV-E2 Temporal Recovery ‣ IV-E
    Missing Information ‣ IV Occluded Person Re-ID ‣ Deep Learning-based Occluded
    Person Re-identification: A Survey") and Table [IV](#S5.T4 "TABLE IV ‣ V-B Future
    Directions ‣ V Discussion ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey"), there have been consistent improvements in different technical routes
    for addressing different issues over the past few years. Based on the analysis
    of issues and solutions, the following insights can be drawn for the future research
    of occluded person Re-ID.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格[III](#S4.T3 "TABLE III ‣ IV-E2 Temporal Recovery ‣ IV-E Missing Information
    ‣ IV Occluded Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey")和表格[IV](#S5.T4 "TABLE IV ‣ V-B Future Directions ‣ V Discussion ‣ Deep
    Learning-based Occluded Person Re-identification: A Survey")所示，过去几年中在解决不同问题的不同技术路线方面取得了一致的进展。根据对问题和解决方案的分析，可以得出以下关于遮挡人
    Re-ID 未来研究的见解。'
- en: From the perspective of four significant issues caused by occlusion, the position
    misalignment and the noisy information issues have been widely studied while the
    scale misalignment and the missing information issues are rarely considered in
    existing methods. With the four issues summarized and analyzed in this survey,
    the deeper understanding of occluded person Re-ID can be obtained to contribute
    a more comprehensive solution and help inspire new ideas in the filed.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 从遮挡造成的四个主要问题的角度来看，位置错位和噪声信息问题已被广泛研究，而尺度错位和缺失信息问题在现有方法中较少考虑。通过对这四个问题进行总结和分析，可以更深入地理解遮挡人
    Re-ID，进而贡献更全面的解决方案并激发该领域的新思路。
- en: From the perspective of promising technical routes, it remains an open question
    since the evaluation results of state-of-the-art methods in different technical
    routes are comparable. In spite of this, we analyze and summarize the advantages
    and disadcantages of different technical routes as follows to help boost future
    research.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 从有前景的技术路线角度来看，由于不同技术路线的最先进方法的评估结果具有可比性，尚无明确答案。尽管如此，我们还是分析并总结了不同技术路线的优缺点，以帮助推动未来的研究。
- en: '1). *Matching.* The well-designed matching components and matching strategies
    greatly improves the performance of occluded person Re-ID. Local and scalable
    matching components with the corresponding matching strategy can help address
    the position misalignment, scale misalignment, and noisy information issues. Moreover,
    it can be easily integrated with other technical routes, e.g., the repeatedly
    reported methods Co-Attention [[40](#bib.bib40)] and ASAN [[41](#bib.bib41)] in
    Table [III](#S4.T3 "TABLE III ‣ IV-E2 Temporal Recovery ‣ IV-E Missing Information
    ‣ IV Occluded Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey").'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '1). *匹配.* 设计良好的匹配组件和匹配策略极大地提高了遮挡人 Re-ID 的性能。具有相应匹配策略的局部和可扩展匹配组件可以帮助解决位置错位、尺度错位和噪声信息问题。此外，它可以与其他技术路线轻松集成，例如表格[III](#S4.T3
    "TABLE III ‣ IV-E2 Temporal Recovery ‣ IV-E Missing Information ‣ IV Occluded
    Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification: A Survey")中反复报告的方法
    Co-Attention [[40](#bib.bib40)] 和 ASAN [[41](#bib.bib41)]。'
- en: '2). *Auxiliary Model and Additional Supervision.* In general, there are mainly
    three types of extra information employed for occluded person Re-ID: poses, segments,
    and attributes. The position information, as well as their estimation confidence,
    provided by pose estimation or segmentation are used to help address the position
    misalignment and the noisy information issues respectively (see Fig. [6](#S4.F6
    "Figure 6 ‣ IV-A2 Auxiliary Model for Position ‣ IV-A Position Misalignment ‣
    IV Occluded Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey")). And the extra attribute information provided by person Re-ID datasets
    is generally utilized to formulate an extra task to help alleviate the issues
    brought by occlusion. The two technical routes bring a lot of benefits and convenience
    while they are dependent on the extra labels or external models.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '2). *辅助模型和额外监督.* 通常，用于遮挡人 Re-ID 的额外信息主要有三种类型：姿态、分割和属性。姿态估计或分割提供的位置信息及其估计置信度分别用于帮助解决位置错位和噪声信息问题（见图[6](#S4.F6
    "Figure 6 ‣ IV-A2 Auxiliary Model for Position ‣ IV-A Position Misalignment ‣
    IV Occluded Person Re-ID ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey")）。此外，遮挡人 Re-ID 数据集提供的额外属性信息通常用于制定额外任务，以帮助缓解遮挡带来的问题。这两种技术路线带来了许多好处和便利，但它们依赖于额外的标签或外部模型。'
- en: '3). *Attention Mechanism.* The attention mechanism has been widely studied
    in existing methods for its huge potential and flexibility. In recent three years,
    the methods [[106](#bib.bib106), [40](#bib.bib40), [92](#bib.bib92), [68](#bib.bib68),
    [93](#bib.bib93)] introduce the self-attention (Transformer) to occluded person
    Re-ID have made remarkable improvements on public datasets. Similar to matching-based
    technical route, the attention mechanism can also be integrated with other technical
    routes, e.g., APN [[64](#bib.bib64)] in Table [IV](#S5.T4 "TABLE IV ‣ V-B Future
    Directions ‣ V Discussion ‣ Deep Learning-based Occluded Person Re-identification:
    A Survey").'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '3). *注意力机制。* 注意力机制在现有方法中因其巨大潜力和灵活性而被广泛研究。在最近三年中，方法 [[106](#bib.bib106), [40](#bib.bib40),
    [92](#bib.bib92), [68](#bib.bib68), [93](#bib.bib93)] 引入了自注意力（Transformer），在公共数据集上取得了显著进展。类似于基于匹配的技术路线，注意力机制也可以与其他技术路线集成，例如表 [IV](#S5.T4
    "TABLE IV ‣ V-B Future Directions ‣ V Discussion ‣ Deep Learning-based Occluded
    Person Re-identification: A Survey")中的 APN [[64](#bib.bib64)]。'
- en: '4). *Image Transformation.* The partial (holistic) image is transformed to
    obtain the image of consistent contents with the holistic (partial) image, addressing
    the position and the scale misalignment issues simultaneously (see Fig. [8](#S4.F8
    "Figure 8 ‣ IV-C Position and Scale Misalignment ‣ IV Occluded Person Re-ID ‣
    Deep Learning-based Occluded Person Re-identification: A Survey") (*a*)). This
    technical route does make sense and is close to the ideal process while it requires
    more computation costs for the conditional image transformation in the inference
    stage. Furthermore, the image transformation has not achieved a satisfying result
    in the current stage and the performance of this technical route is a little bit
    lower than others.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '4). *图像变换。* 部分（整体）图像被转换，以获得与整体（部分）图像内容一致的图像，同时解决位置和尺度不对齐问题（见图 [8](#S4.F8 "Figure
    8 ‣ IV-C Position and Scale Misalignment ‣ IV Occluded Person Re-ID ‣ Deep Learning-based
    Occluded Person Re-identification: A Survey") (*a*)）。这种技术路线确实有意义且接近理想过程，但它在推理阶段需要更多的计算成本。此外，当前阶段图像变换尚未取得令人满意的结果，该技术路线的性能略低于其他方法。'
- en: 'TABLE IV: Performance Comparison on Occ-dukemtmc and Occ-reid.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：Occ-dukemtmc 和 Occ-reid 的性能比较。
- en: '| Issues | Technical Routes | Methods | Publications | Occ-DukeMTMC | Occ-ReID
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 技术路线 | 方法 | 出版物 | Occ-DukeMTMC | Occ-ReID |'
- en: '| Rank-1 | mAP | Rank-1 | mAP |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| Rank-1 | mAP | Rank-1 | mAP |'
- en: '| Position Misalignment | Matching | AMC+SWM [[15](#bib.bib15)] | ICCV2015
    | - | - | 31.1 | 27.3 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 位置不对齐 | 匹配 | AMC+SWM [[15](#bib.bib15)] | ICCV2015 | - | - | 31.1 | 27.3
    |'
- en: '| GASM [[39](#bib.bib39)] | ECCV2020 | - | - | 74.5 | 65.6 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| GASM [[39](#bib.bib39)] | ECCV2020 | - | - | 74.5 | 65.6 |'
- en: '| DSR [[30](#bib.bib30)] | CVPR2018 | 40.8 | 30.4 | 72.8 | 62.8 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| DSR [[30](#bib.bib30)] | CVPR2018 | 40.8 | 30.4 | 72.8 | 62.8 |'
- en: '| HOReID [[42](#bib.bib42)] | CVPR2020 | 55.1 | 43.8 | 80.3 | 70.2 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| HOReID [[42](#bib.bib42)] | CVPR2020 | 55.1 | 43.8 | 80.3 | 70.2 |'
- en: '| DAReID [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| DAReID [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
- en: '| MoS [[44](#bib.bib44)] | AAAI2021 | 66.6 | 55.1 | - | - |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| MoS [[44](#bib.bib44)] | AAAI2021 | 66.6 | 55.1 | - | - |'
- en: '| Auxiliary Model for Position | PVPM [[48](#bib.bib48)] | CVPR2020 | - | -
    | 70.4 | 61.2 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 用于位置的辅助模型 | PVPM [[48](#bib.bib48)] | CVPR2020 | - | - | 70.4 | 61.2 |'
- en: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 51.4 | 37.3 | - | - |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 51.4 | 37.3 | - | - |'
- en: '| PDVM [[45](#bib.bib45)] | PRL2020 | 53.0 | 38.1 | - | - |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| PDVM [[45](#bib.bib45)] | PRL2020 | 53.0 | 38.1 | - | - |'
- en: '| HOReID [[42](#bib.bib42)] | CVPR2020 | 55.1 | 43.8 | 80.3 | 70.2 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| HOReID [[42](#bib.bib42)] | CVPR2020 | 55.1 | 43.8 | 80.3 | 70.2 |'
- en: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 56.3 | 43.5 | - | - |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 56.3 | 43.5 | - | - |'
- en: '| PFD [[52](#bib.bib52)] | AAAI2022 | 69.5 | 61.8 | 81.5 | 83.0 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| PFD [[52](#bib.bib52)] | AAAI2022 | 69.5 | 61.8 | 81.5 | 83.0 |'
- en: '| Additional Supervision for Position | HPNet [[60](#bib.bib60)] | ICME2020
    | - | - | 87.3 | 77.4 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 用于位置的额外监督 | HPNet [[60](#bib.bib60)] | ICME2020 | - | - | 87.3 | 77.4 |'
- en: '| PGFL-KD [[58](#bib.bib58)] | MM2021 | 63.0 | 54.1 | 80.7 | 70.3 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| PGFL-KD [[58](#bib.bib58)] | MM2021 | 63.0 | 54.1 | 80.7 | 70.3 |'
- en: '| DAReID [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| DAReID [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
- en: '| RFCNet [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 | - | - |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| RFCNet [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 | - | - |'
- en: '| Attention Mechanism for Position | MHSA-Net [[68](#bib.bib68)] | TNNLS2022
    | 59.7 | 44.8 | - | - |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 用于位置的注意力机制 | MHSA-Net [[68](#bib.bib68)] | TNNLS2022 | 59.7 | 44.8 | - |
    - |'
- en: '| ISP [[62](#bib.bib62)] | ECCV2020 | 62.8 | 52.3 | - | - |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| ISP [[62](#bib.bib62)] | ECCV2020 | 62.8 | 52.3 | - | - |'
- en: '| PAT [[67](#bib.bib67)] | CVPR2021 | 64.5 | 53.6 | 81.6 | 72.1 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| PAT [[67](#bib.bib67)] | CVPR2021 | 64.5 | 53.6 | 81.6 | 72.1 |'
- en: '| SBPA [[66](#bib.bib66)] | SPL2021 | 64.5 | 54.0 | - | - |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| SBPA [[66](#bib.bib66)] | SPL2021 | 64.5 | 54.0 | - | - |'
- en: '| Scale Misalignment | Multi-scale Features | FPR [[81](#bib.bib81)] | ICCV2019
    | - | - | 78.3 | 68.0 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 尺度不对齐 | 多尺度特征 | FPR [[81](#bib.bib81)] | ICCV2019 | - | - | 78.3 | 68.0 |'
- en: '| DSR [[30](#bib.bib30)] | CVPR2018 | 40.8 | 30.4 | 72.8 | 62.8 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| DSR [[30](#bib.bib30)] | CVPR2018 | 40.8 | 30.4 | 72.8 | 62.8 |'
- en: '| Noisy Information | Auxiliary Model for Noise | PVPM [[48](#bib.bib48)] |
    CVPR2020 | - | - | 70.4 | 61.2 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 噪声信息 | 辅助噪声模型 | PVPM [[48](#bib.bib48)] | CVPR2020 | - | - | 70.4 | 61.2
    |'
- en: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 51.4 | 37.3 | - | - |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| PGFA [[36](#bib.bib36)] | ICCV2019 | 51.4 | 37.3 | - | - |'
- en: '| PDVM [[45](#bib.bib45)] | PRL2020 | 53.0 | 38.1 | - | - |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| PDVM [[45](#bib.bib45)] | PRL2020 | 53.0 | 38.1 | - | - |'
- en: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 56.3 | 43.5 | - | - |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| PMFB [[46](#bib.bib46)] | TNNLS2021 | 56.3 | 43.5 | - | - |'
- en: '| LKWS [[86](#bib.bib86)] | ICCV2021 | 62.2 | 46.3 | 81.0 | 71.0 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| LKWS [[86](#bib.bib86)] | ICCV2021 | 62.2 | 46.3 | 81.0 | 71.0 |'
- en: '| PFD [[52](#bib.bib52)] | AAAI2022 | 69.5 | 61.8 | 81.5 | 83.0 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| PFD [[52](#bib.bib52)] | AAAI2022 | 69.5 | 61.8 | 81.5 | 83.0 |'
- en: '| Additional Supervision for Noise | GASM [[39](#bib.bib39)] | ECCV2020 | -
    | - | 74.5 | 65.6 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 噪声的附加监督 | GASM [[39](#bib.bib39)] | ECCV2020 | - | - | 74.5 | 65.6 |'
- en: '| FPR [[81](#bib.bib81)] | ICCV2019 | - | - | 78.3 | 68.0 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| FPR [[81](#bib.bib81)] | ICCV2019 | - | - | 78.3 | 68.0 |'
- en: '| HPNet [[60](#bib.bib60)] | ICME2020 | - | - | 87.3 | 77.4 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| HPNet [[60](#bib.bib60)] | ICME2020 | - | - | 87.3 | 77.4 |'
- en: '| DAReID [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| DAReID [[38](#bib.bib38)] | KBS2021 | 63.4 | 53.2 | - | - |'
- en: '| Attention Mechanism for Noise | IGOAS [[66](#bib.bib66)] | TIP2021 | 60.1
    | 49.4 | 81.1 | - |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 噪声的注意机制 | IGOAS [[66](#bib.bib66)] | TIP2021 | 60.1 | 49.4 | 81.1 | - |'
- en: '| OAMN [[13](#bib.bib13)] | ICCV2021 | 62.6 | 46.1 | - | - |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| OAMN [[13](#bib.bib13)] | ICCV2021 | 62.6 | 46.1 | - | - |'
- en: '| OCNet [[97](#bib.bib97)] | ICASSP2022 | 64.3 | 54.4 | - | - |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| OCNet [[97](#bib.bib97)] | ICASSP2022 | 64.3 | 54.4 | - | - |'
- en: '| DRL-Net [[92](#bib.bib92)] | TMM2021 | 65.8 | 53.9 | - | - |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| DRL-Net [[92](#bib.bib92)] | TMM2021 | 65.8 | 53.9 | - | - |'
- en: '| SSGR [[91](#bib.bib91)] | ICCV2021 | 65.8 | 57.2 | 78.5 | 72.9 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| SSGR [[91](#bib.bib91)] | ICCV2021 | 65.8 | 57.2 | 78.5 | 72.9 |'
- en: '|  | FED [[93](#bib.bib93)] | CVPR2022 | 68.1 | 56.4 | 86.3 | 79.3 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | FED [[93](#bib.bib93)] | CVPR2022 | 68.1 | 56.4 | 86.3 | 79.3 |'
- en: '| Missing Information | Spatial Recovery | RFCNet [[14](#bib.bib14)] | TPAMI2021
    | 63.9 | 54.5 | - | - |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 缺失信息 | 空间恢复 | RFCNet [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 | - | -
    |'
- en: '| Temporal Recovery | RFCNet [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 |
    - | - |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 时间恢复 | RFCNet [[14](#bib.bib14)] | TPAMI2021 | 63.9 | 54.5 | - | - |'
- en: VI Conclusion
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: 'This paper aims at providing a systematic survey of occluded person Re-ID to
    help promote future research. We first analyze and summarize four issues brought
    by occlusion in person Re-ID: the position misalignment, the scale misalignment,
    the noisy information, and the missing information. The published publications
    of deep learning-based occluded person Re-ID from top conferences and journals
    before June, 2022 are categorized and introduced accordingly. We provide the performance
    comparison of recent occluded person Re-ID methods on four popular datasets: Partial-ReID,
    Partial-iLIDS, Occluded-ReID, and Occluded-DukeMTMC. Based on the analysis of
    evaluation results, we finally discuss the promising future research directions.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在提供一个关于遮挡人物重识别的系统性综述，以促进未来的研究。我们首先分析和总结了遮挡在人物重识别中带来的四个问题：位置不对齐、尺度不对齐、噪声信息和缺失信息。对2022年6月之前在顶级会议和期刊上发表的基于深度学习的遮挡人物重识别文献进行了分类和介绍。我们提供了最近遮挡人物重识别方法在四个热门数据集上的性能比较：Partial-ReID、Partial-iLIDS、Occluded-ReID
    和 Occluded-DukeMTMC。基于评估结果的分析，我们最后讨论了有前景的未来研究方向。
- en: References
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. Ye et al. Deep learning for person re-identification: A survey and outlook.
    IEEE Trans. Pattern Anal. Mach. Intell., 2021.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. Ye 等. 基于深度学习的人物重识别：综述与展望。IEEE 计算机学会模式分析与机器智能期刊，2021年。'
- en: '[2] H. Zhao et al. Spindle net: Person re-identification with human body region
    guided feature decomposition and fusion. In CVPR, pp. 1077–1085, 2017.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] H. Zhao 等. Spindle net: 通过人体区域引导的特征分解与融合进行人物重识别。发表于 CVPR，第1077–1085页，2017年。'
- en: '[3] M. S. Sarfraz et al. A pose-sensitive embedding for person re-identification
    with expanded cross neighborhood re-ranking. In CVPR, pp. 420–429, 2018.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. S. Sarfraz 等. 具有扩展交叉邻域重排序的姿态敏感嵌入用于人物重识别。发表于 CVPR，第420–429页，2018年。'
- en: '[4] J. Liu et al. Pose transferrable person re-identification. In CVPR, pp.
    4099–4108, 2018.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Liu 等. 可转移姿态的人物重识别。发表于 CVPR，第4099–4108页，2018年。'
- en: '[5] X. Qian et al. Long-term cloth-changing person re-identification. In ACCV,
    2020.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X. Qian 等. 长期换衣人物重识别。发表于 ACCV，2020年。'
- en: '[6] P. Hong et al. Fine-grained shape-appearance mutual learning for cloth-changing
    person re-identification. In CVPR, pp. 10513–10522, 2021.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] P. Hong等. 用于换衣行人重识别的细粒度形状-外观互学。 In CVPR, pp. 10513–10522, 2021。'
- en: '[7] Y. Huang et al. Illumination-invariant person re-identification. In MM
    - Proc. ACM Int. Conf. Multimed., pp. 365–373, 2019.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Huang等. 不变照明的行人重识别。 In MM - Proc. ACM Int. Conf. Multimed., pp. 365–373,
    2019。'
- en: '[8] G. Zhang et al. Illumination unification for person re-identification.
    IEEE Trans. Circuits Syst. Video Technol., pp. 1–1, 2022.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] G. Zhang等. 行人重识别的照明统一。IEEE Trans. Circuits Syst. Video Technol., pp. 1–1,
    2022。'
- en: '[9] S. Karanam et al. Person re-identification with discriminatively trained
    viewpoint invariant dictionaries. In ICCV, pp. 4516–4524, 2015.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Karanam等. 使用判别训练的视角不变字典进行行人重识别。 In ICCV, pp. 4516–4524, 2015。'
- en: '[10] X. Sun and L. Zheng. Dissecting person re-identification from the viewpoint
    of viewpoint. In CVPR, pp. 608–617, 2019.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] X. Sun和L. Zheng. 从视角的角度解剖行人重识别。 In CVPR, pp. 608–617, 2019。'
- en: '[11] L. Wu et al. Cross-entropy adversarial view adaptation for person re-identification.
    IEEE Trans. Circuits Syst. Video Technol., 30(7):2081–2092, 2020.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] L. Wu等. 用于行人重识别的交叉熵对抗视角适应。IEEE Trans. Circuits Syst. Video Technol., 30(7):2081–2092,
    2020。'
- en: '[12] J. Miao et al. Identifying visible parts via pose estimation for occluded
    person re-identification. IEEE Trans. Neural Netw. Learn. Syst., 2021.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] J. Miao等. 通过姿态估计识别可见部分以实现遮挡行人重识别。IEEE Trans. Neural Netw. Learn. Syst.,
    2021。'
- en: '[13] P. Chen et al. Occlude them all: Occlusion-aware attention network for
    occluded person re-id. In ICCV, pp. 11833–11842, 2021.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] P. Chen等. 遮挡他们全部：用于遮挡行人重识别的遮挡感知注意网络。 In ICCV, pp. 11833–11842, 2021。'
- en: '[14] R. Hou et al. Feature completion for occluded person re-identification.
    IEEE Trans. Pattern Anal. Mach. Intell., 2021.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] R. Hou等. 用于遮挡行人重识别的特征完成。IEEE Trans. Pattern Anal. Mach. Intell., 2021。'
- en: '[15] W.-S. Zheng et al. Partial person re-identification. In ICCV, pp. 4678–4686,
    2015.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] W.-S. Zheng等. 部分行人重识别。 In ICCV, pp. 4678–4686, 2015。'
- en: '[16] J. Zhuo et al. Occluded person re-identification. In ICME, pp. 1–6\. IEEE,
    2018.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Zhuo等. 遮挡行人重识别。 In ICME, pp. 1–6. IEEE, 2018。'
- en: '[17] L. Zheng et al. Person re-identification: Past, present and future. arXiv
    preprint arXiv:1610.02984, 2016.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] L. Zheng等. 行人重识别：过去、现在与未来。arXiv preprint arXiv:1610.02984, 2016。'
- en: '[18] R. Mazzon et al. Person re-identification in crowd. Pattern Recognit.
    Lett., 33(14):1828–1837, 2012.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] R. Mazzon等. 人群中的行人重识别。Pattern Recognit. Lett., 33(14):1828–1837, 2012。'
- en: '[19] A. Bedagkar-Gala and S. K. Shah. A survey of approaches and trends in
    person re-identification. Image Vision Comput., 32(4):270–286, 2014.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Bedagkar-Gala和S. K. Shah. 行人重识别方法和趋势调查。Image Vision Comput., 32(4):270–286,
    2014。'
- en: '[20] B. Lavi et al. Survey on deep learning techniques for person re-identification
    task. arXiv preprint arXiv:1807.05284, 2018.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] B. Lavi等. 行人重识别任务的深度学习技术调查。arXiv preprint arXiv:1807.05284, 2018。'
- en: '[21] D. Wu et al. Deep learning-based methods for person re-identification:
    A comprehensive review. Neurocomputing, 337:354–371, 2019.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] D. Wu等. 基于深度学习的行人重识别方法：全面综述。Neurocomputing, 337:354–371, 2019。'
- en: '[22] Q. Leng et al. A survey of open-world person re-identification. IEEE Trans.
    Circuits Syst. Video Technol., 30(4):1092–1108, 2019.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Q. Leng等. 开放世界行人重识别的调查。IEEE Trans. Circuits Syst. Video Technol., 30(4):1092–1108,
    2019。'
- en: '[23] B. Lavi et al. Survey on reliable deep learning-based person re-identification
    models: Are we there yet? arXiv preprint arXiv:2005.00355, 2020.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] B. Lavi等. 基于深度学习的行人重识别模型调查：我们已经到达了吗？arXiv preprint arXiv:2005.00355, 2020。'
- en: '[24] S. karanam et al. A systematic evaluation and benchmark for person re-identification:
    Features, metrics, and datasets. IEEE Trans. Pattern Anal. Mach. Intell., 41(3):523–536,
    2019.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Karanam等. 行人重识别的系统评估和基准：特征、度量和数据集。IEEE Trans. Pattern Anal. Mach. Intell.,
    41(3):523–536, 2019。'
- en: '[25] K. Islam. Person search: New paradigm of person re-identification: A survey
    and outlook of recent works. Image Vision Comput., 101:103970, 2020.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] K. Islam. 行人搜索：行人重识别的新范式：对近期工作的调查和展望。Image Vision Comput., 101:103970,
    2020。'
- en: '[26] X. Lin et al. Unsupervised person re-identification: A systematic survey
    of challenges and solutions. arXiv preprint arXiv:2109.06057, 2021.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] X. Lin等. 无监督行人重识别：挑战和解决方案的系统调查。arXiv preprint arXiv:2109.06057, 2021。'
- en: '[27] X. Lin et al. Person search challenges and solutions: A survey. In IJCAI
    Int. Joint Conf. Artif. Intell., pp. 1–10\. RMIT University, 2021.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] X. Lin等. 行人搜索挑战与解决方案：调查。 In IJCAI Int. Joint Conf. Artif. Intell., pp.
    1–10. RMIT University, 2021。'
- en: '[28] Z. Ming et al. Deep learning-based person re-identification methods: A
    survey and outlook of recent works. Image Vision Comput., pp. 104394, 2022.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Z. Ming 等. 基于深度学习的人物再识别方法：近期工作的综述与展望。图像视觉计算, 第 104394 页, 2022。'
- en: '[29] Z. Wang et al. Beyond intra-modality: a survey of heterogeneous person
    re-identification. In IJCAI Int. Joint Conf. Artif. Intell., pp. 4973–4980, 2021.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Z. Wang 等. 超越内部模态：异质人物再识别的综述。发表于 IJCAI 国际联合人工智能会议, 第 4973–4980 页, 2021。'
- en: '[30] L. He et al. Deep spatial feature reconstruction for partial person re-identification:
    Alignment-free approach. In CVPR, pp. 7073–7082, 2018.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] L. He 等. 部分人物再识别的深度空间特征重建：无对齐方法。发表于 CVPR, 第 7073–7082 页, 2018。'
- en: '[31] W.-S. Zheng et al. Person re-identification by probabilistic relative
    distance comparison. In CVPR 2011, pp. 649–656\. IEEE, 2011.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] W.-S. Zheng 等. 基于概率相对距离比较的人物再识别。发表于 CVPR 2011, 第 649–656 页。IEEE, 2011。'
- en: '[32] J. Kim and C. D. Yoo. Deep partial person re-identification via attention
    model. In ICIP, pp. 3425–3429\. IEEE, 2017.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Kim 和 C. D. Yoo. 通过注意力模型进行深度部分人物再识别。发表于 ICIP, 第 3425–3429 页。IEEE, 2017。'
- en: '[33] W. Li et al. Deepreid: Deep filter pairing neural network for person re-identification.
    In CVPR, pp. 152–159, 2014.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] W. Li 等. Deepreid: 用于人物再识别的深度过滤配对神经网络。发表于 CVPR, 第 152–159 页, 2014。'
- en: '[34] A. Ess et al. A mobile vision system for robust multi-person tracking.
    In CVPR, pp. 1–8\. IEEE, 2008.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. Ess 等. 一种用于稳健多人物追踪的移动视觉系统。发表于 CVPR, 第 1–8 页。IEEE, 2008。'
- en: '[35] Z. Zheng et al. Unlabeled samples generated by gan improve the person
    re-identification baseline in vitro. In ICCV, pp. 3754–3762, 2017.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Z. Zheng 等. 生成对抗网络生成的无标签样本提高了体外的人物再识别基线。发表于 ICCV, 第 3754–3762 页, 2017。'
- en: '[36] J. Miao et al. Pose-guided feature alignment for occluded person re-identification.
    In ICCV, pp. 542–551, 2019.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Miao 等. 基于姿态引导的特征对齐用于遮挡人物再识别。发表于 ICCV, 第 542–551 页, 2019。'
- en: '[37] Y. Wu et al. Exploit the unknown gradually: One-shot video-based person
    re-identification by stepwise learning. In CVPR, pp. 5177–5186, 2018.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Y. Wu 等. 逐步开发未知：通过逐步学习的一次性视频基人物再识别。发表于 CVPR, 第 5177–5186 页, 2018。'
- en: '[38] Y. Xu et al. Dual attention-based method for occluded person re-identification.
    Knowledge-Based Systems, 212:106554, 2021.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Xu 等. 基于双重注意力的方法用于遮挡人物再识别。知识基础系统, 212:106554, 2021。'
- en: '[39] L. He and W. Liu. Guided saliency feature learning for person re-identification
    in crowded scenes. In ECCV, pp. 357–373\. Springer, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] L. He 和 W. Liu. 针对拥挤场景中人物再识别的引导显著性特征学习。发表于 ECCV, 第 357–373 页。Springer,
    2020。'
- en: '[40] C.-S. Lin and Y.-C. F. Wang. Self-supervised bodymap-to-appearance co-attention
    for partial person re-identification. In ICIP, pp. 2299–2303\. IEEE, 2021.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] C.-S. Lin 和 Y.-C. F. Wang. 自监督体图到外观的共同注意力用于部分人物再识别。发表于 ICIP, 第 2299–2303
    页。IEEE, 2021。'
- en: '[41] H. Jin et al. Occlusion-sensitive person re-identification via attribute-based
    shift attention. IEEE Trans. Circuits Syst. Video Technol., 32(4):2170–2185, 2022.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Jin 等. 通过基于属性的偏移注意力进行遮挡敏感的人物再识别。IEEE 电路系统与视频技术学报, 32(4):2170–2185,
    2022。'
- en: '[42] G. Wang et al. High-order information matters: Learning relation and topology
    for occluded person re-identification. In CVPR, pp. 6449–6458, 2020.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] G. Wang 等. 高阶信息至关重要：学习关系和拓扑用于遮挡人物再识别。发表于 CVPR, 第 6449–6458 页, 2020。'
- en: '[43] Y. Yan et al. Learning multi-granular hypergraphs for video-based person
    re-identification. In CVPR, pp. 2899–2908, 2020.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y. Yan 等. 学习多粒度超图用于基于视频的人物再识别。发表于 CVPR, 第 2899–2908 页, 2020。'
- en: '[44] M. Jia et al. Matching on sets: Conquer occluded person re-identification
    without alignment. In AAAI Conf. Artif. Intell., pp. 1673–1681, 2021.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] M. Jia 等. 在集合上匹配：无对齐征服遮挡人物再识别。发表于 AAAI 人工智能会议, 第 1673–1681 页, 2021。'
- en: '[45] S. Zhou et al. Depth occlusion perception feature analysis for person
    re-identification. Pattern Recognit. Lett., 138:617–623, 2020.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] S. Zhou 等. 针对人物再识别的深度遮挡感知特征分析。模式识别通讯, 138:617–623, 2020。'
- en: '[46] J. Miao et al. Identifying visible parts via pose estimation for occluded
    person re-identification. IEEE Trans. Neural Netw. Learn. Syst., 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Miao 等. 通过姿态估计识别可见部分用于遮挡人物再识别。IEEE 神经网络与学习系统学报, 2021。'
- en: '[47] C. Han et al. Keypoint-based feature matching for partial person re-identification.
    In ICIP, pp. 226–230\. IEEE, 2020.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] C. Han 等. 基于关键点的特征匹配用于部分人物再识别。发表于 ICIP, 第 226–230 页。IEEE, 2020。'
- en: '[48] S. Gao et al. Pose-guided visible part matching for occluded person reid.
    In CVPR, pp. 11744–11752, 2020.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] S. Gao 等. 基于姿态引导的可见部分匹配用于遮挡人物再识别。发表于 CVPR, 第 11744–11752 页, 2020。'
- en: '[49] Y. Zhai et al. Pgmanet: Pose-guided mixed attention network for occluded
    person re-identification. In IJCNN, pp. 1–8\. IEEE, 2021.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Y. Zhai 等人. Pgmanet: 带有姿态引导混合注意力网络的遮挡人物重识别. 在 IJCNN, 第 1–8 页. IEEE, 2021
    年。'
- en: '[50] J. Liu et al. Spatial-temporal correlation and topology learning for person
    re-identification in videos. In CVPR, pp. 4370–4379, 2021.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Liu 等人. 视频中人物重识别的时空相关性和拓扑学习. 在 CVPR, 第 4370–4379 页, 2021 年。'
- en: '[51] Y. He et al. Adversarial cross-scale alignment pursuit for seriously misaligned
    person re-identification. In ICIP, pp. 2373–2377\. IEEE, 2021.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. He 等人. 为严重失调的人物重识别进行对抗性跨尺度对齐追求. 在 ICIP, 第 2373–2377 页. IEEE, 2021 年。'
- en: '[52] W. Tao et al. Pose-guided feature disentangling for occluded person re-identification
    based on transformer. AAAI Conf. Artif. Intell., 2022.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] W. Tao 等人. 基于变换器的姿态引导特征解缠结用于遮挡人物重识别. AAAI 人工智能会议, 2022 年。'
- en: '[53] M. M. Kalayeh et al. Human semantic parsing for person re-identification.
    In CVPR, pp. 1062–1071, 2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. M. Kalayeh 等人. 人类语义解析用于人物重识别. 在 CVPR, 第 1062–1071 页, 2018 年。'
- en: '[54] R. Quispe and H. Pedrini. Improved person re-identification based on saliency
    and semantic parsing with deep neural network models. Image Vision Comput., 92:103809,
    2019.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] R. Quispe 和 H. Pedrini. 基于显著性和深度神经网络模型的语义解析改进人物重识别. 图像视觉计算, 92:103809,
    2019 年。'
- en: '[55] L. Gao et al. Texture semantically aligned with visibility-aware for partial
    person re-identification. In MM - Proc. ACM Int. Conf. Multimed., pp. 3771–3779,
    2020.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] L. Gao 等人. 与可见性感知对齐的纹理用于部分人物重识别. 在 MM - ACM 国际多媒体会议, 第 3771–3779 页, 2020
    年。'
- en: '[56] J. Xu et al. Attention-aware compositional network for person re-identification.
    In CVPR, pp. 2119–2128, 2018.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] J. Xu 等人. 关注意识的组合网络用于人物重识别. 在 CVPR, 第 2119–2128 页, 2018 年。'
- en: '[57] Z. Zhang et al. Densely semantically aligned person re-identification.
    In CVPR, pp. 667–676, 2019.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Z. Zhang 等人. 密集语义对齐的人物重识别. 在 CVPR, 第 667–676 页, 2019 年。'
- en: '[58] K. Zheng et al. Pose-guided feature learning with knowledge distillation
    for occluded person re-identification. In MM - Proc. ACM Int. Conf. Multimed.,
    pp. 4537–4545, 2021.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] K. Zheng 等人. 带有知识蒸馏的姿态引导特征学习用于遮挡人物重识别. 在 MM - ACM 国际多媒体会议, 第 4537–4545
    页, 2021 年。'
- en: '[59] H. Cai et al. Multi-scale body-part mask guided attention for person re-identification.
    In CVPR Workshops, pp. 0–0, 2019.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] H. Cai 等人. 多尺度身体部件掩码引导注意力用于人物重识别. 在 CVPR 研讨会, 第 0–0 页, 2019 年。'
- en: '[60] H. Huang et al. Human parsing based alignment with multi-task learning
    for occluded person re-identification. In ICME, pp. 1–6\. IEEE, 2020.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] H. Huang 等人. 基于人类解析对齐的多任务学习用于遮挡人物重识别. 在 ICME, 第 1–6 页. IEEE, 2020 年。'
- en: '[61] Q. Zhou et al. Fine-grained spatial alignment model for person re-identification
    with focal triplet loss. IEEE Trans. Image Process., 29:7578–7589, 2020.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Q. Zhou 等人. 具有焦点三元组损失的人物重识别细粒度空间对齐模型. IEEE 图像处理汇刊, 29:7578–7589, 2020
    年。'
- en: '[62] K. Zhu et al. Identity-guided human semantic parsing for person re-identification.
    In ECCV, pp. 346–363\. Springer, 2020.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] K. Zhu 等人. 身份引导的人类语义解析用于人物重识别. 在 ECCV, 第 346–363 页. Springer, 2020 年。'
- en: '[63] Y. Sun et al. Perceive where to focus: Learning visibility-aware part-level
    features for partial person re-identification. In CVPR, pp. 393–402, 2019.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Y. Sun 等人. 识别聚焦区域: 学习可见性感知的部件级特征用于部分人物重识别. 在 CVPR, 第 393–402 页, 2019 年。'
- en: '[64] L. Huo et al. Attentive part-aware networks for partial person re-identification.
    In ICPR, pp. 3652–3659\. IEEE, 2021.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] L. Huo 等人. 关注部件感知网络用于部分人物重识别. 在 ICPR, 第 3652–3659 页. IEEE, 2021 年。'
- en: '[65] S. Li et al. Diversity regularized spatiotemporal attention for video-based
    person re-identification. In CVPR, pp. 369–378, 2018.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. Li 等人. 多样性正则化的时空注意力用于基于视频的人物重识别. 在 CVPR, 第 369–378 页, 2018 年。'
- en: '[66] G. Wang et al. Self-guided body part alignment with relation transformers
    for occluded person re-identification. IEEE Signal Processing Letters, 28:1155–1159,
    2021.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] G. Wang 等人. 带有关系变换器的自引导身体部位对齐用于遮挡人物重识别. IEEE 信号处理通讯, 28:1155–1159, 2021
    年。'
- en: '[67] Y. Li et al. Diverse part discovery: Occluded person re-identification
    with part-aware transformer. In CVPR, pp. 2898–2907, 2021.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Y. Li 等人. 多样化部件发现: 带有部件感知变换器的遮挡人物重识别. 在 CVPR, 第 2898–2907 页, 2021 年。'
- en: '[68] H. Tan et al. Mhsa-net: Multihead self-attention network for occluded
    person re-identification. IEEE Trans. Neural Netw. Learn. Syst., 2022.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] H. Tan 等人. Mhsa-net: 多头自注意力网络用于遮挡人物重识别. IEEE 神经网络与学习系统汇刊, 2022 年。'
- en: '[69] A. Vaswani et al. Attention is all you need. Advances in neural information
    processing systems, 30, 2017.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Vaswani 等人. 关注即你所需. 神经信息处理系统进展, 30, 2017 年。'
- en: '[70] J. Jiang et al. Dynamic hypergraph neural networks. In IJCAI Int. Joint
    Conf. Artif. Intell., pp. 2635–2641, 2019.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Jiang 等. 动态超图神经网络. In IJCAI Int. Joint Conf. Artif. Intell., pp. 2635–2641,
    2019.'
- en: '[71] Z. Cao et al. Realtime multi-person 2d pose estimation using part affinity
    fields. In CVPR, pp. 7291–7299, 2017.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Z. Cao 等. 实时多人人体2D姿态估计使用部件亲和场. In CVPR, pp. 7291–7299, 2017.'
- en: '[72] K. Gong et al. Look into person: Self-supervised structure-sensitive learning
    and a new benchmark for human parsing. In CVPR, pp. 932–940, 2017.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] K. Gong 等. 观察行人: 自监督结构敏感学习和一个新基准用于人体解析. In CVPR, pp. 932–940, 2017.'
- en: '[73] C. Szegedy et al. Rethinking the inception architecture for computer vision.
    In CVPR, pp. 2818–2826, 2016.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] C. Szegedy 等. 重新思考计算机视觉中的Inception架构. In CVPR, pp. 2818–2826, 2016.'
- en: '[74] X. Li et al. Deepsaliency: Multi-task deep neural network model for salient
    object detection. IEEE Trans. Image Process., 25(8):3919–3930, 2016.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] X. Li 等. Deepsaliency: 多任务深度神经网络模型用于显著性目标检测. IEEE Trans. Image Process.,
    25(8):3919–3930, 2016.'
- en: '[75] K. Sun et al. Deep high-resolution representation learning for human pose
    estimation. In CVPR, pp. 5693–5703, 2019.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] K. Sun 等. 深度高分辨率表示学习用于人体姿态估计. In CVPR, pp. 5693–5703, 2019.'
- en: '[76] R. A. Güler et al. Densepose: Dense human pose estimation in the wild.
    In CVPR, pp. 7297–7306, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] R. A. Güler 等. Densepose: 野外密集人体姿态估计. In CVPR, pp. 7297–7306, 2018.'
- en: '[77] X. Liang et al. Look into person: Joint body parsing & pose estimation
    network and a new benchmark. IEEE Trans. Pattern Anal. Mach. Intell., 41(4):871–885,
    2018.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] X. Liang 等. 观察行人: 联合人体解析与姿态估计网络和一个新基准. IEEE Trans. Pattern Anal. Mach.
    Intell., 41(4):871–885, 2018.'
- en: '[78] J. Fu et al. Dual attention network for scene segmentation. In CVPR, pp.
    3146–3154, 2019.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] J. Fu 等. 双重注意力网络用于场景分割. In CVPR, pp. 3146–3154, 2019.'
- en: '[79] T.-Y. Lin et al. Microsoft coco: Common objects in context. In ECCV, pp.
    740–755\. Springer, 2014.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] T.-Y. Lin 等. Microsoft coco: 上下文中的常见物体. In ECCV, pp. 740–755. Springer,
    2014.'
- en: '[80] F. Zheng et al. Pyramidal person re-identification via multi-loss dynamic
    training. In CVPR, pp. 8514–8522, 2019.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] F. Zheng 等. 通过多损失动态训练的金字塔行人重识别. In CVPR, pp. 8514–8522, 2019.'
- en: '[81] L. He et al. Foreground-aware pyramid reconstruction for alignment-free
    occluded person re-identification. In ICCV, pp. 8450–8459, 2019.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] L. He 等. 前景感知金字塔重建用于对齐无关的遮挡行人重识别. In ICCV, pp. 8450–8459, 2019.'
- en: '[82] Y. Zhong et al. Robust partial matching for person search in the wild.
    In CVPR, pp. 6827–6835, 2020.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Zhong 等. 针对野外场景的稳健部分匹配. In CVPR, pp. 6827–6835, 2020.'
- en: '[83] T. He et al. Partial person re-identification with part-part correspondence
    learning. In CVPR, pp. 9105–9115, 2021.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] T. He 等. 基于部分-部分对应学习的部分行人重识别. In CVPR, pp. 9105–9115, 2021.'
- en: '[84] H. Luo et al. Stnreid: Deep convolutional networks with pairwise spatial
    transformer networks for partial person re-identification. IEEE Trans. Multimedia,
    22(11):2905–2913, 2020.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] H. Luo 等. Stnreid: 使用配对空间变换网络的深度卷积网络进行部分行人重识别. IEEE Trans. Multimedia,
    22(11):2905–2913, 2020.'
- en: '[85] S. Yu et al. Neighbourhood-guided feature reconstruction for occluded
    person re-identification. arXiv preprint arXiv:2105.07345, 2021.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Yu 等. 邻域引导特征重建用于遮挡行人重识别. arXiv preprint arXiv:2105.07345, 2021.'
- en: '[86] J. Yang et al. Learning to know where to see: A visibility-aware approach
    for occluded person re-identification. In ICCV, pp. 11885–11894, 2021.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. Yang 等. 学习知道在哪里查看: 一种针对遮挡行人重识别的可见性感知方法. In ICCV, pp. 11885–11894, 2021.'
- en: '[87] G. Chen et al. Spatial-temporal attention-aware learning for video-based
    person re-identification. IEEE Trans. Image Process., 28(9):4192–4205, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] G. Chen 等. 基于视频的人体重识别的时空注意力感知学习. IEEE Trans. Image Process., 28(9):4192–4205,
    2019.'
- en: '[88] X. Zhang et al. Semantic-aware occlusion-robust network for occluded person
    re-identification. IEEE Trans. Circuits Syst. Video Technol., 31(7):2764–2778,
    2021.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] X. Zhang 等. 语义感知的遮挡鲁棒网络用于遮挡行人重识别. IEEE Trans. Circuits Syst. Video Technol.,
    31(7):2764–2778, 2021.'
- en: '[89] Z. Zhong et al. Random erasing data augmentation. In AAAI Conf. Artif.
    Intell., volume 34, pp. 13001–13008, 2020.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Z. Zhong 等. 随机擦除数据增强. In AAAI Conf. Artif. Intell., volume 34, pp. 13001–13008,
    2020.'
- en: '[90] C. Zhao et al. Incremental generative occlusion adversarial suppression
    network for person reid. IEEE Trans. Image Process., 30:4212–4224, 2021.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] C. Zhao 等. 增量生成遮挡对抗抑制网络用于行人重识别. IEEE Trans. Image Process., 30:4212–4224,
    2021.'
- en: '[91] C. Yan et al. Occluded person re-identification with single-scale global
    representations. In ICCV, pp. 11875–11884, 2021.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] C. Yan 等. 单尺度全局表示的遮挡行人重识别. In ICCV, pp. 11875–11884, 2021.'
- en: '[92] M. Jia et al. Learning disentangled representation implicitly via transformer
    for occluded person re-identification. IEEE Trans. Multimedia, 2022.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] M. Jia 等. 通过变换器隐式学习解耦表示用于遮挡人物再识别. IEEE 多媒体期刊, 2022年。'
- en: '[93] Z. Wang et al. Feature erasing and diffusion network for occluded person
    re-identification. In CVPR, pp. 4754–4763, 2022.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Z. Wang 等. 用于遮挡人物再识别的特征擦除与扩散网络. 在 CVPR 会议上，页码 4754–4763, 2022年。'
- en: '[94] M. Zheng et al. Re-identification with consistent attentive siamese networks.
    In CVPR, pp. 5735–5744, 2019.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] M. Zheng 等. 使用一致的注意力孪生网络进行再识别. 在 CVPR 会议上，页码 5735–5744, 2019年。'
- en: '[95] S. Zhao et al. Do not disturb me: Person re-identification under the interference
    of other pedestrians. In ECCV, pp. 647–663\. Springer, 2020.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] S. Zhao 等. 不要打扰我：在其他行人干扰下的人物再识别. 在 ECCV 会议上，页码 647–663。Springer, 2020年。'
- en: '[96] H. Tan et al. Incomplete descriptor mining with elastic loss for person
    re-identification. IEEE Trans. Circuits Syst. Video Technol., 2021.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] H. Tan 等. 带有弹性损失的不完整描述符挖掘用于人物再识别. IEEE 期刊. 电路系统与视频技术, 2021年。'
- en: '[97] M. Kim et al. Occluded person re-identification via relational adaptive
    feature correction learning. In ICASSP IEEE Int Conf Acoust Speech Signal Process
    Proc, pp. 2719–2723\. IEEE, 2022.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] M. Kim 等. 通过关系自适应特征修正学习进行遮挡人物再识别. 在 ICASSP IEEE 国际声学、语音和信号处理会议论文集中，页码
    2719–2723。IEEE, 2022年。'
- en: '[98] T. Ruan et al. Devil in the details: Towards accurate single and multiple
    human parsing. In AAAI Conf. Artif. Intell., volume 33, pp. 4814–4821, 2019.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] T. Ruan 等. 细节中的恶魔：朝着准确的单人和多人解析迈进. 在 AAAI 人工智能会议上，第33卷，页码 4814–4821, 2019年。'
- en: '[99] H. Zhao et al. Pyramid scene parsing network. In CVPR, pp. 2881–2890,
    2017.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] H. Zhao 等. 金字塔场景解析网络. 在 CVPR 会议上，页码 2881–2890, 2017年。'
- en: '[100] X. Zhou et al. Objects as points. arXiv preprint arXiv:1904.07850, 2019.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Zhou 等. 对象作为点. arXiv 预印本 arXiv:1904.07850, 2019年。'
- en: '[101] M. Yin et al. Disentangled non-local neural networks. In ECCV, pp. 191–207\.
    Springer, 2020.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] M. Yin 等. 解耦非局部神经网络. 在 ECCV 会议上，页码 191–207。Springer, 2020年。'
- en: '[102] R. Hou et al. Vrstc: Occlusion-free video person re-identification. In
    CVPR, pp. 7183–7192, 2019.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] R. Hou 等. Vrstc：无遮挡的视频人物再识别. 在 CVPR 会议上，页码 7183–7192, 2019年。'
- en: '[103] Y. Liu et al. Spatial and temporal mutual promotion for video-based person
    re-identification. In AAAI Conf. Artif. Intell., volume 33, pp. 8786–8793, 2019.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y. Liu 等. 基于视频的人物再识别的空间和时间互促进. 在 AAAI 人工智能会议上，第33卷，页码 8786–8793, 2019年。'
- en: '[104] H. Zhang et al. Self-attention generative adversarial networks. In ICML,
    pp. 7354–7363\. PMLR, 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] H. Zhang 等. 自注意力生成对抗网络. 在 ICML 会议上，页码 7354–7363。PMLR, 2019年。'
- en: '[105] L. Zheng et al. Scalable person re-identification: A benchmark. In ICCV,
    pp. 1116–1124, 2015.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] L. Zheng 等. 可扩展的人物再识别：一个基准测试. 在 ICCV 会议上，页码 1116–1124, 2015年。'
- en: '[106] A. Dosovitskiy et al. An image is worth 16x16 words: Transformers for
    image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. Dosovitskiy 等. 一张图片值 16x16 个词：用于大规模图像识别的变换器. arXiv 预印本 arXiv:2010.11929,
    2020年。'
- en: '| Yunjie Peng received her B.S. degree in the College of Computer and Information
    Science & College of Software from Southwest University, China, in 2018. She is
    currently a Ph.D. student in the School of Computer Science and Technology, Beihang
    University, China. Her research interests include gait recognition, perosn re-identification,
    computer vision, and machine learning. |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 彭云杰于2018年在中国西南大学计算机与信息科学学院及软件学院获得学士学位。目前她是中国北航大学计算机科学与技术学院的博士生。她的研究兴趣包括步态识别、人物再识别、计算机视觉和机器学习。
    |'
- en: '| Saihui Hou received the B.E. and Ph.D. degrees from University of Science
    and Technology of China in 2014 and 2019, respectively. He is currently an Assistant
    Professor with School of Artificial Intelligence, Beijing Normal University. His
    research interests include computer vision and machine learning. He recently focuses
    on gait recognition which aims to identify different people according to the walking
    patterns. |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 侯赛辉于2014年和2019年分别获得中国科技大学的工学学士和博士学位。他目前是北京师范大学人工智能学院的助理教授。他的研究兴趣包括计算机视觉和机器学习。他最近关注于步态识别，旨在根据步态模式识别不同的人。
    |'
- en: '| Chunshui Cao received the B.E. and Ph.D. degrees from University of Science
    and Technology of China in 2013 and 2018, respectively. During his Ph.D. study,
    he joined Center for Research on Intelligent Perception and Computing, National
    Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of
    Sciences. From 2018 to 2020, he worked as a Postdoctoral Fellow with PBC School
    of Finance, Tsinghua University. He is currently a Research Scientist with Watrix
    Technology Limited Co. Ltd. His research interests include pattern recognition,
    computer vision and machine learning. |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 曹春水于2013年和2018年分别获得中国科学技术大学的工学学士和博士学位。在攻读博士学位期间，他加入了中国科学院自动化研究所模式识别国家实验室智能感知与计算研究中心。2018年至2020年，他在清华大学五道口金融学院担任博士后研究员。他目前是Watrix
    Technology Limited Co. Ltd.的研究科学家。他的研究兴趣包括模式识别、计算机视觉和机器学习。 |'
- en: '| Xu Liu received the B.S. and Ph.D. degrees in control science and engineering
    from the University of Science and Technology of China (USTC), Hefei, China, in
    2013 and 2018, respectively. He is currently an algorithm researcher at Watrix
    Technology Limited Co. Ltd. His current research interests include gait recognition,
    object detection, segmentation and deep learning. |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 刘旭于2013年和2018年分别获得中国科学技术大学（USTC）的控制科学与工程专业的学士和博士学位。他目前是Watrix Technology
    Limited Co. Ltd.的算法研究员。他的研究兴趣包括步态识别、物体检测、分割和深度学习。 |'
- en: '| Yongzhen Huang received the B.E. degree from Huazhong University of Science
    and Technology in 2006, and the Ph.D. degree from Institute of Automation, Chinese
    Academy of Sciences in 2011. He is currently an Associate Professor with School
    of Artificial Intelligence, Beijing Normal University. He has published one book
    and more than 80 papers at international journals and conferences such as TPAMI,
    IJCV, TIP, TSMCB, TMM, TCSVT, CVPR, ICCV, ECCV, NIPS, AAAI. His research interests
    include pattern recognition, computer vision and machine learning. |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 黄永珍于2006年获得华中科技大学工学学士学位，2011年获得中国科学院自动化研究所博士学位。他目前是北京师范大学人工智能学院的副教授。他已经出版了一本书，并在国际期刊和会议上发表了80多篇论文，如TPAMI、IJCV、TIP、TSMCB、TMM、TCSVT、CVPR、ICCV、ECCV、NIPS、AAAI。他的研究兴趣包括模式识别、计算机视觉和机器学习。
    |'
- en: '| Zhiqiang He is currently the Senior Vice President of Lenovo Company and
    President of Lenovo Capital and Incubator Group. This group is responsible for
    exploring external innovation as well as accelerating internal innovation for
    Lenovo Group, leveraging Lenovo global resources, power of capital, and entrepreneurship.
    Previously, he was the Chief Technology Officer and held various leadership positions
    in Lenovo, particularly in overseeing Lenovoâs Research & Technology initiatives
    and systems. He is a doctoral supervisor at the Institute of Computing Technology,
    Chinese Academy of Sciences and Beihang University. |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 何志强目前是联想公司高级副总裁和联想资本与孵化器集团主席。该集团负责探索外部创新以及加速联想集团内部创新，利用联想的全球资源、资本力量和创业精神。此前，他曾担任联想的首席技术官，并在联想的研究与技术领域以及系统方面担任过多个领导职位。他是中国科学院计算技术研究所和北京航空航天大学的博士生导师。
    |'
