- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:59:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:59:53
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2008.05598] Model-Based Deep Reinforcement Learning for High-Dimensional Problems,
    a Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2008.05598] 基于模型的深度强化学习在高维问题中的应用，综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2008.05598](https://ar5iv.labs.arxiv.org/html/2008.05598)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2008.05598](https://ar5iv.labs.arxiv.org/html/2008.05598)
- en: Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于模型的深度强化学习在高维问题中的应用，综述
- en: Aske Plaat, Walter Kosters, Mike Preuss
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Aske Plaat, Walter Kosters, Mike Preuss
- en: Leiden Institute of Advanced Computer Science
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 莱顿高级计算机科学研究所
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep reinforcement learning has shown remarkable success in the past few years.
    Highly complex sequential decision making problems have been solved in tasks such
    as game playing and robotics. Unfortunately, the sample complexity of most deep
    reinforcement learning methods is high, precluding their use in some important
    applications. Model-based reinforcement learning creates an explicit model of
    the environment dynamics to reduce the need for environment samples.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习在过去几年中取得了显著成功。复杂的序列决策问题已在游戏和机器人等任务中得到解决。不幸的是，大多数深度强化学习方法的样本复杂度较高，这限制了它们在一些重要应用中的使用。基于模型的强化学习通过创建环境动态的显式模型来减少对环境样本的需求。
- en: Current deep learning methods use high-capacity networks to solve high-dimensional
    problems. Unfortunately, high-capacity models typically require many samples,
    negating the potential benefit of lower sample complexity in model-based methods.
    A challenge for deep model-based methods is therefore to achieve high predictive
    power while maintaining low sample complexity.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的深度学习方法使用高容量网络来解决高维问题。不幸的是，高容量模型通常需要大量样本，这抵消了基于模型的方法在样本复杂度较低时的潜在优势。因此，对于深度基于模型的方法，挑战在于实现高预测能力同时保持低样本复杂度。
- en: 'In recent years, many model-based methods have been introduced to address this
    challenge. In this paper, we survey the contemporary model-based landscape. First
    we discuss definitions and relations to other fields. We propose a taxonomy based
    on three approaches: using explicit planning on given transitions, using explicit
    planning on learned transitions, and end-to-end learning of both planning and
    transitions. We use these approaches to organize a comprehensive overview of important
    recent developments such as latent models. We describe methods and benchmarks,
    and we suggest directions for future work for each of the approaches. Among promising
    research directions are curriculum learning, uncertainty modeling, and use of
    latent models for transfer learning.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，许多基于模型的方法被提出以应对这一挑战。在本文中，我们综述了当代基于模型的领域。首先，我们讨论定义及其与其他领域的关系。我们提出了一种基于三种方法的分类法：对给定转移进行显式规划、对学习到的转移进行显式规划，以及对规划和转移进行端到端学习。我们利用这些方法组织了对重要近期进展（如潜在模型）的全面概述。我们描述了方法和基准，并为每种方法建议了未来的研究方向。值得关注的研究方向包括课程学习、不确定性建模和利用潜在模型进行迁移学习。
- en: 'Index Terms:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Model-based reinforcement Learning, latent models, deep learning, machine learning,
    planning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的强化学习、潜在模型、深度学习、机器学习、规划。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Deep reinforcement learning has shown remarkable successes in the past few years.
    Applications in game playing and robotics have shown the power of this paradigm
    with applications such as learning to play Go from scratch or flying an acrobatic
    model helicopter (Mnih et al., [2015](#bib.bib115); Silver et al., [2016](#bib.bib152);
    Abbeel et al., [2007](#bib.bib1)). Reinforcement learning uses an environment
    from which training data is sampled; in contrast to supervised learning it does
    not need a large database of pre-labeled training data. This opens up many applications
    for machine learning for which no such database exists. Unfortunately, however,
    for most interesting applications many samples from the environment are necessary,
    and the computational cost of learning is prohibitive, a problem that is common
    in deep learning (LeCun et al., [2015](#bib.bib106)). Achieving faster learning
    is a major goal of much current research. Many promising approaches are tried,
    among them metalearning (Hospedales et al., [2020](#bib.bib72); Huisman et al.,
    [2020](#bib.bib74)), transfer learning (Pan et al., [2010](#bib.bib129)), curriculum
    learning (Narvekar et al., [2020](#bib.bib125)) and zero-shot learning (Xian et al.,
    [2017](#bib.bib180)). The current paper focuses on model-based methods in deep
    reinforcement learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习在过去几年中取得了显著的成功。在游戏和机器人应用中，这一范式展现了它的强大，例如从零开始学习围棋或操控一架特技模型直升机（Mnih et al.,
    [2015](#bib.bib115); Silver et al., [2016](#bib.bib152); Abbeel et al., [2007](#bib.bib1)）。强化学习使用一个环境从中采样训练数据；与监督学习不同，它不需要一个大型的预标记训练数据数据库。这为机器学习开启了许多应用领域，而这些领域没有这样的数据库。然而，不幸的是，对于大多数有趣的应用，需要从环境中获得许多样本，而且学习的计算成本是巨大的，这在深度学习中是一个常见的问题（LeCun
    et al., [2015](#bib.bib106)）。实现更快的学习是当前许多研究的主要目标。许多有前景的方法正在尝试，其中包括元学习（Hospedales
    et al., [2020](#bib.bib72); Huisman et al., [2020](#bib.bib74)）、迁移学习（Pan et al.,
    [2010](#bib.bib129)）、课程学习（Narvekar et al., [2020](#bib.bib125)）和零样本学习（Xian et
    al., [2017](#bib.bib180)）。当前论文重点关注深度强化学习中的基于模型的方法。
- en: Model-based methods can reduce sample complexity. In contrast to model-free
    methods that sample at will from the environment, model-based methods build up
    a dynamics model of the environment as they sample. By using this dynamics model
    for policy updates, the number of necessary samples can be reduced substantially (Sutton,
    [1991](#bib.bib158)). Especially in robotics sample-efficiency is important (in
    games environment samples can often be generated more cheaply).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的方法可以减少样本复杂度。与随意从环境中采样的无模型方法不同，基于模型的方法在采样的同时建立环境的动态模型。通过使用这个动态模型进行策略更新，可以大幅减少必要的样本数量（Sutton,
    [1991](#bib.bib158)）。尤其在机器人技术中，样本效率很重要（在游戏环境中，样本通常可以更便宜地生成）。
- en: The success of the model-based approach hinges critically on the quality of
    the predictions of the dynamics model, and here the prevalence of deep learning
    presents a challenge (Talvitie, [2015](#bib.bib160)). Modeling the dynamics of
    high dimensional problems usually requires high capacity networks that, unfortunately,
    require many samples for training to achieve high generalization while preventing
    overfitting, potentially undoing the sample efficiency gains of model-based methods.
    Thus, the problem statement of the methods in this survey is how to train a high-capacity
    dynamics model with high predictive power and low sample complexity.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的方法的成功关键在于动态模型预测的质量，而深度学习的普及在这方面提出了挑战（Talvitie, [2015](#bib.bib160)）。建模高维问题的动态通常需要高容量的网络，不幸的是，这些网络需要大量样本进行训练，以实现高泛化能力，同时防止过拟合，这可能会破坏基于模型方法的样本效率。因此，本调查中方法的问题陈述是如何训练一个具有高预测能力和低样本复杂度的高容量动态模型。
- en: In addition to promising better sample efficiency than model-free methods, there
    is another reason for the interest in model-based methods for deep learning. Many
    problems in reinforcement learning are sequential decision problems, and learning
    the transition function is a natural way of capturing the core of long and complex
    decision sequences. This is what is called a forward model in game AI (Risi and
    Preuss, [2020](#bib.bib136); Torrado et al., [2018](#bib.bib169)). When a good
    transition function of the domain is present, then new, unseen, problems can be
    solved efficiently. Hence, model-based reinforcement learning may contribute to
    efficient transfer learning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了比模型自由方法更有前景的样本效率外，对深度学习的模型驱动方法感兴趣还有另一个原因。强化学习中的许多问题是序列决策问题，学习转移函数是捕捉长且复杂决策序列核心的自然方式。这在游戏AI中被称为前向模型（Risi
    and Preuss，[2020](#bib.bib136); Torrado et al., [2018](#bib.bib169)）。当存在一个好的领域转移函数时，可以高效地解决新的、未见过的问题。因此，模型驱动的强化学习可能有助于高效的迁移学习。
- en: 'The contribution of this survey is to give an in-depth overview of recent methods
    for model-based deep reinforcement learning. We describe methods that use (1)
    explicit planning on given transitions, (2) explicit planning on a learned transition
    model, and (3) end-to-end learning of both planning and transitions. For each
    approach future directions are listed (specifically: latent models, uncertainty
    modeling, curriculum learning and multi-agent benchmarks).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的贡献在于深入概述了模型驱动深度强化学习的最新方法。我们描述了使用（1）在给定转移上进行显式规划，（2）在学习到的转移模型上进行显式规划，以及（3）对规划和转移进行端到端学习的方法。每种方法的未来方向都列出了（具体为：潜在模型、不确定性建模、课程学习和多智能体基准）。
- en: Many research papers have been published recently, and the field of model-based
    deep reinforcement learning is advancing rapidly. The papers in this survey are
    selected on recency and impact on the field, for different applications, highlighting
    relationships between papers. Since our focus is on recent work, some of the references
    are to preprints in arXiv (of reputable groups). Excellent works with necessary
    background information exist for reinforcement learning (Sutton and Barto, [2018](#bib.bib159)),
    deep learning (Goodfellow et al., [2016](#bib.bib50)), machine learning (Bishop,
    [2006](#bib.bib17)), and artificial intelligence (Russell and Norvig, [2016](#bib.bib139)).
    As we mentioned, the main purpose of the current survey is to focus on deep learning
    methods, with high-capacity models. Previous surveys provide an overview of the
    uses of classic (non-deep) model-based methods (Deisenroth et al., [2013b](#bib.bib33);
    Kober et al., [2013](#bib.bib96); Kaelbling et al., [1996](#bib.bib84)). Other
    relevant surveys into model-based reinforcement learning are (Justesen et al.,
    [2019](#bib.bib83); Polydoros and Nalpantidis, [2017](#bib.bib134); Hui, [2018](#bib.bib73);
    Wang et al., [2019b](#bib.bib175); Çalışır and Pehlivanoğlu, [2019](#bib.bib23);
    Moerland et al., [2020b](#bib.bib119)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最近发表了许多研究论文，模型驱动的深度强化学习领域正在迅速发展。本调查中的论文基于其时效性和对该领域的影响进行筛选，涵盖了不同的应用，突出了论文之间的关系。由于我们的重点是近期的工作，一些参考文献是来自arXiv的预印本（由有声望的团队提供）。强化学习的优秀工作（Sutton
    and Barto，[2018](#bib.bib159)），深度学习（Goodfellow et al., [2016](#bib.bib50)），机器学习（Bishop，[2006](#bib.bib17)），以及人工智能（Russell
    and Norvig，[2016](#bib.bib139)）提供了必要的背景信息。正如我们所提到的，本次调查的主要目的是聚焦于深度学习方法和高容量模型。之前的调查提供了经典（非深度）模型驱动方法的概述（Deisenroth
    et al., [2013b](#bib.bib33); Kober et al., [2013](#bib.bib96); Kaelbling et al.,
    [1996](#bib.bib84)）。其他相关的模型驱动强化学习调查有（Justesen et al., [2019](#bib.bib83); Polydoros
    and Nalpantidis，[2017](#bib.bib134); Hui，[2018](#bib.bib73); Wang et al., [2019b](#bib.bib175);
    Çalışır and Pehlivanoğlu，[2019](#bib.bib23); Moerland et al., [2020b](#bib.bib119)）。
- en: The remainder of this survey is structured as follows. Section [2](#S2 "2 Background
    ‣ Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey")
    provides necessary background and a familiar formalism of reinforcement learning.
    Section [3](#S3 "3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based
    Deep Reinforcement Learning for High-Dimensional Problems, a Survey") then surveys
    recent papers in the field of model-based deep reinforcement learning. Section [4](#S4
    "4 Benchmarks ‣ Model-Based Deep Reinforcement Learning for High-Dimensional Problems,
    a Survey") introduces the main benchmarks of the field. Section [5](#S5 "5 Discussion
    and Outlook ‣ Model-Based Deep Reinforcement Learning for High-Dimensional Problems,
    a Survey") provides a discussion reflecting on the different approaches and provides
    open problems and future work. Section [6](#S6 "6 Conclusion ‣ Model-Based Deep
    Reinforcement Learning for High-Dimensional Problems, a Survey") concludes the
    survey.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的其余部分结构如下。第 [2](#S2 "2 背景 ‣ 基于模型的深度强化学习在高维问题中的应用：一项调查") 节提供了必要的背景知识和强化学习的基本形式。第
    [3](#S3 "3 基于模型的深度强化学习调查 ‣ 基于模型的深度强化学习在高维问题中的应用：一项调查") 节回顾了基于模型的深度强化学习领域的最新论文。第
    [4](#S4 "4 基准 ‣ 基于模型的深度强化学习在高维问题中的应用：一项调查") 节介绍了该领域的主要基准。第 [5](#S5 "5 讨论与展望 ‣
    基于模型的深度强化学习在高维问题中的应用：一项调查") 节讨论了不同的方法，并提出了开放问题和未来的研究方向。第 [6](#S6 "6 结论 ‣ 基于模型的深度强化学习在高维问题中的应用：一项调查")
    节总结了本次调查。
- en: <svg   height="128.7" overflow="visible" version="1.1" width="241.65"><g transform="translate(0,128.7)
    matrix(1 0 0 -1 0 0) translate(122.32,0) translate(0,15.14)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -39.13 14.96)" fill="#000000"
    stroke="#000000"><foreignobject width="78.26" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Environment</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -18.06 75.36)" fill="#000000" stroke="#000000"><foreignobject width="36.13"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Agent</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -76.88 45.16)" fill="#000000" stroke="#000000"><foreignobject
    width="8.76" height="8.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$r^{\prime}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -117.71 45.16)" fill="#000000" stroke="#000000"><foreignobject
    width="8.62" height="8.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$s^{\prime}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 107.4 46.23)" fill="#000000" stroke="#000000"><foreignobject
    width="7.31" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$a$</foreignobject></g></g></svg>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="128.7" overflow="visible" version="1.1" width="241.65"><g transform="translate(0,128.7)
    matrix(1 0 0 -1 0 0) translate(122.32,0) translate(0,15.14)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -39.13 14.96)" fill="#000000"
    stroke="#000000"><foreignobject width="78.26" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">环境</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -18.06 75.36)" fill="#000000" stroke="#000000"><foreignobject width="36.13"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">代理</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -76.88 45.16)" fill="#000000" stroke="#000000"><foreignobject
    width="8.76" height="8.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$r^{\prime}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -117.71 45.16)" fill="#000000" stroke="#000000"><foreignobject
    width="8.62" height="8.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$s^{\prime}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 107.4 46.23)" fill="#000000" stroke="#000000"><foreignobject
    width="7.31" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$a$</foreignobject></g></g></svg>
- en: 'Figure 1: Reinforcement Learning: Agent Acting on Environment, that provides
    new State and Reward to the Agent'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：强化学习：代理在环境中行动，环境为代理提供新的状态和奖励
- en: 2 Background
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: Reinforcement learning does not assume the presence of a database, as supervised
    learning does. Instead, it derives the ground truth from an internal model or
    from an external environment that can be queried by the learning agent, see Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey"). The environment provides a new state $s^{\prime}$ and its
    reward $r^{\prime}$ (label) for every action $a$ that the agent tries in a certain
    state $s$ (Sutton and Barto, [2018](#bib.bib159)). In this way, as many action-reward
    pairs can be generated as needed, without a large hand-labeled database. Also,
    we can learn behavior beyond that what a supervisor prepared for us to learn.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习不像监督学习那样假设存在一个数据库。相反，它从内部模型或可以被学习代理查询的外部环境中推导出真实情况，见图[1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey")。环境为代理在某个状态$s$尝试的每个动作$a$提供一个新的状态$s^{\prime}$及其奖励$r^{\prime}$（标签）（Sutton
    and Barto, [2018](#bib.bib159)）。通过这种方式，可以根据需要生成任意多的动作-奖励对，而无需大型手工标注的数据库。此外，我们还可以学习到超出监督者为我们准备的知识的行为。
- en: As so much of artificial intelligence, reinforcement learning draws inspiration
    from principles of human and animal learning (Hamrick, [2019](#bib.bib61); Kahneman,
    [2011](#bib.bib85)). In psychology, learning is studied as behaviorial adaptation,
    as a result of reinforcing reward and punishment. Publications in artificial intelligence
    sometimes explicitly reference analogies in how learning in the two fields is
    described (Anthony et al., [2017](#bib.bib3); Duan et al., [2016](#bib.bib38);
    Weng, [2018](#bib.bib178)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如人工智能的许多领域一样，强化学习从人类和动物学习的原则中汲取灵感（Hamrick, [2019](#bib.bib61); Kahneman, [2011](#bib.bib85)）。在心理学中，学习被研究为行为适应，作为奖励和惩罚的结果。人工智能领域的出版物有时明确引用了这两个领域中学习描述的类比（Anthony
    et al., [2017](#bib.bib3); Duan et al., [2016](#bib.bib38); Weng, [2018](#bib.bib178)）。
- en: Supervised learning frequently studies regression and classification problems.
    In reinforcement learning most problems are decision and control problems. Often
    problems are sequential decision problems, in which a goal is reached after a
    sequence of decisions are taken (behavior). In sequential decision making, the
    dynamics of the world are taken into consideration. Sequential decision making
    is a step-by-step approach in which earlier decisions influence later decisions.
    Before we continue, let us formalize key concepts in reinforcement learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通常研究回归和分类问题。在强化学习中，大多数问题是决策和控制问题。通常问题是序列决策问题，在这种问题中，经过一系列决策后实现目标（行为）。在序列决策中，世界的动态被考虑在内。序列决策是一种逐步的方法，其中早期的决策会影响后续的决策。在继续之前，让我们对强化学习中的关键概念进行形式化。
- en: 2.1 Formalizing Reinforcement Learning
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 强化学习的形式化
- en: 'Reinforcement learning problems are often modeled formally as a Markov Decision
    Process (MDP). First we introduce the basics: state, action, transition and reward.
    Then we introduce policy and value. Finally, we define model-based and model-free
    solution approaches.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习问题通常被正式建模为马尔可夫决策过程（MDP）。首先，我们介绍基本概念：状态、动作、转移和奖励。然后我们介绍策略和值。最后，我们定义基于模型和无模型的解决方法。
- en: <svg   height="109.3" overflow="visible" version="1.1" width="149.1"><g transform="translate(0,109.3)
    matrix(1 0 0 -1 0 0) translate(65.49,0) translate(0,87.41)"><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5.91 0)" color="#000000"><foreignobject
    width="11.81" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><g
    stroke="#000000" fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.24 11.32)" fill="#000000" stroke="#000000"><foreignobject width="6.49"
    height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$s$</foreignobject></g><g
    stroke="#000000" fill="#000000" color="#FFFFFF"><path d="M -43.32 -39.37 C -43.32
    -37.2 -45.08 -35.45 -47.24 -35.45 C -49.41 -35.45 -51.17 -37.2 -51.17 -39.37 C
    -51.17 -41.54 -49.41 -43.29 -47.24 -43.29 C -45.08 -43.29 -43.32 -41.54 -43.32
    -39.37 Z M -47.24 -39.37"></path></g><path d="M -4.94 -4.12 L -42.96 -35.8" style="fill:none"><g
    transform="matrix(-0.76822 -0.64018 0.64018 -0.76822 -42.96 -35.8)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.8pt" color="#000000"><path
    d="M -53.17 -78.74 C -53.17 -75.49 -55.81 -72.86 -59.06 -72.86 C -62.3 -72.86
    -64.94 -75.49 -64.94 -78.74 C -64.94 -81.99 -62.3 -84.62 -59.06 -84.62 C -55.81
    -84.62 -53.17 -81.99 -53.17 -78.74 Z M -59.06 -78.74" style="fill:none"></path></g><path
    d="M -48.45 -43.39 L -56.81 -71.25" style="fill:none"><g transform="matrix(-0.28735
    -0.95782 0.95782 -0.28735 -56.81 -71.25)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g
    fill="#000000" stroke="#000000" stroke-width="0.8pt" color="#000000"><path d="M
    -29.55 -78.74 C -29.55 -75.49 -32.18 -72.86 -35.43 -72.86 C -38.68 -72.86 -41.32
    -75.49 -41.32 -78.74 C -41.32 -81.99 -38.68 -84.62 -35.43 -84.62 C -32.18 -84.62
    -29.55 -81.99 -29.55 -78.74 Z M -35.43 -78.74" style="fill:none"></path></g><path
    d="M -46.04 -43.39 L -37.68 -71.25" style="fill:none"><g transform="matrix(0.28735
    -0.95782 0.95782 0.28735 -37.68 -71.25)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g
    stroke="#000000" fill="#000000" color="#FFFFFF"><path d="M 3.92 -39.37 C 3.92
    -37.2 2.17 -35.45 0 -35.45 C -2.17 -35.45 -3.92 -37.2 -3.92 -39.37 C -3.92 -41.54
    -2.17 -43.29 0 -43.29 C 2.17 -43.29 3.92 -41.54 3.92 -39.37 Z M 0 -39.37"></path></g><path
    d="M 0 -6.44 L 0 -33.79" style="fill:none"><g transform="matrix(0.0 -1.0 1.0 0.0
    0 -33.79)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g fill="#000000" stroke="#000000"
    stroke-width="0.8pt" color="#000000"><path d="M -5.93 -78.74 C -5.93 -75.49 -8.56
    -72.86 -11.81 -72.86 C -15.06 -72.86 -17.69 -75.49 -17.69 -78.74 C -17.69 -81.99
    -15.06 -84.62 -11.81 -84.62 C -8.56 -84.62 -5.93 -81.99 -5.93 -78.74 Z M -11.81
    -78.74" style="fill:none"></path></g><path d="M -1.21 -43.39 L -9.56 -71.25" style="fill:none"><g
    transform="matrix(-0.28735 -0.95782 0.95782 -0.28735 -9.56 -71.25)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.8pt" color="#000000"><path
    d="M 17.69 -78.74 C 17.69 -75.49 15.06 -72.86 11.81 -72.86 C 8.56 -72.86 5.93
    -75.49 5.93 -78.74 C 5.93 -81.99 8.56 -84.62 11.81 -84.62 C 15.06 -84.62 17.69
    -81.99 17.69 -78.74 Z M 11.81 -78.74" style="fill:none"></path></g><path d="M
    1.21 -43.39 L 9.56 -71.25" style="fill:none"><g transform="matrix(0.28735 -0.95782
    0.95782 0.28735 9.56 -71.25)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 4.89 -23.78)" fill="#000000" stroke="#000000"><foreignobject
    width="7.89" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\pi$</foreignobject></g><g
    stroke="#000000" fill="#000000" color="#FFFFFF"><path d="M 51.17 -39.37 C 51.17
    -37.2 49.41 -35.45 47.24 -35.45 C 45.08 -35.45 43.32 -37.2 43.32 -39.37 C 43.32
    -41.54 45.08 -43.29 47.24 -43.29 C 49.41 -43.29 51.17 -41.54 51.17 -39.37 Z M
    47.24 -39.37"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 43.59 -30.28)" fill="#000000"
    stroke="#000000"><foreignobject width="7.31" height="5.96" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$a$</foreignobject></g><path d="M 4.94 -4.12
    L 42.96 -35.8" style="fill:none"><g transform="matrix(0.76822 -0.64018 0.64018
    0.76822 42.96 -35.8)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g fill="#000000" stroke="#000000"
    stroke-width="0.8pt" color="#000000"><path d="M 41.32 -78.74 C 41.32 -75.49 38.68
    -72.86 35.43 -72.86 C 32.18 -72.86 29.55 -75.49 29.55 -78.74 C 29.55 -81.99 32.18
    -84.62 35.43 -84.62 C 38.68 -84.62 41.32 -81.99 41.32 -78.74 Z M 35.43 -78.74"
    style="fill:none"></path></g><path d="M 46.04 -43.39 L 37.68 -71.25" style="fill:none"><g
    transform="matrix(-0.28735 -0.95782 0.95782 -0.28735 37.68 -71.25)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.8pt" color="#000000"><path
    d="M 64.94 -78.74 C 64.94 -75.49 62.3 -72.86 59.06 -72.86 C 55.81 -72.86 53.17
    -75.49 53.17 -78.74 C 53.17 -81.99 55.81 -84.62 59.06 -84.62 C 62.3 -84.62 64.94
    -81.99 64.94 -78.74 Z M 59.06 -78.74" style="fill:none"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 70.38 -82.8)" fill="#000000" stroke="#000000"><foreignobject width="8.62"
    height="8.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$s^{\prime}$</foreignobject></g><path
    d="M 48.45 -43.39 L 56.81 -71.25" style="fill:none"><g transform="matrix(0.28735
    -0.95782 0.95782 0.28735 56.81 -71.25)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 57.72 -60.96)" fill="#000000" stroke="#000000"><foreignobject
    width="6.63" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$r$</foreignobject></g></path></path></path></path></path></path></path></path></path></g>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="109.3" overflow="visible" version="1.1" width="149.1"><g transform="translate(0,109.3)
    matrix(1 0 0 -1 0 0) translate(65.49,0) translate(0,87.41)"><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -5.91 0)" color="#000000"><foreignobject
    width="11.81" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><g
    stroke="#000000" fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.24 11.32)" fill="#000000" stroke="#000000"><foreignobject width="6.49"
    height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$s$</foreignobject></g><g
    stroke="#000000" fill="#000000" color="#FFFFFF"><path d="M -43.32 -39.37 C -43.32
    -37.2 -45.08 -35.45 -47.24 -35.45 C -49.41 -35.45 -51.17 -37.2 -51.17 -39.37 C
    -51.17 -41.54 -49.41 -43.29 -47.24 -43.29 C -45.08 -43.29 -43.32 -41.54 -43.32
    -39.37 Z M -47.24 -39.37"></path></g><path d="M -4.94 -4.12 L -42.96 -35.8" style="fill:none"><g
    transform="matrix(-0.76822 -0.64018 0.64018 -0.76822 -42.96 -35.8)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.8pt" color="#000000"><path
    d="M -53.17 -78.74 C -53.17 -75.49 -55.81 -72.86 -59.06 -72.86 C -62.3 -72.86
    -64.94 -75.49 -64.94 -78.74 C -64.94 -81.99 -62.3 -84.62 -59.06 -84.62 C -55.81
    -84.62 -53.17 -81.99 -53.17 -78.74 Z M -59.06 -78.74" style="fill:none"></path></g><path
    d="M -48.45 -43.39 L -56.81 -71.25" style="fill:none"><g transform="matrix(-0.28735
    -0.95782 0.95782 -0.28735 -56.81 -71.25)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g
    fill="#000000" stroke="#000000" stroke-width="0.8pt" color="#000000"><path d="M
    -29.55 -78.74 C -29.55 -75.49 -32.18 -72.86 -35.43 -72.86 C -38.68 -72.86 -41.32
    -75.49 -41.32 -78.74 C -41.32 -81.99 -38.68 -84.62 -35.43 -84.62 C -32.18 -84.62
    -29.55 -81.99 -29.55 -78.74 Z M -35.43 -78.74" style="fill:none"></path></g><path
    d="M -46.04 -43.39 L -37.68 -71.25" style="fill:none"><g transform="matrix(0.28735
    -0.95782 0.95782 0.28735 -37.68 -71.25)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C
    -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g
    stroke="#000000" fill="#000000" color="#FFFFFF"><path d="M 3.92 -39.37 C 3.92
    -37.2 2.17 -35.45 0 -35.45 C -2.17 -35.45 -3.92 -37.2 -3.92 -39.37 C -3.92 -41.54
    -2.17 -43.29 0 -43.29 C 2.17 -43.29 3.92 -41.54 3.92 -39.37 Z M 0 -39.37"></path></g><path
    d="M 0 -6.44 L 0 -33.79" style="fill:none"><g transform="matrix(0.0 -1.0 1.0 0.0
    0 -33.79)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="round"><path
    d="M 1.11 0 C -0.28 0.28 -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8
    C -1.66 -0.83 -0.28 -0.28 1.11 0 Z"></path></g><g fill="#000000" stroke="#000000"
    stroke-width="0.8pt" color="#000000"><path d="M -5.93 -78.74 C -5.93 -75.49 -8.56
    -72.86 -11.81 -72.86 C -15.06 -72.86 -17.69 -75.49 -17.69 -78.74 C -17.69 -81.99
    -15.06 -84.62 -11.81 -84.62 C -8.56 -84.62 -5.93 -81.99 -5.93 -78.74 Z M -11.81
    -78.74" style="fill:none"></path></g><path d="M -1.21 -43.39 L -9.56 -71.25" style="fill:none"><g
    transform="matrix(-0.28735 -0.95782 0.95782 -0.28735 -9.56 -71.25)" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="round"><path d="M 1.11 0 C -0.28 0.28
    -1.66 0.83 -3.32 1.8 C -1.66 0.55 -1.66 -0.55 -3.32 -1.8 C -1.66 -0.83 -0.28 -0.28
    1.11 0 Z"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.8pt" color="#000000"><path
    d="M 17.69 -78.74 C 17.69 -75.49 15.06 -72.86 11.81 -72.86 C 8.56 -72.86 5.93
    -75.49 5.93 -78.74 C 5.93 -81.99 8.56 -84.62 11.81 -84.62 C 15.06 -84.62 17.69
    -81.99 17.69 -78.74 Z M 11.81 -78.74" style="fill:none"></path></g><path d="M
    1.21 -43.39 L 9.56 -71.25" style="fill:none"><g transform="matrix(0.28735 -0.95782
- en: 'Figure 2: Backup Diagram (Sutton and Barto, [2018](#bib.bib159)). Maximizing
    the reward for state $s$ is done by following the transition function to find
    the next state $s^{\prime}$. Note that the policy $\pi(s,a)$ tells the first half
    of this story, going from $s\rightarrow a$; the transition function $T_{a}(s,s^{\prime})$
    completes the story, going from $s\rightarrow s^{\prime}$ (via $a$).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：备份图（Sutton和Barto，[2018](#bib.bib159)）。通过遵循转移函数以找到下一个状态$s^{\prime}$来最大化状态$s$的奖励。请注意，策略$\pi(s,a)$讲述了这个故事的一半，从$s\rightarrow
    a$；转移函数$T_{a}(s,s^{\prime})$完成了这个故事，从$s\rightarrow s^{\prime}$（通过$a$）。
- en: 'A Markov Decision Process is a 4-tuple $(S,A,T_{a},R_{a})$ where $S$ is a finite
    set of states, $A$ is a finite set of actions; $A_{s}\subseteq A$ is the set of
    actions available from state $s$. Furthermore, $T_{a}$ is the transition function:
    $T_{a}(s,s^{\prime})$ is the probability that action $a$ in state $s$ at time
    $t$ will lead to state $s^{\prime}$ at time $t+1$. Finally, $R_{a}(s,s^{\prime})$
    is the immediate reward received after transitioning from state $s$ to state $s^{\prime}$
    due to action $a$. The goal of an MDP is to find the best decision, or action,
    in all states $s\in S$.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程是一个4元组$(S,A,T_{a},R_{a})$，其中$S$是一个有限的状态集合，$A$是一个有限的动作集合；$A_{s}\subseteq
    A$是从状态$s$出发的可用动作集合。此外，$T_{a}$是转移函数：$T_{a}(s,s^{\prime})$是动作$a$在时间$t$状态$s$下导致在时间$t+1$转移到状态$s^{\prime}$的概率。最后，$R_{a}(s,s^{\prime})$是由于动作$a$从状态$s$转移到状态$s^{\prime}$后获得的即时奖励。MDP的目标是找到在所有状态$s\in
    S$中的最佳决策或动作。
- en: 'The goal of reinforcement learning is to find the optimal policy $a=\pi^{\star}(s)$,
    which is the function that gives the best action $a$ in all states $s\in S$. The
    policy contains the actions of the answer to a sequential decision problem: a
    step-by-step prescription of which action must be taken in which state, in order
    to maximize reward for any given state. This policy can be found directly—model-free—or
    with the help of a transition model—model-based. Figure [2](#S2.F2 "Figure 2 ‣
    2.1 Formalizing Reinforcement Learning ‣ 2 Background ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey") shows a diagram of the transitions.
    More formally, the goal of an MDP is to find policy $\pi(s)$ that chooses an action
    in state $s$ that will maximize the reward. This value $V$ is the expected sum
    of future rewards $V^{\pi}(s)=E(\sum_{t=0}^{\infty}\gamma^{t}R_{\pi(s_{t})}(s_{t},s_{t+1}))$
    that are discounted with parameter $\gamma$ over $t$ time periods, with $s=s_{0}$.
    The function $V^{\pi}(s)$ is called the value function of the state. In deep learning
    the policy $\pi$ is determined by the parameters $\theta$ (or weights) of a neural
    network, and the parameterized policy is written as $\pi_{\theta}$.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是找到最优策略$a=\pi^{\star}(s)$，这是一个在所有状态$s\in S$中给出最佳动作$a$的函数。策略包含了对顺序决策问题的答案：逐步说明在每个状态下应采取哪个动作，以最大化任何给定状态的奖励。这个策略可以直接找到——无模型——或者借助转移模型——基于模型。图[2](#S2.F2
    "图 2 ‣ 2.1 强化学习的形式化 ‣ 2 背景 ‣ 基于模型的深度强化学习在高维问题中的调查")展示了转移的示意图。更正式地说，MDP的目标是找到策略$\pi(s)$，该策略在状态$s$下选择一个动作以最大化奖励。这个值$V$是未来奖励的期望总和$V^{\pi}(s)=E(\sum_{t=0}^{\infty}\gamma^{t}R_{\pi(s_{t})}(s_{t},s_{t+1}))$，其中参数$\gamma$用于折扣，时间周期为$t$，且$s=s_{0}$。函数$V^{\pi}(s)$称为状态的价值函数。在深度学习中，策略$\pi$由神经网络的参数$\theta$（或权重）决定，参数化的策略表示为$\pi_{\theta}$。
- en: 'There are algorithms to compute the policy $\pi$ directly, and there are algorithms
    that first compute this function $V^{\pi}(s)$. For stochastic problems often direct
    policy methods work best, for deterministic problems the value-methods are most
    often used (Kaelbling et al., [1996](#bib.bib84)). (A third, quite popular, approach
    combines the best of value and policy methods: actor-critic (Sutton and Barto,
    [2018](#bib.bib159); Konda and Tsitsiklis, [2000](#bib.bib98); Mnih et al., [2016](#bib.bib116)).)
    In classical, table-based, reinforcement learning there is a close relation between
    policy and value, since the best action of a state leads to both the best policy
    and the best value, and finding the other can usually be done with a simple lookup.
    When the value and policy function are approximated, for example with a neural
    network, then this relation becomes weaker, and many advanced policy and value
    algorithms have been devised for deep reinforcement learning.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有些算法可以直接计算策略 $\pi$，而有些算法则先计算该函数 $V^{\pi}(s)$。对于随机问题，通常直接策略方法效果最佳，而对于确定性问题，则最常使用价值方法
    (Kaelbling 等，[1996](#bib.bib84))。 (第三种相当流行的方法结合了价值和策略方法的优点：actor-critic (Sutton
    和 Barto，[2018](#bib.bib159)；Konda 和 Tsitsiklis，[2000](#bib.bib98)；Mnih 等，[2016](#bib.bib116))。)
    在经典的基于表格的强化学习中，策略和价值之间存在紧密关系，因为一个状态的最佳动作同时导致最佳策略和最佳价值，找到另一者通常可以通过简单的查找来完成。当使用神经网络等方法来近似价值和策略函数时，这种关系会变弱，因此为深度强化学习设计了许多先进的策略和价值算法。
- en: Value function algorithms calculate the state-action value $Q^{\pi}(s,a)$. This
    $Q$-function gives the expected sum of discounted rewards when following action
    $a$ in state $s$, and then afterwards policy $\pi$. The value $V(s)$ is the maximum
    of the $Q(s,a)$-values of that state. The optimal value-function is denoted as
    $V^{\star}(s)$. The optimal policy can be found by recursively choosing the argmax
    action with $Q(s,a)=V^{\star}(s)$ in each state.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数算法计算状态-动作值 $Q^{\pi}(s,a)$。这个 $Q$-函数给出了在状态 $s$ 下采取动作 $a$，然后按照策略 $\pi$ 的折扣奖励期望和。值
    $V(s)$ 是该状态下 $Q(s,a)$-值的最大值。最优价值函数记作 $V^{\star}(s)$。可以通过递归选择每个状态下的 argmax 动作 $Q(s,a)=V^{\star}(s)$
    来找到最优策略。
- en: To find the policy by planning, models for $T$ and $R$ must be known. When they
    are not known, an environment is assumed to be present for the agent to query
    in order to get the necessary reinforcing information, see Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey"), after Sutton and Barto ([2018](#bib.bib159)). The samples
    can be used to build the model of $T$ and $R$ (model-based reinforcement learning)
    or they can be used to find the policy without first building the model (direct
    or model-free reinforcement learning). When sampling, the environment is in a
    known state $s$, and the agent chooses an action $a$ which it transmits to the
    environment, that responds with a new state $s^{\prime}$ and the corresponding
    reward value $r^{\prime}=R_{a}(s,s^{\prime})$.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过规划找到策略，必须知道 $T$ 和 $R$ 的模型。当它们未知时，假设环境存在，供代理查询以获取必要的强化信息，见图 [1](#S1.F1 "图
    1 ‣ 1 引言 ‣ 基于模型的深度强化学习在高维问题中的应用调研")，参考 Sutton 和 Barto ([2018](#bib.bib159))。这些样本可以用于建立
    $T$ 和 $R$ 的模型（基于模型的强化学习），或者可以在不先建立模型的情况下找到策略（直接或无模型强化学习）。在采样时，环境处于已知状态 $s$，代理选择一个动作
    $a$ 传递给环境，环境则响应一个新状态 $s^{\prime}$ 和相应的奖励值 $r^{\prime}=R_{a}(s,s^{\prime})$。
- en: The literature provides many solution algorithms. We now very briefly discuss
    classical planning and model-free approaches, before we continue to survey model-based
    algorithms in more depth in the next section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 文献提供了许多解决算法。我们现在非常简要地讨论经典规划和无模型方法，然后在下一节中深入探讨基于模型的算法。
- en: 2.2 Planning
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 规划
- en: Planning algorithms use the transition model to find the optimal policy, by
    selecting actions in states, looking ahead, and backing up reward values, see
    Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Formalizing Reinforcement Learning ‣ 2 Background
    ‣ Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey")
    and Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Planning ‣ 2 Background ‣ Model-Based Deep
    Reinforcement Learning for High-Dimensional Problems, a Survey").
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 规划算法使用过渡模型来寻找**最优策略**，通过在状态中选择动作、展望未来以及备份奖励值，见图 [2](#S2.F2 "图 2 ‣ 2.1 正式化强化学习
    ‣ 2 背景 ‣ 基于模型的深度强化学习在高维问题中的应用调研") 和图 [3](#S2.F3 "图 3 ‣ 2.2 规划 ‣ 2 背景 ‣ 基于模型的深度强化学习在高维问题中的应用调研")。
- en: <svg   height="83.23" overflow="visible" version="1.1" width="297.79"><g transform="translate(0,83.23)
    matrix(1 0 0 -1 0 0) translate(156.58,0) translate(0,-7.6)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -52.14 14.88)" fill="#000000"
    stroke="#000000"><foreignobject width="104.28" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Transition Model</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -38.92 75.28)" fill="#000000" stroke="#000000"><foreignobject width="78.99"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Policy/Value</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -151.97 45.75)" fill="#000000" stroke="#000000"><foreignobject
    width="52.27" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">planning</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 99.69 45.94)" fill="#000000" stroke="#000000"><foreignobject
    width="36.9" height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">acting</foreignobject></g></g></svg>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="83.23" overflow="visible" version="1.1" width="297.79"><g transform="translate(0,83.23)
    matrix(1 0 0 -1 0 0) translate(156.58,0) translate(0,-7.6)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -52.14 14.88)" fill="#000000"
    stroke="#000000"><foreignobject width="104.28" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">过渡模型</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -38.92 75.28)" fill="#000000" stroke="#000000"><foreignobject width="78.99"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">策略/价值</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -151.97 45.75)" fill="#000000" stroke="#000000"><foreignobject
    width="52.27" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">规划</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 99.69 45.94)" fill="#000000" stroke="#000000"><foreignobject
    width="36.9" height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">行动</foreignobject></g></g></svg>
- en: 'Figure 3: Planning'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：规划
- en: Algorithm 1 Value Iteration
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 价值迭代
- en: Initialize $V(s)$ to arbitrary valuesrepeat     for all $s$ do         for all $a$ do              $Q[s,a]=\sum_{s^{\prime}}T_{a}(s,s^{\prime})(R_{a}(s,s^{\prime})+\gamma
    V(s^{\prime}))$         end for         $V[s]=\max_{a}(Q[s,a])$     end foruntil V
    convergesreturn V
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 $V(s)$ 为任意值重复     对所有 $s$ 执行         对所有 $a$ 执行              $Q[s,a]=\sum_{s^{\prime}}T_{a}(s,s^{\prime})(R_{a}(s,s^{\prime})+\gamma
    V(s^{\prime}))$         结束 对         $V[s]=\max_{a}(Q[s,a])$     结束 直到 $V$ 收敛返回
    $V$
- en: In planning algorithms, the agent has access to an explicit transition and reward
    model. In the deterministic case the transition model provides the next state
    for each of the possible actions in the states, it is a function $s^{\prime}=T_{a}(s)$.
    In the stochastic case, it provides the probability distribution $T_{a}(s,s^{\prime})$.
    The reward model provides the immediate reward for transitioning from state $s$
    to state $s^{\prime}$ after taking action $a$. Figure [2](#S2.F2 "Figure 2 ‣ 2.1
    Formalizing Reinforcement Learning ‣ 2 Background ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey") provides a backup diagram for
    the transition and reward function. The transition function moves downward in
    the diagram from state $s$ to $s^{\prime}$, and the reward value goes upward in
    the diagram, backing up the value from the child state to the parent state. The
    transition function follows policy $\pi$ with action $a$, after which state $s^{\prime}$
    is chosen with probability $p$, yielding reward $r^{\prime}$. The policy function
    $\pi(s,a)$ concerns the top layer of the diagram, from $s$ to $a$. The transition
    function $T_{a}(s,s^{\prime})$ covers both layers, from $s$ to $s^{\prime}$. In
    some domains, such as chess, there is a single deterministic state $s^{\prime}$
    for each action $a$. Here each move leads to a single board position, simplifying
    the backup diagram.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在规划算法中，代理有权访问明确的过渡和奖励模型。在确定性情况下，过渡模型提供每个可能动作在状态中的下一个状态，它是一个函数 $s^{\prime}=T_{a}(s)$。在随机情况下，它提供概率分布
    $T_{a}(s,s^{\prime})$。奖励模型提供从状态 $s$ 转移到状态 $s^{\prime}$ 后采取动作 $a$ 的即时奖励。图 [2](#S2.F2
    "图 2 ‣ 2.1 强化学习的形式化 ‣ 2 背景 ‣ 基于模型的深度强化学习在高维问题中的调查") 提供了过渡和奖励函数的备份图。过渡函数在图中从状态
    $s$ 向 $s^{\prime}$ 移动，奖励值在图中向上移动，从子状态向父状态备份。过渡函数遵循策略 $\pi$ 和动作 $a$，然后状态 $s^{\prime}$
    以概率 $p$ 被选择，产生奖励 $r^{\prime}$。策略函数 $\pi(s,a)$ 关注图的顶层，从 $s$ 到 $a$。过渡函数 $T_{a}(s,s^{\prime})$
    涵盖了两个层次，从 $s$ 到 $s^{\prime}$。在一些领域，例如国际象棋，每个动作 $a$ 都有一个确定性的状态 $s^{\prime}$。在这里，每一步都导致一个唯一的棋盘位置，从而简化了备份图。
- en: Together, the transition and reward functions implicitly define a space of states
    that can be searched for the optimal policy $\pi^{\star}$ and value $V^{\star}$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 过渡和奖励函数隐式定义了一个状态空间，可以在其中搜索最优策略 $\pi^{\star}$ 和价值 $V^{\star}$。
- en: The most basic form of planning is Bellman’s dynamic programming (Bellman, [1957,
    2013](#bib.bib12)), a recursive traversal of the state and action space. Value
    iteration is a well-known, very basic, dynamic programming method. The pseudo-code
    for value iteration is shown in Algorithm [1](#alg1 "Algorithm 1 ‣ 2.2 Planning
    ‣ 2 Background ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey") (Alpaydin, [2020](#bib.bib2)). It traverses all actions in
    all states, computing the value of the entire state space.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 规划的最基本形式是贝尔曼的动态规划（Bellman, [1957, 2013](#bib.bib12)），这是一种状态和动作空间的递归遍历。值迭代是一个著名且非常基本的动态规划方法。值迭代的伪代码见于算法[1](#alg1
    "Algorithm 1 ‣ 2.2 Planning ‣ 2 Background ‣ Model-Based Deep Reinforcement Learning
    for High-Dimensional Problems, a Survey")（Alpaydin, [2020](#bib.bib2)）。它遍历所有状态中的所有动作，计算整个状态空间的值。
- en: Many planning algorithms have been devised to efficiently generate and traverse
    state spaces, such as (depth-limited) A*, alpha-beta and Monte Carlo Tree Search
    (MCTS) (Hart et al., [1968](#bib.bib63); Pearl, [1984](#bib.bib131); Korf, [1985](#bib.bib99);
    Plaat et al., [1996](#bib.bib133); Browne et al., [2012](#bib.bib21); Moerland
    et al., [2018](#bib.bib117), [2020c](#bib.bib120)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 许多规划算法被设计用于高效生成和遍历状态空间，例如（深度限制）A*、alpha-beta 和蒙特卡罗树搜索（MCTS）（Hart et al., [1968](#bib.bib63);
    Pearl, [1984](#bib.bib131); Korf, [1985](#bib.bib99); Plaat et al., [1996](#bib.bib133);
    Browne et al., [2012](#bib.bib21); Moerland et al., [2018](#bib.bib117), [2020c](#bib.bib120)）。
- en: Planning algorithms originated from exact, table-based, algorithms (Sutton and
    Barto, [2018](#bib.bib159)) that fit in the symbolic AI tradition. For planning
    it is relevant to know how much of the state space must be traversed to find the
    optimal policy. When state spaces are too large to search fully, deep function
    approximation algorithms can be used to approximate the optimal policy and value (Sutton
    and Barto, [2018](#bib.bib159); Plaat, [2020](#bib.bib132)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 规划算法起源于精确的基于表格的算法（Sutton and Barto, [2018](#bib.bib159)），这些算法符合符号AI传统。在规划中，了解需要遍历多少状态空间才能找到最优策略是很重要的。当状态空间过大而无法完全搜索时，可以使用深度函数逼近算法来近似最优策略和值（Sutton
    and Barto, [2018](#bib.bib159); Plaat, [2020](#bib.bib132)）。
- en: Planning is sample-efficient in the sense that, when the agent has a model,
    a policy can be found without interaction with the environment. Sampling may be
    costly, and sample efficiency is an important concept in reinforcement learning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 规划在样本效率方面是有效的，即当智能体拥有模型时，可以在无需与环境交互的情况下找到策略。采样可能成本高昂，样本效率是强化学习中的一个重要概念。
- en: A sampling action taken in an environment is irreversible, since state changes
    of the environment can not be undone by the agent. In contrast, a planning action
    taken in a transition model is reversible (Moerland et al., [2020a](#bib.bib118)).
    A planning agent can backtrack, a sampling agent cannot. Sampling finds local
    optima easily. For finding global optima the ability to backtrack out of a local
    optimum is useful, which is an advantage for model-based planning methods.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在环境中采取的采样动作是不可逆的，因为环境的状态变化不能被智能体撤销。相比之下，在过渡模型中采取的规划动作是可逆的（Moerland et al., [2020a](#bib.bib118)）。规划智能体可以回溯，而采样智能体则不能。采样容易找到局部最优。为了找到全局最优，能够从局部最优中回溯的能力是有用的，这也是基于模型的规划方法的一个优势。
- en: Note, however, that there are two ways of finding dynamics models. In some problems,
    the transition and reward models are given by the problem, such as in games, where
    the move rules are known, as in Go and chess. Here the dynamics models follow
    the problem perfectly, and many steps can be planned accurately into the future
    without problem, out-performing model-free sampling. In other problems the dynamics
    model must be learned from sampling the environment. Here the model will not be
    perfect, and will contain errors and biases. Planning far ahead will only work
    when the agent has a $T$ and $R$ model of sufficient quality. With learned models,
    it may be more difficult for model-based planning to achieve the performance of
    model-free sampling.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，有两种方法可以找到动态模型。在一些问题中，过渡和奖励模型由问题提供，例如在棋类游戏中，移动规则是已知的，如围棋和国际象棋。在这里，动态模型完全符合问题，许多步骤可以准确规划到未来，表现优于无模型采样。在其他问题中，动态模型必须通过对环境进行采样来学习。在这种情况下，模型将不是完美的，可能包含错误和偏差。只有当智能体具有足够质量的
    $T$ 和 $R$ 模型时，远期规划才会有效。使用学习到的模型，基于模型的规划可能更难达到无模型采样的性能。
- en: <svg   height="83.23" overflow="visible" version="1.1" width="278.55"><g transform="translate(0,83.23)
    matrix(1 0 0 -1 0 0) translate(145.06,0) translate(0,-7.6)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -39.13 14.96)" fill="#000000"
    stroke="#000000"><foreignobject width="78.26" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Environment</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -38.92 75.28)" fill="#000000" stroke="#000000"><foreignobject width="78.99"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Policy/Value</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -140.45 45.75)" fill="#000000" stroke="#000000"><foreignobject
    width="48.47" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">learning</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 91.98 45.94)" fill="#000000" stroke="#000000"><foreignobject
    width="36.9" height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">acting</foreignobject></g></g></svg>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="83.23" overflow="visible" version="1.1" width="278.55"><g transform="translate(0,83.23)
    matrix(1 0 0 -1 0 0) translate(145.06,0) translate(0,-7.6)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -39.13 14.96)" fill="#000000"
    stroke="#000000"><foreignobject width="78.26" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">环境</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -38.92 75.28)" fill="#000000" stroke="#000000"><foreignobject width="78.99"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">策略/价值</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -140.45 45.75)" fill="#000000" stroke="#000000"><foreignobject
    width="48.47" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">学习</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 91.98 45.94)" fill="#000000" stroke="#000000"><foreignobject
    width="36.9" height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">行动</foreignobject></g></g></svg>
- en: 'Figure 4: Model-Free Learning'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 无模型学习'
- en: 2.3 Model-Free
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 无模型
- en: When the transition or reward model are not available to the agent, then the
    policy and value function have to be learned through querying the environment.
    Learning the policy or value function without a model, through sampling the environment,
    is called model-free learning, see Figure [4](#S2.F4 "Figure 4 ‣ 2.2 Planning
    ‣ 2 Background ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey").
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当过渡或奖励模型对代理不可用时，策略和价值函数必须通过查询环境来学习。通过采样环境来学习策略或价值函数，被称为无模型学习，参见图 [4](#S2.F4
    "Figure 4 ‣ 2.2 Planning ‣ 2 Background ‣ Model-Based Deep Reinforcement Learning
    for High-Dimensional Problems, a Survey")。
- en: 'Recall that the policy is a mapping of states to best actions. Each time when
    a new reward is returned by the environment the policy can be improved: the best
    action for the state is updated to reflect the new information. Algorithm [2](#alg2
    "Algorithm 2 ‣ 2.3 Model-Free ‣ 2 Background ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey") shows the simple high-level
    steps of model-free reinforcement learning (later on the algorithms become more
    elaborate).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，策略是状态到最佳动作的映射。每当环境返回新的奖励时，策略可以得到改进：状态的最佳动作会更新以反映新信息。算法 [2](#alg2 "Algorithm
    2 ‣ 2.3 Model-Free ‣ 2 Background ‣ Model-Based Deep Reinforcement Learning for
    High-Dimensional Problems, a Survey") 展示了无模型强化学习的简单高层步骤（后面的算法会变得更加复杂）。
- en: Algorithm 2 Model-Free Learning
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 无模型学习
- en: repeat     Sample env $E$ to generate data $D=(s,a,r^{\prime},s^{\prime})$     Use
    $D$ to update policy $\pi(s,a)$until $\pi$ converges
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 重复采样环境 $E$ 以生成数据 $D=(s,a,r^{\prime},s^{\prime})$ 使用 $D$ 更新策略 $\pi(s,a)$ 直到 $\pi$
    收敛
- en: Model-free reinforcement learning is the most basic form of reinforcement learning.
    It has been successfully applied to a range of challenging problems (Deisenroth
    et al., [2013b](#bib.bib33); Kober et al., [2013](#bib.bib96)). In model-free
    reinforcement learning a policy is learned from the ground up through interactions
    (samples) with the environment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 无模型强化学习是最基本的强化学习形式。它已成功应用于一系列具有挑战性的问题 (Deisenroth et al., [2013b](#bib.bib33);
    Kober et al., [2013](#bib.bib96))。在无模型强化学习中，策略通过与环境的互动（样本）从零开始学习。
- en: 'The main goal of model-free learning is to achieve good generalization: to
    achieve high accuracy on test problems not seen during training. A secondary goal
    is to do so with good sample efficiency: to need as few environment samples as
    possible for good generalization.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 无模型学习的主要目标是实现良好的泛化：在训练过程中未见过的测试问题上取得高准确度。次要目标是以良好的样本效率完成此任务：尽可能少地需要环境样本以获得良好的泛化。
- en: Model-free learning is essentially blind, and learning the policy and value
    takes many samples. A well-known model-free reinforcement learning algorithm is
    Q-learning (Watkins, [1989](#bib.bib176)). Algorithms such as Q-learning can be
    used in a classical table-based setting. Deep neural networks have also been used
    with success in model-free learning, in domains in which samples can be generated
    cheaply and quickly, such as in Atari video games (Mnih et al., [2015](#bib.bib115)).
    Deep model-free algorithms such as Deep Q-Network (DQN) (Mnih et al., [2013](#bib.bib114))
    and Proximal Policy Optimzation (PPO) (Schulman et al., [2017](#bib.bib147)) have
    become quite popular. PPO is an algorithm that computes the policy directly, DQN
    finds the value function first (Section [2.1](#S2.SS1 "2.1 Formalizing Reinforcement
    Learning ‣ 2 Background ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey")).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 无模型学习基本上是盲目的，并且学习策略和价值需要许多样本。一个著名的无模型强化学习算法是Q学习（Watkins，[1989](#bib.bib176)）。像Q-learning这样的算法可以在传统的基于表的设置中使用。深度神经网络在无模型学习中也取得了成功，在可以便宜快速生成样本的领域中，比如在Atari视频游戏中（Mnih等，[2015](#bib.bib115)）。深度无模型算法如深度Q网络（DQN）（Mnih等，[2013](#bib.bib114)）和Proximal
    Policy Optimization（PPO）（Schulman等，[2017](#bib.bib147)）已经变得非常流行。PPO是一种直接计算策略的算法，DQN先找到值函数（第2.1节）。
- en: Although risky, an advantage of flying blind is the absence of bias. Model-free
    reinforcement learning can find global optima without being distracted by a biased
    model (it has no model). Learned models in model-based reinforcement learning
    may introduce bias, and model-based methods may not always be able to find as
    good results as model-free can (although it does find the biased results with
    fewer samples).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有风险，盲目飞行的优势在于没有偏见。无模型的强化学习可以在没有被有偏模型影响的情况下找到全局最优解（它没有模型）。而基于模型的强化学习中学习的模型可能会引入偏见，基于模型的方法可能无法像无模型的方法那样总是找到那么好的结果（尽管它可以在更少的样本中找到有偏的结果）。
- en: Let us look at the cost of our methods. Interaction with the environment can
    be costly. Especially when the environment involves the real world, such as in
    real robot-interaction, then sampling should be minimized, for reasons of cost,
    and to prevent wear of the robot arm. In virtual environments on the other hand,
    model-free approaches have been quite successful, as we have noted in Atari and
    other game play (Mnih et al., [2015](#bib.bib115)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们方法的成本。与环境的交互可能是昂贵的。特别是当环境涉及真实世界，比如真实的机器人交互时，样本采集应该被最小化，出于成本和防止机械手磨损的原因。另一方面，在虚拟环境中，无模型的方法已经取得了相当大的成功，正如我们在Atari和其他游戏中注意到的那样（Mnih等，[2015](#bib.bib115)）。
- en: A good overview of model-free reinforcement learning can be found in (Çalışır
    and Pehlivanoğlu, [2019](#bib.bib23); Sutton and Barto, [2018](#bib.bib159); Kaelbling
    et al., [1996](#bib.bib84)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 关于无模型强化学习的一个很好的概览可以在（Çalışır和Pehlivanoğlu，[2019](#bib.bib23); Sutton和Barto，[2018](#bib.bib159);
    Kaelbling等，[1996](#bib.bib84)）中找到。
- en: 2.4 Model-Based
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 基于模型
- en: It is now time to look at model-based reinforcement learning, a method that
    learns the policy and value in a different way than by sampling the environment
    directly. Recall that the environment samples return $(s^{\prime},r^{\prime})$
    pairs, when the agent selects action $a$ in state $s$. Therefore all information
    is present to learn the transition model $T_{a}(s,s^{\prime})$ and the reward
    model $R_{a}(s,s^{\prime})$, for example by supervised learning. When no transition
    model is given by the problem, then the model can be learned by sampling the environment,
    and be used with planning to update the policy and value as often as we like.
    This alternative approach of finding the policy and the value is called model-based
    learning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看基于模型的强化学习了，这是一种以不同方式学习策略和价值的方法，而不是直接通过对环境进行采样。回想一下，当代理在状态s中选择动作a时，环境对返回的$(s^{\prime},r^{\prime})$对进行抽样。因此，所有信息都存在于学习转移模型$T_{a}(s,s^{\prime})$和奖励模型$R_{a}(s,s^{\prime})$的过程中，例如通过监督学习。当问题没有给出转移模型时，那么模型可以通过对环境进行采样来学习，并且可以用于计划，根据需要随时更新策略和价值。这种找到策略和价值的替代方法称为基于模型的学习。
- en: If the model is given, then no environment samples are needed and model-based
    methods are more sample efficient. But if the model is not given, why would we
    want to go this convoluted model-and-planning route, if the samples can teach
    us the optimal policy and value directly? The reason is that the convoluted route
    may be more sample efficient.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型已知，则不需要环境样本，基于模型的方法更为样本高效。但如果模型未知，为什么要走这条复杂的模型与规划路线，如果样本可以直接教我们最优策略和值？原因是复杂的路线可能更为样本高效。
- en: When the complexity of learning the transition/reward model is smaller than
    the complexity of learning the policy model directly, and planning is fast, then
    the model-based route can be more efficient. In model-free learning a sample is
    used once to optimize the policy, and then thrown away, in model-based learning
    the sample is used to learn a transition model, which can then be used many times
    in planning to optimize the policy. The sample is used more efficiently.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当学习转移/奖励模型的复杂性小于直接学习策略模型的复杂性，并且规划速度较快时，基于模型的方法可能更高效。在无模型学习中，一个样本只被用来优化策略，然后被丢弃；而在基于模型的学习中，样本用于学习转移模型，之后可以在规划中多次使用以优化策略。样本的利用更加高效。
- en: The recent successes in deep learning caused much interest and progress in deep
    model-free learning. Many of the deep function approximation methods that have
    been so successful in supervised learning (Goodfellow et al., [2016](#bib.bib50);
    LeCun et al., [2015](#bib.bib106)) can also be used in model-free reinforcement
    learning for approximating the policy and value function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的最近成功引起了对深度无模型学习的极大兴趣和进展。许多在监督学习中取得成功的深度函数逼近方法 （Goodfellow et al., [2016](#bib.bib50);
    LeCun et al., [2015](#bib.bib106)）也可以用于无模型强化学习中，以逼近策略和值函数。
- en: However, there are reasons for interest in model-based methods as well. Many
    real world problems are long and complex sequential decision making problems,
    and we are now seeing efforts to make progress in model-based methods. Furthermore,
    the interest in lifelong learning stimulates interest in model-based learning (Silver
    et al., [2013](#bib.bib150)). Model-based reinforcement learning is close to human
    and animal learning, in that all new knowledge is interpreted in the context of
    existing knowledge. The dynamics model is used to process and interpret new samples,
    in contrast to model-free learning, where all samples, old and new, are treated
    alike, and are not interpreted using the knowledge that has been accumulated so
    far in the model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对基于模型的方法也有兴趣的理由。许多现实世界的问题都是长时间且复杂的序列决策问题，我们现在看到许多努力在基于模型的方法上取得进展。此外，对终身学习的兴趣激发了对基于模型学习的兴趣 （Silver
    et al., [2013](#bib.bib150)）。基于模型的强化学习接近于人类和动物的学习，因为所有的新知识都在现有知识的背景下进行解释。动态模型用于处理和解释新样本，而与无模型学习形成对比的是，无论旧样本还是新样本，都被同等对待，并且没有使用到迄今为止在模型中积累的知识。
- en: After these introductory words, we are now ready to take a deeper look into
    recent concrete deep model-based reinforcement learning methods.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些介绍性的话语之后，我们现在准备深入探讨近期具体的深度基于模型的强化学习方法。
- en: 3 Survey of Model-Based Deep Reinforcement Learning
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于模型的深度强化学习调查
- en: 'The success of model-based reinforcement learning depends on the quality of
    the dynamics model. The model is typically used by planning algorithms for multiple
    sequential predictions, and errors in predictions accumulate quickly. We group
    the methods in three main approaches. First the transitions are given and used
    by explicit planning, second the transitions are learned and used by explicit
    planning, and third both transitions and planning are learned end-to-end:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的强化学习的成功依赖于动态模型的质量。该模型通常被规划算法用于多个序列预测，而预测中的错误会迅速累积。我们将这些方法分为三种主要的途径。首先是给定转移并通过显式规划使用，其次是学习转移并通过显式规划使用，第三是转移和规划都通过端到端学习。
- en: '1.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Explicit Planning on Given Transitions First, we discuss methods for problems
    that give us clear transition rules. In this case transition models are perfect,
    and classical, explicit, planning methods are used to optimize the value and policy
    functions of large state spaces. Recently, large and complex problems have been
    solved in two-agent games using self-learning methods that give rise to curriculum
    learning. Curriculum learning has also been used in single agent problems.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**明确的规划在给定的转移** 首先，我们讨论那些提供清晰转移规则的问题的方法。在这种情况下，转移模型是完美的，经典的、明确的规划方法被用来优化大状态空间的价值函数和策略函数。最近，使用自学习方法解决了两代理游戏中的大规模复杂问题，这些方法催生了课程学习。课程学习也被应用于单代理问题中。'
- en: '2.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Explicit Planning on Learned Transitions Second, we discuss methods for problems
    where no clear rules exist, and the transition model must be learned from sampling
    the environment. (The transitions are again used with conventional planning methods.)
    The environment samples allow learning by backpropagation of high-capacity models.
    It is important that the model has as few errors as possible. Uncertainty modeling
    and limited lookahead can reduce the impact of prediction errors.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**基于学习的明确规划** 其次，我们讨论那些没有清晰规则的问题的方法，并且转移模型必须通过对环境进行采样来学习。（转移再次与传统规划方法一起使用。）环境样本允许通过高容量模型的反向传播进行学习。重要的是模型尽可能少的错误。不确定性建模和有限的前瞻可以减少预测错误的影响。'
- en: '3.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: End-to-end Learning of Planning and Transitions Third, we discuss the situation
    where both the transition model and the planning algorithm are learned from the
    samples, end-to-end. A neural network can be used in a way that it performs the
    actual steps of certain planners, in addition to learning the transitions from
    the samples, as before. The model-based algorithm is learned fully end-to-end.
    A drawback of this approach is the tight connection between network architecture
    and problem type, limiting its applicabily. This drawback can be resolved with
    the use of latent models, see below.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**端到端学习规划和转移** 第三，我们讨论转移模型和规划算法都通过样本端到端地学习的情况。可以使用神经网络来执行某些规划者的实际步骤，除了从样本中学习转移外。基于模型的算法完全端到端学习。这种方法的一个缺点是网络架构与问题类型之间的紧密关联，限制了它的适用性。这个缺点可以通过使用潜在模型来解决，见下文。'
- en: In addition to the three main approaches, we now discuss two orthogonal approaches.
    These can be used to improve performance of the three main approaches. They are
    the hybrid imagination idea from Sutton’s Dyna (Sutton, [1991](#bib.bib158)),
    and abstract, or latent, models.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这三种主要方法之外，我们现在讨论两种正交的方法。这些方法可以用来提高三种主要方法的性能。它们是来自 Sutton 的 Dyna （Sutton, [1991](#bib.bib158)）的混合想象思路，以及抽象的或潜在的模型。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hybrid Model-Free/Model-Based Imagination We first mention a sub-approach where
    environment samples are not only used to train the transition model, but also
    to train the policy function directly, just as in model-free learning. This hybrid
    approach thus combines model-based and model-free learning. It is also called
    *imagination* because the looking ahead with the dynamics model resembles simulating
    or imagining environment samples outside the real environment. In this approach
    the imagined, or planned, “samples” augment the real (environment) samples. This
    augmentation reduces sample complexity of model-free methods.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**混合模型自由/基于模型的想象** 我们首先提到一个子方法，其中环境样本不仅用于训练转移模型，还直接用于训练策略函数，就像在模型自由学习中一样。因此，这种混合方法结合了基于模型和模型自由的学习。它也被称为*想象*，因为通过动态模型进行的前瞻性分析类似于在真实环境之外模拟或想象环境样本。在这种方法中，想象的或计划的“样本”增强了真实（环境）样本。这种增强降低了模型自由方法的样本复杂性。'
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Latent Models Next, we discuss a sub-approach where the learned dynamics model
    is split into several lower-capacity, specialized, latent models. These latent
    models are then used with planning or imagination to find the policy. Latent models
    have been used with and without end-to-end model training and with and without
    imagination. Latent models thus build on and improve the previous approaches.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**潜在模型** 接下来，我们讨论一个子方法，其中学习到的动态模型被拆分成几个较低容量的、专门的潜在模型。这些潜在模型随后与规划或想象结合使用以找到策略。潜在模型可以与端到端模型训练一起使用，也可以独立使用，与想象结合使用，也可以不结合使用。因此，潜在模型在前述方法的基础上进行改进。'
- en: '| Approach | Name | Learning | Planning | Hybrid | Latent | Application |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 名称 | 学习 | 规划 | 混合 | 潜在 | 应用 |'
- en: '|  |  |  |  | Imagination | Models |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 想象 | 模型 |  |'
- en: '| Explicit Planning | TD-Gammon (Tesauro, [1995a](#bib.bib164)) | Fully connected
    net | Alpha-beta | - | - | Backgammon |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: 显式规划 | TD-Gammon (Tesauro，[1995a](#bib.bib164)) | 完全连接的网络 | Alpha-beta | - |
    - | 黑白棋 |
- en: '| Given Transitions | Expert Iteration (Anthony et al., [2017](#bib.bib3))
    | Policy/Value CNN | MCTS | - | - | Hex |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: 给定过渡 | 专家迭代 (Anthony 等人，[2017](#bib.bib3)) | 策略/价值CNN | MCTS | - | - | 六角游戏
    |
- en: '| (Sect. [3.1](#S3.SS1 "3.1 Explicit Planning on Given Transitions ‣ 3 Survey
    of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning
    for High-Dimensional Problems, a Survey")) | Alpha(Go) Zero (Silver et al., [2017a](#bib.bib153))
    | Policy/Value ResNet | MCTS | - | - | Go/chess/shogi |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: (第 [3.1](#S3.SS1 "3.1 显式计划在给定过渡上 ‣ 3 模型驱动的深度强化学习调查 ‣ 模型驱动的深度强化学习面对高维问题的调查"))
    | Alpha(Go) Zero (Silver等人，[2017a](#bib.bib153)) | 策略/价值ResNet | MCTS | - | -
    | 围棋/国际象棋/将棋 |
- en: '|  | Single Agent (Feng et al., [2020](#bib.bib43)) | Resnet | MCTS | - | -
    | Sokoban |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 单智能体 (Feng 等人，[2020](#bib.bib43)) | Resnet | MCTS | - | - | 货仓守卫 |'
- en: '| Explicit Planning | PILCO (Deisenroth and Rasmussen, [2011](#bib.bib31))
    | Gaussian Processes | Gradient based | - | - | Pendulum |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: 显式规划 | PILCO (Deisenroth和Rasmusen，[2011](#bib.bib31)) | 高斯过程 | 基于梯度 | - | -
    | 摆锤 |
- en: '| Learned Transitions | iLQG (Tassa et al., [2012](#bib.bib162)) | Quadratic
    Non-linear | MPC | - | - | Humanoid |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: 学习过渡 | iLQG (Tassa 等人，[2012](#bib.bib162)) | 二次非线性 | MPC | - | - | 人形 |
- en: '| (Sect. [3.2](#S3.SS2 "3.2 Explicit Planning on Learned Transitions ‣ 3 Survey
    of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning
    for High-Dimensional Problems, a Survey")) | GPS (Levine and Abbeel, [2014](#bib.bib108))
    | iLQG | Trajectory | - | - | Swimmer |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: (第 [3.2](#S3.SS2 "3.2 Explicit Planning on Learned Transitions ‣ 3 Survey of
    Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning
    for High-Dimensional Problems, a Survey")) | GPS (Levine和Abbeel，[2014](#bib.bib108))
    | iLQG | 轨迹 | - | - | 游泳者 |
- en: '|  | SVG (Heess et al., [2015](#bib.bib67)) | Value Gradients | Trajectory
    | - | - | Swimmer |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | SVG (Heess 等人，[2015](#bib.bib67)) | 价值梯度 | 轨迹 | - | - | 游泳者 |'
- en: '|  | PETS (Chua et al., [2018](#bib.bib27)) | Uncertainty Ensemble | MPC |
    - | - | Cheetah |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | PETS (Chua 等人，[2018](#bib.bib27)) | 不确定性集成 | MPC | - | - | 猎豹 |'
- en: '|  | Visual Foresight (Finn and Levine, [2017](#bib.bib44)) | Video Prediction
    | MPC | - | - | Manipulation |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | 视觉预测 (Finn和Levine，[2017](#bib.bib44)) | 视频预测 | MPC | - | - | 操纵 |'
- en: '|  | Local Model (Gu et al., [2016](#bib.bib52)) | Quadratic Non-linear | Short
    rollouts | + | - | Cheetah |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 本地模型 (Gu 等人，[2016](#bib.bib52)) | 二次非线性 | 短rollouts | + | - | 猎豹 |'
- en: '|  | MVE (Feinberg et al., [2018](#bib.bib42)) | Samples | Short rollouts |
    + | - | Cheetah |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | MVE (Feinberg 等人，[2018](#bib.bib42)) | 样本 | 短rollouts | + | - | 猎豹 |'
- en: '|  | Meta Policy (Clavera et al., [2018](#bib.bib28)) | Meta-ensembles | Short
    rollouts | + | - | Cheetah |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | 元策略 (Clavera 等人，[2018](#bib.bib28)) | 元集成 | 短rollouts | + | - | 猎豹 |'
- en: '|  | GATS (Azizzadenesheli et al., [2018](#bib.bib7)) | Pix2pix | MCTS | +
    | - | Cheetah |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | GATS (Azizzadenesheli 等人，[2018](#bib.bib7)) | Pix2pix | MCTS | + | - |
    猎豹 |'
- en: '|  | Policy Optim (Janner et al., [2019](#bib.bib80)) | Ensemble | Short rollouts
    | + | - | Cheetah |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | 策略优化 (Janner 等人，[2019](#bib.bib80)) | 集成 | 短rollouts | + | - | 猎豹 |'
- en: '|  | Video-prediction (Oh et al., [2015](#bib.bib127)) | CNN/LSTM | Action
    | + | + | Atari |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 视频预测 (Oh 等人，[2015](#bib.bib127)) | CNN/LSTM | 行动 | + | + | Atari |'
- en: '|  | VPN (Oh et al., [2017](#bib.bib128)) | CNN encoder | $d$-step | + | +
    | Atari |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | VPN (Oh 等人，[2017](#bib.bib128)) | CNN编码器 | $d$-step | + | + | Atari |'
- en: '|  | SimPLe (Kaiser et al., [2019](#bib.bib86)) | VAE, LSTM | MPC | + | + |
    Atari |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | SimPLe (Kaiser 等人，[2019](#bib.bib86)) | VAE, LSTM | MPC | + | + | Atari
    |'
- en: '|  | PlaNet (Hafner et al., [2018](#bib.bib59)) | RSSM (VAE/RNN) | CEM | -
    | + | Cheetah |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | PlaNet (Hafner 等人，[2018](#bib.bib59)) | RSSM (VAE/RNN) | CEM | - | + |
    猎豹 |'
- en: '|  | Dreamer (Hafner et al., [2019](#bib.bib60)) | RSSM+CNN | Imagine | - |
    + | Hopper |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | Dreamer (Hafner 等人，[2019](#bib.bib60)) | RSSM+CNN | 想象 | - | + | 跳跃者 |'
- en: '|  | Plan2Explore (Sekar et al., [2020](#bib.bib148)) | RSSM | Planning | -
    | + | Hopper |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | Plan2Explore (Sekar 等人，[2020](#bib.bib148)) | RSSM | 计划 | - | + | 跳跃者
    |'
- en: '| End-to-End Learning | VIN (Tamar et al., [2016](#bib.bib161)) | CNN | Rollout
    in network | + | - | Mazes |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: 端到端学习 | VIN (Tamar 等人，[2016](#bib.bib161)) | CNN | 网络内rollout | + | - | 迷宫 |
- en: '| Planning & Transitions | VProp (Nardelli et al., [2018](#bib.bib124)) | CNN
    | Hierarch Rollouts | + | - | Maze, nav |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: 计划和转换 | VProp (Nardelli 等人，[2018](#bib.bib124)) | CNN | 分层Rollouts | + | - |
    迷宫，导航 |
- en: '| (Sect. [3.3](#S3.SS3 "3.3 End-to-end Learning of Planning and Transitions
    ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey")) | TreeQN (Farquhar et al.,
    [2018](#bib.bib40)) | Tree-shape Net | Plan-functions | + | + | Box-push |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| (Sect. [3.3](#S3.SS3 "3.3 End-to-end Learning of Planning and Transitions
    ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey")) | TreeQN（Farquhar 等，[2018](#bib.bib40)）
    | 树形网络 | 计划函数 | + | + | 推箱子 |'
- en: '|  | Planning (Guez et al., [2019](#bib.bib54)) | CNN+LSTM | Rollouts in network
    | + | - | Sokoban |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | 计划（Guez 等，[2019](#bib.bib54)） | CNN+LSTM | 网络中的回放 | + | - | 推箱子 |'
- en: '|  | I2A (Weber et al., [2017](#bib.bib177)) | CNN/LSTM encoder | Meta-controller
    | + | + | Sokoban |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | I2A（Weber 等，[2017](#bib.bib177)） | CNN/LSTM 编码器 | 元控制器 | + | + | 推箱子 |'
- en: '|  | Predictron (Silver et al., [2017b](#bib.bib154)) | $k,\gamma,\lambda$-CNN-predictr
    | $k$-rollout | + | + | Mazes |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | Predictron（Silver 等，[2017b](#bib.bib154)） | $k,\gamma,\lambda$-CNN-predictr
    | $k$-回放 | + | + | 迷宫 |'
- en: '|  | World Model (Ha and Schmidhuber, [2018b](#bib.bib57)) | VAE | CMA-ES |
    + | + | Car Racing |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | 世界模型（Ha 和 Schmidhuber，[2018b](#bib.bib57)） | VAE | CMA-ES | + | + | 赛车
    |'
- en: '|  | MuZero (Schrittwieser et al., [2019](#bib.bib146)) | Latent | MCTS | -
    | + | Atari/Go |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | MuZero（Schrittwieser 等，[2019](#bib.bib146)） | 潜在 | MCTS | - | + | Atari/围棋
    |'
- en: 'TABLE I: Overview of Deep Model-Based Reinforcement Learning Methods'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 深度模型基础强化学习方法概述'
- en: The different approaches can and have been used alone and in combination, as
    we will see shortly. Table [I](#S3.T1 "TABLE I ‣ 3 Survey of Model-Based Deep
    Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey") provides an overview of all approaches and methods that we
    will discuss in this survey. The methods are grouped into the three main categories
    that were introduced above. The use of the two orthogonal approaches by the methods
    (imagination and latent models) is indicated in Table [I](#S3.T1 "TABLE I ‣ 3
    Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey") in two separate columns. The
    final column provides an indication of the application that the method is used
    on (such as Swimmer, Chess, and Cheetah). In the next section, Sect. [4](#S4 "4
    Benchmarks ‣ Model-Based Deep Reinforcement Learning for High-Dimensional Problems,
    a Survey"), these applications will be explained in more depth.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的方法可以单独使用，也可以结合使用，正如我们将很快看到的那样。表[I](#S3.T1 "TABLE I ‣ 3 Survey of Model-Based
    Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey") 提供了我们在本调查中将讨论的所有方法的概述。这些方法被分为上述引入的三个主要类别。方法使用的两个正交方法（想象和潜在模型）在表[I](#S3.T1
    "TABLE I ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep
    Reinforcement Learning for High-Dimensional Problems, a Survey") 的两个独立列中有所指示。最后一列提供了该方法应用的指示（例如游泳者、国际象棋和猎豹）。在下一节
    Sect. [4](#S4 "4 Benchmarks ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey") 中，这些应用将被更深入地解释。
- en: All methods in the table will be explained in detail in the remainder of this
    section (for ease of reference, we will repeat the methods of each subsection
    in their own table). The sections will again mention some of the applications
    on which they were tested. Please refer to the section on Benchmarks.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表中的所有方法将在本节剩余部分详细解释（为方便起见，我们将在各小节中重复每种方法）。各节将再次提到一些测试应用。请参阅基准测试部分。
- en: Model-based methods work well for low-dimensional tasks where the transition
    and reward dynamics are relatively simple (Sutton and Barto, [2018](#bib.bib159)).
    While efficient methods such as Gaussian processes can learn these models quickly—with
    few samples—they struggle to represent complex and discontinuous systems (Wang
    et al., [2019b](#bib.bib175)). Most current model-free methods use deep neural
    networks to deal with problems that have such complex, high-dimensional, and discontinuous
    characteristics, leading to a high sample complexity.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的方法在低维任务中表现良好，其中转换和奖励动态相对简单（Sutton 和 Barto，[2018](#bib.bib159)）。尽管高效的方法如高斯过程可以快速学习这些模型——只需少量样本——但它们难以表示复杂和不连续的系统（Wang
    等，[2019b](#bib.bib175)）。大多数当前的无模型方法使用深度神经网络来处理具有复杂、高维和不连续特征的问题，导致样本复杂度高。
- en: The main challenge that the model-based reinforcement learning algorithms in
    this survey thus address is as follows. For high-dimensional tasks the curse of
    dimensionality causes data to be sparse and variance to be high. Deep methods
    tend to overfit on small datasets, and model-free methods use large data sets
    and have bad sample efficiency. Model-based methods that use poor models make
    poor planning predictions far into the future (Talvitie, [2015](#bib.bib160)).
    The challenge is to learn deep, high-dimensional transition functions from limited
    data, that can account for model uncertainty, and plan over these models to achieve
    policy and value functions that perform as well or better than model-free methods.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查中的模型基础强化学习算法所面临的主要挑战如下。对于高维任务，维度的诅咒导致数据稀疏且方差高。深度方法倾向于在小数据集上过拟合，而模型无关方法使用大数据集且样本效率低。使用差模型的模型基础方法在长期规划预测方面表现较差(Talvitie,
    [2015](#bib.bib160))。挑战在于从有限数据中学习深层、高维的过渡函数，能够考虑模型的不确定性，并在这些模型上进行规划，以实现表现与模型无关方法相当或更好的策略和值函数。
- en: We will now discuss the algorithms. We will discuss (1) methods that use explicit
    planning on given transitions, (2) use explicit planning on a learned transition
    model, and (3) use end-to-end learning of planning and transitions. We will encounter
    the first occurrence of hybrid imagination and latent models approaches in the
    second section, on explicit planning/learned transitions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论这些算法。我们将讨论（1）在给定过渡上进行显式规划的方法，（2）在学习的过渡模型上进行显式规划的方法，以及（3）规划和过渡的端到端学习方法。我们将在第二部分的显式规划/学习的过渡中遇到混合想象和潜在模型方法的首次出现。
- en: 3.1 Explicit Planning on Given Transitions
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 在给定过渡上进行显式规划
- en: The first approach in model-based learning is when the transition and reward
    model is provided clearly in the rules of the problem. This is the case, for example,
    in games such as Go and chess. Table [II](#S3.T2 "TABLE II ‣ 3.1 Explicit Planning
    on Given Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based
    Deep Reinforcement Learning for High-Dimensional Problems, a Survey") summarizes
    the approaches of this subsection. Note the addition of the reinforcement learning
    method in an extra column.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 模型基础学习的第一种方法是当过渡和奖励模型在问题规则中明确提供时。例如，在围棋和国际象棋等游戏中就是这种情况。表[II](#S3.T2 "TABLE II
    ‣ 3.1 Explicit Planning on Given Transitions ‣ 3 Survey of Model-Based Deep Reinforcement
    Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional Problems,
    a Survey")总结了本小节的方法。注意额外列中的强化学习方法。
- en: With this approach high performing results have recently been achieved on large
    and complex domains. These results have been achieved by combining classical,
    explicit, heuristic search planning algorithms such as Alpha-beta and MCTS (Knuth
    and Moore, [1975](#bib.bib95); Browne et al., [2012](#bib.bib21); Plaat, [2020](#bib.bib132)),
    and deep learning with self-play, achieving tabula rasa curriculum learning. Curriculum
    learning is based on the observation that a difficult problem is learned more
    quickly by first learning a sequence of easy, but related problems—just as we
    teach school children easy concepts (such as addition) first before we teach them
    harder concepts (such as multiplication, or logarithms).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法最近在大型和复杂领域取得了高性能结果。这些结果通过结合经典的显式启发式搜索规划算法，如Alpha-beta和MCTS (Knuth and
    Moore, [1975](#bib.bib95); Browne et al., [2012](#bib.bib21); Plaat, [2020](#bib.bib132))，以及自我对弈的深度学习，实现了从零开始的课程学习。课程学习基于这样一种观察：通过首先学习一系列简单但相关的问题——就像我们先教小学生简单的概念（如加法），再教他们难一些的概念（如乘法或对数）——可以更快地学习复杂的问题。
- en: In self-play the agent plays against the environment, which is also the same
    agent with the same network, see Figure [5](#S3.F5 "Figure 5 ‣ 3.1 Explicit Planning
    on Given Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based
    Deep Reinforcement Learning for High-Dimensional Problems, a Survey"). The states
    and actions in the games are then used by a deep learning system to improve the
    policy and value functions. These functions are used as the selection and evaluation
    functions in MCTS, and thus improving them improves the quality of play of MCTS.
    This has the effect that as the agent is getting smarter, so is the environment.
    A virtuous circle of a mutually increasing level of play is the result, a natural
    form of curriculum learning (Bengio et al., [2009](#bib.bib14)). A sequence of
    ever-improving tournaments is played, in which a game can be learned to play from
    scratch, from zero-knowledge to world champion level (Silver et al., [2017a](#bib.bib153)).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在自我对弈中，智能体与环境进行对抗，而这个环境实际上也是相同的智能体，使用的是相同的网络，见图[5](#S3.F5 "图 5 ‣ 3.1 给定过渡的显式规划
    ‣ 3 模型基础的深度强化学习调研 ‣ 高维问题的模型基础深度强化学习调研")。游戏中的状态和动作随后被深度学习系统用来改进策略和价值函数。这些函数作为 MCTS
    中的选择和评估函数使用，因此改善它们可以提升 MCTS 的游戏质量。其效果是随着智能体变得越来越聪明，环境也会变得越来越聪明。结果是形成了一个互相提升的良性循环，这是一种自然的课程学习形式（Bengio
    et al., [2009](#bib.bib14)）。进行了一系列不断改进的锦标赛，其中一个游戏可以从零知识到世界冠军级别进行学习（Silver et al.,
    [2017a](#bib.bib153)）。
- en: <svg   height="114.3" overflow="visible" version="1.1" width="364.4"><g transform="translate(0,114.3)
    matrix(1 0 0 -1 0 0) translate(98.01,0) translate(0,3.78)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 107.51 34.57)" fill="#000000"
    stroke="#000000"><foreignobject width="99.93" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Transition Rules</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -29.98 35.99)" fill="#000000" stroke="#000000"><foreignobject width="59.96"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Opponent</foreignobject></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 56.06 24.6)" fill="#000000"
    stroke="#000000"><foreignobject width="25.37" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">play</foreignobject></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 59.51 94.97)" fill="#000000" stroke="#000000"><foreignobject width="78.99"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Policy/Value</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -90.63 76.48)" fill="#000000" stroke="#000000"><foreignobject
    width="48.47" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">learning</foreignobject></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 63.24 58.46)" fill="#000000"
    stroke="#000000"><foreignobject width="70.38" height="8.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">tournament</foreignobject></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 222.5 76.48)" fill="#000000" stroke="#000000"><foreignobject width="36.9"
    height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">acting</foreignobject></g></g></svg>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="114.3" overflow="visible" version="1.1" width="364.4"><g transform="translate(0,114.3)
    matrix(1 0 0 -1 0 0) translate(98.01,0) translate(0,3.78)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 107.51 34.57)" fill="#000000"
    stroke="#000000"><foreignobject width="99.93" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">过渡规则</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -29.98 35.99)" fill="#000000" stroke="#000000"><foreignobject width="59.96"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">对手</foreignobject></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 56.06 24.6)" fill="#000000"
    stroke="#000000"><foreignobject width="25.37" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">对弈</foreignobject></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 59.51 94.97)" fill="#000000" stroke="#000000"><foreignobject width="78.99"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">策略/价值</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -90.63 76.48)" fill="#000000" stroke="#000000"><foreignobject
    width="48.47" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">学习</foreignobject></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 63.24 58.46)" fill="#000000"
    stroke="#000000"><foreignobject width="70.38" height="8.51" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">锦标赛</foreignobject></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 222.5 76.48)" fill="#000000" stroke="#000000"><foreignobject width="36.9"
    height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">行动</foreignobject></g></g></svg>
- en: 'Figure 5: Explicit Planning/Given Transitions'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 显式规划/给定过渡'
- en: The concept of self-play was invented in multiple places and has a long history
    in two-agent game playing AI. Three well-known examples are Samuel’s checkers
    player (Samuel, [1959](#bib.bib140)), Tesauro’s Backgammon player (Tesauro, [1995a](#bib.bib164),
    [2002](#bib.bib166)) and DeepMind’s Alpha(Go) Zero (Silver et al., [2017a](#bib.bib153),
    [2018](#bib.bib155)).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 自我博弈的概念在多个地方被发明，并且在双代理游戏 AI 中有着悠久的历史。三个著名的例子是 Samuel 的跳棋玩家 (Samuel, [1959](#bib.bib140))、Tesauro
    的西洋双陆棋玩家 (Tesauro, [1995a](#bib.bib164), [2002](#bib.bib166)) 和 DeepMind 的 Alpha(Go)
    Zero (Silver et al., [2017a](#bib.bib153), [2018](#bib.bib155))。
- en: '| Approach | Learning | Planning | Reinforcement Learning | Application |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 学习 | 规划 | 强化学习 | 应用 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| TD-Gammon (Tesauro, [1995a](#bib.bib164)) | Fully connected net | Alpha-beta
    | Temporal Difference | Backgammon |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| TD-Gammon (Tesauro, [1995a](#bib.bib164)) | 全连接网络 | Alpha-beta | 时间差分 | 西洋双陆棋
    |'
- en: '| Expert Iteration (Anthony et al., [2017](#bib.bib3)) | Pol/Val CNN | MCTS
    | Curriculum | Hex |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 专家迭代 (Anthony et al., [2017](#bib.bib3)) | Pol/Val CNN | MCTS | 课程 | Hex
    |'
- en: '| Alpha(Go) Zero (Silver et al., [2017a](#bib.bib153)) | Pol/Val ResNet | MCTS
    | Curriculum | Go/chess/shogi |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Alpha(Go) Zero (Silver et al., [2017a](#bib.bib153)) | Pol/Val ResNet | MCTS
    | 课程 | 围棋/象棋/将棋 |'
- en: '| Single Agent (Feng et al., [2020](#bib.bib43)) | ResNet | MCTS | Curriculum
    | Sokoban |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 单代理 (Feng et al., [2020](#bib.bib43)) | ResNet | MCTS | 课程 | Sokoban |'
- en: 'TABLE II: Overview of Explicit Planning/Given Transitionds Methods'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 显式规划/给定转移方法概述'
- en: Let us discuss some of the self-play approaches.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一些自我博弈的方法。
- en: TD-Gammon (Tesauro, [1995a](#bib.bib164)) is a Backgammon playing program that
    uses a small neural network with a single fully connected hidden layer with just
    80 hidden units and a small (two-level deep) Alpha-beta search (Knuth and Moore,
    [1975](#bib.bib95)). It teaches itself to play Backgammon from scratch using temporal-difference
    learning. A small neural network learns the value function. TD-Gammon was the
    first Backgammon program to play at World-Champion level, and the first program
    to successfully use a self-learning curriculum approach in game playing since
    Samuel’s checkers program (Samuel, [1959](#bib.bib140)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: TD-Gammon (Tesauro, [1995a](#bib.bib164)) 是一个西洋双陆棋程序，它使用了一个具有单个全连接隐藏层和仅 80 个隐藏单元的小型神经网络，以及一个小的（两级深度）Alpha-beta
    搜索 (Knuth and Moore, [1975](#bib.bib95))。它通过时间差分学习从头开始学习如何玩西洋双陆棋。一个小型神经网络学习价值函数。TD-Gammon
    是第一个达到世界冠军级别的西洋双陆棋程序，也是自 Samuel 的跳棋程序 (Samuel, [1959](#bib.bib140)) 以来第一个成功使用自我学习课程方法的游戏程序。
- en: An approach similar to the AlphaGo and AlphaZero programs was presented as Expert
    Iteration (Anthony et al., [2017](#bib.bib3)). The problem was again how to learn
    to play a complex game from scratch. Expert Iteration (ExIt) combines search-based
    planning (the expert) with deep learning (by iteration). The expert finds improvements
    to the current policy. ExIt uses a single multi-task neural network, for the policy
    and the value function. The planner uses the neural network policy and value estimates
    to improve the quality of its plans, resulting in a cycle of mutual improvement.
    The planner in ExIt is MCTS. ExIt uses a version with rollouts. ExIt was used
    with the boardgame Hex (Hayward and Toft, [2019](#bib.bib64)), and compared favorably
    against a strong MCTS-only program MoHex (Arneson et al., [2010](#bib.bib6)).
    A further development of ExIt is Policy Gradient Search, which uses planning without
    an explicit search tree (Anthony et al., [2019](#bib.bib4)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 AlphaGo 和 AlphaZero 程序的方法被称为专家迭代 (Anthony et al., [2017](#bib.bib3))。问题依旧是如何从头开始学习玩一个复杂的游戏。专家迭代
    (ExIt) 结合了基于搜索的规划（专家）和深度学习（通过迭代）。专家对当前策略进行改进。ExIt 使用一个单一的多任务神经网络，用于策略和价值函数。规划者使用神经网络策略和价值估计来提高其计划的质量，从而形成一个相互改进的循环。ExIt
    中的规划者是 MCTS。ExIt 使用了带回滚的版本。ExIt 在棋盘游戏 Hex (Hayward and Toft, [2019](#bib.bib64))
    中应用，并与强大的 MCTS 仅程序 MoHex (Arneson et al., [2010](#bib.bib6)) 进行了有利的比较。ExIt 的进一步发展是策略梯度搜索，它使用没有显式搜索树的规划
    (Anthony et al., [2019](#bib.bib4))。
- en: AlphaZero, and its predecessor AlphaGo Zero, are self-play curriculum learning
    programs that were developed by a team of researchers (Silver et al., [2018](#bib.bib155),
    [2017a](#bib.bib153)). The programs are desiged to play complex board games full
    of tactics and strategy well, specifically Go, chess, and shogi, a Japanese game
    similar to chess, but more complex (Iida et al., [2002](#bib.bib75)). AlphaZero
    and AlphaGo Zero are self-play model-based reinforcement learning programs. The
    environment against which they play is the same program as the agent that is learning
    to play. The transition function and the reward function are defined by the rules
    of the game. The goal is to learn optimal policy and value functions. AlphaZero
    uses a single neural network, a 19-block residual network with a value head and
    a policy head. For each different game—Go, chess, shogi—it uses different input
    and output layers, but the hidden layers are identical, and so is the rest of
    the architecture and the hyperparameters that govern the learning process. The
    loss-function is the sum of the policy-loss and the value-loss (Wang et al., [2019a](#bib.bib173)).
    The planning algorithm is based on Monte Carlo Tree Search (Browne et al., [2012](#bib.bib21);
    Coulom, [2006](#bib.bib29)) although it does not perform random rollouts. Instead,
    it uses the value head of the resnet for evaluation and the policy head of the
    ResNet to augment the UCT selection function (Kocsis and Szepesvári, [2006](#bib.bib97)),
    as in P-UCT (Rosin, [2011](#bib.bib138)).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaZero及其前身AlphaGo Zero是由研究团队（Silver等，[2018](#bib.bib155)，[2017a](#bib.bib153)）开发的自我对弈课程学习程序。这些程序旨在有效地玩复杂的棋类游戏，特别是围棋、国际象棋和将棋，这是一种类似于国际象棋但更复杂的日本游戏（Iida等，[2002](#bib.bib75)）。AlphaZero和AlphaGo
    Zero是自我对弈的基于模型的强化学习程序。它们对弈的环境与学习对弈的代理程序是相同的。转移函数和奖励函数由游戏规则定义。目标是学习最佳策略和价值函数。AlphaZero使用一个单一的神经网络，一个19块残差网络，具有一个价值头和一个策略头。对于每种不同的游戏——围棋、国际象棋、将棋，它使用不同的输入和输出层，但隐藏层是相同的，其余的架构和控制学习过程的超参数也是相同的。损失函数是策略损失和价值损失的总和（Wang等，[2019a](#bib.bib173)）。规划算法基于蒙特卡洛树搜索（Browne等，[2012](#bib.bib21)；Coulom，[2006](#bib.bib29)），尽管它不执行随机回合。相反，它使用ResNet的价值头进行评估，使用ResNet的策略头来增强UCT选择函数（Kocsis和Szepesvári，[2006](#bib.bib97)），如P-UCT（Rosin，[2011](#bib.bib138)）。
- en: <svg   height="108.74" overflow="visible" version="1.1" width="213.66"><g transform="translate(0,108.74)
    matrix(1 0 0 -1 0 0) translate(47.98,0) translate(0,3.51)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(0.9 0.0 0.0 0.9 -16.83 74.42)" fill="#000000"
    stroke="#000000"><foreignobject width="37.4" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">search</foreignobject></g><g transform="matrix(0.9
    0.0 0.0 0.9 107.56 74.42)" fill="#000000" stroke="#000000"><foreignobject width="24.21"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">eval</foreignobject></g><g
    transform="matrix(0.9 0.0 0.0 0.9 45.89 15.53)" fill="#000000" stroke="#000000"><foreignobject
    width="29.25" height="9.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">train</foreignobject></g><g
    stroke-width="0.6pt"><g transform="matrix(1.0 0.0 0.0 1.0 -43.37 32.18)" fill="#000000"
    stroke="#000000"><foreignobject width="49.54" height="11.07" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">examples</foreignobject></g></g><g stroke-width="0.6pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 111.94 31.47)" fill="#000000" stroke="#000000"><foreignobject
    width="42.89" height="7.66" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">new
    net</foreignobject></g></g><g stroke="#000000"><g stroke-width="0.6pt"><g transform="matrix(1.0
    0.0 0.0 1.0 43.67 91.96)" fill="#000000" stroke="#000000"><foreignobject width="37.08"
    height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">reward</foreignobject></g></g></g></g></svg>
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Self-Play/Curriculum Learning Loop'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: The residual network is used in the evaluation and selection of MCTS. The self-play
    mechanism starts from a randomly initialized resnet. MCTS is used to play a tournament
    of games, to generate training positions for the resnet to be trained on, using
    a DQN-style replay buffer (Mnih et al., [2015](#bib.bib115)). This trained resnet
    is then again used by MCTS in the next training tournament to generate training
    positions, etc., see Figure [6](#S3.F6 "Figure 6 ‣ 3.1 Explicit Planning on Given
    Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based
    Deep Reinforcement Learning for High-Dimensional Problems, a Survey"). Self-play
    feeds on itself in multiple ways, and achieving stable learning is a challenging
    task, requiring judicious tuning, exploration, and much training. AlphaZero is
    currently the worldwide strongest player in Go, chess, and shogi (Silver et al.,
    [2018](#bib.bib155)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The success of curriculum learning in two-player self-play has inspired work
    on single-agent curriculum learning. These single-agent approaches do not do self-play,
    but do use curriculum learning. Laterre et al. introduce the Ranked Reward method
    for solving bin packing problems (Laterre et al., [2018](#bib.bib104)) and Wang
    et al. presented a method for Morpion Solitaire (Wang et al., [2020](#bib.bib174)).
    Feng et al. use an AlphaZero based approach to solve hard Sokoban instances (Feng
    et al., [2020](#bib.bib43)). Their model is an 8 block standard residual network,
    with MCTS as planner. Solving Sokoban instances is a hard problem in single-agent
    combinatorial search. The curriculum approach, where the agent learns to solve
    easy instances before it tries to solve harder instances, is a natural fit. In
    two-player games, a curriculum is generated in self-play. Feng et al. create a
    curriculum in a different way, by constructing simpler subproblems from hard instances,
    using the fact that Sokoban problems have a natural hierarchical structure. As
    in AlphaZero, the problem learns from scratch, no Sokoban heuristics are provided
    to the solver. This approach was able to solve harder Sokoban instances than had
    been solved before.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 双人自我对弈中课程学习的成功激发了对单智能体课程学习的研究。这些单智能体方法并不进行自我对弈，但使用了课程学习。Laterre et al. 提出了用于解决箱子打包问题的Ranked
    Reward方法（Laterre et al., [2018](#bib.bib104)），Wang et al. 提出了Morpion Solitaire的解决方法（Wang
    et al., [2020](#bib.bib174)）。Feng et al. 使用基于AlphaZero的方法来解决难度较大的Sokoban实例（Feng
    et al., [2020](#bib.bib43)）。他们的模型是一个8块标准残差网络，以MCTS作为规划器。解决Sokoban实例是单智能体组合搜索中的一个难题。课程方法中，智能体首先学习解决简单实例，然后再尝试解决更难实例，这是一个自然的适配。在双人游戏中，课程是在自我对弈中生成的。Feng
    et al. 以不同的方式创建课程，通过从难实例中构造更简单的子问题，利用Sokoban问题的自然层次结构。与AlphaZero一样，该方法从头开始学习，没有提供Sokoban启发式信息。这个方法能够解决比以前更多的难度较大的Sokoban实例。
- en: Conclusion
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结论
- en: In self-play curriculum learning the opponent has the same model as the agent.
    The opponent is the environment of the agent. As the agent learns, so does its
    opponent, providing tougher counterplay, teaching the agent more. The agent is
    exposed to curriculum learning, a sequence of increasingly harder learning tasks.
    In this way, learning strong play has been achieved in Backgammon, Go, chess and
    shogi (Tesauro, [1995b](#bib.bib165); Silver et al., [2018](#bib.bib155)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在自我对弈课程学习中，对手与智能体具有相同的模型。对手是智能体的环境。随着智能体的学习，它的对手也在学习，提供更强的对抗玩法，教会智能体更多。智能体经历了课程学习，即一系列逐渐困难的学习任务。通过这种方式，在双陆棋、围棋、国际象棋和将棋中实现了强大的玩法（Tesauro,
    [1995b](#bib.bib165); Silver et al., [2018](#bib.bib155)）。
- en: In two-agent search a natural idea is to duplicate the agent as the environment,
    creating a self-play system. Self-play has been used in planning (as minimax),
    with policy learning, and in combination with latent models. Self-generated curriculum
    learning is a powerful paradigm. Work is under way to see if it can be applied
    to single-agent problems as well (Narvekar et al., [2020](#bib.bib125); Feng et al.,
    [2020](#bib.bib43); Doan et al., [2019](#bib.bib35); Laterre et al., [2018](#bib.bib104)),
    and in multi-agent (real-time strategy) games, addressing problems with specialization
    of two-agent games (Sect. [4.4](#S4.SS4 "4.4 Real-Time Strategy and Video Games
    ‣ 4 Benchmarks ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey")  (Vinyals et al., [2019](#bib.bib171))).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在双智能体搜索中，一个自然的想法是将智能体复制为环境，创建一个自我对弈系统。自我对弈已经被用于规划（如最小最大算法）、策略学习以及与潜在模型的结合。自生成课程学习是一个强大的范式。目前正在研究它是否也可以应用于单智能体问题（Narvekar
    et al., [2020](#bib.bib125); Feng et al., [2020](#bib.bib43); Doan et al., [2019](#bib.bib35);
    Laterre et al., [2018](#bib.bib104)），以及在多智能体（实时策略）游戏中，解决双智能体游戏（第[4.4](#S4.SS4
    "4.4 Real-Time Strategy and Video Games ‣ 4 Benchmarks ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey")节(Vinyals et al., [2019](#bib.bib171)）中的专业化问题。
- en: 3.2 Explicit Planning on Learned Transitions
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 学习过渡的明确规划
- en: In the previous section, transition rules could be derived from the problem
    directly (by inspection). In many problems, this is not the case, and we have
    to resort to sampling the environment to learn a model of the transitions. The
    second category of algorithms of this survey is to learn the tranition model by
    backpropagation from environment samples. This learned model is then still used
    by classical, explicit, planning algorithms, as before. We will discuss various
    approaches where the transition model is learned with supervised learning methods
    such as backpropagation through time (Werbos, [1988](#bib.bib179)), see Figure [7](#S3.F7
    "Figure 7 ‣ 3.2 Explicit Planning on Learned Transitions ‣ 3 Survey of Model-Based
    Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey").
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，过渡规则可以直接从问题中得出（通过检查）。在许多问题中情况并非如此，我们必须依靠从环境中采样以学习过渡模型。本次调查的第二类算法是通过从环境样本反向传播学习过渡模型。然后，这个学习的模型仍然被经典的显式规划算法使用。我们将讨论各种方法，其中过渡模型通过监督学习方法（如时间反向传播
    (Werbos, [1988](#bib.bib179))）进行学习，见图 [7](#S3.F7 "Figure 7 ‣ 3.2 Explicit Planning
    on Learned Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣
    Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey")。
- en: <svg   height="93.22" overflow="visible" version="1.1" width="381.45"><g transform="translate(0,93.22)
    matrix(1 0 0 -1 0 0) translate(102.09,0) translate(0,2.39)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 138.04 14.96)" fill="#000000"
    stroke="#000000"><foreignobject width="78.26" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Environment</foreignobject></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -32.45 14.88)"><foreignobject
    width="104.28" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Transition
    Model</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 59.51 75.28)" fill="#000000"
    stroke="#000000"><foreignobject width="78.99" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Policy/Value</foreignobject></g><g stroke-width="0.8pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 80.83 4.91)" fill="#000000" stroke="#000000"><foreignobject
    width="48.47" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">learning</foreignobject></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 183.34 45.94)" fill="#000000" stroke="#000000"><foreignobject
    width="36.9" height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">acting</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -34.76 45.75)" fill="#000000" stroke="#000000"><foreignobject
    width="52.27" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">planning</foreignobject></g></g></svg>
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="93.22" overflow="visible" version="1.1" width="381.45"><g transform="translate(0,93.22)
    matrix(1 0 0 -1 0 0) translate(102.09,0) translate(0,2.39)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 138.04 14.96)" fill="#000000"
    stroke="#000000"><foreignobject width="78.26" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">环境</foreignobject></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -32.45 14.88)"><foreignobject
    width="104.28" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">过渡模型</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 59.51 75.28)" fill="#000000" stroke="#000000"><foreignobject
    width="78.99" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">策略/价值</foreignobject></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 80.83 4.91)" fill="#000000"
    stroke="#000000"><foreignobject width="48.47" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">学习</foreignobject></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 183.34 45.94)" fill="#000000" stroke="#000000"><foreignobject width="36.9"
    height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">执行</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -34.76 45.75)" fill="#000000" stroke="#000000"><foreignobject
    width="52.27" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">规划</foreignobject></g></g></svg>
- en: 'Figure 7: Explicit Planning/Learned Transitions'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：显式规划/学习的过渡
- en: repeat     Sample env $E$ to generate data $D=(s,a,r^{\prime},s^{\prime})$     Use
    $D$ to learn $T_{a}(s,s^{\prime})$     Use $T$ to update policy $\pi(s,a)$ by
    planninguntil $\pi$ converges
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 重复     从环境中采样 $E$ 以生成数据 $D=(s,a,r^{\prime},s^{\prime})$     使用 $D$ 学习 $T_{a}(s,s^{\prime})$     使用
    $T$ 通过规划更新策略 $\pi(s,a)$，直到 $\pi$ 收敛
- en: Algorithm 3 Explicit Planning/Learned Transitions
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 显式规划/学习的过渡
- en: '| Approach | Learning | Planning | Application |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 学习 | 规划 | 应用 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| PILCO (Deisenroth and Rasmussen, [2011](#bib.bib31)) | Gaussian Processes
    | Gradient based | Pendulum |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| PILCO (Deisenroth 和 Rasmussen, [2011](#bib.bib31)) | 高斯过程 | 基于梯度 | 摆 |'
- en: '| iLQG (Tassa et al., [2012](#bib.bib162)) | Quadratic Non-linear | MPC | Humanoid
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| iLQG (Tassa 等, [2012](#bib.bib162)) | 二次非线性 | MPC | 人形机器人 |'
- en: '| GPS (Levine and Abbeel, [2014](#bib.bib108)) | iLQG | Trajectory | Swimmer
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| GPS（Levine 和 Abbeel，[2014](#bib.bib108)） | iLQG | 轨迹 | 游泳者 |'
- en: '| SVG (Heess et al., [2015](#bib.bib67)) | Value Gradients | Trajectory | Swimmer
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| SVG（Heess 等，[2015](#bib.bib67)） | 值梯度 | 轨迹 | 游泳者 |'
- en: '| PETS (Chua et al., [2018](#bib.bib27)) | Uncertainty Ensemble | MPC | Cheetah
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| PETS（Chua 等，[2018](#bib.bib27)） | 不确定性集成 | MPC | 猎豹 |'
- en: '| Visual Foresight (Finn and Levine, [2017](#bib.bib44)) | Video Prediction
    | MPC | Manipulation |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 视觉预见（Finn 和 Levine，[2017](#bib.bib44)） | 视频预测 | MPC | 操作 |'
- en: 'TABLE III: Overview of Explicit Planning/Learned Transitions Methods'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：显式规划/学习转移方法概述
- en: 'Algorithm [3](#alg3 "Algorithm 3 ‣ 3.2 Explicit Planning on Learned Transitions
    ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey") shows the steps of using explicit
    planning and transition learning by backpropagation. Table [III](#S3.T3 "TABLE
    III ‣ 3.2 Explicit Planning on Learned Transitions ‣ 3 Survey of Model-Based Deep
    Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey") summarizes the approaches of this subsection, showing both
    the *learning* and the *planning* approach. Two variants of this approach are
    also discussed in this subsection: hybrid imagination and latent models, see Table [IV](#S3.T4
    "TABLE IV ‣ 3.2.1 Hybrid Model-Free/Model-Based Imagination ‣ 3.2 Explicit Planning
    on Learned Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣
    Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey")
    and Table [V](#S3.T5 "TABLE V ‣ 3.2.2 Latent Models ‣ 3.2 Explicit Planning on
    Learned Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based
    Deep Reinforcement Learning for High-Dimensional Problems, a Survey").'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 算法[3](#alg3 "Algorithm 3 ‣ 3.2 Explicit Planning on Learned Transitions ‣ 3
    Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey") 展示了通过反向传播使用显式规划和转移学习的步骤。 表[III](#S3.T3
    "TABLE III ‣ 3.2 Explicit Planning on Learned Transitions ‣ 3 Survey of Model-Based
    Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey") 总结了本小节的方法，展示了*学习*和*规划*方法。 本小节还讨论了该方法的两种变体：混合想象和潜在模型，请参见表[IV](#S3.T4
    "TABLE IV ‣ 3.2.1 Hybrid Model-Free/Model-Based Imagination ‣ 3.2 Explicit Planning
    on Learned Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣
    Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey")
    和表[V](#S3.T5 "TABLE V ‣ 3.2.2 Latent Models ‣ 3.2 Explicit Planning on Learned
    Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based
    Deep Reinforcement Learning for High-Dimensional Problems, a Survey")。
- en: We will first see how simple Gaussian Processes and quadratic methods can create
    predictive transition models. Next, precision is improved with trajectory methods,
    and we make the step to video prediction methods. Finally, methods that focus
    on uncertainty and ensemble methods will be introduced. We know that deep neural
    nets need much data and learn slowly, or will overfit. Uncertainty modeling is
    based on the insight that early in the training the model has seen little data,
    and tends to overfit, and later on, as it has seen more data, it may underfit.
    This issue can be mitigated by incorporating uncertainty into the dynamics models,
    as we shall see in the later methods (Chua et al., [2018](#bib.bib27)).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先看到简单的高斯过程和二次方法如何创建预测性转移模型。接下来，使用轨迹方法提高精度，我们将进一步探索视频预测方法。最后，将介绍关注不确定性和集成方法的方法。我们知道深度神经网络需要大量数据，并且学习缓慢，或者会发生过拟合。
    不确定性建模基于这样的见解：在训练早期，模型看到的数据较少，倾向于过拟合；而在后期，随着数据的增多，模型可能会欠拟合。通过将不确定性纳入动态模型，这个问题可以得到缓解，如后续方法中所述（Chua
    等，[2018](#bib.bib27)）。
- en: For smaller models, environment samples can be used to approximate a transition
    model as a Gaussian Process of random variables. This approach is followed in
    PILCO, which stands for Probabilistic Inference for Learning Control, see (Deisenroth
    and Rasmussen, [2011](#bib.bib31); Deisenroth et al., [2013a](#bib.bib32); Kamthe
    and Deisenroth, [2017](#bib.bib88)). Gaussian Processes can accurately learn simple
    processes with good sample efficiency (Bishop, [2006](#bib.bib17)), although for
    high dimensional problems they need more samples. PILCO treats the transition
    model $T_{a}(s,s^{\prime})$ as a probabilistic function of the environment samples.
    The planner improves the policy based on the analytic gradients relative to the
    policy parameters $\theta$. PILCO has been used to optimize small problems such
    as Mountain car and Cartpole pendulum swings, for which it works well. Although
    they achieve model learning using higher order model information, Gaussian Processes
    do not scale to high dimensional environments, and the method is limited to smaller
    applications.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的模型，可以使用环境样本来近似转移模型，作为随机变量的高斯过程。PILCO（即概率推断学习控制）采用了这种方法，详见（Deisenroth 和
    Rasmussen，[2011](#bib.bib31)；Deisenroth 等，[2013a](#bib.bib32)；Kamthe 和 Deisenroth，[2017](#bib.bib88)）。高斯过程可以准确地学习简单过程，并具有良好的样本效率（Bishop，[2006](#bib.bib17)），尽管对于高维问题，它们需要更多的样本。PILCO
    将转移模型 $T_{a}(s,s^{\prime})$ 视为环境样本的概率函数。规划器基于相对于策略参数 $\theta$ 的分析梯度来改进策略。PILCO
    已被用于优化小问题，如山地车和摆车摆动，对于这些问题效果很好。尽管他们使用高阶模型信息来实现模型学习，但高斯过程无法扩展到高维环境，因此这种方法仅限于较小的应用。
- en: A related method uses a trajectory optimization approach with nonlinear least-squares
    optimization. In control theory, the linear–quadratic–Gaussian (LQG) control problem
    is one of the most fundamental optimal control problems. Iterative LQG (Tassa
    et al., [2012](#bib.bib162)) is the control analog of the Gauss-Newton method
    for nonlinear least-squares optimization. In contrast to PILCO, the model learner
    uses quadratic approximation on the reward function and linear approximation of
    the transition function. The planning part of this method uses a form of online
    trajectory optimization, model-predictive control (MPC), in which step-by-step
    real-time local optimization is used, as opposed to full-problem optimization (Richards,
    [2005](#bib.bib135)). By using many further improvements throughout the MPC pipeline,
    including the trajectory optimization algorithm, the physics engine, and cost
    function design, Tassa et al. were able to achieve near-real-time performance
    in humanoid simulated robot manipulation tasks, such as grasping.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一种相关的方法使用带有非线性最小二乘优化的轨迹优化方法。在控制理论中，线性-二次-高斯（LQG）控制问题是最基本的最优控制问题之一。迭代 LQG（Tassa
    等，[2012](#bib.bib162)）是高斯-牛顿法在非线性最小二乘优化中的控制类比。与 PILCO 相比，模型学习者在奖励函数上使用二次近似，在转移函数上使用线性近似。这种方法的规划部分使用一种在线轨迹优化形式，即模型预测控制（MPC），其中使用逐步实时局部优化，而不是全面问题优化（Richards，[2005](#bib.bib135)）。通过在
    MPC 流程中包括轨迹优化算法、物理引擎和成本函数设计等多项进一步改进，Tassa 等能够在类人机器人模拟操控任务中（如抓取）实现近实时性能。
- en: Another trajectory optimization method takes its inspiration from model-free
    learning. Levine and Koltun (Levine and Koltun, [2013](#bib.bib109)) introduce
    Guided Policy Search (GPS) in which the search uses trajectory optimization to
    avoid poor local optima. In GPS, the parameterized policy is trained in a supervised
    way with samples from a trajectory distribution. The GPS model optimizes the trajectory
    distribution for cost and the current policy, to create a good training set for
    the policy. Guiding samples are generated by differential dynamic programming
    and are incorporated into the policy with regularized importance sampling. In
    contrast to the previous methods, GPS algorithms can train complex policies with
    thousands of parameters. In a sense, Guided Policy Search transforms the iLQG
    controller into a neural network policy $\pi_{\theta}$ with a trust region in
    which the new controller does not deviate too much from the samples (Levine and
    Abbeel, [2014](#bib.bib108); Finn et al., [2016](#bib.bib45); Montgomery and Levine,
    [2016](#bib.bib121)). GPS has been evaluated on planar swimming, hopping, and
    walking, as well as simulated 3D humanoid running.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种轨迹优化方法受到无模型学习的启发。Levine 和 Koltun（Levine 和 Koltun，[2013](#bib.bib109)）介绍了引导策略搜索（GPS），其中搜索使用轨迹优化来避免糟糕的局部最优解。在
    GPS 中，参数化的策略通过从轨迹分布中获取样本以监督的方式进行训练。GPS 模型优化轨迹分布以降低成本并优化当前策略，从而创建一个良好的策略训练集。引导样本通过微分动态规划生成，并通过正则化重要性采样整合到策略中。与之前的方法相比，GPS
    算法可以训练具有数千个参数的复杂策略。从某种意义上说，引导策略搜索将 iLQG 控制器转变为具有信任区域的神经网络策略 $\pi_{\theta}$，其中新的控制器不会与样本偏离太多（Levine
    和 Abbeel，[2014](#bib.bib108)；Finn 等，[2016](#bib.bib45)；Montgomery 和 Levine，[2016](#bib.bib121)）。GPS
    已在平面游泳、跳跃和行走，以及模拟的 3D 人形跑步上进行了评估。
- en: Another attempt at increasing the accuracy of learned parameterized transition
    models in continuous control problems is Stochastic Value Gradients (SVG) (Heess
    et al., [2015](#bib.bib67)). It mitigates learned model inaccuracy by computing
    value gradients along the real environment trajectories instead of planned ones.
    The mismatch between predicted and real transitions is addressed with re-parametrization
    and backpropagation through the stochastic samples. In comparison, PILCO uses
    Gaussian process models to compute analytic policy gradients that are sensitive
    to model-uncertainty and GPS optimizes policies with the aid of a stochastic trajectory
    optimizer and locally-linear models. SVG in contrast focuses on global neural
    network value function approximators. SVG results are reported on simulated robotics
    applications in Swimmer, Reacher, Gripper, Monoped, Half-Cheetah, and Walker.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 提高连续控制问题中学习的参数化转移模型准确性的另一种尝试是随机值梯度（SVG）（Heess 等，[2015](#bib.bib67)）。它通过沿实际环境轨迹计算值梯度来缓解学习模型的不准确性，而不是使用规划轨迹。通过重新参数化和通过随机样本的反向传播来解决预测和实际转移之间的不匹配。相比之下，PILCO
    使用高斯过程模型来计算对模型不确定性感敏感的解析策略梯度，而 GPS 通过随机轨迹优化器和局部线性模型来优化策略。SVG 则专注于全局神经网络值函数近似器。SVG
    的结果报告在模拟的机器人应用中，如游泳者、到达者、抓取器、单足行走、半猎豹和行走者。
- en: Other methods also focus on uncertainty in high dimensional modeling, but use
    ensembles. Chua et al. propose probabilistic ensembles with trajectory sampling
    (PETS) (Chua et al., [2018](#bib.bib27)). The learned transition model of PETS
    has an uncertainty-aware deep network, which is combined with sampling-based uncertainty
    propagation. PETS uses a combination of probabilistic ensembles (Lakshminarayanan
    et al., [2017](#bib.bib102)). The dynamics are modelled by an ensemble of probabilistic
    neural network models in a model-predictive control setting (the agent only applies
    the first action from the optimal sequence and re-plans at every time-step) (Nagabandi
    et al., [2018](#bib.bib123)). Chua et al. report experiments on simulated robot
    tasks such as Half-Cheetah, Pusher, Reacher. Performance on these tasks is reported
    to approach asymptotic model-free baselines, stressing the importance of uncertainty
    estimation in model-baed reinforcement learning.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法也关注高维建模中的不确定性，但使用集成方法。Chua等人提出了带有轨迹采样的概率集成（PETS）（Chua等人，[2018](#bib.bib27)）。PETS的学习转移模型具有不确定性感知深度网络，并结合了基于采样的不确定性传播。PETS使用了概率集成的组合（Lakshminarayanan等人，[2017](#bib.bib102)）。在模型预测控制环境中，动力学由一组概率神经网络模型建模（智能体仅应用最优序列中的第一个动作，并在每个时间步骤重新规划）（Nagabandi等人，[2018](#bib.bib123)）。Chua等人报告了在模拟机器人任务（如Half-Cheetah、Pusher、Reacher）上的实验。这些任务的性能报告接近渐近模型自由基准，强调了模型基础强化学习中不确定性估计的重要性。
- en: An important problem in robotics is to learn arm manipulation directly from
    video camera input by seeing which movements work and which fail. The video input
    provides a high dimensional and difficult input and increases problem size and
    complexity substantially. Both Finn et al. and Ebert et al. report on learning
    complex robotic manipulation skills from high-dimenstional raw sensory pixel inputs
    in a method called Visual Foresight (Finn and Levine, [2017](#bib.bib44); Ebert
    et al., [2018](#bib.bib39)). The aim of Visual Foresight is to generalize deep
    learning methods to never-before-seen tasks and objects. It uses a training procedure
    where data is sampled according to a probability distribution. Concurrently, a
    video prediction model is trained with the samples. This model generates the corresponding
    sequence of future frames based on an image and a sequence of actions, as in GPS.
    At test time, the least-cost sequence of actions is selected in a model-predictive
    control planning framework. Visual Foresight is able to perform multi-object manipulation,
    pushing, picking and placing, and cloth-folding tasks.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人领域的一个重要问题是通过观察哪些动作有效、哪些失败，直接从视频摄像头输入中学习手臂操作。视频输入提供了高维且困难的输入，显著增加了问题的规模和复杂性。Finn等人和Ebert等人报告了从高维原始传感器像素输入中学习复杂机器人操作技能的方法，称为Visual
    Foresight（Finn和Levine，[2017](#bib.bib44)；Ebert等人，[2018](#bib.bib39)）。Visual Foresight的目标是将深度学习方法推广到前所未见的任务和对象。它使用一种训练程序，其中数据根据概率分布进行采样。同时，视频预测模型与样本一起训练。该模型基于图像和动作序列生成相应的未来帧序列，如GPS。在测试时，选择最小成本的动作序列作为模型预测控制规划框架的一部分。Visual
    Foresight能够执行多物体操作、推挤、抓取和放置以及折叠布料任务。
- en: Conclusion
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结论
- en: Model learning with a single network works well for low-dimensional problems.
    We have seen that Gaussian Process modeling achieves sample efficiency and generalization
    to good policies. For high-dimensional problems, generalization and sample efficiency
    deteriorate, more samples are needed and policies do not perform as well. We have
    discussed methods for improvement by guiding policies with real samples (GPS),
    limiting the scope of predictions with model-predictive control, and using ensembles
    and uncertainty aware neural networks to model uncertainty (PETS).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单一网络的模型学习在低维问题上效果良好。我们已看到，高斯过程建模在样本效率和政策推广方面表现良好。对于高维问题，推广和样本效率会恶化，需要更多样本，政策效果也不如预期。我们讨论了通过真实样本（GPS）指导政策、通过模型预测控制限制预测范围以及使用集成方法和不确定性感知神经网络建模不确定性（PETS）的方法以改善这些问题。
- en: 3.2.1 Hybrid Model-Free/Model-Based Imagination
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 混合模型自由/基于模型的想象
- en: In the preceding subsection, we have looked at how to use environment samples
    to build a transition model. Many methods were covered to learn transition models
    with as few samples as possible. These methods are related to supervised learning
    methods. The transition model was then used by a planning method to optimize the
    policy or value function.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的子章节中，我们研究了如何使用环境样本来建立过渡模型。介绍了许多方法以尽可能少的样本来学习过渡模型。这些方法与监督学习方法相关。然后，过渡模型被规划方法用来优化策略或价值函数。
- en: We will now review methods that use a complementary approach, a hybrid model-based/model-free
    approach of using the environment samples for two purposes. Here the emphasis
    is no longer on learning the model but on using it effectively. This approach
    was introduced by Sutton (Sutton, [1990](#bib.bib157), [1991](#bib.bib158)) in
    the Dyna system, long before deep learning was used widely. Dyna uses the samples
    to update the policy function directly (model-free learning) and also uses the
    samples to learn a transition model, which is then used by planning to augment
    the model-free environment-samples with the model-based imagined “samples.” In
    this way the sample-efficiency of model-free learning is improved quite directly.
    Figure [8](#S3.F8 "Figure 8 ‣ 3.2.1 Hybrid Model-Free/Model-Based Imagination
    ‣ 3.2 Explicit Planning on Learned Transitions ‣ 3 Survey of Model-Based Deep
    Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey") illustrates the working of the Dyna approach. (Note that
    now two arrows learn from the environment samples. Model-free learning is drawn
    bold.)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将回顾使用互补方法的技术，这是一种混合模型基/无模型方法，通过环境样本用于两个目的。在这里，重点不再是学习模型，而是有效地利用它。这种方法由Sutton（Sutton,
    [1990](#bib.bib157), [1991](#bib.bib158)）在Dyna系统中引入，早于深度学习的广泛应用。Dyna直接使用样本来更新策略函数（无模型学习），同时使用样本来学习一个过渡模型，然后通过规划将模型基的“样本”与无模型环境样本进行增强。通过这种方式，无模型学习的样本效率得到了直接改善。图[8](#S3.F8
    "Figure 8 ‣ 3.2.1 Hybrid Model-Free/Model-Based Imagination ‣ 3.2 Explicit Planning
    on Learned Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣
    Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey")展示了Dyna方法的工作原理。（注意现在有两个箭头从环境样本中学习。无模型学习用粗体标出。）
- en: <svg   height="92.94" overflow="visible" version="1.1" width="380.98"><g transform="translate(0,92.94)
    matrix(1 0 0 -1 0 0) translate(121.31,0) translate(0,2.12)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 118.35 14.96)" fill="#000000"
    stroke="#000000"><foreignobject width="78.26" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Environment</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -52.14 14.88)" fill="#000000" stroke="#000000"><foreignobject width="104.28"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Transition
    Model</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 39.82 75.28)" fill="#000000"
    stroke="#000000"><foreignobject width="78.99" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Policy/Value</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 61.01 5.19)" fill="#000000" stroke="#000000"><foreignobject width="48.47"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">learning</foreignobject></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 59.45 41.5)" fill="#000000"
    stroke="#000000"><foreignobject width="48.47" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">learning</foreignobject></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 210.33 45.94)" fill="#000000" stroke="#000000"><foreignobject width="36.9"
    height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">acting</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -116.21 45.75)" fill="#000000" stroke="#000000"><foreignobject
    width="52.27" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">planning</foreignobject></g></g></svg>
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="92.94" overflow="visible" version="1.1" width="380.98"><g transform="translate(0,92.94)
    matrix(1 0 0 -1 0 0) translate(121.31,0) translate(0,2.12)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 118.35 14.96)" fill="#000000"
    stroke="#000000"><foreignobject width="78.26" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">环境</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -52.14 14.88)" fill="#000000" stroke="#000000"><foreignobject width="104.28"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">过渡模型</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 39.82 75.28)" fill="#000000" stroke="#000000"><foreignobject
    width="78.99" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">策略/价值</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 61.01 5.19)" fill="#000000" stroke="#000000"><foreignobject
    width="48.47" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">学习</foreignobject></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 59.45 41.5)" fill="#000000"
    stroke="#000000"><foreignobject width="48.47" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">学习</foreignobject></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 210.33 45.94)" fill="#000000" stroke="#000000"><foreignobject width="36.9"
    height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">行动</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -116.21 45.75)" fill="#000000" stroke="#000000"><foreignobject
    width="52.27" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">规划</foreignobject></g></g></svg>
- en: 'Figure 8: Hybrid Model-Free/Model-Based Imagination'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：混合模型无关/模型基础想象
- en: Dyna was introduced for a table-based approach before deep learning became popular.
    Originally, in Dyna, the transition model is updated directly with samples, without
    learning through backpropagation, however, here we discuss only deep imagination
    methods. Algorithm [4](#alg4 "Algorithm 4 ‣ 3.2.1 Hybrid Model-Free/Model-Based
    Imagination ‣ 3.2 Explicit Planning on Learned Transitions ‣ 3 Survey of Model-Based
    Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey") shows the steps of the algorithm (compared to Algorithm [3](#alg3
    "Algorithm 3 ‣ 3.2 Explicit Planning on Learned Transitions ‣ 3 Survey of Model-Based
    Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey"), the line in italics is new, from Algorithm [2](#alg2 "Algorithm
    2 ‣ 2.3 Model-Free ‣ 2 Background ‣ Model-Based Deep Reinforcement Learning for
    High-Dimensional Problems, a Survey")). Note how the policy is updated twice in
    each iteration, by environment sampling, and by transition planning.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Dyna 在深度学习流行之前引入了一种基于表格的方法。最初，在 Dyna 中，过渡模型直接通过样本更新，而不是通过反向传播学习，然而这里我们只讨论深度想象方法。算法
    [4](#alg4 "算法 4 ‣ 3.2.1 混合模型无关/模型基础想象 ‣ 3.2 学习过渡上的显式规划 ‣ 3 模型基础深度强化学习的调查 ‣ 高维问题的模型基础深度强化学习，调查")
    展示了算法的步骤（与算法 [3](#alg3 "算法 3 ‣ 3.2 学习过渡上的显式规划 ‣ 3 模型基础深度强化学习的调查 ‣ 高维问题的模型基础深度强化学习，调查")
    相比，斜体部分为新增内容，来自算法 [2](#alg2 "算法 2 ‣ 2.3 无模型 ‣ 2 背景 ‣ 高维问题的模型基础深度强化学习，调查")）。注意每次迭代中策略是如何通过环境采样和过渡规划更新两次的。
- en: '| Approach | Learning | Planning | Reinforcement Learning | Application |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 学习 | 规划 | 强化学习 | 应用 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Local Model (Gu et al., [2016](#bib.bib52)) | Quadratic Non-linear | Short
    rollouts | Q-learning | Cheetah |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 局部模型 (Gu et al., [2016](#bib.bib52)) | 二次非线性 | 短期模拟 | Q 学习 | Cheetah |'
- en: '| MVE (Feinberg et al., [2018](#bib.bib42)) | Samples | Short rollouts | Actor-critic
    | Cheetah |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| MVE (Feinberg et al., [2018](#bib.bib42)) | 样本 | 短期模拟 | Actor-critic | Cheetah
    |'
- en: '| Meta Policy (Clavera et al., [2018](#bib.bib28)) | Meta-ensembles | Short
    rollouts | Policy optimization | Cheetah |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 元策略 (Clavera et al., [2018](#bib.bib28)) | 元集成 | 短期模拟 | 策略优化 | Cheetah |'
- en: '| GATS (Azizzadenesheli et al., [2018](#bib.bib7)) | Pix2pix | MCTS | Deep
    Q Network | Cheetah |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| GATS (Azizzadenesheli et al., [2018](#bib.bib7)) | Pix2pix | MCTS | 深度 Q
    网络 | Cheetah |'
- en: '| Policy Optim (Janner et al., [2019](#bib.bib80)) | Ensemble | Short rollouts
    | Soft-Actor-Critic | Cheetah |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 策略优化 (Janner et al., [2019](#bib.bib80)) | 集成 | 短期模拟 | Soft-Actor-Critic
    | Cheetah |'
- en: '| Video predict (Oh et al., [2015](#bib.bib127)) | CNN/LSTM | Action | Curriculum
    | Atari |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 视频预测 (Oh et al., [2015](#bib.bib127)) | CNN/LSTM | 行动 | 课程 | Atari |'
- en: '| VPN (Oh et al., [2017](#bib.bib128)) | CNN encoder | $d$-step | $k$-step
    | Mazes, Atari |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| VPN (Oh et al., [2017](#bib.bib128)) | CNN 编码器 | $d$-步 | $k$-步 | 迷宫，Atari
    |'
- en: '| SimPLe (Kaiser et al., [2019](#bib.bib86)) | VAE, LSTM | MPC | PPO | Atari
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| SimPLe (Kaiser et al., [2019](#bib.bib86)) | VAE, LSTM | MPC | PPO | Atari
    |'
- en: 'TABLE IV: Overview of Hybrid Model-Free/Model-based Imagination Methods'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：混合模型自由/基于模型的想象方法概述
- en: repeat     Sample env $E$ to generate data $D=(s,a,r^{\prime},s^{\prime})$     Use
    $D$ to update policy $\pi(s,a)$     Use $D$ to learn $T_{a}(s,s^{\prime})$     Use
    $T$ to update policy $\pi(s,a)$ by planninguntil $\pi$ converges
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 重复     采样环境 $E$ 以生成数据 $D=(s,a,r^{\prime},s^{\prime})$     使用 $D$ 更新策略 $\pi(s,a)$     使用
    $D$ 学习 $T_{a}(s,s^{\prime})$     使用 $T$ 通过规划更新策略 $\pi(s,a)$直到 $\pi$ 收敛
- en: Algorithm 4 Hybrid Model-Free/Model-Based Imagination
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 混合模型自由/基于模型的想象
- en: We will describe five deep learning approaches that use imagination to augment
    the sample-free data. Table [IV](#S3.T4 "TABLE IV ‣ 3.2.1 Hybrid Model-Free/Model-Based
    Imagination ‣ 3.2 Explicit Planning on Learned Transitions ‣ 3 Survey of Model-Based
    Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey") summarizes these approaches. Note that in the next subsection
    more methods are described that also use hybrid model-free/model-based updating
    of the policy function. These are also listed in Table [IV](#S3.T4 "TABLE IV ‣
    3.2.1 Hybrid Model-Free/Model-Based Imagination ‣ 3.2 Explicit Planning on Learned
    Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based
    Deep Reinforcement Learning for High-Dimensional Problems, a Survey"). We will
    first see how quadratic methods are used with imagination rollouts. Next, short
    rollouts are introduced, and ensembles, to improve the precision of the predictive
    model. Imagination is a hybrid model-free/model-based approach, we will see methods
    that build on successful model-free deep learning approaches such as meta-learning
    and generative-adversarial networks.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将描述五种利用想象力增强无样本数据的深度学习方法。表格[IV](#S3.T4 "TABLE IV ‣ 3.2.1 Hybrid Model-Free/Model-Based
    Imagination ‣ 3.2 Explicit Planning on Learned Transitions ‣ 3 Survey of Model-Based
    Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey")总结了这些方法。请注意，在下一节中还描述了更多方法，这些方法也使用混合模型自由/基于模型的策略函数更新。这些方法也列在表格[IV](#S3.T4
    "TABLE IV ‣ 3.2.1 Hybrid Model-Free/Model-Based Imagination ‣ 3.2 Explicit Planning
    on Learned Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣
    Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey")中。我们将首先看到二次方法如何与想象力模拟一起使用。接下来，将介绍短期模拟和集成方法，以提高预测模型的精度。想象力是一种混合模型自由/基于模型的方法，我们将看到一些建立在成功的无模型深度学习方法之上的方法，如元学习和生成对抗网络。
- en: Let us have a look at how Dyna-style imagination works in deep model-based algorithms.
    Earlier, we saw that linear–quadratic–Gaussian methods were used to improve model
    learning. Gu et al. merge the backpropagation iLQG aproaches with Dyna-style synthetic
    policy rollouts (Gu et al., [2016](#bib.bib52)). To accelerate model-free continuous
    Q-learning they combine locally linear models with local on-policy imagination
    rollouts. The paper introduces a version of continuous Q-learning called normalized
    advantage functions, accelerating the learning with imagination rollouts. Data
    efficiency is improved with model-guided exploration using off-policy iLQG rollouts.
    As application the approach has been tested on simulated robotics tasks such as
    Gripper, Half-Cheetah and Reacher.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解 Dyna 风格的想象如何在深度基于模型的算法中发挥作用。早些时候，我们看到线性-二次-高斯方法被用来改进模型学习。Gu 等人将反向传播
    iLQG 方法与 Dyna 风格的合成策略回滚合并（Gu 等人，[2016](#bib.bib52)）。为了加速无模型连续 Q 学习，他们将局部线性模型与局部策略回滚结合起来。本文介绍了一种称为标准化优势函数的连续
    Q 学习版本，通过想象回滚加速学习。通过使用无策略 iLQG 回滚的模型指导探索，提高了数据效率。作为应用，该方法已在模拟机器人任务中进行了测试，如抓取器、半猎豹和到达者。
- en: Feinberg et al. present model-based value expansion (MVE) which, like the previous
    algorithm (Gu et al., [2016](#bib.bib52)), controls for uncertainty in the deep
    model by only allowing imagination to fixed depth (Feinberg et al., [2018](#bib.bib42)).
    Value estimates are split into a near-future model-based component and a distant
    future model-free component. In contrast to stochastic value gradients (SVG),
    MVE works without differentiable dynamics, which is important since transitions
    can include non-differentiable contact interactions (Heess et al., [2015](#bib.bib67)).
    The planning part of MVE uses short rollouts. The overall reinforcement learning
    algorithm that is used is a combined value-policy actor-critic setting (Sutton
    and Barto, [2018](#bib.bib159)) and deep deterministic policy gradients (DDPG) (Lillicrap
    et al., [2015](#bib.bib110)). As application re-implementations of simulated robotics
    were used such as for Cheetah, Swimmer and Walker.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Feinberg 等人提出了基于模型的价值扩展（MVE），该方法像以前的算法（Gu 等人，[2016](#bib.bib52)）一样，通过仅允许固定深度的想象来控制深度模型中的不确定性（Feinberg
    等人，[2018](#bib.bib42)）。价值估计被分为近未来的基于模型的组件和遥远未来的无模型组件。与随机价值梯度（SVG）不同，MVE 在没有可微分动态的情况下运行，这一点很重要，因为转换可能包括不可微分的接触互动（Heess
    等人，[2015](#bib.bib67)）。MVE 的规划部分使用了短期回滚。所使用的整体强化学习算法是结合了值策略演员-评论员设置（Sutton 和 Barto，[2018](#bib.bib159)）和深度确定性策略梯度（DDPG）（Lillicrap
    等人，[2015](#bib.bib110)）。作为应用，重实现了模拟机器人任务，例如猎豹、游泳者和步态机器人。
- en: 'An ensemble approach has been used in combination with gradient-based meta-learning
    by Clavera et al. who introduced Model-based Reinforcement Learning via Meta-Policy
    Optimization (MP-MPO) (Clavera et al., [2018](#bib.bib28)). This method learns
    an ensemble of dynamics models and then it learns a policy that can be adapted
    quickly to any of the fitted dynamics models with one gradient step (the MAML-like
    meta-learning step (Finn et al., [2017](#bib.bib46))). MB-MPO frames model-based
    reinforcement learning as meta-learning a policy on a distribution of dynamic
    models, in the form of an ensemble of the real environment dynamics. The approach
    builds on the gradient-based meta-learning framework MAML (Finn et al., [2017](#bib.bib46)).
    The planning part of the algorithm samples imagined trajectories. MB-MPO is evaluated
    on continuous control benchmark tasks in a robotics simulator: Ant, Half-Cheetah,
    Hopper, Swimmer, Walker. The results reported indicate that meta-learning a policy
    over an ensemble of learned models approaches the level of performance of model-free
    methods with substantially better sample complexity.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Clavera 等人结合了基于梯度的元学习，使用了一种集成方法，提出了通过元策略优化的基于模型的强化学习（MP-MPO）（Clavera 等人，[2018](#bib.bib28)）。这种方法学习了一组动态模型，然后学习一个可以通过一步梯度更新（类似于
    MAML 的元学习步骤（Finn 等人，[2017](#bib.bib46)））快速适应任何拟合动态模型的策略。MB-MPO 将基于模型的强化学习框架设定为在动态模型的分布上进行元学习，形成实际环境动态的集成。该方法基于基于梯度的元学习框架
    MAML（Finn 等人，[2017](#bib.bib46)）。算法的规划部分采样了想象轨迹。MB-MPO 在机器人模拟器中的连续控制基准任务（如蚂蚁、半猎豹、跳跃者、游泳者、步态机器人）上进行了评估。报告的结果表明，在学习的模型集成上进行元学习的策略接近于无模型方法的性能水平，同时具有显著更好的样本复杂度。
- en: Another attempt to improve the accuracy and efficiency of dynamics models has
    been through generative adversarial networks (Goodfellow et al., [2014](#bib.bib49)).
    Azizzadenesheli et al. aim to combine successes of generative adversarial networks
    with planning robot motion in model-based reinforcement learning (Azizzadenesheli
    et al., [2018](#bib.bib7)). Manipulating robot arms based on video input is an
    important application in AI (see also Visual Foresight in Section [3.2](#S3.SS2
    "3.2 Explicit Planning on Learned Transitions ‣ 3 Survey of Model-Based Deep Reinforcement
    Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional Problems,
    a Survey"), and the SimPLe approach, in Section [9](#S3.F9 "Figure 9 ‣ 3.2.2 Latent
    Models ‣ 3.2 Explicit Planning on Learned Transitions ‣ 3 Survey of Model-Based
    Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey")). A generative dynamics model is introduced to model the
    transition dynamics based on the pix2pix architecture (Isola et al., [2017](#bib.bib78)).
    For planning Monte Carlo Tree Search (Coulom, [2006](#bib.bib29); Browne et al.,
    [2012](#bib.bib21)) is used. GATS is evaluated on Atari games such as Pong, and
    does not perform better than model-free DQN (Mnih et al., [2015](#bib.bib115)).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种提高动态模型准确性和效率的尝试是通过生成对抗网络（Goodfellow et al., [2014](#bib.bib49)）。Azizzadenesheli
    等人旨在将生成对抗网络的成功与基于模型的强化学习中的机器人运动规划相结合（Azizzadenesheli et al., [2018](#bib.bib7)）。基于视频输入操作机器人臂是
    AI 中一个重要的应用（见第 [3.2](#S3.SS2 "3.2 Explicit Planning on Learned Transitions ‣ 3
    Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey") 节），以及第 [9](#S3.F9 "Figure 9
    ‣ 3.2.2 Latent Models ‣ 3.2 Explicit Planning on Learned Transitions ‣ 3 Survey
    of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning
    for High-Dimensional Problems, a Survey") 节中的 SimPLe 方法。引入了生成动态模型以基于 pix2pix 架构建模转换动态（Isola
    et al., [2017](#bib.bib78)）。用于规划的蒙特卡洛树搜索（Coulom, [2006](#bib.bib29); Browne et
    al., [2012](#bib.bib21)）。GATS 在如 Pong 等 Atari 游戏上进行了评估，并且表现不如无模型 DQN（Mnih et al.,
    [2015](#bib.bib115)）。
- en: 'Achieving a good performing high-dimensional predictive model remains a challenge.
    Janner et al.  propose in Model-based Policy Optimization (MBPO) a new approach
    to short rollouts with ensembles (Janner et al., [2019](#bib.bib80)). In this
    approach the model horizon is much shorter than the task horizon. These model
    rollouts are combined with real samples, and matched with plausible environment
    observations (Kalweit and Boedecker, [2017](#bib.bib87)). MBPO uses an ensemble
    of probabilistic networks, as in PETS (Chua et al., [2018](#bib.bib27)). Soft-actor-critic (Haarnoja
    et al., [2018](#bib.bib58)) is used as reinforcement learning method. Experiments
    show that the policy optimization algorithm learns substantially faster with short
    rollouts than other algorithms, while retaining asymptotic performance relative
    to model-free algorithms. The applications used are simulated robotics tasks:
    Hopper, Walker, Half-Cheetah, Ant. The method surpasses the sample efficiency
    of prior model-based algorithms and matches the performance of model-free algorithms.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 实现高维预测模型的良好性能仍然是一个挑战。Janner 等人提出了基于模型的策略优化（MBPO）中一种新的短滚动方法（Janner et al., [2019](#bib.bib80)）。在这种方法中，模型的视野比任务视野短得多。这些模型滚动与真实样本结合，并与合理的环境观测匹配（Kalweit
    和 Boedecker, [2017](#bib.bib87)）。MBPO 使用了概率网络的集成，类似于 PETS（Chua et al., [2018](#bib.bib27)）。软演员评论家（Haarnoja
    et al., [2018](#bib.bib58)）作为强化学习方法。实验表明，策略优化算法通过短滚动学习速度显著快于其他算法，同时保持相对于无模型算法的渐近性能。使用的应用是模拟机器人任务：Hopper、Walker、Half-Cheetah、Ant。该方法超越了先前基于模型算法的样本效率，并与无模型算法的性能相匹配。
- en: Conclusion
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结论
- en: The hybrid imagination methods aim to combine the advantage of model-free methods
    with model-based methods in a hybrid approach augmenting “real” with “imagined”
    samples, to improve sample effciency of deep model-free learning. A problem is
    that inaccuracies in the model may be enlarged in the planned rollouts. Most methods
    limited lookahead to local lookahead. We have discussed interesting approaches
    combining meta-learning and generative-adversarial networks, and ensemble methods
    learning robotic movement directly from images.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 混合想象方法旨在结合无模型方法与有模型方法的优点，通过将“真实”样本与“想象”样本相结合，以提高深度无模型学习的样本效率。一个问题是模型中的不准确性可能在计划的滚动中被放大。大多数方法将前瞻性限制在局部前瞻性上。我们已经讨论了结合元学习和生成对抗网络的有趣方法，以及直接从图像中学习机器人运动的集成方法。
- en: 3.2.2 Latent Models
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 潜在模型
- en: The next group of methods that we describe are the latent or abstract model
    algorithms. Latent models are born out of the need for more accurate predictive
    deep models. Latent models replace the single transition model with separate,
    smaller, specialized, representation models, for the different functions in a
    reinforcement learning algorithm. All of the elements of the MDP-tuple may now
    get their own model. Planning occurs in latent space.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述的下一组方法是潜在或抽象模型算法。潜在模型诞生于对更准确预测深度模型的需求。潜在模型用独立的、更小的、专门的表示模型来替代单一的转移模型，用于强化学习算法中的不同功能。MDP-元组的所有元素现在可以拥有自己的模型。规划发生在潜在空间中。
- en: '| Approach | Learning | Planning | Reinforcement Learning | Application |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Approach | Learning | Planning | Reinforcement Learning | Application |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Video predict (Oh et al., [2015](#bib.bib127)) | CNN/LSTM | Action | Curriculum
    | Atari |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Video predict (Oh et al., [2015](#bib.bib127)) | CNN/LSTM | Action | Curriculum
    | Atari |'
- en: '| VPN (Oh et al., [2017](#bib.bib128)) | CNN encoder | $d$-step | $k$-step
    | Mazes, Atari |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| VPN (Oh et al., [2017](#bib.bib128)) | CNN encoder | $d$-step | $k$-step
    | Mazes, Atari |'
- en: '| SimPLe (Kaiser et al., [2019](#bib.bib86)) | VAE, LSTM | MPC | PPO | Atari
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| SimPLe (Kaiser et al., [2019](#bib.bib86)) | VAE, LSTM | MPC | PPO | Atari
    |'
- en: '| PlaNet (Hafner et al., [2018](#bib.bib59)) | RSSM (VAE/RNN) | CEM | MPC |
    Cheetah |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| PlaNet (Hafner et al., [2018](#bib.bib59)) | RSSM (VAE/RNN) | CEM | MPC |
    Cheetah |'
- en: '| Dreamer (Hafner et al., [2019](#bib.bib60)) | RSSM+CNN | Imagine | Actor-Critic
    | Control |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Dreamer (Hafner et al., [2019](#bib.bib60)) | RSSM+CNN | Imagine | Actor-Critic
    | Control |'
- en: '| Plan2Explore (Sekar et al., [2020](#bib.bib148)) | RSSM | Planning | Few-shot
    | Control |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Plan2Explore (Sekar et al., [2020](#bib.bib148)) | RSSM | Planning | Few-shot
    | Control |'
- en: 'TABLE V: Overview of Latent Modeling Methods'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 潜在建模方法概述'
- en: 'Traditional deep learning models represent input states directly in a single
    model: the layers of neurons and filters are all related in some way to the input
    and output of the domain, be it an image, a sound, a text or a joystick action
    or arm movement. All of the MDP functions, state, value, action, reward, policy,
    and discount, act on this single model. Latent models, on the other hand, are
    not connected directly to the input and output, but are connected to other models
    and signals. They do not work on direct representations, but on latent, more compact,
    representations. The interactions are captured in three to four different models,
    such as observation, representation, transition, and reward models. These may
    be smaller, lower capacity, models. They may be trained with unsupervised or self-supervised
    deep learning such as variational autoencoders (Kingma and Welling, [2013](#bib.bib93),
    [2019](#bib.bib94)) or generative adversarial networks (Goodfellow et al., [2014](#bib.bib49)),
    or recurrent networks. Latent models use multiple specialized networks, one for
    each function to be approximated. The intuition behind the use of latent models
    is dimension reduction: they can better specialize and thus have more precise
    predictions, or can better capture the essence of higher level reasoning in the
    input domains, and need fewer samples (without overfitting) due to their lower
    capacity.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的深度学习模型直接在单一模型中表示输入状态：神经元和滤波器的层次在某种程度上与域的输入和输出有关，无论是图像、声音、文本还是操纵杆动作或手臂运动。所有的
    MDP 功能、状态、价值、动作、奖励、策略和折扣，都作用于这个单一模型。另一方面，潜在模型并不直接与输入和输出连接，而是与其他模型和信号连接。它们不在直接表示上工作，而是在潜在的、更紧凑的表示上工作。交互被捕捉在三到四个不同的模型中，如观察、表示、转移和奖励模型。这些模型可能较小、容量较低。它们可能通过无监督或自监督深度学习进行训练，例如变分自编码器
    (Kingma and Welling, [2013](#bib.bib93), [2019](#bib.bib94)) 或生成对抗网络 (Goodfellow
    et al., [2014](#bib.bib49))，或递归网络。潜在模型使用多个专门的网络，每个函数有一个。潜在模型使用的直觉是降维：它们可以更好地专注，从而具有更精确的预测，或能更好地捕捉输入领域中更高层次推理的本质，并且由于容量较低，需要的样本更少（不会过拟合）。
- en: Figure [9](#S3.F9 "Figure 9 ‣ 3.2.2 Latent Models ‣ 3.2 Explicit Planning on
    Learned Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based
    Deep Reinforcement Learning for High-Dimensional Problems, a Survey") illustrates
    the abstract (latent) learning process (using the modules of Dreamer (Hafner et al.,
    [2019](#bib.bib60)) as an example).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#S3.F9 "Figure 9 ‣ 3.2.2 Latent Models ‣ 3.2 Explicit Planning on Learned
    Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based
    Deep Reinforcement Learning for High-Dimensional Problems, a Survey") 说明了抽象（潜在）学习过程（以
    Dreamer (Hafner et al., [2019](#bib.bib60)) 模块为例）。
- en: <svg   height="171.68" overflow="visible" version="1.1" width="332.17"><g transform="translate(0,171.68)
    matrix(1 0 0 -1 0 0) translate(71.62,0) translate(0,2.12)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 177.41 14.96)" fill="#000000"
    stroke="#000000"><foreignobject width="78.26" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Environment</foreignobject></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -52.14 85.75)"><foreignobject
    width="104.28" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Transition
    Model</foreignobject></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 -66.46 16.23)"><foreignobject width="133.3"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Representation
    Model</foreignobject></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 -57.52 38.5)"><foreignobject width="115.04"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Observation
    Model</foreignobject></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 -44.12 62.12)"><foreignobject width="88.25"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Reward Model</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 72.66 154.02)" fill="#000000" stroke="#000000"><foreignobject
    width="92.05" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Policy
    & Value</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 97.84 5.19)" fill="#000000"
    stroke="#000000"><foreignobject width="48.47" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">learning</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 217.07 111.04)" fill="#000000" stroke="#000000"><foreignobject width="36.9"
    height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">acting</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -36.01 139.47)" fill="#000000" stroke="#000000"><foreignobject
    width="52.27" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">planning</foreignobject></g></g></svg>
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="171.68" overflow="visible" version="1.1" width="332.17"><g transform="translate(0,171.68)
    matrix(1 0 0 -1 0 0) translate(71.62,0) translate(0,2.12)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 177.41 14.96)" fill="#000000"
    stroke="#000000"><foreignobject width="78.26" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">环境</foreignobject></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -52.14 85.75)"><foreignobject
    width="104.28" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">过渡模型</foreignobject></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -66.46 16.23)"><foreignobject width="133.3" height="12.3" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">表征模型</foreignobject></g><g stroke-width="0.8pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -57.52 38.5)"><foreignobject
    width="115.04" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">观察模型</foreignobject></g><g
    stroke-width="0.8pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -44.12 62.12)"><foreignobject width="88.25" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">奖励模型</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 72.66 154.02)" fill="#000000" stroke="#000000"><foreignobject width="92.05"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">策略与价值</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 97.84 5.19)" fill="#000000" stroke="#000000"><foreignobject
    width="48.47" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">学习</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 217.07 111.04)" fill="#000000" stroke="#000000"><foreignobject
    width="36.9" height="11.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">行动</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -36.01 139.47)" fill="#000000" stroke="#000000"><foreignobject
    width="52.27" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">计划</foreignobject></g></g></svg>
- en: 'Figure 9: Latent Models'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 潜在模型'
- en: Table [V](#S3.T5 "TABLE V ‣ 3.2.2 Latent Models ‣ 3.2 Explicit Planning on Learned
    Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based
    Deep Reinforcement Learning for High-Dimensional Problems, a Survey") summarizes
    the methods of this subsection (three are also mentioned in Table [IV](#S3.T4
    "TABLE IV ‣ 3.2.1 Hybrid Model-Free/Model-Based Imagination ‣ 3.2 Explicit Planning
    on Learned Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣
    Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey")).
    Quite a few different latent (abstract) model approaches have been published.
    Latent models work well, both for games and robotics. Different rollout methods
    are proposed, such as local rollouts, and differentiable imagination, and end-to-end
    model learning and planning. Finally, latent models are applied to transfer learning
    in few-shot learning. In the next subsection more methods are described that also
    use latent models (see Table [VI](#S3.T6 "TABLE VI ‣ 3.3 End-to-end Learning of
    Planning and Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning
    ‣ Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey")
    and the overview Table [I](#S3.T1 "TABLE I ‣ 3 Survey of Model-Based Deep Reinforcement
    Learning ‣ Model-Based Deep Reinforcement Learning for High-Dimensional Problems,
    a Survey")).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [V](#S3.T5 "TABLE V ‣ 3.2.2 Latent Models ‣ 3.2 Explicit Planning on Learned
    Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based
    Deep Reinforcement Learning for High-Dimensional Problems, a Survey") 总结了这一小节的方法（表格 [IV](#S3.T4
    "TABLE IV ‣ 3.2.1 Hybrid Model-Free/Model-Based Imagination ‣ 3.2 Explicit Planning
    on Learned Transitions ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣
    Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey")
    中也提到了三种）。已经发布了许多不同的潜在（抽象）模型方法。潜在模型在游戏和机器人领域都表现良好。提出了不同的展开方法，如局部展开、可微分想象，以及端到端的模型学习和规划。最后，潜在模型被应用于少样本学习中的迁移学习。在下一小节中，将描述更多使用潜在模型的方法（参见表格 [VI](#S3.T6
    "TABLE VI ‣ 3.3 End-to-end Learning of Planning and Transitions ‣ 3 Survey of
    Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning
    for High-Dimensional Problems, a Survey") 和概述表格 [I](#S3.T1 "TABLE I ‣ 3 Survey
    of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning
    for High-Dimensional Problems, a Survey")）。
- en: Let us now have a look at the latent model approaches.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下潜在模型的方法。
- en: An important application in games and robotics is the long range prediction
    of video images. Building a generative model for video data is a challenging problem
    involving high-dimensional natural-scene data with temporal dynamics, introduced
    by (Schmidhuber and Huber, [1991](#bib.bib143)). In many applications next-frame
    prediction also depends on control or action variables, especially in games. A
    first paper by (Oh et al., [2015](#bib.bib127)) builds a model to predict Atari
    games using a high-dimensional video encoding model and action-conditional transformation.
    The authors describe three-step experiments with a convolutional and with a recurrent
    (LSTM) encoder. The next step performs action-conditional encoding, after which
    convolutional decoding takes place. To reduce the effect of small prediction errors
    compounding through time, a multi-step prediction target is used. Short-term future
    frames are predicted and fine-tuned to predict longer-term future frames after
    the previous phase converges, using a curriculum that stabilizes training (Bengio
    et al., [2009](#bib.bib14)). Oh et al. perform planning on an abstract, encoded,
    representation; showing the benefit of acting in latent space. Experimental results
    on Atari games showed generation of visually-realistic frames useful for control
    over up to 100-step action-conditional predictions in some games. This architecture
    was developed further into the VPN approach, which we will describe next.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏和机器人领域，一个重要的应用是视频图像的长期预测。构建视频数据的生成模型是一个具有挑战性的问题，涉及高维自然场景数据和时间动态（由 (Schmidhuber
    和 Huber, [1991](#bib.bib143)) 提出）。在许多应用中，下一帧的预测还依赖于控制或动作变量，尤其是在游戏中。Oh 等人 ([2015](#bib.bib127))
    的第一篇论文构建了一个模型来预测 Atari 游戏，使用高维视频编码模型和动作条件转换。作者描述了使用卷积和递归（LSTM）编码器的三步实验。下一步执行动作条件编码，然后进行卷积解码。为了减少小预测误差在时间上的累积影响，使用了多步预测目标。预测短期未来帧，并在前一阶段收敛后微调以预测长期未来帧，使用稳定训练的课程（Bengio
    等人, [2009](#bib.bib14)）。Oh 等人对抽象的编码表示进行规划；展示了在潜在空间中行动的好处。对 Atari 游戏的实验结果显示，生成了在某些游戏中对控制有用的视觉真实帧，支持高达
    100 步的动作条件预测。这一架构进一步发展成 VPN 方法，我们将接下来描述。
- en: 'The Value Prediction Network (VPN) approach (Oh et al., [2017](#bib.bib128))
    integrates model-free and model-based reinforcement learning into a single abstract
    neural network that consists of four modules. For training, VPN combines temporal-difference
    search (Silver et al., [2012](#bib.bib151)) and $n$-step Q-learning (Mnih et al.,
    [2016](#bib.bib116)). VPN performs lookahead planning to choose actions. Classical
    model-based reinforcement learning predicts future observations $T_{a}(s,s^{\prime})$.
    VPN plans future values without having to predict future observations, using abstract
    representations instead. The VPN network architecture consists of the modules:
    encoding, transition, outcome, and value. The encoding module is applied to the
    environment observation to produce a latent state $s$. The value, outcome, and
    transition modules work in latent space, and are recursively applied to expand
    the tree.¹¹1VPN uses a convolutional neural network as the encoding module. The
    transition module consists of an option-conditional convolution layer (see (Oh
    et al., [2015](#bib.bib127))). A residual connection from the previous abstract-state
    to the next asbtract-state is used (He et al., [2016](#bib.bib65)). The outcome
    module is similar to the transition module. The value module consists of two fully-connected
    layers. The number of layers and hidden units varies depending on the application
    domain. It does not use MCTS, but a simpler rollout algorithm that performs planning
    up to a planning horizon. VPN uses imagination to update the policy. It outperforms
    model-free DQN on Mazes and Atari games such as Seaquest, QBert, Krull, and Crazy
    Climber. Value Prediction Networks are related to Value Iteration Networks and
    to the Predictron, which we will describe next.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 值预测网络（VPN）方法（Oh 等，[2017](#bib.bib128)）将无模型和基于模型的强化学习集成到一个由四个模块组成的抽象神经网络中。训练时，VPN
    结合了时间差分搜索（Silver 等，[2012](#bib.bib151)）和 $n$-步 Q 学习（Mnih 等，[2016](#bib.bib116)）。VPN
    执行前瞻性规划以选择动作。经典的基于模型的强化学习预测未来观察 $T_{a}(s,s^{\prime})$。VPN 计划未来值而无需预测未来观察，使用抽象表示代替。VPN
    网络架构由以下模块组成：编码、过渡、结果和价值。编码模块应用于环境观察以生成潜在状态 $s$。价值、结果和过渡模块在潜在空间中工作，并递归应用以扩展树。VPN
    使用卷积神经网络作为编码模块。过渡模块包括一个选项条件卷积层（见（Oh 等，[2015](#bib.bib127)））。从前一个抽象状态到下一个抽象状态的残差连接被使用（He
    等，[2016](#bib.bib65)）。结果模块类似于过渡模块。价值模块由两个全连接层组成。层数和隐藏单元的数量根据应用领域而异。它不使用 MCTS，而是一个更简单的回滚算法，执行到规划视野的规划。VPN
    使用想象来更新策略。它在 Mazes 和 Atari 游戏如 Seaquest、QBert、Krull 和 Crazy Climber 上优于无模型 DQN。值预测网络与值迭代网络和
    Predictron 相关，我们接下来将描述 Predictron。
- en: For robotics and games, video prediction methods are important. Simulated policy
    learning, or SimPLe, uses stochastic video prediction techniques (Kaiser et al.,
    [2019](#bib.bib86)). SimPLe uses video frame prediction as a basis for model-based
    reinforcement learning. In contrast to Visual Foresight, SimPLe builds on model-free
    work on video prediction using variational autoencoders, recurrent world models
    and generative models (Oh et al., [2015](#bib.bib127); Chiappa et al., [2017](#bib.bib26);
    Leibfried et al., [2016](#bib.bib107)) and model-based work (Oh et al., [2017](#bib.bib128);
    Ha and Schmidhuber, [2018b](#bib.bib57); Azizzadenesheli et al., [2018](#bib.bib7)).
    The latent model is formed with a variational autoencoder that is used to deal
    with the limited horizon of past observation frames (Babaeizadeh et al., [2017](#bib.bib8);
    Bengio et al., [2015](#bib.bib13)). The model-free PPO algorithm (Schulman et al.,
    [2017](#bib.bib147)) is used for policy optimization. In an experimental evaluation,
    SimPLe is more sample efficient than the Rainbow algorithm (Hessel et al., [2017](#bib.bib71))
    on 26 ALE games to learn Atari games with 100,000 sample steps (400k frames).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器人技术和游戏，视频预测方法非常重要。模拟策略学习（SimPLe）使用随机视频预测技术（Kaiser 等，[2019](#bib.bib86)）。SimPLe
    将视频帧预测作为基于模型的强化学习的基础。与视觉前瞻（Visual Foresight）不同，SimPLe 建立在使用变分自编码器、递归世界模型和生成模型的视频预测模型无关工作（Oh
    等，[2015](#bib.bib127)；Chiappa 等，[2017](#bib.bib26)；Leibfried 等，[2016](#bib.bib107)）和基于模型的工作（Oh
    等，[2017](#bib.bib128)；Ha 和 Schmidhuber，[2018b](#bib.bib57)；Azizzadenesheli 等，[2018](#bib.bib7)）之上。潜在模型由一个变分自编码器构成，用于处理过去观测帧的有限视野（Babaeizadeh
    等，[2017](#bib.bib8)；Bengio 等，[2015](#bib.bib13)）。用于策略优化的无模型 PPO 算法（Schulman 等，[2017](#bib.bib147)）在实验评估中，SimPLe
    在 26 个 ALE 游戏中比 Rainbow 算法（Hessel 等，[2017](#bib.bib71)）具有更高的样本效率，用于学习 Atari 游戏，样本步数为
    100,000 步（400k 帧）。
- en: 'Learning dynamics models that are accurate enough for planning is a long standing
    challenge, especially in image-based domains. PlaNet trains a model-based agent
    to learn the environment dynamics from images and choose actions through planning
    in latent space with both deterministic and stochastic transition elements. PlaNet
    is introduced in Planning from Pixels (Hafner et al., [2018](#bib.bib59)). PlaNet
    uses a Recurrent State Space Model (RSSM) that consists of a transition model,
    an observation model, a variational encoder and a reward model. Based on these
    models a Model-Predictive Control agent is used to adapt its plan, replanning
    each step. For planning, the RSSM is used by the Cross-Entropy-Method (CEM) to
    search for the best action sequence (Karl et al., [2016](#bib.bib89); Buesing
    et al., [2018](#bib.bib22); Doerr et al., [2018](#bib.bib36)). In contrast to
    many model-free reinforcement learning approaches, no explicit policy or value
    network is used. PlaNet is tested on tasks from MuJoCo and the DeepMind control
    suite: Swing-up, Reacher, Cheetah, Cup Catch. It reaches performance that is close
    to strong model-free algorithms.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 学习足够准确的动态模型以用于规划一直是一个长期挑战，尤其是在基于图像的领域。PlaNet 训练一个基于模型的代理从图像中学习环境动态，并通过在潜在空间中进行规划来选择动作，包括确定性和随机转移元素。PlaNet
    在《Planning from Pixels》（Hafner 等人，[2018](#bib.bib59)）中介绍。PlaNet 使用了一个递归状态空间模型（RSSM），该模型包括一个转移模型、一个观察模型、一个变分编码器和一个奖励模型。基于这些模型，使用模型预测控制代理来调整其计划，每一步进行重新规划。对于规划，RSSM
    由交叉熵法（CEM）使用来搜索最佳动作序列（Karl 等人，[2016](#bib.bib89)；Buesing 等人，[2018](#bib.bib22)；Doerr
    等人，[2018](#bib.bib36)）。与许多无模型强化学习方法不同，不使用显式的策略或价值网络。PlaNet 在 MuJoCo 和 DeepMind
    控制套件中的任务上进行了测试：Swing-up、Reacher、Cheetah、Cup Catch。其性能接近于强大的无模型算法。
- en: 'A year after the PlaNet paper was pubished (Hafner et al., [2019](#bib.bib60))
    published Dream to Control: Learning Behaviors by Latent Imagination. World models
    enable interpolating between past experience, and latent models predict both actions
    and values. The latent models in Dreamer consist of a representation model, an
    observation model, a transition model, and a reward model. It allows the agent
    to plan (imagine) the outcomes of potential action sequences without executing
    them in the environment. It uses an actor-critic approach to learn behaviors that
    consider rewards beyond the horizon. Dreamer backpropagates through the value
    model, similar to DDPG (Lillicrap et al., [2015](#bib.bib110)) and Soft-actor-critic (Haarnoja
    et al., [2018](#bib.bib58)). Dreamer is tested with applications from the DeepMind
    control suite: 20 visual control tasks such as Cup, Acrobot, Hopper, Walker, Quadruped,
    on which it achieves good performance.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '一年后，PlaNet 论文（Hafner 等人，[2019](#bib.bib60)）发表了《Dream to Control: Learning Behaviors
    by Latent Imagination》。世界模型使得能够在过去的经验之间进行插值，而潜在模型则预测动作和价值。Dreamer 的潜在模型包括一个表示模型、一个观察模型、一个转移模型和一个奖励模型。它允许代理在不在环境中执行潜在动作序列的情况下进行计划（想象）。它使用演员-评论家方法来学习考虑超越视野的奖励的行为。Dreamer
    通过价值模型进行反向传播，类似于 DDPG（Lillicrap 等人，[2015](#bib.bib110)）和 Soft-actor-critic（Haarnoja
    等人，[2018](#bib.bib58)）。Dreamer 在 DeepMind 控制套件的应用中进行了测试：20 个视觉控制任务，如 Cup、Acrobot、Hopper、Walker、Quadruped，并取得了良好的表现。'
- en: 'Finally, Plan2Explore (Sekar et al., [2020](#bib.bib148)) studies how reinforcement
    learning with latent models can be used for transfer learning, in partiular, few-shot
    and zero-shot learning (Xian et al., [2017](#bib.bib180)). Plan2Explore is a self-supervised
    reinforcement learning method that learns a world model of its environment through
    unsupervised exploration, which it then uses to solve zero-shot and few-shot tasks.
    Plan2Explore was built on PlaNet (Hafner et al., [2018](#bib.bib59)) and Dreamer (Hafner
    et al., [2019](#bib.bib60)) learning dynamics models from images, using the same
    latent models: image encoder (convolutional neural network), dynamics (recurrent
    state space model), reward predictor, image decoder. With this world model, behaviors
    must be derived for the learning tasks. The agent first uses planning to explore
    to learn a world model in a self-supervised manner. After exploration, it receives
    reward functions to adapt to multiple tasks such as standing, walking, running
    and flipping. Plan2Explore achieved good zero-shot performance on the DeepMind
    Control Suite (Swingup, Hopper, Pendulum, Reacher, Cup Catch, Walker) in the sense
    that the agent’s self-supervised zero-shot performance was competitive to Dreamer’s
    supervised reinforcement learning performance.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Plan2Explore (Sekar et al., [2020](#bib.bib148)) 研究了如何使用潜在模型进行强化学习来进行迁移学习，特别是少样本和零样本学习
    (Xian et al., [2017](#bib.bib180))。Plan2Explore 是一种自监督强化学习方法，通过无监督探索学习环境的世界模型，然后用这个模型解决零样本和少样本任务。Plan2Explore
    基于 PlaNet (Hafner et al., [2018](#bib.bib59)) 和 Dreamer (Hafner et al., [2019](#bib.bib60))，使用相同的潜在模型（图像编码器（卷积神经网络）、动态（递归状态空间模型）、奖励预测器、图像解码器）从图像中学习动态模型。通过这个世界模型，必须为学习任务推导出行为。智能体首先使用规划进行探索，以自监督的方式学习世界模型。探索之后，它接受奖励函数以适应诸如站立、行走、跑步和翻滚等多种任务。Plan2Explore
    在 DeepMind 控制套件（Swingup、Hopper、Pendulum、Reacher、Cup Catch、Walker）上实现了良好的零样本性能，即智能体的自监督零样本性能与
    Dreamer 的监督强化学习性能具有竞争力。
- en: Conclusion
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结论
- en: In the preceding methods we have seen how a single network model can be specialized
    in three or four separate models. Different rollout methods were proposed, such
    as local rollouts, and differentiable imagination. Latent, or abstract, models
    are a direct descendent of model learning networks, with different models for
    different aspects of the reinforcement learning algorithms. The latent representations
    have lower capacity, allowing for greater accuracy, better generalization and
    reduced sample complexity. The smaller latent representation models are often
    learned unsupervised or self-supervised, using variational autoencoders or recurrent
    LSTMs. Latent models were applied to transfer learning in few-shot learning.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方法中，我们已经看到如何将单一网络模型专门化为三个或四个独立的模型。提出了不同的展开方法，如局部展开和可微分的想象。潜在模型或抽象模型是模型学习网络的直接后裔，为强化学习算法的不同方面提供不同的模型。潜在表示具有较低的容量，从而提高了准确性、改善了泛化能力并降低了样本复杂度。较小的潜在表示模型通常采用无监督或自监督学习方式，使用变分自编码器或递归LSTM。潜在模型被应用于少样本学习中的迁移学习。
- en: 3.3 End-to-end Learning of Planning and Transitions
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 规划和过渡的端到端学习
- en: In the previous subsection the approach is (1) to learn a transition model through
    backpropagation and then (2) to do conventional lookahead rollouts using a planning
    algorithm such as value iteration, depth-limited search, or MCTS. A larger trend
    in machine learning is to replace conventional algorithms by differentiable or
    gradient style approaches, that are self-learning and self-adapting. Would it
    be possible to make the conventional rollouts differentiable as well? If updates
    can be made differentiable, why not planning?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一小节中，方法是（1）通过反向传播学习一个过渡模型，然后（2）使用如价值迭代、深度限制搜索或蒙特卡罗树搜索等规划算法进行常规的前瞻性展开。机器学习中的一个大趋势是用可微分或梯度风格的方法取代传统算法，这些方法是自学习和自适应的。是否可以使传统的展开方法也变得可微分？如果更新可以变得可微分，为什么规划不可以呢？
- en: The final approach of this survey is indeed to learn both the transition model
    and planning steps end-to-end. This means that the neural network represents both
    the transition model and executes the planning steps with it. This is a challenge
    that has to do with a single neural network, but we will see that abstract models,
    with latent representations, can more easily be used to achieve the execution
    of planning steps.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的最终方法确实是学习过渡模型和规划步骤的端到端。这意味着神经网络同时表示过渡模型并执行规划步骤。这是一个涉及单一神经网络的挑战，但我们将看到，具有潜在表示的抽象模型可以更容易地用于实现规划步骤。
- en: When we look at the action that a neural network normally performs as a transformation
    and filter activity (selection, or classification) then it is easy to see than
    planning, which consists of state unrolling and selection, is not so far from
    what a neural network is normally used for. Note that especially recurrent neural
    networks and LSTM contain implicit state, making their use as a planner even easier.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们把神经网络的动作视作一种变换和过滤活动（选择或分类）时，我们很容易发现，规划，包括状态展开和选择，实际上与神经网络的常规用途相距不远。值得注意的是，特别是递归神经网络和LSTM包含隐含状态，使得它们作为规划者的使用变得更为容易。
- en: '| Approach | Learning | Planning | Reinforcement Learning | Application |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 学习 | 规划 | 强化学习 | 应用 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| VIN (Tamar et al., [2016](#bib.bib161)) | CNN | Rollout in network | Value
    Iteration | Mazes |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| VIN (Tamar et al., [2016](#bib.bib161)) | CNN | 网络中的回滚 | 值迭代 | 迷宫 |'
- en: '| VProp (Nardelli et al., [2018](#bib.bib124)) | CNN | Hierarch Rollouts |
    Value Iteration | Navigation |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| VProp (Nardelli et al., [2018](#bib.bib124)) | CNN | 层次回滚 | 值迭代 | 导航 |'
- en: '| TreeQN (Farquhar et al., [2018](#bib.bib40)) | Tree-shape Net | Plan-functions
    | DQN/Actor-Critic | Box pushing |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| TreeQN (Farquhar et al., [2018](#bib.bib40)) | 树形网络 | 计划函数 | DQN/演员-评论家 |
    推箱子 |'
- en: '| ConvLSTM (Guez et al., [2019](#bib.bib54)) | CNN+LSTM | Rollouts in network
    | A3C | Sokoban |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| ConvLSTM (Guez et al., [2019](#bib.bib54)) | CNN+LSTM | 网络中的回滚 | A3C | 推箱子
    |'
- en: '| I2A (Weber et al., [2017](#bib.bib177)) | CNN/LSTM encoder | Meta-controller
    | A3C | Sokoban |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| I2A (Weber et al., [2017](#bib.bib177)) | CNN/LSTM 编码器 | 元控制器 | A3C | 推箱子
    |'
- en: '| Predictron (Silver et al., [2017b](#bib.bib154)) | $k,\gamma,\lambda$-CNN-predictr
    | $k$-rollout | $\lambda$-accum | Mazes |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| Predictron (Silver et al., [2017b](#bib.bib154)) | $k,\gamma,\lambda$-CNN预测器
    | $k$-回滚 | $\lambda$-累计 | 迷宫 |'
- en: '| World Model (Ha and Schmidhuber, [2018b](#bib.bib57)) | VAE | CMA-ES | MDN-RNN
    | Car Racing |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 世界模型 (Ha 和 Schmidhuber, [2018b](#bib.bib57)) | VAE | CMA-ES | MDN-RNN | 赛车
    |'
- en: '| MuZero (Schrittwieser et al., [2019](#bib.bib146)) | Latent | MCTS | Curriculum
    | Go/chess/shogi+Atari |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| MuZero (Schrittwieser et al., [2019](#bib.bib146)) | 潜在 | MCTS | 课程 | 围棋/国际象棋/将棋+Atari
    |'
- en: 'TABLE VI: Overview of End-to-End Planning/Transition Methods'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：端到端规划/过渡方法概述
- en: Some progress has been made with this idea. One approach is to map the planning
    iterations onto the layers of a deep neural network, with each layer representing
    a lookahead step. The transition *model* becomes embedded in a transition *network*,
    see Figure [10](#S3.F10 "Figure 10 ‣ 3.3 End-to-end Learning of Planning and Transitions
    ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey").
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个想法上已有一些进展。一种方法是将规划迭代映射到深度神经网络的层上，每一层代表一个前瞻步骤。过渡*模型*嵌入在一个过渡*网络*中，见图[10](#S3.F10
    "Figure 10 ‣ 3.3 End-to-end Learning of Planning and Transitions ‣ 3 Survey of
    Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning
    for High-Dimensional Problems, a Survey")。
- en: <svg   height="93.22" overflow="visible" version="1.1" width="412.17"><g transform="translate(0,93.22)
    matrix(1 0 0 -1 0 0) translate(152.22,0) translate(0,2.39)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 118.35 14.96)" fill="#000000"
    stroke="#000000"><foreignobject width="78.26" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Environment</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -72.43 16.23)" fill="#000000" stroke="#000000"><foreignobject width="106.01"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">(Latent)
    Trans Network</foreignobject></g><g transform="matrix(1.0 0.0 0.0 1.0 39.82 75.28)"
    fill="#000000" stroke="#000000"><foreignobject width="78.99" height="13.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Policy/Value</foreignobject></g><g stroke-width="0.8pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 71.16 4.91)" fill="#000000" stroke="#000000"><foreignobject
    width="48.47" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">learning</foreignobject></g></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 163.38 45.94)" fill="#000000"
    stroke="#000000"><foreignobject width="36.9" height="11.93" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">acting</foreignobject></g></g></g><g stroke-width="0.8pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -144.81 45.75)" fill="#000000" stroke="#000000"><foreignobject
    width="83.44" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">differentiable
    planning</foreignobject></g></g></g></svg>
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: End-to-End Planning/Transitions'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the planner becomes part of one large trained end-to-end agent.
    (In the figure the full circle is made bold to signal end-to-end learning.) Table [VI](#S3.T6
    "TABLE VI ‣ 3.3 End-to-end Learning of Planning and Transitions ‣ 3 Survey of
    Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement Learning
    for High-Dimensional Problems, a Survey") summarizes the approaches of this subsection.
    We will see how the iterations of value iteration can be implemented in the layers
    of a convolutional neural network (CNN). Next, two variations of this method are
    presented, and a way to implement planning with convolutional LSTM modules. All
    these approaches implement differentiable, trainable, planning algorithms, that
    can generalize to different inputs. The later methods use elaborate schemes with
    latent models so that the learning can be applied to different application domains.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Value Iteration Networks (VIN) are introduced by Tamar et al. in (Tamar et al.,
    [2016](#bib.bib161)), see also (Niu et al., [2018](#bib.bib126)). A VIN is a differentiable
    multi-layer network that is used to perform the steps of a simple planning algorithm.
    The core idea it that value iteration (VI, see Algorithm [1](#alg1 "Algorithm
    1 ‣ 2.2 Planning ‣ 2 Background ‣ Model-Based Deep Reinforcement Learning for
    High-Dimensional Problems, a Survey")) or step-by-step planning can be implemented
    by a multi-layer convolutional network: each layer does a step of lookahead. The
    VI iterations are rolled-out in the network layers $Q$ with $A$ channels. Through
    backpropagation the model learns the value iteration parameters. The aim is to
    learn a general model, that can navigate in unseen environments. VIN learns a
    fully differentiable planning algorithm. The idea of planning by gradient descent
    exists for some time, several authors explored learning approximations of dynamics
    in neural networks (Kelley, [1960](#bib.bib90); Schmidhuber, [1990a](#bib.bib144);
    Ilin et al., [2007](#bib.bib76)). VIN can be used for discrete and continuous
    path planning, and has been tried in grid world problems and natural language
    tasks. VIN has achieved generalization of finding shortest paths in unseen mazes.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 价值迭代网络（VIN）由Tamar等人（Tamar et al., [2016](#bib.bib161)）引入，另见（Niu et al., [2018](#bib.bib126)）。VIN是一个可微分的多层网络，用于执行简单规划算法的步骤。核心思想是，通过多层卷积网络可以实现价值迭代（VI，见算法[1](#alg1
    "Algorithm 1 ‣ 2.2 Planning ‣ 2 Background ‣ Model-Based Deep Reinforcement Learning
    for High-Dimensional Problems, a Survey)")或逐步规划：每一层执行一步前瞻。VI迭代在网络层$Q$中展开，$Q$有$A$个通道。通过反向传播，模型学习价值迭代参数。目标是学习一个通用模型，能够在未见过的环境中导航。VIN学习了一个完全可微分的规划算法。梯度下降规划的思想存在已久，一些作者探讨了神经网络中动力学的近似学习（Kelley,
    [1960](#bib.bib90); Schmidhuber, [1990a](#bib.bib144); Ilin et al., [2007](#bib.bib76)）。VIN可用于离散和连续路径规划，并已在网格世界问题和自然语言任务中进行尝试。VIN在未见过的迷宫中找到了最短路径的泛化。
- en: However, a limitation of VIN is that the number of layers of the CNN restricts
    the number of planning steps, restricting VINs to small and low-dimensional domains.
    Schleich et al. (Schleich et al., [2019](#bib.bib142)) extend VINs by adding abstraction,
    and Srinivas et al. (Srinivas et al., [2018](#bib.bib156)) introduce universal
    planning networks, UPN, which generalize to modified robot morphologies. VProp,
    or Value Propagation (Nardelli et al., [2018](#bib.bib124)) is another attempt
    at creating generalizable planners inspired by VIN. By using a hierarchical structure
    VProp has the ability to generalize to larger map sizes and dynamic environments.
    VProp not only learns to plan and navigate in dynamic environments, but their
    hierarchical structure provides a way to generalize to navigation tasks where
    the required planning horizon and the size of the map are much larger than the
    ones seen at training time. VProp is evaluated on grid-worlds and also on dynamic
    environments and on a navigation scenario from StarCraft.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，VIN的一个限制是CNN的层数限制了规划步骤的数量，使得VIN仅适用于小型和低维度领域。Schleich等人（Schleich et al., [2019](#bib.bib142)）通过添加抽象扩展了VIN，而Srinivas等人（Srinivas
    et al., [2018](#bib.bib156)）引入了通用规划网络UPN，这些网络可以推广到修改后的机器人形态。VProp，即价值传播（Nardelli
    et al., [2018](#bib.bib124)），是另一种受VIN启发的尝试，旨在创建可泛化的规划器。通过使用层次结构，VProp具有推广到更大地图尺寸和动态环境的能力。VProp不仅学会了在动态环境中规划和导航，而且其层次结构提供了一种泛化到需要的规划范围和地图大小远大于训练时所见的导航任务的方法。VProp在网格世界、动态环境以及《星际争霸》的导航场景中进行了评估。
- en: A different approach is taken in TreeQN/ATreeC. Again, the aim is to create
    differentiable tree planning for deep reinforcement learning (Farquhar et al.,
    [2018](#bib.bib40)). As VIN, TreeQN is focused on combining planning and deep
    reinforcement learning. Unlike VIN, however, TreeQN does so by incorporating a
    recursive tree structure in the network. It models an MDP by incorporating an
    explicit encoder function, a transition function, a reward function, a value function,
    and a backup function (see also latent models in the next subsection). In this
    way, it aims to achieve the same goal as VIN, that is, to create a differentiable
    neural network architecture that is suitable for planning. TreeQN is based on
    DQN-value-functions, an actor-critic variant is proposed as ATreeC. TreeQN is
    a prelude to latent models methods in the next subsection. In addition to being
    related to VIN, this approach is also related to VPN (Oh et al., [2017](#bib.bib128))
    and the Predictron (Silver et al., [2017b](#bib.bib154)). TreeQN is tried on box
    pushing applications, like Sokoban, and nine Atari games.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TreeQN/ATreeC 中采取了不同的方法。再次，目标是为深度强化学习创建可微分的树形规划（Farquhar 等人，[2018](#bib.bib40)）。与
    VIN 一样，TreeQN 也专注于将规划与深度强化学习结合起来。然而，与 VIN 不同，TreeQN 通过在网络中融入递归树结构来实现这一点。它通过整合显式的编码器函数、转移函数、奖励函数、价值函数和备份函数来建模一个
    MDP（见下节中的潜在模型）。通过这种方式，它旨在实现与 VIN 相同的目标，即创建一个适合规划的可微分神经网络架构。TreeQN 基于 DQN 值函数，并提出了一种作为
    ATreeC 的 actor-critic 变体。TreeQN 是下节潜在模型方法的序曲。除了与 VIN 相关之外，这种方法还与 VPN（Oh 等人，[2017](#bib.bib128)）和
    Predictron（Silver 等人，[2017b](#bib.bib154)）有关。TreeQN 在类似 Sokoban 的箱子推动应用和九款 Atari
    游戏中进行了尝试。
- en: Another approach to differentiable planning is to teach a sequence of convolutional
    neural networks to exhibit planning behavior. A paper by (Guez et al., [2019](#bib.bib54))
    takes this approach. The paper demonstrates that a neural network architecture
    consisting of modules of a convolutional network and LSTM can learn to exhibit
    the behavior of a planner. In this approach the planning occurs implicitly, by
    the network, which the authors call model-free planning, in contrast to the previous
    approaches in which the network structure more explicitly resembles a planner (Farquhar
    et al., [2018](#bib.bib40); Guez et al., [2018](#bib.bib53); Tamar et al., [2016](#bib.bib161)).
    In this method model-based behavior is learned with a general recurrent architecture
    consisting of LSTMs and a convolutional network (Schmidhuber, [1990b](#bib.bib145))
    in the form of a stack of ConvLSTM modules (Xingjian et al., [2015](#bib.bib181)).
    For the learning of the ConvLSTM modules the A3C actor-critic approach is used (Mnih
    et al., [2016](#bib.bib116)). The method is tried on Sokoban and Boxworld (Zambaldi
    et al., [2018](#bib.bib183)). A stack of depth $D$, repreated $N$ times (time-ticks)
    allows the network to plan. In harder Sokoban instances, larger capacity networks
    with larger depth performed better. The experiments used a large number of environment
    steps, future work should investigate how to achieve sample-efficiency with this
    architecture.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可微分规划的方法是教一系列卷积神经网络表现出规划行为。Guez 等人（[2019](#bib.bib54)）的论文采用了这种方法。论文展示了由卷积网络和
    LSTM 模块组成的神经网络架构能够学习展现规划者的行为。在这种方法中，规划是隐式发生的，通过网络实现，这种方法被作者称为无模型规划，与之前的那些网络结构更明确地类似于规划者的方法不同（Farquhar
    等人，[2018](#bib.bib40)；Guez 等人，[2018](#bib.bib53)；Tamar 等人，[2016](#bib.bib161)）。在这种方法中，模型基于的行为是通过一个包含
    LSTM 和卷积网络的通用递归架构来学习的（Schmidhuber，[1990b](#bib.bib145)），其形式是一个 ConvLSTM 模块的堆叠（Xingjian
    等人，[2015](#bib.bib181)）。对于 ConvLSTM 模块的学习，使用了 A3C actor-critic 方法（Mnih 等人，[2016](#bib.bib116)）。该方法在
    Sokoban 和 Boxworld 上进行了尝试（Zambaldi 等人，[2018](#bib.bib183)）。一个深度为 $D$ 的堆栈，重复 $N$
    次（时间刻度），使网络能够进行规划。在更复杂的 Sokoban 实例中，容量更大的网络和更大的深度表现得更好。实验使用了大量的环境步骤，未来的工作应该研究如何在这种架构下实现样本效率。
- en: Conclusion
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结论
- en: Planning networks combine planning and transition learning. They fold the planning
    into the network, making the planning process itself differentiable. The network
    then learns which planning decisions to make. Value Iteration Networks have shown
    how learning can transfer to mazes that have not been seen before. A drawback
    is that due to the marriage of problem size and network topology the approach
    has been limited to smaller sizes, something that subsequent methods have tried
    to reduce. One of these approaches is TreeQN, which uses multiple smaller models
    and a tree-structured network. The related Predictron architecture (Silver et al.,
    [2017b](#bib.bib154)) also learns planning end-to-end, and is applicable to different
    kinds and sizes of problems.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 规划网络将规划和过渡学习结合起来。它们将规划融入网络，使得规划过程本身可微分。网络然后学习做出哪些规划决策。价值迭代网络展示了学习如何迁移到以前未见过的迷宫。一个缺点是，由于问题规模与网络拓扑的结合，这种方法被限制在较小的规模上，后续方法尝试减少这一限制。其中一种方法是
    TreeQN，它使用多个较小的模型和一个树状结构的网络。相关的 Predictron 架构 (Silver et al., [2017b](#bib.bib154))
    也学习端到端的规划，适用于不同种类和规模的问题。
- en: The Predictron uses abstract models, and will be discussed in the next subsection.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Predictron 使用抽象模型，这将在下一小节中讨论。
- en: 3.3.1 End-to-End Planning/Transitions with Latent Models
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 端到端规划/过渡与潜在模型
- en: We will now discuss latent model approaches in end-to-end learning of planning
    and transitions.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论在端到端学习规划和过渡中的潜在模型方法。
- en: 'The first abstract imagination-based approach that we discuss is Imagination-Augmented
    Agent, or I2A, by (Pascanu et al., [2017](#bib.bib130); Weber et al., [2017](#bib.bib177);
    Buesing et al., [2018](#bib.bib22)). A problem of model-based algorithms is the
    sensitivity of the planning to model imperfections. I2A deals with these imperfections
    by introducing a latent model, that learns to interpret internal simulations and
    adapt a strategy to the current state. I2A uses latent models of the environment,
    based on (Chiappa et al., [2017](#bib.bib26); Buesing et al., [2018](#bib.bib22)).
    The core architectural feature of I2A is an environment model, a recurrent architecture
    trained unsupervised from agent trajectories. I2A has four elements that together
    constitute the abstract model: (1) It has a manager that constructs a plan, which
    can be implemented with a CNN. (2) It has a controller that creates an action
    policy. (3) It has an environment model to do imagination. (4) Finally, it has
    a memory, which can be implemented with an LSTM (Pascanu et al., [2017](#bib.bib130)).
    I2A uses a manager or meta-controller to choose between rolling out actions in
    the environment or by imagination (see (Hamrick et al., [2017](#bib.bib62))).
    This allows the use of models which only coarsely capture the environmental dynamics,
    even when those dynamics are not perfect. The I2A network uses a recurrent architecture
    in which a CNN is trained from agent trajectories with A3C (Mnih et al., [2016](#bib.bib116)).
    I2A achieves success with little data and imperfect models, optimizing point-estimates
    of the expected Q-values of the actions in a discrete action space. I2A is applied
    to Sokoban and Mini-Pacman by (Weber et al., [2017](#bib.bib177); Buesing et al.,
    [2018](#bib.bib22)). Performance is compared favorably to model-free and planning
    algorithms (MCTS). Pascanu et al. apply the approach on a maze and a spaceship
    task (Pascanu et al., [2017](#bib.bib130)).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的第一个基于抽象想象的方法是 Imagination-Augmented Agent，或称 I2A，由 (Pascanu et al., [2017](#bib.bib130);
    Weber et al., [2017](#bib.bib177); Buesing et al., [2018](#bib.bib22)) 提出。模型基础算法的问题在于规划对模型缺陷的敏感性。I2A
    通过引入潜在模型来处理这些缺陷，潜在模型学习解释内部模拟并根据当前状态调整策略。I2A 使用基于 (Chiappa et al., [2017](#bib.bib26);
    Buesing et al., [2018](#bib.bib22)) 的环境潜在模型。I2A 的核心架构特征是环境模型，这是一个从代理轨迹中无监督训练的递归架构。I2A
    具有四个元素，这些元素共同构成了抽象模型：(1) 它具有一个构建计划的管理器，可以通过 CNN 实现。(2) 它具有一个创建行动策略的控制器。(3) 它具有进行想象的环境模型。(4)
    最后，它具有一个可以用 LSTM (Pascanu et al., [2017](#bib.bib130)) 实现的记忆。I2A 使用一个管理器或元控制器来选择在环境中实施行动还是通过想象
    (见 (Hamrick et al., [2017](#bib.bib62)))。这允许使用仅粗略捕捉环境动态的模型，即使这些动态并不完美。I2A 网络使用递归架构，其中
    CNN 从代理轨迹中通过 A3C (Mnih et al., [2016](#bib.bib116)) 进行训练。I2A 在数据少和模型不完美的情况下取得成功，优化离散动作空间中行动的期望
    Q 值的点估计。I2A 被应用于 Sokoban 和 Mini-Pacman (Weber et al., [2017](#bib.bib177); Buesing
    et al., [2018](#bib.bib22))。与模型自由和规划算法 (MCTS) 相比，性能有利可比。Pascanu et al. 在迷宫和飞船任务中应用了该方法
    (Pascanu et al., [2017](#bib.bib130))。
- en: 'Planning networks (VIN) combine planning and learning end-to-end. A limitation
    of VIN is that the tight connection between problem domain, iteration algorithm,
    and network architecture limited the applicability to small grid world problem.
    The Predictron introduces an abstract model to remove this limitation. The Predictron
    was introduced by Silver et al. and combines end-to-end planning and model learning (Silver
    et al., [2017b](#bib.bib154)). As with (Oh et al., [2017](#bib.bib128)), the model
    is an abstract model that consists of four components: a representation model,
    a next-state model, a reward model, and a discount model. All models are differentiable.
    The goal of the abstract model in Predictron is to facilitate value prediction
    (not state prediction) or prediction of pseudo-reward functions that can encode
    special events, such as “staying alive” or “reaching the next room.” The planning
    part rolls forward its internal model $k$ steps. As in the Dyna architecture,
    imagined forward steps can be combined with samples from the actual environment,
    combining model-free and model-based updates. The Predictron has been applied
    to procedurally generated mazes and a simulated pool domain. In both cases it
    out-performed model-free algorithms.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 规划网络（VIN）将规划和学习结合起来。VIN的一个限制是问题领域、迭代算法和网络架构之间的紧密联系使其只能应用于小型网格世界问题。Predictron引入了一个抽象模型来消除这一限制。Predictron由Silver等人提出，结合了端到端的规划和模型学习（Silver等，[2017b](#bib.bib154)）。与（Oh等，[2017](#bib.bib128)）类似，该模型是一个包含四个组件的抽象模型：表示模型、下一状态模型、奖励模型和折扣模型。所有模型都是可微分的。Predictron中抽象模型的目标是促进价值预测（而非状态预测）或伪奖励函数的预测，这些函数可以编码特殊事件，如“保持生命”或“到达下一个房间”。规划部分将其内部模型前滚$k$步。与Dyna架构类似，想象中的前滚步骤可以与实际环境中的样本结合，结合了无模型和基于模型的更新。Predictron已应用于程序生成的迷宫和模拟的台球领域。在这两种情况下，它都优于无模型算法。
- en: Latent models of the dynamics of the environment can also be viewed as World
    Models, a term used by (Ha and Schmidhuber, [2018a](#bib.bib56), [b](#bib.bib57)).
    World Models are inspired by the manner in which humans are thought to contruct
    a mental model of the world in which we live. World Models are generative recurrent
    neural networks that are trained unsupervised to generate states for simulation
    using a variational autoencoder and a recurrent network. They learn a compressed
    spatial and temporal representation of the environment. By using features extracted
    from the World Model as inputs to the agent, a compact and simple policy can be
    trained to solve a task, and planning occurs in the compressed or simplified world.
    For a visual environment, World Models consist of a vision model, a memory model,
    and a controller. The vision model is often trained unsupervised with a variational
    autoencoder. The memory model is approximated with a mixture density network of
    a Gaussian distribution (MDN-RNN) (Bishop, [1994](#bib.bib16); Graves, [2013](#bib.bib51)).
    The controller model is a linear model that maps directly to actions. It uses
    the CMA-ES Evolutionary Strategy for optimizing Controller models. Rollouts in
    World Models are also called dreams, to contrast them with samples from the real
    environment. With World Models a policy can in principle even be trained completely
    inside the dream, using imagination only inside the World Model, to test it out
    later in the actual environment. World Models have been applied experimentally
    to VizDoom tasks such as Car Racing (Kempka et al., [2016](#bib.bib91)).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 环境动态的潜在模型也可以视为世界模型，这是（Ha和Schmidhuber，[2018a](#bib.bib56)，[b](#bib.bib57)）使用的术语。世界模型的灵感来自于人们被认为构建的世界的心理模型。世界模型是生成的递归神经网络，采用无监督训练以使用变分自编码器和递归网络生成模拟状态。它们学习环境的压缩空间和时间表示。通过使用从世界模型中提取的特征作为代理的输入，可以训练出一个紧凑且简单的策略来解决任务，规划则在压缩或简化的世界中进行。对于视觉环境，世界模型由视觉模型、记忆模型和控制器组成。视觉模型通常使用变分自编码器进行无监督训练。记忆模型用高斯分布的混合密度网络（MDN-RNN）（Bishop，[1994](#bib.bib16);
    Graves，[2013](#bib.bib51)）进行近似。控制器模型是一个线性模型，直接映射到动作。它使用CMA-ES进化策略来优化控制器模型。世界模型中的回滚也称为梦，与实际环境中的样本相对。通过世界模型，原则上可以完全在梦中训练策略，仅使用想象中的世界模型，之后在实际环境中进行测试。世界模型已在VizDoom任务中进行实验应用，如赛车（Kempka等，[2016](#bib.bib91)）。
- en: 'Taking the development of AlphaZero further is the work on MuZero (Schrittwieser
    et al., [2019](#bib.bib146)). Board games are well suited for model-based methods
    because the transition function is given by the rules of the game. However, would
    it be possible to do well if the rules of the game were not given? In MuZero a
    new architecture is used to learn transition functions for a range of different
    games, from Atari to board games. MuZero learns the transition model for all games
    from interaction with the environment, with one architecture, that is able to
    learn different transition models. As with the Predictron (Silver et al., [2017b](#bib.bib154))
    and Value Prediction Networks (Oh et al., [2017](#bib.bib128)), MuZero has an
    abstract model with different modules: representation, dynamics, and prediction
    function. The dynamics function is a recurrent process that computes transition
    latent state) and reward. The prediction function computes policy and value functions.
    For planning, MuZero uses a version of MCTS, without the rollouts, and with P-UCT
    as selection rule, using information from the abstract model as input for node
    selection. MuZero can be regarded as joining the Predictron with self-play. It
    performs well on Atari games and on board games, learning to play the games from
    scratch, after having learned the rules of the games from scratch from the environment.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 将AlphaZero的发展进一步推进就是MuZero的工作（Schrittwieser等，[2019](#bib.bib146)）。棋盘游戏非常适合基于模型的方法，因为过渡函数由游戏规则给出。然而，如果游戏规则没有给出，这样的做法是否仍然可行？MuZero使用了一种新的架构来学习一系列不同游戏的过渡函数，从Atari到棋盘游戏。MuZero通过与环境的互动学习所有游戏的过渡模型，使用一种能够学习不同过渡模型的架构。与Predictron（Silver等，[2017b](#bib.bib154)）和价值预测网络（Oh等，[2017](#bib.bib128)）类似，MuZero具有一个包含不同模块的抽象模型：表示、动态和预测函数。动态函数是一个递归过程，用于计算过渡潜在状态和奖励。预测函数计算策略和值函数。在规划方面，MuZero使用了一个版本的MCTS，省略了回放，并采用P-UCT作为选择规则，使用来自抽象模型的信息作为节点选择的输入。MuZero可以看作是将Predictron与自我对弈结合在一起。它在Atari游戏和棋盘游戏上表现良好，从零开始学习游戏，通过从环境中学习游戏规则从零开始学习玩游戏。
- en: Conclusion
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结论
- en: Latent models represent states with a number of smaller, latent models, allowing
    planning to happen in a smaller latent space. They are useful for explicit planning
    and with end-to-end learnable planning, as in the Predictron (Silver et al., [2017b](#bib.bib154)).
    Latent models allow end-to-end planning to be applied to a broader range of applications,
    beyond small mazes. The Predictron creates an abstract planning network, in the
    spirit of, but without the limitations of, Value Iteration Networks (Tamar et al.,
    [2016](#bib.bib161)). The World Models interpretation links latent models to the
    way in which humans create mental models of the world that we live in.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在模型通过一系列较小的潜在模型表示状态，从而允许在较小的潜在空间中进行规划。它们对于显式规划以及像Predictron（Silver等，[2017b](#bib.bib154)）那样的端到端可学习规划非常有用。潜在模型使得端到端规划可以应用于更广泛的应用场景，而不仅仅是小型迷宫。Predictron创建了一个抽象的规划网络，其精神类似于价值迭代网络（Tamar等，[2016](#bib.bib161)），但没有价值迭代网络的限制。世界模型解释将潜在模型与人类如何创建我们生活的世界的心理模型联系起来。
- en: After this detailed survey of model-based methods—the agents—it is time to discuss
    our findings and draw conclusions. Before we do so, we will first look at one
    of the most important elements for reproducible reinforcement learning research,
    the benchmark.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在对基于模型的方法——代理——进行详细调查之后，现在是讨论我们的发现并得出结论的时候了。在此之前，我们将首先查看可重复强化学习研究的一个重要元素：基准测试。
- en: '![Refer to caption](img/d25a41198fee22e17736d20ce80ab29b.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d25a41198fee22e17736d20ce80ab29b.png)'
- en: 'Figure 11: Cartpole Pendulum (Sutton and Barto, [2018](#bib.bib159))'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：Cartpole Pendulum（Sutton和Barto，[2018](#bib.bib159)）
- en: 4 Benchmarks
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基准测试
- en: Benchmarks—the environments—play a key role in artificial intelligence. Without
    them, progress cannot be measured, and results cannot be compared in a meaningful
    way. The benchmarks define the kind of intelligence that our artificial methods
    should approach. For reinforcement learning, Mountain car and Cartpole are well-known
    small problems that characterize the kind of problem to solve (see Figure [11](#S3.F11
    "Figure 11 ‣ Conclusion ‣ 3.3 End-to-end Learning of Planning and Transitions
    ‣ 3 Survey of Model-Based Deep Reinforcement Learning ‣ Model-Based Deep Reinforcement
    Learning for High-Dimensional Problems, a Survey")). Chess has been called the
    Drosophila of AI (Landis and Yaglom, [2001](#bib.bib103)). In addition to Mountain
    car and chess a series of benchmark applications have been used to measure progress
    of artificially intelligent methods. Some of the benchmarks are well-known and
    have been driving progress. In image recognition, the ImageNet sequence of competitions
    has stimulated great progress (Fei-Fei et al., [2009](#bib.bib41); Krizhevsky
    et al., [2012](#bib.bib100); Guo et al., [2016](#bib.bib55)). The current focus
    on reproducibility in reinforcement learning emphasizes the importance of benchmarks (Henderson
    et al., [2017](#bib.bib68); Islam et al., [2017](#bib.bib77); Khetarpal et al.,
    [2018](#bib.bib92)).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试——这些环境——在人工智能中发挥着关键作用。如果没有它们，进展无法衡量，结果也无法以有意义的方式进行比较。基准测试定义了我们的人工方法应接近的智能类型。对于强化学习而言，Mountain
    car和Cartpole是广为人知的小问题，它们体现了要解决的问题类型（见图[11](#S3.F11 "图 11 ‣ 结论 ‣ 3.3 端到端规划与过渡学习
    ‣ 3 模型驱动的深度强化学习综述 ‣ 高维问题的模型驱动深度强化学习综述")）。象棋被称为人工智能的果蝇（Landis和Yaglom，[2001](#bib.bib103)）。除了Mountain
    car和象棋之外，还有一系列基准应用被用来衡量人工智能方法的进展。一些基准广为人知，并推动了进步。在图像识别中，ImageNet比赛系列刺激了巨大的进展（Fei-Fei等，[2009](#bib.bib41)；Krizhevsky等，[2012](#bib.bib100)；Guo等，[2016](#bib.bib55)）。当前对强化学习中的可重复性关注强调了基准的重要性（Henderson等，[2017](#bib.bib68)；Islam等，[2017](#bib.bib77)；Khetarpal等，[2018](#bib.bib92)）。
- en: Most papers that introduce new model-based reinforcement learning algorithms
    perform some form of experimental evaluation of the algorithm. Still, since papers
    use different versions and hyper-parameter settings, comparing algorithm performance
    remains difficult in practice. A recent benchmarking study compared the performance
    of 14 algorithms, and some baseline algorithms on a number of MuJoCo (Todorov
    et al., [2012](#bib.bib167)) robotics benchmarks (Wang et al., [2019b](#bib.bib175)).
    There was no clear winner. Performance of methods varied widely from application
    to application. There is much room for further improvement on many applications
    of model-based reinforcement learning algorithms, and for making methods more
    robust. The use of benchmarks should become more standardized to ease meaningful
    performance comparisons.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数介绍新模型驱动强化学习算法的论文都会对算法进行某种形式的实验评估。然而，由于论文使用不同的版本和超参数设置，比较算法性能在实践中仍然很困难。一项最新的基准研究比较了14种算法的性能，以及一些基准算法在多个MuJoCo（Todorov等，[2012](#bib.bib167)）机器人基准上的表现（Wang等，[2019b](#bib.bib175)）。没有明确的胜者。方法的性能在不同应用中差异很大。许多模型驱动的强化学习算法在应用中的改进空间很大，并且使方法更具鲁棒性。基准的使用应变得更加标准化，以便于进行有意义的性能比较。
- en: 'We will now describe benchmarks commonly used in deep model-based reinforcement
    learning. We will discuss five sets of benchmarks: (1) puzzles and mazes, (2)
    Atari arcade games such as Pac-Man, (3) board games such as Go and chess, (4)
    real-time strategy games such as StarCraft, and (5) simulated robotics tasks such
    as Half-Cheetah. As an aside, some of these benchmarks resemble challenges that
    children and adults use to play and learn new skills.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将现在描述在深度模型驱动的强化学习中常用的基准测试。我们将讨论五组基准：（1）谜题和迷宫，（2）如Pac-Man的Atari街机游戏，（3）如围棋和象棋的棋盘游戏，（4）如StarCraft的实时战略游戏，以及（5）如Half-Cheetah的模拟机器人任务。顺便提一下，其中一些基准类似于儿童和成人用来玩耍和学习新技能的挑战。
- en: We will have a closer look at these sets of benchmarks, some with discrete,
    some with continuous action spaces.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地查看这些基准集，其中一些具有离散的，有些具有连续的动作空间。
- en: '![Refer to caption](img/0e365e1630e9ef6bf090a063f6a0d183.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0e365e1630e9ef6bf090a063f6a0d183.png)'
- en: 'Figure 12: Sokoban Puzzle (Chao, [2013](#bib.bib25))'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：Sokoban谜题（Chao，[2013](#bib.bib25)）
- en: 4.1 Mazes
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 迷宫
- en: Trajectory planning algorithms are crucial in robotics (Latombe, [2012](#bib.bib105);
    Gasparetto et al., [2015](#bib.bib48)). There is a long tradition of using 2D
    and 3D path-finding problems in reinforcement learning and AI. The Taxi domain
    was introduced by (Dietterich, [2000](#bib.bib34)) in the context of hierarchical
    problem solving, and box-pushing problems such as Sokoban have been used frequently (Junghanns
    and Schaeffer, [2001](#bib.bib82); Dor and Zwick, [1999](#bib.bib37); Murase et al.,
    [1996](#bib.bib122); Zhou and Dovier, [2013](#bib.bib184)), see Figure [12](#S4.F12
    "Figure 12 ‣ 4 Benchmarks ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey"). The action space of these puzzles and mazes is discrete.
    The related problems are typically NP-hard or PSPACE-hard (Culberson, [1997](#bib.bib30);
    Hearn and Demaine, [2009](#bib.bib66)) and solving them requires basic path and
    motion planning skills.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹规划算法在机器人技术中至关重要（Latombe, [2012](#bib.bib105)；Gasparetto 等, [2015](#bib.bib48)）。在强化学习和人工智能中，使用
    2D 和 3D 路径寻找问题有着悠久的传统。出租车领域由 (Dietterich, [2000](#bib.bib34)) 引入，作为层次性问题解决的背景，而像
    Sokoban 这样的推箱子问题被频繁使用（Junghanns 和 Schaeffer, [2001](#bib.bib82)；Dor 和 Zwick, [1999](#bib.bib37)；Murase
    等, [1996](#bib.bib122)；Zhou 和 Dovier, [2013](#bib.bib184)），见图 [12](#S4.F12 "Figure
    12 ‣ 4 Benchmarks ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey")。这些谜题和迷宫的动作空间是离散的。相关问题通常是 NP-hard 或 PSPACE-hard（Culberson,
    [1997](#bib.bib30)；Hearn 和 Demaine, [2009](#bib.bib66)），解决这些问题需要基本的路径和运动规划技能。
- en: Small versions of the mazes can be solved exactly by planning, larger instances
    are only suitable for approximate planning or learning methods.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 小型迷宫可以通过规划精确解决，而较大的实例仅适合于近似规划或学习方法。
- en: Mazes can be used to test algorithms for “flat” reinforcement learning path
    finding problems (Tamar et al., [2016](#bib.bib161); Nardelli et al., [2018](#bib.bib124);
    Silver et al., [2017b](#bib.bib154)). Grids and box-pushing games such as Sokoban
    can also feature rooms or subgoals, that may then be used to test algorithms for
    hierarchically structured problems (Farquhar et al., [2018](#bib.bib40); Guez
    et al., [2019](#bib.bib54); Weber et al., [2017](#bib.bib177); Feng et al., [2020](#bib.bib43)).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 迷宫可以用于测试“平面”强化学习路径寻找问题的算法（Tamar 等, [2016](#bib.bib161)；Nardelli 等, [2018](#bib.bib124)；Silver
    等, [2017b](#bib.bib154)）。网格和推箱子游戏如 Sokoban 也可以包含房间或子目标，这些可以用于测试层次结构问题的算法（Farquhar
    等, [2018](#bib.bib40)；Guez 等, [2019](#bib.bib54)；Weber 等, [2017](#bib.bib177)；Feng
    等, [2020](#bib.bib43)）。
- en: The problems can be made more difficult by enlarging the grid and by inserting
    more obstacles. Mazes and Sokoban grids are sometimes procedurally generated (Shaker
    et al., [2016](#bib.bib149); Hendrikx et al., [2013](#bib.bib69); Togelius et al.,
    [2013](#bib.bib168)). The goal for the algorithms is typically to find a solution
    for a grid of a certain difficulty class, to find a shortest solution, or to learn
    to solve a class of grids by training on a different class of grids, to test transfer
    learning.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过扩大网格和增加更多障碍来使问题变得更困难。迷宫和 Sokoban 网格有时是程序生成的（Shaker 等, [2016](#bib.bib149)；Hendrikx
    等, [2013](#bib.bib69)；Togelius 等, [2013](#bib.bib168)）。算法的目标通常是为某个难度等级的网格找到解决方案，找到最短解决方案，或者通过在不同类别的网格上训练来学习解决某一类网格，以测试迁移学习。
- en: 4.2 Board Games
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 棋盘游戏
- en: Another classic group of benchmarks for planning and learning algorithms is
    board games.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个经典的规划和学习算法基准是棋盘游戏。
- en: Two-person zero-sum perfect information board games such as tic tac toe, chess,
    checkers, Go, and shogi have been used since the 1950s as benchmarks in AI. The
    action space of these games is discrete. Notable achievements were in checkers,
    chess, and Go, where human world champions were defeated in 1994, 1997, and 2016,
    respectively (Schaeffer et al., [1996](#bib.bib141); Campbell et al., [2002](#bib.bib24);
    Silver et al., [2016](#bib.bib152)). Other games are used as well as benchmarks,
    such as Poker (Brown and Sandholm, [2018](#bib.bib19)) and Diplomacy (Anthony
    et al., [2020](#bib.bib5)).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 两人零和完全信息棋盘游戏，如井字棋、国际象棋、跳棋、围棋和将棋，自1950年代以来一直作为人工智能的基准。 这些游戏的动作空间是离散的。值得注意的成就包括在跳棋、国际象棋和围棋中，人类世界冠军分别在
    1994、1997 和 2016 年被击败（Schaeffer 等, [1996](#bib.bib141)；Campbell 等, [2002](#bib.bib24)；Silver
    等, [2016](#bib.bib152)）。其他游戏也被用作基准，例如扑克（Brown 和 Sandholm, [2018](#bib.bib19)）和外交（Anthony
    等, [2020](#bib.bib5)）。
- en: The board games are typically used “as is” and are not changed for different
    experiments (as with mazes). They are fixed benchmarks, challenging and inspirational
    games where the goal is often beating human world champions. In model-based deep
    reinforcement learning they are used for self-play methods (Tesauro, [1995a](#bib.bib164);
    Anthony et al., [2017](#bib.bib3); Schrittwieser et al., [2019](#bib.bib146)).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 棋盘游戏通常“按原样”使用，并不会为不同的实验进行更改（与迷宫不同）。这些是固定的基准测试，具有挑战性和启发性，目标通常是击败人类世界冠军。在基于模型的深度强化学习中，它们用于自我对弈方法（Tesauro,
    [1995a](#bib.bib164); Anthony et al., [2017](#bib.bib3); Schrittwieser et al.,
    [2019](#bib.bib146)）。
- en: Board games have been traditional mainstays of artificial intelligence, mostly
    associated with the symbolic reasoning approach to AI. In contrast, the next benchmark
    is associated with connectionist AI.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 棋盘游戏一直是人工智能的传统主力，大多与符号推理方法关联。相比之下，下一个基准与联结主义人工智能相关。
- en: 4.3 Atari
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 Atari
- en: Shortly after 2010 the Atari Learning Environment (ALE) (Bellemare et al., [2013](#bib.bib11))
    was introduced for the sole purpose of evaluating reinforcement learning algorithms
    on high-dimensional input, to see if end-to-end pixel-to-joystick learning would
    be possible. ALE has been used widely in the field of reinforcement learning,
    after impressive results such as (Mnih et al., [2015](#bib.bib115)). ALE runs
    on top of an emulator for the classic 1980s Atari gaming console, and features
    more than 50 arcade games such as Pong, Breakout, Space Invaders, Pac-Man, and
    Montezuma’s Revenge. ALE is well suited for benchmarking perception and eye-hand-coordination
    type skills, less so for planning. ALE is mostly used for deep reinforcement learning
    algorithms in sensing and recognition tasks.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 2010年后不久，引入了Atari学习环境（ALE）（Bellemare et al., [2013](#bib.bib11)），其唯一目的是评估高维输入下的强化学习算法，以查看是否可以实现从端到端的像素到操纵杆的学习。ALE在强化学习领域被广泛使用，取得了一些令人印象深刻的成果，例如（Mnih
    et al., [2015](#bib.bib115)）。ALE运行在经典1980年代Atari游戏机的模拟器之上，具有超过50款街机游戏，如Pong、Breakout、Space
    Invaders、Pac-Man和Montezuma’s Revenge。ALE非常适合用于基准测试感知和眼手协调类型的技能，但不太适合用于规划。ALE主要用于深度强化学习算法中的感知和识别任务。
- en: ALE is a popular benchmark that has been used in many model-based papers (Kaiser
    et al., [2019](#bib.bib86); Oh et al., [2015](#bib.bib127), [2017](#bib.bib128);
    Ha and Schmidhuber, [2018b](#bib.bib57); Schrittwieser et al., [2019](#bib.bib146)),
    and many model-free reinforcement learning methods. As with mazes, the action
    space is discrete and low-dimensional—9 joystick directions and a push-button—although
    the input space is high-dimensional.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ALE是一个流行的基准，已被许多基于模型的论文使用（Kaiser et al., [2019](#bib.bib86); Oh et al., [2015](#bib.bib127),
    [2017](#bib.bib128); Ha and Schmidhuber, [2018b](#bib.bib57); Schrittwieser et
    al., [2019](#bib.bib146)），以及许多无模型强化学习方法。与迷宫一样，动作空间是离散和低维的——9个操纵杆方向和一个按钮——尽管输入空间是高维的。
- en: The ALE games are quite varied in nature. There are “easy” eye-hand-coordination
    tasks such as Pong and Breakout, and there are more strategic level games where
    long periods of movement exist without changes in score, such as Pitfall and Montezuma’s
    Revenge. The goal of ALE experiments is typically to achieve a score level comparable
    to humans in as many of the 57 games as possible (which has recently been achieved (Badia
    et al., [2020](#bib.bib10))). After this achievement, some researchers believe
    that the field is ready for more challenging benchmarks (Machado et al., [2018](#bib.bib112)).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ALE游戏的性质非常多样。有“简单”的眼手协调任务，如Pong和Breakout，也有更多战略级别的游戏，其中长时间的移动没有得分变化，如Pitfall和Montezuma’s
    Revenge。ALE实验的目标通常是在尽可能多的57款游戏中达到与人类相当的分数水平（最近已实现（Badia et al., [2020](#bib.bib10)））。在这一成就之后，一些研究人员认为该领域已经准备好迎接更具挑战性的基准（Machado
    et al., [2018](#bib.bib112)）。
- en: 4.4 Real-Time Strategy and Video Games
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 实时战略与视频游戏
- en: The Atari benchmarks are based on simple arcade games of 35–40 years ago, most
    of which are mostly challenging for eye-hand-coordination skills. Real-time strategy
    (RTS) games and games such as StarCraft, DOTA, and Capture the Flag provide more
    challenging tasks. The strategy space is large; the state space of StarCraft has
    been estimated at $10^{1685}$, much larger than chess ($10^{47}$) or go ($10^{147}$).
    Most RTS games are multi-player, non-zero-sum, imperfect information games that
    also feature high-dimensional pixel input, reasoning, team collaboration, as well
    as eye-hand-coordination. The action space is stochastic and is a mix of discrete
    and continuous actions. A very high degree of diversity is necessary to prevent
    specialization.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Atari 基准测试基于 35-40 年前的简单街机游戏，这些游戏大多数对眼手协调技能具有挑战性。实时策略（RTS）游戏以及《星际争霸》、《DOTA》和《夺旗》等游戏提供了更具挑战性的任务。策略空间非常大；《星际争霸》的状态空间被估计为
    $10^{1685}$，远大于国际象棋（$10^{47}$）或围棋（$10^{147}$）。大多数 RTS 游戏都是多人、多方博弈、不完全信息游戏，还具有高维度的像素输入、推理、团队协作以及眼手协调。行动空间是随机的，是离散和连续行动的混合。为了防止专业化，需要非常高程度的多样性。
- en: Despite the challenging nature, impressive achievements have been reported recently
    in all three mentioned games where human performance was matched or even exceeded (Vinyals
    et al., [2019](#bib.bib171); Berner et al., [2019](#bib.bib15); Jaderberg et al.,
    [2019](#bib.bib79)). In these efforts deep model-based reinforcement learning
    is combined with multi-agent and population based methods. These mixed approaches
    achieve impressive results on RTS games; added diversity diminishes the specialization
    trap of two-agent approaches (Vinyals et al., [2019](#bib.bib171)), their approaches
    may combine aspects of self-play and latent models, although often the well-tuned
    combination of a number of methods is credited with the high achievements in these
    games. The mixed approaches are not listed separately in the taxonomy of the next
    section.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管具有挑战性，但最近在所有提到的三款游戏中都有显著的成就报告，其中人的表现已被匹配甚至超越（Vinyals et al., [2019](#bib.bib171);
    Berner et al., [2019](#bib.bib15); Jaderberg et al., [2019](#bib.bib79)）。这些努力结合了深度基于模型的强化学习与多智能体和基于群体的方法。这些混合方法在
    RTS 游戏中取得了显著的成果；增加的多样性减少了双智能体方法的专业化陷阱（Vinyals et al., [2019](#bib.bib171)），他们的方法可能结合了自我博弈和潜在模型的方面，尽管通常是多个方法的良好调配被认为是这些游戏中取得高成就的原因。混合方法在下一节的分类中没有单独列出。
- en: '![Refer to caption](img/bd613fe521daf6b0136b070ef2c315c5.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bd613fe521daf6b0136b070ef2c315c5.png)'
- en: 'Figure 13: Half-Cheetah'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：半人马
- en: 4.5 Robotics
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 机器人技术
- en: Reinforcement learning is a paradigm that is well-suited for modeling planning
    and control problems in robotics. Instead of minutely programming high-dimensional
    robot-movements step-by-step, reinforcement learning is used to train behavior
    more flexibly, and possibly end-to-end from camera input to arm manipulation.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种非常适合于建模机器人规划和控制问题的范式。与其逐步编程高维度的机器人运动，不如使用强化学习更灵活地训练行为，可能实现从相机输入到手臂操作的端到端训练。
- en: Training with real-world robots is expensive and complicated. In robotics sample
    efficiency is of great importance because of the cost of interaction with real-world
    environments and the wear of physical robot arms. For this reason virtual environments
    have been devised. Todorov et al. (Todorov et al., [2012](#bib.bib167)) introduced
    MuJoCo, a software suite for simulated robot behavior. It is used extensively
    in reinforcement learning research. Well-known benchmark tasks are Reacher, Swimmer,
    Half-Cheetah, and Ant, in which the agent’s task is to teach itself the appropriate
    movement actions, see Figure [13](#S4.F13 "Figure 13 ‣ 4.4 Real-Time Strategy
    and Video Games ‣ 4 Benchmarks ‣ Model-Based Deep Reinforcement Learning for High-Dimensional
    Problems, a Survey") for an example. Many model-based deep reinforcement learning
    methods are tested on MuJoCo (Heess et al., [2015](#bib.bib67); Chua et al., [2018](#bib.bib27);
    Janner et al., [2019](#bib.bib80); Gu et al., [2016](#bib.bib52); Feinberg et al.,
    [2018](#bib.bib42); Clavera et al., [2018](#bib.bib28); Azizzadenesheli et al.,
    [2018](#bib.bib7); Hafner et al., [2018](#bib.bib59)) and other robotics tasks (Tassa
    et al., [2012](#bib.bib162); Levine and Abbeel, [2014](#bib.bib108); Finn and
    Levine, [2017](#bib.bib44); Hafner et al., [2019](#bib.bib60); Sekar et al., [2020](#bib.bib148)).
    The action space of these tasks is continuous, and the emphasis in experiments
    is on sample efficiency.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 使用真实世界的机器人进行训练既昂贵又复杂。在机器人学中，样本效率非常重要，因为与真实环境的交互成本以及物理机器人手臂的磨损。因此，虚拟环境应运而生。Todorov
    et al. (Todorov et al., [2012](#bib.bib167)) 引入了 MuJoCo，这是一个用于模拟机器人行为的软件套件。它在强化学习研究中被广泛使用。著名的基准任务包括
    Reacher、Swimmer、Half-Cheetah 和 Ant，在这些任务中，代理的任务是教会自己适当的运动动作，参见图 [13](#S4.F13 "Figure
    13 ‣ 4.4 Real-Time Strategy and Video Games ‣ 4 Benchmarks ‣ Model-Based Deep
    Reinforcement Learning for High-Dimensional Problems, a Survey") 以获取示例。许多模型基础的深度强化学习方法在
    MuJoCo 上进行测试 (Heess et al., [2015](#bib.bib67); Chua et al., [2018](#bib.bib27);
    Janner et al., [2019](#bib.bib80); Gu et al., [2016](#bib.bib52); Feinberg et
    al., [2018](#bib.bib42); Clavera et al., [2018](#bib.bib28); Azizzadenesheli et
    al., [2018](#bib.bib7); Hafner et al., [2018](#bib.bib59)) 和其他机器人任务 (Tassa et
    al., [2012](#bib.bib162); Levine and Abbeel, [2014](#bib.bib108); Finn and Levine,
    [2017](#bib.bib44); Hafner et al., [2019](#bib.bib60); Sekar et al., [2020](#bib.bib148))。这些任务的动作空间是连续的，实验的重点是样本效率。
- en: Conclusion
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结论
- en: No discussion on empirical deep reinforcement learning is complete without the
    mention of OpenAI Gym (Brockman et al., [2016](#bib.bib18)). Gym is a toolkit
    for developing and comparing reinforcement learning algorithms and provides a
    training environment for reinforcement learning agents. Gym includes interfaces
    to benchmark sets such as ALE and MuJoCo. Other software suites are (Tassa et al.,
    [2018](#bib.bib163); Vinyals et al., [2017](#bib.bib170); Yu et al., [2020](#bib.bib182);
    Bellemare et al., [2013](#bib.bib11); Todorov et al., [2012](#bib.bib167)).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 任何关于经验深度强化学习的讨论都离不开对 OpenAI Gym (Brockman et al., [2016](#bib.bib18)) 的提及。Gym
    是一个用于开发和比较强化学习算法的工具包，并为强化学习代理提供了一个训练环境。Gym 包括对基准集如 ALE 和 MuJoCo 的接口。其他软件套件有 (Tassa
    et al., [2018](#bib.bib163); Vinyals et al., [2017](#bib.bib170); Yu et al., [2020](#bib.bib182);
    Bellemare et al., [2013](#bib.bib11); Todorov et al., [2012](#bib.bib167))。
- en: Baseline implementations of many deep reinforcement learning agent algorithms
    can also be found at the Gym website https://gym.openai.com.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度强化学习代理算法的基线实现也可以在 Gym 网站 https://gym.openai.com 上找到。
- en: Research into suitable benchmarks is active. Further interesting approaches
    are Procedural Content Generation (Togelius et al., [2013](#bib.bib168)), MuJoCo
    Soccer (Liu et al., [2019](#bib.bib111)), and the Obstacle Tower Challenge (Juliani
    et al., [2019](#bib.bib81)).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对于合适的基准的研究仍在进行中。其他有趣的方法包括程序化内容生成 (Togelius et al., [2013](#bib.bib168))、MuJoCo
    足球 (Liu et al., [2019](#bib.bib111)) 和障碍塔挑战 (Juliani et al., [2019](#bib.bib81))。
- en: Now that we have seen the benchmarks on which our model-based methods are tested,
    it is time for an in-depth discussion and outlook for future work.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到模型基础方法所测试的基准，是时候进行深入讨论并展望未来的工作了。
- en: 5 Discussion and Outlook
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论与展望
- en: Model-based reinforcement learning promises lower sample complexity. Sutton’s
    work on imagination, where a model is created with environment samples that are
    then used to create extra imagined samples for free, clearly suggests this aspect
    of model-based reinforcement learning. The transition model acts as a multiplier
    on the amount of information that is used from each environment sample.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的强化学习承诺降低样本复杂性。萨顿关于想象的工作中，创建了一个使用环境样本构建的模型，然后用来生成额外的免费想象样本，这明显暗示了基于模型的强化学习的这一方面。过渡模型作为从每个环境样本中使用的信息量的乘数。
- en: Another, and perhaps more important aspect, is generalization performance. Model-based
    reinforcement learning builds a dynamics model of the domain. This model can be
    used multiple times, for new problem instances, but also for new problem classes.
    By learning the transition and reward model, model-based reinforcement learning
    may be better at capturing the essence of a domain than model-free methods. Model-based
    reinforcement learning may thus be better suited for solving transfer learning
    problems, and for solving long sequential decision making problems, a class of
    problems that is important in real world decision making.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个也许更重要的方面是泛化性能。基于模型的强化学习构建了一个领域的动态模型。这个模型可以多次使用，用于新的问题实例，也用于新的问题类别。通过学习过渡和奖励模型，基于模型的强化学习可能比无模型方法更能捕捉领域的本质。因此，基于模型的强化学习可能更适合解决迁移学习问题，以及解决长序列决策问题，这类问题在现实世界决策中非常重要。
- en: Classical table based approaches and Gaussian Process approaches have been quite
    succesful in achieving low sample complexity for problems of moderate complexity (Sutton
    and Barto, [2018](#bib.bib159); Deisenroth et al., [2013b](#bib.bib33); Kober
    et al., [2013](#bib.bib96)). However, the topic of the current survey is *deep*
    models, for high dimensional problems with complex, non-linear, and discontinuous
    functions. These application domains pose a problem for classical model-based
    approaches. Since high-capacity deep neural networks require many samples to achieve
    high generalization (without overfitting), a challenge in deep model-based reinforcement
    learning is to maintain low sample complexity.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的基于表格的方法和高斯过程方法在解决中等复杂度的问题时已经取得了相当大的成功（Sutton 和 Barto，[2018](#bib.bib159)；Deisenroth
    等，[2013b](#bib.bib33)；Kober 等，[2013](#bib.bib96)）。然而，本次调查的主题是*深度*模型，针对具有复杂、非线性和不连续函数的高维问题。这些应用领域对经典的基于模型的方法提出了挑战。由于高容量深度神经网络需要大量样本来实现高泛化（而不发生过拟合），因此在深度基于模型的强化学习中的一个挑战是保持低样本复杂性。
- en: We have seen a wide range of approaches that attempt this goal. Let us now discuss
    and summarize the benchmarks, the approaches for deep models, and possible future
    work.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到许多尝试实现这一目标的方法。现在让我们讨论和总结基准测试、深度模型的方法以及可能的未来工作。
- en: 5.1 Benchmarking
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基准测试
- en: Benchmarks are the lifeblood of AI. We must test our algorithms to know if they
    exhibit intelligent behavior. Many of the benchmarks allow difficult decision
    making situations to be modeled. Two-person games allow modeling of competition.
    In real world decision making, collaboration and negotiation are also important.
    Real-time strategy games allow collaboration, competition and negotiation to be
    modelled, and multi-agent and hierarchical algorithms are being developed for
    these decision making situations (Jaderberg et al., [2019](#bib.bib79); Kulkarni
    et al., [2016](#bib.bib101); Makar et al., [2001](#bib.bib113)).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试是 AI 的命脉。我们必须测试我们的算法，以了解它们是否展现出智能行为。许多基准测试允许建模困难的决策情境。双人游戏允许建模竞争。在现实世界决策中，协作和谈判也很重要。实时战略游戏允许建模协作、竞争和谈判，并且正在为这些决策情境开发多智能体和层次化算法（Jaderberg
    等，[2019](#bib.bib79)；Kulkarni 等，[2016](#bib.bib101)；Makar 等，[2001](#bib.bib113)）。
- en: Unfortunately, the wealth of choice in benchmarks makes it difficult to compare
    results that are reported in the literature. Not all authors publish their code.
    We typically need to rerun experiments with identical benchmarks to compare algorithms
    conclusively. Outcomes often differ from the original works, also because not
    all hyperparameter settings are always clear and implementation details may differ (Henderson
    et al., [2017](#bib.bib68); Islam et al., [2017](#bib.bib77); Wang et al., [2019b](#bib.bib175)).
    Authors should publish their code if our field wants to make progress. The recent
    attention for reproducibility in deep reinforcement learning is helping the field
    move in the right direction. Many papers now publish their code and the hyperparameter
    settings that were used in the reported experiments.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，基准选择的丰富性使得比较文献中报告的结果变得困难。并非所有作者都会发布他们的代码。我们通常需要使用相同的基准重新运行实验，以便明确地比较算法。结果通常与原始工作不同，也因为并非所有超参数设置总是明确，实施细节可能有所不同（Henderson
    et al., [2017](#bib.bib68); Islam et al., [2017](#bib.bib77); Wang et al., [2019b](#bib.bib175)）。如果我们的领域想要取得进展，作者应该发布他们的代码。最近对深度强化学习中可重复性的关注正在帮助该领域朝着正确的方向发展。许多论文现在发布了他们的代码以及在报告的实验中使用的超参数设置。
- en: 5.2 Curriculum Learning and Latent Models
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 课程学习和潜在模型
- en: Model-based reinforcement learning works well when the transition and reward
    models are given by the rules of the problem. We have seen how perfect models
    in games such as chess and Go allow deep and accurate planning. Systems were constructed (Tesauro,
    [1995a](#bib.bib164); Silver et al., [2018](#bib.bib155); Anthony et al., [2017](#bib.bib3))
    where curriculum learning facilitated tabula rasa self-learning of highly complex
    games of strategy; see also (Bengio et al., [2009](#bib.bib14); Plaat, [2020](#bib.bib132);
    Narvekar et al., [2020](#bib.bib125)). The success of self-play has led to interest
    in curriculum learning in single-agent problems (Feng et al., [2020](#bib.bib43);
    Doan et al., [2019](#bib.bib35); Duan et al., [2016](#bib.bib38); Laterre et al.,
    [2018](#bib.bib104)).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的强化学习在过渡和奖励模型由问题规则给出时效果很好。我们已经看到，在象棋和围棋等游戏中，完美的模型允许深度和准确的规划。系统被构建（Tesauro,
    [1995a](#bib.bib164); Silver et al., [2018](#bib.bib155); Anthony et al., [2017](#bib.bib3)），在这些系统中，课程学习促进了复杂策略游戏的从零开始的自我学习；另见（Bengio
    et al., [2009](#bib.bib14); Plaat, [2020](#bib.bib132); Narvekar et al., [2020](#bib.bib125)）。自我博弈的成功引发了对单一代理问题中课程学习的兴趣（Feng
    et al., [2020](#bib.bib43); Doan et al., [2019](#bib.bib35); Duan et al., [2016](#bib.bib38);
    Laterre et al., [2018](#bib.bib104)）。
- en: When the rules are not given, they might be learned, to create a transition
    model. Unfortunately, the planning accuracy of learned models is less than perfect.
    We have seen efforts with Gaussian Processes and ensembles to improve model quality,
    and efforts with local planning and Model-Predictive Control, to limit the damage
    of imperfections in the transition model. We have also discussed, at length, latent,
    abstract, models, where for each of the functions of the Markov Decision Process
    a separate sub-module is learned. Latent models achieve better accuracy with explicit
    planning, as planning occurs in latent space (Oh et al., [2017](#bib.bib128);
    Hafner et al., [2019](#bib.bib60); Kaiser et al., [2019](#bib.bib86)).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当规则未被给出时，它们可能会被学习，以创建一个过渡模型。不幸的是，学习模型的规划准确性不如完美。我们已经看到，使用高斯过程和集成方法来提高模型质量的努力，以及使用局部规划和模型预测控制来限制过渡模型中缺陷的损害。我们还详细讨论了潜在的抽象模型，其中针对马尔可夫决策过程的每个功能学习一个单独的子模块。潜在模型通过显式规划实现更好的准确性，因为规划发生在潜在空间中（Oh
    et al., [2017](#bib.bib128); Hafner et al., [2019](#bib.bib60); Kaiser et al.,
    [2019](#bib.bib86)）。
- en: The work on Value Iteration Networks (Tamar et al., [2016](#bib.bib161)) inspired
    end-to-end learning, where both the transition model and the planning algorithm
    are learned, end-to-end. Combined with latent models (or World Models (Ha and
    Schmidhuber, [2018b](#bib.bib57))) impressive results where achieved (Silver et al.,
    [2017b](#bib.bib154)), and model/planning accuracy was improved to the extent
    that tabula rasa curriculum self-learning was achieved, in Muzero (Schrittwieser
    et al., [2019](#bib.bib146)) for both chess, Go, and Atari games. End-to-end learning
    and latent models together allowed the circle to be closed, achieving curriculum
    learning self-play also for problems where the rules are not given. See Figure [14](#S5.F14
    "Figure 14 ‣ 5.2 Curriculum Learning and Latent Models ‣ 5 Discussion and Outlook
    ‣ Model-Based Deep Reinforcement Learning for High-Dimensional Problems, a Survey")
    for relations between the different approaches of this survey. The main categories
    are color-coded, latent methods are dashed.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代网络（Tamar 等， [2016](#bib.bib161)）的研究启发了端到端学习，其中过渡模型和规划算法都是端到端学习的。结合潜在模型（或世界模型（Ha
    和 Schmidhuber，[2018b](#bib.bib57)）），取得了令人印象深刻的结果（Silver 等，[2017b](#bib.bib154)），并且模型/规划的准确性提高到了一种程度，以至于在
    Muzero（Schrittwieser 等，[2019](#bib.bib146)）中实现了从零开始的课程自学，涵盖了国际象棋、围棋和 Atari 游戏。端到端学习和潜在模型的结合使得循环得以完成，实现了课程学习自我游戏，即使在规则不明确的问题上也是如此。请参见图
    [14](#S5.F14 "Figure 14 ‣ 5.2 Curriculum Learning and Latent Models ‣ 5 Discussion
    and Outlook ‣ Model-Based Deep Reinforcement Learning for High-Dimensional Problems,
    a Survey") 了解本调查中不同方法之间的关系。主要类别用颜色编码，潜在方法用虚线表示。
- en: <svg   height="354.84" overflow="visible" version="1.1" width="344.22"><g transform="translate(0,354.84)
    matrix(1 0 0 -1 0 0) translate(177.44,0) translate(0,185.78)" fill="#000000" stroke="#000000"><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -60.13 -3.78)"><foreignobject width="41.51" height="7.56" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">MBDRL</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -77.43 31.11)"><foreignobject
    width="58.42" height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">given
    model</foreignobject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 -101.97 63.15)"><foreignobject width="57.65"
    height="9.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TDgammon</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -127.87 101.59)"><foreignobject width="21.68" height="7.56" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">ExIt</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -133.93 152.03)"><foreignobject
    width="51.07" height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Single
    Cur</foreignobject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 -74.96 111.41)"><foreignobject width="43.43"
    height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">AlphaGo</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -25.38 154.71)"><foreignobject width="50.77" height="9.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">AlphaZero</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 18.99 35.13)"><foreignobject
    width="39.97" height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">end2end</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt"
    stroke-dashoffset="0.0pt" transform="matrix(1.0 0.0 0.0 1.0 110.09 110.04)"><foreignobject
    width="51.26" height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Predictron</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -6.32 107.11)"><foreignobject width="31.86" height="9.72" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">VProp</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"
    transform="matrix(1.0 0.0 0.0 1.0 46 109.81)"><foreignobject width="38.16" height="9.72"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">TreeQN</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt"
    stroke-dashoffset="0.0pt" transform="matrix(1.0 0.0 0.0 1.0 107.4 16.73)"><foreignobject
    width="17.83" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I2A</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt"
    stroke-dashoffset="0.0pt" transform="matrix(1.0 0.0 0.0 1.0 115.2 156.06)"><foreignobject
    width="37.85" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MuZero</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt"
    stroke-dashoffset="0.0pt" transform="matrix(1.0 0.0 0.0 1.0 34.76 -15.47)"><foreignobject
    width="62.45" height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">World
    model</foreignobject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 -14.5 73.76)"><foreignobject width="56.42" height="7.56"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">ConvLSTM</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 91.99 55.27)"><foreignobject width="20.6" height="7.56" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">VIN</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -53.72 -37.94)"><foreignobject
    width="67.99" height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">learned
    model</foreignobject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 -126.73 -70.51)"><foreignobject width="59.96"
    height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">explicit
    plan</foreignobject></g><g stroke-width="1.2pt" fill="#000000" stroke="#000000"
    transform="matrix(1.0 0.0 0.0 1.0 -114.46 -20.87)"><foreignobject width="35.05"
    height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PILCO</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -159.98 -33.81)"><foreignobject width="27.29" height="9.72" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">iLQG</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -104.82 -115.37)"><foreignobject
    width="22.37" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">GPS</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -167.08 -104.39)"><foreignobject width="44.22" height="9.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Foresight</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -89.01 -149.78)"><foreignobject
    width="22.83" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SVG</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -136.08 -148.54)"><foreignobject width="29.21" height="7.56" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">PETS</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"
    transform="matrix(1.0 0.0 0.0 1.0 -32.28 -180.33)"><foreignobject width="33.67"
    height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">PlaNet</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt"
    stroke-dashoffset="0.0pt" transform="matrix(1.0 0.0 0.0 1.0 -102.39 -178.62)"><foreignobject
    width="41.73" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Dreamer</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt"
    stroke-dashoffset="0.0pt" transform="matrix(1.0 0.0 0.0 1.0 -171.99 -177.69)"><foreignobject
    width="50.43" height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Plan2Expl</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 74.6 -74.88)"><foreignobject width="57.19" height="9.55" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">imagination</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"
    transform="matrix(1.0 0.0 0.0 1.0 132.18 -175.69)"><foreignobject width="24.14"
    height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">VPN</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -40.11 -77.84)"><foreignobject width="26.29" height="7.69" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Local</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -53.39 -114.77)"><foreignobject
    width="25.98" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">MVE</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -30.48 -136.63)"><foreignobject width="61.04" height="9.84" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">PolicyOptim</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 19.18 -101.14)"><foreignobject
    width="24.91" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Meta</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 41.58 -125.1)"><foreignobject width="31.13" height="7.56" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">GATS</foreignobject></g><g stroke-width="1.2pt"
    fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"
    transform="matrix(1.0 0.0 0.0 1.0 44.51 -160.75)"><foreignobject width="37.82"
    height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">SimPLe</foreignobject></g><g
    stroke-width="1.2pt" fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt"
    stroke-dashoffset="0.0pt" transform="matrix(1.0 0.0 0.0 1.0 85.89 -136.54)"><foreignobject
    width="50.92" height="7.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">VideoPred</foreignobject></g></g></svg>
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14: Influence of Model-Based Deep Reinforcement Learning Approaches.
    Green: given transitions/explicit planning; Red: learned transitions/explicit
    planning; Yellow: end-to-end transitions and planning. Dashed: latent models.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：基于模型的深度强化学习方法的影响。绿色：给定过渡/显式规划；红色：学习过渡/显式规划；黄色：端到端过渡和规划。虚线：潜在模型。
- en: Optimizing directly in a latent space has been successful with a generative
    adversarial network (Volz et al., [2018](#bib.bib172)). World Models are linked
    to neuroevolution by (Risi and Stanley, [2019](#bib.bib137)). For future work,
    the combination of curriculum learning, ensembles, and latent models appears quite
    fruitful. Self-play has been used to achieve further success in other challenging
    games, such as StarCraft (Vinyals et al., [2019](#bib.bib171)), DOTA (Berner et al.,
    [2019](#bib.bib15)), Capture the Flag (Jaderberg et al., [2019](#bib.bib79)),
    and Poker (Brown and Sandholm, [2019](#bib.bib20)). In multi-agent real-time strategy
    games the aspect of collaboration and teams is important, and self-play model-based
    reinforcement learning has been combined with multi-agent, hierarchical, and population
    based methods.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在潜在空间中直接优化已经在生成对抗网络中取得成功 （Volz 等, [2018](#bib.bib172)）。World Models 通过 （Risi
    和 Stanley, [2019](#bib.bib137)）与神经进化相关联。对于未来的工作，课程学习、集成和潜在模型的结合似乎非常有前景。自我对弈已被用于在其他挑战性游戏中取得进一步成功，如《星际争霸》 （Vinyals
    等, [2019](#bib.bib171)）、《DOTA》 （Berner 等, [2019](#bib.bib15)）、《夺旗战》 （Jaderberg
    等, [2019](#bib.bib79)）和《扑克》 （Brown 和 Sandholm, [2019](#bib.bib20)）。在多智能体实时策略游戏中，协作和团队的方面很重要，自我对弈的基于模型的强化学习与多智能体、层次化和基于群体的方法相结合。
- en: In future work, more research is needed to explore the potential of (end-to-end)
    planning with latent representational models more fully for larger problems, for
    transfer learning, and for cooperative problems. More research is needed in uncertainty-aware
    neural networks. For such challenging problems as real-time strategy and other
    video games, more combinations of deep model-based reinforcement learning with
    multi-agent and population based methods can be expected (Vinyals et al., [2019](#bib.bib171);
    Risi and Preuss, [2020](#bib.bib136); Bäck, [1996](#bib.bib9)).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的工作中，需要更多研究以更全面地探索（端到端）规划与潜在表示模型在更大问题、迁移学习和协作问题上的潜力。对于不确定性意识的神经网络也需要更多的研究。对于实时策略和其他视频游戏等挑战性问题，可以预期更多的深度基于模型的强化学习与多智能体和基于群体的方法的组合 （Vinyals
    等, [2019](#bib.bib171)；Risi 和 Preuss, [2020](#bib.bib136)；Bäck, [1996](#bib.bib9)）。
- en: In end-to-end planning and learning, latent models introduce differentiable
    versions of more and more classical algorithms. For example, World Models (Ha
    and Schmidhuber, [2018b](#bib.bib57)) have a trio of Vision, Memory, Control models,
    reminding us of the Model, View, Controller design pattern (Gamma et al., [2009](#bib.bib47)),
    or even of classical computer architecture elements such as ALU, RAM, and Control
    Unit (Hennessy and Patterson, [2017](#bib.bib70)). Future work will show if differentiable
    algorithms will be found for even more classical algorithms.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在端到端规划和学习中，潜在模型引入了越来越多经典算法的可微版本。例如，World Models （Ha 和 Schmidhuber, [2018b](#bib.bib57)）拥有一个由视觉、记忆、控制模型组成的三重奏，让我们想起了模型-视图-控制器设计模式 （Gamma
    等, [2009](#bib.bib47)），甚至是经典计算机架构元素，如 ALU、RAM 和控制单元 （Hennessy 和 Patterson, [2017](#bib.bib70)）。未来的工作将显示是否会为更多经典算法找到可微算法。
- en: 6 Conclusion
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Deep learning has revolutionized reinforcement learning. The new methods allow
    us to approach more complicated problems than before. Control and decision making
    tasks involving high dimensional visual input come within reach.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经彻底改变了强化学习。这些新方法使我们能够处理比以往更复杂的问题。涉及高维视觉输入的控制和决策任务逐渐在掌握之中。
- en: In principle, model-based methods offer the advantage of lower sample complexity
    over model-free methods, because of their transition model. However, traditional
    methods, such as Gaussian Processes, that work well on moderately complex problems
    with few samples, do not perform well on high-dimensional problems. High-capacity
    models have high sample complexity, and finding methods that generalize well with
    low sample complexity has been difficult.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，基于模型的方法因其过渡模型而具有比无模型方法更低的样本复杂度的优势。然而，传统方法如高斯过程，在处理样本较少的中等复杂问题时效果良好，但在高维问题上表现不佳。高容量模型具有高样本复杂度，找到能以低样本复杂度良好泛化的方法一直很困难。
- en: In the last five years many new methods have been devised, and great success
    has been achieved in model-free and in model-based deep reinforcement learning.
    This survey has summarized the main ideas of recent papers in three approaches.
    For more and more aspects of model-based reinforcement learning algorithms differentiable
    methods appear. Latent models condense complex problems into compact latent representations
    that are easier to learn. End-to-end curriculum learning of latent planning and
    learning models has been achieved.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去五年中，许多新方法被提出，并在无模型和有模型的深度强化学习中取得了巨大成功。本调查总结了近期论文的三种主要方法的核心思想。越来越多的模型基强化学习算法的方面出现了可微分方法。潜在模型将复杂问题浓缩为更易于学习的紧凑潜在表示。已实现潜在规划和学习模型的端到端课程学习。
- en: In the discussion we mentioned open problems for each of the approaches, where
    we expect worthwhile future work will occur, such as curriculum learning, uncertainty
    modeling and transfer learning by latent models. Benchmarks are important to test
    progress, and more benchmarks of latent models can be expected. Curriculum learning
    has shown how complex problems can be learned relatively quickly from scratch,
    and latent models allow planning in efficient latent spaces. Impressive results
    have been reported; future work can be expected in transfer learning with latent
    models, and the interplay of curriculum learning with (generative) latent models,
    in combination with end-to-end learning of larger problems.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论中，我们提到了每种方法的开放问题，预计未来会有有价值的工作，如课程学习、不确定性建模和通过潜在模型的迁移学习。基准测试对于测试进展非常重要，未来可以期待更多的潜在模型基准测试。课程学习展示了如何从零开始相对快速地学习复杂问题，而潜在模型允许在高效的潜在空间中进行规划。已报告了令人印象深刻的结果；未来的工作可以预期在潜在模型的迁移学习方面，以及课程学习与（生成）潜在模型的相互作用，结合端到端学习更大问题的能力。
- en: Benchmarks in the field have also had to keep up. Benchmarks have progressed
    from single-agent grid worlds to multi-agent Real-time strategy games and complicated
    camera-arm robotics manipulation tasks. Reproducibility and benchmarking studies
    are of great importance for real progress. In real-time strategy games model-based
    methods are being combined with multi-agent, hierarchical and evolutionary approaches,
    allowing the study of collaboration, competation and negotiation.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的基准测试也不得不跟上进度。基准测试已经从单一代理的网格世界发展到多代理的实时战略游戏和复杂的相机-臂机器人操控任务。可重复性和基准测试研究对真正的进展至关重要。在实时战略游戏中，模型基的方法与多代理、分层和进化方法相结合，允许研究合作、竞争和谈判。
- en: Model-based deep reinforcement learning is a vibrant field of AI with a long
    history before deep learning. The field is blessed with a high degree of activity,
    an open culture, clear benchmarks, shared code-bases (Brockman et al., [2016](#bib.bib18);
    Vinyals et al., [2017](#bib.bib170); Tassa et al., [2018](#bib.bib163)) and a
    quick turnaround of ideas. We hope that this survey will lower the barrier of
    entry even further.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的深度强化学习是一个充满活力的人工智能领域，在深度学习之前已有悠久历史。该领域具有高度的活跃性、开放的文化、清晰的基准、共享的代码库（Brockman
    et al., [2016](#bib.bib18); Vinyals et al., [2017](#bib.bib170); Tassa et al.,
    [2018](#bib.bib163)）以及迅速的思想转化。我们希望这项调查能够进一步降低入门门槛。
- en: Acknowledgments
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank the members of the Leiden Reinforcement Learning Group, and especially
    Thomas Moerland and Mike Huisman, for many discussions and insights.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢莱顿强化学习小组的成员，特别是Thomas Moerland和Mike Huisman，感谢他们的诸多讨论和见解。
- en: References
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abbeel et al. [2007] Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y
    Ng. An application of reinforcement learning to aerobatic helicopter flight. In
    *Advances in Neural Information Processing Systems*, pages 1–8, 2007.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbeel et al. [2007] Pieter Abbeel, Adam Coates, Morgan Quigley, 和 Andrew Y
    Ng. 强化学习在特技直升机飞行中的应用。在 *神经信息处理系统进展*，第1–8页，2007年。
- en: Alpaydin [2020] Ethem Alpaydin. *Introduction to machine learning, Third edition*.
    MIT Press, 2020.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alpaydin [2020] Ethem Alpaydin. *机器学习导论，第三版*。麻省理工学院出版社，2020年。
- en: Anthony et al. [2017] Thomas Anthony, Zheng Tian, and David Barber. Thinking
    fast and slow with deep learning and tree search. In *Advances in Neural Information
    Processing Systems*, pages 5360–5370, 2017.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthony et al. [2017] Thomas Anthony, Zheng Tian, 和 David Barber. 使用深度学习和树搜索的快速与缓慢思考。在
    *神经信息处理系统进展*，第5360–5370页，2017年。
- en: 'Anthony et al. [2019] Thomas Anthony, Robert Nishihara, Philipp Moritz, Tim
    Salimans, and John Schulman. Policy gradient search: Online planning and expert
    iteration without search trees. *arXiv preprint arXiv:1904.03646*, 2019.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthony 等人 [2019] Thomas Anthony、Robert Nishihara、Philipp Moritz、Tim Salimans
    和 John Schulman。《策略梯度搜索：无需搜索树的在线规划和专家迭代》。*arXiv 预印本 arXiv:1904.03646*，2019 年。
- en: Anthony et al. [2020] Thomas Anthony, Tom Eccles, Andrea Tacchetti, János Kramár,
    Ian Gemp, Thomas C Hudson, Nicolas Porcel, Marc Lanctot, Julien Pérolat, Richard
    Everett, et al. Learning to play no-press diplomacy with best response policy
    iteration. *arXiv preprint arXiv:2006.04635*, 2020.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthony 等人 [2020] Thomas Anthony、Tom Eccles、Andrea Tacchetti、János Kramár、Ian
    Gemp、Thomas C Hudson、Nicolas Porcel、Marc Lanctot、Julien Pérolat、Richard Everett
    等人。《学习通过最佳响应策略迭代进行无压外交》。*arXiv 预印本 arXiv:2006.04635*，2020 年。
- en: Arneson et al. [2010] Broderick Arneson, Ryan B Hayward, and Philip Henderson.
    Monte Carlo Tree Search in Hex. *IEEE Transactions on Computational Intelligence
    and AI in Games*, 2(4):251–258, 2010.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arneson 等人 [2010] Broderick Arneson、Ryan B Hayward 和 Philip Henderson。《Hex 中的蒙特卡罗树搜索》。*IEEE
    计算智能与游戏 AI 期刊*，2(4):251–258，2010 年。
- en: Azizzadenesheli et al. [2018] Kamyar Azizzadenesheli, Brandon Yang, Weitang
    Liu, Emma Brunskill, Zachary C Lipton, and Animashree Anandkumar. Surprising negative
    results for generative adversarial tree search. *arXiv preprint arXiv:1806.05780*,
    2018.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azizzadenesheli 等人 [2018] Kamyar Azizzadenesheli、Brandon Yang、Weitang Liu、Emma
    Brunskill、Zachary C Lipton 和 Animashree Anandkumar。《生成对抗树搜索的令人惊讶的负面结果》。*arXiv
    预印本 arXiv:1806.05780*，2018 年。
- en: Babaeizadeh et al. [2017] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,
    Roy H Campbell, and Sergey Levine. Stochastic variational video prediction. *arXiv
    preprint arXiv:1710.11252*, 2017.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Babaeizadeh 等人 [2017] Mohammad Babaeizadeh、Chelsea Finn、Dumitru Erhan、Roy H
    Campbell 和 Sergey Levine。《随机变分视频预测》。*arXiv 预印本 arXiv:1710.11252*，2017 年。
- en: 'Bäck [1996] Thomas Bäck. *Evolutionary algorithms in theory and practice: evolutionary
    strategies, evolutionary programming, genetic algorithms*. Oxford University Press,
    1996.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bäck [1996] Thomas Bäck。*理论与实践中的进化算法：进化策略、进化编程、遗传算法*。牛津大学出版社，1996 年。
- en: 'Badia et al. [2020] Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski,
    Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, and Charles Blundell. Agent57:
    Outperforming the Atari human benchmark. *arXiv preprint arXiv:2003.13350*, 2020.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Badia 等人 [2020] Adrià Puigdomènech Badia、Bilal Piot、Steven Kapturowski、Pablo
    Sprechmann、Alex Vitvitskyi、Daniel Guo 和 Charles Blundell。《Agent57：超越 Atari 人类基准》。*arXiv
    预印本 arXiv:2003.13350*，2020 年。
- en: 'Bellemare et al. [2013] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael
    Bowling. The Arcade Learning Environment: An evaluation platform for general agents.
    *Journal of Artificial Intelligence Research*, 47:253–279, 2013.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等人 [2013] Marc G Bellemare、Yavar Naddaf、Joel Veness 和 Michael Bowling。《街机学习环境：通用智能体的评估平台》。*人工智能研究杂志*，47:253–279，2013
    年。
- en: Bellman [1957, 2013] Richard Bellman. *Dynamic programming*. Courier Corporation,
    1957, 2013.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellman [1957, 2013] Richard Bellman。*动态规划*。Courier Corporation，1957 年，2013
    年。
- en: Bengio et al. [2015] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer.
    Scheduled sampling for sequence prediction with recurrent neural networks. In
    *Advances in Neural Information Processing Systems*, pages 1171–1179, 2015.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等人 [2015] Samy Bengio、Oriol Vinyals、Navdeep Jaitly 和 Noam Shazeer。《递归神经网络的计划采样》。在
    *神经信息处理系统进展* 中，页码 1171–1179，2015 年。
- en: Bengio et al. [2009] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason
    Weston. Curriculum learning. In *Proceedings of the 26th Annual International
    Conference on Machine Learning*, pages 41–48, 2009.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等人 [2009] Yoshua Bengio、Jérôme Louradour、Ronan Collobert 和 Jason Weston。《课程学习》。在
    *第26届年度国际机器学习会议论文集* 中，页码 41–48，2009 年。
- en: Berner et al. [2019] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,
    Przemysław Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme,
    Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. *arXiv
    preprint arXiv:1912.06680*, 2019.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berner 等人 [2019] Christopher Berner、Greg Brockman、Brooke Chan、Vicki Cheung、Przemysław
    Debiak、Christy Dennison、David Farhi、Quirin Fischer、Shariq Hashme、Chris Hesse 等人。《使用大规模深度强化学习的
    Dota 2》。*arXiv 预印本 arXiv:1912.06680*，2019 年。
- en: Bishop [1994] Christopher M Bishop. Mixture density networks. Technical Report
    NCRG/94/004, Aston University, 1994.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bishop [1994] Christopher M Bishop。《混合密度网络》。技术报告 NCRG/94/004，阿斯顿大学，1994 年。
- en: Bishop [2006] Christopher M Bishop. *Pattern recognition and machine learning*.
    Information science and statistics. Springer Verlag, Heidelberg, 2006.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bishop [2006] Christopher M Bishop。*模式识别与机器学习*。信息科学与统计学。施普林格出版社，海德堡，2006 年。
- en: Brockman et al. [2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas
    Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. *arXiv preprint
    arXiv:1606.01540*, 2016.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brockman等人[2016] Greg Brockman、Vicki Cheung、Ludwig Pettersson、Jonas Schneider、John
    Schulman、Jie Tang和Wojciech Zaremba. OpenAI Gym。*arXiv预印本 arXiv:1606.01540*，2016年。
- en: 'Brown and Sandholm [2018] Noam Brown and Tuomas Sandholm. Superhuman AI for
    heads-up no-limit poker: Libratus beats top professionals. *Science*, 359(6374):418–424,
    2018.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown和Sandholm [2018] Noam Brown和Tuomas Sandholm. 超人类AI在Heads-up无上限扑克中的应用：Libratus击败顶级专业玩家。*科学*，359(6374):418–424，2018年。
- en: Brown and Sandholm [2019] Noam Brown and Tuomas Sandholm. Superhuman ai for
    multiplayer poker. *Science*, 365(6456):885–890, 2019.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown和Sandholm [2019] Noam Brown和Tuomas Sandholm. 超人类AI在多人扑克中的应用。*科学*，365(6456):885–890，2019年。
- en: Browne et al. [2012] Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M
    Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon
    Samothrakis, and Simon Colton. A survey of Monte Carlo Tree Search methods. *IEEE
    Transactions on Computational Intelligence and AI in Games*, 4(1):1–43, 2012.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Browne等人[2012] Cameron B Browne、Edward Powley、Daniel Whitehouse、Simon M Lucas、Peter
    I Cowling、Philipp Rohlfshagen、Stephen Tavener、Diego Perez、Spyridon Samothrakis和Simon
    Colton. Monte Carlo树搜索方法综述。*IEEE计算智能与游戏中的人工智能汇刊*，4(1):1–43，2012年。
- en: Buesing et al. [2018] Lars Buesing, Theophane Weber, Sébastien Racaniere, SM Eslami,
    Danilo Rezende, David P Reichert, Fabio Viola, Frederic Besse, Karol Gregor, Demis
    Hassabis, et al. Learning and querying fast generative models for reinforcement
    learning. *arXiv preprint arXiv:1802.03006*, 2018.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buesing等人[2018] Lars Buesing、Theophane Weber、Sébastien Racaniere、SM Eslami、Danilo
    Rezende、David P Reichert、Fabio Viola、Frederic Besse、Karol Gregor、Demis Hassabis等.
    学习和查询快速生成模型以用于强化学习。*arXiv预印本 arXiv:1802.03006*，2018年。
- en: 'Çalışır and Pehlivanoğlu [2019] Sinan Çalışır and Meltem Kurt Pehlivanoğlu.
    Model-free reinforcement learning algorithms: A survey. In *2019 27th Signal Processing
    and Communications Applications Conference (SIU)*, pages 1–4, 2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Çalışır和Pehlivanoğlu [2019] Sinan Çalışır和Meltem Kurt Pehlivanoğlu. 无模型强化学习算法：综述。在*2019年第27届信号处理与通信应用会议（SIU）*，第1–4页，2019年。
- en: Campbell et al. [2002] Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu.
    Deep Blue. *Artificial Intelligence*, 134(1-2):57–83, 2002.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Campbell等人[2002] Murray Campbell、A Joseph Hoane Jr和Feng-hsiung Hsu. 深蓝。*人工智能*，134(1-2):57–83，2002年。
- en: Chao [2013] Yang Chao. Sokoban.org, 2013.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chao [2013] Yang Chao. Sokoban.org，2013年。
- en: Chiappa et al. [2017] Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, and
    Shakir Mohamed. Recurrent environment simulators. *arXiv preprint arXiv:1704.02254*,
    2017.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiappa等人[2017] Silvia Chiappa、Sébastien Racaniere、Daan Wierstra和Shakir Mohamed.
    循环环境模拟器。*arXiv预印本 arXiv:1704.02254*，2017年。
- en: Chua et al. [2018] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey
    Levine. Deep reinforcement learning in a handful of trials using probabilistic
    dynamics models. In *Advances in Neural Information Processing Systems*, pages
    4754–4765, 2018.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chua等人[2018] Kurtland Chua、Roberto Calandra、Rowan McAllister和Sergey Levine.
    使用概率动态模型在少量试验中进行深度强化学习。在*神经信息处理系统进展*，第4754–4765页，2018年。
- en: Clavera et al. [2018] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro
    Fujita, Tamim Asfour, and Pieter Abbeel. Model-based reinforcement learning via
    meta-policy optimization. *arXiv preprint arXiv:1809.05214*, 2018.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clavera等人[2018] Ignasi Clavera、Jonas Rothfuss、John Schulman、Yasuhiro Fujita、Tamim
    Asfour和Pieter Abbeel. 通过元策略优化的基于模型的强化学习。*arXiv预印本 arXiv:1809.05214*，2018年。
- en: Coulom [2006] Rémi Coulom. Efficient selectivity and backup operators in Monte-Carlo
    Tree Search. In *International Conference on Computers and Games*, pages 72–83\.
    Springer, 2006.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coulom [2006] Rémi Coulom. Monte-Carlo树搜索中的高效选择性和备份操作符。在*国际计算机与游戏会议*，第72–83页。Springer，2006年。
- en: Culberson [1997] Joseph Culberson. Sokoban is PSPACE-complete. Technical report,
    University of Alberta, 1997.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Culberson [1997] Joseph Culberson. Sokoban是PSPACE完全的。技术报告，阿尔伯塔大学，1997年。
- en: 'Deisenroth and Rasmussen [2011] Marc Deisenroth and Carl E Rasmussen. Pilco:
    A model-based and data-efficient approach to policy search. In *Proceedings of
    the 28th International Conference on Machine Learning (ICML-11)*, pages 465–472,
    2011.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deisenroth和Rasmussen [2011] Marc Deisenroth和Carl E Rasmussen. Pilco: 一种基于模型的数据高效策略搜索方法。在*第28届国际机器学习会议（ICML-11）论文集*，第465–472页，2011年。'
- en: Deisenroth et al. [2013a] Marc Peter Deisenroth, Dieter Fox, and Carl Edward
    Rasmussen. Gaussian processes for data-efficient learning in robotics and control.
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 37(2):408–423,
    2013a.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deisenroth等人[2013a] Marc Peter Deisenroth、Dieter Fox和Carl Edward Rasmussen.
    高斯过程在机器人学和控制中的数据高效学习。*IEEE模式分析与机器智能汇刊*，37(2):408–423，2013a年。
- en: Deisenroth et al. [2013b] Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters.
    A survey on policy search for robotics. In *rFoundations and Trends in Robotics
    2*, pages 1–142\. Now publishers, 2013b.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deisenroth et al. [2013b] Marc Peter Deisenroth、Gerhard Neumann 和 Jan Peters。机器人政策搜索的调查。发表于
    *rFoundations and Trends in Robotics 2*，第 1–142 页。Now publishers，2013b 年。
- en: Dietterich [2000] Thomas G Dietterich. Hierarchical reinforcement learning with
    the MAXQ value function decomposition. *Journal of Artificial Intelligence Research*,
    13:227–303, 2000.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dietterich [2000] Thomas G Dietterich。带有 MAXQ 值函数分解的层次强化学习。*人工智能研究杂志*，13:227–303，2000
    年。
- en: Doan et al. [2019] Thang Doan, Joao Monteiro, Isabela Albuquerque, Bogdan Mazoure,
    Audrey Durand, Joelle Pineau, and R Devon Hjelm. On-line adaptative curriculum
    learning for gans. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 33, pages 3470–3477, 2019.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doan et al. [2019] Thang Doan、Joao Monteiro、Isabela Albuquerque、Bogdan Mazoure、Audrey
    Durand、Joelle Pineau 和 R Devon Hjelm。用于生成对抗网络的在线自适应课程学习。发表于 *AAAI 人工智能会议论文集*，第
    33 卷，第 3470–3477 页，2019 年。
- en: Doerr et al. [2018] Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong,
    Stefan Schaal, Marc Toussaint, and Sebastian Trimpe. Probabilistic recurrent state-space
    models. *arXiv preprint arXiv:1801.10395*, 2018.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doerr et al. [2018] Andreas Doerr、Christian Daniel、Martin Schiegg、Duy Nguyen-Tuong、Stefan
    Schaal、Marc Toussaint 和 Sebastian Trimpe。概率递归状态空间模型。*arXiv 预印本 arXiv:1801.10395*，2018
    年。
- en: Dor and Zwick [1999] Dorit Dor and Uri Zwick. Sokoban and other motion planning
    problems. *Computational Geometry*, 13(4):215–228, 1999.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dor 和 Zwick [1999] Dorit Dor 和 Uri Zwick。Sokoban 和其他运动规划问题。*计算几何*，13(4):215–228，1999
    年。
- en: 'Duan et al. [2016] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya
    Sutskever, and Pieter Abbeel. RL²: Fast reinforcement learning via slow reinforcement
    learning. *arXiv preprint arXiv:1611.02779*, 2016.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan et al. [2016] Yan Duan、John Schulman、Xi Chen、Peter L Bartlett、Ilya Sutskever
    和 Pieter Abbeel。RL²：通过慢速强化学习实现快速强化学习。*arXiv 预印本 arXiv:1611.02779*，2016 年。
- en: 'Ebert et al. [2018] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie,
    Alex Lee, and Sergey Levine. Visual foresight: Model-based deep reinforcement
    learning for vision-based robotic control. *arXiv preprint arXiv:1812.00568*,
    2018.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ebert et al. [2018] Frederik Ebert、Chelsea Finn、Sudeep Dasari、Annie Xie、Alex
    Lee 和 Sergey Levine。视觉预测：基于模型的深度强化学习用于视觉控制的机器人。*arXiv 预印本 arXiv:1812.00568*，2018
    年。
- en: 'Farquhar et al. [2018] Gregory Farquhar, Tim Rocktäschel, Maximilian Igl, and
    SA Whiteson. TreeQN and ATreeC: Differentiable tree planning for deep reinforcement
    learning. International Conference on Learning Representations, 2018.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Farquhar et al. [2018] Gregory Farquhar、Tim Rocktäschel、Maximilian Igl 和 SA
    Whiteson。TreeQN 和 ATreeC：用于深度强化学习的可微树规划。国际学习表征会议，2018 年。
- en: 'Fei-Fei et al. [2009] Li Fei-Fei, Jia Deng, and Kai Li. Imagenet: Constructing
    a large-scale image database. *Journal of Vision*, 9(8):1037–1037, 2009.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei-Fei et al. [2009] Li Fei-Fei、Jia Deng 和 Kai Li。Imagenet：构建大规模图像数据库。*视觉杂志*，9(8):1037–1037，2009
    年。
- en: Feinberg et al. [2018] Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan,
    Joseph E Gonzalez, and Sergey Levine. Model-based value estimation for efficient
    model-free reinforcement learning. *arXiv preprint arXiv:1803.00101*, 2018.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feinberg et al. [2018] Vladimir Feinberg、Alvin Wan、Ion Stoica、Michael I Jordan、Joseph
    E Gonzalez 和 Sergey Levine。基于模型的价值估计用于高效的无模型强化学习。*arXiv 预印本 arXiv:1803.00101*，2018
    年。
- en: Feng et al. [2020] Dieqiao Feng, Carla P Gomes, and Bart Selman. Solving hard
    AI planning instances using curriculum-driven deep reinforcement learning. *arXiv
    preprint arXiv:2006.02689*, 2020.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. [2020] Dieqiao Feng、Carla P Gomes 和 Bart Selman。利用课程驱动的深度强化学习解决困难的人工智能规划实例。*arXiv
    预印本 arXiv:2006.02689*，2020 年。
- en: Finn and Levine [2017] Chelsea Finn and Sergey Levine. Deep visual foresight
    for planning robot motion. In *2017 IEEE International Conference on Robotics
    and Automation (ICRA)*, pages 2786–2793, 2017.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn 和 Levine [2017] Chelsea Finn 和 Sergey Levine。用于规划机器人运动的深度视觉预测。发表于 *2017
    IEEE 国际机器人与自动化会议 (ICRA)*，第 2786–2793 页，2017 年。
- en: 'Finn et al. [2016] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost
    learning: Deep inverse optimal control via policy optimization. In *International
    Conference on Machine Learning*, pages 49–58, 2016.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn et al. [2016] Chelsea Finn、Sergey Levine 和 Pieter Abbeel。指导成本学习：通过策略优化实现深度逆优化控制。发表于
    *国际机器学习会议*，第 49–58 页，2016 年。
- en: Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic
    Meta-Learning for fast adaptation of deep networks. *arXiv preprint arXiv:1703.03400*,
    2017.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn et al. [2017] Chelsea Finn、Pieter Abbeel 和 Sergey Levine。模型无关元学习用于深度网络的快速适应。*arXiv
    预印本 arXiv:1703.03400*，2017 年。
- en: Gamma et al. [2009] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides.
    *Design Patterns Elements of reusable object-oriented sofware*. Addison Wesley,
    2009.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamma et al. [2009] Erich Gamma, Richard Helm, Ralph Johnson, 和 John Vlissides.
    *设计模式 可复用面向对象软件的元素*. Addison Wesley, 2009。
- en: 'Gasparetto et al. [2015] Alessandro Gasparetto, Paolo Boscariol, Albano Lanzutti,
    and Renato Vidoni. Path planning and trajectory planning algorithms: A general
    overview. In *Motion and Operation Planning of Robotic Systems*, pages 3–27\.
    Springer, 2015.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gasparetto et al. [2015] Alessandro Gasparetto, Paolo Boscariol, Albano Lanzutti,
    和 Renato Vidoni. 路径规划和轨迹规划算法：概述。收录于*机器人系统的运动和操作规划*, 页码 3–27。Springer, 2015。
- en: Goodfellow et al. [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. In *Advances in Neural Information Processing Systems*, pages
    2672–2680, 2014.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio. 生成对抗网络。收录于*神经信息处理系统进展*,
    页码 2672–2680, 2014。
- en: Goodfellow et al. [2016] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
    *Deep learning*. MIT Press, Cambridge, 2016.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. [2016] Ian Goodfellow, Yoshua Bengio, 和 Aaron Courville. *深度学习*.
    MIT Press, Cambridge, 2016。
- en: Graves [2013] Alex Graves. Generating sequences with recurrent neural networks.
    *arXiv preprint arXiv:1308.0850*, 2013.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves [2013] Alex Graves. 使用循环神经网络生成序列。*arXiv 预印本 arXiv:1308.0850*, 2013。
- en: Gu et al. [2016] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey
    Levine. Continuous deep Q-learning with model-based acceleration. In *International
    Conference on Machine Learning*, pages 2829–2838, 2016.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. [2016] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, 和 Sergey Levine.
    基于模型的加速的连续深度 Q 学习。收录于*国际机器学习大会*, 页码 2829–2838, 2016。
- en: Guez et al. [2018] Arthur Guez, Théophane Weber, Ioannis Antonoglou, Karen Simonyan,
    Oriol Vinyals, Daan Wierstra, Rémi Munos, and David Silver. Learning to search
    with MCTSnets. *arXiv preprint arXiv:1802.04697*, 2018.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guez et al. [2018] Arthur Guez, Théophane Weber, Ioannis Antonoglou, Karen Simonyan,
    Oriol Vinyals, Daan Wierstra, Rémi Munos, 和 David Silver. 使用 MCTSnets 进行学习搜索。*arXiv
    预印本 arXiv:1802.04697*, 2018。
- en: Guez et al. [2019] Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, Sébastien
    Racanière, Théophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles,
    et al. An investigation of model-free planning. *arXiv preprint arXiv:1901.03559*,
    2019.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guez et al. [2019] Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, Sébastien
    Racanière, Théophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles,
    等. 无模型规划的调查。*arXiv 预印本 arXiv:1901.03559*, 2019。
- en: 'Guo et al. [2016] Yanming Guo, Yu Liu, Ard Oerlemans, Songyang Lao, Song Wu,
    and Michael S Lew. Deep learning for visual understanding: A review. *Neurocomputing*,
    187:27–48, 2016.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. [2016] 闫铭果, 刘宇, Ard Oerlemans, 宋阳劳, 宋吴, 和 Michael S Lew. 深度学习在视觉理解中的应用综述。*神经计算*,
    187:27–48, 2016。
- en: Ha and Schmidhuber [2018a] David Ha and Jürgen Schmidhuber. Recurrent world
    models facilitate policy evolution. In *Advances in Neural Information Processing
    Systems*, pages 2450–2462, 2018a.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ha and Schmidhuber [2018a] David Ha 和 Jürgen Schmidhuber. 循环世界模型促进策略演化。收录于*神经信息处理系统进展*,
    页码 2450–2462, 2018a。
- en: Ha and Schmidhuber [2018b] David Ha and Jürgen Schmidhuber. World models. *arXiv
    preprint arXiv:1803.10122*, 2018b.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ha and Schmidhuber [2018b] David Ha 和 Jürgen Schmidhuber. 世界模型。*arXiv 预印本 arXiv:1803.10122*,
    2018b。
- en: 'Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey
    Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning
    with a stochastic actor. *arXiv preprint arXiv:1801.01290*, 2018.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, 和 Sergey
    Levine. Soft actor-critic: 基于策略的最大熵深度强化学习与随机行为者。*arXiv 预印本 arXiv:1801.01290*,
    2018。'
- en: Hafner et al. [2018] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas,
    David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning
    from pixels. *arXiv preprint arXiv:1811.04551*, 2018.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner et al. [2018] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas,
    David Ha, Honglak Lee, 和 James Davidson. 从像素中学习潜在动态进行规划。*arXiv 预印本 arXiv:1811.04551*,
    2018。
- en: 'Hafner et al. [2019] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad
    Norouzi. Dream to control: Learning behaviors by latent imagination. *arXiv preprint
    arXiv:1912.01603*, 2019.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner et al. [2019] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, 和 Mohammad
    Norouzi. 梦想控制：通过潜在想象学习行为。*arXiv 预印本 arXiv:1912.01603*, 2019。
- en: Hamrick [2019] Jessica B Hamrick. Analogues of mental simulation and imagination
    in deep learning. *Current Opinion in Behavioral Sciences*, 29:8–16, 2019.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamrick [2019] Jessica B Hamrick. 深度学习中心理模拟和想象的类比。*当前行为科学观点*, 29:8–16, 2019。
- en: Hamrick et al. [2017] Jessica B Hamrick, Andrew J Ballard, Razvan Pascanu, Oriol
    Vinyals, Nicolas Heess, and Peter W Battaglia. Metacontrol for adaptive imagination-based
    optimization. *arXiv preprint arXiv:1705.02670*, 2017.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamrick 等人 [2017] Jessica B Hamrick, Andrew J Ballard, Razvan Pascanu, Oriol
    Vinyals, Nicolas Heess 和 Peter W Battaglia. 元控制用于自适应基于想象的优化。*arXiv 预印本 arXiv:1705.02670*,
    2017。
- en: Hart et al. [1968] Peter E Hart, Nils J Nilsson, and Bertram Raphael. A formal
    basis for the heuristic determination of minimum cost paths. *IEEE transactions
    on Systems Science and Cybernetics*, 4(2):100–107, 1968.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hart 等人 [1968] Peter E Hart, Nils J Nilsson 和 Bertram Raphael. 启发式最小成本路径的正式基础。*IEEE系统科学与控制论学报*，4(2)：100–107，1968年。
- en: 'Hayward and Toft [2019] Ryan B Hayward and Bjarne Toft. *Hex: The Full Story*.
    CRC Press, 2019.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hayward 和 Toft [2019] Ryan B Hayward 和 Bjarne Toft. *Hex: 完整故事*。CRC Press,
    2019。'
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, pages 770–778, 2016.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren 和 Jian Sun. 深度残差学习用于图像识别。见于
    *IEEE计算机视觉与模式识别会议论文集*，第770–778页，2016年。
- en: Hearn and Demaine [2009] Robert A Hearn and Erik D Demaine. *Games, puzzles,
    and computation*. CRC Press, 2009.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hearn 和 Demaine [2009] Robert A Hearn 和 Erik D Demaine. *游戏、谜题与计算*。CRC Press,
    2009年。
- en: Heess et al. [2015] Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap,
    Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic
    value gradients. In *Advances in Neural Information Processing Systems*, pages
    2944–2952, 2015.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heess 等人 [2015] Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap,
    Tom Erez 和 Yuval Tassa. 通过随机值梯度学习连续控制策略。见于 *神经信息处理系统进展*，第2944–2952页，2015年。
- en: Henderson et al. [2017] Peter Henderson, Riashat Islam, Philip Bachman, Joelle
    Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters.
    *arXiv preprint arXiv:1709.06560*, 2017.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson 等人 [2017] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau,
    Doina Precup 和 David Meger. 重要的深度强化学习。*arXiv 预印本 arXiv:1709.06560*, 2017。
- en: 'Hendrikx et al. [2013] Mark Hendrikx, Sebastiaan Meijer, Joeri Van Der Velden,
    and Alexandru Iosup. Procedural content generation for games: A survey. *ACM Transactions
    on Multimedia Computing, Communications, and Applications (TOMM)*, 9(1):1–22,
    2013.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrikx 等人 [2013] Mark Hendrikx, Sebastiaan Meijer, Joeri Van Der Velden 和
    Alexandru Iosup. 游戏的程序生成内容：综述。*ACM多媒体计算、通信与应用学报 (TOMM)*，9(1)：1–22，2013年。
- en: 'Hennessy and Patterson [2017] John L Hennessy and David A Patterson. *Computer
    architecture: a quantitative approach*. Elsevier, 2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hennessy 和 Patterson [2017] John L Hennessy 和 David A Patterson. *计算机体系结构：定量方法*。Elsevier,
    2017。
- en: 'Hessel et al. [2017] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul,
    Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David
    Silver. Rainbow: Combining improvements in deep reinforcement learning. *arXiv
    preprint arXiv:1710.02298*, 2017.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hessel 等人 [2017] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul,
    Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar 和 David Silver.
    彩虹：结合深度强化学习的改进。*arXiv 预印本 arXiv:1710.02298*, 2017。
- en: 'Hospedales et al. [2020] Timothy Hospedales, Antreas Antoniou, Paul Micaelli,
    and Amos Storkey. Meta-learning in neural networks: A survey. *arXiv preprint
    arXiv:2004.05439*, 2020.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hospedales 等人 [2020] Timothy Hospedales, Antreas Antoniou, Paul Micaelli 和 Amos
    Storkey. 神经网络中的元学习：综述。*arXiv 预印本 arXiv:2004.05439*, 2020。
- en: Hui [2018] Jonathan Hui. Model-based reinforcement learning https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323.
    Medium post, 2018.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hui [2018] Jonathan Hui. 基于模型的强化学习 https://medium.com/@jonathan_hui/rl-model-based-reinforcement-learning-3c2b6f0aa323。Medium
    博客，2018年。
- en: 'Huisman et al. [2020] Mike Huisman, Jan van Rijn, and Aske Plaat. Metalearning
    for deep neural networks. In Pavel Brazdil, editor, *Metalearning: Applications
    to data mining*. Springer, 2020.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huisman 等人 [2020] Mike Huisman, Jan van Rijn 和 Aske Plaat. 深度神经网络的元学习。见于 Pavel
    Brazdil 编著，*元学习：数据挖掘的应用*。Springer, 2020年。
- en: Iida et al. [2002] Hiroyuki Iida, Makoto Sakuta, and Jeff Rollason. Computer
    shogi. *Artificial Intelligence*, 134(1-2):121–144, 2002.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iida 等人 [2002] Hiroyuki Iida, Makoto Sakuta 和 Jeff Rollason. 计算机将棋。*人工智能*，134(1-2)：121–144，2002年。
- en: Ilin et al. [2007] Roman Ilin, Robert Kozma, and Paul J Werbos. Efficient learning
    in cellular simultaneous recurrent neural networks—the case of maze navigation
    problem. In *2007 IEEE International Symposium on Approximate Dynamic Programming
    and Reinforcement Learning*, pages 324–329, 2007.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilin 等人 [2007] Roman Ilin, Robert Kozma 和 Paul J Werbos. 在细胞同步递归神经网络中的高效学习——以迷宫导航问题为例。见于
    *2007年IEEE国际近似动态编程与强化学习研讨会*，第324–329页，2007年。
- en: Islam et al. [2017] Riashat Islam, Peter Henderson, Maziar Gomrokchi, and Doina
    Precup. Reproducibility of benchmarked deep reinforcement learning tasks for continuous
    control. *arXiv preprint arXiv:1708.04133*, 2017.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Islam 等 [2017] Riashat Islam, Peter Henderson, Maziar Gomrokchi 和 Doina Precup.
    连续控制的基准深度强化学习任务的可重复性。*arXiv 预印本 arXiv:1708.04133*，2017年。
- en: Isola et al. [2017] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.
    Image-to-image translation with conditional adversarial networks. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 1125–1134,
    2017.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isola 等 [2017] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou 和 Alexei A Efros. 使用条件对抗网络的图像到图像翻译。发表于
    *IEEE 计算机视觉与模式识别会议论文集*，第1125–1134页，2017年。
- en: Jaderberg et al. [2019] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke
    Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz,
    Ari S Morcos, Avraham Ruderman, et al. Human-level performance in 3D multiplayer
    games with population-based reinforcement learning. *Science*, 364(6443):859–865,
    2019.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等 [2019] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris,
    Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S
    Morcos, Avraham Ruderman 等。基于人群的强化学习在3D多人游戏中的人类水平表现。*科学*，364(6443)：859–865，2019年。
- en: 'Janner et al. [2019] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.
    When to trust your model: Model-based policy optimization. In *Advances in Neural
    Information Processing Systems*, pages 12498–12509, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janner 等 [2019] Michael Janner, Justin Fu, Marvin Zhang 和 Sergey Levine. 何时信任你的模型：基于模型的策略优化。发表于
    *神经信息处理系统进展*，第12498–12509页，2019年。
- en: 'Juliani et al. [2019] Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges,
    Jonathan Harper, Ervin Teng, Hunter Henry, Adam Crespi, Julian Togelius, and Danny
    Lange. Obstacle tower: A generalization challenge in vision, control, and planning.
    *arXiv preprint arXiv:1902.01378*, 2019.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Juliani 等 [2019] Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan
    Harper, Ervin Teng, Hunter Henry, Adam Crespi, Julian Togelius 和 Danny Lange.
    障碍塔：视觉、控制和规划中的泛化挑战。*arXiv 预印本 arXiv:1902.01378*，2019年。
- en: 'Junghanns and Schaeffer [2001] Andreas Junghanns and Jonathan Schaeffer. Sokoban:
    Enhancing general single-agent search methods using domain knowledge. *Artificial
    Intelligence*, 129(1-2):219–251, 2001.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Junghanns 和 Schaeffer [2001] Andreas Junghanns 和 Jonathan Schaeffer. Sokoban：利用领域知识提升通用单代理搜索方法。*人工智能*，129(1-2)：219–251，2001年。
- en: Justesen et al. [2019] Niels Justesen, Philip Bontrager, Julian Togelius, and
    Sebastian Risi. Deep learning for video game playing. *IEEE Transactions on Games*,
    12(1):1–20, 2019.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Justesen 等 [2019] Niels Justesen, Philip Bontrager, Julian Togelius 和 Sebastian
    Risi. 用于视频游戏的深度学习。*IEEE 游戏学报*，12(1)：1–20，2019年。
- en: 'Kaelbling et al. [1996] Leslie Pack Kaelbling, Michael L Littman, and Andrew W
    Moore. Reinforcement learning: A survey. *Journal of Artificial Intelligence Research*,
    4:237–285, 1996.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaelbling 等 [1996] Leslie Pack Kaelbling, Michael L Littman 和 Andrew W Moore.
    强化学习：综述。*人工智能研究杂志*，4：237–285，1996年。
- en: Kahneman [2011] Daniel Kahneman. *Thinking, fast and slow*. Farrar, Straus and
    Giroux, 2011.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kahneman [2011] Daniel Kahneman. *思考，快与慢*。Farrar, Straus 和 Giroux，2011年。
- en: Kaiser et al. [2019] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej
    Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr
    Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for Atari.
    *arXiv preprint arXiv:1903.00374*, 2019.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaiser 等 [2019] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski,
    Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski,
    Sergey Levine 等。用于Atari的模型基强化学习。*arXiv 预印本 arXiv:1903.00374*，2019年。
- en: Kalweit and Boedecker [2017] Gabriel Kalweit and Joschka Boedecker. Uncertainty-driven
    imagination for continuous deep reinforcement learning. In *Conference on Robot
    Learning*, pages 195–206, 2017.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalweit 和 Boedecker [2017] Gabriel Kalweit 和 Joschka Boedecker. 不确定性驱动的想象力用于连续深度强化学习。发表于
    *机器人学习会议*，第195–206页，2017年。
- en: Kamthe and Deisenroth [2017] Sanket Kamthe and Marc Peter Deisenroth. Data-efficient
    reinforcement learning with probabilistic model predictive control. *arXiv preprint
    arXiv:1706.06491*, 2017.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamthe 和 Deisenroth [2017] Sanket Kamthe 和 Marc Peter Deisenroth. 具有概率模型预测控制的数据高效强化学习。*arXiv
    预印本 arXiv:1706.06491*，2017年。
- en: 'Karl et al. [2016] Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick
    Van der Smagt. Deep variational Bayes filters: Unsupervised learning of state
    space models from raw data. *arXiv preprint arXiv:1605.06432*, 2016.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karl 等 [2016] Maximilian Karl, Maximilian Soelch, Justin Bayer 和 Patrick Van
    der Smagt. 深度变分贝叶斯滤波器：从原始数据中无监督学习状态空间模型。*arXiv 预印本 arXiv:1605.06432*，2016年。
- en: Kelley [1960] Henry J Kelley. Gradient theory of optimal flight paths. *American
    Rocket Society Journal*, 30(10):947–954, 1960.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kelley [1960] Henry J Kelley. 最优飞行路径的梯度理论。*美国火箭学会杂志*，30(10)：947–954，1960年。
- en: 'Kempka et al. [2016] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek,
    and Wojciech Jaśkowski. VizDoom: A Doom-based AI research platform for visual
    reinforcement learning. In *2016 IEEE Conference on Computational Intelligence
    and Games*, pages 1–8, 2016.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khetarpal et al. [2018] Khimya Khetarpal, Zafarali Ahmed, Andre Cianflone,
    Riashat Islam, and Joelle Pineau. Re-evaluate: Reproducibility in evaluating reinforcement
    learning algorithms. In *Reproducibility in Machine Learning Workshop, ICML*,
    2018.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling [2013] Diederik P Kingma and Max Welling. Auto-encoding variational
    Bayes. *arXiv preprint arXiv:1312.6114*, 2013.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling [2019] Diederik P Kingma and Max Welling. An introduction
    to variational autoencoders. *arXiv preprint arXiv:1906.02691*, 2019.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knuth and Moore [1975] Donald E Knuth and Ronald W Moore. An analysis of alpha-beta
    pruning. *Artificial Intelligence*, 6(4):293–326, 1975.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kober et al. [2013] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement
    learning in robotics: A survey. *The International Journal of Robotics Research*,
    32(11):1238–1274, 2013.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kocsis and Szepesvári [2006] Levente Kocsis and Csaba Szepesvári. Bandit based
    Monte-Carlo planning. In *European Conference on Machine Learning*, pages 282–293.
    Springer, 2006.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Konda and Tsitsiklis [2000] Vijay R Konda and John N Tsitsiklis. Actor-critic
    algorithms. In *Advances in Neural Information Processing Systems*, pages 1008–1014,
    2000.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Korf [1985] Richard E Korf. Depth-first iterative-deepening: An optimal admissible
    tree search. *Artificial intelligence*, 27(1):97–109, 1985.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. [2012] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    Imagenet classification with deep convolutional neural networks. In *Advances
    in Neural Information Processing Systems*, pages 1097–1105, 2012.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kulkarni et al. [2016] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi,
    and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal
    abstraction and intrinsic motivation. In *Advances in Neural Information Processing
    Systems*, pages 3675–3683, 2016.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lakshminarayanan et al. [2017] Balaji Lakshminarayanan, Alexander Pritzel, and
    Charles Blundell. Simple and scalable predictive uncertainty estimation using
    deep ensembles. In *Advances in Neural Information Processing Systems*, pages
    6402–6413, 2017.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Landis and Yaglom [2001] Evgenii Mikhailovich Landis and Isaak Moiseevich Yaglom.
    About Aleksandr Semenovich Kronrod. *Russian Mathematical Surveys*, 56(5):993–1007,
    2001.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laterre et al. [2018] Alexandre Laterre, Yunguan Fu, Mohamed Khalil Jabri,
    Alain-Sam Cohen, David Kas, Karl Hajjar, Torbjorn S Dahl, Amine Kerkeni, and Karim
    Beguir. Ranked reward: Enabling self-play reinforcement learning for combinatorial
    optimization. *arXiv preprint arXiv:1807.01672*, 2018.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latombe [2012] Jean-Claude Latombe. *Robot motion planning*, volume 124. Springer
    Science & Business Media, 2012.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. [2015] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning.
    *Nature*, 521(7553):436, 2015.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leibfried et al. [2016] Felix Leibfried, Nate Kushman, and Katja Hofmann. A
    deep learning approach for joint video frame and reward prediction in Atari games.
    *arXiv preprint arXiv:1611.07078*, 2016.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levine and Abbeel [2014] Sergey Levine and Pieter Abbeel. Learning neural network
    policies with guided policy search under unknown dynamics. In *Advances in Neural
    Information Processing Systems*, pages 1071–1079, 2014.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levine and Koltun [2013] Sergey Levine and Vladlen Koltun. Guided policy search.
    In *International Conference on Machine Learning*, pages 1–9, 2013.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lillicrap et al. [2015] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,
    Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous
    control with deep reinforcement learning. *arXiv preprint arXiv:1509.02971*, 2015.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2019] Siqi Liu, Guy Lever, Josh Merel, Saran Tunyasuvunakool, Nicolas
    Heess, and Thore Graepel. Emergent coordination through competition. *arXiv preprint
    arXiv:1902.07151*, 2019.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machado et al. [2018] Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel
    Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning
    environment: Evaluation protocols and open problems for general agents. *Journal
    of Artificial Intelligence Research*, 61:523–562, 2018.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makar et al. [2001] Rajbala Makar, Sridhar Mahadevan, and Mohammad Ghavamzadeh.
    Hierarchical multi-agent reinforcement learning. In *Proceedings of the Fifth
    International Conference on Autonomous Agents*, pages 246–253\. ACM, 2001.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep
    reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A
    Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K
    Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement
    learning. *Nature*, 518(7540):529, 2015.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
    Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous
    methods for deep reinforcement learning. In *International Conference on Machine
    Learning*, pages 1928–1937, 2016.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moerland et al. [2018] Thomas M Moerland, Joost Broekens, Aske Plaat, and Catholijn M
    Jonker. A0c: Alpha zero in continuous action space. *arXiv preprint arXiv:1805.09613*,
    2018.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moerland et al. [2020a] Thomas M Moerland, Joost Broekens, and Catholijn M Jonker.
    A framework for reinforcement learning and planning. *arXiv preprint arXiv:2006.15009*,
    2020a.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moerland et al. [2020b] Thomas M Moerland, Joost Broekens, and Catholijn M
    Jonker. Model-based reinforcement learning: A survey. *arXiv preprint arXiv:2006.16712*,
    2020b.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moerland et al. [2020c] Thomas M Moerland, Joost Broekens, Aske Plaat, and Catholijn M
    Jonker. The second type of uncertainty in Monte Carlo Tree Search. *arXiv preprint
    arXiv:2005.09645*, 2020c.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Montgomery and Levine [2016] William H Montgomery and Sergey Levine. Guided
    policy search via approximate mirror descent. In *Advances in Neural Information
    Processing Systems*, pages 4008–4016, 2016.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Murase et al. [1996] Yoshio Murase, Hitoshi Matsubara, and Yuzuru Hiraga. Automatic
    making of Sokoban problems. In *Pacific Rim International Conference on Artificial
    Intelligence*, pages 592–600\. Springer, 1996.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nagabandi et al. [2018] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and
    Sergey Levine. Neural network dynamics for model-based deep reinforcement learning
    with model-free fine-tuning. In *2018 IEEE International Conference on Robotics
    and Automation (ICRA)*, pages 7559–7566, 2018.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nardelli et al. [2018] Nantas Nardelli, Gabriel Synnaeve, Zeming Lin, Pushmeet
    Kohli, Philip HS Torr, and Nicolas Usunier. Value propagation networks. *arXiv
    preprint arXiv:1805.11199*, 2018.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Narvekar et al. [2020] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov,
    Matthew E Taylor, and Peter Stone. Curriculum learning for reinforcement learning
    domains: A framework and survey. *arXiv preprint arXiv:2003.04960*, 2020.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niu et al. [2018] Sufeng Niu, Siheng Chen, Hanyu Guo, Colin Targonski, Melissa C
    Smith, and Jelena Kovačević. Generalized value iteration networks: Life beyond
    lattices. In *Thirty-Second AAAI Conference on Artificial Intelligence*, 2018.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oh et al. [2015] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and
    Satinder Singh. Action-conditional video prediction using deep networks in Atari
    games. In *Advances in Neural Information Processing Systems*, pages 2863–2871,
    2015.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oh et al. [2017] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction
    network. In *Advances in Neural Information Processing Systems*, pages 6118–6128,
    2017.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. [2010] Sinno Jialin Pan, Qiang Yang, et al. A survey on transfer
    learning. *IEEE Transactions on Knowledge and Data Engineering*, 22(10):1345–1359,
    2010.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pascanu et al. [2017] Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess,
    Lars Buesing, Sebastien Racanière, David Reichert, Théophane Weber, Daan Wierstra,
    and Peter Battaglia. Learning model-based planning from scratch. *arXiv preprint
    arXiv:1707.06170*, 2017.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pearl [1984] Judea Pearl. *Heuristics: Intelligent Search Strategies for Computer
    Problem Solving*. Addison-Wesley, Reading, MA, 1984.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plaat [2020] Aske Plaat. *Learning to Play: Reinforcement Learning and Games*.
    Springer Verlag, Heidelberg, 2020.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plaat et al. [1996] Aske Plaat, Jonathan Schaeffer, Wim Pijls, and Arie De Bruin.
    Best-first fixed-depth minimax algorithms. *Artificial Intelligence*, 87(1-2):255–293,
    1996.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Polydoros and Nalpantidis [2017] Athanasios S Polydoros and Lazaros Nalpantidis.
    Survey of model-based reinforcement learning: Applications on robotics. *Journal
    of Intelligent & Robotic Systems*, 86(2):153–173, 2017.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Richards [2005] Arthur George Richards. *Robust constrained model predictive
    control*. PhD thesis, Massachusetts Institute of Technology, 2005.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Risi and Preuss [2020] Sebastian Risi and Mike Preuss. From Chess and Atari
    to StarCraft and Beyond: How Game AI is Driving the World of AI. *KI-Künstliche
    Intelligenz*, pages 1–11, 2020.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risi and Stanley [2019] Sebastian Risi and Kenneth O Stanley. Deep neuroevolution
    of recurrent and discrete world models. In *Proceedings of the Genetic and Evolutionary
    Computation Conference*, pages 456–462, 2019.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosin [2011] Christopher D Rosin. Multi-armed bandits with episode context.
    *Annals of Mathematics and Artificial Intelligence*, 61(3):203–230, 2011.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Russell and Norvig [2016] Stuart J Russell and Peter Norvig. *Artificial intelligence:
    a modern approach*. Pearson Education Limited, Malaysia, 2016.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samuel [1959] Arthur L Samuel. Some studies in machine learning using the game
    of checkers. *IBM Journal of Research and Development*, 3(3):210–229, 1959.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schaeffer et al. [1996] Jonathan Schaeffer, Robert Lake, Paul Lu, and Martin
    Bryant. Chinook, the world man-machine checkers champion. *AI Magazine*, 17(1):21,
    1996.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schleich et al. [2019] Daniel Schleich, Tobias Klamt, and Sven Behnke. Value
    iteration networks on multiple levels of abstraction. *arXiv preprint arXiv:1905.11068*,
    2019.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmidhuber and Huber [1991] Juergen Schmidhuber and Rudolf Huber. Learning
    to generate artificial fovea trajectories for target detection. *International
    Journal of Neural Systems*, 2(01–02):125–134, 1991.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmidhuber [1990a] Jürgen Schmidhuber. An on-line algorithm for dynamic reinforcement
    learning and planning in reactive environments. In *1990 IJCNN International Joint
    Conference on Neural Networks*, pages 253–258\. IEEE, 1990a.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schmidhuber [1990b] Jürgen Schmidhuber. Making the world differentiable: On
    using self-supervised fully recurrent neural networks for dynamic reinforcement
    learning and planning in non-stationary environments. 1990b.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schrittwieser et al. [2019] Julian Schrittwieser, Ioannis Antonoglou, Thomas
    Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart,
    Demis Hassabis, Thore Graepel, et al. Mastering Atari, Go, chess and shogi by
    planning with a learned model. *arXiv preprint arXiv:1911.08265*, 2019.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv preprint
    arXiv:1707.06347*, 2017.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sekar et al. [2020] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel,
    Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world
    models. *arXiv preprint arXiv:2005.05960*, 2020.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shaker et al. [2016] Noor Shaker, Julian Togelius, and Mark J Nelson. *Procedural
    content generation in games*. Springer, 2016.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silver et al. [2013] Daniel L Silver, Qiang Yang, and Lianghao Li. Lifelong
    machine learning systems: Beyond learning algorithms. In *2013 AAAI Spring Symposium
    Series*, 2013.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. [2012] David Silver, Richard S Sutton, and Martin Müller. Temporal-difference
    search in computer Go. *Machine Learning*, 87(2):183–219, 2012.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. [2016] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez,
    Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
    Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham,
    Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,
    Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks
    and tree search. *Nature*, 529(7587):484, 2016.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. [2017a] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis
    Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian
    Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den
    Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without
    human knowledge. *Nature*, 550(7676):354, 2017a.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silver et al. [2017b] David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul,
    Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz,
    Andre Barreto, et al. The predictron: End-to-end learning and planning. In *Proceedings
    of the 34th International Conference on Machine Learning*, pages 3191–3199, 2017b.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. [2018] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis
    Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran,
    Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. A general
    reinforcement learning algorithm that masters chess, shogi, and Go through self-play.
    *Science*, 362(6419):1140–1144, 2018.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srinivas et al. [2018] Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey
    Levine, and Chelsea Finn. Universal planning networks. *arXiv preprint arXiv:1804.00645*,
    2018.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton [1990] Richard S Sutton. Integrated architectures for learning, planning,
    and reacting based on approximating dynamic programming. In *Machine Learning
    Proceedings 1990*, pages 216–224\. 1990.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton [1991] Richard S Sutton. Dyna, an integrated architecture for learning,
    planning, and reacting. *ACM Sigart Bulletin*, 2(4):160–163, 1991.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. *Reinforcement
    learning, An Introduction, Second Edition*. MIT Press, 2018.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Talvitie [2015] Erik Talvitie. Agnostic system identification for monte carlo
    planning. In *Twenty-Ninth AAAI Conference on Artificial Intelligence*, 2015.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tamar et al. [2016] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter
    Abbeel. Value iteration networks. In *Adv. in Neural Information Processing Systems*,
    pages 2154–2162, 2016.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tassa et al. [2012] Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and
    stabilization of complex behaviors through online trajectory optimization. In
    *2012 IEEE/RSJ International Conference on Intelligent Robots and Systems*, pages
    4906–4913, 2012.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tassa et al. [2018] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe
    Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq,
    et al. Deepmind control suite. *arXiv preprint arXiv:1801.00690*, 2018.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tesauro [1995a] Gerald Tesauro. TD-gammon: A self-teaching backgammon program.
    In *Applications of Neural Networks*, pages 267–285\. Springer, 1995a.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tesauro [1995b] Gerald Tesauro. Temporal difference learning and TD-Gammon.
    *Communications of the ACM*, 38(3):58–68, 1995b.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tesauro [2002] Gerald Tesauro. Programming backgammon using self-teaching neural
    nets. *Artificial Intelligence*, 134(1-2):181–199, 2002.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Todorov et al. [2012] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A
    physics engine for model-based control. In *IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*, pages 5026–5033, 2012.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Togelius et al. [2013] Julian Togelius, Alex J Champandard, Pier Luca Lanzi,
    Michael Mateas, Ana Paiva, Mike Preuss, and Kenneth O Stanley. Procedural content
    generation: Goals, challenges and actionable steps. Schloss Dagstuhl-Leibniz-Zentrum
    fuer Informatik, 2013.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torrado et al. [2018] Ruben Rodriguez Torrado, Philip Bontrager, Julian Togelius,
    Jialin Liu, and Diego Perez-Liebana. Deep reinforcement learning for general video
    game ai. In *2018 IEEE Conference on Computational Intelligence and Games (CIG)*,
    pages 1–8\. IEEE, 2018.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals et al. [2017] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev,
    Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler,
    John Agapiou, Julian Schrittwieser, et al. Starcraft II: A new challenge for reinforcement
    learning. *arXiv preprint arXiv:1708.04782*, 2017.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vinyals et al. [2019] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki,
    Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell,
    Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using multi-agent
    reinforcement learning. *Nature*, 575(7782):350–354, 2019.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volz et al. [2018] Vanessa Volz, Jacob Schrum, Jialin Liu, Simon M Lucas, Adam
    Smith, and Sebastian Risi. Evolving mario levels in the latent space of a deep
    convolutional generative adversarial network. In *Proceedings of the Genetic and
    Evolutionary Computation Conference*, pages 221–228, 2018.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2019a] Hui Wang, Michael Emmerich, Mike Preuss, and Aske Plaat.
    Alternative loss functions in AlphaZero-like self-play. In *2019 IEEE Symposium
    Series on Computational Intelligence (SSCI)*, pages 155–162, 2019a.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2020] Hui Wang, Mike Preuss, Michael Emmerich, and Aske Plaat.
    Tackling Morpion Solitaire with AlphaZero-like Ranked Reward reinforcement learning.
    *arXiv preprint arXiv:2006.07970*, 2020.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2019b] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang,
    Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy
    Ba. Benchmarking model-based reinforcement learning. *preprint arXiv:1907.02057*,
    2019b.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watkins [1989] Christopher JCH Watkins. *Learning from delayed rewards*. PhD
    thesis, King’s College, Cambridge, 1989.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weber et al. [2017] Théophane Weber, Sébastien Racanière, David Reichert, Lars
    Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdomenech Badia, Oriol
    Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented agents for deep
    reinforcement learning. In *Advances in Neural Information Processing Systems*,
    pages 5690–5701, 2017.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weng [2018] Lilian Weng. Meta-learning: Learning to learn fast. Lil’Log https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html,
    November 2018.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Werbos [1988] Paul J Werbos. Generalization of backpropagation with application
    to a recurrent gas market model. *Neural Networks*, 1(4):339–356, 1988.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xian et al. [2017] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot
    learning—the good, the bad and the ugly. In *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, pages 4582–4591, 2017.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xingjian et al. [2015] SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung,
    Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning
    approach for precipitation nowcasting. In *Advances in Neural Information Processing
    Systems*, pages 802–810, 2015.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2020] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol
    Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation
    for multi-task and meta reinforcement learning. In *Conference on Robot Learning*,
    pages 1094–1100, 2020.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zambaldi et al. [2018] Vinicius Zambaldi, David Raposo, Adam Santoro, Victor
    Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap,
    Edward Lockhart, et al. Relational deep reinforcement learning. *arXiv preprint
    arXiv:1806.01830*, 2018.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou and Dovier [2013] Neng-Fa Zhou and Agostino Dovier. A tabled Prolog program
    for solving Sokoban. *Fundamenta Informaticae*, 124(4):561–575, 2013.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
