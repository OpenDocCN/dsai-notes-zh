- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:07:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024年9月6日 20:07:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1809.02165] Deep Learning for Generic Object Detection: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1809.02165] 深度学习在通用目标检测中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1809.02165](https://ar5iv.labs.arxiv.org/html/1809.02165)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1809.02165](https://ar5iv.labs.arxiv.org/html/1809.02165)
- en: ∎
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: '¹¹institutetext: ✉ Li Liu (li.liu@oulu.fi)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹institutetext: ✉ Li Liu (li.liu@oulu.fi)'
- en: Wanli Ouyang (wanli.ouyang@sydney.edu.au)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Wanli Ouyang (wanli.ouyang@sydney.edu.au)
- en: Xiaogang Wang (xgwang@ee.cuhk.edu.hk)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Xiaogang Wang (xgwang@ee.cuhk.edu.hk)
- en: Paul Fieguth (pfieguth@uwaterloo.ca)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Paul Fieguth (pfieguth@uwaterloo.ca)
- en: Jie Chen (jie.chen@oulu.fi)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Jie Chen (jie.chen@oulu.fi)
- en: Xinwang Liu (xinwangliu@nudt.edu.cn)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Xinwang Liu (xinwangliu@nudt.edu.cn)
- en: Matti Pietikäinen (matti.pietikainen@oulu.fi)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Matti Pietikäinen (matti.pietikainen@oulu.fi)
- en: 1 National University of Defense Technology, China
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 中国国防科技大学
- en: 2 University of Oulu, Finland
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 芬兰奥卢大学
- en: 3 University of Sydney, Australia
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 澳大利亚悉尼大学
- en: 4 Chinese University of Hong Kong, China
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 中国香港中文大学
- en: 5 University of Waterloo, Canada
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 加拿大滑铁卢大学
- en: 'Deep Learning for Generic Object Detection: A Survey'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在通用目标检测中的应用：综述
- en: 'Li Liu ^(1,2)    Wanli Ouyang ³    Xiaogang Wang ⁴    Paul Fieguth ⁵    Jie
    Chen ²    Xinwang Liu ¹    Matti Pietikäinen ²(Received: 12 September 2018 )'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Li Liu ^(1,2)    Wanli Ouyang ³    Xiaogang Wang ⁴    Paul Fieguth ⁵    Jie
    Chen ²    Xinwang Liu ¹    Matti Pietikäinen ²(收到日期：2018年9月12日)
- en: Abstract
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Object detection, one of the most fundamental and challenging problems in computer
    vision, seeks to locate object instances from a large number of predefined categories
    in natural images. Deep learning techniques have emerged as a powerful strategy
    for learning feature representations directly from data and have led to remarkable
    breakthroughs in the field of generic object detection. Given this period of rapid
    evolution, the goal of this paper is to provide a comprehensive survey of the
    recent achievements in this field brought about by deep learning techniques. More
    than 300 research contributions are included in this survey, covering many aspects
    of generic object detection: detection frameworks, object feature representation,
    object proposal generation, context modeling, training strategies, and evaluation
    metrics. We finish the survey by identifying promising directions for future research.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是计算机视觉中最基本且最具挑战性的问题之一，它旨在从大量预定义类别中定位自然图像中的目标实例。深度学习技术作为一种强大的策略，通过直接从数据中学习特征表示，在通用目标检测领域取得了显著突破。鉴于这一快速发展的时期，本文的目标是提供对深度学习技术在该领域所带来的最新成就的全面综述。本文综述包含了300多项研究贡献，涵盖了通用目标检测的多个方面：检测框架、目标特征表示、目标提议生成、上下文建模、训练策略和评估指标。我们通过确定未来研究的有前景方向来结束综述。
- en: 'Keywords:'
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Object detection deep learning convolutional neural networks object recognition
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测 深度学习 卷积神经网络 目标识别
- en: 'papertype:'
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 论文类型：
- en: generic article
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通用文章
- en: 1 Introduction
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'As a longstanding, fundamental and challenging problem in computer vision,
    object detection (illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Deep Learning for Generic Object Detection: A Survey")) has been an active area
    of research for several decades Fischler and Elschlager ([1973](#bib.bib76)).
    The goal of object detection is to determine whether there are any instances of
    objects from given categories (such as humans, cars, bicycles, dogs or cats) in
    an image and, if present, to return the spatial location and extent of each object
    instance (*e.g.,* via a bounding box Everingham et al. ([2010](#bib.bib68)); Russakovsky
    et al. ([2015](#bib.bib234))). As the cornerstone of image understanding and computer
    vision, object detection forms the basis for solving complex or high level vision
    tasks such as segmentation, scene understanding, object tracking, image captioning,
    event detection, and activity recognition. Object detection supports a wide range
    of applications, including robot vision, consumer electronics, security, autonomous
    driving, human computer interaction, content based image retrieval, intelligent
    video surveillance, and augmented reality.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作为计算机视觉中的一个长期存在、基础且具有挑战性的问题，目标检测（如图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 通用目标检测的深度学习：综述")
    所示）已经成为活跃的研究领域数十年。目标检测的目标是确定图像中是否存在来自给定类别（如人类、汽车、自行车、狗或猫）的任何物体实例，如果存在，则返回每个物体实例的空间位置和范围（*例如*，通过边界框
    Everingham 等 ([2010](#bib.bib68)); Russakovsky 等 ([2015](#bib.bib234)))。作为图像理解和计算机视觉的基石，目标检测为解决复杂或高级视觉任务如分割、场景理解、目标跟踪、图像描述、事件检测和活动识别奠定了基础。目标检测支持广泛的应用，包括机器人视觉、消费电子、安保、自动驾驶、人机交互、基于内容的图像检索、智能视频监控和增强现实。
- en: 'Recently, deep learning techniques Hinton and Salakhutdinov ([2006](#bib.bib105));
    LeCun et al. ([2015](#bib.bib149)) have emerged as powerful methods for learning
    feature representations automatically from data. In particular, these techniques
    have provided major improvements in object detection, as illustrated in Fig. [3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Generic Object Detection: A Survey").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习技术 Hinton 和 Salakhutdinov ([2006](#bib.bib105)); LeCun 等 ([2015](#bib.bib149))
    已成为从数据中自动学习特征表示的强大方法。特别是，这些技术在目标检测中提供了重大改进，如图 [3](#S1.F3 "图 3 ‣ 1 引言 ‣ 通用目标检测的深度学习：综述")
    所示。
- en: 'As illustrated in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning
    for Generic Object Detection: A Survey"), object detection can be grouped into
    one of two types Grauman and Leibe ([2011](#bib.bib91)); Zhang et al. ([2013](#bib.bib310)):
    detection of specific instances versus the detection of broad categories. The
    first type aims to detect instances of a particular object (such as Donald Trump’s
    face, the Eiffel Tower, or a neighbor’s dog), essentially a matching problem.
    The goal of the second type is to detect (usually previously unseen) instances
    of some predefined object categories (for example humans, cars, bicycles, and
    dogs). Historically, much of the effort in the field of object detection has focused
    on the detection of a single category (typically faces and pedestrians) or a few
    specific categories. In contrast, over the past several years, the research community
    has started moving towards the more challenging goal of building general purpose
    object detection systems where the breadth of object detection ability rivals
    that of humans.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 通用目标检测的深度学习：综述") 所示，目标检测可以分为两种类型 Grauman 和 Leibe
    ([2011](#bib.bib91)); Zhang 等 ([2013](#bib.bib310))：特定实例检测与广泛类别检测。第一种类型旨在检测特定物体的实例（例如特朗普的脸、埃菲尔铁塔或邻居的狗），本质上是一个匹配问题。第二种类型的目标是检测一些预定义物体类别（例如人类、汽车、自行车和狗）的实例（通常是以前未见过的）。历史上，目标检测领域的大部分努力集中在检测单一类别（通常是人脸和行人）或少数特定类别上。相比之下，近年来，研究界已经开始向建立通用目标检测系统的更具挑战性的目标迈进，其中目标检测能力的广度与人类相匹敌。
- en: '![Refer to caption](img/77270d7427029339740841ef83511de6.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/77270d7427029339740841ef83511de6.png)'
- en: 'Figure 1: Most frequent keywords in ICCV and CVPR conference papers from 2016
    to 2018\. The size of each word is proportional to the frequency of that keyword.
    We can see that object detection has received significant attention in recent
    years.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：2016 至 2018 年 ICCV 和 CVPR 会议论文中最常出现的关键词。每个词的大小与该关键词的频率成正比。我们可以看到，目标检测近年来受到了显著关注。
- en: '![Refer to caption](img/31a7749bc072863b628f977300d6a515.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/31a7749bc072863b628f977300d6a515.png)'
- en: 'Figure 2: Object detection includes localizing instances of a particular object
    (top), as well as generalizing to detecting object categories in general (bottom).
    This survey focuses on recent advances for the latter problem of generic object
    detection.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：物体检测包括定位特定对象的实例（上图），以及对一般物体类别进行检测（下图）。本调查重点关注后者，即通用物体检测的最新进展。
- en: '![Refer to caption](img/3eaa1827c639fd590639df5d9db252a5.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/3eaa1827c639fd590639df5d9db252a5.png)'
- en: 'Figure 3: An overview of recent object detection performance: We can observe
    a significant improvement in performance (measured as mean average precision)
    since the arrival of deep learning in 2012\. (a) Detection results of winning
    entries in the VOC2007-2012 competitions, and (b) Top object detection competition
    results in ILSVRC2013-2017 (results in both panels use only the provided training
    data).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：近期物体检测性能概览：我们可以观察到自2012年深度学习出现以来，性能（以平均精度作为度量）显著提高。（a）VOC2007-2012竞赛中获胜条目的检测结果，以及（b）ILSVRC2013-2017中的顶级物体检测竞赛结果（两面板中的结果仅使用提供的训练数据）。
- en: '![Refer to caption](img/d4ff3f0a25c0971468d77d361d309f76.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/d4ff3f0a25c0971468d77d361d309f76.png)'
- en: 'Figure 4: Milestones of object detection and recognition, including feature
    representations Csurka et al. ([2004](#bib.bib47)); Dalal and Triggs ([2005](#bib.bib52));
    He et al. ([2016](#bib.bib101)); Krizhevsky et al. ([2012a](#bib.bib140)); Lazebnik
    et al. ([2006](#bib.bib147)); Lowe ([1999](#bib.bib178), [2004](#bib.bib179));
    Perronnin et al. ([2010](#bib.bib212)); Simonyan and Zisserman ([2015](#bib.bib248));
    Sivic and Zisserman ([2003](#bib.bib252)); Szegedy et al. ([2015](#bib.bib263));
    Viola and Jones ([2001](#bib.bib276)); Wang et al. ([2009](#bib.bib279)), detection
    frameworks Felzenszwalb et al. ([2010b](#bib.bib74)); Girshick et al. ([2014](#bib.bib85));
    Sermanet et al. ([2014](#bib.bib239)); Uijlings et al. ([2013](#bib.bib271));
    Viola and Jones ([2001](#bib.bib276)), and datasets Everingham et al. ([2010](#bib.bib68));
    Lin et al. ([2014](#bib.bib166)); Russakovsky et al. ([2015](#bib.bib234)). The
    time period up to 2012 is dominated by handcrafted features, a transition took
    place in 2012 with the development of DCNNs for image classification by Krizhevsky
    *et al.* Krizhevsky et al. ([2012a](#bib.bib140)), with methods after 2012 dominated
    by related deep networks. Mostof the listed methods are highly cited and won a
    major ICCV or CVPR prize. See Section [2.3](#S2.SS3 "2.3 Progress in the Past
    Two Decades ‣ 2 Generic Object Detection ‣ Deep Learning for Generic Object Detection:
    A Survey") for details.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：物体检测和识别的里程碑，包括特征表示Csurka等（[2004](#bib.bib47)）；Dalal和Triggs（[2005](#bib.bib52)）；He等（[2016](#bib.bib101)）；Krizhevsky等（[2012a](#bib.bib140)）；Lazebnik等（[2006](#bib.bib147)）；Lowe（[1999](#bib.bib178)，[2004](#bib.bib179)）；Perronnin等（[2010](#bib.bib212)）；Simonyan和Zisserman（[2015](#bib.bib248)）；Sivic和Zisserman（[2003](#bib.bib252)）；Szegedy等（[2015](#bib.bib263)）；Viola和Jones（[2001](#bib.bib276)）；Wang等（[2009](#bib.bib279)），检测框架Felzenszwalb等（[2010b](#bib.bib74)）；Girshick等（[2014](#bib.bib85)）；Sermanet等（[2014](#bib.bib239)）；Uijlings等（[2013](#bib.bib271)）；Viola和Jones（[2001](#bib.bib276)），以及数据集Everingham等（[2010](#bib.bib68)）；Lin等（[2014](#bib.bib166)）；Russakovsky等（[2015](#bib.bib234)）。截至2012年的时间段主要由手工特征主导，2012年随着Krizhevsky等（[2012a](#bib.bib140)）开发的用于图像分类的DCNN的出现，发生了转变，2012年后的方法则由相关深度网络主导。大多数列举的方法被广泛引用，并赢得了主要的ICCV或CVPR奖项。详细信息见第[2.3](#S2.SS3
    "2.3 过去二十年的进展 ‣ 2 通用物体检测 ‣ 通用物体检测的深度学习：调查")节。
- en: 'In 2012, Krizhevsky *et al.* Krizhevsky et al. ([2012a](#bib.bib140)) proposed
    a Deep Convolutional Neural Network (DCNN) called AlexNet which achieved record
    breaking image classification accuracy in the Large Scale Visual Recognition Challenge
    (ILSVRC) Russakovsky et al. ([2015](#bib.bib234)). Since that time, the research
    focus in most aspects of computer vision has been specifically on deep learning
    methods, indeed including the domain of generic object detection Girshick et al.
    ([2014](#bib.bib85)); He et al. ([2014](#bib.bib99)); Girshick ([2015](#bib.bib84));
    Sermanet et al. ([2014](#bib.bib239)); Ren et al. ([2017a](#bib.bib230)). Although
    tremendous progress has been achieved, illustrated in Fig. [3](#S1.F3 "Figure
    3 ‣ 1 Introduction ‣ Deep Learning for Generic Object Detection: A Survey"), we
    are unaware of comprehensive surveys of this subject over the past five years.
    Given the exceptionally rapid rate of progress, this article attempts to track
    recent advances and summarize their achievements in order to gain a clearer picture
    of the current panorama in generic object detection.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '2012年，Krizhevsky *等* ([2012a](#bib.bib140)) 提出了一个名为AlexNet的深度卷积神经网络（DCNN），该网络在大规模视觉识别挑战赛（ILSVRC）中取得了破纪录的图像分类准确率
    Russakovsky等 ([2015](#bib.bib234))。自那时以来，大多数计算机视觉领域的研究重点都专注于深度学习方法，包括通用目标检测领域
    Girshick等 ([2014](#bib.bib85))；He等 ([2014](#bib.bib99))；Girshick ([2015](#bib.bib84))；Sermanet等
    ([2014](#bib.bib239))；Ren等 ([2017a](#bib.bib230))。尽管取得了巨大进展，如图[3](#S1.F3 "Figure
    3 ‣ 1 Introduction ‣ Deep Learning for Generic Object Detection: A Survey")所示，但我们尚未发现过去五年内对此主题的全面调查。鉴于进展的极快速度，本文试图跟踪近期的进展，并总结其成就，以便更清晰地了解当前的通用目标检测全景。'
- en: 'Table 1: Summary of related object detection surveys since 2000.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：自2000年以来相关目标检测综述的总结
- en: '|      No. | Survey Title | Ref. | Year | Venue | Content    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|      No. | 综述标题 | 参考文献 | 年份 | 会议 | 内容    |'
- en: '|    1 | Monocular Pedestrian Detection: Survey and Experiments | Enzweiler
    and Gavrila ([2009](#bib.bib66)) | 2009 | PAMI | An evaluation of three pedestrian
    detectors    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|    1 | 单目行人检测：综述与实验 | Enzweiler 和 Gavrila ([2009](#bib.bib66)) | 2009 | PAMI
    | 三种行人检测器的评估    |'
- en: '|    2 | Survey of Pedestrian Detection for Advanced Driver Assistance Systems
    | Geronimo et al. ([2010](#bib.bib79)) | 2010 | PAMI | A survey of pedestrian
    detection for advanced driver assistance systems    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|    2 | 高级驾驶辅助系统中的行人检测综述 | Geronimo等 ([2010](#bib.bib79)) | 2010 | PAMI |
    高级驾驶辅助系统中行人检测的综述    |'
- en: '|    3 | Pedestrian Detection: An Evaluation of the State of The Art | Dollar
    et al. ([2012](#bib.bib59)) | 2012 | PAMI | A thorough and detailed evaluation
    of detectors in monocular images    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|    3 | 行人检测：技术现状评估 | Dollar等 ([2012](#bib.bib59)) | 2012 | PAMI | 单目图像中检测器的全面详细评估
       |'
- en: '|    4 | Detecting Faces in Images: A Survey | Yang et al. ([2002](#bib.bib294))
    | 2002 | PAMI | First survey of face detection from a single image    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|    4 | 图像中的面部检测：综述 | Yang等 ([2002](#bib.bib294)) | 2002 | PAMI | 单张图像面部检测的首个综述
       |'
- en: '|    5 | A Survey on Face Detection in the Wild: Past, Present and Future |
    Zafeiriou et al. ([2015](#bib.bib301)) | 2015 | CVIU | A survey of face detection
    in the wild since 2000    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|    5 | 野外面部检测综述：过去、现在与未来 | Zafeiriou等 ([2015](#bib.bib301)) | 2015 | CVIU
    | 自2000年以来野外面部检测的综述    |'
- en: '|    6 | On Road Vehicle Detection: A Review | Sun et al. ([2006](#bib.bib258))
    | 2006 | PAMI | A review of vision based on-road vehicle detection systems   
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|    6 | 公路车辆检测：综述 | Sun等 ([2006](#bib.bib258)) | 2006 | PAMI | 基于视觉的公路车辆检测系统的综述
       |'
- en: '|    7 | Text Detection and Recognition in Imagery: A Survey | Ye and Doermann
    ([2015](#bib.bib295)) | 2015 | PAMI | A survey of text detection and recognition
    in color imagery    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|    7 | 图像中的文本检测与识别：综述 | Ye 和 Doermann ([2015](#bib.bib295)) | 2015 | PAMI
    | 关于彩色图像中文本检测与识别的综述    |'
- en: '|    8 | Toward Category Level Object Recognition | Ponce et al. ([2007](#bib.bib215))
    | 2007 | Book | Representative papers on object categorization, detection, and
    segmentation    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|    8 | 面向类别级别的目标识别 | Ponce等 ([2007](#bib.bib215)) | 2007 | 书籍 | 关于物体分类、检测和分割的代表性论文
       |'
- en: '|    9 | The Evolution of Object Categorization and the Challenge of Image
    Abstraction | Dickinson et al. ([2009](#bib.bib56)) | 2009 | Book | A trace of
    the evolution of object categorization over four decades    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|    9 | 物体分类的演变与图像抽象的挑战 | Dickinson等 ([2009](#bib.bib56)) | 2009 | 书籍 | 追踪四十年来物体分类的演变
       |'
- en: '|    10 | Context based Object Categorization: A Critical Survey | Galleguillos
    and Belongie ([2010](#bib.bib78)) | 2010 | CVIU | A review of contextual information
    for object categorization    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|    10 | 基于上下文的目标分类：批判性调查 | Galleguillos 和 Belongie ([2010](#bib.bib78)) |
    2010 | CVIU | 目标分类的上下文信息综述    |'
- en: '|    11 | 50 Years of Object Recognition: Directions Forward | Andreopoulos
    and Tsotsos ([2013](#bib.bib5)) | 2013 | CVIU | A review of the evolution of object
    recognition systems over five decades    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|    11 | 50年的目标识别：未来方向 | Andreopoulos 和 Tsotsos ([2013](#bib.bib5)) | 2013
    | CVIU | 目标识别系统在五十年中的演变综述    |'
- en: '|    12 | Visual Object Recognition | Grauman and Leibe ([2011](#bib.bib91))
    | 2011 | Tutorial | Instance and category object recognition techniques    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|    12 | 视觉目标识别 | Grauman 和 Leibe ([2011](#bib.bib91)) | 2011 | 教程 | 实例和类别目标识别技术
       |'
- en: '|    13 | Object Class Detection: A Survey | Zhang et al. ([2013](#bib.bib310))
    | 2013 | ACM CS | Survey of generic object detection methods before 2011    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|    13 | 目标类别检测：调查 | Zhang 等人 ([2013](#bib.bib310)) | 2013 | ACM CS | 2011年前通用目标检测方法的综述
       |'
- en: '|    14 | Feature Representation for Statistical Learning based Object Detection:
    A Review | Li et al. ([2015b](#bib.bib160)) | 2015 | PR | Feature representation
    methods in statistical learning based object detection, including handcrafted
    and deep learning based features    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|    14 | 基于统计学习的目标检测的特征表示：综述 | Li 等人 ([2015b](#bib.bib160)) | 2015 | PR |
    统计学习基于的目标检测中的特征表示方法，包括手工设计和深度学习特征    |'
- en: '|    15 | Salient Object Detection: A Survey | Borji et al. ([2014](#bib.bib19))
    | 2014 | arXiv | A survey for salient object detection    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|    15 | 显著目标检测：综述 | Borji 等人 ([2014](#bib.bib19)) | 2014 | arXiv | 显著目标检测的综述
       |'
- en: '|    16 | Representation Learning: A Review and New Perspectives | Bengio et al.
    ([2013](#bib.bib13)) | 2013 | PAMI | Unsupervised feature learning and deep learning,
    probabilistic models, autoencoders, manifold learning, and deep networks    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|    16 | 表示学习：综述与新视角 | Bengio 等人 ([2013](#bib.bib13)) | 2013 | PAMI | 无监督特征学习和深度学习、概率模型、自编码器、流形学习以及深度网络
       |'
- en: '|    17 | Deep Learning | LeCun et al. ([2015](#bib.bib149)) | 2015 | Nature
    | An introduction to deep learning and applications    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|    17 | 深度学习 | LeCun 等人 ([2015](#bib.bib149)) | 2015 | Nature | 深度学习及其应用的介绍
       |'
- en: '|    18 | A Survey on Deep Learning in Medical Image Analysis | Litjens et al.
    ([2017](#bib.bib170)) | 2017 | MIA | A survey of deep learning for image classification,
    object detection, segmentation and registration in medical image analysis    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|    18 | 深度学习在医学图像分析中的调查 | Litjens 等人 ([2017](#bib.bib170)) | 2017 | MIA |
    深度学习在医学图像分类、目标检测、分割和配准中的调查    |'
- en: '|    19 | Recent Advances in Convolutional Neural Networks | Gu et al. ([2017](#bib.bib92))
    | 2017 | PR | A broad survey of the recent advances in CNN and its applications
    in computer vision, speech and natural language processing    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|    19 | 卷积神经网络的最新进展 | Gu 等人 ([2017](#bib.bib92)) | 2017 | PR | 关于CNN及其在计算机视觉、语音和自然语言处理中的应用的广泛综述
       |'
- en: '|    20 | Tutorial: Tools for Efficient Object Detection | $-$ | 2015 | ICCV15
    | A short course for object detection only covering recent milestones    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|    20 | 教程：高效目标检测工具 | $-$ | 2015 | ICCV15 | 一门简短的课程，仅涵盖目标检测的最新进展    |'
- en: '|    21 | Tutorial: Deep Learning for Objects and Scenes | $-$ | 2017 | CVPR17
    | A high level summary of recent work on deep learning for visual recognition
    of objects and scenes    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|    21 | 教程：目标与场景的深度学习 | $-$ | 2017 | CVPR17 | 关于目标和场景视觉识别的深度学习近期工作的高级总结   
    |'
- en: '|    22 | Tutorial: Instance Level Recognition | $-$ | 2017 | ICCV17 | A short
    course of recent advances on instance level recognition, including object detection,
    instance segmentation and human pose prediction    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|    22 | 教程：实例级别识别 | $-$ | 2017 | ICCV17 | 关于实例级别识别的最新进展的简短课程，包括目标检测、实例分割和人体姿态预测
       |'
- en: '|    23 | Tutorial: Visual Recognition and Beyond | $-$ | 2018 | CVPR18 | A
    tutorial on methods and principles behind image classification, object detection,
    instance segmentation, and semantic segmentation.    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|    23 | 教程：视觉识别及其延伸 | $-$ | 2018 | CVPR18 | 关于图像分类、目标检测、实例分割和语义分割的原理和方法的教程
       |'
- en: '|    24 | Deep Learning for Generic Object Detection | Ours | 2019 | VISI |
    A comprehensive survey of deep learning for generic object detection    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|    24 | 通用目标检测的深度学习 | 我们 | 2019 | VISI | 关于通用目标检测的深度学习的全面综述    |'
- en: '|      |  |  |  |  |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|      |  |  |  |  |  |'
- en: 1.1 Comparison with Previous Reviews
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 与之前综述的比较
- en: 'Many notable object detection surveys have been published, as summarized in
    Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep Learning for Generic Object
    Detection: A Survey"). These include many excellent surveys on the problem of
    specific object detection, such as pedestrian detection Enzweiler and Gavrila
    ([2009](#bib.bib66)); Geronimo et al. ([2010](#bib.bib79)); Dollar et al. ([2012](#bib.bib59)),
    face detection Yang et al. ([2002](#bib.bib294)); Zafeiriou et al. ([2015](#bib.bib301)),
    vehicle detection Sun et al. ([2006](#bib.bib258)) and text detection Ye and Doermann
    ([2015](#bib.bib295)). There are comparatively few recent surveys focusing directly
    on the problem of generic object detection, except for the work by Zhang *et al.*
    Zhang et al. ([2013](#bib.bib310)) who conducted a survey on the topic of object
    class detection. However, the research reviewed in Grauman and Leibe ([2011](#bib.bib91)),
    Andreopoulos and Tsotsos ([2013](#bib.bib5)) and Zhang et al. ([2013](#bib.bib310))
    is mostly pre-2012, and therefore prior to the recent striking success and dominance
    of deep learning and related methods.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '许多著名的目标检测综述已经出版，如表[1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep Learning for
    Generic Object Detection: A Survey")中总结的。这些包括了许多针对特定目标检测问题的优秀综述，如行人检测 Enzweiler
    和 Gavrila ([2009](#bib.bib66))；Geronimo et al. ([2010](#bib.bib79))；Dollar et
    al. ([2012](#bib.bib59))，人脸检测 Yang et al. ([2002](#bib.bib294))；Zafeiriou et al.
    ([2015](#bib.bib301))，车辆检测 Sun et al. ([2006](#bib.bib258)) 和文本检测 Ye 和 Doermann
    ([2015](#bib.bib295))。近年来，直接关注通用目标检测问题的综述相对较少，除了 Zhang *等* Zhang et al. ([2013](#bib.bib310))
    对目标类别检测进行的综述。然而，Grauman 和 Leibe ([2011](#bib.bib91))、Andreopoulos 和 Tsotsos ([2013](#bib.bib5))
    和 Zhang et al. ([2013](#bib.bib310)) 评审的研究大多是2012年前的，因此在深度学习及相关方法最近的显著成功和主导之前。'
- en: Deep learning allows computational models to learn fantastically complex, subtle,
    and abstract representations, driving significant progress in a broad range of
    problems such as visual recognition, object detection, speech recognition, natural
    language processing, medical image analysis, drug discovery and genomics. Among
    different types of deep neural networks, DCNNs LeCun et al. ([1998](#bib.bib148));
    Krizhevsky et al. ([2012a](#bib.bib140)); LeCun et al. ([2015](#bib.bib149)) have
    brought about breakthroughs in processing images, video, speech and audio. To
    be sure, there have been many published surveys on deep learning, including that
    of Bengio *et al.* Bengio et al. ([2013](#bib.bib13)), LeCun *et al.* LeCun et al.
    ([2015](#bib.bib149)), Litjens *et al.* Litjens et al. ([2017](#bib.bib170)),
    Gu *et al.* Gu et al. ([2017](#bib.bib92)), and more recently in tutorials at
    ICCV and CVPR.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习使计算模型能够学习极其复杂、微妙和抽象的表征，在视觉识别、目标检测、语音识别、自然语言处理、医学图像分析、药物发现和基因组学等广泛问题中推动了显著进展。在不同类型的深度神经网络中，DCNNs
    LeCun et al. ([1998](#bib.bib148)); Krizhevsky et al. ([2012a](#bib.bib140));
    LeCun et al. ([2015](#bib.bib149)) 在处理图像、视频、语音和音频方面带来了突破性进展。可以肯定的是，已经有许多关于深度学习的发表综述，包括
    Bengio *等* Bengio et al. ([2013](#bib.bib13))，LeCun *等* LeCun et al. ([2015](#bib.bib149))，Litjens
    *等* Litjens et al. ([2017](#bib.bib170))，Gu *等* Gu et al. ([2017](#bib.bib92))，以及最近在
    ICCV 和 CVPR 的教程中。
- en: In contrast, although many deep learning based methods have been proposed for
    object detection, we are unaware of any comprehensive recent survey. A thorough
    review and summary of existing work is essential for further progress in object
    detection, particularly for researchers wishing to enter the field. Since our
    focus is on generic object detection, the extensive work on DCNNs for specific
    object detection, such as face detection Li et al. ([2015a](#bib.bib154)); Zhang
    et al. ([2016a](#bib.bib306)); Hu and Ramanan ([2017](#bib.bib116)), pedestrian
    detection Zhang et al. ([2016b](#bib.bib307)); Hosang et al. ([2015](#bib.bib109)),
    vehicle detection Zhou et al. ([2016b](#bib.bib322)) and traffic sign detection
    Zhu et al. ([2016b](#bib.bib329)) will not be considered.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，尽管已经提出了许多基于深度学习的方法用于目标检测，但我们尚未发现任何全面的近期综述。对现有工作的彻底回顾和总结对目标检测的进一步进展至关重要，特别是对于希望进入该领域的研究人员。由于我们关注的是通用目标检测，因此对于特定目标检测（如人脸检测
    Li et al. ([2015a](#bib.bib154)); Zhang et al. ([2016a](#bib.bib306)); Hu and
    Ramanan ([2017](#bib.bib116))，行人检测 Zhang et al. ([2016b](#bib.bib307)); Hosang
    et al. ([2015](#bib.bib109))，车辆检测 Zhou et al. ([2016b](#bib.bib322)) 和交通标志检测 Zhu
    et al. ([2016b](#bib.bib329))）的广泛工作将不予考虑。
- en: '![Refer to caption](img/7cdd6ca3e6cc55d8284873f62fad9eba.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7cdd6ca3e6cc55d8284873f62fad9eba.png)'
- en: 'Figure 5: Recognition problems related to generic object detection: (a) Image
    level object classification, (b) Bounding box level generic object detection,
    (c) Pixel-wise semantic segmentation, (d) Instance level semantic segmentation.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：与通用目标检测相关的识别问题：（a）图像级目标分类，（b）边界框级通用目标检测，（c）像素级语义分割，（d）实例级语义分割。
- en: 1.2 Scope
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 范围
- en: 'The number of papers on generic object detection based on deep learning is
    breathtaking. There are so many, in fact, that compiling any comprehensive review
    of the state of the art is beyond the scope of any reasonable length paper. As
    a result, it is necessary to establish selection criteria, in such a way that
    we have limited our focus to top journal and conference papers. Due to these limitations,
    we sincerely apologize to those authors whose works are not included in this paper.
    For surveys of work on related topics, readers are referred to the articles in
    Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Deep Learning for Generic Object
    Detection: A Survey"). This survey focuses on major progress of the last five
    years, and we restrict our attention to still pictures, leaving the important
    subject of video object detection as a topic for separate consideration in the
    future.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '基于深度学习的通用目标检测论文数量令人惊叹。实际上，它们如此之多，以至于编写任何全面的前沿综述超出了合理长度论文的范围。因此，有必要建立选择标准，以便我们将重点限制在顶级期刊和会议论文上。由于这些限制，我们对未被纳入本文的作者的作品深表歉意。有关相关主题工作的综述，请参见表格中的文章[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Deep Learning for Generic Object Detection: A Survey")。本综述主要关注过去五年的重大进展，并将注意力限制在静态图片上，将视频目标检测这一重要主题留待未来单独考虑。'
- en: The main goal of this paper is to offer a comprehensive survey of deep learning
    based generic object detection techniques, and to present some degree of taxonomy,
    a high level perspective and organization, primarily on the basis of popular datasets,
    evaluation metrics, context modeling, and detection proposal methods. The intention
    is that our categorization be helpful for readers to have an accessible understanding
    of similarities and differences between a wide variety of strategies. The proposed
    taxonomy gives researchers a framework to understand current research and to identify
    open challenges for future research.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要目标是提供一个关于深度学习基础的通用目标检测技术的全面综述，并呈现一定程度的分类法、高层次视角和组织，主要基于流行的数据集、评估指标、上下文建模和检测提议方法。我们的意图是使我们的分类有助于读者便捷地理解各种策略之间的相似性和差异。提出的分类法为研究人员提供了一个理解当前研究并识别未来研究开放挑战的框架。
- en: 'The remainder of this paper is organized as follows. Related background and
    the progress made during the last two decades are summarized in Section [2](#S2
    "2 Generic Object Detection ‣ Deep Learning for Generic Object Detection: A Survey").
    A brief introduction to deep learning is given in Section [3](#S3 "3 A Brief Introduction
    to Deep Learning ‣ Deep Learning for Generic Object Detection: A Survey"). Popular
    datasets and evaluation criteria are summarized in Section [4](#S4 "4 Datasets
    and Performance Evaluation ‣ Deep Learning for Generic Object Detection: A Survey").
    We describe the milestone object detection frameworks in Section [5](#S5 "5 Detection
    Frameworks ‣ Deep Learning for Generic Object Detection: A Survey"). From Section
    [6](#S6 "6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey") to Section [9](#S9 "9 Other Issues ‣ Deep Learning for Generic Object
    Detection: A Survey"), fundamental sub-problems and the relevant issues involved
    in designing object detectors are discussed. Finally, in Section [10](#S10 "10
    Discussion and Conclusion ‣ Deep Learning for Generic Object Detection: A Survey"),
    we conclude the paper with an overall discussion of object detection, state-of-the-
    art performance, and future research directions.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分组织如下。第 [2](#S2 "2 Generic Object Detection ‣ Deep Learning for Generic
    Object Detection: A Survey") 节总结了相关背景和过去二十年的进展。第 [3](#S3 "3 A Brief Introduction
    to Deep Learning ‣ Deep Learning for Generic Object Detection: A Survey") 节简要介绍了深度学习。第
    [4](#S4 "4 Datasets and Performance Evaluation ‣ Deep Learning for Generic Object
    Detection: A Survey") 节总结了流行的数据集和评估标准。第 [5](#S5 "5 Detection Frameworks ‣ Deep
    Learning for Generic Object Detection: A Survey") 节描述了重要的物体检测框架。从第 [6](#S6 "6
    Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")
    节到第 [9](#S9 "9 Other Issues ‣ Deep Learning for Generic Object Detection: A Survey")
    节，讨论了设计物体检测器时涉及的基本子问题和相关问题。最后，第 [10](#S10 "10 Discussion and Conclusion ‣ Deep
    Learning for Generic Object Detection: A Survey") 节总结了物体检测的总体讨论、最新性能和未来的研究方向。'
- en: 2 Generic Object Detection
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 通用物体检测
- en: 2.1 The Problem
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题
- en: '*Generic object detection*, also called generic object category detection,
    object class detection, or object category detection Zhang et al. ([2013](#bib.bib310)),
    is defined as follows. Given an image, determine whether or not there are instances
    of objects from predefined categories (usually *many* categories, *e.g.,* 200
    categories in the ILSVRC object detection challenge) and, if present, to return
    the spatial location and extent of each instance. A greater emphasis is placed
    on detecting a broad range of natural categories, as opposed to specific object
    category detection where only a narrower predefined category of interest (*e.g.,*
    faces, pedestrians, or cars) may be present. Although thousands of objects occupy
    the visual world in which we live, currently the research community is primarily
    interested in the localization of highly structured objects (*e.g.,* cars, faces,
    bicycles and airplanes) and articulated objects (*e.g.,* humans, cows and horses)
    rather than unstructured scenes (such as sky, grass and cloud).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*通用物体检测*，也称为通用物体类别检测、物体类别检测或物体分类检测 Zhang 等人 ([2013](#bib.bib310))，定义如下。给定一张图像，确定是否存在来自预定义类别（通常是
    *许多* 类别，例如 ILSVRC 物体检测挑战中的 200 类别）的物体实例，如果存在，返回每个实例的空间位置和范围。重点在于检测广泛的自然类别，而不是仅仅特定物体类别检测，其中可能只存在较窄的预定义兴趣类别（*例如，*
    脸部、行人或汽车）。尽管我们生活的视觉世界中存在成千上万的物体，但目前研究界主要关注高结构化物体的定位（*例如，* 汽车、脸部、自行车和飞机）和关节物体（*例如，*
    人类、牛和马），而非非结构化场景（如天空、草地和云朵）。'
- en: 'The spatial location and extent of an object can be defined coarsely using
    a bounding box (an axis-aligned rectangle tightly bounding the object) Everingham
    et al. ([2010](#bib.bib68)); Russakovsky et al. ([2015](#bib.bib234)), a precise
    pixelwise segmentation mask Zhang et al. ([2013](#bib.bib310)), or a closed boundary
    Lin et al. ([2014](#bib.bib166)); Russell et al. ([2008](#bib.bib235)), as illustrated
    in Fig. [5](#S1.F5 "Figure 5 ‣ 1.1 Comparison with Previous Reviews ‣ 1 Introduction
    ‣ Deep Learning for Generic Object Detection: A Survey"). To the best of our knowledge,
    for the evaluation of generic object detection algorithms, it is bounding boxes
    which are most widely used in the current literature Everingham et al. ([2010](#bib.bib68));
    Russakovsky et al. ([2015](#bib.bib234)), and therefore this is also the approach
    we adopt in this survey. However, as the research community moves towards deeper
    scene understanding (from image level object classification to single object localization,
    to generic object detection, and to pixelwise object segmentation), it is anticipated
    that future challenges will be at the pixel level Lin et al. ([2014](#bib.bib166)).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '对象的空间位置和范围可以粗略地通过边界框（一个轴对齐的矩形紧密包围对象）来定义，参考Everingham等人（[2010](#bib.bib68)）；Russakovsky等人（[2015](#bib.bib234)），或者精确的像素级分割掩码，参考Zhang等人（[2013](#bib.bib310)），或封闭边界，参考Lin等人（[2014](#bib.bib166)）；Russell等人（[2008](#bib.bib235)），如图[5](#S1.F5
    "Figure 5 ‣ 1.1 Comparison with Previous Reviews ‣ 1 Introduction ‣ Deep Learning
    for Generic Object Detection: A Survey")所示。根据我们所知，在通用对象检测算法的评估中，目前文献中最广泛使用的是边界框，参考Everingham等人（[2010](#bib.bib68)）；Russakovsky等人（[2015](#bib.bib234)），因此我们在这项调查中也采用了这种方法。然而，随着研究界逐步向更深入的场景理解（从图像级别对象分类到单一对象定位，再到通用对象检测，最后到像素级对象分割）发展，预计未来的挑战将会在像素级别，参考Lin等人（[2014](#bib.bib166)）。'
- en: 'There are many problems closely related to that of generic object detection¹¹1To
    the best of our knowledge, there is no universal agreement in the literature on
    the definitions of various vision subtasks. Terms such as detection, localization,
    recognition, classification, categorization, verification, identification, annotation,
    labeling, and understanding are often differently defined Andreopoulos and Tsotsos
    ([2013](#bib.bib5)).. The goal of *object classification* or *object categorization*
    (Fig. [5](#S1.F5 "Figure 5 ‣ 1.1 Comparison with Previous Reviews ‣ 1 Introduction
    ‣ Deep Learning for Generic Object Detection: A Survey") (a)) is to assess the
    presence of objects from a given set of object classes in an image; *i.e.,* assigning
    one or more object class labels to a given image, determining the presence without
    the need of location. The additional requirement to locate the instances in an
    image makes detection a more challenging task than classification. The *object
    recognition* problem denotes the more general problem of identifying/localizing
    all the objects present in an image, subsuming the problems of object detection
    and classification Everingham et al. ([2010](#bib.bib68)); Russakovsky et al.
    ([2015](#bib.bib234)); Opelt et al. ([2006](#bib.bib198)); Andreopoulos and Tsotsos
    ([2013](#bib.bib5)). Generic object detection is closely related to *semantic
    image segmentation* (Fig. [5](#S1.F5 "Figure 5 ‣ 1.1 Comparison with Previous
    Reviews ‣ 1 Introduction ‣ Deep Learning for Generic Object Detection: A Survey")
    (c)), which aims to assign each pixel in an image to a semantic class label. *Object
    instance segmentation* (Fig. [5](#S1.F5 "Figure 5 ‣ 1.1 Comparison with Previous
    Reviews ‣ 1 Introduction ‣ Deep Learning for Generic Object Detection: A Survey")
    (d)) aims to distinguish different instances of the same object class, as opposed
    to semantic segmentation which does not.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '有许多问题与通用物体检测密切相关¹¹1据我们所知，文献中对各种视觉子任务的定义没有统一的意见。术语如检测、定位、识别、分类、类别化、验证、识别、注释、标记和理解经常有不同的定义
    Andreopoulos 和 Tsotsos ([2013](#bib.bib5))。*物体分类* 或 *物体类别化*（图 [5](#S1.F5 "Figure
    5 ‣ 1.1 Comparison with Previous Reviews ‣ 1 Introduction ‣ Deep Learning for
    Generic Object Detection: A Survey")（a））的目标是评估图像中是否存在来自给定对象类别集合的物体；*即，* 将一个或多个物体类别标签分配给给定图像，在不需要位置的情况下确定存在性。定位图像中的实例的额外要求使得检测比分类更具挑战性。*物体识别*
    问题表示识别/定位图像中所有存在物体的更一般性问题，涵盖了物体检测和分类的问题 Everingham et al. ([2010](#bib.bib68));
    Russakovsky et al. ([2015](#bib.bib234)); Opelt et al. ([2006](#bib.bib198));
    Andreopoulos 和 Tsotsos ([2013](#bib.bib5))。通用物体检测与 *语义图像分割* （图 [5](#S1.F5 "Figure
    5 ‣ 1.1 Comparison with Previous Reviews ‣ 1 Introduction ‣ Deep Learning for
    Generic Object Detection: A Survey")（c））密切相关，后者旨在将图像中的每个像素分配给一个语义类别标签。*物体实例分割*
    （图 [5](#S1.F5 "Figure 5 ‣ 1.1 Comparison with Previous Reviews ‣ 1 Introduction
    ‣ Deep Learning for Generic Object Detection: A Survey")（d））旨在区分同一物体类别的不同实例，与不区分实例的语义分割不同。'
- en: '![Refer to caption](img/c1e4fccbb60ca406fc88ce8572f48a97.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c1e4fccbb60ca406fc88ce8572f48a97.png)'
- en: 'Figure 6: Taxonomy of challenges in generic object detection.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：通用物体检测中的挑战分类。
- en: '![Refer to caption](img/51ca273ac72bd5b673e2e6a62169625f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/51ca273ac72bd5b673e2e6a62169625f.png)'
- en: 'Figure 7: Changes in appearance of the same class with variations in imaging
    conditions (a-h). There is an astonishing variation in what is meant to be a single
    object class (i). In contrast, the four images in (j) appear very similar, but
    in fact are from four *d*ifferent object classes. Most images are from ImageNet
    Russakovsky et al. ([2015](#bib.bib234)) and MS COCO Lin et al. ([2014](#bib.bib166)).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：同一类别在成像条件变化下的外观变化（a-h）。单一物体类别的定义存在惊人的变化（i）。相比之下，（j）中的四张图像看起来非常相似，但实际上来自四个*d*ifferent
    物体类别。大多数图像来自 ImageNet Russakovsky et al. ([2015](#bib.bib234)) 和 MS COCO Lin et
    al. ([2014](#bib.bib166))。
- en: 2.2 Main Challenges
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 主要挑战
- en: 'The ideal of generic object detection is to develop a general-purpose algorithm
    that achieves two competing goals of *high quality/accuracy* and *high efficiency*
    (Fig. [6](#S2.F6 "Figure 6 ‣ 2.1 The Problem ‣ 2 Generic Object Detection ‣ Deep
    Learning for Generic Object Detection: A Survey")). As illustrated in Fig. [7](#S2.F7
    "Figure 7 ‣ 2.1 The Problem ‣ 2 Generic Object Detection ‣ Deep Learning for Generic
    Object Detection: A Survey"), high quality detection must accurately localize
    and recognize objects in images or video frames, such that the large variety of
    object categories in the real world can be distinguished (*i.e.,* high distinctiveness),
    and that object instances from the same category, subject to intra-class appearance
    variations, can be localized and recognized (*i.e.,* high robustness). High efficiency
    requires that the entire detection task runs in real time with acceptable memory
    and storage demands.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '通用目标检测的理想是开发一个通用算法，旨在实现*高质量/准确性*和*高效率*这两个相互竞争的目标（见图 [6](#S2.F6 "Figure 6 ‣
    2.1 The Problem ‣ 2 Generic Object Detection ‣ Deep Learning for Generic Object
    Detection: A Survey")）。如图 [7](#S2.F7 "Figure 7 ‣ 2.1 The Problem ‣ 2 Generic Object
    Detection ‣ Deep Learning for Generic Object Detection: A Survey")所示，高质量检测必须准确地定位和识别图像或视频帧中的对象，以便区分现实世界中各种不同的对象类别（*即，*高区分度），以及来自同一类别的对象实例，尽管存在类别内的外观变化，也能够被定位和识别（*即，*高鲁棒性）。高效率要求整个检测任务能够实时运行，同时满足可接受的内存和存储需求。'
- en: 2.2.1 Accuracy related challenges
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 准确性相关挑战
- en: Challenges in detection accuracy stem from 1) the vast range of intra-class
    variations and 2) the huge number of object categories.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 检测准确性挑战源于 1) 类别内变异的广泛范围和 2) 大量的对象类别。
- en: 'Intra-class variations can be divided into two types: intrinsic factors and
    imaging conditions. In terms of intrinsic factors, each object category can have
    many different object instances, possibly varying in one or more of color, texture,
    material, shape, and size, such as the “chair” category shown in Fig. [7](#S2.F7
    "Figure 7 ‣ 2.1 The Problem ‣ 2 Generic Object Detection ‣ Deep Learning for Generic
    Object Detection: A Survey") (*i*). Even in a more narrowly defined class, such
    as human or horse, object instances can appear in different poses, subject to
    nonrigid deformations or with the addition of clothing.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '类别内变异可以分为两种类型：内在因素和成像条件。在内在因素方面，每个对象类别可能有许多不同的对象实例，这些实例在颜色、纹理、材料、形状和大小等一个或多个方面可能有所不同，例如图 [7](#S2.F7
    "Figure 7 ‣ 2.1 The Problem ‣ 2 Generic Object Detection ‣ Deep Learning for Generic
    Object Detection: A Survey") 中显示的“椅子”类别（*i*）。即使在更窄定义的类别中，例如人类或马，对象实例也可能出现不同的姿势，受到非刚性变形或附加衣物的影响。'
- en: 'Imaging condition variations are caused by the dramatic impacts unconstrained
    environments can have on object appearance, such as lighting (dawn, day, dusk,
    indoors), physical location, weather conditions, cameras, backgrounds, illuminations,
    occlusion, and viewing distances. All of these conditions produce significant
    variations in object appearance, such as illumination, pose, scale, occlusion,
    clutter, shading, blur and motion, with examples illustrated in Fig. [7](#S2.F7
    "Figure 7 ‣ 2.1 The Problem ‣ 2 Generic Object Detection ‣ Deep Learning for Generic
    Object Detection: A Survey") (*a*-*h*). Further challenges may be added by digitization
    artifacts, noise corruption, poor resolution, and filtering distortions.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '成像条件变异由环境对对象外观的显著影响引起，例如光照（黎明、白天、黄昏、室内）、物理位置、天气条件、相机、背景、照明、遮挡和观看距离。这些条件都会导致对象外观的显著变化，例如照明、姿势、尺度、遮挡、杂乱、阴影、模糊和运动，图 [7](#S2.F7
    "Figure 7 ‣ 2.1 The Problem ‣ 2 Generic Object Detection ‣ Deep Learning for Generic
    Object Detection: A Survey") 中有示例（*a*-*h*）。进一步的挑战可能还包括数字化伪影、噪声干扰、分辨率低和过滤失真。'
- en: 'In addition to *intra*class variations, the large number of object categories,
    on the order of $10^{4}-10^{5}$, demands great discrimination power from the detector
    to distinguish between subtly different *inter*class variations, as illustrated
    in Fig. [7](#S2.F7 "Figure 7 ‣ 2.1 The Problem ‣ 2 Generic Object Detection ‣
    Deep Learning for Generic Object Detection: A Survey") (j). In practice, current
    detectors focus mainly on structured object categories, such as the 20, 200 and
    91 object classes in PASCAL VOC Everingham et al. ([2010](#bib.bib68)), ILSVRC
    Russakovsky et al. ([2015](#bib.bib234)) and MS COCO Lin et al. ([2014](#bib.bib166))
    respectively. Clearly, the number of object categories under consideration in
    existing benchmark datasets is much smaller than can be recognized by humans.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '除了*类内*变化外，物体类别的数量达到$10^{4}-10^{5}$级别，需要探测器具有极大的辨别能力，以区分微妙的*类间*变化，如图[7](#S2.F7
    "Figure 7 ‣ 2.1 The Problem ‣ 2 Generic Object Detection ‣ Deep Learning for Generic
    Object Detection: A Survey") (j)所示。实际上，当前的探测器主要集中在结构化物体类别上，例如PASCAL VOC Everingham
    et al. ([2010](#bib.bib68))、ILSVRC Russakovsky et al. ([2015](#bib.bib234))和MS
    COCO Lin et al. ([2014](#bib.bib166))中的20、200和91个物体类别。显然，现有基准数据集中考虑的物体类别数量远远少于人类可以识别的数量。'
- en: 2.2.2 Efficiency and scalability related challenges
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 效率和可扩展性相关挑战
- en: The prevalence of social media networks and mobile/wearable devices has led
    to increasing demands for analyzing visual data. However, mobile/wearable devices
    have limited computational capabilities and storage space, making efficient object
    detection critical.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体网络和移动/可穿戴设备的普及导致了对视觉数据分析的需求增加。然而，移动/可穿戴设备的计算能力和存储空间有限，使得高效的物体检测变得至关重要。
- en: 'The efficiency challenges stem from the need to localize and recognize, computational
    complexity growing with the (possibly large) number of object categories, and
    with the (possibly very large) number of locations and scales within a single
    image, such as the examples in Fig. [7](#S2.F7 "Figure 7 ‣ 2.1 The Problem ‣ 2
    Generic Object Detection ‣ Deep Learning for Generic Object Detection: A Survey")
    (c, d).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '效率挑战源于需要进行定位和识别，计算复杂性随着物体类别数量的（可能很大）增加，以及图像中位置和尺度的（可能非常大）数量而增长，例如图[7](#S2.F7
    "Figure 7 ‣ 2.1 The Problem ‣ 2 Generic Object Detection ‣ Deep Learning for Generic
    Object Detection: A Survey") (c, d)中的示例。'
- en: 'A further challenge is that of scalability: A detector should be able to handle
    previously unseen objects, unknown situations, and high data rates. As the number
    of images and the number of categories continue to grow, it may become impossible
    to annotate them manually, forcing a reliance on weakly supervised strategies.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是可扩展性：探测器应能处理以前未见过的物体、未知情况以及高数据速率。随着图像数量和类别数量的不断增加，手动标注可能变得不可能，迫使我们依赖于弱监督策略。
- en: '![Refer to caption](img/93ce04cfb0f0aac2115cc4a87d3ba258.png)![Refer to caption](img/776c5ae01ad14f1eef1691f41b771865.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/93ce04cfb0f0aac2115cc4a87d3ba258.png)![参见说明](img/776c5ae01ad14f1eef1691f41b771865.png)'
- en: 'Figure 8: (a) Illustration of three operations that are repeatedly applied
    by a typical CNN: Convolution with a number of linear filters; Nonlinearities
    (*e.g.* ReLU); and Local pooling (*e.g.* Max Pooling). The $M$ feature maps from
    a previous layer are convolved with $N$ different filters (here shown as size
    $3\times 3\times M$), using a stride of 1\. The resulting $N$ feature maps are
    then passed through a nonlinear function (*e.g.* ReLU), and pooled (*e.g.* taking
    a maximum over $2\times 2$ regions) to give $N$ feature maps at a reduced resolution.
    (b) Illustration of the architecture of VGGNet Simonyan and Zisserman ([2015](#bib.bib248)),
    a typical CNN with 11 weight layers. An image with 3 color channels is presented
    as the input. The network has 8 convolutional layers, 3 fully connected layers,
    5 max pooling layers and a softmax classification layer. The last three fully
    connected layers take features from the top convolutional layer as input in vector
    form. The final layer is a $C$-way softmax function, $C$ being the number of classes.
    The whole network can be learned from labeled training data by optimizing an objective
    function (*e.g.* mean squared error or cross entropy loss) via Stochastic Gradient
    Descent.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图8： (a) 典型CNN反复应用的三个操作的示意图：使用多个线性滤波器的卷积；非线性操作（*例如* ReLU）；和局部池化（*例如* 最大池化）。前一层的
    $M$ 个特征图与 $N$ 个不同的滤波器（此处显示为 $3\times 3\times M$ 的大小）进行卷积，步幅为 1。得到的 $N$ 个特征图然后通过非线性函数（*例如*
    ReLU），并进行池化（*例如* 在 $2\times 2$ 区域内取最大值），以给出分辨率降低的 $N$ 个特征图。 (b) VGGNet Simonyan
    和 Zisserman ([2015](#bib.bib248)) 的架构示意图，这是一个典型的具有 11 个权重层的 CNN。输入为具有 3 个颜色通道的图像。网络有
    8 个卷积层，3 个全连接层，5 个最大池化层和一个 softmax 分类层。最后三个全连接层以向量形式从顶部卷积层获取特征。最终层是一个 $C$ 维 softmax
    函数，$C$ 为类别数。整个网络可以通过优化目标函数（*例如* 均方误差或交叉熵损失）来从标记训练数据中学习，方法是随机梯度下降。
- en: 2.3 Progress in the Past Two Decades
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 过去二十年的进展
- en: Early research on object recognition was based on template matching techniques
    and simple part-based models Fischler and Elschlager ([1973](#bib.bib76)), focusing
    on specific objects whose spatial layouts are roughly rigid, such as faces. Before
    1990 the leading paradigm of object recognition was based on geometric representations Mundy
    ([2006](#bib.bib190)); Ponce et al. ([2007](#bib.bib215)), with the focus later
    moving away from geometry and prior models towards the use of statistical classifiers
    (such as Neural Networks Rowley et al. ([1998](#bib.bib233)), SVM Osuna et al.
    ([1997](#bib.bib201)) and Adaboost Viola and Jones ([2001](#bib.bib276)); Xiao
    et al. ([2003](#bib.bib290))) based on appearance features Murase and Nayar ([1995a](#bib.bib191));
    Schmid and Mohr ([1997](#bib.bib236)). This successful family of object detectors
    set the stage for most subsequent research in this field.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对象识别的早期研究基于模板匹配技术和简单的基于部分的模型 Fischler 和 Elschlager ([1973](#bib.bib76))，专注于空间布局大致刚性的特定对象，如人脸。在1990年前，对象识别的主流范式基于几何表示
    Mundy ([2006](#bib.bib190)); Ponce 等 ([2007](#bib.bib215))，后来研究重点从几何学和先前模型转向使用统计分类器（如神经网络
    Rowley 等 ([1998](#bib.bib233))，SVM Osuna 等 ([1997](#bib.bib201)) 和 Adaboost Viola
    和 Jones ([2001](#bib.bib276)); Xiao 等 ([2003](#bib.bib290)))，基于外观特征 Murase 和 Nayar
    ([1995a](#bib.bib191)); Schmid 和 Mohr ([1997](#bib.bib236))。这一成功的对象检测器家族为后续大多数研究奠定了基础。
- en: 'The milestones of object detection in more recent years are presented in Fig. [4](#S1.F4
    "Figure 4 ‣ 1 Introduction ‣ Deep Learning for Generic Object Detection: A Survey"),
    in which two main eras (SIFT *vs.* DCNN) are highlighted. The appearance features
    moved from global representations Murase and Nayar ([1995b](#bib.bib192)); Swain
    and Ballard ([1991](#bib.bib260)); Turk and Pentland ([1991](#bib.bib267)) to
    local representations that are designed to be invariant to changes in translation,
    scale, rotation, illumination, viewpoint and occlusion. Handcrafted local invariant
    features gained tremendous popularity, starting from the Scale Invariant Feature
    Transform (SIFT) feature Lowe ([1999](#bib.bib178)), and the progress on various
    visual recognition tasks was based substantially on the use of local descriptors
    Mikolajczyk and Schmid ([2005](#bib.bib187)) such as Haar-like features Viola
    and Jones ([2001](#bib.bib276)), SIFT Lowe ([2004](#bib.bib179)), Shape Contexts
    Belongie et al. ([2002](#bib.bib12)), Histogram of Gradients (HOG) Dalal and Triggs
    ([2005](#bib.bib52)) Local Binary Patterns (LBP) Ojala et al. ([2002](#bib.bib196)),
    and region covariances Tuzel et al. ([2006](#bib.bib268)). These local features
    are usually aggregated by simple concatenation or feature pooling encoders such
    as the Bag of Visual Words approach, introduced by Sivic and Zisserman Sivic and
    Zisserman ([2003](#bib.bib252)) and Csurka *et al.* Csurka et al. ([2004](#bib.bib47)),
    Spatial Pyramid Matching (SPM) of BoW models Lazebnik et al. ([2006](#bib.bib147)),
    and Fisher Vectors Perronnin et al. ([2010](#bib.bib212)).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '最近几年物体检测的里程碑展示在图. [4](#S1.F4 "Figure 4 ‣ 1 Introduction ‣ Deep Learning for
    Generic Object Detection: A Survey")中，其中突出了两个主要时代（SIFT *vs.* DCNN）。外观特征从全局表示（Murase
    and Nayar ([1995b](#bib.bib192))；Swain and Ballard ([1991](#bib.bib260))；Turk
    and Pentland ([1991](#bib.bib267))）转变为局部表示，这些局部表示设计为对平移、缩放、旋转、照明、视角和遮挡变化不变。手工制作的局部不变特征从尺度不变特征变换（SIFT）特征（Lowe
    ([1999](#bib.bib178))）开始获得了极大的流行，视觉识别任务的进展在很大程度上依赖于局部描述符（Mikolajczyk and Schmid
    ([2005](#bib.bib187))），例如Haar-like特征（Viola and Jones ([2001](#bib.bib276))）、SIFT（Lowe
    ([2004](#bib.bib179))）、形状上下文（Belongie et al. ([2002](#bib.bib12))）、梯度直方图（HOG）（Dalal
    and Triggs ([2005](#bib.bib52))）、局部二值模式（LBP）（Ojala et al. ([2002](#bib.bib196))）以及区域协方差（Tuzel
    et al. ([2006](#bib.bib268))）。这些局部特征通常通过简单的拼接或特征池化编码器进行聚合，如视觉词袋方法（Bag of Visual
    Words），由Sivic和Zisserman（Sivic and Zisserman ([2003](#bib.bib252))）提出，以及Csurka
    *et al.*（Csurka et al. ([2004](#bib.bib47))）、BoW模型的空间金字塔匹配（SPM）（Lazebnik et al.
    ([2006](#bib.bib147))）和Fisher向量（Perronnin et al. ([2010](#bib.bib212))）。'
- en: For years, the multistage hand tuned pipelines of handcrafted local descriptors
    and discriminative classifiers dominated a variety of domains in computer vision,
    including object detection, until the significant turning point in 2012 when DCNNs
    Krizhevsky et al. ([2012a](#bib.bib140)) achieved their record-breaking results
    in image classification.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，手工调节的多阶段管道结合手工制作的局部描述符和判别分类器在计算机视觉的各种领域（包括物体检测）中占据了主导地位，直到2012年DCNN（Krizhevsky
    et al. ([2012a](#bib.bib140))）在图像分类中取得了突破性的成果，标志着一个重要的转折点。
- en: The use of CNNs for detection and localization Rowley et al. ([1998](#bib.bib233))
    can be traced back to the 1990s, with a modest number of hidden layers used for
    object detection Vaillant et al. ([1994](#bib.bib272)); Rowley et al. ([1998](#bib.bib233));
    Sermanet et al. ([2013](#bib.bib238)), successful in restricted domains such as
    face detection. However, more recently, deeper CNNs have led to record-breaking
    improvements in the detection of more general object categories, a shift which
    came about when the successful application of DCNNs in image classification Krizhevsky
    et al. ([2012a](#bib.bib140)) was transferred to object detection, resulting in
    the milestone Region-based CNN (RCNN) detector of Girshick *et al.* Girshick et al.
    ([2014](#bib.bib85)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: CNN用于检测和定位的应用可以追溯到1990年代，当时使用了少量隐藏层进行物体检测（Vaillant et al. ([1994](#bib.bib272))；Rowley
    et al. ([1998](#bib.bib233))；Sermanet et al. ([2013](#bib.bib238))），在诸如人脸检测等限制性领域取得了成功。然而，最近更深层次的CNN在检测更多通用物体类别方面取得了突破性的进展，这一转变发生在成功应用DCNN进行图像分类（Krizhevsky
    et al. ([2012a](#bib.bib140))）之后，最终催生了具有里程碑意义的区域卷积神经网络（RCNN）检测器（Girshick *et al.*
    Girshick et al. ([2014](#bib.bib85))）。
- en: The successes of deep detectors rely heavily on vast training data and large
    networks with millions or even billions of parameters. The availability of GPUs
    with very high computational capability and large-scale detection datasets (such
    as ImageNet Deng et al. ([2009](#bib.bib54)); Russakovsky et al. ([2015](#bib.bib234))
    and MS COCO Lin et al. ([2014](#bib.bib166))) play a key role in their success.
    Large datasets have allowed researchers to target more realistic and complex problems
    from images with large intra-class variations and inter-class similarities Lin
    et al. ([2014](#bib.bib166)); Russakovsky et al. ([2015](#bib.bib234)). However,
    accurate annotations are labor intensive to obtain, so detectors must consider
    methods that can relieve annotation difficulties or can learn with smaller training
    datasets.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 深度检测器的成功在很大程度上依赖于大量的训练数据和具有数百万甚至数十亿个参数的大型网络。具备极高计算能力的GPU和大规模检测数据集（例如ImageNet
    Deng et al. ([2009](#bib.bib54)); Russakovsky et al. ([2015](#bib.bib234)) 和 MS
    COCO Lin et al. ([2014](#bib.bib166)))在这些成功中发挥了关键作用。大规模数据集使研究人员能够针对具有大类内变异和类间相似性的图像中的更现实和复杂的问题
    Lin et al. ([2014](#bib.bib166)); Russakovsky et al. ([2015](#bib.bib234))。然而，准确的标注获取需要大量劳动，因此检测器必须考虑能够减轻标注难度或能够在较小训练数据集上学习的方法。
- en: 'The research community has started moving towards the challenging goal of building
    general purpose object detection systems whose ability to detect many object categories
    matches that of humans. This is a major challenge: according to cognitive scientists,
    human beings can identify around 3,000 entry level categories and 30,000 visual
    categories overall, and the number of categories distinguishable with domain expertise
    may be to the order of $10^{5}$ Biederman ([1987a](#bib.bib15)). Despite the remarkable
    progress of the past years, designing an accurate, robust, efficient detection
    and recognition system that approaches human-level performance on $10^{4}-10^{5}$
    categories is undoubtedly an unresolved problem.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 研究界已经开始朝着建立通用目标检测系统的挑战性目标迈进，该系统能够检测的目标类别数量与人类相匹配。这是一个重大挑战：根据认知科学家，人类可以识别大约3,000个入门级类别和总计30,000个视觉类别，并且具有领域专长的可区分类别数量可能达到$10^{5}$
    Biederman ([1987a](#bib.bib15))。尽管近年来取得了显著进展，设计一个准确、可靠、高效的检测和识别系统，使其在$10^{4}-10^{5}$类别上的性能接近人类水平，无疑仍是一个未解决的问题。
- en: 3 A Brief Introduction to Deep Learning
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习简要介绍
- en: Deep learning has revolutionized a wide range of machine learning tasks, from
    image classification and video processing to speech recognition and natural language
    understanding. Given this tremendously rapid evolution, there exist many recent
    survey papers on deep learning Bengio et al. ([2013](#bib.bib13)); Goodfellow
    et al. ([2016](#bib.bib89)); Gu et al. ([2017](#bib.bib92)); LeCun et al. ([2015](#bib.bib149));
    Litjens et al. ([2017](#bib.bib170)); Pouyanfar et al. ([2018](#bib.bib216));
    Wu et al. ([2019](#bib.bib287)); Young et al. ([2018](#bib.bib297)); Zhang et al.
    ([2018d](#bib.bib313)); Zhou et al. ([2018a](#bib.bib320)); Zhu et al. ([2017](#bib.bib325)).
    These surveys have reviewed deep learning techniques from different perspectives
    Bengio et al. ([2013](#bib.bib13)); Goodfellow et al. ([2016](#bib.bib89)); Gu
    et al. ([2017](#bib.bib92)); LeCun et al. ([2015](#bib.bib149)); Pouyanfar et al.
    ([2018](#bib.bib216)); Wu et al. ([2019](#bib.bib287)); Zhou et al. ([2018a](#bib.bib320)),
    or with applications to medical image analysis Litjens et al. ([2017](#bib.bib170)),
    natural language processing Young et al. ([2018](#bib.bib297)), speech recognition
    systems Zhang et al. ([2018d](#bib.bib313)), and remote sensing Zhu et al. ([2017](#bib.bib325)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经彻底改变了广泛的机器学习任务，从图像分类和视频处理到语音识别和自然语言理解。鉴于这种极其迅速的演变，存在许多关于深度学习的最新综述论文 Bengio
    et al. ([2013](#bib.bib13)); Goodfellow et al. ([2016](#bib.bib89)); Gu et al.
    ([2017](#bib.bib92)); LeCun et al. ([2015](#bib.bib149)); Litjens et al. ([2017](#bib.bib170));
    Pouyanfar et al. ([2018](#bib.bib216)); Wu et al. ([2019](#bib.bib287)); Young
    et al. ([2018](#bib.bib297)); Zhang et al. ([2018d](#bib.bib313)); Zhou et al.
    ([2018a](#bib.bib320)); Zhu et al. ([2017](#bib.bib325))。这些综述从不同的角度回顾了深度学习技术 Bengio
    et al. ([2013](#bib.bib13)); Goodfellow et al. ([2016](#bib.bib89)); Gu et al.
    ([2017](#bib.bib92)); LeCun et al. ([2015](#bib.bib149)); Pouyanfar et al. ([2018](#bib.bib216));
    Wu et al. ([2019](#bib.bib287)); Zhou et al. ([2018a](#bib.bib320))，或应用于医学图像分析
    Litjens et al. ([2017](#bib.bib170))，自然语言处理 Young et al. ([2018](#bib.bib297))，语音识别系统
    Zhang et al. ([2018d](#bib.bib313))，以及遥感 Zhu et al. ([2017](#bib.bib325))。
- en: 'Convolutional Neural Networks (CNNs), the most representative models of deep
    learning, are able to exploit the basic properties underlying natural signals:
    translation invariance, local connectivity, and compositional hierarchies LeCun
    et al. ([2015](#bib.bib149)). A typical CNN, illustrated in Fig. [8](#S2.F8 "Figure
    8 ‣ 2.2.2 Efficiency and scalability related challenges ‣ 2.2 Main Challenges
    ‣ 2 Generic Object Detection ‣ Deep Learning for Generic Object Detection: A Survey"),
    has a hierarchical structure and is composed of a number of layers to learn representations
    of data with multiple levels of abstraction LeCun et al. ([2015](#bib.bib149)).
    We begin with a convolution'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '卷积神经网络（CNNs），作为深度学习的最具代表性的模型，能够利用自然信号的基本属性：平移不变性、局部连接性和组合层次结构 LeCun等人 ([2015](#bib.bib149))。典型的CNN，如图
    [8](#S2.F8 "Figure 8 ‣ 2.2.2 Efficiency and scalability related challenges ‣ 2.2
    Main Challenges ‣ 2 Generic Object Detection ‣ Deep Learning for Generic Object
    Detection: A Survey") 所示，具有层次结构，并由若干层组成，以学习具有多个抽象层次的数据表示 LeCun等人 ([2015](#bib.bib149))。我们从卷积开始'
- en: '|  | $\textbf{\emph{x}}^{l-1}*\textbf{\emph{w}}^{l}$ |  | (1) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{\emph{x}}^{l-1}*\textbf{\emph{w}}^{l}$ |  | (1) |'
- en: between an input feature map $\textbf{\emph{x}}^{l-1}$ at a feature map from
    previous layer $l-1$, convolved with a 2D convolutional kernel (or filter or weights)
    $\textbf{\emph{w}}^{l}$. This convolution appears over a sequence of layers, subject
    to a nonlinear operation $\sigma$, such that
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在来自前一层$l-1$的输入特征图$\textbf{\emph{x}}^{l-1}$与二维卷积核（或滤波器或权重）$\textbf{\emph{w}}^{l}$进行卷积之间，这种卷积出现在一系列层上，经过非线性操作$\sigma$，这样
- en: '|  | $\textbf{\emph{x}}^{l}_{j}=\sigma(\sum_{i=1}^{N^{l-1}}\textbf{\emph{x}}^{l-1}_{i}*\textbf{\emph{w}}^{l}_{i,j}+b^{l}_{j}),$
    |  | (2) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{\emph{x}}^{l}_{j}=\sigma(\sum_{i=1}^{N^{l-1}}\textbf{\emph{x}}^{l-1}_{i}*\textbf{\emph{w}}^{l}_{i,j}+b^{l}_{j}),$
    |  | (2) |'
- en: with a convolution now between the $N^{l-1}$ input feature maps $\textbf{\emph{x}}^{l-1}_{i}$
    and the corresponding kernel $\textbf{\emph{w}}^{l}_{i,j}$, plus a bias term $b^{l}_{j}$.
    The elementwise nonlinear function $\sigma(\cdot)$ is typically a rectified linear
    unit (ReLU) for each element,
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进行卷积操作，将$N^{l-1}$个输入特征图$\textbf{\emph{x}}^{l-1}_{i}$与相应的核$\textbf{\emph{w}}^{l}_{i,j}$进行卷积，再加上偏置项$b^{l}_{j}$。逐元素的非线性函数$\sigma(\cdot)$通常是每个元素的修正线性单元（ReLU），
- en: '|  | $\sigma(x)=\max\{x,0\}.$ |  | (3) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma(x)=\max\{x,0\}.$ |  | (3) |'
- en: 'Finally, pooling corresponds to the downsampling/upsampling of feature maps.
    These three operations (convolution, nonlinearity, pooling) are illustrated in
    Fig. [8](#S2.F8 "Figure 8 ‣ 2.2.2 Efficiency and scalability related challenges
    ‣ 2.2 Main Challenges ‣ 2 Generic Object Detection ‣ Deep Learning for Generic
    Object Detection: A Survey") (a); CNNs having a large number of layers, a “deep”
    network, are referred to as Deep CNNs (DCNNs), with a typical DCNN architecture
    illustrated in Fig. [8](#S2.F8 "Figure 8 ‣ 2.2.2 Efficiency and scalability related
    challenges ‣ 2.2 Main Challenges ‣ 2 Generic Object Detection ‣ Deep Learning
    for Generic Object Detection: A Survey") (b).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，池化对应于特征图的下采样/上采样。这三种操作（卷积、非线性、池化）在图 [8](#S2.F8 "Figure 8 ‣ 2.2.2 Efficiency
    and scalability related challenges ‣ 2.2 Main Challenges ‣ 2 Generic Object Detection
    ‣ Deep Learning for Generic Object Detection: A Survey") (a) 中进行了说明；具有大量层的卷积神经网络（CNNs），即“深度”网络，被称为深度卷积神经网络（DCNNs），其典型的DCNN架构在图
    [8](#S2.F8 "Figure 8 ‣ 2.2.2 Efficiency and scalability related challenges ‣ 2.2
    Main Challenges ‣ 2 Generic Object Detection ‣ Deep Learning for Generic Object
    Detection: A Survey") (b) 中进行了说明。'
- en: 'Most layers of a CNN consist of a number of feature maps, within which each
    pixel acts like a neuron. Each neuron in a convolutional layer is connected to
    feature maps of the previous layer through a set of weights $\textbf{\emph{w}}_{i,j}$
    (essentially a set of 2D filters). As can be seen in Fig. [8](#S2.F8 "Figure 8
    ‣ 2.2.2 Efficiency and scalability related challenges ‣ 2.2 Main Challenges ‣
    2 Generic Object Detection ‣ Deep Learning for Generic Object Detection: A Survey")
    (b), where the early CNN layers are typically composed of convolutional and pooling
    layers, the later layers are normally fully connected. From earlier to later layers,
    the input image is repeatedly convolved, and with each layer, the receptive field
    or region of support increases. In general, the initial CNN layers extract low-level
    features (*e.g.,* edges), with later layers extracting more general features of
    increasing complexity Zeiler and Fergus ([2014](#bib.bib303)); Bengio et al. ([2013](#bib.bib13));
    LeCun et al. ([2015](#bib.bib149)); Oquab et al. ([2014](#bib.bib199)).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的大多数层由若干特征图组成，其中每个像素都像一个神经元。卷积层中的每个神经元通过一组权重 $\textbf{\emph{w}}_{i,j}$（本质上是一组
    2D 滤波器）与前一层的特征图相连。如图 [8](#S2.F8 "图 8 ‣ 2.2.2 效率和可扩展性相关挑战 ‣ 2.2 主要挑战 ‣ 2 通用目标检测
    ‣ 通用目标检测的深度学习：综述") (b) 所示，早期 CNN 层通常由卷积层和池化层组成，而后续层通常是全连接层。从早期到后期层，输入图像会重复进行卷积，并且每层的感受野或支持区域会增加。通常，初始
    CNN 层提取低级特征 (*例如,* 边缘)，后续层提取更一般的、复杂度递增的特征 Zeiler 和 Fergus ([2014](#bib.bib303));
    Bengio 等人 ([2013](#bib.bib13)); LeCun 等人 ([2015](#bib.bib149)); Oquab 等人 ([2014](#bib.bib199)).
- en: 'DCNNs have a number of outstanding advantages: a hierarchical structure to
    learn representations of data with multiple levels of abstraction, the capacity
    to learn very complex functions, and learning feature representations directly
    and automatically from data with minimal domain knowledge. What has particularly
    made DCNNs successful has been the availability of large scale labeled datasets
    and of GPUs with very high computational capability.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: DCNNs 具有许多突出的优势：一个分层结构可以学习具有多层抽象的数据表示，能够学习非常复杂的函数，并且能够直接和自动地从数据中学习特征表示，几乎不需要领域知识。使
    DCNNs 特别成功的因素是大规模标记数据集的可用性和计算能力非常高的 GPU。
- en: Despite the great successes, known deficiencies remain. In particular, there
    is an extreme need for labeled training data and a requirement of expensive computing
    resources, and considerable skill and experience are still needed to select appropriate
    learning parameters and network architectures. Trained networks are poorly interpretable,
    there is a lack of robustness to degradations, and many DCNNs have shown serious
    vulnerability to attacks Goodfellow et al. ([2015](#bib.bib88)), all of which
    currently limit the use of DCNNs in real-world applications.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了巨大的成功，但仍存在已知的不足之处。特别是，对标记训练数据的极度需求和昂贵计算资源的要求，以及选择合适的学习参数和网络架构仍然需要相当的技能和经验。训练后的网络解释性差，对退化缺乏鲁棒性，许多
    DCNNs 在面对攻击时表现出严重的脆弱性 Goodfellow 等人 ([2015](#bib.bib88))，这些因素目前限制了 DCNNs 在实际应用中的使用。
- en: 'Table 2: Most frequent object classes for each detection challenge. The size
    of each word is proportional to the frequency of that class in the training dataset.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 每个检测挑战中的最常见对象类别。每个单词的大小与该类别在训练数据集中的频率成正比。'
- en: '|      ![[Uncaptioned image]](img/a2c11f40b24591745d16260db2801546.png)   
    | ![[Uncaptioned image]](img/a4ef712ba48d2bf2bbbd1822a306c279.png)    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|      ![[无标题图片]](img/a2c11f40b24591745d16260db2801546.png)    | ![[无标题图片]](img/a4ef712ba48d2bf2bbbd1822a306c279.png)
       |'
- en: '|        (a) PASCAL VOC (20 Classes)      | (b) MS COCO (80 Classes) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|        (a) PASCAL VOC (20 类别)      | (b) MS COCO (80 类别) |'
- en: '|            ![[Uncaptioned image]](img/04ae0f104fafc2d350a893c0b695d8d6.png)
       |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|            ![[无标题图片]](img/04ae0f104fafc2d350a893c0b695d8d6.png)    |'
- en: '|           (c) ILSVRC (200 Classes) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|           (c) ILSVRC (200 类别) |'
- en: '|            ![[Uncaptioned image]](img/75f6c5ba0cd2a4feaf3d3f438ea12393.png)
       |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|            ![[无标题图片]](img/75f6c5ba0cd2a4feaf3d3f438ea12393.png)    |'
- en: '|           (d) Open Images Detection Challenge (500 Classes) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|           (d) Open Images Detection Challenge (500 类别) |'
- en: 4 Datasets and Performance Evaluation
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据集与性能评估
- en: 4.1 Datasets
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: Datasets have played a key role throughout the history of object recognition
    research, not only as a common ground for measuring and comparing the performance
    of competing algorithms, but also pushing the field towards increasingly complex
    and challenging problems. In particular, recently, deep learning techniques have
    brought tremendous success to many visual recognition problems, and it is the
    large amounts of annotated data which play a key role in their success. Access
    to large numbers of images on the Internet makes it possible to build comprehensive
    datasets in order to capture a vast richness and diversity of objects, enabling
    unprecedented performance in object recognition.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在目标识别研究历史中发挥了关键作用，不仅作为测量和比较竞争算法性能的共同基础，还推动了领域向越来越复杂和具有挑战性的问题发展。尤其是，最近深度学习技术在许多视觉识别问题上取得了巨大成功，而大量的标注数据在这些成功中发挥了关键作用。通过互联网访问大量图像，使得构建全面的数据集成为可能，从而捕捉到对象的丰富性和多样性，实现了目标识别的前所未有的性能。
- en: 'For generic object detection, there are four famous datasets: PASCAL VOC Everingham
    et al. ([2010](#bib.bib68), [2015](#bib.bib69)), ImageNet Deng et al. ([2009](#bib.bib54)),
    MS COCO Lin et al. ([2014](#bib.bib166)) and Open Images Kuznetsova et al. ([2018](#bib.bib143)).
    The attributes of these datasets are summarized in Table [3](#S4.T3 "Table 3 ‣
    4.1 Datasets ‣ 4 Datasets and Performance Evaluation ‣ Deep Learning for Generic
    Object Detection: A Survey"), and selected sample images are shown in Fig. [9](#S4.F9
    "Figure 9 ‣ 4.1 Datasets ‣ 4 Datasets and Performance Evaluation ‣ Deep Learning
    for Generic Object Detection: A Survey"). There are three steps to creating large-scale
    annotated datasets: determining the set of target object categories, collecting
    a diverse set of candidate images to represent the selected categories on the
    Internet, and annotating the collected images, typically by designing crowdsourcing
    strategies. Recognizing space limitations, we refer interested readers to the
    original papers Everingham et al. ([2010](#bib.bib68), [2015](#bib.bib69)); Lin
    et al. ([2014](#bib.bib166)); Russakovsky et al. ([2015](#bib.bib234)); Kuznetsova
    et al. ([2018](#bib.bib143)) for detailed descriptions of these datasets in terms
    of construction and properties.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '对于通用目标检测，有四个著名的数据集：PASCAL VOC Everingham 等 ([2010](#bib.bib68), [2015](#bib.bib69))，ImageNet
    Deng 等 ([2009](#bib.bib54))，MS COCO Lin 等 ([2014](#bib.bib166)) 和 Open Images
    Kuznetsova 等 ([2018](#bib.bib143))。这些数据集的属性汇总在表 [3](#S4.T3 "Table 3 ‣ 4.1 Datasets
    ‣ 4 Datasets and Performance Evaluation ‣ Deep Learning for Generic Object Detection:
    A Survey")中，选定的示例图像见图 [9](#S4.F9 "Figure 9 ‣ 4.1 Datasets ‣ 4 Datasets and Performance
    Evaluation ‣ Deep Learning for Generic Object Detection: A Survey")。创建大规模标注数据集有三个步骤：确定目标对象类别集合，收集多样化的候选图像以代表选定类别，通常通过设计众包策略对收集的图像进行标注。鉴于篇幅限制，我们建议感兴趣的读者查阅原始论文
    Everingham 等 ([2010](#bib.bib68), [2015](#bib.bib69))；Lin 等 ([2014](#bib.bib166))；Russakovsky
    等 ([2015](#bib.bib234))；Kuznetsova 等 ([2018](#bib.bib143))，以获得有关这些数据集构建和属性的详细描述。'
- en: 'Table 3: Popular databases for object recognition. Example images from PASCAL
    VOC, ImageNet, MS COCO and Open Images are shown in Fig. [9](#S4.F9 "Figure 9
    ‣ 4.1 Datasets ‣ 4 Datasets and Performance Evaluation ‣ Deep Learning for Generic
    Object Detection: A Survey").'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 用于目标识别的流行数据库。图像示例来自 PASCAL VOC、ImageNet、MS COCO 和 Open Images，见图 [9](#S4.F9
    "Figure 9 ‣ 4.1 Datasets ‣ 4 Datasets and Performance Evaluation ‣ Deep Learning
    for Generic Object Detection: A Survey")。'
- en: '|      Dataset Name | Total Images | Categories | Images Per Category | Objects
    Per Image | Image Size | Started Year | Highlights    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|      数据集名称 | 总图像数 | 类别 | 每类别图像数 | 每图像对象数 | 图像尺寸 | 开始年份 | 亮点    |'
- en: '|    PASCAL VOC (2012) Everingham et al. ([2015](#bib.bib69)) | $11,540$ |
    $20$ | $303\sim 4087$ | $2.4$ | $470\times 380$ | $2005$ | Covers only 20 categories
    that are common in everyday life; Large number of training images; Close to real-world
    applications; Significantly larger intraclass variations; Objects in scene context;
    Multiple objects in one image; Contains many difficult samples.    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|    PASCAL VOC (2012) Everingham 等 ([2015](#bib.bib69)) | $11,540$ | $20$
    | $303\sim 4087$ | $2.4$ | $470\times 380$ | $2005$ | 仅涵盖 20 个日常生活中的常见类别；大量训练图像；接近实际应用；类别内部变化显著；场景中的对象；每图像包含多个对象；包含许多难度样本。
       |'
- en: '|    ImageNet Russakovsky et al. ([2015](#bib.bib234)) | 14 millions+ | $21,841$
    | $-$ | $1.5$ | $500\times 400$ | $2009$ | Large number of object categories;
    More instances and more categories of objects per image; More challenging than
    PASCAL VOC; Backbone of the ILSVRC challenge; Images are object-centric.    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|     ImageNet Russakovsky 等 ([2015](#bib.bib234)) | 1400万+ | $21,841$ | $-$
    | $1.5$ | $500\times 400$ | $2009$ | 大量的对象类别；每张图像中对象的实例和类别更多；比PASCAL VOC更具挑战性；ILSVRC挑战的基础；图像以对象为中心。
       |'
- en: '|    MS COCO Lin et al. ([2014](#bib.bib166)) | $328,000+$ | $91$ | $-$ | $7.3$
    | $640\times 480$ | $2014$ | Even closer to real world scenarios; Each image contains
    more instances of objects and richer object annotation information; Contains object
    segmentation notation data that is not available in the ImageNet dataset.    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|     MS COCO Lin 等 ([2014](#bib.bib166)) | $328,000+$ | $91$ | $-$ | $7.3$
    | $640\times 480$ | $2014$ | 更接近现实世界场景；每张图像包含更多对象实例和更丰富的对象标注信息；包含ImageNet数据集中未提供的对象分割标注数据。
       |'
- en: '|    Places Zhou et al. ([2017a](#bib.bib319)) | 10 millions+ | $434$ | $-$
    | $-$ | $256\times 256$ | $2014$ | The largest labeled dataset for scene recognition;
    Four subsets Places365 Standard, Places365 Challenge, Places 205 and Places88
    as benchmarks.    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|     Places Zhou 等 ([2017a](#bib.bib319)) | 1000万+ | $434$ | $-$ | $-$ | $256\times
    256$ | $2014$ | 最大的场景识别标注数据集；四个子集：Places365 Standard、Places365 Challenge、Places
    205 和 Places88 作为基准。    |'
- en: '|    Open Images Kuznetsova et al. ([2018](#bib.bib143)) | 9 millions+ | $6000$+
    | $-$ | $8.3$ | varied | $2017$ | Annotated with image level labels, object bounding
    boxes and visual relationships; Open Images V5 supports large scale object detection,
    object instance segmentation and visual relationship detection.    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|     Open Images Kuznetsova 等 ([2018](#bib.bib143)) | 900万+ | $6000$+ | $-$
    | $8.3$ | 各异 | $2017$ | 具有图像级标签、对象边界框和视觉关系的标注；Open Images V5 支持大规模对象检测、对象实例分割和视觉关系检测。
       |'
- en: '|      |  |  |  |  |  |  |  | ![Refer to caption](img/32bcd7e32a5badaaa4b0c3a53eb69b60.png)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '|       |  |  |  |  |  |  |  | ![参见说明](img/32bcd7e32a5badaaa4b0c3a53eb69b60.png)'
- en: 'Figure 9: Some example images with object annotations from PASCAL VOC, ILSVRC,
    MS COCO and Open Images. See Table [3](#S4.T3 "Table 3 ‣ 4.1 Datasets ‣ 4 Datasets
    and Performance Evaluation ‣ Deep Learning for Generic Object Detection: A Survey")
    for a summary of these datasets.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：PASCAL VOC、ILSVRC、MS COCO 和 Open Images 中的一些带有对象标注的示例图像。有关这些数据集的摘要，请参见表[3](#S4.T3
    "表 3 ‣ 4.1 数据集 ‣ 4 数据集和性能评估 ‣ 深度学习用于通用对象检测：调查")。
- en: 'The four datasets form the backbone of their respective detection challenges.
    Each challenge consists of a publicly available dataset of images together with
    ground truth annotation and standardized evaluation software, and an annual competition
    and corresponding workshop. Statistics for the number of images and object instances
    in the training, validation and testing datasets²²2The annotations on the test
    set are not publicly released, except for PASCAL VOC2007. for the detection challenges
    are given in Table [4](#S4.T4 "Table 4 ‣ 4.1 Datasets ‣ 4 Datasets and Performance
    Evaluation ‣ Deep Learning for Generic Object Detection: A Survey"). The most
    frequent object classes in VOC, COCO, ILSVRC and Open Images detection datasets
    are visualized in Table [2](#S3.T2 "Table 2 ‣ 3 A Brief Introduction to Deep Learning
    ‣ Deep Learning for Generic Object Detection: A Survey").'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个数据集构成了各自检测挑战的基础。每个挑战包括一个公开可用的图像数据集，附有真实标注和标准化评估软件，以及一个年度竞赛和相关工作坊。训练、验证和测试数据集中图像和对象实例的统计数据²²2
    测试集的标注未公开，除非是PASCAL VOC2007。请参见表[4](#S4.T4 "表 4 ‣ 4.1 数据集 ‣ 4 数据集和性能评估 ‣ 深度学习用于通用对象检测：调查")。VOC、COCO、ILSVRC和Open
    Images检测数据集中最频繁的对象类别在表[2](#S3.T2 "表 2 ‣ 3 深度学习简要介绍 ‣ 深度学习用于通用对象检测：调查")中可视化展示。
- en: PASCAL VOC Everingham et al. ([2010](#bib.bib68), [2015](#bib.bib69)) is a multi-year
    effort devoted to the creation and maintenance of a series of benchmark datasets
    for classification and object detection, creating the precedent for standardized
    evaluation of recognition algorithms in the form of annual competitions. Starting
    from only four categories in 2005, the dataset has increased to 20 categories
    that are common in everyday life. Since 2009, the number of images has grown every
    year, but with all previous images retained to allow test results to be compared
    from year to year. Due the availability of larger datasets like ImageNet, MS COCO
    and Open Images, PASCAL VOC has gradually fallen out of fashion.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: PASCAL VOC Everingham等人（[2010](#bib.bib68), [2015](#bib.bib69)）是一个多年的努力，致力于创建和维护一系列分类和物体检测的基准数据集，为识别算法的标准化评估设立了年度竞赛的先例。从2005年只有四个类别开始，该数据集已经增加到20个日常生活中常见的类别。自2009年以来，图像数量每年增长，但保留所有先前的图像以便进行年度测试结果的比较。由于ImageNet、MS
    COCO和Open Images等大型数据集的出现，PASCAL VOC逐渐失去了流行。
- en: ILSVRC, the ImageNet Large Scale Visual Recognition Challenge Russakovsky et al.
    ([2015](#bib.bib234)), is derived from ImageNet Deng et al. ([2009](#bib.bib54)),
    scaling up PASCAL VOC’s goal of standardized training and evaluation of detection
    algorithms by more than an order of magnitude in the number of object classes
    and images. ImageNet1000, a subset of ImageNet images with 1000 different object
    categories and a total of 1.2 million images, has been fixed to provide a standardized
    benchmark for the ILSVRC image classification challenge.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ILSVRC，即ImageNet大规模视觉识别挑战赛Russakovsky等人（[2015](#bib.bib234)），源自ImageNet Deng等人（[2009](#bib.bib54)），在物体类别和图像数量上将PASCAL
    VOC的标准化训练和检测算法评估目标扩大了一个数量级。ImageNet1000，是ImageNet的一个子集，包含1000种不同物体类别，总计120万张图像，为ILSVRC图像分类挑战提供了一个标准化的基准。
- en: 'MS COCO is a response to the criticism of ImageNet that objects in its dataset
    tend to be large and well centered, making the ImageNet dataset atypical of real-world
    scenarios. To push for richer image understanding, researchers created the MS
    COCO database Lin et al. ([2014](#bib.bib166)) containing complex everyday scenes
    with common objects in their natural context, closer to real life, where objects
    are labeled using fully-segmented instances to provide more accurate detector
    evaluation. The COCO object detection challenge Lin et al. ([2014](#bib.bib166))
    features two object detection tasks: using either bounding box output or object
    instance segmentation output. COCO introduced three new challenges:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: MS COCO是对ImageNet批评的回应，批评意见认为其数据集中的物体往往较大且中心化，使得ImageNet数据集在现实场景中不典型。为了推动更丰富的图像理解，研究人员创建了MS
    COCO数据库 Lin 等人（[2014](#bib.bib166)），包含复杂的日常场景以及自然背景下的常见物体，更接近现实生活，其中物体使用完全分割的实例进行标记，以提供更准确的检测器评估。COCO物体检测挑战
    Lin 等人（[2014](#bib.bib166)）包含两个物体检测任务：使用边界框输出或物体实例分割输出。COCO引入了三个新挑战：
- en: '1.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: It contains objects at a wide range of scales, including a high percentage of
    small objects Singh and Davis ([2018](#bib.bib249));
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它包含各种尺度的物体，包括高比例的小物体 Singh 和 Davis（[2018](#bib.bib249)）；
- en: '2.'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Objects are less iconic and amid clutter or heavy occlusion;
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 物体较少标志性，且处于杂乱或重度遮挡中；
- en: '3.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'The evaluation metric (see Table [5](#S4.T5 "Table 5 ‣ 4.2 Evaluation Criteria
    ‣ 4 Datasets and Performance Evaluation ‣ Deep Learning for Generic Object Detection:
    A Survey")) encourages more accurate object localization.'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '评估指标（见表[5](#S4.T5 "Table 5 ‣ 4.2 Evaluation Criteria ‣ 4 Datasets and Performance
    Evaluation ‣ Deep Learning for Generic Object Detection: A Survey")）鼓励更准确的物体定位。'
- en: Just like ImageNet in its time, MS COCO has become the standard for object detection
    today.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 就像当年的ImageNet一样，MS COCO现在已经成为物体检测的标准。
- en: OICOD (the Open Image Challenge Object Detection) is derived from Open Images
    V4 (now V5 in 2019) Kuznetsova et al. ([2018](#bib.bib143)), currently the largest
    publicly available object detection dataset. OICOD is different from previous
    large scale object detection datasets like ILSVRC and MS COCO, not merely in terms
    of the significantly increased number of classes, images, bounding box annotations
    and instance segmentation mask annotations, but also regarding the annotation
    process. In ILSVRC and MS COCO, instances of all classes in the dataset are exhaustively
    annotated, whereas for Open Images V4 a classifier was applied to each image and
    only those labels with sufficiently high scores were sent for human verification.
    Therefore in OICOD only the object instances of human-confirmed positive labels
    are annotated.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: OICOD（开放图像挑战对象检测）源于 Open Images V4（现在是2019年的 V5）Kuznetsova 等人 ([2018](#bib.bib143))，目前是最大公开的对象检测数据集。OICOD
    与以前的大规模对象检测数据集如 ILSVRC 和 MS COCO 不同，不仅在于类别、图像、边界框标注和实例分割掩膜标注数量的大幅增加，还包括标注过程。在
    ILSVRC 和 MS COCO 中，数据集中的所有类别实例都经过详尽标注，而在 Open Images V4 中，每张图像都应用了分类器，只有那些得分足够高的标签才会被送去人工验证。因此，在
    OICOD 中，只有人类确认的正标签的对象实例才会被标注。
- en: 'Table 4: Statistics of commonly used object detection datasets. Object statistics
    for VOC challenges list the non-difficult objects used in the evaluation (all
    annotated objects). For the COCO challenge, prior to 2017, the test set had four
    splits (*Dev*, *Standard*, *Reserve*, and *Challenge*), with each having about
    20K images. Starting in 2017, the test set has only the *Dev* and *Challenge*
    splits, with the other two splits removed. Starting in 2017, the train and val
    sets are arranged differently, and the test set is divided into two roughly equally
    sized splits of about $20,000$ images each: Test Dev and Test Challenge. Note
    that the 2017 Test Dev/Challenge splits contain the same images as the 2015 Test
    Dev/Challenge splits, so results across the years are directly comparable.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：常用对象检测数据集的统计信息。VOC 挑战中的对象统计列出了评估中使用的非困难对象（所有标注对象）。对于 COCO 挑战，2017 年之前，测试集有四个拆分（*Dev*，*Standard*，*Reserve*
    和 *Challenge*），每个拆分大约有 20K 张图像。从 2017 年开始，测试集只有 *Dev* 和 *Challenge* 拆分，其他两个拆分被移除。从
    2017 年起，训练和验证集的安排有所不同，测试集被分为两个大致相等的拆分，每个拆分约 $20,000$ 张图像：Test Dev 和 Test Challenge。请注意，2017
    年的 Test Dev/Challenge 拆分包含与 2015 年的 Test Dev/Challenge 拆分相同的图像，因此跨年度的结果是直接可比的。
- en: '|      Challenge | Object Classes | Number of Images    | Number of Annotated
    Objects    | Summary (Train$+$Val)    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|      挑战 | 对象类别 | 图像数量    | 标注对象数量    | 摘要（训练$+$验证）    |'
- en: '|   | Train | Val | Test    | Train | Val    | Images | Boxes | Boxes/Image
       |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|   | 训练 | 验证 | 测试    | 训练 | 验证    | 图像 | 框 | 框/图像    |'
- en: '|                                  PASCAL VOC Object Detection Challenge   
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|                                  PASCAL VOC 对象检测挑战    |'
- en: '|    VOC07 | $20$ | $2,501$ | $2,510$ | $4,952$  | $6,301(7,844)$ | $6,307(7,818)$  |
    $5,011$ | $12,608$ | $2.5$  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|    VOC07 | $20$ | $2,501$ | $2,510$ | $4,952$  | $6,301(7,844)$ | $6,307(7,818)$  |
    $5,011$ | $12,608$ | $2.5$  |'
- en: '|    VOC08 | $20$ | $2,111$ | $2,221$ | $4,133$  | $5,082(6,337)$ | $5,281(6,347)$  |
    $4,332$ | $10,364$ | $2.4$  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|    VOC08 | $20$ | $2,111$ | $2,221$ | $4,133$  | $5,082(6,337)$ | $5,281(6,347)$  |
    $4,332$ | $10,364$ | $2.4$  |'
- en: '|    VOC09 | $20$ | $3,473$ | $3,581$ | $6,650$  | $8,505(9,760)$ | $8,713(9,779)$  |
    $7,054$ | $17,218$ | $2.3$  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|    VOC09 | $20$ | $3,473$ | $3,581$ | $6,650$  | $8,505(9,760)$ | $8,713(9,779)$  |
    $7,054$ | $17,218$ | $2.3$  |'
- en: '|    VOC10 | $20$ | $4,998$ | $5,105$ | $9,637$  | $11,577(13,339)$ | $11,797(13,352)$  |
    $10,103$ | $23,374$ | $2.4$  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|    VOC10 | $20$ | $4,998$ | $5,105$ | $9,637$  | $11,577(13,339)$ | $11,797(13,352)$  |
    $10,103$ | $23,374$ | $2.4$  |'
- en: '|    VOC11 | $20$ | $5,717$ | $5,823$ | $10,994$  | $13,609(15,774)$ | $13,841(15,787)$  |
    $11,540$ | $27,450$ | $2.4$  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|    VOC11 | $20$ | $5,717$ | $5,823$ | $10,994$  | $13,609(15,774)$ | $13,841(15,787)$  |
    $11,540$ | $27,450$ | $2.4$  |'
- en: '|    VOC12 | $20$ | $5,717$ | $5,823$ | $10,991$  | $13,609(15,774)$ | $13,841(15,787)$  |
    $11,540$ | $27,450$ | $2.4$  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|    VOC12 | $20$ | $5,717$ | $5,823$ | $10,991$  | $13,609(15,774)$ | $13,841(15,787)$  |
    $11,540$ | $27,450$ | $2.4$  |'
- en: '|                                  ILSVRC Object Detection Challenge    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|                                  ILSVRC 对象检测挑战    |'
- en: '|    ILSVRC13 | $200$ | $395,909$ | $20,121$ | $40,152$  | $345,854$ | $55,502$  |
    $416,030$ | $401,356$ | $1.0$  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|    ILSVRC13 | $200$ | $395,909$ | $20,121$ | $40,152$  | $345,854$ | $55,502$  |
    $416,030$ | $401,356$ | $1.0$  |'
- en: '|    ILSVRC14 | $200$ | $456,567$ | $20,121$ | $40,152$  | $478,807$ | $55,502$  |
    $476,668$ | $534,309$ | $1.1$  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|    ILSVRC14 | $200$ | $456,567$ | $20,121$ | $40,152$  | $478,807$ | $55,502$  |
    $476,668$ | $534,309$ | $1.1$  |'
- en: '|    ILSVRC15 | $200$ | $456,567$ | $20,121$ | $51,294$  | $478,807$ | $55,502$  |
    $476,668$ | $534,309$ | $1.1$  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|    ILSVRC15 | $200$ | $456,567$ | $20,121$ | $51,294$  | $478,807$ | $55,502$  |
    $476,668$ | $534,309$ | $1.1$  |'
- en: '|    ILSVRC16 | $200$ | $456,567$ | $20,121$ | $60,000$  | $478,807$ | $55,502$  |
    $476,668$ | $534,309$ | $1.1$  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|    ILSVRC16 | $200$ | $456,567$ | $20,121$ | $60,000$  | $478,807$ | $55,502$  |
    $476,668$ | $534,309$ | $1.1$  |'
- en: '|    ILSVRC17 | $200$ | $456,567$ | $20,121$ | $65,500$  | $478,807$ | $55,502$  |
    $476,668$ | $534,309$ | $1.1$  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|    ILSVRC17 | $200$ | $456,567$ | $20,121$ | $65,500$  | $478,807$ | $55,502$  |
    $476,668$ | $534,309$ | $1.1$  |'
- en: '|                                  MS COCO Object Detection Challenge    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|                                  MS COCO 目标检测挑战    |'
- en: '|    MS COCO15 | $80$ | $82,783$ | $40,504$ | $81,434$  | $604,907$ | $291,875$  |
    $123,287$ | $896,782$ | $7.3$  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|    MS COCO15 | $80$ | $82,783$ | $40,504$ | $81,434$  | $604,907$ | $291,875$  |
    $123,287$ | $896,782$ | $7.3$  |'
- en: '|    MS COCO16 | $80$ | $82,783$ | $40,504$ | $81,434$  | $604,907$ | $291,875$  |
    $123,287$ | $896,782$ | $7.3$  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|    MS COCO16 | $80$ | $82,783$ | $40,504$ | $81,434$  | $604,907$ | $291,875$  |
    $123,287$ | $896,782$ | $7.3$  |'
- en: '|    MS COCO17 | $80$ | $118,287$ | $5,000$ | $40,670$  | $860,001$ | $36,781$  |
    $123,287$ | $896,782$ | $7.3$  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|    MS COCO17 | $80$ | $118,287$ | $5,000$ | $40,670$  | $860,001$ | $36,781$  |
    $123,287$ | $896,782$ | $7.3$  |'
- en: '|    MS COCO18 | $80$ | $118,287$ | $5,000$ | $40,670$  | $860,001$ | $36,781$  |
    $123,287$ | $896,782$ | $7.3$  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|    MS COCO18 | $80$ | $118,287$ | $5,000$ | $40,670$  | $860,001$ | $36,781$  |
    $123,287$ | $896,782$ | $7.3$  |'
- en: '|                                  Open Images Challenge Object Detection (OICOD)
    (Based on Open Images V4 Kuznetsova et al. ([2018](#bib.bib143)))    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|                                  开放图像挑战目标检测 (OICOD) (基于 Open Images V4 Kuznetsova
    等 ([2018](#bib.bib143)))    |'
- en: '|    OICOD18 | $500$ | $1,643,042$ | $100,000$ | $99,999$  | $11,498,734$ |
    $696,410$  | $1,743,042$ | $12,195,144$ | $7.0$  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|    OICOD18 | $500$ | $1,643,042$ | $100,000$ | $99,999$  | $11,498,734$ |
    $696,410$  | $1,743,042$ | $12,195,144$ | $7.0$  |'
- en: '|      |  |  |  |  |  |  |  |  |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|      |  |  |  |  |  |  |  |  |  |'
- en: 4.2 Evaluation Criteria
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估标准
- en: 'There are three criteria for evaluating the performance of detection algorithms:
    detection speed in Frames Per Second (FPS), precision, and recall. The most commonly
    used metric is *Average Precision* (AP), derived from precision and recall. AP
    is usually evaluated in a category specific manner, *i.e.*, computed for each
    object category separately. To compare performance over all object categories,
    the *mean AP* (mAP) averaged over all object categories is adopted as the final
    measure of performance³³3In object detection challenges, such as PASCAL VOC and
    ILSVRC, the winning entry of each object category is that with the highest AP
    score, and the winner of the challenge is the team that wins on the most object
    categories. The mAP is also used as the measure of a team’s performance, and is
    justified since the ranking of teams by mAP was always the same as the ranking
    by the number of object categories won Russakovsky et al. ([2015](#bib.bib234))..
    More details on these metrics can be found in Everingham et al. ([2010](#bib.bib68),
    [2015](#bib.bib69)); Russakovsky et al. ([2015](#bib.bib234)); Hoiem et al. ([2012](#bib.bib108)).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 评估检测算法性能有三个标准：每秒帧数 (FPS)、精确度和召回率。最常用的度量是*平均精度* (AP)，由精确度和召回率得出。AP 通常按类别特定的方式评估，即对每个目标类别单独计算。为了比较所有目标类别的性能，采用所有目标类别的*平均
    AP* (mAP) 作为最终性能指标³³在目标检测挑战中，如 PASCAL VOC 和 ILSVRC，每个目标类别的获胜条目是 AP 分数最高的那个，而挑战的获胜者是赢得最多目标类别的团队。mAP
    也被用作团队表现的衡量标准，并且被证明合理，因为按 mAP 排名的团队排名总是与按赢得的目标类别数量的排名一致 Russakovsky 等 ([2015](#bib.bib234))..
    关于这些指标的更多细节请参见 Everingham 等 ([2010](#bib.bib68), [2015](#bib.bib69)); Russakovsky
    等 ([2015](#bib.bib234)); Hoiem 等 ([2012](#bib.bib108))。
- en: '![Refer to caption](img/bb0fcbdfb960fd99620cebe14f0d9d95.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bb0fcbdfb960fd99620cebe14f0d9d95.png)'
- en: 'Figure 10: The algorithm for determining TPs and FPs by greedily matching object
    detection results to ground truth boxes.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：通过贪婪地将目标检测结果与真实框匹配来确定真正例和假正例的算法。
- en: The standard outputs of a detector applied to a testing image I are the predicted
    detections $\{(b_{j},c_{j},p_{j})\}_{j}$, indexed by object $j$, of Bounding Box
    (BB) $b_{j}$, predicted category $c_{j}$, and confidence $p_{j}$. A predicted
    detection $(b,c,p)$ is regarded as a True Positive (TP) if
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 探测器应用于测试图像 I 的标准输出是预测的检测结果 $\{(b_{j},c_{j},p_{j})\}_{j}$，由目标 $j$ 索引，包括边界框 (BB)
    $b_{j}$、预测类别 $c_{j}$ 和置信度 $p_{j}$。如果预测的检测 $(b,c,p)$ 的预测类别 $c$ 等于真实标签 $c_{g}$，则被视为真正例
    (TP)。
- en: $\bullet$
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: The predicted category $c$ equals the ground truth label $c_{g}$.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测类别 $c$ 等于真实标签 $c_{g}$。
- en: $\bullet$
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: The overlap ratio IOU (Intersection Over Union) Everingham et al. ([2010](#bib.bib68));
    Russakovsky et al. ([2015](#bib.bib234))
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重叠率 IOU（Intersection Over Union）由 Everingham 等人（[2010](#bib.bib68)）和 Russakovsky
    等人（[2015](#bib.bib234)）提出。
- en: '|  | $\textrm{IOU}(b,b^{g})=\frac{area(b\cap b^{g})}{area(b\cup b^{g})},$ |  |
    (4) |'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\textrm{IOU}(b,b^{g})=\frac{area(b\cap b^{g})}{area(b\cup b^{g})},$ |  |
    (4) |'
- en: between the predicted BB $b$ and the ground truth $b^{g}$ is not smaller than
    a predefined threshold $\varepsilon$, where $\cap$ and $cup$ denote intersection
    and union, respectively. A typical value of $\varepsilon$ is 0.5.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测的边界框 $b$ 和真实标注 $b^{g}$ 之间的 IOU 不小于预定义的阈值 $\varepsilon$，其中 $\cap$ 和 $cup$ 分别表示交集和并集。$\varepsilon$
    的典型值为 0.5。
- en: Otherwise, it is considered as a False Positive (FP). The confidence level $p$
    is usually compared with some threshold $\beta$ to determine whether the predicted
    class label $c$ is accepted.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，该结果被视为假阳性（FP）。置信度水平 $p$ 通常与某个阈值 $\beta$ 进行比较，以确定预测的类别标签 $c$ 是否被接受。
- en: 'AP is computed separately for each of the object classes, based on *Precision*
    and *Recall*. For a given object class $c$ and a testing image $\textbf{I}_{i}$,
    let $\{(b_{ij},p_{ij})\}_{j=1}^{M}$ denote the detections returned by a detector,
    ranked by confidence $p_{ij}$ in decreasing order. Each detection $(b_{ij},p_{ij})$
    is either a TP or an FP, which can be determined via the algorithm⁴⁴4It is worth
    noting that for a given threshold $\beta$, multiple detections of the same object
    in an image are not considered as all correct detections, and only the detection
    with the highest confidence level is considered as a TP and the rest as FPs. in
    Fig. [10](#S4.F10 "Figure 10 ‣ 4.2 Evaluation Criteria ‣ 4 Datasets and Performance
    Evaluation ‣ Deep Learning for Generic Object Detection: A Survey"). Based on
    the TP and FP detections, the precision $P(\beta)$ and recall $R(\beta)$ Everingham
    et al. ([2010](#bib.bib68)) can be computed as a function of the confidence threshold
    $\beta$, so by varying the confidence threshold different pairs $(P,R)$ can be
    obtained, in principle allowing precision to be regarded as a function of recall,
    *i.e.* $P(R)$, from which the Average Precision (AP) Everingham et al. ([2010](#bib.bib68));
    Russakovsky et al. ([2015](#bib.bib234)) can be found.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 'AP 是根据 *精确度* 和 *召回率* 分别计算的。对于给定的物体类别 $c$ 和测试图像 $\textbf{I}_{i}$，设 $\{(b_{ij},p_{ij})\}_{j=1}^{M}$
    表示由检测器返回的检测结果，按置信度 $p_{ij}$ 递减排序。每个检测 $(b_{ij},p_{ij})$ 是 TP 还是 FP 可以通过算法确定⁴⁴4值得注意的是，对于给定的阈值
    $\beta$，同一图像中的多个检测结果不会被视为全部正确检测，只有置信度最高的检测结果被视为 TP，其余的视为 FP。见图 [10](#S4.F10 "Figure
    10 ‣ 4.2 Evaluation Criteria ‣ 4 Datasets and Performance Evaluation ‣ Deep Learning
    for Generic Object Detection: A Survey")。根据 TP 和 FP 检测，精确度 $P(\beta)$ 和召回率 $R(\beta)$（Everingham
    等人（[2010](#bib.bib68)））可以作为置信度阈值 $\beta$ 的函数进行计算，因此通过调整置信度阈值可以获得不同的 (P, R) 对，从而使得精确度可以视为召回率的函数，即
    $P(R)$，从中可以找到平均精度（AP）（Everingham 等人（[2010](#bib.bib68)）；Russakovsky 等人（[2015](#bib.bib234)））。'
- en: 'Since the introduction of MS COCO, more attention has been placed on the accuracy
    of the bounding box location. Instead of using a fixed IOU threshold, MS COCO
    introduces a few metrics (summarized in Table [5](#S4.T5 "Table 5 ‣ 4.2 Evaluation
    Criteria ‣ 4 Datasets and Performance Evaluation ‣ Deep Learning for Generic Object
    Detection: A Survey")) for characterizing the performance of an object detector.
    For instance, in contrast to the traditional mAP computed at a single IoU of $0.5$,
    $AP_{coco}$ is averaged across all object categories and multiple IOU values from
    $0.5$ to $0.95$ in steps of $0.05$. Because $41\%$ of the objects in MS COCO are
    small and $24\%$ are large, metrics $AP_{coco}^{small}$, $AP_{coco}^{medium}$
    and $AP_{coco}^{large}$ are also introduced. Finally, Table [5](#S4.T5 "Table
    5 ‣ 4.2 Evaluation Criteria ‣ 4 Datasets and Performance Evaluation ‣ Deep Learning
    for Generic Object Detection: A Survey") summarizes the main metrics used in the
    PASCAL, ILSVRC and MS COCO object detection challenges, with metric modifications
    for the Open Images challenges proposed in Kuznetsova et al. ([2018](#bib.bib143)).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 自MS COCO引入以来，更多的关注被放在了边界框位置的准确性上。MS COCO引入了几个指标（总结见表[5](#S4.T5 "表 5 ‣ 4.2 评估标准
    ‣ 4 数据集与性能评估 ‣ 泛用目标检测的深度学习综述")）来表征目标检测器的性能。比如，与传统在IOU $0.5$下计算的mAP相比，$AP_{coco}$在所有物体类别和从$0.5$到$0.95$（步长为$0.05$）的多个IOU值上取平均。由于MS
    COCO中$41\%$的对象为小型且$24\%$为大型，因此还引入了指标$AP_{coco}^{small}$、$AP_{coco}^{medium}$和$AP_{coco}^{large}$。最后，表[5](#S4.T5
    "表 5 ‣ 4.2 评估标准 ‣ 4 数据集与性能评估 ‣ 泛用目标检测的深度学习综述")总结了PASCAL、ILSVRC和MS COCO目标检测挑战中使用的主要指标，并提出了Kuznetsova等（[2018](#bib.bib143)）为Open
    Images挑战提出的指标修改方案。
- en: 'Table 5: Summary of commonly used metrics for evaluating object detectors.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 评估目标检测器常用指标的总结。'
- en: '|      Metric | Meaning | Definition and Description    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|       指标 | 含义 | 定义与描述    |'
- en: '|    TP | True Positive | A true positive detection, per Fig. [10](#S4.F10
    "Figure 10 ‣ 4.2 Evaluation Criteria ‣ 4 Datasets and Performance Evaluation ‣
    Deep Learning for Generic Object Detection: A Survey").    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|   TP | 真阳性 | 真阳性检测，如图[10](#S4.F10 "图 10 ‣ 4.2 评估标准 ‣ 4 数据集与性能评估 ‣ 泛用目标检测的深度学习综述")所示。
       |'
- en: '|    FP | False Positive | A false positive detection, per Fig. [10](#S4.F10
    "Figure 10 ‣ 4.2 Evaluation Criteria ‣ 4 Datasets and Performance Evaluation ‣
    Deep Learning for Generic Object Detection: A Survey").    |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|   FP | 假阳性 | 假阳性检测，如图[10](#S4.F10 "图 10 ‣ 4.2 评估标准 ‣ 4 数据集与性能评估 ‣ 泛用目标检测的深度学习综述")所示。
       |'
- en: '|    $\beta$ | Confidence Threshold | A confidence threshold for computing
    $P(\beta)$ and $R(\beta)$.    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|   $β$ | 置信度阈值 | 用于计算$P(\beta)$和$R(\beta)$的置信度阈值。    |'
- en: '|    $\varepsilon$ | IOU Threshold | VOC | Typically around $0.5$    |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|   $\varepsilon$ | IOU阈值 | VOC | 通常在$0.5$左右    |'
- en: '|   | ILSVRC | $\min(0.5,\frac{wh}{(w+10)(h+10)})$; $w\times h$ is the size
    of a GT box.    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|   | ILSVRC | $\min(0.5,\frac{wh}{(w+10)(h+10)})$; $w\times h$是GT框的大小。   
    |'
- en: '|   | MS COCO | Ten IOU thresholds $\varepsilon\in\{0.5:0.05:0.95\}$    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|   | MS COCO | 十个IOU阈值 $\varepsilon\in\{0.5:0.05:0.95\}$    |'
- en: '|    $P(\beta)$ | Precision | The fraction of correct detections out of the
    total detections returned by the detector with confidence of at least $\beta$.
       |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|   $P(\beta)$ | 精度 | 在检测器返回的所有检测中，置信度至少为$\beta$的正确检测的比例。    |'
- en: '|    $R(\beta)$ | Recall | The fraction of all $N_{c}$ objects detected by
    the detector having a confidence of at least $\beta$.    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|   $R(\beta)$ | 召回率 | 检测器检测到的所有$N_{c}$对象中，置信度至少为$\beta$的比例。    |'
- en: '|    AP | Average Precision | Computed over the different levels of recall
    achieved by varying the confidence $\beta$.    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|   AP | 平均精度 | 在通过改变置信度$\beta$来获得的不同召回率水平上计算。    |'
- en: '|    mAP | mean Average Precision | VOC | AP at a single IOU and averaged over
    all classes.    |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|   mAP | 平均平均精度 | VOC | 在单一IOU下的AP，并对所有类别取平均。    |'
- en: '|   | ILSVRC | AP at a modified IOU and averaged over all classes.    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|   | ILSVRC | 在修改后的IOU下的AP，并对所有类别取平均。    |'
- en: '|   | MS COCO | $\bullet AP_{coco}$: mAP averaged over ten IOUs: $\{0.5:0.05:0.95\}$;
       |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|   | MS COCO | $\bullet AP_{coco}$: 在十个IOU值上取平均的mAP: $\{0.5:0.05:0.95\}$；
       |'
- en: '|   | $\bullet$ $AP^{\textrm{IOU}=0.5}_{coco}$: mAP at IOU=0.50 (PASCAL VOC
    metric); |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|   | $\bullet$ $AP^{\textrm{IOU}=0.5}_{coco}$: IOU=0.50时的mAP（PASCAL VOC指标）；
    |'
- en: '|   | $\bullet$ $AP^{\textrm{IOU}=0.75}_{coco}$: mAP at IOU=0.75 (strict metric);
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|   | $\bullet$ $AP^{\textrm{IOU}=0.75}_{coco}$: IOU=0.75时的mAP（严格指标）； |'
- en: '|   | $\bullet$ $AP^{\textrm{small}}_{coco}$: mAP for small objects of area
    smaller than $32^{2}$; |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|   | $\bullet$ $AP^{\textrm{small}}_{coco}$: 对于面积小于 $32^{2}$ 的小物体的 mAP； |'
- en: '|   | $\bullet$ $AP^{\textrm{medium}}_{coco}$: mAP for objects of area between
    $32^{2}$ and $96^{2}$; |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|   | $\bullet$ $AP^{\textrm{medium}}_{coco}$: 对于面积在 $32^{2}$ 和 $96^{2}$ 之间的物体的
    mAP； |'
- en: '|   | $\bullet$ $AP^{\textrm{large}}_{coco}$: mAP for large objects of area
    bigger than $96^{2}$; |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|   | $\bullet$ $AP^{\textrm{large}}_{coco}$: 对于面积大于 $96^{2}$ 的大物体的 mAP； |'
- en: '|    AR | Average Recall | The maximum recall given a fixed number of detections
    per image, averaged over all categories and IOU thresholds.    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|    AR | 平均召回率 | 在每张图像上给定固定数量的检测时的最大召回率，平均所有类别和 IOU 阈值。    |'
- en: '|    AR | Average Recall | MS COCO | $\bullet AR^{\textrm{max}=1}_{coco}$:
    AR given 1 detection per image;    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|    AR | 平均召回率 | MS COCO | $\bullet AR^{\textrm{max}=1}_{coco}$: 每张图像检测1次时的平均召回率；
       |'
- en: '|   | $\bullet$ $AR^{\textrm{max}=10}_{coco}$: AR given 10 detection per image;
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|   | $\bullet$ $AR^{\textrm{max}=10}_{coco}$: 每张图像检测10次时的平均召回率； |'
- en: '|   | $\bullet$ $AR^{\textrm{max}=100}_{coco}$: AR given 100 detection per
    image; |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|   | $\bullet$ $AR^{\textrm{max}=100}_{coco}$: 每张图像检测100次时的平均召回率； |'
- en: '|   | $\bullet$ $AR^{\textrm{small}}_{coco}$: AR for small objects of area
    smaller than $32^{2}$; |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|   | $\bullet$ $AR^{\textrm{small}}_{coco}$: 对于面积小于 $32^{2}$ 的小物体的平均召回率； |'
- en: '|   | $\bullet$ $AR^{\textrm{medium}}_{coco}$: AR for objects of area between
    $32^{2}$ and $96^{2}$; |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|   | $\bullet$ $AR^{\textrm{medium}}_{coco}$: 对于面积在 $32^{2}$ 和 $96^{2}$ 之间的物体的平均召回率；
    |'
- en: '|   | $\bullet$ $AR^{\textrm{large}}_{coco}$: AR for large objects of area
    bigger than $96^{2}$; |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|   | $\bullet$ $AR^{\textrm{large}}_{coco}$: 对于面积大于 $96^{2}$ 的大物体的平均召回率； |'
- en: '|      |  |  |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|      |  |  |  |'
- en: 5 Detection Frameworks
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个检测框架
- en: There has been steady progress in object feature representations and classifiers
    for recognition, as evidenced by the dramatic change from handcrafted features
    Viola and Jones ([2001](#bib.bib276)); Dalal and Triggs ([2005](#bib.bib52));
    Felzenszwalb et al. ([2008](#bib.bib72)); Harzallah et al. ([2009](#bib.bib98));
    Vedaldi et al. ([2009](#bib.bib275)) to learned DCNN features Girshick et al.
    ([2014](#bib.bib85)); Ouyang et al. ([2015](#bib.bib203)); Girshick ([2015](#bib.bib84));
    Ren et al. ([2015](#bib.bib229)); Dai et al. ([2016c](#bib.bib50)). In contrast,
    in terms of localization, the basic “sliding window” strategy Dalal and Triggs
    ([2005](#bib.bib52)); Felzenszwalb et al. ([2010b](#bib.bib74), [2008](#bib.bib72))
    remains mainstream, although with some efforts to avoid exhaustive search Lampert
    et al. ([2008](#bib.bib145)); Uijlings et al. ([2013](#bib.bib271)). However,
    the number of windows is large and grows quadratically with the number of image
    pixels, and the need to search over multiple scales and aspect ratios further
    increases the search space. Therefore, the design of efficient and effective detection
    frameworks plays a key role in reducing this computational cost. Commonly adopted
    strategies include cascading, sharing feature computation, and reducing per-window
    computation.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在对象特征表示和分类器方面已经取得了稳步进展，这从手工特征到学习的 DCNN 特征的显著变化中可以看出，例如 Viola 和 Jones ([2001](#bib.bib276))；Dalal
    和 Triggs ([2005](#bib.bib52))；Felzenszwalb 等 ([2008](#bib.bib72))；Harzallah 等
    ([2009](#bib.bib98))；Vedaldi 等 ([2009](#bib.bib275)) 变为 Girshick 等 ([2014](#bib.bib85))；Ouyang
    等 ([2015](#bib.bib203))；Girshick ([2015](#bib.bib84))；Ren 等 ([2015](#bib.bib229))；Dai
    等 ([2016c](#bib.bib50))。相比之下，在定位方面，基本的“滑动窗口”策略 Dalal 和 Triggs ([2005](#bib.bib52))；Felzenszwalb
    等 ([2010b](#bib.bib74), [2008](#bib.bib72)) 仍然是主流，尽管有一些努力来避免穷举搜索 Lampert 等 ([2008](#bib.bib145))；Uijlings
    等 ([2013](#bib.bib271))。然而，窗口的数量很大，并且随着图像像素数量的增加而呈二次增长，而需要在多个尺度和纵横比上进行搜索进一步增加了搜索空间。因此，设计高效且有效的检测框架在降低计算成本方面发挥了关键作用。常用的策略包括级联、共享特征计算和减少每个窗口的计算。
- en: 'This section reviews detection frameworks, listed in Fig. [11](#S5.F11 "Figure
    11 ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object Detection: A Survey")
    and Table [11](#S10.T11 "Table 11 ‣ 10.3 Research Directions ‣ 10 Discussion and
    Conclusion ‣ Deep Learning for Generic Object Detection: A Survey"), the milestone
    approaches appearing since deep learning entered the field, organized into two
    main categories:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '本节回顾了检测框架，列于图 [11](#S5.F11 "Figure 11 ‣ 5 Detection Frameworks ‣ Deep Learning
    for Generic Object Detection: A Survey") 和表 [11](#S10.T11 "Table 11 ‣ 10.3 Research
    Directions ‣ 10 Discussion and Conclusion ‣ Deep Learning for Generic Object Detection:
    A Survey") 中，这些里程碑方法自深度学习进入该领域以来出现，分为两个主要类别：'
- en: a.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: a.
- en: Two stage detection frameworks, which include a preprocessing step for generating
    object proposals;
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两阶段检测框架，包括用于生成目标提议的预处理步骤；
- en: b.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b.
- en: One stage detection frameworks, or region proposal free frameworks, having a
    single proposed method which does not separate the process of the detection proposal.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一阶段检测框架，或称为区域提议自由框架，具有单一的提议方法，该方法并不将检测提议的过程分开。
- en: 'Sections [6](#S6 "6 Object Representation ‣ Deep Learning for Generic Object
    Detection: A Survey") through [9](#S9 "9 Other Issues ‣ Deep Learning for Generic
    Object Detection: A Survey") will discuss fundamental sub-problems involved in
    detection frameworks in greater detail, including DCNN features, detection proposals,
    and context modeling.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '第 [6](#S6 "6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey") 节到第 [9](#S9 "9 Other Issues ‣ Deep Learning for Generic Object Detection:
    A Survey") 节将更详细地讨论检测框架中涉及的基本子问题，包括DCNN特征、检测提议和上下文建模。'
- en: '![Refer to caption](img/ecb1fb7ad43991c5daf6d3542f54614f.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ecb1fb7ad43991c5daf6d3542f54614f.png)'
- en: 'Figure 11: Milestones in generic object detection.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：通用目标检测中的里程碑。
- en: 5.1 Region Based (Two Stage) Frameworks
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基于区域的（两阶段）框架
- en: 'In a region-based framework, category-independent region proposals⁵⁵5Object
    proposals, also called region proposals or detection proposals, are a set of candidate
    regions or bounding boxes in an image that may potentially contain an object.
    Chavali et al. ([2016](#bib.bib27)); Hosang et al. ([2016](#bib.bib110)) are generated
    from an image, CNN Krizhevsky et al. ([2012a](#bib.bib140)) features are extracted
    from these regions, and then category-specific classifiers are used to determine
    the category labels of the proposals. As can be observed from Fig. [11](#S5.F11
    "Figure 11 ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object Detection:
    A Survey"), DetectorNet Szegedy et al. ([2013](#bib.bib261)), OverFeat Sermanet
    et al. ([2014](#bib.bib239)), MultiBox Erhan et al. ([2014](#bib.bib67)) and RCNN
    Girshick et al. ([2014](#bib.bib85)) independently and almost simultaneously proposed
    using CNNs for generic object detection.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '在基于区域的框架中，类别无关的区域提议⁵⁵5物体提议，也称为区域提议或检测提议，是图像中可能包含物体的一组候选区域或边界框。Chavali 等人 ([2016](#bib.bib27));
    Hosang 等人 ([2016](#bib.bib110)) 是从图像中生成的，从这些区域中提取CNN Krizhevsky 等人 ([2012a](#bib.bib140))
    特征，然后使用类别特定的分类器来确定提议的类别标签。如图 [11](#S5.F11 "Figure 11 ‣ 5 Detection Frameworks
    ‣ Deep Learning for Generic Object Detection: A Survey") 所示，DetectorNet Szegedy
    等人 ([2013](#bib.bib261))、OverFeat Sermanet 等人 ([2014](#bib.bib239))、MultiBox Erhan
    等人 ([2014](#bib.bib67)) 和 RCNN Girshick 等人 ([2014](#bib.bib85)) 独立且几乎同时地提出了使用CNN进行通用目标检测。'
- en: '![Refer to caption](img/8563a18af7285490db757ebf5e1bb057.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8563a18af7285490db757ebf5e1bb057.png)'
- en: 'Figure 12: Illustration of the RCNN detection framework Girshick et al. ([2014](#bib.bib85),
    [2016](#bib.bib87)).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：RCNN检测框架 Girshick 等人 ([2014](#bib.bib85), [2016](#bib.bib87)) 的示意图。
- en: '![Refer to caption](img/e09a3497284045915aa6246098b2bd8a.png)![Refer to caption](img/117a19c491c36c3170ba312acc631cf4.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e09a3497284045915aa6246098b2bd8a.png)![参见标题](img/117a19c491c36c3170ba312acc631cf4.png)'
- en: 'Figure 13: High level diagrams of the leading frameworks for generic object
    detection. The properties of these methods are summarized in Table [11](#S10.T11
    "Table 11 ‣ 10.3 Research Directions ‣ 10 Discussion and Conclusion ‣ Deep Learning
    for Generic Object Detection: A Survey").'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13：通用目标检测的主要框架的高级示意图。这些方法的属性在表格 [11](#S10.T11 "Table 11 ‣ 10.3 Research Directions
    ‣ 10 Discussion and Conclusion ‣ Deep Learning for Generic Object Detection: A
    Survey") 中进行了总结。'
- en: 'RCNN Girshick et al. ([2014](#bib.bib85)): Inspired by the breakthrough image
    classification results obtained by CNNs and the success of the selective search
    in region proposal for handcrafted features Uijlings et al. ([2013](#bib.bib271)),
    Girshick *et al.* were among the first to explore CNNs for generic object detection
    and developed RCNN Girshick et al. ([2014](#bib.bib85), [2016](#bib.bib87)), which
    integrates AlexNet Krizhevsky et al. ([2012a](#bib.bib140)) with a region proposal
    selective search Uijlings et al. ([2013](#bib.bib271)). As illustrated in detail
    in Fig. [12](#S5.F12 "Figure 12 ‣ 5.1 Region Based (Two Stage) Frameworks ‣ 5
    Detection Frameworks ‣ Deep Learning for Generic Object Detection: A Survey"),
    training an RCNN framework consists of multistage pipelines:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 'RCNN Girshick 等人 ([2014](#bib.bib85))：受CNN在图像分类结果突破以及Uijlings 等人 ([2013](#bib.bib271))
    在手工特征的区域提议方面成功的启发，Girshick *等人* 是最早探索CNN用于通用目标检测的研究者之一，并开发了RCNN Girshick 等人 ([2014](#bib.bib85),
    [2016](#bib.bib87))，该方法将AlexNet Krizhevsky 等人 ([2012a](#bib.bib140)) 与区域提议选择性搜索
    Uijlings 等人 ([2013](#bib.bib271)) 集成在一起。如图 [12](#S5.F12 "Figure 12 ‣ 5.1 Region
    Based (Two Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep Learning for Generic
    Object Detection: A Survey") 详细说明，训练RCNN框架包含多阶段的流程：'
- en: '1.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '*Region proposal computation:* Class agnostic region proposals, which are candidate
    regions that might contain objects, are obtained via a selective search Uijlings
    et al. ([2013](#bib.bib271)).'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*区域提议计算：* 通过选择性搜索Uijlings等人（[2013](#bib.bib271)）获得的与类别无关的区域提议，这些是可能包含对象的候选区域。'
- en: '2.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '*CNN model finetuning:* Region proposals, which are cropped from the image
    and warped into the same size, are used as the input for fine-tuning a CNN model
    pre-trained using a large-scale dataset such as ImageNet. At this stage, all region
    proposals with $\geqslant 0.5$ IOU ⁶⁶6Please refer to Section [4.2](#S4.SS2 "4.2
    Evaluation Criteria ‣ 4 Datasets and Performance Evaluation ‣ Deep Learning for
    Generic Object Detection: A Survey") for the definition of IOU. overlap with a
    ground truth box are defined as positives for that ground truth box’s class and
    the rest as negatives.'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*CNN模型微调：* 从图像中裁剪并变形为相同大小的区域提议用作微调CNN模型的输入，该模型是使用如ImageNet等大规模数据集预训练的。在这一阶段，所有与真实框有$\geqslant
    0.5$ IOU ⁶⁶6请参考第[4.2节](#S4.SS2 "4.2 评估标准 ‣ 4 数据集和性能评估 ‣ 泛用目标检测深度学习：综述")定义的IOU重叠的区域提议被定义为该真实框类别的正样本，其余为负样本。'
- en: '3.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: '*Class specific SVM classifiers training:* A set of class-specific linear SVM
    classifiers are trained using fixed length features extracted with CNN, replacing
    the softmax classifier learned by fine-tuning. For training SVM classifiers, positive
    examples are defined to be the ground truth boxes for each class. A region proposal
    with less than 0.3 IOU overlap with all ground truth instances of a class is negative
    for that class. Note that the positive and negative examples defined for training
    the SVM classifiers are different from those for fine-tuning the CNN.'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*类别特定SVM分类器训练：* 使用CNN提取的固定长度特征训练一组类别特定的线性SVM分类器，替代了通过微调学习的softmax分类器。在训练SVM分类器时，正样本被定义为每个类别的真实框。与该类别的所有真实框重叠小于0.3
    IOU的区域提议对该类别来说是负样本。请注意，用于训练SVM分类器的正负样本与用于微调CNN的样本不同。'
- en: '4.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: '*Class specific bounding box regressor training:* Bounding box regression is
    learned for each object class with CNN features.'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*类别特定的边界框回归器训练：* 对每个对象类别使用CNN特征学习边界框回归。'
- en: 'In spite of achieving high object detection quality, RCNN has notable drawbacks
    Girshick ([2015](#bib.bib84)):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管达到了高质量的目标检测，RCNN仍然存在显著的缺点Girshick（[2015](#bib.bib84)）：
- en: '1.'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Training is a multistage pipeline, slow and hard to optimize because each individual
    stage must be trained separately.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练是一个多阶段的管道，速度慢且难以优化，因为每个独立阶段必须分别训练。
- en: '2.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: For SVM classifier and bounding box regressor training, it is expensive in both
    disk space and time, because CNN features need to be extracted from each object
    proposal in each image, posing great challenges for large scale detection, particularly
    with very deep networks, such as VGG16 Simonyan and Zisserman ([2015](#bib.bib248)).
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于SVM分类器和边界框回归器的训练，在磁盘空间和时间上都是昂贵的，因为需要从每张图像中的每个对象提议中提取CNN特征，这对大规模检测构成了巨大挑战，特别是对于非常深的网络，如VGG16
    Simonyan和Zisserman（[2015](#bib.bib248)）。
- en: '3.'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Testing is slow, since CNN features are extracted per object proposal in each
    test image, without shared computation.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试速度慢，因为每张测试图像中的CNN特征是对每个对象提议单独提取的，没有共享计算。
- en: All of these drawbacks have motivated successive innovations, leading to a number
    of improved detection frameworks such as SPPNet, Fast RCNN, Faster RCNN *etc*.,
    as follows.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些缺点促使了后续的创新，导致了许多改进的检测框架，如SPPNet、Fast RCNN、Faster RCNN *等*，如下所示。
- en: 'SPPNet He et al. ([2014](#bib.bib99)): During testing, CNN feature extraction
    is the main bottleneck of the RCNN detection pipeline, which requires the extraction
    of CNN features from thousands of warped region proposals per image. As a result,
    He *et al.* He et al. ([2014](#bib.bib99)) introduced traditional spatial pyramid
    pooling (SPP) Grauman and Darrell ([2005](#bib.bib90)); Lazebnik et al. ([2006](#bib.bib147))
    into CNN architectures. Since convolutional layers accept inputs of arbitrary
    sizes, the requirement of fixed-sized images in CNNs is due only to the Fully
    Connected (FC) layers, therefore He *et al.* added an SPP layer on top of the
    last convolutional (CONV) layer to obtain features of fixed length for the FC
    layers. With this SPPNet, RCNN obtains a significant speedup without sacrificing
    any detection quality, because it only needs to run the convolutional layers *o*nce
    on the entire test image to generate fixed-length features for region proposals
    of arbitrary size. While SPPNet accelerates RCNN evaluation by orders of magnitude,
    it does not result in a comparable speedup of the detector training. Moreover,
    fine-tuning in SPPNet He et al. ([2014](#bib.bib99)) is unable to update the convolutional
    layers before the SPP layer, which limits the accuracy of very deep networks.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: SPPNet He 等人 ([2014](#bib.bib99))：在测试过程中，CNN 特征提取是 RCNN 检测管道的主要瓶颈，因为它需要从每张图像中的数千个变形区域提案中提取
    CNN 特征。因此，He *等人* He 等人 ([2014](#bib.bib99)) 将传统的空间金字塔池化（SPP） Grauman 和 Darrell
    ([2005](#bib.bib90)); Lazebnik 等人 ([2006](#bib.bib147)) 引入到 CNN 架构中。由于卷积层可以接受任意大小的输入，CNN
    中对固定大小图像的要求仅由于全连接（FC）层，因此 He *等人* 在最后一个卷积（CONV）层上添加了一个 SPP 层，以获取固定长度的特征用于 FC 层。使用这个
    SPPNet，RCNN 实现了显著的加速而不牺牲任何检测质量，因为它只需在整个测试图像上运行一次卷积层即可为任意大小的区域提案生成固定长度的特征。虽然 SPPNet
    将 RCNN 评估的速度提高了几个数量级，但它没有带来检测器训练的相应加速。此外，SPPNet He 等人 ([2014](#bib.bib99)) 中的微调无法更新
    SPP 层之前的卷积层，这限制了非常深网络的准确性。
- en: 'Fast RCNN Girshick ([2015](#bib.bib84)): Girshick proposed Fast RCNN Girshick
    ([2015](#bib.bib84)) that addresses some of the disadvantages of RCNN and SPPNet,
    while improving on their detection speed and quality. As illustrated in Fig. [13](#S5.F13
    "Figure 13 ‣ 5.1 Region Based (Two Stage) Frameworks ‣ 5 Detection Frameworks
    ‣ Deep Learning for Generic Object Detection: A Survey"), Fast RCNN enables end-to-end
    detector training by developing a streamlined training process that simultaneously
    learns a softmax classifier and class-specific bounding box regression, rather
    than separately training a softmax classifier, SVMs, and Bounding Box Regressors
    (BBRs) as in RCNN/SPPNet. Fast RCNN employs the idea of sharing the computation
    of convolution across region proposals, and adds a Region of Interest (RoI) pooling
    layer between the last CONV layer and the first FC layer to extract a fixed-length
    feature for each region proposal. Essentially, RoI pooling uses warping at the
    feature level to approximate warping at the image level. The features after the
    RoI pooling layer are fed into a sequence of FC layers that finally branch into
    two sibling output layers: softmax probabilities for object category prediction,
    and class-specific bounding box regression offsets for proposal refinement. Compared
    to RCNN/SPPNet, Fast RCNN improves the efficiency considerably – typically 3 times
    faster in training and 10 times faster in testing. Thus there is higher detection
    quality, a single training process that updates all network layers, and no storage
    required for feature caching.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 'Fast RCNN Girshick ([2015](#bib.bib84))：Girshick 提出了 Fast RCNN Girshick ([2015](#bib.bib84))，它解决了
    RCNN 和 SPPNet 的一些缺点，同时提高了检测速度和质量。如图 [13](#S5.F13 "Figure 13 ‣ 5.1 Region Based
    (Two Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object
    Detection: A Survey") 所示，Fast RCNN 通过开发一个简化的训练过程来实现端到端的检测器训练，这个过程同时学习了 softmax
    分类器和类别特定的边界框回归，而不是像 RCNN/SPPNet 中那样单独训练 softmax 分类器、SVM 和边界框回归器（BBRs）。Fast RCNN
    采用了在区域提案中共享卷积计算的思想，并在最后一个 CONV 层和第一个 FC 层之间添加了一个兴趣区域（RoI）池化层，以提取每个区域提案的固定长度特征。从
    RoI 池化层获得的特征会输入到一系列 FC 层中，最后分支为两个姐妹输出层：用于物体类别预测的 softmax 概率，以及用于提案精细化的类别特定边界框回归偏移。与
    RCNN/SPPNet 相比，Fast RCNN 显著提高了效率——训练速度通常快 3 倍，测试速度快 10 倍。因此，检测质量更高，单一训练过程可以更新所有网络层，并且不需要存储特征缓存。'
- en: 'Faster RCNN Ren et al. ([2015](#bib.bib229), [2017a](#bib.bib230)): Although
    Fast RCNN significantly sped up the detection process, it still relies on external
    region proposals, whose computation is exposed as the new speed bottleneck in
    Fast RCNN. Recent work has shown that CNNs have a remarkable ability to localize
    objects in CONV layers Zhou et al. ([2015](#bib.bib317), [2016a](#bib.bib318));
    Cinbis et al. ([2017](#bib.bib46)); Oquab et al. ([2015](#bib.bib200)); Hariharan
    et al. ([2016](#bib.bib97)), an ability which is weakened in the FC layers. Therefore,
    the selective search can be replaced by a CNN in producing region proposals. The
    Faster RCNN framework proposed by Ren *et al.* Ren et al. ([2015](#bib.bib229),
    [2017a](#bib.bib230)) offered an efficient and accurate Region Proposal Network
    (RPN) for generating region proposals. They utilize the same backbone network,
    using features from the last shared convolutional layer to accomplish the task
    of RPN for region proposal and Fast RCNN for region classification, as shown in
    Fig. [13](#S5.F13 "Figure 13 ‣ 5.1 Region Based (Two Stage) Frameworks ‣ 5 Detection
    Frameworks ‣ Deep Learning for Generic Object Detection: A Survey").'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 'Faster RCNN Ren et al. ([2015](#bib.bib229), [2017a](#bib.bib230))：尽管Fast RCNN显著加快了检测过程，但它仍然依赖于外部区域建议，而这些建议的计算暴露了Fast
    RCNN中的新速度瓶颈。最近的研究表明，CNN在CONV层中具有显著的物体定位能力 Zhou et al. ([2015](#bib.bib317), [2016a](#bib.bib318));
    Cinbis et al. ([2017](#bib.bib46)); Oquab et al. ([2015](#bib.bib200)); Hariharan
    et al. ([2016](#bib.bib97))，这种能力在FC层中减弱。因此，选择性搜索可以被CNN替代，用于生成区域建议。Ren *et al.*
    提出的Faster RCNN框架 Ren et al. ([2015](#bib.bib229), [2017a](#bib.bib230)) 提供了一个高效且准确的区域建议网络（RPN）来生成区域建议。他们利用相同的骨干网络，使用最后一个共享卷积层的特征来完成RPN的区域建议任务和Fast
    RCNN的区域分类任务，如图[13](#S5.F13 "Figure 13 ‣ 5.1 Region Based (Two Stage) Frameworks
    ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object Detection: A Survey")所示。'
- en: RPN first initializes $k$ reference boxes (*i.e.* the so called *anchors*) of
    different scales and aspect ratios at each CONV feature map location. The anchor
    *p*ositions are image content independent, but the feature vectors themselves,
    extracted from anchors, are image content dependent. Each anchor is mapped to
    a lower dimensional vector, which is fed into two sibling FC layers — an object
    category classification layer and a box regression layer. In contrast to detection
    in Fast RCNN, the features used for regression in RPN are of the same shape as
    the anchor box, thus $k$ anchors lead to $k$ regressors. RPN shares CONV features
    with Fast RCNN, thus enabling highly efficient region proposal computation. RPN
    is, in fact, a kind of Fully Convolutional Network (FCN) Long et al. ([2015](#bib.bib177));
    Shelhamer et al. ([2017](#bib.bib241)); Faster RCNN is thus a purely CNN based
    framework without using handcrafted features.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: RPN首先在每个CONV特征图位置初始化$k$个参考框（*即*所谓的*anchors*），这些框具有不同的尺度和长宽比。锚点的位置与图像内容无关，但从锚点中提取的特征向量则依赖于图像内容。每个锚点被映射到一个低维向量，输入到两个兄弟FC层——一个对象类别分类层和一个框回归层。与Fast
    RCNN中的检测相比，RPN中用于回归的特征与锚框的形状相同，因此$k$个锚点产生$k$个回归器。RPN与Fast RCNN共享CONV特征，从而实现了高效的区域建议计算。实际上，RPN是一种全卷积网络（FCN）
    Long et al. ([2015](#bib.bib177)); Shelhamer et al. ([2017](#bib.bib241)); Faster
    RCNN因此是一个纯粹基于CNN的框架，没有使用手工特征。
- en: For the VGG16 model Simonyan and Zisserman ([2015](#bib.bib248)), Faster RCNN
    can test at 5 FPS (including all stages) on a GPU, while achieving state-of-the-art
    object detection accuracy on PASCAL VOC 2007 using 300 proposals per image. The
    initial Faster RCNN in Ren et al. ([2015](#bib.bib229)) contains several alternating
    training stages, later simplified in Ren et al. ([2017a](#bib.bib230)).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对于VGG16模型 Simonyan和Zisserman ([2015](#bib.bib248))，Faster RCNN可以在GPU上以5 FPS的速度测试（包括所有阶段），同时在PASCAL
    VOC 2007上使用每张图像300个建议实现了最先进的目标检测精度。Ren et al. ([2015](#bib.bib229))中的初始Faster
    RCNN包含几个交替训练阶段，后在Ren et al. ([2017a](#bib.bib230))中简化。
- en: Concurrent with the development of Faster RCNN, Lenc and Vedaldi Lenc and Vedaldi
    ([2015](#bib.bib151)) challenged the role of region proposal generation methods
    such as selective search, studied the role of region proposal generation in CNN
    based detectors, and found that CNNs contain sufficient geometric information
    for accurate object detection in the CONV rather than FC layers. They showed the
    possibility of building integrated, simpler, and faster object detectors that
    rely exclusively on CNNs, removing region proposal generation methods such as
    selective search.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 与Faster RCNN的发展同步，Lenc和Vedaldi（[2015](#bib.bib151)）质疑了区域提议生成方法（如选择性搜索）的作用，研究了区域提议生成在基于CNN的检测器中的作用，并发现CNN在CONV层中包含足够的几何信息以实现准确的物体检测，而不是在FC层中。他们展示了构建集成的、更简单且更快速的仅依赖CNN的物体检测器的可能性，从而移除了诸如选择性搜索的区域提议生成方法。
- en: 'RFCN (Region based Fully Convolutional Network): While Faster RCNN is an order
    of magnitude faster than Fast RCNN, the fact that the region-wise sub-network
    still needs to be applied per RoI (several hundred RoIs per image) led Dai *et
    al.* Dai et al. ([2016c](#bib.bib50)) to propose the RFCN detector which is *fully
    convolutional* (no hidden FC layers) with almost all computations shared over
    the entire image. As shown in Fig. [13](#S5.F13 "Figure 13 ‣ 5.1 Region Based
    (Two Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object
    Detection: A Survey"), RFCN differs from Faster RCNN only in the RoI sub-network.
    In Faster RCNN, the computation after the RoI pooling layer cannot be shared,
    so Dai *et al.* Dai et al. ([2016c](#bib.bib50)) proposed using all CONV layers
    to construct a shared RoI sub-network, and RoI crops are taken from the last layer
    of CONV features prior to prediction. However, Dai *et al.* Dai et al. ([2016c](#bib.bib50))
    found that this naive design turns out to have considerably inferior detection
    accuracy, conjectured to be that deeper CONV layers are more sensitive to category
    semantics, and less sensitive to translation, whereas object detection needs localization
    representations that respect translation invariance. Based on this observation,
    Dai *et al.* Dai et al. ([2016c](#bib.bib50)) constructed a set of position-sensitive
    score maps by using a bank of specialized CONV layers as the FCN output, on top
    of which a position-sensitive RoI pooling layer is added. They showed that RFCN
    with ResNet101 He et al. ([2016](#bib.bib101)) could achieve comparable accuracy
    to Faster RCNN, often at faster running times.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 'RFCN（基于区域的全卷积网络）：尽管Faster RCNN的速度比Fast RCNN快一个数量级，但由于区域级子网络仍需在每个RoI（每张图像有数百个RoI）上应用，这促使Dai
    *et al.*（Dai et al. ([2016c](#bib.bib50))）提出了RFCN检测器，它是*全卷积*的（没有隐藏的FC层），几乎所有计算都在整张图像上共享。如图[13](#S5.F13
    "Figure 13 ‣ 5.1 Region Based (Two Stage) Frameworks ‣ 5 Detection Frameworks
    ‣ Deep Learning for Generic Object Detection: A Survey")所示，RFCN与Faster RCNN的区别仅在于RoI子网络。在Faster
    RCNN中，RoI池化层后的计算不能被共享，因此Dai *et al.*（Dai et al. ([2016c](#bib.bib50))）提出使用所有CONV层构建一个共享的RoI子网络，RoI裁剪是在预测之前从CONV特征的最后一层获取的。然而，Dai
    *et al.*（Dai et al. ([2016c](#bib.bib50))）发现这一简单设计的检测准确度相当低，推测是因为更深的CONV层对类别语义更敏感，对平移不那么敏感，而物体检测需要尊重平移不变性的定位表示。基于这一观察，Dai
    *et al.*（Dai et al. ([2016c](#bib.bib50))）通过使用一组专门的CONV层作为FCN输出，构建了一组位置敏感的得分图，在其上添加了一个位置敏感的RoI池化层。他们展示了RFCN与ResNet101（He
    et al. ([2016](#bib.bib101))）能够达到与Faster RCNN相媲美的准确度，且通常具有更快的运行时间。'
- en: 'Mask RCNN: He *et al.* He et al. ([2017](#bib.bib102)) proposed Mask RCNN to
    tackle pixelwise object instance segmentation by extending Faster RCNN. Mask RCNN
    adopts the same two stage pipeline, with an identical first stage (RPN), but in
    the second stage, in parallel to predicting the class and box offset, Mask RCNN
    adds a branch which outputs a binary mask for each RoI. The new branch is a Fully
    Convolutional Network (FCN) Long et al. ([2015](#bib.bib177)); Shelhamer et al.
    ([2017](#bib.bib241)) on top of a CNN feature map. In order to avoid the misalignments
    caused by the original RoI pooling (RoIPool) layer, a RoIAlign layer was proposed
    to preserve the pixel level spatial correspondence. With a backbone network ResNeXt101-FPN
    Xie et al. ([2017](#bib.bib291)); Lin et al. ([2017a](#bib.bib167)), Mask RCNN
    achieved top results for the COCO object instance segmentation and bounding box
    object detection. It is simple to train, generalizes well, and adds only a small
    overhead to Faster RCNN, running at 5 FPS He et al. ([2017](#bib.bib102)).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Mask RCNN：**He et al.** ([2017](#bib.bib102)) 提出了 Mask RCNN，通过扩展 Faster RCNN
    来处理像素级别的物体实例分割。Mask RCNN 采用了相同的两阶段流程，第一阶段（RPN）相同，但在第二阶段，除了预测类别和框偏移之外，Mask RCNN
    增加了一个分支，该分支为每个 RoI 输出二进制掩膜。这个新分支是一个全卷积网络（FCN），由 **Long et al.** ([2015](#bib.bib177))
    和 **Shelhamer et al.** ([2017](#bib.bib241)) 提出的，建立在 CNN 特征图之上。为了避免由原始 RoI 池化（RoIPool）层引起的错位，提出了
    RoIAlign 层以保留像素级的空间对应关系。在一个骨干网络 ResNeXt101-FPN **Xie et al.** ([2017](#bib.bib291));
    **Lin et al.** ([2017a](#bib.bib167)) 的支持下，Mask RCNN 在 COCO 物体实例分割和边界框物体检测中取得了最佳结果。它训练简单，泛化效果良好，并且仅对
    Faster RCNN 增加了少量开销，以每秒 5 帧的速度运行 **He et al.** ([2017](#bib.bib102))。
- en: 'Chained Cascade Network and Cascade RCNN: The essence of cascade Felzenszwalb
    et al. ([2010a](#bib.bib73)); Bourdev and Brandt ([2005](#bib.bib20)); Li and
    Zhang ([2004](#bib.bib159)) is to learn more discriminative classifiers by using
    multistage classifiers, such that early stages discard a large number of easy
    negative samples so that later stages can focus on handling more difficult examples.
    Two-stage object detection can be considered as a cascade, the first detector
    removing large amounts of background, and the second stage classifying the remaining
    regions. Recently, end-to-end learning of more than two cascaded classifiers and
    DCNNs for generic object detection were proposed in the Chained Cascade Network
    Ouyang et al. ([2017a](#bib.bib205)), extended in Cascade RCNN Cai and Vasconcelos
    ([2018](#bib.bib23)), and more recently applied for simultaneous object detection
    and instance segmentation Chen et al. ([2019a](#bib.bib31)), winning the COCO
    2018 Detection Challenge.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 链式级联网络和级联 RCNN：级联的本质 **Felzenszwalb et al.** ([2010a](#bib.bib73)); **Bourdev
    and Brandt** ([2005](#bib.bib20)); **Li and Zhang** ([2004](#bib.bib159)) 是通过使用多阶段分类器来学习更具辨别力的分类器，使早期阶段丢弃大量容易的负样本，以便后续阶段可以专注于处理更困难的例子。两阶段物体检测可以被视为级联，第一阶段检测器去除大量背景，第二阶段对剩余区域进行分类。最近，**Ouyang
    et al.** ([2017a](#bib.bib205)) 在链式级联网络中提出了对两个以上级联分类器和 DCNN 的端到端学习，**Cai and Vasconcelos**
    ([2018](#bib.bib23)) 在级联 RCNN 中进行了扩展，**Chen et al.** ([2019a](#bib.bib31)) 最近将其应用于同时物体检测和实例分割，赢得了
    COCO 2018 检测挑战赛。
- en: 'Light Head RCNN: In order to further increase the detection speed of RFCN Dai
    et al. ([2016c](#bib.bib50)), Li *et al.* Li et al. ([2018c](#bib.bib165)) proposed
    Light Head RCNN, making the head of the detection network as light as possible
    to reduce the RoI computation. In particular, Li *et al.* Li et al. ([2018c](#bib.bib165))
    applied a convolution to produce thin feature maps with small channel numbers
    (*e.g.,* 490 channels for COCO) and a cheap RCNN sub-network, leading to an excellent
    trade-off of speed and accuracy.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Light Head RCNN：为了进一步提高 RFCN **Dai et al.** ([2016c](#bib.bib50)) 的检测速度，**Li
    et al.** ([2018c](#bib.bib165)) 提出了 Light Head RCNN，使检测网络的头尽可能轻量化，以减少 RoI 计算。特别是，**Li
    et al.** ([2018c](#bib.bib165)) 应用了卷积来生成具有小通道数的细特征图（*例如*，COCO 的 490 个通道）和一个廉价的
    RCNN 子网络，从而实现了速度和准确性的优良折中。
- en: 5.2 Unified (One Stage) Frameworks
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 统一（单阶段）框架
- en: 'The region-based pipeline strategies of Section [5.1](#S5.SS1 "5.1 Region Based
    (Two Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object
    Detection: A Survey") have dominated since RCNN Girshick et al. ([2014](#bib.bib85)),
    such that the leading results on popular benchmark datasets are all based on Faster
    RCNN Ren et al. ([2015](#bib.bib229)). Nevertheless, region-based approaches are
    computationally expensive for current mobile/wearable devices, which have limited
    storage and computational capability, therefore instead of trying to optimize
    the individual components of a complex region-based pipeline, researchers have
    begun to develop *unified* detection strategies.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '自RCNN Girshick等（[2014](#bib.bib85)）以来，第[5.1节](#S5.SS1 "5.1 Region Based (Two
    Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object
    Detection: A Survey")的区域基础管道策略已经主导了领域，因此在流行基准数据集上的领先结果都基于Faster RCNN Ren等（[2015](#bib.bib229)）。然而，对于当前的移动/可穿戴设备而言，区域基础方法在计算上非常昂贵，这些设备具有有限的存储和计算能力，因此，研究人员开始开发*统一*检测策略，而不是尝试优化复杂区域基础管道的各个组件。'
- en: Unified pipelines refer to architectures that directly predict class probabilities
    and bounding box offsets from full images with a single feed-forward CNN in a
    monolithic setting that does not involve region proposal generation or post classification
    / feature resampling, encapsulating all computation in a single network. Since
    the whole pipeline is a single network, it can be optimized end-to-end directly
    on detection performance.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 统一管道指的是一种架构，该架构通过一个单一的前馈卷积神经网络（CNN）从完整图像中直接预测类别概率和边界框偏移量，在一个不涉及区域提议生成或后期分类/特征重采样的单体设置中，将所有计算封装在一个网络中。由于整个管道是一个单一的网络，它可以直接在检测性能上进行端到端优化。
- en: 'DetectorNet: Szegedy *et al.* Szegedy et al. ([2013](#bib.bib261)) were among
    the first to explore CNNs for object detection. DetectorNet formulated object
    detection a regression problem to object bounding box masks. They use AlexNet
    Krizhevsky et al. ([2012a](#bib.bib140)) and replace the final softmax classifier
    layer with a regression layer. Given an image window, they use one network to
    predict foreground pixels over a coarse grid, as well as four additional networks
    to predict the object’s top, bottom, left and right halves. A grouping process
    then converts the predicted masks into detected bounding boxes. The network needs
    to be trained per object type and mask type, and does not scale to multiple classes.
    DetectorNet must take many crops of the image, and run multiple networks for each
    part on every crop, thus making it slow.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 'DetectorNet: Szegedy *等* Szegedy等（[2013](#bib.bib261)）是最早探索用于目标检测的CNN的研究者之一。DetectorNet将目标检测公式化为对目标边界框掩膜的回归问题。他们使用了AlexNet
    Krizhevsky等（[2012a](#bib.bib140)），并将最终的softmax分类层替换为回归层。给定一个图像窗口，他们使用一个网络来预测粗网格上的前景像素，同时使用另外四个网络来预测对象的顶部、底部、左侧和右侧。然后，分组过程将预测的掩膜转换为检测到的边界框。网络需要针对每种对象类型和掩膜类型进行训练，并且无法扩展到多个类别。DetectorNet必须对图像进行许多裁剪，并且对每个裁剪的每个部分运行多个网络，因此速度较慢。'
- en: '![Refer to caption](img/7ab802a8951b7812083522b376d2f21a.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/7ab802a8951b7812083522b376d2f21a.png)'
- en: 'Figure 14: Illustration of the OverFeat Sermanet et al. ([2014](#bib.bib239))
    detection framework.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：OverFeat Sermanet等（[2014](#bib.bib239)）检测框架的示意图。
- en: 'OverFeat, proposed by Sermanet *et al.* Sermanet et al. ([2014](#bib.bib239))
    and illustrated in Fig. [14](#S5.F14 "Figure 14 ‣ 5.2 Unified (One Stage) Frameworks
    ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object Detection: A Survey"),
    can be considered as one of the first single-stage object detectors based on fully
    convolutional deep networks. It is one of the most influential object detection
    frameworks, winning the ILSVRC2013 localization and detection competition. OverFeat
    performs object detection via a single forward pass through the fully convolutional
    layers in the network (*i.e.* the “Feature Extractor”, shown in Fig. [14](#S5.F14
    "Figure 14 ‣ 5.2 Unified (One Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep
    Learning for Generic Object Detection: A Survey") (a)). The key steps of object
    detection at test time can be summarized as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 'OverFeat，由 Sermanet *等人* 提出的 Sermanet 等人（[2014](#bib.bib239)）并在图 [14](#S5.F14
    "Figure 14 ‣ 5.2 Unified (One Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep
    Learning for Generic Object Detection: A Survey") 中说明，可以被视为基于全卷积深度网络的第一个单阶段目标检测器之一。它是最具影响力的目标检测框架之一，赢得了
    ILSVRC2013 定位和检测竞赛。OverFeat 通过网络中的全卷积层进行单次前向传播来执行目标检测（*即* “特征提取器”，如图 [14](#S5.F14
    "Figure 14 ‣ 5.2 Unified (One Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep
    Learning for Generic Object Detection: A Survey") (a) 所示）。测试时的关键步骤可以总结如下：'
- en: '1.'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '*Generate object candidates by performing object classification via a sliding
    window fashion on multiscale images.* OverFeat uses a CNN like AlexNet Krizhevsky
    et al. ([2012a](#bib.bib140)), which would require input images ofa fixed size
    due to its fully connected layers, in order to make the sliding window approach
    computationally efficient, OverFeat casts the network (as shown in Fig. [14](#S5.F14
    "Figure 14 ‣ 5.2 Unified (One Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep
    Learning for Generic Object Detection: A Survey") (a)) into a fully convolutional
    network, taking inputs of any size, by viewing fully connected layers as convolutions
    with kernels of size $1\times 1$. OverFeat leverages multiscale features to improve
    the overall performance by passing up to six enlarged scales of the original image
    through the network (as shown in Fig. [14](#S5.F14 "Figure 14 ‣ 5.2 Unified (One
    Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object
    Detection: A Survey") (b)), resulting in a significantly increased number of evaluated
    context views. For each of the multiscale inputs, the classifier outputs a grid
    of predictions (class and confidence).'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*通过在多尺度图像上采用滑动窗口方式进行目标分类来生成目标候选。* OverFeat 使用类似 AlexNet Krizhevsky 等人（[2012a](#bib.bib140)）的
    CNN，这需要固定大小的输入图像，因为其全连接层。为了使滑动窗口方法在计算上更高效，OverFeat 将网络（如图 [14](#S5.F14 "Figure
    14 ‣ 5.2 Unified (One Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep Learning
    for Generic Object Detection: A Survey") (a) 所示）转换为全卷积网络，接受任意大小的输入，通过将全连接层视为 $1\times
    1$ 大小的卷积。OverFeat 利用多尺度特征通过网络传递最多六个原始图像的放大尺度（如图 [14](#S5.F14 "Figure 14 ‣ 5.2
    Unified (One Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep Learning for Generic
    Object Detection: A Survey") (b) 所示），从而显著增加评估的上下文视图数量。对于每个多尺度输入，分类器输出一个预测网格（类别和置信度）。'
- en: '2.'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '*Increase the number of predictions by offset max pooling*. In order to increase
    resolution, OverFeat applies offset max pooling after the last CONV layer, *i.e.*
    performing a subsampling operation at every offset, yielding many more views for
    voting, increasing robustness while remaining efficient.'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*通过偏移最大池化增加预测数量。* 为了提高分辨率，OverFeat 在最后一个 CONV 层后应用偏移最大池化，*即* 在每个偏移处执行下采样操作，产生更多的视图进行投票，提高了鲁棒性，同时保持了高效性。'
- en: '3.'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: '*Bounding box regression.* Once an object is identified, a single bounding
    box regressor is applied. The classifier and the regressor share the same feature
    extraction (CONV) layers, only the FC layers need to be recomputed after computing
    the classification network.'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*边界框回归。* 一旦识别出目标，就应用单一的边界框回归器。分类器和回归器共享相同的特征提取（CONV）层，只有在计算分类网络后需要重新计算 FC 层。'
- en: '4.'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: '*Combine predictions.* OverFeat uses a greedy merge strategy to combine the
    individual bounding box predictions across all locations and scales.'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*合并预测。* OverFeat 使用贪婪合并策略来结合所有位置和尺度的单个边界框预测。'
- en: OverFeat has a significant speed advantage, but is less accurate than RCNN Girshick
    et al. ([2014](#bib.bib85)), because it was difficult to train fully convolutional
    networks at the time. The speed advantage derives from sharing the computation
    of convolution between overlapping windows in the fully convolutional network.
    OverFeat is similar to later frameworks such as YOLO Redmon et al. ([2016](#bib.bib227))
    and SSD Liu et al. ([2016](#bib.bib175)), except that the classifier and the regressors
    in OverFeat are trained sequentially.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: OverFeat具有显著的速度优势，但其准确性低于RCNN Girshick等人（[2014](#bib.bib85)），因为当时训练完全卷积网络是困难的。速度优势来自于在完全卷积网络中共享重叠窗口的卷积计算。OverFeat与后来的框架如YOLO
    Redmon等人（[2016](#bib.bib227)）和SSD Liu等人（[2016](#bib.bib175)）类似，不同之处在于OverFeat中的分类器和回归器是按顺序训练的。
- en: 'YOLO: Redmon *et al.* Redmon et al. ([2016](#bib.bib227)) proposed YOLO (You
    Only Look Once), a unified detector casting object detection as a regression problem
    from image pixels to spatially separated bounding boxes and associated class probabilities,
    illustrated in Fig. [13](#S5.F13 "Figure 13 ‣ 5.1 Region Based (Two Stage) Frameworks
    ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object Detection: A Survey").
    Since the region proposal generation stage is completely dropped, YOLO directly
    predicts detections using a small set of candidate regions⁷⁷7YOLO uses far fewer
    bounding boxes, only 98 per image, compared to about 2000 from Selective Search..
    Unlike region based approaches (*e.g.* Faster RCNN) that predict detections based
    on features from a local region, YOLO uses features from an entire image globally.
    In particular, YOLO divides an image into an $S\times S$ grid, each predicting
    $C$ class probabilities, $B$ bounding box locations, and confidence scores. By
    throwing out the region proposal generation step entirely, YOLO is fast by design,
    running in real time at 45 FPS and Fast YOLO Redmon et al. ([2016](#bib.bib227))
    at 155 FPS. Since YOLO sees the entire image when making predictions, it implicitly
    encodes contextual information about object classes, and is less likely to predict
    false positives in the background. YOLO makes more localization errors than Fast
    RCNN, resulting from the coarse division of bounding box location, scale and aspect
    ratio. As discussed in Redmon et al. ([2016](#bib.bib227)), YOLO may fail to localize
    some objects, especially small ones, possibly because of the coarse grid division,
    and because each grid cell can only contain one object. It is unclear to what
    extent YOLO can translate to good performance on datasets with many objects per
    image, such as MS COCO.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 'YOLO: Redmon *et al.* Redmon等人（[2016](#bib.bib227)）提出了YOLO（You Only Look Once），这是一个统一的检测器，将物体检测视为从图像像素到空间分离的边界框及其相关类别概率的回归问题，如图[13](#S5.F13
    "Figure 13 ‣ 5.1 Region Based (Two Stage) Frameworks ‣ 5 Detection Frameworks
    ‣ Deep Learning for Generic Object Detection: A Survey")所示。由于完全省略了区域提议生成阶段，YOLO直接使用一小组候选区域进行检测⁷⁷7YOLO使用的边界框远少于选择性搜索的约2000个，每张图像仅使用98个。与基于区域的方法（*例如*
    Faster RCNN）根据局部区域的特征预测检测不同，YOLO使用整个图像的全局特征。特别是，YOLO将图像划分为一个$S\times S$网格，每个网格预测$C$类别概率、$B$边界框位置和置信度分数。通过完全省略区域提议生成步骤，YOLO在设计上是快速的，实时运行在45
    FPS，Fast YOLO Redmon等人（[2016](#bib.bib227)）则在155 FPS下运行。由于YOLO在进行预测时查看整个图像，它隐式地编码了关于物体类别的上下文信息，并且在背景中预测虚假正例的可能性较小。YOLO的定位误差比Fast
    RCNN多，原因在于边界框位置、尺度和纵横比的粗略划分。如Redmon等人（[2016](#bib.bib227)）所讨论的，YOLO可能会未能定位一些物体，尤其是小物体，这可能是由于网格划分粗略以及每个网格单元只能包含一个物体。YOLO在具有每张图像多个物体的数据集上（如MS
    COCO）的表现如何，尚不清楚。'
- en: 'YOLOv2 and YOLO9000: Redmon and Farhadi Redmon and Farhadi ([2017](#bib.bib226))
    proposed YOLOv2, an improved version of YOLO, in which the custom GoogLeNet Szegedy
    et al. ([2015](#bib.bib263)) network is replaced with the simpler DarkNet19, plus
    batch normalization He et al. ([2015](#bib.bib100)), removing the fully connected
    layers, and using good anchor boxes⁸⁸8Boxes of various sizes and aspect ratios
    that serve as object candidates. learned via *k*means and multiscale training.
    YOLOv2 achieved state-of-the-art on standard detection tasks. Redmon and Farhadi
    Redmon and Farhadi ([2017](#bib.bib226)) also introduced YOLO9000, which can detect
    over 9000 object categories in real time by proposing a joint optimization method
    to train simultaneously on an ImageNet classification dataset and a COCO detection
    dataset with WordTree to combine data from multiple sources. Such joint training
    allows YOLO9000 to perform weakly supervised detection, *i.e.* detecting object
    classes that do not have bounding box annotations.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv2 和 YOLO9000：Redmon 和 Farhadi Redmon 和 Farhadi ([2017](#bib.bib226)) 提出了
    YOLOv2，这是 YOLO 的改进版，其中将自定义的 GoogLeNet Szegedy 等人 ([2015](#bib.bib263)) 网络替换为更简单的
    DarkNet19，并增加了批量归一化 He 等人 ([2015](#bib.bib100))，去除了全连接层，并使用经过 *k*均值和多尺度训练学习的优质锚框⁸⁸8Boxes，具有各种大小和纵横比作为物体候选框。YOLOv2
    在标准检测任务中实现了最先进的性能。Redmon 和 Farhadi Redmon 和 Farhadi ([2017](#bib.bib226)) 还推出了
    YOLO9000，该方法通过提出一种联合优化方法来同时在 ImageNet 分类数据集和 COCO 检测数据集上进行训练，并结合 WordTree 来整合来自多个来源的数据，从而实现了实时检测超过
    9000 种物体类别。这种联合训练使 YOLO9000 能够进行弱监督检测，即检测没有边界框注释的物体类别。
- en: 'SSD: In order to preserve real-time speed without sacrificing too much detection
    accuracy, Liu *et al.* Liu et al. ([2016](#bib.bib175)) proposed SSD (Single Shot
    Detector), faster than YOLO Redmon et al. ([2016](#bib.bib227)) and with an accuracy
    competitive with region-based detectors such as Faster RCNN Ren et al. ([2015](#bib.bib229)).
    SSD effectively combines ideas from RPN in Faster RCNN Ren et al. ([2015](#bib.bib229)),
    YOLO Redmon et al. ([2016](#bib.bib227)) and multiscale CONV features Hariharan
    et al. ([2016](#bib.bib97)) to achieve fast detection speed, while still retaining
    high detection quality. Like YOLO, SSD predicts a fixed number of bounding boxes
    and scores, followed by an NMS step to produce the final detection. The CNN network
    in SSD is fully convolutional, whose early layers are based on a standard architecture,
    such as VGG Simonyan and Zisserman ([2015](#bib.bib248)), followed by several
    auxiliary CONV layers, progressively decreasing in size. The information in the
    last layer may be too coarse spatially to allow precise localization, so SSD performs
    detection over multiple scales by operating on multiple CONV feature maps, each
    of which predicts category scores and box offsets for bounding boxes of appropriate
    sizes. For a $300\times 300$ input, SSD achieves $74.3\%$ mAP on the VOC2007 test
    at 59 FPS versus Faster RCNN 7 FPS / mAP $73.2\%$ or YOLO 45 FPS / mAP $63.4\%$.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: SSD：为了在不牺牲太多检测准确性的情况下保持实时速度，Liu *et al.* Liu 等人 ([2016](#bib.bib175)) 提出了 SSD（Single
    Shot Detector），其速度比 YOLO Redmon 等人 ([2016](#bib.bib227)) 更快，且准确性与基于区域的检测器（如 Faster
    RCNN Ren 等人 ([2015](#bib.bib229))）相当。SSD 有效地结合了 Faster RCNN Ren 等人 ([2015](#bib.bib229))、YOLO
    Redmon 等人 ([2016](#bib.bib227)) 和多尺度 CONV 特征 Hariharan 等人 ([2016](#bib.bib97))
    的思想，实现了快速检测速度，同时保持了高检测质量。与 YOLO 类似，SSD 预测固定数量的边界框和分数，然后进行 NMS 步骤以生成最终检测。SSD 中的
    CNN 网络是全卷积的，其早期层基于标准架构，如 VGG Simonyan 和 Zisserman ([2015](#bib.bib248))，随后是几个逐渐减小的辅助
    CONV 层。最后一层的信息可能在空间上过于粗糙，无法进行精确定位，因此 SSD 通过在多个 CONV 特征图上操作来进行多尺度检测，每个特征图预测适当大小边界框的类别分数和框偏移量。对于
    $300\times 300$ 的输入，SSD 在 VOC2007 测试中以 59 FPS 实现 $74.3\%$ 的 mAP，而 Faster RCNN
    为 7 FPS / mAP $73.2\%$ 或 YOLO 为 45 FPS / mAP $63.4\%$。
- en: 'CornerNet: Recently, Law *et al.* Law and Deng ([2018](#bib.bib146)) questioned
    the dominant role that anchor boxes have come to play in SoA object detection
    frameworks Girshick ([2015](#bib.bib84)); He et al. ([2017](#bib.bib102)); Redmon
    et al. ([2016](#bib.bib227)); Liu et al. ([2016](#bib.bib175)). Law *et al.* Law
    and Deng ([2018](#bib.bib146)) argue that the use of anchor boxes, especially
    in one stage detectors Fu et al. ([2017](#bib.bib77)); Lin et al. ([2017b](#bib.bib168));
    Liu et al. ([2016](#bib.bib175)); Redmon et al. ([2016](#bib.bib227)), has drawbacks
    Law and Deng ([2018](#bib.bib146)); Lin et al. ([2017b](#bib.bib168)) such as
    causing a huge imbalance between positive and negative examples, slowing down
    training and introducing extra hyperparameters. Borrowing ideas from the work
    on Associative Embedding in multiperson pose estimation Newell et al. ([2017](#bib.bib195)),
    Law *et al.* Law and Deng ([2018](#bib.bib146)) proposed CornerNet by formulating
    bounding box object detection as detecting paired top-left and bottom-right keypoints⁹⁹9The
    idea of using keypoints for object detection appeared previously in DeNet TychsenSmith
    and Petersson ([2017](#bib.bib269)). . In CornerNet, the backbone network consists
    of two stacked Hourglass networks Newell et al. ([2016](#bib.bib194)), with a
    simple corner pooling approach to better localize corners. CornerNet achieved
    a $42.1\%$ AP on MS COCO, outperforming all previous one stage detectors; however,
    the average inference time is about 4FPS on a Titan X GPU, significantly slower
    than SSD Liu et al. ([2016](#bib.bib175)) and YOLO Redmon et al. ([2016](#bib.bib227)).
    CornerNet generates incorrect bounding boxes because it is challenging to decide
    which pairs of keypoints should be grouped into the same objects. To further improve
    on CornerNet, Duan *et al.* Duan et al. ([2019](#bib.bib62)) proposed CenterNet
    to detect each object as a triplet of keypoints, by introducing one extra keypoint
    at the centre of a proposal, raising the MS COCO AP to $47.0\%$, but with an inference
    speed slower than CornerNet.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 'CornerNet: 最近，Law *et al.* Law 和 Deng ([2018](#bib.bib146)) 对锚框在当前最先进对象检测框架中扮演的主导角色提出了质疑（Girshick
    ([2015](#bib.bib84)); He et al. ([2017](#bib.bib102)); Redmon et al. ([2016](#bib.bib227));
    Liu et al. ([2016](#bib.bib175))）。Law *et al.* Law 和 Deng ([2018](#bib.bib146))
    认为，使用锚框，尤其是在单阶段检测器中（Fu et al. ([2017](#bib.bib77)); Lin et al. ([2017b](#bib.bib168));
    Liu et al. ([2016](#bib.bib175)); Redmon et al. ([2016](#bib.bib227))），存在一些缺陷（Law
    和 Deng ([2018](#bib.bib146)); Lin et al. ([2017b](#bib.bib168))），例如导致正负样本之间的巨大不平衡，训练速度变慢，并引入额外的超参数。借鉴了在多人体姿态估计中的关联嵌入工作（Newell
    et al. ([2017](#bib.bib195))），Law *et al.* Law 和 Deng ([2018](#bib.bib146)) 提出了
    CornerNet，将边界框对象检测形式化为检测成对的左上角和右下角关键点⁹⁹9 使用关键点进行对象检测的想法早在 DeNet 中就出现了（Tychsen-Smith
    和 Petersson ([2017](#bib.bib269))）。在 CornerNet 中，骨干网络由两个堆叠的 Hourglass 网络（Newell
    et al. ([2016](#bib.bib194))）组成，使用简单的角点池化方法以更好地定位角点。CornerNet 在 MS COCO 上取得了 $42.1\%$
    的 AP，优于所有之前的单阶段检测器；然而，其平均推理时间约为 4FPS，在 Titan X GPU 上，比 SSD Liu et al. ([2016](#bib.bib175))
    和 YOLO Redmon et al. ([2016](#bib.bib227)) 慢得多。CornerNet 生成不正确的边界框，因为决定哪些关键点对应该归为同一对象是具有挑战性的。为了进一步改进
    CornerNet，Duan *et al.* Duan et al. ([2019](#bib.bib62)) 提出了 CenterNet，通过在提议的中心引入一个额外的关键点，将每个对象检测为一个由三个关键点组成的三元组，将
    MS COCO 的 AP 提高到 $47.0\%$，但推理速度比 CornerNet 慢。'
- en: 6 Object Representation
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 对象表示
- en: As one of the main components in any detector, good feature representations
    are of primary importance in object detection Dickinson et al. ([2009](#bib.bib56));
    Girshick et al. ([2014](#bib.bib85)); Gidaris and Komodakis ([2015](#bib.bib82));
    Zhu et al. ([2016a](#bib.bib324)). In the past, a great deal of effort was devoted
    to designing local descriptors (*e.g.,* SIFT Lowe ([1999](#bib.bib178)) and HOG
    Dalal and Triggs ([2005](#bib.bib52))) and to explore approaches (*e.g.,* Bag
    of Words Sivic and Zisserman ([2003](#bib.bib252)) and Fisher Vector Perronnin
    et al. ([2010](#bib.bib212))) to group and abstract descriptors into higher level
    representations in order to allow the discriminative parts to emerge; however,
    these feature representation methods required careful engineering and considerable
    domain expertise.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 作为任何检测器中的主要组件之一，良好的特征表示在对象检测中至关重要（Dickinson et al. ([2009](#bib.bib56)); Girshick
    et al. ([2014](#bib.bib85)); Gidaris and Komodakis ([2015](#bib.bib82)); Zhu et
    al. ([2016a](#bib.bib324))）。过去，许多精力被投入到设计局部描述符（*例如，* SIFT Lowe ([1999](#bib.bib178))
    和 HOG Dalal 和 Triggs ([2005](#bib.bib52)))，并探索方法（*例如，* Bag of Words Sivic 和 Zisserman
    ([2003](#bib.bib252)) 和 Fisher Vector Perronnin et al. ([2010](#bib.bib212)))，以将描述符归类并抽象为更高层次的表示，以便使区分性的部分能够显现；然而，这些特征表示方法需要精心设计和相当的领域专业知识。
- en: In contrast, deep learning methods (especially deep CNNs) can learn powerful
    feature representations with multiple levels of abstraction directly from raw
    images Bengio et al. ([2013](#bib.bib13)); LeCun et al. ([2015](#bib.bib149)).
    As the learning procedure reduces the dependency of specific domain knowledge
    and complex procedures needed in traditional feature engineering Bengio et al.
    ([2013](#bib.bib13)); LeCun et al. ([2015](#bib.bib149)), the burden for feature
    representation has been transferred to the design of better network architectures
    and training procedures.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，深度学习方法（尤其是深度CNN）可以直接从原始图像中学习具有多层抽象的强大特征表示 Bengio et al. ([2013](#bib.bib13))；LeCun
    et al. ([2015](#bib.bib149))。由于学习过程减少了对特定领域知识和传统特征工程中复杂程序的依赖 Bengio et al. ([2013](#bib.bib13))；LeCun
    et al. ([2015](#bib.bib149))，特征表示的负担已转移到更好的网络架构和训练程序的设计上。
- en: 'The leading frameworks reviewed in Section [5](#S5 "5 Detection Frameworks
    ‣ Deep Learning for Generic Object Detection: A Survey") (RCNN Girshick et al.
    ([2014](#bib.bib85)), Fast RCNN Girshick ([2015](#bib.bib84)), Faster RCNN Ren
    et al. ([2015](#bib.bib229)), YOLO Redmon et al. ([2016](#bib.bib227)), SSD Liu
    et al. ([2016](#bib.bib175))) have persistently promoted detection accuracy and
    speed, in which it is generally accepted that the CNN architecture (Section [6.1](#S6.SS1
    "6.1 Popular CNN Architectures ‣ 6 Object Representation ‣ Deep Learning for Generic
    Object Detection: A Survey") and Table [15](#S6.F15 "Figure 15 ‣ 6 Object Representation
    ‣ Deep Learning for Generic Object Detection: A Survey")) plays a crucial role.
    As a result, most of the recent improvements in detection accuracy have been via
    research into the development of novel networks. Therefore we begin by reviewing
    popular CNN architectures used in Generic Object Detection, followed by a review
    of the effort devoted to improving object feature representations, such as developing
    invariant features to accommodate geometric variations in object scale, pose,
    viewpoint, part deformation and performing multiscale analysis to improve object
    detection over a wide range of scales.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[5](#S5 "5 检测框架 ‣ 泛用目标检测的深度学习：综述")节中回顾的领先框架（RCNN Girshick et al. ([2014](#bib.bib85))，Fast
    RCNN Girshick ([2015](#bib.bib84))，Faster RCNN Ren et al. ([2015](#bib.bib229))，YOLO
    Redmon et al. ([2016](#bib.bib227))，SSD Liu et al. ([2016](#bib.bib175)))持续推动了检测准确性和速度的提升，其中普遍接受的观点是CNN架构（第[6.1](#S6.SS1
    "6.1 流行的CNN架构 ‣ 6 目标表示 ‣ 泛用目标检测的深度学习：综述")节和表[15](#S6.F15 "图15 ‣ 6 目标表示 ‣ 泛用目标检测的深度学习：综述")）发挥了关键作用。因此，最近检测准确性的大多数改进都通过对新型网络的研究实现。因此，我们首先回顾了用于泛用目标检测的流行CNN架构，然后审视了致力于改进对象特征表示的工作，例如开发不变特征以适应对象尺度、姿态、视角、部分变形的几何变化，并进行多尺度分析以提高在各种尺度下的目标检测能力。
- en: '![Refer to caption](img/a8ae239226da1175d0a165e3b04b7e28.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a8ae239226da1175d0a165e3b04b7e28.png)'
- en: 'Figure 15: Performance of winning entries in the ILSVRC competitions from 2011
    to 2017 in the image classification task.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：2011年至2017年ILSVRC竞赛中图像分类任务获胜条目的性能。
- en: 'Table 6: DCNN architectures that were commonly used for generic object detection.
    Regarding the statistics for “#Paras” and “#Layers”, the final FC prediction layer
    is not taken into consideration. “Test Error” column indicates the Top 5 classification
    test error on ImageNet1000\. When ambiguous, the “#Paras”, “#Layers”, and “Test
    Error” refer to: OverFeat (accurate model), VGGNet16, ResNet101 DenseNet201 (Growth
    Rate 32, DenseNet-BC), ResNeXt50 (32*4d), and SE ResNet50.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：用于泛用目标检测的常见DCNN架构。关于“参数数量”和“层数”的统计，最终的FC预测层未被考虑。“测试误差”列表示在ImageNet1000上的Top
    5分类测试误差。当存在歧义时，“参数数量”，“层数”和“测试误差”指：OverFeat（准确模型），VGGNet16，ResNet101 DenseNet201（增长率32，DenseNet-BC），ResNeXt50（32*4d），和SE
    ResNet50。
- en: '|      No. | DCNN Architecture | #Paras ($\times 10^{6}$) | #Layers (CONV+FC)
    | Test Error (Top 5) | First Used In | Highlights    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|      编号 | DCNN架构 | 参数数量 ($\times 10^{6}$) | 层数（CONV+FC） | 测试误差（Top 5） | 首次使用于
    | 亮点    |'
- en: '|      $1$ | AlexNet Krizhevsky et al. ([2012b](#bib.bib141)) | $57$ | $5+2$
    | $15.3\%$ | Girshick et al. ([2014](#bib.bib85)) | The first DCNN found effective
    for ImageNet classification; the historical turning point from hand-crafted features
    to CNN; Winning the ILSVRC2012 Image classification competition.    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|      $1$ | AlexNet Krizhevsky et al. ([2012b](#bib.bib141)) | $57$ | $5+2$
    | $15.3\%$ | Girshick et al. ([2014](#bib.bib85)) | 第一个在ImageNet分类中表现有效的DCNN；从手工特征到CNN的历史转折点；赢得了ILSVRC2012图像分类竞赛。
       |'
- en: '|    $2$ | ZFNet (fast) Zeiler and Fergus ([2014](#bib.bib303)) | $58$ | $5+2$
    | $14.8\%$ | He et al. ([2014](#bib.bib99)) | Similar to AlexNet, different in
    stride for convolution, filter size, and number of filters for some layers.   
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|    $2$ | ZFNet (fast) Zeiler 和 Fergus ([2014](#bib.bib303)) | $58$ | $5+2$
    | $14.8\%$ | He 等人 ([2014](#bib.bib99)) | 类似于 AlexNet，但在卷积步幅、滤波器大小和某些层的滤波器数量上有所不同。
       |'
- en: '|    $3$ | OverFeat Sermanet et al. ([2014](#bib.bib239)) | $140$ | $6+2$ |
    $13.6\%$ | Sermanet et al. ([2014](#bib.bib239)) | Similar to AlexNet, different
    in stride for convolution, filter size, and number of filters for some layers.
       |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|    $3$ | OverFeat Sermanet 等人 ([2014](#bib.bib239)) | $140$ | $6+2$ | $13.6\%$
    | Sermanet 等人 ([2014](#bib.bib239)) | 类似于 AlexNet，但在卷积步幅、滤波器大小和某些层的滤波器数量上有所不同。
       |'
- en: '|    $4$ | VGGNet Simonyan and Zisserman ([2015](#bib.bib248)) | $134$ | $13+2$
    | $6.8\%$ | Girshick ([2015](#bib.bib84)) | Increasing network depth significantly
    by stacking $3\times 3$ convolution filters and increasing the network depth step
    by step.    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|    $4$ | VGGNet Simonyan 和 Zisserman ([2015](#bib.bib248)) | $134$ | $13+2$
    | $6.8\%$ | Girshick ([2015](#bib.bib84)) | 通过堆叠 $3\times 3$ 卷积滤波器显著增加网络深度，并逐步增加网络深度。
       |'
- en: '|    $5$ | GoogLeNet Szegedy et al. ([2015](#bib.bib263)) | $6$ | $22$ | $6.7\%$
    | Szegedy et al. ([2015](#bib.bib263)) | Use Inception module, which uses multiple
    branches of convolutional layers with different filter sizes and then concatenates
    feature maps produced by these branches. The first inclusion of bottleneck structure
    and global average pooling.    |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|    $5$ | GoogLeNet Szegedy 等人 ([2015](#bib.bib263)) | $6$ | $22$ | $6.7\%$
    | Szegedy 等人 ([2015](#bib.bib263)) | 使用 Inception 模块，它使用具有不同滤波器大小的多个卷积层分支，然后连接这些分支产生的特征图。首次引入了瓶颈结构和全局平均池化。
       |'
- en: '|    $6$ | Inception v2 Ioffe and Szegedy ([2015](#bib.bib125)) | $12$ | $31$
    | $4.8\%$ | Howard et al. ([2017](#bib.bib112)) | Faster training with the introduce
    of Batch Normalization.    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|    $6$ | Inception v2 Ioffe 和 Szegedy ([2015](#bib.bib125)) | $12$ | $31$
    | $4.8\%$ | Howard 等人 ([2017](#bib.bib112)) | 通过引入批量归一化，加快了训练速度。    |'
- en: '|    $7$ | Inception v3 Szegedy et al. ([2016](#bib.bib264)) | $22$ | $47$
    | $3.6\%$ |  | Inclusion of separable convolution and spatial resolution reduction.
       |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|    $7$ | Inception v3 Szegedy 等人 ([2016](#bib.bib264)) | $22$ | $47$ | $3.6\%$
    |  | 包含可分离卷积和空间分辨率降低。    |'
- en: '|    $8$ | YOLONet Redmon et al. ([2016](#bib.bib227)) | $64$ | $24+1$ | $-$
    | Redmon et al. ([2016](#bib.bib227)) | A network inspired by GoogLeNet used in
    YOLO detector.    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|    $8$ | YOLONet Redmon 等人 ([2016](#bib.bib227)) | $64$ | $24+1$ | $-$ |
    Redmon 等人 ([2016](#bib.bib227)) | 受 GoogLeNet 启发的网络，用于 YOLO 检测器。    |'
- en: '|    $9$ | ResNet50 He et al. ([2016](#bib.bib101)) | $23.4$ | $49$ | $3.6\%$
    | He et al. ([2016](#bib.bib101)) | With identity mapping, substantially deeper
    networks can be learned.    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|    $9$ | ResNet50 He 等人 ([2016](#bib.bib101)) | $23.4$ | $49$ | $3.6\%$ |
    He 等人 ([2016](#bib.bib101)) | 通过身份映射，可以学习到实质上更深的网络。    |'
- en: '|    $10$ | ResNet101 He et al. ([2016](#bib.bib101)) | $42$ | $100$ | (ResNets)
    | He et al. ([2016](#bib.bib101)) | Requires fewer parameters than VGG by using
    the global average pooling and bottleneck introduced in GoogLeNet.    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|    $10$ | ResNet101 He 等人 ([2016](#bib.bib101)) | $42$ | $100$ | (ResNets)
    | He 等人 ([2016](#bib.bib101)) | 通过使用全局平均池化和在 GoogLeNet 中引入的瓶颈结构，比 VGG 需要更少的参数。
       |'
- en: '|    $11$ | InceptionResNet v1 Szegedy et al. ([2017](#bib.bib265)) | $21$
    | $87$ | $3.1\%$ |  | Combination of identity mapping and Inception module, with
    similar computational cost of Inception v3, but faster training process.    |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|    $11$ | InceptionResNet v1 Szegedy 等人 ([2017](#bib.bib265)) | $21$ | $87$
    | $3.1\%$ |  | 结合了身份映射和 Inception 模块，计算成本与 Inception v3 类似，但训练过程更快。    |'
- en: '|    $12$ | InceptionResNet v2 Szegedy et al. ([2017](#bib.bib265)) | $30$
    | $95$ | (Ensemble) | Huang et al. ([2017b](#bib.bib120)) | A costlier residual
    version of Inception, with significantly improved recognition performance.   
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|    $12$ | InceptionResNet v2 Szegedy 等人 ([2017](#bib.bib265)) | $30$ | $95$
    | (Ensemble) | Huang 等人 ([2017b](#bib.bib120)) | 成本更高的 Inception 残差版本，识别性能显著提升。
       |'
- en: '|    $13$ | Inception v4 Szegedy et al. ([2017](#bib.bib265)) | $41$ | $75$
    |  |  | An Inception variant without residual connections, with roughly the same
    recognition performance as InceptionResNet v2, but significantly slower.    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|    $13$ | Inception v4 Szegedy 等人 ([2017](#bib.bib265)) | $41$ | $75$ |  |  |
    一种没有残差连接的 Inception 变体，识别性能与 InceptionResNet v2 大致相同，但显著较慢。    |'
- en: '|    $14$ | ResNeXt Xie et al. ([2017](#bib.bib291)) | $23$ | $49$ | $3.0\%$
    | Xie et al. ([2017](#bib.bib291)) | Repeating a building block that aggregates
    a set of transformations with the same topology.    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|    $14$ | ResNeXt Xie 等人 ([2017](#bib.bib291)) | $23$ | $49$ | $3.0\%$ |
    Xie 等人 ([2017](#bib.bib291)) | 重复一个构建块，聚合具有相同拓扑的变换集合。    |'
- en: '|    $15$ | DenseNet201 Huang et al. ([2017a](#bib.bib118)) | $18$ | $200$
    | $-$ | Zhou et al. ([2018b](#bib.bib321)) | Concatenate each layer with every
    other layer in a feed forward fashion. Alleviate the vanishing gradient problem,
    encourage feature reuse, reduction in number of parameters.    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|    $15$ | DenseNet201 Huang et al. ([2017a](#bib.bib118)) | $18$ | $200$
    | $-$ | Zhou et al. ([2018b](#bib.bib321)) | 以前馈的方式将每一层与其他每一层连接。缓解梯度消失问题，鼓励特征重用，减少参数数量。
       |'
- en: '|    $16$ | DarkNet Redmon and Farhadi ([2017](#bib.bib226)) | $20$ | $19$
    | $-$ | Redmon and Farhadi ([2017](#bib.bib226)) | Similar to VGGNet, but with
    significantly fewer parameters.    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|    $16$ | DarkNet Redmon and Farhadi ([2017](#bib.bib226)) | $20$ | $19$
    | $-$ | Redmon and Farhadi ([2017](#bib.bib226)) | 类似于VGGNet，但参数显著更少。    |'
- en: '|    $17$ | MobileNet Howard et al. ([2017](#bib.bib112)) | $3.2$ | $27+1$
    | $-$ | Howard et al. ([2017](#bib.bib112)) | Light weight deep CNNs using depth-wise
    separable convolutions.    |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|    $17$ | MobileNet Howard et al. ([2017](#bib.bib112)) | $3.2$ | $27+1$
    | $-$ | Howard et al. ([2017](#bib.bib112)) | 使用深度可分离卷积的轻量级深度CNN。    |'
- en: '|    $18$ | SE ResNet Hu et al. ([2018b](#bib.bib115)) | $26$ | $50$ | $2.3\%$
    (SENets) | Hu et al. ([2018b](#bib.bib115)) | Channel-wise attention by a novel
    block called *Squeeze and Excitation*. Complementary to existing backbone CNNs.
       |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|    $18$ | SE ResNet Hu et al. ([2018b](#bib.bib115)) | $26$ | $50$ | $2.3\%$
    (SENets) | Hu et al. ([2018b](#bib.bib115)) | 通过一种名为*Squeeze and Excitation*的新颖模块进行通道级注意力。补充现有的主干CNN。
       |'
- en: '|      |  |  |  |  |  |  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|      |  |  |  |  |  |  |'
- en: 6.1 Popular CNN Architectures
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 流行的CNN架构
- en: 'CNN architectures (Section [3](#S3 "3 A Brief Introduction to Deep Learning
    ‣ Deep Learning for Generic Object Detection: A Survey")) serve as network backbones
    used in the detection frameworks of Section [5](#S5 "5 Detection Frameworks ‣
    Deep Learning for Generic Object Detection: A Survey"). Representative frameworks
    include AlexNet Krizhevsky et al. ([2012b](#bib.bib141)), ZFNet Zeiler and Fergus
    ([2014](#bib.bib303)) VGGNet Simonyan and Zisserman ([2015](#bib.bib248)), GoogLeNet
    Szegedy et al. ([2015](#bib.bib263)), Inception series Ioffe and Szegedy ([2015](#bib.bib125));
    Szegedy et al. ([2016](#bib.bib264), [2017](#bib.bib265)), ResNet He et al. ([2016](#bib.bib101)),
    DenseNet Huang et al. ([2017a](#bib.bib118)) and SENet Hu et al. ([2018b](#bib.bib115)),
    summarized in Table [6](#S6.T6 "Table 6 ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey"), and where the improvement over time
    is seen in Fig. [15](#S6.F15 "Figure 15 ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey"). A further review of recent CNN advances
    can be found in Gu et al. ([2017](#bib.bib92)).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 'CNN架构（第[3](#S3 "3 A Brief Introduction to Deep Learning ‣ Deep Learning for
    Generic Object Detection: A Survey")节）作为第[5](#S5 "5 Detection Frameworks ‣ Deep
    Learning for Generic Object Detection: A Survey")节中检测框架使用的网络主干。代表性框架包括AlexNet
    Krizhevsky et al. ([2012b](#bib.bib141))，ZFNet Zeiler和Fergus ([2014](#bib.bib303))，VGGNet
    Simonyan和Zisserman ([2015](#bib.bib248))，GoogLeNet Szegedy et al. ([2015](#bib.bib263))，Inception系列
    Ioffe和Szegedy ([2015](#bib.bib125))；Szegedy et al. ([2016](#bib.bib264), [2017](#bib.bib265))，ResNet
    He et al. ([2016](#bib.bib101))，DenseNet Huang et al. ([2017a](#bib.bib118)) 和
    SENet Hu et al. ([2018b](#bib.bib115))，总结在表[6](#S6.T6 "Table 6 ‣ 6 Object Representation
    ‣ Deep Learning for Generic Object Detection: A Survey")中，时间上的改进见于图[15](#S6.F15
    "Figure 15 ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey")。对近期CNN进展的进一步综述可以在Gu et al. ([2017](#bib.bib92))中找到。'
- en: 'The trend in architecture evolution is for greater depth: AlexNet has 8 layers,
    VGGNet 16 layers, more recently ResNet and DenseNet both surpassed the 100 layer
    mark, and it was VGGNet Simonyan and Zisserman ([2015](#bib.bib248)) and GoogLeNet
    Szegedy et al. ([2015](#bib.bib263)) which showed that increasing depth can improve
    the representational power. As can be observed from Table [6](#S6.T6 "Table 6
    ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey"),
    networks such as AlexNet, OverFeat, ZFNet and VGGNet have an enormous number of
    parameters, despite being only a few layers deep, since a large fraction of the
    parameters come from the FC layers. Newer networks like Inception, ResNet, and
    DenseNet, although having a great depth, actually have far fewer parameters by
    avoiding the use of FC layers.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '架构演变的趋势是更大的深度：AlexNet有8层，VGGNet有16层，最近ResNet和DenseNet都超过了100层，VGGNet的Simonyan和Zisserman
    ([2015](#bib.bib248)) 以及GoogLeNet的Szegedy et al. ([2015](#bib.bib263)) 显示了增加深度可以提高表示能力。从表[6](#S6.T6
    "Table 6 ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey")可以观察到，像AlexNet、OverFeat、ZFNet和VGGNet这样的网络尽管层数较少，但由于FC层占用了大量参数，所以参数数量巨大。新型网络如Inception、ResNet和DenseNet，尽管深度很大，但通过避免使用FC层实际上具有更少的参数。'
- en: With the use of Inception modules Szegedy et al. ([2015](#bib.bib263)) in carefully
    designed topologies, the number of parameters of GoogLeNet is dramatically reduced,
    compared to AlexNet, ZFNet or VGGNet. Similarly, ResNet demonstrated the effectiveness
    of skip connections for learning extremely deep networks with hundreds of layers,
    winning the ILSVRC 2015 classification task. Inspired by ResNet He et al. ([2016](#bib.bib101)),
    InceptionResNets Szegedy et al. ([2017](#bib.bib265)) combined the Inception networks
    with shortcut connections, on the basis that shortcut connections can significantly
    accelerate network training. Extending ResNets, Huang *et al.* Huang et al. ([2017a](#bib.bib118))
    proposed DenseNets, which are built from dense blocksconnecting each layer to
    every other layer in a feedforward fashion, leading to compelling advantages such
    as parameter efficiency, implicit deep supervision^(10)^(10)10DenseNets perform
    deep supervision in an implicit way, *i.e.* individual layers receive additional
    supervision from other layers through the shorter connections. The benefits of
    deep supervision have previously been demonstrated in Deeply Supervised Nets (DSN)
    Lee et al. ([2015](#bib.bib150))., and feature reuse. Recently, Hu *et al.* He
    et al. ([2016](#bib.bib101)) proposed Squeeze and Excitation (SE) blocks, which
    can be combined with existing deep architectures to boost their performance at
    minimal additional computational cost, adaptively recalibrating channel-wise feature
    responses by explicitly modeling the interdependencies between convolutional feature
    channels, and which led to winning the ILSVRC 2017 classification task. Research
    on CNN architectures remains active, with emerging networks such as Hourglass
    Law and Deng ([2018](#bib.bib146)), Dilated Residual Networks Yu et al. ([2017](#bib.bib299)),
    Xception Chollet ([2017](#bib.bib45)), DetNet Li et al. ([2018b](#bib.bib164)),
    Dual Path Networks (DPN) Chen et al. ([2017b](#bib.bib37)), FishNet Sun et al.
    ([2018](#bib.bib257)), and GLoRe Chen et al. ([2019b](#bib.bib38)).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Inception模块（Szegedy et al. ([2015](#bib.bib263)））在精心设计的拓扑结构中，GoogLeNet的参数数量相比AlexNet、ZFNet或VGGNet显著减少。类似地，ResNet展示了跳跃连接在学习具有数百层的极深网络中的有效性，赢得了ILSVRC
    2015分类任务。受到ResNet（He et al. ([2016](#bib.bib101)））的启发，InceptionResNets（Szegedy
    et al. ([2017](#bib.bib265)））将Inception网络与快捷连接结合在一起，基于快捷连接可以显著加速网络训练。扩展ResNets，Huang
    *et al.*（Huang et al. ([2017a](#bib.bib118)））提出了DenseNets，这些网络由密集块组成，将每一层与其他层以前向传播的方式连接，从而带来了参数效率、隐式深度监督^(10)
- en: The training of a CNN requires a large-scale labeled dataset with intraclass
    diversity. Unlike image classification, detection requires localizing (possibly
    many) objects from an image. It has been shown Ouyang et al. ([2017b](#bib.bib206))
    that pretraining a deep model with a large scale dataset having object level annotations
    (such as ImageNet), instead of only the image level annotations, improves the
    detection performance. However, collecting bounding box labels is expensive, especially
    for hundreds of thousands of categories. A common scenario is for a CNN to be
    pretrained on a large dataset (usually with a large number of visual categories)
    with image-level labels; the pretrained CNN can then be applied to a small dataset,
    directly, as a generic feature extractor Razavian et al. ([2014](#bib.bib223));
    Azizpour et al. ([2016](#bib.bib8)); Donahue et al. ([2014](#bib.bib60)); Yosinski
    et al. ([2014](#bib.bib296)), which can support a wider range of visual recognition
    tasks. For detection, the pre-trained network is typically fine-tuned^(11)^(11)11Fine-tuning
    is done by initializing a network with weights optimized for a large labeled dataset
    like ImageNet. and then updating the network’s weights using the target-task training
    set. on a given detection dataset Donahue et al. ([2014](#bib.bib60)); Girshick
    et al. ([2014](#bib.bib85), [2016](#bib.bib87)). Several large scale image classification
    datasets are used for CNN pre-training, among them ImageNet1000 Deng et al. ([2009](#bib.bib54));
    Russakovsky et al. ([2015](#bib.bib234)) with 1.2 million images of 1000 object
    categories, Places Zhou et al. ([2017a](#bib.bib319)), which is much larger than
    ImageNet1000 but with fewer classes, a recent Places-Imagenet hybrid Zhou et al.
    ([2017a](#bib.bib319)), or JFT300M Hinton et al. ([2015](#bib.bib106)); Sun et al.
    ([2017](#bib.bib254)).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的训练需要一个大规模的带标签数据集，并且具有类内多样性。与图像分类不同，检测需要从图像中定位（可能是多个）对象。Ouyang等人（[2017b](#bib.bib206)）的研究表明，用一个具有对象级注释的大规模数据集（如ImageNet）预训练深度模型，而不是仅使用图像级注释，可以提高检测性能。然而，收集边界框标签成本高昂，尤其是对于数十万类别的情况。一个常见的场景是将CNN在一个带有图像级标签的大数据集上进行预训练；然后将预训练的CNN直接应用于一个小数据集，作为通用特征提取器
    Razavian等人（[2014](#bib.bib223)）；Azizpour等人（[2016](#bib.bib8)）；Donahue等人（[2014](#bib.bib60)）；Yosinski等人（[2014](#bib.bib296)），这可以支持更广泛的视觉识别任务。对于检测，通常对预训练的网络进行微调^(11)^(11)11微调是通过初始化一个具有针对大规模带标签数据集如ImageNet优化的权重的网络来完成的，然后使用目标任务训练集更新网络的权重。以适应给定的检测数据集
    Donahue等人（[2014](#bib.bib60)）；Girshick等人（[2014](#bib.bib85)，[2016](#bib.bib87)）。几个大规模图像分类数据集被用于CNN预训练，其中包括ImageNet1000
    Deng等人（[2009](#bib.bib54)）；Russakovsky等人（[2015](#bib.bib234)），包含120万张图像和1000个对象类别，Places
    Zhou等人（[2017a](#bib.bib319)），它比ImageNet1000大得多，但类别较少，还有最近的Places-Imagenet混合数据集
    Zhou等人（[2017a](#bib.bib319)），或JFT300M Hinton等人（[2015](#bib.bib106)）；Sun等人（[2017](#bib.bib254)）。
- en: Pretrained CNNs without fine-tuning were explored for object classification
    and detection in Donahue et al. ([2014](#bib.bib60)); Girshick et al. ([2016](#bib.bib87));
    Agrawal et al. ([2014](#bib.bib1)), where it was shown that detection accuracies
    are different for features extracted from different layers; for example, for AlexNet
    pre-trained on ImageNet, FC6 / FC7 / Pool5 are in descending order of detection
    accuracy Donahue et al. ([2014](#bib.bib60)); Girshick et al. ([2016](#bib.bib87)).
    Fine-tuning a pre-trained network can increase detection performance significantly
    Girshick et al. ([2014](#bib.bib85), [2016](#bib.bib87)), although in the case
    of AlexNet, the fine-tuning performance boost was shown to be much larger for
    FC6 / FC7 than for Pool5, suggesting that Pool5 features are more general. Furthermore,
    the relationship between the source and target datasets plays a critical role,
    for example that ImageNet based CNN features show better performance for object
    detection than for human action Zhou et al. ([2015](#bib.bib317)); Azizpour et al.
    ([2016](#bib.bib8)).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: Donahue等人（[2014](#bib.bib60)）；Girshick等人（[2016](#bib.bib87)）；Agrawal等人（[2014](#bib.bib1)）探讨了未微调的预训练CNN在对象分类和检测中的表现，其中显示不同层提取的特征的检测准确性不同；例如，对于在ImageNet上预训练的AlexNet，FC6
    / FC7 / Pool5的检测准确性依次递减 Donahue等人（[2014](#bib.bib60)）；Girshick等人（[2016](#bib.bib87)）。微调预训练网络可以显著提高检测性能
    Girshick等人（[2014](#bib.bib85)，[2016](#bib.bib87)），尽管在AlexNet的情况下，微调性能提升在FC6 /
    FC7中比在Pool5中更大，表明Pool5特征更为通用。此外，源数据集和目标数据集之间的关系起着关键作用，例如，基于ImageNet的CNN特征在对象检测中比在人类动作检测中表现更好
    Zhou等人（[2015](#bib.bib317)）；Azizpour等人（[2016](#bib.bib8)）。
- en: 'Table 7: Summary of properties of representative methods in improving DCNN
    feature representations for generic object detection. Details for Groups (1),
    (2), and (3) are provided in Section [6.2](#S6.SS2 "6.2 Methods For Improving
    Object Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object
    Detection: A Survey"). Abbreviations: Selective Search (SS), EdgeBoxes (EB), InceptionResNet
    (IRN). *Conv-Deconv* denotes the use of upsampling and convolutional layers with
    lateral connections to supplement the standard backbone network. Detection results
    on VOC07, VOC12 and COCO were reported with mAP@IoU=0.5, and the additional COCO
    results are computed as the average of mAP for IoU thresholds from 0.5 to 0.95\.
    Training data: “07”$\leftarrow$VOC2007 trainval; “07T”$\leftarrow$VOC2007 trainval
    and test; “12”$\leftarrow$VOC2012 trainval; CO$\leftarrow$ COCO trainval. The
    COCO detection results were reported with COCO2015 Test-Dev, except for MPN Zagoruyko
    et al. ([2016](#bib.bib302)) which reported with COCO2015 Test-Standard.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 7: 代表性方法在提升 DCNN 特征表示用于通用目标检测的属性总结。第 [6.2](#S6.SS2 "6.2 方法改善目标表示 ‣ 6
    目标表示 ‣ 深度学习用于通用目标检测：综述") 节提供了第 (1)、(2) 和 (3) 组的详细信息。缩略词：选择性搜索 (SS)、边缘框 (EB)、InceptionResNet
    (IRN)。*Conv-Deconv* 表示使用上采样和带有侧向连接的卷积层来补充标准主干网络。在 VOC07、VOC12 和 COCO 上报告了检测结果，mAP@IoU=0.5，额外的
    COCO 结果是计算 IoU 阈值从 0.5 到 0.95 的 mAP 平均值。训练数据：“07”$\leftarrow$VOC2007 训练集；“07T”$\leftarrow$VOC2007
    训练集和测试集；“12”$\leftarrow$VOC2012 训练集；CO$\leftarrow$ COCO 训练集。COCO 检测结果报告使用了 COCO2015
    Test-Dev，除了 MPN Zagoruyko 等 ([2016](#bib.bib302)) 外，报告使用了 COCO2015 Test-Standard。'
- en: '|      | Detector | Region | Backbone | Pipelined | mAP@IoU=0.5 | mAP | Published
    |    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|      | 检测器 | 区域 | 主干网络 | 流水线 | mAP@IoU=0.5 | mAP | 发布 |    |'
- en: '|    Group | Name | Proposal | DCNN | Used | VOC07 | VOC12 | COCO | COCO |
    In | Highlights    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|    组 | 名称 | 提议 | DCNN | 使用 | VOC07 | VOC12 | COCO | COCO | 在 | 亮点    |'
- en: '|      (1) Single detection with multilayer features | ION Bell et al. ([2016](#bib.bib11))
    | SS+EB MCG+RPN | VGG16 | Fast RCNN | $79.4$ (07+12) | $76.4$ (07+12) | $55.7$
    | $33.1$ | CVPR16 | Use features from multiple layers; use spatial recurrent neural
    networks for modeling contextual information; the Best Student Entry and the $3^{\textrm{rd}}$
    overall in the COCO detection challenge 2015.    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|      (1) 多层特征单次检测 | ION Bell 等 ([2016](#bib.bib11)) | SS+EB MCG+RPN | VGG16
    | Fast RCNN | $79.4$ (07+12) | $76.4$ (07+12) | $55.7$ | $33.1$ | CVPR16 | 使用多个层的特征；使用空间递归神经网络建模上下文信息；最佳学生作品，COCO检测挑战赛2015年总体第$3^{\textrm{rd}}$。
       |'
- en: '|   | HyperNet Kong et al. ([2016](#bib.bib135)) | RPN | VGG16 | Faster RCNN
    | $76.3$ (07+12) | $71.4$ (07T+12) | $-$ | $-$ | CVPR16 | Use features from multiple
    layers for both region proposal and region classification.    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|   | HyperNet Kong 等 ([2016](#bib.bib135)) | RPN | VGG16 | Faster RCNN | $76.3$
    (07+12) | $71.4$ (07T+12) | $-$ | $-$ | CVPR16 | 在区域提议和区域分类中使用多个层的特征。    |'
- en: '|   | PVANet Kim et al. ([2016](#bib.bib132)) | RPN | PVANet | Faster RCNN
    | 84.9 (07+12+CO) | 84.2 (07T+12+CO) | $-$ | $-$ | NIPSW16 | Deep but lightweight;
    Combine ideas from concatenated ReLU Shang et al. ([2016](#bib.bib240)), Inception
    Szegedy et al. ([2015](#bib.bib263)), and HyperNet Kong et al. ([2016](#bib.bib135)).
       |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|   | PVANet Kim 等 ([2016](#bib.bib132)) | RPN | PVANet | Faster RCNN | 84.9
    (07+12+CO) | 84.2 (07T+12+CO) | $-$ | $-$ | NIPSW16 | 深度但轻量；结合了来自连接ReLU Shang
    等 ([2016](#bib.bib240))、Inception Szegedy 等 ([2015](#bib.bib263)) 和 HyperNet Kong
    等 ([2016](#bib.bib135)) 的理念。    |'
- en: '|      (2) Detection at multiple layers | SDP+CRC Yang et al. ([2016b](#bib.bib293))
    | EB | VGG16 | Fast RCNN | $69.4$ (07) | $-$ | $-$ | $-$ | CVPR16 | Use features
    in multiple layers to reject easy negatives via CRC, and then classify remaining
    proposals using SDP.    |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|      (2) 多层级检测 | SDP+CRC Yang 等 ([2016b](#bib.bib293)) | EB | VGG16 | Fast
    RCNN | $69.4$ (07) | $-$ | $-$ | $-$ | CVPR16 | 使用多个层的特征通过 CRC 拒绝简单的负样本，然后使用 SDP
    对剩余提议进行分类。    |'
- en: '|   | MSCNN Cai et al. ([2016](#bib.bib24)) | RPN | VGG | Faster RCNN | Only
    Tested on KITTI | ECCV16 | Region proposal and classification are performed at
    multiple layers; includes feature upsampling; end to end learning.    |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|   | MSCNN Cai 等 ([2016](#bib.bib24)) | RPN | VGG | Faster RCNN | 仅在 KITTI
    上测试 | ECCV16 | 在多个层中进行区域提议和分类；包括特征上采样；端到端学习。    |'
- en: '|   | MPN Zagoruyko et al. ([2016](#bib.bib302)) | SharpMask Pinheiro et al.
    ([2016](#bib.bib214)) | VGG16 | Fast RCNN | $-$ | $-$ | $51.9$ | $33.2$ | BMVC16
    | Concatenate features from different convolutional layers and features of different
    contextual regions; loss function for multiple overlap thresholds; ranked $2^{\textrm{nd}}$
    in both the COCO15 detection and segmentation challenges.    |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|   | MPN Zagoruyko et al. ([2016](#bib.bib302)) | SharpMask Pinheiro et al.
    ([2016](#bib.bib214)) | VGG16 | Fast RCNN | $-$ | $-$ | $51.9$ | $33.2$ | BMVC16
    | 将来自不同卷积层的特征和不同上下文区域的特征连接起来；针对多个重叠阈值的损失函数；在COCO15检测和分割挑战中排名第$2^{\textrm{nd}}$。
       |'
- en: '|   | DSOD Shen et al. ([2017](#bib.bib242)) | Free | DenseNet | SSD | $77.7$
    (07+12) | $72.2$ (07T+12) | $47.3$ | $29.3$ | ICCV17 | Concatenate feature sequentially,
    like DenseNet. Train from scratch on the target dataset without pre-training.
       |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|   | DSOD Shen et al. ([2017](#bib.bib242)) | 免费 | DenseNet | SSD | $77.7$
    (07+12) | $72.2$ (07T+12) | $47.3$ | $29.3$ | ICCV17 | 特征按顺序连接，类似于DenseNet。在目标数据集上从头训练，无需预训练。
       |'
- en: '|   | RFBNet Liu et al. ([2018b](#bib.bib173)) | Free | VGG16 | SSD | $82.2$
    (07+12) | $81.2$ (07T+12) | $55.7$ | $34.4$ | ECCV18 | Propose a multi-branch
    convolutional block similar to Inception Szegedy et al. ([2015](#bib.bib263)),
    but using dilated convolution.    |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|   | RFBNet Liu et al. ([2018b](#bib.bib173)) | 免费 | VGG16 | SSD | $82.2$
    (07+12) | $81.2$ (07T+12) | $55.7$ | $34.4$ | ECCV18 | 提出了一个类似于Inception Szegedy
    et al. ([2015](#bib.bib263)) 的多分支卷积块，但使用了扩张卷积。    |'
- en: '|      (3) Combination of (1) and (2)                                     
    | DSSD Fu et al. ([2017](#bib.bib77)) | Free | ResNet101 | SSD | $81.5$ (07+12)
    | $80.0$ (07T+12) | $53.3$ | $33.2$ | 2017 | Use Conv-Deconv, as shown in Fig.
    [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods
    For Improving Object Representation ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey") (c1, c2).    |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|      (3) 组合 (1) 和 (2)                                      | DSSD Fu et al.
    ([2017](#bib.bib77)) | 免费 | ResNet101 | SSD | $81.5$ (07+12) | $80.0$ (07T+12)
    | $53.3$ | $33.2$ | 2017 | 使用Conv-Deconv，如图 [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling
    of Object Scale Variations ‣ 6.2 Methods For Improving Object Representation ‣
    6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")
    (c1, c2) 所示。    |'
- en: '|   | FPN Lin et al. ([2017a](#bib.bib167)) | RPN | ResNet101 | Faster RCNN
    | $-$ | $-$ | $59.1$ | $36.2$ | CVPR17 | Use Conv-Deconv, as shown in Fig. [17](#S6.F17
    "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving
    Object Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object
    Detection: A Survey") (a1, a2); Widely used in detectors.    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|   | FPN Lin et al. ([2017a](#bib.bib167)) | RPN | ResNet101 | Faster RCNN
    | $-$ | $-$ | $59.1$ | $36.2$ | CVPR17 | 使用Conv-Deconv，如图 [17](#S6.F17 "Figure
    17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object
    Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey") (a1, a2) 所示；广泛应用于检测器。    |'
- en: '|   | TDM Shrivastava et al. ([2017](#bib.bib247)) | RPN | ResNet101 VGG16
    | Faster RCNN | $-$ | $-$ | $57.7$ | $36.8$ | CVPR17 | Use Conv-Deconv, as shown
    in Fig. [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣
    6.2 Methods For Improving Object Representation ‣ 6 Object Representation ‣ Deep
    Learning for Generic Object Detection: A Survey") (b2).    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|   | TDM Shrivastava et al. ([2017](#bib.bib247)) | RPN | ResNet101 VGG16
    | Faster RCNN | $-$ | $-$ | $57.7$ | $36.8$ | CVPR17 | 使用Conv-Deconv，如图 [17](#S6.F17
    "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving
    Object Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object
    Detection: A Survey") (b2) 所示。    |'
- en: '|   | RON Kong et al. ([2017](#bib.bib136)) | RPN | VGG16 | Faster RCNN | $81.3$
    (07+12+CO) | $80.7$ (07T+12+CO) | $49.5$ | $27.4$ | CVPR17 | Use Conv-deconv,
    as shown in Fig. [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations
    ‣ 6.2 Methods For Improving Object Representation ‣ 6 Object Representation ‣
    Deep Learning for Generic Object Detection: A Survey") (d2); Add the objectness
    prior to significantly reduce object search space.    |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|   | RON Kong et al. ([2017](#bib.bib136)) | RPN | VGG16 | Faster RCNN | $81.3$
    (07+12+CO) | $80.7$ (07T+12+CO) | $49.5$ | $27.4$ | CVPR17 | 使用Conv-deconv，如图
    [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods
    For Improving Object Representation ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey") (d2) 所示；增加对象性先验以显著减少对象搜索空间。    |'
- en: '|   | ZIP Li et al. ([2018a](#bib.bib156)) | RPN | Inceptionv2 | Faster RCNN
    | $79.8$ (07+12) | $-$ | $-$ | $-$ | IJCV18 | Use Conv-Deconv, as shown in Fig.
    [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods
    For Improving Object Representation ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey") (f1). Propose a map attention decision
    (MAD) unit for features from different layers.    |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|   | ZIP Li 等人 ([2018a](#bib.bib156)) | RPN | Inceptionv2 | Faster RCNN |
    $79.8$ (07+12) | $-$ | $-$ | $-$ | IJCV18 | 使用 Conv-Deconv，如图 [17](#S6.F17 "Figure
    17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object
    Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey") (f1) 所示。提出了一种用于不同层特征的地图注意力决策（MAD）单元。    |'
- en: '|   | STDN Zhou et al. ([2018b](#bib.bib321)) | Free | DenseNet169 | SSD |
    $80.9$ (07+12) | $-$ | $51.0$ | $31.8$ | CVPR18 | A new scale transfer module,
    which resizes features of different scales to the same scale in parallel.    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|   | STDN Zhou 等人 ([2018b](#bib.bib321)) | Free | DenseNet169 | SSD | $80.9$
    (07+12) | $-$ | $51.0$ | $31.8$ | CVPR18 | 一种新的尺度转移模块，将不同尺度的特征并行调整为相同的尺度。    |'
- en: '|   | RefineDet Zhang et al. ([2018a](#bib.bib308)) | RPN | VGG16 ResNet101
    | Faster RCNN | $83.8$ (07+12) | $83.5$ (07T+12) | $62.9$ | $41.8$ | CVPR18 |
    Use cascade to obtain better and less anchors. Use Conv-deconv, as shown in Fig.
    [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods
    For Improving Object Representation ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey") (e2) to improve features.    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|   | RefineDet Zhang 等人 ([2018a](#bib.bib308)) | RPN | VGG16 ResNet101 | Faster
    RCNN | $83.8$ (07+12) | $83.5$ (07T+12) | $62.9$ | $41.8$ | CVPR18 | 使用级联获得更好且更少的锚点。使用
    Conv-deconv，如图 [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations
    ‣ 6.2 Methods For Improving Object Representation ‣ 6 Object Representation ‣
    Deep Learning for Generic Object Detection: A Survey") (e2) 所示，改进特征。    |'
- en: '|   | PANet Liu et al. ([2018c](#bib.bib174)) | RPN | ResNeXt101 +FPN | Mask
    RCNN | $-$ | $-$ | 67.2 | 47.4 | CVPR18 | Shown in Fig. [17](#S6.F17 "Figure 17
    ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object
    Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey") (g). Based on FPN, add another bottom-up path to pass information between
    lower and topmost layers; adaptive feature pooling. Ranked $1^{st}$ and $2^{nd}$
    in COCO 2017 tasks.    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|   | PANet Liu 等人 ([2018c](#bib.bib174)) | RPN | ResNeXt101 +FPN | Mask RCNN
    | $-$ | $-$ | 67.2 | 47.4 | CVPR18 | 如图 [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling
    of Object Scale Variations ‣ 6.2 Methods For Improving Object Representation ‣
    6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")
    (g) 所示。基于 FPN，添加另一个自底向上的路径以在低层和最上层之间传递信息；自适应特征池化。COCO 2017 任务中排名第 $1^{st}$ 和第
    $2^{nd}$。    |'
- en: '|   | DetNet Li et al. ([2018b](#bib.bib164)) | RPN | DetNet59+FPN | Faster
    RCNN | $-$ | $-$ | $61.7$ | $40.2$ | ECCV18 | Introduces dilated convolution into
    the ResNet backbone to maintain high resolution in deeper layers; Shown in Fig.
    [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods
    For Improving Object Representation ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey") (i).    |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|   | DetNet Li 等人 ([2018b](#bib.bib164)) | RPN | DetNet59+FPN | Faster RCNN
    | $-$ | $-$ | $61.7$ | $40.2$ | ECCV18 | 在 ResNet 主干中引入膨胀卷积，以在更深的层中保持高分辨率；如图 [17](#S6.F17
    "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving
    Object Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object
    Detection: A Survey") (i) 所示。    |'
- en: '|   | FPR Kong et al. ([2018](#bib.bib137)) | $-$ | VGG16 ResNet101 | SSD |
    $82.4$ (07+12) | $81.1$ (07T+12) | $54.3$ | $34.6$ | ECCV18 | Fuse task oriented
    features across different spatial locations and scales, globally and locally;
    Shown in Fig. [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations
    ‣ 6.2 Methods For Improving Object Representation ‣ 6 Object Representation ‣
    Deep Learning for Generic Object Detection: A Survey") (h).    |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|   | FPR Kong 等人 ([2018](#bib.bib137)) | $-$ | VGG16 ResNet101 | SSD | $82.4$
    (07+12) | $81.1$ (07T+12) | $54.3$ | $34.6$ | ECCV18 | 融合不同空间位置和尺度的任务导向特征，包括全局和局部特征；如图
    [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods
    For Improving Object Representation ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey") (h) 所示。    |'
- en: '|   | M2Det Zhao et al. ([2019](#bib.bib315)) | $-$ | SSD | VGG16 ResNet101
    | $-$ | $-$ | $64.6$ | $44.2$ | AAAI19 | Shown in Fig. [17](#S6.F17 "Figure 17
    ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object
    Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey") (j), newly designed top down path to learn a set of multilevel features,
    recombined to construct a feature pyramid for object detection.    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|   | M2Det Zhao等人 ([2019](#bib.bib315)) | $-$ | SSD | VGG16 ResNet101 | $-$
    | $-$ | $64.6$ | $44.2$ | AAAI19 | 如图 [17](#S6.F17 "图 17 ‣ 6.2.1 处理对象尺度变化 ‣ 6.2
    提高对象表示的方法 ‣ 6 对象表示 ‣ 通用对象检测的深度学习：一项调查") (j) 所示，设计了一条新的自顶向下路径来学习一组多层级特征，并重新组合构建物体检测的特征金字塔。  
    |'
- en: '|      (4) Model Geometric Transforms    | DeepIDNet Ouyang et al. ([2015](#bib.bib203))
    | SS+ EB | AlexNet ZFNet OverFeat GoogLeNet | RCNN | $69.0$ (07) | $-$ | $-$ |
    $25.6$ | CVPR15 | Introduce a deformation constrained pooling layer, jointly learned
    with convolutional layers in existing DCNNs. Utilize the following modules that
    are not trained end to end: cascade, context modeling, model averaging, and bounding
    box location refinement in the multistage detection pipeline.    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|      (4) 模型几何变换    | DeepIDNet Ouyang等人 ([2015](#bib.bib203)) | SS+ EB |
    AlexNet ZFNet OverFeat GoogLeNet | RCNN | $69.0$ (07) | $-$ | $-$ | $25.6$ | CVPR15
    | 引入一个与现有深度卷积神经网络中的卷积层联合学习的变形约束池化层。利用以下模块，这些模块不是端对端训练的：级联、上下文建模、模型平均以及多阶段检测管道中的边界框位置细化。  
    |'
- en: '|   | DCN Dai et al. ([2017](#bib.bib51)) | RPN | ResNet101 IRN | RFCN | $82.6$
    (07+12) | $-$ | $58.0$ | $37.5$ | CVPR17 | Design deformable convolution and deformable
    RoI pooling modules that can replace plain convolution in existing DCNNs.    |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|   | DCN Dai等人 ([2017](#bib.bib51)) | RPN | ResNet101 IRN | RFCN | $82.6$
    (07+12) | $-$ | $58.0$ | $37.5$ | CVPR17 | 设计了可替代现有深度卷积神经网络中普通卷积的变形卷积和变形RoI池化模块。  
    |'
- en: '|   | DPFCN Mordan et al. ([2018](#bib.bib188)) | AttractioNet Gidaris and
    Komodakis ([2016](#bib.bib83)) | ResNet | RFCN | $83.3$ (07+12) | $81.2$ (07T+12)
    | $59.1$ | $39.1$ | IJCV18 | Design a deformable part based RoI pooling layer
    to explicitly select discriminative regions around object proposals.    |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|   | DPFCN Mordan等人 ([2018](#bib.bib188)) | AttractioNet Gidaris和Komodakis
    ([2016](#bib.bib83)) | ResNet | RFCN | $83.3$ (07+12) | $81.2$ (07T+12) | $59.1$
    | $39.1$ | IJCV18 | 设计了一个基于变形部件的RoI池化层，用于明确选择围绕对象提议的区域。   |'
- en: '|      |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|      |  |  |  |  |  |  |  |  |  |  |'
- en: 6.2 Methods For Improving Object Representation
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 提高对象表示的方法
- en: 'Deep CNN based detectors such as RCNN Girshick et al. ([2014](#bib.bib85)),
    Fast RCNN Girshick ([2015](#bib.bib84)), Faster RCNN Ren et al. ([2015](#bib.bib229))
    and YOLO Redmon et al. ([2016](#bib.bib227)), typically use the deep CNN architectures
    listed in Table [6](#S6.T6 "Table 6 ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey") as the backbone network and use features
    from the top layer of the CNN as object representations; however, detecting objects
    across a large range of scales is a fundamental challenge. A classical strategy
    to address this issue is to run the detector over a number of scaled input images
    (*e.g.,* an image pyramid) Felzenszwalb et al. ([2010b](#bib.bib74)); Girshick
    et al. ([2014](#bib.bib85)); He et al. ([2014](#bib.bib99)), which typically produces
    more accurate detection, with, however, obvious limitations of inference time
    and memory.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度卷积神经网络的检测器，例如RCNN Girshick等人 ([2014](#bib.bib85))、Fast RCNN Girshick ([2015](#bib.bib84))、Faster
    RCNN Ren等人 ([2015](#bib.bib229))和YOLO Redmon等人 ([2016](#bib.bib227))，通常使用表 [6](#S6.T6
    "表 6 ‣ 6 对象表示 ‣ 通用对象检测的深度学习：一项调查") 中列出的深度卷积神经网络结构作为主干网络，并使用来自CNN顶层的特征作为对象表示；然而，在跨越大范围尺度的对象检测方面是一个基本挑战。解决此问题的经典策略是在多个缩放的输入图像上运行检测器（*例如*，图像金字塔）Felzenszwalb等人
    ([2010b](#bib.bib74)); Girshick等人 ([2014](#bib.bib85)); He等人 ([2014](#bib.bib99))，通常能够产生更精确的检测结果，但推理时间和内存消耗显然有明显的限制。
- en: 6.2.1 Handling of Object Scale Variations
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 处理对象尺度变化
- en: 'Since a CNN computes its feature hierarchy layer by layer, the sub-sampling
    layers in the feature hierarchy already lead to an inherent multiscale pyramid,
    producing feature maps at different spatial resolutions, but subject to challenges
    Hariharan et al. ([2016](#bib.bib97)); Long et al. ([2015](#bib.bib177)); Shrivastava
    et al. ([2017](#bib.bib247)). In particular, the higher layers have a large receptive
    field and strong semantics, and are the most robust to variations such as object
    pose, illumination and part deformation, but the resolution is low and the geometric
    details are lost. In contrast, lower layers have a small receptive field and rich
    geometric details, but the resolution is high and much less sensitive to semantics.
    Intuitively, semantic concepts of objects can emerge in different layers, depending
    on the size of the objects. So if a target object is small it requires fine detail
    information in earlier layers and may very well disappear at later layers, in
    principle making small object detection very challenging, for which tricks such
    as dilated or “atrous” convolution Yu and Koltun ([2016](#bib.bib298)); Dai et al.
    ([2016c](#bib.bib50)); Chen et al. ([2018b](#bib.bib33)) have been proposed, increasing
    feature resolution, but increasing computational complexity. On the other hand,
    if the target object is large, then the semantic concept will emerge in much later
    layers. A number of methods Shrivastava et al. ([2017](#bib.bib247)); Zhang et al.
    ([2018e](#bib.bib314)); Lin et al. ([2017a](#bib.bib167)); Kong et al. ([2017](#bib.bib136))
    have been proposed to improve detection accuracy by exploiting multiple CNN layers,
    broadly falling into three types of multiscale object detection:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CNN 逐层计算特征层级，特征层级中的子采样层已经形成了固有的多尺度金字塔，产生不同空间分辨率的特征图，但面临挑战 Hariharan 等人 ([2016](#bib.bib97));
    Long 等人 ([2015](#bib.bib177)); Shrivastava 等人 ([2017](#bib.bib247))。特别是，高层具有较大的感受野和强的语义特征，对如目标姿态、光照和部件变形等变化最为稳健，但分辨率较低，几何细节丢失。相比之下，低层具有较小的感受野和丰富的几何细节，但分辨率较高，对语义的敏感度较低。直观上，物体的语义概念可以在不同层次中出现，这取决于物体的大小。因此，如果目标物体较小，它需要早期层中的细节信息，而在后期层中可能会消失，这使得小物体检测非常具有挑战性，为此，提出了扩张或“atrous”卷积
    Yu 和 Koltun ([2016](#bib.bib298)); Dai 等人 ([2016c](#bib.bib50)); Chen 等人 ([2018b](#bib.bib33))，以提高特征分辨率，但增加了计算复杂性。另一方面，如果目标物体较大，则语义概念将在较晚的层次中出现。已有一些方法
    Shrivastava 等人 ([2017](#bib.bib247)); Zhang 等人 ([2018e](#bib.bib314)); Lin 等人
    ([2017a](#bib.bib167)); Kong 等人 ([2017](#bib.bib136)) 被提出，以通过利用多个 CNN 层来提高检测精度，广泛分为三种多尺度目标检测类型：
- en: '1.'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Detecting with combined features of multiple layers;
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 利用多个层次的组合特征进行检测；
- en: '2.'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Detecting at multiple layers;
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在多个层次上进行检测；
- en: '3.'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Combinations of the above two methods.
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以上两种方法的组合。
- en: '![Refer to caption](img/4d0bcf8b6f8d498b5e1ceeaeef83f43b.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4d0bcf8b6f8d498b5e1ceeaeef83f43b.png)'
- en: 'Figure 16: Comparison of HyperNet and ION. LRN is Local Response Normalization,
    which performs a kind of “lateral inhibition” by normalizing over local input
    regions Jia et al. ([2014](#bib.bib127)).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：HyperNet 和 ION 的比较。LRN 是局部响应归一化，它通过对局部输入区域进行归一化来实现一种“侧向抑制” Jia 等人 ([2014](#bib.bib127))。
- en: '(1) Detecting with combined features of multiple CNN layers: Many approaches,
    including Hypercolumns Hariharan et al. ([2016](#bib.bib97)), HyperNet Kong et al.
    ([2016](#bib.bib135)), and ION Bell et al. ([2016](#bib.bib11)), combine features
    from multiple layers before making a prediction. Such feature combination is commonly
    accomplished via concatenation, a classic neural network idea that concatenates
    features from different layers, architectures which have recently become popular
    for semantic segmentation Long et al. ([2015](#bib.bib177)); Shelhamer et al.
    ([2017](#bib.bib241)); Hariharan et al. ([2016](#bib.bib97)). As shown in Fig. [16](#S6.F16
    "Figure 16 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving
    Object Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object
    Detection: A Survey") (a), ION Bell et al. ([2016](#bib.bib11)) uses RoI pooling
    to extract RoI features from multiple layers, and then the object proposals generated
    by selective search and edgeboxes are classified by using the concatenated features.
    HyperNet Kong et al. ([2016](#bib.bib135)), shown in Fig. [16](#S6.F16 "Figure
    16 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object
    Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey") (b), follows a similar idea, and integrates deep, intermediate and
    shallow features to generate object proposals and to predict objects via an end
    to end joint training strategy. The combined feature is more descriptive, and
    is more beneficial for localization and classification, but at increased computational
    complexity.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 使用多个CNN层的组合特征进行检测：许多方法，包括Hypercolumns Hariharan et al. ([2016](#bib.bib97))，HyperNet
    Kong et al. ([2016](#bib.bib135))，以及ION Bell et al. ([2016](#bib.bib11))，在进行预测之前结合了多个层的特征。这种特征组合通常通过连接实现，这是一种经典的神经网络思想，通过连接来自不同层的特征，这些架构最近在语义分割中变得流行
    Long et al. ([2015](#bib.bib177))；Shelhamer et al. ([2017](#bib.bib241))；Hariharan
    et al. ([2016](#bib.bib97))。如图[16](#S6.F16 "图16 ‣ 6.2.1 处理目标尺度变化 ‣ 6.2 改善目标表示的方法
    ‣ 6 目标表示 ‣ 深度学习用于通用目标检测：综述") (a)所示，ION Bell et al. ([2016](#bib.bib11))使用RoI池化从多个层中提取RoI特征，然后使用连接的特征对选择性搜索和边缘盒生成的目标提案进行分类。HyperNet
    Kong et al. ([2016](#bib.bib135))，如图[16](#S6.F16 "图16 ‣ 6.2.1 处理目标尺度变化 ‣ 6.2 改善目标表示的方法
    ‣ 6 目标表示 ‣ 深度学习用于通用目标检测：综述") (b)所示，采用类似的思想，将深层、中间层和浅层特征集成，以生成目标提案并通过端到端的联合训练策略预测目标。结合的特征更具描述性，有利于定位和分类，但计算复杂度增加。
- en: '![Refer to caption](img/28a0922160dee46caf9b1e7ae0f5393f.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/28a0922160dee46caf9b1e7ae0f5393f.png)'
- en: 'Figure 17: Hourglass architectures: Conv1 to Conv5 are the main Conv blocks
    in backbone networks such as VGG or ResNet. The figure compares a number of Feature
    Fusion Blocks (FFB) commonly used in recent approaches: FPN Lin et al. ([2017a](#bib.bib167)),
    TDM Shrivastava et al. ([2017](#bib.bib247)), DSSD Fu et al. ([2017](#bib.bib77)),
    RON Kong et al. ([2017](#bib.bib136)), RefineDet Zhang et al. ([2018a](#bib.bib308)),
    ZIP Li et al. ([2018a](#bib.bib156)), PANet Liu et al. ([2018c](#bib.bib174)),
    FPR Kong et al. ([2018](#bib.bib137)), DetNet Li et al. ([2018b](#bib.bib164))
    and M2Det Zhao et al. ([2019](#bib.bib315)). FFM: Feature Fusion Module, TUM:
    Thinned U-shaped Module'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：沙漏结构：Conv1到Conv5是VGG或ResNet等主干网络中的主要卷积块。该图比较了最近方法中常用的一些特征融合块（FFB）：FPN Lin
    et al. ([2017a](#bib.bib167))，TDM Shrivastava et al. ([2017](#bib.bib247))，DSSD
    Fu et al. ([2017](#bib.bib77))，RON Kong et al. ([2017](#bib.bib136))，RefineDet
    Zhang et al. ([2018a](#bib.bib308))，ZIP Li et al. ([2018a](#bib.bib156))，PANet
    Liu et al. ([2018c](#bib.bib174))，FPR Kong et al. ([2018](#bib.bib137))，DetNet
    Li et al. ([2018b](#bib.bib164)) 和 M2Det Zhao et al. ([2019](#bib.bib315))。FFM：特征融合模块，TUM：细化的U形模块
- en: '(2) Detecting at multiple CNN layers: A number of recent approaches improve
    detection by predicting objects of different resolutions at different layers and
    then combining these predictions: SSD Liu et al. ([2016](#bib.bib175)) and MSCNN
    Cai et al. ([2016](#bib.bib24)), RBFNet Liu et al. ([2018b](#bib.bib173)), and
    DSOD Shen et al. ([2017](#bib.bib242)). SSD Liu et al. ([2016](#bib.bib175)) spreads
    out default boxes of different scales to multiple layers within a CNN, and forces
    each layer to focus on predicting objects of a certain scale. RFBNet Liu et al.
    ([2018b](#bib.bib173)) replaces the later convolution layers of SSD with a Receptive
    Field Block (RFB) to enhance the discriminability and robustness of features.
    The RFB is a multibranch convolutional block, similar to the Inception block Szegedy
    et al. ([2015](#bib.bib263)), but combining multiple branches with different kernels
    and convolution layers Chen et al. ([2018b](#bib.bib33)). MSCNN Cai et al. ([2016](#bib.bib24))
    applies deconvolution on multiple layers of a CNN to increase feature map resolution
    before using the layers to learn region proposals and pool features. Similar to
    RFBNet Liu et al. ([2018b](#bib.bib173)), TridentNet Li et al. ([2019b](#bib.bib163))
    constructs a parallel multibranch architecture where each branch shares the same
    transformation parameters but with different receptive fields; dilated convolution
    with different dilation rates are used to adapt the receptive fields for objects
    of different scales.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 在多个CNN层上进行检测：最近的一些方法通过在不同层上预测不同分辨率的对象并将这些预测结果结合起来来提高检测精度：SSD Liu et al.
    ([2016](#bib.bib175)) 和 MSCNN Cai et al. ([2016](#bib.bib24))、RBFNet Liu et al.
    ([2018b](#bib.bib173)) 和 DSOD Shen et al. ([2017](#bib.bib242))。SSD Liu et al.
    ([2016](#bib.bib175)) 将不同尺度的默认框分布到CNN的多个层上，并强制每一层专注于预测特定尺度的对象。RFBNet Liu et al.
    ([2018b](#bib.bib173)) 用接受域块（RFB）替换了SSD的后期卷积层，以增强特征的辨别能力和鲁棒性。RFB是一个多分支卷积块，类似于Inception块
    Szegedy et al. ([2015](#bib.bib263))，但结合了具有不同卷积核和卷积层的多个分支 Chen et al. ([2018b](#bib.bib33))。MSCNN
    Cai et al. ([2016](#bib.bib24)) 在CNN的多个层上应用反卷积，以在使用这些层进行区域提议和特征池化之前增加特征图分辨率。类似于RFBNet
    Liu et al. ([2018b](#bib.bib173))，TridentNet Li et al. ([2019b](#bib.bib163))
    构建了一个并行的多分支结构，其中每个分支共享相同的变换参数，但具有不同的接受域；使用不同膨胀率的膨胀卷积来调整接受域，以适应不同尺度的对象。
- en: '(3) Combinations of the above two methods: Features from different layers are
    complementary to each other and can improve detection accuracy, as shown by Hypercolumns
    Hariharan et al. ([2016](#bib.bib97)), HyperNet Kong et al. ([2016](#bib.bib135))
    and ION Bell et al. ([2016](#bib.bib11)). On the other hand, however, it is natural
    to detect objects of different scales using features of approximately the same
    size, which can be achieved by detecting large objects from downscaled feature
    maps while detecting small objects from upscaled feature maps. Therefore, in order
    to combine the best of both worlds, some recent works propose to detect objects
    at multiple layers, and the resulting features obtained by combining features
    from different layers. This approach has been found to be effective for segmentation
    Long et al. ([2015](#bib.bib177)); Shelhamer et al. ([2017](#bib.bib241)) and
    human pose estimation Newell et al. ([2016](#bib.bib194)), has been widely exploited
    by both one-stage and two-stage detectors to alleviate problems of scale variation
    across object instances. Representative methods include SharpMask Pinheiro et al.
    ([2016](#bib.bib214)), Deconvolutional Single Shot Detector (DSSD) Fu et al. ([2017](#bib.bib77)),
    Feature Pyramid Network (FPN) Lin et al. ([2017a](#bib.bib167)), Top Down Modulation
    (TDM)Shrivastava et al. ([2017](#bib.bib247)), Reverse connection with Objectness
    prior Network (RON) Kong et al. ([2017](#bib.bib136)), ZIP Li et al. ([2018a](#bib.bib156)),
    Scale Transfer Detection Network (STDN) Zhou et al. ([2018b](#bib.bib321)), RefineDet
    Zhang et al. ([2018a](#bib.bib308)), StairNet Woo et al. ([2018](#bib.bib283)),
    Path Aggregation Network (PANet) Liu et al. ([2018c](#bib.bib174)), Feature Pyramid
    Reconfiguration (FPR) Kong et al. ([2018](#bib.bib137)), DetNet Li et al. ([2018b](#bib.bib164)),
    Scale Aware Network (SAN) Kim et al. ([2018](#bib.bib133)), Multiscale Location
    aware Kernel Representation (MLKP) Wang et al. ([2018](#bib.bib278)) and M2Det
    Zhao et al. ([2019](#bib.bib315)), as shown in Table [7](#S6.T7 "Table 7 ‣ 6.1
    Popular CNN Architectures ‣ 6 Object Representation ‣ Deep Learning for Generic
    Object Detection: A Survey") and contrasted in Fig. [17](#S6.F17 "Figure 17 ‣
    6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object Representation
    ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey").'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '(3) 上述两种方法的组合：不同层的特征互补，可以提高检测精度，正如Hypercolumns Hariharan等人 ([2016](#bib.bib97))、HyperNet
    Kong等人 ([2016](#bib.bib135)) 和ION Bell等人 ([2016](#bib.bib11)) 所示。然而，使用大致相同大小的特征来检测不同尺度的物体是自然的，这可以通过从缩小的特征图中检测大物体，同时从放大的特征图中检测小物体来实现。因此，为了兼顾两者的优势，一些最新的研究提议在多个层级上检测物体，并将不同层的特征进行组合。这种方法被发现对分割
    Long等人 ([2015](#bib.bib177)); Shelhamer等人 ([2017](#bib.bib241)) 和人体姿态估计 Newell等人
    ([2016](#bib.bib194)) 是有效的，已被单阶段和双阶段检测器广泛利用，以缓解物体实例尺度变化的问题。代表性方法包括SharpMask Pinheiro等人
    ([2016](#bib.bib214))、Deconvolutional Single Shot Detector (DSSD) Fu等人 ([2017](#bib.bib77))、Feature
    Pyramid Network (FPN) Lin等人 ([2017a](#bib.bib167))、Top Down Modulation (TDM) Shrivastava等人
    ([2017](#bib.bib247))、Reverse connection with Objectness prior Network (RON) Kong等人
    ([2017](#bib.bib136))、ZIP Li等人 ([2018a](#bib.bib156))、Scale Transfer Detection
    Network (STDN) Zhou等人 ([2018b](#bib.bib321))、RefineDet Zhang等人 ([2018a](#bib.bib308))、StairNet
    Woo等人 ([2018](#bib.bib283))、Path Aggregation Network (PANet) Liu等人 ([2018c](#bib.bib174))、Feature
    Pyramid Reconfiguration (FPR) Kong等人 ([2018](#bib.bib137))、DetNet Li等人 ([2018b](#bib.bib164))、Scale
    Aware Network (SAN) Kim等人 ([2018](#bib.bib133))、Multiscale Location aware Kernel
    Representation (MLKP) Wang等人 ([2018](#bib.bib278)) 和M2Det Zhao等人 ([2019](#bib.bib315))，如表
    [7](#S6.T7 "Table 7 ‣ 6.1 Popular CNN Architectures ‣ 6 Object Representation
    ‣ Deep Learning for Generic Object Detection: A Survey") 所示，并在图 [17](#S6.F17 "Figure
    17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object
    Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey") 中对比。'
- en: 'Early works like FPN Lin et al. ([2017a](#bib.bib167)), DSSD Fu et al. ([2017](#bib.bib77)),
    TDM Shrivastava et al. ([2017](#bib.bib247)), ZIP Li et al. ([2018a](#bib.bib156)),
    RON Kong et al. ([2017](#bib.bib136)) and RefineDet Zhang et al. ([2018a](#bib.bib308))
    construct the feature pyramid according to the inherent multiscale, pyramidal
    architecture of the backbone, and achieved encouraging results. As can be observed
    from Fig. [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations
    ‣ 6.2 Methods For Improving Object Representation ‣ 6 Object Representation ‣
    Deep Learning for Generic Object Detection: A Survey") (a1) to (f1), these methods
    have very similar detection architectures which incorporate a top-down network
    with lateral connections to supplement the standard bottom-up, feed-forward network.
    Specifically, after a bottom-up pass the final high level semantic features are
    transmitted back by the top-down network to combine with the bottom-up features
    from intermediate layers after lateral processing, and the combined features are
    then used for detection. As can be seen from Fig. [17](#S6.F17 "Figure 17 ‣ 6.2.1
    Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object Representation
    ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")
    (a2) to (e2), the main differences lie in the design of the simple Feature Fusion
    Block (FFB), which handles the selection of features from different layers and
    the combination of multilayer features.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '早期的研究如 FPN Lin et al. ([2017a](#bib.bib167))、DSSD Fu et al. ([2017](#bib.bib77))、TDM
    Shrivastava et al. ([2017](#bib.bib247))、ZIP Li et al. ([2018a](#bib.bib156))、RON
    Kong et al. ([2017](#bib.bib136)) 和 RefineDet Zhang et al. ([2018a](#bib.bib308))
    根据骨干网固有的多尺度金字塔结构构建特征金字塔，并取得了令人鼓舞的结果。从图 [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling
    of Object Scale Variations ‣ 6.2 Methods For Improving Object Representation ‣
    6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")
    (a1) 到 (f1) 可以观察到，这些方法具有非常相似的检测架构，它们结合了自上而下的网络和横向连接，以补充标准的自下而上的前馈网络。具体来说，在自下而上的传递之后，最终的高层语义特征通过自上而下的网络传回，与经过横向处理的中间层自下而上的特征结合，合成后的特征用于检测。从图
    [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods
    For Improving Object Representation ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey") (a2) 到 (e2) 可以看出，主要差异在于简单特征融合块（FFB）的设计，它处理来自不同层的特征选择和多层特征的结合。'
- en: FPN Lin et al. ([2017a](#bib.bib167)) shows significant improvement as a generic
    feature extractor in several applications including object detection Lin et al.
    ([2017a](#bib.bib167), [b](#bib.bib168)) and instance segmentation He et al. ([2017](#bib.bib102)).
    Using FPN in a basic Faster RCNN system achieved state-of-the-art results on the
    COCO detection dataset. STDN Zhou et al. ([2018b](#bib.bib321)) used DenseNet
    Huang et al. ([2017a](#bib.bib118)) to combine features of different layers and
    designed a scale transfer module to obtain feature maps with different resolutions.
    The scale transfer module can be directly embedded into DenseNet with little additional
    cost.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: FPN Lin et al. ([2017a](#bib.bib167)) 作为通用特征提取器在多个应用中表现出显著的改进，包括目标检测 Lin et
    al. ([2017a](#bib.bib167), [b](#bib.bib168)) 和实例分割 He et al. ([2017](#bib.bib102))。在基本的
    Faster RCNN 系统中使用 FPN 取得了 COCO 检测数据集的最先进结果。STDN Zhou et al. ([2018b](#bib.bib321))
    使用 DenseNet Huang et al. ([2017a](#bib.bib118)) 结合了不同层的特征，并设计了一个尺度转移模块以获得具有不同分辨率的特征图。尺度转移模块可以直接嵌入
    DenseNet，且几乎没有额外的成本。
- en: 'More recent work, such as PANet Liu et al. ([2018c](#bib.bib174)), FPR Kong
    et al. ([2018](#bib.bib137)), DetNet Li et al. ([2018b](#bib.bib164)), and M2Det
    Zhao et al. ([2019](#bib.bib315)), as shown in Fig. [17](#S6.F17 "Figure 17 ‣
    6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object Representation
    ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")
    (g-j), propose to further improve on the pyramid architectures like FPN in different
    ways. Based on FPN, Liu *et al.* designed PANet Liu et al. ([2018c](#bib.bib174))
    (Fig. [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2
    Methods For Improving Object Representation ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey") (g1)) by adding another bottom-up path
    with clean lateral connections from low to top levels, in order to shorten the
    information path and to enhance the feature pyramid. Then, an adaptive feature
    pooling was proposed to aggregate features from all feature levels for each proposal.
    In addition, in the proposal sub-network, a complementary branch capturing different
    views for each proposal is created to further improve mask prediction. These additional
    steps bring only slightly extra computational overhead, but are effective and
    allowed PANet to reach 1st place in the COCO 2017 Challenge Instance Segmentation
    task and 2nd place in the Object Detection task. Kong *et al.* proposed FPR Kong
    et al. ([2018](#bib.bib137)) by explicitly reformulating the feature pyramid construction
    process (*e.g.* FPN Lin et al. ([2017a](#bib.bib167))) as feature reconfiguration
    functions in a highly nonlinear but efficient way. As shown in Fig. [17](#S6.F17
    "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving
    Object Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object
    Detection: A Survey") (h1), instead of using a top-down path to propagate strong
    semantic features from the topmost layer down as in FPN, FPR first extracts features
    from multiple layers in the backbone network by adaptive concatenation, and then
    designs a more complex FFB module (Fig. [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling
    of Object Scale Variations ‣ 6.2 Methods For Improving Object Representation ‣
    6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")
    (h2)) to spread strong semantics to all scales. Li *et al.* proposed DetNet Li
    et al. ([2018b](#bib.bib164)) (Fig. [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of
    Object Scale Variations ‣ 6.2 Methods For Improving Object Representation ‣ 6
    Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")
    (i1)) by introducing dilated convolutions to the later layers of the backbone
    network in order to maintain high spatial resolution in deeper layers. Zhao *et
    al.* Zhao et al. ([2019](#bib.bib315)) proposed a MultiLevel Feature Pyramid Network
    (MLFPN) to build more effective feature pyramids for detecting objects of different
    scales. As can be seen from Fig. [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object
    Scale Variations ‣ 6.2 Methods For Improving Object Representation ‣ 6 Object
    Representation ‣ Deep Learning for Generic Object Detection: A Survey") (j1),
    features from two different layers of the backbone are first fused as the base
    feature, after which a top-down path with lateral connections from the base feature
    is created to build the feature pyramid. As shown in Fig. [17](#S6.F17 "Figure
    17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object
    Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey") (j2) and (j5), the FFB module is much more complex than those like
    FPN, in that FFB involves a Thinned U-shaped Module (TUM) to generate a second
    pyramid structure, after which the feature maps with equivalent sizes from multiple
    TUMs are combined for object detection. The authors proposed M2Det by integrating
    MLFPN into SSD, and achieved better detection performance than other one-stage
    detectors.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '更近期的研究，如PANet Liu et al. ([2018c](#bib.bib174))、FPR Kong et al. ([2018](#bib.bib137))、DetNet
    Li et al. ([2018b](#bib.bib164)) 和 M2Det Zhao et al. ([2019](#bib.bib315))（见图
    [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods
    For Improving Object Representation ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey") (g-j)），提出了以不同方式进一步改进像FPN这样的金字塔结构。基于FPN，Liu
    *et al.* 设计了PANet Liu et al. ([2018c](#bib.bib174))（图 [17](#S6.F17 "Figure 17
    ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object
    Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey") (g1)），通过添加另一条从低层到高层的底向上路径，并保持清晰的横向连接，以缩短信息路径并增强特征金字塔。随后，提出了一种自适应特征池化方法，用于将来自所有特征层的特征聚合到每个提案中。此外，在提案子网络中，创建了一个补充分支来捕捉每个提案的不同视角，从而进一步改善掩膜预测。这些额外的步骤仅带来略微的计算开销，但有效且使PANet在COCO
    2017挑战实例分割任务中获得了第一名，在目标检测任务中获得了第二名。Kong *et al.* 提出了FPR Kong et al. ([2018](#bib.bib137))，通过以高度非线性但高效的方式将特征金字塔构建过程（*例如*
    FPN Lin et al. ([2017a](#bib.bib167))) 显式重新表述为特征重配置函数。正如图 [17](#S6.F17 "Figure
    17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object
    Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey") (h1) 所示，FPR不是像FPN那样使用自上而下的路径从最顶层向下传播强语义特征，而是通过自适应拼接从骨干网络中的多个层提取特征，然后设计了一个更复杂的FFB模块（图
    [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods
    For Improving Object Representation ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey") (h2)），将强语义传播到所有尺度。Li *et al.* 提出了DetNet
    Li et al. ([2018b](#bib.bib164))（图 [17](#S6.F17 "Figure 17 ‣ 6.2.1 Handling of
    Object Scale Variations ‣ 6.2 Methods For Improving Object Representation ‣ 6
    Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")
    (i1)），通过将膨胀卷积引入到骨干网络的后续层，以在更深层保持高空间分辨率。Zhao *et al.* Zhao et al. ([2019](#bib.bib315))
    提出了多层次特征金字塔网络（MLFPN），以构建更有效的特征金字塔来检测不同尺度的物体。正如图 [17](#S6.F17 "Figure 17 ‣ 6.2.1
    Handling of Object Scale Variations ‣ 6.2 Methods For Improving Object Representation
    ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")
    (j1) 所示，首先将来自骨干网络的两个不同层的特征融合为基础特征，然后创建一个从基础特征开始的自上而下路径，并具有横向连接来构建特征金字塔。正如图 [17](#S6.F17
    "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving
    Object Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object
    Detection: A Survey") (j2) 和 (j5) 所示，FFB模块比像FPN这样的模块要复杂得多，因为FFB涉及一个稀疏的U形模块（TUM）来生成第二个金字塔结构，然后将来自多个TUM的等效尺寸的特征图结合起来进行目标检测。作者通过将MLFPN整合到SSD中提出了M2Det，并取得了比其他单阶段检测器更好的检测性能。'
- en: 6.3 Handling of Other Intraclass Variations
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 其他类别内变化的处理
- en: 'Powerful object representations should combine distinctiveness and robustness.
    A large amount of recent work has been devoted to handling changes in object scale,
    as reviewed in Section [6.2.1](#S6.SS2.SSS1 "6.2.1 Handling of Object Scale Variations
    ‣ 6.2 Methods For Improving Object Representation ‣ 6 Object Representation ‣
    Deep Learning for Generic Object Detection: A Survey"). As discussed in Section [2.2](#S2.SS2
    "2.2 Main Challenges ‣ 2 Generic Object Detection ‣ Deep Learning for Generic
    Object Detection: A Survey") and summarized in Fig. [6](#S2.F6 "Figure 6 ‣ 2.1
    The Problem ‣ 2 Generic Object Detection ‣ Deep Learning for Generic Object Detection:
    A Survey"), object detection still requires robustness to real-world variations
    other than just scale, which we group into three categories:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的对象表示应结合独特性和鲁棒性。最近的大量工作致力于处理对象尺度的变化，如第[6.2.1节](#S6.SS2.SSS1 "6.2.1 处理对象尺度变化
    ‣ 6.2 改进对象表示的方法 ‣ 6 对象表示 ‣ 通用对象检测的深度学习：综述")中回顾的那样。如第[2.2节](#S2.SS2 "2.2 主要挑战 ‣
    2 通用对象检测 ‣ 通用对象检测的深度学习：综述")中讨论并在图[6](#S2.F6 "图6 ‣ 2.1 问题 ‣ 2 通用对象检测 ‣ 通用对象检测的深度学习：综述")中总结的，对象检测仍然需要对除尺度以外的实际变化具备鲁棒性，这些变化我们将其分为三类：
- en: $\bullet$
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Geometric transformations,
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 几何变换，
- en: $\bullet$
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Occlusions, and
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 遮挡，以及
- en: $\bullet$
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Image degradations.
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像退化。
- en: To handle these intra-class variations, the most straightforward approach is
    to augment the training datasets with a sufficient amount of variations; for example,
    robustness to rotation could be achieved by adding rotated objects at many orientations
    to the training data. Robustness can frequently be learned this way, but usually
    at the cost of expensive training and complex model parameters. Therefore, researchers
    have proposed alternative solutions to these problems.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这些类别内的变化，最直接的方法是通过在训练数据集中增加足够多的变化来进行数据增强；例如，通过将许多方向上的旋转对象添加到训练数据中，可以实现对旋转的鲁棒性。通常可以通过这种方式学习鲁棒性，但通常需要昂贵的训练和复杂的模型参数。因此，研究人员提出了这些问题的替代解决方案。
- en: 'Handling of geometric transformations: DCNNs are inherently limited by the
    lack of ability to be spatially invariant to geometric transformations of the
    input data Lenc and Vedaldi ([2018](#bib.bib152)); Liu et al. ([2017](#bib.bib172));
    Chellappa ([2016](#bib.bib28)). The introduction of local max pooling layers has
    allowed DCNNs to enjoy some translation invariance, however the intermediate feature
    maps are not actually invariant to large geometric transformations of the input
    data Lenc and Vedaldi ([2018](#bib.bib152)). Therefore, many approaches have been
    presented to enhance robustness, aiming at learning invariant CNN representations
    with respect to different types of transformations such as scale Kim et al. ([2014](#bib.bib131));
    Bruna and Mallat ([2013](#bib.bib21)), rotation Bruna and Mallat ([2013](#bib.bib21));
    Cheng et al. ([2016](#bib.bib42)); Worrall et al. ([2017](#bib.bib284)); Zhou
    et al. ([2017b](#bib.bib323)), or both Jaderberg et al. ([2015](#bib.bib126)).
    One representative work is Spatial Transformer Network (STN) Jaderberg et al.
    ([2015](#bib.bib126)), which introduces a new learnable module to handle scaling,
    cropping, rotations, as well as nonrigid deformations via a global parametric
    transformation. STN has now been used in rotated text detection Jaderberg et al.
    ([2015](#bib.bib126)), rotated face detection and generic object detection Wang
    et al. ([2017](#bib.bib280)).'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 几何变换的处理：深度卷积神经网络（DCNNs）由于无法对输入数据的几何变换具有空间不变性而本质上受到限制 Lenc 和 Vedaldi ([2018](#bib.bib152))；Liu
    等 ([2017](#bib.bib172))；Chellappa ([2016](#bib.bib28))。局部最大池化层的引入使得 DCNNs 在一定程度上具备了平移不变性，但中间特征图实际上对输入数据的大尺度几何变换并不不变
    Lenc 和 Vedaldi ([2018](#bib.bib152))。因此，已经提出了许多方法来增强鲁棒性，旨在学习对不同类型的变换（如尺度 Kim 等
    ([2014](#bib.bib131))；Bruna 和 Mallat ([2013](#bib.bib21))，旋转 Bruna 和 Mallat ([2013](#bib.bib21))；Cheng
    等 ([2016](#bib.bib42))；Worrall 等 ([2017](#bib.bib284))；Zhou 等 ([2017b](#bib.bib323))，或两者兼具
    Jaderberg 等 ([2015](#bib.bib126))）不变的 CNN 表示。一个代表性的工作是空间变换网络（STN） Jaderberg 等
    ([2015](#bib.bib126))，它引入了一个新的可学习模块来处理缩放、裁剪、旋转以及非刚性变形，通过全局参数化变换进行处理。STN 现在已经应用于旋转文本检测
    Jaderberg 等 ([2015](#bib.bib126))，旋转人脸检测以及通用对象检测 Wang 等 ([2017](#bib.bib280))。
- en: Although rotation invariance may be attractive in certain applications, such
    as scene text detection He et al. ([2018](#bib.bib103)); Ma et al. ([2018](#bib.bib184)),
    face detection Shi et al. ([2018](#bib.bib243)), and aerial imagery Ding et al.
    ([2018](#bib.bib57)); Xia et al. ([2018](#bib.bib288)), there is limited generic
    object detection work focusing on rotation invariance because popular benchmark
    detection datasets (*e.g.* PASCAL VOC, ImageNet, COCO) do not actually present
    rotated images.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管旋转不变性在某些应用中可能很有吸引力，例如场景文本检测 He 等人 ([2018](#bib.bib103)); Ma 等人 ([2018](#bib.bib184))、人脸检测
    Shi 等人 ([2018](#bib.bib243)) 和航空影像 Ding 等人 ([2018](#bib.bib57)); Xia 等人 ([2018](#bib.bib288))，但在通用物体检测工作中关注旋转不变性的研究有限，因为流行的基准检测数据集（*例如*
    PASCAL VOC、ImageNet、COCO）实际上并不呈现旋转图像。
- en: 'Before deep learning, Deformable Part based Models (DPMs) Felzenszwalb et al.
    ([2010b](#bib.bib74)) were successful for generic object detection, representing
    objects by component parts arranged in a deformable configuration. Although DPMs
    have been significantly outperformed by more recent object detectors, their spirit
    still deeply influences many recent detectors. DPM modeling is less sensitive
    to transformations in object pose, viewpoint and nonrigid deformations, motivating
    researchers Dai et al. ([2017](#bib.bib51)); Girshick et al. ([2015](#bib.bib86));
    Mordan et al. ([2018](#bib.bib188)); Ouyang et al. ([2015](#bib.bib203)); Wan
    et al. ([2015](#bib.bib277)) to explicitly model object composition to improve
    CNN based detection. The first attempts Girshick et al. ([2015](#bib.bib86));
    Wan et al. ([2015](#bib.bib277)) combined DPMs with CNNs by using deep features
    learned by AlexNet in DPM based detection, but without region proposals. To enable
    a CNN to benefit from the built-in capability of modeling the deformations of
    object parts, a number of approaches were proposed, including DeepIDNet Ouyang
    et al. ([2015](#bib.bib203)), DCN Dai et al. ([2017](#bib.bib51)) and DPFCN Mordan
    et al. ([2018](#bib.bib188)) (shown in Table [7](#S6.T7 "Table 7 ‣ 6.1 Popular
    CNN Architectures ‣ 6 Object Representation ‣ Deep Learning for Generic Object
    Detection: A Survey")). Although similar in spirit, deformations are computed
    in different ways: DeepIDNet Ouyang et al. ([2017b](#bib.bib206)) designed a deformation
    constrained pooling layer to replace regular max pooling, to learn the shared
    visual patterns and their deformation properties across different object classes;
    DCN Dai et al. ([2017](#bib.bib51)) designed a deformable convolution layer and
    a deformable RoI pooling layer, both of which are based on the idea of augmenting
    regular grid sampling locations in feature maps; and DPFCN Mordan et al. ([2018](#bib.bib188))
    proposed a deformable part-based RoI pooling layer which selects discriminative
    parts of objects around object proposals by simultaneously optimizing latent displacements
    of all parts.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '在深度学习之前，Deformable Part based Models (DPMs) Felzenszwalb 等人 ([2010b](#bib.bib74))
    在通用物体检测中取得了成功，通过可变形配置排列的组件部分来表示物体。虽然 DPMs 已被更新的物体检测器显著超越，但其精神仍然深刻影响了许多近期的检测器。DPM
    建模对物体姿态、视角和非刚性变形的变化不太敏感，这激励了研究人员 Dai 等人 ([2017](#bib.bib51)); Girshick 等人 ([2015](#bib.bib86));
    Mordan 等人 ([2018](#bib.bib188)); Ouyang 等人 ([2015](#bib.bib203)); Wan 等人 ([2015](#bib.bib277))
    明确建模物体组成，以改进基于 CNN 的检测。最初的尝试 Girshick 等人 ([2015](#bib.bib86)); Wan 等人 ([2015](#bib.bib277))
    通过使用 AlexNet 学习到的深度特征将 DPMs 与 CNNs 结合在 DPM 基础检测中，但没有区域提议。为了使 CNN 能够利用建模物体部件变形的内在能力，提出了许多方法，包括
    DeepIDNet Ouyang 等人 ([2015](#bib.bib203))、DCN Dai 等人 ([2017](#bib.bib51)) 和 DPFCN
    Mordan 等人 ([2018](#bib.bib188))（见表 [7](#S6.T7 "Table 7 ‣ 6.1 Popular CNN Architectures
    ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")）。尽管在精神上类似，但变形的计算方式不同：DeepIDNet
    Ouyang 等人 ([2017b](#bib.bib206)) 设计了一种变形约束池化层以替代常规的最大池化，以学习不同物体类别中的共享视觉模式及其变形特性；DCN
    Dai 等人 ([2017](#bib.bib51)) 设计了一个可变形卷积层和一个可变形 RoI 池化层，这两者都基于增强特征图中常规网格采样位置的思想；而
    DPFCN Mordan 等人 ([2018](#bib.bib188)) 提出了一个可变形基于部件的 RoI 池化层，通过同时优化所有部件的潜在位移来选择物体提议周围的判别部件。'
- en: 'Handling of occlusions: In real-world images, occlusions are common, resulting
    in information loss from object instances. A deformable parts idea can be useful
    for occlusion handling, so deformable RoI Pooling Dai et al. ([2017](#bib.bib51));
    Mordan et al. ([2018](#bib.bib188)); Ouyang and Wang ([2013](#bib.bib202)) and
    deformable convolution Dai et al. ([2017](#bib.bib51)) have been proposed to alleviate
    occlusion by giving more flexibility to the typically fixed geometric structures.
    Wang *et al.* Wang et al. ([2017](#bib.bib280)) propose to learn an adversarial
    network that generates examples with occlusions and deformations, and context
    may be helpful in dealing with occlusions Zhang et al. ([2018b](#bib.bib309)).
    Despite these efforts, the occlusion problem is far from being solved; applying
    GANs to this problem may be a promising research direction.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 处理遮挡：在实际图像中，遮挡是常见的，导致对象实例的信息丢失。可变形部件的想法对处理遮挡可能有用，因此提出了可变形RoI池化 Dai 等 ([2017](#bib.bib51));
    Mordan 等 ([2018](#bib.bib188)); Ouyang 和 Wang ([2013](#bib.bib202)) 和可变形卷积 Dai
    等 ([2017](#bib.bib51))，通过给通常固定的几何结构更多灵活性来缓解遮挡。Wang *et al.* Wang 等 ([2017](#bib.bib280))
    提出学习一个生成带有遮挡和变形示例的对抗网络，并且上下文可能对处理遮挡有帮助 Zhang 等 ([2018b](#bib.bib309))。尽管有这些努力，但遮挡问题仍远未解决；将GANs应用于此问题可能是一个有前景的研究方向。
- en: 'Handling of image degradations: Image noise is a common problem in many real-world
    applications. It is frequently caused by insufficient lighting, low quality cameras,
    image compression, or the intentional low-cost sensors on edge devices and wearable
    devices. While low image quality may be expected to degrade the performance of
    visual recognition, most current methods are evaluated in a degradation free and
    clean environment, evidenced by the fact that PASCAL VOC, ImageNet, MS COCO and
    Open Images all focus on relatively high quality images. To the best of our knowledge,
    there is so far very limited work to address this problem.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 处理图像退化：图像噪声是许多实际应用中的常见问题。它通常由光线不足、低质量摄像头、图像压缩或边缘设备和可穿戴设备上的廉价传感器引起。虽然低图像质量可能会降低视觉识别的性能，但大多数当前方法在无退化和干净的环境中评估，PASCAL
    VOC、ImageNet、MS COCO 和 Open Images 都关注相对高质量的图像。据我们所知，目前针对这一问题的研究非常有限。
- en: 7 Context Modeling
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 上下文建模
- en: 'Table 8: Summary of detectors that exploit context information, with labelling
    details as in Table [7](#S6.T7 "Table 7 ‣ 6.1 Popular CNN Architectures ‣ 6 Object
    Representation ‣ Deep Learning for Generic Object Detection: A Survey").'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '表8：利用上下文信息的检测器总结，标注细节见表 [7](#S6.T7 "Table 7 ‣ 6.1 Popular CNN Architectures
    ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")。'
- en: '|      | Detector | Region | Backbone | Pipelined | mAP@IoU=0.5 | mAP | Published
    |    |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|      | 检测器 | 区域 | 主干 | 管道化 | mAP@IoU=0.5 | mAP | 发布 |    |'
- en: '|    Group | Name | Proposal | DCNN | Used | VOC07 | VOC12 | COCO | In | Highlights
       |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|    组别 | 名称 | 提案 | DCNN | 使用 | VOC07 | VOC12 | COCO | 位置 | 亮点    |'
- en: '|      Global Context | SegDeepM Zhu et al. ([2015](#bib.bib326)) | SS+CMPC
    | VGG16 | RCNN | VOC10 | VOC12 | $-$ | CVPR15 | Additional features extracted
    from an enlarged object proposal as context information.    |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|      全球上下文 | SegDeepM Zhu 等 ([2015](#bib.bib326)) | SS+CMPC | VGG16 | RCNN
    | VOC10 | VOC12 | $-$ | CVPR15 | 从放大的对象提案中提取附加特征作为上下文信息。    |'
- en: '|   | DeepIDNet Ouyang et al. ([2015](#bib.bib203)) | SS+EB | AlexNet ZFNet
    | RCNN | $69.0$ (07) | $-$ | $-$ | CVPR15 | Use image classification scores as
    global contextual information to refine the detection scores of each object proposal.
       |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|   | DeepIDNet Ouyang 等 ([2015](#bib.bib203)) | SS+EB | AlexNet ZFNet | RCNN
    | $69.0$ (07) | $-$ | $-$ | CVPR15 | 使用图像分类分数作为全球上下文信息来细化每个对象提案的检测分数。    |'
- en: '|   | ION Bell et al. ([2016](#bib.bib11)) | SS+EB | VGG16 | Fast RCNN | $80.1$
    | $77.9$ | $33.1$ | CVPR16 | The contextual information outside the region of
    interest is integrated using spatial recurrent neural networks.    |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|   | ION Bell 等 ([2016](#bib.bib11)) | SS+EB | VGG16 | Fast RCNN | $80.1$
    | $77.9$ | $33.1$ | CVPR16 | 使用空间递归神经网络整合兴趣区域外的上下文信息。    |'
- en: '|   | CPF Shrivastava and Gupta ([2016](#bib.bib245)) | RPN | VGG16 | Faster
    RCNN | $76.4$ (07+12) | $72.6$ (07T+12) | $-$ | ECCV16 | Use semantic segmentation
    to provide top-down feedback.    |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|   | CPF Shrivastava 和 Gupta ([2016](#bib.bib245)) | RPN | VGG16 | Faster
    RCNN | $76.4$ (07+12) | $72.6$ (07T+12) | $-$ | ECCV16 | 使用语义分割提供自上而下的反馈。    |'
- en: '|      Local Context | MRCNN Gidaris and Komodakis ([2015](#bib.bib82)) | SS
    | VGG16 | SPPNet | $78.2$ (07+12) | $73.9$ (07+12) | $-$ | ICCV15 | Extract features
    from multiple regions surrounding or inside the object proposals. Integrate the
    semantic segmentation-aware features.    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|      局部上下文 | MRCNN Gidaris 和 Komodakis ([2015](#bib.bib82)) | SS | VGG16
    | SPPNet | $78.2$ (07+12) | $73.9$ (07+12) | $-$ | ICCV15 | 从围绕或位于物体提议内的多个区域中提取特征。整合语义分割感知特征。
       |'
- en: '|   | GBDNet Zeng et al. ([2016](#bib.bib304), [2017](#bib.bib305)) | CRAFT
    Yang et al. ([2016a](#bib.bib292)) | Inception v2 ResNet269 PolyNet Zhang et al.
    ([2017](#bib.bib311)) | Fast RCNN | $77.2$ (07+12) | $-$ | $27.0$ | ECCV16 TPAMI18
    | A GBDNet module to learn the relations of multiscale contextualized regions
    surrounding an object proposal; GBDNet passes messages among features from different
    context regions through convolution between neighboring support regions in two
    directions.    |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|   | GBDNet Zeng 等人 ([2016](#bib.bib304), [2017](#bib.bib305)) | CRAFT Yang
    等人 ([2016a](#bib.bib292)) | Inception v2 ResNet269 PolyNet Zhang 等人 ([2017](#bib.bib311))
    | Fast RCNN | $77.2$ (07+12) | $-$ | $27.0$ | ECCV16 TPAMI18 | 一个 GBDNet 模块，用于学习围绕物体提议的多尺度上下文区域的关系；GBDNet
    通过两个方向之间相邻支持区域的卷积在不同上下文区域之间传递信息。    |'
- en: '|   | ACCNNLi et al. ([2017b](#bib.bib157)) | SS | VGG16 | Fast RCNN | $72.0$
    (07+12) | $70.6$ (07T+12) | $-$ | TMM17 | Use LSTM to capture global context.
    Concatenate features from multi-scale contextual regions surrounding an object
    proposal. The global and local context features are concatenated for recognition.
       |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|   | ACCNNLi 等人 ([2017b](#bib.bib157)) | SS | VGG16 | Fast RCNN | $72.0$ (07+12)
    | $70.6$ (07T+12) | $-$ | TMM17 | 使用 LSTM 捕捉全局上下文。从围绕物体提议的多尺度上下文区域中连接特征。全局和局部上下文特征用于识别。
       |'
- en: '|   | CoupleNetZhu et al. ([2017a](#bib.bib327)) | RPN | ResNet101 | RFCN |
    82.7 (07+12) | 80.4 (07T+12) | $34.4$ | ICCV17 | Concatenate features from multiscale
    contextual regions surrounding an object proposal. Features of different contextual
    regions are then combined by convolution and element-wise sum.    |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '|   | CoupleNetZhu 等人 ([2017a](#bib.bib327)) | RPN | ResNet101 | RFCN | 82.7
    (07+12) | 80.4 (07T+12) | $34.4$ | ICCV17 | 从围绕物体提议的多尺度上下文区域中连接特征。不同上下文区域的特征通过卷积和逐元素求和进行组合。
       |'
- en: '|   | SMN Chen and Gupta ([2017](#bib.bib35)) | RPN | VGG16 | Faster RCNN |
    $70.0$ (07) | $-$ | $-$ | ICCV17 | Model object-object relationships efficiently
    through a spatial memory network. Learn the functionality of NMS automatically.
       |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|   | SMN Chen 和 Gupta ([2017](#bib.bib35)) | RPN | VGG16 | Faster RCNN | $70.0$
    (07) | $-$ | $-$ | ICCV17 | 通过空间记忆网络有效建模对象-对象关系。自动学习 NMS 的功能。    |'
- en: '|   | ORN Hu et al. ([2018a](#bib.bib114)) | RPN | ResNet101 +DCN | Faster
    RCNN | $-$ | $-$ | 39.0 | CVPR18 | Model the relations of a set of object proposals
    through the interactions between their appearance features and geometry. Learn
    the functionality of NMS automatically.    |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|   | ORN Hu 等人 ([2018a](#bib.bib114)) | RPN | ResNet101 +DCN | Faster RCNN
    | $-$ | $-$ | 39.0 | CVPR18 | 通过对象提议的外观特征和几何形状之间的交互建模一组对象提议的关系。自动学习 NMS 的功能。   
    |'
- en: '|   | SIN Liu et al. ([2018d](#bib.bib176)) | RPN | VGG16 | Faster RCNN | $76.0$
    (07+12) | $73.1$ (07T+12) | $23.2$ | CVPR18 | Formulate object detection as graph-structured
    inference, where objects are graph nodes and relationships the edges.    |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|   | SIN Liu 等人 ([2018d](#bib.bib176)) | RPN | VGG16 | Faster RCNN | $76.0$
    (07+12) | $73.1$ (07T+12) | $23.2$ | CVPR18 | 将对象检测形式化为图结构推理，其中对象是图节点，关系是边。'
- en: '|      |  |  |  |  |  |  |  |  |  | ![Refer to caption](img/182b3e54206eb271a2a4a478cb518f00.png)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '|      |  |  |  |  |  |  |  |  |  | ![参见说明](img/182b3e54206eb271a2a4a478cb518f00.png)'
- en: 'Figure 18: Representative approaches that explore local surrounding contextual
    features: MRCNN Gidaris and Komodakis ([2015](#bib.bib82)), GBDNet Zeng et al.
    ([2016](#bib.bib304), [2017](#bib.bib305)), ACCNN Li et al. ([2017b](#bib.bib157))
    and CoupleNet Zhu et al. ([2017a](#bib.bib327)); also see Table [8](#S7.T8 "Table
    8 ‣ 7 Context Modeling ‣ Deep Learning for Generic Object Detection: A Survey").'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18：探索局部周围上下文特征的代表性方法：MRCNN Gidaris 和 Komodakis ([2015](#bib.bib82))，GBDNet
    Zeng 等人 ([2016](#bib.bib304), [2017](#bib.bib305))，ACCNN Li 等人 ([2017b](#bib.bib157))
    和 CoupleNet Zhu 等人 ([2017a](#bib.bib327))；另见表 [8](#S7.T8 "Table 8 ‣ 7 Context
    Modeling ‣ Deep Learning for Generic Object Detection: A Survey")。'
- en: 'In the physical world, visual objects occur in particular environments and
    usually coexist with other related objects. There is strong psychological evidence
    Biederman ([1972](#bib.bib14)); Bar ([2004](#bib.bib10)) that context plays an
    essential role in human object recognition, and it is recognized that a proper
    modeling of context helps object detection and recognition Torralba ([2003](#bib.bib266));
    Oliva and Torralba ([2007](#bib.bib197)); Chen et al. ([2018b](#bib.bib33), [2015a](#bib.bib32));
    Divvala et al. ([2009](#bib.bib58)); Galleguillos and Belongie ([2010](#bib.bib78)),
    especially when object appearance features are insufficient because of small object
    size, object occlusion, or poor image quality. Many different types of context
    have been discussed Divvala et al. ([2009](#bib.bib58)); Galleguillos and Belongie
    ([2010](#bib.bib78)), and can broadly be grouped into one of three categories:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理世界中，视觉物体通常出现在特定的环境中，并且通常与其他相关物体共存。有强有力的心理学证据（Biederman ([1972](#bib.bib14));
    Bar ([2004](#bib.bib10)）表明，上下文在人的物体识别中扮演着重要角色，并且已认识到，适当的上下文建模有助于物体检测和识别（Torralba
    ([2003](#bib.bib266)); Oliva and Torralba ([2007](#bib.bib197)); Chen et al. ([2018b](#bib.bib33),
    [2015a](#bib.bib32)); Divvala et al. ([2009](#bib.bib58)); Galleguillos and Belongie
    ([2010](#bib.bib78))），特别是在物体外观特征因物体尺寸过小、物体遮挡或图像质量差而不足时。许多不同类型的上下文已经被讨论（Divvala
    et al. ([2009](#bib.bib58)); Galleguillos and Belongie ([2010](#bib.bib78))），可以大致分为三类：
- en: '1.'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Semantic context: The likelihood of an object to be found in some scenes, but
    not in others;'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语义上下文：某个物体在某些场景中出现的可能性，而在其他场景中则不出现；
- en: '2.'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Spatial context: The likelihood of finding an object in some position and not
    others with respect to other objects in the scene;'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 空间上下文：在场景中，相对于其他物体，某个物体出现在某个位置的可能性；
- en: '3.'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Scale context: Objects have a limited set of sizes relative to other objects
    in the scene.'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尺度上下文：物体相对于场景中的其他物体具有有限的尺寸集合。
- en: A great deal of work Chen et al. ([2015b](#bib.bib34)); Divvala et al. ([2009](#bib.bib58));
    Galleguillos and Belongie ([2010](#bib.bib78)); Malisiewicz and Efros ([2009](#bib.bib185));
    Murphy et al. ([2003](#bib.bib193)); Rabinovich et al. ([2007](#bib.bib220));
    Parikh et al. ([2012](#bib.bib207)) preceded the prevalence of deep learning,
    and much of this work has yet to be explored in DCNN-based object detectors Chen
    and Gupta ([2017](#bib.bib35)); Hu et al. ([2018a](#bib.bib114)).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习普及之前，进行了大量的工作（Chen et al. ([2015b](#bib.bib34)); Divvala et al. ([2009](#bib.bib58));
    Galleguillos and Belongie ([2010](#bib.bib78)); Malisiewicz and Efros ([2009](#bib.bib185));
    Murphy et al. ([2003](#bib.bib193)); Rabinovich et al. ([2007](#bib.bib220));
    Parikh et al. ([2012](#bib.bib207)），这些工作中大部分仍未在基于DCNN的物体检测器中得到探讨（Chen and Gupta
    ([2017](#bib.bib35)); Hu et al. ([2018a](#bib.bib114))）。
- en: 'The current state of the art in object detection Ren et al. ([2015](#bib.bib229));
    Liu et al. ([2016](#bib.bib175)); He et al. ([2017](#bib.bib102)) detects objects
    without explicitly exploiting any contextual information. It is broadly agreed
    that DCNNs make use of contextual information implicitly Zeiler and Fergus ([2014](#bib.bib303));
    Zheng et al. ([2015](#bib.bib316)) since they learn hierarchical representations
    with multiple levels of abstraction. Nevertheless, there is value in exploring
    contextual information explicitly in DCNN based detectors Hu et al. ([2018a](#bib.bib114));
    Chen and Gupta ([2017](#bib.bib35)); Zeng et al. ([2017](#bib.bib305)), so the
    following reviews recent work in exploiting contextual cues in DCNN- based object
    detectors, organized into categories of global and local contexts, motivated by
    earlier work in Zhang et al. ([2013](#bib.bib310)); Galleguillos and Belongie
    ([2010](#bib.bib78)). Representative approaches are summarized in Table [8](#S7.T8
    "Table 8 ‣ 7 Context Modeling ‣ Deep Learning for Generic Object Detection: A
    Survey").'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '当前的物体检测技术（Ren et al. ([2015](#bib.bib229)); Liu et al. ([2016](#bib.bib175));
    He et al. ([2017](#bib.bib102))）在不明确利用任何上下文信息的情况下检测物体。普遍认为，深度卷积神经网络（DCNNs）隐式地利用了上下文信息（Zeiler
    and Fergus ([2014](#bib.bib303)); Zheng et al. ([2015](#bib.bib316))），因为它们学习了具有多个抽象层次的层级表示。然而，探索在基于DCNN的检测器中显式利用上下文信息是有价值的（Hu
    et al. ([2018a](#bib.bib114)); Chen and Gupta ([2017](#bib.bib35)); Zeng et al.
    ([2017](#bib.bib305))），因此以下内容回顾了在基于DCNN的物体检测器中利用上下文线索的近期研究，并按全球和局部上下文的类别进行组织，受到Zhang
    et al. ([2013](#bib.bib310)); Galleguillos and Belongie ([2010](#bib.bib78))等早期工作的启发。代表性的方法总结在表[8](#S7.T8
    "Table 8 ‣ 7 Context Modeling ‣ Deep Learning for Generic Object Detection: A
    Survey")中。'
- en: 7.1 Global Context
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 全球上下文
- en: Global context Zhang et al. ([2013](#bib.bib310)); Galleguillos and Belongie
    ([2010](#bib.bib78)) refers to image or scene level contexts, which can serve
    as cues for object detection (*e.g.,* a bedroom will predict the presence of a
    bed). In DeepIDNet Ouyang et al. ([2015](#bib.bib203)), the image classification
    scores were used as contextual features, and concatenated with the object detection
    scores to improve detection results. In ION Bell et al. ([2016](#bib.bib11)),
    Bell *et al.* proposed to use spatial Recurrent Neural Networks (RNNs) to explore
    contextual information across the entire image. In SegDeepM Zhu et al. ([2015](#bib.bib326)),
    Zhu *et al.* proposed a Markov random field model that scores appearance as well
    as context for each detection, and allows each candidate box to select a segment
    out of a large pool of object segmentation proposals and score the agreement between
    them. In Shrivastava and Gupta ([2016](#bib.bib245)), semantic segmentation was
    used as a form of contextual priming.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 全局上下文 Zhang et al. ([2013](#bib.bib310)); Galleguillos 和 Belongie ([2010](#bib.bib78))
    指的是图像或场景级别的上下文，这些上下文可以作为对象检测的线索（*例如*，一个卧室会预测床的存在）。在 DeepIDNet Ouyang et al. ([2015](#bib.bib203))
    中，图像分类分数被用作上下文特征，并与对象检测分数连接以改进检测结果。在 ION Bell et al. ([2016](#bib.bib11)) 中，Bell
    *et al.* 提议使用空间递归神经网络（RNNs）来探索整个图像的上下文信息。在 SegDeepM Zhu et al. ([2015](#bib.bib326))
    中，Zhu *et al.* 提出了一个马尔科夫随机场模型，该模型对每个检测进行外观以及上下文评分，并允许每个候选框从大量对象分割提案中选择一个分割，并对它们之间的一致性进行评分。在
    Shrivastava 和 Gupta ([2016](#bib.bib245)) 中，语义分割被用作一种上下文引导的形式。
- en: 7.2 Local Context
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 本地上下文
- en: 'Local context Zhang et al. ([2013](#bib.bib310)); Galleguillos and Belongie
    ([2010](#bib.bib78)); Rabinovich et al. ([2007](#bib.bib220)) considers the relationship
    among locally nearby objects, as well as the interactions between an object and
    its surrounding area. In general, modeling object relations is challenging, requiring
    reasoning about bounding boxes of different classes, locations, scales *etc*.
    Deep learning research that explicitly models object relations is quite limited,
    with representative ones being Spatial Memory Network (SMN) Chen and Gupta ([2017](#bib.bib35)),
    Object Relation Network Hu et al. ([2018a](#bib.bib114)), and Structure Inference
    Network (SIN) Liu et al. ([2018d](#bib.bib176)). In SMN, spatial memory essentially
    assembles object instances back into a pseudo image representation that is easy
    to be fed into another CNN for object relations reasoning, leading to a new sequential
    reasoning architecture where image and memory are processed in parallel to obtain
    detections which further update memory. Inspired by the recent success of attention
    modules in natural language processing Vaswani et al. ([2017](#bib.bib274)), ORN
    processes a set of objects simultaneously through the interaction between their
    appearance feature and geometry. It does not require additional supervision, and
    it is easy to embed into existing networks, effective in improving object recognition
    and duplicate removal steps in modern object detection pipelines, giving rise
    to the first fully end-to-end object detector. SIN Liu et al. ([2018d](#bib.bib176))
    considered two kinds of context: scene contextual information and object relationships
    within a single image. It formulates object detection as a problem of graph inference,
    where the objects are treated as nodes in a graph and relationships between objects
    are modeled as edges.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 本地上下文 Zhang et al. ([2013](#bib.bib310)); Galleguillos 和 Belongie ([2010](#bib.bib78));
    Rabinovich et al. ([2007](#bib.bib220)) 考虑了局部附近对象之间的关系，以及对象与其周围区域的互动。一般来说，建模对象关系具有挑战性，需要推理不同类别、位置、尺度的边界框
    *等*。明确建模对象关系的深度学习研究相当有限，具有代表性的有空间记忆网络（SMN）Chen 和 Gupta ([2017](#bib.bib35))，对象关系网络
    Hu et al. ([2018a](#bib.bib114))，以及结构推断网络（SIN）Liu et al. ([2018d](#bib.bib176))。在
    SMN 中，空间记忆实质上将对象实例重新组装成一个伪图像表示，这样就可以将其输入到另一个 CNN 中进行对象关系推理，从而形成一种新的序列推理架构，其中图像和记忆并行处理，以获得检测结果并进一步更新记忆。受到自然语言处理中的注意力模块最近成功的启发
    Vaswani et al. ([2017](#bib.bib274))，ORN 通过对象外观特征和几何形状之间的互动同时处理一组对象。它不需要额外的监督，并且容易嵌入到现有网络中，能够有效提高现代对象检测管道中的对象识别和重复去除步骤，从而催生了第一个完全端到端的对象检测器。SIN
    Liu et al. ([2018d](#bib.bib176)) 考虑了两种上下文：场景上下文信息和单幅图像中的对象关系。它将对象检测表述为图推断问题，其中对象被视为图中的节点，对象之间的关系被建模为边。
- en: 'A wider range of methods has approached the context challenge with a simpler
    idea: enlarging the detection window size to extract some form of local context.
    Representative approaches include MRCNN Gidaris and Komodakis ([2015](#bib.bib82)),
    Gated BiDirectional CNN (GBDNet) Zeng et al. ([2016](#bib.bib304), [2017](#bib.bib305)),
    Attention to Context CNN (ACCNN) Li et al. ([2017b](#bib.bib157)), CoupleNet Zhu
    et al. ([2017a](#bib.bib327)), and Sermanet *et al.* Sermanet et al. ([2013](#bib.bib238)).
    In MRCNN Gidaris and Komodakis ([2015](#bib.bib82)) (Fig. [18](#S7.F18 "Figure
    18 ‣ 7 Context Modeling ‣ Deep Learning for Generic Object Detection: A Survey")
    (a)), in addition to the features extracted from the original object proposal
    at the last CONV layer of the backbone, Gidaris and Komodakis proposed to extract
    features from a number of different regions of an object proposal (half regions,
    border regions, central regions, contextual region and semantically segmented
    regions), in order to obtain a richer and more robust object representation. All
    of these features are combined by concatenation.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '更广泛的方法已经采用更简单的思路来应对上下文挑战：扩大检测窗口大小以提取某种形式的局部上下文。代表性的方法包括 MRCNN Gidaris 和 Komodakis
    ([2015](#bib.bib82))，Gated BiDirectional CNN (GBDNet) Zeng 等 ([2016](#bib.bib304)，[2017](#bib.bib305))，Attention
    to Context CNN (ACCNN) Li 等 ([2017b](#bib.bib157))，CoupleNet Zhu 等 ([2017a](#bib.bib327))，以及
    Sermanet *et al.* Sermanet 等 ([2013](#bib.bib238))。在 MRCNN Gidaris 和 Komodakis
    ([2015](#bib.bib82)) (图 [18](#S7.F18 "Figure 18 ‣ 7 Context Modeling ‣ Deep Learning
    for Generic Object Detection: A Survey") (a)) 中，除了从主干网络最后 CONV 层提取的原始对象提案特征外，Gidaris
    和 Komodakis 提议从对象提案的多个不同区域（半区域、边界区域、中心区域、上下文区域和语义分割区域）提取特征，以获得更丰富和更强健的对象表示。所有这些特征通过连接进行组合。'
- en: 'Quite a number of methods, all closely related to MRCNN, have been proposed
    since then. The method in Zagoruyko et al. ([2016](#bib.bib302)) used only four
    contextual regions, organized in a foveal structure, where the classifiers along
    multiple paths are trained jointly end-to-end. Zeng *et al.* proposed GBDNet Zeng
    et al. ([2016](#bib.bib304), [2017](#bib.bib305)) (Fig. [18](#S7.F18 "Figure 18
    ‣ 7 Context Modeling ‣ Deep Learning for Generic Object Detection: A Survey")
    (b)) to extract features from multiscale contextualized regions surrounding an
    object proposal to improve detection performance. In contrast to the somewhat
    naive approach of learning CNN features for each region separately and then concatenating
    them, GBDNet passes messages among features from different contextual regions.
    Noting that message passing is not always helpful, but dependent on individual
    samples, Zeng *et al.* Zeng et al. ([2016](#bib.bib304)) used gated functions
    to control message transmission. Li *et al.* Li et al. ([2017b](#bib.bib157))
    presented ACCNN (Fig. [18](#S7.F18 "Figure 18 ‣ 7 Context Modeling ‣ Deep Learning
    for Generic Object Detection: A Survey") (c)) to utilize both global and local
    contextual information: the global context was captured using a Multiscale Local
    Contextualized (MLC) subnetwork, which recurrently generates an attention map
    for an input image to highlight promising contextual locations; local context
    adopted a method similar to that of MRCNN Gidaris and Komodakis ([2015](#bib.bib82)).
    As shown in Fig. [18](#S7.F18 "Figure 18 ‣ 7 Context Modeling ‣ Deep Learning
    for Generic Object Detection: A Survey") (d), CoupleNet Zhu et al. ([2017a](#bib.bib327))
    is conceptually similar to ACCNN Li et al. ([2017b](#bib.bib157)), but built upon
    RFCN Dai et al. ([2016c](#bib.bib50)), which captures object information with
    position sensitive RoI pooling, CoupleNet added a branch to encode the global
    context with RoI pooling.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 自那时起，提出了许多方法，这些方法都与MRCNN紧密相关。Zagoruyko等人（[2016](#bib.bib302)）的方法仅使用了四个上下文区域，这些区域以中心结构组织，其中沿多条路径的分类器共同进行端到端训练。Zeng
    *et al.* 提出了GBDNet Zeng等人（[2016](#bib.bib304)，[2017](#bib.bib305)）（图[18](#S7.F18
    "图18 ‣ 7 上下文建模 ‣ 通用目标检测的深度学习：综述") (b)）以从环绕目标提案的多尺度上下文区域中提取特征，从而提高检测性能。与将CNN特征分别学习每个区域然后进行拼接的较为天真的方法相比，GBDNet在不同上下文区域的特征之间传递信息。注意到消息传递并不总是有帮助，而是依赖于个体样本，Zeng
    *et al.* Zeng等人（[2016](#bib.bib304)）使用门控函数来控制消息传递。Li *et al.* Li等人（[2017b](#bib.bib157)）提出了ACCNN（图[18](#S7.F18
    "图18 ‣ 7 上下文建模 ‣ 通用目标检测的深度学习：综述") (c)），利用了全球和局部上下文信息：全球上下文通过多尺度局部上下文（MLC）子网络捕获，该子网络递归生成一个注意力图以突出有前景的上下文位置；局部上下文采用类似于MRCNN
    Gidaris和Komodakis（[2015](#bib.bib82)）的方法。如图[18](#S7.F18 "图18 ‣ 7 上下文建模 ‣ 通用目标检测的深度学习：综述")
    (d)所示，CoupleNet Zhu等人（[2017a](#bib.bib327)）在概念上类似于ACCNN Li等人（[2017b](#bib.bib157)），但基于RFCN
    Dai等人（[2016c](#bib.bib50)），RFCN通过位置敏感的RoI池化来捕获目标信息，CoupleNet添加了一个分支，用于通过RoI池化编码全球上下文。
- en: 8 Detection Proposal Methods
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 种检测提案方法
- en: An object can be located at any position and scale in an image. During the heyday
    of handcrafted feature descriptors (SIFT Lowe ([2004](#bib.bib179)), HOG Dalal
    and Triggs ([2005](#bib.bib52)) and LBP Ojala et al. ([2002](#bib.bib196))), the
    most successful methods for object detection (*e.g.* DPM Felzenszwalb et al. ([2008](#bib.bib72)))
    used *sliding window* techniques Viola and Jones ([2001](#bib.bib276)); Dalal
    and Triggs ([2005](#bib.bib52)); Felzenszwalb et al. ([2008](#bib.bib72)); Harzallah
    et al. ([2009](#bib.bib98)); Vedaldi et al. ([2009](#bib.bib275)). However, the
    number of windows is huge, growing with the number of pixels in an image, and
    the need to search at multiple scales and aspect ratios further increases the
    search space^(12)^(12)12Sliding window based detection requires classifying around
    $10^{4}$-$10^{5}$ windows per image. The number of windows grows significantly
    to $10^{6}$-$10^{7}$ windows per image when considering multiple scales and aspect
    ratios.. Therefore, it is computationally too expensive to apply sophisticated
    classifiers.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 对象可以在图像中的任何位置和尺度上被定位。在手工特征描述符（SIFT Lowe ([2004](#bib.bib179)), HOG Dalal 和 Triggs
    ([2005](#bib.bib52)) 和 LBP Ojala 等 ([2002](#bib.bib196))) 的鼎盛时期，最成功的目标检测方法（*例如*
    DPM Felzenszwalb 等 ([2008](#bib.bib72))) 使用了*滑动窗口*技术 Viola 和 Jones ([2001](#bib.bib276));
    Dalal 和 Triggs ([2005](#bib.bib52)); Felzenszwalb 等 ([2008](#bib.bib72)); Harzallah
    等 ([2009](#bib.bib98)); Vedaldi 等 ([2009](#bib.bib275))。然而，窗口的数量巨大，随着图像中像素数量的增加而增长，并且需要在多个尺度和纵横比下搜索进一步增加了搜索空间^(12)^(12)12基于滑动窗口的检测需要对每个图像分类大约$10^{4}$-$10^{5}$个窗口。当考虑多个尺度和纵横比时，窗口的数量显著增长到$10^{6}$-$10^{7}$个窗口。因此，应用复杂的分类器在计算上代价过高。
- en: 'Around 2011, researchers proposed to relieve the tension between computational
    tractability and high detection quality by using *detection proposals*^(13)^(13)13We
    use the terminology *detection proposals*, *object proposals* and *region proposals*
    interchangeably. Van de Sande et al. ([2011](#bib.bib273)); Uijlings et al. ([2013](#bib.bib271)).
    Originating in the idea of *objectness* proposed by Alexe et al. ([2010](#bib.bib2)),
    object proposals are a set of candidate regions in an image that are likely to
    contain objects, and if high object recall can be achieved with a modest number
    of object proposals (like one hundred), significant speed-ups over the sliding
    window approach can be gained, allowing the use of more sophisticated classifiers.
    Detection proposals are usually used as a pre-processing step, limiting the number
    of regions that need to be evaluated by the detector, and should have the following
    characteristics:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在2011年，研究人员提出通过使用*检测提议*^(13)^(13)13我们使用术语*检测提议*、*对象提议*和*区域提议*可互换。Van de Sande
    等 ([2011](#bib.bib273)); Uijlings 等 ([2013](#bib.bib271))来缓解计算可行性与高检测质量之间的紧张关系。源于
    Alexe 等 ([2010](#bib.bib2)) 提出的*对象性*概念，对象提议是一组图像中的候选区域，这些区域可能包含对象，并且如果可以通过适量的对象提议（如一百个）实现高对象召回率，则可以显著加快速度，超越滑动窗口方法，从而允许使用更复杂的分类器。检测提议通常作为预处理步骤使用，限制了检测器需要评估的区域数量，并应具备以下特征：
- en: '1.'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: High recall, which can be achieved with only a few proposals;
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高召回率，这可以通过仅几个提议来实现；
- en: '2.'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Accurate localization, such that the proposals match the object bounding boxes
    as accurately as possible; and
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确的定位，使得提议尽可能准确地匹配对象边界框；并且
- en: '3.'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Low computational cost.
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 低计算成本。
- en: The success of object detection based on detection proposals Van de Sande et al.
    ([2011](#bib.bib273)); Uijlings et al. ([2013](#bib.bib271)) has attracted broad
    interest Carreira and Sminchisescu ([2012](#bib.bib25)); Arbeláez et al. ([2014](#bib.bib7));
    Alexe et al. ([2012](#bib.bib3)); Cheng et al. ([2014](#bib.bib43)); Zitnick and
    Dollár ([2014](#bib.bib330)); Endres and Hoiem ([2010](#bib.bib65)); Krähenbühl1
    and Koltun ([2014](#bib.bib138)); Manen et al. ([2013](#bib.bib186)). A comprehensive
    review of object proposal algorithms is beyond the scope of this paper, because
    object proposals have applications beyond object detection Arbeláez et al. ([2012](#bib.bib6));
    Guillaumin et al. ([2014](#bib.bib93)); Zhu et al. ([2017b](#bib.bib328)). We
    refer interested readers to the recent surveys Hosang et al. ([2016](#bib.bib110));
    Chavali et al. ([2016](#bib.bib27)) which provide in-depth analysis of many classical
    object proposal algorithms and their impact on detection performance. Our interest
    here is to review object proposal methods that are based on DCNNs, output class
    agnostic proposals, and are related to generic object detection.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 基于检测提议 Van de Sande 等人 ([2011](#bib.bib273))；Uijlings 等人 ([2013](#bib.bib271))
    的目标检测成功引起了广泛关注 Carreira 和 Sminchisescu ([2012](#bib.bib25))；Arbeláez 等人 ([2014](#bib.bib7))；Alexe
    等人 ([2012](#bib.bib3))；Cheng 等人 ([2014](#bib.bib43))；Zitnick 和 Dollár ([2014](#bib.bib330))；Endres
    和 Hoiem ([2010](#bib.bib65))；Krähenbühl1 和 Koltun ([2014](#bib.bib138))；Manen
    等人 ([2013](#bib.bib186))。全面回顾目标提议算法超出了本文的范围，因为目标提议在目标检测之外还有其他应用 Arbeláez 等人 ([2012](#bib.bib6))；Guillaumin
    等人 ([2014](#bib.bib93))；Zhu 等人 ([2017b](#bib.bib328))。我们建议感兴趣的读者参考最近的调查报告 Hosang
    等人 ([2016](#bib.bib110))；Chavali 等人 ([2016](#bib.bib27))，它们对许多经典目标提议算法及其对检测性能的影响进行了深入分析。我们这里的兴趣是回顾基于
    DCNNs 的目标提议方法，这些方法输出与类别无关的提议，并与通用目标检测相关。
- en: In 2014, the integration of object proposals Van de Sande et al. ([2011](#bib.bib273));
    Uijlings et al. ([2013](#bib.bib271)) and DCNN features Krizhevsky et al. ([2012a](#bib.bib140))
    led to the milestone RCNN Girshick et al. ([2014](#bib.bib85)) in generic object
    detection. Since then, detection proposal has quickly become a standard preprocessing
    step, based on the fact that all winning entries in the PASCAL VOC Everingham
    et al. ([2010](#bib.bib68)), ILSVRC Russakovsky et al. ([2015](#bib.bib234)) and
    MS COCO Lin et al. ([2014](#bib.bib166)) object detection challenges since 2014
    used detection proposals Girshick et al. ([2014](#bib.bib85)); Ouyang et al. ([2015](#bib.bib203));
    Girshick ([2015](#bib.bib84)); Ren et al. ([2015](#bib.bib229)); Zeng et al. ([2017](#bib.bib305));
    He et al. ([2017](#bib.bib102)).
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在2014年，目标提议 Van de Sande 等人 ([2011](#bib.bib273))；Uijlings 等人 ([2013](#bib.bib271))
    与 DCNN 特征 Krizhevsky 等人 ([2012a](#bib.bib140)) 的整合带来了里程碑式的 RCNN Girshick 等人 ([2014](#bib.bib85))
    在通用目标检测中的成功。从那时起，检测提议迅速成为标准预处理步骤，因为自2014年以来，PASCAL VOC Everingham 等人 ([2010](#bib.bib68))、ILSVRC
    Russakovsky 等人 ([2015](#bib.bib234)) 和 MS COCO Lin 等人 ([2014](#bib.bib166)) 目标检测挑战赛的所有获胜作品都使用了检测提议
    Girshick 等人 ([2014](#bib.bib85))；Ouyang 等人 ([2015](#bib.bib203))；Girshick ([2015](#bib.bib84))；Ren
    等人 ([2015](#bib.bib229))；Zeng 等人 ([2017](#bib.bib305))；He 等人 ([2017](#bib.bib102))。
- en: Among object proposal approaches based on traditional low-level cues (*e.g.,*
    color, texture, edge and gradients), Selective Search Uijlings et al. ([2013](#bib.bib271)),
    MCG Arbeláez et al. ([2014](#bib.bib7)) and EdgeBoxes Zitnick and Dollár ([2014](#bib.bib330))
    are among the more popular. As the domain rapidly progressed, traditional object
    proposal approaches Uijlings et al. ([2013](#bib.bib271)); Hosang et al. ([2016](#bib.bib110));
    Zitnick and Dollár ([2014](#bib.bib330)), which were adopted as external modules
    independent of the detectors, became the speed bottleneck of the detection pipeline
    Ren et al. ([2015](#bib.bib229)). An emerging class of object proposal algorithms
    Erhan et al. ([2014](#bib.bib67)); Ren et al. ([2015](#bib.bib229)); Kuo et al.
    ([2015](#bib.bib142)); Ghodrati et al. ([2015](#bib.bib81)); Pinheiro et al. ([2015](#bib.bib213));
    Yang et al. ([2016a](#bib.bib292)) using DCNNs has attracted broad attention.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 基于传统低级线索（*例如*，颜色、纹理、边缘和梯度）的目标提议方法中，选择性搜索 Uijlings 等人 ([2013](#bib.bib271))、MCG
    Arbeláez 等人 ([2014](#bib.bib7)) 和 EdgeBoxes Zitnick 和 Dollár ([2014](#bib.bib330))
    是较为流行的方法。随着领域的快速发展，传统目标提议方法 Uijlings 等人 ([2013](#bib.bib271))；Hosang 等人 ([2016](#bib.bib110))；Zitnick
    和 Dollár ([2014](#bib.bib330))，作为独立于检测器的外部模块，成为了检测管道的速度瓶颈 Ren 等人 ([2015](#bib.bib229))。一类新兴的目标提议算法
    Erhan 等人 ([2014](#bib.bib67))；Ren 等人 ([2015](#bib.bib229))；Kuo 等人 ([2015](#bib.bib142))；Ghodrati
    等人 ([2015](#bib.bib81))；Pinheiro 等人 ([2015](#bib.bib213))；Yang 等人 ([2016a](#bib.bib292))
    使用 DCNNs 已引起广泛关注。
- en: 'Table 9: Summary of object proposal methods using DCNN. Blue indicates the
    number of object proposals. The detection results on COCO are based on mAP@IoU[0.5,
    0.95], unless stated otherwise.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：使用DCNN的对象提议方法汇总。蓝色表示对象提议的数量。COCO上的检测结果基于mAP@IoU[0.5, 0.95]，除非另有说明。
- en: '|      | Proposer | Backbone | Detector | Recall@IoU (VOC07) | Detection Results
    (mAP) | Published |    |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '|      | 提出者 | 主干网络 | 检测器 | Recall@IoU (VOC07) | 检测结果 (mAP) | 发表年份 |    |'
- en: '|   | Name | Network | Tested | $0.5$ | $0.7$ | $0.9$ | VOC07 | VOC12 | COCO
    | In | Highlights    |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|   | 名称 | 网络 | 测试 | $0.5$ | $0.7$ | $0.9$ | VOC07 | VOC12 | COCO | 在 | 高亮
       |'
- en: '|    Bounding Box Object Proposal Methods                         | MultiBox1Erhan
    et al. ([2014](#bib.bib67)) | AlexNet | RCNN | $-$ | $-$ | $-$ | $29.0$ (10) (12)
    | $-$ | $-$ | CVPR14 | Learns a class agnostic regressor on a small set of 800
    predefined anchor boxes. Do not share features for detection.    |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|    边界框对象提议方法                         | MultiBox1Erhan et al. ([2014](#bib.bib67))
    | AlexNet | RCNN | $-$ | $-$ | $-$ | $29.0$ (10) (12) | $-$ | $-$ | CVPR14 | 在小量的800个预定义锚框上学习一个类别无关的回归器。不共享检测特征。
       |'
- en: '|   | DeepBox Kuo et al. ([2015](#bib.bib142)) | VGG16 | Fast RCNN | $0.96$
    (1000) | $0.84$ (1000) | $0.15$ (1000) | $-$ | $-$ | $37.8$ (500) (IoU@0.5) |
    ICCV15 | Use a lightweight CNN to learn to rerank proposals generated by EdgeBox.
    Can run at 0.26s per image. Do not share features for detection.    |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|   | DeepBox Kuo et al. ([2015](#bib.bib142)) | VGG16 | Fast RCNN | $0.96$
    (1000) | $0.84$ (1000) | $0.15$ (1000) | $-$ | $-$ | $37.8$ (500) (IoU@0.5) |
    ICCV15 | 使用轻量级CNN学习重新排序由EdgeBox生成的提议。每张图像运行时间为0.26秒。不共享检测特征。    |'
- en: '|   | RPNRen et al. ([2015](#bib.bib229), [2017a](#bib.bib230)) | VGG16 | Faster
    RCNN | $0.97$ (300) 0.98 (1000) | $0.79$ (300) 0.84 (1000) | $0.04$ (300) 0.04
    (1000) | $73.2$ (300) (07+12) | $70.4$ (300) (07++12) | $21.9$ (300) | NIPS15
    | The first to generate object proposals by sharing full image convolutional features
    with detection. Most widely used object proposal method. Significant improvements
    in detection speed.    |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|   | RPNRen et al. ([2015](#bib.bib229), [2017a](#bib.bib230)) | VGG16 | Faster
    RCNN | $0.97$ (300) 0.98 (1000) | $0.79$ (300) 0.84 (1000) | $0.04$ (300) 0.04
    (1000) | $73.2$ (300) (07+12) | $70.4$ (300) (07++12) | $21.9$ (300) | NIPS15
    | 首次通过与检测共享完整的图像卷积特征生成对象提议。最广泛使用的对象提议方法。显著提高检测速度。    |'
- en: '|   | DeepProposalGhodrati et al. ([2015](#bib.bib81)) | VGG16 | Fast RCNN
    | $0.74$ (100) 0.92 (1000) | $0.58$ (100) 0.80 (1000) | $0.12$ (100) 0.16 (1000)
    | $53.2$ (100) (07) | $-$ | $-$ | ICCV15 | Generate proposals inside a DCNN in
    a multiscale manner. Share features with the detection network.    |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|   | DeepProposalGhodrati et al. ([2015](#bib.bib81)) | VGG16 | Fast RCNN
    | $0.74$ (100) 0.92 (1000) | $0.58$ (100) 0.80 (1000) | $0.12$ (100) 0.16 (1000)
    | $53.2$ (100) (07) | $-$ | $-$ | ICCV15 | 在多尺度下生成DCNN内部的提议。与检测网络共享特征。    |'
- en: '|   | CRAFT Yang et al. ([2016a](#bib.bib292)) | VGG16 | Faster RCNN | $0.98$
    (300) | $0.90$ (300) | $0.13$ (300) | $75.7$ (07+12) | 71.3 (12) | $-$ | CVPR16
    | Introduced a classification network (*i.e.* two class Fast RCNN) cascade that
    comes after the RPN. Not sharing features extracted for detection.    |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|   | CRAFT Yang et al. ([2016a](#bib.bib292)) | VGG16 | Faster RCNN | $0.98$
    (300) | $0.90$ (300) | $0.13$ (300) | $75.7$ (07+12) | 71.3 (12) | $-$ | CVPR16
    | 引入了一个分类网络（*即* 两类Fast RCNN）级联在RPN之后。未共享为检测提取的特征。    |'
- en: '|   | AZNet Lu et al. ([2016](#bib.bib181)) | VGG16 | Fast RCNN | $0.91$ (300)
    | $0.71$ (300) | $0.11$ (300) | $70.4$ (07) | $-$ | $22.3$ | CVPR16 | Use coarse-to-fine
    search: start from large regions, then recursively search for subregions that
    may contain objects. Adaptively guide computational resources to focus on likely
    subregions.    |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|   | AZNet Lu et al. ([2016](#bib.bib181)) | VGG16 | Fast RCNN | $0.91$ (300)
    | $0.71$ (300) | $0.11$ (300) | $70.4$ (07) | $-$ | $22.3$ | CVPR16 | 使用粗到细的搜索方法：从大区域开始，然后递归搜索可能包含对象的子区域。自适应地引导计算资源，集中在可能的子区域上。
       |'
- en: '|   | ZIP Li et al. ([2018a](#bib.bib156)) | Inception v2 | Faster RCNN | $0.85$
    (300) COCO | $0.74$ (300) COCO | $0.35$ (300) COCO | $79.8$ (07+12) | $-$ | $-$
    | IJCV18 | Generate proposals using conv-deconv network with multilayers; Proposed
    a map attention decision (MAD) unit to assign the weights for features from different
    layers.    |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|   | ZIP Li et al. ([2018a](#bib.bib156)) | Inception v2 | Faster RCNN | $0.85$
    (300) COCO | $0.74$ (300) COCO | $0.35$ (300) COCO | $79.8$ (07+12) | $-$ | $-$
    | IJCV18 | 使用具有多层的卷积-反卷积网络生成提议；提出了一种映射注意力决策 (MAD) 单元，以为不同层的特征分配权重。    |'
- en: '|   | DeNetTychsenSmith and Petersson ([2017](#bib.bib269)) | ResNet101 | Fast
    RCNN | $0.82$ (300) | $0.74$ (300) | $0.48$ (300) | $77.1$ (07+12) | $73.9$ (07++12)
    | $33.8$ | ICCV17 | A lot faster than Faster RCNN; Introduces a bounding box corner
    estimation for predicting object proposals efficiently to replace RPN; Does not
    require predefined anchors.    |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|   | DeNetTychsenSmith 和 Petersson ([2017](#bib.bib269)) | ResNet101 | Fast
    RCNN | $0.82$ (300) | $0.74$ (300) | $0.48$ (300) | $77.1$ (07+12) | $73.9$ (07++12)
    | $33.8$ | ICCV17 | 比 Faster RCNN 快很多；引入了一个边界框角点估计来高效预测目标提议，以取代 RPN；不需要预定义的锚点。
       |'
- en: '|      | Proposer Name | Backbone Network | Detector Tested | Box Proposals
    (AR, COCO) | Segment Proposals (AR, COCO) | Published In | Highlights    |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|      | 提议者名称 | 主干网络 | 测试检测器 | 框提议 (AR, COCO) | 分割提议 (AR, COCO) | 发表在 | 亮点
       |'
- en: '|    Segment Proposal Methods        | DeepMask Pinheiro et al. ([2015](#bib.bib213))
    | VGG16 | Fast RCNN | $0.33$ (100), $0.48({\color[rgb]{0,0,1}1000})$ | $0.26$
    (100), $0.37({\color[rgb]{0,0,1}1000})$ | NIPS15 | First to generate object mask
    proposals with DCNN; Slow inference time; Need segmentation annotations for training;
    Not sharing features with detection network; Achieved mAP of $69.9\%$ (500) with
    Fast RCNN.    |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|    分割提议方法        | DeepMask Pinheiro 等 ([2015](#bib.bib213)) | VGG16 | Fast
    RCNN | $0.33$ (100), $0.48({\color[rgb]{0,0,1}1000})$ | $0.26$ (100), $0.37({\color[rgb]{0,0,1}1000})$
    | NIPS15 | 首次使用 DCNN 生成目标掩模提议；推理时间较慢；需要分割注释进行训练；与检测网络不共享特征；使用 Fast RCNN 达到了 $69.9\%$
    (500) 的 mAP。    |'
- en: '|   | InstanceFCN Dai et al. ([2016a](#bib.bib48)) | VGG16 | $-$ | $-$ | $0.32$
    (100), $0.39({\color[rgb]{0,0,1}1000})$ | ECCV16 | Combines ideas of FCN Long
    et al. ([2015](#bib.bib177)) and DeepMask Pinheiro et al. ([2015](#bib.bib213)).
    Introduces instance sensitive score maps. Needs segmentation annotations to train
    the network.    |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|   | InstanceFCN Dai 等 ([2016a](#bib.bib48)) | VGG16 | $-$ | $-$ | $0.32$
    (100), $0.39({\color[rgb]{0,0,1}1000})$ | ECCV16 | 结合了 FCN Long 等 ([2015](#bib.bib177))
    和 DeepMask Pinheiro 等 ([2015](#bib.bib213)) 的思想。引入了实例敏感分数图。需要分割注释来训练网络。    |'
- en: '|   | SharpMask Pinheiro et al. ([2016](#bib.bib214)) | MPN Zagoruyko et al.
    ([2016](#bib.bib302)) | Fast RCNN | $0.39$ (100), $0.53({\color[rgb]{0,0,1}1000})$
    | $0.30$ (100), $0.39({\color[rgb]{0,0,1}1000})$ | ECCV16 | Leverages features
    at multiple convolutional layers by introducing a top-down refinement module.
    Does not share features with detection network. Needs segmentation annotations
    for training.    |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|   | SharpMask Pinheiro 等 ([2016](#bib.bib214)) | MPN Zagoruyko 等 ([2016](#bib.bib302))
    | Fast RCNN | $0.39$ (100), $0.53({\color[rgb]{0,0,1}1000})$ | $0.30$ (100), $0.39({\color[rgb]{0,0,1}1000})$
    | ECCV16 | 通过引入自上而下的细化模块，利用多个卷积层的特征。与检测网络不共享特征。需要分割注释进行训练。    |'
- en: '|   | FastMaskHu et al. ([2017](#bib.bib113)) | ResNet39 | $-$ | $0.43$ (100),
    $0.57({\color[rgb]{0,0,1}1000})$ | $0.32$ (100), $0.41({\color[rgb]{0,0,1}1000})$
    | CVPR17 | Generates instance segment proposals efficiently in one-shot manner
    similar to SSD Liu et al. ([2016](#bib.bib175)). Uses multiscale convolutional
    features. Uses segmentation annotations for training.    |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|   | FastMaskHu 等 ([2017](#bib.bib113)) | ResNet39 | $-$ | $0.43$ (100), $0.57({\color[rgb]{0,0,1}1000})$
    | $0.32$ (100), $0.41({\color[rgb]{0,0,1}1000})$ | CVPR17 | 以类似 SSD Liu 等 ([2016](#bib.bib175))
    的一体化方式高效生成实例分割提议。使用多尺度卷积特征。使用分割注释进行训练。    |'
- en: '|      |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|      |  |  |  |  |  |  |  |  |  |  |  |'
- en: 'Recent DCNN based object proposal methods generally fall into two categories:
    bounding box based and object segment based, with representative methods summarized
    in Table [9](#S8.T9 "Table 9 ‣ 8 Detection Proposal Methods ‣ Deep Learning for
    Generic Object Detection: A Survey").'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '最近基于 DCNN 的目标提议方法通常分为两类：基于边界框的和基于目标分割的，代表性方法总结在表 [9](#S8.T9 "Table 9 ‣ 8 Detection
    Proposal Methods ‣ Deep Learning for Generic Object Detection: A Survey") 中。'
- en: '![Refer to caption](img/66deb7ce62e3cfa619c650b5b2723bb9.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/66deb7ce62e3cfa619c650b5b2723bb9.png)'
- en: 'Figure 19: Illustration of the Region Proposal Network (RPN) introduced in
    Ren et al. ([2015](#bib.bib229)).'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：Ren 等 ([2015](#bib.bib229)) 介绍的区域提议网络 (RPN) 的示意图。
- en: 'Bounding Box Proposal Methods are best exemplified by the RPC method Ren et al.
    ([2015](#bib.bib229)) of Ren *et al.*, illustrated in Fig. [19](#S8.F19 "Figure
    19 ‣ 8 Detection Proposal Methods ‣ Deep Learning for Generic Object Detection:
    A Survey"). RPN predicts object proposals by sliding a small network over the
    feature map of the last shared CONV layer. At each sliding window location, $k$
    proposals are predicted by using $k$ anchor boxes, where each anchor box^(14)^(14)14The
    concept of “anchor” first appeared in Ren et al. ([2015](#bib.bib229)). is centered
    at some location in the image, and is associated with a particular scale and aspect
    ratio. Ren *et al.* Ren et al. ([2015](#bib.bib229)) proposed integrating RPN
    and Fast RCNN into a single network by sharing their convolutional layers, leading
    to Faster RCNN, the first end-to-end detection pipeline. RPN has been broadly
    selected as the proposal method by many state-of-the-art object detectors, as
    can be observed from Tables [7](#S6.T7 "Table 7 ‣ 6.1 Popular CNN Architectures
    ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")
    and [8](#S7.T8 "Table 8 ‣ 7 Context Modeling ‣ Deep Learning for Generic Object
    Detection: A Survey").'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '边界框提议方法的最佳示例是Ren *et al.* Ren等人（[2015](#bib.bib229)）的RPC方法，如图[19](#S8.F19 "Figure
    19 ‣ 8 Detection Proposal Methods ‣ Deep Learning for Generic Object Detection:
    A Survey")所示。RPN通过在最后共享的CONV层的特征图上滑动一个小网络来预测物体提议。在每个滑动窗口位置，使用$k$个锚点框预测$k$个提议，每个锚点框^(14)^(14)14“锚点”这一概念首次出现在Ren等人（[2015](#bib.bib229)）的研究中。都以图像中的某个位置为中心，并与特定的尺度和长宽比相关联。Ren
    *et al.* Ren等人（[2015](#bib.bib229)）提出将RPN和Fast RCNN集成到一个网络中，通过共享其卷积层，形成了Faster
    RCNN，这是第一个端到端检测管道。RPN已被许多最先进的目标检测器广泛选用，如表[7](#S6.T7 "Table 7 ‣ 6.1 Popular CNN
    Architectures ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey")和表[8](#S7.T8 "Table 8 ‣ 7 Context Modeling ‣ Deep Learning for Generic
    Object Detection: A Survey")所示。'
- en: Instead of fixing a priori a set of anchors as MultiBox Erhan et al. ([2014](#bib.bib67));
    Szegedy et al. ([2014](#bib.bib262)) and RPN Ren et al. ([2015](#bib.bib229)),
    Lu *et al.* Lu et al. ([2016](#bib.bib181)) proposed generating anchor locations
    by using a recursive search strategy which can adaptively guide computational
    resources to focus on sub-regions likely to contain objects. Starting with the
    whole image, all regions visited during the search process serve as anchors. For
    any anchor region encountered during the search procedure, a scalar zoom indicator
    is used to decide whether to further partition the region, and a set of bounding
    boxes with objectness scores are computed by an Adjacency and Zoom Network (AZNet),
    which extends RPN by adding a branch to compute the scalar zoom indicator in parallel
    with the existing branch.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于固定一组先验锚点的方法，如MultiBox Erhan等人（[2014](#bib.bib67)）；Szegedy等人（[2014](#bib.bib262)）和RPN
    Ren等人（[2015](#bib.bib229)），Lu *et al.* Lu等人（[2016](#bib.bib181)）提出了通过递归搜索策略生成锚点位置的方法，该策略能够自适应地引导计算资源聚焦于可能包含物体的子区域。从整个图像开始，在搜索过程中访问的所有区域都作为锚点。对于在搜索过程中遇到的任何锚点区域，使用一个标量缩放指示器来决定是否进一步划分该区域，并通过一个邻接与缩放网络（AZNet）计算一组带有物体性得分的边界框，该网络通过添加一个与现有分支并行计算标量缩放指示器的分支来扩展RPN。
- en: Further work attempts to generate object proposals by exploiting multilayer
    convolutional features. Concurrent with RPN Ren et al. ([2015](#bib.bib229)),
    Ghodrati *et al.* Ghodrati et al. ([2015](#bib.bib81)) proposed DeepProposal,
    which generates object proposals by using a cascade of multiple convolutional
    features, building an inverse cascade to select the most promising object locations
    and to refine their boxes in a coarse-to-fine manner. An improved variant of RPN,
    HyperNet Kong et al. ([2016](#bib.bib135)) designs Hyper Features which aggregate
    multilayer convolutional features and shares them both in generating proposals
    and detecting objects via an end-to-end joint training strategy. Yang *et al.*
    proposed CRAFT Yang et al. ([2016a](#bib.bib292)) which also used a cascade strategy,
    first training an RPN network to generate object proposals and then using them
    to train another binary Fast RCNN network to further distinguish objects from
    background. Li *et al.* Li et al. ([2018a](#bib.bib156)) proposed ZIP to improve
    RPN by predicting object proposals with multiple convolutional feature maps at
    different network depths to integrate both low level details and high level semantics.
    The backbone used in ZIP is a “zoom out and in” network inspired by the conv and
    deconv structure Long et al. ([2015](#bib.bib177)).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的工作尝试通过利用多层卷积特征来生成对象提议。与RPN Ren等人 ([2015](#bib.bib229)) 同时，Ghodrati *et al.*
    Ghodrati等人 ([2015](#bib.bib81)) 提出了DeepProposal，该方法通过使用多个卷积特征的级联来生成对象提议，建立一个逆级联以选择最有前途的对象位置，并以粗到细的方式精炼其边界框。RPN的改进变体HyperNet
    Kong等人 ([2016](#bib.bib135)) 设计了Hyper Features，这些特征汇聚了多层卷积特征，并通过端到端的联合训练策略在生成提议和检测对象时共享。Yang
    *et al.* 提出了CRAFT Yang等人 ([2016a](#bib.bib292))，该方法也使用了级联策略，首先训练一个RPN网络生成对象提议，然后使用这些提议训练另一个二分类Fast
    RCNN网络，以进一步区分对象和背景。Li *et al.* Li等人 ([2018a](#bib.bib156)) 提出了ZIP，以通过在不同网络深度上预测对象提议的多个卷积特征图来改进RPN，以整合低级细节和高级语义。ZIP中使用的骨干网是受conv和deconv结构启发的“放大和缩小”网络
    Long等人 ([2015](#bib.bib177))。
- en: Finally, recent work which deserves mention includes Deepbox Kuo et al. ([2015](#bib.bib142)),
    which proposed a lightweight CNN to learn to rerank proposals generated by EdgeBox,
    and DeNet TychsenSmith and Petersson ([2017](#bib.bib269)) which introduces bounding
    box corner estimation to predict object proposals efficiently to replace RPN in
    a Faster RCNN style detector.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得一提的近期工作包括Deepbox Kuo等人 ([2015](#bib.bib142))，该工作提出了一种轻量级CNN，用于学习重新排序由EdgeBox生成的提议；以及DeNet
    TychsenSmith和Petersson ([2017](#bib.bib269))，它引入了边界框角点估计，以高效预测对象提议，取代Faster RCNN风格检测器中的RPN。
- en: 'Object Segment Proposal Methods Pinheiro et al. ([2015](#bib.bib213), [2016](#bib.bib214))
    aim to generate segment proposals that are likely to correspond to objects. Segment
    proposals are more informative than bounding box proposals, and take a step further
    towards object instance segmentation Hariharan et al. ([2014](#bib.bib96)); Dai
    et al. ([2016b](#bib.bib49)); Li et al. ([2017e](#bib.bib162)). In addition, using
    instance segmentation supervision can improve the performance of bounding box
    object detection. The pioneering work of DeepMask, proposed by Pinheiro *et al.*
    Pinheiro et al. ([2015](#bib.bib213)), segments proposals learnt directly from
    raw image data with a deep network. Similarly to RPN, after a number of shared
    convolutional layers DeepMask splits the network into two branches in order to
    predict a class agnostic mask and an associated objectness score. Also similar
    to the efficient sliding window strategy in OverFeat Sermanet et al. ([2014](#bib.bib239)),
    the trained DeepMask network is applied in a sliding window manner to an image
    (and its rescaled versions) during inference. More recently, Pinheiro *et al.*
    Pinheiro et al. ([2016](#bib.bib214)) proposed SharpMask by augmenting the DeepMask
    architecture with a refinement module, similar to the architectures shown in Fig. [17](#S6.F17
    "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving
    Object Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object
    Detection: A Survey") (b1) and (b2), augmenting the feed-forward network with
    a top-down refinement process. SharpMask can efficiently integrate spatially rich
    information from early features with strong semantic information encoded in later
    layers to generate high fidelity object masks.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '对象分割建议方法 Pinheiro 等人（[2015](#bib.bib213), [2016](#bib.bib214)）旨在生成可能与对象对应的分割建议。分割建议比边界框建议更具信息量，并且在对象实例分割
    Hariharan等人（[2014](#bib.bib96)）；Dai等人（[2016b](#bib.bib49)）；Li等人（[2017e](#bib.bib162)）上更进一步。此外，使用实例分割监督可以提高边界框对象检测的性能。DeepMask
    的开创性工作由 Pinheiro *et al.* 提出的 Pinheiro 等人（[2015](#bib.bib213)），通过深度网络直接从原始图像数据中学习分割建议。与RPN类似，在若干共享卷积层之后，DeepMask将网络分为两个分支，以预测无类别掩码和相关物体得分。类似于
    OverFeat Sermanet 等人（[2014](#bib.bib239)）中的高效滑动窗口策略，训练后的 DeepMask 网络在推断期间以滑动窗口的方式应用于图像（及其重新缩放版本）。更近期，Pinheiro
    *et al.* 提出的 Pinheiro 等人（[2016](#bib.bib214)）通过增强 DeepMask 架构的细化模块提出了 SharpMask，类似于图 [17](#S6.F17
    "Figure 17 ‣ 6.2.1 Handling of Object Scale Variations ‣ 6.2 Methods For Improving
    Object Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object
    Detection: A Survey")（b1）和（b2）中所示的架构，通过自上而下的细化过程增强了前馈网络。SharpMask 能够高效地将来自早期特征的空间丰富信息与后期层中编码的强语义信息集成，以生成高保真度的对象掩码。'
- en: Motivated by Fully Convolutional Networks (FCN) for semantic segmentation Long
    et al. ([2015](#bib.bib177)) and DeepMask Pinheiro et al. ([2015](#bib.bib213)),
    Dai *et al.* proposed InstanceFCN Dai et al. ([2016a](#bib.bib48)) to generate
    instance segment proposals. Similar to DeepMask, the InstanceFCN network is split
    into two fully convolutional branches, one to generate instance sensitive score
    maps, the other to predict the objectness score. Hu *et al.* proposed FastMask
    Hu et al. ([2017](#bib.bib113)) to efficiently generate instance segment proposals
    in a one-shot manner, similar to SSD Liu et al. ([2016](#bib.bib175)), in order
    to make use of multiscale convolutional features. Sliding windows extracted densely
    from multiscale convolutional feature maps were input to a scale-tolerant attentional
    head module in order to predict segmentation masks and objectness scores. FastMask
    is claimed to run at 13 FPS on $800\times 600$ images.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 受到用于语义分割的全卷积网络（FCN）Long等人（[2015](#bib.bib177)）和DeepMask Pinheiro等人（[2015](#bib.bib213)）的启发，Dai
    *et al.* 提出了InstanceFCN Dai等人（[2016a](#bib.bib48)）以生成实例分割建议。类似于DeepMask，InstanceFCN网络被分为两个全卷积分支，一个用于生成实例敏感的得分图，另一个用于预测物体得分。Hu
    *et al.* 提出了FastMask Hu等人（[2017](#bib.bib113)），以一种类似于SSD Liu等人（[2016](#bib.bib175)）的一次性方式高效生成实例分割建议，利用多尺度卷积特征。多尺度卷积特征图中提取的滑动窗口被输入到一个尺度容忍的注意力头模块中，以预测分割掩码和物体得分。FastMask
    宣称在 $800\times 600$ 图像上以 13 FPS 运行。
- en: 'Table 10: Representative methods for training strategies and class imbalance
    handling. Results on COCO are reported with Test Dev. The detection results on
    COCO are based on mAP@IoU[0.5, 0.95].'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：训练策略和类别不平衡处理的代表性方法。COCO上的结果报告为Test Dev。COCO上的检测结果基于 mAP@IoU[0.5, 0.95]。
- en: '|      Detector Name | Region Proposal | Backbone DCNN | Pipelined Used | VOC07
    Results | VOC12 Results | COCO Results | Published In | Highlights    |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '|      检测器名称 | 区域提议 | 主干DCNN | 使用的流水线 | VOC07 结果 | VOC12 结果 | COCO 结果 | 发布年份
    | 亮点    |'
- en: '|      MegDet Peng et al. ([2018](#bib.bib209)) | RPN | ResNet50 +FPN | Faster
    RCNN | $-$ | $-$ | $52.5$ | CVPR18 | Allow training with much larger minibatch
    size than before by introducing cross GPU batch normalization; Can finish the
    COCO training in 4 hours on 128 GPUs and achieved improved accuracy; Won COCO2017
    detection challenge.    |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|      MegDet Peng 等人 ([2018](#bib.bib209)) | RPN | ResNet50 +FPN | Faster
    RCNN | $-$ | $-$ | $52.5$ | CVPR18 | 通过引入跨GPU批量归一化，允许使用比以前更大的小批量大小进行训练；可以在128
    GPU上在4小时内完成COCO训练，并取得了更好的准确性；赢得了COCO2017检测挑战赛。    |'
- en: '|    SNIP Singh et al. ([2018b](#bib.bib251)) | RPN | DPN Chen et al. ([2017b](#bib.bib37))
    +DCN Dai et al. ([2017](#bib.bib51)) | RFCN | $-$ | $-$ | $48.3$ | CVPR18 | A
    new multiscale training scheme. Empirically examined the effect of up-sampling
    for small object detection. During training, only select objects that fit the
    scale of features as positive samples.    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|    SNIP Singh 等人 ([2018b](#bib.bib251)) | RPN | DPN Chen 等人 ([2017b](#bib.bib37))
    +DCN Dai 等人 ([2017](#bib.bib51)) | RFCN | $-$ | $-$ | $48.3$ | CVPR18 | 一种新的多尺度训练方案。实证检查了小物体检测中的上采样效果。在训练过程中，仅选择适合特征尺度的物体作为正样本。
       |'
- en: '|    SNIPER Singh et al. ([2018b](#bib.bib251)) | RPN | ResNet101 +DCN | Faster
    RCNN | $-$ | $-$ | $47.6$ | 2018 | An efficient multiscale training strategy.
    Process context regions around ground-truth instances at the appropriate scale.
       |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '|    SNIPER Singh 等人 ([2018b](#bib.bib251)) | RPN | ResNet101 +DCN | Faster
    RCNN | $-$ | $-$ | $47.6$ | 2018 | 一种高效的多尺度训练策略。以适当的尺度处理地面实况的上下文区域。    |'
- en: '|    OHEM Shrivastava et al. ([2016](#bib.bib246)) | SS | VGG16 | Fast RCNN
    | $78.9$ (07+12) | $76.3$ (07++12) | $22.4$ | CVPR16 | A simple and effective
    Online Hard Example Mining algorithm to improve training of region based detectors.
       |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|    OHEM Shrivastava 等人 ([2016](#bib.bib246)) | SS | VGG16 | Fast RCNN | $78.9$
    (07+12) | $76.3$ (07++12) | $22.4$ | CVPR16 | 一种简单有效的在线困难样本挖掘算法，以提高基于区域的检测器的训练效果。
       |'
- en: '|    FactorNet Ouyang et al. ([2016](#bib.bib204)) | SS | GooglNet | RCNN |
    $-$ | $-$ | $-$ | CVPR16 | Identify the imbalance in the number of samples for
    different object categories; propose a divide-and-conquer feature learning scheme.
       |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|    FactorNet Ouyang 等人 ([2016](#bib.bib204)) | SS | GooglNet | RCNN | $-$
    | $-$ | $-$ | CVPR16 | 识别不同物体类别样本数量的不平衡；提出了一种分治特征学习方案。    |'
- en: '|    Chained Cascade Cai and Vasconcelos ([2018](#bib.bib23)) | SS CRAFT |
    VGG Inceptionv2 | Fast RCNN, Faster RCNN | $80.4$ (07+12) (SS+VGG) | $-$ | $-$
    | ICCV17 | Jointly learn DCNN and multiple stages of cascaded classifiers. Boost
    detection accuracy on PASCAL VOC 2007 and ImageNet for both fast RCNN and Faster
    RCNN using different region proposal methods.    |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '|    Chained Cascade Cai 和 Vasconcelos ([2018](#bib.bib23)) | SS CRAFT | VGG
    Inceptionv2 | Fast RCNN, Faster RCNN | $80.4$ (07+12) (SS+VGG) | $-$ | $-$ | ICCV17
    | 联合学习DCNN和多个级联分类器阶段。使用不同的区域提议方法提高了PASCAL VOC 2007和ImageNet上Fast RCNN和Faster RCNN的检测准确性。
       |'
- en: '|    Cascade RCNN Cai and Vasconcelos ([2018](#bib.bib23)) | RPN | VGG ResNet101
    +FPN | Faster RCNN | $-$ | $-$ | $42.8$ | CVPR18 | Jointly learn DCNN and multiple
    stages of cascaded classifiers, which are learned using different localization
    accuracy for selecting positive samples. Stack bounding box regression at multiple
    stages.    |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|    Cascade RCNN Cai 和 Vasconcelos ([2018](#bib.bib23)) | RPN | VGG ResNet101
    +FPN | Faster RCNN | $-$ | $-$ | $42.8$ | CVPR18 | 联合学习DCNN和多个级联分类器阶段，使用不同的定位精度来选择正样本。在多个阶段堆叠边界框回归。
       |'
- en: '|    RetinaNet Lin et al. ([2017b](#bib.bib168)) | $-$ | ResNet101 +FPN | RetinaNet
    | $-$ | $-$ | $39.1$ | ICCV17 | Propose a novel Focal Loss which focuses training
    on hard examples. Handles well the problem of imbalance of positive and negative
    samples when training a one-stage detector.    |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|    RetinaNet Lin 等人 ([2017b](#bib.bib168)) | $-$ | ResNet101 +FPN | RetinaNet
    | $-$ | $-$ | $39.1$ | ICCV17 | 提出了新颖的Focal Loss，将训练集中于困难样本。有效处理了训练单阶段检测器时正负样本不平衡的问题。
       |'
- en: '|      |  |  |  |  |  |  |  |  |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|      |  |  |  |  |  |  |  |  |'
- en: 9 Other Issues
  id: totrans-452
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 其他问题
- en: Data Augmentation. Performing data augmentation for learning DCNNs Chatfield
    et al. ([2014](#bib.bib26)); Girshick ([2015](#bib.bib84)); Girshick et al. ([2014](#bib.bib85))
    is generally recognized to be important for visual recognition. Trivial data augmentation
    refers to perturbing an image by transformations that leave the underlying category
    unchanged, such as cropping, flipping, rotating, scaling, translating, color perturbations,
    and adding noise. By artificially enlarging the number of samples, data augmentation
    helps in reducing overfitting and improving generalization. It can be used at
    training time, at test time, or both. Nevertheless, it has the obvious limitation
    that the time required for training increases significantly. Data augmentation
    may synthesize completely new training images Peng et al. ([2015](#bib.bib210));
    Wang et al. ([2017](#bib.bib280)), however it is hard to guarantee that the synthetic
    images generalize well to real ones. Some researchers Dwibedi et al. ([2017](#bib.bib64));
    Gupta et al. ([2016](#bib.bib94)) proposed augmenting datasets by pasting real
    segmented objects into natural images; indeed, Dvornik *et al.* Dvornik et al.
    ([2018](#bib.bib63)) showed that appropriately modeling the visual context surrounding
    objects is crucial to place them in the right environment, and proposed a context
    model to automatically find appropriate locations on images to place new objects
    for data augmentation.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强。进行数据增强以学习 DCNNs Chatfield 等人 ([2014](#bib.bib26))；Girshick ([2015](#bib.bib84))；Girshick
    等人 ([2014](#bib.bib85)) 通常被认为对视觉识别至关重要。简单的数据增强指的是通过一些变换来扰动图像，同时保持其基础类别不变，例如裁剪、翻转、旋转、缩放、平移、颜色扰动和添加噪声。通过人工增加样本数量，数据增强有助于减少过拟合并提高泛化能力。它可以在训练时、测试时或两者都用。然而，它有一个明显的限制，即训练所需的时间显著增加。数据增强可能会合成完全新的训练图像
    Peng 等人 ([2015](#bib.bib210))；Wang 等人 ([2017](#bib.bib280))，但很难保证这些合成图像能够很好地泛化到真实图像上。一些研究人员
    Dwibedi 等人 ([2017](#bib.bib64))；Gupta 等人 ([2016](#bib.bib94)) 提出了通过将真实的分割对象粘贴到自然图像中来增强数据集；实际上，Dvornik
    *et al.* Dvornik 等人 ([2018](#bib.bib63)) 显示，适当地建模对象周围的视觉环境对于将其放置在正确的环境中至关重要，并提出了一种上下文模型来自动找到图像上的合适位置，以便放置新对象进行数据增强。
- en: 'Novel Training Strategies. Detecting objects under a wide range of scale variations,
    especially the detection of very small objects, stands out as a key challenge.
    It has been shown Huang et al. ([2017b](#bib.bib120)); Liu et al. ([2016](#bib.bib175))
    that image resolution has a considerable impact on detection accuracy, therefore
    scaling is particularly commonly used in data augmentation, since higher resolutions
    increase the possibility of detecting small objects Huang et al. ([2017b](#bib.bib120)).
    Recently, Singh *et al.* proposed advanced and efficient data argumentation methods
    SNIP Singh and Davis ([2018](#bib.bib249)) and SNIPER Singh et al. ([2018b](#bib.bib251))
    to illustrate the scale invariance problem, as summarized in Table [10](#S8.T10
    "Table 10 ‣ 8 Detection Proposal Methods ‣ Deep Learning for Generic Object Detection:
    A Survey"). Motivated by the intuitive understanding that small and large objects
    are difficult to detect at smaller and larger scales, respectively, SNIP introduces
    a novel training scheme that can reduce scale variations during training, but
    without reducing training samples; SNIPER allows for efficient multiscale training,
    only processing context regions around ground truth objects at the appropriate
    scale, instead of processing a whole image pyramid. Peng *et al.* Peng et al.
    ([2018](#bib.bib209)) studied a key factor in training, the minibatch size, and
    proposed MegDet, a Large MiniBatch Object Detector, to enable the training with
    a much larger minibatch size than before (from 16 to 256). To avoid the failure
    of convergence and significantly speed up the training process, Peng *et al.*
    Peng et al. ([2018](#bib.bib209)) proposed a learning rate policy and Cross GPU
    Batch Normalization, and effectively utilized 128 GPUs, allowing MegDet to finish
    COCO training in 4 hours on 128 GPUs, and winning the COCO 2017 Detection Challenge.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '新颖的训练策略。检测对象在各种尺度变化下，尤其是非常小的对象检测，是一个关键挑战。已有研究表明**黄等**（[2017b](#bib.bib120)）；**刘等**（[2016](#bib.bib175)）发现，图像分辨率对检测精度有显著影响，因此，尺度变换在数据增强中被广泛使用，因为更高的分辨率增加了检测小物体的可能性**黄等**（[2017b](#bib.bib120)）。最近，**辛格*等**提出了先进高效的数据增强方法SNIP
    **辛格**和**戴维斯**（[2018](#bib.bib249)）和SNIPER **辛格*等**（[2018b](#bib.bib251)），以阐明尺度不变性问题，如表[10](#S8.T10
    "Table 10 ‣ 8 Detection Proposal Methods ‣ Deep Learning for Generic Object Detection:
    A Survey")所总结。基于直观理解，即在较小和较大尺度下，小物体和大物体分别很难被检测到，SNIP引入了一种新颖的训练方案，可以减少训练过程中尺度变化，但不减少训练样本；SNIPER允许高效的多尺度训练，仅处理围绕真实物体的上下文区域，而不是处理整个图像金字塔。**彭*等**
    **彭等**（[2018](#bib.bib209)）研究了训练中的一个关键因素——小批量大小，并提出了MegDet，一个大规模小批量目标检测器，能够进行比以前更大的小批量训练（从16增加到256）。为避免收敛失败并显著加快训练过程，**彭*等**
    **彭等**（[2018](#bib.bib209)）提出了学习率策略和跨GPU批量归一化，并有效利用了128个GPU，使MegDet能够在128个GPU上完成COCO训练，赢得了COCO
    2017检测挑战。'
- en: '![Refer to caption](img/8e964700112ddc1016f934be22044741.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8e964700112ddc1016f934be22044741.png)'
- en: 'Figure 20: Localization error could stem from insufficient overlap or duplicate
    detections. Localization error is a frequent cause of false positives.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：定位误差可能源于重叠不足或重复检测。定位误差是虚假正例的一个常见原因。
- en: 'Reducing Localization Error. In object detection, the Intersection Over Union^(15)^(15)15Please
    refer to Section [4.2](#S4.SS2 "4.2 Evaluation Criteria ‣ 4 Datasets and Performance
    Evaluation ‣ Deep Learning for Generic Object Detection: A Survey") for more details
    on the definition of IOU. (IOU) between a detected bounding box and its ground
    truth box is the most popular evaluation metric, and an IOU threshold (*e.g.*
    typical value of $0.5$) is required to define positives and negatives. From Fig. [13](#S5.F13
    "Figure 13 ‣ 5.1 Region Based (Two Stage) Frameworks ‣ 5 Detection Frameworks
    ‣ Deep Learning for Generic Object Detection: A Survey"), in most state of the
    art detectors Girshick ([2015](#bib.bib84)); Liu et al. ([2016](#bib.bib175));
    He et al. ([2017](#bib.bib102)); Ren et al. ([2015](#bib.bib229)); Redmon et al.
    ([2016](#bib.bib227)) object detection is formulated as a multitask learning problem,
    *i.e.,* jointly optimizing a softmax classifier which assigns object proposals
    with class labels and bounding box regressors, localizing objects by maximizing
    IOU or other metrics between detection results and ground truth. Bounding boxes
    are only a crude approximation for articulated objects, consequently background
    pixels are almost invariably included in a bounding box, which affects the accuracy
    of classification and localization. The study in Hoiem et al. ([2012](#bib.bib108))
    shows that object localization error is one of the most influential forms of error,
    in addition to confusion between similar objects. Localization error could stem
    from insufficient overlap (smaller than the required IOU threshold, such as the
    green box in Fig. [20](#S9.F20 "Figure 20 ‣ 9 Other Issues ‣ Deep Learning for
    Generic Object Detection: A Survey")) or duplicate detections (*i.e.,* multiple
    overlapping detections for an object instance). Usually, some post-processing
    step like NonMaximum Suppression (NMS) Bodla et al. ([2017](#bib.bib18)); Hosang
    et al. ([2017](#bib.bib111)) is used for eliminating duplicate detections. However,
    due to misalignments the bounding box with better localization could be suppressed
    during NMS, leading to poorer localization quality (such as the purple box shown
    in Fig. [20](#S9.F20 "Figure 20 ‣ 9 Other Issues ‣ Deep Learning for Generic Object
    Detection: A Survey")). Therefore, there are quite a few methods aiming at improving
    detection performance by reducing localization error.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 降低定位误差。在目标检测中，检测到的边界框与其真实边界框之间的交并比^(15)^(15)15有关 IOU 的定义，请参考第 [4.2](#S4.SS2
    "4.2 评估标准 ‣ 4 数据集和性能评估 ‣ 深度学习在通用目标检测中的应用：综述") 节。 (IOU) 是最受欢迎的评估指标，需要一个 IOU 阈值
    (*例如*，典型值为 $0.5$) 来定义正样本和负样本。从图 [13](#S5.F13 "图 13 ‣ 5.1 区域基础 (双阶段) 框架 ‣ 5 检测框架
    ‣ 深度学习在通用目标检测中的应用：综述") 可以看出，在大多数先进的检测器中，Girshick ([2015](#bib.bib84)); Liu 等 ([2016](#bib.bib175));
    He 等 ([2017](#bib.bib102)); Ren 等 ([2015](#bib.bib229)); Redmon 等 ([2016](#bib.bib227))
    目标检测被公式化为一个多任务学习问题，*即*，联合优化一个软最大分类器，该分类器将目标提议分配类标签以及边界框回归器，通过最大化 IOU 或其他指标来定位目标，检测结果与真实情况之间的差异。边界框仅是对关节目标的粗略近似，因此背景像素几乎总是包含在边界框中，这会影响分类和定位的准确性。Hoiem
    等 ([2012](#bib.bib108)) 的研究表明，除了相似物体之间的混淆外，目标定位误差是最具影响力的错误形式之一。定位误差可能来源于重叠不足（小于所需的
    IOU 阈值，如图 [20](#S9.F20 "图 20 ‣ 9 其他问题 ‣ 深度学习在通用目标检测中的应用：综述") 中的绿色框）或重复检测 (*即*，一个目标实例的多个重叠检测)。通常，像非极大值抑制（NMS）Bodla
    等 ([2017](#bib.bib18)); Hosang 等 ([2017](#bib.bib111)) 这样的后处理步骤用于消除重复检测。然而，由于对齐不准确，定位较好的边界框可能在
    NMS 过程中被抑制，导致较差的定位质量（如图 [20](#S9.F20 "图 20 ‣ 9 其他问题 ‣ 深度学习在通用目标检测中的应用：综述") 中的紫色框）。因此，有相当多的方法旨在通过减少定位误差来提高检测性能。
- en: MRCNN Gidaris and Komodakis ([2015](#bib.bib82)) introduces iterative bounding
    box regression, where an RCNN is applied several times. CRAFT Yang et al. ([2016a](#bib.bib292))
    and AttractioNet Gidaris and Komodakis ([2016](#bib.bib83)) use a multi-stage
    detection sub-network to generate accurate proposals, to forward to Fast RCNN.
    Cai and Vasconcelos proposed Cascade RCNN Cai and Vasconcelos ([2018](#bib.bib23)),
    a multistage extension of RCNN, in which a sequence of detectors is trained sequentially
    with increasing IOU thresholds, based on the observation that the output of a
    detector trained with a certain IOU is a good distribution to train the detector
    of the next higher IOU threshold, in order to be sequentially more selective against
    close false positives. This approach can be built with any RCNN-based detector,
    and is demonstrated to achieve consistent gains (about 2 to 4 points) independent
    of the baseline detector strength, at a marginal increase in computation. There
    is also recent work Jiang et al. ([2018](#bib.bib128)); Rezatofighi et al. ([2019](#bib.bib232));
    Huang et al. ([2019](#bib.bib121)) formulating IOU directly as the optimization
    objective, and in proposing improved NMS results Bodla et al. ([2017](#bib.bib18));
    He et al. ([2019](#bib.bib104)); Hosang et al. ([2017](#bib.bib111)); TychsenSmith
    and Petersson ([2018](#bib.bib270)), such as Soft NMS Bodla et al. ([2017](#bib.bib18))
    and learning NMS Hosang et al. ([2017](#bib.bib111)).
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: MRCNN Gidaris 和 Komodakis ([2015](#bib.bib82)) 引入了迭代边界框回归，其中一个 RCNN 被应用多次。CRAFT
    Yang 等人 ([2016a](#bib.bib292)) 和 AttractioNet Gidaris 和 Komodakis ([2016](#bib.bib83))
    使用多阶段检测子网络来生成准确的提议，并将其传递给 Fast RCNN。Cai 和 Vasconcelos 提出了 Cascade RCNN Cai 和 Vasconcelos
    ([2018](#bib.bib23))，这是 RCNN 的多阶段扩展，其中一系列检测器按序列训练，IOU 阈值逐渐增加，基于观察到的，使用某个 IOU 训练的检测器的输出是训练下一个更高
    IOU 阈值的检测器的良好分布，以便在对接近的假阳性进行更具选择性的处理。该方法可以与任何基于 RCNN 的检测器一起构建，并被证明能够在计算量仅略有增加的情况下，独立于基线检测器的强度，实现一致的提升（大约
    2 到 4 分）。还有最近的工作 Jiang 等人 ([2018](#bib.bib128))；Rezatofighi 等人 ([2019](#bib.bib232))；Huang
    等人 ([2019](#bib.bib121)) 直接将 IOU 作为优化目标进行公式化，并提出改进的 NMS 结果 Bodla 等人 ([2017](#bib.bib18))；He
    等人 ([2019](#bib.bib104))；Hosang 等人 ([2017](#bib.bib111))；TychsenSmith 和 Petersson
    ([2018](#bib.bib270))，例如 Soft NMS Bodla 等人 ([2017](#bib.bib18)) 和学习 NMS Hosang
    等人 ([2017](#bib.bib111))。
- en: 'Class Imbalance Handling. Unlike image classification, object detection has
    another unique problem: the serious imbalance between the number of labeled object
    instances and the number of background examples (image regions not belonging to
    any object class of interest). Most background examples are easy negatives, however
    this imbalance can make the training very inefficient, and the large number of
    easy negatives tends to overwhelm the training. In the past, this issue has typically
    been addressed via techniques such as bootstrapping Sung et al. ([1994](#bib.bib259)).
    More recently, this problem has also seen some attention Li et al. ([2019a](#bib.bib153));
    Lin et al. ([2017b](#bib.bib168)); Shrivastava et al. ([2016](#bib.bib246)). Because
    the region proposal stage rapidly filters out most background regions and proposes
    a small number of object candidates, this class imbalance issue is mitigated to
    some extent in two-stage detectors Girshick et al. ([2014](#bib.bib85)); Girshick
    ([2015](#bib.bib84)); Ren et al. ([2015](#bib.bib229)); He et al. ([2017](#bib.bib102)),
    although example mining approaches, such as Online Hard Example Mining (OHEM)
    Shrivastava et al. ([2016](#bib.bib246)), may be used to maintain a reasonable
    balance between foreground and background. In the case of one-stage object detectors
    Redmon et al. ([2016](#bib.bib227)); Liu et al. ([2016](#bib.bib175)), this imbalance
    is extremely serious (*e.g.* 100,000 background examples to every object). Lin
    *et al.* Lin et al. ([2017b](#bib.bib168)) proposed Focal Loss to address this
    by rectifying the Cross Entropy loss, such that it down-weights the loss assigned
    to correctly classified examples. Li *et al.* Li et al. ([2019a](#bib.bib153))
    studied this issue from the perspective of gradient norm distribution, and proposed
    a Gradient Harmonizing Mechanism (GHM) to handle it.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡处理。与图像分类不同，目标检测面临另一个独特的问题：标记对象实例数量与背景示例数量（不属于任何感兴趣的对象类别的图像区域）之间的严重不平衡。大多数背景示例是容易的负样本，但这种不平衡可能使训练变得非常低效，并且大量的容易负样本往往会淹没训练。过去，通常通过引导法等技术来解决这一问题，如Sung等人（[1994](#bib.bib259)）。最近，这一问题也受到了一些关注，如Li等人（[2019a](#bib.bib153)）；Lin等人（[2017b](#bib.bib168)）；Shrivastava等人（[2016](#bib.bib246)）。由于区域提议阶段会快速过滤掉大多数背景区域，并提出少量对象候选框，这种类别不平衡问题在两阶段检测器中得到了某种程度的缓解，如Girshick等人（[2014](#bib.bib85)）；Girshick（[2015](#bib.bib84)）；Ren等人（[2015](#bib.bib229)）；He等人（[2017](#bib.bib102)），尽管例子挖掘方法，如在线困难例子挖掘（OHEM）Shrivastava等人（[2016](#bib.bib246)），可能会被用来保持前景与背景之间的合理平衡。在单阶段目标检测器的情况下，如Redmon等人（[2016](#bib.bib227)）；Liu等人（[2016](#bib.bib175)），这种不平衡极为严重（*例如*
    每个对象对应100,000个背景示例）。Lin *等人* Lin等人（[2017b](#bib.bib168)）提出了Focal Loss，通过修正交叉熵损失来解决这个问题，从而降低分配给正确分类示例的损失。Li
    *等人* Li等人（[2019a](#bib.bib153)）从梯度范数分布的角度研究了这一问题，并提出了梯度协调机制（GHM）来处理它。
- en: 10 Discussion and Conclusion
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 讨论与结论
- en: 'Generic object detection is an important and challenging problem in computer
    vision and has received considerable attention. Thanks to remarkable developments
    in deep learning techniques, the field of object detection has dramatically evolved.
    As a comprehensive survey on deep learning for generic object detection, this
    paper has highlighted the recent achievements, provided a structural taxonomy
    for methods according to their roles in detection, summarized existing popular
    datasets and evaluation criteria, and discussed performance for the most representative
    methods. We conclude this review with a discussion of the state of the art in
    Section [10.1](#S10.SS1 "10.1 State of the Art Performance ‣ 10 Discussion and
    Conclusion ‣ Deep Learning for Generic Object Detection: A Survey"), an overall
    discussion of key issues in Section [10.2](#S10.SS2 "10.2 Summary and Discussion
    ‣ 10 Discussion and Conclusion ‣ Deep Learning for Generic Object Detection: A
    Survey"), and finally suggested future research directions in Section [10.3](#S10.SS3
    "10.3 Research Directions ‣ 10 Discussion and Conclusion ‣ Deep Learning for Generic
    Object Detection: A Survey").'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '通用物体检测是计算机视觉中的一个重要而具有挑战性的问题，受到了相当多的关注。得益于深度学习技术的显著进展，物体检测领域已发生了重大变化。作为关于通用物体检测深度学习的全面综述，本文突出了最近的成就，根据方法在检测中的角色提供了结构化分类法，总结了现有的流行数据集和评估标准，并讨论了最具代表性方法的性能。我们在第[10.1](#S10.SS1
    "10.1 State of the Art Performance ‣ 10 Discussion and Conclusion ‣ Deep Learning
    for Generic Object Detection: A Survey)节中总结了最先进的性能，在第[10.2](#S10.SS2 "10.2 Summary
    and Discussion ‣ 10 Discussion and Conclusion ‣ Deep Learning for Generic Object
    Detection: A Survey)节中讨论了关键问题的整体情况，并在第[10.3](#S10.SS3 "10.3 Research Directions
    ‣ 10 Discussion and Conclusion ‣ Deep Learning for Generic Object Detection: A
    Survey)节中建议了未来的研究方向。'
- en: 10.1 State of the Art Performance
  id: totrans-462
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1 最先进的性能
- en: 'A large variety of detectors has appeared in the last few years, and the introduction
    of standard benchmarks, such as PASCAL VOC Everingham et al. ([2010](#bib.bib68),
    [2015](#bib.bib69)), ImageNet Russakovsky et al. ([2015](#bib.bib234)) and COCO
    Lin et al. ([2014](#bib.bib166)), has made it easier to compare detectors. As
    can be seen from our earlier discussion in Sections [5](#S5 "5 Detection Frameworks
    ‣ Deep Learning for Generic Object Detection: A Survey") through [9](#S9 "9 Other
    Issues ‣ Deep Learning for Generic Object Detection: A Survey"), it may be misleading
    to compare detectors in terms of their originally reported performance (*e.g.*
    accuracy, speed), as they can differ in fundamental / contextual respects, including
    the following choices:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '过去几年中出现了各种各样的检测器，标准基准的引入，例如 PASCAL VOC Everingham 等人 ([2010](#bib.bib68)、[2015](#bib.bib69))、ImageNet
    Russakovsky 等人 ([2015](#bib.bib234)) 和 COCO Lin 等人 ([2014](#bib.bib166))，使得检测器之间的比较变得更容易。从我们在第[5](#S5
    "5 Detection Frameworks ‣ Deep Learning for Generic Object Detection: A Survey)节到第[9](#S9
    "9 Other Issues ‣ Deep Learning for Generic Object Detection: A Survey)节的早期讨论可以看出，在比较检测器时，依据它们最初报告的性能（*例如*
    准确性、速度）可能具有误导性，因为它们可能在基本/上下文方面存在差异，包括以下选择：'
- en: $\bullet$
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Meta detection frameworks, such as RCNN Girshick et al. ([2014](#bib.bib85)),
    Fast RCNN Girshick ([2015](#bib.bib84)), Faster RCNN Ren et al. ([2015](#bib.bib229)),
    RFCN Dai et al. ([2016c](#bib.bib50)), Mask RCNN He et al. ([2017](#bib.bib102)),
    YOLO Redmon et al. ([2016](#bib.bib227)) and SSD Liu et al. ([2016](#bib.bib175));
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 元检测框架，例如 RCNN Girshick 等人 ([2014](#bib.bib85))、Fast RCNN Girshick ([2015](#bib.bib84))、Faster
    RCNN Ren 等人 ([2015](#bib.bib229))、RFCN Dai 等人 ([2016c](#bib.bib50))、Mask RCNN
    He 等人 ([2017](#bib.bib102))、YOLO Redmon 等人 ([2016](#bib.bib227)) 和 SSD Liu 等人
    ([2016](#bib.bib175))；
- en: $\bullet$
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'Backbone networks such as VGG Simonyan and Zisserman ([2015](#bib.bib248)),
    Inception Szegedy et al. ([2015](#bib.bib263)); Ioffe and Szegedy ([2015](#bib.bib125));
    Szegedy et al. ([2016](#bib.bib264)), ResNet He et al. ([2016](#bib.bib101)),
    ResNeXt Xie et al. ([2017](#bib.bib291)), and Xception Chollet ([2017](#bib.bib45))
    *etc.* listed in Table [6](#S6.T6 "Table 6 ‣ 6 Object Representation ‣ Deep Learning
    for Generic Object Detection: A Survey");'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '主干网络，例如 VGG Simonyan 和 Zisserman ([2015](#bib.bib248))、Inception Szegedy 等人
    ([2015](#bib.bib263))；Ioffe 和 Szegedy ([2015](#bib.bib125))；Szegedy 等人 ([2016](#bib.bib264))，ResNet
    He 等人 ([2016](#bib.bib101))，ResNeXt Xie 等人 ([2017](#bib.bib291)) 和 Xception Chollet
    ([2017](#bib.bib45)) *等* 列在表 [6](#S6.T6 "Table 6 ‣ 6 Object Representation ‣ Deep
    Learning for Generic Object Detection: A Survey)；'
- en: $\bullet$
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Innovations such as multilayer feature combination Lin et al. ([2017a](#bib.bib167));
    Shrivastava et al. ([2017](#bib.bib247)); Fu et al. ([2017](#bib.bib77)), deformable
    convolutional networks Dai et al. ([2017](#bib.bib51)), deformable RoI pooling
    Ouyang et al. ([2015](#bib.bib203)); Dai et al. ([2017](#bib.bib51)), heavier
    heads Ren et al. ([2017b](#bib.bib231)); Peng et al. ([2018](#bib.bib209)), and
    lighter heads Li et al. ([2018c](#bib.bib165));
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创新如多层特征组合 Lin 等 ([2017a](#bib.bib167))；Shrivastava 等 ([2017](#bib.bib247))；Fu
    等 ([2017](#bib.bib77))，可变形卷积网络 Dai 等 ([2017](#bib.bib51))，可变形 RoI 池化 Ouyang 等
    ([2015](#bib.bib203))；Dai 等 ([2017](#bib.bib51))，更重的头 Ren 等 ([2017b](#bib.bib231))；Peng
    等 ([2018](#bib.bib209))，以及更轻的头 Li 等 ([2018c](#bib.bib165))；
- en: $\bullet$
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Pretraining with datasets such as ImageNet Russakovsky et al. ([2015](#bib.bib234)),
    COCO Lin et al. ([2014](#bib.bib166)), Places Zhou et al. ([2017a](#bib.bib319)),
    JFT Hinton et al. ([2015](#bib.bib106)) and Open Images Krasin et al. ([2017](#bib.bib139));
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用数据集如 ImageNet Russakovsky 等 ([2015](#bib.bib234))，COCO Lin 等 ([2014](#bib.bib166))，Places
    Zhou 等 ([2017a](#bib.bib319))，JFT Hinton 等 ([2015](#bib.bib106)) 和 Open Images
    Krasin 等 ([2017](#bib.bib139)) 进行预训练；
- en: $\bullet$
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Different detection proposal methods and different numbers of object proposals;
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不同的检测提议方法和不同数量的目标提议；
- en: $\bullet$
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Train/test data augmentation, novel multiscale training strategies Singh and
    Davis ([2018](#bib.bib249)); Singh et al. ([2018b](#bib.bib251)) *etc*, and model
    ensembling.
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练/测试数据增强、新颖的多尺度训练策略 Singh 和 Davis ([2018](#bib.bib249))；Singh 等 ([2018b](#bib.bib251))
    *等*，以及模型集成。
- en: Although it may be impractical to compare every recently proposed detector,
    it is nevertheless valuable to integrate representative and publicly available
    detectors into a common platform and to compare them in a unified manner. There
    has been very limited work in this regard, except for Huang’s study Huang et al.
    ([2017b](#bib.bib120)) of the three main families of detectors (Faster RCNN Ren
    et al. ([2015](#bib.bib229)), RFCN Dai et al. ([2016c](#bib.bib50)) and SSD Liu
    et al. ([2016](#bib.bib175))) by varying the backbone network, image resolution,
    and the number of box proposals.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可能不切实际对每一个最近提出的检测器进行比较，但将具有代表性且公开可用的检测器整合到一个共同的平台中，并以统一的方式进行比较仍然是有价值的。除 Huang
    的研究 Huang 等 ([2017b](#bib.bib120)) 外，在这方面的工作非常有限，该研究通过变化骨干网络、图像分辨率和框提议数量，对三大类检测器（Faster
    RCNN Ren 等 ([2015](#bib.bib229))，RFCN Dai 等 ([2016c](#bib.bib50)) 和 SSD Liu 等
    ([2016](#bib.bib175))）进行了比较。
- en: '![Refer to caption](img/0c28ea060d424f601682c08aaf6f8b66.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0c28ea060d424f601682c08aaf6f8b66.png)'
- en: 'Figure 21: Evolution of object detection performance on COCO (Test-Dev results).
    Results are quoted from Girshick ([2015](#bib.bib84)); He et al. ([2017](#bib.bib102));
    Ren et al. ([2017a](#bib.bib230)). The backbone network, the design of detection
    framework and the availability of good and large scale datasets are the three
    most important factors in detection accuracy.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：COCO 上目标检测性能的演变（Test-Dev 结果）。结果引用自 Girshick ([2015](#bib.bib84))；He 等 ([2017](#bib.bib102))；Ren
    等 ([2017a](#bib.bib230))。骨干网络、检测框架的设计以及良好且大规模数据集的可用性是检测准确性的三个最重要因素。
- en: 'As can be seen from Tables [7](#S6.T7 "Table 7 ‣ 6.1 Popular CNN Architectures
    ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey"),
    [8](#S7.T8 "Table 8 ‣ 7 Context Modeling ‣ Deep Learning for Generic Object Detection:
    A Survey"), [9](#S8.T9 "Table 9 ‣ 8 Detection Proposal Methods ‣ Deep Learning
    for Generic Object Detection: A Survey"), [10](#S8.T10 "Table 10 ‣ 8 Detection
    Proposal Methods ‣ Deep Learning for Generic Object Detection: A Survey"), [11](#S10.T11
    "Table 11 ‣ 10.3 Research Directions ‣ 10 Discussion and Conclusion ‣ Deep Learning
    for Generic Object Detection: A Survey"), we have summarized the best reported
    performance of many methods on three widely used standard benchmarks. The results
    of these methods were reported on the same test benchmark, despite their differing
    in one or more of the aspects listed above.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '从表 [7](#S6.T7 "Table 7 ‣ 6.1 Popular CNN Architectures ‣ 6 Object Representation
    ‣ Deep Learning for Generic Object Detection: A Survey")，[8](#S7.T8 "Table 8 ‣
    7 Context Modeling ‣ Deep Learning for Generic Object Detection: A Survey")，[9](#S8.T9
    "Table 9 ‣ 8 Detection Proposal Methods ‣ Deep Learning for Generic Object Detection:
    A Survey")，[10](#S8.T10 "Table 10 ‣ 8 Detection Proposal Methods ‣ Deep Learning
    for Generic Object Detection: A Survey")，[11](#S10.T11 "Table 11 ‣ 10.3 Research
    Directions ‣ 10 Discussion and Conclusion ‣ Deep Learning for Generic Object Detection:
    A Survey") 可以看出，我们总结了许多方法在三个广泛使用的标准基准上的最佳报告性能。这些方法的结果是在相同的测试基准上报告的，尽管它们在上述一个或多个方面存在差异。'
- en: 'Figs. [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep Learning for Generic Object
    Detection: A Survey") and [21](#S10.F21 "Figure 21 ‣ 10.1 State of the Art Performance
    ‣ 10 Discussion and Conclusion ‣ Deep Learning for Generic Object Detection: A
    Survey") present a very brief overview of the state of the art, summarizing the
    best detection results of the PASCAL VOC, ILSVRC and MSCOCO challenges; more results
    can be found at detection challenge websites ILSVRC detection challenge results
    ([2018](#bib.bib124)); MS COCO detection leaderboard ([2018](#bib.bib189)); PASCAL
    VOC detection leaderboard ([2018](#bib.bib208)). The competition winner of the
    open image challenge object detection task achieved $61.71\%$ mAP in the public
    leader board and $58.66\%$ mAP on the private leader board, obtained by combining
    the detection results of several two-stage detectors including Fast RCNN Girshick
    ([2015](#bib.bib84)), Faster RCNN Ren et al. ([2015](#bib.bib229)), FPN Lin et al.
    ([2017a](#bib.bib167)), Deformable RCNN Dai et al. ([2017](#bib.bib51)), and Cascade
    RCNN Cai and Vasconcelos ([2018](#bib.bib23)). In summary, the backbone network,
    the detection framework, and the availability of large scale datasets are the
    three most important factors in detection accuracy. Ensembles of multiple models,
    the incorporation of context features, and data augmentation all help to achieve
    better accuracy.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S1.F3 "图 3 ‣ 1 引言 ‣ 通用物体检测的深度学习：综述") 和 [21](#S10.F21 "图 21 ‣ 10.1 最先进的性能
    ‣ 10 讨论与结论 ‣ 通用物体检测的深度学习：综述") 简要概述了最先进的技术，总结了 PASCAL VOC、ILSVRC 和 MSCOCO 挑战的最佳检测结果；更多结果可以在检测挑战网站找到，如
    ILSVRC 检测挑战结果 ([2018](#bib.bib124))；MS COCO 检测排行榜 ([2018](#bib.bib189))；PASCAL
    VOC 检测排行榜 ([2018](#bib.bib208))。公开排行榜上的开放图像挑战物体检测任务的竞赛获胜者在公共排行榜上达到了 $61.71\%$
    mAP，在私人排行榜上达到了 $58.66\%$ mAP，这是通过结合包括 Fast RCNN Girshick ([2015](#bib.bib84))、Faster
    RCNN Ren et al. ([2015](#bib.bib229))、FPN Lin et al. ([2017a](#bib.bib167))、Deformable
    RCNN Dai et al. ([2017](#bib.bib51)) 和 Cascade RCNN Cai 和 Vasconcelos ([2018](#bib.bib23))
    在内的多个两阶段检测器的检测结果获得的。总之，主干网络、检测框架以及大规模数据集的可用性是影响检测准确性的三大重要因素。多个模型的集成、上下文特征的融合以及数据增强都有助于实现更好的准确性。
- en: 'In less than five years, since AlexNet Krizhevsky et al. ([2012a](#bib.bib140))
    was proposed, the Top5 error on ImageNet classification Russakovsky et al. ([2015](#bib.bib234))
    with 1000 classes has dropped from 16% to 2%, as shown in Fig. [15](#S6.F15 "Figure
    15 ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection: A Survey").
    However, the mAP of the best performing detector Peng et al. ([2018](#bib.bib209))
    on COCO Lin et al. ([2014](#bib.bib166)), trained to detect only 80 classes, is
    only at $73\%$, even at 0.5 IoU, illustrating how object detection is much harder
    than image classification. The accuracy and robustness achieved by the state-of-the-art
    detectors far from satisfies the requirements of real world applications, so there
    remains significant room for future improvement.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 AlexNet Krizhevsky et al. ([2012a](#bib.bib140)) 提出以来，不到五年的时间里，ImageNet 分类
    Russakovsky et al. ([2015](#bib.bib234)) 在 1000 个类别上的 Top5 错误率从 16% 降至 2%，如图 [15](#S6.F15
    "图 15 ‣ 6 物体表示 ‣ 通用物体检测的深度学习：综述") 所示。然而，最佳检测器 Peng et al. ([2018](#bib.bib209))
    在 COCO Lin et al. ([2014](#bib.bib166)) 上的 mAP 仅为 $73\%$，即使在 0.5 IoU 下，这也说明物体检测比图像分类要困难得多。现有最先进检测器的准确性和鲁棒性远未满足实际应用的需求，因此未来仍有很大的改进空间。
- en: 10.2 Summary and Discussion
  id: totrans-482
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2 总结与讨论
- en: With hundreds of references and many dozens of methods discussed throughout
    this paper, we would now like to focus on the key factors which have emerged in
    generic object detection based on deep learning.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本文涉及了数百个参考文献和数十种方法，我们现在希望聚焦于基于深度学习的通用物体检测中的关键因素。
- en: '(1) Detection Frameworks: Two Stage vs. One Stage'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 检测框架：两阶段与单阶段
- en: 'In Section [5](#S5 "5 Detection Frameworks ‣ Deep Learning for Generic Object
    Detection: A Survey") we identified two major categories of detection frameworks:
    region based (two stage) and unified (one stage):'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [5](#S5 "5 检测框架 ‣ 通用物体检测的深度学习：综述") 节中，我们识别了两种主要的检测框架类别：基于区域的（两阶段）和统一的（单阶段）：
- en: $\bullet$
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: When large computational cost is allowed, two-stage detectors generally produce
    higher detection accuracies than one-stage, evidenced by the fact that most winning
    approaches used in famous detection challenges like are predominantly based on
    two-stage frameworks, because their structure is more flexible and better suited
    for region based classification. The most widely used frameworks are Faster RCNN
    Ren et al. ([2015](#bib.bib229)), RFCN Dai et al. ([2016c](#bib.bib50)) and Mask
    RCNN He et al. ([2017](#bib.bib102)).
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当允许大量计算成本时，两阶段检测器通常比一阶段产生更高的检测精度，这一事实已被证明，因为大多数在著名检测挑战中获胜的方法主要基于两阶段框架，因为其结构更灵活，更适合基于区域的分类。最广泛使用的框架包括
    Faster RCNN Ren 等（[2015](https://bib.bib229)），RFCN Dai 等（[2016c](https://bib.bib50)）和
    Mask RCNN He 等（[2017](https://bib.bib102)）。
- en: $\bullet$
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: It has been shown in Huang et al. ([2017b](#bib.bib120)) that the detection
    accuracy of one-stage SSD Liu et al. ([2016](#bib.bib175)) is less sensitive to
    the quality of the backbone network than representative two-stage frameworks.
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 Huang 等（[2017b](https://bib.bib120)）的研究中表明，与典型的两阶段框架相比，单阶段 SSD Liu 等（[2016](https://bib.bib175)）的检测精度对主干网络质量不太敏感。
- en: $\bullet$
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: One-stage detectors like YOLO Redmon et al. ([2016](#bib.bib227)) and SSD Liu
    et al. ([2016](#bib.bib175)) are generally faster than two-stage ones, because
    of avoiding preprocessing algorithms, using lightweight backbone networks, performing
    prediction with fewer candidate regions, and making the classification subnetwork
    fully convolutional. However, two-stage detectors can run in real time with the
    introduction of similar techniques. In any event, whether one stage or two, the
    most time consuming step is the feature extractor (backbone network) Law and Deng
    ([2018](#bib.bib146)); Ren et al. ([2015](#bib.bib229)).
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 像 YOLO Redmon 等（[2016](https://bib.bib227)）和 SSD Liu 等（[2016](https://bib.bib175)）这样的单阶段检测器通常比两阶段的速度更快，因为它们避免了预处理算法，使用轻量级的主干网络，通过更少的候选区域进行预测，并使分类子网络完全卷积化。然而，引入类似技术后，两阶段检测器也可以实时运行。无论是一阶段还是两阶段，最耗时的步骤都是特征提取器（主干网络）Law
    和 Deng（[2018](https://bib.bib146)）；Ren 等（[2015](https://bib.bib229)）。
- en: $\bullet$
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: It has been shown Huang et al. ([2017b](#bib.bib120)); Redmon et al. ([2016](#bib.bib227));
    Liu et al. ([2016](#bib.bib175)) that one-stage frameworks like YOLO and SSD typically
    have much poorer performance when detecting small objects than two-stage architectures
    like Faster RCNN and RFCN, but are competitive in detecting large objects.
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Huang 等（[2017b](https://bib.bib120)）；Redmon 等（[2016](https://bib.bib227)）；Liu
    等（[2016](https://bib.bib175)）的研究表明，YOLO 和 SSD 这样的单阶段框架在检测小物体时通常性能较差，而在检测大物体方面则具有竞争力，与
    Faster RCNN 和 RFCN 等两阶段架构相比。
- en: 'There have been many attempts to build better (faster, more accurate, or more
    robust) detectors by attacking each stage of the detection framework. No matter
    whether one, two or multiple stages, the design of the detection framework has
    converged towards a number of crucial design choices:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 通过攻击检测框架的每个阶段，已经进行了许多尝试来构建更好（更快、更准确或更强大）的检测器。无论是一阶段、两阶段还是多阶段，检测框架的设计都朝着一些关键的设计选择趋于统一：
- en: $\bullet$
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: A fully convolutional pipeline
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完全卷积的流水线
- en: $\bullet$
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Exploring complementary information from other correlated tasks, *e.g.*, Mask
    RCNN He et al. ([2017](#bib.bib102))
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索来自其他相关任务的补充信息，*例如*，Mask RCNN He 等（[2017](https://bib.bib102)）。
- en: $\bullet$
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Sliding windows Ren et al. ([2015](#bib.bib229))
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 滑动窗口 Ren 等（[2015](https://bib.bib229)）。
- en: $\bullet$
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: Fusing information from different layers of the backbone.
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 融合来自主干网络不同层的信息。
- en: The evidence from recent success of cascade for object detection Cai and Vasconcelos
    ([2018](#bib.bib23)); Cheng et al. ([2018a](#bib.bib40), [b](#bib.bib41)) and
    instance segmentation on COCO Chen et al. ([2019a](#bib.bib31)) and other challenges
    has shown that multistage object detection could be a future framework for a speed-accuracy
    trade-off. A teaser investigation is being done in the 2019 WIDER Challenge Loy
    et al. ([2019](#bib.bib180)).
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来级联在目标检测 Cai 和 Vasconcelos（[2018](https://bib.bib23)）；Cheng 等（2018a，[b](https://bib.bib41)）和
    COCO 实例分割 Chen 等（[2019a](https://bib.bib31)）等方面的成功证据表明，多阶段目标检测可能是速度与准确性的未来框架。在2019年的WIDER挑战中正在进行一项引人注目的研究
    Loy 等（[2019](https://bib.bib180)）。
- en: (2) Backbone Networks
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: （2）主干网络
- en: 'As discussed in Section [6.1](#S6.SS1 "6.1 Popular CNN Architectures ‣ 6 Object
    Representation ‣ Deep Learning for Generic Object Detection: A Survey"), backbone
    networks are one of the main driving forces behind the rapid improvement of detection
    performance, because of the key role played by discriminative object feature representation.
    Generally, deeper backbones such as ResNet He et al. ([2016](#bib.bib101)), ResNeXt
    Xie et al. ([2017](#bib.bib291)), InceptionResNet Szegedy et al. ([2017](#bib.bib265))
    perform better; however, they are computationally more expensive and require much
    more data and massive computing for training. Some backbones Howard et al. ([2017](#bib.bib112));
    Iandola et al. ([2016](#bib.bib123)); Zhang et al. ([2018c](#bib.bib312)) were
    proposed for focusing on speed instead, such as MobileNet Howard et al. ([2017](#bib.bib112))
    which has been shown to achieve VGGNet16 accuracy on ImageNet with only $\frac{1}{30}$
    the computational cost and model size. Backbone training from scratch may become
    possible as more training data and better training strategies are available Wu
    and He ([2018](#bib.bib285)); Luo et al. ([2019](#bib.bib183), [2018](#bib.bib182)).'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[6.1](#S6.SS1 "6.1 Popular CNN Architectures ‣ 6 Object Representation ‣
    Deep Learning for Generic Object Detection: A Survey")节中讨论的，骨干网络是检测性能迅速提升的主要驱动力之一，因为它们在区分物体特征表示中发挥了关键作用。一般来说，像
    ResNet He 等人 ([2016](#bib.bib101))、ResNeXt Xie 等人 ([2017](#bib.bib291))、InceptionResNet
    Szegedy 等人 ([2017](#bib.bib265)) 等更深的骨干网络表现更好；然而，它们计算开销更大，训练需要更多的数据和大量的计算。一些骨干网络
    Howard 等人 ([2017](#bib.bib112))；Iandola 等人 ([2016](#bib.bib123))；Zhang 等人 ([2018c](#bib.bib312))
    被提出用于专注于速度，比如 MobileNet Howard 等人 ([2017](#bib.bib112))，该网络在 ImageNet 上以仅为 $\frac{1}{30}$
    的计算成本和模型大小实现了 VGGNet16 的准确性。随着更多训练数据和更好的训练策略的可用，骨干网络从头开始训练可能变得可行 Wu 和 He ([2018](#bib.bib285))；Luo
    等人 ([2019](#bib.bib183), [2018](#bib.bib182))。'
- en: (3) Improving the Robustness of Object Representation The variation of real
    world images is a key challenge in object recognition. The variations include
    lighting, pose, deformations, background clutter, occlusions, blur, resolution,
    noise, and camera distortions.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 提高物体表示的鲁棒性 现实世界图像的变化是物体识别中的一个关键挑战。这些变化包括光照、姿态、变形、背景杂乱、遮挡、模糊、分辨率、噪声和相机畸变。
- en: (3.1) Object Scale and Small Object Size
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: (3.1) 物体尺度和小物体尺寸
- en: 'Large variations of object scale, particularly those of small objects, pose
    a great challenge. Here a summary and discussion on the main strategies identified
    in Section [6.2](#S6.SS2 "6.2 Methods For Improving Object Representation ‣ 6
    Object Representation ‣ Deep Learning for Generic Object Detection: A Survey"):'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '物体尺度的大变异，特别是小物体的变异，提出了很大的挑战。以下是第[6.2](#S6.SS2 "6.2 Methods For Improving Object
    Representation ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey")节中识别的主要策略的总结和讨论：'
- en: $\bullet$
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'Using image pyramids: They are simple and effective, helping to enlarge small
    objects and to shrink large ones. They are computationally expensive, but are
    nevertheless commonly used during inference for better accuracy.'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用图像金字塔：它们简单且有效，有助于放大小物体并缩小大物体。虽然计算开销较大，但在推断过程中仍被广泛使用以提高准确性。
- en: $\bullet$
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'Using features from convolutional layers of different resolutions: In early
    work like SSD Liu et al. ([2016](#bib.bib175)), predictions are performed independently,
    and no information from other layers is combined or merged. Now it is quite standard
    to combine features from different layers, e.g. in FPN Lin et al. ([2017a](#bib.bib167)).'
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用来自不同分辨率卷积层的特征：在早期工作如 SSD Liu 等人 ([2016](#bib.bib175)) 中，预测是独立进行的，且没有结合或合并来自其他层的信息。现在，结合来自不同层的特征已成为相当标准的做法，例如在
    FPN Lin 等人 ([2017a](#bib.bib167)) 中。
- en: $\bullet$
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'Using dilated convolutions Li et al. ([2018b](#bib.bib164), [2019b](#bib.bib163)):
    A simple and effective method to incorporate broader context and maintain high
    resolution feature maps.'
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用扩张卷积 Li 等人 ([2018b](#bib.bib164), [2019b](#bib.bib163))：一种简单且有效的方法，用于整合更广泛的上下文并保持高分辨率特征图。
- en: $\bullet$
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'Using anchor boxes of different scales and aspect ratios: Drawbacks of having
    many parameters, and scales and aspect ratios of anchor boxes are usually heuristically
    determined.'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用不同尺度和长宽比的锚框：存在许多参数的缺点，锚框的尺度和长宽比通常是通过启发式方法确定的。
- en: $\bullet$
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'Up-scaling: Particularly for the detection of small objects, high-resolution
    networks Sun et al. ([2019a](#bib.bib255), [b](#bib.bib256)) can be developed.
    It remains unclear whether super-resolution techniques improve detection accuracy
    or not.'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 放大：特别是在小物体检测方面，可以开发高分辨率网络 Sun et al. ([2019a](#bib.bib255), [b](#bib.bib256))。目前尚不清楚超分辨率技术是否能提高检测精度。
- en: Despite recent advances, the detection accuracy for small objects is still much
    lower than that of larger ones. Therefore, the detection of small objects remains
    one of the key challenges in object detection. Perhaps localization requirements
    need to be generalized as a function of scale, since certain applications, e.g.
    autonomous driving, only require the identification of the existence of small
    objects within a larger region, and exact localization is not necessary.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了最近的进展，小物体的检测精度仍远低于大物体。因此，小物体的检测仍然是目标检测中的一个关键挑战。也许需要将定位要求泛化为尺度的函数，因为某些应用，例如自动驾驶，只需要在较大区域内识别小物体的存在，而不需要精确的定位。
- en: (3.2) Deformation, Occlusion, and other factors
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: (3.2) 变形、遮挡和其他因素
- en: 'As discussed in Section [2.2](#S2.SS2 "2.2 Main Challenges ‣ 2 Generic Object
    Detection ‣ Deep Learning for Generic Object Detection: A Survey"), there are
    approaches to handling geometric transformation, occlusions, and deformation mainly
    based on two paradigms. The first is a spatial transformer network, which uses
    regression to obtain a deformation field and then warp features according to the
    deformation field Dai et al. ([2017](#bib.bib51)). The second is based on a deformable
    part-based model Felzenszwalb et al. ([2010b](#bib.bib74)), which finds the maximum
    response to a part filter with spatial constraints taken into consideration Ouyang
    et al. ([2015](#bib.bib203)); Girshick et al. ([2015](#bib.bib86)); Wan et al.
    ([2015](#bib.bib277)).'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[2.2](#S2.SS2 "2.2 主要挑战 ‣ 2 通用目标检测 ‣ 深度学习在通用目标检测中的应用：综述")节中讨论了处理几何变换、遮挡和变形的方法，这些方法主要基于两种范式。第一种是空间变换网络，它使用回归来获得变形场，然后根据变形场对特征进行扭曲
    Dai et al. ([2017](#bib.bib51))。第二种是基于可变形部件模型 Felzenszwalb et al. ([2010b](#bib.bib74))，该模型在考虑空间约束的情况下，寻找对部件滤波器的最大响应
    Ouyang et al. ([2015](#bib.bib203)); Girshick et al. ([2015](#bib.bib86)); Wan
    et al. ([2015](#bib.bib277))。
- en: Rotation invariance may be attractive in certain applications, but there are
    limited generic object detection work focusing on rotation invariance, because
    popular benchmark detection datasets (PASCAL VOC, ImageNet, COCO) do not have
    large variations in rotation. Occlusion handling is intensively studied in face
    detection and pedestrian detection, but very little work has been devoted to occlusion
    handling for generic object detection. In general, despite recent advances, deep
    networks are still limited by the lack of robustness to a number of variations,
    which significantly constrains their real-world applications.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些应用中，旋转不变性可能具有吸引力，但关于旋转不变性的通用目标检测工作较少，因为流行的基准检测数据集（PASCAL VOC、ImageNet、COCO）没有大幅度的旋转变化。遮挡处理在面部检测和行人检测中得到了广泛研究，但对通用目标检测的遮挡处理研究非常有限。总体而言，尽管有了最近的进展，深度网络仍然受到对多种变化缺乏鲁棒性的限制，这大大限制了其在现实世界中的应用。
- en: (4) Context Reasoning
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 上下文推理
- en: 'As introduced in Section [7](#S7 "7 Context Modeling ‣ Deep Learning for Generic
    Object Detection: A Survey"), objects in the wild typically coexist with other
    objects and environments. It has been recognized that contextual information (object
    relations, global scene statistics) helps object detection and recognition Oliva
    and Torralba ([2007](#bib.bib197)), especially for small objects, occluded objects,
    and with poor image quality. There was extensive work preceding deep learning
    Malisiewicz and Efros ([2009](#bib.bib185)); Murphy et al. ([2003](#bib.bib193));
    Rabinovich et al. ([2007](#bib.bib220)); Divvala et al. ([2009](#bib.bib58));
    Galleguillos and Belongie ([2010](#bib.bib78)), and also quite a few works in
    the era of deep learning Gidaris and Komodakis ([2015](#bib.bib82)); Zeng et al.
    ([2016](#bib.bib304), [2017](#bib.bib305)); Chen and Gupta ([2017](#bib.bib35));
    Hu et al. ([2018a](#bib.bib114)). How to efficiently and effectively incorporate
    contextual information remains to be explored, possibly guided by how human vision
    uses context, based on scene graphs Li et al. ([2017d](#bib.bib161)), or via the
    full segmentation of objects and scenes using panoptic segmentation Kirillov et al.
    ([2018](#bib.bib134)).'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第 [7](#S7 "7 上下文建模 ‣ 深度学习用于通用物体检测：一项调查") 节中介绍的那样，野外的物体通常与其他物体和环境共存。已经认识到，上下文信息（物体关系、全局场景统计）有助于物体检测和识别。特别是对于小物体、遮挡物体和图像质量差的情况。在深度学习之前就有大量工作，如
    Malisiewicz 和 Efros（[2009](#bib.bib185)）；Murphy 等（[2003](#bib.bib193)）；Rabinovich
    等（[2007](#bib.bib220)）；Divvala 等（[2009](#bib.bib58)）；Galleguillos 和 Belongie（[2010](#bib.bib78)）；以及深度学习时代的许多工作，如
    Gidaris 和 Komodakis（[2015](#bib.bib82)）；Zeng 等（[2016](#bib.bib304), [2017](#bib.bib305)）；Chen
    和 Gupta（[2017](#bib.bib35)）；Hu 等（[2018a](#bib.bib114)）。如何高效有效地整合上下文信息仍然需要探索，可能会受到人类视觉如何使用上下文的启发，基于场景图
    Li 等（[2017d](#bib.bib161)），或通过全景分割对物体和场景进行完全分割 Kirillov 等（[2018](#bib.bib134)）。
- en: (5) Detection Proposals
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 检测提议
- en: Detection proposals significantly reduce search spaces. As recommended in Hosang
    et al. ([2016](#bib.bib110)), future detection proposals will surely have to improve
    in repeatability, recall, localization accuracy, and speed. Since the success
    of RPN Ren et al. ([2015](#bib.bib229)), which integrated proposal generation
    and detection into a common framework, CNN based detection proposal generation
    methods have dominated region proposal. It is recommended that new detection proposals
    should be assessed for object detection, instead of evaluating detection proposals
    alone.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 检测提议显著减少了搜索空间。正如 Hosang 等（[2016](#bib.bib110)）建议的那样，未来的检测提议必须在可重复性、召回率、定位精度和速度方面有所改进。自
    RPN 的成功 Ren 等（[2015](#bib.bib229)）以来，将提议生成和检测整合到一个通用框架中的方法已经主导了基于CNN的检测提议生成方法。建议新的检测提议应该被评估用于物体检测，而不仅仅是评估检测提议本身。
- en: (6) Other Factors
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 其他因素
- en: 'As discussed in Section [9](#S9 "9 Other Issues ‣ Deep Learning for Generic
    Object Detection: A Survey"), there are many other factors affecting object detection
    quality: data augmentation, novel training strategies, combinations of backbone
    models, multiple detection frameworks, incorporating information from other related
    tasks, methods for reducing localization error, handling the huge imbalance between
    positive and negative samples, mining of hard negative samples, and improving
    loss functions.'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第 [9](#S9 "9 其他问题 ‣ 深度学习用于通用物体检测：一项调查") 节中讨论的那样，有许多其他因素影响物体检测质量：数据增强、新颖的训练策略、骨干模型的组合、多个检测框架、整合来自其他相关任务的信息、减少定位错误的方法、处理正负样本巨大不平衡、挖掘难负样本以及改进损失函数。
- en: 10.3 Research Directions
  id: totrans-529
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3 研究方向
- en: 'Despite the recent tremendous progress in the field of object detection, the
    technology remains significantly more primitive than human vision and cannot yet
    satisfactorily address real-world challenges like those of Section [2.2](#S2.SS2
    "2.2 Main Challenges ‣ 2 Generic Object Detection ‣ Deep Learning for Generic
    Object Detection: A Survey"). We see a number of long-standing challenges:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管物体检测领域最近取得了巨大的进展，但该技术仍然比人类视觉显著原始，尚无法令人满意地应对真实世界的挑战，例如第 [2.2](#S2.SS2 "2.2
    主要挑战 ‣ 2 通用物体检测 ‣ 深度学习用于通用物体检测：一项调查") 节的挑战。我们看到一些长期存在的挑战：
- en: $\bullet$
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'Working in an open world: being robust to any number of environmental changes,
    being able to evolve or adapt.'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在开放世界中工作：对任何环境变化都具有鲁棒性，能够进化或适应。
- en: $\bullet$
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'Object detection under constrained conditions: learning from weakly labeled
    data or few bounding box annotations, wearable devices, unseen object categories
    etc.'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在约束条件下的物体检测：从弱标记数据或少量边界框注释中学习，穿戴设备，未见过的物体类别等。
- en: $\bullet$
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\bullet$
- en: 'Object detection in other modalities: video, RGBD images, 3D point clouds,
    lidar, remotely sensed imagery *etc*.'
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他模态下的物体检测：视频，RGBD 图像，3D 点云，激光雷达，遥感影像*等*。
- en: 'Based on these challenges, we see the following directions of future research:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些挑战，我们可以看到未来研究的以下方向：
- en: '(1) Open World Learning: The ultimate goal is to develop object detection capable
    of accurately and efficiently recognizing and localizing instances in thousands
    or more object categories in open-world scenes, at a level competitive with the
    human visual system. Object detection algorithms are unable, in general, to recognize
    object categories outside of their training dataset, although ideally there should
    be the ability to recognize novel object categories Lake et al. ([2015](#bib.bib144));
    Hariharan and Girshick ([2017](#bib.bib95)). Current detection datasets Everingham
    et al. ([2010](#bib.bib68)); Russakovsky et al. ([2015](#bib.bib234)); Lin et al.
    ([2014](#bib.bib166)) contain only a few dozen to hundreds of categories, significantly
    fewer than those which can be recognized by humans. New larger-scale datasets
    Hoffman et al. ([2014](#bib.bib107)); Singh et al. ([2018a](#bib.bib250)); Redmon
    and Farhadi ([2017](#bib.bib226)) with significantly more categories will need
    to be developed.'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 开放世界学习：最终目标是开发能够准确高效地识别和定位开放世界场景中成千上万种物体类别的物体检测技术，达到与人类视觉系统竞争的水平。物体检测算法通常无法识别训练数据集中之外的物体类别，尽管理想情况下应该能够识别新的物体类别
    Lake 等人 ([2015](#bib.bib144))；Hariharan 和 Girshick ([2017](#bib.bib95))。当前的检测数据集
    Everingham 等人 ([2010](#bib.bib68))；Russakovsky 等人 ([2015](#bib.bib234))；Lin 等人
    ([2014](#bib.bib166)) 仅包含几十到几百个类别，远少于人类可以识别的类别。需要开发新的更大规模的数据集 Hoffman 等人 ([2014](#bib.bib107))；Singh
    等人 ([2018a](#bib.bib250))；Redmon 和 Farhadi ([2017](#bib.bib226))，这些数据集包含的类别显著更多。
- en: '(2) Better and More Efficient Detection Frameworks: One of the reasons for
    the success in generic object detection has been the development of superior detection
    frameworks, both region-based (RCNN Girshick et al. ([2014](#bib.bib85)), Fast
    RCNN Girshick ([2015](#bib.bib84)), Faster RCNN Ren et al. ([2015](#bib.bib229)),
    Mask RCNN He et al. ([2017](#bib.bib102))) and one-stage detectors (YOLO Redmon
    et al. ([2016](#bib.bib227)), SSD Liu et al. ([2016](#bib.bib175))). Region-based
    detectors have higher accuracy, one-stage detectors are generally faster and simpler.
    Object detectors depend heavily on the underlying backbone networks, which have
    been optimized for image classification, possibly causing a learning bias; learning
    object detectors from scratch could be helpful for new detection frameworks.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 更好且更高效的检测框架：在通用物体检测成功的原因之一是开发了更优的检测框架，包括基于区域的（RCNN Girshick 等人 ([2014](#bib.bib85))，Fast
    RCNN Girshick ([2015](#bib.bib84))，Faster RCNN Ren 等人 ([2015](#bib.bib229))，Mask
    RCNN He 等人 ([2017](#bib.bib102))) 和单阶段检测器（YOLO Redmon 等人 ([2016](#bib.bib227))，SSD
    Liu 等人 ([2016](#bib.bib175))）。基于区域的检测器具有更高的准确性，而单阶段检测器通常更快且更简单。物体检测器严重依赖于基础骨干网络，这些网络已经针对图像分类进行了优化，可能导致学习偏差；从头学习物体检测器可能对新检测框架有帮助。
- en: '(3) Compact and Efficient CNN Features: CNNs have increased remarkably in depth,
    from several layers (AlexNet Krizhevsky et al. ([2012b](#bib.bib141))) to hundreds
    of layers (ResNet He et al. ([2016](#bib.bib101)), DenseNet Huang et al. ([2017a](#bib.bib118))).
    These networks have millions to hundreds of millions of parameters, requiring
    massive data and GPUs for training. In order reduce or remove network redundancy,
    there has been growing research interest in designing compact and lightweight
    networks Chen et al. ([2017a](#bib.bib29)); Alvarez and Salzmann ([2016](#bib.bib4));
    Huang et al. ([2018](#bib.bib119)); Howard et al. ([2017](#bib.bib112)); Lin et al.
    ([2017c](#bib.bib169)); Yu et al. ([2018](#bib.bib300)) and network acceleration
    Cheng et al. ([2018c](#bib.bib44)); Hubara et al. ([2016](#bib.bib122)); Song Han
    ([2016](#bib.bib253)); Li et al. ([2017a](#bib.bib155), [c](#bib.bib158)); Wei
    et al. ([2018](#bib.bib282)).'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 紧凑且高效的 CNN 特征：CNN 的深度显著增加，从几个层（AlexNet Krizhevsky 等人 ([2012b](#bib.bib141)))
    到数百层（ResNet He 等人 ([2016](#bib.bib101)), DenseNet Huang 等人 ([2017a](#bib.bib118)))。这些网络拥有数百万到数亿个参数，需要大量数据和
    GPU 进行训练。为了减少或消除网络冗余，设计紧凑且轻量化的网络引起了越来越多的研究兴趣 Chen 等人 ([2017a](#bib.bib29)); Alvarez
    和 Salzmann ([2016](#bib.bib4)); Huang 等人 ([2018](#bib.bib119)); Howard 等人 ([2017](#bib.bib112));
    Lin 等人 ([2017c](#bib.bib169)); Yu 等人 ([2018](#bib.bib300)) 和网络加速 Cheng 等人 ([2018c](#bib.bib44));
    Hubara 等人 ([2016](#bib.bib122)); Song Han ([2016](#bib.bib253)); Li 等人 ([2017a](#bib.bib155),
    [c](#bib.bib158)); Wei 等人 ([2018](#bib.bib282))。
- en: '(4) Automatic Neural Architecture Search: Deep learning bypasses manual feature
    engineering which requires human experts with strong domain knowledge, however
    DCNNs require similarly significant expertise. It is natural to consider automated
    design of detection backbone architectures, such as the recent Automated Machine
    Learning (AutoML) Quanming et al. ([2018](#bib.bib219)), which has been applied
    to image classification and object detection Cai et al. ([2018](#bib.bib22));
    Chen et al. ([2019c](#bib.bib39)); Ghiasi et al. ([2019](#bib.bib80)); Liu et al.
    ([2018a](#bib.bib171)); Zoph and Le ([2017](#bib.bib331)); Zoph et al. ([2018](#bib.bib332)).'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 自动化神经网络架构搜索：深度学习绕过了需要具备强大领域知识的人类专家的手工特征工程，但 DCNN 也同样需要显著的专业知识。自然地，我们考虑自动化设计检测骨干架构，例如最近的自动化机器学习（AutoML）Quanming
    等人 ([2018](#bib.bib219))，已应用于图像分类和目标检测 Cai 等人 ([2018](#bib.bib22)); Chen 等人 ([2019c](#bib.bib39));
    Ghiasi 等人 ([2019](#bib.bib80)); Liu 等人 ([2018a](#bib.bib171)); Zoph 和 Le ([2017](#bib.bib331));
    Zoph 等人 ([2018](#bib.bib332))。
- en: '(5) Object Instance Segmentation: For a richer and more detailed understanding
    of image content, there is a need to tackle pixel-level object instance segmentation
    Lin et al. ([2014](#bib.bib166)); He et al. ([2017](#bib.bib102)); Hu et al. ([2018c](#bib.bib117)),
    which can play an important role in potential applications that require the precise
    boundaries of individual objects.'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 目标实例分割：为了更丰富和详细地理解图像内容，需要处理像素级的目标实例分割 Lin 等人 ([2014](#bib.bib166)); He 等人
    ([2017](#bib.bib102)); Hu 等人 ([2018c](#bib.bib117))，这在需要精确物体边界的潜在应用中发挥着重要作用。
- en: '(6) Weakly Supervised Detection: Current state-of-the-art detectors employ
    fully supervised models learned from labeled data with object bounding boxes or
    segmentation masks Everingham et al. ([2015](#bib.bib69)); Lin et al. ([2014](#bib.bib166));
    Russakovsky et al. ([2015](#bib.bib234)); Lin et al. ([2014](#bib.bib166)). However,
    fully supervised learning has serious limitations, particularly where the collection
    of bounding box annotations is labor intensive and where the number of images
    is large. Fully supervised learning is not scalable in the absence of fully labeled
    training data, so it is essential to understand how the power of CNNs can be leveraged
    where only weakly / partially annotated data are provided Bilen and Vedaldi ([2016](#bib.bib17));
    Diba et al. ([2017](#bib.bib55)); Shi et al. ([2017](#bib.bib244)).'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 弱监督检测：当前最先进的检测器使用从带有目标边界框或分割掩模的标注数据中学习的完全监督模型 Everingham 等人 ([2015](#bib.bib69));
    Lin 等人 ([2014](#bib.bib166)); Russakovsky 等人 ([2015](#bib.bib234)); Lin 等人 ([2014](#bib.bib166))。然而，完全监督学习存在严重的局限性，特别是在收集边界框标注工作量大以及图像数量庞大的情况下。完全监督学习在缺乏完全标注训练数据的情况下不可扩展，因此了解如何在仅提供弱标注/部分标注数据的情况下利用
    CNN 的能力至关重要 Bilen 和 Vedaldi ([2016](#bib.bib17)); Diba 等人 ([2017](#bib.bib55));
    Shi 等人 ([2017](#bib.bib244))。
- en: '(7) Few / Zero Shot Object Detection: The success of deep detectors relies
    heavily on gargantuan amounts of annotated training data. When the labeled data
    are scarce, the performance of deep detectors frequently deteriorates and fails
    to generalize well. In contrast, humans (even children) can learn a visual concept
    quickly from very few given examples and can often generalize well Biederman ([1987b](#bib.bib16));
    Lake et al. ([2015](#bib.bib144)); FeiFei et al. ([2006](#bib.bib71)). Therefore,
    the ability to learn from only few examples, *few* shot detection, is very appealing
    Chen et al. ([2018a](#bib.bib30)); Dong et al. ([2018](#bib.bib61)); Finn et al.
    ([2017](#bib.bib75)); Kang et al. ([2018](#bib.bib129)); Lake et al. ([2015](#bib.bib144));
    Ren et al. ([2018](#bib.bib228)); Schwartz et al. ([2019](#bib.bib237)). Even
    more constrained, *zero* shot object detection localizes and recognizes object
    classes that have never been seen^(16)^(16)16Although side information may be
    provided, such as a wikipedia page or an attributes vector. before Bansal et al.
    ([2018](#bib.bib9)); Demirel et al. ([2018](#bib.bib53)); Rahman et al. ([2018b](#bib.bib222),
    [a](#bib.bib221)), essential for life-long learning machines that need to intelligently
    and incrementally discover new object categories.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: (7) 少量 / 零样本目标检测：深度检测器的成功在很大程度上依赖于庞大的标注训练数据。当标注数据稀缺时，深度检测器的性能经常会下降，无法很好地泛化。相比之下，人类（甚至是孩子）可以从极少的示例中快速学习视觉概念，并且通常可以很好地泛化（Biederman
    ([1987b](#bib.bib16)); Lake et al. ([2015](#bib.bib144)); FeiFei et al. ([2006](#bib.bib71))）。因此，仅凭少量示例学习的能力，*少量*样本检测，非常吸引人（Chen
    et al. ([2018a](#bib.bib30)); Dong et al. ([2018](#bib.bib61)); Finn et al. ([2017](#bib.bib75));
    Kang et al. ([2018](#bib.bib129)); Lake et al. ([2015](#bib.bib144)); Ren et al.
    ([2018](#bib.bib228)); Schwartz et al. ([2019](#bib.bib237))）。更为苛刻的是，*零*样本目标检测可以定位和识别以前从未见过的对象类别（Bansal
    et al. ([2018](#bib.bib9)); Demirel et al. ([2018](#bib.bib53)); Rahman et al.
    ([2018b](#bib.bib222), [a](#bib.bib221))），这对于需要智能和逐步发现新对象类别的终身学习机器非常重要。
- en: '(8) Object Detection in Other Modalities: Most detectors are based on still
    2D images; object detection in other modalities can be highly relevant in domains
    such as autonomous vehicles, unmanned aerial vehicles, and robotics. These modalities
    raise new challenges in effectively using depth Chen et al. ([2015c](#bib.bib36));
    Pepik et al. ([2015](#bib.bib211)); Xiang et al. ([2014](#bib.bib289)); Wu et al.
    ([2015](#bib.bib286)), video Feichtenhofer et al. ([2017](#bib.bib70)); Kang et al.
    ([2016](#bib.bib130)), and point clouds Qi et al. ([2017](#bib.bib217), [2018](#bib.bib218)).'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: (8) 其他模态中的目标检测：大多数检测器基于静态的 2D 图像；在其他模态中的目标检测在自动驾驶车辆、无人机和机器人等领域具有高度相关性。这些模态在有效利用深度（Chen
    et al. ([2015c](#bib.bib36)); Pepik et al. ([2015](#bib.bib211)); Xiang et al.
    ([2014](#bib.bib289)); Wu et al. ([2015](#bib.bib286))）、视频（Feichtenhofer et al.
    ([2017](#bib.bib70)); Kang et al. ([2016](#bib.bib130))）和点云（Qi et al. ([2017](#bib.bib217),
    [2018](#bib.bib218))）方面带来了新的挑战。
- en: '(9) Universal Object Detection: Recently, there has been increasing effort
    in learning *universal representations*, those which are effective in multiple
    image domains, such as natural images, videos, aerial images, and medical CT images
    Rebuffi et al. ([2017](#bib.bib224), [2018](#bib.bib225)). Most such research
    focuses on image classification, rarely targeting object detection Wang et al.
    ([2019](#bib.bib281)), and developed detectors are usually domain specific. Object
    detection independent of image domain and cross-domain object detection represent
    important future directions.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: (9) 通用对象检测：最近，人们在学习*通用表示*方面投入了越来越多的努力，这些表示在多个图像领域（如自然图像、视频、航空图像和医学 CT 图像）中都非常有效（Rebuffi
    et al. ([2017](#bib.bib224), [2018](#bib.bib225))）。大多数此类研究集中于图像分类，很少关注目标检测（Wang
    et al. ([2019](#bib.bib281))），而且开发的检测器通常是特定领域的。独立于图像领域的对象检测和跨领域对象检测代表着重要的未来方向。
- en: The research field of generic object detection is still far from complete. However
    given the breakthroughs over the past five years we are optimistic of future developments
    and opportunities.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 通用目标检测研究领域仍然远未完成。然而，考虑到过去五年的突破，我们对未来的发展和机会持乐观态度。
- en: 'Table 11: Summary of properties and performance of milestone detection frameworks
    for generic object detection. See Section [5](#S5 "5 Detection Frameworks ‣ Deep
    Learning for Generic Object Detection: A Survey") for a detailed discussion. Some
    architectures are illustrated in Fig. [13](#S5.F13 "Figure 13 ‣ 5.1 Region Based
    (Two Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object
    Detection: A Survey"). The properties of the backbone DCNNs can be found in Table [6](#S6.T6
    "Table 6 ‣ 6 Object Representation ‣ Deep Learning for Generic Object Detection:
    A Survey").'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '表11：用于通用目标检测的里程碑检测框架的属性和性能总结。详细讨论见第[5](#S5 "5 Detection Frameworks ‣ Deep Learning
    for Generic Object Detection: A Survey")节。一些架构在图[13](#S5.F13 "Figure 13 ‣ 5.1
    Region Based (Two Stage) Frameworks ‣ 5 Detection Frameworks ‣ Deep Learning for
    Generic Object Detection: A Survey")中有所说明。主干DCNN的属性可以在表[6](#S6.T6 "Table 6 ‣ 6
    Object Representation ‣ Deep Learning for Generic Object Detection: A Survey")中找到。'
- en: '|      | Detector Name | RP | Backbone DCNN | Input ImgSize | VOC07 Results
    | VOC12 Results | Speed (FPS) | Published In | Source Code | Highlights and Disadvantages
       |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '|      | 检测器名称 | RP | 主干DCNN | 输入图像大小 | VOC07结果 | VOC12结果 | 速度（FPS） | 发表在 |
    源代码 | 亮点和缺点    |'
- en: '|      Region based (Section [5.1](#S5.SS1 "5.1 Region Based (Two Stage) Frameworks
    ‣ 5 Detection Frameworks ‣ Deep Learning for Generic Object Detection: A Survey"))
    | RCNN Girshick et al. ([2014](#bib.bib85)) | SS | AlexNet | Fixed | $58.5$ (07)
    | $53.3$ (12) | $<0.1$ | CVPR14 | Caffe Matlab | Highlights: First to integrate
    CNN with RP methods; Dramatic performance improvement over previous state of the
    artP. Disadvantages: Multistage pipeline of sequentially-trained (External RP
    computation, CNN finetuning, each warped RP passing through CNN, SVM and BBR training);
    Training is expensive in space and time; Testing is slow.    |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '|      基于区域的（第[5.1](#S5.SS1 "5.1 Region Based (Two Stage) Frameworks ‣ 5 Detection
    Frameworks ‣ Deep Learning for Generic Object Detection: A Survey")节） | RCNN Girshick等人（[2014](#bib.bib85)）
    | SS | AlexNet | 固定 | $58.5$ (07) | $53.3$ (12) | $<0.1$ | CVPR14 | Caffe Matlab
    | 亮点：首次将CNN与RP方法集成；相较于之前的技术，性能大幅提升。缺点：多阶段流水线的顺序训练（外部RP计算、CNN微调、每个扭曲的RP通过CNN、SVM和BBR训练）；训练在空间和时间上代价高；测试速度慢。
       |'
- en: '|   | SPPNet He et al. ([2014](#bib.bib99)) | SS | ZFNet | Arbitrary | $60.9$
    (07) | $-$ | $<1$ | ECCV14 | Caffe Matlab | Highlights: First to introduce SPP
    into CNN architecture; Enable convolutional feature sharing; Accelerate RCNN evaluation
    by orders of magnitude without sacrificing performance; Faster than OverFeat.
    Disadvantages: Inherit disadvantages of RCNN; Does not result in much training
    speedup; Fine-tuning not able to update the CONV layers before SPP layer.    |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '|   | SPPNet He等人（[2014](#bib.bib99)） | SS | ZFNet | 任意 | $60.9$ (07) | $-$
    | $<1$ | ECCV14 | Caffe Matlab | 亮点：首次将SPP引入CNN架构；启用卷积特征共享；在不牺牲性能的情况下，加速RCNN评估；比OverFeat更快。缺点：继承了RCNN的缺点；训练加速效果不明显；微调无法更新SPP层之前的CONV层。
       |'
- en: '|   | Fast RCNN Girshick ([2015](#bib.bib84)) | SS | AlexNet VGGM VGG16 | Arbitrary
    | $70.0$ (VGG) (07+12) | $68.4$ (VGG) (07++12) | $<1$ | ICCV15 | Caffe Python
    | Highlights: First to enable end-to-end detector training (ignoring RP generation);
    Design a RoI pooling layer; Much faster and more accurate than SPPNet; No disk
    storage required for feature caching. Disadvantages: External RP computation is
    exposed as the new bottleneck; Still too slow for real time applications.    |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '|   | Fast RCNN Girshick（[2015](#bib.bib84)） | SS | AlexNet VGGM VGG16 | 任意
    | $70.0$ (VGG) (07+12) | $68.4$ (VGG) (07++12) | $<1$ | ICCV15 | Caffe Python
    | 亮点：首次实现了端到端检测器训练（忽略RP生成）；设计了RoI池化层；比SPPNet更快更准确；不需要磁盘存储用于特征缓存。缺点：外部RP计算暴露为新的瓶颈；仍然对于实时应用程序过于缓慢。
       |'
- en: '|   | Faster RCNN Ren et al. ([2015](#bib.bib229)) | RPN | ZFnet VGG | Arbitrary
    | $73.2$ (VGG) (07+12) | $70.4$ (VGG) (07++12) | $<5$ | NIPS15 | Caffe Matlab
    Python | Highlights: Propose RPN for generating nearly cost-free and high quality
    RPs instead of selective search; Introduce translation invariant and multiscale
    anchor boxes as references in RPN; Unify RPN and Fast RCNN into a single network
    by sharing CONV layers; An order of magnitude faster than Fast RCNN without performance
    loss; Can run testing at 5 FPS with VGG16. Disadvantages: Training is complex,
    not a streamlined process; Still falls short of real time.    |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '|   | Faster RCNN Ren et al. ([2015](#bib.bib229)) | RPN | ZFnet VGG | 任意 |
    $73.2$ (VGG) (07+12) | $70.4$ (VGG) (07++12) | $<5$ | NIPS15 | Caffe Matlab Python
    | 亮点：提出了RPN以生成几乎无需成本的高质量RP，而不是选择性搜索；在RPN中引入了平移不变和多尺度锚框作为参考；通过共享CONV层将RPN和Fast
    RCNN统一为一个网络；比Fast RCNN快一个数量级，且没有性能损失；使用VGG16时可以以5 FPS的速度进行测试。缺点：训练复杂，不流畅；仍然无法满足实时需求。
       |'
- en: '|   | RCNN$\ominus$R Lenc and Vedaldi ([2015](#bib.bib151)) | New | ZFNet +SPP
    | Arbitrary | $59.7$ (07) | $-$ | $<5$ | BMVC15 | $-$ | Highlights: Replace selective
    search with static RPs; Prove the possibility of building integrated, simpler
    and faster detectors that rely exclusively on CNN. Disadvantages: Falls short
    of real time; Decreased accuracy from poor RPs.    |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '|   | RCNN$\ominus$R Lenc和Vedaldi ([2015](#bib.bib151)) | 新 | ZFNet + SPP |
    任意 | $59.7$ (07) | $-$ | $<5$ | BMVC15 | $-$ | 亮点：用静态RP替代选择性搜索；证明了仅依靠CNN构建集成、简单且快速检测器的可能性。缺点：无法满足实时需求；由于RP质量差，准确性下降。
       |'
- en: '|   | RFCN Dai et al. ([2016c](#bib.bib50)) | RPN | ResNet101 | Arbitrary |
    $80.5$ (07+12) $83.6$ (07+12+CO) | $77.6$ (07++12) $82.0$ (07++12+CO) | $<10$
    | NIPS16 | Caffe Matlab | Highlights: Fully convolutional detection network; Design
    a set of position sensitive score maps using a bank of specialized CONV layers;
    Faster than Faster RCNN without sacrificing much accuracy. Disadvantages: Training
    is not a streamlined process; Still falls short of real time.    |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '|   | RFCN Dai et al. ([2016c](#bib.bib50)) | RPN | ResNet101 | 任意 | $80.5$
    (07+12) $83.6$ (07+12+CO) | $77.6$ (07++12) $82.0$ (07++12+CO) | $<10$ | NIPS16
    | Caffe Matlab | 亮点：完全卷积检测网络；使用一组位置敏感得分图来设计一系列专用的CONV层；比Faster RCNN更快，同时不牺牲太多准确性。缺点：训练过程不流畅；仍然无法满足实时需求。
       |'
- en: '|   | Mask RCNN He et al. ([2017](#bib.bib102)) | RPN | ResNet101 ResNeXt101
    | Arbitrary | $50.3$ (ResNeXt101) (COCO Result) | $<5$ | ICCV17 | Caffe Matlab
    Python | Highlights: A simple, flexible, and effective framework for object instance
    segmentation; Extends Faster RCNN by adding another branch for predicting an object
    mask in parallel with the existing branch for BB prediction; Feature Pyramid Network
    (FPN) is utilized; Outstanding performance. Disadvantages: Falls short of real
    time applications.    |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|   | Mask RCNN He et al. ([2017](#bib.bib102)) | RPN | ResNet101 ResNeXt101
    | 任意 | $50.3$ (ResNeXt101) (COCO结果) | $<5$ | ICCV17 | Caffe Matlab Python | 亮点：一个简单、灵活且有效的物体实例分割框架；通过增加另一个分支来预测物体掩码，从而扩展了Faster
    RCNN；利用了特征金字塔网络（FPN）；表现出色。缺点：无法满足实时应用的需求。    |'
- en: '|      Unified (Section [5.2](#S5.SS2 "5.2 Unified (One Stage) Frameworks ‣
    5 Detection Frameworks ‣ Deep Learning for Generic Object Detection: A Survey"))
                       | OverFeat Sermanet et al. ([2014](#bib.bib239)) | $-$ | AlexNet
    like | Arbitrary | $-$ | $-$ | $<0.1$ | ICLR14 | c++ | Highlights: Convolutional
    feature sharing; Multiscale image pyramid CNN feature extraction; Won the ISLVRC2013
    localization competition; Significantly faster than RCNN. Disadvantages: Multi-stage
    pipeline sequentially trained; Single bounding box regressor; Cannot handle multiple
    object instances of the same class; Too slow for real time applications.    |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '|      统一 (第 [5.2](#S5.SS2 "5.2 Unified (One Stage) Frameworks ‣ 5 Detection
    Frameworks ‣ Deep Learning for Generic Object Detection: A Survey") 节)                   
    | OverFeat Sermanet et al. ([2014](#bib.bib239)) | $-$ | 类似于AlexNet | 任意 | $-$
    | $-$ | $<0.1$ | ICLR14 | c++ | 亮点：卷积特征共享；多尺度图像金字塔CNN特征提取；赢得了ISLVRC2013定位竞赛；比RCNN显著更快。缺点：多阶段管道顺序训练；单一的边界框回归器；无法处理同一类别的多个物体实例；对于实时应用来说太慢。
       |'
- en: '|   | YOLO Redmon et al. ([2016](#bib.bib227)) | $-$ | GoogLeNet like | Fixed
    | $66.4$ (07+12) | $57.9$ (07++12) | $<25$ (VGG) | CVPR16 | DarkNet | Highlights:
    First efficient unified detector; Drop RP process completely; Elegant and efficient
    detection framework; Significantly faster than previous detectors; YOLO runs at
    45 FPS, Fast YOLO at 155 FPS; Disadvantages: Accuracy falls far behind state of
    the art detectors; Struggle to localize small objects.    |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '|   | YOLO Redmon 等人 ([2016](#bib.bib227)) | $-$ | 类似于 GoogLeNet | 固定 | $66.4$
    (07+12) | $57.9$ (07++12) | $<25$ (VGG) | CVPR16 | DarkNet | 亮点：首个高效的统一检测器；完全省略了RP过程；优雅且高效的检测框架；比之前的检测器显著更快；YOLO运行在45
    FPS，Fast YOLO运行在155 FPS；缺点：准确性远远落后于最新的检测器；难以定位小物体。    |'
- en: '|   | YOLOv2Redmon and Farhadi ([2017](#bib.bib226)) | $-$ | DarkNet | Fixed
    | $78.6$ (07+12) | $73.5$ (07++12) | $<50$ | CVPR17 | DarkNet | Highlights: Propose
    a faster DarkNet19; Use a number of existing strategies to improve both speed
    and accuracy; Achieve high accuracy and high speed; YOLO9000 can detect over 9000
    object categories in real time. Disadvantages: Not good at detecting small objects.
       |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '|   | YOLOv2Redmon 和 Farhadi ([2017](#bib.bib226)) | $-$ | DarkNet | 固定 | $78.6$
    (07+12) | $73.5$ (07++12) | $<50$ | CVPR17 | DarkNet | 亮点：提出了更快的DarkNet19；使用了多种现有策略来提高速度和准确性；实现了高准确性和高速度；YOLO9000能够实时检测超过9000个目标类别。缺点：不擅长检测小物体。
       |'
- en: '|   | SSD Liu et al. ([2016](#bib.bib175)) | $-$ | VGG16 | Fixed | $76.8$ (07+12)
    $81.5$ (07+12+CO) | $74.9$ (07++12) $80.0$ (07++12+CO) | $<60$ | ECCV16 | Caffe
    Python | Highlights: First accurate and efficient unified detector; Effectively
    combine ideas from RPN and YOLO to perform detection at multi-scale CONV layers;
    Faster and significantly more accurate than YOLO; Can run at 59 FPS; Disadvantages:
    Not good at detecting small objects.    |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '|   | SSD Liu 等人 ([2016](#bib.bib175)) | $-$ | VGG16 | 固定 | $76.8$ (07+12)
    $81.5$ (07+12+CO) | $74.9$ (07++12) $80.0$ (07++12+CO) | $<60$ | ECCV16 | Caffe
    Python | 亮点：首个准确且高效的统一检测器；有效结合了RPN和YOLO的思想，在多尺度CONV层上进行检测；比YOLO更快且准确得多；可在59 FPS下运行；缺点：不擅长检测小物体。
       |'
- en: '|                                                         |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '|                                                              |'
- en: '*Abbreviations in this table: Region Proposal (RP), Selective Search (SS),
    Region Proposal Network (RPN), RCNN$\ominus$R represents “RCNN minus R” and used
    a trivial RP method. Training data: “07”$\leftarrow$VOC2007 trainval; “07T”$\leftarrow$VOC2007
    trainval and test; “12”$\leftarrow$VOC2012 trainval; “CO”$\leftarrow$COCO trainval.
    The “Speed” column roughly estimates the detection speed with a single Nvidia
    Titan X GPU.*'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '*表中的缩写：区域提议（RP），选择性搜索（SS），区域提议网络（RPN），RCNN$\ominus$R 表示“RCNN 减去 R”，并使用了一个简单的RP方法。训练数据：“07”$\leftarrow$VOC2007
    训练和验证；“07T”$\leftarrow$VOC2007 训练和测试；“12”$\leftarrow$VOC2012 训练和验证；“CO”$\leftarrow$COCO
    训练和验证。“速度”列大致估算了使用单个 Nvidia Titan X GPU 的检测速度。*'
- en: 11 Acknowledgments
  id: totrans-563
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11 致谢
- en: The authors would like to thank the pioneering researchers in generic object
    detection and other related fields. The authors would also like to express their
    sincere appreciation to Professor Jiří Matas, the associate editor and the anonymous
    reviewers for their comments and suggestions. This work has been supported by
    the Center for Machine Vision and Signal Analysis at the University of Oulu (Finland)
    and the National Natural Science Foundation of China under Grant 61872379.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们感谢了在通用目标检测及其他相关领域的开创性研究者。作者们还要衷心感谢副编辑Jiří Matas教授以及匿名审稿人对他们的意见和建议。这项工作得到了芬兰奥卢大学机器视觉与信号分析中心（Center
    for Machine Vision and Signal Analysis）的支持，以及中国国家自然科学基金资助（项目编号：61872379）。
- en: References
  id: totrans-565
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Agrawal et al. (2014) Agrawal P., Girshick R., Malik J. (2014) Analyzing the
    performance of multilayer neural networks for object recognition. In: ECCV, pp.
    329–344'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agrawal 等人（2014）Agrawal P., Girshick R., Malik J.（2014）《分析用于目标识别的多层神经网络性能》。在：ECCV，pp.
    329–344
- en: 'Alexe et al. (2010) Alexe B., Deselaers T., Ferrari V. (2010) What is an object?
    In: CVPR, pp. 73–80'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexe 等人（2010）Alexe B., Deselaers T., Ferrari V.（2010）《什么是物体？》。在：CVPR，pp. 73–80
- en: Alexe et al. (2012) Alexe B., Deselaers T., Ferrari V. (2012) Measuring the
    objectness of image windows. IEEE TPAMI 34(11):2189–2202
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexe 等人（2012）Alexe B., Deselaers T., Ferrari V.（2012）《测量图像窗口的物体性》。IEEE TPAMI
    34(11):2189–2202
- en: 'Alvarez and Salzmann (2016) Alvarez J., Salzmann M. (2016) Learning the number
    of neurons in deep networks. In: NIPS, pp. 2270–2278'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alvarez 和 Salzmann（2016）Alvarez J., Salzmann M.（2016）《学习深度网络中的神经元数量》。在：NIPS，pp.
    2270–2278
- en: 'Andreopoulos and Tsotsos (2013) Andreopoulos A., Tsotsos J. (2013) 50 years
    of object recognition: Directions forward. Computer Vision and Image Understanding
    117(8):827–891'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andreopoulos and Tsotsos (2013) Andreopoulos A., Tsotsos J. (2013) 50年的目标识别：未来的方向。《计算机视觉与图像理解》117(8):827–891
- en: 'Arbeláez et al. (2012) Arbeláez P., Hariharan B., Gu C., Gupta S., Bourdev
    L., Malik J. (2012) Semantic segmentation using regions and parts. In: CVPR, pp.
    3378–3385'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arbeláez et al. (2012) Arbeláez P., Hariharan B., Gu C., Gupta S., Bourdev L.,
    Malik J. (2012) 使用区域和部件的语义分割。在：CVPR，第3378–3385页
- en: 'Arbeláez et al. (2014) Arbeláez P., Pont-Tuset J., Barron J., Marques F., Malik
    J. (2014) Multiscale combinatorial grouping. In: CVPR, pp. 328–335'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arbeláez et al. (2014) Arbeláez P., Pont-Tuset J., Barron J., Marques F., Malik
    J. (2014) 多尺度组合分组。在：CVPR，第328–335页
- en: Azizpour et al. (2016) Azizpour H., Razavian A., Sullivan J., Maki A., Carlsson
    S. (2016) Factors of transferability for a generic convnet representation. IEEE
    TPAMI 38(9):1790–1802
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azizpour et al. (2016) Azizpour H., Razavian A., Sullivan J., Maki A., Carlsson
    S. (2016) 通用卷积网络表示的迁移性因素。《IEEE TPAMI》38(9):1790–1802
- en: 'Bansal et al. (2018) Bansal A., Sikka K., Sharma G., Chellappa R., Divakaran
    A. (2018) Zero shot object detection. In: ECCV'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bansal et al. (2018) Bansal A., Sikka K., Sharma G., Chellappa R., Divakaran
    A. (2018) 零样本目标检测。在：ECCV
- en: Bar (2004) Bar M. (2004) Visual objects in context. Nature Reviews Neuroscience
    5(8):617–629
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bar (2004) Bar M. (2004) 语境中的视觉对象。《自然评论神经科学》5(8):617–629
- en: 'Bell et al. (2016) Bell S., Lawrence Z., Bala K., Girshick R. (2016) Inside
    Outside Net: Detecting objects in context with skip pooling and recurrent neural
    networks. In: CVPR, pp. 2874–2883'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bell et al. (2016) Bell S., Lawrence Z., Bala K., Girshick R. (2016) Inside
    Outside Net：通过跳跃池化和递归神经网络检测上下文中的目标。在：CVPR，第2874–2883页
- en: Belongie et al. (2002) Belongie S., Malik J., Puzicha J. (2002) Shape matching
    and object recognition using shape contexts. IEEE TPAMI 24(4):509–522
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belongie et al. (2002) Belongie S., Malik J., Puzicha J. (2002) 使用形状上下文进行形状匹配和目标识别。《IEEE
    TPAMI》24(4):509–522
- en: 'Bengio et al. (2013) Bengio Y., Courville A., Vincent P. (2013) Representation
    learning: A review and new perspectives. IEEE TPAMI 35(8):1798–1828'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio et al. (2013) Bengio Y., Courville A., Vincent P. (2013) 表示学习：回顾与新视角。《IEEE
    TPAMI》35(8):1798–1828
- en: Biederman (1972) Biederman I. (1972) Perceiving real world scenes. IJCV 177(7):77–80
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biederman (1972) Biederman I. (1972) 感知真实世界场景。《IJCV》177(7):77–80
- en: 'Biederman (1987a) Biederman I. (1987a) Recognition by components: a theory
    of human image understanding. Psychological review 94(2):115'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biederman (1987a) Biederman I. (1987a) 通过组件识别：人类图像理解理论。《心理学评论》94(2):115
- en: 'Biederman (1987b) Biederman I. (1987b) Recognition by components: a theory
    of human image understanding. Psychological review 94(2):115'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biederman (1987b) Biederman I. (1987b) 通过组件识别：人类图像理解理论。《心理学评论》94(2):115
- en: 'Bilen and Vedaldi (2016) Bilen H., Vedaldi A. (2016) Weakly supervised deep
    detection networks. In: CVPR, pp. 2846–2854'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bilen and Vedaldi (2016) Bilen H., Vedaldi A. (2016) 弱监督深度检测网络。在：CVPR，第2846–2854页
- en: 'Bodla et al. (2017) Bodla N., Singh B., Chellappa R., Davis L. S. (2017) SoftNMS
    improving object detection with one line of code. In: ICCV, pp. 5562–5570'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bodla et al. (2017) Bodla N., Singh B., Chellappa R., Davis L. S. (2017) SoftNMS：用一行代码改进目标检测。在：ICCV，第5562–5570页
- en: 'Borji et al. (2014) Borji A., Cheng M., Jiang H., Li J. (2014) Salient object
    detection: A survey. arXiv: 14115878v1 1:1–26'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Borji et al. (2014) Borji A., Cheng M., Jiang H., Li J. (2014) 显著目标检测：综述。arXiv:
    14115878v1 1:1–26'
- en: 'Bourdev and Brandt (2005) Bourdev L., Brandt J. (2005) Robust object detection
    via soft cascade. In: CVPR, vol 2, pp. 236–243'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bourdev and Brandt (2005) Bourdev L., Brandt J. (2005) 通过软级联的鲁棒目标检测。在：CVPR，第2卷，第236–243页
- en: Bruna and Mallat (2013) Bruna J., Mallat S. (2013) Invariant scattering convolution
    networks. IEEE TPAMI 35(8):1872–1886
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruna and Mallat (2013) Bruna J., Mallat S. (2013) 不变散射卷积网络。《IEEE TPAMI》35(8):1872–1886
- en: Cai et al. (2018) Cai H., Yang J., Zhang W., Han S., Yu Y. (2018) Path level
    network transformation for efficient architecture search
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. (2018) Cai H., Yang J., Zhang W., Han S., Yu Y. (2018) 路径级网络转换以进行高效架构搜索
- en: 'Cai and Vasconcelos (2018) Cai Z., Vasconcelos N. (2018) Cascade RCNN: Delving
    into high quality object detection. In: CVPR'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai and Vasconcelos (2018) Cai Z., Vasconcelos N. (2018) Cascade RCNN：深入探讨高质量目标检测。在：CVPR
- en: 'Cai et al. (2016) Cai Z., Fan Q., Feris R., Vasconcelos N. (2016) A unified
    multiscale deep convolutional neural network for fast object detection. In: ECCV,
    pp. 354–370'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. (2016) Cai Z., Fan Q., Feris R., Vasconcelos N. (2016) 用于快速目标检测的统一多尺度深度卷积神经网络。在：ECCV，第354–370页
- en: 'Carreira and Sminchisescu (2012) Carreira J., Sminchisescu C. (2012) CMPC:
    Automatic object segmentation using constrained parametric mincuts. IEEE TPAMI
    34(7):1312–1328'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carreira and Sminchisescu (2012) Carreira J., Sminchisescu C. (2012) CMPC：使用约束参数化最小割进行自动目标分割。《IEEE
    TPAMI》34(7):1312–1328
- en: 'Chatfield et al. (2014) Chatfield K., Simonyan K., Vedaldi A., Zisserman A.
    (2014) Return of the devil in the details: Delving deep into convolutional nets.
    In: BMVC'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chatfield 等（2014）Chatfield K.、Simonyan K.、Vedaldi A.、Zisserman A.（2014）细节中的恶魔回归：深入研究卷积网络。发表于：BMVC
- en: 'Chavali et al. (2016) Chavali N., Agrawal H., Mahendru A., Batra D. (2016)
    Object proposal evaluation protocol is gameable. In: CVPR, pp. 835–844'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chavali 等（2016）Chavali N.、Agrawal H.、Mahendru A.、Batra D.（2016）物体提议评估协议是可以被操控的。发表于：CVPR，第
    835–844 页
- en: Chellappa (2016) Chellappa R. (2016) The changing fortunes of pattern recognition
    and computer vision. Image and Vision Computing 55:3–5
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chellappa（2016）Chellappa R.（2016）模式识别和计算机视觉的变化命运。图像与视觉计算 55：3–5
- en: 'Chen et al. (2017a) Chen G., Choi W., Yu X., Han T., Chandraker M. (2017a)
    Learning efficient object detection models with knowledge distillation. In: NIPS'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2017a）陈 G.、Choi W.、Yu X.、Han T.、Chandraker M.（2017a）通过知识蒸馏学习高效的目标检测模型。发表于：NIPS
- en: 'Chen et al. (2018a) Chen H., Wang Y., Wang G., Qiao Y. (2018a) LSTD: A low
    shot transfer detector for object detection. In: AAAI'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2018a）陈 H.、Wang Y.、Wang G.、Qiao Y.（2018a）LSTD：用于目标检测的低样本迁移检测器。发表于：AAAI
- en: 'Chen et al. (2019a) Chen K., Pang J., Wang J., Xiong Y., Li X., Sun S., Feng
    W., Liu Z., Shi J., Ouyang W., et al. (2019a) Hybrid task cascade for instance
    segmentation. In: CVPR'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019a）陈 K.、Pang J.、Wang J.、Xiong Y.、Li X.、Sun S.、Feng W.、Liu Z.、Shi J.、Ouyang
    W. 等（2019a）用于实例分割的混合任务级联。发表于：CVPR
- en: 'Chen et al. (2015a) Chen L., Papandreou G., Kokkinos I., Murphy K., Yuille
    A. (2015a) Semantic image segmentation with deep convolutional nets and fully
    connected CRFs. In: ICLR'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2015a）陈 L.、Papandreou G.、Kokkinos I.、Murphy K.、Yuille A.（2015a）使用深度卷积网络和全连接
    CRF 的语义图像分割。发表于：ICLR
- en: 'Chen et al. (2018b) Chen L., Papandreou G., Kokkinos I., Murphy K., Yuille
    A. (2018b) DeepLab: Semantic image segmentation with deep convolutional nets,
    atrous convolution, and fully connected CRFs. IEEE TPAMI 40(4):834–848'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2018b）陈 L.、Papandreou G.、Kokkinos I.、Murphy K.、Yuille A.（2018b）DeepLab：使用深度卷积网络、空洞卷积和全连接
    CRF 的语义图像分割。IEEE TPAMI 40(4)：834–848
- en: Chen et al. (2015b) Chen Q., Song Z., Dong J., Huang Z., Hua Y., Yan S. (2015b)
    Contextualizing object detection and classification. IEEE TPAMI 37(1):13–27
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2015b）陈 Q.、Song Z.、Dong J.、Huang Z.、Hua Y.、Yan S.（2015b）上下文化的目标检测与分类。IEEE
    TPAMI 37(1)：13–27
- en: 'Chen and Gupta (2017) Chen X., Gupta A. (2017) Spatial memory for context reasoning
    in object detection. In: ICCV'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈和 Gupta（2017）陈 X.、Gupta A.（2017）用于目标检测的空间记忆和上下文推理。发表于：ICCV
- en: 'Chen et al. (2015c) Chen X., Kundu K., Zhu Y., Berneshawi A. G., Ma H., Fidler
    S., Urtasun R. (2015c) 3d object proposals for accurate object class detection.
    In: NIPS, pp. 424–432'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2015c）陈 X.、Kundu K.、Zhu Y.、Berneshawi A. G.、Ma H.、Fidler S.、Urtasun R.（2015c）用于准确的物体类别检测的
    3d 物体提议。发表于：NIPS，第 424–432 页
- en: 'Chen et al. (2017b) Chen Y., Li J., Xiao H., Jin X., Yan S., Feng J. (2017b)
    Dual path networks. In: NIPS, pp. 4467–4475'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2017b）陈 Y.、Li J.、Xiao H.、Jin X.、Yan S.、Feng J.（2017b）双路径网络。发表于：NIPS，第 4467–4475
    页
- en: 'Chen et al. (2019b) Chen Y., Rohrbach M., Yan Z., Yan S., Feng J., Kalantidis
    Y. (2019b) Graph based global reasoning networks. In: CVPR'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019b）陈 Y.、Rohrbach M.、Yan Z.、Yan S.、Feng J.、Kalantidis Y.（2019b）基于图的全局推理网络。发表于：CVPR
- en: 'Chen et al. (2019c) Chen Y., Yang T., Zhang X., Meng G., Pan C., Sun J. (2019c)
    DetNAS: Neural architecture search on object detection. arXiv:190310979'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019c）陈 Y.、Yang T.、Zhang X.、Meng G.、Pan C.、Sun J.（2019c）DetNAS：针对目标检测的神经网络架构搜索。arXiv:190310979
- en: 'Cheng et al. (2018a) Cheng B., Wei Y., Shi H., Feris R., Xiong J., Huang T.
    (2018a) Decoupled classification refinement: Hard false positive suppression for
    object detection. arXiv:181004002'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程等（2018a）程 B.、Wei Y.、Shi H.、Feris R.、Xiong J.、Huang T.（2018a）解耦分类细化：用于目标检测的困难假阳性抑制。arXiv:181004002
- en: 'Cheng et al. (2018b) Cheng B., Wei Y., Shi H., Feris R., Xiong J., Huang T.
    (2018b) Revisiting RCNN: on awakening the classification power of faster RCNN.
    In: ECCV'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程等（2018b）程 B.、Wei Y.、Shi H.、Feris R.、Xiong J.、Huang T.（2018b）重访 RCNN：唤醒 Faster
    RCNN 的分类能力。发表于：ECCV
- en: 'Cheng et al. (2016) Cheng G., Zhou P., Han J. (2016) RIFDCNN: Rotation invariant
    and fisher discriminative convolutional neural networks for object detection.
    In: CVPR, pp. 2884–2893'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程等（2016）程 G.、Zhou P.、Han J.（2016）RIFDCNN：用于目标检测的旋转不变和 Fisher 判别卷积神经网络。发表于：CVPR，第
    2884–2893 页
- en: 'Cheng et al. (2014) Cheng M., Zhang Z., Lin W., Torr P. (2014) BING: Binarized
    normed gradients for objectness estimation at 300fps. In: CVPR, pp. 3286–3293'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程等（2014）程 M.、Zhang Z.、Lin W.、Torr P.（2014）BING：用于物体性估计的二值化归一化梯度，300fps。发表于：CVPR，第
    3286–3293 页
- en: 'Cheng et al. (2018c) Cheng Y., Wang D., Zhou P., Zhang T. (2018c) Model compression
    and acceleration for deep neural networks: The principles, progress, and challenges.
    IEEE Signal Processing Magazine 35(1):126–136'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等 (2018c) Cheng Y., Wang D., Zhou P., Zhang T. (2018c) 深度神经网络的模型压缩与加速：原则、进展与挑战。IEEE
    信号处理杂志 35(1):126–136
- en: 'Chollet (2017) Chollet F. (2017) Xception: Deep learning with depthwise separable
    convolutions. In: CVPR, pp. 1800–1807'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet (2017) Chollet F. (2017) Xception：深度学习与深度可分离卷积。在：CVPR，第 1800–1807 页
- en: Cinbis et al. (2017) Cinbis R., Verbeek J., Schmid C. (2017) Weakly supervised
    object localization with multi-fold multiple instance learning. IEEE TPAMI 39(1):189–203
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cinbis 等 (2017) Cinbis R., Verbeek J., Schmid C. (2017) 具有多重实例学习的弱监督目标定位。IEEE
    TPAMI 39(1):189–203
- en: 'Csurka et al. (2004) Csurka G., Dance C., Fan L., Willamowski J., Bray C. (2004)
    Visual categorization with bags of keypoints. In: ECCV Workshop on statistical
    learning in computer vision'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Csurka 等 (2004) Csurka G., Dance C., Fan L., Willamowski J., Bray C. (2004)
    用于视觉分类的关键点袋。在：ECCV 统计学习研讨会
- en: 'Dai et al. (2016a) Dai J., He K., Li Y., Ren S., Sun J. (2016a) Instance sensitive
    fully convolutional networks. In: ECCV, pp. 534–549'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等 (2016a) Dai J., He K., Li Y., Ren S., Sun J. (2016a) 实例敏感全卷积网络。在：ECCV，第
    534–549 页
- en: 'Dai et al. (2016b) Dai J., He K., Sun J. (2016b) Instance aware semantic segmentation
    via multitask network cascades. In: CVPR, pp. 3150–3158'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等 (2016b) Dai J., He K., Sun J. (2016b) 通过多任务网络级联进行实例感知语义分割。在：CVPR，第 3150–3158
    页
- en: 'Dai et al. (2016c) Dai J., Li Y., He K., Sun J. (2016c) RFCN: object detection
    via region based fully convolutional networks. In: NIPS, pp. 379–387'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等 (2016c) Dai J., Li Y., He K., Sun J. (2016c) RFCN：基于区域的全卷积网络进行目标检测。在：NIPS，第
    379–387 页
- en: 'Dai et al. (2017) Dai J., Qi H., Xiong Y., Li Y., Zhang G., Hu H., Wei Y. (2017)
    Deformable convolutional networks. In: ICCV'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等 (2017) Dai J., Qi H., Xiong Y., Li Y., Zhang G., Hu H., Wei Y. (2017)
    形变卷积网络。在：ICCV
- en: 'Dalal and Triggs (2005) Dalal N., Triggs B. (2005) Histograms of oriented gradients
    for human detection. In: CVPR, vol 1, pp. 886–893'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalal 和 Triggs (2005) Dalal N., Triggs B. (2005) 用于人体检测的梯度方向直方图。在：CVPR，卷 1，第
    886–893 页
- en: 'Demirel et al. (2018) Demirel B., Cinbis R. G., Ikizler-Cinbis N. (2018) Zero
    shot object detection by hybrid region embedding. In: BMVC'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Demirel 等 (2018) Demirel B., Cinbis R. G., Ikizler-Cinbis N. (2018) 通过混合区域嵌入进行零样本目标检测。在：BMVC
- en: 'Deng et al. (2009) Deng J., Dong W., Socher R., Li L., Li K., Li F. (2009)
    ImageNet: A large scale hierarchical image database. In: CVPR, pp. 248–255'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2009) Deng J., Dong W., Socher R., Li L., Li K., Li F. (2009) ImageNet：大规模层次图像数据库。在：CVPR，第
    248–255 页
- en: 'Diba et al. (2017) Diba A., Sharma V., Pazandeh A. M., Pirsiavash H., Van Gool
    L. (2017) Weakly supervised cascaded convolutional networks. In: CVPR, vol 3,
    p. 9'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diba 等 (2017) Diba A., Sharma V., Pazandeh A. M., Pirsiavash H., Van Gool L.
    (2017) 弱监督级联卷积网络。在：CVPR，卷 3，第 9 页
- en: 'Dickinson et al. (2009) Dickinson S., Leonardis A., Schiele B., Tarr M. (2009)
    The Evolution of Object Categorization and the Challenge of Image Abstraction
    in *Object Categorization: Computer and Human Vision Perspectives*. Cambridge
    University Press'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dickinson 等 (2009) Dickinson S., Leonardis A., Schiele B., Tarr M. (2009) 目标分类的发展及*目标分类：计算机与人类视觉视角*中的图像抽象挑战。剑桥大学出版社
- en: 'Ding et al. (2018) Ding J., Xue N., Long Y., Xia G., Lu Q. (2018) Learning
    RoI transformer for detecting oriented objects in aerial images. In: CVPR'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等 (2018) Ding J., Xue N., Long Y., Xia G., Lu Q. (2018) 学习 RoI 转换器以检测航拍图像中的方向目标。在：CVPR
- en: 'Divvala et al. (2009) Divvala S., Hoiem D., Hays J., Efros A., Hebert M. (2009)
    An empirical study of context in object detection. In: CVPR, pp. 1271–1278'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Divvala 等 (2009) Divvala S., Hoiem D., Hays J., Efros A., Hebert M. (2009) 对目标检测中上下文的实证研究。在：CVPR，第
    1271–1278 页
- en: 'Dollar et al. (2012) Dollar P., Wojek C., Schiele B., Perona P. (2012) Pedestrian
    detection: An evaluation of the state of the art. IEEE TPAMI 34(4):743–761'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dollar 等 (2012) Dollar P., Wojek C., Schiele B., Perona P. (2012) 行人检测：对现有技术的评估。IEEE
    TPAMI 34(4):743–761
- en: 'Donahue et al. (2014) Donahue J., Jia Y., Vinyals O., Hoffman J., Zhang N.,
    Tzeng E., Darrell T. (2014) DeCAF: A deep convolutional activation feature for
    generic visual recognition. In: ICML, vol 32, pp. 647–655'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Donahue 等 (2014) Donahue J., Jia Y., Vinyals O., Hoffman J., Zhang N., Tzeng
    E., Darrell T. (2014) DeCAF：一种用于通用视觉识别的深度卷积激活特征。在：ICML，卷 32，第 647–655 页
- en: Dong et al. (2018) Dong X., Zheng L., Ma F., Yang Y., Meng D. (2018) Few example
    object detection with model communication. IEEE TPAMI
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等 (2018) Dong X., Zheng L., Ma F., Yang Y., Meng D. (2018) 使用模型通信的少样本目标检测。IEEE
    TPAMI
- en: 'Duan et al. (2019) Duan K., Bai S., Xie L., Qi H., Huang Q., Tian Q. (2019)
    CenterNet: Keypoint triplets for object detection. arXiv preprint arXiv:190408189'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等（2019）Duan K., Bai S., Xie L., Qi H., Huang Q., Tian Q. (2019) CenterNet：用于对象检测的关键点三元组。arXiv
    预印本 arXiv:190408189
- en: 'Dvornik et al. (2018) Dvornik N., Mairal J., Schmid C. (2018) Modeling visual
    context is key to augmenting object detection datasets. In: ECCV, pp. 364–380'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dvornik 等（2018）Dvornik N., Mairal J., Schmid C. (2018) 建模视觉上下文是增强对象检测数据集的关键。载于：ECCV，第
    364–380 页
- en: 'Dwibedi et al. (2017) Dwibedi D., Misra I., Hebert M. (2017) Cut, paste and
    learn: Surprisingly easy synthesis for instance detection. In: ICCV, pp. 1301–1310'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dwibedi 等（2017）Dwibedi D., Misra I., Hebert M. (2017) 剪切、粘贴和学习：令人惊讶的简单实例检测合成方法。载于：ICCV，第
    1301–1310 页
- en: Endres and Hoiem (2010) Endres I., Hoiem D. (2010) Category independent object
    proposals
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Endres 和 Hoiem（2010）Endres I., Hoiem D. (2010) 类别无关的对象提议
- en: 'Enzweiler and Gavrila (2009) Enzweiler M., Gavrila D. M. (2009) Monocular pedestrian
    detection: Survey and experiments. IEEE TPAMI 31(12):2179–2195'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Enzweiler 和 Gavrila（2009）Enzweiler M., Gavrila D. M. (2009) 单目行人检测：调查和实验。IEEE
    TPAMI 31(12):2179–2195
- en: 'Erhan et al. (2014) Erhan D., Szegedy C., Toshev A., Anguelov D. (2014) Scalable
    object detection using deep neural networks. In: CVPR, pp. 2147–2154'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erhan 等（2014）Erhan D., Szegedy C., Toshev A., Anguelov D. (2014) 使用深度神经网络进行可扩展的对象检测。载于：CVPR，第
    2147–2154 页
- en: Everingham et al. (2010) Everingham M., Gool L. V., Williams C., Winn J., Zisserman
    A. (2010) The pascal visual object classes (voc) challenge. IJCV 88(2):303–338
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Everingham 等（2010）Everingham M., Gool L. V., Williams C., Winn J., Zisserman
    A. (2010) Pascal 视觉对象类别（VOC）挑战。IJCV 88(2):303–338
- en: 'Everingham et al. (2015) Everingham M., Eslami S., Gool L. V., Williams C.,
    Winn J., Zisserman A. (2015) The pascal visual object classes challenge: A retrospective.
    IJCV 111(1):98–136'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Everingham 等（2015）Everingham M., Eslami S., Gool L. V., Williams C., Winn J.,
    Zisserman A. (2015) Pascal 视觉对象类别挑战：回顾。IJCV 111(1):98–136
- en: 'Feichtenhofer et al. (2017) Feichtenhofer C., Pinz A., Zisserman A. (2017)
    Detect to track and track to detect. In: ICCV, pp. 918–927'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feichtenhofer 等（2017）Feichtenhofer C., Pinz A., Zisserman A. (2017) 检测跟踪和跟踪检测。载于：ICCV，第
    918–927 页
- en: FeiFei et al. (2006) FeiFei L., Fergus R., Perona P. (2006) One shot learning
    of object categories. IEEE TPAMI 28(4):594–611
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FeiFei 等（2006）FeiFei L., Fergus R., Perona P. (2006) 对象类别的一次性学习。IEEE TPAMI 28(4):594–611
- en: 'Felzenszwalb et al. (2008) Felzenszwalb P., McAllester D., Ramanan D. (2008)
    A discriminatively trained, multiscale, deformable part model. In: CVPR, pp. 1–8'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Felzenszwalb 等（2008）Felzenszwalb P., McAllester D., Ramanan D. (2008) 一种具有多尺度、可变形部件模型的判别训练方法。载于：CVPR，第
    1–8 页
- en: 'Felzenszwalb et al. (2010a) Felzenszwalb P., Girshick R., McAllester D. (2010a)
    Cascade object detection with deformable part models. In: CVPR, pp. 2241–2248'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Felzenszwalb 等（2010a）Felzenszwalb P., Girshick R., McAllester D. (2010a) 具有可变形部件模型的级联对象检测。载于：CVPR，第
    2241–2248 页
- en: Felzenszwalb et al. (2010b) Felzenszwalb P., Girshick R., McAllester D., Ramanan
    D. (2010b) Object detection with discriminatively trained part based models. IEEE
    TPAMI 32(9):1627–1645
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Felzenszwalb 等（2010b）Felzenszwalb P., Girshick R., McAllester D., Ramanan D.
    (2010b) 使用判别训练的基于部件的模型进行对象检测。IEEE TPAMI 32(9):1627–1645
- en: 'Finn et al. (2017) Finn C., Abbeel P., Levine S. (2017) Model agnostic meta
    learning for fast adaptation of deep networks. In: ICML, pp. 1126–1135'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn 等（2017）Finn C., Abbeel P., Levine S. (2017) 模型无关的元学习用于深度网络的快速适应。载于：ICML，第
    1126–1135 页
- en: Fischler and Elschlager (1973) Fischler M., Elschlager R. (1973) The representation
    and matching of pictorial structures. IEEE Transactions on computers 100(1):67–92
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fischler 和 Elschlager（1973）Fischler M., Elschlager R. (1973) 图像结构的表示和匹配。IEEE
    计算机学报 100(1):67–92
- en: 'Fu et al. (2017) Fu C.-Y., Liu W., Ranga A., Tyagi A., Berg A. C. (2017) DSSD:
    Deconvolutional single shot detector. In: arXiv preprint arXiv:1701.06659'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2017）Fu C.-Y., Liu W., Ranga A., Tyagi A., Berg A. C. (2017) DSSD：去卷积单次检测器。载于：arXiv
    预印本 arXiv:1701.06659
- en: 'Galleguillos and Belongie (2010) Galleguillos C., Belongie S. (2010) Context
    based object categorization: A critical survey. Computer Vision and Image Understanding
    114:712–722'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galleguillos 和 Belongie（2010）Galleguillos C., Belongie S. (2010) 基于上下文的对象分类：一项关键调查。计算机视觉与图像理解
    114:712–722
- en: Geronimo et al. (2010) Geronimo D., Lopez A. M., Sappa A. D., Graf T. (2010)
    Survey of pedestrian detection for advanced driver assistance systems. IEEE TPAMI
    32(7):1239–1258
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geronimo 等（2010）Geronimo D., Lopez A. M., Sappa A. D., Graf T. (2010) 高级驾驶辅助系统中的行人检测调查。IEEE
    TPAMI 32(7):1239–1258
- en: 'Ghiasi et al. (2019) Ghiasi G., Lin T., Pang R., Le Q. (2019) NASFPN: learning
    scalable feature pyramid architecture for object detection. arXiv:190407392'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghiasi 等（2019）Ghiasi G., Lin T., Pang R., Le Q. (2019) NASFPN：学习可扩展特征金字塔架构用于对象检测。arXiv:190407392
- en: 'Ghodrati et al. (2015) Ghodrati A., Diba A., Pedersoli M., Tuytelaars T., Van
    Gool L. (2015) DeepProposal: Hunting objects by cascading deep convolutional layers.
    In: ICCV, pp. 2578–2586'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghodrati 等人（2015年）Ghodrati A., Diba A., Pedersoli M., Tuytelaars T., Van Gool
    L.（2015年）DeepProposal：通过级联深度卷积层寻找对象。在：ICCV，第2578–2586页
- en: 'Gidaris and Komodakis (2015) Gidaris S., Komodakis N. (2015) Object detection
    via a multiregion and semantic segmentation aware CNN model. In: ICCV, pp. 1134–1142'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gidaris 和 Komodakis（2015年）Gidaris S., Komodakis N.（2015年）通过多区域和语义分割感知CNN模型进行对象检测。在：ICCV，第1134–1142页
- en: 'Gidaris and Komodakis (2016) Gidaris S., Komodakis N. (2016) Attend refine
    repeat: Active box proposal generation via in out localization. In: BMVC'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gidaris 和 Komodakis（2016年）Gidaris S., Komodakis N.（2016年）通过进出定位进行主动边界框提议生成。在：BMVC
- en: 'Girshick (2015) Girshick R. (2015) Fast R-CNN. In: ICCV, pp. 1440–1448'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girshick（2015年）Girshick R.（2015年）快速R-CNN。在：ICCV，第1440–1448页
- en: 'Girshick et al. (2014) Girshick R., Donahue J., Darrell T., Malik J. (2014)
    Rich feature hierarchies for accurate object detection and semantic segmentation.
    In: CVPR, pp. 580–587'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girshick 等人（2014年）Girshick R., Donahue J., Darrell T., Malik J.（2014年）用于精确目标检测和语义分割的丰富特征层次结构。在：CVPR，第580–587页
- en: 'Girshick et al. (2015) Girshick R., Iandola F., Darrell T., Malik J. (2015)
    Deformable part models are convolutional neural networks. In: CVPR, pp. 437–446'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girshick 等人（2015年）Girshick R., Iandola F., Darrell T., Malik J.（2015年）可变形部件模型是卷积神经网络。在：CVPR，第437–446页
- en: Girshick et al. (2016) Girshick R., Donahue J., Darrell T., Malik J. (2016)
    Region-based convolutional networks for accurate object detection and segmentation.
    IEEE TPAMI 38(1):142–158
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girshick 等人（2016年）Girshick R., Donahue J., Darrell T., Malik J.（2016年）基于区域的卷积网络用于精确目标检测和分割。《IEEE
    TPAMI》38卷1期：142–158页
- en: 'Goodfellow et al. (2015) Goodfellow I., Shlens J., Szegedy C. (2015) Explaining
    and harnessing adversarial examples. In: ICLR'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人（2015年）Goodfellow I., Shlens J., Szegedy C.（2015年）解释和利用对抗样本。在：ICLR
- en: Goodfellow et al. (2016) Goodfellow I., Bengio Y., Courville A. (2016) Deep
    Learning. MIT press
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人（2016年）Goodfellow I., Bengio Y., Courville A.（2016年）深度学习。麻省理工学院出版社
- en: 'Grauman and Darrell (2005) Grauman K., Darrell T. (2005) The pyramid match
    kernel: Discriminative classification with sets of image features. In: ICCV, vol 2,
    pp. 1458–1465'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grauman 和 Darrell（2005年）Grauman K., Darrell T.（2005年）金字塔匹配核：使用图像特征集进行辨别分类。在：ICCV，第2卷，第1458–1465页
- en: Grauman and Leibe (2011) Grauman K., Leibe B. (2011) Visual object recognition.
    Synthesis lectures on artificial intelligence and machine learning 5(2):1–181
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grauman 和 Leibe（2011年）Grauman K., Leibe B.（2011年）视觉对象识别。《人工智能和机器学习综合讲座》5卷2期：1–181页
- en: Gu et al. (2017) Gu J., Wang Z., Kuen J., Ma L., Shahroudy A., Shuai B., Liu
    T., Wang X., Wang G., Cai J., Chen T. (2017) Recent advances in convolutional
    neural networks. Pattern Recognition pp. 1–24
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人（2017年）Gu J., Wang Z., Kuen J., Ma L., Shahroudy A., Shuai B., Liu T.,
    Wang X., Wang G., Cai J., Chen T.（2017年）卷积神经网络的最新进展。《模式识别》第1–24页
- en: Guillaumin et al. (2014) Guillaumin M., Küttel D., Ferrari V. (2014) Imagenet
    autoannotation with segmentation propagation. International Journal of Computer
    Vision 110(3):328–348
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guillaumin 等人（2014年）Guillaumin M., Küttel D., Ferrari V.（2014年）通过分割传播进行Imagenet自动标注。《国际计算机视觉期刊》110卷3期：328–348页
- en: 'Gupta et al. (2016) Gupta A., Vedaldi A., Zisserman A. (2016) Synthetic data
    for text localisation in natural images. In: CVPR, pp. 2315–2324'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等人（2016年）Gupta A., Vedaldi A., Zisserman A.（2016年）合成数据用于自然图像中的文本定位。在：CVPR，第2315–2324页
- en: 'Hariharan and Girshick (2017) Hariharan B., Girshick R. B. (2017) Low shot
    visual recognition by shrinking and hallucinating features. In: ICCV, pp. 3037–3046'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hariharan 和 Girshick（2017年）Hariharan B., Girshick R. B.（2017年）通过收缩和虚构特征进行低射击视觉识别。在：ICCV，第3037–3046页
- en: 'Hariharan et al. (2014) Hariharan B., Arbeláez P., Girshick R., Malik J. (2014)
    Simultaneous detection and segmentation. In: ECCV, pp. 297–312'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hariharan 等人（2014年）Hariharan B., Arbeláez P., Girshick R., Malik J.（2014年）同时检测和分割。在：ECCV，第297–312页
- en: Hariharan et al. (2016) Hariharan B., Arbeláez P., Girshick R., Malik J. (2016)
    Object instance segmentation and fine grained localization using hypercolumns.
    IEEE TPAMI
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hariharan 等人（2016年）Hariharan B., Arbeláez P., Girshick R., Malik J.（2016年）通过超列进行对象实例分割和细粒度定位。《IEEE
    TPAMI》
- en: 'Harzallah et al. (2009) Harzallah H., Jurie F., Schmid C. (2009) Combining
    efficient object localization and image classification. In: ICCV, pp. 237–244'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harzallah 等人（2009年）Harzallah H., Jurie F., Schmid C.（2009年）结合高效的对象定位和图像分类。在：ICCV，第237–244页
- en: 'He et al. (2014) He K., Zhang X., Ren S., Sun J. (2014) Spatial pyramid pooling
    in deep convolutional networks for visual recognition. In: ECCV, pp. 346–361'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2014年）He K., Zhang X., Ren S., Sun J.（2014年）深度卷积网络中的空间金字塔池化用于视觉识别。在：ECCV，第346–361页
- en: 'He et al. (2015) He K., Zhang X., Ren S., Sun J. (2015) Delving deep into rectifiers:
    Surpassing human-level performance on ImageNet classification. In: ICCV, pp. 1026–1034'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2015）He K.、Zhang X.、Ren S.、Sun J.（2015）《深入研究整流器：超越ImageNet分类的人类水平表现》。收录于：ICCV，第1026–1034页
- en: 'He et al. (2016) He K., Zhang X., Ren S., Sun J. (2016) Deep residual learning
    for image recognition. In: CVPR, pp. 770–778'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2016）He K.、Zhang X.、Ren S.、Sun J.（2016）《用于图像识别的深度残差学习》。收录于：CVPR，第770–778页
- en: 'He et al. (2017) He K., Gkioxari G., Dollár P., Girshick R. (2017) Mask RCNN.
    In: ICCV'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2017）He K.、Gkioxari G.、Dollár P.、Girshick R.（2017）《Mask RCNN》。收录于：ICCV
- en: 'He et al. (2018) He T., Tian Z., Huang W., Shen C., Qiao Y., Sun C. (2018)
    An end to end textspotter with explicit alignment and attention. In: CVPR, pp.
    5020–5029'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2018）He T.、Tian Z.、Huang W.、Shen C.、Qiao Y.、Sun C.（2018）《一个端到端的文本探测器，具有明确的对齐和注意力》。收录于：CVPR，第5020–5029页
- en: 'He et al. (2019) He Y., Zhu C., Wang J., Savvides M., Zhang X. (2019) Bounding
    box regression with uncertainty for accurate object detection. In: CVPR'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2019）He Y.、Zhu C.、Wang J.、Savvides M.、Zhang X.（2019）《带有不确定性的边界框回归以实现准确的对象检测》。收录于：CVPR
- en: Hinton and Salakhutdinov (2006) Hinton G., Salakhutdinov R. (2006) Reducing
    the dimensionality of data with neural networks. science 313(5786):504–507
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 和 Salakhutdinov（2006）Hinton G.、Salakhutdinov R.（2006）《利用神经网络降低数据的维度》。science
    313(5786)：504–507
- en: Hinton et al. (2015) Hinton G., Vinyals O., Dean J. (2015) Distilling the knowledge
    in a neural network. arXiv:150302531
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等人（2015）Hinton G.、Vinyals O.、Dean J.（2015）《提炼神经网络中的知识》。arXiv:150302531
- en: 'Hoffman et al. (2014) Hoffman J., Guadarrama S., Tzeng E. S., Hu R., Donahue
    J., Girshick R., Darrell T., Saenko K. (2014) LSDA: large scale detection through
    adaptation. In: NIPS, pp. 3536–3544'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffman 等人（2014）Hoffman J.、Guadarrama S.、Tzeng E. S.、Hu R.、Donahue J.、Girshick
    R.、Darrell T.、Saenko K.（2014）《LSDA：通过适应实现的大规模检测》。收录于：NIPS，第3536–3544页
- en: 'Hoiem et al. (2012) Hoiem D., Chodpathumwan Y., Dai Q. (2012) Diagnosing error
    in object detectors. In: ECCV, pp. 340–353'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoiem 等人（2012）Hoiem D.、Chodpathumwan Y.、Dai Q.（2012）《诊断对象检测器中的错误》。收录于：ECCV，第340–353页
- en: 'Hosang et al. (2015) Hosang J., Omran M., Benenson R., Schiele B. (2015) Taking
    a deeper look at pedestrians. In: Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 4073–4082'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosang 等人（2015）Hosang J.、Omran M.、Benenson R.、Schiele B.（2015）《更深入地观察行人》。收录于：IEEE计算机视觉与模式识别会议论文集，第4073–4082页
- en: Hosang et al. (2016) Hosang J., Benenson R., Doll r P., Schiele B. (2016) What
    makes for effective detection proposals? IEEE TPAMI 38(4):814–829
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosang 等人（2016）Hosang J.、Benenson R.、Dollár P.、Schiele B.（2016）《什么使得检测提案有效？》IEEE
    TPAMI 38(4)：814–829
- en: 'Hosang et al. (2017) Hosang J., Benenson R., Schiele B. (2017) Learning nonmaximum
    suppression. In: ICCV'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosang 等人（2017）Hosang J.、Benenson R.、Schiele B.（2017）《学习非极大值抑制》。收录于：ICCV
- en: 'Howard et al. (2017) Howard A., Zhu M., Chen B., Kalenichenko D., Wang W.,
    Weyand T., Andreetto M., Adam H. (2017) Mobilenets: Efficient convolutional neural
    networks for mobile vision applications. In: CVPR'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard 等人（2017）Howard A.、Zhu M.、Chen B.、Kalenichenko D.、Wang W.、Weyand T.、Andreetto
    M.、Adam H.（2017）《Mobilenets：用于移动视觉应用的高效卷积神经网络》。收录于：CVPR
- en: 'Hu et al. (2017) Hu H., Lan S., Jiang Y., Cao Z., Sha F. (2017) FastMask: Segment
    multiscale object candidates in one shot. In: CVPR, pp. 991–999'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2017）Hu H.、Lan S.、Jiang Y.、Cao Z.、Sha F.（2017）《FastMask：一次性分割多尺度对象候选》。收录于：CVPR，第991–999页
- en: 'Hu et al. (2018a) Hu H., Gu J., Zhang Z., Dai J., Wei Y. (2018a) Relation networks
    for object detection. In: CVPR'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2018a）Hu H.、Gu J.、Zhang Z.、Dai J.、Wei Y.（2018a）《用于对象检测的关系网络》。收录于：CVPR
- en: 'Hu et al. (2018b) Hu J., Shen L., Sun G. (2018b) Squeeze and excitation networks.
    In: CVPR'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2018b）Hu J.、Shen L.、Sun G.（2018b）《挤压与激励网络》。收录于：CVPR
- en: 'Hu and Ramanan (2017) Hu P., Ramanan D. (2017) Finding tiny faces. In: CVPR,
    pp. 1522–1530'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 和 Ramanan（2017）Hu P.、Ramanan D.（2017）《寻找微小的人脸》。收录于：CVPR，第1522–1530页
- en: 'Hu et al. (2018c) Hu R., Dollár P., He K., Darrell T., Girshick R. (2018c)
    Learning to segment every thing. In: CVPR'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2018c）Hu R.、Dollár P.、He K.、Darrell T.、Girshick R.（2018c）《学习分割所有对象》。收录于：CVPR
- en: 'Huang et al. (2017a) Huang G., Liu Z., Weinberger K. Q., van der Maaten L.
    (2017a) Densely connected convolutional networks. In: CVPR'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2017a）Huang G.、Liu Z.、Weinberger K. Q.、van der Maaten L.（2017a）《密集连接卷积网络》。收录于：CVPR
- en: 'Huang et al. (2018) Huang G., Liu S., van der Maaten L., Weinberger K. (2018)
    CondenseNet: An efficient densenet using learned group convolutions. In: CVPR'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2018）Huang G.、Liu S.、van der Maaten L.、Weinberger K.（2018）《CondenseNet：一种使用学习组卷积的高效DenseNet》。收录于：CVPR
- en: 'Huang et al. (2017b) Huang J., Rathod V., Sun C., Zhu M., Korattikara A., Fathi
    A., Fischer I., Wojna Z., Song Y., Guadarrama S., Murphy K. (2017b) Speed/accuracy
    trade offs for modern convolutional object detectors. In: CVPR'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2017b) Huang J., Rathod V., Sun C., Zhu M., Korattikara A., Fathi A.,
    Fischer I., Wojna Z., Song Y., Guadarrama S., Murphy K. (2017b) 现代卷积目标检测器的速度/精度权衡。载于：CVPR
- en: 'Huang et al. (2019) Huang Z., Huang L., Gong Y., Huang C., Wang X. (2019) Mask
    scoring rcnn. In: CVPR'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2019) Huang Z., Huang L., Gong Y., Huang C., Wang X. (2019) Mask scoring
    rcnn。载于：CVPR
- en: 'Hubara et al. (2016) Hubara I., Courbariaux M., Soudry D., ElYaniv R., Bengio
    Y. (2016) Binarized neural networks. In: NIPS, pp. 4107–4115'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara 等 (2016) Hubara I., Courbariaux M., Soudry D., ElYaniv R., Bengio Y.
    (2016) 二值神经网络。载于：NIPS，第4107–4115页
- en: 'Iandola et al. (2016) Iandola F., Han S., Moskewicz M., Ashraf K., Dally W.,
    Keutzer K. (2016) SqueezeNet: Alexnet level accuracy with 50x fewer parameters
    and 0.5 mb model size. In: arXiv preprint arXiv:1602.07360'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Iandola 等 (2016) Iandola F., Han S., Moskewicz M., Ashraf K., Dally W., Keutzer
    K. (2016) SqueezeNet: 具有 50 倍更少参数和 0.5 mb 模型大小的 Alexnet 级准确度。载于：arXiv 预印本 arXiv:1602.07360'
- en: ILSVRC detection challenge results (2018) ILSVRC detection challenge results
    (2018) [http://www.image-net.org/challenges/LSVRC/](http://www.image-net.org/challenges/LSVRC/)
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ILSVRC 检测挑战结果 (2018) ILSVRC 检测挑战结果 (2018) [http://www.image-net.org/challenges/LSVRC/](http://www.image-net.org/challenges/LSVRC/)
- en: 'Ioffe and Szegedy (2015) Ioffe S., Szegedy C. (2015) Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. In: International
    Conference on Machine Learning, pp. 448–456'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe 和 Szegedy (2015) Ioffe S., Szegedy C. (2015) 批量归一化：通过减少内部协变量偏移来加速深度网络训练。载于：国际机器学习会议，第448–456页
- en: 'Jaderberg et al. (2015) Jaderberg M., Simonyan K., Zisserman A., et al. (2015)
    Spatial transformer networks. In: NIPS, pp. 2017–2025'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等 (2015) Jaderberg M., Simonyan K., Zisserman A., 等 (2015) 空间变换网络。载于：NIPS，第2017–2025页
- en: 'Jia et al. (2014) Jia Y., Shelhamer E., Donahue J., Karayev S., Long J., Girshick
    R., Guadarrama S., Darrell T. (2014) Caffe: Convolutional architecture for fast
    feature embedding. In: ACM MM, pp. 675–678'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jia 等 (2014) Jia Y., Shelhamer E., Donahue J., Karayev S., Long J., Girshick
    R., Guadarrama S., Darrell T. (2014) Caffe: 用于快速特征嵌入的卷积架构。载于：ACM MM，第675–678页'
- en: 'Jiang et al. (2018) Jiang B., Luo R., Mao J., Xiao T., Jiang Y. (2018) Acquisition
    of localization confidence for accurate object detection. In: ECCV, pp. 784–799'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2018) Jiang B., Luo R., Mao J., Xiao T., Jiang Y. (2018) 为准确目标检测获取定位置信度。载于：ECCV，第784–799页
- en: Kang et al. (2018) Kang B., Liu Z., Wang X., Yu F., Feng J., Darrell T. (2018)
    Few shot object detection via feature reweighting. arXiv preprint arXiv:181201866
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 等 (2018) Kang B., Liu Z., Wang X., Yu F., Feng J., Darrell T. (2018) 通过特征重标定的少样本目标检测。arXiv
    预印本 arXiv:181201866
- en: 'Kang et al. (2016) Kang K., Ouyang W., Li H., Wang X. (2016) Object detection
    from video tubelets with convolutional neural networks. In: CVPR, pp. 817–825'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 等 (2016) Kang K., Ouyang W., Li H., Wang X. (2016) 从视频管道中进行目标检测与卷积神经网络。载于：CVPR，第817–825页
- en: 'Kim et al. (2014) Kim A., Sharma A., Jacobs D. (2014) Locally scale invariant
    convolutional neural networks. In: NIPS'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2014) Kim A., Sharma A., Jacobs D. (2014) 局部尺度不变卷积神经网络。载于：NIPS
- en: 'Kim et al. (2016) Kim K., Hong S., Roh B., Cheon Y., Park M. (2016) PVANet:
    Deep but lightweight neural networks for real time object detection. In: NIPSW'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等 (2016) Kim K., Hong S., Roh B., Cheon Y., Park M. (2016) PVANet: 深度但轻量级的实时目标检测神经网络。载于：NIPSW'
- en: 'Kim et al. (2018) Kim Y., Kang B.-N., Kim D. (2018) SAN: learning relationship
    between convolutional features for multiscale object detection. In: ECCV, pp.
    316–331'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等 (2018) Kim Y., Kang B.-N., Kim D. (2018) SAN: 学习卷积特征之间的关系以进行多尺度目标检测。载于：ECCV，第316–331页'
- en: Kirillov et al. (2018) Kirillov A., He K., Girshick R., Rother C., Dollár P.
    (2018) Panoptic segmentation. arXiv:180100868
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirillov 等 (2018) Kirillov A., He K., Girshick R., Rother C., Dollár P. (2018)
    Panoptic segmentation. arXiv:180100868
- en: 'Kong et al. (2016) Kong T., Yao A., Chen Y., Sun F. (2016) HyperNet: towards
    accurate region proposal generation and joint object detection. In: CVPR, pp.
    845–853'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kong 等 (2016) Kong T., Yao A., Chen Y., Sun F. (2016) HyperNet: 迈向准确的区域提议生成与联合目标检测。载于：CVPR，第845–853页'
- en: 'Kong et al. (2017) Kong T., Sun F., Yao A., Liu H., Lu M., Chen Y. (2017) RON:
    Reverse connection with objectness prior networks for object detection. In: CVPR'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kong 等 (2017) Kong T., Sun F., Yao A., Liu H., Lu M., Chen Y. (2017) RON: 带有目标性先验网络的反向连接用于目标检测。载于：CVPR'
- en: 'Kong et al. (2018) Kong T., Sun F., Tan C., Liu H., Huang W. (2018) Deep feature
    pyramid reconfiguration for object detection. In: ECCV, pp. 169–185'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kong 等 (2018) Kong T., Sun F., Tan C., Liu H., Huang W. (2018) 深度特征金字塔重配置用于目标检测。载于：ECCV，第169–185页
- en: 'Krähenbühl1 and Koltun (2014) Krähenbühl1 P., Koltun V. (2014) Geodesic object
    proposals. In: ECCV'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krähenbühl1 和 Koltun（2014）**Krähenbühl1 P.**, **Koltun V.**（2014）《测地对象提议》。在：ECCV
- en: 'Krasin et al. (2017) Krasin I., Duerig T., Alldrin N., Ferrari V., AbuElHaija
    S., Kuznetsova A., Rom H., Uijlings J., Popov S., Kamali S., Malloci M., PontTuset
    J., Veit A., Belongie S., Gomes V., Gupta A., Sun C., Chechik G., Cai D., Feng
    Z., Narayanan D., Murphy K. (2017) OpenImages: A public dataset for large scale
    multilabel and multiclass image classification. Dataset available from https://storagegoogleapiscom/openimages/web/indexhtml'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krasin 等（2017）**Krasin I.**, **Duerig T.**, **Alldrin N.**, **Ferrari V.**,
    **AbuElHaija S.**, **Kuznetsova A.**, **Rom H.**, **Uijlings J.**, **Popov S.**,
    **Kamali S.**, **Malloci M.**, **PontTuset J.**, **Veit A.**, **Belongie S.**,
    **Gomes V.**, **Gupta A.**, **Sun C.**, **Chechik G.**, **Cai D.**, **Feng Z.**,
    **Narayanan D.**, **Murphy K.**（2017）《OpenImages：一个用于大规模多标签和多类图像分类的公共数据集》。数据集可从
    [https://storagegoogleapiscom/openimages/web/indexhtml](https://storagegoogleapiscom/openimages/web/indexhtml)
    获取
- en: 'Krizhevsky et al. (2012a) Krizhevsky A., Sutskever I., Hinton G. (2012a) ImageNet
    classification with deep convolutional neural networks. In: NIPS, pp. 1097–1105'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等（2012a）**Krizhevsky A.**, **Sutskever I.**, **Hinton G.**（2012a）《使用深度卷积神经网络进行
    ImageNet 分类》。在：NIPS，第 1097–1105 页
- en: 'Krizhevsky et al. (2012b) Krizhevsky A., Sutskever I., Hinton G. (2012b) ImageNet
    classification with deep convolutional neural networks. In: NIPS, pp. 1097–1105'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等（2012b）**Krizhevsky A.**, **Sutskever I.**, **Hinton G.**（2012b）《使用深度卷积神经网络进行
    ImageNet 分类》。在：NIPS，第 1097–1105 页
- en: 'Kuo et al. (2015) Kuo W., Hariharan B., Malik J. (2015) DeepBox: Learning objectness
    with convolutional networks. In: ICCV, pp. 2479–2487'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuo 等（2015）**Kuo W.**, **Hariharan B.**, **Malik J.**（2015）《DeepBox：使用卷积网络学习对象性》。在：ICCV，第
    2479–2487 页
- en: 'Kuznetsova et al. (2018) Kuznetsova A., Rom H., Alldrin N., Uijlings J., Krasin
    I., PontTuset J., Kamali S., Popov S., Malloci M., Duerig T., et al. (2018) The
    open images dataset v4: Unified image classification, object detection, and visual
    relationship detection at scale. arXiv preprint arXiv:181100982'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuznetsova 等（2018）**Kuznetsova A.**, **Rom H.**, **Alldrin N.**, **Uijlings
    J.**, **Krasin I.**, **PontTuset J.**, **Kamali S.**, **Popov S.**, **Malloci
    M.**, **Duerig T.**, 等（2018）《Open Images 数据集 v4：统一的图像分类、对象检测和视觉关系检测》。arXiv 预印本
    arXiv:181100982
- en: Lake et al. (2015) Lake B., Salakhutdinov R., Tenenbaum J. (2015) Human level
    concept learning through probabilistic program induction. Science 350(6266):1332–1338
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lake 等（2015）**Lake B.**, **Salakhutdinov R.**, **Tenenbaum J.**（2015）《通过概率程序归纳进行人类水平概念学习》。科学
    350(6266):1332–1338
- en: 'Lampert et al. (2008) Lampert C. H., Blaschko M. B., Hofmann T. (2008) Beyond
    sliding windows: Object localization by efficient subwindow search. In: CVPR,
    pp. 1–8'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lampert 等（2008）**Lampert C. H.**, **Blaschko M. B.**, **Hofmann T.**（2008）《超越滑动窗口：通过高效的子窗口搜索进行对象定位》。在：CVPR，第
    1–8 页
- en: 'Law and Deng (2018) Law H., Deng J. (2018) CornerNet: Detecting objects as
    paired keypoints. In: ECCV'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Law 和 Deng（2018）**Law H.**, **Deng J.**（2018）《CornerNet：将对象检测为配对关键点》。在：ECCV
- en: 'Lazebnik et al. (2006) Lazebnik S., Schmid C., Ponce J. (2006) Beyond bags
    of features: Spatial pyramid matching for recognizing natural scene categories.
    In: CVPR, vol 2, pp. 2169–2178'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lazebnik 等（2006）**Lazebnik S.**, **Schmid C.**, **Ponce J.**（2006）《超越特征袋：用于识别自然场景类别的空间金字塔匹配》。在：CVPR，第
    2 卷，第 2169–2178 页
- en: LeCun et al. (1998) LeCun Y., Bottou L., Bengio Y., Haffner P. (1998) Gradient
    based learning applied to document recognition. Proceedings of the IEEE 86(11):2278–2324
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等（1998）**LeCun Y.**, **Bottou L.**, **Bengio Y.**, **Haffner P.**（1998）《基于梯度的学习应用于文档识别》。IEEE
    会议录 86(11):2278–2324
- en: LeCun et al. (2015) LeCun Y., Bengio Y., Hinton G. (2015) Deep learning. Nature
    521:436–444
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等（2015）**LeCun Y.**, **Bengio Y.**, **Hinton G.**（2015）《深度学习》。自然 521:436–444
- en: 'Lee et al. (2015) Lee C., Xie S., Gallagher P., Zhang Z., Tu Z. (2015) Deeply
    supervised nets. In: Artificial Intelligence and Statistics, pp. 562–570'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2015）**Lee C.**, **Xie S.**, **Gallagher P.**, **Zhang Z.**, **Tu Z.**（2015）《深度监督网络》。在：人工智能与统计，第
    562–570 页
- en: 'Lenc and Vedaldi (2015) Lenc K., Vedaldi A. (2015) R-CNN minus R. In: BMVC15'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lenc 和 Vedaldi（2015）**Lenc K.**, **Vedaldi A.**（2015）《R-CNN 去除 R》。在：BMVC15
- en: Lenc and Vedaldi (2018) Lenc K., Vedaldi A. (2018) Understanding image representations
    by measuring their equivariance and equivalence. IJCV
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lenc 和 Vedaldi（2018）**Lenc K.**, **Vedaldi A.**（2018）《通过测量图像表示的等变性和等价性来理解图像表示》。IJCV
- en: 'Li et al. (2019a) Li B., Liu Y., Wang X. (2019a) Gradient harmonized single
    stage detector. In: AAAI'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2019a）**Li B.**, **Liu Y.**, **Wang X.**（2019a）《梯度调和单阶段检测器》。在：AAAI
- en: 'Li et al. (2015a) Li H., Lin Z., Shen X., Brandt J., Hua G. (2015a) A convolutional
    neural network cascade for face detection. In: CVPR, pp. 5325–5334'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2015a）**Li H.**, **Lin Z.**, **Shen X.**, **Brandt J.**, **Hua G.**（2015a）《用于人脸检测的卷积神经网络级联》。在：CVPR，第
    5325–5334 页
- en: 'Li et al. (2017a) Li H., Kadav A., Durdanovic I., Samet H., Graf H. P. (2017a)
    Pruning filters for efficient convnets. In: ICLR'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2017a）**Li H.**, **Kadav A.**, **Durdanovic I.**, **Samet H.**, **Graf
    H. P.**（2017a）《用于高效卷积网络的滤波器剪枝》。在：ICLR
- en: Li et al. (2018a) Li H., Liu Y., Ouyang W., XiaogangWang (2018a) Zoom out and
    in network with map attention decision for region proposal and object detection.
    IJCV
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2018a）李辉，刘洋，欧阳文，肖刚王（2018a）《带有地图注意决策的缩放网络用于区域提议和物体检测》。IJCV
- en: Li et al. (2017b) Li J., Wei Y., Liang X., Dong J., Xu T., Feng J., Yan S. (2017b)
    Attentive contexts for object detection. IEEE Transactions on Multimedia 19(5):944–954
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2017b）李江，魏杨，梁旭，董俊，徐涛，冯洁，阎深（2017b）《物体检测中的注意力上下文》。IEEE 多媒体学报 19(5):944–954
- en: 'Li et al. (2017c) Li Q., Jin S., Yan J. (2017c) Mimicking very efficient network
    for object detection. In: CVPR, pp. 7341–7349'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2017c）李强，金松，阎军（2017c）《模拟高效网络用于物体检测》。在：CVPR，页7341–7349
- en: Li and Zhang (2004) Li S. Z., Zhang Z. (2004) Floatboost learning and statistical
    face detection. IEEE TPAMI 26(9):1112–1123
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李斯和张志（2004）李斯·Z.，张志（2004）《浮动增强学习与统计人脸检测》。IEEE TPAMI 26(9):1112–1123
- en: 'Li et al. (2015b) Li Y., Wang S., Tian Q., Ding X. (2015b) Feature representation
    for statistical learning based object detection: A review. Pattern Recognition
    48(11):3542–3559'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2015b）李扬，王胜，田奇，丁鑫（2015b）《用于统计学习的特征表示：综述》。模式识别 48(11):3542–3559
- en: 'Li et al. (2017d) Li Y., Ouyang W., Zhou B., Wang K., Wang X. (2017d) Scene
    graph generation from objects, phrases and region captions. In: ICCV, pp. 1261–1270'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2017d）李扬，欧阳文，周博，王凯，王晓（2017d）《从对象、短语和区域标题生成场景图》。在：ICCV，页1261–1270
- en: 'Li et al. (2017e) Li Y., Qi H., Dai J., Ji X., Wei Y. (2017e) Fully convolutional
    instance aware semantic segmentation. In: CVPR, pp. 4438–4446'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2017e）李扬，齐宏，戴江，季晓，魏杨（2017e）《全卷积实例感知语义分割》。在：CVPR，页4438–4446
- en: Li et al. (2019b) Li Y., Chen Y., Wang N., Zhang Z. (2019b) Scale aware trident
    networks for object detection. arXiv preprint arXiv:190101892
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2019b）李扬，陈宇，王南，张志（2019b）《尺度感知三叉神经网络用于物体检测》。arXiv 预印本 arXiv:190101892
- en: 'Li et al. (2018b) Li Z., Peng C., Yu G., Zhang X., Deng Y., Sun J. (2018b)
    DetNet: A backbone network for object detection. In: ECCV'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2018b）李志，彭晨，余光，张晓，邓洋，孙健（2018b）《DetNet：用于物体检测的骨干网络》。在：ECCV
- en: 'Li et al. (2018c) Li Z., Peng C., Yu G., Zhang X., Deng Y., Sun J. (2018c)
    Light head RCNN: In defense of two stage object detector. In: CVPR'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2018c）李志，彭晨，余光，张晓，邓洋，孙健（2018c）《轻量级头部RCNN：为双阶段物体检测器辩护》。在：CVPR
- en: 'Lin et al. (2014) Lin T., Maire M., Belongie S., Hays J., Perona P., Ramanan
    D., Dollár P., Zitnick L. (2014) Microsoft COCO: Common objects in context. In:
    ECCV, pp. 740–755'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2014）林堂，迈尔，贝隆吉，海斯，佩罗纳，拉曼南，美元，齐特尼克（2014）《微软COCO：上下文中的常见对象》。在：ECCV，页740–755
- en: 'Lin et al. (2017a) Lin T., Dollár P., Girshick R., He K., Hariharan B., Belongie
    S. (2017a) Feature pyramid networks for object detection. In: CVPR'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2017a）林堂，美元，吉尔什克，何凯，哈里哈兰，贝隆吉（2017a）《用于物体检测的特征金字塔网络》。在：CVPR
- en: 'Lin et al. (2017b) Lin T., Goyal P., Girshick R., He K., Dollár P. (2017b)
    Focal loss for dense object detection. In: ICCV'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2017b）林堂，郭耀，吉尔什克，何凯，美元（2017b）《用于密集物体检测的焦点损失》。在：ICCV
- en: 'Lin et al. (2017c) Lin X., Zhao C., Pan W. (2017c) Towards accurate binary
    convolutional neural network. In: NIPS, pp. 344–352'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2017c）林轩，赵辰，潘伟（2017c）《朝向准确的二进制卷积神经网络》。在：NIPS，页344–352
- en: Litjens et al. (2017) Litjens G., Kooi T., Bejnordi B., Setio A., Ciompi F.,
    Ghafoorian M., J. van der Laak B. v., Sánchez C. (2017) A survey on deep learning
    in medical image analysis. Medical Image Analysis 42:60–88
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利特杰斯等（2017）利特杰斯，库伊，贝伊诺迪，塞提奥，奇奥皮，卡霍里安，J. 范德拉克，Sánchez C.（2017）《医学图像分析中的深度学习综述》。医学图像分析
    42:60–88
- en: 'Liu et al. (2018a) Liu C., Zoph B., Neumann M., Shlens J., Hua W., Li L., FeiFei
    L., Yuille A., Huang J., Murphy K. (2018a) Progressive neural architecture search.
    In: ECCV, pp. 19–34'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2018a）刘成，佐普，诺伊曼，施伦斯，华伟，李亮，费菲，尤伊尔，黄俊，墨菲（2018a）《渐进神经架构搜索》。在：ECCV，页19–34
- en: 'Liu et al. (2017) Liu L., Fieguth P., Guo Y., Wang X., Pietikäinen M. (2017)
    Local binary features for texture classification: Taxonomy and experimental study.
    Pattern Recognition 62:135–160'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2017）刘亮，费戈，郭毅，王晓，皮埃提凯宁（2017）《用于纹理分类的局部二进制特征：分类与实验研究》。模式识别 62:135–160
- en: 'Liu et al. (2018b) Liu S., Huang D., Wang Y. (2018b) Receptive field block
    net for accurate and fast object detection. In: ECCV'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2018b）刘帅，黄丹，王洋（2018b）《用于准确和快速物体检测的感受野块网络》。在：ECCV
- en: 'Liu et al. (2018c) Liu S., Qi L., Qin H., Shi J., Jia J. (2018c) Path aggregation
    network for instance segmentation. In: CVPR, pp. 8759–8768'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2018c）刘帅，齐磊，秦辉，石磊，贾金（2018c）《路径聚合网络用于实例分割》。在：CVPR，页8759–8768
- en: 'Liu et al. (2016) Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.,
    Berg A. (2016) SSD: single shot multibox detector. In: ECCV, pp. 21–37'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2016) Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C., Berg
    A. (2016) SSD：单次多框检测器。见：ECCV，第 21–37 页
- en: 'Liu et al. (2018d) Liu Y., Wang R., Shan S., Chen X. (2018d) Structure Inference
    Net: Object detection using scene level context and instance level relationships.
    In: CVPR, pp. 6985–6994'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2018d) Liu Y., Wang R., Shan S., Chen X. (2018d) 结构推断网络：利用场景级上下文和实例级关系进行对象检测。见：CVPR，第
    6985–6994 页
- en: 'Long et al. (2015) Long J., Shelhamer E., Darrell T. (2015) Fully convolutional
    networks for semantic segmentation. In: CVPR, pp. 3431–3440'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long 等人 (2015) Long J., Shelhamer E., Darrell T. (2015) 用于语义分割的全卷积网络。见：CVPR，第
    3431–3440 页
- en: 'Lowe (1999) Lowe D. (1999) Object recognition from local scale invariant features.
    In: ICCV, vol 2, pp. 1150–1157'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe (1999) Lowe D. (1999) 从局部尺度不变特征中进行对象识别。见：ICCV，第 2 卷，第 1150–1157 页
- en: Lowe (2004) Lowe D. (2004) Distinctive image features from scale-invariant keypoints.
    IJCV 60(2):91–110
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe (2004) Lowe D. (2004) 从尺度不变关键点中提取的独特图像特征。IJCV 60(2):91–110
- en: 'Loy et al. (2019) Loy C., Lin D., Ouyang W., Xiong Y., Yang S., Huang Q., Zhou
    D., Xia W., Li Q., Luo P., et al. (2019) WIDER face and pedestrian challenge 2018:
    Methods and results. arXiv:190206854'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loy 等人 (2019) Loy C., Lin D., Ouyang W., Xiong Y., Yang S., Huang Q., Zhou D.,
    Xia W., Li Q., Luo P., 等 (2019) WIDER 人脸和行人挑战 2018：方法和结果。arXiv:190206854
- en: 'Lu et al. (2016) Lu Y., Javidi T., Lazebnik S. (2016) Adaptive object detection
    using adjacency and zoom prediction. In: CVPR, pp. 2351–2359'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人 (2016) Lu Y., Javidi T., Lazebnik S. (2016) 使用邻接和缩放预测的自适应对象检测。见：CVPR，第
    2351–2359 页
- en: 'Luo et al. (2018) Luo P., Wang X., Shao W., Peng Z. (2018) Towards understanding
    regularization in batch normalization. In: ICLR'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人 (2018) Luo P., Wang X., Shao W., Peng Z. (2018) 旨在理解批量归一化中的正则化。见：ICLR
- en: Luo et al. (2019) Luo P., Ren J., Peng Z., Zhang R., Li J. (2019) Switchable
    normalization for learning to normalize deep representation. IEEE TPAMI
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人 (2019) Luo P., Ren J., Peng Z., Zhang R., Li J. (2019) 用于学习归一化深度表示的可切换归一化。IEEE
    TPAMI
- en: Ma et al. (2018) Ma J., Shao W., Ye H., Wang L., Wang H., Zheng Y., Xue X. (2018)
    Arbitrary oriented scene text detection via rotation proposals. IEEE TMM 20(11):3111–3122
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等人 (2018) Ma J., Shao W., Ye H., Wang L., Wang H., Zheng Y., Xue X. (2018)
    通过旋转提案进行任意方向场景文本检测。IEEE TMM 20(11):3111–3122
- en: 'Malisiewicz and Efros (2009) Malisiewicz T., Efros A. (2009) Beyond categories:
    The visual memex model for reasoning about object relationships. In: NIPS'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malisiewicz 和 Efros (2009) Malisiewicz T., Efros A. (2009) 超越类别：用于推理对象关系的视觉记忆模型。见：NIPS
- en: 'Manen et al. (2013) Manen S., Guillaumin M., Van Gool L. (2013) Prime object
    proposals with randomized prim’s algorithm. In: CVPR, pp. 2536–2543'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manen 等人 (2013) Manen S., Guillaumin M., Van Gool L. (2013) 使用随机 Prim 算法的优质对象提案。见：CVPR，第
    2536–2543 页
- en: Mikolajczyk and Schmid (2005) Mikolajczyk K., Schmid C. (2005) A performance
    evaluation of local descriptors. IEEE TPAMI 27(10):1615–1630
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolajczyk 和 Schmid (2005) Mikolajczyk K., Schmid C. (2005) 局部描述符的性能评估。IEEE
    TPAMI 27(10):1615–1630
- en: Mordan et al. (2018) Mordan T., Thome N., Henaff G., Cord M. (2018) End to end
    learning of latent deformable part based representations for object detection.
    IJCV pp. 1–21
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mordan 等人 (2018) Mordan T., Thome N., Henaff G., Cord M. (2018) 端到端学习潜在可变形部件表示用于对象检测。IJCV
    第 1–21 页
- en: MS COCO detection leaderboard (2018) MS COCO detection leaderboard (2018) [http://cocodataset.org/#](http://cocodataset.org/#)  [detection-leaderboard](detection-leaderboard)
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MS COCO 检测排行榜 (2018) MS COCO 检测排行榜 (2018) [http://cocodataset.org/#](http://cocodataset.org/#)  [detection-leaderboard](detection-leaderboard)
- en: 'Mundy (2006) Mundy J. (2006) Object recognition in the geometric era: A retrospective.
    in book Toward Category Level Object Recognition edited by J Ponce, M Hebert,
    C Schmid and A Zisserman pp. 3–28'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mundy (2006) Mundy J. (2006) 几何时代的对象识别：回顾。书中《面向类别级对象识别》由 J Ponce、M Hebert、C
    Schmid 和 A Zisserman 编辑，第 3–28 页
- en: Murase and Nayar (1995a) Murase H., Nayar S. (1995a) Visual learning and recognition
    of 3D objects from appearance. IJCV 14(1):5–24
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murase 和 Nayar (1995a) Murase H., Nayar S. (1995a) 从外观中学习和识别 3D 对象。IJCV 14(1):5–24
- en: Murase and Nayar (1995b) Murase H., Nayar S. (1995b) Visual learning and recognition
    of 3d objects from appearance. IJCV 14(1):5–24
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murase 和 Nayar (1995b) Murase H., Nayar S. (1995b) 从外观中学习和识别 3D 对象。IJCV 14(1):5–24
- en: 'Murphy et al. (2003) Murphy K., Torralba A., Freeman W. (2003) Using the forest
    to see the trees: a graphical model relating features, objects and scenes. In:
    NIPS'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murphy 等人 (2003) Murphy K., Torralba A., Freeman W. (2003) 使用森林来看树木：一个图形模型关联特征、对象和场景。见：NIPS
- en: 'Newell et al. (2016) Newell A., Yang K., Deng J. (2016) Stacked hourglass networks
    for human pose estimation. In: ECCV, pp. 483–499'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newell等人（2016）Newell A., Yang K., Deng J.（2016）用于人体姿势估计的堆叠沙漏网络。在：ECCV，pp. 483–499
- en: 'Newell et al. (2017) Newell A., Huang Z., Deng J. (2017) Associative embedding:
    end to end learning for joint detection and grouping. In: NIPS, pp. 2277–2287'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newell等人（2017）Newell A., Huang Z., Deng J.（2017）关联嵌入：端到端学习用于联合检测和分组。在：NIPS，pp.
    2277–2287
- en: Ojala et al. (2002) Ojala T., Pietikäinen M., Maenpää T. (2002) Multiresolution
    gray-scale and rotation invariant texture classification with local binary patterns.
    IEEE TPAMI 24(7):971–987
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ojala等人（2002）Ojala T., Pietikäinen M., Maenpää T.（2002）具有局部二值模式的多分辨率灰度和旋转不变纹理分类。IEEE
    TPAMI 24(7):971–987
- en: Oliva and Torralba (2007) Oliva A., Torralba A. (2007) The role of context in
    object recognition. Trends in cognitive sciences 11(12):520–527
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oliva和Torralba（2007）Oliva A., Torralba A.（2007）上下文在对象识别中的作用。认知科学趋势 11(12):520–527
- en: Opelt et al. (2006) Opelt A., Pinz A., Fussenegger M., Auer P. (2006) Generic
    object recognition with boosting. IEEE TPAMI 28(3):416–431
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Opelt等人（2006）Opelt A., Pinz A., Fussenegger M., Auer P.（2006）使用增强进行通用对象识别。IEEE
    TPAMI 28(3):416–431
- en: 'Oquab et al. (2014) Oquab M., Bottou L., Laptev I., Sivic J. (2014) Learning
    and transferring midlevel image representations using convolutional neural networks.
    In: CVPR, pp. 1717–1724'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oquab等人（2014）Oquab M., Bottou L., Laptev I., Sivic J.（2014）使用卷积神经网络学习和转移中层图像表示。在：CVPR，pp.
    1717–1724
- en: 'Oquab et al. (2015) Oquab M., Bottou L., Laptev I., Sivic J. (2015) Is object
    localization for free? weakly supervised learning with convolutional neural networks.
    In: CVPR, pp. 685–694'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oquab等人（2015）Oquab M., Bottou L., Laptev I., Sivic J.（2015）物体定位是否免费？使用卷积神经网络的弱监督学习。在：CVPR，pp.
    685–694
- en: 'Osuna et al. (1997) Osuna E., Freund R., Girosit F. (1997) Training support
    vector machines: an application to face detection. In: CVPR, pp. 130–136'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osuna等人（1997）Osuna E., Freund R., Girosit F.（1997）训练支持向量机：应用于人脸检测。在：CVPR，pp.
    130–136
- en: 'Ouyang and Wang (2013) Ouyang W., Wang X. (2013) Joint deep learning for pedestrian
    detection. In: ICCV, pp. 2056–2063'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang和Wang（2013）Ouyang W., Wang X.（2013）联合深度学习用于行人检测。在：ICCV，pp. 2056–2063
- en: 'Ouyang et al. (2015) Ouyang W., Wang X., Zeng X., Qiu S., Luo P., Tian Y.,
    Li H., Yang S., Wang Z., Loy C.-C., et al. (2015) DeepIDNet: Deformable deep convolutional
    neural networks for object detection. In: CVPR, pp. 2403–2412'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2015）Ouyang W., Wang X., Zeng X., Qiu S., Luo P., Tian Y., Li H., Yang
    S., Wang Z., Loy C.-C., et al.（2015）DeepIDNet：用于对象检测的可变形深度卷积神经网络。在：CVPR，pp. 2403–2412
- en: 'Ouyang et al. (2016) Ouyang W., Wang X., Zhang C., Yang X. (2016) Factors in
    finetuning deep model for object detection with long tail distribution. In: CVPR,
    pp. 864–873'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2016）Ouyang W., Wang X., Zhang C., Yang X.（2016）微调深度模型以处理长尾分布的对象检测因素。在：CVPR，pp.
    864–873
- en: Ouyang et al. (2017a) Ouyang W., Wang K., Zhu X., Wang X. (2017a) Chained cascade
    network for object detection. ICCV
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2017a）Ouyang W., Wang K., Zhu X., Wang X.（2017a）用于对象检测的链式级联网络。ICCV
- en: 'Ouyang et al. (2017b) Ouyang W., Zeng X., Wang X., Qiu S., Luo P., Tian Y.,
    Li H., Yang S., Wang Z., Li H., Wang K., Yan J., Loy C. C., Tang X. (2017b) DeepIDNet:
    Object detection with deformable part based convolutional neural networks. IEEE
    TPAMI 39(7):1320–1334'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang等人（2017b）Ouyang W., Zeng X., Wang X., Qiu S., Luo P., Tian Y., Li H.,
    Yang S., Wang Z., Li H., Wang K., Yan J., Loy C. C., Tang X.（2017b）DeepIDNet：基于可变形部件的卷积神经网络对象检测。IEEE
    TPAMI 39(7):1320–1334
- en: 'Parikh et al. (2012) Parikh D., Zitnick C., Chen T. (2012) Exploring tiny images:
    The roles of appearance and contextual information for machine and human object
    recognition. IEEE TPAMI 34(10):1978–1991'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parikh等人（2012）Parikh D., Zitnick C., Chen T.（2012）探索微小图像：外观和上下文信息在机器和人类对象识别中的作用。IEEE
    TPAMI 34(10):1978–1991
- en: PASCAL VOC detection leaderboard (2018) PASCAL VOC detection leaderboard (2018)
    [http://host.robots.ox.ac.uk:8080/leaderboard/](http://host.robots.ox.ac.uk:8080/leaderboard/)  [main_bootstrap.php](main_bootstrap.php)
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PASCAL VOC检测排行榜（2018）PASCAL VOC检测排行榜（2018）[http://host.robots.ox.ac.uk:8080/leaderboard/](http://host.robots.ox.ac.uk:8080/leaderboard/)
    [main_bootstrap.php](main_bootstrap.php)
- en: 'Peng et al. (2018) Peng C., Xiao T., Li Z., Jiang Y., Zhang X., Jia K., Yu
    G., Sun J. (2018) MegDet: A large minibatch object detector. In: CVPR'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng等人（2018）Peng C., Xiao T., Li Z., Jiang Y., Zhang X., Jia K., Yu G., Sun
    J.（2018）MegDet：大型小批量对象检测器。在：CVPR
- en: 'Peng et al. (2015) Peng X., Sun B., Ali K., Saenko K. (2015) Learning deep
    object detectors from 3d models. In: ICCV, pp. 1278–1286'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng等人（2015）Peng X., Sun B., Ali K., Saenko K.（2015）从3D模型学习深度物体检测器。在：ICCV，pp.
    1278–1286
- en: 'Pepik et al. (2015) Pepik B., Benenson R., Ritschel T., Schiele B. (2015) What
    is holding back convnets for detection? In: German Conference on Pattern Recognition,
    pp. 517–528'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pepik 等（2015）Pepik B.、Benenson R.、Ritschel T.、Schiele B.（2015）是什么阻碍了卷积网络的检测？在：德国模式识别会议，第517–528页
- en: 'Perronnin et al. (2010) Perronnin F., Sánchez J., Mensink T. (2010) Improving
    the fisher kernel for large scale image classification. In: ECCV, pp. 143–156'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perronnin 等（2010）Perronnin F.、Sánchez J.、Mensink T.（2010）改进大规模图像分类的 Fisher 核。在：ECCV，第143–156页
- en: 'Pinheiro et al. (2015) Pinheiro P., Collobert R., Dollar P. (2015) Learning
    to segment object candidates. In: NIPS, pp. 1990–1998'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinheiro 等（2015）Pinheiro P.、Collobert R.、Dollar P.（2015）学习对象候选区域的分割。在：NIPS，第1990–1998页
- en: 'Pinheiro et al. (2016) Pinheiro P., Lin T., Collobert R., Dollár P. (2016)
    Learning to refine object segments. In: ECCV, pp. 75–91'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinheiro 等（2016）Pinheiro P.、Lin T.、Collobert R.、Dollár P.（2016）学习精细化对象分段。在：ECCV，第75–91页
- en: Ponce et al. (2007) Ponce J., Hebert M., Schmid C., Zisserman A. (2007) Toward
    Category Level Object Recognition. Springer
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ponce 等（2007）Ponce J.、Hebert M.、Schmid C.、Zisserman A.（2007）迈向类别级别对象识别。Springer
- en: 'Pouyanfar et al. (2018) Pouyanfar S., Sadiq S., Yan Y., Tian H., Tao Y., Reyes
    M. P., Shyu M., Chen S., Iyengar S. (2018) A survey on deep learning: Algorithms,
    techniques, and applications. ACM Computing Surveys 51(5):92:1–92:36'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pouyanfar 等（2018）Pouyanfar S.、Sadiq S.、Yan Y.、Tian H.、Tao Y.、Reyes M. P.、Shyu
    M.、Chen S.、Iyengar S.（2018）深度学习调查：算法、技术和应用。ACM Computing Surveys 51(5):92:1–92:36
- en: 'Qi et al. (2017) Qi C. R., Su H., Mo K., Guibas L. J. (2017) PointNet: Deep
    learning on point sets for 3D classification and segmentation. In: CVPR, pp. 652–660'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等（2017）Qi C. R.、Su H.、Mo K.、Guibas L. J.（2017）PointNet：用于 3D 分类和分割的点集深度学习。在：CVPR，第652–660页
- en: 'Qi et al. (2018) Qi C. R., Liu W., Wu C., Su H., Guibas L. J. (2018) Frustum
    pointnets for 3D object detection from RGBD data. In: CVPR, pp. 918–927'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等（2018）Qi C. R.、Liu W.、Wu C.、Su H.、Guibas L. J.（2018）用于从 RGBD 数据中进行 3D 对象检测的
    Frustum pointnets。在：CVPR，第918–927页
- en: 'Quanming et al. (2018) Quanming Y., Mengshuo W., Hugo J. E., Isabelle G., Yiqi
    H., Yufeng L., Weiwei T., Qiang Y., Yang Y. (2018) Taking human out of learning
    applications: A survey on automated machine learning. arXiv:181013306'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quanming 等（2018）Quanming Y.、Mengshuo W.、Hugo J. E.、Isabelle G.、Yiqi H.、Yufeng
    L.、Weiwei T.、Qiang Y.、Yang Y.（2018）将人工智能从学习应用中移除：自动化机器学习的调查。arXiv:181013306
- en: 'Rabinovich et al. (2007) Rabinovich A., Vedaldi A., Galleguillos C., Wiewiora
    E., Belongie S. (2007) Objects in context. In: ICCV'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabinovich 等（2007）Rabinovich A.、Vedaldi A.、Galleguillos C.、Wiewiora E.、Belongie
    S.（2007）上下文中的对象。在：ICCV
- en: Rahman et al. (2018a) Rahman S., Khan S., Barnes N. (2018a) Polarity loss for
    zero shot object detection. arXiv preprint arXiv:181108982
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahman 等（2018a）Rahman S.、Khan S.、Barnes N.（2018a）零样本对象检测的极性损失。arXiv 预印本 arXiv:181108982
- en: 'Rahman et al. (2018b) Rahman S., Khan S., Porikli F. (2018b) Zero shot object
    detection: Learning to simultaneously recognize and localize novel concepts. In:
    ACCV'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahman 等（2018b）Rahman S.、Khan S.、Porikli F.（2018b）零样本对象检测：学习同时识别和定位新概念。在：ACCV
- en: 'Razavian et al. (2014) Razavian R., Azizpour H., Sullivan J., Carlsson S. (2014)
    CNN features off the shelf: an astounding baseline for recognition. In: CVPR Workshops,
    pp. 806–813'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Razavian 等（2014）Razavian R.、Azizpour H.、Sullivan J.、Carlsson S.（2014）CNN 特征即取即用：惊人的识别基线。在：CVPR
    研讨会，第806–813页
- en: 'Rebuffi et al. (2017) Rebuffi S., Bilen H., Vedaldi A. (2017) Learning multiple
    visual domains with residual adapters. In: Advances in Neural Information Processing
    Systems, pp. 506–516'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rebuffi 等（2017）Rebuffi S.、Bilen H.、Vedaldi A.（2017）通过残差适配器学习多个视觉领域。在：神经信息处理系统进展，第506–516页
- en: 'Rebuffi et al. (2018) Rebuffi S., Bilen H., Vedaldi A. (2018) Efficient parametrization
    of multidomain deep neural networks. In: CVPR, pp. 8119–8127'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rebuffi 等（2018）Rebuffi S.、Bilen H.、Vedaldi A.（2018）多域深度神经网络的高效参数化。在：CVPR，第8119–8127页
- en: 'Redmon and Farhadi (2017) Redmon J., Farhadi A. (2017) YOLO9000: Better, faster,
    stronger. In: CVPR'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redmon 和 Farhadi（2017）Redmon J.、Farhadi A.（2017）YOLO9000：更好、更快、更强。在：CVPR
- en: 'Redmon et al. (2016) Redmon J., Divvala S., Girshick R., Farhadi A. (2016)
    You only look once: Unified, real time object detection. In: CVPR, pp. 779–788'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redmon 等（2016）Redmon J.、Divvala S.、Girshick R.、Farhadi A.（2016）你只需看一次：统一的实时对象检测。在：CVPR，第779–788页
- en: 'Ren et al. (2018) Ren M., Triantafillou E., Ravi S., Snell J., Swersky K.,
    Tenenbaum J. B., Larochelle H., Zemel R. S. (2018) Meta learning for semisupervised
    few shot classification. In: ICLR'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等（2018）Ren M.、Triantafillou E.、Ravi S.、Snell J.、Swersky K.、Tenenbaum J.
    B.、Larochelle H.、Zemel R. S.（2018）用于半监督少样本分类的元学习。在：ICLR
- en: 'Ren et al. (2015) Ren S., He K., Girshick R., Sun J. (2015) Faster R-CNN: Towards
    real time object detection with region proposal networks. In: NIPS, pp. 91–99'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren等人（2015）Ren S.、He K.、Girshick R.、Sun J.（2015）Faster R-CNN：通过区域提议网络实现实时对象检测。发表于：NIPS，第91–99页
- en: 'Ren et al. (2017a) Ren S., He K., Girshick R., Sun J. (2017a) Faster RCNN:
    Towards real time object detection with region proposal networks. IEEE TPAMI 39(6):1137–1149'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren等人（2017a）Ren S.、He K.、Girshick R.、Sun J.（2017a）Faster RCNN：通过区域提议网络实现实时对象检测。IEEE
    TPAMI 39(6)：1137–1149
- en: Ren et al. (2017b) Ren S., He K., Girshick R., Zhang X., Sun J. (2017b) Object
    detection networks on convolutional feature maps. IEEE TPAMI
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren等人（2017b）Ren S.、He K.、Girshick R.、Zhang X.、Sun J.（2017b）卷积特征图上的对象检测网络。IEEE
    TPAMI
- en: 'Rezatofighi et al. (2019) Rezatofighi H., Tsoi N., Gwak J., Sadeghian A., Reid
    I., Savarese S. (2019) Generalized intersection over union: A metric and a loss
    for bounding box regression. In: CVPR'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezatofighi等人（2019）Rezatofighi H.、Tsoi N.、Gwak J.、Sadeghian A.、Reid I.、Savarese
    S.（2019）广义交并比：用于边界框回归的度量和损失。发表于：CVPR
- en: Rowley et al. (1998) Rowley H., Baluja S., Kanade T. (1998) Neural network based
    face detection. IEEE TPAMI 20(1):23–38
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rowley等人（1998）Rowley H.、Baluja S.、Kanade T.（1998）基于神经网络的人脸检测。IEEE TPAMI 20(1)：23–38
- en: Russakovsky et al. (2015) Russakovsky O., Deng J., Su H., Krause J., Satheesh
    S., Ma S., Huang Z., Karpathy A., Khosla A., Bernstein M., Berg A., Li F. (2015)
    ImageNet large scale visual recognition challenge. IJCV 115(3):211–252
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russakovsky等人（2015）Russakovsky O.、Deng J.、Su H.、Krause J.、Satheesh S.、Ma S.、Huang
    Z.、Karpathy A.、Khosla A.、Bernstein M.、Berg A.、Li F.（2015）ImageNet大规模视觉识别挑战。IJCV
    115(3)：211–252
- en: 'Russell et al. (2008) Russell B., Torralba A., Murphy K., Freeman W. (2008)
    LabelMe: A database and web based tool for image annotation. IJCV 77(1-3):157–173'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russell等人（2008）Russell B.、Torralba A.、Murphy K.、Freeman W.（2008）LabelMe：用于图像标注的数据库和基于网络的工具。IJCV
    77(1-3)：157–173
- en: Schmid and Mohr (1997) Schmid C., Mohr R. (1997) Local grayvalue invariants
    for image retrieval. IEEE TPAMI 19(5):530–535
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmid和Mohr（1997）Schmid C.、Mohr R.（1997）用于图像检索的局部灰度不变量。IEEE TPAMI 19(5)：530–535
- en: 'Schwartz et al. (2019) Schwartz E., Karlinsky L., Shtok J., Harary S., Marder
    M., Pankanti S., Feris R., Kumar A., Giries R., Bronstein A. (2019) RepMet: Representative
    based metric learning for classification and one shot object detection. In: CVPR'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwartz等人（2019）Schwartz E.、Karlinsky L.、Shtok J.、Harary S.、Marder M.、Pankanti
    S.、Feris R.、Kumar A.、Giries R.、Bronstein A.（2019）RepMet：基于代表性的度量学习用于分类和单次对象检测。发表于：CVPR
- en: 'Sermanet et al. (2013) Sermanet P., Kavukcuoglu K., Chintala S., LeCun Y. (2013)
    Pedestrian detection with unsupervised multistage feature learning. In: CVPR,
    pp. 3626–3633'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sermanet等人（2013）Sermanet P.、Kavukcuoglu K.、Chintala S.、LeCun Y.（2013）使用无监督多阶段特征学习进行行人检测。发表于：CVPR，第3626–3633页
- en: 'Sermanet et al. (2014) Sermanet P., Eigen D., Zhang X., Mathieu M., Fergus
    R., LeCun Y. (2014) OverFeat: Integrated recognition, localization and detection
    using convolutional networks. In: ICLR'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sermanet等人（2014）Sermanet P.、Eigen D.、Zhang X.、Mathieu M.、Fergus R.、LeCun Y.（2014）OverFeat：利用卷积网络进行集成识别、定位和检测。发表于：ICLR
- en: 'Shang et al. (2016) Shang W., Sohn K., Almeida D., Lee H. (2016) Understanding
    and improving convolutional neural networks via concatenated rectified linear
    units. In: ICML, pp. 2217–2225'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang等人（2016）Shang W.、Sohn K.、Almeida D.、Lee H.（2016）通过串联修正线性单元理解和改进卷积神经网络。发表于：ICML，第2217–2225页
- en: Shelhamer et al. (2017) Shelhamer E., Long J., Darrell T. (2017) Fully convolutional
    networks for semantic segmentation. IEEE TPAMI
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shelhamer等人（2017）Shelhamer E.、Long J.、Darrell T.（2017）用于语义分割的全卷积网络。IEEE TPAMI
- en: 'Shen et al. (2017) Shen Z., Liu Z., Li J., Jiang Y., Chen Y., Xue X. (2017)
    DSOD: Learning deeply supervised object detectors from scratch. In: ICCV'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen等人（2017）Shen Z.、Liu Z.、Li J.、Jiang Y.、Chen Y.、Xue X.（2017）DSOD：从头开始学习深度监督对象检测器。发表于：ICCV
- en: 'Shi et al. (2018) Shi X., Shan S., Kan M., Wu S., Chen X. (2018) Real time
    rotation invariant face detection with progressive calibration networks. In: CVPR'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等人（2018）Shi X.、Shan S.、Kan M.、Wu S.、Chen X.（2018）实时旋转不变人脸检测与渐进校准网络。发表于：CVPR
- en: Shi et al. (2017) Shi Z., Yang Y., Hospedales T., Xiang T. (2017) Weakly supervised
    image annotation and segmentation with objects and attributes. IEEE TPAMI 39(12):2525–2538
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等人（2017）Shi Z.、Yang Y.、Hospedales T.、Xiang T.（2017）弱监督图像标注和分割与对象及属性。IEEE
    TPAMI 39(12)：2525–2538
- en: 'Shrivastava and Gupta (2016) Shrivastava A., Gupta A. (2016) Contextual priming
    and feedback for Faster RCNN. In: ECCV, pp. 330–348'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shrivastava和Gupta（2016）Shrivastava A.、Gupta A.（2016）上下文引导和反馈用于Faster RCNN。发表于：ECCV，第330–348页
- en: 'Shrivastava et al. (2016) Shrivastava A., Gupta A., Girshick R. (2016) Training
    region based object detectors with online hard example mining. In: CVPR, pp. 761–769'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shrivastava 等 (2016) Shrivastava A., Gupta A., Girshick R. (2016) 使用在线难例挖掘训练基于区域的对象检测器。In:
    CVPR，页761–769'
- en: 'Shrivastava et al. (2017) Shrivastava A., Sukthankar R., Malik J., Gupta A.
    (2017) Beyond skip connections: Top down modulation for object detection. In:
    CVPR'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shrivastava 等 (2017) Shrivastava A., Sukthankar R., Malik J., Gupta A. (2017)
    超越跳跃连接：用于对象检测的自上而下调制。In: CVPR'
- en: 'Simonyan and Zisserman (2015) Simonyan K., Zisserman A. (2015) Very deep convolutional
    networks for large scale image recognition. In: ICLR'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Simonyan 和 Zisserman (2015) Simonyan K., Zisserman A. (2015) 用于大规模图像识别的非常深度卷积网络。In:
    ICLR'
- en: 'Singh and Davis (2018) Singh B., Davis L. (2018) An analysis of scale invariance
    in object detection-SNIP. In: CVPR'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh 和 Davis (2018) Singh B., Davis L. (2018) 对象检测中尺度不变性的分析-SNIP。In: CVPR'
- en: 'Singh et al. (2018a) Singh B., Li H., Sharma A., Davis L. S. (2018a) RFCN 3000
    at 30fps: Decoupling detection and classification. In: CVPR'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh 等 (2018a) Singh B., Li H., Sharma A., Davis L. S. (2018a) RFCN 3000在30fps下：解耦检测与分类。In:
    CVPR'
- en: 'Singh et al. (2018b) Singh B., Najibi M., Davis L. S. (2018b) SNIPER: Efficient
    multiscale training. arXiv:180509300'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等 (2018b) Singh B., Najibi M., Davis L. S. (2018b) SNIPER：高效的多尺度训练。arXiv:180509300
- en: 'Sivic and Zisserman (2003) Sivic J., Zisserman A. (2003) Video google: A text
    retrieval approach to object matching in videos. In: International Conference
    on Computer Vision (ICCV), vol 2, pp. 1470–1477'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sivic 和 Zisserman (2003) Sivic J., Zisserman A. (2003) 视频谷：一种基于文本的对象匹配方法。In:
    国际计算机视觉大会 (ICCV)，第2卷，页1470–1477'
- en: 'Song Han (2016) Song Han W. J. D. Huizi Mao (2016) Deep Compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. In:
    ICLR'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song Han (2016) Song Han W. J. D. Huizi Mao (2016) 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。In:
    ICLR'
- en: 'Sun et al. (2017) Sun C., Shrivastava A., Singh S., Gupta A. (2017) Revisiting
    unreasonable effectiveness of data in deep learning era. In: ICCV, pp. 843–852'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 (2017) Sun C., Shrivastava A., Singh S., Gupta A. (2017) 重新审视深度学习时代数据的不合理有效性。In:
    ICCV，页843–852'
- en: 'Sun et al. (2019a) Sun K., Xiao B., Liu D., Wang J. (2019a) Deep high resolution
    representation learning for human pose estimation. In: CVPR'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 (2019a) Sun K., Xiao B., Liu D., Wang J. (2019a) 深度高分辨率表示学习用于人体姿态估计。In:
    CVPR'
- en: Sun et al. (2019b) Sun K., Zhao Y., Jiang B., Cheng T., Xiao B., Liu D., Mu
    Y., Wang X., Liu W., Wang J. (2019b) High resolution representations for labeling
    pixels and regions. CoRR abs/1904.04514
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2019b) Sun K., Zhao Y., Jiang B., Cheng T., Xiao B., Liu D., Mu Y., Wang
    X., Liu W., Wang J. (2019b) 高分辨率表示用于像素和区域标注。CoRR abs/1904.04514
- en: 'Sun et al. (2018) Sun S., Pang J., Shi J., Yi S., Ouyang W. (2018) FishNet:
    A versatile backbone for image, region, and pixel level prediction. In: NIPS,
    pp. 754–764'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等 (2018) Sun S., Pang J., Shi J., Yi S., Ouyang W. (2018) FishNet：一种多功能骨干网络用于图像、区域和像素级预测。In:
    NIPS，页754–764'
- en: 'Sun et al. (2006) Sun Z., Bebis G., Miller R. (2006) On road vehicle detection:
    A review. IEEE TPAMI 28(5):694–711'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2006) Sun Z., Bebis G., Miller R. (2006) 道路车辆检测综述。IEEE TPAMI 28(5):694–711
- en: Sung et al. (1994) Sung K., , Poggio T. (1994) Learning and example selection
    for object and pattern detection. MIT AI Memo (1521)
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sung 等 (1994) Sung K., Poggio T. (1994) 对象和模式检测的学习与示例选择。MIT AI Memo (1521)
- en: Swain and Ballard (1991) Swain M., Ballard D. (1991) Color indexing. IJCV 7(1):11–32
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swain 和 Ballard (1991) Swain M., Ballard D. (1991) 颜色索引。IJCV 7(1):11–32
- en: 'Szegedy et al. (2013) Szegedy C., Toshev A., Erhan D. (2013) Deep neural networks
    for object detection. In: NIPS, pp. 2553–2561'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Szegedy 等 (2013) Szegedy C., Toshev A., Erhan D. (2013) 用于对象检测的深度神经网络。In: NIPS，页2553–2561'
- en: 'Szegedy et al. (2014) Szegedy C., Reed S., Erhan D., Anguelov D., Ioffe S.
    (2014) Scalable, high quality object detection. In: arXiv preprint arXiv:1412.1441'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Szegedy 等 (2014) Szegedy C., Reed S., Erhan D., Anguelov D., Ioffe S. (2014)
    可扩展的高质量对象检测。In: arXiv 预印本 arXiv:1412.1441'
- en: 'Szegedy et al. (2015) Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov
    D., Erhan D., Vanhoucke V., Rabinovich A. (2015) Going deeper with convolutions.
    In: CVPR, pp. 1–9'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Szegedy 等 (2015) Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov
    D., Erhan D., Vanhoucke V., Rabinovich A. (2015) 深度卷积网络。In: CVPR，页1–9'
- en: 'Szegedy et al. (2016) Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna
    Z. (2016) Rethinking the inception architecture for computer vision. In: CVPR,
    pp. 2818–2826'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Szegedy 等 (2016) Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z. (2016)
    重新思考计算机视觉中的Inception架构。In: CVPR，页2818–2826'
- en: Szegedy et al. (2017) Szegedy C., Ioffe S., Vanhoucke V., Alemi A. (2017) Inception
    v4, inception resnet and the impact of residual connections on learning. AAAI
    pp. 4278–4284
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等 (2017) Szegedy C., Ioffe S., Vanhoucke V., Alemi A. (2017) Inception
    v4, inception resnet 及残差连接对学习的影响。AAAI 页 4278–4284
- en: Torralba (2003) Torralba A. (2003) Contextual priming for object detection.
    IJCV 53(2):169–191
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torralba (2003) Torralba A. (2003) 面向目标检测的上下文引导。IJCV 53(2):169–191
- en: 'Turk and Pentland (1991) Turk M. A., Pentland A. (1991) Face recognition using
    eigenfaces. In: CVPR, pp. 586–591'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turk 和 Pentland (1991) Turk M. A., Pentland A. (1991) 使用特征脸进行面部识别。发表于：CVPR，页
    586–591
- en: 'Tuzel et al. (2006) Tuzel O., Porikli F., Meer P. (2006) Region covariance:
    A fast descriptor for detection and classification. In: ECCV, pp. 589–600'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tuzel 等 (2006) Tuzel O., Porikli F., Meer P. (2006) 区域协方差：一种快速的检测与分类描述符。发表于：ECCV，页
    589–600
- en: 'TychsenSmith and Petersson (2017) TychsenSmith L., Petersson L. (2017) DeNet:
    scalable real time object detection with directed sparse sampling. In: ICCV'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TychsenSmith 和 Petersson (2017) TychsenSmith L., Petersson L. (2017) DeNet：通过定向稀疏采样实现可扩展的实时目标检测。发表于：ICCV
- en: 'TychsenSmith and Petersson (2018) TychsenSmith L., Petersson L. (2018) Improving
    object localization with fitness nms and bounded iou loss. In: CVPR'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TychsenSmith 和 Petersson (2018) TychsenSmith L., Petersson L. (2018) 使用适应性非最大抑制和有界
    IOU 损失改进目标定位。发表于：CVPR
- en: Uijlings et al. (2013) Uijlings J., van de Sande K., Gevers T., Smeulders A.
    (2013) Selective search for object recognition. IJCV 104(2):154–171
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uijlings 等 (2013) Uijlings J., van de Sande K., Gevers T., Smeulders A. (2013)
    面向目标识别的选择性搜索。IJCV 104(2):154–171
- en: Vaillant et al. (1994) Vaillant R., Monrocq C., LeCun Y. (1994) Original approach
    for the localisation of objects in images. IEE Proceedings Vision, Image and Signal
    Processing 141(4):245–250
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaillant 等 (1994) Vaillant R., Monrocq C., LeCun Y. (1994) 一种图像中物体定位的原始方法。IEE
    会议论文集视觉、图像与信号处理 141(4):245–250
- en: 'Van de Sande et al. (2011) Van de Sande K., Uijlings J., Gevers T., Smeulders
    A. (2011) Segmentation as selective search for object recognition. In: ICCV, pp.
    1879–1886'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van de Sande 等 (2011) Van de Sande K., Uijlings J., Gevers T., Smeulders A.
    (2011) 将分割作为目标识别的选择性搜索。发表于：ICCV，页 1879–1886
- en: 'Vaswani et al. (2017) Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones
    L., Gomez A. N., Kaiser Ł., Polosukhin I. (2017) Attention is all you need. In:
    NIPS, pp. 6000–6010'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 (2017) Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L.,
    Gomez A. N., Kaiser Ł., Polosukhin I. (2017) 注意力机制就是你所需要的。发表于：NIPS，页 6000–6010
- en: 'Vedaldi et al. (2009) Vedaldi A., Gulshan V., Varma M., Zisserman A. (2009)
    Multiple kernels for object detection. In: ICCV, pp. 606–613'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vedaldi 等 (2009) Vedaldi A., Gulshan V., Varma M., Zisserman A. (2009) 面向目标检测的多重核。发表于：ICCV，页
    606–613
- en: 'Viola and Jones (2001) Viola P., Jones M. (2001) Rapid object detection using
    a boosted cascade of simple features. In: CVPR, vol 1, pp. 1–8'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Viola 和 Jones (2001) Viola P., Jones M. (2001) 使用简单特征的增强级联进行快速目标检测。发表于：CVPR，第
    1 卷，页 1–8
- en: 'Wan et al. (2015) Wan L., Eigen D., Fergus R. (2015) End to end integration
    of a convolution network, deformable parts model and nonmaximum suppression. In:
    CVPR, pp. 851–859'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等 (2015) Wan L., Eigen D., Fergus R. (2015) 卷积网络、可变形部件模型和非最大抑制的端到端集成。发表于：CVPR，页
    851–859
- en: 'Wang et al. (2018) Wang H., Wang Q., Gao M., Li P., Zuo W. (2018) Multiscale
    location aware kernel representation for object detection. In: CVPR'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2018) Wang H., Wang Q., Gao M., Li P., Zuo W. (2018) 面向目标检测的多尺度位置感知核表示。发表于：CVPR
- en: 'Wang et al. (2009) Wang X., Han T., Yan S. (2009) An HOG-LBP human detector
    with partial occlusion handling. In: International Conference on Computer Vision,
    pp. 32–39'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2009) Wang X., Han T., Yan S. (2009) 一种具有部分遮挡处理的 HOG-LBP 人体检测器。发表于：国际计算机视觉会议，页
    32–39
- en: 'Wang et al. (2017) Wang X., Shrivastava A., Gupta A. (2017) A Fast RCNN: Hard
    positive generation via adversary for object detection. In: CVPR'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2017) Wang X., Shrivastava A., Gupta A. (2017) 一种快速 RCNN：通过对抗生成硬正样本进行目标检测。发表于：CVPR
- en: Wang et al. (2019) Wang X., Cai Z., Gao D., Vasconcelos N. (2019) Towards universal
    object detection by domain attention. arXiv:190404402
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2019) Wang X., Cai Z., Gao D., Vasconcelos N. (2019) 通过领域注意力实现通用目标检测。arXiv:190404402
- en: 'Wei et al. (2018) Wei Y., Pan X., Qin H., Ouyang W., Yan J. (2018) Quantization
    mimic: Towards very tiny CNN for object detection. In: ECCV, pp. 267–283'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 (2018) Wei Y., Pan X., Qin H., Ouyang W., Yan J. (2018) 量化模拟：面向非常小型 CNN
    的目标检测。发表于：ECCV，页 267–283
- en: 'Woo et al. (2018) Woo S., Hwang S., Kweon I. (2018) StairNet: Top down semantic
    aggregation for accurate one shot detection. In: WACV, pp. 1093–1102'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Woo 等 (2018) Woo S., Hwang S., Kweon I. (2018) StairNet：自上而下的语义聚合用于准确的一次性检测。发表于：WACV，页
    1093–1102
- en: 'Worrall et al. (2017) Worrall D. E., Garbin S. J., Turmukhambetov D., Brostow
    G. J. (2017) Harmonic networks: Deep translation and rotation equivariance. In:
    CVPR, vol 2'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Worrall 等人 (2017) Worrall D. E., Garbin S. J., Turmukhambetov D., Brostow G.
    J. (2017) Harmonic networks: Deep translation and rotation equivariance. 见：CVPR，第
    2 卷'
- en: 'Wu and He (2018) Wu Y., He K. (2018) Group normalization. In: ECCV, pp. 3–19'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 和 He (2018) Wu Y., He K. (2018) 组归一化。见：ECCV，第 3–19 页
- en: 'Wu et al. (2015) Wu Z., Song S., Khosla A., Yu F., Zhang L., Tang X., Xiao
    J. (2015) 3D ShapeNets: A deep representation for volumetric shapes. In: CVPR,
    pp. 1912–1920'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 (2015) Wu Z., Song S., Khosla A., Yu F., Zhang L., Tang X., Xiao J. (2015)
    3D ShapeNets: 体积形状的深度表示。见：CVPR，第 1912–1920 页'
- en: Wu et al. (2019) Wu Z., Pan S., Chen F., Long G., Zhang C., Yu P. S. (2019)
    A comprehensive survey on graph neural networks. arXiv preprint arXiv:190100596
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2019) Wu Z., Pan S., Chen F., Long G., Zhang C., Yu P. S. (2019) 图神经网络的综合调查。arXiv
    预印本 arXiv:190100596
- en: 'Xia et al. (2018) Xia G., Bai X., Ding J., Zhu Z., Belongie S., Luo J., Datcu
    M., Pelillo M., Zhang L. (2018) DOTA: a large-scale dataset for object detection
    in aerial images. In: CVPR, pp. 3974–3983'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia 等人 (2018) Xia G., Bai X., Ding J., Zhu Z., Belongie S., Luo J., Datcu M.,
    Pelillo M., Zhang L. (2018) DOTA: 一个用于航空图像中目标检测的大规模数据集。见：CVPR，第 3974–3983 页'
- en: 'Xiang et al. (2014) Xiang Y., Mottaghi R., Savarese S. (2014) Beyond PASCAL:
    A benchmark for 3D object detection in the wild. In: WACV, pp. 75–82'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang 等人 (2014) Xiang Y., Mottaghi R., Savarese S. (2014) 超越 PASCAL：一个用于野外 3D
    目标检测的基准。见：WACV，第 75–82 页
- en: 'Xiao et al. (2003) Xiao R., Zhu L., Zhang H. (2003) Boosting chain learning
    for object detection. In: ICCV, pp. 709–715'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人 (2003) Xiao R., Zhu L., Zhang H. (2003) 用于目标检测的链式提升学习。见：ICCV，第 709–715
    页
- en: 'Xie et al. (2017) Xie S., Girshick R., Dollár P., Tu Z., He K. (2017) Aggregated
    residual transformations for deep neural networks. In: CVPR'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人 (2017) Xie S., Girshick R., Dollár P., Tu Z., He K. (2017) 用于深度神经网络的聚合残差变换。见：CVPR
- en: 'Yang et al. (2016a) Yang B., Yan J., Lei Z., Li S. (2016a) CRAFT objects from
    images. In: CVPR, pp. 6043–6051'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 (2016a) Yang B., Yan J., Lei Z., Li S. (2016a) 从图像中提取 CRAFT 对象。见：CVPR，第
    6043–6051 页
- en: 'Yang et al. (2016b) Yang F., Choi W., Lin Y. (2016b) Exploit all the layers:
    Fast and accurate CNN object detector with scale dependent pooling and cascaded
    rejection classifiers. In: CVPR, pp. 2129–2137'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 (2016b) Yang F., Choi W., Lin Y. (2016b) 利用所有层：具有尺度依赖池化和级联拒绝分类器的快速准确
    CNN 目标检测器。见：CVPR，第 2129–2137 页
- en: 'Yang et al. (2002) Yang M., Kriegman D., Ahuja N. (2002) Detecting faces in
    images: A survey. IEEE TPAMI 24(1):34–58'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 (2002) Yang M., Kriegman D., Ahuja N. (2002) 图像中的人脸检测：调查。IEEE TPAMI
    24(1):34–58
- en: 'Ye and Doermann (2015) Ye Q., Doermann D. (2015) Text detection and recognition
    in imagery: A survey. IEEE TPAMI 37(7):1480–1500'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 和 Doermann (2015) Ye Q., Doermann D. (2015) 图像中的文本检测和识别：调查。IEEE TPAMI 37(7):1480–1500
- en: 'Yosinski et al. (2014) Yosinski J., Clune J., Bengio Y., Lipson H. (2014) How
    transferable are features in deep neural networks? In: NIPS, pp. 3320–3328'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yosinski 等人 (2014) Yosinski J., Clune J., Bengio Y., Lipson H. (2014) 深度神经网络中的特征可迁移性。见：NIPS，第
    3320–3328 页
- en: Young et al. (2018) Young T., Hazarika D., Poria S., Cambria E. (2018) Recent
    trends in deep learning based natural language processing. IEEE Computational
    Intelligence Magazine 13(3):55–75
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Young 等人 (2018) Young T., Hazarika D., Poria S., Cambria E. (2018) 基于深度学习的自然语言处理的最新趋势。IEEE
    计算智能杂志 13(3):55–75
- en: Yu and Koltun (2016) Yu F., Koltun V. (2016) Multiscale context aggregation
    by dilated convolutions
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 和 Koltun (2016) Yu F., Koltun V. (2016) 通过扩张卷积进行多尺度上下文聚合
- en: 'Yu et al. (2017) Yu F., Koltun V., Funkhouser T. (2017) Dilated residual networks.
    In: CVPR, vol 2, p. 3'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 (2017) Yu F., Koltun V., Funkhouser T. (2017) 扩张残差网络。见：CVPR，第 2 卷，第 3
    页
- en: 'Yu et al. (2018) Yu R., Li A., Chen C., Lai J., et al. (2018) NISP: Pruning
    networks using neuron importance score propagation. CVPR'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等人 (2018) Yu R., Li A., Chen C., Lai J., 等 (2018) NISP: 使用神经元重要性评分传播来修剪网络。CVPR'
- en: 'Zafeiriou et al. (2015) Zafeiriou S., Zhang C., Zhang Z. (2015) A survey on
    face detection in the wild: Past, present and future. Computer Vision and Image
    Understanding 138:1–24'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zafeiriou 等人 (2015) Zafeiriou S., Zhang C., Zhang Z. (2015) 野外人脸检测调查：过去、现在和未来。计算机视觉与图像理解
    138:1–24
- en: 'Zagoruyko et al. (2016) Zagoruyko S., Lerer A., Lin T., Pinheiro P., Gross
    S., Chintala S., Dollár P. (2016) A multipath network for object detection. In:
    BMVC'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zagoruyko 等人 (2016) Zagoruyko S., Lerer A., Lin T., Pinheiro P., Gross S., Chintala
    S., Dollár P. (2016) 一种用于目标检测的多路径网络。见：BMVC
- en: 'Zeiler and Fergus (2014) Zeiler M., Fergus R. (2014) Visualizing and understanding
    convolutional networks. In: ECCV, pp. 818–833'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler 和 Fergus (2014) Zeiler M., Fergus R. (2014) 视觉化和理解卷积网络。见：ECCV，第 818–833
    页
- en: 'Zeng et al. (2016) Zeng X., Ouyang W., Yang B., Yan J., Wang X. (2016) Gated
    bidirectional cnn for object detection. In: ECCV, pp. 354–369'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng等（2016）Zeng X.，Ouyang W.，Yang B.，Yan J.，Wang X.（2016）用于目标检测的门控双向CNN。发表于：ECCV，第354–369页
- en: Zeng et al. (2017) Zeng X., Ouyang W., Yan J., Li H., Xiao T., Wang K., Liu
    Y., Zhou Y., Yang B., Wang Z., Zhou H., Wang X. (2017) Crafting gbdnet for object
    detection. IEEE TPAMI
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng等（2017）Zeng X.，Ouyang W.，Yan J.，Li H.，Xiao T.，Wang K.，Liu Y.，Zhou Y.，Yang
    B.，Wang Z.，Zhou H.，Wang X.（2017）为目标检测打造GBDNet。IEEE TPAMI
- en: Zhang et al. (2016a) Zhang K., Zhang Z., Li Z., Qiao Y. (2016a) Joint face detection
    and alignment using multitask cascaded convolutional networks. IEEE SPL 23(10):1499–1503
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2016a）Zhang K.，Zhang Z.，Li Z.，Qiao Y.（2016a）利用多任务级联卷积网络进行联合人脸检测与对齐。IEEE
    SPL 23(10)：1499–1503
- en: 'Zhang et al. (2016b) Zhang L., Lin L., Liang X., He K. (2016b) Is faster RCNN
    doing well for pedestrian detection? In: ECCV, pp. 443–457'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2016b）Zhang L.，Lin L.，Liang X.，He K.（2016b）Faster RCNN在行人检测中表现如何？发表于：ECCV，第443–457页
- en: 'Zhang et al. (2018a) Zhang S., Wen L., Bian X., Lei Z., Li S. (2018a) Single
    shot refinement neural network for object detection. In: CVPR'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2018a）Zhang S.，Wen L.，Bian X.，Lei Z.，Li S.（2018a）用于目标检测的单次修正神经网络。发表于：CVPR
- en: 'Zhang et al. (2018b) Zhang S., Yang J., Schiele B. (2018b) Occluded pedestrian
    detection through guided attention in CNNs. In: CVPR, pp. 2056–2063'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2018b）Zhang S.，Yang J.，Schiele B.（2018b）通过CNN中的引导注意力进行遮挡行人检测。发表于：CVPR，第2056–2063页
- en: 'Zhang et al. (2013) Zhang X., Yang Y., Han Z., Wang H., Gao C. (2013) Object
    class detection: A survey. ACM Computing Surveys 46(1):10:1–10:53'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2013）Zhang X.，Yang Y.，Han Z.，Wang H.，Gao C.（2013）目标类别检测：综述。ACM计算调查 46(1)：10:1–10:53
- en: 'Zhang et al. (2017) Zhang X., Li Z., Change Loy C., Lin D. (2017) PolyNet:
    a pursuit of structural diversity in very deep networks. In: CVPR, pp. 718–726'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2017）Zhang X.，Li Z.，Change Loy C.，Lin D.（2017）PolyNet：在非常深的网络中追求结构多样性。发表于：CVPR，第718–726页
- en: 'Zhang et al. (2018c) Zhang X., Zhou X., Lin M., Sun J. (2018c) ShuffleNet:
    an extremely efficient convolutional neural network for mobile devices. In: CVPR'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等（2018c）Zhang X.，Zhou X.，Lin M.，Sun J.（2018c）ShuffleNet: 一种针对移动设备的极其高效的卷积神经网络。发表于：CVPR'
- en: 'Zhang et al. (2018d) Zhang Z., Geiger J., Pohjalainen J., Mousa A. E., Jin
    W., Schuller B. (2018d) Deep learning for environmentally robust speech recognition:
    An overview of recent developments. ACM Trans Intell Syst Technol 9(5):49:1–49:28'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2018d）Zhang Z.，Geiger J.，Pohjalainen J.，Mousa A. E.，Jin W.，Schuller B.（2018d）环境鲁棒语音识别的深度学习：近期发展的概述。ACM智能系统技术学报
    9(5)：49:1–49:28
- en: 'Zhang et al. (2018e) Zhang Z., Qiao S., Xie C., Shen W., Wang B., Yuille A.
    (2018e) Single shot object detection with enriched semantics. In: CVPR'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2018e）Zhang Z.，Qiao S.，Xie C.，Shen W.，Wang B.，Yuille A.（2018e）具有丰富语义的单次目标检测。发表于：CVPR
- en: 'Zhao et al. (2019) Zhao Q., Sheng T., Wang Y., Tang Z., Chen Y., Cai L., Ling
    H. (2019) M2Det: A single shot object detector based on multilevel feature pyramid
    network. In: AAAI'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao等（2019）Zhao Q.，Sheng T.，Wang Y.，Tang Z.，Chen Y.，Cai L.，Ling H.（2019）M2Det:
    基于多级特征金字塔网络的单次目标检测器。发表于：AAAI'
- en: 'Zheng et al. (2015) Zheng S., Jayasumana S., Romera-Paredes B., Vineet V.,
    Su Z., Du D., Huang C., Torr P. (2015) Conditional random fields as recurrent
    neural networks. In: ICCV, pp. 1529–1537'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等（2015）Zheng S.，Jayasumana S.，Romera-Paredes B.，Vineet V.，Su Z.，Du D.，Huang
    C.，Torr P.（2015）条件随机场作为递归神经网络。发表于：ICCV，第1529–1537页
- en: 'Zhou et al. (2015) Zhou B., Khosla A., Lapedriza A., Oliva A., Torralba A.
    (2015) Object detectors emerge in deep scene CNNs. In: ICLR'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等（2015）Zhou B.，Khosla A.，Lapedriza A.，Oliva A.，Torralba A.（2015）在深度场景CNN中出现的目标检测器。发表于：ICLR
- en: 'Zhou et al. (2016a) Zhou B., Khosla A., Lapedriza A., Oliva A., Torralba A.
    (2016a) Learning deep features for discriminative localization. In: CVPR, pp.
    2921–2929'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等（2016a）Zhou B.，Khosla A.，Lapedriza A.，Oliva A.，Torralba A.（2016a）学习深度特征以进行判别性定位。发表于：CVPR，第2921–2929页
- en: 'Zhou et al. (2017a) Zhou B., Lapedriza A., Khosla A., Oliva A., Torralba A.
    (2017a) Places: A 10 million image database for scene recognition. IEEE Trans
    Pattern Analysis and Machine Intelligence'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等（2017a）Zhou B.，Lapedriza A.，Khosla A.，Oliva A.，Torralba A.（2017a）Places：一个用于场景识别的1000万图像数据库。IEEE模式分析与机器智能
- en: 'Zhou et al. (2018a) Zhou J., Cui G., Zhang Z., Yang C., Liu Z., Sun M. (2018a)
    Graph neural networks: A review of methods and applications. arXiv preprint arXiv:181208434'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等（2018a）Zhou J.，Cui G.，Zhang Z.，Yang C.，Liu Z.，Sun M.（2018a）图神经网络：方法与应用的综述。arXiv预印本
    arXiv:181208434
- en: 'Zhou et al. (2018b) Zhou P., Ni B., Geng C., Hu J., Xu Y. (2018b) Scale transferrable
    object detection. In: CVPR'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等（2018b）Zhou P.，Ni B.，Geng C.，Hu J.，Xu Y.（2018b）尺度可转移目标检测。发表于：CVPR
- en: 'Zhou et al. (2016b) Zhou Y., Liu L., Shao L., Mellor M. (2016b) DAVE: A unified
    framework for fast vehicle detection and annotation. In: ECCV, pp. 278–293'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2016b）Zhou Y., Liu L., Shao L., Mellor M.（2016b）《DAVE：用于快速车辆检测和标注的统一框架》。发表于：ECCV，第
    278–293 页
- en: 'Zhou et al. (2017b) Zhou Y., Ye Q., Qiu Q., Jiao J. (2017b) Oriented response
    networks. In: CVPR, pp. 4961–4970'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2017b）Zhou Y., Ye Q., Qiu Q., Jiao J.（2017b）《定向响应网络》。发表于：CVPR，第 4961–4970
    页
- en: Zhu et al. (2016a) Zhu X., Vondrick C., Fowlkes C., Ramanan D. (2016a) Do we
    need more training data? IJCV 119(1):76–92
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2016a）Zhu X., Vondrick C., Fowlkes C., Ramanan D.（2016a）《我们是否需要更多的训练数据？》IJCV
    119(1):76–92
- en: 'Zhu et al. (2017) Zhu X., Tuia D., Mou L., Xia G., Zhang L., Xu F., Fraundorfer
    F. (2017) Deep learning in remote sensing: A comprehensive review and list of
    resources. IEEE Geoscience and Remote Sensing Magazine 5(4):8–36'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2017）Zhu X., Tuia D., Mou L., Xia G., Zhang L., Xu F., Fraundorfer F.（2017）《遥感中的深度学习：全面回顾及资源清单》。IEEE
    地球科学与遥感杂志 5(4):8–36
- en: 'Zhu et al. (2015) Zhu Y., Urtasun R., Salakhutdinov R., Fidler S. (2015) SegDeepM:
    Exploiting segmentation and context in deep neural networks for object detection.
    In: CVPR, pp. 4703–4711'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2015）Zhu Y., Urtasun R., Salakhutdinov R., Fidler S.（2015）《SegDeepM：在深度神经网络中利用分割和上下文进行物体检测》。发表于：CVPR，第
    4703–4711 页
- en: 'Zhu et al. (2017a) Zhu Y., Zhao C., Wang J., Zhao X., Wu Y., Lu H. (2017a)
    CoupleNet: Coupling global structure with local parts for object detection. In:
    ICCV'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2017a）Zhu Y., Zhao C., Wang J., Zhao X., Wu Y., Lu H.（2017a）《CoupleNet：将全局结构与局部部分结合进行物体检测》。发表于：ICCV
- en: 'Zhu et al. (2017b) Zhu Y., Zhou Y., Ye Q., Qiu Q., Jiao J. (2017b) Soft proposal
    networks for weakly supervised object localization. In: ICCV, pp. 1841–1850'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2017b）Zhu Y., Zhou Y., Ye Q., Qiu Q., Jiao J.（2017b）《用于弱监督物体定位的软提案网络》。发表于：ICCV，第
    1841–1850 页
- en: 'Zhu et al. (2016b) Zhu Z., Liang D., Zhang S., Huang X., Li B., Hu S. (2016b)
    Traffic sign detection and classification in the wild. In: CVPR, pp. 2110–2118'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2016b）Zhu Z., Liang D., Zhang S., Huang X., Li B., Hu S.（2016b）《野外交通标志检测与分类》。发表于：CVPR，第
    2110–2118 页
- en: 'Zitnick and Dollár (2014) Zitnick C., Dollár P. (2014) Edge boxes: Locating
    object proposals from edges. In: ECCV, pp. 391–405'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zitnick 和 Dollár（2014）Zitnick C., Dollár P.（2014）《边缘盒：从边缘定位物体提案》。发表于：ECCV，第
    391–405 页
- en: Zoph and Le (2017) Zoph B., Le Q. (2017) Neural architecture search with reinforcement
    learning
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoph 和 Le（2017）Zoph B., Le Q.（2017）《通过强化学习进行神经架构搜索》
- en: 'Zoph et al. (2018) Zoph B., Vasudevan V., Shlens J., Le Q. (2018) Learning
    transferable architectures for scalable image recognition. In: CVPR, pp. 8697–8710'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoph 等人（2018）Zoph B., Vasudevan V., Shlens J., Le Q.（2018）《学习可转移的架构以实现可扩展的图像识别》。发表于：CVPR，第
    8697–8710 页
