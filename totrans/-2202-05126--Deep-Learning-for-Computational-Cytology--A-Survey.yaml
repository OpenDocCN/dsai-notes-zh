- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:48:05'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2202.05126] Deep Learning for Computational Cytology: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2202.05126](https://ar5iv.labs.arxiv.org/html/2202.05126)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Computational Cytology: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hao Jiang Yanning Zhou Yi Lin Ronald CK Chan Jiang Liu Hao Chen Department of
    Computer Science and Engineering, The Hong Kong University of Science and Technology,
    Hong Kong, China Department of Computer Science and Engineering, The Chinese University
    of Hong Kong, Hong Kong, China Department of Anatomical and Cellular Pathology,
    The Chinese University of Hong Kong, Hong Kong, China School of Computer Science
    and Engineering, Southern University of Science and Technology, Shenzhen, China
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Computational cytology is a critical, rapid-developing, yet challenging topic
    in the field of medical image computing which analyzes the digitized cytology
    image by computer-aided technologies for cancer screening. Recently, an increasing
    number of deep learning (DL) algorithms have made significant progress in medical
    image analysis, leading to the boosting publications of cytological studies. To
    investigate the advanced methods and comprehensive applications, we survey more
    than 120 publications of DL-based cytology image analysis in this article. We
    first introduce various deep learning methods, including fully supervised, weakly
    supervised, unsupervised, and transfer learning. Then, we systematically summarize
    the public datasets, evaluation metrics, versatile cytology image analysis applications
    including classification, detection, segmentation, and other related tasks. Finally,
    we discuss current challenges and potential research directions of computational
    cytology.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '\KWDArtificial Intelligence, Deep Learning, Computational Cytology, Pathology,
    Cancer Screening, Survey^†^†journal: XXX'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cytology is a branch of pathology to study the cells under microscopes to analyze
    the cellular morphology, and compositions, usually for cancer screening [[31](#bib.bib31),
    [1](#bib.bib1), [118](#bib.bib118)]. Compared with histopathology, cytology focuses
    on the pathological characteristics of cells instead of tissues, which is a collection
    of thousands of cells in a specific architecture [[112](#bib.bib112), [34](#bib.bib34)].
    Cells being the structural and functional unit of living organisms [[2](#bib.bib2)],
    their morphologies reflect the biological of the organ and even the body [[62](#bib.bib62),
    [147](#bib.bib147), [61](#bib.bib61)]. The clinical cytology testing procedure
    can be divided into collection and preservation, centrifugation, slide making,
    and staining (Fig. [1](#S1.F1 "Fig. 1 ‣ 1 Introduction ‣ Deep Learning for Computational
    Cytology: A Survey")(A)). For cytology screening, cytologists observe cytology
    slides under microscopes and analyze the properties and morphologies of cells
    (Fig. [1](#S1.F1 "Fig. 1 ‣ 1 Introduction ‣ Deep Learning for Computational Cytology:
    A Survey")(B)). These slides can be also scanned into whole slide images (WSI)
    (Fig. [1](#S1.F1 "Fig. 1 ‣ 1 Introduction ‣ Deep Learning for Computational Cytology:
    A Survey")(C)) for further digital analysis and processing. In addition, there
    are three types of cytology specimens, depending on the collection techniques:
    1) Exfoliative cytology, including sputum, urine sediment, pleural eﬀusion, and
    ascites [[100](#bib.bib100)]. 2) Abrasive cytology, including cervical scraping,
    gastrointestinal tract, and endoscopic brushing [[126](#bib.bib126)]. 3) Aspiration
    cytology, also named fine-needle aspiration cell inspection (FNAC) [[78](#bib.bib78)]
    usually from breast, thyroid, and lung [[72](#bib.bib72), [59](#bib.bib59)]. Unlike
    histology, cytology specimens do not require removal of intact sizable tissue,
    allowing much less invasive sampling procedures. Therefore, the sampling is frequently
    painless, low-cost, and equipment-undemanding, making cytology useful for cancer
    screening and early diagnosis [[68](#bib.bib68)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/11ec89ff42db910925f5ba2c906c51c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: Illustration of clinical cytology screening. (A) The procedure of cytology
    specimen preparation (taking the cervix as an example). (B) Cytology images in
    different categories. (C) WSI for digital analysis and processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Diagnosing cytological specimens is a highly professional task, requiring formal
    training and assessments overseen by international bodies. Currently, there is
    a trend to standardize cytological diagnosis reporting, allowing reports and implied
    risk of cancers to be understood by clinicians without ambiguity [[11](#bib.bib11)].
    It also provides well-defined features to be looked for among the cells being
    examined. Fig. [2](#S1.F2 "Fig. 2 ‣ 1 Introduction ‣ Deep Learning for Computational
    Cytology: A Survey") illustrates typical cytological morphologies of commonly
    encountered specimens. Specifically, cytology screening was ﬁrst applied in cervix
    cancers almost 100 years ago. Present reporting of cervical cytology is guided
    by the Bethesda system [[113](#bib.bib113)]. Pre-cancerous and cancerous cells
    are first categorized into squamous cells and glandular cells, then they can be
    identified and graded by combinations of cytological features including enlarged
    nuclei, multinucleation, perinuclear halo, increased nuclear to cytoplasm ratio,
    wrinkled nuclear membrane, dyskeratosis (abnormal keratin formation), prominent
    nucleoli and tumor diathesis. Following the success of Bethesda system, present
    reporting of breast aspiration cytology is guided by [[42](#bib.bib42)]. Aspiration
    of malignant breast lesions are often hypercellular and the cancer cells possess
    large sometimes irregular nuclei with prominent nucleoli and the lack of myoepithelial
    cells. Different grades of atypia were also established to estimate the risk of
    malignancy [[14](#bib.bib14)]. Similarly, bladder cancer cells from urine often
    show irregular nuclei with very high N/C ratio ($>$0.7), prominent nucleoli and
    clumped/coarse chromatin, as outlined in [[135](#bib.bib135)]. Thyroid cancer
    aspirates show distinctive features including papillary structures, psammoma bodies
    and optically clear nuclei with nuclear grooves and nuclear inclusions [[30](#bib.bib30)].
    Together with examples from lung and oral mucosa illustrated in Fig. [2](#S1.F2
    "Fig. 2 ‣ 1 Introduction ‣ Deep Learning for Computational Cytology: A Survey"),
    cytological morphology from diﬀerent organs provides diagnostic information for
    cancer screening and guide patient management at a minimal cost [[18](#bib.bib18),
    [69](#bib.bib69)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bf78ae09c897e5cee16bb528a70835fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: Typical cytological morphologies of commonly encountered specimens,
    including (A) Cervix [[195](#bib.bib195)], (B) Breast [[139](#bib.bib139)], (C)
    Urine [[7](#bib.bib7)], (D) Thyroid [[38](#bib.bib38)], (E) Lung [[161](#bib.bib161)],
    (F) Oral [[105](#bib.bib105)]. Single cellular structures are zoomed by red boxes.'
  prefs: []
  type: TYPE_NORMAL
- en: In clinical cytology screening, scrutinizing every cell under the microscope
    (or in gigapixel whole slide images) in search of malignancy can be very time-consuming
    and tedious for cytologists [[106](#bib.bib106), [32](#bib.bib32)]. Considering
    ever increase in caseload, researchers have attempted to develop automatic methods
    for accurate and eﬃcient cancer screening. The ﬁrst successful trial could date
    back to the 1950s when an automatic screening system was developed for cervix
    [[167](#bib.bib167), [166](#bib.bib166)]. Afterwards, a series of cervical screening
    systems were launched with varying market success [[71](#bib.bib71), [175](#bib.bib175),
    [17](#bib.bib17)]. These automated cytology screening systems have been shown
    to improve the eﬃciency without compromising accuracy of cytology screening procedures.
  prefs: []
  type: TYPE_NORMAL
- en: In recent decades, the automation technology and artiﬁcial intelligence (AI)
    have achieved remarkable progress in the ﬁeld of medicine. Machine learning (ML),
    which is a subfield of AI, focuses on learning algorithms to represent the underlying
    patterns of data by imitating human beings. With rapid progress in ML, medical
    image interpretation and computer assisted-diagnosis in pathology (e.g., histopathology,
    cytology [[24](#bib.bib24), [191](#bib.bib191), [199](#bib.bib199)]), and radiology
    (e.g., computed tomography (CT), magnetic resonance imaging (MRI), X-ray, ultrasound
    [[76](#bib.bib76), [63](#bib.bib63), [19](#bib.bib19)]). For cytology, previous
    studies demonstrated the feasibility of various ML approaches in cytology image
    analysis, including support vector machine (SVM), fuzzy c-means (FCM) [[22](#bib.bib22)],
    k-means [[58](#bib.bib58)], and fuzzy clustering [[122](#bib.bib122)]. However,
    there remains challenges in these machine learning algorithms, such as developing
    accurate and efficient cytology image analysis approaches, establishing human-machine
    collaborative cytological screening systems.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning, as a branch of the ML family, was developed with multilayers
    neural networks for leveraging feature representations of input data. DL aims
    to reduce the heavy reliance on task-related features designed from expert knowledge
    in traditional ML approaches. It can also increase the model’s capability of feature
    representation by end-to-end learning. In computational cytology, DL could provide
    cytologists with feasible solutions for accurate and efficient cytological screening.
    These approaches have been widely investigated in versatile types of cancers,
    such as cervix [[124](#bib.bib124)], breast [[43](#bib.bib43)], bladder [[38](#bib.bib38)],
    and lung [[161](#bib.bib161)]. Among these DL-based methods, supervised learning
    involves mapping input images to predeﬁned labels and it has been the most commonly
    developed DL scheme. Most existing studies on cytological applications focus on
    improving DL models performance by introducing specific constrains or architecture
    designs of DL models, such as introducing morphological constraints [[196](#bib.bib196),
    [21](#bib.bib21), [25](#bib.bib25)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The advancement of DL has greatly accelerated the development of computational
    cytology. There is a 10-fold increase in DL-based computational research from
    2014 to 2021 with a booming trend in recent two years. The number of related publications
    is illustrated in Fig. [3](#S1.F3 "Fig. 3 ‣ 1 Introduction ‣ Deep Learning for
    Computational Cytology: A Survey"), after searching literature databases (Google
    Scholar, PubMed, and arXiv). These publications focus on developing various DL
    approaches for cytological screening, such as classifying between normal and abnormal
    cells [[189](#bib.bib189)], locating and identifying cells in cytological smears
    [[121](#bib.bib121)], and segmentation of different cellular compartments [[194](#bib.bib194)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59852a86492babb146f434da435dfcb4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: Number of publications in deep learning-based computational cytology
    of classification, detection, segmentation and other tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: There exists several surveys in the field of cytology image analysis [[74](#bib.bib74),
    [109](#bib.bib109), [124](#bib.bib124)]. However, these reviews were far from
    exhaustive in terms of the advanced algorithms, publicly available datasets, and
    promising trends in this field. Besides, most of the DL-based cytology surveys
    focused on the cervix, ignoring the progress in other types of cancer, such as
    lung and bladder [[124](#bib.bib124)]. Afterwards, [[109](#bib.bib109)] focused
    on various cytology applications instead of analyzing them in the DL methodology
    perspective. In this paper, we have surveyed over 120 publications since 2014,
    and systematically reviewed the progress of DL approaches and techniques in computational
    cytology, also covering cytology specimens from various parts of the body.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are six sections in this paper. Section [1](#S1 "1 Introduction ‣ Deep
    Learning for Computational Cytology: A Survey") briefly introduces the background
    and objective of this review. Section [2](#S2 "2 Deep learning methodology ‣ Deep
    Learning for Computational Cytology: A Survey") gives an overview of different
    learning approaches in the context of computational cytology. Section [3](#S3
    "3 Datasets and metrics ‣ Deep Learning for Computational Cytology: A Survey")
    summarizes public cytology datasets and common evaluation metrics. Section [4](#S4
    "4 Deep learning in cytology application ‣ Deep Learning for Computational Cytology:
    A Survey") presents the progress and achievements on the DL-based cytology image
    analysis. Section [5](#S5 "5 Challenges and Promises ‣ Deep Learning for Computational
    Cytology: A Survey") discusses existing challenges and potential research directions
    in computational cytology. Section [6](#S6 "6 Conclusion ‣ Deep Learning for Computational
    Cytology: A Survey") concludes this survey paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep learning methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present the definition, formulations, and general procedures
    of DL methods, which can be developed for various cytological applications. According
    to the availability of annotations, DL can be categorized as supervised learning
    (section [2.1](#S2.SS1 "2.1 Supervised learning ‣ 2 Deep learning methodology
    ‣ Deep Learning for Computational Cytology: A Survey")), weakly supervised learning
    (section [2.2](#S2.SS2 "2.2 Weakly supervised learning ‣ 2 Deep learning methodology
    ‣ Deep Learning for Computational Cytology: A Survey")), unsupervised learning
    (section [2.3](#S2.SS3 "2.3 Unsupervised learning ‣ 2 Deep learning methodology
    ‣ Deep Learning for Computational Cytology: A Survey")), together with transfer
    learning (section [2.4](#S2.SS4 "2.4 Transfer Learning ‣ 2 Deep learning methodology
    ‣ Deep Learning for Computational Cytology: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f95d7ec46284f07dd9d087dcfd99bf87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 4: Standard workflow of DL-based supervised learning for cytological classification.
    (A) Cell-level: Firstly, ROIs are extracted from WSIs and cut into cell patches,
    then they are input into the DNN for extracting features and predicting category
    of each cell patch. (B) Slide-level: Patches cut from WSIs are input into a DNN
    to obtain multi-level predictions (e.g., instance-level and patch-level), then
    these predictions are used to predict final WSI-level results.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised learning aims to learn functional mappings between input data and
    corresponding labels. For medical image analysis, the inputs are medical images,
    while labels are varied according to different tasks, e.g., image-level categories
    for classification, object-level localizations (e.g., boxes, points) for detection,
    and pixel-wise masks for segmentation. Formally, the input images $X=\{x_{i}\}_{i=1}^{N}$
    together with corresponding labels $Y=\{y_{i}\}_{i=1}^{N}$ are used to train a
    predictive model by minimizing the objective function. The typical deep models
    in supervised learning include multilayer perceptron (MLP) [[134](#bib.bib134)],
    convolutional neural network (CNN) [[77](#bib.bib77)], recurrent neural network
    (RNN) [[184](#bib.bib184)], and transformer [[160](#bib.bib160)].
  prefs: []
  type: TYPE_NORMAL
- en: 'CNN is regarded as the most successful DL architecture in image analysis [[90](#bib.bib90)].
    It mainly consists of three types of hidden layers: convolutional layers for feature
    extraction, pooling layers for reducing the feature resolution, and fully connected
    layers for compiling the features extracted by previous layers and outputting
    prediction results. Then, backpropagation algorithm is introduced to update parameters
    of different layers during training [[77](#bib.bib77)]. Due to the strategies
    of local receptive fields, shared weights, and downsampling in pooling layers,
    CNN has achieved great success in many image analysis tasks, such as autonomous
    driving [[136](#bib.bib136)], face recognition [[173](#bib.bib173)], and biomedicine
    [[90](#bib.bib90)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Commonly-used CNN architectures in computer vision fields have been employed
    and developed for various cytological applications, e.g., AlexNet [[110](#bib.bib110)],
    VGGNet [[3](#bib.bib3)], ResNet [[108](#bib.bib108)]. Currently, most cytology
    researches focus on developing new algorithms based on these basic architectures
    in various DL tasks: classification, detection, and segmentation [[87](#bib.bib87),
    [153](#bib.bib153), [57](#bib.bib57)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification. This classification model aims at predicting the category of
    the input image. It can be essentially formulated as $\hat{y}=f(x,\theta)$, where
    $x$ and $\hat{y}$ is input image and its predicted category, and $\theta$ represents
    learnable parameters of classification architecture. For training these architectures,
    cross-entropy loss $L_{CE}$ measures the discrepancy between predicted probability
    $\hat{y_{ic}}$ and true label $y_{ic}$ by probability distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{CE}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{M}y_{ic}\log\left(\hat{y}_{ic}\right)$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where, $N$ is the amount of image samples, and $M$ is the number of categories.
    Then, the prediction loss is used to optimize parameters $\theta$ of network by
    backpropagation [[137](#bib.bib137)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, the label space of the cytology image refers to its benign/malignant
    or sub-category. In the standard cell-level classification workflow, regions of
    interest (ROIs) are extracted from collected slides by cytologists or technicians.
    Then, ROIs are cut into cell patches as input of deep models. After that, a DL-based
    feature extractor is responsible for representing high-level features and outputting
    prediction categories (Fig. [4](#S2.F4 "Fig. 4 ‣ 2 Deep learning methodology ‣
    Deep Learning for Computational Cytology: A Survey")). For slide-level screening,
    existing studies mainly divide this task into two stages: First, the deep neural
    network (e.g., CNN, RNN, and Transformer) is responsible for predicting multi-level
    results, such as detection of malignant or benign cells and the category of patches.
    Then, another network aggregates these results and predicts the final WSI-level
    results [[174](#bib.bib174), [86](#bib.bib86)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Detection. Unlike classification, the detection task is to locate objects from
    whole images and predict categories of these objects. Thus, it can be regarded
    as the combination of two tasks: regressing the object’s location and classifying
    the types of objects. CNN-based object detection algorithms are mainly divided
    into two categories: two-stage method and one-stage method. In two-stage, the
    workflow includes feature extraction, region proposal, and prediction. The first
    stage is to regress coarse prediction (box location and predicted probability)
    by region proposal. The second stage aims to output fine predictions (box location
    and object category). Typical models for two-stage algorithms include R-CNN [[45](#bib.bib45)],
    Fast R-CNN [[44](#bib.bib44)], Faster R-CNN [[128](#bib.bib128)]. One-stage detection
    methods aim to abandon the strategy of region proposal. Instead, they directly
    predict the category and location of the objects in an end-to-end architecture,
    including SSD [[92](#bib.bib92)], YOLO [[127](#bib.bib127)], FCOS [[165](#bib.bib165)]
    and RetinaNet [[89](#bib.bib89)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of loss functions in object detection task, classification
    loss and location loss. For classification loss, by improving $L_{CE}$, focal
    loss $L_{focal}$ is proposed in RetinaNet to balance classification samples [[89](#bib.bib89)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{focal}=\left\{\begin{array}[]{ccc}-\alpha(1-p)^{\gamma}\log(p),&amp;\text{if}&amp;y=1\\
    -(1-\alpha)p^{\gamma}\log(1-p),&amp;\text{if}&amp;y=0\end{array}\right.$ |  |
    (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $p$ is the model’s estimated probability with label $y$, and $\gamma$
    and $\alpha$ are tunable parameters. For location loss, mean absolute error loss
    $L_{MAE}$ calculates the average distance between the predicted and the true locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{MAE}=\frac{\sum_{i=1}^{N}\left&#124;f\left(x_{i}\right)-y_{i}\right&#124;}{N}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Then, intersection over union (IoU) loss was introduced to calculate the loss
    of predicted boxes instead of coordinates in $L_{MAE}$ [[182](#bib.bib182)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{IoU}=-\ln\frac{\text{Intersection}\left(box_{gt},box_{p}\right)}{\text{
    Union }\left(box_{gt},box_{p}\right)}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $box_{gt}$ and $box_{p}$ represent ground truth box and predicted box,
    respectively. After that, some advanced loss functions are designed recently,
    including GIoU loss for non-intersection area [[129](#bib.bib129)], CIoU loss
    for closing center points [[192](#bib.bib192)], etc.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are still someFor example, fully common issues for both one-stage
    and two-stage algorithms. For example, multiple overlapping predicted boxes in
    the prediction results. These repetitive and redundant proposals can be removed
    by the strategy of non-maximum suppression [[114](#bib.bib114)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Segmentation. This is a fundamental and essential task in medical image analysis.
    Segmentation is to make the pixel-wise prediction which represents the morphology
    of biomedical structures, such as cell [[194](#bib.bib194)], gland [[26](#bib.bib26)],
    and organ [[124](#bib.bib124)]. According to whether to distinguish each instance
    object, DL-based segmentation models can be divided into two branches: semantic
    segmentation and instance segmentation. Semantic segmentation aims to predict
    the category of each pixel to obtain masks of objects, which can be regarded as
    a pixel-wise classification task. Fully convolutional network (FCN) is one of
    the successful segmentation architectures, which replaced the fully connected
    layers in traditional CNN with convolutional layers for outputting segmentation
    map [[93](#bib.bib93)]. Then, [[41](#bib.bib41)] proposed U-Net for biomedical
    image segmentation by multi-scale feature fusion with a downsampling-upsampling
    architecture. In instance segmentation, models not only segment pixels into categories
    but also assigned them corresponding instance ID. Its popular structures mainly
    follow the detect-then-segment pipeline (Fig. [5](#S2.F5 "Fig. 5 ‣ 2.1 Supervised
    learning ‣ 2 Deep learning methodology ‣ Deep Learning for Computational Cytology:
    A Survey")). For instance, Mask R-CNN was proposed by introducing a mask prediction
    head based on Faster R-CNN [[51](#bib.bib51)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/af40f26a9365e86ed7235cc49b5ef848.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 5: Instance segmentation in cytology image analysis by Mask R-CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When training mentioned semantic and instance segmentation models, classification
    loss $L_{CE}$ is introduced as the pixel-wise classification supervision for segmentation.
    Another widely-used loss function, dice coefficient loss $L_{dice}$ is designed
    to measure the similarity between predicted masks $Y_{m}$ and ground truth $Y_{gt}$
    by calculating the dice coefficient, and is defined as [[107](#bib.bib107)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{dice}=1-\frac{2&#124;Y_{m}\cap Y_{gt}&#124;}{&#124;Y_{m}&#124;+&#124;Y_{gt}&#124;}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 2.2 Weakly supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Weakly supervised learning is proposed for the scenarios where labels are not
    fully available, including incomplete supervision, inaccurate supervision, and
    inexact supervision [[197](#bib.bib197)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/528b0924789c3074d708c4a1a77a6eab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 6: Overview of multiple instance learning paradigm. Original WSIs are
    extracted to ROIs and cut into patches, then they are formed instance bags with
    bag-level label (positive or negative). For learning algorithms, these bags are
    used to learn a classification function that can predict the labels of bags and
    instances in the testing data.'
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised learning is one successful learning paradigm of incomplete supervision.
    It leverages both labeled and unlabeled data by extracting hidden information
    in unlabeled sets to enhance the feature representation of the labeled set. For
    inaccurate supervision, methods for noisy label problem can identify the potentially
    mislabeled samples and make corrections, thus improving the reliability of supervision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-instance learning (MIL) is an effective inexact supervision method, which
    aims at utilizing coarse-level annotations (e.g., image-level) for learning fine-level
    (e.g., pixel-level, patch-level) tasks [[103](#bib.bib103), [179](#bib.bib179)].
    The standard workflow of MIL is illustrated in Fig. [6](#S2.F6 "Fig. 6 ‣ 2.2 Weakly
    supervised learning ‣ 2 Deep learning methodology ‣ Deep Learning for Computational
    Cytology: A Survey"). Firstly, a series of patches are extracted from whole images
    with patch-level annotations. Then, these patches are cut into instances and formed
    bags. Finally, a multi-instance classifier is established by learning for multi-instance
    bags, which is used for the prediction of unknown bags or instances. Specifically,
    given a training dataset $\{(X_{i},Y_{i})_{i=1}^{N}\}$, where $X_{i}=\left\{x_{i1},x_{i2},\ldots,x_{i,m}\right\}$
    are instance bags, $x_{i,m}$ is the $m\text{-}th$ instance of the $i\text{-}th$
    bag. $Y_{i}\in\{-1,+1\}$ is its corresponding label of the $i\text{-}th$ bag,
    +1 represents positive bag with at least one positive instance in this bag, and
    -1 represents negative bag with no positive instance. Then, bags $X_{i}$ together
    with labels are used to learn a classification function that can predict the labels
    of bags and instances.'
  prefs: []
  type: TYPE_NORMAL
- en: Weakly supervised learning is particularly appealing in computational cytology
    scenarios (e.g., whole slide thyroid malignancy prediction [[38](#bib.bib38)]),
    where full labels are expensive to obtain [[154](#bib.bib154)]. Because fully
    labeling of all lesions and cells in cytological screening WSIs is hardly possible
    for cytologists. Hence, weakly supervised learning is introduced to effectively
    represent and enhance features in the scenarios of limited annotations.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Unsupervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unsupervised learning is effective for learning useful and underlying representations
    from unlabeled data, which can be utilized for downstream tasks. For example,
    unsupervised image augmentation can increase the amount and variety of original
    dataset for increasing the performance of classification models. Afterwards, unsupervised
    stain transformation can be adopted to normalize datasets in preprocessing pathology
    images. Auto-encoder (AE) is a typical structure in unsupervised learning, which
    is formulated as: $\mathrm{P}(x_{i})\rightarrow z\rightarrow\mathrm{P}\left(x_{i}^{\prime}\right)$,
    where AE is trained to encode the input image $x_{i}$ to obtain latent representation
    $z$. Then, decoders generate reconstructed image $x_{i}^{\prime}$ with the supervision
    of the raw input $x_{i}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two typical unsupervised models have gained popularities: variational auto-encoder
    (VAE) [[75](#bib.bib75), [67](#bib.bib67)] and generative adversarial network
    (GAN) [[47](#bib.bib47)]. As shown in Fig. [7](#S2.F7 "Fig. 7 ‣ 2.3 Unsupervised
    learning ‣ 2 Deep learning methodology ‣ Deep Learning for Computational Cytology:
    A Survey")(A), VAE improved AE by constraining latent variables to be normally
    distributed, then sampling a latent vector into a decoder for outputting the image.
    GAN is another promising architecture that can mitigate the difficulty of collecting
    large-scale labeled medical datasets by synthesizing high-quality fake images.
    For its structure, GAN is formed by a generator-discriminator architecture, as
    shown in Fig. [7](#S2.F7 "Fig. 7 ‣ 2.3 Unsupervised learning ‣ 2 Deep learning
    methodology ‣ Deep Learning for Computational Cytology: A Survey")(B). The generator
    aims to generate realistic images, while the discriminator competes with generator
    to differentiate real images and generated images. Therefore, this generator-discriminator
    architecture is optimized via the adversarial training [[47](#bib.bib47)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\min_{G}\max_{D}V(D,G)=\mathbb{E}_{x\sim p_{\text{data }}(x)}[\log
    D(x)]\\ +\mathbb{E}_{z\sim p_{\text{z }}(z)}[\log(1-D(G(z)))]\end{split}$ |  |
    (6) |'
  prefs: []
  type: TYPE_TB
- en: 'where $G(\cdot)$ denotes the generator, and $D(\cdot)$ denotes the discriminator.
    $\mathbb{E(\cdot)}$ is the expectation value of distribution, $p_{\text{data }}(x)$
    and $p_{\text{z }}(z)$ are the distribution of the real sample and noise, respectively.
    During training, generator $G(\cdot)$ aims to learn the distribution of real samples
    $p_{\text{data}}$, and discriminator $D(\cdot)$ is responsible for discriminating
    generated and real images, thus forcing the generator to generate realistic images.
    After the emergence of this basic GAN structure and adversarial loss, many advanced
    GAN models are proposed to satisfy higher requirements of generated images (e.g.,
    quality, fidelity, and diversity) [[180](#bib.bib180)]. For example, one promising
    architecture, CycleGAN is designed for style transformation between unpaired images
    ($s$, $t$), which is a symmetrical structure consisting of two generators $\{G_{S\rightarrow
    T},G_{T\rightarrow S}\}$ for mutual generation between two domains ($S$ and $T$),
    and two discriminators $\{D_{S},D_{T}\}$ for discriminating generated images of
    respective domains. In addition, cycle consistency loss $L_{cyc}(G_{S\rightarrow
    T},G_{T\rightarrow S})$ is designed for one-to-one mapping in CycleGAN architecture,
    defined as [[198](#bib.bib198)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}L_{cyc}(G_{S\rightarrow T},G_{T\rightarrow S})=\mathbb{E}_{s\sim
    p_{\text{data }}(x)}\left[\&#124;G_{T\rightarrow S}(G_{S\rightarrow T}(s))-s\&#124;_{1}\right]\\
    +\mathbb{E}_{t\sim p_{\text{data }}(y)}\left[\&#124;G_{S\rightarrow T}(G_{T\rightarrow
    S}(t))-t\&#124;_{1}\right]\end{split}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $p_{\text{data}}(s)$ and $p_{\text{data}}(t)$ are the distribution of
    images in domain $S$ and domain $T$.
  prefs: []
  type: TYPE_NORMAL
- en: In cytology, unsupervised learning algorithms have been designed for various
    DL tasks, such as stain conversion by CycleGAN [[164](#bib.bib164)], data augmentation
    for improving the accuracy of classification by cGAN [[35](#bib.bib35)], and generating
    high-resolution images by GAN-based super-resolution model [[99](#bib.bib99)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9b07f2c2f61117b0d4c6f3e1b3a42d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 7: AE-based unsupervised learning models. (A) Variational auto-encoder
    (VAE) improved AE by constraining the latent representation to be the normal distribution.
    (B) Generative adversarial network (GAN), which consists of a generator and a
    discriminator. The generator is responsible for generating fake images from random
    noise while discriminator forcing the generator generating realistic images by
    discriminating generated fake image and real image.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Transfer Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transfer learning is a subfield of deep learning that focuses on transferring
    knowledge from source to target domain for enhancing target tasks. Two transfer
    learning approaches are commonly used in medical image analysis, including fine-tuning
    and domain adaptation (DA).
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning is regarded as a common model initialization trick for training
    DL models. It can reduce the overfitting issue and improve the generalization
    capability of deep models by transferring knowledge from large public datasets
    (e.g., ImageNet [[33](#bib.bib33)]) to domain-specific tasks (e.g., cervical cell
    classification) [[153](#bib.bib153), [181](#bib.bib181)]. Formally, the goal of
    fine-tuning is training a task $\mathcal{T}^{t}$ by a small dataset $T$. Specifically,
    it exploits a large-scale and task-similar $\mathcal{T}^{t}$ dataset $S=\{s_{i}\}_{i=1}^{M}(M>>N)$
    to pre-train a network $f(\sim;\theta)$ first, then a small target dataset $T=\{t_{i}\}_{i=1}^{N}$
    is used to train the several last layers of the pre-trained model for obtaining
    the target model $N_{t}$. Under the premise that different datasets for training
    similar tasks have similar low-level feature representations, fine-tuning is regarded
    as a common and effective training strategy in various DL tasks as well as cytology
    image analysis [[189](#bib.bib189)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Domain adaptation (DA) is another transfer learning approach that transfers
    knowledge by learning to narrow the distribution gap of datasets in different
    domains. The paradigm of DA can be defined as: giving two different datasets ($T$
    and $S$) with different distributions ($p(S)\neq p(T)$), DA methods can align
    the distributions of these datasets by marginal, conditional or joint distribution
    adaptation. As a result, the knowledge is transferred from source to target domain,
    thus improving the performance of target models [[117](#bib.bib117)]. In medical
    image analysis, data heterogeneity hinders the successful practice of deep models
    in the clinic. This issue can be mitigated by domain adaptation for improving
    the validity and reproducibility of deep models clinically [[48](#bib.bib48)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of publicly available and representative private databases
    in computational cytology'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference/Year | Task | Organ | Stain | Size | Description | Link |'
  prefs: []
  type: TYPE_TB
- en: '| Herlev 2005 [[60](#bib.bib60)] | Classification | Cervix | Pap | variable
    | 917 cells | [http://mde-lab.aegean.gr/downloads](http://mde-lab.aegean.gr/downloads)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ISBI 2014 [[95](#bib.bib95)] | Segmentation | Cervix | Pap | 512 × 512 |
    16 images (645 cells) | [https://github.com/luzhi/cellsegmentation_TIP2015](https://github.com/luzhi/cellsegmentation_TIP2015)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ISBI 2015 [[96](#bib.bib96)] | Segmentation | Cervix | Pap | 512 × 512 |
    945 images synthesized by ISBI 2014 | [http://goo.gl/KcpLrQ](http://goo.gl/KcpLrQ)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sipakmed 2018 [[123](#bib.bib123)] | Classification | Cervix | Pap | 2,048
    × 1,536 | 966 images (4,049 annotated cells) | [https://www.cs.uoi.gr/~marina/sipakmed.html](https://www.cs.uoi.gr/~marina/sipakmed.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CERVIX93 2018 [[120](#bib.bib120)] | Classification Detection | Cervix |
    Pap | 1,280 × 960 | 93 stacks of images (2,705 nuclei) | [https://github.com/parham-ap/cytology_dataset](https://github.com/parham-ap/cytology_dataset)
    |'
  prefs: []
  type: TYPE_TB
- en: '| FNAC 2019 [[139](#bib.bib139)] | Classification | Breast | Pap | 2,048 ×
    1,536 | 212 images in two classes: benign (99) and malignant (113) | [https://1drv.ms/u/s!Al-T6d-_ENf6axsEbvhbEc2gUFs](https://1drv.ms/u/s!Al-T6d-_ENf6axsEbvhbEc2gUFs)
    |'
  prefs: []
  type: TYPE_TB
- en: '| BHS 2019 [[6](#bib.bib6)] | Segmentation Ranking | Cervix | Pap | 1,392 ×
    1,040 | 194 images in classes of carcinoma, HSIL, LSIL, ASC-US and ASC-H | [https://sites.google.com/view/centercric](https://sites.google.com/view/centercric)
    |'
  prefs: []
  type: TYPE_TB
- en: '| AgNOR 2020 [[4](#bib.bib4)] | Segmentation | Cervix | AgNOR | 1,600 × 1,200
    | 2,540 images (4,515 nuclei) | [https://arquivos.ufsc.br/d/373be2177a33426a9e6c/](https://arquivos.ufsc.br/d/373be2177a33426a9e6c/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LBC 2020 [[55](#bib.bib55)] | Classification | Cervix | Pap | 2,048 × 1,536
    | 963 LBC images in classes of NILM, LSIL, HSIL, and SCC | [https://data.mendeley.com/datasets/zddtpgzv63/4](https://data.mendeley.com/datasets/zddtpgzv63/4)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Oral 2021 [[105](#bib.bib105)] | Classification Detection Segmentation |
    Oral | Pap | 1,200 × 1,600 | 1,934 images (4,287 annotations) | [https://arquivos.ufsc.br/d/5035aec3c24f421a95d0/](https://arquivos.ufsc.br/d/5035aec3c24f421a95d0/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Cric 2021 [[130](#bib.bib130)] | Classification | Cervix | Pap | 1,376 ×
    1,020 | 400 images (11,534 cells) | [https://database.cric.com.br](https://database.cric.com.br)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CDetector 2021 [[83](#bib.bib83)] | Detection | Cervix | Pap | 224 × 224
    | 7,410 images (48,587 object instance bounding boxes) in 11 classes | [https://github.com/kuku-sichuan/ComparisonDetector](https://github.com/kuku-sichuan/ComparisonDetector)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Ascites 2020 [[155](#bib.bib155)] | Classification Detection | Stomach |
    H&E, Pap | 1,064 × 690 | 487 images for classification in two classes: malignant
    (18,558) and benign (6,089). 176 images for detection (6,573 annotated cell bounding
    boxes) | [https://pan.baidu.com/s/1r0cd0PVm5DiUmaNozMSxgg](https://pan.baidu.com/s/1r0cd0PVm5DiUmaNozMSxgg)
    |'
  prefs: []
  type: TYPE_TB
- en: '| RSDC 2021 [[98](#bib.bib98)] | Super resolution | Cervix | Pap | 128 × 128
    (HR) 64 × 164 (LR) | 5 slides (25000 patches) | [https://www.kaggle.com/birkhoff007/rsdcdata](https://www.kaggle.com/birkhoff007/rsdcdata)
    |'
  prefs: []
  type: TYPE_TB
- en: '| IRNet 2019 [[195](#bib.bib195)] | Segmentation | Cervix | Pap | 1,000 × 1,000
    | 413 images (4,439 cytoplasm and 4,789 nuclei) | Private dataset |'
  prefs: []
  type: TYPE_TB
- en: '| DCCL 2020 [[186](#bib.bib186)] | Detection | Cervix | Pap | 1,200 × 2,000
    | 1,167 WSIs (14,432 patches, and 27,972 labeled lesion cells) | Private dataset
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dual 2021 [[86](#bib.bib86)] | Risk stratification | Cervix | Pap | Up to
    50,000 × 50,000 | 19,303 WSIs in two classes: abnormal (202,557) and normal (272,933)
    | Private dataset |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid 2021 [[199](#bib.bib199)] | Classification Detection Segmentation
    | Cervix | Pap | 6000 × 6000 | 24 categories and 2000 images in each category,
    81,727 smears (1.7 million annotated targets) for detection model | Private dataset
    |'
  prefs: []
  type: TYPE_TB
- en: 3 Datasets and metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning relies on large amounts of labeled data, we summarize publicly
    available datasets as well as representative private datasets in cytology. As
    illustrated in Table [1](#S2.T1 "Table 1 ‣ 2.4 Transfer Learning ‣ 2 Deep learning
    methodology ‣ Deep Learning for Computational Cytology: A Survey"), the majority
    of public datasets are from the cervix, with a small number of other cancer types,
    such as breast, oral, and stomach. These publicly available cytology datasets
    can be used to develop deep learning algorithms for various tasks, including classification,
    detection, and segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of evaluation metrics in computational cytology'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric | Definition | Description | Application in cytology |'
  prefs: []
  type: TYPE_TB
- en: '| Classification |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TP/TN/FP/FN | True Positive, True Negative, False Positive, False Negative
    | A test result that correctly indicates the presence of a condition (TP), correctly
    indicates the absence of a condition (TF), wrongly indicates the presence of a
    particular condition (FP), wrongly indicates the absence of a particular condition
    (FN). | Classification of FNAC images; formulate other metrics (e.g., accuracy,
    precision, recall) [[142](#bib.bib142), [82](#bib.bib82)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Confusion matrix | A Matrix. Row: actual class; Column: predicted class.
    | The number of correct and incorrect predictions are summarized with count values
    and broken down by each class. | Classification of lung cancer sub-type; pap smear
    image; quantitative analysis of abnormalities; WSI-level risk stratification [[161](#bib.bib161),
    [110](#bib.bib110), [65](#bib.bib65), [7](#bib.bib7)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy (Acc) | $\frac{TP+TN}{FP+FN+TP+TN}$ | Proportion of all positive
    and negative classes with correct predictions in all samples. | Cervical squamous
    lesions classification; FNAC image classification [[91](#bib.bib91), [9](#bib.bib9),
    [3](#bib.bib3)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Precision (P) | $\frac{TP}{FP+TP}$ | The proportion of positive samples classified
    as positive examples by the classifier. | Cervical cells classification; cervical
    lesions classification; multi-cell classification in liquid-based cytology images
    [[153](#bib.bib153), [91](#bib.bib91), [125](#bib.bib125)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Recall (R) | $\frac{TP}{FN+TP}$ | The proportion of the samples predicted
    to be positive cases in the total positive cases. | Pap smear image classification;
    classification of cervical cells [[110](#bib.bib110), [125](#bib.bib125)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Specificity (Spec) | $\frac{TN}{FP+TN}$ | The proportion of samples that
    are correctly predicted as negative classes in all negative classes. | Cell classification;
    differential diagnosing of papillary thyroid carcinomas; detection of cervical
    intraepithelial neoplasia or invasive cancer [[189](#bib.bib189), [50](#bib.bib50),
    [10](#bib.bib10)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Sensitivity (Sens) | $\frac{TP}{FN+TP}$ | The proportion of the samples predicted
    to be positive cases in the total positive cases. | Distinguish large cell neuroendocrine;
    identify cells; high resolution image classification [[46](#bib.bib46), [79](#bib.bib79)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| F1-score (F1) | $\frac{2\times Precision\times Recall}{Precision+Recall}$
    =$\frac{2\times TP}{FP+FN+2TP}$ | Harmonic average of precision and recall, and
    it is defined as the final evaluation index in many classification tasks. | Classification
    of cervical cells; multi-cell classification in liquid-based cytology images;
    cell image ranking [[153](#bib.bib153), [6](#bib.bib6), [125](#bib.bib125)]. |'
  prefs: []
  type: TYPE_TB
- en: '| ROC curve | (FP rate, TP rate) | ROC is a graph showing the performance of
    a classification model at all classification thresholds. | Prediction of malignancy;
    cervical cancer screening; smear-level risk stratification [[40](#bib.bib40),
    [158](#bib.bib158), [86](#bib.bib86)]. |'
  prefs: []
  type: TYPE_TB
- en: '| AUC | Area under the ROC curve | The closer the AUC is to 1, the better the
    classifier performance. | Cancer screening (cell-level detection, patch-level
    and case-level classification); quantitative analysis of abnormalities; automating
    the paris system for cytopathology [[169](#bib.bib169), [65](#bib.bib65), [20](#bib.bib20)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Detection |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| IoU | $\frac{P\cap GT}{P\cup GT}$  $=\frac{TP}{FP+FN+TP}$ | $P$ denotes predicted
    bounding box, and GT is ground truth box. The ratio of the intersection and union
    of the predicted bounding box and the ground truth bounding box. | Nuclei/Cell
    detection; automation-assisted cervical cancer reading [[66](#bib.bib66), [177](#bib.bib177),
    [82](#bib.bib82)]. |'
  prefs: []
  type: TYPE_TB
- en: '| AP | Average precision | The mean value of precision on precision-recall
    curve. | Detection of abnormal cervical cells [[20](#bib.bib20)]. |'
  prefs: []
  type: TYPE_TB
- en: '| mAP | mean AP | Average of AP in all categories. | Cell/Clumps detection;
    quantification of pulmonary hemosiderophages [[104](#bib.bib104), [21](#bib.bib21),
    [83](#bib.bib83)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Segmentation |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Pixel Precision (P[p]) | $\frac{TP_{p}}{TP_{p}+FP_{p}}$ | The ${p}$ means
    this is a pixel-level metric. Proportion of correctly segmented pixels to all
    segmented pixels. | Cytological examination (overlapping cell segmentation) [[159](#bib.bib159)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pixel Recall (R[p]) | $\frac{TP_{p}}{TP_{p}+FN_{p}}$ | Proportion of correctly
    segmented pixels to all pixels in the ground truth. | Cytological examination
    (overlapping cell segmentation) [[159](#bib.bib159)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Pixel Accuracy (Acc[p]) | $\frac{TP_{p}+TN_{p}}{FP_{p}+FN_{p}+TP_{p}+TN_{p}}$
    | Pixel level accuracy. | Segmentation of cytoplasm and nuclei [[150](#bib.bib150),
    [151](#bib.bib151), [65](#bib.bib65)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Hausdorff Distance | $\max(\sup\limits_{x\in X}d(x,Y)$, $\sup\limits_{y\in
    Y}d(X,y))$ | X and Y are two sets, $sup$ represents the supremum. It measures
    the similarity between two point sets. | Cell nuclei segmentation [[73](#bib.bib73)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dice coefficient (Dice) | $\frac{2\times TP}{FP+FN+2TP}$ | Dice coefficient
    is a statistical tool which measures the similarity between two sets of data.
    It can be used for comparing algorithm output against reference masks. | Semantic
    instance segmentation of touching and overlapping objects; cytoplasm segmentation;
    instance segmentation [[16](#bib.bib16), [172](#bib.bib172), [171](#bib.bib171)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Zijdenbos similarity index (ZSI) | $2\frac{\left&#124;R_{GT}\cap R_{Seg}\right&#124;}{\left&#124;R_{GT}\right&#124;+\left&#124;R_{Seg}\right&#124;}$
    | $R_{GT}$ and $R_{Seg}$ denote the ground truth and segmented regions, respectively.
    ZSI computes the ratio of aggregated union between cardinality predicted segmentation
    output and manual segmentation output. | Overlapping cell segmentation; cervical
    nuclei segmentation [[159](#bib.bib159), [56](#bib.bib56)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Average Jaccard Index (AJI) | $\frac{\sum_{i=1}^{N}\left&#124;G_{i}\cap P_{M}^{i}\right&#124;}{\sum_{i=1}^{N}\left&#124;G_{i}\cup
    P_{M}^{i}\right&#124;+\sum_{F\in U}\left&#124;P_{F}\right&#124;}$ | $G_{i}$ is
    the $i\text{-}{th}$ object from the ground truth with $N$ objects. $P_{M}^{i}$
    means the $M\text{-}{th}$ connected component in prediction which has the largest
    Jaccard Index with $G_{i}$. AJI measures the ratio of the aggregated intersection
    and aggregated union for all the predictions and ground truths in the image. |
    Cell segmentation [[195](#bib.bib195)]. |'
  prefs: []
  type: TYPE_TB
- en: Herlev [[60](#bib.bib60)]. This database consists of 917 Papanicolaou (Pap)
    smear cervical images in 7 classes (3 normal cell classes and 4 abnormal cell
    classes), which are collected from the Herlev University Hospital. As the earliest
    established public cytology dataset, Herlev dataset is extensively adopted for
    developing DL-based coarse and fine-grained classification models for cervical
    cancer screening [[189](#bib.bib189), [87](#bib.bib87)].
  prefs: []
  type: TYPE_NORMAL
- en: ISBI 2014 [[95](#bib.bib95)]. Another widely developed cervical dataset comes
    from the ISBI challenge. Different from Herlev dataset, this dataset focuses on
    the segmentation task with pixel-wise annotations. It consists of 16 non-overlapping
    fields of view images (×40 magnification) with 645 cells obtained from four cervical
    cytology specimens. Each sample in this dataset contains 20 to 60 Pap-stained
    cervical cells with varying degrees of overlapping.
  prefs: []
  type: TYPE_NORMAL
- en: ISBI 2015 [[96](#bib.bib96)]. ISBI 2015 extends ISBI 2014 dataset to 945 cervical
    cytology images by synthesizing. ISBI 2015 has a varying number of cells and different
    degrees of cell overlapping (size of 512 × 512 pixels), which contains 45 training
    images (taken from the 4 extended depth field images) and 900 testing images (from
    12 images).
  prefs: []
  type: TYPE_NORMAL
- en: Sipakmed [[123](#bib.bib123)]. This database consists of 4049 images of isolated
    cells that have been manually cropped from 966 cluster cell images of Pap smear
    slides. Sipakmed dataset has 5 types of cervical cells, including superficial-intermediate,
    parabasal, koilocytotic, dyskeratotic, and metaplastic cells.
  prefs: []
  type: TYPE_NORMAL
- en: 'CERVIX93 [[120](#bib.bib120)]. This is the first dataset established for nuclei
    detection tasks in cytology. It consists of 93 stacks of images at 40$\times$
    magnification. Each stack has 10-20 images acquired at the equally spaced field
    of views from the top to the bottom of the slide. In this dataset, 2705 nuclei
    are annotated by bounding boxes with three different Pap test grades: negative,
    low-grade squamous in the intraepithelial lesion (LSIL) or high-grade squamous
    intraepithelial lesion (HSIL).'
  prefs: []
  type: TYPE_NORMAL
- en: FNAC [[139](#bib.bib139)]. This is the only public breast cytology dataset developed
    for classification model. These breast images are collected from 20 patients,
    comprising of 212 fine-needle aspiration cell inspection images in classes of
    benign (99) and malignant (113).
  prefs: []
  type: TYPE_NORMAL
- en: 'BHS [[6](#bib.bib6)]. It collects 194 Pap-smear cervical slides from the Brazilian
    Health System (BHS). Among them, 108 images have at least one abnormal cell and
    86 images with normal cells only. In sum, it has 5 types of abnormalities: carcinoma,
    HSIL, LSIL, atypical squamous cells of undetermined significance (ASC-US) and
    atypical squamous cells cannot exclude HSIL (ASC-H).'
  prefs: []
  type: TYPE_NORMAL
- en: AgNOR [[4](#bib.bib4)]. The dataset is composed of 2540 images with $1200\times
    1600$ pixels each. Different from other public cervical datasets, it contains
    cells stained with the silver technique, which is known as argyrophilic nucleolar
    organizer regions (AgNOR). For developing segmentation approaches, experts annotate
    objects by the Labelme tool [[138](#bib.bib138)], including nuclei, clusters,
    and satellites.
  prefs: []
  type: TYPE_NORMAL
- en: 'LBC [[55](#bib.bib55)]. Recently, liquid-based cytology (LBC) is developed
    for providing more uniform fixation with a cleaner background and well-preserved
    samples than conventional Pap smear tests. This dataset consists of 963 images
    with four classes: NILM, LSIL, HSIL, and squamous cell carcinoma (SCC).'
  prefs: []
  type: TYPE_NORMAL
- en: Oral [[105](#bib.bib105)]. Totally, 1,934 oral images of $1200\times 1600$ pixels
    are acquired from two Pap-stained slides of cancer diagnosed oral brush samples.
    With different types of annotation (category, box, mask), various DL tasks can
    be conducted by this dataset, including classification, detection, and segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'CRIC [[130](#bib.bib130)]. The collection CRIC cervix has 400 images of pap
    smears with 11,534 classified cells. Based on the Bethesda system [[113](#bib.bib113)],
    CRIC dataset covers conventional cytology cervical cells with six types: NILM
    (6,779), ASC-US (606), LSIL (1,360), ASC-H (925), HSIL (1,703), and SCC (161).'
  prefs: []
  type: TYPE_NORMAL
- en: 'CDetector [[83](#bib.bib83)]. This dataset consists of 7,410 cervical images
    cropped from the WSIs. According to the Bethesda system (TBS), 48,587 object instance
    bounding boxes are annotated by experienced pathologists which belong to 11 categories:
    ASC-US, ASC-H, HSIL, LSIL, SCC, atypical glandular cells (AGC), trichomonas (TRICH),
    candida (CAND), flora, herps, actinomyces (ACTIN). Till now, CDetector is the
    largest public dataset for the object detection task in cytology.'
  prefs: []
  type: TYPE_NORMAL
- en: Ascites [[155](#bib.bib155)]. This dataset is established for screening gastric
    cancer and collected from Peking University. It consists of 176 H&E stained and
    Pap stained images cropped from ascites cytopathology images at 40 × magnification.
    A total of 6573 cells (benign and malignant) are annotated using bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: RSDC [[98](#bib.bib98)]. It is the only public cytology dataset for developing
    the refocusing and super-resolution task. The images in the dataset are collected
    from 5 LBC slides with the resolution 0.243$\mu m/pixel$. Strategies of bicubic
    interpolation and gaussian blur are used to generate 15,000 low-resolution images
    (64 × 64) and corresponding high-resolution images (128 × 128) from original slides.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, to evaluate the performance of proposed deep learning models,
    we summarize the evaluation metrics in terms of three canonical DL approaches:
    classification, detection, and segmentation along with typical cytological applications
    adopting these metrics, more details are shown in Table [2](#S3.T2 "Table 2 ‣
    3 Datasets and metrics ‣ Deep Learning for Computational Cytology: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Among these summarized evaluation metrics, classification metrics evaluate the
    classifier’s capability of predicting the category. Accuracy is the most straightforward
    metric, yet it ignores the imbalance problem between different categories. The
    confusion matrix can represent the prediction result of each category. To measure
    the detection task, $IoU$ is a commonly-used metric, which can measure the overlap
    between the predicted box and the ground truth box. Based on various set thresholds,
    average precision ($AP$) is utilized to evaluate the performance of the detector
    in different overlapping levels, including $AP_{50}$, $AP_{75}$, etc. Segmentation
    models can be evaluated by various metrics. For example, $Pixel\ accuracy$ measures
    the predicted result of each pixel, and $Dice$ calculates the similarity coefficient
    of predicted masks and ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Deep learning in cytology application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we survey and summarize literatures on various deep learning
    models applied in computational cytology. Firstly, we introduce preprocessing
    techniques in cytology image analysis, followed by representative clinical tasks:
    classification, detection, segmentation, and others. More details of these surveyed
    literatures are as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Staining techniques. In cytology, staining techniques are introduced to enhance
    the image features of cells (e.g., texture, structure, and biochemical properties)
    for visually presenting cellular structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [8](#S4.F8 "Fig. 8 ‣ 4.1 Preprocessing ‣ 4 Deep learning in cytology application
    ‣ Deep Learning for Computational Cytology: A Survey") shows various staining
    techniques in cytology. 1) Pap. As the extensive staining protocols, it has four
    steps: fixation, nuclear staining, cytosol staining, and transparency. Cell stained
    by Pap has a clearly-structured nucleus and transparent cytoplasm. According to
    surveyed literatures, Pap is the most common staining method in cytology images,
    especially in cervical cancer. 2) Hematoxylin and Eosin (H&E). Hematoxylin stains
    cell nuclei purplish-blue, and eosin stains the extracellular matrix and cytoplasm
    pink. In the clinic, H&E is mainly used for staining cells and tissues. 3) Giemsa.
    It is particularly effective for staining cytoplasm, so Giemsa is mainly used
    for blood and bone marrow cytological evaluation. Other staining techniques are
    used for some specific situations. For example, [[4](#bib.bib4)] stained cervical
    cells by AgNOR to present cell proliferation, differentiation, and malignant transformation.
    In another work, [[177](#bib.bib177)] stained cervical specimens of LBC by Feulgen.
    [[104](#bib.bib104)] stained pulmonary hemosiderophages cytology by Perlss’ Prussian
    Blue, Turnbull’s Blue.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9850e9ac50d060503f93bb5fbd95e0a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 8: Staining techniques. (A) Pap [[195](#bib.bib195)]. (B) H&E [[155](#bib.bib155)].
    (C) Giemsa [[9](#bib.bib9)]. (D) Feulgen [[177](#bib.bib177)]. (E) AgNOR [[4](#bib.bib4)].
    (F) Diff-quik [[46](#bib.bib46)].'
  prefs: []
  type: TYPE_NORMAL
- en: Stain Normalization. A significant amount of color variations exist in cytology
    images due to various staining techniques mentioned and other issues (e.g., imaging
    environment). These differences bring challenges for building robust and generated
    DL-based cytology models. Besides, normalization can accelerate the convergence
    when training networks. Therefore, normalization can be a crucial preprocessing
    step, especially when analyzing stained images, like cytology images, and histopathology
    images. Commonly-used methods include whiting for removing redundant information
    from input data, and linear normalization for scaling gray values of input data,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation. When the amount of images is not sufficient to learn a robust
    DL model, especially for medical images. Data augmentation strategies are introduced
    to increase the amount of input images for improving the model’s generalization.
    Conventional augmentation methods include geometric transformation (e.g., flipping,
    rotating, and scaling) and color transformation (e.g., noise, blurring, and contrast).
    Recently, generative adversarial network (GAN) has been adopted to synthesize
    a large-scale dataset based on a limited set. [[183](#bib.bib183)] utilized GAN
    to generate 16,000 images from 961 real images for improving cervical cell classification
    models. In another study, [[35](#bib.bib35)] generated 180 images by conditional
    GAN and learnable class-specific priors for improving classifier performance on
    cytology tasks. To overcome the data limitation issue, [[162](#bib.bib162)] proposed
    a GAN-based augmentation structure, progressive growing of GANs (PGGAN). In this
    study, real lung cytological images together with synthesized images by PGGAN
    are used to train classification CNNs, leading to the performance improvements
    in cytology image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In cytology, the DL approaches are feasible and promising for image classification
    with distinguishable characteristics of cytology samples. The clinical cytologists
    can distinguish between benign and malignant cells based on their cytological
    features. For example, the abnormalities are displayed in malignant cells, such
    as larger and irregular nuclei, enlarged nuclear-cytoplasmic ratio, and the altered
    shape of the nucleolus. Within cytology image classification tasks, DL-based models
    aim to extract the underlying patterns of input images for identifying objects
    (e.g., nucleus, cell) or making slide-level predictions (e.g., cytopathology screening).
    Therefore, we further divide the cytological classification task into two categories:
    1) cell-level, 2) slide-level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Overview of deep learning-based classification studies for computational
    cytology'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Application | Staining | Organ | Method | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| Cell-level classification |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[189](#bib.bib189)] | Cell classification | Pap H&E | Cervix | Data preprocessing
    (patch extraction, data augmentation) + CNN + Transfer learning (fine-tune) |
    Herlev; HEMLBC | Herlev: Sens=0.982, Spec=0.983, Acc=0.983, F1=0.988, AUC=0.998;
          HEMLBC: Sens=0.983, Spec=0.990, Acc=0.986. |'
  prefs: []
  type: TYPE_TB
- en: '| [[161](#bib.bib161)] | Classification of cancer types (adenocarcinoma, squamous
    cell carcinoma, and small cell carcinoma) | Pap | Lung | Data augmentation + CNN
    | Private dataset: 76 images in classes of adenocarcinoma (40), squamous cell
    carcinoma (20), and small cell carcinoma (16) | Adenocarcinoma: Acc=0.89; Squamous
    cell carcinoma: Acc=0.600; Small cell carcinoma: Acc=0.703; Total: Acc=0.711.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[36](#bib.bib36)] | Cell classification | Pap | Nasal | Three-block CNN
    | Private dataset: 3,423 images (cell) | Sens=0.97, Acc=0.99. |'
  prefs: []
  type: TYPE_TB
- en: '| [[144](#bib.bib144)] | Malignancy detection and classification | Pap | Cervix
    | Three-layer CNN | Herlev | 5-class: Acc=0.941; 4-class: Acc=0.962; 3-class:
    Acc=0.948; 2-class: Acc=0.957. |'
  prefs: []
  type: TYPE_TB
- en: '| [[163](#bib.bib163)] | Classification of benign and malignant cells | Pap
    | Lung | Data augmentation + VGG-16 + GradCAM | Private dataset: 621 images (patch)
    in classes of benign (306) and malignant (315) | Patch-level: Acc=0.792, AUC=0.872;
    Case-level: Acc=0.870, AUC=0.932. |'
  prefs: []
  type: TYPE_TB
- en: '| [[91](#bib.bib91)] | Classification of cervical squamous lesions | Pap |
    Cervix | VGG-16 | Private dataset: 3,290 images in classes of abnormal cells (1,736
    ) and normal cells (1,554) | Acc=0.9807, P=0.9791, Sens=0.9801, F1=0.9809. |'
  prefs: []
  type: TYPE_TB
- en: '| [[87](#bib.bib87)] | Fine-grained cell classification | Pap | Cervix | Fine-tune
    + CNN (AlexNet, GoogLeNet, ResNet, and DenseNet) | Herlev | GoogLeNet: Acc=0.945
    (2-class), Acc=0.713 (4-class), Acc=0.645 (7-class). |'
  prefs: []
  type: TYPE_TB
- en: '| [[153](#bib.bib153)] | Multi-cell classification in liquid-based cytology
    images | Pap | Cervix | Fine-tune + CNN (ResNet-50, VGG-19, DenseNet-121, Inception-v3)
    | Herlev; Private dataset: 25 images | ResNet-50: F1=0.8865, AUC=0.95; VGG-19:
    F1=0.8896, AUC=0.95; Densenet-121: F1=0.8546, AUC=0.94; Inception-v3: F1=0.8072,
    AUC=0.88. |'
  prefs: []
  type: TYPE_TB
- en: '| [[57](#bib.bib57)] | Cervical cancer diagnostic prediction | Pap | Cervix
    | CNN (AlexNet, VGG-16, VGG-19, ResNet-50, ResNet-101, and GoogLeNet) | Herlev
    | AlexNet: Acc=0.8;     VGG-16: Acc=0.8337;   VGG-19: Acc=0.8455;   ResNet-50:
    Acc=0.8937; ResNet-101: Acc=0.9450; GoogLeNet: Acc=0.9567. |'
  prefs: []
  type: TYPE_TB
- en: '| [[110](#bib.bib110)] | Smear classification | Pap | Cervix | 10 popular pre-trained
    CNN | Sipakmed | DenseNet-169: Acc=0.990, P=0.974, R=0.974, F1=0.974. |'
  prefs: []
  type: TYPE_TB
- en: '| [[3](#bib.bib3)] | Classification of cervical cancer risk | Pap | Cervix
    | 9 popular CNN | Herlev | Acc=0.756 (7-class), Acc=0.813 (4-class). |'
  prefs: []
  type: TYPE_TB
- en: '| [[108](#bib.bib108)] | Classification of FANC images | H&E | Breast | CNN
    (AlexNet, GoogLeNet, SqueezeNet, DenseNet, Inception-V3) | Private dataset: 737
    images (ROIs from specimens) in classes of benign (275) and malignant (462) |
    AlexNet: AUC=0.9730; GoogLeNet: AUC=0.9455; SqueezeNe: AUC=0.9152; DenseNet: AUC=0.9244;
    Inception-V3: AUC=0.9730. |'
  prefs: []
  type: TYPE_TB
- en: '| [[102](#bib.bib102)] | Classification of cervical cells | Pap | Cervix |
    Fuzzy rank + Pre-trained CNN (Inception-V3, Xception and DenseNet‑169) | Sipakmed
    | Acc=0.9855, Sens=0.9852. |'
  prefs: []
  type: TYPE_TB
- en: '| [[125](#bib.bib125)] | Classification of cervical cells | Pap | Cervix |
    Hybrid deep feature fusion + CNN (VGG-16, VGG-19, XceptionNet, and ResNet-50)
    | Sipakmed | Acc=0.9985 (2-class), Acc=0.9914 (3-class), Acc=0.9914 (5-class).
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[183](#bib.bib183)] | Classification of cervical cells | Pap | Cervix |
    AlexNet + GAN | Private dataset: 22,124 images (cell) in classes of abnormal (1,202)
    and normal (20,922) | AUC=0.984 |'
  prefs: []
  type: TYPE_TB
- en: '| [[35](#bib.bib35)] | FNAC cytology image classification | H&E | Breast |
    Conditional GAN (synthesis) + CNN (ResNet-152, DenseNet-161, Inception-V3) | Private
    dataset: 150 images in classes of begin (75) and malignant (75) | 180 generated
    images. ResNet-152: Acc=0.7667; DenseNet-161: Acc=0.8667; Inception-V3: Acc=0.8000.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[162](#bib.bib162)] | Classification of cytological images | Pap | Lung
    | CNN + PGGAN | Private dataset: 511 images (patch) in classes of benign (244)
    and malignant (267) | Acc=0.853, Sens=0.854, Spec=0.853. |'
  prefs: []
  type: TYPE_TB
- en: '| [[8](#bib.bib8)] | Classification of FNAC images | Pap | Thyroid | CNN (VGG-19,
    AlexNet) + Transfer learning (Fine-tune) | Private dataset: 9,209 images (cell)
    in 5 classes | VGG-19: Acc=0.9305; AlexNet: Acc=0.9288. |'
  prefs: []
  type: TYPE_TB
- en: '| [[169](#bib.bib169)] | Automating the Paris system for cytopathology | Pap
    | Urine | VGG-19 + Morphometric model | Private dataset: 217 WSIs in classes of
    negative (51), atypical (60), suspicious (52), and positive (54) | Acc=0.972,
    Spec=0.976, Sens=0.970. |'
  prefs: []
  type: TYPE_TB
- en: '| continued on the next page |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Overview of deep learning-based classification studies for computational
    cytology (continued)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Application | Staining | Organ | Method | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| [[64](#bib.bib64)] | Cell image recognition | Pap | Urine | EfficientNet
    | Private dataset: 4,637 images (cell) | Acc=0.95, Sens=0.97, Spec=0.95, and AUC=0.99.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[185](#bib.bib185)] | Classification of cancer cytological specimen | H&E
    | Breast | CNN (AlexNet, GoogLeNet) | 550 images (ROIs) in classes of malignant
    (275) and benign (275) | AlexNet: Acc=0.80; GoogLeNet: Acc=0.83. |'
  prefs: []
  type: TYPE_TB
- en: '| [[146](#bib.bib146)] | Classification of cervical cells | Pap | Cervix |
    Graph convolutional Network (GCN) | Sipakmed | Acc=98.37 ± 0.57, Sens=99.80 ±
    0.10, Spec=99.60 ± 0.20, F1=99.80 ± 0.10. |'
  prefs: []
  type: TYPE_TB
- en: '| [[43](#bib.bib43)] | Classification of FANC cell samples | H&E | Breast |
    GoogLeNet | Private dataset: 37 images in classes of benign (24) and malignant
    (13) | Acc=0.8076. |'
  prefs: []
  type: TYPE_TB
- en: '| [[139](#bib.bib139)] | Classification of FNAC images | Pap | Breast | CNN
    (VGG-16, VGG-19, ResNet-50, and GoogLeNet-V3) | FANC 2019 | VGG-16: Acc=0.8867;
    VGG-19: Acc=0.882; ResNet-50: Acc=0.9056; GoogLeNet-V3: Acc=0.9625. |'
  prefs: []
  type: TYPE_TB
- en: '| [[115](#bib.bib115)] | Diagnose the malignant potential of carcinoma cells
    | Pap | Urine | Visual geometry group CNN | Private dataset: 203 images | AUC=0.9890,
    F1=0.9002. |'
  prefs: []
  type: TYPE_TB
- en: '| [[84](#bib.bib84)] | Cell classification | Pap | Urine | VGG-16 | Private
    dataset: 690 images in classes of urothelial normal cells (274) and abnormal cells
    (416) | Acc=0.899. |'
  prefs: []
  type: TYPE_TB
- en: '| [[50](#bib.bib50)] | Differential diagnosing of papillary thyroid carcinomas
    | H&E | Thyroid | VGG-16 and Inception-v3 | Private dataset: 279 images (thyroid
    nodules) | VGG-16: 0.9766 (image-level), 0.95 (patient-level); Inception-v3: 0.9275
    (image-level), 0.875 (patient-level). |'
  prefs: []
  type: TYPE_TB
- en: '| [[9](#bib.bib9)] | Classification of FNAC images | Giemsa H&E | Breast |
    CNN (13 layers, convolution and fully-connected layers) | Private dataset: Giemsa
    (1020 images in classes of benign and malignant) and H&E (631 images in classes
    of benign and malignant) | Giemsa: Acc=0.9781, P=0.977, R=0.973, Spec=0.982, F1=0.975;
    H&E: Acc=0.9753, P=0.973, R=0.950, Spec=0.987, F1=0.961. |'
  prefs: []
  type: TYPE_TB
- en: '| [[49](#bib.bib49)] | Differential diagnosing of lymph node | H&E | Cervix
    | Inception-v3 | Private dataset: 742 images in 4 classes | Acc=0.8962. |'
  prefs: []
  type: TYPE_TB
- en: '| [[15](#bib.bib15)] | Cervical cancer detection | Pap | Cervix | EfficientNet
    + Grad-CAM | Herlev, Sipakmed | Acc=0.9970, P=0.9970, R=0.9972, F1=0.9963, Kappa=0.9931.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[116](#bib.bib116)] | Cell identification | Giemsa | Skin | ResNet-50 |
    Private dataset: 2,260 images (Tzanck smear) | Acc=0.943, Sens=0.837, Spec=0.973,
    AUC=0.974. |'
  prefs: []
  type: TYPE_TB
- en: '| [[10](#bib.bib10)] | Detection of cervical intraepithelial neoplasia or invasive
    cancer | Pap | Cervix | VGG-16 | Private dataset: 188,542 mages | CIN 2: Acc=0.926;
    CIN 3+: Acc=0.961. |'
  prefs: []
  type: TYPE_TB
- en: '| [[142](#bib.bib142)] | Classification of FNAC images | Giemsa Pap | Thyroid
    | CNN | Private dataset: 370 images in classes of non‑PTCA (184) and PTCA (186)
    | Sens=0.9048, Spec=0.8333, Acc=0.8506. |'
  prefs: []
  type: TYPE_TB
- en: '| [[176](#bib.bib176)] | Classification of cancer types | H&E | Cervix | AlexNet
    | Private dataset: 79 specimens in 3 classes | Acc=0.9333. |'
  prefs: []
  type: TYPE_TB
- en: '| [[80](#bib.bib80)] | Cervical cell classification | Pap | Cervix | ResNet-50
    + Attention mechanism + LSTM | Sipakmed | Sensitivity=0.999, specificity=0.998,
    F1=0.9989. |'
  prefs: []
  type: TYPE_TB
- en: '| Slide-level classification |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[40](#bib.bib40)] | Classification (prediction of malignancy) | Pap | Thyroid
    | AlexNet | Private dataset: 908 WSIs | Sens=0.92, Spec=0.905, AUC=0.932. |'
  prefs: []
  type: TYPE_TB
- en: '| [[39](#bib.bib39)] | Classification (prediction of malignancy) | Pap | Thyroid
    | VGG-11 + Multiple instance learning | Private dataset: 908 WSIs | AUC=0.932,
    AP=0.872. |'
  prefs: []
  type: TYPE_TB
- en: '| [[158](#bib.bib158)] | Cervical cancer screening | Pap | Cervix | Faster
    R-CNN | Private dataset: 408,030 images | Sens=0.994, Spec=0.348, AUC=0.67. |'
  prefs: []
  type: TYPE_TB
- en: '| [[65](#bib.bib65)] | Quantitative analysis of abnormalities | Pap | Cervix
    | U-Net, ResNet-50 | Private dataset: 130 WSIs | Segmentation: Pixel Acc=0.974
    ± 0.001, IoU=0.913 ± 0.007; Classification: Acc=0.945 ± 0.006. |'
  prefs: []
  type: TYPE_TB
- en: '| [[20](#bib.bib20)] | Cancer screening (cell-level detection, patch-level
    and case-level classification) | Pap | Cervix | Multi-scale region-based CNN +
    Attention mechanism | Private dataset: 7030 images | Cell-level detection: AP=0.7509;
    Patch-level classification: AUC=0.9909; Case-level classification: AUC=0.934,
    Sens=0.913, Spec=0.906 and Acc=0.909. |'
  prefs: []
  type: TYPE_TB
- en: '| [[79](#bib.bib79)] | High resolution image classification | Pap | Cervix
    | Mixed supervision learning (image-level + pixel-level) | Private dataset: 862
    images from 2 data centers | Center A: Sens=1, Spec=0.86; Center B: Sens=1, Spec=0.87.
    |'
  prefs: []
  type: TYPE_TB
- en: '| continued on the next page |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Overview of deep learning-based classification studies for computational
    cytology (continued)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Application | Staining | Organ | Method | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| [[38](#bib.bib38)] | Classification (prediction of malignancy) | Pap | Thyroid
    | MIL+ NoisyAND + Attention mechanism + Maximum likelihood estimation | Private
    dataset: 142 WSIs with 4,494 instances | AUC=0.870 ± 0.017, AP=0.743 ± 0.037.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[46](#bib.bib46)] | Distinguish large cell neuroendocrine | Pap, H&E, Diff-Quik
    | Lung | CNN | Private dataset: 40 images in high-grade neuroendocrine carcinoma
    (17 small cell, 13 large cell, 10 mixed/unclassifiable) | H&E: Acc=0.900; Pap:
    Acc=0.875; Diff-Quik: Acc=0.889. |'
  prefs: []
  type: TYPE_TB
- en: '| [[199](#bib.bib199)] | Rapid TBS classification of cervical liquid-based
    thin-layer cell smears | Pap | Cervix | Model assembly: Xception (classification),
    YOLOv3 (object detection), and U-Net (segmentation) | Private dataset: 81,727
    images | Speed=180s/slide, Sens=0.9474. |'
  prefs: []
  type: TYPE_TB
- en: '| [[174](#bib.bib174)] | Cervical lesion detection, WSI-level classification
    of normal and abnormal | Pap | Cervix | YOLOv3 + Transformer | Private dataset:
    2,019 images (slide) from four scanning devices | AUC=0.872. |'
  prefs: []
  type: TYPE_TB
- en: '| [[29](#bib.bib29)] | WSI-level cervical cancer screening | Pap | Cervix |
    CNN (ResNet50) + RNN | Private dataset: 3,545 images (slide) with 79,911 annotations
    | Spec=0.935, Sens=0.951, Speed=1.5min/slide. |'
  prefs: []
  type: TYPE_TB
- en: '| [[7](#bib.bib7)] | Cell-level classification, WSI-level risk stratification
    | Pap | Urine | RetinaNet + Counting of atypical and malignant cells | Private
    dataset: 398 images (slide) in classes of normal (243), inflammatory (13), CA
    (76), ASM (38) and TCC (28) | Cell-level classification: AUC=0.99; Risk stratification:
    AUC=0.83. |'
  prefs: []
  type: TYPE_TB
- en: '| [[86](#bib.bib86)] | Smear-level risk stratification | Pap | Cervix | CNN
    with dual-path encode + Synergistic grouping loss | Private dataset: 19,303 WSIs
    (13,486 for training, 2,486 for validation and 3,331 for testing) from 4 centers
    in 6 classes of cells | Sens=0.907, Spec=0.80, AUC=0.925. |'
  prefs: []
  type: TYPE_TB
- en: 4.2.1 Cell-level classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cell-level classification could be one of the most successful tasks in DL-based
    cytology image analysis [[60](#bib.bib60)]. Due to the giga-pixel resolution of
    collected cytology WSIs, they are usually cut into cell patches for image analysis
    [[186](#bib.bib186)]. When training a deep network for classifying cells, cell
    patches are cropped from these whole images first. Then, these cell patches are
    fed into DL models to train cell-level classification models after preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward method is to directly feed cell patches into a multi-layer
    CNN for extracting feature maps, then crossing the output layer to get the predicted
    category. A series of CNN-based methods have been proposed. For lung cytology
    classification, [[161](#bib.bib161)] designed a deep convolutional neural network
    consisting of three convolutional layers, three pooling layers, and two fully
    connected layers. Similarly, [[36](#bib.bib36)] constructed a three-block CNN
    model for nasal cell classification. For cervical cytology, [[144](#bib.bib144)]
    designed a CNN architecture consisting of three convolutional layers. Its experimental
    results in different settings (2 class, 3 class, 4 class, and 5 class) showed
    an effective performance of different grades of cancer in cervical images. In
    addition, [[189](#bib.bib189)] proposed a simple ConvNet, which was first pre-trained
    in a natural image dataset, ImageNet. Then, the model was fine-tuned in two cervical
    cytological datasets, Herlev and HEMLBC [[60](#bib.bib60), [188](#bib.bib188)],
    achieving outperforming performance than previous algorithms. However, the performances
    and generalization capabilities of these simply-designed CNN with several layers
    are limited to specific datasets and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: A large amount of advanced deep models are proposed in the computer vision field,
    such as Inception [[157](#bib.bib157)], ResNet [[52](#bib.bib52)] and DenseNet
    [[54](#bib.bib54)]. These networks can be directly adopted for cytology image
    analysis and achieve better performance than simply-designed structures in the
    classification task. For example, [[163](#bib.bib163)] presented a VGG-based model
    for classifying benign and malignant cells from lung cytology images. [[116](#bib.bib116)]
    proposed TzanckNet based on ResNet-50 to identify cells in the cytology of erosive‑vesiculobullous
    diseases. Additionally, some studies compared the performance of advanced CNN
    architectures in cytology image classification tasks [[153](#bib.bib153), [57](#bib.bib57),
    [110](#bib.bib110), [3](#bib.bib3)]. From their experimental results, popular
    architectures (e.g., ResNet, Inception, and DenseNet) achieved promising performance
    in cell classification. To provide the interpretability analysis of this CNN-based
    classification, [[143](#bib.bib143)] designed Grad-CAM to show region of interests
    (ROIs) in network decision-making using the gradient information of the last convolution
    layer of CNN. [[163](#bib.bib163)] utilized Grad-CAM to generate heatmaps for
    observing high activation areas on typical regions lung cytology images. By observing
    the model’s high response regions of urothelial cytology images via Grad-CAM,
    [[115](#bib.bib115)] concluded that the color of tumor nuclei contributes to the
    prediction of the model most.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from binary classification (i.e., benign and malignant), multi-class scenarios
    are more common, yet challenging in cytology image analysis, because benign and
    malignant cells mainly include several sub-categories [[169](#bib.bib169)]. For
    example, Herlev dataset contains 3 types of normal cervical cells and 4 types
    of abnormal cervical cells [[60](#bib.bib60)]. However, the boundaries between
    the image features of two sub-categories are usually ambiguous, which brings challenges
    for CNNs to learn distinguishable features. To solve these issues, [[87](#bib.bib87)]
    proposed a fine-grained classification model for cervical cells. This model introduced
    mask maps as the morphological appearance information for enhancing fine-grained
    distinguishable features of cells.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, several cytological studies focus on improving the model’s performance
    in limited or imbalanced datasets. GAN-based models can be utilized to augment
    original dataset for improving the performance of the classification task. For
    example, [[183](#bib.bib183)] adopted GAN-based data augmentation to improve cervical
    cell classification models. [[35](#bib.bib35)] synthesized 180 images by conditional
    GAN. Together with the original data, these images are utilized to train three
    common models (ResNet-152, DenseNet-161, and Inception-V3), achieving significant
    improvement in FNAC image classification. For the imbalanced dataset problem,
    the number of positive samples is always far less than the negative ones in cytological
    scenarios [[183](#bib.bib183), [8](#bib.bib8)]. A few studies employed sampling
    techniques to balance different classes [[81](#bib.bib81), [8](#bib.bib8)]. For
    learning-based solutions, [[183](#bib.bib183)] adopted GAN to balance different
    classes by synthesizing images for those classes with far less amount than others.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, some other advanced methods have been investigated for cytology classification.
    [[80](#bib.bib80)] introduced an attention mechanism block to guide the network
    to focus on cell areas, thus improving the capability of extracting deep features.
    Then, they added a pyramid pooling layer and a long short-term memory module (LSTM)
    to aggregate image features in different regions. To improve the classification
    performance, [[146](#bib.bib146)] proposed a cervical cell classification method
    based on graph convolutional network (GCN), which can explore the potential relationship
    of cervical cell images.
  prefs: []
  type: TYPE_NORMAL
- en: In clinic practice, DL-based classification approaches have been widely applied
    for various types of cancers, including cervix [[144](#bib.bib144)], breast [[108](#bib.bib108)],
    lung [[161](#bib.bib161)], thyroid [[8](#bib.bib8)], urine [[169](#bib.bib169)],
    nasal [[36](#bib.bib36)] and skin [[116](#bib.bib116)]. Specifically, [[169](#bib.bib169)]
    proposed a VGG-based model for classifying urine cytopathology images. [[8](#bib.bib8)]
    developed a VGG-based model for thyroid nodule cell classification. For cervical
    cytology, [[10](#bib.bib10)] compared AI-assisted techniques with skilled cytologists
    in detecting cervical intraepithelial neoplasia or invasive cancer. For skin cytology,
    [[116](#bib.bib116)] proposed a ResNet-based model to identify cells in the cytology
    of erosive‑vesiculobullous diseases. These studies demonstrated the substantial
    clinical value of classification-assisted cytology image analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Slide-level classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different from cell-level classification, the goal of the slide-level classification
    model is to predict the category of whole images instead of cell samples.
  prefs: []
  type: TYPE_NORMAL
- en: Giga-pixel WSI classification systems are being investigated for efficient and
    high-accuracy predictions. Some studies built slide-level classification systems
    by multi-stage designs. For example, [[29](#bib.bib29)] designed a robust and
    progressive WSI analysis method for cervical cancer screening. In the first stage,
    the authors developed a progressive lesion cell recognition method combining low-
    and high-resolution WSIs. Then, a RNN-based WSI classification model was built
    for WSI-level predictions in the second stage. In another slide-level study, [[174](#bib.bib174)]
    designed a lightweight model (YOLCO) based on YOLO series [[127](#bib.bib127)]
    to make local predictions (e.g., cell-level, patch-level) in the first stage,
    which can enrich the multi-scale connectivity by additional supervision of spatial
    information. In the second stage, these local predictions were input to a transformer
    architecture for WSI-level results. Its experimental results showed that the framework
    presented a higher AUC score and $2.51\times$ faster than the state-of-the-art
    methods in WSI classification. For accurate and efficient screening of cervical
    cancer, [[199](#bib.bib199)] developed a complete cervical LBC smear TBS diagnostic
    system. This system integrated XGBoost and a logical decision tree with three
    typical DL models, i.e., Xception for classification, YOLOv3 for object detection,
    and U-Net for segmentation. This diagnostic system can reduce cytologists’ workload,
    improve the accuracy of cervical cancer screening.
  prefs: []
  type: TYPE_NORMAL
- en: Weakly supervised learning strategies are introduced to learn information from
    limited annotations in slide-level classification. Weakly supervised learning
    is appealing for this scenario. For example, [[38](#bib.bib38)] presented a MIL
    model for thyroid cancer malignancy prediction from cytopathology images. Then,
    an attention module was integrated into this MIL-based model with maximum likelihood
    estimation (MLE) architecture. The experimental results showed the competitive
    performance in thyroid malignancy prediction. [[79](#bib.bib79)] developed mixed
    supervision learning for WSI classification by effectively utilizing their various
    labels (e.g., sufficient image-level coarse annotations and a few pixel-level
    fine labels).
  prefs: []
  type: TYPE_NORMAL
- en: By introducing advanced strategies or designs, quite a few cytology studies
    investigated to improve classification performance. For example, [[20](#bib.bib20)]
    integrated the attention module into multi-scale region-based CNN (feature pyramid
    network) between upsampling and downsampling pathway. Three experiments in different
    levels consisting of cell-level detection, patch-level, and case-level classification
    demonstrated the effectiveness of the introduced attention mechanism. Other studies
    designed different auxiliary tasks (e.g., detection, segmentation) to assist the
    classification task. [[158](#bib.bib158)] employed an object detection model (Faster
    R-CNN) to assist classification for cancer screening. [[65](#bib.bib65)] introduced
    U-Net for improving the classification of squamous cell abnormalities.
  prefs: []
  type: TYPE_NORMAL
- en: Risk stratification is one important task of slide-level classification, which
    determines the risk level of patients suffering from diseases. [[7](#bib.bib7)]
    designed a DL-based digital cell profile for risk stratification of urine cytology
    images. In this system, RetinaNet was adopted for cell-level classification and
    detection in the first stage. For WSI-level risk stratification, they identified
    low-risk and high-risk cases using the count of atypical cells and the total count
    of atypical and malignant cells. [[86](#bib.bib86)] presented a dual-path network
    for cervix risk stratification, which can be divided into two steps. Firstly,
    an efficient CNN with a dual-path encoder was proposed for lesion retrieval, which
    can ensure the inference efficiency and sensitivity on both tiny and large lesions.
    Then, a smear-level classifier (rule-based risk stratification) was introduced
    to align reasonably with the intricate cytological definition of the classes.
    Extensive experiments on a huge dataset consisting of 19,303 WSIs from multiple
    medical centers validated the robustness of this risk stratification method.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In cytology image analysis, developing automatic detection methods to find
    tiny objects (e.g., malignant cells and nuclei) in the whole image is crucial
    to reduce experts’ tedious and time-consuming workflow. DL-based object detection
    has achieved significant progress in medical image analysis, which can be divided
    into two categories: 1) One-stage method, which directly regresses the category
    and location of instance objects in a single architecture. 2) Two-stage method,
    which firstly predicts object candidates in the first stage and then classifies
    and localizes them in the second stage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Overview of deep learning-based detection studies for computational
    cytology'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Application | Staining | Organ | Method | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| One-stage methods |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[66](#bib.bib66)] | Nuclei detection | Pap | Pleural effusion | YOLOv3 |
    Private dataset: 200 images with 11,157 nuclei | Precision: 0.941, Recall=0.9898,
    F1=0.9648, Test time=0.060 sec/img. |'
  prefs: []
  type: TYPE_TB
- en: '| [[177](#bib.bib177)] | Automation-assisted cervical cancer reading | Feulgen
    | Cervix | YOLOv3 | Private dataset: 12,909 images with 58,995 ground truth boxes
    in 10 categories | Detection: mAP=0.602; Classification: Sens=0.975, Spec=0.687.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[111](#bib.bib111)] | Hematological diagnosis | Giemsa | Bone marrow | YOLOv4
    | Private dataset: 75,000 annotated tiles of bone marrow aspirate | Region detection:
    Acc=0.97, AUC=0.99; Cell detection: mAP=0.75, F1-score=0.78. |'
  prefs: []
  type: TYPE_TB
- en: '| [[82](#bib.bib82)] | Cell detection | Pap | Cervix | Global context-aware
    + Soft scale anchor matching | Private dataset: 12,909 cervical images with 58,995
    ground truth boxes corresponding to 10 categories objects | mAP=0.6544. |'
  prefs: []
  type: TYPE_TB
- en: '| Two-stage methods |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[81](#bib.bib81)] | Cell detection and classification | Pap | Cervix | Faster
    R-CNN + Transfer learning | Private dataset: 680 LBC cervical exfoliated cell
    samples | Classification: Acc=0.9161 Detection: mAP=0.6698. |'
  prefs: []
  type: TYPE_TB
- en: '| [[53](#bib.bib53)] | Nuclei detection, Estimate proliferation rate | H&E
    | Kidney | R-CNN | Private dataset: 16,905 segmented cancer cell and 22,948 normal
    cell nuclei | P=0.9901, R=0.9870, F1=0.988. |'
  prefs: []
  type: TYPE_TB
- en: '| [[121](#bib.bib121)] | Localization and detection of abnormalities | Pap
    | Cervix | Weakly supervised CNN + Regression constraint | Herlev | Severity classification:
    Acc=0.952; Normal/abnormal classification: Acc=0.952, KAPPA score=0.870; Detection:
    Acc=0.804. |'
  prefs: []
  type: TYPE_TB
- en: '| [[21](#bib.bib21)] | Cell detection | Pap | Cervix | Faster R-CNN + Deep
    metric learning | Private dataset: 240,860 images | 100% labeled: mAP=0.27; 75%
    labeled: mAP=0.254; 50% labeled: mAP=0.195. |'
  prefs: []
  type: TYPE_TB
- en: '| [[155](#bib.bib155)] | Ascites cytopathology interpretation | Pap, H&E |
    Stomach | Classification: pre-trained AlexNet, VGG-16, GooleNet, ResNet18, and
    ResNet-50\. Detection: Faster R-CNN | Ascites 2020 | Classification: AUC=88.51
    (ResNet50); Detection: IoU=0.8722, mAP=0.8316. |'
  prefs: []
  type: TYPE_TB
- en: '| [[178](#bib.bib178)] | Cell detection | H&E | Cervix | Fully residual CNN
    + Structured regression | HeLa cervical cancer | Precision=0.98, Recall=0.98,
    F1=0.98. |'
  prefs: []
  type: TYPE_TB
- en: '| [[104](#bib.bib104)] | Quantification of pulmonary hemosiderophages | Prussian
    turnbull | Lung | ResNet-18, FPN | Private dataset: 17 WSIs with 78,047 hemosiderophages
    | Concordance=0.85, mAP=0.66. |'
  prefs: []
  type: TYPE_TB
- en: '| [[186](#bib.bib186)] | Cervical cytology analysis | Pap | Cervix | Lesion
    cell detection: Faster R-CNN and RetinaNet. Cell type classification: Inception-v3,
    ResNet-101, and DenseNet-121 | Private dataset: 1,167 WSIs with 14,432 image patches,
    and 27,972 labeled lesion cells | Detection: mAP=0.2116 (Faster R-CNN); Classification:
    Acc=0.8884, F1=0.5996 (DenseNet-121). |'
  prefs: []
  type: TYPE_TB
- en: '| [[83](#bib.bib83)] | Cervical cancer screening (cell/clumps detection) |
    Pap | Cervix | Faster R-CNN + Few-shot learning + Prototype representation | CDetector
    | mAP=0.488. |'
  prefs: []
  type: TYPE_TB
- en: '| [[12](#bib.bib12)] | Nuclei detection | Pap | Pleural effusion | Detector:
    Faster R-CNN, R-FCN and SSD | Private dataset: 200 images (11,157 nuclei) | Faster
    R-CNN (ResNet-101): F1=0.9812. |'
  prefs: []
  type: TYPE_TB
- en: 4.3.1 One-stage methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One-stage algorithms detect objects by directly generating the category and
    coordinates of objects with the advantages of high detection efficiency, such
    as SSD [[92](#bib.bib92)], YOLO [[127](#bib.bib127)], and RetinaNet [[89](#bib.bib89)].
  prefs: []
  type: TYPE_NORMAL
- en: Many works in cytology introduce the YOLO model as their base network due to
    its high efficiency. For the structure of YOLO, it divides the original image
    into an $S\times S$ grid cell. Then, YOLO predicts bounding boxes, confidence
    for each cell. Afterwards, redundant boxes are removed by the confidence threshold
    and non-maximum suppression. [[177](#bib.bib177)] used YOLO as their detector
    for cervical cells. Similarly, [[66](#bib.bib66)] adopted YOLO to detect nuclei
    in pleural effusion cytology. The authors compared the detection efficiency between
    one-stage and two-stage detectors [[128](#bib.bib128)]. Its experimental result
    showed that YOLO achieved a test speed of 0.060 second/image that was much faster
    than 1.627 second/image in Faster R-CNN (two-stage detector). Besides, [[111](#bib.bib111)]
    applied YOLO on selected appropriate ROI tiles to automatically detect and classify
    bone marrow cellular and non-cellular objects. To improve the performance of YOLO
    in cervical cell detection, [[82](#bib.bib82)] proposed a global context-aware
    framework by introducing an image-level classification branch and a weighted loss
    that can filter false positive predictions.
  prefs: []
  type: TYPE_NORMAL
- en: To improve the feature extractor for learning multi-scale features, RetinaNet
    was proposed by using feature pyramid network (FPN) as its feature extractor,
    which achieved the state-of-the-art detection performance [[88](#bib.bib88)].
    [[104](#bib.bib104)] employed RetinaNet for generating rich and multi-scale features
    for functional head (e.g., box, regression, and classification). These results
    contributed to the quantification of pulmonary hemosiderophages in this work.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Two-stage methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Two-stage methods use different region proposal strategies to generate bounding
    boxes, such as sliding windows [[44](#bib.bib44)], selective search [[141](#bib.bib141)],
    and region proposal network [[128](#bib.bib128)]. For example, Fast R-CNN designed
    selective search strategy to generate bounding boxes. Then, the ROI pooling layer
    extracts the features of each ROI. Fast R-CNN outputs softmax probabilities and
    per-class bounding-box regression offsets with a multi-task loss [[44](#bib.bib44)].
    To integrate different modules and increase the speed [[128](#bib.bib128)], Faster
    R-CNN improves Fast R-CNN by integrating feature extraction, proposal, bounding
    box regression, and classification. It designs region proposal networks (RPN),
    which uses bounding box regression for accurate region proposal.
  prefs: []
  type: TYPE_NORMAL
- en: To detect cell objects in the whole cytology image, some studies employed Faster
    R-CNN as their base architecture. For example, [[81](#bib.bib81)] utilized Faster
    R-CNN to detect cervical exfoliated cells on the LBC dataset. Similarly, Faster
    R-CNN was also used to detect tumor cells for further classification, which formed
    an ascites cytopathology image interpretation system [[155](#bib.bib155)].
  prefs: []
  type: TYPE_NORMAL
- en: For different cytological scenarios, researchers improved detection performance
    by modifying architectures or integrating with other strategies [[53](#bib.bib53),
    [104](#bib.bib104)]. For efficient cell and robust detection, [[178](#bib.bib178)]
    presented a structured regression model based on a proposed fully residual CNN.
    This model produced a dense proximity map that exhibited higher responses at locations
    near cell center. Then, training this model only required annotations of the dot
    instead of the traditional box, which can improve efficiency of annotating. Several
    studies paid attention to weakly supervised learning settings in cytological detection.
    For example, [[121](#bib.bib121)] proposed a computer-aided diagnosis tool for
    cervical cancer screening. In this method, the authors designed a weakly supervised
    localization strategy, which performed the Integrated Gradient method [[156](#bib.bib156)]
    to compute attribution maps and morphological operations to obtain the localization
    boxes. In another work, [[21](#bib.bib21)] proposed a semi-supervised deep metric
    learning method to improve intra-class feature compactness for cervical cancer
    cell detection. This model learned an embedding metric space and conducted dual
    alignment of semantic features on both the proposal and prototype levels. From
    their quantitative experiments, detection of cervical cancer cell can be a challenging
    study, especially for some cell classes, like ASC-US.
  prefs: []
  type: TYPE_NORMAL
- en: The clinic practice not only requires high detection accuracy but also efficiency
    because faster detection speed is more suitable for large-scale screening scenarios
    [[85](#bib.bib85)]. Detection models usually face trade-offs between the accuracy
    and the speed. For example, two-stage detectors (e.g., Faster R-CNN) can achieve
    higher detection results while one-stage detectors (e.g., YOLO) have advantages
    in faster detection speed. In cytological studies, [[186](#bib.bib186)] compared
    the performance between two-stage (Faster R-CNN) and one-stage (RetinaNet) methods
    for the detection of cervical lesion cells. The results showed that the former
    one achieved better experimental results in average precision. In another work
    [[83](#bib.bib83)], the authors improved Faster R-CNN and compared it with baseline
    and RetinaNet in a limited data scenario. The results in cervical cell/clumps
    detection showed that RetinaNet (one-stage) achieved significantly faster speed
    (FPS). [[12](#bib.bib12)] compared three detectors, i.e., Faster R-CNN (two-stage),
    R-FCN (two-stage), and SSD (one-stage). As a result, R-FCN achieved a higher mAP
    score while SSD spent less time when testing. Their experimental results validated
    the trade-offs of these detection models between speed and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The segmentation task aims at morphologically delineating the object contour.
    For segmentation models, they assign each pixel of the image to a specific category,
    so it can be regarded as a pixel-wise classification task. In cytological screening,
    segmentation is an essential step for different applications, including 1) separating
    cells/clump and background from specimens, 2) morphologically distinguishing cell
    types, 3) accurately segmenting cellular structures, such as nuclei and cytoplasm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main challenge of cytology segmentation is accurately segmenting overlapping
    areas between cells [[95](#bib.bib95), [96](#bib.bib96)]. To address this issue,
    there are mainly two solution schemes. One is dividing the cytological segmentation
    task into two stages. The first stage is to utilize a semantic segmentation model
    (e.g., U-Net) for a coarse result, followed by a series of refinement designs
    for overlapping areas, thus obtaining the final accurate segmentation result.
    The other is based on the detect-then-segment paradigm (e.g., Mask R-CNN), which
    detects cytology objects in whole images and output a segmentation map by mask
    prediction head. This type of approach can segment objects from each detected
    instance in an end-to-end architecture without any refinement design. Therefore,
    we divide the solutions of cytology segmentation into two categories: 1) segment-then-refine
    method, 2) detect-then-segment method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Overview of deep learning-based segmentation studies for computational
    cytology'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Application | Staining | Organ | Method | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| Segment-then-refine methods |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[41](#bib.bib41)] | Cell counting, detection, and morphometry | Fluore-
    scence | Various | U-Net | ISBI cell tracking 2015 | IoU=0.9203 (PhC-U373), IoU=0.7756
    (DIC-HeLa). |'
  prefs: []
  type: TYPE_TB
- en: '| [[105](#bib.bib105)] | Segmentation, detection, and classification of cell
    nuclei | Pap | Oral | Classification: ResNet-34\. Detection: Faster R-CNN. Segmentation:
    U-Net | Oral 2021 | Classification: Acc=0.88, F1=0.86; Detection: IoU=0.5832;
    Segmentation: IoU=0.4607. |'
  prefs: []
  type: TYPE_TB
- en: '| [[151](#bib.bib151)] | Segmentation of cytoplasm and nuclei | H&E | Cervix
    | CNN+ Coarse to fine segmentation | Private dataset: 53 slides with 1400 cells
    | Nuclei region detection: Acc=0.9450, F1=0.9453; Segmentation: F1=0.8951±0.0215.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[150](#bib.bib150)] | Segmentation of cytoplasm and Nuclei | H&E | Cervix
    | Multi-scale CNN + Graph partitioning + Touching cell splitting | Private dataset:
    53 images (slide) | Cytoplasm: Dice=0.95; Nuclei: Dice=0.99. |'
  prefs: []
  type: TYPE_TB
- en: '| [[149](#bib.bib149)] | Cell segmentation | Pap H&E | Cervix | Multi-scale
    CNN + Dynamic multi-template deformation | ISBI 2015\. Private dataset: 21 images
    (each image has 30$\sim$80 cells) | ISBI 2015: Dice=0.89; Private dataset: Dice=0.84.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[6](#bib.bib6)] | Cell image segmentation and ranking | Pap | Cervix | CNN
    | BHS 2019 | Segmentation: P=0.73, R=0.65, F1=0.69, Time=4.75s; Ranking: mAP=0.936.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[73](#bib.bib73)] | Cell nuclei segmentation | H&E | Breast | CNN + Seeded
    watershed | Public dataset: 80 images | Benign: Hausdorff distance=0.840, Jaccard
    distance=0.776; Malignant: Hausdorff distance=0.781, Jaccard distance=0.732. |'
  prefs: []
  type: TYPE_TB
- en: '| [[16](#bib.bib16)] | Semantic instance segmentation of touching and overlapping
    objects | Pap | Cervix | U-Net | OSC-ISBI | Dice=0.895±.0.079. |'
  prefs: []
  type: TYPE_TB
- en: '| [[187](#bib.bib187)] | Cervical cell segmentation | Pap | Cervix | Attention
    mechanism + U-Net + Random walk | ISBI 2014 | Nuclei: $P_{p}$=0.94 ±0.06, $R_{p}$=0.95
    ±0.05, Dice=0.93 ±0.04; Cytoplasm: $TP_{p}$=0.94 ±0.06, $FP_{p}$=0.003 ±0.004,
    Dice=0.93 ±0.07. |'
  prefs: []
  type: TYPE_TB
- en: '| [[171](#bib.bib171)] | Instance segmentation | Pap | Cervix | U-Net + Star-convex
    polygons | OSC-ISBI | Dice=0.85 ± 0.07. |'
  prefs: []
  type: TYPE_TB
- en: '| [[56](#bib.bib56)] | Segmentation and classification of cervical nuclei |
    Pap | Cervix | U-Net | Herlev | Classification: Acc=0.988; Segmentation: ZSI=0.97.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[159](#bib.bib159)] | Cytological examination (overlapping cell segmentation)
    | Pap | Cervix | CNN+ Shape prior (dynamic shape modeling) | ISBI 2014 | Nuclei:
    $P_{p}$=0.94 ±0.06, $R_{p}$=0.95 ±0.06, ZSI=0.94 ±0.04; Cytoplasm: ZSI=0.90 ±0.08.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[152](#bib.bib152)] | Overlapping cytoplasms segmentation | Pap H&E | Cervix
    | Shape mask generator + Refining shape priors | ISBI 2015\. Private dataset:
    160 clumps with 962 cytoplasms | Pap: Dice=0.854 ± 0.049; H&E: Dice=0.846 ± 0.054.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[172](#bib.bib172)] | Nuclei detection, cytoplasm segmentation | Pap | Cervix
    | CNN + Double-window + Image processing + Deeplab V2 + CRFs + Cell boundary refinement
    | ISBI 2014; ISBI 2015; Private dataset: 580 image (patch) | ISBI 2014: Dice=0.93
    ± 0.04; ISBI 2015: Dice=0.92 ± 0.05; Private: Dice=0.92 ± 0.04. |'
  prefs: []
  type: TYPE_TB
- en: '| Detect-then-segment methods |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[148](#bib.bib148)] | Pap smear cancer screening | Pap | Cervix | Mask R-CNN
    + Fine-tuning | Private dataset: 178 images in classes of normal (2,734 ), atypical
    (494), low-grade (148), and high-grade cells (84) | Image-level: mAP=0.578, Acc=0.917,
    Sens=0.917,Spec=0.917; Nucleus: Acc=0.898, Sens=0.725, Spec=0.943. |'
  prefs: []
  type: TYPE_TB
- en: '| [[195](#bib.bib195)] | Cell segmentation | Pap | Cervix | PRN + Cell association
    matrix | Private dataset: 413 images (annotated 4,439 cytoplasm and 4,789 nuclei)
    | Cytoplasm: AJI=0.7185, F1=0.7497; Nuclei: AJI=0.5496, F1=0.7554. |'
  prefs: []
  type: TYPE_TB
- en: '| [[194](#bib.bib194)] | Cell instance segmentation | Pap | Cervix | RPN +
    Knowledge distillation | Private dataset: 413 labeled (4,439 cytoplasm and 4,789
    nuclei) and 4,371 unlabeled images | 100% labeled: AJI=0.6643, mAP=40.52; 80%.
    labeled: AJI=0.6692, mAP=0.4013; 40% labeled: AJI=0.6449, mAP=0.3726. |'
  prefs: []
  type: TYPE_TB
- en: 4.4.1 Segment-then-refine method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cytological structures can be segmented by segmentation models, like U-Net.
    However, overlapping areas belonging to several cells bring defiance of accurately
    segmenting each cell structure. To overcome this issue, different refinement strategies
    are proposed to add after the coarse segmentation model for fine-level segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: The segmentation network was originally implemented by establishing a pixel-wise
    classification network through CNN. [[150](#bib.bib150)] designed a multi-scale
    convolutional network for coarse segmentation. [[73](#bib.bib73)] proposed a more
    complex CNN structure consisting of four convolutional layers, two max-pooling
    layers, and one fully connected layer. This architecture was utilized in the first
    stage for nuclei segmentation in cytological images.
  prefs: []
  type: TYPE_NORMAL
- en: Afterwards, U-Net almost replaces the previous pixel-level classification network
    after its occurrence, especially in biomedical image segmentation [[133](#bib.bib133)].
    U-Net is a downsampling-upsampling structure with skip connections for combining
    low-level and high-level features. Recently, U-Net has made great achievements
    in medical image segmentation. For example, [[41](#bib.bib41)] designed U-Net
    for cell counting, detection, and segmentation. This work illustrated its potential
    for cellular structure analysis. In cytology image segmentation, U-Net has been
    introduced as the backbone for segmenting cellular objects in various cytology,
    such as oral [[105](#bib.bib105)], cervix [[6](#bib.bib6)], and breast [[73](#bib.bib73)].
    Several works focus on improving the performance of U-Net to enhance their capacities
    of cell segmentation. For instance, [[16](#bib.bib16)] proposed to mix 2D and
    3D U-Net for semantic instance segmentation of touching objects. [[187](#bib.bib187)]
    introduced attention mechanism to improve U-Net for focusing on ROIs. Besides,
    [[56](#bib.bib56)] improved U-Net by adding residual blocks, densely connected
    blocks, and a fully convolutional layer as a bottleneck between encoder-decoder
    blocks for nuclei segmentation in cervical images.
  prefs: []
  type: TYPE_NORMAL
- en: 'In segment-then-refine methods, the second stage is to address the issue of
    overlapping and refine segmentation results. Most of them take the shape prior
    of cell into considerations. For instance, [[149](#bib.bib149)] proposed a dynamic
    multi-template deformation model together with high-level morphological constrain
    for further boundary refinement. [[73](#bib.bib73)] designed a series of refinement
    strategies in the second stage: conditional erosion for determining nuclei seeds,
    the seeded watershed for separation overlapping nuclei, and aggregating segmentation
    results for overlapping and non-overlapping nuclei. In addition, [[187](#bib.bib187)]
    proposed a graph-based random walk method for extracting both nucleus and cytoplasm
    of overlapping cervical cells. This method utilized polar coordinate sampling
    for removing fake nuclei. Its experimental results in ISBI 2014 dataset showed
    the performance improvement on extracting an individual cell from heavy overlapping
    cell clumps. [[171](#bib.bib171)] proposed to predict object probability, star
    distance, and overlap probability based on U-Net. Then, non-maximum suppression
    was used to generate overlapping cell segmentation results. These refinement strategies
    can achieve more accurate segmentation results, especially for overlapping regions.
    However, complex clinical data will present more challenges to the reproducibility
    and generalizability.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, some other models have been proposed for cytological segmentation.
    [[159](#bib.bib159)] designed a two-stage segmentation model, which consists of
    initial segmentation based on Voronoi diagram, and final segmentation with learning
    shape prior model. In order to segment overlapping cervical cytoplasms, [[152](#bib.bib152)]
    proposed a shape mask generator to refine shape priors. [[172](#bib.bib172)] presented
    an architecture for cell detection and cytoplasm segmentation. In this method,
    conditional random field algorithm (CRFs) and cell boundary refinement were utilized
    to achieve accurate segmentation of overlapping cells in cervical cytology.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Overview of deep learning-based studies of other tasks for computational
    cytology'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Application | Staining | Organ | Method | Dataset | Result |'
  prefs: []
  type: TYPE_TB
- en: '| [[99](#bib.bib99)] | Super resolution | Pap | Cervix | Image registration
    + GAN | Private dataset: 142 WSIs (118 for training and 24 for testing) with 174,500
    patches | PSNR=26.92, SSIM=0.88, MOS=3.80. |'
  prefs: []
  type: TYPE_TB
- en: '| [[98](#bib.bib98)] | Super resolution | Pap | Cervix | Backbone + Self-texture
    + Flexible reconstruction | Public dataset: 5 slides (25,000 patches) | PSNR=35.47,
    SSIM=0.958, MSE=22.07. |'
  prefs: []
  type: TYPE_TB
- en: '| [[164](#bib.bib164)] | Mutual stain conversion | Giemsa and Pap | Lung |
    CycleGAN | Private dataset: 191 Giemsa-stained images and 209 Papanicolaou-stained
    images | T test: P-value $<$0.001. |'
  prefs: []
  type: TYPE_TB
- en: 4.4.2 Detect-then-segment method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the segmentation of instance objects in cytology images, these methods
    follow the detect-then-segment paradigm, which divides this task into two steps:
    detecting all objects in whole images, then segmenting instances from each detected
    object. As one popular architecture of this paradigm, Mask R-CNN improves Faster
    R-CNN by adding full connected layers as the segmentation head. Thus, it can output
    the prediction of classification, detection, and segmentation via a single architecture
    [[51](#bib.bib51), [128](#bib.bib128)].'
  prefs: []
  type: TYPE_NORMAL
- en: Instance segmentation is regarded as one of the most challenging tasks in cytology
    image analysis, because it not only predicts the instance morphology but also
    distinguishes different instances (e.g., cytoplasm, nucleus). Building detect-then-segment
    models can solve this problem, since they can predict instance detection and segmentation
    results through a single architecture. A few studies investigated this category
    of cytological segmentation methods. Existing researches almost employ Mask R-CNN
    as their architecture, because there is no need for further design of overlapping
    areas in this category of methods. For example, [[148](#bib.bib148)] adopted Mask
    R-CNN for specifying the bounding box, nucleus mask, and class of each cervical
    cell. In another study [[195](#bib.bib195)], authors utilized the multi-head attention
    mechanism to explore instance-level association by propagating features based
    on attention scores. Consequently, this proposed model improved the instance representation
    and achieved better instance segmentation performance than original Mask R-CNN.
    To further reduce reliance on large amounts of labeled data, [[194](#bib.bib194)]
    proposed a semi-supervised learning approach to leverage both labeled and unlabeled
    data for instance segmentation by knowledge distillation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Other tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to the typical deep learning tasks, i.e., classification, detection,
    and segmentation, a few other tasks of cytology image analysis have also been
    investigated, such as super-resolution (SR), and stain conversion (Table [8](#S4.T8
    "Table 8 ‣ 4.4.1 Segment-then-refine method ‣ 4.4 Segmentation ‣ 4 Deep learning
    in cytology application ‣ Deep Learning for Computational Cytology: A Survey")).
    In the cytopathology screening, low-resolution and out-of-focus images will harm
    the decision-making process of cytologists, thus high-resolution digital cytopathology
    slides are the prerequisite for the interpretation of lesion cells. To control
    the image quality, super-resolution models are designed to generate high-resolution
    images. [[99](#bib.bib99)] introduced a GAN-based progressive multi-supervised
    super-resolution model (PathSRGAN) to learn the mapping of real low-resolution
    and high-resolution images. After that, they designed a self-texture transfer
    super-resolution and refocusing network (STSRNet) to reconstruct HR multi-focal
    plane (MFP) images from a single 2D low-resolution (LR) wide filed image [[5](#bib.bib5)].
    As mentioned in section [4.1](#S4.SS1 "4.1 Preprocessing ‣ 4 Deep learning in
    cytology application ‣ Deep Learning for Computational Cytology: A Survey"), different
    staining methods are used to observe different cell structures and components.
    DL-based stain conversion can be used for staining normalization and eliminate
    data heterogeneity issues. [[164](#bib.bib164)] proposed a CycleGAN-based style
    transfer model for stain conversion between Giemsa-stained and Pap-stained images.
    This study performed visual evaluations of the authenticity of cell nuclei, cytoplasm,
    and cell layouts of synthetic lung cytology images.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Challenges and Promises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite great advancements and improvements in computational cytology over the
    last few years, there are still quite a few challenges and opening problems that
    are waiting to be resolved. Meanwhile, the development of deep learning technologies
    and pathology is continuously bringing vigor and vitality into this emerging field.
    In this section, we further discuss prospects and potential research directions
    of computational cytology.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Label-efficient learning with limited annotations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data is regarded as the prerequisite of learning-based methods, because it
    is hard to develop effective models without good quality datasets [[41](#bib.bib41),
    [63](#bib.bib63)]. In medical image analysis, large-scale labeling can be a heavy
    burden for cytologists, because they need to first manually delineate ROIs in
    WSIs, then annotate each object (e.g., nucleus, cell, and cluster) in these ROIs
    by the box or mask. Compared with the extensive dataset in histopathology (such
    as TCGA [[168](#bib.bib168)]), public datasets of cytology are more limited not
    only in their numbers, but also in cancer types and annotation types (see in Table
    [1](#S2.T1 "Table 1 ‣ 2.4 Transfer Learning ‣ 2 Deep learning methodology ‣ Deep
    Learning for Computational Cytology: A Survey")). Therefore, how to efficiently
    utilize datasets with limited annotations can be challenging for developing cytological
    analysis models [[38](#bib.bib38), [21](#bib.bib21)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, the concept of label-efficient learning is proposed to makes
    full use of the limited annotations for leveraging information, including semi-supervised
    learning, multiple instance learning, mixed supervised learning, etc. Specifically,
    semi-supervised learning aims to solve this problem by learning knowledge from
    both labeled data and unlabeled data. Recently, [[194](#bib.bib194)] designed
    a semi-supervised learning method by a mask-guided teacher-student framework for
    overlapping cell instance segmentation. Another learning scheme, MIL utilizes
    image-level annotations for instance-level tasks, which has been investigated
    in the field of medical image analysis, especially for histopathology. Recently,
    [[38](#bib.bib38)] proposed a MIL-based algorithm in thyroid cytology, which can
    simultaneously predict multiple bag and instance-level labels for thyroid malignancy
    prediction from WSIs. However, the potential of MIL in pap smear image and other
    cytology images remains to be explored. As illustrated in Table [1](#S2.T1 "Table
    1 ‣ 2.4 Transfer Learning ‣ 2 Deep learning methodology ‣ Deep Learning for Computational
    Cytology: A Survey"), there are usually different annotation types in different
    datasets, including box, mask, and the image-level label. Recently, mixed supervised
    learning gains popularity in analyzing images with different types of annotations.
    It has been demonstrated as an effective learning scheme in medical domain. For
    example, [[97](#bib.bib97)] present a deep omni-supervised thoracic disease detection
    network from chest X-rays with massive image-level annotations and scarce lesion-level
    annotations. This learning paradigm can substantially reduce the demand for fine
    annotation, thus reducing workload of doctors significantly.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the effective utilization of annotations, it is also promising
    to build efficient labeling approaches for reducing the burden on annotators.
    For example, introducing human knowledge into the loop of annotation can effectively
    and actively obtain accurate and credible annotations. [[70](#bib.bib70)] proposed
    an annotating method named NuClick with a squiggle as a guiding signal, enabling
    it to segment the glandular boundaries. However, this method still requires human
    full attention to annotate samples, which is a huge cost for society and tedious
    for human experts. For cytology images, they always contain numerous cells, especially
    in giga-pixel WSIs, how to establish an effective labeling process remains largely
    unexplored.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Fine-grained classification and morphological feature characterization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although various deep learning applications have been developed in cytology
    image analysis (e.g., classification, detection, and segmentation), some of these
    tasks are worthy of further investigation, such as fine-grained classification
    for cancer diagnosis and instance segmentation in overlapping cell scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: As a fundamental task, cytological classification aims to distinguish between
    benign and malignant cells. However, there are many types of cells with varying
    degrees of cancerization. For example, the types of cervical cells include squamous
    cells and glandular cells, and they can be further subdivided into subcategories
    [[113](#bib.bib113)]. Besides, each type of cell has large intra-variance and
    small inter-variance. These factors bring significant challenges to deep feature
    extractors for learning distinguishable features. Further fine-grained classification
    of malignant cells can solve these problems and assist cytologists in accurate
    cancer diagnosis. [[186](#bib.bib186)] demonstrated the difficulty of fine-grained
    tasks compared to coarse-grained tasks. In their experiments, the classifier of
    cervical cytology showed significantly worse performance of fine-grained than
    coarse-grained tasks. Some studies introduced additional information and prior
    to learn fine-grained feature representations. For example, [[87](#bib.bib87)]
    built a fine-grained classification model for distinguishing cervical cells. The
    authors introduced cytoplasm and nucleus masks as morphological information to
    extract fine-grained features. Therefore, fine-grained feature extractors for
    learning subtle feature diversity are essential for cytology image analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another fundamental application, instance-level segmentation of cytoplasm and
    nucleus can be used to calculate the nuclear-cytoplasmic ratio, which is an essential
    indicator for distinguishing malignant cells. However, cell overlapping brings
    challenges for accurately instance segmenting cellular structures [[151](#bib.bib151),
    [150](#bib.bib150)]. A large amount of studies focus on this challenge, and most
    of them divide this task into two stages: semantic segmentation models for coarse
    segmentation, followed by refinement processing technologies for overlapping areas
    [[149](#bib.bib149), [187](#bib.bib187)]. These multiple-stage architectures introduce
    lots of human designs and interventions, leading to increasing training difficulties
    and poor generalizations. Recently, few studies investigate the feasibility of
    building end-to-end models by the detect-then-segment paradigm [[195](#bib.bib195)],
    which detect objects and then segment each detected object in a single learning
    framework. Although cytological segmentation has made some progress, for complex
    morphological feature representation, building end-to-end instance segmentation
    models still remains value to be studied and explored.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Effective feature representation learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In deep learning, the performance of downstream tasks will be greatly influenced
    by the feature representation capability of feature extractors. The main goal
    of DL models in cytology is to learn effective features of cytology images for
    cell classification, cellular objects detection and segmentation. Thus, building
    models that effectively represent features is crucial for cytology image analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, attention mechanism has made promising achievements in effective representation
    of image features, thus improving performances of down-stream tasks [[170](#bib.bib170)].
    The deep neural networks with attention modules can reduce the dependence on external
    information, and be better at capturing internal correlations of data or features.
    In cytology images, there are too many useless objects, like background, mucus,
    blood, and inflammatory cells [[55](#bib.bib55)]. Attention mechanism-based strategies
    can make deep models focus more on lesion-related regions or object-related (e.g.,
    nuclei, cytoplasm) regions. Besides, the attention mechanism not only improves
    the deep learning model but also provides convenience for model visualization
    and understanding. A few studies have validated the superiority of the attention
    mechanism in cytology image analysis. [[187](#bib.bib187)] utilized the attention
    mechanism to enhance U-Net for segmentation of overlapping cervical cells. Besides,
    [[195](#bib.bib195)] introduced a multi-head attention mechanism module into the
    instance segmentation model for improving instance representation. These studies
    integrated attention modules into the framework and achieved improved performances.
    After that, the popular structure with attention mechanism, transformer has been
    demonstrated its superiority of learning global dependencies in various applications
    [[37](#bib.bib37)]. Transformer-based structures are waiting to be investigated
    in cytology image analysis, such as building dependencies between different cells
    in patch images, or different regions in WSIs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Generalizability and robustness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The generalizability of DL-based algorithms in computational cytology determines
    whether the model can be successfully applied to actual clinical scenarios with
    various influences. At present, many DL approaches for medical image analysis
    can obtain acceptable performance in their own datasets, but their clinical performance
    is far from reaching practical standards [[90](#bib.bib90)].
  prefs: []
  type: TYPE_NORMAL
- en: Due to various specimen collection methods, staining techniques and imaging
    protocols, the data heterogeneity is the key reason for the poor clinical performance.
    It can lead to the weak robustness and generalization capability of DL models,
    thus performing poorly when applied to unseen data scenarios. Clinically, the
    performance of these DL models could degrade significantly, leading to low clinical
    reproducibility [[73](#bib.bib73)].
  prefs: []
  type: TYPE_NORMAL
- en: Extensive researchers are investigating to mitigate this issue. For image processing
    strategies, normalization methods are utilized to pre-process input data in many
    image analysis tasks. These methods provide limited benefit in DL-based medical
    image analysis, because two datasets can be influenced and normalized against
    each other [[101](#bib.bib101)]. For learning strategy, domain adaptation can
    alleviate this problem by transferring knowledge for decreasing the need for annotations
    of target tasks [[117](#bib.bib117)]. Domain adaptation has been used for other
    medical scenarios with cross-domain data (e.g., CT and MRI [[23](#bib.bib23)]),
    the potential of domain adaptation methods in cytology image analysis remains
    to be explored. For instance, building cross-domain learning framework to analyze
    multi-domain images, such as images from multi-center, differently stained cytology
    images, and even different pathology images (e.g., cytology and histopathology).
  prefs: []
  type: TYPE_NORMAL
- en: Designing new models for robust feature extraction of cytology images is also
    worthy of being explored, especially for feature extractor, which aims to map
    cytology images from multi-center to the same feature space for learning domain-invariant
    representations, thereby addressing the data heterogeneity issue [[79](#bib.bib79)].
    In addition, multimodal data has been demonstrated to compensate for the missing
    information in single modality data [[190](#bib.bib190)]. For medical scenarios,
    [[145](#bib.bib145)] designs a multimodal fusion framework to combine histopathological
    images and genomic sequences for early-stage cancer prognosis. Similarly, multimodality
    or full modality learning is also worth exploring and studying in cytology applications.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Transparency and interpretability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike other deep learning application scenarios, the medical domain not only
    focuses on the model performance in clinical practice but also its interpretability,
    which is of paramount importance in clinical decision-making. However, the black-box
    nature of DL algorithms lacking clinical interpretability and transparency has
    restricted its clinical adoption [[94](#bib.bib94)]. Therefore, explainable AI
    for medicine is introduced to establish the confidence between AI technologies
    and doctors/patients. There are some explorable issues in computational cytology,
    such as slide-level cytology screening based on rules, and visualization of the
    decision-making process of deep models.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization is regarded as one of techniques for interpretation. Current techniques
    mainly utilize attention mechanism to generate heatmaps to visualize the deep
    features and models, such as class activation mapping (CAM) [[193](#bib.bib193)].
    In cytology image analysis, a few studies have employed heat maps or feature maps
    for exploring the decision-making basis of deep learning models in the classification
    task. [[115](#bib.bib115)] used Grad-CAM to observe the model’s high response
    regions of urothelial cytology images, which can improve medical decision-making
    from the gradient of the differentiable model. For model-agnostic visualization,
    another technique, local interpretable model-agnostic explanation (LIME) [[131](#bib.bib131)]
    can be effective to provide the interpretability and transparency. LIME presents
    a locally faithful explanation by fitting a set of perturbed samples near the
    target sample using a potentially interpretable model. In addition to these mentioned
    methods, more visualization techniques are under exploration for interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: To increase the credibility of deep models in computational cytology, researchers
    constructed slide-level screening system by assembling multiple tasks rather than
    predicting the final diagnosis results of cytology slides, which can assist pathologists
    to obtain multi-stage analysis results. [[199](#bib.bib199)] integrated DL-based
    classification, detection, and segmentation models to build a cervical LBC smear
    TBS diagnostic system. Another study, [[86](#bib.bib86)] built a dual-path network
    for outputting the detected lesions, followed by a rule-based risk stratification
    system. [[174](#bib.bib174)] divided the cervical WSI analysis into two stages,
    i.e., cervical lesion detection at the patch level in preselected ROIs, and normal/abnormal
    classification at the WSI level. The output of each stage of these multi-stage
    methods can provide intermediate explanations for model prediction and cytology
    screening, thus improving the transparency of the diagnostic process.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Digital medicine and human-AI collaboration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Digital medicine integrates medicine and information technology for clinical
    diagnosis and treatment, it aims to transform digital models to clinical scenarios
    [[13](#bib.bib13), [140](#bib.bib140)]. Although a large number of DL models report
    that they can achieve state-of-the-art performance, they are mostly validated
    on domain-specific datasets and cannot achieve the same good performance in clinical
    practice. Unavailability of data is one of the crucial factors leading to this
    problem, because data privacy cannot be overemphasized in medicine domain. Currently,
    privacy-preserving learning approaches are being explored and studied for improving
    the availability of multi-center data [[27](#bib.bib27)], e.g., federated learning
    [[132](#bib.bib132)]. Another crucial issue facing clinical transformation is
    that clinical data is usually more diverse and complex than collected training
    data, caused by variable clinical factors regarding imaging microscopes, staining
    techniques, patch extraction, and selection, etc. To address this issue, designing
    more robust architectures can make the model less dependent on data quality in
    digital medicine. In addition, cytologists analyze specimens of different cancer
    types using different diagnostic criteria [[113](#bib.bib113)], which makes it
    difficult for DL algorithms that focus on domain-specific cytology to adapt to
    different cancer scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, numerous studies show that the human-machine collaborative
    medical diagnosis system can achieve better diagnosis performance than conventional
    diagnosis systems by the intervention of human experts and assistance of machines
    [[199](#bib.bib199), [119](#bib.bib119)]. The DL algorithm in the intelligent
    medical system provides doctors with multi-level, and high-confidence prediction
    results. Besides, the digital diagnostic system provides doctors with real-time
    diagnostic information through human-computer interaction technology. The mart
    microscope system (ARM), designed by Google Health, has made an early breakthrough
    in this field [[28](#bib.bib28)]. ARM integrates AI algorithms with optical microscope
    to analyze pathological slides and provide pathologists analysis results (e.g.,
    lesion contour, probability heatmap) in the field of view by augmented reality
    technologies. Then, pathologists make the final diagnosis based on these quantitative
    and qualitative results. This computer-assisted diagnostic system has provided
    prospects for the automation of histopathology, cytology, parasitology, etc. However,
    there is still a long way to go in terms of accurate and efficient diagnosis,
    system integration, hardware resources, and AI algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, the development of deep learning has enabled great success
    in computational cytology, showing signiﬁcant promise for efficient cancer screening.
    In this paper, we have comprehensively reviewed the current progress of deep learning-based
    methods in computational cytology, including supervised learning, weakly supervised
    learning, unsupervised learning, and transfer learning. More specifically, we
    survey image analysis-based approaches and state-of-the-art DL algorithms with
    the applications of classification, detection, and segmentation in cytology. Various
    applications of advanced DL-based works of various cytology were investigated
    in this paper, including the cervix, breast, lung, thyroid, oral, kidney, stomach,
    etc. We also summarize the evaluation metrics and public datasets for developing
    new models. Finally, we outline current challenges and potential directions for
    future research of computational cytology.
  prefs: []
  type: TYPE_NORMAL
- en: Declaration of Competing Interest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors declare that they have no known competing financial interests or
    personal relationships that could have appeared to influence the work reported
    in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: CRediT authorship contribution statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hao Jiang: Conceptualization, Methodology, Writing - original draft, Visualization.
    Yanning Zhou: Conceptualization, Formal analysis, Writing - review & editing.
    Yi Lin: Conceptualization, Writing - review & editing, Investigation. Ronald CK
    Chan: Formal analysis, Writing - review & editing. Jiang Liu: Conceptualization,
    Investigation. Hao Chen: Conceptualization, Funding acquisition, Project administration,
    Resources, Supervision, Writing - review & editing.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by Beijing Institute of Collaborative Innovation Program
    (No. BICI22EG01).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alberts et al. [2015] Alberts, B., Bray, D., Hopkin, K., Johnson, A.D., Lewis,
    J., Raff, M., Roberts, K., Walter, P., 2015. Essential cell biology. Garland Science.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alberts et al. [2003] Alberts, B., Johnson, A., Lewis, J., Raff, M., Roberts,
    K., Walter, P., et al., 2003. Molecular biology of the cell. Scandinavian Journal
    of Rheumatology 32, 125–125.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Albuquerque et al. [2021] Albuquerque, T., Cruz, R., Cardoso, J.S., 2021. Ordinal
    losses for classification of cervical cancer risk. PeerJ Computer Science 7, e457.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amorim et al. [2020] Amorim, J.G.A., Macarini, L.A.B., Matias, A.V., Cerentini,
    A., Onofre, F.B.D.M., Onofre, A.S.C., Von Wangenheim, A., 2020. A novel approach
    on segmentation of agnor-stained cytology images using deep learning, in: 2020
    IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS), IEEE.
    pp. 552–557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An et al. [2021] An, Y., Shen, H.W., Shan, G., Li, G., Liu, J., 2021. Stsrnet:
    Deep joint space–time super-resolution for vector field visualization. IEEE Computer
    Graphics and Applications 41, 122–132.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Araujo et al. [2019] Araujo, F.H., Silva, R.R., Ushizima, D.M., Rezende, M.T.,
    Carneiro, C.M., Bianchi, A.G.C., Medeiros, F.N., 2019. Deep learning for cell
    image segmentation and ranking. Computerized Medical Imaging and Graphics 72,
    13–21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Awan et al. [2021] Awan, R., Benes, K., Azam, A., Song, T.H., Shaban, M., Verrill,
    C., Tsang, Y.W., Snead, D., Minhas, F., Rajpoot, N., 2021. Deep learning based
    digital cell profiles for risk stratification of urine cytology images. Cytometry
    Part A .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bakht et al. [2020] Bakht, A.B., Javed, S., Dina, R., Almarzouqi, H., Khandoker,
    A., Werghi, N., 2020. Thyroid nodule cell classification in cytology images using
    transfer learning approach., in: International Conference on Soft Computing and
    Pattern Recognition, pp. 539–549.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bal et al. [2021] Bal, A., Das, M., Satapathy, S.M., Jena, M., Das, S.K., 2021.
    Bfcnet: a cnn for diagnosis of ductal carcinoma in breast from cytology images.
    Pattern Analysis and Applications , 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bao et al. [2020] Bao, H., Bi, H., Zhang, X., Zhao, Y., Dong, Y., Luo, X.,
    Zhou, D., You, Z., Wu, Y., Liu, Z., et al., 2020. Artificial intelligence-assisted
    cytology for detection of cervical intraepithelial neoplasia or invasive cancer:
    A multicenter, clinical-based, observational study. Gynecologic Oncology 159,
    171–178.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barkan et al. [2016] Barkan, G.A., Wojcik, E.M., Nayar, R., Savic-Prince, S.,
    Quek, M.L., Kurtycz, D.F., Rosenthal, D.L., 2016. The paris system for reporting
    urinary cytology: the quest to develop a standardized terminology. Acta Cytologica
    60, 185–197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baykal et al. [2020] Baykal, E., Dogan, H., Ercin, M.E., Ersoz, S., Ekinci,
    M., 2020. Modern convolutional object detectors for nuclei detection on pleural
    effusion cytology images. Multimedia Tools and Applications 79, 15417–15436.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beam et al. [2020] Beam, A.L., Manrai, A.K., Ghassemi, M., 2020. Challenges
    to the reproducibility of machine learning models in health care. Jama 323, 305–306.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beca and Schmitt [2019] Beca, F., Schmitt, F.C., 2019. Ancillary tests in breast
    cytology: a practical guide. Acta cytologica 63, 302–313.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhatt et al. [2021] Bhatt, A.R., Ganatra, A., Kotecha, K., 2021. Cervical cancer
    detection in pap smear whole slide images using convnet with transfer learning
    and progressive resizing. PeerJ Computer Science 7, e348.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Böhm et al. [2019] Böhm, A., Tatarchenko, M., Falk, T., 2019. Isoo v2 dl-semantic
    instance segmentation of touching and overlapping objects, in: 2019 IEEE 16th
    International Symposium on Biomedical Imaging (ISBI 2019), IEEE. pp. 343–347.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown and Garber [1999] Brown, A.D., Garber, A.M., 1999. Cost-effectiveness
    of 3 methods to enhance the sensitivity of papanicolaou testing. Jama 281, 347–353.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caddy et al. [2005] Caddy, G., Conron, M., Wright, G., Desmond, P., Hart, D.,
    Chen, R., 2005. The accuracy of eus-fna in assessing mediastinal lymphadenopathy
    and staging patients with nsclc. European Respiratory Journal 25, 410–415.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Çallı et al. [2021] Çallı, E., Sogancioglu, E., van Ginneken, B., van Leeuwen,
    K.G., Murphy, K., 2021. Deep learning for chest x-ray analysis: A survey. Medical
    Image Analysis , 102125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. [2021] Cao, L., Yang, J., Rong, Z., Li, L., Xia, B., You, C., Lou,
    G., Jiang, L., Du, C., Meng, H., et al., 2021. A novel attention-guided convolutional
    network for the detection of abnormal cervical cells in cervical cancer screening.
    Medical Image Analysis , 102197.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chai et al. [2021] Chai, Z., Luo, L., Lin, H., Chen, H., Heng, P.A., 2021. Deep
    semi-supervised metric learning with dual alignment for cervical cancer cell detection.
    arXiv preprint arXiv:2104.03265 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chankong et al. [2014] Chankong, T., Theera-Umpon, N., Auephanwiriyakul, S.,
    2014. Automatic cervical cell segmentation and classification in pap smears. Computer
    Methods and Programs in Biomedicine 113, 539–556.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2020] Chen, C., Dou, Q., Chen, H., Qin, J., Heng, P.A., 2020. Unsupervised
    bidirectional cross-modality adaptation via deeply synergistic image and feature
    alignment for medical image segmentation. IEEE Transactions on Medical Imaging
    39, 2494–2505.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2016a] Chen, H., Dou, Q., Wang, X., Qin, J., Heng, P.A., 2016a.
    Mitosis detection in breast cancer histology images via deep cascaded networks,
    in: Thirtieth AAAI conference on artificial intelligence, pp. 1160–1166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2017] Chen, H., Qi, X., Yu, L., Dou, Q., Qin, J., Heng, P.A.,
    2017. Dcan: Deep contour-aware networks for object instance segmentation from
    histology images. Medical Image Analysis 36, 135–146.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2016b] Chen, H., Qi, X., Yu, L., Heng, P.A., 2016b. Dcan: deep
    contour-aware networks for accurate gland segmentation, in: Proceedings of the
    IEEE conference on Computer Vision and Pattern Recognition, pp. 2487–2496.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2021] Chen, M., Zhang, Z., Wang, T., Backes, M., Humbert, M.,
    Zhang, Y., 2021. When machine unlearning jeopardizes privacy, in: Proceedings
    of the 2021 ACM SIGSAC Conference on Computer and Communications Security, pp.
    896–911.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2019] Chen, P.H.C., Gadepalli, K., MacDonald, R., Liu, Y., Kadowaki,
    S., Nagpal, K., Kohlberger, T., Dean, J., Corrado, G.S., Hipp, J.D., et al., 2019.
    An augmented reality microscope with real-time artificial intelligence integration
    for cancer diagnosis. Nature Medicine 25, 1453–1457.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. [2021] Cheng, S., Liu, S., Yu, J., Rao, G., Xiao, Y., Han, W.,
    Zhu, W., Lv, X., Li, N., Cai, J., et al., 2021. Robust whole slide image analysis
    for cervical cancer screening using deep learning. Nature Communications 12, 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cibas and Ali [2017] Cibas, E.S., Ali, S.Z., 2017. The 2017 bethesda system
    for reporting thyroid cytopathology. Thyroid 27, 1341–1346.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Davey et al. [2006] Davey, E., Barratt, A., Irwig, L., Chan, S.F., Macaskill,
    P., Mannes, P., Saville, A.M., 2006. Effect of study design and quality on unsatisfactory
    rates, cytology classifications, and accuracy in liquid-based versus conventional
    cervical cytology: a systematic review. The Lancet 367, 122–132.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De Vito et al. [2014] De Vito, C., Angeloni, C., De Feo, E., Marzuillo, C.,
    Lattanzi, A., Ricciardi, W., Villari, P., Boccia, S., 2014. A large cross-sectional
    survey investigating the knowledge of cervical cancer risk aetiology and the predictors
    of the adherence to cervical cancer screening related to mass media campaign.
    BioMed Research International 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. [2009] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei,
    L., 2009. Imagenet: A large-scale hierarchical image database, in: 2009 IEEE conference
    on computer vision and pattern recognition, Ieee. pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dey [2018] Dey, P., 2018. Basic and advanced laboratory techniques in histopathology
    and cytology. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dey et al. [2019] Dey, S., Das, S., Ghosh, S., Mitra, S., Chakrabarty, S.,
    Das, N., 2019. Syncgan: Using learnable class specific priors to generate synthetic
    data for improving classifier performance on cytological images, in: National
    Conference on Computer Vision, Pattern Recognition, Image Processing, and Graphics,
    Springer. pp. 32–42.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimauro et al. [2019] Dimauro, G., Ciprandi, G., Deperte, F., Girardi, F., Ladisa,
    E., Latrofa, S., Gelardi, M., 2019. Nasal cytology with deep learning techniques.
    International journal of medical informatics 122, 13–19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. [2020] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., et al., 2020. An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dov et al. [2021] Dov, D., Kovalsky, S.Z., Assaad, S., Cohen, J., Range, D.E.,
    Pendse, A.A., Henao, R., Carin, L., 2021. Weakly supervised instance learning
    for thyroid malignancy prediction from whole slide cytopathology images. Medical
    Image Analysis 67, 101814.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dov et al. [2019] Dov, D., Kovalsky, S.Z., Cohen, J., Range, D.E., Henao, R.,
    Carin, L., 2019. Thyroid cancer malignancy prediction from whole slide cytopathology
    images, in: Machine Learning for Healthcare Conference, PMLR. pp. 553–570.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elliott Range et al. [2020] Elliott Range, D.D., Dov, D., Kovalsky, S.Z., Henao,
    R., Carin, L., Cohen, J., 2020. Application of a machine learning algorithm to
    predict malignancy in thyroid cytopathology. Cancer cytopathology 128, 287–295.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Falk et al. [2019] Falk, T., Mai, D., Bensch, R., Çiçek, Ö., Abdulkadir, A.,
    Marrakchi, Y., Böhm, A., Deubner, J., Jäckel, Z., Seiwald, K., et al., 2019. U-net:
    deep learning for cell counting, detection, and morphometry. Nature Methods 16,
    67–70.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Field et al. [2019] Field, A.S., Raymond, W.A., Rickard, M., Arnold, L., Brachtel,
    E.F., Chaiwun, B., Chen, L., Di Bonito, L., Kurtycz, D.F., Lee, A.H., et al.,
    2019. The international academy of cytology yokohama system for reporting breast
    fine-needle aspiration biopsy cytopathology. Acta Cytologica 63, 257–273.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garud et al. [2017] Garud, H., Karri, S.P.K., Sheet, D., Chatterjee, J., Mahadevappa,
    M., Ray, A.K., Ghosh, A., Maity, A.K., 2017. High-magnification multi-views based
    classification of breast fine needle aspiration cytology cell samples using fusion
    of decisions from deep convolutional networks, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition workshops, pp. 76–81.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Girshick [2015] Girshick, R., 2015. Fast r-cnn, in: Proceedings of the IEEE
    international conference on computer vision, pp. 1440–1448.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Girshick et al. [2014] Girshick, R., Donahue, J., Darrell, T., Malik, J., 2014.
    Rich feature hierarchies for accurate object detection and semantic segmentation,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 580–587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gonzalez et al. [2020] Gonzalez, D., Dietz, R.L., Pantanowitz, L., 2020. Feasibility
    of a deep learning algorithm to distinguish large cell neuroendocrine from small
    cell lung carcinoma in cytology specimens. Cytopathology 31, 426–431.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2014] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014. Generative adversarial
    nets. Advances in Neural Information Processing Systems 27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guan and Liu [2021] Guan, H., Liu, M., 2021. Domain adaptation for medical
    image analysis: a survey. arXiv preprint arXiv:2102.09508 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guan et al. [2019a] Guan, Q., Wan, X., Lu, H., Ping, B., Li, D., Wang, L.,
    Zhu, Y., Wang, Y., Xiang, J., 2019a. Deep convolutional neural network inception-v3
    model for differential diagnosing of lymph node in cytological images: a pilot
    study. Annals of Translational Medicine 7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guan et al. [2019b] Guan, Q., Wang, Y., Ping, B., Li, D., Du, J., Qin, Y.,
    Lu, H., Wan, X., Xiang, J., 2019b. Deep convolutional neural network vgg-16 model
    for differential diagnosing of papillary thyroid carcinomas in cytological images:
    a pilot study. Journal of Cancer 10, 4876.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2017] He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask
    r-cnn, in: Proceedings of the IEEE international conference on computer vision,
    pp. 2961–2969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2016] He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning
    for image recognition, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hossain et al. [2019] Hossain, M.S., Jalab, H.A., Zulfiqar, F., Pervin, M.,
    2019. Renal cancer cell nuclei detection from cytological images using convolutional
    neural network for estimating proliferation rate. Journal of Telecommunication,
    Electronic and Computer Engineering (JTEC) 11, 63–71.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2017] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.,
    2017. Densely connected convolutional networks, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 4700–4708.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hussain et al. [2020a] Hussain, E., Mahanta, L.B., Borah, H., Das, C.R., 2020a.
    Liquid based-cytology pap smear dataset for automated multi-class diagnosis of
    pre-cancerous and cervical cancer lesions. Data in brief 30, 105589.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hussain et al. [2020b] Hussain, E., Mahanta, L.B., Das, C.R., Choudhury, M.,
    Chowdhury, M., 2020b. A shape context fully convolutional neural network for segmentation
    and classification of cervical nuclei in pap smear images. Artificial Intelligence
    in Medicine 107, 101897.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hussain et al. [2020c] Hussain, E., Mahanta, L.B., Das, C.R., Talukdar, R.K.,
    2020c. A comprehensive study on the multi-class cervical cancer diagnostic prediction
    on pap smear images using a fusion-based decision from ensemble deep convolutional
    neural network. Tissue and Cell 65, 101347.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isa [2005] Isa, N.M., 2005. Automated edge detection technique for pap smear
    images using moving k-means clustering and modified seed based region growing
    algorithm. International Journal of The Computer, the Internet and Management
    13, 45–59.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivanovic [2014] Ivanovic, M., 2014. Overview of cytopathology procedures and
    techniques. Cytopathology in Oncology , 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jantzen et al. [2005] Jantzen, J., Norup, J., Dounias, G., Bjerregaard, B.,
    2005. Pap-smear benchmark data for pattern classification. Nature inspired Smart
    Information Systems (NiSIS 2005) , 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. [2020] Ji, A.L., Rubin, A.J., Thrane, K., Jiang, S., Reynolds, D.L.,
    Meyers, R.M., Guo, M.G., George, B.M., Mollbrink, A., Bergenstråhle, J., et al.,
    2020. Multimodal analysis of composition and spatial architecture in human squamous
    cell carcinoma. Cell 182, 497–514.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnston [1952] Johnston, D., 1952. Cytoplasmic: nuclear ratios in the cytological
    diagnosis of cancer. Cancer 5, 945–949.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jónsson et al. [2019] Jónsson, B.A., Bjornsdottir, G., Thorgeirsson, T., Ellingsen,
    L.M., Walters, G.B., Gudbjartsson, D., Stefansson, H., Stefansson, K., Ulfarsson,
    M., 2019. Brain age prediction using deep learning uncovers associated sequence
    variants. Nature Communications 10, 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaneko et al. [2021] Kaneko, M., Tsuji, K., Masuda, K., Ueno, K., Henmi, K.,
    Nakagawa, S., Fujita, R., Suzuki, K., Inoue, Y., Teramukai, S., et al., 2021.
    Urine cell image recognition using a deep learning model for an automated slide
    evaluation system. BJU international .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ke et al. [2021] Ke, J., Shen, Y., Lu, Y., Deng, J., Wright, J.D., Zhang, Y.,
    Huang, Q., Wang, D., Jing, N., Liang, X., et al., 2021. Quantitative analysis
    of abnormalities in gynecologic cytopathology with deep learning. Laboratory Investigation
    101, 513–524.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kilic et al. [2019] Kilic, B., Baykal, E., Ekinci, M., Dogan, H., Ercin, M.E.,
    Ersoz, S., 2019. Automated nuclei detection on pleural effusion cytopathology
    images using yolov3, in: 2019 4th International Conference on Computer Science
    and Engineering (UBMK), IEEE. pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling [2013] Kingma, D.P., Welling, M., 2013. Auto-encoding variational
    bayes. arXiv preprint arXiv:1312.6114 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kitchener et al. [2006] Kitchener, H.C., Castle, P.E., Cox, J.T., 2006. Achievements
    and limitations of cervical cytology screening. Vaccine 24, S63–S70.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kontzoglou et al. [2005] Kontzoglou, K., Moulakakis, K.G., Konofaos, P., Kyriazi,
    M., Kyroudes, A., Karakitsos, P., 2005. The role of liquid-based cytology in the
    investigation of breast lesions using fine-needle aspiration: a cytohistopathological
    evaluation. Journal of Surgical Oncology 89, 75–78.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Koohbanani et al. [2020] Koohbanani, N.A., Jahanifar, M., Tajadin, N.Z., Rajpoot,
    N., 2020. Nuclick: a deep learning framework for interactive segmentation of microscopic
    images. Medical Image Analysis 65, 101771.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koss et al. [1994] Koss, L.G., Lin, E., Schreiber, K., Elgert, P., Mango, L.,
    1994. Evaluation of the papnet™ cytologic screening system for quality control
    of cervical smears. American Journal of Clinical Pathology 101, 220–229.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koss and Melamed [2006] Koss, L.G., Melamed, M.R., 2006. Koss’ diagnostic cytology
    and its histopathologic bases. volume 1. Lippincott Williams & Wilkins.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kowal et al. [2020] Kowal, M., Żejmo, M., Skobel, M., Korbicz, J., Monczak,
    R., 2020. Cell nuclei segmentation in cytological images using convolutional neural
    network and seeded watershed algorithm. Journal of Digital Imaging 33, 231–242.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Landau and Pantanowitz [2019] Landau, M.S., Pantanowitz, L., 2019. Artificial
    intelligence in cytopathology: a review of the literature and overview of commercial
    landscape. Journal of the American Society of Cytopathology 8, 230–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Larsen et al. [2016] Larsen, A.B.L., Sønderby, S.K., Larochelle, H., Winther,
    O., 2016. Autoencoding beyond pixels using a learned similarity metric, in: International
    conference on machine learning, PMLR. pp. 1558–1566.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lassau et al. [2021] Lassau, N., Ammari, S., Chouzenoux, E., Gortais, H., Herent,
    P., Devilder, M., Soliman, S., Meyrignac, O., Talabard, M.P., Lamarque, J.P.,
    et al., 2021. Integrating deep learning ct-scan model, biological and clinical
    variables to predict severity of covid-19 patients. Nature Communications 12,
    1–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. [1998] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., 1998. Gradient-based
    learning applied to document recognition. Proceedings of the IEEE 86, 2278–2324.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lever et al. [1985] Lever, J., Trott, P., Webb, A., 1985. Fine needle aspiration
    cytology. Journal of Clinical Pathology 38, 1–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2021] Li, J., Chen, W., Huang, X., Hu, Z., Duan, Q., Li, H., Metaxas,
    D.N., Zhang, S., 2021. Mixed supervision learning for whole slide image classification.
    arXiv preprint arXiv:2107.00934 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2022] Li, J., Dou, Q., Yang, H., Liu, J., Fu, L., Zhang, Y., Zheng,
    L., Zhang, D., 2022. Cervical cell multi-classification algorithm using global
    context information and attention mechanism. Tissue and Cell 74, 101677.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2019] Li, X., Li, Q., et al., 2019. Detection and classification
    of cervical exfoliated cells based on faster r-cnn, in: 2019 IEEE 11th international
    conference on advanced infocomm technology (ICAIT), IEEE. pp. 52–57.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. [2021a] Liang, Y., Pan, C., Sun, W., Liu, Q., Du, Y., 2021a. Global
    context-aware cervical cell detection with soft scale anchor matching. Computer
    Methods and Programs in Biomedicine 204, 106061.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. [2021b] Liang, Y., Tang, Z., Yan, M., Chen, J., Liu, Q., Xiang,
    Y., 2021b. Comparison detector for cervical cell/clumps detection in the limited
    data scenario. Neurocomputing 437, 195–205.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lilli et al. [2021] Lilli, L., Giarnieri, E., Scardapane, S., 2021. A calibrated
    multiexit neural network for detecting urothelial cancer cells. Computational
    and Mathematical Methods in Medicine 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2019a] Lin, H., Chen, H., Graham, S., Dou, Q., Rajpoot, N., Heng,
    P.A., 2019a. Fast scannet: Fast and dense analysis of multi-gigapixel whole-slide
    images for cancer metastasis detection. IEEE Transactions on Medical Imaging 38,
    1948–1958.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. [2021] Lin, H., Chen, H., Wang, X., Wang, Q., Wang, L., Heng, P.A.,
    2021. Dual-path network with synergistic grouping loss and evidence driven risk
    stratification for whole slide cervical image analysis. Medical Image Analysis
    69, 101955.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. [2019b] Lin, H., Hu, Y., Chen, S., Yao, J., Zhang, L., 2019b. Fine-grained
    classification of cervical cells using morphological and appearance based convolutional
    neural networks. IEEE Access 7, 71541–71549.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2017a] Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan,
    B., Belongie, S., 2017a. Feature pyramid networks for object detection, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2017b] Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P.,
    2017b. Focal loss for dense object detection, in: Proceedings of the IEEE international
    conference on computer vision, pp. 2980–2988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Litjens et al. [2017] Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A.,
    Ciompi, F., Ghafoorian, M., Van Der Laak, J.A., Van Ginneken, B., Sánchez, C.I.,
    2017. A survey on deep learning in medical image analysis. Medical Image Analysis
    42, 60–88.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2020] Liu, L., Wang, Y., Ma, Q., Tan, L., Wu, Y., Xiao, J., 2020.
    Artificial classification of cervical squamous lesions in thinprep cytologic tests
    using a deep convolutional neural network. Oncology Letters 20, 1–1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2016] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.,
    Fu, C.Y., Berg, A.C., 2016. Ssd: Single shot multibox detector, in: European conference
    on computer vision, Springer. pp. 21–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Long et al. [2015] Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional
    networks for semantic segmentation, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp. 3431–3440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. [2021] Lu, M.Y., Williamson, D.F., Chen, T.Y., Chen, R.J., Barbieri,
    M., Mahmood, F., 2021. Data-efficient and weakly supervised computational pathology
    on whole-slide images. Nature Biomedical Engineering 5, 555–570.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. [2015] Lu, Z., Carneiro, G., Bradley, A.P., 2015. An improved joint
    optimization of multiple level set functions for the segmentation of overlapping
    cervical cells. IEEE Transactions on Image Processing 24, 1261–1272.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. [2016] Lu, Z., Carneiro, G., Bradley, A.P., Ushizima, D., Nosrati,
    M.S., Bianchi, A.G., Carneiro, C.M., Hamarneh, G., 2016. Evaluation of three algorithms
    for the segmentation of overlapping cervical cells. IEEE Journal of Biomedical
    and Health Informatics 21, 441–450.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. [2021] Luo, L., Chen, H., Zhou, Y., Lin, H., Pheng, P.A., 2021.
    Oxnet: Omni-supervised thoracic disease detection from chest x-rays. arXiv preprint
    arXiv:2104.03218 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. [2021] Ma, J., Liu, S., Cheng, S., Chen, R., Liu, X., Chen, L., Zeng,
    S., 2021. Stsrnet: Self-texture transfer super-resolution and refocusing network.
    IEEE Transactions on Medical Imaging .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. [2020] Ma, J., Yu, J., Liu, S., Chen, L., Li, X., Feng, J., Chen,
    Z., Zeng, S., Liu, X., Cheng, S., 2020. Pathsrgan: Multi-supervised super-resolution
    for cytopathological images using generative adversarial network. IEEE Transactions
    on Medical Imaging 39, 2920–2930.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maharjan et al. [2017] Maharjan, S., Ranabhat, S., Tiwari, M., Bhandari, A.,
    Osti, B.P., Neopane, P., 2017. Exfoliative cytology analysis from different sites
    of the body. Journal of Chitwan Medical College 7, 33–39.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahmood et al. [2019] Mahmood, F., Borders, D., Chen, R.J., McKay, G.N., Salimian,
    K.J., Baras, A., Durr, N.J., 2019. Deep adversarial training for multi-organ nuclei
    segmentation in histopathology images. IEEE Transactions on Medical Imaging 39,
    3257–3267.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manna et al. [2021] Manna, A., Kundu, R., Kaplun, D., Sinitca, A., Sarkar, R.,
    2021. A fuzzy rank-based ensemble of cnn models for classification of cervical
    cytology. Scientific Reports 11, 1–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maron and Lozano-Pérez [1998] Maron, O., Lozano-Pérez, T., 1998. A framework
    for multiple-instance learning. Advances in Neural Information Processing Systems
    , 570–576.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marzahl et al. [2020] Marzahl, C., Aubreville, M., Bertram, C.A., Stayt, J.,
    Jasensky, A.K., Bartenschlager, F., Fragoso-Garcia, M., Barton, A.K., Elsemann,
    S., Jabari, S., et al., 2020. Deep learning-based quantification of pulmonary
    hemosiderophages in cytology slides. Scientific Reports 10, 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matias et al. [2021] Matias, A.V., Cerentini, A., Macarini, L.A.B., Amorim,
    J.G.A., Daltoé, F.P., von Wangenheim, A., 2021. Segmentation, detection, and classification
    of cell nuclei on oral cytology samples stained with papanicolaou. SN Computer
    Science 2, 1–15.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mehrotra et al. [2011] Mehrotra, R., Mishra, S., Singh, M., Singh, M., 2011.
    The efficacy of oral brush biopsy with computer-assisted analysis in identifying
    precancerous and cancerous lesions. Head & Neck Oncology 3, 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Milletari et al. [2016] Milletari, F., Navab, N., Ahmadi, S.A., 2016. V-net:
    Fully convolutional neural networks for volumetric medical image segmentation,
    in: 2016 fourth international conference on 3D vision (3DV), IEEE. pp. 565–571.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miselis et al. [2019] Miselis, B., Fevens, T., Krzyżak, A., Kowal, M., Monczak,
    R., 2019. Deep neural networks for breast cancer diagnosis: fine needle biopsy
    scenario, in: Polish Conference on Biocybernetics and Biomedical Engineering,
    Springer. pp. 131–142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mitra et al. [2021] Mitra, S., Das, N., Dey, S., Chakraborty, S., Nasipuri,
    M., Naskar, M.K., 2021. Cytology image analysis techniques toward automation:
    Systematically revisited. ACM Computing Surveys (CSUR) 54, 1–41.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohammed et al. [2021] Mohammed, M.A., Abdurahman, F., Ayalew, Y.A., 2021. Single-cell
    conventional pap smear image classification using pre-trained deep neural network
    architectures. BMC Biomedical Engineering 3, 11–11.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moosavi Tayebi et al. [2021] Moosavi Tayebi, R., Mu, Y., Dehkharghanian, T.,
    Ross, C., Sur, M., Foley, R., Tizhoosh, H.R., Campbell, C.J., 2021. Histogram
    of cell types: Deep learning for automated bone marrow cytology. arXiv e-prints
    , arXiv–2107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Morrison and DeNicola [1993] Morrison, W., DeNicola, D., 1993. Advantages and
    disadvantages of cytology and histopathology for the diagnosis of cancer., in:
    Seminars in veterinary medicine and surgery (small animal), pp. 222–227.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nayar and Wilbur [2015] Nayar, R., Wilbur, D.C., 2015. The Bethesda system
    for reporting cervical cytology: definitions, criteria, and explanatory notes.
    Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Neubeck and Van Gool [2006] Neubeck, A., Van Gool, L., 2006. Efficient non-maximum
    suppression, in: 18th International Conference on Pattern Recognition (ICPR’06),
    IEEE. pp. 850–855.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nojima et al. [2021] Nojima, S., Terayama, K., Shimoura, S., Hijiki, S., Nonomura,
    N., Morii, E., Okuno, Y., Fujita, K., 2021. A deep learning system to diagnose
    the malignant potential of urothelial carcinoma cells in cytology specimens. Cancer
    Cytopathology .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Noyan et al. [2020] Noyan, M.A., Durdu, M., Eskiocak, A.H., 2020. Tzancknet:
    a convolutional neural network to identify cells in the cytology of erosive-vesiculobullous
    diseases. Scientific Reports 10, 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oza et al. [2021] Oza, P., Sindagi, V.A., VS, V., Patel, V.M., 2021. Unsupervised
    domain adaption of object detectors: A survey. arXiv preprint arXiv:2105.13502
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: O’Flynn et al. [2021] O’Flynn, H., Ryan, N.A., Narine, N., Shelton, D., Rana,
    D., Crosbie, E.J., 2021. Diagnostic accuracy of cytology for the detection of
    endometrial cancer in urine and vaginal samples. Nature Communications 12, 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patel et al. [2019] Patel, B.N., Rosenberg, L., Willcox, G., Baltaxe, D., Lyons,
    M., Irvin, J., Rajpurkar, P., Amrhein, T., Gupta, R., Halabi, S., et al., 2019.
    Human–machine partnership with artificial intelligence for chest radiograph diagnosis.
    NPJ Digital Medicine 2, 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phoulady and Mouton [2018] Phoulady, H.A., Mouton, P.R., 2018. A new cervical
    cytology dataset for nucleus detection and image classification (cervix93) and
    methods for cervical nucleus detection. arXiv preprint arXiv:1811.09651 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pirovano et al. [2021] Pirovano, A., Almeida, L.G., Ladjal, S., Bloch, I., Berlemont,
    S., 2021. Computer-aided diagnosis tool for cervical cancer screening with weakly
    supervised localization and detection of abnormalities using adaptable and explainable
    classifier. Medical Image Analysis , 102167.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plissiti et al. [2009] Plissiti, M., Tripoliti, E., Charchanti, A., Krikoni,
    O., Fotiadis, D., 2009. Automated detection of cell nuclei in pap stained cervical
    smear images using fuzzy clustering, in: 4th European Conference of the International
    Federation for Medical and Biological Engineering, Springer. pp. 637–641.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plissiti et al. [2018] Plissiti, M.E., Dimitrakopoulos, P., Sfikas, G., Nikou,
    C., Krikoni, O., Charchanti, A., 2018. Sipakmed: A new dataset for feature and
    image based classification of normal and pathological cervical cells in pap smear
    images, in: 2018 25th IEEE International Conference on Image Processing (ICIP),
    IEEE. pp. 3144–3148.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahaman et al. [2020] Rahaman, M.M., Li, C., Wu, X., Yao, Y., Hu, Z., Jiang,
    T., Li, X., Qi, S., 2020. A survey for cervical cytopathology image analysis using
    deep learning. IEEE Access 8, 61687–61710.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rahaman et al. [2021] Rahaman, M.M., Li, C., Yao, Y., Kulwa, F., Wu, X., Li,
    X., Wang, Q., 2021. Deepcervix: A deep learning-based framework for the classification
    of cervical cells using hybrid deep feature fusion techniques. arXiv preprint
    arXiv:2102.12191 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kour Raina et al. [2021] kour Raina, M., Gupta, N., Kumar, S., Kusum, A., 2021.
    Evaluation of cell block technique versus conventional cytology on bronchoscopy
    guided needle aspiration/brush cytology for diagnosis of lung cancer. International
    Journal of Scientific Research 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon et al. [2016] Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016.
    You only look once: Unified, real-time object detection, in: Proceedings of the
    IEEE conference on computer vision and pattern recognition, pp. 779–788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. [2015] Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn:
    Towards real-time object detection with region proposal networks. Advances in
    Neural Information Processing Systems 28, 91–99.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rezatofighi et al. [2019] Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A.,
    Reid, I., Savarese, S., 2019. Generalized intersection over union: A metric and
    a loss for bounding box regression, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 658–666.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rezende et al. [2021] Rezende, M.T., Silva, R., Bernardo, F.d.O., Tobias, A.H.,
    Oliveira, P.H., Machado, T.M., Costa, C.S., Medeiros, F.N., Ushizima, D.M., Carneiro,
    C.M., et al., 2021. Cric searchable image database as a public platform for conventional
    pap smear cytology data. Scientific Data 8, 1–8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ribeiro et al. [2016] Ribeiro, M.T., Singh, S., Guestrin, C., 2016. ” why should
    i trust you?” explaining the predictions of any classifier, in: Proceedings of
    the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,
    pp. 1135–1144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rieke et al. [2020] Rieke, N., Hancox, J., Li, W., Milletari, F., Roth, H.R.,
    Albarqouni, S., Bakas, S., Galtier, M.N., Landman, B.A., Maier-Hein, K., et al.,
    2020. The future of digital health with federated learning. NPJ Digital Medicine
    3, 1–7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. [2015] Ronneberger, O., Fischer, P., Brox, T., 2015. U-net:
    Convolutional networks for biomedical image segmentation, in: International Conference
    on Medical image computing and computer-assisted intervention, Springer. pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosenblatt [1961] Rosenblatt, F., 1961. Principles of neurodynamics. perceptrons
    and the theory of brain mechanisms. Technical Report. Cornell Aeronautical Lab
    Inc Buffalo NY.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosenthal et al. [2016] Rosenthal, D.L., Wojcik, E.M., Kurtycz, D.F., 2016.
    The Paris system for reporting urinary cytology. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosenzweig and Bartl [2015] Rosenzweig, J., Bartl, M., 2015. A review and analysis
    of literature on autonomous driving. E-Journal Making-of Innovation , 1–57.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. [1986] Rumelhart, D.E., Hinton, G.E., Williams, R.J., 1986.
    Learning representations by back-propagating errors. Nature 323, 533–536.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Russell et al. [2008] Russell, B.C., Torralba, A., Murphy, K.P., Freeman, W.T.,
    2008. Labelme: a database and web-based tool for image annotation. International
    Journal of Computer Vision 77, 157–173.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saikia et al. [2019] Saikia, A.R., Bora, K., Mahanta, L.B., Das, A.K., 2019.
    Comparative assessment of cnn architectures for classification of breast fnac
    images. Tissue and Cell 57, 8–14.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Samuel et al. [2020] Samuel, S., Löffler, F., König-Ries, B., 2020. Machine
    learning pipelines: provenance, reproducibility and fair data principles. arXiv
    preprint arXiv:2006.12117 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Van de Sande et al. [2011] Van de Sande, K.E., Uijlings, J.R., Gevers, T.,
    Smeulders, A.W., 2011. Segmentation as selective search for object recognition,
    in: 2011 international conference on computer vision, IEEE. pp. 1879–1886.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanyal et al. [2018] Sanyal, P., Mukherjee, T., Barui, S., Das, A., Gangopadhyay,
    P., 2018. Artificial intelligence in cytopathology: a neural network to identify
    papillary carcinoma on thyroid fine-needle aspiration cytology smears. Journal
    of pathology informatics 9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selvaraju et al. [2017] Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R.,
    Parikh, D., Batra, D., 2017. Grad-cam: Visual explanations from deep networks
    via gradient-based localization, in: Proceedings of the IEEE international conference
    on computer vision, pp. 618–626.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shanthi et al. [2019] Shanthi, P., Faruqi, F., Hareesha, K., Kudva, R., 2019.
    Deep convolution neural network for malignancy detection and classification in
    microscopic uterine cervix cell images. Asian Pacific Journal of Cancer Prevention:
    APJCP 20, 3447.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shao et al. [2019] Shao, W., Han, Z., Cheng, J., Cheng, L., Wang, T., Sun, L.,
    Lu, Z., Zhang, J., Zhang, D., Huang, K., 2019. Integrative analysis of pathological
    images and multi-dimensional genomic data for early-stage cancer prognosis. IEEE
    Transactions on Medical Imaging 39, 99–110.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. [2021] Shi, J., Wang, R., Zheng, Y., Jiang, Z., Zhang, H., Yu, L.,
    2021. Cervical cell classification with graph convolutional network. Computer
    Methods and Programs in Biomedicine 198, 105807.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Skaarland [1986] Skaarland, E., 1986. New concept in diagnostic endometrial
    cytology: diagnostic criteria based on composition and architecture of large tissue
    fragments in smears. Journal of Clinical Pathology 39, 36–43.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sompawong et al. [2019] Sompawong, N., Mopan, J., Pooprasert, P., Himakhun,
    W., Suwannarurk, K., Ngamvirojcharoen, J., Vachiramon, T., Tantibundhit, C., 2019.
    Automated pap smear cervical cancer screening using deep learning, in: 2019 41st
    Annual International Conference of the IEEE Engineering in Medicine and Biology
    Society (EMBC), IEEE. pp. 7044–7048.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2016] Song, Y., Tan, E.L., Jiang, X., Cheng, J.Z., Ni, D., Chen,
    S., Lei, B., Wang, T., 2016. Accurate cervical cell segmentation from overlapping
    clumps in pap smear images. IEEE transactions on medical imaging 36, 288–300.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2015] Song, Y., Zhang, L., Chen, S., Ni, D., Lei, B., Wang, T.,
    2015. Accurate segmentation of cervical cytoplasm and nuclei based on multiscale
    convolutional network and graph partitioning. IEEE Transactions on Biomedical
    Engineering 62, 2421–2433.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. [2014] Song, Y., Zhang, L., Chen, S., Ni, D., Li, B., Zhou, Y.,
    Lei, B., Wang, T., 2014. A deep learning based framework for accurate segmentation
    of cervical cytoplasm and nuclei, in: 2014 36th Annual International Conference
    of the IEEE Engineering in Medicine and Biology Society, IEEE. pp. 2903–2906.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. [2020] Song, Y., Zhu, L., Lei, B., Sheng, B., Dou, Q., Qin, J.,
    Choi, K.S., 2020. Shape mask generator: Learning to refine shape priors for segmenting
    overlapping cervical cytoplasms, in: International Conference on Medical Image
    Computing and Computer-Assisted Intervention, Springer. pp. 639–649.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sornapudi et al. [2019] Sornapudi, S., Brown, G.T., Xue, Z., Long, R., Allen,
    L., Antani, S., 2019. Comparing deep learning models for multi-cell classification
    in liquid-based cervical cytology image, in: AMIA Annual Symposium Proceedings,
    American Medical Informatics Association. p. 820.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srinidhi et al. [2020] Srinidhi, C.L., Ciga, O., Martel, A.L., 2020. Deep neural
    network models for computational histopathology: A survey. Medical Image Analysis
    , 101813.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. [2020] Su, F., Sun, Y., Hu, Y., Yuan, P., Wang, X., Wang, Q., Li,
    J., Ji, J.F., 2020. Development and validation of a deep learning system for ascites
    cytopathology interpretation. Gastric Cancer 23, 1041–1050.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sundararajan et al. [2017] Sundararajan, M., Taly, A., Yan, Q., 2017. Axiomatic
    attribution for deep networks, in: International Conference on Machine Learning,
    PMLR. pp. 3319–3328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Szegedy et al. [2015] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
    Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper with
    convolutions, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. [2021] Tan, X., Li, K., Zhang, J., Wang, W., Wu, B., Wu, J., Li,
    X., Huang, X., 2021. Automatic model for cervical cancer screening based on convolutional
    neural network: a retrospective, multicohort, multicenter study. Cancer Cell International
    21, 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tareef et al. [2017] Tareef, A., Song, Y., Huang, H., Wang, Y., Feng, D., Chen,
    M., Cai, W., 2017. Optimizing the cervix cytological examination based on deep
    learning and dynamic shape modeling. Neurocomputing 248, 28–40.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tay et al. [2020] Tay, Y., Dehghani, M., Bahri, D., Metzler, D., 2020. Efficient
    transformers: A survey. arXiv preprint arXiv:2009.06732 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teramoto et al. [2017] Teramoto, A., Tsukamoto, T., Kiriyama, Y., Fujita, H.,
    2017. Automated classification of lung cancer types from cytological images using
    deep convolutional neural networks. BioMed research international 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Teramoto et al. [2020] Teramoto, A., Tsukamoto, T., Yamada, A., Kiriyama, Y.,
    Imaizumi, K., Saito, K., Fujita, H., 2020. Deep learning approach to classification
    of lung cytological images: Two-step training using actual and synthesized images
    by progressive growing of generative adversarial networks. PloS one 15, e0229951.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teramoto et al. [2019] Teramoto, A., Yamada, A., Kiriyama, Y., Tsukamoto, T.,
    Yan, K., Zhang, L., Imaizumi, K., Saito, K., Fujita, H., 2019. Automated classification
    of benign and malignant cells from lung cytological images using deep convolutional
    neural network. Informatics in Medicine Unlocked 16, 100205.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teramoto et al. [2021] Teramoto, A., Yamada, A., Tsukamoto, T., Kiriyama, Y.,
    Sakurai, E., Shiogama, K., Michiba, A., Imaizumi, K., Saito, K., Fujita, H., 2021.
    Mutual stain conversion between giemsa and papanicolaou in cytological images
    using cycle generative adversarial network. Heliyon 7, e06331.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. [2019] Tian, Z., Shen, C., Chen, H., He, T., 2019. Fcos: Fully
    convolutional one-stage object detection, in: Proceedings of the IEEE/CVF international
    conference on computer vision, pp. 9627–9636.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tolles [1955] Tolles, W.E., 1955. Section of biology: The cytoanalyzer—an example
    of physics in medical research. Transactions of the New York Academy of Sciences
    17, 250–256.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tolles and Bostrom [1956] Tolles, W.E., Bostrom, R., 1956. Automatic screening
    of cytological smears for cancer: the instrumentation. Annals of the New York
    Academy of Sciences 63, 1211–1218.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tomczak et al. [2015] Tomczak, K., Czerwińska, P., Wiznerowicz, M., 2015. The
    cancer genome atlas (tcga): an immeasurable source of knowledge. Contemporary
    Oncology 19, A68.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaickus et al. [2019] Vaickus, L.J., Suriawinata, A.A., Wei, J.W., Liu, X.,
    2019. Automating the paris system for urine cytopathology—a hybrid deep-learning
    and morphometric approach. Cancer Cytopathology 127, 98–115.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vaswani et al. [2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017. Attention is all you
    need, in: Advances in neural information processing systems, pp. 5998–6008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Walter et al. [2021] Walter, F.C., Damrich, S., Hamprecht, F.A., 2021. Multistar:
    Instance segmentation of overlapping objects with star-convex polygons, in: 2021
    IEEE 18th International Symposium on Biomedical Imaging (ISBI), IEEE. pp. 295–298.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. [2019] Wan, T., Xu, S., Sang, C., Jin, Y., Qin, Z., 2019. Accurate
    segmentation of overlapping cells in cervical cytology with deep convolutional
    neural networks. Neurocomputing 365, 157–170.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Deng [2021] Wang, M., Deng, W., 2021. Deep face recognition: A survey.
    Neurocomputing 429, 215–244.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2021] Wei, Z., Cheng, S., Liu, X., Zeng, S., 2021. An efficient
    cervical whole slide image analysis framework based on multi-scale semantic and
    spatial deep features. arXiv preprint arXiv:2106.15113 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wilbur et al. [2009] Wilbur, D.C., Black-Schaffer, W.S., Luff, R.D., Abraham,
    K.P., Kemper, C., Molina, J.T., Tench, W.D., 2009. The becton dickinson focalpoint
    gs imaging system: clinical trials demonstrate significantly improved sensitivity
    for the detection of important cervical lesions. American Journal of Clinical
    Pathology 132, 767–775.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2018] Wu, M., Yan, C., Liu, H., Liu, Q., Yin, Y., 2018. Automatic
    classification of cervical cancer from cytological images by using convolutional
    neural network. Bioscience reports 38, BSR20181769.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiang et al. [2020] Xiang, Y., Sun, W., Pan, C., Yan, M., Yin, Z., Liang, Y.,
    2020. A novel automation-assisted cervical cancer reading method based on convolutional
    neural network. Biocybernetics and Biomedical Engineering 40, 611–623.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. [2018] Xie, Y., Xing, F., Shi, X., Kong, X., Su, H., Yang, L., 2018.
    Efficient and robust cell detection: A structured regression approach. Medical
    Image Analysis 44, 245–254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2014] Xu, Y., Mo, T., Feng, Q., Zhong, P., Lai, M., Eric, I., Chang,
    C., 2014. Deep learning of feature representation with multiple instance learning
    for medical image analysis, in: 2014 IEEE international conference on acoustics,
    speech and signal processing (ICASSP), IEEE. pp. 1626–1630.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. [2019] Yi, X., Walia, E., Babyn, P., 2019. Generative adversarial
    network in medical imaging: A review. Medical Image Analysis 58, 101552.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yosinski et al. [2014] Yosinski, J., Clune, J., Bengio, Y., Lipson, H., 2014.
    How transferable are features in deep neural networks? arXiv preprint arXiv:1411.1792
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2016] Yu, J., Jiang, Y., Wang, Z., Cao, Z., Huang, T., 2016. Unitbox:
    An advanced object detection network, in: Proceedings of the 24th ACM international
    conference on Multimedia, pp. 516–520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2021] Yu, S., Zhang, S., Wang, B., Dun, H., Xu, L., Huang, X., Shi,
    E., Feng, X., 2021. Generative adversarial network based data augmentation to
    improve cervical cell classification model. Mathematical Biosciences and Engineering:
    MBE 18, 1740–1752.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zaremba et al. [2014] Zaremba, W., Sutskever, I., Vinyals, O., 2014. Recurrent
    neural network regularization. arXiv preprint arXiv:1409.2329 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Żejmo et al. [2017] Żejmo, M., Kowal, M., Korbicz, J., Monczak, R., 2017. Classification
    of breast cancer cytological specimen using convolutional neural network, in:
    Journal of Physics: Conference Series, IOP Publishing. p. 012060.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2019] Zhang, C., Liu, D., Wang, L., Li, Y., Chen, X., Luo, R.,
    Che, S., Liang, H., Li, Y., Liu, S., et al., 2019. Dccl: a benchmark for cervical
    cytology analysis, in: International Workshop on Machine Learning in Medical Imaging,
    Springer. pp. 63–72.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2020] Zhang, H., Zhu, H., Ling, X., 2020. Polar coordinate sampling-based
    segmentation of overlapping cervical cells using attention u-net and random walk.
    Neurocomputing 383, 212–223.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2014] Zhang, L., Kong, H., Ting Chin, C., Liu, S., Fan, X., Wang,
    T., Chen, S., 2014. Automation-assisted cervical cancer screening in manual liquid-based
    cytology with hematoxylin and eosin staining. Cytometry Part A 85, 214–230.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2017] Zhang, L., Lu, L., Nogues, I., Summers, R.M., Liu, S.,
    Yao, J., 2017. Deeppap: deep convolutional networks for cervical cell classification.
    IEEE journal of biomedical and health informatics 21, 1633–1643.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2021] Zhang, Y., Sidibé, D., Morel, O., Mériaudeau, F., 2021.
    Deep multimodal fusion for semantic image segmentation: A survey. Image and Vision
    Computing 105, 104042.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. [2019] Zhao, Z., Lin, H., Chen, H., Heng, P.A., 2019. Pfa-scannet:
    Pyramidal feature aggregation with synergistic learning for breast cancer metastasis
    analysis, in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer. pp. 586–594.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. [2020] Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., Ren, D.,
    2020. Distance-iou loss: Faster and better learning for bounding box regression,
    in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 12993–13000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2016] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba,
    A., 2016. Learning deep features for discriminative localization, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp. 2921–2929.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2020] Zhou, Y., Chen, H., Lin, H., Heng, P.A., 2020. Deep semi-supervised
    knowledge distillation for overlapping cervical cell instance segmentation, in:
    International Conference on Medical Image Computing and Computer-Assisted Intervention,
    Springer. pp. 521–531.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2019a] Zhou, Y., Chen, H., Xu, J., Dou, Q., Heng, P.A., 2019a.
    Irnet: Instance relation network for overlapping cervical cell segmentation, in:
    International Conference on Medical Image Computing and Computer-Assisted Intervention,
    Springer. pp. 640–648.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2019b] Zhou, Y., Onder, O.F., Dou, Q., Tsougenis, E., Chen, H.,
    Heng, P.A., 2019b. Cia-net: Robust nuclei instance segmentation with contour-aware
    information aggregation, in: International Conference on Information Processing
    in Medical Imaging, Springer. pp. 682–693.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou [2018] Zhou, Z.H., 2018. A brief introduction to weakly supervised learning.
    National Science Review 5, 44–53.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. [2017] Zhu, J.Y., Park, T., Isola, P., Efros, A.A., 2017. Unpaired
    image-to-image translation using cycle-consistent adversarial networks, in: Proceedings
    of the IEEE international conference on computer vision, pp. 2223–2232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2021] Zhu, X., Li, X., Ong, K., Zhang, W., Li, W., Li, L., Young,
    D., Su, Y., Shang, B., Peng, L., et al., 2021. Hybrid ai-assistive diagnostic
    model permits rapid tbs classification of cervical liquid-based thin-layer cell
    smears. Nature Communications 12, 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
