- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:02:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:02:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2003.01200] Natural Language Processing Advancements By Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2003.01200] 深度学习驱动的自然语言处理进展：一项调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2003.01200](https://ar5iv.labs.arxiv.org/html/2003.01200)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2003.01200](https://ar5iv.labs.arxiv.org/html/2003.01200)
- en: 'Natural Language Processing Advancements By Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习驱动的自然语言处理进展：一项调查
- en: 'Amirsina Torfi,  Rouzbeh A. Shirvani,  Yaser Keneshloo, Nader Tavaf, and Edward A. Fox
    Amirsina Torfi, Yaser Keneshloo, and Edward A. Fox were with the Department of
    Computer Science, Virginia Polytechnic Institute and State University, Blacksburg,
    VA, 24060 USA e-mail: (amirsina.torfi@gmail.com, yaserkl@vt.edu, fox@vt.edu).
    Rouzbeh A. Shirvani is an independent researcher, e-mail: (rouzbeh.asghari@gmail.com).
    Nader Tavaf was with the University of Minnesota Twin Cities, Minneapolis, MN,
    55455 USA e-mail: (tavaf001@umn.edu).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Amirsina Torfi、Rouzbeh A. Shirvani、Yaser Keneshloo、Nader Tavaf 和 Edward A. Fox
    Amirsina Torfi、Yaser Keneshloo 和 Edward A. Fox 曾在美国弗吉尼亚理工大学计算机科学系工作，地址：Blacksburg,
    VA, 24060 USA，电子邮件：(amirsina.torfi@gmail.com, yaserkl@vt.edu, fox@vt.edu)。Rouzbeh
    A. Shirvani 是独立研究员，电子邮件：(rouzbeh.asghari@gmail.com)。Nader Tavaf 曾在明尼苏达大学双子城校区工作，地址：Minneapolis,
    MN, 55455 USA，电子邮件：(tavaf001@umn.edu)。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Natural Language Processing (NLP) helps empower intelligent machines by enhancing
    a better understanding of the human language for linguistic-based human-computer
    communication. Recent developments in computational power and the advent of large
    amounts of linguistic data have heightened the need and demand for automating
    semantic analysis using data-driven approaches. The utilization of data-driven
    strategies is pervasive now due to the significant improvements demonstrated through
    the usage of deep learning methods in areas such as Computer Vision, Automatic
    Speech Recognition, and in particular, NLP. This survey categorizes and addresses
    the different aspects and applications of NLP that have benefited from deep learning.
    It covers core NLP tasks and applications, and describes how deep learning methods
    and models advance these areas. We further analyze and compare different approaches
    and state-of-the-art models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）通过增强对人类语言的理解，帮助实现智能机器的智能化，从而改善基于语言的人机通信。计算能力的最近发展和大量语言数据的出现增加了对使用数据驱动方法自动化语义分析的需求和需求。由于深度学习方法在计算机视觉、自动语音识别以及特别是在NLP等领域的显著进展，数据驱动策略的使用如今无处不在。这项调查对受益于深度学习的NLP的不同方面和应用进行了分类和讨论。它涵盖了核心的NLP任务和应用，并描述了深度学习方法和模型如何推动这些领域的发展。我们进一步分析和比较了不同的方法和最先进的模型。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Natural Language Processing, Deep Learning, Artificial Intelligence
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理，深度学习，人工智能
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Natural Language Processing (NLP) is a sub-discipline of computer science providing
    a bridge between natural languages and computers. It helps empower machines to
    understand, process, and analyze human language [[1](#bib.bib1)]. NLP’s significance
    as a tool aiding comprehension of human-generated data is a logical consequence
    of the context-dependency of data. Data becomes more meaningful through a deeper
    understanding of its context, which in turn facilitates text analysis and mining.
    NLP enables this with the communication structures and patterns of humans.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是计算机科学的一个子学科，提供了自然语言与计算机之间的桥梁。它帮助机器理解、处理和分析人类语言[[1](#bib.bib1)]。NLP作为一种工具，帮助理解人类生成的数据，其重要性是数据上下文依赖性的逻辑结果。通过更深入理解数据的上下文，数据变得更有意义，从而促进了文本分析和挖掘。NLP通过人类的交流结构和模式实现了这一点。
- en: 'Development of NLP methods is increasingly reliant on data-driven approaches
    which help with building more powerful and robust models [[2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)]. Recent advances in computational power, as well as greater availability
    of big data, enable deep learning, one of the most appealing approaches in the
    NLP domain [[5](#bib.bib5), [2](#bib.bib2), [3](#bib.bib3)], especially given
    that deep learning has already demonstrated superior performance in adjoining
    fields like Computer Vision [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10)] and Speech Recognition [[11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)]. These developments led to a paradigm shift from traditional
    to novel data-driven approaches aimed at advancing NLP. The reason behind this
    shift was simple: new approaches are more promising regarding results, and are
    easier to engineer.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: NLP方法的发展越来越依赖于数据驱动的方法，这些方法有助于构建更强大和更稳健的模型[[2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4)]。计算能力的最新进展以及大数据的更大可用性使得深度学习成为NLP领域最具吸引力的方法之一[[5](#bib.bib5),
    [2](#bib.bib2), [3](#bib.bib3)]，特别是考虑到深度学习在计算机视觉[[6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]和语音识别[[11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)]等相关领域中已经表现出了卓越的性能。这些发展导致了从传统方法到新型数据驱动方法的范式转变，旨在推动NLP的进步。这种转变的原因很简单：新方法在结果方面更具前景，并且更容易工程实现。
- en: As a sequitur to remarkable progress achieved in adjacent disciplines utilizing
    deep learning methods, deep neural networks have been applied to various NLP tasks, including
    part-of-speech tagging [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16),
    [17](#bib.bib17)], named entity recognition [[18](#bib.bib18), [19](#bib.bib19),
    [18](#bib.bib18), [20](#bib.bib20), [21](#bib.bib21)], and semantic role labeling [[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]. Most of the research efforts
    in deep learning associated with NLP applications involve either supervised learning¹¹1Learning
    from training data to predict the type of new unseen test examples by mapping
    them to known pre-defined labels. or unsupervised learning²²2Making sense of data
    without sticking to specific tasks and supervisory signals..
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为在使用深度学习方法的相关学科中取得的显著进展的延续，深度神经网络已被应用于各种NLP任务，包括词性标注[[14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16), [17](#bib.bib17)]、命名实体识别[[18](#bib.bib18), [19](#bib.bib19),
    [18](#bib.bib18), [20](#bib.bib20), [21](#bib.bib21)]以及语义角色标注[[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]。大多数与NLP应用相关的深度学习研究工作涉及有监督学习¹¹1
    从训练数据中学习，以通过将新见样本映射到已知的预定义标签来预测新样本的类型。或无监督学习²²2 从数据中提取意义，而不依赖于特定任务和监督信号。
- en: This survey covers the emerging role of deep learning in the area of NLP, across
    a broad range of categories. The research presented in [[26](#bib.bib26)] is primarily
    focused on architectures, with little discussion of applications. More recent
    works [[27](#bib.bib27), [4](#bib.bib4)] are specific to certain applications
    or certain sub-fields of NLP [[21](#bib.bib21)]. Here we build on previous works
    by describing the challenges, opportunities, and evaluations of the impact of
    applying deep learning to NLP problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查涵盖了深度学习在NLP领域中新兴的角色，涉及广泛的类别。研究[[26](#bib.bib26)]主要集中在架构上，应用讨论较少。更近期的研究[[27](#bib.bib27),
    [4](#bib.bib4)]则专注于特定应用或NLP的某些子领域[[21](#bib.bib21)]。在这里，我们在之前的工作基础上，描述了将深度学习应用于NLP问题的挑战、机遇和影响评估。
- en: 'This survey has six sections, including this introduction. Section 2 lays out
    the theoretical dimensions of NLP and artificial intelligence, and looks at deep
    learning as an approach to solving real-world problems. It motivates this study
    by addressing the question: Why use deep learning in NLP? The third section discusses
    fundamental concepts necessary to understand NLP, covering exemplary issues in
    representation, frameworks, and machine learning. The fourth section summarizes
    benchmark datasets employed in the NLP domain. Section 5 focuses on some of the
    NLP applications where deep learning has demonstrated significant benefit. Finally,
    Section 6 provides a conclusion, also addressing some open problems and promising
    areas for improvement.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查包含六个部分，包括本引言。第2部分阐述了NLP和人工智能的理论维度，并将深度学习视为解决实际问题的一种方法。它通过回答这个问题来激发本研究的动机：为什么在NLP中使用深度学习？第三部分讨论了理解NLP所需的基本概念，涵盖了表示、框架和机器学习中的典型问题。第四部分总结了NLP领域中使用的基准数据集。第5部分重点介绍了一些深度学习在NLP中表现出显著好处的应用。最后，第6部分提供了结论，并讨论了一些开放性问题和有前景的改进领域。
- en: II Background
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: NLP has long been viewed as one aspect of artificial intelligence (AI), since
    understanding and generating natural language are high-level indications of intelligence.
    Deep learning is an effective AI tool, so we next situate deep learning in the
    AI world. After that we explain motivations for applying deep learning to NLP.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）长期以来被视为人工智能（AI）的一个方面，因为理解和生成自然语言是智力的高级体现。深度学习是一个有效的AI工具，因此接下来我们将深度学习置于AI领域中。之后，我们解释将深度学习应用于NLP的动机。
- en: II-A Artificial Intelligence and Deep Learning
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 人工智能与深度学习
- en: 'There have been “islands of success” where big data are processed via AI capabilities
    to produce information to achieve critical operational goals (e.g., fraud detection).
    Accordingly, scientists and consumers anticipate enhancement across a variety
    of applications. However, achieving this requires understanding of AI and its
    mechanisms and means (e.g., algorithms). Ted Greenwald, explaining AI to those
    who are not AI experts, comments: ”Generally AI is anything a computer can do
    that formerly was considered a job for a human” [[28](#bib.bib28)].'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些“成功的孤岛”中，大数据通过AI能力进行处理，以产生信息以实现关键的操作目标（例如欺诈检测）。因此，科学家和消费者期望在各种应用中获得增强。然而，达到这一目标需要理解AI及其机制和手段（例如算法）。Ted
    Greenwald 在向非AI专家解释AI时评论道：“通常，AI是指计算机可以完成的、以前被认为是人类工作的任何事情”[[28](#bib.bib28)]。
- en: An AI goal is to extend the capabilities of information technology (IT) from
    those to (1) generate, communicate, and store data, to also (2) process data into
    the knowledge that decision makers and others need [[29](#bib.bib29)]. One reason
    is that the available data volume is increasing so rapidly that it is now impossible
    for people to process all available data. This leaves two choices: (1) much or
    even most existing data must be ignored or (2) AI must be developed to process
    the vast volumes of available data into the essential pieces of information that
    decision-makers and others can comprehend. Deep learning is a bridge between the
    massive amounts of data and AI.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: AI的目标是将信息技术（IT）的能力从（1）生成、传输和存储数据扩展到（2）将数据处理成决策者和其他人所需的知识[[29](#bib.bib29)]。其中一个原因是可用数据量增长如此迅速，以至于现在不可能处理所有可用数据。这就留下了两个选择：（1）必须忽略大量甚至大部分现有数据，或者（2）必须开发AI来处理大量的可用数据，将其转化为决策者和其他人可以理解的关键信息。深度学习是海量数据与AI之间的桥梁。
- en: II-A1 Definitions
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 定义
- en: Deep learning refers to applying deep neural networks to massive amounts of
    data to learn a procedure aimed at handling a task. The task can range from simple
    classification to complex reasoning. In other words, deep learning is a set of
    mechanisms ideally capable of deriving an optimum solution to any problem given
    a sufficiently extensive and relevant input dataset. Loosely speaking, deep learning
    is detecting and analyzing important structures/features in the data aimed at
    formulating a solution to a given problem. Here, AI and deep learning meet. One
    version of the goal or ambition behind AI is enabling a machine to outperform
    what the human brain does. Deep learning is a means to this end.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是指将深度神经网络应用于大量数据，以学习一种旨在处理任务的过程。任务可以从简单的分类到复杂的推理。换句话说，深度学习是一组机制，理想情况下能够在给定足够广泛和相关的输入数据集的情况下，得出任何问题的最佳解决方案。宽泛地说，深度学习是检测和分析数据中的重要结构/特征，以制定解决给定问题的方案。在这里，AI
    和深度学习相遇。AI 的目标或雄心之一是使机器超越人脑的能力。深度学习是实现这一目标的手段。
- en: II-A2 Deep Learning Architectures
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 深度学习架构
- en: Numerous deep learning architectures have been developed in different research
    areas, e.g., in NLP applications employing recurrent neural networks (RNNs) [[30](#bib.bib30)],
    convolutional neural networks (CNNs) [[31](#bib.bib31)], and more recently, recursive
    neural networks [[32](#bib.bib32)]. We focus our discussion on a review of the
    essential models, explained in relevant seminal publications.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同研究领域，已开发了许多深度学习架构，例如在 NLP 应用中使用的递归神经网络（RNNs）[[30](#bib.bib30)]，卷积神经网络（CNNs）[[31](#bib.bib31)]，以及最近的递归神经网络[[32](#bib.bib32)]。我们重点讨论对相关经典文献中重要模型的回顾。
- en: 'Multi Layer Perceptron: A multilayer perceptron (MLP) has at least three layers (input,
    hidden, and output layers). A layer is simply a collection of neurons operating
    to transform information from the previous layer to the next layer. In the MLP
    architecture, the neurons in a layer do not communicate with each other. An MLP
    employs nonlinear activation functions. Every node in a layer connects to all
    nodes in the next layer, creating a fully connected network (Fig. [1](#S2.F1 "Figure
    1 ‣ II-A2 Deep Learning Architectures ‣ II-A Artificial Intelligence and Deep
    Learning ‣ II Background ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey")). MLPs are the simplest type of Feed-Forward Neural Networks (FNNs).
    FNNs represent a general category of neural networks in which the connections
    between the nodes do not create any cycle, i.e., in a FNN there is no cycle of
    information flow.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '多层感知器：一个多层感知器（MLP）至少有三层（输入层、隐藏层和输出层）。一层只是神经元的集合，用于将信息从上一层转换到下一层。在 MLP 架构中，一层中的神经元不会相互通信。MLP
    使用非线性激活函数。每一层中的节点都连接到下一层中的所有节点，形成一个完全连接的网络（图 [1](#S2.F1 "Figure 1 ‣ II-A2 Deep
    Learning Architectures ‣ II-A Artificial Intelligence and Deep Learning ‣ II Background
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey")）。MLP 是最简单的前馈神经网络（FNNs）类型。FNNs
    表示一个通用的神经网络类别，其中节点之间的连接不会形成任何循环，即在 FNN 中没有信息流的循环。'
- en: '![Refer to caption](img/deb92729af814df112406bacdfde1eb0.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/deb92729af814df112406bacdfde1eb0.png)'
- en: 'Figure 1: The general architecture of a MLP.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: MLP 的总体架构。'
- en: 'Convolutional Neural Networks: Convolutional neural networks (CNNs), whose
    architecture is inspired by the human visual cortex, are a subclass of feed-forward
    neural networks. CNNs are named after the underlying mathematical operation, convolution,
    which yields a measure of the interoperability of its input functions. Convolutional
    neural networks are usually employed in situations where data is or needs to be
    represented with a 2D or 3D data map. In the data map representation, the proximity
    of data points usually corresponds to their information correlation.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络：卷积神经网络（CNNs）其架构受到人类视觉皮层的启发，是前馈神经网络的一个子类。CNN 的命名来源于其底层数学运算，即卷积，这会生成其输入函数的可互操作性度量。卷积神经网络通常用于数据需要以
    2D 或 3D 数据图表示的情况。在数据图表示中，数据点的接近度通常对应于它们的信息关联。
- en: 'In convolutional neural networks where the input is an image, the data map
    indicates that image pixels are highly correlated to their neighboring pixels. Consequently,
    the convolutional layers have 3 dimensions: width, height, and depth. That assumption
    possibly explains why the majority of research efforts dedicated to CNNs are conducted
    in the Computer Vision field [[33](#bib.bib33)].'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络中，当输入是图像时，数据图表表明图像像素与其邻近像素高度相关。因此，卷积层具有 3 个维度：宽度、高度和深度。这一假设可能解释了为什么大多数致力于
    CNN 的研究工作都在计算机视觉领域进行[[33](#bib.bib33)]。
- en: 'A CNN takes an image represented as an array of numeric values. After performing
    specific mathematical operations, it represents the image in a new output space.
    This operation is also called feature extraction, and helps to capture and represent
    key image content. The extracted features can be used for further analysis, for
    different tasks. One example is image classification, which aims to categorize
    images according to some predefined classes. Other examples include determining
    which objects are present in an image and where they are located. See Fig. [2](#S2.F2
    "Figure 2 ‣ II-A2 Deep Learning Architectures ‣ II-A Artificial Intelligence and
    Deep Learning ‣ II Background ‣ Natural Language Processing Advancements By Deep
    Learning: A Survey").'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '一个 CNN 接收作为数值数组表示的图像。经过特定的数学操作后，它在新的输出空间中表示图像。这一操作也称为特征提取，帮助捕捉和表示图像的关键内容。提取的特征可以用于进一步分析，完成不同的任务。例如，图像分类旨在根据一些预定义的类别对图像进行分类。其他例子包括确定图像中存在哪些对象及其位置。参见图
    [2](#S2.F2 "Figure 2 ‣ II-A2 Deep Learning Architectures ‣ II-A Artificial Intelligence
    and Deep Learning ‣ II Background ‣ Natural Language Processing Advancements By
    Deep Learning: A Survey")。'
- en: In the case of utilizing CNNs for NLP, the inputs are sentences or documents
    represented as matrices. Each row of the matrix is associated with a language
    element such as a word or a character. The majority of CNN architectures learn
    word or sentence representations in their training phase. A variety of CNN architectures
    were used in various classification tasks such as Sentiment Analysis and Topic
    Categorization [[31](#bib.bib31), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)].
    CNNs were employed for Relation Extraction and Relation Classification as well [[37](#bib.bib37),
    [38](#bib.bib38)].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在利用 CNN 进行 NLP 的情况下，输入是表示为矩阵的句子或文档。矩阵的每一行与语言元素如词或字符相关联。大多数 CNN 架构在其训练阶段学习词或句子的表示。各种
    CNN 架构被用于不同的分类任务，如情感分析和主题分类[[31](#bib.bib31), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36)]。CNN 还用于关系抽取和关系分类[[37](#bib.bib37), [38](#bib.bib38)]。
- en: '![Refer to caption](img/237c358f23dd7601ee840b30d97074d0.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/237c358f23dd7601ee840b30d97074d0.png)'
- en: 'Figure 2: A typical CNN architecture for object detection. The network provides
    a feature representation with attention to the specific region of an image (example
    shown on the left) that contains the object of interest. Out of the multiple regions
    represented (see an ordering of the image blocks, giving image pixel intensity,
    on the right) by the network, the one with the highest score will be selected
    as the main candidate.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：用于对象检测的典型 CNN 架构。网络提供了对图像中特定区域的特征表示（左侧示例），该区域包含感兴趣的对象。在网络表示的多个区域中（参见右侧的图像块顺序，显示图像像素强度），得分最高的区域将被选为主要候选区域。
- en: '![Refer to caption](img/869fc61a1c0ca4b79729a8eb686c7697.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/869fc61a1c0ca4b79729a8eb686c7697.png)'
- en: 'Figure 3: Recurrent Neural Network (RNN), summarized on the left, expanded
    on the right, for $N$ timesteps, with $X$ indicating input, $h$ hidden layer,
    and $O$ output'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：递归神经网络（RNN），左侧为总结，右侧为展开，针对 $N$ 个时间步，$X$ 表示输入，$h$ 为隐藏层，$O$ 为输出
- en: 'Recurrent Neural Network: If we line up a sequence of FNNs and feed the output
    of each FNN as an input to the next one, a recurrent neural network (RNN) will
    be constructed. Like FNNs, layers in an RNN can be categorized into input, hidden,
    and output layers. In discrete time frames, sequences of input vectors are fed
    as the input, one vector at a time, e.g., after inputting each batch of vectors,
    conducting some operations and updating the network weights, the next input batch
    will be fed to the network. Thus, as shown in Fig. [3](#S2.F3 "Figure 3 ‣ II-A2
    Deep Learning Architectures ‣ II-A Artificial Intelligence and Deep Learning ‣
    II Background ‣ Natural Language Processing Advancements By Deep Learning: A Survey"),
    at each time step we make predictions and use parameters of the current hidden
    layer as input to the next time step.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '循环神经网络（RNN）：如果我们将一系列的前馈神经网络（FNN）排列起来，并将每个FNN的输出作为下一个FNN的输入，就会构建一个循环神经网络（RNN）。与FNN一样，RNN中的层可以分为输入层、隐藏层和输出层。在离散时间帧中，输入向量的序列被逐个输入，例如，在输入每批向量后，进行一些操作并更新网络权重，然后将下一批输入数据喂入网络。因此，如图
    [3](#S2.F3 "Figure 3 ‣ II-A2 Deep Learning Architectures ‣ II-A Artificial Intelligence
    and Deep Learning ‣ II Background ‣ Natural Language Processing Advancements By
    Deep Learning: A Survey") 所示，在每个时间步中，我们进行预测并将当前隐藏层的参数作为下一个时间步的输入。'
- en: Hidden layers in recurrent neural networks can carry information from the past,
    in other words, memory. This characteristic makes them specifically useful for
    applications that deal with a sequence of inputs such as language modeling [[39](#bib.bib39)],
    i.e., representing language in a way that the machine understands. This concept
    will be described later in detail.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络中的隐藏层可以携带来自过去的信息，换句话说，就是记忆。这一特性使得它们在处理序列输入的应用中非常有用，比如语言建模 [[39](#bib.bib39)]，即以机器可以理解的方式表示语言。这一概念将在后续详细描述。
- en: 'RNNs can carry rich information from the past. Consider the sentence: “Michael
    Jackson was a singer; some people consider him King of Pop.” It’s easy for a human
    to identify him as referring to Michael Jackson. The pronoun him happens seven
    words after Michael Jackson; capturing this dependency is one of the benefits
    of RNNs, where the hidden layers in an RNN act as memory units. Long Short Term
    Memory Network (LSTM) [[40](#bib.bib40)] is one of the most widely used classes
    of RNNs. LSTMs try to capture even long time dependencies between inputs from
    different time steps. Modern Machine Translation and Speech Recognition often
    rely on LSTMs.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: RNN能够携带丰富的过去信息。考虑句子：“Michael Jackson was a singer; some people consider him
    King of Pop。”人类很容易识别出这里的“him”指的是Michael Jackson。代词“him”出现在Michael Jackson之后七个词的位置；捕捉这种依赖关系是RNN的一个好处，其中RNN的隐藏层充当记忆单元。长短期记忆网络（LSTM）[[40](#bib.bib40)]
    是最广泛使用的RNN类之一。LSTM试图捕捉来自不同时间步的输入之间的长时间依赖关系。现代机器翻译和语音识别通常依赖于LSTM。
- en: '![Refer to caption](img/089e1c53e45d54bb27c5a0c088c34c37.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/089e1c53e45d54bb27c5a0c088c34c37.png)'
- en: 'Figure 4: Schematic of an Autoencoder'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：自编码器示意图
- en: 'Autoencoders: Autoencoders implement unsupervised methods in deep learning.
    They are widely used in dimensionality reduction³³3Dimensionality reduction is
    an unsupervised learning approach which is the process of reducing the number
    of variables that were used to represent the data by identifying the most crucial
    information. or NLP applications which consist of sequence to sequence modeling (see
    Section [III-B](#S3.SS2 "III-B Seq2Seq Framework ‣ III Core Concepts in NLP ‣
    Natural Language Processing Advancements By Deep Learning: A Survey") [[39](#bib.bib39)].
    Fig. [4](#S2.F4 "Figure 4 ‣ II-A2 Deep Learning Architectures ‣ II-A Artificial
    Intelligence and Deep Learning ‣ II Background ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey") illustrates the schematic of an Autoencoder. Since
    autoencoders are unsupervised, there is no label corresponding to each input.
    They aim to learn a code representation for each input. The encoder is like a
    feed-forward neural network in which the input gets encoded into a vector (code).
    The decoder operates similarly to the encoder, but in reverse, i.e., constructing
    an output based on the encoded input. In data compression applications, we want
    the created output to be as close as possible to the original input. Autoencoders
    are lossy, meaning the output is an approximate reconstruction of the input.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '自编码器：自编码器在深度学习中实现了无监督方法。它们广泛用于降维³³降维是一种无监督学习方法，其过程是通过识别最关键的信息来减少表示数据所用的变量数量。或自然语言处理应用，这些应用包括序列到序列建模（参见第[III-B节](#S3.SS2
    "III-B Seq2Seq Framework ‣ III Core Concepts in NLP ‣ Natural Language Processing
    Advancements By Deep Learning: A Survey") [[39](#bib.bib39)]）。图[4](#S2.F4 "Figure
    4 ‣ II-A2 Deep Learning Architectures ‣ II-A Artificial Intelligence and Deep
    Learning ‣ II Background ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey")展示了自编码器的示意图。由于自编码器是无监督的，因此每个输入没有相应的标签。它们的目标是为每个输入学习一个代码表示。编码器类似于一个前馈神经网络，其中输入被编码为一个向量（代码）。解码器的操作类似于编码器，但方向相反，即基于编码的输入构造一个输出。在数据压缩应用中，我们希望创建的输出尽可能接近原始输入。自编码器是有损的，这意味着输出是输入的近似重构。'
- en: '![Refer to caption](img/8d49fdc610070003d7cb758f21742cb6.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8d49fdc610070003d7cb758f21742cb6.png)'
- en: 'Figure 5: Generative Adversarial Networks'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：生成对抗网络
- en: 'Generative Adversarial Networks: Goodfellow [[41](#bib.bib41)] introduced Generative
    Adversarial Networks (GANs). As shown in Fig. [5](#S2.F5 "Figure 5 ‣ II-A2 Deep
    Learning Architectures ‣ II-A Artificial Intelligence and Deep Learning ‣ II Background
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey"), a GAN
    is a combination of two neural networks, a discriminator and a generator. The
    whole network is trained in an iterative process. First, the generator network
    generates a fake sample. Then the discriminator network tries to determine whether
    this sample (ex.: an input image) is real or fake, i.e., whether it came from
    the real training data (data used for building the model) or not. The goal of
    the generator is to fool the discriminator in a way that the discriminator believes
    the artificial (i.e., generated) samples synthesized by the generator are real.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '生成对抗网络：Goodfellow [[41](#bib.bib41)] 引入了生成对抗网络（GANs）。如图[5](#S2.F5 "Figure 5
    ‣ II-A2 Deep Learning Architectures ‣ II-A Artificial Intelligence and Deep Learning
    ‣ II Background ‣ Natural Language Processing Advancements By Deep Learning: A
    Survey")所示，GAN是两个神经网络的组合，一个是鉴别器，另一个是生成器。整个网络在一个迭代过程中进行训练。首先，生成器网络生成一个假样本。然后，鉴别器网络尝试确定这个样本（例如：一个输入图像）是实际的还是虚假的，即它是否来自真实的训练数据（用于构建模型的数据）。生成器的目标是以一种让鉴别器相信生成器合成的人工（即生成的）样本是真实的方式来欺骗鉴别器。'
- en: 'This iterative process continues until the generator produces samples that
    are indistinguishable by the discriminator. In other words, the probability of
    classifying a sample as fake or real becomes like flipping a fair coin for the
    discriminator. The goal of the generative model is to capture the distribution
    of real data while the discriminator tries to identify the fake data. One of the
    interesting features of GANs (regarding being generative) is: once the training
    phase is finished, there is no need for the discrimination network, so we solely
    can work with the generation network. In other words, having access to the trained
    generative model is sufficient.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这一迭代过程持续进行，直到生成器生成的样本无法被鉴别器区分。换句话说，鉴别样本是假的还是实际的概率就像掷一枚公平的硬币一样。生成模型的目标是捕捉真实数据的分布，而鉴别器则尝试识别假数据。GANs（生成对抗网络）的一个有趣特点是：一旦训练阶段完成，就不再需要鉴别网络，因此我们只需使用生成网络。换句话说，拥有经过训练的生成模型就足够了。
- en: Different forms of GANs has been introduced, e.g., Sim GAN [[8](#bib.bib8)],
    Wasserstein GAN [[42](#bib.bib42)], info GAN [[43](#bib.bib43)], and DC GAN [[44](#bib.bib44)].
    In one of the most elegant GAN implementations [[45](#bib.bib45)], entirely artificial,
    yet almost perfect, celebrity faces are generated; the pictures are not real,
    but fake photos produced by the network. GAN’s have since received significant
    attention in various applications and have generated astonishing result [[46](#bib.bib46)].
    In the NLP domain, GANs often are used for text generation [[47](#bib.bib47),
    [48](#bib.bib48)].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 已经介绍了不同形式的GANs，例如，Sim GAN[[8](#bib.bib8)]、Wasserstein GAN[[42](#bib.bib42)]、info
    GAN[[43](#bib.bib43)]和DC GAN[[44](#bib.bib44)]。在其中一个最优雅的GAN实现[[45](#bib.bib45)]中，生成了完全人工但几乎完美的名人面孔；这些图片不是真实的，而是网络生成的假照片。GANs自此在各种应用中引起了显著关注，并生成了惊人的结果[[46](#bib.bib46)]。在自然语言处理领域，GANs通常用于文本生成[[47](#bib.bib47),
    [48](#bib.bib48)]。
- en: II-B Motivation for Deep Learning in NLP
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 深度学习在自然语言处理中的动机
- en: Deep learning applications are predicated on the choices of (1) feature representation
    and (2) deep learning algorithm alongside architecture. These are associated with
    data representation and learning structure, respectively. For data representation,
    surprisingly, there usually is a disjunction between what information is thought
    to be important for the task at hand, versus what representation actually yields
    good results. For instance, in sentiment analysis, lexicon semantics, syntactic
    structure, and context are assumed by some linguists to be of primary significance.
    Nevertheless, previous studies based on the bag-of-words (BoW) model demonstrated
    acceptable performance [[49](#bib.bib49)]. The bag-of-words model [[50](#bib.bib50)],
    often viewed as the vector space model, involves a representation which accounts
    only for the words and their frequency of occurrence. BoW ignores the order and
    interaction of words, and treats each word as a unique feature. BoW disregards
    syntactic structure, yet provides decent results for what some would consider
    syntax-dependent applications. This observation suggests that simple representations,
    when coupled with large amounts of data, may work as well or better than more
    complex representations. These findings corroborate the argument in favor of the
    importance of deep learning algorithms and architectures.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习应用基于（1）特征表示和（2）深度学习算法及其架构的选择。这些与数据表示和学习结构分别相关。对于数据表示，令人惊讶的是，任务所需的重要信息与实际能产生良好结果的表示之间通常存在脱节。例如，在情感分析中，一些语言学家认为词汇语义、句法结构和上下文具有主要重要性。然而，基于词袋模型（BoW）的先前研究展示了可接受的性能[[49](#bib.bib49)]。词袋模型[[50](#bib.bib50)]，通常被视为向量空间模型，涉及仅考虑单词及其出现频率的表示。BoW忽略了单词的顺序和互动，将每个单词视为一个独特的特征。BoW忽略了句法结构，但对于一些认为依赖于句法的应用来说，仍提供了不错的结果。这一观察表明，简单的表示与大量数据结合时，可能效果与复杂表示一样好，甚至更好。这些发现证实了深度学习算法和架构重要性的观点。
- en: Often the progress of NLP is bound to effective language modeling. A goal of
    statistical language modeling is the probabilistic representation of word sequences
    in language, which is a complicated task due to the curse of dimensionality. The
    research presented in [[51](#bib.bib51)] was a breakthrough for language modeling
    with neural networks aimed at overcoming the curse of dimensionality by (1) learning
    a distributed representation of words and (2) providing a probability function
    for sequences.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的进展通常依赖于有效的语言建模。统计语言建模的目标是对语言中的词序列进行概率表示，由于维度诅咒，这是一项复杂的任务。在[[51](#bib.bib51)]中提出的研究在语言建模方面取得了突破，旨在通过（1）学习词的分布式表示和（2）为序列提供概率函数来克服维度诅咒。
- en: A key challenge in NLP research, compared to other domains such as Computer
    Vision, seems to be the complexity of achieving an in-depth representation of
    language using statistical models. A primary task in NLP applications is to provide
    a representation of texts, such as documents. This involves feature learning,
    i.e., extracting meaningful information to enable further processing and analysis
    of the raw data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算机视觉等其他领域相比，自然语言处理研究中的一个主要挑战似乎是使用统计模型实现语言的深入表示的复杂性。自然语言处理应用中的主要任务是提供文本的表示，例如文档。这涉及特征学习，即提取有意义的信息，以便进一步处理和分析原始数据。
- en: Traditional methods begin with time-consuming hand-crafting of features, through
    careful human analysis of a specific application, and are followed by development
    of algorithms to extract and utilize instances of those features. On the other
    hand, deep supervised feature learning methods are highly data-driven and can
    be used in more general efforts aimed at providing a robust data representation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法开始于耗时的特征手工设计，通过对特定应用的细致人工分析，然后开发算法以提取和利用这些特征的实例。另一方面，深度监督特征学习方法高度数据驱动，可以用于更一般的目标，旨在提供一个强大的数据表示。
- en: Due to the vast amounts of unlabeled data, unsupervised feature learning is
    considered to be a crucial task in NLP. Unsupervised feature learning is, in essence,
    learning the features from unlabeled data to provide a low-dimensional representation
    of a high-dimensional data space. Several approaches such as K-means clustering
    and principal component analysis have been proposed and successfully implemented
    to this end. With the advent of deep learning and abundance of unlabeled data,
    unsupervised feature learning becomes a crucial task for representation learning,
    a precursor in NLP applications. Currently, most of the NLP tasks rely on annotated
    data, while a preponderance of unannotated data further motivates research in
    leveraging deep data-driven unsupervised methods.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大量未标记的数据，无监督特征学习被认为是自然语言处理中的一项关键任务。无监督特征学习本质上是从未标记的数据中学习特征，以提供高维数据空间的低维表示。已经提出并成功实现了几种方法，如K均值聚类和主成分分析。随着深度学习的出现和未标记数据的丰富，无监督特征学习成为表示学习的关键任务，而表示学习是自然语言处理应用中的前提。目前，大多数自然语言处理任务依赖于标注数据，而大量未标注的数据进一步激发了利用深度数据驱动的无监督方法进行研究的动力。
- en: Given the potential superiority of deep learning approaches in NLP applications,
    it seems crucial to perform a comprehensive analysis of various deep learning
    methods and architectures with particular attention to NLP applications.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于深度学习方法在自然语言处理应用中的潜在优势，进行对各种深度学习方法和架构的全面分析，特别是关注自然语言处理应用，似乎至关重要。
- en: III Core Concepts in NLP
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 核心概念在自然语言处理
- en: III-A Feature Representation
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 特征表示
- en: Distributed representations are a series of compact, low dimensional representations
    of data, each representing some distinct informative property. For NLP systems,
    due to issues related to the atomic representation of the symbols, it is imperative
    to learn word representations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式表示是一系列紧凑的、低维的数据表示，每种表示都代表某些独特的信息特性。对于自然语言处理（NLP）系统来说，由于符号的原子表示问题，学习词表示是至关重要的。
- en: At first, let’s concentrate on how the features are represented, and then we
    focus on different approaches for learning word representations. The encoded input
    features can be characters, words [[32](#bib.bib32)], sentences [[52](#bib.bib52)],
    or other linguistic elements. Generally, it is more desirable to provide a compact
    representation of the words than a sparse one.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们集中在特征的表示方式上，然后我们再关注学习词汇表示的不同方法。编码后的输入特征可以是字符、词汇[[32](#bib.bib32)]、句子[[52](#bib.bib52)]或其他语言元素。一般来说，比起稀疏表示，提供更紧凑的词汇表示是更为理想的。
- en: '![Refer to caption](img/df5a54e295cd5670eb9caaaddc002b23.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/df5a54e295cd5670eb9caaaddc002b23.png)'
- en: 'Figure 6: Considering a given sequence, the skip-thought model generates the
    surrounding sequences using the trained encoder. The assumption is that the surrounding
    sentences are closely related, contextually.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：考虑给定的序列，skip-thought模型使用训练好的编码器生成周围的序列。假设是周围的句子在语境上是紧密相关的。
- en: 'How to select the structure and level of text representation used to be an
    unresolved question. After proposing the word2vec approach [[53](#bib.bib53)],
    subsequently, doc2vec was proposed in [[52](#bib.bib52)] as an unsupervised algorithm
    and was called Paragraph Vector (PV). The goal behind PV is to learn fixed-length
    representations from variable-length text parts such as sentences and documents.
    One of the main objectives of doc2vec is to overcome the drawbacks of models such
    as BoW and to provide promising results for applications such as text classification
    and sentiment analysis. A more recent approach is the skip-thought model which
    applies word2vec at the sentence-level [[54](#bib.bib54)]. By utilizing an encoder-decoder
    architecture, this model generates the surrounding sentences using the given sentence (Fig. [6](#S3.F6
    "Figure 6 ‣ III-A Feature Representation ‣ III Core Concepts in NLP ‣ Natural
    Language Processing Advancements By Deep Learning: A Survey")). Next, let’s investigate
    different kinds of feature representation.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '如何选择文本表示的结构和层次曾经是一个未解之谜。在提出word2vec方法[[53](#bib.bib53)]之后，随后提出了doc2vec[[52](#bib.bib52)]，作为一种无监督算法，被称为段落向量（PV）。PV的目标是从可变长度的文本部分（如句子和文档）中学习固定长度的表示。doc2vec的主要目标之一是克服BoW等模型的缺点，并为文本分类和情感分析等应用提供有希望的结果。更近期的方法是skip-thought模型，它在句子级别应用word2vec[[54](#bib.bib54)]。通过利用编码器-解码器架构，该模型使用给定的句子生成周围的句子（图[6](#S3.F6
    "Figure 6 ‣ III-A Feature Representation ‣ III Core Concepts in NLP ‣ Natural
    Language Processing Advancements By Deep Learning: A Survey")）。接下来，让我们探讨不同种类的特征表示。'
- en: III-A1 One-Hot Representation
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 独热表示
- en: In one-hot encoding, each unique element that needs to be represented has its
    dimension which results in a very high dimensional, very sparse representation. Assume
    the words are represented with the one-hot encoding method. Regarding representation
    structure, there is no meaningful connection between different words in the feature
    space. For example, highly correlated words such as ‘ocean’ and ‘water’ will not
    be closer to each other (in the representation space) compared to less correlated
    pairs such as ‘ocean’ and ‘fire.’ Nevertheless, some research efforts present
    promising results using one-hot encoding [[2](#bib.bib2)].
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在独热编码中，每个需要表示的唯一元素都有其维度，这导致了一个非常高维且非常稀疏的表示。假设词汇是通过独热编码方法进行表示的。在表示结构方面，特征空间中的不同词汇之间没有有意义的联系。例如，高度相关的词汇如‘ocean’和‘water’在表示空间中不会比相关性较低的词汇对如‘ocean’和‘fire’更接近。然而，一些研究工作使用独热编码呈现了有希望的结果[[2](#bib.bib2)]。
- en: III-A2 Continuous Bag of Words
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 连续词袋模型
- en: Continuous Bag-of-Words model (CBOW) has frequently been used in NLP applications.
    CBOW tries to predict a word given its surrounding context, which usually consists
    of a few nearby words [[55](#bib.bib55)]. CBOW is neither dependent on the sequential
    order of words nor necessarily on probabilistic characteristics. So it is not
    generally used for language modeling. This model is typically trained to be utilized
    as a pre-trained model for more sophisticated tasks. An alternative to CBOW is
    the weighted CBOW (WCBOW) [[56](#bib.bib56)] in which different vectors get different
    weights reflective of relative importance in context. The simplest example can
    be document categorization where features are words and weights are TF-IDF scores [[57](#bib.bib57)]
    of the associated words.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 连续词袋模型（CBOW）在自然语言处理应用中经常被使用。CBOW 试图根据周围的上下文来预测一个词，这个上下文通常包括几个相邻的词[[55](#bib.bib55)]。CBOW
    不依赖于词的顺序，也不一定依赖于概率特征，因此它通常不用于语言建模。该模型通常被训练用于作为更复杂任务的预训练模型。CBOW 的一种替代方法是加权 CBOW（WCBOW）[[56](#bib.bib56)]，其中不同的向量根据上下文的重要性赋予不同的权重。最简单的例子是文档分类，其中特征是词，权重是相关词的
    TF-IDF 分数[[57](#bib.bib57)]。
- en: III-A3 Word-Level Embedding
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 词级嵌入
- en: Word embedding is a learned representation for context elements in which, ideally,
    words with related semantics become highly correlated in the representation space.
    One of the main incentives behind word embedding representations is the high generalization
    power as opposed to sparse, higher dimensional representations [[58](#bib.bib58)].
    Unlike the traditional bag-of-words model in which different words have entirely
    different representations regardless of their usage or collocations, learning
    a distributed representation takes advantage of word usage in context to provide
    similar representations for semantically correlated words. There are different
    approaches to create word embeddings. Several research efforts, including [[55](#bib.bib55),
    [53](#bib.bib53)], used random initialization by uniformly sampling random numbers
    with the objective of training an efficient representation of the model on a large
    dataset. This setup is intuitively acceptable for initialization of the embedding
    for common features such as part-of-speech tags. However, this may not be the
    optimum method for representation of less frequent features such as individual
    words. For the latter, pre-trained models, trained in a supervised or unsupervised
    manner, are usually leveraged for increasing the performance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是一种学习到的上下文元素表示，其中理想情况下，具有相关语义的词在表示空间中高度相关。词嵌入表示背后的主要动机之一是其高泛化能力，相对于稀疏的高维表示[[58](#bib.bib58)]。与传统的词袋模型不同，传统模型中不同的词具有完全不同的表示，不管它们的使用或搭配如何，学习分布式表示利用上下文中的词使用，为语义相关的词提供相似的表示。有不同的方法来创建词嵌入。包括[[55](#bib.bib55),
    [53](#bib.bib53)]在内的几个研究工作使用了随机初始化，通过均匀抽样随机数来训练模型在大型数据集上的有效表示。这种设置在初始化如词性标签等常见特征的嵌入时直观上是可以接受的。然而，这可能不是表示较少见特征如个别词的最佳方法。对于后者，通常利用经过监督或无监督方式训练的预训练模型来提高性能。
- en: III-A4 Character-Level Embedding
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 字符级嵌入
- en: The methods mentioned earlier are mostly at higher levels of representation.
    Lower-level representations such as character-level representation require special
    attention as well, due to their simplicity of representation and the potential
    for correction of unusual character combinations such as misspellings [[2](#bib.bib2)].
    For generating character-level embeddings, CNNs have successfully been utilized [[14](#bib.bib14)].
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的方法大多属于较高层次的表示。由于字符级表示的简单性和纠正如拼写错误等不寻常字符组合的潜力[[2](#bib.bib2)]，低层次表示如字符级表示也需要特别关注。在生成字符级嵌入时，卷积神经网络（CNN）已成功应用[[14](#bib.bib14)]。
- en: Character-level embeddings have been used in different NLP applications [[59](#bib.bib59)].
    One of the main advantages is the ability to use small model sizes and represent
    words with lower-level language elements [[14](#bib.bib14)]. Here word embeddings
    are models utilizing CNNs over the characters. Another motivation for employing
    character-level embeddings is the out-of-vocabulary word (OOV) issue which is
    usually encountered when, for the given word, there is no equivalent vector in
    the word embedding. The character-level approach may significantly alleviate this
    problem. Nevertheless, this approach suffers from a weak correlation between characters
    and semantic and syntactic parts of the language. So, considering the aforementioned
    pros and cons of utilizing character-level embeddings, several research efforts
    tried to propose and implement higher-level approaches such as using sub-words [[60](#bib.bib60)]
    to create word embeddings for OOV instances as well as creating a semantic bridge
    between the correlated words [[61](#bib.bib61)].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 字符级嵌入已在不同的 NLP 应用中使用 [[59](#bib.bib59)]。主要优势之一是能够使用较小的模型并用较低级的语言元素表示单词 [[14](#bib.bib14)]。在这里，词嵌入是利用
    CNN 对字符进行建模的模型。使用字符级嵌入的另一个动机是处理词汇表外的单词（OOV）问题，当给定单词在词嵌入中没有等效向量时通常会遇到此问题。字符级方法可能显著缓解这个问题。然而，这种方法在字符与语言的语义和句法部分之间存在较弱的相关性。因此，考虑到使用字符级嵌入的上述优缺点，多个研究工作尝试提出和实施更高级的方法，例如使用子词 [[60](#bib.bib60)]
    为 OOV 实例创建词嵌入，以及在相关单词之间创建语义桥梁 [[61](#bib.bib61)]。
- en: III-B Seq2Seq Framework
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B Seq2Seq 框架
- en: Most underlying frameworks in NLP applications rely on sequence-to-sequence
    (seq2seq) models in which not only the input but also the output is represented
    as a sequence. These models are common in various applications including machine
    translation⁴⁴4The input is a sequence of words from one language (e.g., English)
    and the output is the translation to another language (e.g., French)., text summarization⁵⁵5The
    input is a complete document (sequence of words) and the output is a summary of
    it (sequence of words)., speech-to-text, and text-to-speech applications⁶⁶6The
    input is an audio recording of a speech (sequence of audible elements) and the
    output is the speech text (sequence of words)..
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数自然语言处理（NLP）应用中的基础框架依赖于序列到序列（seq2seq）模型，其中不仅输入，而且输出也被表示为一个序列。这些模型在各种应用中很常见，包括机器翻译⁴⁴4输入是来自一种语言（例如，英语）的单词序列，输出是翻译成另一种语言（例如，法语）。,
    文本摘要⁵⁵5输入是完整的文档（单词序列），输出是对其的摘要（单词序列）。, 语音转文本和文本转语音应用⁶⁶6输入是演讲的音频录音（可听元素序列），输出是演讲文本（单词序列）。
- en: The most common seq2seq framework is comprised of an encoder and a decoder.
    The encoder ingests the sequence of input data and generates a mid-level output
    which is subsequently consumed by the decoder to produce the series of final outputs.
    The encoder and decoder are usually implemented via a series of Recurrent Neural
    Networks or LSTM [[40](#bib.bib40)] cells.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的 seq2seq 框架由编码器和解码器组成。编码器接收输入数据序列并生成中级输出，解码器随后使用这些中级输出生成最终的输出序列。编码器和解码器通常通过一系列递归神经网络或
    LSTM [[40](#bib.bib40)] 单元来实现。
- en: The encoder takes a sequence of length $T$, $X=\{x_{1},x_{2},\cdots,x_{T}\}$,
    where $x_{t}\in V=\{1,\cdots,|V|\}$ is the representation of a single input coming
    from the vocabulary $V$, and then generates the output state $h_{t}$. Subsequently,
    the decoder takes the last state from the encoder, i.e., $h_{t}$, and starts generating
    an output of size $L$, $Y^{\prime}=\{y^{\prime}_{1},y^{\prime}_{2},\cdots,y^{\prime}_{L}\}$,
    based on its current state, $s_{t}$, and the ground-truth output $y_{t}$. In different
    applications, the decoder could take advantage of more information such as a context
    vector [[62](#bib.bib62)] or intra-attention vectors [[63](#bib.bib63)] to generate
    better outputs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器接收一个长度为 $T$ 的序列 $X=\{x_{1},x_{2},\cdots,x_{T}\}$，其中 $x_{t}\in V=\{1,\cdots,|V|\}$
    是来自词汇表 $V$ 的单个输入的表示，然后生成输出状态 $h_{t}$。随后，解码器接收编码器的最后一个状态，即 $h_{t}$，并开始生成大小为 $L$
    的输出 $Y^{\prime}=\{y^{\prime}_{1},y^{\prime}_{2},\cdots,y^{\prime}_{L}\}$，基于其当前状态
    $s_{t}$ 和真实输出 $y_{t}$。在不同的应用中，解码器可以利用更多的信息，例如上下文向量 [[62](#bib.bib62)] 或内部注意力向量 [[63](#bib.bib63)]
    来生成更好的输出。
- en: 'One of the most widely training approaches for seq2seq models is called Teacher
    Forcing [[64](#bib.bib64)]. Let us define $y=\{y_{1},y_{2},\cdots,y_{L}\}$ as
    the ground-truth output sequence correspondent to a given input sequence $X$.
    The model training based on the maximum-likelihood criterion employs the following
    cross-entropy (CE) loss minimization:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一种广泛使用的 seq2seq 模型训练方法叫做 Teacher Forcing [[64](#bib.bib64)]。设 $y=\{y_{1},y_{2},\cdots,y_{L}\}$
    为对应于给定输入序列 $X$ 的真实输出序列。基于最大似然准则的模型训练采用以下交叉熵 (CE) 损失最小化：
- en: '|  | $\mathcal{L}_{CE}=-\sum_{t=1}^{L}\log{p_{\theta}(y_{t}&#124;y_{t-1},s_{t},X)}$
    |  | (1) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{CE}=-\sum_{t=1}^{L}\log{p_{\theta}(y_{t}&#124;y_{t-1},s_{t},X)}$
    |  | (1) |'
- en: where $\theta$ is the parameters of the model optimized during the training.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta$ 是在训练过程中优化的模型参数。
- en: 'Once the model is optimized using the cross-entropy loss, it can generate an
    entire sequence as follows. Let $\hat{y}_{t}$ denote the output generated by the
    model at time $t$. Then, the next output is generated by:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型使用交叉熵损失进行优化，它可以生成一个完整的序列。设 $\hat{y}_{t}$ 为模型在时间 $t$ 生成的输出。然后，下一步的输出通过以下方式生成：
- en: '|  | $\hat{y}_{t}=\operatorname*{arg\,max}_{y}p_{\theta}(y&#124;\hat{y}_{t-1},s_{t})$
    |  | (2) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}_{t}=\operatorname*{arg\,max}_{y}p_{\theta}(y&#124;\hat{y}_{t-1},s_{t})$
    |  | (2) |'
- en: In NLP applications, one can improve the output by using beam search to find
    a reasonably good output sequence [[3](#bib.bib3)]. During beam search, rather
    than using $\verb|argmax|$ for selecting the best output, we choose the top $K$
    outputs at each step, generate $K$ different paths for the output sequence, and
    finally choose the one that provides better performance as the final output. Although,
    there has been some recent studies [[65](#bib.bib65), [66](#bib.bib66)] on improving
    the beam search by incorporating a similar mechanism during training of them model,
    studying this is outside the scope of this paper.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 应用中，可以通过使用束搜索来改进输出，以找到一个合理的输出序列 [[3](#bib.bib3)]。在束搜索过程中，与其使用 $\verb|argmax|$
    选择最佳输出，不如在每一步选择前 $K$ 个输出，生成 $K$ 条不同的输出序列路径，最终选择性能更好的作为最终输出。尽管最近有一些研究 [[65](#bib.bib65),
    [66](#bib.bib66)] 通过在模型训练期间引入类似机制来改进束搜索，但研究这一点超出了本文的范围。
- en: Given a series of the ground-truth output $Y$ and the generated model output
    $\hat{Y}$, the model performance is evaluated using a task-specific measures such
    as ROUGE [[67](#bib.bib67)], BLEU [[68](#bib.bib68)], and METEOR [[69](#bib.bib69)].
    As an example, $\textrm{ROUGE}_{L}$, which is an evaluation metric in NLP tasks,
    uses the largest common sub-string between ground-truth $Y$ and model output $\hat{Y}$
    to evaluate the generated output.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一系列真实输出 $Y$ 和生成的模型输出 $\hat{Y}$，模型性能通过特定任务的评估指标来衡量，如 ROUGE [[67](#bib.bib67)]、BLEU [[68](#bib.bib68)]
    和 METEOR [[69](#bib.bib69)]。例如，$\textrm{ROUGE}_{L}$ 作为一种 NLP 任务中的评估指标，通过真实值 $Y$
    和模型输出 $\hat{Y}$ 之间的最大公共子串来评估生成的输出。
- en: III-C Reinforcement Learning in NLP
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 强化学习在 NLP 中
- en: 'Although the seq2seq models explained in Section [III-B](#S3.SS2 "III-B Seq2Seq
    Framework ‣ III Core Concepts in NLP ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey") achieve great successes w.r.t. traditional methods,
    there are some issues with how these models are trained. Generally speaking, seq2seq
    models like the ones used in NLP applications face two issues: (1) exposure bias
    and (2) inconsistency between training time and test time measurements [[70](#bib.bib70)].'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管第 [III-B](#S3.SS2 "III-B Seq2Seq Framework ‣ III Core Concepts in NLP ‣ Natural
    Language Processing Advancements By Deep Learning: A Survey") 节中解释的 seq2seq 模型在传统方法上取得了巨大成功，但这些模型的训练仍然存在一些问题。一般而言，用于
    NLP 应用的 seq2seq 模型面临两个问题：（1）暴露偏差和（2）训练时与测试时测量之间的不一致 [[70](#bib.bib70)]。'
- en: 'Most of the popular seq2seq models are minimizing cross-entropy loss as their
    optimization objective via Teacher Forcing (Section [III-B](#S3.SS2 "III-B Seq2Seq
    Framework ‣ III Core Concepts in NLP ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey")). In teacher forcing, during the training of the
    model, the decoder utilizes two inputs, the former decoder output state $s_{t-1}$
    and the ground-truth input $y_{t}$, to determine its current output state $s_{t}$.
    Moreover, it employs them to create the next token, i.e., $\hat{y}_{t}$. However,
    at test time, the decoder fully relies on the previously created token from the
    model distribution. As the ground-truth data is not available, such a step is
    necessary to predict the next action. Henceforth, in training, the decoder input
    is coming from the ground truth, while, in the test phase, it relies on the previous
    prediction.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数流行的seq2seq模型通过教师强迫（第[III-B节](#S3.SS2 "III-B Seq2Seq Framework ‣ III Core
    Concepts in NLP ‣ Natural Language Processing Advancements By Deep Learning: A
    Survey")）将交叉熵损失作为优化目标。在教师强迫中，在模型训练期间，解码器利用两个输入，即前一个解码器输出状态$s_{t-1}$和实际输入$y_{t}$，来确定其当前输出状态$s_{t}$。此外，它们用于生成下一个标记，即$\hat{y}_{t}$。然而，在测试时，解码器完全依赖于模型分布生成的先前标记。由于实际数据不可用，因此需要这样的步骤来预测下一个动作。因此，在训练中，解码器输入来自实际值，而在测试阶段，它依赖于之前的预测。'
- en: This exposure bias [[71](#bib.bib71)] induces error growth through output creation
    at the test phase. One approach to remedy this problem is to remove the ground-truth
    dependency in training by solely relying on model distribution to minimize the
    cross-entropy loss. Scheduled sampling [[64](#bib.bib64)] is one popular method
    to handle this setback. During scheduled sampling, we first pre-train the model
    using cross-entropy loss and then slowly replace the ground-truth with samples
    the model generates.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种曝光偏差[[71](#bib.bib71)]通过测试阶段的输出创建引发了错误增长。解决这个问题的一种方法是通过仅依赖模型分布来最小化交叉熵损失，从而去除训练中的实际值依赖。调度采样[[64](#bib.bib64)]是一种处理这一挫折的流行方法。在调度采样过程中，我们首先使用交叉熵损失对模型进行预训练，然后逐渐用模型生成的样本替换实际值。
- en: The second obstacle with seq2seq models is that, when training is finished using
    the cross-entropy loss, it is typically evaluated using non-differentiable measures
    such as ROUGE or METEOR. This will form an inconsistency between the training
    objective and the test evaluation metric. Recently, it has been demonstrated that
    both of these problems can be tackled by utilizing techniques from reinforcement
    learning [[70](#bib.bib70)].
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个障碍是，当使用交叉熵损失完成训练后，通常使用非可微分的指标如ROUGE或METEOR进行评估。这将导致训练目标与测试评估指标之间的不一致。最近已证明，这两个问题可以通过利用强化学习技术[[70](#bib.bib70)]来解决。
- en: Among most of the well-known models in reinforcement learning, policy gradient
    techniques [[72](#bib.bib72)] such as the REINFORCE algorithm [[73](#bib.bib73)]
    and actor-critic based models such as value-based iteration [[74](#bib.bib74)],
    and Q-learning [[75](#bib.bib75)], are among the most common techniques used in
    deep learning in NLP.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数知名的强化学习模型中，政策梯度技术[[72](#bib.bib72)]如REINFORCE算法[[73](#bib.bib73)]和基于Actor-Critic的模型如基于值的迭代[[74](#bib.bib74)]以及Q学习[[75](#bib.bib75)]，是深度学习在自然语言处理中的常用技术。
- en: 'Using the model predictions (versus the ground-truth) for the sequence to sequence
    modeling and generation, at training time, was initially introduced by Daume et
    al. [[76](#bib.bib76)]. According to their approach, SEARN, the structured prediction
    can be characterized as one of the reinforcement learning cases as follows: The
    model employs its predictions to produce a sequence of actions (words sequences).
    Then, at each time step, a greedy search algorithm is employed to learn the optimal
    action, and the policy will be trained to predict that particular action.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型预测（与实际值相比）进行序列到序列建模和生成，最初由Daume等人[[76](#bib.bib76)]提出。根据他们的方法，SEARN，结构化预测可以被描述为如下的强化学习案例：模型利用其预测生成一系列动作（词序列）。然后，在每个时间步，采用贪婪搜索算法来学习最佳动作，策略将被训练以预测该特定动作。
- en: '![Refer to caption](img/11fd576b81881dd9447c32a947260a4e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/11fd576b81881dd9447c32a947260a4e.png)'
- en: 'Figure 7: A simple Actor-Critic framework.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：一个简单的Actor-Critic框架。
- en: 'In Actor-Critic training, the actor is usually the same neural network used
    to generate the output, while the critic is a regression model that estimates
    how the actor performed on the input data. The actor later receives the feedback
    from the critic and improves its actions. Fig [7](#S3.F7 "Figure 7 ‣ III-C Reinforcement
    Learning in NLP ‣ III Core Concepts in NLP ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey") shows this framework. It is worth noting that action
    in most of the NLP-related applications is like selecting the next output token
    while the state is the decoder output state at each stage of decoding. These models
    have mostly been used for robotic [[77](#bib.bib77)] and Atari games [[78](#bib.bib78)]
    due to the small action space in these applications. However, when we use them
    in NLP applications, they face multiple challenges. The action space in most of
    the NLP applications could be defined as the number of tokens in the vocabulary
    (usually between 50K to 150K tokens). Comparing this to the action space in a
    simple Atari game, which on average has less than 20 actions [[78](#bib.bib78)],
    shows why these Actor-Critic models face difficulties when applied to NLP applications.
    A major challenge is the massive action space in NLP applications, which not only
    causes difficulty for the right action selection, but also will make the training
    process very slow. This makes the process of finding the best Actor-Critic model
    very complicated and model convergence usually requires a lot of tweaks to the
    models.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '在Actor-Critic训练中，actor通常是用于生成输出的神经网络，而critic则是一个回归模型，用于估计actor在输入数据上的表现。actor随后接收critic的反馈并改进其行为。图 [7](#S3.F7
    "Figure 7 ‣ III-C Reinforcement Learning in NLP ‣ III Core Concepts in NLP ‣ Natural
    Language Processing Advancements By Deep Learning: A Survey")展示了这个框架。值得注意的是，在大多数与NLP相关的应用中，行动类似于选择下一个输出标记，而状态是每个解码阶段的解码器输出状态。这些模型主要用于机器人[[77](#bib.bib77)]和Atari游戏[[78](#bib.bib78)]，因为这些应用中的动作空间较小。然而，当我们在NLP应用中使用它们时，会面临多种挑战。大多数NLP应用中的动作空间可以定义为词汇表中的标记数量（通常在50K到150K标记之间）。与平均不到20个动作的简单Atari游戏的动作空间相比[[78](#bib.bib78)]，这说明了为什么这些Actor-Critic模型在应用于NLP应用时会遇到困难。一个主要挑战是NLP应用中的庞大动作空间，这不仅使正确选择动作变得困难，而且使训练过程非常缓慢。这使得找到最佳Actor-Critic模型的过程非常复杂，而且模型的收敛通常需要大量的调整。'
- en: IV Datasets
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 数据集
- en: Many different researchers for different tasks use benchmark datasets, such
    as those discussed below. Benchmarking in machine learning refers to the assessment
    of methods and algorithms, comparing those regarding their capability to learn
    specific patterns. Benchmarking aids validation of a new approach or practice,
    relative to other existing methods.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 许多不同的研究人员用于不同任务的基准数据集，如下文讨论的那些。机器学习中的基准测试是指评估方法和算法，比较它们在学习特定模式方面的能力。基准测试有助于验证新方法或实践，相对于其他现有方法。
- en: Benchmark datasets typically take one of three forms.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基准数据集通常有三种形式。
- en: '1.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The first is real-world data, obtained from various real-world experiments.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一类是真实世界数据，从各种真实世界实验中获得。
- en: '2.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The second is synthetic data, artificially generated to mimic real-world patterns.
    Synthetic data is generated for use instead of real data. Such datasets are of
    special interest in applications where the amount of data required is much larger
    than that which is available, or where privacy considerations are crucial and
    strict, such as in the healthcare domain.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二类是合成数据，人工生成以模拟真实世界的模式。合成数据被生成用于替代真实数据。这类数据集在数据需求远大于可用数据量，或隐私考虑至关重要且严格的应用中尤为重要，例如在医疗领域。
- en: '3.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The third type are toy datasets, used for demonstration and visualization purposes.
    Typically they are artificially generated; often there is no need to represent
    real-world data patterns.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三类是玩具数据集，用于演示和可视化目的。它们通常是人工生成的；通常不需要表示真实世界的数据模式。
- en: The foundation of Deep Learning utilization is the availability of data to teach
    the system about pattern identification. The effectiveness of the model depends
    on the quality of the data. Despite the successful implementation of universal
    language modeling techniques such as BERT [[79](#bib.bib79)], however, such models
    can be used solely for pre-training the models. Afterward, the model needs to
    be trained on the data associated with the desired task. Henceforth, based on
    the everyday demands in different machine domains such as NLP, creating new datasets
    is crucial.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习应用的基础是数据的可用性，以便教系统识别模式。模型的有效性依赖于数据的质量。尽管成功实施了诸如BERT这样的通用语言建模技术[[79](#bib.bib79)]，这些模型仍然只能用于预训练模型。之后，模型需要在与所需任务相关的数据上进行训练。因此，基于不同机器领域的日常需求，创建新数据集是至关重要的。
- en: 'On the other hand, creating new datasets is not usually an easy matter. Informally
    speaking, the newly created dataset should be: the right data to train on, sufficient
    for the evaluation, and accurate to work on. Answering the questions of “what
    is the meaning of right and accurate data” is highly application-based. Basically,
    the data should have sufficient information, which depends on the quality and
    quantity of the data.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，创建新数据集通常不是一件容易的事。非正式地说，新创建的数据集应该是：适合训练的数据、足够用于评估的数据，以及准确的数据。回答“什么是正确和准确的数据”高度依赖于应用。基本上，数据应具有足够的信息，这取决于数据的质量和数量。
- en: To create a dataset, the first step is always asking “what are we trying to
    do and what problem do we need to solve?” and “what kind of data do we need and
    how much of it is required?” The next step is to create training and testing portions.
    The training data set is used to train a model to know how to find the connections
    between the inputs and the associated outputs. The test data set is used to assess
    the intelligence of the machine, i.e., how well the trained model can operate
    on the unseen test samples. Next, we must conduct data preparation to make sure
    the data and its format is simple and understandable for human experts. After
    that, the issue of data accessibility and ownership may arise. Distribution of
    data may need to have specific authorizations, especially if we are dealing with
    sensitive or private data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据集的第一步总是要问“我们想做什么，需解决什么问题？”以及“我们需要什么类型的数据，需要多少数据？”接下来是创建训练集和测试集。训练数据集用于训练模型，了解如何找到输入与相关输出之间的连接。测试数据集用于评估机器的智能，即训练好的模型在未见过的测试样本上表现如何。接下来，我们必须进行数据准备，以确保数据及其格式对人类专家来说简单易懂。之后，数据访问权限和所有权的问题可能会出现。数据分发可能需要特定的授权，特别是当我们处理敏感或私密数据时。
- en: 'Given the aforementioned roadmap, creating proper datasets is complicated and
    of great importance. That’s why few datasets are frequently chosen by the researchers
    and developers for benchmarking. A summary of widely used benchmark datasets is
    provided in Table [I](#S4.T1 "TABLE I ‣ IV Datasets ‣ Natural Language Processing
    Advancements By Deep Learning: A Survey").'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '鉴于上述路线图，创建合适的数据集是复杂且非常重要的。这就是为什么研究人员和开发人员经常选择少数数据集进行基准测试。表[I](#S4.T1 "TABLE
    I ‣ IV Datasets ‣ Natural Language Processing Advancements By Deep Learning: A
    Survey")提供了广泛使用的基准数据集的总结。'
- en: 'TABLE I: Benchmark datasets.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：基准数据集。
- en: '| Task | Dataset | Link |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 链接 |'
- en: '| --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Machine Translation |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 机器翻译 |'
- en: '&#124; WMT 2014 EN-DE &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WMT 2014 英德 &#124;'
- en: '&#124; WMT 2014 EN-FR &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WMT 2014 英法 &#124;'
- en: '| [http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/](http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/)
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| [http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/](http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/)
    |'
- en: '| Text Summarization |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 文本总结 |'
- en: '&#124; CNN/DM &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN/DM &#124;'
- en: '&#124; Newsroom &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 新闻编辑室 &#124;'
- en: '&#124; DUC &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DUC &#124;'
- en: '&#124; Gigaword &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Gigaword &#124;'
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [https://cs.nyu.edu/~kcho/DMQA/](https://cs.nyu.edu/~kcho/DMQA/) &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://cs.nyu.edu/~kcho/DMQA/](https://cs.nyu.edu/~kcho/DMQA/) &#124;'
- en: '&#124; [https://summari.es/](https://summari.es/) &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://summari.es/](https://summari.es/) &#124;'
- en: '&#124; [https://www-nlpir.nist.gov/projects/duc/data.html](https://www-nlpir.nist.gov/projects/duc/data.html)
    &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://www-nlpir.nist.gov/projects/duc/data.html](https://www-nlpir.nist.gov/projects/duc/data.html)
    &#124;'
- en: '&#124; [https://catalog.ldc.upenn.edu/LDC2012T21](https://catalog.ldc.upenn.edu/LDC2012T21)
    &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://catalog.ldc.upenn.edu/LDC2012T21](https://catalog.ldc.upenn.edu/LDC2012T21)
    &#124;'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Reading Comprehension &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 阅读理解 &#124;'
- en: '&#124; Question Answering &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 问答系统 &#124;'
- en: '&#124; Question Generation &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 问题生成 &#124;'
- en: '|'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ARC &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ARC &#124;'
- en: '&#124; CliCR &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CliCR &#124;'
- en: '&#124; CNN/DM &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN/DM &#124;'
- en: '&#124; NewsQA &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NewsQA &#124;'
- en: '&#124; RACE &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RACE &#124;'
- en: '&#124; SQuAD &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SQuAD &#124;'
- en: '&#124; Story Cloze Test &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Story Cloze Test &#124;'
- en: '&#124; NarativeQA &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NarativeQA &#124;'
- en: '&#124; Quasar &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Quasar &#124;'
- en: '&#124; SearchQA &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SearchQA &#124;'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [http://data.allenai.org/arc/](http://data.allenai.org/arc/) &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [http://data.allenai.org/arc/](http://data.allenai.org/arc/) &#124;'
- en: '&#124; [http://aclweb.org/anthology/N18-1140](http://aclweb.org/anthology/N18-1140)
    &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [http://aclweb.org/anthology/N18-1140](http://aclweb.org/anthology/N18-1140)
    &#124;'
- en: '&#124; [https://cs.nyu.edu/~kcho/DMQA/](https://cs.nyu.edu/~kcho/DMQA/) &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://cs.nyu.edu/~kcho/DMQA/](https://cs.nyu.edu/~kcho/DMQA/) &#124;'
- en: '&#124; [https://datasets.maluuba.com/NewsQA](https://datasets.maluuba.com/NewsQA)
    &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://datasets.maluuba.com/NewsQA](https://datasets.maluuba.com/NewsQA)
    &#124;'
- en: '&#124; [http://www.qizhexie.com/data/RACE_leaderboard](http://www.qizhexie.com/data/RACE_leaderboard)
    &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [http://www.qizhexie.com/data/RACE_leaderboard](http://www.qizhexie.com/data/RACE_leaderboard)
    &#124;'
- en: '&#124; [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)
    &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)
    &#124;'
- en: '&#124; [http://aclweb.org/anthology/W17-0906.pdf](http://aclweb.org/anthology/W17-0906.pdf)
    &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [http://aclweb.org/anthology/W17-0906.pdf](http://aclweb.org/anthology/W17-0906.pdf)
    &#124;'
- en: '&#124; [https://github.com/deepmind/narrativeqa](https://github.com/deepmind/narrativeqa)
    &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://github.com/deepmind/narrativeqa](https://github.com/deepmind/narrativeqa)
    &#124;'
- en: '&#124; [https://github.com/bdhingra/quasar](https://github.com/bdhingra/quasar)
    &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://github.com/bdhingra/quasar](https://github.com/bdhingra/quasar)
    &#124;'
- en: '&#124; [https://github.com/nyu-dl/SearchQA](https://github.com/nyu-dl/SearchQA)
    &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://github.com/nyu-dl/SearchQA](https://github.com/nyu-dl/SearchQA)
    &#124;'
- en: '|'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Semantic Parsing |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 语义解析 |'
- en: '&#124; AMR parsing &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AMR 解析 &#124;'
- en: '&#124; ATIS (SQL Parsing) &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ATIS（SQL 解析） &#124;'
- en: '&#124; WikiSQL (SQL Parsing) &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WikiSQL（SQL 解析） &#124;'
- en: '|'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [https://amr.isi.edu/index.html](https://amr.isi.edu/index.html) &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://amr.isi.edu/index.html](https://amr.isi.edu/index.html) &#124;'
- en: '&#124; [https://github.com/jkkummerfeld/text2sql-data/tree/master/data](https://github.com/jkkummerfeld/text2sql-data/tree/master/data)
    &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://github.com/jkkummerfeld/text2sql-data/tree/master/data](https://github.com/jkkummerfeld/text2sql-data/tree/master/data)
    &#124;'
- en: '&#124; [https://github.com/salesforce/WikiSQL](https://github.com/salesforce/WikiSQL)
    &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://github.com/salesforce/WikiSQL](https://github.com/salesforce/WikiSQL)
    &#124;'
- en: '|'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sentiment Analysis |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 |'
- en: '&#124; IMDB Reviews &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; IMDB 评论 &#124;'
- en: '&#124; SST &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SST &#124;'
- en: '&#124; Yelp Reviews &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Yelp 评论 &#124;'
- en: '&#124; Subjectivity Dataset &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 主观性数据集 &#124;'
- en: '|'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    &#124;'
- en: '&#124; [https://nlp.stanford.edu/sentiment/index.html](https://nlp.stanford.edu/sentiment/index.html)
    &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://nlp.stanford.edu/sentiment/index.html](https://nlp.stanford.edu/sentiment/index.html)
    &#124;'
- en: '&#124; [https://www.yelp.com/dataset/challenge](https://www.yelp.com/dataset/challenge)
    &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://www.yelp.com/dataset/challenge](https://www.yelp.com/dataset/challenge)
    &#124;'
- en: '&#124; [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)
    &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)
    &#124;'
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Text Classification |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 文本分类 |'
- en: '&#124; AG News &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AG News &#124;'
- en: '&#124; DBpedia &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DBpedia &#124;'
- en: '&#124; TREC &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TREC &#124;'
- en: '&#124; 20 NewsGroup &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 20 NewsGroup &#124;'
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)
    &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)
    &#124;'
- en: '&#124; [https://wiki.dbpedia.org/Datasets](https://wiki.dbpedia.org/Datasets)
    &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://wiki.dbpedia.org/Datasets](https://wiki.dbpedia.org/Datasets)
    &#124;'
- en: '&#124; [https://trec.nist.gov/data.html](https://trec.nist.gov/data.html) &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://trec.nist.gov/data.html](https://trec.nist.gov/data.html) &#124;'
- en: '&#124; [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)
    &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)
    &#124;'
- en: '|'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Natural Language Inference |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言推断 |'
- en: '&#124; SNLI Corpus &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SNLI 语料库 &#124;'
- en: '&#124; MultiNLI &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MultiNLI &#124;'
- en: '&#124; SciTail &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SciTail &#124;'
- en: '|'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [https://nlp.stanford.edu/projects/snli/](https://nlp.stanford.edu/projects/snli/)
    &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://nlp.stanford.edu/projects/snli/](https://nlp.stanford.edu/projects/snli/)
    &#124;'
- en: '&#124; [https://www.nyu.edu/projects/bowman/multinli/](https://www.nyu.edu/projects/bowman/multinli/)
    &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://www.nyu.edu/projects/bowman/multinli/](https://www.nyu.edu/projects/bowman/multinli/)
    &#124;'
- en: '&#124; [http://data.allenai.org/scitail/](http://data.allenai.org/scitail/)
    &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [http://data.allenai.org/scitail/](http://data.allenai.org/scitail/)
    &#124;'
- en: '|'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Semantic Role Labeling |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 语义角色标注 |'
- en: '&#124; Proposition Bank &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Proposition Bank &#124;'
- en: '&#124; OneNotes &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; OneNotes &#124;'
- en: '|'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [http://propbank.github.io/](http://propbank.github.io/) &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [http://propbank.github.io/](http://propbank.github.io/) &#124;'
- en: '&#124; [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)
    &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)
    &#124;'
- en: '|'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: V Deep Learning for NLP Tasks
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 深度学习用于 NLP 任务
- en: 'This section describes NLP applications using deep learning. Fig. [8](#S5.F8
    "Figure 8 ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey") shows representative NLP tasks (and the categories
    they belong to). A fundamental question is: ”How can we evaluate an NLP algorithm,
    model, or system?” In [[80](#bib.bib80)], some of the most common evaluation metrics
    have been described. This reference explains the fundamental principles of evaluating
    NLP systems.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '本节描述了使用深度学习的 NLP 应用。图 [8](#S5.F8 "Figure 8 ‣ V Deep Learning for NLP Tasks
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey") 展示了代表性的
    NLP 任务（以及它们所属的类别）。一个基本的问题是：“我们如何评估 NLP 算法、模型或系统？”在 [[80](#bib.bib80)] 中，描述了一些最常见的评估指标。该参考文献解释了评估
    NLP 系统的基本原理。'
- en: '![Refer to caption](img/17b74fc0b275d716ed5fb6269c20523f.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17b74fc0b275d716ed5fb6269c20523f.png)'
- en: 'Figure 8: NLP tasks investigated in this study.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：本研究中调查的 NLP 任务。
- en: V-A Basic Tasks
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 基本任务
- en: V-A1 Part-Of-Speech Tagging
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 词性标注
- en: Part-of-Speech tagging is one of the basic tasks in Natural Language Processing.
    It is the process of labeling words with their part of speech categories. Part
    of speech is leveraged for many crucial tasks such as named entity recognition.
    One commonly used dataset for Part-of-Speech tagging is the WSJ corpus⁷⁷7Penn
    Treebank Wall Street Journal (WSJ-PTB).. This dataset contains over a million
    tokens and has been utilized widely as a benchmark dataset for the performance
    assessment of POS tagging systems. Traditional methods are still performing very
    well for this task [[16](#bib.bib16)]. However, neural network based methods have
    been proposed for Part-of-Speech tagging [[81](#bib.bib81)].
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注是自然语言处理中的基本任务之一。它是将单词标记为其词性类别的过程。词性在许多关键任务中被利用，例如命名实体识别。一个常用的词性标注数据集是 WSJ
    语料库⁷⁷7Penn Treebank Wall Street Journal (WSJ-PTB)。该数据集包含超过一百万个标记，并被广泛用作评估 POS
    标注系统性能的基准数据集。传统方法在此任务中仍表现优异 [[16](#bib.bib16)]。然而，基于神经网络的方法已被提出用于词性标注 [[81](#bib.bib81)]。
- en: 'For example, the deep neural network architecture named CharWNN has been developed
    to join word-level and character-level representations using convolutional neural
    networks for POS tagging [[14](#bib.bib14)]. The emphasis in [[14](#bib.bib14)]
    is the importance of character-level feature extraction as their experimental
    results show the necessity of employing hand-crafted features in the absence of
    character-level features for achieving the state-of-the-art. In [[82](#bib.bib82)],
    a wide variety of neural network based models have been proposed for sequence
    tagging tasks, e.g., LSTM networks, bidirectional LSTM networks, LSTM networks
    with a CRF⁸⁸8Conditional Random Field. layer, etc. Sequence tagging itself includes
    part of speech tagging, chunking, and named entity recognition. Likewise, a globally
    normalized transition-based neural network architecture has been proposed for
    POS-tagging [[83](#bib.bib83)]. State-of-the-art results are summarized in Table
     [II](#S5.T2 "TABLE II ‣ V-A1 Part-Of-Speech Tagging ‣ V-A Basic Tasks ‣ V Deep
    Learning for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey"). In [[17](#bib.bib17)], authors propose a bidirectional LSTM to perform
    parts of speech tagging and show that it performs better than conventional machine
    learning techniques on the same dataset. More recently, in [[84](#bib.bib84)],
    authors use a pretrained BERT model in combination with one bidirectional LSTM
    layer and train the latter layer only and outperform the prior state-of-the art
    POS architectures.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，深度神经网络架构CharWNN已被开发用于结合词级和字符级表示，使用卷积神经网络进行词性标注[[14](#bib.bib14)]。在[[14](#bib.bib14)]中强调了字符级特征提取的重要性，因为其实验结果表明，在缺少字符级特征的情况下，采用手工制作的特征是实现最先进技术的必要条件。在[[82](#bib.bib82)]中，提出了多种基于神经网络的序列标注模型，例如，LSTM网络、双向LSTM网络、带有CRF层的LSTM网络等。序列标注本身包括词性标注、分块和命名实体识别。同样，为词性标注提出了一种全球归一化的过渡基础神经网络架构[[83](#bib.bib83)]。最先进的结果总结在表 [II](#S5.T2
    "TABLE II ‣ V-A1 Part-Of-Speech Tagging ‣ V-A Basic Tasks ‣ V Deep Learning for
    NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning: A Survey")。在[[17](#bib.bib17)]中，作者提出了一种双向LSTM用于执行词性标注，并表明其在同一数据集上表现优于传统的机器学习技术。最近，在[[84](#bib.bib84)]中，作者结合一个预训练的BERT模型和一个双向LSTM层，仅训练后者层，超越了之前的最先进词性标注架构。'
- en: 'TABLE II: POS tagging state-of-the-art models evaluated on the WSJ-PTB dataset.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：在WSJ-PTB数据集上评估的词性标注最先进模型。
- en: '| Model | Accuracy |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 准确率 |'
- en: '| --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Character-aware neural language models [[85](#bib.bib85)] | 97.53 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 基于字符的神经语言模型 [[85](#bib.bib85)] | 97.53 |'
- en: '| Transfer Learning + GRU[[86](#bib.bib86)] | 97.55 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 迁移学习 + GRU[[86](#bib.bib86)] | 97.55 |'
- en: '| Bi-directional LSTM + CNNs + CRF[[87](#bib.bib87)] | 97.55 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 双向LSTM + CNNs + CRF[[87](#bib.bib87)] | 97.55 |'
- en: '| Adversarial Training + Bi-LSTM [[88](#bib.bib88)] | 97.59 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 对抗训练 + Bi-LSTM [[88](#bib.bib88)] | 97.59 |'
- en: '| Character Composition + Bi-LSTM[[89](#bib.bib89)] | 97.78 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 字符组成 + Bi-LSTM[[89](#bib.bib89)] | 97.78 |'
- en: '| String Embedding + LSTM[[90](#bib.bib90)] | 97.85 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 字符嵌入 + LSTM[[90](#bib.bib90)] | 97.85 |'
- en: '| Meta-BiLSTM [[91](#bib.bib91)] | 97.96 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Meta-BiLSTM [[91](#bib.bib91)] | 97.96 |'
- en: V-A2 Parsing
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 解析
- en: 'Parsing is assigning a structure to a recognized string. There are different
    types of parsing. Constituency Parsing refers in particular to assigning a syntactic
    structure to a sentence. A greedy parser has been introduced in [[92](#bib.bib92)]
    which performs a syntactic and semantic summary of content using vector representations.
    To enhance the results achieved by [[92](#bib.bib92)], the approach proposed in
    [[93](#bib.bib93)] focuses on learning morphological embeddings. Recently, deep
    neural network models outperformed traditional algorithms. State-of-the-art results
    are summarized in Table [III](#S5.T3 "TABLE III ‣ V-A2 Parsing ‣ V-A Basic Tasks
    ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements By
    Deep Learning: A Survey").'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '解析是将结构分配给识别出的字符串。有不同类型的解析。句法解析特别指的是将句子分配给一个句法结构。[[92](#bib.bib92)]中介绍了一种贪婪解析器，它使用向量表示对内容进行句法和语义摘要。为了增强[[92](#bib.bib92)]所取得的结果，[[93](#bib.bib93)]中提出的方法专注于学习形态学嵌入。最近，深度神经网络模型超越了传统算法。最先进的结果总结在表 [III](#S5.T3
    "TABLE III ‣ V-A2 Parsing ‣ V-A Basic Tasks ‣ V Deep Learning for NLP Tasks ‣
    Natural Language Processing Advancements By Deep Learning: A Survey")。'
- en: 'TABLE III: Constituency parsing state-of-the-art models evaluated on the WSJ-PTB
    dataset.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：在WSJ-PTB数据集上评估的句法解析最先进模型。
- en: '| Model | Accuracy |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 准确率 |'
- en: '| --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Recurrent neural network grammars (RNNG) [[94](#bib.bib94)] | 93.6 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 循环神经网络语法（RNNG）[[94](#bib.bib94)] | 93.6 |'
- en: '| In-order traversal over syntactic trees + LSTM [[95](#bib.bib95)] | 94.2
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 语法树的中序遍历 + LSTM[[95](#bib.bib95)] | 94.2 |'
- en: '| Model Combination and Reranking [[96](#bib.bib96)] | 94.6 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 模型组合和重排序[[96](#bib.bib96)] | 94.6 |'
- en: '| Self-Attentive Encoder [[97](#bib.bib97)] | 95.1 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 自注意力编码器[[97](#bib.bib97)] | 95.1 |'
- en: Another type of parsing is called Dependency Parsing. Dependency structure shows
    the structural relationships between the words in a targeted sentence. In dependency
    parsing, phrasal elements and phrase-structure rules do not contribute to the
    process. Rather, the syntactic structure of the sentence is expressed only in
    terms of the words in the sentence and the associated relations between the words.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解析类型称为依赖解析。依赖结构展示了目标句子中单词之间的结构关系。在依赖解析中，短语元素和短语结构规则不会对过程产生贡献。相反，句子的句法结构仅通过句子中的单词及其相互关系来表达。
- en: Neural networks have shown their superiority regarding generalizability and
    reducing the feature computation cost. In [[98](#bib.bib98)], a novel neural network-based
    approach was proposed for a transition-based dependency parser. Neural network
    based models that operate on task-specific transition systems have also been utilized
    for dependency parsing [[83](#bib.bib83)]. A regularized parser with bi-affine
    classifiers has been proposed for the prediction of arcs and labels [[99](#bib.bib99)].
    Bidirectional-LSTMs have been used in dependency parsers for feature representation [[100](#bib.bib100)].
    A new control structure has been introduced for sequence-to-sequence neural networks
    based on the stack LSTM and has been used in transition-based parsing [[101](#bib.bib101)].
    [[102](#bib.bib102)] presents a transition based multilingual dependency parser
    which uses a bidirectional LSTM to adapt to target languages. In [[103](#bib.bib103)],
    the authors provide a comparison on the state of the art deep learning based parsing
    methods on a clinical text parsing task. More recently, in [[104](#bib.bib104)],
    a second-order TreeCRF extension was added to the biaffine [[105](#bib.bib105)]
    parser to demonstrate that structural learning can further improve parsing performance
    over the state-of-the-art bi-affine models.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在通用性和减少特征计算成本方面显示了其优越性。在[[98](#bib.bib98)]中，提出了一种基于神经网络的新型过渡依赖解析器方法。基于神经网络的模型也被用于任务特定的过渡系统进行依赖解析[[83](#bib.bib83)]。提出了一种带有双仿射分类器的正则化解析器用于弧和标签的预测[[99](#bib.bib99)]。双向LSTM被用于依赖解析器中的特征表示[[100](#bib.bib100)]。为基于堆栈LSTM的序列到序列神经网络引入了一种新的控制结构，并用于过渡解析[[101](#bib.bib101)]。[[102](#bib.bib102)]介绍了一种基于过渡的多语言依赖解析器，该解析器使用双向LSTM以适应目标语言。在[[103](#bib.bib103)]中，作者对临床文本解析任务中的最先进深度学习解析方法进行了比较。最近，在[[104](#bib.bib104)]中，向双仿射解析器[[105](#bib.bib105)]中添加了二阶TreeCRF扩展，以证明结构学习可以进一步提升解析性能，超过了最先进的双仿射模型。
- en: V-A3 Semantic Role Labeling
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A3 语义角色标注
- en: Semantic Role Labeling (SRL) is the process of identification and classification
    of text arguments. It is aimed at the characterization of elements to determine
    “who” did “what” to “whom” as well as “how,” “where,” and “when.” It identifies
    the predicate-argument structure of a sentence. The predicate, in essence, refers
    to “what,” while the arguments consist of the associated participants and properties
    in the text. The goal of SRL is to extract the semantic relations between the
    predicate and the related arguments.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 语义角色标注（SRL）是识别和分类文本论元的过程。其目的是对元素进行特征化，以确定“谁”对“谁”做了“什么”，以及“如何”、“在哪里”和“何时”。它识别句子的谓词-论元结构。谓词本质上指的是“什么”，而论元则由文本中的相关参与者和属性组成。SRL
    的目标是提取谓词与相关论元之间的语义关系。
- en: 'Most of the previously-reported research efforts are based on explicit representations
    of semantic roles. Recently, deep learning approaches have achieved the SRL state-of-the-art
    without taking the explicit syntax representation into consideration [[106](#bib.bib106)].
    On the other hand, it is argued that the utilization of syntactic information
    can be leveraged to improve the performance of syntactic-agnostic⁹⁹9Note that
    being syntactic-agnostic does not imply discarding syntactic information. It means
    they are not explicitly employed. models [[107](#bib.bib107)]. A linguistically-informed
    self-attention (LISA) model has been proposed to leverage both multi-task learning
    and self-attention for effective utilization of the syntactic information for
    SRL [[108](#bib.bib108)]. Current state-of-the-art methods employ joint prediction
    of predicates and arguments [[109](#bib.bib109)], novel word representation approaches [[110](#bib.bib110)],
    and self-attention models [[111](#bib.bib111)]; see Table [IV](#S5.T4 "TABLE IV
    ‣ V-A3 Semantic Role Labeling ‣ V-A Basic Tasks ‣ V Deep Learning for NLP Tasks
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey").'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '之前大多数研究工作基于语义角色的显式表示。最近，深度学习方法在没有考虑显式语法表示的情况下实现了SRL的最先进水平[[106](#bib.bib106)]。另一方面，有人认为利用句法信息可以提高语法无关模型的性能[[107](#bib.bib107)]。需要注意的是，语法无关并不意味着丢弃语法信息，而是它们没有被显式使用。提出了一种语言学信息自注意（LISA）模型，利用多任务学习和自注意机制有效利用句法信息进行SRL[[108](#bib.bib108)]。当前的最先进方法采用谓词和论元的联合预测[[109](#bib.bib109)]、新型词表示方法[[110](#bib.bib110)]和自注意模型[[111](#bib.bib111)]；见表[IV](#S5.T4
    "TABLE IV ‣ V-A3 Semantic Role Labeling ‣ V-A Basic Tasks ‣ V Deep Learning for
    NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning: A Survey")。'
- en: Researchers in [[25](#bib.bib25)] focus on syntax and contextualized word representation
    to present a unique multilingual SRL model based on a biaffine scorer, argument
    pruning and bidirectional LSTMs, (see also [[112](#bib.bib112)]).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[[25](#bib.bib25)]的研究人员专注于句法和上下文化词表示，提出了一种基于双仿射评分器、论元修剪和双向LSTM的独特多语言SRL模型（另见[[112](#bib.bib112)]）。'
- en: 'TABLE IV: Semantic Role Labeling current state-of-the-art models evaluated
    on the OntoNotes dataset [[113](#bib.bib113)]. The accuracy metric is $F_{1}$
    score.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：在OntoNotes数据集上评估的语义角色标注当前最先进模型[[113](#bib.bib113)]。准确性指标是$F_{1}$得分。
- en: '| Model | Accuracy ($F_{1}$) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 准确性($F_{1}$) |'
- en: '| --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Self-Attention + RNN [[111](#bib.bib111)] | 83.9 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 自注意 + RNN[[111](#bib.bib111)] | 83.9 |'
- en: '| Contextualized Word Representations [[110](#bib.bib110)] | 84.6 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 上下文化词表示[[110](#bib.bib110)] | 84.6 |'
- en: '| Argumented Representations + BiLSTM [[109](#bib.bib109)] | 85.3 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 增强表示 + BiLSTM[[109](#bib.bib109)] | 85.3 |'
- en: V-B Text Classification
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 文本分类
- en: The primary objective of text classification is to assign predefined categories
    to text parts (which could be a word, sentence, or whole document) for preliminary
    classification purposes and further organization and analysis. A simple example
    is the categorization of given documents as to political or non-political news
    articles.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类的主要目标是将预定义的类别分配给文本部分（可以是单词、句子或整个文档），以便进行初步分类以及进一步的组织和分析。一个简单的例子是将给定文档分类为政治类或非政治类新闻文章。
- en: The use of CNNs for sentence classification, in which training the model on
    top of pretrained word-vectors through fine-tuning, has resulted in considerable
    improvements in learning task-specific vectors [[31](#bib.bib31)]. Later, a Dynamic
    Convolutional Neural Network (DCNN) architecture – essentially a CNN with a dynamic
    k-max pooling method – was applied to capture the semantic modeling of sentences [[114](#bib.bib114)].
    In addition to CNNs, RNNs have been used for text classification. An LSTM-RNN
    architecture has been utilized in [[115](#bib.bib115)] for sentence embedding
    with particular superiority in a defined web search task. A Hierarchical Attention
    Network (HAN) has been utilized to capture the hierarchical structure of text,
    with a word-level and sentence-level attention mechanism [[116](#bib.bib116)].
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CNN进行句子分类，其中在预训练的词向量上通过微调来训练模型，已经在学习任务特定向量方面取得了显著的改进[[31](#bib.bib31)]。后来，动态卷积神经网络（DCNN）架构——本质上是一个带有动态k-max池化方法的CNN——被应用于捕捉句子的语义建模[[114](#bib.bib114)]。除了CNN，RNN也被用于文本分类。在[[115](#bib.bib115)]中使用了LSTM-RNN架构进行句子嵌入，在特定的网页搜索任务中具有明显优势。层次注意网络（HAN）被用于捕捉文本的层次结构，具有词级和句级注意机制[[116](#bib.bib116)]。
- en: 'Some models used the combination of both RNNs and CNNs for text classification
    such as [[117](#bib.bib117)]. This is a recurrent architecture in addition to
    max-pooling with an effective word representation method, and demonstrates superiority
    compared to simple window-based neural network approaches. Another unified architecture
    is the C-LSTM proposed in [[118](#bib.bib118)] for sentence and document modeling
    in classification. Current state-of-the-art methods are summarized in Table [V](#S5.T5
    "TABLE V ‣ V-B Text Classification ‣ V Deep Learning for NLP Tasks ‣ Natural Language
    Processing Advancements By Deep Learning: A Survey"). A more recent review of
    the deep learning based methods for text classification is provided in [[119](#bib.bib119)].
    The latter focuses on different architectures used for this task, including most
    recent works in CNN based models, as well as RNN based models, and graph neural
    networks. In [[120](#bib.bib120)], authors provide a comparison between various
    deep learning methods for text classification, concluding that GRUs and LSTMs
    can actually perform better than CNN-based models.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '一些模型使用了RNN和CNN的组合进行文本分类，例如[[117](#bib.bib117)]。这是一种递归架构，除了最大池化外，还采用了有效的词表示方法，并展示了相比于简单的基于窗口的神经网络方法的优越性。另一种统一的架构是[[118](#bib.bib118)]中提出的C-LSTM，用于句子和文档建模中的分类。目前最先进的方法总结在表 [V](#S5.T5
    "TABLE V ‣ V-B Text Classification ‣ V Deep Learning for NLP Tasks ‣ Natural Language
    Processing Advancements By Deep Learning: A Survey")。[[119](#bib.bib119)]提供了对基于深度学习的文本分类方法的最新综述。后者重点关注了用于此任务的不同架构，包括最近的CNN模型、RNN模型以及图神经网络。在[[120](#bib.bib120)]中，作者对各种深度学习文本分类方法进行了比较，得出结论GRUs和LSTMs实际上可能比基于CNN的模型表现更好。'
- en: 'TABLE V: The classification accuracy of state-of-the-art methods, evaluated
    on the AG News Corpus dataset [[2](#bib.bib2)].'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：在AG News Corpus数据集 [[2](#bib.bib2)]上评估的最先进方法的分类准确率。
- en: '| Model | Accuracy |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 准确率 |'
- en: '| --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| CNN [[121](#bib.bib121)] | 91.33 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| CNN [[121](#bib.bib121)] | 91.33 |'
- en: '| Deep Pyramid CNN [[122](#bib.bib122)] | 93.13 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 深度金字塔CNN [[122](#bib.bib122)] | 93.13 |'
- en: '| CNN [[123](#bib.bib123)] | 93.43 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| CNN [[123](#bib.bib123)] | 93.43 |'
- en: '| Universal Language Model Fine-tuning (ULMFiT) [[124](#bib.bib124)] | 94.99
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 通用语言模型微调 (ULMFiT) [[124](#bib.bib124)] | 94.99 |'
- en: V-C Information Extraction
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 信息提取
- en: Information extraction identifies structured information from “unstructured”
    data such as social media posts and online news. Deep learning has been utilized
    for information extraction regarding subtasks such as Named Entity Recognition,
    Relation Extraction, Coreference Resolution, and Event Extraction.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 信息提取从“非结构化”数据中识别结构化信息，例如社交媒体帖子和在线新闻。深度学习已被用于信息提取中的子任务，如命名实体识别、关系提取、共指消解和事件提取。
- en: V-C1 Named Entity Recognition
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C1 命名实体识别
- en: 'Named Entity Recognition (NER) aims to locate and categorize named entities
    in context into pre-defined categories such as the names of people and places.
    The application of deep neural networks in NER has been investigated by the employment
    of CNN [[125](#bib.bib125)] and RNN architectures [[126](#bib.bib126)], as well
    as hybrid bidirectional LSTM and CNN architectures [[19](#bib.bib19)]. NeuroNER [[127](#bib.bib127)],
    a named-entity recognition tool, operates based on artificial neural networks.
    State-of-the-art models are reported in Table [VI](#S5.T6 "TABLE VI ‣ V-C1 Named
    Entity Recognition ‣ V-C Information Extraction ‣ V Deep Learning for NLP Tasks
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey"). [[21](#bib.bib21)]
    provides an extensive discussion on recent deep learning methods for named entity
    recognition. The latter concludes that the work presented in [[128](#bib.bib128)]
    outperforms other recent models (with an F-score of 93.5 on the CoNLL03 dataset).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '命名实体识别（NER）旨在将上下文中的命名实体定位并分类到预定义的类别中，例如人名和地名。深度神经网络在NER中的应用已经通过使用CNN [[125](#bib.bib125)]和RNN架构 [[126](#bib.bib126)]，以及混合的双向LSTM和CNN架构 [[19](#bib.bib19)]进行了研究。NeuroNER [[127](#bib.bib127)]，一种基于人工神经网络的命名实体识别工具，提供了最先进的模型，见表 [VI](#S5.T6
    "TABLE VI ‣ V-C1 Named Entity Recognition ‣ V-C Information Extraction ‣ V Deep
    Learning for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey")。[[21](#bib.bib21)]对命名实体识别的最新深度学习方法进行了广泛讨论。后者得出结论，在 [[128](#bib.bib128)]中提出的工作超越了其他近期模型（在CoNLL03数据集上F-score为93.5）。'
- en: 'TABLE VI: State of the art models regarding Name Entity Recognition. Evaluation
    is performed on the CoNLL-2003 Shared Task dataset [[129](#bib.bib129)]. The evaluation
    metric is $F_{1}$ score.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：关于命名实体识别的*最先进的*模型。评估是在CoNLL-2003共享任务数据集[[129](#bib.bib129)]上进行的。评估指标是$F_{1}$分数。
- en: '| Model | Accuracy |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 准确率 |'
- en: '| --- | --- |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Semi-supervised Sequence Modeling [[130](#bib.bib130)] | 92.61 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 半监督序列建模 [[130](#bib.bib130)] | 92.61 |'
- en: '| Google BERT [[131](#bib.bib131)] | 92.8 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Google BERT [[131](#bib.bib131)] | 92.8 |'
- en: '| Contextual String Embeddings [[90](#bib.bib90)] | 93.09 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 上下文字符串嵌入 [[90](#bib.bib90)] | 93.09 |'
- en: V-C2 Relation Extraction
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C2 关系提取
- en: Relation Extraction aims to find the semantic relationships between entity pairs. The
    recursive neural network (RNN) model has been proposed for semantic relationship
    classification by learning compositional vector representations [[132](#bib.bib132)].
    For relation classification, CNN architectures have been employed as well, by
    extracting lexical and sentence level features [[37](#bib.bib37)]. More recently,
    in [[133](#bib.bib133)], bidirectional tree-structured LSTMs were shown to perform
    well for relation extraction. [[134](#bib.bib134)] provides a more recent review
    on relation extraction.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 关系提取旨在找到实体对之间的语义关系。递归神经网络（RNN）模型被提出用于通过学习组合向量表示来进行语义关系分类[[132](#bib.bib132)]。在关系分类中，也采用了CNN架构，通过提取词汇和句子级别特征[[37](#bib.bib37)]。最近，在[[133](#bib.bib133)]中，双向树结构LSTM显示出在关系提取方面的良好表现。[[134](#bib.bib134)]
    提供了关于关系提取的*最新*综述。
- en: V-C3 Coreference Resolution
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C3 共指消解
- en: Coreference resolution includes identification of the mentions in a context
    that refer to the same entity. For instance, the mentions “car,” “Camry,” and
    “it” could all refer to the same entity. For the first time in [[135](#bib.bib135)],
    Reinforcement Learning (RL) was applied to coreference resolution. Current widely
    used methods leverage an attention mechanism [[136](#bib.bib136)]. More recently,
    in [[137](#bib.bib137)], authors adopt a reinforcement learning policy gradient
    approach to coreference resolution and provide state-of-the art performance on
    the English OntoNotes v5.0 benchmark task. [[138](#bib.bib138)] reformulates coreference
    resolution as a span prediction task as in question answering and provide superior
    performance on the CoNLL-2012 benchmark task.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 共指消解包括识别上下文中指向相同实体的提及。例如，提及“车”，“Camry”和“它”都可能指代同一个实体。在[[135](#bib.bib135)]中，首次将强化学习（RL）应用于共指消解。当前广泛使用的方法利用了注意力机制[[136](#bib.bib136)]。最近，在[[137](#bib.bib137)]中，作者采用了强化学习策略梯度方法进行共指消解，并在英语OntoNotes
    v5.0基准任务中提供了*最先进的*性能。[[138](#bib.bib138)] 将共指消解重新表述为类似于问答的跨度预测任务，并在CoNLL-2012基准任务中提供了*卓越的*表现。
- en: V-C4 Event Extraction
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C4 事件提取
- en: A specific type of extracted information from text is an event. Such extraction
    may involve recognizing trigger words related to an event and assigning labels
    to entity mentions that represent event triggers. Convolutional neural networks
    have been utilized for event detection; they handle problems with feature-based
    approaches including exhaustive feature engineering and error propagation phenomena
    for feature generation [[139](#bib.bib139)]. In 2018, Nguyen and Grishman applied
    graph-CNN (GCCN) where the convolutional operations are applied to syntactically
    dependent words as well as consecutive words [[140](#bib.bib140)]; their adding
    entity information reflected the state-of-the-art using CNN models. [[141](#bib.bib141)]
    uses a novel inverse reinforcement learning approach based on generative adversarial
    networks (imitation learning) to tackle joint entity and event extraction. More
    recently, in [[142](#bib.bib142)], authors proposed a model for document-level
    event extraction using a combined dependency-based GCN (for local context) and
    a hypergraph (as an aggregator for global context).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中提取的一种特定类型的信息是事件。这种提取可能涉及识别与事件相关的触发词，并为表示事件触发器的实体提及分配标签。卷积神经网络被用于事件检测；它们处理特征基础的方法中的问题，包括详尽的特征工程和特征生成中的错误传播现象[[139](#bib.bib139)]。在2018年，Nguyen和Grishman应用了图卷积网络（GCCN），其中卷积操作应用于句法依赖词以及连续词汇[[140](#bib.bib140)]；他们添加的实体信息反映了使用CNN模型的*最先进的*表现。[[141](#bib.bib141)]
    使用了一种基于生成对抗网络（模仿学习）的新颖反向强化学习方法来处理联合实体和事件提取。最近，在[[142](#bib.bib142)]中，作者提出了一种文档级事件提取模型，结合了基于依赖的GCN（用于局部上下文）和超图（作为全局上下文的聚合器）。
- en: V-D Sentiment analysis
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 情感分析
- en: The primary goal in sentiment analysis is the extraction of subjective information
    from text by contextual mining. Sentiment analysis is considered high-level reasoning
    based on source data. Sentiment analysis is sometimes called opinion mining, as
    its primary goal is to analyze human opinion, sentiments, and even emotions regarding
    products, problems, and varied subjects. Seminal works on sentiment analysis or
    opinion mining include [[143](#bib.bib143), [144](#bib.bib144)]. Since 2000, much
    attention has been given to sentiment analysis, due to its relation to a wide
    variety of applications [[145](#bib.bib145)], its associations with new research
    challenges, and the availability of abundant data. [[146](#bib.bib146)] provides
    a more recent review of the sentiment analysis methods relying on deep learning
    and gives an insightful discussion on the drawbacks as well as merits of deep
    learning methods for sentiment analysis.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析的主要目标是通过上下文挖掘从文本中提取主观信息。情感分析被视为基于源数据的高级推理。情感分析有时也称为观点挖掘，因为其主要目标是分析关于产品、问题和各种主题的人类观点、情感甚至情绪。情感分析或观点挖掘的开创性工作包括[[143](#bib.bib143),
    [144](#bib.bib144)]。自2000年以来，情感分析受到广泛关注，这与其涉及的各种应用[[145](#bib.bib145)]、与新的研究挑战的关联以及大量数据的可用性有关。[[146](#bib.bib146)]提供了基于深度学习的情感分析方法的最新综述，并对深度学习方法在情感分析中的优缺点进行了有见地的讨论。
- en: 'A critical aspect of research in sentiment analysis is content granularity.
    Considering this criterion, sentiment analysis is generally divided into three
    categories/levels: document level, sentence level, and aspect level.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析研究的一个关键方面是内容的粒度。根据这一标准，情感分析通常分为三类/级别：文档级、句子级和方面级。
- en: V-D1 Document-level Sentiment Analysis
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-D1 文档级情感分析
- en: At the document level, the task is to determine whether the whole document reflects
    a positive or negative sentiment about exactly one entity. This differs from opinion
    mining regarding multiple entries. The Gated Recurrent Neural Network architecture
    has been utilized successfully for effectively encoding the sentences’ relations
    in the semantic structure of the document [[147](#bib.bib147)]. Domain adaptation
    has been investigated as well, to deploy the trained model on unseen new sources [[148](#bib.bib148)].
    More recently, in [[149](#bib.bib149)] authors provide an LSTM-based model for
    document-level sentiment analysis that captures semantic relations between sentences.
    In [[150](#bib.bib150)], authors use a CNN-bidirectional LSTM model to process
    long texts.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档级，任务是确定整个文档是否反映了对某一实体的积极或消极情感。这与处理多个条目的观点挖掘有所不同。门控递归神经网络架构已成功用于有效地编码文档中句子的语义关系[[147](#bib.bib147)]。领域适应也已经被研究，以将训练好的模型应用于新的未见数据源[[148](#bib.bib148)]。最近，[[149](#bib.bib149)]中的作者提供了一种基于LSTM的文档级情感分析模型，该模型捕捉了句子之间的语义关系。在[[150](#bib.bib150)]中，作者使用CNN-双向LSTM模型来处理长文本。
- en: V-D2 Sentence-level Sentiment Analysis
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-D2 句子级情感分析
- en: At the sentence-level, sentiment analysis determines the positivity, negativity,
    or neutrality regarding an opinion expressed in a sentence. One general assumption
    for sentence-level sentiment classification is the existence of only one opinion
    from a single opinion holder in an expressed sentence. Recursive autoencoders
    have been employed for sentence-level sentiment label prediction by learning the
    vector space representations for phrases [[151](#bib.bib151)]. Long Short-Term
    Memory (LSTM) recurrent models have also been utilized for tweet sentiment prediction [[152](#bib.bib152)].
    The Sentiment Treebank and Recursive Neural Tensor Networks [[153](#bib.bib153)]
    have shown promise for predicting fine-grained sentiment labels. [[154](#bib.bib154)]
    provides a cloud-based hybrid machine learning model for sentence level sentiment
    analysis. More recently in [[155](#bib.bib155)], propose A Lexicalized Domain
    Ontology and a Regularized Neural Attention model (ALDONAr) for sentence-level
    aspect-based sentiment analysis that uses a CNN classification module with BERT
    word embeddings and achieves state-of-the art results.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子级别上，情感分析确定了句子中表达的观点的积极性、消极性或中性。句子级情感分类的一个常见假设是，在一个表达的句子中，只存在一个观点来自一个单一的观点持有者。通过学习短语的向量空间表示，递归自动编码器已经被用于句子级情感标签预测[[151](#bib.bib151)]。长期短期记忆（LSTM）循环模型也被用于推特情感预测[[152](#bib.bib152)]。情感树库和递归神经张量网络[[153](#bib.bib153)]已经展示了预测细粒度情感标签的潜力。[[154](#bib.bib154)]提供了一个基于云的混合机器学习模型，用于句子级情感分析。最近在[[155](#bib.bib155)]中，提出了一个词汇化领域本体和一个带有BERT词嵌入的正则化神经注意模型（ALDONAr）用于句子级方面情感分析，并取得了最先进的结果。
- en: V-D3 Aspect-level Sentiment Analysis
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-D3方面级情感分析
- en: Document-level and sentence-level sentiment analysis usually focus on the sentiment
    itself, not the target of the sentiment, e.g., a product. Aspect-level sentiment
    analysis directly targets an opinion, with the assumption of the existence of
    the sentiment and its target. A document or sentence may not have a generally
    positive or negative sentiment, but may have multiple subparts with different
    targets, each with a positive or negative sentiment. This can make aspect-level
    analysis even more challenging than other types of sentiment categorization.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 文档级和句子级情感分析通常关注情感本身，而不是情感的对象，例如产品。方面级情感分析直接针对观点，假设存在情感及其目标。一个文档或句子可能没有明确的积极或消极情感，但可能有多个具有不同目标的子部分，每个部分都有积极或消极的情感。这使得方面级分析比其他类型的情感分类更具挑战性。
- en: 'Aspect-level sentiment analysis usually involves Aspect Sentiment Classification
    and Aspect Extraction. The former determines opinions on different aspects (positive,
    neutral, or negative) while the latter identifies the target aspect for evaluation
    in context. As an example consider the following sentence: “This car is old. It
    must be repaired and sold!”. “This car” is what is subject to evaluation and must
    be extracted first. Here, the opinion about this aspect is negative.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 方面级情感分析通常涉及方面情感分类和方面提取。前者确定了不同方面的观点（积极、中性或消极），而后者在上下文中确定了目标方面进行评估。以以下句子为例：“这辆车很旧。必须修理和出售！”“这辆车”是需要评估的对象，首先必须提取出来。在这里，关于这个方面的观点是消极的。
- en: For aspect-level sentiment classification, attention-based LSTMs are proposed
    to connect the aspect and sentence content for sentiment classification [[156](#bib.bib156)].
    For aspect extraction, deep learning has successfully been proposed in opinion
    mining [[157](#bib.bib157)]. State-of-the-art methods rely on converting aspect-based
    sentiment analysis to sentence-pair classification tasks [[79](#bib.bib79)], post-training
    approaches [[158](#bib.bib158)] on the popular language model BERT [[131](#bib.bib131)],
    and employment of pre-trained embeddings [[159](#bib.bib159)]. [[160](#bib.bib160)]
    provides a recent comparative review on aspect-based sentiment analysis. Also
    recently, [[161](#bib.bib161)] proposed a dual-attention model which tries to
    extract the implicit relation between the aspect and opinion terms. In [[162](#bib.bib162)]
    authors propose a novel Aspect-Guided Deep Transition model for aspect-based sentiment
    analysis.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对于层面级情感分类，提出了基于注意力的LSTM模型，用于将层面与句子内容连接进行情感分类[[156](#bib.bib156)]。对于层面提取，深度学习在意见挖掘中取得了成功[[157](#bib.bib157)]。最先进的方法依赖于将基于层面的情感分析转换为句子对分类任务[[79](#bib.bib79)]，在流行的语言模型BERT上进行后训练方法[[158](#bib.bib158)]，以及使用预训练的嵌入[[159](#bib.bib159)]。[[160](#bib.bib160)]提供了关于基于层面的情感分析的最新比较综述。最近，[[161](#bib.bib161)]提出了一个双重注意力模型，试图提取层面与意见术语之间的隐含关系。在[[162](#bib.bib162)]中，作者提出了一种新颖的层面引导深度过渡模型用于基于层面的情感分析。
- en: V-E Machine Translation
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 机器翻译
- en: Machine Translation (MT) is one of the areas of NLP that has been profoundly
    affected by the advances in deep learning. The first subsection below explains
    methods used in the pre-deep learning period, as explained in reference NLP textbooks
    such as “Speech and Language Processing” [[163](#bib.bib163)]. The remainder of
    this section is dedicated to delving into recent innovations in MT which are based
    on neural networks, started by [[164](#bib.bib164)]. [[165](#bib.bib165), [166](#bib.bib166)]
    provide reviews on various deep learning architectures used for MT.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译（MT）是自然语言处理（NLP）领域中受深度学习进展影响深远的一个领域。下面的第一小节解释了在深度学习之前所使用的方法，如参考NLP教科书《Speech
    and Language Processing》所述[[163](#bib.bib163)]。本节的其余部分致力于深入探讨基于神经网络的机器翻译的最新创新，始于[[164](#bib.bib164)]。[[165](#bib.bib165),
    [166](#bib.bib166)]提供了关于用于机器翻译的各种深度学习架构的综述。
- en: V-E1 Traditional Machine Translation
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-E1 传统机器翻译
- en: One of the first demonstrations of machine translation happened in 1954 [[167](#bib.bib167)]
    in which the authors tried to translate from Russian to English. This translation
    system was based on six simple rules, but had a very limited vocabulary. It was
    not until the 1990s that successful statistical implementations of machine translation
    emerged as more bilingual corpora became available [[163](#bib.bib163)]. In [[68](#bib.bib68)]
    the BLEU score was introduced as a new evaluation metric, allowing more rapid
    improvement than when the only approach involved using human labor for evaluation.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译的第一次展示发生在1954年[[167](#bib.bib167)]，当时作者尝试从俄语翻译到英语。该翻译系统基于六条简单规则，但词汇量非常有限。直到1990年代，随着更多双语语料库的出现，成功的统计机器翻译实现才出现[[163](#bib.bib163)]。在[[68](#bib.bib68)]中引入了BLEU评分作为新的评估指标，使得改进速度比仅使用人工评估的方法更快。
- en: V-E2 Neural Machine Translation
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-E2 神经机器翻译
- en: It was after the success of the neural network in image classification tasks
    that researchers started to use neural networks in machine translation (NMT).
    Around 2013, research groups started to achieve breakthrough results in NMT. Unlike
    traditional statistical machine translation, NMT is based on an end-to-end neural
    network [[168](#bib.bib168)]. This implies that there is no need for extensive
    preprocessing and word alignments. Instead, the focus shifted toward network structure.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络在图像分类任务中取得成功之后，研究人员开始在机器翻译（NMT）中使用神经网络。大约在2013年，研究小组开始在NMT中取得突破性成果。与传统的统计机器翻译不同，NMT基于端到端神经网络[[168](#bib.bib168)]。这意味着不需要大量的预处理和词对齐。相反，焦点转向了网络结构。
- en: 'Fig. [11](#S5.F11 "Figure 11 ‣ V-E2 Neural Machine Translation ‣ V-E Machine
    Translation ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey") shows an example of an end-to-end recurrent neural
    network for machine translation. A sequence of input tokens is fed into the network.
    Once it reaches an end-of-sentence (EOS) token, it starts generating the output
    sequence. The output sequence is generated in the same recurrent manner as the
    input sequence until it reaches an end-of-sentence token. One major advantage
    of this approach is that there is no need to specify the length of the sequence;
    the network takes it into account automatically. In other words, the end-of-sentence
    token determines the length of the sequence. Networks implicitly learn that longer
    input sentences usually lead to longer output sentences with varying length, and
    that ordering can change. For instance, the second example in Fig. [9](#S5.F9
    "Figure 9 ‣ V-E2 Neural Machine Translation ‣ V-E Machine Translation ‣ V Deep
    Learning for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey") shows that adjectives generally come before nouns in English but after
    nouns in Spanish. There is no need to explicitly specify this since the network
    can capture such properties. Moreover, the amount of memory that is used by NMT
    is just a fraction of the memory that is used in traditional statistical machine
    translation [[169](#bib.bib169)].'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [11](#S5.F11 "图 11 ‣ V-E2 神经机器翻译 ‣ V-E 机器翻译 ‣ V 深度学习在自然语言处理任务中的应用 ‣ 深度学习推动的自然语言处理进展：调查")
    显示了一个用于机器翻译的端到端递归神经网络示例。输入标记的序列被送入网络。一旦到达句子结束（EOS）标记，它开始生成输出序列。输出序列以与输入序列相同的递归方式生成，直到到达句子结束标记。这种方法的一个主要优势是无需指定序列的长度；网络会自动考虑这一点。换句话说，句子结束标记决定了序列的长度。网络隐式地学习到较长的输入句子通常会产生较长的输出句子，且长度会有所变化，排序也可能发生变化。例如，图
    [9](#S5.F9 "图 9 ‣ V-E2 神经机器翻译 ‣ V-E 机器翻译 ‣ V 深度学习在自然语言处理任务中的应用 ‣ 深度学习推动的自然语言处理进展：调查")
    中的第二个示例表明，形容词通常在英语中出现在名词之前，但在西班牙语中则出现在名词之后。不需要明确指定这一点，因为网络能够捕捉到这些属性。此外，NMT 使用的内存量仅为传统统计机器翻译使用的内存的一小部分
    [[169](#bib.bib169)]。
- en: '![Refer to caption](img/166eef1b0679d28fec259260764d11cb.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/166eef1b0679d28fec259260764d11cb.png)'
- en: 'Figure 9: Alignment in Machine Translation'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 机器翻译中的对齐'
- en: '[[164](#bib.bib164)] was one of the early works that incorporated recurrent
    neural networks for machine translation. They were able to achieve a perplexity
    (a measure where lower values indicate better models) that was 43% less than the
    state-of-the-art alignment based translation models. Their recurrent continuous
    translation model (RCTM) is able to capture word ordering, syntax, and meaning
    of the source sentence explicitly. It maps a source sentence into a probability
    distribution over sentences in the target language. RCTM estimates the probability
    $P(f|e)$ of translating a sentence $e=e_{1}+...+e_{k}$ in the source language
    to target language sentence $f=f_{1}+...+f_{m}$. RCTM estimates $P(f|e)$ by considering
    source sentence $e$ as well as the preceding words in the target language $f_{1:i-1}$:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[[164](#bib.bib164)] 是早期采用递归神经网络进行机器翻译的工作之一。他们实现了比最先进的基于对齐的翻译模型低 43% 的困惑度（困惑度是一种测量标准，值越低表示模型越好）。他们的递归连续翻译模型（RCTM）能够明确捕捉源句子的词序、语法和含义。它将源句子映射到目标语言句子的概率分布上。RCTM
    通过考虑源句子 $e$ 以及目标语言中前面的词 $f_{1:i-1}$ 来估计翻译一个句子 $e=e_{1}+...+e_{k}$ 到目标语言句子 $f=f_{1}+...+f_{m}$
    的概率 $P(f\mid e)$：'
- en: '|  | $P(f&#124;e)=\prod_{i=1}^{m}P(f_{i}&#124;f_{1:i-1},e)$ |  | (3) |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(f\mid e)=\prod_{i=1}^{m}P(f_{i}\mid f_{1:i-1},e)$ |  | (3) |'
- en: 'The representation generated by RCTM acts on n-grams in the lower layers, and
    acts more on the whole sentence as one moves to the upper layers. This hierarchical
    representation is performed by applying different layers of convolution. First
    a continuous representation of each word is generated; i.e., if the sentence is
    $e=e_{1}...e_{k}$, the representation of the word $e_{i}$ will be $v(e_{i})\in\mathbb{R}^{q\times
    1}$. This will result in sentence matrix $\textbf{E}^{e}\in\mathbb{R}^{q\times{k}}$
    in which $\textbf{E}_{:,i}^{e}=v(e_{i})$. This matrix representation of the sentence
    will be fed into a series of convolution layers in order to generate the final
    representation e for the recurrent neural network. The approach is illustrated
    in Fig. [10](#S5.F10 "Figure 10 ‣ V-E2 Neural Machine Translation ‣ V-E Machine
    Translation ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey"). Equations for the pipeline are as follows.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 'RCTM 生成的表示在较低层中作用于 n-grams，而在较高层中则更多地作用于整个句子。这种层次化表示是通过应用不同层的卷积来实现的。首先生成每个词的连续表示；即，如果句子是
    $e=e_{1}...e_{k}$，则词 $e_{i}$ 的表示将是 $v(e_{i})\in\mathbb{R}^{q\times 1}$。这将生成句子矩阵
    $\textbf{E}^{e}\in\mathbb{R}^{q\times{k}}$，其中 $\textbf{E}_{:,i}^{e}=v(e_{i})$。该句子矩阵表示将输入到一系列卷积层中，以生成递归神经网络的最终表示
    e。该方法在图 [10](#S5.F10 "Figure 10 ‣ V-E2 Neural Machine Translation ‣ V-E Machine
    Translation ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey")中进行了说明。该流程的方程式如下。'
- en: '|  | $s=\textbf{S}.csm(e)$ |  | (4) |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | $s=\textbf{S}.csm(e)$ |  | (4) |'
- en: '|  | $h_{1}=\sigma(\textbf{I}.v(f_{1})+\textbf{s})$ |  | (5) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{1}=\sigma(\textbf{I}.v(f_{1})+\textbf{s})$ |  | (5) |'
- en: '|  | $h_{i+1}=\sigma(\textbf{R}.h_{i}+\textbf{I}.v(f_{i+1})+\textbf{s})$ |  |
    (6) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{i+1}=\sigma(\textbf{R}.h_{i}+\textbf{I}.v(f_{i+1})+\textbf{s})$ |  |
    (6) |'
- en: '|  | $o_{i+1}=\textbf{O}.h_{i}$ |  | (7) |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | $o_{i+1}=\textbf{O}.h_{i}$ |  | (7) |'
- en: 'In order to take into account the sentence length, the authors introduced RCTM
    II which estimates the length of the target sentence. RCTM II was able to achieve
    better perplexity on WMT datasets (see top portion of Table [I](#S4.T1 "TABLE
    I ‣ IV Datasets ‣ Natural Language Processing Advancements By Deep Learning: A
    Survey")) than other existing machine translation systems.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '为了考虑句子的长度，作者引入了 RCTM II，该方法估计目标句子的长度。RCTM II 能够在 WMT 数据集上获得比其他现有机器翻译系统更好的困惑度（见表 [I](#S4.T1
    "TABLE I ‣ IV Datasets ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey")的顶部部分）。'
- en: '![Refer to caption](img/77661059319e0ecfa045953c41ad33b0.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/77661059319e0ecfa045953c41ad33b0.png)'
- en: 'Figure 10: Recurrent Continuous Translation Models (RCTM) [[164](#bib.bib164)].'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 递归连续翻译模型 (RCTM) [[164](#bib.bib164)]。'
- en: 'In another line of work,  [[170](#bib.bib170)] presented an end-to-end sequence
    learning approach without heavy assumptions on the structure of the sequence.
    Their approach consists of two LSTMs, one for mapping the input to a vector of
    fixed dimension and another LSTM for decoding the output sequence from the vector.
    Their model was able to handle long sentences as well as sentence representations
    that are sensitive to word order. As shown in Fig. [11](#S5.F11 "Figure 11 ‣ V-E2
    Neural Machine Translation ‣ V-E Machine Translation ‣ V Deep Learning for NLP
    Tasks ‣ Natural Language Processing Advancements By Deep Learning: A Survey"),
    the model reads ”ABC” as an input sequence and produces ”WXYZ” as output sequence.
    The $<EOS>$ token indicates the end of prediction. The network was trained by
    maximizing the log probability of the translation ($\eta$) given the input sequence
    ($\zeta$). In other words, the objective function is:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '在另一项工作中，[[170](#bib.bib170)] 提出了一个端到端序列学习方法，不依赖于序列结构的强假设。他们的方法包括两个 LSTM，一个用于将输入映射到固定维度的向量，另一个
    LSTM 用于从该向量解码输出序列。他们的模型能够处理长句子以及对词序敏感的句子表示。如图 [11](#S5.F11 "Figure 11 ‣ V-E2 Neural
    Machine Translation ‣ V-E Machine Translation ‣ V Deep Learning for NLP Tasks
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey")所示，该模型将“ABC”作为输入序列，并生成“WXYZ”作为输出序列。$<EOS>$
    标记表示预测的结束。网络通过最大化给定输入序列 ($\zeta$) 的翻译 ($\eta$) 的对数概率来训练。换句话说，目标函数为：'
- en: '![Refer to caption](img/4f785debc717c2fbc150fcc7186c6f8d.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4f785debc717c2fbc150fcc7186c6f8d.png)'
- en: 'Figure 11: Sequence to sequence learning with LSTM.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 使用 LSTM 的序列到序列学习。'
- en: '|  | $1/&#124;\mathcal{D}&#124;\sum_{\begin{subarray}{c}(\eta,\zeta)\in\mathcal{D}\end{subarray}}logP(\eta&#124;\zeta)$
    |  | (8) |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | $1/&#124;\mathcal{D}&#124;\sum_{\begin{subarray}{c}(\eta,\zeta)\in\mathcal{D}\end{subarray}}logP(\eta&#124;\zeta)$
    |  | (8) |'
- en: $\mathcal{D}$ is the training set and $|\mathcal{D}|$ is its size. One of the
    novelties of their approach was reversing word order of the source sentence. This
    helps the LSTM to learn long term dependencies.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{D}$ 是训练集，$|\mathcal{D}|$ 是其大小。他们方法的一个新颖之处在于反转源句子的单词顺序。这有助于LSTM学习长期依赖性。
- en: 'Having a fixed-length vector in the decoder phase is one of the bottlenecks
    of the encoder-decoder approach.  [[168](#bib.bib168)] argues that a network will
    have a hard time compressing all the information from the input sentence into
    a fixed-size vector. They address this by allowing the network to search segments
    of the source sentence that are useful for predicting the translation. Instead
    of representing the input sentence as a fixed-size vector, in [[168](#bib.bib168)]
    the input sentence is encoded to a sequence of vectors and a subset of them is
    chosen by using a method called attention mechanism as shown in Fig. [12](#S5.F12
    "Figure 12 ‣ V-E2 Neural Machine Translation ‣ V-E Machine Translation ‣ V Deep
    Learning for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey").'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '在解码阶段固定长度向量是编码器-解码器方法的瓶颈之一。[[168](#bib.bib168)] 认为网络很难将输入句子中的所有信息压缩到固定大小的向量中。他们通过允许网络搜索对预测翻译有用的源句子片段来解决这个问题。在[[168](#bib.bib168)]中，输入句子不是表示为固定大小的向量，而是编码为向量序列，并使用称为注意力机制的方法选择其中的一个子集，如图[12](#S5.F12
    "Figure 12 ‣ V-E2 Neural Machine Translation ‣ V-E Machine Translation ‣ V Deep
    Learning for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey")所示。'
- en: 'In their approach $P(y_{i}|y_{1},...,y_{i-1},X)=g(y_{i-1},s_{i},c_{i})$, in
    which $s_{i}=f(s_{i-1},y_{i-1},c_{i})$. While previously $c$ was the same for
    all time steps, here $c$ takes a different value, $c_{i}$, at each time step.
    This accounts for the attention mechasim (context vector) around that specific
    time step. $c_{i}$ is computed according to the following:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的方法中 $P(y_{i}|y_{1},...,y_{i-1},X)=g(y_{i-1},s_{i},c_{i})$，其中 $s_{i}=f(s_{i-1},y_{i-1},c_{i})$。之前
    $c$ 在所有时间步长中是相同的，而这里 $c$ 在每个时间步长中取不同的值 $c_{i}$。这考虑了特定时间步长周围的注意力机制（上下文向量）。$c_{i}$
    是根据以下方式计算的：
- en: $c_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j},\ \alpha_{ij}=\frac{exp(e_{ij})}{{\sum_{k=1}^{T_{x}}exp(e_{ik})}},\
    e_{ij}=a(s_{i-1},h_{j})$.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: $c_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j},\ \alpha_{ij}=\frac{exp(e_{ij})}{{\sum_{k=1}^{T_{x}}exp(e_{ik})}},\
    e_{ij}=a(s_{i-1},h_{j})$。
- en: Here $a$ is the alignment model that is represented by a feed forward neural
    network. Also $h_{j}=[\overset{\rightarrow}{h_{j}^{T}},\overset{\leftarrow}{h_{j}^{T}}]$,
    which is a way to include information both about preceding and following words
    in $h_{j}$. The model was able to outperform the simple encoder-decoder approach
    regardless of input sentence length.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $a$ 是通过前馈神经网络表示的对齐模型。同时 $h_{j}=[\overset{\rightarrow}{h_{j}^{T}},\overset{\leftarrow}{h_{j}^{T}}]$，这是一种在
    $h_{j}$ 中包含前后单词信息的方法。该模型能够超越简单的编码器-解码器方法，而不受输入句子长度的影响。
- en: 'Improved machine translation models continue to emerge, driven in part by the
    growth in people’s interest and need to understand other languages Most of them
    are variants of the end-to-end decoder-encoder approach. For example,  [[171](#bib.bib171)]
    tries to deal with the problem of rare words. Their LSTM network consists of encoder
    and decoder layers using residual layers along with the attention mechanism. Their
    system was able to decrease training time, speed up inference, and handle translation
    of rare words. Comparisons between some of the state-of-the-art neural machine
    translation models are summarized in Table [VII](#S5.T7 "TABLE VII ‣ V-E2 Neural
    Machine Translation ‣ V-E Machine Translation ‣ V Deep Learning for NLP Tasks
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey").'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '改进的机器翻译模型不断出现，部分原因是人们对理解其他语言的兴趣和需求的增长。它们大多数是端到端解码器-编码器方法的变体。例如，[[171](#bib.bib171)]
    试图解决稀有词问题。他们的LSTM网络由编码器和解码器层组成，使用了残差层和注意力机制。他们的系统能够减少训练时间，加快推理速度，并处理稀有词的翻译。部分先进的神经机器翻译模型的比较总结在表[VII](#S5.T7
    "TABLE VII ‣ V-E2 Neural Machine Translation ‣ V-E Machine Translation ‣ V Deep
    Learning for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey")中。'
- en: '![Refer to caption](img/33545956bb7f1a525ac9502281944d8a.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/33545956bb7f1a525ac9502281944d8a.png)'
- en: 'Figure 12: Attention Mechasim for Neural Machine Translation [[168](#bib.bib168)].'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 神经机器翻译的注意力机制 [[168](#bib.bib168)]。'
- en: 'TABLE VII: The machine translation state-of-the-art models evaluated on the
    English-German dataset of ACL 2014 Ninth Workshop on Statistical Machine TRranslation.
    The evaluation metric is $BLEU$ score.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：评估的机器翻译最先进模型，数据集为 ACL 2014 第九届统计机器翻译研讨会的英德数据集。评估指标是 $BLEU$ 分数。
- en: '| Model | Accuracy |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 准确率 |'
- en: '| --- | --- |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Convolutional Seq-to-Seq  [[172](#bib.bib172)] | 25.2 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 卷积序列到序列 [[172](#bib.bib172)] | 25.2 |'
- en: '| Attention Is All You Need [[173](#bib.bib173)] | 28.4 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 注意力机制是你所需的 [[173](#bib.bib173)] | 28.4 |'
- en: '| Weighted Transformer [[174](#bib.bib174)] | 28.9 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 加权 Transformer [[174](#bib.bib174)] | 28.9 |'
- en: '| Self Attention [[175](#bib.bib175)] | 29.2 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 自注意力 [[175](#bib.bib175)] | 29.2 |'
- en: '| DeepL Translation Machine ^(10)^(10)10https://www.deepl.com/press.html |
    33.3 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| DeepL 翻译机器 ^(10)^(10)10https://www.deepl.com/press.html | 33.3 |'
- en: '| Back-translation [[176](#bib.bib176)] | 35.0 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 回译 [[176](#bib.bib176)] | 35.0 |'
- en: More recently, [[177](#bib.bib177)] provides an interesting single-model implementation
    of massively multilingual NMT. In [[178](#bib.bib178)], authors use BERT to extract
    contextual embeddings and combine BERT with an attention-based NMT model and provide
    state-of-the-art results on various benchmark datasets. [[179](#bib.bib179)] proposes
    mBART which is a seq-to-seq denoising autoencoder and reports that using a pretrained,
    locked (i.e. no modifications) mBART improves performance in terms of the BLEU
    point. [[180](#bib.bib180)] proposes an interesting adversarial framework for
    robustifying NMT against noisy inputs and reports performance gains over the Transformer
    model. [[181](#bib.bib181)] is also an insightful recent work where the authors
    sample context words from the predicted sequence as well as the ground truth to
    try to reconcile the training and inference processes. Finally, [[182](#bib.bib182)]
    is a successful recent effort to prevent the forgetting that often accompanies
    in translating pre-trained language models to other NMT task. [[182](#bib.bib182)]
    achieves that aim primarily by using a dynamically gated model and asymptotic
    distillation.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[[177](#bib.bib177)] 提供了一个有趣的单模型实现的大规模多语言 NMT。在 [[178](#bib.bib178)] 中，作者使用
    BERT 提取上下文嵌入，并将 BERT 与基于注意力的 NMT 模型结合，提供了在各种基准数据集上的最先进结果。[[179](#bib.bib179)]
    提出了 mBART，这是一种序列到序列的去噪自编码器，并报告了使用预训练的、固定的（即没有修改的）mBART 在 BLEU 分数方面提高了性能。[[180](#bib.bib180)]
    提出了一个有趣的对抗性框架，用于增强 NMT 对噪声输入的鲁棒性，并报告了比 Transformer 模型更好的性能。[[181](#bib.bib181)]
    也是一项有洞察力的近期工作，作者从预测序列以及实际情况中采样上下文词，以尝试协调训练和推理过程。最后，[[182](#bib.bib182)] 是一项成功的近期努力，旨在防止在将预训练语言模型转换到其他
    NMT 任务时常见的遗忘问题。[[182](#bib.bib182)] 主要通过使用动态门控模型和渐近蒸馏来实现这一目标。
- en: V-F Question Answering
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-F 问答系统
- en: Question answering (QA) is a fine-grained version of Information Retrieval (IR).
    In IR a desired set of information has to be retrieved from a set of documents.
    The desired information could be a specific document, text, image, etc. On the
    other hand, in QA specific answers are sought, typically ones that can be inferred
    from available documents. Other areas of NLP such as reading comprehension and
    dialogue systems intersect with question answering.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 问答（QA）是信息检索（IR）的一个更精细的版本。在 IR 中，需要从一组文档中检索出所需的信息。这些信息可能是特定的文档、文本、图像等。另一方面，在
    QA 中，寻找的是具体的答案，通常是可以从现有文档中推断出来的答案。其他 NLP 领域如阅读理解和对话系统也与问答系统交叉。
- en: Research in computerized question answering has proceeded since the 1960s. In
    this section, we present a general overview of question answering system history,
    and focus on the breakthroughs in the field. Like all other fields in NLP, question
    answering was also impacted by the advancement of deep learning  [[183](#bib.bib183)],
    so we provide an overview of QA in deep learning contexts. We briefly visit visual
    question answering as well.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机问答系统的研究自1960年代以来一直在进行。在本节中，我们提供了问答系统历史的概述，并重点介绍了该领域的突破。与 NLP 领域的其他所有领域一样，问答系统也受到了深度学习
    [[183](#bib.bib183)] 进展的影响，因此我们提供了深度学习背景下的 QA 概述。我们还简要地讨论了视觉问答系统。
- en: V-F1 Rule-based Question Answering
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-F1 基于规则的问答系统
- en: Baseball  [[184](#bib.bib184)] is one of the early works (1961) on QA where
    an effort was made to answer questions related to baseball games by using a game
    database. The baseball system consists of (1) question read-in, (2) dictionary
    lookup for words in the question, (3) syntactic (POS) analysis of the words in
    question, (4) content analysis for extracting the input question, and (5) estimating
    relevance regarding answering the input question.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: Baseball [[184](#bib.bib184)]是早期的问答系统之一（1961年），它尝试通过使用游戏数据库来回答与棒球比赛相关的问题。该棒球系统包括（1）问题读取，（2）对问题中单词的字典查找，（3）对问题中单词的句法（POS）分析，（4）内容分析以提取输入问题，以及（5）估计回答输入问题的相关性。
- en: 'IBM’s  [[185](#bib.bib185)] statistical question answering system consisted
    of four major components:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: IBM的[[185](#bib.bib185)]统计问答系统由四个主要组件组成：
- en: '1.'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Question/Answer Type Classification
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题/答案类型分类
- en: '2.'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Query Expansion/Information Retrieval
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查询扩展/信息检索
- en: '3.'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Name Entity Making
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实体命名
- en: '4.'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Answer Selection
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案选择
- en: Some QA systems fail when semantically equivalent relationships are phrased
    differently.  [[186](#bib.bib186)] addressed this by proposing fuzzy relation
    matching based on mutual information and expectation maximization.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 一些问答系统在语义等价关系以不同方式表达时会失败。[[186](#bib.bib186)]通过提出基于互信息和期望最大化的模糊关系匹配来解决这个问题。
- en: V-F2 Question answering in the era of deep learning
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-F2 深度学习时代的问答
- en: 'Smartphones (Siri, Ok Google, Alexa, etc.) and virtual personal assistants
    are common examples of QA systems with which many interact on a daily basis. While
    earlier such systems employed rule-based methods, today their core algorithm is
    based on deep learning. Table [VIII](#S5.T8 "TABLE VIII ‣ V-F2 Question answering
    in the era of deep learning ‣ V-F Question Answering ‣ V Deep Learning for NLP
    Tasks ‣ Natural Language Processing Advancements By Deep Learning: A Survey")
    presents some questions and answers provided by Siri on an iPhone.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '智能手机（Siri、Ok Google、Alexa等）和虚拟个人助理是许多人每天互动的常见问答系统示例。虽然早期这些系统采用基于规则的方法，但如今它们的核心算法基于深度学习。表[VIII](#S5.T8
    "TABLE VIII ‣ V-F2 Question answering in the era of deep learning ‣ V-F Question
    Answering ‣ V-F Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey")展示了Siri在iPhone上提供的一些问题和答案。'
- en: 'TABLE VIII: Typical Question Answering performance based on deep learning.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 表VIII：基于深度学习的典型问答性能。
- en: '| Question | Answer |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 答案 |'
- en: '| --- | --- |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Who invented polio vaccine? | The answer I found is Jonas Salk |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 谁发明了脊髓灰质炎疫苗？ | 我找到的答案是乔纳斯·索尔克 |'
- en: '| Who wrote Harry Potter? | J.K.Rowling wrote Harry Potter in 1997 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 谁写了哈利·波特？ | J.K.罗琳在1997年写了哈利·波特 |'
- en: '| When was Einstein born? | Albert Einstein was born March 14, 1879 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 爱因斯坦什么时候出生的？ | 阿尔伯特·爱因斯坦于1879年3月14日出生 |'
- en: '![Refer to caption](img/6059be555e9cbc3796e965ec4fddb7f8.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6059be555e9cbc3796e965ec4fddb7f8.png)'
- en: 'Figure 13: Fixed length vector sentence representation for input Questions
    and Answers [[187](#bib.bib187)].'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：输入问题和答案的固定长度向量句子表示[[187](#bib.bib187)]。
- en: '[[188](#bib.bib188)] was one of the first machine learning based papers that
    reported results on QA for a reading comprehension test. The system tries to pick
    a sentence in the database that has an answer to a question, and a feature vector
    represents each question-sentence pair. The main contribution of [[188](#bib.bib188)]
    is proposing a feature vector representation framework which is aimed to provide
    information for learning the model. There are five classifiers (location, date,
    etc.), one for each type of question. They were able to achieve accuracy competitive
    with previous approaches.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[[188](#bib.bib188)]是第一篇基于机器学习的论文之一，报告了阅读理解测试的问答结果。该系统试图从数据库中挑选出对问题有答案的句子，并且为每个问题-句子对表示一个特征向量。[[188](#bib.bib188)]的主要贡献是提出了一种特征向量表示框架，旨在提供学习模型所需的信息。共有五个分类器（地点、日期等），每种问题类型有一个。他们能够达到与之前方法相竞争的准确性。'
- en: 'As illustrated in Fig. [13](#S5.F13 "Figure 13 ‣ V-F2 Question answering in
    the era of deep learning ‣ V-F Question Answering ‣ V Deep Learning for NLP Tasks
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey"), [[187](#bib.bib187)]
    uses convolutional neural networks in order to encode Question-Answer sentence
    pairs in the form of fixed length vectors regardless of the length of the input
    sentence. Instead of using distance measures like cosine correlation, they incorporate
    a non-linear tensor layer to match the relevance between question and answer.
    Equation [9](#S5.E9 "In V-F2 Question answering in the era of deep learning ‣
    V-F Question Answering ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing
    Advancements By Deep Learning: A Survey") calculates the matching degree between
    question $q$ and its corresponding answer $a$.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [13](#S5.F13 "图 13 ‣ V-F2 深度学习时代的问题回答 ‣ V-F 问题回答 ‣ V NLP 任务的深度学习 ‣ 深度学习推动的自然语言处理进展：一项调查")
    所示，[[187](#bib.bib187)] 使用卷积神经网络将问题-答案句对编码为固定长度向量，而不受输入句子长度的影响。他们没有使用余弦相关等距离度量，而是融入了非线性张量层来匹配问题和答案之间的相关性。方程
    [9](#S5.E9 "在 V-F2 深度学习时代的问题回答 ‣ V-F 问题回答 ‣ V NLP 任务的深度学习 ‣ 深度学习推动的自然语言处理进展：一项调查")
    计算问题 $q$ 和其对应答案 $a$ 之间的匹配程度。
- en: '|  | $s(q,a)=\textbf{u}^{T}\textbf{f}(\textbf{v}^{T}_{q}\textbf{M}^{[1:r]}\textbf{v}_{a}+\textbf{V}\begin{bmatrix}\textbf{v}_{q}\\
    \textbf{v}_{a}\end{bmatrix}+\textbf{b})$ |  | (9) |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | $s(q,a)=\textbf{u}^{T}\textbf{f}(\textbf{v}^{T}_{q}\textbf{M}^{[1:r]}\textbf{v}_{a}+\textbf{V}\begin{bmatrix}\textbf{v}_{q}\\
    \textbf{v}_{a}\end{bmatrix}+\textbf{b})$ |  | (9) |'
- en: f is the standard element-wise non-linearity function, $\textbf{M}^{[1:r]\in
    R^{n_{s}\times n_{s}\times r}}$ is a tensor, $\textbf{V}\in R^{r\times 2n_{s}}$,
    $\textbf{b}\in R^{r}$, $\textbf{u}\in R^{r}$.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: f 是标准的元素级非线性函数，$\textbf{M}^{[1:r]\in R^{n_{s}\times n_{s}\times r}}$ 是一个张量，$\textbf{V}\in
    R^{r\times 2n_{s}}$，$\textbf{b}\in R^{r}$，$\textbf{u}\in R^{r}$。
- en: '![Refer to caption](img/f053b394d250c1d01ea947c1ad46ac85.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f053b394d250c1d01ea947c1ad46ac85.png)'
- en: 'Figure 14: Example of Dynamic Memory Network (DMN) input-question-answer triplet'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：动态记忆网络（DMN）输入-问题-答案三元组示例
- en: 'The model tries to capture the interaction between question and answer. Inspired
    by findings in neuroscience, [[81](#bib.bib81)] incorporated episodic memory^(11)^(11)11A
    kind of long-term memory that includes conscious recall of previous activities
    together with their meaning. in their Dynamic Memory Network (DMN). By processing
    input sequences and questions, DMN forms episodic memories to answer relevant
    questions. As illustrated in Fig. [14](#S5.F14 "Figure 14 ‣ V-F2 Question answering
    in the era of deep learning ‣ V-F Question Answering ‣ V Deep Learning for NLP
    Tasks ‣ Natural Language Processing Advancements By Deep Learning: A Survey"),
    their system is trained based on raw Input-Question-Answer triplets.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型试图捕捉问题和答案之间的互动。受神经科学发现的启发，[[81](#bib.bib81)] 在他们的动态记忆网络（DMN）中融入了情景记忆^(11)^(11)11一种长期记忆，包括对先前活动的有意识回忆以及其意义。通过处理输入序列和问题，DMN
    形成情景记忆来回答相关问题。如图 [14](#S5.F14 "图 14 ‣ V-F2 深度学习时代的问题回答 ‣ V-F 问题回答 ‣ V NLP 任务的深度学习
    ‣ 深度学习推动的自然语言处理进展：一项调查") 所示，他们的系统基于原始的输入-问题-答案三元组进行训练。
- en: 'DMN consists of four modules that communicate with each other as shown in Fig. [15](#S5.F15
    "Figure 15 ‣ V-F2 Question answering in the era of deep learning ‣ V-F Question
    Answering ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey"). The input module encodes raw input text into a distributed
    vector representation; likewise the question module encodes a question into its
    distributed vector representation. The episodic memory module uses the attention
    mechanism in order to focus on a specific part of the input module. Through an
    iterative process, this module produces a memory vector representation that considers
    the question as well as previous memory. The answer module uses the final memory
    vector to generate an answer. The model improved upon state-of-the-art results
    on tasks such as the ones shown in Fig. [14](#S5.F14 "Figure 14 ‣ V-F2 Question
    answering in the era of deep learning ‣ V-F Question Answering ‣ V Deep Learning
    for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning: A Survey").
    DMN is one of the architectures that could potentially be used for a variety of
    NLP applications such as classification, question answering, and sequence modeling.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: DMN 由四个模块组成，这些模块之间进行通信，如图 [15](#S5.F15 "图 15 ‣ V-F2 深度学习时代的问题回答 ‣ V-F 问题回答 ‣
    V 深度学习在 NLP 任务中的应用 ‣ 深度学习推动的自然语言处理进展：综述")所示。输入模块将原始输入文本编码成分布式向量表示；同样，问题模块将问题编码成其分布式向量表示。情景记忆模块使用注意力机制来关注输入模块的特定部分。通过迭代过程，该模块生成一个记忆向量表示，考虑了问题以及之前的记忆。答案模块使用最终的记忆向量来生成答案。该模型在如图
    [14](#S5.F14 "图 14 ‣ V-F2 深度学习时代的问题回答 ‣ V-F 问题回答 ‣ V 深度学习在 NLP 任务中的应用 ‣ 深度学习推动的自然语言处理进展：综述")所示的任务上提高了最先进的结果。DMN
    是可以用于各种 NLP 应用程序的架构之一，例如分类、问题回答和序列建模。
- en: '![Refer to caption](img/34d04490c196ab59c82669e11e7adb0c.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/34d04490c196ab59c82669e11e7adb0c.png)'
- en: 'Figure 15: Interaction between four modules of Dynamic Memory Network [[78](#bib.bib78)].'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：动态记忆网络四个模块之间的交互[[78](#bib.bib78)]。
- en: '[[189](#bib.bib189)] introduced a Dynamic Coattention Network (DCN) in order
    to address local maxima corresponding to incorrect answers; it is considered to
    be one of the best approaches to question answering.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '[[189](#bib.bib189)] 提出了动态共注意力网络（DCN），以解决对应于错误答案的局部极值问题；这被认为是问题回答的最佳方法之一。'
- en: V-F3 Visual Question Answering
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-F3 视觉问题回答
- en: 'Given an input image, Visual Question Answering (VQA) tries to answer a natural
    language question about the image [[190](#bib.bib190)]. VQN addresses multiple
    problems such as object detection, image segmentation, sentiment analysis, etc.
    [[190](#bib.bib190)] introduced the task of VQA by providing a dataset containing
    over 250K images, 760K questions, and around 10M answers. [[191](#bib.bib191)]
    proposed a neural-based approach to answer the questions regarding the input images.
    As illustrated in Fig. [16](#S5.F16 "Figure 16 ‣ V-F3 Visual Question Answering
    ‣ V-F Question Answering ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing
    Advancements By Deep Learning: A Survey"), Neural-Image-QA is a deep network consisting
    of CNN and LSTM. Since the questions can have multiple answers, the problem is
    decomposed into predicting a set of answer words $a_{q,x}=\{a_{1},a_{2},...,a_{N(q,x)}\}$
    from a finite vocabulary set $\nu$ where $N(q,x)$ represents the count of answer
    words regarding a given question.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一张输入图像，视觉问题回答（VQA）尝试回答关于图像的自然语言问题[[190](#bib.bib190)]。VQN 解决了多个问题，如物体检测、图像分割、情感分析等。[[190](#bib.bib190)]
    通过提供包含超过 250K 张图像、760K 个问题和大约 10M 个答案的数据集，介绍了 VQA 任务。[[191](#bib.bib191)] 提出了基于神经网络的方法来回答有关输入图像的问题。如图
    [16](#S5.F16 "图 16 ‣ V-F3 视觉问题回答 ‣ V-F 问题回答 ‣ V 深度学习在 NLP 任务中的应用 ‣ 深度学习推动的自然语言处理进展：综述")所示，神经图像
    QA 是一个包含 CNN 和 LSTM 的深度网络。由于问题可能有多个答案，因此该问题被分解为从有限词汇集 $\nu$ 中预测一组答案词 $a_{q,x}=\{a_{1},a_{2},...,a_{N(q,x)}\}$，其中
    $N(q,x)$ 代表与给定问题相关的答案词的数量。
- en: '![Refer to caption](img/ae7ec0b22badef0f1370da11ef05dcb7.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ae7ec0b22badef0f1370da11ef05dcb7.png)'
- en: 'Figure 16: Neural Image Question Answering [[191](#bib.bib191)].'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：神经图像问题回答[[191](#bib.bib191)]。
- en: '![Refer to caption](img/cf5c9dc202153f9a0179f472e898889d.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cf5c9dc202153f9a0179f472e898889d.png)'
- en: 'Figure 17: Spatial Memory Network for VQA. Bright Areas are regions the model
    is attending [[192](#bib.bib192)].'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：用于 VQA 的空间记忆网络。亮区是模型正在关注的区域[[192](#bib.bib192)]。
- en: 'Do humans and computers look at the same regions to answer questions about
    an image? [[193](#bib.bib193)] tries to answer this question by conducting large-scale
    studies on human attention in VQA. Their findings show that VQAs do not seem to
    be looking at the same regions as humans. Finally, [[192](#bib.bib192)] incorporates
    a spatial memory network for VQA. Fig. [17](#S5.F17 "Figure 17 ‣ V-F3 Visual Question
    Answering ‣ V-F Question Answering ‣ V Deep Learning for NLP Tasks ‣ Natural Language
    Processing Advancements By Deep Learning: A Survey") shows the inference process
    of their model. As illustrated in the figure, the specific attention mechanism
    in their system can highlight areas of interest in the input image. [[194](#bib.bib194)]
    introduces BLOCK, a bilinear fusion model based on superdiagonal tensor decomposition
    for the VQA task, with state-of-the-art performance and the code made public on
    github. To improve the generalization of existing models to test data of different
    distribution, [[195](#bib.bib195)] introduces a self-critical training objective
    to help find visual regions of prominent visual/textual correlation with a focus
    on recognizing influential objects and detecting and devaluing incorrect dominant
    answers.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '人类和计算机是否在相同的区域查看以回答关于图像的问题？[[193](#bib.bib193)] 通过对人类在视觉问答（VQA）中的注意力进行大规模研究来尝试回答这个问题。他们的发现表明，视觉问答似乎并不像人类那样查看相同的区域。最后，[[192](#bib.bib192)]
    为视觉问答引入了一个空间记忆网络。图 [17](#S5.F17 "Figure 17 ‣ V-F3 Visual Question Answering ‣
    V-F Question Answering ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing
    Advancements By Deep Learning: A Survey") 展示了他们模型的推理过程。如图所示，他们系统中的特定注意力机制可以突出输入图像中的兴趣区域。[[194](#bib.bib194)]
    介绍了 BLOCK，一个基于超对角张量分解的双线性融合模型，用于视觉问答任务，性能达到最先进水平，代码已在 github 上公开。为了提高现有模型在不同分布测试数据上的泛化能力，[[195](#bib.bib195)]
    引入了一种自我批判的训练目标，以帮助找到与视觉/文本相关的显著视觉区域，重点是识别影响力大的对象并检测和贬低不正确的主要答案。'
- en: V-G Document Summarization
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-G 文档总结
- en: Document summarization refers to a set of problems involving generation of summary
    sentences given one or multiple documents as input.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 文档总结指的是一组涉及生成摘要句子的问题，给定一个或多个文档作为输入。
- en: 'Generally, text summarization fits into two categories:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，文本总结分为两类：
- en: '1.'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Extractive Summarization, where the goal is to identify the most salient sentences
    in the document and return them as the summary.
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提取式总结，目标是识别文档中最突出的句子，并将其作为摘要返回。
- en: '2.'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Abstractive Summarization, where the goal is to generate summary sentences from
    scratch; they may contain novel words that do not appear in the original document.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 抽象式总结，目标是从头生成摘要句子；这些句子可能包含原始文档中未出现的新词。
- en: Each of these methods has its own advantages and disadvantages. Extractive summarization
    is prone to generate long and sometimes overlapping summary sentences; however,
    the result reflects the author’s mode of expression. Abstractive methods generate
    a shorter summary but they are hard to train.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法各有优缺点。提取式总结容易生成长且有时重叠的摘要句子；然而，这种结果反映了作者的表达方式。抽象式方法生成较短的摘要，但训练起来较为困难。
- en: 'There is a vast amount of research on the topic of text summarization using
    extractive and abstractive methods. As one of the earliest works on using neural
    networks for extractive summarization,  [[196](#bib.bib196)] proposed a framework
    that used a ranking technique to extract the most salient sentences in the input.
    This model was improved by [[197](#bib.bib197)] which used a document-level encoder
    to represent sentences, and a classifier to rank these sentences. On the other
    hand, in abstractive summarization, it was  [[198](#bib.bib198)] which, for the
    first time, used attention over a sequence-to-sequence (seq2seq) model for the
    problem of headline generation. However, since simple attention models perform
    worse than extractive models, therefore more effective attention models such as
    graph-based attention [[199](#bib.bib199)] and transformers [[173](#bib.bib173)]
    have been proposed for this task. To further improve abstractive text summarization
    models,  [[200](#bib.bib200)] proposed the first pointer-generator model and applied
    it to the DeepMind QA dataset [[201](#bib.bib201)]. As a result of this work,
    the CNN/Daily Mail dataset emerged which is now one of the widely used datasets
    for the summarization task. A copy mechanism was also adopted by [[202](#bib.bib202)]
    for similar tasks. But their analysis reveals a key problem with attention-based
    encoder-decoder models: they often generate unusual summaries consisting of repeated
    phrases. Recently,  [[62](#bib.bib62)] reached state-of-the-art results on the
    abstractive text summarization using a similar framework. They alleviated the
    unnatural summaries by avoiding generating unknown tokens and replacing these
    words with tokens from the input article. Later, researchers moved their focus
    to methods that use sentence-embedding to first select the most salient sentence
    in the document and then change them to make them more abstractive [[203](#bib.bib203),
    [204](#bib.bib204)]. In these models, salient sentences are extracted first and
    then a paraphrasing model is used to make them abstractive. The extraction employs
    a sentence classifier or ranker while the abstractor tries to remove the extra
    information in a sentence and present it as a shorter summary. Fast-RL [[203](#bib.bib203)]
    is the first framework in this family of works. In Fast-RL, the extractor is pre-trained
    to select salient sentences and the abstractor is pre-trained using a pointer-generator
    model to generate paraphrases. Finally, to merge these two non-differentiable
    components, they propose using Actor-Critic Q-learning methods in which the actor
    receives a single document and generates the output while the critic evaluates
    the output based on comparison with the ground-truth summary.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用提取式和抽象式方法进行文本总结的主题上，已有大量研究。作为最早使用神经网络进行提取式总结的工作之一，[[196](#bib.bib196)] 提出了一个使用排序技术来提取输入中最显著句子的框架。该模型由[[197](#bib.bib197)]
    改进，后者使用了文档级编码器来表示句子，并用分类器对这些句子进行排名。另一方面，在抽象式总结中，[[198](#bib.bib198)] 首次使用了序列到序列（seq2seq）模型的注意力机制来解决标题生成问题。然而，由于简单的注意力模型表现不如提取式模型，因此提出了更有效的注意力模型，如基于图的注意力[[199](#bib.bib199)]和变换器[[173](#bib.bib173)]。为了进一步改进抽象文本总结模型，[[200](#bib.bib200)]
    提出了第一个指针-生成器模型，并将其应用于DeepMind QA数据集[[201](#bib.bib201)]。这项工作的结果是出现了CNN/Daily Mail数据集，该数据集现在是总结任务中广泛使用的数据集之一。[[202](#bib.bib202)]
    也采用了类似任务的复制机制。但他们的分析揭示了基于注意力的编码器-解码器模型的一个关键问题：它们通常生成由重复短语组成的不寻常总结。最近，[[62](#bib.bib62)]
    在使用类似框架的抽象文本总结中达到了最先进的结果。他们通过避免生成未知词汇并用输入文章中的词汇替代这些词汇，缓解了不自然的总结。后来，研究人员将关注点转向了使用句子嵌入的方法，首先选择文档中最显著的句子，然后进行改写以使其更具抽象性[[203](#bib.bib203),
    [204](#bib.bib204)]。在这些模型中，首先提取显著句子，然后使用改写模型使其更具抽象性。提取使用句子分类器或排名器，而抽象化器则尝试去除句子中的多余信息，并将其呈现为更简短的总结。Fast-RL[[203](#bib.bib203)]
    是这一系列工作的第一个框架。在Fast-RL中，提取器经过预训练以选择显著句子，而抽象化器则使用指针-生成器模型进行改写预训练。最后，为了将这两个不可微分的组件合并，他们提出了使用Actor-Critic
    Q学习方法，其中演员接收单个文档并生成输出，而评论员则根据与真实总结的比较评估输出。
- en: Though the standard way to evaluate the performance of summarization models
    is with ROUGE [[67](#bib.bib67)] and BLEU [[68](#bib.bib68)], there are major
    problems with such measures. For instance, the ROUGE measure focuses on the number
    of shared n-grams between two sentences. Such a method incorrectly assigns a low
    score to an abstractive summary that uses different words yet provides an excellent
    paraphrase that humans would rate highly. Clearly, better automated evaluation
    methods are needed in such cases.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管评估总结模型性能的标准方法是使用ROUGE [[67](#bib.bib67)] 和 BLEU [[68](#bib.bib68)]，但这些措施存在主要问题。例如，ROUGE
    衡量标准侧重于两个句子之间共享的 n-gram 数量。这种方法错误地将低分赋予使用不同词语但提供优秀释义的抽象总结，而人类会高度评价这种总结。显然，在这种情况下需要更好的自动化评估方法。
- en: There are additional problems with current summarization models. Shi et al. [[205](#bib.bib205)]
    provides a comprehensive survey on text summarization.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 当前总结模型还存在其他问题。Shi 等人 [[205](#bib.bib205)] 提供了关于文本总结的全面调查。
- en: '[[206](#bib.bib206)] provides a recent survey on summarization methods. [[207](#bib.bib207)]
    provides an advanced composite deep learning model, based on LSTMs and Restricted
    Boltzmann Machine, for multi-doc opinion summarization. A very influential recent
    work, [[208](#bib.bib208)], introduces Hibert ( HIerachical Bidirectional Encoder
    Representations from Transformers) as a pre-trained initialization for document
    summarization and report state-of-the-art performance.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[[206](#bib.bib206)] 提供了关于总结方法的最新调查。[[207](#bib.bib207)] 提供了一种基于 LSTM 和限制玻尔兹曼机的高级复合深度学习模型，用于多文档意见总结。一项非常有影响力的最新工作
    [[208](#bib.bib208)] 引入了 Hibert (HIerachical Bidirectional Encoder Representations
    from Transformers) 作为文档总结的预训练初始化，并报告了最先进的性能。'
- en: V-H Dialogue Systems
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-H 对话系统
- en: Dialogue Systems are quickly becoming a principal instrument in human-computer
    interaction, due in part to their promising potential and commercial value [[209](#bib.bib209)].
    One application is automated customer service, supporting both online and bricks-and-mortar
    businesses. Customers expect an ever-increasing level of speed, accuracy, and
    respect while dealing with companies and their services. Due to the high cost
    of knowledgeable human resources, companies frequently turn to intelligent conversational
    machines. Note that the phrases conversational machines and dialogue machines
    are often used interchangeably.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统迅速成为人机交互的主要工具，这部分归功于其有前景的潜力和商业价值 [[209](#bib.bib209)]。其中一个应用是自动化客户服务，支持在线和实体业务。客户期望在处理公司及其服务时能提供越来越高的速度、准确性和尊重。由于熟练人力资源的高成本，公司常常转向智能对话机器。请注意，对话机器和对话系统这两个词通常可以互换使用。
- en: 'Dialogue systems are usually task-based or non-task-based (Fig. [18](#S5.F18
    "Figure 18 ‣ V-H Dialogue Systems ‣ V Deep Learning for NLP Tasks ‣ Natural Language
    Processing Advancements By Deep Learning: A Survey")). Though there might be Automatic
    Speech Recognition (ASR) and Language-to-Speech (L2S) components in a dialogue
    system, the discussion of this section is solely about the linguistic components
    of dialogue systems; concepts associated with speech technology are ignored.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '对话系统通常是基于任务或非基于任务的 (见图 [18](#S5.F18 "Figure 18 ‣ V-H Dialogue Systems ‣ V Deep
    Learning for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey"))。虽然对话系统中可能存在自动语音识别 (ASR) 和语言到语音 (L2S) 组件，但本节讨论仅涉及对话系统的语言学组件；与语音技术相关的概念被忽略。'
- en: '![Refer to caption](img/4fba3bb9777b8d1996235adcf3378fd2.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4fba3bb9777b8d1996235adcf3378fd2.png)'
- en: 'Figure 18: The framework of a dialogue system. A dialogue system can be task
    oriented or used for natural language generation based on the user input which
    is also known as a chat bot.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：对话系统的框架。对话系统可以是面向任务的，也可以基于用户输入用于自然语言生成，这也称为聊天机器人。
- en: Despite useful statistical models employed in the backend of dialogue systems (especially
    in language understanding modules), most deployed dialogue systems rely on expensive
    hand-crafted and manual features for operation. Furthermore, the generalizability
    of these manually engineered systems to other domains and functionalities is problematic. Hence,
    recent attention has focused on deep learning for the enhancement of performance,
    generalizability, and robustness. Deep learning facilitates the creation of end-to-end
    task-oriented dialogue systems, which enriches the framework to generalize conversations
    beyond annotated task-specific dialogue resources.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在对话系统的后端（特别是在语言理解模块中）采用了有用的统计模型，但大多数部署的对话系统依赖于昂贵的手工制作和人工特征进行操作。此外，这些手工工程化系统在其他领域和功能上的通用性存在问题。因此，最近的关注点集中在深度学习上，以提高性能、通用性和鲁棒性。深度学习促进了端到端任务导向对话系统的创建，这丰富了框架，使对话超越注释的任务特定对话资源。
- en: V-H1 Task-based Systems
  id: totrans-377
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-H1 基于任务的系统
- en: 'The structure of a task-based dialogue system usually consists of the following
    elements:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 基于任务的对话系统的结构通常包括以下元素：
- en: •
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Natural Language Understanding (NLU): This component deals with understanding
    and interpreting user’s spoken context by assigning a constituent structure to
    the spoken utterance (e.g., a sentence) and captures its syntactic representation
    and semantic interpretation, to allow the back-end operation/task. NLU is usually
    leveraged regardless of the dialogue context.'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自然语言理解（NLU）：该组件通过为口语话语（例如句子）分配组成结构来处理理解和解释用户的口语上下文，并捕捉其句法表示和语义解释，以允许后端操作/任务。NLU
    通常在对话上下文中发挥作用。
- en: •
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dialogue Manager (DM): The generated representation by NLU would be handled
    by the dialogue manager, which investigates the context and returns a reasonable
    semantic-related response.'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对话管理器（DM）：由 NLU 生成的表示将由对话管理器处理，该管理器调查上下文并返回合理的语义相关响应。
- en: •
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Natural Language Generation (NLG): The natural language generation (NLG) component
    produces an utterance based on the response provided by the DM component.'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自然语言生成（NLG）：自然语言生成（NLG）组件根据 DM 组件提供的响应生成话语。
- en: 'The general pipeline is as follows: NLU module (i.e., semantic decoder) transforms
    the output of the speech recognition module to some dialogue elements. Then the
    DM processes these dialogue elements and provides a suitable response which is
    fed to the NLG for response generation. The main pipeline in NLU is to classify
    the user query domain and user intent, and fill a set of slots to create a semantic
    frame. It is usually customary to perform the intent prediction and the slot filling
    simultaneously [[210](#bib.bib210)]. Most of the task-oriented dialogue systems
    employ slot-filling approaches to classify user intent in the specific domain
    of the conversation. For this aim, having predefined tasks is required; this depends
    on manually crafted states with different associated slots. Henceforth, a designed
    dialogue system would be of limited or no use for other tasks.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 一般流程如下：NLU 模块（即语义解码器）将语音识别模块的输出转换为一些对话元素。然后，DM 处理这些对话元素并提供合适的响应，这些响应传递给 NLG
    进行响应生成。NLU 的主要流程是分类用户查询领域和用户意图，并填充一组槽位以创建语义框架。通常习惯于同时进行意图预测和槽位填充[[210](#bib.bib210)]。大多数任务导向对话系统采用槽位填充方法来分类对话的特定领域中的用户意图。为此，需要预定义任务；这依赖于不同相关槽位的手工制作状态。因此，设计的对话系统可能对其他任务的使用有限或无用。
- en: Recent task-oriented dialogue systems have been designed based on deep reinforcement
    learning, which provided promising results regarding performance [[211](#bib.bib211)], domain
    adaptation [[212](#bib.bib212)], and dialogue generation [[213](#bib.bib213)].
    This was due to a shift towards end-to-end trainable frameworks to design and
    deploy task-oriented dialogue systems. Instead of the traditionally utilized pipeline,
    an end-to-end framework incorporates and uses a single module that deals with
    external databases. Despite the tractability of end-to-end dialogue systems (i.e.,
    easy to train and simple to engineer), due to their need for interoperability
    with external databases via queries, they are not well-suited for task-oriented
    settings. Some approaches to this challenge include converting the user input
    into internal representations [[214](#bib.bib214)], combining supervised and reinforced
    learning [[215](#bib.bib215)], and extending the memory network approach [[216](#bib.bib216)]
    for question-answering to a dialog system [[217](#bib.bib217)].
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，任务导向的对话系统基于深度强化学习进行设计，这在性能[[211](#bib.bib211)]、领域适应[[212](#bib.bib212)]和对话生成[[213](#bib.bib213)]方面提供了令人满意的结果。这是由于向端到端可训练框架的转变，以设计和部署任务导向的对话系统。与传统的流水线方法不同，端到端框架集成并使用一个处理外部数据库的单一模块。尽管端到端对话系统具有可训练性（即易于训练和工程实现），由于需要通过查询与外部数据库进行互操作，它们不适合任务导向的设置。一些应对这一挑战的方法包括将用户输入转换为内部表示[[214](#bib.bib214)]、结合监督学习和强化学习[[215](#bib.bib215)]以及将记忆网络方法[[216](#bib.bib216)]扩展到问答对话系统[[217](#bib.bib217)]。
- en: V-H2 Non-task-based Systems
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-H2 非任务型系统
- en: 'As opposed to task-based dialogue systems, the goal behind designing and deploying
    non-task-based dialogue systems is to empower a machine with the ability to have
    a natural conversation with humans [[218](#bib.bib218)]. Typically, chatbots are
    of one of the following types: retrieval-based methods and generative methods.
    Retrieval-based models have access to information resources and can provide more
    concise, fluent, and accurate responses. However, they are limited regarding the
    variety of responses they can provide due to their dependency on backend data
    resources. Generative models, on the other hand, have the advantage of being able
    to produce suitable responses when such responses are not in the corpus. However,
    as opposed to retrieval-based models, they are more prone to grammatical and conceptual
    mistakes arising from their generative models.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于任务的对话系统相比，设计和部署非基于任务的对话系统的目标是赋予机器与人类进行自然对话的能力[[218](#bib.bib218)]。通常，聊天机器人可以分为以下几种类型：检索型方法和生成型方法。检索型模型可以访问信息资源，提供更简洁、流畅和准确的回应。然而，由于依赖于后端数据资源，它们在回应的多样性上受到限制。生成型模型则具有在语料库中不存在的情况下生成合适回应的优势。然而，与检索型模型相比，它们更容易产生语法和概念上的错误。
- en: Retrieval-based methods select an appropriate response from the candidate responses.
    Therefore, the key element is the query-response operation. In general, this problem
    has been formulated as a search problem and uses IR techniques for task completion [[219](#bib.bib219)].
    Retrieval-based methods usually employ either Single-turn Response Matching or
    Multi-turn Response Matching. In the first type, the current query (message) is
    solely used to select a suitable response [[220](#bib.bib220)]. The latter type
    takes the current message and previous utterances as the system input and retrieves
    a response based on the instant and temporal information. The model tries to choose
    a response which considers the whole context to guarantee conversation consistency.
    An LSTM-based model has been proposed [[221](#bib.bib221)] for context and response
    vectors creation. In [[222](#bib.bib222)], various features and multiple data
    inputs have been incorporated to be ingested using a deep learning framework.
    Current base models regarding retrieval-based chatbots rely on multi-turn response
    selection augmented by an attention mechanism and sequence matching [[223](#bib.bib223)].
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 基于检索的方法从候选回应中选择一个合适的回应。因此，关键要素是查询-回应操作。一般来说，这个问题被表述为搜索问题，并使用信息检索（IR）技术来完成任务 [[219](#bib.bib219)]。基于检索的方法通常采用单轮回应匹配或多轮回应匹配。在第一种类型中，当前的查询（消息）仅用于选择一个合适的回应 [[220](#bib.bib220)]。后一种类型将当前消息和之前的发言作为系统输入，并根据即时和时间信息检索回应。该模型尝试选择一个考虑整个上下文的回应，以保证对话的一致性。已经提出了一种基于LSTM的模型
    [[221](#bib.bib221)] 来创建上下文和回应向量。在 [[222](#bib.bib222)] 中，已经结合了各种特征和多个数据输入，通过深度学习框架进行处理。目前关于基于检索的聊天机器人，基本模型依赖于通过注意力机制和序列匹配增强的多轮回应选择 [[223](#bib.bib223)]。
- en: 'Generative models don’t assume the availability of pre-defined responses. New
    responses are produced from scratch and are based on the trained model. Generative
    models are typically based on sequence to sequence models and map an input query
    to a target element as the response. In general, designing and implementing a
    dialogue agent to be able to converse at the human level is very challenging.
    The typical approach usually consists of learning and imitating human conversation.
    For this goal, the machine is generally trained on large corpora of conversations.
    However, this does not directly remedy the issue of encountering out-of-corpus
    conversation. The question is: How can an agent be taught to generate proper responses
    to conversations that it never has seen? It must handle content that is not exactly
    available in the data corpus that the machine has been trained on, due to the
    lack of content matching between the query and the corresponding response, resulting
    from the wide range of plausible queries that humans can provide.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型不假设存在预定义的回应。新的回应是从头生成的，并且基于训练的模型。生成模型通常基于序列到序列模型，将输入查询映射到目标元素作为回应。一般来说，设计和实现一个能够以人类水平对话的对话代理是非常具有挑战性的。典型的方法通常包括学习和模仿人类对话。为了这个目标，机器通常在大量对话语料库上进行训练。然而，这并不能直接解决遇到语料库外对话的问题。问题是：如何教导一个代理生成对其从未见过的对话的适当回应？它必须处理在机器训练的数据语料库中没有完全匹配的内容，由于人类可以提供的查询范围广泛，导致查询和对应回应之间缺乏内容匹配。
- en: 'To tackle the aforementioned general problem, some fundamental questions must
    be answered: (1) What are the core characteristics of a natural conversation?
    (2) How can these characteristics be measured? (3) How can we incorporate this
    knowledge in a machine, i.e., the dialogue system? Effective integration of these
    three elements determines the intelligence of a machine. A qualitative criterion
    is to observe if the generated utterances can be distinguished from natural human
    dialogues. For quantitative evaluation, adversarial evaluation was initially used
    for quality assessment of sentence generation [[224](#bib.bib224)] and employed
    for quality evaluation of dialogue systems [[225](#bib.bib225)]. Recent advancements
    in sequence to sequence modeling encouraged many research efforts regarding natural
    language generation [[226](#bib.bib226)]. Furthermore, deep reinforcement learning
    yields promising performance in natural language generation [[213](#bib.bib213)].'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 为解决上述的一般问题，必须回答一些基本问题：（1）自然对话的核心特征是什么？（2）这些特征如何被衡量？（3）我们如何将这些知识融入到机器中，即对话系统？有效地整合这三要素决定了机器的智能水平。一个定性标准是观察生成的发言是否能与自然的人类对话区分开来。为了进行定量评估，最初采用了对抗性评估来评估句子生成的质量[[224](#bib.bib224)]，并用于对话系统的质量评估[[225](#bib.bib225)]。序列到序列建模的最新进展鼓励了许多关于自然语言生成的研究[[226](#bib.bib226)]。此外，深度强化学习在自然语言生成中表现出良好的性能[[213](#bib.bib213)]。
- en: V-H3 Final note on dialogue systems
  id: totrans-392
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-H3 对话系统的最终说明
- en: Despite remarkable advancements in AI and much attention dedicated to dialogue
    systems, in reality, successful commercial tools, such as Apple’s Siri and Amazon’s
    Alexa, still heavily rely on handcrafted features. It still is very challenging
    to design and train data-driven dialogue machines given the complexity of the
    natural language, the difficulties in framework design, and the complex nature
    of available data sources.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在AI领域取得了显著进展，并且大量关注集中在对话系统上，但实际上，成功的商业工具，如苹果的Siri和亚马逊的Alexa，仍然严重依赖手工特征。鉴于自然语言的复杂性、框架设计的困难以及可用数据源的复杂性质，设计和训练数据驱动的对话机器仍然非常具有挑战性。
- en: VI Conclusion
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: In this article, we presented a comprehensive survey of the most distinguished
    works in Natural Language Processing using deep learning. We provided a categorized
    context for introducing different NLP core concepts, aspects, and applications,
    and emphasized the most significant conducted research efforts in each associated
    category. Deep learning and NLP are two of the most rapidly developing research
    topics nowadays. Due to this rapid progress, it is hoped that soon, new effective
    models will supersede the current state-of-the-art approaches. This may cause
    some of the references provided in the survey to become dated, but those are likely
    to be cited by new publications that describe improved methods
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们提供了一个关于使用深度学习的自然语言处理领域的最杰出工作的综合调查。我们提供了一个分类背景，用于介绍不同的NLP核心概念、方面和应用，并强调了每个相关类别中最重要的研究工作。深度学习和NLP是当前发展最快的研究主题之一。由于这种快速进展，希望不久之后，新有效的模型将取代当前的最先进方法。这可能会导致调查中提供的一些参考文献变得过时，但这些文献可能会被描述改进方法的新出版物引用。
- en: Neverthless, one of the essential characteristics of this survey is its educational
    aspect, which provides a precise understanding of the critical elements of this
    field and explains the most notable research works. Hopefully, this survey will
    guide students and researchers with essential resources, both to learn what is
    necessary to know, and to advance further the integration of NLP with deep learning.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，本调查的一个重要特征是其教育方面，它提供了对该领域关键要素的准确理解，并解释了最显著的研究工作。希望本调查能为学生和研究人员提供必要的资源，以了解所需知识，并进一步推动NLP与深度学习的融合。
- en: References
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] C. D. Manning, C. D. Manning, and H. Schütze, Foundations of statistical
    natural language processing. MIT Press, 1999.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] C. D. Manning, C. D. Manning, and H. Schütze, Foundations of statistical
    natural language processing. MIT Press, 1999.'
- en: '[2] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional networks
    for text classification,” in Advances in neural information processing systems,
    pp. 649–657, 2015.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional networks
    for text classification,” in Advances in neural information processing systems,
    pp. 649–657, 2015.'
- en: '[3] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using RNN encoder-decoder for
    statistical machine translation,” arXiv preprint arXiv:1406.1078, 2014.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    和 Y. Bengio，“使用 RNN 编码器-解码器学习短语表示用于统计机器翻译”，arXiv 预印本 arXiv:1406.1078，2014年。'
- en: '[4] S. Wu, K. Roberts, S. Datta, J. Du, Z. Ji, Y. Si, S. Soni, Q. Wang, Q. Wei,
    Y. Xiang, B. Zhao, and H. Xu, “Deep learning in clinical natural language processing:
    a methodical review,” Journal of the American Medical Informatics Association,
    vol. 27, pp. 457–470, mar 2020.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Wu, K. Roberts, S. Datta, J. Du, Z. Ji, Y. Si, S. Soni, Q. Wang, Q.
    Wei, Y. Xiang, B. Zhao, 和 H. Xu，“临床自然语言处理中的深度学习：方法论综述”，《美国医学信息学协会杂志》，第27卷，第457–470页，2020年3月。'
- en: '[5] R. Collobert and J. Weston, “A unified architecture for natural language
    processing: Deep neural networks with multitask learning,” in Proceedings of the
    25th international conference on Machine learning, pp. 160–167, ACM, 2008.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] R. Collobert 和 J. Weston，“自然语言处理的统一架构：具有多任务学习的深度神经网络”，在《第25届国际机器学习会议论文集》，第160–167页，ACM，2008年。'
- en: '[6] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei,
    “Large-scale video classification with convolutional neural networks,” in Proceedings
    of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1725–1732,
    2014.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, 和 L. Fei-Fei，“使用卷积神经网络进行大规模视频分类”，在《IEEE计算机视觉与模式识别会议论文集》，第1725–1732页，2014年。'
- en: '[7] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring
    mid-level image representations using convolutional neural networks,” in Proceedings
    of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1717–1724,
    2014.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Oquab, L. Bottou, I. Laptev, 和 J. Sivic，“使用卷积神经网络学习和迁移中层图像表示”，在《IEEE计算机视觉与模式识别会议论文集》，第1717–1724页，2014年。'
- en: '[8] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb,
    “Learning from simulated and unsupervised images through adversarial training,”
    in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 2107–2116, 2017.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, 和 R. Webb，“通过对抗训练从模拟和无监督图像中学习”，在《IEEE计算机视觉与模式识别会议论文集》，第2107–2116页，2017年。'
- en: '[9] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, “Deep Learning
    for Computer Vision: A Brief Review,” Computational Intelligence and Neuroscience,
    Feb 2018.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Voulodimos, N. Doulamis, A. Doulamis, 和 E. Protopapadakis，“计算机视觉中的深度学习：简要综述”，《计算智能与神经科学》，2018年2月。'
- en: '[10] N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V. Hernandez,
    L. Krpalkova, D. Riordan, and J. Walsh, “Deep learning vs. traditional computer
    vision,” in Advances in Computer Vision (K. Arai and S. Kapoor, eds.), (Cham),
    pp. 128–144, Springer International Publishing, 2020.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V. Hernandez,
    L. Krpalkova, D. Riordan, 和 J. Walsh，“深度学习与传统计算机视觉”，在《计算机视觉进展》（K. Arai 和 S. Kapoor
    编），（Cham），第128–144页，Springer International Publishing，2020年。'
- en: '[11] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with recurrent
    neural networks,” in International Conference on Machine Learning, pp. 1764–1772,
    2014.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Graves 和 N. Jaitly，“朝着端到端语音识别的递归神经网络”，在国际机器学习会议， 第1764–1772页，2014年。'
- en: '[12] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case,
    J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al., “Deep speech 2: End-to-end
    speech recognition in English and Mandarin,” in ICML, pp. 173–182, 2016.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C.
    Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, 等，“深度语音2：英语和普通话的端到端语音识别”，在 ICML，第173–182页，2016年。'
- en: '[13] U. Kamath, J. Liu, and J. Whitaker, Deep learning for NLP and speech recognition,
    vol. 84. Springer, 2019.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] U. Kamath, J. Liu, 和 J. Whitaker，《用于 NLP 和语音识别的深度学习》，第84卷。Springer，2019年。'
- en: '[14] C. D. Santos and B. Zadrozny, “Learning character-level representations
    for part-of-speech tagging,” in Proceedings of the 31st International Conference
    on Machine Learning (ICML-14), pp. 1818–1826, 2014.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] C. D. Santos 和 B. Zadrozny，“为词性标注学习字符级表示”，在《第31届国际机器学习会议（ICML-14）论文集》，第1818–1826页，2014年。'
- en: '[15] B. Plank, A. Søgaard, and Y. Goldberg, “Multilingual part-of-speech tagging
    with bidirectional long short-term memory models and auxiliary loss,” arXiv preprint
    arXiv:1604.05529, 2016.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] B. Plank, A. Søgaard, 和 Y. Goldberg，“使用双向长短期记忆模型和辅助损失进行多语言词性标注”，arXiv
    预印本 arXiv:1604.05529，2016年。'
- en: '[16] C. D. Manning, “Part-of-speech tagging from 97% to 100%: is it time for
    some linguistics?,” in International Conference on Intelligent Text Processing
    and Computational Linguistics, pp. 171–189, Springer, 2011.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] C. D. Manning, “词性标注从97%到100%：是否该引入一些语言学？” 载于智能文本处理与计算语言学国际会议论文集，第171–189页,
    Springer, 2011。'
- en: '[17] R. D. Deshmukh and A. Kiwelekar, “Deep learning techniques for part of
    speech tagging by natural language processing,” in 2020 2nd International Conference
    on Innovative Mechanisms for Industry Applications (ICIMIA), pp. 76–81, IEEE,
    2020.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] R. D. Deshmukh 和 A. Kiwelekar, “自然语言处理中的词性标注深度学习技术，” 载于2020年第2届创新机制工业应用国际会议（ICIMIA）论文集，第76–81页,
    IEEE, 2020。'
- en: '[18] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer, “Neural
    architectures for named entity recognition,” arXiv preprint arXiv:1603.01360,
    2016.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, 和 C. Dyer, “命名实体识别的神经网络架构，”
    arXiv 预印本 arXiv:1603.01360, 2016。'
- en: '[19] J. P. Chiu and E. Nichols, “Named entity recognition with bidirectional
    LSTM-CNNs,” arXiv preprint arXiv:1511.08308, 2015.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. P. Chiu 和 E. Nichols, “使用双向 LSTM-CNNs 的命名实体识别，” arXiv 预印本 arXiv:1511.08308,
    2015。'
- en: '[20] V. Yadav and S. Bethard, “A survey on recent advances in named entity
    recognition from deep learning models,” arXiv preprint arXiv:1910.11470, 2019.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] V. Yadav 和 S. Bethard, “深度学习模型在命名实体识别中的近期进展综述，” arXiv 预印本 arXiv:1910.11470,
    2019。'
- en: '[21] J. Li, A. Sun, J. Han, and C. Li, “A survey on deep learning for named
    entity recognition,” IEEE Transactions on Knowledge and Data Engineering, 2020.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Li, A. Sun, J. Han, 和 C. Li, “深度学习在命名实体识别中的应用综述，” IEEE 知识与数据工程汇刊, 2020。'
- en: '[22] J. Zhou and W. Xu, “End-to-end learning of semantic role labeling using
    recurrent neural networks,” in Proceedings of the 53rd Annual Meeting of the Association
    for Computational Linguistics and the 7th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers), vol. 1, pp. 1127–1137, 2015.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J. Zhou 和 W. Xu, “使用递归神经网络的端到端语义角色标注学习，” 载于第53届计算语言学协会年会及第7届国际自然语言处理联合会议（第1卷：长篇论文）的论文集，第1卷，第1127–1137页,
    2015。'
- en: '[23] D. Marcheggiani, A. Frolov, and I. Titov, “A simple and accurate syntax-agnostic
    neural model for dependency-based semantic role labeling,” arXiv preprint arXiv:1701.02593,
    2017.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] D. Marcheggiani, A. Frolov, 和 I. Titov, “一种简单而准确的无语法依赖神经模型用于基于依赖的语义角色标注，”
    arXiv 预印本 arXiv:1701.02593, 2017。'
- en: '[24] L. He, K. Lee, M. Lewis, and L. Zettlemoyer, “Deep semantic role labeling:
    What works and what’s next,” in Proceedings of the 55th Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers), vol. 1, pp. 473–483,
    2017.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] L. He, K. Lee, M. Lewis, 和 L. Zettlemoyer, “深度语义角色标注：有效的方法及未来方向，” 载于第55届计算语言学协会年会（第1卷：长篇论文）的论文集，第1卷，第473–483页,
    2017。'
- en: '[25] S. He, Z. Li, and H. Zhao, “Syntax-aware multilingual semantic role labeling,”
    arXiv preprint arXiv:1909.00310, 2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. He, Z. Li, 和 H. Zhao, “语法感知的多语言语义角色标注，” arXiv 预印本 arXiv:1909.00310,
    2019。'
- en: '[26] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in deep
    learning based natural language processing,” IEEE Computational Intelligence Magazine,
    vol. 13, no. 3, pp. 55–75, 2018.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] T. Young, D. Hazarika, S. Poria, 和 E. Cambria, “基于深度学习的自然语言处理的最新趋势，” IEEE
    计算智能杂志，第13卷，第3期，第55–75页, 2018。'
- en: '[27] Y. Kang, Z. Cai, C.-W. Tan, Q. Huang, and H. Liu, “Natural language processing
    (NLP) in management research: A literature review,” Journal of Management Analytics,
    vol. 7, pp. 139–172, apr 2020.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Y. Kang, Z. Cai, C.-W. Tan, Q. Huang, 和 H. Liu, “管理研究中的自然语言处理（NLP）：文献综述，”
    管理分析杂志，第7卷，第139–172页, 2020年4月。'
- en: '[28] T. Greenwald, “What exactly is artificial intelligence, anyway?.” [https://www.wsj.com/articles/what-exactly-is-artificial-intelligence-anyway-1525053960](https://www.wsj.com/articles/what-exactly-is-artificial-intelligence-anyway-1525053960),
    April 2018. Wall Street Journal Online Article.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] T. Greenwald, “究竟什么是人工智能？” [https://www.wsj.com/articles/what-exactly-is-artificial-intelligence-anyway-1525053960](https://www.wsj.com/articles/what-exactly-is-artificial-intelligence-anyway-1525053960),
    2018年4月。《华尔街日报》在线文章。'
- en: '[29] U. Sivarajah, M. M. Kamal, Z. Irani, and V. Weerakkody, “Critical analysis
    of big data challenges and analytical methods,” Journal of Business Research,
    vol. 70, pp. 263–286, 2017.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] U. Sivarajah, M. M. Kamal, Z. Irani, 和 V. Weerakkody, “大数据挑战及分析方法的关键分析，”
    商业研究杂志，第70卷，第263–286页, 2017。'
- en: '[30] Z. C. Lipton, J. Berkowitz, and C. Elkan, “A critical review of recurrent
    neural networks for sequence learning,” arXiv preprint arXiv:1506.00019, 2015.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Z. C. Lipton, J. Berkowitz, 和 C. Elkan, “递归神经网络在序列学习中的关键评估，” arXiv 预印本
    arXiv:1506.00019, 2015。'
- en: '[31] Y. Kim, “Convolutional neural networks for sentence classification,” arXiv
    preprint arXiv:1408.5882, 2014.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y. Kim，“用于句子分类的卷积神经网络，” arXiv预印本 arXiv:1408.5882，2014年。'
- en: '[32] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng, “Parsing natural scenes
    and natural language with recursive neural networks,” in Proceedings of the 28th
    international conference on machine learning (ICML-11), pp. 129–136, 2011.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] R. Socher, C. C. Lin, C. Manning, 和 A. Y. Ng，“使用递归神经网络解析自然场景和自然语言，” 收录于第28届国际机器学习会议（ICML-11）论文集，pp. 129–136，2011年。'
- en: '[33] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in Advances in neural information processing
    systems, pp. 1097–1105, 2012.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton，“使用深度卷积神经网络的Imagenet分类，” 收录于神经信息处理系统进展，pp. 1097–1105，2012年。'
- en: '[34] C. dos Santos and M. Gatti, “Deep convolutional neural networks for sentiment
    analysis of short texts,” in Proceedings of COLING 2014, the 25th International
    Conference on Computational Linguistics: Technical Papers, pp. 69–78, 2014.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] C. dos Santos 和 M. Gatti，“用于短文本情感分析的深度卷积神经网络，” 收录于COLING 2014，第25届国际计算语言学会议：技术论文集，pp. 69–78，2014年。'
- en: '[35] R. Johnson and T. Zhang, “Effective use of word order for text categorization
    with convolutional neural networks,” arXiv preprint arXiv:1412.1058, 2014.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] R. Johnson 和 T. Zhang，“利用卷积神经网络有效使用词序进行文本分类，” arXiv预印本 arXiv:1412.1058，2014年。'
- en: '[36] R. Johnson and T. Zhang, “Semi-supervised convolutional neural networks
    for text categorization via region embedding,” in Advances in neural information
    processing systems, pp. 919–927, 2015.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] R. Johnson 和 T. Zhang，“通过区域嵌入进行文本分类的半监督卷积神经网络，” 收录于神经信息处理系统进展，pp. 919–927，2015年。'
- en: '[37] D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao, “Relation classification
    via convolutional deep neural network,” in Proceedings of COLING 2014, the 25th
    International Conference on Computational Linguistics: Technical Papers, pp. 2335–2344,
    2014.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] D. Zeng, K. Liu, S. Lai, G. Zhou, 和 J. Zhao，“通过卷积深度神经网络进行关系分类，” 收录于COLING
    2014，第25届国际计算语言学会议：技术论文集，pp. 2335–2344，2014年。'
- en: '[38] T. H. Nguyen and R. Grishman, “Relation extraction: Perspective from convolutional
    neural networks,” in Proceedings of the 1st Workshop on Vector Space Modeling
    for Natural Language Processing, pp. 39–48, 2015.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] T. H. Nguyen 和 R. Grishman，“关系提取：卷积神经网络的视角，” 收录于第1届自然语言处理向量空间建模研讨会论文集，pp. 39–48，2015年。'
- en: '[39] T. Mikolov, M. Karafiát, L. Burget, J. Černockỳ, and S. Khudanpur, “Recurrent
    neural network based language model,” in Eleventh Annual Conference of the International
    Speech Communication Association, 2010.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] T. Mikolov, M. Karafiát, L. Burget, J. Černockỳ, 和 S. Khudanpur，“基于递归神经网络的语言模型，”
    收录于第十一届国际语音通信协会年会，2010年。'
- en: '[40] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] S. Hochreiter 和 J. Schmidhuber，“长短期记忆，” 《神经计算》，第9卷，第8期，pp. 1735–1780，1997年。'
- en: '[41] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in Advances in neural
    information processing systems, pp. 2672–2680, 2014.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, 和 Y. Bengio，“生成对抗网络，” 收录于神经信息处理系统进展，pp. 2672–2680，2014年。'
- en: '[42] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv preprint
    arXiv:1701.07875, 2017.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. Arjovsky, S. Chintala, 和 L. Bottou，“Wasserstein GAN，” arXiv预印本 arXiv:1701.07875，2017年。'
- en: '[43] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel,
    “Infogan: Interpretable representation learning by information maximizing generative
    adversarial nets,” in Advances in neural information processing systems, pp. 2172–2180,
    2016.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, 和 P. Abbeel，“Infogan：通过信息最大化生成对抗网络进行可解释表示学习，”
    收录于神经信息处理系统进展，pp. 2172–2180，2016年。'
- en: '[44] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
    with deep convolutional generative adversarial networks,” arXiv preprint arXiv:1511.06434,
    2015.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] A. Radford, L. Metz, 和 S. Chintala，“通过深度卷积生成对抗网络进行无监督表示学习，” arXiv预印本 arXiv:1511.06434，2015年。'
- en: '[45] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of
    GANs for improved quality, stability, and variation,” arXiv preprint arXiv:1710.10196,
    2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] T. Karras, T. Aila, S. Laine, 和 J. Lehtinen，“为改进质量、稳定性和变异性而逐步增长的GAN，”
    arXiv预印本 arXiv:1710.10196，2017年。'
- en: '[46] N. Tavaf, A. Torfi, K. Ugurbil, and P.-F. Van de Moortele, “GRAPPA-GANs
    for Parallel MRI Reconstruction,” arXiv preprint arXiv:2101.03135, Jan 2021.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] N. Tavaf, A. Torfi, K. Ugurbil, 和 P.-F. Van de Moortele，“GRAPPA-GANs用于并行MRI重建，”arXiv预印本arXiv:2101.03135，2021年1月。'
- en: '[47] L. Yu, W. Zhang, J. Wang, and Y. Yu, “Seqgan: Sequence generative adversarial
    nets with policy gradient,” in Thirty-First AAAI Conference on Artificial Intelligence,
    2017.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] L. Yu, W. Zhang, J. Wang, 和 Y. Yu，“Seqgan：带有策略梯度的序列生成对抗网络，”在第三十一届AAAI人工智能会议上，2017年。'
- en: '[48] J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky, “Adversarial
    learning for neural dialogue generation,” arXiv preprint arXiv:1701.06547, 2017.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, 和 D. Jurafsky，“用于神经对话生成的对抗学习，”arXiv预印本arXiv:1701.06547，2017年。'
- en: '[49] B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up?: sentiment classification
    using machine learning techniques,” in Proceedings of the ACL-02 conference on
    Empirical methods in natural language processing-Volume 10, pp. 79–86, Association
    for Computational Linguistics, 2002.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] B. Pang, L. Lee, 和 S. Vaithyanathan，“好评如潮？：使用机器学习技术进行情感分类，”在ACL-02会议的自然语言处理实证方法第10卷，第79–86页，计算语言学协会，2002年。'
- en: '[50] Z. S. Harris, “Distributional structure,” Word, vol. 10, no. 2-3, pp. 146–162,
    1954.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Z. S. Harris，“分布式结构，”《Word》，第10卷，第2-3期，第146–162页，1954年。'
- en: '[51] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic
    language model,” Journal of machine learning research, vol. 3, no. Feb., pp. 1137–1155,
    2003.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. Bengio, R. Ducharme, P. Vincent, 和 C. Jauvin，“一个神经概率语言模型，”《机器学习研究杂志》，第3卷，2月号，第1137–1155页，2003年。'
- en: '[52] Q. Le and T. Mikolov, “Distributed representations of sentences and documents,”
    in International Conference on Machine Learning, pp. 1188–1196, 2014.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Q. Le 和 T. Mikolov，“句子和文档的分布式表示，”在国际机器学习会议上，第1188–1196页，2014年。'
- en: '[53] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed
    representations of words and phrases and their compositionality,” in Advances
    in neural information processing systems, pp. 3111–3119, 2013.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, 和 J. Dean，“词语和短语的分布式表示及其组合性，”在神经信息处理系统进展中，第3111–3119页，2013年。'
- en: '[54] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba,
    and S. Fidler, “Skip-thought vectors,” in Advances in neural information processing
    systems, pp. 3294–3302, 2015.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba,
    和 S. Fidler，“跳跃思想向量，”在神经信息处理系统进展中，第3294–3302页，2015年。'
- en: '[55] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of
    word representations in vector space,” arXiv preprint arXiv:1301.3781, 2013.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] T. Mikolov, K. Chen, G. Corrado, 和 J. Dean，“在向量空间中高效估计词语表示，”arXiv预印本arXiv:1301.3781，2013年。'
- en: '[56] G. Lebanon et al., Riemannian geometry and statistical machine learning.
    LAP LAMBERT Academic Publishing, 2015.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] G. Lebanon 等，《黎曼几何与统计机器学习》。LAP LAMBERT学术出版，2015年。'
- en: '[57] J. Leskovec, A. Rajaraman, and J. D. Ullman, Mining of massive datasets.
    Cambridge University Press, 2014.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] J. Leskovec, A. Rajaraman, 和 J. D. Ullman，《大规模数据集挖掘》。剑桥大学出版社，2014年。'
- en: '[58] Y. Goldberg, “Neural network methods for natural language processing,”
    Synthesis Lectures on Human Language Technologies, vol. 10, no. 1, pp. 1–309,
    2017.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. Goldberg，“自然语言处理的神经网络方法，”《人类语言技术合成讲座》，第10卷，第1期，第1–309页，2017年。'
- en: '[59] J. Wehrmann, W. Becker, H. E. Cagnini, and R. C. Barros, “A character-based
    convolutional neural network for language-agnostic Twitter sentiment analysis,”
    in Neural Networks (IJCNN), 2017 International Joint Conference on, pp. 2384–2391,
    IEEE, 2017.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] J. Wehrmann, W. Becker, H. E. Cagnini, 和 R. C. Barros，“用于语言无关的Twitter情感分析的字符级卷积神经网络，”在2017年国际联合会议上的神经网络（IJCNN）中，第2384–2391页，IEEE，2017年。'
- en: '[60] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word vectors
    with subword information,” arXiv preprint arXiv:1607.04606, 2016.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] P. Bojanowski, E. Grave, A. Joulin, 和 T. Mikolov，“用子词信息丰富词向量，”arXiv预印本arXiv:1607.04606，2016年。'
- en: '[61] J. Botha and P. Blunsom, “Compositional morphology for word representations
    and language modelling,” in International Conference on Machine Learning, pp. 1899–1907,
    2014.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] J. Botha 和 P. Blunsom，“用于词语表示和语言建模的组合形态学，”在国际机器学习会议上，第1899–1907页，2014年。'
- en: '[62] A. See, P. J. Liu, and C. D. Manning, “Get to the point: Summarization
    with pointer-generator networks,” in ACL, vol. 1, pp. 1073–1083, 2017.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] A. See, P. J. Liu, 和 C. D. Manning，“直截了当：使用指针-生成网络的摘要，”在ACL会议上，第1卷，第1073–1083页，2017年。'
- en: '[63] R. Paulus, C. Xiong, and R. Socher, “A deep reinforced model for abstractive
    summarization,” arXiv preprint arXiv:1705.04304, 2017.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] R. Paulus, C. Xiong, 和 R. Socher，“一种用于抽象摘要的深度强化模型”，arXiv预印本 arXiv:1705.04304，2017年。'
- en: '[64] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled sampling
    for sequence prediction with recurrent neural networks,” in Advances in Neural
    Information Processing Systems, pp. 1171–1179, 2015.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] S. Bengio, O. Vinyals, N. Jaitly, 和 N. Shazeer，“递归神经网络的序列预测调度采样”，发表于《神经信息处理系统进展》，第1171–1179页，2015年。'
- en: '[65] K. Goyal, G. Neubig, C. Dyer, and T. Berg-Kirkpatrick, “A continuous relaxation
    of beam search for end-to-end training of neural sequence models,” in Thirty-Second
    AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] K. Goyal, G. Neubig, C. Dyer, 和 T. Berg-Kirkpatrick，“神经序列模型的端到端训练的连续松弛束搜索”，发表于《第三十二届AAAI人工智能会议》，2018年。'
- en: '[66] W. Kool, H. Van Hoof, and M. Welling, “Stochastic beams and where to find
    them: The gumbel-top-k trick for sampling sequences without replacement,” in International
    Conference on Machine Learning, pp. 3499–3508, 2019.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] W. Kool, H. Van Hoof, 和 M. Welling，“随机束及其发现：用于不重复采样序列的Gumbel-top-k技巧”，发表于《国际机器学习会议》，第3499–3508页，2019年。'
- en: '[67] C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,” in
    Text summarization branches out, pp. 74–81, 2004.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] C.-Y. Lin，“Rouge：自动评估摘要的工具包”，发表于《文本摘要分支》，第74–81页，2004年。'
- en: '[68] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method for automatic
    evaluation of machine translation,” in Proceedings of the 40th annual meeting
    on Association for Computational Linguistics, pp. 311–318, Association for Computational
    Linguistics, 2002.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] K. Papineni, S. Roukos, T. Ward, 和 W.-J. Zhu，“BLEU：一种自动评估机器翻译的方法”，发表于《第40届计算语言学协会年会论文集》，第311–318页，计算语言学协会，2002年。'
- en: '[69] S. Banerjee and A. Lavie, “METEOR: An automatic metric for MT evaluation
    with improved correlation with human judgments,” in Proceedings of the ACL workshop
    on intrinsic and extrinsic evaluation measures for machine translation and/or
    summarization, pp. 65–72, 2005.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] S. Banerjee 和 A. Lavie，“METEOR：一种用于机器翻译评估的自动度量，与人工评估的相关性提高”，发表于《ACL机器翻译和/或摘要内在与外在评估测度研讨会论文集》，第65–72页，2005年。'
- en: '[70] Y. Keneshloo, T. Shi, C. K. Reddy, and N. Ramakrishnan, “Deep reinforcement
    learning for sequence to sequence models,” arXiv preprint arXiv:1805.09461, 2018.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Y. Keneshloo, T. Shi, C. K. Reddy, 和 N. Ramakrishnan，“序列到序列模型的深度强化学习”，arXiv预印本
    arXiv:1805.09461，2018年。'
- en: '[71] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence level training
    with recurrent neural networks,” arXiv preprint arXiv:1511.06732, 2015.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. Ranzato, S. Chopra, M. Auli, 和 W. Zaremba，“使用递归神经网络的序列级训练”，arXiv预印本
    arXiv:1511.06732，2015年。'
- en: '[72] W. Zaremba and I. Sutskever, “Reinforcement learning neural Turing machines-revised,”
    arXiv preprint arXiv:1505.00521, 2015.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] W. Zaremba 和 I. Sutskever，“强化学习神经图灵机——修订版”，arXiv预印本 arXiv:1505.00521，2015年。'
- en: '[73] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” in Reinforcement Learning, pp. 5–32, Springer,
    1992.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] R. J. Williams，“用于联结主义强化学习的简单统计梯度跟随算法”，发表于《强化学习》，第5–32页，Springer，1992年。'
- en: '[74] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.
    MIT Press, 2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] R. S. Sutton 和 A. G. Barto，《强化学习：介绍》。MIT出版社，2018年。'
- en: '[75] C. J. Watkins and P. Dayan, “Q-learning,” Machine Learning, vol. 8, no. 3-4,
    pp. 279–292, 1992.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] C. J. Watkins 和 P. Dayan，“Q学习”，《机器学习》，第8卷，第3-4期，第279–292页，1992年。'
- en: '[76] H. Daumé, J. Langford, and D. Marcu, “Search-based structured prediction,”
    Machine learning, vol. 75, no. 3, pp. 297–325, 2009.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] H. Daumé, J. Langford, 和 D. Marcu，“基于搜索的结构化预测”，《机器学习》，第75卷，第3期，第297–325页，2009年。'
- en: '[77] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
    deep visuomotor policies,” The Journal of Machine Learning Research, vol. 17,
    no. 1, pp. 1334–1373, 2016.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] S. Levine, C. Finn, T. Darrell, 和 P. Abbeel，“深度视觉运动策略的端到端训练”，《机器学习研究杂志》，第17卷，第1期，第1334–1373页，2016年。'
- en: '[78] V. Mnih, N. Heess, A. Graves, et al., “Recurrent models of visual attention,”
    in Advances in neural information processing systems, pp. 2204–2212, 2014.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] V. Mnih, N. Heess, A. Graves 等，“视觉注意的递归模型”，发表于《神经信息处理系统进展》，第2204–2212页，2014年。'
- en: '[79] C. Sun, L. Huang, and X. Qiu, “Utilizing BERT for aspect-based sentiment
    analysis via constructing auxiliary sentence,” arXiv preprint arXiv:1903.09588,
    2019.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] C. Sun, L. Huang, 和 X. Qiu，“利用BERT通过构造辅助句进行基于方面的情感分析”，arXiv预印本 arXiv:1903.09588，2019年。'
- en: '[80] P. Resnik and J. Lin, “Evaluation of NLP systems,” The handbook of computational
    linguistics and natural language processing, vol. 57, pp. 271–295, 2010.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] P. Resnik 和 J. Lin, “NLP 系统的评估，” 计算语言学和自然语言处理手册, vol. 57, pp. 271–295,
    2010。'
- en: '[81] A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury, I. Gulrajani,
    V. Zhong, R. Paulus, and R. Socher, “Ask me anything: Dynamic memory networks
    for natural language processing,” in International Conference on Machine Learning,
    pp. 1378–1387, 2016.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury, I. Gulrajani,
    V. Zhong, R. Paulus, 和 R. Socher, “问我任何事：用于自然语言处理的动态记忆网络，” 在国际机器学习会议中, pp. 1378–1387,
    2016。'
- en: '[82] Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF models for sequence
    tagging,” arXiv preprint arXiv:1508.01991, 2015.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Z. Huang, W. Xu, 和 K. Yu, “用于序列标注的双向 LSTM-CRF 模型，” arXiv 预印本 arXiv:1508.01991,
    2015。'
- en: '[83] D. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev, S. Petrov,
    and M. Collins, “Globally normalized transition-based neural networks,” arXiv
    preprint arXiv:1603.06042, 2016.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] D. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev, S.
    Petrov, 和 M. Collins, “全局归一化转移基神经网络，” arXiv 预印本 arXiv:1603.06042, 2016。'
- en: '[84] X. Xue and J. Zhang, “Part-of-speech tagging of building codes empowered
    by deep learning and transformational rules,” Advanced Engineering Informatics,
    vol. 47, p. 101235, 2021.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] X. Xue 和 J. Zhang, “基于深度学习和变换规则的建筑代码词性标注，” 高级工程信息学, vol. 47, p. 101235,
    2021。'
- en: '[85] L. Liu, J. Shang, X. Ren, F. F. Xu, H. Gui, J. Peng, and J. Han, “Empower
    sequence labeling with task-aware neural language model,” in Thirty-Second AAAI
    Conference on Artificial Intelligence, 2018.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] L. Liu, J. Shang, X. Ren, F. F. Xu, H. Gui, J. Peng, 和 J. Han, “通过任务感知神经语言模型增强序列标注，”
    在第三十二届 AAAI 人工智能大会中, 2018。'
- en: '[86] Z. Yang, R. Salakhutdinov, and W. W. Cohen, “Transfer learning for sequence
    tagging with hierarchical recurrent networks,” arXiv preprint arXiv:1703.06345,
    2017.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Z. Yang, R. Salakhutdinov, 和 W. W. Cohen, “使用层次递归网络的序列标注迁移学习，” arXiv 预印本
    arXiv:1703.06345, 2017。'
- en: '[87] X. Ma and E. Hovy, “End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF,”
    arXiv preprint arXiv:1603.01354, 2016.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] X. Ma 和 E. Hovy, “通过双向 LSTM-CNN-CRF 的端到端序列标注，” arXiv 预印本 arXiv:1603.01354,
    2016。'
- en: '[88] M. Yasunaga, J. Kasai, and D. Radev, “Robust multilingual part-of-speech
    tagging via adversarial training,” arXiv preprint arXiv:1711.04903, 2017.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] M. Yasunaga, J. Kasai, 和 D. Radev, “通过对抗训练实现稳健的多语言词性标注，” arXiv 预印本 arXiv:1711.04903,
    2017。'
- en: '[89] W. Ling, T. Luís, L. Marujo, R. F. Astudillo, S. Amir, C. Dyer, A. W.
    Black, and I. Trancoso, “Finding function in form: Compositional character models
    for open vocabulary word representation,” arXiv preprint arXiv:1508.02096, 2015.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] W. Ling, T. Luís, L. Marujo, R. F. Astudillo, S. Amir, C. Dyer, A. W.
    Black, 和 I. Trancoso, “在形式中发现功能：用于开放词汇词表示的组合字符模型，” arXiv 预印本 arXiv:1508.02096,
    2015。'
- en: '[90] A. Akbik, D. Blythe, and R. Vollgraf, “Contextual string embeddings for
    sequence labeling,” in Proceedings of the 27th International Conference on Computational
    Linguistics, pp. 1638–1649, 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. Akbik, D. Blythe, 和 R. Vollgraf, “用于序列标注的上下文字符串嵌入，” 在第 27 届国际计算语言学会议中,
    pp. 1638–1649, 2018。'
- en: '[91] B. Bohnet, R. McDonald, G. Simoes, D. Andor, E. Pitler, and J. Maynez,
    “Morphosyntactic tagging with a Meta-BiLSTM model over context sensitive token
    encodings,” arXiv preprint arXiv:1805.08237, 2018.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] B. Bohnet, R. McDonald, G. Simoes, D. Andor, E. Pitler, 和 J. Maynez, “使用
    Meta-BiLSTM 模型对上下文敏感的标记编码进行形态句法标注，” arXiv 预印本 arXiv:1805.08237, 2018。'
- en: '[92] J. Legrand and R. Collobert, “Joint RNN-based greedy parsing and word
    composition,” arXiv preprint arXiv:1412.7028, 2014.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Legrand 和 R. Collobert, “基于 RNN 的贪婪解析和词汇组合，” arXiv 预印本 arXiv:1412.7028,
    2014。'
- en: '[93] J. Legrand and R. Collobert, “Deep neural networks for syntactic parsing
    of morphologically rich languages,” in Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 573–578,
    2016.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Legrand 和 R. Collobert, “用于形态丰富语言的句法解析的深度神经网络，” 在第 54 届计算语言学协会年会上 (第
    2 卷：简短论文), pp. 573–578, 2016。'
- en: '[94] A. Kuncoro, M. Ballesteros, L. Kong, C. Dyer, G. Neubig, and N. A. Smith,
    “What do recurrent neural network grammars learn about syntax?,” arXiv preprint
    arXiv:1611.05774, 2016.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] A. Kuncoro, M. Ballesteros, L. Kong, C. Dyer, G. Neubig, 和 N. A. Smith,
    “递归神经网络语法学到的句法是什么？，” arXiv 预印本 arXiv:1611.05774, 2016。'
- en: '[95] J. Liu and Y. Zhang, “In-order transition-based constituent parsing,”
    arXiv preprint arXiv:1707.05000, 2017.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] J. Liu 和 Y. Zhang, “基于顺序转移的成分解析，” arXiv 预印本 arXiv:1707.05000, 2017。'
- en: '[96] D. Fried, M. Stern, and D. Klein, “Improving neural parsing by disentangling
    model combination and reranking effects,” arXiv preprint arXiv:1707.03058, 2017.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] D. Fried, M. Stern, 和 D. Klein，“通过解开模型组合和重新排序效果来改进神经解析，” arXiv 预印本 arXiv:1707.03058，2017。'
- en: '[97] N. Kitaev and D. Klein, “Constituency parsing with a self-attentive encoder,”
    arXiv preprint arXiv:1805.01052, 2018.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] N. Kitaev 和 D. Klein，“使用自注意编码器的成分解析，” arXiv 预印本 arXiv:1805.01052，2018。'
- en: '[98] D. Chen and C. Manning, “A fast and accurate dependency parser using neural
    networks,” in Proceedings of the 2014 conference on empirical methods in natural
    language processing (EMNLP), pp. 740–750, 2014.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] D. Chen 和 C. Manning，“使用神经网络的快速且准确的依赖解析器，” 收录于2014年自然语言处理经验方法会议（EMNLP）论文集，pp.
    740–750，2014。'
- en: '[99] T. Dozat and C. D. Manning, “Deep biaffine attention for neural dependency
    parsing,” arXiv preprint arXiv:1611.01734, 2016.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] T. Dozat 和 C. D. Manning，“用于神经依赖解析的深度双线性注意力，” arXiv 预印本 arXiv:1611.01734，2016。'
- en: '[100] E. Kiperwasser and Y. Goldberg, “Simple and accurate dependency parsing
    using bidirectional LSTM feature representations,” arXiv preprint arXiv:1603.04351,
    2016.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] E. Kiperwasser 和 Y. Goldberg，“使用双向LSTM特征表示的简单而准确的依赖解析，” arXiv 预印本 arXiv:1603.04351，2016。'
- en: '[101] C. Dyer, M. Ballesteros, W. Ling, A. Matthews, and N. A. Smith, “Transition-based
    dependency parsing with stack long short-term memory,” arXiv preprint arXiv:1505.08075,
    2015.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] C. Dyer, M. Ballesteros, W. Ling, A. Matthews, 和 N. A. Smith，“基于转换的依赖解析与堆栈长短期记忆网络，”
    arXiv 预印本 arXiv:1505.08075，2015。'
- en: '[102] S. Jaf and C. Calder, “Deep learning for natural language parsing,” IEEE
    Access, vol. 7, pp. 131363–131373, 2019.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] S. Jaf 和 C. Calder，“用于自然语言解析的深度学习，” IEEE Access，第7卷，pp. 131363–131373，2019。'
- en: '[103] Y. Zhang, F. Tiryaki, M. Jiang, and H. Xu, “Parsing clinical text using
    the state-of-the-art deep learning based parsers: a systematic comparison,” BMC
    medical informatics and decision making, vol. 19, no. 3, p. 77, 2019.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y. Zhang, F. Tiryaki, M. Jiang, 和 H. Xu，“使用最先进的深度学习解析器解析临床文本：系统比较，” BMC
    医学信息与决策， 第19卷，第3期，p. 77，2019。'
- en: '[104] Y. Zhang, Z. Li, and M. Zhang, “Efficient second-order treecrf for neural
    dependency parsing,” arXiv preprint arXiv:2005.00975, 2020.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Y. Zhang, Z. Li, 和 M. Zhang，“用于神经依赖解析的高效二阶 TreeCRF，” arXiv 预印本 arXiv:2005.00975，2020。'
- en: '[105] T. Dozat and C. D. Manning, “Deep biaffine attention for neural dependency
    parsing,” 2017.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] T. Dozat 和 C. D. Manning，“用于神经依赖解析的深度双线性注意力，” 2017。'
- en: '[106] Z. Tan, M. Wang, J. Xie, Y. Chen, and X. Shi, “Deep semantic role labeling
    with self-attention,” arXiv preprint arXiv:1712.01586, 2017.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Z. Tan, M. Wang, J. Xie, Y. Chen, 和 X. Shi，“使用自注意力的深度语义角色标注，” arXiv 预印本
    arXiv:1712.01586，2017。'
- en: '[107] D. Marcheggiani and I. Titov, “Encoding sentences with graph convolutional
    networks for semantic role labeling,” arXiv preprint arXiv:1703.04826, 2017.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] D. Marcheggiani 和 I. Titov，“通过图卷积网络编码句子以进行语义角色标注，” arXiv 预印本 arXiv:1703.04826，2017。'
- en: '[108] E. Strubell, P. Verga, D. Andor, D. Weiss, and A. McCallum, “Linguistically-informed
    self-attention for semantic role labeling,” arXiv preprint arXiv:1804.08199, 2018.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] E. Strubell, P. Verga, D. Andor, D. Weiss, 和 A. McCallum，“语言学信息自注意力用于语义角色标注，”
    arXiv 预印本 arXiv:1804.08199，2018。'
- en: '[109] L. He, K. Lee, O. Levy, and L. Zettlemoyer, “Jointly predicting predicates
    and arguments in neural semantic role labeling,” arXiv preprint arXiv:1805.04787,
    2018.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] L. He, K. Lee, O. Levy, 和 L. Zettlemoyer，“在神经语义角色标注中联合预测谓词和论元，” arXiv
    预印本 arXiv:1805.04787，2018。'
- en: '[110] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and
    L. Zettlemoyer, “Deep contextualized word representations,” arXiv preprint arXiv:1802.05365,
    2018.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, 和 L.
    Zettlemoyer，“深度上下文化词表示，” arXiv 预印本 arXiv:1802.05365，2018。'
- en: '[111] Z. Tan, M. Wang, J. Xie, Y. Chen, and X. Shi, “Deep semantic role labeling
    with self-attention,” in Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Z. Tan, M. Wang, J. Xie, Y. Chen, 和 X. Shi，“使用自注意力的深度语义角色标注，” 第三十二届AAAI人工智能会议，2018。'
- en: '[112] Z. Li, S. He, H. Zhao, Y. Zhang, Z. Zhang, X. Zhou, and X. Zhou, “Dependency
    or span, end-to-end uniform semantic role labeling,” in Proceedings of the AAAI
    Conference on Artificial Intelligence, vol. 33, pp. 6730–6737, 2019.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Z. Li, S. He, H. Zhao, Y. Zhang, Z. Zhang, X. Zhou, 和 X. Zhou，“依赖或跨度，端到端统一的语义角色标注，”
    收录于AAAI人工智能会议论文集，第33卷，pp. 6730–6737，2019。'
- en: '[113] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Björkelund, O. Uryupina,
    Y. Zhang, and Z. Zhong, “Towards robust linguistic analysis using OntoNotes,”
    in Proceedings of the Seventeenth Conference on Computational Natural Language
    Learning, pp. 143–152, 2013.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Björkelund, O. Uryupina,
    Y. Zhang 和 Z. Zhong, “使用 OntoNotes 进行鲁棒语言分析的探索，” 发表在第十七届计算自然语言学习会议论文集，页 143–152,
    2013。'
- en: '[114] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolutional neural
    network for modelling sentences,” arXiv preprint arXiv:1404.2188, 2014.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] N. Kalchbrenner, E. Grefenstette 和 P. Blunsom, “用于建模句子的卷积神经网络，” arXiv
    预印本 arXiv:1404.2188, 2014。'
- en: '[115] H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, and R. Ward,
    “Deep sentence embedding using long short-term memory networks: Analysis and application
    to information retrieval,” IEEE/ACM Transactions on Audio, Speech and Language
    Processing (TASLP), vol. 24, no. 4, pp. 694–707, 2016.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song 和 R. Ward,
    “使用长短期记忆网络的深度句子嵌入：分析及其在信息检索中的应用，” IEEE/ACM 音频、语音与语言处理交易（TASLP），卷 24, 第 4 期, 页
    694–707, 2016。'
- en: '[116] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical
    attention networks for document classification,” in Proceedings of the 2016 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, pp. 1480–1489, 2016.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola 和 E. Hovy, “用于文档分类的层次注意力网络，”
    发表在第 2016 届北美计算语言学协会会议：人类语言技术会议论文集，页 1480–1489, 2016。'
- en: '[117] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural networks
    for text classification.,” in AAAI, vol. 333, pp. 2267–2273, 2015.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] S. Lai, L. Xu, K. Liu 和 J. Zhao, “用于文本分类的递归卷积神经网络，” 发表在 AAAI，卷 333, 页
    2267–2273, 2015。'
- en: '[118] C. Zhou, C. Sun, Z. Liu, and F. Lau, “A C-LSTM neural network for text
    classification,” arXiv preprint arXiv:1511.08630, 2015.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] C. Zhou, C. Sun, Z. Liu 和 F. Lau, “一种用于文本分类的 C-LSTM 神经网络，” arXiv 预印本
    arXiv:1511.08630, 2015。'
- en: '[119] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, and
    J. Gao, “Deep learning based text classification: A comprehensive review,” arXiv
    preprint arXiv:2004.03705, 2020.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu 和 J. Gao,
    “基于深度学习的文本分类：全面综述，” arXiv 预印本 arXiv:2004.03705, 2020。'
- en: '[120] M. Zulqarnain, R. Ghazali, Y. M. M. Hassim, and M. Rehan, “A comparative
    review on deep learning models for text classification,” Indones. J. Electr. Eng.
    Comput. Sci, vol. 19, no. 1, pp. 325–335, 2020.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] M. Zulqarnain, R. Ghazali, Y. M. M. Hassim 和 M. Rehan, “对深度学习模型进行文本分类的比较评估，”
    印度尼西亚电气工程与计算机科学杂志，卷 19, 第 1 期, 页 325–335, 2020。'
- en: '[121] A. Conneau, H. Schwenk, L. Barrault, and Y. LeCun, “Very deep convolutional
    networks for text classification,” in Proceedings of the 15th Conference of the
    European Chapter of the Association for Computational Linguistics: Volume 1, Long
    Papers, vol. 1, pp. 1107–1116, 2017.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] A. Conneau, H. Schwenk, L. Barrault 和 Y. LeCun, “用于文本分类的非常深的卷积网络，” 发表在第
    15 届欧洲计算语言学协会会议论文集：第 1 卷，长篇论文，卷 1, 页 1107–1116, 2017。'
- en: '[122] R. Johnson and T. Zhang, “Deep pyramid convolutional neural networks
    for text categorization,” in Proceedings of the 55th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), vol. 1, pp. 562–570, 2017.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] R. Johnson 和 T. Zhang, “用于文本分类的深度金字塔卷积神经网络，” 发表在第 55 届计算语言学协会年会论文集（第
    1 卷：长篇论文），卷 1, 页 562–570, 2017。'
- en: '[123] R. Johnson and T. Zhang, “Supervised and semi-supervised text categorization
    using LSTM for region embeddings,” arXiv preprint arXiv:1602.02373, 2016.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] R. Johnson 和 T. Zhang, “使用 LSTM 进行区域嵌入的监督和半监督文本分类，” arXiv 预印本 arXiv:1602.02373,
    2016。'
- en: '[124] J. Howard and S. Ruder, “Universal language model fine-tuning for text
    classification,” in Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), vol. 1, pp. 328–339, 2018.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] J. Howard 和 S. Ruder, “通用语言模型微调用于文本分类，” 发表在第 56 届计算语言学协会年会论文集（第 1 卷：长篇论文），卷
    1, 页 328–339, 2018。'
- en: '[125] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa,
    “Natural language processing (almost) from scratch,” Journal of Machine Learning
    Research, vol. 12, no. Aug., pp. 2493–2537, 2011.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu 和 P. Kuksa,
    “几乎从零开始的自然语言处理，” 机器学习研究期刊，卷 12, 第 8 期, 页 2493–2537, 2011。'
- en: '[126] G. Mesnil, X. He, L. Deng, and Y. Bengio, “Investigation of recurrent-neural-network
    architectures and learning methods for spoken language understanding.,” in Interspeech,
    pp. 3771–3775, 2013.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] G. Mesnil, X. He, L. Deng, 和 Y. Bengio, “对口语理解的递归神经网络架构和学习方法的调查，” 见于
    Interspeech, 页码 3771–3775, 2013。'
- en: '[127] F. Dernoncourt, J. Y. Lee, and P. Szolovits, “NeuroNER: an easy-to-use
    program for named-entity recognition based on neural networks,” Conference on
    Empirical Methods on Natural Language Processing (EMNLP), 2017.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] F. Dernoncourt, J. Y. Lee, 和 P. Szolovits, “NeuroNER: 一种基于神经网络的易用命名实体识别程序，”
    自然语言处理实证方法会议（EMNLP），2017。'
- en: '[128] A. Baevski, S. Edunov, Y. Liu, L. Zettlemoyer, and M. Auli, “Cloze-driven
    pretraining of self-attention networks,” 2019.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Baevski, S. Edunov, Y. Liu, L. Zettlemoyer, 和 M. Auli, “基于完形填空的自注意力网络预训练，”
    2019。'
- en: '[129] E. F. Tjong Kim Sang and F. De Meulder, “Introduction to the CoNLL-2003
    shared task: Language-independent named entity recognition,” in Proceedings of
    the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4,
    pp. 142–147, Association for Computational Linguistics, 2003.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] E. F. Tjong Kim Sang 和 F. De Meulder, “CoNLL-2003 共享任务简介：语言无关的命名实体识别，”
    见于第七届自然语言学习会议（HLT-NAACL 2003-第4卷）的论文集, 页码 142–147, 计算语言学协会, 2003。'
- en: '[130] K. Clark, M.-T. Luong, C. D. Manning, and Q. V. Le, “Semi-supervised
    sequence modeling with cross-view training,” arXiv preprint arXiv:1809.08370,
    2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] K. Clark, M.-T. Luong, C. D. Manning, 和 Q. V. Le, “使用交叉视图训练的半监督序列建模，”
    arXiv 预印本 arXiv:1809.08370, 2018。'
- en: '[131] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of deep bidirectional transformers for language understanding,” arXiv preprint
    arXiv:1810.04805, 2018.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova, “BERT: 用于语言理解的深度双向变换器的预训练，”
    arXiv 预印本 arXiv:1810.04805, 2018。'
- en: '[132] R. Socher, B. Huval, C. D. Manning, and A. Y. Ng, “Semantic compositionality
    through recursive matrix-vector spaces,” in Proceedings of the 2012 joint conference
    on empirical methods in natural language processing and computational natural
    language learning, pp. 1201–1211, Association for Computational Linguistics, 2012.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] R. Socher, B. Huval, C. D. Manning, 和 A. Y. Ng, “通过递归矩阵-向量空间的语义组合性，”
    见于2012年自然语言处理与计算自然语言学习联合会议的论文集, 页码 1201–1211, 计算语言学协会, 2012。'
- en: '[133] Z. Geng, G. Chen, Y. Han, G. Lu, and F. Li, “Semantic relation extraction
    using sequential and tree-structured lstm with attention,” Information Sciences,
    vol. 509, pp. 183–192, 2020.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Z. Geng, G. Chen, Y. Han, G. Lu, 和 F. Li, “使用顺序和树结构 LSTM 及注意力的语义关系提取，”
    信息科学, vol. 509, 页码 183–192, 2020。'
- en: '[134] X. Han, T. Gao, Y. Lin, H. Peng, Y. Yang, C. Xiao, Z. Liu, P. Li, M. Sun,
    and J. Zhou, “More data, more relations, more context and more openness: A review
    and outlook for relation extraction,” arXiv preprint arXiv:2004.03186, 2020.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] X. Han, T. Gao, Y. Lin, H. Peng, Y. Yang, C. Xiao, Z. Liu, P. Li, M.
    Sun, 和 J. Zhou, “更多数据，更多关系，更多上下文和更多开放性：关系提取的综述与展望，” arXiv 预印本 arXiv:2004.03186,
    2020。'
- en: '[135] K. Clark and C. D. Manning, “Deep reinforcement learning for mention-ranking
    coreference models,” arXiv preprint arXiv:1609.08667, 2016.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] K. Clark 和 C. D. Manning, “用于提及排序的深度强化学习核心ference模型，” arXiv 预印本 arXiv:1609.08667,
    2016。'
- en: '[136] K. Lee, L. He, and L. Zettlemoyer, “Higher-order coreference resolution
    with coarse-to-fine inference,” arXiv preprint arXiv:1804.05392, 2018.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] K. Lee, L. He, 和 L. Zettlemoyer, “利用粗到细推理的高阶核心ference解析，” arXiv 预印本 arXiv:1804.05392,
    2018。'
- en: '[137] H. Fei, X. Li, D. Li, and P. Li, “End-to-end deep reinforcement learning
    based coreference resolution,” in Proceedings of the 57th Annual Meeting of the
    Association for Computational Linguistics, pp. 660–665, 2019.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] H. Fei, X. Li, D. Li, 和 P. Li, “基于端到端深度强化学习的核心ference解析，” 见于第57届计算语言学协会年会论文集,
    页码 660–665, 2019。'
- en: '[138] W. Wu, F. Wang, A. Yuan, F. Wu, and J. Li, “Corefqa: Coreference resolution
    as query-based span prediction,” in Proceedings of the 58th Annual Meeting of
    the Association for Computational Linguistics, pp. 6953–6963, 2020.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] W. Wu, F. Wang, A. Yuan, F. Wu, 和 J. Li, “Corefqa: 以查询为基础的核心ference分数预测，”
    见于第58届计算语言学协会年会论文集, 页码 6953–6963, 2020。'
- en: '[139] Y. Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao, “Event extraction via dynamic
    multi-pooling convolutional neural networks,” in Proceedings of the 53rd Annual
    Meeting of the Association for Computational Linguistics and the 7th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers), vol. 1,
    pp. 167–176, 2015.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Y. Chen，L. Xu，K. Liu，D. Zeng，和 J. Zhao，“通过动态多池化卷积神经网络进行事件提取，”在第 53 届计算语言学协会年会上以及第
    7 届国际联合自然语言处理会议（第 1 卷：长篇论文）论文集中，第 1 卷，页码 167–176，2015 年。'
- en: '[140] T. H. Nguyen and R. Grishman, “Graph convolutional networks with argument-aware
    pooling for event detection,” in Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] T. H. Nguyen 和 R. Grishman，“带有论点感知池化的图卷积网络用于事件检测，”在第 32 届 AAAI 人工智能会议上，2018
    年。'
- en: '[141] T. Zhang, H. Ji, and A. Sil, “Joint entity and event extraction with
    generative adversarial imitation learning,” Data Intelligence, vol. 1, no. 2,
    pp. 99–120, 2019.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] T. 张，H. 纪，和 A. Sil，“使用生成对抗模仿学习的实体和事件联合提取，”《数据智能》，第 1 卷，第 2 期，页码 99–120，2019
    年。'
- en: '[142] W. Zhao, J. Zhang, J. Yang, T. He, H. Ma, and Z. Li, “A novel joint biomedical
    event extraction framework via two-level modeling of documents,” Information Sciences,
    vol. 550, pp. 27–40, 2021.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] W. Zhao，J. Zhang，J. Yang，T. He，H. Ma，和 Z. Li，“一种通过文档的两级建模的新型联合生物医学事件提取框架，”《信息科学》，第
    550 卷，页码 27–40，2021 年。'
- en: '[143] T. Nasukawa and J. Yi, “Sentiment analysis: Capturing favorability using
    natural language processing,” in Proceedings of the 2nd International Conference
    on Knowledge Capture, pp. 70–77, ACM, 2003.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] T. Nasukawa 和 J. Yi，“情感分析：使用自然语言处理捕捉偏好，”在第 2 届国际知识捕捉会议论文集中，页码 70–77，ACM，2003
    年。'
- en: '[144] K. Dave, S. Lawrence, and D. M. Pennock, “Mining the peanut gallery:
    Opinion extraction and semantic classification of product reviews,” in Proceedings
    of the 12th international conference on World Wide Web, pp. 519–528, ACM, 2003.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] K. Dave，S. Lawrence，和 D. M. Pennock，“挖掘花生画廊：产品评论的意见提取和语义分类，”在第 12 届国际万维网会议论文集中，页码
    519–528，ACM，2003 年。'
- en: '[145] A. R. Pathak, B. Agarwal, M. Pandey, and S. Rautaray, “Application of
    deep learning approaches for sentiment analysis,” in Deep Learning-Based Approaches
    for Sentiment Analysis, pp. 1–31, Springer, 2020.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] A. R. Pathak，B. Agarwal，M. Pandey，和 S. Rautaray，“情感分析的深度学习方法应用，”在《基于深度学习的方法用于情感分析》中，页码
    1–31，Springer，2020 年。'
- en: '[146] A. Yadav and D. K. Vishwakarma, “Sentiment analysis using deep learning
    architectures: a review,” Artificial Intelligence Review, vol. 53, no. 6, pp. 4335–4385,
    2020.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] A. Yadav 和 D. K. Vishwakarma，“使用深度学习架构的情感分析：综述，”《人工智能评论》，第 53 卷，第 6 期，页码
    4335–4385，2020 年。'
- en: '[147] D. Tang, B. Qin, and T. Liu, “Document modeling with gated recurrent
    neural network for sentiment classification,” in Proceedings of the 2015 conference
    on empirical methods in natural language processing, pp. 1422–1432, 2015.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] D. Tang，B. Qin，和 T. Liu，“使用门控递归神经网络进行文档建模以进行情感分类，”在 2015 年自然语言处理实证方法会议论文集中，页码
    1422–1432，2015 年。'
- en: '[148] X. Glorot, A. Bordes, and Y. Bengio, “Domain adaptation for large-scale
    sentiment classification: A deep learning approach,” in Proceedings of the 28th
    international conference on machine learning (ICML-11), pp. 513–520, 2011.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] X. Glorot，A. Bordes，和 Y. Bengio，“大规模情感分类的领域适应：一种深度学习方法，”在第 28 届国际机器学习会议（ICML-11）论文集中，页码
    513–520，2011 年。'
- en: '[149] G. Rao, W. Huang, Z. Feng, and Q. Cong, “Lstm with sentence representations
    for document-level sentiment classification,” Neurocomputing, vol. 308, pp. 49–57,
    2018.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] G. Rao，W. Huang，Z. Feng，和 Q. Cong，“使用句子表示的 LSTM 进行文档级情感分类，”《神经计算》，第 308
    卷，页码 49–57，2018 年。'
- en: '[150] M. Rhanoui, M. Mikram, S. Yousfi, and S. Barzali, “A cnn-bilstm model
    for document-level sentiment analysis,” Machine Learning and Knowledge Extraction,
    vol. 1, no. 3, pp. 832–847, 2019.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] M. Rhanoui，M. Mikram，S. Yousfi，和 S. Barzali，“用于文档级情感分析的 CNN-BiLSTM 模型，”《机器学习与知识提取》，第
    1 卷，第 3 期，页码 832–847，2019 年。'
- en: '[151] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning, “Semi-supervised
    recursive autoencoders for predicting sentiment distributions,” in Proceedings
    of the conference on empirical methods in natural language processing, pp. 151–161,
    Association for Computational Linguistics, 2011.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] R. Socher，J. Pennington，E. H. Huang，A. Y. Ng，和 C. D. Manning，“半监督递归自编码器用于预测情感分布，”在自然语言处理实证方法会议论文集中，页码
    151–161，计算语言学协会，2011 年。'
- en: '[152] X. Wang, Y. Liu, S. Chengjie, B. Wang, and X. Wang, “Predicting polarities
    of tweets by composing word embeddings with long short-term memory,” in Proceedings
    of the 53rd Annual Meeting of the Association for Computational Linguistics and
    the 7th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers), vol. 1, pp. 1343–1353, 2015.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] X. Wang, Y. Liu, S. Chengjie, B. Wang 和 X. Wang，“通过将词嵌入与长短期记忆结合预测推文的极性，”发表于第
    53 届计算语言学协会年会和第 7 届国际自然语言处理联合会议（第 1 卷：长论文），第 1 卷，第 1343–1353 页，2015。'
- en: '[153] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and
    C. Potts, “Recursive deep models for semantic compositionality over a sentiment
    treebank,” in Proceedings of the 2013 conference on empirical methods in natural
    language processing, pp. 1631–1642, 2013.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng 和 C.
    Potts，“用于情感树库的语义组合的递归深度模型，”发表于 2013 年自然语言处理实证方法会议论文集，第 1631–1642 页，2013。'
- en: '[154] R. Arulmurugan, K. Sabarmathi, and H. Anandakumar, “Classification of
    sentence level sentiment analysis using cloud machine learning techniques,” Cluster
    Computing, vol. 22, no. 1, pp. 1199–1209, 2019.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] R. Arulmurugan, K. Sabarmathi 和 H. Anandakumar，“使用云机器学习技术进行句子级情感分析的分类，”《簇计算》，第
    22 卷，第 1 期，第 1199–1209 页，2019。'
- en: '[155] D. Meškelė and F. Frasincar, “Aldonar: A hybrid solution for sentence-level
    aspect-based sentiment analysis using a lexicalized domain ontology and a regularized
    neural attention model,” Information Processing & Management, vol. 57, no. 3,
    p. 102211, 2020.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] D. Meškelė 和 F. Frasincar，“Aldonar：一种混合解决方案，用于句子级基于方面的情感分析，结合了词汇化领域本体和正则化神经注意力模型，”《信息处理与管理》，第
    57 卷，第 3 期，第 102211 页，2020。'
- en: '[156] Y. Wang, M. Huang, L. Zhao, et al., “Attention-based LSTM for aspect-level
    sentiment classification,” in Proceedings of the 2016 Conference on Empirical
    Methods in Natural Language Processing, pp. 606–615, 2016.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Y. Wang, M. Huang, L. Zhao 等，“基于注意力的 LSTM 进行方面级情感分类，”发表于 2016 年自然语言处理实证方法会议论文集，第
    606–615 页，2016。'
- en: '[157] Y. Ma, H. Peng, T. Khan, E. Cambria, and A. Hussain, “Sentic lstm: a
    hybrid network for targeted aspect-based sentiment analysis,” Cognitive Computation,
    vol. 10, no. 4, pp. 639–650, 2018.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Y. Ma, H. Peng, T. Khan, E. Cambria 和 A. Hussain，“Sentic lstm：一种针对基于方面的情感分析的混合网络，”《认知计算》，第
    10 卷，第 4 期，第 639–650 页，2018。'
- en: '[158] H. Xu, B. Liu, L. Shu, and P. S. Yu, “BERT post-training for review reading
    comprehension and aspect-based sentiment analysis,” arXiv preprint arXiv:1904.02232,
    2019.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] H. Xu, B. Liu, L. Shu 和 P. S. Yu，“BERT 后训练用于评论阅读理解和基于方面的情感分析，”arXiv 预印本
    arXiv:1904.02232, 2019。'
- en: '[159] H. Xu, B. Liu, L. Shu, and P. S. Yu, “Double embeddings and CNN-based
    sequence labeling for aspect extraction,” arXiv preprint arXiv:1805.04601, 2018.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] H. Xu, B. Liu, L. Shu 和 P. S. Yu，“双重嵌入和基于 CNN 的序列标注用于方面提取，”arXiv 预印本
    arXiv:1805.04601, 2018。'
- en: '[160] H. H. Do, P. Prasad, A. Maag, and A. Alsadoon, “Deep learning for aspect-based
    sentiment analysis: a comparative review,” Expert Systems with Applications, vol. 118,
    pp. 272–299, 2019.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] H. H. Do, P. Prasad, A. Maag 和 A. Alsadoon，“基于方面的情感分析的深度学习：比较综述，”《专家系统与应用》，第
    118 卷，第 272–299 页，2019。'
- en: '[161] S. Rida-E-Fatima, A. Javed, A. Banjar, A. Irtaza, H. Dawood, H. Dawood,
    and A. Alamri, “A multi-layer dual attention deep learning model with refined
    word embeddings for aspect-based sentiment analysis,” IEEE Access, vol. 7, pp. 114795–114807,
    2019.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] S. Rida-E-Fatima, A. Javed, A. Banjar, A. Irtaza, H. Dawood, H. Dawood
    和 A. Alamri，“一种多层双重注意力深度学习模型，具有优化的词嵌入用于基于方面的情感分析，”《IEEE Access》，第 7 卷，第 114795–114807
    页，2019。'
- en: '[162] Y. Liang, F. Meng, J. Zhang, J. Xu, Y. Chen, and J. Zhou, “A novel aspect-guided
    deep transition model for aspect based sentiment analysis,” arXiv preprint arXiv:1909.00324,
    2019.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Y. Liang, F. Meng, J. Zhang, J. Xu, Y. Chen 和 J. Zhou，“一种新型的方面引导深度转换模型用于基于方面的情感分析，”arXiv
    预印本 arXiv:1909.00324, 2019。'
- en: '[163] D. Jurafsky and J. H. Martin, Speech and Language Processing. Prentice
    Hall, 2008.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] D. Jurafsky 和 J. H. Martin，《语音和语言处理》。普伦蒂斯霍尔，2008。'
- en: '[164] N. Kalchbrenner and P. Blunsom, “Recurrent continuous translation models,”
    in Proceedings of the 2013 Conference on Empirical Methods in Natural Language
    Processing, pp. 1700–1709, 2013.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] N. Kalchbrenner 和 P. Blunsom，“递归连续翻译模型，”发表于 2013 年自然语言处理实证方法会议论文集，第 1700–1709
    页，2013。'
- en: '[165] S. P. Singh, A. Kumar, H. Darbari, L. Singh, A. Rastogi, and S. Jain,
    “Machine translation using deep learning: An overview,” in 2017 international
    conference on computer, communications and electronics (comptelix), pp. 162–167,
    IEEE, 2017.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] S. P. Singh, A. Kumar, H. Darbari, L. Singh, A. Rastogi, 和 S. Jain, “使用深度学习的机器翻译：概述,”
    在 2017 国际计算机、通信与电子会议（comptelix）, pp. 162–167, IEEE, 2017。'
- en: '[166] S. Yang, Y. Wang, and X. Chu, “A survey of deep learning techniques for
    neural machine translation,” arXiv preprint arXiv:2002.07526, 2020.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] S. Yang, Y. Wang, 和 X. Chu, “深度学习技术在神经机器翻译中的调查,” arXiv 预印本 arXiv:2002.07526,
    2020。'
- en: '[167] L. E. Dostert, “The Georgetown-IBM experiment,” 1955). Machine translation
    of languages. John Wiley & Sons, New York, pp. 124–135, 1955.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] L. E. Dostert, “乔治城-IBM 实验,” 1955）。语言机器翻译。John Wiley & Sons, 纽约, pp.
    124–135, 1955。'
- en: '[168] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] D. Bahdanau, K. Cho, 和 Y. Bengio, “通过联合学习对齐和翻译进行神经机器翻译,” arXiv 预印本 arXiv:1409.0473,
    2014。'
- en: '[169] K. Cho, B. Van Merriënboer, D. Bahdanau, and Y. Bengio, “On the properties
    of neural machine translation: Encoder-decoder approaches,” arXiv preprint arXiv:1409.1259,
    2014.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] K. Cho, B. Van Merriënboer, D. Bahdanau, 和 Y. Bengio, “神经机器翻译的属性：编码器-解码器方法,”
    arXiv 预印本 arXiv:1409.1259, 2014。'
- en: '[170] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in Advances in neural information processing systems, pp. 3104–3112,
    2014.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] I. Sutskever, O. Vinyals, 和 Q. V. Le, “使用神经网络的序列到序列学习,” 在《神经信息处理系统进展》,
    pp. 3104–3112, 2014。'
- en: '[171] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun,
    Y. Cao, Q. Gao, K. Macherey, et al., “Google’s neural machine translation system:
    Bridging the gap between human and machine translation,” arXiv preprint arXiv:1609.08144,
    2016.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun,
    Y. Cao, Q. Gao, K. Macherey, 等, “谷歌的神经机器翻译系统：弥合人类与机器翻译之间的差距,” arXiv 预印本 arXiv:1609.08144,
    2016。'
- en: '[172] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, “Convolutional
    sequence to sequence learning,” arXiv preprint arXiv:1705.03122, 2017.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] J. Gehring, M. Auli, D. Grangier, D. Yarats, 和 Y. N. Dauphin, “卷积序列到序列学习,”
    arXiv 预印本 arXiv:1705.03122, 2017。'
- en: '[173] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in Neural
    Information Processing Systems, pp. 5998–6008, 2017.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin, “注意力机制就是你所需要的一切,” 在《神经信息处理系统进展》, pp. 5998–6008, 2017。'
- en: '[174] K. Ahmed, N. S. Keskar, and R. Socher, “Weighted transformer network
    for machine translation,” arXiv preprint arXiv:1711.02132, 2017.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] K. Ahmed, N. S. Keskar, 和 R. Socher, “用于机器翻译的加权变换网络,” arXiv 预印本 arXiv:1711.02132,
    2017。'
- en: '[175] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative
    position representations,” arXiv preprint arXiv:1803.02155, 2018.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] P. Shaw, J. Uszkoreit, 和 A. Vaswani, “具有相对位置表示的自注意力,” arXiv 预印本 arXiv:1803.02155,
    2018。'
- en: '[176] S. Edunov, M. Ott, M. Auli, and D. Grangier, “Understanding back-translation
    at scale,” arXiv preprint arXiv:1808.09381, 2018.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] S. Edunov, M. Ott, M. Auli, 和 D. Grangier, “大规模理解反向翻译,” arXiv 预印本 arXiv:1808.09381,
    2018。'
- en: '[177] R. Aharoni, M. Johnson, and O. Firat, “Massively multilingual neural
    machine translation,” 2019.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] R. Aharoni, M. Johnson, 和 O. Firat, “大规模多语言神经机器翻译,” 2019。'
- en: '[178] J. Zhu, Y. Xia, L. Wu, D. He, T. Qin, W. Zhou, H. Li, and T.-Y. Liu,
    “Incorporating bert into neural machine translation,” 2020.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] J. Zhu, Y. Xia, L. Wu, D. He, T. Qin, W. Zhou, H. Li, 和 T.-Y. Liu, “将
    BERT 融入神经机器翻译,” 2020。'
- en: '[179] Y. Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad, M. Lewis,
    and L. Zettlemoyer, “Multilingual denoising pre-training for neural machine translation,”
    Transactions of the Association for Computational Linguistics, vol. 8, pp. 726–742,
    2020.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Y. Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad, M. Lewis,
    和 L. Zettlemoyer, “用于神经机器翻译的多语言去噪预训练,” 计算语言学学会会刊, vol. 8, pp. 726–742, 2020。'
- en: '[180] Y. Cheng, L. Jiang, and W. Macherey, “Robust neural machine translation
    with doubly adversarial inputs,” arXiv preprint arXiv:1906.02443, 2019.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Y. Cheng, L. Jiang, 和 W. Macherey, “通过双重对抗输入实现鲁棒的神经机器翻译,” arXiv 预印本 arXiv:1906.02443,
    2019。'
- en: '[181] W. Zhang, Y. Feng, F. Meng, D. You, and Q. Liu, “Bridging the gap between
    training and inference for neural machine translation,” 2019.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] W. Zhang, Y. Feng, F. Meng, D. You, 和 Q. Liu, “弥合神经机器翻译训练与推理之间的差距,” 2019。'
- en: '[182] J. Yang, M. Wang, H. Zhou, C. Zhao, W. Zhang, Y. Yu, and L. Li, “Towards
    making the most of bert in neural machine translation,” in Proceedings of the
    AAAI Conference on Artificial Intelligence, vol. 34, pp. 9378–9385, 2020.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] J. Yang, M. Wang, H. Zhou, C. Zhao, W. Zhang, Y. Yu 和 L. Li，“在神经机器翻译中最大化
    BERT 的利用，”发表在 AAAI 人工智能会议论文集中，第 34 卷，第 9378–9385 页，2020 年。'
- en: '[183] A. Bordes, S. Chopra, and J. Weston, “Question answering with subgraph
    embeddings,” arXiv preprint arXiv:1406.3676, 2014.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] A. Bordes, S. Chopra 和 J. Weston，“基于子图嵌入的问答，”arXiv 预印本 arXiv:1406.3676，2014
    年。'
- en: '[184] B. F. Green Jr, A. K. Wolf, C. Chomsky, and K. Laughery, “Baseball: an
    automatic question-answerer,” in Papers presented at the May 9-11, 1961, Western
    Joint IRE-AIEE-ACM Computer Conference, pp. 219–224, ACM, 1961.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] B. F. Green Jr, A. K. Wolf, C. Chomsky 和 K. Laughery，“Baseball: 自动问答系统，”发表在
    1961 年 5 月 9-11 日的西部联合 IRE-AIEE-ACM 计算机会议论文集中，第 219–224 页，ACM，1961 年。'
- en: '[185] A. Ittycheriah, M. Franz, W.-J. Zhu, A. Ratnaparkhi, and R. J. Mammone,
    “IBM’s statistical question answering system.,” in TREC, 2000.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] A. Ittycheriah, M. Franz, W.-J. Zhu, A. Ratnaparkhi 和 R. J. Mammone，“IBM
    的统计问答系统，”发表在 TREC 会议论文集中，2000 年。'
- en: '[186] H. Cui, R. Sun, K. Li, M.-Y. Kan, and T.-S. Chua, “Question answering
    passage retrieval using dependency relations,” in Proceedings of the 28th annual
    international ACM SIGIR conference on Research and development in information
    retrieval, pp. 400–407, ACM, 2005.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] H. Cui, R. Sun, K. Li, M.-Y. Kan 和 T.-S. Chua，“基于依赖关系的问答段检索，”发表在第 28
    届国际 ACM SIGIR 信息检索研究与发展会议论文集中，第 400–407 页，ACM，2005 年。'
- en: '[187] X. Qiu and X. Huang, “Convolutional neural tensor network architecture
    for community-based question answering.,” in IJCAI, pp. 1305–1311, 2015.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] X. Qiu 和 X. Huang，“用于社区问答的卷积神经张量网络架构，”发表在 IJCAI 会议论文集中，第 1305–1311 页，2015
    年。'
- en: '[188] H. T. Ng, L. H. Teo, and J. L. P. Kwan, “A machine learning approach
    to answering questions for reading comprehension tests,” in Proceedings of the
    2000 Joint SIGDAT conference on Empirical methods in natural language processing
    and very large corpora: held in conjunction with the 38th Annual Meeting of the
    Association for Computational Linguistics-Volume 13, pp. 124–132, Association
    for Computational Linguistics, 2000.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] H. T. Ng, L. H. Teo 和 J. L. P. Kwan，“一种机器学习方法用于回答阅读理解测试的问题，”发表在 2000
    年联合 SIGDAT 会议上，处理自然语言处理和非常大语料库的方法，与第 38 届计算语言学协会年会同时举行，第 13 卷，第 124–132 页，计算语言学协会，2000
    年。'
- en: '[189] C. Xiong, V. Zhong, and R. Socher, “Dynamic coattention networks for
    question answering,” arXiv preprint arXiv:1611.01604, 2016.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] C. Xiong, V. Zhong 和 R. Socher，“用于问答的动态共同注意网络，”arXiv 预印本 arXiv:1611.01604，2016
    年。'
- en: '[190] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick,
    and D. Parikh, “VQA: Visual question answering,” in Proceedings of the IEEE international
    conference on computer vision, pp. 2425–2433, 2015.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick
    和 D. Parikh，“VQA: 视觉问答，”发表在 IEEE 国际计算机视觉会议论文集中，第 2425–2433 页，2015 年。'
- en: '[191] M. Malinowski, M. Rohrbach, and M. Fritz, “Ask your neurons: A neural-based
    approach to answering questions about images,” in Proceedings of the IEEE international
    conference on computer vision, pp. 1–9, 2015.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] M. Malinowski, M. Rohrbach 和 M. Fritz，“询问你的神经元：一种基于神经网络的图像问答方法，”发表在 IEEE
    国际计算机视觉会议论文集中，第 1–9 页，2015 年。'
- en: '[192] H. Xu and K. Saenko, “Ask, attend and answer: Exploring question-guided
    spatial attention for visual question answering,” in European Conference on Computer
    Vision, pp. 451–466, Springer, 2016.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] H. Xu 和 K. Saenko，“询问、注意和回答：探索视觉问答中的问题引导空间注意力，”发表在欧洲计算机视觉会议论文集中，第 451–466
    页，Springer，2016 年。'
- en: '[193] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human attention
    in visual question answering: Do humans and deep networks look at the same regions?,”
    Computer Vision and Image Understanding, vol. 163, pp. 90–100, 2017.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] A. Das, H. Agrawal, L. Zitnick, D. Parikh 和 D. Batra，“视觉问答中的人类注意力：人类和深度网络是否关注相同的区域？”，计算机视觉与图像理解，第
    163 卷，第 90–100 页，2017 年。'
- en: '[194] H. Ben-Younes, R. Cadene, N. Thome, and M. Cord, “Block: Bilinear superdiagonal
    fusion for visual question answering and visual relationship detection,” in Proceedings
    of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 8102–8109, 2019.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] H. Ben-Younes, R. Cadene, N. Thome, 和 M. Cord，“Block: 双线性超对角融合用于视觉问答和视觉关系检测，”发表在
    AAAI 人工智能会议论文集中，第 33 卷，第 8102–8109 页，2019 年。'
- en: '[195] J. Wu and R. J. Mooney, “Self-critical reasoning for robust visual question
    answering,” 2019.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] J. Wu 和 R. J. Mooney，“用于鲁棒视觉问答的自我批判推理，”2019 年。'
- en: '[196] R. Nallapati, F. Zhai, and B. Zhou, “SummaRuNNer: A recurrent neural
    network based sequence model for extractive summarization of documents.,” in AAAI,
    pp. 3075–3081, 2017.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] R. Nallapati, F. Zhai, 和 B. Zhou，“SummaRuNNer：基于递归神经网络的抽取式文档摘要序列模型”，发表于AAAI，页码
    3075–3081，2017年。'
- en: '[197] S. Narayan, S. B. Cohen, and M. Lapata, “Ranking sentences for extractive
    summarization with reinforcement learning,” in NAACL:HLT, vol. 1, pp. 1747–1759,
    2018.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] S. Narayan, S. B. Cohen, 和 M. Lapata，“使用强化学习对抽取式摘要进行句子排名”，发表于NAACL:HLT，第1卷，页码
    1747–1759，2018年。'
- en: '[198] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for abstractive
    sentence summarization,” in EMNLP, 2015.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] A. M. Rush, S. Chopra, 和 J. Weston，“用于抽象句子摘要的神经注意力模型”，发表于EMNLP，2015年。'
- en: '[199] J. Tan, X. Wan, and J. Xiao, “Abstractive document summarization with
    a graph-based attentional neural model,” in ACL, vol. 1, pp. 1171–1181, 2017.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] J. Tan, X. Wan, 和 J. Xiao，“基于图的注意力神经模型的抽象文档摘要”，发表于ACL，第1卷，页码 1171–1181，2017年。'
- en: '[200] R. Nallapati, B. Zhou, C. dos Santos, C. Gulcehre, and B. Xiang, “Abstractive
    text summarization using sequence-to-sequence RNNs and beyond,” in Proceedings
    of The 20th SIGNLL Conference on Computational Natural Language Learning, pp. 280–290,
    2016.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] R. Nallapati, B. Zhou, C. dos Santos, C. Gulcehre, 和 B. Xiang，“使用序列到序列
    RNN 的抽象文本摘要及其发展”，发表于第20届SIGNLL计算自然语言学习会议，页码 280–290，2016年。'
- en: '[201] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman,
    and P. Blunsom, “Teaching machines to read and comprehend,” in NIPS, pp. 1693–1701,
    2015.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman,
    和 P. Blunsom，“教机器阅读和理解”，发表于NIPS，页码 1693–1701，2015年。'
- en: '[202] J. Gu, Z. Lu, H. Li, and V. O. Li, “Incorporating copying mechanism in
    sequence-to-sequence learning,” in ACL, vol. 1, pp. 1631–1640, 2016.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] J. Gu, Z. Lu, H. Li, 和 V. O. Li，“在序列到序列学习中引入复制机制”，发表于ACL，第1卷，页码 1631–1640，2016年。'
- en: '[203] Y.-C. Chen and M. Bansal, “Fast abstractive summarization with reinforce-selected
    sentence rewriting,” in ACL, 2018.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Y.-C. Chen 和 M. Bansal，“快速抽象摘要生成与强化选择句子重写”，发表于ACL，2018年。'
- en: '[204] Q. Zhou, N. Yang, F. Wei, S. Huang, M. Zhou, and T. Zhao, “Neural document
    summarization by jointly learning to score and select sentences,” in ACL, pp. 654–663,
    ACL, 2018.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Q. Zhou, N. Yang, F. Wei, S. Huang, M. Zhou, 和 T. Zhao，“通过联合学习评分和选择句子进行神经文档摘要”，发表于ACL，页码
    654–663，ACL，2018年。'
- en: '[205] T. Shi, Y. Keneshloo, N. Ramakrishnan, and C. K. Reddy, “Neural abstractive
    text summarization with sequence-to-sequence models,” arXiv preprint arXiv:1812.02303,
    2018.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] T. Shi, Y. Keneshloo, N. Ramakrishnan, 和 C. K. Reddy，“基于序列到序列模型的神经抽象文本摘要”，arXiv预印本
    arXiv:1812.02303，2018年。'
- en: '[206] C. Ma, W. E. Zhang, M. Guo, H. Wang, and Q. Z. Sheng, “Multi-document
    summarization via deep learning techniques: A survey,” arXiv preprint arXiv:2011.04843,
    2020.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] C. Ma, W. E. Zhang, M. Guo, H. Wang, 和 Q. Z. Sheng，“通过深度学习技术进行多文档摘要：综述”，arXiv预印本
    arXiv:2011.04843，2020年。'
- en: '[207] A. Abdi, S. Hasan, S. M. Shamsuddin, N. Idris, and J. Piran, “A hybrid
    deep learning architecture for opinion-oriented multi-document summarization based
    on multi-feature fusion,” Knowledge-Based Systems, vol. 213, p. 106658, 2021.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] A. Abdi, S. Hasan, S. M. Shamsuddin, N. Idris, 和 J. Piran，“基于多特征融合的面向意见的多文档摘要的混合深度学习架构”，知识基础系统，第213卷，文章编号
    106658，2021年。'
- en: '[208] X. Zhang, F. Wei, and M. Zhou, “Hibert: Document level pre-training of
    hierarchical bidirectional transformers for document summarization,” arXiv preprint
    arXiv:1905.06566, 2019.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] X. Zhang, F. Wei, 和 M. Zhou，“Hibert：文档级层次双向变换器的预训练用于文档摘要”，arXiv预印本 arXiv:1905.06566，2019年。'
- en: '[209] E. Merdivan, D. Singh, S. Hanke, and A. Holzinger, “Dialogue systems
    for intelligent human computer interactions,” Electronic Notes in Theoretical
    Computer Science, vol. 343, pp. 57–71, 2019.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] E. Merdivan, D. Singh, S. Hanke, 和 A. Holzinger，“智能人机交互的对话系统”，电子笔记理论计算机科学，第343卷，页码
    57–71，2019年。'
- en: '[210] D. Hakkani-Tür, G. Tür, A. Celikyilmaz, Y.-N. Chen, J. Gao, L. Deng,
    and Y.-Y. Wang, “Multi-domain joint semantic frame parsing using bi-directional
    RNN-LSTM,” in Interspeech, pp. 715–719, 2016.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] D. Hakkani-Tür, G. Tür, A. Celikyilmaz, Y.-N. Chen, J. Gao, L. Deng,
    和 Y.-Y. Wang，“使用双向RNN-LSTM的多领域联合语义框架解析”，发表于Interspeech，页码 715–719，2016年。'
- en: '[211] C. Toxtli, J. Cranshaw, et al., “Understanding chatbot-mediated task
    management,” in Proceedings of the 2018 CHI Conference on Human Factors in Computing
    Systems, p. 58, ACM, 2018.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] C. Toxtli, J. Cranshaw, 等，“理解聊天机器人中介的任务管理”，发表于2018年CHI人因计算系统会议论文集，页码
    58，ACM，2018年。'
- en: '[212] V. Ilievski, C. Musat, A. Hossmann, and M. Baeriswyl, “Goal-oriented
    chatbot dialog management bootstrapping with transfer learning,” arXiv preprint
    arXiv:1802.00500, 2018.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] V. Ilievski, C. Musat, A. Hossmann, 和 M. Baeriswyl， “目标导向聊天机器人对话管理的转移学习启动，”
    arXiv 预印本 arXiv:1802.00500，2018年。'
- en: '[213] J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao, “Deep
    reinforcement learning for dialogue generation,” in Proceedings of the Conference
    on Empirical Methods in Natural Language Processing, pp. 1192–1202, 2016.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, 和 J. Gao， “对话生成的深度强化学习，”
    见于自然语言处理经验方法会议论文集，页1192–1202，2016年。'
- en: '[214] T.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. M. Rojas-Barahona, P.-H.
    Su, S. Ultes, and S. Young, “A network-based end-to-end trainable task-oriented
    dialogue system,” arXiv preprint arXiv:1604.04562, 2016.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] T.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. M. Rojas-Barahona, P.-H.
    Su, S. Ultes, 和 S. Young， “一种基于网络的端到端可训练的任务导向对话系统，” arXiv 预印本 arXiv:1604.04562，2016年。'
- en: '[215] J. D. Williams and G. Zweig, “End-to-end LSTM-based dialog control optimized
    with supervised and reinforcement learning,” arXiv preprint arXiv:1606.01269,
    2016.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] J. D. Williams 和 G. Zweig， “基于 LSTM 的端到端对话控制，经过监督学习和强化学习优化，” arXiv 预印本
    arXiv:1606.01269，2016年。'
- en: '[216] S. Sukhbaatar, J. Weston, R. Fergus, et al., “End-to-end memory networks,”
    in Advances in neural information processing systems, pp. 2440–2448, 2015.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] S. Sukhbaatar, J. Weston, R. Fergus 等， “端到端记忆网络，” 见于神经信息处理系统进展，页2440–2448，2015年。'
- en: '[217] A. Bordes, Y.-L. Boureau, and J. Weston, “Learning end-to-end goal-oriented
    dialog,” arXiv preprint arXiv:1605.07683, 2016.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] A. Bordes, Y.-L. Boureau, 和 J. Weston， “学习端到端目标导向对话，” arXiv 预印本 arXiv:1605.07683，2016年。'
- en: '[218] A. Ritter, C. Cherry, and W. B. Dolan, “Data-driven response generation
    in social media,” in Proceedings of the conference on empirical methods in natural
    language processing, pp. 583–593, Association for Computational Linguistics, 2011.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] A. Ritter, C. Cherry, 和 W. B. Dolan， “社交媒体中的数据驱动响应生成，” 见于自然语言处理经验方法会议论文集，页583–593，计算语言学协会，2011年。'
- en: '[219] Z. Ji, Z. Lu, and H. Li, “An information retrieval approach to short
    text conversation,” arXiv preprint arXiv:1408.6988, 2014.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Z. Ji, Z. Lu, 和 H. Li， “一种用于短文本对话的信息检索方法，” arXiv 预印本 arXiv:1408.6988，2014年。'
- en: '[220] B. Hu, Z. Lu, H. Li, and Q. Chen, “Convolutional neural network architectures
    for matching natural language sentences,” in Advances in neural information processing
    systems, pp. 2042–2050, 2014.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] B. Hu, Z. Lu, H. Li, 和 Q. Chen， “用于匹配自然语言句子的卷积神经网络架构，” 见于神经信息处理系统进展，页2042–2050，2014年。'
- en: '[221] R. Lowe, N. Pow, I. Serban, and J. Pineau, “The Ubuntu dialogue corpus:
    A large dataset for research in unstructured multi-turn dialogue systems,” arXiv
    preprint arXiv:1506.08909, 2015.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] R. Lowe, N. Pow, I. Serban, 和 J. Pineau， “Ubuntu 对话语料库：用于无结构多轮对话系统研究的大型数据集，”
    arXiv 预印本 arXiv:1506.08909，2015年。'
- en: '[222] R. Yan, Y. Song, and H. Wu, “Learning to respond with deep neural networks
    for retrieval-based human-computer conversation system,” in Proceedings of the
    39th International ACM SIGIR conference on Research and Development in Information
    Retrieval, pp. 55–64, ACM, 2016.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] R. Yan, Y. Song, 和 H. Wu， “基于深度神经网络的响应学习用于检索式人机对话系统，” 见于第39届国际 ACM SIGIR
    信息检索研究与开发会议论文集，页55–64，ACM，2016年。'
- en: '[223] X. Zhou, L. Li, D. Dong, Y. Liu, Y. Chen, W. X. Zhao, D. Yu, and H. Wu,
    “Multi-turn response selection for chatbots with deep attention matching network,”
    in Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), vol. 1, pp. 1118–1127, 2018.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] X. Zhou, L. Li, D. Dong, Y. Liu, Y. Chen, W. X. Zhao, D. Yu, 和 H. Wu，
    “用于聊天机器人的多轮响应选择与深度注意力匹配网络，” 见于第56届计算语言学协会年会（第1卷：长篇论文），第1卷，页1118–1127，2018年。'
- en: '[224] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio,
    “Generating sentences from a continuous space,” arXiv preprint arXiv:1511.06349,
    2015.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, 和 S. Bengio，
    “从连续空间生成句子，” arXiv 预印本 arXiv:1511.06349，2015年。'
- en: '[225] A. Kannan and O. Vinyals, “Adversarial evaluation of dialogue models,”
    arXiv preprint arXiv:1701.08198, 2017.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] A. Kannan 和 O. Vinyals， “对话模型的对抗性评估，” arXiv 预印本 arXiv:1701.08198，2017年。'
- en: '[226] O. Vinyals and Q. Le, “A neural conversational model,” arXiv preprint
    arXiv:1506.05869, 2015.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] O. Vinyals 和 Q. Le， “一种神经对话模型，” arXiv 预印本 arXiv:1506.05869，2015年。'
