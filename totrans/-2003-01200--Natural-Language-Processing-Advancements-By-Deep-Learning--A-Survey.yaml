- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:02:11'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2003.01200] Natural Language Processing Advancements By Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2003.01200](https://ar5iv.labs.arxiv.org/html/2003.01200)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Natural Language Processing Advancements By Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amirsina Torfi,  Rouzbeh A. Shirvani,  Yaser Keneshloo, Nader Tavaf, and Edward A. Fox
    Amirsina Torfi, Yaser Keneshloo, and Edward A. Fox were with the Department of
    Computer Science, Virginia Polytechnic Institute and State University, Blacksburg,
    VA, 24060 USA e-mail: (amirsina.torfi@gmail.com, yaserkl@vt.edu, fox@vt.edu).
    Rouzbeh A. Shirvani is an independent researcher, e-mail: (rouzbeh.asghari@gmail.com).
    Nader Tavaf was with the University of Minnesota Twin Cities, Minneapolis, MN,
    55455 USA e-mail: (tavaf001@umn.edu).'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Natural Language Processing (NLP) helps empower intelligent machines by enhancing
    a better understanding of the human language for linguistic-based human-computer
    communication. Recent developments in computational power and the advent of large
    amounts of linguistic data have heightened the need and demand for automating
    semantic analysis using data-driven approaches. The utilization of data-driven
    strategies is pervasive now due to the significant improvements demonstrated through
    the usage of deep learning methods in areas such as Computer Vision, Automatic
    Speech Recognition, and in particular, NLP. This survey categorizes and addresses
    the different aspects and applications of NLP that have benefited from deep learning.
    It covers core NLP tasks and applications, and describes how deep learning methods
    and models advance these areas. We further analyze and compare different approaches
    and state-of-the-art models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Natural Language Processing, Deep Learning, Artificial Intelligence
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Natural Language Processing (NLP) is a sub-discipline of computer science providing
    a bridge between natural languages and computers. It helps empower machines to
    understand, process, and analyze human language [[1](#bib.bib1)]. NLP’s significance
    as a tool aiding comprehension of human-generated data is a logical consequence
    of the context-dependency of data. Data becomes more meaningful through a deeper
    understanding of its context, which in turn facilitates text analysis and mining.
    NLP enables this with the communication structures and patterns of humans.
  prefs: []
  type: TYPE_NORMAL
- en: 'Development of NLP methods is increasingly reliant on data-driven approaches
    which help with building more powerful and robust models [[2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)]. Recent advances in computational power, as well as greater availability
    of big data, enable deep learning, one of the most appealing approaches in the
    NLP domain [[5](#bib.bib5), [2](#bib.bib2), [3](#bib.bib3)], especially given
    that deep learning has already demonstrated superior performance in adjoining
    fields like Computer Vision [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10)] and Speech Recognition [[11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13)]. These developments led to a paradigm shift from traditional
    to novel data-driven approaches aimed at advancing NLP. The reason behind this
    shift was simple: new approaches are more promising regarding results, and are
    easier to engineer.'
  prefs: []
  type: TYPE_NORMAL
- en: As a sequitur to remarkable progress achieved in adjacent disciplines utilizing
    deep learning methods, deep neural networks have been applied to various NLP tasks, including
    part-of-speech tagging [[14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16),
    [17](#bib.bib17)], named entity recognition [[18](#bib.bib18), [19](#bib.bib19),
    [18](#bib.bib18), [20](#bib.bib20), [21](#bib.bib21)], and semantic role labeling [[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]. Most of the research efforts
    in deep learning associated with NLP applications involve either supervised learning¹¹1Learning
    from training data to predict the type of new unseen test examples by mapping
    them to known pre-defined labels. or unsupervised learning²²2Making sense of data
    without sticking to specific tasks and supervisory signals..
  prefs: []
  type: TYPE_NORMAL
- en: This survey covers the emerging role of deep learning in the area of NLP, across
    a broad range of categories. The research presented in [[26](#bib.bib26)] is primarily
    focused on architectures, with little discussion of applications. More recent
    works [[27](#bib.bib27), [4](#bib.bib4)] are specific to certain applications
    or certain sub-fields of NLP [[21](#bib.bib21)]. Here we build on previous works
    by describing the challenges, opportunities, and evaluations of the impact of
    applying deep learning to NLP problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey has six sections, including this introduction. Section 2 lays out
    the theoretical dimensions of NLP and artificial intelligence, and looks at deep
    learning as an approach to solving real-world problems. It motivates this study
    by addressing the question: Why use deep learning in NLP? The third section discusses
    fundamental concepts necessary to understand NLP, covering exemplary issues in
    representation, frameworks, and machine learning. The fourth section summarizes
    benchmark datasets employed in the NLP domain. Section 5 focuses on some of the
    NLP applications where deep learning has demonstrated significant benefit. Finally,
    Section 6 provides a conclusion, also addressing some open problems and promising
    areas for improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: II Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLP has long been viewed as one aspect of artificial intelligence (AI), since
    understanding and generating natural language are high-level indications of intelligence.
    Deep learning is an effective AI tool, so we next situate deep learning in the
    AI world. After that we explain motivations for applying deep learning to NLP.
  prefs: []
  type: TYPE_NORMAL
- en: II-A Artificial Intelligence and Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There have been “islands of success” where big data are processed via AI capabilities
    to produce information to achieve critical operational goals (e.g., fraud detection).
    Accordingly, scientists and consumers anticipate enhancement across a variety
    of applications. However, achieving this requires understanding of AI and its
    mechanisms and means (e.g., algorithms). Ted Greenwald, explaining AI to those
    who are not AI experts, comments: ”Generally AI is anything a computer can do
    that formerly was considered a job for a human” [[28](#bib.bib28)].'
  prefs: []
  type: TYPE_NORMAL
- en: An AI goal is to extend the capabilities of information technology (IT) from
    those to (1) generate, communicate, and store data, to also (2) process data into
    the knowledge that decision makers and others need [[29](#bib.bib29)]. One reason
    is that the available data volume is increasing so rapidly that it is now impossible
    for people to process all available data. This leaves two choices: (1) much or
    even most existing data must be ignored or (2) AI must be developed to process
    the vast volumes of available data into the essential pieces of information that
    decision-makers and others can comprehend. Deep learning is a bridge between the
    massive amounts of data and AI.
  prefs: []
  type: TYPE_NORMAL
- en: II-A1 Definitions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deep learning refers to applying deep neural networks to massive amounts of
    data to learn a procedure aimed at handling a task. The task can range from simple
    classification to complex reasoning. In other words, deep learning is a set of
    mechanisms ideally capable of deriving an optimum solution to any problem given
    a sufficiently extensive and relevant input dataset. Loosely speaking, deep learning
    is detecting and analyzing important structures/features in the data aimed at
    formulating a solution to a given problem. Here, AI and deep learning meet. One
    version of the goal or ambition behind AI is enabling a machine to outperform
    what the human brain does. Deep learning is a means to this end.
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 Deep Learning Architectures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Numerous deep learning architectures have been developed in different research
    areas, e.g., in NLP applications employing recurrent neural networks (RNNs) [[30](#bib.bib30)],
    convolutional neural networks (CNNs) [[31](#bib.bib31)], and more recently, recursive
    neural networks [[32](#bib.bib32)]. We focus our discussion on a review of the
    essential models, explained in relevant seminal publications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi Layer Perceptron: A multilayer perceptron (MLP) has at least three layers (input,
    hidden, and output layers). A layer is simply a collection of neurons operating
    to transform information from the previous layer to the next layer. In the MLP
    architecture, the neurons in a layer do not communicate with each other. An MLP
    employs nonlinear activation functions. Every node in a layer connects to all
    nodes in the next layer, creating a fully connected network (Fig. [1](#S2.F1 "Figure
    1 ‣ II-A2 Deep Learning Architectures ‣ II-A Artificial Intelligence and Deep
    Learning ‣ II Background ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey")). MLPs are the simplest type of Feed-Forward Neural Networks (FNNs).
    FNNs represent a general category of neural networks in which the connections
    between the nodes do not create any cycle, i.e., in a FNN there is no cycle of
    information flow.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/deb92729af814df112406bacdfde1eb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The general architecture of a MLP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional Neural Networks: Convolutional neural networks (CNNs), whose
    architecture is inspired by the human visual cortex, are a subclass of feed-forward
    neural networks. CNNs are named after the underlying mathematical operation, convolution,
    which yields a measure of the interoperability of its input functions. Convolutional
    neural networks are usually employed in situations where data is or needs to be
    represented with a 2D or 3D data map. In the data map representation, the proximity
    of data points usually corresponds to their information correlation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In convolutional neural networks where the input is an image, the data map
    indicates that image pixels are highly correlated to their neighboring pixels. Consequently,
    the convolutional layers have 3 dimensions: width, height, and depth. That assumption
    possibly explains why the majority of research efforts dedicated to CNNs are conducted
    in the Computer Vision field [[33](#bib.bib33)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'A CNN takes an image represented as an array of numeric values. After performing
    specific mathematical operations, it represents the image in a new output space.
    This operation is also called feature extraction, and helps to capture and represent
    key image content. The extracted features can be used for further analysis, for
    different tasks. One example is image classification, which aims to categorize
    images according to some predefined classes. Other examples include determining
    which objects are present in an image and where they are located. See Fig. [2](#S2.F2
    "Figure 2 ‣ II-A2 Deep Learning Architectures ‣ II-A Artificial Intelligence and
    Deep Learning ‣ II Background ‣ Natural Language Processing Advancements By Deep
    Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of utilizing CNNs for NLP, the inputs are sentences or documents
    represented as matrices. Each row of the matrix is associated with a language
    element such as a word or a character. The majority of CNN architectures learn
    word or sentence representations in their training phase. A variety of CNN architectures
    were used in various classification tasks such as Sentiment Analysis and Topic
    Categorization [[31](#bib.bib31), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)].
    CNNs were employed for Relation Extraction and Relation Classification as well [[37](#bib.bib37),
    [38](#bib.bib38)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/237c358f23dd7601ee840b30d97074d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A typical CNN architecture for object detection. The network provides
    a feature representation with attention to the specific region of an image (example
    shown on the left) that contains the object of interest. Out of the multiple regions
    represented (see an ordering of the image blocks, giving image pixel intensity,
    on the right) by the network, the one with the highest score will be selected
    as the main candidate.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/869fc61a1c0ca4b79729a8eb686c7697.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Recurrent Neural Network (RNN), summarized on the left, expanded
    on the right, for $N$ timesteps, with $X$ indicating input, $h$ hidden layer,
    and $O$ output'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recurrent Neural Network: If we line up a sequence of FNNs and feed the output
    of each FNN as an input to the next one, a recurrent neural network (RNN) will
    be constructed. Like FNNs, layers in an RNN can be categorized into input, hidden,
    and output layers. In discrete time frames, sequences of input vectors are fed
    as the input, one vector at a time, e.g., after inputting each batch of vectors,
    conducting some operations and updating the network weights, the next input batch
    will be fed to the network. Thus, as shown in Fig. [3](#S2.F3 "Figure 3 ‣ II-A2
    Deep Learning Architectures ‣ II-A Artificial Intelligence and Deep Learning ‣
    II Background ‣ Natural Language Processing Advancements By Deep Learning: A Survey"),
    at each time step we make predictions and use parameters of the current hidden
    layer as input to the next time step.'
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layers in recurrent neural networks can carry information from the past,
    in other words, memory. This characteristic makes them specifically useful for
    applications that deal with a sequence of inputs such as language modeling [[39](#bib.bib39)],
    i.e., representing language in a way that the machine understands. This concept
    will be described later in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs can carry rich information from the past. Consider the sentence: “Michael
    Jackson was a singer; some people consider him King of Pop.” It’s easy for a human
    to identify him as referring to Michael Jackson. The pronoun him happens seven
    words after Michael Jackson; capturing this dependency is one of the benefits
    of RNNs, where the hidden layers in an RNN act as memory units. Long Short Term
    Memory Network (LSTM) [[40](#bib.bib40)] is one of the most widely used classes
    of RNNs. LSTMs try to capture even long time dependencies between inputs from
    different time steps. Modern Machine Translation and Speech Recognition often
    rely on LSTMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/089e1c53e45d54bb27c5a0c088c34c37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Schematic of an Autoencoder'
  prefs: []
  type: TYPE_NORMAL
- en: 'Autoencoders: Autoencoders implement unsupervised methods in deep learning.
    They are widely used in dimensionality reduction³³3Dimensionality reduction is
    an unsupervised learning approach which is the process of reducing the number
    of variables that were used to represent the data by identifying the most crucial
    information. or NLP applications which consist of sequence to sequence modeling (see
    Section [III-B](#S3.SS2 "III-B Seq2Seq Framework ‣ III Core Concepts in NLP ‣
    Natural Language Processing Advancements By Deep Learning: A Survey") [[39](#bib.bib39)].
    Fig. [4](#S2.F4 "Figure 4 ‣ II-A2 Deep Learning Architectures ‣ II-A Artificial
    Intelligence and Deep Learning ‣ II Background ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey") illustrates the schematic of an Autoencoder. Since
    autoencoders are unsupervised, there is no label corresponding to each input.
    They aim to learn a code representation for each input. The encoder is like a
    feed-forward neural network in which the input gets encoded into a vector (code).
    The decoder operates similarly to the encoder, but in reverse, i.e., constructing
    an output based on the encoded input. In data compression applications, we want
    the created output to be as close as possible to the original input. Autoencoders
    are lossy, meaning the output is an approximate reconstruction of the input.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d49fdc610070003d7cb758f21742cb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Generative Adversarial Networks'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative Adversarial Networks: Goodfellow [[41](#bib.bib41)] introduced Generative
    Adversarial Networks (GANs). As shown in Fig. [5](#S2.F5 "Figure 5 ‣ II-A2 Deep
    Learning Architectures ‣ II-A Artificial Intelligence and Deep Learning ‣ II Background
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey"), a GAN
    is a combination of two neural networks, a discriminator and a generator. The
    whole network is trained in an iterative process. First, the generator network
    generates a fake sample. Then the discriminator network tries to determine whether
    this sample (ex.: an input image) is real or fake, i.e., whether it came from
    the real training data (data used for building the model) or not. The goal of
    the generator is to fool the discriminator in a way that the discriminator believes
    the artificial (i.e., generated) samples synthesized by the generator are real.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This iterative process continues until the generator produces samples that
    are indistinguishable by the discriminator. In other words, the probability of
    classifying a sample as fake or real becomes like flipping a fair coin for the
    discriminator. The goal of the generative model is to capture the distribution
    of real data while the discriminator tries to identify the fake data. One of the
    interesting features of GANs (regarding being generative) is: once the training
    phase is finished, there is no need for the discrimination network, so we solely
    can work with the generation network. In other words, having access to the trained
    generative model is sufficient.'
  prefs: []
  type: TYPE_NORMAL
- en: Different forms of GANs has been introduced, e.g., Sim GAN [[8](#bib.bib8)],
    Wasserstein GAN [[42](#bib.bib42)], info GAN [[43](#bib.bib43)], and DC GAN [[44](#bib.bib44)].
    In one of the most elegant GAN implementations [[45](#bib.bib45)], entirely artificial,
    yet almost perfect, celebrity faces are generated; the pictures are not real,
    but fake photos produced by the network. GAN’s have since received significant
    attention in various applications and have generated astonishing result [[46](#bib.bib46)].
    In the NLP domain, GANs often are used for text generation [[47](#bib.bib47),
    [48](#bib.bib48)].
  prefs: []
  type: TYPE_NORMAL
- en: II-B Motivation for Deep Learning in NLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning applications are predicated on the choices of (1) feature representation
    and (2) deep learning algorithm alongside architecture. These are associated with
    data representation and learning structure, respectively. For data representation,
    surprisingly, there usually is a disjunction between what information is thought
    to be important for the task at hand, versus what representation actually yields
    good results. For instance, in sentiment analysis, lexicon semantics, syntactic
    structure, and context are assumed by some linguists to be of primary significance.
    Nevertheless, previous studies based on the bag-of-words (BoW) model demonstrated
    acceptable performance [[49](#bib.bib49)]. The bag-of-words model [[50](#bib.bib50)],
    often viewed as the vector space model, involves a representation which accounts
    only for the words and their frequency of occurrence. BoW ignores the order and
    interaction of words, and treats each word as a unique feature. BoW disregards
    syntactic structure, yet provides decent results for what some would consider
    syntax-dependent applications. This observation suggests that simple representations,
    when coupled with large amounts of data, may work as well or better than more
    complex representations. These findings corroborate the argument in favor of the
    importance of deep learning algorithms and architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Often the progress of NLP is bound to effective language modeling. A goal of
    statistical language modeling is the probabilistic representation of word sequences
    in language, which is a complicated task due to the curse of dimensionality. The
    research presented in [[51](#bib.bib51)] was a breakthrough for language modeling
    with neural networks aimed at overcoming the curse of dimensionality by (1) learning
    a distributed representation of words and (2) providing a probability function
    for sequences.
  prefs: []
  type: TYPE_NORMAL
- en: A key challenge in NLP research, compared to other domains such as Computer
    Vision, seems to be the complexity of achieving an in-depth representation of
    language using statistical models. A primary task in NLP applications is to provide
    a representation of texts, such as documents. This involves feature learning,
    i.e., extracting meaningful information to enable further processing and analysis
    of the raw data.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional methods begin with time-consuming hand-crafting of features, through
    careful human analysis of a specific application, and are followed by development
    of algorithms to extract and utilize instances of those features. On the other
    hand, deep supervised feature learning methods are highly data-driven and can
    be used in more general efforts aimed at providing a robust data representation.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the vast amounts of unlabeled data, unsupervised feature learning is
    considered to be a crucial task in NLP. Unsupervised feature learning is, in essence,
    learning the features from unlabeled data to provide a low-dimensional representation
    of a high-dimensional data space. Several approaches such as K-means clustering
    and principal component analysis have been proposed and successfully implemented
    to this end. With the advent of deep learning and abundance of unlabeled data,
    unsupervised feature learning becomes a crucial task for representation learning,
    a precursor in NLP applications. Currently, most of the NLP tasks rely on annotated
    data, while a preponderance of unannotated data further motivates research in
    leveraging deep data-driven unsupervised methods.
  prefs: []
  type: TYPE_NORMAL
- en: Given the potential superiority of deep learning approaches in NLP applications,
    it seems crucial to perform a comprehensive analysis of various deep learning
    methods and architectures with particular attention to NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: III Core Concepts in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Feature Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Distributed representations are a series of compact, low dimensional representations
    of data, each representing some distinct informative property. For NLP systems,
    due to issues related to the atomic representation of the symbols, it is imperative
    to learn word representations.
  prefs: []
  type: TYPE_NORMAL
- en: At first, let’s concentrate on how the features are represented, and then we
    focus on different approaches for learning word representations. The encoded input
    features can be characters, words [[32](#bib.bib32)], sentences [[52](#bib.bib52)],
    or other linguistic elements. Generally, it is more desirable to provide a compact
    representation of the words than a sparse one.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/df5a54e295cd5670eb9caaaddc002b23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Considering a given sequence, the skip-thought model generates the
    surrounding sequences using the trained encoder. The assumption is that the surrounding
    sentences are closely related, contextually.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How to select the structure and level of text representation used to be an
    unresolved question. After proposing the word2vec approach [[53](#bib.bib53)],
    subsequently, doc2vec was proposed in [[52](#bib.bib52)] as an unsupervised algorithm
    and was called Paragraph Vector (PV). The goal behind PV is to learn fixed-length
    representations from variable-length text parts such as sentences and documents.
    One of the main objectives of doc2vec is to overcome the drawbacks of models such
    as BoW and to provide promising results for applications such as text classification
    and sentiment analysis. A more recent approach is the skip-thought model which
    applies word2vec at the sentence-level [[54](#bib.bib54)]. By utilizing an encoder-decoder
    architecture, this model generates the surrounding sentences using the given sentence (Fig. [6](#S3.F6
    "Figure 6 ‣ III-A Feature Representation ‣ III Core Concepts in NLP ‣ Natural
    Language Processing Advancements By Deep Learning: A Survey")). Next, let’s investigate
    different kinds of feature representation.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 One-Hot Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In one-hot encoding, each unique element that needs to be represented has its
    dimension which results in a very high dimensional, very sparse representation. Assume
    the words are represented with the one-hot encoding method. Regarding representation
    structure, there is no meaningful connection between different words in the feature
    space. For example, highly correlated words such as ‘ocean’ and ‘water’ will not
    be closer to each other (in the representation space) compared to less correlated
    pairs such as ‘ocean’ and ‘fire.’ Nevertheless, some research efforts present
    promising results using one-hot encoding [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Continuous Bag of Words
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Continuous Bag-of-Words model (CBOW) has frequently been used in NLP applications.
    CBOW tries to predict a word given its surrounding context, which usually consists
    of a few nearby words [[55](#bib.bib55)]. CBOW is neither dependent on the sequential
    order of words nor necessarily on probabilistic characteristics. So it is not
    generally used for language modeling. This model is typically trained to be utilized
    as a pre-trained model for more sophisticated tasks. An alternative to CBOW is
    the weighted CBOW (WCBOW) [[56](#bib.bib56)] in which different vectors get different
    weights reflective of relative importance in context. The simplest example can
    be document categorization where features are words and weights are TF-IDF scores [[57](#bib.bib57)]
    of the associated words.
  prefs: []
  type: TYPE_NORMAL
- en: III-A3 Word-Level Embedding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Word embedding is a learned representation for context elements in which, ideally,
    words with related semantics become highly correlated in the representation space.
    One of the main incentives behind word embedding representations is the high generalization
    power as opposed to sparse, higher dimensional representations [[58](#bib.bib58)].
    Unlike the traditional bag-of-words model in which different words have entirely
    different representations regardless of their usage or collocations, learning
    a distributed representation takes advantage of word usage in context to provide
    similar representations for semantically correlated words. There are different
    approaches to create word embeddings. Several research efforts, including [[55](#bib.bib55),
    [53](#bib.bib53)], used random initialization by uniformly sampling random numbers
    with the objective of training an efficient representation of the model on a large
    dataset. This setup is intuitively acceptable for initialization of the embedding
    for common features such as part-of-speech tags. However, this may not be the
    optimum method for representation of less frequent features such as individual
    words. For the latter, pre-trained models, trained in a supervised or unsupervised
    manner, are usually leveraged for increasing the performance.
  prefs: []
  type: TYPE_NORMAL
- en: III-A4 Character-Level Embedding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The methods mentioned earlier are mostly at higher levels of representation.
    Lower-level representations such as character-level representation require special
    attention as well, due to their simplicity of representation and the potential
    for correction of unusual character combinations such as misspellings [[2](#bib.bib2)].
    For generating character-level embeddings, CNNs have successfully been utilized [[14](#bib.bib14)].
  prefs: []
  type: TYPE_NORMAL
- en: Character-level embeddings have been used in different NLP applications [[59](#bib.bib59)].
    One of the main advantages is the ability to use small model sizes and represent
    words with lower-level language elements [[14](#bib.bib14)]. Here word embeddings
    are models utilizing CNNs over the characters. Another motivation for employing
    character-level embeddings is the out-of-vocabulary word (OOV) issue which is
    usually encountered when, for the given word, there is no equivalent vector in
    the word embedding. The character-level approach may significantly alleviate this
    problem. Nevertheless, this approach suffers from a weak correlation between characters
    and semantic and syntactic parts of the language. So, considering the aforementioned
    pros and cons of utilizing character-level embeddings, several research efforts
    tried to propose and implement higher-level approaches such as using sub-words [[60](#bib.bib60)]
    to create word embeddings for OOV instances as well as creating a semantic bridge
    between the correlated words [[61](#bib.bib61)].
  prefs: []
  type: TYPE_NORMAL
- en: III-B Seq2Seq Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most underlying frameworks in NLP applications rely on sequence-to-sequence
    (seq2seq) models in which not only the input but also the output is represented
    as a sequence. These models are common in various applications including machine
    translation⁴⁴4The input is a sequence of words from one language (e.g., English)
    and the output is the translation to another language (e.g., French)., text summarization⁵⁵5The
    input is a complete document (sequence of words) and the output is a summary of
    it (sequence of words)., speech-to-text, and text-to-speech applications⁶⁶6The
    input is an audio recording of a speech (sequence of audible elements) and the
    output is the speech text (sequence of words)..
  prefs: []
  type: TYPE_NORMAL
- en: The most common seq2seq framework is comprised of an encoder and a decoder.
    The encoder ingests the sequence of input data and generates a mid-level output
    which is subsequently consumed by the decoder to produce the series of final outputs.
    The encoder and decoder are usually implemented via a series of Recurrent Neural
    Networks or LSTM [[40](#bib.bib40)] cells.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder takes a sequence of length $T$, $X=\{x_{1},x_{2},\cdots,x_{T}\}$,
    where $x_{t}\in V=\{1,\cdots,|V|\}$ is the representation of a single input coming
    from the vocabulary $V$, and then generates the output state $h_{t}$. Subsequently,
    the decoder takes the last state from the encoder, i.e., $h_{t}$, and starts generating
    an output of size $L$, $Y^{\prime}=\{y^{\prime}_{1},y^{\prime}_{2},\cdots,y^{\prime}_{L}\}$,
    based on its current state, $s_{t}$, and the ground-truth output $y_{t}$. In different
    applications, the decoder could take advantage of more information such as a context
    vector [[62](#bib.bib62)] or intra-attention vectors [[63](#bib.bib63)] to generate
    better outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most widely training approaches for seq2seq models is called Teacher
    Forcing [[64](#bib.bib64)]. Let us define $y=\{y_{1},y_{2},\cdots,y_{L}\}$ as
    the ground-truth output sequence correspondent to a given input sequence $X$.
    The model training based on the maximum-likelihood criterion employs the following
    cross-entropy (CE) loss minimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{CE}=-\sum_{t=1}^{L}\log{p_{\theta}(y_{t}&#124;y_{t-1},s_{t},X)}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\theta$ is the parameters of the model optimized during the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is optimized using the cross-entropy loss, it can generate an
    entire sequence as follows. Let $\hat{y}_{t}$ denote the output generated by the
    model at time $t$. Then, the next output is generated by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{y}_{t}=\operatorname*{arg\,max}_{y}p_{\theta}(y&#124;\hat{y}_{t-1},s_{t})$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: In NLP applications, one can improve the output by using beam search to find
    a reasonably good output sequence [[3](#bib.bib3)]. During beam search, rather
    than using $\verb|argmax|$ for selecting the best output, we choose the top $K$
    outputs at each step, generate $K$ different paths for the output sequence, and
    finally choose the one that provides better performance as the final output. Although,
    there has been some recent studies [[65](#bib.bib65), [66](#bib.bib66)] on improving
    the beam search by incorporating a similar mechanism during training of them model,
    studying this is outside the scope of this paper.
  prefs: []
  type: TYPE_NORMAL
- en: Given a series of the ground-truth output $Y$ and the generated model output
    $\hat{Y}$, the model performance is evaluated using a task-specific measures such
    as ROUGE [[67](#bib.bib67)], BLEU [[68](#bib.bib68)], and METEOR [[69](#bib.bib69)].
    As an example, $\textrm{ROUGE}_{L}$, which is an evaluation metric in NLP tasks,
    uses the largest common sub-string between ground-truth $Y$ and model output $\hat{Y}$
    to evaluate the generated output.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Reinforcement Learning in NLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although the seq2seq models explained in Section [III-B](#S3.SS2 "III-B Seq2Seq
    Framework ‣ III Core Concepts in NLP ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey") achieve great successes w.r.t. traditional methods,
    there are some issues with how these models are trained. Generally speaking, seq2seq
    models like the ones used in NLP applications face two issues: (1) exposure bias
    and (2) inconsistency between training time and test time measurements [[70](#bib.bib70)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the popular seq2seq models are minimizing cross-entropy loss as their
    optimization objective via Teacher Forcing (Section [III-B](#S3.SS2 "III-B Seq2Seq
    Framework ‣ III Core Concepts in NLP ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey")). In teacher forcing, during the training of the
    model, the decoder utilizes two inputs, the former decoder output state $s_{t-1}$
    and the ground-truth input $y_{t}$, to determine its current output state $s_{t}$.
    Moreover, it employs them to create the next token, i.e., $\hat{y}_{t}$. However,
    at test time, the decoder fully relies on the previously created token from the
    model distribution. As the ground-truth data is not available, such a step is
    necessary to predict the next action. Henceforth, in training, the decoder input
    is coming from the ground truth, while, in the test phase, it relies on the previous
    prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: This exposure bias [[71](#bib.bib71)] induces error growth through output creation
    at the test phase. One approach to remedy this problem is to remove the ground-truth
    dependency in training by solely relying on model distribution to minimize the
    cross-entropy loss. Scheduled sampling [[64](#bib.bib64)] is one popular method
    to handle this setback. During scheduled sampling, we first pre-train the model
    using cross-entropy loss and then slowly replace the ground-truth with samples
    the model generates.
  prefs: []
  type: TYPE_NORMAL
- en: The second obstacle with seq2seq models is that, when training is finished using
    the cross-entropy loss, it is typically evaluated using non-differentiable measures
    such as ROUGE or METEOR. This will form an inconsistency between the training
    objective and the test evaluation metric. Recently, it has been demonstrated that
    both of these problems can be tackled by utilizing techniques from reinforcement
    learning [[70](#bib.bib70)].
  prefs: []
  type: TYPE_NORMAL
- en: Among most of the well-known models in reinforcement learning, policy gradient
    techniques [[72](#bib.bib72)] such as the REINFORCE algorithm [[73](#bib.bib73)]
    and actor-critic based models such as value-based iteration [[74](#bib.bib74)],
    and Q-learning [[75](#bib.bib75)], are among the most common techniques used in
    deep learning in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the model predictions (versus the ground-truth) for the sequence to sequence
    modeling and generation, at training time, was initially introduced by Daume et
    al. [[76](#bib.bib76)]. According to their approach, SEARN, the structured prediction
    can be characterized as one of the reinforcement learning cases as follows: The
    model employs its predictions to produce a sequence of actions (words sequences).
    Then, at each time step, a greedy search algorithm is employed to learn the optimal
    action, and the policy will be trained to predict that particular action.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/11fd576b81881dd9447c32a947260a4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A simple Actor-Critic framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Actor-Critic training, the actor is usually the same neural network used
    to generate the output, while the critic is a regression model that estimates
    how the actor performed on the input data. The actor later receives the feedback
    from the critic and improves its actions. Fig [7](#S3.F7 "Figure 7 ‣ III-C Reinforcement
    Learning in NLP ‣ III Core Concepts in NLP ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey") shows this framework. It is worth noting that action
    in most of the NLP-related applications is like selecting the next output token
    while the state is the decoder output state at each stage of decoding. These models
    have mostly been used for robotic [[77](#bib.bib77)] and Atari games [[78](#bib.bib78)]
    due to the small action space in these applications. However, when we use them
    in NLP applications, they face multiple challenges. The action space in most of
    the NLP applications could be defined as the number of tokens in the vocabulary
    (usually between 50K to 150K tokens). Comparing this to the action space in a
    simple Atari game, which on average has less than 20 actions [[78](#bib.bib78)],
    shows why these Actor-Critic models face difficulties when applied to NLP applications.
    A major challenge is the massive action space in NLP applications, which not only
    causes difficulty for the right action selection, but also will make the training
    process very slow. This makes the process of finding the best Actor-Critic model
    very complicated and model convergence usually requires a lot of tweaks to the
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: IV Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many different researchers for different tasks use benchmark datasets, such
    as those discussed below. Benchmarking in machine learning refers to the assessment
    of methods and algorithms, comparing those regarding their capability to learn
    specific patterns. Benchmarking aids validation of a new approach or practice,
    relative to other existing methods.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark datasets typically take one of three forms.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first is real-world data, obtained from various real-world experiments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second is synthetic data, artificially generated to mimic real-world patterns.
    Synthetic data is generated for use instead of real data. Such datasets are of
    special interest in applications where the amount of data required is much larger
    than that which is available, or where privacy considerations are crucial and
    strict, such as in the healthcare domain.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third type are toy datasets, used for demonstration and visualization purposes.
    Typically they are artificially generated; often there is no need to represent
    real-world data patterns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The foundation of Deep Learning utilization is the availability of data to teach
    the system about pattern identification. The effectiveness of the model depends
    on the quality of the data. Despite the successful implementation of universal
    language modeling techniques such as BERT [[79](#bib.bib79)], however, such models
    can be used solely for pre-training the models. Afterward, the model needs to
    be trained on the data associated with the desired task. Henceforth, based on
    the everyday demands in different machine domains such as NLP, creating new datasets
    is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, creating new datasets is not usually an easy matter. Informally
    speaking, the newly created dataset should be: the right data to train on, sufficient
    for the evaluation, and accurate to work on. Answering the questions of “what
    is the meaning of right and accurate data” is highly application-based. Basically,
    the data should have sufficient information, which depends on the quality and
    quantity of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: To create a dataset, the first step is always asking “what are we trying to
    do and what problem do we need to solve?” and “what kind of data do we need and
    how much of it is required?” The next step is to create training and testing portions.
    The training data set is used to train a model to know how to find the connections
    between the inputs and the associated outputs. The test data set is used to assess
    the intelligence of the machine, i.e., how well the trained model can operate
    on the unseen test samples. Next, we must conduct data preparation to make sure
    the data and its format is simple and understandable for human experts. After
    that, the issue of data accessibility and ownership may arise. Distribution of
    data may need to have specific authorizations, especially if we are dealing with
    sensitive or private data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the aforementioned roadmap, creating proper datasets is complicated and
    of great importance. That’s why few datasets are frequently chosen by the researchers
    and developers for benchmarking. A summary of widely used benchmark datasets is
    provided in Table [I](#S4.T1 "TABLE I ‣ IV Datasets ‣ Natural Language Processing
    Advancements By Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Benchmark datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Dataset | Link |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Machine Translation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; WMT 2014 EN-DE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WMT 2014 EN-FR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| [http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/](http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Text Summarization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN/DM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Newsroom &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DUC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gigaword &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://cs.nyu.edu/~kcho/DMQA/](https://cs.nyu.edu/~kcho/DMQA/) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://summari.es/](https://summari.es/) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://www-nlpir.nist.gov/projects/duc/data.html](https://www-nlpir.nist.gov/projects/duc/data.html)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://catalog.ldc.upenn.edu/LDC2012T21](https://catalog.ldc.upenn.edu/LDC2012T21)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reading Comprehension &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Question Answering &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Question Generation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ARC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CliCR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN/DM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NewsQA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RACE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SQuAD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Story Cloze Test &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NarativeQA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Quasar &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SearchQA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [http://data.allenai.org/arc/](http://data.allenai.org/arc/) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [http://aclweb.org/anthology/N18-1140](http://aclweb.org/anthology/N18-1140)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://cs.nyu.edu/~kcho/DMQA/](https://cs.nyu.edu/~kcho/DMQA/) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://datasets.maluuba.com/NewsQA](https://datasets.maluuba.com/NewsQA)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [http://www.qizhexie.com/data/RACE_leaderboard](http://www.qizhexie.com/data/RACE_leaderboard)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [http://aclweb.org/anthology/W17-0906.pdf](http://aclweb.org/anthology/W17-0906.pdf)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://github.com/deepmind/narrativeqa](https://github.com/deepmind/narrativeqa)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://github.com/bdhingra/quasar](https://github.com/bdhingra/quasar)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://github.com/nyu-dl/SearchQA](https://github.com/nyu-dl/SearchQA)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Semantic Parsing |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AMR parsing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ATIS (SQL Parsing) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WikiSQL (SQL Parsing) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://amr.isi.edu/index.html](https://amr.isi.edu/index.html) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://github.com/jkkummerfeld/text2sql-data/tree/master/data](https://github.com/jkkummerfeld/text2sql-data/tree/master/data)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://github.com/salesforce/WikiSQL](https://github.com/salesforce/WikiSQL)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sentiment Analysis |'
  prefs: []
  type: TYPE_TB
- en: '&#124; IMDB Reviews &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SST &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Yelp Reviews &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Subjectivity Dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://nlp.stanford.edu/sentiment/index.html](https://nlp.stanford.edu/sentiment/index.html)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://www.yelp.com/dataset/challenge](https://www.yelp.com/dataset/challenge)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Text Classification |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AG News &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DBpedia &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TREC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 20 NewsGroup &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://wiki.dbpedia.org/Datasets](https://wiki.dbpedia.org/Datasets)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://trec.nist.gov/data.html](https://trec.nist.gov/data.html) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Natural Language Inference |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SNLI Corpus &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MultiNLI &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SciTail &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://nlp.stanford.edu/projects/snli/](https://nlp.stanford.edu/projects/snli/)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://www.nyu.edu/projects/bowman/multinli/](https://www.nyu.edu/projects/bowman/multinli/)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [http://data.allenai.org/scitail/](http://data.allenai.org/scitail/)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Semantic Role Labeling |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Proposition Bank &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; OneNotes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [http://propbank.github.io/](http://propbank.github.io/) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: V Deep Learning for NLP Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section describes NLP applications using deep learning. Fig. [8](#S5.F8
    "Figure 8 ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey") shows representative NLP tasks (and the categories
    they belong to). A fundamental question is: ”How can we evaluate an NLP algorithm,
    model, or system?” In [[80](#bib.bib80)], some of the most common evaluation metrics
    have been described. This reference explains the fundamental principles of evaluating
    NLP systems.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/17b74fc0b275d716ed5fb6269c20523f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: NLP tasks investigated in this study.'
  prefs: []
  type: TYPE_NORMAL
- en: V-A Basic Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-A1 Part-Of-Speech Tagging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Part-of-Speech tagging is one of the basic tasks in Natural Language Processing.
    It is the process of labeling words with their part of speech categories. Part
    of speech is leveraged for many crucial tasks such as named entity recognition.
    One commonly used dataset for Part-of-Speech tagging is the WSJ corpus⁷⁷7Penn
    Treebank Wall Street Journal (WSJ-PTB).. This dataset contains over a million
    tokens and has been utilized widely as a benchmark dataset for the performance
    assessment of POS tagging systems. Traditional methods are still performing very
    well for this task [[16](#bib.bib16)]. However, neural network based methods have
    been proposed for Part-of-Speech tagging [[81](#bib.bib81)].
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the deep neural network architecture named CharWNN has been developed
    to join word-level and character-level representations using convolutional neural
    networks for POS tagging [[14](#bib.bib14)]. The emphasis in [[14](#bib.bib14)]
    is the importance of character-level feature extraction as their experimental
    results show the necessity of employing hand-crafted features in the absence of
    character-level features for achieving the state-of-the-art. In [[82](#bib.bib82)],
    a wide variety of neural network based models have been proposed for sequence
    tagging tasks, e.g., LSTM networks, bidirectional LSTM networks, LSTM networks
    with a CRF⁸⁸8Conditional Random Field. layer, etc. Sequence tagging itself includes
    part of speech tagging, chunking, and named entity recognition. Likewise, a globally
    normalized transition-based neural network architecture has been proposed for
    POS-tagging [[83](#bib.bib83)]. State-of-the-art results are summarized in Table
     [II](#S5.T2 "TABLE II ‣ V-A1 Part-Of-Speech Tagging ‣ V-A Basic Tasks ‣ V Deep
    Learning for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey"). In [[17](#bib.bib17)], authors propose a bidirectional LSTM to perform
    parts of speech tagging and show that it performs better than conventional machine
    learning techniques on the same dataset. More recently, in [[84](#bib.bib84)],
    authors use a pretrained BERT model in combination with one bidirectional LSTM
    layer and train the latter layer only and outperform the prior state-of-the art
    POS architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: POS tagging state-of-the-art models evaluated on the WSJ-PTB dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Character-aware neural language models [[85](#bib.bib85)] | 97.53 |'
  prefs: []
  type: TYPE_TB
- en: '| Transfer Learning + GRU[[86](#bib.bib86)] | 97.55 |'
  prefs: []
  type: TYPE_TB
- en: '| Bi-directional LSTM + CNNs + CRF[[87](#bib.bib87)] | 97.55 |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial Training + Bi-LSTM [[88](#bib.bib88)] | 97.59 |'
  prefs: []
  type: TYPE_TB
- en: '| Character Composition + Bi-LSTM[[89](#bib.bib89)] | 97.78 |'
  prefs: []
  type: TYPE_TB
- en: '| String Embedding + LSTM[[90](#bib.bib90)] | 97.85 |'
  prefs: []
  type: TYPE_TB
- en: '| Meta-BiLSTM [[91](#bib.bib91)] | 97.96 |'
  prefs: []
  type: TYPE_TB
- en: V-A2 Parsing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Parsing is assigning a structure to a recognized string. There are different
    types of parsing. Constituency Parsing refers in particular to assigning a syntactic
    structure to a sentence. A greedy parser has been introduced in [[92](#bib.bib92)]
    which performs a syntactic and semantic summary of content using vector representations.
    To enhance the results achieved by [[92](#bib.bib92)], the approach proposed in
    [[93](#bib.bib93)] focuses on learning morphological embeddings. Recently, deep
    neural network models outperformed traditional algorithms. State-of-the-art results
    are summarized in Table [III](#S5.T3 "TABLE III ‣ V-A2 Parsing ‣ V-A Basic Tasks
    ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements By
    Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Constituency parsing state-of-the-art models evaluated on the WSJ-PTB
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Recurrent neural network grammars (RNNG) [[94](#bib.bib94)] | 93.6 |'
  prefs: []
  type: TYPE_TB
- en: '| In-order traversal over syntactic trees + LSTM [[95](#bib.bib95)] | 94.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| Model Combination and Reranking [[96](#bib.bib96)] | 94.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Self-Attentive Encoder [[97](#bib.bib97)] | 95.1 |'
  prefs: []
  type: TYPE_TB
- en: Another type of parsing is called Dependency Parsing. Dependency structure shows
    the structural relationships between the words in a targeted sentence. In dependency
    parsing, phrasal elements and phrase-structure rules do not contribute to the
    process. Rather, the syntactic structure of the sentence is expressed only in
    terms of the words in the sentence and the associated relations between the words.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks have shown their superiority regarding generalizability and
    reducing the feature computation cost. In [[98](#bib.bib98)], a novel neural network-based
    approach was proposed for a transition-based dependency parser. Neural network
    based models that operate on task-specific transition systems have also been utilized
    for dependency parsing [[83](#bib.bib83)]. A regularized parser with bi-affine
    classifiers has been proposed for the prediction of arcs and labels [[99](#bib.bib99)].
    Bidirectional-LSTMs have been used in dependency parsers for feature representation [[100](#bib.bib100)].
    A new control structure has been introduced for sequence-to-sequence neural networks
    based on the stack LSTM and has been used in transition-based parsing [[101](#bib.bib101)].
    [[102](#bib.bib102)] presents a transition based multilingual dependency parser
    which uses a bidirectional LSTM to adapt to target languages. In [[103](#bib.bib103)],
    the authors provide a comparison on the state of the art deep learning based parsing
    methods on a clinical text parsing task. More recently, in [[104](#bib.bib104)],
    a second-order TreeCRF extension was added to the biaffine [[105](#bib.bib105)]
    parser to demonstrate that structural learning can further improve parsing performance
    over the state-of-the-art bi-affine models.
  prefs: []
  type: TYPE_NORMAL
- en: V-A3 Semantic Role Labeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Semantic Role Labeling (SRL) is the process of identification and classification
    of text arguments. It is aimed at the characterization of elements to determine
    “who” did “what” to “whom” as well as “how,” “where,” and “when.” It identifies
    the predicate-argument structure of a sentence. The predicate, in essence, refers
    to “what,” while the arguments consist of the associated participants and properties
    in the text. The goal of SRL is to extract the semantic relations between the
    predicate and the related arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the previously-reported research efforts are based on explicit representations
    of semantic roles. Recently, deep learning approaches have achieved the SRL state-of-the-art
    without taking the explicit syntax representation into consideration [[106](#bib.bib106)].
    On the other hand, it is argued that the utilization of syntactic information
    can be leveraged to improve the performance of syntactic-agnostic⁹⁹9Note that
    being syntactic-agnostic does not imply discarding syntactic information. It means
    they are not explicitly employed. models [[107](#bib.bib107)]. A linguistically-informed
    self-attention (LISA) model has been proposed to leverage both multi-task learning
    and self-attention for effective utilization of the syntactic information for
    SRL [[108](#bib.bib108)]. Current state-of-the-art methods employ joint prediction
    of predicates and arguments [[109](#bib.bib109)], novel word representation approaches [[110](#bib.bib110)],
    and self-attention models [[111](#bib.bib111)]; see Table [IV](#S5.T4 "TABLE IV
    ‣ V-A3 Semantic Role Labeling ‣ V-A Basic Tasks ‣ V Deep Learning for NLP Tasks
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Researchers in [[25](#bib.bib25)] focus on syntax and contextualized word representation
    to present a unique multilingual SRL model based on a biaffine scorer, argument
    pruning and bidirectional LSTMs, (see also [[112](#bib.bib112)]).
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Semantic Role Labeling current state-of-the-art models evaluated
    on the OntoNotes dataset [[113](#bib.bib113)]. The accuracy metric is $F_{1}$
    score.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Accuracy ($F_{1}$) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Self-Attention + RNN [[111](#bib.bib111)] | 83.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Contextualized Word Representations [[110](#bib.bib110)] | 84.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Argumented Representations + BiLSTM [[109](#bib.bib109)] | 85.3 |'
  prefs: []
  type: TYPE_TB
- en: V-B Text Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The primary objective of text classification is to assign predefined categories
    to text parts (which could be a word, sentence, or whole document) for preliminary
    classification purposes and further organization and analysis. A simple example
    is the categorization of given documents as to political or non-political news
    articles.
  prefs: []
  type: TYPE_NORMAL
- en: The use of CNNs for sentence classification, in which training the model on
    top of pretrained word-vectors through fine-tuning, has resulted in considerable
    improvements in learning task-specific vectors [[31](#bib.bib31)]. Later, a Dynamic
    Convolutional Neural Network (DCNN) architecture – essentially a CNN with a dynamic
    k-max pooling method – was applied to capture the semantic modeling of sentences [[114](#bib.bib114)].
    In addition to CNNs, RNNs have been used for text classification. An LSTM-RNN
    architecture has been utilized in [[115](#bib.bib115)] for sentence embedding
    with particular superiority in a defined web search task. A Hierarchical Attention
    Network (HAN) has been utilized to capture the hierarchical structure of text,
    with a word-level and sentence-level attention mechanism [[116](#bib.bib116)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Some models used the combination of both RNNs and CNNs for text classification
    such as [[117](#bib.bib117)]. This is a recurrent architecture in addition to
    max-pooling with an effective word representation method, and demonstrates superiority
    compared to simple window-based neural network approaches. Another unified architecture
    is the C-LSTM proposed in [[118](#bib.bib118)] for sentence and document modeling
    in classification. Current state-of-the-art methods are summarized in Table [V](#S5.T5
    "TABLE V ‣ V-B Text Classification ‣ V Deep Learning for NLP Tasks ‣ Natural Language
    Processing Advancements By Deep Learning: A Survey"). A more recent review of
    the deep learning based methods for text classification is provided in [[119](#bib.bib119)].
    The latter focuses on different architectures used for this task, including most
    recent works in CNN based models, as well as RNN based models, and graph neural
    networks. In [[120](#bib.bib120)], authors provide a comparison between various
    deep learning methods for text classification, concluding that GRUs and LSTMs
    can actually perform better than CNN-based models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: The classification accuracy of state-of-the-art methods, evaluated
    on the AG News Corpus dataset [[2](#bib.bib2)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CNN [[121](#bib.bib121)] | 91.33 |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Pyramid CNN [[122](#bib.bib122)] | 93.13 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN [[123](#bib.bib123)] | 93.43 |'
  prefs: []
  type: TYPE_TB
- en: '| Universal Language Model Fine-tuning (ULMFiT) [[124](#bib.bib124)] | 94.99
    |'
  prefs: []
  type: TYPE_TB
- en: V-C Information Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Information extraction identifies structured information from “unstructured”
    data such as social media posts and online news. Deep learning has been utilized
    for information extraction regarding subtasks such as Named Entity Recognition,
    Relation Extraction, Coreference Resolution, and Event Extraction.
  prefs: []
  type: TYPE_NORMAL
- en: V-C1 Named Entity Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Named Entity Recognition (NER) aims to locate and categorize named entities
    in context into pre-defined categories such as the names of people and places.
    The application of deep neural networks in NER has been investigated by the employment
    of CNN [[125](#bib.bib125)] and RNN architectures [[126](#bib.bib126)], as well
    as hybrid bidirectional LSTM and CNN architectures [[19](#bib.bib19)]. NeuroNER [[127](#bib.bib127)],
    a named-entity recognition tool, operates based on artificial neural networks.
    State-of-the-art models are reported in Table [VI](#S5.T6 "TABLE VI ‣ V-C1 Named
    Entity Recognition ‣ V-C Information Extraction ‣ V Deep Learning for NLP Tasks
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey"). [[21](#bib.bib21)]
    provides an extensive discussion on recent deep learning methods for named entity
    recognition. The latter concludes that the work presented in [[128](#bib.bib128)]
    outperforms other recent models (with an F-score of 93.5 on the CoNLL03 dataset).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: State of the art models regarding Name Entity Recognition. Evaluation
    is performed on the CoNLL-2003 Shared Task dataset [[129](#bib.bib129)]. The evaluation
    metric is $F_{1}$ score.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Semi-supervised Sequence Modeling [[130](#bib.bib130)] | 92.61 |'
  prefs: []
  type: TYPE_TB
- en: '| Google BERT [[131](#bib.bib131)] | 92.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Contextual String Embeddings [[90](#bib.bib90)] | 93.09 |'
  prefs: []
  type: TYPE_TB
- en: V-C2 Relation Extraction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Relation Extraction aims to find the semantic relationships between entity pairs. The
    recursive neural network (RNN) model has been proposed for semantic relationship
    classification by learning compositional vector representations [[132](#bib.bib132)].
    For relation classification, CNN architectures have been employed as well, by
    extracting lexical and sentence level features [[37](#bib.bib37)]. More recently,
    in [[133](#bib.bib133)], bidirectional tree-structured LSTMs were shown to perform
    well for relation extraction. [[134](#bib.bib134)] provides a more recent review
    on relation extraction.
  prefs: []
  type: TYPE_NORMAL
- en: V-C3 Coreference Resolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Coreference resolution includes identification of the mentions in a context
    that refer to the same entity. For instance, the mentions “car,” “Camry,” and
    “it” could all refer to the same entity. For the first time in [[135](#bib.bib135)],
    Reinforcement Learning (RL) was applied to coreference resolution. Current widely
    used methods leverage an attention mechanism [[136](#bib.bib136)]. More recently,
    in [[137](#bib.bib137)], authors adopt a reinforcement learning policy gradient
    approach to coreference resolution and provide state-of-the art performance on
    the English OntoNotes v5.0 benchmark task. [[138](#bib.bib138)] reformulates coreference
    resolution as a span prediction task as in question answering and provide superior
    performance on the CoNLL-2012 benchmark task.
  prefs: []
  type: TYPE_NORMAL
- en: V-C4 Event Extraction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A specific type of extracted information from text is an event. Such extraction
    may involve recognizing trigger words related to an event and assigning labels
    to entity mentions that represent event triggers. Convolutional neural networks
    have been utilized for event detection; they handle problems with feature-based
    approaches including exhaustive feature engineering and error propagation phenomena
    for feature generation [[139](#bib.bib139)]. In 2018, Nguyen and Grishman applied
    graph-CNN (GCCN) where the convolutional operations are applied to syntactically
    dependent words as well as consecutive words [[140](#bib.bib140)]; their adding
    entity information reflected the state-of-the-art using CNN models. [[141](#bib.bib141)]
    uses a novel inverse reinforcement learning approach based on generative adversarial
    networks (imitation learning) to tackle joint entity and event extraction. More
    recently, in [[142](#bib.bib142)], authors proposed a model for document-level
    event extraction using a combined dependency-based GCN (for local context) and
    a hypergraph (as an aggregator for global context).
  prefs: []
  type: TYPE_NORMAL
- en: V-D Sentiment analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The primary goal in sentiment analysis is the extraction of subjective information
    from text by contextual mining. Sentiment analysis is considered high-level reasoning
    based on source data. Sentiment analysis is sometimes called opinion mining, as
    its primary goal is to analyze human opinion, sentiments, and even emotions regarding
    products, problems, and varied subjects. Seminal works on sentiment analysis or
    opinion mining include [[143](#bib.bib143), [144](#bib.bib144)]. Since 2000, much
    attention has been given to sentiment analysis, due to its relation to a wide
    variety of applications [[145](#bib.bib145)], its associations with new research
    challenges, and the availability of abundant data. [[146](#bib.bib146)] provides
    a more recent review of the sentiment analysis methods relying on deep learning
    and gives an insightful discussion on the drawbacks as well as merits of deep
    learning methods for sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'A critical aspect of research in sentiment analysis is content granularity.
    Considering this criterion, sentiment analysis is generally divided into three
    categories/levels: document level, sentence level, and aspect level.'
  prefs: []
  type: TYPE_NORMAL
- en: V-D1 Document-level Sentiment Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the document level, the task is to determine whether the whole document reflects
    a positive or negative sentiment about exactly one entity. This differs from opinion
    mining regarding multiple entries. The Gated Recurrent Neural Network architecture
    has been utilized successfully for effectively encoding the sentences’ relations
    in the semantic structure of the document [[147](#bib.bib147)]. Domain adaptation
    has been investigated as well, to deploy the trained model on unseen new sources [[148](#bib.bib148)].
    More recently, in [[149](#bib.bib149)] authors provide an LSTM-based model for
    document-level sentiment analysis that captures semantic relations between sentences.
    In [[150](#bib.bib150)], authors use a CNN-bidirectional LSTM model to process
    long texts.
  prefs: []
  type: TYPE_NORMAL
- en: V-D2 Sentence-level Sentiment Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the sentence-level, sentiment analysis determines the positivity, negativity,
    or neutrality regarding an opinion expressed in a sentence. One general assumption
    for sentence-level sentiment classification is the existence of only one opinion
    from a single opinion holder in an expressed sentence. Recursive autoencoders
    have been employed for sentence-level sentiment label prediction by learning the
    vector space representations for phrases [[151](#bib.bib151)]. Long Short-Term
    Memory (LSTM) recurrent models have also been utilized for tweet sentiment prediction [[152](#bib.bib152)].
    The Sentiment Treebank and Recursive Neural Tensor Networks [[153](#bib.bib153)]
    have shown promise for predicting fine-grained sentiment labels. [[154](#bib.bib154)]
    provides a cloud-based hybrid machine learning model for sentence level sentiment
    analysis. More recently in [[155](#bib.bib155)], propose A Lexicalized Domain
    Ontology and a Regularized Neural Attention model (ALDONAr) for sentence-level
    aspect-based sentiment analysis that uses a CNN classification module with BERT
    word embeddings and achieves state-of-the art results.
  prefs: []
  type: TYPE_NORMAL
- en: V-D3 Aspect-level Sentiment Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Document-level and sentence-level sentiment analysis usually focus on the sentiment
    itself, not the target of the sentiment, e.g., a product. Aspect-level sentiment
    analysis directly targets an opinion, with the assumption of the existence of
    the sentiment and its target. A document or sentence may not have a generally
    positive or negative sentiment, but may have multiple subparts with different
    targets, each with a positive or negative sentiment. This can make aspect-level
    analysis even more challenging than other types of sentiment categorization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aspect-level sentiment analysis usually involves Aspect Sentiment Classification
    and Aspect Extraction. The former determines opinions on different aspects (positive,
    neutral, or negative) while the latter identifies the target aspect for evaluation
    in context. As an example consider the following sentence: “This car is old. It
    must be repaired and sold!”. “This car” is what is subject to evaluation and must
    be extracted first. Here, the opinion about this aspect is negative.'
  prefs: []
  type: TYPE_NORMAL
- en: For aspect-level sentiment classification, attention-based LSTMs are proposed
    to connect the aspect and sentence content for sentiment classification [[156](#bib.bib156)].
    For aspect extraction, deep learning has successfully been proposed in opinion
    mining [[157](#bib.bib157)]. State-of-the-art methods rely on converting aspect-based
    sentiment analysis to sentence-pair classification tasks [[79](#bib.bib79)], post-training
    approaches [[158](#bib.bib158)] on the popular language model BERT [[131](#bib.bib131)],
    and employment of pre-trained embeddings [[159](#bib.bib159)]. [[160](#bib.bib160)]
    provides a recent comparative review on aspect-based sentiment analysis. Also
    recently, [[161](#bib.bib161)] proposed a dual-attention model which tries to
    extract the implicit relation between the aspect and opinion terms. In [[162](#bib.bib162)]
    authors propose a novel Aspect-Guided Deep Transition model for aspect-based sentiment
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: V-E Machine Translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine Translation (MT) is one of the areas of NLP that has been profoundly
    affected by the advances in deep learning. The first subsection below explains
    methods used in the pre-deep learning period, as explained in reference NLP textbooks
    such as “Speech and Language Processing” [[163](#bib.bib163)]. The remainder of
    this section is dedicated to delving into recent innovations in MT which are based
    on neural networks, started by [[164](#bib.bib164)]. [[165](#bib.bib165), [166](#bib.bib166)]
    provide reviews on various deep learning architectures used for MT.
  prefs: []
  type: TYPE_NORMAL
- en: V-E1 Traditional Machine Translation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the first demonstrations of machine translation happened in 1954 [[167](#bib.bib167)]
    in which the authors tried to translate from Russian to English. This translation
    system was based on six simple rules, but had a very limited vocabulary. It was
    not until the 1990s that successful statistical implementations of machine translation
    emerged as more bilingual corpora became available [[163](#bib.bib163)]. In [[68](#bib.bib68)]
    the BLEU score was introduced as a new evaluation metric, allowing more rapid
    improvement than when the only approach involved using human labor for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: V-E2 Neural Machine Translation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It was after the success of the neural network in image classification tasks
    that researchers started to use neural networks in machine translation (NMT).
    Around 2013, research groups started to achieve breakthrough results in NMT. Unlike
    traditional statistical machine translation, NMT is based on an end-to-end neural
    network [[168](#bib.bib168)]. This implies that there is no need for extensive
    preprocessing and word alignments. Instead, the focus shifted toward network structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [11](#S5.F11 "Figure 11 ‣ V-E2 Neural Machine Translation ‣ V-E Machine
    Translation ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey") shows an example of an end-to-end recurrent neural
    network for machine translation. A sequence of input tokens is fed into the network.
    Once it reaches an end-of-sentence (EOS) token, it starts generating the output
    sequence. The output sequence is generated in the same recurrent manner as the
    input sequence until it reaches an end-of-sentence token. One major advantage
    of this approach is that there is no need to specify the length of the sequence;
    the network takes it into account automatically. In other words, the end-of-sentence
    token determines the length of the sequence. Networks implicitly learn that longer
    input sentences usually lead to longer output sentences with varying length, and
    that ordering can change. For instance, the second example in Fig. [9](#S5.F9
    "Figure 9 ‣ V-E2 Neural Machine Translation ‣ V-E Machine Translation ‣ V Deep
    Learning for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey") shows that adjectives generally come before nouns in English but after
    nouns in Spanish. There is no need to explicitly specify this since the network
    can capture such properties. Moreover, the amount of memory that is used by NMT
    is just a fraction of the memory that is used in traditional statistical machine
    translation [[169](#bib.bib169)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/166eef1b0679d28fec259260764d11cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Alignment in Machine Translation'
  prefs: []
  type: TYPE_NORMAL
- en: '[[164](#bib.bib164)] was one of the early works that incorporated recurrent
    neural networks for machine translation. They were able to achieve a perplexity
    (a measure where lower values indicate better models) that was 43% less than the
    state-of-the-art alignment based translation models. Their recurrent continuous
    translation model (RCTM) is able to capture word ordering, syntax, and meaning
    of the source sentence explicitly. It maps a source sentence into a probability
    distribution over sentences in the target language. RCTM estimates the probability
    $P(f|e)$ of translating a sentence $e=e_{1}+...+e_{k}$ in the source language
    to target language sentence $f=f_{1}+...+f_{m}$. RCTM estimates $P(f|e)$ by considering
    source sentence $e$ as well as the preceding words in the target language $f_{1:i-1}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(f&#124;e)=\prod_{i=1}^{m}P(f_{i}&#124;f_{1:i-1},e)$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'The representation generated by RCTM acts on n-grams in the lower layers, and
    acts more on the whole sentence as one moves to the upper layers. This hierarchical
    representation is performed by applying different layers of convolution. First
    a continuous representation of each word is generated; i.e., if the sentence is
    $e=e_{1}...e_{k}$, the representation of the word $e_{i}$ will be $v(e_{i})\in\mathbb{R}^{q\times
    1}$. This will result in sentence matrix $\textbf{E}^{e}\in\mathbb{R}^{q\times{k}}$
    in which $\textbf{E}_{:,i}^{e}=v(e_{i})$. This matrix representation of the sentence
    will be fed into a series of convolution layers in order to generate the final
    representation e for the recurrent neural network. The approach is illustrated
    in Fig. [10](#S5.F10 "Figure 10 ‣ V-E2 Neural Machine Translation ‣ V-E Machine
    Translation ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey"). Equations for the pipeline are as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s=\textbf{S}.csm(e)$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $h_{1}=\sigma(\textbf{I}.v(f_{1})+\textbf{s})$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $h_{i+1}=\sigma(\textbf{R}.h_{i}+\textbf{I}.v(f_{i+1})+\textbf{s})$ |  |
    (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $o_{i+1}=\textbf{O}.h_{i}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'In order to take into account the sentence length, the authors introduced RCTM
    II which estimates the length of the target sentence. RCTM II was able to achieve
    better perplexity on WMT datasets (see top portion of Table [I](#S4.T1 "TABLE
    I ‣ IV Datasets ‣ Natural Language Processing Advancements By Deep Learning: A
    Survey")) than other existing machine translation systems.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/77661059319e0ecfa045953c41ad33b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Recurrent Continuous Translation Models (RCTM) [[164](#bib.bib164)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'In another line of work,  [[170](#bib.bib170)] presented an end-to-end sequence
    learning approach without heavy assumptions on the structure of the sequence.
    Their approach consists of two LSTMs, one for mapping the input to a vector of
    fixed dimension and another LSTM for decoding the output sequence from the vector.
    Their model was able to handle long sentences as well as sentence representations
    that are sensitive to word order. As shown in Fig. [11](#S5.F11 "Figure 11 ‣ V-E2
    Neural Machine Translation ‣ V-E Machine Translation ‣ V Deep Learning for NLP
    Tasks ‣ Natural Language Processing Advancements By Deep Learning: A Survey"),
    the model reads ”ABC” as an input sequence and produces ”WXYZ” as output sequence.
    The $<EOS>$ token indicates the end of prediction. The network was trained by
    maximizing the log probability of the translation ($\eta$) given the input sequence
    ($\zeta$). In other words, the objective function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4f785debc717c2fbc150fcc7186c6f8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Sequence to sequence learning with LSTM.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1/&#124;\mathcal{D}&#124;\sum_{\begin{subarray}{c}(\eta,\zeta)\in\mathcal{D}\end{subarray}}logP(\eta&#124;\zeta)$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: $\mathcal{D}$ is the training set and $|\mathcal{D}|$ is its size. One of the
    novelties of their approach was reversing word order of the source sentence. This
    helps the LSTM to learn long term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having a fixed-length vector in the decoder phase is one of the bottlenecks
    of the encoder-decoder approach.  [[168](#bib.bib168)] argues that a network will
    have a hard time compressing all the information from the input sentence into
    a fixed-size vector. They address this by allowing the network to search segments
    of the source sentence that are useful for predicting the translation. Instead
    of representing the input sentence as a fixed-size vector, in [[168](#bib.bib168)]
    the input sentence is encoded to a sequence of vectors and a subset of them is
    chosen by using a method called attention mechanism as shown in Fig. [12](#S5.F12
    "Figure 12 ‣ V-E2 Neural Machine Translation ‣ V-E Machine Translation ‣ V Deep
    Learning for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In their approach $P(y_{i}|y_{1},...,y_{i-1},X)=g(y_{i-1},s_{i},c_{i})$, in
    which $s_{i}=f(s_{i-1},y_{i-1},c_{i})$. While previously $c$ was the same for
    all time steps, here $c$ takes a different value, $c_{i}$, at each time step.
    This accounts for the attention mechasim (context vector) around that specific
    time step. $c_{i}$ is computed according to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: $c_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j},\ \alpha_{ij}=\frac{exp(e_{ij})}{{\sum_{k=1}^{T_{x}}exp(e_{ik})}},\
    e_{ij}=a(s_{i-1},h_{j})$.
  prefs: []
  type: TYPE_NORMAL
- en: Here $a$ is the alignment model that is represented by a feed forward neural
    network. Also $h_{j}=[\overset{\rightarrow}{h_{j}^{T}},\overset{\leftarrow}{h_{j}^{T}}]$,
    which is a way to include information both about preceding and following words
    in $h_{j}$. The model was able to outperform the simple encoder-decoder approach
    regardless of input sentence length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Improved machine translation models continue to emerge, driven in part by the
    growth in people’s interest and need to understand other languages Most of them
    are variants of the end-to-end decoder-encoder approach. For example,  [[171](#bib.bib171)]
    tries to deal with the problem of rare words. Their LSTM network consists of encoder
    and decoder layers using residual layers along with the attention mechanism. Their
    system was able to decrease training time, speed up inference, and handle translation
    of rare words. Comparisons between some of the state-of-the-art neural machine
    translation models are summarized in Table [VII](#S5.T7 "TABLE VII ‣ V-E2 Neural
    Machine Translation ‣ V-E Machine Translation ‣ V Deep Learning for NLP Tasks
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/33545956bb7f1a525ac9502281944d8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Attention Mechasim for Neural Machine Translation [[168](#bib.bib168)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: The machine translation state-of-the-art models evaluated on the
    English-German dataset of ACL 2014 Ninth Workshop on Statistical Machine TRranslation.
    The evaluation metric is $BLEU$ score.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional Seq-to-Seq  [[172](#bib.bib172)] | 25.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Attention Is All You Need [[173](#bib.bib173)] | 28.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Weighted Transformer [[174](#bib.bib174)] | 28.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Self Attention [[175](#bib.bib175)] | 29.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepL Translation Machine ^(10)^(10)10https://www.deepl.com/press.html |
    33.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Back-translation [[176](#bib.bib176)] | 35.0 |'
  prefs: []
  type: TYPE_TB
- en: More recently, [[177](#bib.bib177)] provides an interesting single-model implementation
    of massively multilingual NMT. In [[178](#bib.bib178)], authors use BERT to extract
    contextual embeddings and combine BERT with an attention-based NMT model and provide
    state-of-the-art results on various benchmark datasets. [[179](#bib.bib179)] proposes
    mBART which is a seq-to-seq denoising autoencoder and reports that using a pretrained,
    locked (i.e. no modifications) mBART improves performance in terms of the BLEU
    point. [[180](#bib.bib180)] proposes an interesting adversarial framework for
    robustifying NMT against noisy inputs and reports performance gains over the Transformer
    model. [[181](#bib.bib181)] is also an insightful recent work where the authors
    sample context words from the predicted sequence as well as the ground truth to
    try to reconcile the training and inference processes. Finally, [[182](#bib.bib182)]
    is a successful recent effort to prevent the forgetting that often accompanies
    in translating pre-trained language models to other NMT task. [[182](#bib.bib182)]
    achieves that aim primarily by using a dynamically gated model and asymptotic
    distillation.
  prefs: []
  type: TYPE_NORMAL
- en: V-F Question Answering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Question answering (QA) is a fine-grained version of Information Retrieval (IR).
    In IR a desired set of information has to be retrieved from a set of documents.
    The desired information could be a specific document, text, image, etc. On the
    other hand, in QA specific answers are sought, typically ones that can be inferred
    from available documents. Other areas of NLP such as reading comprehension and
    dialogue systems intersect with question answering.
  prefs: []
  type: TYPE_NORMAL
- en: Research in computerized question answering has proceeded since the 1960s. In
    this section, we present a general overview of question answering system history,
    and focus on the breakthroughs in the field. Like all other fields in NLP, question
    answering was also impacted by the advancement of deep learning  [[183](#bib.bib183)],
    so we provide an overview of QA in deep learning contexts. We briefly visit visual
    question answering as well.
  prefs: []
  type: TYPE_NORMAL
- en: V-F1 Rule-based Question Answering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Baseball  [[184](#bib.bib184)] is one of the early works (1961) on QA where
    an effort was made to answer questions related to baseball games by using a game
    database. The baseball system consists of (1) question read-in, (2) dictionary
    lookup for words in the question, (3) syntactic (POS) analysis of the words in
    question, (4) content analysis for extracting the input question, and (5) estimating
    relevance regarding answering the input question.
  prefs: []
  type: TYPE_NORMAL
- en: 'IBM’s  [[185](#bib.bib185)] statistical question answering system consisted
    of four major components:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Question/Answer Type Classification
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query Expansion/Information Retrieval
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name Entity Making
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answer Selection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Some QA systems fail when semantically equivalent relationships are phrased
    differently.  [[186](#bib.bib186)] addressed this by proposing fuzzy relation
    matching based on mutual information and expectation maximization.
  prefs: []
  type: TYPE_NORMAL
- en: V-F2 Question answering in the era of deep learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Smartphones (Siri, Ok Google, Alexa, etc.) and virtual personal assistants
    are common examples of QA systems with which many interact on a daily basis. While
    earlier such systems employed rule-based methods, today their core algorithm is
    based on deep learning. Table [VIII](#S5.T8 "TABLE VIII ‣ V-F2 Question answering
    in the era of deep learning ‣ V-F Question Answering ‣ V Deep Learning for NLP
    Tasks ‣ Natural Language Processing Advancements By Deep Learning: A Survey")
    presents some questions and answers provided by Siri on an iPhone.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: Typical Question Answering performance based on deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question | Answer |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Who invented polio vaccine? | The answer I found is Jonas Salk |'
  prefs: []
  type: TYPE_TB
- en: '| Who wrote Harry Potter? | J.K.Rowling wrote Harry Potter in 1997 |'
  prefs: []
  type: TYPE_TB
- en: '| When was Einstein born? | Albert Einstein was born March 14, 1879 |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/6059be555e9cbc3796e965ec4fddb7f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Fixed length vector sentence representation for input Questions
    and Answers [[187](#bib.bib187)].'
  prefs: []
  type: TYPE_NORMAL
- en: '[[188](#bib.bib188)] was one of the first machine learning based papers that
    reported results on QA for a reading comprehension test. The system tries to pick
    a sentence in the database that has an answer to a question, and a feature vector
    represents each question-sentence pair. The main contribution of [[188](#bib.bib188)]
    is proposing a feature vector representation framework which is aimed to provide
    information for learning the model. There are five classifiers (location, date,
    etc.), one for each type of question. They were able to achieve accuracy competitive
    with previous approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in Fig. [13](#S5.F13 "Figure 13 ‣ V-F2 Question answering in
    the era of deep learning ‣ V-F Question Answering ‣ V Deep Learning for NLP Tasks
    ‣ Natural Language Processing Advancements By Deep Learning: A Survey"), [[187](#bib.bib187)]
    uses convolutional neural networks in order to encode Question-Answer sentence
    pairs in the form of fixed length vectors regardless of the length of the input
    sentence. Instead of using distance measures like cosine correlation, they incorporate
    a non-linear tensor layer to match the relevance between question and answer.
    Equation [9](#S5.E9 "In V-F2 Question answering in the era of deep learning ‣
    V-F Question Answering ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing
    Advancements By Deep Learning: A Survey") calculates the matching degree between
    question $q$ and its corresponding answer $a$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s(q,a)=\textbf{u}^{T}\textbf{f}(\textbf{v}^{T}_{q}\textbf{M}^{[1:r]}\textbf{v}_{a}+\textbf{V}\begin{bmatrix}\textbf{v}_{q}\\
    \textbf{v}_{a}\end{bmatrix}+\textbf{b})$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: f is the standard element-wise non-linearity function, $\textbf{M}^{[1:r]\in
    R^{n_{s}\times n_{s}\times r}}$ is a tensor, $\textbf{V}\in R^{r\times 2n_{s}}$,
    $\textbf{b}\in R^{r}$, $\textbf{u}\in R^{r}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f053b394d250c1d01ea947c1ad46ac85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Example of Dynamic Memory Network (DMN) input-question-answer triplet'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model tries to capture the interaction between question and answer. Inspired
    by findings in neuroscience, [[81](#bib.bib81)] incorporated episodic memory^(11)^(11)11A
    kind of long-term memory that includes conscious recall of previous activities
    together with their meaning. in their Dynamic Memory Network (DMN). By processing
    input sequences and questions, DMN forms episodic memories to answer relevant
    questions. As illustrated in Fig. [14](#S5.F14 "Figure 14 ‣ V-F2 Question answering
    in the era of deep learning ‣ V-F Question Answering ‣ V Deep Learning for NLP
    Tasks ‣ Natural Language Processing Advancements By Deep Learning: A Survey"),
    their system is trained based on raw Input-Question-Answer triplets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DMN consists of four modules that communicate with each other as shown in Fig. [15](#S5.F15
    "Figure 15 ‣ V-F2 Question answering in the era of deep learning ‣ V-F Question
    Answering ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing Advancements
    By Deep Learning: A Survey"). The input module encodes raw input text into a distributed
    vector representation; likewise the question module encodes a question into its
    distributed vector representation. The episodic memory module uses the attention
    mechanism in order to focus on a specific part of the input module. Through an
    iterative process, this module produces a memory vector representation that considers
    the question as well as previous memory. The answer module uses the final memory
    vector to generate an answer. The model improved upon state-of-the-art results
    on tasks such as the ones shown in Fig. [14](#S5.F14 "Figure 14 ‣ V-F2 Question
    answering in the era of deep learning ‣ V-F Question Answering ‣ V Deep Learning
    for NLP Tasks ‣ Natural Language Processing Advancements By Deep Learning: A Survey").
    DMN is one of the architectures that could potentially be used for a variety of
    NLP applications such as classification, question answering, and sequence modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/34d04490c196ab59c82669e11e7adb0c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Interaction between four modules of Dynamic Memory Network [[78](#bib.bib78)].'
  prefs: []
  type: TYPE_NORMAL
- en: '[[189](#bib.bib189)] introduced a Dynamic Coattention Network (DCN) in order
    to address local maxima corresponding to incorrect answers; it is considered to
    be one of the best approaches to question answering.'
  prefs: []
  type: TYPE_NORMAL
- en: V-F3 Visual Question Answering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given an input image, Visual Question Answering (VQA) tries to answer a natural
    language question about the image [[190](#bib.bib190)]. VQN addresses multiple
    problems such as object detection, image segmentation, sentiment analysis, etc.
    [[190](#bib.bib190)] introduced the task of VQA by providing a dataset containing
    over 250K images, 760K questions, and around 10M answers. [[191](#bib.bib191)]
    proposed a neural-based approach to answer the questions regarding the input images.
    As illustrated in Fig. [16](#S5.F16 "Figure 16 ‣ V-F3 Visual Question Answering
    ‣ V-F Question Answering ‣ V Deep Learning for NLP Tasks ‣ Natural Language Processing
    Advancements By Deep Learning: A Survey"), Neural-Image-QA is a deep network consisting
    of CNN and LSTM. Since the questions can have multiple answers, the problem is
    decomposed into predicting a set of answer words $a_{q,x}=\{a_{1},a_{2},...,a_{N(q,x)}\}$
    from a finite vocabulary set $\nu$ where $N(q,x)$ represents the count of answer
    words regarding a given question.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae7ec0b22badef0f1370da11ef05dcb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Neural Image Question Answering [[191](#bib.bib191)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cf5c9dc202153f9a0179f472e898889d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Spatial Memory Network for VQA. Bright Areas are regions the model
    is attending [[192](#bib.bib192)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do humans and computers look at the same regions to answer questions about
    an image? [[193](#bib.bib193)] tries to answer this question by conducting large-scale
    studies on human attention in VQA. Their findings show that VQAs do not seem to
    be looking at the same regions as humans. Finally, [[192](#bib.bib192)] incorporates
    a spatial memory network for VQA. Fig. [17](#S5.F17 "Figure 17 ‣ V-F3 Visual Question
    Answering ‣ V-F Question Answering ‣ V Deep Learning for NLP Tasks ‣ Natural Language
    Processing Advancements By Deep Learning: A Survey") shows the inference process
    of their model. As illustrated in the figure, the specific attention mechanism
    in their system can highlight areas of interest in the input image. [[194](#bib.bib194)]
    introduces BLOCK, a bilinear fusion model based on superdiagonal tensor decomposition
    for the VQA task, with state-of-the-art performance and the code made public on
    github. To improve the generalization of existing models to test data of different
    distribution, [[195](#bib.bib195)] introduces a self-critical training objective
    to help find visual regions of prominent visual/textual correlation with a focus
    on recognizing influential objects and detecting and devaluing incorrect dominant
    answers.'
  prefs: []
  type: TYPE_NORMAL
- en: V-G Document Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Document summarization refers to a set of problems involving generation of summary
    sentences given one or multiple documents as input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, text summarization fits into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extractive Summarization, where the goal is to identify the most salient sentences
    in the document and return them as the summary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Abstractive Summarization, where the goal is to generate summary sentences from
    scratch; they may contain novel words that do not appear in the original document.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each of these methods has its own advantages and disadvantages. Extractive summarization
    is prone to generate long and sometimes overlapping summary sentences; however,
    the result reflects the author’s mode of expression. Abstractive methods generate
    a shorter summary but they are hard to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a vast amount of research on the topic of text summarization using
    extractive and abstractive methods. As one of the earliest works on using neural
    networks for extractive summarization,  [[196](#bib.bib196)] proposed a framework
    that used a ranking technique to extract the most salient sentences in the input.
    This model was improved by [[197](#bib.bib197)] which used a document-level encoder
    to represent sentences, and a classifier to rank these sentences. On the other
    hand, in abstractive summarization, it was  [[198](#bib.bib198)] which, for the
    first time, used attention over a sequence-to-sequence (seq2seq) model for the
    problem of headline generation. However, since simple attention models perform
    worse than extractive models, therefore more effective attention models such as
    graph-based attention [[199](#bib.bib199)] and transformers [[173](#bib.bib173)]
    have been proposed for this task. To further improve abstractive text summarization
    models,  [[200](#bib.bib200)] proposed the first pointer-generator model and applied
    it to the DeepMind QA dataset [[201](#bib.bib201)]. As a result of this work,
    the CNN/Daily Mail dataset emerged which is now one of the widely used datasets
    for the summarization task. A copy mechanism was also adopted by [[202](#bib.bib202)]
    for similar tasks. But their analysis reveals a key problem with attention-based
    encoder-decoder models: they often generate unusual summaries consisting of repeated
    phrases. Recently,  [[62](#bib.bib62)] reached state-of-the-art results on the
    abstractive text summarization using a similar framework. They alleviated the
    unnatural summaries by avoiding generating unknown tokens and replacing these
    words with tokens from the input article. Later, researchers moved their focus
    to methods that use sentence-embedding to first select the most salient sentence
    in the document and then change them to make them more abstractive [[203](#bib.bib203),
    [204](#bib.bib204)]. In these models, salient sentences are extracted first and
    then a paraphrasing model is used to make them abstractive. The extraction employs
    a sentence classifier or ranker while the abstractor tries to remove the extra
    information in a sentence and present it as a shorter summary. Fast-RL [[203](#bib.bib203)]
    is the first framework in this family of works. In Fast-RL, the extractor is pre-trained
    to select salient sentences and the abstractor is pre-trained using a pointer-generator
    model to generate paraphrases. Finally, to merge these two non-differentiable
    components, they propose using Actor-Critic Q-learning methods in which the actor
    receives a single document and generates the output while the critic evaluates
    the output based on comparison with the ground-truth summary.'
  prefs: []
  type: TYPE_NORMAL
- en: Though the standard way to evaluate the performance of summarization models
    is with ROUGE [[67](#bib.bib67)] and BLEU [[68](#bib.bib68)], there are major
    problems with such measures. For instance, the ROUGE measure focuses on the number
    of shared n-grams between two sentences. Such a method incorrectly assigns a low
    score to an abstractive summary that uses different words yet provides an excellent
    paraphrase that humans would rate highly. Clearly, better automated evaluation
    methods are needed in such cases.
  prefs: []
  type: TYPE_NORMAL
- en: There are additional problems with current summarization models. Shi et al. [[205](#bib.bib205)]
    provides a comprehensive survey on text summarization.
  prefs: []
  type: TYPE_NORMAL
- en: '[[206](#bib.bib206)] provides a recent survey on summarization methods. [[207](#bib.bib207)]
    provides an advanced composite deep learning model, based on LSTMs and Restricted
    Boltzmann Machine, for multi-doc opinion summarization. A very influential recent
    work, [[208](#bib.bib208)], introduces Hibert ( HIerachical Bidirectional Encoder
    Representations from Transformers) as a pre-trained initialization for document
    summarization and report state-of-the-art performance.'
  prefs: []
  type: TYPE_NORMAL
- en: V-H Dialogue Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dialogue Systems are quickly becoming a principal instrument in human-computer
    interaction, due in part to their promising potential and commercial value [[209](#bib.bib209)].
    One application is automated customer service, supporting both online and bricks-and-mortar
    businesses. Customers expect an ever-increasing level of speed, accuracy, and
    respect while dealing with companies and their services. Due to the high cost
    of knowledgeable human resources, companies frequently turn to intelligent conversational
    machines. Note that the phrases conversational machines and dialogue machines
    are often used interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dialogue systems are usually task-based or non-task-based (Fig. [18](#S5.F18
    "Figure 18 ‣ V-H Dialogue Systems ‣ V Deep Learning for NLP Tasks ‣ Natural Language
    Processing Advancements By Deep Learning: A Survey")). Though there might be Automatic
    Speech Recognition (ASR) and Language-to-Speech (L2S) components in a dialogue
    system, the discussion of this section is solely about the linguistic components
    of dialogue systems; concepts associated with speech technology are ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4fba3bb9777b8d1996235adcf3378fd2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: The framework of a dialogue system. A dialogue system can be task
    oriented or used for natural language generation based on the user input which
    is also known as a chat bot.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite useful statistical models employed in the backend of dialogue systems (especially
    in language understanding modules), most deployed dialogue systems rely on expensive
    hand-crafted and manual features for operation. Furthermore, the generalizability
    of these manually engineered systems to other domains and functionalities is problematic. Hence,
    recent attention has focused on deep learning for the enhancement of performance,
    generalizability, and robustness. Deep learning facilitates the creation of end-to-end
    task-oriented dialogue systems, which enriches the framework to generalize conversations
    beyond annotated task-specific dialogue resources.
  prefs: []
  type: TYPE_NORMAL
- en: V-H1 Task-based Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The structure of a task-based dialogue system usually consists of the following
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Natural Language Understanding (NLU): This component deals with understanding
    and interpreting user’s spoken context by assigning a constituent structure to
    the spoken utterance (e.g., a sentence) and captures its syntactic representation
    and semantic interpretation, to allow the back-end operation/task. NLU is usually
    leveraged regardless of the dialogue context.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dialogue Manager (DM): The generated representation by NLU would be handled
    by the dialogue manager, which investigates the context and returns a reasonable
    semantic-related response.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Natural Language Generation (NLG): The natural language generation (NLG) component
    produces an utterance based on the response provided by the DM component.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The general pipeline is as follows: NLU module (i.e., semantic decoder) transforms
    the output of the speech recognition module to some dialogue elements. Then the
    DM processes these dialogue elements and provides a suitable response which is
    fed to the NLG for response generation. The main pipeline in NLU is to classify
    the user query domain and user intent, and fill a set of slots to create a semantic
    frame. It is usually customary to perform the intent prediction and the slot filling
    simultaneously [[210](#bib.bib210)]. Most of the task-oriented dialogue systems
    employ slot-filling approaches to classify user intent in the specific domain
    of the conversation. For this aim, having predefined tasks is required; this depends
    on manually crafted states with different associated slots. Henceforth, a designed
    dialogue system would be of limited or no use for other tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Recent task-oriented dialogue systems have been designed based on deep reinforcement
    learning, which provided promising results regarding performance [[211](#bib.bib211)], domain
    adaptation [[212](#bib.bib212)], and dialogue generation [[213](#bib.bib213)].
    This was due to a shift towards end-to-end trainable frameworks to design and
    deploy task-oriented dialogue systems. Instead of the traditionally utilized pipeline,
    an end-to-end framework incorporates and uses a single module that deals with
    external databases. Despite the tractability of end-to-end dialogue systems (i.e.,
    easy to train and simple to engineer), due to their need for interoperability
    with external databases via queries, they are not well-suited for task-oriented
    settings. Some approaches to this challenge include converting the user input
    into internal representations [[214](#bib.bib214)], combining supervised and reinforced
    learning [[215](#bib.bib215)], and extending the memory network approach [[216](#bib.bib216)]
    for question-answering to a dialog system [[217](#bib.bib217)].
  prefs: []
  type: TYPE_NORMAL
- en: V-H2 Non-task-based Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As opposed to task-based dialogue systems, the goal behind designing and deploying
    non-task-based dialogue systems is to empower a machine with the ability to have
    a natural conversation with humans [[218](#bib.bib218)]. Typically, chatbots are
    of one of the following types: retrieval-based methods and generative methods.
    Retrieval-based models have access to information resources and can provide more
    concise, fluent, and accurate responses. However, they are limited regarding the
    variety of responses they can provide due to their dependency on backend data
    resources. Generative models, on the other hand, have the advantage of being able
    to produce suitable responses when such responses are not in the corpus. However,
    as opposed to retrieval-based models, they are more prone to grammatical and conceptual
    mistakes arising from their generative models.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-based methods select an appropriate response from the candidate responses.
    Therefore, the key element is the query-response operation. In general, this problem
    has been formulated as a search problem and uses IR techniques for task completion [[219](#bib.bib219)].
    Retrieval-based methods usually employ either Single-turn Response Matching or
    Multi-turn Response Matching. In the first type, the current query (message) is
    solely used to select a suitable response [[220](#bib.bib220)]. The latter type
    takes the current message and previous utterances as the system input and retrieves
    a response based on the instant and temporal information. The model tries to choose
    a response which considers the whole context to guarantee conversation consistency.
    An LSTM-based model has been proposed [[221](#bib.bib221)] for context and response
    vectors creation. In [[222](#bib.bib222)], various features and multiple data
    inputs have been incorporated to be ingested using a deep learning framework.
    Current base models regarding retrieval-based chatbots rely on multi-turn response
    selection augmented by an attention mechanism and sequence matching [[223](#bib.bib223)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative models don’t assume the availability of pre-defined responses. New
    responses are produced from scratch and are based on the trained model. Generative
    models are typically based on sequence to sequence models and map an input query
    to a target element as the response. In general, designing and implementing a
    dialogue agent to be able to converse at the human level is very challenging.
    The typical approach usually consists of learning and imitating human conversation.
    For this goal, the machine is generally trained on large corpora of conversations.
    However, this does not directly remedy the issue of encountering out-of-corpus
    conversation. The question is: How can an agent be taught to generate proper responses
    to conversations that it never has seen? It must handle content that is not exactly
    available in the data corpus that the machine has been trained on, due to the
    lack of content matching between the query and the corresponding response, resulting
    from the wide range of plausible queries that humans can provide.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle the aforementioned general problem, some fundamental questions must
    be answered: (1) What are the core characteristics of a natural conversation?
    (2) How can these characteristics be measured? (3) How can we incorporate this
    knowledge in a machine, i.e., the dialogue system? Effective integration of these
    three elements determines the intelligence of a machine. A qualitative criterion
    is to observe if the generated utterances can be distinguished from natural human
    dialogues. For quantitative evaluation, adversarial evaluation was initially used
    for quality assessment of sentence generation [[224](#bib.bib224)] and employed
    for quality evaluation of dialogue systems [[225](#bib.bib225)]. Recent advancements
    in sequence to sequence modeling encouraged many research efforts regarding natural
    language generation [[226](#bib.bib226)]. Furthermore, deep reinforcement learning
    yields promising performance in natural language generation [[213](#bib.bib213)].'
  prefs: []
  type: TYPE_NORMAL
- en: V-H3 Final note on dialogue systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite remarkable advancements in AI and much attention dedicated to dialogue
    systems, in reality, successful commercial tools, such as Apple’s Siri and Amazon’s
    Alexa, still heavily rely on handcrafted features. It still is very challenging
    to design and train data-driven dialogue machines given the complexity of the
    natural language, the difficulties in framework design, and the complex nature
    of available data sources.
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we presented a comprehensive survey of the most distinguished
    works in Natural Language Processing using deep learning. We provided a categorized
    context for introducing different NLP core concepts, aspects, and applications,
    and emphasized the most significant conducted research efforts in each associated
    category. Deep learning and NLP are two of the most rapidly developing research
    topics nowadays. Due to this rapid progress, it is hoped that soon, new effective
    models will supersede the current state-of-the-art approaches. This may cause
    some of the references provided in the survey to become dated, but those are likely
    to be cited by new publications that describe improved methods
  prefs: []
  type: TYPE_NORMAL
- en: Neverthless, one of the essential characteristics of this survey is its educational
    aspect, which provides a precise understanding of the critical elements of this
    field and explains the most notable research works. Hopefully, this survey will
    guide students and researchers with essential resources, both to learn what is
    necessary to know, and to advance further the integration of NLP with deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] C. D. Manning, C. D. Manning, and H. Schütze, Foundations of statistical
    natural language processing. MIT Press, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional networks
    for text classification,” in Advances in neural information processing systems,
    pp. 649–657, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using RNN encoder-decoder for
    statistical machine translation,” arXiv preprint arXiv:1406.1078, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] S. Wu, K. Roberts, S. Datta, J. Du, Z. Ji, Y. Si, S. Soni, Q. Wang, Q. Wei,
    Y. Xiang, B. Zhao, and H. Xu, “Deep learning in clinical natural language processing:
    a methodical review,” Journal of the American Medical Informatics Association,
    vol. 27, pp. 457–470, mar 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] R. Collobert and J. Weston, “A unified architecture for natural language
    processing: Deep neural networks with multitask learning,” in Proceedings of the
    25th international conference on Machine learning, pp. 160–167, ACM, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei,
    “Large-scale video classification with convolutional neural networks,” in Proceedings
    of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1725–1732,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring
    mid-level image representations using convolutional neural networks,” in Proceedings
    of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1717–1724,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb,
    “Learning from simulated and unsupervised images through adversarial training,”
    in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 2107–2116, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, “Deep Learning
    for Computer Vision: A Brief Review,” Computational Intelligence and Neuroscience,
    Feb 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V. Hernandez,
    L. Krpalkova, D. Riordan, and J. Walsh, “Deep learning vs. traditional computer
    vision,” in Advances in Computer Vision (K. Arai and S. Kapoor, eds.), (Cham),
    pp. 128–144, Springer International Publishing, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with recurrent
    neural networks,” in International Conference on Machine Learning, pp. 1764–1772,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case,
    J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al., “Deep speech 2: End-to-end
    speech recognition in English and Mandarin,” in ICML, pp. 173–182, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] U. Kamath, J. Liu, and J. Whitaker, Deep learning for NLP and speech recognition,
    vol. 84. Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] C. D. Santos and B. Zadrozny, “Learning character-level representations
    for part-of-speech tagging,” in Proceedings of the 31st International Conference
    on Machine Learning (ICML-14), pp. 1818–1826, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] B. Plank, A. Søgaard, and Y. Goldberg, “Multilingual part-of-speech tagging
    with bidirectional long short-term memory models and auxiliary loss,” arXiv preprint
    arXiv:1604.05529, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] C. D. Manning, “Part-of-speech tagging from 97% to 100%: is it time for
    some linguistics?,” in International Conference on Intelligent Text Processing
    and Computational Linguistics, pp. 171–189, Springer, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] R. D. Deshmukh and A. Kiwelekar, “Deep learning techniques for part of
    speech tagging by natural language processing,” in 2020 2nd International Conference
    on Innovative Mechanisms for Industry Applications (ICIMIA), pp. 76–81, IEEE,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer, “Neural
    architectures for named entity recognition,” arXiv preprint arXiv:1603.01360,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. P. Chiu and E. Nichols, “Named entity recognition with bidirectional
    LSTM-CNNs,” arXiv preprint arXiv:1511.08308, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] V. Yadav and S. Bethard, “A survey on recent advances in named entity
    recognition from deep learning models,” arXiv preprint arXiv:1910.11470, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. Li, A. Sun, J. Han, and C. Li, “A survey on deep learning for named
    entity recognition,” IEEE Transactions on Knowledge and Data Engineering, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Zhou and W. Xu, “End-to-end learning of semantic role labeling using
    recurrent neural networks,” in Proceedings of the 53rd Annual Meeting of the Association
    for Computational Linguistics and the 7th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers), vol. 1, pp. 1127–1137, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] D. Marcheggiani, A. Frolov, and I. Titov, “A simple and accurate syntax-agnostic
    neural model for dependency-based semantic role labeling,” arXiv preprint arXiv:1701.02593,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. He, K. Lee, M. Lewis, and L. Zettlemoyer, “Deep semantic role labeling:
    What works and what’s next,” in Proceedings of the 55th Annual Meeting of the
    Association for Computational Linguistics (Volume 1: Long Papers), vol. 1, pp. 473–483,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. He, Z. Li, and H. Zhao, “Syntax-aware multilingual semantic role labeling,”
    arXiv preprint arXiv:1909.00310, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in deep
    learning based natural language processing,” IEEE Computational Intelligence Magazine,
    vol. 13, no. 3, pp. 55–75, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Kang, Z. Cai, C.-W. Tan, Q. Huang, and H. Liu, “Natural language processing
    (NLP) in management research: A literature review,” Journal of Management Analytics,
    vol. 7, pp. 139–172, apr 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] T. Greenwald, “What exactly is artificial intelligence, anyway?.” [https://www.wsj.com/articles/what-exactly-is-artificial-intelligence-anyway-1525053960](https://www.wsj.com/articles/what-exactly-is-artificial-intelligence-anyway-1525053960),
    April 2018. Wall Street Journal Online Article.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] U. Sivarajah, M. M. Kamal, Z. Irani, and V. Weerakkody, “Critical analysis
    of big data challenges and analytical methods,” Journal of Business Research,
    vol. 70, pp. 263–286, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Z. C. Lipton, J. Berkowitz, and C. Elkan, “A critical review of recurrent
    neural networks for sequence learning,” arXiv preprint arXiv:1506.00019, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. Kim, “Convolutional neural networks for sentence classification,” arXiv
    preprint arXiv:1408.5882, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng, “Parsing natural scenes
    and natural language with recursive neural networks,” in Proceedings of the 28th
    international conference on machine learning (ICML-11), pp. 129–136, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in Advances in neural information processing
    systems, pp. 1097–1105, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] C. dos Santos and M. Gatti, “Deep convolutional neural networks for sentiment
    analysis of short texts,” in Proceedings of COLING 2014, the 25th International
    Conference on Computational Linguistics: Technical Papers, pp. 69–78, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] R. Johnson and T. Zhang, “Effective use of word order for text categorization
    with convolutional neural networks,” arXiv preprint arXiv:1412.1058, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] R. Johnson and T. Zhang, “Semi-supervised convolutional neural networks
    for text categorization via region embedding,” in Advances in neural information
    processing systems, pp. 919–927, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao, “Relation classification
    via convolutional deep neural network,” in Proceedings of COLING 2014, the 25th
    International Conference on Computational Linguistics: Technical Papers, pp. 2335–2344,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] T. H. Nguyen and R. Grishman, “Relation extraction: Perspective from convolutional
    neural networks,” in Proceedings of the 1st Workshop on Vector Space Modeling
    for Natural Language Processing, pp. 39–48, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] T. Mikolov, M. Karafiát, L. Burget, J. Černockỳ, and S. Khudanpur, “Recurrent
    neural network based language model,” in Eleventh Annual Conference of the International
    Speech Communication Association, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in Advances in neural
    information processing systems, pp. 2672–2680, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv preprint
    arXiv:1701.07875, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel,
    “Infogan: Interpretable representation learning by information maximizing generative
    adversarial nets,” in Advances in neural information processing systems, pp. 2172–2180,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
    with deep convolutional generative adversarial networks,” arXiv preprint arXiv:1511.06434,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of
    GANs for improved quality, stability, and variation,” arXiv preprint arXiv:1710.10196,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] N. Tavaf, A. Torfi, K. Ugurbil, and P.-F. Van de Moortele, “GRAPPA-GANs
    for Parallel MRI Reconstruction,” arXiv preprint arXiv:2101.03135, Jan 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] L. Yu, W. Zhang, J. Wang, and Y. Yu, “Seqgan: Sequence generative adversarial
    nets with policy gradient,” in Thirty-First AAAI Conference on Artificial Intelligence,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky, “Adversarial
    learning for neural dialogue generation,” arXiv preprint arXiv:1701.06547, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up?: sentiment classification
    using machine learning techniques,” in Proceedings of the ACL-02 conference on
    Empirical methods in natural language processing-Volume 10, pp. 79–86, Association
    for Computational Linguistics, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Z. S. Harris, “Distributional structure,” Word, vol. 10, no. 2-3, pp. 146–162,
    1954.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic
    language model,” Journal of machine learning research, vol. 3, no. Feb., pp. 1137–1155,
    2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Q. Le and T. Mikolov, “Distributed representations of sentences and documents,”
    in International Conference on Machine Learning, pp. 1188–1196, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed
    representations of words and phrases and their compositionality,” in Advances
    in neural information processing systems, pp. 3111–3119, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba,
    and S. Fidler, “Skip-thought vectors,” in Advances in neural information processing
    systems, pp. 3294–3302, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of
    word representations in vector space,” arXiv preprint arXiv:1301.3781, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] G. Lebanon et al., Riemannian geometry and statistical machine learning.
    LAP LAMBERT Academic Publishing, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] J. Leskovec, A. Rajaraman, and J. D. Ullman, Mining of massive datasets.
    Cambridge University Press, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. Goldberg, “Neural network methods for natural language processing,”
    Synthesis Lectures on Human Language Technologies, vol. 10, no. 1, pp. 1–309,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] J. Wehrmann, W. Becker, H. E. Cagnini, and R. C. Barros, “A character-based
    convolutional neural network for language-agnostic Twitter sentiment analysis,”
    in Neural Networks (IJCNN), 2017 International Joint Conference on, pp. 2384–2391,
    IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word vectors
    with subword information,” arXiv preprint arXiv:1607.04606, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] J. Botha and P. Blunsom, “Compositional morphology for word representations
    and language modelling,” in International Conference on Machine Learning, pp. 1899–1907,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] A. See, P. J. Liu, and C. D. Manning, “Get to the point: Summarization
    with pointer-generator networks,” in ACL, vol. 1, pp. 1073–1083, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] R. Paulus, C. Xiong, and R. Socher, “A deep reinforced model for abstractive
    summarization,” arXiv preprint arXiv:1705.04304, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled sampling
    for sequence prediction with recurrent neural networks,” in Advances in Neural
    Information Processing Systems, pp. 1171–1179, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] K. Goyal, G. Neubig, C. Dyer, and T. Berg-Kirkpatrick, “A continuous relaxation
    of beam search for end-to-end training of neural sequence models,” in Thirty-Second
    AAAI Conference on Artificial Intelligence, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] W. Kool, H. Van Hoof, and M. Welling, “Stochastic beams and where to find
    them: The gumbel-top-k trick for sampling sequences without replacement,” in International
    Conference on Machine Learning, pp. 3499–3508, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,” in
    Text summarization branches out, pp. 74–81, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method for automatic
    evaluation of machine translation,” in Proceedings of the 40th annual meeting
    on Association for Computational Linguistics, pp. 311–318, Association for Computational
    Linguistics, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] S. Banerjee and A. Lavie, “METEOR: An automatic metric for MT evaluation
    with improved correlation with human judgments,” in Proceedings of the ACL workshop
    on intrinsic and extrinsic evaluation measures for machine translation and/or
    summarization, pp. 65–72, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Y. Keneshloo, T. Shi, C. K. Reddy, and N. Ramakrishnan, “Deep reinforcement
    learning for sequence to sequence models,” arXiv preprint arXiv:1805.09461, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence level training
    with recurrent neural networks,” arXiv preprint arXiv:1511.06732, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] W. Zaremba and I. Sutskever, “Reinforcement learning neural Turing machines-revised,”
    arXiv preprint arXiv:1505.00521, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” in Reinforcement Learning, pp. 5–32, Springer,
    1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.
    MIT Press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] C. J. Watkins and P. Dayan, “Q-learning,” Machine Learning, vol. 8, no. 3-4,
    pp. 279–292, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] H. Daumé, J. Langford, and D. Marcu, “Search-based structured prediction,”
    Machine learning, vol. 75, no. 3, pp. 297–325, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
    deep visuomotor policies,” The Journal of Machine Learning Research, vol. 17,
    no. 1, pp. 1334–1373, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] V. Mnih, N. Heess, A. Graves, et al., “Recurrent models of visual attention,”
    in Advances in neural information processing systems, pp. 2204–2212, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] C. Sun, L. Huang, and X. Qiu, “Utilizing BERT for aspect-based sentiment
    analysis via constructing auxiliary sentence,” arXiv preprint arXiv:1903.09588,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] P. Resnik and J. Lin, “Evaluation of NLP systems,” The handbook of computational
    linguistics and natural language processing, vol. 57, pp. 271–295, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury, I. Gulrajani,
    V. Zhong, R. Paulus, and R. Socher, “Ask me anything: Dynamic memory networks
    for natural language processing,” in International Conference on Machine Learning,
    pp. 1378–1387, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF models for sequence
    tagging,” arXiv preprint arXiv:1508.01991, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] D. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev, S. Petrov,
    and M. Collins, “Globally normalized transition-based neural networks,” arXiv
    preprint arXiv:1603.06042, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] X. Xue and J. Zhang, “Part-of-speech tagging of building codes empowered
    by deep learning and transformational rules,” Advanced Engineering Informatics,
    vol. 47, p. 101235, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] L. Liu, J. Shang, X. Ren, F. F. Xu, H. Gui, J. Peng, and J. Han, “Empower
    sequence labeling with task-aware neural language model,” in Thirty-Second AAAI
    Conference on Artificial Intelligence, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Z. Yang, R. Salakhutdinov, and W. W. Cohen, “Transfer learning for sequence
    tagging with hierarchical recurrent networks,” arXiv preprint arXiv:1703.06345,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] X. Ma and E. Hovy, “End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF,”
    arXiv preprint arXiv:1603.01354, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] M. Yasunaga, J. Kasai, and D. Radev, “Robust multilingual part-of-speech
    tagging via adversarial training,” arXiv preprint arXiv:1711.04903, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] W. Ling, T. Luís, L. Marujo, R. F. Astudillo, S. Amir, C. Dyer, A. W.
    Black, and I. Trancoso, “Finding function in form: Compositional character models
    for open vocabulary word representation,” arXiv preprint arXiv:1508.02096, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A. Akbik, D. Blythe, and R. Vollgraf, “Contextual string embeddings for
    sequence labeling,” in Proceedings of the 27th International Conference on Computational
    Linguistics, pp. 1638–1649, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] B. Bohnet, R. McDonald, G. Simoes, D. Andor, E. Pitler, and J. Maynez,
    “Morphosyntactic tagging with a Meta-BiLSTM model over context sensitive token
    encodings,” arXiv preprint arXiv:1805.08237, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Legrand and R. Collobert, “Joint RNN-based greedy parsing and word
    composition,” arXiv preprint arXiv:1412.7028, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Legrand and R. Collobert, “Deep neural networks for syntactic parsing
    of morphologically rich languages,” in Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 573–578,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] A. Kuncoro, M. Ballesteros, L. Kong, C. Dyer, G. Neubig, and N. A. Smith,
    “What do recurrent neural network grammars learn about syntax?,” arXiv preprint
    arXiv:1611.05774, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] J. Liu and Y. Zhang, “In-order transition-based constituent parsing,”
    arXiv preprint arXiv:1707.05000, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] D. Fried, M. Stern, and D. Klein, “Improving neural parsing by disentangling
    model combination and reranking effects,” arXiv preprint arXiv:1707.03058, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] N. Kitaev and D. Klein, “Constituency parsing with a self-attentive encoder,”
    arXiv preprint arXiv:1805.01052, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] D. Chen and C. Manning, “A fast and accurate dependency parser using neural
    networks,” in Proceedings of the 2014 conference on empirical methods in natural
    language processing (EMNLP), pp. 740–750, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] T. Dozat and C. D. Manning, “Deep biaffine attention for neural dependency
    parsing,” arXiv preprint arXiv:1611.01734, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] E. Kiperwasser and Y. Goldberg, “Simple and accurate dependency parsing
    using bidirectional LSTM feature representations,” arXiv preprint arXiv:1603.04351,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] C. Dyer, M. Ballesteros, W. Ling, A. Matthews, and N. A. Smith, “Transition-based
    dependency parsing with stack long short-term memory,” arXiv preprint arXiv:1505.08075,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] S. Jaf and C. Calder, “Deep learning for natural language parsing,” IEEE
    Access, vol. 7, pp. 131363–131373, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y. Zhang, F. Tiryaki, M. Jiang, and H. Xu, “Parsing clinical text using
    the state-of-the-art deep learning based parsers: a systematic comparison,” BMC
    medical informatics and decision making, vol. 19, no. 3, p. 77, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Y. Zhang, Z. Li, and M. Zhang, “Efficient second-order treecrf for neural
    dependency parsing,” arXiv preprint arXiv:2005.00975, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] T. Dozat and C. D. Manning, “Deep biaffine attention for neural dependency
    parsing,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Z. Tan, M. Wang, J. Xie, Y. Chen, and X. Shi, “Deep semantic role labeling
    with self-attention,” arXiv preprint arXiv:1712.01586, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] D. Marcheggiani and I. Titov, “Encoding sentences with graph convolutional
    networks for semantic role labeling,” arXiv preprint arXiv:1703.04826, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] E. Strubell, P. Verga, D. Andor, D. Weiss, and A. McCallum, “Linguistically-informed
    self-attention for semantic role labeling,” arXiv preprint arXiv:1804.08199, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] L. He, K. Lee, O. Levy, and L. Zettlemoyer, “Jointly predicting predicates
    and arguments in neural semantic role labeling,” arXiv preprint arXiv:1805.04787,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and
    L. Zettlemoyer, “Deep contextualized word representations,” arXiv preprint arXiv:1802.05365,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Z. Tan, M. Wang, J. Xie, Y. Chen, and X. Shi, “Deep semantic role labeling
    with self-attention,” in Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Z. Li, S. He, H. Zhao, Y. Zhang, Z. Zhang, X. Zhou, and X. Zhou, “Dependency
    or span, end-to-end uniform semantic role labeling,” in Proceedings of the AAAI
    Conference on Artificial Intelligence, vol. 33, pp. 6730–6737, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Björkelund, O. Uryupina,
    Y. Zhang, and Z. Zhong, “Towards robust linguistic analysis using OntoNotes,”
    in Proceedings of the Seventeenth Conference on Computational Natural Language
    Learning, pp. 143–152, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolutional neural
    network for modelling sentences,” arXiv preprint arXiv:1404.2188, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, and R. Ward,
    “Deep sentence embedding using long short-term memory networks: Analysis and application
    to information retrieval,” IEEE/ACM Transactions on Audio, Speech and Language
    Processing (TASLP), vol. 24, no. 4, pp. 694–707, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical
    attention networks for document classification,” in Proceedings of the 2016 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, pp. 1480–1489, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural networks
    for text classification.,” in AAAI, vol. 333, pp. 2267–2273, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] C. Zhou, C. Sun, Z. Liu, and F. Lau, “A C-LSTM neural network for text
    classification,” arXiv preprint arXiv:1511.08630, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, and
    J. Gao, “Deep learning based text classification: A comprehensive review,” arXiv
    preprint arXiv:2004.03705, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] M. Zulqarnain, R. Ghazali, Y. M. M. Hassim, and M. Rehan, “A comparative
    review on deep learning models for text classification,” Indones. J. Electr. Eng.
    Comput. Sci, vol. 19, no. 1, pp. 325–335, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] A. Conneau, H. Schwenk, L. Barrault, and Y. LeCun, “Very deep convolutional
    networks for text classification,” in Proceedings of the 15th Conference of the
    European Chapter of the Association for Computational Linguistics: Volume 1, Long
    Papers, vol. 1, pp. 1107–1116, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] R. Johnson and T. Zhang, “Deep pyramid convolutional neural networks
    for text categorization,” in Proceedings of the 55th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), vol. 1, pp. 562–570, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] R. Johnson and T. Zhang, “Supervised and semi-supervised text categorization
    using LSTM for region embeddings,” arXiv preprint arXiv:1602.02373, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. Howard and S. Ruder, “Universal language model fine-tuning for text
    classification,” in Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), vol. 1, pp. 328–339, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa,
    “Natural language processing (almost) from scratch,” Journal of Machine Learning
    Research, vol. 12, no. Aug., pp. 2493–2537, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] G. Mesnil, X. He, L. Deng, and Y. Bengio, “Investigation of recurrent-neural-network
    architectures and learning methods for spoken language understanding.,” in Interspeech,
    pp. 3771–3775, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] F. Dernoncourt, J. Y. Lee, and P. Szolovits, “NeuroNER: an easy-to-use
    program for named-entity recognition based on neural networks,” Conference on
    Empirical Methods on Natural Language Processing (EMNLP), 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] A. Baevski, S. Edunov, Y. Liu, L. Zettlemoyer, and M. Auli, “Cloze-driven
    pretraining of self-attention networks,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] E. F. Tjong Kim Sang and F. De Meulder, “Introduction to the CoNLL-2003
    shared task: Language-independent named entity recognition,” in Proceedings of
    the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4,
    pp. 142–147, Association for Computational Linguistics, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] K. Clark, M.-T. Luong, C. D. Manning, and Q. V. Le, “Semi-supervised
    sequence modeling with cross-view training,” arXiv preprint arXiv:1809.08370,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of deep bidirectional transformers for language understanding,” arXiv preprint
    arXiv:1810.04805, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] R. Socher, B. Huval, C. D. Manning, and A. Y. Ng, “Semantic compositionality
    through recursive matrix-vector spaces,” in Proceedings of the 2012 joint conference
    on empirical methods in natural language processing and computational natural
    language learning, pp. 1201–1211, Association for Computational Linguistics, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Z. Geng, G. Chen, Y. Han, G. Lu, and F. Li, “Semantic relation extraction
    using sequential and tree-structured lstm with attention,” Information Sciences,
    vol. 509, pp. 183–192, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] X. Han, T. Gao, Y. Lin, H. Peng, Y. Yang, C. Xiao, Z. Liu, P. Li, M. Sun,
    and J. Zhou, “More data, more relations, more context and more openness: A review
    and outlook for relation extraction,” arXiv preprint arXiv:2004.03186, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] K. Clark and C. D. Manning, “Deep reinforcement learning for mention-ranking
    coreference models,” arXiv preprint arXiv:1609.08667, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] K. Lee, L. He, and L. Zettlemoyer, “Higher-order coreference resolution
    with coarse-to-fine inference,” arXiv preprint arXiv:1804.05392, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] H. Fei, X. Li, D. Li, and P. Li, “End-to-end deep reinforcement learning
    based coreference resolution,” in Proceedings of the 57th Annual Meeting of the
    Association for Computational Linguistics, pp. 660–665, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] W. Wu, F. Wang, A. Yuan, F. Wu, and J. Li, “Corefqa: Coreference resolution
    as query-based span prediction,” in Proceedings of the 58th Annual Meeting of
    the Association for Computational Linguistics, pp. 6953–6963, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Y. Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao, “Event extraction via dynamic
    multi-pooling convolutional neural networks,” in Proceedings of the 53rd Annual
    Meeting of the Association for Computational Linguistics and the 7th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers), vol. 1,
    pp. 167–176, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] T. H. Nguyen and R. Grishman, “Graph convolutional networks with argument-aware
    pooling for event detection,” in Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] T. Zhang, H. Ji, and A. Sil, “Joint entity and event extraction with
    generative adversarial imitation learning,” Data Intelligence, vol. 1, no. 2,
    pp. 99–120, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] W. Zhao, J. Zhang, J. Yang, T. He, H. Ma, and Z. Li, “A novel joint biomedical
    event extraction framework via two-level modeling of documents,” Information Sciences,
    vol. 550, pp. 27–40, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] T. Nasukawa and J. Yi, “Sentiment analysis: Capturing favorability using
    natural language processing,” in Proceedings of the 2nd International Conference
    on Knowledge Capture, pp. 70–77, ACM, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] K. Dave, S. Lawrence, and D. M. Pennock, “Mining the peanut gallery:
    Opinion extraction and semantic classification of product reviews,” in Proceedings
    of the 12th international conference on World Wide Web, pp. 519–528, ACM, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] A. R. Pathak, B. Agarwal, M. Pandey, and S. Rautaray, “Application of
    deep learning approaches for sentiment analysis,” in Deep Learning-Based Approaches
    for Sentiment Analysis, pp. 1–31, Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] A. Yadav and D. K. Vishwakarma, “Sentiment analysis using deep learning
    architectures: a review,” Artificial Intelligence Review, vol. 53, no. 6, pp. 4335–4385,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] D. Tang, B. Qin, and T. Liu, “Document modeling with gated recurrent
    neural network for sentiment classification,” in Proceedings of the 2015 conference
    on empirical methods in natural language processing, pp. 1422–1432, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] X. Glorot, A. Bordes, and Y. Bengio, “Domain adaptation for large-scale
    sentiment classification: A deep learning approach,” in Proceedings of the 28th
    international conference on machine learning (ICML-11), pp. 513–520, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] G. Rao, W. Huang, Z. Feng, and Q. Cong, “Lstm with sentence representations
    for document-level sentiment classification,” Neurocomputing, vol. 308, pp. 49–57,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] M. Rhanoui, M. Mikram, S. Yousfi, and S. Barzali, “A cnn-bilstm model
    for document-level sentiment analysis,” Machine Learning and Knowledge Extraction,
    vol. 1, no. 3, pp. 832–847, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning, “Semi-supervised
    recursive autoencoders for predicting sentiment distributions,” in Proceedings
    of the conference on empirical methods in natural language processing, pp. 151–161,
    Association for Computational Linguistics, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] X. Wang, Y. Liu, S. Chengjie, B. Wang, and X. Wang, “Predicting polarities
    of tweets by composing word embeddings with long short-term memory,” in Proceedings
    of the 53rd Annual Meeting of the Association for Computational Linguistics and
    the 7th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers), vol. 1, pp. 1343–1353, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and
    C. Potts, “Recursive deep models for semantic compositionality over a sentiment
    treebank,” in Proceedings of the 2013 conference on empirical methods in natural
    language processing, pp. 1631–1642, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] R. Arulmurugan, K. Sabarmathi, and H. Anandakumar, “Classification of
    sentence level sentiment analysis using cloud machine learning techniques,” Cluster
    Computing, vol. 22, no. 1, pp. 1199–1209, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] D. Meškelė and F. Frasincar, “Aldonar: A hybrid solution for sentence-level
    aspect-based sentiment analysis using a lexicalized domain ontology and a regularized
    neural attention model,” Information Processing & Management, vol. 57, no. 3,
    p. 102211, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Y. Wang, M. Huang, L. Zhao, et al., “Attention-based LSTM for aspect-level
    sentiment classification,” in Proceedings of the 2016 Conference on Empirical
    Methods in Natural Language Processing, pp. 606–615, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Y. Ma, H. Peng, T. Khan, E. Cambria, and A. Hussain, “Sentic lstm: a
    hybrid network for targeted aspect-based sentiment analysis,” Cognitive Computation,
    vol. 10, no. 4, pp. 639–650, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] H. Xu, B. Liu, L. Shu, and P. S. Yu, “BERT post-training for review reading
    comprehension and aspect-based sentiment analysis,” arXiv preprint arXiv:1904.02232,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] H. Xu, B. Liu, L. Shu, and P. S. Yu, “Double embeddings and CNN-based
    sequence labeling for aspect extraction,” arXiv preprint arXiv:1805.04601, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] H. H. Do, P. Prasad, A. Maag, and A. Alsadoon, “Deep learning for aspect-based
    sentiment analysis: a comparative review,” Expert Systems with Applications, vol. 118,
    pp. 272–299, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] S. Rida-E-Fatima, A. Javed, A. Banjar, A. Irtaza, H. Dawood, H. Dawood,
    and A. Alamri, “A multi-layer dual attention deep learning model with refined
    word embeddings for aspect-based sentiment analysis,” IEEE Access, vol. 7, pp. 114795–114807,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Y. Liang, F. Meng, J. Zhang, J. Xu, Y. Chen, and J. Zhou, “A novel aspect-guided
    deep transition model for aspect based sentiment analysis,” arXiv preprint arXiv:1909.00324,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] D. Jurafsky and J. H. Martin, Speech and Language Processing. Prentice
    Hall, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] N. Kalchbrenner and P. Blunsom, “Recurrent continuous translation models,”
    in Proceedings of the 2013 Conference on Empirical Methods in Natural Language
    Processing, pp. 1700–1709, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] S. P. Singh, A. Kumar, H. Darbari, L. Singh, A. Rastogi, and S. Jain,
    “Machine translation using deep learning: An overview,” in 2017 international
    conference on computer, communications and electronics (comptelix), pp. 162–167,
    IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] S. Yang, Y. Wang, and X. Chu, “A survey of deep learning techniques for
    neural machine translation,” arXiv preprint arXiv:2002.07526, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] L. E. Dostert, “The Georgetown-IBM experiment,” 1955). Machine translation
    of languages. John Wiley & Sons, New York, pp. 124–135, 1955.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] K. Cho, B. Van Merriënboer, D. Bahdanau, and Y. Bengio, “On the properties
    of neural machine translation: Encoder-decoder approaches,” arXiv preprint arXiv:1409.1259,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in Advances in neural information processing systems, pp. 3104–3112,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun,
    Y. Cao, Q. Gao, K. Macherey, et al., “Google’s neural machine translation system:
    Bridging the gap between human and machine translation,” arXiv preprint arXiv:1609.08144,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, “Convolutional
    sequence to sequence learning,” arXiv preprint arXiv:1705.03122, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in Neural
    Information Processing Systems, pp. 5998–6008, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] K. Ahmed, N. S. Keskar, and R. Socher, “Weighted transformer network
    for machine translation,” arXiv preprint arXiv:1711.02132, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative
    position representations,” arXiv preprint arXiv:1803.02155, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] S. Edunov, M. Ott, M. Auli, and D. Grangier, “Understanding back-translation
    at scale,” arXiv preprint arXiv:1808.09381, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] R. Aharoni, M. Johnson, and O. Firat, “Massively multilingual neural
    machine translation,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] J. Zhu, Y. Xia, L. Wu, D. He, T. Qin, W. Zhou, H. Li, and T.-Y. Liu,
    “Incorporating bert into neural machine translation,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Y. Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad, M. Lewis,
    and L. Zettlemoyer, “Multilingual denoising pre-training for neural machine translation,”
    Transactions of the Association for Computational Linguistics, vol. 8, pp. 726–742,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Y. Cheng, L. Jiang, and W. Macherey, “Robust neural machine translation
    with doubly adversarial inputs,” arXiv preprint arXiv:1906.02443, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] W. Zhang, Y. Feng, F. Meng, D. You, and Q. Liu, “Bridging the gap between
    training and inference for neural machine translation,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] J. Yang, M. Wang, H. Zhou, C. Zhao, W. Zhang, Y. Yu, and L. Li, “Towards
    making the most of bert in neural machine translation,” in Proceedings of the
    AAAI Conference on Artificial Intelligence, vol. 34, pp. 9378–9385, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] A. Bordes, S. Chopra, and J. Weston, “Question answering with subgraph
    embeddings,” arXiv preprint arXiv:1406.3676, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] B. F. Green Jr, A. K. Wolf, C. Chomsky, and K. Laughery, “Baseball: an
    automatic question-answerer,” in Papers presented at the May 9-11, 1961, Western
    Joint IRE-AIEE-ACM Computer Conference, pp. 219–224, ACM, 1961.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] A. Ittycheriah, M. Franz, W.-J. Zhu, A. Ratnaparkhi, and R. J. Mammone,
    “IBM’s statistical question answering system.,” in TREC, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] H. Cui, R. Sun, K. Li, M.-Y. Kan, and T.-S. Chua, “Question answering
    passage retrieval using dependency relations,” in Proceedings of the 28th annual
    international ACM SIGIR conference on Research and development in information
    retrieval, pp. 400–407, ACM, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] X. Qiu and X. Huang, “Convolutional neural tensor network architecture
    for community-based question answering.,” in IJCAI, pp. 1305–1311, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] H. T. Ng, L. H. Teo, and J. L. P. Kwan, “A machine learning approach
    to answering questions for reading comprehension tests,” in Proceedings of the
    2000 Joint SIGDAT conference on Empirical methods in natural language processing
    and very large corpora: held in conjunction with the 38th Annual Meeting of the
    Association for Computational Linguistics-Volume 13, pp. 124–132, Association
    for Computational Linguistics, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] C. Xiong, V. Zhong, and R. Socher, “Dynamic coattention networks for
    question answering,” arXiv preprint arXiv:1611.01604, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick,
    and D. Parikh, “VQA: Visual question answering,” in Proceedings of the IEEE international
    conference on computer vision, pp. 2425–2433, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] M. Malinowski, M. Rohrbach, and M. Fritz, “Ask your neurons: A neural-based
    approach to answering questions about images,” in Proceedings of the IEEE international
    conference on computer vision, pp. 1–9, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] H. Xu and K. Saenko, “Ask, attend and answer: Exploring question-guided
    spatial attention for visual question answering,” in European Conference on Computer
    Vision, pp. 451–466, Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human attention
    in visual question answering: Do humans and deep networks look at the same regions?,”
    Computer Vision and Image Understanding, vol. 163, pp. 90–100, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] H. Ben-Younes, R. Cadene, N. Thome, and M. Cord, “Block: Bilinear superdiagonal
    fusion for visual question answering and visual relationship detection,” in Proceedings
    of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 8102–8109, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] J. Wu and R. J. Mooney, “Self-critical reasoning for robust visual question
    answering,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] R. Nallapati, F. Zhai, and B. Zhou, “SummaRuNNer: A recurrent neural
    network based sequence model for extractive summarization of documents.,” in AAAI,
    pp. 3075–3081, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] S. Narayan, S. B. Cohen, and M. Lapata, “Ranking sentences for extractive
    summarization with reinforcement learning,” in NAACL:HLT, vol. 1, pp. 1747–1759,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for abstractive
    sentence summarization,” in EMNLP, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] J. Tan, X. Wan, and J. Xiao, “Abstractive document summarization with
    a graph-based attentional neural model,” in ACL, vol. 1, pp. 1171–1181, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] R. Nallapati, B. Zhou, C. dos Santos, C. Gulcehre, and B. Xiang, “Abstractive
    text summarization using sequence-to-sequence RNNs and beyond,” in Proceedings
    of The 20th SIGNLL Conference on Computational Natural Language Learning, pp. 280–290,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman,
    and P. Blunsom, “Teaching machines to read and comprehend,” in NIPS, pp. 1693–1701,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] J. Gu, Z. Lu, H. Li, and V. O. Li, “Incorporating copying mechanism in
    sequence-to-sequence learning,” in ACL, vol. 1, pp. 1631–1640, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Y.-C. Chen and M. Bansal, “Fast abstractive summarization with reinforce-selected
    sentence rewriting,” in ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Q. Zhou, N. Yang, F. Wei, S. Huang, M. Zhou, and T. Zhao, “Neural document
    summarization by jointly learning to score and select sentences,” in ACL, pp. 654–663,
    ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] T. Shi, Y. Keneshloo, N. Ramakrishnan, and C. K. Reddy, “Neural abstractive
    text summarization with sequence-to-sequence models,” arXiv preprint arXiv:1812.02303,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] C. Ma, W. E. Zhang, M. Guo, H. Wang, and Q. Z. Sheng, “Multi-document
    summarization via deep learning techniques: A survey,” arXiv preprint arXiv:2011.04843,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] A. Abdi, S. Hasan, S. M. Shamsuddin, N. Idris, and J. Piran, “A hybrid
    deep learning architecture for opinion-oriented multi-document summarization based
    on multi-feature fusion,” Knowledge-Based Systems, vol. 213, p. 106658, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] X. Zhang, F. Wei, and M. Zhou, “Hibert: Document level pre-training of
    hierarchical bidirectional transformers for document summarization,” arXiv preprint
    arXiv:1905.06566, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] E. Merdivan, D. Singh, S. Hanke, and A. Holzinger, “Dialogue systems
    for intelligent human computer interactions,” Electronic Notes in Theoretical
    Computer Science, vol. 343, pp. 57–71, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] D. Hakkani-Tür, G. Tür, A. Celikyilmaz, Y.-N. Chen, J. Gao, L. Deng,
    and Y.-Y. Wang, “Multi-domain joint semantic frame parsing using bi-directional
    RNN-LSTM,” in Interspeech, pp. 715–719, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] C. Toxtli, J. Cranshaw, et al., “Understanding chatbot-mediated task
    management,” in Proceedings of the 2018 CHI Conference on Human Factors in Computing
    Systems, p. 58, ACM, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] V. Ilievski, C. Musat, A. Hossmann, and M. Baeriswyl, “Goal-oriented
    chatbot dialog management bootstrapping with transfer learning,” arXiv preprint
    arXiv:1802.00500, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao, “Deep
    reinforcement learning for dialogue generation,” in Proceedings of the Conference
    on Empirical Methods in Natural Language Processing, pp. 1192–1202, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] T.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. M. Rojas-Barahona, P.-H.
    Su, S. Ultes, and S. Young, “A network-based end-to-end trainable task-oriented
    dialogue system,” arXiv preprint arXiv:1604.04562, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] J. D. Williams and G. Zweig, “End-to-end LSTM-based dialog control optimized
    with supervised and reinforcement learning,” arXiv preprint arXiv:1606.01269,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] S. Sukhbaatar, J. Weston, R. Fergus, et al., “End-to-end memory networks,”
    in Advances in neural information processing systems, pp. 2440–2448, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] A. Bordes, Y.-L. Boureau, and J. Weston, “Learning end-to-end goal-oriented
    dialog,” arXiv preprint arXiv:1605.07683, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] A. Ritter, C. Cherry, and W. B. Dolan, “Data-driven response generation
    in social media,” in Proceedings of the conference on empirical methods in natural
    language processing, pp. 583–593, Association for Computational Linguistics, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Z. Ji, Z. Lu, and H. Li, “An information retrieval approach to short
    text conversation,” arXiv preprint arXiv:1408.6988, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] B. Hu, Z. Lu, H. Li, and Q. Chen, “Convolutional neural network architectures
    for matching natural language sentences,” in Advances in neural information processing
    systems, pp. 2042–2050, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] R. Lowe, N. Pow, I. Serban, and J. Pineau, “The Ubuntu dialogue corpus:
    A large dataset for research in unstructured multi-turn dialogue systems,” arXiv
    preprint arXiv:1506.08909, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] R. Yan, Y. Song, and H. Wu, “Learning to respond with deep neural networks
    for retrieval-based human-computer conversation system,” in Proceedings of the
    39th International ACM SIGIR conference on Research and Development in Information
    Retrieval, pp. 55–64, ACM, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] X. Zhou, L. Li, D. Dong, Y. Liu, Y. Chen, W. X. Zhao, D. Yu, and H. Wu,
    “Multi-turn response selection for chatbots with deep attention matching network,”
    in Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), vol. 1, pp. 1118–1127, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio,
    “Generating sentences from a continuous space,” arXiv preprint arXiv:1511.06349,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] A. Kannan and O. Vinyals, “Adversarial evaluation of dialogue models,”
    arXiv preprint arXiv:1701.08198, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] O. Vinyals and Q. Le, “A neural conversational model,” arXiv preprint
    arXiv:1506.05869, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
