- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:07:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1808.01462] A survey on Deep Learning Advances on Different 3D Data Representations'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1808.01462] 关于不同3D数据表示的深度学习进展综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1808.01462](https://ar5iv.labs.arxiv.org/html/1808.01462)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1808.01462](https://ar5iv.labs.arxiv.org/html/1808.01462)
- en: A survey on Deep Learning Advances on Different 3D Data Representations
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于不同3D数据表示的深度学习进展综述
- en: Eman Ahmed [1234-5678-9012-3456](https://orcid.org/1234-5678-9012-3456 "ORCID
    identifier") SnT, University of Luxembourg [eman.ahmed@uni.lu](mailto:eman.ahmed@uni.lu)
    ,  Alexandre Saint SnT, University of Luxembourg [alexandre.saint@uni.lu](mailto:alexandre.saint@uni.lu)
    ,  Abdelrahman Shabayek SnT, University of Luxembourg [abdelrahman.shabayek@uni.lu](mailto:abdelrahman.shabayek@uni.lu)
    ,  Kseniya Cherenkova SnT, University of Luxembourg; Artec group, Luxembourg [kcherenkova@artec-group.com](mailto:kcherenkova@artec-group.com)
    ,  Rig Das SnT, University of Luxembourg [rig.das@uni.lu](mailto:rig.das@uni.lu)
    ,  Gleb Gusev Artec groupLuxembourg [gleb@artec-group.com](mailto:gleb@artec-group.com)
     and  Djamila Aouada SnT, University of Luxembourg [djamila.aouada@uni.lu](mailto:djamila.aouada@uni.lu)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Eman Ahmed [1234-5678-9012-3456](https://orcid.org/1234-5678-9012-3456 "ORCID
    identifier") SnT，卢森堡大学 [eman.ahmed@uni.lu](mailto:eman.ahmed@uni.lu) ，Alexandre
    Saint SnT，卢森堡大学 [alexandre.saint@uni.lu](mailto:alexandre.saint@uni.lu) ，Abdelrahman
    Shabayek SnT，卢森堡大学 [abdelrahman.shabayek@uni.lu](mailto:abdelrahman.shabayek@uni.lu)
    ，Kseniya Cherenkova SnT，卢森堡大学；Artec 组，卢森堡 [kcherenkova@artec-group.com](mailto:kcherenkova@artec-group.com)
    ，Rig Das SnT，卢森堡大学 [rig.das@uni.lu](mailto:rig.das@uni.lu) ，Gleb Gusev Artec 组卢森堡
    [gleb@artec-group.com](mailto:gleb@artec-group.com) 和 Djamila Aouada SnT，卢森堡大学
    [djamila.aouada@uni.lu](mailto:djamila.aouada@uni.lu)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Abstract: 3D data is a valuable asset the computer vision filed as it provides
    rich information about the full geometry of sensed objects and scenes. Recently,
    with the availability of both large 3D datasets and computational power, it is
    today possible to consider applying deep learning to learn specific tasks on 3D
    data such as segmentation, recognition and correspondence. Depending on the considered
    3D data representation, different challenges may be foreseen in using existent
    deep learning architectures. In this work, we provide a comprehensive overview
    about various 3D data representations highlighting the difference between Euclidean
    and non-Euclidean ones. We also discuss how Deep Learning methods are applied
    on each representation, analyzing the challenges to overcome.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要：3D数据是计算机视觉领域的宝贵资产，因为它提供了关于被感知物体和场景完整几何信息的丰富数据。近年来，随着大规模3D数据集和计算能力的出现，现如今可以考虑将深度学习应用于3D数据的特定任务，如分割、识别和对应。根据考虑的3D数据表示，使用现有深度学习架构可能会面临不同的挑战。在这项工作中，我们提供了关于各种3D数据表示的全面概述，突出展示了欧几里得和非欧几里得表示之间的区别。我们还讨论了深度学习方法如何应用于每种表示，并分析了需要克服的挑战。
- en: \useunder
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \useunder
- en: \ul
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \ul
- en: 0.0.1\. Deep learning architectures on multi-view data
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 0.0.1\. 多视角数据上的深度学习架构
- en: Despite the effectiveness of volumetric deep learning methods, most of these
    approaches are computationally expensive because of the volumetric nature of the
    convolutional filters to extract the features which increase the computational
    complexity cubically with respect to the voxels resolution which limits the usage
    of 3D volumetric DL models. That is why exploiting multi-views of 3D objects is
    practical. Indeed, it enables exploiting the already established 2D DL paradigms
    without the need to design a tailored model for 3D volumetric data with high computational
    complexity as shown in pipeline illustrated in Fig. LABEL:fig:taxonomy. One of
    the first attempts to exploit 2D DL models for learning multi-view 3D data was
    presented by Leng et at. in (Leng et al., [2014](#bib.bib82)) where DBN was employed
    on various view-based depth images to extract high-level features of the 3D object.
    A later-wise training manner was used to train the DBN using the “Contractive
    Divergence” method. The proposed model produced better results than the composite
    descriptors approach employed in (Daras and Axenopoulos, [2010](#bib.bib36)).
    Xie et al. (Xie et al., [2015](#bib.bib146)) proposed “Multi-View Deep Extreme
    Learning Machine (MVD-ELM)”. The proposed MVD-ELM was employed on 20 multi-view
    depth images that were uniformly captured with a sphere at the centre of the 3D
    object. The proposed MVD-ELM contained Conv layers that had shared weights across
    all the views. The output activation weights were optimized according to the feature
    maps extracted. This work has been extended to be Fully Convolutional, resulting
    in (FC-MVD-ELM). FC-MVD-ELM was trained using the multi-view depth images to be
    tested for 3D segmentation. The predicted labels from the training stage were
    then projected back to the 3D object where the final result was smoothed using
    the graph cut optimization method. Both MVD-ELM and FC-MVD-ELM were tested on
    3D shape classification and segmentation tasks and outperformed the previous work (Wu
    et al., [2015a](#bib.bib143)) and reduced the processing time significantly.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管体积深度学习方法有效，但大多数这些方法由于卷积滤波器的体积特性而计算开销巨大，这种特性使得计算复杂度随着体素分辨率立方增长，从而限制了3D体积深度学习模型的使用。这就是为什么利用3D对象的多视角是实际可行的。事实上，它使得利用已建立的2D深度学习范式成为可能，而无需设计针对3D体积数据的高计算复杂度的专门模型，如在图示的流程图中所示（见
    Fig. LABEL:fig:taxonomy）。Leng 等人首次尝试利用2D深度学习模型学习多视角3D数据（见 Leng et al., [2014](#bib.bib82)），他们在各种基于视角的深度图像上应用了
    DBN，以提取3D对象的高层特征。采用了渐进式训练方式来训练 DBN，使用了“收缩散度”方法。所提出的模型比在 Daras 和 Axenopoulos（见
    [2010](#bib.bib36)）中使用的复合描述符方法表现更好。Xie 等人（见 Xie et al., [2015](#bib.bib146)）提出了“多视角深度极限学习机（MVD-ELM）”。所提出的
    MVD-ELM 应用于20张均匀捕获的多视角深度图像，这些图像围绕3D对象的中心球体捕获。所提出的 MVD-ELM 包含在所有视角间共享权重的卷积层。输出激活权重根据提取的特征图进行优化。这项工作已经扩展为完全卷积的形式，
    resulting in（FC-MVD-ELM）。FC-MVD-ELM 使用多视角深度图像进行训练，以进行3D分割测试。训练阶段的预测标签随后被投影回3D对象，并使用图割优化方法对最终结果进行平滑处理。MVD-ELM
    和 FC-MVD-ELM 都在3D形状分类和分割任务上进行了测试，并超越了先前的工作（见 Wu et al., [2015a](#bib.bib143)），显著减少了处理时间。
- en: 'More research investigations were carried by Leng et al. to employ DL paradigms
    on multi-view 3D data. Leng et al. in (Leng et al., [2015a](#bib.bib80)) proposed
    an extension of classical AEs in a similar way to the CNN architecture. Their
    proposed framework is called “Stacked Local Convolutional AutoEncoders (SLCAE)”.
    SLCAE operated on multiple multi-view depth images of the 3D object. In SLCAE,
    FC layers were substituted by layers that were connected locally with the use
    of the convolution operation. Multiple AEs ere stacked where the output of the
    last AE was used as a final representation of the 3D object. Experiments on different
    datasets: SHREC’09, NTU and PSB proved the capabilities of this model. As an extension
    to the previous work, Leng et al. proposed a “3D Convolutional Neural Network
    (3DCNN)” to simultaneously process different 2D views of the 3D object (Leng et al.,
    [2015b](#bib.bib81)). Different views are sorted in a specific order to guarantee
    that all the objects’ views follow the same convention while training. The proposed
    3DCNN is composed of four Conv layers, three sub-sampling layers and two FC layers.
    The proposed network was tested for retrieval task on the same datasets used for
    testing (Leng et al., [2015a](#bib.bib80)). However, the results showed that the
    later model performed better on the three datasets which implies that the previous
    model was able to learn more discriminative features to represent various 3D objects.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Leng 等人进行了更多的研究，利用深度学习（DL）范式处理多视角 3D 数据。在 (Leng et al., [2015a](#bib.bib80))
    中，Leng 等人提出了一种类似于卷积神经网络（CNN）架构的经典自编码器（AEs）扩展。他们提出的框架称为“堆叠局部卷积自编码器（SLCAE）”。SLCAE
    在 3D 物体的多个多视角深度图像上运行。在 SLCAE 中，全连接层被通过卷积操作局部连接的层所替代。多个自编码器被堆叠，其中最后一个自编码器的输出作为
    3D 物体的最终表示。在不同数据集上的实验：SHREC’09、NTU 和 PSB 证明了该模型的能力。作为对先前工作的扩展，Leng 等人提出了一种“3D
    卷积神经网络（3DCNN）”来同时处理 3D 物体的不同 2D 视图 (Leng et al., [2015b](#bib.bib81))。不同视图按特定顺序排序，以确保所有物体的视图在训练时遵循相同的规范。提出的
    3DCNN 由四个卷积层、三个子采样层和两个全连接层组成。提出的网络在与测试相同的数据集上进行了检索任务的测试 (Leng et al., [2015a](#bib.bib80))。然而，结果显示，后者模型在三个数据集上表现更好，这意味着先前的模型能够学习到更多区分特征来表示不同的
    3D 物体。
- en: A novel “Multi-View CNN (MVCNN)” was proposed by Su et al. in (Su et al., [2015](#bib.bib125))
    for 3D object retrieval and recognition/classification tasks. In contrast to Leng’s
    model in (Leng et al., [2015b](#bib.bib81)), MVCNN processed multiple views for
    the 3D objects in no specific order using a view pooling layer. [1](#S0.F1 "Figure
    1 ‣ 0.0.1\. Deep learning architectures on multi-view data ‣ A survey on Deep
    Learning Advances on Different 3D Data Representations") shows the full architecture
    of the model. Two different setups to capture the 3D objects multi-views were
    tested. The first one rendered 12 views for the object by placing 12 equidistant
    virtual cameras surrounding the object while the other setup included 80 virtual
    views. MVCNN was pre-trained using $ImageNet1K$ dataset and fine-tuned on $ModelNet40$ (Wu
    et al., [2015a](#bib.bib143)). The proposed network has two parts, the first part
    is where the object’s views are processed separately and the second part is where
    the max pooling operation is taking place across all the processed views in the
    view-pooling layer, resulting in a single compact representation for the whole
    3D shape. In the view-pooling layer, the view with the maximal activation is the
    only one considered while ignoring all the other views with non-maximal activations.
    This means that only few views are contributing towards the final representation
    of the shape which causes a loss of the visual information. To overcome this problem,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Su 等人在 (Su et al., [2015](#bib.bib125)) 中提出了一种新颖的“多视角卷积神经网络（MVCNN）”用于 3D 物体的检索和识别/分类任务。与
    Leng 的模型 (Leng et al., [2015b](#bib.bib81)) 相比，MVCNN 处理 3D 物体的多个视角不按特定顺序，使用视图池化层。[1](#S0.F1
    "Figure 1 ‣ 0.0.1\. Deep learning architectures on multi-view data ‣ A survey
    on Deep Learning Advances on Different 3D Data Representations") 展示了模型的完整架构。测试了捕捉
    3D 物体多视角的两种不同设置。第一种通过将 12 个等距虚拟摄像头环绕物体来渲染 12 个视图，而另一种设置包括 80 个虚拟视图。MVCNN 使用 $ImageNet1K$
    数据集进行预训练，并在 $ModelNet40$ 上进行了微调 (Wu et al., [2015a](#bib.bib143))。提出的网络分为两部分，第一部分是处理物体视图，第二部分是视图池化层中进行的最大池化操作，结果是
    3D 形状的单一紧凑表示。在视图池化层中，只有激活最大的视图被考虑，而忽略所有其他激活不最大化的视图。这意味着只有少数视图对形状的最终表示有贡献，这导致了视觉信息的丢失。为了解决这个问题，
- en: Experiments showed that the MVCNN with the max-view pooling layer outperformed
    ShapeNet (Wu et al., [2015a](#bib.bib143)) on classification and retrieval tasks
    by a remarkable margin. In (Johns et al., [2016](#bib.bib68)) Johns et al. exploited
    multi-view data representation using CNNs by representing 3D objects under unconstrained
    camera trajectories with a set of 2D image pairs. The proposed method classifies
    each pair separately and then weight the contribution of each pair to get the
    final result. The VGG-M architecture was adopted in this framework consists of
    five Conv layers and three FC layers. The views of the 3D objects are represented
    as either depth images or grayscale images or both. This model outperformed MVCNN
    proposed by Su et al. (Su et al., [2015](#bib.bib125)) and voxel-based ShapeNet
    architectures (Wu et al., [2015a](#bib.bib143)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，带有最大视图池化层的MVCNN在分类和检索任务中显著超越了ShapeNet (Wu et al., [2015a](#bib.bib143))。在(Johns
    et al., [2016](#bib.bib68))中，Johns等人利用CNNs通过一组2D图像对表示在不受约束的相机轨迹下的3D对象，从而利用多视图数据表示。他们提出的方法对每对图像分别进行分类，然后加权每对图像的贡献以得到最终结果。该框架采用了VGG-M架构，由五个卷积层和三个全连接层组成。3D对象的视图可以表示为深度图像、灰度图像或两者兼有。该模型超越了Su
    et al. (Su et al., [2015](#bib.bib125)) 提出的MVCNN和基于体素的ShapeNet架构 (Wu et al., [2015a](#bib.bib143))。
- en: 'The efficiency of multi-view DL models pushed researchers to investigate more
    GPU-based methods to learn multi-view 3D data features. This is what pushed Bai
    et al. (Bai et al., [2016](#bib.bib10)) to propose a real-time GPU-based CNN search
    engine for multi 2D-views of 3D objects. The proposed model called $GIFT$ utilizes
    two files that are inverted: the first is to accelerate process of the multi-view
    matching and the second one is to rank the initial results. The processed query
    is completed within one second. $GIFT$ was tested on a set of various datasets:
    ModelNet, $PSB$, $SHREC14LSGTB$, $McGill$ and $SHREC^{\prime}07$ watertight models.
    GIFT produced a better performance compared to the state-of-the-art methods.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多视图深度学习模型的高效性推动了研究人员探索更多基于GPU的方法来学习多视图3D数据特征。这也促使Bai et al. (Bai et al., [2016](#bib.bib10))提出了一种实时基于GPU的CNN搜索引擎，用于3D对象的多2D视图。该模型称为$GIFT$，利用了两个反向的文件：第一个加速多视图匹配过程，第二个对初步结果进行排序。处理的查询在一秒内完成。$GIFT$在一组不同的数据集上进行了测试：ModelNet,
    $PSB$, $SHREC14LSGTB$, $McGill$和$SHREC^{\prime}07$密闭模型。与最先进的方法相比，GIFT表现更佳。
- en: The efforts to learn multi-view 3D data representations kept evolving and in (Zanuttigh
    and Minto, [2017](#bib.bib155)), Zanuttigh and Minto proposed a multi-branch CNN
    for classifying 3D objects. The input to this model is rendered depth maps from
    different view points for the 3D object. Each CNN branch consists of five Conv
    layers to process one depth map producing a classification vector. The resulted
    classification vectors are the input to a linear classifier to identify the 3D
    object’s category/class. The proposed model produced comparable results to the
    state-of-the-art. Based on the dominant sets, Want et al. in (Wang et al., [2017b](#bib.bib135))
    proposed recurrent view-clustering and pooling layers . The key concept in this
    model is to pool similar views and recurrently cluster them to build a pooled
    feature vector. Then, the constructed pooled feature vectors are fed as inputs
    in the same layer in a recurrent training fashion in the recurrent clustering
    layer. Within this layer, a view similarity graph is computed whose nodes represent
    the feature vectors and the edges represent the similarity weights between the
    views. Within the constructed graph, the similarities and dissimilarities between
    different views are exhibited which is very effective in the 3D shape recognition
    task. The proposed model achieved a highly comparable results to previous methods
     (Wu et al., [2015a](#bib.bib143); Su et al., [2015](#bib.bib125)) as shown in
    Table 1 in the supplementary material. Driven the advances in the multi-view DL
    models, Qi et al. (Qi et al., [2016b](#bib.bib101)) provided a comparison study
    between multi-view DL techniques and volumetric DL techniques for the object recognition
    task. As part of the study, the authors proposed a $Sphererendering$ approach
    for filtering multi-resolution 3D objects at multiple scales. With data augmentation,
    the authors managed to enhance the results of MVCNNs on $ModelNet40$. Recently,
    Kanezaki et at. (Kanezaki et al., [2016](#bib.bib70)) achieved state-of-the-art
    results on both $ModelNet10$ and $ModelNet40$ in the classification problem using
    $RotationNet$. $RotationNet$ trains a set of multi-view images for the 3D object
    but doesn’t require all the views at once. Instead, it allows for sequential input
    and updates the likelihood of the object’s category accordingly.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 学习多视角三维数据表示的努力不断发展，2017年，Zanuttigh 和 Minto提出了一种用于分类三维物体的多分支CNN。该模型的输入是来自不同视角的渲染深度图像。每个CNN分支包括五个卷积层来处理一个深度图像，生成一个分类向量。产生的分类向量是线性分类器的输入，用于识别三维物体的类别。该提出的模型产生了与最先进技术相媲美的结果。基于主导集合，Wang等人在2017年提出了递归视图聚类和汇聚层。该模型的关键概念是汇集相似视角并通过递归地对其进行聚类来构建一个汇集特征向量。然后，构建的汇集特征向量以递归训练的方式作为输入传入递归聚类层中的同一层。在该层内，计算视图相似性图，其中节点表示特征向量，边表示视图之间的相似度权重。在构建的图中展示了不同视图之间的相似性和不同性，这对于三维形状识别任务非常有效。所提出的模型在表现上达到了与以往方法高度相当的结果，如附录中的表1所示。在多视角深度学习模型的推动下，Qi等人在2016年提供了多视角深度学习技术和体积式深度学习技术的比较研究，用于目标识别任务。作为研究的一部分，作者提出了一个$Sphererendering$方法，用于在多个尺度上过滤多分辨率三维物体。通过数据增强，作者成功地提升了MVCNNs在$ModelNet40$上的结果。最近，Kanezaki等人利用$RotationNet$在$ModelNet10$和$ModelNet40$上实现了最先进的分类结果。$RotationNet$训练了一组多视角图像用于三维物体，但不需要同时获取所有视图。相反，它允许顺序输入并相应地更新物体类别的可能性。
- en: Multi-view representation proved to perform slightly better than volumetric
    representation with less computational power needed. However, there are some challenges
    imposed with this representation. The sufficient number of views and the way they
    were acquired is a critical factor for representing the 3D shape. Also, the multi-view
    representation does not preserve the intrinsic geometric properties of the 3D
    shape. This is what pushed towards defining new notion of convolution operating
    on 3D shapes to capture their intrinsic properties.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角表示法表现比体积表示法稍好，且需要较少的计算资源。然而，这种表示法也带来了一些挑战。视角的充分数量及其获取方式对于表达三维形状至关重要。此外，多视角表示法无法保留三维形状的固有几何属性。这促使人们定义新的卷积概念，以捕捉其固有属性。
- en: '![Refer to caption](img/e842147a9abe819972a1e580b92fa755.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e842147a9abe819972a1e580b92fa755.png)'
- en: Figure 1\. MVCNN architecture (Su et al., [2015](#bib.bib125)) applied on multi-view
    of 3D objects without a specific order. The figure reused from (Su et al., [2015](#bib.bib125))
    with permission of authors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. MVCNN架构（Su 等，[2015](#bib.bib125)）应用于无特定顺序的三维对象多视角。图源自（Su 等，[2015](#bib.bib125)），已获得作者许可。
- en: 0.0.2\. Deep learning architectures on hybrid data representations
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 0.0.2\. 深度学习架构在混合数据表示上的应用
- en: 'Some efforts towards combining various 3D data representations to exploit the
    advantages that each representation brings. Recently, Wang et al. proposed  (Wang
    et al., [2015](#bib.bib137)), where each of the 3D objects are represented by
    a pair of multi-views and 2D sketches. The learning model is composed of Siamese
    CNNs where are two identical sub-convolutional networks; one for processing the
    multi-views input and the other one is for processing the 2D sketches. Each of
    these network consisted of three Conv layers where each of them was succeeded
    by a max-pooling layer and a Fully Connected Layer. The networks were trained
    separately using the “Stochastic Gradient Descent (SGD)” method. The proposed
    model was tested on SHREC’13 dataset for 3D shape retrieval and achieved competitive
    results compared to previous methods. Wang et al. continued their investigations
    for hybrid 3D data representations and in (Wang et al., [2016](#bib.bib139)),
    the authors proposed the “Convolutional Auto-Encoder Extreme Learning Machine
    (CAE-ELM)” 3D descriptor which merges the learning power of ConvNets, AEs and
    “Extreme Learning Machine (ELM)” (Huang et al., [2006](#bib.bib65)). ELM is an
    efficient unsupervised learning technique which learns high-level discriminative
    features about the input data. ELM is faster than most of DL models (Kasun et al.,
    [2013](#bib.bib71)) which is practical for processing large-scale 3D data. The
    input to the CAE-ELM architecture is two data representations: voxel data and
    “Signed Distance Field data (SDF)”. Voxel data explains the structure of the 3D
    object while the SDF extracts local and global features about the 3D object. CAE-ELM
    was tested on $ModelNet40$ and $ModelNet10$ datasets for classification tasks
    and achieved a superior performance compared to previous methods. CAE-ELM is considered
    as a hybrid approach that exploits the structure of the 3D objects in addition
    to 3D descriptors. Ben-Shabat et al. (Ben-Shabat et al., [2017](#bib.bib12)) proposed
    a novel 3D point cloud representation named “3D Modified Fisher Vectors (3DmFV)”
    which is a kind of DL model that uses a hybrid data representation of the discrete
    structure of a grid with the continuous generalization of Fisher vectors to represent
    the 3D data. The hybrid input data is processed using deep ConvNet for classification
    and part segmentation tasks. 3DmFV has two modules: the initial one changes the
    input point cloud into a 3D modified “Fisher vector (FV)” which can be considered
    as a descriptor-based representation and the second one is the DL module represented
    in the CNN. FV data representation empowers the proposed framework to become order,
    input data sample size and structure invariant. The network architecture is composed
    of an inception module (Szegedy et al., [2017](#bib.bib129)), max-pooling layers
    and four FC layers on top. this network has achieved better results in comparison
    with state-of-the-art techniques.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一些努力旨在结合各种3D数据表示，以利用每种表示所带来的优势。最近，王等人提出了（王等人，[2015](#bib.bib137)），其中每个3D对象由一对多视图和2D草图表示。该学习模型由Siamese
    CNNs组成，这是一种有两个相同的子卷积网络；一个用于处理多视图输入，另一个用于处理2D草图。这些网络中的每一个都由三个卷积层组成，每个卷积层后接一个最大池化层和一个全连接层。网络使用“随机梯度下降（SGD）”方法单独训练。所提出的模型在SHREC'13数据集上进行了3D形状检索测试，并与之前的方法相比取得了有竞争力的结果。王等人继续对混合3D数据表示进行研究，在（王等人，[2016](#bib.bib139)）中，作者提出了“卷积自编码极限学习机（CAE-ELM）”3D描述符，该描述符融合了卷积网络、自动编码器（AEs）和“极限学习机（ELM）”的学习能力（黄等人，[2006](#bib.bib65)）。ELM是一种高效的无监督学习技术，它学习关于输入数据的高级区分特征。与大多数深度学习模型相比，ELM速度更快（卡松等人，[2013](#bib.bib71)），这在处理大规模3D数据时非常实用。CAE-ELM架构的输入是两种数据表示：体素数据和“符号距离场数据（SDF）”。体素数据描述了3D对象的结构，而SDF提取了有关3D对象的局部和全局特征。CAE-ELM在$ModelNet40$和$ModelNet10$数据集上进行了分类任务的测试，并与之前的方法相比取得了卓越的性能。CAE-ELM被视为一种混合方法，除了3D描述符外，还利用了3D对象的结构。Ben-Shabat等人（Ben-Shabat等人，[2017](#bib.bib12)）提出了一种名为“3D修改费舍尔向量（3DmFV）”的新型3D点云表示，这是一种使用混合数据表示的深度学习模型，结合了离散网格的结构与费舍尔向量的连续泛化来表示3D数据。混合输入数据通过深度卷积网络处理，用于分类和部分分割任务。3DmFV有两个模块：第一个模块将输入点云转换为3D修改的“费舍尔向量（FV）”，这可以视为基于描述符的表示，第二个模块是以CNN表示的深度学习模块。FV数据表示使得所提出的框架在顺序、输入数据样本大小和结构上具有不变性。网络架构由一个引导模块（Szegedy
    et al., [2017](#bib.bib129)）、最大池化层和四个全连接层组成。该网络与最先进的技术相比取得了更好的结果。
- en: Inspired by the performance of 3D convolution and 2D multi-view CNNs, some work
    examined the fusion of both representations. Hedge and Zadeh (Hegde and Zadeh,
    [2016](#bib.bib62)), proposed to fuse the volumetric representations (voxels)
    and 2D representation (2D views) for the object classification task. Authors have
    examined different models for processing the data for $ModelNet$ classification.
    The model one was a combination of both modalities (3D and 2D) while using two
    3D CNNs for processing the 3D voxels and AlexNet for processing the 2D views.
    The other experiments were carried by processing the 3D voxels only and compare
    it with the performance of AlexNet on the 2D views. Experiments showed that that
    network that combined both modalities called $FusionNet$ performed the best. However,
    the multi-view network performed better than the volumetric CNNs. Although Brock
    et al. in  (Brock et al., [2016](#bib.bib16)) has achieved a state-of-the-art
    results on $ModelNet$ classification, $FusionNet$ achieved comparable results
    with no need for the data augmentation or the very heavy computations needed in
    Brock’s model. This implies that practically employing 2D and 3D representations
    surpass the volumetric methods with less computations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 受到3D卷积和2D多视图CNN性能的启发，一些研究考察了这两种表示的融合。Hedge 和 Zadeh (Hegde and Zadeh, [2016](#bib.bib62))
    提出了将体积表示（体素）和2D表示（2D视图）融合用于对象分类任务。作者研究了不同的模型来处理数据进行$ModelNet$分类。第一个模型是结合了两种模态（3D和2D）的模型，同时使用两个3D
    CNN处理3D体素，使用AlexNet处理2D视图。另一个实验则仅处理3D体素，并与AlexNet在2D视图上的表现进行比较。实验结果表明，结合两种模态的网络（称为$FusionNet$）表现最好。然而，多视图网络的表现优于体积CNN。尽管Brock等人在(Brock
    et al., [2016](#bib.bib16))上在$ModelNet$分类中取得了最先进的结果，$FusionNet$取得了可比的结果，并且不需要数据增强或Brock模型中所需的重计算。这表明，在实际应用中，2D和3D表示超越了体积方法，并且计算量较少。
- en: 0.1\. Deep learning architectures on 3D non-Euclidean structured data
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 0.1\. 在3D非欧几里得结构数据上的深度学习架构
- en: The second type of 3D DL approaches is the non-Euclidean approaches that try
    to extend the DL concept to geometric data. However, the nature of the data is
    imposing challenges on how to perform the main DL operations such as convolution.
    A number of architectures that tried to extend DL to the 3D geometric domain were
    proposed. Some of these architectures addresses “3D point clouds”, in order to
    learn the geometry of a 3D shape and use it for modeling tasks. Results encouraged
    researchers to leverage the surface information provided in 3D meshes where the
    connectivity between vertices can be exploited to define local pseudo-coordinates
    to perform a convolution-like operation on 3D meshes. At the same time, some efforts
    were directed towards investigating graph-based 3D data attempting to leverage
    the spectral properties of graphs to define intrinsic descriptors to be used in
    the DL framework. In this section, we will go over the latest innovations in applying
    DL on non-Euclidean 3D data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种3D深度学习方法是非欧几里得方法，试图将深度学习概念扩展到几何数据中。然而，数据的性质对如何执行主要的深度学习操作（如卷积）提出了挑战。已经提出了一些架构，尝试将深度学习扩展到3D几何领域。其中一些架构涉及“3D点云”，旨在学习3D形状的几何信息并用于建模任务。结果鼓励研究人员利用3D网格中提供的表面信息，在该网格中，顶点之间的连接可以被利用来定义局部伪坐标，从而对3D网格进行类似卷积的操作。同时，一些努力也致力于研究基于图的3D数据，试图利用图的谱特性来定义用于深度学习框架的内在描述符。在这一部分，我们将探讨在非欧几里得3D数据上应用深度学习的最新创新。
- en: 0.1.1\. Point clouds
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 0.1.1\. 点云
- en: Point clouds provide an expressive, homogeneous and compact representation of
    the 3D surface geometry without the combinatorial irregularities and complexities
    of meshes. That is why point clouds are easy to learn from. However, processing
    point clouds is tricky due to their dual nature. Point clouds can be seen as a
    Euclidean-structured data locally when considering a point with respect to its
    neighborhood (a subset of points) such that the interaction among the points forms
    a Euclidean space with a distance metric which is invariant to transformations
    like translation, rotation. However, considering the global structure of the point
    cloud, it is an unordered set of points with no specific order which imposes the
    irregular non-Euclidean nature on the global structure of the data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 点云提供了一种表达性、同质且紧凑的3D表面几何表示，没有网格的组合不规则性和复杂性。这就是为什么点云容易学习的原因。然而，由于点云的双重性质，处理点云很棘手。点云可以在考虑点及其邻域（点的子集）时被视为具有欧几里得结构的数据，这样点之间的交互形成一个具有距离度量的欧几里得空间，对诸如平移、旋转等变换不变。然而，考虑到点云的全局结构，它是一个没有特定顺序的无序点集，这对数据的全局结构施加了不规则的非欧几里得特性。
- en: Some recent works have considered point clouds as a collection of sets with
    different sizes. Vinyals et al. in (Vinyals et al., [2015](#bib.bib134)) use a
    read-process-write network for processing point sets to show the network ability
    to learn how to sort numbers. This was a direct application of DL on an unordered
    set for the Natural Language Processing (NLP) application. Inspired by this work,
    Ravanbakhsh et al. (Ravanbakhsh et al., [2016](#bib.bib104)), proposed what they
    called the permutation equivariant layer within a supervised and semi-supervised
    settings. This layer is obtained by parameter-sharing to learn the permutation
    invariance as well as rigid transformations across the data. This network performed
    3D classification and MNIST digit addition. Although this network is relatively
    simple, it didn’t perform well on the 3D classification task on ModelNet dataset.
    A deeper version of this model was extended in (Zaheer et al., [2017](#bib.bib154))
    where the proposed $DeepSet$ framework produced better results than the state-of-the-art
    methods in 3D classification job on $ModelNet$ dataset. This is due to the permutation
    invariance property that the permutation equivarant layer is bringing to the previous
    models. (Qi et al., [2016a](#bib.bib99)) also used a similar layer with a major
    difference as the permutation equivariant layer is max-normalized.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究考虑了点云作为具有不同大小集合的集合。Vinyals 等人在 (Vinyals et al., [2015](#bib.bib134)) 中使用了一个读取-处理-写入网络来处理点集，以展示网络学习如何排序数字的能力。这是深度学习在自然语言处理
    (NLP) 应用中的无序集合的直接应用。受此工作的启发，Ravanbakhsh 等人 (Ravanbakhsh et al., [2016](#bib.bib104))
    提出了他们所称的置换等变层，在监督和半监督设置中。这一层通过参数共享来学习置换不变性以及跨数据的刚性变换。该网络进行了3D分类和MNIST数字加法。尽管该网络相对简单，但在
    ModelNet 数据集上的3D分类任务表现不佳。在 (Zaheer et al., [2017](#bib.bib154)) 中，提出了 $DeepSet$
    框架的更深版本，在 $ModelNet$ 数据集上的3D分类任务中比最先进的方法取得了更好的结果。这是由于置换等变层为先前模型带来的置换不变性属性。(Qi
    et al., [2016a](#bib.bib99)) 也使用了类似的层，但主要区别在于置换等变层是最大值归一化的。
- en: '$PointNet$ (Qi et al., [2017a](#bib.bib100)) is the pioneer in making a direct
    use of the point cloud as an input where each of its points is represented using
    the $(x,y,z)$ coordinates. As a pre-processing step, feature transformation and
    inputs are feeded into the $PointNet$ architecture. $PointNet$ is composed of
    three main modules: a “Spatial Transformer Network (STN)” module, an RNN module
    and a simple symmetric function that aggregates all the information from each
    point in the point cloud. The STN canonicalizes the data before feeding them to
    the RNN, i.e., process all the data into one canonical form, and learns the key
    points of the point cloud which approximately corresponds to the skeleton of the
    3D object. Then comes the RNN module which learns the point cloud like a sequential
    signal of points and while training this model with some randomly permuted sequence,
    this RNN becomes invariant to the sequence of the input order of the point cloud’s
    point. Lastly, the network aggregates all the resulted point features using the
    max-pooling operation which is also permutation invariant. $PointNet$ proved that
    it is robust against partial data and input perturbation. It was tested on classification
    and segmentation tasks where it proved to produce results comparable to the state-of-the-art
    as shown the supplementary material, Table 1.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: $PointNet$ （齐等人，[2017a](#bib.bib100)）是将点云直接作为输入的先驱，其中每个点使用$(x,y,z)$坐标表示。作为预处理步骤，特征变换和输入被馈送到$PointNet$架构中。$PointNet$由三个主要模块组成：一个“空间变换网络（STN）”模块，一个RNN模块和一个简单的对称函数，聚合点云中每个点的所有信息。STN在将数据馈送到RNN之前将数据规范化，即将所有数据处理成一个规范形式，并学习点云的关键点，这大致对应于3D对象的骨架。然后是RNN模块，它像处理序列信号一样学习点云，并在使用一些随机排列序列训练该模型时，该RNN对点云点的输入顺序不变。最后，网络使用最大池化操作聚合所有生成的点特征，这也是置换不变的。$PointNet$证明了它对部分数据和输入扰动具有鲁棒性。它在分类和分割任务上进行了测试，在这些任务中，它证明可以产生与最先进技术相媲美的结果，如补充材料表1所示。
- en: Despite the competitive results achieved by $PointNet$, it is not able to take
    full advantage of the point’s local structure to capture the detailed fine-grained
    patterns because of aggregating all the point features together. To address this
    point, PointNet++ (Qi et al., [2017b](#bib.bib102)) builds on $PointNet$ by recursively
    applying it to a nested partition of the input point sets. Despite capturing more
    features, the resulted architecture is complicated which increases the size of
    the higher features and the computational time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管$PointNet$取得了竞争性的结果，但由于聚合所有点特征，无法充分利用点的局部结构来捕获详细的细粒度模式。为了解决这一点，PointNet++ （齐等人，[2017b](#bib.bib102)）通过递归地将其应用于输入点集的嵌套分区，构建在$PointNet$的基础上。尽管捕获了更多特征，结果的架构变得复杂，增加了更高级特征和计算时间的大小。
- en: Instead of operating directly on the point clouds structure, Kd-Networks by
    Klokov et al. (Klokov and Lempitsky, [2017](#bib.bib75)) proposes to impose a
    kd-tree structure of the input point cloud to be used for learning the shared
    weights across the points of the tree. Kd-tree is a feed-forward network that
    has learnable parameters associated with the weights of the nodes in the tree.
    This model was tested for shape classification, shape retrieval and shape part
    segmentation producing competitive results. Following the same concept of not
    working directly on the point cloud structure, Roveri et al. (Roveri et al., [2018](#bib.bib106))
    proposed to extract a set of 2D depth maps from different views of the point cloud
    and process them using Residual Nets ($ResNet50$) (He et al., [2015](#bib.bib59)).
    The proposed framework contains 3 modules. The initial module is responsible of
    learning $k$ directional views of the input point cloud to generate the depth
    maps accordingly in the second module. The third and last module is processing
    the generated $k$ depth maps for object classification. The innovation of this
    framework is mainly focus on automatically transforming un-ordered point clouds
    to informative 2D depth maps without the need to adapt the network module to account
    for permutation invariance and different transformations of the input data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于直接处理点云结构，Klokov等人（Klokov和Lempitsky，[2017](#bib.bib75)）提出的Kd-Networks建议对输入点云施加kd树结构，以便学习树中点的共享权重。Kd-tree是一种前馈网络，具有与树节点权重相关的可学习参数。该模型在形状分类、形状检索和形状部分分割中进行了测试，产生了具有竞争力的结果。沿用不直接处理点云结构的相同概念，Roveri等人（Roveri等人，[2018](#bib.bib106)）建议从点云的不同视图中提取一组2D深度图，并使用Residual
    Nets（$ResNet50$）（He等人，[2015](#bib.bib59)）对其进行处理。该框架包含3个模块。第一个模块负责学习输入点云的$k$个方向视图，以便在第二个模块中生成深度图。第三个也是最后一个模块则处理生成的$k$个深度图以进行对象分类。该框架的创新点主要在于自动将无序点云转换为有信息的2D深度图，无需调整网络模块以适应置换不变性和输入数据的不同变换。
- en: In recent past, few articles have reported their work on unsupervised learning
    over the point clouds. In $FoldingNet$ (Yang et al., [2018](#bib.bib149)), Yang
    et al. proposed to use AE for modelling different 3D objects represented as point
    clouds by a novel folding-based decoder that deforms a 2D canonical grid into
    the underlying surface of the 3D point cloud. $FoldingNet$ is able to learn how
    to generate cuts on the 2D grid to create 3D surfaces and generalize to some intra-class
    variations of the same 3D object class. An SVM was used on top of this $FoldingNet$
    to be used for 3D classification where it proved to perform well with the learned
    discriminative representation for different 3D objects. FoldingNet achieved high
    classification accuracy on $ModelNet40$. Another unsupervised model was proposed
    by Li et al. called SO-Net (Li et al., [2018](#bib.bib83)). SO-Net is a permutation
    invariant network that can tolerate unordered point clouds inputs. SO-Net builds
    that spatial distribution of the points in the point cloud using “Self-Organizing
    Maps (SOMs)”. Then, a hierarchical feature extraction on the points of the point
    cloud and the SOM nodes is employed which results in a singular feature vector
    which represents the entire point cloud Local feature aggregation happens according
    to an adjustable receptive field where the overlap is controlled to get more effective
    features. SO-Net was tested on classification and segmentation tasks producing
    promising results highly comparable with the state-of-the-art techniques as shown
    in Table 1 of the supplementary material.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，少量文章报告了他们在点云上进行无监督学习的工作。在$FoldingNet$（Yang等人，[2018](#bib.bib149)）中，Yang等人提出使用AE来建模不同的3D对象，这些对象以点云形式表示，通过一种新型的基于折叠的解码器，将2D规范网格变形为3D点云的底层表面。$FoldingNet$能够学习如何在2D网格上生成切口以创建3D表面，并对相同3D对象类别的一些类别内变异进行泛化。在$FoldingNet$之上使用了SVM用于3D分类，证明了它在不同3D对象的学习判别表示上表现良好。$FoldingNet$在$ModelNet40$上达到了高分类准确率。另一个由Li等人提出的无监督模型是SO-Net（Li等人，[2018](#bib.bib83)）。SO-Net是一个置换不变网络，能够容忍无序点云输入。SO-Net使用“自组织映射（SOMs）”构建点云中点的空间分布。然后，对点云中的点和SOM节点进行分层特征提取，产生一个代表整个点云的唯一特征向量。局部特征聚合根据可调整的接收场进行，其中重叠受控，以获取更有效的特征。SO-Net在分类和分割任务中进行了测试，产生了与最先进技术高度可比的有希望的结果，如补充材料中的表1所示。
- en: As noticed in all the previously proposed methods, the main problem in processing
    point clouds is the un-ordered structure of this representation where researchers
    are trying to make the learning process invariant to the order of the point cloud.
    Most of these methods resorted to clustering techniques to opt for similar points
    and process them together.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所有先前提出的方法中所注意到的，处理点云的主要问题是其表示的无序结构，研究人员正在努力使学习过程对点云的顺序不变。大多数这些方法依赖于聚类技术，以选择相似的点并一起处理。
- en: 0.1.2\. Graphs and meshes
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 0.1.2\. 图和网格
- en: The ideal representation for graphs and meshes is the one that can capture all
    the intrinsic structure of the object and also can be learned with the gradient
    descent methods. This is due to their stability and frequent usage in the CNNs.
    However, learning such irregular representations is a challenging task due to
    the structural properties of these representations. Motivated by the success of
    CNNs in a broad range of computer vision tasks, recent research efforts were directed
    towards generalizing CNNs to such irregular structures. Analyzing the properties
    of such data shows that meshes can be converted to graphs as discussed in Section LABEL:dataRep_3D_meshes_and_graphs..
    Hence, the models proposed for graphs can be employed on mesh-structured data
    but not vice versa. Most of the existing work addresses the graph-structured data
    explicitly with some few works were tailored towards mesh representations. We
    herein overview recent works on each representation providing a broad classification
    for the existing methods based on the used approach.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图和网格的理想表示方法是能够捕捉对象所有固有结构并且能够通过梯度下降方法学习的表示方法。这是由于它们在卷积神经网络中的稳定性和频繁使用所致。然而，学习这些不规则表示是一项具有挑战性的任务，这是由于这些表示的结构特性。受卷积神经网络在广泛的计算机视觉任务中的成功启发，最近的研究工作集中于将卷积神经网络推广到这种不规则结构上。分析这些数据的特性表明，网格可以转换为图，如第
    LABEL:dataRep_3D_meshes_and_graphs 节所述。因此，提出的图模型可以应用于网格结构数据，但反之则不行。现有工作大部分明确地处理图结构数据，少数工作专门针对网格表示进行了定制。在此，我们概述了每种表示的最新工作，并基于使用的方法对现有方法进行了广泛分类。
- en: 'Graphs: Studying the structural properties of both graphs and meshes suggests
    that the proposed learning methods for graphs are also applicable on meshes. Existing
    methods for Graph Convolutional Neural Networks (GCNN) can be broadly categorized
    into two main directions: spectral filtering methods and spatial filtering methods.
    Here we discuss the underlying concept behind each method and overview the work
    done in each direction. The distinction between both directions is in how the
    filtering is employed and how the locally processed information is combined.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图：研究图和网格的结构特性表明，为图提出的学习方法也适用于网格。现有的图卷积神经网络（GCNN）方法可以大致分为两个主要方向：谱过滤方法和空间过滤方法。这里我们讨论了每种方法背后的概念，并概述了在每个方向上的工作。这两个方向之间的区别在于过滤器的应用方式以及如何组合本地处理的信息。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Spectral filtering methods. The notion of spectral convolution on graph-structured
    data was introduced by Bruna et al. in (Bruna et al., [2013](#bib.bib24)) where
    the authors proposed Spectral CNN (SCNN) operating on graphs. The foundation of
    the spectral filtering methods is to use the spectral eigen-decomposition of the
    graph Laplacian to define a convolution-like operator. This redefines the convolution
    operation in the spectral domain where the main two core stones are analogous:
    the patches of the signal in the Euclidean domain correspond to the functions
    defined on the graph nodes e.g. features, mapped to the spectral domain by projecting
    on the eigenvectors of the graph Laplacian. The filtering operation itself happens
    in the Euclidean domain and corresponds to scaling the signals in the eigenbasis.
    This definition implies that convolution is a linear operator that commutes with
    the Laplacian operator (Bronstein et al., [2017](#bib.bib21)). Despite the innovation
    aspect of Bruna’s model, it has serious limitations due to being basis dependent
    and computationally expensive. Being basis-dependent means that if the spectral
    filter’s coefficients were learned with respect to a specific basis, applying
    the learned coefficients on another domain with another basis will produce very
    different results as illustrated in (Bronstein et al., [2017](#bib.bib21)). The
    other limitation of being computationally costly arises from the fact that spectral
    filtering is a non-local operation that involves data across the whole graph besides
    that the graph Laplacian is expensive to compute. This constitutes a computational
    burden towards generalizing to other bases and processing large-scale graphs.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 光谱过滤方法。光谱卷积在图结构数据上的概念是由布鲁纳等人在（布鲁纳等人，[2013](#bib.bib24)）中引入的，作者提出了在图上操作的光谱CNN（SCNN）。光谱过滤方法的基础是使用图拉普拉斯的光谱特征分解来定义类似卷积的算子。这将卷积操作重新定义到光谱域，其中两个核心要素是类似的：欧几里得域中信号的片段对应于定义在图节点上的函数，例如特征，通过在图拉普拉斯的特征向量上投影映射到光谱域。过滤操作本身发生在欧几里得域中，并且对信号在特征基中的缩放对应。这个定义意味着卷积是一个线性算子，与拉普拉斯算子交换（布朗斯坦等人，[2017](#bib.bib21)）。尽管布鲁纳模型具有创新性，但由于依赖基础和计算昂贵，它具有严重的局限性。依赖基础意味着如果光谱滤波器的系数是根据特定基础学习的，将学习的系数应用于具有另一个基础的另一个域将产生非常不同的结果，如布朗斯坦等人在（布朗斯坦等人，[2017](#bib.bib21)）中所示。计算昂贵的另一个限制来自于光谱过滤是一个涉及整个图数据的非局部操作，除此之外，图拉普拉斯的计算昂贵。这构成了向其他基础泛化和处理大规模图的计算负担。
- en: The work in (Kovnatsky et al., [2013](#bib.bib76)) addressed the basis dependency
    problem by constructing a compatible orthogonal basis across various domains through
    a joint diagonalization. However, this required a prior knowledge about the correspondence
    between the domains. For some applications like social networks, this is a valid
    assumption because the correspondence can be easily computed between two time
    instances in which new edges and vertices have been added. However, applying this
    on meshes is rather unreasonable because finding correspondence between two meshes
    is challenging task on its own. Therefore, assuming the knowledge of correspondence
    between domains in such case is unrealistic (Bronstein et al., [2017](#bib.bib21)).
    Since the non-local nature of the spectral filtering and the need to involve all
    the graph data in the processing, recent works proposed the idea of approximation
    to produce local spectral filters (Defferrard et al., [2016](#bib.bib37)) (Kipf
    and Welling, [2016](#bib.bib74)). These methods propose to represent the filters
    via a polynomial expansion instead of directly operating on the spectral domain.
    Defferrard et al. in  (Defferrard et al., [2016](#bib.bib37)) performed local
    spectral filtering on graphs by using Chebyshev polynomials in order to approximate
    the spectral graph filters. The features yielding from the convolution operation
    are then coarsened using the graph pooling operation. Kipf and Welling (Kipf and
    Welling, [2016](#bib.bib74)) simplified the polynomial approximation proposed
    in (Defferrard et al., [2016](#bib.bib37)) and used a first-order linear approximation
    of the graph spectral filters to produce local spectral filters which are then
    employed in a two-layer GCNN. Each of these two layers uses the local spectral
    filters and aggregates the information from the immediate neighbourhood of the
    vertices. Note that the filters proposed in (Defferrard et al., [2016](#bib.bib37))
    and  (Kipf and Welling, [2016](#bib.bib74)) are employed on r- or 1-hop neighbourhood
    of the graph returns these constructions into the spatial domain.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在(Kovnatsky et al., [2013](#bib.bib76))的工作中，通过联合对角化构建了跨各种领域的兼容正交基，从而解决了基础依赖问题。然而，这需要事先了解领域之间的对应关系。对于一些应用，如社交网络，这是一个有效的假设，因为可以很容易计算出在添加了新边和新顶点的两个时间点之间的对应关系。然而，将其应用于网格是不太合理的，因为在两个网格之间找到对应关系本身就是一个具有挑战性的任务。因此，在这种情况下假设领域之间的对应关系是不切实际的(Bronstein
    et al., [2017](#bib.bib21))。由于光谱滤波的非局部性质和涉及所有图数据处理的需要，最近的工作提出了近似的概念，以生成局部光谱滤波器(Defferrard
    et al., [2016](#bib.bib37)) (Kipf and Welling, [2016](#bib.bib74))。这些方法建议通过多项式展开来表示滤波器，而不是直接在光谱域上操作。Defferrard
    et al. 在(Defferrard et al., [2016](#bib.bib37))中通过使用切比雪夫多项式在图上进行局部光谱滤波，以近似光谱图滤波器。然后，卷积操作产生的特征使用图池化操作进行粗化。Kipf和Welling
    (Kipf and Welling, [2016](#bib.bib74))简化了(Defferrard et al., [2016](#bib.bib37))中提出的多项式近似，并使用图光谱滤波器的一阶线性近似来生成局部光谱滤波器，这些滤波器随后被用于两层GCNN中。每一层使用局部光谱滤波器并聚合来自顶点的直接邻域的信息。请注意，(Defferrard
    et al., [2016](#bib.bib37)) 和 (Kipf and Welling, [2016](#bib.bib74))中提出的滤波器在r-或1跳邻域上应用，这些构造被带回空间域。
- en: Driven by the success of the local spectral filtering models, Wang et al. (Wang
    et al., [2018](#bib.bib136)) proposed to take the advantages of the power of spectral
    GCNNs in the pointNet++ framework (Qi et al., [2017b](#bib.bib102)) to process
    unordered point clouds. This model fuses the innovation of the pointNet++ framework
    with local spectral filtering while addressing two shortcomings of these models
    independently. Therefore, instead of processing each point independently in the
    point clouds as proposed in pointNet++, this model uses spectral filtering as
    a learning technique to change the structural information of each points’ neighborhood.
    Moreover, rather than using the greedy winner-takes all method in the graph max
    pooling operation, this method adopts a recursive pooling and clustering strategy.
    Unlike the previous spectral filtering methods, this method does not require any
    pre-computation and it is trainable by an end-to-end manner which allows building
    the graph dynamically and computing the graph Laplacian and the pooling hierarchy
    on the fly unlike (Bruna et al., [2013](#bib.bib24); Defferrard et al., [2016](#bib.bib37);
    Kipf and Welling, [2016](#bib.bib74)). This method have been able to achieve better
    recognition results than the existing state-of-the-art techniques on diverse datasets
    as shown in Table 1 of the supplementary material.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在局部光谱过滤模型取得成功的驱动下，王等人（Wang et al., [2018](#bib.bib136)）建议在pointNet++框架（Qi et
    al., [2017b](#bib.bib102)）中利用光谱GCNN的优势来处理无序点云。这种模型将pointNet++框架的创新与局部光谱过滤相融合，同时解决了这些模型各自的两个缺点。因此，不同于pointNet++中建议的对点云中的每个点独立处理，这种模型采用光谱过滤作为学习技术，改变每个点邻域的结构信息。此外，不同于图最大池化操作中的贪婪胜者全拿策略，该方法采用递归池化和聚类策略。与先前的光谱过滤方法不同，该方法不需要任何预计算，并且可以通过端到端的方式进行训练，允许动态构建图并即时计算图拉普拉斯和池化层次，不像(Bruna
    et al., [2013](#bib.bib24); Defferrard et al., [2016](#bib.bib37); Kipf and Welling,
    [2016](#bib.bib74))那样。该方法已经能够在多样的数据集上取得比现有最先进技术更好的识别结果，详见补充材料的表1。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Spatial filtering methods. The concept of graph spatial filtering started in (Scarselli
    et al., [2009](#bib.bib112)) when GNNs were first proposed as an attempt to generalize
    DL models to graphs. GNNs are simple constructions that try to generalize the
    notion of spatial filtering on graphs via the weights of the graph. GNNs are composed
    of multiple layers where each layer is a linear combination of graph high-pass
    and low-pass operators. This formulation suggests that learning the graph features
    is dependent on each vertex’s neighborhood. Similar to Euclidean CNNs, a non-linear
    function is applied to all the nodes of the graph where the choice of this function
    varies depending on the task. Varying the nature of the vertex non-linear function
    lead to rich architectures (Li et al., [2015](#bib.bib84); Sukhbaatar et al.,
    [2016](#bib.bib126); Duvenaud et al., [2015](#bib.bib40); Chang et al., [2016](#bib.bib29);
    Battaglia et al., [2016](#bib.bib11)). Also, analogous to CNNs, pooling operation
    can be employed on graph-structured data by graph coarsening. Graph pooling layers
    can be performed by interleaving the graph learning layers. In comparison with
    the spectral graph filtering, spatial filtering methods have two key points which
    distinguish them from spectral methods. Spatial methods aggregate the feature
    vectors from the neighborhood nodes directly based on the graph topology considering
    the spatial structure of the input graph. The aggregated features are then summarized
    via an additional operation. The GNN framework presented in (Scarselli et al.,
    [2009](#bib.bib112); Gori et al., [2005](#bib.bib50)), proposed to embed each
    vertex in the graph into a Euclidean space with an RNN. Instead of using the recursive
    connections in the RNN, the authors used a simple diffusion function for their
    transition function, propagating the node representation repeatedly until it is
    stable and fixed. The resulting node representations are considered as the features
    for classification and regression problems. The repeated propagation for node
    features in this framework constitutes a computational burden which is alleviated
    in the work proposed by Li et al. (Li et al., [2015](#bib.bib84)). They have proposed
    a variant of the previous model which uses the gated recurrent units to perform
    the state updates to learn the optimal graph representation. Bruna et al. in (Bruna
    et al., [2013](#bib.bib24)) imposed the spatial local receptive field on GNN to
    produce their local spatial formulation of GNN. The main idea behind the local
    receptive field is to decrease the number of the learned parameters by grouping
    similar features based on a similarity measure (Coates and Ng, [2011](#bib.bib32);
    Gregor and LeCun, [2010](#bib.bib51)). In (Bruna et al., [2013](#bib.bib24)),
    the authors used this concept to compute a multi-scale clustering of the graph
    to be fed to the pooling layer afterwards. This model imposes locality on the
    processed features and reduces the amount of processed parameters. However, it
    doesn’t perform any weight sharing similar to 2D CNNs. Niepert et al.in (Niepert
    et al., [2016](#bib.bib95)) performs spatial graph convolution in a simple way
    by converting the graph locally into sequences and feeding these sequences into
    a 1D CNN. This method is simple but requires an explicit definition for the nodes
    orders of the graphs in a pre-processing step. In (Venkatakrishnan et al., [2018](#bib.bib132)),
    the authors provided a detailed study proving that spectral methods and spatial
    methods are mathematically equivalent in terms of their representation capabilities.
    The difference resides in how the convolution and the aggregation of the learned
    features are performed. Depending on the task, the architecture of the GCNN (spectral
    or spatial) is formed where the convolution layers may be interleaved with coarsening
    and pooling layers to summarize the output of the convolution filters for a compact
    representation of the graph. This is crucial in classification applications where
    the output is only one class inferred from the learned features (Bruna et al.,
    [2013](#bib.bib24)). Some other applications require a decision per node such
    as community detection. A common practice in such cases is to have multiple convolution
    layers that compute the graph representations at the node level  (Khalil et al.,
    [2017](#bib.bib73); Nowak et al., [2017](#bib.bib97); Bruna and Li, [2017](#bib.bib23)).
    All these GCNNs are end-to-end differentiable methods that can be trained in supervised,
    semi-supervised or reinforcement learning techniques.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 空间滤波方法。图空间滤波的概念始于（Scarselli等人，[2009](#bib.bib112)），当时GNN首次被提出，试图将DL模型推广到图形数据上。GNN是一种简单的结构，通过图的权重尝试推广图的空间滤波的概念。GNN由多个层组成，每一层都是图高通和低通操作符的线性组合。这种表述表明，学习图特征依赖于每个顶点的邻域。类似于欧几里得CNN，非线性函数应用于图的所有节点，函数的选择取决于任务的性质。改变顶点非线性函数的性质导致了丰富的架构（Li等，[2015](#bib.bib84);
    Sukhbaatar等，[2016](#bib.bib126); Duvenaud等，[2015](#bib.bib40); Chang等，[2016](#bib.bib29);
    Battaglia等，[2016](#bib.bib11)）。此外，类似于CNN，可以通过图的粗化在图结构数据上使用池化操作。图池化层可以通过交错的图学习层执行。与谱图滤波相比，空间滤波方法有两个关键点，使其与谱方法有所区别。空间方法直接基于图的拓扑结构聚合邻域节点的特征向量，考虑输入图的空间结构。然后，通过附加操作对聚合特征进行总结。在（Scarselli等人，[2009](#bib.bib112);
    Gori等人，[2005](#bib.bib50)）提出的GNN框架中，建议将图中的每个顶点嵌入到欧几里得空间中，并使用RNN。作者未使用RNN中的递归连接，而是使用了简单的扩散函数作为它们的过渡函数，重复传播节点表示直到其稳定固定。得到的节点表示被认为是分类和回归问题的特征。在这个框架中，节点特征的重复传播构成了一个计算负担，这在Li等人提出的工作中得到了缓解（Li等，[2015](#bib.bib84)）。他们提出了一个先前模型的变体，该模型使用门控循环单元执行状态更新以学习最优的图表示。在（Bruna等人，[2013](#bib.bib24)）中，Bruna等人将空间局部接受域应用于GNN，以生成他们的GNN的局部空间表述。局部接受域背后的主要思想是通过基于相似度测量将类似特征分组来减少学习参数的数量（Coates和Ng，[2011](#bib.bib32);
    Gregor和LeCun，[2010](#bib.bib51)）。在（Bruna等人，[2013](#bib.bib24)）中，作者使用了这个概念来计算图的多尺度聚类，然后将其馈送到池化层中。这种模型对处理特征施加了局部性，并减少了处理参数的数量。然而，它不像2D
    CNN那样执行任何权重共享。在（Niepert等人，[2016](#bib.bib95)）中，Niepert等人通过将图局部地转换为序列，并将这些序列馈送到1D
    CNN中来执行简单的空间图卷积。这种方法简单但需要在预处理步骤中明确定义图的节点顺序。在（Venkatakrishnan等人，[2018](#bib.bib132)）中，作者提供了一项详细的研究，证明了谱方法和空间方法在其表示能力上在数学上是等价的。差异在于如何执行卷积和学习特征的聚合。根据任务，形成GCNN的架构（谱或空间），其中卷积层可以与粗化和池化层交错以总结卷积滤波器的输出，以获得图的紧凑表示。这在分类应用中至关重要，其中输出仅是从学习特征推断出的一个类（Bruna等人，[2013](#bib.bib24)）。一些其他应用需要每个节点的决策，如社区检测。在这种情况下的常见做法是具有多个卷积层，这些卷积层在节点级别计算图表示（Khalil等人，[2017](#bib.bib73);
    Nowak等人，[2017](#bib.bib97); Bruna和Li，[2017](#bib.bib23)）。所有这些GCNN都是端到端可微分的方法，可以在监督、半监督或强化学习技术中进行训练。
- en: 'Meshes: On the Euclidean domain, the convolution operation is performed by
    passing a template at each point on the spatial domain and recording the correlation
    between the templates using the function that is defined at this point. This is
    feasible due to the shift-invariance property on the Euclidean domain. Howeverm,
    unfortunately, this is not directly applicable on meshes because there is a lack
    of the shift-invariance property. This is what pushed towards defining local patches
    that represent the 3D surface in a way that allows performing convolution. However,
    due to the lack of global parametrization on non-Euclidean data, these patches
    are defined in a local system of coordinates locally meaning that these patches
    are also position-dependent. Recently, various non-Euclidean CNNs frameworks were
    proposed. The main schema used by these frameworks is very similar except for
    how the patches are defined mostly. The local patches are defined either by handcrafting
    them or depending on the connectivity of the vertices while using the features
    of the 1-hop neighborhood as the patch directly (Fey et al., [2017](#bib.bib47)).
    The convolution employed in such frameworks is very similar to the classical 2D
    convolution where it is basically an element-wise multiplication between the convolution
    filter and the patch and summing up the results. This is because the patches extracted
    by such frameworks boils down the representation into 2D where the classical convolution
    can be employed.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 网格：在欧几里得域上，卷积操作通过在空间域的每一点上传递模板并使用在该点定义的函数记录模板之间的相关性来完成。这是可行的，因为欧几里得域具有平移不变性属性。然而，不幸的是，由于缺乏平移不变性属性，这种方法不能直接应用于网格。这推动了定义局部补丁的工作，这些补丁以一种允许执行卷积的方式表示三维表面。然而，由于非欧几里得数据缺乏
- en: Geodesic CNN (Masci et al., [2015](#bib.bib92)) was introduced as a generalization
    of classical CNNs to triangular meshes. The main idea of this approach is to construct
    local patches in local polar coordinates. The values of the functions around each
    vertex in the mesh are mapped into local polar coordinates using the patch operator.
    This defines the patches where the geodesic convolution is employed. Geodesic
    convolution follows the idea of multiplication by a template. However, the convolution
    filters in this framework are subjected to some arbitrary rotations due to the
    angular coordinate ambiguity (Masci et al., [2015](#bib.bib92)). This method opened
    the door for new innovations on extending CNN paradigm to triangular meshes. However,
    this framework suffers from multiple drawbacks. First, it can only be applied
    on triangular meshes where it is sensitive to the triangulation of the mesh and
    it might fail if the mesh is extremely irregular. Second, the radius of the constructed
    geodesic patch has to be small with respect to the injectivity radius of the actual
    shape to guarantee that the resulted patch is topologically a disk. Third, the
    rotations employed on the convolution filters make the framework computationally
    expensive which limits the usage of such a framework. Anisotropic CNN (ACNN) (Boscaini
    et al., [2016](#bib.bib15)) was proposed to overcome some of the limitations in
    the geodesic CNN. Compared to the geodesic CNN, the ACNN framework is not limited
    to triangular meshes and can be also applied to graphs. Also, the construction
    of the local patches is simpler and is independent on the injectivity radius of
    the mesh. ACNN uses the concept of spectral filtering where the spatial information
    is also incorporated by a weighting function to extract a local function defined
    on the meshes. The learnt spectral filters are applied to the eigenvalues of the
    anisotropic Laplacian Beltrami Operator (LBO) and the anisotropic heat kernels
    act as a spatial weighting functions for the convolution filters. This method
    has shown a very good performance for local correspondence tasks. Rather than
    using a fixed kernel construction as in the previous models, Monti et al. (Monti
    et al., [2017](#bib.bib94)) proposed $MoNet$ as a general construction of patches.
    The authors proposed to define a local system of coordinates of pseudo-coordinates
    around each vertex with weight functions. On these coordinates, a set of parametric
    kernels are applied on these pseudo-coordinates to define the weighting functions
    at each vertex. That is why the previous methods (Masci et al., [2015](#bib.bib92);
    Boscaini et al., [2016](#bib.bib15)) can be considered as specific instances of
    MoNet. Some recent work has been proposed to eliminate the need to explicitly
    define the local patches on the graphs or meshes such as SplineCNN (Fey et al.,
    [2017](#bib.bib47)). SplineCNN is a convolutional framework that can be employed
    on directed graphs of any dimensionality. Hence, it can also be applied on meshes.
    Instead of defining the local patches by a charting-based method like the previous
    methods, SplineCNN uses the 1-hop neighborhood ring features of the graph as the
    patch where the convolutional filter can operate. The convolutional filter itself
    is a spatial continuous filter based on B-Spline basis functions that have local
    support. This framework produces state-of-the-art results on the correspondence
    task while being computationally very efficient. This is due to the local support
    of the B-Spline basis which makes the computational time independent of the kernel
    size.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Geodesic CNN (Masci 等人，[2015](#bib.bib92)) 是对三角网格上经典 CNN 的一般化引入。该方法的主要思想是在局部极坐标中构造局部补丁。将网格中每个顶点周围的函数值映射到局部极坐标中，使用补丁运算符定义补丁，在此处应用测地卷积。测地卷积遵循模板乘法的思想。然而，在这个框架中，卷积滤波器由于角坐标的歧义性而受到某些任意旋转的影响 (Masci
    等人，[2015](#bib.bib92))。这种方法为将 CNN 范式扩展到三角网格开辟了新的创新空间。然而，这个框架存在多个缺点。首先，它只能应用于三角网格，在这种情况下对网格的三角化敏感，如果网格极不规则，可能会失败。其次，构建的测地补丁的半径必须相对于实际形状的单射半径足够小，以确保结果的补丁在拓扑上是一个圆盘。第三，卷积滤波器上的旋转使得该框架在计算上昂贵，限制了这种框架的使用。提出了各向异性
    CNN (ACNN) (Boscaini 等人，[2016](#bib.bib15)) 以克服测地 CNN 的一些限制。与测地 CNN 相比，ACNN 框架不局限于三角网格，也可以应用于图形。此外，局部补丁的构建更简单，并且与网格的单射半径无关。ACNN
    使用了谱滤波的概念，其中通过加权函数来提取定义在网格上的局部函数。学习得到的谱滤波器应用于各向异性拉普拉斯-贝尔特拉米算子 (LBO) 的特征值，而各向异性热核作为空间加权函数用于卷积滤波器。该方法在局部对应任务中表现出非常好的性能。Monti
    等人 (Monti 等人，[2017](#bib.bib94)) 提出了 $MoNet$ 作为补丁的一般构造。作者们建议在每个顶点周围定义一个伪坐标或加权函数的局部坐标系。在这些坐标上，一组参数化核函数被应用于这些伪坐标，以定义每个顶点的加权函数。因此，前述方法 (Masci
    等人，[2015](#bib.bib92); Boscaini 等人，[2016](#bib.bib15)) 可以被认为是 $MoNet$ 的具体实例。一些最近的工作提出了消除在图形或网格上明确定义局部补丁的需要的方法，如
    SplineCNN (Fey 等人，[2017](#bib.bib47))。SplineCNN 是一个可以应用于任意维度有向图的卷积框架。因此，它也可以应用于网格。与之前的方法类似，SplineCNN
    不是通过基于图表的方法来定义局部补丁，而是使用图的 1-hop 邻域环特征作为卷积滤波器可以操作的补丁。卷积滤波器本身是基于 B-Spline 基函数的空间连续滤波器，具有局部支持。由于
    B-Spline 基函数的局部支持，这个框架在保持计算时间独立于核大小的同时产生了最先进的对应任务结果。
- en: 1\. Analysis and discussions
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 分析和讨论
- en: DL paradigms are successfully architect and deployed to various 3D data representations
    as discussed in the previous sections. Several approaches have been proposed.
    We herein discuss the main 3D datasets and their exploitation in various 3D computer
    vision tasks. Also, we present DL advances in three main tasks; 3D recognition/classification,
    retrieval and correspondence.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DL范式已成功构建并应用于各种3D数据表示，如前文所讨论的。已经提出了几种方法。我们在这里讨论主要的3D数据集及其在各种3D计算机视觉任务中的利用。此外，我们介绍了DL在三个主要任务中的进展：3D识别/分类、检索和对应。
- en: 1.1\. 3D Datasets
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1. 3D数据集
- en: 'We overview below the most recent 3D datasets. There are two main categories
    of data used by the research community: real-world datasets and synthetic data
    rendered from CAD models. It is preferable to use the real-world data; however,
    real data is expensive to collect and usually suffers from noise and occlusion
    problems. In contrast, synthetic data can produce a huge amount of clean data
    with limited modelling problems. While this can be seen advantageous, it is quite
    limiting to the generalization ability of the learned model to real-world test
    data. It is also important to note that most 3D datasets are smaller than large
    2D datasets such as, $ImageNet$ (Deng et al., [2009](#bib.bib38)). However, there
    are some recent exceptions as described below.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面概述了最新的3D数据集。研究社区使用的数据主要有两大类：真实世界数据集和从CAD模型渲染的合成数据。优先使用真实世界数据；然而，真实数据的收集成本高，并且通常存在噪声和遮挡问题。相比之下，合成数据可以生成大量清晰的数据，建模问题有限。虽然这被认为是有利的，但对模型在真实世界测试数据上的泛化能力有一定限制。同样值得注意的是，大多数3D数据集比大型2D数据集（如$ImageNet$
    (Deng et al., [2009](#bib.bib38))）要小。然而，下面描述了一些最近的例外。
- en: 'ModelNet (Wu et al., [2015a](#bib.bib143)) is the most commonly used dataset
    for 3D object recognition and classification. It contains roughly 130k annotated
    CAD models on 662 distinct categories. This dataset was collected using online
    search engines by querying for each of the categories. Then, the data were manually
    annotated. ModelNet provides the 3D geometry of the shape without any information
    about the texture. ModelNet has two subsets: ModelNet10 and ModelNet40. These
    subsets are used in most of the recently published work as shown in Table 1 and
    Table 2 in the supplementary material. In Section [1.2.1](#S1.SS2.SSS1 "1.2.1\.
    3D recognition/classification ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis
    and discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations"),
    we provide an extensive analysis of the methods employed on the ModelNet dataset
    for recognition and retrieval tasks highlighting the evolution of DL methods for
    learning such data.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ModelNet (Wu et al., [2015a](#bib.bib143)) 是最常用的3D物体识别和分类数据集。它包含约130k个标注的CAD模型，覆盖662个不同的类别。该数据集是通过在线搜索引擎查询每个类别收集的。然后，这些数据被手动标注。ModelNet提供了形状的3D几何信息，但没有纹理信息。ModelNet有两个子集：ModelNet10和ModelNet40。这些子集在最近发表的工作中被广泛使用，如补充材料中的表1和表2所示。在第[1.2.1](#S1.SS2.SSS1
    "1.2.1\. 3D识别/分类 ‣ 1.2\. 3D计算机视觉任务 ‣ 1\. 分析和讨论 ‣ 深度学习在不同3D数据表示上的进展调查")节中，我们提供了对ModelNet数据集在识别和检索任务中所使用方法的深入分析，重点介绍了DL方法在学习这些数据方面的发展。
- en: SUNCG (Song et al., [2017](#bib.bib124)) contains about 400K of full room models.
    This dataset is synthetic, however, each of these models was validated to be realistic
    and it was processed to be annotated with labelled object models. This dataset
    is important for learning the “scene-object” relationship and to fine-tune real-world
    data for scene understanding tasks. SceneNet (Handa et al., [2016](#bib.bib57))
    is also an RGB-D dataset that uses synthetic indoor rooms. This dataset contains
    about 5M scenes that are randomly sampled from a distribution to reflect the real
    world. However, not all the generated scenes are realistic and in practice, some
    of them are highly unrealistic. Still, this dataset can be used for fine-tuning
    and pre-training. In contrast, ScanNet (Dai et al., [2017](#bib.bib35)) is a very
    rich dataset for real-world scenes. It is an annotated dataset which is labelled
    with some semantic segmentation, camera orientation and the 3D information that
    is gathered from 3D video sequences of real indoor scenes. It includes 2.5M views,
    which allows for training directly without pre-training on other datasets, as
    it is the case with different datasets.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: SUNCG（宋等人，[2017](#bib.bib124)）包含约 400K 个完整的房间模型。这个数据集是合成的，但每个模型都经过验证是现实的，并且经过处理以带有标记的对象模型注释。这个数据集对学习“场景-对象”关系以及为场景理解任务微调真实世界数据非常重要。SceneNet（Handa
    等人，[2016](#bib.bib57)）也是一个使用合成室内房间的 RGB-D 数据集。该数据集包含约 5M 个从分布中随机抽样的场景，以反映真实世界。然而，并非所有生成的场景都是现实的，在实践中，有些场景是高度不现实的。尽管如此，这个数据集可以用于微调和预训练。相比之下，ScanNet（戴等人，[2017](#bib.bib35)）是一个非常丰富的真实世界场景数据集。它是一个带有一些语义分割、摄像机方向和从真实室内场景的
    3D 视频序列中收集的 3D 信息标注数据集。它包括 2.5M 个视角，可以直接在其他数据集上进行训练而不需要预训练，这与其他数据集的情况不同。
- en: In addition, datasets for 3D meshes are available for the 3D computer vision
    community. Most of the 3D meshes datasets are for 3D objects, body models or face
    data. The TOSCA (Bronstein et al., [2008](#bib.bib20)) dataset provides high-resolution
    3D synthetic meshes for non-rigid shapes. It contains a total of 80 objects in
    various poses. Objects within the same category have the same number of vertices
    and the same triangulation connectivity. TOSCA (Bronstein et al., [2008](#bib.bib20))
    provides artistic deformations on the meshes to simulate the real-world deformations
    of real scans. SHREC (Bronstein et al., [2010](#bib.bib19)) adds a variety of
    artificial noise and artistic deformations on TOSCA scans. However, the artificial
    noise and deformations are not realistic and can’t generalize to new unseen real-world
    data which is a requirement for practical solutions. FAUST (Bogo et al., [2014](#bib.bib14))
    dataset, however, provides 300 real-scans of 10 people in various poses. The 3DBodyTex
    dataset (Saint et al., [2018](#bib.bib109)) is recently proposed with 200 real
    3D body scans with high-resolution texture. 3DBodyTex is a registered dataset
    with the landmarks available for 3D human body models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，3D 网格数据集可供 3D 计算机视觉社区使用。大多数 3D 网格数据集用于 3D 对象、身体模型或面部数据。TOSCA（Bronstein 等人，[2008](#bib.bib20)）数据集为非刚性形状提供高分辨率的
    3D 合成网格。它包含多个姿势的共 80 个对象。同一类别内的对象具有相同数量的顶点和相同的三角形连接。TOSCA（Bronstein 等人，[2008](#bib.bib20)）对网格施加艺术变形以模拟真实扫描的真实世界变形。SHREC（Bronstein
    等人，[2010](#bib.bib19)）在 TOSCA 扫描上添加了各种人工噪声和艺术变形。然而，这些人工噪声和变形并不现实，不能泛化到新的未见真实世界数据，这是实际解决方案的要求。FAUST（Bogo
    等人，[2014](#bib.bib14)）数据集提供了 10 个人物在不同姿势下的 300 个真实扫描。最近提出的 3DBodyTex 数据集（Saint
    等人，[2018](#bib.bib109)）带有 200 个真实的 3D 人体扫描，具有高分辨率的纹理。3DBodyTex 是一个带有 3D 人体模型标记的注册数据集。
- en: The series of the BU datasets is very popular for 3D faces under various expressions.
    BU-3DFE (Yin et al., [2006](#bib.bib152)) is a static dataset which has 100 subjects
    (56 female and 44 male) of different ages and races. Each subject has in addition
    to the neutral face, six expressions as (happiness, sadness, anger, disgust, fear
    and surprise) of different intensities. There is 25 meshes for each subject in
    total, resulting in 2500 3D facial expressions dataset. Another very popular dataset
    is BU-4DFE (Yin et al., [2008](#bib.bib151)), which is a dynamic facial expression
    dataset that has 101 subjects in total (58 female and 43 male). Similar to the
    BU-3DFE, each subject has six expressions. Other 3D faces datasets were available
    as well like the BP4D-Spontanous (Zhang et al., [2014](#bib.bib157)) and BP4D+ (Zhang
    et al., [2016](#bib.bib158)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: BU系列数据集在各种表情下的3D面部数据中非常受欢迎。BU-3DFE（Yin等，[2006](#bib.bib152)）是一个静态数据集，包含100名受试者（56名女性和44名男性），他们的年龄和种族各不相同。每个受试者除了中性面孔外，还有六种表情（快乐、悲伤、愤怒、厌恶、恐惧和惊讶），每种表情有不同的强度。每个受试者总共有25个网格，共形成2500个3D面部表情数据集。另一个非常受欢迎的数据集是BU-4DFE（Yin等，[2008](#bib.bib151)），这是一个动态面部表情数据集，总共有101名受试者（58名女性和43名男性）。与BU-3DFE类似，每个受试者有六种表情。还有其他3D面部数据集，如BP4D-Spontanous（Zhang等，[2014](#bib.bib157)）和BP4D+（Zhang等，[2016](#bib.bib158)）。
- en: 1.2\. 3D Computer vision tasks
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 3D计算机视觉任务
- en: After the huge success of DL approaches in various computer vision tasks in
    the 2D domain, DL methods have gained more popularity as they are producing some
    remarkable performances on different tasks. Here, we overview 3D DL advances on
    the 3D object recognition/classification, retrieval and correspondence tasks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在DL方法在2D领域的各种计算机视觉任务中取得巨大成功后，DL方法因其在不同任务中表现出色而获得了更多的关注。在这里，我们概述了3D DL在3D物体识别/分类、检索和匹配任务中的进展。
- en: 1.2.1\. 3D recognition/classification
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.1\. 3D识别/分类
- en: The tasks of 3D recognition/classification is fundamental in computer vision.
    Given a 3D shape, the goal is to identify the class to which this shape belongs
    (3D classification) or given a 3D scene, recognize different 3D shapes in the
    scene along with their positions (3D recognition and localization). There is an
    active research to exploit Deep Neural Networks (DNNs) for 3D object recognition/classification.
    Existing approaches can be classified according to their input to learn the task
    as shown in Table 1 and Table 2 in the supplementary material. Multi-view approaches
    perform the task of classification using the learned features after applying 2D
    CNNs on each view. Generally, the multi-view methods outperform the other methods;
    however, there are still some unsolved drawbacks. For example, the recent Multi-View
    CNN (MVCNN) (Su et al., [2015](#bib.bib125)) applies a max-pooling operation on
    the features of each view to produce global features that represent the 3D object.
    The max-pooling ignores the non-maximal activation and only keeps the maximal
    ones from a specific view which results into losing some visual cues (Su et al.,
    [2015](#bib.bib125)). Yu et al. (Yu et al., [2018](#bib.bib153)) tried to incorporate
    the other views by a sum-pool operation. However, it performed worse than the
    max-pooling.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 识别/分类任务在计算机视觉中是基础性的。给定一个3D形状，目标是识别该形状所属的类别（3D分类），或者给定一个3D场景，识别场景中的不同3D形状及其位置（3D识别和定位）。目前，利用深度神经网络（DNNs）进行3D物体识别/分类的研究十分活跃。现有的方法可以根据其输入来学习任务，如补充材料中的表1和表2所示。多视角方法通过对每个视角应用2D
    CNN后，使用学习到的特征进行分类。通常，多视角方法的表现优于其他方法；然而，仍然存在一些未解决的问题。例如，近期的多视角CNN（MVCNN）（Su等，[2015](#bib.bib125)）对每个视角的特征应用了最大池化操作，以生成代表3D物体的全局特征。最大池化忽略了非最大激活值，仅保留来自特定视角的最大值，从而导致丢失一些视觉线索（Su等，[2015](#bib.bib125)）。Yu等（Yu等，[2018](#bib.bib153)）尝试通过求和池化操作来结合其他视角。然而，其效果不如最大池化。
- en: Considering the recognition and classification tasks, (Wang et al., [2017b](#bib.bib135))
    improved the discrimination between objects by applying a recurrent clustering
    and pooling strategy that increases the likelihood of variations in the aggregated
    activation output of the multi-views. Their objective was to capture the subtle
    changes in the activation function space. Similarly, $GIFT$ (Bai et al., [2016](#bib.bib10))
    extracts the features from each view but does not apply any pooling. It matches
    views to find the similarity between two 3D objects. It counts the best matched
    views only, however the greedy selection of the best matched view may discard
    useful information. Another framework called Group View CNN (GVCNN) (Feng et al.,
    [2018](#bib.bib46)) contains a hierarchical architecture of content descriptions
    from the view level, group of views level and the shape level. The framework defines
    the groups based on their discrimination power and the weights are adapted accordingly.
    Another very recent work (Yu et al., [2018](#bib.bib153)) proposes a Multi-view
    Harmonized Bilinear Network (MHBN) to improve the similarity level between two
    3D objects by utilizing patch features rather than view features.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到识别和分类任务，（王等人，[2017b](#bib.bib135)）通过应用循环聚类和汇聚策略提高了对象之间的区分度，增加了多视角聚合激活输出的变化可能性。他们的目标是捕捉激活函数空间中的细微变化。类似地，$GIFT$（白等人，[2016](#bib.bib10)）从每个视角提取特征，但不应用任何汇聚。它匹配视角以找到两个三维对象之间的相似性。它只计算最佳匹配的视角，然而贪婪地选择最佳匹配视角可能会丢弃有用信息。另一个名为Group
    View CNN（GVCNN）的框架（冯等人，[2018](#bib.bib46)）包含了从视角级别、视角组级别到形状级别的内容描述的分层体系结构。该框架根据它们的区分能力定义组，并相应地调整权重。另一项非常新的工作（于等人，[2018](#bib.bib153)）提出了一个名为Multi-view
    Harmonized Bilinear Network（MHBN）的方法，通过利用补丁特征而非视角特征来提高两个三维对象之间的相似性水平。
- en: Volumetric approaches classify 3D objects by using directly the shape voxels.
    They mitigate the challenge of having orderless points by voxelizing the input
    point cloud like 3D ShapeNets (Wu et al., [2015b](#bib.bib144)), volumetric CNNs (Qi
    et al., [2016b](#bib.bib101)), $OctNet$ (Riegler et al., [2017](#bib.bib105))
    and VRN Ensemble (Brock et al., [2016](#bib.bib16)). Although VRN Ensemble outperforms
    the multi-view methods, this performance is thanks to the model ensemble and their
    advanced base model as it ensembles five ResNet models and one Inception model
    while most of the existing multi-view methods rely on a single VGG-M model. Generally,
    these methods are not as accurate as the multi-view methods. Due to data sparsity
    and heavy 3D convolution computations, they are heavily constrained by their resolution.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 体积方法通过直接使用形状体素来对三维对象进行分类。它们通过对输入点云进行体素化来缓解无序点的挑战，例如3D ShapeNets（吴等人，[2015b](#bib.bib144)）、体积CNNs（齐等人，[2016b](#bib.bib101)）、$OctNet$（Riegler等人，[2017](#bib.bib105)）和VRN
    Ensemble（布罗克等人，[2016](#bib.bib16)）。尽管VRN Ensemble优于多视角方法，这种性能要归功于模型集成及其先进的基础模型，因为它集成了五个ResNet模型和一个Inception模型，而大多数现有的多视角方法则依赖于单个VGG-M模型。总体而言，这些方法的准确性不如多视角方法。由于数据稀疏性和复杂的三维卷积计算，它们受到分辨率的严重限制。
- en: $Pointset$ (Non-Euclidean) approaches classify directly the unordered point
    sets in order to address the sparsity problem found in volumetric methods as proposed
    in $PointNet$ (Qi et al., [2017a](#bib.bib100)). For each point, $PointNet$ learns
    a spatial encoding and aggregates all the features to a global representation.
    PointNet++ (Qi et al., [2017b](#bib.bib102)) improves $PointNet$ by utilizing
    local structures formed by the metric space. The points are partitioned into local
    regions that overlap by the distance metric of their space. The features are then
    extracted in a hierarchical fine to coarse process. At the same time of $PointNet$,
    Kd-networks (Klokov and Lempitsky, [2017](#bib.bib75)) were proposed, it recognizes
    3D models by performing multiplicative transformations and sharing their parameters
    given the point clouds subdivisions imposed by kd-trees.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: $Pointset$（非欧几里得）方法直接分类无序点集，以解决体积方法中发现的稀疏性问题，正如$PointNet$（齐等人，[2017a](#bib.bib100)）所提出的那样。对于每个点，$PointNet$学习空间编码并将所有特征聚合到全局表示中。PointNet++（齐等人，[2017b](#bib.bib102)）通过利用由度量空间形成的局部结构来改进$PointNet$。点被划分为重叠的局部区域，这些区域的距离度量形成了分层的细到粗的过程中提取特征。与$PointNet$同时提出的Kd-networks（Klokov和Lempitsky，[2017](#bib.bib75)）通过执行乘法变换并共享其参数来识别三维模型，这些模型由kd树强制实施的点云细分决定。
- en: Given any of the existing input modalities, a set of descriptors can be directly
    learned. The 3D representations can be also re-defined as a set of 2D geometric
    projections then a set of descriptors will be extracted. There is a direction
    to make the best out of all worlds by combining different input modalities for
    efficient feature extraction. A good example is the work of (Bu et al., [2017](#bib.bib27))
    where their scheme consists of a view-based feature learning, geometry-based feature
    learning from volumetric representations and a modality feature fusion in which
    the aforementioned learnt features were associated through a Restricted Boltzman
    Machine (RBM).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '给定任何现有的输入模式，可以直接学习一组描述符。3D 表示也可以重新定义为一组 2D 几何投影，然后提取一组描述符。有一种方向是通过结合不同的输入模式来充分利用所有领域，以实现高效的特征提取。一个好的例子是
    (Bu et al., [2017](#bib.bib27)) 的工作，其中他们的方案包括基于视图的特征学习、基于几何的特征学习从体积表示中以及通过限制玻尔兹曼机
    (RBM) 关联的模式特征融合。 '
- en: A comprehensive comparison between the recent state-of-the-art-methods is given
    in Table 1 in the supplementary material and compared based on experiments on
    $ModelNet10$ and $ModelNet40$ datasets in Fig. [2](#S1.F2 "Figure 2 ‣ 1.2.1\.
    3D recognition/classification ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis
    and discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations")
    and Fig. [3](#S1.F3 "Figure 3 ‣ 1.2.1\. 3D recognition/classification ‣ 1.2\.
    3D Computer vision tasks ‣ 1\. Analysis and discussions ‣ A survey on Deep Learning
    Advances on Different 3D Data Representations") respectively. Results shown in
    both figures highlight the power of the multi-view techniques achieving the state-of-the-art
    on the classification task on both datasets, $ModelNet10$ (98.46%) and $ModelNet40$
    (97.37%). Also, it shows the competition between such methods and volumetric methods
    such as Brock et al. (Brock et al., [2016](#bib.bib16)) achieving very competitive
    results to the multi-view ($ModelNet10$ (97.14%) and $ModelNet40$ (95.54%)), but
    with a more complex architecture and a serious need for data augmentation due
    to the complexity of the model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在补充材料的表 1 中提供了近期最先进方法的全面比较，并在图 [2](#S1.F2 "Figure 2 ‣ 1.2.1\. 3D recognition/classification
    ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis and discussions ‣ A survey on
    Deep Learning Advances on Different 3D Data Representations") 和图 [3](#S1.F3 "Figure
    3 ‣ 1.2.1\. 3D recognition/classification ‣ 1.2\. 3D Computer vision tasks ‣ 1\.
    Analysis and discussions ‣ A survey on Deep Learning Advances on Different 3D
    Data Representations") 中基于 $ModelNet10$ 和 $ModelNet40$ 数据集的实验进行了比较。两图中的结果突出显示了多视角技术在两个数据集上（$ModelNet10$（98.46%）和
    $ModelNet40$（97.37%））的分类任务上实现了最先进水平的能力。同时，它展示了这些方法与体积方法如 Brock 等（Brock et al.,
    [2016](#bib.bib16)）之间的竞争关系，后者在多视角方法（$ModelNet10$（97.14%）和 $ModelNet40$（95.54%））中取得了非常有竞争力的结果，但由于模型复杂性，架构更为复杂且需要大量的数据增强。
- en: '![Refer to caption](img/a7bbb50b6d1256dd2f11d133c555005f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a7bbb50b6d1256dd2f11d133c555005f.png)'
- en: Figure 2\. Class accuracy improvements on ModelNet10 dataset for classification/recognition
    tasks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 在 ModelNet10 数据集上进行分类/识别任务的类别准确率改进。
- en: '![Refer to caption](img/d92ead8eaaa8160755c8baed3ab8e959.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d92ead8eaaa8160755c8baed3ab8e959.png)'
- en: Figure 3\. Class accuracy improvements on ModelNet40 dataset for classification/recognition
    tasks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 在 ModelNet40 数据集上进行分类/识别任务的类别准确率改进。
- en: 1.2.2\. 3D retrieval
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.2\. 3D 检索
- en: 3D object retrieval is another fundamental task in shape analysis. The target
    here is to find the most similar 3D object from a database to match with the tested
    one. However, in the literature, most DNNs focus on leveraging the discrimination
    strength of these networks for the classification and recognition tasks. There
    are less DNNs specifically designed for 3D shape retrieval, see Table 2 in the
    supplementary material. Conceptually, the input processing is similar to the classification
    and recognition tasks. Generally, 3D object retrieval approaches are similar to
    other image or object retrieval methods where several loss functions are trained
    to learn an embedding space to get elements closer to each other. A very recent
    work (He et al., [2018](#bib.bib61)) train the center loss (Wen et al., [2016](#bib.bib140))
    and triplet loss (Schroff et al., [2015](#bib.bib113)) specifically for the distance
    measure which superpasses the state-of-the-art on $ModelNet40$ and $ShapeNet$.
    Multi-view methods usually outperform other methods in terms of retrieval accuracy.
    A comprehensive list of the recent 3D object retrieval state-of-the-art methods
    is given in Table 2 in the supplementary material.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 对象检索是形状分析中的另一个基础任务。目标是从数据库中找到最相似的 3D 对象，以匹配被测试的对象。然而，在文献中，大多数深度神经网络（DNN）专注于利用这些网络的判别能力进行分类和识别任务。专门为
    3D 形状检索设计的 DNN 较少，见附录中的表 2。从概念上讲，输入处理类似于分类和识别任务。通常，3D 对象检索方法类似于其他图像或对象检索方法，其中训练了几个损失函数以学习嵌入空间，使元素彼此接近。一个非常近期的工作（He
    等，[2018](#bib.bib61)）专门训练了中心损失（Wen 等，[2016](#bib.bib140)）和三元组损失（Schroff 等，[2015](#bib.bib113)）来进行距离度量，超越了
    $ModelNet40$ 和 $ShapeNet$ 上的现有技术水平。多视角方法通常在检索准确性方面优于其他方法。附录中的表 2 提供了最近的 3D 对象检索最先进方法的全面列表。
- en: 1.2.3\. 3D Correspondence
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.3\. 3D 对应
- en: The goal of 3D correspondence is to predict the mapping between a set of vertices
    of a test mesh and a reference or template mesh. There are two types of correspondences;
    sparse and dense correspondence. Sparse correspondence means that only a subset
    of the vertices of the test mesh are mapped to the reference mesh. However, in
    dense correspondence, all the vertices of the test mesh are mapped to the reference
    mesh.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 对应的目标是预测测试网格的一组顶点与参考或模板网格之间的映射。有两种类型的对应：稀疏对应和密集对应。稀疏对应意味着只有测试网格的部分顶点被映射到参考网格。然而，在密集对应中，测试网格的所有顶点都被映射到参考网格。
- en: Several works (Fey et al., [2017](#bib.bib47); Verma et al., [2018](#bib.bib133))
    perform 3D shape correspondence on the registered meshes of FAUST dataset (Bogo
    et al., [2014](#bib.bib14)). Only registered meshes of the dataset are considered
    because they are in dense point-to-point correspondence, providing the ground
    truth. There are in total 100 registered meshes of 10 people in 10 different poses
    each. Since the meshes are registrations of a template mesh, the number of vertices
    is fixed and the connectivity pattern is identical. In previous works, the task
    is cast as a classification problem. The model has to map $N$ input vertices to
    the corresponding vertices on the template mesh. This is achieved with a one-hot
    encoding of the target vertex for each input vertex.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 几项研究（Fey 等，[2017](#bib.bib47)；Verma 等，[2018](#bib.bib133)）在 FAUST 数据集（Bogo 等，[2014](#bib.bib14)）的注册网格上执行
    3D 形状对应。仅考虑数据集中的注册网格，因为它们处于密集的点对点对应中，提供了真实的基础数据。数据集中总共有 100 个注册网格，来自 10 个人，每个人有
    10 种不同姿势。由于这些网格是模板网格的注册，因此顶点的数量是固定的，连接模式是相同的。在以前的研究中，这一任务被设定为分类问题。模型必须将 $N$ 个输入顶点映射到模板网格上的对应顶点。这通过对每个输入顶点的目标顶点进行一热编码来实现。
- en: In the reported experiments of (Fey et al., [2017](#bib.bib47)), FAUST dataset
    is divided into 80 meshes for training and 20 for testing. was divided into $80$
    and $20$ meshes for training and testing respectively. The results show very high
    accuracy in the correspondence prediction. Those results are very accurate, but
    seem to mostly reflect the simplicity of the experiment. In fact, the meshes of
    the dataset all have the same topology, i.e., the vertices are in the same order
    and the connectivity pattern is the same. Moreover, because of the registration,
    the vertices are already in dense one-to-one correspondence. There is thus no
    ambiguity in the possible assignments with neighboring vertices. This makes the
    correspondence task simple because the mapping from input vertices to ouput vertices
    is a trivial copy rather than learning the actual topology of the 3D mesh.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 (Fey 等，[2017](#bib.bib47)) 报告的实验中，FAUST 数据集被分为 80 个用于训练的网格和 20 个用于测试的网格。结果显示了非常高的对应预测准确性。这些结果非常准确，但似乎主要反映了实验的简单性。实际上，数据集的网格都具有相同的拓扑结构，即，顶点的顺序相同，连接模式也相同。此外，由于已经进行了配准，顶点已经是密集的一对一对应。因此，在邻近顶点的可能分配中没有歧义。这使得对应任务变得简单，因为从输入顶点到输出顶点的映射只是一个简单的拷贝，而不是学习
    3D 网格的实际拓扑结构。
- en: 'For assessing the performance of a sample state-of-the-art technique for the
    correspondnce task, we experimented on SplineCNN (Fey et al., [2017](#bib.bib47)).
    We test the pre-trained model of SplineCNN on different test data to validate
    the performance under different conditions. We have three different experiments:
    1) Data with the same topology varying in the shape, pose and geometry generated
    from the Skinned Multi-Person Linear Model (SMPL) model (Loper et al., [2015](#bib.bib89)).
    2) Test data from FAUST dataset with synthetic noise of different intensities.
    3) Various unclothed full human body scans from the 3DBodyTex dataset (Saint et al.,
    [2018](#bib.bib109)) and some additional clothed scans acquired in a similar setup.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为评估用于对应任务的最新技术的性能，我们在 SplineCNN（Fey 等，[2017](#bib.bib47)）上进行了实验。我们在不同的测试数据上测试了
    SplineCNN 的预训练模型，以验证其在不同条件下的性能。我们进行了三种不同的实验：1) 数据具有相同的拓扑结构，但形状、姿势和几何特征不同，这些数据由
    Skinned Multi-Person Linear Model (SMPL) 模型（Loper 等，[2015](#bib.bib89)）生成。2) 从
    FAUST 数据集中获取的测试数据，带有不同强度的合成噪声。3) 来自 3DBodyTex 数据集（Saint 等，[2018](#bib.bib109)）的各种裸体全身扫描，以及在类似设置下获取的一些额外着装扫描。
- en: '![Refer to caption](img/01592b4df2bd6ec7e94389df11e537a7.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/01592b4df2bd6ec7e94389df11e537a7.png)'
- en: Figure 4\. Correspondence results of testing the pre-trained model of SplineCNN (Fey
    et al., [2017](#bib.bib47)) on meshes generated from SMPL model (Loper et al.,
    [2015](#bib.bib89))
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 在从 SMPL 模型（Loper 等，[2015](#bib.bib89)）生成的网格上测试 SplineCNN 预训练模型（Fey 等，[2017](#bib.bib47)）的对应结果
- en: In the fist testcase, we generate different 3D meshes varying in both the shape
    and pose considering both genders, male and female, using the SMPL model (Loper
    et al., [2015](#bib.bib89)). The generated meshes have the same topology as the
    FAUST dataset, i.e., the same number of verticies and connectivity. The generated
    shapes vary in size from thin to fat which in turn varies the geometry of the
    generated subjects. We have used a pre-trained model such as SplineCNN to test
    he generated data for the correspondence task. Each vertex in the test subject
    is assigned to a specific vertex in the reference mesh and then each vertex is
    assigned a specific color to visualize the results.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个测试案例中，我们使用 SMPL 模型（Loper 等，[2015](#bib.bib89)）生成了形状和姿势都不同的 3D 网格，考虑了男性和女性两种性别。生成的网格具有与
    FAUST 数据集相同的拓扑结构，即，顶点数量和连接方式相同。生成的形状在从瘦到胖的范围内变化，从而改变了生成主体的几何特征。我们使用了诸如 SplineCNN
    这样的预训练模型来测试生成的数据以完成对应任务。测试对象中的每个顶点都被分配到参考网格中的一个特定顶点，然后每个顶点都被分配一个特定的颜色以可视化结果。
- en: In Fig. [4](#S1.F4 "Figure 4 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer
    vision tasks ‣ 1\. Analysis and discussions ‣ A survey on Deep Learning Advances
    on Different 3D Data Representations"), the reference mesh is depicted on the
    left and all the correspondence results on the generated test data are on the
    right. As shown in Fig. [4](#S1.F4 "Figure 4 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\.
    3D Computer vision tasks ‣ 1\. Analysis and discussions ‣ A survey on Deep Learning
    Advances on Different 3D Data Representations"), the model is not able to generalize
    to new unseen but very similar data and does not report as good results as those
    reported in (Fey et al., [2017](#bib.bib47)). This performance is noticeable even
    on simple poses like the $T$ pose, shown in the first and fourth columns in Fig. [4](#S1.F4
    "Figure 4 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis
    and discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations"),
    which is actually included in the FAUST dataset. Also, the results show that varying
    the size of the shape has the largest effect on the results more than varying
    the pose, as there are more wrongly colored faces in comparison with the reference
    mesh. However, changing the pose or the shape (male or female) does not affect
    the performance much.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [4](#S1.F4 "Figure 4 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer vision
    tasks ‣ 1\. Analysis and discussions ‣ A survey on Deep Learning Advances on Different
    3D Data Representations") 所示，左侧显示了参考网格，右侧显示了生成的测试数据上的所有对应结果。正如图 [4](#S1.F4 "Figure
    4 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis
    and discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations")
    中所示，该模型无法推广到新的但非常相似的未见数据，并且没有像在 (Fey 等，[2017](#bib.bib47)) 中报告的那样良好的结果。即使在像第一列和第四列中显示的
    $T$ 姿势这样简单的姿势上，这种性能也是显而易见的，该姿势实际上包含在 FAUST 数据集中。此外，结果显示，与参考网格相比，改变形状的大小对结果的影响最大，因为有更多的错误着色面。然而，改变姿势或形状（男性或女性）并不会对性能产生太大影响。
- en: '![Refer to caption](img/65ba3f815162c6cb3402dff7e08264bd.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/65ba3f815162c6cb3402dff7e08264bd.png)'
- en: Figure 5\. The correspondence results of testing the pre-trained model of SplineCNN (Fey
    et al., [2017](#bib.bib47)) on FAUST data after adding 4 different levels of synthetic
    noise. The first row is FAUST data without adding any noise. From the second row
    till the fifth row, the noise is added gradually from the lowest to the highest.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 在给 SplineCNN 预训练模型测试 FAUST 数据后，添加了四个不同级别的合成噪声的对应结果（Fey 等，[2017](#bib.bib47)）。第一行是未添加任何噪声的
    FAUST 数据。从第二行到第五行，噪声逐渐从最低到最高添加。
- en: The registered scans of the FAUST dataset are smooth, clean and noise-free.
    All the experiments reported in (Fey et al., [2017](#bib.bib47)) are on the clean
    FAUST data. These experiments suggest that it is important to investigate how
    robust to noise a given model is.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: FAUST 数据集的注册扫描平滑、干净且无噪声。所有在 (Fey 等，[2017](#bib.bib47)) 中报告的实验都是在干净的 FAUST 数据上进行的。这些实验表明，调查给定模型对噪声的鲁棒性是很重要的。
- en: In another experiment, we test on four meshes from the FAUST dataset after adding
    synthetic noise of different levels as depicted in Fig. [5](#S1.F5 "Figure 5 ‣
    1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis and
    discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations").
    The initial row of meshes shows four of the original test meshes of FAUST dataset
    without adding any noise. The following rows show different levels of added noise
    from level 1 (lowest noise) to level 4 (highest). The mesh shown on the left represents
    the reference mesh and the set of meshes on the right show the correspondence
    results. For level 1 noise (second row), the geometry of meshes has barely changed
    with respect to the original meshes (first row), except for some noise on the
    faces. However, the correspondence results show that there are some erroneous
    areas in the arms for the ‘hands up pose’ (last pose from the left), which should
    not be the case since the arms did not change. Moreover, the more noisy the mesh
    is, the more the correspondence results get erroneous in terms of the wrongly
    labelled vertices and this is clear in terms if the difference in the color map
    between the reference mesh and the test meshes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个实验中，我们在添加不同级别的合成噪声后，对FAUST数据集中的四个网格进行了测试，如图[5](#S1.F5 "Figure 5 ‣ 1.2.3\.
    3D Correspondence ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis and discussions
    ‣ A survey on Deep Learning Advances on Different 3D Data Representations")所示。初始行显示了四个原始测试网格，未添加任何噪声。接下来的行显示了从级别1（最低噪声）到级别4（最高噪声）的不同噪声级别。左侧的网格表示参考网格，右侧的网格集合显示了对应结果。对于级别1噪声（第二行），网格的几何形状几乎没有相对于原始网格（第一行）发生变化，只是脸部有一些噪声。然而，对应结果显示在‘举手姿势’（最左侧姿势）中，手臂有一些错误区域，这不应该发生，因为手臂没有变化。此外，网格的噪声越多，对应结果在错误标记顶点方面的错误就越多，这在参考网格与测试网格之间的颜色图差异上表现得很明显。
- en: '![Refer to caption](img/6969425be4a1b71bdae1517f7731f9d5.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6969425be4a1b71bdae1517f7731f9d5.png)'
- en: Figure 6\. Testing the pre-trained model of SplineCNN (Fey et al., [2017](#bib.bib47))
    on some clothed and unclothed scans from the 3DBodyTexdata (Saint et al., [2018](#bib.bib109))
    dataset.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 在3DBodyTexdata（Saint et al., [2018](#bib.bib109)）数据集中对一些穿衣和未穿衣的扫描进行预训练模型SplineCNN（Fey
    et al., [2017](#bib.bib47)）的测试。
- en: Real mesh data is more challenging because it is more noisy, incomplete and
    less regular with variable sampling. Because of the noise, the vertices of an
    input mesh do not have an exact correspondence with the template mesh, making
    the correspondence ambiguous. Also, the connectivity pattern usually varies from
    mesh to another even if re-sampled to the same number of vertices. Possible holes
    in the data make it hard to find the exact correspondence to the reference mesh.
    Variability in the sampling requires robust methods that can adapt to different
    scales and handle different levels of features. Recently, Monte Carlo Convolution (Hermosilla
    et al., [2018](#bib.bib63)) is proposed to handle sampling at different levels
    for point clouds learning. The authors in this work propose to represent the convolution
    kernel as a “Multi-Layer Perceptron (MLP)” where the convolution is formulated
    as a Monte Carlo intergration problem. This notion enables to combine information
    of the point cloud from multiple samplings at different levels, where Poisson
    disk sampling is used as a scalable means of hierarchical point cloud learning.
    This showed robustness even when all the training data is non-uniformly samples.
    This method achieves relatively better results compared to PointNet++ (Qi et al.,
    [2017b](#bib.bib102)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 真实网格数据更具挑战性，因为它更嘈杂、不完整且不规则，采样变量大。由于噪声，输入网格的顶点与模板网格之间没有确切的对应关系，导致对应关系模糊。此外，即使重新采样到相同数量的顶点，网格的连接模式通常也会有所不同。数据中的可能空洞使得找到与参考网格的确切对应关系变得困难。采样的变化需要能够适应不同尺度并处理不同特征级别的稳健方法。最近，Monte
    Carlo Convolution（Hermosilla et al., [2018](#bib.bib63)）被提出用于处理点云学习中的不同级别的采样。本文的作者提出将卷积核表示为“多层感知机（MLP）”，其中卷积被公式化为Monte
    Carlo积分问题。这一概念使得可以将来自不同级别的多次采样的点云信息结合起来，其中使用了Poisson圆盘采样作为分层点云学习的可扩展手段。这在所有训练数据非均匀采样的情况下表现出了鲁棒性。与PointNet++（Qi
    et al., [2017b](#bib.bib102)）相比，这种方法实现了相对更好的结果。
- en: To test for the robustness of the SplineCNN model with respect to all of these
    challenges, we test on real-world clothed and unclothed scans from the 3DBodyTex
    dataset (Saint et al., [2018](#bib.bib109)) as shown in Fig. [6](#S1.F6 "Figure
    6 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis
    and discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations").
    We down-sample 10 meshes of the 3DBodyTex dataset to 6890 vertices to be equivalent
    to the number of vertices of FAUST dataset. However, the down-sampled data have
    different connectivity pattern from the FAUST data, which makes the task of the
    correspondence harder. As depicted in Fig. [6](#S1.F6 "Figure 6 ‣ 1.2.3\. 3D Correspondence
    ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis and discussions ‣ A survey on
    Deep Learning Advances on Different 3D Data Representations"), SplineCNN model
    is not able to handle the 3DBodyTex data which is of different topology. This
    results in highly erroneous correspondence results where the resulting color map
    on the test data is very far from the color map of the reference mesh.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试SplineCNN模型对所有这些挑战的鲁棒性，我们在3DBodyTex数据集中的真实穿衣和未穿衣扫描上进行了测试（Saint等，[2018](#bib.bib109)），如图[6](#S1.F6
    "Figure 6 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis
    and discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations")所示。我们将3DBodyTex数据集中的10个网格降采样至6890个顶点，以便与FAUST数据集的顶点数量相当。然而，降采样的数据与FAUST数据具有不同的连接模式，这使得对应任务变得更加困难。如图[6](#S1.F6
    "Figure 6 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis
    and discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations")所示，SplineCNN模型无法处理具有不同拓扑结构的3DBodyTex数据。这导致了高度错误的对应结果，其中测试数据上的颜色图与参考网格的颜色图相差甚远。
- en: 2\. Conclusion
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 结论
- en: The ongoing evolution of scanning devices caused a huge increase in the amount
    of 3D data available in the 3D computer vision research community. This opens
    the door for new opportunities investigating the properties of different 3D objects
    and learning their geometric properties despite challenges imposed by the data
    itself. Fortunately, DL techniques revolutionized the learning performance on
    various 2D computer vision tasks which encouraged the 3D research community to
    adopt the same path. However, extending 2D DL to 3D data is not a straightforward
    tasks depending on the data representation itself and the task at hand. In this
    work, we categorized the 3D data representations based on their internal structure
    to Euclidean and non-Euclidean representations. Following the same classification,
    we discussed different DL techniques applied to 3D data based on the data representation
    and how the internal structure of the data is treated. In the Euclidean DL family,
    the reported results in the literature shows that multi-view representations achieve
    the state-of-the-art classification performance and outperforms other methods
    that exploit the full geometry of the 3D shape (i.e., volumetric methods) providing
    a more efficient way to learn the properties of 3D shapes. On the other branch
    of the non-Euclidean DL techniques, results are reported near perfect on the correspondence
    task in various recent papers such as (Fey et al., [2017](#bib.bib47); Verma et al.,
    [2018](#bib.bib133)). These correspondence experiments were carried on clean,
    smooth and ideal data. In this paper, state-of-the-art SplineCNN (Fey et al.,
    [2017](#bib.bib47)) method have been tested over different dataset, under different
    different conditions that emulate the real-world scenarios. The obtained results
    showed that, even with the same topology and similar poses, this model does not
    generalize to new or noisy data. There is clearly a need to further investigate
    ways to improve the robustness of 3D DL models and ensure their generalization
    to real data while exploiting the different existent representations of 3D data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描设备的持续演变导致了3D计算机视觉研究领域中3D数据量的大幅增加。这为探索不同3D物体的属性和学习其几何特性打开了新的机会，尽管数据本身带来了挑战。幸运的是，深度学习（DL）技术在各种2D计算机视觉任务上的表现发生了革命性变化，这鼓励了3D研究社区采用相同的方法。然而，将2D
    DL扩展到3D数据并不是一项简单的任务，这取决于数据表示本身和具体任务。在这项工作中，我们根据其内部结构将3D数据表示分类为欧几里得和非欧几里得表示。按照相同的分类，我们讨论了应用于3D数据的不同DL技术，这些技术依据数据表示以及数据的内部结构处理方法。在欧几里得DL家族中，文献中报告的结果表明，多视图表示达到了最先进的分类性能，并优于其他利用3D形状全几何信息（即体积方法）的技术，提供了更高效的3D形状属性学习方式。另一方面，在非欧几里得DL技术的分支中，最近的一些论文（例如（Fey
    et al., [2017](#bib.bib47); Verma et al., [2018](#bib.bib133)）报告了在对应任务上的近乎完美的结果。这些对应实验是在干净、光滑和理想的数据上进行的。在本文中，最先进的SplineCNN（Fey
    et al., [2017](#bib.bib47)）方法在不同的数据集和不同条件下进行了测试，这些条件模拟了实际世界的场景。结果显示，即使在相同的拓扑和类似的姿态下，该模型也无法对新数据或噪声数据进行有效泛化。显然，需要进一步研究改进3D
    DL模型的鲁棒性，并确保其对实际数据的泛化，同时利用不同的3D数据表示。
- en: 3\. Acknowledgement
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 致谢
- en: This work has been funded by FNR project IDform under the agreement CPPP17/IS/11643091/IDform/
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了FNR项目IDform的资助，协议编号CPP17/IS/11643091/IDform/
- en: Aouada, Luxembourg and by Artec Europe SARL.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Aouada, Luxembourg 和 Artec Europe SARL。
- en: References
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Abdul-Rahman and Pilouk (2007) Alias Abdul-Rahman and Morakot Pilouk. 2007.
    *Spatial data modelling for 3D GIS*. Springer Science & Business Media.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdul-Rahman 和 Pilouk（2007）Alias Abdul-Rahman 和 Morakot Pilouk. 2007. *3D GIS空间数据建模*。Springer
    Science & Business Media。
- en: Afzal et al. (2014) H. Afzal, D. Aouada, D. Font, B. Mirbach, and B. Ottersten.
    2014. RGB-D Multi-view System Calibration for Full 3D Scene Reconstruction. In
    *2014 22nd International Conference on Pattern Recognition*. IEEE, 2459–2464.
    [https://doi.org/10.1109/ICPR.2014.425](https://doi.org/10.1109/ICPR.2014.425)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Afzal等人（2014）H. Afzal, D. Aouada, D. Font, B. Mirbach, 和 B. Ottersten. 2014.
    RGB-D 多视图系统标定用于全3D场景重建。*2014年第22届国际模式识别大会*。IEEE，2459–2464。 [https://doi.org/10.1109/ICPR.2014.425](https://doi.org/10.1109/ICPR.2014.425)
- en: Aldoma et al. (2012) Aitor Aldoma, Federico Tombari, Radu Bogdan Rusu, and Markus
    Vincze. 2012. OUR-CVFH–oriented, unique and repeatable clustered viewpoint feature
    histogram for object recognition and 6DOF pose estimation. In *Joint DAGM (German
    Association for Pattern Recognition) and OAGM Symposium*. Springer, 113–122.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aldoma 等（2012） Aitor Aldoma, Federico Tombari, Radu Bogdan Rusu, 和 Markus Vincze.
    2012. OUR-CVFH–定向、唯一且可重复的集群视点特征直方图用于对象识别和 6DOF 位姿估计。载于 *联合 DAGM（德国模式识别协会）和 OAGM
    研讨会*。Springer，113–122。
- en: Alexandre (2016) Luís A Alexandre. 2016. 3D object recognition using convolutional
    neural networks with transfer learning between input channels. In *Intelligent
    Autonomous Systems 13*. Springer, 889–898.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexandre（2016） Luís A Alexandre. 2016. 使用卷积神经网络和输入通道间的迁移学习进行 3D 对象识别。载于 *智能自主系统
    13*。Springer，889–898。
- en: Aouada et al. (2008) D. Aouada, D. W. Dreisigmeyer, and H. Krim. 2008. Geometric
    modeling of rigid and non-rigid 3D shapes using the global geodesic function.
    In *2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
    Workshops*. IEEE, 1–8. [https://doi.org/10.1109/CVPRW.2008.4563075](https://doi.org/10.1109/CVPRW.2008.4563075)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aouada 等（2008） D. Aouada, D. W. Dreisigmeyer, 和 H. Krim. 2008. 使用全局测地函数进行刚性和非刚性
    3D 形状的几何建模。载于 *2008 IEEE计算机学会计算机视觉与模式识别研讨会*。IEEE，1–8。 [https://doi.org/10.1109/CVPRW.2008.4563075](https://doi.org/10.1109/CVPRW.2008.4563075)
- en: Aouada et al. (2007) D. Aouada, S. Feng, and H. Krim. 2007. Statistical Analysis
    of the Global Geodesic Function for 3D Object Classification. In *2007 IEEE International
    Conference on Acoustics, Speech and Signal Processing - ICASSP ’07*, Vol. 1\.
    IEEE, I–645–I–648. [https://doi.org/10.1109/ICASSP.2007.365990](https://doi.org/10.1109/ICASSP.2007.365990)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aouada 等（2007） D. Aouada, S. Feng, 和 H. Krim. 2007. 全球测地函数的统计分析用于 3D 对象分类。载于
    *2007 IEEE国际声学、语音和信号处理会议 - ICASSP ’07*，第 1 卷。IEEE，I–645–I–648。 [https://doi.org/10.1109/ICASSP.2007.365990](https://doi.org/10.1109/ICASSP.2007.365990)
- en: Aouada and Krim (2010) D. Aouada and H. Krim. 2010. Squigraphs for Fine and
    Compact Modeling of 3-D Shapes. *IEEE Transactions on Image Processing* 19, 2
    (Feb 2010), 306–321. [https://doi.org/10.1109/TIP.2009.2034693](https://doi.org/10.1109/TIP.2009.2034693)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aouada 和 Krim（2010） D. Aouada 和 H. Krim. 2010. 用于精细和紧凑建模 3D 形状的 Squigraphs。*IEEE
    图像处理学报* 19，第 2 期（2010 年 2 月），306–321。 [https://doi.org/10.1109/TIP.2009.2034693](https://doi.org/10.1109/TIP.2009.2034693)
- en: 'Aubry et al. (2011) Mathieu Aubry, Ulrich Schlickewei, and Daniel Cremers.
    2011. The wave kernel signature: A quantum mechanical approach to shape analysis.
    In *Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference
    on*. IEEE, 1626–1633.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aubry 等（2011） Mathieu Aubry, Ulrich Schlickewei, 和 Daniel Cremers. 2011. 波动核签名：一种量子力学方法用于形状分析。载于
    *计算机视觉研讨会（ICCV Workshops），2011 IEEE国际会议*。IEEE，1626–1633。
- en: 'Bai et al. (2016) Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan
    Latecki. 2016. Gift: A real-time and scalable 3d shape search engine. In *Computer
    Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on*. IEEE, 5023–5032.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等（2016） Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, 和 Longin Jan
    Latecki. 2016. Gift：实时且可扩展的 3D 形状搜索引擎。载于 *计算机视觉与模式识别（CVPR），2016 IEEE会议*。IEEE，5023–5032。
- en: Battaglia et al. (2016) Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez
    Rezende, et al. 2016. Interaction networks for learning about objects, relations
    and physics. In *Advances in neural information processing systems*. arXiv, 4502–4510.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Battaglia 等（2016） Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez
    Rezende 等. 2016. 学习对象、关系和物理的交互网络。载于 *神经信息处理系统进展*。arXiv，4502–4510。
- en: Ben-Shabat et al. (2017) Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fischer.
    2017. 3D Point Cloud Classification and Segmentation using 3D Modified Fisher
    Vector Representation for Convolutional Neural Networks. *arXiv:1711.08241* abs/1711.08241
    (2017).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben-Shabat 等（2017） Yizhak Ben-Shabat, Michael Lindenbaum, 和 Anath Fischer. 2017.
    使用 3D 修改过的 Fisher 向量表示法进行 3D 点云分类和分割以适应卷积神经网络。*arXiv:1711.08241* abs/1711.08241（2017）。
- en: 'Berger (2013) Kai Berger. 2013. The role of rgb-d benchmark datasets: an overview.
    *arXiv preprint arXiv:1310.2053* (2013).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berger（2013） Kai Berger. 2013. RGB-D 基准数据集的角色：概述。*arXiv 预印本 arXiv:1310.2053*（2013）。
- en: 'Bogo et al. (2014) Federica Bogo, Javier Romero, Matthew Loper, and Michael J.
    Black. 2014. FAUST: Dataset and evaluation for 3D mesh registration. In *Proceedings
    IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*. IEEE, Piscataway,
    NJ, USA.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bogo 等（2014） Federica Bogo, Javier Romero, Matthew Loper, 和 Michael J. Black.
    2014. FAUST：3D 网格配准的数据集和评估。载于 *IEEE计算机视觉与模式识别会议（CVPR）*。IEEE，Piscataway, NJ, USA。
- en: Boscaini et al. (2016) Davide Boscaini, Jonathan Masci, Emanuele Rodolà, and
    Michael Bronstein. 2016. Learning shape correspondence with anisotropic convolutional
    neural networks. In *Advances in Neural Information Processing Systems*. NIPS,
    3189–3197.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boscaini 等 (2016) Davide Boscaini, Jonathan Masci, Emanuele Rodolà, 和 Michael
    Bronstein. 2016. 使用各向异性卷积神经网络学习形状对应。 在 *Advances in Neural Information Processing
    Systems*。NIPS, 3189–3197。
- en: Brock et al. (2016) Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston.
    2016. Generative and discriminative voxel modeling with convolutional neural networks.
    *arXiv preprint arXiv:1608.04236* (2016).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brock 等 (2016) Andrew Brock, Theodore Lim, James M Ritchie, 和 Nick Weston. 2016.
    使用卷积神经网络的生成和判别体素建模。*arXiv preprint arXiv:1608.04236* (2016)。
- en: Bronstein and Bronstein (2007) Alexander Bronstein and Michael Bronstein. 2007.
    [http://vision.mas.ecp.fr/Personnel/iasonas/descriptors.html](http://vision.mas.ecp.fr/Personnel/iasonas/descriptors.html)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bronstein 和 Bronstein (2007) Alexander Bronstein 和 Michael Bronstein. 2007.
    [http://vision.mas.ecp.fr/Personnel/iasonas/descriptors.html](http://vision.mas.ecp.fr/Personnel/iasonas/descriptors.html)
- en: Bronstein and Bronstein (2018) Alexander Bronstein and Michael Bronstein. 2018.
    Discrete geometry tutorial 1.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bronstein 和 Bronstein (2018) Alexander Bronstein 和 Michael Bronstein. 2018.
    离散几何教程 1。
- en: 'Bronstein et al. (2010) AM Bronstein, MM Bronstein, U Castellani, B Falcidieno,
    A Fusiello, A Godil, LJ Guibas, I Kokkinos, Zhouhui Lian, M Ovsjanikov, et al.
    2010. Shrec 2010: robust large-scale shape retrieval benchmark. *Proc. 3DOR* 5
    (2010), 4.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bronstein 等 (2010) AM Bronstein, MM Bronstein, U Castellani, B Falcidieno,
    A Fusiello, A Godil, LJ Guibas, I Kokkinos, Zhouhui Lian, M Ovsjanikov, 等. 2010.
    Shrec 2010: 鲁棒的大规模形状检索基准。*Proc. 3DOR* 5 (2010), 4。'
- en: Bronstein et al. (2008) Alexander M Bronstein, Michael M Bronstein, and Ron
    Kimmel. 2008. *Numerical geometry of non-rigid shapes*. Springer Science & Business
    Media.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bronstein 等 (2008) Alexander M Bronstein, Michael M Bronstein, 和 Ron Kimmel.
    2008. *非刚性形状的数值几何*。Springer Science & Business Media。
- en: 'Bronstein et al. (2017) Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur
    Szlam, and Pierre Vandergheynst. 2017. Geometric deep learning: going beyond euclidean
    data. *IEEE Signal Processing Magazine* 34, 4 (2017), 18–42.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bronstein 等 (2017) Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam,
    和 Pierre Vandergheynst. 2017. 几何深度学习：超越欧几里得数据。*IEEE Signal Processing Magazine*
    34, 4 (2017), 18–42。
- en: Bronstein and Kokkinos (2010) Michael M Bronstein and Iasonas Kokkinos. 2010.
    Scale-invariant heat kernel signatures for non-rigid shape recognition. In *Computer
    Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on*. IEEE, 1704–1711.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bronstein 和 Kokkinos (2010) Michael M Bronstein 和 Iasonas Kokkinos. 2010. 用于非刚性形状识别的尺度不变热核签名。在
    *Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on*。IEEE,
    1704–1711。
- en: Bruna and Li (2017) Joan Bruna and Xiang Li. 2017. Community detection with
    graph neural networks. *arXiv preprint arXiv:1705.08415* (2017).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruna 和 Li (2017) Joan Bruna 和 Xiang Li. 2017. 使用图神经网络进行社区检测。*arXiv preprint
    arXiv:1705.08415* (2017)。
- en: Bruna et al. (2013) Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun.
    2013. Spectral networks and locally connected networks on graphs. *arXiv preprint
    arXiv:1312.6203* (2013).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruna 等 (2013) Joan Bruna, Wojciech Zaremba, Arthur Szlam, 和 Yann LeCun. 2013.
    谱网络和图上的局部连接网络。*arXiv preprint arXiv:1312.6203* (2013)。
- en: Bu et al. (2015) Shuhui Bu, Pengcheng Han, Zhenbao Liu, Junwei Han, and Hongwei
    Lin. 2015. Local deep feature learning framework for 3D shape. *Computers & Graphics*
    46 (2015), 117–129.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bu 等 (2015) Shuhui Bu, Pengcheng Han, Zhenbao Liu, Junwei Han, 和 Hongwei Lin.
    2015. 用于三维形状的局部深度特征学习框架。*Computers & Graphics* 46 (2015), 117–129。
- en: Bu et al. (2014) Shuhui Bu, Zhenbao Liu, Junwei Han, Jun Wu, and Rongrong Ji.
    2014. Learning High-Level Feature by Deep Belief Networks for 3-D Model Retrieval
    and Recognition. *IEEE Transactions on Multimedia* 16 (2014), 2154–2167.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bu 等 (2014) Shuhui Bu, Zhenbao Liu, Junwei Han, Jun Wu, 和 Rongrong Ji. 2014.
    通过深度信念网络学习高层特征用于三维模型检索和识别。*IEEE Transactions on Multimedia* 16 (2014), 2154–2167。
- en: Bu et al. (2017) Shuhui Bu, Lei Wang, Pengcheng Han, Zhenbao Liu, and Ke Li.
    2017. 3D shape recognition and retrieval based on multi-modality deep learning.
    *Neurocomputing* 259 (2017), 183 – 193. Multimodal Media Data Understanding and
    Analytics.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bu 等 (2017) Shuhui Bu, Lei Wang, Pengcheng Han, Zhenbao Liu, 和 Ke Li. 2017.
    基于多模态深度学习的三维形状识别和检索。*Neurocomputing* 259 (2017), 183 – 193. 多模态媒体数据理解与分析。
- en: Cao et al. (2017) Zhangjie Cao, Qixing Huang, and Karthik Ramani. 2017. 3D Object
    Classification via Spherical Projections. *arXiv preprint arXiv:1712.04426* (2017).
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等 (2017) Zhangjie Cao, Qixing Huang, 和 Karthik Ramani. 2017. 通过球面投影进行三维物体分类。*arXiv
    preprint arXiv:1712.04426* (2017)。
- en: Chang et al. (2016) Michael B Chang, Tomer Ullman, Antonio Torralba, and Joshua B
    Tenenbaum. 2016. A compositional object-based approach to learning physical dynamics.
    *arXiv preprint arXiv:1612.00341* (2016).
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2015) Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz. 2015.
    Utd-mhad: A multimodal dataset for human action recognition utilizing a depth
    camera and a wearable inertial sensor. In *Image Processing (ICIP), 2015 IEEE
    International Conference on*. IEEE, 168–172.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2003) Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming Ouhyoung.
    2003. On visual similarity based 3D model retrieval. In *Computer graphics forum*,
    Vol. 22\. Wiley Online Library, 223–232.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coates and Ng (2011) Adam Coates and Andrew Y Ng. 2011. Selecting receptive
    fields in deep networks. In *Advances in Neural Information Processing Systems*.
    NIPS, 2528–2536.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cosmo et al. (2016) L Cosmo, E Rodolà, MM Bronstein, A Torsello, D Cremers,
    and Y Sahillioglu. 2016. SHREC’16: Partial matching of deformable shapes. *Proc.
    3DOR* 2, 9 (2016), 12.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Couprie et al. (2013) Camille Couprie, Clément Farabet, Laurent Najman, and
    Yann LeCun. 2013. Indoor semantic segmentation using depth information. *arXiv
    preprint arXiv:1301.3572* (2013).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2017) Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber,
    Thomas Funkhouser, and Matthias Nießner. 2017. Scannet: Richly-annotated 3d reconstructions
    of indoor scenes. In *Proc. IEEE Conf. on Computer Vision and Pattern Recognition
    (CVPR)*, Vol. 1.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daras and Axenopoulos (2010) Petros Daras and Apostolos Axenopoulos. 2010. A
    3D shape retrieval framework supporting multimodal queries. *International Journal
    of Computer Vision* 89, 2-3 (2010), 229–247.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defferrard et al. (2016) Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst.
    2016. Convolutional neural networks on graphs with fast localized spectral filtering.
    In *Advances in Neural Information Processing Systems*. NIPS, 3844–3852.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
    2009. ImageNet: A Large-Scale Hierarchical Image Database. In *CVPR09*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong (1996) Feng Dong. 1996. Three-dimensional models and applications in subsurface
    modeling. (1996).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duvenaud et al. (2015) David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre,
    Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional
    networks on graphs for learning molecular fingerprints. In *Advances in neural
    information processing systems*. NIPS, 2224–2232.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eitel et al. (2015) Andreas Eitel, Jost Tobias Springenberg, Luciano Spinello,
    Martin Riedmiller, and Wolfram Burgard. 2015. Multimodal deep learning for robust
    RGB-D object recognition. In *Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ
    International Conference on*. IEEE, 681–687.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Erdogmus and Marcel (2013) Nesli Erdogmus and Sebastien Marcel. 2013. Spoofing
    in 2D face recognition with 3D masks and anti-spoofing with Kinect. In *Biometrics:
    Theory, Applications and Systems (BTAS), 2013 IEEE Sixth International Conference
    on*. IEEE, 1–6.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erdogmus 和 Marcel (2013) Nesli Erdogmus 和 Sebastien Marcel. 2013. 用3D面具进行2D人脸识别的欺骗和使用Kinect进行反欺骗。在*生物识别：理论、应用和系统（BTAS），2013年IEEE第六届国际会议*中。IEEE,
    1–6.
- en: Fanelli et al. (2011) Gabriele Fanelli, Thibaut Weise, Juergen Gall, and Luc
    Van Gool. 2011. Real time head pose estimation from consumer depth cameras. In
    *Joint Pattern Recognition Symposium*. Springer, 101–110.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fanelli 等 (2011) Gabriele Fanelli, Thibaut Weise, Juergen Gall, 和 Luc Van Gool.
    2011. 来自消费者深度摄像头的实时头部姿态估计。在*联合模式识别研讨会*中。Springer, 101–110.
- en: Farabet et al. (2013) Clement Farabet, Camille Couprie, Laurent Najman, and
    Yann LeCun. 2013. Learning hierarchical features for scene labeling. *IEEE transactions
    on pattern analysis and machine intelligence* 35, 8 (2013), 1915–1929.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Farabet 等 (2013) Clement Farabet, Camille Couprie, Laurent Najman, 和 Yann LeCun.
    2013. 用于场景标记的学习分层特征。*IEEE模式分析与机器智能交易* 35, 8 (2013), 1915–1929.
- en: Feng et al. (2016) Jie Feng, Yan Wang, and Shih-Fu Chang. 2016. 3D shape retrieval
    using a single depth image from low-cost sensors. In *Applications of Computer
    Vision (WACV), 2016 IEEE Winter Conference on*. IEEE, 1–9.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等 (2016) Jie Feng, Yan Wang, 和 Shih-Fu Chang. 2016. 使用单一深度图像进行3D形状检索。在*应用计算机视觉会议（WACV）2016年IEEE冬季会议*中。IEEE,
    1–9.
- en: 'Feng et al. (2018) Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and Yue
    Gao. 2018. GVCNN: Group-View Convolutional Neural Networks for 3D Shape Recognition.
    In *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng 等 (2018) Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, 和 Yue Gao.
    2018. GVCNN: 用于3D形状识别的群视图卷积神经网络。在*IEEE计算机视觉与模式识别会议（CVPR）*中。'
- en: 'Fey et al. (2017) Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich
    Müller. 2017. SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline
    Kernels. *arXiv preprint arXiv:1711.08920* (2017).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fey 等 (2017) Matthias Fey, Jan Eric Lenssen, Frank Weichert, 和 Heinrich Müller.
    2017. SplineCNN：使用连续B样条核快速几何深度学习。*arXiv preprint arXiv:1711.08920* (2017).
- en: 'Firman (2016) Michael Firman. 2016. RGBD Datasets: Past, Present and Future.
    In *CVPR Workshop on Large Scale 3D Data: Acquisition, Modelling and Analysis*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Firman (2016) Michael Firman. 2016. RGBD数据集：过去、现在和未来。在*CVPR大规模3D数据获取、建模和分析研讨会*中。
- en: 'Geng (2011) Jason Geng. 2011. Structured-light 3D surface imaging: a tutorial.
    *Adv. Opt. Photon.* 3, 2 (Jun 2011), 128–160. [https://doi.org/10.1364/AOP.3.000128](https://doi.org/10.1364/AOP.3.000128)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geng (2011) Jason Geng. 2011. 结构光3D表面成像：教程。*光学与光子学进展* 3, 2 (2011年6月), 128–160。[https://doi.org/10.1364/AOP.3.000128](https://doi.org/10.1364/AOP.3.000128)
- en: Gori et al. (2005) Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005.
    A new model for learning in graph domains. In *Neural Networks, 2005\. IJCNN’05\.
    Proceedings. 2005 IEEE International Joint Conference on*, Vol. 2. IEEE, 729–734.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gori 等 (2005) Marco Gori, Gabriele Monfardini, 和 Franco Scarselli. 2005. 图领域学习的新模型。在*神经网络，2005.
    IJCNN’05. 2005 IEEE国际联合会议*中，Vol. 2. IEEE, 729–734.
- en: Gregor and LeCun (2010) Karo Gregor and Yann LeCun. 2010. Emergence of complex-like
    cells in a temporal product network with local receptive fields. *arXiv preprint
    arXiv:1006.0448* (2010).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gregor 和 LeCun (2010) Karo Gregor 和 Yann LeCun. 2010. 在具有局部感受域的时间乘积网络中复杂细胞的出现。*arXiv
    preprint arXiv:1006.0448* (2010).
- en: Guo et al. (2013) Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Min Lu, and
    Jianwei Wan. 2013. Rotational projection statistics for 3D local surface description
    and object recognition. *International journal of computer vision* 105, 1 (2013),
    63–86.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2013) Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Min Lu, 和 Jianwei
    Wan. 2013. 用于3D局部表面描述和物体识别的旋转投影统计。*国际计算机视觉期刊* 105, 1 (2013), 63–86.
- en: Guo et al. (2014) Yulan Guo, Jun Zhang, Min Lu, Jianwei Wan, and Yanxin Ma.
    2014. Benchmark datasets for 3D computer vision. In *Industrial Electronics and
    Applications (ICIEA), 2014 IEEE 9th Conference on*. IEEE, 1846–1851.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2014) Yulan Guo, Jun Zhang, Min Lu, Jianwei Wan, 和 Yanxin Ma. 2014. 用于3D计算机视觉的基准数据集。在*2014年IEEE工业电子与应用会议(ICIEA)*中。IEEE,
    1846–1851.
- en: Han et al. (2017b) Xiaoguang Han, Zhen Li, Haibin Huang, Evangelos Kalogerakis,
    and Yizhou Yu. 2017b. High-Resolution Shape Completion Using Deep Neural Networks
    for Global Structure and Local Geometry Inference. In *Proceedings of IEEE International
    Conference on Computer Vision (ICCV)*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2017b) Xiaoguang Han, Zhen Li, Haibin Huang, Evangelos Kalogerakis, 和
    Yizhou Yu. 2017b. 使用深度神经网络进行高分辨率形状补全，用于全局结构和局部几何推断。在*IEEE国际计算机视觉会议（ICCV）*中的论文集。
- en: Han et al. (2017a) Xian-Feng Han, Jesse S Jin, Ming-Jie Wang, and Wei Jiang.
    2017a. Guided 3D point cloud filtering. *Multimedia Tools and Applications* (2017),
    1–15.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2017a) Xian-Feng Han, Jesse S Jin, Ming-Jie Wang, 和 Wei Jiang。2017a年。引导3D点云过滤。*多媒体工具与应用*（2017），1–15。
- en: Han et al. (2017c) Zhizhong Han, Zhenbao Liu, Junwei Han, Chi-Man Vong, Shuhui
    Bu, and Chun Lung Philip Chen. 2017c. Mesh convolutional restricted Boltzmann
    machines for unsupervised learning of features with structure preservation on
    3-D meshes. *IEEE transactions on neural networks and learning systems* 28, 10
    (2017), 2268–2281.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2017c) Zhizhong Han, Zhenbao Liu, Junwei Han, Chi-Man Vong, Shuhui
    Bu, 和 Chun Lung Philip Chen。2017c年。网格卷积限制玻尔兹曼机用于具有结构保持的无监督特征学习。*IEEE 神经网络与学习系统汇刊*
    28, 10（2017年），2268–2281。
- en: 'Handa et al. (2016) Ankur Handa, Viorica Pătrăucean, Simon Stent, and Roberto
    Cipolla. 2016. Scenenet: An annotated model generator for indoor scene understanding.
    In *Robotics and Automation (ICRA), 2016 IEEE International Conference on*. IEEE,
    5737–5743.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Handa et al. (2016) Ankur Handa, Viorica Pătrăucean, Simon Stent, 和 Roberto
    Cipolla。2016年。Scenenet: 一个用于室内场景理解的注释模型生成器。发表于 *机器人与自动化（ICRA），2016 IEEE 国际会议*。IEEE，5737–5743。'
- en: 'Hansard et al. (2012) Miles Hansard, Seungkyu Lee, Ouk Choi, and Radu Horaud.
    2012. *Time-of-Flight Cameras: Principles, Methods and Applications*. Springer
    Publishing Company, Incorporated.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hansard et al. (2012) Miles Hansard, Seungkyu Lee, Ouk Choi, 和 Radu Horaud。2012年。*飞行时间相机：原理、方法与应用*。Springer
    出版公司。
- en: He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.
    Deep Residual Learning for Image Recognition. *arXiv preprint arXiv:1512.03385*
    (2015).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun。2015年。深度残差学习用于图像识别。*arXiv
    预印本 arXiv:1512.03385*（2015年）。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun。2016年。深度残差学习用于图像识别。发表于
    *IEEE 计算机视觉与模式识别会议*。770–778。
- en: He et al. (2018) Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang Bai.
    2018. Triplet-Center Loss for Multi-View 3D Object Retrieval. In *The IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2018) Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, 和 Xiang Bai。2018年。三元组中心损失用于多视角3D物体检索。发表于
    *IEEE 计算机视觉与模式识别会议（CVPR）*。
- en: 'Hegde and Zadeh (2016) Vishakh Hegde and Reza Zadeh. 2016. Fusionnet: 3d object
    classification using multiple data representations. *arXiv preprint arXiv:1607.05695*
    (2016).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hegde and Zadeh (2016) Vishakh Hegde 和 Reza Zadeh。2016年。Fusionnet：使用多种数据表示的3D物体分类。*arXiv
    预印本 arXiv:1607.05695*（2016年）。
- en: Hermosilla et al. (2018) Pedro Hermosilla, Tobias Ritschel, Pere-Pau Vázquez,
    Àlvar Vinacua, and Timo Ropinski. 2018. Monte Carlo Convolution for Learning on
    Non-Uniformly Sampled Point Clouds. *arXiv preprint arXiv:1806.01759* (2018).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hermosilla et al. (2018) Pedro Hermosilla, Tobias Ritschel, Pere-Pau Vázquez,
    Àlvar Vinacua, 和 Timo Ropinski。2018年。蒙特卡罗卷积用于非均匀采样点云上的学习。*arXiv 预印本 arXiv:1806.01759*（2018年）。
- en: Huang et al. (2016) Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q
    Weinberger. 2016. Deep networks with stochastic depth. In *European Conference
    on Computer Vision*. Springer, 646–661.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2016) Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, 和 Kilian Q
    Weinberger。2016年。具有随机深度的深度网络。发表于 *欧洲计算机视觉会议*。Springer，646–661。
- en: 'Huang et al. (2006) Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew. 2006.
    Extreme learning machine: theory and applications. *Neurocomputing* 70, 1-3 (2006),
    489–501.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2006) Guang-Bin Huang, Qin-Yu Zhu, 和 Chee-Kheong Siew。2006年。极限学习机：理论与应用。*神经计算*
    70, 1-3（2006年），489–501。
- en: 'Ioannidou et al. (2017) Anastasia Ioannidou, Elisavet Chatzilari, Spiros Nikolopoulos,
    and Ioannis Kompatsiaris. 2017. Deep learning advances in computer vision with
    3d data: A survey. *ACM Computing Surveys (CSUR)* 50, 2 (2017), 20.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioannidou et al. (2017) Anastasia Ioannidou, Elisavet Chatzilari, Spiros Nikolopoulos,
    和 Ioannis Kompatsiaris。2017年。深度学习在3D数据计算机视觉中的进展：综述。*ACM 计算调查（CSUR）* 50, 2（2017年），20。
- en: 'Ioffe and Szegedy (2015) Sergey Ioffe and Christian Szegedy. 2015. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. *arXiv
    preprint arXiv:1502.03167* (2015).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe and Szegedy (2015) Sergey Ioffe 和 Christian Szegedy。2015年。批量归一化：通过减少内部协变量偏移来加速深度网络训练。*arXiv
    预印本 arXiv:1502.03167*（2015年）。
- en: Johns et al. (2016) Edward Johns, Stefan Leutenegger, and Andrew J Davison.
    2016. Pairwise decomposition of image sequences for active multi-view recognition.
    In *Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on*.
    IEEE, 3813–3822.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johns et al. (2016) Edward Johns, Stefan Leutenegger, 和 Andrew J Davison. 2016.
    图像序列的成对分解用于主动多视图识别。见 *计算机视觉与模式识别 (CVPR)，2016 IEEE 会议*。IEEE，3813–3822。
- en: Johnson and Hebert (1999) Andrew E. Johnson and Martial Hebert. 1999. Using
    spin images for efficient object recognition in cluttered 3D scenes. *IEEE Transactions
    on pattern analysis and machine intelligence* 21, 5 (1999), 433–449.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson and Hebert (1999) Andrew E. Johnson 和 Martial Hebert. 1999. 使用旋转图像在混乱的
    3D 场景中进行高效的物体识别。*IEEE 模式分析与机器智能汇刊* 21, 5 (1999), 433–449。
- en: 'Kanezaki et al. (2016) Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida.
    2016. RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews
    from Unsupervised Viewpoints. *arXiv preprint arXiv:1603.06208* (2016).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kanezaki et al. (2016) Asako Kanezaki, Yasuyuki Matsushita, 和 Yoshifumi Nishida.
    2016. RotationNet: 利用来自无监督视角的多视图进行物体分类和姿态估计。*arXiv 预印本 arXiv:1603.06208* (2016)。'
- en: Kasun et al. (2013) Liyanaarachchi Lekamalage Chamara Kasun, Hongming Zhou,
    Guang-Bin Huang, and Chi Man Vong. 2013. Representational learning with ELMs for
    big data. (2013).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kasun et al. (2013) Liyanaarachchi Lekamalage Chamara Kasun, Hongming Zhou,
    Guang-Bin Huang, 和 Chi Man Vong. 2013. 使用 ELM 进行大数据的表征学习。 (2013)。
- en: Kazmi et al. (2013) Ismail Khalid Kazmi, Lihua You, and Jian Jun Zhang. 2013.
    A survey of 2d and 3d shape descriptors. In *Computer graphics, imaging and visualization
    (cgiv), 2013 10th international conference*. IEEE, 1–10.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kazmi et al. (2013) Ismail Khalid Kazmi, Lihua You, 和 Jian Jun Zhang. 2013.
    2D 和 3D 形状描述符综述。见 *计算机图形学、成像与视觉 (cgiv)，2013 第 10 届国际会议*。IEEE，1–10。
- en: Khalil et al. (2017) Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and
    Le Song. 2017. Learning combinatorial optimization algorithms over graphs. In
    *Advances in Neural Information Processing Systems*. NIPS, 6351–6361.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khalil et al. (2017) Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, 和
    Le Song. 2017. 在图上学习组合优化算法。见 *神经信息处理系统进展*。NIPS，6351–6361。
- en: Kipf and Welling (2016) Thomas N Kipf and Max Welling. 2016. Semi-supervised
    classification with graph convolutional networks. *arXiv preprint arXiv:1609.02907*
    (2016).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf and Welling (2016) Thomas N Kipf 和 Max Welling. 2016. 使用图卷积网络进行半监督分类。*arXiv
    预印本 arXiv:1609.02907* (2016)。
- en: 'Klokov and Lempitsky (2017) Roman Klokov and Victor Lempitsky. 2017. Escape
    from cells: Deep kd-networks for the recognition of 3d point cloud models. In
    *2017 IEEE International Conference on Computer Vision (ICCV)*. IEEE, 863–872.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klokov and Lempitsky (2017) Roman Klokov 和 Victor Lempitsky. 2017. 摆脱束缚：用于识别
    3D 点云模型的深度 kd-网络。见 *2017 IEEE 国际计算机视觉会议 (ICCV)*。IEEE，863–872。
- en: Kovnatsky et al. (2013) Artiom Kovnatsky, Michael M Bronstein, Alexander M Bronstein,
    Klaus Glashoff, and Ron Kimmel. 2013. Coupled quasi-harmonic bases. In *Computer
    Graphics Forum*, Vol. 32\. Wiley Online Library, 439–448.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kovnatsky et al. (2013) Artiom Kovnatsky, Michael M Bronstein, Alexander M Bronstein,
    Klaus Glashoff, 和 Ron Kimmel. 2013. 配对的准谐波基。见 *计算机图形学论坛*，第 32 卷。Wiley 在线图书馆，439–448。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. In *Advances
    in neural information processing systems*. 1097–1105.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, 和 Geoffrey E Hinton.
    2012. 使用深度卷积神经网络进行 Imagenet 分类。见 *神经信息处理系统进展*。1097–1105。
- en: Lai et al. (2011) Kevin Lai, Liefeng Bo, Xiaofeng Ren, and Dieter Fox. 2011.
    A large-scale hierarchical multi-view rgb-d object dataset. In *Robotics and Automation
    (ICRA), 2011 IEEE International Conference on*. IEEE, 1817–1824.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai et al. (2011) Kevin Lai, Liefeng Bo, Xiaofeng Ren, 和 Dieter Fox. 2011. 大规模分层多视图
    RGB-D 物体数据集。见 *机器人与自动化 (ICRA)，2011 IEEE 国际会议*。IEEE，1817–1824。
- en: Lee et al. (2009) Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y
    Ng. 2009. Convolutional deep belief networks for scalable unsupervised learning
    of hierarchical representations. In *Proceedings of the 26th annual international
    conference on machine learning*. ACM, 609–616.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2009) Honglak Lee, Roger Grosse, Rajesh Ranganath, 和 Andrew Y Ng.
    2009. 卷积深度置信网络用于可扩展的无监督学习层次表征。见 *第 26 届年度国际机器学习会议论文集*。ACM，609–616。
- en: Leng et al. (2015a) Biao Leng, Shuang Guo, Xiangyang Zhang, and Zhang Xiong.
    2015a. 3D object retrieval with stacked local convolutional autoencoder. *Signal
    Processing* 112 (2015), 119–128.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leng et al. (2015a) Biao Leng, Shuang Guo, Xiangyang Zhang, 和 Zhang Xiong. 2015a.
    使用堆叠的局部卷积自编码器进行 3D 物体检索。*信号处理* 112 (2015), 119–128。
- en: Leng et al. (2015b) Biao Leng, Yu Liu, Kai Yu, Xiangyang Zhang, and Zhang Xiong.
    2015b. 3D object understanding with 3D convolutional neural networks. *Information
    sciences* 366 (2015), 188–201.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leng et al. (2015b) Biao Leng, Yu Liu, Kai Yu, Xiangyang Zhang, 和 Zhang Xiong。2015b年。使用3D卷积神经网络的3D对象理解。*信息科学*
    366 (2015)，第188–201页。
- en: Leng et al. (2014) Biao Leng, Xiangyang Zhang, Ming Yao, and Zhang Xiong. 2014.
    3d object classification using deep belief networks. In *International Conference
    on Multimedia Modeling*. Springer, 128–139.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leng et al. (2014) Biao Leng, Xiangyang Zhang, Ming Yao, 和 Zhang Xiong。2014年。使用深度信念网络的3D对象分类。发表于*国际多媒体建模会议*。Springer，第128–139页。
- en: 'Li et al. (2018) Jiaxin Li, Ben M Chen, and Gim Hee Lee. 2018. SO-Net: Self-Organizing
    Network for Point Cloud Analysis. *arXiv preprint arXiv:1803.04249* (2018).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2018) Jiaxin Li, Ben M Chen, 和 Gim Hee Lee。2018年。SO-Net：用于点云分析的自组织网络。*arXiv预印本
    arXiv:1803.04249* (2018)。
- en: Li et al. (2015) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
    2015. Gated graph sequence neural networks. *arXiv preprint arXiv:1511.05493*
    (2015).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2015) Yujia Li, Daniel Tarlow, Marc Brockschmidt, 和 Richard Zemel。2015年。门控图序列神经网络。*arXiv预印本
    arXiv:1511.05493* (2015)。
- en: Ling and Jacobs (2007) Haibin Ling and David W Jacobs. 2007. Shape classification
    using the inner-distance. *IEEE transactions on pattern analysis and machine intelligence*
    29, 2 (2007), 286–299.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling and Jacobs (2007) Haibin Ling 和 David W Jacobs。2007年。使用内距的形状分类。*IEEE模式分析与机器智能汇刊*
    29, 2 (2007)，第286–299页。
- en: Liu et al. (2014) Zhenbao Liu, Shaoguang Chen, Shuhui Bu, and Ke Li. 2014. High-level
    semantic feature for 3D shape based on deep belief networks. In *Multimedia and
    Expo (ICME), 2014 IEEE International Conference on*. IEEE, 1–6.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2014) Zhenbao Liu, Shaoguang Chen, Shuhui Bu, 和 Ke Li。2014年。基于深度信念网络的3D形状高层语义特征。发表于*2014
    IEEE国际多媒体与展览会议（ICME）*。IEEE，第1–6页。
- en: Liu et al. (2013) Zhen-Bao Liu, Shu-Hui Bu, Kun Zhou, Shu-Ming Gao, Jun-Wei
    Han, and Jun Wu. 2013. A survey on partial retrieval of 3D shapes. *Journal of
    Computer Science and Technology* 28, 5 (2013), 836–851.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2013) Zhen-Bao Liu, Shu-Hui Bu, Kun Zhou, Shu-Ming Gao, Jun-Wei
    Han, 和 Jun Wu。2013年。3D形状部分检索的综述。*计算机科学与技术杂志* 28, 5 (2013)，第836–851页。
- en: Long et al. (2015) Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015.
    Fully convolutional networks for semantic segmentation. In *Proceedings of the
    IEEE conference on computer vision and pattern recognition*. 3431–3440.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. (2015) Jonathan Long, Evan Shelhamer, 和 Trevor Darrell。2015年。用于语义分割的全卷积网络。发表于*IEEE计算机视觉与模式识别会议论文集*。第3431–3440页。
- en: 'Loper et al. (2015) Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll,
    and Michael J. Black. 2015. SMPL: A Skinned Multi-Person Linear Model. *ACM Trans.
    Graphics (Proc. SIGGRAPH Asia)* 34, 6 (Oct. 2015), 248:1–248:16.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loper et al. (2015) Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll,
    和 Michael J. Black。2015年。SMPL：一种皮肤多人人体线性模型。*ACM图形学汇刊（SIGGRAPH亚洲会议论文集）* 34, 6 (2015年10月)，第248:1–248:16页。
- en: Maron et al. (2017) Haggai Maron, Meirav Galun, Noam Aigerman, Miri Trope, Nadav
    Dym, Ersin Yumer, Vladimir G Kim, and Yaron Lipman. 2017. Convolutional neural
    networks on surfaces via seamless toric covers. *ACM Trans. Graph* 36, 4 (2017).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maron et al. (2017) Haggai Maron, Meirav Galun, Noam Aigerman, Miri Trope, Nadav
    Dym, Ersin Yumer, Vladimir G Kim, 和 Yaron Lipman。2017年。通过无缝环面覆盖的表面卷积神经网络。*ACM图形学汇刊*
    36, 4 (2017)。
- en: Marton et al. (2011) Zoltan-Csaba Marton, Dejan Pangercic, Nico Blodow, and
    Michael Beetz. 2011. Combined 2D–3D categorization and classification for multimodal
    perception systems. *The International Journal of Robotics Research* 30, 11 (2011),
    1378–1402.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marton et al. (2011) Zoltan-Csaba Marton, Dejan Pangercic, Nico Blodow, 和 Michael
    Beetz。2011年。用于多模态感知系统的2D–3D组合分类。*国际机器人研究杂志* 30, 11 (2011)，第1378–1402页。
- en: Masci et al. (2015) Jonathan Masci, Davide Boscaini, Michael Bronstein, and
    Pierre Vandergheynst. 2015. Geodesic convolutional neural networks on riemannian
    manifolds. In *Proceedings of the IEEE international conference on computer vision
    workshops*. IEEE, 37–45.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masci et al. (2015) Jonathan Masci, Davide Boscaini, Michael Bronstein, 和 Pierre
    Vandergheynst。2015年。黎曼流形上的测地卷积神经网络。发表于*IEEE国际计算机视觉会议论文集*。IEEE，第37–45页。
- en: 'Maturana and Scherer (2015) Daniel Maturana and Sebastian Scherer. 2015. Voxnet:
    A 3d convolutional neural network for real-time object recognition. In *Intelligent
    Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on*. IEEE, 922–928.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maturana and Scherer (2015) Daniel Maturana 和 Sebastian Scherer。2015年。Voxnet：一种用于实时对象识别的3D卷积神经网络。发表于*2015
    IEEE/RSJ国际智能机器人与系统会议*。IEEE，第922–928页。
- en: Monti et al. (2017) Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele
    Rodola, Jan Svoboda, and Michael M Bronstein. 2017. Geometric deep learning on
    graphs and manifolds using mixture model CNNs. In *Proc. CVPR*, Vol. 1\. IEEE,
    3.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Monti 等（2017）Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola,
    Jan Svoboda 和 Michael M Bronstein。2017。使用混合模型 CNN 的图形和流形上的几何深度学习。在 *CVPR 会议论文集*，第
    1 卷。IEEE，3。
- en: Niepert et al. (2016) Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov.
    2016. Learning convolutional neural networks for graphs. In *International conference
    on machine learning*. JMLR.org, 2014–2023.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niepert 等（2016）Mathias Niepert, Mohamed Ahmed 和 Konstantin Kutzkov。2016。学习用于图形的卷积神经网络。在
    *国际机器学习会议*。JMLR.org，2014–2023。
- en: Noh et al. (2015) Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. 2015. Learning
    deconvolution network for semantic segmentation. In *Proceedings of the IEEE International
    Conference on Computer Vision*. 1520–1528.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noh 等（2015）Hyeonwoo Noh, Seunghoon Hong 和 Bohyung Han。2015。学习用于语义分割的去卷积网络。在
    *IEEE 国际计算机视觉会议论文集*。1520–1528。
- en: Nowak et al. (2017) Alex Nowak, Soledad Villar, Afonso S Bandeira, and Joan
    Bruna. 2017. A note on learning algorithms for quadratic assignment with graph
    neural networks. *arXiv preprint arXiv:1706.07450* (2017).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nowak 等（2017）Alex Nowak, Soledad Villar, Afonso S Bandeira 和 Joan Bruna。2017。关于使用图神经网络进行二次分配学习算法的注释。*arXiv
    预印本 arXiv:1706.07450*（2017）。
- en: Park et al. (2011) Jaesik Park, Hyeongwoo Kim, Yu-Wing Tai, Michael S Brown,
    and Inso Kweon. 2011. High quality depth map upsampling for 3d-tof cameras. In
    *Computer Vision (ICCV), 2011 IEEE International Conference on*. IEEE, 1623–1630.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等（2011）Jaesik Park, Hyeongwoo Kim, Yu-Wing Tai, Michael S Brown 和 Inso
    Kweon。2011。用于 3D-TOF 相机的高质量深度图上采样。在 *计算机视觉（ICCV），2011 IEEE 国际会议*。IEEE，1623–1630。
- en: 'Qi et al. (2016a) Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
    2016a. PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.
    *arXiv preprint arXiv:1612.00593* (2016).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等（2016a）Charles R Qi, Hao Su, Kaichun Mo 和 Leonidas J Guibas。2016a。PointNet：对点集进行
    3D 分类和分割的深度学习。*arXiv 预印本 arXiv:1612.00593*（2016）。
- en: 'Qi et al. (2017a) Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
    2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation.
    *Proc. Computer Vision and Pattern Recognition (CVPR), IEEE* 1, 2 (2017), 4.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等（2017a）Charles R Qi, Hao Su, Kaichun Mo 和 Leonidas J Guibas。2017a。Pointnet：对点集进行
    3D 分类和分割的深度学习。*计算机视觉与模式识别会议论文集（CVPR），IEEE* 1, 2（2017），4。
- en: Qi et al. (2016b) Charles R Qi, Hao Su, Matthias Nießner, Angela Dai, Mengyuan
    Yan, and Leonidas J Guibas. 2016b. Volumetric and multi-view cnns for object classification
    on 3d data. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*. IEEE, 5648–5656.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等（2016b）Charles R Qi, Hao Su, Matthias Nießner, Angela Dai, Mengyuan Yan
    和 Leonidas J Guibas。2016b。用于 3D 数据对象分类的体积和多视图 CNN。在 *IEEE 计算机视觉与模式识别会议论文集*。IEEE，5648–5656。
- en: 'Qi et al. (2017b) Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas.
    2017b. Pointnet++: Deep hierarchical feature learning on point sets in a metric
    space. In *Advances in Neural Information Processing Systems*. NIPS, 5105–5114.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等（2017b）Charles Ruizhongtai Qi, Li Yi, Hao Su 和 Leonidas J Guibas。2017b。Pointnet++：在度量空间中对点集进行深度分层特征学习。在
    *神经信息处理系统进展*。NIPS，5105–5114。
- en: Rangel et al. (2017) José Carlos Rangel, Vicente Morell, Miguel Cazorla, Sergio
    Orts-Escolano, and José García-Rodríguez. 2017. Object recognition in noisy RGB-D
    data using GNG. *Pattern Analysis and Applications* 20, 4 (2017), 1061–1076.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rangel 等（2017）José Carlos Rangel, Vicente Morell, Miguel Cazorla, Sergio Orts-Escolano
    和 José García-Rodríguez。2017。利用 GNG 在噪声 RGB-D 数据中进行物体识别。*模式分析与应用* 20, 4（2017），1061–1076。
- en: Ravanbakhsh et al. (2016) Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos.
    2016. Deep learning with sets and point clouds. *arXiv preprint arXiv:1611.04500*
    (2016).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravanbakhsh 等（2016）Siamak Ravanbakhsh, Jeff Schneider 和 Barnabas Poczos。2016。利用集合和点云的深度学习。*arXiv
    预印本 arXiv:1611.04500*（2016）。
- en: 'Riegler et al. (2017) Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.
    2017. OctNet: Learning Deep 3D Representations at High Resolutions. In *The IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riegler 等（2017）Gernot Riegler, Ali Osman Ulusoy 和 Andreas Geiger。2017。OctNet：在高分辨率下学习深度
    3D 表征。在 *IEEE 计算机视觉与模式识别会议（CVPR）*。
- en: Roveri et al. (2018) Riccardo Roveri, Lukas Rahmann, A Cengiz Oztireli, and
    Markus Gross. 2018. A Network Architecture for Point Cloud Classification via
    Automatic Depth Images Generation. In *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*. 4176–4184.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roveri 等（2018）Riccardo Roveri, Lukas Rahmann, A Cengiz Oztireli 和 Markus Gross。2018。用于点云分类的网络架构，通过自动生成深度图像。在
    *IEEE 计算机视觉与模式识别会议论文集*。4176–4184。
- en: Rusu et al. (2009) Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. 2009. Fast
    point feature histograms (FPFH) for 3D registration. In *Robotics and Automation,
    2009. ICRA’09\. IEEE International Conference on*. IEEE, 3212–3217.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rusu等人（2009）Radu Bogdan Rusu、Nico Blodow 和 Michael Beetz。2009年。用于3D配准的快速点特征直方图（FPFH）。发表于*机器人与自动化，2009年
    ICRA’09 IEEE国际会议*。IEEE，3212–3217。
- en: Rusu et al. (2008) Radu Bogdan Rusu, Nico Blodow, Zoltan Csaba Marton, and Michael
    Beetz. 2008. Aligning point cloud views using persistent feature histograms. In
    *Intelligent Robots and Systems, 2008\. IROS 2008\. IEEE/RSJ International Conference
    on*. IEEE, 3384–3391.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rusu等人（2008）Radu Bogdan Rusu、Nico Blodow、Zoltan Csaba Marton 和 Michael Beetz。2008年。使用持久特征直方图对齐点云视图。发表于*智能机器人与系统，2008年
    IROS 2008 IEEE/RSJ国际会议*。IEEE，3384–3391。
- en: 'Saint et al. (2018) Alexandre Fabian A Saint, Eman Ahmed, Abd El Rahman Shabayek,
    Kseniya Cherenkova, Gleb Gusev, Djamila Aouada, and Björn Ottersten. 2018. 3DBodyTex:
    Textured 3D Body Dataset. In *2018 Sixth International Conference on 3D Vision
    (3DV 2018)*. [https://cvdatasets.uni.lu/datasets/](https://cvdatasets.uni.lu/datasets/)
    Accessed on 05.04.2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Saint等人（2018）Alexandre Fabian A Saint、Eman Ahmed、Abd El Rahman Shabayek、Kseniya
    Cherenkova、Gleb Gusev、Djamila Aouada 和 Björn Ottersten。2018年。3DBodyTex: 纹理化3D人体数据集。发表于*2018年第六届国际3D视觉会议（3DV
    2018）*。 [https://cvdatasets.uni.lu/datasets/](https://cvdatasets.uni.lu/datasets/)
    访问时间：2019年4月5日。'
- en: Saito et al. (2016) Shunsuke Saito, Tianye Li, and Hao Li. 2016. Real-time facial
    segmentation and performance capture from rgb input. In *European Conference on
    Computer Vision*. Springer, 244–261.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saito等人（2016）Shunsuke Saito、Tianye Li 和 Hao Li。2016年。从RGB输入中进行实时面部分割和性能捕捉。发表于*欧洲计算机视觉会议*。Springer，244–261。
- en: Samet (1984) Hanan Samet. 1984. The quadtree and related hierarchical data structures.
    *ACM Computing Surveys (CSUR)* 16, 2 (1984), 187–260.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samet（1984）Hanan Samet。1984年。四叉树及相关的层次数据结构。*ACM计算调查（CSUR）* 16, 2（1984年），187–260。
- en: Scarselli et al. (2009) Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
    Hagenbuchner, and Gabriele Monfardini. 2009. The graph neural network model. *IEEE
    Transactions on Neural Networks* 20, 1 (2009), 61–80.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scarselli等人（2009）Franco Scarselli、Marco Gori、Ah Chung Tsoi、Markus Hagenbuchner
    和 Gabriele Monfardini。2009年。图神经网络模型。*IEEE神经网络汇刊* 20, 1（2009年），61–80。
- en: 'Schroff et al. (2015) Florian Schroff, Dmitry Kalenichenko, and James Philbin.
    2015. FaceNet: A Unified Embedding for Face Recognition and Clustering. In *The
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schroff等人（2015）Florian Schroff、Dmitry Kalenichenko 和 James Philbin。2015年。FaceNet:
    用于面部识别和聚类的统一嵌入。发表于*IEEE计算机视觉与模式识别会议（CVPR）*。'
- en: Schwarz et al. (2015) Max Schwarz, Hannes Schulz, and Sven Behnke. 2015. RGB-D
    object recognition and pose estimation based on pre-trained convolutional neural
    network features. In *Robotics and Automation (ICRA), 2015 IEEE International
    Conference on*. IEEE, 1329–1335.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwarz等人（2015）Max Schwarz、Hannes Schulz 和 Sven Behnke。2015年。基于预训练卷积神经网络特征的RGB-D对象识别和姿态估计。发表于*机器人与自动化（ICRA），2015
    IEEE国际会议*。IEEE，1329–1335。
- en: Sedaghat et al. (2016) Nima Sedaghat, Mohammadreza Zolfaghari, Ehsan Amiri,
    and Thomas Brox. 2016. Orientation-boosted voxel nets for 3D object recognition.
    *arXiv preprint arXiv:1604.03351* (2016).
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sedaghat等人（2016）Nima Sedaghat、Mohammadreza Zolfaghari、Ehsan Amiri 和 Thomas Brox。2016年。用于3D对象识别的方向增强体素网络。*arXiv预印本
    arXiv:1604.03351*（2016年）。
- en: 'Sermanet et al. (2013) Pierre Sermanet, David Eigen, Xiang Zhang, Michaël Mathieu,
    Rob Fergus, and Yann LeCun. 2013. Overfeat: Integrated recognition, localization
    and detection using convolutional networks. *arXiv preprint arXiv:1312.6229* (2013).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sermanet等人（2013）Pierre Sermanet、David Eigen、Xiang Zhang、Michaël Mathieu、Rob
    Fergus 和 Yann LeCun。2013年。Overfeat: 使用卷积网络进行集成识别、定位和检测。*arXiv预印本 arXiv:1312.6229*（2013年）。'
- en: Sfikas et al. (2017a) Konstantinos Sfikas, Ioannis Pratikakis, and Theoharis
    Theoharis. 2017a. Ensemble of PANORAMA-based convolutional neural networks for
    3D model classification and retrieval. *Computers & Graphics* (2017).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sfikas等人（2017a）Konstantinos Sfikas、Ioannis Pratikakis 和 Theoharis Theoharis。2017a年。基于PANORAMA的卷积神经网络集成用于3D模型分类和检索。*计算机与图形学*（2017年）。
- en: Sfikas et al. (2017b) Konstantinos Sfikas, Theoharis Theoharis, and Ioannis
    Pratikakis. 2017b. Exploiting the PANORAMA representation for convolutional neural
    network classification and retrieval. In *Eurographics Workshop on 3D Object Retrieval*.
    The Eurographics Association.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sfikas等人（2017b）Konstantinos Sfikas、Theoharis Theoharis 和 Ioannis Pratikakis。2017b年。利用PANORAMA表示进行卷积神经网络分类和检索。发表于*Eurographics
    3D对象检索研讨会*。欧洲图形学协会。
- en: 'Sharma et al. (2016) Abhishek Sharma, Oliver Grau, and Mario Fritz. 2016. VConv-DAE:
    Deep Volumetric Shape Learning Without Object Labels. In *Geometry Meets Deep
    Learning Workshop at European Conference on Computer Vision (ECCV-W)*.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sharma 等（2016）Abhishek Sharma, Oliver Grau, 和 Mario Fritz。2016。《VConv-DAE:
    无需物体标签的深度体积形状学习》。发表于 *Geometry Meets Deep Learning Workshop at European Conference
    on Computer Vision (ECCV-W)*。'
- en: 'Shi et al. (2015) Baoguang Shi, Song Bai, Zhichao Zhou, and Xiang Bai. 2015.
    Deeppano: Deep panoramic representation for 3-d shape recognition. *IEEE Signal
    Processing Letters* 22, 12 (2015), 2339–2343.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 等（2015）Baoguang Shi, Song Bai, Zhichao Zhou, 和 Xiang Bai。2015。《Deeppano:
    用于 3D 形状识别的深度全景表示》。*IEEE Signal Processing Letters* 22，第 12 期（2015），2339–2343。'
- en: Sinha et al. (2016) Ayan Sinha, Jing Bai, and Karthik Ramani. 2016. Deep learning
    3D shape surfaces using geometry images. In *European Conference on Computer Vision*.
    Springer, 223–240.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinha 等（2016）Ayan Sinha, Jing Bai, 和 Karthik Ramani。2016。《利用几何图像进行深度学习 3D 形状表面》。发表于
    *European Conference on Computer Vision*。Springer，223–240。
- en: Socher et al. (2012) Richard Socher, Brody Huval, Bharath Bath, Christopher D
    Manning, and Andrew Y Ng. 2012. Convolutional-recursive deep learning for 3d object
    classification. In *Advances in Neural Information Processing Systems*. NIPS,
    656–664.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher 等（2012）Richard Socher, Brody Huval, Bharath Bath, Christopher D Manning,
    和 Andrew Y Ng。2012。《用于 3D 物体分类的卷积递归深度学习》。发表于 *Advances in Neural Information Processing
    Systems*。NIPS，656–664。
- en: Song and Xiao (2016) Shuran Song and Jianxiong Xiao. 2016. Deep Sliding Shapes
    for Amodal 3D Object Detection in RGB-D Images. In *CVPR*. IEEE.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 和 Xiao（2016）Shuran Song 和 Jianxiong Xiao。2016。《用于 RGB-D 图像中的无模态 3D 物体检测的深度滑动形状》。发表于
    *CVPR*。IEEE。
- en: Song et al. (2017) Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis
    Savva, and Thomas Funkhouser. 2017. Semantic scene completion from a single depth
    image. In *Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference
    on*. IEEE, 190–198.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等（2017）Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva,
    和 Thomas Funkhouser。2017。《从单一深度图像中进行语义场景补全》。发表于 *Computer Vision and Pattern Recognition
    (CVPR), 2017 IEEE Conference on*。IEEE，190–198。
- en: Su et al. (2015) Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller.
    2015. Multi-view convolutional neural networks for 3d shape recognition. In *Proceedings
    of the IEEE international conference on computer vision*. 945–953.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等（2015）Hang Su, Subhransu Maji, Evangelos Kalogerakis, 和 Erik Learned-Miller。2015。《用于
    3D 形状识别的多视角卷积神经网络》。发表于 *Proceedings of the IEEE International Conference on Computer
    Vision*。945–953。
- en: Sukhbaatar et al. (2016) Sainbayar Sukhbaatar, Rob Fergus, et al. 2016. Learning
    multiagent communication with backpropagation. In *Advances in Neural Information
    Processing Systems*. NIPS, 2244–2252.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sukhbaatar 等（2016）Sainbayar Sukhbaatar, Rob Fergus 等。2016。《通过反向传播学习多智能体通信》。发表于
    *Advances in Neural Information Processing Systems*。NIPS，2244–2252。
- en: Sun et al. (2009) Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. 2009. A concise
    and provably informative multi-scale signature based on heat diffusion. In *Computer
    graphics forum*, Vol. 28\. Wiley Online Library, 1383–1392.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2009）Jian Sun, Maks Ovsjanikov, 和 Leonidas Guibas。2009。《基于热扩散的简洁且可以证明有效的多尺度特征》。发表于
    *Computer Graphics Forum*，第 28 卷。Wiley Online Library，1383–1392。
- en: Sylvain Lefebvre and Neyret (2005) Samuel Hornus Sylvain Lefebvre and Fabrice
    Neyret. April,2005. [https://developer.nvidia.com/gpugems/GPUGems2.html](https://developer.nvidia.com/gpugems/GPUGems2.html)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sylvain Lefebvre 和 Neyret（2005）Samuel Hornus Sylvain Lefebvre 和 Fabrice Neyret。2005年4月。[https://developer.nvidia.com/gpugems/GPUGems2.html](https://developer.nvidia.com/gpugems/GPUGems2.html)
- en: Szegedy et al. (2017) Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
    Alexander A Alemi. 2017. Inception-v4, inception-resnet and the impact of residual
    connections on learning.. In *AAAI*, Vol. 4\. AAAI, 12.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等（2017）Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, 和 Alexander
    A Alemi。2017。《Inception-v4, inception-resnet 及残差连接对学习的影响》。发表于 *AAAI*，第 4 卷。AAAI，12。
- en: Tangelder and Veltkamp (2004) Johan WH Tangelder and Remco C Veltkamp. 2004.
    A survey of content based 3D shape retrieval methods. In *Shape Modeling Applications,
    2004. Proceedings*. IEEE, 145–156.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tangelder 和 Veltkamp（2004）Johan WH Tangelder 和 Remco C Veltkamp。2004。《基于内容的
    3D 形状检索方法综述》。发表于 *Shape Modeling Applications, 2004. Proceedings*。IEEE，145–156。
- en: 'Tatarchenko et al. (2017) Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas
    Brox. 2017. Octree generating networks: Efficient convolutional architectures
    for high-resolution 3d outputs. *CoRR, abs/1703.09438* (2017).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tatarchenko 等（2017）Maxim Tatarchenko, Alexey Dosovitskiy, 和 Thomas Brox。2017。《八叉树生成网络：用于高分辨率
    3D 输出的高效卷积架构》。*CoRR, abs/1703.09438*（2017）。
- en: 'Venkatakrishnan et al. (2018) Shaileshh Bojja Venkatakrishnan, Mohammad Alizadeh,
    and Pramod Viswanath. 2018. Graph2Seq: Scalable Learning Dynamics for Graphs.
    *arXiv preprint arXiv:1802.04948* (2018).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Venkatakrishnan 等（2018）Shaileshh Bojja Venkatakrishnan, Mohammad Alizadeh,
    和 Pramod Viswanath。2018。《Graph2Seq: Scalable Learning Dynamics for Graphs》。*arXiv
    预印本 arXiv:1802.04948*（2018）。'
- en: 'Verma et al. (2018) Nitika Verma, Edmond Boyer, and Jakob Verbeek. 2018. FeaStNet:
    Feature-Steered Graph Convolutions for 3D Shape Analysis. In *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*. 2598–2606.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Verma et al. (2018) Nitika Verma, Edmond Boyer, 和 Jakob Verbeek. 2018. FeaStNet:
    3D 形状分析的特征引导图卷积. 见 *IEEE计算机视觉与模式识别会议论文集*，2598–2606。'
- en: 'Vinyals et al. (2015) Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. 2015.
    Order matters: Sequence to sequence for sets. *arXiv preprint arXiv:1511.06391*
    (2015).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals et al. (2015) Oriol Vinyals, Samy Bengio, 和 Manjunath Kudlur. 2015.
    顺序很重要：集的序列到序列. *arXiv 预印本 arXiv:1511.06391* (2015)。
- en: Wang et al. (2017b) Chu Wang, Marcello Pelillo, and Kaleem Siddiqi. 2017b. Dominant
    Set Clustering and Pooling for Multi-View 3D Object Recognition.. In *Proceedings
    of British Machine Vision Conference (BMVC)*. BMVC.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2017b) Chu Wang, Marcello Pelillo, 和 Kaleem Siddiqi. 2017b. 主导集聚类和多视图
    3D 物体识别的池化. 见 *英国机器视觉会议论文集 (BMVC)*。BMVC。
- en: Wang et al. (2018) Chu Wang, Babak Samari, and Kaleem Siddiqi. 2018. Local Spectral
    Graph Convolution for Point Set Feature Learning. *arXiv preprint arXiv:1803.05827*
    (2018).
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2018) Chu Wang, Babak Samari, 和 Kaleem Siddiqi. 2018. 点集特征学习的局部谱图卷积.
    *arXiv 预印本 arXiv:1803.05827* (2018)。
- en: Wang et al. (2015) Fang Wang, Le Kang, and Yi Li. 2015. Sketch-based 3d shape
    retrieval using convolutional neural networks. In *Computer Vision and Pattern
    Recognition (CVPR), 2015 IEEE Conference on*. IEEE, 1875–1883.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2015) Fang Wang, Le Kang, 和 Yi Li. 2015. 基于草图的 3D 形状检索使用卷积神经网络.
    见 *计算机视觉与模式识别 (CVPR) 2015 IEEE 会议*。IEEE, 1875–1883。
- en: Wang et al. (2017a) Weiyue Wang, Qiangui Huang, Suya You, Chao Yang, and Ulrich
    Neumann. 2017a. Shape inpainting using 3d generative adversarial network and recurrent
    convolutional networks. *arXiv preprint arXiv:1711.06375* abs/1711.06375 (2017).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2017a) Weiyue Wang, Qiangui Huang, Suya You, Chao Yang, 和 Ulrich
    Neumann. 2017a. 使用 3D 生成对抗网络和递归卷积网络的形状修复. *arXiv 预印本 arXiv:1711.06375* abs/1711.06375
    (2017)。
- en: Wang et al. (2016) Yueqing Wang, Zhige Xie, Kai Xu, Yong Dou, and Yuanwu Lei.
    2016. An efficient and effective convolutional auto-encoder extreme learning machine
    network for 3d feature learning. *Neurocomputing* 174 (2016), 988–998.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2016) Yueqing Wang, Zhige Xie, Kai Xu, Yong Dou, 和 Yuanwu Lei.
    2016. 高效且有效的卷积自编码极限学习机网络用于 3D 特征学习. *神经计算* 174 (2016), 988–998。
- en: Wen et al. (2016) Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. 2016.
    A Discriminative Feature Learning Approach for Deep Face Recognition. In *Computer
    Vision – ECCV 2016*, Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (Eds.).
    Springer International Publishing, Cham, 499–515.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen et al. (2016) Yandong Wen, Kaipeng Zhang, Zhifeng Li, 和 Yu Qiao. 2016. 一种用于深度人脸识别的判别特征学习方法.
    见 *计算机视觉 – ECCV 2016*，Bastian Leibe, Jiri Matas, Nicu Sebe, 和 Max Welling (编)。施普林格国际出版公司，查姆，499–515。
- en: Wohlkinger and Vincze (2011) Walter Wohlkinger and Markus Vincze. 2011. Ensemble
    of shape functions for 3d object classification. In *Robotics and Biomimetics
    (ROBIO), 2011 IEEE International Conference on*. IEEE, 2987–2992.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wohlkinger and Vincze (2011) Walter Wohlkinger 和 Markus Vincze. 2011. 3D 物体分类的形状函数集成.
    见 *2011 IEEE 国际机器人与生物仿真会议*。IEEE, 2987–2992。
- en: Wu et al. (2016) Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh
    Tenenbaum. 2016. Learning a probabilistic latent space of object shapes via 3d
    generative-adversarial modeling. In *Advances in Neural Information Processing
    Systems*. NIPS, 82–90.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2016) Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, 和 Josh
    Tenenbaum. 2016. 通过 3D 生成对抗建模学习物体形状的概率潜在空间. 见 *神经信息处理系统进展*。NIPS, 82–90。
- en: 'Wu et al. (2015a) Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang
    Zhang, Xiaoou Tang, and Jianxiong Xiao. 2015a. 3d shapenets: A deep representation
    for volumetric shapes. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. IEEE, 1912–1920.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2015a) Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang
    Zhang, Xiaoou Tang, 和 Jianxiong Xiao. 2015a. 3D ShapeNets: 体积形状的深度表示. 见 *IEEE
    计算机视觉与模式识别会议论文集*。IEEE, 1912–1920。'
- en: 'Wu et al. (2015b) Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang
    Zhang, Xiaoou Tang, and Jianxiong Xiao. 2015b. 3D ShapeNets: A Deep Representation
    for Volumetric Shapes. In *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*. IEEE.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2015b) Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang
    Zhang, Xiaoou Tang, 和 Jianxiong Xiao. 2015b. 3D ShapeNets: 体积形状的深度表示. 见 *IEEE
    计算机视觉与模式识别会议*。IEEE。'
- en: Xiang et al. (2015) Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese.
    2015. Data-driven 3d voxel patterns for object category recognition. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. IEEE, 1903–1911.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang et al. (2015) Yu Xiang、Wongun Choi、Yuanqing Lin 和 Silvio Savarese。2015年。数据驱动的3D体素模式用于对象类别识别。发表于*IEEE计算机视觉与模式识别会议论文集*。IEEE，1903–1911。
- en: Xie et al. (2015) Zhige Xie, Kai Xu, Wen Shan, Ligang Liu, Yueshan Xiong, and
    Hui Huang. 2015. Projective Feature Learning for 3D Shapes with Multi-View Depth
    Images. *Computer Graphics Forum* 34, 7 (2015), 1–11.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2015) Zhige Xie、Kai Xu、Wen Shan、Ligang Liu、Yueshan Xiong 和 Hui Huang。2015年。基于多视角深度图像的3D形状投影特征学习。*计算机图形学论坛*
    34，第7期（2015年），1–11。
- en: Xu and Todorovic (2016) Xu Xu and Sinisa Todorovic. 2016. Beam search for learning
    a deep convolutional neural network of 3d shapes. In *Pattern Recognition (ICPR),
    2016 23rd International Conference on*. IEEE, 3506–3511.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu and Todorovic (2016) Xu Xu 和 Sinisa Todorovic。2016年。用于学习3D形状深度卷积神经网络的束搜索。发表于*模式识别（ICPR），2016年第23届国际会议*。IEEE，3506–3511。
- en: Yan et al. (2018) Chenggang Yan, Hongtao Xie, Dongbao Yang, Jian Yin, Yongdong
    Zhang, and Qionghai Dai. 2018. Supervised hash coding with deep neural network
    for environment perception of intelligent vehicles. *IEEE transactions on intelligent
    transportation systems* 19, 1 (2018), 284–295.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan et al. (2018) Chenggang Yan、Hongtao Xie、Dongbao Yang、Jian Yin、Yongdong Zhang
    和 Qionghai Dai。2018年。基于深度神经网络的有监督哈希编码用于智能车辆的环境感知。*IEEE智能交通系统汇刊* 19，第1期（2018年），284–295。
- en: 'Yang et al. (2018) Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. 2018.
    FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation. In *Proc. IEEE
    Conf. on Computer Vision and Pattern Recognition (CVPR)*, Vol. 3. IEEE.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2018) Yaoqing Yang、Chen Feng、Yiru Shen 和 Dong Tian。2018年。FoldingNet：通过深度网格变形的点云自动编码器。发表于*IEEE计算机视觉与模式识别会议（CVPR）论文集*，第3卷。IEEE。
- en: Yi et al. (2016) Li Yi, Vladimir G Kim, Duygu Ceylan, I Shen, Mengyan Yan, Hao
    Su, Cewu Lu, Qixing Huang, Alla Sheffer, Leonidas Guibas, et al. 2016. A scalable
    active framework for region annotation in 3d shape collections. *ACM Transactions
    on Graphics (TOG)* 35, 6 (2016), 210.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi et al. (2016) Li Yi、Vladimir G Kim、Duygu Ceylan、I Shen、Mengyan Yan、Hao Su、Cewu
    Lu、Qixing Huang、Alla Sheffer、Leonidas Guibas 等。2016年。一种用于3D形状集合区域注释的可扩展主动框架。*ACM图形学汇刊（TOG）*
    35，第6期（2016年），210。
- en: Yin et al. (2008) Lijun Yin, Xiaochen Chen, Yi Sun, Tony Worm, and Michael Reale.
    2008. A high-resolution 3D dynamic facial expression database. In *Automatic Face
    & Gesture Recognition, 2008\. FG’08\. 8th IEEE International Conference on*. IEEE,
    1–6.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2008) Lijun Yin、Xiaochen Chen、Yi Sun、Tony Worm 和 Michael Reale。2008年。高分辨率3D动态面部表情数据库。发表于*自动面部与手势识别，2008年\.
    FG’08\. 第8届IEEE国际会议*。IEEE，1–6。
- en: Yin et al. (2006) Lijun Yin, Xiaozhou Wei, Yi Sun, Jun Wang, and Matthew J Rosato.
    2006. A 3D facial expression database for facial behavior research. In *Automatic
    face and gesture recognition, 2006\. FGR 2006\. 7th international conference on*.
    IEEE, 211–216.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2006) Lijun Yin、Xiaozhou Wei、Yi Sun、Jun Wang 和 Matthew J Rosato。2006年。用于面部行为研究的3D面部表情数据库。发表于*自动面部与手势识别，2006年\.
    FGR 2006\. 第7届国际会议*。IEEE，211–216。
- en: Yu et al. (2018) Tan Yu, Meng, Jingjing, Yuan, and Junsong. 2018. Multi-view
    Harmonized Bilinear Network for 3D Object Recognition. In *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*. IEEE, 186–194.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2018) Tan Yu、Meng、Jingjing、Yuan 和 Junsong。2018年。多视角协调双线性网络用于3D对象识别。发表于*IEEE计算机视觉与模式识别会议论文集*。IEEE，186–194。
- en: Zaheer et al. (2017) Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas
    Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. 2017. Deep sets. In *Advances
    in Neural Information Processing Systems*. Curran Associates, Inc., 3394–3404.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaheer et al. (2017) Manzil Zaheer、Satwik Kottur、Siamak Ravanbakhsh、Barnabas
    Poczos、Ruslan R Salakhutdinov 和 Alexander J Smola。2017年。深度集合。发表于*神经信息处理系统进展*。Curran
    Associates, Inc.，3394–3404。
- en: Zanuttigh and Minto (2017) P. Zanuttigh and L. Minto. 2017. Deep learning for
    3D shape classification from multiple depth maps. In *2017 IEEE International
    Conference on Image Processing (ICIP)*. IEEE, 3615–3619. [https://doi.org/10.1109/ICIP.2017.8296956](https://doi.org/10.1109/ICIP.2017.8296956)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zanuttigh and Minto (2017) P. Zanuttigh 和 L. Minto。2017年。基于多深度图的3D形状分类的深度学习。发表于*2017年IEEE国际图像处理会议（ICIP）*。IEEE，3615–3619。
    [https://doi.org/10.1109/ICIP.2017.8296956](https://doi.org/10.1109/ICIP.2017.8296956)
- en: Zhang et al. (2007) Lisha Zhang, M João da Fonseca, Alfredo Ferreira, and Combinando
    Realidade Aumentada e Recuperação. 2007. Survey on 3D shape descriptors. *FundaÃgao
    para a Cincia ea Tecnologia, Lisboa, Portugal, Tech. Rep. Technical Report, DecorAR
    (FCT POSC/EIA/59938/2004)* 3 (2007).
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2007）Lisha Zhang, M João da Fonseca, Alfredo Ferreira, 和 Combinando
    Realidade Aumentada e Recuperação。2007。3D 形状描述符的综述。*FundaÃgao para a Cincia ea
    Tecnologia, Lisboa, Portugal, Tech. Rep. Technical Report, DecorAR (FCT POSC/EIA/59938/2004)*
    3（2007）。
- en: 'Zhang et al. (2014) Xing Zhang, Lijun Yin, Jeffrey F Cohn, Shaun Canavan, Michael
    Reale, Andy Horowitz, Peng Liu, and Jeffrey M Girard. 2014. Bp4d-spontaneous:
    a high-resolution spontaneous 3d dynamic facial expression database. *Image and
    Vision Computing* 32, 10 (2014), 692–706.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2014）Xing Zhang, Lijun Yin, Jeffrey F Cohn, Shaun Canavan, Michael Reale,
    Andy Horowitz, Peng Liu, 和 Jeffrey M Girard。2014。Bp4d-spontaneous：一个高分辨率自发 3D
    动态面部表情数据库。*Image and Vision Computing* 32, 10（2014），692–706。
- en: Zhang et al. (2016) Zheng Zhang, Jeff M Girard, Yue Wu, Xing Zhang, Peng Liu,
    Umur Ciftci, Shaun Canavan, Michael Reale, Andy Horowitz, Huiyuan Yang, et al.
    2016. Multimodal spontaneous emotion corpus for human behavior analysis. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. IEEE, 3438–3446.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2016）Zheng Zhang, Jeff M Girard, Yue Wu, Xing Zhang, Peng Liu, Umur
    Ciftci, Shaun Canavan, Michael Reale, Andy Horowitz, Huiyuan Yang, 等。2016。用于人类行为分析的多模态自发情感语料库。在
    *IEEE 计算机视觉与模式识别会议论文集* 中。IEEE，3438–3446。
- en: 'Zhao et al. (2017) Jing Zhao, Xijiong Xie, Xin Xu, and Shiliang Sun. 2017.
    Multi-view learning overview: Recent progress and new challenges. *Information
    Fusion* 38 (2017), 43–54.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2017）Jing Zhao, Xijiong Xie, Xin Xu, 和 Shiliang Sun。2017。多视角学习概述：最近的进展和新挑战。*Information
    Fusion* 38（2017），43–54。
- en: 'Zhi et al. (2018) Shuaifeng Zhi, Yongxiang Liu, Xiang Li, and Yulan Guo. 2018.
    Toward real-time 3D object recognition: A lightweight volumetric CNN framework
    using multitask learning. *Computers & Graphics* 71 (2018), 199 – 207. [https://doi.org/10.1016/j.cag.2017.10.007](https://doi.org/10.1016/j.cag.2017.10.007)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhi 等（2018）Shuaifeng Zhi, Yongxiang Liu, Xiang Li, 和 Yulan Guo。2018。面向实时 3D
    物体识别：一种使用多任务学习的轻量级体积 CNN 框架。*Computers & Graphics* 71（2018），199 – 207。 [https://doi.org/10.1016/j.cag.2017.10.007](https://doi.org/10.1016/j.cag.2017.10.007)
- en: Zhu et al. (2016) Zhuotun Zhu, Xinggang Wang, Song Bai, Cong Yao, and Xiang
    Bai. 2016. Deep learning representation using autoencoder for 3D shape retrieval.
    *Neurocomputing* 204 (2016), 41–50.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2016）Zhuotun Zhu, Xinggang Wang, Song Bai, Cong Yao, 和 Xiang Bai。2016。使用自编码器的深度学习表示用于
    3D 形状检索。*Neurocomputing* 204（2016），41–50。
- en: Zollhöfer et al. (2014) Michael Zollhöfer, Matthias Nießner, Shahram Izadi,
    Christoph Rehmann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon,
    Charles Loop, Christian Theobalt, et al. 2014. Real-time non-rigid reconstruction
    using an RGB-D camera. *ACM Transactions on Graphics (TOG)* 33, 4 (2014), 156.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zollhöfer 等（2014）Michael Zollhöfer, Matthias Nießner, Shahram Izadi, Christoph
    Rehmann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles
    Loop, Christian Theobalt, 等。2014。使用 RGB-D 相机的实时非刚性重建。*ACM Transactions on Graphics
    (TOG)* 33, 4（2014），156。
