- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:07:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[1808.01462] A survey on Deep Learning Advances on Different 3D Data Representations'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1808.01462](https://ar5iv.labs.arxiv.org/html/1808.01462)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A survey on Deep Learning Advances on Different 3D Data Representations
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Eman Ahmed [1234-5678-9012-3456](https://orcid.org/1234-5678-9012-3456 "ORCID
    identifier") SnT, University of Luxembourg [eman.ahmed@uni.lu](mailto:eman.ahmed@uni.lu)
    ,  Alexandre Saint SnT, University of Luxembourg [alexandre.saint@uni.lu](mailto:alexandre.saint@uni.lu)
    ,  Abdelrahman Shabayek SnT, University of Luxembourg [abdelrahman.shabayek@uni.lu](mailto:abdelrahman.shabayek@uni.lu)
    ,  Kseniya Cherenkova SnT, University of Luxembourg; Artec group, Luxembourg [kcherenkova@artec-group.com](mailto:kcherenkova@artec-group.com)
    ,  Rig Das SnT, University of Luxembourg [rig.das@uni.lu](mailto:rig.das@uni.lu)
    ,  Gleb Gusev Artec groupLuxembourg [gleb@artec-group.com](mailto:gleb@artec-group.com)
     and  Djamila Aouada SnT, University of Luxembourg [djamila.aouada@uni.lu](mailto:djamila.aouada@uni.lu)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Abstract: 3D data is a valuable asset the computer vision filed as it provides
    rich information about the full geometry of sensed objects and scenes. Recently,
    with the availability of both large 3D datasets and computational power, it is
    today possible to consider applying deep learning to learn specific tasks on 3D
    data such as segmentation, recognition and correspondence. Depending on the considered
    3D data representation, different challenges may be foreseen in using existent
    deep learning architectures. In this work, we provide a comprehensive overview
    about various 3D data representations highlighting the difference between Euclidean
    and non-Euclidean ones. We also discuss how Deep Learning methods are applied
    on each representation, analyzing the challenges to overcome.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: \useunder
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: \ul
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 0.0.1\. Deep learning architectures on multi-view data
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite the effectiveness of volumetric deep learning methods, most of these
    approaches are computationally expensive because of the volumetric nature of the
    convolutional filters to extract the features which increase the computational
    complexity cubically with respect to the voxels resolution which limits the usage
    of 3D volumetric DL models. That is why exploiting multi-views of 3D objects is
    practical. Indeed, it enables exploiting the already established 2D DL paradigms
    without the need to design a tailored model for 3D volumetric data with high computational
    complexity as shown in pipeline illustrated in Fig. LABEL:fig:taxonomy. One of
    the first attempts to exploit 2D DL models for learning multi-view 3D data was
    presented by Leng et at. in (Leng et al., [2014](#bib.bib82)) where DBN was employed
    on various view-based depth images to extract high-level features of the 3D object.
    A later-wise training manner was used to train the DBN using the “Contractive
    Divergence” method. The proposed model produced better results than the composite
    descriptors approach employed in (Daras and Axenopoulos, [2010](#bib.bib36)).
    Xie et al. (Xie et al., [2015](#bib.bib146)) proposed “Multi-View Deep Extreme
    Learning Machine (MVD-ELM)”. The proposed MVD-ELM was employed on 20 multi-view
    depth images that were uniformly captured with a sphere at the centre of the 3D
    object. The proposed MVD-ELM contained Conv layers that had shared weights across
    all the views. The output activation weights were optimized according to the feature
    maps extracted. This work has been extended to be Fully Convolutional, resulting
    in (FC-MVD-ELM). FC-MVD-ELM was trained using the multi-view depth images to be
    tested for 3D segmentation. The predicted labels from the training stage were
    then projected back to the 3D object where the final result was smoothed using
    the graph cut optimization method. Both MVD-ELM and FC-MVD-ELM were tested on
    3D shape classification and segmentation tasks and outperformed the previous work (Wu
    et al., [2015a](#bib.bib143)) and reduced the processing time significantly.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管体积深度学习方法有效，但大多数这些方法由于卷积滤波器的体积特性而计算开销巨大，这种特性使得计算复杂度随着体素分辨率立方增长，从而限制了3D体积深度学习模型的使用。这就是为什么利用3D对象的多视角是实际可行的。事实上，它使得利用已建立的2D深度学习范式成为可能，而无需设计针对3D体积数据的高计算复杂度的专门模型，如在图示的流程图中所示（见
    Fig. LABEL:fig:taxonomy）。Leng 等人首次尝试利用2D深度学习模型学习多视角3D数据（见 Leng et al., [2014](#bib.bib82)），他们在各种基于视角的深度图像上应用了
    DBN，以提取3D对象的高层特征。采用了渐进式训练方式来训练 DBN，使用了“收缩散度”方法。所提出的模型比在 Daras 和 Axenopoulos（见
    [2010](#bib.bib36)）中使用的复合描述符方法表现更好。Xie 等人（见 Xie et al., [2015](#bib.bib146)）提出了“多视角深度极限学习机（MVD-ELM）”。所提出的
    MVD-ELM 应用于20张均匀捕获的多视角深度图像，这些图像围绕3D对象的中心球体捕获。所提出的 MVD-ELM 包含在所有视角间共享权重的卷积层。输出激活权重根据提取的特征图进行优化。这项工作已经扩展为完全卷积的形式，
    resulting in（FC-MVD-ELM）。FC-MVD-ELM 使用多视角深度图像进行训练，以进行3D分割测试。训练阶段的预测标签随后被投影回3D对象，并使用图割优化方法对最终结果进行平滑处理。MVD-ELM
    和 FC-MVD-ELM 都在3D形状分类和分割任务上进行了测试，并超越了先前的工作（见 Wu et al., [2015a](#bib.bib143)），显著减少了处理时间。
- en: 'More research investigations were carried by Leng et al. to employ DL paradigms
    on multi-view 3D data. Leng et al. in (Leng et al., [2015a](#bib.bib80)) proposed
    an extension of classical AEs in a similar way to the CNN architecture. Their
    proposed framework is called “Stacked Local Convolutional AutoEncoders (SLCAE)”.
    SLCAE operated on multiple multi-view depth images of the 3D object. In SLCAE,
    FC layers were substituted by layers that were connected locally with the use
    of the convolution operation. Multiple AEs ere stacked where the output of the
    last AE was used as a final representation of the 3D object. Experiments on different
    datasets: SHREC’09, NTU and PSB proved the capabilities of this model. As an extension
    to the previous work, Leng et al. proposed a “3D Convolutional Neural Network
    (3DCNN)” to simultaneously process different 2D views of the 3D object (Leng et al.,
    [2015b](#bib.bib81)). Different views are sorted in a specific order to guarantee
    that all the objects’ views follow the same convention while training. The proposed
    3DCNN is composed of four Conv layers, three sub-sampling layers and two FC layers.
    The proposed network was tested for retrieval task on the same datasets used for
    testing (Leng et al., [2015a](#bib.bib80)). However, the results showed that the
    later model performed better on the three datasets which implies that the previous
    model was able to learn more discriminative features to represent various 3D objects.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Leng 等人进行了更多的研究，利用深度学习（DL）范式处理多视角 3D 数据。在 (Leng et al., [2015a](#bib.bib80))
    中，Leng 等人提出了一种类似于卷积神经网络（CNN）架构的经典自编码器（AEs）扩展。他们提出的框架称为“堆叠局部卷积自编码器（SLCAE）”。SLCAE
    在 3D 物体的多个多视角深度图像上运行。在 SLCAE 中，全连接层被通过卷积操作局部连接的层所替代。多个自编码器被堆叠，其中最后一个自编码器的输出作为
    3D 物体的最终表示。在不同数据集上的实验：SHREC’09、NTU 和 PSB 证明了该模型的能力。作为对先前工作的扩展，Leng 等人提出了一种“3D
    卷积神经网络（3DCNN）”来同时处理 3D 物体的不同 2D 视图 (Leng et al., [2015b](#bib.bib81))。不同视图按特定顺序排序，以确保所有物体的视图在训练时遵循相同的规范。提出的
    3DCNN 由四个卷积层、三个子采样层和两个全连接层组成。提出的网络在与测试相同的数据集上进行了检索任务的测试 (Leng et al., [2015a](#bib.bib80))。然而，结果显示，后者模型在三个数据集上表现更好，这意味着先前的模型能够学习到更多区分特征来表示不同的
    3D 物体。
- en: A novel “Multi-View CNN (MVCNN)” was proposed by Su et al. in (Su et al., [2015](#bib.bib125))
    for 3D object retrieval and recognition/classification tasks. In contrast to Leng’s
    model in (Leng et al., [2015b](#bib.bib81)), MVCNN processed multiple views for
    the 3D objects in no specific order using a view pooling layer. [1](#S0.F1 "Figure
    1 ‣ 0.0.1\. Deep learning architectures on multi-view data ‣ A survey on Deep
    Learning Advances on Different 3D Data Representations") shows the full architecture
    of the model. Two different setups to capture the 3D objects multi-views were
    tested. The first one rendered 12 views for the object by placing 12 equidistant
    virtual cameras surrounding the object while the other setup included 80 virtual
    views. MVCNN was pre-trained using $ImageNet1K$ dataset and fine-tuned on $ModelNet40$ (Wu
    et al., [2015a](#bib.bib143)). The proposed network has two parts, the first part
    is where the object’s views are processed separately and the second part is where
    the max pooling operation is taking place across all the processed views in the
    view-pooling layer, resulting in a single compact representation for the whole
    3D shape. In the view-pooling layer, the view with the maximal activation is the
    only one considered while ignoring all the other views with non-maximal activations.
    This means that only few views are contributing towards the final representation
    of the shape which causes a loss of the visual information. To overcome this problem,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Su 等人在 (Su et al., [2015](#bib.bib125)) 中提出了一种新颖的“多视角卷积神经网络（MVCNN）”用于 3D 物体的检索和识别/分类任务。与
    Leng 的模型 (Leng et al., [2015b](#bib.bib81)) 相比，MVCNN 处理 3D 物体的多个视角不按特定顺序，使用视图池化层。[1](#S0.F1
    "Figure 1 ‣ 0.0.1\. Deep learning architectures on multi-view data ‣ A survey
    on Deep Learning Advances on Different 3D Data Representations") 展示了模型的完整架构。测试了捕捉
    3D 物体多视角的两种不同设置。第一种通过将 12 个等距虚拟摄像头环绕物体来渲染 12 个视图，而另一种设置包括 80 个虚拟视图。MVCNN 使用 $ImageNet1K$
    数据集进行预训练，并在 $ModelNet40$ 上进行了微调 (Wu et al., [2015a](#bib.bib143))。提出的网络分为两部分，第一部分是处理物体视图，第二部分是视图池化层中进行的最大池化操作，结果是
    3D 形状的单一紧凑表示。在视图池化层中，只有激活最大的视图被考虑，而忽略所有其他激活不最大化的视图。这意味着只有少数视图对形状的最终表示有贡献，这导致了视觉信息的丢失。为了解决这个问题，
- en: Experiments showed that the MVCNN with the max-view pooling layer outperformed
    ShapeNet (Wu et al., [2015a](#bib.bib143)) on classification and retrieval tasks
    by a remarkable margin. In (Johns et al., [2016](#bib.bib68)) Johns et al. exploited
    multi-view data representation using CNNs by representing 3D objects under unconstrained
    camera trajectories with a set of 2D image pairs. The proposed method classifies
    each pair separately and then weight the contribution of each pair to get the
    final result. The VGG-M architecture was adopted in this framework consists of
    five Conv layers and three FC layers. The views of the 3D objects are represented
    as either depth images or grayscale images or both. This model outperformed MVCNN
    proposed by Su et al. (Su et al., [2015](#bib.bib125)) and voxel-based ShapeNet
    architectures (Wu et al., [2015a](#bib.bib143)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'The efficiency of multi-view DL models pushed researchers to investigate more
    GPU-based methods to learn multi-view 3D data features. This is what pushed Bai
    et al. (Bai et al., [2016](#bib.bib10)) to propose a real-time GPU-based CNN search
    engine for multi 2D-views of 3D objects. The proposed model called $GIFT$ utilizes
    two files that are inverted: the first is to accelerate process of the multi-view
    matching and the second one is to rank the initial results. The processed query
    is completed within one second. $GIFT$ was tested on a set of various datasets:
    ModelNet, $PSB$, $SHREC14LSGTB$, $McGill$ and $SHREC^{\prime}07$ watertight models.
    GIFT produced a better performance compared to the state-of-the-art methods.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The efforts to learn multi-view 3D data representations kept evolving and in (Zanuttigh
    and Minto, [2017](#bib.bib155)), Zanuttigh and Minto proposed a multi-branch CNN
    for classifying 3D objects. The input to this model is rendered depth maps from
    different view points for the 3D object. Each CNN branch consists of five Conv
    layers to process one depth map producing a classification vector. The resulted
    classification vectors are the input to a linear classifier to identify the 3D
    object’s category/class. The proposed model produced comparable results to the
    state-of-the-art. Based on the dominant sets, Want et al. in (Wang et al., [2017b](#bib.bib135))
    proposed recurrent view-clustering and pooling layers . The key concept in this
    model is to pool similar views and recurrently cluster them to build a pooled
    feature vector. Then, the constructed pooled feature vectors are fed as inputs
    in the same layer in a recurrent training fashion in the recurrent clustering
    layer. Within this layer, a view similarity graph is computed whose nodes represent
    the feature vectors and the edges represent the similarity weights between the
    views. Within the constructed graph, the similarities and dissimilarities between
    different views are exhibited which is very effective in the 3D shape recognition
    task. The proposed model achieved a highly comparable results to previous methods
     (Wu et al., [2015a](#bib.bib143); Su et al., [2015](#bib.bib125)) as shown in
    Table 1 in the supplementary material. Driven the advances in the multi-view DL
    models, Qi et al. (Qi et al., [2016b](#bib.bib101)) provided a comparison study
    between multi-view DL techniques and volumetric DL techniques for the object recognition
    task. As part of the study, the authors proposed a $Sphererendering$ approach
    for filtering multi-resolution 3D objects at multiple scales. With data augmentation,
    the authors managed to enhance the results of MVCNNs on $ModelNet40$. Recently,
    Kanezaki et at. (Kanezaki et al., [2016](#bib.bib70)) achieved state-of-the-art
    results on both $ModelNet10$ and $ModelNet40$ in the classification problem using
    $RotationNet$. $RotationNet$ trains a set of multi-view images for the 3D object
    but doesn’t require all the views at once. Instead, it allows for sequential input
    and updates the likelihood of the object’s category accordingly.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 学习多视角三维数据表示的努力不断发展，2017年，Zanuttigh 和 Minto提出了一种用于分类三维物体的多分支CNN。该模型的输入是来自不同视角的渲染深度图像。每个CNN分支包括五个卷积层来处理一个深度图像，生成一个分类向量。产生的分类向量是线性分类器的输入，用于识别三维物体的类别。该提出的模型产生了与最先进技术相媲美的结果。基于主导集合，Wang等人在2017年提出了递归视图聚类和汇聚层。该模型的关键概念是汇集相似视角并通过递归地对其进行聚类来构建一个汇集特征向量。然后，构建的汇集特征向量以递归训练的方式作为输入传入递归聚类层中的同一层。在该层内，计算视图相似性图，其中节点表示特征向量，边表示视图之间的相似度权重。在构建的图中展示了不同视图之间的相似性和不同性，这对于三维形状识别任务非常有效。所提出的模型在表现上达到了与以往方法高度相当的结果，如附录中的表1所示。在多视角深度学习模型的推动下，Qi等人在2016年提供了多视角深度学习技术和体积式深度学习技术的比较研究，用于目标识别任务。作为研究的一部分，作者提出了一个$Sphererendering$方法，用于在多个尺度上过滤多分辨率三维物体。通过数据增强，作者成功地提升了MVCNNs在$ModelNet40$上的结果。最近，Kanezaki等人利用$RotationNet$在$ModelNet10$和$ModelNet40$上实现了最先进的分类结果。$RotationNet$训练了一组多视角图像用于三维物体，但不需要同时获取所有视图。相反，它允许顺序输入并相应地更新物体类别的可能性。
- en: Multi-view representation proved to perform slightly better than volumetric
    representation with less computational power needed. However, there are some challenges
    imposed with this representation. The sufficient number of views and the way they
    were acquired is a critical factor for representing the 3D shape. Also, the multi-view
    representation does not preserve the intrinsic geometric properties of the 3D
    shape. This is what pushed towards defining new notion of convolution operating
    on 3D shapes to capture their intrinsic properties.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角表示法表现比体积表示法稍好，且需要较少的计算资源。然而，这种表示法也带来了一些挑战。视角的充分数量及其获取方式对于表达三维形状至关重要。此外，多视角表示法无法保留三维形状的固有几何属性。这促使人们定义新的卷积概念，以捕捉其固有属性。
- en: '![Refer to caption](img/e842147a9abe819972a1e580b92fa755.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e842147a9abe819972a1e580b92fa755.png)'
- en: Figure 1\. MVCNN architecture (Su et al., [2015](#bib.bib125)) applied on multi-view
    of 3D objects without a specific order. The figure reused from (Su et al., [2015](#bib.bib125))
    with permission of authors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. MVCNN架构（Su 等，[2015](#bib.bib125)）应用于无特定顺序的三维对象多视角。图源自（Su 等，[2015](#bib.bib125)），已获得作者许可。
- en: 0.0.2\. Deep learning architectures on hybrid data representations
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 0.0.2\. 深度学习架构在混合数据表示上的应用
- en: 'Some efforts towards combining various 3D data representations to exploit the
    advantages that each representation brings. Recently, Wang et al. proposed  (Wang
    et al., [2015](#bib.bib137)), where each of the 3D objects are represented by
    a pair of multi-views and 2D sketches. The learning model is composed of Siamese
    CNNs where are two identical sub-convolutional networks; one for processing the
    multi-views input and the other one is for processing the 2D sketches. Each of
    these network consisted of three Conv layers where each of them was succeeded
    by a max-pooling layer and a Fully Connected Layer. The networks were trained
    separately using the “Stochastic Gradient Descent (SGD)” method. The proposed
    model was tested on SHREC’13 dataset for 3D shape retrieval and achieved competitive
    results compared to previous methods. Wang et al. continued their investigations
    for hybrid 3D data representations and in (Wang et al., [2016](#bib.bib139)),
    the authors proposed the “Convolutional Auto-Encoder Extreme Learning Machine
    (CAE-ELM)” 3D descriptor which merges the learning power of ConvNets, AEs and
    “Extreme Learning Machine (ELM)” (Huang et al., [2006](#bib.bib65)). ELM is an
    efficient unsupervised learning technique which learns high-level discriminative
    features about the input data. ELM is faster than most of DL models (Kasun et al.,
    [2013](#bib.bib71)) which is practical for processing large-scale 3D data. The
    input to the CAE-ELM architecture is two data representations: voxel data and
    “Signed Distance Field data (SDF)”. Voxel data explains the structure of the 3D
    object while the SDF extracts local and global features about the 3D object. CAE-ELM
    was tested on $ModelNet40$ and $ModelNet10$ datasets for classification tasks
    and achieved a superior performance compared to previous methods. CAE-ELM is considered
    as a hybrid approach that exploits the structure of the 3D objects in addition
    to 3D descriptors. Ben-Shabat et al. (Ben-Shabat et al., [2017](#bib.bib12)) proposed
    a novel 3D point cloud representation named “3D Modified Fisher Vectors (3DmFV)”
    which is a kind of DL model that uses a hybrid data representation of the discrete
    structure of a grid with the continuous generalization of Fisher vectors to represent
    the 3D data. The hybrid input data is processed using deep ConvNet for classification
    and part segmentation tasks. 3DmFV has two modules: the initial one changes the
    input point cloud into a 3D modified “Fisher vector (FV)” which can be considered
    as a descriptor-based representation and the second one is the DL module represented
    in the CNN. FV data representation empowers the proposed framework to become order,
    input data sample size and structure invariant. The network architecture is composed
    of an inception module (Szegedy et al., [2017](#bib.bib129)), max-pooling layers
    and four FC layers on top. this network has achieved better results in comparison
    with state-of-the-art techniques.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the performance of 3D convolution and 2D multi-view CNNs, some work
    examined the fusion of both representations. Hedge and Zadeh (Hegde and Zadeh,
    [2016](#bib.bib62)), proposed to fuse the volumetric representations (voxels)
    and 2D representation (2D views) for the object classification task. Authors have
    examined different models for processing the data for $ModelNet$ classification.
    The model one was a combination of both modalities (3D and 2D) while using two
    3D CNNs for processing the 3D voxels and AlexNet for processing the 2D views.
    The other experiments were carried by processing the 3D voxels only and compare
    it with the performance of AlexNet on the 2D views. Experiments showed that that
    network that combined both modalities called $FusionNet$ performed the best. However,
    the multi-view network performed better than the volumetric CNNs. Although Brock
    et al. in  (Brock et al., [2016](#bib.bib16)) has achieved a state-of-the-art
    results on $ModelNet$ classification, $FusionNet$ achieved comparable results
    with no need for the data augmentation or the very heavy computations needed in
    Brock’s model. This implies that practically employing 2D and 3D representations
    surpass the volumetric methods with less computations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 0.1\. Deep learning architectures on 3D non-Euclidean structured data
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second type of 3D DL approaches is the non-Euclidean approaches that try
    to extend the DL concept to geometric data. However, the nature of the data is
    imposing challenges on how to perform the main DL operations such as convolution.
    A number of architectures that tried to extend DL to the 3D geometric domain were
    proposed. Some of these architectures addresses “3D point clouds”, in order to
    learn the geometry of a 3D shape and use it for modeling tasks. Results encouraged
    researchers to leverage the surface information provided in 3D meshes where the
    connectivity between vertices can be exploited to define local pseudo-coordinates
    to perform a convolution-like operation on 3D meshes. At the same time, some efforts
    were directed towards investigating graph-based 3D data attempting to leverage
    the spectral properties of graphs to define intrinsic descriptors to be used in
    the DL framework. In this section, we will go over the latest innovations in applying
    DL on non-Euclidean 3D data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 0.1.1\. Point clouds
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Point clouds provide an expressive, homogeneous and compact representation of
    the 3D surface geometry without the combinatorial irregularities and complexities
    of meshes. That is why point clouds are easy to learn from. However, processing
    point clouds is tricky due to their dual nature. Point clouds can be seen as a
    Euclidean-structured data locally when considering a point with respect to its
    neighborhood (a subset of points) such that the interaction among the points forms
    a Euclidean space with a distance metric which is invariant to transformations
    like translation, rotation. However, considering the global structure of the point
    cloud, it is an unordered set of points with no specific order which imposes the
    irregular non-Euclidean nature on the global structure of the data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 点云提供了一种表达性、同质且紧凑的3D表面几何表示，没有网格的组合不规则性和复杂性。这就是为什么点云容易学习的原因。然而，由于点云的双重性质，处理点云很棘手。点云可以在考虑点及其邻域（点的子集）时被视为具有欧几里得结构的数据，这样点之间的交互形成一个具有距离度量的欧几里得空间，对诸如平移、旋转等变换不变。然而，考虑到点云的全局结构，它是一个没有特定顺序的无序点集，这对数据的全局结构施加了不规则的非欧几里得特性。
- en: Some recent works have considered point clouds as a collection of sets with
    different sizes. Vinyals et al. in (Vinyals et al., [2015](#bib.bib134)) use a
    read-process-write network for processing point sets to show the network ability
    to learn how to sort numbers. This was a direct application of DL on an unordered
    set for the Natural Language Processing (NLP) application. Inspired by this work,
    Ravanbakhsh et al. (Ravanbakhsh et al., [2016](#bib.bib104)), proposed what they
    called the permutation equivariant layer within a supervised and semi-supervised
    settings. This layer is obtained by parameter-sharing to learn the permutation
    invariance as well as rigid transformations across the data. This network performed
    3D classification and MNIST digit addition. Although this network is relatively
    simple, it didn’t perform well on the 3D classification task on ModelNet dataset.
    A deeper version of this model was extended in (Zaheer et al., [2017](#bib.bib154))
    where the proposed $DeepSet$ framework produced better results than the state-of-the-art
    methods in 3D classification job on $ModelNet$ dataset. This is due to the permutation
    invariance property that the permutation equivarant layer is bringing to the previous
    models. (Qi et al., [2016a](#bib.bib99)) also used a similar layer with a major
    difference as the permutation equivariant layer is max-normalized.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究考虑了点云作为具有不同大小集合的集合。Vinyals 等人在 (Vinyals et al., [2015](#bib.bib134)) 中使用了一个读取-处理-写入网络来处理点集，以展示网络学习如何排序数字的能力。这是深度学习在自然语言处理
    (NLP) 应用中的无序集合的直接应用。受此工作的启发，Ravanbakhsh 等人 (Ravanbakhsh et al., [2016](#bib.bib104))
    提出了他们所称的置换等变层，在监督和半监督设置中。这一层通过参数共享来学习置换不变性以及跨数据的刚性变换。该网络进行了3D分类和MNIST数字加法。尽管该网络相对简单，但在
    ModelNet 数据集上的3D分类任务表现不佳。在 (Zaheer et al., [2017](#bib.bib154)) 中，提出了 $DeepSet$
    框架的更深版本，在 $ModelNet$ 数据集上的3D分类任务中比最先进的方法取得了更好的结果。这是由于置换等变层为先前模型带来的置换不变性属性。(Qi
    et al., [2016a](#bib.bib99)) 也使用了类似的层，但主要区别在于置换等变层是最大值归一化的。
- en: '$PointNet$ (Qi et al., [2017a](#bib.bib100)) is the pioneer in making a direct
    use of the point cloud as an input where each of its points is represented using
    the $(x,y,z)$ coordinates. As a pre-processing step, feature transformation and
    inputs are feeded into the $PointNet$ architecture. $PointNet$ is composed of
    three main modules: a “Spatial Transformer Network (STN)” module, an RNN module
    and a simple symmetric function that aggregates all the information from each
    point in the point cloud. The STN canonicalizes the data before feeding them to
    the RNN, i.e., process all the data into one canonical form, and learns the key
    points of the point cloud which approximately corresponds to the skeleton of the
    3D object. Then comes the RNN module which learns the point cloud like a sequential
    signal of points and while training this model with some randomly permuted sequence,
    this RNN becomes invariant to the sequence of the input order of the point cloud’s
    point. Lastly, the network aggregates all the resulted point features using the
    max-pooling operation which is also permutation invariant. $PointNet$ proved that
    it is robust against partial data and input perturbation. It was tested on classification
    and segmentation tasks where it proved to produce results comparable to the state-of-the-art
    as shown the supplementary material, Table 1.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: $PointNet$ （齐等人，[2017a](#bib.bib100)）是将点云直接作为输入的先驱，其中每个点使用$(x,y,z)$坐标表示。作为预处理步骤，特征变换和输入被馈送到$PointNet$架构中。$PointNet$由三个主要模块组成：一个“空间变换网络（STN）”模块，一个RNN模块和一个简单的对称函数，聚合点云中每个点的所有信息。STN在将数据馈送到RNN之前将数据规范化，即将所有数据处理成一个规范形式，并学习点云的关键点，这大致对应于3D对象的骨架。然后是RNN模块，它像处理序列信号一样学习点云，并在使用一些随机排列序列训练该模型时，该RNN对点云点的输入顺序不变。最后，网络使用最大池化操作聚合所有生成的点特征，这也是置换不变的。$PointNet$证明了它对部分数据和输入扰动具有鲁棒性。它在分类和分割任务上进行了测试，在这些任务中，它证明可以产生与最先进技术相媲美的结果，如补充材料表1所示。
- en: Despite the competitive results achieved by $PointNet$, it is not able to take
    full advantage of the point’s local structure to capture the detailed fine-grained
    patterns because of aggregating all the point features together. To address this
    point, PointNet++ (Qi et al., [2017b](#bib.bib102)) builds on $PointNet$ by recursively
    applying it to a nested partition of the input point sets. Despite capturing more
    features, the resulted architecture is complicated which increases the size of
    the higher features and the computational time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管$PointNet$取得了竞争性的结果，但由于聚合所有点特征，无法充分利用点的局部结构来捕获详细的细粒度模式。为了解决这一点，PointNet++ （齐等人，[2017b](#bib.bib102)）通过递归地将其应用于输入点集的嵌套分区，构建在$PointNet$的基础上。尽管捕获了更多特征，结果的架构变得复杂，增加了更高级特征和计算时间的大小。
- en: Instead of operating directly on the point clouds structure, Kd-Networks by
    Klokov et al. (Klokov and Lempitsky, [2017](#bib.bib75)) proposes to impose a
    kd-tree structure of the input point cloud to be used for learning the shared
    weights across the points of the tree. Kd-tree is a feed-forward network that
    has learnable parameters associated with the weights of the nodes in the tree.
    This model was tested for shape classification, shape retrieval and shape part
    segmentation producing competitive results. Following the same concept of not
    working directly on the point cloud structure, Roveri et al. (Roveri et al., [2018](#bib.bib106))
    proposed to extract a set of 2D depth maps from different views of the point cloud
    and process them using Residual Nets ($ResNet50$) (He et al., [2015](#bib.bib59)).
    The proposed framework contains 3 modules. The initial module is responsible of
    learning $k$ directional views of the input point cloud to generate the depth
    maps accordingly in the second module. The third and last module is processing
    the generated $k$ depth maps for object classification. The innovation of this
    framework is mainly focus on automatically transforming un-ordered point clouds
    to informative 2D depth maps without the need to adapt the network module to account
    for permutation invariance and different transformations of the input data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: In recent past, few articles have reported their work on unsupervised learning
    over the point clouds. In $FoldingNet$ (Yang et al., [2018](#bib.bib149)), Yang
    et al. proposed to use AE for modelling different 3D objects represented as point
    clouds by a novel folding-based decoder that deforms a 2D canonical grid into
    the underlying surface of the 3D point cloud. $FoldingNet$ is able to learn how
    to generate cuts on the 2D grid to create 3D surfaces and generalize to some intra-class
    variations of the same 3D object class. An SVM was used on top of this $FoldingNet$
    to be used for 3D classification where it proved to perform well with the learned
    discriminative representation for different 3D objects. FoldingNet achieved high
    classification accuracy on $ModelNet40$. Another unsupervised model was proposed
    by Li et al. called SO-Net (Li et al., [2018](#bib.bib83)). SO-Net is a permutation
    invariant network that can tolerate unordered point clouds inputs. SO-Net builds
    that spatial distribution of the points in the point cloud using “Self-Organizing
    Maps (SOMs)”. Then, a hierarchical feature extraction on the points of the point
    cloud and the SOM nodes is employed which results in a singular feature vector
    which represents the entire point cloud Local feature aggregation happens according
    to an adjustable receptive field where the overlap is controlled to get more effective
    features. SO-Net was tested on classification and segmentation tasks producing
    promising results highly comparable with the state-of-the-art techniques as shown
    in Table 1 of the supplementary material.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: As noticed in all the previously proposed methods, the main problem in processing
    point clouds is the un-ordered structure of this representation where researchers
    are trying to make the learning process invariant to the order of the point cloud.
    Most of these methods resorted to clustering techniques to opt for similar points
    and process them together.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所有先前提出的方法中所注意到的，处理点云的主要问题是其表示的无序结构，研究人员正在努力使学习过程对点云的顺序不变。大多数这些方法依赖于聚类技术，以选择相似的点并一起处理。
- en: 0.1.2\. Graphs and meshes
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 0.1.2\. 图和网格
- en: The ideal representation for graphs and meshes is the one that can capture all
    the intrinsic structure of the object and also can be learned with the gradient
    descent methods. This is due to their stability and frequent usage in the CNNs.
    However, learning such irregular representations is a challenging task due to
    the structural properties of these representations. Motivated by the success of
    CNNs in a broad range of computer vision tasks, recent research efforts were directed
    towards generalizing CNNs to such irregular structures. Analyzing the properties
    of such data shows that meshes can be converted to graphs as discussed in Section LABEL:dataRep_3D_meshes_and_graphs..
    Hence, the models proposed for graphs can be employed on mesh-structured data
    but not vice versa. Most of the existing work addresses the graph-structured data
    explicitly with some few works were tailored towards mesh representations. We
    herein overview recent works on each representation providing a broad classification
    for the existing methods based on the used approach.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图和网格的理想表示方法是能够捕捉对象所有固有结构并且能够通过梯度下降方法学习的表示方法。这是由于它们在卷积神经网络中的稳定性和频繁使用所致。然而，学习这些不规则表示是一项具有挑战性的任务，这是由于这些表示的结构特性。受卷积神经网络在广泛的计算机视觉任务中的成功启发，最近的研究工作集中于将卷积神经网络推广到这种不规则结构上。分析这些数据的特性表明，网格可以转换为图，如第
    LABEL:dataRep_3D_meshes_and_graphs 节所述。因此，提出的图模型可以应用于网格结构数据，但反之则不行。现有工作大部分明确地处理图结构数据，少数工作专门针对网格表示进行了定制。在此，我们概述了每种表示的最新工作，并基于使用的方法对现有方法进行了广泛分类。
- en: 'Graphs: Studying the structural properties of both graphs and meshes suggests
    that the proposed learning methods for graphs are also applicable on meshes. Existing
    methods for Graph Convolutional Neural Networks (GCNN) can be broadly categorized
    into two main directions: spectral filtering methods and spatial filtering methods.
    Here we discuss the underlying concept behind each method and overview the work
    done in each direction. The distinction between both directions is in how the
    filtering is employed and how the locally processed information is combined.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图：研究图和网格的结构特性表明，为图提出的学习方法也适用于网格。现有的图卷积神经网络（GCNN）方法可以大致分为两个主要方向：谱过滤方法和空间过滤方法。这里我们讨论了每种方法背后的概念，并概述了在每个方向上的工作。这两个方向之间的区别在于过滤器的应用方式以及如何组合本地处理的信息。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Spectral filtering methods. The notion of spectral convolution on graph-structured
    data was introduced by Bruna et al. in (Bruna et al., [2013](#bib.bib24)) where
    the authors proposed Spectral CNN (SCNN) operating on graphs. The foundation of
    the spectral filtering methods is to use the spectral eigen-decomposition of the
    graph Laplacian to define a convolution-like operator. This redefines the convolution
    operation in the spectral domain where the main two core stones are analogous:
    the patches of the signal in the Euclidean domain correspond to the functions
    defined on the graph nodes e.g. features, mapped to the spectral domain by projecting
    on the eigenvectors of the graph Laplacian. The filtering operation itself happens
    in the Euclidean domain and corresponds to scaling the signals in the eigenbasis.
    This definition implies that convolution is a linear operator that commutes with
    the Laplacian operator (Bronstein et al., [2017](#bib.bib21)). Despite the innovation
    aspect of Bruna’s model, it has serious limitations due to being basis dependent
    and computationally expensive. Being basis-dependent means that if the spectral
    filter’s coefficients were learned with respect to a specific basis, applying
    the learned coefficients on another domain with another basis will produce very
    different results as illustrated in (Bronstein et al., [2017](#bib.bib21)). The
    other limitation of being computationally costly arises from the fact that spectral
    filtering is a non-local operation that involves data across the whole graph besides
    that the graph Laplacian is expensive to compute. This constitutes a computational
    burden towards generalizing to other bases and processing large-scale graphs.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 光谱过滤方法。光谱卷积在图结构数据上的概念是由布鲁纳等人在（布鲁纳等人，[2013](#bib.bib24)）中引入的，作者提出了在图上操作的光谱CNN（SCNN）。光谱过滤方法的基础是使用图拉普拉斯的光谱特征分解来定义类似卷积的算子。这将卷积操作重新定义到光谱域，其中两个核心要素是类似的：欧几里得域中信号的片段对应于定义在图节点上的函数，例如特征，通过在图拉普拉斯的特征向量上投影映射到光谱域。过滤操作本身发生在欧几里得域中，并且对信号在特征基中的缩放对应。这个定义意味着卷积是一个线性算子，与拉普拉斯算子交换（布朗斯坦等人，[2017](#bib.bib21)）。尽管布鲁纳模型具有创新性，但由于依赖基础和计算昂贵，它具有严重的局限性。依赖基础意味着如果光谱滤波器的系数是根据特定基础学习的，将学习的系数应用于具有另一个基础的另一个域将产生非常不同的结果，如布朗斯坦等人在（布朗斯坦等人，[2017](#bib.bib21)）中所示。计算昂贵的另一个限制来自于光谱过滤是一个涉及整个图数据的非局部操作，除此之外，图拉普拉斯的计算昂贵。这构成了向其他基础泛化和处理大规模图的计算负担。
- en: The work in (Kovnatsky et al., [2013](#bib.bib76)) addressed the basis dependency
    problem by constructing a compatible orthogonal basis across various domains through
    a joint diagonalization. However, this required a prior knowledge about the correspondence
    between the domains. For some applications like social networks, this is a valid
    assumption because the correspondence can be easily computed between two time
    instances in which new edges and vertices have been added. However, applying this
    on meshes is rather unreasonable because finding correspondence between two meshes
    is challenging task on its own. Therefore, assuming the knowledge of correspondence
    between domains in such case is unrealistic (Bronstein et al., [2017](#bib.bib21)).
    Since the non-local nature of the spectral filtering and the need to involve all
    the graph data in the processing, recent works proposed the idea of approximation
    to produce local spectral filters (Defferrard et al., [2016](#bib.bib37)) (Kipf
    and Welling, [2016](#bib.bib74)). These methods propose to represent the filters
    via a polynomial expansion instead of directly operating on the spectral domain.
    Defferrard et al. in  (Defferrard et al., [2016](#bib.bib37)) performed local
    spectral filtering on graphs by using Chebyshev polynomials in order to approximate
    the spectral graph filters. The features yielding from the convolution operation
    are then coarsened using the graph pooling operation. Kipf and Welling (Kipf and
    Welling, [2016](#bib.bib74)) simplified the polynomial approximation proposed
    in (Defferrard et al., [2016](#bib.bib37)) and used a first-order linear approximation
    of the graph spectral filters to produce local spectral filters which are then
    employed in a two-layer GCNN. Each of these two layers uses the local spectral
    filters and aggregates the information from the immediate neighbourhood of the
    vertices. Note that the filters proposed in (Defferrard et al., [2016](#bib.bib37))
    and  (Kipf and Welling, [2016](#bib.bib74)) are employed on r- or 1-hop neighbourhood
    of the graph returns these constructions into the spatial domain.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Driven by the success of the local spectral filtering models, Wang et al. (Wang
    et al., [2018](#bib.bib136)) proposed to take the advantages of the power of spectral
    GCNNs in the pointNet++ framework (Qi et al., [2017b](#bib.bib102)) to process
    unordered point clouds. This model fuses the innovation of the pointNet++ framework
    with local spectral filtering while addressing two shortcomings of these models
    independently. Therefore, instead of processing each point independently in the
    point clouds as proposed in pointNet++, this model uses spectral filtering as
    a learning technique to change the structural information of each points’ neighborhood.
    Moreover, rather than using the greedy winner-takes all method in the graph max
    pooling operation, this method adopts a recursive pooling and clustering strategy.
    Unlike the previous spectral filtering methods, this method does not require any
    pre-computation and it is trainable by an end-to-end manner which allows building
    the graph dynamically and computing the graph Laplacian and the pooling hierarchy
    on the fly unlike (Bruna et al., [2013](#bib.bib24); Defferrard et al., [2016](#bib.bib37);
    Kipf and Welling, [2016](#bib.bib74)). This method have been able to achieve better
    recognition results than the existing state-of-the-art techniques on diverse datasets
    as shown in Table 1 of the supplementary material.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatial filtering methods. The concept of graph spatial filtering started in (Scarselli
    et al., [2009](#bib.bib112)) when GNNs were first proposed as an attempt to generalize
    DL models to graphs. GNNs are simple constructions that try to generalize the
    notion of spatial filtering on graphs via the weights of the graph. GNNs are composed
    of multiple layers where each layer is a linear combination of graph high-pass
    and low-pass operators. This formulation suggests that learning the graph features
    is dependent on each vertex’s neighborhood. Similar to Euclidean CNNs, a non-linear
    function is applied to all the nodes of the graph where the choice of this function
    varies depending on the task. Varying the nature of the vertex non-linear function
    lead to rich architectures (Li et al., [2015](#bib.bib84); Sukhbaatar et al.,
    [2016](#bib.bib126); Duvenaud et al., [2015](#bib.bib40); Chang et al., [2016](#bib.bib29);
    Battaglia et al., [2016](#bib.bib11)). Also, analogous to CNNs, pooling operation
    can be employed on graph-structured data by graph coarsening. Graph pooling layers
    can be performed by interleaving the graph learning layers. In comparison with
    the spectral graph filtering, spatial filtering methods have two key points which
    distinguish them from spectral methods. Spatial methods aggregate the feature
    vectors from the neighborhood nodes directly based on the graph topology considering
    the spatial structure of the input graph. The aggregated features are then summarized
    via an additional operation. The GNN framework presented in (Scarselli et al.,
    [2009](#bib.bib112); Gori et al., [2005](#bib.bib50)), proposed to embed each
    vertex in the graph into a Euclidean space with an RNN. Instead of using the recursive
    connections in the RNN, the authors used a simple diffusion function for their
    transition function, propagating the node representation repeatedly until it is
    stable and fixed. The resulting node representations are considered as the features
    for classification and regression problems. The repeated propagation for node
    features in this framework constitutes a computational burden which is alleviated
    in the work proposed by Li et al. (Li et al., [2015](#bib.bib84)). They have proposed
    a variant of the previous model which uses the gated recurrent units to perform
    the state updates to learn the optimal graph representation. Bruna et al. in (Bruna
    et al., [2013](#bib.bib24)) imposed the spatial local receptive field on GNN to
    produce their local spatial formulation of GNN. The main idea behind the local
    receptive field is to decrease the number of the learned parameters by grouping
    similar features based on a similarity measure (Coates and Ng, [2011](#bib.bib32);
    Gregor and LeCun, [2010](#bib.bib51)). In (Bruna et al., [2013](#bib.bib24)),
    the authors used this concept to compute a multi-scale clustering of the graph
    to be fed to the pooling layer afterwards. This model imposes locality on the
    processed features and reduces the amount of processed parameters. However, it
    doesn’t perform any weight sharing similar to 2D CNNs. Niepert et al.in (Niepert
    et al., [2016](#bib.bib95)) performs spatial graph convolution in a simple way
    by converting the graph locally into sequences and feeding these sequences into
    a 1D CNN. This method is simple but requires an explicit definition for the nodes
    orders of the graphs in a pre-processing step. In (Venkatakrishnan et al., [2018](#bib.bib132)),
    the authors provided a detailed study proving that spectral methods and spatial
    methods are mathematically equivalent in terms of their representation capabilities.
    The difference resides in how the convolution and the aggregation of the learned
    features are performed. Depending on the task, the architecture of the GCNN (spectral
    or spatial) is formed where the convolution layers may be interleaved with coarsening
    and pooling layers to summarize the output of the convolution filters for a compact
    representation of the graph. This is crucial in classification applications where
    the output is only one class inferred from the learned features (Bruna et al.,
    [2013](#bib.bib24)). Some other applications require a decision per node such
    as community detection. A common practice in such cases is to have multiple convolution
    layers that compute the graph representations at the node level  (Khalil et al.,
    [2017](#bib.bib73); Nowak et al., [2017](#bib.bib97); Bruna and Li, [2017](#bib.bib23)).
    All these GCNNs are end-to-end differentiable methods that can be trained in supervised,
    semi-supervised or reinforcement learning techniques.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Meshes: On the Euclidean domain, the convolution operation is performed by
    passing a template at each point on the spatial domain and recording the correlation
    between the templates using the function that is defined at this point. This is
    feasible due to the shift-invariance property on the Euclidean domain. Howeverm,
    unfortunately, this is not directly applicable on meshes because there is a lack
    of the shift-invariance property. This is what pushed towards defining local patches
    that represent the 3D surface in a way that allows performing convolution. However,
    due to the lack of global parametrization on non-Euclidean data, these patches
    are defined in a local system of coordinates locally meaning that these patches
    are also position-dependent. Recently, various non-Euclidean CNNs frameworks were
    proposed. The main schema used by these frameworks is very similar except for
    how the patches are defined mostly. The local patches are defined either by handcrafting
    them or depending on the connectivity of the vertices while using the features
    of the 1-hop neighborhood as the patch directly (Fey et al., [2017](#bib.bib47)).
    The convolution employed in such frameworks is very similar to the classical 2D
    convolution where it is basically an element-wise multiplication between the convolution
    filter and the patch and summing up the results. This is because the patches extracted
    by such frameworks boils down the representation into 2D where the classical convolution
    can be employed.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Geodesic CNN (Masci et al., [2015](#bib.bib92)) was introduced as a generalization
    of classical CNNs to triangular meshes. The main idea of this approach is to construct
    local patches in local polar coordinates. The values of the functions around each
    vertex in the mesh are mapped into local polar coordinates using the patch operator.
    This defines the patches where the geodesic convolution is employed. Geodesic
    convolution follows the idea of multiplication by a template. However, the convolution
    filters in this framework are subjected to some arbitrary rotations due to the
    angular coordinate ambiguity (Masci et al., [2015](#bib.bib92)). This method opened
    the door for new innovations on extending CNN paradigm to triangular meshes. However,
    this framework suffers from multiple drawbacks. First, it can only be applied
    on triangular meshes where it is sensitive to the triangulation of the mesh and
    it might fail if the mesh is extremely irregular. Second, the radius of the constructed
    geodesic patch has to be small with respect to the injectivity radius of the actual
    shape to guarantee that the resulted patch is topologically a disk. Third, the
    rotations employed on the convolution filters make the framework computationally
    expensive which limits the usage of such a framework. Anisotropic CNN (ACNN) (Boscaini
    et al., [2016](#bib.bib15)) was proposed to overcome some of the limitations in
    the geodesic CNN. Compared to the geodesic CNN, the ACNN framework is not limited
    to triangular meshes and can be also applied to graphs. Also, the construction
    of the local patches is simpler and is independent on the injectivity radius of
    the mesh. ACNN uses the concept of spectral filtering where the spatial information
    is also incorporated by a weighting function to extract a local function defined
    on the meshes. The learnt spectral filters are applied to the eigenvalues of the
    anisotropic Laplacian Beltrami Operator (LBO) and the anisotropic heat kernels
    act as a spatial weighting functions for the convolution filters. This method
    has shown a very good performance for local correspondence tasks. Rather than
    using a fixed kernel construction as in the previous models, Monti et al. (Monti
    et al., [2017](#bib.bib94)) proposed $MoNet$ as a general construction of patches.
    The authors proposed to define a local system of coordinates of pseudo-coordinates
    around each vertex with weight functions. On these coordinates, a set of parametric
    kernels are applied on these pseudo-coordinates to define the weighting functions
    at each vertex. That is why the previous methods (Masci et al., [2015](#bib.bib92);
    Boscaini et al., [2016](#bib.bib15)) can be considered as specific instances of
    MoNet. Some recent work has been proposed to eliminate the need to explicitly
    define the local patches on the graphs or meshes such as SplineCNN (Fey et al.,
    [2017](#bib.bib47)). SplineCNN is a convolutional framework that can be employed
    on directed graphs of any dimensionality. Hence, it can also be applied on meshes.
    Instead of defining the local patches by a charting-based method like the previous
    methods, SplineCNN uses the 1-hop neighborhood ring features of the graph as the
    patch where the convolutional filter can operate. The convolutional filter itself
    is a spatial continuous filter based on B-Spline basis functions that have local
    support. This framework produces state-of-the-art results on the correspondence
    task while being computationally very efficient. This is due to the local support
    of the B-Spline basis which makes the computational time independent of the kernel
    size.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Analysis and discussions
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DL paradigms are successfully architect and deployed to various 3D data representations
    as discussed in the previous sections. Several approaches have been proposed.
    We herein discuss the main 3D datasets and their exploitation in various 3D computer
    vision tasks. Also, we present DL advances in three main tasks; 3D recognition/classification,
    retrieval and correspondence.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. 3D Datasets
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We overview below the most recent 3D datasets. There are two main categories
    of data used by the research community: real-world datasets and synthetic data
    rendered from CAD models. It is preferable to use the real-world data; however,
    real data is expensive to collect and usually suffers from noise and occlusion
    problems. In contrast, synthetic data can produce a huge amount of clean data
    with limited modelling problems. While this can be seen advantageous, it is quite
    limiting to the generalization ability of the learned model to real-world test
    data. It is also important to note that most 3D datasets are smaller than large
    2D datasets such as, $ImageNet$ (Deng et al., [2009](#bib.bib38)). However, there
    are some recent exceptions as described below.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'ModelNet (Wu et al., [2015a](#bib.bib143)) is the most commonly used dataset
    for 3D object recognition and classification. It contains roughly 130k annotated
    CAD models on 662 distinct categories. This dataset was collected using online
    search engines by querying for each of the categories. Then, the data were manually
    annotated. ModelNet provides the 3D geometry of the shape without any information
    about the texture. ModelNet has two subsets: ModelNet10 and ModelNet40. These
    subsets are used in most of the recently published work as shown in Table 1 and
    Table 2 in the supplementary material. In Section [1.2.1](#S1.SS2.SSS1 "1.2.1\.
    3D recognition/classification ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis
    and discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations"),
    we provide an extensive analysis of the methods employed on the ModelNet dataset
    for recognition and retrieval tasks highlighting the evolution of DL methods for
    learning such data.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: SUNCG (Song et al., [2017](#bib.bib124)) contains about 400K of full room models.
    This dataset is synthetic, however, each of these models was validated to be realistic
    and it was processed to be annotated with labelled object models. This dataset
    is important for learning the “scene-object” relationship and to fine-tune real-world
    data for scene understanding tasks. SceneNet (Handa et al., [2016](#bib.bib57))
    is also an RGB-D dataset that uses synthetic indoor rooms. This dataset contains
    about 5M scenes that are randomly sampled from a distribution to reflect the real
    world. However, not all the generated scenes are realistic and in practice, some
    of them are highly unrealistic. Still, this dataset can be used for fine-tuning
    and pre-training. In contrast, ScanNet (Dai et al., [2017](#bib.bib35)) is a very
    rich dataset for real-world scenes. It is an annotated dataset which is labelled
    with some semantic segmentation, camera orientation and the 3D information that
    is gathered from 3D video sequences of real indoor scenes. It includes 2.5M views,
    which allows for training directly without pre-training on other datasets, as
    it is the case with different datasets.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: In addition, datasets for 3D meshes are available for the 3D computer vision
    community. Most of the 3D meshes datasets are for 3D objects, body models or face
    data. The TOSCA (Bronstein et al., [2008](#bib.bib20)) dataset provides high-resolution
    3D synthetic meshes for non-rigid shapes. It contains a total of 80 objects in
    various poses. Objects within the same category have the same number of vertices
    and the same triangulation connectivity. TOSCA (Bronstein et al., [2008](#bib.bib20))
    provides artistic deformations on the meshes to simulate the real-world deformations
    of real scans. SHREC (Bronstein et al., [2010](#bib.bib19)) adds a variety of
    artificial noise and artistic deformations on TOSCA scans. However, the artificial
    noise and deformations are not realistic and can’t generalize to new unseen real-world
    data which is a requirement for practical solutions. FAUST (Bogo et al., [2014](#bib.bib14))
    dataset, however, provides 300 real-scans of 10 people in various poses. The 3DBodyTex
    dataset (Saint et al., [2018](#bib.bib109)) is recently proposed with 200 real
    3D body scans with high-resolution texture. 3DBodyTex is a registered dataset
    with the landmarks available for 3D human body models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The series of the BU datasets is very popular for 3D faces under various expressions.
    BU-3DFE (Yin et al., [2006](#bib.bib152)) is a static dataset which has 100 subjects
    (56 female and 44 male) of different ages and races. Each subject has in addition
    to the neutral face, six expressions as (happiness, sadness, anger, disgust, fear
    and surprise) of different intensities. There is 25 meshes for each subject in
    total, resulting in 2500 3D facial expressions dataset. Another very popular dataset
    is BU-4DFE (Yin et al., [2008](#bib.bib151)), which is a dynamic facial expression
    dataset that has 101 subjects in total (58 female and 43 male). Similar to the
    BU-3DFE, each subject has six expressions. Other 3D faces datasets were available
    as well like the BP4D-Spontanous (Zhang et al., [2014](#bib.bib157)) and BP4D+ (Zhang
    et al., [2016](#bib.bib158)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. 3D Computer vision tasks
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the huge success of DL approaches in various computer vision tasks in
    the 2D domain, DL methods have gained more popularity as they are producing some
    remarkable performances on different tasks. Here, we overview 3D DL advances on
    the 3D object recognition/classification, retrieval and correspondence tasks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1\. 3D recognition/classification
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The tasks of 3D recognition/classification is fundamental in computer vision.
    Given a 3D shape, the goal is to identify the class to which this shape belongs
    (3D classification) or given a 3D scene, recognize different 3D shapes in the
    scene along with their positions (3D recognition and localization). There is an
    active research to exploit Deep Neural Networks (DNNs) for 3D object recognition/classification.
    Existing approaches can be classified according to their input to learn the task
    as shown in Table 1 and Table 2 in the supplementary material. Multi-view approaches
    perform the task of classification using the learned features after applying 2D
    CNNs on each view. Generally, the multi-view methods outperform the other methods;
    however, there are still some unsolved drawbacks. For example, the recent Multi-View
    CNN (MVCNN) (Su et al., [2015](#bib.bib125)) applies a max-pooling operation on
    the features of each view to produce global features that represent the 3D object.
    The max-pooling ignores the non-maximal activation and only keeps the maximal
    ones from a specific view which results into losing some visual cues (Su et al.,
    [2015](#bib.bib125)). Yu et al. (Yu et al., [2018](#bib.bib153)) tried to incorporate
    the other views by a sum-pool operation. However, it performed worse than the
    max-pooling.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Considering the recognition and classification tasks, (Wang et al., [2017b](#bib.bib135))
    improved the discrimination between objects by applying a recurrent clustering
    and pooling strategy that increases the likelihood of variations in the aggregated
    activation output of the multi-views. Their objective was to capture the subtle
    changes in the activation function space. Similarly, $GIFT$ (Bai et al., [2016](#bib.bib10))
    extracts the features from each view but does not apply any pooling. It matches
    views to find the similarity between two 3D objects. It counts the best matched
    views only, however the greedy selection of the best matched view may discard
    useful information. Another framework called Group View CNN (GVCNN) (Feng et al.,
    [2018](#bib.bib46)) contains a hierarchical architecture of content descriptions
    from the view level, group of views level and the shape level. The framework defines
    the groups based on their discrimination power and the weights are adapted accordingly.
    Another very recent work (Yu et al., [2018](#bib.bib153)) proposes a Multi-view
    Harmonized Bilinear Network (MHBN) to improve the similarity level between two
    3D objects by utilizing patch features rather than view features.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Volumetric approaches classify 3D objects by using directly the shape voxels.
    They mitigate the challenge of having orderless points by voxelizing the input
    point cloud like 3D ShapeNets (Wu et al., [2015b](#bib.bib144)), volumetric CNNs (Qi
    et al., [2016b](#bib.bib101)), $OctNet$ (Riegler et al., [2017](#bib.bib105))
    and VRN Ensemble (Brock et al., [2016](#bib.bib16)). Although VRN Ensemble outperforms
    the multi-view methods, this performance is thanks to the model ensemble and their
    advanced base model as it ensembles five ResNet models and one Inception model
    while most of the existing multi-view methods rely on a single VGG-M model. Generally,
    these methods are not as accurate as the multi-view methods. Due to data sparsity
    and heavy 3D convolution computations, they are heavily constrained by their resolution.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: $Pointset$ (Non-Euclidean) approaches classify directly the unordered point
    sets in order to address the sparsity problem found in volumetric methods as proposed
    in $PointNet$ (Qi et al., [2017a](#bib.bib100)). For each point, $PointNet$ learns
    a spatial encoding and aggregates all the features to a global representation.
    PointNet++ (Qi et al., [2017b](#bib.bib102)) improves $PointNet$ by utilizing
    local structures formed by the metric space. The points are partitioned into local
    regions that overlap by the distance metric of their space. The features are then
    extracted in a hierarchical fine to coarse process. At the same time of $PointNet$,
    Kd-networks (Klokov and Lempitsky, [2017](#bib.bib75)) were proposed, it recognizes
    3D models by performing multiplicative transformations and sharing their parameters
    given the point clouds subdivisions imposed by kd-trees.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Given any of the existing input modalities, a set of descriptors can be directly
    learned. The 3D representations can be also re-defined as a set of 2D geometric
    projections then a set of descriptors will be extracted. There is a direction
    to make the best out of all worlds by combining different input modalities for
    efficient feature extraction. A good example is the work of (Bu et al., [2017](#bib.bib27))
    where their scheme consists of a view-based feature learning, geometry-based feature
    learning from volumetric representations and a modality feature fusion in which
    the aforementioned learnt features were associated through a Restricted Boltzman
    Machine (RBM).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: A comprehensive comparison between the recent state-of-the-art-methods is given
    in Table 1 in the supplementary material and compared based on experiments on
    $ModelNet10$ and $ModelNet40$ datasets in Fig. [2](#S1.F2 "Figure 2 ‣ 1.2.1\.
    3D recognition/classification ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis
    and discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations")
    and Fig. [3](#S1.F3 "Figure 3 ‣ 1.2.1\. 3D recognition/classification ‣ 1.2\.
    3D Computer vision tasks ‣ 1\. Analysis and discussions ‣ A survey on Deep Learning
    Advances on Different 3D Data Representations") respectively. Results shown in
    both figures highlight the power of the multi-view techniques achieving the state-of-the-art
    on the classification task on both datasets, $ModelNet10$ (98.46%) and $ModelNet40$
    (97.37%). Also, it shows the competition between such methods and volumetric methods
    such as Brock et al. (Brock et al., [2016](#bib.bib16)) achieving very competitive
    results to the multi-view ($ModelNet10$ (97.14%) and $ModelNet40$ (95.54%)), but
    with a more complex architecture and a serious need for data augmentation due
    to the complexity of the model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a7bbb50b6d1256dd2f11d133c555005f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Class accuracy improvements on ModelNet10 dataset for classification/recognition
    tasks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d92ead8eaaa8160755c8baed3ab8e959.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Class accuracy improvements on ModelNet40 dataset for classification/recognition
    tasks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2\. 3D retrieval
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 3D object retrieval is another fundamental task in shape analysis. The target
    here is to find the most similar 3D object from a database to match with the tested
    one. However, in the literature, most DNNs focus on leveraging the discrimination
    strength of these networks for the classification and recognition tasks. There
    are less DNNs specifically designed for 3D shape retrieval, see Table 2 in the
    supplementary material. Conceptually, the input processing is similar to the classification
    and recognition tasks. Generally, 3D object retrieval approaches are similar to
    other image or object retrieval methods where several loss functions are trained
    to learn an embedding space to get elements closer to each other. A very recent
    work (He et al., [2018](#bib.bib61)) train the center loss (Wen et al., [2016](#bib.bib140))
    and triplet loss (Schroff et al., [2015](#bib.bib113)) specifically for the distance
    measure which superpasses the state-of-the-art on $ModelNet40$ and $ShapeNet$.
    Multi-view methods usually outperform other methods in terms of retrieval accuracy.
    A comprehensive list of the recent 3D object retrieval state-of-the-art methods
    is given in Table 2 in the supplementary material.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3\. 3D Correspondence
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal of 3D correspondence is to predict the mapping between a set of vertices
    of a test mesh and a reference or template mesh. There are two types of correspondences;
    sparse and dense correspondence. Sparse correspondence means that only a subset
    of the vertices of the test mesh are mapped to the reference mesh. However, in
    dense correspondence, all the vertices of the test mesh are mapped to the reference
    mesh.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Several works (Fey et al., [2017](#bib.bib47); Verma et al., [2018](#bib.bib133))
    perform 3D shape correspondence on the registered meshes of FAUST dataset (Bogo
    et al., [2014](#bib.bib14)). Only registered meshes of the dataset are considered
    because they are in dense point-to-point correspondence, providing the ground
    truth. There are in total 100 registered meshes of 10 people in 10 different poses
    each. Since the meshes are registrations of a template mesh, the number of vertices
    is fixed and the connectivity pattern is identical. In previous works, the task
    is cast as a classification problem. The model has to map $N$ input vertices to
    the corresponding vertices on the template mesh. This is achieved with a one-hot
    encoding of the target vertex for each input vertex.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: In the reported experiments of (Fey et al., [2017](#bib.bib47)), FAUST dataset
    is divided into 80 meshes for training and 20 for testing. was divided into $80$
    and $20$ meshes for training and testing respectively. The results show very high
    accuracy in the correspondence prediction. Those results are very accurate, but
    seem to mostly reflect the simplicity of the experiment. In fact, the meshes of
    the dataset all have the same topology, i.e., the vertices are in the same order
    and the connectivity pattern is the same. Moreover, because of the registration,
    the vertices are already in dense one-to-one correspondence. There is thus no
    ambiguity in the possible assignments with neighboring vertices. This makes the
    correspondence task simple because the mapping from input vertices to ouput vertices
    is a trivial copy rather than learning the actual topology of the 3D mesh.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'For assessing the performance of a sample state-of-the-art technique for the
    correspondnce task, we experimented on SplineCNN (Fey et al., [2017](#bib.bib47)).
    We test the pre-trained model of SplineCNN on different test data to validate
    the performance under different conditions. We have three different experiments:
    1) Data with the same topology varying in the shape, pose and geometry generated
    from the Skinned Multi-Person Linear Model (SMPL) model (Loper et al., [2015](#bib.bib89)).
    2) Test data from FAUST dataset with synthetic noise of different intensities.
    3) Various unclothed full human body scans from the 3DBodyTex dataset (Saint et al.,
    [2018](#bib.bib109)) and some additional clothed scans acquired in a similar setup.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/01592b4df2bd6ec7e94389df11e537a7.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Correspondence results of testing the pre-trained model of SplineCNN (Fey
    et al., [2017](#bib.bib47)) on meshes generated from SMPL model (Loper et al.,
    [2015](#bib.bib89))
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: In the fist testcase, we generate different 3D meshes varying in both the shape
    and pose considering both genders, male and female, using the SMPL model (Loper
    et al., [2015](#bib.bib89)). The generated meshes have the same topology as the
    FAUST dataset, i.e., the same number of verticies and connectivity. The generated
    shapes vary in size from thin to fat which in turn varies the geometry of the
    generated subjects. We have used a pre-trained model such as SplineCNN to test
    he generated data for the correspondence task. Each vertex in the test subject
    is assigned to a specific vertex in the reference mesh and then each vertex is
    assigned a specific color to visualize the results.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: In Fig. [4](#S1.F4 "Figure 4 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer
    vision tasks ‣ 1\. Analysis and discussions ‣ A survey on Deep Learning Advances
    on Different 3D Data Representations"), the reference mesh is depicted on the
    left and all the correspondence results on the generated test data are on the
    right. As shown in Fig. [4](#S1.F4 "Figure 4 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\.
    3D Computer vision tasks ‣ 1\. Analysis and discussions ‣ A survey on Deep Learning
    Advances on Different 3D Data Representations"), the model is not able to generalize
    to new unseen but very similar data and does not report as good results as those
    reported in (Fey et al., [2017](#bib.bib47)). This performance is noticeable even
    on simple poses like the $T$ pose, shown in the first and fourth columns in Fig. [4](#S1.F4
    "Figure 4 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis
    and discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations"),
    which is actually included in the FAUST dataset. Also, the results show that varying
    the size of the shape has the largest effect on the results more than varying
    the pose, as there are more wrongly colored faces in comparison with the reference
    mesh. However, changing the pose or the shape (male or female) does not affect
    the performance much.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65ba3f815162c6cb3402dff7e08264bd.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. The correspondence results of testing the pre-trained model of SplineCNN (Fey
    et al., [2017](#bib.bib47)) on FAUST data after adding 4 different levels of synthetic
    noise. The first row is FAUST data without adding any noise. From the second row
    till the fifth row, the noise is added gradually from the lowest to the highest.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: The registered scans of the FAUST dataset are smooth, clean and noise-free.
    All the experiments reported in (Fey et al., [2017](#bib.bib47)) are on the clean
    FAUST data. These experiments suggest that it is important to investigate how
    robust to noise a given model is.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In another experiment, we test on four meshes from the FAUST dataset after adding
    synthetic noise of different levels as depicted in Fig. [5](#S1.F5 "Figure 5 ‣
    1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis and
    discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations").
    The initial row of meshes shows four of the original test meshes of FAUST dataset
    without adding any noise. The following rows show different levels of added noise
    from level 1 (lowest noise) to level 4 (highest). The mesh shown on the left represents
    the reference mesh and the set of meshes on the right show the correspondence
    results. For level 1 noise (second row), the geometry of meshes has barely changed
    with respect to the original meshes (first row), except for some noise on the
    faces. However, the correspondence results show that there are some erroneous
    areas in the arms for the ‘hands up pose’ (last pose from the left), which should
    not be the case since the arms did not change. Moreover, the more noisy the mesh
    is, the more the correspondence results get erroneous in terms of the wrongly
    labelled vertices and this is clear in terms if the difference in the color map
    between the reference mesh and the test meshes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6969425be4a1b71bdae1517f7731f9d5.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. Testing the pre-trained model of SplineCNN (Fey et al., [2017](#bib.bib47))
    on some clothed and unclothed scans from the 3DBodyTexdata (Saint et al., [2018](#bib.bib109))
    dataset.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Real mesh data is more challenging because it is more noisy, incomplete and
    less regular with variable sampling. Because of the noise, the vertices of an
    input mesh do not have an exact correspondence with the template mesh, making
    the correspondence ambiguous. Also, the connectivity pattern usually varies from
    mesh to another even if re-sampled to the same number of vertices. Possible holes
    in the data make it hard to find the exact correspondence to the reference mesh.
    Variability in the sampling requires robust methods that can adapt to different
    scales and handle different levels of features. Recently, Monte Carlo Convolution (Hermosilla
    et al., [2018](#bib.bib63)) is proposed to handle sampling at different levels
    for point clouds learning. The authors in this work propose to represent the convolution
    kernel as a “Multi-Layer Perceptron (MLP)” where the convolution is formulated
    as a Monte Carlo intergration problem. This notion enables to combine information
    of the point cloud from multiple samplings at different levels, where Poisson
    disk sampling is used as a scalable means of hierarchical point cloud learning.
    This showed robustness even when all the training data is non-uniformly samples.
    This method achieves relatively better results compared to PointNet++ (Qi et al.,
    [2017b](#bib.bib102)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: To test for the robustness of the SplineCNN model with respect to all of these
    challenges, we test on real-world clothed and unclothed scans from the 3DBodyTex
    dataset (Saint et al., [2018](#bib.bib109)) as shown in Fig. [6](#S1.F6 "Figure
    6 ‣ 1.2.3\. 3D Correspondence ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis
    and discussions ‣ A survey on Deep Learning Advances on Different 3D Data Representations").
    We down-sample 10 meshes of the 3DBodyTex dataset to 6890 vertices to be equivalent
    to the number of vertices of FAUST dataset. However, the down-sampled data have
    different connectivity pattern from the FAUST data, which makes the task of the
    correspondence harder. As depicted in Fig. [6](#S1.F6 "Figure 6 ‣ 1.2.3\. 3D Correspondence
    ‣ 1.2\. 3D Computer vision tasks ‣ 1\. Analysis and discussions ‣ A survey on
    Deep Learning Advances on Different 3D Data Representations"), SplineCNN model
    is not able to handle the 3DBodyTex data which is of different topology. This
    results in highly erroneous correspondence results where the resulting color map
    on the test data is very far from the color map of the reference mesh.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Conclusion
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ongoing evolution of scanning devices caused a huge increase in the amount
    of 3D data available in the 3D computer vision research community. This opens
    the door for new opportunities investigating the properties of different 3D objects
    and learning their geometric properties despite challenges imposed by the data
    itself. Fortunately, DL techniques revolutionized the learning performance on
    various 2D computer vision tasks which encouraged the 3D research community to
    adopt the same path. However, extending 2D DL to 3D data is not a straightforward
    tasks depending on the data representation itself and the task at hand. In this
    work, we categorized the 3D data representations based on their internal structure
    to Euclidean and non-Euclidean representations. Following the same classification,
    we discussed different DL techniques applied to 3D data based on the data representation
    and how the internal structure of the data is treated. In the Euclidean DL family,
    the reported results in the literature shows that multi-view representations achieve
    the state-of-the-art classification performance and outperforms other methods
    that exploit the full geometry of the 3D shape (i.e., volumetric methods) providing
    a more efficient way to learn the properties of 3D shapes. On the other branch
    of the non-Euclidean DL techniques, results are reported near perfect on the correspondence
    task in various recent papers such as (Fey et al., [2017](#bib.bib47); Verma et al.,
    [2018](#bib.bib133)). These correspondence experiments were carried on clean,
    smooth and ideal data. In this paper, state-of-the-art SplineCNN (Fey et al.,
    [2017](#bib.bib47)) method have been tested over different dataset, under different
    different conditions that emulate the real-world scenarios. The obtained results
    showed that, even with the same topology and similar poses, this model does not
    generalize to new or noisy data. There is clearly a need to further investigate
    ways to improve the robustness of 3D DL models and ensure their generalization
    to real data while exploiting the different existent representations of 3D data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Acknowledgement
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work has been funded by FNR project IDform under the agreement CPPP17/IS/11643091/IDform/
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Aouada, Luxembourg and by Artec Europe SARL.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abdul-Rahman and Pilouk (2007) Alias Abdul-Rahman and Morakot Pilouk. 2007.
    *Spatial data modelling for 3D GIS*. Springer Science & Business Media.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Afzal et al. (2014) H. Afzal, D. Aouada, D. Font, B. Mirbach, and B. Ottersten.
    2014. RGB-D Multi-view System Calibration for Full 3D Scene Reconstruction. In
    *2014 22nd International Conference on Pattern Recognition*. IEEE, 2459–2464.
    [https://doi.org/10.1109/ICPR.2014.425](https://doi.org/10.1109/ICPR.2014.425)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aldoma et al. (2012) Aitor Aldoma, Federico Tombari, Radu Bogdan Rusu, and Markus
    Vincze. 2012. OUR-CVFH–oriented, unique and repeatable clustered viewpoint feature
    histogram for object recognition and 6DOF pose estimation. In *Joint DAGM (German
    Association for Pattern Recognition) and OAGM Symposium*. Springer, 113–122.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alexandre (2016) Luís A Alexandre. 2016. 3D object recognition using convolutional
    neural networks with transfer learning between input channels. In *Intelligent
    Autonomous Systems 13*. Springer, 889–898.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aouada et al. (2008) D. Aouada, D. W. Dreisigmeyer, and H. Krim. 2008. Geometric
    modeling of rigid and non-rigid 3D shapes using the global geodesic function.
    In *2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
    Workshops*. IEEE, 1–8. [https://doi.org/10.1109/CVPRW.2008.4563075](https://doi.org/10.1109/CVPRW.2008.4563075)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aouada et al. (2007) D. Aouada, S. Feng, and H. Krim. 2007. Statistical Analysis
    of the Global Geodesic Function for 3D Object Classification. In *2007 IEEE International
    Conference on Acoustics, Speech and Signal Processing - ICASSP ’07*, Vol. 1\.
    IEEE, I–645–I–648. [https://doi.org/10.1109/ICASSP.2007.365990](https://doi.org/10.1109/ICASSP.2007.365990)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aouada and Krim (2010) D. Aouada and H. Krim. 2010. Squigraphs for Fine and
    Compact Modeling of 3-D Shapes. *IEEE Transactions on Image Processing* 19, 2
    (Feb 2010), 306–321. [https://doi.org/10.1109/TIP.2009.2034693](https://doi.org/10.1109/TIP.2009.2034693)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aubry et al. (2011) Mathieu Aubry, Ulrich Schlickewei, and Daniel Cremers.
    2011. The wave kernel signature: A quantum mechanical approach to shape analysis.
    In *Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference
    on*. IEEE, 1626–1633.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2016) Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan
    Latecki. 2016. Gift: A real-time and scalable 3d shape search engine. In *Computer
    Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on*. IEEE, 5023–5032.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Battaglia et al. (2016) Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez
    Rezende, et al. 2016. Interaction networks for learning about objects, relations
    and physics. In *Advances in neural information processing systems*. arXiv, 4502–4510.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ben-Shabat et al. (2017) Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fischer.
    2017. 3D Point Cloud Classification and Segmentation using 3D Modified Fisher
    Vector Representation for Convolutional Neural Networks. *arXiv:1711.08241* abs/1711.08241
    (2017).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Berger (2013) Kai Berger. 2013. The role of rgb-d benchmark datasets: an overview.
    *arXiv preprint arXiv:1310.2053* (2013).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bogo et al. (2014) Federica Bogo, Javier Romero, Matthew Loper, and Michael J.
    Black. 2014. FAUST: Dataset and evaluation for 3D mesh registration. In *Proceedings
    IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*. IEEE, Piscataway,
    NJ, USA.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boscaini et al. (2016) Davide Boscaini, Jonathan Masci, Emanuele Rodolà, and
    Michael Bronstein. 2016. Learning shape correspondence with anisotropic convolutional
    neural networks. In *Advances in Neural Information Processing Systems*. NIPS,
    3189–3197.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brock et al. (2016) Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston.
    2016. Generative and discriminative voxel modeling with convolutional neural networks.
    *arXiv preprint arXiv:1608.04236* (2016).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bronstein and Bronstein (2007) Alexander Bronstein and Michael Bronstein. 2007.
    [http://vision.mas.ecp.fr/Personnel/iasonas/descriptors.html](http://vision.mas.ecp.fr/Personnel/iasonas/descriptors.html)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bronstein and Bronstein (2018) Alexander Bronstein and Michael Bronstein. 2018.
    Discrete geometry tutorial 1.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bronstein et al. (2010) AM Bronstein, MM Bronstein, U Castellani, B Falcidieno,
    A Fusiello, A Godil, LJ Guibas, I Kokkinos, Zhouhui Lian, M Ovsjanikov, et al.
    2010. Shrec 2010: robust large-scale shape retrieval benchmark. *Proc. 3DOR* 5
    (2010), 4.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bronstein et al. (2008) Alexander M Bronstein, Michael M Bronstein, and Ron
    Kimmel. 2008. *Numerical geometry of non-rigid shapes*. Springer Science & Business
    Media.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bronstein et al. (2017) Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur
    Szlam, and Pierre Vandergheynst. 2017. Geometric deep learning: going beyond euclidean
    data. *IEEE Signal Processing Magazine* 34, 4 (2017), 18–42.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bronstein and Kokkinos (2010) Michael M Bronstein and Iasonas Kokkinos. 2010.
    Scale-invariant heat kernel signatures for non-rigid shape recognition. In *Computer
    Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on*. IEEE, 1704–1711.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bruna and Li (2017) Joan Bruna and Xiang Li. 2017. Community detection with
    graph neural networks. *arXiv preprint arXiv:1705.08415* (2017).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bruna et al. (2013) Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun.
    2013. Spectral networks and locally connected networks on graphs. *arXiv preprint
    arXiv:1312.6203* (2013).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bu et al. (2015) Shuhui Bu, Pengcheng Han, Zhenbao Liu, Junwei Han, and Hongwei
    Lin. 2015. Local deep feature learning framework for 3D shape. *Computers & Graphics*
    46 (2015), 117–129.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bu et al. (2014) Shuhui Bu, Zhenbao Liu, Junwei Han, Jun Wu, and Rongrong Ji.
    2014. Learning High-Level Feature by Deep Belief Networks for 3-D Model Retrieval
    and Recognition. *IEEE Transactions on Multimedia* 16 (2014), 2154–2167.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bu et al. (2017) Shuhui Bu, Lei Wang, Pengcheng Han, Zhenbao Liu, and Ke Li.
    2017. 3D shape recognition and retrieval based on multi-modality deep learning.
    *Neurocomputing* 259 (2017), 183 – 193. Multimodal Media Data Understanding and
    Analytics.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2017) Zhangjie Cao, Qixing Huang, and Karthik Ramani. 2017. 3D Object
    Classification via Spherical Projections. *arXiv preprint arXiv:1712.04426* (2017).
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2016) Michael B Chang, Tomer Ullman, Antonio Torralba, and Joshua B
    Tenenbaum. 2016. A compositional object-based approach to learning physical dynamics.
    *arXiv preprint arXiv:1612.00341* (2016).
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2015) Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz. 2015.
    Utd-mhad: A multimodal dataset for human action recognition utilizing a depth
    camera and a wearable inertial sensor. In *Image Processing (ICIP), 2015 IEEE
    International Conference on*. IEEE, 168–172.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2003) Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming Ouhyoung.
    2003. On visual similarity based 3D model retrieval. In *Computer graphics forum*,
    Vol. 22\. Wiley Online Library, 223–232.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coates and Ng (2011) Adam Coates and Andrew Y Ng. 2011. Selecting receptive
    fields in deep networks. In *Advances in Neural Information Processing Systems*.
    NIPS, 2528–2536.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cosmo et al. (2016) L Cosmo, E Rodolà, MM Bronstein, A Torsello, D Cremers,
    and Y Sahillioglu. 2016. SHREC’16: Partial matching of deformable shapes. *Proc.
    3DOR* 2, 9 (2016), 12.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Couprie et al. (2013) Camille Couprie, Clément Farabet, Laurent Najman, and
    Yann LeCun. 2013. Indoor semantic segmentation using depth information. *arXiv
    preprint arXiv:1301.3572* (2013).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2017) Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber,
    Thomas Funkhouser, and Matthias Nießner. 2017. Scannet: Richly-annotated 3d reconstructions
    of indoor scenes. In *Proc. IEEE Conf. on Computer Vision and Pattern Recognition
    (CVPR)*, Vol. 1.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daras and Axenopoulos (2010) Petros Daras and Apostolos Axenopoulos. 2010. A
    3D shape retrieval framework supporting multimodal queries. *International Journal
    of Computer Vision* 89, 2-3 (2010), 229–247.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defferrard et al. (2016) Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst.
    2016. Convolutional neural networks on graphs with fast localized spectral filtering.
    In *Advances in Neural Information Processing Systems*. NIPS, 3844–3852.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2009) J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
    2009. ImageNet: A Large-Scale Hierarchical Image Database. In *CVPR09*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong (1996) Feng Dong. 1996. Three-dimensional models and applications in subsurface
    modeling. (1996).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duvenaud et al. (2015) David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre,
    Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional
    networks on graphs for learning molecular fingerprints. In *Advances in neural
    information processing systems*. NIPS, 2224–2232.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eitel et al. (2015) Andreas Eitel, Jost Tobias Springenberg, Luciano Spinello,
    Martin Riedmiller, and Wolfram Burgard. 2015. Multimodal deep learning for robust
    RGB-D object recognition. In *Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ
    International Conference on*. IEEE, 681–687.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Erdogmus and Marcel (2013) Nesli Erdogmus and Sebastien Marcel. 2013. Spoofing
    in 2D face recognition with 3D masks and anti-spoofing with Kinect. In *Biometrics:
    Theory, Applications and Systems (BTAS), 2013 IEEE Sixth International Conference
    on*. IEEE, 1–6.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fanelli et al. (2011) Gabriele Fanelli, Thibaut Weise, Juergen Gall, and Luc
    Van Gool. 2011. Real time head pose estimation from consumer depth cameras. In
    *Joint Pattern Recognition Symposium*. Springer, 101–110.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Farabet et al. (2013) Clement Farabet, Camille Couprie, Laurent Najman, and
    Yann LeCun. 2013. Learning hierarchical features for scene labeling. *IEEE transactions
    on pattern analysis and machine intelligence* 35, 8 (2013), 1915–1929.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2016) Jie Feng, Yan Wang, and Shih-Fu Chang. 2016. 3D shape retrieval
    using a single depth image from low-cost sensors. In *Applications of Computer
    Vision (WACV), 2016 IEEE Winter Conference on*. IEEE, 1–9.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2018) Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and Yue
    Gao. 2018. GVCNN: Group-View Convolutional Neural Networks for 3D Shape Recognition.
    In *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fey et al. (2017) Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich
    Müller. 2017. SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline
    Kernels. *arXiv preprint arXiv:1711.08920* (2017).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Firman (2016) Michael Firman. 2016. RGBD Datasets: Past, Present and Future.
    In *CVPR Workshop on Large Scale 3D Data: Acquisition, Modelling and Analysis*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng (2011) Jason Geng. 2011. Structured-light 3D surface imaging: a tutorial.
    *Adv. Opt. Photon.* 3, 2 (Jun 2011), 128–160. [https://doi.org/10.1364/AOP.3.000128](https://doi.org/10.1364/AOP.3.000128)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gori et al. (2005) Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005.
    A new model for learning in graph domains. In *Neural Networks, 2005\. IJCNN’05\.
    Proceedings. 2005 IEEE International Joint Conference on*, Vol. 2. IEEE, 729–734.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gregor and LeCun (2010) Karo Gregor and Yann LeCun. 2010. Emergence of complex-like
    cells in a temporal product network with local receptive fields. *arXiv preprint
    arXiv:1006.0448* (2010).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2013) Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Min Lu, and
    Jianwei Wan. 2013. Rotational projection statistics for 3D local surface description
    and object recognition. *International journal of computer vision* 105, 1 (2013),
    63–86.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2014) Yulan Guo, Jun Zhang, Min Lu, Jianwei Wan, and Yanxin Ma.
    2014. Benchmark datasets for 3D computer vision. In *Industrial Electronics and
    Applications (ICIEA), 2014 IEEE 9th Conference on*. IEEE, 1846–1851.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2017b) Xiaoguang Han, Zhen Li, Haibin Huang, Evangelos Kalogerakis,
    and Yizhou Yu. 2017b. High-Resolution Shape Completion Using Deep Neural Networks
    for Global Structure and Local Geometry Inference. In *Proceedings of IEEE International
    Conference on Computer Vision (ICCV)*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2017a) Xian-Feng Han, Jesse S Jin, Ming-Jie Wang, and Wei Jiang.
    2017a. Guided 3D point cloud filtering. *Multimedia Tools and Applications* (2017),
    1–15.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2017c) Zhizhong Han, Zhenbao Liu, Junwei Han, Chi-Man Vong, Shuhui
    Bu, and Chun Lung Philip Chen. 2017c. Mesh convolutional restricted Boltzmann
    machines for unsupervised learning of features with structure preservation on
    3-D meshes. *IEEE transactions on neural networks and learning systems* 28, 10
    (2017), 2268–2281.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Handa et al. (2016) Ankur Handa, Viorica Pătrăucean, Simon Stent, and Roberto
    Cipolla. 2016. Scenenet: An annotated model generator for indoor scene understanding.
    In *Robotics and Automation (ICRA), 2016 IEEE International Conference on*. IEEE,
    5737–5743.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hansard et al. (2012) Miles Hansard, Seungkyu Lee, Ouk Choi, and Radu Horaud.
    2012. *Time-of-Flight Cameras: Principles, Methods and Applications*. Springer
    Publishing Company, Incorporated.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.
    Deep Residual Learning for Image Recognition. *arXiv preprint arXiv:1512.03385*
    (2015).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2018) Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang Bai.
    2018. Triplet-Center Loss for Multi-View 3D Object Retrieval. In *The IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hegde and Zadeh (2016) Vishakh Hegde and Reza Zadeh. 2016. Fusionnet: 3d object
    classification using multiple data representations. *arXiv preprint arXiv:1607.05695*
    (2016).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hermosilla et al. (2018) Pedro Hermosilla, Tobias Ritschel, Pere-Pau Vázquez,
    Àlvar Vinacua, and Timo Ropinski. 2018. Monte Carlo Convolution for Learning on
    Non-Uniformly Sampled Point Clouds. *arXiv preprint arXiv:1806.01759* (2018).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2016) Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q
    Weinberger. 2016. Deep networks with stochastic depth. In *European Conference
    on Computer Vision*. Springer, 646–661.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2006) Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew. 2006.
    Extreme learning machine: theory and applications. *Neurocomputing* 70, 1-3 (2006),
    489–501.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioannidou et al. (2017) Anastasia Ioannidou, Elisavet Chatzilari, Spiros Nikolopoulos,
    and Ioannis Kompatsiaris. 2017. Deep learning advances in computer vision with
    3d data: A survey. *ACM Computing Surveys (CSUR)* 50, 2 (2017), 20.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioffe and Szegedy (2015) Sergey Ioffe and Christian Szegedy. 2015. Batch normalization:
    Accelerating deep network training by reducing internal covariate shift. *arXiv
    preprint arXiv:1502.03167* (2015).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johns et al. (2016) Edward Johns, Stefan Leutenegger, and Andrew J Davison.
    2016. Pairwise decomposition of image sequences for active multi-view recognition.
    In *Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on*.
    IEEE, 3813–3822.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson and Hebert (1999) Andrew E. Johnson and Martial Hebert. 1999. Using
    spin images for efficient object recognition in cluttered 3D scenes. *IEEE Transactions
    on pattern analysis and machine intelligence* 21, 5 (1999), 433–449.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kanezaki et al. (2016) Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida.
    2016. RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews
    from Unsupervised Viewpoints. *arXiv preprint arXiv:1603.06208* (2016).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kasun et al. (2013) Liyanaarachchi Lekamalage Chamara Kasun, Hongming Zhou,
    Guang-Bin Huang, and Chi Man Vong. 2013. Representational learning with ELMs for
    big data. (2013).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kazmi et al. (2013) Ismail Khalid Kazmi, Lihua You, and Jian Jun Zhang. 2013.
    A survey of 2d and 3d shape descriptors. In *Computer graphics, imaging and visualization
    (cgiv), 2013 10th international conference*. IEEE, 1–10.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khalil et al. (2017) Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and
    Le Song. 2017. Learning combinatorial optimization algorithms over graphs. In
    *Advances in Neural Information Processing Systems*. NIPS, 6351–6361.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling (2016) Thomas N Kipf and Max Welling. 2016. Semi-supervised
    classification with graph convolutional networks. *arXiv preprint arXiv:1609.02907*
    (2016).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Klokov and Lempitsky (2017) Roman Klokov and Victor Lempitsky. 2017. Escape
    from cells: Deep kd-networks for the recognition of 3d point cloud models. In
    *2017 IEEE International Conference on Computer Vision (ICCV)*. IEEE, 863–872.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kovnatsky et al. (2013) Artiom Kovnatsky, Michael M Bronstein, Alexander M Bronstein,
    Klaus Glashoff, and Ron Kimmel. 2013. Coupled quasi-harmonic bases. In *Computer
    Graphics Forum*, Vol. 32\. Wiley Online Library, 439–448.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. In *Advances
    in neural information processing systems*. 1097–1105.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lai et al. (2011) Kevin Lai, Liefeng Bo, Xiaofeng Ren, and Dieter Fox. 2011.
    A large-scale hierarchical multi-view rgb-d object dataset. In *Robotics and Automation
    (ICRA), 2011 IEEE International Conference on*. IEEE, 1817–1824.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2009) Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y
    Ng. 2009. Convolutional deep belief networks for scalable unsupervised learning
    of hierarchical representations. In *Proceedings of the 26th annual international
    conference on machine learning*. ACM, 609–616.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leng et al. (2015a) Biao Leng, Shuang Guo, Xiangyang Zhang, and Zhang Xiong.
    2015a. 3D object retrieval with stacked local convolutional autoencoder. *Signal
    Processing* 112 (2015), 119–128.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leng et al. (2015b) Biao Leng, Yu Liu, Kai Yu, Xiangyang Zhang, and Zhang Xiong.
    2015b. 3D object understanding with 3D convolutional neural networks. *Information
    sciences* 366 (2015), 188–201.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leng et al. (2014) Biao Leng, Xiangyang Zhang, Ming Yao, and Zhang Xiong. 2014.
    3d object classification using deep belief networks. In *International Conference
    on Multimedia Modeling*. Springer, 128–139.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Jiaxin Li, Ben M Chen, and Gim Hee Lee. 2018. SO-Net: Self-Organizing
    Network for Point Cloud Analysis. *arXiv preprint arXiv:1803.04249* (2018).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2015) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
    2015. Gated graph sequence neural networks. *arXiv preprint arXiv:1511.05493*
    (2015).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ling and Jacobs (2007) Haibin Ling and David W Jacobs. 2007. Shape classification
    using the inner-distance. *IEEE transactions on pattern analysis and machine intelligence*
    29, 2 (2007), 286–299.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2014) Zhenbao Liu, Shaoguang Chen, Shuhui Bu, and Ke Li. 2014. High-level
    semantic feature for 3D shape based on deep belief networks. In *Multimedia and
    Expo (ICME), 2014 IEEE International Conference on*. IEEE, 1–6.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2013) Zhen-Bao Liu, Shu-Hui Bu, Kun Zhou, Shu-Ming Gao, Jun-Wei
    Han, and Jun Wu. 2013. A survey on partial retrieval of 3D shapes. *Journal of
    Computer Science and Technology* 28, 5 (2013), 836–851.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2015) Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015.
    Fully convolutional networks for semantic segmentation. In *Proceedings of the
    IEEE conference on computer vision and pattern recognition*. 3431–3440.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loper et al. (2015) Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll,
    and Michael J. Black. 2015. SMPL: A Skinned Multi-Person Linear Model. *ACM Trans.
    Graphics (Proc. SIGGRAPH Asia)* 34, 6 (Oct. 2015), 248:1–248:16.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maron et al. (2017) Haggai Maron, Meirav Galun, Noam Aigerman, Miri Trope, Nadav
    Dym, Ersin Yumer, Vladimir G Kim, and Yaron Lipman. 2017. Convolutional neural
    networks on surfaces via seamless toric covers. *ACM Trans. Graph* 36, 4 (2017).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marton et al. (2011) Zoltan-Csaba Marton, Dejan Pangercic, Nico Blodow, and
    Michael Beetz. 2011. Combined 2D–3D categorization and classification for multimodal
    perception systems. *The International Journal of Robotics Research* 30, 11 (2011),
    1378–1402.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masci et al. (2015) Jonathan Masci, Davide Boscaini, Michael Bronstein, and
    Pierre Vandergheynst. 2015. Geodesic convolutional neural networks on riemannian
    manifolds. In *Proceedings of the IEEE international conference on computer vision
    workshops*. IEEE, 37–45.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maturana and Scherer (2015) Daniel Maturana and Sebastian Scherer. 2015. Voxnet:
    A 3d convolutional neural network for real-time object recognition. In *Intelligent
    Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on*. IEEE, 922–928.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monti et al. (2017) Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele
    Rodola, Jan Svoboda, and Michael M Bronstein. 2017. Geometric deep learning on
    graphs and manifolds using mixture model CNNs. In *Proc. CVPR*, Vol. 1\. IEEE,
    3.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Niepert et al. (2016) Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov.
    2016. Learning convolutional neural networks for graphs. In *International conference
    on machine learning*. JMLR.org, 2014–2023.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noh et al. (2015) Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. 2015. Learning
    deconvolution network for semantic segmentation. In *Proceedings of the IEEE International
    Conference on Computer Vision*. 1520–1528.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nowak et al. (2017) Alex Nowak, Soledad Villar, Afonso S Bandeira, and Joan
    Bruna. 2017. A note on learning algorithms for quadratic assignment with graph
    neural networks. *arXiv preprint arXiv:1706.07450* (2017).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2011) Jaesik Park, Hyeongwoo Kim, Yu-Wing Tai, Michael S Brown,
    and Inso Kweon. 2011. High quality depth map upsampling for 3d-tof cameras. In
    *Computer Vision (ICCV), 2011 IEEE International Conference on*. IEEE, 1623–1630.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2016a) Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
    2016a. PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.
    *arXiv preprint arXiv:1612.00593* (2016).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2017a) Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
    2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation.
    *Proc. Computer Vision and Pattern Recognition (CVPR), IEEE* 1, 2 (2017), 4.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qi et al. (2016b) Charles R Qi, Hao Su, Matthias Nießner, Angela Dai, Mengyuan
    Yan, and Leonidas J Guibas. 2016b. Volumetric and multi-view cnns for object classification
    on 3d data. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*. IEEE, 5648–5656.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2017b) Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas.
    2017b. Pointnet++: Deep hierarchical feature learning on point sets in a metric
    space. In *Advances in Neural Information Processing Systems*. NIPS, 5105–5114.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rangel et al. (2017) José Carlos Rangel, Vicente Morell, Miguel Cazorla, Sergio
    Orts-Escolano, and José García-Rodríguez. 2017. Object recognition in noisy RGB-D
    data using GNG. *Pattern Analysis and Applications* 20, 4 (2017), 1061–1076.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ravanbakhsh et al. (2016) Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos.
    2016. Deep learning with sets and point clouds. *arXiv preprint arXiv:1611.04500*
    (2016).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Riegler et al. (2017) Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.
    2017. OctNet: Learning Deep 3D Representations at High Resolutions. In *The IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roveri et al. (2018) Riccardo Roveri, Lukas Rahmann, A Cengiz Oztireli, and
    Markus Gross. 2018. A Network Architecture for Point Cloud Classification via
    Automatic Depth Images Generation. In *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*. 4176–4184.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rusu et al. (2009) Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. 2009. Fast
    point feature histograms (FPFH) for 3D registration. In *Robotics and Automation,
    2009. ICRA’09\. IEEE International Conference on*. IEEE, 3212–3217.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rusu et al. (2008) Radu Bogdan Rusu, Nico Blodow, Zoltan Csaba Marton, and Michael
    Beetz. 2008. Aligning point cloud views using persistent feature histograms. In
    *Intelligent Robots and Systems, 2008\. IROS 2008\. IEEE/RSJ International Conference
    on*. IEEE, 3384–3391.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saint et al. (2018) Alexandre Fabian A Saint, Eman Ahmed, Abd El Rahman Shabayek,
    Kseniya Cherenkova, Gleb Gusev, Djamila Aouada, and Björn Ottersten. 2018. 3DBodyTex:
    Textured 3D Body Dataset. In *2018 Sixth International Conference on 3D Vision
    (3DV 2018)*. [https://cvdatasets.uni.lu/datasets/](https://cvdatasets.uni.lu/datasets/)
    Accessed on 05.04.2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saito et al. (2016) Shunsuke Saito, Tianye Li, and Hao Li. 2016. Real-time facial
    segmentation and performance capture from rgb input. In *European Conference on
    Computer Vision*. Springer, 244–261.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samet (1984) Hanan Samet. 1984. The quadtree and related hierarchical data structures.
    *ACM Computing Surveys (CSUR)* 16, 2 (1984), 187–260.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scarselli et al. (2009) Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
    Hagenbuchner, and Gabriele Monfardini. 2009. The graph neural network model. *IEEE
    Transactions on Neural Networks* 20, 1 (2009), 61–80.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schroff et al. (2015) Florian Schroff, Dmitry Kalenichenko, and James Philbin.
    2015. FaceNet: A Unified Embedding for Face Recognition and Clustering. In *The
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwarz et al. (2015) Max Schwarz, Hannes Schulz, and Sven Behnke. 2015. RGB-D
    object recognition and pose estimation based on pre-trained convolutional neural
    network features. In *Robotics and Automation (ICRA), 2015 IEEE International
    Conference on*. IEEE, 1329–1335.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sedaghat et al. (2016) Nima Sedaghat, Mohammadreza Zolfaghari, Ehsan Amiri,
    and Thomas Brox. 2016. Orientation-boosted voxel nets for 3D object recognition.
    *arXiv preprint arXiv:1604.03351* (2016).
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sermanet et al. (2013) Pierre Sermanet, David Eigen, Xiang Zhang, Michaël Mathieu,
    Rob Fergus, and Yann LeCun. 2013. Overfeat: Integrated recognition, localization
    and detection using convolutional networks. *arXiv preprint arXiv:1312.6229* (2013).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sfikas et al. (2017a) Konstantinos Sfikas, Ioannis Pratikakis, and Theoharis
    Theoharis. 2017a. Ensemble of PANORAMA-based convolutional neural networks for
    3D model classification and retrieval. *Computers & Graphics* (2017).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sfikas et al. (2017b) Konstantinos Sfikas, Theoharis Theoharis, and Ioannis
    Pratikakis. 2017b. Exploiting the PANORAMA representation for convolutional neural
    network classification and retrieval. In *Eurographics Workshop on 3D Object Retrieval*.
    The Eurographics Association.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharma et al. (2016) Abhishek Sharma, Oliver Grau, and Mario Fritz. 2016. VConv-DAE:
    Deep Volumetric Shape Learning Without Object Labels. In *Geometry Meets Deep
    Learning Workshop at European Conference on Computer Vision (ECCV-W)*.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2015) Baoguang Shi, Song Bai, Zhichao Zhou, and Xiang Bai. 2015.
    Deeppano: Deep panoramic representation for 3-d shape recognition. *IEEE Signal
    Processing Letters* 22, 12 (2015), 2339–2343.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sinha et al. (2016) Ayan Sinha, Jing Bai, and Karthik Ramani. 2016. Deep learning
    3D shape surfaces using geometry images. In *European Conference on Computer Vision*.
    Springer, 223–240.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Socher et al. (2012) Richard Socher, Brody Huval, Bharath Bath, Christopher D
    Manning, and Andrew Y Ng. 2012. Convolutional-recursive deep learning for 3d object
    classification. In *Advances in Neural Information Processing Systems*. NIPS,
    656–664.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song and Xiao (2016) Shuran Song and Jianxiong Xiao. 2016. Deep Sliding Shapes
    for Amodal 3D Object Detection in RGB-D Images. In *CVPR*. IEEE.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2017) Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis
    Savva, and Thomas Funkhouser. 2017. Semantic scene completion from a single depth
    image. In *Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference
    on*. IEEE, 190–198.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2015) Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller.
    2015. Multi-view convolutional neural networks for 3d shape recognition. In *Proceedings
    of the IEEE international conference on computer vision*. 945–953.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sukhbaatar et al. (2016) Sainbayar Sukhbaatar, Rob Fergus, et al. 2016. Learning
    multiagent communication with backpropagation. In *Advances in Neural Information
    Processing Systems*. NIPS, 2244–2252.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2009) Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. 2009. A concise
    and provably informative multi-scale signature based on heat diffusion. In *Computer
    graphics forum*, Vol. 28\. Wiley Online Library, 1383–1392.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sylvain Lefebvre and Neyret (2005) Samuel Hornus Sylvain Lefebvre and Fabrice
    Neyret. April,2005. [https://developer.nvidia.com/gpugems/GPUGems2.html](https://developer.nvidia.com/gpugems/GPUGems2.html)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2017) Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
    Alexander A Alemi. 2017. Inception-v4, inception-resnet and the impact of residual
    connections on learning.. In *AAAI*, Vol. 4\. AAAI, 12.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tangelder and Veltkamp (2004) Johan WH Tangelder and Remco C Veltkamp. 2004.
    A survey of content based 3D shape retrieval methods. In *Shape Modeling Applications,
    2004. Proceedings*. IEEE, 145–156.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tatarchenko et al. (2017) Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas
    Brox. 2017. Octree generating networks: Efficient convolutional architectures
    for high-resolution 3d outputs. *CoRR, abs/1703.09438* (2017).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Venkatakrishnan et al. (2018) Shaileshh Bojja Venkatakrishnan, Mohammad Alizadeh,
    and Pramod Viswanath. 2018. Graph2Seq: Scalable Learning Dynamics for Graphs.
    *arXiv preprint arXiv:1802.04948* (2018).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Verma et al. (2018) Nitika Verma, Edmond Boyer, and Jakob Verbeek. 2018. FeaStNet:
    Feature-Steered Graph Convolutions for 3D Shape Analysis. In *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*. 2598–2606.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals et al. (2015) Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. 2015.
    Order matters: Sequence to sequence for sets. *arXiv preprint arXiv:1511.06391*
    (2015).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017b) Chu Wang, Marcello Pelillo, and Kaleem Siddiqi. 2017b. Dominant
    Set Clustering and Pooling for Multi-View 3D Object Recognition.. In *Proceedings
    of British Machine Vision Conference (BMVC)*. BMVC.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Chu Wang, Babak Samari, and Kaleem Siddiqi. 2018. Local Spectral
    Graph Convolution for Point Set Feature Learning. *arXiv preprint arXiv:1803.05827*
    (2018).
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2015) Fang Wang, Le Kang, and Yi Li. 2015. Sketch-based 3d shape
    retrieval using convolutional neural networks. In *Computer Vision and Pattern
    Recognition (CVPR), 2015 IEEE Conference on*. IEEE, 1875–1883.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017a) Weiyue Wang, Qiangui Huang, Suya You, Chao Yang, and Ulrich
    Neumann. 2017a. Shape inpainting using 3d generative adversarial network and recurrent
    convolutional networks. *arXiv preprint arXiv:1711.06375* abs/1711.06375 (2017).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2016) Yueqing Wang, Zhige Xie, Kai Xu, Yong Dou, and Yuanwu Lei.
    2016. An efficient and effective convolutional auto-encoder extreme learning machine
    network for 3d feature learning. *Neurocomputing* 174 (2016), 988–998.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2016) Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. 2016.
    A Discriminative Feature Learning Approach for Deep Face Recognition. In *Computer
    Vision – ECCV 2016*, Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (Eds.).
    Springer International Publishing, Cham, 499–515.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wohlkinger and Vincze (2011) Walter Wohlkinger and Markus Vincze. 2011. Ensemble
    of shape functions for 3d object classification. In *Robotics and Biomimetics
    (ROBIO), 2011 IEEE International Conference on*. IEEE, 2987–2992.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2016) Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh
    Tenenbaum. 2016. Learning a probabilistic latent space of object shapes via 3d
    generative-adversarial modeling. In *Advances in Neural Information Processing
    Systems*. NIPS, 82–90.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2015a) Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang
    Zhang, Xiaoou Tang, and Jianxiong Xiao. 2015a. 3d shapenets: A deep representation
    for volumetric shapes. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. IEEE, 1912–1920.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2015b) Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang
    Zhang, Xiaoou Tang, and Jianxiong Xiao. 2015b. 3D ShapeNets: A Deep Representation
    for Volumetric Shapes. In *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*. IEEE.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiang et al. (2015) Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese.
    2015. Data-driven 3d voxel patterns for object category recognition. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. IEEE, 1903–1911.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2015) Zhige Xie, Kai Xu, Wen Shan, Ligang Liu, Yueshan Xiong, and
    Hui Huang. 2015. Projective Feature Learning for 3D Shapes with Multi-View Depth
    Images. *Computer Graphics Forum* 34, 7 (2015), 1–11.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu and Todorovic (2016) Xu Xu and Sinisa Todorovic. 2016. Beam search for learning
    a deep convolutional neural network of 3d shapes. In *Pattern Recognition (ICPR),
    2016 23rd International Conference on*. IEEE, 3506–3511.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2018) Chenggang Yan, Hongtao Xie, Dongbao Yang, Jian Yin, Yongdong
    Zhang, and Qionghai Dai. 2018. Supervised hash coding with deep neural network
    for environment perception of intelligent vehicles. *IEEE transactions on intelligent
    transportation systems* 19, 1 (2018), 284–295.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2018) Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. 2018.
    FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation. In *Proc. IEEE
    Conf. on Computer Vision and Pattern Recognition (CVPR)*, Vol. 3. IEEE.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2016) Li Yi, Vladimir G Kim, Duygu Ceylan, I Shen, Mengyan Yan, Hao
    Su, Cewu Lu, Qixing Huang, Alla Sheffer, Leonidas Guibas, et al. 2016. A scalable
    active framework for region annotation in 3d shape collections. *ACM Transactions
    on Graphics (TOG)* 35, 6 (2016), 210.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2008) Lijun Yin, Xiaochen Chen, Yi Sun, Tony Worm, and Michael Reale.
    2008. A high-resolution 3D dynamic facial expression database. In *Automatic Face
    & Gesture Recognition, 2008\. FG’08\. 8th IEEE International Conference on*. IEEE,
    1–6.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2006) Lijun Yin, Xiaozhou Wei, Yi Sun, Jun Wang, and Matthew J Rosato.
    2006. A 3D facial expression database for facial behavior research. In *Automatic
    face and gesture recognition, 2006\. FGR 2006\. 7th international conference on*.
    IEEE, 211–216.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2018) Tan Yu, Meng, Jingjing, Yuan, and Junsong. 2018. Multi-view
    Harmonized Bilinear Network for 3D Object Recognition. In *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*. IEEE, 186–194.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zaheer et al. (2017) Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas
    Poczos, Ruslan R Salakhutdinov, and Alexander J Smola. 2017. Deep sets. In *Advances
    in Neural Information Processing Systems*. Curran Associates, Inc., 3394–3404.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zanuttigh and Minto (2017) P. Zanuttigh and L. Minto. 2017. Deep learning for
    3D shape classification from multiple depth maps. In *2017 IEEE International
    Conference on Image Processing (ICIP)*. IEEE, 3615–3619. [https://doi.org/10.1109/ICIP.2017.8296956](https://doi.org/10.1109/ICIP.2017.8296956)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2007) Lisha Zhang, M João da Fonseca, Alfredo Ferreira, and Combinando
    Realidade Aumentada e Recuperação. 2007. Survey on 3D shape descriptors. *FundaÃgao
    para a Cincia ea Tecnologia, Lisboa, Portugal, Tech. Rep. Technical Report, DecorAR
    (FCT POSC/EIA/59938/2004)* 3 (2007).
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2014) Xing Zhang, Lijun Yin, Jeffrey F Cohn, Shaun Canavan, Michael
    Reale, Andy Horowitz, Peng Liu, and Jeffrey M Girard. 2014. Bp4d-spontaneous:
    a high-resolution spontaneous 3d dynamic facial expression database. *Image and
    Vision Computing* 32, 10 (2014), 692–706.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2016) Zheng Zhang, Jeff M Girard, Yue Wu, Xing Zhang, Peng Liu,
    Umur Ciftci, Shaun Canavan, Michael Reale, Andy Horowitz, Huiyuan Yang, et al.
    2016. Multimodal spontaneous emotion corpus for human behavior analysis. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. IEEE, 3438–3446.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2017) Jing Zhao, Xijiong Xie, Xin Xu, and Shiliang Sun. 2017.
    Multi-view learning overview: Recent progress and new challenges. *Information
    Fusion* 38 (2017), 43–54.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhi et al. (2018) Shuaifeng Zhi, Yongxiang Liu, Xiang Li, and Yulan Guo. 2018.
    Toward real-time 3D object recognition: A lightweight volumetric CNN framework
    using multitask learning. *Computers & Graphics* 71 (2018), 199 – 207. [https://doi.org/10.1016/j.cag.2017.10.007](https://doi.org/10.1016/j.cag.2017.10.007)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2016) Zhuotun Zhu, Xinggang Wang, Song Bai, Cong Yao, and Xiang
    Bai. 2016. Deep learning representation using autoencoder for 3D shape retrieval.
    *Neurocomputing* 204 (2016), 41–50.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zollhöfer et al. (2014) Michael Zollhöfer, Matthias Nießner, Shahram Izadi,
    Christoph Rehmann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon,
    Charles Loop, Christian Theobalt, et al. 2014. Real-time non-rigid reconstruction
    using an RGB-D camera. *ACM Transactions on Graphics (TOG)* 33, 4 (2014), 156.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
