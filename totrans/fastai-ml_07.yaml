- en: 'Machine Learning 1: Lesson 7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习1：第7课
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-7-69c50bc5e9af](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-7-69c50bc5e9af)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-7-69c50bc5e9af](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-7-69c50bc5e9af)
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自* [*机器学习课程*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*
    的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢* [*Jeremy*](https://twitter.com/jeremyphoward)
    *和* [*Rachel*](https://twitter.com/math_rachel) *给了我这个学习的机会。*'
- en: '[Video](https://youtu.be/O5F9vR2CNYI)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[视频](https://youtu.be/O5F9vR2CNYI)'
- en: We are going to finish building our own random forest from scratch! But before
    we do, I wanted to tackle a few things that have come up during the week.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将完成从头开始构建我们自己的随机森林！但在此之前，我想解决一些在这一周中出现的问题。
- en: Position of random forests in general [[0:17](https://youtu.be/O5F9vR2CNYI?t=17s)]
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林在一般情况下的位置
- en: We spent about half of this course doing random forests and then after today,
    the second half of this course will be neural network broadly defined. This is
    because these two represent the two key classes of techniques which cover nearly
    everything that you are likely need to do. Random forest belongs to the class
    of techniques of decision tree ensembles along with gradient boosting machines
    being the other key type, and some variants like extremely randomized trees. They
    have the benefit that they are highly interpretable, scalable, flexible, work
    well for most kinds of data. They have the downside that they don’t extrapolate
    at all to data that’s outside the range that you’ve seen as we looked at at the
    end of last week’s session. But they are great starting point. I think there is
    a huge catalogue of machine learning tools out there and a lot of courses and
    books don’t attempt to curate that down and say for these kinds of problems, use
    this, for those kinds problems, use that, finished. But they are rather like here
    is a description of 100 different algorithms and you just don’t need them. I don’t
    see why you would ever use a support vector machine today, for instance. No reason
    at all I could think of doing that. People loved studying them in the 90s because
    they are very theoretically elegant and you can really write a lot of math about
    support vector machines and people did, but in practice I don’t see them as having
    any place.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大约花了一半的课程时间来讲解随机森林，然后在今天之后，这门课程的第二半将广义地讲解神经网络。这是因为这两者代表了覆盖几乎所有你可能需要做的技术的两个关键类别。随机森林属于决策树集成技术类别，与梯度提升机是另一种关键类型，还有一些变体，比如极端随机树。它们的好处是它们非常易解释，可扩展，灵活，适用于大多数类型的数据。它们的缺点是它们完全不会对超出你所见范围的数据进行外推，就像我们在上周课程结束时看到的那样。但它们是一个很好的起点。我认为有很多机器学习工具的目录，很多课程和书籍并没有试图筛选出来，说对于这些问题，使用这个，对于那些问题，使用那个，完了。但它们更像是这里有100种不同算法的描述，而你根本不需要它们。比如，我不明白为什么今天你会使用支持向量机。我想不出任何理由去做那样的事情。人们在90年代喜欢研究它们，因为它们在理论上非常优雅，你可以写很多关于支持向量机的数学，人们确实这样做了，但实际上我认为它们没有任何用武之地。
- en: There’s a lot of techniques that you could include in an exhaustive list of
    every way that people adopt machine learning problems, but I would rather tell
    you how to actually solve machine learning problems in practice. We are about
    to finish today the first class which is one type of decision tree ensemble, in
    part two, Yanett will tell you about the other key type being gradient boosting,
    and we are about to launch next lesson into neural nets which include all kinds
    of generalized linear model (GLM), ridge regression, elastic net lasso, logistic
    regression etc are all variants of neural nets.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在详尽的列表中，有很多技术可以包括在人们采用机器学习问题的每一种方式中，但我更愿意告诉你如何实际解决机器学习问题。我们即将结束今天的第一堂课，这是一种决策树集成的一种类型，在第二部分，Yanett将告诉你另一种关键类型，即梯度提升，我们即将启动下一课程，介绍神经网络，其中包括各种广义线性模型（GLM）、岭回归、弹性网络套索、逻辑回归等都是神经网络的变体。
- en: Interestingly, Leo Breiman who created random forests did so very late in his
    life and unfortunately passed away not many years later. So partly because of
    that, very little has been written about them in the academic literature, partly
    because SVM were just taken over at that point that other people didn’t look at
    them. Also because they are quite hard to grasp at a theoretical level (analyzing
    them theoretically), it’s quite hard to write conference papers about them or
    academic papers about them. So there hasn’t been that much written about them.
    But there’s been a new wave in recent years of empirical machine learning like
    what actually works. Kaggle has been a part of that but also like companies using
    machine learning to make loads of money like Amazon and Google. So nowadays a
    lot of people are writing about decision tree ensemble and creating better software
    for decision tree ensembles like GBM and xgboost, and ranger for R, and scikit-learn,
    and so forth. But a lot of this is being done in industry rather than academia
    but it’s encouraging to see. There’s certainly more work being done in deep learning
    than in decision tree ensembles particularly in academia but there is a lot of
    progress being made in both. If you look at off the packages being used today
    for decision tree ensembles, all the best ones the top 5 or 6, I don’t know that
    any of them really existed five years ago, maybe other than sklearn, or even three
    years ago. So that’s been good. But I think there’s a lot of work still to be
    done. We talked about, for example, figuring out what interactions are the most
    important last week. Some of you pointed out in the forum that actually there
    is such a project already for a gradient boosting machines which is great but
    it doesn’t seem that there’s anything like that yet for random forests. Random
    forests do have a nice benefit over GBMs that they are harder to screw up and
    easier to scale. So hopefully that’s something that this community might help
    fix.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，创造随机森林的Leo Breiman在他的晚年才做出了这一成就，不幸的是，他在之后不久就去世了。因此，关于随机森林的学术文献很少，部分原因是因为在那个时候支持向量机（SVM）开始流行，其他人没有关注它们。另一个原因是随机森林在理论层面上相当难以理解（在理论上分析它们），很难撰写关于它们的会议论文或学术论文。因此，关于它们的研究并不多。但近年来出现了一股新的经验机器学习浪潮，关注的是什么实际上有效。Kaggle是其中的一部分，但也有像亚马逊和谷歌这样的公司利用机器学习赚取大量利润。因此，如今很多人都在写关于决策树集成的文章，并为决策树集成创建更好的软件，如GBM和xgboost，以及R的ranger和scikit-learn等等。但很多这方面的工作是在工业界而不是学术界进行的，但这是令人鼓舞的。当然，目前在学术界进行的深度学习工作比决策树集成要多，但两者都在取得很大进展。如果看看今天用于决策树集成的软件包，排名前五或六名的最好的软件包，我不确定其中有哪个在五年前真的存在，也许除了sklearn之外，甚至三年前也没有。这是一个好的现象。但我认为还有很多工作要做。比如，上周我们讨论了找出哪些交互作用最重要的问题。你们中一些人在论坛中指出，实际上已经有一个梯度提升机的项目，这很棒，但似乎还没有类似的项目用于随机森林。随机森林相比GBM有一个很好的优势，那就是它们更难出错，更容易扩展。因此，希望这个社区可以帮助解决这个问题。
- en: Size of your validation set [[5:42](https://youtu.be/O5F9vR2CNYI?t=5m42s)]
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 您的验证集大小 [[5:42](https://youtu.be/O5F9vR2CNYI?t=5m42s)]
- en: Another question I had during the week was about the size of your validation
    set. How big should it be? So to answer this question about how big does your
    validation set need to be, you first need to answer the question how precisely
    do I need to know the accuracy of this algorithm. If the validation set that you
    have is saying this is 70% accurate and if somebody said well, is it 75% or 65%
    or 70% and the answer was “I don’t know, anything in that range is close enough”
    that would be one answer. Where else, if it’s like is that 70% or 70.01% or 69.99%?
    Then that’s something else again. So you need to start out by saying how accurate
    do I need this.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我在这周期间遇到的问题是关于验证集的大小。它应该有多大呢？所以要回答这个关于验证集需要多大的问题，你首先需要回答这个算法的准确度我需要知道多精确。如果你的验证集显示这是70%的准确率，如果有人问说，那是75%还是65%还是70%，答案是“我不知道，这个范围内的任何值都足够接近”，那就是一个答案。另一方面，如果是70%还是70.01%还是69.99%呢？那又是另一回事了。所以你需要先问自己，我需要多准确。
- en: For example, in the deep learning course, we’ve been looking at dogs vs. cats
    images and the models that we are looking at had about a 99.4, 99.5% accuracy
    on the validation set. And a validation set size was 2000\. In fact, let’s do
    this in Excel, that’ll be a bit easier. So the number of incorrect is something
    around `(1 - accuracy) * n`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在深度学习课程中，我们一直在研究狗和猫的图像，我们所研究的模型在验证集上的准确率大约为99.4%，99.5%。验证集的大小为2000。实际上，让我们在Excel中做这个，这会更容易一些。因此，错误的数量大约是`(1
    - 准确率) * n`。
- en: '![](../Images/e10f5e11c9f6390985282d883e29b918.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e10f5e11c9f6390985282d883e29b918.png)'
- en: So we were getting about 12 wrong. And the number of cats we had is a half so
    the number of wrong cats is about 6\. Then we run a new model and we find instead
    that the accuracy has gone to 99.2%. Then it’s like okay, is this less good at
    finding cats? That’s like well, it got 2 more cats wrong, so it’s probably not.
    Does this matter? Does 99.4 vs. 99.2 matter? If it wasn’t about cats and dogs
    but it was about finding fraud, then the difference between a .6% error rate and
    .8% error rate is like 25% of your cost of fraud so that can be huge.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们大约有12只错误。我们拥有的猫的数量是一半，所以错误的猫的数量大约是6只。然后我们运行了一个新模型，发现准确率提高到了99.2%。然后就像，好吧，这个模型在找猫方面是否不太好？嗯，它多找错了2只猫，所以可能不是。这重要吗？99.4和99.2有关系吗？如果不是关于猫和狗，而是关于发现欺诈，那么0.6%的错误率和0.8%的错误率之间的差异就相当于欺诈成本的25%，这可能是巨大的。
- en: It’s really interesting when ImageNet came out earlier this year, the new competition
    results came out and the error had gone down from 3% to 2% and I saw a lot of
    people on the internet, famous machine learning researchers being like meh, some
    Chinese guys got it better from like 97% to 98% — it’s like statistically not
    even significant who cares kind of a thing. But actually I thought holy crap this
    Chinese team just blew away the state-of-the-art image recognition and the old
    one was 50% less accurate than the new one. That’s actually the right way to think
    about it, isn’t it. Because it’s like we were trying to recognize which tomatoes
    were ripe and which ones weren’t, and the old approach, 50% of the time more was
    letting in the unripe tomatoes or 50% more of the time, we were accepting fraudulent
    customers. That’s a really big difference. So just because this particular validation
    set, we can’t really see 6 versus 8 doesn’t mean the 0.2% difference isn’t important.
    It could be. So my rule of thumb is that this number of how many observations
    you are actually looking at, I want that generally be somewhere higher than 22\.
    Why 22? Because 22 is the magic number where the t-distribution roughly turns
    into the normal distribution. So as you may have learnt, the t-distribution is
    the normal distribution for small datasets. So in another words, once we have
    22 of something or more, it starts to behave kind of normally in both sense of
    the words like it’s kind of more stable and you can kind of understand it better.
    So that’s my magic number when somebody says do I have enough of something, I
    kind of start out by saying do you have 22 observations of the thing of interest.
    So if you were looking at lung cancer and you had a dataset that had a thousand
    people without lung cancer and 20 people with lung cancer, I’d be like I very
    much doubt we are going to make much progress because we haven’t even got 20 of
    the thing you want. So ditto with a validation set. If you don’t have 20 of the
    thing you want, that is very unlikely to be useful or like at the level of accuracy
    we need. It’s not plus or minus 20, it’s just that that’s the point where I’m
    thinking be a bit careful.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 今年早些时候，ImageNet发布时真的很有趣，新的竞赛结果出来了，错误率从3%降到了2%，我看到很多人在互联网上，一些著名的机器学习研究人员都觉得，一些中国人把准确率从97%提高到了98%
    —— 这在统计上甚至不重要，谁在乎呢。但实际上我觉得哇塞，这支中国团队刚刚超越了最先进的图像识别技术，旧技术的准确率比新技术低了50%。这才是正确的思考方式，不是吗。因为我们试图识别哪些西红柿是成熟的，哪些不是，而旧方法，有50%的时间多让进了未成熟的西红柿，或者说有50%的时间，我们接受了欺诈性的客户。这是一个非常大的差异。所以仅仅因为这个特定的验证集，我们看不出6和8的区别，并不意味着0.2%的差异不重要。它可能很重要。所以我的经验法则是，你实际上看了多少观察值，我希望这个数字通常要高于22。为什么是22？因为22是t-分布大致变成正态分布的魔法数字。所以你可能已经学过，t-分布是小数据集的正态分布。换句话说，一旦我们有了22个或更多的东西，它开始在两个意义上表现得有点正常，就像它更加稳定，你可以更好地理解它。所以当有人问我是否有足够的东西时，我通常会说你是否有22个感兴趣的事物的观察值。所以如果你在研究肺癌，你有一个数据集，其中有一千个没有肺癌的人和20个患有肺癌的人，我会说我非常怀疑我们会取得多少进展，因为我们甚至没有得到你想要的20个东西。同样适用于验证集。如果你没有你想要的20个东西，那很可能是没有用的，或者说不符合我们需要的准确度水平。这不是加减20，只是我在考虑时会有点小心。
- en: '**Question**: So just to be clear, you want 22 to be the number of samples
    in each set like in the validation, the test, and the train [[11:26](https://youtu.be/O5F9vR2CNYI?t=11m26s)]?
    So what I’m saying is if there’s less than 22 of a class in any of the sets then
    it’s getting pretty unstable at that point. So that’s just like the first rule
    of thumb. But then what I would actually do is start practicing what we learnt
    about the binomial distribution or actually Bernoulli distribution. So what is
    the mean of the binomial distribution of n samples and probability p? `n*p`. n
    times p is the mean. So if you’ve got a 50% chance of getting a head and you toss
    it a hundred times, on average you get 50 heads. Then what’s the standard deviation?
    `n*p*(1-p)`.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：所以清楚一点，你想要每组样本的数量是22，就像在验证集、测试集和训练集中一样吗？所以我的意思是，如果任何一组中某个类别的样本少于22个，那么在那一点上就会变得非常不稳定。这就像是第一个经验法则。但接下来我会开始练习我们学到的关于二项分布或伯努利分布的知识。那么n个样本和概率p的二项分布的均值是多少？`n*p`。n乘以p就是均值。所以如果你有50%的机会抛硬币得到正面，你抛100次，平均得到50次正面。那么标准差是多少？`n*p*(1-p)`。
- en: '![](../Images/0d40e591c369d12b90392c0c108b6aae.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: So the first number you don’t have to remember — it’s intuitively obvious. The
    second one is one that try to remember forevermore because not only does it come
    up all the time, the people that you work with will all have forgotten it so you’ll
    be like the one person in the conversation who could immediately go “we don’t
    have to run this 100 times, I can tell you straight away it’s binomial, it’s going
    to be `n*p*(1-p)`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Then there is the standard error. The standard error is if you run a bunch of
    trials, each time getting a mean, what is the standard deviation of the mean.
    I don’t think you guys have covered this yet. So this is really important because
    this means if you train a hundred models, each time the validation set accuracy
    is like the mean of a distribution. So therefore, the standard deviation of that
    validation set accuracy, it can be calculated with the standard error and this
    is equal to the standard deviation divided by square root n.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是标准误差。标准误差是指如果你运行一堆试验，每次得到一个平均值，那么平均值的标准偏差是多少。我不认为你们已经涵盖了这个内容。这是非常重要的，因为这意味着如果你训练了一百个模型，每次验证集的准确率就像是一个分布的平均值。因此，验证集准确率的标准偏差可以用标准误差来计算，这等于标准偏差除以n的平方根。
- en: '![](../Images/4b1d0561e37d2f978ca3080123f53275.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: 把这个文本按行翻译成中文。
- en: So one approach to figuring out is my validation set big enough is train your
    model 5 times with exactly the same hyper parameters each time and look at the
    validation set accuracy each time and there is a mean and a standard deviation
    of 5 numbers you could use or a maximum and a minimum you can use. But to save
    yourself some time, you can figure out straight away that okay, I have a .99 accuracy
    as to whether I get the cat correct or not correct. So therefore the standard
    deviation is equal to 0.99 * 0.01 and then I can get the standard error of that.
    So basically the size of the validation set you need, it’s like however big it
    has to be such that your insights about accuracy good enough for your particular
    business problem. So like I say, the simple way to do it is to pick a validation
    set of like a size of a thousand, train 5 models, and see how much the validation
    set accuracy varies and if they are all close enough for what you need, then you
    are fine. If it’s not, maybe you should make it bigger or maybe you should consider
    using cross-validation instead. So as you can see, it really depends on what it
    is you are trying to do, how common your less common class is, and how accurate
    your model is.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，确定我的验证集是否足够大的一种方法是，每次使用完全相同的超参数训练模型5次，然后查看每次的验证集准确率，可以计算出5个数字的平均值和标准差，或者可以使用最大值和最小值。但为了节省时间，您可以立即确定，我有一个0.99的准确率，无论我是否正确地识别了猫。因此，标准差等于0.99
    * 0.01，然后可以得到标准误差。因此，您需要的验证集大小，就像它必须足够大，以便您对准确性的洞察对于您特定的业务问题足够好。因此，简单的方法是选择一个大小为一千的验证集，训练5个模型，查看验证集准确率的变化情况，如果它们都足够接近您所需的水平，那么就可以了。如果不是，也许您应该使其更大，或者考虑改用交叉验证。因此，可以看到，这取决于您试图做什么，您的较不常见类别有多常见，以及您的模型有多准确。
- en: '**Question**: About the less common classes, if you have less than 22, let’s
    say you have one sample of something, let’s say it’s a face and I only have one
    representation from that particular country, do I toss that into the training
    set and adds variety, so I pull it out completely out of the dataset or do I put
    it in a test set instead of the validation set [[15:47](https://youtu.be/O5F9vR2CNYI?t=15m47s)]?
    So you certainly couldn’t put it in the test or the validation set in general
    because you are asking can I recognize something I’ve never seen before. But actually
    this question of can I recognize something I have not seen before, there is actually
    a whole class of models specifically for that purpose — it’s called one-shot learning
    which is you get to see something once and you have to recognize it again or zero-shot
    learning which is where you have to recognize something you have never seen before.
    We are not going to cover them in this course but they can be useful for things
    like face recognition like is this the same person I have seen before. So generally
    speaking, obviously for something like that to work, it’s not that you’ve never
    seen a face before, it’s that you’ve never seen Melissa’s face before. So you
    see Melissa’s face once and you have to recognize it again. So in general, your
    validation set and test set need to have the same mix or frequency of the observations
    that you are going to see in production in the real world. And your training set
    should have an equal number in each class and if you don’t, just replicate the
    less common one until it is equal. I think we’ve mentioned this paper before,
    a very recent paper that came out, they tried lots of different approaches to
    training with unbalanced datasets and found consistently that over sampling the
    less common class until that is the same size as the more common class is always
    the right thing to do. So you could literally copy, like I’ve only got ten example
    of people with cancer and a hundred without, so I can just copy those 10 another
    90 times, that’s kind of a little memory inefficient so a lot of things including
    I think sklearn’s random forests have a class weights parameter that says each
    time you are boot strapping or resampling, I want you to sample the less common
    class with a higher probability. Or ditto if you are doing deep learning, make
    sure in your mini-batch, it’s not randomly sampled but it’s a stratified samples
    of the less common class is picked more often.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：关于较少常见的类别，如果你少于22个，比如你有一个样本，比如是一个脸，我只有一个来自那个特定国家的代表，我是把它放入训练集并增加多样性，还是完全从数据集中删除，或者我把它放入测试集而不是验证集？所以你肯定不能把它放入测试集或验证集，因为你在问我能否识别我以前从未见过的东西。但实际上，关于我能否识别我以前从未见过的东西，实际上有一个专门用于这个目的的模型类别——它被称为一次性学习，你只能看到一次东西，然后必须再次识别它，或者零次学习，你必须识别你以前从未见过的东西。我们在本课程中不会涵盖它们，但它们对于像人脸识别这样的事情可能会有用，比如这是我以前见过的同一个人吗。所以一般来说，显然，为了使这样的事情起作用，不是你以前从未见过一个脸，而是你以前从未见过Melissa的脸。所以你看到Melissa的脸一次，然后你必须再次识别它。所以一般来说，你的验证集和测试集需要具有与你将在实际生产中看到的观察频率相同的混合或频率。你的训练集应该每个类别有相等数量，如果没有，只需复制较少常见的类别直到相等。我想我们之前提到过这篇论文，一篇最近发表的论文，他们尝试了许多不同的方法来训练不平衡的数据集，并一直发现，直到较少常见的类别与较常见的类别大小相同为止，过采样较少常见的类别总是正确的做法。所以你可以简单地复制，比如我只有十个患癌症的人的例子，而没有百个，所以我可以再复制这10个另外90次，这在一定程度上是一种内存效率低下的方式，包括我认为sklearn的随机森林有一个类别权重参数，每次你进行自助抽样或重新采样时，我希望你以更高的概率对较少常见的类别进行抽样。或者如果你正在进行深度学习，确保在你的小批量中，不是随机抽样，而是较少常见的类别的分层样本更频繁地被选中。
- en: Getting back to finishing off random forests [[18:39](https://youtu.be/O5F9vR2CNYI?t=18m39s)]
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回到完成随机森林的部分 [18:39](https://youtu.be/O5F9vR2CNYI?t=18m39s)
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson3-rf_foundations.ipynb)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson3-rf_foundations.ipynb)'
- en: 'So let’s get back to finishing off our random forests. What we are going to
    do today is we are going to finish off writing our random forests and then after
    today, your homework will be to take this class and to add to it all of the random
    forest interpretation algorithms that we’ve learned. Obviously to be able to do
    that, you’re going to need to totally understand how this class works, so please
    ask lots of questions as necessary as we go along. To remind you, we are doing
    bulldozers Kaggle competition dataset again. We split it as before into 12,000
    validation (the last 12,000 records), and just to make it easier for us to keep
    track of what we are doing, we are going to just pick two columns out to start
    with: `YearMade` and `MachineHoursCurrentMeter`.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到完成随机森林的工作。今天我们要做的是完成编写我们的随机森林，然后在今天之后，你的作业就是拿这节课并添加我们学到的所有随机森林解释算法。显然，为了能够做到这一点，你需要完全理解这节课的工作原理，所以在我们进行时，请尽可能多地提问。提醒一下，我们再次使用推土机Kaggle竞赛数据集。我们将其分为12,000个验证集（最后12,000条记录），为了更容易跟踪我们的工作，我们将从中挑选两列开始：`YearMade`和`MachineHoursCurrentMeter`。
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: What we did last time was we started out by creating a tree ensemble and the
    tree ensemble had a bunch of trees which was literally a list of `n_trees` trees
    where each time we just called `create_tree`. `create_tree` contained a sample
    size (`sample_sz`) number of random indexes (`rnd_idxs`). This one is drawn without
    replacement. So remember, bootstrapping means sampling with replacement. Normally
    with scikit-learn, if you’ve got n rows, we grab n rows with replacement which
    means many of them will appear more than once. So each time we get a different
    sample but it’s always the same size as the original dataset. Then we have our
    `set_rf_samples` function we can use which does with replacement sampling of less
    than n rows. create_tree is doing something again which is it’s sampling without
    replacement `sample_sz` rows. Because we are permuting the numbers from naught
    to `self.y-1` and then grabbing the first `self.sample_sz` of them. Actually there
    is a faster way to do this. You can just use `np.random.choice` (instead of `np.random.permutation`)
    which is a slightly more direct way, but this way works as well. So `rnd_idxs`
    is our random sample for this one of our `n_trees` trees. So then we are going
    to create a `DecisionTree`. And our decision tree, we don’t pass it all of `x`,
    we pass it these specific indexes and remember x is a Pandas DataFrame so if we
    want to index into it with a bunch of integers, we use `iloc` (integer locations)
    which makes it behave indexing-wise just like numpy. Now `y` vector is numpy so
    we can just index into it directly. Then we are going to keep track about minimum
    leaf size (`min_leaf`).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上次我们做的是创建了一个树集合，这个树集合包含了一堆树，实际上是一个包含`n_trees`棵树的列表，每次我们只是调用`create_tree`。`create_tree`包含了一个样本大小（`sample_sz`）的随机索引（`rnd_idxs`）。这里是无重复抽样。所以记住，自助法意味着有放回抽样。通常在scikit-learn中，如果有n行数据，我们用有放回抽样抽取n行数据，这意味着很多行会出现多次。所以每次我们得到一个不同的样本，但它的大小总是与原始数据集相同。然后我们有一个`set_rf_samples`函数，我们可以使用它进行少于n行的有放回抽样。`create_tree`再次做的是无重复抽样`sample_sz`行。因为我们对从零到`self.y-1`的数字进行排列，然后抽取其中的前`self.sample_sz`个。实际上有一种更快的方法可以做到这一点。你可以直接使用`np.random.choice`（而不是`np.random.permutation`），这是一种稍微更直接的方法，但这种方法也可以。所以`rnd_idxs`是我们`n_trees`棵树中的一个的随机样本。然后我们将创建一个`DecisionTree`。我们的决策树，我们不会传递所有的`x`，而是传递这些特定的索引，记住x是一个Pandas
    DataFrame，所以如果我们想用一堆整数对其进行索引，我们使用`iloc`（整数位置），这使得它在索引方面的行为就像numpy一样。现在`y`向量是numpy，所以我们可以直接对其进行索引。然后我们将跟踪最小叶子大小（`min_leaf`）。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then the only other thing we really need in ensemble is somewhere to make a
    prediction. So we are just going to do the mean of the tree prediction for each
    tree. So that was that.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在集成中我们真正需要的另一件事情就是一个地方来进行预测。因此我们只需要对每棵树的预测取平均值。就是这样。
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Then in order to be able to run that, we need a decision tree class because
    it’s being called by `create_tree`. So there we go. That’s the starting point.
    The next thing we need to do is to flesh out our decision tree. So the important
    thing to remember is all of our randomness happened back here in the `TreeEnsemble`.
    The DecisionTree class, we are going to create doesn’t have randomness in it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后为了能够运行它，我们需要一个决策树类，因为它被`create_tree`调用。所以我们开始吧。这就是起点。接下来我们需要做的是完善我们的决策树。所要记住的重要一点是我们所有的随机性都发生在`TreeEnsemble`中。我们将要创建的DecisionTree类中没有随机性。
- en: '**Question**: Right now we are building a random tree regressor, so that’s
    why we are taking the mean of the tree outputs. If we were to work with classification,
    do we take the max? Like the classifier will give you either zeros or ones [[22:36](https://youtu.be/O5F9vR2CNYI?t=22m36s)]?
    No, I would still take the mean. So each tree is going to tell you what percentage
    of that leaf node contains cats and what percentage contains dogs. So then I would
    average all those percentages and say across the trees on average, there is 19%
    cats and 81% dogs.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：现在我们正在构建一个随机树回归器，这就是为什么我们要取树输出的平均值。如果我们要处理分类，我们要取最大值吗？就像分类器会给你零或一[[22:36](https://youtu.be/O5F9vR2CNYI?t=22m36s)]？不，我仍然会取平均值。因此，每棵树都会告诉你叶节点中包含猫的百分比和包含狗的百分比。然后我会平均所有这些百分比，并说在所有树上平均，有19%的猫和81%的狗。
- en: Random tree classifiers are almost identical or can be almost identical to the
    random tree regressors. The technique we are going to use to build this today
    will basically exactly work for a classification. Certainly for binary classification,
    you can do with exactly the same code. For multi-class classification, you just
    need to change your data structure so that you have like a one hot encoded matrix
    or a list of integers that you treat as a one hot encoded matrix.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 随机树分类器几乎与随机树回归器相同，或者几乎可以相同。我们今天要使用的技术基本上完全适用于分类。对于二元分类，您可以使用完全相同的代码。对于多类分类，您只需要更改数据结构，使其像一个独热编码矩阵或一个整数列表，您将其视为一个独热编码矩阵。
- en: So our decision tree, remember, our idea here is that we are going to try to
    avoid thinking so we are going to basically write it as if everything we need
    already exists [[23:54](https://youtu.be/O5F9vR2CNYI?t=23m54s)]. We know from
    when we created the decision tree we will pass in the x, the y, and the minimum
    leaf size. So here we need to make sure we’ve got the `x`, `y`, and `min_leaf`
    in `__init__`. There’s one other thing which is as we split our tree into sub
    trees, we are going to need to keep track of the row indexes went into the left-hand
    side of the tree, which went into the right-hand side of the tree. So we are going
    to have this thing called `idxs` as well. At first, we didn’t bother passing in
    `idxs` at all so if idxs is not passed in (i.e. `if idxs is None`), then we are
    just going to set it to entire length of y. So `np.arange` is the same as just
    `range` in Python but it returns a numpy array. So the root of a decision tree
    contains all the rows. That’s the definition really of the root of a decision
    tree (row 0, row 1, … up to row y-1). We are going to store away all the information
    that we were given. We are going to keep track of how many rows there are, and
    how many columns there are.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们的决策树，记住，我们的想法是我们要尽量避免思考，所以我们基本上会写成如果我们需要的一切已经存在的样子。我们知道当我们创建决策树时，我们将传入x、y和最小叶子大小。所以在这里我们需要确保在`__init__`中有`x`、`y`和`min_leaf`。还有一件事是，当我们将树分割成子树时，我们需要跟踪哪些行索引进入了树的左侧，哪些进入了树的右侧。所以我们还会有一个叫做`idxs`的东西。起初，我们根本不费心传入`idxs`，所以如果没有传入`idxs`（即`if
    idxs is None`），那么我们就会将其设置为y的整个长度。`np.arange`在Python中与`range`相同，但它返回一个numpy数组。所以决策树的根包含所有行。这实际上就是决策树根的定义（第0行，第1行，直到第y-1行）。我们将存储我们得到的所有信息。我们将跟踪有多少行，有多少列。
- en: Then every leaf and every node in a tree has a value/prediction [[25:28](https://youtu.be/O5F9vR2CNYI?t=25m28s)].
    That prediction is just equal to the average of the dependent variable. So every
    node in the tree, `y` indexed with `idxs` is the value of the dependent variable
    that are in this branch of the tree and so here is the mean. Some nodes in a tree
    also have a score which is like how effective was the split here. But that’s only
    going to be true if it’s not a leaf node. A leaf node has no further splits. And
    at this point when we create a tree, we haven’t done any splits yet so its score
    starts out as being infinity. Having built that the root of the tree, our next
    job is to find out which variable should we split on and what level of that variable
    should we split on. So let’s pretend that there is something that does that —
    `find_varsplit` . Then we are done.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后树中的每个叶子和每个节点都有一个值/预测。该预测只是等于因变量的平均值。因此，树中的每个节点，用`idxs`索引的`y`是在树的这一分支中的因变量的值，因此这里是平均值。树中的一些节点还有一个分数，这就像这里的分割有多有效。但只有在它不是叶子节点时才会成立。叶子节点没有进一步的分割。在创建树时，我们还没有进行任何分割，因此其分数开始时为无穷大。构建了树的根节点后，我们的下一个任务是找出应该在哪个变量上进行分割，以及应该在该变量的哪个水平上进行分割。因此，让我们假设有一个可以做到这一点的东西——`find_varsplit`。然后我们就完成了。
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So how do we find a variable to split on? Well, we could just go through each
    potential variable, so `c` contains the number of columns we have so go through
    each one and see if we can find a better split than we have so far on that column.
    Now notice this is not the full random forest definition. This is assuming that
    max features are set to all. Remember, we could set max features to 0.5 in which
    case we wouldn’t check all the numbers from zero to `c`, we would check half the
    numbers at random from zero to `c`. So if you want to turn this into a random
    forest that has the max features support, you could easily add one line o code
    to do that. But we are not going to do it in our implementation today. So then
    we just need to find better split and since we are not interested in thinking
    at the moment, for now we are just going to leave that empty.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何找到一个变量来分割呢？嗯，我们可以逐个检查每个潜在的变量，所以`c`包含我们拥有的列数，逐个检查并查看是否能在该列上找到比目前更好的分割。现在请注意，这并不是完整的随机森林定义。这是假设最大特征被设置为全部的情况。请记住，我们可以将最大特征设置为0.5，这样我们就不会检查从零到`c`的所有数字，而是会随机检查从零到`c`的一半数字。因此，如果您想将其转换为支持最大特征的随机森林，您可以轻松添加一行代码来实现。但是在我们今天的实现中，我们不打算这样做。因此，我们只需要找到更好的分割点，由于我们目前不感兴趣，所以现在我们将其留空。
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The one other thing I like to do when start writing a class is I’d like to
    have some way to print out what’s in that class [[27:52](https://youtu.be/O5F9vR2CNYI?t=27m52s)].
    If you type print followed by an object or if Jupyter Notebook, you just type
    the name of the object. At the moment, it’s just printing out `<__main__.DecisionTree
    at 0x7f645ec22358>` which is not very helpful. So if we want to replace this with
    something helpful, we have to define the special Python method named `__repr__`
    to get a representation of this object. So when we basically just write the name
    in Jupyter Notebook cell, behind the scene, it calls that function and the default
    implementation of that method is just to print out that unhelpful stuff. So we
    can replace it by instead saying let’s create a format string where we are going
    to print out `f''n: **{self.n}**; val:**{self.val}''**` so how many rows are in
    this node and what’s the average of the dependent variable. Then if it’s not a
    leaf node, so if it has a split, then we should also be able to print out the
    score, the value we split out, and the variable we split on. Now you’ll notice
    here, `self.is_leaf` is defined as a method but I don’t have any parentheses after
    it. This is a special kind of method called a property. A property is something
    that looks like a regular variable but it’s actually calculated on the fly. So
    when I call `is_leaf`, it actually calls `**def** is_leaf(self)` function. But
    I’ve got this special decorator `@property`. What this says is basically you don’t
    have to include the parentheses when you call it. So it is going to say is this
    a leaf or not. So a leaf is something that we don’t split on. If we haven’t split
    on it, then its score is still set to infinity, so that’s my logic.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '在开始编写一个类时，我喜欢做的另一件事是，我想要有一种方法来打印出该类中的内容。如果你输入print，后面跟着一个对象，或者在Jupyter Notebook中，你只需输入对象的名称。目前，它只是打印出`<__main__.DecisionTree
    at 0x7f645ec22358>`，这并不是很有帮助。所以如果我们想要用有用的东西来替换它，我们必须定义一个特殊的Python方法，名为`__repr__`，以获得这个对象的表示。所以当我们在Jupyter
    Notebook单元格中基本上只写出名称时，在幕后，它调用那个函数，而该方法的默认实现只是打印出那些无用的东西。所以我们可以替换它，而不是说让我们创建一个格式化字符串，在这里我们将打印出`f''n:
    **{self.n}**; val:**{self.val}''**`，所以这个节点中有多少行，以及因变量的平均值是多少。然后，如果它不是叶节点，也就是说如果它有一个分裂，那么我们还应该能够打印出分数，我们分裂出的值，以及我们分裂的变量。现在你会注意到这里，`self.is_leaf`被定义为一个方法，但我后面没有加括号。这是一种特殊类型的方法，称为属性。属性看起来像一个普通的变量，但实际上是动态计算的。所以当我调用`is_leaf`时，实际上调用的是`**def**
    is_leaf(self)`函数。但我有这个特殊的装饰器`@property`。这意味着当你调用它时，你不必包括括号。所以它会说这是一个叶子还是不是。所以叶子是我们不分裂的东西。如果我们没有对它进行分裂，那么它的分数仍然设置为无穷大，这就是我的逻辑。'
- en: This `@` notation is called a decorator. It’s basically a way of telling Python
    more information about your method. Anybody who’s done any web programming before
    with something like Flask or a similar framework would have had to have said this
    method is going to respond to this bit of the URL and either to POST or to GET
    and you put it in a special decorator. Behind the scene, that’s telling Python
    to treat this method in a special way. So `@property` is another decorator. If
    you get more advanced with Python, you can actually learn how to write your own
    decorators which as was mentioned basically insert some additional code but for
    now just know there’s a bunch of predefined decorators we can use to change how
    our method behave and one of them is `@property` which basically means you don’t
    have to put parentheses anymore which of course means you can’t add any more parameters
    beyond `self`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`@`符号被称为装饰器。基本上是告诉Python关于你的方法的更多信息的一种方式。任何之前使用过像Flask或类似框架进行过web编程的人都必须声明这个方法将响应URL的这一部分，要么是POST，要么是GET，并将其放在一个特殊的装饰器中。在幕后，这告诉Python以一种特殊的方式处理这个方法。所以`@property`是另一个装饰器。如果你在Python中变得更加高级，你实际上可以学习如何编写自己的装饰器，就像之前提到的那样，基本上插入一些额外的代码，但现在只需要知道有一堆预定义的装饰器可以用来改变我们的方法的行为，其中之一就是`@property`，这基本上意味着你不再需要加括号，当然这意味着你不能再添加任何参数了，只能是`self`。
- en: '**Question**: Why is it leaf if score is infinity? Doesn’t infinity mean you
    are at the root [[31:42](https://youtu.be/O5F9vR2CNYI?t=31m42s)]? No, infinity
    means that you are not at the root. It means you are at a leaf. So the root will
    have a split assuming we find one. Everything will have a split till we get all
    the way to the bottom (i.e. the leaf) so the leaves will have a score of infinity
    because they won’t split.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：如果分数是无穷大，为什么它是叶子？无穷大不是意味着你在根节点吗？不，无穷大意味着你不在根节点。它意味着你在叶子节点。所以根节点将会有一个分裂，假设我们找到一个。一切都会分裂，直到我们到达底部（即叶子节点），所以叶子节点的分数将是无穷大，因为它们不会分裂。
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So that’s our decision tree [[32:10](https://youtu.be/O5F9vR2CNYI?t=32m10s)].
    It doesn’t do very much but at least we can create an ensemble. 10 trees, sample
    size of 1,000 and we can print out. Now when we go `m.trees[0]` it doesn’t say
    `<__main__.DecisionTree at 0x7f645ec22358>` but it says what we asked it to say.
    This is the leaf, because we haven’t split on it yet, so we’ve got nothing more
    to say.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的决策树。它并没有做太多事情，但至少我们可以创建一个集成。10棵树，样本量为1,000，我们可以打印出来。现在当我们输入`m.trees[0]`时，它不会显示`<__main__.DecisionTree
    at 0x7f645ec22358>`，而是显示我们要求它显示的内容。这是叶子节点，因为我们还没有在其上进行分割，所以我们没有更多要说的。
- en: Then the indexes are, all the numbers from nought to a thousand because the
    base of the tree has everything. This is everything in the random sample that
    was passed to it because by the time we get to the point where it’s a decision
    tree where we don’t have to worry about any of the randomness in the random forest
    anymore.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后索引是，所有从零到一千的数字，因为树的基础包含了一切。这是随机样本中的所有内容，因为当我们到达决策树的时候，我们不再需要担心随机森林中的任何随机性。
- en: Find best split given variable [[33:09](https://youtu.be/O5F9vR2CNYI?t=33m9s)]
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 给定变量找到最佳分割点
- en: Let’s try to write the thing which finds a split. So we need to implement `find_better_split`.
    It’s going to take the index of a variable and it’s going to figure out what’s
    the best split point, determine if it is better than any split we have so far,
    and for the first variable the answer will always be yes because the best one
    so far is none at all which is infinity bad.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试编写找到分割点的函数。因此，我们需要实现`find_better_split`。它将接受一个变量的索引，并找出最佳的分割点，确定它是否比我们目前为止的任何分割更好，对于第一个变量，答案总是肯定的，因为到目前为止最好的分割点是没有分割，这是无穷糟糕的。
- en: So let’s start by making sure we’ve got something to compare to. So the thing
    we are going to compare to will be scikit-learn’s random forest. We need to make
    sure that scikit-learn’s random forest gets exactly the same data that we have,
    so we start out by creating ensemble, grab a tree out of it, and then find out
    which particular random sample of `x` and `y` did this tree use and we are going
    to store them away so that we can pass them to scikit-learn (so we have exactly
    the same information).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们首先确保我们有东西可以进行比较。我们要进行比较的是scikit-learn的随机森林。我们需要确保scikit-learn的随机森林获得与我们完全相同的数据，因此我们首先创建集成，从中提取一棵树，然后找出这棵树使用了哪个特定的随机样本`x`和`y`，然后将它们存储起来，以便我们可以将它们传递给scikit-learn（这样我们就有完全相同的信息）。
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: So let’s go ahead and now create a random forest using scikit-learn. One tree
    (`n_estimators`), one decision (`max_depth`), no bootstrapping so the whole dataset.
    So this should be exactly the same as the thing that we are going to create. So
    let’s try.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/d6d63100dd7da503f0e85e87e84a88a7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: We need to `define find_better_split` and it takes a variable. Let’s define
    our `x` (i.e. independent variables) and say okay well it’s everything inside
    our tree but only those indexes that are in this node which at the top of the
    tree is everything, and just this one variable (`var_idx`). Then for our `y`,
    it’s just whatever our dependent variable is at the indexes in this node. So there
    are our `x` and `y`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要定义`find_better_split`函数，并且它需要一个变量。让我们定义我们的`x`（即自变量），说好，它是树中的所有内容，但只有在这个节点中的那些索引，而在树的顶部是所有内容，只有这一个变量（`var_idx`）。然后对于我们的`y`，它就是在这个节点中的索引处的因变量。所以这就是我们的`x`和`y`。
- en: 'So let’s now go through every single value in our independent variable. And
    I’ll show you what’s going to happen [[35:22](https://youtu.be/O5F9vR2CNYI?t=35m22s)].
    Let’s say our independent variable is YearMade and it’s not going to be in an
    order. So we are going to go to the very first row and we are going to say okay,
    YearMade here is 3\. So what I’m going to do is I’m going to try and calculate
    the score if we decided to branch on the number 3\. So I need to know which rows
    are greater than 3, which rows are less than or equal to 3 and they are going
    to become my left-hand side and my right hand side. Then we need a score. There’s
    lots of scores we could use so in random forests, we call this the information
    gain. The information gain is like how much better does our score get because
    we split it into these two groups of data. There’s lots of ways we could calculate
    it: Gini, cross-entropy, root mean squared error, etc. If you think about it,
    there is an alternative formulation of root mean squared error which is mathematically
    the same to within a constraint scale but a little bit easier to deal with which
    is we are going to try and find a split which causes the two groups to each have
    as lower standard deviation as possible. So I want to find a split that puts all
    the cats over here and all the dogs over there. So if these are all cats and those
    are all dogs, then this has a standard deviation of zero and that has a standard
    deviation of zero. Or else this is a totally random mix of cats and dogs, and
    that is a totally random mix of cats and dogs, they are going to have a much higher
    standard deviation. Does that make sense? So it turns out if you find a split
    that minimizes those group standard deviations or specifically the weighted average
    of the two standard deviations, it’s mathematically the same as minimizing the
    root mean squared error. That’s something you can prove to yourself after class
    if you want to.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在逐个检查我们独立变量中的每个值。我会告诉你接下来会发生什么。假设我们的独立变量是YearMade，它不会按顺序排列。所以我们要去到第一行，然后说好，这里的YearMade是3。那么我要做的是尝试计算如果我们决定以数字3为分支时的得分。我需要知道哪些行大于3，哪些行小于或等于3，它们将成为我的左侧和右侧。然后我们需要一个得分。我们可以使用很多得分，所以在随机森林中，我们称之为信息增益。信息增益就像我们的得分因为我们将数据分成这两组而变得更好了多少。我们可以用很多方法来计算它：基尼系数、交叉熵、均方根误差等等。如果你考虑一下，有一个均方根误差的替代公式，数学上与一个约束尺度内相同，但稍微容易处理一些，那就是我们要找到一个分割点，使得这两组数据的标准差尽可能低。所以我想找到一个分割点，把所有的猫放在这边，所有的狗放在那边。所以如果这些都是猫，那些都是狗，那么这边的标准差为零，那边的标准差也为零。否则，这是一群完全随机混合的猫和狗，那是一群完全随机混合的猫和狗，它们的标准差会高得多。明白了吗？事实证明，如果找到一个最小化这两组标准差或者具体来说是两个标准差的加权平均的分割点，数学上与最小化均方根误差是相同的。如果你想的话，课后你可以自己证明这一点。
- en: '![](../Images/afb018634866d7dde2b692c79f5777b2.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afb018634866d7dde2b692c79f5777b2.png)'
- en: So we are going to need to find, first of all, split this into two groups [[37:29](https://youtu.be/O5F9vR2CNYI?t=37m29s)].
    So where’s all the stuff that is greater than three? 4, 6, and 4\. So we need
    standard deviation of their price.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要找到，将其分成两组[[37:29](https://youtu.be/O5F9vR2CNYI?t=37m29s)]。那么所有大于三的东西在哪里？4、6和4。所以我们需要它们价格的标准差。
- en: '![](../Images/e2d72fdac4a0045fe378f58a5bd38fb5.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2d72fdac4a0045fe378f58a5bd38fb5.png)'
- en: The next would be the standard deviation of less than or equal to 3, and we
    just take the weighted average of those two and that’s our score. That would be
    our score if we split on 3.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是标准差小于或等于3，我们只需取这两者的加权平均值，这就是我们的得分。如果我们在3上分割，那就是我们的得分。
- en: '![](../Images/343d3cee51889b034d69db56504522c9.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/343d3cee51889b034d69db56504522c9.png)'
- en: 'Then the next step would be try to split on 4, try splitting on 1, try splitting
    on 6, redundantly try splitting on 4 again, then 1 again and find out which works
    the bet. So that’s our code here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后下一步是尝试在4上分割，尝试在1上分割，尝试在6上分割，多余地再次尝试在4上分割，然后再次在1上分割，找出哪个效果最好。这就是我们的代码：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We are going to go through every row and let’s say okay left hand side is any
    values in `x` that are less than or equal to this particular value. Our right
    hand side is every value in x that are greater than this particular value.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐行进行，假设左侧是`x`中小于或等于特定值的任何值。右侧是`x`中大于此特定值的每个值。
- en: So what’s the data type that’s going to be in `lhs` and `rhs`? What are they
    actually going to contain? They are going to be arrays of booleans which we can
    treat as zero and one. So `lhs` will be an array of false every time it’s not
    less than or equal to; and true otherwise, and `rhs` will be a boolean array of
    the opposite. Now we can’t take a standard deviation of an empty set, so if there’s
    nothing that’s greater than this number (`x[i]`) then `rhs` will be all false
    which means the sum will be zero. In that case, let’s not go any further with
    this step because there’s nothing to take the standard deviation of and it’s obviously
    not a useful split.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`lhs` 和 `rhs` 中将包含什么数据类型？它们实际上会包含什么？它们将是布尔数组，我们可以将其视为零和一。因此，`lhs` 将是一个数组，每次它不小于或等于时为
    false；否则为 true，而 `rhs` 将是相反的布尔数组。现在我们不能对空集合取标准差，所以如果没有任何大于这个数字 (`x[i]`) 的值，那么
    `rhs` 将全部为 false，这意味着总和为零。在这种情况下，让我们不再继续这一步，因为没有什么可以取标准差，显然这不是一个有用的分割。'
- en: 'So assuming we’ve got this far, we can now calculate the standard deviation
    of the left hand side and of the right hand side and take the weighted average
    or the sums are the same thing to a scaler and so there is our score. So we can
    then check is this better than our best score so far, and our best score so far,
    we initially initialized it to infinity, so initially, this is better. So if it’s
    better, let’s store away all of the information we need: which variable has found
    this better split, what was the score we found, and what was the value that we
    split on. So there it is. If we run that and I’m using `%timeit` so what it does
    is it sees how long this command takes to run and it tries to give you a statistically
    valid measure of that so you can see here, it has run it 10 times to get an average
    and then it’s done that 7 times to get a mean and standard deviation across runs,
    and so it’s taking me 76 milliseconds plus or minus 11.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经走到这一步，现在我们可以计算左侧和右侧的标准差，然后取加权平均值或求和，这两者对于一个标量来说是相同的，因此这就是我们的得分。然后我们可以检查这个得分是否比迄今为止的最佳得分更好，我们迄今为止的最佳得分，最初将其初始化为无穷大，因此最初这是更好的。如果更好，让我们存储所有我们需要的信息：哪个变量找到了这个更好的分割，我们找到的得分是多少，以及我们分割的值是多少。就是这样。如果我们运行这个，并且我正在使用`％timeit`，它会看这个命令运行需要多长时间，并试图给出一个统计上有效的度量，这样你就可以看到，它已经运行了10次以获得平均值，然后又运行了7次以获得运行间的平均值和标准差，所以它花了我76毫秒加减11。
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So let’s check that this works [[41:05](https://youtu.be/O5F9vR2CNYI?t=41m5s)].
    `find_better_split(tree, 0)` and 0 is `YearMade`, 1 is `MachineHoursCurrentMeter`
    so with 1, we got back `MachineHoursCurrentMeter` with `score:681.0184057251435`
    and then we ran it again with zero and we got a better score (658) and split 1974.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们来检查这是否有效。`find_better_split(tree, 0)`，0代表`YearMade`，1代表`MachineHoursCurrentMeter`，所以当我们用1时，我们得到了`MachineHoursCurrentMeter`，得分为681.0184057251435，然后我们再次用零运行，得到了更好的分数（658），并分割了1974。
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So 1974, let’s compare with scikit-learn’s random forest above, and yes, that
    was what this tree did as well. So we’ve confirmed that this method is giving
    the same result that sklearn’s random forest did. You can also see here the value
    10.08 is matching the value in sklearn’s root node’s value. So we’ve got something
    that can handle one split.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6d63100dd7da503f0e85e87e84a88a7.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: '**Question**: Why don’t we put a unique on the x [[42:02](https://youtu.be/O5F9vR2CNYI?t=42m2s)]?
    Because I am not trying to optimize the performance yet. You can see in the Excel
    I checked this `1` twice, `4` twice unnecessarily.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：为什么我们不在x上放一个unique？因为我还没有尝试优化性能。你可以在Excel中看到我检查了这个`1`两次，`4`两次，这是不必要的。'
- en: Okay, so Yannet is already thinking about performance which is good. So tell
    me what is the computational complexity of this section of the code?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，Yannet已经在考虑性能，这是好事。那告诉我这段代码的计算复杂度是多少？
- en: '![](../Images/553d4fdb21654aad9661ce13548e75cc.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: ！[](../Images/553d4fdb21654aad9661ce13548e75cc.png)
- en: O(n²) because there is for loop and `x<=x[i]` which we have to check every value
    to see if it’s less than `x[i]`. It’s useful to know how to quickly calculate
    computational complexity. I can guarantee most of the interviews you do are going
    to ask you to calculate computational complexity on the fly. And also when you
    are coding, you want it to be second nature. The technique is basically “is there
    a loop?” if so we are obviously doing this `n` times so there is an `n` involved.
    Is there a loop inside the loop? If there is, then you need to multiply those
    two together. In this case, there is not. Is there anything inside the loop that’s
    not a constant time operation? So you might see a sort in there and you just need
    to know that sort is `nlog(n)` — that should be second nature. If you see a matrix
    multiply, you need to know what that is. In this case, there are some things that
    are doing element-wise array operations so keep an eye out for anything where
    numpy is doing something to every value of an array. In this case is checking
    every value of `x` against a constant so it’s going to have to do that `n` times.
    So to flesh this out into a computational complexity, you just take the number
    of things in the loop and you multiply it by the highest computational complexity
    inside the loop, `n` times `n` is `n²`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: O(n²) 是因为有一个循环和 `x<=x[i]`，我们必须检查每个值，看它是否小于 `x[i]`。了解如何快速计算计算复杂度是很有用的。我可以保证你做的大多数面试都会要求你即兴计算计算复杂度。而且当你编码时，你希望它成为第二天性。这种技巧基本上是“有循环吗？”如果有，那么显然我们要做这个
    `n` 次，所以涉及到一个 `n`。循环里面还有循环吗？如果有，那么你需要将它们两个相乘。在这种情况下，没有。循环里面有任何不是常数时间操作的东西吗？所以你可能会看到一个排序在里面，你只需要知道排序是
    `nlog(n)` —— 这应该是第二天性的。如果你看到一个矩阵相乘，你需要知道那是什么。在这种情况下，有一些东西在进行逐元素数组操作，所以要留意任何地方，numpy
    在对数组的每个值做一些操作。在这种情况下，它正在检查每个 `x` 的值是否小于一个常数，所以它将不得不这样做 `n` 次。所以要将这个扩展成一个计算复杂度，你只需要将循环中的事物数量乘以循环内部的最高计算复杂度，`n`
    次 `n` 是 `n²`。
- en: '**Question**: In this case, couldn’t we just presort the list and then do one
    `nlog(n)` computation [[45:10](https://youtu.be/O5F9vR2CNYI?t=45m10s)]? There’s
    lots of things we can do to speed this up, so at this stage is just what is the
    computational complexity we have. But absolutely. It’s certainly not as good as
    it can be. So that’s where we’re going to go next. Just like alright, n² is not
    great so let’s try and make it better.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：在这种情况下，我们不能只是预先对列表进行排序，然后进行一次`nlog(n)`的计算吗？有很多事情可以做来加快速度，所以在这个阶段我们只关心的是计算复杂度。但绝对可以。当然，它肯定不是最好的。所以接下来我们要做的就是这个。就像好吧，n²不太好，所以让我们试着让它变得更好。'
- en: So here is my attempt at making it better. First of all, what is the equation
    for standard deviation?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是我尝试改进的地方。首先，标准差的方程式是什么？
- en: '![](../Images/6222fdd9476b78e5280763151bb4c171.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6222fdd9476b78e5280763151bb4c171.png)'
- en: In practice, we don’t normally use that formulation because it requires us calculating
    `x` minus the mean lots of times. Does anybody know the formulation that just
    requires x and x²?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们通常不使用那个公式，因为它要求我们多次计算`x`减去平均值。有谁知道只需要x和x²的公式吗？
- en: '![](../Images/8ee6282a2424a05a8776b3439e7d4cc7.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ee6282a2424a05a8776b3439e7d4cc7.png)'
- en: '[http://www.wolframalpha.com/input/?i=standard+deviation](http://www.wolframalpha.com/input/?i=standard+deviation)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: That’s a really good one to know because you can now calculate variances or
    standard deviation of anything. You just have to first of all grab the column
    as it is. Column squared. As long as you’ve got those stored away somewhere, you
    can immediately calculate the standard deviation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'So the reason this is handy for us is that if we first of all sort our data
    [[47:13](https://youtu.be/O5F9vR2CNYI?t=47m13s)]. Then if you think about it,
    as we start going down one step at a time, then each group is exactly the same
    as the previous group on the left hand side with one more thing in it and on the
    right hand side with one less thing in it. So given that we just have to keep
    track of sum of x and sum of x², we can just add one more thing to x, one more
    thing to x² on the left and remove one thing on the right. So we don’t have to
    go through the whole lot each time and so we can turn this into O(n) algorithm.
    So that’s all I do here:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这对我们有用的原因是，如果我们首先对我们的数据进行排序。然后如果你考虑一下，当我们一步一步地向下走时，每一组都与左边的前一组完全相同，只是多了一件东西，右边则少了一件东西。因此，我们只需要跟踪x的总和和x²的总和，我们只需在左边添加一个东西，x²再添加一个东西，在右边移除一个东西。因此，我们不必每次都遍历整个数据集，因此我们可以将其转化为O(n)算法。这就是我在这里所做的一切：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'I sort the data, and then I’ll keep track of the count of things on the right
    (`rhs_cnt`), the sum of things on the right (`rhs_sum`), and sum of squares on
    the right (`rhs_sum2`). Initially everything is on the right hand side. So initially
    `n` is the count, `y.sum()` is the sum on the right, and y² (`y**2`) sum is the
    sum of squares on the right. Then nothing is initially on the left, so it’s zeros.
    Then we just have to loop through each observation:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我对数据进行排序，然后我会跟踪右侧的事物数量（`rhs_cnt`），右侧事物的总和（`rhs_sum`）和右侧的平方和（`rhs_sum2`）。最初所有事物都在右侧。因此最初`n`是计数，`y.sum()`是右侧的总和，y²（`y**2`）的总和是右侧的平方和。然后最初左侧没有任何事物，因此为零。然后我们只需要循环遍历每个观察值：
- en: Add one to the left hand count, subtract one from the right hand count.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左手计数加一，右手计数减一。
- en: Add the value to the left hand sum, subtract it from the right hand sum.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将值加到左手总和，从右手总和减去。
- en: Add the value squared to the left hand, subtract it from the right hand.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将值的平方加到左手，从右手减去。
- en: Now we do need to be careful though because if we are saying less than or equal
    to one, for example, we are not stopping at the first row but we have to have
    everything in that group.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要小心，因为如果我们说小于或等于一，例如，我们不会停在第一行，而是必须将该组中的所有内容都包括在内。
- en: '![](../Images/82d2acc02952ea7024c5adcd890d91ac.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82d2acc02952ea7024c5adcd890d91ac.png)'
- en: 'So the other thing I’m going to do is I’m going to make sure that the next
    value is not the same as this value. If it is, I’m going to skip over it. So I’m
    just going to double check that this value and the next one aren’t the same (`**if**
    xi==sort_x[i+1]:`). As long as they are not the same, I can keep going ahead and
    calculate my standard deviation now by passing in the count, the sum, and the
    sum squared. And there is that formula:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ce0dc6b4f1fb1fc2569bd82d393ab1b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: Do that for the right hand side and so now we can calculate the weighted average
    score just like before and all the lines below are the same.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以对右侧进行同样的操作，这样我们就可以像之前一样计算加权平均分数，下面的所有行都是一样的。
- en: So we’ve turned O(n²) algorithm to O(n) algorithm. In general, stuff like this
    is going to get you a lot more value than pushing something onto a Spark cluster
    or ordering faster RAM or using more cores in your CPU, etc. This is the way you
    want to be improving your code. Specifically, write your code without thinking
    too much about performance. Run it and see if it is fast enough for what you need.
    If so, then you’re done. If not, profile it. In Jupyter, you can say `%prun` and
    it will tell you exactly where the time was spent in your algorithm. Then you
    can go to that bit that’s actually taking the time and think about if it is algorithmically
    as efficient as it can be. In this case, we run it and we’ve gone down from 76
    milliseconds to less than 2 milliseconds. Now some people that are new to programming
    might think “oh great, I’ve saved 60 something milliseconds” but the point is
    this is going to get run tens of millions of times. So the 76 millisecond version
    is so slow that it’s got to be impractical for any random forest you use in practice.
    Where else, the one millisecond version I found is actually quite acceptable.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们把O(n²)的算法转换成了O(n)的算法。一般来说，像这样的东西会给你带来比将某些东西推送到Spark集群或者更快的RAM或者在CPU中使用更多核心等更多价值。这是你想要改进你的代码的方式。具体来说，编写代码时不要过多考虑性能。运行它，看看它是否对你的需求足够快。如果是，那么你就完成了。如果不是，进行性能分析。在Jupyter中，你可以使用`%prun`，它会告诉你算法中时间花在哪里。然后你可以去看看实际花费时间的部分，思考它在算法上是否尽可能高效。在这种情况下，我们运行它，从76毫秒降到不到2毫秒。现在一些新手可能会认为“哦，太好了，我节省了60多毫秒”，但关键是这将被运行数千万次。所以76毫秒版本太慢了，对于实际使用的任何随机森林来说都是不切实际的。而另一方面，我找到的1毫秒版本实际上是相当可接受的。
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: And then check, the numbers should be exactly same as before and they are.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后检查，数字应该与之前完全相同，而且确实如此。
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So now that we have a function, `find_better_split`, that does what we want,
    I want to insert it into my `DecisionTree` class [[51:34](https://youtu.be/O5F9vR2CNYI?t=51m34s)].
    And this is a really cool Python trick. Python does everything dynamically, so
    we can actually say the method called `find_better_split` in `DecisionTree` is
    that function I just created.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个函数`find_better_split`，它可以做我们想要的事情，我想把它插入到我的`DecisionTree`类中。这是一个非常酷的Python技巧。Python可以动态执行所有操作，因此我们实际上可以说`DecisionTree`中名为`find_better_split`的方法就是我刚刚创建的那个函数。
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'That sticks it inside that class. Now I’ll tell you what’s slightly confusing
    about this is that `find_better_split` on the left and `find_better_split` actually
    have no relationship to each other. They just happen to have the same letters
    in the same order. So I could call this `find_better_split_foo` and then I could
    call that. Now my function is actually called `find_better_split_foo`, but a method
    I’m expecting to call is something called DecisionTree.find_better_split. So here,
    I could say:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 将其放在该类中。现在我告诉你这件事稍微令人困惑的地方是，左边的`find_better_split`和右边的`find_better_split`实际上并没有任何关系。它们只是恰好以相同的顺序拥有相同的字母。所以我可以将其命名为`find_better_split_foo`，然后我可以调用它。现在我的函数实际上被称为`find_better_split_foo`，但我期望调用的方法是名为DecisionTree.find_better_split的东西。所以在这里，我可以说：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It’s important to understand how namespaces work in every language that you
    use. One of the most important thing is understanding how it fitures out what
    a name refers to. So this here (`DecisionTree.find_varsplit`) means `find_better_split`
    as defined inside `DecisionTree` class and nowhere else. This one on the right
    means `find_better_split_foo` in the global namespace. A lot of languages do not
    have a global namespace but Python does. So even if they happen to have the same
    letters in the same order, they are not referring in any way to the same thing.
    It’s like this family over here may have somebody called Jeremy, and my family
    has somebody called Jeremy. Our names happen to be the same but we are not the
    same person.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用的每种语言中，了解命名空间的工作原理是很重要的。其中最重要的一点是了解它是如何确定名称所指的内容的。所以这里（`DecisionTree.find_varsplit`）意味着`DecisionTree`类内部定义的`find_better_split`，而不是其他地方。右边的这个意味着全局命名空间中的`find_better_split_foo`。许多语言没有全局命名空间，但Python有。因此，即使它们恰好以相同的顺序拥有相同的字母，它们也不以任何方式指向相同的内容。就像这边的家庭可能有一个叫Jeremy的人，而我的家庭也有一个叫Jeremy的人。我们的名字恰好相同，但我们并不是同一个人。
- en: Now that we’ve stuck the `find_better_split` method inside the `DecisionTree`
    with this new definition, when I now call the `TreeEnsemble` constructor the decision
    tree ensemble constructor, it calls `create_tree`, and `create_tree` instantiates
    `DecisionTree`, `DecitionTree` calls `find_varsplit`, which goes through every
    column to see if it could find a better split and we’ve now defined `find_better_split`,
    and therefore `TreeEnsemble` when we create it has gone ahead and done this split
    [[53:57](https://youtu.be/O5F9vR2CNYI?t=53m57s)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将`find_better_split`方法放入了具有这个新定义的`DecisionTree`中，当我现在调用`TreeEnsemble`构造函数时，决策树集合构造函数会调用`create_tree`，`create_tree`实例化`DecisionTree`，`DecisionTree`调用`find_varsplit`，它会遍历每一列以查看是否可以找到更好的分割点，我们现在已经定义了`find_better_split`，因此当我们创建`TreeEnsemble`时，它已经执行了这个分割点。
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Alright. So this is pretty neat, right? We kind of just do a little bit at a
    time, testing everything as we go. As you all implement the random forest interpretation
    techniques, you may want to try programming this way to check every step what
    you are doing matches up with what scikit-learn does or with a test that you’ve
    build.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。这很不错，对吧？我们一次只做一点点，测试每一步。当你们实现随机森林解释技术时，你们可能想尝试以这种方式编程，检查每一步是否与scikit-learn所做的匹配，或者与你们构建的测试匹配。
- en: Full single tree [[55:13](https://youtu.be/O5F9vR2CNYI?t=55m13s)]
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整的单棵树[[55:13](https://youtu.be/O5F9vR2CNYI?t=55m13s)]
- en: At this point, we should try to go deeper for inception. Let’s go now max_depth
    is 2\. So here is what scikit-learn did. After breaking at YearMade 74, it then
    broke at MachineHoursCurrentMeter 2956.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/61e3d18abf413cbeeb57bf7c12c5a593.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: So we had this thing called `find_varsplit` which just went through every column
    and try to see if there is a better split there. But actually, we need to go a
    bit further than that. Not only do we have to go through every column and see
    if there is a better split in this node, but then we also have to see whether
    there is a better split in the left and the right sides that we just created.
    In other words, the left hand side and the right hand side should become decision
    tree themselves.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有一个叫做`find_varsplit`的东西，它只是遍历每一列，尝试看看是否有更好的分割点。但实际上，我们需要再进一步。我们不仅需要遍历每一列，看看这个节点是否有更好的分割点，而且还需要看看我们刚刚创建的左侧和右侧是否有更好的分割点。换句话说，左侧和右侧应该成为决策树本身。
- en: '![](../Images/5737b670435fb406e623fbae9bef82f4.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: There is no difference at all between what we do on the right to create this
    tree and what we do on the left to create this tree other than the one on the
    left contains 159 samples and the left one contains a thousand.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce5e1cf96abe399751ad31ac590d4dfa.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: So the first line of code is exactly the same as we had before. Then we check
    if it is a leaf node. If it is a leaf node, then we have nothing further to do.
    So that means we are right at the bottom, there is no split that’s been made,
    so we don’t have to do anything further. On the other hand, if it’s not a leaf
    node, then we need to split it into the left hand side and the right hand side.
    Now, earlier on, we created a left hand side and the right hand side array of
    booleans. Better would be to have an array of indexes and that’s because we don’t
    want to have a full array of all the booleans in every single node. Because remember,
    although it doesn’t look like there are many nodes when you see a tree of this
    size, when it’s fully expanded, the bottom level (i.e. if there is a minimum leaf
    size of 1) contains the same number of nodes as the entire dataset. So if every
    one of those contained a full boolean array of size of the whole dataset, you’ve
    got squared memory requirements which would be bad. On the other hand, if we just
    store the indexes of everything in this node, then that is going to get smaller
    and smaller.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一行代码与之前完全相同。然后我们检查它是否是叶节点。如果是叶节点，那么我们就没有更多的事情要做了。这意味着我们就在底部，没有进行分割，所以我们不需要做任何进一步的操作。另一方面，如果它不是叶节点，那么我们需要将其分割成左侧和右侧。现在，早些时候，我们创建了一个左侧和右侧的布尔数组。最好是有一个索引数组，因为我们不想在每个节点中都有一个完整的布尔数组。因为请记住，尽管在这个大小的树中看起来似乎没有很多节点，但当它完全展开时，底层（即如果最小叶大小为1）包含与整个数据集相同数量的节点。因此，如果每个节点都包含整个数据集大小的完整布尔数组，那么内存需求会增加。另一方面，如果我们只存储此节点中所有内容的索引，那么它将变得越来越小。
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: np.nonzero is exactly the same as `x<=self.split` which gets the boolean array
    but it turns it into the indexes of the `true`’s [[58:07](https://youtu.be/O5F9vR2CNYI?t=58m7s)].
    So this `lhs` is now a list of indexes of the left hand side and indexes of the
    right hand side. Now that we have indexes of the left hand side and the right
    hand side, we can now just go ahead and create a decision tree. So `self.lhs`
    is our decision tree for the left, `self.rhs` is our decision tree for the right.
    And we don’t have to do anything else. We’ve already written these. We already
    have a constructor that can create a decision tree. So when you really think about
    this is doing, it kind of hurts your head, right? Because the whole reason `find_varsplit`
    got called is because the decision tree constructor called it. But then `find_varsplit`
    itself then calls the decision tree constructor. So we actually have circular
    recursion. And I’m not nearly smart enough to be able to think through recursion,
    so I just choose not to. I just write what I mean and then I don’t think about
    it anymore. What do I want? To find a variable split. I’ve got to go through every
    column, see if there’s something better, if it managed to do a split, figure out
    left hand side and the right hand side and make them into decision trees. Now
    try to think through how these two methods call each other would just drive me
    crazy but I don’t need to. I know I have a decision tree constructor that works,
    I know I have a `find_varsplit` that works, so that’s it. That’s how I do recursive
    programming is by pretending I don’t. I just ignore it. That’s my advice. A lot
    of you are probably smart enough to be able to think through it better than I
    can, so that’s fine. If you can.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: np.nonzero与`x<=self.split`完全相同，它得到布尔数组，但将其转换为`true`的索引[[58:07](https://youtu.be/O5F9vR2CNYI?t=58m7s)]。因此，这个`lhs`现在是左侧和右侧的索引列表。现在我们有了左侧和右侧的索引，我们现在可以继续创建一个决策树。所以`self.lhs`是我们左侧的决策树，`self.rhs`是我们右侧的决策树。我们不需要做其他事情。我们已经写好了这些。我们已经有一个可以创建决策树的构造函数。所以当你真正思考这在做什么时，会有点让人头疼，对吧？因为`find_varsplit`被调用的原因是因为决策树构造函数调用了它。但是`find_varsplit`本身又调用了决策树构造函数。所以我们实际上有循环递归。我并不聪明到足以能够思考递归，所以我选择不去想。我只是写出我的意思，然后不再考虑。我想要什么？找到一个变量分割。我必须遍历每一列，看看是否有更好的东西，如果成功进行了分割，找出左侧和右侧，然后将它们转换为决策树。现在尝试思考这两种方法如何相互调用会让我发疯，但我不需要这样做。我知道我有一个有效的决策树构造函数，我知道我有一个有效的`find_varsplit`，所以就这样。这就是我进行递归编程的方式，就是假装我没有。我只是忽略它。这是我的建议。你们中很多人可能足够聪明，能够比我更好地思考这个问题，那就好。如果你能的话。
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: So now that I’ve written that, again, I can patch it into the DecisionTree class
    and as soon as I do, the TreeEnsemble constructor will now use that, because Python
    is dynamic.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我已经写好了，我可以将其打补丁到DecisionTree类中，一旦我这样做了，TreeEnsemble构造函数将会使用它，因为Python是动态的。
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now I can check [[1:00:31](https://youtu.be/O5F9vR2CNYI?t=1h31s)]. My left hand
    side should have 159 samples and the value of 9.66.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我可以检查[1:00:31](https://youtu.be/O5F9vR2CNYI?t=1h31s)。我的左手边应该有159个样本和值为9.66。
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Right hand side, 841 samples and 10.15.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 右手边，841个样本和10.15。
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Left hand side of left hand side, 150 samples and 9.62.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 左手边的左手边，150个样本和9.62。
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: So you can see, because I’m not nearly clever enough to write machine learning
    algorithms, not only can I not write them correctly the first time, often every
    single line I write will be wrong. So I always start from the assumption that
    the line of code I just typed is almost certainly wrong. And I just have to see
    why and how. So I just make sure. Eventually I get to the point where much to
    my surprise, it’s not broken anymore. So here, I can feel like okay, it would
    be surprising if all of these things accidentally happen to be exactly the same
    as scikit-learn. So this is looking pretty good.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到，因为我并不聪明到足以编写机器学习算法，不仅我第一次写不正确，通常每一行我写的都是错误的。所以我总是从这样的假设开始，我刚刚输入的代码几乎肯定是错误的。我只需要看看为什么以及如何。所以我只是确保。最终我会到达这样一个点，让我很惊讶的是，它不再出错了。所以在这里，我可以感觉到好吧，如果所有这些事情碰巧与scikit-learn完全相同，那将是令人惊讶的。所以看起来还不错。
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Predictions [1:01:43]
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测 [1:01:43]
- en: Now that we have something that can build a whole tree, we want to have something
    that can calculate predictions. So to remind you, we already have something that
    calculates prediction for for `TreeEnsemble` (by calling `tree.predict(x)`), but
    there is nothing called `tree.predict` in `DecisionTree` so we are going to have
    to write that.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个可以构建整个树的东西，我们想要有一个可以计算预测的东西。所以提醒一下，我们已经有了一个可以为`TreeEnsemble`计算预测的东西（通过调用`tree.predict(x)`），但在`DecisionTree`中没有叫做`tree.predict`的东西，所以我们需要写一个。
- en: '![](../Images/cf2a09b7b7b846f277cc41426d6a18c7.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf2a09b7b7b846f277cc41426d6a18c7.png)'
- en: To make this more interesting, let’s start bringing up the number of columns
    that we use.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这更有趣，让我们开始增加我们使用的列数。
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let’s create our `TreeEnsemble` again.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次创建我们的`TreeEnsemble`。
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: And this time, let’s go to a maximum depth of 3.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，让我们将最大深度设为3。
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/79d6042b4df9157317b8f34116ed14e1.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79d6042b4df9157317b8f34116ed14e1.png)'
- en: So now our tree is getting more interesting. Let’s now define how we create
    a set of prediction for a tree. So a set of predictions for a tree is simply the
    prediction for a row for every row. That’s it. That’s our predictions. So the
    predictions for a tree are every row’s predictions in an array. So again, we’re
    skipping thinking, thinking is hard. So let’s just keep pushing it back. So this
    `**for** xi **in** x` is kind of handy, right? Notice that you can do `for` blah
    in array with a numpy array regardless of the rank of the array. Regardless of
    the number of axes in the array. What it does is it will loop through the leading
    axis.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们的树变得更加有趣。现在让我们定义如何为树创建一组预测。因此，树的一组预测就是每一行的预测。就是这样。这就是我们的预测。因此，树的预测是数组中每一行的预测。所以再次，我们跳过思考，思考很困难。所以让我们继续推迟。这个`**for**
    xi **in** x`很方便，对吧？请注意，无论数组的秩如何，您都可以使用numpy数组中的`for` blah。无论数组中的轴数是多少。它的作用是遍历主轴。
- en: These concepts are going to be very very important as we get into more and more
    neural networks because we are going to be all doing tensor computations all the
    time. So the leading axis of a vector is the vector itself. The leading axis of
    a matrix are the rows. The leading axis of a tree dimensional tensor is the matrices
    that represent the slices and so forth. In this case, because x is a matrix, this
    is going to loop through the rows. And if you write your tensor code this way,
    then it will tend to generalize nicely to higher dimensions. It doesn’t really
    matter how many dimensions there are in this `x`. This is going to loop through
    each of the leading axis. So we can now call that `DecisionTree.predict`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念在我们进入越来越多的神经网络时将变得非常重要，因为我们将一直在进行张量计算。因此，向量的主轴是向量本身。矩阵的主轴是行。三维张量的主轴是表示切片的矩阵等等。在这种情况下，因为
    x 是一个矩阵，这将循环遍历行。如果您以这种方式编写您的张量代码，那么它将很好地推广到更高的维度。在这个 `x` 中有多少维度并不重要。这将循环遍历每个主轴。因此，我们现在可以称之为
    `DecisionTree.predict`。
- en: '[PRE28]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: So all I need to do is write `predict_row` [[1:04:17](https://youtu.be/O5F9vR2CNYI?t=1h4m17s)].
    And I’ve delayed thinking so much, which is great, that the actual point where
    I actually have to do the work, it’s now basically trivial. If we are at a leaf
    node, then the prediction is just equal to whatever that value was which we calculated
    right back in the original tree constructor (just the average of the `y`’s). If
    it’s not a leaf node, then we have to figure out whether to down the left hand
    path or the right hand path to get the prediction. So if this variable in this
    row (`xi[self.var_idx]`) is less than or equal to the amount we decided to split
    on, then we go down the left path; otherwise we go down the right path. And then
    having figured out what path/tree we want, then we can just call predict_row on
    that. Again, we’ve accidentally created something recursive. If it’s a leaf, return
    the value; otherwise return the prediction for the left hand side or the right
    hand side as appropriate.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我需要做的就是编写`predict_row`。我一直在拖延思考，这很好，实际上我需要做工作的地方，现在基本上是微不足道的。如果我们在叶节点，那么预测值就等于我们在原始树构造函数中计算的那个值（即`y`的平均值）。如果不是叶节点，那么我们必须弄清楚是沿左路径还是右路径进行预测。因此，如果这一行中的变量（`xi[self.var_idx]`）小于或等于我们决定拆分的值，则我们沿左路径前进；否则我们沿右路径前进。然后，确定我们想要的路径/树之后，我们只需在其上调用`predict_row`。再次，我们无意中创建了递归的东西。如果是叶节点，则返回该值；否则根据需要返回左侧或右侧的预测值。
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Notice this here `self.lhs **if** xi[self.var_idx]<=self.split **else** self.rhs`
    , this if has nothing to do with this if:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里的`self.lhs **if** xi[self.var_idx]<=self.split **else** self.rhs`，这个if与上面的if没有任何关系：
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This if (right above) is a control flow statement that tells Python to go down
    on that path or that path to do some calculation. This if (below) is an operator
    that returns a value.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的这个if是一个控制流语句，告诉Python沿着这条路径或那条路径进行一些计算。下面的这个if是一个返回值的运算符。
- en: '[PRE31]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'So those of you that have done C or C++ will recognize it as being identical
    to this (i.e. ternary operator):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你们做过C或C++的人会认出它与这个是完全相同的（即三元运算符）：
- en: '[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Basically what we are doing is we are going to get a value where we are going
    to say it’s this value (`do1()`) if `something` is true and that value (`do2()`)
    otherwise. You could write it in the lengthy way but that would require writing
    4 lines of code to do one thing, and also require you to code that if you read
    it to yourself or to somebody else, is not at all naturally the way you would
    express it. I want to say “the tree I’m going to go down is the left hand side
    if the variable is less than the split or the right hand side otherwise. So I
    want to write my code the way I would think about or the way I would say my code.
    So this kind of ternary operator can be quite helpful for that.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上我们要做的是，我们要得到一个值，如果`something`为真，我们会说这个值是(`do1()`)，否则是另一个值(`do2()`)。你可以用冗长的方式来写，但那将需要写4行代码来做一件事，而且还需要你编写的代码，如果你自己或向别人阅读时，表达方式并不自然。我想说“我要走的树是左边，如果变量小于分割值，否则是右边。所以我想按照我思考或说代码的方式来编写我的代码。因此，这种三元运算符对此非常有帮助。
- en: 'So now that I’ve got a prediction for a row, I can dump that into my class
    [[1:07:09](https://youtu.be/O5F9vR2CNYI?t=1h7m9s)]:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我已经对一行进行了预测，我可以将其放入我的类中：
- en: '[PRE33]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: And now I can calculate predictions
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我可以计算预测。
- en: '[PRE34]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: And I can now plot my actuals against my predictions [[1:07:17](https://youtu.be/O5F9vR2CNYI?t=1h7m17s)].
    When you do a scatter plot, you’ll often have a lot of dots sitting on top of
    each other, so a good trick is to use alpha. Alpha means how transparent the things,
    not just in matplotlib but in every graphics package in the world pretty much.
    So if you set alpha to less than 1, then this is saying you would need 20 dots
    on top of each other for it to be fully blue. So this is a good way to see how
    much things are sitting on top of each other — a good trick for scatter plots.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我可以将我的实际数据与我的预测数据进行对比。当你做散点图时，通常会有很多点重叠在一起，所以一个好的技巧是使用alpha。Alpha表示透明度，不仅在matplotlib中，在世界上几乎所有的图形包中都是如此。因此，如果将alpha设置为小于1，那么这意味着你需要将20个点叠加在一起才能完全显示为蓝色。这是一个很好的方法来看看有多少点重叠在一起
    - 散点图的一个好技巧。
- en: '[PRE35]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](../Images/296362b55926d59abb5a22c586cd3787.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/296362b55926d59abb5a22c586cd3787.png)'
- en: There’s my R².
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的R²。
- en: '[PRE36]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: So let’s now go ahead and do a random forest with no max amount of splitting,
    and our tree ensemble had no max amount of splitting, we can compare our R² to
    their R².
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 那么现在让我们继续进行一个没有最大分裂次数的随机森林，我们的树集合也没有最大分裂次数，我们可以将我们的R²与他们的R²进行比较。
- en: '[PRE37]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](../Images/ce944c854612b7af633893596700c1da.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce944c854612b7af633893596700c1da.png)'
- en: '[PRE38]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: They are not the same but actually ours is a little better. I don’t know what
    we did differently, but we’ll take it 😊 So we have now something which for a forest
    with a single tree in is giving as good accuracy on a validation set using an
    actual real-world dataset (blue book for bulldozers) compare to scikit-learn.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 它们并不相同，但实际上我们的稍微好一点。我不知道我们做了什么不同，但我们会接受它😊 所以现在我们有了一个对于一个只有一棵树的森林，在使用一个真实的实际数据集（推土机的蓝皮书）进行验证时，与scikit-learn相比提供了同样好的准确性。
- en: Putting it together [[1:08:50](https://youtu.be/O5F9vR2CNYI?t=1h8m50s)]
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 把它放在一起
- en: Let’s go ahead and round this out. So what I would want to do now is to create
    a package that has this code in. And I created it by creating a method here, a
    method there, and patching them together so what I did now is I went back through
    in my notebook and collected up all the cells that implemented methods and pasted
    them all together.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续完善这个。现在我想要做的是创建一个包含这段代码的包。我通过创建一个方法，再创建一个方法，然后将它们拼接在一起来创建这个包。现在我回到笔记本中，收集了所有实现方法的单元格，然后将它们全部粘贴在一起。
- en: '[PRE39]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: That was it. That was the code we wrote together.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。这就是我们一起编写的代码。
- en: '[PRE40]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](../Images/efebadb4e82db4232ac7b5d816264dd8.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efebadb4e82db4232ac7b5d816264dd8.png)'
- en: '[PRE41]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Here we are. We have a model of blue book for bulldozers with a 71 R² with a
    random forest we wrote entirely from scratch. That’s pretty cool.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有一个蓝色推土机的模型，使用了我们完全从头开始编写的随机森林，R²为71。这很酷。
- en: Performance and Cython [[1:12:30](https://youtu.be/O5F9vR2CNYI?t=1h12m30s)]
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能和Cython
- en: When I tried comparing the performance of this against scikit-learn, this is
    quite a lot slower and the reason why is that although a lot of the work is being
    done by numpy which is nicely optimized C code, think about the very bottom level
    of a tree. If we’ve got a million data points and the bottom level of the tree
    has something like 500,000 decision points with a million leaves underneath. That’s
    like 500,000 split methods being called, which contains multiple calls to numpy
    which only have like one item that’s being calculated on. That’s very inefficient.
    It’s the kind of thing that Python is particularly not good at performance-wise
    (i.e. calling lots of functions lots of times). We can see it’s not bad. For a
    random forest which 15 years ago would have been considered pretty big, this would
    be considered pretty good performance. But nowadays, this is some hundreds of
    times at least slower than it should be.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当我尝试比较这个与scikit-learn的性能时，这个要慢得多，原因是虽然很多工作是由numpy完成的，numpy是优化良好的C代码，但想想树的最底层。如果我们有一百万个数据点，树的底层有大约500,000个决策点，底下有一百万个叶子。这就像调用了500,000个分割方法，其中包含多次调用numpy，而numpy只有一个要计算的项目。这是非常低效的。这是Python在性能方面特别不擅长的事情（即多次调用大量函数）。我们可以看到它并不差。对于15年前被认为是相当大的随机森林来说，这被认为是相当不错的性能。但是现在，这至少比应该的速度慢了几百倍。
- en: What scikit-learn folks did to avoid this problem was, they wrote their implementation
    in something called Cython. Cython is a superset of Python. So any Python you’ve
    written pretty much, you can use as Cython. But then what happens is Cython runs
    it in a very different way. Rather than passing it to the Python interpreter,
    it instead converts it to C, compiles that, and then runs that C code. Which means,
    the first time you run it, it takes a little longer since it has to go through
    the translation and compilation, but then after that it can be quite a bit faster.
    So I wanted just to quickly show you what that looks like because you are absolutely
    going to be in a position where Cython is going to help you with your work and
    most of the people you are working with will have never used it (may not even
    know it exists), so this is like a great superpower to have.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的开发人员为了避免这个问题所做的是，他们使用了一种叫做Cython的东西来实现。Cython是Python的一个超集。所以你写的任何Python代码基本上都可以作为Cython来使用。但是Cython运行方式有所不同。它不是直接传递给Python解释器，而是将其转换为C语言，编译，然后运行该C代码。这意味着，第一次运行时会花费一些时间，因为需要进行翻译和编译，但之后运行会快得多。所以我想快速向你展示一下这是什么样子，因为你肯定会遇到Cython可以帮助你工作的情况，而你大部分一起工作的人可能从未使用过它（甚至可能不知道它的存在），所以拥有这种超能力是非常棒的。
- en: 'To use Cython in a notebook, you say:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中使用Cython，你可以这样说：
- en: '[PRE42]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Here is a Python function `fib1`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个Python函数`fib1`：
- en: '[PRE43]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Here is the same as a Cython function. It’s exactly the same thing with `%%cython`
    at the top. This actually runs about twice as fast as `fib1` just because it does
    the compilation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个Cython函数。它与顶部的`%%cython`完全相同。实际上，它的运行速度大约是`fib1`的两倍，因为它进行了编译。
- en: '[PRE44]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Here is the same version again where I’ve used a special Cython extension called
    `cdef` which defines the C data type of the return value and of each variable.
    Basically that’s the trick that you can use to start making things run quickly.
    And that point, now it knows it’s not just some Python object called T. So fib3,
    it’s exactly the same as before but we say what the data type of the thing we
    passed to it was is and then define the data types of each of the variables.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是同样的版本，我使用了一个特殊的Cython扩展叫做`cdef`，它定义了返回值和每个变量的C数据类型。基本上这就是你可以用来开始加快运行速度的技巧。在那一点上，现在它知道它不只是一个名为T的Python对象。所以fib3，它和之前完全一样，但我们说我们传递给它的东西的数据类型是什么，然后定义每个变量的数据类型。
- en: '[PRE45]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: So then if we call that, we’ve now got something that’s 10 times faster.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们这样做，现在我们有了一个快10倍的东西。
- en: '[PRE46]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: It doesn’t really take that much extra and it’s just Python with a few little
    bits of markup, so it’s good to know that it exists because if there’s something
    custom you’re trying to do, it’s actually painful having to go out and going to
    C and compile it and link it back and all that. Where else doing it here is pretty
    easy.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不需要太多额外的工作，只是用一点Python和一些标记，所以知道它的存在是很好的，因为如果有一些定制的东西你想要做，实际上要去C语言编译并链接回来是很痛苦的。而在这里做起来相当容易。
- en: '**Question**: When you are doing the Cython version, in for numpy array, is
    there a specific C type of it [[1:17:16](https://youtu.be/O5F9vR2CNYI?t=1h17m16s)]?
    Yes, there are a lot of specific stuff for integrating Cython with numpy and there’s
    a whole page about it. So we don’t worry about going over it but you can read
    that and you can basically see the basic ideas.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：当你在使用Cython版本时，对于numpy数组，是否有特定的C类型[[1:17:16](https://youtu.be/O5F9vR2CNYI?t=1h17m16s)]？是的，有很多特定的内容用于将Cython与numpy集成，有一个完整的页面介绍了这些内容。所以我们不用担心过多细节，但你可以阅读那个页面，基本上可以了解基本思想。'
- en: '[## Working with NumPy — Cython 0.29a0 documentation'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '## 使用NumPy与Cython — Cython 0.29a0 文档'
- en: cython.readthedocs.io](https://cython.readthedocs.io/en/latest/src/tutorial/numpy.html?source=post_page-----69c50bc5e9af--------------------------------)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[cython.readthedocs.io](https://cython.readthedocs.io/en/latest/src/tutorial/numpy.html?source=post_page-----69c50bc5e9af--------------------------------)'
- en: There is this `cimport` which basically imports a certain types of Python library
    into the kind of the C bit of the code and you can then use it in your Cython.
    It’s pretty straight forward.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有这个`cimport`，基本上是将某种类型的Python库导入到代码的C部分，然后你可以在Cython中使用它。这很简单直接。
- en: 'So your mission now is to implement:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你现在的任务是实现：
- en: Confidence based on tree variance
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于树方差的置信度
- en: Feature importance
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征重要性
- en: Partial dependence
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分依赖
- en: Tree interpreter
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树解释器
- en: for that random forest. Removing redundant features doesn’t use a random forest
    at all, so you don’t have to worry about that. The extrapolation is not an interpretation
    technique so you don’t have to worry about that. So it’s just the other ones.
    So confidence based on tree variance, we’ve already written that code so I suspect
    that the exact same code we have in the notebook should continue to work. So you
    can try and make sure to get that working. Feature importance is with the variable
    shuffling technique and once you have that working, partial dependence will just
    be a couple of lines of code away because rather than shuffling a column, you’re
    just replacing it with a constant value. It’s nearly the same code.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那个随机森林。删除冗余特征根本不使用随机森林，所以你不必担心这个。外推不是一种解释技术，所以你也不必担心这个。所以只是其他的。所以基于树方差的置信度，我们已经编写了那段代码，所以我怀疑我们在笔记本中拥有的完全相同的代码应该继续工作。所以你可以尝试确保让它工作。特征重要性是通过变量洗牌技术实现的，一旦你让它工作，偏依赖只是几行代码之遥，因为你不是洗牌一列，而是用一个常数值替换它。几乎是相同的代码。
- en: Then tree interpreter, it’s going to require you writing some code and thinking
    about that. Once you’ve written tree interpreter, you’re very close, if you want
    to, to creating the second approach to feature importance — the one where you
    add up the importance across all of the rows. Which means, you would then be very
    close to doing interaction importance. So it turns out that there’s actually a
    very good library for interaction importance for xgboost but there doesn’t seem
    to be one for random forest, so you could start by getting it working on our version
    (if you want to do interaction importance) and then you could get it working on
    the original scikit-learn version that would be a cool contribution. Sometimes
    writing it against your own implementation is nicer because you can see exactly
    what’s going on.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后树解释器，这将需要您编写一些代码并思考。一旦您编写了树解释器，如果您愿意，您就非常接近创建特征重要性的第二种方法 - 即在所有行中累加重要性的方法。这意味着，您将非常接近进行交互重要性。事实证明，xgboost实际上有一个非常好的交互重要性库，但似乎没有一个适用于随机森林，因此您可以从使其在我们的版本上运行开始（如果您想进行交互重要性），然后您可以使其在原始的scikit-learn版本上运行，这将是一个很酷的贡献。有时，针对自己的实现编写代码更好，因为您可以清楚地看到发生了什么。
- en: If you get stuck at any point, ask on the forum. There is a whole page on wiki
    about [how to ask for help](http://wiki.fast.ai/index.php/How_to_ask_for_Help).
    When you ask your co-workers on Slack for help, when you ask people in a technical
    community on Github or Discourse for help, asking for help the right way will
    go a long way towards having people want to help you and be able to help you.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在任何时候遇到困难，请在论坛上提问。关于如何寻求帮助的整个页面都在维基上有。当你在Slack上向同事寻求帮助，当你在Github或Discourse上向技术社区的人寻求帮助时，正确地寻求帮助将有助于让人们愿意帮助你并能够帮助你。
- en: Search for the error you are getting, see if somebody’s already asked about
    it.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索你遇到的错误，看看是否已经有人问过。
- en: How have you tried to fix it already?
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经尝试过如何修复它了吗？
- en: What do you think is going wrong?
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你认为出了什么问题？
- en: What kind of computer are you on? How is it setup? What are the software versions?
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你用的是什么样的电脑？它是如何设置的？软件版本是什么？
- en: Exactly what did you type and exactly what happened?
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你确切地输入了什么，确切地发生了什么？
- en: You could do that by taking a screenshot, so make sure you’ve got some screenshot
    software that’s really easy to use. So if I were to take a screenshot, I just
    hit a button, select the area, copy to clipboard, go to forum, paste it in, and
    there we go (You can even make the image smaller!).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过截屏来做到这一点，所以确保你有一些非常容易使用的截屏软件。所以如果我要截屏，我只需按下一个按钮，选择区域，复制到剪贴板，转到论坛，粘贴进去，然后就完成了（你甚至可以将图像缩小！）。
- en: '![](../Images/92d1bbfc6db820b7cc4eb9435895a35a.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92d1bbfc6db820b7cc4eb9435895a35a.png)'
- en: 'Better still, if there are a few lines of code and error messages to look at,
    create a Gist. Gist is a handy little Github thing which basically lets you share
    code. If I wanted to create a Gist of this, I actually have an extension:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的做法是，如果有几行代码和错误消息需要查看，可以创建一个Gist。Gist是一个很方便的Github工具，基本上可以让你分享代码。如果我想要创建一个这样的Gist，我实际上有一个扩展：
- en: '![](../Images/a82bdd28d15272abac0235ea5f63dc1b.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a82bdd28d15272abac0235ea5f63dc1b.png)'
- en: So click on that, give it a name, and say make public. That takes my Jupyter
    Notebook and shares it publicly. I can then grab that URL, copy link location,
    and paste it into my forum post. Then when people click on it, then they’ll immediately
    see my notebook.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 点击那个，给它起个名字，然后点击“公开”。这样就可以将我的Jupyter笔记本公开分享。然后我可以复制那个URL，复制链接位置，然后粘贴到我的论坛帖子中。然后当人们点击它时，他们会立即看到我的笔记本。
- en: Now that particular button is an extension, so on Jupyter, you need to click
    on Nbextensions and click on Gist-it. While you are there, you should also click
    on Collapsible Headings that’s this handy thing I use that lets me collapse things
    and open them up.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个特定的按钮是一个扩展，所以在Jupyter上，您需要点击Nbextensions，然后点击Gist-it。当您在那里时，您还应该点击Collapsible
    Headings，这是我使用的一个方便的功能，让我可以折叠和展开内容。
- en: '![](../Images/2444444b8b82c9c6c022af4852dc783a.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2444444b8b82c9c6c022af4852dc783a.png)'
- en: If you go to your Jupyter and don’t see this Nbextensions button, then just
    google for Jupyter Nbextensions — it’ll show you how to pip install it and get
    it set up.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您打开Jupyter时没有看到这个Nbextensions按钮，那么只需搜索Jupyter Nbextensions — 它会告诉您如何使用pip安装并设置它。
- en: Neural network broadly defined [[1:23:20](https://youtu.be/O5F9vR2CNYI?t=1h23m20s)]
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson4-mnist_sgd.ipynb)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Other than the assignment, we are done with random forests and until the next
    course when you look at GBMs, we are done with decision tree ensembles. We are
    going to move onto neural networks broadly defined. Neural networks are going
    to allow us to go beyond just the kind of nearest neighbors approach of random
    forests. All the random forests can do is to average data that it’s already seen.
    It can’t extrapolate or calculate. Linear regression can calculate and can extrapolate
    but only in very limited ways. Neural nets give us the best of both worlds.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: We are going to start by applying them to unstructured data. Unstructured data
    means pixels, amplitudes of sound waves, or words — data where everything in all
    the columns are all the same type as opposed to a database table where you’ve
    got a revenue, cost, zipcode, and state (structured data). We are going to use
    it for structured data as well but we are going to do that a little bit later.
    Unstructured data is a little bit easier and it’s also the area which more people
    have been applying deep learning to for longer.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: If you are doing the deep learning course as well, you’ll see that we are going
    to be approaching kind of the same conclusion from two different directions. So
    the deep learning course is starting out with big complicated convolutional neural
    networks being solved with sophisticated optimization schemes, and we are going
    to gradually drill down into exactly how they work. Where else with the machine
    learning course, we are going to be starting out more with how does stochastic
    gradient descent actually work, what can we do with one single layer which would
    allow us to create things like logistic regression. When we add regularization
    to that, how does that give us things like ridge regression, elastic net lasso.
    As we add additional layers to that, how does that let us handle more complex
    problems. We are only going to be looking at fully connected layers in this machine
    learning course and then I think next semester with Yannet, you’re probably going
    to be looking at some more sophisticated approaches. So this machine learning,
    we are going to be looking much more at what’s actually happening with the matrices
    and how they are actually calculated, and the deep learning, it’s much more like
    what are the best practices for actually solving at world-class level, real-world
    deep learning problems.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Next week, we are going to be looking at the classic MNIST problem which is
    how do we recognize digits. If you are interested, you can skip ahead and try
    and do this with a random forest and you’ll find it’s not bad. Given that random
    forest is basically a type of nearest neighbors (it’s finding what are the nearest
    neighbors in tree space), then a random forest could absolutely recognize that
    this 9, those pixels, are similar to pixels we’ve seen in these other ones, and
    on average, they were 9s as well. So it can absolutely solve these kind of problems
    to an extent using random forests. But we end up being rather data limited because
    every time we put in another decision point, we are halving our data roughly and
    so this is this limitation on the amount of calculation we can do. Where else
    with neural nets, we are going to be able to use lots and lots and lots of parameters
    using these tricks we are going to learn about regularization and so we are going
    to be able to do lots of computation and there’s got to be very little limitation
    on what we can actually end up calculating as a result.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Good luck with your random forest interpretation and I will see you next time.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
