- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:49:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:49:04
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2112.02814] A Survey of Deep Learning for Low-Shot Object Detection'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2112.02814] 深度学习在低样本物体检测中的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2112.02814](https://ar5iv.labs.arxiv.org/html/2112.02814)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2112.02814](https://ar5iv.labs.arxiv.org/html/2112.02814)
- en: '¹¹footnotetext: $\dagger$ Corresponding author.²²footnotetext: This work is
    funded by National Natural Science Foundation of China (U20B2066, 61976186, 62106220),
    Ningbo Natural Science Foundation (2021J189), and the Fundamental Research Funds
    for the Central Universities (2021FZZX001-23, 226-2023-00048).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹脚注：$\dagger$ 通讯作者。²²脚注：本工作由中国国家自然科学基金（U20B2066、61976186、62106220）、宁波自然科学基金（2021J189）以及中央高校基本科研业务费（2021FZZX001-23、226-2023-00048）资助。
- en: A Survey of Deep Learning for Low-Shot Object Detection
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在低样本物体检测中的综述
- en: Qihan Huang Zhejiang UniversityChina ,  Haofei Zhang Zhejiang UniversityChina
    ,  Mengqi Xue Zhejiang UniversityChina ,  Jie Song Zhejiang UniversityChina  and 
    Mingli Song^† Zhejiang UniversityChina(2023)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 黄启涵 浙江大学中国，张浩飞 浙江大学中国，薛梦琦 浙江大学中国，宋洁 浙江大学中国，宋明立^† 浙江大学中国（2023）
- en: Abstract.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Object detection has achieved a huge breakthrough with deep neural networks
    and massive annotated data. However, current detection methods cannot be directly
    transferred to the scenario where the annotated data is scarce due to the severe
    overfitting problem. Although few-shot learning and zero-shot learning have been
    extensively explored in the field of image classification, it is indispensable
    to design new methods for object detection in the data-scarce scenario since object
    detection has an additional challenging localization task. Low-Shot Object Detection (LSOD)
    is an emerging research topic of detecting objects from a few or even no annotated
    samples, consisting of One-Shot Object Localization (OSOL), Few-Shot Object Detection (FSOD),
    and Zero-Shot Object Detection (ZSOD). This survey provides a comprehensive review
    of LSOD methods. First, we propose a thorough taxonomy of LSOD methods and analyze
    them systematically, comprising some extensional topics of LSOD (semi-supervised
    LSOD, weakly-supervised LSOD, and incremental LSOD). Then, we indicate the pros
    and cons of current LSOD methods with a comparison of their performance. Finally,
    we discuss the challenges and promising directions of LSOD to provide guidance
    for future works.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测在深度神经网络和大量标注数据的支持下取得了巨大突破。然而，当前的检测方法无法直接迁移到标注数据稀缺的场景中，因为存在严重的过拟合问题。虽然少样本学习和零样本学习在图像分类领域已经被广泛探讨，但由于物体检测还涉及额外的挑战性定位任务，因此在数据稀缺场景中设计新方法是不可或缺的。低样本物体检测（LSOD）是一个新兴的研究主题，涉及从少量甚至没有标注样本中检测物体，包括一-shot物体定位（OSOL）、少-shot物体检测（FSOD）和零-shot物体检测（ZSOD）。本综述对LSOD方法进行了全面回顾。首先，我们提出了LSOD方法的详细分类，并系统分析了它们，包括一些LSOD的扩展主题（半监督LSOD、弱监督LSOD和增量LSOD）。然后，我们指出了当前LSOD方法的优缺点，并比较了它们的性能。最后，我们讨论了LSOD的挑战和有前景的方向，以为未来的研究提供指导。
- en: 'Few-Shot Object Detection, One-Shot Object Detection, Zero-Shot Object detection,
    Transfer-Learning, Meta-Learning^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†doi:
    10.1145/3626312^†^†journal: CSUR^†^†article: 1^†^†publicationmonth: 10^†^†ccs:
    General and reference Surveys and overviews^†^†ccs: Computing methodologies Object
    detection'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本物体检测、一-shot物体检测、零-shot物体检测、迁移学习、元学习^†^†版权：acmcopyright^†^†期刊年份：2023^†^†doi：10.1145/3626312^†^†期刊：CSUR^†^†文章：1^†^†出版月份：10^†^†ccs：一般与参考
    综述和概述^†^†ccs：计算方法 物体检测
- en: 1\. Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Object detection is a fundamental yet challenging task in computer vision, aiming
    to locate objects of certain classes in images. It has been widely applied to
    many computer vision tasks like object tracking (Yilmaz et al., [2006](#bib.bib139);
    Wang et al., [2019b](#bib.bib117); Voigtlaender et al., [2019](#bib.bib114)),
    autonomous driving (Grigorescu et al., [2020](#bib.bib32); Yurtsever et al., [2020](#bib.bib140)),
    scene graph generation (Teng and Wang, [2021](#bib.bib108); Yang et al., [2018](#bib.bib134);
    Tang et al., [2020](#bib.bib107)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测是计算机视觉中的一个基础而具有挑战性的任务，旨在定位图像中某些类别的物体。它已广泛应用于许多计算机视觉任务，如物体跟踪（Yilmaz等，[2006](#bib.bib139)；Wang等，[2019b](#bib.bib117)；Voigtlaender等，[2019](#bib.bib114)），自动驾驶（Grigorescu等，[2020](#bib.bib32)；Yurtsever等，[2020](#bib.bib140)），场景图生成（Teng和Wang，[2021](#bib.bib108)；Yang等，[2018](#bib.bib134)；Tang等，[2020](#bib.bib107)）。
- en: 'The general process of object detection is to predict classes for a set of
    bounding boxes (imaginary rectangles for reference in the image). Most traditional
    methods are slow since they generate the bounding boxes using brute force by sliding
    a window through the whole image. Viola-Jones (VJ) detector (Viola and Jones,
    [2001](#bib.bib113)) first achieves real-time detection of human faces with three
    speed-up techniques: integral image, feature selection, and detection cascades.
    Later, histogram of oriented gradients (HOG) (Dalal and Triggs, [2005](#bib.bib16))
    is proposed, and many traditional object detectors adopt it for feature description.
    Deformable part-based model (DPM) (Felzenszwalb et al., [2008](#bib.bib24)) is
    a representative traditional method. DPM divides an object detection task into
    several fine-grained detection tasks, then uses some part-filters to detect parts
    of an object and aggregates them for final prediction. Although people have made
    many improvements, traditional methods are restricted by their slow speed and
    low accuracy.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测的一般过程是对一组边界框（图像中的虚拟矩形）进行类别预测。大多数传统方法速度较慢，因为它们通过在整个图像中滑动窗口来生成边界框，采用的是蛮力方法。Viola-Jones
    (VJ) 检测器（Viola 和 Jones, [2001](#bib.bib113)）首次通过三种加速技术实现了人脸的实时检测：积分图像、特征选择和检测级联。随后，方向梯度直方图（HOG）（Dalal
    和 Triggs, [2005](#bib.bib16)）被提出，许多传统物体检测器采用它进行特征描述。变形部件模型（DPM）（Felzenszwalb 等,
    [2008](#bib.bib24)）是一个代表性的传统方法。DPM 将物体检测任务分解为若干个细粒度检测任务，然后使用一些部件滤波器检测物体的部分，并将其聚合以进行最终预测。尽管人们做了许多改进，传统方法仍然受到速度慢和精度低的限制。
- en: '![Refer to caption](img/a198e6b05b225c14e2b3a3e76b4f929e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a198e6b05b225c14e2b3a3e76b4f929e.png)'
- en: 'Figure 1\. Overview of this survey. This survey gives a general introduction
    to Low-Shot Object Detection (LSOD), then categorizes LSOD into three domains:
    One-Shot Object Localization (OSOL), Few-Shot Object Detection (FSOD) and Zero-Shot
    Object Detection (ZSOD). The more fine-grained categorization of these three domains
    is also demonstrated in the figure with three colors, which will be discussed
    detailedly in later sections. Each category is demonstrated with a part of representative
    works in the figure. Then the benchmarks for OSOL, FSOD and ZSOD are summarized,
    and the performance of different LSOD methods on these benchmarks is compared
    and analyzed. Finally, the future directions of LSOD are discussed.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 本调查概述。本调查对低样本物体检测（LSOD）进行了总体介绍，然后将LSOD分为三个领域：单样本物体定位（OSOL）、少样本物体检测（FSOD）和零样本物体检测（ZSOD）。这三个领域的更细粒度分类也在图中以三种颜色展示，这将在后续章节中详细讨论。每个类别在图中展示了部分代表性工作。然后，总结了OSOL、FSOD和ZSOD的基准，并比较和分析了不同LSOD方法在这些基准上的表现。最后，讨论了LSOD的未来方向。
- en: Compared with these traditional methods, deep-learning-based methods have significantly
    improved performance. Current deep detectors roughly consist of two-stage detectors
    and single-stage detectors. Two-stage detectors first generate region proposals
    (i.e., image regions which are more likely to contain objects) and next make predictions
    on them, following a similar framework to traditional methods. R-CNN (Girshick
    et al., [2014](#bib.bib31)) is one of the earliest works of two-stage detectors.
    It uses selective search to obtain region proposals then extracts their features
    with a pre-trained CNN model for further classification and regression. Fast R-CNN
    (Girshick, [2015](#bib.bib30)) improves R-CNN by using a region of interest (RoI)
    pooling layer to generate feature maps for region proposals from the integral
    feature map. Faster R-CNN (Ren et al., [2015](#bib.bib100)) further proposes a
    region proposal network (RPN) to generate region proposals from the whole image
    feature map using anchors (i.e., pre-defined bounding boxes with specific height
    and width). However, the generation of region proposals requires high computation
    cost and storage costs. To mitigate this problem, single-stage detectors are proposed
    to combine these two stages. YOLO-style object detectors (Redmon and Farhadi,
    [2018](#bib.bib99); Bochkovskiy et al., [2020](#bib.bib4); Ge et al., [2021](#bib.bib29))
    are the representative works of single-stage detectors. Given the feature map
    extracted from the original image, YOLO-style detectors directly pre-define anchors
    with multiple scales over all locations of the image and predict the class probabilities,
    regression offsets and object confidence scores of each anchor. Single-stage detectors
    achieve higher speed, but they generally underperform two-stage detectors. Moreover,
    some methods like focal loss (Lin et al., [2017b](#bib.bib72)) have been proposed
    to decrease the performance gap between single-stage and two-stage detectors.
    Recently, a transformer-based detector named DETR (Carion et al., [2020](#bib.bib9))
    has been proposed. DETR achieves end-to-end detection and has comparable performance
    to many classic detectors. Some extended methods (Zhu et al., [2021b](#bib.bib161);
    Dai et al., [2021](#bib.bib15)) are proposed to mitigate the slow convergence
    problem of DETR.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与这些传统方法相比，基于深度学习的方法显著提升了性能。当前的深度检测器大致分为两阶段检测器和单阶段检测器。两阶段检测器首先生成区域提议（即，更可能包含对象的图像区域），然后对其进行预测，遵循与传统方法类似的框架。R-CNN（Girshick
    等，[2014](#bib.bib31)）是最早的两阶段检测器之一。它使用选择性搜索获得区域提议，然后使用预训练的CNN模型提取特征，以进行进一步的分类和回归。Fast
    R-CNN（Girshick，[2015](#bib.bib30)）通过使用区域兴趣（RoI）池化层从整体特征图中生成区域提议的特征图，从而改进了R-CNN。Faster
    R-CNN（Ren 等，[2015](#bib.bib100)）进一步提出了区域提议网络（RPN），利用锚点（即，具有特定高度和宽度的预定义边界框）从整个图像特征图中生成区域提议。然而，区域提议的生成需要高计算成本和存储成本。为了解决这个问题，提出了单阶段检测器来将这两个阶段结合起来。YOLO风格的目标检测器（Redmon
    和 Farhadi，[2018](#bib.bib99)；Bochkovskiy 等，[2020](#bib.bib4)；Ge 等，[2021](#bib.bib29)）是单阶段检测器的代表作。给定从原始图像中提取的特征图，YOLO风格的检测器直接在图像的所有位置上预定义多个尺度的锚点，并预测每个锚点的类别概率、回归偏移量和对象置信度分数。单阶段检测器具有更高的速度，但它们通常表现不如两阶段检测器。此外，一些方法如焦点损失（Lin
    等，[2017b](#bib.bib72)）已经被提出，以减少单阶段检测器和两阶段检测器之间的性能差距。最近，提出了一种基于变换器的检测器，名为DETR（Carion
    等，[2020](#bib.bib9)）。DETR实现了端到端检测，并且性能可与许多经典检测器相媲美。一些扩展方法（Zhu 等，[2021b](#bib.bib161)；Dai
    等，[2021](#bib.bib15)）被提出以缓解DETR的慢收敛问题。
- en: However, these deep detectors tend to overfit when the training data is scarce
    and thus require abundant annotated data. In real life, it is hard to collect
    sufficient annotated data for some object classes due to the scarcity of these
    classes or special labeling costs, and current deep detectors are not competent
    in this situation. Therefore, the ability to detect objects from a few or even
    zero annotated samples is desired for modern detectors. To achieve this goal,
    Low-Shot Object Detection (LSOD) is introduced into object detection, including
    One-Shot Object Localization (OSOL), Few-Shot Object Detection (FSOD), Zero-Shot
    Object Detection (ZSOD). These three settings of LSOD mainly differ in the number
    of annotated samples for each category. Concretely, OSOL and FSOD tackle the situation
    that each object category has one or more annotated image samples, while ZSOD
    differentiates different classes according to the semantic information of each
    category instead of image samples.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些深度检测器在训练数据稀缺时往往会过拟合，因此需要大量标注数据。在现实生活中，由于这些类别的稀缺或特殊标注成本，难以收集足够的标注数据，而当前的深度检测器在这种情况下表现不佳。因此，现代检测器期望能够从少量甚至零标注样本中检测对象。为实现这一目标，低样本对象检测（LSOD）被引入对象检测中，包括单样本对象定位（OSOL）、少样本对象检测（FSOD）和零样本对象检测（ZSOD）。这三种LSOD设置主要在每个类别的标注样本数量上有所不同。具体而言，OSOL和FSOD解决每个对象类别具有一个或多个标注图像样本的情况，而ZSOD则根据每个类别的语义信息而不是图像样本来区分不同类别。
- en: OSOL and FSOD are developed following the mainstream scheme of few-shot learning (FSL).
    Few-shot learning divides the object classes into base classes with many annotated
    samples (denoted as base dataset) and novel classes with a few annotated samples
    (denoted as novel dataset). Note that the annotated samples and the test samples
    in novel classes are named as support samples and query samples, respectively.
    Few-shot learning requires to pre-train the model on the base dataset then uses
    the model to predict novel classes on the novel dataset for evaluation. Current
    few-shot learning methods are roughly categorized into meta-learning methods and
    transfer-learning methods. Meta-learning methods adopt a “learning-to-learn” mechanism,
    which defines multiple few-shot tasks on the base dataset to train the model,
    and enables the model to adapt to the real few-shot tasks quickly. Moreover, transfer-learning
    methods learn a good image representation by directly training the model on the
    base dataset, which is used for the novel dataset. Although meta-learning is a
    more natural approach to tackle the few-shot problem, Tian et al. (Tian et al.,
    [2020](#bib.bib110)) find that the baseline transfer-learning methods surpass
    some classic meta-learning methods, especially in the cross-domain few-shot learning.
    Current few-shot learning methods are mainly explored on the task of image classification.
    OSOL and FSOD are more challenging than few-shot image classification because
    object detection requires an extra task to locate the objects. As the branches
    of few-shot learning, OSOL and FSOD also inherit the core methods (meta-learning
    & transfer-learning) of it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: OSOL和FSOD是根据主流的少样本学习（FSL）方案开发的。少样本学习将对象类别分为具有大量标注样本的基础类别（称为基础数据集）和只有少量标注样本的新增类别（称为新增数据集）。请注意，新增类别中的标注样本和测试样本分别称为支持样本和查询样本。少样本学习要求在基础数据集上对模型进行预训练，然后使用模型对新增数据集上的新增类别进行预测以进行评估。目前的少样本学习方法大致可分为元学习方法和迁移学习方法。元学习方法采用“学习如何学习”机制，通过在基础数据集上定义多个少样本任务来训练模型，并使模型能够迅速适应真实的少样本任务。此外，迁移学习方法通过直接在基础数据集上训练模型来学习良好的图像表示，这些表示用于新增数据集。尽管元学习是一种更自然的解决少样本问题的方法，但Tian等人（Tian
    et al., [2020](#bib.bib110)）发现基线迁移学习方法在一些经典元学习方法中表现更好，尤其是在跨领域少样本学习中。目前的少样本学习方法主要在图像分类任务上进行探索。OSOL和FSOD比少样本图像分类更具挑战性，因为对象检测需要额外的任务来定位对象。作为少样本学习的分支，OSOL和FSOD也继承了其核心方法（元学习和迁移学习）。
- en: OSOL is a few-shot learning setting on object detection which locates objects
    using only one labeled image of each category in the image. Current OSOL methods
    all adopt the scheme of meta-learning following few-shot learning, where a large
    number of one-shot tasks are defined on the base dataset to train the model. OSOL
    has a strong guarantee that the model precisely knows the object classes contained
    in each test image. With this strong guarantee, the latest OSOL methods have achieved
    relatively high performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: OSOL 是一种少样本学习设置，用于目标检测，仅使用每个类别的一张标注图像来定位对象。当前的 OSOL 方法都采用基于少样本学习的元学习方案，其中在基础数据集上定义大量的一次性任务来训练模型。OSOL
    强有力地保证了模型准确知道每张测试图像中包含的对象类别。凭借这一强有力的保证，最新的 OSOL 方法已取得了相对较高的性能。
- en: Table 1\. Key Notations Used in This Article
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 本文使用的关键符号
- en: '| Notation | Description | Notation | Description |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 描述 | 符号 | 描述 |'
- en: '| $\phi_{q}$ | Feature map of integral query image $q$ | ${\rm Pool}(\cdot)$
    | Pool operation |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| $\phi_{q}$ | 整体查询图像 $q$ 的特征图 | ${\rm Pool}(\cdot)$ | 池化操作 |'
- en: '| $\phi_{s}$ | Feature map of integral support image | $\oplus$ | Element-wise
    sum |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| $\phi_{s}$ | 积分支持图像的特征图 | $\oplus$ | 元素级加法 |'
- en: '| $\phi_{\rm fused}$ | The aggregated feature map of $\phi_{q}$ and $\phi_{c}$
    | $\otimes$ | Channel-wise multiplication |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| $\phi_{\rm fused}$ | $\phi_{q}$ 和 $\phi_{c}$ 的聚合特征图 | $\otimes$ | 按通道乘法 |'
- en: '| $\phi_{r}$ | The RoI feature map in the query image | ${\rm Conv}(\cdot)$
    | Convolutional operation |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| $\phi_{r}$ | 查询图像中的 RoI 特征图 | ${\rm Conv}(\cdot)$ | 卷积操作 |'
- en: '| $v_{r}$ | The RoI feature vector in the query image | ${\rm FC}(\cdot)$ |
    FC layer |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| $v_{r}$ | 查询图像中的 RoI 特征向量 | ${\rm FC}(\cdot)$ | FC 层 |'
- en: '| $s_{r}$ | The RoI semantic embedding in the query image | ${\rm Softmax}(\cdot)$
    | Softmax operation |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| $s_{r}$ | 查询图像中的 RoI 语义嵌入 | ${\rm Softmax}(\cdot)$ | Softmax 操作 |'
- en: '| $v_{s}$ | The pooled feature vector | $\sigma(\cdot)$ | Sigmoid function
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| $v_{s}$ | 池化后的特征向量 | $\sigma(\cdot)$ | Sigmoid 函数 |'
- en: '| $v_{\rm fused}$ | The aggregated feature vector of $v_{q}^{i}$ and $v_{c}$
    | ${\rm RELU}(\cdot)$ | RELU function |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| $v_{\rm fused}$ | $v_{q}^{i}$ 和 $v_{c}$ 的聚合特征向量 | ${\rm RELU}(\cdot)$ | RELU
    函数 |'
- en: '| $s_{c}$ | The semantic embedding of class $c$ | $&#124;&#124;\cdot&#124;&#124;$
    | The norm of a vector |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| $s_{c}$ | 类别 $c$ 的语义嵌入 | $&#124;&#124;\cdot&#124;&#124;$ | 向量的范数 |'
- en: '| $p_{c}$ | The prediction score for class $c$ of a RoI | $[\cdot]$ | Concatenation
    operation |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| $p_{c}$ | RoI 的类别 $c$ 的预测得分 | $[\cdot]$ | 连接操作 |'
- en: '| $&#124;\cdot&#124;$ | The absolute value of a vector |  |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| $&#124;\cdot&#124;$ | 向量的绝对值 |  |  |'
- en: 'However, OSOL setting is not realistic enough since the object classes in the
    test images are not pre-known in real life. Therefore, another few-shot setting
    on object detection is adopted by more papers, which is named Few-Shot Object
    Detection (FSOD). The major differences between FSOD and OSOL are as follows:
    (1) FSOD needs to predict the correct category of potential objects in the test
    image. (2) OSOL samples support images independently for each test image, FSOD
    samples the support images only once for all test images. (3) In FSOD, the number
    of labeled samples per category can be larger than one. Similar to methods on
    few-shot image classification, FSOD methods are categorized into two mainstream
    methods: meta-learning methods and transfer-learning methods. Early FSOD methods
    mainly adopt the meta-learning scheme. The core operation of meta-learning FSOD
    methods is to extract the features of a few annotated samples (support features)
    and aggregate them into the features of query images (query features) for guidance
    on the prediction of query images. This aggregation operation promotes the model
    to learn adequate information from a few annotated samples. Early meta-learning
    FSOD methods simply aggregate the support features with the features of RoIs (RoI
    features) in the query images. Afterwards, researchers find that the aggregation
    of integral features is essential for performance improvement since the shallow
    components in the model also require the information of annotated samples (e.g.,
    the RPN component in Faster R-CNN needs the support features to filter out unmatched
    region proposals). Therefore, this survey categorizes meta-learning FSOD methods
    into RoI feature aggregation and mixed feature aggregation methods (“mixed” means
    using RoI feature aggregation and integral feature aggregation together). Unlike
    meta-learning methods, transfer-learning FSOD methods directly pre-train the detector
    on the base dataset and fine-tune it on the novel dataset. Early FSOD methods
    rarely adopt transfer-learning due to its poor performance. TFA (Wang et al.,
    [2020a](#bib.bib119)) subverts this cognition, which proposes a two-stage fine-tuning
    strategy to fine-tune the model and achieves better performance than contemporary
    meta-learning methods. In addition to the standard FSOD discussed above, other
    extensional settings like semi-supervised FSOD (Misra et al., [2015](#bib.bib85);
    Dong et al., [2019](#bib.bib19)), weakly-supervised FSOD (Gao et al., [2019](#bib.bib27);
    Karlinsky et al., [2021](#bib.bib49)) and incremental FSOD (Pérez-Rúa et al.,
    [2020](#bib.bib91); Li et al., [2021a](#bib.bib63)) are also explored by researchers
    and investigated in this survey.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，OSOL 设置并不够现实，因为在实际生活中测试图像中的物体类别并不是预先已知的。因此，更多的论文采用了另一种物体检测的少样本设置，称为少样本物体检测（FSOD）。FSOD
    和 OSOL 之间的主要区别如下：（1）FSOD 需要预测测试图像中潜在物体的正确类别。（2）OSOL 为每个测试图像独立采样支持图像，而 FSOD 只为所有测试图像采样一次支持图像。（3）在
    FSOD 中，每个类别的标注样本数量可以大于一个。类似于少样本图像分类的方法，FSOD 方法分为两种主流方法：元学习方法和迁移学习方法。早期 FSOD 方法主要采用元学习方案。元学习
    FSOD 方法的核心操作是提取少量标注样本的特征（支持特征），并将其汇总到查询图像的特征（查询特征）中，以指导查询图像的预测。这种汇总操作促进模型从少量标注样本中学习足够的信息。早期元学习
    FSOD 方法仅将支持特征与查询图像中的 RoIs 特征（RoI 特征）汇总在一起。后来，研究人员发现，整体特征的汇总对性能提升至关重要，因为模型中的浅层组件也需要标注样本的信息（例如，Faster
    R-CNN 中的 RPN 组件需要支持特征来筛选不匹配的区域建议）。因此，本调查将元学习 FSOD 方法分类为 RoI 特征汇总和混合特征汇总方法（“混合”指的是同时使用
    RoI 特征汇总和整体特征汇总）。与元学习方法不同，迁移学习 FSOD 方法直接在基础数据集上进行预训练，并在新数据集上进行微调。早期 FSOD 方法由于性能不佳，鲜少采用迁移学习。TFA（Wang
    等，[2020a](#bib.bib119)）颠覆了这种认知，提出了一种两阶段微调策略来微调模型，并且取得了比当代元学习方法更好的性能。除了上述标准 FSOD，研究人员还探讨了其他扩展设置，如半监督
    FSOD（Misra 等，[2015](#bib.bib85)；Dong 等，[2019](#bib.bib19)）、弱监督 FSOD（Gao 等，[2019](#bib.bib27)；Karlinsky
    等，[2021](#bib.bib49)）和增量 FSOD（Pérez-Rúa 等，[2020](#bib.bib91)；Li 等，[2021a](#bib.bib63)），并在本调查中进行了研究。
- en: ZSOD assigns abundant labeled samples to base classes, but it assigns no annotated
    image samples to novel classes. Instead, mainstream ZSOD allocates semantic attributes
    to each class (including base and novel classes), and it classifies object proposals
    according to their semantic similarities with different classes. Mainstream ZSOD
    methods include visual-semantic mapping methods, semantic relation methods, and
    data augmentation methods. Most early ZSOD methods belong to visual-semantic mapping
    methods. These methods aim to learn a visual-semantic function using the annotated
    samples of the base dataset, which projects visual features into semantic embeddings
    for comparison with class semantic attributes. Next, semantic relation methods
    utilize the semantic relation between different classes to make predictions. Moreover,
    data augmentation methods attempt to generate visual samples for novel classes
    and re-train the model. Besides the mainstream ZSOD setting described above, this
    survey discusses some rarely explored settings like transductive ZSOD and textual-description-based
    inductive ZSOD. Recently, with the emergence of large-scale cross-modal models (e.g.,
    CLIP (Radford et al., [2021](#bib.bib94))), Open-Vocabulary Object Detection (OVD)
    attracts more and more research interest, which first trains a stronger visual-semantic
    mapping function for multiple classes and significantly improves the performance
    of the further ZSOD task.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ZSOD将大量标记样本分配给基础类，但没有将注释图像样本分配给新颖类。相反，主流的ZSOD方法将语义属性分配给每个类别（包括基础类和新颖类），并根据与不同类别的语义相似性对对象提议进行分类。主流ZSOD方法包括视觉-语义映射方法、语义关系方法和数据增强方法。早期的大多数ZSOD方法属于视觉-语义映射方法。这些方法旨在利用基础数据集的注释样本学习视觉-语义函数，将视觉特征投影到语义嵌入中，以便与类别语义属性进行比较。接下来，语义关系方法利用不同类别之间的语义关系进行预测。此外，数据增强方法试图为新颖类生成视觉样本并重新训练模型。除了上述主流ZSOD设置，这项调查还讨论了一些少见的设置，如传导性ZSOD和基于文本描述的归纳ZSOD。最近，随着大规模跨模态模型的出现（例如，CLIP（Radford等，[2021](#bib.bib94)）），开放词汇对象检测（OVD）吸引了越来越多的研究兴趣，该方法首先训练了一个更强大的视觉-语义映射函数用于多个类别，并显著提高了进一步ZSOD任务的性能。
- en: The overview of this survey is illustrated in [Figure 1](#S1.F1 "Figure 1 ‣
    1\. Introduction ‣ A Survey of Deep Learning for Low-Shot Object Detection").
    The preliminaries for meta-learning and transfer-learning are given in [section 2](#S2
    "2\. Preliminaries ‣ A Survey of Deep Learning for Low-Shot Object Detection").
    The more fine-grained categorization and analysis of methods for LSOD are described
    in [section 3](#S3 "3\. One-Shot Object Localization ‣ A Survey of Deep Learning
    for Low-Shot Object Detection"), [section 4](#S4 "4\. Standard Few-Shot Object
    Detection ‣ A Survey of Deep Learning for Low-Shot Object Detection"), [section 5](#S5
    "5\. Zero-Shot Object Detection ‣ A Survey of Deep Learning for Low-Shot Object
    Detection"), [section 6](#S6 "6\. Extensional Zero-Shot Object Detection ‣ A Survey
    of Deep Learning for Low-Shot Object Detection"). The two popular datasets (MS
    COCO dataset (Lin et al., [2014](#bib.bib73)) and PASCAL VOC dataset (Everingham
    et al., [2010](#bib.bib21))) and evaluation criteria of LSOD are described in
    [section 7](#S7 "7\. Popular Benchmarks For Low-Shot Object Detection ‣ A Survey
    of Deep Learning for Low-Shot Object Detection"). The performance of current LSOD
    methods is summarized in [section 8](#S8 "8\. Performance ‣ A Survey of Deep Learning
    for Low-Shot Object Detection"). The promising directions LSOD are discussed in
    [section 9](#S9 "9\. Promising Directions ‣ A Survey of Deep Learning for Low-Shot
    Object Detection"). Finally, [section 10](#S10 "10\. Conclusion ‣ A Survey of
    Deep Learning for Low-Shot Object Detection") concludes the contents of this survey.
    The key notations used in this survey are summarized in [Table 1](#S1.T1 "Table
    1 ‣ 1\. Introduction ‣ A Survey of Deep Learning for Low-Shot Object Detection").
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的概述在[图 1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey of Deep Learning
    for Low-Shot Object Detection")中进行了说明。元学习和迁移学习的基础知识见[第 2 节](#S2 "2\. Preliminaries
    ‣ A Survey of Deep Learning for Low-Shot Object Detection")。对 LSOD 方法的更细致的分类和分析描述在[第
    3 节](#S3 "3\. One-Shot Object Localization ‣ A Survey of Deep Learning for Low-Shot
    Object Detection")、[第 4 节](#S4 "4\. Standard Few-Shot Object Detection ‣ A Survey
    of Deep Learning for Low-Shot Object Detection")、[第 5 节](#S5 "5\. Zero-Shot Object
    Detection ‣ A Survey of Deep Learning for Low-Shot Object Detection")、[第 6 节](#S6
    "6\. Extensional Zero-Shot Object Detection ‣ A Survey of Deep Learning for Low-Shot
    Object Detection")中进行了描述。两个流行的数据集（MS COCO 数据集 (Lin et al., [2014](#bib.bib73))
    和 PASCAL VOC 数据集 (Everingham et al., [2010](#bib.bib21))) 和 LSOD 的评估标准在[第 7 节](#S7
    "7\. Popular Benchmarks For Low-Shot Object Detection ‣ A Survey of Deep Learning
    for Low-Shot Object Detection")中进行了描述。当前 LSOD 方法的性能总结在[第 8 节](#S8 "8\. Performance
    ‣ A Survey of Deep Learning for Low-Shot Object Detection")中。LSOD 的有前景方向在[第 9
    节](#S9 "9\. Promising Directions ‣ A Survey of Deep Learning for Low-Shot Object
    Detection")中进行了讨论。最后，[第 10 节](#S10 "10\. Conclusion ‣ A Survey of Deep Learning
    for Low-Shot Object Detection")总结了本调查的内容。本调查中使用的主要符号总结在[表 1](#S1.T1 "Table 1 ‣
    1\. Introduction ‣ A Survey of Deep Learning for Low-Shot Object Detection")中。
- en: 2\. Preliminaries
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 基础
- en: '![Refer to caption](img/1fc0b38e4564636af5926228383fddca.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1fc0b38e4564636af5926228383fddca.png)'
- en: Figure 2\. Illustration of meta-learning and transfer-learning in LSOD. LSOD
    divides the object classes into base classes with many annotated samples (denoted
    as base dataset) and novel classes with a few annotated samples (denoted as novel
    dataset). Meta-learning samples multiple tasks from the base dataset and trains
    the model on these tasks (each task requires to make predictions on $D_{i}^{\mathrm{query}}$
    according to the annotated $D_{i}^{\mathrm{support}}$), aiming to acquire the
    knowledge about “how to learn” and generalize it to the novel dataset. On the
    other hand, transfer-learning directly trains the model on the base dataset and
    transfers a good feature representation to the novel dataset, enabling the representation
    of objects from novel classes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. LSOD 中的元学习与迁移学习示意图。LSOD 将对象类别分为带有大量标注样本的基础类别（记作基础数据集）和带有少量标注样本的新的类别（记作新颖数据集）。元学习从基础数据集中采样多个任务，并在这些任务上训练模型（每个任务要求根据标注的
    $D_{i}^{\mathrm{support}}$ 对 $D_{i}^{\mathrm{query}}$ 进行预测），旨在获取“如何学习”的知识，并将其推广到新颖数据集上。另一方面，迁移学习直接在基础数据集上训练模型，并将良好的特征表示迁移到新颖数据集上，从而实现对新类别对象的表示。
- en: 2.1\. Meta-Learning
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 元学习
- en: Meta-learning is a “learning-to-learn” (Thrun and Pratt, [1998](#bib.bib109);
    Hospedales et al., [2022](#bib.bib42)) paradigm extended from the conventional
    “learning” paradigm. Conventional learning paradigm directly trains the model
    from scratch on the whole dataset as a single task. Differently, meta-learning
    learns the training pattern (e.g., parameter initialization) from multiple tasks,
    which is capable of generalizing across different tasks and facilitating the learning
    of new tasks. Therefore, meta-learning is suitable for quick adaptation of the
    model to the new tasks in few-shot learning. The framework of meta-learning is
    shown in [Figure 2](#S2.F2 "Figure 2 ‣ 2\. Preliminaries ‣ A Survey of Deep Learning
    for Low-Shot Object Detection") (a), and a more detailed illustration is in section
    S1 of the supplementary online-only material.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习是从传统的“学习”范式扩展出的“学习如何学习”（Thrun and Pratt, [1998](#bib.bib109); Hospedales
    et al., [2022](#bib.bib42)）范式。传统学习范式直接在整个数据集上从头开始训练模型作为单一任务。不同的是，元学习从多个任务中学习训练模式（例如参数初始化），能够跨任务泛化并促进新任务的学习。因此，元学习适用于在少样本学习中快速适应新任务。元学习的框架如[图
    2](#S2.F2 "Figure 2 ‣ 2\. Preliminaries ‣ A Survey of Deep Learning for Low-Shot
    Object Detection")（a）所示，更详细的说明见补充在线材料的 S1 节。
- en: 2.2\. Transfer-Learning
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 转移学习
- en: Transfer-learning methods aim to transfer the knowledge (good feature representation)
    from a related domain (named source domain) to the current domain (named target
    domain), in order to improve the performance of model on the target domain, as
    shown in [Figure 2](#S2.F2 "Figure 2 ‣ 2\. Preliminaries ‣ A Survey of Deep Learning
    for Low-Shot Object Detection") (b). Traditional transfer-learning approaches
    include instance-based methods, feature-based methods, parameter-based methods,
    and relational-based methods (Zhuang et al., [2021](#bib.bib162)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习方法旨在将来自相关领域（称为源领域）的知识（良好的特征表示）转移到当前领域（称为目标领域），以提高模型在目标领域的性能，如[图 2](#S2.F2
    "Figure 2 ‣ 2\. Preliminaries ‣ A Survey of Deep Learning for Low-Shot Object
    Detection")（b）所示。传统的转移学习方法包括基于实例的方法、基于特征的方法、基于参数的方法和基于关系的方法（Zhuang et al., [2021](#bib.bib162)）。
- en: For the transfer-learning methods in few-shot learning, the base dataset is
    viewed as the source domain, and the novel dataset is viewed as the target dataset.
    Tian et al. (Tian et al., [2020](#bib.bib110)) find that simply transferring a
    strong feature extractor from the base dataset to the novel dataset outperforms
    many meta-learning methods on few-shot image classification, and many FSL methods
    follow this paradigm. Transfer-learning is not suitable for OSOL since the target
    domain consists of only one image for each task, yet it is widely adopted in FSOD
    after the emergence of TFA (Wang et al., [2020a](#bib.bib119)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于少样本学习中的转移学习方法，基础数据集被视为源领域，新的数据集被视为目标数据集。Tian 等人（Tian et al., [2020](#bib.bib110)）发现，将强大的特征提取器从基础数据集转移到新数据集，能在少样本图像分类中优于许多元学习方法，许多
    FSL 方法遵循这一范式。由于 OSOL 的目标领域每个任务仅包含一张图像，因此转移学习不适用于 OSOL，但在 TFA（Wang et al., [2020a](#bib.bib119)）出现后，它被广泛应用于
    FSOD。
- en: 3\. One-Shot Object Localization
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 单样本目标定位
- en: Task Settings. One-Shot Object Localization (OSOL) needs to locate objects in
    a query image according to only one support image for each novel class existing
    in this query image. The training dataset (base dataset $D_{B}$) of OSOL comprises
    abundant annotated instances of base classes $C_{B}$, and the test dataset (novel
    dataset $D_{N}$) comprises instances of novel classes $C_{N}$ ($C_{B}$ and $C_{N}$
    are not intersected). Specifically, for each query image in $D_{N}$, OSOL randomly
    samples a support image for each novel class existing in the image. Next, OSOL
    locates the novel objects in the query image according to the corresponding support
    image. The main difference from FSOD is that OSOL only requires a binary classification
    task to discriminate whether the potential object is foreground or background
    according to the given support image, while FSOD requires a multi-class classification
    task because FSOD doesn’t pre-know the classes of the existing objects in the
    query images.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 任务设置。一次性目标定位（OSOL）需要根据每个新颖类别在查询图像中的唯一支持图像来定位目标。OSOL的训练数据集（基础数据集$D_{B}$）包含大量标注的基础类$C_{B}$实例，测试数据集（新颖数据集$D_{N}$）包含新颖类$C_{N}$的实例（$C_{B}$和$C_{N}$没有交集）。具体来说，对于$D_{N}$中的每个查询图像，OSOL随机抽取一个支持图像，用于图像中每个新颖类。接下来，OSOL根据相应的支持图像在查询图像中定位新颖目标。与FSOD的主要区别在于，OSOL只需要进行二分类任务，以区分潜在目标是前景还是背景，而FSOD需要进行多分类任务，因为FSOD无法预先知道查询图像中现有对象的类别。
- en: Framework of Current OSOL Methods. Some previous object tracking methods like
    SiamFC (Cen and Jung, [2018](#bib.bib10)), SiamRPN (Li et al., [2018](#bib.bib60))
    are forerunners of OSOL, which are used for comparison with early OSOL methods.
    Current OSOL methods adopt the meta-learning scheme, and their framework is based
    on Faster R-CNN, as shown in [Figure 3](#S3.F3 "Figure 3 ‣ 3\. One-Shot Object
    Localization ‣ A Survey of Deep Learning for Low-Shot Object Detection"). First,
    they extract the integral features of the query image and the support image using
    the same convolutional backbone (named query features and support features, respectively),
    then conduct “integral feature aggregation” to generate a fused feature map by
    aggregating the query features with the support features. This fused feature map
    is fed into RPN and RoI layer to generate category-specific region proposals and
    the corresponding RoI features, respectively. Finally, these RoI features are
    used for the final classification and localization tasks. Furthermore, some methods
    additionally conduct “RoI feature aggregation” to further aggregate the RoI features
    with the support features.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当前OSOL方法的框架。一些早期的目标跟踪方法，如SiamFC （Cen和Jung，[2018](#bib.bib10)）、SiamRPN（Li等，[2018](#bib.bib60)）是OSOL的前驱，用于与早期OSOL方法进行比较。当前的OSOL方法采用元学习方案，其框架基于Faster
    R-CNN，如[图3](#S3.F3 "Figure 3 ‣ 3\. One-Shot Object Localization ‣ A Survey of
    Deep Learning for Low-Shot Object Detection")所示。首先，它们使用相同的卷积骨干网络提取查询图像和支持图像的整体特征（分别称为查询特征和支持特征），然后进行“整体特征聚合”，通过将查询特征与支持特征进行聚合生成融合特征图。该融合特征图输入RPN和RoI层，以生成类别特定的区域提案及相应的RoI特征。最后，这些RoI特征用于最终的分类和定位任务。此外，一些方法还额外进行“RoI特征聚合”，以进一步将RoI特征与支持特征进行聚合。
- en: Current OSOL methods mainly differ in the feature aggregation method, and this
    survey accordingly categorizes OSOL methods into concatenation-based methods,
    attention-based methods, and transformation-based methods. In the following sections,
    $\phi_{q}\in\mathbb{R}^{C\times H_{q}\times H_{q}}$, $\phi_{r}\in\mathbb{R}^{C\times
    H_{r}\times H_{r}}$ and $\phi_{s}\in\mathbb{R}^{C\times H_{s}\times H_{s}}$ denote
    the query feature map, the RoI feature map and the support feature map, respectively.
    Note that $C$, $H_{q}$, $H_{r}$, and $H_{s}$ are the channel and sizes of the
    feature maps.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的OSOL方法主要在特征聚合方法上有所不同，因此本文对OSOL方法进行了分类，包括基于串联的方法、基于注意力的方法和基于变换的方法。在接下来的部分中，$\phi_{q}\in\mathbb{R}^{C\times
    H_{q}\times H_{q}}$、$\phi_{r}\in\mathbb{R}^{C\times H_{r}\times H_{r}}$和$\phi_{s}\in\mathbb{R}^{C\times
    H_{s}\times H_{s}}$分别表示查询特征图、RoI特征图和支持特征图。注意，$C$、$H_{q}$、$H_{r}$和$H_{s}$是特征图的通道数和尺寸。
- en: '![Refer to caption](img/d968474f7c4e69d69df1bc73d350e00b.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d968474f7c4e69d69df1bc73d350e00b.png)'
- en: Figure 3\. The overall framework of One-Shot Object Localization (based on Faster
    R-CNN). The model takes a query image and a support image as inputs, then uses
    a siamese convolutional feature extractor to extract the query feature map and
    the support feature map. Then it applies integral feature aggregation to aggregate
    these two feature maps into a fused feature map and forwards it into RPN and RoI
    layer to generate region proposals and RoI features, respectively. The aggregation
    method is implemented differently in different OSOL methods. Finally, the RoI
    features are used for the classification task and the regression task. Some methods
    additionally apply RoI feature aggregation to aggregate the RoI features with
    the support features.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. One-Shot 物体定位的总体框架（基于 Faster R-CNN）。该模型将查询图像和支持图像作为输入，然后使用一个孪生卷积特征提取器提取查询特征图和支持特征图。接着，它应用积分特征聚合将这两个特征图聚合成一个融合特征图，并将其送入
    RPN 和 RoI 层，分别生成区域提议和 RoI 特征。聚合方法在不同的 OSOL 方法中实现有所不同。最后，RoI 特征用于分类任务和回归任务。一些方法还额外应用
    RoI 特征聚合，将 RoI 特征与支持特征进行聚合。
- en: 3.1\. Concatenation-Based Methods
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 基于拼接的方法
- en: Concatenation-based methods simply adopt the concatenation operation to aggregate
    $\phi_{q}$ and $\phi_{s}$, which are mainly adopted by early OSOL methods (SiamMask (Michaelis
    et al., [2018](#bib.bib83)), OSCD (Fu et al., [2021](#bib.bib25)), FOC OSOL (Yang
    et al., [2021a](#bib.bib133)) and OSOLwT (Li et al., [2020c](#bib.bib66))), as
    shown in [Figure 4](#S3.F4 "Figure 4 ‣ 3.2\. Attention-Based Methods ‣ 3\. One-Shot
    Object Localization ‣ A Survey of Deep Learning for Low-Shot Object Detection").
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基于拼接的方法简单地采用拼接操作来聚合 $\phi_{q}$ 和 $\phi_{s}$，这些方法主要被早期的 OSOL 方法所采用（SiamMask（Michaelis
    等， [2018](#bib.bib83)），OSCD（Fu 等， [2021](#bib.bib25)），FOC OSOL（Yang 等， [2021a](#bib.bib133)）和
    OSOLwT（Li 等， [2020c](#bib.bib66)）），如 [图 4](#S3.F4 "图 4 ‣ 3.2\. 基于注意力的方法 ‣ 3\.
    One-Shot 物体定位 ‣ 深度学习在少样本物体检测中的应用概述") 所示。
- en: $\bullet$ SiamMask (Michaelis et al., [2018](#bib.bib83)). SiamMask is one of
    the early deep-learning-based methods for OSOL, which concatenates $\phi_{q}$
    with the absolute difference between $\phi_{q}$ and the pooled embedding vector
    $v_{s}\in\mathrm{R}^{C}$ of $\phi_{s}$ to generate the aggregated feature map
    $\phi_{\rm fused}\in\mathbb{R}^{2C\times H_{q}\times H_{q}}$, as shown in [Equation 1](#S3.E1
    "1 ‣ 3.1\. Concatenation-Based Methods ‣ 3\. One-Shot Object Localization ‣ A
    Survey of Deep Learning for Low-Shot Object Detection"). In SiamMask, $\phi_{\rm
    fused}$ is directly used for further components (RPN, RoI layer) in Faster R-CNN
    without other modifications. SiamMask does not achieve satisfying performance
    since it tackles a segmentation task simultaneously. Nevertheless, as the first
    method for OSOL, SiamMask proposes a benchmark based on MS COCO dataset for performance
    comparison, pioneering many future works on OSOL and establishing a baseline for
    future work.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ SiamMask（Michaelis 等， [2018](#bib.bib83)）。SiamMask 是早期基于深度学习的 OSOL
    方法之一，它将 $\phi_{q}$ 与 $\phi_{q}$ 和池化嵌入向量 $v_{s}\in\mathrm{R}^{C}$ 的绝对差异拼接，以生成聚合特征图
    $\phi_{\rm fused}\in\mathbb{R}^{2C\times H_{q}\times H_{q}}$，如 [方程 1](#S3.E1 "1
    ‣ 3.1\. 基于拼接的方法 ‣ 3\. One-Shot 物体定位 ‣ 深度学习在少样本物体检测中的应用概述") 所示。在 SiamMask 中，$\phi_{\rm
    fused}$ 直接用于 Faster R-CNN 中的进一步组件（RPN、RoI 层），没有其他修改。由于它同时处理了分割任务，SiamMask 没有实现令人满意的性能。然而，作为
    OSOL 的首个方法，SiamMask 基于 MS COCO 数据集提出了一个性能比较的基准，为未来的 OSOL 研究开创了许多工作，并为未来的工作奠定了基线。
- en: '| (1) |  | $\phi_{\rm fused}=[\phi_{q},&#124;\phi_{q}-{\rm Pool}(\phi_{s})&#124;]\text{.}$
    |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\phi_{\rm fused}=[\phi_{q},\;|\phi_{q}-{\rm Pool}(\phi_{s})\;|]\text{.}$
    |  |'
- en: $\bullet$ OSCD (Fu et al., [2021](#bib.bib25)). Different from SiamMask, OSCD
    directly concatenates $\phi_{q}\in\mathbb{R}^{C\times H_{q}\times H_{q}}$ with
    the pooled embedding vector $v_{s}$ of $\phi_{s}\in\mathbb{R}^{C\times H_{s}\times
    H_{s}}$ to generate $\phi_{\rm fused}\in\mathbb{R}^{2C\times H_{q}\times H_{q}}$,
    as shown in [Equation 2](#S3.E2 "2 ‣ 3.1\. Concatenation-Based Methods ‣ 3\. One-Shot
    Object Localization ‣ A Survey of Deep Learning for Low-Shot Object Detection").
    Besides, OSCD further conducts RoI feature aggregation to leverage the information
    of $\phi_{s}$ to facilitate the prediction of RoIs, which concatenates the RoI
    feature map $\phi_{r}$ and $\phi_{s}$ in depth. OSCD proposes another OSOL benchmark
    based on PASCAL VOC dataset for evaluation, and it outperforms SiamFC and SiamRPN
    by a large margin on this benchmark.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ OSCD（Fu 等人，[2021](#bib.bib25)）。与 SiamMask 不同，OSCD 直接将 $\phi_{q}\in\mathbb{R}^{C\times
    H_{q}\times H_{q}}$ 与 $\phi_{s}\in\mathbb{R}^{C\times H_{s}\times H_{s}}$ 的池化嵌入向量
    $v_{s}$ 进行连接，以生成 $\phi_{\rm fused}\in\mathbb{R}^{2C\times H_{q}\times H_{q}}$，如
    [公式 2](#S3.E2 "2 ‣ 3.1\. Concatenation-Based Methods ‣ 3\. One-Shot Object Localization
    ‣ A Survey of Deep Learning for Low-Shot Object Detection") 所示。此外，OSCD 还进一步进行
    RoI 特征聚合，以利用 $\phi_{s}$ 的信息来促进 RoI 的预测，这种方法在深度上连接了 RoI 特征图 $\phi_{r}$ 和 $\phi_{s}$。OSCD
    基于 PASCAL VOC 数据集提出了另一个 OSOL 基准进行评估，在该基准上，它比 SiamFC 和 SiamRPN 具有更大的优势。
- en: '| (2) |  | $\phi_{\rm fused}=[\phi_{q},{\rm Pool}(\phi_{s})]\text{.}$ |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\phi_{\rm fused}=[\phi_{q},{\rm Pool}(\phi_{s})]\text{.}$ |  |'
- en: $\bullet$ OSOLwT (Li et al., [2020c](#bib.bib66)) and FOC OSOL (Yang et al.,
    [2021a](#bib.bib133)). They add convolutional blocks into the concatenated features,
    which capture the relation between different feature units for performance improvement,
    as shown in [Equation 3](#S3.E3 "3 ‣ 3.1\. Concatenation-Based Methods ‣ 3\. One-Shot
    Object Localization ‣ A Survey of Deep Learning for Low-Shot Object Detection").
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ OSOLwT（Li 等人，[2020c](#bib.bib66)）和 FOC OSOL（Yang 等人，[2021a](#bib.bib133)）。它们将卷积块添加到连接的特征中，以捕捉不同特征单元之间的关系，从而提高性能，如
    [公式 3](#S3.E3 "3 ‣ 3.1\. Concatenation-Based Methods ‣ 3\. One-Shot Object Localization
    ‣ A Survey of Deep Learning for Low-Shot Object Detection") 所示。
- en: '| (3) |  | $\phi_{\rm fused}={\rm Conv}([\phi_{q},&#124;\phi_{q}-{\rm Pool}(\phi_{s})&#124;])\text{.}$
    |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\phi_{\rm fused}={\rm Conv}([\phi_{q},\,|\phi_{q}-{\rm Pool}(\phi_{s})|])\text{.}$
    |  |'
- en: $\bigstar$ Discussion of Concatenation-Based Methods. Concatenation-based methods
    are mainly adopted by early OSOL methods. SiamMask and OSCD are the earliest concatenation-based
    methods for feature aggregation, while FOC OSOL and OSOLwT extend SiamMask and
    OSCD with convolutional blocks and some other elaborated training strategies.
    However, the limitation of concatenation-based methods is that they simply aggregate
    features without fully excavating the relation between different local parts of
    two feature maps, thus impairing the matching between the foreground parts of
    query feature map with support feature map.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: $\bigstar$ 基于连接的方法讨论。基于连接的方法主要被早期的 OSOL 方法采用。SiamMask 和 OSCD 是最早的基于连接的特征聚合方法，而
    FOC OSOL 和 OSOLwT 则在 SiamMask 和 OSCD 的基础上扩展了卷积块和一些其他精细的训练策略。然而，基于连接的方法的局限性在于它们只是简单地聚合特征，而没有充分挖掘两个特征图的不同局部部分之间的关系，从而损害了查询特征图的前景部分与支持特征图之间的匹配。
- en: 3.2\. Attention-Based Methods
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 基于注意力的方法
- en: Attention-based methods take advantage of the correspondence between different
    parts of the support features and the query features, as shown in [Figure 5](#S3.F5
    "Figure 5 ‣ 3.2\. Attention-Based Methods ‣ 3\. One-Shot Object Localization ‣
    A Survey of Deep Learning for Low-Shot Object Detection").
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的方法利用了支持特征和查询特征之间不同部分的对应关系，如 [图 5](#S3.F5 "Figure 5 ‣ 3.2\. Attention-Based
    Methods ‣ 3\. One-Shot Object Localization ‣ A Survey of Deep Learning for Low-Shot
    Object Detection") 所示。
- en: '$\bullet$ CoAE (Hsieh et al., [2019](#bib.bib44)). CoAE is the first attention-based
    OSOL method, which proposes two operations for integral feature aggregation: co-attention (ca)
    operation and co-excitation (ce) operation. The co-attention operation is implemented
    using the non-local operation (Wang et al., [2018](#bib.bib118)) (an attention
    operation), which aggregates two feature maps according to their element-wise
    attention:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CoAE（Hsieh 等人，[2019](#bib.bib44)）。CoAE 是第一个基于注意力的 OSOL 方法，它提出了两种用于整体特征聚合的操作：共同注意（ca）操作和共同激发（ce）操作。共同注意操作使用非局部操作（Wang
    等人，[2018](#bib.bib118)）（一种注意力操作）来实现，根据其逐元素的注意力对两个特征图进行聚合：
- en: '| (4) |  | $\phi_{\rm fused}^{\mathrm{ca}}=\phi_{q}\oplus\psi(\phi_{q},\phi_{s})\text{,}$
    |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\phi_{\rm fused}^{\mathrm{ca}}=\phi_{q}\oplus\psi(\phi_{q},\phi_{s})\text{,}$
    |  |'
- en: 'where $\psi$ denotes the non-local operation and $\phi_{\rm fused}\in\mathbb{R}^{C\times
    H_{q}\times H_{q}}$. The co-excitation operation generates $\phi_{\rm fused}^{\mathrm{ce}}\in\mathbb{R}^{C\times
    H_{q}\times H_{q}}$ by aggregating $\phi_{q}$ with the pooled embedding vector
    $v_{s}\in\mathbb{R}^{C}$ of $\phi_{s}$ with a channel-wise multiplication:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\psi$ 表示非本地操作，$\phi_{\rm fused}\in\mathbb{R}^{C\times H_{q}\times H_{q}}$。共同激励操作通过使用通道级乘法将
    $\phi_{q}$ 与 $\phi_{s}$ 的池化嵌入向量 $v_{s}\in\mathbb{R}^{C}$ 聚合生成 $\phi_{\rm fused}^{\mathrm{ce}}\in\mathbb{R}^{C\times
    H_{q}\times H_{q}}$。
- en: '| (5) |  | $\phi_{\rm fused}^{\mathrm{ce}}=\phi_{q}\otimes{\rm Pool}(\phi_{s})\text{.}$
    |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\phi_{\rm fused}^{\mathrm{ce}}=\phi_{q}\otimes{\rm Pool}(\phi_{s})\text{.}$
    |  |'
- en: CoAE adopts both these two operations for integral feature aggregation. Besides,
    CoAE proposes a proposal ranking loss to supervise RPN based on RoI feature aggregation.
    CoAE outperforms SiamMask on the MS COCO benchmark and OSCD on the PASCAL VOC
    benchmark, demonstrating the capacity of the attention mechanism on OSOL.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: CoAE 采用这两种操作进行积分特征聚合。此外，CoAE 提出了一个基于 RoI 特征聚合的提案排名损失来监督 RPN。CoAE 在 MS COCO 基准测试中优于
    SiamMask，在 PASCAL VOC 基准测试中优于 OSCD，展示了注意力机制在 OSOL 上的能力。
- en: '![Refer to caption](img/c8ff337fec3c6b266ada832fcb7f58e3.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c8ff337fec3c6b266ada832fcb7f58e3.png)'
- en: (a) SiamMask (Michaelis et al., [2018](#bib.bib83))
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: (a) SiamMask (Michaelis et al., [2018](#bib.bib83))
- en: '![Refer to caption](img/9b353365b077c47b24367105865110d6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9b353365b077c47b24367105865110d6.png)'
- en: (b) OSCD (Fu et al., [2021](#bib.bib25)) OSOLwT (Li et al., [2020c](#bib.bib66))
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (b) OSCD (Fu et al., [2021](#bib.bib25)) OSOLwT (Li et al., [2020c](#bib.bib66))
- en: '![Refer to caption](img/4420e43888b9725c1df865b8bbe0f09c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4420e43888b9725c1df865b8bbe0f09c.png)'
- en: (c) FOC OSOL (Yang et al., [2021a](#bib.bib133))
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (c) FOC OSOL (Yang et al., [2021a](#bib.bib133))
- en: Figure 4\. Overview of concatenation-based integral feature aggregation methods
    in OSOL. OSCD and OSOLwT concatenate the query feature map with the pooled embedding
    vector of the support feature map. SiamMask concatenates the query feature map
    with the absolute difference between the query feature map and the pooled embedding
    vector of the support feature map instead. FOC OSOL additionally applies convolution
    blocks on the integral feature map generated in SiamMask.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. OSOL 中基于串联的积分特征聚合方法概览。OSCD 和 OSOLwT 将查询特征图与支持特征图的池化嵌入向量进行串联。SiamMask 则将查询特征图与查询特征图和支持特征图的池化嵌入向量之间的绝对差异进行串联。FOC
    OSOL 额外在 SiamMask 生成的积分特征图上应用卷积块。
- en: $\bullet$ BHRL (Yang et al., [2022](#bib.bib132)), ABA OSOL (Hsieh et al., [2023](#bib.bib43)),
    ADA OSOL (Zhang et al., [2022a](#bib.bib148)), and AUG OSOL (Du et al., [2022](#bib.bib20)).
    These later methods follow the co-attention and co-excitation operations in CoAE
    with some elaborated modifications.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ BHRL (Yang et al., [2022](#bib.bib132)), ABA OSOL (Hsieh et al., [2023](#bib.bib43)),
    ADA OSOL (Zhang et al., [2022a](#bib.bib148)), 和 AUG OSOL (Du et al., [2022](#bib.bib20))。这些后续方法在
    CoAE 的共同注意力和共同激励操作的基础上进行了一些精细修改。
- en: $\bullet$ AIT (Chen et al., [2021a](#bib.bib11)), CAT (Lin et al., [2021](#bib.bib74)),
    SaFT (Zhao et al., [2022a](#bib.bib153)). With the wide usage of transformers (Subakan
    et al., [2021](#bib.bib105)) in computer vision, some methods (AIT, CAT, SaFT)
    adopt multi-head attention into OSOL for feature aggregation. These methods flatten
    the query feature map $\phi_{q}$ and the support feature map $\phi_{s}$ to be
    feature sequences $\phi_{q}^{{}^{\prime}}\in\mathbb{R}^{C\times H_{q}H_{q}}$ and
    $\phi_{s}^{{}^{\prime}}\in\mathbb{R}^{C\times H_{s}H_{s}}$, then generates $\phi_{\rm
    fused}\in\mathbb{R}^{C\times H_{q}\times H_{q}}$ using multi-head attention to
    capture bidirectional correspondence between grids of them.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ AIT (Chen et al., [2021a](#bib.bib11)), CAT (Lin et al., [2021](#bib.bib74)),
    SaFT (Zhao et al., [2022a](#bib.bib153))。随着变换器 (Subakan et al., [2021](#bib.bib105))
    在计算机视觉中的广泛应用，一些方法 (AIT, CAT, SaFT) 将多头注意力引入 OSOL 进行特征聚合。这些方法将查询特征图 $\phi_{q}$
    和支持特征图 $\phi_{s}$ 扁平化为特征序列 $\phi_{q}^{{}^{\prime}}\in\mathbb{R}^{C\times H_{q}H_{q}}$
    和 $\phi_{s}^{{}^{\prime}}\in\mathbb{R}^{C\times H_{s}H_{s}}$，然后使用多头注意力生成 $\phi_{\rm
    fused}\in\mathbb{R}^{C\times H_{q}\times H_{q}}$ 以捕捉它们网格之间的双向对应关系。
- en: $\bigstar$ Discussion of Attention-Based Methods. Compared to attention-based
    methods with transformer, attention-based methods with co-attention require fewer
    extra parameters and less computation cost. However, CoAE is an early OSOL method,
    and the simple non-local operation is not enough for feature aggregation of current
    OSOL methods. Actually, recent methods of this type integrate co-attention with
    other elaborated operations to improve their performance. On the other hand, methods
    based on transformer significantly improve performance, and they can easily integrate
    other elaborated variants of transformer structure into this framework for further
    performance improvement. However, current transformer-based methods bring too
    much extra computation cost into model training. Therefore, the efficient transformer
    structure is expected to be adopted for the trade-off between performance and
    computation cost.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: $\bigstar$ 对基于注意力的方法的讨论。与基于变换器的注意力方法相比，基于共同注意力的注意力方法需要更少的额外参数和较低的计算成本。然而，CoAE
    是一种早期的 OSOL 方法，简单的非局部操作不足以满足当前 OSOL 方法的特征聚合需求。实际上，这类方法的最新研究将共同注意力与其他精细操作相结合，以提高其性能。另一方面，基于变换器的方法显著提高了性能，并且可以轻松地将其他精细变换器结构变体集成到此框架中，以进一步提升性能。然而，目前基于变换器的方法在模型训练中带来了过多的额外计算成本。因此，期望采用高效的变换器结构，以在性能和计算成本之间进行权衡。
- en: '![Refer to caption](img/c25a303f5340fecb9d2d31137267a167.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c25a303f5340fecb9d2d31137267a167.png)'
- en: (a) CoAE, BHRL, ABA OSOL, ADA OSOL, AUG OSOL
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (a) CoAE、BHRL、ABA OSOL、ADA OSOL、AUG OSOL
- en: '![Refer to caption](img/78b1a0363ff774cc1006743e861a2675.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/78b1a0363ff774cc1006743e861a2675.png)'
- en: (b) AIT, CAT, SaFT
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (b) AIT、CAT、SaFT
- en: Figure 5\. Overview of attention-based integral feature aggregation methods
    in OSOL. Some methods (CoAE, BHRL, ABA OSOL, ADA OSOL, AUG OSOL) use a non-local
    operation for integral feature aggregation, while some methods (AIT, CAT, SaFT)
    use transformer to capture attention between query and support images.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. OSOL 中基于注意力的整体特征聚合方法概述。一些方法 (CoAE、BHRL、ABA OSOL、ADA OSOL、AUG OSOL) 使用非局部操作进行整体特征聚合，而一些方法
    (AIT、CAT、SaFT) 使用变换器来捕捉查询和支持图像之间的注意力。
- en: 3.3\. Transformation-Based Methods
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. 基于变换的方法
- en: OS2D (Osokin et al., [2020](#bib.bib87)) proposes a transformation-based method
    for feature aggregation, which conducts feature map transformation to match query
    feature map and support feature map. Given the query feature map $\phi_{q}$ and
    the support feature map $\phi_{s}$, OS2D first computes a 4D correlation matrix
    of shape $\mathbb{R}^{H_{q}\times H_{q}\times H_{s}\times H_{s}}$ which represents
    the correspondence between all pairs of locations from these two feature maps.
    Then, it uses a pre-trained TransformNet (Rocco et al., [2018](#bib.bib101)) to
    generate a transformation matrix that spatially aligns the support feature map
    with the query feature map. Finally, the classification score of each location
    of the query feature map is obtained from the combination of the correlation matrix
    and the transformation matrix.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: OS2D (Osokin et al., [2020](#bib.bib87)) 提出了基于变换的方法来进行特征聚合，它通过对特征图进行变换，以匹配查询特征图和支持特征图。给定查询特征图
    $\phi_{q}$ 和支持特征图 $\phi_{s}$，OS2D 首先计算一个形状为 $\mathbb{R}^{H_{q}\times H_{q}\times
    H_{s}\times H_{s}}$ 的 4D 相关矩阵，该矩阵表示这两个特征图中所有位置对之间的对应关系。然后，它使用预训练的 TransformNet
    (Rocco et al., [2018](#bib.bib101)) 生成一个变换矩阵，将支持特征图在空间上对齐到查询特征图上。最后，查询特征图中每个位置的分类分数是通过相关矩阵和变换矩阵的组合得到的。
- en: $\bigstar$ Discussion of OSOL Methods. To sum up, concatenation-based methods
    are easy to implement, and they require smaller computation cost, but they have
    poorer performance. Attention-based methods can capture the correspondence between
    support images and the foreground of query images, thus outperforming concatenation-based
    methods. The weakness of attention-based methods is that it is more complicated
    to implement them, and they require larger computation cost. Transformation-based
    methods make the decision process of OSOL more interpretable, but they require
    a large pre-trained model to capture the spatial correspondence between query
    and support images.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: $\bigstar$ 对 OSOL 方法的讨论。总之，基于连接的方法易于实现，计算成本较低，但性能较差。基于注意力的方法可以捕捉支持图像和查询图像前景之间的对应关系，从而超越基于连接的方法。基于注意力的方法的缺点是实现更复杂，并且需要较高的计算成本。基于变换的方法使
    OSOL 的决策过程更具解释性，但它们需要一个大型预训练模型来捕捉查询图像和支持图像之间的空间对应关系。
- en: 4\. Standard Few-Shot Object Detection
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 标准少样本目标检测
- en: 'Task Settings. The previous OSOL setting guarantees that every query image
    contains objects with the same category as the given support image, i.e., the
    model knows precisely the object classes contained in each test image. However,
    this setting is not realistic in the real world, and a more challenging LSOD setting,
    named Few-Shot Object Detection (FSOD), is adopted by more papers. This section
    first introduces the standard FSOD, and other FSOD settings (named extensional
    FSOD) are based on the standard FSOD, which will be analyzed in the later sections.
    Specifically, the base dataset ($D_{B}$) of standard FSOD consists of abundant
    annotated instances of base classes $C_{B}$, and the novel dataset ($D_{N}$) consists
    of scarce annotated instances of novel classes $C_{N}$ ($C_{B}$ and $C_{N}$ are
    not intersected). During testing, the model is evaluated on the test dataset comprising
    objects of both base classes and novel classes. The differences between FSOD and
    OSOL are as below:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 任务设置。之前的 OSOL 设置保证每个查询图像都包含与给定支持图像相同类别的物体，即模型准确知道每个测试图像中包含的物体类别。然而，这种设置在现实世界中并不现实，更多的论文采用了一种更具挑战性的
    LSOD 设置，称为小样本物体检测 (FSOD)。本节首先介绍标准 FSOD，其他 FSOD 设置（称为扩展 FSOD）基于标准 FSOD，将在后续部分进行分析。具体而言，标准
    FSOD 的基础数据集 ($D_{B}$) 包含大量标注的基础类别实例 $C_{B}$，而新数据集 ($D_{N}$) 包含稀少标注的新类别实例 $C_{N}$
    ($C_{B}$ 和 $C_{N}$ 不重叠)。在测试期间，模型在包括基础类别和新类别物体的测试数据集上进行评估。FSOD 和 OSOL 之间的区别如下：
- en: (1)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Since OSOL precisely knows the object categories contained in each test image,
    it only requires a binary classification task to discriminate whether the potential
    object is foreground or background according to the given support image. In contrast,
    FSOD requires a multi-class classification task to predict the category of the
    potential object.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于 OSOL 准确知道每个测试图像中包含的物体类别，因此只需一个二分类任务来根据给定的支持图像区分潜在的物体是前景还是背景。相比之下，FSOD 需要一个多分类任务来预测潜在物体的类别。
- en: (2)
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: OSOL samples support images independently for each test image, FSOD samples
    the support images only once for all test images.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: OSOL 为每个测试图像独立采样支持图像，而 FSOD 只为所有测试图像一次性采样支持图像。
- en: (3)
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: The shot number of support images per category can be larger than one in FSOD.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个类别的支持图像的拍摄数量在 FSOD 中可以大于一个。
- en: Method Categorization. Current standard FSOD methods can be categorized into
    fine-tune-based methods and fine-tune-free methods. Most methods are fine-tune-based
    methods, which require to fine-tune the model on the novel dataset for the significant
    improvement of performance. Early fine-tune-based methods adopt the scheme of
    meta-learning, and they also concentrate on the methods of feature aggregation
    as OSOL methods. Furthermore, the increased number of annotated samples opens
    up the possibility for standard FSOD methods to adopt the scheme of transfer-learning,
    which pre-trains an object detector on the base dataset and fine-tunes this pre-trained
    model for novel classes on the novel dataset. Early transfer-learning methods
    like LSTD (Chen et al., [2018](#bib.bib12)) are outperformed by the meta-learning
    methods in that period until the emergence of TFA (Wang et al., [2020a](#bib.bib119)).
    On the other hand, fine-tune-free methods aim to remove the fine-tuning step because
    fine-tuning step is not suitable for FSOD in real life for its nonnegligible computation
    cost. In this survey, the meta-learning methods are first analyzed since they
    are highly correlated to OSOL methods, then transfer-learning methods and fine-tune-free
    methods are analyzed later.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 方法分类。目前标准的 FSOD 方法可以分为基于微调的方法和无微调的方法。大多数方法是基于微调的方法，这些方法需要在新数据集上微调模型，以显著提升性能。早期的基于微调的方法采用了元学习方案，并且也集中于与
    OSOL 方法类似的特征聚合方法。此外，标注样本数量的增加为标准 FSOD 方法采用迁移学习方案打开了可能性，这种方案在基础数据集上预训练物体检测器，然后在新数据集上微调这个预训练模型以适应新类别。早期的迁移学习方法如
    LSTD (Chen et al., [2018](#bib.bib12)) 在那段时间里被元学习方法超越，直到 TFA (Wang et al., [2020a](#bib.bib119))
    的出现。另一方面，无微调的方法旨在去除微调步骤，因为微调步骤由于其不可忽视的计算成本在实际生活中的 FSOD 中并不适用。在这项调查中，首先分析元学习方法，因为它们与
    OSOL 方法高度相关，然后再分析迁移学习方法和无微调的方法。
- en: 4.1\. Meta-Learning Methods
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1. 元学习方法
- en: Similar to OSOL, the meta-learning methods for standard FSOD first define a
    large number of few-shot detection tasks on the base dataset to train the model.
    The difference is that each few-shot task contains a query image and multiple
    support images since FSOD requires support images from all base classes for the
    multi-class classification task. Another difference is that meta-learning methods
    for standard FSOD have an additional fine-tuning stage that OSOL methods lack,
    which continues to meta-train the model by sampling support images from both base
    classes and novel classes for each few-shot task. The meta-learning framework
    of standard FSOD is similar to that of OSOL, which conducts “integral feature
    aggregation” and “RoI feature aggregation” to aggregate the query features with
    support features to incorporate the information of support images into the query
    image for prediction. Early meta-learning methods only conduct RoI feature aggregation,
    and later methods conduct both integral and RoI feature aggregation (named “mixed
    feature aggregation”) for better performance. Therefore, meta-learning methods
    for standard FSOD are categorized into RoI feature aggregation methods and mixed
    feature aggregation methods for a more explicit presentation in this survey.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于OSOL，标准FSOD的元学习方法首先在基础数据集上定义大量的少样本检测任务来训练模型。不同之处在于，每个少样本任务包含一个查询图像和多个支持图像，因为FSOD需要来自所有基础类别的支持图像以进行多类别分类任务。另一个区别是，标准FSOD的元学习方法有一个OSOL方法所缺乏的额外微调阶段，该阶段通过从基础类别和新类别中采样支持图像来继续对模型进行元训练。标准FSOD的元学习框架类似于OSOL，它进行“整体特征聚合”和“RoI特征聚合”，将查询特征与支持特征聚合，以将支持图像的信息融入查询图像进行预测。早期的元学习方法仅进行RoI特征聚合，而后期的方法则进行整体和RoI特征聚合（称为“混合特征聚合”）以获得更好的性能。因此，标准FSOD的元学习方法被分类为RoI特征聚合方法和混合特征聚合方法，以便在本调查中更清晰地呈现。
- en: 4.1.1\. RoI Feature Aggregation Methods
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1. RoI特征聚合方法
- en: 'RoI feature aggregation methods aggregate the RoI features with support features
    to generate class-specific RoI features for the classification and regression
    tasks. Unlike OSOL methods that almost all adopt Faster R-CNN as the detection
    framework, early meta-learning methods explore RoI feature aggregation methods
    on both single-stage and two-stage detectors. These RoI feature aggregation methods
    can be categorized into two types according to the type of aggregated features:
    RoI feature-vector aggregation methods (FSRW (Kang et al., [2019](#bib.bib48)),
    Meta R-CNN (Yan et al., [2019](#bib.bib131)), CME (Li et al., [2021c](#bib.bib61)),
    TIP (Li and Li, [2021](#bib.bib57)), VFA (Han et al., [2023](#bib.bib39)), FSOD-KT (Kim
    et al., [2020](#bib.bib53)), GenDet (Liu et al., [2022a](#bib.bib76)), FsDet (Xiao
    and Marlet, [2020](#bib.bib125)), DRL (Liu et al., [2021b](#bib.bib77)), and AFD-Net (Liu
    et al., [2021a](#bib.bib75))) and RoI feature-map aggregation methods (Attention-RPN (Fan
    et al., [2020](#bib.bib22)), QA-FewDet (Han et al., [2021](#bib.bib35)), KFSOD (Zhang
    et al., [2022d](#bib.bib147)), PNSD (Zhang et al., [2020a](#bib.bib145)), MM-FSOD (Han
    et al., [2022c](#bib.bib38)), SQMG-FSOD (Zhang et al., [2021c](#bib.bib144)),
    ICPE (Lu et al., [2022](#bib.bib79)), DAnA-FasterRCNN (Chen et al., [2021b](#bib.bib13)),
    TENET (Zhang et al., [2022c](#bib.bib146)), Hierarchy-FasterRCNN (Park and Lee,
    [2022](#bib.bib88)), IQ-SAM (Lee et al., [2022a](#bib.bib55)), and Meta Faster
    R-CNN (Han et al., [2022a](#bib.bib36))).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: RoI特征聚合方法通过将RoI特征与支持特征聚合，生成特定类别的RoI特征以用于分类和回归任务。与几乎所有采用Faster R-CNN作为检测框架的OSOL方法不同，早期的元学习方法在单阶段和两阶段检测器上探索了RoI特征聚合方法。这些RoI特征聚合方法根据聚合特征的类型可以分为两类：RoI特征向量聚合方法（FSRW (Kang
    et al., [2019](#bib.bib48)), Meta R-CNN (Yan et al., [2019](#bib.bib131)), CME (Li
    et al., [2021c](#bib.bib61)), TIP (Li and Li, [2021](#bib.bib57)), VFA (Han et al.,
    [2023](#bib.bib39)), FSOD-KT (Kim et al., [2020](#bib.bib53)), GenDet (Liu et al.,
    [2022a](#bib.bib76)), FsDet (Xiao and Marlet, [2020](#bib.bib125)), DRL (Liu et al.,
    [2021b](#bib.bib77)), 和 AFD-Net (Liu et al., [2021a](#bib.bib75))) 和RoI特征图聚合方法（Attention-RPN (Fan
    et al., [2020](#bib.bib22)), QA-FewDet (Han et al., [2021](#bib.bib35)), KFSOD (Zhang
    et al., [2022d](#bib.bib147)), PNSD (Zhang et al., [2020a](#bib.bib145)), MM-FSOD (Han
    et al., [2022c](#bib.bib38)), SQMG-FSOD (Zhang et al., [2021c](#bib.bib144)),
    ICPE (Lu et al., [2022](#bib.bib79)), DAnA-FasterRCNN (Chen et al., [2021b](#bib.bib13)),
    TENET (Zhang et al., [2022c](#bib.bib146)), Hierarchy-FasterRCNN (Park and Lee,
    [2022](#bib.bib88)), IQ-SAM (Lee et al., [2022a](#bib.bib55)), 和Meta Faster R-CNN (Han
    et al., [2022a](#bib.bib36))）。
- en: '![Refer to caption](img/c97a864a476a3ab5f6a68bc700ae437f.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c97a864a476a3ab5f6a68bc700ae437f.png)'
- en: (a) FSRW (Kang et al., [2019](#bib.bib48)) Meta R-CNN (Yan et al., [2019](#bib.bib131))
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (a) FSRW (康等, [2019](#bib.bib48)) Meta R-CNN (严等, [2019](#bib.bib131))
- en: '![Refer to caption](img/54afae4340491d308126acfd63804566.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/54afae4340491d308126acfd63804566.png)'
- en: (b) FsDet (Xiao and Marlet, [2020](#bib.bib125)) AFD-Net (Liu et al., [2021a](#bib.bib75))
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (b) FsDet (肖和马尔莱, [2020](#bib.bib125)) AFD-Net (刘等, [2021a](#bib.bib75))
- en: Figure 6\. Overview of RoI feature vector concatenation method for standard
    FSOD. The symbols $\odot$, $\ominus$ and ⓒ denotes element-wise multiplication,
    element-wise subtraction and concatenation operation, respectively. FSRW and Meta
    R-CNN aggregates the support feature vector and the query feature vector with
    a simple element-wise multiplication. FsDet and AFD-Net concatenate the query
    feature vector with an element-wise multiplication, subtraction between the query
    feature vector and the support feature vector using two additional FC layers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 标准FSOD的RoI特征向量连接方法概述。符号 $\odot$、$\ominus$ 和 ⓒ 分别表示逐元素乘法、逐元素减法和连接操作。FSRW
    和 Meta R-CNN 通过简单的逐元素乘法将支持特征向量与查询特征向量进行聚合。FsDet 和 AFD-Net 使用两个额外的全连接层，通过逐元素乘法和查询特征向量与支持特征向量之间的减法，将查询特征向量与支持特征向量进行连接。
- en: The “RoI feature-vector aggregation methods” can be categorized into two types,
    which are first proposed by FSRW and FsDet, respectively.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: “RoI特征向量聚合方法”可以分为两种类型，这两种方法分别由FSRW和FsDet首次提出。
- en: '$\bullet$ FSRW (Kang et al., [2019](#bib.bib48)) is the first meta-learning
    method for standard FSOD based on the YOLOv2 detection framework. FSRW simply
    aggregates each feature vector $v_{r}\in\mathbb{R}^{C}$ at each pixel of the query
    feature map with the pooled embedding $v_{s}\in\mathbb{R}^{C}$ of the support
    feature map, aiming to highlight the important features corresponding to the support
    image using a simple element-wise multiplication:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ FSRW (康等, [2019](#bib.bib48)) 是基于YOLOv2检测框架的第一个标准FSOD元学习方法。FSRW通过对每个查询特征图像素的特征向量
    $v_{r}\in\mathbb{R}^{C}$ 与支持特征图的池化嵌入 $v_{s}\in\mathbb{R}^{C}$ 进行简单的逐元素乘法来聚合，旨在通过简单的逐元素乘法突出支持图像对应的重要特征：
- en: '| (6) |  | $v_{\rm fused}=v_{r}\otimes v_{s}\text{.}$ |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $v_{\rm fused}=v_{r}\otimes v_{s}\text{.}$ |  |'
- en: The fused feature vector $v_{\rm fused}\in\mathbb{R}^{C}$ is used to predict
    the classification score (for the class that $v_{s}$ is from) and location regression,
    as shown in [6(a)](#S4.F6.sf1 "6(a) ‣ Figure 6 ‣ 4.1.1\. RoI Feature Aggregation
    Methods ‣ 4.1\. Meta-Learning Methods ‣ 4\. Standard Few-Shot Object Detection
    ‣ A Survey of Deep Learning for Low-Shot Object Detection"). Meta R-CNN (Yan et al.,
    [2019](#bib.bib131)), CME (Li et al., [2021c](#bib.bib61)), TIP (Li and Li, [2021](#bib.bib57)),
    VFA (Han et al., [2023](#bib.bib39)), FSOD-KT (Kim et al., [2020](#bib.bib53))
    and GenDet (Liu et al., [2022a](#bib.bib76)) follow this simple element-wise multiplication
    operation with other elaborated extensions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 融合特征向量 $v_{\rm fused}\in\mathbb{R}^{C}$ 用于预测分类分数（针对 $v_{s}$ 所属的类别）和位置回归，如[6(a)](#S4.F6.sf1
    "6(a) ‣ Figure 6 ‣ 4.1.1\. RoI Feature Aggregation Methods ‣ 4.1\. Meta-Learning
    Methods ‣ 4\. Standard Few-Shot Object Detection ‣ A Survey of Deep Learning for
    Low-Shot Object Detection")所示。Meta R-CNN (严等, [2019](#bib.bib131))、CME (李等, [2021c](#bib.bib61))、TIP
    (李和李, [2021](#bib.bib57))、VFA (韩等, [2023](#bib.bib39))、FSOD-KT (金等, [2020](#bib.bib53))
    和 GenDet (刘等, [2022a](#bib.bib76)) 都遵循这种简单的逐元素乘法操作，并进行了其他详细的扩展。
- en: $\bullet$ FsDet (Xiao and Marlet, [2020](#bib.bib125)) upgrades this simple
    element-wise multiplication operation to a more complex yet effective version,
    as shown in [6(b)](#S4.F6.sf2 "6(b) ‣ Figure 6 ‣ 4.1.1\. RoI Feature Aggregation
    Methods ‣ 4.1\. Meta-Learning Methods ‣ 4\. Standard Few-Shot Object Detection
    ‣ A Survey of Deep Learning for Low-Shot Object Detection"). Specifically, given
    the RoI feature vector $v_{r}$ and the support feature vector $v_{s}$, the aggregated
    feature vector $v_{\rm fused}$ is calculated as the concatenation of their linearly
    transformed element-wise multiplication, subtraction and the original $v_{r}$,
    as shown in [Equation 7](#S4.E7 "7 ‣ 4.1.1\. RoI Feature Aggregation Methods ‣
    4.1\. Meta-Learning Methods ‣ 4\. Standard Few-Shot Object Detection ‣ A Survey
    of Deep Learning for Low-Shot Object Detection") (${\rm FC}$ denotes a fully-connected
    layer that reduces the dimension). With this extended aggregation method, FsDet
    outperforms Meta R-CNN on both MS COCO benchmark and PASCAL VOC benchmark.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ FsDet (Xiao 和 Marlet, [2020](#bib.bib125)) 将这种简单的逐元素乘法操作升级为更复杂但有效的版本，如[6(b)](#S4.F6.sf2
    "6(b) ‣ Figure 6 ‣ 4.1.1\. RoI Feature Aggregation Methods ‣ 4.1\. Meta-Learning
    Methods ‣ 4\. Standard Few-Shot Object Detection ‣ A Survey of Deep Learning for
    Low-Shot Object Detection")所示。具体而言，给定 RoI 特征向量 $v_{r}$ 和支持特征向量 $v_{s}$，聚合特征向量
    $v_{\rm fused}$ 计算为它们线性变换后的逐元素乘法、减法和原始 $v_{r}$ 的拼接，如[Equation 7](#S4.E7 "7 ‣ 4.1.1\.
    RoI Feature Aggregation Methods ‣ 4.1\. Meta-Learning Methods ‣ 4\. Standard Few-Shot
    Object Detection ‣ A Survey of Deep Learning for Low-Shot Object Detection")所示
    (${\rm FC}$ 表示降维的全连接层)。通过这种扩展的聚合方法，FsDet 在 MS COCO 基准测试和 PASCAL VOC 基准测试中均优于 Meta
    R-CNN。
- en: '| (7) |  | $v_{\rm fused}=[{\rm FC}(v_{r}\otimes v_{s}),{\rm FC}(v_{r}-v_{s}),v_{r}]\text{,}$
    |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $v_{\rm fused}=[{\rm FC}(v_{r}\otimes v_{s}),{\rm FC}(v_{r}-v_{s}),v_{r}]\text{,}$
    |  |'
- en: $\bullet$ AFD-Net (Liu et al., [2021a](#bib.bib75)) and DRL (Liu et al., [2021b](#bib.bib77)).
    These two methods follow FsDet in this RoI feature-vector aggregation method with
    some other modifications.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ AFD-Net (Liu 等, [2021a](#bib.bib75)) 和 DRL (Liu 等, [2021b](#bib.bib77))。这两种方法在
    RoI 特征向量聚合方法中遵循 FsDet，但进行了其他一些修改。
- en: Unlike the above RoI feature-vector aggregation methods which concentrate on
    the aggregation of feature vectors, RoI feature-map aggregation methods focus
    on the aggregation of feature maps that preserves spatial information for better
    excavating the relation between query and support images. Some methods only adopt
    simple concatenation operation and element-wise operation for the feature map
    aggregation, while newly proposed methods tend to adopt attention operation for
    feature map aggregation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述集中于特征向量聚合的 RoI 特征向量聚合方法不同，RoI 特征图聚合方法专注于特征图的聚合，以保留空间信息，更好地挖掘查询图像与支持图像之间的关系。一些方法仅采用简单的拼接操作和逐元素操作进行特征图聚合，而新提出的方法则倾向于采用注意力操作进行特征图聚合。
- en: $\bullet$ Concatenation operation & element-wise operation for RoI feature-map
    aggregation. SQMG-FSOD (Zhang et al., [2021c](#bib.bib144)) simply concatenates
    the RoI feature map with the support feature map for the RoI feature-map aggregation.
    While some methods (Attention-RPN (Fan et al., [2020](#bib.bib22)), QA-FewDet (Han
    et al., [2021](#bib.bib35)), KFSOD (Zhang et al., [2022d](#bib.bib147)), PNSD (Zhang
    et al., [2020a](#bib.bib145)), FCT (Han et al., [2022b](#bib.bib37)), and MM-FSOD (Han
    et al., [2022c](#bib.bib38))) utilize a multi-relation head that adopt both concatenation
    operation and element-wise operation. Specifically, this multi-relation head consists
    of a global-relation head, a patch-relation head, and a local-relation head. The
    global-relation head concatenates $\phi_{r}$ and $\phi_{s}$ in depth with a pooling
    operation. The patch-relation head concatenates $\phi_{r}$ and $\phi_{s}$ with
    several convolutional blocks on it. And the local-relation head aggregates $\phi_{r}$
    and $\phi_{s}$ by calculating the pixel-wise and depth-wise similarities between
    them. These methods conduct both integral and RoI feature aggregation, which will
    be specified later.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ RoI 特征图聚合的连接操作和逐元素操作。SQMG-FSOD （Zhang et al., [2021c](#bib.bib144)）简单地将
    RoI 特征图与支持特征图连接用于 RoI 特征图聚合。而一些方法 （Attention-RPN （Fan et al., [2020](#bib.bib22)），QA-FewDet （Han
    et al., [2021](#bib.bib35)），KFSOD （Zhang et al., [2022d](#bib.bib147)），PNSD （Zhang
    et al., [2020a](#bib.bib145)），FCT （Han et al., [2022b](#bib.bib37)），和 MM-FSOD （Han
    et al., [2022c](#bib.bib38)））利用多关系头，采用连接操作和逐元素操作。具体而言，这个多关系头包括一个全局关系头、一个补丁关系头和一个局部关系头。全局关系头将
    $\phi_{r}$ 和 $\phi_{s}$ 在深度上进行连接，并进行池化操作。补丁关系头将 $\phi_{r}$ 和 $\phi_{s}$ 通过几个卷积块进行连接。局部关系头通过计算它们之间的像素级和深度级相似性来聚合
    $\phi_{r}$ 和 $\phi_{s}$。这些方法同时进行整体和 RoI 特征聚合，具体将在后面说明。
- en: $\bullet$ Attention operation for RoI feature-map aggregation. Some methods (ICPE (Lu
    et al., [2022](#bib.bib79)), DAnA-FasterRCNN (Chen et al., [2021b](#bib.bib13)),
    TENET (Zhang et al., [2022c](#bib.bib146)), Hierarchy-FasterRCNN (Park and Lee,
    [2022](#bib.bib88)), IQ-SAM (Lee et al., [2022a](#bib.bib55)), and Meta Faster
    R-CNN (Han et al., [2022a](#bib.bib36))) adopt the attention operation to conduct
    RoI feature-map aggregation. Specifically, they calculate the aggregated feature
    map according to the similarity score (attention) between each pair of elements
    from $\phi_{r}$ and $\phi_{s}$. In these methods, ICPE conducts only RoI feature
    aggregation with some proposed modifications. Specifically, it additionally incorporates
    the information of query images into support images before the final feature aggregation,
    and it adjusts the importance of different support images instead of treating
    them as equals. Other methods conduct both integral and RoI feature aggregation,
    which will be specified later.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ RoI 特征图聚合的注意力操作。一些方法 （ICPE （Lu et al., [2022](#bib.bib79)），DAnA-FasterRCNN （Chen
    et al., [2021b](#bib.bib13)），TENET （Zhang et al., [2022c](#bib.bib146)），Hierarchy-FasterRCNN （Park
    and Lee, [2022](#bib.bib88)），IQ-SAM （Lee et al., [2022a](#bib.bib55)），和 Meta Faster
    R-CNN （Han et al., [2022a](#bib.bib36)））采用注意力操作进行 RoI 特征图聚合。具体而言，它们根据 $\phi_{r}$
    和 $\phi_{s}$ 中每对元素之间的相似度分数（注意力）计算聚合特征图。在这些方法中，ICPE 仅进行 RoI 特征聚合，并进行了一些提出的修改。具体来说，它在最终特征聚合之前将查询图像的信息并入支持图像，并调整不同支持图像的重要性，而不是将它们视为平等。其他方法同时进行整体和
    RoI 特征聚合，具体将在后面说明。
- en: $\bigstar$ Discussion of RoI Feature Aggregation Methods. RoI Feature Aggregation
    Methods are categorized into RoI feature-vector aggregation methods and RoI feature-map
    aggregation methods. RoI feature-vector aggregation methods are early meta-learning
    methods for FSOD, whose approaches are simple and limit their performance. On
    the other hand, RoI feature-map aggregation methods preserve spatial information
    of query and support samples, towards fully extracting the spatial relations between
    query and support features. Therefore, RoI feature-map aggregation methods can
    better discriminate features of different objects and achieve higher performance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: $\bigstar$ RoI 特征聚合方法讨论。RoI 特征聚合方法分为 RoI 特征向量聚合方法和 RoI 特征图聚合方法。RoI 特征向量聚合方法是早期的
    FSOD 元学习方法，其方法简单，限制了其性能。另一方面，RoI 特征图聚合方法保留了查询样本和支持样本的空间信息，旨在完全提取查询特征和支持特征之间的空间关系。因此，RoI
    特征图聚合方法可以更好地区分不同对象的特征，并实现更高的性能。
- en: 4.1.2\. Mixed Feature Aggregation Methods
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 混合特征聚合方法
- en: The above section discusses only the RoI feature aggregation, while most newly
    proposed methods (named “mixed feature aggregation methods”) additionally conduct
    integral feature aggregation to incorporate class-specific information into the
    shallow components of the detection model. The integral feature aggregation methods
    are mainly conducted on the feature-maps (not feature-vectors) and can be categorized
    into concatenation & element-wise operations (Attention-RPN (Fan et al., [2020](#bib.bib22)),
    QA-FewDet (Han et al., [2021](#bib.bib35)), KFSOD (Zhang et al., [2022d](#bib.bib147)),
    PNSD (Zhang et al., [2020a](#bib.bib145)), MM-FSOD (Han et al., [2022c](#bib.bib38)),
    Meta Faster R-CNN (Han et al., [2022a](#bib.bib36))), convolutional operation (SQMG-FSOD (Zhang
    et al., [2021c](#bib.bib144))), and attention operation (DAnA-FasterRCNN (Chen
    et al., [2021b](#bib.bib13)), TENET (Zhang et al., [2022c](#bib.bib146)), Hierarchy-FasterRCNN (Park
    and Lee, [2022](#bib.bib88)), IQ-SAM (Lee et al., [2022a](#bib.bib55)), DCNet (Hu
    et al., [2021](#bib.bib45)), Meta-DETR (Zhang et al., [2021b](#bib.bib142)), FCT (Han
    et al., [2022b](#bib.bib37))).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上述部分仅讨论了 RoI 特征聚合，而大多数新提出的方法（称为“混合特征聚合方法”）还进行整体特征聚合，以将类特定信息整合到检测模型的浅层组件中。整体特征聚合方法主要在特征图上进行（而不是特征向量），可以分为连接与逐元素操作（Attention-RPN (Fan
    et al., [2020](#bib.bib22)), QA-FewDet (Han et al., [2021](#bib.bib35)), KFSOD (Zhang
    et al., [2022d](#bib.bib147)), PNSD (Zhang et al., [2020a](#bib.bib145)), MM-FSOD (Han
    et al., [2022c](#bib.bib38)), Meta Faster R-CNN (Han et al., [2022a](#bib.bib36))），卷积操作（SQMG-FSOD (Zhang
    et al., [2021c](#bib.bib144)))，以及注意力操作（DAnA-FasterRCNN (Chen et al., [2021b](#bib.bib13)),
    TENET (Zhang et al., [2022c](#bib.bib146)), Hierarchy-FasterRCNN (Park and Lee,
    [2022](#bib.bib88)), IQ-SAM (Lee et al., [2022a](#bib.bib55)), DCNet (Hu et al.,
    [2021](#bib.bib45)), Meta-DETR (Zhang et al., [2021b](#bib.bib142)), FCT (Han
    et al., [2022b](#bib.bib37)))。
- en: $\bullet$ Concatenation & element-wise operations for integral feature aggregation.
    Attention-RPN (Fan et al., [2020](#bib.bib22)) conducts integral feature map aggregation
    by using $\phi_{s}\in\mathbb{R}^{C\times H_{s}\times H_{s}}$ as a kernel and sliding
    it across $\phi_{q}\in\mathbb{R}^{C\times H_{q}\times H_{q}}$ to compute similarities
    at each location. Specifically, the element at the location $(c,h,w)$ of the aggregated
    feature map $\phi_{\rm fused}$ is calculated in [Equation 8](#S4.E8 "8 ‣ 4.1.2\.
    Mixed Feature Aggregation Methods ‣ 4.1\. Meta-Learning Methods ‣ 4\. Standard
    Few-Shot Object Detection ‣ A Survey of Deep Learning for Low-Shot Object Detection") (note
    that $i,j\in\{1,\cdots,H_{s}\}$). Some methods (QA-FewDet (Han et al., [2021](#bib.bib35)),
    KFSOD (Zhang et al., [2022d](#bib.bib147)), PNSD (Zhang et al., [2020a](#bib.bib145)),
    MM-FSOD (Han et al., [2022c](#bib.bib38)), Meta Faster R-CNN (Han et al., [2022a](#bib.bib36)))
    follow this integral feature aggregation method with other extensions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 连接与逐元素操作用于整体特征聚合。Attention-RPN (Fan et al., [2020](#bib.bib22)) 通过使用
    $\phi_{s}\in\mathbb{R}^{C\times H_{s}\times H_{s}}$ 作为内核，并在 $\phi_{q}\in\mathbb{R}^{C\times
    H_{q}\times H_{q}}$ 上滑动以计算每个位置的相似性，从而进行整体特征图聚合。具体来说，聚合特征图 $\phi_{\rm fused}$ 中位置
    $(c,h,w)$ 的元素是通过 [方程 8](#S4.E8 "8 ‣ 4.1.2\. 混合特征聚合方法 ‣ 4.1\. 元学习方法 ‣ 4\. 标准少样本目标检测
    ‣ 深度学习在低样本目标检测中的综述") 计算的（注意 $i,j\in\{1,\cdots,H_{s}\}$）。一些方法（QA-FewDet (Han et al.,
    [2021](#bib.bib35)), KFSOD (Zhang et al., [2022d](#bib.bib147)), PNSD (Zhang et al.,
    [2020a](#bib.bib145)), MM-FSOD (Han et al., [2022c](#bib.bib38)), Meta Faster
    R-CNN (Han et al., [2022a](#bib.bib36))) 在此整体特征聚合方法的基础上进行了一些扩展。
- en: '| (8) |  | ${\phi_{\rm fused}}_{(c,h,w)}=\sum\limits_{i,j}{\phi_{q}}_{(c,h+i-1,w+j-1)}\cdot{\phi_{s}}_{(c,i,j)}\text{.}$
    |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | ${\phi_{\rm fused}}_{(c,h,w)}=\sum\limits_{i,j}{\phi_{q}}_{(c,h+i-1,w+j-1)}\cdot{\phi_{s}}_{(c,i,j)}\text{.}$
    |  |'
- en: $\bullet$ Convolutional operation for integral feature aggregation. SQMG-FSOD (Zhang
    et al., [2021c](#bib.bib144)) proposes another integral feature aggregation method
    by generating convolutional kernels from support features and using the generated
    kernels to enhance query features. Furthermore, SQMG-FSOD not only learns a distance
    metric to compare RoI features and support features for filtering out irrelevant
    RoIs but also utilizes this metric to assign weights to support samples by comparing
    them with query images. Additionally, it proposes a hybrid loss to mitigate the
    false positive problem (i.e., some background RoIs are misclassified into objects).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 卷积操作用于整体特征聚合。SQMG-FSOD (Zhang et al., [2021c](#bib.bib144)) 提出了另一种整体特征聚合方法，通过从支持特征生成卷积内核，并使用生成的内核来增强查询特征。此外，SQMG-FSOD
    不仅学习了一个距离度量来比较 RoI 特征和支持特征，以筛选掉不相关的 RoI，还利用该度量通过与查询图像进行比较来为支持样本分配权重。此外，它还提出了一种混合损失来缓解误报问题（即一些背景
    RoI 被错误分类为对象）。
- en: $\bullet$ Attention operation for integral feature aggregation. Newly proposed
    methods (DCNet (Hu et al., [2021](#bib.bib45)), DAnA-FasterRCNN (Chen et al.,
    [2021b](#bib.bib13)), TENET (Zhang et al., [2022c](#bib.bib146)), Hierarchy-FasterRCNN (Park
    and Lee, [2022](#bib.bib88)), IQ-SAM (Lee et al., [2022a](#bib.bib55)), Meta-DETR (Zhang
    et al., [2021b](#bib.bib142)), and FCT (Han et al., [2022b](#bib.bib37))) tend
    to adopt attention operation for integral feature aggregation. Attention operation
    aggregates two feature-maps using a similar manner as scaled dot-product attention (Subakan
    et al., [2021](#bib.bib105)). It extracts the key map and the value map from the
    query image and the support image, respectively, then calculates the pixel-wise
    similarities between these two key maps and uses them to aggregate two value maps.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 积分特征聚合的注意力操作。新提出的方法（DCNet（Hu et al., [2021](#bib.bib45)）、DAnA-FasterRCNN（Chen
    et al., [2021b](#bib.bib13)）、TENET（Zhang et al., [2022c](#bib.bib146)）、Hierarchy-FasterRCNN（Park
    and Lee, [2022](#bib.bib88)）、IQ-SAM（Lee et al., [2022a](#bib.bib55)）、Meta-DETR（Zhang
    et al., [2021b](#bib.bib142)）和FCT（Han et al., [2022b](#bib.bib37)））倾向于采用注意力操作进行积分特征聚合。注意力操作通过与缩放点积注意力（Subakan
    et al., [2021](#bib.bib105)）类似的方式来聚合两个特征图。它分别从查询图像和支持图像中提取关键图和价值图，然后计算这两个关键图之间的逐像素相似度，并利用这些相似度来聚合两个价值图。
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Meta-DETR also adopts attention operation for integral feature aggregation with
    a significant boost in performance. The major difference is that it adopts Deformable
    DETR (Zhu et al., [2021b](#bib.bib161)) as the detection framework. DETR is an
    end-to-end transformer-based detector that eliminates anchor boxes in former detectors.
    Besides, Meta-DETR proposes a correlational aggregation module (CAM) that uses
    single-head attention to aggregate the query feature-maps with the support feature-maps.
    The aggregated features are finally fed into a class-agnostic transformer to predict
    object categories and locations.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Meta-DETR也采用了注意力操作进行积分特征聚合，并且在性能上取得了显著提升。主要区别在于它采用了Deformable DETR（Zhu et al.,
    [2021b](#bib.bib161)）作为检测框架。DETR是一种端到端的基于变换器的检测器，消除了以前检测器中的锚框。此外，Meta-DETR提出了一个相关聚合模块（CAM），该模块使用单头注意力来聚合查询特征图和支持特征图。最后，将聚合的特征输入到一个无类别的变换器中以预测物体类别和位置。
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Most of these methods aggregate the query and support features that are extracted
    from the backbone independently, while FCT surpasses this limit and instead aggregates
    the features in each layer of the ViT backbones, which achieves significant performance
    improvement. First, it splits query images and support images into image tokens
    and add position & branch embeddings into them (i.e., position embedding discriminates
    the position of the token, and branch embedding discriminates whether the token
    is from support image or query image). Next, it concatenates all query and support
    tokens into a sequence and feeds them into a transformer to generate the aggregated
    integral features.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些方法中的大多数独立地聚合从骨干网络中提取的查询特征和支持特征，而FCT超越了这一限制，改为在ViT骨干网络的每一层中聚合特征，从而实现了显著的性能提升。首先，它将查询图像和支持图像分割为图像标记，并将位置和分支嵌入添加到其中（即位置嵌入区分标记的位置，而分支嵌入区分标记来自支持图像还是查询图像）。接下来，它将所有查询和支持标记连接成一个序列，并将其输入到变换器中生成聚合的积分特征。
- en: '$\bigstar$ Discussion of Mixed Feature Aggregation Methods. Compared to RoI
    feature aggregation methods, mixed feature aggregation methods additionally conduct
    integral feature aggregation to incorporate category-specific information into
    the shallow components (mainly RPN) of the detection model, which extracts more
    positive region proposals for the further classification & regression tasks and
    improves the performance. Mixed feature aggregation methods are categorized into
    three types: concatenation & element-wise operations, convolutional operation,
    and attention operation. Simple concatenation & element-wise operations are mostly
    adopted by early FSOD methods, which have poor performance and need to combine
    other components altogether for performance improvement. Convolutional operation
    is still simple, which cannot fully incorporate the information of support features
    into query features. Attention operation captures the relation between local regions
    in query feature maps and support feature maps, which better discriminates different
    local regions, and these methods overall achieve better performance.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: $\bigstar$ 混合特征聚合方法的讨论。与 RoI 特征聚合方法相比，混合特征聚合方法额外进行整体特征聚合，将类别特定的信息纳入检测模型的浅层组件（主要是
    RPN），从而为进一步的分类和回归任务提取更多正样本区域提议，并提升性能。混合特征聚合方法分为三种类型：拼接与逐元素操作、卷积操作和注意力操作。早期 FSOD
    方法大多采用简单的拼接与逐元素操作，这些方法性能较差，需要结合其他组件来提高性能。卷积操作仍然较为简单，无法完全将支持特征的信息整合到查询特征中。注意力操作捕捉查询特征图与支持特征图中局部区域之间的关系，更好地区分不同的局部区域，这些方法整体上实现了更好的性能。
- en: 4.1.3\. Other Meta-Learning Methods
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3. 其他元学习方法
- en: There are some other meta-learning methods that focus on issues other than the
    aggregation method of features, which are weight-prediction-based methods and
    metric-learning-based methods.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的元学习方法关注于除了特征聚合方法之外的问题，这些方法包括基于权重预测的方法和基于度量学习的方法。
- en: $\bullet$ Weight-Prediction-Based Methods. MetaDet (Wang et al., [2019a](#bib.bib120))
    proposes a meta-learning method that learns to predict the weights of category-specific
    components of the model. MetaDet predicts category-specific (e.g., the classification
    and regression branches) weights for novel classes from few samples and fine-tunes
    the model on the novel dataset. Meta-RetinaNet (Li et al., [2020b](#bib.bib65))
    is another method which adopts RetinaNet as the detection framework and predicts
    the weights of the whole network.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于权重预测的方法。MetaDet (Wang et al., [2019a](#bib.bib120)) 提出了一种元学习方法，学习预测模型中特定类别组件的权重。MetaDet
    从少量样本中预测特定类别（例如分类和回归分支）的权重，并在新数据集上微调模型。Meta-RetinaNet (Li et al., [2020b](#bib.bib65))
    是另一种方法，它采用 RetinaNet 作为检测框架并预测整个网络的权重。
- en: $\bullet$ Metric-Learning-Based Methods. IR-FSOD (Huang et al., [2021](#bib.bib46))
    directly learns to compare the similarity between the RoI features with support
    features from different classes to generate the classification scores. CAReD (Quan
    et al., [2022](#bib.bib93)) also adds another metric learning branch for classification
    apart from the main classification branch.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于度量学习的方法。IR-FSOD (Huang et al., [2021](#bib.bib46)) 直接学习比较 RoI 特征与来自不同类别的支持特征之间的相似性，以生成分类分数。CAReD
    (Quan et al., [2022](#bib.bib93)) 除了主分类分支外，还增加了另一个度量学习分支用于分类。
- en: 4.2\. Transfer-Learning Methods
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2. 迁移学习方法
- en: 'Transfer-learning methods regard FSOD as a transfer-learning problem in which
    the source domain is the base dataset, and the target domain is the novel dataset.
    Current transfer-learning methods mainly adopt Faster R-CNN as the detection framework,
    consisting of two stages: base training and few-shot fine-tuning, as shown in
    [Figure 7](#S4.F7 "Figure 7 ‣ 4.2\. Transfer-Learning Methods ‣ 4\. Standard Few-Shot
    Object Detection ‣ A Survey of Deep Learning for Low-Shot Object Detection").
    The base training stage trains an object detector on the base dataset. After this
    stage, the object detector will obtain an effective feature extractor and achieve
    good performance on base classes. Then, in the few-shot fine-tuning stage, this
    pre-trained object detector will be fine-tuned on the novel dataset to detect
    novel classes. In this way, the common knowledge for feature extraction and proposal
    generation can be transferred from base classes to novel classes.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习方法将 FSOD 视为迁移学习问题，其中源领域是基础数据集，目标领域是新颖数据集。目前的迁移学习方法主要采用 Faster R-CNN 作为检测框架，分为两个阶段：基础训练和少样本微调，如[图
    7](#S4.F7 "Figure 7 ‣ 4.2\. Transfer-Learning Methods ‣ 4\. Standard Few-Shot
    Object Detection ‣ A Survey of Deep Learning for Low-Shot Object Detection")所示。基础训练阶段在基础数据集上训练目标检测器。经过这个阶段，目标检测器将获得有效的特征提取器，并在基础类别上取得良好的性能。然后，在少样本微调阶段，这个预训练的目标检测器将在新颖数据集上进行微调，以检测新类别。通过这种方式，特征提取和建议生成的通用知识可以从基础类别转移到新类别。
- en: '![Refer to caption](img/bd121d5c58ec876d3a5c2a13fe0d48ef.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/bd121d5c58ec876d3a5c2a13fe0d48ef.png)'
- en: (a) Base Training Stage on Faster R-CNN
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 基础训练阶段基于 Faster R-CNN
- en: '![Refer to caption](img/ca92d8f9eff5423065c99c93344e4015.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/ca92d8f9eff5423065c99c93344e4015.png)'
- en: (b) Few-Shot Fine-tuning Stage on Faster R-CNN framework
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 在 Faster R-CNN 框架上的少样本微调阶段
- en: Figure 7\. Overview of the two-stage transfer-learning framework for standard
    FSOD (Wang et al., [2020a](#bib.bib119)). In the base training stage, the model
    is trained on the base dataset with abundant instances of base classes, while
    in the few-shot fine-tuning stage, the model is trained on a small dataset containing
    data for both base classes and novel classes. Current transfer-learning methods
    mostly adopt Faster R-CNN as the detection framework, as shown in the figure.
    The yellow components in these two figures denote intermediate tensors, the blue
    components denote modules in Faster R-CNN, and the lock symbol denotes that the
    parameters of the corresponding module are frozen.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 标准 FSOD 的两阶段迁移学习框架概述（Wang 等，[2020a](#bib.bib119)）。在基础训练阶段，模型在包含大量基础类别实例的基础数据集上进行训练，而在少样本微调阶段，模型在包含基础类别和新类别数据的小数据集上进行训练。目前的迁移学习方法大多采用
    Faster R-CNN 作为检测框架，如图所示。这两个图中的黄色组件表示中间张量，蓝色组件表示 Faster R-CNN 中的模块，锁定符号表示相应模块的参数被冻结。
- en: $\bullet$ LSTD (Chen et al., [2018](#bib.bib12)) is the first method to adopt
    the transfer-learning scheme for FSOD. It adopts Faster R-CNN as the detection
    framework with two regularization terms in the few-shot fine-tuning stage. Specifically,
    the first term suppresses background regions in the feature maps, and the second
    term promotes the fine-tuned model to generate similar predictions with the source
    model. Regrettably, the performance of LSTD is exceeded by the meta-learning methods
    during the same period.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ LSTD（Chen 等，[2018](#bib.bib12)）是第一个采用迁移学习方案进行 FSOD 的方法。它采用 Faster
    R-CNN 作为检测框架，并在少样本微调阶段加入了两个正则化项。具体而言，第一个项抑制特征图中的背景区域，第二个项促使微调后的模型生成与源模型相似的预测。遗憾的是，LSTD
    的性能在同一时期被元学习方法超越。
- en: $\bullet$ TFA (Wang et al., [2020a](#bib.bib119)) (Two-Stage Fine-tuning Approach)
    significantly improves the performance of transfer-learning methods based on the
    Faster R-CNN detection framework. In the base training stage, TFA pre-trains the
    model on the base dataset as previous transfer-learning methods. Then, in the
    few-shot fine-tuning stage, it freezes the main components of Faster R-CNN and
    only fine-tunes the last two layers (box classification and regression layers)
    of Faster R-CNN. The loss function used in the few-shot fine-tuning stage is the
    same as the base training stage but with a lower learning rate. The dataset used
    in the few-shot fine-tuning stage is a balanced dataset containing a few training
    samples of novel classes and a few selected training samples of base classes.
    This design retains the model’s detection ability for base classes and mitigates
    the problem that some objects of base classes are misclassified into novel classes.
    With this simple but effective training strategy, TFA outperforms early meta-learning
    methods like FSRW, MetaDet, and Meta R-CNN on both MS COCO benchmark and PASCAL
    VOC benchmark.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ TFA (Wang et al., [2020a](#bib.bib119)) (双阶段微调方法)显著提高了基于Faster R-CNN检测框架的转移学习方法的性能。在基础训练阶段，TFA像之前的转移学习方法一样在基础数据集上预训练模型。然后，在少样本微调阶段，它冻结Faster
    R-CNN的主要组件，仅微调Faster R-CNN的最后两层（框分类和回归层）。在少样本微调阶段使用的损失函数与基础训练阶段相同，但学习率较低。在少样本微调阶段使用的数据集是一个平衡的数据集，包含少量的新类训练样本和少量选择的基础类训练样本。这一设计保留了模型对基础类的检测能力，并减轻了一些基础类对象被误分类为新类的问题。凭借这一简单但有效的训练策略，TFA在MS
    COCO基准测试和PASCAL VOC基准测试中超越了早期的元学习方法，如FSRW、MetaDet和Meta R-CNN。
- en: '$\bullet$ DeFRCN (Qiao et al., [2021](#bib.bib92)) significantly improves the
    performance of TFA with two concise modifications: (1) DeFRCN assigns different
    importance values to the gradients from RPN module and R-CNN module, which is
    motivated by the viewpoint that RPN module and R-CNN module may learn paradoxically
    and the learning of these two modules should be decoupled. (2) DeFRCN utilizes
    a pre-trained classifier as an auxiliary branch for the classification of region
    proposals. DeFRCN further validates the effectiveness of transfer-learning methods
    for FSOD, and many methods are proposed following this transfer-learning paradigm.
    In this survey, transfer-learning methods are categorized into feature-augmentation-based
    methods, classification-based methods, regression-based methods, RPN-based methods,
    data-augmentation-based methods, and pre-train-based methods according to the
    detection stage they focus on.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ DeFRCN (Qiao et al., [2021](#bib.bib92)) 通过两个简洁的修改显著提高了TFA的性能：(1)
    DeFRCN对RPN模块和R-CNN模块的梯度分配不同的重要性值，这一观点源于RPN模块和R-CNN模块可能会以相反的方式进行学习，因此这两个模块的学习应当解耦。(2)
    DeFRCN利用预训练分类器作为辅助分支，用于区域提议的分类。DeFRCN进一步验证了转移学习方法在FSOD中的有效性，并且许多方法都在这一转移学习范式的基础上提出。在本次调研中，转移学习方法根据其关注的检测阶段被分类为基于特征增强的方法、基于分类的方法、基于回归的方法、基于RPN的方法、基于数据增强的方法和基于预训练的方法。
- en: 4.2.1\. Feature-Augmentation-Based Methods
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 基于特征增强的方法
- en: 'Feature-augmentation-based methods focus on the feature extraction stage of
    an FSOD model. These methods apply different augmentations to the features, aiming
    to better transfer the features learned on the base dataset to the novel dataset.
    Current feature-augmentation-based methods can be categorized into three types:
    self-attention-based methods (CT-FSOD (Yang et al., [2020a](#bib.bib137)), AttFDNet (Chen
    et al., [2020](#bib.bib14))), feature-discretization-based methods (SVD-FSOD (Wu
    et al., [2021b](#bib.bib122)), KD-FSOD (Pei et al., [2022](#bib.bib89))), and
    feature-inheritance-based methods (${\rm FSOD}^{\rm up}$ (Wu et al., [2021a](#bib.bib121)),
    FADI (Cao et al., [2021](#bib.bib7))).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征增强的方法侧重于FSOD模型的特征提取阶段。这些方法对特征应用不同的增强技术，旨在更好地将基础数据集上学到的特征转移到新数据集上。目前的基于特征增强的方法可以分为三种类型：自注意力机制的方法 (CT-FSOD (Yang
    et al., [2020a](#bib.bib137)), AttFDNet (Chen et al., [2020](#bib.bib14))), 特征离散化的方法 (SVD-FSOD (Wu
    et al., [2021b](#bib.bib122)), KD-FSOD (Pei et al., [2022](#bib.bib89))), 和特征继承的方法 (${\rm
    FSOD}^{\rm up}$ (Wu et al., [2021a](#bib.bib121)), FADI (Cao et al., [2021](#bib.bib7)))。
- en: $\bullet$ Self-Attention-Based Methods for Feature-Augmentation. Self-attention-based
    methods (CT-FSOD (Yang et al., [2020a](#bib.bib137)), AttFDNet (Chen et al., [2020](#bib.bib14)))
    adopt self-attention to augment the extracted features.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于自注意力的方法用于特征增强。基于自注意力的方法（CT-FSOD（Yang 等人，[2020a](#bib.bib137)），AttFDNet（Chen
    等人，[2020](#bib.bib14)））采用自注意力来增强提取的特征。
- en: $\bullet$ Feature-Discretization-Based Methods for Feature-Augmentation. Feature-discretization-based
    methods (SVD-FSOD (Wu et al., [2021b](#bib.bib122)), KD-FSOD (Pei et al., [2022](#bib.bib89)))
    discretize the feature-map by projecting each pixel of the feature-map into a
    learned codebook (i.e., replacing each pixel of the feature-map with its nearest
    code), thus enhancing the discrimination of features from different categories.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于特征离散化的方法用于特征增强。基于特征离散化的方法（SVD-FSOD（Wu 等人，[2021b](#bib.bib122)），KD-FSOD（Pei
    等人，[2022](#bib.bib89)））通过将特征图的每个像素投影到一个学习到的代码本中（即，用其最近的代码替换特征图的每个像素）来离散化特征图，从而增强不同类别特征的区分度。
- en: $\bullet$ Feature-Inheritance-Based Methods for Feature-Augmentation. Feature-inheritance-based
    methods (${\rm FSOD}^{\rm up}$ (Wu et al., [2021a](#bib.bib121)), FADI (Cao et al.,
    [2021](#bib.bib7))) inherit the features of base classes to the features of novel
    classes for augmentation, which mitigates the data scarcity problem of novel classes.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于特征继承的方法用于特征增强。基于特征继承的方法（${\rm FSOD}^{\rm up}$（Wu 等人，[2021a](#bib.bib121)），FADI（Cao
    等人，[2021](#bib.bib7)））将基类的特征继承到新类的特征中以进行增强，从而缓解了新类数据稀缺的问题。
- en: $\bigstar$ Discussion of Feature-Augmentation-Based Methods. Self-attention-based
    methods incorporate interpretability into the decision-making of FSOD through
    the attention heatmaps. However, self-attention-based methods are early FSOD methods,
    and the attention operations they adopt are primitive, restricting their performance.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: $\bigstar$ 基于特征增强的方法的讨论。基于自注意力的方法通过注意力热图将可解释性融入 FSOD 的决策中。然而，基于自注意力的方法是早期的 FSOD
    方法，它们采用的注意力操作是原始的，限制了其性能。
- en: Feature-discretization-based methods utilize feature discretization to enhance
    the discrimination of features from different categories, but they haven’t demonstrated
    the visual concepts that the discretized features represent. Besides, KD-FSOD
    requires an additional step to train an extra visual-word model and needs knowledge
    distillation to inherit the knowledge of this visual-word model into the few-shot
    detector, bringing a non-negligible burden into model training.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征离散化的方法利用特征离散化来增强不同类别特征的区分度，但尚未展示离散化特征所代表的视觉概念。此外，KD-FSOD 需要额外的步骤来训练额外的视觉词模型，并需要知识蒸馏将该视觉词模型的知识继承到少样本检测器中，这为模型训练带来了不可忽视的负担。
- en: Feature-inheritance-based methods utilize the knowledge from base classes as
    “free lunch” to augment the features of novel classes with negligible cost. However,
    in the scenario that base classes and novel classes are not in the same domain,
    it is unclear whether these methods still work since base classes and novel classes
    share less common knowledge.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征继承的方法利用来自基类的知识作为“免费午餐”来以微小的成本增强新类的特征。然而，在基类和新类不在同一领域的情况下，尚不清楚这些方法是否仍然有效，因为基类和新类共享的知识较少。
- en: 4.2.2\. Classification-Based Methods
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 基于分类的方法
- en: Classification-based methods aim to improve the classification branch of the
    detection model. Early classification-based methods focus on improving the main
    classification branch with some elaborated metric learning methods (RepMet (Karlinsky
    et al., [2019](#bib.bib50)), NP-RepMet (Yang et al., [2020b](#bib.bib136)), PNPDet (Zhang
    et al., [2021a](#bib.bib141)), FSOD-KI (Yang et al., [2023](#bib.bib138))). New
    classification-based methods mostly propose another classification branch to assist
    the main classification branch, including additional-classifier-based methods (FSCN (Li
    et al., [2021e](#bib.bib69))), contrastive-learning-based methods (FSCE (Sun et al.,
    [2021](#bib.bib106)), FSRC (Shangguan et al., [2022](#bib.bib103)), CoCo-RCNN (Ma
    et al., [2022](#bib.bib81))), knowledge-graph-based methods (KR-FSOD (Wang and
    Chen, [2022](#bib.bib115))), and semantic-infor-mation-based methods (SRR-FSOD (Zhu
    et al., [2021a](#bib.bib158))).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Metric Learning Methods for Classification. These methods (RepMet (Karlinsky
    et al., [2019](#bib.bib50)), NP-RepMet (Yang et al., [2020b](#bib.bib136)), PNPDet (Zhang
    et al., [2021a](#bib.bib141)), FSOD-KI (Yang et al., [2023](#bib.bib138))) propose
    elaborated metric learning methods to directly improve the main classification
    branch.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Additional-Classifier-Based Methods for Classification. FSCN (Li et al.,
    [2021e](#bib.bib69)) proposes a few-shot correction network (FSCN) as an additional
    classification branch of the model, which makes class predictions for the cropped
    region proposals with a pre-trained image classifier. These classification scores
    are used to refine the classification scores from the main branch. Besides, this
    paper proposes a semi-supervised distractor utilization method to select unlabeled
    distractor proposals for novel classes and a confidence-guided dataset pruning (CGDP)
    method for filtering out training images containing unlabeled objects of novel-classes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Contrastive-Learning-Based Methods for Classification. These methods (FSCE (Sun
    et al., [2021](#bib.bib106)), CoCo-RCNN (Ma et al., [2022](#bib.bib81)), FSRC (Shangguan
    et al., [2022](#bib.bib103))) adopt contrastive learning to assist the classification
    of region proposals.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FSCE introduces a contrastive loss to improve the classification performance
    of the model. FSCE proposes a contrastive loss function to maximize the similarity
    between objects of the same category and promote the distinctiveness of region
    proposals from different categories. This work is the first attempt to adopt contrastive
    learning into transfer-learning-based FSOD, which significantly improves the performance
    of the baseline TFA.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$ Knowledge-Graph-Based Methods for Classification. KR-FSOD (Wang and
    Chen, [2022](#bib.bib115)) proposes an additional classification branch based
    on an external knowledge graph with potential objects as nodes. The model predicts
    the category of each potential object according to the information of its nearby
    objects, which is extracted from this external knowledge graph. KR-FSOD improves
    the performance by incorporating the external knowledge graph into the FSOD model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于知识图谱的分类方法。KR-FSOD（Wang和Chen，[2022](#bib.bib115)）提出了一种基于外部知识图谱的附加分类分支，其中潜在对象作为节点。该模型根据从外部知识图谱中提取的附近对象的信息预测每个潜在对象的类别。KR-FSOD通过将外部知识图谱融入FSOD模型中来提高性能。
- en: $\bullet$ Semantic-Information-Based Methods for Classification. SRR-FSOD (Zhu
    et al., [2021a](#bib.bib158)) proposes an additional classification branch utilizing
    class semantic information to promote the classification, which utilizes the external
    semantic information into the FSOD model for higher performance. Specifically,
    SRR-FSOD projects the visual features into the semantic space using a linear projection.
    In this semantic space, multiple word embeddings are used as semantic embeddings
    to represent all base and novel classes. It generates class probabilities for
    the projected semantic embeddings by calculating the similarities between the
    projected visual features and the class semantic embeddings.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 基于语义信息的分类方法。SRR-FSOD（Zhu等，[2021a](#bib.bib158)）提出了一种利用类别语义信息促进分类的附加分类分支，将外部语义信息融入FSOD模型以提高性能。具体而言，SRR-FSOD通过线性投影将视觉特征映射到语义空间。在这个语义空间中，多个词嵌入作为语义嵌入表示所有基础和新类别。通过计算投影的视觉特征与类别语义嵌入之间的相似度，生成投影语义嵌入的类别概率。
- en: $\bigstar$ Discussion of Classification-Based Methods. Metric learning methods
    are early methods for FSOD with insufficient performance compared with the latest
    FSOD methods, indicating that simple modification on the RoI classifier is not
    enough for FSOD.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: $\bigstar$ 关于基于分类的方法的讨论。度量学习方法是早期的FSOD方法，与最新的FSOD方法相比性能不足，这表明仅对RoI分类器进行简单修改不足以满足FSOD的需求。
- en: Additional-classifier-based method (FSCN) achieves a large performance improvement.
    However, it requires a pre-trained image classifier, resulting in an unfair comparison
    with other FSOD methods.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 基于额外分类器的方法（FSCN）实现了显著的性能提升。然而，它需要一个预训练的图像分类器，这导致与其他FSOD方法的不公平比较。
- en: Contrastive-learning-based methods incur minimal additional cost during model
    training while yielding a substantial improvement in performance. Besides, they
    can be seamlessly integrated into other FSOD methods.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对比学习的方法在模型训练过程中产生的额外成本极小，同时带来了显著的性能提升。此外，它们可以无缝地集成到其他FSOD方法中。
- en: Knowledge-graph-based method (KR-FSOD) is well motivated, but the performance
    is currently not promising. Additionally, like FSCN, it cannot be readily applied
    to novel classes in real-world FSOD applications due to the unavailability of
    corresponding knowledge graphs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 基于知识图谱的方法（KR-FSOD）有很好的动机，但目前性能不尽如人意。此外，与FSCN类似，由于缺乏相应的知识图谱，它不能很容易地应用于实际FSOD应用中的新类别。
- en: Semantic-information-based method (SRR-FSOD) serves as a bridge between FSOD
    and zero-shot learning by incorporating class semantic information into the model.
    This approach has the potential for enhancing performance with the large-scale
    cross-modal models. Nevertheless, it may not be suitable for novel classes that
    haven’t been learned before.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 基于语义信息的方法（SRR-FSOD）通过将类别语义信息融入模型，作为FSOD和零样本学习之间的桥梁。这种方法具有通过大规模跨模态模型提升性能的潜力。然而，它可能不适用于尚未学习过的新类别。
- en: 4.2.3\. Regression-Based Methods
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 基于回归的方法
- en: Regression-based methods focus on improving the regression branch of detection
    model. SRR-FSD (Kim et al., [2022](#bib.bib54)) proposes a refinement approach
    to improve the regression of region proposals in RPN. Specifically, SRR-FSD expands
    the regression branch into multiple successive regression heads. Each regression
    head receives the region proposals generated from the preceding regression head
    and continues to refine these region proposals for generating more positive samples.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 回归基方法专注于改进检测模型的回归分支。SRR-FSD (Kim et al., [2022](#bib.bib54)) 提出了一种改进方法，以提高 RPN
    中区域提议的回归精度。具体来说，SRR-FSD 将回归分支扩展为多个连续的回归头。每个回归头接收来自前一个回归头生成的区域提议，并继续优化这些区域提议，以生成更多的正样本。
- en: $\bigstar$ Discussion of Regression-Based Methods. While the performance of
    the current regression-based method (SRR-FSD) is currently not ideal, it’s important
    to note that such methods are still rare, and there is ample opportunity for future
    exploration and improvement.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: $\bigstar$ 回归基方法的讨论。虽然目前回归基方法（SRR-FSD）的性能不理想，但值得注意的是，这类方法仍然比较少见，未来有很大的探索和改进空间。
- en: 4.2.4\. RPN-Based Methods
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 基于 RPN 的方法
- en: CoRPN (Zhang et al., [2020b](#bib.bib150)) improves the RPN in Faster R-CNN
    for standard FSOD. CoRPN assumes that the RPN pre-trained on base classes will
    miss some objects of novel classes. Therefore, it uses multiple foreground-background
    classifiers in RPN instead of the original single one to mitigate this problem.
    During testing, a given proposal box is assigned with the score from the most
    certain RPN. During training, only the most certain RPN will get the gradient
    from the corresponding bounding box. CoRPN proposes a diversity loss to encourage
    the diversity of these RPNs and a cooperation loss to mitigate firm rejection
    of foreground proposals.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: CoRPN (Zhang et al., [2020b](#bib.bib150)) 改进了 Faster R-CNN 中的 RPN，以应对标准 FSOD。CoRPN
    假设在基础类别上预训练的 RPN 会遗漏一些新类别的物体。因此，它在 RPN 中使用多个前景-背景分类器，而不是原来的单一分类器，以缓解这个问题。在测试期间，给定的提议框将分配给最确定的
    RPN 的得分。在训练期间，只有最确定的 RPN 会从相应的边界框中获得梯度。CoRPN 提出了一个多样性损失来鼓励这些 RPN 的多样性，并提出了一个合作损失来减轻对前景提议的坚决拒绝。
- en: $\bigstar$ Discussion of RPN-Based Methods. RPN-based method (CoRPN) directly
    devises multiple RPNs to retrieve those missed novel objects, which addresses
    the problem that novel objects tend to be missed by the RPN trained on the base
    dataset. However, it is limited in R-CNN-based model, and it is unclear whether
    it still works when integrated into other FSOD models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: $\bigstar$ 基于 RPN 的方法的讨论。基于 RPN 的方法 (CoRPN) 直接设计多个 RPN 来检索那些被遗漏的新物体，这解决了由基础数据集训练的
    RPN 常常遗漏新物体的问题。然而，这在基于 R-CNN 的模型中有限，尚不清楚当集成到其他 FSOD 模型中时是否仍然有效。
- en: 4.2.5\. Data-Augmentation-Based Methods
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. 基于数据增强的方法
- en: 'Data-augmentation-methods aim to generate more samples for each novel class,
    thus directly tackling the data-scarce problem of few-shot setting. Current data
    augmentation methods can be divided into two categories: sample generation in
    the input-pixel space and sample generation in the feature space. The former type
    directly generates samples in the input-pixel space that are understandable and
    perceivable by humans, which can be further divided into multi-scale augmentation
    methods and novel-instance-mining methods. The latter type synthesizes more deep
    features for the novel classes, which can be further divided into distribution
    inheritance methods and generator-based methods.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强方法旨在为每个新类别生成更多样本，从而直接解决少样本设置中的数据稀缺问题。当前的数据增强方法可以分为两类：输入像素空间的样本生成和特征空间的样本生成。前一种类型直接生成在输入像素空间中的样本，这些样本对人类而言是可理解和可感知的，可以进一步分为多尺度增强方法和新实例挖掘方法。后一种类型则合成更多的新类别深层特征，可以进一步分为分布继承方法和基于生成器的方法。
- en: $\bullet$ Sample Generation In the Input-Pixel Space $\rightarrow$ Multi-Scale
    Augmentation Methods. MPSR (Wu et al., [2020](#bib.bib123)) and FSSP (Xu et al.,
    [2021](#bib.bib128)) both apply data augmentation to enrich the scales of positive
    samples.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 在输入像素空间生成样本 $\rightarrow$ 多尺度增强方法。MPSR (Wu et al., [2020](#bib.bib123))
    和 FSSP (Xu et al., [2021](#bib.bib128)) 都应用了数据增强技术来丰富正样本的尺度。
- en: •
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MPSR claims that although feature pyramid network (FPN) (Lin et al., [2017a](#bib.bib71))
    may mitigate the scale variation issue, it cannot address the sparsity of scale
    distribution in FSOD. Therefore, MPSR proposes a strategy to directly augment
    the scales of objects in the input pixel space, which extracts each positive object
    independently and resizes them to multiple scales. The augmented multi-scale samples
    are fed into the RPN module and detection heads for training.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$ Sample Generation In the Input-Pixel Space $\rightarrow$ Novel-Instance-Mining
    Methods. MINI (Cao et al., [2022](#bib.bib8)), PSEUDO (Kaul et al., [2022](#bib.bib51)),
    Decoupling (Gao et al., [2022a](#bib.bib26)), and N-PME (Liu et al., [2022b](#bib.bib78))
    excavate the unlabeled novel objects in the dataset for data augmentation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Sample Generation In the Feature Space $\rightarrow$ Distribution
    Inheritance Methods. FSOD-KD (Zhao et al., [2022b](#bib.bib154)), PDC (Li et al.,
    [2022a](#bib.bib58)), and FSOD-DIS (Wu et al., [2022](#bib.bib124)) generate more
    novel features by transferring the feature distribution from the base dataset
    for data augmentation, which stem from the same few-shot learning method (Yang
    et al., [2021b](#bib.bib135)). Specifically, these methods assume that the feature
    distribution of a class can be approximated as a Gaussian distribution and similar
    classes have similar feature distributions. Therefore, they calculate the feature
    distribution of base classes using their abundant samples and estimate the feature
    distribution of each novel class according to their nearest base classes. Finally,
    these methods sample more novel features from the estimated feature distribution
    and use them for training.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Sample Generation In the Feature Space $\rightarrow$ Generator-Based
    Methods. Halluc (Zhang and Wang, [2021](#bib.bib149)) aims to synthesize additional
    RoI features for novel classes. It proposes a simple hallucinator to generate
    hallucinated RoI features, implemented as a simple two-layer MLP. In the base-training
    stage, Halluc first trains a Faster R-CNN on the base dataset as regular object
    detection. Then, it freezes the parameters of the detector and pre-trains the
    hallucinator with a classification loss for the synthesized samples. Next, in
    the few-shot fine-tuning stage, Halluc unfreezes the parameters of detection heads (classification
    head & regression head) and adopts an EM-like algorithm to train the hallucinator
    and detection heads alternately. It is noted that this method shows impressive
    performance when the number of training samples is extremely small. However, its
    superiority over baseline methods such as TFA cannot be guaranteed as the number
    of training samples increases.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: $\bigstar$ Discussion of Data-Augmentation-Based Methods. Methods for sample
    generation in the input-pixel space are categorized into multi-scale augmentation
    methods and novel instance mining methods. Multi-scale augmentation methods are
    effective data-augmentation methods for FSOD, and they are easy to implement.
    However, conducting data-augmentation only on the aspect of scale does not tackle
    the core of data-scarcity problem of FSOD, and they are early FSOD methods with
    insufficient performance. For the novel instance mining methods, it is true that
    on current FSOD benchmarks, many objects from novel classes indeed exist in the
    images without annotation. Capturing these objects effectively mitigates the data-scarcity
    problem in FSOD and significantly improves the performance. These methods have
    great potential to be integrated into other FSOD methods. However, this setting
    is not realistic. In real-life FSOD, it is not guaranteed that the images of the
    base dataset contain objects from novel classes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Methods for sample generation in the feature space are categorized into two
    categories: distribution inheritance methods and generator-based methods. The
    former type effectively generates more samples for novel classes using the data
    distribution from the data-abundant base classes. It introduces no extra parameters
    and can be considered as a “free lunch” from the base dataset. However, it is
    not applicable in the real-world scenario that there is a significant difference
    between the data distribution of the base classes and novel classes. The latter
    type is more suitable for the scenario that base classes and novel classes differ
    a lot, but it introduces an extra generator which may increase the burden for
    model training.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.6\. Pre-Train-Based Methods
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Almost all transfer-learning methods adopt a backbone pre-trained on ImageNet
    before the base training stage. Some methods (DETReg (Bar et al., [2022](#bib.bib3)),
    imTED (Zhang et al., [2022b](#bib.bib151))) focus on improving this pre-training
    stage.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DETReg pre-trains a DETR model in an unsupervised manner. On the one hand, it
    uses Selective Search (Uijlings et al., [2013](#bib.bib112)) to excavate object
    proposals and uses them to train the object localization branch of the model.
    On the other hand, it uses another pre-trained self-supervised model to generate
    object encodings and enforces the DETR model to mimic these object encodings.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: imTED integrally migrates a pre-trained MAE model (He et al., [2022](#bib.bib41))
    to be a detection model. Concretely, imTED adds a region proposal network and
    a detection head into the MAE model following the design of Faster R-CNN. Besides,
    it proposes a multi-scale feature modulator to fuse multi-scale features extracted
    from a FPN (Lin et al., [2017a](#bib.bib71)).
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bigstar$ Discussion of Pre-Train-Based Methods. These methods explore the
    current FSOD problem in a new perspective that pursues a stronger backbone before
    the few-shot training stage, while current FSOD methods most simply adopt a backbone
    pre-trained with a classification task on ImageNet. Besides, the performance of
    these methods is significantly superior to other methods. However, these methods
    require a stronger pre-trained backbone (DETReg requires SwAV, and imTED requires
    MAE). Besides, these methods never clarify whether these stronger backbones cover
    the knowledge of novel classes in the FSOD setting, which will bring an unfair
    comparison with other FSOD methods.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Fine-Tune-Free Methods
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tune-free methods focus on directly transferring the trained model from
    the base dataset to the novel dataset without fine-tuning. Existing fine-tune-free
    methods (AirDet (Li et al., [2022b](#bib.bib59)), FS-DETR (Bulat et al., [2022](#bib.bib6)))
    adopt the scheme of meta-learning, and they also focus on the method of feature
    aggregation. Specifically, AirDet conducts integral feature aggregation with element-wise
    multiplication and concatenation operations, and it proposes to learn the weights
    of different support samples instead of treating them as equals. Besides, AirDet
    aggregates RoI features with support features for the regression branch. FS-DETR
    concatenates query features with support features into a common sequence and feeds
    it into the DETR model. FS-DETR proposes the learnable pseudo-class embeddings
    with the same shape as support features and adds them into support features to
    facilitate the model training.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: $\bigstar$ Discussion of Fine-Tune-Free Methods. Fine-tune-free method requires
    less computation cost and are more suitable for real life. However, the performance
    of these methods is currently not ideal compared to the fine-tune-based methods.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Zero-Shot Object Detection
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zero-Shot Object Detection (ZSOD) is an extreme scenario of LSOD that novel
    classes do not contain any image sample. Concretely, the training dataset (base
    dataset $D_{B}$) of ZSOD consists of abundant annotated instances of base classes
    $C_{B}$, and the test dataset (novel dataset $D_{N}$) does not consist of annotated
    instances of novel classes $C_{N}$ ($C_{B}$ and $C_{N}$ are not intersected).
    As a substitute, ZSOD utilizes semantic information to assist in detecting objects
    of novel classes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'According to whether utilizing unlabeled test images for model training, this
    survey categorizes ZSOD into two domains: “transductive ZSOD” and “inductive ZSOD”.
    Inductive ZSOD is the mainstream of ZSOD, which does not require accessing the
    test images in advance. Differently, transductive ZSOD is rarely explored, which
    utilizes unlabeled test images to assist model training. Furthermore, inductive
    ZSOD is categorized according to the type of semantic information: semantic attributes
    and textual description. The former type utilizes the semantic attributes (word
    vector) as the auxiliary semantic information to represent each class. In contrast,
    the latter type utilizes the textual description (e.g., a description sentence
    for an image or a class) as the auxiliary semantic information. This section gives
    a comprehensive introduction to semantic-attributes-based inductive ZSOD (standard
    ZSOD). Textual-description-based inductive ZSOD and transductive ZSOD will be
    discussed in the later sections.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Current semantic-attributes-based inductive ZSOD methods adopt Faster R-CNN
    or YOLO-style model as the detection framework, as shown in [Figure 8](#S5.F8
    "Figure 8 ‣ 5\. Zero-Shot Object Detection ‣ A Survey of Deep Learning for Low-Shot
    Object Detection"). Ankan Bansal et al. (Bansal et al., [2018](#bib.bib2)) propose
    one of the earliest methods for semantic-attributes-based inductive ZSOD based
    on Faster R-CNN. This work first establishes a simple baseline built on Faster
    R-CNN, which uses a simple linear projection to project RoI features $v_{r}$ into
    semantic space and calculates the class probabilities of $v_{r}$ as the cosine
    similarities between the projected semantic embeddings $s_{r}$ and the semantic
    attributes of each class. As one of the earliest methods for ZSOD, this work sets
    up a benchmark adopted by many future works.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ff9dcf40d2500919497e479b424385d.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: (a) ZSOD model based on Faster R-CNN
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/33f0359cbadf276350e99bae02dcc45d.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: (b) ZSOD model based on YOLO-style model
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8\. Overview of two detection frameworks for ZSOD methods: Faster R-CNN
    and YOLO-style model. Most of the current methods apply a visual-semantic mapping
    operation to project visual features into semantic space and compare these projected
    semantic embeddings with class semantic embeddings for classification.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ ZS-YOLO (Zhu et al., [2020b](#bib.bib160)) is another early work for
    semantic-attributes-based inductive ZSOD based on YOLOv2. It projects each cell
    in the feature map into semantic embeddings for class prediction. Compared to
    the contemporaneous work (Bansal et al., [2018](#bib.bib2)), ZS-YOLO adopts a
    different detection framework, and it does not require external training data
    and semantic embeddings of background class. However, these two methods are evaluated
    using different dataset settings, making it difficult to directly compare their
    performance.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: As the forerunners of two mainstream detection frameworks for semantic-attributes-based
    inductive ZSOD, the above two methods (Bansal et al., [2018](#bib.bib2); Zhu et al.,
    [2020b](#bib.bib160)) are followed by many future works. The later methods mainly
    follow their framework with some extensions on different components of the framework.
    According to the modified components they focus on, this survey categorizes semantic-attributes-based
    inductive ZSOD methods into semantic relation methods, data augmentation methods,
    and visual-semantic mapping methods.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Semantic Relation Methods
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Semantic relation methods utilize the semantic relation between classes to detect
    objects of novel classes, which are further categorized into base-novel class
    relation and super-class relation. Methods based on base-novel class relation
    utilize semantic similarities between base classes and novel classes to transfer
    knowledge from base classes to novel classes. Methods based on super-class relation
    assume that there is a hierarchical relationship among categories, i.e., some
    similar classes can be grouped into a super-class (e.g., bed, sofa, and chair
    can be grouped into furniture), and they utilize this hierarchical relationship
    to assist prediction.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1\. Base-Novel Class Relation
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6e4ccb0baadd9a88d2acd22627f1f46b.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9\. Illustration of semantic relation methods: base-novel class relation
    and super-class relation.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Methods based on base-novel class relation can be categorized into two types:
    linear-transform-based methods (TOPM-ZSOD (Shao et al., [2019](#bib.bib104)),
    LSA-ZSOD (Wang et al., [2020b](#bib.bib116)), DPIF (Li et al., [2021b](#bib.bib67))),
    and graph-based methods (SPGP (Yan et al., [2020](#bib.bib130)), VSRG (Nie et al.,
    [2022](#bib.bib86)), CRF-ZSOD (Luo et al., [2020](#bib.bib80))). Linear-transform-based
    methods utilize the base-novel semantic relation to assist prediction through
    linear transforms of these semantic relations, and graph-based methods construct
    graphs with each node as a category, towards fully excavating the relation between
    base classes and novel classes through graph neural networks or conditional random
    fields.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Base-Novel Class Relation $\rightarrow$ Linear-Transform-Based Methods.
    TOPM-ZSOD (Shao et al., [2019](#bib.bib104)), LSA-ZSOD (Wang et al., [2020b](#bib.bib116)),
    and DPIF (Li et al., [2021b](#bib.bib67)) are all linear-transform-based methods
    to utilize base-novel class relation for ZSOD.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Base-Novel Class Relation $\rightarrow$ Graph-Based Methods. SPGP (Yan
    et al., [2020](#bib.bib130)), VSRG (Nie et al., [2022](#bib.bib86)), and CRF-ZSOD (Luo
    et al., [2020](#bib.bib80)) are graph-based methods to better excavate the relation
    between base and novel classes into the classification branch for higher performance.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: $\bigstar$ Discussion of Methods Based on Base-Novel Class Relation. Linear-transform-based
    methods are simple approaches to directly utilize the base-novel semantic relation
    to assist prediction. However, linear transform does not fully excavate the base-novel
    relation for prediction, and it does not connect RoI features with the class semantic
    attributes together. Graph-based methods deeply excavate the relation between
    base classes and novel classes for prediction through graph neural networks or
    conditional random fields. Although they improve the performance through the graph
    structure modeling the relation between categories, they haven’t provided a quantitative
    analysis of whether the trained graph matches human intuition.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2\. Super-Class Relation
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Methods based on super-class relation (CG-ZSOD (Li et al., [2020a](#bib.bib68)),
    JRLNC-ZSOD (Rahman et al., [2020b](#bib.bib98)), ACS-ZSOD (Ma et al., [2020](#bib.bib82)))
    define some coarse-grained classes (super-classes) to cluster all classes into
    several groups, which separate the original classification problem into two sub-problems (coarse-grained
    classification and fine-grained classification).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: $\bigstar$ Discussion of Methods Based on Super-Class Relation. These methods
    provide “free lunch” for the performance improvement of ZSOD, but they are unsuitable
    for situation where there is no hierarchical relationship between categories.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Visual-Semantic Mapping Methods
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Visual-semantic mapping methods aim to find a proper mapping function to align
    visual features with the class semantic attributes. Visual-semantic mapping methods
    can be categorized into linear-projection-based methods (e.g., LSA-ZSOD (Wang
    et al., [2020b](#bib.bib116)), DPIF (Li et al., [2021b](#bib.bib67)), ZSDTR (Zheng
    and Cui, [2021](#bib.bib155))), weighted-combination-based methods (HRE-ZSOD (Demirel
    et al., [2018](#bib.bib17))), inverse-mapping methods (MS-ZSOD (Gupta et al.,
    [2020](#bib.bib34)), CCFA-ZSOD (Li et al., [2022c](#bib.bib62)), SMFL-ZSOD (Li
    et al., [2021d](#bib.bib64))), auxiliary-loss-based methods (ContrastZSOD (Yan
    et al., [2022](#bib.bib129)), VSA-ZSOD (Rahman et al., [2020a](#bib.bib96))),
    external-resource-based methods (CLIP-ZSOD (Xie and Zheng, [2022](#bib.bib126)),
    BLC (Zheng et al., [2020](#bib.bib156))).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Linear-Projection-Based Methods. The earliest ZSOD method (Bansal
    et al., [2018](#bib.bib2)) adopts this simplest visual-semantic mapping method
    that projects visual features into semantic space through a linear projection,
    which is followed by many ZSOD methods (e.g., LSA-ZSOD (Wang et al., [2020b](#bib.bib116)),
    DPIF (Li et al., [2021b](#bib.bib67)), ZSDTR (Zheng and Cui, [2021](#bib.bib155))).
    These methods are mostly based on CNN backbones, and only ZSDTR adopts DETR (Carion
    et al., [2020](#bib.bib9)) (a vision-transformer-based detector) which projects
    the proposal encodings into semantic space.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Weighted-Combination-Based Methods. HRE-ZSOD (Demirel et al., [2018](#bib.bib17))
    calculates the semantic embeddings $s_{r}$ of the RoI feature $v_{r}$ as the weighted
    combination of different semantic attributes from all base classes $C_{B}$ according
    to their classification scores, as shown in [Equation 9](#S5.E9 "9 ‣ 5.2\. Visual-Semantic
    Mapping Methods ‣ 5\. Zero-Shot Object Detection ‣ A Survey of Deep Learning for
    Low-Shot Object Detection") ($p_{c}$ denotes the probability that this RoI is
    predicted to be the base class $c$).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $s_{r}=\frac{1}{\sum\limits_{c\in C_{B}}p_{c}}\sum\limits_{c\in
    C_{B}}p_{c}s_{c}\text{,}$ |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: $\bullet$ Inverse-Mapping Methods. Inverse-mapping methods (MS-ZSOD (Gupta et al.,
    [2020](#bib.bib34)), CCFA-ZSOD (Li et al., [2022c](#bib.bib62)), SMFL-ZSOD (Li
    et al., [2021d](#bib.bib64))) conversely project class semantic attributes into
    visual space to align the class semantic attributes with the visual features.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Auxiliary-Loss-Based methods. Auxiliary-loss-based methods (ContrastZSOD (Yan
    et al., [2022](#bib.bib129)), VSA-ZSOD (Rahman et al., [2020a](#bib.bib96))) propose
    some auxiliary losses to facilitate the visual-semantic mapping.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ External-Resource-Based Methods. These methods utilize external resources (CLIP-ZSOD (Xie
    and Zheng, [2022](#bib.bib126)), BLC (Zheng et al., [2020](#bib.bib156))) to better
    project visual features into semantic space. Specifically, CLIP-ZSOD utilizes
    a strong pre-trained CLIP model (Radford et al., [2021](#bib.bib94)) for visual-semantic
    mapping, and BLC adopts external vocabulary for visual-semantic mapping.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Data Augmentation Methods
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data augmentation methods aim to generate multiple visual features for novel
    classes to mitigate the data-scarcity problem. The generated features are used
    to re-train the classifier of the detection model. Early data augmentation methods (DELO (Zhu
    et al., [2020a](#bib.bib159))) train a conditional generator with some auxiliary
    losses for data generalization, and later methods (GTNet (Zhao et al., [2020](#bib.bib152)),
    SYN-ZSOD (Hayat et al., [2020](#bib.bib40)), RSC-ZSOD (Sarma et al., [2022](#bib.bib102)),
    RRFS-ZSOD (Huang et al., [2022](#bib.bib47))) all adopt GAN (generative adversarial
    network).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ DELO (Zhu et al., [2020a](#bib.bib159)) adopts a conditional generator
    to synthesize visual features for novel classes. Specifically, the generator consists
    of an encoder to extract the latent features of the corresponding semantic embeddings,
    and a decoder to synthesize the visual features from the latent features. DELO
    adopts the conditional VAE loss to train this generator, including a KL divergence
    loss and a reconstruction loss. Besides, it proposes three additional losses to
    encourage the consistency between the reconstructed visual features and the original
    visual features.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ GTNet (Zhao et al., [2020](#bib.bib152)), SYN-ZSOD (Hayat et al.,
    [2020](#bib.bib40)), RSC-ZSOD (Sarma et al., [2022](#bib.bib102)), and RRFS-ZSOD (Huang
    et al., [2022](#bib.bib47)). These methods all adopt GAN (generative adversarial
    network) to generate visual features for novel classes. The GAN consists of a
    generator to synthesize visual features and a discriminator to determine whether
    the visual features are synthesized or not. These methods propose some elaborated
    extensions on this framework respectively.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: $\bigstar$ Discussion of Data Augmentation Methods. Data augmentation methods
    directly tackle the data-scarcity problem in ZSOD in an intuitive way. Actually,
    data augmentation methods can be seen as the inverse of visual-semantic mapping
    methods (i.e., mapping the class semantic attributes back into visual features).
    An important difference is that data augmentation methods incorporate intra-class
    variance into this mapping process, i.e., these methods generate different image
    features from different random noises for the same class. However, these methods
    can only synthesize visual features instead of visual samples (in the input pixel
    space), making it hard to interpret or visualize the synthesized samples. Besides,
    it is possible to substitute these methods by inverting the visual-semantic mapping
    functions.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Extensional Zero-Shot Object Detection
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1\. Open-Vocabulary Object Detection
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Conventional ZSOD only learns to align visual features with semantic information
    for detection from a small set of base classes ($C_{B}$) and generalizes to the
    novel classes ($C_{N}$), while Open-Vocabulary Object Detection (OVD) first accesses
    a much larger dataset (consisting of massive image-text pairs from multiple classes
    $C_{O}$) to train a stronger visual-semantic mapping function for multiple classes (intersecting
    with the base and novel classes for the later ZSOD task). We provide a detailed
    analysis of OVD in section S3 of the supplementary online-only material.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Textual-Description-Based Inductive ZSOD
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Previous ZSOD methods use semantic attributes as semantic information to represent
    each class. Instead, textual-description-based methods use textual description
    as semantic information. Currently, only a few methods uncover textual-description-based
    inductive ZSOD, and they use different types of textual-description: class textual
    description (description text for each class) and image textual description (description
    text for each image).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Methods Based on Class Textual Description. ZSOD-TD (Li et al., [2019](#bib.bib70))
    adopts textual description to represent each class instead of semantic attributes (e.g.,
    “stripe, equid” is used to describe zebra). ZSOD-TD projects the RoI features
    into semantic embeddings and makes predictions by comparing them with the features
    extracted from textual description.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Methods Based on Image Textual Description. In addition to the class
    textual description, ZSOD-CNN (Zhang et al., [2020c](#bib.bib143)) adopts textual
    description to represent each image (e.g., “A bathroom with a sink and three towels.”),
    which also adopts Faster R-CNN as the detection framework. It uses a text CNN
    to extract text features, and concatenates the RoI features with the text features
    for further predictions. Besides, this method utilizes the OHEM technique to select
    hard samples for model training. During testing, it predicts the classification
    scores of novel classes according to those of base classes according to the semantic
    similarities between base and novel classes.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Transductive ZSOD
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ Transductive ZSOD (Rahman et al., [2019](#bib.bib95)). Transductive
    ZSOD is an extended setting of inductive ZSOD, which incorporates unlabeled test
    images into model training. Rahman et al. (Rahman et al., [2019](#bib.bib95))
    propose the first work to uncover transductive ZSOD, which conducts transductive
    learning on a pre-trained ZSOD model. For transductive learning, it applies a
    pseudo-labeling paradigm on the unlabeled data, including a fixed pseudo-labeling
    step to generate fixed pseudo-labels for base classes using the pre-trained model,
    and a dynamic pseudo-labeling step to generate pseudo-labels for both base classes
    and novel classes iteratively. This work is the first to explore transductive
    learning on ZSOD, which shows promising potential for significant performance
    improvement, as other transductive methods in few-shot image classification.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Popular Benchmarks For Low-Shot Object Detection
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1\. Dataset Overview
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In three settings (i.e., OSOL, FSOD, and ZSOD) of LSOD, the classes of the
    dataset are all split into two types: base classes with large labeled samples
    and novel classes with few or no labeled samples. The mainstream benchmarks for
    Low-Shot Object Detection are modified from widely-used object detection datasets
    like the PASCAL VOC dataset, MS COCO dataset. This survey summarizes the basic
    information of mainstream benchmarks for LSOD in [Table 2](#S7.T2 "Table 2 ‣ 7.2\.
    Evaluation Criteria ‣ 7\. Popular Benchmarks For Low-Shot Object Detection ‣ A
    Survey of Deep Learning for Low-Shot Object Detection") but omits some rarely-used
    benchmarks since they are not representative. In this table, the number of base
    classes, the number of novel classes, and the number of labeled samples per category
    for each benchmark are recorded. Moreover, split number denotes the number of
    category split schemes for each benchmark.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Evaluation Criteria
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OSOL. OSOL has a guarantee that the model knows precisely the object classes
    contained in each test image. For each test image in the test stage, OSOL randomly
    samples one support image for each category existing in this image to locate the
    objects of this category and average their accuracy scores as the final results.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: FSOD. Different from OSOL, FSOD methods randomly sample a small set of support
    samples for the whole test set instead of only one image. For the K-shot setting,
    some methods like LSTD (Chen et al., [2018](#bib.bib12)) sample K support images
    for each novel category. This sampling strategy is not ideal since the number
    of objects in the images may differ. Current methods mostly sample K bounding
    boxes for each novel category instead, and this survey records the performance
    of FSOD methods under this setting. Early FSOD methods mostly adopt the support
    samples released by FSRW (Kang et al., [2019](#bib.bib48)) for fair performance
    comparison, which are sampled only once. TFA (Wang et al., [2020a](#bib.bib119))
    samples support samples multiple times to obtain the average performance of the
    model. Currently, newly proposed FSOD methods mostly adopt this multiple sampling
    strategy to obtain more accurate performance.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: ZSOD. ZSOD methods adopt two evaluation criteria for model performance comparison.
    The first criterion evaluates the model on a subset of test data that contains
    only objects of novel classes (ZSOD). The second setting, generalized ZSOD (GZSOD),
    evaluates the model on the complete test data, requiring the model to detect objects
    of both base classes and novel classes. Generalized ZSOD separately computes the
    mean average precision and recall of base classes and novel classes and uses a
    harmonic average to generate the average performance.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'It is noted that the class semantic attributes for ZSOD are mainly borrowed
    from pre-trained word vectors or manually designed attributes: GloVe (300-dim) (Pennington
    et al., [2014](#bib.bib90)), BERT (768-dim) (Devlin et al., [2019](#bib.bib18)),
    word2vec (300-dim) (Mikolov et al., [2013](#bib.bib84)), fastText (Bojanowski
    et al., [2017](#bib.bib5)) and aPaY (64-dim) (Farhadi et al., [2009](#bib.bib23)).
    Among them, aPaY contains manually designed attributes, and others contain pre-trained
    word vectors.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Summation Of Mainstream Benchmarks for Low-Shot Object Detection
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '| LSOD Type | Dataset | Base Classes | Novel Classes | Shots Per Category |
    Split Number |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| OSOL | PASCAL VOC Dataset | 16 | 4 | 1 | 1 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| MS COCO Dataset | 60 | 20 | 1 | 4 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| FSOD | PASCAL VOC Dataset | 15 | 5 | 1, 2, 3, 5, 10 | 3 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| MS COCO Dataset | 60 | 20 | 10, 30 | 1 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| ZSOD | PASCAL VOC Dataset | 16 | 4 | 0 | 1 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| MS COCO Dataset | 48 | 17 | 0 | 1 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| MS COCO Dataset | 65 | 15 | 0 | 1 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: 7.3\. Evaluation Metrics
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '$\bullet$ Preliminaries for the calculation of evaluation metrics:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Intersection over Union (IoU). Intersection over Union (IoU) is a value that
    measures the degree of overlap between two bounding boxes. Specifically, let $\mathrm{bbox}_{1}\cap\mathrm{bbox}_{2}$
    and $\mathrm{bbox}_{1}\cup\mathrm{bbox}_{2}$ respectively denote the area of overlap
    and union of two bounding boxes $\mathrm{bbox}_{1}$ and $\mathrm{bbox}_{2}$, the
    IoU between them $\mathrm{IoU}(\mathrm{bbox}_{1},\mathrm{bbox}_{2})$ is calculated
    as $\mathrm{IoU}(\mathrm{bbox}_{1},\mathrm{bbox}_{2})=\frac{\mathrm{bbox}_{1}\
    \cap\ \mathrm{bbox}_{2}}{\mathrm{bbox}_{1}\ \cup\ \mathrm{bbox}_{2}}$. Two bounding
    boxes are considered to be matched if their IoU is larger than a pre-determined
    threshold $\mu$.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ The evaluation metrics for LSOD:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Precision. Precision is the fraction of correctly retrieved bounding boxes out
    of total retrieved bounding boxes.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Recall@K. In converse to Precision, Recall is the fraction of correctly retrieved
    bounding boxes out of total ground-truth bounding boxes (K denotes the number
    of total retrieved bounding boxes).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: mAP50. AP50 (average precision with $\mu=0.5$) is the precision averaged over
    different levels of recall. Let $\mathrm{Prec}\ ({\mathrm{recall\_value}})$ denote
    the precision when “$\mathrm{recall\_value}$” is achieved, and AP50 is calculated
    averaged over some specific values $\mathcal{R}$ of recall ($\mathcal{R}=\{0,0.1,0.2,...,1.0\}$
    is usually selected), as shown in [Equation 10](#S7.E10 "10 ‣ 7.3\. Evaluation
    Metrics ‣ 7\. Popular Benchmarks For Low-Shot Object Detection ‣ A Survey of Deep
    Learning for Low-Shot Object Detection"). AP50 is calculated for each category
    and their results are averaged as the final mAP50 (mean average precision with
    $\mu=0.5$). Note that mAP50 is commonly adopted on the PASCAL VOC benchmark.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $\mathrm{AP50}=\frac{1}{&#124;\mathcal{R}&#124;}\sum\limits_{\mathrm{recall\_value}\in\mathcal{R}}\mathrm{Prec}\
    (\mathrm{recall\_value}).$ |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: 'mAP. mAP is the extension of mAP50 that is averaged over ten IoU thresholds:
    $\{0.5,0.55,0.60,...\ ,\\'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 0.95\}" display="inline"><semantics ><mrow ><mo stretchy="false" >{</mo><mn  >0.5</mn><mo
    >,</mo><mn >0.55</mn><mo  >,</mo><mn >0.60</mn><mo >,</mo><mi mathvariant="normal"
    >…</mi><mo lspace="0.500em" >,</mo><mn  >0.95</mn><mo stretchy="false"  >}</mo></mrow><annotation-xml
    encoding="MathML-Content" ><set ><cn type="float"  >0.5</cn><cn type="float"  >0.55</cn><cn
    type="float"  >0.60</cn><ci >…</ci><cn type="float" >0.95</cn></set></annotation-xml><annotation
    encoding=$, which is commonly adopted on the MS COCO benchmark.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Performance
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section demonstrates and analyzes the performance of different Low-Shot
    Object Detection methods on the most widely-used benchmarks.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 8.1\. One-Shot Object Localization
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 3](#S8.T3 "Table 3 ‣ 8.1\. One-Shot Object Localization ‣ 8\. Performance
    ‣ A Survey of Deep Learning for Low-Shot Object Detection") lists the performance
    of current OSOL methods on the PASCAL VOC benchmark and the MS COCO benchmark (the
    results on the MS COCO benchmark are averaged over $4$ splits). SiamFC and SiamRPN
    are two methods initially proposed for video object tracking, which are the baselines
    for OSOL, and their performance is reasonably poor than authentic OSOL methods.
    SiamMask, OSCD, OSOLwT, and FOC OSOL use simple concatenation-based methods for
    feature aggregation with different modifications. These methods significantly
    outperform SiamFC & SiamRPN, but they have performance inferior to the attention-based
    methods, and FOC OSOL achieves the best performance among these methods on the
    PASCAL VOC benchmark. Differently, recently proposed methods (CoAE, ADA OSOL,
    AUG OSOL, AIT, CAT, BHRL, SaFT, ABA OSOL) most adopt the attention mechanism for
    feature aggregation, and CAT is the best method among them. Moreover, CAT (a transformer-based
    method) achieves $4.5$ points better than FOC OSOL on the PASCAL VOC benchmark,
    which indicates that attention-based methods are more promising for future One-Shot
    Object Localization.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Performance (mAP50) of OSOL methods on novel classes. On each benchmark,
    the red font denotes the best performance, and the gray font denotes the second-best
    performance. Note that PASCAL VOC has only one class split, while the results
    on MS COCO are averaged over four different class splits. R-50 & R-101 denotes
    ResNet-50 & ResNet-101.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Method | Detector (Backbone) | PASCAL VOC | MS COCO |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| Object Tracking Methods | SiamFC (2018) (Cen and Jung, [2018](#bib.bib10))
    | SNet & ENet (VGG-16) | 13.3 | N/A |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: '| SiamRPN (2018) (Li et al., [2018](#bib.bib60)) | Faster R-CNN (AlexNet) |
    14.2 | N/A |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| Concatenation-Based Methods | SiamMask (2019) (Michaelis et al., [2018](#bib.bib83))
    | Faster R-CNN (R-50) | N/A | 16.8 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| OSCD (2020) (Fu et al., [2021](#bib.bib25)) | Faster R-CNN (AlexNet) | 52.1
    | N/A |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| OSOLwT (2020) (Li et al., [2020c](#bib.bib66)) | Faster R-CNN (R-50) | 69.1
    | N/A |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| FOC OSOL (2021) (Yang et al., [2021a](#bib.bib133)) | Faster R-CNN (R-50)
    | 71.0 | N/A |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| Attention-Based Methods | CoAE (2019) (Hsieh et al., [2019](#bib.bib44))
    | Faster R-CNN (R-50) | 68.2 | 22.0 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| ADA OSOL (2022) (Zhang et al., [2022a](#bib.bib148)) | Faster R-CNN (R-50)
    | 72.3 | 23.6 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| AUG OSOL (2022) (Du et al., [2022](#bib.bib20)) | Faster R-CNN (R-50) | 73.2
    | 23.9 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| AIT (2021) (Chen et al., [2021a](#bib.bib11)) | Faster R-CNN (R-50) | 73.1
    | 24.3 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| BHRL (2022) (Yang et al., [2022](#bib.bib132)) | Faster R-CNN (R-50) | 73.8
    | 25.6 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: '| SaFT (2022) (Zhao et al., [2022a](#bib.bib153)) | FCOS (Tian et al., [2019](#bib.bib111)) (R-101)
    | 74.5 | 24.9 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: '| CAT (2021) (Lin et al., [2021](#bib.bib74)) | Faster R-CNN (R-50) | 75.5
    | 24.4 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| ABA OSOL (2023) (Hsieh et al., [2023](#bib.bib43)) | Faster R-CNN (R-50)
    | 74.6 | 23.6 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: 8.2\. Few-Shot Object Detection
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This subsection demonstrates the performance of standard Few-Shot Object Detection
    methods on two most commonly used benchmarks: PASCAL VOC benchmark and MS COCO
    benchmark. For a fair comparison, this survey only lists the performance of FSOD
    methods with released codes.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 4](#S8.T4 "Table 4 ‣ 8.2\. Few-Shot Object Detection ‣ 8\. Performance
    ‣ A Survey of Deep Learning for Low-Shot Object Detection"), [Table 5](#S8.T5
    "Table 5 ‣ 8.2\. Few-Shot Object Detection ‣ 8\. Performance ‣ A Survey of Deep
    Learning for Low-Shot Object Detection") and [Table 6](#S9.T6 "Table 6 ‣ 9.1.1\.
    Efficient FSOD ‣ 9.1\. Promising Directions for FSOD ‣ 9\. Promising Directions
    ‣ A Survey of Deep Learning for Low-Shot Object Detection") present the performance
    on novel classes of PASCAL VOC benchmark and MS COCO benchmark, respectively.
    Some conclusions can be summarized from these two tables: (1) The best-performing
    transfer-learning method have superior performance to the best performing meta-learning
    methods on the most commonly used backbone (ResNet-101). Specifically, FSOD-DIS (the
    best-performing transfer-learning method on ResNet-101) exceeds VFA (the best-performing
    meta-learning method on ResNet-101) on the MS COCO benchmark. (2) For meta-learning
    methods, mixed feature aggregation methods outperform RoI feature aggregation
    methods on two benchmarks overall. The reasons for this phenomenon is that mixed
    feature aggregation methods incorporate category-specific information into the
    shallow components (RPN, mainly) of the detection model, which directly guides
    the prediction of these components using the support information. (3) For transfer-learning
    methods, data augmentation methods (e.g., Halluc, PSEUDO, FSOD-DIS) show strong
    performance in an extremely few-shot condition (${\rm shot}=1,2,3$), demonstrating
    that data augmentation methods effectively tackle the data-scarcity problem in
    the extremely few-shot condition. (4) Methods on advanced backbones (FCT on PVTv2-B2-Li,
    Meta-DETR & DETReg on Def. DETR, PSEUDO on Swin-S, imTED on ViT-B) show significantly
    higher performance than methods on the regular backbone (ResNet-50 & ResNet-101),
    which point out a promising direction for the development of FSOD. (5) The performance
    ranking of a method can differ across these two benchmarks.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Performance (mAP50) of FSOD methods on the PASCAL VOC benchmark (only
    the methods with released codes are listed). These FSOD methods are evaluated
    on the three splits of PASCAL VOC dataset under the $1,2,3,5,10$-shot condition.
    For each shot, the red font denotes the best performance, and the gray font denotes
    the second-best performance. ^⋆ denotes that the results are averaged over multiple
    runs, and R-101 denotes ResNet-101.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Detector (Backbone) | Novel Set 1 | Novel Set 2 | Novel Set 3
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 3 | 5 | 10 | 1 | 2 | 3 | 5 | 10 | 1 | 2 | 3 | 5 | 10 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| Meta-Learning | FSRW (2018) | YOLOv2 | 14.8 | 15.5 | 26.7 | 33.9 | 47.2 |
    15.7 | 15.3 | 22.7 | 30.1 | 40.5 | 21.3 | 25.6 | 28.4 | 42.8 | 45.9 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| Meta-RCNN (2019) | Faster R-CNN (R-101) | 19.9 | 25.5 | 35.0 | 45.7 | 51.5
    | 10.4 | 19.4 | 29.6 | 34.8 | 45.4 | 14.3 | 18.2 | 27.5 | 41.2 | 48.1 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| FsDet (2020)^⋆ | Faster R-CNN (R-101) | 24.2 | 35.3 | 42.2 | 49.1 | 57.4
    | 21.6 | 24.6 | 31.9 | 37.0 | 45.7 | 21.2 | 30.0 | 37.2 | 43.8 | 49.6 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| DRL (2021)^⋆ | Faster R-CNN (R-101) | 30.3 | 40.8 | 49.1 | 48.0 | 58.6 |
    22.4 | 36.1 | 36.9 | 35.4 | 51.8 | 24.8 | 29.3 | 37.9 | 43.6 | 50.4 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| DCNet (2021)^⋆ | Faster R-CNN (R-101) | 33.9 | 37.4 | 43.7 | 51.1 | 59.6
    | 23.2 | 24.8 | 30.6 | 36.7 | 46.6 | 32.3 | 34.9 | 39.7 | 42.6 | 50.7 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| CME (2021) | YOLOv2 | 17.8 | 26.1 | 31.5 | 44.8 | 47.5 | 12.7 | 17.4 | 27.1
    | 33.7 | 40.0 | 15.7 | 27.4 | 30.7 | 44.9 | 48.8 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| Meta-DETR (2022)^⋆ | Def. DETR (R-101) | 35.1 | 49.0 | 53.2 | 57.4 | 62.0
    | 27.9 | 32.3 | 38.4 | 43.2 | 51.8 | 34.9 | 41.8 | 47.1 | 54.1 | 58.2 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| QA-FewDet (2021) | Faster R-CNN (R-101) | 42.4 | 51.9 | 55.7 | 62.6 | 63.4
    | 25.9 | 37.8 | 46.6 | 48.9 | 51.1 | 35.2 | 42.9 | 47.8 | 54.8 | 53.5 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| FCT (2022)^⋆ | Faster R-CNN (PVTv2-B2-Li) | 38.5 | 49.6 | 53.5 | 59.8 | 64.3
    | 25.9 | 34.2 | 40.1 | 44.9 | 47.4 | 34.7 | 43.9 | 49.3 | 53.1 | 56.3 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| VFA (2023) | Faster R-CNN (R-101) | 57.7 | 64.6 | 64.7 | 67.2 | 67.4 | 41.4
    | 46.2 | 51.1 | 51.8 | 51.6 | 48.9 | 54.8 | 56.6 | 59.0 | 58.9 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| Transfer-Learning | TFA w/cos (2020) | Faster R-CNN (R-101) | 39.8 | 36.1
    | 44.7 | 55.7 | 56.0 | 23.5 | 26.9 | 34.1 | 35.1 | 39.1 | 30.8 | 34.8 | 42.8 |
    49.5 | 49.8 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| Halluc (2021) | Faster R-CNN (R-101) | 47.0 | 44.9 | 46.5 | 54.7 | 54.7 |
    26.3 | 31.8 | 37.4 | 37.4 | 41.2 | 40.4 | 42.1 | 43.3 | 51.4 | 49.6 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| MPSR (2020) | Faster R-CNN (R-101) | 41.7 | N/A | 51.4 | 55.2 | 61.8 | 24.4
    | N/A | 39.2 | 39.9 | 47.8 | 35.6 | N/A | 42.3 | 48.0 | 49.7 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| ${\rm FSOD}^{\rm up}(2021)$ | Faster R-CNN (R-101) | 43.8 | 47.8 | 50.3 |
    55.4 | 61.7 | 31.2 | 30.5 | 41.2 | 42.2 | 48.3 | 35.5 | 39.7 | 43.9 | 50.6 | 53.5
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| FSCE (2021)^⋆ | Faster R-CNN (R-101) | 32.9 | 44.0 | 46.8 | 52.9 | 59.7 |
    23.7 | 30.6 | 38.4 | 46.0 | 48.5 | 22.6 | 33.4 | 39.5 | 47.3 | 54.0 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| DeFRCN (2021)^⋆ | Faster R-CNN (R-101) | 40.2 | 53.6 | 58.2 | 63.6 | 66.5
    | 29.5 | 39.7 | 43.4 | 48.1 | 52.8 | 35.0 | 38.3 | 52.9 | 57.7 | 60.8 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| FSOD-KI (2022) | Faster R-CNN (R-101) | 57.0 | 62.3 | 63.3 | 66.2 | 67.6
    | 42.8 | 44.9 | 50.5 | 52.3 | 52.2 | 50.8 | 56.9 | 58.5 | 62.1 | 63.1 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| FSOD-KD (2022) | Faster R-CNN (R-101) | 46.7 | 53.1 | 53.8 | 61.0 | 62.1
    | 30.1 | 34.2 | 41.6 | 41.9 | 44.8 | 41.0 | 46.0 | 47.2 | 55.4 | 55.6 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| FADI (2022) | Faster R-CNN (R-101) | 50.3 | 54.8 | 54.2 | 59.3 | 63.2 | 30.6
    | 35.0 | 40.3 | 42.8 | 48.0 | 45.7 | 49.7 | 49.1 | 55.0 | 59.6 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| PSEUDO (2022) | Faster R-CNN (R-101) | 54.5 | 53.2 | 58.8 | 63.2 | 65.7 |
    32.8 | 29.2 | 50.7 | 49.8 | 50.6 | 48.4 | 52.7 | 55.0 | 59.6 | 59.6 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| FSOD-DIS (2022) | Faster R-CNN (R-101) | 63.4 | 66.3 | 67.7 | 69.4 | 68.1
    | 42.1 | 46.5 | 53.4 | 55.3 | 53.8 | 56.1 | 58.3 | 59.0 | 62.2 | 63.7 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: Table 5\. Performance (mAP50) of FSOD methods on the PASCAL VOC benchmark (only
    the methods with released codes are listed). These FSOD methods are evaluated
    on the PASCAL VOC dataset under the $1,2,3,5,10$-shot condition. The results are
    averaged over three splits of base & novel classes. For each shot, the red font
    denotes the best performance, and the gray font denotes the second-best performance.
    ^⋆ denotes that the results are averaged over multiple runs, and R-101 denotes
    ResNet-101.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Detector (Backbone) | 3 Novel Sets (Averaged) |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 3 | 5 | 10 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| Meta-Learning | FSRW (2018) | YOLOv2 | 17.3 | 18.8 | 25.9 | 35.6 | 44.5 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| Meta-RCNN (2019) | Faster R-CNN (R-101) | 14.9 | 21.0 | 30.7 | 40.6 | 48.3
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| FsDet (2020)^⋆ | Faster R-CNN (R-101) | 22.3 | 30.0 | 37.1 | 43.3 | 50.9
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| DRL (2021)^⋆ | Faster R-CNN (R-101) | 25.8 | 35.4 | 41.3 | 42.3 | 53.6 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| DCNet (2021)^⋆ | Faster R-CNN (R-101) | 29.8 | 32.4 | 38.0 | 43.5 | 52.3
    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| CME (2021) | YOLOv2 | 15.4 | 23.6 | 29.8 | 41.1 | 45.4 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| Meta-DETR (2022)^⋆ | Def. DETR (R-101) | 32.6 | 41.0 | 46.2 | 51.6 | 57.3
    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| QA-FewDet (2021) | Faster R-CNN (R-101) | 34.5 | 44.2 | 50.0 | 55.4 | 56.0
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| FCT (2022)^⋆ | Faster R-CNN (PVTv2-B2-Li) | 33.0 | 42.6 | 47.6 | 52.6 | 56.0
    |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| VFA (2023) | Faster R-CNN (R-101) | 49.3 | 55.2 | 57.5 | 59.3 | 59.3 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| Transfer-Learning | TFA w/cos (2020) | Faster R-CNN (R-101) | 31.4 | 32.6
    | 40.5 | 46.8 | 48.3 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| Halluc (2021) | Faster R-CNN (R-101) | 37.9 | 39.6 | 42.4 | 47.8 | 48.5 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| MPSR (2020) | Faster R-CNN (R-101) | 33.9 | N/A | 44.3 | 47.7 | 53.1 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| ${\rm FSOD}^{\rm up}(2021)$ | Faster R-CNN (R-101) | 36.8 | 39.3 | 45.1 |
    49.4 | 54.5 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| FSCE (2021)^⋆ | Faster R-CNN (R-101) | 26.4 | 36.0 | 41.6 | 48.7 | 54.1 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| DeFRCN (2021)^⋆ | Faster R-CNN (R-101) | 34.9 | 43.9 | 51.5 | 56.5 | 60.0
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| FSOD-KI (2022) | Faster R-CNN (R-101) | 50.2 | 54.7 | 57.4 | 60.2 | 61.0
    |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| FSOD-KD (2022) | Faster R-CNN (R-101) | 39.3 | 44.4 | 47.5 | 52.8 | 54.2
    |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| FADI (2022) | Faster R-CNN (R-101) | 42.2 | 46.5 | 47.9 | 52.4 | 56.9 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| PSEUDO (2022) | Faster R-CNN (R-101) | 45.2 | 45.0 | 54.8 | 57.5 | 58.6 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| FSOD-DIS (2022) | Faster R-CNN (R-101) | 53.9 | 57.0 | 60.0 | 62.3 | 61.9
    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: 8.3\. Zero-Shot Object Detection
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 7](#S9.T7 "Table 7 ‣ 9.1.1\. Efficient FSOD ‣ 9.1\. Promising Directions
    for FSOD ‣ 9\. Promising Directions ‣ A Survey of Deep Learning for Low-Shot Object
    Detection") and [Table 8](#S9.T8 "Table 8 ‣ 9.1.1\. Efficient FSOD ‣ 9.1\. Promising
    Directions for FSOD ‣ 9\. Promising Directions ‣ A Survey of Deep Learning for
    Low-Shot Object Detection") demonstrate the performance of standard ZSOD methods
    under two evaluation protocols (ZSOD, GZSOD) on the most commonly used benchmark:
    MS COCO benchmark. Some trends can be found in this table. (1) Early ZSOD methods
    are not consistent in the choice of semantic attributes, and only a few of them
    are evaluated under the GZSOD protocol. Nevertheless, the newly proposed ZSOD
    methods mostly adopt word2vec as their semantic attributes and use both ZSOD protocol
    and GZSOD protocol to evaluate the model, which is more convenient for performance
    comparison. (2) The model performance of $48/17$ base-novel split is generally
    inferior to that of $65/15$ base-novel split, which is attributed to the fewer
    classes and samples in the base dataset. (3) Current data augmentation methods
    for ZSOD cannot achieve satisfying performance compared to the newly proposed
    ZSOD methods. However, data augmentation methods can outperform other methods
    when the shot number is small in FSOD, which is still promising in ZSOD. (4) The
    newly proposed ZSOD methods, such as CLIP-ZSOD, incorporate pre-trained cross-modal
    models like CLIP in their training process and achieve remarkable performance
    compared to state-of-the-art methods. This demonstrates the potential to transfer
    external foundation models in future ZSOD research, leading to even higher performance.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Promising Directions
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 9.1\. Promising Directions for FSOD
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since FSOD extends OSOL by withdrawing the prior information of test images,
    this survey discusses the promising directions of FSOD to provide guidance for
    both FSOD and OSOL.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.1\. Efficient FSOD
  id: totrans-335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: FSOD models are generally modified from representative object detectors like
    Faster R-CNN, YOLO-style detectors. Current FSOD methods need to first pre-train
    these models on the data-abundant base dataset, then fine-tune them on the data-scarce
    novel dataset. The pre-training on the base dataset requires a large device cost
    and time cost similar to general object detection. Besides, current methods spend
    much time during the few-shot fine-tuning stage for the model to converge (usually
    more than $10$ epochs). The high computing cost of the model and long convergence
    time prevent FSOD from the real-life application. Therefore, lightweight and quickly-converged
    methods are required for future FSOD.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Table 6\. Performance (mAP) of FSOD methods on the MS COCO benchmark (only the
    methods with released codes are listed). These FSOD methods are evaluated under
    the $1,2,3,5,10,30$-shot conditions. For each shot, the red font denotes the best
    performance, and the gray font denotes the second-best performance. ^⋆ denotes
    that the results are averaged over multiple runs, and R-50 & R-101 denote ResNet-50
    & ResNet-101.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Backbone | 1 | 2 | 3 | 5 | 10 | 30 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| Meta-Learning | FSRW (2018) | YOLOv2 | N/A | N/A | N/A | N/A | 5.6 | 9.1
    |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '| Meta-RCNN (2019) | Faster R-CNN (R-101) | N/A | N/A | N/A | N/A | 8.7 | 12.4
    |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| FsDet (2020)^⋆ | Faster R-CNN (R-101) | 4.5 | 6.6 | 7.2 | 10.7 | 12.5 | 14.7
    |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '| Attention-RPN (2020) | Faster R-CNN (R-50) | 4.2 | 5.6 | 6.6 | 8.0 | 11.1
    | 13.5 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| DRL (2021)^⋆ | Faster R-CNN (R-101) | N/A | N/A | N/A | N/A | 11.9 | 14.6
    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| DCNet (2021)^⋆ | Faster R-CNN (R-101) | N/A | N/A | N/A | N/A | 12.8 | 18.6
    |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: '| CME (2021) | YOLOv2 | N/A | N/A | N/A | N/A | 15.1 | 16.9 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: '| Meta-DETR (2022)^⋆ | Def. DETR (R-101) | 7.5 | N/A | 13.5 | 15.4 | 19.0 |
    22.2 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: '| QA-FewDet (2021) | Faster R-CNN (R-101) | 4.9 | 7.6 | 8.4 | 9.7 | 11.6 |
    16.5 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '| DAnA-FasterRCNN (2021) | Faster R-CNN (R-50) | N/A | N/A | N/A | N/A | 18.6
    | 21.6 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '| Meta Faster R-CNN (2022) | Faster R-CNN (R-101) | 5.1 | 7.6 | 9.8 | 10.8
    | 12.7 | 16.6 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| FCT (2022)^⋆ | Faster R-CNN (PVTv2-B2-Li) | 5.1 | 7.2 | 9.8 | 12.0 | 15.3
    | 20.2 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| VFA (2023) | Faster R-CNN (R-101) | N/A | N/A | N/A | N/A | 16.2 | 18.9 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| Transfer-Learning | TFA w/cos (2020) | Faster R-CNN (R-101) | 3.4 | 4.6 |
    6.6 | 8.3 | 10.0 | 13.7 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| Halluc (2021) | Faster R-CNN (R-101) | 4.4 | 5.6 | 7.2 | N/A | N/A | N/A
    |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| MPSR (2020) | Faster R-CNN (R-101) | 2.3 | 3.5 | 5.2 | 6.7 | 9.8 | 14.1 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| ${\rm FSOD}^{\rm up}~{}(2021)$ | Faster R-CNN (R-101) | N/A | N/A | N/A |
    N/A | 11.0 | 15.6 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| FSCE (2021)^⋆ | Faster R-CNN (R-101) | N/A | N/A | N/A | N/A | 11.9 | 16.4
    |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| DeFRCN (2021)^⋆ | Faster R-CNN (R-101) | 4.8 | 8.5 | 10.7 | 13.6 | 16.8 |
    21.2 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| N-PME (2022) | Faster R-CNN (R-101) | N/A | N/A | N/A | N/A | 10.6 | 14.1
    |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| FSOD-KI (2022) | Faster R-CNN (R-101) | N/A | N/A | N/A | N/A | 13.0 | 16.8
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| FSOD-KD (2022) | Faster R-CNN (R-101) | N/A | N/A | N/A | N/A | 12.5 | 17.1
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| FADI (2022) | Faster R-CNN (R-101) | N/A | N/A | N/A | N/A | 12.2 | 16.1
    |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| PSEUDO (2022) | Faster R-CNN (Swin-S) | N/A | N/A | N/A | N/A | 19.0 | 26.8
    |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| FSOD-DIS (2022) | Faster R-CNN (R-101) | 10.8 | 13.9 | 15.0 | 16.4 | 19.4
    | 22.7 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| imTED (2022) | Faster R-CNN (ViT-B) | N/A | N/A | N/A | N/A | 22.5 | 30.2
    |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| DETReg (2022) | Def. DETR (R-50) | N/A | N/A | N/A | N/A | 25.0 | 30.0 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: Table 7\. Performance (mAP50) of ZSOD methods on the MS COCO Benchmark (Seen
    classes/Unseen classes = 48/17). ZSOD denotes the performance under ZSOD protocol.
    Seen, Unseen and HM denote the performance of base classes, novel classes and
    their harmonic average under GZSOD protocol, respectively. For each column, the
    red font denotes the best performance, and the gray font denotes the second-best
    performance. R-50 & R-101 denote ResNet-50 & ResNet-101.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Semantic | Detector (Backbone) | ZSOD | Seen | Unseen | HM |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| mAP | Recall | mAP | Recall | mAP | Recall | mAP | Recall |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| SB (2018) (Bansal et al., [2018](#bib.bib2)) | GloVe | Faster R-CNN (Inception)
    | 0.70 | 24.39 | N/A | N/A | N/A | N/A | N/A | N/A |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| DSES (2018) (Bansal et al., [2018](#bib.bib2)) | GloVe | Faster R-CNN (Inception)
    | 0.54 | 27.19 | N/A | 15.02 | N/A | 15.32 | N/A | 15.17 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| TOPM (2019) (Shao et al., [2019](#bib.bib104)) | GloVe | YOLOv3 (DarkNet-53)
    | 15.43 | 39.20 | N/A | N/A | N/A | N/A | N/A | N/A |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| CG-ZSOD (2020) (Li et al., [2020a](#bib.bib68)) | BERT | YOLOv3 (DarkNet-53)
    | 7.20 | N/A | N/A | N/A | N/A | N/A | N/A | N/A |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| GTNet (2020) (Zhao et al., [2020](#bib.bib152)) | fastText | Faster R-CNN (R-101)
    | N/A | 44.6 | N/A | N/A | N/A | N/A | N/A | N/A |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| JRLNC-ZSOD (2020) (Rahman et al., [2020b](#bib.bib98)) | word2vec | Faster
    R-CNN (R-50) | 5.05 | 12.27 | 13.93 | 20.42 | 2.55 | 12.42 | 4.31 | 15.45 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| SPGP (2020) (Yan et al., [2020](#bib.bib130)) | word2vec | Faster R-CNN (R-101)
    | N/A | 35.40 | N/A | N/A | N/A | N/A | N/A | N/A |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| VSA-ZSOD (2020) (Rahman et al., [2020a](#bib.bib96)) | word2vec | RetinaNet (R-50)
    | 10.01 | 43.56 | 35.92 | 38.24 | 4.12 | 26.32 | 7.39 | 31.18 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| MS-Zero++ (2020) (Gupta et al., [2020](#bib.bib34)) | word2vec | Faster R-CNN (R-101)
    | N/A | N/A | 35.00 | N/A | 13.80 | 35.00 | 19.80 | N/A |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| BLC (2020) (Zheng et al., [2020](#bib.bib156)) | word2vec | Faster R-CNN (R-50)
    | 10.60 | 48.87 | 42.10 | 57.56 | 4.50 | 46.39 | 8.20 | 51.37 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| ZSI (2021) (Zheng et al., [2021](#bib.bib157)) | word2vec | Faster R-CNN (R-101)
    | 11.40 | 53.90 | 46.51 | 70.76 | 4.83 | 53.85 | 8.75 | 61.16 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| ZSDTR (2021) (Zheng and Cui, [2021](#bib.bib155)) | word2vec | Def. DETR (R-50)
    | 10.40 | 48.50 | 48.53 | 74.31 | 5.62 | 48.44 | 9.45 | 60.53 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| VSRG (2022) (Nie et al., [2022](#bib.bib86)) | word2vec | Faster R-CNN (R-50)
    | 11.40 | 55.03 | 43.90 | 66.70 | 4.70 | 54.54 | 8.50 | 60.01 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| ContrastZSOD (2022) (Yan et al., [2022](#bib.bib129)) | word2vec | Faster
    R-CNN (R-101) | 12.50 | 52.40 | 45.10 | 65.70 | 6.30 | 52.40 | 11.10 | 58.30 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| RRFS-ZSOD (2022) (Huang et al., [2022](#bib.bib47))) | fastText | Faster
    R-CNN (R-101) | 13.40 | 53.50 | 42.30 | 59.70 | 13.40 | 58.80 | 20.40 | 59.20
    |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| CLIP-ZSOD (2022) (Xie and Zheng, [2022](#bib.bib126)) | word2vec | YOLOv5 (CSPDarkNet-53)
    | 13.40 | 55.80 | 31.70 | 63.30 | 13.60 | 45.20 | 19.00 | 52.70 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: Table 8\. Performance (mAP50) of ZSOD methods on the MS COCO Benchmark (Seen
    classes/Unseen classes = 65/15). ZSOD denotes the performance under ZSOD protocol.
    Seen, Unseen and HM denote the performance of base classes, novel classes and
    their harmonic average under GZSOD protocol, respectively. For each column, red
    font denotes the best performance, and gray font denotes the second-best performance.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Semantic | Detector (Backbone) | ZSOD | Seen | Unseen | HM |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| mAP | Recall | mAP | Recall | mAP | Recall | mAP | Recall |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| Transductive (2019) (Rahman et al., [2019](#bib.bib95)) | word2vec | RetinaNet (R-50)
    | 14.57 | 48.15 | 28.78 | 54.14 | 14.05 | 37.16 | 18.89 | 44.07 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| CG-ZSOD (2020) (Li et al., [2020a](#bib.bib68)) | BERT | YOLOv3 (DarkNet-53)
    | 10.90 | N/A | N/A | N/A | N/A | N/A | N/A | N/A |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| LSA-ZSOD (2020) (Wang et al., [2020b](#bib.bib116)) | aPaY | RetinaNet (R-50)
    | 13.55 | 37.78 | 34.18 | 40.32 | 13.42 | 38.73 | 19.27 | 39.51 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| ACS-ZSOD (2020) (Ma et al., [2020](#bib.bib82)) | aPaY | RetinaNet (R-50)
    | 15.34 | 47.83 | N/A | N/A | N/A | N/A | N/A | N/A |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| SYN-ZSOD (2020) (Hayat et al., [2020](#bib.bib40)) | fastText | Faster R-CNN (R-101)
    | 19.00 | 54.00 | 36.90 | 57.70 | 19.00 | 53.90 | 25.08 | 55.74 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| VSA-ZSOD (2020) (Rahman et al., [2020a](#bib.bib96)) | word2vec | RetinaNet (R-50)
    | 12.40 | 37.72 | 34.07 | 36.38 | 12.40 | 37.16 | 18.18 | 36.76 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| BLC (2020) (Zheng et al., [2020](#bib.bib156)) | word2vec | Faster R-CNN (R-50)
    | 14.70 | 54.68 | 36.00 | 56.39 | 13.10 | 51.65 | 19.20 | 53.92 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| ZSI (2021) (Zheng et al., [2021](#bib.bib157)) | word2vec | Faster R-CNN (R-101)
    | 13.60 | 58.90 | 38.68 | 67.11 | 13.60 | 58.93 | 20.13 | 62.76 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| ZSDTR (2021) (Zheng and Cui, [2021](#bib.bib155)) | word2vec | Def. DETR (R-50)
    | 13.20 | 60.30 | 40.55 | 69.12 | 13.22 | 59.45 | 20.16 | 61.12 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| DPIF (2021) (Li et al., [2021b](#bib.bib67)) | word2vec | Faster R-CNN (R-50)
    | 19.82 | 55.73 | 29.82 | 56.68 | 19.46 | 38.70 | 23.55 | 46.00 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| VSRG (2022) (Nie et al., [2022](#bib.bib86)) | word2vec | Faster R-CNN (R-50)
    | 14.90 | 62.70 | 38.10 | 65.31 | 13.90 | 60.52 | 20.40 | 62.82 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| RSC-ZSOD (2022) (Sarma et al., [2022](#bib.bib102)) | word2vec | Faster R-CNN (R-101)
    | 20.10 | 65.10 | 37.40 | 58.60 | 20.10 | 64.00 | 26.15 | 61.18 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| ContrastZSOD (2022) (Yan et al., [2022](#bib.bib129)) | word2vec | Faster
    R-CNN (R-101) | 18.60 | 59.50 | 40.20 | 62.90 | 16.50 | 58.60 | 23.40 | 60.70
    |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| RRFS-ZSOD (2022) (Huang et al., [2022](#bib.bib47))) | fastText | Faster
    R-CNN (R-101) | 19.80 | 62.30 | 37.40 | 58.60 | 19.80 | 61.80 | 26.00 | 60.20
    |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| CLIP-ZSOD (2022) (Xie and Zheng, [2022](#bib.bib126)) | word2vec | YOLOv5 (CSPDarkNet-53)
    | 18.30 | 69.50 | 31.70 | 61.00 | 17.90 | 65.20 | 22.90 | 63.00 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| CCFA-ZSOD (2022) (Li et al., [2022c](#bib.bib62)) | word2vec | RetinaNet (R-50)
    | 24.62 | 55.32 | 33.35 | 38.64 | 24.62 | 54.72 | 28.31 | 45.29 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: 9.1.2\. Cross-Domain FSOD
  id: totrans-404
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Almost all of the current FSOD methods are evaluated in the single-domain condition.
    Cross-domain few-shot learning is a more realistic setting that the data for base
    classes and novel classes are drawn from two domains. Some studies (Guo et al.,
    [2020](#bib.bib33)) on cross-domain few-shot image classification indicate that
    the few-shot method does not have consistent performance in the single-domain
    condition and cross-domain condition. For example, this paper demonstrates that
    although some meta-learning methods achieve better performance than fine-tuning
    methods in the single-domain condition, they significantly underperform even some
    simple fine-tuning methods in the cross-domain condition. Cross-domain few-shot
    object detection is a more complicated task than cross-domain few-shot image classification.
    Recently a few methods (Gao et al., [2022b](#bib.bib28); Lee et al., [2022b](#bib.bib56);
    Xiong and Liu, [2022](#bib.bib127)) propose some benchmarks on cross-domain FSOD
    and set up some baselines for this area. Nevertheless, cross-domain FSOD deserves
    more exploration in the future for its practicality.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.3\. New Detection Framework for FSOD
  id: totrans-406
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most of the current FSOD methods adopt Faster R-CNN as the detection framework.
    Some other powerful frameworks are worth exploring in the future. For example,
    vision transformer focuses more on holistic information of the image than local
    information, which can capture features missed by traditional CNN models. Currently,
    it has been widely applied in many other computer vision areas. In FSOD, the recently
    proposed Meta-DETR has improved the performance of FSOD to the SOTA on the MS
    COCO benchmark, which exceeds previous Faster R-CNN based detectors by several
    points. Therefore, the potential of vision transformer on FSOD still requires
    exploration.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 9.2\. Promising Directions for ZSOD
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 9.2.1\. Combining Auxiliary Information for ZSOD
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Combining information from an external source to assist ZSOD is a potential
    direction for performance improvement. Some ZSOD methods attempt to exploit the
    information of external classes (not intersecting with base classes and novel
    classes) to augment semantic attributes of base classes and novel classes. Moreover,
    some other ZSOD methods utilize an external word vocabulary to enhance the visual-semantic
    mapping. However, no ZSOD method delves into the utilization of external auxiliary
    information as a whole, which requires more attention in the future.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2\. Large Cross-Modal Foundation Model for ZSOD
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently some large pre-trained cross-modal model show incredibly strong performance
    in aligning the context semantic between images and their text descriptions. CLIP (Radford
    et al., [2021](#bib.bib94)) is the representative work of these large cross-modal
    models. Specifically, CLIP pre-trains the model on a large-scale dataset comprising
    abundant image-text pairs. CLIP encodes the images and texts with two parallel
    transformer-based models and adopts a contrastive learning strategy for training.
    CLIP has the strong capacity of projecting images and texts into a common feature
    space, thus it can be directly transferred to the zero-shot scenario. Recently,
    CLIP has been widely adopted for open-vocabulary object detection.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.3\. ZSOD combined with FSOD
  id: totrans-413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A more generic scenario may appear in real-life where only some novel classes
    have annotated samples, yet other novel classes have semantic attributes, which
    requires the combination of ZSOD and FSOD. Some methods have been proposed to
    tackle this scenario. For example, ASD (Rahman et al., [2020c](#bib.bib97)) and
    UniT (Khandelwal et al., [2021](#bib.bib52)) introduce an LSOD setting that the
    model makes predictions utilizing both semantic information and image samples.
    Moreover, UniT significantly improves the performance of FSOD with auxiliary semantic
    information. Therefore, this generalized setting has more practical significance
    for the application of LSOD in the future.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Conclusion
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enhancing the deep object detectors to quickly learn from very few or even zero
    samples is of great significance to future object detection. This paper conducts
    a comprehensive survey on Low-Shot Object Detection (LSOD), consisting of One-Shot
    Object Localization (OSOL), Few-Shot Object Detection (FSOD) and Zero-Shot Object
    Detection (ZSOD). In this survey, the emergence background and evolution history
    of LSOD are first reviewed. Then, current LSOD methods are analyzed systematically
    based on an explicit and complete taxonomy of these methods, including some extensional
    topics of LSOD. Moreover, the pros and cons of LSOD methods are indicated with
    a comparison of their performance. Finally, the challenges and promising directions
    of LSOD are discussed. Hopefully, this survey can promote future research on LSOD.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bansal et al. (2018) Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa,
    and Ajay Divakaran. 2018. Zero-Shot Object Detection. In *ECCV*. Springer, 397–414.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bar et al. (2022) Amir Bar, Xin Wang, Vadim Kantorov, Colorado J. Reed, Roei
    Herzig, Gal Chechik, Anna Rohrbach, Trevor Darrell, and Amir Globerson. 2022.
    DETReg: Unsupervised Pretraining with Region Priors for Object Detection. In *CVPR*.
    IEEE, 14585–14595.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bochkovskiy et al. (2020) Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark
    Liao. 2020. Yolov4: Optimal speed and accuracy of object detection. *arXiv preprint
    arXiv:2004.10934* (2020).'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bojanowski et al. (2017) Piotr Bojanowski, Edouard Grave, Armand Joulin, and
    Tomás Mikolov. 2017. Enriching Word Vectors with Subword Information. *Trans.
    Assoc. Comput. Linguistics* 5 (2017), 135–146.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bulat et al. (2022) Adrian Bulat, Ricardo Guerrero, Brais Martinez, and Georgios
    Tzimiropoulos. 2022. FS-DETR: Few-Shot DEtection TRansformer with prompting and
    without re-training. *arXiv preprint arXiv:2210.04845* (2022).'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2021) Yuhang Cao, Jiaqi Wang, Ying Jin, Tong Wu, Kai Chen, Ziwei
    Liu, and Dahua Lin. 2021. Few-Shot Object Detection via Association and DIscrimination.
    In *NIPS*. 16570–16581.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2022) Yuhang Cao, Jiaqi Wang, Yiqi Lin, and Dahua Lin. 2022. MINI:
    Mining Implicit Novel Instances for Few-Shot Object Detection. *arXiv preprint
    arXiv:2205.03381* (2022).'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carion et al. (2020) Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
    Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-End Object Detection
    with Transformers. In *ECCV*. Springer, 213–229.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cen and Jung (2018) Miaobin Cen and Cheolkon Jung. 2018. Fully Convolutional
    Siamese Fusion Networks for Object Tracking. In *ICIP*. IEEE, 3718–3722.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021a) Ding-Jie Chen, He-Yen Hsieh, and Tyng-Luh Liu. 2021a. Adaptive
    Image Transformer for One-Shot Object Detection. In *CVPR*. IEEE, 12247–12256.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018) Hao Chen, Yali Wang, Guoyou Wang, and Yu Qiao. 2018. LSTD:
    A Low-Shot Transfer Detector for Object Detection. In *AAAI*. AAAI Press, 2836–2843.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021b) Tung-I Chen, Yueh-Cheng Liu, Hung-Ting Su, Yu-Cheng Chang,
    Yu-Hsiang Lin, Jia-Fong Yeh, and Winston H Hsu. 2021b. Should I Look at the Head
    or the Tail? Dual-awareness Attention for Few-Shot Object Detection. *arXiv preprint
    arXiv:2102.12152* (2021).
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) Xianyu Chen, Ming Jiang, and Qi Zhao. 2020. Leveraging Bottom-Up
    and Top-Down Attention for Few-Shot Object Detection. *arXiv preprint arXiv:2007.12104*
    (2020).
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2021) Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. 2021.
    UP-DETR: Unsupervised Pre-Training for Object Detection With Transformers. In
    *CVPR*. IEEE, 1601–1610.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dalal and Triggs (2005) Navneet Dalal and Bill Triggs. 2005. Histograms of Oriented
    Gradients for Human Detection. In *CVPR*. IEEE, 886–893.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demirel et al. (2018) Berkan Demirel, Ramazan Gokberk Cinbis, and Nazli Ikizler-Cinbis.
    2018. Zero-Shot Object Detection by Hybrid Region Embedding. In *BMVC*. BMVA Press,
    56.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *NAACL-HLT*. Association for Computational Linguistics, 4171–4186.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2019) Xuanyi Dong, Liang Zheng, Fan Ma, Yi Yang, and Deyu Meng.
    2019. Few-Example Object Detection with Model Communication. *TPAMI* 41, 7 (2019),
    1641–1654.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2022) Yaoyang Du, Fang Liu, Licheng Jiao, Zehua Hao, Shuo Li, Xu
    Liu, and Jing Liu. 2022. Augmentative contrastive learning for one-shot object
    detection. *Neurocomputing* 513 (2022), 13–24.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everingham et al. (2010) Mark Everingham, Luc Van Gool, Christopher KI Williams,
    John Winn, and Andrew Zisserman. 2010. The pascal visual object classes (voc)
    challenge. *IJCV* 88, 2 (2010), 303–338.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) Qi Fan, Wei Zhuo, Chi-Keung Tang, and Yu-Wing Tai. 2020. Few-Shot
    Object Detection With Attention-RPN and Multi-Relation Detector. In *CVPR*. IEEE,
    4012–4021.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Farhadi et al. (2009) Ali Farhadi, Ian Endres, Derek Hoiem, and David A. Forsyth.
    2009. Describing objects by their attributes. In *CVPR 2009*. IEEE Computer Society,
    1778–1785.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Felzenszwalb et al. (2008) Pedro F. Felzenszwalb, David A. McAllester, and Deva
    Ramanan. 2008. A discriminatively trained, multiscale, deformable part model.
    In *CVPR*. IEEE.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2021) Kun Fu, Tengfei Zhang, Yue Zhang, and Xian Sun. 2021. OSCD:
    A one-shot conditional object detection framework. *Neurocomputing* 425 (2021),
    243–255.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2022a) Bin-Bin Gao, Xiaochen Chen, Zhongyi Huang, Congchong Nie,
    Jun Liu, Jinxiang Lai, Guannan Jiang, Xi Wang, and Chengjie Wang. 2022a. Decoupling
    Classifier for Boosting Few-shot Object Detection and Instance Segmentation. In
    *NIPS*.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2019) Jiyang Gao, Jiang Wang, Shengyang Dai, Li-Jia Li, and Ram
    Nevatia. 2019. NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object
    Detection. In *ICCV*. IEEE, 9507–9516.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2022b) Yipeng Gao, Lingxiao Yang, Yunmu Huang, Song Xie, Shiyong
    Li, and Wei-Shi Zheng. 2022b. AcroFOD: An Adaptive Method for Cross-Domain Few-Shot
    Object Detection. In *ECCV*. Springer, 673–690.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ge et al. (2021) Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun.
    2021. Yolox: Exceeding yolo series in 2021. *arXiv preprint arXiv:2107.08430*
    (2021).'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Girshick (2015) Ross B. Girshick. 2015. Fast R-CNN. In *ICCV*. IEEE, 1440–1448.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Girshick et al. (2014) Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
    Malik. 2014. Rich Feature Hierarchies for Accurate Object Detection and Semantic
    Segmentation. In *CVPR*. IEEE, 580–587.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grigorescu et al. (2020) Sorin Mihai Grigorescu, Bogdan Trasnea, Tiberiu T.
    Cocias, and Gigel Macesanu. 2020. A survey of deep learning techniques for autonomous
    driving. *J. Field Robotics* 37, 3 (2020), 362–386.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2020) Yunhui Guo, Noel Codella, Leonid Karlinsky, James V. Codella,
    John R. Smith, Kate Saenko, Tajana Rosing, and Rogério Feris. 2020. A Broader
    Study of Cross-Domain Few-Shot Learning. In *ECCV*. Springer, 124–141.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta et al. (2020) Dikshant Gupta, Aditya Anantharaman, Nehal Mamgain, Sowmya Kamath
    S., Vineeth N. Balasubramanian, and C. V. Jawahar. 2020. A Multi-Space Approach
    to Zero-Shot Object Detection. In *WACV*. IEEE, 1198–1206.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2021) Guangxing Han, Yicheng He, Shiyuan Huang, Jiawei Ma, and Shih-Fu
    Chang. 2021. Query Adaptive Few-Shot Object Detection With Heterogeneous Graph
    Convolutional Networks. In *ICCV*. IEEE, 3263–3272.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2022a) Guangxing Han, Shiyuan Huang, Jiawei Ma, Yicheng He, and
    Shih-Fu Chang. 2022a. Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection
    with Attentive Feature Alignment. In *AAAI*. AAAI Press, 780–789.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2022b) Guangxing Han, Jiawei Ma, Shiyuan Huang, Long Chen, and Shih-Fu
    Chang. 2022b. Few-Shot Object Detection with Fully Cross-Transformer. In *CVPR*.
    IEEE, 5311–5320.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2022c) Guangxing Han, Jiawei Ma, Shiyuan Huang, Long Chen, Rama
    Chellappa, and Shih-Fu Chang. 2022c. Multimodal few-shot object detection with
    meta-learning based cross-modal prompting. *arXiv preprint arXiv:2204.07841* (2022).
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2023) Jiaming Han, Yuqiang Ren, Jian Ding, Ke Yan, and Gui-Song
    Xia. 2023. Few-Shot Object Detection via Variational Feature Aggregation. *arXiv
    preprint arXiv:2301.13411* (2023).
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hayat et al. (2020) Nasir Hayat, Munawar Hayat, Shafin Rahman, Salman H. Khan,
    Syed Waqas Zamir, and Fahad Shahbaz Khan. 2020. Synthesizing the Unseen for Zero-Shot
    Object Detection. In *ACCV*. Springer, 155–170.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2022) Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár,
    and Ross B. Girshick. 2022. Masked Autoencoders Are Scalable Vision Learners.
    In *CVPR*. IEEE, 15979–15988.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hospedales et al. (2022) Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli,
    and Amos J. Storkey. 2022. Meta-Learning in Neural Networks: A Survey. *TPAMI*
    44, 9 (2022), 5149–5169.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsieh et al. (2023) He-Yen Hsieh, Ding-Jie Chen, Cheng-Wei Chang, and Tyng-Luh
    Liu. 2023. Aggregating Bilateral Attention for Few-Shot Instance Localization.
    In *WACV*. IEEE, 6314–6323.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hsieh et al. (2019) Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, and Tyng-Luh
    Liu. 2019. One-Shot Object Detection with Co-Attention and Co-Excitation. In *NIPS*.
    2721–2730.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2021) Hanzhe Hu, Shuai Bai, Aoxue Li, Jinshi Cui, and Liwei Wang.
    2021. Dense Relation Distillation With Context-Aware Aggregation for Few-Shot
    Object Detection. In *CVPR*. IEEE, 10185–10194.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2021) Junying Huang, Fan Chen, Sibo Huang, and Dongyu Zhang. 2021.
    Instant Response Few-shot Object Detection with Meta Strategy and Explicit Localization
    Inference. *arXiv preprint arXiv:2110.13377* (2021).
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2022) Peiliang Huang, Junwei Han, De Cheng, and Dingwen Zhang.
    2022. Robust Region Feature Synthesizer for Zero-Shot Object Detection. In *CVPR*.
    IEEE, 7612–7621.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. (2019) Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng,
    and Trevor Darrell. 2019. Few-Shot Object Detection via Feature Reweighting. In
    *ICCV*. IEEE, 8419–8428.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karlinsky et al. (2021) Leonid Karlinsky, Joseph Shtok, Amit Alfassy, Moshe
    Lichtenstein, Sivan Harary, Eli Schwartz, Sivan Doveh, Prasanna Sattigeri, Rogério
    Feris, Alex M. Bronstein, and Raja Giryes. 2021. StarNet: towards Weakly Supervised
    Few-Shot Object Detection. In *AAAI*. AAAI Press, 1743–1753.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karlinsky et al. (2019) Leonid Karlinsky, Joseph Shtok, Sivan Harary, Eli Schwartz,
    Amit Aides, Rogério Schmidt Feris, Raja Giryes, and Alexander M. Bronstein. 2019.
    RepMet: Representative-Based Metric Learning for Classification and Few-Shot Object
    Detection. In *CVPR*. IEEE, 5197–5206.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaul et al. (2022) Prannay Kaul, Weidi Xie, and Andrew Zisserman. 2022. Label,
    Verify, Correct: A Simple Few Shot Object Detection Method. In *CVPR*. IEEE, 14217–14227.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khandelwal et al. (2021) Siddhesh Khandelwal, Raghav Goyal, and Leonid Sigal.
    2021. UniT: Unified Knowledge Transfer for Any-Shot Object Detection and Segmentation.
    In *CVPR*. IEEE, 5951–5961.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2020) Geonuk Kim, Honggyu Jung, and Seong-Whan Lee. 2020. Few-Shot
    Object Detection via Knowledge Transfer. In *SMC*. IEEE, 3564–3569.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2022) Sueyeon Kim, Woo-Jeoung Nam, and Seong-Whan Lee. 2022. Few-Shot
    Object Detection with Proposal Balance Refinement. In *ICPR*. IEEE, 4700–4707.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2022a) Hojun Lee, Myunggi Lee, and Nojun Kwak. 2022a. Few-Shot Object
    Detection by Attending to Per-Sample-Prototype. In *WACV*. IEEE, 1101–1110.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2022b) Kibok Lee, Hao Yang, Satyaki Chakraborty, Zhaowei Cai, Gurumurthy
    Swaminathan, Avinash Ravichandran, and Onkar Dabeer. 2022b. Rethinking Few-Shot
    Object Detection on a Multi-Domain Benchmark. In *ECCV*. Springer, 366–382.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Li (2021) Aoxue Li and Zhenguo Li. 2021. Transformation Invariant Few-Shot
    Object Detection. In *CVPR*. IEEE, 3094–3102.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022a) Bohao Li, Chang Liu, Mengnan Shi, Xiaozhong Chen, Xiangyang
    Ji, and Qixiang Ye. 2022a. Proposal Distribution Calibration for Few-Shot Object
    Detection. *arXiv preprint arXiv:2212.07618* (2022).
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022b) Bowen Li, Chen Wang, Pranay Reddy, Seungchan Kim, and Sebastian A.
    Scherer. 2022b. AirDet: Few-Shot Detection Without Fine-Tuning for Autonomous
    Exploration. In *ECCV*. Springer, 427–444.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2018) Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. 2018.
    High Performance Visual Tracking With Siamese Region Proposal Network. In *CVPR*.
    IEEE, 8971–8980.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021c) Bohao Li, Boyu Yang, Chang Liu, Feng Liu, Rongrong Ji, and
    Qixiang Ye. 2021c. Beyond Max-Margin: Class Margin Equilibrium for Few-Shot Object
    Detection. In *CVPR*. IEEE, 7363–7372.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022c) Haohe Li, Chong Wang, Shenghao Yu, Zheng Huo, Yujie Zheng,
    Li Dong, and Jiafei Wu. 2022c. Zero-Shot Object Detection with Partitioned Contrastive
    Feature Alignment. *Research Square* (2022). [https://doi.org/10.21203/rs.3.rs-1770867/v1](https://doi.org/10.21203/rs.3.rs-1770867/v1)
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021a) Pengyang Li, Yanan Li, and Donghui Wang. 2021a. Class-Incremental
    Few-Shot Object Detection. *arXiv preprint arXiv:2105.07637* (2021).
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021d) Qianzhong Li, Yujia Zhang, Shiying Sun, Xiaoguang Zhao, Kang
    Li, and Min Tan. 2021d. Rethinking semantic-visual alignment in zero-shot object
    detection via a softplus margin focal loss. *Neurocomputing* 449 (2021), 117–135.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020b) Shaoqi Li, Wenfeng Song, Shuai Li, Aimin Hao, and Hong Qin.
    2020b. Meta-RetinaNet for Few-shot Object Detection. In *BMVC*. BMVA Press.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020c) Xiang Li, Lin Zhang, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung
    Tang. 2020c. One-shot object detection without fine-tuning. *arXiv preprint arXiv:2005.03819*
    (2020).
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021b) Yanan Li, Pengyang Li, Han Cui, and Donghui Wang. 2021b. Inference
    Fusion with Associative Semantics for Unseen Object Detection. In *AAAI*. AAAI
    Press, 1993–2001.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020a) Yanan Li, Yilan Shao, and Donghui Wang. 2020a. Context-Guided
    Super-Class Inference for Zero-Shot Detection. In *CVPR*. IEEE, 4064–4068.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021e) Yiting Li, Haiyue Zhu, Yu Cheng, Wenxin Wang, Chek Sing Teo,
    Cheng Xiang, Prahlad Vadakkepat, and Tong Heng Lee. 2021e. Few-Shot Object Detection
    via Classification Refinement and Distractor Retreatment. In *CVPR*. IEEE, 15395–15403.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Zhihui Li, Lina Yao, Xiaoqin Zhang, Xianzhi Wang, Salil S.
    Kanhere, and Huaxiang Zhang. 2019. Zero-Shot Object Detection with Textual Descriptions.
    In *AAAI*. AAAI Press, 8690–8697.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2017a) Tsung-Yi Lin, Piotr Dollár, Ross B. Girshick, Kaiming He,
    Bharath Hariharan, and Serge J. Belongie. 2017a. Feature Pyramid Networks for
    Object Detection. In *CVPR*. IEEE, 936–944.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2017b) Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,
    and Piotr Dollár. 2017b. Focal Loss for Dense Object Detection. In *ICCV*. IEEE,
    2999–3007.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft
    COCO: Common Objects in Context. In *ECCV*. Springer, 740–755.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2021) Weidong Lin, Yuyan Deng, Yang Gao, Ning Wang, Jinghao Zhou,
    Lingqiao Liu, Lei Zhang, and Peng Wang. 2021. CAT: Cross-Attention Transformer
    for One-Shot Object Detection. *arXiv preprint arXiv:2104.14984* (2021).'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021a) Longyao Liu, Bo Ma, Yulin Zhang, Xin Yi, and Haozhi Li.
    2021a. AFD-Net: Adaptive Fully-Dual Network for Few-Shot Object Detection. In
    *ACM MM*. ACM, 2549–2557.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022a) Liyang Liu, Bochao Wang, Zhanghui Kuang, Jing-Hao Xue, Yimin
    Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang. 2022a. GenDet: Meta Learning
    to Generate Detectors From Few Shots. *TNNLS* 33, 8 (2022), 3448–3460.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021b) Weijie Liu, Chong Wang, Haohe Li, Shenghao Yu, Jiangbo Qian,
    Jun Wang, and Jiafei Wu. 2021b. Dynamic relevance learning for few-shot object
    detection. *arXiv preprint arXiv:2108.02235* (2021).
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022b) Weijie Liu, Chong Wang, Shenghao Yu, Chenchen Tao, Jun Wang,
    and Jiafei Wu. 2022b. Novel Instance Mining with Pseudo-Margin Evaluation for
    Few-Shot Object Detection. In *ICASSP*. IEEE, 2250–2254.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2022) Xiaonan Lu, Wenhui Diao, Yongqiang Mao, Junxi Li, Peijin Wang,
    Xian Sun, and Kun Fu. 2022. Breaking Immutable: Information-Coupled Prototype
    Elaboration for Few-Shot Object Detection. *arXiv preprint arXiv:2211.14782* (2022).'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2020) Ruotian Luo, Ning Zhang, Bohyung Han, and Linjie Yang. 2020.
    Context-Aware Zero-Shot Recognition. In *AAAI*. AAAI Press, 11709–11716.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2022) Jiawei Ma, Guangxing Han, Shiyuan Huang, Yuncong Yang, and
    Shih-Fu Chang. 2022. Few-Shot End-to-End Object Detection via Constantly Concentrated
    Encoding Across Heads. In *ECCV*. Springer, 57–73.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2020) Qiao-mei Ma, Chong Wang, Shenghao Yu, Ye Zheng, and Yuqi Li.
    2020. Zero-Shot Object Detection With Attributes-Based Category Similarity. *IEEE
    Trans. Circuits Syst. II Express Briefs* 67-II, 5 (2020), 921–925.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Michaelis et al. (2018) Claudio Michaelis, Ivan Ustyuzhaninov, Matthias Bethge,
    and Alexander S Ecker. 2018. One-shot instance segmentation. *arXiv preprint arXiv:1811.11507*
    (2018).
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013) Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013. Efficient Estimation of Word Representations in Vector Space. In *ICLR*.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Misra et al. (2015) Ishan Misra, Abhinav Shrivastava, and Martial Hebert. 2015.
    Watch and learn: Semi-supervised learning of object detectors from videos. In
    *CVPR*. IEEE, 3593–3602.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nie et al. (2022) Hui Nie, Ruiping Wang, and Xilin Chen. 2022. From Node to
    Graph: Joint Reasoning on Visual-Semantic Relational Graph for Zero-Shot Detection.
    In *WACV*. IEEE, 1648–1657.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Osokin et al. (2020) Anton Osokin, Denis Sumin, and Vasily Lomakin. 2020. OS2D:
    One-Stage One-Shot Object Detection by Matching Anchor Features. In *ECCV*. Springer,
    635–652.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park and Lee (2022) Dongwoo Park and Jongmin Lee. 2022. Hierarchical Attention
    Network for Few-Shot Object Detection via Meta-Contrastive Learning. *arXiv preprint
    arXiv:2208.07039* (2022).
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pei et al. (2022) Wenjie Pei, Shuang Wu, Dianwen Mei, Fanglin Chen, Jiandong
    Tian, and Guangming Lu. 2022. Few-Shot Object Detection by Knowledge Distillation
    Using Bag-of-Visual-Words Representations. In *ECCV*. Springer, 283–299.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D.
    Manning. 2014. Glove: Global Vectors for Word Representation. In *EMNLP*. ACL,
    1532–1543.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pérez-Rúa et al. (2020) Juan-Manuel Pérez-Rúa, Xiatian Zhu, Timothy M. Hospedales,
    and Tao Xiang. 2020. Incremental Few-Shot Object Detection. In *CVPR*. IEEE, 13843–13852.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiao et al. (2021) Limeng Qiao, Yuxuan Zhao, Zhiyuan Li, Xi Qiu, Jianan Wu,
    and Chi Zhang. 2021. DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection.
    In *ICCV*. IEEE, 8681–8690.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quan et al. (2022) Jianing Quan, Baozhen Ge, and Lei Chen. 2022. Cross attention
    redistribution with contrastive learning for few shot object detection. *Displays*
    72 (2022), 102162.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual
    Models From Natural Language Supervision. In *ICML*. PMLR, 8748–8763.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahman et al. (2019) Shafin Rahman, Salman H. Khan, and Nick Barnes. 2019. Transductive
    Learning for Zero-Shot Object Detection. In *ICCV*. IEEE, 6081–6090.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahman et al. (2020a) Shafin Rahman, Salman H. Khan, and Nick Barnes. 2020a.
    Improved Visual-Semantic Alignment for Zero-Shot Object Detection. In *AAAI*.
    AAAI Press, 11932–11939.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahman et al. (2020c) Shafin Rahman, Salman H. Khan, Nick Barnes, and Fahad Shahbaz
    Khan. 2020c. Any-Shot Object Detection. In *ACCV*. Springer, 89–106.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rahman et al. (2020b) Shafin Rahman, Salman H. Khan, and Fatih Porikli. 2020b.
    Zero-Shot Object Detection: Joint Recognition and Localization of Novel Concepts.
    *IJCV* 12 (2020), 2979–2999.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon and Farhadi (2018) Joseph Redmon and Ali Farhadi. 2018. Yolov3: An incremental
    improvement. *arXiv preprint arXiv:1804.02767* (2018).'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.
    2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.
    In *NIPS*. 91–99.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rocco et al. (2018) Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. 2018.
    End-to-End Weakly-Supervised Semantic Alignment. In *CVPR*. IEEE, 6917–6925.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sarma et al. (2022) Sandipan Sarma, Sushil Kumar, and Arijit Sur. 2022. Resolving
    Semantic Confusions for Improved Zero-Shot Detection. *arXiv preprint arXiv:2212.06097*
    (2022).
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shangguan et al. (2022) Zeyu Shangguan, Lian Huai, Tong Liu, and Xingqun Jiang.
    2022. Few-shot Object Detection with Refined Contrastive Learning. *arXiv preprint
    arXiv:2211.13495* (2022).
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shao et al. (2019) Yilan Shao, Yanan Li, and Donghui Wang. 2019. Zero-Shot Detection
    with Transferable Object Proposal Mechanism. In *ICIP*. IEEE, 3666–3670.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subakan et al. (2021) Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi,
    and Jianyuan Zhong. 2021. Attention Is All You Need In Speech Separation. In *ICASSP*.
    IEEE, 21–25.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2021) Bo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, and Chi Zhang.
    2021. FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding. In *CVPR*.
    IEEE, 7352–7362.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2020) Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and
    Hanwang Zhang. 2020. Unbiased Scene Graph Generation From Biased Training. In
    *CVPR*. IEEE, 3713–3722.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teng and Wang (2021) Yao Teng and Limin Wang. 2021. Structured Sparse R-CNN
    for Direct Scene Graph Generation. *arXiv preprint arXiv:2106.10815* (2021).
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thrun and Pratt (1998) Sebastian Thrun and Lorien Y. Pratt. 1998. Learning
    to Learn: Introduction and Overview. In *Learning to Learn*. Springer, 3–17.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2020) Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum,
    and Phillip Isola. 2020. Rethinking few-shot image classification: a good embedding
    is all you need?. In *ECCV*. Springer, 266–282.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2019) Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. 2019. FCOS:
    Fully Convolutional One-Stage Object Detection. In *ICCV*. IEEE, 9626–9635.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uijlings et al. (2013) Jasper R. R. Uijlings, Koen E. A. van de Sande, Theo
    Gevers, and Arnold W. M. Smeulders. 2013. Selective Search for Object Recognition.
    *IJCV* 104, 2 (2013), 154–171.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viola and Jones (2001) Paul A. Viola and Michael J. Jones. 2001. Rapid Object
    Detection using a Boosted Cascade of Simple Features. In *CVPR*. IEEE, 511–518.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voigtlaender et al. (2019) Paul Voigtlaender, Michael Krause, Aljosa Osep,
    Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, and Bastian Leibe.
    2019. MOTS: Multi-Object Tracking and Segmentation. In *CVPR*. IEEE, 7942–7951.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Chen (2022) Jianwei Wang and Deyun Chen. 2022. Few-Shot Object Detection
    Method Based on Knowledge Reasoning. *Electronics* 11, 9 (2022), 1327.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Kang Wang, Lu Zhang, Yifan Tan, Jiajia Zhao, and Shuigeng
    Zhou. 2020b. Learning Latent Semantic Attributes for Zero-Shot Object Detection.
    In *ICTAI*. IEEE, 230–237.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019b) Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and
    Philip H. S. Torr. 2019b. Fast Online Object Tracking and Segmentation: A Unifying
    Approach. In *CVPR*. IEEE, 1328–1338.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming
    He. 2018. Non-local neural networks. In *CVPR*. IEEE, 7794–7803.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020a) Xin Wang, Thomas E. Huang, Joseph Gonzalez, Trevor Darrell,
    and Fisher Yu. 2020a. Frustratingly Simple Few-Shot Object Detection. In *ICML*.
    9919–9928.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019a) Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. 2019a.
    Meta-Learning to Detect Rare Objects. In *ICCV*. IEEE, 9924–9933.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021a) Aming Wu, Yahong Han, Linchao Zhu, and Yi Yang. 2021a. Universal-Prototype
    Enhancing for Few-Shot Object Detection. In *ICCV*. IEEE, 9567–9576.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2021b) Aming Wu, Suqi Zhao, Cheng Deng, and Wei Liu. 2021b. Generalized
    and Discriminative Few-Shot Object Detection via SVD-Dictionary Enhancement. In
    *NIPS*. 6353–6364.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) Jiaxi Wu, Songtao Liu, Di Huang, and Yunhong Wang. 2020. Multi-scale
    Positive Sample Refinement for Few-Shot Object Detection. In *ECCV*. Springer,
    456–472.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022) Shuang Wu, Wenjie Pei, Dianwen Mei, Fanglin Chen, Jiandong
    Tian, and Guangming Lu. 2022. Multi-faceted Distillation of Base-Novel Commonality
    for Few-Shot Object Detection. In *ECCV*. Springer, 578–594.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao and Marlet (2020) Yang Xiao and Renaud Marlet. 2020. Few-Shot Object Detection
    and Viewpoint Estimation for Objects in the Wild. In *ECCV*. Springer, 192–210.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie and Zheng (2022) Johnathan Xie and Shuai Zheng. 2022. Zero-shot Object Detection
    Through Vision-Language Embedding Alignment. In *ICDM*. IEEE, 1–15.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiong and Liu (2022) Wuti Xiong and Li Liu. 2022. CD-FSOD: A Benchmark for
    Cross-domain Few-shot Object Detection. *arXiv preprint arXiv:2210.05311* (2022).'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2021) Honghui Xu, Xinqing Wang, Faming Shao, Baoguo Duan, and Peng
    Zhang. 2021. Few-Shot Object Detection via Sample Processing. *IEEE Access* 9
    (2021), 29207–29221.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2022) Caixia Yan, Xiaojun Chang, Minnan Luo, Huan Liu, Xiaoqin Zhang,
    and Qinghua Zheng. 2022. Semantics-guided contrastive network for zero-shot object
    detection. *TPAMI* (2022).
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2020) Caixia Yan, Qinghua Zheng, Xiaojun Chang, Minnan Luo, Chung-Hsing
    Yeh, and Alexander G. Hauptmann. 2020. Semantics-Preserving Graph Propagation
    for Zero-Shot Object Detection. *TIP* 29 (2020), 8163–8176.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2019) Xiaopeng Yan, Ziliang Chen, Anni Xu, Xiaoxi Wang, Xiaodan
    Liang, and Liang Lin. 2019. Meta R-CNN: Towards General Solver for Instance-Level
    Low-Shot Learning. In *ICCV*. IEEE, 9576–9585.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022) Hanqing Yang, Sijia Cai, Hualian Sheng, Bing Deng, Jianqiang
    Huang, Xian-Sheng Hua, Yong Tang, and Yu Zhang. 2022. Balanced and Hierarchical
    Relation Learning for One-shot Object Detection. In *CVPR*. IEEE, 7581–7590.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2021a) Hanqing Yang, Yongliang Lin, Hong Zhang, Yu Zhang, and Bin
    Xu. 2021a. Towards improving classification power for one-shot object detection.
    *Neurocomputing* 455 (2021), 390–400.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2018) Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi
    Parikh. 2018. Graph R-CNN for Scene Graph Generation. In *ECCV*. Springer, 690–706.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021b) Shuo Yang, Lu Liu, and Min Xu. 2021b. Free Lunch for Few-shot
    Learning: Distribution Calibration. In *ICLR*. OpenReview.net.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2020b) Yukuan Yang, Fangyun Wei, Miaojing Shi, and Guoqi Li. 2020b.
    Restoring Negative Information in Few-Shot Object Detection. In *NIPS*. 43–76.
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2020a) Ze Yang, Yali Wang, Xianyu Chen, Jianzhuang Liu, and Yu
    Qiao. 2020a. Context-Transformer: Tackling Object Confusion for Few-Shot Detection.
    In *AAAI*. AAAI Press, 12653–12660.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Ze Yang, Chi Zhang, Ruibo Li, Yi Xu, and Guosheng Lin. 2023.
    Efficient Few-Shot Object Detection via Knowledge Inheritance. *TIP* 32 (2023),
    321–334.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yilmaz et al. (2006) Alper Yilmaz, Omar Javed, and Mubarak Shah. 2006. Object
    tracking: A survey. *ACM Comput. Surv.* 38, 4 (2006), 13.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yurtsever et al. (2020) Ekim Yurtsever, Jacob Lambert, Alexander Carballo,
    and Kazuya Takeda. 2020. A Survey of Autonomous Driving: Common Practices and
    Emerging Technologies. *IEEE Access* 8 (2020), 58443–58469.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021a) Gongjie Zhang, Kaiwen Cui, Rongliang Wu, Shijian Lu, and
    Yonghong Tian. 2021a. PNPDet: Efficient Few-shot Detection without Forgetting
    via Plug-and-Play Sub-networks. In *WACV*. IEEE, 3822–3831.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021b) Gongjie Zhang, Zhipeng Luo, Kaiwen Cui, and Shijian Lu.
    2021b. Meta-detr: Few-shot object detection via unified image-level meta-learning.
    *arXiv preprint arXiv:2103.11731* (2021).'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020c) Licheng Zhang, Xianzhi Wang, Lina Yao, and Feng Zheng.
    2020c. Zero-Shot Object Detection with Textual Descriptions Using Convolutional
    Neural Networks. In *IJCNN*. IEEE, 1–6.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021c) Lu Zhang, Shuigeng Zhou, Jihong Guan, and Ji Zhang. 2021c.
    Accurate Few-Shot Object Detection With Support-Query Mutual Guidance and Hybrid
    Loss. In *CVPR*. Computer Vision Foundation / IEEE, 14424–14432.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020a) Shan Zhang, Dawei Luo, Lei Wang, and Piotr Koniusz. 2020a.
    Few-Shot Object Detection by Second-Order Pooling. In *ACCV*. Springer, 369–387.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022c) Shan Zhang, Naila Murray, Lei Wang, and Piotr Koniusz.
    2022c. Time-rEversed DiffusioN tEnsor Transformer: A New TENET of Few-Shot Object
    Detection. In *ECCV*. Springer, 310–328.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022d) Shan Zhang, Lei Wang, Naila Murray, and Piotr Koniusz.
    2022d. Kernelized Few-shot Object Detection with Efficient Integral Aggregation.
    In *CVPR*. IEEE, 19185–19194.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022a) Wenwen Zhang, Chengdong Dong, Jun Zhang, Hangguan Shan,
    and Eryun Liu. 2022a. Adaptive context- and scale-aware aggregation with feature
    alignment for one-shot object detection. *Neurocomputing* 514 (2022), 216–230.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Wang (2021) Weilin Zhang and Yu-Xiong Wang. 2021. Hallucination Improves
    Few-Shot Object Detection. In *CVPR*. IEEE, 13008–13017.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020b) Weilin Zhang, Yu-Xiong Wang, and David A Forsyth. 2020b.
    Cooperating RPN’s Improve Few-Shot Object Detection. *arXiv preprint arXiv:2011.10142*
    (2020).
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022b) Xiaosong Zhang, Feng Liu, Zhiliang Peng, Zonghao Guo, Fang
    Wan, Xiangyang Ji, and Qixiang Ye. 2022b. Integral Migrating Pre-trained Transformer
    Encoder-decoders for Visual Object Detection. *arXiv preprint arXiv:2205.09613*
    (2022).
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2020) Shizhen Zhao, Changxin Gao, Yuanjie Shao, Lerenhan Li, Changqian
    Yu, Zhong Ji, and Nong Sang. 2020. GTNet: Generative Transfer Network for Zero-Shot
    Object Detection. In *AAAI*. AAAI Press, 12967–12974.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2022a) Yizhou Zhao, Xun Guo, and Yan Lu. 2022a. Semantic-aligned
    Fusion Transformer for One-shot Object Detection. In *CVPR*. IEEE, 7591–7601.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2022b) Zhiyuan Zhao, Qingjie Liu, and Yunhong Wang. 2022b. Exploring
    Effective Knowledge Transfer for Few-shot Object Detection. In *ACM MM*. ACM,
    6831–6839.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng and Cui (2021) Ye Zheng and Li Cui. 2021. Zero-Shot Object Detection With
    Transformers. In *ICIP*. IEEE, 444–448.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2020) Ye Zheng, Ruoran Huang, Chuanqi Han, Xi Huang, and Li Cui.
    2020. Background Learnable Cascade for Zero-Shot Object Detection. In *ACCV*.
    Springer, 107–123.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2021) Ye Zheng, Jiahong Wu, Yongqiang Qin, Faen Zhang, and Li
    Cui. 2021. Zero-Shot Instance Segmentation. In *CVPR*. IEEE, 2593–2602.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2021a) Chenchen Zhu, Fangyi Chen, Uzair Ahmed, Zhiqiang Shen, and
    Marios Savvides. 2021a. Semantic Relation Reasoning for Shot-Stable Few-Shot Object
    Detection. In *CVPR*. IEEE, 8782–8791.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020a) Pengkai Zhu, Hanxiao Wang, and Venkatesh Saligrama. 2020a.
    Don’t Even Look Once: Synthesizing Features for Zero-Shot Detection. In *CVPR*.
    IEEE, 11690–11699.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2020b) Pengkai Zhu, Hanxiao Wang, and Venkatesh Saligrama. 2020b.
    Zero Shot Detection. *IEEE Trans. Circuits Syst. Video Technol.* 30, 4 (2020),
    998–1010.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2021b) Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
    and Jifeng Dai. 2021b. Deformable DETR: Deformable Transformers for End-to-End
    Object Detection. In *ICLR*. OpenReview.net.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuang et al. (2021) Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun
    Zhu, Hengshu Zhu, Hui Xiong, and Qing He. 2021. A Comprehensive Survey on Transfer
    Learning. *Proc. IEEE* 109, 1 (2021), 43–76.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
