- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: æœªåˆ†ç±»'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:51:18'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2109.14545] Activation Functions in Deep Learning: A Comprehensive Survey
    and Benchmark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2109.14545](https://ar5iv.labs.arxiv.org/html/2109.14545)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shiv Ram DubeyÂ¹, Satish Kumar SinghÂ¹, Bidyut Baran ChaudhuriÂ² Â¹Computer Vision
    and Biometrics Laboratory, Indian Institute of Information Technology, Allahabad,
    India.
  prefs: []
  type: TYPE_NORMAL
- en: Â²Techno India University, Kolkata, India and Indian Statistical Institute, Kolkata,
    India.
  prefs: []
  type: TYPE_NORMAL
- en: srdubey@iiita.ac.in, sk.singh@iiita.ac.in, bidyutbaranchaudhuri@gmail.com
  prefs: []
  type: TYPE_NORMAL
- en: This paper is accepted in Neurocomputing. Copyright will be transferred to Elsevier.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Neural networks have shown tremendous growth in recent years to solve numerous
    problems. Various types of neural networks have been introduced to deal with different
    types of problems. However, the main goal of any neural network is to transform
    the non-linearly separable input data into more linearly separable abstract features
    using a hierarchy of layers. These layers are combinations of linear and nonlinear
    functions. The most popular and common non-linearity layers are activation functions
    (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper,
    a comprehensive overview and survey is presented for AFs in neural networks for
    deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based,
    ReLU based, ELU based, and Learning based are covered. Several characteristics
    of AFs such as output range, monotonicity, and smoothness are also pointed out.
    A performance comparison is also performed among 18 state-of-the-art AFs with
    different networks on different types of data. The insights of AFs are presented
    to benefit the researchers for doing further research and practitioners to select
    among different choices. The code used for experimental comparison is released
    at: [https://github.com/shivram1987/ActivationFunctions](https://github.com/shivram1987/ActivationFunctions).'
  prefs: []
  type: TYPE_NORMAL
- en: '^â€ ^â€ journal: Neurocomputing'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, deep learning has shown a tremondous growth to solve the challenging
    problems such as object detection [[1](#bib.bib1)], semantic segmentation [[2](#bib.bib2)],
    person re-identification [[3](#bib.bib3)], image retrieval [[4](#bib.bib4)], anomaly
    detection [[5](#bib.bib5)], skin disease diagnosis [[6](#bib.bib6)], and many
    more. Various types of neural networks have been defined in deep learning to learn
    abstract features from data, such as Multilayer Perceptron (MLP) [[7](#bib.bib7)],
    Convolutional Neural Networks (CNN) [[8](#bib.bib8)], Recurrent Neural Networks
    (RNN) [[9](#bib.bib9)], and Generative Adversarial Networks (GAN) [[10](#bib.bib10)].
    The important aspects of neural networks include weight initialization [[11](#bib.bib11)],
    loss functions [[12](#bib.bib12)], different layers [[13](#bib.bib13)], overfitting
    [[14](#bib.bib14)], and optimization [[15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The activation functions (AFs) play a very crucial role in neural networks
    [[16](#bib.bib16)] by learning the abstract features through non-linear transformations.
    Some common properties of the AFs are as follows: a) it should add the non-linear
    curvature in the optimization landscape to improve the training convergence of
    the network; b) it should not increase the computational complexity of the model
    extensively; c) it should not hamper the gradient flow during training; d) it
    should retain the distribution of data to facilitate the better training of the
    network. Several AFs have been explored in recent years for deep learning to achieve
    the above mentioned properties. This survey is dedicated to the developments in
    the area of AFs in neural networks. The insights of the different AFs are presented
    along with the reasoning to benefit the deep learning community. The major contributions
    of this survey are outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This survey provides a detailed classification for a wide range of AFs. It also
    includes the AFs very comprehensively, including Logistic Sigmoid/Tanh, Rectified
    Unit, Exponential Unit, and Adaptive AFs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This survey enriches the reader with the state-of-the-art AFs with analysis
    from various perspectives. It specifically covers the progress in AFs for deep
    learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This survey also summarizes the AFs with brief highlights and important discussions
    to depict its suitability for different types of data (Refer to Table [6](#S7.T6
    "Table 6 â€£ 7.5 Kernel Activation Functions â€£ 7 Miscellaneous Activation Functions
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This survey is compared with the existing survey and performance analysis to
    show its importance (Refer to Table [7](#S9.T7 "Table 7 â€£ 9.1 Comparison with
    Existing Survey/Performance Analysis â€£ 9 Performance Comparison and Analysis â€£
    Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This paper also presents the performance comparisons on 4 benchmark datasets
    of different modalities using 18 state-of-the-art AFs with different types of
    networks (Refer to Tables [8](#S9.T8 "Table 8 â€£ 9.1 Comparison with Existing Survey/Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"), [9](#S9.T9 "Table 9 â€£ 9.1 Comparison
    with Existing Survey/Performance Analysis â€£ 9 Performance Comparison and Analysis
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")
    and [11](#S9.T11 "Table 11 â€£ 9.2 Experimental Performance Analysis â€£ 9 Performance
    Comparison and Analysis â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The evolution of AFs is illustrated in Section [2](#S2 "2 Evolution of Activation
    Functions â€£ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark"). The progress in Logistic Sigmoid and Tanh, rectified, exponential,
    adaptive and miscellaneous AFs are summarized in Section [3](#S3 "3 Logistic Sigmoid
    and Tanh Based AFs â€£ Activation Functions in Deep Learning: A Comprehensive Survey
    and Benchmark"), [4](#S4 "4 Rectified Activation Functions â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark"), [5](#S5 "5 Exponential
    Activation Functions â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark"), [6](#S6 "6 Learning/Adaptive Activation Functions â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark"), and [7](#S7
    "7 Miscellaneous Activation Functions â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark"), respectively. Some aspects of AFs are
    discussed in Section [8](#S8 "8 Aspects of Activation Functions â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark"). A comprehensive performance
    analysis is conducted in Section [9](#S9 "9 Performance Comparison and Analysis
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark").
    A summary with conclusions and recommendations is provided in Section [10](#S10
    "10 Conclusion and Recommendations â€£ Activation Functions in Deep Learning: A
    Comprehensive Survey and Benchmark").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Evolution of Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A linear function can be thought of as a simple AF which outputs $c\times x$
    for input $x$ with $c$ as a constant. The linear AF is illustrated in Fig. [1](#S2.F1
    "Figure 1 â€£ 2 Evolution of Activation Functions â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") for $c=1$, i.e., identity function.
    Note that the linear AF does not add non-linearity into the network. However,
    the non-linearity needs to be introduced in the neural networks. Otherwise, a
    neural network produces the output as a linear function of inputs inspite of having
    several layers. Moreover, in practice data is generally not linearly separable;
    hence, the non-linear layers help to project the data in non-linear fashion in
    feature space which can be used with different objective functions. This section
    provides an overview of the evolution of AFs for deep learning. A classification
    is presented in Fig. [2](#S2.F2 "Figure 2 â€£ 2 Evolution of Activation Functions
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")
    in terms of the different properties and characteristic types.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/feda0172686c2a943b0799c0d5215638.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An illustration of Linear, Logistic Sigmoid and Tanh AFs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic Sigmoid/Tanh Unit Based Activation Functions: In order to introduce
    the non-linearity into the neural networks, the Logistic Sigmoid and Tanh AFs
    have been used in the early days. The firing of bilogical neurons was the motivation
    of using the Logistic Sigmoid and Tanh AFs with artificial neurons. The Logistic
    Sigmoid AF is a very popular and traditional non-linear function. It is given
    as,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Logistic Sigmoid}(x)=\frac{1}{1+e^{-x}}.$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'This AF squashes the output between [$0$, $1$] as shown in Fig. [1](#S2.F1
    "Figure 1 â€£ 2 Evolution of Activation Functions â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"). The output of the Logistic Sigmoid
    function is saturated for higher and lower inputs, which leads to vanishing gradient
    problem. The vanishing gradient problem depicts to a scenario where the gradient
    of objective function w.r.t. a parameter becomes very close to zero and leads
    to almost no update in the parameters during the training of the network using
    stochastic gradient descent technique. Hence, the training is almost killed under
    vanishing gradient scenario. Moreover, the output not following a zero-centric
    nature leads to poor convergence. The Tanh function has also been used as the
    AF in neural networks. It is similar to the Logistic Sigmoid function while exhibiting
    the zero centric property as depicted in Fig. [1](#S2.F1 "Figure 1 â€£ 2 Evolution
    of Activation Functions â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark"). The Tanh function is written as,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{Tanh}(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}.$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'The Tanh function also squashes the inputs, but in $[-1,1]$. The drawbacks
    of Logistic Sigmoid function such as vanishing gradient and computational complexity
    also exist with Tanh function. The Logistic Sigmoid and Tanh AFs majorly suffer
    from vanishing gradient. Several improvements have been proposed based on the
    Logistic Sigmoid and Tanh AFs which are described in Section [3](#S3 "3 Logistic
    Sigmoid and Tanh Based AFs â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark") in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/73af0dc8a5973ff1c8fc86ce1160d453.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Classification of activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rectified Linear Unit Based Activation Functions: The saturated output and
    increased complexity are the key limitations of above-mentioned Logistic Sigmoid
    and Tanh based AFs. The Rectified Linear Unit (ReLU) [[17](#bib.bib17)] has become
    the state-of-the-art AF due to its simplicity and improved performance. The ReLU
    was also used in the AlexNet model [[8](#bib.bib8)]. Various variants of ReLU
    have been investigated by tackling its drawbacks, such as non-utilization of negative
    values, limited non-linearity and unbounded output, as detailed in Section [4](#S4
    "4 Rectified Activation Functions â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Advantage and disadvantage of primary AFs.'
  prefs: []
  type: TYPE_NORMAL
- en: '| AFs | Diminishing | Limited | Optimization | Lack of | Computational |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| gradients | non-linearity | difficulty | adaptibility | inefficiency |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sigmoid | Yes | No | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Tanh | Yes | No | Partial | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| ReLU | Partial | Yes | Partial | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| ELU | No | Partial | No | Yes | Partial |'
  prefs: []
  type: TYPE_TB
- en: '| APL | No | Partial | No | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Swish | No | Partial | No | No | Partial |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Summary of Logistic Sigmoid and Tanh based activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name of AF | Parametric | Monotonic | Smooth | Bounded |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic Sigmoid | No | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Tanh | No | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Scaled Tanh (sTanh), 1998 [[18](#bib.bib18)] | Yes | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Rectified Hyperbolic Secant (ReSech), 2016 [[19](#bib.bib19)] | No | No |
    Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Scaled Sigmoid (sSigmoid), 2016 [[20](#bib.bib20)] | No | Yes | Yes | Yes
    |'
  prefs: []
  type: TYPE_TB
- en: '| Penalized Tanh (pTanh), 2016 [[20](#bib.bib20)] | No | Yes | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Hexpo, 2017 [[21](#bib.bib21)] | No | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Improved Sigmoid (ISigmoid), 2018 [[22](#bib.bib22)] | No | Yes | Yes | No
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sigmoid-Weighted Linear Units (SiLU), 2018 [[23](#bib.bib23)] | No | No |
    Yes | For negative inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Linearly Scaled Hyperbolic Tangent (LiSHT), 2019 [[24](#bib.bib24)] | No
    | No | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Elliott, 2019 [[25](#bib.bib25)] | No | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Soft-Root-Sign (SRS), 2020 [[26](#bib.bib26)] | Yes | No | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: 'Exponential Unit Based Activation Functions: The major problem faced by the
    Logistic Sigmoid and Tanh based AFs is with its saturated output for large positive
    and negative input. Similarly, the major problem with ReLU based AFs is with the
    under-utilization of negative values leading to vanishing gradient. In order to
    cope up with these limitations the exponential function based AFs have been used
    in the literature. The Exponential Linear Unit (ELU) [[27](#bib.bib27)] based
    AF utilizes the negative values with the help of the exponential function. Several
    AFs have been introduced in the literature as the ELU variants which are presented
    in Section [5](#S5 "5 Exponential Activation Functions â€£ Activation Functions
    in Deep Learning: A Comprehensive Survey and Benchmark") in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning/Adaptive Activation Functions: Most of the Sigmoid, Tanh, ReLU, and
    ELU based AFs are designed manually which might not be able to exploit the data
    complexity. The learning based adaptive AFs are the recent trends. This class
    of AFs contains learnable parameters, e.g. Adaptive Piecewise Linear (APL) [[28](#bib.bib28)]
    and Swish [[29](#bib.bib29)] AFs contain two and one learnable parameters, respectively.
    Recently, several learning based AFs have been proposed as illustrated in Section
    [6](#S6 "6 Learning/Adaptive Activation Functions â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Miscellaneous Activation Functions: In recent years, many other AFs have also
    been investigated as presented in Section [7](#S7 "7 Miscellaneous Activation
    Functions â€£ Activation Functions in Deep Learning: A Comprehensive Survey and
    Benchmark"). These activations include Softplus units, probabilistic functions,
    polynomial functions, and kernel functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S2.T1 "Table 1 â€£ 2 Evolution of Activation Functions â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark") highlights
    the advantage and disadvantage of the primary AFs in terms of the diminishing
    gradients, limited non-linearity, optimization difficulty, computational inefficiency
    and lack of adaptibility. It can be noticed that the Tanh function is computationally
    inefficient because it involves the computation of exponential multiple times
    [[30](#bib.bib30)]. However, in implementation it can be computed using single
    exponential with the help of Sigmoid function. These limitations in the existing
    AFs have been the driving factors for the development of recent AFs as surveyed
    in the further sections of this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Logistic Sigmoid and Tanh Based AFs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The traditional AFs such as Logistic Sigmoid and Tanh were used very extensively
    in the early days of neural networks. However, these AFs had shown the hurdle
    to train the deep networks due to their saturated output. Several attempts have
    also been made to improve these AFs for different networks. Table [2](#S2.T2 "Table
    2 â€£ 2 Evolution of Activation Functions â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") presents the comparison of Logistic Sigmoid
    and Tanh based AFs in terms of their properties including parametric, monotonic,
    smooth and bounded.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to tackle the limited output range and zero gradient problems of Tanh,
    a scaled Hyperbolic Tangent (sTanh) is used in [[18](#bib.bib18)] which is defined
    as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $sTanh(x)=A\times Tanh(B\times x)$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: with the output range in $[-A,A]$. A Parametric Sigmoid Function (PSF) is proposed
    as a continuous, differentiable, and bounded function as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PSF(x)=\frac{1}{(1+e^{-x})^{m}}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $m$ is a hyperparameter [[31](#bib.bib31)]. The gradient flow is improved
    for the higher value of $m$. The sum of shifted log-sigmoid is also explored as
    an AF [[32](#bib.bib32)] which retains the symmetry in the generated features.
    The Rectified Hyperbolic Secant (ReSech) AF is differentiable, symmetric, and
    bounded [[19](#bib.bib19)] which is given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ReSech(x)=x\times Sech(x)$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: with the output range in $[-1,1]$. However, it exhibits the vanishing gradient
    problem due to saturating behavior for both large positive and large negative
    inputs. The training of deep networks become difficult due to the uniform slope
    of the Logistic Sigmoid and Tanh AFs near the origin [[20](#bib.bib20)]. To minimize
    this limitation, the Scaled Sigmoid (sSigmoid) is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $sSigmoid(x)=(4\times Sigmoid(x)-2)$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: with the output range in $[-2,2]$ and the Penalized Tanh (pTanh) is defined
    as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $pTanh(x)=\begin{cases}Tanh(x),&amp;x\geq 0\\ a\times Tanh(x),&amp;x<0\end{cases}$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: with the output range in $[-a,1]$ where $a\in(0,1)$. However, sSigmoid and pTanh
    AFs also suffer from the vanishing gradient problem. It is noticed that the pTanh
    AF performs better for Natural Language Processing (NLP) tasks [[33](#bib.bib33)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Summary of Rectified Linear Unit based activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Parametric | Monotonic | Smooth | Bounded |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rectified Linear Unit (ReLU), 2010 [[17](#bib.bib17)] | No | Yes | No | For
    negative inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Leaky ReLU (LReLU), 2013 [[34](#bib.bib34)] | No | Yes | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Parametric ReLU (PReLU), 2015 [[35](#bib.bib35)] | Yes | Yes | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Randomized ReLU (RReLU), 2015 [[35](#bib.bib35)] | No | Yes | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Concatenated ReLU (CReLU), 2016 [[36](#bib.bib36)] | No | Yes | No | For
    negative inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Bounded ReLU (BReLU), 2016 [[37](#bib.bib37)] | No | Yes | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Parametric Tanh Linear Unit (PTELU), 2017 [[38](#bib.bib38)] | Yes | Yes
    | Yes | For negative inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Flexible ReLU (FReLU), 2018 [[39](#bib.bib39)] | Yes | Yes | No | For negative
    inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Elastic ReLU (EReLU), 2018 [[40](#bib.bib40)] | No | Yes | No | For negative
    inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Randomly Translational ReLU (RTReLU), 2018 [[41](#bib.bib41)] | No | Yes
    | No | For negative inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Dual ReLU (DualReLU), 2018 [[42](#bib.bib42)] | No | Yes | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Paired ReLU (PairedReLU), 2018 [[43](#bib.bib43)] | Yes | Yes | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Average Biased ReLU (ABReLU), 2018 [[44](#bib.bib44)] | No | Yes | No | For
    negative inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Natural-Logarithm (NLReLU), 2019 [[45](#bib.bib45)] | No | Yes | No | For
    negative inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-bin Trainable Linear Units (MTLU), 2019 [[46](#bib.bib46)] | Yes |
    No | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Lipschitz ReLU (L-ReLU), 2020 [[47](#bib.bib47)] | Yes | Depends upon $\phi$
    and $\eta$ | Depends upon $\phi$ and $\eta$ | Depends upon $\phi$ and $\eta$ |'
  prefs: []
  type: TYPE_TB
- en: A noisy AF is defined to overcome the vanishing gradient problem [[48](#bib.bib48)].
    Due to the added noise the gradients may flow easily even in the saturating regime.
    The vanishing gradient problem is minimized by the Hexpo function [[21](#bib.bib21)]
    which is similar to Tanh with a scaled gradient. It is given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Hexpo(x)=\begin{cases}-a\times(e^{-x/b}-1),&amp;x\geq 0\\ c\times(e^{x/d}-1),&amp;x<0\end{cases}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $[-c,a]$. The output of the sigmoid function is multiplied
    with its input in sigmoid-weighted linear unit (SiLU) AF [[23](#bib.bib23)] as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $SiLU(x)=x\times Sigmoid(x)$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $(-0.5,\infty)$. At the same time an improved logistic
    Sigmoid (ISigmoid) AF [[22](#bib.bib22)] is proposed to solve the vanishing gradient
    problem of Sigmoid with the help of a piecewise combination of sigmoidal and linear
    functions. It is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="ISigmoid(x)=\begin{cases}\alpha\times(x-a)+Sigmoid(a),&amp;x\geq
    a\\ Sigmoid(x),&amp;-a<x<a\\'
  prefs: []
  type: TYPE_NORMAL
- en: \alpha\times(x+a)+Sigmoid(a),&amp;x\leq-a\end{cases}" display="block"><semantics
    ><mrow  ><mrow 
    ><mi  >I</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >S</mi><mo lspace="0em" rspace="0em"
     >â€‹</mo><mi 
    >i</mi><mo lspace="0em" rspace="0em" 
    >â€‹</mo><mi  >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >m</mi><mo lspace="0em" rspace="0em"
     >â€‹</mo><mi 
    >o</mi><mo lspace="0em" rspace="0em" 
    >â€‹</mo><mi  >i</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >d</mi><mo lspace="0em" rspace="0em"
     >â€‹</mo><mrow 
    ><mo stretchy="false"  >(</mo><mi
     >x</mi><mo stretchy="false" 
    >)</mo></mrow></mrow><mo  >=</mo><mrow
     ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" 
    ><mtr  ><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mrow 
    ><mrow  ><mi
     >Î±</mi><mo
    lspace="0.222em" rspace="0.222em"  >Ã—</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mrow
     ><mi
     >x</mi><mo
     >âˆ’</mo><mi
     >a</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >+</mo><mrow
     ><mi 
    >S</mi><mo lspace="0em" rspace="0em"
     >â€‹</mo><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >m</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >o</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >d</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >a</mi><mo stretchy="false"
     >)</mo></mrow></mrow></mrow><mo
     >,</mo></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mi 
    >x</mi><mo  >â‰¥</mo><mi
     >a</mi></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><mrow  ><mi
     >S</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >m</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >o</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >d</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi
     >x</mi><mo stretchy="false"
     >)</mo></mrow></mrow><mo
     >,</mo></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mrow 
    ><mo  >âˆ’</mo><mi
     >a</mi></mrow><mo
     ><</mo><mi 
    >x</mi><mo  ><</mo><mi
     >a</mi></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><mrow  ><mrow
     ><mi 
    >Î±</mi><mo lspace="0.222em" rspace="0.222em"
     >Ã—</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mrow
     ><mi
     >x</mi><mo
     >+</mo><mi
     >a</mi></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >+</mo><mrow
     ><mi 
    >S</mi><mo lspace="0em" rspace="0em"
     >â€‹</mo><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >m</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >o</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >d</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >a</mi><mo stretchy="false"
     >)</mo></mrow></mrow></mrow><mo
     >,</mo></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mi 
    >x</mi><mo  >â‰¤</mo><mrow
     ><mo 
    >âˆ’</mo><mi  >a</mi></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><ci 
    >ğ¼</ci><ci  >ğ‘†</ci><ci
     >ğ‘–</ci><ci 
    >ğ‘”</ci><ci  >ğ‘š</ci><ci
     >ğ‘œ</ci><ci 
    >ğ‘–</ci><ci  >ğ‘‘</ci><ci
     >ğ‘¥</ci></apply><apply 
    ><csymbol cd="latexml"  >cases</csymbol><apply
     ><apply 
    ><ci 
    >ğ›¼</ci><apply 
    ><ci 
    >ğ‘¥</ci><ci 
    >ğ‘</ci></apply></apply><apply 
    ><ci 
    >ğ‘†</ci><ci 
    >ğ‘–</ci><ci 
    >ğ‘”</ci><ci 
    >ğ‘š</ci><ci 
    >ğ‘œ</ci><ci 
    >ğ‘–</ci><ci 
    >ğ‘‘</ci><ci 
    >ğ‘</ci></apply></apply><apply 
    ><ci  >ğ‘¥</ci><ci
     >ğ‘</ci></apply><apply
     ><ci 
    >ğ‘†</ci><ci 
    >ğ‘–</ci><ci 
    >ğ‘”</ci><ci 
    >ğ‘š</ci><ci 
    >ğ‘œ</ci><ci 
    >ğ‘–</ci><ci 
    >ğ‘‘</ci><ci 
    >ğ‘¥</ci></apply><apply 
    ><apply  ><apply
     ><ci 
    >ğ‘</ci></apply><ci 
    >ğ‘¥</ci></apply><apply 
    ><ci  >ğ‘</ci></apply></apply><apply
     ><apply 
    ><ci 
    >ğ›¼</ci><apply 
    ><ci 
    >ğ‘¥</ci><ci 
    >ğ‘</ci></apply></apply><apply 
    ><ci 
    >ğ‘†</ci><ci 
    >ğ‘–</ci><ci 
    >ğ‘”</ci><ci 
    >ğ‘š</ci><ci 
    >ğ‘œ</ci><ci 
    >ğ‘–</ci><ci 
    >ğ‘‘</ci><ci 
    >ğ‘</ci></apply></apply><apply 
    ><ci  >ğ‘¥</ci><apply
     ><ci 
    >ğ‘</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >ISigmoid(x)=\begin{cases}\alpha\times(x-a)+Sigmoid(a),&x\geq
    a\\ Sigmoid(x),&-a<x<a\\ \alpha\times(x+a)+Sigmoid(a),&x\leq-a\end{cases}</annotation></semantics></math>
    |  | (10) |
  prefs: []
  type: TYPE_NORMAL
- en: in the output range of $(-\infty,\infty)$. The Linearly scaled hyperbolic tangent
    (LiSHT) AF scales the Tanh in a linear fashion to overcome the vanishing gradient
    issue [[24](#bib.bib24)]. The LiSHT can be defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $LiSHT(x)=x\times Tanh(x)$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $[0,\infty)$. The LiSHT function is symmetric, but is
    has the shortcoming of including unbounded and non-negative outputs only. The
    Elliott AF [[25](#bib.bib25)] is similar to Sigmoid function in terms of the characteristics
    diagram and defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Elliott(x)=\frac{0.5\times x}{1+&#124;x&#124;}+0.5$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $[0,1]$. The Soft-Root-Sign (SRS) AF [[26](#bib.bib26)]
    is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $SRS(x)=\frac{x}{\frac{x}{\alpha}+e^{-x/\beta}}$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $[\frac{\alpha\times\beta}{\beta-\alpha\times e},\alpha]$
    where $\alpha$ and $\beta$ are the learnable parameters. The use of additional
    parameters increases the complexity of the SRS function. Most of the variants
    of Sigmoid/Tanh AFs have tried to overcome the vanishing gradient issue. However,
    this issue is still present in most of these AFs.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Rectified Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A summary of rectified AFs is illustrated in Table [3](#S3.T3 "Table 3 â€£ 3
    Logistic Sigmoid and Tanh Based AFs â€£ Activation Functions in Deep Learning: A
    Comprehensive Survey and Benchmark"). Rectified Linear Unit (ReLU) is a simple
    function which is the identity function for positive input and zero for negative
    input and given as,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ReLU(x)=max(0,x)=\begin{cases}x,&amp;\text{if }x\geq 0\\ 0,&amp;\text{otherwise}\end{cases}.$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: Hence, the range of ReLU is $[0,\infty)$. The gradient for positive and negative
    inputs is one and zero, respectively. The ReLU function solves the problem of
    computational complexity of the Logistic Sigmoid and Tanh functions. The downside
    of ReLU is with the vanishing gradient problem for the negative inputs. In spite
    of having the vanishing gradient problem, the ReLU AF has been used very extensively
    with the deep learning models. The advancements in ReLU based AFs are discussed
    in the rest of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 On the Non-utilization of Negative Values of ReLU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vanishing gradient is the main problem with ReLU AF which is caused due to the
    non-utilization of negative values. A Leaky Rectified Linear Unit (LReLU) is the
    extension of ReLU by utilizing the negative values [[34](#bib.bib34)]. The LReLU
    is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $LReLU(x)=\begin{cases}x,&amp;x\geq 0\\ 0.01\times x,&amp;x<0\end{cases}$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $(-\infty,\infty)$. The LReLU has been used in many applications
    with promising performance. One major problem associated with LReLU is the finding
    of the right slope in linear function for negative inputs. Different slopes might
    be suited for different problems and different networks. Thus, it is extended
    to Parametric ReLU (PReLU) by considering the slope for negative input as a trainable
    parameter [[35](#bib.bib35)]. The PReLU is given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PReLU(x)=\begin{cases}x,&amp;x\geq 0\\ p\times x,&amp;x<0\end{cases}$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $(-\infty,\infty)$ where $p$ is the trainable parameter.
    However, it can lead to overfitting easily which is the downside of PReLU. The
    Maxout layer, which computes the maximum of several linear units, is also used
    as AF [[49](#bib.bib49)]. Both ReLU and Leaky ReLU can be seen as the special
    cases of Maxout. The randomized ReLU (RReLU) considers the slope of LReLU randomly
    during training sampled from an uniform distribution $U(l,u)$ [[50](#bib.bib50)].
    The RReLU is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $RReLU(x)=\begin{cases}x,&amp;x\geq 0\\ R\times x,&amp;x<0\end{cases}$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $(-\infty,\infty)$ where $R\sim{~{}}U(l,u)$, $l<u$ and
    $l,u\in[0,1)$. It uses a deterministic value $x/\left(\frac{l+u}{2}\right)$ during
    test time.
  prefs: []
  type: TYPE_NORMAL
- en: The ReLU is not able to utilize the potential useful information from the negative
    values. In most of the networks, the feature map given as the input to AF is dense
    near zero. Thus, a small jitter in the rectification point can lead to difficulty
    in training. Concatenated ReLU (CReLU) [[36](#bib.bib36)] concatenates the ReLUâ€™s
    output over original input and negated input. The CReLU can be given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $CReLU(x)=[ReLU(x),ReLU(-x)]$ |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $[0,\infty)$. The CReLU is derived from the fact that
    the lower layer kernels in CNN models form pairs with opposite phases. The shifting
    of the feature map with multiple biases is also performed before the ReLU layer
    [[51](#bib.bib51)]. However, it increases the model complexity as more ReLUs are
    required. A Parametric Tan Hyperbolic Linear Unit (P-TELU) is also used as an
    AF [[38](#bib.bib38)]. The P-TELU is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PTELU(x)=\begin{cases}x,&amp;x\geq 0\\ \alpha\times\text{Tanh}(\beta\times
    x),&amp;x<0\end{cases}$ |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $[-\alpha,\infty)$ where $\{\alpha,\beta\}\geq 0$ are
    the learnable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Summary of Exponential Linear Unit based activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Parametric | Monotonic | Smooth | Bounded |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Exponential Linear Unit (ELU), 2016 [[27](#bib.bib27)] | Yes | Yes | Yes
    | For negative inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Scaled ELU (SELU), 2017 [[52](#bib.bib52)] | Yes | Yes | Yes | For negative
    inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Continuously Differentiable ELU (CELU), 2017 [[53](#bib.bib53)] | Yes | Yes
    | No | For negative inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Parametric ELU (PELU), 2017 [[54](#bib.bib54)] | Yes | Yes | No | For negative
    inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Multiple PELU (MPELU), 2018 [[55](#bib.bib55)] | Yes | Yes | No | For negative
    inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Fast ELU (FELU), 2019 [[56](#bib.bib56)] | Yes | Yes | No | For negative
    inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Parametric Rectified Exponential Unit (PREU), 2019 [[57](#bib.bib57)] | Yes
    | No | Yes | For negative inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Elastic ELU (EELU), 2020 [[58](#bib.bib58)] | Yes | Yes | No | For negative
    inputs |'
  prefs: []
  type: TYPE_TB
- en: '| Parametric Deformable ELU (PDELU), 2020 [[59](#bib.bib59)] | Yes | Yes |
    Yes | For negative inputs |'
  prefs: []
  type: TYPE_TB
- en: The Flexible ReLU (FReLU) [[39](#bib.bib39)] captures the negative values with
    a rectified point which is considered as trainable in the Shifted ReLU [[39](#bib.bib39)].
    The FReLU is given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $FReLU(x)=ReLU(x)+b$ |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $[b,\infty)$. A similar arrangement is also followed
    by Random Translation ReLU (RTReLU) [[41](#bib.bib41)] by utilizing an offset,
    sampled from a Gaussian distribution, given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $RTReLU(x)=\begin{cases}x+a,&amp;x+a>0\\ 0,&amp;x+a\leq 0\end{cases}$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $[0,\infty)$ where $a$ is a random number. At test time,
    the offset is set to zero. A data dependent Average Biased ReLU (AB-ReLU) [[44](#bib.bib44)]
    is also investigated to tackle the negative values by a horizontal shifting based
    on the average of features. The ABReLU can be written as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ABReLU(x)=\begin{cases}x-\beta,&amp;x-\beta\geq 0\\ 0,&amp;x-\beta<0\end{cases}$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[0,\infty)$ where $\beta$ is computed as the average
    of input activation map to the activation function. The batch dependent threshold
    for the ReLU is used by the Dynamic ReLU (D-ReLU) [[60](#bib.bib60)]. The Dual
    ReLU (DualReLU) [[42](#bib.bib42)] is a two dimensional AF for recurrent neural
    networks. The DualReLU is given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $DualReLU(a,b)=\max(0,a)-\max(0,b)$ |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $(-\infty,\infty)$ where $a$ and $b$ are the inputs in
    different dimensions. Similar to the CReLU, the PairedReLU AF is used for image
    super-resolution [[43](#bib.bib43)]. The PairedReLU is given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PairedReLU(x)=[\max(s\times x-\theta,0),max(s_{p}\times x-\theta_{p},0)]$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $(-\infty,\infty)$. However, the computational complexity
    of PairedReLU is increased as compared to CReLU. In another attempt, V-shaped
    ReLU (vReLU) AF [[61](#bib.bib61)] is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $vReLU(x)=\begin{cases}x,&amp;x\geq 0\\ -x,&amp;x<0\end{cases}$ |  | (25)
    |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[0,\infty]$. The vReLU activation function suffers
    from the non-symmetric output. The SignReLU AF utilizes the negative values using
    the Softsign function [[62](#bib.bib62)]. The positive part of SignReLU is the
    same as the ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: A Displaced ReLU (DisReLU) [[63](#bib.bib63)] is designed as a generalization
    of Shifted ReLU [[39](#bib.bib39)]. The DisReLU displaces the rectification point
    to consider the negative values, given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $DisReLU(x)=\begin{cases}x,&amp;x\geq-\delta\\ -\delta,&amp;x<-\delta\end{cases}$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[-\delta,\infty]$. A Bendable Linear Unit (BLU)
    AF is investigated as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $BLU(x)=\beta\times(\sqrt{x^{2}+1}-1)+x$ |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: where $-1\leq\beta\leq 1$ is a learnable parameter to adapt the shape between
    the identity function and a rectifier function [[64](#bib.bib64)]. A Lipschitz
    ReLU (L-ReLU) AF uses the piecewise linear functions to model the degree of presence
    and the degree of absence of features [[47](#bib.bib47)]. The L-ReLU is defined
    as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L\text{-}ReLU(x)=\begin{cases}\max(\phi(x),0),&amp;x\geq 0\\ \min(\eta(x),0),&amp;x<0\end{cases}$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: where $\phi$ and $\eta$ are non-linear functions. Moreover, the range of L-ReLU
    also depends upon the values of $\phi$ and $\eta$ functions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 On the Limited Non-linearity of ReLU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: S-shaped ReLU (SReLU) increases the non-linearity in ReLU by combining three
    linear functions with four learnable parameters [[65](#bib.bib65)]. On a similar
    line, Multi-bin Trainable Linear Unit (MTLU) [[46](#bib.bib46)] considers multiple
    bins to increase the non-linear capacity. The MTLU can be written as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="MTLU(x)=\begin{cases}a_{0}\times
    x+b_{0},&amp;x\leq c_{0}\\ a_{k}\times x+b_{k},&amp;c_{k-1}<x\leq c_{k}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '...&amp;\\'
  prefs: []
  type: TYPE_NORMAL
- en: a_{K}\times x+b_{K},&amp;c_{K-1}<x\end{cases}" display="block"><semantics ><mrow
     ><mrow  ><mi
     >M</mi><mo lspace="0em" rspace="0em"
     >â€‹</mo><mi 
    >T</mi><mo lspace="0em" rspace="0em" 
    >â€‹</mo><mi  >L</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >U</mi><mo lspace="0em" rspace="0em"
     >â€‹</mo><mrow 
    ><mo stretchy="false"  >(</mo><mi
     >x</mi><mo stretchy="false" 
    >)</mo></mrow></mrow><mo  >=</mo><mrow
     ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" 
    ><mtr  ><mtd
    class="ltx_align_left" columnalign="left"  ><mrow
     ><mrow 
    ><mrow  ><msub
     ><mi
     >a</mi><mn
     >0</mn></msub><mo
    lspace="0.222em" rspace="0.222em"  >Ã—</mo><mi
     >x</mi></mrow><mo
     >+</mo><msub
     ><mi 
    >b</mi><mn 
    >0</mn></msub></mrow><mo 
    >,</mo></mrow></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><mi  >x</mi><mo
     >â‰¤</mo><msub 
    ><mi  >c</mi><mn
     >0</mn></msub></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><mrow  ><mrow
     ><msub
     ><mi
     >a</mi><mi
     >k</mi></msub><mo
    lspace="0.222em" rspace="0.222em"  >Ã—</mo><mi
     >x</mi></mrow><mo
     >+</mo><msub
     ><mi 
    >b</mi><mi 
    >k</mi></msub></mrow><mo 
    >,</mo></mrow></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><msub  ><mi
     >c</mi><mrow
     ><mi 
    >k</mi><mo 
    >âˆ’</mo><mn 
    >1</mn></mrow></msub><mo 
    ><</mo><mi  >x</mi><mo
     >â‰¤</mo><msub 
    ><mi  >c</mi><mi
     >k</mi></msub></mrow></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mi mathvariant="normal"
     >â€¦</mi></mtd></mtr><mtr
     ><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><mrow  ><mrow
     ><msub
     ><mi
     >a</mi><mi
     >K</mi></msub><mo
    lspace="0.222em" rspace="0.222em"  >Ã—</mo><mi
     >x</mi></mrow><mo
     >+</mo><msub
     ><mi 
    >b</mi><mi 
    >K</mi></msub></mrow><mo 
    >,</mo></mrow></mtd><mtd class="ltx_align_left"
    columnalign="left"  ><mrow 
    ><msub  ><mi
     >c</mi><mrow
     ><mi 
    >K</mi><mo 
    >âˆ’</mo><mn 
    >1</mn></mrow></msub><mo 
    ><</mo><mi  >x</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><ci 
    >ğ‘€</ci><ci  >ğ‘‡</ci><ci
     >ğ¿</ci><ci 
    >ğ‘ˆ</ci><ci  >ğ‘¥</ci></apply><apply
     ><csymbol cd="latexml" 
    >cases</csymbol><apply 
    ><apply  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘</ci><cn
    type="integer"  >0</cn></apply><ci
     >ğ‘¥</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘</ci><cn
    type="integer"  >0</cn></apply></apply><apply
     ><ci 
    >ğ‘¥</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ğ‘</ci><cn type="integer" 
    >0</cn></apply></apply><apply 
    ><apply  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘</ci><ci
     >ğ‘˜</ci></apply><ci
     >ğ‘¥</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘</ci><ci
     >ğ‘˜</ci></apply></apply><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘</ci><apply
     ><ci 
    >ğ‘˜</ci><cn type="integer" 
    >1</cn></apply></apply><ci 
    >ğ‘¥</ci></apply><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘</ci><ci
     >ğ‘˜</ci></apply></apply></apply><ci
     >â€¦</ci><ci 
    ><mtext class="ltx_mathvariant_italic" 
    >otherwise</mtext></ci><apply 
    ><apply  ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘</ci><ci
     >ğ¾</ci></apply><ci
     >ğ‘¥</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘</ci><ci
     >ğ¾</ci></apply></apply><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ğ‘</ci><apply 
    ><ci  >ğ¾</ci><cn
    type="integer"  >1</cn></apply></apply><ci
     >ğ‘¥</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >MTLU(x)=\begin{cases}a_{0}\times
    x+b_{0},&x\leq c_{0}\\ a_{k}\times x+b_{k},&c_{k-1}<x\leq c_{k}\\ ...&\\ a_{K}\times
    x+b_{K},&c_{K-1}<x\end{cases}</annotation></semantics></math> |  | (29) |
  prefs: []
  type: TYPE_NORMAL
- en: having the output range in $(-\infty,\infty)$. The number of bins and the range
    of bins are the hyperparameters, whereas the linear function of a bin is trainable
    (i.e., $a_{0},...,a_{K}$ $b_{0},...,b_{K}$ are the learnable parameters). The
    non-differentiable nature at multiple points is the drawback of the MTLU. An Elastic
    ReLU (EReLU) considers a slope randomly drawn from a uniform distribution during
    the training for the positive inputs to control the amount of non-linearity [[40](#bib.bib40)].
    The EReLU is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $EReLU(x)=max(R\times x,0)$ |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $[0,\infty)$ where $R$ is a random number. At the test
    time, the EReLU becomes the identity function for positive inputs. The Linearized
    Sigmoidal Activation (LiSHA) function considers three linear functions to increase
    the non-linearity characteristics [[66](#bib.bib66)]. It is also extended to adaptive
    linear sigmoidal AF by learning the slope of upper and lower linear functions.
    The ReLU is combined with Tanh as Rectified Linear Tanh (ReLTanh) [[67](#bib.bib67)]
    to increase the non-linearity of ReLU and to overcome the vanishing gradient problem
    of Tanh. However, the ReLTanh is unbounded in both the positive and negative directions.
    Natural-Logarithm ReLU (NLReLU) modifies the ReLUâ€™s output for positive inputs
    using the logarithm function to increase the degree of nonlinearity [[45](#bib.bib45)].
    The NLReLU is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $NLReLU(x)=\ln(\beta\times\max(0,x)+1.0)$ |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[0,\infty)$ where $\beta$ is a constant. The NLReLU
    does not affect the negative regime, thus suffers from vanishing gradient. The
    concept of Leaky ReLU (LReLU) is further improved to Dynamic ReLU [[68](#bib.bib68)]
    by considering a mean square error (MSE) based additional hyperparameter. Thus,
    it can control the slope of the Dynamic ReLU in every epoch based on the convergence.
    A Piecewise Linear Unit (PLU) [[69](#bib.bib69)] is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PLU(x)=max(\alpha\times(x+c)-c,min(\alpha\times(x-c)+c,x))$ |  | (32)
    |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[-\infty,+\infty]$, where $\alpha$ and $c$ are the
    constants. Basically, the PLU activation function consists of three linear functions
    in pieces, but continuous. Hence, it avoids the saturation and leads to a good
    amount of gradient flow through the activation function during backpropagation
    in order to resolve the vanishing gradient problems of ReLU and Tanh. However,
    the PLU activation is unbounded in both positive and negative directions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 On the Unbounded Output of ReLU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The unbounded outputs of ReLU and many of its variants may lead to training
    instability. Moreover, the bounded AF is needed for the dedicated hardware based
    embedded system applications. ReLU is extended to Bounded ReLU (BReLU) [[37](#bib.bib37)]
    defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $BReLU(x)=\min(\max(0,x),A)$ |  | (33) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[0,A])$. The training stability is improved in BReLU
    due to two rectifications (i.e., at $0$ and $A$). ReLU is a common choice in practice
    in deep learning. ReLU based AFs are generally efficient. The major drawbacks
    of ReLU, such as gradient diminishing for negative inputs, limited non-linearity
    and unboundedness, are improved in the different AFs. However, the ReLU variants
    are not able to resolve all the issues of ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Exponential Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The exponential AFs tackle the gradient diminishing problem of ReLU. Table
    [4](#S4.T4 "Table 4 â€£ 4.1 On the Non-utilization of Negative Values of ReLU â€£
    4 Rectified Activation Functions â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark") lists the properties of the exponential AFs. The Exponential
    Linear Unit (ELU) [[27](#bib.bib27)] is given as,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x}-1),&amp;x\leq 0\end{cases}$
    |  | (34) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[-1,\infty)$ where $\alpha$ is a learnable parameter.
    The ELU function exhibits all the benefits of the ReLU function. The ELU is differentiable,
    saturates for large negative inputs and reduces the bias shift. The negative saturation
    regime of ELU adds some robustness to noise as compared to the Leaky ReLU and
    Parametric ReLU. The ELU is extended to Scaled ELU (SELU) [[52](#bib.bib52)] by
    using a scaling hyperparameter to make the slope larger than one for positive
    inputs. The SELU can be defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $SELU(x)=\lambda\times\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x}-1),&amp;x\leq
    0\end{cases}$ |  | (35) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[-\lambda,\infty)$ where $\alpha$ is a hyperparameter.
    Basically, the SELU induces self-normalization to automatically converge towards
    zero mean and unit variance. The Parametric ELU (PELU) [[54](#bib.bib54)] changes
    the saturation point and exponential decay and also regulates the slope of the
    linear function for the positive inputs for differentiability. The PELU AF can
    be written as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PELU(x)=\lambda\times\begin{cases}\frac{a}{b}\times x,&amp;x\geq 0\\
    a\times(e^{x/b}-1),&amp;x<0\end{cases}$ |  | (36) |'
  prefs: []
  type: TYPE_TB
- en: having $[-a,\infty)$ output range, where $a$ and $b$ are the trainable parameters.
    The parametric ELU is also explored in Continuously differentiable ELU (CELU)
    [[53](#bib.bib53)] for the negative inputs. The CELU is given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $CELU(x)=\begin{cases}x,&amp;x\geq 0\\ \alpha\times(e^{x/\alpha}-1),&amp;x<0\end{cases}$
    |  | (37) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[-\alpha,\infty)$ where $\alpha$ is a learnable
    parameter. The PELU is also extended to multiple PELU (MPELU) [[55](#bib.bib55)]
    by using two learnable parameters to represent MPELU as either rectified, exponential
    or combined. The MPELU can be expressed as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $MPELU(x)=\begin{cases}x,&amp;x>0\\ \alpha_{c}\times(e^{\beta_{c}\times
    x}-1),&amp;x\leq 0\end{cases}$ |  | (38) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[-\alpha_{c},\infty)$, where $\alpha_{c}$ and $\beta_{c}$
    are the trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: A soft exponential AF interpolates between the exponential, linear and logarithmic
    functions using the trainable parameter [[70](#bib.bib70)]. A Shifted ELU (ShELU)
    AF is also explored as a locally optimal function [[71](#bib.bib71)]. A Parametric
    Rectified Exponential Unit (PREU) [[57](#bib.bib57)] is designed as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PREU(x)=\begin{cases}\alpha\times x,&amp;x>0\\ \alpha\times x\times e^{\beta\times
    x},&amp;x\leq 0\end{cases}$ |  | (39) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[-1,\infty)$, where $\alpha$ and $\beta$ are the
    trainable parameters. The PREU utilizes the negative information near to zero
    effectively. The efficiency of ELU is improved in Fast ELU (FELU) AF [[56](#bib.bib56)]
    with the help of the simple displacement bits and integer algebra operations.
    The FELU is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $FELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times(e^{x/\ln(2)}-1),&amp;x\leq
    0\end{cases}$ |  | (40) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[-\alpha,\infty)$ with $\alpha$ as a learnable parameter.
    Recently, the properties of ELU and RELU have been utilized to design an Elastic
    ELU (EELU) AF [[58](#bib.bib58)]. The EELU is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $EELU(x)=\begin{cases}k\times x,&amp;x>0\\ \alpha\times(e^{\beta\times
    x}-1),&amp;x\leq 0\end{cases}$ |  | (41) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[-\alpha,\infty)$ where $\alpha$ and $\beta$ are
    the trainable parameters. The EELU preserves a small non-zero gradient for the
    negative input and exhibits an elastic slope for the positive input. A Parametric
    Deformable ELU (PDELU) AF tries to shift the mean value of output closer to zero
    using the flexible map shape [[59](#bib.bib59)]. The PDELU is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PDELU(x)=\begin{cases}x,&amp;x>0\\ \alpha\times([1+(1-t)\times x]^{\frac{1}{1-t}}-1),&amp;x\leq
    0\end{cases}$ |  | (42) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[-1,\infty)$ where $\alpha$ is a learnable parameter.
    A ReLU-Memristor-like AF (RMAF) [[72](#bib.bib72)] uses two hyperparameters to
    have ReLU like shape for positive input and to give more importance to the negative
    values near to zero. An Exponential Linear Sigmoid SquasHing (ELiSH) is defined
    in [[73](#bib.bib73)] as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ELiSH(x)=\begin{cases}x/(1+e^{-x}),&amp;x\geq 0\\ (e^{x}-1)/(1+e^{-x}),&amp;x<0\end{cases}$
    |  | (43) |'
  prefs: []
  type: TYPE_TB
- en: Moreover, it is also extended to HardELiSH which is a multiplication of HardSigmoid
    and Linear in the positive part and HardSigmoid and ELU in the negative part.
    Here, HardSigmoid is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $HardELish(x)=max(0,min(1,(x+1)/2)).$ |  | (44) |'
  prefs: []
  type: TYPE_TB
- en: The ELU based AFs exploit the negative inputs without compromising with the
    non-linearity. Some ELU variants also modify the function for positive inputs
    to make it bounded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Summary of adaptive and learning based activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Parametric | Monotonic | Smooth | Bounded |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Adaptive Piecewise Linear Unit (APL), 2015 [[28](#bib.bib28)] | Yes | No
    | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Spline AF (SAF), 2016 [[74](#bib.bib74)] | Yes | Yes | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Bi-Modal Derivative Adaptive Activation (BDAA), 2017 [[75](#bib.bib75)] |
    Yes | Yes | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Adaptive AF (AAF), 2018 [[76](#bib.bib76)] | Yes | Yes | No | No |'
  prefs: []
  type: TYPE_TB
- en: '| Swish, 2018 [[29](#bib.bib29)] | Yes | No | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| ESwish, 2018 [[77](#bib.bib77)] | Yes | No | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Trainable AF (TAF), 2018 [[78](#bib.bib78)] | Yes | No | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Self-Learnable AF (SLAF), 2019 [[79](#bib.bib79)] | Yes | No | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Mexican ReLU (MeLU), 2019 [[80](#bib.bib80)] | Yes | No | No | No |'
  prefs: []
  type: TYPE_TB
- en: 6 Learning/Adaptive Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the aforementioned AFs are not adaptive and might not be able to adjust
    based on the dataset complexity. This problem is tackled using learning/adaptive
    AFs as summarized in Table [5](#S5.T5 "Table 5 â€£ 5 Exponential Activation Functions
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark").
    Some of the earlier mentioned AFs are also adaptive, such as PReLU [[57](#bib.bib57)],
    SReLU [[65](#bib.bib65)], PTELU [[38](#bib.bib38)], MTLU [[46](#bib.bib46)], PELU
    [[54](#bib.bib54)], MPELU [[55](#bib.bib55)], PREU [[57](#bib.bib57)], EELU [[58](#bib.bib58)],
    PDELU [[59](#bib.bib59)], SRS [[26](#bib.bib26)], etc.'
  prefs: []
  type: TYPE_NORMAL
- en: The Adaptive Piecewise Linear (APL) is defined as a sum of hinge-shape functions
    [[28](#bib.bib28)]. It is given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $APL(x)=\text{max}(0,x)+\sum^{S}_{s=1}a_{s}\times\text{max}(0,b_{s}-x),$
    |  | (45) |'
  prefs: []
  type: TYPE_TB
- en: where $a$ and $b$ are the trainable parameters and $S$ is a hyperparameter representing
    the number of hinges. The output range of APL is $[0,\infty)$. Due to the trainable
    parameters, different neurons can learn different AFs.
  prefs: []
  type: TYPE_NORMAL
- en: Ramachandran et al. [[29](#bib.bib29)] have performed an automatic search, which
    resulted in a Swish AF. It is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Swish(x)=x\times Sigmoid(\beta\times x)$ |  | (46) |'
  prefs: []
  type: TYPE_TB
- en: where $\beta$ is a learnable parameter. The output range of Swish is $(-\infty,\infty)$.
    Based on the learnt value of $\beta$ the shape of the Swish AF is adjusted between
    the linear and ReLU functions. The smaller and higher values of $\beta$ lead towards
    the linear and ReLU functions, respectively. Thus, it can control the amount of
    non-linearity based on the dataset and network complexity. Swish is also extended
    to E-Swish by multiplying the Swish with a learnable parameter to control the
    slope in the positive direction [[77](#bib.bib77)]. The E-Swish is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ESwish(x)=\beta\times x\times Sigmoid(x)$ |  | (47) |'
  prefs: []
  type: TYPE_TB
- en: having the output the range in $(-\infty,\infty)$ and $\beta$ is trainable parameter.
    A flatten-T Swish considers zero function for negative inputs similar to the ReLU
    [[81](#bib.bib81)]. The Adaptive Richardâ€™s Curve weighted Activation (ARiA) is
    also motivated from Swish and replaces the sigmoidal function with Richardâ€™s Curve
    [[82](#bib.bib82)]. The ARiA AF uses five hyper-parameters to control the shape
    of the non-linearity.
  prefs: []
  type: TYPE_NORMAL
- en: The basic AFs are combined with learnable weights in adaptive AFs [[76](#bib.bib76)].
    The Adaptive AF (AAF) designed over PReLU [[35](#bib.bib35)] and PELU [[54](#bib.bib54)]
    is given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $AAF(x)=\sigma(w\times x)\times PRELU(x)+(1-\sigma(w\times x))\times PELU(x)$
    |  | (48) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $[0,1]$, where $\sigma$ is the sigmoidal function
    and $w$ is a learnable parameter. In practice, AAF is costly as multiple AFs are
    involved. In [[83](#bib.bib83)], the AF for each neuron is selected from a library
    of AFs. In [[84](#bib.bib84)], different combinations of the identity function,
    ReLU, and Tanh are learnt automatically. In another attempt, an Adaptive Blending
    Unit (ABU) is defined to allow the networks to learn its preferred AFs [[85](#bib.bib85)].
    The ABU combines a set of AFs with trainable weights. A Lookup Table Unit (LuTU)
    function [[86](#bib.bib86)] uses a single period cosine mask based smoothing and
    linear interpolation using a set of anchor points. Activation ensembles are used
    at each layer in [[87](#bib.bib87)] with the contribution of each AF controlled
    by the trainable weights. Similarly, the Self-Learnable AF (SLAF) computes the
    sum of the different functions in an ensemble with the learnt coefficients [[79](#bib.bib79)].
    The SLAF can be expressed as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $SLAF(x)=\sum_{i=0}^{N-1}a_{i}\times x^{i}$ |  | (49) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $(-\infty,\infty)$, where $a_{i}$ is the trainable parameter.
    A Mexican ReLU (MeLU) AF is proposed in [[80](#bib.bib80)] by using a â€œMexican
    hat typeâ€ function and given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $MeLU(x)=PReLU(x)+\sum_{j=1}^{k}{c_{j}\times\max(\lambda_{j}-&#124;x-a_{j}&#124;,0)}$
    |  | (50) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $(-\infty,\infty)$, where $c_{j}$ is the trainable parameter
    and $\lambda_{j}$ & $a_{j}$ are the real numbers.
  prefs: []
  type: TYPE_NORMAL
- en: A cubic spline interpolation is also used to learn the AF from data [[74](#bib.bib74)]
    which is given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $SAF(x)=\Phi(s;\textbf{q})$ |  | (51) |'
  prefs: []
  type: TYPE_TB
- en: having the output range in $(-\infty,\infty)$ where $\Phi(.)$ is parameterized
    by a vector q cubic in nature. Fourier series basis expansion is used for nonparametrically
    learning AFs (NPF) [[88](#bib.bib88)]. Hyperactivations utilize a hypernetwork
    on top of an activation network, which are used to explore the AFs search space
    [[89](#bib.bib89)]. A shallow neural network is used in the activation network
    to produce the output for each input, whereas a neural network is used in the
    hypernetwork to produce weights for another network. A bi-modal derivative adaptive
    activation (BDAA) function uses twin maxima derivative sigmoidal function [[75](#bib.bib75)]
    by controlling the maximaâ€™s position with an adaptive parameter. The BDAA is given
    as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $BDAA(x)=\frac{1}{2}\times\left(\frac{1}{1+e^{-x}}-\frac{1}{1+e^{-x-a}}\right)$
    |  | (52) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $[0,1]$ where $a$ is a learnable parameter. The authors
    have exploited the Bi-modal derivatives on four AFs. Linear regression is used
    in [[78](#bib.bib78)] to train AF for each neuron which results in different AFs
    for the different neurons. The TAF is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $TAF(x)=\sqrt{(x-a)^{2}+b^{2}}$ |  | (53) |'
  prefs: []
  type: TYPE_TB
- en: in the output range of $[b,\infty)$, where $a$ and $b$ are the trainable parameters.
    Recently, a trainable parameter was used in different non-adaptive AFs such as
    Sigmoid, Tanh, and ReLU to make it adaptive [[90](#bib.bib90)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The adaptive and trainable AFs are the recent trend to adjust the non-linearity
    based on the data and network complexity. However, the minimal burden is increased
    in terms of the increased number of parameters. Though the complexity of tunable
    AFs is relatively increased w.r.t. non-tunable AFs, it is negligible w.r.t. all
    parameters of the entire network in practice. The same is also observed experimentally
    as reported in Table [10](#S9.T10 "Table 10 â€£ 9.2 Experimental Performance Analysis
    â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") in terms of the training time.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Miscellaneous Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section covers other attempts in AFs such as Softplus, Probabilistic, Polynomial,
    Subnetwork and Kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Softplus Activation Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The softplus function [[91](#bib.bib91)] was proposed in 2001 as $\log(e^{x}+1)$
    and mostly used in statistical applications. After the breakthrough of deep learning
    the softmax function is used as the AF [[92](#bib.bib92)]. Softmax function produces
    the categorical probability distribution equivalent output. Softplus unit based
    AF is also used in deep neural networks [[93](#bib.bib93)]. The smooth nature
    of the Softplus facilitates the differentiability. The noisy softplus AF [[94](#bib.bib94)]
    is suitable for the spiking neural networks (SNNs). A Softplus Linear Unit (SLU)
    is also proposed by considering softplus with rectified unit [[95](#bib.bib95)].
    The SLU AF is defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $SLU(x)=\begin{cases}\alpha\times x,&amp;x\geq 0\\ \beta\times\log(e^{x}+1)-\gamma,&amp;x<0\end{cases}$
    |  | (54) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$, $\beta$ and $\gamma$ are the trainable parameters with $\alpha$
    controlling the slope in the positive direction, $\beta$ controlling the saturation
    points in the negative direction and $\gamma$ controlling the offset in the negative
    direction w.r.t. the horizontal axis. The Rectified Softplus (ReSP) AF introduces
    the rectification for positive input in Softplus activation [[96](#bib.bib96)].
    In order to make the softplus function to follow the zero mean, a shifting and
    scaling of the outputs is performed in [[97](#bib.bib97)]. A Rand Softplus (RSP)
    AF models the stochasticity-adaptability of biological neurons as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $RSP(x)=(1-\rho)\times\max(0,x)+\rho\times\log(1+e^{x})$ |  | (55) |'
  prefs: []
  type: TYPE_TB
- en: where $\rho$ is a stochastic hyperparameter [[98](#bib.bib98)]. It improves
    the capability of the network towards the noise. The softplus function is also
    used with Tanh function in Mish activation function [[99](#bib.bib99)], which
    is given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Mish(x)=x\times Tanh(Softplus(x)).$ |  | (56) |'
  prefs: []
  type: TYPE_TB
- en: The Mish is a non-monotonic and smooth AF. It has recently been used by the
    YOLOv4 model for object detection [[100](#bib.bib100)]. However, the increased
    complexity in Mish due to the multiple functions can be a limitation for the deep
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Probabilistic Activation Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, stochastic AFs have not been much explored due to expensive sampling
    processes. Few AFs exist in this category such as Randomized ReLU (RReLU) [[50](#bib.bib50)],
    Elastic ReLU (EReLU) [[40](#bib.bib40)], Randomly Translational ReLU (RTReLU)
    [[41](#bib.bib41)] and Gaussian Error Linear Unit (GELU) [[101](#bib.bib101)].
    GELU [[101](#bib.bib101)] considers nonlinearity as the stochastic regularization
    driven transformation and defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $GELU(x)=x\times P(X\leq x).$ |  | (57) |'
  prefs: []
  type: TYPE_TB
- en: where $P$ is the probability. The complexity of GELU increases due to use of
    probabilistic nature. The GELU is also extended to the Symmetrical Gaussian Error
    Linear Unit (SGELU) [[102](#bib.bib102)] to enhance its ability of bidirectional
    convergence. Doubly truncated Gaussian distributions [[103](#bib.bib103)] is a
    family of nonlinearities which can generate different AFs such as Sigmoid, Tanh
    and ReLU by setting the appropriate truncation points. Probabilistic AF (ProbAct)
    introduces the adaptable and trainable variance in the ReLUâ€™s output [[104](#bib.bib104)].
    It leads to the generalization of the models. However, all other drawbacks of
    ReLU exist with ProbAct also.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Polynomial Activation Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Smooth Adaptive AF (SAAF) is defined as the piecewise polynomial function [[105](#bib.bib105)].
    Two power functions symmetric to the linear part of ReLU are combined in [[106](#bib.bib106)]
    to improve the performance of ReLU. A piecewise polynomial approximation based
    AF is also learnt from the data [[107](#bib.bib107)]. This activation leads to
    the light-weight models suitable for the FPGAs and microcontrollers. The AF is
    also treated as the cumulative distribution function [[108](#bib.bib108)]. The
    ReLU is also extended to a Rectified Power Unit (RePU) for positive inputs as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $RePU(x)=\begin{cases}x^{s},&amp;x\geq 0\\ 0,&amp;x<0\end{cases}$ |  |
    (58) |'
  prefs: []
  type: TYPE_TB
- en: where $s$ is a hyperparameter [[109](#bib.bib109)]. The RePU is suitable for
    smoother gradients near zero. However, vanishing gradient, unbounded and asymmetric
    nature are the downsides of RePU. The rational function of polynomials is better
    suited as compared to the polynomial functions in order to approximate the ReLU
    [[110](#bib.bib110)]. Recently, a PadÃ© approximation is used to develop a non-smooth
    PadÃ© Activation Unit (PAU) [[111](#bib.bib111)] as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PAU(x)=\frac{P(x)}{Q(x)}$ |  | (59) |'
  prefs: []
  type: TYPE_TB
- en: where $P(x)$ and $Q(x)$ are two polynomials of order $m$ and $n$, respectively.
    The PAUs can approximate the commonly used hand-designed AFs. Moreover, it can
    also learn the new AFs with compact representations. Recently, a Rational AF (RAF)
    [[112](#bib.bib112)] was proposed to tackle the problem of non-smooth nature of
    the PAU function.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Activations as a Subnetwork
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Variable AF (VAF) is used as a subnetwork of ReLUs [[113](#bib.bib113)]. It
    uses the ensemble of ReLUs in a subnetwork using learnable parameters. In a very
    similar approach, the maximum of multiple linear functions is used in the Dynamic
    ReLU (DY-ReLU) [[114](#bib.bib114)]. In Wide Hidden Expansion (WHE) [[115](#bib.bib115)],
    each WHE intermediate channel is followed by one AF before connecting to the output
    channel to increase the non-linearity of the network. An AF Unit (AFU) [[116](#bib.bib116)]
    uses a small neural network to model the activation. All neurons in the original
    network share the weights in AFU. The advantage of the AFU is that different AFs
    can be learnt by different layers.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Kernel Activation Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Kernel-based non-parametric AF (KAF) [[117](#bib.bib117)] uses an inexpensive
    kernel expansion to make the activation flexible. The KAF is further extended
    to multikernel AFs (multi-KAF) [[118](#bib.bib118)]. Several AFs are also introduced
    for complex valued neural networks [[119](#bib.bib119)], [[120](#bib.bib120)],
    [[121](#bib.bib121)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Summary of the existing state-of-the-art activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Activation | Models | Datasets | Insights and Remarks |'
  prefs: []
  type: TYPE_TB
- en: '| On Image Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| Wide Hidden Expansion (WHE) - 2020 [[115](#bib.bib115)] | ResNet, SENet,
    and MobileNet | CIFAR100 and ImageNet classification, Pascal VOC 2007 and COCO
    detection | Upto 2% higher Top-1 accuracy than baseline models of recognition
    and detection |'
  prefs: []
  type: TYPE_TB
- en: '| Soft-Root-Sign (SRS) - 2020 [[26](#bib.bib26)] | VGG and MobileNet | CIFAR10
    and CIFAR100 classification | The SRS is better with MobileNet over both datasets
    and with VGG over CIFAR100\. The LReLU is better with VGG over CIFAR10. |'
  prefs: []
  type: TYPE_TB
- en: '| Relu-Memristor-Like AF (RMAF) - 2020 [[72](#bib.bib72)] | ResNet, AlexNet,
    SqueezeNet, and DenseNet | CIFAR10, CIFAR100, MNIST and ImageNet classification
    | The RMAF performs better than the ReLU, ELU, SELU, PReLU, Tanh and Swish. |'
  prefs: []
  type: TYPE_TB
- en: '| Parametric Deformable ELU (PDELU) - 2020 [[59](#bib.bib59)] | NIN and ResNet
    | CIFAR10 and CIFAR100 classification | The PDELU performs better than the ReLU,
    ELU and FReLU. |'
  prefs: []
  type: TYPE_TB
- en: '| Pade Activation Unit (PAU) - 2020 [[111](#bib.bib111)] | VGG8, MobileNetV2,
    ResNet and DenseNet | MNIST, Fashion-MNIST, CIFAR10 and ImageNet classification
    | The PAU encode AFs as rational functions and performs better than many existing
    AFs. |'
  prefs: []
  type: TYPE_TB
- en: '| Elastic Exponential Linear Unit (EELU) - 2020 [[58](#bib.bib58)] | A simple
    CNN model and VGG16 | CIFAR10, CIFAR100, ImageNet, and Tiny ImageNet classification
    | The EELU shows better results than the ReLU, ELU, EPReLU and Swish. |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamic ReLU (DY-ReLU) - 2020 [[114](#bib.bib114)] | MobileNetV2 | ImageNet
    classification and COCO detection | The DY-ReLU is suitable for light-weight networks.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Variable AF (VAF) - 2019 [[113](#bib.bib113)] | Shallow CNN models | MNIST,
    Fashion MNIST and CIFAR10 classification | The VAF shows promising performance.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-bin Trainable Linear Unit (MTLU) - 2019 [[46](#bib.bib46)] | FDnet
    and FSRnet | Image denoising and Super-resolution | The MTLU is significantly
    faster having comparable results with the state-of-the-arts. |'
  prefs: []
  type: TYPE_TB
- en: '| Swish - 2018 [[29](#bib.bib29)] | MobileNet, ResNet, WRN and DenseNet | CIFAR10,
    CIFAR100 and ImageNet classification | The learnable parameter in Swish leads
    to improved performance than Softplus. |'
  prefs: []
  type: TYPE_TB
- en: '| On Time Series Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| Variable AF (VAF) - 2019 [[113](#bib.bib113)] | Multi-Layered Neural Network
    | Regression tasks (Kinematics, Energy Cooling, Yatch, etc.) | Better performance
    over Kinematics, Energy Cooling and Yatch datasets. |'
  prefs: []
  type: TYPE_TB
- en: '| Self-Learnable AFs (SLAF) - 2019 [[79](#bib.bib79)] | Multi-Layered Neural
    Network | Boston Housing and Learning Sparse Polynomial regression | The newer
    parameter space makes the optimization easier. |'
  prefs: []
  type: TYPE_TB
- en: '| On Text Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| Soft-Root-Sign (SRS) - 2020 [[26](#bib.bib26)] | A 6 layer transformer network
    | IWSLT 2016 German-English translation | The SRS is better over tst2011 and tst2012
    test sets, whereas the SELU and LReLU are better over tst2013 and tst2014 test
    sets, respectively. |'
  prefs: []
  type: TYPE_TB
- en: '| Swish - 2018 [[29](#bib.bib29)] | A 12 layer transformer network | WMT 2014
    English-German dataset | The performance of Swish is comparable to state-of-the-arts.
    |'
  prefs: []
  type: TYPE_TB
- en: '| PenalizedTanh - 2018 [[33](#bib.bib33)] | MLP, CNN and RNN | Sentence classification,
    Document classification and Sentence tagging | The PenalizedTanh exhibits the
    stability across the different tasks in contrast to the Swish function. |'
  prefs: []
  type: TYPE_TB
- en: '| On Signal Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| Rectified Linear Tanh (ReLTanh) - 2019 [[67](#bib.bib67)] | Stacked autoencoder
    (SAE) based DNN | Vibration signals for rotating machinery fault diagnosis | The
    ReLTanh leads to larger gradients for faster learning and reduces the vanishing
    gradient. |'
  prefs: []
  type: TYPE_TB
- en: '| On Game Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| Sigmoid-weighted Linear Unit (SiLU) - 2018 [[23](#bib.bib23)] | Deep reinforcement
    learning algorithm | SZ-Tetris, $10\times 10$ Tetris, and Atari 2600 games | The
    SiLU AF outperforms the ReLU function for reinforcement learning. |'
  prefs: []
  type: TYPE_TB
- en: 8 Aspects of Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section summarizes the effect of weight initialization, understanding of
    AFs and suitability with different types of data. The learning of the network
    speeds up drastically by using the orthogonal weight initialization based on the
    dynamical isometry [[122](#bib.bib122)]. A set of conditions in parameter initialization
    also boosts the performance of networks with sigmoidal activations [[123](#bib.bib123)].
    The symmetric probability distribution based weights and biases initialization
    leads the network to suffer with the dying ReLU problem. However, the asymmetric
    initialization resolves the dying ReLU problem [[124](#bib.bib124)]. The over-parameterization
    during initialization also benefits in the training [[125](#bib.bib125)]. The
    data-dependent weight initialization using a subset of data minimizes the issues
    of the ReLU [[126](#bib.bib126)], whereas an initial parameter sharing based initialization
    guarantees the dynamical isometry for the ReLU [[127](#bib.bib127)].
  prefs: []
  type: TYPE_NORMAL
- en: Several researchers have tried to understand the working and impact of AFs through
    different strategies. The lower and upper bounds are established for network complexity
    to realize that the ReLU in deep networks approximates the smooth functions more
    efficiently as compared to shallow networks [[128](#bib.bib128)]. A ReLU network
    with only one hidden layer is trained to reach the global optimum in polynomial
    time even with exponentially growing input dimension [[129](#bib.bib129)]. The
    ReLU type AF based neural networks produce the overconfident predictions far away
    from the training data [[130](#bib.bib130)]. However, this can be resolved by
    employing adversarial confidence enhanced training. A Gaussian margin driven time
    and accuracy tradeoff analysis is also done on the ReLUâ€™s learning [[131](#bib.bib131)].
    The singular values for ReLU layers are analyzed to understand the interaction
    of ReLU with the linear components [[132](#bib.bib132)]. The approximation of
    Gaussian posterior distribution over the ReLU network weightâ€™s fixes the overconfidence
    problem [[133](#bib.bib133)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite most of the AFs are tested over image data, there are few research
    papers dealing with the AFs over other types of data. Table [6](#S7.T6 "Table
    6 â€£ 7.5 Kernel Activation Functions â€£ 7 Miscellaneous Activation Functions â€£ Activation
    Functions in Deep Learning: A Comprehensive Survey and Benchmark") summarizes
    the insights and remarks of state-of-the-art AFs for various networks and datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 9 Performance Comparison and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This survey is compared with the existing survey/performance analysis and the
    experimental performance analysis of selected AFs is performed over Image, Text
    and Speech data.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Comparison with Existing Survey/Performance Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A performance analysis of AFs was conducted using multilayer perceptron network
    in [[134](#bib.bib134)]. Among compared AFs, the Tanh has shown better performance.
    A comparative performance analysis of different AFs suggests an Elliott function
    as better suited for classification using LSTM networks [[25](#bib.bib25)]. The
    ELU outperforms the ReLU, LReLU, and SELU AFs over MNIST classification task using
    Deep Neural Networks [[135](#bib.bib135)]. As per [[136](#bib.bib136)], the ELU
    is reported in [[135](#bib.bib135)] to outperform the ReLU, LReLU, PReLU and PELU
    over sufficiently large datasets for speech recognition. However, for smaller
    datasets, the ReLU is preferred. A similar trend is also reported in [[137](#bib.bib137)]
    with a note that the ELU and SELU AFs exhibit faster convergence as compared to
    the ReLU and LReLU AFs. In [[138](#bib.bib138)], 21 AFs are listed without experimental
    results comparison. In contrast to [[138](#bib.bib138)], this paper presents a
    comprehensive survey of AFs. The ReLU based deep networks perform superior or
    mildly worse than the spline methods [[139](#bib.bib139)]. A review of adaptive
    functions is conducted in [[140](#bib.bib140)] by considering 9 functions, including
    Sigmoid, Tanh, PReLU, and adaptTanh. In [[141](#bib.bib141)], the comparison between
    ReLU and LReLU is performed using CNN on MNIST dataset. An empirical study is
    also done for the variations of ReLU activation by generalizing it with the help
    of parameters [[142](#bib.bib142)]. The comparison of AFs is also performed for
    generalized learning vector quantization [[143](#bib.bib143)]. The ReLU activation
    has performed better for object, face, and text datasets [[144](#bib.bib144)].
    However, the SELU and Maxout have performed better for medical and sound datasets,
    respectively [[144](#bib.bib144)]. The piecewise AF is better suited for facial
    expression recognition in [[145](#bib.bib145)]. A survey of adaptive AFs is conducted
    in [[146](#bib.bib146)] without experimental comparison. The evaluation of seven
    AFs is conducted in [[147](#bib.bib147)] using a simple network over CIFAR10 dataset,
    whereas in our survey we cover different AFs and also perform the experimental
    comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Comparison of this survey with the existing surveys and performance
    evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Models | Activations | Datasets | Remarks |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Karlik and Olgac [[134](#bib.bib134)] | Multilayer Perceptron (MLP) | 5 AFs,
    including Bi-polar sigmoid, Uni-polar sigmoid, Tanh, etc. | Classification | The
    Tanh performs better compared to other traditional AFs. |'
  prefs: []
  type: TYPE_TB
- en: '| Vydana and Vuppala (2017) [[136](#bib.bib136)] | Hidden Markov Model-Deep
    Neural Network (HMM-DNN) | 5 AFs, including ReLU, LReLU, PReLU, ELU, and PELU
    | TIMIT and WSJ speech recognition | The ELU is better over sufficiently larger
    size datasets. However, the ReLU is preferred for smaller datasets. |'
  prefs: []
  type: TYPE_TB
- en: '| Alcantara (2017) [[135](#bib.bib135)] | A neural network with 2 hidden layers
    having 100 neurons/layer | 4 AFs, including ReLU, LReLU, ELU, and SELU | MNIST
    classification | The ELU AF outperforms others. |'
  prefs: []
  type: TYPE_TB
- en: '| Pedamonti (2018) [[137](#bib.bib137)] | A neural network with 2 hidden layers
    having 100 neurons/layer | 5 AFs, including Sigmoid, ReLU, LReLU, ELU, and SELU
    | MNIST classification | The ELU and SELU AFs exhibit the faster convergence as
    compared to the ReLU and LReLU AFs. |'
  prefs: []
  type: TYPE_TB
- en: '| Lau and Lim (2018) [[140](#bib.bib140)] | Deep Neural Network (DNN) | ReLU
    and Adaptive ReLU | MNIST classification | The adaptive AFs improve the generalization
    of the network. |'
  prefs: []
  type: TYPE_TB
- en: '| Farzad et al. (2019) [[25](#bib.bib25)] | Long Short Term Memory (LSTM) |
    23 AFs, including Elliott, Gaussian, Logarithmic, Loglog, etc. | IMDB, Movie Review,
    MNIST classification | Elliott function is better suited to the LSTM network.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dubey and Jain (2019) [[141](#bib.bib141)] | Simple Convolutional Neural
    Network (CNN) | 2 AFs, including ReLU and Leaky ReLU | MNIST classification |
    The ReLU performed better than Leaky ReLU (LReLU). |'
  prefs: []
  type: TYPE_TB
- en: '| Banerjee et al. (2019) [[142](#bib.bib142)] | Convolutional Neural Network
    (CNN) | Generalized ReLU | MNIST classification | Network learns the parameters
    for different ReLU variations. |'
  prefs: []
  type: TYPE_TB
- en: '| Villmann et al. (2019) [[143](#bib.bib143)] | Generalized learning vector
    quantization (GLVQ) | 12 AFs, including Sigmoid, Swish, ReLU, Softplus, etc. |
    Tecator, Indian Pine and Wisconsin-Breast-Cancer classification | The Sigmoid,
    Swish and Softplus AFs are better suited with GLVQ. |'
  prefs: []
  type: TYPE_TB
- en: '| Castaneda et al. (2019) [[144](#bib.bib144)] | 6 different models for different
    applications | 3 AFs, including ReLU, SELU and Maxout | Object, Face, Text, Medical
    and Sound datasets | The ReLU is better for object, face and text datasets, whereas
    SELU and Maxout are better for medical and sound datasets, respectively. |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. (2020) [[145](#bib.bib145)] | Inception-v3 model | 6 AFs, including
    Sigmoid, Tanh, ReLu, etc. | JAFFE and FER2013 facial expression recognition |
    The combination of log, softdesign and ReLU AFs provides improved performance.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Szandala (2020) [[147](#bib.bib147)] | A simple network | 7 AFs, including
    Sigmoid, Tanh, ReLU, LReLU, Swish, etc. | CIFAR10 classification | The LReLU performs
    better. The ReLU is efficient. |'
  prefs: []
  type: TYPE_TB
- en: '| Our survey and performance analysis | MobileNet, VGG, GoogLeNet, ResNet,
    SENet, DenseNet, etc. | Exhaustive list of AFs, including performance analysis
    over $18$ state-of-the-art activations | CIFAR10 classification, Language translation,
    Speech recognition | A classification to categorize and analyze the AFs and a
    performance comparison of the state-of-the-art activations. |'
  prefs: []
  type: TYPE_TB
- en: 'A summary of the comparison with existing surveys and performance analysis
    of AF is shown in Table [7](#S9.T7 "Table 7 â€£ 9.1 Comparison with Existing Survey/Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"). Following are the observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This survey presents a detailed classification to cover the wide range of AFs
    as compared to the existing surveys and performance analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This survey covers exhaustive state-of-the-art AFs to date, whereas the existing
    survey/performance analysis covers either a limited number of AFs or only basic
    AFs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance analysis conducted in this paper considers a wide range of neural
    networks over different types of data for eighteen AFs, whereas the existing analysis
    is limited to a single type of data and network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This survey highlights the trends to help the researchers to further explore
    the better AFs and practitioners to choose based on the data and network types.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table 8: Experimental results comparison over CIFAR10 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Accuracy | CNN Models |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Activations | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sigmoid | 88.60 $\pm$ 0.17 | 87.69 $\pm$ 0.49 | 87.33 $\pm$ 2.48 | 80.13
    $\pm$ 3.33 | 90.29 $\pm$ 0.29 | 89.92 $\pm$ 1.96 |'
  prefs: []
  type: TYPE_TB
- en: '| Tanh | 87.21 $\pm$ 0.24 | 90.49 $\pm$ 0.11 | 90.16 $\pm$ 1.86 | 89.09 $\pm$
    1.47 | 90.44 $\pm$ 0.09 | 91.80 $\pm$ 0.69 |'
  prefs: []
  type: TYPE_TB
- en: '| Elliott [[25](#bib.bib25)] | 88.48 $\pm$ 0.18 | 87.94 $\pm$ 0.49 | 89.84
    $\pm$ 3.43 | 81.60 $\pm$ 3.91 | 90.25 $\pm$ 0.25 | 91.53 $\pm$ 1.04 |'
  prefs: []
  type: TYPE_TB
- en: '| ReLU [[8](#bib.bib8)] | 90.10 $\pm$ 0.22 | 92.84  $\pm$ 0.19 | 93.43  $\pm$
    0.48 | 93.74 $\pm$ 0.34 | 93.70 $\pm$ 0.16 | 93.96  $\pm$ 0.51 |'
  prefs: []
  type: TYPE_TB
- en: '| LReLU [[17](#bib.bib17)] | 90.10 $\pm$ 0.19 | 91.09 $\pm$ 0.09 | 89.28 $\pm$
    0.82 | 93.83  $\pm$ 0.42 | 93.66 $\pm$ 0.19 | 93.85 $\pm$ 0.48 |'
  prefs: []
  type: TYPE_TB
- en: '| PReLU [[35](#bib.bib35)] | 90.43 $\pm$ 0.18 | 92.19 $\pm$ 0.08 | 92.85 $\pm$
    0.55 | 92.99 $\pm$ 0.62 | 92.76 $\pm$ 0.26 | 92.82 $\pm$ 0.63 |'
  prefs: []
  type: TYPE_TB
- en: '| ELU [[27](#bib.bib27)] | 90.92 $\pm$ 0.25 | 88.55 $\pm$ 1.17 | 92.47 $\pm$
    0.76 | 93.53 $\pm$ 0.66 | 93.39 $\pm$ 0.20 | 92.89 $\pm$ 0.62 |'
  prefs: []
  type: TYPE_TB
- en: '| SELU [[52](#bib.bib52)] | 90.11 $\pm$ 0.32 | 92.25 $\pm$ 0.28 | 91.87 $\pm$
    0.84 | 93.53 $\pm$ 0.52 | 89.96 $\pm$ 0.31 | 92.71 $\pm$ 0.73 |'
  prefs: []
  type: TYPE_TB
- en: '| GELU [[101](#bib.bib101)] | 90.71 $\pm$ 0.20 | 92.42 $\pm$ 0.09 | 93.16 $\pm$
    0.61 | 93.81 $\pm$ 0.46 | 93.72  $\pm$ 0.18 | 93.90  $\pm$ 0.41 |'
  prefs: []
  type: TYPE_TB
- en: '| CELU [[53](#bib.bib53)] | 91.04  $\pm$ 0.17 | 88.11 $\pm$ 0.14 | 92.60 $\pm$
    0.60 | 94.09  $\pm$ 0.17 | 91.63 $\pm$ 0.22 | 93.46 $\pm$ 0.35 |'
  prefs: []
  type: TYPE_TB
- en: '| Softplus [[93](#bib.bib93)] | 91.05  $\pm$ 0.22 | 92.69 $\pm$ 0.20 | 92.66
    $\pm$ 0.66 | 93.34 $\pm$ 0.65 | 93.25 $\pm$ 0.11 | 93.07 $\pm$ 0.70 |'
  prefs: []
  type: TYPE_TB
- en: '| Swish [[29](#bib.bib29)] | 90.66 $\pm$ 0.34 | 92.32 $\pm$ 0.20 | 92.68 $\pm$
    0.53 | 93.02 $\pm$ 0.85 | 93.24 $\pm$ 0.19 | 93.16 $\pm$ 0.51 |'
  prefs: []
  type: TYPE_TB
- en: '| ABReLU [[44](#bib.bib44)] | 88.97 $\pm$ 0.47 | 92.36 $\pm$ 0.15 | 93.34 $\pm$
    0.23 | 93.29 $\pm$ 0.52 | 93.35 $\pm$ 0.14 | 93.26 $\pm$ 0.55 |'
  prefs: []
  type: TYPE_TB
- en: '| LiSHT [[24](#bib.bib24)] | 86.53 $\pm$ 0.49 | 89.83 $\pm$ 0.28 | 90.27 $\pm$
    0.80 | 90.89 $\pm$ 0.66 | 90.25 $\pm$ 0.84 | 87.91 $\pm$ 0.93 |'
  prefs: []
  type: TYPE_TB
- en: '| SRS [[26](#bib.bib26)] | 89.43 $\pm$ 0.81 | 92.06 $\pm$ 0.26 | 91.36 $\pm$
    1.19 | 92.28 $\pm$ 0.48 | 78.05 $\pm$ 1.37 | 90.64 $\pm$ 1.93 |'
  prefs: []
  type: TYPE_TB
- en: '| Mish [[99](#bib.bib99)] | 90.82 $\pm$ 0.15 | 92.85  $\pm$ 0.25 | 93.29 $\pm$
    0.61 | 93.69 $\pm$ 0.63 | 93.66 $\pm$ 0.12 | 93.62 $\pm$ 0.62 |'
  prefs: []
  type: TYPE_TB
- en: '| PAU [[111](#bib.bib111)] | 90.67 $\pm$ 0.17 | 92.00 $\pm$ 0.26 | 92.80 $\pm$
    0.65 | 93.67 $\pm$ 0.52 | 93.08 $\pm$ 0.20 | 93.05 $\pm$ 0.53 |'
  prefs: []
  type: TYPE_TB
- en: '| PDELU [[59](#bib.bib59)] | 90.18 $\pm$ 0.19 | 92.80 $\pm$ 0.13 | 93.49  $\pm$
    0.30 | 93.42 $\pm$ 0.71 | 93.71  $\pm$ 0.07 | 93.96  $\pm$ 0.59 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Experimental results comparison over CIFAR100 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Accuracy | CNN Models |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Activations | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sigmoid | 61.88 $\pm$ 0.18 | 37.75 $\pm$ 0.59 | 70.31 $\pm$ 0.54 | 46.78
    $\pm$ 5.42 | 66.17 $\pm$ 1.16 | 68.31 $\pm$ 2.41 |'
  prefs: []
  type: TYPE_TB
- en: '| Tanh | 53.10 $\pm$ 0.51 | 58.43 $\pm$ 0.38 | 67.66 $\pm$ 2.32 | 64.32 $\pm$
    1.69 | 60.13 $\pm$ 1.86 | 69.53 $\pm$ 1.68 |'
  prefs: []
  type: TYPE_TB
- en: '| Elliott [[25](#bib.bib25)] | 60.70 $\pm$ 0.34 | 33.20 $\pm$ 0.97 | 64.85
    $\pm$ 6.28 | 49.88 $\pm$ 4.03 | 66.30 $\pm$ 0.28 | 69.58 $\pm$ 2.40 |'
  prefs: []
  type: TYPE_TB
- en: '| ReLU [[8](#bib.bib8)] | 61.33 $\pm$ 0.34 | 67.47 $\pm$ 0.44 | 74.05 $\pm$
    1.69 | 71.96 $\pm$ 0.94 | 70.45 $\pm$ 0.73 | 72.99 $\pm$ 1.35 |'
  prefs: []
  type: TYPE_TB
- en: '| LReLU [[17](#bib.bib17)] | 61.13 $\pm$ 0.41 | 65.72 $\pm$ 0.14 | 63.79 $\pm$
    2.38 | 72.77  $\pm$ 0.49 | 70.58 $\pm$ 0.45 | 73.33 $\pm$ 1.25 |'
  prefs: []
  type: TYPE_TB
- en: '| PReLU [[35](#bib.bib35)] | 59.86 $\pm$ 0.35 | 65.26 $\pm$ 0.40 | 69.57 $\pm$
    1.50 | 71.08 $\pm$ 1.70 | 69.77 $\pm$ 0.48 | 68.23 $\pm$ 1.55 |'
  prefs: []
  type: TYPE_TB
- en: '| ELU [[27](#bib.bib27)] | 61.97  $\pm$ 0.24 | 51.35 $\pm$ 3.01 | 72.57 $\pm$
    1.76 | 71.41 $\pm$ 1.63 | 71.27  $\pm$ 0.58 | 72.06 $\pm$ 1.93 |'
  prefs: []
  type: TYPE_TB
- en: '| SELU [[52](#bib.bib52)] | 59.62 $\pm$ 0.39 | 64.55 $\pm$ 0.43 | 71.47 $\pm$
    1.39 | 69.94 $\pm$ 1.92 | 55.01 $\pm$ 0.98 | 70.15 $\pm$ 1.04 |'
  prefs: []
  type: TYPE_TB
- en: '| GELU [[101](#bib.bib101)] | 61.20 $\pm$ 0.61 | 67.25 $\pm$ 0.38 | 74.27  $\pm$
    0.70 | 71.58 $\pm$ 0.87 | 71.14 $\pm$ 0.29 | 73.31 $\pm$ 1.70 |'
  prefs: []
  type: TYPE_TB
- en: '| CELU [[53](#bib.bib53)] | 61.90 $\pm$ 0.21 | 55.78 $\pm$ 0.69 | 72.87 $\pm$
    1.52 | 70.95 $\pm$ 1.40 | 63.43 $\pm$ 0.81 | 72.68 $\pm$ 1.16 |'
  prefs: []
  type: TYPE_TB
- en: '| Softplus [[93](#bib.bib93)] | 62.59  $\pm$ 0.21 | 67.70 $\pm$ 0.19 | 73.08
    $\pm$ 1.66 | 71.99 $\pm$ 2.03 | 71.16  $\pm$ 0.46 | 72.54 $\pm$ 1.73 |'
  prefs: []
  type: TYPE_TB
- en: '| Swish [[29](#bib.bib29)] | 59.40 $\pm$ 0.41 | 66.05 $\pm$ 0.82 | 71.56 $\pm$
    1.66 | 71.12 $\pm$ 2.08 | 68.42 $\pm$ 1.62 | 71.34 $\pm$ 1.10 |'
  prefs: []
  type: TYPE_TB
- en: '| ABReLU [[44](#bib.bib44)] | 56.21 $\pm$ 0.53 | 66.95 $\pm$ 0.09 | 71.83 $\pm$
    2.26 | 71.96 $\pm$ 1.43 | 70.47 $\pm$ 0.91 | 73.79  $\pm$ 1.45 |'
  prefs: []
  type: TYPE_TB
- en: '| LiSHT [[24](#bib.bib24)] | 54.09 $\pm$ 1.54 | 58.87 $\pm$ 0.81 | 66.66 $\pm$
    2.50 | 65.28 $\pm$ 1.33 | 66.01 $\pm$ 1.04 | 65.61 $\pm$ 1.10 |'
  prefs: []
  type: TYPE_TB
- en: '| SRS [[26](#bib.bib26)] | 54.93 $\pm$ 0.80 | 58.22 $\pm$ 1.09 | 70.39 $\pm$
    1.09 | 67.11 $\pm$ 1.46 | 36.95 $\pm$ 0.93 | 64.52 $\pm$ 1.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Mish [[99](#bib.bib99)] | 61.81 $\pm$ 0.54 | 68.13  $\pm$ 0.40 | 73.76 $\pm$
    1.48 | 71.89 $\pm$ 1.12 | 70.80 $\pm$ 0.68 | 73.49 $\pm$ 1.39 |'
  prefs: []
  type: TYPE_TB
- en: '| PAU [[111](#bib.bib111)] | 59.81 $\pm$ 0.61 | 64.14 $\pm$ 0.62 | 70.48 $\pm$
    1.53 | 68.59 $\pm$ 2.15 | 68.29 $\pm$ 0.77 | 67.83 $\pm$ 0.35 |'
  prefs: []
  type: TYPE_TB
- en: '| PDELU [[59](#bib.bib59)] | 61.35 $\pm$ 0.56 | 67.92  $\pm$ 0.32 | 74.48  $\pm$
    1.23 | 72.11  $\pm$ 1.60 | 70.81 $\pm$ 0.47 | 73.71  $\pm$ 1.64 |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/f5331dcc88b4026195f29da0a2c88d56.png)![Refer to caption](img/2b7dba249145771874a090e8cb1c7c45.png)![Refer
    to caption](img/a37314d201c61434ffee27ead63a536b.png)![Refer to caption](img/f54b2d2f5973c53657d09b59643f1c60.png)![Refer
    to caption](img/dbffd53297d0d9f34e8ee2f76407f523.png)![Refer to caption](img/c830f4eb908caee10e73dad286ef189d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Convergence plots over CIFAR100 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Experimental Performance Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to compare the AFs, three experiments are conducted in this paper,
    including image classification, language translation and speech recognition. Eighteen
    state-of-the-art AFs are considered for analysis, including Logistic Sigmoid,
    Tanh, Elliott [[25](#bib.bib25)], ReLU [[8](#bib.bib8)], LReLU [[34](#bib.bib34)]
    PReLU [[35](#bib.bib35)], ELU [[27](#bib.bib27)], SELU [[52](#bib.bib52)], GELU
    [[101](#bib.bib101)], CELU [[53](#bib.bib53)], Softplus [[93](#bib.bib93)], Swish
    [[29](#bib.bib29)], ABReLU [[44](#bib.bib44)], LiSHT [[24](#bib.bib24)], Soft-Root-Sign
    (SRS) [[26](#bib.bib26)], Mish [[99](#bib.bib99)], PAU [[111](#bib.bib111)] and
    PDELU [[59](#bib.bib59)]. Note that Swish, ABReLU, LiSHT, SRS, Mish, PAU and PDELU
    are the most recent functions. Google Colab based computational resource is used
    in most of the experiments. Few experiments are also performed over a desktop
    system consisting of 8 GB GPU. The PyTorch framework is used in all the experiments.
  prefs: []
  type: TYPE_NORMAL
- en: The CIFAR10 and CIFAR100 datasetsÂ¹Â¹1[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
    [[148](#bib.bib148)] are used for the image classification experiment in this
    paper. The CIFAR10 dataset contains $50,000$ training images and $10,000$ test
    images from $10$ object categories. The CIFAR100 dataset contains $50,000$ training
    images and $10,000$ test images from $100$ object categories. We also utilize
    the language translation and speech recognition datasets for the experiments.
    For the experiments over CIFAR-10 and CIFAR-100 datasets, training is performed
    for 100 Epochs. The batch size is 128 for CIFAR-10 and 64 for CIFAR-100\. The
    learning rate is 0.001 for first 80 Epochs and 0.0001 for last 20 Epochs. Random
    crop and random horizontal flip are the data augmentation used during training.
    Data normalization is performed both during train and test times. Adam optimizer
    is used for the training with cross entropy loss. All existing activation functions
    except softmax are replaced with the corresponding activation function in different
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test accuracy is reported in Tables [8](#S9.T8 "Table 8 â€£ 9.1 Comparison
    with Existing Survey/Performance Analysis â€£ 9 Performance Comparison and Analysis
    â€£ Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark")
    and [9](#S9.T9 "Table 9 â€£ 9.1 Comparison with Existing Survey/Performance Analysis
    â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") on CIFAR10 and CIFAR100 datasets, respectively.
    In these Tables, the mean and standard deviation of image classification accuracy
    over 5 trials are reported for each AF. Moreover, the better results are highlighted.
    Different types of CNN models are used in this experiment, such as plain models
    (i.e., MobileNet [[149](#bib.bib149)] and VGG16 [[150](#bib.bib150)]), inception
    model (i.e., GoogLeNet [[151](#bib.bib151)]) and skip/residual connection based
    models (i.e., ResNet50 [[152](#bib.bib152)], SENet18 [[153](#bib.bib153)], and
    DenseNet121 [[154](#bib.bib154)]). The MobileNet, GoogLeNet and SENet18 are light
    models, whereas the VGG16, ResNet50 and DenseNet121 are heavy models in terms
    of the number of trainable parameters. Overall, it is observed that the Softplus,
    ELU and CELU are better suited with MobileNet. The ReLU, Mish and PDELU exhibit
    good performance with VGG16, GoogleNet and DenseNet. The ReLU, LReLU, ELU, GELU,
    CELU, ABReLU, and PDELU activation functions are better for the networks having
    residual connections, such as ResNet50, SENet18 and DenseNet121. In order to demonstrate
    the convergence of different AFs, the training loss vs epochs is plotted in Fig.
    [3](#S9.F3 "Figure 3 â€£ 9.1 Comparison with Existing Survey/Performance Analysis
    â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep Learning:
    A Comprehensive Survey and Benchmark") on CIFAR100 dataset using different models.
    The PAU has emerged as a promising AF with fastest convergence in most of the
    cases. The PReLU, GELU and PDELU AFs are also consistent with good convergence.
    Note that the training diverges with SRS for the SENet18 model. Sigmoid and Elliott
    AFs showed the poorest convergence. The time taken for the training is also computed
    for different AFs using different CNN models on CIFAR100 dataset and reported
    in Table [10](#S9.T10 "Table 10 â€£ 9.2 Experimental Performance Analysis â€£ 9 Performance
    Comparison and Analysis â€£ Activation Functions in Deep Learning: A Comprehensive
    Survey and Benchmark"). These results are computed using a desktop computer system
    having 32 GB RAM and 8 GB Nvidia GPU Card for 100 epochs of training. The time
    is represented in hh:mm:ss format. It is clear that PDELU AF is very inefficient.
    Moreover, SRS and Elliott also take more time for training. The activations such
    as ReLU, ELU, CELU, and Softplus depict a good tradeoff between the accuracy and
    training time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: Training time (hh:mm:ss) comparison over CIFAR100 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training Time | CNN Models |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Activations | MobileNet | VGG16 | GoogleNet | ResNet50 | SENet18 | DenseNet121
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sigmoid | 00:33:15 | 00:49:16 | 04:55:54 | 03:36:03 | 01:13:14 | 04:12:24
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tanh | 00:33:18 | 00:49:55 | 04:58:02 | 03:33:03 | 01:13:18 | 04:09:24 |'
  prefs: []
  type: TYPE_TB
- en: '| Elliott [[25](#bib.bib25)] | 00:49:52 | 00:59:13 | 06:53:55 | 05:38:49 |
    01:41:38 | 07:46:55 |'
  prefs: []
  type: TYPE_TB
- en: '| ReLU [[8](#bib.bib8)] | 00:31:22 | 00:47:19 | 04:55:10 | 03:32:30 | 01:15:33
    | 04:15:06 |'
  prefs: []
  type: TYPE_TB
- en: '| LReLU [[34](#bib.bib34)] | 00:31:48 | 00:49:03 | 05:01:30 | 03:33:00 | 01:18:38
    | 04:14:09 |'
  prefs: []
  type: TYPE_TB
- en: '| PReLU [[35](#bib.bib35)] | 00:44:24 | 00:49:01 | 05:42:18 | 03:55:57 | 01:27:05
    | 04:55:47 |'
  prefs: []
  type: TYPE_TB
- en: '| ELU [[27](#bib.bib27)] | 00:31:05 | 00:47:38 | 04:57:37 | 03:36:47 | 01:13:25
    | 04:08:39 |'
  prefs: []
  type: TYPE_TB
- en: '| SELU [[52](#bib.bib52)] | 00:29:40 | 00:47:31 | 04:54:57 | 03:33:47 | 01:13:27
    | 04:09:17 |'
  prefs: []
  type: TYPE_TB
- en: '| GELU [[101](#bib.bib101)] | 00:29:43 | 00:47:22 | 04:55:53 | 03:32:32 | 01:13:32
    | 04:11:26 |'
  prefs: []
  type: TYPE_TB
- en: '| CELU [[53](#bib.bib53)] | 00:29:36 | 00:46:47 | 05:00:44 | 03:31:40 | 01:14:08
    | 04:18:11 |'
  prefs: []
  type: TYPE_TB
- en: '| Softplus [[93](#bib.bib93)] | 00:29:44 | 00:47:06 | 04:58:55 | 03:32:03 |
    01:14:02 | 04:12:08 |'
  prefs: []
  type: TYPE_TB
- en: '| Swish [[29](#bib.bib29)] | 00:43:13 | 00:55:37 | 06:18:38 | 04:58:38 | 01:32:15
    | 06:41:14 |'
  prefs: []
  type: TYPE_TB
- en: '| ABReLU [[44](#bib.bib44)] | 00:38:51 | 00:53:49 | 05:43:59 | 04:27:02 | 01:25:30
    | 05:42:53 |'
  prefs: []
  type: TYPE_TB
- en: '| LiSHT [[24](#bib.bib24)] | 00:37:01 | 00:54:10 | 05:40:00 | 04:25:57 | 01:23:59
    | 05:38:15 |'
  prefs: []
  type: TYPE_TB
- en: '| SRS [[26](#bib.bib26)] | 01:06:38 | 01:11:36 | 08:43:09 | 07:35:35 | 02:05:33
    | 11:10:27 |'
  prefs: []
  type: TYPE_TB
- en: '| Mish [[99](#bib.bib99)] | 00:40:19 | 00:54:23 | 05:59:48 | 04:46:45 | 01:28:53
    | 06:10:27 |'
  prefs: []
  type: TYPE_TB
- en: '| PAU [[111](#bib.bib111)] | 00:41:59 | 00:54:10 | 05:54:22 | 04:12:31 | 01:25:37
    | 05:39:57 |'
  prefs: []
  type: TYPE_TB
- en: '| PDELU [[59](#bib.bib59)] | 05:23:38 | 04:01:55 | 34:22:00 | 36:48:48 | 08:32:40
    | 50:23:00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Experimental results for German to English language translation and
    speech recognition tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Language Translation |  | Speech Recognition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Activations | Bleu Score |  | Average CER | Average WER |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sigmoid | 14.59 $\pm$ 0.47 |  | 0.53 $\pm$ 0.18 | 1.19 $\pm$ 0.39 |'
  prefs: []
  type: TYPE_TB
- en: '| Tanh | 20.93  $\pm$ 0.91 |  | 0.26 $\pm$ 0 | 0.68 $\pm$ 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Elliott [[25](#bib.bib25)] | 14.49 $\pm$ 0.96 |  | 0.40 $\pm$ 0.01 | 0.93
    $\pm$ 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| ReLU [[8](#bib.bib8)] | 18.88 $\pm$ 0.86 |  | 0.24  $\pm$ 0.01 | 0.66  $\pm$
    0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| LReLU [[34](#bib.bib34)] | 18.89 $\pm$ 0.82 |  | 0.24  $\pm$ 0 | 0.66  $\pm$
    0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| PReLU [[35](#bib.bib35)] | 20.04 $\pm$ 0.98 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
  prefs: []
  type: TYPE_TB
- en: '| ELU [[27](#bib.bib27)] | 19.40 $\pm$ 1.33 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0 |'
  prefs: []
  type: TYPE_TB
- en: '| SELU [[52](#bib.bib52)] | 20.85  $\pm$ 0.64 |  | 0.26 $\pm$ 0 | 0.69 $\pm$
    0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| GELU [[101](#bib.bib101)] | 18.75 $\pm$ 1.83 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
  prefs: []
  type: TYPE_TB
- en: '| CELU [[53](#bib.bib53)] | 18.71 $\pm$ 0.55 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0 |'
  prefs: []
  type: TYPE_TB
- en: '| Softplus [[93](#bib.bib93)] | 16.78 $\pm$ 0.84 |  | 0.30 $\pm$ 0.01 | 0.76
    $\pm$ 0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| Swish [[29](#bib.bib29)] | 19.51 $\pm$ 0.97 |  | 0.24  $\pm$ 0.01 | 0.65  $\pm$
    0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| ABReLU [[44](#bib.bib44)] | 17.55 $\pm$ 0.63 |  | 0.25  $\pm$ 0 | 0.68 $\pm$
    0 |'
  prefs: []
  type: TYPE_TB
- en: '| LiSHT [[24](#bib.bib24)] | 20.39 $\pm$ 0.93 |  | 0.29 $\pm$ 0.01 | 0.74 $\pm$
    0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| SRS [[26](#bib.bib26)] | 20.66 $\pm$ 0.78 |  | 0.28 $\pm$ 0 | 0.72 $\pm$
    0 |'
  prefs: []
  type: TYPE_TB
- en: '| Mish [[99](#bib.bib99)] | 19.56 $\pm$ 1.15 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0 |'
  prefs: []
  type: TYPE_TB
- en: '| PAU [[111](#bib.bib111)] | 20.11 $\pm$ 1.24 |  | 0.24  $\pm$ 0 | 0.65  $\pm$
    0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| PDELU [[59](#bib.bib59)] | 19.07 $\pm$ 0.95 |  | 0.25  $\pm$ 0 | 0.67 $\pm$
    0.01 |'
  prefs: []
  type: TYPE_TB
- en: 'The results for language translation and speech recognition for different AFs
    are illustrated in Table [11](#S9.T11 "Table 11 â€£ 9.2 Experimental Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark"). The German to English translation
    is used to test the performance of the AFs over text data. Benchmark Seq2Seq model
    consisting of a Long Short Term Memory (LSTM) based autoencoder network is used
    for the experiment. The model and dataset are downloaded from KaggleÂ²Â²2https://www.kaggle.com/parthplc/pytorch-seq2seq-machine-translation/notebook.
    The AF is applied to the feature embedding before the dropout layer. For the language
    translation experiments, the number of Epochs is set to 50 with 0.001 learning
    rate and 256 batch size. The embedding size of encoder and decoder is 300. The
    dropout factor is 0.5 for both encoder and decoder. Adam optimizer is used for
    the training with cross entropy loss. The Bleu score [[155](#bib.bib155)] with
    $4$-gram is reported in Table [11](#S9.T11 "Table 11 â€£ 9.2 Experimental Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") in $2^{nd}$ column for different
    AFs. The mean and standard deviation of Bleu score over 5 trials are reported
    for each AF. It is noticed that the Tanh and SELU AFs are better suitable for
    language translation. The PReLU, LiSHT, SRS and PAU AFs also perform better for
    language translation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The speech recognition experiment is also performed to show the performance
    of the different AFs for time-series signal data. The end-to-end speech recognition
    based Deep Speech 2 framework available from assemblyaiÂ³Â³3https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch
    is used. The model consists of $2$ layers of residual convolution layers to learn
    the relevant audio features, and $2$ layers of bidirectional gated recurrent units
    (GRUs) to use the learned residual convolutional audio features. The $100$ hours
    of transcribed audio English data from LibriSpeech dataset is used for the experiment.
    For the speech recognition experiments, torchaudio 0.4.0 and torch 1.4.0 are used.
    The model consists of 2 CNN layers and 2 RNN layers. The dimension of a RNN layer
    is 512\. Number of classes is 29 in the dataset. Dropout factor is 0.5\. The learning
    rate is 0.0005, batch size is 10 and the number of Epochs is 10. The mean and
    standard deviation over 5 trials of character error rate (CER) and word error
    rate (WER) are reported in Table [11](#S9.T11 "Table 11 â€£ 9.2 Experimental Performance
    Analysis â€£ 9 Performance Comparison and Analysis â€£ Activation Functions in Deep
    Learning: A Comprehensive Survey and Benchmark") for speech recognition. The recent
    AFs such as PReLU, GELU, Swish, Mish and PAU AFs are found as the most suitable
    for speech recognition in this experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: 10 Conclusion and Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An extensive and up to date survey of activation functions is conducted in this
    paper. Different types of AFs are considered, including Logistic Sigmoid and Tanh
    based, ReLU based, ELU based, and Learning based. However, the main focus is given
    to the recent developments in AFs in view of the deep learning applications of
    neural networks. The overview of AFs presented in this paper focuses on the aspects
    including the detailed coverage of AFs, classification and performance comparison
    over image, text and speech data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are the concluding remarks of the survey and performance analysis
    conducted through this paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the improvements in Logistic Sigmoid and Tanh targets to tackle the
    non zero-mean and zero-gradient problems. However, these improvements carry forward
    the drawback of increased complexity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ReLU variants try to tackle the three major problems of ReLU, namely under-utilization
    of negative values, limited nonlinearity and unbounded output. These activations
    perform well for some applications, e.g. LReLU and ABReLU works better with residual
    networks. However, most of these activations fail to perform better than ReLU,
    e.g. LReLU, PReLU and ABReLU do not improve for MobileNet, VGG and GoogleNet models.
    Note that, the ReLU, Leaky ReLU and PReLU AFs are the most common choice among
    researchers due to its simplicity. Moreover, many networks consider the ReLU as
    a default choice for the AF.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exponential based AFs also focus over the better utilization of the negative
    values and to avoid the saturation for important features. However, most of the
    exponential activations suffer due to the non-smooth functions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning based adaptive AFs try to find the best parameters to represent
    the non-linearity needed for the given dataset. This category of AF has gained
    more popularity in recent years. However, the major problem associated with such
    AF is to find the better base function and number of trainable parameters. Some
    AFs diverge during the training if not initialized properly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In contrast to existing surveys, this survey covers an exhaustive list different
    types of AFs. Moreover, a performance analysis on different types of data using
    several AFs provides new insights for future research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Following are the recommendations curated from this survey and performance
    analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to speed up the training, both negative & positive values should be
    used to ensure the near zero mean.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most important aspect in deep learning is to find the network having matching
    complexity as the dataset complexity. If the complexity of the model is high then
    it may lead to overfitting and if the complexity of the model is low then it may
    lead to under convergence. Thus, the AF should bridge this gap based on the model
    and dataset complexity during training automatically.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Logistic Sigmoid and Tanh AFs should be avoided for Convolutional Neural
    Networks as it leads to poor convergence. However, this type of AF is commonly
    used as gates in recurrent neural networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite the ReLU being a popular choice, recently proposed AFs such as Swish,
    Mish, and PAU are also worth trying for different problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ReLU, Mish and PDELU activation functions have shown a good performance
    with VGG16 and GoogleNet. The ReLU, LReLU, ELU, GELU, CELU, and PDELU functions
    are better for the networks having residual connections for image classification.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, the parametric AFs show better convergence as it can adapt the data
    faster by learning the parameter from the data. Specially, PAU, PReLU and PDELU
    have shown better convergence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some AFs lead to increased training time complexity. PDELU and SRS are such
    examples. However, AFs such as ReLU, SELU, GELU, and Softplus depict a promising
    tradeoff between the accuracy and training time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exponential AFs generally lead to the increased non-linearity due to utilization
    of the negative values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Tanh and SELU AFs are found better for language translation along with PReLU,
    LiSHT, SRS and PAU.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is suggested to use the PReLU, GELU, Swish, Mish and PAU AFs for speech recognition.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] F.Â Shao, L.Â Chen, J.Â Shao, W.Â Ji, S.Â Xiao, L.Â Ye, Y.Â Zhuang, J.Â Xiao, Deep
    learning for weakly-supervised object detection and localization: A survey, Neurocomputing
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y.Â Mo, Y.Â Wu, X.Â Yang, F.Â Liu, Y.Â Liao, Review the state-of-the-art technologies
    of semantic segmentation based on deep learning, Neurocomputing (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y.Â Guo, F.Â Feng, X.Â Hao, X.Â Chen, Jac-net: Joint learning with adaptive
    exploration and concise attention for unsupervised domain adaptive person re-identification,
    Neurocomputing (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] S.Â R. Dubey, A decade survey of content based image retrieval using deep
    learning, IEEE Transactions on Circuits and Systems for Video Technology (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] X.Â Xia, X.Â Pan, N.Â Li, X.Â He, L.Â Ma, X.Â Zhang, N.Â Ding, Gan-based anomaly
    detection: A review, Neurocomputing (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] H.Â Li, Y.Â Pan, J.Â Zhao, L.Â Zhang, Skin disease diagnosis with deep learning:
    a review, Neurocomputing 464 (2021) 364â€“393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] C.Â H. Dagli, Artificial neural networks for intelligent manufacturing,
    Springer Science & Business Media, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A.Â Krizhevsky, I.Â Sutskever, G.Â E. Hinton, Imagenet classification with
    deep convolutional neural networks, in: Advances in Neural Information Processing
    Systems, 2012, pp. 1097â€“1105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A.Â Graves, A.-r. Mohamed, G.Â Hinton, Speech recognition with deep recurrent
    neural networks, in: IEEE International Conference on Acoustics, Speech and Signal
    Processing, 2013, pp. 6645â€“6649.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] K.Â K. Babu, S.Â R. Dubey, Pcsgan: Perceptual cyclic-synthesized generative
    adversarial networks for thermal and nir to visible image transformation, Neurocomputing
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J.Â Liu, Y.Â Liu, Q.Â Zhang, A weight initialization method based on neural
    network with asymmetric activation function, Neurocomputing (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Y.Â Srivastava, V.Â Murali, S.Â R. Dubey, A performance evaluation of loss
    functions for deep face recognition, in: National Conference on Computer Vision,
    Pattern Recognition, Image Processing, and Graphics, Springer, 2019, pp. 322â€“332.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S.Â S. Basha, S.Â R. Dubey, V.Â Pulabaigari, S.Â Mukherjee, Impact of fully
    connected layers on performance of convolutional neural networks for image classification,
    Neurocomputing 378 (2020) 112â€“119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Q.Â Xu, M.Â Zhang, Z.Â Gu, G.Â Pan, Overfitting remedy by sparsifying regularization
    on fully-connected layers of cnns, Neurocomputing 328 (2019) 69â€“74.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S.Â R. Dubey, S.Â Chakraborty, S.Â K. Roy, S.Â Mukherjee, S.Â K. Singh, B.Â B.
    Chaudhuri, diffgrad: An optimization method for convolutional neural networks,
    IEEE transactions on neural networks and learning systems 31Â (11) (2019) 4500â€“4511.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] W.Â Duch, N.Â Jankowski, Survey of neural transfer functions, Neural Computing
    Surveys 2Â (1) (1999) 163â€“212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] V.Â Nair, G.Â E. Hinton, Rectified linear units improve restricted boltzmann
    machines, in: International Conference on Machine Learning, 2010, pp. 807â€“814.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Y.Â LeCun, L.Â Bottou, Y.Â Bengio, P.Â Haffner, Gradient-based learning applied
    to document recognition, Proceedings of the IEEE 86Â (11) (1998) 2278â€“2324.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] A.Â N.Â S. Njikam, H.Â Zhao, A novel activation function for multilayer feed-forward
    neural networks, Applied Intelligence 45Â (1) (2016) 75â€“82.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] B.Â Xu, R.Â Huang, M.Â Li, Revise saturated activation functions, International
    Conference on Learning Representations Workshop (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] S.Â Kong, M.Â Takatsuka, Hexpo: A vanishing-proof activation function, in:
    International Joint Conference on Neural Networks, 2017, pp. 2562â€“2567.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Y.Â Qin, X.Â Wang, J.Â Zou, The optimized deep belief networks with improved
    logistic sigmoid units and their application in fault diagnosis for planetary
    gearboxes of wind turbines, IEEE Transactions on Industrial Electronics 66Â (5)
    (2018) 3814â€“3824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S.Â Elfwing, E.Â Uchibe, K.Â Doya, Sigmoid-weighted linear units for neural
    network function approximation in reinforcement learning, Neural Networks 107
    (2018) 3â€“11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S.Â K. Roy, S.Â Manna, S.Â R. Dubey, B.Â B. Chaudhuri, Lisht: Non-parametric
    linearly scaled hyperbolic tangent activation function for neural networks, arXiv
    preprint arXiv:1901.05894 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A.Â Farzad, H.Â Mashayekhi, H.Â Hassanpour, A comparative performance analysis
    of different activation functions in lstm networks for classification, Neural
    Computing and Applications 31Â (7) (2019) 2507â€“2521.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Y.Â Zhou, D.Â Li, S.Â Huo, S.-Y. Kung, Soft-root-sign activation function,
    arXiv preprint arXiv:2003.00547 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] D.-A. Clevert, T.Â Unterthiner, S.Â Hochreiter, Fast and accurate deep network
    learning by exponential linear units (elus), in: International Conference on Learning
    Representations, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] F.Â Agostinelli, M.Â Hoffman, P.Â Sadowski, P.Â Baldi, Learning activation
    functions to improve deep neural networks, International Conference on Learning
    Representations Workshops (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] P.Â Ramachandran, B.Â Zoph, Q.Â V. Le, Searching for activation functions,
    International Conference on Learning Representations Workshops (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Y.Â LeCun, Y.Â Bengio, G.Â Hinton, Deep learning, nature 521Â (7553) (2015)
    436â€“444.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] P.Â Chandra, Y.Â Singh, An activation function adapting training algorithm
    for sigmoidal feedforward networks, Neurocomputing 61 (2004) 429â€“437.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S.Â S. Sodhi, P.Â Chandra, Bi-modal derivative activation function for sigmoidal
    feedforward networks, Neurocomputing 143 (2014) 182â€“196.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] S.Â Eger, P.Â Youssef, I.Â Gurevych, Is it time to swish? comparing deep
    learning activation functions across nlp tasks, arXiv preprint arXiv:1901.02671
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A.Â L. Maas, A.Â Y. Hannun, A.Â Y. Ng, Rectifier nonlinearities improve neural
    network acoustic models, in: International Conference on Machine Learning, Vol.Â 30,
    2013, p.Â 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] K.Â He, X.Â Zhang, S.Â Ren, J.Â Sun, Delving deep into rectifiers: Surpassing
    human-level performance on imagenet classification, in: IEEE international conference
    on computer vision, 2015, pp. 1026â€“1034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] W.Â Shang, K.Â Sohn, D.Â Almeida, H.Â Lee, Understanding and improving convolutional
    neural networks via concatenated rectified linear units, in: International Conference
    on Machine Learning, 2016, pp. 2217â€“2225.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S.Â S. Liew, M.Â Khalil-Hani, R.Â Bakhteri, Bounded activation functions
    for enhanced training stability of deep neural networks on visual pattern recognition
    problems, Neurocomputing 216 (2016) 718â€“734.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] R.Â Duggal, A.Â Gupta, P-telu: Parametric tan hyperbolic linear unit activation
    for deep neural networks, in: IEEE International Conference on Computer Vision
    Workshops, 2017, pp. 974â€“978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S.Â Qiu, X.Â Xu, B.Â Cai, Frelu: Flexible rectified linear units for improving
    convolutional neural networks, in: International Conference on Pattern Recognition,
    2018, pp. 1223â€“1228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] X.Â Jiang, Y.Â Pang, X.Â Li, J.Â Pan, Y.Â Xie, Deep neural networks with elastic
    rectified linear units for object recognition, Neurocomputing 275 (2018) 1132â€“1139.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J.Â Cao, Y.Â Pang, X.Â Li, J.Â Liang, Randomly translational activation inspired
    by the input distributions of relu, Neurocomputing 275 (2018) 859â€“868.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] F.Â Godin, J.Â Degrave, J.Â Dambre, W.Â DeÂ Neve, Dual rectified linear units
    (drelus): A replacement for tanh activation functions in quasi-recurrent neural
    networks, Pattern Recognition Letters 116 (2018) 8â€“14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Z.Â Tang, L.Â Luo, H.Â Peng, S.Â Li, A joint residual network with paired
    relus activation for image super-resolution, Neurocomputing 273 (2018) 37â€“46.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S.Â R. Dubey, S.Â Chakraborty, Average biased relu based cnn descriptor
    for improved face retrieval, arXiv preprint arXiv:1804.02051 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Y.Â Liu, J.Â Zhang, C.Â Gao, J.Â Qu, L.Â Ji, Natural-logarithm-rectified activation
    function in convolutional neural networks, in: International Conference on Computer
    and Communications, 2019, pp. 2000â€“2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S.Â Gu, W.Â Li, L.Â V. Gool, R.Â Timofte, Fast image restoration with multi-bin
    trainable linear units, in: IEEE International Conference on Computer Vision,
    2019, pp. 4190â€“4199.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M.Â Basirat, P.Â Roth, L* relu: Piece-wise linear activation functions for
    deep fine-grained visual categorization, in: IEEE Winter Conference on Applications
    of Computer Vision, 2020, pp. 1218â€“1227.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] C.Â Gulcehre, M.Â Moczulski, M.Â Denil, Y.Â Bengio, Noisy activation functions,
    in: International Conference on Machine Learning, 2016, pp. 3059â€“3068.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] I.Â J. Goodfellow, D.Â Warde-Farley, M.Â Mirza, A.Â Courville, Y.Â Bengio,
    Maxout networks, arXiv preprint arXiv:1302.4389 (2013).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] B.Â Xu, N.Â Wang, T.Â Chen, M.Â Li, Empirical evaluation of rectified activations
    in convolutional network, arXiv preprint arXiv:1505.00853 (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] H.Â Li, W.Â Ouyang, X.Â Wang, Multi-bias non-linear activation in deep neural
    networks, in: International Conference on Machine Learning, 2016, pp. 221â€“229.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] G.Â Klambauer, T.Â Unterthiner, A.Â Mayr, S.Â Hochreiter, Self-normalizing
    neural networks, in: Advances in Neural Information Processing Systems, 2017,
    pp. 971â€“980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] J.Â T. Barron, Continuously differentiable exponential linear units, arXiv
    (2017) arXivâ€“1704.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] L.Â Trottier, P.Â Gigu, B.Â Chaib-draa, etÂ al., Parametric exponential linear
    unit for deep convolutional neural networks, in: IEEE International Conference
    on Machine Learning and Applications, 2017, pp. 207â€“214.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y.Â Li, C.Â Fan, Y.Â Li, Q.Â Wu, Y.Â Ming, Improving deep neural network with
    multiple parametric exponential linear units, Neurocomputing 301 (2018) 11â€“24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Z.Â Qiumei, T.Â Dan, W.Â Fenghua, Improved convolutional neural network based
    on fast exponentially linear unit activation function, IEEE Access 7 (2019) 151359â€“151367.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Y.Â Ying, J.Â Su, P.Â Shan, L.Â Miao, X.Â Wang, S.Â Peng, Rectified exponential
    units for convolutional neural networks, IEEE Access 7 (2019) 101633â€“101640.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] D.Â Kim, J.Â Kim, J.Â Kim, Elastic exponential linear units for convolutional
    neural networks, Neurocomputing 406 (2020) 253â€“266.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Q.Â Cheng, H.Â Li, Q.Â Wu, L.Â Ma, N.Â N. King, Parametric deformable exponential
    linear units for deep neural networks, Neural Networks 125 (2020) 281â€“289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] J.Â Si, S.Â L. Harris, E.Â Yfantis, A dynamic relu on neural network, in:
    IEEE Dallas Circuits and Systems Conference, 2018, pp. 1â€“6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] H.Â Hu, Vrelu activation functions for artificial neural networks, in:
    International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery,
    2018, pp. 856â€“860.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] G.Â Lin, W.Â Shen, Research on convolutional neural network based on improved
    relu piecewise activation function, Procedia Computer Science 131 (2018) 977â€“984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] D.Â MacÃªdo, C.Â Zanchettin, A.Â L. Oliveira, T.Â Ludermir, Enhancing batch
    normalized convolutional networks using displaced rectifier linear units: A systematic
    comparative study, Expert Systems with Applications 124 (2019) 271â€“281.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] L.Â B. Godfrey, An evaluation of parametric activation functions for deep
    learning, in: IEEE International Conference on Systems, Man and Cybernetics, 2019,
    pp. 3006â€“3011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] X.Â Jin, C.Â Xu, J.Â Feng, Y.Â Wei, J.Â Xiong, S.Â Yan, Deep learning with s-shaped
    rectified linear activation units, in: AAAI Conference on Artificial Intelligence,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] V.Â S. Bawa, V.Â Kumar, Linearized sigmoidal activation: A novel activation
    function with tractable non-linear characteristics to boost representation capability,
    Expert Systems with Applications 120 (2019) 346â€“356.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] X.Â Wang, Y.Â Qin, Y.Â Wang, S.Â Xiang, H.Â Chen, Reltanh: An activation function
    with vanishing gradient resistance for sae-based dnns and its application to rotating
    machinery fault diagnosis, Neurocomputing 363 (2019) 88â€“98.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] X.Â Hu, P.Â Niu, J.Â Wang, X.Â Zhang, A dynamic rectified linear activation
    units, IEEE Access 7 (2019) 180409â€“180416.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A.Â Nicolae, Plu: The piecewise linear unit activation function, arXiv
    preprint arXiv:1809.09534 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] L.Â B. Godfrey, M.Â S. Gashler, A continuum among logarithmic, linear, and
    exponential functions, and its potential to improve generalization in neural networks,
    in: International Joint Conference on Knowledge Discovery, Knowledge Engineering
    and Knowledge Management, Vol.Â 1, 2015, pp. 481â€“486.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] B.Â Grelsson, M.Â Felsberg, Improved learning in convolutional neural networks
    with shifted exponential linear units (shelus), in: International Conference on
    Pattern Recognition, 2018, pp. 517â€“522.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Y.Â Yu, K.Â Adu, N.Â Tashi, P.Â Anokye, X.Â Wang, M.Â A. Ayidzoe, Rmaf: Relu-memristor-like
    activation function for deep learning, IEEE Access 8 (2020) 72727â€“72741.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] M.Â Basirat, P.Â M. Roth, The quest for the golden activation function,
    arXiv preprint arXiv:1808.00783 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] S.Â Scardapane, M.Â Scarpiniti, D.Â Comminiello, A.Â Uncini, Learning activation
    functions from data using cubic spline interpolation, in: Italian Workshop on
    Neural Nets, 2017, pp. 73â€“83.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] A.Â Mishra, P.Â Chandra, U.Â Ghose, S.Â S. Sodhi, Bi-modal derivative adaptive
    activation function sigmoidal feedforward artificial neural networks, Applied
    Soft Computing 61 (2017) 983â€“994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S.Â Qian, H.Â Liu, C.Â Liu, S.Â Wu, H.Â SanÂ Wong, Adaptive activation functions
    in convolutional neural networks, Neurocomputing 272 (2018) 204â€“212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] E.Â Alcaide, E-swish: Adjusting activations to different network depths,
    arXiv preprint arXiv:1801.07145 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Ã–.Â F. ErtuÄŸrul, A novel type of activation function in artificial neural
    networks: Trained activation function, Neural Networks 99 (2018) 148â€“157.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] M.Â Goyal, R.Â Goyal, B.Â Lall, Learning activation functions: A new paradigm
    of understanding neural networks, arXiv preprint arXiv:1906.09529 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] G.Â Maguolo, L.Â Nanni, S.Â Ghidoni, Ensemble of convolutional neural networks
    trained with different activation functions, arXiv preprint arXiv:1905.02473 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] H.Â H. Chieng, N.Â Wahid, P.Â Ong, S.Â R.Â K. Perla, Flatten-t swish: a thresholded
    relu-swish-like activation function for deep learning, arXiv preprint arXiv:1812.06247
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] N.Â Patwardhan, M.Â Ingalhalikar, R.Â Walambe, Aria: Utilizing richardâ€™s
    curve for controlling the non-monotonicity of the activation function in deep
    neural nets, arXiv preprint arXiv:1805.08878 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] M.Â Dushkoff, R.Â Ptucha, Adaptive activation functions for deep networks,
    Electronic Imaging 2016Â (19) (2016) 1â€“5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] F.Â Manessi, A.Â Rozza, Learning combinations of activation functions, in:
    IEEE International Conference on Pattern Recognition, 2018, pp. 61â€“66.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] L.Â R. SÃ¼tfeld, F.Â Brieger, H.Â Finger, S.Â FÃ¼llhase, G.Â Pipa, Adaptive blending
    units: Trainable activation functions for deep neural networks, arXiv preprint
    arXiv:1806.10064 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] M.Â Wang, B.Â Liu, H.Â Foroosh, Look-up table unit activation function for
    deep convolutional neural networks, in: IEEE Winter Conference on Applications
    of Computer Vision, 2018, pp. 1225â€“1233.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] D.Â Klabjan, M.Â Harmon, Activation ensembles for deep neural networks,
    in: IEEE International Conference on Big Data, 2019, pp. 206â€“214.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C.Â Eisenach, Z.Â Wang, H.Â Liu, Nonparametrically learning activation functions
    in deep neural nets, in: International Conference on Learning Representations
    Workshops, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] C.Â J. Vercellino, W.Â Y. Wang, Hyperactivations for activation function
    exploration, in: Conference on Neural Information Processing Systems Workshop
    on Meta-learning, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A.Â D. Jagtap, K.Â Kawaguchi, G.Â E. Karniadakis, Adaptive activation functions
    accelerate convergence in deep and physics-informed neural networks, Journal of
    Computational Physics 404 (2020) 109136.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] C.Â Dugas, Y.Â Bengio, F.Â BÃ©lisle, C.Â Nadeau, R.Â Garcia, Incorporating second-order
    functional knowledge for better option pricing, in: Advances in Neural Information
    Processing Systems, 2001, pp. 472â€“478.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] X.Â Glorot, A.Â Bordes, Y.Â Bengio, Deep sparse rectifier neural networks,
    in: International Conference on Artificial Intelligence and Statistics, 2011,
    pp. 315â€“323.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] H.Â Zheng, Z.Â Yang, W.Â Liu, J.Â Liang, Y.Â Li, Improving deep neural networks
    using softplus units, in: International Joint Conference on Neural Networks, 2015,
    pp. 1â€“4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Q.Â Liu, S.Â Furber, Noisy softplus: a biology inspired activation function,
    in: International Conference on Neural Information Processing, 2016, pp. 405â€“412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] H.Â Zhao, F.Â Liu, L.Â Li, C.Â Luo, A novel softplus linear unit for deep
    convolutional neural networks, Applied Intelligence 48Â (7) (2018) 1707â€“1720.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] C.Â Xu, J.Â Huang, S.-p. Wang, A.-q. Hu, A novel parameterized activation
    function in visual geometry group, in: International Conference on Data Science
    and Business Analytics, 2018, pp. 386â€“389.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] K.Â Sun, J.Â Yu, L.Â Zhang, Z.Â Dong, A convolutional neural network model
    based on improved softplus activation function, in: International Conference on
    Applications and Techniques in Cyber Security and Intelligence, 2019, pp. 1326â€“1335.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Y.Â Chen, Y.Â Mai, J.Â Xiao, L.Â Zhang, Improving the antinoise ability of
    dnns via a bio-inspired noise adaptive activation function rand softplus, Neural
    Computation 31Â (6) (2019) 1215â€“1233.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] D.Â Misra, Mish: A self regularized non-monotonic neural activation function,
    arXiv preprint arXiv:1908.08681 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] A.Â Bochkovskiy, C.-Y. Wang, H.-Y.Â M. Liao, Yolov4: Optimal speed and
    accuracy of object detection, arXiv preprint arXiv:2004.10934 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] D.Â Hendrycks, K.Â Gimpel, Gaussian error linear units (gelus), arXiv preprint
    arXiv:1606.08415 (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] C.Â Yu, Z.Â Su, Symmetrical gaussian error linear units (sgelus), arXiv
    preprint arXiv:1911.03925 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Q.Â Su, L.Â Carin, etÂ al., A probabilistic framework for nonlinearities
    in stochastic neural networks, in: Advances in Neural Information Processing Systems,
    2017, pp. 4486â€“4495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] J.Â Lee, K.Â Shridhar, H.Â Hayashi, B.Â K. Iwana, S.Â Kang, S.Â Uchida, Probact:
    A probabilistic activation function for deep neural networks, arXiv preprint arXiv:1905.10761
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] L.Â Hou, D.Â Samaras, T.Â M. Kurc, Y.Â Gao, J.Â H. Saltz, Convnets with smooth
    adaptive activation functions for regression, Proceedings of Machine Learning
    Research 54 (2017) 430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Y.Â Berradi, Symmetric power activation functions for deep neural networks,
    in: International Conference on Learning and Optimization Algorithms: Theory and
    Applications, 2018, pp. 1â€“6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] E.Â LÃ³pez-Rubio, F.Â Ortega-Zamorano, E.Â DomÃ­nguez, J.Â MuÃ±oz-PÃ©rez, Piecewise
    polynomial activation functions for feedforward neural networks, Neural Processing
    Letters 50Â (1) (2019) 121â€“147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] F.Â Farhadi, V.Â P. Nia, A.Â Lodi, Activation adaptation in neural networks,
    arXiv preprint arXiv:1901.09849 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] B.Â Li, S.Â Tang, H.Â Yu, Powernet: Efficient representations of polynomials
    and smooth functions by deep neural networks with rectified power units, arXiv
    preprint arXiv:1909.05136 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] M.Â Telgarsky, Neural networks and rational functions, in: International
    Conference on Machine Learning, 2017, pp. 3387â€“3393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] A.Â Molina, P.Â Schramowski, K.Â Kersting, Pad$\acute{e}$ activation units:
    End-to-end learning of flexible activation functions in deep networks, International
    Conference on Learning Representations (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] A.Â T. NicolasÂ BoullÃ©, YujiÂ Nakatsukasa, Rational neural networks, arXiv
    preprint arXiv:2004.01902 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A.Â Apicella, F.Â IsgrÃ², R.Â Prevete, A simple and efficient architecture
    for trainable activation functions, Neurocomputing 370 (2019) 1â€“15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Y.Â Chen, X.Â Dai, M.Â Liu, D.Â Chen, L.Â Yuan, Z.Â Liu, Dynamic relu, arXiv
    preprint arXiv:2003.10027 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] M.Â Wang, B.Â Liu, H.Â Foroosh, Wide hidden expansion layer for deep convolutional
    neural networks, in: IEEE Winter Conference on Applications of Computer Vision,
    2020, pp. 934â€“942.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] A.Â Asif, etÂ al., Learning neural activations, arXiv preprint arXiv:1912.12187
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] S.Â Scardapane, S.Â VanÂ Vaerenbergh, S.Â Totaro, A.Â Uncini, Kafnets: Kernel-based
    non-parametric activation functions for neural networks, Neural Networks 110 (2019)
    19â€“32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] S.Â Scardapane, E.Â Nieddu, D.Â Firmani, P.Â Merialdo, Multikernel activation
    functions: formulation and a case study, in: INNS Big Data and Deep Learning conference,
    2019, pp. 320â€“329.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] S.Â Scardapane, S.Â VanÂ Vaerenbergh, A.Â Hussain, A.Â Uncini, Complex-valued
    neural networks with nonparametric activation functions, IEEE Transactions on
    Emerging Topics in Computational Intelligence (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] S.Â Scardapane, S.Â VanÂ Vaerenbergh, D.Â Comminiello, A.Â Uncini, Widely
    linear kernels for complex-valued kernel activation functions, in: IEEE International
    Conference on Acoustics, Speech and Signal Processing, 2019, pp. 8528â€“8532.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] M.Â Kobayashi, Singularities of three-layered complex-valued neural networks
    with split activation function, IEEE Transactions on Neural Networks and Learning
    Systems 29Â (5) (2017) 1900â€“1907.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] J.Â Pennington, S.Â Schoenholz, S.Â Ganguli, Resurrecting the sigmoid in
    deep learning through dynamical isometry: theory and practice, in: Advances in
    Neural Information Processing Systems, 2017, pp. 4785â€“4795.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] E.Â Sansone, F.Â G. DeÂ Natale, Training feedforward neural networks with
    standard logistic activations is feasible, arXiv preprint arXiv:1710.01013 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] L.Â Lu, Y.Â Shin, Y.Â Su, G.Â E. Karniadakis, Dying relu and initialization:
    Theory and numerical examples, arXiv preprint arXiv:1903.06733 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] D.Â Arpit, Y.Â Bengio, The benefits of over-parameterization at initialization
    in deep relu networks, arXiv preprint arXiv:1901.03611 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] D.Â Aguirre, O.Â Fuentes, Improving weight initialization of relu and output
    layers, in: International Conference on Artificial Neural Networks, 2019, pp.
    170â€“184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] R.Â Burkholz, A.Â Dubatovka, Initialization of relus for dynamical isometry,
    in: Advances in Neural Information Processing Systems, 2019, pp. 2382â€“2392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] D.Â Yarotsky, Error bounds for approximations with deep relu networks,
    Neural Networks 94 (2017) 103â€“114.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] R.Â Arora, A.Â Basu, P.Â Mianjy, A.Â Mukherjee, Understanding deep neural
    networks with rectified linear units, arXiv preprint arXiv:1611.01491 (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] M.Â Hein, M.Â Andriushchenko, J.Â Bitterwolf, Why relu networks yield high-confidence
    predictions far away from the training data and how to mitigate the problem, in:
    IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 41â€“50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] S.Â Goel, S.Â Karmalkar, A.Â Klivans, Time/accuracy tradeoffs for learning
    a relu with respect to gaussian marginals, in: Advances in Neural Information
    Processing Systems, 2019, pp. 8582â€“8591.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] S.Â Dittmer, J.Â Emily, P.Â Maass, Singular values for relu layers, IEEE
    Transactions on Neural Networks and Learning Systems (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] A.Â Kristiadi, M.Â Hein, P.Â Hennig, Being bayesian, even just a bit, fixes
    overconfidence in relu networks, arXiv preprint arXiv:2002.10118 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] B.Â Karlik, A.Â V. Olgac, Performance analysis of various activation functions
    in generalized mlp architectures of neural networks, International Journal of
    Artificial Intelligence and Expert Systems 1Â (4) (2011) 111â€“122.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] G.Â Alcantara, Empirical analysis of non-linear activation functions for
    deep neural networks in classification tasks, arXiv preprint arXiv:1710.11272
    (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] H.Â K. Vydana, A.Â K. Vuppala, Investigative study of various activation
    functions for speech recognition, in: National Conference on Communications, 2017,
    pp. 1â€“5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] D.Â Pedamonti, Comparison of non-linear activation functions for deep
    neural networks on mnist classification task, arXiv preprint arXiv:1804.02763
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] C.Â Nwankpa, W.Â Ijomah, A.Â Gachagan, S.Â Marshall, Activation functions:
    Comparison of trends in practice and research for deep learning, arXiv preprint
    arXiv:1811.03378 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] K.Â Eckle, J.Â Schmidt-Hieber, A comparison of deep networks with relu
    activation function and linear spline-type methods, Neural Networks 110 (2019)
    232â€“242.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] M.Â M. Lau, K.Â H. Lim, Review of adaptive activation function in deep
    neural network, in: IEEE-EMBS Conference on Biomedical Engineering and Sciences,
    2018, pp. 686â€“690.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] A.Â K. Dubey, V.Â Jain, Comparative study of convolution neural networkâ€™s
    relu and leaky-relu activation functions, in: Applications of Computing, Automation
    and Wireless Systems in Electrical Engineering, Springer, 2019, pp. 873â€“880.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] C.Â Banerjee, T.Â Mukherjee, E.Â PasiliaoÂ Jr, An empirical study on generalizations
    of the relu activation function, in: ACM Southeast Conference, 2019, pp. 164â€“167.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] T.Â Villmann, J.Â Ravichandran, A.Â Villmann, D.Â Nebel, M.Â Kaden, Activation
    functions for generalized learning vector quantization-a performance comparison,
    arXiv preprint arXiv:1901.05995 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] G.Â Castaneda, P.Â Morris, T.Â M. Khoshgoftaar, Evaluation of maxout activations
    in deep learning across several big data domains, Journal of Big Data 6Â (1) (2019)
    72.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Y.Â Wang, Y.Â Li, Y.Â Song, X.Â Rong, The influence of the activation function
    in a convolution neural network model of facial expression recognition, Applied
    Sciences 10Â (5) (2020) 1897.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] A.Â Apicella, F.Â Donnarumma, F.Â IsgrÃ², R.Â Prevete, A survey on modern
    trainable activation functions, arXiv preprint arXiv:2005.00817 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] T.Â SzandaÅ‚a, Review and comparison of commonly used activation functions
    for deep neural networks, in: Bio-inspired Neurocomputing, 2020, pp. 203â€“224.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] A.Â Krizhevsky, Learning multiple layers of features from tiny images,
    Tech Report, Univ. of Toronto (2009).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] A.Â G. Howard, M.Â Zhu, B.Â Chen, D.Â Kalenichenko, W.Â Wang, T.Â Weyand, M.Â Andreetto,
    H.Â Adam, Mobilenets: Efficient convolutional neural networks for mobile vision
    applications, arXiv preprint arXiv:1704.04861 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] K.Â Simonyan, A.Â Zisserman, Very deep convolutional networks for large-scale
    image recognition, arXiv preprint arXiv:1409.1556 (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] C.Â Szegedy, W.Â Liu, Y.Â Jia, P.Â Sermanet, S.Â Reed, D.Â Anguelov, D.Â Erhan,
    V.Â Vanhoucke, A.Â Rabinovich, Going deeper with convolutions, in: IEEE Conference
    on Computer Vision and Pattern Recognition, 2015, pp. 1â€“9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] K.Â He, X.Â Zhang, S.Â Ren, J.Â Sun, Deep residual learning for image recognition,
    in: IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770â€“778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] J.Â Hu, L.Â Shen, G.Â Sun, Squeeze-and-excitation networks, in: IEEE Conference
    on Computer Vision and Pattern Recognition, 2018, pp. 7132â€“7141.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] G.Â Huang, Z.Â Liu, L.Â Van DerÂ Maaten, K.Â Q. Weinberger, Densely connected
    convolutional networks, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2017, pp. 4700â€“4708.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] K.Â Papineni, S.Â Roukos, T.Â Ward, W.-J. Zhu, Bleu: a method for automatic
    evaluation of machine translation, in: Proceedings of the 40th annual meeting
    of the Association for Computational Linguistics, 2002, pp. 311â€“318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
