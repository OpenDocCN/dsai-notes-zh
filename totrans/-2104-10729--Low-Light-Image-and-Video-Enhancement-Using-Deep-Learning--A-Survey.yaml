- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:55:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:55:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2104.10729] Low-Light Image and Video Enhancement Using Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2104.10729] 使用深度学习的低光照图像和视频增强：一项综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2104.10729](https://ar5iv.labs.arxiv.org/html/2104.10729)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2104.10729](https://ar5iv.labs.arxiv.org/html/2104.10729)
- en: \WarningFilter
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \WarningFilter
- en: latexFont shape \WarningFilterlatexfontFont shape
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: latexFont shape \WarningFilterlatexfontFont shape
- en: \justify
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \justify
- en: Low-Light Image and Video Enhancement
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低光照图像和视频增强
- en: 'Using Deep Learning: A Survey'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习：一项综述
- en: Chongyi Li, Chunle Guo, Linghao Han, Jun Jiang, Ming-Ming Cheng, ,
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Chongyi Li、Chunle Guo、Linghao Han、Jun Jiang、Ming-Ming Cheng、
- en: 'Jinwei Gu, , and Chen Change Loy C. Li and C. C. Loy are with the S-Lab, Nanyang
    Technological University (NTU), Singapore (e-mail: chongyi.li@ntu.edu.sg and ccloy@ntu.edu.sg).C.
    Guo, L. Han, and M. M. Cheng are with the College of Computer Science, Nankai
    University, Tianjin, China (e-mail: guochunle@nankai.edu.cn, lhhan@mail.nankai.edu.cn,
    and cmm@nankai.edu.cn).J. Jiang and J. Gu are with the SenseTime (e-mail: jiangjun@sensebrain.site
    and gujinwei@sensebrain.site).C. Li and C. Guo contribute equally.C. C. Loy is
    the corresponding author.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Jinwei Gu、Chen Change Loy C. Li 和 C. C. Loy 均隶属于新加坡南洋理工大学（NTU）的 S-Lab（电子邮件：chongyi.li@ntu.edu.sg
    和 ccloy@ntu.edu.sg）。C. Guo、L. Han 和 M. M. Cheng 隶属于中国天津南开大学计算机学院（电子邮件：guochunle@nankai.edu.cn、lhhan@mail.nankai.edu.cn
    和 cmm@nankai.edu.cn）。J. Jiang 和 J. Gu 隶属于 SenseTime（电子邮件：jiangjun@sensebrain.site
    和 gujinwei@sensebrain.site）。C. Li 和 C. Guo 贡献相等。C. C. Loy 为通讯作者。
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Low-light image enhancement (LLIE) aims at improving the perception or interpretability
    of an image captured in an environment with poor illumination. Recent advances
    in this area are dominated by deep learning-based solutions, where many learning
    strategies, network structures, loss functions, training data, etc. have been
    employed. In this paper, we provide a comprehensive survey to cover various aspects
    ranging from algorithm taxonomy to unsolved open issues. To examine the generalization
    of existing methods, we propose a low-light image and video dataset, in which
    the images and videos are taken by different mobile phones’ cameras under diverse
    illumination conditions. Besides, for the first time, we provide a unified online
    platform that covers many popular LLIE methods, of which the results can be produced
    through a user-friendly web interface. In addition to qualitative and quantitative
    evaluation of existing methods on publicly available and our proposed datasets,
    we also validate their performance in face detection in the dark. This survey
    together with the proposed dataset and online platform could serve as a reference
    source for future study and promote the development of this research field. The
    proposed platform and dataset as well as the collected methods, datasets, and
    evaluation metrics are publicly available and will be regularly updated. Project
    page: [https://www.mmlab-ntu.com/project/lliv_survey/index.html](https://www.mmlab-ntu.com/project/lliv_survey/index.html).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 低光照图像增强（LLIE）的目标是提高在光照不足环境下拍摄图像的感知或可解释性。该领域的最新进展主要由基于深度学习的解决方案主导，许多学习策略、网络结构、损失函数、训练数据等已被应用于此论文中。我们提供了一个全面的调查，涵盖从算法分类到未解决的开放问题的各个方面。为了检验现有方法的泛化能力，我们提出了一个低光照图像和视频数据集，其中图像和视频是由不同手机摄像头在不同光照条件下拍摄的。此外，我们首次提供了一个统一的在线平台，涵盖了许多流行的
    LLIE 方法，用户可以通过友好的网页界面生成结果。除了对公开可用数据集和我们提出的数据集上的现有方法进行定性和定量评估外，我们还验证了它们在黑暗中的人脸检测性能。该调查以及提出的数据集和在线平台可作为未来研究的参考来源，促进该研究领域的发展。所提出的平台和数据集，以及收集的方法、数据集和评估指标都是公开的，并将定期更新。项目页面：[https://www.mmlab-ntu.com/project/lliv_survey/index.html](https://www.mmlab-ntu.com/project/lliv_survey/index.html)。
- en: 'Index Terms:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: image and video restoration, low-light image dataset, low-light image enhancement
    platform, computational photography.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和视频修复，低光照图像数据集，低光照图像增强平台，计算摄影学。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Images are often taken under sub-optimal lighting conditions, under the influence
    of backlit, uneven light, and dim light, due to inevitable environmental and/or
    technical constraints such as insufficient illumination and limited exposure time.
    Such images suffer from the compromised aesthetic quality and unsatisfactory transmission
    of information for high-level tasks such as object tracking, recognition, and
    detection. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Low-Light Image and
    Video Enhancement Using Deep Learning: A Survey") shows some examples of the degradations
    induced by sub-optimal lighting conditions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '图像常常在次优光照条件下拍摄，受背光、不均匀光线和暗光的影响，这些情况是由于不可避免的环境和/或技术限制，比如照明不足和曝光时间有限。这些图像在美学质量上有所妥协，并且在高层次任务如目标跟踪、识别和检测中传递信息效果不佳。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Low-Light Image and Video Enhancement Using Deep
    Learning: A Survey")展示了一些由次优光照条件引起的退化示例。'
- en: Low-light enhancement enjoys a wide range of applications in different areas,
    including visual surveillance, autonomous driving, and computational photography.
    In particular, smartphone photography has become ubiquitous and prominent. Limited
    by the size of the camera aperture, the requirement of real-time processing, and
    the constraint of memory, taking photographs with a smartphone’s camera in a dim
    environment is especially challenging. There is an exciting research arena of
    enhancing low-light images and videos in such applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 低光增强在不同领域中有广泛的应用，包括视觉监控、自动驾驶和计算摄影。特别是，智能手机摄影已经变得无处不在且非常显著。由于相机光圈的大小、实时处理的需求以及内存限制，在昏暗环境中用智能手机摄像头拍照尤其具有挑战性。在这些应用中，增强低光图像和视频是一个令人兴奋的研究领域。
- en: '| ![Refer to caption](img/8e661f73253a3f5f807b5c76b29ed126.png) | ![Refer to
    caption](img/c3c8e40e1f3f08aec903f259bfa70b38.png) | ![Refer to caption](img/fc623bba47267cf053b6b59ce3139e55.png)
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/8e661f73253a3f5f807b5c76b29ed126.png) | ![参见说明](img/c3c8e40e1f3f08aec903f259bfa70b38.png)
    | ![参见说明](img/fc623bba47267cf053b6b59ce3139e55.png) |'
- en: '| (a) back lit | (b) uneven light | (c) dim light |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| (a) 背光 | (b) 不均匀光 | (c) 暗光 |'
- en: '| ![Refer to caption](img/ef74b3cf4c3d18374b433d8109493445.png) | ![Refer to
    caption](img/3b3c2fef80abf17dc846a1d1f296f800.png) | ![Refer to caption](img/8fb604861d144169a71e6c2821222b64.png)
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/ef74b3cf4c3d18374b433d8109493445.png) | ![参见说明](img/3b3c2fef80abf17dc846a1d1f296f800.png)
    | ![参见说明](img/8fb604861d144169a71e6c2821222b64.png) |'
- en: '| (d) extremely low | (e) colored light | (f) boosted noise |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| (d) 极低光 | (e) 彩色光 | (f) 增强噪声 |'
- en: 'Figure 1: Examples of images taken under sub-optimal lighting conditions. These
    images suffer from the buried scene content, reduced contrast, boosted noise,
    and inaccurate color.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：在次优光照条件下拍摄的图像示例。这些图像受到场景内容隐藏、对比度降低、噪声增强和颜色不准确的影响。
- en: '![Refer to caption](img/e121c55be713e292ee171491930af5e9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e121c55be713e292ee171491930af5e9.png)'
- en: 'Figure 2: A concise milestone of deep learning-based low-light image and video
    enhancement methods. Supervised learning-based methods: LLNet [[1](#bib.bib1)],
    Chen et al. [[2](#bib.bib2)], MBLLEN [[3](#bib.bib3)], Retinex-Net [[4](#bib.bib4)],
    LightenNet [[5](#bib.bib5)], SCIE [[6](#bib.bib6)], DeepUPE [[7](#bib.bib7)],
    Chen et al. [[8](#bib.bib8)], Jiang and Zheng [[9](#bib.bib9)], Wang et al. [[10](#bib.bib10)],
    KinD [[11](#bib.bib11)], Ren et al. [[12](#bib.bib12)], Xu et al. [[13](#bib.bib13)],
    Fan et al. [[14](#bib.bib14)], Lv et al. [[15](#bib.bib15)], EEMEFN [[16](#bib.bib16)],
    SIDGAN. [[17](#bib.bib17)], LPNet [[18](#bib.bib18)], DLN [[19](#bib.bib19)],
    TBEFN [[20](#bib.bib20)], DSLR [[21](#bib.bib21)], Zhang et al. [[22](#bib.bib22)],
    PRIEN [[23](#bib.bib23)], and Retinex-Net [[24](#bib.bib24)]. Reinforcement learning-based
    method: DeepExposure [[25](#bib.bib25)]. Unsupervised learning-based method: EnlightenGAN
    [[26](#bib.bib26)]. Zero-shot learning-based methods: ExCNet [[27](#bib.bib27)],
    Zero-DCE [[28](#bib.bib28)], RRDNet [[29](#bib.bib29)], Zero-DCE++ [[30](#bib.bib30)],
    RetinexDIP [[31](#bib.bib31)], and RUAS [[32](#bib.bib32)]. Semi-supervised learning-based
    method: DRBN [[33](#bib.bib33)] and DRBN [[34](#bib.bib34)].'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于深度学习的低光图像和视频增强方法的简明里程碑。监督学习方法：LLNet [[1](#bib.bib1)], Chen等人 [[2](#bib.bib2)],
    MBLLEN [[3](#bib.bib3)], Retinex-Net [[4](#bib.bib4)], LightenNet [[5](#bib.bib5)],
    SCIE [[6](#bib.bib6)], DeepUPE [[7](#bib.bib7)], Chen等人 [[8](#bib.bib8)], Jiang和Zheng
    [[9](#bib.bib9)], Wang等人 [[10](#bib.bib10)], KinD [[11](#bib.bib11)], Ren等人 [[12](#bib.bib12)],
    Xu等人 [[13](#bib.bib13)], Fan等人 [[14](#bib.bib14)], Lv等人 [[15](#bib.bib15)], EEMEFN
    [[16](#bib.bib16)], SIDGAN [[17](#bib.bib17)], LPNet [[18](#bib.bib18)], DLN [[19](#bib.bib19)],
    TBEFN [[20](#bib.bib20)], DSLR [[21](#bib.bib21)], Zhang等人 [[22](#bib.bib22)],
    PRIEN [[23](#bib.bib23)], 和Retinex-Net [[24](#bib.bib24)]。强化学习方法：DeepExposure
    [[25](#bib.bib25)]。无监督学习方法：EnlightenGAN [[26](#bib.bib26)]。零样本学习方法：ExCNet [[27](#bib.bib27)],
    Zero-DCE [[28](#bib.bib28)], RRDNet [[29](#bib.bib29)], Zero-DCE++ [[30](#bib.bib30)],
    RetinexDIP [[31](#bib.bib31)], 和RUAS [[32](#bib.bib32)]。半监督学习方法：DRBN [[33](#bib.bib33)]
    和DRBN [[34](#bib.bib34)]。
- en: 'Traditional methods for low-light enhancement include Histogram Equalization-based
    methods [[35](#bib.bib35), [36](#bib.bib36)] and Retinex model-based methods [[37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)]. The latter received relatively more attention.
    A typical Retinex model-based approach decomposes a low-light image into a reflection
    component and an illumination component by priors or regularizations. The estimated
    reflection component is treated as the enhanced result. Such methods have some
    limitations: 1) the ideal assumption that treats the reflection component as the
    enhanced result does not always hold, especially given various illumination properties,
    which could lead to unrealistic enhancement such as loss of details and distorted
    colors, 2) the noise is usually ignored in the Retinex model, thus it is remained
    or amplified in the enhanced results, 3) finding an effective prior or regularization
    is challenging. Inaccurate prior or regularization may result in artifacts and
    color deviations in the enhanced results, and 4) the runtime is relatively long
    because of their complicated optimization process.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的低光增强方法包括基于直方图均衡化的方法 [[35](#bib.bib35), [36](#bib.bib36)] 和基于Retinex模型的方法
    [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)]。后者受到了相对更多的关注。典型的基于Retinex模型的方法通过先验或正则化将低光图像分解为反射成分和照明成分。估计的反射成分被视为增强结果。这些方法存在一些限制：1）假设反射成分为增强结果的理想假设并不总是成立，尤其是面对各种照明特性时，这可能导致不切实际的增强效果，如细节丢失和颜色失真；2）噪声通常在Retinex模型中被忽略，因此在增强结果中可能会被保留或放大；3）找到有效的先验或正则化是具有挑战性的。不准确的先验或正则化可能会导致增强结果中的伪影和颜色偏差；4）由于其复杂的优化过程，运行时间相对较长。
- en: 'Recent years have witnessed the compelling success of deep learning-based LLIE
    since the first seminal work [[1](#bib.bib1)]. Deep learning-based solutions enjoy
    better accuracy, robustness, and speed over conventional methods, thus attracting
    increasing attention. A concise milestone of deep learning-based LLIE methods
    is shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Low-Light Image and
    Video Enhancement Using Deep Learning: A Survey"). As shown, since 2017, the number
    of deep learning-based solutions has grown year by year. Learning strategies used
    in these solutions cover Supervised Learning (SL), Reinforcement Learning (RL),
    Unsupervised Learning (UL), Zero-Shot Learning (ZSL), and Semi-Supervised Learning
    (SSL). Note that we only report some representative methods in Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Low-Light Image and Video Enhancement Using Deep
    Learning: A Survey"). In fact, there are more than 100 papers on deep learning-based
    methods from 2017 to 2021\. Moreover, although some general photo enhancement
    methods [[45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)]
    can improve the brightness of images to some extent, we omit them in this survey
    as they are not designed to handle diverse low-light conditions. We concentrate
    on deep learning-based solutions that are specially developed for low-light image
    and video enhancement.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，深度学习基础的低光图像增强（LLIE）取得了显著的成功，自首个开创性研究[[1](#bib.bib1)]以来，深度学习解决方案在准确性、鲁棒性和速度方面优于传统方法，因此受到越来越多的关注。深度学习基础的LLIE方法的一个简明里程碑如图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Low-Light Image and Video Enhancement Using Deep
    Learning: A Survey")所示。如图所示，自2017年以来，深度学习基础的解决方案数量逐年增加。这些解决方案使用的学习策略包括**监督学习（SL）**、**强化学习（RL）**、**无监督学习（UL）**、**零样本学习（ZSL）**和**半监督学习（SSL）**。请注意，我们在图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Low-Light Image and Video Enhancement Using Deep
    Learning: A Survey")中仅报告了一些具有代表性的方法。实际上，从2017年到2021年，有超过100篇关于深度学习基础方法的论文。此外，虽然一些通用的照片增强方法[[45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)]可以在一定程度上提高图像的亮度，但由于这些方法并未针对多样化的低光条件进行设计，我们在本综述中将其省略。我们重点关注那些专门开发用于低光图像和视频增强的深度学习基础解决方案。'
- en: Despite deep learning has dominated the research of LLIE, an in-depth and comprehensive
    survey on deep learning-based solutions is lacking. There are two reviews of LLIE
    [[54](#bib.bib54), [55](#bib.bib55)]. Wang et al. [[54](#bib.bib54)] mainly reviews
    conventional LLIE methods while our work systematically and comprehensively reviews
    recent advances of deep learning-based LLIE. In comparison to Liu et al. [[55](#bib.bib55)]
    that reviews existing LLIE algorithms, measures the machine vision performance
    of different methods, provides a low-light image dataset serving both low-level
    and high-level vision enhancement, and develops an enhanced face detector, our
    survey reviews the low-light image and video enhancement from different aspects
    and has the following unique characteristics. 1) Our work mainly focuses on recent
    advances of deep learning-based low-light image and video enhancement, where we
    provide in-depth analysis and discussion in various aspects, covering learning
    strategies, network structures, loss functions, training datasets, test datasets,
    evaluation metrics, model sizes, inference speed, enhancement performance, etc.
    Thus, this survey centers on deep learning and its applications in low-light image
    and video enhancement. 2) We propose a dataset that contains images and videos
    captured by different mobile phones’ cameras under diverse illumination conditions
    to evaluate the generalization of existing methods. This new and challenging dataset
    is a supplement of existing low-light image and video enhancement datasets as
    such a dataset is lacking in this research area. Besides, we are the first, to
    the best of our knowledge, to compare the performance of deep learning-based low-light
    image enhancement methods on this kind of data. 3) We provide an online platform
    that covers many popular deep learning-based low-light image enhancement methods,
    where the results can be produced by a user-friendly web interface. With our platform,
    one without any GPUs can assess the results of different methods for any input
    images online, which speeds up the development of this research field and helps
    to create new research. We hope that our survey could provide novel insights and
    inspiration to facilitate the understanding of deep learning-based LLIE, foster
    research on the raised open issues, and speed up the development of this research
    field.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习已经主导了LLIE的研究，但关于基于深度学习的解决方案的深入全面综述仍然缺乏。现有两篇关于LLIE的综述[[54](#bib.bib54),
    [55](#bib.bib55)]。Wang等人[[54](#bib.bib54)]主要综述了传统的LLIE方法，而我们的工作系统地、全面地回顾了基于深度学习的LLIE的最新进展。相比之下，Liu等人[[55](#bib.bib55)]综述了现有的LLIE算法，衡量了不同方法的机器视觉性能，提供了一个同时服务于低级和高级视觉增强的低光照图像数据集，并开发了一个增强的面部检测器，而我们的综述从不同的角度回顾了低光照图像和视频增强，具有以下独特特点。1)
    我们的工作主要集中在基于深度学习的低光照图像和视频增强的最新进展上，提供了各种方面的深入分析和讨论，涵盖学习策略、网络结构、损失函数、训练数据集、测试数据集、评估指标、模型大小、推理速度、增强性能等。因此，本综述专注于深度学习及其在低光照图像和视频增强中的应用。2)
    我们提出了一个数据集，其中包含在不同光照条件下由不同手机摄像头拍摄的图像和视频，以评估现有方法的泛化能力。这个新的具有挑战性的数据集是现有低光照图像和视频增强数据集的补充，因为在该研究领域中缺乏这样的数据集。此外，据我们所知，我们首次比较了基于深度学习的低光照图像增强方法在这种数据上的性能。3)
    我们提供了一个在线平台，涵盖了许多流行的基于深度学习的低光照图像增强方法，结果可以通过用户友好的网页界面生成。通过我们的平台，即使没有GPU的用户也可以在线评估不同方法对任何输入图像的结果，这加速了该研究领域的发展，并有助于创造新的研究。我们希望我们的综述能够提供新颖的见解和灵感，以促进对基于深度学习的LLIE的理解，促进对提出的开放问题的研究，并加快该研究领域的发展。
- en: 2 Deep Learning-Based LLIE
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基于深度学习的LLIE
- en: 2.1 Problem Definition
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题定义
- en: 'We first give a common formulation of the deep learning-based LLIE problem.
    For a low-light image $I\in\mathbb{R}^{W\times H\times 3}$ of width $W$ and height
    $H$, the process can be modeled as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先给出基于深度学习的低光照图像增强（LLIE）问题的常见公式。对于宽度为$W$、高度为$H$的低光照图像$I\in\mathbb{R}^{W\times
    H\times 3}$，该过程可以建模为：
- en: '|  | $\widehat{R}=\mathcal{F}(I;\theta),$ |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widehat{R}=\mathcal{F}(I;\theta),$ |  | (1) |'
- en: 'where $\widehat{R}\in\mathbb{R}^{W\times H\times 3}$ is the enhanced result
    and $\mathcal{F}$ represents the network with trainable parameters $\theta$. The
    purpose of deep learning is to find optimal network parameters $\widehat{\theta}$
    that minimizes the error:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\widehat{R}\in\mathbb{R}^{W\times H\times 3}$是增强结果，$\mathcal{F}$表示具有可训练参数$\theta$的网络。深度学习的目的是找到能够最小化误差的最佳网络参数$\widehat{\theta}$：
- en: '|  | $\widehat{\theta}=\operatorname*{argmin}_{\theta}\mathcal{L}(\widehat{R},R),$
    |  | (2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widehat{\theta}=\operatorname*{argmin}_{\theta}\mathcal{L}(\widehat{R},R),$
    |  | (2) |'
- en: 'where $R\in\mathbb{R}^{W\times H\times 3}$ is the ground truth, and the loss
    function $\mathcal{L}(\widehat{R},R)$ drives the optimization of network. Various
    loss functions such as supervised loss and unsupervised loss can be used. More
    details will be presented in Section [3](#S3 "3 Technical Review and Discussion
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $R\in\mathbb{R}^{W\times H\times 3}$ 是真实值，而损失函数 $\mathcal{L}(\widehat{R},R)$
    驱动网络的优化。可以使用各种损失函数，如监督损失和无监督损失。更多细节将在第[3](#S3 "3 Technical Review and Discussion
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")节中介绍。'
- en: 2.2 Learning Strategies
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 学习策略
- en: 'According to different learning strategies, we categorize existing LLIE methods
    into supervised learning, reinforcement learning, unsupervised learning, zero-shot
    learning, and semi-supervised learning. A statistic analysis from different perspectives
    is presented in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep
    Learning-Based LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"). In what follows, we review some representative methods of each strategy.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '根据不同的学习策略，我们将现有的LLIE方法分为监督学习、强化学习、无监督学习、零样本学习和半监督学习。从不同角度的统计分析见图[3](#S2.F3
    "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based LLIE ‣ Low-Light Image
    and Video Enhancement Using Deep Learning: A Survey")。接下来，我们将回顾每种策略的一些代表性方法。'
- en: Supervised Learning. For supervised learning-based LLIE methods, they are further
    divided into end-to-end, deep Retinex-based, and realistic data-driven methods.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习。对于基于监督学习的LLIE方法，它们进一步分为端到端、深度Retinex基础和现实数据驱动的方法。
- en: 'The first deep learning-based LLIE method LLNet [[1](#bib.bib1)] employs a
    variant of stacked-sparse denoising autoencoder [[56](#bib.bib56)] to brighten
    and denoise low-light images simultaneously. This pioneering work inspires the
    usage of end-to-end networks in LLIE. Lv et al. [[3](#bib.bib3)] propose an end-to-end
    multi-branch enhancement network (MBLLEN). The MBLLEN improves the performance
    of LLIE via extracting effective feature representations by a feature extraction
    module, an enhancement module, and a fusion module. The same authors [[15](#bib.bib15)]
    propose other three subnetworks including an Illumination-Net, a Fusion-Net, and
    a Restoration-Net to further improve the performance. Ren et al. [[12](#bib.bib12)]
    design a more complex end-to-end network that comprises an encoder-decoder network
    for image content enhancement and a recurrent neural network for image edge enhancement.
    Similar to Ren et al. [[12](#bib.bib12)], Zhu et al. [[16](#bib.bib16)] propose
    a method called EEMEFN. The EEMEFN consists of two stages: multi-exposure fusion
    and edge enhancement. A multi-exposure fusion network, TBEFN [[20](#bib.bib20)],
    is proposed for LLIE. The TBEFN estimates a transfer function in two branches,
    of which two enhancement results can be obtained. At last, a simple average scheme
    is employed to fuse these two images and further refine the result via a refinement
    unit. In addition, pyramid network (LPNet) [[18](#bib.bib18)], residual network
    [[19](#bib.bib19)], and Laplacian pyramid [[21](#bib.bib21)] (DSLR) are introduced
    into LLIE. These methods learn to effectively and efficiently integrate feature
    representations via commonly used end-to-end network structures for LLIE. Based
    on the observation that noise exhibits different levels of contrast in different
    frequency layers, Xu et al. [[57](#bib.bib57)] proposed a frequency-based decomposition-and-enhancement
    network. This network recovers image contents with noise suppression in the low-frequency
    layer while inferring the details in the high-frequency layer. Recently, a progressive-recursive
    low-light image enhancement network [[23](#bib.bib23)] is proposed, which uses
    a recursive unit to gradually enhance the input image. To solve temporal instability
    when handling low-light videos, Zhang et al. [[22](#bib.bib22)] propose to learn
    and infer motion field from a single image then enforce temporal consistency.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个基于深度学习的低光图像增强（LLIE）方法 LLNet [[1](#bib.bib1)] 使用了堆叠稀疏去噪自编码器的变体 [[56](#bib.bib56)]，同时进行图像亮度增强和去噪。这一开创性工作激发了端到端网络在LLIE中的应用。Lv
    等人 [[3](#bib.bib3)] 提出了一个端到端的多分支增强网络（MBLLEN）。MBLLEN 通过特征提取模块、增强模块和融合模块提取有效的特征表示，从而提高了
    LLIE 的性能。同样的作者 [[15](#bib.bib15)] 提出了其他三个子网络，包括 Illumination-Net、Fusion-Net 和
    Restoration-Net，以进一步提升性能。Ren 等人 [[12](#bib.bib12)] 设计了一个更复杂的端到端网络，包括一个用于图像内容增强的编码器-解码器网络和一个用于图像边缘增强的递归神经网络。类似于
    Ren 等人 [[12](#bib.bib12)]，Zhu 等人 [[16](#bib.bib16)] 提出了一个名为 EEMEFN 的方法。EEMEFN
    包含两个阶段：多曝光融合和边缘增强。为 LLIE 提出了一个多曝光融合网络 TBEFN [[20](#bib.bib20)]。TBEFN 在两个分支中估计一个传递函数，可以获得两个增强结果。最后，采用简单的平均方案来融合这两张图像，并通过一个细化单元进一步优化结果。此外，将金字塔网络（LPNet）
    [[18](#bib.bib18)]、残差网络 [[19](#bib.bib19)] 和拉普拉斯金字塔 [[21](#bib.bib21)]（DSLR）引入
    LLIE。这些方法通过常用的端到端网络结构有效地整合特征表示。基于噪声在不同频率层中表现出不同对比度的观察，Xu 等人 [[57](#bib.bib57)]
    提出了一个基于频率的分解和增强网络。该网络在低频层中恢复图像内容并抑制噪声，同时在高频层中推断细节。最近，提出了一个渐进递归低光图像增强网络 [[23](#bib.bib23)]，使用递归单元逐步增强输入图像。为了解决处理低光视频时的时间不稳定性，Zhang
    等人 [[22](#bib.bib22)] 提出了从单幅图像中学习和推断运动场，然后强制时间一致性。
- en: In comparison to directly learning an enhanced result in an end-to-end network,
    deep Retinex-based methods enjoy better enhancement performance in most cases
    owing to the physically explicable Retinex theory [[58](#bib.bib58), [59](#bib.bib59)].
    Deep Retinex-based methods usually separately enhance the illuminance component
    and the reflectance components via specialized subnetworks. A Retinex-Net [[4](#bib.bib4)]
    is proposed, which includes a Decom-Net that splits the input image into light-independent
    reflectance and structure-aware smooth illumination and an Enhance-Net that adjusts
    the illumination map for low-light enhancement. Recently, the Retinex-Net [[4](#bib.bib4)]
    is extended by adding new constraints and advanced network designs for better
    enhancement performance [[24](#bib.bib24)]. To reduce the computational burden,
    Li et al. [[5](#bib.bib5)] propose a lightweight LightenNet for weakly illuminated
    image enhancement, which only consists of four layers. The LightenNet takes a
    weakly illuminated image as the input and then estimates its illumination map.
    Based on the Retinex theory [[58](#bib.bib58), [59](#bib.bib59)], the enhanced
    image is obtained by dividing the input image by the illumination map. To accurately
    estimate the illumination map, Wang et al. [[60](#bib.bib60)] extract the global
    and local features to learn an image-to-illumination mapping by their proposed
    DeepUPE network. Zhang et al. [[11](#bib.bib11)] separately develop three subnetworks
    for layer decomposition, reflectance restoration, and illumination adjustment,
    called KinD. Furthermore, the authors alleviate the visual defects left in the
    results of KinD [[11](#bib.bib11)] by a multi-scale illumination attention module.
    The improved KinD is called KinD++ [[61](#bib.bib61)]. To solve the issue that
    the noise is omitted in the deep Retinex-based methods, Wang et al. [[10](#bib.bib10)]
    propose a progressive Retinex network, where an IM-Net estimates the illumination
    and a NM-Net estimates the noise level. These two subnetworks work in a progressive
    mechanism until obtaining stable results. Fan et al. [[14](#bib.bib14)] integrate
    semantic segmentation and Retinex model for further improving the enhancement
    performance in real cases. The core idea is to use semantic prior to guide the
    enhancement of both the illumination component and the reflectance component.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接在端到端网络中学习增强结果相比，基于深度 Retinex 的方法在大多数情况下由于物理上可解释的 Retinex 理论 [[58](#bib.bib58),
    [59](#bib.bib59)]，享有更好的增强性能。基于深度 Retinex 的方法通常通过专门的子网络分别增强光照成分和反射成分。提出了一种 Retinex-Net
    [[4](#bib.bib4)]，它包括一个 Decom-Net，用于将输入图像分为光照无关的反射成分和结构感知的平滑光照，以及一个 Enhance-Net，用于调整低光照增强的光照图。最近，通过添加新的约束和先进的网络设计来扩展
    Retinex-Net [[4](#bib.bib4)]，以获得更好的增强性能 [[24](#bib.bib24)]。为了减少计算负担，Li 等 [[5](#bib.bib5)]
    提出了一个轻量级的 LightenNet 用于弱光图像增强，该网络仅由四层组成。LightenNet 以弱光图像作为输入，然后估计其光照图。基于 Retinex
    理论 [[58](#bib.bib58), [59](#bib.bib59)]，通过将输入图像除以光照图来获得增强图像。为了准确估计光照图，Wang 等 [[60](#bib.bib60)]
    提出了 DeepUPE 网络，通过提取全局和局部特征来学习图像到光照的映射。Zhang 等 [[11](#bib.bib11)] 分别开发了三个子网络用于层分解、反射恢复和光照调整，称为
    KinD。此外，作者通过多尺度光照注意力模块减轻了 KinD [[11](#bib.bib11)] 结果中留下的视觉缺陷。改进后的 KinD 被称为 KinD++
    [[61](#bib.bib61)]。为了解决深度 Retinex 方法中噪声被忽略的问题，Wang 等 [[10](#bib.bib10)] 提出了一个渐进式
    Retinex 网络，其中 IM-Net 估计光照，NM-Net 估计噪声水平。这两个子网络以渐进机制工作，直到获得稳定结果。Fan 等 [[14](#bib.bib14)]
    将语义分割与 Retinex 模型结合起来，以进一步提升实际情况中的增强性能。核心思想是利用语义先验指导光照成分和反射成分的增强。
- en: Although some methods can achieve decent performance, they show poor generalization
    capability in real low-light cases due to the usage of synthetic training data.
    To solve this issue, some works attempt to generate more realistic training data
    or capture real data. Cai et al. [[6](#bib.bib6)] build a multi-exposure image
    dataset, where the low-contrast images of different exposure levels have their
    corresponding high-quality reference images. Each high-quality reference image
    is obtained by subjectively selecting the best output from 13 results enhanced
    by different methods. Moreover, a frequency decomposition network is trained on
    the built dataset and separately enhances the high-frequency layer and the low-frequency
    layer via a two-stage structure. Chen et al. [[2](#bib.bib2)] collect a real low-light
    image dataset (SID) and train the U-Net [[62](#bib.bib62)] to learn a mapping
    from low-light raw data to the corresponding long-exposure high-quality reference
    image. Further, Chen et al. [[8](#bib.bib8)] extend the SID dataset to low-light
    videos (DRV). The DRV contains static videos with the corresponding long-exposure
    ground truths. To ensure the generalization capability of processing the videos
    of dynamic scenes, a siamese network is proposed. To enhance the moving objects
    in the dark, Jiang and Zheng [[9](#bib.bib9)] design a co-axis optical system
    to capture temporally synchronized and spatially aligned low-light and well-lighted
    video pairs (SMOID). Unlike the DRV video dataset [[8](#bib.bib8)], the SMOID
    video dataset contains dynamic scenes. To learn the mapping from raw low-light
    video to well-lighted video, a 3D U-Net-based network is proposed. Considering
    the limitations of previous low-light video datasets such as DRV dataset [[8](#bib.bib8)]
    only containing statistic videos and SMOID dataset [[9](#bib.bib9)] only having
    179 video pairs, Triantafyllidou et al. [[17](#bib.bib17)] propose a low-light
    video synthesis pipeline, dubbed SIDGAN. The SIDGAN can produce dynamic video
    data (raw-to-RGB) by a semi-supervised dual CycleGAN with intermediate domain
    mapping. To train this pipeline, the real-world videos are collected from Vimeo-90K
    dataset [[63](#bib.bib63)]. The low-light raw video data and the corresponding
    long-exposure images are sampled from DRV dataset [[8](#bib.bib8)]. With the synthesized
    training data, this work adopts the same U-Net network as Chen et al. [[2](#bib.bib2)]
    for low-light video enhancement.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些方法能够达到不错的性能，但由于使用了合成训练数据，在真实的低光情况下它们表现出较差的泛化能力。为了解决这个问题，一些工作尝试生成更真实的训练数据或捕获真实数据。蔡等人[[6](#bib.bib6)]构建了一个多曝光图像数据集，其中不同曝光级别的低对比度图像有对应的高质量参考图像。每个高质量的参考图像是通过从13个不同方法增强的结果中主观选择最佳输出得到的。此外，通过一个两阶段结构，对构建的数据集训练了一个频率分解网络，分别增强高频层和低频层。陈等人[[2](#bib.bib2)]收集了一个真实的低光图像数据集（SID），并训练了U-Net[[62](#bib.bib62)]来学习从低光原始数据到相应的长曝光高质量参考图像的映射。此外，陈等人[[8](#bib.bib8)]将SID数据集扩展到低光视频（DRV）。DRV包含了与相应的长曝光真实图像相对应的静态视频。为了确保处理动态场景视频的泛化能力，他们提出了一个孪生网络。为了增强暗处的移动物体，蒋和郑[[9](#bib.bib9)]设计了一个共轴光学系统来捕捉时间上同步的、在空间上对齐的低光和高光视频对（SMOID）。与DRV视频数据集[[8](#bib.bib8)]不同，SMOID视频数据集包含了动态场景。为了学习从原始低光视频到高光视频的映射，提出了一个基于3D
    U-Net的网络。考虑到之前的低光视频数据集的限制，如DRV数据集[[8](#bib.bib8)]只包含静态视频，而SMOID数据集[[9](#bib.bib9)]只有179个视频对，Triantafyllidou等人[[17](#bib.bib17)]提出了一种低光视频合成管线，称为SIDGAN。SIDGAN可以通过一种半监督的双向CycleGAN和中间域映射产生动态视频数据（原始到RGB）。为了训练这个管线，他们从Vimeo-90K数据集[[63](#bib.bib63)]中收集了真实世界的视频。低光原始视频数据和相应的长曝光图像是从DRV数据集[[8](#bib.bib8)]中采样得到的。通过合成训练数据，该工作采用了与陈等人[[2](#bib.bib2)]相同的U-Net网络用于低光视频增强。
- en: Reinforcement Learning. Without paired training data, Yu et al. [[25](#bib.bib25)]
    learn to expose photos with reinforcement adversarial learning, named DeepExposure.
    Specifically, an input image is first segmented into sub-images according to exposures.
    For each sub-image, local exposure is learned by the policy network sequentially
    based on reinforcement learning. The reward evaluation function is approximated
    by adversarial learning. At last, each local exposure is employed to retouch the
    input, thus obtaining multiple retouched images under different exposures. The
    final result is achieved by fusing these images.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习。在没有配对训练数据的情况下，Yu等人[[25](#bib.bib25)] 通过强化对抗学习来曝光照片，命名为DeepExposure。具体来说，首先根据曝光将输入图像分割成子图像。对于每个子图像，策略网络根据强化学习依次学习局部曝光。奖励评估函数通过对抗学习来近似。最后，每个局部曝光被用来修饰输入图像，从而获得在不同曝光下的多个修饰图像。最终结果是通过融合这些图像实现的。
- en: Unsupervised Learning. Training a deep model on paired data may result in overfitting
    and limited generalization capability. To solve this issue, an unsupervised learning
    method named EnligthenGAN [[26](#bib.bib26)] is proposed. The EnlightenGAN adopts
    an attention-guided U-Net [[62](#bib.bib62)] as the generator and uses the global-local
    discriminators to ensure the enhanced results look like realistic normal-light
    images. In addition to global and local adversarial losses, the global and local
    self feature preserving losses are proposed to preserve the image content before
    and after the enhancement. This is a key point for the stable training of such
    a one-path Generative Adversarial Network (GAN) structure.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习。在配对数据上训练深度模型可能导致过拟合和有限的泛化能力。为了解决这个问题，提出了一种名为EnlightenGAN的无监督学习方法[[26](#bib.bib26)]。EnlightenGAN采用了一个注意力引导的U-Net
    [[62](#bib.bib62)] 作为生成器，并使用全球-局部判别器来确保增强结果看起来像真实的正常光照图像。除了全球和局部对抗损失，还提出了全球和局部自特征保留损失，以保留图像在增强前后的内容。这是这种单路径生成对抗网络（GAN）结构稳定训练的关键点。
- en: Zero-Shot Learning. The supervised learning, reinforcement learning, and unsupervised
    learning methods either have limited generalization capability or suffer from
    unstable training. To remedy these issues, zero-shot learning is proposed to learn
    the enhancement solely from the testing images. Note that the concept of zero-shot
    learning in the low-level vision tasks is used to emphasize that the method does
    not require paired or unpaired training data, which is different from its definition
    in high-level visual tasks. Zhang et al. [[27](#bib.bib27)] propose a zero-shot
    learning method, called ExCNet, for back-lit image restoration. A network is first
    used to estimate the S-curve that best fits the input image. Once the S-curve
    is estimated, the input image is separated into a base layer and a detail layer
    using the guided filter [[64](#bib.bib64)]. Then the base layer is adjusted by
    the estimated S-curve. Finally, the Weber contrast [[65](#bib.bib65)] is used
    to fuse the detailed layer and the adjusted base layer. To train the ExCNet, the
    authors formulate the loss function as a block-based energy minimization problem.
    Zhu et al. [[29](#bib.bib29)] propose a three-branch CNN, called RRDNet, for underexposed
    images restoration. The RRDNet decomposes an input image into illumination, reflectance,
    and noise via iteratively minimizing specially designed loss functions. To drive
    the zero-shot learning, a combination of Retinex reconstruction loss, texture
    enhancement loss, and illumination-guided noise estimation loss is proposed. Zhao
    et al. [[31](#bib.bib31)] perform Retinex decomposition via neural networks and
    then enhance the low-light image based on the Retinex model, called RetinexDIP.
    Inspired by Deep Image Prior (DIP) [[66](#bib.bib66)], RetinexDIP generates the
    reflectance component and illumination component of an input image by randomly
    sampled white noise, in which the component characteristics-related losses such
    as illumination smoothness are used for training. Liu et al. [[32](#bib.bib32)]
    propose a Retinex-inspired unrolling method for LLIE, in which the cooperative
    architecture search is used to discover lightweight prior architectures of basic
    blocks and non-reference losses are used to train the network. Different from
    the image reconstruction-based methods [[1](#bib.bib1), [3](#bib.bib3), [12](#bib.bib12),
    [21](#bib.bib21), [4](#bib.bib4), [11](#bib.bib11), [61](#bib.bib61)], a deep
    curve estimation network, Zero-DCE [[28](#bib.bib28)], is proposed. Zero-DCE formulates
    the light enhancement as a task of image-specific curve estimation, which takes
    a low-light image as input and produces high-order curves as its output. These
    curves are used for pixel-wise adjustment on the dynamic range of the input to
    obtain an enhanced image. Further, an accelerated and lightweight version is proposed,
    called Zero-DCE++ [[30](#bib.bib30)]. Such curve-based methods do not require
    any paired or unpaired data during training. They achieve zero-reference learning
    via a set of non-reference loss functions. Besides, unlike the image reconstruction-based
    methods that need high computational resources, the image-to-curve mapping only
    requires lightweight networks, thus achieving a fast inference speed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本学习。监督学习、强化学习和无监督学习方法要么具有有限的泛化能力，要么在训练过程中不稳定。为了解决这些问题，提出了零样本学习，旨在仅从测试图像中学习增强。需要注意的是，在低级视觉任务中，零样本学习的概念用于强调该方法不需要配对或未配对的训练数据，这与其在高级视觉任务中的定义不同。张等人[[27](#bib.bib27)]
    提出了一个名为 ExCNet 的零样本学习方法，用于背光图像恢复。首先使用网络估计与输入图像最匹配的 S 曲线。一旦估计出 S 曲线，就使用引导滤波器[[64](#bib.bib64)]
    将输入图像分离为基础层和细节层。然后，基础层通过估计的 S 曲线进行调整。最后，使用 Weber 对比度[[65](#bib.bib65)] 融合细节层和调整后的基础层。为了训练
    ExCNet，作者将损失函数形式化为基于块的能量最小化问题。朱等人[[29](#bib.bib29)] 提出了一个三分支 CNN，称为 RRDNet，用于低曝光图像恢复。RRDNet
    通过迭代最小化特别设计的损失函数将输入图像分解为照明、反射率和噪声。为了驱动零样本学习，提出了结合 Retinex 重建损失、纹理增强损失和照明引导噪声估计损失的方法。赵等人[[31](#bib.bib31)]
    通过神经网络执行 Retinex 分解，然后基于 Retinex 模型增强低光图像，称为 RetinexDIP。受到 Deep Image Prior (DIP)
    [[66](#bib.bib66)] 启发，RetinexDIP 通过随机采样的白噪声生成输入图像的反射成分和照明成分，其中如照明平滑度等与成分特征相关的损失用于训练。刘等人[[32](#bib.bib32)]
    提出了一个受 Retinex 启发的解卷积方法用于低光图像增强（LLIE），在该方法中使用合作架构搜索发现轻量级的基本块先验架构，并使用非参考损失来训练网络。与基于图像重建的方法[[1](#bib.bib1),
    [3](#bib.bib3), [12](#bib.bib12), [21](#bib.bib21), [4](#bib.bib4), [11](#bib.bib11),
    [61](#bib.bib61)] 不同，提出了一个深度曲线估计网络 Zero-DCE[[28](#bib.bib28)]。Zero-DCE 将光增强形式化为图像特定曲线估计任务，该任务以低光图像为输入，输出高阶曲线。这些曲线用于对输入图像的动态范围进行像素级调整，以获得增强图像。此外，还提出了加速和轻量化版本，称为
    Zero-DCE++[[30](#bib.bib30)]。这种基于曲线的方法在训练过程中不需要任何配对或未配对的数据。它们通过一组非参考损失函数实现零参考学习。此外，与需要高计算资源的图像重建方法不同，图像到曲线的映射仅需轻量级网络，从而实现了快速推断速度。
- en: '| ![Refer to caption](img/3c05d749696222c732238f19c01a4bb9.png) | ![Refer to
    caption](img/75f9b5236a6d545fb5dac913e04a5750.png) | ![Refer to caption](img/72a6454902d1ffce2c7d31a0902cd87b.png)
    | ![Refer to caption](img/058e15ee24ec5f9900f0b11d6dc6655c.png) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/3c05d749696222c732238f19c01a4bb9.png) | ![参见说明](img/75f9b5236a6d545fb5dac913e04a5750.png)
    | ![参见说明](img/72a6454902d1ffce2c7d31a0902cd87b.png) | ![参见说明](img/058e15ee24ec5f9900f0b11d6dc6655c.png)
    |'
- en: '| (a) learning strategy | (b) network structure | (c) Retinex model | (d) data
    format |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| (a) 学习策略 | (b) 网络结构 | (c) Retinex 模型 | (d) 数据格式 |'
- en: '| ![Refer to caption](img/ba9cd43b95793faf4b0191ad2cf826a0.png) | ![Refer to
    caption](img/7c51dae45c60efd86f25e0bebcc490a4.png) | ![Refer to caption](img/b1c7bcfb2a59658b7d471b434de64d14.png)
    | ![Refer to caption](img/3285a97f3d602476db3e4d1ff2e43ed8.png) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/ba9cd43b95793faf4b0191ad2cf826a0.png) | ![参见说明](img/7c51dae45c60efd86f25e0bebcc490a4.png)
    | ![参见说明](img/b1c7bcfb2a59658b7d471b434de64d14.png) | ![参见说明](img/3285a97f3d602476db3e4d1ff2e43ed8.png)
    |'
- en: '| (e) loss function | (f) training dataset | (g) testing dataset | (h) evaluation
    metric |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| (e) 损失函数 | (f) 训练数据集 | (g) 测试数据集 | (h) 评估指标 |'
- en: 'Figure 3: A statictic analysis of deep learning-based LLIE methods, including
    learning strategy, network characteristic, Retinex model, data format, loss function,
    training dataset, testing dataset, and evaluation metric. Best viewed by zooming
    in.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：基于深度学习的低光图像增强（LLIE）方法的统计分析，包括学习策略、网络特征、Retinex 模型、数据格式、损失函数、训练数据集、测试数据集和评估指标。最佳查看方式是放大。
- en: Semi-Supervised Learning. To combine the strengths of supervised learning and
    unsupervised learning, semi-supervised learning has been proposed in recent years.
    Yang et al. [[33](#bib.bib33)] propose a semi-supervised deep recursive band network
    (DRBN). The DRBN first recovers a linear band representation of an enhanced image
    under supervised learning, and then obtains an improved one by recomposing the
    given bands via a learnable linear transformation based on unsupervised adversarial
    learning. The DRBN is extended by introducing Long Short Term Memory (LSTM) networks
    and an image quality assessment network pre-trained on an aesthetic visual analysis
    dataset, which achieves better enhancement performance [[34](#bib.bib34)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习。为了结合监督学习和无监督学习的优点，近年来提出了半监督学习。Yang 等人 [[33](#bib.bib33)] 提出了半监督深度递归带网络（DRBN）。DRBN
    首先在监督学习下恢复增强图像的线性带表示，然后通过基于无监督对抗学习的可学习线性变换重新组合给定的带，从而获得改进的结果。通过引入长短期记忆（LSTM）网络和在美学视觉分析数据集上预训练的图像质量评估网络，DRBN
    得到了扩展，从而实现了更好的增强性能 [[34](#bib.bib34)]。
- en: 'Observing Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based
    LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(a),
    we can find that supervised learning is the mainstream among deep learning-based
    LLIE methods, of which the percentage reaches 73%. This is because supervised
    learning is relatively easy when paired training data such as LOL [[4](#bib.bib4)],
    SID [[2](#bib.bib2)] and diverse low-/normal-light image synthesis approaches
    are used. However, supervised learning-based methods suffer from some challenges:
    1) collecting a large-scale paired dataset that covers diverse real-world low-light
    conditions is difficult, 2) synthetic low-light images do not accurately represent
    real-world illuminance conditions such as spatially varying lighting and different
    levels of noise, and 3) training a deep model on paired data may result in limited
    generalization to real-world images of diverse illumination properties.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '观察图 [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based
    LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(a)，我们可以发现，监督学习是基于深度学习的
    LLIE 方法中的主流，其中比例达到 73%。这是因为使用配对训练数据集，如 LOL [[4](#bib.bib4)]、SID [[2](#bib.bib2)]
    和多样的低光/正常光图像合成方法时，监督学习相对简单。然而，基于监督学习的方法面临一些挑战：1) 收集覆盖多样真实低光条件的大规模配对数据集困难，2) 合成低光图像不能准确代表真实的光照条件，如空间变化的光照和不同水平的噪声，3)
    在配对数据上训练深度模型可能导致对多样化光照属性的真实世界图像的泛化能力有限。'
- en: 'Therefore, some methods adopt unsupervised learning, reinforcement learning,
    semi-supervised learning, and zero-shot learning to bypass the challenges in supervised
    learning. Although these methods achieve competing performance, they still suffer
    from some limitations: 1) for unsupervised learning/semi-supervised learning methods,
    how to implement stable training, avoid color deviations, and build the relations
    of cross-domain information challenges current methods, 2) for reinforcement learning
    methods, designing an effective reward mechanism and implementing efficient and
    stable training are intricate, and 3) for zero-shot learning methods, the design
    of non-reference losses is non-trivial when the color preservation, artifact removal,
    and gradient back-propagation should be taken into account.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一些方法采用无监督学习、强化学习、半监督学习和零样本学习来绕过监督学习中的挑战。虽然这些方法取得了竞争性能，但它们仍然存在一些限制：1) 对于无监督学习/半监督学习方法，如何实现稳定训练、避免颜色偏差和建立跨领域信息的关系挑战当前方法，2)
    对于强化学习方法，设计有效的奖励机制和实施高效稳定的训练是复杂的，3) 对于零样本学习方法，当需要考虑颜色保留、伪影去除和梯度反向传播时，非参考损失的设计并非简单。
- en: 'TABLE I: Summary of essential characteristics of representative deep learning-based
    methods. “Retinex” indicates whether the models are Retinex-based or not. “simulated”
    means the testing data are simulated by the same approach as the synthetic training
    data. “self-selected” stands for the real-world images selected by the authors.
    “#P”represents the number of trainable parameters. “-” means this item is not
    available or not indicated in the paper.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 代表性深度学习方法的基本特征总结。“Retinex”表示模型是否基于 Retinex。“模拟的”意味着测试数据由与合成训练数据相同的方法模拟。“自选”指作者选择的真实世界图像。“#P”代表可训练参数的数量。“-”表示该项目在论文中不可用或未指明。'
- en: '|  | Method | Learning | Network Structure | Loss Function | Training Data
    | Testing Data | Evaluation Metric | Format | Platform | Retinex |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 学习 | 网络结构 | 损失函数 | 训练数据 | 测试数据 | 评估指标 | 格式 | 平台 | Retinex |'
- en: '| 2017 | LLNet [[1](#bib.bib1)] | SL | SSDA | SRR loss |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | LLNet [[1](#bib.bib1)] | SL | SSDA | SRR 损失 |'
- en: '&#124; simulated by &#124;'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 由…模拟 &#124;'
- en: '&#124; Gamma Correction & &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 伽马校正 & &#124;'
- en: '&#124; Gaussian Noise &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高斯噪声 &#124;'
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟的 &#124;'
- en: '&#124; self-selected &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自选 &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '| RGB | Theano |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| RGB | Theano |  |'
- en: '| 2018 | LightenNet [[5](#bib.bib5)] | SL | four layers | $L_{2}$ loss |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | LightenNet [[5](#bib.bib5)] | SL | 四层 | $L_{2}$ 损失 |'
- en: '&#124; simulated by &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 由…模拟 &#124;'
- en: '&#124; random illumination values &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 随机光照值 &#124;'
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟的 &#124;'
- en: '&#124; self-selected &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自选 &#124;'
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR MAE &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR MAE &#124;'
- en: '&#124; SSIM &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM &#124;'
- en: '&#124; User Study &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户研究 &#124;'
- en: '| RGB |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| RGB |'
- en: '&#124; Caffe &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Caffe &#124;'
- en: '&#124; MATLAB &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MATLAB &#124;'
- en: '| ✓ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |'
- en: '|  | Retinex-Net [[4](#bib.bib4)] | SL | multi-scale network |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | Retinex-Net [[4](#bib.bib4)] | SL | 多尺度网络 |'
- en: '&#124; $L_{1}$ loss smoothness loss &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{1}$ 损失 平滑损失 &#124;'
- en: '&#124; invariable reflectance loss &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不变反射率损失 &#124;'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LOL &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL &#124;'
- en: '&#124; simulated by &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 由…模拟 &#124;'
- en: '&#124; adjusting histogram &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 调整直方图 &#124;'
- en: '| self-selected | - | RGB | TensorFlow | ✓ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 自选 | - | RGB | TensorFlow | ✓ |'
- en: '|  | MBLLEN [[3](#bib.bib3)] | SL | multi-branch fusion |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | MBLLEN [[3](#bib.bib3)] | SL | 多分支融合 |'
- en: '&#124; SSIM loss region loss &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM 损失 区域损失 &#124;'
- en: '&#124; perceptual loss &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated by &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 由…模拟 &#124;'
- en: '&#124; Gamma Correction & &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 伽马校正 & &#124;'
- en: '&#124; Poisson Noise &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 泊松噪声 &#124;'
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟的 &#124;'
- en: '&#124; self-selected &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自选 &#124;'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; AB VIF &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AB VIF &#124;'
- en: '&#124; LOE TOMI &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOE TOMI &#124;'
- en: '| RGB |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| RGB |'
- en: '&#124; TensorFlow &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TensorFlow &#124;'
- en: '|  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | SCIE [[6](#bib.bib6)] | SL | frequency decomposition |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | SCIE [[6](#bib.bib6)] | SL | 频率分解 |'
- en: '&#124; $L_{2}$ loss $L_{1}$ loss SSIM loss &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{2}$ 损失 $L_{1}$ 损失 SSIM 损失 &#124;'
- en: '| SCIE | SCIE |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| SCIE | SCIE |'
- en: '&#124; PSNR FSIM &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR FSIM &#124;'
- en: '&#124; Runtime FLOPs &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运行时 FLOPs &#124;'
- en: '| RGB |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| RGB |'
- en: '&#124; Caffe &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Caffe &#124;'
- en: '&#124; MATLAB &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MATLAB &#124;'
- en: '|  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | Chen et al. [[2](#bib.bib2)] | SL | U-Net | $L_{1}$ loss | SID | SID |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | Chen et al. [[2](#bib.bib2)] | SL | U-Net | $L_{1}$ 损失 | SID | SID |'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '| raw | TensorFlow |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | TensorFlow |  |'
- en: '|  | Deepexposure [[25](#bib.bib25)] | RL |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | Deepexposure [[25](#bib.bib25)] | RL |'
- en: '&#124; policy network &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 策略网络 &#124;'
- en: '&#124; GAN &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GAN &#124;'
- en: '|'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; deterministic policy gradient &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 确定性策略梯度 &#124;'
- en: '&#124; adversarial loss &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗损失 &#124;'
- en: '| MIT-Adobe FiveK | MIT-Adobe FiveK |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| MIT-Adobe FiveK | MIT-Adobe FiveK |'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '| raw | TensorFlow |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | TensorFlow |  |'
- en: '| 2019 | Chen et al. [[8](#bib.bib8)] | SL | siamese network |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Chen 等人 [[8](#bib.bib8)] | SL | Siamese 网络 |'
- en: '&#124; $L_{1}$ loss &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{1}$ 损失 &#124;'
- en: '&#124; self-consistency loss &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自一致性损失 &#124;'
- en: '| DRV | DRV |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| DRV | DRV |'
- en: '&#124; PSNR &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR &#124;'
- en: '&#124; SSIM &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM &#124;'
- en: '&#124; MAE &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MAE &#124;'
- en: '| raw | TensorFlow |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | TensorFlow |  |'
- en: '|  | Jiang and Zheng [[9](#bib.bib9)] | SL | 3D U-Net | $L_{1}$ loss | SMOID
    | SMOID |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | Jiang 和 Zheng [[9](#bib.bib9)] | SL | 3D U-Net | $L_{1}$ 损失 | SMOID |
    SMOID |'
- en: '&#124; PSNR SSIM MSE &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM MSE &#124;'
- en: '| raw | TensorFlow |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | TensorFlow |  |'
- en: '|  | DeepUPE [[60](#bib.bib60)] | SL | illumination map |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | DeepUPE [[60](#bib.bib60)] | SL | 照明图 |'
- en: '&#124; $L_{1}$ loss color loss &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{1}$ 损失 颜色损失 &#124;'
- en: '&#124; smoothness loss &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平滑性损失 &#124;'
- en: '| retouched image pairs | MIT-Adobe FiveK |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 修复图像对 | MIT-Adobe FiveK |'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; User Study &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户研究 &#124;'
- en: '| RGB | TensorFlow | ✓ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| RGB | TensorFlow | ✓ |'
- en: '|  | KinD [[11](#bib.bib11)] | SL |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | KinD [[11](#bib.bib11)] | SL |'
- en: '&#124; three subnetworks &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三个子网络 &#124;'
- en: '&#124; U-Net &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; U-Net &#124;'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; reflectance similarity loss &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反射相似性损失 &#124;'
- en: '&#124; illumination smoothness loss &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 照明平滑性损失 &#124;'
- en: '&#124; mutual consistency loss &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互一致性损失 &#124;'
- en: '&#124; $L_{1}$ loss $L_{2}$ loss SSIM loss &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{1}$ 损失 $L_{2}$ 损失 SSIM 损失 &#124;'
- en: '&#124; texture similarity loss &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 纹理相似性损失 &#124;'
- en: '&#124; illumination adjustment loss &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 照明调整损失 &#124;'
- en: '| LOL |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| LOL |'
- en: '&#124; LOL LIME &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL LIME &#124;'
- en: '&#124; NPE MEF &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NPE MEF &#124;'
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; LOE NIQE &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOE NIQE &#124;'
- en: '| RGB | TensorFlow | ✓ |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| RGB | TensorFlow | ✓ |'
- en: '|  | Wang et al. [[10](#bib.bib10)] | SL |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | Wang 等人 [[10](#bib.bib10)] | SL |'
- en: '&#124; two subnetworks &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两个子网络 &#124;'
- en: '&#124; pointwise Conv &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 点对点卷积 &#124;'
- en: '| $L_{1}$ loss |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| $L_{1}$ 损失 |'
- en: '&#124; simulated by &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟的 &#124;'
- en: '&#124; camera imaging model &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相机成像模型 &#124;'
- en: '|'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; IP100 FNF38 &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; IP100 FNF38 &#124;'
- en: '&#124; MPI LOL NPE &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPI LOL NPE &#124;'
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; NIQE &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIQE &#124;'
- en: '| RGB | Caffe | ✓ |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| RGB | Caffe | ✓ |'
- en: '|  | Ren et al. [[12](#bib.bib12)] | SL |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | Ren 等人 [[12](#bib.bib12)] | SL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类 U-Net 网络 &#124;'
- en: '&#124; RNN dilated Conv &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RNN 膨胀卷积 &#124;'
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $L_{2}$ loss perceptual loss &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{2}$ 损失 感知损失 &#124;'
- en: '&#124; adversarial loss &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗损失 &#124;'
- en: '|'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MIT-Adobe FiveK &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MIT-Adobe FiveK &#124;'
- en: '&#124; with Gamma correction &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带伽马校正 &#124;'
- en: '&#124; & Gaussion noise &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; & 高斯噪声 &#124;'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟的 &#124;'
- en: '&#124; self-selected DPED &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自选 DPED &#124;'
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; Runtime &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运行时 &#124;'
- en: '| RGB | Caffe |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| RGB | Caffe |  |'
- en: '|  | EnlightenGAN [[26](#bib.bib26)] | UL | U-Net like network |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | EnlightenGAN [[26](#bib.bib26)] | UL | 类 U-Net 网络 |'
- en: '&#124; adversarial loss &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗损失 &#124;'
- en: '&#124; self feature preserving loss &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自特征保持损失 &#124;'
- en: '| unpaired real images |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 无配对的真实图像 |'
- en: '&#124; NPE LIME &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NPE LIME &#124;'
- en: '&#124; MEF DICM &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MEF DICM &#124;'
- en: '&#124; VV BBD-100K &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VV BBD-100K &#124;'
- en: '&#124; ExDARK &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ExDARK &#124;'
- en: '|'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; User Study NIQE &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户研究 NIQE &#124;'
- en: '&#124; Classification &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '| RGB | PyTorch |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch |  |'
- en: '|  | ExCNet. [[27](#bib.bib27)] | ZSL |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | ExCNet. [[27](#bib.bib27)] | ZSL |'
- en: '&#124; fully connected layers &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 全连接层 &#124;'
- en: '| energy minimization loss | real images | $IE_{ps}D$ |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 能量最小化损失 | 真实图像 | $IE_{ps}D$ |'
- en: '&#124; User Study &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户研究 &#124;'
- en: '&#124; CDIQA LOD &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CDIQA LOD &#124;'
- en: '| RGB | PyTorch |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch |  |'
- en: '| 2020 | Zero-DCE [[28](#bib.bib28)] | ZSL | U-Net like network |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Zero-DCE [[28](#bib.bib28)] | ZSL | 类 U-Net 网络 |'
- en: '&#124; spatial consistency loss &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空间一致性损失 &#124;'
- en: '&#124; exposure control loss &#124;'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 曝光控制损失 &#124;'
- en: '&#124; color constancy loss &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 颜色恒常性损失 &#124;'
- en: '&#124; illumination smoothness loss &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 照明平滑性损失 &#124;'
- en: '| SICE |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| SICE |'
- en: '&#124; SICE NPE &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SICE NPE &#124;'
- en: '&#124; LIME MEF &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LIME MEF &#124;'
- en: '&#124; DICM VV &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DICM VV &#124;'
- en: '&#124; DARK FACE &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DARK FACE &#124;'
- en: '|'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; User Study PI &#124;'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户研究 PI &#124;'
- en: '&#124; PNSR SSIM &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PNSR SSIM &#124;'
- en: '&#124; MAE Runtime &#124;'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MAE 运行时 &#124;'
- en: '&#124; Face detection &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人脸检测 &#124;'
- en: '| RGB | PyTorch |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch |  |'
- en: '|  | DRBN [[33](#bib.bib33)] | SSL | recursive network |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | DRBN [[33](#bib.bib33)] | SSL | 递归网络 |'
- en: '&#124; SSIM loss perceptual loss &#124;'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM 损失 感知损失 &#124;'
- en: '&#124; adversarial loss &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对抗损失 &#124;'
- en: '|'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LOL &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL &#124;'
- en: '&#124; images selected by MOS &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 由 MOS 选择的图像 &#124;'
- en: '| LOL |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| LOL |'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; SSIM-GC &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM-GC &#124;'
- en: '| RGB | PyTorch |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch |  |'
- en: '|  | Lv et al. [[15](#bib.bib15)] | SL | U-Net like network |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | Lv 等人 [[15](#bib.bib15)] | SL | 类 U-Net 网络 |'
- en: '&#124; Huber loss &#124;'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Huber 损失 &#124;'
- en: '&#124; SSIM loss &#124;'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM 损失 &#124;'
- en: '&#124; perceptual loss &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '&#124; illumination smoothness loss &#124;'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 光照平滑损失 &#124;'
- en: '|'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated by a &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过一个 &#124;'
- en: '&#124; retouching module &#124;'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 修复模块 &#124;'
- en: '|'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LOL SICE &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL SICE &#124;'
- en: '&#124; DeepUPE &#124;'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DeepUPE &#124;'
- en: '|'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; User Study PSNR &#124;'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户研究 PSNR &#124;'
- en: '&#124; SSIM VIF &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM VIF &#124;'
- en: '&#124; LOE NIQE &#124;'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOE NIQE &#124;'
- en: '&#124; #P Runtime &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; #P 运行时 &#124;'
- en: '&#124; Face detection &#124;'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人脸检测 &#124;'
- en: '| RGB |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| RGB |'
- en: '&#124; TensorFlow &#124;'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TensorFlow &#124;'
- en: '| ✓ |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |'
- en: '|  | Fan et al. [[14](#bib.bib14)] | SL |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | Fan 等人 [[14](#bib.bib14)] | SL |'
- en: '&#124; four subnetworks &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 四个子网络 &#124;'
- en: '&#124; U-Net like network &#124;'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类 U-Net 网络 &#124;'
- en: '&#124; feature modulation &#124;'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征调制 &#124;'
- en: '|'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; mutual smoothness loss &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互相平滑损失 &#124;'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 &#124;'
- en: '&#124; illumination smoothness loss &#124;'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 光照平滑损失 &#124;'
- en: '&#124; cross entropy loss &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交叉熵损失 &#124;'
- en: '&#124; consistency loss &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一致性损失 &#124;'
- en: '&#124; SSIM loss &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM 损失 &#124;'
- en: '&#124; gradient loss &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 梯度损失 &#124;'
- en: '&#124; ratio learning loss &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 比率学习损失 &#124;'
- en: '|'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated by &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过 &#124;'
- en: '&#124; illumination adjustment, &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 光照调整， &#124;'
- en: '&#124; slight color distortion, &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轻微的颜色失真， &#124;'
- en: '&#124; and noise simulation &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和噪声模拟 &#124;'
- en: '|'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟的 &#124;'
- en: '&#124; self-selected &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自选 &#124;'
- en: '|'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; NIQE &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIQE &#124;'
- en: '| RGB | - | ✓ |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| RGB | - | ✓ |'
- en: '|  | Xu et al. [[57](#bib.bib57)] | SL |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | Xu 等人 [[57](#bib.bib57)] | SL |'
- en: '&#124; frequency decomposition &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 频率分解 &#124;'
- en: '&#124; U-Net like network &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类 U-Net 网络 &#124;'
- en: '|'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $L_{2}$ loss &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{2}$ 损失 &#124;'
- en: '&#124; perceptual loss &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '| SID in RGB |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| RGB 中的 SID |'
- en: '&#124; SID in RGB &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RGB 中的 SID &#124;'
- en: '&#124; self-selected &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自选 &#124;'
- en: '|'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '| RGB | PyTorch |  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch |  |'
- en: '|  | EEMEFN [[16](#bib.bib16)] | SL |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | EEMEFN [[16](#bib.bib16)] | SL |'
- en: '&#124; U-Net like network &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类 U-Net 网络 &#124;'
- en: '&#124; edge detection network &#124;'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 边缘检测网络 &#124;'
- en: '|'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $L_{1}$ loss &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{1}$ 损失 &#124;'
- en: '&#124; weighted cross-entropy loss &#124;'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 加权交叉熵损失 &#124;'
- en: '| SID | SID |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| SID | SID |'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '| raw |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 原始 |'
- en: '&#124; TensorFlow &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TensorFlow &#124;'
- en: '&#124; PaddlePaddle &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PaddlePaddle &#124;'
- en: '|  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | DLN [[19](#bib.bib19)] | SL |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | DLN [[19](#bib.bib19)] | SL |'
- en: '&#124; residual learning &#124;'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 残差学习 &#124;'
- en: '&#124; interactive factor &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互动因子 &#124;'
- en: '&#124; back projection network &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反向投影网络 &#124;'
- en: '|'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SSIM loss &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM 损失 &#124;'
- en: '&#124; total variation loss &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总变差损失 &#124;'
- en: '|'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated by &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过 &#124;'
- en: '&#124; illumination adjustment, &#124;'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 光照调整， &#124;'
- en: '&#124; slight color distortion, &#124;'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轻微的颜色失真， &#124;'
- en: '&#124; and noise simulation &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和噪声模拟 &#124;'
- en: '|'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟的 &#124;'
- en: '&#124; LOL &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL &#124;'
- en: '|'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; User Study PSNR &#124;'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户研究 PSNR &#124;'
- en: '&#124; SSIM NIQE &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM NIQE &#124;'
- en: '| RGB | PyTorch |  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch |  |'
- en: '|  | LPNet [[18](#bib.bib18)] | SL |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  | LPNet [[18](#bib.bib18)] | SL |'
- en: '&#124; pyramid network &#124;'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 金字塔网络 &#124;'
- en: '|'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $L_{1}$ loss &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{1}$ 损失 &#124;'
- en: '&#124; perceptual loss &#124;'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '&#124; luminance loss &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 亮度损失 &#124;'
- en: '|'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LOL SID in RGB &#124;'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL RGB 中的 SID &#124;'
- en: '&#124; MIT-Adobe FiveK &#124;'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MIT-Adobe FiveK &#124;'
- en: '|'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LOL SID in RGB &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL RGB 中的 SID &#124;'
- en: '&#124; MIT-Adobe FiveK &#124;'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MIT-Adobe FiveK &#124;'
- en: '&#124; MEF NPE DICM VV &#124;'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MEF NPE DICM VV &#124;'
- en: '|'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; NIQE #P &#124;'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIQE #P &#124;'
- en: '&#124; FLOPs Runtime &#124;'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FLOPs 运行时 &#124;'
- en: '| RGB | PyTorch |  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch |  |'
- en: '|  | SIDGAN [[17](#bib.bib17)] | SL | U-Net | CycleGAN loss | SIDGAN | SIDGAN
    |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  | SIDGAN [[17](#bib.bib17)] | SL | U-Net | CycleGAN 损失 | SIDGAN | SIDGAN
    |'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; TPSNR TSSIM ATWE &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TPSNR TSSIM ATWE &#124;'
- en: '| raw |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 原始 |'
- en: '&#124; TensorFlow &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TensorFlow &#124;'
- en: '|  |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | RRDNet [[29](#bib.bib29)] | ZSL | three subnetworks |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | RRDNet [[29](#bib.bib29)] | ZSL | 三个子网络 |'
- en: '&#124; retinex reconstruction loss &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Retinex 重建损失 &#124;'
- en: '&#124; texture enhancement loss &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 纹理增强损失 &#124;'
- en: '&#124; noise estimation loss &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 噪声估计损失 &#124;'
- en: '| - |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| - |'
- en: '&#124; NPE LIME &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NPE LIME &#124;'
- en: '&#124; MEF DICM &#124;'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MEF DICM &#124;'
- en: '|'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; NIQE CPCQI &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIQE CPCQI &#124;'
- en: '| RGB | PyTorch | ✓ |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch | ✓ |'
- en: '|  | TBEFN [[20](#bib.bib20)] | SL |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | TBEFN [[20](#bib.bib20)] | SL |'
- en: '&#124; three stages &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 三个阶段 &#124;'
- en: '&#124; U-Net like network &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类似 U-Net 的网络 &#124;'
- en: '|'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SSIM loss &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM 损失 &#124;'
- en: '&#124; perceptual loss &#124;'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '&#124; smoothness loss &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平滑损失 &#124;'
- en: '|'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SCIE &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SCIE &#124;'
- en: '&#124; LOL &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL &#124;'
- en: '|'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SCIE LOL &#124;'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SCIE LOL &#124;'
- en: '&#124; DICM MEF &#124;'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DICM MEF &#124;'
- en: '&#124; NPE VV &#124;'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NPE VV &#124;'
- en: '|'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; NIQE Runtime &#124;'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIQE 运行时 &#124;'
- en: '&#124; #P FLOPs &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; #P FLOPs &#124;'
- en: '| RGB | TensorFlow | ✓ |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| RGB | TensorFlow | ✓ |'
- en: '|  | DSLR [[21](#bib.bib21)] | SL |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  | DSLR [[21](#bib.bib21)] | SL |'
- en: '&#124; Laplacian pyramid &#124;'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 拉普拉斯金字塔 &#124;'
- en: '&#124; U-Net like network &#124;'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类似 U-Net 的网络 &#124;'
- en: '|'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $L_{2}$ loss &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{2}$ 损失 &#124;'
- en: '&#124; Laplacian loss &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 拉普拉斯损失 &#124;'
- en: '&#124; color loss &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 颜色损失 &#124;'
- en: '| MIT-Adobe FiveK |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| MIT-Adobe FiveK |'
- en: '&#124; MIT-Adobe FiveK &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MIT-Adobe FiveK &#124;'
- en: '&#124; self-selected &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自选 &#124;'
- en: '|'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; NIQMC NIQE &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIQMC NIQE &#124;'
- en: '&#124; BTMQI CaHDC &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BTMQI CaHDC &#124;'
- en: '| RGB | PyTorch |  |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch |  |'
- en: '| 2021 | RUAS [[32](#bib.bib32)] | ZSL | neural architecture search |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | RUAS [[32](#bib.bib32)] | ZSL | 神经架构搜索 |'
- en: '&#124; cooperative loss &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 协作损失 &#124;'
- en: '&#124; similar loss &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相似损失 &#124;'
- en: '&#124; total variation loss &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总变差损失 &#124;'
- en: '|'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LOL &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL &#124;'
- en: '&#124; MIT-Adobe FiveK &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MIT-Adobe FiveK &#124;'
- en: '|'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LOL &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL &#124;'
- en: '&#124; MIT-Adobe FiveK &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MIT-Adobe FiveK &#124;'
- en: '|'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; Runtime #P FLOPs &#124;'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运行时 #P FLOPs &#124;'
- en: '| RGB | PyTorch | ✓ |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch | ✓ |'
- en: '|  | Zhang et al. [[22](#bib.bib22)] | SL | U-Net |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | Zhang et al. [[22](#bib.bib22)] | SL | U-Net |'
- en: '&#124; $L_{1}$ loss &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{1}$ 损失 &#124;'
- en: '&#124; consistency loss &#124;'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一致性损失 &#124;'
- en: '|'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated by illumination &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过照明模拟 &#124;'
- en: '&#124; adjustmentand noise simulation &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 调整和噪声模拟 &#124;'
- en: '|'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; simulated &#124;'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟 &#124;'
- en: '&#124; self-selected &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自选 &#124;'
- en: '|'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; User Study PSNR &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户研究 PSNR &#124;'
- en: '&#124; SSIM AB &#124;'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM AB &#124;'
- en: '&#124; MABD WE &#124;'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MABD WE &#124;'
- en: '| RGB |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| RGB |'
- en: '&#124; PyTorch &#124;'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PyTorch &#124;'
- en: '|  |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | Zero-DCE++ [[30](#bib.bib30)] | ZSL | U-Net like network |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|  | Zero-DCE++ [[30](#bib.bib30)] | ZSL | 类似 U-Net 的网络 |'
- en: '&#124; spatial consistency loss &#124;'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空间一致性损失 &#124;'
- en: '&#124; exposure control loss &#124;'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 曝光控制损失 &#124;'
- en: '&#124; color constancy loss &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 颜色恒常性损失 &#124;'
- en: '&#124; illumination smoothness loss &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 照明平滑损失 &#124;'
- en: '| SICE |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| SICE |'
- en: '&#124; SICE NPE &#124;'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SICE NPE &#124;'
- en: '&#124; LIME MEF &#124;'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LIME MEF &#124;'
- en: '&#124; DICM VV &#124;'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DICM VV &#124;'
- en: '&#124; DARK FACE &#124;'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DARK FACE &#124;'
- en: '|'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; User Study PI &#124;'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户研究 PI &#124;'
- en: '&#124; PNSR SSIM #P &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PNSR SSIM #P &#124;'
- en: '&#124; MAE Runtime &#124;'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MAE 运行时 &#124;'
- en: '&#124; Face detection FLOPs &#124;'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人脸检测 FLOPs &#124;'
- en: '| RGB | PyTorch |  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch |  |'
- en: '|  | DRBN [[34](#bib.bib34)] | SSL | recursive network |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  | DRBN [[34](#bib.bib34)] | SSL | 递归网络 |'
- en: '&#124; perceptual loss &#124;'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感知损失 &#124;'
- en: '&#124; detail loss quality loss &#124;'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 细节损失质量损失 &#124;'
- en: '| LOL | LOL |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| LOL | LOL |'
- en: '&#124; PSNR SSIM &#124;'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PSNR SSIM &#124;'
- en: '&#124; SSIM-GC &#124;'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM-GC &#124;'
- en: '| RGB | PyTorch |  |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch |  |'
- en: '|  | Retinex-Net [[24](#bib.bib24)] | SL | three subnetworks |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|  | Retinex-Net [[24](#bib.bib24)] | SL | 三个子网络 |'
- en: '&#124; $L_{1}$ loss $L_{2}$ loss &#124;'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $L_{1}$ 损失 $L_{2}$ 损失 &#124;'
- en: '&#124; SSIM loss &#124;'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SSIM 损失 &#124;'
- en: '&#124; total variation loss &#124;'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总变差损失 &#124;'
- en: '|'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LOL &#124;'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL &#124;'
- en: '&#124; simulated by adjusting histogram &#124;'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过调整直方图模拟 &#124;'
- en: '|'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LOL simulated &#124;'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL 模拟 &#124;'
- en: '&#124; NPE DICM VV &#124;'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NPE DICM VV &#124;'
- en: '|'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PNSR SSIM &#124;'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PNSR SSIM &#124;'
- en: '&#124; UQI OSS User Study &#124;'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UQI OSS 用户研究 &#124;'
- en: '| RGB | PyTorch | ✓ |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch | ✓ |'
- en: '|  | RetinexDIP[[31](#bib.bib31)] | ZSL | encoder-decoder networks |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '|  | RetinexDIP [[31](#bib.bib31)] | ZSL | 编码器-解码器网络 |'
- en: '&#124; reconstruction loss &#124;'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建损失 &#124;'
- en: '&#124; illumination-consistency loss &#124;'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 照明一致性损失 &#124;'
- en: '&#124; reflectnce loss &#124;'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反射损失 &#124;'
- en: '&#124; illumination smoothness loss &#124;'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 照明平滑性损失 &#124;'
- en: '| - |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| - |'
- en: '&#124; DICM, ExDark &#124;'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DICM, ExDark &#124;'
- en: '&#124; Fusion LIME &#124;'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fusion LIME &#124;'
- en: '&#124; NASA NPE VV &#124;'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NASA NPE VV &#124;'
- en: '|'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; NIQE &#124;'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIQE &#124;'
- en: '&#124; NIQMC CPCQI &#124;'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NIQMC CPCQI &#124;'
- en: '| RGB | PyTorch | ✓ |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch | ✓ |'
- en: '|  | PRIEN [[23](#bib.bib23)] | SL | recursive network | SSIM loss |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '|  | PRIEN [[23](#bib.bib23)] | SL | 递归网络 | SSIM损失 |'
- en: '&#124; MEF LOL &#124;'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MEF LOL &#124;'
- en: '&#124; simulated by adjusting histogram &#124;'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过调整直方图进行模拟 &#124;'
- en: '|'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LOL LIME &#124;'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOL LIME &#124;'
- en: '&#124; NPE MEF VV &#124;'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NPE MEF VV &#124;'
- en: '|'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PNSR SSIM &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PNSR SSIM &#124;'
- en: '&#124; LOE TMQI &#124;'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LOE TMQI &#124;'
- en: '| RGB | PyTorch |  |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| RGB | PyTorch |  |'
- en: 3 Technical Review and Discussion
  id: totrans-481
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 技术评审与讨论
- en: 'In this section, we first summarize the representative deep learning-based
    LLIE methods in Table [I](#S2.T1 "TABLE I ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based
    LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey"),
    then analyze and discuss their technical characteristics.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们首先总结了基于深度学习的LLIE方法的代表性方法，如表[1](#S2.T1 "TABLE I ‣ 2.2 Learning Strategies
    ‣ 2 Deep Learning-Based LLIE ‣ Low-Light Image and Video Enhancement Using Deep
    Learning: A Survey")所示，然后分析和讨论它们的技术特点。'
- en: 3.1 Network Structure
  id: totrans-483
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 网络结构
- en: 'Diverse network structures and designs have been used in the existing models,
    spanning from the basic U-Net, pyramid network, multi-stage network to frequency
    decomposition network. After analyzing Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning
    Strategies ‣ 2 Deep Learning-Based LLIE ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey")(b), it can be observed that the U-Net and U-Net-like
    networks are mainly adopted network structures in LLIE. This is because U-Net
    can effectively integrate multi-scale features and employ both low-level and high-level
    features. Such characteristics are essential for achieving satisfactory low-light
    enhancement.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '现有模型中使用了多种网络结构和设计，涵盖了基本的U-Net、金字塔网络、多阶段网络以及频域分解网络。分析图[3](#S2.F3 "Figure 3 ‣
    2.2 Learning Strategies ‣ 2 Deep Learning-Based LLIE ‣ Low-Light Image and Video
    Enhancement Using Deep Learning: A Survey")(b)后可以观察到，U-Net和U-Net-like网络是LLIE中主要采用的网络结构。这是因为U-Net可以有效整合多尺度特征，并利用低层次和高层次特征。这些特性对于实现满意的低光照增强至关重要。'
- en: 'Nevertheless, some key issues may be ignored in the current LLIE network structures:
    1) after going through several convolutional layers, the gradients of an extremely
    low light image may vanish during the gradient back-propagation due to its small
    pixel values. This would degrade the enhancement performance and affect the convergence
    of network training, 2) the skip-connections used in the U-Net-like networks might
    introduce noise and redundant features into the final results. How to effectively
    filter out the noise and integrate both low-level and high-level features should
    be carefully considered, and 3) although some designs and components are proposed
    for LLIE, most of them are borrowed or modified from related low-level visual
    tasks. The characteristics of low-light data should be considered when designing
    the network structures.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，当前LLIE网络结构中可能忽视了一些关键问题：1) 在经过若干卷积层后，极低光照图像的梯度可能在梯度反向传播过程中消失，因为其像素值很小。这会降低增强性能并影响网络训练的收敛性。2)
    U-Net-like网络中使用的跳跃连接可能会在最终结果中引入噪声和冗余特征。如何有效过滤噪声并整合低层次和高层次特征应仔细考虑。3) 尽管为LLIE提出了一些设计和组件，但大多数都是从相关低层次视觉任务中借用或修改而来的。在设计网络结构时应考虑低光照数据的特性。
- en: 3.2 Combination of Deep Model and Retinex Theory
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 深度模型与Retinex理论的结合
- en: 'As presented in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep
    Learning-Based LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey")(c), almost 1/3 of methods combine the designs of deep networks with
    the Retinex theory, e.g., designing different subnetworks to estimate the components
    of the Retinex model and estimating the illumination map to guide the learning
    of networks. Despite such a combination can bridge deep learning-based and model-based
    methods, their respective weaknesses may be introduced into the final models:
    1) the ideal assumption that the reflectance is the final enhanced result used
    in Retinex-based LLIE methods would still affect the final results, and 2) the
    risk of overfitting in deep networks still exists despite the use of Retinex theory.
    How to cream off the best and filter out the impurities should be carefully considered
    when researchers combine deep learning with the Retinex theory.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based LLIE
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(c) 所示，几乎
    1/3 的方法将深度网络设计与 Retinex 理论结合，例如，设计不同的子网络以估计 Retinex 模型的组件，并估计照明图以指导网络的学习。尽管这种结合可以弥合基于深度学习和基于模型的方法，但它们各自的弱点可能会被引入到最终模型中：1)
    在 Retinex 基于 LLIE 方法中，反射率是最终增强结果的理想假设仍会影响最终结果；2) 尽管使用了 Retinex 理论，深度网络中的过拟合风险仍然存在。当研究人员将深度学习与
    Retinex 理论结合时，应仔细考虑如何筛选出最佳部分并过滤掉杂质。'
- en: 3.3 Data Format
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据格式
- en: 'As shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep
    Learning-Based LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey")(d), RGB data format dominates most methods as it is commonly found
    as the final imagery form produced by smartphone cameras, Go-Pro cameras, and
    drone cameras. Although raw data are limited to specific sensors such as those
    based on Bayer patterns, the data cover wider color gamut and higher dynamic range.
    Hence, deep models trained on raw data usually recover clear details and high
    contrast, obtain vivid color, reduce the effects of noises and artifacts, and
    improve the brightness of extremely low-light images. In future research, a smooth
    transformation from raw data of different patterns to RGB format would have the
    potentials to combine the convenience of RGB data and the advantage of high-quality
    enhancement of raw data for LLIE.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based LLIE
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(d) 所示，RGB
    数据格式主导了大多数方法，因为它通常是智能手机相机、Go-Pro 相机和无人机相机生成的最终图像格式。虽然原始数据仅限于特定传感器，如基于 Bayer 模式的传感器，但这些数据涵盖了更广的色域和更高的动态范围。因此，基于原始数据训练的深度模型通常能恢复清晰的细节和高对比度，获得生动的颜色，减少噪声和伪影的影响，并改善极低光照图像的亮度。在未来的研究中，将不同模式的原始数据平滑地转换为
    RGB 格式，将有可能结合 RGB 数据的便利性和原始数据高质量增强的优势，用于低光照图像增强。'
- en: 3.4 Loss Function
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 损失函数
- en: 'In Figure[3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based
    LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(e),
    the commonly adopted loss functions in LLIE models include reconstruction loss
    ($L_{1}$, $L_{2}$, SSIM), perceptual loss, and smoothness loss. Besides, according
    to different demands and formulations, color loss, exposure loss, adversarial
    loss, etc are also adopted. We detail representative loss functions as follows.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based LLIE
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(e) 中，LLIE
    模型中常用的损失函数包括重建损失 ($L_{1}$, $L_{2}$, SSIM)、感知损失和光滑损失。此外，根据不同的需求和公式，还会采用颜色损失、曝光损失、对抗损失等。我们将详细介绍具有代表性的损失函数。'
- en: Reconstruction Loss. Different reconstruction losses have their advantages and
    disadvantages. $L_{2}$ loss tends to penalize larger errors, but is tolerant to
    small errors. $L_{1}$ loss preserves colors and luminance well since an error
    is weighted equally regardless of the local structure. SSIM loss preserves the
    structure and texture well. Refer to this research paper [[67](#bib.bib67)] for
    detailed analysis.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 重建损失。不同的重建损失各有优缺点。$L_{2}$ 损失倾向于惩罚较大的错误，但对小错误较为宽容。$L_{1}$ 损失由于错误的权重与局部结构无关，因此能较好地保留颜色和亮度。SSIM
    损失则能很好地保留结构和纹理。详细分析请参见这篇研究论文[[67](#bib.bib67)]。
- en: Perceptual Loss. Perceptual loss [[68](#bib.bib68)], particularly the feature
    reconstruction loss, is proposed to constrain the results similar to the ground
    truth in the feature space. The loss improves the visual quality of results. It
    is defined as the Euclidean distance between the feature representations of an
    enhanced result and those of corresponding ground truth. The feature representations
    are typically extracted from the VGG network [[69](#bib.bib69)] pre-trained on
    ImageNet dataset [[70](#bib.bib70)].
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 感知损失。感知损失[[68](#bib.bib68)]，特别是特征重建损失，旨在在特征空间中使结果与真实情况相似。该损失改善了结果的视觉质量。它被定义为增强结果的特征表示与对应真实情况的特征表示之间的欧几里得距离。特征表示通常从在ImageNet数据集[[70](#bib.bib70)]上预训练的VGG网络[[69](#bib.bib69)]中提取。
- en: Smoothness Loss. To remove noise in the enhanced results or preserve the relationship
    of neighboring pixels, smoothness loss (TV loss) is often used to constrain the
    enhanced result or the estimated illumination map.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑损失。为了去除增强结果中的噪声或保持邻近像素的关系，平滑损失（TV损失）常常用于约束增强结果或估计的光照图。
- en: Adversarial Loss. To encourage enhanced results to be indistinguishable from
    reference images, adversarial learning solves a max-min optimization problem [[71](#bib.bib71),
    [72](#bib.bib72)].
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗损失。为了鼓励增强结果与参考图像难以区分，对抗学习解决了一个最大最小优化问题[[71](#bib.bib71), [72](#bib.bib72)]。
- en: Exposure Loss. As one of key non-reference losses, exposure loss measures the
    exposure levels of enhanced results without paired or unpaired images as reference
    images.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 曝光损失。作为关键的非参考损失之一，曝光损失在没有配对或非配对图像作为参考图像的情况下衡量增强结果的曝光水平。
- en: The commonly used loss functions in LLIE networks are also employed in image
    reconstruction networks for image super-resolution [[73](#bib.bib73)], image denoising
    [[74](#bib.bib74)], image detraining [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77)],
    and image deblurring [[78](#bib.bib78)]. Different from these versatile losses,
    the specially designed exposure loss for LLIE inspires the design of non-reference
    losses. A non-reference loss makes a model enjoying better generalization capability.
    It is an on-going research to consider image characteristics for the design of
    loss functions.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLIE网络中常用的损失函数也被用于图像重建网络中的图像超分辨率[[73](#bib.bib73)]、图像去噪[[74](#bib.bib74)]、图像去训练[[75](#bib.bib75),
    [76](#bib.bib76), [77](#bib.bib77)]和图像去模糊[[78](#bib.bib78)]。不同于这些多功能损失，专为LLIE设计的曝光损失激发了非参考损失的设计。非参考损失使模型具有更好的泛化能力。考虑图像特征以设计损失函数仍是一个正在进行的研究。
- en: 3.5 Training Datasets
  id: totrans-498
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 训练数据集
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based
    LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(f)
    reports the usage of a variety of paired training datasets for training low-light
    enhancement networks. These datasets include real-world captured datasets and
    synthetic datasets. We list them in Table [II](#S3.T2 "TABLE II ‣ 3.5 Training
    Datasets ‣ 3 Technical Review and Discussion ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey").'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](#S2.F3 "图3 ‣ 2.2 学习策略 ‣ 2 基于深度学习的低光照图像增强 ‣ 低光照图像和视频增强的深度学习综述")(f)报告了用于训练低光照增强网络的多种配对训练数据集的使用。这些数据集包括真实世界捕获的数据集和合成数据集。我们在表[II](#S3.T2
    "表II ‣ 3.5 训练数据集 ‣ 3 技术回顾与讨论 ‣ 低光照图像和视频增强的深度学习综述")中列出了它们。
- en: 'Simulated by Gamma Correction. Owing to its nonlinearity and simplicity, Gamma
    correction is used to adjust the luminance or tristimulus values in video or still
    image systems. It is defined by a power-law expression:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 由伽马校正模拟。由于其非线性和简单性，伽马校正用于调整视频或静态图像系统中的亮度或三刺激值。它由一个幂律表达式定义：
- en: '|  | $V_{\text{out}}=AV_{\text{in}}^{\gamma},$ |  | (3) |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  | $V_{\text{out}}=AV_{\text{in}}^{\gamma},$ |  | (3) |'
- en: where the input $V_{\text{in}}$ and output $V_{\text{out}}$ are typically in
    the range of [0,1]. The constant $A$ is set to 1 in the common case. The power
    $\gamma$ controls the luminance of the output. Intuitively, the input is brightened
    when $\gamma<$1 while the input is darkened when $\gamma>$1\. The input can be
    the three RGB channels of an image or the luminance-related channels such as $L$
    channel in the CIELab color space and $Y$ channel in the YCbCr color space. After
    adjusting the luminance-related channel using Gamma correction, the corresponding
    channels in the color space are adjusted by equal proportion to avoid producing
    artifacts and color deviations.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 其中输入 $V_{\text{in}}$ 和输出 $V_{\text{out}}$ 通常在 [0,1] 范围内。常数 $A$ 在常见情况下设置为 1。功率
    $\gamma$ 控制输出的亮度。直观地，当 $\gamma<$1 时，输入变亮；而当 $\gamma>$1 时，输入变暗。输入可以是图像的三个 RGB 通道或与亮度相关的通道，例如
    CIELab 色彩空间中的 $L$ 通道和 YCbCr 色彩空间中的 $Y$ 通道。调整亮度相关通道后，通过相同比例调整色彩空间中的相应通道，以避免产生伪影和颜色偏差。
- en: 'To simulate images taken in real-world low-light scenes, Gaussian noise, Poisson
    noise, or realistic noise is added to the Gamma corrected images. The low-light
    image synthesized using Gamma correction can be expressed as:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟在现实世界低光场景中拍摄的图像，将高斯噪声、泊松噪声或现实噪声添加到 Gamma 校正图像中。使用 Gamma 校正合成的低光图像可以表示为：
- en: '|  | $I_{\text{low}}=n(g(I_{\text{in}};\gamma)),$ |  | (4) |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{\text{low}}=n(g(I_{\text{in}};\gamma)),$ |  | (4) |'
- en: where $n$ represents the noise model, $g(I_{\text{in}};\gamma)$ represents the
    Gamma correction function with Gamma value $\gamma$, $I_{\text{in}}$ is a normal-light
    and high-quality image or luminance-related channel. Although this function produces
    low-light images of different lighting levels by changing the Gamma value $\gamma$,
    it tends to introduce artifacts and color deviations into the synthetic low-light
    images due to the nonlinear adjustment.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n$ 代表噪声模型，$g(I_{\text{in}};\gamma)$ 代表具有 Gamma 值 $\gamma$ 的 Gamma 校正函数，$I_{\text{in}}$
    是普通光照和高质量图像或与亮度相关的通道。虽然这个函数通过改变 Gamma 值 $\gamma$ 生成不同光照水平的低光图像，但由于非线性调整，它倾向于在合成低光图像中引入伪影和颜色偏差。
- en: 'TABLE II: Summary of paired training datasets. ‘Syn’ represents Synthetic.'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 配对训练数据集汇总。‘Syn’ 代表合成数据。'
- en: '| Name | Number | Format | Real/Syn | Video |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 数量 | 格式 | 实际/合成 | 视频 |'
- en: '| Gamma Correction | +$\infty$ | RGB | Syn |  |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| Gamma Correction | +$\infty$ | RGB | 合成 |  |'
- en: '| Random Illumination | +$\infty$ | RGB | Syn |  |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 随机光照 | +$\infty$ | RGB | 合成 |  |'
- en: '| LOL [[4](#bib.bib4)] | 500 | RGB | Real |  |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| LOL [[4](#bib.bib4)] | 500 | RGB | 实际 |  |'
- en: '| SCIE [[6](#bib.bib6)] | 4,413 | RGB | Real |  |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| SCIE [[6](#bib.bib6)] | 4,413 | RGB | 实际 |  |'
- en: '| VE-LOL-L [[55](#bib.bib55)] | 2,500 | RGB | Real+Syn |  |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| VE-LOL-L [[55](#bib.bib55)] | 2,500 | RGB | 实际+合成 |  |'
- en: '| MIT-Adobe FiveK [[79](#bib.bib79)] | 5,000 | raw | Real |  |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| MIT-Adobe FiveK [[79](#bib.bib79)] | 5,000 | 原始 | 实际 |  |'
- en: '| SID [[4](#bib.bib4)] | 5,094 | raw | Real |  |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| SID [[4](#bib.bib4)] | 5,094 | 原始 | 实际 |  |'
- en: '| DRV [[8](#bib.bib8)] | 202 | raw | Real | ✓ |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| DRV [[8](#bib.bib8)] | 202 | 原始 | 实际 | ✓ |'
- en: '| SMOID [[9](#bib.bib9)] | 179 | raw | Real | ✓ |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| SMOID [[9](#bib.bib9)] | 179 | 原始 | 实际 | ✓ |'
- en: Simulated by Random Illumination. According to the Retinex model, an image can
    be decomposed into a reflectance component and an illumination component. Assuming
    image content is independent of illumination component and local region in the
    illumination component have the same intensity, a low-light image can be obtained
    by
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 通过随机光照模拟。根据 Retinex 模型，图像可以分解为反射成分和光照成分。假设图像内容与光照成分独立，并且光照成分中的局部区域具有相同的强度，可以通过
- en: '|  | $I_{\text{low}}=I_{\text{in}}L,$ |  | (5) |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{\text{low}}=I_{\text{in}}L,$ |  | (5) |'
- en: where $L$ is a random illumination value in the range of [0,1]. Noises can be
    added to the synthetic image. Such a linear function avoids artifacts, but the
    strong assumption requires the synthesis to operate only on image patches where
    local regions have the same brightness. A deep model trained on such image patches
    may lead to sub-optimal performance due to the negligence of context information.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L$ 是范围为 [0,1] 的随机光照值。噪声可以添加到合成图像中。这样一个线性函数可以避免伪影，但强假设要求合成仅在局部区域具有相同亮度的图像块上进行。基于这样的图像块训练的深度模型可能因忽视上下文信息而导致次优性能。
- en: LOL. LOL [[4](#bib.bib4)] is the first paired low-/normal-light image dataset
    taken in real scenes. The low-light images are collected by changing the exposure
    time and ISO. LOL contains 500 pairs of low-/normal-light images of size 400$\times$600
    saved in RGB format.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: LOL。LOL [[4](#bib.bib4)] 是第一个在真实场景中拍摄的低光/正常光图像配对数据集。低光图像通过改变曝光时间和 ISO 进行收集。LOL
    包含500对大小为400$\times$600的低光/正常光图像，保存为 RGB 格式。
- en: SCIE. SCIE is a multi-exposure image dataset of low-contrast and good-contrast
    image pairs. It includes multi-exposure sequences of 589 indoor and outdoor scenes.
    Each sequence has 3 to 18 low-contrast images of different exposure levels, thus
    containing 4,413 multi-exposure images in total. The 589 high-quality reference
    images are obtained by selecting from the results of 13 representative enhancement
    algorithms. That is many multi-exposure images have the same high-contrast reference
    image. The image resolutions are between 3,000$\times$2,000 and 6,000$\times$4,000\.
    The images in SCIE are saved in RGB format.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: SCIE。SCIE 是一个低对比度和高对比度图像对的多曝光图像数据集。它包括589个室内和室外场景的多曝光序列。每个序列有3到18张不同曝光级别的低对比度图像，总共包含4,413张多曝光图像。589张高质量参考图像是从13种代表性增强算法的结果中挑选出来的。也就是说，许多多曝光图像具有相同的高对比度参考图像。图像分辨率在3,000$\times$2,000到6,000$\times$4,000之间。SCIE中的图像以
    RGB 格式保存。
- en: MIT-Adobe FiveK. MIT-Adobe FiveK [[79](#bib.bib79)] was collected for global
    tone adjustment but has been used in LLIE. This is because the input images have
    low light and low contrast. MIT-Adobe FiveK contains 5,000 images, each of which
    is retouched by 5 trained photographers towards visually pleasing renditions,
    akin to a postcard. The images are all in raw format. To train the networks that
    can handle images of RGB format, one needs to use Adobe Lightroom to pre-process
    the images and save them as RGB format following a dedicated pipeline¹¹1[https://github.com/nothinglo/Deep-Photo-Enhancer/issues/38#issuecomment-449786636](https://github.com/nothinglo/Deep-Photo-Enhancer/issues/38#issuecomment-449786636).
    The images are commonly resized to have a long edge of 500 pixels.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: MIT-Adobe FiveK。MIT-Adobe FiveK [[79](#bib.bib79)] 收集用于全局色调调整，但也用于 LLIE。这是因为输入图像具有低光和低对比度。MIT-Adobe
    FiveK 包含5,000张图像，每张图像由5名训练有素的摄影师修饰，以获得视觉上令人愉悦的效果，类似于明信片。所有图像均为原始格式。为了训练能够处理 RGB
    格式图像的网络，需要使用 Adobe Lightroom 对图像进行预处理，并按照专用管道保存为 RGB 格式¹¹1[https://github.com/nothinglo/Deep-Photo-Enhancer/issues/38#issuecomment-449786636](https://github.com/nothinglo/Deep-Photo-Enhancer/issues/38#issuecomment-449786636)。图像通常被调整为长边500像素。
- en: 'SID. SID [[2](#bib.bib2)] contains 5,094 raw short-exposure images, each with
    a corresponding long-exposure reference image. The number of distinct long-exposure
    reference images is 424\. In other words, multiple short-exposure images correspond
    to the same long-exposure reference image. The images were taken using two cameras:
    Sony $\alpha$7S II and Fujifilm X-T2 in both indoor and outdoor scenes. Thus,
    the images have different sensor patterns (Sony camera’ Bayer sensor and Fuji
    camera’s APS-C X-Trans sensor). The resolution is 4,240$\times$2,832 for Sony
    and 6,000$\times$4,000 for Fuji. Usually, the long-exposure images are processed
    by libraw (a raw image processing library) and saved in the RGB color space, and
    randomly cropped 512$\times$512 patches for training.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: SID。SID [[2](#bib.bib2)] 包含5,094张原始短曝光图像，每张都有对应的长曝光参考图像。长曝光参考图像的数量为424张。换句话说，多张短曝光图像对应同一张长曝光参考图像。这些图像使用两台相机拍摄：Sony
    $\alpha$7S II 和 Fujifilm X-T2，拍摄场景包括室内和室外。因此，这些图像具有不同的传感器模式（Sony 相机的 Bayer 传感器和
    Fuji 相机的 APS-C X-Trans 传感器）。分辨率为 Sony 的4,240$\times$2,832和 Fuji 的6,000$\times$4,000。通常，长曝光图像使用
    libraw（一个原始图像处理库）进行处理，并以 RGB 颜色空间保存，训练时随机裁剪为512$\times$512的补丁。
- en: 'VE-LOL. VE-LOL [[55](#bib.bib55)] consists of two subsets: paired VE-LOL-L
    that is used for training and evaluating LLIE methods and unpaired VE-LOL-H that
    is used for evaluating the effect of LLIE methods on face detection. Specifically,
    VE-LOL-L includes 2,500 paired images. Among them, 1,000 pairs are synthetic,
    while 1,500 pairs are real. VE-LOL-H includes 10,940 unpaired images, where human
    faces are manually annotated with bounding boxes.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: VE-LOL。VE-LOL [[55](#bib.bib55)] 包含两个子集：用于训练和评估 LLIE 方法的配对 VE-LOL-L 和用于评估 LLIE
    方法在面部检测中效果的未配对 VE-LOL-H。具体来说，VE-LOL-L 包含2,500对配对图像。其中，1,000对是合成的，1,500对是实际的。VE-LOL-H
    包含10,940张未配对图像，其中人脸经过手动标注并加上了边界框。
- en: DRV. DRV [[8](#bib.bib8)] contains 202 static raw videos, each of which has
    a corresponding long-exposure ground truth. Each video was taken at approximately
    16 to 18 frames per second in a continuous shooting mode and is with up to 110
    frames. The images were taken by a Sony RX100 VI camera in both indoor and outdoor
    scenes, thus all in raw format of Bayer pattern. The resolution is 3,672$\times$5,496.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: DRV。DRV [[8](#bib.bib8)] 包含202个静态原始视频，每个视频都有一个对应的长时间曝光真实图像。每个视频以大约16到18帧每秒的连续拍摄模式拍摄，最多有110帧。图像由Sony
    RX100 VI相机在室内和室外场景中拍摄，因此全部为Bayer模式的原始格式。分辨率为3,672$\times$5,496。
- en: SMOID. SMOID [[9](#bib.bib9)] contains 179 pairs of videos taken by a co-axis
    optical system, each of which has 200 frames. Thus, SMOID includes 35,800 extremely
    low light raw data of Bayer pattern and their corresponding well-lightened RGB
    counterparts. SMOID consists of moving vehicles and pedestrians under different
    illumination conditions.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: SMOID。SMOID [[9](#bib.bib9)] 包含179对由共轴光学系统拍摄的视频，每对视频有200帧。因此，SMOID包括35,800个极低光照下的Bayer模式原始数据及其对应的良好照明RGB图像。SMOID由在不同光照条件下的移动车辆和行人组成。
- en: 'Some issues challenge the aforementioned paired training datasets: 1) deep
    models trained on synthetic data may introduce artifacts and color deviations
    when processing real-world images and videos due to the gap between synthetic
    data and real data, 2) the scale and diversity of real training data are unsatisfactory,
    thus some methods incorporate synthetic data to augment the training data. This
    may lead to sub-optimal enhancement, and 3) the input images and corresponding
    ground truths may exist misalignment due to the effects of motion, hardware, and
    environment. This would affect the performance of deep networks trained using
    pixel-wise loss functions.'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 一些问题挑战了上述配对训练数据集：1）在合成数据上训练的深度模型可能会在处理真实世界图像和视频时引入伪影和色彩偏差，因为合成数据和真实数据之间存在差距，2）真实训练数据的规模和多样性令人不满，因此一些方法结合合成数据来增强训练数据。这可能导致次优的增强效果，3）由于运动、硬件和环境的影响，输入图像和对应的真实图像可能存在对齐问题。这会影响使用逐像素损失函数训练的深度网络的性能。
- en: 3.6 Testing Datasets
  id: totrans-528
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 测试数据集
- en: 'In addition to the testing subsets in the paired datasets [[4](#bib.bib4),
    [6](#bib.bib6), [79](#bib.bib79), [2](#bib.bib2), [8](#bib.bib8), [9](#bib.bib9),
    [55](#bib.bib55)], there are several testing data collected from related works
    or commonly used for experimental comparisons. Besides, some datasets such as
    face detection in the dark [[80](#bib.bib80)] and detection and recognition in
    low-light images [[81](#bib.bib81)] are employed to test the effects of LLIE on
    high-level visual tasks. We summarize the commonly used testing datasets in Table
    [III](#S3.T3 "TABLE III ‣ 3.6 Testing Datasets ‣ 3 Technical Review and Discussion
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey") and introduce
    the representative testing datasets as follows.'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '除了配对数据集中的测试子集 [[4](#bib.bib4), [6](#bib.bib6), [79](#bib.bib79), [2](#bib.bib2),
    [8](#bib.bib8), [9](#bib.bib9), [55](#bib.bib55)]，还收集了若干来自相关工作的测试数据或常用于实验比较的数据。此外，一些数据集如暗光下的人脸检测
    [[80](#bib.bib80)] 和低光图像中的检测与识别 [[81](#bib.bib81)] 被用于测试LLIE对高级视觉任务的效果。我们在表 [III](#S3.T3
    "TABLE III ‣ 3.6 Testing Datasets ‣ 3 Technical Review and Discussion ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey") 中总结了常用的测试数据集，并介绍了代表性的测试数据集如下。'
- en: 'TABLE III: Summary of testing datasets.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：测试数据集总结。
- en: '| Name | Number | Format | Application | Video |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| Name | Number | Format | Application | Video |'
- en: '| LIME [[39](#bib.bib39)] | 10 | RGB |  |  |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| LIME [[39](#bib.bib39)] | 10 | RGB |  |  |'
- en: '| NPE [[37](#bib.bib37)] | 84 | RGB |  |  |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| NPE [[37](#bib.bib37)] | 84 | RGB |  |  |'
- en: '| MEF [[82](#bib.bib82)] | 17 | RGB |  |  |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| MEF [[82](#bib.bib82)] | 17 | RGB |  |  |'
- en: '| DICM [[83](#bib.bib83)] | 64 | RGB |  |  |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| DICM [[83](#bib.bib83)] | 64 | RGB |  |  |'
- en: '| VV²²2[https://sites.google.com/site/vonikakis/datasets](https://sites.google.com/site/vonikakis/datasets)
    | 24 | RGB |  |  |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| VV²²2[https://sites.google.com/site/vonikakis/datasets](https://sites.google.com/site/vonikakis/datasets)
    | 24 | RGB |  |  |'
- en: '| BBD-100K [[84](#bib.bib84)] | 10,000 | RGB | ✓ | ✓ |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| BBD-100K [[84](#bib.bib84)] | 10,000 | RGB | ✓ | ✓ |'
- en: '| ExDARK [[81](#bib.bib81)] | 7,363 | RGB | ✓ |  |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| ExDARK [[81](#bib.bib81)] | 7,363 | RGB | ✓ |  |'
- en: '| DARK FACE [[80](#bib.bib80)] | 6,000 | RGB | ✓ |  |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| DARK FACE [[80](#bib.bib80)] | 6,000 | RGB | ✓ |  |'
- en: '| VE-LOL-H [[55](#bib.bib55)] | 10,940 | RGB | ✓ |  |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| VE-LOL-H [[55](#bib.bib55)] | 10,940 | RGB | ✓ |  |'
- en: BBD-100K. BBD-100K [[84](#bib.bib84)] is the largest driving video dataset with
    10,000 videos taken over 1,100-hour driving experience across many different times
    in the day, weather conditions, and driving scenarios, and 10 tasks annotations.
    The videos taken at nighttime in BBD-100K are used to validate the effects of
    LLIE on high-level visual tasks and the enhancement performance in real scenarios.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: BBD-100K。BBD-100K [[84](#bib.bib84)] 是最大的驾驶视频数据集，包含 10,000 个视频，涵盖了 1,100 小时的驾驶体验，涉及一天中的多个时间、天气条件和驾驶场景，并且有
    10 个任务注释。BBD-100K 中夜间拍摄的视频用于验证 LLIE 在高级视觉任务中的效果以及在实际场景中的增强性能。
- en: ExDARK. ExDARK [[81](#bib.bib81)] dataset is built for object detection and
    recognition in low-light images. ExDARK dataset contains 7,363 low-light images
    from extremely low-light environments to twilight with 12 object classes annotated
    with image class labels and local object bounding boxes.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: ExDARK。ExDARK [[81](#bib.bib81)] 数据集用于低光图像中的目标检测和识别。ExDARK 数据集包含 7,363 张从极低光环境到暮光的低光图像，具有
    12 个对象类别，并标注了图像类别标签和局部对象边界框。
- en: DARK FACE. DARK FACE [[80](#bib.bib80)] dataset contains 6,000 low-light images
    captured during the nighttime, each of which is labeled with bounding boxes of
    the human face.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: DARK FACE。DARK FACE [[80](#bib.bib80)] 数据集包含 6,000 张夜间拍摄的低光图像，每张图像都标记有人脸的边界框。
- en: 'From Figure [III](#S3.T3 "TABLE III ‣ 3.6 Testing Datasets ‣ 3 Technical Review
    and Discussion ‣ Low-Light Image and Video Enhancement Using Deep Learning: A
    Survey")(g) and Table [I](#S2.T1 "TABLE I ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based
    LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey"),
    we can observe that one prefers using the self-collected testing data in the experiments.
    The main reasons lie into three-fold: 1) besides the test partition of paired
    datasets, there is no acknowledged benchmark for evaluations, 2) the commonly
    used test sets suffer from some shortcomings such as small scale (some test sets
    contain 10 images only), repeated content and illumination properties, and unknown
    experimental settings, and 3) some of the commonly used testing data are not originally
    collected for evaluating LLIE. In general, current testing datasets may lead to
    bias and unfair comparisons.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 [III](#S3.T3 "表 III ‣ 3.6 测试数据集 ‣ 3 技术审查与讨论 ‣ 基于深度学习的低光图像和视频增强：综述")(g) 和表
    [I](#S2.T1 "表 I ‣ 2.2 学习策略 ‣ 2 基于深度学习的 LLIE ‣ 基于深度学习的低光图像和视频增强：综述") 中，我们可以观察到实验中更倾向于使用自收集的测试数据。主要原因有三点：1)
    除了成对数据集的测试分区外，尚无公认的评估基准，2) 常用的测试集存在一些缺陷，例如规模较小（某些测试集仅包含 10 张图像）、内容和照明属性重复以及实验设置不明确，3)
    一些常用的测试数据不是最初为评估 LLIE 收集的。总体而言，当前的测试数据集可能导致偏差和不公平的比较。
- en: 3.7 Evaluation Metrics
  id: totrans-545
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 评估指标
- en: 'Besides human perception-based subjective evaluations, image quality assessment
    (IQA) metrics, including both full-reference and non-reference IQA metrics, are
    able to evaluate image quality objectively. In addition, user study, number of
    trainable parameters, FLOPs, runtime, and applications also reflect the performance
    of LLIE models, as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies
    ‣ 2 Deep Learning-Based LLIE ‣ Low-Light Image and Video Enhancement Using Deep
    Learning: A Survey")(h). We will detail them as follows.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于人类感知的主观评价外，图像质量评估（IQA）指标，包括全参考和无参考 IQA 指标，可以客观地评估图像质量。此外，用户研究、可训练参数数量、FLOPs、运行时间和应用也反映了
    LLIE 模型的性能，如图 [3](#S2.F3 "图 3 ‣ 2.2 学习策略 ‣ 2 基于深度学习的 LLIE ‣ 基于深度学习的低光图像和视频增强：综述")(h)
    所示。我们将详细介绍它们。
- en: PSNR and MSE. PSNR and MSE are widely used IQA metrics. They are always non-negative,
    and values closer to infinite (PSNR) and zero (MSE) are better. Nevertheless,
    the pixel-wise PSNR and MSE may provide an inaccurate indication of the visual
    perception of image quality since they neglect the relation of neighboring pixels.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: PSNR 和 MSE。PSNR 和 MSE 是广泛使用的 IQA 指标。它们总是非负的，值越接近无穷大（PSNR）和零（MSE）越好。然而，逐像素的 PSNR
    和 MSE 可能无法准确指示图像质量的视觉感知，因为它们忽略了相邻像素之间的关系。
- en: MAE. MAE represents the mean absolute error, serving as a measure of errors
    between paired observations. The smaller the MAE value is, the better similarity
    is.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: MAE。MAE 代表均值绝对误差，用于衡量成对观测值之间的误差。MAE 值越小，相似度越好。
- en: SSIM. SSIM is used to measure the similarity between two images. It is a perception-based
    model that considers image degradation as perceived change in structural information.
    The value 1 is only reachable in the case of two identical sets of data, indicating
    perfect structural similarity.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: SSIM。SSIM用于衡量两幅图像之间的相似度。这是一个基于感知的模型，考虑到图像退化被感知为结构信息的变化。值为1仅在两组数据完全相同时才可能达到，表示完美的结构相似性。
- en: LOE. LOE represents the lightness order error that reflects the naturalness
    of an enhanced image. For LOE, the smaller the LOE value is, the better the lightness
    order is preserved.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: LOE。LOE表示光亮度顺序误差，反映了增强图像的自然性。对于LOE而言，LOE值越小，光亮度顺序保持得越好。
- en: Application. Besides improving the visual quality, one of the purposes of image
    enhancement is to serve high-level visual tasks. Thus, the effects of LLIE on
    high-level visual applications are commonly examined to validate the performance
    of different methods.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 应用。除了提高视觉质量外，图像增强的一个目的也是服务于高层次视觉任务。因此，LLIE对高层次视觉应用的效果通常会被检查，以验证不同方法的性能。
- en: 'The current evaluation approaches used in LLIE need to be improved in several
    aspects: 1) although the PSNR, MSE, MAE, and SSIM are classic and popular metrics,
    they are still far from capturing real visual perception of human, 2) some metrics
    are not originally designed for low-light images. They are used for assessing
    the fidelity of image information and contrast. Using these metrics may reflect
    the image quality, but they are far from the real purpose of low-light enhancement,
    3) metrics especially designed for low-light images are lacking, except for the
    LOE metric. Moreover, there is no metric for evaluating low-light video enhancement,
    and 4) a metric that can balance both the human vision and the machine perception
    is expected.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 当前在LLIE中使用的评估方法在几个方面需要改进：1）尽管PSNR、MSE、MAE和SSIM是经典且流行的指标，但它们仍远未捕捉到人类的真实视觉感知；2）一些指标最初并非为低光图像设计。它们用于评估图像信息和对比度的保真度。使用这些指标可能反映图像质量，但距离低光增强的真实目的还很远；3）专门为低光图像设计的指标缺乏，除了LOE指标外。而且，尚无评估低光视频增强的指标；4）期待一种能够平衡人类视觉和机器感知的指标。
- en: 4 Benchmarking and Empirical Analysis
  id: totrans-553
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基准测试与实证分析
- en: This section provides empirical analysis and highlights some key challenges
    in deep learning-based LLIE. To facilitate the analysis, we propose a low-light
    image and video dataset to examine the performance of different solutions. We
    also develop the first online platform, where the results of LLIE models can be
    produced via a user-friendly web interface. In this section, we conduct extensive
    evaluations on several benchmarks and our proposed dataset.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了实证分析，并突出了基于深度学习的LLIE的一些关键挑战。为方便分析，我们提出了一个低光图像和视频数据集，以检验不同解决方案的性能。我们还开发了第一个在线平台，通过用户友好的网络界面生成LLIE模型的结果。在本节中，我们对几个基准测试和我们提出的数据集进行了广泛评估。
- en: In the experiments, we compare 13 representative RGB format-based methods, including
    eight supervised learning-based methods (LLNet [[1](#bib.bib1)], LightenNet [[5](#bib.bib5)],
    Retinex-Net [[4](#bib.bib4)], MBLLEN [[3](#bib.bib3)], KinD [[11](#bib.bib11)],
    KinD++ [[61](#bib.bib61)], TBEFN [[20](#bib.bib20)], DSLR [[21](#bib.bib21)]),
    one unsupervised learning-based method (EnlightenGAN [[26](#bib.bib26)]), one
    semi-supervised learning-based method (DRBN [[33](#bib.bib33)]), and three zero-shot
    learning-based methods (ExCNet [[27](#bib.bib27)], Zero-DCE [[28](#bib.bib28)],
    RRDNet [[29](#bib.bib29)]). Besides, we also compare two raw format-based methods,
    including SID [[85](#bib.bib85)] and EEMEFN [[16](#bib.bib16)]. Note that RGB
    format-based methods dominate LLIE. Moreover, most raw format-based methods do
    not release their code. Thus, we choose two representative methods to provide
    empirical analysis and insights. For all compared methods, we use the publicly
    available code to produce their results for fair comparisons.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们比较了 13 种代表性的基于 RGB 格式的方法，包括八种基于监督学习的方法（LLNet [[1](#bib.bib1)]，LightenNet
    [[5](#bib.bib5)]，Retinex-Net [[4](#bib.bib4)]，MBLLEN [[3](#bib.bib3)]，KinD [[11](#bib.bib11)]，KinD++
    [[61](#bib.bib61)]，TBEFN [[20](#bib.bib20)]，DSLR [[21](#bib.bib21)]），一种基于无监督学习的方法（EnlightenGAN
    [[26](#bib.bib26)]），一种基于半监督学习的方法（DRBN [[33](#bib.bib33)]），以及三种基于零样本学习的方法（ExCNet
    [[27](#bib.bib27)]，Zero-DCE [[28](#bib.bib28)]，RRDNet [[29](#bib.bib29)]）。此外，我们还比较了两种基于原始格式的方法，包括
    SID [[85](#bib.bib85)] 和 EEMEFN [[16](#bib.bib16)]。请注意，基于 RGB 格式的方法在 LLIE 中占据主导地位。而且，大多数基于原始格式的方法未公开其代码。因此，我们选择了两种代表性的方法进行实证分析和洞察。对于所有比较的方法，我们使用公开的代码生成结果以确保公平比较。
- en: 'TABLE IV: Summary of LLIV-Phone dataset. LLIV-Phone dataset contains 120 videos
    (45,148 images) taken by 18 different mobile phones’ cameras. “#Video” and “#Image”
    represent the number of videos and images, respectively.'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: LLIV-Phone 数据集汇总。LLIV-Phone 数据集包含 120 个视频（45,148 张图像），由 18 款不同手机的摄像头拍摄。“#视频”和“#图像”分别表示视频和图像的数量。'
- en: '| Phone’s Brand | #Video | #Image | Resolution |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| 手机品牌 | #视频 | #图像 | 分辨率 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| iPhone 6s | 4 | 1,029 | 1920$\times$1080 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| iPhone 6s | 4 | 1,029 | 1920$\times$1080 |'
- en: '| iPhone 7 | 13 | 6,081 | 1920$\times$1080 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| iPhone 7 | 13 | 6,081 | 1920$\times$1080 |'
- en: '| iPhone7 Plus | 2 | 900 | 1920$\times$1080 |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| iPhone7 Plus | 2 | 900 | 1920$\times$1080 |'
- en: '| iPhone8 Plus | 1 | 489 | 1280$\times$720 |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| iPhone8 Plus | 1 | 489 | 1280$\times$720 |'
- en: '| iPhone 11 | 7 | 2,200 | 1920$\times$1080 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| iPhone 11 | 7 | 2,200 | 1920$\times$1080 |'
- en: '| iPhone 11 Pro | 17 | 7,739 | 1920$\times$1080 |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| iPhone 11 Pro | 17 | 7,739 | 1920$\times$1080 |'
- en: '| iPhone XS | 11 | 2,470 | 1920$\times$1080 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| iPhone XS | 11 | 2,470 | 1920$\times$1080 |'
- en: '| iPhone XR | 16 | 4,997 | 1920$\times$1080 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| iPhone XR | 16 | 4,997 | 1920$\times$1080 |'
- en: '| iPhone SE | 1 | 455 | 1920$\times$1080 |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| iPhone SE | 1 | 455 | 1920$\times$1080 |'
- en: '| Xiaomi Mi 9 | 2 | 1,145 | 1920$\times$1080 |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| 小米 Mi 9 | 2 | 1,145 | 1920$\times$1080 |'
- en: '| Xiaomi Mi Mix 3 | 6 | 2,972 | 1920$\times$1080 |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| 小米 Mi Mix 3 | 6 | 2,972 | 1920$\times$1080 |'
- en: '| Pixel 3 | 4 | 1,311 | 1920$\times$1080 |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| Pixel 3 | 4 | 1,311 | 1920$\times$1080 |'
- en: '| Pixel 4 | 3 | 1,923 | 1920$\times$1080 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| Pixel 4 | 3 | 1,923 | 1920$\times$1080 |'
- en: '| Oppo R17 | 6 | 2,126 | 1920$\times$1080 |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| Oppo R17 | 6 | 2,126 | 1920$\times$1080 |'
- en: '| Vivo Nex | 12 | 4,097 | 1280$\times$720 |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| Vivo Nex | 12 | 4,097 | 1280$\times$720 |'
- en: '| LG M322 | 2 | 761 | 1920$\times$1080 |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| LG M322 | 2 | 761 | 1920$\times$1080 |'
- en: '| OnePlus 5T | 1 | 293 | 1920$\times$1080 |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| OnePlus 5T | 1 | 293 | 1920$\times$1080 |'
- en: '| Huawei Mate 20 Pro | 12 | 4,160 | 1920$\times$1080 |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| 华为 Mate 20 Pro | 12 | 4,160 | 1920$\times$1080 |'
- en: 4.1 A New Low-Light Image and Video Dataset
  id: totrans-577
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 一个新的低光图像和视频数据集
- en: 'We propose a Low-Light Image and Video dataset, called LLIV-Phone, to comprehensively
    and thoroughly validate the performance of LLIE methods. LLIV-Phone is the largest
    and most challenging real-world testing dataset of its kind. In particular, the
    dataset contains 120 videos (45,148 images) taken by 18 different mobile phones’
    cameras including iPhone 6s, iPhone 7, iPhone7 Plus, iPhone8 Plus, iPhone 11,
    iPhone 11 Pro, iPhone XS, iPhone XR, iPhone SE, Xiaomi Mi 9, Xiaomi Mi Mix 3,
    Pixel 3, Pixel 4, Oppo R17, Vivo Nex, LG M322, OnePlus 5T, Huawei Mate 20 Pro
    under diverse illumination conditions (e.g., weak lighting, underexposure, moonlight,
    twilight, dark, extremely dark, back-lit, non-uniform light, and colored light.)
    in both indoor and outdoor scenes. A summary of the LLIV-Phone dataset is provided
    in Table [IV](#S4.T4 "TABLE IV ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey"). We present several
    samples of LLIV-Phone dataset in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 A New Low-Light
    Image and Video Dataset ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image
    and Video Enhancement Using Deep Learning: A Survey"). The LLIV-Phone dataset
    is available at the project page.'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出了一种低光照图像和视频数据集，称为 LLIV-Phone，以全面和深入地验证 LLIE 方法的性能。LLIV-Phone 是此类数据集中最大的和最具挑战性的真实世界测试数据集。特别是，该数据集包含
    120 个视频（45,148 张图像），由 18 部不同的手机摄像头拍摄，包括 iPhone 6s、iPhone 7、iPhone7 Plus、iPhone8
    Plus、iPhone 11、iPhone 11 Pro、iPhone XS、iPhone XR、iPhone SE、小米 Mi 9、小米 Mi Mix 3、Pixel
    3、Pixel 4、Oppo R17、Vivo Nex、LG M322、OnePlus 5T、华为 Mate 20 Pro，在不同的光照条件下（例如，弱光、曝光不足、月光、黄昏、黑暗、极暗、逆光、不均匀光和彩色光）拍摄，包括室内和室外场景。LLIV-Phone
    数据集的总结见表 [IV](#S4.T4 "TABLE IV ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey")。图 [4](#S4.F4 "Figure
    4 ‣ 4.1 A New Low-Light Image and Video Dataset ‣ 4 Benchmarking and Empirical
    Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")
    展示了 LLIV-Phone 数据集的几个样本。LLIV-Phone 数据集可以在项目页面上获取。'
- en: This challenging dataset is collected in real scenes and contains diverse low-light
    images and videos. Consequently, it is suitable for evaluating the generalization
    capability of different low-light image and video enhancement models. Notably,
    the dataset can be used as the training dataset for unsupervised learning and
    the reference dataset for synthesis methods to generate realistic low-light data.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 这个具有挑战性的数据集是在真实场景中收集的，包含了多样化的低光照图像和视频。因此，它适合用来评估不同低光照图像和视频增强模型的泛化能力。特别地，该数据集可以作为无监督学习的训练数据集，也可以作为合成方法生成真实低光数据的参考数据集。
- en: '![Refer to caption](img/bd64b3f5572943e39ae67448e8492757.png)'
  id: totrans-580
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bd64b3f5572943e39ae67448e8492757.png)'
- en: 'Figure 4: Several images sampled from the proposed LLIV-Phone dataset. The
    images and videos are taken by different devices under diverse lighting conditions
    and scenes.'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：从所提议的 LLIV-Phone 数据集中采样的几个图像。这些图像和视频是由不同设备在不同光照条件和场景下拍摄的。
- en: '| ![Refer to caption](img/3c706fc44a84e9110beb2e71b8298821.png) | ![Refer to
    caption](img/5eb6e871b477e8c14341de16e6272c73.png) | ![Refer to caption](img/3f7c201ce07eddc43673dd88c820c4bb.png)
    | ![Refer to caption](img/8434dfec4684e78191ed0ce297a28da3.png) | ![Refer to caption](img/1615b227424e3bef5ed0a436758e143d.png)
    |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/3c706fc44a84e9110beb2e71b8298821.png) | ![参见说明](img/5eb6e871b477e8c14341de16e6272c73.png)
    | ![参见说明](img/3f7c201ce07eddc43673dd88c820c4bb.png) | ![参见说明](img/8434dfec4684e78191ed0ce297a28da3.png)
    | ![参见说明](img/1615b227424e3bef5ed0a436758e143d.png) |'
- en: '| (a) input | (b) LLNet [[1](#bib.bib1)] | (c) LightenNet [[5](#bib.bib5)]
    | (d) Retinex-Net [[4](#bib.bib4)] | (e) MBLLEN [[3](#bib.bib3)] |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| (a) 输入 | (b) LLNet [[1](#bib.bib1)] | (c) LightenNet [[5](#bib.bib5)] | (d)
    Retinex-Net [[4](#bib.bib4)] | (e) MBLLEN [[3](#bib.bib3)] |'
- en: '| ![Refer to caption](img/3a9ee856055f029c3358505a7d355da1.png) | ![Refer to
    caption](img/0f87209a6e0e00d0663ebfa6aae5097e.png) | ![Refer to caption](img/925478fa0a7f0953b26a2295b5228067.png)
    | ![Refer to caption](img/f5353e3bbbf76dc68596b9db8559a57f.png) | ![Refer to caption](img/8f0320c3d604f6816671ce45f15b6acf.png)
    |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/3a9ee856055f029c3358505a7d355da1.png) | ![参见说明](img/0f87209a6e0e00d0663ebfa6aae5097e.png)
    | ![参见说明](img/925478fa0a7f0953b26a2295b5228067.png) | ![参见说明](img/f5353e3bbbf76dc68596b9db8559a57f.png)
    | ![参见说明](img/8f0320c3d604f6816671ce45f15b6acf.png) |'
- en: '| (f) KinD [[11](#bib.bib11)] | (g) KinD++ [[61](#bib.bib61)] | (h) TBEFN [[20](#bib.bib20)]
    | (i) DSLR [[21](#bib.bib21)] | (j) EnlightenGAN [[26](#bib.bib26)] |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| (f) KinD [[11](#bib.bib11)] | (g) KinD++ [[61](#bib.bib61)] | (h) TBEFN [[20](#bib.bib20)]
    | (i) DSLR [[21](#bib.bib21)] | (j) EnlightenGAN [[26](#bib.bib26)] |'
- en: '| ![Refer to caption](img/d9064d7fab408801cf8edf59ffcd8197.png) | ![Refer to
    caption](img/13f512cb9d35b49ec7a643de81769429.png) | ![Refer to caption](img/96df4cf9d8efa487770bda5f4fcdd24a.png)
    | ![Refer to caption](img/cc53ff9c303e13cf4430a0be5e201f73.png) | ![Refer to caption](img/325808bcf76c3befcf155f43db6ebbd0.png)
    |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/d9064d7fab408801cf8edf59ffcd8197.png) | ![参见说明](img/13f512cb9d35b49ec7a643de81769429.png)
    | ![参见说明](img/96df4cf9d8efa487770bda5f4fcdd24a.png) | ![参见说明](img/cc53ff9c303e13cf4430a0be5e201f73.png)
    | ![参见说明](img/325808bcf76c3befcf155f43db6ebbd0.png) |'
- en: '| (k) DRBN [[33](#bib.bib33)] | (l) ExCNet [[27](#bib.bib27)] | (m) Zero-DCE
    [[28](#bib.bib28)] | (n) RRDNet [[29](#bib.bib29)] | (o) GT |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| (k) DRBN [[33](#bib.bib33)] | (l) ExCNet [[27](#bib.bib27)] | (m) Zero-DCE
    [[28](#bib.bib28)] | (n) RRDNet [[29](#bib.bib29)] | (o) GT |'
- en: 'Figure 5: Visual results of different methods on a low-light image sampled
    from LOL-test dataset.'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：来自 LOL-test 数据集的低光图像上不同方法的视觉结果。
- en: '| ![Refer to caption](img/648ccd67d0c0e08c6183971b34a61f68.png) | ![Refer to
    caption](img/c65ce3ca0a8d328e7e5c2f64a1bfccad.png) | ![Refer to caption](img/12c63d6ef5c6c279537f3dff22548fd6.png)
    | ![Refer to caption](img/8da1c513509ae9c4b773de3d3a355839.png) | ![Refer to caption](img/a7a2ff9425630293b7de905ba7549cce.png)
    |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/648ccd67d0c0e08c6183971b34a61f68.png) | ![参见说明](img/c65ce3ca0a8d328e7e5c2f64a1bfccad.png)
    | ![参见说明](img/12c63d6ef5c6c279537f3dff22548fd6.png) | ![参见说明](img/8da1c513509ae9c4b773de3d3a355839.png)
    | ![参见说明](img/a7a2ff9425630293b7de905ba7549cce.png) |'
- en: '| (a) input | (b) LLNet [[1](#bib.bib1)] | (c) LightenNet [[5](#bib.bib5)]
    | (d) Retinex-Net [[4](#bib.bib4)] | (e) MBLLEN [[3](#bib.bib3)] |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| (a) 输入 | (b) LLNet [[1](#bib.bib1)] | (c) LightenNet [[5](#bib.bib5)] | (d)
    Retinex-Net [[4](#bib.bib4)] | (e) MBLLEN [[3](#bib.bib3)] |'
- en: '| ![Refer to caption](img/3652d82b7f8d49835b290f1e4c2f853e.png) | ![Refer to
    caption](img/2939216df2b529bee7cc24f43dc688f5.png) | ![Refer to caption](img/68ca8b893a718fc5242c8d7d96516348.png)
    | ![Refer to caption](img/34a6f4dac1fe026bec8749b47729b5ea.png) | ![Refer to caption](img/4c27d9885ce4271e2deb05a135056a14.png)
    |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/3652d82b7f8d49835b290f1e4c2f853e.png) | ![参见说明](img/2939216df2b529bee7cc24f43dc688f5.png)
    | ![参见说明](img/68ca8b893a718fc5242c8d7d96516348.png) | ![参见说明](img/34a6f4dac1fe026bec8749b47729b5ea.png)
    | ![参见说明](img/4c27d9885ce4271e2deb05a135056a14.png) |'
- en: '| (f) KinD [[11](#bib.bib11)] | (g) KinD++ [[61](#bib.bib61)] | (h) TBEFN [[20](#bib.bib20)]
    | (i) DSLR [[21](#bib.bib21)] | (j) EnlightenGAN [[26](#bib.bib26)] |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| (f) KinD [[11](#bib.bib11)] | (g) KinD++ [[61](#bib.bib61)] | (h) TBEFN [[20](#bib.bib20)]
    | (i) DSLR [[21](#bib.bib21)] | (j) EnlightenGAN [[26](#bib.bib26)] |'
- en: '| ![Refer to caption](img/057c22b7c61b5b88c03c22a9729f9efe.png) | ![Refer to
    caption](img/c61c20584af1c45baf8f3b02e19d1b59.png) | ![Refer to caption](img/3bc27dbdfd93f4803909aaa357b58144.png)
    | ![Refer to caption](img/197b6f69bc06b20bb34331d103c1f6da.png) | ![Refer to caption](img/540ed9a338ca54e58652826b2dd2c1ac.png)
    |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/057c22b7c61b5b88c03c22a9729f9efe.png) | ![参见说明](img/c61c20584af1c45baf8f3b02e19d1b59.png)
    | ![参见说明](img/3bc27dbdfd93f4803909aaa357b58144.png) | ![参见说明](img/197b6f69bc06b20bb34331d103c1f6da.png)
    | ![参见说明](img/540ed9a338ca54e58652826b2dd2c1ac.png) |'
- en: '| (k) DRBN [[33](#bib.bib33)] | (l) ExCNet [[27](#bib.bib27)] | (m) Zero-DCE
    [[28](#bib.bib28)] | (n) RRDNet [[29](#bib.bib29)] | (o) GT |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| (k) DRBN [[33](#bib.bib33)] | (l) ExCNet [[27](#bib.bib27)] | (m) Zero-DCE
    [[28](#bib.bib28)] | (n) RRDNet [[29](#bib.bib29)] | (o) GT |'
- en: 'Figure 6: Visual results of different methods on a low-light image sampled
    from MIT-Adobe FiveK-test dataset.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：来自 MIT-Adobe FiveK-test 数据集的低光图像上不同方法的视觉结果。
- en: 4.2 Online Evaluation Platform
  id: totrans-596
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 在线评估平台
- en: Different deep models may be implemented in different platforms such as Caffe,
    Theano, TensorFlow, and PyTorch. As a result, different algorithms demand different
    configurations, GPU versions, and hardware specifications. Such requirements are
    prohibitive to many researchers, especially for beginners who are new to this
    area and may not even have GPU resources. To resolve these problems, we develop
    an LLIE online platform, called LLIE-Platform, which is available at [http://mc.nankai.edu.cn/ll/](http://mc.nankai.edu.cn/ll/).
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的深度模型可以在不同的平台上实现，如 Caffe、Theano、TensorFlow 和 PyTorch。因此，不同的算法需要不同的配置、GPU 版本和硬件规格。这些要求对许多研究人员来说是难以承受的，尤其是对那些刚入门且可能没有
    GPU 资源的初学者。为了解决这些问题，我们开发了一个 LLIE 在线平台，称为 LLIE-Platform，网址为 [http://mc.nankai.edu.cn/ll/](http://mc.nankai.edu.cn/ll/)。
- en: To the date of this submission, the LLIE-Platform covers 14 popular deep learning-based
    LLIE methods including LLNet [[1](#bib.bib1)], LightenNet [[5](#bib.bib5)], Retinex-Net
    [[4](#bib.bib4)], EnlightenGAN [[26](#bib.bib26)], MBLLEN [[3](#bib.bib3)], KinD
    [[11](#bib.bib11)], KinD++ [[61](#bib.bib61)], TBEFN [[20](#bib.bib20)], DSLR
    [[21](#bib.bib21)], DRBN [[33](#bib.bib33)], ExCNet [[27](#bib.bib27)], Zero-DCE
    [[28](#bib.bib28)], Zero-DCE++ [[30](#bib.bib30)], and RRDNet [[29](#bib.bib29)],
    where the results of any input can be produced through a user-friendly web interface.
    We will regularly offer new methods on this platform. We wish that this LLIE-Platform
    could serve the growing research community by providing users a flexible interface
    to run existing deep learning-based LLIE methods and develop their own new LLIE
    methods.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 截至提交之日，LLIE-Platform 覆盖了包括 LLNet [[1](#bib.bib1)]、LightenNet [[5](#bib.bib5)]、Retinex-Net
    [[4](#bib.bib4)]、EnlightenGAN [[26](#bib.bib26)]、MBLLEN [[3](#bib.bib3)]、KinD
    [[11](#bib.bib11)]、KinD++ [[61](#bib.bib61)]、TBEFN [[20](#bib.bib20)]、DSLR [[21](#bib.bib21)]、DRBN
    [[33](#bib.bib33)]、ExCNet [[27](#bib.bib27)]、Zero-DCE [[28](#bib.bib28)]、Zero-DCE++
    [[30](#bib.bib30)] 和 RRDNet [[29](#bib.bib29)] 在内的 14 种流行的深度学习基础 LLIE 方法，其中任何输入的结果都可以通过用户友好的网页界面生成。我们将定期在此平台上提供新方法。我们希望
    LLIE-Platform 能通过提供灵活的接口，帮助不断增长的研究社区运行现有的深度学习基础 LLIE 方法，并开发他们自己的新 LLIE 方法。
- en: '| ![Refer to caption](img/240239c26df304051fafcc5fdf003fed.png) | ![Refer to
    caption](img/3d7862df1db6f481305cb6b43a2c95c9.png) | ![Refer to caption](img/8a6ed72a21ae3cb38f0286a20d5f14ef.png)
    | ![Refer to caption](img/3e164b60ff575f8e57b173643b43aefd.png) | ![Refer to caption](img/f05e45efb338edf24c76f6a636a4999e.png)
    | ![Refer to caption](img/f9a383552b7b23a083f5ca8e603a7585.png) | ![Refer to caption](img/4aef6b046430d6cd73ea3d294dc911ec.png)
    |  |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/240239c26df304051fafcc5fdf003fed.png) | ![参考说明](img/3d7862df1db6f481305cb6b43a2c95c9.png)
    | ![参考说明](img/8a6ed72a21ae3cb38f0286a20d5f14ef.png) | ![参考说明](img/3e164b60ff575f8e57b173643b43aefd.png)
    | ![参考说明](img/f05e45efb338edf24c76f6a636a4999e.png) | ![参考说明](img/f9a383552b7b23a083f5ca8e603a7585.png)
    | ![参考说明](img/4aef6b046430d6cd73ea3d294dc911ec.png) |  |'
- en: '| (a) | (b) | (c) | (d) | (e) | (f) | (g) |  |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| (a) | (b) | (c) | (d) | (e) | (f) | (g) |  |'
- en: '| ![Refer to caption](img/3ce2762623e7008563487b79db8d72df.png) | ![Refer to
    caption](img/f690645750495c695fb32d992e37cd45.png) | ![Refer to caption](img/7004379667dcf0b7199a9af933584757.png)
    | ![Refer to caption](img/76ae236e44766ae3aeb47f4eafe6c3eb.png) | ![Refer to caption](img/79de4fa5501f63d6939556ac7c8a2d5f.png)
    | ![Refer to caption](img/501151d4631af3a252cbae243196672d.png) | ![Refer to caption](img/92d46ac6145fecda543b87676f5cf028.png)
    |  |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/3ce2762623e7008563487b79db8d72df.png) | ![参考说明](img/f690645750495c695fb32d992e37cd45.png)
    | ![参考说明](img/7004379667dcf0b7199a9af933584757.png) | ![参考说明](img/76ae236e44766ae3aeb47f4eafe6c3eb.png)
    | ![参考说明](img/79de4fa5501f63d6939556ac7c8a2d5f.png) | ![参考说明](img/501151d4631af3a252cbae243196672d.png)
    | ![参考说明](img/92d46ac6145fecda543b87676f5cf028.png) |'
- en: '| (h) | (i) | (j) | (k) | (l) | (m) | (n) |  |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| (h) | (i) | (j) | (k) | (l) | (m) | (n) |  |'
- en: 'Figure 7: Visual results of different methods on a low-light image sampled
    from LLIV-Phone-imgT dataset. (a) input. (b) LLNet [[1](#bib.bib1)]. (c) LightenNet
    [[5](#bib.bib5)]. (d) Retinex-Net [[4](#bib.bib4)]. (e) MBLLEN [[3](#bib.bib3)].
    (f) KinD [[11](#bib.bib11)]. (g) KinD++ [[61](#bib.bib61)]. (h) TBEFN [[20](#bib.bib20)].
    (i) DSLR [[21](#bib.bib21)]. (j) EnlightenGAN [[26](#bib.bib26)]. (k) DRBN [[33](#bib.bib33)].
    (l) ExCNet [[27](#bib.bib27)]. (m) Zero-DCE [[28](#bib.bib28)]. (n) RRDNet [[29](#bib.bib29)].'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：来自 LLIV-Phone-imgT 数据集的低光照图像不同方法的视觉结果。 (a) 输入图像。 (b) LLNet [[1](#bib.bib1)]。
    (c) LightenNet [[5](#bib.bib5)]。 (d) Retinex-Net [[4](#bib.bib4)]。 (e) MBLLEN
    [[3](#bib.bib3)]。 (f) KinD [[11](#bib.bib11)]。 (g) KinD++ [[61](#bib.bib61)]。
    (h) TBEFN [[20](#bib.bib20)]。 (i) DSLR [[21](#bib.bib21)]。 (j) EnlightenGAN [[26](#bib.bib26)]。
    (k) DRBN [[33](#bib.bib33)]。 (l) ExCNet [[27](#bib.bib27)]。 (m) Zero-DCE [[28](#bib.bib28)]。
    (n) RRDNet [[29](#bib.bib29)]。
- en: '| ![Refer to caption](img/26d9d849145089665d56680fc9cec5d7.png) | ![Refer to
    caption](img/937a56ae27589d9240734fa6e1ea37e4.png) | ![Refer to caption](img/30d4bcbced36618e25a73177cbb7dd38.png)
    | ![Refer to caption](img/4c9d1f69b70ba039b216fd7f0aa37bbd.png) | ![Refer to caption](img/272104ba9ecfe994c95ef8d7535f7d5c.png)
    | ![Refer to caption](img/0caa6284058f72ffde4708deda63fca8.png) | ![Refer to caption](img/2624a152cc77414acecfe03c90a8f6b3.png)
    |  |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/26d9d849145089665d56680fc9cec5d7.png) | ![参考说明](img/937a56ae27589d9240734fa6e1ea37e4.png)
    | ![参考说明](img/30d4bcbced36618e25a73177cbb7dd38.png) | ![参考说明](img/4c9d1f69b70ba039b216fd7f0aa37bbd.png)
    | ![参考说明](img/272104ba9ecfe994c95ef8d7535f7d5c.png) | ![参考说明](img/0caa6284058f72ffde4708deda63fca8.png)
    | ![参考说明](img/2624a152cc77414acecfe03c90a8f6b3.png) |  |'
- en: '| (a) | (b) | (c) | (d) | (e) | (f) | (g) |  |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| (a) | (b) | (c) | (d) | (e) | (f) | (g) |  |'
- en: '| ![Refer to caption](img/e7d192b0fc44d92262a519120d0fc4f6.png) | ![Refer to
    caption](img/626e64c0472fa2b43a3bc57026e59f78.png) | ![Refer to caption](img/95835e303bdae7e5803b2b15fa05c09e.png)
    | ![Refer to caption](img/2fc3a9d5058769a6c2aad74c7d693804.png) | ![Refer to caption](img/f40fe1fa766e81020e9c5072becc9057.png)
    | ![Refer to caption](img/56540a06e92f2ae2f44c3a2d9ed9c0d2.png) | ![Refer to caption](img/b332e26780e0527d91758dd49034c88f.png)
    |  |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/e7d192b0fc44d92262a519120d0fc4f6.png) | ![参见说明](img/626e64c0472fa2b43a3bc57026e59f78.png)
    | ![参见说明](img/95835e303bdae7e5803b2b15fa05c09e.png) | ![参见说明](img/2fc3a9d5058769a6c2aad74c7d693804.png)
    | ![参见说明](img/f40fe1fa766e81020e9c5072becc9057.png) | ![参见说明](img/56540a06e92f2ae2f44c3a2d9ed9c0d2.png)
    | ![参见说明](img/b332e26780e0527d91758dd49034c88f.png) |  |'
- en: '| (h) | (i) | (j) | (k) | (l) | (m) | (n) |  |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| (h) | (i) | (j) | (k) | (l) | (m) | (n) |  |'
- en: 'Figure 8: Visual results of different methods on a low-light image sampled
    from LLIV-Phone-imgT dataset. (a) input. (b) LLNet [[1](#bib.bib1)]. (c) LightenNet
    [[5](#bib.bib5)]. (d) Retinex-Net [[4](#bib.bib4)]. (e) MBLLEN [[3](#bib.bib3)].
    (f) KinD [[11](#bib.bib11)]. (g) KinD++ [[61](#bib.bib61)]. (h) TBEFN [[20](#bib.bib20)].
    (i) DSLR [[21](#bib.bib21)]. (j) EnlightenGAN [[26](#bib.bib26)]. (k) DRBN [[33](#bib.bib33)].
    (l) ExCNet [[27](#bib.bib27)]. (m) Zero-DCE [[28](#bib.bib28)]. (n) RRDNet [[29](#bib.bib29)].'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：不同方法在 LLIV-Phone-imgT 数据集中采样的低光图像上的视觉结果。 (a) 输入。 (b) LLNet [[1](#bib.bib1)]。
    (c) LightenNet [[5](#bib.bib5)]。 (d) Retinex-Net [[4](#bib.bib4)]。 (e) MBLLEN
    [[3](#bib.bib3)]。 (f) KinD [[11](#bib.bib11)]。 (g) KinD++ [[61](#bib.bib61)]。
    (h) TBEFN [[20](#bib.bib20)]。 (i) DSLR [[21](#bib.bib21)]。 (j) EnlightenGAN [[26](#bib.bib26)]。
    (k) DRBN [[33](#bib.bib33)]。 (l) ExCNet [[27](#bib.bib27)]。 (m) Zero-DCE [[28](#bib.bib28)]。
    (n) RRDNet [[29](#bib.bib29)]。
- en: '|  Bayer            APS-C X-Trans  | ![Refer to caption](img/ef774913bc25c6f679de87ab2bffce1c.png)
    |    ![Refer to caption](img/69e61d87c3bc522a57cb31b6ea092174.png) |    ![Refer
    to caption](img/dabffbc6fa15442d8bfb49b7330d1414.png) |    ![Refer to caption](img/3c026a23f4434c08d53972c1ab7d02b7.png)
    |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '|  Bayer            APS-C X-Trans  | ![参见说明](img/ef774913bc25c6f679de87ab2bffce1c.png)
    |    ![参见说明](img/69e61d87c3bc522a57cb31b6ea092174.png) |    ![参见说明](img/dabffbc6fa15442d8bfb49b7330d1414.png)
    |    ![参见说明](img/3c026a23f4434c08d53972c1ab7d02b7.png) |'
- en: '|  |   (a) inputs |    (b) SID [[85](#bib.bib85)] |    (c) EEMEFN [[16](#bib.bib16)]
    |    (d) GT |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '|  |   (a) 输入 |    (b) SID [[85](#bib.bib85)] |    (c) EEMEFN [[16](#bib.bib16)]
    |    (d) GT |'
- en: 'Figure 9: Visual results of different methods on two raw low-light images sampled
    from SID-test-Bayer and SID-test-X-Trans test datasets. The inputs are amplified
    for visualization.'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：不同方法在 SID-test-Bayer 和 SID-test-X-Trans 测试数据集中采样的两张原始低光图像上的视觉结果。为了可视化，输入被放大。
- en: 4.3 Benchmarking Results
  id: totrans-612
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基准测试结果
- en: To qualitatively and quantitatively evaluate different methods, in addition
    to the proposed LLIV-Phone dataset, we also adopt the commonly used LOL [[4](#bib.bib4)]
    and MIT-Adobe FiveK [[79](#bib.bib79)] datasets for RGB format-based methods,
    and SID [[85](#bib.bib85)] dataset for raw format-based methods. More visual results
    can be found in the supplementary material. The comparative results on the real
    low-light videos taken by different mobile phones’ cameras can be found at YouTube
    [https://www.youtube.com/watch?v=Elo9TkrG5Oo&t=6s](https://www.youtube.com/watch?v=Elo9TkrG5Oo&t=6s).
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定性和定量评估不同的方法，除了提出的 LLIV-Phone 数据集外，我们还采用了常用的 LOL [[4](#bib.bib4)] 和 MIT-Adobe
    FiveK [[79](#bib.bib79)] 数据集用于 RGB 格式的方法，以及 SID [[85](#bib.bib85)] 数据集用于原始格式的方法。更多的视觉结果可以在补充材料中找到。不同手机摄像头拍摄的真实低光视频的对比结果可以在
    YouTube [https://www.youtube.com/watch?v=Elo9TkrG5Oo&t=6s](https://www.youtube.com/watch?v=Elo9TkrG5Oo&t=6s)
    中查看。
- en: We select five images on average from each video of the LLIV-Phone dataset,
    forming an image testing dataset with a total of 600 images (denoted as LLIV-Phone-imgT).
    Furthermore, we randomly select one video from the videos of each phone’s brand
    of LLIV-Phone dataset, forming a video testing dataset with a total of 18 videos
    (denoted as LLIV-Phone-vidT). We half the resolutions of the frames in both LLIV-Phone-imgT
    and LLIV-Phone-vidT because some deep learning-based methods cannot process the
    full resolution of test images and videos. For the LOL dataset, we adopt the original
    test set including 15 low-light images captured in real scenes for testing, denoted
    as LOL-test. For the MIT-Adobe FiveK dataset, we follow the protocol in Chen et
    al. [[47](#bib.bib47)] to decode the images into PNG format and resize them to
    have a long edge of 512 pixels using Lightroom. We adopt the same testing dataset
    as Chen et al. [[47](#bib.bib47)], MIT-Adobe FiveK-test, including 500 images
    with the retouching results by expert C as the corresponding ground truths. For
    the SID dataset, we use the default test set used in EEMEFN [[16](#bib.bib16)]
    for fair comparisons, denoted as SID-test (SID-test-Bayer and SID-test-X-Trans),
    which is a partial test set of SID [[85](#bib.bib85)]. The SID-test-Bayer includes
    93 images of the Bayer pattern while the SID-test-X-Trans includes 94 images of
    the APS-C X-Trans pattern.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 我们平均从LLIV-Phone数据集的每个视频中选择五张图像，形成一个包含600张图像的测试数据集（标记为LLIV-Phone-imgT）。此外，我们从LLIV-Phone数据集中每个手机品牌的视频中随机选择一个视频，形成一个包含18个视频的测试数据集（标记为LLIV-Phone-vidT）。我们将LLIV-Phone-imgT和LLIV-Phone-vidT中的帧分辨率减半，因为一些基于深度学习的方法无法处理全分辨率的测试图像和视频。对于LOL数据集，我们采用包括15张在真实场景中捕捉的低光图像的原始测试集进行测试，标记为LOL-test。对于MIT-Adobe
    FiveK数据集，我们按照Chen等人[[47](#bib.bib47)]的协议将图像解码为PNG格式，并使用Lightroom将其长边调整为512像素。我们采用与Chen等人[[47](#bib.bib47)]相同的测试数据集MIT-Adobe
    FiveK-test，包括500张由专家C修饰的图像作为相应的真实值。对于SID数据集，我们使用EEMEFN [[16](#bib.bib16)]中使用的默认测试集进行公平比较，标记为SID-test（SID-test-Bayer和SID-test-X-Trans），这是SID
    [[85](#bib.bib85)]的一个部分测试集。SID-test-Bayer包括93张Bayer模式的图像，而SID-test-X-Trans包括94张APS-C
    X-Trans模式的图像。
- en: 'TABLE V: Quantitative comparisons on LOL-test and MIT-Adobe FiveK-test test
    datasets in terms of MSE ($\times 10^{3}$), PSNR (in dB), SSIM [[86](#bib.bib86)],
    and LPIPS [[87](#bib.bib87)]. The best result is in red whereas the second and
    third best results are in blue and purple under each case, respectively.'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 表V：在LOL-test和MIT-Adobe FiveK-test测试数据集上，MSE（$\times 10^{3}$）、PSNR（dB）、SSIM [[86](#bib.bib86)]和LPIPS
    [[87](#bib.bib87)]的定量比较。最佳结果用红色标记，第二和第三最佳结果分别用蓝色和紫色标记。
- en: '| Learning | Method | LOL-test | MIT-Adobe FiveK-test |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| 学习 | 方法 | LOL-test | MIT-Adobe FiveK-test |'
- en: '| MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |
    MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |
    MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |'
- en: '|  | input | 12.613 | 7.773 | 0.181 | 0.560 | 1.670 | 17.824 | 0.779 | 0.148
    |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入 | 12.613 | 7.773 | 0.181 | 0.560 | 1.670 | 17.824 | 0.779 | 0.148 |'
- en: '|  | LLNet [[1](#bib.bib1)] | 1.290 | 17.959 | 0.713 | 0.360 | 4.465 | 12.177
    | 0.645 | 0.292 |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '|  | LLNet [[1](#bib.bib1)] | 1.290 | 17.959 | 0.713 | 0.360 | 4.465 | 12.177
    | 0.645 | 0.292 |'
- en: '|  | LightenNet [[5](#bib.bib5)] | 7.614 | 10.301 | 0.402 | 0.394 | 4.127 |
    13.579 | 0.744 | 0.166 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '|  | LightenNet [[5](#bib.bib5)] | 7.614 | 10.301 | 0.402 | 0.394 | 4.127 |
    13.579 | 0.744 | 0.166 |'
- en: '|  | Retinex-Net [[4](#bib.bib4)] | 1.651 | 16.774 | 0.462 | 0.474 | 4.406
    | 12.310 | 0.671 | 0.239 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '|  | Retinex-Net [[4](#bib.bib4)] | 1.651 | 16.774 | 0.462 | 0.474 | 4.406
    | 12.310 | 0.671 | 0.239 |'
- en: '| SL | MBLLEN [[3](#bib.bib3)] | 1.444 | 17.902 | 0.715 | 0.247 | 1.296 | 19.781
    | 0.825 | 0.108 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| SL | MBLLEN [[3](#bib.bib3)] | 1.444 | 17.902 | 0.715 | 0.247 | 1.296 | 19.781
    | 0.825 | 0.108 |'
- en: '|  | KinD [[11](#bib.bib11)] | 1.431 | 17.648 | 0.779 | 0.175 | 2.675 | 14.535
    | 0.741 | 0.177 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '|  | KinD [[11](#bib.bib11)] | 1.431 | 17.648 | 0.779 | 0.175 | 2.675 | 14.535
    | 0.741 | 0.177 |'
- en: '|  | KinD++ [[61](#bib.bib61)] | 1.298 | 17.752 | 0.760 | 0.198 | 7.582 | 9.732
    | 0.568 | 0.336 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '|  | KinD++ [[61](#bib.bib61)] | 1.298 | 17.752 | 0.760 | 0.198 | 7.582 | 9.732
    | 0.568 | 0.336 |'
- en: '|  | TBEFN [[20](#bib.bib20)] | 1.764 | 17.351 | 0.786 | 0.210 | 3.865 | 12.769
    | 0.704 | 0.178 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '|  | TBEFN [[20](#bib.bib20)] | 1.764 | 17.351 | 0.786 | 0.210 | 3.865 | 12.769
    | 0.704 | 0.178 |'
- en: '|  | DSLR [[21](#bib.bib21)] | 3.536 | 15.050 | 0.597 | 0.337 | 1.925 | 16.632
    | 0.782 | 0.167 |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '|  | DSLR [[21](#bib.bib21)] | 3.536 | 15.050 | 0.597 | 0.337 | 1.925 | 16.632
    | 0.782 | 0.167 |'
- en: '| UL | EnlightenGAN [[26](#bib.bib26)] | 1.998 | 17.483 | 0.677 | 0.322 | 3.628
    | 13.260 | 0.745 | 0.170 |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| UL | EnlightenGAN [[26](#bib.bib26)] | 1.998 | 17.483 | 0.677 | 0.322 | 3.628
    | 13.260 | 0.745 | 0.170 |'
- en: '| SSL | DRBN [[33](#bib.bib33)] | 2.359 | 15.125 | 0.472 | 0.316 | 3.314 |
    13.355 | 0.378 | 0.281 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| SSL | DRBN [[33](#bib.bib33)] | 2.359 | 15.125 | 0.472 | 0.316 | 3.314 |
    13.355 | 0.378 | 0.281 |'
- en: '|  | ExCNet [[27](#bib.bib27)] | 2.292 | 15.783 | 0.515 | 0.373 | 2.927 | 13.978
    | 0.710 | 0.187 |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '|  | ExCNet [[27](#bib.bib27)] | 2.292 | 15.783 | 0.515 | 0.373 | 2.927 | 13.978
    | 0.710 | 0.187 |'
- en: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 3.282 | 14.861 | 0.589 | 0.335 | 3.476
    | 13.199 | 0.709 | 0.203 |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 3.282 | 14.861 | 0.589 | 0.335 | 3.476
    | 13.199 | 0.709 | 0.203 |'
- en: '|  | RRDNet [[29](#bib.bib29)] | 6.313 | 11.392 | 0.468 | 0.361 | 7.057 | 10.135
    | 0.620 | 0.303 |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '|  | RRDNet [[29](#bib.bib29)] | 6.313 | 11.392 | 0.468 | 0.361 | 7.057 | 10.135
    | 0.620 | 0.303 |'
- en: 'TABLE VI: Quantitative comparisons on SID-test test dataset in terms of MSE
    ($\times 10^{3}$), PSNR (in dB), SSIM [[86](#bib.bib86)], and LPIPS [[87](#bib.bib87)].
    The best result is in red under each case. To compute the quantitative scores
    of input raw data, we use the corresponding camera ISP pipelines provided by Chen
    et al. [[85](#bib.bib85)] to transfer raw data to RGB format.'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 在 SID-test 测试数据集上的定量比较，涉及 MSE ($\times 10^{3}$), PSNR（单位 dB），SSIM [[86](#bib.bib86)]
    和 LPIPS [[87](#bib.bib87)]。每种情况的最佳结果用红色标出。为了计算输入原始数据的定量评分，我们使用了 Chen 等人 [[85](#bib.bib85)]
    提供的相机 ISP 流程将原始数据转换为 RGB 格式。'
- en: '| Learning | Method | SID-test–Bayer | SID-test–X-Trans |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| 学习 | 方法 | SID-test–Bayer | SID-test–X-Trans |'
- en: '| MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |
    MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |
    MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |'
- en: '|  | input | 5.378 | 11.840 | 0.063 | 0.711 | 4.803 | 11.880 | 0.075 | 0.796
    |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入 | 5.378 | 11.840 | 0.063 | 0.711 | 4.803 | 11.880 | 0.075 | 0.796 |'
- en: '| SL | SID [[85](#bib.bib85)] | 0.140 | 28.614 | 0.757 | 0.465 | 0.235 | 26.663
    | 0.680 | 0.586 |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| SL | SID [[85](#bib.bib85)] | 0.140 | 28.614 | 0.757 | 0.465 | 0.235 | 26.663
    | 0.680 | 0.586 |'
- en: '|  | EEMEFN [[16](#bib.bib16)] | 0.126 | 29.212 | 0.768 | 0.448 | 0.191 | 27.423
    | 0.695 | 0.546 |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '|  | EEMEFN [[16](#bib.bib16)] | 0.126 | 29.212 | 0.768 | 0.448 | 0.191 | 27.423
    | 0.695 | 0.546 |'
- en: 'Qualitative Comparison. We first present the results of different methods on
    the images sampled from LOL-test and MIT-Adobe FiveK-test datasets in Figures
    [5](#S4.F5 "Figure 5 ‣ 4.1 A New Low-Light Image and Video Dataset ‣ 4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey") and [6](#S4.F6 "Figure 6 ‣ 4.1 A New Low-Light Image and Video Dataset
    ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey").'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 定性比较。我们首先展示了不同方法在从 LOL-test 和 MIT-Adobe FiveK-test 数据集中采样的图像上的结果，如图 [5](#S4.F5
    "图 5 ‣ 4.1 新的低光图像和视频数据集 ‣ 4 基准测试和实证分析 ‣ 基于深度学习的低光图像和视频增强：综述") 和 [6](#S4.F6 "图
    6 ‣ 4.1 新的低光图像和视频数据集 ‣ 4 基准测试和实证分析 ‣ 基于深度学习的低光图像和视频增强：综述")。
- en: 'As shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.1 A New Low-Light Image and Video
    Dataset ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey"), all methods improve the brightness and contrast
    of the input image. However, none of them successfully recovers the accurate color
    of the input image when the results are compared with the ground truth. In particular,
    LLNet [[1](#bib.bib1)] produces blurring result. LightenNet [[5](#bib.bib5)] and
    RRDNet [[29](#bib.bib29)] produce under-exposed results while MBLLEN [[3](#bib.bib3)]
    and ExCNet [[27](#bib.bib27)] over-expose the image. KinD [[11](#bib.bib11)],
    KinD++ [[61](#bib.bib61)], TBEFN [[20](#bib.bib20)], DSLR [[21](#bib.bib21)],
    EnlightenGAN [[26](#bib.bib26)], and DRBN [[33](#bib.bib33)] introduce obvious
    artifacts. In Figure [6](#S4.F6 "Figure 6 ‣ 4.1 A New Low-Light Image and Video
    Dataset ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey"), LLNet [[5](#bib.bib5)], KinD++ [[61](#bib.bib61)],
    TBEFN [[20](#bib.bib20)], and RRDNet [[29](#bib.bib29)] produce over-exposed results.
    Retinex-Net [[4](#bib.bib4)], KinD++ [[61](#bib.bib61)], and RRDNet [[29](#bib.bib29)]
    yield artifacts and blurring in the results.'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[5](#S4.F5 "Figure 5 ‣ 4.1 A New Low-Light Image and Video Dataset ‣ 4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey")所示，所有方法都改善了输入图像的亮度和对比度。然而，当结果与真实情况进行比较时，没有一种方法能够成功恢复输入图像的准确颜色。特别是，LLNet
    [[1](#bib.bib1)] 产生了模糊的结果。LightenNet [[5](#bib.bib5)] 和 RRDNet [[29](#bib.bib29)]
    产生了曝光不足的结果，而 MBLLEN [[3](#bib.bib3)] 和 ExCNet [[27](#bib.bib27)] 则过度曝光。KinD [[11](#bib.bib11)]、KinD++
    [[61](#bib.bib61)]、TBEFN [[20](#bib.bib20)]、DSLR [[21](#bib.bib21)]、EnlightenGAN
    [[26](#bib.bib26)] 和 DRBN [[33](#bib.bib33)] 引入了明显的伪影。在图[6](#S4.F6 "Figure 6 ‣
    4.1 A New Low-Light Image and Video Dataset ‣ 4 Benchmarking and Empirical Analysis
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")中，LLNet
    [[5](#bib.bib5)]、KinD++ [[61](#bib.bib61)]、TBEFN [[20](#bib.bib20)] 和 RRDNet [[29](#bib.bib29)]
    产生了过度曝光的结果。Retinex-Net [[4](#bib.bib4)]、KinD++ [[61](#bib.bib61)] 和 RRDNet [[29](#bib.bib29)]
    在结果中出现了伪影和模糊。'
- en: We found that the ground truths of MIT-Adobe FiveK dataset still contain some
    dark regions. This is because the dataset is originally designed for global image
    retouching, where restoring low light regions is not the main priority in this
    task. We also observed that the input images in LOL dataset and MIT-Adobe FiveK
    dataset are relatively clean from noise, which is different from real low-light
    scenes. Although some methods [[60](#bib.bib60), [18](#bib.bib18), [21](#bib.bib21)]
    take the MIT-Adobe FiveK dataset as the training or testing dataset, we argue
    that this dataset is not appropriate for the task of LLIE due to its mismatched/unsatisfactory
    ground truth for LLIE.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现 MIT-Adobe FiveK 数据集的真实图像仍然包含一些暗区。这是因为该数据集最初设计用于全球图像修饰，其中恢复低光区域不是任务的主要优先级。我们还观察到
    LOL 数据集和 MIT-Adobe FiveK 数据集中的输入图像相对干净，没有噪声，这与真实的低光场景不同。尽管一些方法 [[60](#bib.bib60)、[18](#bib.bib18)、[21](#bib.bib21)]
    将 MIT-Adobe FiveK 数据集作为训练或测试数据集，我们认为该数据集不适用于 LLIE 任务，因为其与 LLIE 不匹配或不令人满意的真实情况。
- en: 'To examine the generalization capability of different methods, we conduct comparisons
    on the images sampled from our LLIV-Phone-imgT dataset. The visual results of
    different methods are shown in Figures [7](#S4.F7 "Figure 7 ‣ 4.2 Online Evaluation
    Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey") and [8](#S4.F8 "Figure 8 ‣ 4.2 Online Evaluation
    Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey"). As presented in Figure [7](#S4.F7 "Figure 7 ‣
    4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey"), all methods cannot
    effectively improve the brightness and remove the noise of the input low-light
    image. Moreover, Retinex-Net [[4](#bib.bib4)], MBLLEN [[3](#bib.bib3)], and DRBN
    [[33](#bib.bib33)] produce obvious artifacts. In Figure [8](#S4.F8 "Figure 8 ‣
    4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey"), all methods enhance
    the brightness of this input image. However, only MBLLEN [[3](#bib.bib3)] and
    RRDNet [[29](#bib.bib29)] obtain visually pleasing enhancement without color deviation,
    artifacts, and over-/under-exposure. Notably, for regions with a light source,
    none of the methods can brighten the image without amplifying the noise around
    these regions. Taking light sources into account for LLIE would be an interesting
    direction to explore. The results suggest the difficulty of enhancing the images
    of the LLIV-Phone-imgT dataset. Real low-light images fail most existing LLIE
    methods due to the limited generalization capability of these methods. The potential
    reasons are the use of synthetic training data, small-scaled training data, or
    unrealistic assumptions such as the local illumination consistency and treating
    the reflectance component as the final result in the Retinex model in these methods.'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '为了检验不同方法的泛化能力，我们对从LLIV-Phone-imgT数据集中采样的图像进行了比较。不同方法的视觉结果展示在图[7](#S4.F7 "Figure
    7 ‣ 4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey")和图[8](#S4.F8 "Figure
    8 ‣ 4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey")中。如图[7](#S4.F7 "Figure
    7 ‣ 4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey")所示，所有方法都无法有效提高输入低光图像的亮度并去除噪声。此外，Retinex-Net
    [[4](#bib.bib4)]、MBLLEN [[3](#bib.bib3)]和DRBN [[33](#bib.bib33)]产生了明显的伪影。在图[8](#S4.F8
    "Figure 8 ‣ 4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")中，所有方法都增强了输入图像的亮度。然而，只有MBLLEN
    [[3](#bib.bib3)]和RRDNet [[29](#bib.bib29)]在视觉上获得了令人满意的增强效果，没有颜色偏差、伪影或过/欠曝光。值得注意的是，对于有光源的区域，没有一种方法能够在不放大这些区域噪声的情况下提亮图像。考虑到光源的LLIE将是一个有趣的研究方向。结果表明，增强LLIV-Phone-imgT数据集的图像非常困难。真实的低光图像由于这些方法的泛化能力有限，几乎无法被现有的LLIE方法处理。潜在原因包括使用合成训练数据、小规模训练数据，或不切实际的假设，如局部光照一致性和在Retinex模型中将反射成分视为最终结果。'
- en: 'We further present the visual comparisons of raw format-based methods in Figure
    [9](#S4.F9 "Figure 9 ‣ 4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical
    Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey").
    As shown, the input raw data have obvious noises. Both SID [[2](#bib.bib2)] and
    EEMEFN [[16](#bib.bib16)] can effectively remove the effects of noises. In comparison
    to the simple U-Net structure used in SID [[2](#bib.bib2)], the more complex structure
    of EEMEFN [[16](#bib.bib16)] obtains better brightness recovery. However, their
    results are far from the corresponding GT, especially for the input of APS-C X-Trans
    pattern.'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步展示了基于原始格式方法的视觉比较，如图[9](#S4.F9 "Figure 9 ‣ 4.2 Online Evaluation Platform
    ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey")所示。可以看出，输入的原始数据存在明显的噪声。SID [[2](#bib.bib2)]和EEMEFN
    [[16](#bib.bib16)]都能有效去除噪声的影响。与SID [[2](#bib.bib2)]中使用的简单U-Net结构相比，EEMEFN [[16](#bib.bib16)]的复杂结构在亮度恢复方面表现更好。然而，它们的结果仍然远离相应的GT，尤其是在APS-C
    X-Trans模式的输入下。'
- en: 'Quantitative Comparison. For test sets with ground truth i.e., LOL-test, MIT-Adobe
    FiveK-test, and SID-test, we adopt MSE, PSNR, SSIM [[86](#bib.bib86)], and LPIPS
    [[87](#bib.bib87)] metrics to quantitatively compare different methods. LPIPS
    [[87](#bib.bib87)] is a deep learning-based image quality assessment metric that
    measures the perceptual similarity between a result and its corresponding ground
    truth by deep visual representations. For LPIPS, we employ the AlexNet-based model
    to compute the perceptual similarity. A lower LPIPS value suggests a result that
    is closer to the corresponding ground truth in terms of perceptual similarity.
    In Table [V](#S4.T5 "TABLE V ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking and Empirical
    Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")
    and Table [VI](#S4.T6 "TABLE VI ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking and
    Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"), we show the quantitative results of RGB format-based methods and raw
    format-based methods, respectively.'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '定量比较。对于具有真实标签的测试集，如 LOL-test、MIT-Adobe FiveK-test 和 SID-test，我们采用 MSE、PSNR、SSIM
    [[86](#bib.bib86)] 和 LPIPS [[87](#bib.bib87)] 指标来定量比较不同方法。LPIPS [[87](#bib.bib87)]
    是一种基于深度学习的图像质量评估指标，通过深度视觉表示来衡量结果与其对应真实标签之间的感知相似度。对于 LPIPS，我们采用基于 AlexNet 的模型来计算感知相似度。较低的
    LPIPS 值表明结果在感知相似度上更接近对应的真实标签。在表格 [V](#S4.T5 "TABLE V ‣ 4.3 Benchmarking Results
    ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey") 和表格 [VI](#S4.T6 "TABLE VI ‣ 4.3 Benchmarking Results
    ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey") 中，我们分别展示了基于 RGB 格式的方法和基于原始格式的方法的定量结果。'
- en: 'As presented in Table [V](#S4.T5 "TABLE V ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"), the quantitative scores of supervised learning-based methods are better
    than those of unsupervised learning-based, semi-supervised learning-based, and
    zero-shot learning-based methods on LOL-test and MIT-Adobe FiveK-test datasets.
    Among them, LLNet [[1](#bib.bib1)] obtains the best MSE and PSNR values on the
    LOL-test dataset; however, its performance drops on the MIT-Adobe FiveK-test dataset.
    This may be caused by the bias of LLNet [[1](#bib.bib1)] towards the LOL dataset
    since it was trained using the LOL training dataset. For the LOL-test dataset,
    TBEFN [[20](#bib.bib20)] obtains the highest SSIM value while KinD [[11](#bib.bib11)]
    achieves the lowest LPIPS value. There is no winner across these four evaluation
    metrics on the LOL-test dataset despite the fact that some methods were trained
    on the LOL training dataset. For the MIT-Adobe FiveK-test dataset, MBLLEN [[3](#bib.bib3)]
    outperforms all compared methods under the four evaluation metrics in spite of
    being trained on synthetic training data. Nevertheless, MBLLEN [[3](#bib.bib3)]
    still cannot obtain the best performance on both two test datasets.'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格 [V](#S4.T5 "TABLE V ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking and Empirical
    Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")
    所示，监督学习方法的定量评分优于无监督学习、半监督学习和零样本学习方法在 LOL-test 和 MIT-Adobe FiveK-test 数据集上的表现。其中，LLNet
    [[1](#bib.bib1)] 在 LOL-test 数据集上获得了最佳的 MSE 和 PSNR 值，但在 MIT-Adobe FiveK-test 数据集上的表现下降。这可能是由于
    LLNet [[1](#bib.bib1)] 对 LOL 数据集的偏向，因为它是使用 LOL 训练数据集训练的。在 LOL-test 数据集上，TBEFN
    [[20](#bib.bib20)] 获得了最高的 SSIM 值，而 KinD [[11](#bib.bib11)] 实现了最低的 LPIPS 值。尽管某些方法是在
    LOL 训练数据集上训练的，但在 LOL-test 数据集上这四项评估指标中并没有赢家。对于 MIT-Adobe FiveK-test 数据集，MBLLEN
    [[3](#bib.bib3)] 在四项评估指标中优于所有比较方法，尽管它是在合成训练数据上训练的。然而，MBLLEN [[3](#bib.bib3)] 仍未能在两个测试数据集上取得最佳表现。'
- en: 'As presented in Table [VI](#S4.T6 "TABLE VI ‣ 4.3 Benchmarking Results ‣ 4
    Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using
    Deep Learning: A Survey"), both SID [[85](#bib.bib85)] and EEMEFN [[16](#bib.bib16)]
    improve the quality of input raw data. Compared with the quantitative scores of
    SID [[85](#bib.bib85)], EEMEFN [[16](#bib.bib16)] achieves consistently better
    performance across different raw data patterns and evaluation metrics.'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格 [VI](#S4.T6 "TABLE VI ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking and Empirical
    Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")
    所示，SID [[85](#bib.bib85)] 和 EEMEFN [[16](#bib.bib16)] 都能提升输入原始数据的质量。与 SID [[85](#bib.bib85)]
    的定量评分相比，EEMEFN [[16](#bib.bib16)] 在不同的原始数据模式和评估指标中始终表现更佳。'
- en: 'For LLIV-Phone-imgT test set, we use the non-reference IQA metrics, i.e., NIQE
    [[88](#bib.bib88)], perceptual index (PI) [[89](#bib.bib89), [90](#bib.bib90),
    [88](#bib.bib88)], LOE [[37](#bib.bib37)], and SPAQ [[91](#bib.bib91)] to quantitatively
    compare different methods. In terms of LOE, the smaller the LOE value is, the
    better the lightness order is preserved. For NIQE, the smaller the NIQE value
    is, the better the visual quality is. A lower PI value indicates better perceptual
    quality. SPAQ is devised for the perceptual quality assessment of smartphone photography.
    A larger SPAQ value suggests better perceptual quality of smartphone photography.
    The quantitative results are provided in Table [VII](#S4.T7 "TABLE VII ‣ 4.3 Benchmarking
    Results ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey").'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 LLIV-Phone-imgT 测试集，我们使用无参考 IQA 指标，即 NIQE [[88](#bib.bib88)]、感知指数（PI）[[89](#bib.bib89),
    [90](#bib.bib90), [88](#bib.bib88)]、LOE [[37](#bib.bib37)] 和 SPAQ [[91](#bib.bib91)]
    来定量比较不同的方法。在 LOE 方面，LOE 值越小，光亮顺序保持得越好。对于 NIQE，NIQE 值越小，视觉质量越好。较低的 PI 值表明感知质量更好。SPAQ
    是用于智能手机摄影感知质量评估的指标。SPAQ 值越大，表明智能手机摄影的感知质量越好。定量结果见表 [VII](#S4.T7 "TABLE VII ‣ 4.3
    Benchmarking Results ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image
    and Video Enhancement Using Deep Learning: A Survey")。'
- en: 'TABLE VII: Quantitative comparisons on LLIV-Phone-imgT dataset in terms of
    NIQE [[88](#bib.bib88)], LOE [[37](#bib.bib37)], PI [[89](#bib.bib89), [90](#bib.bib90),
    [88](#bib.bib88)], and SPAQ [[91](#bib.bib91)]. The best result is in red whereas
    the second and third best results are in blue and purple under each case, respectively.'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE VII: LLIV-Phone-imgT 数据集的定量比较，涉及到 NIQE [[88](#bib.bib88)]、LOE [[37](#bib.bib37)]、PI
    [[89](#bib.bib89), [90](#bib.bib90), [88](#bib.bib88)] 和 SPAQ [[91](#bib.bib91)]。最佳结果用红色标记，而第二和第三最佳结果分别用蓝色和紫色标记。'
- en: '| Learning | Method | LoLi-Phone-imgT |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| Learning | Method | LoLi-Phone-imgT |'
- en: '| NIQE$\downarrow$ | LOE $\downarrow$ | PI$\downarrow$ | SPAQ$\uparrow$ |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| NIQE$\downarrow$ | LOE $\downarrow$ | PI$\downarrow$ | SPAQ$\uparrow$ |'
- en: '|  | input | 6.99 | 0.00 | 5.86 | 44.45 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '|  | input | 6.99 | 0.00 | 5.86 | 44.45 |'
- en: '|  | LLNet [[1](#bib.bib1)] | 5.86 | 5.86 | 5.66 | 40.56 |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '|  | LLNet [[1](#bib.bib1)] | 5.86 | 5.86 | 5.66 | 40.56 |'
- en: '|  | LightenNet [[5](#bib.bib5)] | 5.34 | 952.33 | 4.58 | 45.74 |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '|  | LightenNet [[5](#bib.bib5)] | 5.34 | 952.33 | 4.58 | 45.74 |'
- en: '|  | Retinex-Net [[4](#bib.bib4)] | 5.01 | 790.21 | 3.48 | 50.95 |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '|  | Retinex-Net [[4](#bib.bib4)] | 5.01 | 790.21 | 3.48 | 50.95 |'
- en: '| SL | MBLLEN [[3](#bib.bib3)] | 5.08 | 220.63 | 4.27 | 42.50 |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| SL | MBLLEN [[3](#bib.bib3)] | 5.08 | 220.63 | 4.27 | 42.50 |'
- en: '|  | KinD [[11](#bib.bib11)] | 4.97 | 405.88 | 4.37 | 44.79 |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '|  | KinD [[11](#bib.bib11)] | 4.97 | 405.88 | 4.37 | 44.79 |'
- en: '|  | KinD++ [[61](#bib.bib61)] | 4.73 | 681.97 | 3.99 | 46.89 |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '|  | KinD++ [[61](#bib.bib61)] | 4.73 | 681.97 | 3.99 | 46.89 |'
- en: '|  | TBEFN [[20](#bib.bib20)] | 4.81 | 552.91 | 4.30 | 44.14 |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '|  | TBEFN [[20](#bib.bib20)] | 4.81 | 552.91 | 4.30 | 44.14 |'
- en: '|  | DSLR [[21](#bib.bib21)] | 4.77 | 447.98 | 4.31 | 41.08 |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '|  | DSLR [[21](#bib.bib21)] | 4.77 | 447.98 | 4.31 | 41.08 |'
- en: '| UL | EnlightenGAN [[26](#bib.bib26)] | 4.79 | 821.87 | 4.19 | 45.48 |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| UL | EnlightenGAN [[26](#bib.bib26)] | 4.79 | 821.87 | 4.19 | 45.48 |'
- en: '| SSL | DRBN [[33](#bib.bib33)] | 5.80 | 885.75 | 5.54 | 42.74 |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| SSL | DRBN [[33](#bib.bib33)] | 5.80 | 885.75 | 5.54 | 42.74 |'
- en: '|  | ExCNet [[27](#bib.bib27)] | 5.55 | 723.56 | 4.38 | 46.74 |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '|  | ExCNet [[27](#bib.bib27)] | 5.55 | 723.56 | 4.38 | 46.74 |'
- en: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 5.82 | 307.09 | 4.76 | 46.85 |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 5.82 | 307.09 | 4.76 | 46.85 |'
- en: '|  | RRDNet [[29](#bib.bib29)] | 5.97 | 142.89 | 4.84 | 45.31 |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '|  | RRDNet [[29](#bib.bib29)] | 5.97 | 142.89 | 4.84 | 45.31 |'
- en: 'TABLE VIII: Quantitative comparisons on LLIV-Phone-vidT dataset in terms of
    average luminance variance (ALV) score. The best result is in red whereas the
    second and third best results are in blue and purple.'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE VIII: LLIV-Phone-vidT 数据集的定量比较，涉及到的指标是平均亮度方差（ALV）得分。最佳结果用红色标记，而第二和第三最佳结果分别用蓝色和紫色标记。'
- en: '| Learning | Method | LoLi-Phone-vidT |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| Learning | Method | LoLi-Phone-vidT |'
- en: '| ALV$\downarrow$ |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| ALV$\downarrow$ |'
- en: '|  | input | 185.60 |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '|  | input | 185.60 |'
- en: '|  | LLNet [[1](#bib.bib1)] | 85.72 |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '|  | LLNet [[1](#bib.bib1)] | 85.72 |'
- en: '|  | LightenNet [[5](#bib.bib5)] | 643.93 |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '|  | LightenNet [[5](#bib.bib5)] | 643.93 |'
- en: '|  | Retinex-Net [[4](#bib.bib4)] | 94.05 |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '|  | Retinex-Net [[4](#bib.bib4)] | 94.05 |'
- en: '| SL | MBLLEN [[3](#bib.bib3)] | 113.18 |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| SL | MBLLEN [[3](#bib.bib3)] | 113.18 |'
- en: '|  | KinD [[11](#bib.bib11)] | 98.05 |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '|  | KinD [[11](#bib.bib11)] | 98.05 |'
- en: '|  | KinD++ [[61](#bib.bib61)] | 115.21 |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '|  | KinD++ [[61](#bib.bib61)] | 115.21 |'
- en: '|  | TBEFN [[20](#bib.bib20)] | 58.69 |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '|  | TBEFN [[20](#bib.bib20)] | 58.69 |'
- en: '|  | DSLR [[21](#bib.bib21)] | 175.35 |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '|  | DSLR [[21](#bib.bib21)] | 175.35 |'
- en: '| UL | EnlightenGAN [[26](#bib.bib26)] | 90.69 |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| UL | EnlightenGAN [[26](#bib.bib26)] | 90.69 |'
- en: '| SSL | DRBN [[33](#bib.bib33)] | 115.04 |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| SSL | DRBN [[33](#bib.bib33)] | 115.04 |'
- en: '|  | ExCNet [[27](#bib.bib27)] | 1375.29 |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '|  | ExCNet [[27](#bib.bib27)] | 1375.29 |'
- en: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 117.22 |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 117.22 |'
- en: '|  | RRDNet [[29](#bib.bib29)] | 147.11 |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '|  | RRDNet [[29](#bib.bib29)] | 147.11 |'
- en: 'Observing Table [VII](#S4.T7 "TABLE VII ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"), we can find that the performance of Retinex-Net [[4](#bib.bib4)],
    KinD++ [[61](#bib.bib61)], and EnlightenGAN [[26](#bib.bib26)] is relatively better
    than the other methods. Retinex-Net [[4](#bib.bib4)] achieves the best PI and
    SPAQ scores. The scores suggest the good perceptual quality of the results enhanced
    by Retinex-Net [[4](#bib.bib4)]. However, from Figure [7](#S4.F7 "Figure 7 ‣ 4.2
    Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey")(d) and Figure [8](#S4.F8
    "Figure 8 ‣ 4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(d), the
    results of Retinex-Net [[4](#bib.bib4)] evidently suffer from artifacts and color
    deviations. Moreover, KinD++ [[61](#bib.bib61)] attains the lowest NIQE score
    while the original input achieves the lowest LOE score. For the de-facto standard
    LOE metric, we question if the lightness order can effectively reflect the enhancement
    performance. Overall, the non-reference IQA metrics experience biases on the evaluations
    of the quality of enhanced low-light images in some cases.'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '观察表格 [VII](#S4.T7 "TABLE VII ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking and
    Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey")，我们可以发现 Retinex-Net [[4](#bib.bib4)]、KinD++ [[61](#bib.bib61)] 和 EnlightenGAN
    [[26](#bib.bib26)] 的表现相对优于其他方法。Retinex-Net [[4](#bib.bib4)] 达到了最佳的 PI 和 SPAQ 分数。这些分数表明
    Retinex-Net [[4](#bib.bib4)] 增强的结果具有良好的感知质量。然而，从图 [7](#S4.F7 "Figure 7 ‣ 4.2 Online
    Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image
    and Video Enhancement Using Deep Learning: A Survey")(d) 和图 [8](#S4.F8 "Figure
    8 ‣ 4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey")(d) 中可以明显看出，Retinex-Net
    [[4](#bib.bib4)] 的结果显然存在伪影和颜色偏差。此外，KinD++ [[61](#bib.bib61)] 的 NIQE 分数最低，而原始输入的
    LOE 分数最低。对于实际标准 LOE 指标，我们质疑亮度顺序是否能够有效地反映增强性能。总体而言，非参考 IQA 指标在某些情况下对低光图像质量的评估存在偏差。'
- en: 'To prepare videos in the LLIV-vidT testing set, we first discard videos without
    obvious objects in consecutive frames. A total of 10 videos are chosen. For each
    video, we select one object that appears in all frames. We then use a tracker
    [[92](#bib.bib92)] to track the object in consecutive frames of the input video
    and ensure the same object appears in the bounding boxes. We discard the frames
    with inaccurate object tracking. The coordinates of the bounding box in each frame
    are collected. We employ these coordinates to crop the corresponding regions in
    the results enhanced by different methods and compute the average luminance variance
    (ALV) scores of the object in the consecutive frames as: $ALV=\frac{1}{N}\sum\limits_{i=1}^{N}(L_{i}-L_{\text{avg}})^{2}$,
    where $N$ is the number of frames of a video, $L_{i}$ represents the average luminance
    value of the region of bounding box in the $i$th frame, and $L_{\text{avg}}$ denotes
    the average luminance value of all bounding box regions in the video. A lower
    ALV value suggests better temporal coherence of the enhanced video. The ALV values
    of different methods averaged over the 10 videos of the LLIV-vidT testing set
    are shown in Table [VIII](#S4.T8 "TABLE VIII ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"). The ALV values of different methods on each video can be found in
    the supplementary material. Besides, we follow Jiang and Zheng [[9](#bib.bib9)]
    to plot their luminance curves in the supplementary material.'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '为了准备 LLIV-vidT 测试集中的视频，我们首先丢弃连续帧中没有明显对象的视频。共选择了 10 个视频。对于每个视频，我们选择一个在所有帧中出现的对象。然后，我们使用跟踪器
    [[92](#bib.bib92)] 来跟踪输入视频中连续帧的对象，并确保同一对象出现在边界框中。我们丢弃了跟踪不准确的帧。每帧中的边界框坐标被收集。我们使用这些坐标来裁剪不同方法增强结果中的相应区域，并计算对象在连续帧中的平均亮度方差
    (ALV) 分数，公式为：$ALV=\frac{1}{N}\sum\limits_{i=1}^{N}(L_{i}-L_{\text{avg}})^{2}$，其中
    $N$ 是视频帧的数量，$L_{i}$ 表示第 $i$ 帧中边界框区域的平均亮度值，$L_{\text{avg}}$ 表示视频中所有边界框区域的平均亮度值。较低的
    ALV 值表示增强视频的时间一致性更好。表 [VIII](#S4.T8 "TABLE VIII ‣ 4.3 Benchmarking Results ‣ 4
    Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using
    Deep Learning: A Survey") 显示了不同方法在 LLIV-vidT 测试集的 10 个视频中的平均 ALV 值。每个视频中不同方法的
    ALV 值可以在补充材料中找到。此外，我们按照 Jiang 和 Zheng [[9](#bib.bib9)] 的方法在补充材料中绘制了他们的亮度曲线。'
- en: 'As shown in Table [VIII](#S4.T8 "TABLE VIII ‣ 4.3 Benchmarking Results ‣ 4
    Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using
    Deep Learning: A Survey"), TBEFN [[20](#bib.bib20)] obtains the best temporal
    coherence in terms of ALV value whereas LLNet [[1](#bib.bib1)] and EnlightenGAN
    [[26](#bib.bib26)] rank the second and third best, respectively. In contrast,
    the ALV value of ExCNet [[27](#bib.bib27)], as the worst performer, reaches 1375.29\.
    This is because the performance of the zero-reference learning-based ExCNet [[27](#bib.bib27)]
    is unstable for the enhancement of consecutive frames. ExCNet [[27](#bib.bib27)]
    can effectively improve the brightness of some frames while it does not work well
    on other frames.'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [VIII](#S4.T8 "TABLE VIII ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking and
    Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey") 所示，TBEFN [[20](#bib.bib20)] 在 ALV 值方面获得了最佳的时间一致性，而 LLNet [[1](#bib.bib1)]
    和 EnlightenGAN [[26](#bib.bib26)] 分别排名第二和第三。相比之下，ExCNet [[27](#bib.bib27)] 的 ALV
    值为 1375.29，是表现最差的。这是因为基于零参考学习的 ExCNet [[27](#bib.bib27)] 在连续帧的增强中表现不稳定。ExCNet
    [[27](#bib.bib27)] 可以有效提升一些帧的亮度，但在其他帧上效果不佳。'
- en: 'TABLE IX: Quantitative comparisons of computational complexity in terms of
    runtime (in second), number of trainable parameters (#Parameters) (in M), and
    FLOPs (in G). The best result is in red whereas the second and third best results
    are in blue and purple under each case, respectively. ‘-’ indicates the result
    is not available.'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IX：计算复杂度的定量比较，包括运行时间（以秒为单位）、可训练参数的数量（#Parameters）（以百万为单位）以及 FLOPs（以十亿为单位）。最佳结果用红色标出，第二和第三最佳结果分别用蓝色和紫色标出。‘-’表示结果不可用。
- en: '| Learning | Method | RunTime$\downarrow$ | #Parameters $\downarrow$ | FLOPs$\downarrow$
    | Platform |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '| Learning | Method | RunTime$\downarrow$ | #Parameters $\downarrow$ | FLOPs$\downarrow$
    | Platform |'
- en: '|  | LLNet [[1](#bib.bib1)] | 36.270 | 17.908 | 4124.177 | Theano |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '|  | LLNet [[1](#bib.bib1)] | 36.270 | 17.908 | 4124.177 | Theano |'
- en: '|  | LightenNet [[5](#bib.bib5)] | - | 0.030 | 30.540 | MATLAB |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '|  | LightenNet [[5](#bib.bib5)] | - | 0.030 | 30.540 | MATLAB |'
- en: '|  | Retinex-Net [[4](#bib.bib4)] | 0.120 | 0.555 | 587.470 | TensorFlow |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '|  | Retinex-Net [[4](#bib.bib4)] | 0.120 | 0.555 | 587.470 | TensorFlow |'
- en: '| SL | MBLLEN [[3](#bib.bib3)] | 13.995 | 0.450 | 301.120 | TensorFlow |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '| SL | MBLLEN [[3](#bib.bib3)] | 13.995 | 0.450 | 301.120 | TensorFlow |'
- en: '|  | KinD [[11](#bib.bib11)] | 0.148 | 8.160 | 574.954 | TensorFlow |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '|  | KinD [[11](#bib.bib11)] | 0.148 | 8.160 | 574.954 | TensorFlow |'
- en: '|  | KinD++ [[61](#bib.bib61)] | 1.068 | 8.275 | 12238.026 | TensorFlow |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '|  | KinD++ [[61](#bib.bib61)] | 1.068 | 8.275 | 12238.026 | TensorFlow |'
- en: '|  | TBEFN [[20](#bib.bib20)] | 0.050 | 0.486 | 108.532 | TensorFlow |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '|  | TBEFN [[20](#bib.bib20)] | 0.050 | 0.486 | 108.532 | TensorFlow |'
- en: '|  | DSLR [[21](#bib.bib21)] | 0.074 | 14.931 | 96.683 | PyTorch |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '|  | DSLR [[21](#bib.bib21)] | 0.074 | 14.931 | 96.683 | PyTorch |'
- en: '| UL | EnlightenGAN [[26](#bib.bib26)] | 0.008 | 8.637 | 273.240 | PyTorch
    |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| UL | EnlightenGAN [[26](#bib.bib26)] | 0.008 | 8.637 | 273.240 | PyTorch
    |'
- en: '| SSL | DRBN [[33](#bib.bib33)] | 0.878 | 0.577 | 196.359 | PyTorch |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '| SSL | DRBN [[33](#bib.bib33)] | 0.878 | 0.577 | 196.359 | PyTorch |'
- en: '|  | ExCNet [[27](#bib.bib27)] | 23.280 | 8.274 | - | PyTorch |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '|  | ExCNet [[27](#bib.bib27)] | 23.280 | 8.274 | - | PyTorch |'
- en: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 0.003 | 0.079 | 84.990 | PyTorch |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 0.003 | 0.079 | 84.990 | PyTorch |'
- en: '|  | RRDNet [[29](#bib.bib29)] | 167.260 | 0.128 | - | PyTorch |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '|  | RRDNet [[29](#bib.bib29)] | 167.260 | 0.128 | - | PyTorch |'
- en: 4.4 Computational Complexity
  id: totrans-699
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 计算复杂度
- en: 'In Table [IX](#S4.T9 "TABLE IX ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"), we compare the computational complexity of RGB format-based methods,
    including runtime, trainable parameters, and FLOPs averaged over 32 images of
    size 1200$\times$900$\times$3 using an NVIDIA 1080Ti GPU. We omit LightenNet [[5](#bib.bib5)]
    for fair comparisons because only the CPU version of its code is publicly available.
    Besides, we do not report the FLOPs of ExCNet [[27](#bib.bib27)] and RRDNet [[29](#bib.bib29)]
    as the number depends on the input images (different inputs require different
    numbers of iterations).'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [IX](#S4.T9 "TABLE IX ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking and Empirical
    Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")
    中，我们比较了基于 RGB 格式的方法的计算复杂度，包括运行时间、可训练参数和 FLOPs，这些是基于使用 NVIDIA 1080Ti GPU 处理 32
    张 1200$\times$900$\times$3 尺寸的图像的平均值。为了公平比较，我们省略了 LightenNet [[5](#bib.bib5)]，因为它的代码只有
    CPU 版本公开。此外，我们未报告 ExCNet [[27](#bib.bib27)] 和 RRDNet [[29](#bib.bib29)] 的 FLOPs，因为这些数字依赖于输入图像（不同的输入需要不同的迭代次数）。'
- en: 'As presented in Table [IX](#S4.T9 "TABLE IX ‣ 4.3 Benchmarking Results ‣ 4
    Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using
    Deep Learning: A Survey"), Zero-DCE [[28](#bib.bib28)] has the shortest runtime
    because it only estimates several curve parameters via a lightweight network.
    As a result, its number of trainable parameters and FLOPs are much fewer. Moreover,
    the number of trainable parameters and FLOPs of LightenNet [[5](#bib.bib5)] are
    the least among the compared methods. This is because LightenNet [[5](#bib.bib5)]
    estimates the illumination map of input image via a tiny network of four convolutional
    layers. In contrast, the FLOPs of LLNet [[1](#bib.bib1)] and KinD++ [[61](#bib.bib61)]
    are extremely large, reaching 4124.177G and 12238.026G, respectively. The runtime
    of SSL-based ExCNet [[27](#bib.bib27)] and RRDNet [[29](#bib.bib29)] is long due
    to the time-consuming optimization process.'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [IX](#S4.T9 "TABLE IX ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking and Empirical
    Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")
    所示，Zero-DCE [[28](#bib.bib28)] 的运行时间最短，因为它只通过轻量级网络估计几个曲线参数。因此，它的可训练参数和FLOPs远少于其他方法。此外，LightenNet
    [[5](#bib.bib5)] 的可训练参数和FLOPs在比较的方法中最少。这是因为 LightenNet [[5](#bib.bib5)] 通过一个包含四层卷积层的小型网络估计输入图像的光照图。相比之下，LLNet
    [[1](#bib.bib1)] 和 KinD++ [[61](#bib.bib61)] 的 FLOPs 极其庞大，分别达到 4124.177G 和 12238.026G。SSL
    基于的 ExCNet [[27](#bib.bib27)] 和 RRDNet [[29](#bib.bib29)] 的运行时间较长，因为优化过程耗时较长。'
- en: '![Refer to caption](img/6f2327be92968582d74caf6ccf72cf39.png)'
  id: totrans-702
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6f2327be92968582d74caf6ccf72cf39.png)'
- en: 'Figure 10: The P-R curves of face detection in the dark.'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 黑暗环境中人脸检测的 P-R 曲线。'
- en: 'TABLE X: Quantitative comparisons of AP under different IoU thresholds of face
    detection in the dark. The best result is in red whereas the second and third
    best results are in blue and purple under each case, respectively.'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '表 X: 不同 IoU 阈值下的黑暗环境中人脸检测的 AP 定量比较。最佳结果用红色标出，而第二和第三最佳结果分别用蓝色和紫色标出。'
- en: '| Learning | Method | IoU thresholds |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '| 学习 | 方法 | IoU 阈值 |'
- en: '| 0.5 | 0.6 | 0.7 |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 0.6 | 0.7 |'
- en: '|  | input | 0.195 | 0.061 | 0.007 |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '|  | input | 0.195 | 0.061 | 0.007 |'
- en: '|  | LLNet [[1](#bib.bib1)] | 0.208 | 0.063 | 0.006 |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '|  | LLNet [[1](#bib.bib1)] | 0.208 | 0.063 | 0.006 |'
- en: '|  | LightenNet [[5](#bib.bib5)] | 0.249 | 0.085 | 0.010 |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
  zh: '|  | LightenNet [[5](#bib.bib5)] | 0.249 | 0.085 | 0.010 |'
- en: '|  | Retinex-Net [[4](#bib.bib4)] | 0.261 | 0.101 | 0.013 |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
  zh: '|  | Retinex-Net [[4](#bib.bib4)] | 0.261 | 0.101 | 0.013 |'
- en: '| SL | MBLLEN [[3](#bib.bib3)] | 0.249 | 0.092 | 0.010 |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '| SL | MBLLEN [[3](#bib.bib3)] | 0.249 | 0.092 | 0.010 |'
- en: '|  | KinD [[11](#bib.bib11)] | 0.235 | 0.081 | 0.010 |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
  zh: '|  | KinD [[11](#bib.bib11)] | 0.235 | 0.081 | 0.010 |'
- en: '|  | KinD++ [[61](#bib.bib61)] | 0.251 | 0.090 | 0.011 |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
  zh: '|  | KinD++ [[61](#bib.bib61)] | 0.251 | 0.090 | 0.011 |'
- en: '|  | TBEFN [[20](#bib.bib20)] | 0.268 | 0.099 | 0.011 |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
  zh: '|  | TBEFN [[20](#bib.bib20)] | 0.268 | 0.099 | 0.011 |'
- en: '|  | DSLR [[21](#bib.bib21)] | 0.223 | 0.067 | 0.007 |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
  zh: '|  | DSLR [[21](#bib.bib21)] | 0.223 | 0.067 | 0.007 |'
- en: '| UL | EnlightenGAN [[26](#bib.bib26)] | 0.246 | 0.088 | 0.011 |'
  id: totrans-716
  prefs: []
  type: TYPE_TB
  zh: '| UL | EnlightenGAN [[26](#bib.bib26)] | 0.246 | 0.088 | 0.011 |'
- en: '| SSL | DRBN [[33](#bib.bib33)] | 0.199 | 0.061 | 0.007 |'
  id: totrans-717
  prefs: []
  type: TYPE_TB
  zh: '| SSL | DRBN [[33](#bib.bib33)] | 0.199 | 0.061 | 0.007 |'
- en: '|  | ExCNet [[27](#bib.bib27)] | 0.256 | 0.092 | 0.010 |'
  id: totrans-718
  prefs: []
  type: TYPE_TB
  zh: '|  | ExCNet [[27](#bib.bib27)] | 0.256 | 0.092 | 0.010 |'
- en: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 0.259 | 0.092 | 0.011 |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
  zh: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 0.259 | 0.092 | 0.011 |'
- en: '|  | RRDNet [[29](#bib.bib29)] | 0.248 | 0.083 | 0.010 |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '|  | RRDNet [[29](#bib.bib29)] | 0.248 | 0.083 | 0.010 |'
- en: 4.5 Application-Based Evaluation
  id: totrans-721
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 基于应用的评估
- en: 'We investigate the performance of low-light image enhancement methods on face
    detection in the dark. Following the setting presented in Guo et al. [[28](#bib.bib28)],
    we use the DARK FACE dataset [[80](#bib.bib80)] that is composed of images with
    faces taken in the dark. Since the bounding boxes of the test set are not publicly
    available, we perform the evaluation on 500 images randomly sampled from the training
    and validation sets. The Dual Shot Face Detector (DSFD) [[93](#bib.bib93)] trained
    on WIDER FACE dataset [[94](#bib.bib94)] is used as the face detector. We feed
    the results of different LLIE methods to the DSFD [[93](#bib.bib93)] and depict
    the precision-recall (P-R) curves under 0.5 IoU threshold in Figure [10](#S4.F10
    "Figure 10 ‣ 4.4 Computational Complexity ‣ 4 Benchmarking and Empirical Analysis
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey"). We compare
    the average precision (AP) under different IoU thresholds using the evaluation
    tool³³3[https://github.com/Ir1d/DARKFACE_eval_tools](https://github.com/Ir1d/DARKFACE_eval_tools)
    provided in DARK FACE dataset [[80](#bib.bib80)] in Table [X](#S4.T10 "TABLE X
    ‣ 4.4 Computational Complexity ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey").'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '我们调查了低光照图像增强方法在黑暗中的人脸检测性能。根据 Guo 等人所提出的设置[[28](#bib.bib28)]，我们使用了 DARK FACE
    数据集[[80](#bib.bib80)]，该数据集由黑暗中拍摄的含有面孔的图像组成。由于测试集的边界框没有公开，我们在从训练集和验证集中随机抽取的 500
    张图像上进行评估。我们使用在 WIDER FACE 数据集[[94](#bib.bib94)] 上训练的双重拍摄人脸检测器 (DSFD) [[93](#bib.bib93)]
    作为人脸检测器。我们将不同 LLIE 方法的结果输入 DSFD [[93](#bib.bib93)]，并在图 [10](#S4.F10 "Figure 10
    ‣ 4.4 Computational Complexity ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey") 下描绘 0.5 IoU 阈值下的精度-召回
    (P-R) 曲线。我们使用 DARK FACE 数据集[[80](#bib.bib80)] 提供的评估工具³³3[https://github.com/Ir1d/DARKFACE_eval_tools](https://github.com/Ir1d/DARKFACE_eval_tools)
    比较了不同 IoU 阈值下的平均精度 (AP)，结果见表 [X](#S4.T10 "TABLE X ‣ 4.4 Computational Complexity
    ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey")。'
- en: 'As shown in Figure [10](#S4.F10 "Figure 10 ‣ 4.4 Computational Complexity ‣
    4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey"), all the deep learning-based solutions improve
    the performance of face detection in the dark, suggesting the effectiveness of
    deep learning-based LLIE solutions for face detection in the dark. As shown in
    Table [X](#S4.T10 "TABLE X ‣ 4.4 Computational Complexity ‣ 4 Benchmarking and
    Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"), the AP scores of best performers under different IoU thresholds range
    from 0.268 to 0.013 and the AP scores of input under different IoU thresholds
    are very low. The results suggest that there is still room for improvement. It
    is noteworthy that Retinex-Net [[4](#bib.bib4)], Zero-DCE [[28](#bib.bib28)],
    and TBEFN [[20](#bib.bib20)] achieve relatively robust performance on face detection
    in the dark. We show the visual results of different methods in Figure [11](#S4.F11
    "Figure 11 ‣ 4.6 Discussion ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey"). Although Retinex-Net
    [[4](#bib.bib4)] performs better than other methods on the AP score, its visual
    result contains obvious artifacts and unnatural textures. In general, Zero-DCE
    [[28](#bib.bib28)] obtains a good balance between the AP score and the perceptual
    quality for face detection in the dark. Note that the results of face detection
    in the dark are related to not only the enhanced results but also the face detector
    including the detector model and the training data of the detector. Here, we only
    take the pre-trained DSFD [[93](#bib.bib93)] as an example to validate the low-light
    image enhancement performance of different methods to some extent.'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [10](#S4.F10 "图 10 ‣ 4.4 计算复杂度 ‣ 4 基准测试与实证分析 ‣ 基于深度学习的低光照图像与视频增强：一项综述") 所示，所有基于深度学习的解决方案都提高了黑暗中面部检测的性能，表明基于深度学习的低光照图像增强解决方案在黑暗环境中的面部检测效果显著。如表
    [X](#S4.T10 "表 X ‣ 4.4 计算复杂度 ‣ 4 基准测试与实证分析 ‣ 基于深度学习的低光照图像与视频增强：一项综述") 所示，不同 IoU
    阈值下最佳表现者的 AP 分数范围为 0.268 到 0.013，而输入在不同 IoU 阈值下的 AP 分数非常低。结果表明，仍有改进空间。值得注意的是，Retinex-Net
    [[4](#bib.bib4)]、Zero-DCE [[28](#bib.bib28)] 和 TBEFN [[20](#bib.bib20)] 在黑暗中的面部检测上表现出相对较强的鲁棒性。我们在图
    [11](#S4.F11 "图 11 ‣ 4.6 讨论 ‣ 4 基准测试与实证分析 ‣ 基于深度学习的低光照图像与视频增强：一项综述") 中展示了不同方法的视觉结果。尽管
    Retinex-Net [[4](#bib.bib4)] 在 AP 分数上表现优于其他方法，但其视觉结果中存在明显的伪影和不自然的纹理。总体而言，Zero-DCE
    [[28](#bib.bib28)] 在 AP 分数和黑暗环境中面部检测的感知质量之间取得了良好的平衡。请注意，黑暗中面部检测的结果不仅与增强结果有关，还与面部检测器，包括检测器模型和检测器的训练数据相关。在此，我们仅以预训练的
    DSFD [[93](#bib.bib93)] 为例，验证不同方法在低光照图像增强方面的性能。
- en: 4.6 Discussion
  id: totrans-724
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 讨论
- en: 'From the experimental results, we obtain several interesting observations and
    insights:'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 从实验结果中，我们获得了一些有趣的观察和见解：
- en: '| ![Refer to caption](img/7850694808f2c3be24cfe312a36ae7b2.png) | ![Refer to
    caption](img/6c28abfaa92d25d66ece7297517f8431.png) | ![Refer to caption](img/ce0770b99deb0cd18da04633145bde58.png)
    | ![Refer to caption](img/c6224de9e140d1184d0fc645bc5483c5.png) |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/7850694808f2c3be24cfe312a36ae7b2.png) | ![参见说明](img/6c28abfaa92d25d66ece7297517f8431.png)
    | ![参见说明](img/ce0770b99deb0cd18da04633145bde58.png) | ![参见说明](img/c6224de9e140d1184d0fc645bc5483c5.png)
    |'
- en: '| (a) input | (b) LightenNet [[5](#bib.bib5)] | (c) Retinex-Net [[4](#bib.bib4)]
    | (d) MBLLEN [[3](#bib.bib3)] |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '| (a) 输入 | (b) LightenNet [[5](#bib.bib5)] | (c) Retinex-Net [[4](#bib.bib4)]
    | (d) MBLLEN [[3](#bib.bib3)] |'
- en: '| ![Refer to caption](img/3394b8aa666c935488fbd44890e9e228.png) | ![Refer to
    caption](img/1701ffa1e14d2e5017a8a8a2f08b9962.png) | ![Refer to caption](img/4fa7144834439baab46e3c821b21581a.png)
    | ![Refer to caption](img/f9fba7cead24e131bf299c97ea565e64.png) |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/3394b8aa666c935488fbd44890e9e228.png) | ![参见说明](img/1701ffa1e14d2e5017a8a8a2f08b9962.png)
    | ![参见说明](img/4fa7144834439baab46e3c821b21581a.png) | ![参见说明](img/f9fba7cead24e131bf299c97ea565e64.png)
    |'
- en: '| (e) KinD++ [[61](#bib.bib61)] | (f) TBEFN [[20](#bib.bib20)] | (g) DSLR [[21](#bib.bib21)]
    | (h) EnlightenGAN [[26](#bib.bib26)] |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
  zh: '| (e) KinD++ [[61](#bib.bib61)] | (f) TBEFN [[20](#bib.bib20)] | (g) DSLR [[21](#bib.bib21)]
    | (h) EnlightenGAN [[26](#bib.bib26)] |'
- en: '| ![Refer to caption](img/3c9ca7f03571bc9152e4011411a8a432.png) | ![Refer to
    caption](img/1eaede4ac09756e4cfa19fd2b06f9adf.png) | ![Refer to caption](img/973bafbb43b5346c807f7fbbca2cbceb.png)
    | ![Refer to caption](img/4b41b4b90028da2fedd0de9ae536e205.png) |'
  id: totrans-730
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/3c9ca7f03571bc9152e4011411a8a432.png) | ![参见说明](img/1eaede4ac09756e4cfa19fd2b06f9adf.png)
    | ![参见说明](img/973bafbb43b5346c807f7fbbca2cbceb.png) | ![参见说明](img/4b41b4b90028da2fedd0de9ae536e205.png)
    |'
- en: '| (i) DRBN [[33](#bib.bib33)] | (j) ExCNet [[27](#bib.bib27)] | (k) Zero-DCE
    [[28](#bib.bib28)] | (l) RRDNet [[29](#bib.bib29)] |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
  zh: '| (i) DRBN [[33](#bib.bib33)] | (j) ExCNet [[27](#bib.bib27)] | (k) Zero-DCE
    [[28](#bib.bib28)] | (l) RRDNet [[29](#bib.bib29)] |'
- en: 'Figure 11: Visual results of different methods on a low-light image sampled
    from DARK FACE dataset. Better see with zoom in for the bounding boxes of faces.'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 从 DARK FACE 数据集中采样的低光图像上不同方法的视觉结果。为了更好地查看人脸的边界框，请放大查看。'
- en: 1) The performance of different methods significantly varies based on the test
    datasets and evaluation metrics. In terms of the full-reference IQA metrics on
    commonly used test datasets, MBLLEN [[3](#bib.bib3)], KinD++ [[61](#bib.bib61)],
    and DSLR [[21](#bib.bib21)] are generally better than other compared methods.
    For real-world low-light images taken by mobile phones, supervised learning-based
    Retinex-Net [[4](#bib.bib4)] and KinD++ [[61](#bib.bib61)] obtain better scores
    measured in the non-reference IQA metrics. For real-world low-light videos taken
    by mobile phones, TBEFN [[20](#bib.bib20)] preserves the temporal coherence better.
    When coming to the computational efficiency, LightenNet [[5](#bib.bib5)] and Zero-DCE
    [[28](#bib.bib28)] are outstanding. From the aspect of face detection in the dark,
    TBEFN [[20](#bib.bib20)], Retinex-Net [[4](#bib.bib4)], and Zero-DCE [[28](#bib.bib28)]
    rank the first three. No method always wins. Overall, Retinex-Net [[4](#bib.bib4)],
    [[20](#bib.bib20)], Zero-DCE [[28](#bib.bib28)], and DSLR [[21](#bib.bib21)] are
    better choice in most cases. For raw data, EEMEFN [[16](#bib.bib16)] obtains relatively
    better qualitative and quantitative performance than SID [[85](#bib.bib85)]. However,
    from the visual results, EEMEFN [[16](#bib.bib16)] and [[85](#bib.bib85)] cannot
    recover the color well when compared with the corresponding ground truth.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 不同方法的性能根据测试数据集和评估指标的不同而显著变化。在常用测试数据集的全参考图像质量评估指标方面，MBLLEN [[3](#bib.bib3)]、KinD++
    [[61](#bib.bib61)] 和 DSLR [[21](#bib.bib21)] 通常优于其他对比方法。对于由手机拍摄的真实低光图像，基于监督学习的
    Retinex-Net [[4](#bib.bib4)] 和 KinD++ [[61](#bib.bib61)] 在非参考图像质量评估指标中获得了更好的分数。对于由手机拍摄的真实低光视频，TBEFN
    [[20](#bib.bib20)] 更好地保持了时间一致性。在计算效率方面，LightenNet [[5](#bib.bib5)] 和 Zero-DCE
    [[28](#bib.bib28)] 表现突出。从黑暗中的人脸检测方面来看，TBEFN [[20](#bib.bib20)]、Retinex-Net [[4](#bib.bib4)]
    和 Zero-DCE [[28](#bib.bib28)] 排名前三。没有任何一种方法始终获胜。总体而言，Retinex-Net [[4](#bib.bib4)]、[[20](#bib.bib20)]、Zero-DCE
    [[28](#bib.bib28)] 和 DSLR [[21](#bib.bib21)] 在大多数情况下是更好的选择。对于原始数据，EEMEFN [[16](#bib.bib16)]
    在定性和定量性能方面相对于 SID [[85](#bib.bib85)] 表现较好。然而，从视觉结果来看，EEMEFN [[16](#bib.bib16)]
    和 [[85](#bib.bib85)] 在与对应的真实情况相比时，色彩恢复效果不佳。
- en: 2) LLIV-Phone dataset fails most methods. The generalization capability of existing
    methods needs further improvements. It is worth noting that it is inadequate to
    use only the average luminance variance to evaluate the performance of different
    methods for low-light video enhancement. More effective and comprehensive assessment
    metrics would guide the development of low-light video enhancement towards the
    right track.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 2) LLIV-Phone 数据集对大多数方法表现不佳。现有方法的泛化能力需要进一步提升。值得注意的是，仅使用平均亮度方差来评估不同方法在低光视频增强中的性能是不够的。更有效和全面的评估指标将引导低光视频增强的发展走向正确的轨道。
- en: 3) Regarding learning strategies, supervised learning achieves better performance
    in most cases, but requires high computational resources and paired training data.
    In comparison, zero-shot learning is more appealing in practical applications
    because it does not require paired or unpaired training data. Consequently, zero-shot
    learning-based methods enjoy better generalization capability. However, their
    quantitative performance is inferior to other methods.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 关于学习策略，监督学习在大多数情况下表现更好，但需要高计算资源和配对的训练数据。相比之下，零样本学习在实际应用中更具吸引力，因为它不需要配对或未配对的训练数据。因此，基于零样本学习的方法具有更好的泛化能力。然而，它们的定量性能不如其他方法。
- en: 4) There is a gap between visual results and quantitative IQA scores. In other
    words, a good visual appearance does not always yield a good IQA score. The relationships
    between human perception and IQA scores are worth more investigation. Pursuing
    better visual perception or quantitative scores depends on specific applications.
    For instance, to show the results to observers, more attention should be paid
    to visual perception. In contrast, accuracy is more important when LLIE methods
    are applied to face detection in the dark. Thus, more comprehensive comparisons
    should be performed when comparing different methods.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉结果与定量IQ评分之间存在差距。换句话说，良好的视觉效果不一定会产生良好的IQ评分。人类感知与IQ评分之间的关系值得更多的研究。追求更好的视觉感知或定量评分取决于具体应用。例如，为了向观察者展示结果时，应更加关注视觉感知。相比之下，当LLIE方法应用于黑暗中的人脸检测时，准确性更为重要。因此，在比较不同方法时应进行更全面的比较。
- en: 5) Deep learning-based LLIE methods are beneficial to face detection in the
    dark. Such results further support the significance of enhancing low-light images
    and videos. However, in comparison to the high accuracy of face detection in normal-light
    images, the accuracy of face detection in the dark is extremely low, despite using
    LLIE methods.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的LLIE方法对黑暗中的人脸检测有益。这些结果进一步支持了增强低光图像和视频的重要性。然而，与正常光照图像中人脸检测的高准确率相比，尽管使用了LLIE方法，在黑暗中的人脸检测准确率仍然极低。
- en: 6) In comparison to RGB format-based LLIE methods, raw format-based LLIE methods
    usually recover details better, obtain more vivid color, and reduce noises and
    artifacts more effectively. This is because raw data contain more information
    such as wider color gamut and higher dynamic range. However, raw format-based
    LLIE methods are limited to specific sensors and formats such as the Bayer pattern
    of the Sony camera and the APS-C X-Trans pattern of the Fuji camera. In contrast,
    RGB format-based LLIE methods are more convenient and versatile since RGB images
    are commonly found as the final imagery form produced by mobile devices. However,
    RGB format-based LLIE methods cannot cope well with cases that exhibit low light
    and excessive noise.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于RGB格式的低光图像增强（LLIE）方法相比，基于原始格式的LLIE方法通常能更好地恢复细节，获得更生动的颜色，并更有效地减少噪声和伪影。这是因为原始数据包含更多的信息，如更广的色域和更高的动态范围。然而，基于原始格式的LLIE方法受到特定传感器和格式的限制，例如索尼相机的Bayer模式和富士相机的APS-C
    X-Trans模式。相比之下，基于RGB格式的LLIE方法更为方便和多样化，因为RGB图像通常是移动设备生成的最终图像形式。然而，RGB格式的LLIE方法在处理低光和过多噪声的情况下表现不佳。
- en: 5 Open Issues
  id: totrans-739
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 开放问题
- en: In this section, we summarize the open issues in low-light image and video enhancement
    as follows.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们总结了低光图像和视频增强中的开放问题，如下所示。
- en: Generalization Capability. Although existing methods can produce some visually
    pleasing results, they have limited generalization capability. For example, a
    method trained on MIT-Adobe FiveK dataset [[79](#bib.bib79)] cannot effectively
    enhance the low-light images of LOL dataset [[4](#bib.bib4)]. Albeit synthetic
    data are used to augment the diversity of training data, the models trained on
    the combination of real and synthetic data cannot solve this issue well. Improving
    the generalization capability of LLIE methods is an unsolved open issue.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化能力。尽管现有方法可以产生一些视觉上令人满意的结果，但它们的泛化能力有限。例如，在MIT-Adobe FiveK数据集[[79](#bib.bib79)]上训练的方法不能有效地增强LOL数据集[[4](#bib.bib4)]中的低光图像。尽管使用合成数据来增加训练数据的多样性，但在真实和合成数据组合上训练的模型无法很好地解决这一问题。提高LLIE方法的泛化能力仍然是一个未解决的开放问题。
- en: Removing Unknown Noises. Observing the results of existing methods on the low-light
    images captured by different types of phones’ cameras, we can find that these
    methods cannot remove the noises well and even amplify the noises, especially
    when the types of noises are unknown. Despite some methods add Gaussian and/or
    Poisson noises in their training data, the noise types are different from real
    noises, thus the performance of these methods is unsatisfactory in real scenarios.
    Removing unknown noises is still unsolved.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 去除未知噪声。观察现有方法在不同类型手机摄像头捕捉的低光图像上的效果，我们可以发现这些方法无法很好地去除噪声，甚至会放大噪声，尤其是在噪声类型未知的情况下。尽管一些方法在训练数据中加入了高斯噪声和/或泊松噪声，但噪声类型与真实噪声不同，因此这些方法在真实场景中的表现并不令人满意。去除未知噪声仍然是一个未解决的问题。
- en: Removing Unknown Artifacts. One may enhance a low-light image downloaded from
    the Internet. The image may have gone through a serial of degradations such as
    JPEG compression or editing. Thus, the image may contain unknown artifacts. Suppressing
    unknown artifacts still challenges existing low-light image and video enhancement
    methods.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 去除未知伪影。可能会增强从互联网下载的低光图像。该图像可能经历了一系列的降解，例如JPEG压缩或编辑。因此，图像可能包含未知的伪影。抑制未知伪影仍然是现有低光图像和视频增强方法面临的挑战。
- en: Correcting Uneven Illumination. Images taken in real scenes usually exhibit
    uneven illumination. For example, an image captured at night has both dark regions
    and normal-light or over-exposed regions such as the regions of light sources.
    Existing methods tend to brighten both the dark regions and the light source regions,
    affecting the visual quality of the enhanced result. It is expected to enhance
    dark regions but suppress over-exposed regions. However, this open issue is not
    solved well in existing LLIE methods.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 修正不均匀照明。真实场景中的图像通常表现出不均匀的照明。例如，夜间拍摄的图像既有黑暗区域，也有正常光照或过曝区域，例如光源区域。现有的方法往往会将黑暗区域和光源区域一起增亮，从而影响增强结果的视觉质量。理想情况下，应该增强黑暗区域而抑制过曝区域。然而，现有的低光图像增强（LLIE）方法尚未很好地解决这一开放性问题。
- en: Distinguishing Semantic Regions. Existing methods tend to enhance a low-light
    image without considering the semantic information of its different regions. For
    example, the black hair of a man in a low-light image is enhanced to be off-white
    as the black hair is treated as the low-light regions. An ideal enhancement method
    is expected to only enhance the low-light regions induced by external environments.
    How to distinguish semantic regions is an open issue.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 区分语义区域。现有方法往往在没有考虑不同区域的语义信息的情况下增强低光图像。例如，低光图像中的黑色头发被增强为灰白色，因为黑色头发被视为低光区域。理想的增强方法应仅增强由外部环境引起的低光区域。如何区分语义区域仍是一个开放性问题。
- en: Using Neighbouring Frames. Despite some methods that have been proposed to enhance
    low-light videos, they commonly process a video frame-by-frame. How to make full
    use of the neighboring frames to improve the enhancement performance and speed
    up the processing speed is an unsolved open issue. For example, the well-lit regions
    of neighboring frames are used to enhance the current frame. For another example,
    the estimated parameters for processing neighboring frames can be reused to enhance
    the current frame for reducing the time of parameter estimation.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 使用邻近帧。尽管已经提出了一些方法来增强低光视频，但它们通常是逐帧处理视频。如何充分利用邻近帧来提高增强性能并加快处理速度仍是一个未解决的开放性问题。例如，可以利用邻近帧中的良好照明区域来增强当前帧。另一个例子是，可以重用处理邻近帧的估计参数来增强当前帧，以减少参数估计的时间。
- en: 6 Future Research Directions
  id: totrans-747
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来研究方向
- en: 'Low-light enhancement is a challenging research topic. As can be observed from
    the experiments presented in Section [4](#S4 "4 Benchmarking and Empirical Analysis
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey") and the
    unsolved open issues in Section [5](#S5 "5 Open Issues ‣ Low-Light Image and Video
    Enhancement Using Deep Learning: A Survey"), there is still room for improvement.
    We suggest potential future research directions as follows.'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: '低光增强是一个具有挑战性的研究课题。从第[4](#S4 "4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey")节中展示的实验和第[5](#S5 "5
    Open Issues ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")节中的未解决的开放性问题可以观察到，仍有改进的空间。我们建议以下潜在的未来研究方向。'
- en: Effective Learning Strategies. As aforementioned, current LLIE models mainly
    adopt supervised learning that requires massive paired training data and may overfit
    on a specific dataset. Although some researchers attempted to introduce unsupervised
    learning into LLIE, the inherent relationships between LLIE and these learning
    strategies are not clear and their effectiveness in LLIE needs further improvements.
    Zero-shot learning has shown robust performance for real scenes while not requiring
    paired training data. The unique advantage suggests zero-shot learning as a potential
    research direction, especially on the formulation of zero-reference losses, deep
    priors, and optimization strategies.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的学习策略。如前所述，目前的LLIE（低光照图像增强）模型主要采用监督学习，这需要大量的配对训练数据，并且可能在特定的数据集上出现过拟合。尽管一些研究人员尝试将无监督学习引入LLIE，但LLIE与这些学习策略之间的内在关系尚不明确，并且它们在LLIE中的有效性还需要进一步的改进。零样本学习在实际场景中表现出强大的性能，同时不需要配对训练数据。其独特的优势表明，零样本学习可能是一个有潜力的研究方向，特别是在零参考损失、深度先验和优化策略的制定上。
- en: Specialized Network Structures. A network structure can significantly affect
    enhancement performance. As previously analyzed, most LLIE deep models employ
    U-Net or U-Net-like structures. Though they have achieved promising performance
    in some cases, the investigation if such an encoder-decoder network structure
    is most suitable for the LLIE task is still lacking. Some network structures require
    a high memory footprint and long inference time due to their large parameter space.
    Such network structures are unacceptable for practical applications. Thus, it
    is worthwhile to investigate a more effective network structure for LLIE, considering
    the characteristics of low-light images such as non-uniform illumination, small
    pixel values, noise suppression, and color constancy. One can also design more
    efficient network structures via taking into account the local similarity of low-light
    images or considering more efficient operations such as depthwise separable convolution
    layer [[95](#bib.bib95)] and self-calibrated convolution [[96](#bib.bib96)]. Neural
    architecture search (NAS) technique [[97](#bib.bib97), [98](#bib.bib98)] may be
    considered to obtain more effective and efficient LLIE network structures. Adapting
    the transformer architecture [[99](#bib.bib99), [100](#bib.bib100)] into LLIE
    may be a potential and interesting research direction.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 专用网络结构。网络结构可以显著影响增强性能。如前所述，大多数LLIE深度模型采用U-Net或类似U-Net的结构。尽管在某些情况下它们已经取得了令人鼓舞的性能，但是否这种编码器-解码器网络结构最适合LLIE任务的调查仍然缺乏。一些网络结构由于其较大的参数空间，需要较高的内存占用和较长的推理时间。这些网络结构对于实际应用来说是不可接受的。因此，考虑到低光照图像的特性，如非均匀照明、小的像素值、噪声抑制和颜色恒常性，研究更有效的LLIE网络结构是有价值的。还可以通过考虑低光照图像的局部相似性或考虑更高效的操作，例如深度可分离卷积层[[95](#bib.bib95)]和自校准卷积[[96](#bib.bib96)]，来设计更高效的网络结构。神经架构搜索（NAS）技术[[97](#bib.bib97),
    [98](#bib.bib98)]可以考虑用来获得更有效和高效的LLIE网络结构。将变换器架构[[99](#bib.bib99), [100](#bib.bib100)]引入LLIE可能是一个有潜力且有趣的研究方向。
- en: Loss Functions. Loss functions constrain the relationships between an input
    image and ground truth and drive the optimization of deep networks. In LLIE, the
    commonly used loss functions are borrowed from related vision tasks. Thus, designing
    loss functions that are more well-suited for LLIE is desired. Recent studies have
    shown the possibility of using deep neural networks to approximate human visual
    perception of image quality [[101](#bib.bib101), [102](#bib.bib102)]. These ideas
    and fundamental theories could be used to guide the designs of loss functions
    for low-light enhancement networks.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数。损失函数约束了输入图像与真实情况之间的关系，并驱动深度网络的优化。在LLIE中，常用的损失函数借鉴了相关视觉任务。因此，设计更适合LLIE的损失函数是理想的。最近的研究表明，使用深度神经网络来逼近人类对图像质量的视觉感知是可能的[[101](#bib.bib101),
    [102](#bib.bib102)]。这些思想和基本理论可以用来指导低光照增强网络的损失函数设计。
- en: 'Realistic Training Data. Although there are several training datasets for LLIE,
    their authenticity, scales, and diversities fall behind real low-light conditions.
    Thus, as shown in Section [4](#S4 "4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey"), current LLIE deep
    models cannot achieve satisfactory performance when encountering low-light images
    captured in real-world scenes. More efforts are needed to study the collection
    of large-scale and diverse real-world paired LLIE training datasets or to generate
    more realistic synthetic data.'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: '真实的训练数据。虽然有几个 LLIE 训练数据集，但它们的真实性、规模和多样性仍然不及真实低光照条件。因此，如第 [4](#S4 "4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey") 节所示，当前的 LLIE 深度模型在处理真实世界场景中捕获的低光照图像时无法达到令人满意的性能。需要更多努力来研究收集大规模和多样化的真实世界配对
    LLIE 训练数据集，或生成更真实的合成数据。'
- en: Standard Test Data. Currently, there is no well-accepted LLIE evaluation benchmark.
    Researchers prefer selecting their test data that may bias to their proposed methods.
    Despite some researchers leave some paired data as test data, the division of
    training and test partitions are mostly ad-hoc across the literature. Consequently,
    conducting a fair comparison among different methods is often laborious if not
    impossible. Besides, some test data are either easy to be handled or not originally
    collected for low-light enhancement. It is desired to have a standard low-light
    image and video test dataset, which includes a large number of test samples with
    the corresponding ground truths, covering diverse scenes and challenging illumination
    conditions.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 标准测试数据。目前，没有公认的 LLIE 评估基准。研究人员倾向于选择可能偏向于其提出方法的测试数据。尽管一些研究人员留下一些配对数据作为测试数据，但训练和测试数据的划分在文献中大多是随意的。因此，在不同方法之间进行公平比较往往是艰巨的，甚至是不可能的。此外，一些测试数据要么容易处理，要么原本未用于低光照增强。希望能拥有一个标准的低光照图像和视频测试数据集，其中包括大量测试样本及其相应的真实数据，涵盖各种场景和具有挑战性的照明条件。
- en: Task-Specific Evaluation Metrics. The commonly adopted evaluation metrics in
    LLIE can reflect the image quality to some extent. However, how to measure how
    good a result is enhanced by an LLIE method still challenges current IQA metrics,
    especially for non-reference measurements. The current IQA metrics either focus
    on human visual perceptual such as subjective quality or emphasize machine perceptual
    such as the effects on high-level visual tasks. Therefore, more works are expected
    in this research direction to make efforts on designing more accurate and task-specific
    evaluation metrics for LLIE.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 特定任务评估指标。LLIE 中常用的评估指标在一定程度上可以反映图像质量。然而，如何衡量 LLIE 方法增强结果的好坏仍然挑战当前的 IQA 指标，尤其是对无参考测量的情况。当前的
    IQA 指标要么关注人类视觉感知，如主观质量，要么强调机器感知，如对高级视觉任务的影响。因此，预计在这一研究方向上会有更多工作，努力设计更准确和特定任务的评估指标。
- en: Robust Generalization Capability. Observing the experimental results on real-world
    test data, most methods fail due to their limited generalization capability. The
    poor generalization is caused by several factors such as synthetic training data,
    small-scaled training data, ineffective network structures, or unrealistic assumptions.
    It is important to explore ways to improve the generalization.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒的泛化能力。观察真实世界测试数据上的实验结果，大多数方法由于其有限的泛化能力而失败。泛化能力差是由合成训练数据、小规模训练数据、无效的网络结构或不切实际的假设等多个因素造成的。探索提高泛化能力的方法非常重要。
- en: Extension to Low-Light Video Enhancement. Unlike the rapid development of video
    enhancement in other low-level vision tasks such as video deblurring [[103](#bib.bib103)],
    video denoising [[104](#bib.bib104)], and video super-resolution [[105](#bib.bib105)],
    low-light video enhancement receives less attention. A direct application of existing
    LLIE methods to videos often leads to unsatisfactory results and flickering artifacts.
    More efforts are needed to remove visual flickering effectively, exploit the temporal
    information between neighboring frames, and speed up the enhancement speed.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展到低光照视频增强。与视频去模糊 [[103](#bib.bib103)]、视频去噪 [[104](#bib.bib104)] 和视频超分辨率 [[105](#bib.bib105)]
    等其他低级视觉任务中视频增强的快速发展不同，低光照视频增强的关注度较低。现有 LLIE 方法直接应用于视频通常会导致不满意的结果和闪烁伪影。需要更多努力来有效去除视觉闪烁，利用相邻帧之间的时间信息，并加快增强速度。
- en: Integrating Semantic Information. Semantic information is crucial for low-light
    enhancement. It guides the networks to distinguish different regions in the process
    of enhancement. A network without access to semantic priors can easily deviate
    the original color of a region, e.g., turning black hair to gray color after enhancement.
    Therefore, integrating semantic priors into LLIE models is a promising research
    direction. Similar work has been done on image super-resolution [[106](#bib.bib106),
    [107](#bib.bib107)] and face restoration [[108](#bib.bib108)].
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 集成语义信息。语义信息对低光增强至关重要。它引导网络在增强过程中区分不同区域。没有语义先验的网络可能会轻易偏离区域的原始颜色，例如，增强后将黑发变为灰色。因此，将语义先验集成到LLIE模型中是一个有前途的研究方向。类似的工作已经在图像超分辨率[[106](#bib.bib106),
    [107](#bib.bib107)]和面部修复[[108](#bib.bib108)]上完成。
- en: Acknowledgments
  id: totrans-758
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This study is supported under the RIE2020 Industry Alignment Fund Industry Collaboration
    Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution
    from the industry partner(s). It is also partially supported by the NTU SUG and
    NAP grant. Chunle Guo is sponsored by CAAI-Huawei MindSpore Open Fund.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究在RIE2020产业对接基金产业合作项目（IAF-ICP）资助计划下得到支持，并得到了行业合作伙伴的现金和实物贡献。同时，部分资金由NTU SUG和NAP资助。Chunle
    Guo得到了CAAI-Huawei MindSpore开放基金的赞助。
- en: References
  id: totrans-760
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] K. G. Lore, A. Akintayo, and S. Sarkar, “LLNet: A deep autoencoder approach
    to natural low-light image enhancement,” *PR*, vol. 61, pp. 650–662, 2017.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] K. G. Lore, A. Akintayo, 和 S. Sarkar，“LLNet：一种用于自然低光图像增强的深度自编码器方法，” *PR*，第61卷，页650–662，2017年。'
- en: '[2] C. Chen, Q. Chen, J. Xu, and V. Koltun, “Learning to see in the dark,”
    in *CVPR*, 2018, pp. 3291–3300.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] C. Chen, Q. Chen, J. Xu, 和 V. Koltun，“学习在黑暗中观察，” *CVPR*，2018年，页3291–3300。'
- en: '[3] F. Lv, F. Lu, J. Wu, and C. Lim, “MBLLEN: Low-light image/video enhancement
    using cnns,” in *BMVC*, 2018.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] F. Lv, F. Lu, J. Wu, 和 C. Lim，“MBLLEN：使用CNN的低光图像/视频增强，” *BMVC*，2018年。'
- en: '[4] C. Wei, W. Wang, W. Yang, and J. Liu, “Deep retinex decomposition for low-light
    enhancement,” in *BMVC*, 2018.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] C. Wei, W. Wang, W. Yang, 和 J. Liu，“用于低光增强的深度Retinex分解，” *BMVC*，2018年。'
- en: '[5] C. Li, J. Guo, F. Porikli, and Y. Pang, “LightenNet: A convolutional neural
    network for weakly illuminated image enhancement,” *PRL*, vol. 104, pp. 15–22,
    2018.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] C. Li, J. Guo, F. Porikli, 和 Y. Pang，“LightenNet：一种用于弱光图像增强的卷积神经网络，” *PRL*，第104卷，页15–22，2018年。'
- en: '[6] J. Cai, S. Gu, and L. Zhang, “Learning a deep single image contrast enhancer
    from multi-exposure images,” *TIP*, vol. 27, no. 4, pp. 2049–2062, 2018.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] J. Cai, S. Gu, 和 L. Zhang，“从多曝光图像中学习深度单图像对比度增强器，” *TIP*，第27卷，第4期，页2049–2062，2018年。'
- en: '[7] R. Wang, Q. Zhang, C.-W. Fu, X. Shen, W.-S. Zheng, and J. Jia, “Underexposed
    photo enhancement usign deep illumination estimation,” in *CVPR*, 2019, pp. 6849–6857.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] R. Wang, Q. Zhang, C.-W. Fu, X. Shen, W.-S. Zheng, 和 J. Jia，“使用深度照明估计进行曝光不足的照片增强，”
    *CVPR*，2019年，页6849–6857。'
- en: '[8] C. Chen, Q. Chen, M. N. Do, and V. Koltun, “Seeing motion in the dark,”
    in *ICCV*, 2019, pp. 3185–3194.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] C. Chen, Q. Chen, M. N. Do, 和 V. Koltun，“在黑暗中观察运动，” *ICCV*，2019年，页3185–3194。'
- en: '[9] H. Jiang and Y. Zheng, “Learning to see moving object in the dark,” in
    *ICCV*, 2019, pp. 7324–7333.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] H. Jiang 和 Y. Zheng，“学习在黑暗中观察运动物体，” *ICCV*，2019年，页7324–7333。'
- en: '[10] Y. Wang, Y. Cao, Z. Zha, J. Zhang, Z. Xiong, W. Zhang, and F. Wu, “Progressive
    retinex: Mutually reinforced illumination-noise perception network for low-light
    image enhancement,” in *ACMMM*, 2019, pp. 2015–2023.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Wang, Y. Cao, Z. Zha, J. Zhang, Z. Xiong, W. Zhang, 和 F. Wu，“渐进式Retinex：用于低光图像增强的互相强化的照明噪声感知网络，”
    *ACMMM*，2019年，页2015–2023。'
- en: '[11] Y. Zhang, J. Zhang, and X. Guo, “Kindling the darkness: A practical low-light
    image enhancer,” in *ACMMM*, 2019, pp. 1632–1640.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Y. Zhang, J. Zhang, 和 X. Guo，“点燃黑暗：一种实用的低光图像增强方法，” *ACMMM*，2019年，页1632–1640。'
- en: '[12] W. Ren, S. Liu, L. Ma, Q. Xu, X. Xu, X. Cao, J. Du, and M.-H. Yang, “Low-light
    image enhancement via a deep hybrid network,” *TIP*, vol. 28, no. 9, pp. 4364–4375,
    2019.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] W. Ren, S. Liu, L. Ma, Q. Xu, X. Xu, X. Cao, J. Du, 和 M.-H. Yang，“通过深度混合网络进行低光图像增强，”
    *TIP*，第28卷，第9期，页4364–4375，2019年。'
- en: '[13] K. Xu, X. Yang, B. Yin, and R. W. H. Lau, “Learning to restore low-light
    images via decomposition-and-enhancement,” in *CVPR*, 2020, pp. 2281–2290.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] K. Xu, X. Yang, B. Yin, 和 R. W. H. Lau，“通过分解与增强学习恢复低光图像，” *CVPR*，2020年，页2281–2290。'
- en: '[14] M. Fan, W. Wang, W. Yang, and J. Liu, “Integrating semantic segmentation
    and retinex model for low light image enhancement,” in *ACMMM*, 2020, pp. 2317–2325.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. Fan, W. Wang, W. Yang, 和 J. Liu，“将语义分割与Retinex模型集成用于低光图像增强，” *ACMMM*，2020年，页2317–2325。'
- en: '[15] F. Lv, B. Liu, and F. Lu, “Fast enhancement for non-uniform illumination
    images using light-weight cnns,” in *ACMMM*, 2020, pp. 1450–1458.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] F. Lv, B. Liu, 和 F. Lu，“使用轻量级CNNs进行非均匀光照图像的快速增强”，在*ACMMM*，2020年，1450–1458页。'
- en: '[16] M. Zhu, P. Pan, W. Chen, and Y. Yang, “EEMEFN: Low-light image enhancement
    via edge-enhanced multi-exposure fusion network,” in *AAAI*, 2020, pp. 13 106–13 113.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Zhu, P. Pan, W. Chen, 和 Y. Yang，“EEMEFN：通过边缘增强多曝光融合网络进行低光图像增强”，在*AAAI*，2020年，13 106–13 113页。'
- en: '[17] D. Triantafyllidou, S. Moran, S. McDonagh, S. Parisot, and G. Slabaugh,
    “Low light video enhancement using synthetic data produced with an intermediate
    domain mapping,” in *ECCV*, 2020, pp. 103–119.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] D. Triantafyllidou, S. Moran, S. McDonagh, S. Parisot, 和 G. Slabaugh，“使用合成数据和中间域映射进行低光视频增强”，在*ECCV*，2020年，103–119页。'
- en: '[18] J. Li, J. Li, F. Fang, F. Li, and G. Zhang, “Luminance-aware pyramid network
    for low-light image enhancement,” *TMM*, 2020.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Li, J. Li, F. Fang, F. Li, 和 G. Zhang，“感光度感知金字塔网络用于低光图像增强”，*TMM*，2020年。'
- en: '[19] L. Wang, Z. Liu, W. Siu, and D. P. K. Lun, “Lightening network for low-light
    image enhancement,” *TIP*, vol. 29, pp. 7984–7996, 2020.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] L. Wang, Z. Liu, W. Siu, 和 D. P. K. Lun，“低光图像增强的光网络”，*TIP*，第29卷，7984–7996页，2020年。'
- en: '[20] K. Lu and L. Zhang, “TBEFN: A two-branch exposure-fusion network for low-light
    image enhancement,” *TMM*, 2020.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. Lu 和 L. Zhang，“TBEFN：用于低光图像增强的双分支曝光融合网络”，*TMM*，2020年。'
- en: '[21] S. Lim and W. Kim, “DSLR: Deep stacked laplacian restorer for low-light
    image enhancement,” *TMM*, 2020.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. Lim 和 W. Kim，“DSLR：深度堆叠拉普拉斯修复器用于低光图像增强”，*TMM*，2020年。'
- en: '[22] F. Zhang, Y. Li, S. You, and Y. Fu, “Learning temporal consistency for
    low light video enhancement from single images,” in *CVPR*, 2021.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] F. Zhang, Y. Li, S. You, 和 Y. Fu，“从单张图像中学习时间一致性以增强低光视频”，在*CVPR*，2021年。'
- en: '[23] J. Li, X. Feng, and Z. Hua, “Low-light image enhancement via progressive-recursive
    network,” *TCSVT*, 2021.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Li, X. Feng, 和 Z. Hua，“通过渐进递归网络增强低光图像”，*TCSVT*，2021年。'
- en: '[24] W. Yang, W. Wang, H. Huang, S. Wang, and J. Liu, “Sparse gradient regularized
    deep retinex network for robust low-light image enhancement,” *TIP*, vol. 30,
    pp. 2072–2086, 2021.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] W. Yang, W. Wang, H. Huang, S. Wang, 和 J. Liu，“稀疏梯度正则化的深度Retinex网络用于鲁棒低光图像增强”，*TIP*，第30卷，2072–2086页，2021年。'
- en: '[25] R. Yu, W. Liu, Y. Zhang, Z. Qu, D. Zhao, and B. Zhang, “DeepExposure:
    Learning to expose photos with asynchronously reinforced adversarial learning,”
    in *NeurIPS*, 2018, pp. 2149–2159.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] R. Yu, W. Liu, Y. Zhang, Z. Qu, D. Zhao, 和 B. Zhang，“DeepExposure：通过异步增强对抗学习学习曝光照片”，在*NeurIPS*，2018年，2149–2159页。'
- en: '[26] Y. Jiang, X. Gong, D. Liu, Y. Cheng, C. Fang, X. Shen, J. Yang, P. Zhou,
    and Z. Wang, “EnlightenGAN: Deep light enhancement without paired supervision,”
    *TIP*, vol. 30, pp. 2340–2349, 2021.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. Jiang, X. Gong, D. Liu, Y. Cheng, C. Fang, X. Shen, J. Yang, P. Zhou,
    和 Z. Wang，“EnlightenGAN：无需配对监督的深度光线增强”，*TIP*，第30卷，2340–2349页，2021年。'
- en: '[27] L. Zhang, L. Zhang, X. Liu, Y. Shen, S. Zhang, and S. Zhao, “Zero-shot
    restoration of back-lit images using deep internal learning,” in *ACMMM*, 2019,
    pp. 1623–1631.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] L. Zhang, L. Zhang, X. Liu, Y. Shen, S. Zhang, 和 S. Zhao，“使用深度内部学习的零样本反向光图像恢复”，在*ACMMM*，2019年，1623–1631页。'
- en: '[28] C. Guo, C. Li, J. Guo, C. C. Loy, J. Hou, S. Kwong, and R. Cong, “Zero-reference
    deep curve estimation for low-light image enhancement,” in *CVPR*, 2020, pp. 1780–1789.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] C. Guo, C. Li, J. Guo, C. C. Loy, J. Hou, S. Kwong, 和 R. Cong，“零参考深度曲线估计用于低光图像增强”，在*CVPR*，2020年，1780–1789页。'
- en: '[29] A. Zhu, L. Zhang, Y. Shen, Y. Ma, S. Zhao, and Y. Zhou, “Zero-shot restoration
    of underexposed images via robust retinex decomposition,” in *ICME*, 2020, pp.
    1–6.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] A. Zhu, L. Zhang, Y. Shen, Y. Ma, S. Zhao, 和 Y. Zhou，“通过鲁棒的Retinex分解进行零样本曝光图像恢复”，在*ICME*，2020年，1–6页。'
- en: '[30] C. Li, C. Guo, and C. C. Loy, “Learning to enhance low-light image via
    zero-reference deep curve estimation,” *TPAMI*, 2021.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] C. Li, C. Guo, 和 C. C. Loy，“通过零参考深度曲线估计增强低光图像”，*TPAMI*，2021年。'
- en: '[31] Z. Zhao, B. Xiong, L. Wang, Q. Ou, L. Yu, and F. Kuang, “Retinexdip: A
    unified deep framework for low-light image enhancement,” *TCSVT*, 2021.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Z. Zhao, B. Xiong, L. Wang, Q. Ou, L. Yu, 和 F. Kuang，“Retinexdip：统一的深度框架用于低光图像增强”，*TCSVT*，2021年。'
- en: '[32] R. Liu, L. Ma, J. Zhang, X. Fan, and Z. Luo, “Retinex-inspired unrolling
    with cooperative prior architecture search for low-light image enhancement,” in
    *CVPR*, 2021.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] R. Liu, L. Ma, J. Zhang, X. Fan, 和 Z. Luo，“受Retinex启发的解卷积与合作先验架构搜索用于低光图像增强”，在*CVPR*，2021年。'
- en: '[33] W. Yang, S. Wang, Y. Fang, Y. Wang, and J. Liu, “From fidelity to perceptual
    quality: A semi-supervised approach for low-light image enhancement,” in *CVPR*,
    2020, pp. 3063–3072.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] W. Yang, S. Wang, Y. Fang, Y. Wang, 和 J. Liu，“从保真度到感知质量：一种半监督方法用于低光图像增强”，在*CVPR*，2020年，3063–3072页。'
- en: '[34] W. Yang, S. Wang, Y. F. nd Yue Wang, and J. Liu, “Band representation-based
    semi-supervised low-light image enhancement: Bridging the gap between signal fidelity
    and perceptual quality,” *TIP*, vol. 30, pp. 3461–3473, 2021.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] W. Yang, S. Wang, Y. F. 和 Yue Wang, 和 J. Liu, “基于带状表示的半监督低光图像增强：弥合信号保真度和感知质量之间的差距，”
    *TIP*，第30卷，第3461–3473页，2021年。'
- en: '[35] H. Ibrahim and N. S. P. Kong, “Brightness preserving dynamic histogram
    equalization for image contrast enhancement,” *TCE*, vol. 53, no. 4, pp. 1752–1758,
    2007.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] H. Ibrahim 和 N. S. P. Kong, “保持亮度的动态直方图均衡化用于图像对比度增强，” *TCE*，第53卷，第4期，第1752–1758页，2007年。'
- en: '[36] M. Abdullah-AI-Wadud, M. H. Kabir, M. A. A. Dewan, and O. Chae, “A dynamic
    histogram equalization for image contrast enhancement,” *TCE*, vol. 53, no. 2,
    pp. 593–600, 2007.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] M. Abdullah-AI-Wadud, M. H. Kabir, M. A. A. Dewan, 和 O. Chae, “用于图像对比度增强的动态直方图均衡化，”
    *TCE*，第53卷，第2期，第593–600页，2007年。'
- en: '[37] S. Wang, J. Zheng, H. Hu, and B. Li, “Naturalness preserved enhancement
    algorithm for non-uniform illumination images,” *TIP*, vol. 22, no. 9, pp. 3538–3548,
    2013.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. Wang, J. Zheng, H. Hu, 和 B. Li, “用于非均匀光照图像的自然保持增强算法，” *TIP*，第22卷，第9期，第3538–3548页，2013年。'
- en: '[38] X. Fu, Y. Liao, D. Zeng, Y. Huang, X. Zhang, and X. Ding, “A probabilistic
    method for image enhancement with simultaneous illumination and reflectance estimation,”
    *TIP*, vol. 24, no. 12, pp. 4965–4977, 2015.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] X. Fu, Y. Liao, D. Zeng, Y. Huang, X. Zhang, 和 X. Ding, “一种带有同步光照和反射估计的图像增强概率方法，”
    *TIP*，第24卷，第12期，第4965–4977页，2015年。'
- en: '[39] X. Guo, Y. Li, and H. Ling, “LIME: Low-light image enhancement via illumination
    map estimation,” *TIP*, vol. 26, no. 2, pp. 982–993, 2016.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] X. Guo, Y. Li, 和 H. Ling, “LIME：通过光照图估计的低光图像增强，” *TIP*，第26卷，第2期，第982–993页，2016年。'
- en: '[40] S. Park, S. Yu, B. Moon, S. Ko, and J. Paik, “Low-light image enhancement
    using variational optimization-based retinex model,” *TCE*, vol. 63, no. 2, pp.
    178–184, 2017.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] S. Park, S. Yu, B. Moon, S. Ko, 和 J. Paik, “使用变分优化基于Retinex模型的低光图像增强，”
    *TCE*，第63卷，第2期，第178–184页，2017年。'
- en: '[41] M. Li, J. Liu, W. Yang, X. Sun, and Z. Guo, “Structure-revealing low-light
    image enhancement via robust retinex model,” *TIP*, vol. 27, no. 6, pp. 2828–2841,
    2018.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] M. Li, J. Liu, W. Yang, X. Sun, 和 Z. Guo, “通过鲁棒Retinex模型揭示结构的低光图像增强，”
    *TIP*，第27卷，第6期，第2828–2841页，2018年。'
- en: '[42] Z. Gu, F. Li, F. Fang, and G. Zhang, “A novel retinex-based fractional-order
    variational model for images with severely low light,” *TIP*, vol. 29, pp. 3239–3253,
    2019.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Z. Gu, F. Li, F. Fang, 和 G. Zhang, “一种针对严重低光图像的新型Retinex基础分数阶变分模型，” *TIP*，第29卷，第3239–3253页，2019年。'
- en: '[43] X. Ren, W. Yang, W.-H. Cheng, and J. Liu, “LR3M: Robust low-light enhancement
    via low-rank regularized retinex model,” *TIP*, vol. 29, pp. 5862–5876, 2020.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] X. Ren, W. Yang, W.-H. Cheng, 和 J. Liu, “LR3M：通过低秩正则化的Retinex模型实现鲁棒的低光增强，”
    *TIP*，第29卷，第5862–5876页，2020年。'
- en: '[44] S. Hao, X. Han, Y. Guo, X. Xu, and M. Wang, “Low-light image enhancement
    with semi-decoupled decomposition,” *TMM*, vol. 22, no. 12, pp. 3025–3038, 2020.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. Hao, X. Han, Y. Guo, X. Xu, 和 M. Wang, “使用半解耦分解的低光图像增强，” *TMM*，第22卷，第12期，第3025–3038页，2020年。'
- en: '[45] Y. Hu, H. He, C. Xu, B. Wang, and S. Lin, “Exposure: A white-box photo
    post-processing framework,” *ACM Graph.*, vol. 37, no. 2, pp. 1–17, 2018.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Y. Hu, H. He, C. Xu, B. Wang, 和 S. Lin, “Exposure：一个白盒照片后处理框架，” *ACM Graph.*，第37卷，第2期，第1–17页，2018年。'
- en: '[46] M. Gharbi, J. Chen, J. T. Barron, S. W. Hasinoff, and F. Durand, “Deep
    bilateral learning for real-time image enhancement,” *ACM Graph.*, vol. 36, no. 4,
    pp. 1–12, 2017.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] M. Gharbi, J. Chen, J. T. Barron, S. W. Hasinoff, 和 F. Durand, “用于实时图像增强的深度双边学习，”
    *ACM Graph.*，第36卷，第4期，第1–12页，2017年。'
- en: '[47] Y. Chen, Y. Wang, M. Kao, and Y. Chuang, “Deep photo enhancer: Unpaired
    learning for image enhancement form photographs wigh gans,” in *CVPR*, 2018, pp.
    6306–6314.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Y. Chen, Y. Wang, M. Kao, 和 Y. Chuang, “深度照片增强器：通过GANs的无配对学习进行图像增强，” 见
    *CVPR*，2018年，第6306–6314页。'
- en: '[48] Y. Deng, C. C. Loy, and X. Tang, “Aesthetic-driven image enhancement by
    adversarial learning,” in *ACMMM*, 2018, pp. 870–878.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Deng, C. C. Loy, 和 X. Tang, “基于对抗学习的美学驱动图像增强，” 见 *ACMMM*，2018年，第870–878页。'
- en: '[49] Z. Yan, H. Zhang, B. Wang, S. Paris, and Y. Yu, “Automatic photo adjustment
    using deep neural networks,” *ACM Graph.*, vol. 35, no. 2, pp. 1–15, 2016.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Z. Yan, H. Zhang, B. Wang, S. Paris, 和 Y. Yu, “使用深度神经网络的自动照片调整，” *ACM
    Graph.*，第35卷，第2期，第1–15页，2016年。'
- en: '[50] Q. Chen, J. Xu, and V. Koltun, “Fast image processing with fully-convolutional
    networks,” in *CVPR*, 2017, pp. 2497–2506.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Q. Chen, J. Xu, 和 V. Koltun, “利用全卷积网络进行快速图像处理，” 见 *CVPR*，2017年，第2497–2506页。'
- en: '[51] Z. Ni, W. Yang, S. Wang, L. Ma, and S. Kwong, “Towards unsupervised deep
    image enhancement with generative adversarial network,” *TIP*, vol. 29, pp. 9140–9151,
    2020.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Z. Ni, W. Yang, S. Wang, L. Ma, 和 S. Kwong, “面向无监督深度图像增强的生成对抗网络，” *TIP*，第
    29 卷，页 9140–9151，2020 年。'
- en: '[52] H. Zeng, J. Cai, L. Li, Z. Cao, and L. Zhang, “Learning image-adaptive
    3D lookup tables for high performance photo enhancement in real-time,” *TPAMI*,
    2020.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] H. Zeng, J. Cai, L. Li, Z. Cao, 和 L. Zhang, “实时高性能照片增强的图像自适应 3D 查找表学习，”
    *TPAMI*，2020 年。'
- en: '[53] C. Li, C. Guo, Q. Ai, S. Zhou, and C. C. Loy, “Flexible piecewise curves
    estimation for photo enhancement,” *arXiv preprint arXiv:2010.13412*, 2020.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] C. Li, C. Guo, Q. Ai, S. Zhou, 和 C. C. Loy, “用于照片增强的灵活分段曲线估计，” *arXiv
    预印本 arXiv:2010.13412*，2020 年。'
- en: '[54] W. Wang, X. Wu, X. Yuan, and Z. Gao, “An experiment-based review of low-light
    image enhancement methods,” *IEEE Access*, vol. 8, pp. 87 884–87 917, 2020.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] W. Wang, X. Wu, X. Yuan, 和 Z. Gao, “基于实验的低光图像增强方法综述，” *IEEE Access*，第
    8 卷，页 87 884–87 917，2020 年。'
- en: '[55] J. Liu, D. Xu, W. Yang, M. Fan, and H. Huang, “Benchmarking low-light
    image enhancement and beyond,” *IJCV*, 2021.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] J. Liu, D. Xu, W. Yang, M. Fan, 和 H. Huang, “低光图像增强及其超越的基准测试，” *IJCV*，2021
    年。'
- en: '[56] V. Jain and S. Seung, “Natural image denoising with convolutional networks,”
    in *NeurIPS*, 2008, pp. 1–8.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] V. Jain 和 S. Seung, “使用卷积网络的自然图像去噪，” 见 *NeurIPS*，2008 年，页 1–8。'
- en: '[57] K. Xu, X. Yang, B. Yin, and R. W. H. Lau, “Learning to restore low-light
    images via decomposition-and-enhancement,” in *CVPR*, 2020, pp. 2281–2290.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] K. Xu, X. Yang, B. Yin, 和 R. W. H. Lau, “通过分解和增强学习恢复低光图像，” 见 *CVPR*，2020
    年，页 2281–2290。'
- en: '[58] E. H. Land, “An alternative technique for the computation of the designator
    in the retinex theory of color vision,” *National Academy of Sciences*, vol. 83,
    no. 10, pp. 3078–3080, 1986.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] E. H. Land, “在视网膜理论中计算设计者的替代技术，” *国家科学院院刊*，第 83 卷，第 10 期，页 3078–3080，1986
    年。'
- en: '[59] D. J. Jobson, Z. ur Rahman, and G. A. Woodell, “Properties and performance
    of a center/surround retinex,” *TIP*, vol. 6, no. 3, pp. 451–462, 1997.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] D. J. Jobson, Z. ur Rahman, 和 G. A. Woodell, “中心/周围视网膜的属性和性能，” *TIP*，第
    6 卷，第 3 期，页 451–462，1997 年。'
- en: '[60] R. Wang, Q. Zhang, C.-W. Fu, X. Shen, W.-S. Zheng, and J. Jia, “Underexposed
    photo enhancement using deep illumination estimation,” in *CVPR*, 2019, pp. 6849–6857.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] R. Wang, Q. Zhang, C.-W. Fu, X. Shen, W.-S. Zheng, 和 J. Jia, “使用深度光照估计进行曝光不足照片增强，”
    见 *CVPR*，2019 年，页 6849–6857。'
- en: '[61] X. Guo, Y. Zhang, J. Ma, W. Liu, and J. Zhang, “Beyond brightening low-light
    images,” *IJCV*, 2020.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] X. Guo, Y. Zhang, J. Ma, W. Liu, 和 J. Zhang, “超越亮化低光图像，” *IJCV*，2020 年。'
- en: '[62] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional networks
    for biomedical image segmentation,” in *MICCAI*, 2015, pp. 234–241.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] O. Ronneberger, P. Fischer, 和 T. Brox, “U-Net：用于生物医学图像分割的卷积网络，” 见 *MICCAI*，2015
    年，页 234–241。'
- en: '[63] T. Xue, B. Chen, J. Wu, D. Wei, and W. T. Freeman, “Video enhancement
    with task-oriented flow,” *IJCV*, vol. 127, no. 8, pp. 1106–1125, 2019.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] T. Xue, B. Chen, J. Wu, D. Wei, 和 W. T. Freeman, “基于任务导向流的视频增强，” *IJCV*，第
    127 卷，第 8 期，页 1106–1125，2019 年。'
- en: '[64] K. He, J. Sun, and X. Tang, “Guided image filtering,” *TPAMI*, vol. 35,
    no. 6, pp. 1397–1409, 2013.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] K. He, J. Sun, 和 X. Tang, “引导图像滤波，” *TPAMI*，第 35 卷，第 6 期，页 1397–1409，2013
    年。'
- en: '[65] P. Whittle, “The psychophysics of contrast brightness,” *A. L. Gilchrist
    (Ed.), Brightness, lightness, and transparenc (1994),*, pp. 35–110, 1993.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] P. Whittle, “对比度亮度的心理物理学，” *A. L. Gilchrist (编)，亮度、光度和透明度 (1994 年)*，页
    35–110，1993 年。'
- en: '[66] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Deep image prior,” in *CVPR*,
    2018.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] D. Ulyanov, A. Vedaldi, 和 V. Lempitsky, “深度图像先验，” 见 *CVPR*，2018 年。'
- en: '[67] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for image
    restoration with neural networks,” *TIP*, vol. 3, no. 1, pp. 47–56, 2017.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] H. Zhao, O. Gallo, I. Frosio, 和 J. Kautz, “用于图像恢复的神经网络损失函数，” *TIP*，第 3
    卷，第 1 期，页 47–56，2017 年。'
- en: '[68] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
    A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, “Photo-realistic single image
    super-resolution using a generative adversarial network,” in *CVPR*, 2017, pp.
    4681–4690.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
    A. Aitken, A. Tejani, J. Totz, Z. Wang, 和 W. Shi, “使用生成对抗网络的照片真实单图像超分辨率，” 见 *CVPR*，2017
    年，页 4681–4690。'
- en: '[69] K. Simoayan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] K. Simoayan 和 A. Zisserman, “用于大规模图像识别的非常深卷积网络，” *arXiv 预印本 arXiv:1409.1556*，2014
    年。'
- en: '[70] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei, “ImageNet:
    A large-scale hierarchical image database,” in *CVPR*, 2009, pp. 248–255.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Deng, W. Dong, R. Socher, L. Li, K. Li, 和 L. Fei-Fei, “ImageNet：一个大规模分层图像数据库，”
    见 *CVPR*，2009 年，页 248–255。'
- en: '[71] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
    A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, “Photo-realistic single image
    super-resolution using a generative adversarial network,” in *CVPR*, 2017.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
    A. Aitken, A. Tejani, J. Totz, Z. Wang, 和 W. Shi, “使用生成对抗网络进行照片级真实单幅图像超分辨率，” 发表在*CVPR*，2017年。'
- en: '[72] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, and C. C. Loy,
    “ESRGAN: Enhanced super-resolution generative adversarial networks,” in *ECCVW*,
    2018.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, 和 C. C. Loy, “ESRGAN:
    增强的超分辨率生成对抗网络，” 发表在*ECCVW*，2018年。'
- en: '[73] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution using
    deep convolutional networks,” *TPAMI*, vol. 38, no. 2, pp. 295–307, 2015.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] C. Dong, C. C. Loy, K. He, 和 X. Tang, “使用深度卷积网络进行图像超分辨率，” *TPAMI*，第38卷，第2期，第295–307页，2015年。'
- en: '[74] Q. Xu, C. Zhang, and L. Zhang, “Denoising convolutional neural network,”
    in *ICIA*, 2015, pp. 1184–1187.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Q. Xu, C. Zhang, 和 L. Zhang, “去噪卷积神经网络，” 发表在*ICIA*，2015年，第1184–1187页。'
- en: '[75] X. Fu, J. Huang, D. Zeng, Y. Huang, X. Ding, and J. Paisley, “Removing
    rain from single images via a deep detail network,” in *CVPR*, 2017, pp. 3855–3863.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] X. Fu, J. Huang, D. Zeng, Y. Huang, X. Ding, 和 J. Paisley, “通过深度细节网络从单幅图像中去除雨水，”
    发表在*CVPR*，2017年，第3855–3863页。'
- en: '[76] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, “Deep joint rain
    detection and removal from a single image,” in *CVPR*, 2017, pp. 1357–1366.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, 和 S. Yan, “深度联合雨水检测与去除，”
    发表在*CVPR*，2017年，第1357–1366页。'
- en: '[77] X. Fu, Q. Qi, Z. Zha, X. Ding, F. Wu, and J. Paisley, “Successive graph
    convolutional network for image deraining,” *IJCV*, vol. 129, no. 5, pp. 1691–1711,
    2021.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] X. Fu, Q. Qi, Z. Zha, X. Ding, F. Wu, 和 J. Paisley, “图像去雨的成功图卷积网络，” *IJCV*，第129卷，第5期，第1691–1711页，2021年。'
- en: '[78] J. Sun, W. Cao, Z. Xu, and J. Ponce, “Learning a convolutional neural
    network for non-uniform motion blur removal,” in *CVPR*, 2015, pp. 769–777.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] J. Sun, W. Cao, Z. Xu, 和 J. Ponce, “学习一个卷积神经网络用于去除非均匀运动模糊，” 发表在*CVPR*，2015年，第769–777页。'
- en: '[79] V. Bychkovsky, S. Paris, E. Chan, and F. Durand, “Learning photographic
    global tonal adjustment with a database of input/output image pairs,” in *CVPR*,
    2011, pp. 97–104.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] V. Bychkovsky, S. Paris, E. Chan, 和 F. Durand, “通过输入/输出图像对数据库学习摄影全球色调调整，”
    发表在*CVPR*，2011年，第97–104页。'
- en: '[80] Y. Yuan, W. Yang, W. Ren, J. Liu, W. JScheirer, and W. Zhangyang, “UG+
    Track 2: A collective benchmark effort for evaluating and advancing image understanding
    in poor visibility environments,” *arXiv arXiv:1904.04474*, 2019.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Yuan, W. Yang, W. Ren, J. Liu, W. JScheirer, 和 W. Zhangyang, “UG+ Track
    2: 一个评估和推动恶劣可见环境下图像理解的集体基准努力，” *arXiv arXiv:1904.04474*，2019年。'
- en: '[81] Y. P. Loh and C. S. Chan, “Getting to know low-light images with the exclusively
    dark dataset,” *CVIU*, vol. 178, pp. 30–42, 2019.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Y. P. Loh 和 C. S. Chan, “通过专用黑暗数据集了解低光图像，” *CVIU*，第178卷，第30–42页，2019年。'
- en: '[82] L. Chulwoo, L. Chul, L. Young-Yoon, and K. Chang-su, “Power-constrained
    contrast enhancement for emissive displays based on histogram equalization,” *TIP*,
    vol. 21, no. 1, pp. 80–93, 2012.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] L. Chulwoo, L. Chul, L. Young-Yoon, 和 K. Chang-su, “基于直方图均衡化的发光显示器的功率约束对比度增强，”
    *TIP*，第21卷，第1期，第80–93页，2012年。'
- en: '[83] C. Lee, C. Lee, and C.-S. Kim, “Contrast enhancement based on layered
    difference representation of 2d histograms,” *TIP*, vol. 22, no. 12, pp. 5372–5384,
    2013.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] C. Lee, C. Lee, 和 C.-S. Kim, “基于2d直方图分层差异表示的对比度增强，” *TIP*，第22卷，第12期，第5372–5384页，2013年。'
- en: '[84] F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell,
    “BDD100K: A diverse driving video database with scalable annotation tooling,”
    *arXiv preprint arXiv:1805.04687*, 2018.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, 和 T. Darrell, “BDD100K:
    一个多样化的驾驶视频数据库，具有可扩展的注释工具，” *arXiv preprint arXiv:1805.04687*，2018年。'
- en: '[85] C. Chen, Q. Chen, J. Xu, and V. Koltun, “Learning to see in the dark,”
    in *CVPR*, 2018, pp. 3291–3300.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] C. Chen, Q. Chen, J. Xu, 和 V. Koltun, “学习在黑暗中看见，” 发表在*CVPR*，2018年，第3291–3300页。'
- en: '[86] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: From error visibility to structural similarity,” *TIP*, vol. 13, no. 4,
    pp. 600–612, 2004.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Z. Wang, A. C. Bovik, H. R. Sheikh, 和 E. P. Simoncelli, “图像质量评估：从错误可见性到结构相似性，”
    *TIP*，第13卷，第4期，第600–612页，2004年。'
- en: '[87] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *CVPR*, 2018, pp. 586–595.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, 和 O. Wang, “深度特征作为感知度量的非凡有效性，”
    发表在*CVPR*，2018年，第586–595页。'
- en: '[88] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a completely blind
    image quality analyzer,” *SPL*, vol. 20, no. 3, pp. 209–212, 2013.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] A. Mittal, R. Soundararajan, 和 A. C. Bovik, “制作一个完全盲的图像质量分析器，” *SPL*，第20卷，第3期，第209–212页，2013年。'
- en: '[89] Y. Blau and T. Michaeli, “The perception-distortion tradeoff,” in *CVPR*,
    2018, pp. 6228–6237.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Y. Blau 和 T. Michaeli，“感知-失真权衡，”发表于*CVPR*，2018年，页码6228–6237。'
- en: '[90] C. Ma, C.-Y. Yang, X. Yang, and M.-H. Yang, “Learning a non-reference
    quality metric for single-image super-resolution,” *CVIU*, vol. 158, pp. 1–16,
    2017.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] C. Ma, C.-Y. Yang, X. Yang, 和 M.-H. Yang，“为单图像超分辨率学习一种无参考质量度量，”*CVIU*，第158卷，页码1–16，2017年。'
- en: '[91] Y. Fang, H. Zhu, Y. Zeng, K. Ma, and Z. Wang, “Perceptual quality assessment
    of smartphone photography,” in *ICCV*, 2020, pp. 3677–3686.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Y. Fang, H. Zhu, Y. Zeng, K. Ma, 和 Z. Wang，“智能手机摄影的感知质量评估，”发表于*ICCV*，2020年，页码3677–3686。'
- en: '[92] M. Danelljan, G. Bhat, F. S. Khan, and M. Felsberg, “ECO: Efficient convolution
    operators for tracking,” in *CVPR*, 2017, pp. 6638–6646.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] M. Danelljan, G. Bhat, F. S. Khan, 和 M. Felsberg，“ECO：用于跟踪的高效卷积算子，”发表于*CVPR*，2017年，页码6638–6646。'
- en: '[93] J. Li, Y. Wang, C. Wang, Y. Tai, J. Qian, J. Yang, C. Wang, J. Li, and
    F. Huang, “DSFD: Dual shot face detector,” in *CVPR*, 2019, pp. 5060–5069.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Li, Y. Wang, C. Wang, Y. Tai, J. Qian, J. Yang, C. Wang, J. Li, 和 F.
    Huang，“DSFD：双镜头人脸检测器，”发表于*CVPR*，2019年，页码5060–5069。'
- en: '[94] S. Yang, P. Luo, C. C. Loy, and X. Tang, “Wider Face: A face detection
    benchmark,” in *CVPR*, 2016, pp. 5525–5533.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] S. Yang, P. Luo, C. C. Loy, 和 X. Tang，“Wider Face：一个人脸检测基准，”发表于*CVPR*，2016年，页码5525–5533。'
- en: '[95] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “MobileNets: Efficient convolutional neural networks for mobile vision
    application,” *arXiv preprint arXiv:1704.04861*, 2017.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M.
    Andreetto, 和 H. Adam，“MobileNets：用于移动视觉应用的高效卷积神经网络，”*arXiv 预印本 arXiv:1704.04861*，2017年。'
- en: '[96] J. Liu, Q. Hou, M. M. Cheng, C. Wang, and J. Feng, “Improving convolutional
    networks with self-calibrated convolutions,” in *CVPR*, 2020, pp. 10 096–10 105.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] J. Liu, Q. Hou, M. M. Cheng, C. Wang, 和 J. Feng，“通过自校准卷积改进卷积网络，”发表于*CVPR*，2020年，页码10 096–10 105。'
- en: '[97] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L. Li, L. F. Fei, A. Yuille,
    J. Huang, and K. Murphy, “Progressive neural architecture search,” in *ECCV*,
    2018, pp. 19–34.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L. Li, L. F. Fei, A. Yuille,
    J. Huang, 和 K. Murphy，“渐进式神经架构搜索，”发表于*ECCV*，2018年，页码19–34。'
- en: '[98] C. Liu, L. C. Chen, F. Schroff, H. Adam, W. Hua, A. Yuille, and L. F.
    Fei, “Auto-Deeplab: Hierarchical neural architecture search for semantic image
    segmentation,” in *CVPR*, 2019, pp. 82–92.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] C. Liu, L. C. Chen, F. Schroff, H. Adam, W. Hua, A. Yuille, 和 L. F. Fei，“Auto-Deeplab：用于语义图像分割的分层神经架构搜索，”发表于*CVPR*，2019年，页码82–92。'
- en: '[99] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. D. M. Minderer, G. Heigold, S. Gelly, J. Uszekoreit, and N. Houlsby, “An image
    is worth 16x16 words: Transformers for image recognition at scale,” *arXiv preprint
    arXiv:2010.11929*, 2020.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. D. M. Minderer, G. Heigold, S. Gelly, J. Uszekoreit, 和 N. Houlsby，“一张图像值16x16个词：用于大规模图像识别的变压器，”*arXiv
    预印本 arXiv:2010.11929*，2020年。'
- en: '[100] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu,
    and W. Gao, “Pre-trained image processing transformer,” *arXiv preprint arXiv:2012.00364*,
    2020.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu,
    和 W. Gao，“预训练图像处理变压器，”*arXiv 预印本 arXiv:2012.00364*，2020年。'
- en: '[101] Y. Fang, H. Zhu, Y. Zeng, K. Ma, and Z. Wang, “Perceptual quality assessment
    of smartphone phtography,” in *CVPR*, 2020, pp. 3677–3686.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Y. Fang, H. Zhu, Y. Zeng, K. Ma, 和 Z. Wang，“智能手机摄影的感知质量评估，”发表于*CVPR*，2020年，页码3677–3686。'
- en: '[102] H. Talebi and P. Milanfar, “NIMA: Neural image assessment,” *TIP*, vol. 27,
    no. 8, pp. 3998–4011, 2018.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] H. Talebi 和 P. Milanfar，“NIMA：神经图像评估，”*TIP*，第27卷，第8期，页码3998–4011，2018年。'
- en: '[103] T. H. Kim, K. M. Lee, B. Scholkopf, and M. Hirsch, “Online video deblurring
    via dynamic temporal blending network,” in *ICCV*, 2017, pp. 4038–4047.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] T. H. Kim, K. M. Lee, B. Scholkopf, 和 M. Hirsch，“通过动态时间融合网络进行在线视频去模糊，”发表于*ICCV*，2017年，页码4038–4047。'
- en: '[104] T. Ehret, A. Davy, J.-M. Morel, G. Facciolo, and P. Arias, “Model-blind
    video denoising via frame-to-frame training,” in *CVPR*, 2019, pp. 11 369–11 378.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] T. Ehret, A. Davy, J.-M. Morel, G. Facciolo, 和 P. Arias，“通过逐帧训练进行模型盲视频去噪，”发表于*CVPR*，2019年，页码11 369–11 378。'
- en: '[105] K. C. K. Chan, X. Wang, K. Yu, C. Dong, and C. C. Loy, “BasicVSR: The
    search for essential components in video super-resolution and beyond,” in *CVPR*,
    2021.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] K. C. K. Chan, X. Wang, K. Yu, C. Dong, 和 C. C. Loy，“BasicVSR：视频超分辨率及其以外的基本组件搜索，”发表于*CVPR*，2021年。'
- en: '[106] X. Wang, K. Yu, C. Dong, and C. C. Loy, “Recovering realistic texture
    in image super-resolution by deep spatial feature transform,” in *CVPR*, 2018,
    pp. 606–615.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] X. Wang, K. Yu, C. Dong, 和 C. C. Loy，“通过深度空间特征变换恢复图像超分辨率中的真实纹理，”发表于*CVPR*，2018年，页码606–615。'
- en: '[107] K. C. K. Chan, X. Wang, X. Xu, J. Gu, and C. C. Loy, “GLEAN: Generative
    latent bank for large-factor image super-resolution,” in *CVPR*, 2021.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] K. C. K. Chan, X. Wang, X. Xu, J. Gu, and C. C. Loy, “GLEAN: 大因子图像超分辨率的生成潜在库”，发表于
    *CVPR*，2021年。'
- en: '[108] X. Li, C. Chen, S. Zhou, X. Lin, W. Zuo, and L. Zhang, “Blind face restoration
    via deep multi-scale component dictionaries,” in *ECCV*, 2020, pp. 399–415.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] X. Li, C. Chen, S. Zhou, X. Lin, W. Zuo, and L. Zhang, “通过深度多尺度组件字典进行盲人脸修复”，发表于
    *ECCV*，2020年，第399–415页。'
- en: '| ![[Uncaptioned image]](img/0459bb4441d0e984facf544269493f84.png) | Chongyi
    Li is a Research Assistant Professor with the School of Computer Science and Engineering,
    Nanyang Technological University, Singapore. He received the Ph.D. degree from
    Tianjin University, China in 2018\. From 2016 to 2017, he was a joint-training
    Ph.D. Student with Australian National University, Australia. Prior to joining
    NTU, he was a postdoctoral fellow with City University of Hong Kong and Nanyang
    Technological University from 2018 to 2021\. His current research focuses on image
    processing, computer vision, and deep learning, particularly in the domains of
    image restoration and enhancement. He serves as an associate editor of the Journal
    of Signal, Image and Video Processing and a lead guest editor of the IEEE Journal
    of Oceanic Engineering. |'
  id: totrans-869
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/0459bb4441d0e984facf544269493f84.png) | 李崇义是新加坡南洋理工大学计算机科学与工程学院的研究助理教授。他于2018年获得中国天津大学的博士学位。2016年至2017年，他是澳大利亚国立大学的联合培养博士生。在加入南洋理工大学之前，他曾在2018至2021年期间担任香港城市大学和南洋理工大学的博士后研究员。他目前的研究重点是图像处理、计算机视觉和深度学习，特别是在图像修复和增强领域。他担任《信号、图像与视频处理杂志》的副编辑，以及IEEE《海洋工程杂志》的首席客座编辑。'
- en: '| ![[Uncaptioned image]](img/48273b9fa9f26bbc51966676caeea411.png) | Chunle
    Guo received his PhD degree from Tianjin University in China under the supervision
    of Prof. Jichang Guo. He conducted the Ph.D. research as a Visiting Student with
    the School of Electronic Engineering and Computer Science, Queen Mary University
    of London (QMUL), UK. He continued his research as a Research Associate with the
    Department of Computer Science, City University of Hong Kong (CityU), from 2018
    to 2019\. Now he is a postdoc research fellow working with Prof. Ming-Ming Cheng
    at Nankai University. His research interests lie in image processing, computer
    vision, and deep learning. |'
  id: totrans-870
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/48273b9fa9f26bbc51966676caeea411.png) | 郭春乐在中国天津大学获得博士学位，在郭纪昌教授的指导下进行研究。他在伦敦大学皇后玛丽学院（QMUL）电子工程与计算机科学学院作为访问学生进行了博士研究。从2018年到2019年，他在香港城市大学计算机科学系担任研究副助理。他目前是南开大学程明明教授团队的博士后研究员。他的研究兴趣包括图像处理、计算机视觉和深度学习。
    |'
- en: '| ![[Uncaptioned image]](img/800006ebed0e2ebe55ab7451995a1fa9.png) | Linhao
    Han is currently a master student at the College of Computer Science, Nankai University,
    under the supervision of Prof. Ming-Ming Cheng. His research interests include
    deep learning and computer vision. |'
  id: totrans-871
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/800006ebed0e2ebe55ab7451995a1fa9.png) | 韩霖浩目前是南开大学计算机学院的硕士研究生，在程明明教授的指导下进行研究。他的研究兴趣包括深度学习和计算机视觉。
    |'
- en: '| ![[Uncaptioned image]](img/b7fe11913bc18b8b30e91178edb61e1e.png) | Jun Jiang
    received the PhD degree in Color Science from Rochester Institute of Technology
    in 2013\. He is a Senior Researcher in SenseBrain focusing on algorithm development
    to improve image quality on smartphone cameras. His research interest includes
    computational photography, low-level computer vision, and deep learning. |'
  id: totrans-872
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/b7fe11913bc18b8b30e91178edb61e1e.png) | 冈江俊获得了2013年罗切斯特理工学院的色彩科学博士学位。他是SenseBrain的高级研究员，专注于算法开发，以改善智能手机相机的图像质量。他的研究兴趣包括计算摄影、低级计算机视觉和深度学习。
    |'
- en: '| ![[Uncaptioned image]](img/81ebc4192603be9885c0cf76b7de7edc.png) | Ming-Ming
    Cheng (Senior Member, IEEE) received the Ph.D. degree from Tsinghua University
    in 2012\. Then he did two years research fellowship with Prof. Philip Torr at
    Oxford. He is currently a Professor at Nankai University and leading the Media
    Computing Laboratory. His research interests include computer graphics, computer
    vision, and image processing. He received research awards, including the ACM China
    Rising Star Award, the IBM Global SUR Award, and the CCF-Intel Young Faculty Researcher
    Program. He is on the Editorial Board Member of IEEE Transactions on Image Processing
    (TIP). |'
  id: totrans-873
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/81ebc4192603be9885c0cf76b7de7edc.png) | 程明明（IEEE高级会员）于2012年获得清华大学博士学位。随后，他在牛津大学与Philip
    Torr教授进行了为期两年的研究员工作。他目前是南开大学的教授，并领导媒体计算实验室。他的研究兴趣包括计算机图形学、计算机视觉和图像处理。他获得了包括ACM中国新星奖、IBM全球SUR奖和CCF-Intel青年教师研究员计划在内的多个研究奖项。他还是IEEE《图像处理学报》（TIP）的编辑委员会成员。
    |'
- en: '| ![[Uncaptioned image]](img/4c0832ab6f7b3282a476ae918cc382a9.png) | Jinwei
    Gu (Senior Member, IEEE) is the R&D Executive Director of SenseTime USA. His current
    research focuses on low-level computer vision, computational photography, smart
    visual sensing and perception, and robotics. He obtained his Ph.D. degree in 2010
    from Columbia University, and his B.S and M.S. from Tsinghua University, in 2002
    and 2005 respectively. Before joining SenseTime, he was a senior research scientist
    in NVIDIA Research from 2015 to 2018\. Prior to that, he was an assistant professor
    in Rochester Institute of Technology from 2010 to 2013, and a senior researcher
    in the media lab of Futurewei Technologies from 2013 to 2015\. He is an associate
    editor for IEEE Transactions on Computational Imaging and an IEEE senior member
    since 2018. |'
  id: totrans-874
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/4c0832ab6f7b3282a476ae918cc382a9.png) | 顾金伟（IEEE高级会员）是SenseTime
    USA的研发执行董事。他目前的研究集中在低级计算机视觉、计算摄影、智能视觉感知和机器人技术。他于2010年在哥伦比亚大学获得博士学位，2002年和2005年分别获得清华大学的本科和硕士学位。在加入SenseTime之前，他曾在2015至2018年担任NVIDIA研究院高级研究科学家。在此之前，他在2010至2013年期间是罗切斯特理工学院的助理教授，并在2013至2015年期间担任未来伟科技媒体实验室的高级研究员。他是IEEE《计算成像学报》的副编辑，并自2018年起成为IEEE高级会员。'
- en: '| ![[Uncaptioned image]](img/c31cca88521d432e787ceed6c3d63b80.png) | Chen Change
    Loy (Senior Member, IEEE) is an Associate Professor with the School of Computer
    Science and Engineering, Nanyang Technological University, Singapore. He is also
    an Adjunct Associate Professor at The Chinese University of Hong Kong. He received
    his Ph.D. (2010) in Computer Science from the Queen Mary University of London.
    Prior to joining NTU, he served as a Research Assistant Professor at the MMLab
    of The Chinese University of Hong Kong, from 2013 to 2018\. He was a postdoctoral
    researcher at Queen Mary University of London and Vision Semantics Limited, from
    2010 to 2013\. He serves as an Associate Editor of the IEEE Transactions on Pattern
    Analysis and Machine Intelligence and International Journal of Computer Vision.
    He also serves/served as an Area Chair of major conferences such as ICCV, CVPR,
    ECCV and AAAI. His research interests include image/video restoration and enhancement,
    generative tasks, and representation learning. |'
  id: totrans-875
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/c31cca88521d432e787ceed6c3d63b80.png) | 陈常磊（IEEE高级会员）是新加坡南洋理工大学计算机科学与工程学院的副教授。他还是香港中文大学的兼职副教授。他于2010年在伦敦玛丽女王大学获得计算机科学博士学位。在加入南洋理工大学之前，他曾在2013至2018年担任香港中文大学MMLab的研究助理教授。他在2010至2013年期间曾是伦敦玛丽女王大学和Vision
    Semantics Limited的博士后研究员。他担任IEEE《模式分析与机器智能学报》和《计算机视觉国际杂志》的副编辑。他还曾是ICCV、CVPR、ECCV和AAAI等主要会议的领域主席。他的研究兴趣包括图像/视频恢复与增强、生成任务和表征学习。
    |'
