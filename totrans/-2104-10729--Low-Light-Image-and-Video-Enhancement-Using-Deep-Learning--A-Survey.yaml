- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:55:51'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2104.10729] Low-Light Image and Video Enhancement Using Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2104.10729](https://ar5iv.labs.arxiv.org/html/2104.10729)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \WarningFilter
  prefs: []
  type: TYPE_NORMAL
- en: latexFont shape \WarningFilterlatexfontFont shape
  prefs: []
  type: TYPE_NORMAL
- en: \justify
  prefs: []
  type: TYPE_NORMAL
- en: Low-Light Image and Video Enhancement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using Deep Learning: A Survey'
  prefs: []
  type: TYPE_NORMAL
- en: Chongyi Li, Chunle Guo, Linghao Han, Jun Jiang, Ming-Ming Cheng, ,
  prefs: []
  type: TYPE_NORMAL
- en: 'Jinwei Gu, , and Chen Change Loy C. Li and C. C. Loy are with the S-Lab, Nanyang
    Technological University (NTU), Singapore (e-mail: chongyi.li@ntu.edu.sg and ccloy@ntu.edu.sg).C.
    Guo, L. Han, and M. M. Cheng are with the College of Computer Science, Nankai
    University, Tianjin, China (e-mail: guochunle@nankai.edu.cn, lhhan@mail.nankai.edu.cn,
    and cmm@nankai.edu.cn).J. Jiang and J. Gu are with the SenseTime (e-mail: jiangjun@sensebrain.site
    and gujinwei@sensebrain.site).C. Li and C. Guo contribute equally.C. C. Loy is
    the corresponding author.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Low-light image enhancement (LLIE) aims at improving the perception or interpretability
    of an image captured in an environment with poor illumination. Recent advances
    in this area are dominated by deep learning-based solutions, where many learning
    strategies, network structures, loss functions, training data, etc. have been
    employed. In this paper, we provide a comprehensive survey to cover various aspects
    ranging from algorithm taxonomy to unsolved open issues. To examine the generalization
    of existing methods, we propose a low-light image and video dataset, in which
    the images and videos are taken by different mobile phones’ cameras under diverse
    illumination conditions. Besides, for the first time, we provide a unified online
    platform that covers many popular LLIE methods, of which the results can be produced
    through a user-friendly web interface. In addition to qualitative and quantitative
    evaluation of existing methods on publicly available and our proposed datasets,
    we also validate their performance in face detection in the dark. This survey
    together with the proposed dataset and online platform could serve as a reference
    source for future study and promote the development of this research field. The
    proposed platform and dataset as well as the collected methods, datasets, and
    evaluation metrics are publicly available and will be regularly updated. Project
    page: [https://www.mmlab-ntu.com/project/lliv_survey/index.html](https://www.mmlab-ntu.com/project/lliv_survey/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: image and video restoration, low-light image dataset, low-light image enhancement
    platform, computational photography.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Images are often taken under sub-optimal lighting conditions, under the influence
    of backlit, uneven light, and dim light, due to inevitable environmental and/or
    technical constraints such as insufficient illumination and limited exposure time.
    Such images suffer from the compromised aesthetic quality and unsatisfactory transmission
    of information for high-level tasks such as object tracking, recognition, and
    detection. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Low-Light Image and
    Video Enhancement Using Deep Learning: A Survey") shows some examples of the degradations
    induced by sub-optimal lighting conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: Low-light enhancement enjoys a wide range of applications in different areas,
    including visual surveillance, autonomous driving, and computational photography.
    In particular, smartphone photography has become ubiquitous and prominent. Limited
    by the size of the camera aperture, the requirement of real-time processing, and
    the constraint of memory, taking photographs with a smartphone’s camera in a dim
    environment is especially challenging. There is an exciting research arena of
    enhancing low-light images and videos in such applications.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/8e661f73253a3f5f807b5c76b29ed126.png) | ![Refer to
    caption](img/c3c8e40e1f3f08aec903f259bfa70b38.png) | ![Refer to caption](img/fc623bba47267cf053b6b59ce3139e55.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| (a) back lit | (b) uneven light | (c) dim light |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/ef74b3cf4c3d18374b433d8109493445.png) | ![Refer to
    caption](img/3b3c2fef80abf17dc846a1d1f296f800.png) | ![Refer to caption](img/8fb604861d144169a71e6c2821222b64.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| (d) extremely low | (e) colored light | (f) boosted noise |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 1: Examples of images taken under sub-optimal lighting conditions. These
    images suffer from the buried scene content, reduced contrast, boosted noise,
    and inaccurate color.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e121c55be713e292ee171491930af5e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A concise milestone of deep learning-based low-light image and video
    enhancement methods. Supervised learning-based methods: LLNet [[1](#bib.bib1)],
    Chen et al. [[2](#bib.bib2)], MBLLEN [[3](#bib.bib3)], Retinex-Net [[4](#bib.bib4)],
    LightenNet [[5](#bib.bib5)], SCIE [[6](#bib.bib6)], DeepUPE [[7](#bib.bib7)],
    Chen et al. [[8](#bib.bib8)], Jiang and Zheng [[9](#bib.bib9)], Wang et al. [[10](#bib.bib10)],
    KinD [[11](#bib.bib11)], Ren et al. [[12](#bib.bib12)], Xu et al. [[13](#bib.bib13)],
    Fan et al. [[14](#bib.bib14)], Lv et al. [[15](#bib.bib15)], EEMEFN [[16](#bib.bib16)],
    SIDGAN. [[17](#bib.bib17)], LPNet [[18](#bib.bib18)], DLN [[19](#bib.bib19)],
    TBEFN [[20](#bib.bib20)], DSLR [[21](#bib.bib21)], Zhang et al. [[22](#bib.bib22)],
    PRIEN [[23](#bib.bib23)], and Retinex-Net [[24](#bib.bib24)]. Reinforcement learning-based
    method: DeepExposure [[25](#bib.bib25)]. Unsupervised learning-based method: EnlightenGAN
    [[26](#bib.bib26)]. Zero-shot learning-based methods: ExCNet [[27](#bib.bib27)],
    Zero-DCE [[28](#bib.bib28)], RRDNet [[29](#bib.bib29)], Zero-DCE++ [[30](#bib.bib30)],
    RetinexDIP [[31](#bib.bib31)], and RUAS [[32](#bib.bib32)]. Semi-supervised learning-based
    method: DRBN [[33](#bib.bib33)] and DRBN [[34](#bib.bib34)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional methods for low-light enhancement include Histogram Equalization-based
    methods [[35](#bib.bib35), [36](#bib.bib36)] and Retinex model-based methods [[37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)]. The latter received relatively more attention.
    A typical Retinex model-based approach decomposes a low-light image into a reflection
    component and an illumination component by priors or regularizations. The estimated
    reflection component is treated as the enhanced result. Such methods have some
    limitations: 1) the ideal assumption that treats the reflection component as the
    enhanced result does not always hold, especially given various illumination properties,
    which could lead to unrealistic enhancement such as loss of details and distorted
    colors, 2) the noise is usually ignored in the Retinex model, thus it is remained
    or amplified in the enhanced results, 3) finding an effective prior or regularization
    is challenging. Inaccurate prior or regularization may result in artifacts and
    color deviations in the enhanced results, and 4) the runtime is relatively long
    because of their complicated optimization process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent years have witnessed the compelling success of deep learning-based LLIE
    since the first seminal work [[1](#bib.bib1)]. Deep learning-based solutions enjoy
    better accuracy, robustness, and speed over conventional methods, thus attracting
    increasing attention. A concise milestone of deep learning-based LLIE methods
    is shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Low-Light Image and
    Video Enhancement Using Deep Learning: A Survey"). As shown, since 2017, the number
    of deep learning-based solutions has grown year by year. Learning strategies used
    in these solutions cover Supervised Learning (SL), Reinforcement Learning (RL),
    Unsupervised Learning (UL), Zero-Shot Learning (ZSL), and Semi-Supervised Learning
    (SSL). Note that we only report some representative methods in Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Low-Light Image and Video Enhancement Using Deep
    Learning: A Survey"). In fact, there are more than 100 papers on deep learning-based
    methods from 2017 to 2021\. Moreover, although some general photo enhancement
    methods [[45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)]
    can improve the brightness of images to some extent, we omit them in this survey
    as they are not designed to handle diverse low-light conditions. We concentrate
    on deep learning-based solutions that are specially developed for low-light image
    and video enhancement.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite deep learning has dominated the research of LLIE, an in-depth and comprehensive
    survey on deep learning-based solutions is lacking. There are two reviews of LLIE
    [[54](#bib.bib54), [55](#bib.bib55)]. Wang et al. [[54](#bib.bib54)] mainly reviews
    conventional LLIE methods while our work systematically and comprehensively reviews
    recent advances of deep learning-based LLIE. In comparison to Liu et al. [[55](#bib.bib55)]
    that reviews existing LLIE algorithms, measures the machine vision performance
    of different methods, provides a low-light image dataset serving both low-level
    and high-level vision enhancement, and develops an enhanced face detector, our
    survey reviews the low-light image and video enhancement from different aspects
    and has the following unique characteristics. 1) Our work mainly focuses on recent
    advances of deep learning-based low-light image and video enhancement, where we
    provide in-depth analysis and discussion in various aspects, covering learning
    strategies, network structures, loss functions, training datasets, test datasets,
    evaluation metrics, model sizes, inference speed, enhancement performance, etc.
    Thus, this survey centers on deep learning and its applications in low-light image
    and video enhancement. 2) We propose a dataset that contains images and videos
    captured by different mobile phones’ cameras under diverse illumination conditions
    to evaluate the generalization of existing methods. This new and challenging dataset
    is a supplement of existing low-light image and video enhancement datasets as
    such a dataset is lacking in this research area. Besides, we are the first, to
    the best of our knowledge, to compare the performance of deep learning-based low-light
    image enhancement methods on this kind of data. 3) We provide an online platform
    that covers many popular deep learning-based low-light image enhancement methods,
    where the results can be produced by a user-friendly web interface. With our platform,
    one without any GPUs can assess the results of different methods for any input
    images online, which speeds up the development of this research field and helps
    to create new research. We hope that our survey could provide novel insights and
    inspiration to facilitate the understanding of deep learning-based LLIE, foster
    research on the raised open issues, and speed up the development of this research
    field.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep Learning-Based LLIE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Problem Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We first give a common formulation of the deep learning-based LLIE problem.
    For a low-light image $I\in\mathbb{R}^{W\times H\times 3}$ of width $W$ and height
    $H$, the process can be modeled as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\widehat{R}=\mathcal{F}(I;\theta),$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\widehat{R}\in\mathbb{R}^{W\times H\times 3}$ is the enhanced result
    and $\mathcal{F}$ represents the network with trainable parameters $\theta$. The
    purpose of deep learning is to find optimal network parameters $\widehat{\theta}$
    that minimizes the error:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\widehat{\theta}=\operatorname*{argmin}_{\theta}\mathcal{L}(\widehat{R},R),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $R\in\mathbb{R}^{W\times H\times 3}$ is the ground truth, and the loss
    function $\mathcal{L}(\widehat{R},R)$ drives the optimization of network. Various
    loss functions such as supervised loss and unsupervised loss can be used. More
    details will be presented in Section [3](#S3 "3 Technical Review and Discussion
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Learning Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to different learning strategies, we categorize existing LLIE methods
    into supervised learning, reinforcement learning, unsupervised learning, zero-shot
    learning, and semi-supervised learning. A statistic analysis from different perspectives
    is presented in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep
    Learning-Based LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"). In what follows, we review some representative methods of each strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Learning. For supervised learning-based LLIE methods, they are further
    divided into end-to-end, deep Retinex-based, and realistic data-driven methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first deep learning-based LLIE method LLNet [[1](#bib.bib1)] employs a
    variant of stacked-sparse denoising autoencoder [[56](#bib.bib56)] to brighten
    and denoise low-light images simultaneously. This pioneering work inspires the
    usage of end-to-end networks in LLIE. Lv et al. [[3](#bib.bib3)] propose an end-to-end
    multi-branch enhancement network (MBLLEN). The MBLLEN improves the performance
    of LLIE via extracting effective feature representations by a feature extraction
    module, an enhancement module, and a fusion module. The same authors [[15](#bib.bib15)]
    propose other three subnetworks including an Illumination-Net, a Fusion-Net, and
    a Restoration-Net to further improve the performance. Ren et al. [[12](#bib.bib12)]
    design a more complex end-to-end network that comprises an encoder-decoder network
    for image content enhancement and a recurrent neural network for image edge enhancement.
    Similar to Ren et al. [[12](#bib.bib12)], Zhu et al. [[16](#bib.bib16)] propose
    a method called EEMEFN. The EEMEFN consists of two stages: multi-exposure fusion
    and edge enhancement. A multi-exposure fusion network, TBEFN [[20](#bib.bib20)],
    is proposed for LLIE. The TBEFN estimates a transfer function in two branches,
    of which two enhancement results can be obtained. At last, a simple average scheme
    is employed to fuse these two images and further refine the result via a refinement
    unit. In addition, pyramid network (LPNet) [[18](#bib.bib18)], residual network
    [[19](#bib.bib19)], and Laplacian pyramid [[21](#bib.bib21)] (DSLR) are introduced
    into LLIE. These methods learn to effectively and efficiently integrate feature
    representations via commonly used end-to-end network structures for LLIE. Based
    on the observation that noise exhibits different levels of contrast in different
    frequency layers, Xu et al. [[57](#bib.bib57)] proposed a frequency-based decomposition-and-enhancement
    network. This network recovers image contents with noise suppression in the low-frequency
    layer while inferring the details in the high-frequency layer. Recently, a progressive-recursive
    low-light image enhancement network [[23](#bib.bib23)] is proposed, which uses
    a recursive unit to gradually enhance the input image. To solve temporal instability
    when handling low-light videos, Zhang et al. [[22](#bib.bib22)] propose to learn
    and infer motion field from a single image then enforce temporal consistency.'
  prefs: []
  type: TYPE_NORMAL
- en: In comparison to directly learning an enhanced result in an end-to-end network,
    deep Retinex-based methods enjoy better enhancement performance in most cases
    owing to the physically explicable Retinex theory [[58](#bib.bib58), [59](#bib.bib59)].
    Deep Retinex-based methods usually separately enhance the illuminance component
    and the reflectance components via specialized subnetworks. A Retinex-Net [[4](#bib.bib4)]
    is proposed, which includes a Decom-Net that splits the input image into light-independent
    reflectance and structure-aware smooth illumination and an Enhance-Net that adjusts
    the illumination map for low-light enhancement. Recently, the Retinex-Net [[4](#bib.bib4)]
    is extended by adding new constraints and advanced network designs for better
    enhancement performance [[24](#bib.bib24)]. To reduce the computational burden,
    Li et al. [[5](#bib.bib5)] propose a lightweight LightenNet for weakly illuminated
    image enhancement, which only consists of four layers. The LightenNet takes a
    weakly illuminated image as the input and then estimates its illumination map.
    Based on the Retinex theory [[58](#bib.bib58), [59](#bib.bib59)], the enhanced
    image is obtained by dividing the input image by the illumination map. To accurately
    estimate the illumination map, Wang et al. [[60](#bib.bib60)] extract the global
    and local features to learn an image-to-illumination mapping by their proposed
    DeepUPE network. Zhang et al. [[11](#bib.bib11)] separately develop three subnetworks
    for layer decomposition, reflectance restoration, and illumination adjustment,
    called KinD. Furthermore, the authors alleviate the visual defects left in the
    results of KinD [[11](#bib.bib11)] by a multi-scale illumination attention module.
    The improved KinD is called KinD++ [[61](#bib.bib61)]. To solve the issue that
    the noise is omitted in the deep Retinex-based methods, Wang et al. [[10](#bib.bib10)]
    propose a progressive Retinex network, where an IM-Net estimates the illumination
    and a NM-Net estimates the noise level. These two subnetworks work in a progressive
    mechanism until obtaining stable results. Fan et al. [[14](#bib.bib14)] integrate
    semantic segmentation and Retinex model for further improving the enhancement
    performance in real cases. The core idea is to use semantic prior to guide the
    enhancement of both the illumination component and the reflectance component.
  prefs: []
  type: TYPE_NORMAL
- en: Although some methods can achieve decent performance, they show poor generalization
    capability in real low-light cases due to the usage of synthetic training data.
    To solve this issue, some works attempt to generate more realistic training data
    or capture real data. Cai et al. [[6](#bib.bib6)] build a multi-exposure image
    dataset, where the low-contrast images of different exposure levels have their
    corresponding high-quality reference images. Each high-quality reference image
    is obtained by subjectively selecting the best output from 13 results enhanced
    by different methods. Moreover, a frequency decomposition network is trained on
    the built dataset and separately enhances the high-frequency layer and the low-frequency
    layer via a two-stage structure. Chen et al. [[2](#bib.bib2)] collect a real low-light
    image dataset (SID) and train the U-Net [[62](#bib.bib62)] to learn a mapping
    from low-light raw data to the corresponding long-exposure high-quality reference
    image. Further, Chen et al. [[8](#bib.bib8)] extend the SID dataset to low-light
    videos (DRV). The DRV contains static videos with the corresponding long-exposure
    ground truths. To ensure the generalization capability of processing the videos
    of dynamic scenes, a siamese network is proposed. To enhance the moving objects
    in the dark, Jiang and Zheng [[9](#bib.bib9)] design a co-axis optical system
    to capture temporally synchronized and spatially aligned low-light and well-lighted
    video pairs (SMOID). Unlike the DRV video dataset [[8](#bib.bib8)], the SMOID
    video dataset contains dynamic scenes. To learn the mapping from raw low-light
    video to well-lighted video, a 3D U-Net-based network is proposed. Considering
    the limitations of previous low-light video datasets such as DRV dataset [[8](#bib.bib8)]
    only containing statistic videos and SMOID dataset [[9](#bib.bib9)] only having
    179 video pairs, Triantafyllidou et al. [[17](#bib.bib17)] propose a low-light
    video synthesis pipeline, dubbed SIDGAN. The SIDGAN can produce dynamic video
    data (raw-to-RGB) by a semi-supervised dual CycleGAN with intermediate domain
    mapping. To train this pipeline, the real-world videos are collected from Vimeo-90K
    dataset [[63](#bib.bib63)]. The low-light raw video data and the corresponding
    long-exposure images are sampled from DRV dataset [[8](#bib.bib8)]. With the synthesized
    training data, this work adopts the same U-Net network as Chen et al. [[2](#bib.bib2)]
    for low-light video enhancement.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning. Without paired training data, Yu et al. [[25](#bib.bib25)]
    learn to expose photos with reinforcement adversarial learning, named DeepExposure.
    Specifically, an input image is first segmented into sub-images according to exposures.
    For each sub-image, local exposure is learned by the policy network sequentially
    based on reinforcement learning. The reward evaluation function is approximated
    by adversarial learning. At last, each local exposure is employed to retouch the
    input, thus obtaining multiple retouched images under different exposures. The
    final result is achieved by fusing these images.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning. Training a deep model on paired data may result in overfitting
    and limited generalization capability. To solve this issue, an unsupervised learning
    method named EnligthenGAN [[26](#bib.bib26)] is proposed. The EnlightenGAN adopts
    an attention-guided U-Net [[62](#bib.bib62)] as the generator and uses the global-local
    discriminators to ensure the enhanced results look like realistic normal-light
    images. In addition to global and local adversarial losses, the global and local
    self feature preserving losses are proposed to preserve the image content before
    and after the enhancement. This is a key point for the stable training of such
    a one-path Generative Adversarial Network (GAN) structure.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Learning. The supervised learning, reinforcement learning, and unsupervised
    learning methods either have limited generalization capability or suffer from
    unstable training. To remedy these issues, zero-shot learning is proposed to learn
    the enhancement solely from the testing images. Note that the concept of zero-shot
    learning in the low-level vision tasks is used to emphasize that the method does
    not require paired or unpaired training data, which is different from its definition
    in high-level visual tasks. Zhang et al. [[27](#bib.bib27)] propose a zero-shot
    learning method, called ExCNet, for back-lit image restoration. A network is first
    used to estimate the S-curve that best fits the input image. Once the S-curve
    is estimated, the input image is separated into a base layer and a detail layer
    using the guided filter [[64](#bib.bib64)]. Then the base layer is adjusted by
    the estimated S-curve. Finally, the Weber contrast [[65](#bib.bib65)] is used
    to fuse the detailed layer and the adjusted base layer. To train the ExCNet, the
    authors formulate the loss function as a block-based energy minimization problem.
    Zhu et al. [[29](#bib.bib29)] propose a three-branch CNN, called RRDNet, for underexposed
    images restoration. The RRDNet decomposes an input image into illumination, reflectance,
    and noise via iteratively minimizing specially designed loss functions. To drive
    the zero-shot learning, a combination of Retinex reconstruction loss, texture
    enhancement loss, and illumination-guided noise estimation loss is proposed. Zhao
    et al. [[31](#bib.bib31)] perform Retinex decomposition via neural networks and
    then enhance the low-light image based on the Retinex model, called RetinexDIP.
    Inspired by Deep Image Prior (DIP) [[66](#bib.bib66)], RetinexDIP generates the
    reflectance component and illumination component of an input image by randomly
    sampled white noise, in which the component characteristics-related losses such
    as illumination smoothness are used for training. Liu et al. [[32](#bib.bib32)]
    propose a Retinex-inspired unrolling method for LLIE, in which the cooperative
    architecture search is used to discover lightweight prior architectures of basic
    blocks and non-reference losses are used to train the network. Different from
    the image reconstruction-based methods [[1](#bib.bib1), [3](#bib.bib3), [12](#bib.bib12),
    [21](#bib.bib21), [4](#bib.bib4), [11](#bib.bib11), [61](#bib.bib61)], a deep
    curve estimation network, Zero-DCE [[28](#bib.bib28)], is proposed. Zero-DCE formulates
    the light enhancement as a task of image-specific curve estimation, which takes
    a low-light image as input and produces high-order curves as its output. These
    curves are used for pixel-wise adjustment on the dynamic range of the input to
    obtain an enhanced image. Further, an accelerated and lightweight version is proposed,
    called Zero-DCE++ [[30](#bib.bib30)]. Such curve-based methods do not require
    any paired or unpaired data during training. They achieve zero-reference learning
    via a set of non-reference loss functions. Besides, unlike the image reconstruction-based
    methods that need high computational resources, the image-to-curve mapping only
    requires lightweight networks, thus achieving a fast inference speed.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/3c05d749696222c732238f19c01a4bb9.png) | ![Refer to
    caption](img/75f9b5236a6d545fb5dac913e04a5750.png) | ![Refer to caption](img/72a6454902d1ffce2c7d31a0902cd87b.png)
    | ![Refer to caption](img/058e15ee24ec5f9900f0b11d6dc6655c.png) |'
  prefs: []
  type: TYPE_TB
- en: '| (a) learning strategy | (b) network structure | (c) Retinex model | (d) data
    format |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/ba9cd43b95793faf4b0191ad2cf826a0.png) | ![Refer to
    caption](img/7c51dae45c60efd86f25e0bebcc490a4.png) | ![Refer to caption](img/b1c7bcfb2a59658b7d471b434de64d14.png)
    | ![Refer to caption](img/3285a97f3d602476db3e4d1ff2e43ed8.png) |'
  prefs: []
  type: TYPE_TB
- en: '| (e) loss function | (f) training dataset | (g) testing dataset | (h) evaluation
    metric |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 3: A statictic analysis of deep learning-based LLIE methods, including
    learning strategy, network characteristic, Retinex model, data format, loss function,
    training dataset, testing dataset, and evaluation metric. Best viewed by zooming
    in.'
  prefs: []
  type: TYPE_NORMAL
- en: Semi-Supervised Learning. To combine the strengths of supervised learning and
    unsupervised learning, semi-supervised learning has been proposed in recent years.
    Yang et al. [[33](#bib.bib33)] propose a semi-supervised deep recursive band network
    (DRBN). The DRBN first recovers a linear band representation of an enhanced image
    under supervised learning, and then obtains an improved one by recomposing the
    given bands via a learnable linear transformation based on unsupervised adversarial
    learning. The DRBN is extended by introducing Long Short Term Memory (LSTM) networks
    and an image quality assessment network pre-trained on an aesthetic visual analysis
    dataset, which achieves better enhancement performance [[34](#bib.bib34)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Observing Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based
    LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(a),
    we can find that supervised learning is the mainstream among deep learning-based
    LLIE methods, of which the percentage reaches 73%. This is because supervised
    learning is relatively easy when paired training data such as LOL [[4](#bib.bib4)],
    SID [[2](#bib.bib2)] and diverse low-/normal-light image synthesis approaches
    are used. However, supervised learning-based methods suffer from some challenges:
    1) collecting a large-scale paired dataset that covers diverse real-world low-light
    conditions is difficult, 2) synthetic low-light images do not accurately represent
    real-world illuminance conditions such as spatially varying lighting and different
    levels of noise, and 3) training a deep model on paired data may result in limited
    generalization to real-world images of diverse illumination properties.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, some methods adopt unsupervised learning, reinforcement learning,
    semi-supervised learning, and zero-shot learning to bypass the challenges in supervised
    learning. Although these methods achieve competing performance, they still suffer
    from some limitations: 1) for unsupervised learning/semi-supervised learning methods,
    how to implement stable training, avoid color deviations, and build the relations
    of cross-domain information challenges current methods, 2) for reinforcement learning
    methods, designing an effective reward mechanism and implementing efficient and
    stable training are intricate, and 3) for zero-shot learning methods, the design
    of non-reference losses is non-trivial when the color preservation, artifact removal,
    and gradient back-propagation should be taken into account.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Summary of essential characteristics of representative deep learning-based
    methods. “Retinex” indicates whether the models are Retinex-based or not. “simulated”
    means the testing data are simulated by the same approach as the synthetic training
    data. “self-selected” stands for the real-world images selected by the authors.
    “#P”represents the number of trainable parameters. “-” means this item is not
    available or not indicated in the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Learning | Network Structure | Loss Function | Training Data
    | Testing Data | Evaluation Metric | Format | Platform | Retinex |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | LLNet [[1](#bib.bib1)] | SL | SSDA | SRR loss |'
  prefs: []
  type: TYPE_TB
- en: '&#124; simulated by &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gamma Correction & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gaussian Noise &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self-selected &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | Theano |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | LightenNet [[5](#bib.bib5)] | SL | four layers | $L_{2}$ loss |'
  prefs: []
  type: TYPE_TB
- en: '&#124; simulated by &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; random illumination values &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self-selected &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR MAE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; User Study &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Caffe &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MATLAB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Retinex-Net [[4](#bib.bib4)] | SL | multi-scale network |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $L_{1}$ loss smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; invariable reflectance loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated by &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adjusting histogram &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| self-selected | - | RGB | TensorFlow | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | MBLLEN [[3](#bib.bib3)] | SL | multi-branch fusion |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SSIM loss region loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated by &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gamma Correction & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Poisson Noise &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self-selected &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AB VIF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOE TOMI &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TensorFlow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | SCIE [[6](#bib.bib6)] | SL | frequency decomposition |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $L_{2}$ loss $L_{1}$ loss SSIM loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SCIE | SCIE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR FSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Runtime FLOPs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Caffe &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MATLAB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Chen et al. [[2](#bib.bib2)] | SL | U-Net | $L_{1}$ loss | SID | SID |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| raw | TensorFlow |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Deepexposure [[25](#bib.bib25)] | RL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; policy network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; deterministic policy gradient &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| MIT-Adobe FiveK | MIT-Adobe FiveK |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| raw | TensorFlow |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Chen et al. [[8](#bib.bib8)] | SL | siamese network |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $L_{1}$ loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self-consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| DRV | DRV |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MAE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| raw | TensorFlow |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Jiang and Zheng [[9](#bib.bib9)] | SL | 3D U-Net | $L_{1}$ loss | SMOID
    | SMOID |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR SSIM MSE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| raw | TensorFlow |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | DeepUPE [[60](#bib.bib60)] | SL | illumination map |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $L_{1}$ loss color loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| retouched image pairs | MIT-Adobe FiveK |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; User Study &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | TensorFlow | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | KinD [[11](#bib.bib11)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three subnetworks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reflectance similarity loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; illumination smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mutual consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $L_{1}$ loss $L_{2}$ loss SSIM loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; texture similarity loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; illumination adjustment loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| LOL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; LOL LIME &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NPE MEF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOE NIQE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | TensorFlow | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Wang et al. [[10](#bib.bib10)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two subnetworks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pointwise Conv &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $L_{1}$ loss |'
  prefs: []
  type: TYPE_TB
- en: '&#124; simulated by &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; camera imaging model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; IP100 FNF38 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MPI LOL NPE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NIQE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | Caffe | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ren et al. [[12](#bib.bib12)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RNN dilated Conv &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $L_{2}$ loss perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MIT-Adobe FiveK &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with Gamma correction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; & Gaussion noise &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self-selected DPED &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Runtime &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | Caffe |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | EnlightenGAN [[26](#bib.bib26)] | UL | U-Net like network |'
  prefs: []
  type: TYPE_TB
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self feature preserving loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| unpaired real images |'
  prefs: []
  type: TYPE_TB
- en: '&#124; NPE LIME &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MEF DICM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; VV BBD-100K &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ExDARK &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; User Study NIQE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | ExCNet. [[27](#bib.bib27)] | ZSL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; fully connected layers &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| energy minimization loss | real images | $IE_{ps}D$ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; User Study &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CDIQA LOD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Zero-DCE [[28](#bib.bib28)] | ZSL | U-Net like network |'
  prefs: []
  type: TYPE_TB
- en: '&#124; spatial consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; exposure control loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; color constancy loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; illumination smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SICE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SICE NPE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LIME MEF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DICM VV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DARK FACE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; User Study PI &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PNSR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MAE Runtime &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Face detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | DRBN [[33](#bib.bib33)] | SSL | recursive network |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SSIM loss perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; images selected by MOS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| LOL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM-GC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Lv et al. [[15](#bib.bib15)] | SL | U-Net like network |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Huber loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; illumination smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated by a &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; retouching module &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL SICE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DeepUPE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; User Study PSNR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM VIF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOE NIQE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; #P Runtime &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Face detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TensorFlow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fan et al. [[14](#bib.bib14)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; four subnetworks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; feature modulation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mutual smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; illumination smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cross entropy loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; gradient loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ratio learning loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated by &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; illumination adjustment, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; slight color distortion, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and noise simulation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self-selected &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NIQE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Xu et al. [[57](#bib.bib57)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; frequency decomposition &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $L_{2}$ loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SID in RGB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SID in RGB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self-selected &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | EEMEFN [[16](#bib.bib16)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; edge detection network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $L_{1}$ loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; weighted cross-entropy loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SID | SID |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| raw |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TensorFlow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PaddlePaddle &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | DLN [[19](#bib.bib19)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; residual learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; interactive factor &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; back projection network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; total variation loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated by &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; illumination adjustment, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; slight color distortion, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and noise simulation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; User Study PSNR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM NIQE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | LPNet [[18](#bib.bib18)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; pyramid network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $L_{1}$ loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; luminance loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL SID in RGB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MIT-Adobe FiveK &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL SID in RGB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MIT-Adobe FiveK &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MEF NPE DICM VV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NIQE #P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FLOPs Runtime &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | SIDGAN [[17](#bib.bib17)] | SL | U-Net | CycleGAN loss | SIDGAN | SIDGAN
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TPSNR TSSIM ATWE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| raw |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TensorFlow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | RRDNet [[29](#bib.bib29)] | ZSL | three subnetworks |'
  prefs: []
  type: TYPE_TB
- en: '&#124; retinex reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; texture enhancement loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; noise estimation loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| - |'
  prefs: []
  type: TYPE_TB
- en: '&#124; NPE LIME &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MEF DICM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NIQE CPCQI &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | TBEFN [[20](#bib.bib20)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three stages &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCIE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SCIE LOL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DICM MEF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NPE VV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NIQE Runtime &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; #P FLOPs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | TensorFlow | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | DSLR [[21](#bib.bib21)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Laplacian pyramid &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $L_{2}$ loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Laplacian loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; color loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| MIT-Adobe FiveK |'
  prefs: []
  type: TYPE_TB
- en: '&#124; MIT-Adobe FiveK &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self-selected &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NIQMC NIQE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BTMQI CaHDC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | RUAS [[32](#bib.bib32)] | ZSL | neural architecture search |'
  prefs: []
  type: TYPE_TB
- en: '&#124; cooperative loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; similar loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; total variation loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MIT-Adobe FiveK &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MIT-Adobe FiveK &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Runtime #P FLOPs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zhang et al. [[22](#bib.bib22)] | SL | U-Net |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $L_{1}$ loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated by illumination &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adjustmentand noise simulation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; self-selected &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; User Study PSNR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM AB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MABD WE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zero-DCE++ [[30](#bib.bib30)] | ZSL | U-Net like network |'
  prefs: []
  type: TYPE_TB
- en: '&#124; spatial consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; exposure control loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; color constancy loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; illumination smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SICE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SICE NPE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LIME MEF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DICM VV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DARK FACE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; User Study PI &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PNSR SSIM #P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MAE Runtime &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Face detection FLOPs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | DRBN [[34](#bib.bib34)] | SSL | recursive network |'
  prefs: []
  type: TYPE_TB
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; detail loss quality loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| LOL | LOL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM-GC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Retinex-Net [[24](#bib.bib24)] | SL | three subnetworks |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $L_{1}$ loss $L_{2}$ loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSIM loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; total variation loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated by adjusting histogram &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NPE DICM VV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PNSR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; UQI OSS User Study &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | RetinexDIP[[31](#bib.bib31)] | ZSL | encoder-decoder networks |'
  prefs: []
  type: TYPE_TB
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; illumination-consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reflectnce loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; illumination smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| - |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DICM, ExDark &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fusion LIME &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NASA NPE VV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NIQE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NIQMC CPCQI &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | PRIEN [[23](#bib.bib23)] | SL | recursive network | SSIM loss |'
  prefs: []
  type: TYPE_TB
- en: '&#124; MEF LOL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated by adjusting histogram &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOL LIME &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NPE MEF VV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PNSR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LOE TMQI &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | PyTorch |  |'
  prefs: []
  type: TYPE_TB
- en: 3 Technical Review and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we first summarize the representative deep learning-based
    LLIE methods in Table [I](#S2.T1 "TABLE I ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based
    LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey"),
    then analyze and discuss their technical characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Network Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Diverse network structures and designs have been used in the existing models,
    spanning from the basic U-Net, pyramid network, multi-stage network to frequency
    decomposition network. After analyzing Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning
    Strategies ‣ 2 Deep Learning-Based LLIE ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey")(b), it can be observed that the U-Net and U-Net-like
    networks are mainly adopted network structures in LLIE. This is because U-Net
    can effectively integrate multi-scale features and employ both low-level and high-level
    features. Such characteristics are essential for achieving satisfactory low-light
    enhancement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, some key issues may be ignored in the current LLIE network structures:
    1) after going through several convolutional layers, the gradients of an extremely
    low light image may vanish during the gradient back-propagation due to its small
    pixel values. This would degrade the enhancement performance and affect the convergence
    of network training, 2) the skip-connections used in the U-Net-like networks might
    introduce noise and redundant features into the final results. How to effectively
    filter out the noise and integrate both low-level and high-level features should
    be carefully considered, and 3) although some designs and components are proposed
    for LLIE, most of them are borrowed or modified from related low-level visual
    tasks. The characteristics of low-light data should be considered when designing
    the network structures.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Combination of Deep Model and Retinex Theory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As presented in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep
    Learning-Based LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey")(c), almost 1/3 of methods combine the designs of deep networks with
    the Retinex theory, e.g., designing different subnetworks to estimate the components
    of the Retinex model and estimating the illumination map to guide the learning
    of networks. Despite such a combination can bridge deep learning-based and model-based
    methods, their respective weaknesses may be introduced into the final models:
    1) the ideal assumption that the reflectance is the final enhanced result used
    in Retinex-based LLIE methods would still affect the final results, and 2) the
    risk of overfitting in deep networks still exists despite the use of Retinex theory.
    How to cream off the best and filter out the impurities should be carefully considered
    when researchers combine deep learning with the Retinex theory.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Data Format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep
    Learning-Based LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey")(d), RGB data format dominates most methods as it is commonly found
    as the final imagery form produced by smartphone cameras, Go-Pro cameras, and
    drone cameras. Although raw data are limited to specific sensors such as those
    based on Bayer patterns, the data cover wider color gamut and higher dynamic range.
    Hence, deep models trained on raw data usually recover clear details and high
    contrast, obtain vivid color, reduce the effects of noises and artifacts, and
    improve the brightness of extremely low-light images. In future research, a smooth
    transformation from raw data of different patterns to RGB format would have the
    potentials to combine the convenience of RGB data and the advantage of high-quality
    enhancement of raw data for LLIE.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Loss Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Figure[3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based
    LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(e),
    the commonly adopted loss functions in LLIE models include reconstruction loss
    ($L_{1}$, $L_{2}$, SSIM), perceptual loss, and smoothness loss. Besides, according
    to different demands and formulations, color loss, exposure loss, adversarial
    loss, etc are also adopted. We detail representative loss functions as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: Reconstruction Loss. Different reconstruction losses have their advantages and
    disadvantages. $L_{2}$ loss tends to penalize larger errors, but is tolerant to
    small errors. $L_{1}$ loss preserves colors and luminance well since an error
    is weighted equally regardless of the local structure. SSIM loss preserves the
    structure and texture well. Refer to this research paper [[67](#bib.bib67)] for
    detailed analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptual Loss. Perceptual loss [[68](#bib.bib68)], particularly the feature
    reconstruction loss, is proposed to constrain the results similar to the ground
    truth in the feature space. The loss improves the visual quality of results. It
    is defined as the Euclidean distance between the feature representations of an
    enhanced result and those of corresponding ground truth. The feature representations
    are typically extracted from the VGG network [[69](#bib.bib69)] pre-trained on
    ImageNet dataset [[70](#bib.bib70)].
  prefs: []
  type: TYPE_NORMAL
- en: Smoothness Loss. To remove noise in the enhanced results or preserve the relationship
    of neighboring pixels, smoothness loss (TV loss) is often used to constrain the
    enhanced result or the estimated illumination map.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial Loss. To encourage enhanced results to be indistinguishable from
    reference images, adversarial learning solves a max-min optimization problem [[71](#bib.bib71),
    [72](#bib.bib72)].
  prefs: []
  type: TYPE_NORMAL
- en: Exposure Loss. As one of key non-reference losses, exposure loss measures the
    exposure levels of enhanced results without paired or unpaired images as reference
    images.
  prefs: []
  type: TYPE_NORMAL
- en: The commonly used loss functions in LLIE networks are also employed in image
    reconstruction networks for image super-resolution [[73](#bib.bib73)], image denoising
    [[74](#bib.bib74)], image detraining [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77)],
    and image deblurring [[78](#bib.bib78)]. Different from these versatile losses,
    the specially designed exposure loss for LLIE inspires the design of non-reference
    losses. A non-reference loss makes a model enjoying better generalization capability.
    It is an on-going research to consider image characteristics for the design of
    loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Training Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based
    LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(f)
    reports the usage of a variety of paired training datasets for training low-light
    enhancement networks. These datasets include real-world captured datasets and
    synthetic datasets. We list them in Table [II](#S3.T2 "TABLE II ‣ 3.5 Training
    Datasets ‣ 3 Technical Review and Discussion ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simulated by Gamma Correction. Owing to its nonlinearity and simplicity, Gamma
    correction is used to adjust the luminance or tristimulus values in video or still
    image systems. It is defined by a power-law expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V_{\text{out}}=AV_{\text{in}}^{\gamma},$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where the input $V_{\text{in}}$ and output $V_{\text{out}}$ are typically in
    the range of [0,1]. The constant $A$ is set to 1 in the common case. The power
    $\gamma$ controls the luminance of the output. Intuitively, the input is brightened
    when $\gamma<$1 while the input is darkened when $\gamma>$1\. The input can be
    the three RGB channels of an image or the luminance-related channels such as $L$
    channel in the CIELab color space and $Y$ channel in the YCbCr color space. After
    adjusting the luminance-related channel using Gamma correction, the corresponding
    channels in the color space are adjusted by equal proportion to avoid producing
    artifacts and color deviations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simulate images taken in real-world low-light scenes, Gaussian noise, Poisson
    noise, or realistic noise is added to the Gamma corrected images. The low-light
    image synthesized using Gamma correction can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I_{\text{low}}=n(g(I_{\text{in}};\gamma)),$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $n$ represents the noise model, $g(I_{\text{in}};\gamma)$ represents the
    Gamma correction function with Gamma value $\gamma$, $I_{\text{in}}$ is a normal-light
    and high-quality image or luminance-related channel. Although this function produces
    low-light images of different lighting levels by changing the Gamma value $\gamma$,
    it tends to introduce artifacts and color deviations into the synthetic low-light
    images due to the nonlinear adjustment.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Summary of paired training datasets. ‘Syn’ represents Synthetic.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Number | Format | Real/Syn | Video |'
  prefs: []
  type: TYPE_TB
- en: '| Gamma Correction | +$\infty$ | RGB | Syn |  |'
  prefs: []
  type: TYPE_TB
- en: '| Random Illumination | +$\infty$ | RGB | Syn |  |'
  prefs: []
  type: TYPE_TB
- en: '| LOL [[4](#bib.bib4)] | 500 | RGB | Real |  |'
  prefs: []
  type: TYPE_TB
- en: '| SCIE [[6](#bib.bib6)] | 4,413 | RGB | Real |  |'
  prefs: []
  type: TYPE_TB
- en: '| VE-LOL-L [[55](#bib.bib55)] | 2,500 | RGB | Real+Syn |  |'
  prefs: []
  type: TYPE_TB
- en: '| MIT-Adobe FiveK [[79](#bib.bib79)] | 5,000 | raw | Real |  |'
  prefs: []
  type: TYPE_TB
- en: '| SID [[4](#bib.bib4)] | 5,094 | raw | Real |  |'
  prefs: []
  type: TYPE_TB
- en: '| DRV [[8](#bib.bib8)] | 202 | raw | Real | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| SMOID [[9](#bib.bib9)] | 179 | raw | Real | ✓ |'
  prefs: []
  type: TYPE_TB
- en: Simulated by Random Illumination. According to the Retinex model, an image can
    be decomposed into a reflectance component and an illumination component. Assuming
    image content is independent of illumination component and local region in the
    illumination component have the same intensity, a low-light image can be obtained
    by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I_{\text{low}}=I_{\text{in}}L,$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $L$ is a random illumination value in the range of [0,1]. Noises can be
    added to the synthetic image. Such a linear function avoids artifacts, but the
    strong assumption requires the synthesis to operate only on image patches where
    local regions have the same brightness. A deep model trained on such image patches
    may lead to sub-optimal performance due to the negligence of context information.
  prefs: []
  type: TYPE_NORMAL
- en: LOL. LOL [[4](#bib.bib4)] is the first paired low-/normal-light image dataset
    taken in real scenes. The low-light images are collected by changing the exposure
    time and ISO. LOL contains 500 pairs of low-/normal-light images of size 400$\times$600
    saved in RGB format.
  prefs: []
  type: TYPE_NORMAL
- en: SCIE. SCIE is a multi-exposure image dataset of low-contrast and good-contrast
    image pairs. It includes multi-exposure sequences of 589 indoor and outdoor scenes.
    Each sequence has 3 to 18 low-contrast images of different exposure levels, thus
    containing 4,413 multi-exposure images in total. The 589 high-quality reference
    images are obtained by selecting from the results of 13 representative enhancement
    algorithms. That is many multi-exposure images have the same high-contrast reference
    image. The image resolutions are between 3,000$\times$2,000 and 6,000$\times$4,000\.
    The images in SCIE are saved in RGB format.
  prefs: []
  type: TYPE_NORMAL
- en: MIT-Adobe FiveK. MIT-Adobe FiveK [[79](#bib.bib79)] was collected for global
    tone adjustment but has been used in LLIE. This is because the input images have
    low light and low contrast. MIT-Adobe FiveK contains 5,000 images, each of which
    is retouched by 5 trained photographers towards visually pleasing renditions,
    akin to a postcard. The images are all in raw format. To train the networks that
    can handle images of RGB format, one needs to use Adobe Lightroom to pre-process
    the images and save them as RGB format following a dedicated pipeline¹¹1[https://github.com/nothinglo/Deep-Photo-Enhancer/issues/38#issuecomment-449786636](https://github.com/nothinglo/Deep-Photo-Enhancer/issues/38#issuecomment-449786636).
    The images are commonly resized to have a long edge of 500 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'SID. SID [[2](#bib.bib2)] contains 5,094 raw short-exposure images, each with
    a corresponding long-exposure reference image. The number of distinct long-exposure
    reference images is 424\. In other words, multiple short-exposure images correspond
    to the same long-exposure reference image. The images were taken using two cameras:
    Sony $\alpha$7S II and Fujifilm X-T2 in both indoor and outdoor scenes. Thus,
    the images have different sensor patterns (Sony camera’ Bayer sensor and Fuji
    camera’s APS-C X-Trans sensor). The resolution is 4,240$\times$2,832 for Sony
    and 6,000$\times$4,000 for Fuji. Usually, the long-exposure images are processed
    by libraw (a raw image processing library) and saved in the RGB color space, and
    randomly cropped 512$\times$512 patches for training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'VE-LOL. VE-LOL [[55](#bib.bib55)] consists of two subsets: paired VE-LOL-L
    that is used for training and evaluating LLIE methods and unpaired VE-LOL-H that
    is used for evaluating the effect of LLIE methods on face detection. Specifically,
    VE-LOL-L includes 2,500 paired images. Among them, 1,000 pairs are synthetic,
    while 1,500 pairs are real. VE-LOL-H includes 10,940 unpaired images, where human
    faces are manually annotated with bounding boxes.'
  prefs: []
  type: TYPE_NORMAL
- en: DRV. DRV [[8](#bib.bib8)] contains 202 static raw videos, each of which has
    a corresponding long-exposure ground truth. Each video was taken at approximately
    16 to 18 frames per second in a continuous shooting mode and is with up to 110
    frames. The images were taken by a Sony RX100 VI camera in both indoor and outdoor
    scenes, thus all in raw format of Bayer pattern. The resolution is 3,672$\times$5,496.
  prefs: []
  type: TYPE_NORMAL
- en: SMOID. SMOID [[9](#bib.bib9)] contains 179 pairs of videos taken by a co-axis
    optical system, each of which has 200 frames. Thus, SMOID includes 35,800 extremely
    low light raw data of Bayer pattern and their corresponding well-lightened RGB
    counterparts. SMOID consists of moving vehicles and pedestrians under different
    illumination conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some issues challenge the aforementioned paired training datasets: 1) deep
    models trained on synthetic data may introduce artifacts and color deviations
    when processing real-world images and videos due to the gap between synthetic
    data and real data, 2) the scale and diversity of real training data are unsatisfactory,
    thus some methods incorporate synthetic data to augment the training data. This
    may lead to sub-optimal enhancement, and 3) the input images and corresponding
    ground truths may exist misalignment due to the effects of motion, hardware, and
    environment. This would affect the performance of deep networks trained using
    pixel-wise loss functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Testing Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to the testing subsets in the paired datasets [[4](#bib.bib4),
    [6](#bib.bib6), [79](#bib.bib79), [2](#bib.bib2), [8](#bib.bib8), [9](#bib.bib9),
    [55](#bib.bib55)], there are several testing data collected from related works
    or commonly used for experimental comparisons. Besides, some datasets such as
    face detection in the dark [[80](#bib.bib80)] and detection and recognition in
    low-light images [[81](#bib.bib81)] are employed to test the effects of LLIE on
    high-level visual tasks. We summarize the commonly used testing datasets in Table
    [III](#S3.T3 "TABLE III ‣ 3.6 Testing Datasets ‣ 3 Technical Review and Discussion
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey") and introduce
    the representative testing datasets as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Summary of testing datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Number | Format | Application | Video |'
  prefs: []
  type: TYPE_TB
- en: '| LIME [[39](#bib.bib39)] | 10 | RGB |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| NPE [[37](#bib.bib37)] | 84 | RGB |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MEF [[82](#bib.bib82)] | 17 | RGB |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| DICM [[83](#bib.bib83)] | 64 | RGB |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| VV²²2[https://sites.google.com/site/vonikakis/datasets](https://sites.google.com/site/vonikakis/datasets)
    | 24 | RGB |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| BBD-100K [[84](#bib.bib84)] | 10,000 | RGB | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| ExDARK [[81](#bib.bib81)] | 7,363 | RGB | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| DARK FACE [[80](#bib.bib80)] | 6,000 | RGB | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| VE-LOL-H [[55](#bib.bib55)] | 10,940 | RGB | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: BBD-100K. BBD-100K [[84](#bib.bib84)] is the largest driving video dataset with
    10,000 videos taken over 1,100-hour driving experience across many different times
    in the day, weather conditions, and driving scenarios, and 10 tasks annotations.
    The videos taken at nighttime in BBD-100K are used to validate the effects of
    LLIE on high-level visual tasks and the enhancement performance in real scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: ExDARK. ExDARK [[81](#bib.bib81)] dataset is built for object detection and
    recognition in low-light images. ExDARK dataset contains 7,363 low-light images
    from extremely low-light environments to twilight with 12 object classes annotated
    with image class labels and local object bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: DARK FACE. DARK FACE [[80](#bib.bib80)] dataset contains 6,000 low-light images
    captured during the nighttime, each of which is labeled with bounding boxes of
    the human face.
  prefs: []
  type: TYPE_NORMAL
- en: 'From Figure [III](#S3.T3 "TABLE III ‣ 3.6 Testing Datasets ‣ 3 Technical Review
    and Discussion ‣ Low-Light Image and Video Enhancement Using Deep Learning: A
    Survey")(g) and Table [I](#S2.T1 "TABLE I ‣ 2.2 Learning Strategies ‣ 2 Deep Learning-Based
    LLIE ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey"),
    we can observe that one prefers using the self-collected testing data in the experiments.
    The main reasons lie into three-fold: 1) besides the test partition of paired
    datasets, there is no acknowledged benchmark for evaluations, 2) the commonly
    used test sets suffer from some shortcomings such as small scale (some test sets
    contain 10 images only), repeated content and illumination properties, and unknown
    experimental settings, and 3) some of the commonly used testing data are not originally
    collected for evaluating LLIE. In general, current testing datasets may lead to
    bias and unfair comparisons.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides human perception-based subjective evaluations, image quality assessment
    (IQA) metrics, including both full-reference and non-reference IQA metrics, are
    able to evaluate image quality objectively. In addition, user study, number of
    trainable parameters, FLOPs, runtime, and applications also reflect the performance
    of LLIE models, as shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Learning Strategies
    ‣ 2 Deep Learning-Based LLIE ‣ Low-Light Image and Video Enhancement Using Deep
    Learning: A Survey")(h). We will detail them as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: PSNR and MSE. PSNR and MSE are widely used IQA metrics. They are always non-negative,
    and values closer to infinite (PSNR) and zero (MSE) are better. Nevertheless,
    the pixel-wise PSNR and MSE may provide an inaccurate indication of the visual
    perception of image quality since they neglect the relation of neighboring pixels.
  prefs: []
  type: TYPE_NORMAL
- en: MAE. MAE represents the mean absolute error, serving as a measure of errors
    between paired observations. The smaller the MAE value is, the better similarity
    is.
  prefs: []
  type: TYPE_NORMAL
- en: SSIM. SSIM is used to measure the similarity between two images. It is a perception-based
    model that considers image degradation as perceived change in structural information.
    The value 1 is only reachable in the case of two identical sets of data, indicating
    perfect structural similarity.
  prefs: []
  type: TYPE_NORMAL
- en: LOE. LOE represents the lightness order error that reflects the naturalness
    of an enhanced image. For LOE, the smaller the LOE value is, the better the lightness
    order is preserved.
  prefs: []
  type: TYPE_NORMAL
- en: Application. Besides improving the visual quality, one of the purposes of image
    enhancement is to serve high-level visual tasks. Thus, the effects of LLIE on
    high-level visual applications are commonly examined to validate the performance
    of different methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The current evaluation approaches used in LLIE need to be improved in several
    aspects: 1) although the PSNR, MSE, MAE, and SSIM are classic and popular metrics,
    they are still far from capturing real visual perception of human, 2) some metrics
    are not originally designed for low-light images. They are used for assessing
    the fidelity of image information and contrast. Using these metrics may reflect
    the image quality, but they are far from the real purpose of low-light enhancement,
    3) metrics especially designed for low-light images are lacking, except for the
    LOE metric. Moreover, there is no metric for evaluating low-light video enhancement,
    and 4) a metric that can balance both the human vision and the machine perception
    is expected.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Benchmarking and Empirical Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section provides empirical analysis and highlights some key challenges
    in deep learning-based LLIE. To facilitate the analysis, we propose a low-light
    image and video dataset to examine the performance of different solutions. We
    also develop the first online platform, where the results of LLIE models can be
    produced via a user-friendly web interface. In this section, we conduct extensive
    evaluations on several benchmarks and our proposed dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the experiments, we compare 13 representative RGB format-based methods, including
    eight supervised learning-based methods (LLNet [[1](#bib.bib1)], LightenNet [[5](#bib.bib5)],
    Retinex-Net [[4](#bib.bib4)], MBLLEN [[3](#bib.bib3)], KinD [[11](#bib.bib11)],
    KinD++ [[61](#bib.bib61)], TBEFN [[20](#bib.bib20)], DSLR [[21](#bib.bib21)]),
    one unsupervised learning-based method (EnlightenGAN [[26](#bib.bib26)]), one
    semi-supervised learning-based method (DRBN [[33](#bib.bib33)]), and three zero-shot
    learning-based methods (ExCNet [[27](#bib.bib27)], Zero-DCE [[28](#bib.bib28)],
    RRDNet [[29](#bib.bib29)]). Besides, we also compare two raw format-based methods,
    including SID [[85](#bib.bib85)] and EEMEFN [[16](#bib.bib16)]. Note that RGB
    format-based methods dominate LLIE. Moreover, most raw format-based methods do
    not release their code. Thus, we choose two representative methods to provide
    empirical analysis and insights. For all compared methods, we use the publicly
    available code to produce their results for fair comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Summary of LLIV-Phone dataset. LLIV-Phone dataset contains 120 videos
    (45,148 images) taken by 18 different mobile phones’ cameras. “#Video” and “#Image”
    represent the number of videos and images, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Phone’s Brand | #Video | #Image | Resolution |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| iPhone 6s | 4 | 1,029 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| iPhone 7 | 13 | 6,081 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| iPhone7 Plus | 2 | 900 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| iPhone8 Plus | 1 | 489 | 1280$\times$720 |'
  prefs: []
  type: TYPE_TB
- en: '| iPhone 11 | 7 | 2,200 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| iPhone 11 Pro | 17 | 7,739 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| iPhone XS | 11 | 2,470 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| iPhone XR | 16 | 4,997 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| iPhone SE | 1 | 455 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| Xiaomi Mi 9 | 2 | 1,145 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| Xiaomi Mi Mix 3 | 6 | 2,972 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| Pixel 3 | 4 | 1,311 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| Pixel 4 | 3 | 1,923 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| Oppo R17 | 6 | 2,126 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| Vivo Nex | 12 | 4,097 | 1280$\times$720 |'
  prefs: []
  type: TYPE_TB
- en: '| LG M322 | 2 | 761 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| OnePlus 5T | 1 | 293 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: '| Huawei Mate 20 Pro | 12 | 4,160 | 1920$\times$1080 |'
  prefs: []
  type: TYPE_TB
- en: 4.1 A New Low-Light Image and Video Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We propose a Low-Light Image and Video dataset, called LLIV-Phone, to comprehensively
    and thoroughly validate the performance of LLIE methods. LLIV-Phone is the largest
    and most challenging real-world testing dataset of its kind. In particular, the
    dataset contains 120 videos (45,148 images) taken by 18 different mobile phones’
    cameras including iPhone 6s, iPhone 7, iPhone7 Plus, iPhone8 Plus, iPhone 11,
    iPhone 11 Pro, iPhone XS, iPhone XR, iPhone SE, Xiaomi Mi 9, Xiaomi Mi Mix 3,
    Pixel 3, Pixel 4, Oppo R17, Vivo Nex, LG M322, OnePlus 5T, Huawei Mate 20 Pro
    under diverse illumination conditions (e.g., weak lighting, underexposure, moonlight,
    twilight, dark, extremely dark, back-lit, non-uniform light, and colored light.)
    in both indoor and outdoor scenes. A summary of the LLIV-Phone dataset is provided
    in Table [IV](#S4.T4 "TABLE IV ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey"). We present several
    samples of LLIV-Phone dataset in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 A New Low-Light
    Image and Video Dataset ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image
    and Video Enhancement Using Deep Learning: A Survey"). The LLIV-Phone dataset
    is available at the project page.'
  prefs: []
  type: TYPE_NORMAL
- en: This challenging dataset is collected in real scenes and contains diverse low-light
    images and videos. Consequently, it is suitable for evaluating the generalization
    capability of different low-light image and video enhancement models. Notably,
    the dataset can be used as the training dataset for unsupervised learning and
    the reference dataset for synthesis methods to generate realistic low-light data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bd64b3f5572943e39ae67448e8492757.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Several images sampled from the proposed LLIV-Phone dataset. The
    images and videos are taken by different devices under diverse lighting conditions
    and scenes.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/3c706fc44a84e9110beb2e71b8298821.png) | ![Refer to
    caption](img/5eb6e871b477e8c14341de16e6272c73.png) | ![Refer to caption](img/3f7c201ce07eddc43673dd88c820c4bb.png)
    | ![Refer to caption](img/8434dfec4684e78191ed0ce297a28da3.png) | ![Refer to caption](img/1615b227424e3bef5ed0a436758e143d.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| (a) input | (b) LLNet [[1](#bib.bib1)] | (c) LightenNet [[5](#bib.bib5)]
    | (d) Retinex-Net [[4](#bib.bib4)] | (e) MBLLEN [[3](#bib.bib3)] |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/3a9ee856055f029c3358505a7d355da1.png) | ![Refer to
    caption](img/0f87209a6e0e00d0663ebfa6aae5097e.png) | ![Refer to caption](img/925478fa0a7f0953b26a2295b5228067.png)
    | ![Refer to caption](img/f5353e3bbbf76dc68596b9db8559a57f.png) | ![Refer to caption](img/8f0320c3d604f6816671ce45f15b6acf.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| (f) KinD [[11](#bib.bib11)] | (g) KinD++ [[61](#bib.bib61)] | (h) TBEFN [[20](#bib.bib20)]
    | (i) DSLR [[21](#bib.bib21)] | (j) EnlightenGAN [[26](#bib.bib26)] |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/d9064d7fab408801cf8edf59ffcd8197.png) | ![Refer to
    caption](img/13f512cb9d35b49ec7a643de81769429.png) | ![Refer to caption](img/96df4cf9d8efa487770bda5f4fcdd24a.png)
    | ![Refer to caption](img/cc53ff9c303e13cf4430a0be5e201f73.png) | ![Refer to caption](img/325808bcf76c3befcf155f43db6ebbd0.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| (k) DRBN [[33](#bib.bib33)] | (l) ExCNet [[27](#bib.bib27)] | (m) Zero-DCE
    [[28](#bib.bib28)] | (n) RRDNet [[29](#bib.bib29)] | (o) GT |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5: Visual results of different methods on a low-light image sampled
    from LOL-test dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/648ccd67d0c0e08c6183971b34a61f68.png) | ![Refer to
    caption](img/c65ce3ca0a8d328e7e5c2f64a1bfccad.png) | ![Refer to caption](img/12c63d6ef5c6c279537f3dff22548fd6.png)
    | ![Refer to caption](img/8da1c513509ae9c4b773de3d3a355839.png) | ![Refer to caption](img/a7a2ff9425630293b7de905ba7549cce.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| (a) input | (b) LLNet [[1](#bib.bib1)] | (c) LightenNet [[5](#bib.bib5)]
    | (d) Retinex-Net [[4](#bib.bib4)] | (e) MBLLEN [[3](#bib.bib3)] |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/3652d82b7f8d49835b290f1e4c2f853e.png) | ![Refer to
    caption](img/2939216df2b529bee7cc24f43dc688f5.png) | ![Refer to caption](img/68ca8b893a718fc5242c8d7d96516348.png)
    | ![Refer to caption](img/34a6f4dac1fe026bec8749b47729b5ea.png) | ![Refer to caption](img/4c27d9885ce4271e2deb05a135056a14.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| (f) KinD [[11](#bib.bib11)] | (g) KinD++ [[61](#bib.bib61)] | (h) TBEFN [[20](#bib.bib20)]
    | (i) DSLR [[21](#bib.bib21)] | (j) EnlightenGAN [[26](#bib.bib26)] |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/057c22b7c61b5b88c03c22a9729f9efe.png) | ![Refer to
    caption](img/c61c20584af1c45baf8f3b02e19d1b59.png) | ![Refer to caption](img/3bc27dbdfd93f4803909aaa357b58144.png)
    | ![Refer to caption](img/197b6f69bc06b20bb34331d103c1f6da.png) | ![Refer to caption](img/540ed9a338ca54e58652826b2dd2c1ac.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| (k) DRBN [[33](#bib.bib33)] | (l) ExCNet [[27](#bib.bib27)] | (m) Zero-DCE
    [[28](#bib.bib28)] | (n) RRDNet [[29](#bib.bib29)] | (o) GT |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 6: Visual results of different methods on a low-light image sampled
    from MIT-Adobe FiveK-test dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Online Evaluation Platform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different deep models may be implemented in different platforms such as Caffe,
    Theano, TensorFlow, and PyTorch. As a result, different algorithms demand different
    configurations, GPU versions, and hardware specifications. Such requirements are
    prohibitive to many researchers, especially for beginners who are new to this
    area and may not even have GPU resources. To resolve these problems, we develop
    an LLIE online platform, called LLIE-Platform, which is available at [http://mc.nankai.edu.cn/ll/](http://mc.nankai.edu.cn/ll/).
  prefs: []
  type: TYPE_NORMAL
- en: To the date of this submission, the LLIE-Platform covers 14 popular deep learning-based
    LLIE methods including LLNet [[1](#bib.bib1)], LightenNet [[5](#bib.bib5)], Retinex-Net
    [[4](#bib.bib4)], EnlightenGAN [[26](#bib.bib26)], MBLLEN [[3](#bib.bib3)], KinD
    [[11](#bib.bib11)], KinD++ [[61](#bib.bib61)], TBEFN [[20](#bib.bib20)], DSLR
    [[21](#bib.bib21)], DRBN [[33](#bib.bib33)], ExCNet [[27](#bib.bib27)], Zero-DCE
    [[28](#bib.bib28)], Zero-DCE++ [[30](#bib.bib30)], and RRDNet [[29](#bib.bib29)],
    where the results of any input can be produced through a user-friendly web interface.
    We will regularly offer new methods on this platform. We wish that this LLIE-Platform
    could serve the growing research community by providing users a flexible interface
    to run existing deep learning-based LLIE methods and develop their own new LLIE
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/240239c26df304051fafcc5fdf003fed.png) | ![Refer to
    caption](img/3d7862df1db6f481305cb6b43a2c95c9.png) | ![Refer to caption](img/8a6ed72a21ae3cb38f0286a20d5f14ef.png)
    | ![Refer to caption](img/3e164b60ff575f8e57b173643b43aefd.png) | ![Refer to caption](img/f05e45efb338edf24c76f6a636a4999e.png)
    | ![Refer to caption](img/f9a383552b7b23a083f5ca8e603a7585.png) | ![Refer to caption](img/4aef6b046430d6cd73ea3d294dc911ec.png)
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (a) | (b) | (c) | (d) | (e) | (f) | (g) |  |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/3ce2762623e7008563487b79db8d72df.png) | ![Refer to
    caption](img/f690645750495c695fb32d992e37cd45.png) | ![Refer to caption](img/7004379667dcf0b7199a9af933584757.png)
    | ![Refer to caption](img/76ae236e44766ae3aeb47f4eafe6c3eb.png) | ![Refer to caption](img/79de4fa5501f63d6939556ac7c8a2d5f.png)
    | ![Refer to caption](img/501151d4631af3a252cbae243196672d.png) | ![Refer to caption](img/92d46ac6145fecda543b87676f5cf028.png)
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (h) | (i) | (j) | (k) | (l) | (m) | (n) |  |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 7: Visual results of different methods on a low-light image sampled
    from LLIV-Phone-imgT dataset. (a) input. (b) LLNet [[1](#bib.bib1)]. (c) LightenNet
    [[5](#bib.bib5)]. (d) Retinex-Net [[4](#bib.bib4)]. (e) MBLLEN [[3](#bib.bib3)].
    (f) KinD [[11](#bib.bib11)]. (g) KinD++ [[61](#bib.bib61)]. (h) TBEFN [[20](#bib.bib20)].
    (i) DSLR [[21](#bib.bib21)]. (j) EnlightenGAN [[26](#bib.bib26)]. (k) DRBN [[33](#bib.bib33)].
    (l) ExCNet [[27](#bib.bib27)]. (m) Zero-DCE [[28](#bib.bib28)]. (n) RRDNet [[29](#bib.bib29)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/26d9d849145089665d56680fc9cec5d7.png) | ![Refer to
    caption](img/937a56ae27589d9240734fa6e1ea37e4.png) | ![Refer to caption](img/30d4bcbced36618e25a73177cbb7dd38.png)
    | ![Refer to caption](img/4c9d1f69b70ba039b216fd7f0aa37bbd.png) | ![Refer to caption](img/272104ba9ecfe994c95ef8d7535f7d5c.png)
    | ![Refer to caption](img/0caa6284058f72ffde4708deda63fca8.png) | ![Refer to caption](img/2624a152cc77414acecfe03c90a8f6b3.png)
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (a) | (b) | (c) | (d) | (e) | (f) | (g) |  |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/e7d192b0fc44d92262a519120d0fc4f6.png) | ![Refer to
    caption](img/626e64c0472fa2b43a3bc57026e59f78.png) | ![Refer to caption](img/95835e303bdae7e5803b2b15fa05c09e.png)
    | ![Refer to caption](img/2fc3a9d5058769a6c2aad74c7d693804.png) | ![Refer to caption](img/f40fe1fa766e81020e9c5072becc9057.png)
    | ![Refer to caption](img/56540a06e92f2ae2f44c3a2d9ed9c0d2.png) | ![Refer to caption](img/b332e26780e0527d91758dd49034c88f.png)
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (h) | (i) | (j) | (k) | (l) | (m) | (n) |  |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 8: Visual results of different methods on a low-light image sampled
    from LLIV-Phone-imgT dataset. (a) input. (b) LLNet [[1](#bib.bib1)]. (c) LightenNet
    [[5](#bib.bib5)]. (d) Retinex-Net [[4](#bib.bib4)]. (e) MBLLEN [[3](#bib.bib3)].
    (f) KinD [[11](#bib.bib11)]. (g) KinD++ [[61](#bib.bib61)]. (h) TBEFN [[20](#bib.bib20)].
    (i) DSLR [[21](#bib.bib21)]. (j) EnlightenGAN [[26](#bib.bib26)]. (k) DRBN [[33](#bib.bib33)].
    (l) ExCNet [[27](#bib.bib27)]. (m) Zero-DCE [[28](#bib.bib28)]. (n) RRDNet [[29](#bib.bib29)].'
  prefs: []
  type: TYPE_NORMAL
- en: '|  Bayer            APS-C X-Trans  | ![Refer to caption](img/ef774913bc25c6f679de87ab2bffce1c.png)
    |    ![Refer to caption](img/69e61d87c3bc522a57cb31b6ea092174.png) |    ![Refer
    to caption](img/dabffbc6fa15442d8bfb49b7330d1414.png) |    ![Refer to caption](img/3c026a23f4434c08d53972c1ab7d02b7.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |   (a) inputs |    (b) SID [[85](#bib.bib85)] |    (c) EEMEFN [[16](#bib.bib16)]
    |    (d) GT |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 9: Visual results of different methods on two raw low-light images sampled
    from SID-test-Bayer and SID-test-X-Trans test datasets. The inputs are amplified
    for visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Benchmarking Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To qualitatively and quantitatively evaluate different methods, in addition
    to the proposed LLIV-Phone dataset, we also adopt the commonly used LOL [[4](#bib.bib4)]
    and MIT-Adobe FiveK [[79](#bib.bib79)] datasets for RGB format-based methods,
    and SID [[85](#bib.bib85)] dataset for raw format-based methods. More visual results
    can be found in the supplementary material. The comparative results on the real
    low-light videos taken by different mobile phones’ cameras can be found at YouTube
    [https://www.youtube.com/watch?v=Elo9TkrG5Oo&t=6s](https://www.youtube.com/watch?v=Elo9TkrG5Oo&t=6s).
  prefs: []
  type: TYPE_NORMAL
- en: We select five images on average from each video of the LLIV-Phone dataset,
    forming an image testing dataset with a total of 600 images (denoted as LLIV-Phone-imgT).
    Furthermore, we randomly select one video from the videos of each phone’s brand
    of LLIV-Phone dataset, forming a video testing dataset with a total of 18 videos
    (denoted as LLIV-Phone-vidT). We half the resolutions of the frames in both LLIV-Phone-imgT
    and LLIV-Phone-vidT because some deep learning-based methods cannot process the
    full resolution of test images and videos. For the LOL dataset, we adopt the original
    test set including 15 low-light images captured in real scenes for testing, denoted
    as LOL-test. For the MIT-Adobe FiveK dataset, we follow the protocol in Chen et
    al. [[47](#bib.bib47)] to decode the images into PNG format and resize them to
    have a long edge of 512 pixels using Lightroom. We adopt the same testing dataset
    as Chen et al. [[47](#bib.bib47)], MIT-Adobe FiveK-test, including 500 images
    with the retouching results by expert C as the corresponding ground truths. For
    the SID dataset, we use the default test set used in EEMEFN [[16](#bib.bib16)]
    for fair comparisons, denoted as SID-test (SID-test-Bayer and SID-test-X-Trans),
    which is a partial test set of SID [[85](#bib.bib85)]. The SID-test-Bayer includes
    93 images of the Bayer pattern while the SID-test-X-Trans includes 94 images of
    the APS-C X-Trans pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Quantitative comparisons on LOL-test and MIT-Adobe FiveK-test test
    datasets in terms of MSE ($\times 10^{3}$), PSNR (in dB), SSIM [[86](#bib.bib86)],
    and LPIPS [[87](#bib.bib87)]. The best result is in red whereas the second and
    third best results are in blue and purple under each case, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning | Method | LOL-test | MIT-Adobe FiveK-test |'
  prefs: []
  type: TYPE_TB
- en: '| MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |
    MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | input | 12.613 | 7.773 | 0.181 | 0.560 | 1.670 | 17.824 | 0.779 | 0.148
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLNet [[1](#bib.bib1)] | 1.290 | 17.959 | 0.713 | 0.360 | 4.465 | 12.177
    | 0.645 | 0.292 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LightenNet [[5](#bib.bib5)] | 7.614 | 10.301 | 0.402 | 0.394 | 4.127 |
    13.579 | 0.744 | 0.166 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Retinex-Net [[4](#bib.bib4)] | 1.651 | 16.774 | 0.462 | 0.474 | 4.406
    | 12.310 | 0.671 | 0.239 |'
  prefs: []
  type: TYPE_TB
- en: '| SL | MBLLEN [[3](#bib.bib3)] | 1.444 | 17.902 | 0.715 | 0.247 | 1.296 | 19.781
    | 0.825 | 0.108 |'
  prefs: []
  type: TYPE_TB
- en: '|  | KinD [[11](#bib.bib11)] | 1.431 | 17.648 | 0.779 | 0.175 | 2.675 | 14.535
    | 0.741 | 0.177 |'
  prefs: []
  type: TYPE_TB
- en: '|  | KinD++ [[61](#bib.bib61)] | 1.298 | 17.752 | 0.760 | 0.198 | 7.582 | 9.732
    | 0.568 | 0.336 |'
  prefs: []
  type: TYPE_TB
- en: '|  | TBEFN [[20](#bib.bib20)] | 1.764 | 17.351 | 0.786 | 0.210 | 3.865 | 12.769
    | 0.704 | 0.178 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DSLR [[21](#bib.bib21)] | 3.536 | 15.050 | 0.597 | 0.337 | 1.925 | 16.632
    | 0.782 | 0.167 |'
  prefs: []
  type: TYPE_TB
- en: '| UL | EnlightenGAN [[26](#bib.bib26)] | 1.998 | 17.483 | 0.677 | 0.322 | 3.628
    | 13.260 | 0.745 | 0.170 |'
  prefs: []
  type: TYPE_TB
- en: '| SSL | DRBN [[33](#bib.bib33)] | 2.359 | 15.125 | 0.472 | 0.316 | 3.314 |
    13.355 | 0.378 | 0.281 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ExCNet [[27](#bib.bib27)] | 2.292 | 15.783 | 0.515 | 0.373 | 2.927 | 13.978
    | 0.710 | 0.187 |'
  prefs: []
  type: TYPE_TB
- en: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 3.282 | 14.861 | 0.589 | 0.335 | 3.476
    | 13.199 | 0.709 | 0.203 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RRDNet [[29](#bib.bib29)] | 6.313 | 11.392 | 0.468 | 0.361 | 7.057 | 10.135
    | 0.620 | 0.303 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: Quantitative comparisons on SID-test test dataset in terms of MSE
    ($\times 10^{3}$), PSNR (in dB), SSIM [[86](#bib.bib86)], and LPIPS [[87](#bib.bib87)].
    The best result is in red under each case. To compute the quantitative scores
    of input raw data, we use the corresponding camera ISP pipelines provided by Chen
    et al. [[85](#bib.bib85)] to transfer raw data to RGB format.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning | Method | SID-test–Bayer | SID-test–X-Trans |'
  prefs: []
  type: TYPE_TB
- en: '| MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |
    MSE$\downarrow$ | PSNR $\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | input | 5.378 | 11.840 | 0.063 | 0.711 | 4.803 | 11.880 | 0.075 | 0.796
    |'
  prefs: []
  type: TYPE_TB
- en: '| SL | SID [[85](#bib.bib85)] | 0.140 | 28.614 | 0.757 | 0.465 | 0.235 | 26.663
    | 0.680 | 0.586 |'
  prefs: []
  type: TYPE_TB
- en: '|  | EEMEFN [[16](#bib.bib16)] | 0.126 | 29.212 | 0.768 | 0.448 | 0.191 | 27.423
    | 0.695 | 0.546 |'
  prefs: []
  type: TYPE_TB
- en: 'Qualitative Comparison. We first present the results of different methods on
    the images sampled from LOL-test and MIT-Adobe FiveK-test datasets in Figures
    [5](#S4.F5 "Figure 5 ‣ 4.1 A New Low-Light Image and Video Dataset ‣ 4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey") and [6](#S4.F6 "Figure 6 ‣ 4.1 A New Low-Light Image and Video Dataset
    ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.1 A New Low-Light Image and Video
    Dataset ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey"), all methods improve the brightness and contrast
    of the input image. However, none of them successfully recovers the accurate color
    of the input image when the results are compared with the ground truth. In particular,
    LLNet [[1](#bib.bib1)] produces blurring result. LightenNet [[5](#bib.bib5)] and
    RRDNet [[29](#bib.bib29)] produce under-exposed results while MBLLEN [[3](#bib.bib3)]
    and ExCNet [[27](#bib.bib27)] over-expose the image. KinD [[11](#bib.bib11)],
    KinD++ [[61](#bib.bib61)], TBEFN [[20](#bib.bib20)], DSLR [[21](#bib.bib21)],
    EnlightenGAN [[26](#bib.bib26)], and DRBN [[33](#bib.bib33)] introduce obvious
    artifacts. In Figure [6](#S4.F6 "Figure 6 ‣ 4.1 A New Low-Light Image and Video
    Dataset ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey"), LLNet [[5](#bib.bib5)], KinD++ [[61](#bib.bib61)],
    TBEFN [[20](#bib.bib20)], and RRDNet [[29](#bib.bib29)] produce over-exposed results.
    Retinex-Net [[4](#bib.bib4)], KinD++ [[61](#bib.bib61)], and RRDNet [[29](#bib.bib29)]
    yield artifacts and blurring in the results.'
  prefs: []
  type: TYPE_NORMAL
- en: We found that the ground truths of MIT-Adobe FiveK dataset still contain some
    dark regions. This is because the dataset is originally designed for global image
    retouching, where restoring low light regions is not the main priority in this
    task. We also observed that the input images in LOL dataset and MIT-Adobe FiveK
    dataset are relatively clean from noise, which is different from real low-light
    scenes. Although some methods [[60](#bib.bib60), [18](#bib.bib18), [21](#bib.bib21)]
    take the MIT-Adobe FiveK dataset as the training or testing dataset, we argue
    that this dataset is not appropriate for the task of LLIE due to its mismatched/unsatisfactory
    ground truth for LLIE.
  prefs: []
  type: TYPE_NORMAL
- en: 'To examine the generalization capability of different methods, we conduct comparisons
    on the images sampled from our LLIV-Phone-imgT dataset. The visual results of
    different methods are shown in Figures [7](#S4.F7 "Figure 7 ‣ 4.2 Online Evaluation
    Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey") and [8](#S4.F8 "Figure 8 ‣ 4.2 Online Evaluation
    Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey"). As presented in Figure [7](#S4.F7 "Figure 7 ‣
    4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey"), all methods cannot
    effectively improve the brightness and remove the noise of the input low-light
    image. Moreover, Retinex-Net [[4](#bib.bib4)], MBLLEN [[3](#bib.bib3)], and DRBN
    [[33](#bib.bib33)] produce obvious artifacts. In Figure [8](#S4.F8 "Figure 8 ‣
    4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey"), all methods enhance
    the brightness of this input image. However, only MBLLEN [[3](#bib.bib3)] and
    RRDNet [[29](#bib.bib29)] obtain visually pleasing enhancement without color deviation,
    artifacts, and over-/under-exposure. Notably, for regions with a light source,
    none of the methods can brighten the image without amplifying the noise around
    these regions. Taking light sources into account for LLIE would be an interesting
    direction to explore. The results suggest the difficulty of enhancing the images
    of the LLIV-Phone-imgT dataset. Real low-light images fail most existing LLIE
    methods due to the limited generalization capability of these methods. The potential
    reasons are the use of synthetic training data, small-scaled training data, or
    unrealistic assumptions such as the local illumination consistency and treating
    the reflectance component as the final result in the Retinex model in these methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We further present the visual comparisons of raw format-based methods in Figure
    [9](#S4.F9 "Figure 9 ‣ 4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical
    Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey").
    As shown, the input raw data have obvious noises. Both SID [[2](#bib.bib2)] and
    EEMEFN [[16](#bib.bib16)] can effectively remove the effects of noises. In comparison
    to the simple U-Net structure used in SID [[2](#bib.bib2)], the more complex structure
    of EEMEFN [[16](#bib.bib16)] obtains better brightness recovery. However, their
    results are far from the corresponding GT, especially for the input of APS-C X-Trans
    pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantitative Comparison. For test sets with ground truth i.e., LOL-test, MIT-Adobe
    FiveK-test, and SID-test, we adopt MSE, PSNR, SSIM [[86](#bib.bib86)], and LPIPS
    [[87](#bib.bib87)] metrics to quantitatively compare different methods. LPIPS
    [[87](#bib.bib87)] is a deep learning-based image quality assessment metric that
    measures the perceptual similarity between a result and its corresponding ground
    truth by deep visual representations. For LPIPS, we employ the AlexNet-based model
    to compute the perceptual similarity. A lower LPIPS value suggests a result that
    is closer to the corresponding ground truth in terms of perceptual similarity.
    In Table [V](#S4.T5 "TABLE V ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking and Empirical
    Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")
    and Table [VI](#S4.T6 "TABLE VI ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking and
    Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"), we show the quantitative results of RGB format-based methods and raw
    format-based methods, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As presented in Table [V](#S4.T5 "TABLE V ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"), the quantitative scores of supervised learning-based methods are better
    than those of unsupervised learning-based, semi-supervised learning-based, and
    zero-shot learning-based methods on LOL-test and MIT-Adobe FiveK-test datasets.
    Among them, LLNet [[1](#bib.bib1)] obtains the best MSE and PSNR values on the
    LOL-test dataset; however, its performance drops on the MIT-Adobe FiveK-test dataset.
    This may be caused by the bias of LLNet [[1](#bib.bib1)] towards the LOL dataset
    since it was trained using the LOL training dataset. For the LOL-test dataset,
    TBEFN [[20](#bib.bib20)] obtains the highest SSIM value while KinD [[11](#bib.bib11)]
    achieves the lowest LPIPS value. There is no winner across these four evaluation
    metrics on the LOL-test dataset despite the fact that some methods were trained
    on the LOL training dataset. For the MIT-Adobe FiveK-test dataset, MBLLEN [[3](#bib.bib3)]
    outperforms all compared methods under the four evaluation metrics in spite of
    being trained on synthetic training data. Nevertheless, MBLLEN [[3](#bib.bib3)]
    still cannot obtain the best performance on both two test datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As presented in Table [VI](#S4.T6 "TABLE VI ‣ 4.3 Benchmarking Results ‣ 4
    Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using
    Deep Learning: A Survey"), both SID [[85](#bib.bib85)] and EEMEFN [[16](#bib.bib16)]
    improve the quality of input raw data. Compared with the quantitative scores of
    SID [[85](#bib.bib85)], EEMEFN [[16](#bib.bib16)] achieves consistently better
    performance across different raw data patterns and evaluation metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For LLIV-Phone-imgT test set, we use the non-reference IQA metrics, i.e., NIQE
    [[88](#bib.bib88)], perceptual index (PI) [[89](#bib.bib89), [90](#bib.bib90),
    [88](#bib.bib88)], LOE [[37](#bib.bib37)], and SPAQ [[91](#bib.bib91)] to quantitatively
    compare different methods. In terms of LOE, the smaller the LOE value is, the
    better the lightness order is preserved. For NIQE, the smaller the NIQE value
    is, the better the visual quality is. A lower PI value indicates better perceptual
    quality. SPAQ is devised for the perceptual quality assessment of smartphone photography.
    A larger SPAQ value suggests better perceptual quality of smartphone photography.
    The quantitative results are provided in Table [VII](#S4.T7 "TABLE VII ‣ 4.3 Benchmarking
    Results ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Quantitative comparisons on LLIV-Phone-imgT dataset in terms of
    NIQE [[88](#bib.bib88)], LOE [[37](#bib.bib37)], PI [[89](#bib.bib89), [90](#bib.bib90),
    [88](#bib.bib88)], and SPAQ [[91](#bib.bib91)]. The best result is in red whereas
    the second and third best results are in blue and purple under each case, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning | Method | LoLi-Phone-imgT |'
  prefs: []
  type: TYPE_TB
- en: '| NIQE$\downarrow$ | LOE $\downarrow$ | PI$\downarrow$ | SPAQ$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | input | 6.99 | 0.00 | 5.86 | 44.45 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLNet [[1](#bib.bib1)] | 5.86 | 5.86 | 5.66 | 40.56 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LightenNet [[5](#bib.bib5)] | 5.34 | 952.33 | 4.58 | 45.74 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Retinex-Net [[4](#bib.bib4)] | 5.01 | 790.21 | 3.48 | 50.95 |'
  prefs: []
  type: TYPE_TB
- en: '| SL | MBLLEN [[3](#bib.bib3)] | 5.08 | 220.63 | 4.27 | 42.50 |'
  prefs: []
  type: TYPE_TB
- en: '|  | KinD [[11](#bib.bib11)] | 4.97 | 405.88 | 4.37 | 44.79 |'
  prefs: []
  type: TYPE_TB
- en: '|  | KinD++ [[61](#bib.bib61)] | 4.73 | 681.97 | 3.99 | 46.89 |'
  prefs: []
  type: TYPE_TB
- en: '|  | TBEFN [[20](#bib.bib20)] | 4.81 | 552.91 | 4.30 | 44.14 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DSLR [[21](#bib.bib21)] | 4.77 | 447.98 | 4.31 | 41.08 |'
  prefs: []
  type: TYPE_TB
- en: '| UL | EnlightenGAN [[26](#bib.bib26)] | 4.79 | 821.87 | 4.19 | 45.48 |'
  prefs: []
  type: TYPE_TB
- en: '| SSL | DRBN [[33](#bib.bib33)] | 5.80 | 885.75 | 5.54 | 42.74 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ExCNet [[27](#bib.bib27)] | 5.55 | 723.56 | 4.38 | 46.74 |'
  prefs: []
  type: TYPE_TB
- en: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 5.82 | 307.09 | 4.76 | 46.85 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RRDNet [[29](#bib.bib29)] | 5.97 | 142.89 | 4.84 | 45.31 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VIII: Quantitative comparisons on LLIV-Phone-vidT dataset in terms of
    average luminance variance (ALV) score. The best result is in red whereas the
    second and third best results are in blue and purple.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning | Method | LoLi-Phone-vidT |'
  prefs: []
  type: TYPE_TB
- en: '| ALV$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | input | 185.60 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLNet [[1](#bib.bib1)] | 85.72 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LightenNet [[5](#bib.bib5)] | 643.93 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Retinex-Net [[4](#bib.bib4)] | 94.05 |'
  prefs: []
  type: TYPE_TB
- en: '| SL | MBLLEN [[3](#bib.bib3)] | 113.18 |'
  prefs: []
  type: TYPE_TB
- en: '|  | KinD [[11](#bib.bib11)] | 98.05 |'
  prefs: []
  type: TYPE_TB
- en: '|  | KinD++ [[61](#bib.bib61)] | 115.21 |'
  prefs: []
  type: TYPE_TB
- en: '|  | TBEFN [[20](#bib.bib20)] | 58.69 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DSLR [[21](#bib.bib21)] | 175.35 |'
  prefs: []
  type: TYPE_TB
- en: '| UL | EnlightenGAN [[26](#bib.bib26)] | 90.69 |'
  prefs: []
  type: TYPE_TB
- en: '| SSL | DRBN [[33](#bib.bib33)] | 115.04 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ExCNet [[27](#bib.bib27)] | 1375.29 |'
  prefs: []
  type: TYPE_TB
- en: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 117.22 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RRDNet [[29](#bib.bib29)] | 147.11 |'
  prefs: []
  type: TYPE_TB
- en: 'Observing Table [VII](#S4.T7 "TABLE VII ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"), we can find that the performance of Retinex-Net [[4](#bib.bib4)],
    KinD++ [[61](#bib.bib61)], and EnlightenGAN [[26](#bib.bib26)] is relatively better
    than the other methods. Retinex-Net [[4](#bib.bib4)] achieves the best PI and
    SPAQ scores. The scores suggest the good perceptual quality of the results enhanced
    by Retinex-Net [[4](#bib.bib4)]. However, from Figure [7](#S4.F7 "Figure 7 ‣ 4.2
    Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey")(d) and Figure [8](#S4.F8
    "Figure 8 ‣ 4.2 Online Evaluation Platform ‣ 4 Benchmarking and Empirical Analysis
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey")(d), the
    results of Retinex-Net [[4](#bib.bib4)] evidently suffer from artifacts and color
    deviations. Moreover, KinD++ [[61](#bib.bib61)] attains the lowest NIQE score
    while the original input achieves the lowest LOE score. For the de-facto standard
    LOE metric, we question if the lightness order can effectively reflect the enhancement
    performance. Overall, the non-reference IQA metrics experience biases on the evaluations
    of the quality of enhanced low-light images in some cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To prepare videos in the LLIV-vidT testing set, we first discard videos without
    obvious objects in consecutive frames. A total of 10 videos are chosen. For each
    video, we select one object that appears in all frames. We then use a tracker
    [[92](#bib.bib92)] to track the object in consecutive frames of the input video
    and ensure the same object appears in the bounding boxes. We discard the frames
    with inaccurate object tracking. The coordinates of the bounding box in each frame
    are collected. We employ these coordinates to crop the corresponding regions in
    the results enhanced by different methods and compute the average luminance variance
    (ALV) scores of the object in the consecutive frames as: $ALV=\frac{1}{N}\sum\limits_{i=1}^{N}(L_{i}-L_{\text{avg}})^{2}$,
    where $N$ is the number of frames of a video, $L_{i}$ represents the average luminance
    value of the region of bounding box in the $i$th frame, and $L_{\text{avg}}$ denotes
    the average luminance value of all bounding box regions in the video. A lower
    ALV value suggests better temporal coherence of the enhanced video. The ALV values
    of different methods averaged over the 10 videos of the LLIV-vidT testing set
    are shown in Table [VIII](#S4.T8 "TABLE VIII ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"). The ALV values of different methods on each video can be found in
    the supplementary material. Besides, we follow Jiang and Zheng [[9](#bib.bib9)]
    to plot their luminance curves in the supplementary material.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [VIII](#S4.T8 "TABLE VIII ‣ 4.3 Benchmarking Results ‣ 4
    Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using
    Deep Learning: A Survey"), TBEFN [[20](#bib.bib20)] obtains the best temporal
    coherence in terms of ALV value whereas LLNet [[1](#bib.bib1)] and EnlightenGAN
    [[26](#bib.bib26)] rank the second and third best, respectively. In contrast,
    the ALV value of ExCNet [[27](#bib.bib27)], as the worst performer, reaches 1375.29\.
    This is because the performance of the zero-reference learning-based ExCNet [[27](#bib.bib27)]
    is unstable for the enhancement of consecutive frames. ExCNet [[27](#bib.bib27)]
    can effectively improve the brightness of some frames while it does not work well
    on other frames.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IX: Quantitative comparisons of computational complexity in terms of
    runtime (in second), number of trainable parameters (#Parameters) (in M), and
    FLOPs (in G). The best result is in red whereas the second and third best results
    are in blue and purple under each case, respectively. ‘-’ indicates the result
    is not available.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning | Method | RunTime$\downarrow$ | #Parameters $\downarrow$ | FLOPs$\downarrow$
    | Platform |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLNet [[1](#bib.bib1)] | 36.270 | 17.908 | 4124.177 | Theano |'
  prefs: []
  type: TYPE_TB
- en: '|  | LightenNet [[5](#bib.bib5)] | - | 0.030 | 30.540 | MATLAB |'
  prefs: []
  type: TYPE_TB
- en: '|  | Retinex-Net [[4](#bib.bib4)] | 0.120 | 0.555 | 587.470 | TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| SL | MBLLEN [[3](#bib.bib3)] | 13.995 | 0.450 | 301.120 | TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '|  | KinD [[11](#bib.bib11)] | 0.148 | 8.160 | 574.954 | TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '|  | KinD++ [[61](#bib.bib61)] | 1.068 | 8.275 | 12238.026 | TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '|  | TBEFN [[20](#bib.bib20)] | 0.050 | 0.486 | 108.532 | TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '|  | DSLR [[21](#bib.bib21)] | 0.074 | 14.931 | 96.683 | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| UL | EnlightenGAN [[26](#bib.bib26)] | 0.008 | 8.637 | 273.240 | PyTorch
    |'
  prefs: []
  type: TYPE_TB
- en: '| SSL | DRBN [[33](#bib.bib33)] | 0.878 | 0.577 | 196.359 | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '|  | ExCNet [[27](#bib.bib27)] | 23.280 | 8.274 | - | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 0.003 | 0.079 | 84.990 | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '|  | RRDNet [[29](#bib.bib29)] | 167.260 | 0.128 | - | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: 4.4 Computational Complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [IX](#S4.T9 "TABLE IX ‣ 4.3 Benchmarking Results ‣ 4 Benchmarking
    and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"), we compare the computational complexity of RGB format-based methods,
    including runtime, trainable parameters, and FLOPs averaged over 32 images of
    size 1200$\times$900$\times$3 using an NVIDIA 1080Ti GPU. We omit LightenNet [[5](#bib.bib5)]
    for fair comparisons because only the CPU version of its code is publicly available.
    Besides, we do not report the FLOPs of ExCNet [[27](#bib.bib27)] and RRDNet [[29](#bib.bib29)]
    as the number depends on the input images (different inputs require different
    numbers of iterations).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As presented in Table [IX](#S4.T9 "TABLE IX ‣ 4.3 Benchmarking Results ‣ 4
    Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement Using
    Deep Learning: A Survey"), Zero-DCE [[28](#bib.bib28)] has the shortest runtime
    because it only estimates several curve parameters via a lightweight network.
    As a result, its number of trainable parameters and FLOPs are much fewer. Moreover,
    the number of trainable parameters and FLOPs of LightenNet [[5](#bib.bib5)] are
    the least among the compared methods. This is because LightenNet [[5](#bib.bib5)]
    estimates the illumination map of input image via a tiny network of four convolutional
    layers. In contrast, the FLOPs of LLNet [[1](#bib.bib1)] and KinD++ [[61](#bib.bib61)]
    are extremely large, reaching 4124.177G and 12238.026G, respectively. The runtime
    of SSL-based ExCNet [[27](#bib.bib27)] and RRDNet [[29](#bib.bib29)] is long due
    to the time-consuming optimization process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6f2327be92968582d74caf6ccf72cf39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The P-R curves of face detection in the dark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE X: Quantitative comparisons of AP under different IoU thresholds of face
    detection in the dark. The best result is in red whereas the second and third
    best results are in blue and purple under each case, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning | Method | IoU thresholds |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 0.6 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | input | 0.195 | 0.061 | 0.007 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LLNet [[1](#bib.bib1)] | 0.208 | 0.063 | 0.006 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LightenNet [[5](#bib.bib5)] | 0.249 | 0.085 | 0.010 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Retinex-Net [[4](#bib.bib4)] | 0.261 | 0.101 | 0.013 |'
  prefs: []
  type: TYPE_TB
- en: '| SL | MBLLEN [[3](#bib.bib3)] | 0.249 | 0.092 | 0.010 |'
  prefs: []
  type: TYPE_TB
- en: '|  | KinD [[11](#bib.bib11)] | 0.235 | 0.081 | 0.010 |'
  prefs: []
  type: TYPE_TB
- en: '|  | KinD++ [[61](#bib.bib61)] | 0.251 | 0.090 | 0.011 |'
  prefs: []
  type: TYPE_TB
- en: '|  | TBEFN [[20](#bib.bib20)] | 0.268 | 0.099 | 0.011 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DSLR [[21](#bib.bib21)] | 0.223 | 0.067 | 0.007 |'
  prefs: []
  type: TYPE_TB
- en: '| UL | EnlightenGAN [[26](#bib.bib26)] | 0.246 | 0.088 | 0.011 |'
  prefs: []
  type: TYPE_TB
- en: '| SSL | DRBN [[33](#bib.bib33)] | 0.199 | 0.061 | 0.007 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ExCNet [[27](#bib.bib27)] | 0.256 | 0.092 | 0.010 |'
  prefs: []
  type: TYPE_TB
- en: '| ZSL | Zero-DCE [[28](#bib.bib28)] | 0.259 | 0.092 | 0.011 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RRDNet [[29](#bib.bib29)] | 0.248 | 0.083 | 0.010 |'
  prefs: []
  type: TYPE_TB
- en: 4.5 Application-Based Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We investigate the performance of low-light image enhancement methods on face
    detection in the dark. Following the setting presented in Guo et al. [[28](#bib.bib28)],
    we use the DARK FACE dataset [[80](#bib.bib80)] that is composed of images with
    faces taken in the dark. Since the bounding boxes of the test set are not publicly
    available, we perform the evaluation on 500 images randomly sampled from the training
    and validation sets. The Dual Shot Face Detector (DSFD) [[93](#bib.bib93)] trained
    on WIDER FACE dataset [[94](#bib.bib94)] is used as the face detector. We feed
    the results of different LLIE methods to the DSFD [[93](#bib.bib93)] and depict
    the precision-recall (P-R) curves under 0.5 IoU threshold in Figure [10](#S4.F10
    "Figure 10 ‣ 4.4 Computational Complexity ‣ 4 Benchmarking and Empirical Analysis
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey"). We compare
    the average precision (AP) under different IoU thresholds using the evaluation
    tool³³3[https://github.com/Ir1d/DARKFACE_eval_tools](https://github.com/Ir1d/DARKFACE_eval_tools)
    provided in DARK FACE dataset [[80](#bib.bib80)] in Table [X](#S4.T10 "TABLE X
    ‣ 4.4 Computational Complexity ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [10](#S4.F10 "Figure 10 ‣ 4.4 Computational Complexity ‣
    4 Benchmarking and Empirical Analysis ‣ Low-Light Image and Video Enhancement
    Using Deep Learning: A Survey"), all the deep learning-based solutions improve
    the performance of face detection in the dark, suggesting the effectiveness of
    deep learning-based LLIE solutions for face detection in the dark. As shown in
    Table [X](#S4.T10 "TABLE X ‣ 4.4 Computational Complexity ‣ 4 Benchmarking and
    Empirical Analysis ‣ Low-Light Image and Video Enhancement Using Deep Learning:
    A Survey"), the AP scores of best performers under different IoU thresholds range
    from 0.268 to 0.013 and the AP scores of input under different IoU thresholds
    are very low. The results suggest that there is still room for improvement. It
    is noteworthy that Retinex-Net [[4](#bib.bib4)], Zero-DCE [[28](#bib.bib28)],
    and TBEFN [[20](#bib.bib20)] achieve relatively robust performance on face detection
    in the dark. We show the visual results of different methods in Figure [11](#S4.F11
    "Figure 11 ‣ 4.6 Discussion ‣ 4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey"). Although Retinex-Net
    [[4](#bib.bib4)] performs better than other methods on the AP score, its visual
    result contains obvious artifacts and unnatural textures. In general, Zero-DCE
    [[28](#bib.bib28)] obtains a good balance between the AP score and the perceptual
    quality for face detection in the dark. Note that the results of face detection
    in the dark are related to not only the enhanced results but also the face detector
    including the detector model and the training data of the detector. Here, we only
    take the pre-trained DSFD [[93](#bib.bib93)] as an example to validate the low-light
    image enhancement performance of different methods to some extent.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From the experimental results, we obtain several interesting observations and
    insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/7850694808f2c3be24cfe312a36ae7b2.png) | ![Refer to
    caption](img/6c28abfaa92d25d66ece7297517f8431.png) | ![Refer to caption](img/ce0770b99deb0cd18da04633145bde58.png)
    | ![Refer to caption](img/c6224de9e140d1184d0fc645bc5483c5.png) |'
  prefs: []
  type: TYPE_TB
- en: '| (a) input | (b) LightenNet [[5](#bib.bib5)] | (c) Retinex-Net [[4](#bib.bib4)]
    | (d) MBLLEN [[3](#bib.bib3)] |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/3394b8aa666c935488fbd44890e9e228.png) | ![Refer to
    caption](img/1701ffa1e14d2e5017a8a8a2f08b9962.png) | ![Refer to caption](img/4fa7144834439baab46e3c821b21581a.png)
    | ![Refer to caption](img/f9fba7cead24e131bf299c97ea565e64.png) |'
  prefs: []
  type: TYPE_TB
- en: '| (e) KinD++ [[61](#bib.bib61)] | (f) TBEFN [[20](#bib.bib20)] | (g) DSLR [[21](#bib.bib21)]
    | (h) EnlightenGAN [[26](#bib.bib26)] |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/3c9ca7f03571bc9152e4011411a8a432.png) | ![Refer to
    caption](img/1eaede4ac09756e4cfa19fd2b06f9adf.png) | ![Refer to caption](img/973bafbb43b5346c807f7fbbca2cbceb.png)
    | ![Refer to caption](img/4b41b4b90028da2fedd0de9ae536e205.png) |'
  prefs: []
  type: TYPE_TB
- en: '| (i) DRBN [[33](#bib.bib33)] | (j) ExCNet [[27](#bib.bib27)] | (k) Zero-DCE
    [[28](#bib.bib28)] | (l) RRDNet [[29](#bib.bib29)] |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 11: Visual results of different methods on a low-light image sampled
    from DARK FACE dataset. Better see with zoom in for the bounding boxes of faces.'
  prefs: []
  type: TYPE_NORMAL
- en: 1) The performance of different methods significantly varies based on the test
    datasets and evaluation metrics. In terms of the full-reference IQA metrics on
    commonly used test datasets, MBLLEN [[3](#bib.bib3)], KinD++ [[61](#bib.bib61)],
    and DSLR [[21](#bib.bib21)] are generally better than other compared methods.
    For real-world low-light images taken by mobile phones, supervised learning-based
    Retinex-Net [[4](#bib.bib4)] and KinD++ [[61](#bib.bib61)] obtain better scores
    measured in the non-reference IQA metrics. For real-world low-light videos taken
    by mobile phones, TBEFN [[20](#bib.bib20)] preserves the temporal coherence better.
    When coming to the computational efficiency, LightenNet [[5](#bib.bib5)] and Zero-DCE
    [[28](#bib.bib28)] are outstanding. From the aspect of face detection in the dark,
    TBEFN [[20](#bib.bib20)], Retinex-Net [[4](#bib.bib4)], and Zero-DCE [[28](#bib.bib28)]
    rank the first three. No method always wins. Overall, Retinex-Net [[4](#bib.bib4)],
    [[20](#bib.bib20)], Zero-DCE [[28](#bib.bib28)], and DSLR [[21](#bib.bib21)] are
    better choice in most cases. For raw data, EEMEFN [[16](#bib.bib16)] obtains relatively
    better qualitative and quantitative performance than SID [[85](#bib.bib85)]. However,
    from the visual results, EEMEFN [[16](#bib.bib16)] and [[85](#bib.bib85)] cannot
    recover the color well when compared with the corresponding ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: 2) LLIV-Phone dataset fails most methods. The generalization capability of existing
    methods needs further improvements. It is worth noting that it is inadequate to
    use only the average luminance variance to evaluate the performance of different
    methods for low-light video enhancement. More effective and comprehensive assessment
    metrics would guide the development of low-light video enhancement towards the
    right track.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Regarding learning strategies, supervised learning achieves better performance
    in most cases, but requires high computational resources and paired training data.
    In comparison, zero-shot learning is more appealing in practical applications
    because it does not require paired or unpaired training data. Consequently, zero-shot
    learning-based methods enjoy better generalization capability. However, their
    quantitative performance is inferior to other methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4) There is a gap between visual results and quantitative IQA scores. In other
    words, a good visual appearance does not always yield a good IQA score. The relationships
    between human perception and IQA scores are worth more investigation. Pursuing
    better visual perception or quantitative scores depends on specific applications.
    For instance, to show the results to observers, more attention should be paid
    to visual perception. In contrast, accuracy is more important when LLIE methods
    are applied to face detection in the dark. Thus, more comprehensive comparisons
    should be performed when comparing different methods.
  prefs: []
  type: TYPE_NORMAL
- en: 5) Deep learning-based LLIE methods are beneficial to face detection in the
    dark. Such results further support the significance of enhancing low-light images
    and videos. However, in comparison to the high accuracy of face detection in normal-light
    images, the accuracy of face detection in the dark is extremely low, despite using
    LLIE methods.
  prefs: []
  type: TYPE_NORMAL
- en: 6) In comparison to RGB format-based LLIE methods, raw format-based LLIE methods
    usually recover details better, obtain more vivid color, and reduce noises and
    artifacts more effectively. This is because raw data contain more information
    such as wider color gamut and higher dynamic range. However, raw format-based
    LLIE methods are limited to specific sensors and formats such as the Bayer pattern
    of the Sony camera and the APS-C X-Trans pattern of the Fuji camera. In contrast,
    RGB format-based LLIE methods are more convenient and versatile since RGB images
    are commonly found as the final imagery form produced by mobile devices. However,
    RGB format-based LLIE methods cannot cope well with cases that exhibit low light
    and excessive noise.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Open Issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we summarize the open issues in low-light image and video enhancement
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Generalization Capability. Although existing methods can produce some visually
    pleasing results, they have limited generalization capability. For example, a
    method trained on MIT-Adobe FiveK dataset [[79](#bib.bib79)] cannot effectively
    enhance the low-light images of LOL dataset [[4](#bib.bib4)]. Albeit synthetic
    data are used to augment the diversity of training data, the models trained on
    the combination of real and synthetic data cannot solve this issue well. Improving
    the generalization capability of LLIE methods is an unsolved open issue.
  prefs: []
  type: TYPE_NORMAL
- en: Removing Unknown Noises. Observing the results of existing methods on the low-light
    images captured by different types of phones’ cameras, we can find that these
    methods cannot remove the noises well and even amplify the noises, especially
    when the types of noises are unknown. Despite some methods add Gaussian and/or
    Poisson noises in their training data, the noise types are different from real
    noises, thus the performance of these methods is unsatisfactory in real scenarios.
    Removing unknown noises is still unsolved.
  prefs: []
  type: TYPE_NORMAL
- en: Removing Unknown Artifacts. One may enhance a low-light image downloaded from
    the Internet. The image may have gone through a serial of degradations such as
    JPEG compression or editing. Thus, the image may contain unknown artifacts. Suppressing
    unknown artifacts still challenges existing low-light image and video enhancement
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Correcting Uneven Illumination. Images taken in real scenes usually exhibit
    uneven illumination. For example, an image captured at night has both dark regions
    and normal-light or over-exposed regions such as the regions of light sources.
    Existing methods tend to brighten both the dark regions and the light source regions,
    affecting the visual quality of the enhanced result. It is expected to enhance
    dark regions but suppress over-exposed regions. However, this open issue is not
    solved well in existing LLIE methods.
  prefs: []
  type: TYPE_NORMAL
- en: Distinguishing Semantic Regions. Existing methods tend to enhance a low-light
    image without considering the semantic information of its different regions. For
    example, the black hair of a man in a low-light image is enhanced to be off-white
    as the black hair is treated as the low-light regions. An ideal enhancement method
    is expected to only enhance the low-light regions induced by external environments.
    How to distinguish semantic regions is an open issue.
  prefs: []
  type: TYPE_NORMAL
- en: Using Neighbouring Frames. Despite some methods that have been proposed to enhance
    low-light videos, they commonly process a video frame-by-frame. How to make full
    use of the neighboring frames to improve the enhancement performance and speed
    up the processing speed is an unsolved open issue. For example, the well-lit regions
    of neighboring frames are used to enhance the current frame. For another example,
    the estimated parameters for processing neighboring frames can be reused to enhance
    the current frame for reducing the time of parameter estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Future Research Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Low-light enhancement is a challenging research topic. As can be observed from
    the experiments presented in Section [4](#S4 "4 Benchmarking and Empirical Analysis
    ‣ Low-Light Image and Video Enhancement Using Deep Learning: A Survey") and the
    unsolved open issues in Section [5](#S5 "5 Open Issues ‣ Low-Light Image and Video
    Enhancement Using Deep Learning: A Survey"), there is still room for improvement.
    We suggest potential future research directions as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: Effective Learning Strategies. As aforementioned, current LLIE models mainly
    adopt supervised learning that requires massive paired training data and may overfit
    on a specific dataset. Although some researchers attempted to introduce unsupervised
    learning into LLIE, the inherent relationships between LLIE and these learning
    strategies are not clear and their effectiveness in LLIE needs further improvements.
    Zero-shot learning has shown robust performance for real scenes while not requiring
    paired training data. The unique advantage suggests zero-shot learning as a potential
    research direction, especially on the formulation of zero-reference losses, deep
    priors, and optimization strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized Network Structures. A network structure can significantly affect
    enhancement performance. As previously analyzed, most LLIE deep models employ
    U-Net or U-Net-like structures. Though they have achieved promising performance
    in some cases, the investigation if such an encoder-decoder network structure
    is most suitable for the LLIE task is still lacking. Some network structures require
    a high memory footprint and long inference time due to their large parameter space.
    Such network structures are unacceptable for practical applications. Thus, it
    is worthwhile to investigate a more effective network structure for LLIE, considering
    the characteristics of low-light images such as non-uniform illumination, small
    pixel values, noise suppression, and color constancy. One can also design more
    efficient network structures via taking into account the local similarity of low-light
    images or considering more efficient operations such as depthwise separable convolution
    layer [[95](#bib.bib95)] and self-calibrated convolution [[96](#bib.bib96)]. Neural
    architecture search (NAS) technique [[97](#bib.bib97), [98](#bib.bib98)] may be
    considered to obtain more effective and efficient LLIE network structures. Adapting
    the transformer architecture [[99](#bib.bib99), [100](#bib.bib100)] into LLIE
    may be a potential and interesting research direction.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Functions. Loss functions constrain the relationships between an input
    image and ground truth and drive the optimization of deep networks. In LLIE, the
    commonly used loss functions are borrowed from related vision tasks. Thus, designing
    loss functions that are more well-suited for LLIE is desired. Recent studies have
    shown the possibility of using deep neural networks to approximate human visual
    perception of image quality [[101](#bib.bib101), [102](#bib.bib102)]. These ideas
    and fundamental theories could be used to guide the designs of loss functions
    for low-light enhancement networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Realistic Training Data. Although there are several training datasets for LLIE,
    their authenticity, scales, and diversities fall behind real low-light conditions.
    Thus, as shown in Section [4](#S4 "4 Benchmarking and Empirical Analysis ‣ Low-Light
    Image and Video Enhancement Using Deep Learning: A Survey"), current LLIE deep
    models cannot achieve satisfactory performance when encountering low-light images
    captured in real-world scenes. More efforts are needed to study the collection
    of large-scale and diverse real-world paired LLIE training datasets or to generate
    more realistic synthetic data.'
  prefs: []
  type: TYPE_NORMAL
- en: Standard Test Data. Currently, there is no well-accepted LLIE evaluation benchmark.
    Researchers prefer selecting their test data that may bias to their proposed methods.
    Despite some researchers leave some paired data as test data, the division of
    training and test partitions are mostly ad-hoc across the literature. Consequently,
    conducting a fair comparison among different methods is often laborious if not
    impossible. Besides, some test data are either easy to be handled or not originally
    collected for low-light enhancement. It is desired to have a standard low-light
    image and video test dataset, which includes a large number of test samples with
    the corresponding ground truths, covering diverse scenes and challenging illumination
    conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Task-Specific Evaluation Metrics. The commonly adopted evaluation metrics in
    LLIE can reflect the image quality to some extent. However, how to measure how
    good a result is enhanced by an LLIE method still challenges current IQA metrics,
    especially for non-reference measurements. The current IQA metrics either focus
    on human visual perceptual such as subjective quality or emphasize machine perceptual
    such as the effects on high-level visual tasks. Therefore, more works are expected
    in this research direction to make efforts on designing more accurate and task-specific
    evaluation metrics for LLIE.
  prefs: []
  type: TYPE_NORMAL
- en: Robust Generalization Capability. Observing the experimental results on real-world
    test data, most methods fail due to their limited generalization capability. The
    poor generalization is caused by several factors such as synthetic training data,
    small-scaled training data, ineffective network structures, or unrealistic assumptions.
    It is important to explore ways to improve the generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Extension to Low-Light Video Enhancement. Unlike the rapid development of video
    enhancement in other low-level vision tasks such as video deblurring [[103](#bib.bib103)],
    video denoising [[104](#bib.bib104)], and video super-resolution [[105](#bib.bib105)],
    low-light video enhancement receives less attention. A direct application of existing
    LLIE methods to videos often leads to unsatisfactory results and flickering artifacts.
    More efforts are needed to remove visual flickering effectively, exploit the temporal
    information between neighboring frames, and speed up the enhancement speed.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Semantic Information. Semantic information is crucial for low-light
    enhancement. It guides the networks to distinguish different regions in the process
    of enhancement. A network without access to semantic priors can easily deviate
    the original color of a region, e.g., turning black hair to gray color after enhancement.
    Therefore, integrating semantic priors into LLIE models is a promising research
    direction. Similar work has been done on image super-resolution [[106](#bib.bib106),
    [107](#bib.bib107)] and face restoration [[108](#bib.bib108)].
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study is supported under the RIE2020 Industry Alignment Fund Industry Collaboration
    Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution
    from the industry partner(s). It is also partially supported by the NTU SUG and
    NAP grant. Chunle Guo is sponsored by CAAI-Huawei MindSpore Open Fund.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] K. G. Lore, A. Akintayo, and S. Sarkar, “LLNet: A deep autoencoder approach
    to natural low-light image enhancement,” *PR*, vol. 61, pp. 650–662, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] C. Chen, Q. Chen, J. Xu, and V. Koltun, “Learning to see in the dark,”
    in *CVPR*, 2018, pp. 3291–3300.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] F. Lv, F. Lu, J. Wu, and C. Lim, “MBLLEN: Low-light image/video enhancement
    using cnns,” in *BMVC*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] C. Wei, W. Wang, W. Yang, and J. Liu, “Deep retinex decomposition for low-light
    enhancement,” in *BMVC*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] C. Li, J. Guo, F. Porikli, and Y. Pang, “LightenNet: A convolutional neural
    network for weakly illuminated image enhancement,” *PRL*, vol. 104, pp. 15–22,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] J. Cai, S. Gu, and L. Zhang, “Learning a deep single image contrast enhancer
    from multi-exposure images,” *TIP*, vol. 27, no. 4, pp. 2049–2062, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] R. Wang, Q. Zhang, C.-W. Fu, X. Shen, W.-S. Zheng, and J. Jia, “Underexposed
    photo enhancement usign deep illumination estimation,” in *CVPR*, 2019, pp. 6849–6857.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] C. Chen, Q. Chen, M. N. Do, and V. Koltun, “Seeing motion in the dark,”
    in *ICCV*, 2019, pp. 3185–3194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] H. Jiang and Y. Zheng, “Learning to see moving object in the dark,” in
    *ICCV*, 2019, pp. 7324–7333.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Wang, Y. Cao, Z. Zha, J. Zhang, Z. Xiong, W. Zhang, and F. Wu, “Progressive
    retinex: Mutually reinforced illumination-noise perception network for low-light
    image enhancement,” in *ACMMM*, 2019, pp. 2015–2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y. Zhang, J. Zhang, and X. Guo, “Kindling the darkness: A practical low-light
    image enhancer,” in *ACMMM*, 2019, pp. 1632–1640.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] W. Ren, S. Liu, L. Ma, Q. Xu, X. Xu, X. Cao, J. Du, and M.-H. Yang, “Low-light
    image enhancement via a deep hybrid network,” *TIP*, vol. 28, no. 9, pp. 4364–4375,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] K. Xu, X. Yang, B. Yin, and R. W. H. Lau, “Learning to restore low-light
    images via decomposition-and-enhancement,” in *CVPR*, 2020, pp. 2281–2290.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] M. Fan, W. Wang, W. Yang, and J. Liu, “Integrating semantic segmentation
    and retinex model for low light image enhancement,” in *ACMMM*, 2020, pp. 2317–2325.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] F. Lv, B. Liu, and F. Lu, “Fast enhancement for non-uniform illumination
    images using light-weight cnns,” in *ACMMM*, 2020, pp. 1450–1458.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Zhu, P. Pan, W. Chen, and Y. Yang, “EEMEFN: Low-light image enhancement
    via edge-enhanced multi-exposure fusion network,” in *AAAI*, 2020, pp. 13 106–13 113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] D. Triantafyllidou, S. Moran, S. McDonagh, S. Parisot, and G. Slabaugh,
    “Low light video enhancement using synthetic data produced with an intermediate
    domain mapping,” in *ECCV*, 2020, pp. 103–119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] J. Li, J. Li, F. Fang, F. Li, and G. Zhang, “Luminance-aware pyramid network
    for low-light image enhancement,” *TMM*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] L. Wang, Z. Liu, W. Siu, and D. P. K. Lun, “Lightening network for low-light
    image enhancement,” *TIP*, vol. 29, pp. 7984–7996, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] K. Lu and L. Zhang, “TBEFN: A two-branch exposure-fusion network for low-light
    image enhancement,” *TMM*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] S. Lim and W. Kim, “DSLR: Deep stacked laplacian restorer for low-light
    image enhancement,” *TMM*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] F. Zhang, Y. Li, S. You, and Y. Fu, “Learning temporal consistency for
    low light video enhancement from single images,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. Li, X. Feng, and Z. Hua, “Low-light image enhancement via progressive-recursive
    network,” *TCSVT*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] W. Yang, W. Wang, H. Huang, S. Wang, and J. Liu, “Sparse gradient regularized
    deep retinex network for robust low-light image enhancement,” *TIP*, vol. 30,
    pp. 2072–2086, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] R. Yu, W. Liu, Y. Zhang, Z. Qu, D. Zhao, and B. Zhang, “DeepExposure:
    Learning to expose photos with asynchronously reinforced adversarial learning,”
    in *NeurIPS*, 2018, pp. 2149–2159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Y. Jiang, X. Gong, D. Liu, Y. Cheng, C. Fang, X. Shen, J. Yang, P. Zhou,
    and Z. Wang, “EnlightenGAN: Deep light enhancement without paired supervision,”
    *TIP*, vol. 30, pp. 2340–2349, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] L. Zhang, L. Zhang, X. Liu, Y. Shen, S. Zhang, and S. Zhao, “Zero-shot
    restoration of back-lit images using deep internal learning,” in *ACMMM*, 2019,
    pp. 1623–1631.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] C. Guo, C. Li, J. Guo, C. C. Loy, J. Hou, S. Kwong, and R. Cong, “Zero-reference
    deep curve estimation for low-light image enhancement,” in *CVPR*, 2020, pp. 1780–1789.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] A. Zhu, L. Zhang, Y. Shen, Y. Ma, S. Zhao, and Y. Zhou, “Zero-shot restoration
    of underexposed images via robust retinex decomposition,” in *ICME*, 2020, pp.
    1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] C. Li, C. Guo, and C. C. Loy, “Learning to enhance low-light image via
    zero-reference deep curve estimation,” *TPAMI*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Z. Zhao, B. Xiong, L. Wang, Q. Ou, L. Yu, and F. Kuang, “Retinexdip: A
    unified deep framework for low-light image enhancement,” *TCSVT*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] R. Liu, L. Ma, J. Zhang, X. Fan, and Z. Luo, “Retinex-inspired unrolling
    with cooperative prior architecture search for low-light image enhancement,” in
    *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] W. Yang, S. Wang, Y. Fang, Y. Wang, and J. Liu, “From fidelity to perceptual
    quality: A semi-supervised approach for low-light image enhancement,” in *CVPR*,
    2020, pp. 3063–3072.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] W. Yang, S. Wang, Y. F. nd Yue Wang, and J. Liu, “Band representation-based
    semi-supervised low-light image enhancement: Bridging the gap between signal fidelity
    and perceptual quality,” *TIP*, vol. 30, pp. 3461–3473, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] H. Ibrahim and N. S. P. Kong, “Brightness preserving dynamic histogram
    equalization for image contrast enhancement,” *TCE*, vol. 53, no. 4, pp. 1752–1758,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] M. Abdullah-AI-Wadud, M. H. Kabir, M. A. A. Dewan, and O. Chae, “A dynamic
    histogram equalization for image contrast enhancement,” *TCE*, vol. 53, no. 2,
    pp. 593–600, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Wang, J. Zheng, H. Hu, and B. Li, “Naturalness preserved enhancement
    algorithm for non-uniform illumination images,” *TIP*, vol. 22, no. 9, pp. 3538–3548,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X. Fu, Y. Liao, D. Zeng, Y. Huang, X. Zhang, and X. Ding, “A probabilistic
    method for image enhancement with simultaneous illumination and reflectance estimation,”
    *TIP*, vol. 24, no. 12, pp. 4965–4977, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] X. Guo, Y. Li, and H. Ling, “LIME: Low-light image enhancement via illumination
    map estimation,” *TIP*, vol. 26, no. 2, pp. 982–993, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S. Park, S. Yu, B. Moon, S. Ko, and J. Paik, “Low-light image enhancement
    using variational optimization-based retinex model,” *TCE*, vol. 63, no. 2, pp.
    178–184, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] M. Li, J. Liu, W. Yang, X. Sun, and Z. Guo, “Structure-revealing low-light
    image enhancement via robust retinex model,” *TIP*, vol. 27, no. 6, pp. 2828–2841,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Z. Gu, F. Li, F. Fang, and G. Zhang, “A novel retinex-based fractional-order
    variational model for images with severely low light,” *TIP*, vol. 29, pp. 3239–3253,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] X. Ren, W. Yang, W.-H. Cheng, and J. Liu, “LR3M: Robust low-light enhancement
    via low-rank regularized retinex model,” *TIP*, vol. 29, pp. 5862–5876, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. Hao, X. Han, Y. Guo, X. Xu, and M. Wang, “Low-light image enhancement
    with semi-decoupled decomposition,” *TMM*, vol. 22, no. 12, pp. 3025–3038, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Y. Hu, H. He, C. Xu, B. Wang, and S. Lin, “Exposure: A white-box photo
    post-processing framework,” *ACM Graph.*, vol. 37, no. 2, pp. 1–17, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] M. Gharbi, J. Chen, J. T. Barron, S. W. Hasinoff, and F. Durand, “Deep
    bilateral learning for real-time image enhancement,” *ACM Graph.*, vol. 36, no. 4,
    pp. 1–12, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Y. Chen, Y. Wang, M. Kao, and Y. Chuang, “Deep photo enhancer: Unpaired
    learning for image enhancement form photographs wigh gans,” in *CVPR*, 2018, pp.
    6306–6314.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Y. Deng, C. C. Loy, and X. Tang, “Aesthetic-driven image enhancement by
    adversarial learning,” in *ACMMM*, 2018, pp. 870–878.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Z. Yan, H. Zhang, B. Wang, S. Paris, and Y. Yu, “Automatic photo adjustment
    using deep neural networks,” *ACM Graph.*, vol. 35, no. 2, pp. 1–15, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Q. Chen, J. Xu, and V. Koltun, “Fast image processing with fully-convolutional
    networks,” in *CVPR*, 2017, pp. 2497–2506.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Z. Ni, W. Yang, S. Wang, L. Ma, and S. Kwong, “Towards unsupervised deep
    image enhancement with generative adversarial network,” *TIP*, vol. 29, pp. 9140–9151,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] H. Zeng, J. Cai, L. Li, Z. Cao, and L. Zhang, “Learning image-adaptive
    3D lookup tables for high performance photo enhancement in real-time,” *TPAMI*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] C. Li, C. Guo, Q. Ai, S. Zhou, and C. C. Loy, “Flexible piecewise curves
    estimation for photo enhancement,” *arXiv preprint arXiv:2010.13412*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] W. Wang, X. Wu, X. Yuan, and Z. Gao, “An experiment-based review of low-light
    image enhancement methods,” *IEEE Access*, vol. 8, pp. 87 884–87 917, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] J. Liu, D. Xu, W. Yang, M. Fan, and H. Huang, “Benchmarking low-light
    image enhancement and beyond,” *IJCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] V. Jain and S. Seung, “Natural image denoising with convolutional networks,”
    in *NeurIPS*, 2008, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] K. Xu, X. Yang, B. Yin, and R. W. H. Lau, “Learning to restore low-light
    images via decomposition-and-enhancement,” in *CVPR*, 2020, pp. 2281–2290.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] E. H. Land, “An alternative technique for the computation of the designator
    in the retinex theory of color vision,” *National Academy of Sciences*, vol. 83,
    no. 10, pp. 3078–3080, 1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] D. J. Jobson, Z. ur Rahman, and G. A. Woodell, “Properties and performance
    of a center/surround retinex,” *TIP*, vol. 6, no. 3, pp. 451–462, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] R. Wang, Q. Zhang, C.-W. Fu, X. Shen, W.-S. Zheng, and J. Jia, “Underexposed
    photo enhancement using deep illumination estimation,” in *CVPR*, 2019, pp. 6849–6857.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] X. Guo, Y. Zhang, J. Ma, W. Liu, and J. Zhang, “Beyond brightening low-light
    images,” *IJCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional networks
    for biomedical image segmentation,” in *MICCAI*, 2015, pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] T. Xue, B. Chen, J. Wu, D. Wei, and W. T. Freeman, “Video enhancement
    with task-oriented flow,” *IJCV*, vol. 127, no. 8, pp. 1106–1125, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] K. He, J. Sun, and X. Tang, “Guided image filtering,” *TPAMI*, vol. 35,
    no. 6, pp. 1397–1409, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] P. Whittle, “The psychophysics of contrast brightness,” *A. L. Gilchrist
    (Ed.), Brightness, lightness, and transparenc (1994),*, pp. 35–110, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Deep image prior,” in *CVPR*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for image
    restoration with neural networks,” *TIP*, vol. 3, no. 1, pp. 47–56, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
    A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, “Photo-realistic single image
    super-resolution using a generative adversarial network,” in *CVPR*, 2017, pp.
    4681–4690.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] K. Simoayan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei, “ImageNet:
    A large-scale hierarchical image database,” in *CVPR*, 2009, pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
    A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, “Photo-realistic single image
    super-resolution using a generative adversarial network,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, and C. C. Loy,
    “ESRGAN: Enhanced super-resolution generative adversarial networks,” in *ECCVW*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution using
    deep convolutional networks,” *TPAMI*, vol. 38, no. 2, pp. 295–307, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Q. Xu, C. Zhang, and L. Zhang, “Denoising convolutional neural network,”
    in *ICIA*, 2015, pp. 1184–1187.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] X. Fu, J. Huang, D. Zeng, Y. Huang, X. Ding, and J. Paisley, “Removing
    rain from single images via a deep detail network,” in *CVPR*, 2017, pp. 3855–3863.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, “Deep joint rain
    detection and removal from a single image,” in *CVPR*, 2017, pp. 1357–1366.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] X. Fu, Q. Qi, Z. Zha, X. Ding, F. Wu, and J. Paisley, “Successive graph
    convolutional network for image deraining,” *IJCV*, vol. 129, no. 5, pp. 1691–1711,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] J. Sun, W. Cao, Z. Xu, and J. Ponce, “Learning a convolutional neural
    network for non-uniform motion blur removal,” in *CVPR*, 2015, pp. 769–777.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] V. Bychkovsky, S. Paris, E. Chan, and F. Durand, “Learning photographic
    global tonal adjustment with a database of input/output image pairs,” in *CVPR*,
    2011, pp. 97–104.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Y. Yuan, W. Yang, W. Ren, J. Liu, W. JScheirer, and W. Zhangyang, “UG+
    Track 2: A collective benchmark effort for evaluating and advancing image understanding
    in poor visibility environments,” *arXiv arXiv:1904.04474*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. P. Loh and C. S. Chan, “Getting to know low-light images with the exclusively
    dark dataset,” *CVIU*, vol. 178, pp. 30–42, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] L. Chulwoo, L. Chul, L. Young-Yoon, and K. Chang-su, “Power-constrained
    contrast enhancement for emissive displays based on histogram equalization,” *TIP*,
    vol. 21, no. 1, pp. 80–93, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] C. Lee, C. Lee, and C.-S. Kim, “Contrast enhancement based on layered
    difference representation of 2d histograms,” *TIP*, vol. 22, no. 12, pp. 5372–5384,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell,
    “BDD100K: A diverse driving video database with scalable annotation tooling,”
    *arXiv preprint arXiv:1805.04687*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] C. Chen, Q. Chen, J. Xu, and V. Koltun, “Learning to see in the dark,”
    in *CVPR*, 2018, pp. 3291–3300.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: From error visibility to structural similarity,” *TIP*, vol. 13, no. 4,
    pp. 600–612, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *CVPR*, 2018, pp. 586–595.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a completely blind
    image quality analyzer,” *SPL*, vol. 20, no. 3, pp. 209–212, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. Blau and T. Michaeli, “The perception-distortion tradeoff,” in *CVPR*,
    2018, pp. 6228–6237.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] C. Ma, C.-Y. Yang, X. Yang, and M.-H. Yang, “Learning a non-reference
    quality metric for single-image super-resolution,” *CVIU*, vol. 158, pp. 1–16,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Fang, H. Zhu, Y. Zeng, K. Ma, and Z. Wang, “Perceptual quality assessment
    of smartphone photography,” in *ICCV*, 2020, pp. 3677–3686.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] M. Danelljan, G. Bhat, F. S. Khan, and M. Felsberg, “ECO: Efficient convolution
    operators for tracking,” in *CVPR*, 2017, pp. 6638–6646.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Li, Y. Wang, C. Wang, Y. Tai, J. Qian, J. Yang, C. Wang, J. Li, and
    F. Huang, “DSFD: Dual shot face detector,” in *CVPR*, 2019, pp. 5060–5069.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] S. Yang, P. Luo, C. C. Loy, and X. Tang, “Wider Face: A face detection
    benchmark,” in *CVPR*, 2016, pp. 5525–5533.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “MobileNets: Efficient convolutional neural networks for mobile vision
    application,” *arXiv preprint arXiv:1704.04861*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. Liu, Q. Hou, M. M. Cheng, C. Wang, and J. Feng, “Improving convolutional
    networks with self-calibrated convolutions,” in *CVPR*, 2020, pp. 10 096–10 105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L. Li, L. F. Fei, A. Yuille,
    J. Huang, and K. Murphy, “Progressive neural architecture search,” in *ECCV*,
    2018, pp. 19–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] C. Liu, L. C. Chen, F. Schroff, H. Adam, W. Hua, A. Yuille, and L. F.
    Fei, “Auto-Deeplab: Hierarchical neural architecture search for semantic image
    segmentation,” in *CVPR*, 2019, pp. 82–92.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. D. M. Minderer, G. Heigold, S. Gelly, J. Uszekoreit, and N. Houlsby, “An image
    is worth 16x16 words: Transformers for image recognition at scale,” *arXiv preprint
    arXiv:2010.11929*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu,
    and W. Gao, “Pre-trained image processing transformer,” *arXiv preprint arXiv:2012.00364*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Fang, H. Zhu, Y. Zeng, K. Ma, and Z. Wang, “Perceptual quality assessment
    of smartphone phtography,” in *CVPR*, 2020, pp. 3677–3686.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] H. Talebi and P. Milanfar, “NIMA: Neural image assessment,” *TIP*, vol. 27,
    no. 8, pp. 3998–4011, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] T. H. Kim, K. M. Lee, B. Scholkopf, and M. Hirsch, “Online video deblurring
    via dynamic temporal blending network,” in *ICCV*, 2017, pp. 4038–4047.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] T. Ehret, A. Davy, J.-M. Morel, G. Facciolo, and P. Arias, “Model-blind
    video denoising via frame-to-frame training,” in *CVPR*, 2019, pp. 11 369–11 378.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] K. C. K. Chan, X. Wang, K. Yu, C. Dong, and C. C. Loy, “BasicVSR: The
    search for essential components in video super-resolution and beyond,” in *CVPR*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] X. Wang, K. Yu, C. Dong, and C. C. Loy, “Recovering realistic texture
    in image super-resolution by deep spatial feature transform,” in *CVPR*, 2018,
    pp. 606–615.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] K. C. K. Chan, X. Wang, X. Xu, J. Gu, and C. C. Loy, “GLEAN: Generative
    latent bank for large-factor image super-resolution,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] X. Li, C. Chen, S. Zhou, X. Lin, W. Zuo, and L. Zhang, “Blind face restoration
    via deep multi-scale component dictionaries,” in *ECCV*, 2020, pp. 399–415.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/0459bb4441d0e984facf544269493f84.png) | Chongyi
    Li is a Research Assistant Professor with the School of Computer Science and Engineering,
    Nanyang Technological University, Singapore. He received the Ph.D. degree from
    Tianjin University, China in 2018\. From 2016 to 2017, he was a joint-training
    Ph.D. Student with Australian National University, Australia. Prior to joining
    NTU, he was a postdoctoral fellow with City University of Hong Kong and Nanyang
    Technological University from 2018 to 2021\. His current research focuses on image
    processing, computer vision, and deep learning, particularly in the domains of
    image restoration and enhancement. He serves as an associate editor of the Journal
    of Signal, Image and Video Processing and a lead guest editor of the IEEE Journal
    of Oceanic Engineering. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/48273b9fa9f26bbc51966676caeea411.png) | Chunle
    Guo received his PhD degree from Tianjin University in China under the supervision
    of Prof. Jichang Guo. He conducted the Ph.D. research as a Visiting Student with
    the School of Electronic Engineering and Computer Science, Queen Mary University
    of London (QMUL), UK. He continued his research as a Research Associate with the
    Department of Computer Science, City University of Hong Kong (CityU), from 2018
    to 2019\. Now he is a postdoc research fellow working with Prof. Ming-Ming Cheng
    at Nankai University. His research interests lie in image processing, computer
    vision, and deep learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/800006ebed0e2ebe55ab7451995a1fa9.png) | Linhao
    Han is currently a master student at the College of Computer Science, Nankai University,
    under the supervision of Prof. Ming-Ming Cheng. His research interests include
    deep learning and computer vision. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/b7fe11913bc18b8b30e91178edb61e1e.png) | Jun Jiang
    received the PhD degree in Color Science from Rochester Institute of Technology
    in 2013\. He is a Senior Researcher in SenseBrain focusing on algorithm development
    to improve image quality on smartphone cameras. His research interest includes
    computational photography, low-level computer vision, and deep learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/81ebc4192603be9885c0cf76b7de7edc.png) | Ming-Ming
    Cheng (Senior Member, IEEE) received the Ph.D. degree from Tsinghua University
    in 2012\. Then he did two years research fellowship with Prof. Philip Torr at
    Oxford. He is currently a Professor at Nankai University and leading the Media
    Computing Laboratory. His research interests include computer graphics, computer
    vision, and image processing. He received research awards, including the ACM China
    Rising Star Award, the IBM Global SUR Award, and the CCF-Intel Young Faculty Researcher
    Program. He is on the Editorial Board Member of IEEE Transactions on Image Processing
    (TIP). |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/4c0832ab6f7b3282a476ae918cc382a9.png) | Jinwei
    Gu (Senior Member, IEEE) is the R&D Executive Director of SenseTime USA. His current
    research focuses on low-level computer vision, computational photography, smart
    visual sensing and perception, and robotics. He obtained his Ph.D. degree in 2010
    from Columbia University, and his B.S and M.S. from Tsinghua University, in 2002
    and 2005 respectively. Before joining SenseTime, he was a senior research scientist
    in NVIDIA Research from 2015 to 2018\. Prior to that, he was an assistant professor
    in Rochester Institute of Technology from 2010 to 2013, and a senior researcher
    in the media lab of Futurewei Technologies from 2013 to 2015\. He is an associate
    editor for IEEE Transactions on Computational Imaging and an IEEE senior member
    since 2018. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/c31cca88521d432e787ceed6c3d63b80.png) | Chen Change
    Loy (Senior Member, IEEE) is an Associate Professor with the School of Computer
    Science and Engineering, Nanyang Technological University, Singapore. He is also
    an Adjunct Associate Professor at The Chinese University of Hong Kong. He received
    his Ph.D. (2010) in Computer Science from the Queen Mary University of London.
    Prior to joining NTU, he served as a Research Assistant Professor at the MMLab
    of The Chinese University of Hong Kong, from 2013 to 2018\. He was a postdoctoral
    researcher at Queen Mary University of London and Vision Semantics Limited, from
    2010 to 2013\. He serves as an Associate Editor of the IEEE Transactions on Pattern
    Analysis and Machine Intelligence and International Journal of Computer Vision.
    He also serves/served as an Area Chair of major conferences such as ICCV, CVPR,
    ECCV and AAAI. His research interests include image/video restoration and enhancement,
    generative tasks, and representation learning. |'
  prefs: []
  type: TYPE_TB
