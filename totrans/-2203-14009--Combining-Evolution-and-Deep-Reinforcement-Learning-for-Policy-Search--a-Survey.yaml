- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:47:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2203.14009] Combining Evolution and Deep Reinforcement Learning for Policy
    Search: a Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2203.14009](https://ar5iv.labs.arxiv.org/html/2203.14009)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[![[Uncaptioned image]](img/7a79f54fc141ff6f269955be00e2e38d.png) Olivier Sigaud](https://orcid.org/0000-0002-8544-0229),'
  prefs: []
  type: TYPE_NORMAL
- en: Sorbonne Université, CNRS, Institut des Systèmes Intelligents et de Robotique,
  prefs: []
  type: TYPE_NORMAL
- en: F-75005 Paris, France
  prefs: []
  type: TYPE_NORMAL
- en: Olivier.Sigaud@isir.upmc.fr
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep neuroevolution and deep Reinforcement Learning have received a lot of attention
    in the last years. Some works have compared them, highlighting theirs pros and
    cons, but an emerging trend consists in combining them so as to benefit from the
    best of both worlds. In this paper, we provide a survey of this emerging trend
    by organizing the literature into related groups of works and casting all the
    existing combinations in each group into a generic framework. We systematically
    cover all easily available papers irrespective of their publication status, focusing
    on the combination mechanisms rather than on the experimental results. In total,
    we cover 45 algorithms more recent than 2017\. We hope this effort will favor
    the growth of the domain by facilitating the understanding of the relationships
    between the methods, leading to deeper analyses, outlining missing useful comparisons
    and suggesting new combinations of mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea that the extraordinary adaptive capabilities of living species results
    from a combination of evolutionary mechanisms acting at the level of a population
    and learning mechanisms acting at the level of individuals is ancient in life
    sciences (Simpson, [1953](#bib.bib88)) and has inspired early work in Artificial
    Intelligence (AI) research (Holland and Reitman, [1978](#bib.bib30)). This early
    starting point has led to the independent growth of two bodies of formal frameworks,
    evolutionary methods and reinforcement learning (RL). The early history of the
    evolutionary side is well covered in Bäck et al. ([1997](#bib.bib2)) and from
    the RL side in Sutton and Barto ([2018](#bib.bib92)). Despite these independent
    developments, research dedicated to the combination has remained active, in particular
    around Learning Classifier Systems (Lanzi, [1999](#bib.bib42); Sigaud and Wilson,
    [2007](#bib.bib86)) and studies of the Baldwin effect (Weber and Depew, [2003](#bib.bib101)).
    A broader perspective and survey on all the evolutionary and RL combinations anterior
    to the advent of the so called ”deep learning” methods using large neural networks
    can be found in Drugan ([2019](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we propose a survey of a renewed approach to this combination
    that builds on the unprecedented progress made possible in evolutionary and deep
    RL methods by the growth of computational power and the availability of efficient
    libraries to use deep neural networks. As this survey shows, the topic is rapidly
    gaining popularity with a wide variety of approaches and even emerging libraries
    dedicated to their implementation (Tangri et al., [2022](#bib.bib95)). Thus we
    believe it is the right time for laying solid foundations to this growing field,
    by listing the approaches and providing a unified view that encompasses them.
    There are recent surveys about the comparison of evolutionary and RL methods (Qian
    and Yu, [2021](#bib.bib75); Majid et al., [2021](#bib.bib58)) which mention the
    emergence of some of these combinations. With respect to these surveys, ours is
    strictly focused on the combinations and attempts to provide a list of relevant
    papers as exhaustive as possible at the time of its publication, irrespective
    of their publication status.
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey is organized into groups of algorithms using the evolutionary part
    for the same purpose. In Section [2](#S2 "2 Evolution of policies for performance
    ‣ Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey"),
    we first review algorithms where evolution is looking for efficient policies,
    that is combining deep neuroevolution and deep RL. We then cover in Section [3](#S3
    "3 Evolution of actions for performance ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey") algorithms where evolution directly looks
    for efficient actions in a given state rather than for policies. In Section [4](#S4
    "4 Evolution of policies for diversity ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey"), we cover the combination of deep RL algorithm
    with diversity seeking methods. Finally, in Section [5](#S5 "5 Evolution of something
    else ‣ Combining Evolution and Deep Reinforcement Learning for Policy Search:
    a Survey"), we cover various other uses of evolutionary methods, such as optimizing
    hyperparameters or the system’s morphology. To keep the survey as short as possible,
    we consider that the reader is familiar with evolutionary and RL methods in the
    context of policy search, and has a good understanding of their respective advantages
    and weaknesses. We refer the reader to Sigaud and Stulp ([2019](#bib.bib85)) for
    an introduction of the methods and to surveys about comparisons to know more about
    their pros and cons (Qian and Yu, [2021](#bib.bib75); Majid et al., [2021](#bib.bib58)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Evolution of policies for performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The methods in our first family combine a deep neuroevolution loop and a deep
    RL loop. Figure [1](#S2.F1 "Figure 1 ‣ 2 Evolution of policies for performance
    ‣ Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey")
    provides a generic template to illustrate such combinations. The central question
    left open by the template is how both loops interact with each other. Note that
    this template is not much adapted to account for works where the combination is
    purely sequential, such as Kim et al. ([2007](#bib.bib39)) or the gep-pg algorithm
    (Colas et al., [2018](#bib.bib11)). Besides, to avoid any confusion with the multi-agent
    setting, note that agents are interacting in isolation with their own copy of
    the environment and cannot interact with each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d9875312c88142db8d3cc36407100064.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The general template of algorithms combining deep neuroevolution
    and deep RL. A population of agents interact with an environment, and produce
    trajectories composed of states, actions and rewards. From the left-hand side,
    an evolutionary loop selects and evolves these agents based on their fitness,
    which is computed holistically over trajectories. From the right-hand side, a
    deep RL loop improves one or several agents using a gradient computed over the
    elementary steps of trajectories stored into a replay buffer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Combinations evolving policies for performance. The table states whether
    the algorithms in the rows use the mechanisms in the columns. The colors are as
    follows. In the column about other combination mechanisms (+ Comb. Mech.): Critic
    gradient addition $\bullet$ (green), Population from Actor $\spadesuit$ (blue),
    None x (red). In all other columns: $\bullet$ (green): yes, x (red): no. In bnet,
    BBNE stands for Behavior-Based NeuroEvolution and CPG stands for Cartesian Genetic
    Programming (Miller and Harding, [2009](#bib.bib61)). The different GA labels
    stand for various genetic algorithms, we do not go into the details.'
  prefs: []
  type: TYPE_NORMAL
- en: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3)
    scale(1, -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15)
    scale(1, -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | RL algo. | Evo. algo. | Actor Injec. | + Comb. Mech. | Surr. Fitness | Soft
    Update | Buffer Filt. |'
  prefs: []
  type: TYPE_TB
- en: '| erl Khadka and Tumer ([2018](#bib.bib37)) | ddpg | GA | $\bullet$ | x | x
    | x | x |'
  prefs: []
  type: TYPE_TB
- en: '| cerl Khadka et al. ([2019](#bib.bib36)) | td3 | GA | $\bullet$ | x | x |
    x | x |'
  prefs: []
  type: TYPE_TB
- en: '| pderl Bodnar et al. ([2020](#bib.bib4)) | td3 | GA | $\bullet$ | x | x |
    x | x |'
  prefs: []
  type: TYPE_TB
- en: '| esac Suri et al. ([2020](#bib.bib91)) | sac | ES | $\bullet$ | x | x | x
    | x |'
  prefs: []
  type: TYPE_TB
- en: '| fidi-rl Shi et al. ([2019](#bib.bib82)) | ddpg | ars | $\bullet$ | x | x
    | x | x |'
  prefs: []
  type: TYPE_TB
- en: '| x-ddpg Espositi and Bonarini ([2020](#bib.bib19)) | ddpg | GA | $\bullet$
    | x | x | x | x |'
  prefs: []
  type: TYPE_TB
- en: '| cem-rl Pourchot and Sigaud ([2019](#bib.bib73)) | td3 | cem | x | $\bullet$
    | x | x | x |'
  prefs: []
  type: TYPE_TB
- en: '| cem-acer Tang ([2021](#bib.bib93)) | acer | cem | x | $\bullet$ | x | x |
    x |'
  prefs: []
  type: TYPE_TB
- en: '| serl Wang et al. ([2022](#bib.bib99)) | ddpg | GA | $\bullet$ | x | $\bullet$
    | x | x |'
  prefs: []
  type: TYPE_TB
- en: '| spderl Wang et al. ([2022](#bib.bib99)) | td3 | GA | $\bullet$ | x | $\bullet$
    | x | x |'
  prefs: []
  type: TYPE_TB
- en: '| pgps Kim et al. ([2020](#bib.bib40)) | td3 | cem | $\bullet$ | x | $\bullet$
    | $\bullet$ | x |'
  prefs: []
  type: TYPE_TB
- en: '| bnet Stork et al. ([2021](#bib.bib89)) | BBNE | CPG | $\bullet$ | x | $\bullet$
    | x | x |'
  prefs: []
  type: TYPE_TB
- en: '| cspc Zheng et al. ([2020](#bib.bib104)) | sac + ppo | cem | $\bullet$ | x
    | x | x | $\bullet$ |'
  prefs: []
  type: TYPE_TB
- en: '| supe-rl Marchesini et al. ([2021](#bib.bib60)) | rainbow or ppo | GA | $\bullet$
    | $\spadesuit$ | x | $\bullet$ | x |'
  prefs: []
  type: TYPE_TB
- en: '| g2ac Chang et al. ([2018](#bib.bib5)) | a2c | GA | x | $\spadesuit$ | x |
    x | x |'
  prefs: []
  type: TYPE_TB
- en: '| g2ppo Chang et al. ([2018](#bib.bib5)) | ppo | GA | x | $\spadesuit$ | x
    | x | x |'
  prefs: []
  type: TYPE_TB
- en: The main motivation for combining evolution and deep RL is the improved performance
    that may result from the combination. For instance, through simple experiments
    with simple fitness landscapes and simplified versions of the components, combining
    evolution and RL can be shown to work better than using either of the two in isolation
    (Todd et al., [2020](#bib.bib97)). Why is this so? One of the explanations is
    the following. A weakness of policy gradient methods at the heart of deep RL is
    that they compute an estimate of the true gradient based on a limited set of samples.
    This gradient can be quite wrong due to the high variance of the estimation, but
    it is applied blindly to the current policy without checking that this actually
    improves it. By contrast, variation-selection methods at the heart of evolutionary
    methods evaluate all the policies they generate and remove the poorly performing
    ones. Thus a first good reason to combine policy gradient and variation-selection
    methods is that the latter may remove policies that have been deteriorated by
    the gradient step. Below we list different approaches building on this idea. This
    perspective is the one that gives rise to the largest list of combinations. We
    further split this list into several groups of works in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Deep RL actor injection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the main algorithms at the origin of the renewal of combining evolution
    and RL is erl (Khadka and Tumer, [2018](#bib.bib37)), see Figure LABEL:fig:erl_cerl.
    It was published simultaneously with the g2ac and g2ppo algorithms (Chang et al.,
    [2018](#bib.bib5)) but its impact was much greater. Its combination mechanism
    consists in injecting the RL actor into the evolutionary population.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce01cd169992b805e1d685adc1cbd51a.png)'
  prefs: []
  type: TYPE_IMG
- en: ((a))
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bafb19b1850035f62074c59820f7c8e5.png)'
  prefs: []
  type: TYPE_IMG
- en: ((b))
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: The template architecture for erl, esac, fidi-rl and cerl (a) and
    the pderl architecture (b). In erl, an actor learned by ddpg (Lillicrap et al.,
    [2016](#bib.bib48)) is periodically injected into the population and submitted
    to evolutionary selection. If ddpg performs better than the GA, this will accelerate
    the evolutionary process. Otherwise the ddpg agent is just ignored. In esac, ddpg
    is replaced by sac (Haarnoja et al., [2018](#bib.bib26)) and in fidi-rl, the GA
    is replaced by ars (Mania et al., [2018](#bib.bib59)). In cerl, the ddpg agent
    is replaced by a set of td3 actors sharing the same replay buffer, but each using
    a different discount factor. Again, those of such actors that perform better than
    the rest of the population are kept and enhance the evolutionary process, whereas
    the rest is discarded by evolutionary selection. In pderl, the genetic operators
    of erl are replaced by operators using local replay buffers so as to better leverage
    the step-based experience of each agent.'
  prefs: []
  type: TYPE_NORMAL
- en: The erl algorithm was soon followed by cerl (Khadka et al., [2019](#bib.bib36))
    which extends erl from RL to distributed RL where several agents learn in parallel,
    and all these agents are injected into the evolutionary population. The main weakness
    of erl and cerl is their reliance on a genetic algorithm which applies a standard
    $n$-point-based crossover and a Gaussian weight mutation operator to a direct
    encoding of the neural network architecture as a simple vector of parameters.
    This approach is known to require tedious hyperparameter tuning and generally
    perform worse than evolution strategies which are also mathematically more founded
    (Salimans et al., [2017](#bib.bib76)). In particular, the genetic operators used
    in erl and cerl based on a direct encoding have been shown to induce a risk of
    catastrophic forgetting of the behavior of efficient individuals.
  prefs: []
  type: TYPE_NORMAL
- en: The pderl algorithm (Bodnar et al., [2020](#bib.bib4)), see Figure LABEL:fig:pderl,
    builds on this criticism and proposes two alternative evolution operators. Instead
    of standard crossover, all agents carry their own replay buffer and crossover
    selects the best experience in both parents to fill the buffer of the offspring,
    before applying behavioral cloning to get a new policy that behaves in accordance
    with the data in the buffer. This operator is inspired by the work of Gangwani
    and Peng ([2018](#bib.bib22)). For mutation, they take as is the improved operator
    proposed in Lehman et al. ([2018](#bib.bib44)), which can be seen as applying
    a Gauss-Newton method to perform the policy gradient step (Pierrot et al., [2018](#bib.bib70)).
  prefs: []
  type: TYPE_NORMAL
- en: Another follow-up of erl is the esac algorithm (Suri et al., [2020](#bib.bib91)).
    It uses the sac algorithm instead of ddpg and a modified evolution strategy instead
    of a genetic algorithm, but the architecture follows the same template. Similarly,
    the fidi-rl algorithm (Shi et al., [2019](#bib.bib82)) combines ddpg with Augmented
    Random Search (ars), a finite difference algorithm which can be seen as as simplified
    version of evolution strategies (Mania et al., [2018](#bib.bib59)). fidi-rl uses
    the erl architecture as is. The method is shown to outperform ars alone and ddpg
    alone, but neither esac nor fidi-rl are compared to any other combination listed
    in this survey. Finally, the x-ddpg algorithm is a version of erl with several
    asynchronous ddpg actors where the buffers from the evolutionary agents and from
    the ddpg agents are separated, and the most recent ddpg agent is injected into
    the evolutionary population at each time step (Espositi and Bonarini, [2020](#bib.bib19)).
  prefs: []
  type: TYPE_NORMAL
- en: The bnet algorithm (Stork et al., [2021](#bib.bib89)) is borderline in this
    survey as it does not truly use an RL algorithm, but uses a Behavior-Based Neuroevolution
    (BBNE) mechanism which is only loosely inspired from RL algorithms, without relying
    on gradient descent. bnet combines a robust selection method based on standard
    fitness, a second mechanism based on the advantage of the behavior of an agent,
    and a third mechanism based on a surrogate estimate of the return of policies.
    The BBNE mechanism is reminiscent of the Advantage Weighted Regression (awr) algorithm
    (Peng et al., [2019](#bib.bib68)), but it uses an evolutionary approach to optimize
    this behavior-based criterion instead of standard gradient-based methods. The
    reasons for this choice is that the evolutionary part relies on Cartesian Genetic
    Programming (Miller and Harding, [2009](#bib.bib61)) which evolves the structure
    of the neural networks, but gradient descent operators cannot be applied to networks
    whose structure is evolving over episodes.
  prefs: []
  type: TYPE_NORMAL
- en: The chdrl architecture (Zheng et al., [2020](#bib.bib104)) extends the erl approach
    in several ways to improve the sample efficiency of the combination. First, it
    uses two levels of RL algorithms, one on-policy and one off-policy, to benefit
    from the higher sample efficiency of off-policy learning. Second, instead of injecting
    an actor periodically in the evolutionary population, it does so only when the
    actor to be injected performs substantially better than the evolutionary agents.
    Third, it combines the standard replay buffer with a smaller local one which is
    filled with filtered data to ensure using the most beneficial samples. The cspc
    algorithm, depicted in Figure LABEL:fig:cspc is an instance of chdrl using the
    sac and ppo (Schulman et al., [2017](#bib.bib78)) algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e0f5803afde0b734725972de8b18a96.png)'
  prefs: []
  type: TYPE_IMG
- en: ((a))
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bac0358e0cc30492ccf2889ef87e82d6.png)'
  prefs: []
  type: TYPE_IMG
- en: ((b))
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: The cspc (a) and cem-rl (b) architectures. In cspc, an on-policy
    and an off-policy algorithms are combined, together with two replay buffers and
    a performance-based actor injection rule, to improve the sample efficiency of
    erl-like methods. In cem-rl, gradient steps from the td3 critic are applied to
    half the population of evolutionary agents. If applying this gradient is favorable,
    the corresponding individuals are kept, otherwise they are discarded.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that if an RL actor is injected in an evolutionary population and if evolution
    uses a direct encoding, the RL actor and evolution individuals need to share a
    common structure. Removing this constraint might be useful, as evolutionary methods
    are often applied to smaller policies than RL methods. For doing so, one might
    call upon any policy distillation mechanism that strives to obtain from a large
    policy a smaller policy with similar capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 RL gradient addition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of injecting an RL actor into the population, another approach consists
    in applying gradient steps to some members of this population. This is the approach
    of the cem-rl algorithm (Pourchot and Sigaud, [2019](#bib.bib73)), see Figure LABEL:fig:cemrl.
    This work was followed by cem-acer (Tang, [2021](#bib.bib93)) which simply replaces
    td3 (Fujimoto et al., [2018](#bib.bib21)) with acer (Wang et al., [2017](#bib.bib100)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Evolution from the RL actor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ((a))
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f35fe6f65be8ff588ec7fedd76180623.png)'
  prefs: []
  type: TYPE_IMG
- en: ((b))
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: In the g2n (a) and supe-rl (b) architectures, the evolutionary population
    is built locally from the RL actor. In g2n, the evolutionary part explores the
    structure of the central layer of the actor network. In supe-rl, more standard
    mutations are applied, the non-mutated actor is inserted in the evolutionary population
    and the actor is soft-updated towards its best offspring.'
  prefs: []
  type: TYPE_NORMAL
- en: In the algorithms listed so far, the main loop is evolutionary and the RL loop
    is used at a slower pace to accelerate it. In the g2n (Chang et al., [2018](#bib.bib5))
    and supe-rl (Marchesini et al., [2021](#bib.bib60)) algorithms, by contrast, the
    main loop is the RL loop and evolution is used to favor exploration.
  prefs: []
  type: TYPE_NORMAL
- en: In g2n, shown in Figure LABEL:fig:ggn, evolution is used to activate or deactivate
    neurons of the central layer in the architecture of the actor according to a binary
    genome. By sampling genomes using evolutionary operators, various actor architectures
    are evaluated and the one that performs best benefits from a critic gradient step,
    before its genome is used to generate a new population of architectures. This
    mechanism provides a fair amount of exploration both in the actor structures and
    in the generated trajectories and outperforms random sampling of the genomes.
    Two instances of the g2n approach are studied, g2ac based on a2c and g2ppo based
    on ppo, and they both outperform the RL algorithm they use.
  prefs: []
  type: TYPE_NORMAL
- en: The supe-rl algorithm, shown in Figure LABEL:fig:superl, is similar to g2n apart
    from the fact that evolving the structure of the central layer is replaced by
    performing standard Gaussian noise mutation of all the parameters of the actor.
    Besides, if one of the offspring is better than the current RL agent, the latter
    is modified towards this better offspring through a soft update mechanism. Finally,
    the non-mutated actor is also inserted in the evolutionary population, which is
    not the case in g2n.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Using a surrogate fitness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/77399daa7332fa4277c4e9761d0f27ad.png)'
  prefs: []
  type: TYPE_IMG
- en: ((a))
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f5ed39b6018698c0c0abb1695aa56b25.png)'
  prefs: []
  type: TYPE_IMG
- en: ((b))
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: The sc-erl (a) and pgps (b) architectures are two approaches to improve
    sample efficiency by using a critic network as a surrogate for evaluating evolutionary
    individuals. In sc-erl, the surrogate control part is generic and can be applied
    to several architectures such as erl, cerl or cem-rl. It considers the critic
    as a surrogate model of fitness, making it possible to estimate the fitness of
    a new individual without generating additional samples. (b) The pgps uses the
    same idea but combines it with several other mechanisms, such as performing a
    soft update of the actor towards the best evolutionary agent or filling half the
    population using the surrogate fitness and the other half from cem generated agents.'
  prefs: []
  type: TYPE_NORMAL
- en: A weakness of all the methods combining evolution and RL that we have listed
    so far is that they require evaluating the agents to perform the evolutionary
    selection step, which may impair sample efficiency. In the sc-erl (Wang et al.,
    [2022](#bib.bib99)) and pgps (Kim et al., [2020](#bib.bib40)) architectures, this
    concern is addressed by using a critic network as a surrogate for evaluating an
    agent. Importantly, the evaluation of individuals must initially rely on the true
    fitness but can call upon the critic more and more often as its accuracy gets
    better. As shown in Figure LABEL:fig:scerl, the sc-erl architecture is generic
    and can be applied on top of any of the combinations we have listed so far. In
    practice, it is applied to erl, pderl and cem-rl, resulting in the serl and spderl
    algorithms in the first two cases.
  prefs: []
  type: TYPE_NORMAL
- en: The pgps algorithm (Kim et al., [2020](#bib.bib40)), shown in Figure LABEL:fig:pgps,
    builds on the same idea but uses it in the context of a specific combination of
    evolutionary and RL mechanisms which borrows ideas from several of the previously
    described methods. In more details, half of the population is filled with agents
    evaluated from the surrogate fitness whereas the other half are generated with
    cem. Furthermore, the current td3 actor is injected into the population and benefits
    from a soft update towards the best agent in the population.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Evolution of actions for performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we cover algorithms where evolution is used to optimize an action
    in a given state, rather than optimizing policy parameters. The general idea is
    that variation-selection methods such as cem can optimize any vector of parameters
    given some performance function of these parameters. In the methods listed in
    the previous section, the parameters were those of a policy and the performance
    was the return of that policy. In the methods listed here, the parameters specify
    the action in a given state and the performance is the Q-value of this action
    in that state.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an RL algorithm like Q-learning, the agent needs to find the action with
    the highest value in a given state for two things: for performing critic updates,
    that is updating its estimates of the action-value function using $Q(s_{t},a_{t})\leftarrow
    r(s_{t},a_{t})+\max_{a}Q(s_{t+1},a)-Q(s_{t},a_{t})$, and for acting using $\mathop{\rm
    argmax}_{a}Q(s_{t},a)$. When the action space is continuous, this amounts to solving
    an expensive optimization problem, and this is required at each training step.
    The standard solution to this problem in actor-critic methods consists in considering
    the action of the actor as a good proxy for the best action. The estimated best
    action, that we note $\bar{a_{t}}$, is taken to be the actor’s action $\bar{a_{t}}=\pi(s_{t})$,
    resulting in using $Q(s_{t},a_{t})\leftarrow r(s_{t},a_{t})+\max_{a}Q(s_{t+1},\pi(s_{t+1}))-Q(s_{t},a_{t})$
    for the critic update and using $\bar{a_{t}}$ for acting.'
  prefs: []
  type: TYPE_NORMAL
- en: But as an alternative, one can call upon a variation-selection method to find
    the best performing action over a limited set of sampled actions. This approach
    is used in the qt-opt algorithm (Kalashnikov et al., [2018](#bib.bib35)), as well
    as in the cgp (Simmons-Edler et al., [2019](#bib.bib87)), sac-cepo (Shi and Singh,
    [2021](#bib.bib84)), grac (Shao et al., [2021](#bib.bib81)) and eas-rl (Ma et al.,
    [2022](#bib.bib56)) algorithms. This is the approach we first cover in this section.
    The zospi algorithm (Sun et al., [2020](#bib.bib90)) also benefits from optimizing
    actions with a variation-selection method, though it stems from a different perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Combinations evolving actions for performance. The cells in green
    denote where evolutionary optimization takes places. We specify the use of cem
    for optimizing an action with $\bar{a_{t}}=\text{{\sc cem}}(\textit{source},N,N_{e},I)$,
    where source is the source from which we sample initial actions, $N$ is the size
    of this sample (the population), $N_{e}$ is the number of elite solutions that
    are retained from a generation to the next and $I$ is the number of iterations.
    For pso, the shown parameters are the number of action $N$ and the number of iterations
    $T$. And we use $\bar{a_{t}}=\mathop{\rm argmax}(\textit{source},N)$ for simply
    take the best action over $N$ samples from a given source.'
  prefs: []
  type: TYPE_NORMAL
- en: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3)
    scale(1, -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15)
    scale(1, -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | Critic update | Action Selection | Policy Update |'
  prefs: []
  type: TYPE_TB
- en: '| qt-opt Kalashnikov et al. ([2018](#bib.bib35)) | $\bar{a_{t}}=$cem (random,
    64, 6, 2) | $\bar{a_{t}}=$cem (random, 64, 6, 2) | No policy |'
  prefs: []
  type: TYPE_TB
- en: '| cgp Simmons-Edler et al. ([2019](#bib.bib87)) | $\bar{a_{t}}=$cem (random,
    64, 6, 2) | $\bar{a_{t}}=\pi(s_{t})$ | BC or DPG |'
  prefs: []
  type: TYPE_TB
- en: '| eas-rl Ma et al. ([2022](#bib.bib56)) | $\bar{a_{t}}=$pso (10,10) | $\bar{a_{t}}=\pi(s_{t})$
    | BC + DPG |'
  prefs: []
  type: TYPE_TB
- en: '| sac-cepo Shi and Singh ([2021](#bib.bib84)) | sac update | $\bar{a_{t}}=$cem
    ($\pi$, 60 \clipbox*.250pt 0pt 0pt 0pt →140, $3\%$ \clipbox*.250pt 0pt 0pt 0pt
    →$7\%$, 6 \clipbox*.250pt 0pt 0pt 0pt →14) | BC |'
  prefs: []
  type: TYPE_TB
- en: '| grac Shao et al. ([2021](#bib.bib81)) | $\bar{a_{t}}=$cem ($\pi$, 256, 5,
    2) | $\bar{a_{t}}=$cem ($\pi$, 256, 5, 2) | PG with two losses |'
  prefs: []
  type: TYPE_TB
- en: '| zospi Sun et al. ([2020](#bib.bib90)) | ddpg update | $\bar{a_{t}}=\pi(s_{t})$
    + perturb. network | BC($\bar{a_{t}}=\mathop{\rm argmax}(random,50)$) |'
  prefs: []
  type: TYPE_TB
- en: 'As Table [2](#S3.T2 "Table 2 ‣ 3 Evolution of actions for performance ‣ Combining
    Evolution and Deep Reinforcement Learning for Policy Search: a Survey") shows,
    the qt-opt algorithm (Kalashnikov et al., [2018](#bib.bib35)) simply samples 64
    random actions in the action space and performs two iterations of cem to get a
    high performing action, both for critic updates and action selection. It is striking
    that such a simple method can perform well even in large action spaces. This simple
    idea was then improved in the cgp algorithm (Simmons-Edler et al., [2019](#bib.bib87))
    so as to avoid the computational cost of action inference. Instead of using cem
    to sample an action at each time step, a policy network is learned based on the
    behavior of the cem. This network can be seen as a surrogate of the cem sampling
    process and is trained either from the sampled $\bar{a_{t}}$ using Behavioral
    Cloning (BC) or following a Deterministic Policy Gradient (DPG) step from the
    critic.'
  prefs: []
  type: TYPE_NORMAL
- en: The eas-rl algorithm (Ma et al., [2022](#bib.bib56)) is similar to cgp apart
    from the fact that it uses Particle Swarm Optimization (pso) instead of cem. Besides,
    depending on the sign of the advantage of the obtained action $\bar{a_{t}}$, it
    uses either BC or DPG to update the policy for each sample.
  prefs: []
  type: TYPE_NORMAL
- en: Symmetrically to cgp, the sac-cepo algorithm (Shi and Singh, [2021](#bib.bib84))
    performs standard critic updates using sac but selects actions using cem. More
    precisely, it introduces the idea to sample the action from the current policy
    rather than randomly, and updates this policy using BC from the sampled actions.
    Besides, the paper investigates the effect of the cem parameters but does not
    provide solid conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: The grac algorithm (Shao et al., [2021](#bib.bib81)) combines ideas from cgp
    and sac-cepo. A stochastic policy network outputs an initial Gaussian distribution
    for the action at each step. Then, a step of cem drawing 256 actions out of this
    distribution is used to further optimize the choice of action both for critic
    updates and action selection. The policy itself is updated with a combination
    of two training losses.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the zospi algorithm (Sun et al., [2020](#bib.bib90)) calls upon variation-selection
    for updating the policy rather than for updating the critic or selecting the action.
    Its point is rather than gradient descent algorithms tend to get stuck into local
    minima and may miss the appropriate direction due to various approximations, whereas
    a variation-selection method is more robust. Thus, to update its main policy,
    zospi simply samples a set of actions and performs BC towards the best of these
    actions, which can be seen as a trivial variation-selection method. The typical
    number of sampled actions is 50\. It then adds a policy perturbation network to
    perform exploration, which is trained using gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evolution of policies for diversity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The trade-off between exploration and exploitation is central to RL. In particular,
    when the reward signal is sparse, efficient exploration becomes crucial. All the
    papers studied in this survey manage a population of agents, hence their capability
    to explore can benefit from maintaining behavioral diversity between the agents.
    This idea of maintaining behavioral diversity is central to two families of diversity
    seeking algorithms, the novelty search (NS) (Lehman and Stanley, [2011](#bib.bib45))
    algorithms which do not use the reward signal at all, see Figure LABEL:fig:ns_rl,
    and the quality-diversity (QD) algorithms (Pugh et al., [2016](#bib.bib74); Cully
    and Demiris, [2017](#bib.bib14)), see Figure LABEL:fig:me, which try to maximize
    both diversity and performance. As the NS approach only looks for diversity, it
    is better in the absence of reward, or when the reward signal is very sparse or
    deceptive as the best one can do in the absence of reward is try to cover a relevant
    space as uniformly as possible (Doncieux et al., [2019](#bib.bib17)). By contrast,
    the QD approach is more appropriate when the reward signal can contribute to the
    policy search process. In this section we cover both families separately.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1b24db0ae1987840a248501caa9a8942.png)'
  prefs: []
  type: TYPE_IMG
- en: ((a))
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/61588e134dde1358d9c369eb1bc06b1e.png)'
  prefs: []
  type: TYPE_IMG
- en: ((b))
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Template architectures for combining deep RL with novelty search
    (a) and quality-diversity (b). The latter builds on Fig. 2 in Mouret ([2020](#bib.bib63)).
    Both architectures rely on a behavioral characterization space and maintain an
    archive in that space.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Novelty seeking approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 3: Combinations evolving policies for diversity. NS: Novelty Search.
    Policy params: distance is computed in the policy parameters space. GC-ddpg: goal-conditioned
    ddpg. Manual BC: distances are computed in a manually defined behavior characterization
    space.'
  prefs: []
  type: TYPE_NORMAL
- en: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3)
    scale(1, -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15)
    scale(1, -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | RL algo. | Diversity algo. | Distance space |'
  prefs: []
  type: TYPE_TB
- en: '| p3s-td3 Jung et al. ([2020](#bib.bib34)) | td3 | Find best | Policy params.
    |'
  prefs: []
  type: TYPE_TB
- en: '| deprl Liu and Feng ([2021](#bib.bib51)) | td3 | cem | Policy params. |'
  prefs: []
  type: TYPE_TB
- en: '| arac Doan et al. ([2019](#bib.bib16)) | sac | NS-like | Policy params. |'
  prefs: []
  type: TYPE_TB
- en: '| ns-rl Shi et al. ([2020](#bib.bib83)) | GC-ddpg | True NS | Manual BC |'
  prefs: []
  type: TYPE_TB
- en: '| pns-rl Liu et al. ([2018](#bib.bib52)) | td3 | True NS | Manual BC |'
  prefs: []
  type: TYPE_TB
- en: 'Maintaining a distance between agents in a population can be achieved in different
    spaces. For instance, the svpg algorithm (Liu et al., [2017](#bib.bib53)) defines
    distances in a kernel space and adds to the policy gradient a loss term dedicated
    to increasing the pairwise distance between agents. Alternatively, the dvd algorithm
    (Parker-Holder et al., [2020](#bib.bib66)) defines distances in an action embedding
    space, corresponding to the actions specified by each agent in a large enough
    set of random states. Then dvd optimizes a global distance between all policies
    by maximizing the volume of the space between them through the computation of
    a determinant. Despite their interest, these two methods depicted in Figure [7](#S4.F7
    "Figure 7 ‣ 4.1 Novelty seeking approaches ‣ 4 Evolution of policies for diversity
    ‣ Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey")
    do not appear in Table [3](#S4.T3 "Table 3 ‣ 4.1 Novelty seeking approaches ‣
    4 Evolution of policies for diversity ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey") as the former does not have an evolutionary
    component and the latter uses nsr-es (Conti et al., [2018](#bib.bib13)) but does
    not have an RL component.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c6495f537bddda9bbe54024fdd710a0.png)'
  prefs: []
  type: TYPE_IMG
- en: ((a))
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/983c396c3976e6bf1d6ec2cba230d1a7.png)'
  prefs: []
  type: TYPE_IMG
- en: ((b))
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: The svpg (a) and dvd (b) architectures. In svpg, individual policy
    gradients computed by each agent are combined so as to ensure both diversity between
    agents and performance improvement. In dvd, a purely evolutionary approach is
    combined with a diversity mechanism which seeks to maximize the volume between
    the behavioral characterization of agents in an action embedding space. Both architectures
    fail to combine evolution and RL, though they both try to maximize diversity and
    performance in a population of agents.'
  prefs: []
  type: TYPE_NORMAL
- en: A more borderline case with respect to the focus of this survey is the p3s-td3
    algorithm (Jung et al., [2020](#bib.bib34)). Though p3s-td3 is used as a baseline
    in several of the papers mentioned in this survey, its equivalent of the evolutionary
    loop is limited to finding the best agent in the population, as shown in Figure LABEL:fig:p3s.
    This implies evaluating all these agents, but not using neither variation nor
    selection. Besides, the mechanism to maintain a distance between solutions in
    p3s-td3 is ad hoc and acts in the space of policy parameters. This is also the
    case in the deprl algorithm (Liu and Feng, [2021](#bib.bib51)), which is just
    a variation of cem-rl where an ad hoc mechanism is added to enforce some distance
    between members of the evolutionary population.
  prefs: []
  type: TYPE_NORMAL
- en: The arac algorithm (Doan et al., [2019](#bib.bib16)) also uses a distance in
    the policy parameter space, but it truly qualifies as a combination of evolution
    and deep RL, see Figure LABEL:fig:arac. An original feature of arac is that it
    selects the data coming into the replay buffer based on the novelty of agents,
    which can result in saving a lot of poorly informative gradient computations.
    A similar idea is also present in (Chen, [2019](#bib.bib6)) where instead of filtering
    based on novelty, the algorithm uses an elite replay buffer containing only the
    top trajectories, similarly to what we have already seen in the cspc algorithm
    (Zheng et al., [2020](#bib.bib104)).
  prefs: []
  type: TYPE_NORMAL
- en: The methods listed so far were neither using a manually defined behavior characterization
    space for computing distances between agents nor an archive of previous agents
    to evaluate novelty. Thus they do not truly qualify as NS approaches. We now turn
    to algorithms which combine both features.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a17396be8f321d44634ad371846eddb0.png)'
  prefs: []
  type: TYPE_IMG
- en: ((a))
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bfbbce1d4833ff852ec2a7460d908d25.png)'
  prefs: []
  type: TYPE_IMG
- en: ((b))
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: The p3s-td3 (a) and arac (b) architectures. In p3s-td3, all agents
    are trained with RL and evaluated, then they all perform a soft update towards
    the best agent. The arac algorithm maintains a population of policies following
    a gradient from a common sac critic (Haarnoja et al., [2018](#bib.bib26)). The
    critic itself is trained from trajectories of the most novel agents. Besides,
    diversity in the population is ensured by adding an attraction-repulsion loss
    $\mathcal{L}_{AR}$ to the update of the agents. This loss is computed with respect
    to an archive of previous agents themselves selected using a novelty criterion.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure LABEL:fig:ns_rl suggests that, when combining evolution and RL, novelty
    can be used as a fitness function, as a reward signal to learn a critic, or both.
    Actually, in the two algorithms described below, it is used for both. More precisely,
    the RL part is used to move the rest of the population towards the most novel
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: In the pns-rl algorithm (Liu et al., [2018](#bib.bib52)), a group of agents
    is following a leader combining a standard policy gradient update and a soft update
    towards the leader. Then, for any agent in the group, if its performance is high
    enough with respect to the mean performance in an archive, it is added to the
    archive. Crucially, the leader is selected as the one that maximizes novelty in
    the archive given a manually defined behavioral characterization. In addition,
    for efficient parallelization, the algorithm considers several groups instead
    of one, but where all groups share the same leader.
  prefs: []
  type: TYPE_NORMAL
- en: The ns-rl algorithm (Shi et al., [2020](#bib.bib83)) can be seen as a version
    of cem-rl whose RL part targets higher novelty by training less novel agents to
    minimize in each step the distance to the BC of the most novel agent. As the most
    novel agent and its BC change in each iteration, the RL part is implemented with
    goal-conditioned policies. This implies that the goal space is identical to the
    behavioral characterization space.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Quality-diversity approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 4: Quality-Diversity algorithms including an RL component. All these
    algorithms rely on the Map-Elites approach and the BC space is defined manually.
    For each algorithm in the rows, the table states whether quality and diversity
    are optimized using an RL approach or an evolutionary approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3)
    scale(1, -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15)
    scale(1, -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | Type of Archive | Q. improvement | D. improvement |'
  prefs: []
  type: TYPE_TB
- en: '| pga-me Nilsson and Cully ([2021](#bib.bib64)) | Map-Elites | td3 or GA |
    td3 or GA |'
  prefs: []
  type: TYPE_TB
- en: '| qd-pg-PF Cideron et al. ([2020](#bib.bib10)) | Pareto front | td3 | td3 |'
  prefs: []
  type: TYPE_TB
- en: '| qd-pg-ME Pierrot et al. ([2020](#bib.bib69)) | Map-Elites | td3 | td3 |'
  prefs: []
  type: TYPE_TB
- en: '| cma-mega-ES Tjanaka et al. ([2022](#bib.bib96)) | Map-Elites | cma-es | cma-es
    |'
  prefs: []
  type: TYPE_TB
- en: '| cma-mega-(td3, ES) Tjanaka et al. ([2022](#bib.bib96)) | Map-Elites | td3
    + cma-es | cma-es |'
  prefs: []
  type: TYPE_TB
- en: By contrast with NS approaches which only try to optimize diversity in the population,
    QD approaches combine this first objective with optimize the performance of registered
    policies, their quality. As Figure LABEL:fig:me suggests, when combined with an
    RL loop, the QD loop can give rise to various solutions depending on whether quality
    and diversity are improved with a evolutionary algorithm or a deep RL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The space of resulting possibilities is covered in Table [4](#S4.T4 "Table
    4 ‣ 4.2 Quality-diversity approaches ‣ 4 Evolution of policies for diversity ‣
    Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey").
    In more details, the pga-me algorithm (Nilsson and Cully, [2021](#bib.bib64))
    uses two optimization mechanisms, td3 and a GA, to generate new solutions that
    are added to the archive if they are either novel enough or more efficient than
    previously registered ones with the same behavioral characterization. By contrast,
    in the qd-rl approach, the mechanisms to improve quality and diversity are explicitly
    separated and consist in improving a quality critic and a diversity critic using
    td3\. Two implementations exist. First, the qd-pg-PF algorithm (Cideron et al.,
    [2020](#bib.bib10)) maintains a Pareto front of high quality and diversity solutions.
    From its side, the qd-pg-ME algorithm (Pierrot et al., [2020](#bib.bib69)) maintains
    a Map-Elites archive and introduces an additional notion of state descriptor to
    justify learning a state-based quality critic. Finally, the cma-mega approach
    (Tjanaka et al., [2022](#bib.bib96)) uses an ES to improve diversity and either
    an ES or a combination of ES and RL to improve quality.'
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, one can see that both quality and diversity can be improved through
    RL, evolution, or both.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Evolution of something else
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In all the architecture we have surveyed so far, the evolutionary part was used
    to optimize either policy parameters or a set of rewarding actions in a given
    state. In this section, we briefly cover combinations of evolution and deep RL
    where evolution is used to optimize something else that matters in the RL process,
    or where RL mechanisms are used to improve evolution without calling upon a full
    RL algorithm. We dedicate a separate part to optimizing hyperparameters, as it
    is an important and active domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Algorithms where evolution is applied to something else than action
    or policy parameters, or to more than policy parameters. All algorithms in the
    first half optimize hyperparameters. *: The algorithm in Park and Lee ([2021](#bib.bib65))
    is given no name. BT stands for Behavior Tree.'
  prefs: []
  type: TYPE_NORMAL
- en: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3)
    scale(1, -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15)
    scale(1, -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | RL algo. | Evo algo. | Evolves what? |'
  prefs: []
  type: TYPE_TB
- en: '| ga-drl Sehgal et al. ([2019](#bib.bib79)); Sehgal et al. ([2022](#bib.bib80))
    | ddpg (+her) | GA | Hyper-parameters |'
  prefs: []
  type: TYPE_TB
- en: '| pbt Jaderberg et al. ([2017](#bib.bib33)) | Any | Ad hoc | Parameters and
    Hyper-parameters |'
  prefs: []
  type: TYPE_TB
- en: '| aac Grigsby et al. ([2021](#bib.bib23)) | sac | Ad hoc | Parameters and Hyper-parameters
    |'
  prefs: []
  type: TYPE_TB
- en: '| searl Franke et al. ([2021](#bib.bib20)) | td3 | GA | Architecture, Parameters
    and Hyper-parameters |'
  prefs: []
  type: TYPE_TB
- en: '| oht-es Tang and Choromanski ([2020](#bib.bib94)) | Any | ES | Hyper-parameters
    |'
  prefs: []
  type: TYPE_TB
- en: '| epg Houthooft et al. ([2018](#bib.bib31)) | Ad hoc ($\sim$ ppo) | ES | Reward-related
    functions |'
  prefs: []
  type: TYPE_TB
- en: '| eQ Leite et al. ([2020](#bib.bib47)) | $\sim$ ddpg | $\sim$ cem | Critic
    |'
  prefs: []
  type: TYPE_TB
- en: '| evo-RL Hallawa et al. ([2017](#bib.bib29)) | Q-learning, dqn, ppo | BT |
    Partial policies |'
  prefs: []
  type: TYPE_TB
- en: '| derl Gupta et al. ([2021](#bib.bib24)) | ppo | GA | System’s morphology |'
  prefs: []
  type: TYPE_TB
- en: '| * Park and Lee ([2021](#bib.bib65)) | ppo | GA | System’s morphology |'
  prefs: []
  type: TYPE_TB
- en: 5.1 Evolution in MBRL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The cem algorithm can be used to optimize open-loop controllers to perform
    Model Predictive Control (MPC) on robotic systems in the plaNet (Hafner et al.,
    [2019](#bib.bib27)) and poplin (Wang and Ba, [2019](#bib.bib98)) algorithms, and
    an improved version of cem for this specific context is proposed in (Pinneri et al.,
    [2020](#bib.bib71); Pinneri et al., [2021](#bib.bib72)). Besides, this approach
    combining open-loop controllers and MPC is seen in the pets algorithm Chua et al.
    ([2018](#bib.bib9)) as implementing a form of Model-Based Reinforcement Learning
    (MBRL), and cem is used in pets to choose the points from where to start MPC,
    improving over random shooting. Finally, in Bharadhwaj et al. ([2020](#bib.bib3)),
    the authors propose to interleave cem iterations and Stochastic Gradient Descent
    (SGD) iterations to improve the efficiency of optimization of MPC plans, in a
    way reminiscent to cem-rl combining policy gradient steps and cem steps. But all
    these methods are applied to an open-loop control context where true reinforcement
    learning algorithms can not be applied, hence they do not appear in Table [5](#S5.T5
    "Table 5 ‣ 5 Evolution of something else ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Evolution of hyper-parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hyperparameter optimization (HPO) is notoriously hard and often critical in
    deep RL. The most straightforward way to leverage evolutionary methods in this
    context consists in nesting the deep RL algorithm within an evolutionary loop
    which tunes the hyper-parameters. This is the approach of the ga-drl algorithm
    (Sehgal et al., [2019](#bib.bib79); Sehgal et al., [2022](#bib.bib80)), but this
    obviously suffers from a very high computational cost. Note that the authors write
    that ga-drl uses ddpg + her, but the use of her is in no way clear as the algorithm
    does not seem to use goal-conditioned policies.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c7941bd2ec26d57e7df7202249551878.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The pbt architecture. The evolution part consists of two operators,
    explore and exploit which act both on the hyperparameters and the parameters of
    the agents.'
  prefs: []
  type: TYPE_NORMAL
- en: More interestingly, the pbt architecture (Jaderberg et al., [2017](#bib.bib33))
    is designed to solve this problem by combining distributed RL with an evolutionary
    mechanism which acts both on the parameters and hyperparameters within the RL
    training loop. It was successfully used in several challenging applications (Jaderberg
    et al., [2019](#bib.bib32)) and benefits from an interesting capability to tune
    the hyperparameters according to the current training dynamics, which is an important
    meta-learning capability (Khamassi et al., [2017](#bib.bib38)). A follow-up of
    the pbt algorithm is the aac algorithm (Grigsby et al., [2021](#bib.bib23)), which
    basically applies the same approach but with a better set of hyperparameters building
    on lessons learned in the recent deep RL literature.
  prefs: []
  type: TYPE_NORMAL
- en: A limitation of pbt is that each actor uses its own replay buffer. Instead,
    in the searl algorithm (Franke et al., [2021](#bib.bib20)), the experience of
    all agents is shared into a unique buffer. Furthermore, searl  simultaneously
    performs HPO and Neural Architecture Search, resulting in better performance than
    pbt. Finally, the oht-es algorithm (Tang and Choromanski, [2020](#bib.bib94))
    also uses a shared replay buffer, but limits the role of evolution to optimizing
    hyperparameters and does so with an ES algorithm. Given the importance of the
    problem, there are many other HPO methods, most of which are not explicitly calling
    upon an evolutionary approach. For a wider survey of the topic, we refer the reader
    to Parker-Holder et al. ([2022](#bib.bib67)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Evolution of miscellaneous RL or control components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, we briefly survey the rest of algorithms listed in Table [5](#S5.T5
    "Table 5 ‣ 5 Evolution of something else ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey"). The epg algorithm (Houthooft et al., [2018](#bib.bib31))
    uses a meta-learning approach to evolve the parameters of a loss function that
    replaces the policy gradient surrogate loss in policy gradient algorithms. The
    goal is to find a reward function that will maximize the capability of an RL algorithm
    to achieve a given task. A consequence of its design is that it cannot be applied
    to an actor-critic approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of evolving a population of agents, the eQ algorithm (Leite et al.,
    [2020](#bib.bib47)) evolves a population of critics, which are fixed over the
    course of learning for a given agent. This is somewhat symmetric to the previously
    mentioned zoac algorithm (Lei et al., [2022](#bib.bib46)) which uses evolution
    to update an actor given a critic trained with RL.
  prefs: []
  type: TYPE_NORMAL
- en: The evo-RL algorithm (Hallawa et al., [2021](#bib.bib28)) evolves partial policies.
    Evolution is performed in a discrete action context with a Genetic Programming
    approach (Koza et al., [1994](#bib.bib41)) that only specifies a partial policy
    as Behavior Trees (Colledanchise and Ögren, [2018](#bib.bib12)). An RL algorithm
    such as dqn (Mnih et al., [2015](#bib.bib62)) or ppo is then in charge of learning
    a policy for the states for which an action is not specified. The fitness of individuals
    is evaluated over their overall behavior combining the BT part and the learned
    part, but only the BT part is evolved to generate the next generation, benefiting
    from a Baldwin effect (Simpson, [1953](#bib.bib88)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, several works consider evolving the morphology of a mechanical system
    whose control is learned with RL. Table [5](#S5.T5 "Table 5 ‣ 5 Evolution of something
    else ‣ Combining Evolution and Deep Reinforcement Learning for Policy Search:
    a Survey") only mentions two recent instances, one where the algorithm is not
    named (Gupta et al., [2021](#bib.bib24)) and derl (Park and Lee, [2021](#bib.bib65)),
    but this idea has led to a larger body of works, e.g. (Ha, [2019](#bib.bib25);
    Luck et al., [2020](#bib.bib55)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Evolution improved with RL mechanisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Without using a full RL part, a few algorithms augment an evolutionary approach
    with components taken from RL.
  prefs: []
  type: TYPE_NORMAL
- en: First, the tres (for Trust Region Evolution Strategies) algorithm (Liu et al.,
    [2019](#bib.bib50)) incorporates into an ES several ideas from the trpo (Schulman
    et al., [2015](#bib.bib77)) and ppo (Schulman et al., [2017](#bib.bib78)) algorithms,
    such as introducing an importance sampling mechanism and using a clipped surrogate
    objective so as to enforce a natural gradient update. Unfortunately, tres is neither
    compared to the nes algorithm (Wierstra et al., [2014](#bib.bib102)) which also
    enforces a natural gradient update nor to the safe mutation mechanism of Lehman
    et al. ([2018](#bib.bib44)) which has similar properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, there are two perspectives about the zoac algorithm (Lei et al., [2022](#bib.bib46)).
    One can see it as close to the zospi algorithm described in Section [3](#S3 "3
    Evolution of actions for performance ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey"), that is an actor-critic method where gradient
    descent to update the actor given the critic is replaced by a more robust derivative-free
    approach. But the more accurate perspective, as put forward by the authors, is
    that zoac is an ES method where the standard ES gradient estimator is replaced
    by a gradient estimator using the advantage function so as to benefit from the
    capabilities of the temporal difference methods to efficiently deal with the temporal
    credit assignment problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, with their guided ES algorithm (Maheswaranathan et al., [2019](#bib.bib57)),
    the authors study how a simple ES gradient estimator can be improved by leveraging
    knowledge of an approximate gradient suffering from bias and variance. Though
    their study is general, it is natural to apply it to the context where the approximate
    gradient is a policy gradient, in which case guided ES combines evolution and
    RL. This work is often cited in a very active recent trend which consists in improving
    the exploration capabilities of ES algorithms by drawing better than Gaussian
    directions to get a more informative gradient approximator (Choromanski et al.,
    [2018](#bib.bib8); [2019](#bib.bib7); Zhang et al., [2020](#bib.bib103); Dereventsov
    et al., [2022](#bib.bib15)). In particular, the sges algorithm (Liu et al., [2020](#bib.bib49))
    leverages both the guided ES idea and the improved exploration ideas to produce
    a competitive ES-based policy search algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper we have provided a list of all the algorithms combining evolutionary
    processes and deep reinforcement learning we could find, irrespective of the publication
    status of the corresponding papers. Our focus was on the mechanisms and our main
    contribution was to provide a categorization of these algorithms into several
    groups of methods, based on the role of evolutionary optimization in the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: We have not covered related fields such as algorithm combining deep RL and imitation
    learning, though at least one of them also includes evolution (Lü et al., [2021](#bib.bib54)).
    Besides, we have not covered works which focus on the implementation of evolution
    and deep RL combinations, such as (Lee et al., [2020](#bib.bib43)) which shows
    the importance of asynchronism in such combinations.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, the scope of the survey was still too broad to enable
    deeper analyses of the different combination methods or a comparative evaluation
    of their performance. In the future, we intend to focus separately on the different
    categories so as to provide these more in-depth analyses and perform comparative
    evaluation of these algorithms between each other and with respect to state of
    the art deep RL algorithms, based on a unified benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Our focus on elementary mechanisms also suggests the possibility to design new
    combinations of such mechanisms, that is combining the combinations. For instance,
    one may include into a single architecture the idea of selecting samples sent
    to the replay buffer so as to maximize the efficiency of the RL component, more
    efficient crossover or mutation operators as in pderl, soft policy updates, hyperparameter
    tuning etc. No doubt that such combinations will emerge in the future if they
    can result in additional performance gains, despite the additional implementation
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The author wants to thank Giuseppe Paolo, Stéphane Doncieux and Antonin Raffin
    for useful remarks about this manuscript as well as several colleagues from ISIR
    for their questions and remarks about the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bäck et al. (1997) Thomas Bäck, Ulrich Hammel, and H.-P. Schwefel. 1997. Evolutionary
    computation: Comments on the history and current state. *IEEE transactions on
    Evolutionary Computation* 1, 1 (1997), 3–17.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bharadhwaj et al. (2020) Homanga Bharadhwaj, Kevin Xie, and Florian Shkurti.
    2020. Model-predictive control via cross-entropy and gradient-based optimization.
    In *Learning for Dynamics and Control*. PMLR, 277–286.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bodnar et al. (2020) Cristian Bodnar, Ben Day, and Pietro Lió. 2020. Proximal
    Distilled Evolutionary Reinforcement Learning. In *The Thirty-Fourth AAAI Conference
    on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications
    of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on
    Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA,
    February 7-12, 2020*. AAAI Press, 3283–3290. [https://aaai.org/ojs/index.php/AAAI/article/view/5728](https://aaai.org/ojs/index.php/AAAI/article/view/5728)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chang et al. (2018) Simyung Chang, John Yang, Jaeseok Choi, and Nojun Kwak.
    2018. Genetic-Gated Networks for Deep Reinforcement Learning. In *Advances in
    Neural Information Processing Systems 31: Annual Conference on Neural Information
    Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada*,
    Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi,
    and Roman Garnett (Eds.). 1754–1763. [https://proceedings.neurips.cc/paper/2018/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen (2019) Gang Chen. 2019. Merging Deterministic Policy Gradient Estimations
    with Varied Bias-Variance Tradeoff for Effective Deep Reinforcement Learning.
    *ArXiv preprint* abs/1911.10527 (2019). [https://arxiv.org/abs/1911.10527](https://arxiv.org/abs/1911.10527)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choromanski et al. (2019) Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder,
    Yunhao Tang, and Vikas Sindhwani. 2019. From Complexity to Simplicity: Adaptive
    ES-Active Subspaces for Blackbox Optimization. In *Advances in Neural Information
    Processing Systems 32: Annual Conference on Neural Information Processing Systems
    2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada*, Hanna M. Wallach,
    Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman
    Garnett (Eds.). 10299–10309. [https://proceedings.neurips.cc/paper/2019/hash/88bade49e98db8790df275fcebb37a13-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/88bade49e98db8790df275fcebb37a13-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choromanski et al. (2018) Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani,
    Richard E. Turner, and Adrian Weller. 2018. Structured Evolution with Compact
    Architectures for Scalable Policy Optimization. In *Proceedings of the 35th International
    Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,
    July 10-15, 2018* *(Proceedings of Machine Learning Research)*, Jennifer G. Dy
    and Andreas Krause (Eds.), Vol. 80. PMLR, 969–977. [http://proceedings.mlr.press/v80/choromanski18a.html](http://proceedings.mlr.press/v80/choromanski18a.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chua et al. (2018) Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey
    Levine. 2018. Deep reinforcement learning in a handful of trials using probabilistic
    dynamics models. *Advances in neural information processing systems* 31 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cideron et al. (2020) Geoffrey Cideron, Thomas Pierrot, Nicolas Perrin, Karim
    Beguir, and Olivier Sigaud. 2020. QD-RL: Efficient Mixing of Quality and Diversity
    in Reinforcement Learning. *ArXiv preprint* abs/2006.08505 (2020). [https://arxiv.org/abs/2006.08505](https://arxiv.org/abs/2006.08505)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Colas et al. (2018) Cédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer.
    2018. GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning
    Algorithms. In *Proceedings of the 35th International Conference on Machine Learning,
    ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018* *(Proceedings
    of Machine Learning Research)*, Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80.
    PMLR, 1038–1047. [http://proceedings.mlr.press/v80/colas18a.html](http://proceedings.mlr.press/v80/colas18a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Colledanchise and Ögren (2018) Michele Colledanchise and Petter Ögren. 2018.
    *Behavior trees in robotics and AI: An introduction*. CRC Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conti et al. (2018) Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such,
    Joel Lehman, Kenneth O. Stanley, and Jeff Clune. 2018. Improving Exploration in
    Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking
    Agents. In *Advances in Neural Information Processing Systems 31: Annual Conference
    on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,
    Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 5032–5043. [https://proceedings.neurips.cc/paper/2018/hash/b1301141feffabac455e1f90a7de2054-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/b1301141feffabac455e1f90a7de2054-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cully and Demiris (2017) Antoine Cully and Yiannis Demiris. 2017. Quality and
    diversity optimization: A unifying modular framework. *IEEE Transactions on Evolutionary
    Computation* 22, 2 (2017), 245–259.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dereventsov et al. (2022) Anton Dereventsov, Clayton G. Webster, and Joseph
    Daws. 2022. An adaptive stochastic gradient-free approach for high-dimensional
    blackbox optimization. In *Proceedings of International Conference on Computational
    Intelligence*. Springer, 333–348.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doan et al. (2019) Thang Doan, Bogdan Mazoure, Audrey Durand, Joelle Pineau,
    and R Devon Hjelm. 2019. Attraction-Repulsion Actor-Critic for Continuous Control
    Reinforcement Learning. *ArXiv preprint* abs/1909.07543 (2019). [https://arxiv.org/abs/1909.07543](https://arxiv.org/abs/1909.07543)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Doncieux et al. (2019) Stephane Doncieux, Alban Laflaquière, and Alexandre
    Coninx. 2019. Novelty search: a theoretical perspective. In *Proceedings of the
    Genetic and Evolutionary Computation Conference*. 99–106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Drugan (2019) Madalina M. Drugan. 2019. Reinforcement learning versus evolutionary
    computation: A survey on hybrid algorithms. *Swarm and evolutionary computation*
    44 (2019), 228–246.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Espositi and Bonarini (2020) Federico Espositi and Andrea Bonarini. 2020. Gradient
    Bias to Solve the Generalization Limit of Genetic Algorithms Through Hybridization
    with Reinforcement Learning. In *International Conference on Machine Learning,
    Optimization, and Data Science*. Springer, 273–284.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Franke et al. (2021) Jörg K. H. Franke, Gregor Köhler, André Biedenkapp, and
    Frank Hutter. 2021. Sample-Efficient Automated Deep Reinforcement Learning. In
    *9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021*. OpenReview.net. [https://openreview.net/forum?id=hSjxQ3B7GWq](https://openreview.net/forum?id=hSjxQ3B7GWq)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fujimoto et al. (2018) Scott Fujimoto, Herke van Hoof, and David Meger. 2018.
    Addressing Function Approximation Error in Actor-Critic Methods. In *Proceedings
    of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
    Stockholm, Sweden, July 10-15, 2018* *(Proceedings of Machine Learning Research)*,
    Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80. PMLR, 1582–1591. [http://proceedings.mlr.press/v80/fujimoto18a.html](http://proceedings.mlr.press/v80/fujimoto18a.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gangwani and Peng (2018) Tanmay Gangwani and Jian Peng. 2018. Policy Optimization
    by Genetic Distillation. In *6th International Conference on Learning Representations,
    ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*.
    OpenReview.net. [https://openreview.net/forum?id=ByOnmlWC-](https://openreview.net/forum?id=ByOnmlWC-)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grigsby et al. (2021) Jake Grigsby, Jin Yong Yoo, and Yanjun Qi. 2021. Towards
    Automatic Actor-Critic Solutions to Continuous Control. *ArXiv preprint* abs/2106.08918
    (2021). [https://arxiv.org/abs/2106.08918](https://arxiv.org/abs/2106.08918)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta et al. (2021) Agrim Gupta, Silvio Savarese, Surya Ganguli, and Li Fei-Fei.
    2021. Embodied Intelligence via Learning and Evolution. *ArXiv preprint* abs/2102.02202
    (2021). [https://arxiv.org/abs/2102.02202](https://arxiv.org/abs/2102.02202)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ha (2019) David Ha. 2019. Reinforcement learning for improving agent design.
    *Artificial life* 25, 4 (2019), 352–365.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haarnoja et al. (2018) Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George
    Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel,
    et al. 2018. Soft actor-critic algorithms and applications. *ArXiv preprint* abs/1812.05905
    (2018). [https://arxiv.org/abs/1812.05905](https://arxiv.org/abs/1812.05905)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hafner et al. (2019) Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas,
    David Ha, Honglak Lee, and James Davidson. 2019. Learning latent dynamics for
    planning from pixels. In *International conference on machine learning*. PMLR,
    2555–2565.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hallawa et al. (2021) Ahmed Hallawa, Thorsten Born, Anke Schmeink, Guido Dartmann,
    Arne Peine, Lukas Martin, Giovanni Iacca, AE Eiben, and Gerd Ascheid. 2021. Evo-RL:
    evolutionary-driven reinforcement learning. In *Proceedings of the Genetic and
    Evolutionary Computation Conference Companion*. 153–154.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hallawa et al. (2017) Ahmed Hallawa, Jaro De Roose, Martin Andraud, Marian
    Verhelst, and Gerd Ascheid. 2017. Instinct-driven dynamic hardware reconfiguration:
    evolutionary algorithm optimized compression for autonomous sensory agents. In
    *Proceedings of the Genetic and Evolutionary Computation Conference Companion*.
    1727–1734.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holland and Reitman (1978) John H Holland and Judith S Reitman. 1978. Cognitive
    systems based on adaptive algorithms. In *Pattern-directed inference systems*.
    Elsevier, 313–329.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Houthooft et al. (2018) Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly C.
    Stadie, Filip Wolski, Jonathan Ho, and Pieter Abbeel. 2018. Evolved Policy Gradients.
    In *Advances in Neural Information Processing Systems 31: Annual Conference on
    Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,
    Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 5405–5414. [https://proceedings.neurips.cc/paper/2018/hash/7876acb66640bad41f1e1371ef30c180-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/7876acb66640bad41f1e1371ef30c180-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaderberg et al. (2019) Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke
    Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz,
    Ari S Morcos, Avraham Ruderman, et al. 2019. Human-level performance in 3D multiplayer
    games with population-based reinforcement learning. *Science* 364, 6443 (2019),
    859–865.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaderberg et al. (2017) Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M
    Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen
    Simonyan, et al. 2017. Population-based training of neural networks. *ArXiv preprint*
    abs/1711.09846 (2017). [https://arxiv.org/abs/1711.09846](https://arxiv.org/abs/1711.09846)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jung et al. (2020) Whiyoung Jung, Giseung Park, and Youngchul Sung. 2020. Population-Guided
    Parallel Policy Search for Reinforcement Learning. In *8th International Conference
    on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*.
    OpenReview.net. [https://openreview.net/forum?id=rJeINp4KwH](https://openreview.net/forum?id=rJeINp4KwH)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kalashnikov et al. (2018) Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian
    Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan,
    Vincent Vanhoucke, et al. 2018. Qt-opt: Scalable deep reinforcement learning for
    vision-based robotic manipulation. *ArXiv preprint* abs/1806.10293 (2018). [https://arxiv.org/abs/1806.10293](https://arxiv.org/abs/1806.10293)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khadka et al. (2019) Shauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel,
    Evren Tumer, Santiago Miret, Yinyin Liu, and Kagan Tumer. 2019. Collaborative
    Evolutionary Reinforcement Learning. In *Proceedings of the 36th International
    Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
    USA* *(Proceedings of Machine Learning Research)*, Kamalika Chaudhuri and Ruslan
    Salakhutdinov (Eds.), Vol. 97. PMLR, 3341–3350. [http://proceedings.mlr.press/v97/khadka19a.html](http://proceedings.mlr.press/v97/khadka19a.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khadka and Tumer (2018) Shauharda Khadka and Kagan Tumer. 2018. Evolution-Guided
    Policy Gradient in Reinforcement Learning. In *Advances in Neural Information
    Processing Systems 31: Annual Conference on Neural Information Processing Systems
    2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada*, Samy Bengio, Hanna M.
    Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett
    (Eds.). 1196–1208. [https://proceedings.neurips.cc/paper/2018/hash/85fc37b18c57097425b52fc7afbb6969-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/85fc37b18c57097425b52fc7afbb6969-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khamassi et al. (2017) Mehdi Khamassi, George Velentzas, Theodore Tsitsimis,
    and Costas Tzafestas. 2017. Active exploration and parameterized reinforcement
    learning applied to a simulated human-robot interaction task. In *2017 First IEEE
    International Conference on Robotic Computing (IRC)*. IEEE, 28–35.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2007) Kyung-Joong Kim, Heejin Choi, and Sung-Bae Cho. 2007. Hybrid
    of evolution and reinforcement learning for othello players. In *2007 IEEE Symposium
    on Computational Intelligence and Games*. IEEE, 203–209.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2020) Namyong Kim, Hyunsuk Baek, and Hayong Shin. 2020. PGPS: Coupling
    Policy Gradient with Population-based Search. In *Submitted to ICLR 2021*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koza et al. (1994) John R. Koza et al. 1994. *Genetic programming II*. Vol. 17.
    MIT press Cambridge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lanzi (1999) Pier Luca Lanzi. 1999. An analysis of generalization in the XCS
    classifier system. *Evolutionary computation* 7, 2 (1999), 125–149.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2020) Kyunghyun Lee, Byeong-Uk Lee, Ukcheol Shin, and In So Kweon.
    2020. An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based
    Policy Search. In *Advances in Neural Information Processing Systems 33: Annual
    Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
    6-12, 2020, virtual*, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
    Balcan, and Hsuan-Tien Lin (Eds.). [https://proceedings.neurips.cc/paper/2020/hash/731309c4bb223491a9f67eac5214fb2e-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/731309c4bb223491a9f67eac5214fb2e-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lehman et al. (2018) Joel Lehman, Jay Chen, Jeff Clune, and Kenneth O Stanley.
    2018. Safe mutations for deep and recurrent neural networks through output gradients.
    In *Proceedings of the Genetic and Evolutionary Computation Conference*. 117–124.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lehman and Stanley (2011) Joel Lehman and Kenneth O Stanley. 2011. Abandoning
    objectives: Evolution through the search for novelty alone. *Evolutionary computation*
    19, 2 (2011), 189–223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2022) Yuheng Lei, Jianyu Chen, Shengbo Eben Li, and Sifa Zheng.
    2022. Zeroth-Order Actor-Critic. *ArXiv preprint* abs/2201.12518 (2022). [https://arxiv.org/abs/2201.12518](https://arxiv.org/abs/2201.12518)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leite et al. (2020) Abe Leite, Madhavun Candadai, and Eduardo J Izquierdo.
    2020. Reinforcement learning beyond the Bellman equation: Exploring critic objectives
    using evolution. In *ALIFE 2020: The 2020 Conference on Artificial Life*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lillicrap et al. (2016) Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,
    Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous
    control with deep reinforcement learning. In *4th International Conference on
    Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference
    Track Proceedings*, Yoshua Bengio and Yann LeCun (Eds.). [http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) Fei-Yu Liu, Zi-Niu Li, and Chao Qian. 2020. Self-Guided Evolution
    Strategies with Historical Estimated Gradients. In *Proceedings of the Twenty-Ninth
    International Joint Conference on Artificial Intelligence, IJCAI 2020*, Christian
    Bessiere (Ed.). ijcai.org, 1474–1480. [https://doi.org/10.24963/ijcai.2020/205](https://doi.org/10.24963/ijcai.2020/205)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019) Guoqing Liu, Li Zhao, Feidiao Yang, Jiang Bian, Tao Qin, Nenghai
    Yu, and Tie-Yan Liu. 2019. Trust Region Evolution Strategies. In *The Thirty-Third
    AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative
    Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu,
    Hawaii, USA, January 27 - February 1, 2019*. AAAI Press, 4352–4359. [https://doi.org/10.1609/aaai.v33i01.33014352](https://doi.org/10.1609/aaai.v33i01.33014352)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Feng (2021) Jian Liu and Liming Feng. 2021. Diversity Evolutionary Policy
    Deep Reinforcement Learning. *Computational Intelligence and Neuroscience* 2021
    (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018) Qihao Liu, Yujia Wang, and Xiaofeng Liu. 2018. PNS: Population-Guided
    Novelty Search for Reinforcement Learning in Hard Exploration Environments. In
    *2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.
    IEEE, 5627–5634.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2017) Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. 2017.
    Stein Variational Policy Gradient. In *Proceedings of the Thirty-Third Conference
    on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August
    11-15, 2017*, Gal Elidan, Kristian Kersting, and Alexander T. Ihler (Eds.). AUAI
    Press. [http://auai.org/uai2017/proceedings/papers/239.pdf](http://auai.org/uai2017/proceedings/papers/239.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lü et al. (2021) Shuai Lü, Shuai Han, Wenbo Zhou, and Junwei Zhang. 2021. Recruitment-imitation
    mechanism for evolutionary reinforcement learning. *Information Sciences* 553
    (2021), 172–188.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luck et al. (2020) Kevin Sebastian Luck, Heni Ben Amor, and Roberto Calandra.
    2020. Data-efficient co-adaptation of morphology and behaviour with deep reinforcement
    learning. In *Conference on Robot Learning*. PMLR, 854–869.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2022) Yan Ma, Tianxing Liu, Bingsheng Wei, Yi Liu, Kang Xu, and Wei
    Li. 2022. Evolutionary Action Selection for Gradient-based Policy Learning. *ArXiv
    preprint* abs/2201.04286 (2022). [https://arxiv.org/abs/2201.04286](https://arxiv.org/abs/2201.04286)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maheswaranathan et al. (2019) Niru Maheswaranathan, Luke Metz, George Tucker,
    Dami Choi, and Jascha Sohl-Dickstein. 2019. Guided evolutionary strategies: augmenting
    random search with surrogate gradients. In *Proceedings of the 36th International
    Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
    USA* *(Proceedings of Machine Learning Research)*, Kamalika Chaudhuri and Ruslan
    Salakhutdinov (Eds.), Vol. 97\. PMLR, 4264–4273. [http://proceedings.mlr.press/v97/maheswaranathan19a.html](http://proceedings.mlr.press/v97/maheswaranathan19a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Majid et al. (2021) Amjad Yousef Majid, Serge Saaybi, Tomas van Rietbergen,
    Vincent Francois-Lavet, R. Venkatesha Prasad, and Chris Verhoeven. 2021. Deep
    Reinforcement Learning Versus Evolution Strategies: A Comparative Survey. *ArXiv
    preprint* abs/2110.01411 (2021). [https://arxiv.org/abs/2110.01411](https://arxiv.org/abs/2110.01411)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mania et al. (2018) Horia Mania, Aurelia Guy, and Benjamin Recht. 2018. Simple
    random search of static linear policies is competitive for reinforcement learning.
    In *Advances in Neural Information Processing Systems 31: Annual Conference on
    Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,
    Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 1805–1814. [https://proceedings.neurips.cc/paper/2018/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marchesini et al. (2021) Enrico Marchesini, Davide Corsi, and Alessandro Farinelli.
    2021. Genetic Soft Updates for Policy Evolution in Deep Reinforcement Learning.
    In *9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021*. OpenReview.net. [https://openreview.net/forum?id=TGFO0DbD_pk](https://openreview.net/forum?id=TGFO0DbD_pk)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miller and Harding (2009) Julian Francis Miller and Simon L Harding. 2009.
    Cartesian genetic programming. In *Proceedings of the 11th annual conference companion
    on genetic and evolutionary computation conference: late breaking papers*. 3489–3512.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. (2015) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A.
    Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K.
    Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement
    learning. *Nature* 518, 7540 (2015), 529.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mouret (2020) Jean-Baptiste Mouret. 2020. Evolving the behavior of machines:
    from micro to macroevolution. *Iscience* 23, 11 (2020), 101731.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nilsson and Cully (2021) Olle Nilsson and Antoine Cully. 2021. Policy gradient
    assisted MAP-Elites. In *Proceedings of the Genetic and Evolutionary Computation
    Conference*. 866–875.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park and Lee (2021) Jai Hoon Park and Kang Hoon Lee. 2021. Computational Design
    of Modular Robots Based on Genetic Algorithm and Reinforcement Learning. *Symmetry*
    13, 3 (2021), 471.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parker-Holder et al. (2020) Jack Parker-Holder, Aldo Pacchiano, Krzysztof Marcin
    Choromanski, and Stephen J. Roberts. 2020. Effective Diversity in Population Based
    Reinforcement Learning. In *Advances in Neural Information Processing Systems
    33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
    December 6-12, 2020, virtual*, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
    Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). [https://proceedings.neurips.cc/paper/2020/hash/d1dc3a8270a6f9394f88847d7f0050cf-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/d1dc3a8270a6f9394f88847d7f0050cf-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parker-Holder et al. (2022) Jack Parker-Holder, Raghu Rajan, Xingyou Song,
    André Biedenkapp, Yingjie Miao, Theresa Eimer, Baohe Zhang, Vu Nguyen, Roberto
    Calandra, Aleksandra Faust, et al. 2022. Automated Reinforcement Learning (AutoRL):
    A Survey and Open Problems. *ArXiv preprint* abs/2201.03916 (2022). [https://arxiv.org/abs/2201.03916](https://arxiv.org/abs/2201.03916)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2019) Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
    2019. Advantage-weighted regression: Simple and scalable off-policy reinforcement
    learning. *ArXiv preprint* abs/1910.00177 (2019). [https://arxiv.org/abs/1910.00177](https://arxiv.org/abs/1910.00177)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pierrot et al. (2020) Thomas Pierrot, Valentin Macé, Geoffrey Cideron, Nicolas
    Perrin, Karim Beguir, and Olivier Sigaud. 2020. Sample efficient Quality Diversity
    for neural continuous control. *unpublished* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pierrot et al. (2018) Thomas Pierrot, Nicolas Perrin, and Olivier Sigaud. 2018.
    First-order and second-order variants of the gradient descent in a unified framework.
    *ArXiv preprint* abs/1810.08102 (2018). [https://arxiv.org/abs/1810.08102](https://arxiv.org/abs/1810.08102)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinneri et al. (2020) Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes,
    Jan Achterhold, Joerg Stueckler, Michal Rolinek, and Georg Martius. 2020. Sample-efficient
    cross-entropy method for real-time planning. *ArXiv preprint* abs/2008.06389 (2020).
    [https://arxiv.org/abs/2008.06389](https://arxiv.org/abs/2008.06389)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinneri et al. (2021) Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes,
    and Georg Martius. 2021. Extracting Strong Policies for Robotics Tasks from Zero-Order
    Trajectory Optimizers. In *9th International Conference on Learning Representations,
    ICLR 2021, Virtual Event, Austria, May 3-7, 2021*. OpenReview.net. [https://openreview.net/forum?id=Nc3TJqbcl3](https://openreview.net/forum?id=Nc3TJqbcl3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pourchot and Sigaud (2019) Aloïs Pourchot and Olivier Sigaud. 2019. CEM-RL:
    Combining evolutionary and gradient-based methods for policy search. In *7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019*. OpenReview.net. [https://openreview.net/forum?id=BkeU5j0ctQ](https://openreview.net/forum?id=BkeU5j0ctQ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pugh et al. (2016) Justin K. Pugh, Lisa B. Soros, and Kenneth O. Stanley. 2016.
    Quality diversity: A new frontier for evolutionary computation. *Frontiers in
    Robotics and AI* 3 (2016), 40.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qian and Yu (2021) Hong Qian and Yang Yu. 2021. Derivative-free reinforcement
    learning: A review. *ArXiv preprint* abs/2102.05710 (2021). [https://arxiv.org/abs/2102.05710](https://arxiv.org/abs/2102.05710)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salimans et al. (2017) Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and
    Ilya Sutskever. 2017. Evolution strategies as a scalable alternative to reinforcement
    learning. *ArXiv preprint* abs/1703.03864 (2017). [https://arxiv.org/abs/1703.03864](https://arxiv.org/abs/1703.03864)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael I.
    Jordan, and Philipp Moritz. 2015. Trust Region Policy Optimization. In *Proceedings
    of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France,
    6-11 July 2015* *(JMLR Workshop and Conference Proceedings)*, Francis R. Bach
    and David M. Blei (Eds.), Vol. 37. JMLR.org, 1889–1897. [http://proceedings.mlr.press/v37/schulman15.html](http://proceedings.mlr.press/v37/schulman15.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *ArXiv
    preprint* abs/1707.06347 (2017). [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sehgal et al. (2019) Adarsh Sehgal, Hung La, Sushil Louis, and Hai Nguyen. 2019.
    Deep reinforcement learning using genetic algorithm for parameter optimization.
    In *2019 Third IEEE International Conference on Robotic Computing (IRC)*. IEEE,
    596–601.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sehgal et al. (2022) Adarsh Sehgal, Nicholas Ward, Hung Manh La, Christos Papachristos,
    and Sushil Louis. 2022. GA-DRL: Genetic Algorithm-Based Function Optimizer in
    Deep Reinforcement Learning for Robotic Manipulation Tasks. *ArXiv preprint* abs/2203.00141
    (2022). [https://arxiv.org/abs/2203.00141](https://arxiv.org/abs/2203.00141)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al. (2021) Lin Shao, Yifan You, Mengyuan Yan, Shenli Yuan, Qingyun
    Sun, and Jeannette Bohg. 2021. GRAC: Self-guided and self-regularized actor-critic.
    In *Conference on Robot Learning*. PMLR, 267–276.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2019) Longxiang Shi, Shijian Li, Longbing Cao, Long Yang, Gang
    Zheng, and Gang Pan. 2019. FiDi-RL: Incorporating Deep Reinforcement Learning
    with Finite-Difference Policy Search for Efficient Learning of Continuous Control.
    *ArXiv preprint* abs/1907.00526 (2019). [https://arxiv.org/abs/1907.00526](https://arxiv.org/abs/1907.00526)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2020) Longxiang Shi, Shijian Li, Qian Zheng, Min Yao, and Gang Pan.
    2020. Efficient novelty search through deep reinforcement learning. *IEEE Access*
    8 (2020), 128809–128818.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi and Singh (2021) Zhenyang Shi and Surya PN Singh. 2021. Soft Actor-Critic
    with Cross-Entropy Policy Optimization. *ArXiv preprint* abs/2112.11115 (2021).
    [https://arxiv.org/abs/2112.11115](https://arxiv.org/abs/2112.11115)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sigaud and Stulp (2019) Olivier Sigaud and Freek Stulp. 2019. Policy Search
    in Continuous Action Domains: an Overview. *Neural Networks* 113 (2019), 28–40.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sigaud and Wilson (2007) Olivier Sigaud and S. W. Wilson. 2007. Learning Classifier
    Systems: A Survey. *Journal of Soft Computing* 11, 11 (2007), 1065–1078.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simmons-Edler et al. (2019) Riley Simmons-Edler, Ben Eisner, Eric Mitchell,
    Sebastian Seung, and Daniel Lee. 2019. Q-learning for continuous actions with
    cross-entropy guided policies. *ArXiv preprint* abs/1903.10605 (2019). [https://arxiv.org/abs/1903.10605](https://arxiv.org/abs/1903.10605)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simpson (1953) George Gaylord Simpson. 1953. The baldwin effect. *Evolution*
    7, 2 (1953), 110–117.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stork et al. (2021) Jörg Stork, Martin Zaefferer, Nils Eisler, Patrick Tichelmann,
    Thomas Bartz-Beielstein, and AE Eiben. 2021. Behavior-based neuroevolutionary
    training in reinforcement learning. In *Proceedings of the Genetic and Evolutionary
    Computation Conference Companion*. 1753–1761.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2020) Hao Sun, Ziping Xu, Yuhang Song, Meng Fang, Jiechao Xiong,
    Bo Dai, and Bolei Zhou. 2020. Zeroth-order supervised policy improvement. *ArXiv
    preprint* abs/2006.06600 (2020). [https://arxiv.org/abs/2006.06600](https://arxiv.org/abs/2006.06600)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suri et al. (2020) Karush Suri, Xiao Qi Shi, Konstantinos N. Plataniotis, and
    Yuri A. Lawryshyn. 2020. Maximum Mutation Reinforcement Learning for Scalable
    Control. *ArXiv preprint* abs/2007.13690 (2020). [https://arxiv.org/abs/2007.13690](https://arxiv.org/abs/2007.13690)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton and Barto (2018) Richard S. Sutton and Andrew G. Barto. 2018. *Reinforcement
    learning: An introduction*. MIT Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang (2021) Yunhao Tang. 2021. Guiding Evolutionary Strategies with Off-Policy
    Actor-Critic. In *Proceedings of the 20th International Conference on Autonomous
    Agents and MultiAgent Systems*. 1317–1325.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang and Choromanski (2020) Yunhao Tang and Krzysztof Choromanski. 2020. Online
    hyper-parameter tuning in off-policy learning via evolutionary strategies. *ArXiv
    preprint* abs/2006.07554 (2020). [https://arxiv.org/abs/2006.07554](https://arxiv.org/abs/2006.07554)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tangri et al. (2022) Rohan Tangri, Danilo P. Mandic, and Anthony G. Constantinides.
    2022. Pearl: Parallel Evolutionary and Reinforcement Learning Library. *ArXiv
    preprint* abs/2201.09568 (2022). [https://arxiv.org/abs/2201.09568](https://arxiv.org/abs/2201.09568)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tjanaka et al. (2022) Bryon Tjanaka, Matthew C Fontaine, Julian Togelius, and
    Stefanos Nikolaidis. 2022. Approximating Gradients for Differentiable Quality
    Diversity in Reinforcement Learning. *ArXiv preprint* abs/2202.03666 (2022). [https://arxiv.org/abs/2202.03666](https://arxiv.org/abs/2202.03666)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Todd et al. (2020) Graham Todd, Madhavun Candadai, and Eduardo J. Izquierdo.
    2020. Interaction between evolution and learning in nk fitness landscapes. In
    *Artificial Life Conference Proceedings*. MIT Press, 761–767.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Ba (2019) Tingwu Wang and Jimmy Ba. 2019. Exploring model-based planning
    with policy networks. *arXiv preprint arXiv:1906.08649* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Yuxing Wang, Tiantian Zhang, Yongzhe Chang, Bin Liang, Xueqian
    Wang, and Bo Yuan. 2022. A Surrogate-Assisted Controller for Expensive Evolutionary
    Reinforcement Learning. *ArXiv preprint* abs/2201.00129 (2022). [https://arxiv.org/abs/2201.00129](https://arxiv.org/abs/2201.00129)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Rémi
    Munos, Koray Kavukcuoglu, and Nando de Freitas. 2017. Sample Efficient Actor-Critic
    with Experience Replay. In *5th International Conference on Learning Representations,
    ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*. OpenReview.net.
    [https://openreview.net/forum?id=HyM25Mqel](https://openreview.net/forum?id=HyM25Mqel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weber and Depew (2003) Bruce H. Weber and David J. Depew. 2003. *Evolution
    and learning: The Baldwin effect reconsidered*. Mit Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wierstra et al. (2014) Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun,
    Jan Peters, and Jürgen Schmidhuber. 2014. Natural evolution strategies. *The Journal
    of Machine Learning Research* 15, 1 (2014), 949–980.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020) Jiaxing Zhang, Hoang Tran, and Guannan Zhang. 2020. Accelerating
    Reinforcement Learning with a Directional-Gaussian-Smoothing Evolution Strategy.
    *ArXiv preprint* abs/2002.09077 (2020). [https://arxiv.org/abs/2002.09077](https://arxiv.org/abs/2002.09077)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2020) Han Zheng, Pengfei Wei, Jing Jiang, Guodong Long, Qinghua
    Lu, and Chengqi Zhang. 2020. Cooperative Heterogeneous Deep Reinforcement Learning.
    In *Advances in Neural Information Processing Systems 33: Annual Conference on
    Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual*, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
    and Hsuan-Tien Lin (Eds.). [https://proceedings.neurips.cc/paper/2020/hash/ca3a9be77f7e88708afb20c8cdf44b60-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/ca3a9be77f7e88708afb20c8cdf44b60-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
