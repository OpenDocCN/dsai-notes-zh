- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:47:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:47:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2203.14009] Combining Evolution and Deep Reinforcement Learning for Policy
    Search: a Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2203.14009] 结合进化和深度强化学习进行策略搜索：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2203.14009](https://ar5iv.labs.arxiv.org/html/2203.14009)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2203.14009](https://ar5iv.labs.arxiv.org/html/2203.14009)
- en: 'Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合进化和深度强化学习进行策略搜索：综述
- en: '[![[Uncaptioned image]](img/7a79f54fc141ff6f269955be00e2e38d.png) Olivier Sigaud](https://orcid.org/0000-0002-8544-0229),'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[![[未标注图片]](img/7a79f54fc141ff6f269955be00e2e38d.png) 奥利维耶·西戈](https://orcid.org/0000-0002-8544-0229)，'
- en: Sorbonne Université, CNRS, Institut des Systèmes Intelligents et de Robotique,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 索邦大学，CNRS，智能系统与机器人研究所，
- en: F-75005 Paris, France
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: F-75005 巴黎，法国
- en: Olivier.Sigaud@isir.upmc.fr
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Olivier.Sigaud@isir.upmc.fr
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep neuroevolution and deep Reinforcement Learning have received a lot of attention
    in the last years. Some works have compared them, highlighting theirs pros and
    cons, but an emerging trend consists in combining them so as to benefit from the
    best of both worlds. In this paper, we provide a survey of this emerging trend
    by organizing the literature into related groups of works and casting all the
    existing combinations in each group into a generic framework. We systematically
    cover all easily available papers irrespective of their publication status, focusing
    on the combination mechanisms rather than on the experimental results. In total,
    we cover 45 algorithms more recent than 2017\. We hope this effort will favor
    the growth of the domain by facilitating the understanding of the relationships
    between the methods, leading to deeper analyses, outlining missing useful comparisons
    and suggesting new combinations of mechanisms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经进化和深度强化学习近年来受到了很多关注。一些研究对它们进行了比较，突出了它们的优缺点，但一个新兴的趋势是将它们结合起来，以便从两者的优势中受益。在本文中，我们通过将文献组织成相关的研究组，并将每组中的所有现有组合纳入一个通用框架，来提供对这一新兴趋势的综述。我们系统地覆盖了所有易于获取的论文，不论其出版状态如何，重点关注组合机制而不是实验结果。总共，我们覆盖了2017年之后的45种算法。我们希望这一努力将有助于领域的发展，促进对方法之间关系的理解，从而进行更深入的分析，指出缺失的有用比较，并提出新的机制组合。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The idea that the extraordinary adaptive capabilities of living species results
    from a combination of evolutionary mechanisms acting at the level of a population
    and learning mechanisms acting at the level of individuals is ancient in life
    sciences (Simpson, [1953](#bib.bib88)) and has inspired early work in Artificial
    Intelligence (AI) research (Holland and Reitman, [1978](#bib.bib30)). This early
    starting point has led to the independent growth of two bodies of formal frameworks,
    evolutionary methods and reinforcement learning (RL). The early history of the
    evolutionary side is well covered in Bäck et al. ([1997](#bib.bib2)) and from
    the RL side in Sutton and Barto ([2018](#bib.bib92)). Despite these independent
    developments, research dedicated to the combination has remained active, in particular
    around Learning Classifier Systems (Lanzi, [1999](#bib.bib42); Sigaud and Wilson,
    [2007](#bib.bib86)) and studies of the Baldwin effect (Weber and Depew, [2003](#bib.bib101)).
    A broader perspective and survey on all the evolutionary and RL combinations anterior
    to the advent of the so called ”deep learning” methods using large neural networks
    can be found in Drugan ([2019](#bib.bib18)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 认为生物物种的非凡适应能力源于在种群层面上作用的进化机制和在个体层面上作用的学习机制的结合，这一思想在生命科学中已有悠久历史（Simpson，[1953](#bib.bib88)），并且激发了早期的人工智能（AI）研究（Holland
    和 Reitman，[1978](#bib.bib30)）。这一早期起点导致了两种正式框架的独立发展，即进化方法和强化学习（RL）。进化方面的早期历史在 Bäck
    等（[1997](#bib.bib2)）中有很好的覆盖，强化学习方面则在 Sutton 和 Barto（[2018](#bib.bib92)）中得到了描述。尽管这些独立的发展，专注于组合的研究仍然保持活跃，特别是在学习分类器系统（Lanzi，[1999](#bib.bib42)；Sigaud
    和 Wilson，[2007](#bib.bib86)）和鲍德温效应的研究（Weber 和 Depew，[2003](#bib.bib101)）方面。关于所有进化和强化学习组合的更广泛视角和综述可以在
    Drugan（[2019](#bib.bib18)）中找到。
- en: In this paper, we propose a survey of a renewed approach to this combination
    that builds on the unprecedented progress made possible in evolutionary and deep
    RL methods by the growth of computational power and the availability of efficient
    libraries to use deep neural networks. As this survey shows, the topic is rapidly
    gaining popularity with a wide variety of approaches and even emerging libraries
    dedicated to their implementation (Tangri et al., [2022](#bib.bib95)). Thus we
    believe it is the right time for laying solid foundations to this growing field,
    by listing the approaches and providing a unified view that encompasses them.
    There are recent surveys about the comparison of evolutionary and RL methods (Qian
    and Yu, [2021](#bib.bib75); Majid et al., [2021](#bib.bib58)) which mention the
    emergence of some of these combinations. With respect to these surveys, ours is
    strictly focused on the combinations and attempts to provide a list of relevant
    papers as exhaustive as possible at the time of its publication, irrespective
    of their publication status.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了对这种结合的新方法的调查，这种方法建立在进化和深度RL方法因计算能力的提升和高效深度神经网络库的可用性而取得的前所未有的进展上。正如本调查所示，该主题正迅速获得广泛关注，出现了各种不同的方法，甚至有专门实施这些方法的库（Tangri等，[2022](#bib.bib95)）。因此，我们认为现在是为这个不断增长的领域奠定坚实基础的最佳时机，通过列出这些方法并提供一个统一的视角来涵盖它们。最近有关于进化和RL方法比较的综述（Qian和Yu，[2021](#bib.bib75)；Majid等，[2021](#bib.bib58)）提到了一些这些组合的出现。与这些综述相比，我们的调查严格关注这些组合，并尝试在发布时尽可能详尽地列出相关论文，无论其出版状态如何。
- en: 'This survey is organized into groups of algorithms using the evolutionary part
    for the same purpose. In Section [2](#S2 "2 Evolution of policies for performance
    ‣ Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey"),
    we first review algorithms where evolution is looking for efficient policies,
    that is combining deep neuroevolution and deep RL. We then cover in Section [3](#S3
    "3 Evolution of actions for performance ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey") algorithms where evolution directly looks
    for efficient actions in a given state rather than for policies. In Section [4](#S4
    "4 Evolution of policies for diversity ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey"), we cover the combination of deep RL algorithm
    with diversity seeking methods. Finally, in Section [5](#S5 "5 Evolution of something
    else ‣ Combining Evolution and Deep Reinforcement Learning for Policy Search:
    a Survey"), we cover various other uses of evolutionary methods, such as optimizing
    hyperparameters or the system’s morphology. To keep the survey as short as possible,
    we consider that the reader is familiar with evolutionary and RL methods in the
    context of policy search, and has a good understanding of their respective advantages
    and weaknesses. We refer the reader to Sigaud and Stulp ([2019](#bib.bib85)) for
    an introduction of the methods and to surveys about comparisons to know more about
    their pros and cons (Qian and Yu, [2021](#bib.bib75); Majid et al., [2021](#bib.bib58)).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查将算法分组，利用进化部分达到相同目的。在第[2](#S2 "2 性能策略的演变 ‣ 结合进化和深度强化学习进行策略搜索：综述")节中，我们首先回顾了那些进化在寻找高效策略的算法，即结合深度神经进化和深度RL的算法。接着在第[3](#S3
    "3 动作的演变以提升性能 ‣ 结合进化和深度强化学习进行策略搜索：综述")节中，我们涵盖了那些进化直接寻找给定状态下高效动作的算法，而非策略的算法。在第[4](#S4
    "4 策略的多样性演变 ‣ 结合进化和深度强化学习进行策略搜索：综述")节中，我们讨论了深度RL算法与寻求多样性方法的结合。最后，在第[5](#S5 "5
    其他方面的演变 ‣ 结合进化和深度强化学习进行策略搜索：综述")节中，我们涵盖了进化方法的各种其他应用，如优化超参数或系统的形态。为了使调查尽可能简短，我们假设读者熟悉策略搜索中的进化和RL方法，并对其各自的优缺点有较好的理解。我们建议读者参考Sigaud和Stulp（[2019](#bib.bib85)）了解方法介绍，并通过关于比较的综述了解其优缺点（Qian和Yu，[2021](#bib.bib75)；Majid等，[2021](#bib.bib58)）。
- en: 2 Evolution of policies for performance
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 性能策略的演变
- en: 'The methods in our first family combine a deep neuroevolution loop and a deep
    RL loop. Figure [1](#S2.F1 "Figure 1 ‣ 2 Evolution of policies for performance
    ‣ Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey")
    provides a generic template to illustrate such combinations. The central question
    left open by the template is how both loops interact with each other. Note that
    this template is not much adapted to account for works where the combination is
    purely sequential, such as Kim et al. ([2007](#bib.bib39)) or the gep-pg algorithm
    (Colas et al., [2018](#bib.bib11)). Besides, to avoid any confusion with the multi-agent
    setting, note that agents are interacting in isolation with their own copy of
    the environment and cannot interact with each other.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的第一个家族的方法结合了深度神经进化循环和深度强化学习循环。图[1](#S2.F1 "Figure 1 ‣ 2 Evolution of policies
    for performance ‣ Combining Evolution and Deep Reinforcement Learning for Policy
    Search: a Survey")提供了一个通用模板来展示这种组合。模板留下的核心问题是两个循环如何相互作用。请注意，这个模板并不太适用于那些组合纯粹是顺序的工作，比如
    Kim 等 ([2007](#bib.bib39)) 或者 gep-pg 算法（Colas 等，[2018](#bib.bib11)）。此外，为避免与多智能体设置混淆，请注意，智能体在与自身环境副本的孤立互动中，不能相互作用。'
- en: '![Refer to caption](img/d9875312c88142db8d3cc36407100064.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d9875312c88142db8d3cc36407100064.png)'
- en: 'Figure 1: The general template of algorithms combining deep neuroevolution
    and deep RL. A population of agents interact with an environment, and produce
    trajectories composed of states, actions and rewards. From the left-hand side,
    an evolutionary loop selects and evolves these agents based on their fitness,
    which is computed holistically over trajectories. From the right-hand side, a
    deep RL loop improves one or several agents using a gradient computed over the
    elementary steps of trajectories stored into a replay buffer.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：结合深度神经进化和深度强化学习的算法通用模板。一个智能体群体与环境互动，产生由状态、动作和奖励组成的轨迹。从左侧，一个进化循环根据智能体的适应度选择和进化这些智能体，这种适应度是基于轨迹整体计算的。从右侧，一个深度强化学习循环使用存储在重放缓冲区中的轨迹的基本步骤计算的梯度来改进一个或多个智能体。
- en: 'Table 1: Combinations evolving policies for performance. The table states whether
    the algorithms in the rows use the mechanisms in the columns. The colors are as
    follows. In the column about other combination mechanisms (+ Comb. Mech.): Critic
    gradient addition $\bullet$ (green), Population from Actor $\spadesuit$ (blue),
    None x (red). In all other columns: $\bullet$ (green): yes, x (red): no. In bnet,
    BBNE stands for Behavior-Based NeuroEvolution and CPG stands for Cartesian Genetic
    Programming (Miller and Harding, [2009](#bib.bib61)). The different GA labels
    stand for various genetic algorithms, we do not go into the details.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：演变策略的组合。表中说明了行中的算法是否使用列中的机制。颜色说明如下。关于其他组合机制（+ 组合机制）：Critic 梯度加法 $\bullet$（绿色），来自
    Actor 的种群 $\spadesuit$（蓝色），无 x（红色）。在所有其他列中：$\bullet$（绿色）：是，x（红色）：否。在 bnet 中，BBNE
    代表基于行为的神经进化，CPG 代表笛卡尔遗传编程（Miller 和 Harding，[2009](#bib.bib61)）。不同的 GA 标签代表各种遗传算法，我们不详细讨论。
- en: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3) scale(1,
    -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15) scale(1,
    -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | RL algo. | Evo. algo. | Actor Injec. | + Comb. Mech. | Surr. Fitness | Soft
    Update | Buffer Filt. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3) scale(1,
    -1)"><foreignobject width="31.9" height="12.3" overflow="visible">算法</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15) scale(1,
    -1)"><foreignobject width="33.29" height="12.15" overflow="visible">属性</foreignobject></g></g></g></svg>
    | RL 算法 | 进化算法 | Actor 注入 | + 组合机制 | 适应度 | 软更新 | 缓冲过滤 |'
- en: '| erl Khadka and Tumer ([2018](#bib.bib37)) | ddpg | GA | $\bullet$ | x | x
    | x | x |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| erl Khadka 和 Tumer ([2018](#bib.bib37)) | ddpg | GA | $\bullet$ | x | x |
    x | x |'
- en: '| cerl Khadka et al. ([2019](#bib.bib36)) | td3 | GA | $\bullet$ | x | x |
    x | x |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| cerl Khadka 等 ([2019](#bib.bib36)) | td3 | GA | $\bullet$ | x | x | x | x
    |'
- en: '| pderl Bodnar et al. ([2020](#bib.bib4)) | td3 | GA | $\bullet$ | x | x |
    x | x |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| pderl Bodnar 等 ([2020](#bib.bib4)) | td3 | GA | $\bullet$ | x | x | x | x
    |'
- en: '| esac Suri et al. ([2020](#bib.bib91)) | sac | ES | $\bullet$ | x | x | x
    | x |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| esac Suri 等 ([2020](#bib.bib91)) | sac | ES | $\bullet$ | x | x | x | x |'
- en: '| fidi-rl Shi et al. ([2019](#bib.bib82)) | ddpg | ars | $\bullet$ | x | x
    | x | x |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| fidi-rl Shi 等人 ([2019](#bib.bib82)) | ddpg | ars | $\bullet$ | x | x | x
    | x |'
- en: '| x-ddpg Espositi and Bonarini ([2020](#bib.bib19)) | ddpg | GA | $\bullet$
    | x | x | x | x |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| x-ddpg Espositi 和 Bonarini ([2020](#bib.bib19)) | ddpg | GA | $\bullet$ |
    x | x | x | x |'
- en: '| cem-rl Pourchot and Sigaud ([2019](#bib.bib73)) | td3 | cem | x | $\bullet$
    | x | x | x |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| cem-rl Pourchot 和 Sigaud ([2019](#bib.bib73)) | td3 | cem | x | $\bullet$
    | x | x | x |'
- en: '| cem-acer Tang ([2021](#bib.bib93)) | acer | cem | x | $\bullet$ | x | x |
    x |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| cem-acer Tang ([2021](#bib.bib93)) | acer | cem | x | $\bullet$ | x | x |
    x |'
- en: '| serl Wang et al. ([2022](#bib.bib99)) | ddpg | GA | $\bullet$ | x | $\bullet$
    | x | x |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| serl 王等人 ([2022](#bib.bib99)) | ddpg | GA | $\bullet$ | x | $\bullet$ | x
    | x |'
- en: '| spderl Wang et al. ([2022](#bib.bib99)) | td3 | GA | $\bullet$ | x | $\bullet$
    | x | x |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| spderl 王等人 ([2022](#bib.bib99)) | td3 | GA | $\bullet$ | x | $\bullet$ |
    x | x |'
- en: '| pgps Kim et al. ([2020](#bib.bib40)) | td3 | cem | $\bullet$ | x | $\bullet$
    | $\bullet$ | x |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| pgps Kim 等人 ([2020](#bib.bib40)) | td3 | cem | $\bullet$ | x | $\bullet$
    | $\bullet$ | x |'
- en: '| bnet Stork et al. ([2021](#bib.bib89)) | BBNE | CPG | $\bullet$ | x | $\bullet$
    | x | x |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| bnet Stork 等人 ([2021](#bib.bib89)) | BBNE | CPG | $\bullet$ | x | $\bullet$
    | x | x |'
- en: '| cspc Zheng et al. ([2020](#bib.bib104)) | sac + ppo | cem | $\bullet$ | x
    | x | x | $\bullet$ |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| cspc Zheng 等人 ([2020](#bib.bib104)) | sac + ppo | cem | $\bullet$ | x | x
    | x | $\bullet$ |'
- en: '| supe-rl Marchesini et al. ([2021](#bib.bib60)) | rainbow or ppo | GA | $\bullet$
    | $\spadesuit$ | x | $\bullet$ | x |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| supe-rl Marchesini 等人 ([2021](#bib.bib60)) | rainbow 或 ppo | GA | $\bullet$
    | $\spadesuit$ | x | $\bullet$ | x |'
- en: '| g2ac Chang et al. ([2018](#bib.bib5)) | a2c | GA | x | $\spadesuit$ | x |
    x | x |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| g2ac Chang 等人 ([2018](#bib.bib5)) | a2c | GA | x | $\spadesuit$ | x | x |
    x |'
- en: '| g2ppo Chang et al. ([2018](#bib.bib5)) | ppo | GA | x | $\spadesuit$ | x
    | x | x |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| g2ppo Chang 等人 ([2018](#bib.bib5)) | ppo | GA | x | $\spadesuit$ | x | x
    | x |'
- en: The main motivation for combining evolution and deep RL is the improved performance
    that may result from the combination. For instance, through simple experiments
    with simple fitness landscapes and simplified versions of the components, combining
    evolution and RL can be shown to work better than using either of the two in isolation
    (Todd et al., [2020](#bib.bib97)). Why is this so? One of the explanations is
    the following. A weakness of policy gradient methods at the heart of deep RL is
    that they compute an estimate of the true gradient based on a limited set of samples.
    This gradient can be quite wrong due to the high variance of the estimation, but
    it is applied blindly to the current policy without checking that this actually
    improves it. By contrast, variation-selection methods at the heart of evolutionary
    methods evaluate all the policies they generate and remove the poorly performing
    ones. Thus a first good reason to combine policy gradient and variation-selection
    methods is that the latter may remove policies that have been deteriorated by
    the gradient step. Below we list different approaches building on this idea. This
    perspective is the one that gives rise to the largest list of combinations. We
    further split this list into several groups of works in the following sections.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 结合进化与深度强化学习的主要动机是组合可能带来的性能提升。例如，通过对简单的适应度景观和简化版组件的简单实验，结合进化和强化学习可以显示出比单独使用任一方法效果更好（Todd
    等人，[2020](#bib.bib97)）。这是为什么呢？其中一个解释如下。深度强化学习中策略梯度方法的一个弱点是，它们基于有限的样本集计算真实梯度的估计值。由于估计的高方差，这个梯度可能非常不准确，但它会盲目地应用于当前策略，而没有检查这是否确实改善了策略。相比之下，进化方法中的变异选择方法评估它们生成的所有策略，并剔除表现不佳的策略。因此，结合策略梯度和变异选择方法的第一个好理由是，后者可能会剔除那些已经因梯度步骤而恶化的策略。下面我们列出了基于这一理念的不同方法。这一视角产生了最多的组合列表。我们在以下章节中进一步将这个列表分成几个组。
- en: 2.1 Deep RL actor injection
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 深度强化学习演员注入
- en: One of the main algorithms at the origin of the renewal of combining evolution
    and RL is erl (Khadka and Tumer, [2018](#bib.bib37)), see Figure LABEL:fig:erl_cerl.
    It was published simultaneously with the g2ac and g2ppo algorithms (Chang et al.,
    [2018](#bib.bib5)) but its impact was much greater. Its combination mechanism
    consists in injecting the RL actor into the evolutionary population.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 结合进化与强化学习的复兴的主要算法之一是 erl (Khadka 和 Tumer，[2018](#bib.bib37))，见图 LABEL:fig:erl_cerl。它与
    g2ac 和 g2ppo 算法（Chang 等人，[2018](#bib.bib5)）同时发布，但其影响更大。其结合机制在于将强化学习演员注入到进化种群中。
- en: '![Refer to caption](img/ce01cd169992b805e1d685adc1cbd51a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ce01cd169992b805e1d685adc1cbd51a.png)'
- en: ((a))
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ((a))
- en: '![Refer to caption](img/bafb19b1850035f62074c59820f7c8e5.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bafb19b1850035f62074c59820f7c8e5.png)'
- en: ((b))
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ((b))
- en: 'Figure 2: The template architecture for erl, esac, fidi-rl and cerl (a) and
    the pderl architecture (b). In erl, an actor learned by ddpg (Lillicrap et al.,
    [2016](#bib.bib48)) is periodically injected into the population and submitted
    to evolutionary selection. If ddpg performs better than the GA, this will accelerate
    the evolutionary process. Otherwise the ddpg agent is just ignored. In esac, ddpg
    is replaced by sac (Haarnoja et al., [2018](#bib.bib26)) and in fidi-rl, the GA
    is replaced by ars (Mania et al., [2018](#bib.bib59)). In cerl, the ddpg agent
    is replaced by a set of td3 actors sharing the same replay buffer, but each using
    a different discount factor. Again, those of such actors that perform better than
    the rest of the population are kept and enhance the evolutionary process, whereas
    the rest is discarded by evolutionary selection. In pderl, the genetic operators
    of erl are replaced by operators using local replay buffers so as to better leverage
    the step-based experience of each agent.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：erl、esac、fidi-rl 和 cerl（a）以及 pderl 架构（b）的模板架构。在 erl 中，一个由 ddpg（Lillicrap
    等，[2016](#bib.bib48)）学习到的演员定期被注入到种群中并进行进化选择。如果 ddpg 的表现优于 GA，这将加速进化过程。否则，ddpg
    代理将被忽略。在 esac 中，ddpg 被 sac（Haarnoja 等，[2018](#bib.bib26)）替代，在 fidi-rl 中，GA 被 ars（Mania
    等，[2018](#bib.bib59)）替代。在 cerl 中，ddpg 代理被一组 td3 演员替代，这些演员共享相同的回放缓冲区，但每个演员使用不同的折扣因子。再一次，那些表现优于其余种群的演员被保留并增强进化过程，而其余的则通过进化选择被丢弃。在
    pderl 中，erl 的遗传操作符被使用局部回放缓冲区的操作符替代，以更好地利用每个代理的基于步长的经验。
- en: The erl algorithm was soon followed by cerl (Khadka et al., [2019](#bib.bib36))
    which extends erl from RL to distributed RL where several agents learn in parallel,
    and all these agents are injected into the evolutionary population. The main weakness
    of erl and cerl is their reliance on a genetic algorithm which applies a standard
    $n$-point-based crossover and a Gaussian weight mutation operator to a direct
    encoding of the neural network architecture as a simple vector of parameters.
    This approach is known to require tedious hyperparameter tuning and generally
    perform worse than evolution strategies which are also mathematically more founded
    (Salimans et al., [2017](#bib.bib76)). In particular, the genetic operators used
    in erl and cerl based on a direct encoding have been shown to induce a risk of
    catastrophic forgetting of the behavior of efficient individuals.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: erl 算法很快被 cerl（Khadka 等，[2019](#bib.bib36)）所跟进，cerl 扩展了 erl 从 RL 到分布式 RL，其中多个代理并行学习，所有这些代理都被注入到进化种群中。erl
    和 cerl 的主要弱点在于它们依赖于遗传算法，该算法对神经网络架构的直接编码应用标准的 $n$-点交叉和高斯权重变异操作符。这种方法已知需要繁琐的超参数调整，并且通常表现不如数学基础更扎实的进化策略（Salimans
    等，[2017](#bib.bib76)）。特别是，基于直接编码的遗传操作符在 erl 和 cerl 中已被证明会引发高效个体行为的灾难性遗忘风险。
- en: The pderl algorithm (Bodnar et al., [2020](#bib.bib4)), see Figure LABEL:fig:pderl,
    builds on this criticism and proposes two alternative evolution operators. Instead
    of standard crossover, all agents carry their own replay buffer and crossover
    selects the best experience in both parents to fill the buffer of the offspring,
    before applying behavioral cloning to get a new policy that behaves in accordance
    with the data in the buffer. This operator is inspired by the work of Gangwani
    and Peng ([2018](#bib.bib22)). For mutation, they take as is the improved operator
    proposed in Lehman et al. ([2018](#bib.bib44)), which can be seen as applying
    a Gauss-Newton method to perform the policy gradient step (Pierrot et al., [2018](#bib.bib70)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: pderl 算法（Bodnar 等，[2020](#bib.bib4)，见图 LABEL:fig:pderl），基于这一批评，并提出了两种替代的进化操作符。所有代理携带自己的回放缓冲区，交叉选择两个父代中的最佳经验来填充子代的缓冲区，然后应用行为克隆以获得符合缓冲区数据的新策略。这个操作符受
    Gangwani 和 Peng（[2018](#bib.bib22)）工作的启发。对于变异，他们采用了 Lehman 等（[2018](#bib.bib44)）提出的改进操作符，这可以看作是应用高斯-牛顿方法执行策略梯度步骤（Pierrot
    等，[2018](#bib.bib70)）。
- en: Another follow-up of erl is the esac algorithm (Suri et al., [2020](#bib.bib91)).
    It uses the sac algorithm instead of ddpg and a modified evolution strategy instead
    of a genetic algorithm, but the architecture follows the same template. Similarly,
    the fidi-rl algorithm (Shi et al., [2019](#bib.bib82)) combines ddpg with Augmented
    Random Search (ars), a finite difference algorithm which can be seen as as simplified
    version of evolution strategies (Mania et al., [2018](#bib.bib59)). fidi-rl uses
    the erl architecture as is. The method is shown to outperform ars alone and ddpg
    alone, but neither esac nor fidi-rl are compared to any other combination listed
    in this survey. Finally, the x-ddpg algorithm is a version of erl with several
    asynchronous ddpg actors where the buffers from the evolutionary agents and from
    the ddpg agents are separated, and the most recent ddpg agent is injected into
    the evolutionary population at each time step (Espositi and Bonarini, [2020](#bib.bib19)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: erl 的另一个后续算法是 esac 算法（Suri 等，[2020](#bib.bib91)）。它使用 sac 算法代替 ddpg，并用修改后的进化策略代替遗传算法，但架构遵循相同的模板。同样，fidi-rl
    算法（Shi 等，[2019](#bib.bib82)）结合了 ddpg 和增强随机搜索（ars），这是一种有限差分算法，可以看作是进化策略的简化版本（Mania
    等，[2018](#bib.bib59)）。fidi-rl 使用 erl 架构。该方法被证明优于单独使用 ars 和 ddpg，但 esac 和 fidi-rl
    都没有与本调查中列出的其他组合进行比较。最后，x-ddpg 算法是 erl 的一个版本，具有多个异步 ddpg 代理，其中进化代理和 ddpg 代理的缓冲区是分开的，并且每个时间步骤将最新的
    ddpg 代理注入到进化种群中（Espositi 和 Bonarini，[2020](#bib.bib19)）。
- en: The bnet algorithm (Stork et al., [2021](#bib.bib89)) is borderline in this
    survey as it does not truly use an RL algorithm, but uses a Behavior-Based Neuroevolution
    (BBNE) mechanism which is only loosely inspired from RL algorithms, without relying
    on gradient descent. bnet combines a robust selection method based on standard
    fitness, a second mechanism based on the advantage of the behavior of an agent,
    and a third mechanism based on a surrogate estimate of the return of policies.
    The BBNE mechanism is reminiscent of the Advantage Weighted Regression (awr) algorithm
    (Peng et al., [2019](#bib.bib68)), but it uses an evolutionary approach to optimize
    this behavior-based criterion instead of standard gradient-based methods. The
    reasons for this choice is that the evolutionary part relies on Cartesian Genetic
    Programming (Miller and Harding, [2009](#bib.bib61)) which evolves the structure
    of the neural networks, but gradient descent operators cannot be applied to networks
    whose structure is evolving over episodes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: bnet 算法（Stork 等，[2021](#bib.bib89)）在本调查中处于边缘位置，因为它并不真正使用 RL 算法，而是使用行为基础的神经进化（BBNE）机制，该机制仅受到
    RL 算法的松散启发，而不依赖于梯度下降。bnet 结合了基于标准适应度的稳健选择方法、基于代理行为优势的第二机制以及基于政策回报的替代估计的第三机制。BBNE
    机制类似于优势加权回归（awr）算法（Peng 等，[2019](#bib.bib68)），但它使用进化方法来优化基于行为的标准，而不是标准的基于梯度的方法。选择这种方法的原因是进化部分依赖于笛卡尔遗传编程（Miller
    和 Harding，[2009](#bib.bib61)），它进化神经网络的结构，但梯度下降操作无法应用于在训练过程中结构不断变化的网络。
- en: The chdrl architecture (Zheng et al., [2020](#bib.bib104)) extends the erl approach
    in several ways to improve the sample efficiency of the combination. First, it
    uses two levels of RL algorithms, one on-policy and one off-policy, to benefit
    from the higher sample efficiency of off-policy learning. Second, instead of injecting
    an actor periodically in the evolutionary population, it does so only when the
    actor to be injected performs substantially better than the evolutionary agents.
    Third, it combines the standard replay buffer with a smaller local one which is
    filled with filtered data to ensure using the most beneficial samples. The cspc
    algorithm, depicted in Figure LABEL:fig:cspc is an instance of chdrl using the
    sac and ppo (Schulman et al., [2017](#bib.bib78)) algorithms.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: chdrl 架构（Zheng 等，[2020](#bib.bib104)）通过多种方式扩展了 erl 方法，以提高组合的样本效率。首先，它使用两个级别的
    RL 算法，一个是策略内的，一个是策略外的，以从策略外学习的较高样本效率中受益。其次，它不再定期将演员注入进化种群，而是仅当被注入的演员的表现显著优于进化代理时才这样做。第三，它将标准重放缓冲区与一个较小的本地缓冲区结合使用，该缓冲区填充了过滤后的数据，以确保使用最有益的样本。图
    LABEL:fig:cspc 中描绘的 cspc 算法是使用 sac 和 ppo（Schulman 等，[2017](#bib.bib78)）算法的 chdrl
    实例。
- en: '![Refer to caption](img/7e0f5803afde0b734725972de8b18a96.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7e0f5803afde0b734725972de8b18a96.png)'
- en: ((a))
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ((a))
- en: '![Refer to caption](img/bac0358e0cc30492ccf2889ef87e82d6.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bac0358e0cc30492ccf2889ef87e82d6.png)'
- en: ((b))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ((b))
- en: 'Figure 3: The cspc (a) and cem-rl (b) architectures. In cspc, an on-policy
    and an off-policy algorithms are combined, together with two replay buffers and
    a performance-based actor injection rule, to improve the sample efficiency of
    erl-like methods. In cem-rl, gradient steps from the td3 critic are applied to
    half the population of evolutionary agents. If applying this gradient is favorable,
    the corresponding individuals are kept, otherwise they are discarded.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：cspc (a) 和 cem-rl (b) 架构。在cspc中，结合了一个on-policy和一个off-policy算法，两个回放缓冲区和一个基于性能的演员注入规则，以提高erl-like方法的样本效率。在cem-rl中，td3评论家的梯度步骤应用于进化代理的一半种群。如果应用该梯度是有利的，相应的个体会被保留，否则会被丢弃。
- en: Note that if an RL actor is injected in an evolutionary population and if evolution
    uses a direct encoding, the RL actor and evolution individuals need to share a
    common structure. Removing this constraint might be useful, as evolutionary methods
    are often applied to smaller policies than RL methods. For doing so, one might
    call upon any policy distillation mechanism that strives to obtain from a large
    policy a smaller policy with similar capabilities.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果在进化种群中注入RL演员，并且进化使用直接编码，那么RL演员和进化个体需要共享一个共同的结构。去除这一约束可能是有用的，因为进化方法通常应用于比RL方法更小的策略。为此，可以调用任何政策蒸馏机制，该机制致力于从一个大型策略中获得一个具有类似能力的小型策略。
- en: 2.2 RL gradient addition
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 RL梯度加法
- en: Instead of injecting an RL actor into the population, another approach consists
    in applying gradient steps to some members of this population. This is the approach
    of the cem-rl algorithm (Pourchot and Sigaud, [2019](#bib.bib73)), see Figure LABEL:fig:cemrl.
    This work was followed by cem-acer (Tang, [2021](#bib.bib93)) which simply replaces
    td3 (Fujimoto et al., [2018](#bib.bib21)) with acer (Wang et al., [2017](#bib.bib100)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是对这个种群中的一些成员应用梯度步骤，而不是将RL演员注入种群。这就是cem-rl算法（Pourchot and Sigaud, [2019](#bib.bib73)）的方法，见图LABEL:fig:cemrl。这项工作随后由cem-acer（Tang,
    [2021](#bib.bib93)）跟进，cem-acer简单地用acer（Wang et al., [2017](#bib.bib100)）替代了td3（Fujimoto
    et al., [2018](#bib.bib21)）。
- en: 2.3 Evolution from the RL actor
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 从RL演员的进化
- en: ((a))
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ((a))
- en: '![Refer to caption](img/f35fe6f65be8ff588ec7fedd76180623.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f35fe6f65be8ff588ec7fedd76180623.png)'
- en: ((b))
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ((b))
- en: 'Figure 4: In the g2n (a) and supe-rl (b) architectures, the evolutionary population
    is built locally from the RL actor. In g2n, the evolutionary part explores the
    structure of the central layer of the actor network. In supe-rl, more standard
    mutations are applied, the non-mutated actor is inserted in the evolutionary population
    and the actor is soft-updated towards its best offspring.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：在g2n (a) 和 supe-rl (b) 架构中，进化种群是从RL演员本地构建的。在g2n中，进化部分探索演员网络中央层的结构。在supe-rl中，应用了更标准的变异，非变异的演员被插入进化种群，并且演员向其最佳后代软更新。
- en: In the algorithms listed so far, the main loop is evolutionary and the RL loop
    is used at a slower pace to accelerate it. In the g2n (Chang et al., [2018](#bib.bib5))
    and supe-rl (Marchesini et al., [2021](#bib.bib60)) algorithms, by contrast, the
    main loop is the RL loop and evolution is used to favor exploration.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在目前列出的算法中，主要循环是进化的，而RL循环则以较慢的速度用于加速它。相比之下，在g2n (Chang et al., [2018](#bib.bib5))
    和 supe-rl (Marchesini et al., [2021](#bib.bib60)) 算法中，主要循环是RL循环，进化用于促进探索。
- en: In g2n, shown in Figure LABEL:fig:ggn, evolution is used to activate or deactivate
    neurons of the central layer in the architecture of the actor according to a binary
    genome. By sampling genomes using evolutionary operators, various actor architectures
    are evaluated and the one that performs best benefits from a critic gradient step,
    before its genome is used to generate a new population of architectures. This
    mechanism provides a fair amount of exploration both in the actor structures and
    in the generated trajectories and outperforms random sampling of the genomes.
    Two instances of the g2n approach are studied, g2ac based on a2c and g2ppo based
    on ppo, and they both outperform the RL algorithm they use.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在g2n中，如图LABEL:fig:ggn所示，进化用于根据二进制基因组激活或停用演员架构中的中央层神经元。通过使用进化操作符对基因组进行采样，各种演员架构被评估，并且表现最佳的架构会通过一个评论家梯度步骤，从而使其基因组用于生成新的人口架构。这个机制在演员结构和生成的轨迹中提供了相当多的探索，并且优于基因组的随机采样。研究了两种g2n方法实例，基于a2c的g2ac和基于ppo的g2ppo，它们都优于它们使用的RL算法。
- en: The supe-rl algorithm, shown in Figure LABEL:fig:superl, is similar to g2n apart
    from the fact that evolving the structure of the central layer is replaced by
    performing standard Gaussian noise mutation of all the parameters of the actor.
    Besides, if one of the offspring is better than the current RL agent, the latter
    is modified towards this better offspring through a soft update mechanism. Finally,
    the non-mutated actor is also inserted in the evolutionary population, which is
    not the case in g2n.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如图LABEL:fig:superl所示，supe-rl算法与g2n类似，只是中央层结构的进化被替换为对演员所有参数进行标准高斯噪声突变。此外，如果一个后代比当前的RL智能体更好，后者将通过软更新机制向这个更好的后代进行修改。最后，非突变演员也被插入进化种群中，这与g2n不同。
- en: 2.4 Using a surrogate fitness
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 使用替代适应度
- en: '![Refer to caption](img/77399daa7332fa4277c4e9761d0f27ad.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/77399daa7332fa4277c4e9761d0f27ad.png)'
- en: ((a))
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ((a))
- en: '![Refer to caption](img/f5ed39b6018698c0c0abb1695aa56b25.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f5ed39b6018698c0c0abb1695aa56b25.png)'
- en: ((b))
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ((b))
- en: 'Figure 5: The sc-erl (a) and pgps (b) architectures are two approaches to improve
    sample efficiency by using a critic network as a surrogate for evaluating evolutionary
    individuals. In sc-erl, the surrogate control part is generic and can be applied
    to several architectures such as erl, cerl or cem-rl. It considers the critic
    as a surrogate model of fitness, making it possible to estimate the fitness of
    a new individual without generating additional samples. (b) The pgps uses the
    same idea but combines it with several other mechanisms, such as performing a
    soft update of the actor towards the best evolutionary agent or filling half the
    population using the surrogate fitness and the other half from cem generated agents.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：sc-erl（a）和pgps（b）架构是两种通过使用评论网络作为评估进化个体的替代品来提高样本效率的方法。在sc-erl中，替代控制部分是通用的，可以应用于erl、c
    erl或cem-rl等多个架构。它将评论网络视为适应度的替代模型，使得可以在不生成额外样本的情况下估计新个体的适应度。（b）pgps使用了相同的思想，但结合了其他几种机制，例如对演员进行软更新以接近最佳进化智能体，或者使用替代适应度填充一半种群，其余一半则来自cem生成的智能体。
- en: A weakness of all the methods combining evolution and RL that we have listed
    so far is that they require evaluating the agents to perform the evolutionary
    selection step, which may impair sample efficiency. In the sc-erl (Wang et al.,
    [2022](#bib.bib99)) and pgps (Kim et al., [2020](#bib.bib40)) architectures, this
    concern is addressed by using a critic network as a surrogate for evaluating an
    agent. Importantly, the evaluation of individuals must initially rely on the true
    fitness but can call upon the critic more and more often as its accuracy gets
    better. As shown in Figure LABEL:fig:scerl, the sc-erl architecture is generic
    and can be applied on top of any of the combinations we have listed so far. In
    practice, it is applied to erl, pderl and cem-rl, resulting in the serl and spderl
    algorithms in the first two cases.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今列举的所有结合进化和强化学习（RL）的方法的一个弱点是，它们需要评估智能体以执行进化选择步骤，这可能会影响样本效率。在sc-erl（Wang et
    al., [2022](#bib.bib99)）和pgps（Kim et al., [2020](#bib.bib40)）架构中，这个问题通过使用一个评论网络作为评估智能体的替代品来解决。重要的是，个体的评估必须最初依赖于真实的适应度，但可以随着评论网络准确度的提高而越来越频繁地调用评论网络。如图LABEL:fig:scerl所示，sc-erl架构是通用的，可以应用于我们迄今列举的任何组合中。在实践中，它被应用于erl、pderl和cem-rl，导致在前两个案例中得到了serl和spderl算法。
- en: The pgps algorithm (Kim et al., [2020](#bib.bib40)), shown in Figure LABEL:fig:pgps,
    builds on the same idea but uses it in the context of a specific combination of
    evolutionary and RL mechanisms which borrows ideas from several of the previously
    described methods. In more details, half of the population is filled with agents
    evaluated from the surrogate fitness whereas the other half are generated with
    cem. Furthermore, the current td3 actor is injected into the population and benefits
    from a soft update towards the best agent in the population.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: pgps算法（Kim et al., [2020](#bib.bib40)），如图LABEL:fig:pgps所示，建立在相同的思想上，但在特定的进化和RL机制组合的背景下使用这一思想，该组合借鉴了之前描述的几种方法的思想。更详细地说，种群的一半填充了通过替代适应度评估的智能体，而另一半则由cem生成。此外，当前的td3演员被注入到种群中，并通过软更新受益于种群中的最佳智能体。
- en: 3 Evolution of actions for performance
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 动作进化以提高性能
- en: In this section we cover algorithms where evolution is used to optimize an action
    in a given state, rather than optimizing policy parameters. The general idea is
    that variation-selection methods such as cem can optimize any vector of parameters
    given some performance function of these parameters. In the methods listed in
    the previous section, the parameters were those of a policy and the performance
    was the return of that policy. In the methods listed here, the parameters specify
    the action in a given state and the performance is the Q-value of this action
    in that state.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了使用进化方法来优化给定状态下的动作，而不是优化策略参数。一般的想法是，变异选择方法如cem可以根据这些参数的某些性能函数来优化任何参数向量。在前一节中列出的方法中，参数是策略的参数，性能是该策略的回报。在这里列出的方法中，参数指定了给定状态下的动作，性能是该动作在该状态下的Q值。
- en: 'In an RL algorithm like Q-learning, the agent needs to find the action with
    the highest value in a given state for two things: for performing critic updates,
    that is updating its estimates of the action-value function using $Q(s_{t},a_{t})\leftarrow
    r(s_{t},a_{t})+\max_{a}Q(s_{t+1},a)-Q(s_{t},a_{t})$, and for acting using $\mathop{\rm
    argmax}_{a}Q(s_{t},a)$. When the action space is continuous, this amounts to solving
    an expensive optimization problem, and this is required at each training step.
    The standard solution to this problem in actor-critic methods consists in considering
    the action of the actor as a good proxy for the best action. The estimated best
    action, that we note $\bar{a_{t}}$, is taken to be the actor’s action $\bar{a_{t}}=\pi(s_{t})$,
    resulting in using $Q(s_{t},a_{t})\leftarrow r(s_{t},a_{t})+\max_{a}Q(s_{t+1},\pi(s_{t+1}))-Q(s_{t},a_{t})$
    for the critic update and using $\bar{a_{t}}$ for acting.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在像Q-learning这样的RL算法中，代理需要找到给定状态下的最高价值动作，这有两个目的：进行评论更新，即使用$Q(s_{t},a_{t})\leftarrow
    r(s_{t},a_{t})+\max_{a}Q(s_{t+1},a)-Q(s_{t},a_{t})$来更新对动作值函数的估计，以及进行动作选择时使用$\mathop{\rm
    argmax}_{a}Q(s_{t},a)$。当动作空间是连续的，这就相当于解决一个昂贵的优化问题，这在每次训练步骤中都是必要的。演员-评论家方法的标准解决方案是将演员的动作视为最佳动作的良好代理。我们记作$\bar{a_{t}}$的最佳动作被认为是演员的动作$\bar{a_{t}}=\pi(s_{t})$，因此使用$Q(s_{t},a_{t})\leftarrow
    r(s_{t},a_{t})+\max_{a}Q(s_{t+1},\pi(s_{t+1}))-Q(s_{t},a_{t})$来进行评论更新，并使用$\bar{a_{t}}$进行动作选择。
- en: But as an alternative, one can call upon a variation-selection method to find
    the best performing action over a limited set of sampled actions. This approach
    is used in the qt-opt algorithm (Kalashnikov et al., [2018](#bib.bib35)), as well
    as in the cgp (Simmons-Edler et al., [2019](#bib.bib87)), sac-cepo (Shi and Singh,
    [2021](#bib.bib84)), grac (Shao et al., [2021](#bib.bib81)) and eas-rl (Ma et al.,
    [2022](#bib.bib56)) algorithms. This is the approach we first cover in this section.
    The zospi algorithm (Sun et al., [2020](#bib.bib90)) also benefits from optimizing
    actions with a variation-selection method, though it stems from a different perspective.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但作为一种替代方法，可以调用变异选择方法在有限的采样动作集合中找到表现最好的动作。这种方法在qt-opt算法（Kalashnikov等，[2018](#bib.bib35)）、cgp（Simmons-Edler等，[2019](#bib.bib87)）、sac-cepo（Shi和Singh，[2021](#bib.bib84)）、grac（Shao等，[2021](#bib.bib81)）和eas-rl（Ma等，[2022](#bib.bib56)）算法中都有应用。这是我们在本节中首先介绍的方法。zospi算法（Sun等，[2020](#bib.bib90)）也通过变异选择方法优化动作，尽管它来自不同的视角。
- en: 'Table 2: Combinations evolving actions for performance. The cells in green
    denote where evolutionary optimization takes places. We specify the use of cem
    for optimizing an action with $\bar{a_{t}}=\text{{\sc cem}}(\textit{source},N,N_{e},I)$,
    where source is the source from which we sample initial actions, $N$ is the size
    of this sample (the population), $N_{e}$ is the number of elite solutions that
    are retained from a generation to the next and $I$ is the number of iterations.
    For pso, the shown parameters are the number of action $N$ and the number of iterations
    $T$. And we use $\bar{a_{t}}=\mathop{\rm argmax}(\textit{source},N)$ for simply
    take the best action over $N$ samples from a given source.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：性能进化行动的组合。绿色单元格表示进化优化发生的地方。我们指定使用cem来优化动作，表示为$\bar{a_{t}}=\text{{\sc cem}}(\textit{source},N,N_{e},I)$，其中source是我们从中采样初始动作的来源，$N$是样本的大小（即种群），$N_{e}$是从一代到下一代保留的精英解的数量，$I$是迭代次数。对于pso，所示参数是动作数量$N$和迭代次数$T$。我们使用$\bar{a_{t}}=\mathop{\rm
    argmax}(\textit{source},N)$来从给定来源的$N$个样本中简单地选择最佳动作。
- en: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3) scale(1,
    -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15) scale(1,
    -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | Critic update | Action Selection | Policy Update |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3) scale(1,
    -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15) scale(1,
    -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | 批评者更新 | 动作选择 | 策略更新 |'
- en: '| qt-opt Kalashnikov et al. ([2018](#bib.bib35)) | $\bar{a_{t}}=$cem (random,
    64, 6, 2) | $\bar{a_{t}}=$cem (random, 64, 6, 2) | No policy |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| qt-opt Kalashnikov 等人 ([2018](#bib.bib35)) | $\bar{a_{t}}=$cem（随机，64，6，2）
    | $\bar{a_{t}}=$cem（随机，64，6，2） | 无策略 |'
- en: '| cgp Simmons-Edler et al. ([2019](#bib.bib87)) | $\bar{a_{t}}=$cem (random,
    64, 6, 2) | $\bar{a_{t}}=\pi(s_{t})$ | BC or DPG |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| cgp Simmons-Edler 等人 ([2019](#bib.bib87)) | $\bar{a_{t}}=$cem（随机，64，6，2）
    | $\bar{a_{t}}=\pi(s_{t})$ | BC 或 DPG |'
- en: '| eas-rl Ma et al. ([2022](#bib.bib56)) | $\bar{a_{t}}=$pso (10,10) | $\bar{a_{t}}=\pi(s_{t})$
    | BC + DPG |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| eas-rl Ma 等人 ([2022](#bib.bib56)) | $\bar{a_{t}}=$pso（10，10） | $\bar{a_{t}}=\pi(s_{t})$
    | BC + DPG |'
- en: '| sac-cepo Shi and Singh ([2021](#bib.bib84)) | sac update | $\bar{a_{t}}=$cem
    ($\pi$, 60 \clipbox*.250pt 0pt 0pt 0pt →140, $3\%$ \clipbox*.250pt 0pt 0pt 0pt
    →$7\%$, 6 \clipbox*.250pt 0pt 0pt 0pt →14) | BC |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| sac-cepo Shi 和 Singh ([2021](#bib.bib84)) | sac 更新 | $\bar{a_{t}}=$cem（$\pi$，60
    \clipbox*.250pt 0pt 0pt 0pt →140，$3\%$ \clipbox*.250pt 0pt 0pt 0pt →$7\%$，6 \clipbox*.250pt
    0pt 0pt 0pt →14） | BC |'
- en: '| grac Shao et al. ([2021](#bib.bib81)) | $\bar{a_{t}}=$cem ($\pi$, 256, 5,
    2) | $\bar{a_{t}}=$cem ($\pi$, 256, 5, 2) | PG with two losses |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| grac Shao 等人 ([2021](#bib.bib81)) | $\bar{a_{t}}=$cem（$\pi$，256，5，2） | $\bar{a_{t}}=$cem（$\pi$，256，5，2）
    | PG 具有两个损失 |'
- en: '| zospi Sun et al. ([2020](#bib.bib90)) | ddpg update | $\bar{a_{t}}=\pi(s_{t})$
    + perturb. network | BC($\bar{a_{t}}=\mathop{\rm argmax}(random,50)$) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| zospi Sun 等人 ([2020](#bib.bib90)) | ddpg 更新 | $\bar{a_{t}}=\pi(s_{t})$ +
    扰动网络 | BC（$\bar{a_{t}}=\mathop{\rm argmax}(随机，50)$） |'
- en: 'As Table [2](#S3.T2 "Table 2 ‣ 3 Evolution of actions for performance ‣ Combining
    Evolution and Deep Reinforcement Learning for Policy Search: a Survey") shows,
    the qt-opt algorithm (Kalashnikov et al., [2018](#bib.bib35)) simply samples 64
    random actions in the action space and performs two iterations of cem to get a
    high performing action, both for critic updates and action selection. It is striking
    that such a simple method can perform well even in large action spaces. This simple
    idea was then improved in the cgp algorithm (Simmons-Edler et al., [2019](#bib.bib87))
    so as to avoid the computational cost of action inference. Instead of using cem
    to sample an action at each time step, a policy network is learned based on the
    behavior of the cem. This network can be seen as a surrogate of the cem sampling
    process and is trained either from the sampled $\bar{a_{t}}$ using Behavioral
    Cloning (BC) or following a Deterministic Policy Gradient (DPG) step from the
    critic.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[2](#S3.T2 "表 2 ‣ 3 动作演变以提高性能 ‣ 结合进化与深度强化学习进行策略搜索：综述")所示，qt-opt 算法（Kalashnikov
    等人，[2018](#bib.bib35)）简单地在动作空间中随机抽取 64 个动作，并执行两次 cem 迭代以获得高性能的动作，适用于批评者更新和动作选择。令人惊讶的是，即使在大动作空间中，这种简单的方法也能表现良好。这个简单的想法随后在
    cgp 算法（Simmons-Edler 等人，[2019](#bib.bib87)）中得到了改进，以避免动作推断的计算成本。该算法不是在每个时间步骤使用
    cem 来抽样动作，而是基于 cem 的行为学习了一个策略网络。这个网络可以看作是 cem 抽样过程的替代品，并通过行为克隆（BC）从抽样的 $\bar{a_{t}}$
    中进行训练，或通过批评者的确定性策略梯度（DPG）步骤进行训练。
- en: The eas-rl algorithm (Ma et al., [2022](#bib.bib56)) is similar to cgp apart
    from the fact that it uses Particle Swarm Optimization (pso) instead of cem. Besides,
    depending on the sign of the advantage of the obtained action $\bar{a_{t}}$, it
    uses either BC or DPG to update the policy for each sample.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: eas-rl 算法（Ma 等人，[2022](#bib.bib56)）与 cgp 类似，但它使用粒子群优化（pso）代替 cem。此外，根据获得的动作
    $\bar{a_{t}}$ 的优势符号，它使用 BC 或 DPG 来更新每个样本的策略。
- en: Symmetrically to cgp, the sac-cepo algorithm (Shi and Singh, [2021](#bib.bib84))
    performs standard critic updates using sac but selects actions using cem. More
    precisely, it introduces the idea to sample the action from the current policy
    rather than randomly, and updates this policy using BC from the sampled actions.
    Besides, the paper investigates the effect of the cem parameters but does not
    provide solid conclusions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与cgp对称，sac-cepo算法（Shi和Singh，[2021](#bib.bib84)）使用sac进行标准的评论员更新，但使用cem选择动作。更准确地说，它引入了从当前策略中采样动作而不是随机选择的思想，并使用BC从采样的动作中更新该策略。此外，论文探讨了cem参数的效果，但没有提供确凿的结论。
- en: The grac algorithm (Shao et al., [2021](#bib.bib81)) combines ideas from cgp
    and sac-cepo. A stochastic policy network outputs an initial Gaussian distribution
    for the action at each step. Then, a step of cem drawing 256 actions out of this
    distribution is used to further optimize the choice of action both for critic
    updates and action selection. The policy itself is updated with a combination
    of two training losses.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: grac算法（Shao等，[2021](#bib.bib81)）结合了cgp和sac-cepo的思想。一个随机策略网络在每一步输出一个初始的高斯分布。然后，使用cem从这个分布中抽取256个动作的步骤来进一步优化评论员更新和动作选择。策略本身通过两种训练损失的组合进行更新。
- en: Finally, the zospi algorithm (Sun et al., [2020](#bib.bib90)) calls upon variation-selection
    for updating the policy rather than for updating the critic or selecting the action.
    Its point is rather than gradient descent algorithms tend to get stuck into local
    minima and may miss the appropriate direction due to various approximations, whereas
    a variation-selection method is more robust. Thus, to update its main policy,
    zospi simply samples a set of actions and performs BC towards the best of these
    actions, which can be seen as a trivial variation-selection method. The typical
    number of sampled actions is 50\. It then adds a policy perturbation network to
    perform exploration, which is trained using gradient descent.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，zospi算法（Sun等，[2020](#bib.bib90)）调用变异选择来更新策略，而不是更新评论员或选择动作。其要点在于，与梯度下降算法倾向于陷入局部最小值并由于各种近似而可能错过适当方向相比，变异选择方法更具鲁棒性。因此，为了更新其主要策略，zospi简单地从一组动作中进行采样，并对这些动作中最佳的动作执行BC，这可以看作是一个简单的变异选择方法。典型的采样动作数量是50个。然后，它添加了一个策略扰动网络来进行探索，该网络使用梯度下降进行训练。
- en: 4 Evolution of policies for diversity
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 多样性策略的演变
- en: The trade-off between exploration and exploitation is central to RL. In particular,
    when the reward signal is sparse, efficient exploration becomes crucial. All the
    papers studied in this survey manage a population of agents, hence their capability
    to explore can benefit from maintaining behavioral diversity between the agents.
    This idea of maintaining behavioral diversity is central to two families of diversity
    seeking algorithms, the novelty search (NS) (Lehman and Stanley, [2011](#bib.bib45))
    algorithms which do not use the reward signal at all, see Figure LABEL:fig:ns_rl,
    and the quality-diversity (QD) algorithms (Pugh et al., [2016](#bib.bib74); Cully
    and Demiris, [2017](#bib.bib14)), see Figure LABEL:fig:me, which try to maximize
    both diversity and performance. As the NS approach only looks for diversity, it
    is better in the absence of reward, or when the reward signal is very sparse or
    deceptive as the best one can do in the absence of reward is try to cover a relevant
    space as uniformly as possible (Doncieux et al., [2019](#bib.bib17)). By contrast,
    the QD approach is more appropriate when the reward signal can contribute to the
    policy search process. In this section we cover both families separately.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与利用之间的权衡是强化学习的核心。特别是当奖励信号稀疏时，高效的探索变得至关重要。本调查研究的所有论文都管理一个代理群体，因此它们的探索能力可以从保持代理之间的行为多样性中受益。这种保持行为多样性的思想是两类寻求多样性算法的核心，即完全不使用奖励信号的新颖性搜索（NS）算法（Lehman和Stanley，[2011](#bib.bib45)），见图LABEL:fig:ns_rl，以及尝试最大化多样性和表现的质量-多样性（QD）算法（Pugh等，[2016](#bib.bib74)；Cully和Demiris，[2017](#bib.bib14)），见图LABEL:fig:me。由于NS方法只关注多样性，它在没有奖励的情况下表现更好，或者当奖励信号非常稀疏或具有误导性时，因为在没有奖励的情况下，最好的做法是尽可能均匀地覆盖相关空间（Doncieux等，[2019](#bib.bib17)）。相比之下，当奖励信号可以为策略搜索过程提供贡献时，QD方法更为合适。在本节中，我们将分别讨论这两类方法。
- en: '![Refer to caption](img/1b24db0ae1987840a248501caa9a8942.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1b24db0ae1987840a248501caa9a8942.png)'
- en: ((a))
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ((a))
- en: '![Refer to caption](img/61588e134dde1358d9c369eb1bc06b1e.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/61588e134dde1358d9c369eb1bc06b1e.png)'
- en: ((b))
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ((b))
- en: 'Figure 6: Template architectures for combining deep RL with novelty search
    (a) and quality-diversity (b). The latter builds on Fig. 2 in Mouret ([2020](#bib.bib63)).
    Both architectures rely on a behavioral characterization space and maintain an
    archive in that space.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 结合深度 RL 与新颖性搜索 (a) 和质量-多样性 (b) 的模板架构。后者基于 Mouret ([2020](#bib.bib63))
    中的图 2。这两种架构依赖于行为特征空间，并在该空间中维护一个档案库。'
- en: 4.1 Novelty seeking approaches
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 新颖性搜索方法
- en: 'Table 3: Combinations evolving policies for diversity. NS: Novelty Search.
    Policy params: distance is computed in the policy parameters space. GC-ddpg: goal-conditioned
    ddpg. Manual BC: distances are computed in a manually defined behavior characterization
    space.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 组合进化的多样性策略。NS: 新颖性搜索。策略参数: 在策略参数空间中计算距离。GC-ddpg: 目标条件 ddpg。手动 BC: 在手动定义的行为特征空间中计算距离。'
- en: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3) scale(1,
    -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15) scale(1,
    -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | RL algo. | Diversity algo. | Distance space |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3) scale(1,
    -1)"><foreignobject width="31.9" height="12.3" overflow="visible">算法.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15) scale(1,
    -1)"><foreignobject width="33.29" height="12.15" overflow="visible">属性.</foreignobject></g></g></g></svg>
    | RL 算法 | 多样性算法 | 距离空间 |'
- en: '| p3s-td3 Jung et al. ([2020](#bib.bib34)) | td3 | Find best | Policy params.
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| p3s-td3 Jung et al. ([2020](#bib.bib34)) | td3 | 寻找最佳 | 策略参数. |'
- en: '| deprl Liu and Feng ([2021](#bib.bib51)) | td3 | cem | Policy params. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| deprl Liu and Feng ([2021](#bib.bib51)) | td3 | cem | 策略参数. |'
- en: '| arac Doan et al. ([2019](#bib.bib16)) | sac | NS-like | Policy params. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| arac Doan et al. ([2019](#bib.bib16)) | sac | 类似 NS | 策略参数. |'
- en: '| ns-rl Shi et al. ([2020](#bib.bib83)) | GC-ddpg | True NS | Manual BC |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ns-rl Shi et al. ([2020](#bib.bib83)) | GC-ddpg | 真正的 NS | 手动 BC |'
- en: '| pns-rl Liu et al. ([2018](#bib.bib52)) | td3 | True NS | Manual BC |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| pns-rl Liu et al. ([2018](#bib.bib52)) | td3 | 真正的 NS | 手动 BC |'
- en: 'Maintaining a distance between agents in a population can be achieved in different
    spaces. For instance, the svpg algorithm (Liu et al., [2017](#bib.bib53)) defines
    distances in a kernel space and adds to the policy gradient a loss term dedicated
    to increasing the pairwise distance between agents. Alternatively, the dvd algorithm
    (Parker-Holder et al., [2020](#bib.bib66)) defines distances in an action embedding
    space, corresponding to the actions specified by each agent in a large enough
    set of random states. Then dvd optimizes a global distance between all policies
    by maximizing the volume of the space between them through the computation of
    a determinant. Despite their interest, these two methods depicted in Figure [7](#S4.F7
    "Figure 7 ‣ 4.1 Novelty seeking approaches ‣ 4 Evolution of policies for diversity
    ‣ Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey")
    do not appear in Table [3](#S4.T3 "Table 3 ‣ 4.1 Novelty seeking approaches ‣
    4 Evolution of policies for diversity ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey") as the former does not have an evolutionary
    component and the latter uses nsr-es (Conti et al., [2018](#bib.bib13)) but does
    not have an RL component.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '在群体中保持代理之间的距离可以在不同的空间中实现。例如，svpg 算法（Liu et al., [2017](#bib.bib53)）在核空间中定义距离，并在策略梯度中增加一个损失项，用于增加代理之间的成对距离。或者，dvd
    算法（Parker-Holder et al., [2020](#bib.bib66)）在行动嵌入空间中定义距离，这些距离对应于每个代理在一个足够大的随机状态集合中指定的动作。然后，dvd
    通过计算行列式来优化所有策略之间的全局距离，通过最大化它们之间空间的体积。尽管这些方法很有趣，但图 [7](#S4.F7 "Figure 7 ‣ 4.1 Novelty
    seeking approaches ‣ 4 Evolution of policies for diversity ‣ Combining Evolution
    and Deep Reinforcement Learning for Policy Search: a Survey") 中描绘的这两种方法并未出现在表
    [3](#S4.T3 "Table 3 ‣ 4.1 Novelty seeking approaches ‣ 4 Evolution of policies
    for diversity ‣ Combining Evolution and Deep Reinforcement Learning for Policy
    Search: a Survey") 中，因为前者没有进化成分，后者使用 nsr-es（Conti et al., [2018](#bib.bib13)）但没有
    RL 成分。'
- en: '![Refer to caption](img/7c6495f537bddda9bbe54024fdd710a0.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c6495f537bddda9bbe54024fdd710a0.png)'
- en: ((a))
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ((a))
- en: '![Refer to caption](img/983c396c3976e6bf1d6ec2cba230d1a7.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/983c396c3976e6bf1d6ec2cba230d1a7.png)'
- en: ((b))
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ((b))
- en: 'Figure 7: The svpg (a) and dvd (b) architectures. In svpg, individual policy
    gradients computed by each agent are combined so as to ensure both diversity between
    agents and performance improvement. In dvd, a purely evolutionary approach is
    combined with a diversity mechanism which seeks to maximize the volume between
    the behavioral characterization of agents in an action embedding space. Both architectures
    fail to combine evolution and RL, though they both try to maximize diversity and
    performance in a population of agents.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：svpg (a) 和 dvd (b) 架构。在 svpg 中，每个代理计算的个体策略梯度被组合，以确保代理之间的多样性和性能提升。在 dvd 中，纯粹的进化方法与一个多样性机制结合，该机制试图最大化行为特征在动作嵌入空间中的体积。虽然这两种架构都尝试最大化代理群体中的多样性和性能，但它们未能结合进化和
    RL。
- en: A more borderline case with respect to the focus of this survey is the p3s-td3
    algorithm (Jung et al., [2020](#bib.bib34)). Though p3s-td3 is used as a baseline
    in several of the papers mentioned in this survey, its equivalent of the evolutionary
    loop is limited to finding the best agent in the population, as shown in Figure LABEL:fig:p3s.
    This implies evaluating all these agents, but not using neither variation nor
    selection. Besides, the mechanism to maintain a distance between solutions in
    p3s-td3 is ad hoc and acts in the space of policy parameters. This is also the
    case in the deprl algorithm (Liu and Feng, [2021](#bib.bib51)), which is just
    a variation of cem-rl where an ad hoc mechanism is added to enforce some distance
    between members of the evolutionary population.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本调查的重点，p3s-td3 算法（Jung 等，[2020](#bib.bib34)）是一个更具边界性的案例。尽管 p3s-td3 在本调查中提到的几篇论文中被用作基线，但其进化循环的等效部分仅限于在种群中寻找最佳代理，如图
    LABEL:fig:p3s 所示。这意味着评估所有这些代理，但不使用变异或选择。此外，p3s-td3 中维护解决方案之间距离的机制是专门设计的，作用于策略参数空间。这也是
    deprl 算法（Liu 和 Feng，[2021](#bib.bib51)）的情况，deprl 算法只是 cem-rl 的一种变体，其中添加了一个专门的机制以强制执行进化种群成员之间的距离。
- en: The arac algorithm (Doan et al., [2019](#bib.bib16)) also uses a distance in
    the policy parameter space, but it truly qualifies as a combination of evolution
    and deep RL, see Figure LABEL:fig:arac. An original feature of arac is that it
    selects the data coming into the replay buffer based on the novelty of agents,
    which can result in saving a lot of poorly informative gradient computations.
    A similar idea is also present in (Chen, [2019](#bib.bib6)) where instead of filtering
    based on novelty, the algorithm uses an elite replay buffer containing only the
    top trajectories, similarly to what we have already seen in the cspc algorithm
    (Zheng et al., [2020](#bib.bib104)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: arac 算法（Doan 等， [2019](#bib.bib16)）也使用了策略参数空间中的距离，但它真正符合进化和深度 RL 的结合，见图 LABEL:fig:arac。arac
    的一个独特特点是，它基于代理的创新性选择进入重放缓冲区的数据，这可以节省大量不太有用的梯度计算。类似的想法也出现在（Chen，[2019](#bib.bib6)）中，在那里，算法使用一个精英重放缓冲区，包含仅有的顶级轨迹，这与我们在
    cspc 算法（Zheng 等，[2020](#bib.bib104)）中看到的相似。
- en: The methods listed so far were neither using a manually defined behavior characterization
    space for computing distances between agents nor an archive of previous agents
    to evaluate novelty. Thus they do not truly qualify as NS approaches. We now turn
    to algorithms which combine both features.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止列出的方法既没有使用手动定义的行为特征空间来计算代理之间的距离，也没有使用以前代理的档案来评估创新性。因此，它们并不真正符合 NS 方法。我们现在转向结合这两种特征的算法。
- en: '![Refer to caption](img/a17396be8f321d44634ad371846eddb0.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a17396be8f321d44634ad371846eddb0.png)'
- en: ((a))
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ((a))
- en: '![Refer to caption](img/bfbbce1d4833ff852ec2a7460d908d25.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bfbbce1d4833ff852ec2a7460d908d25.png)'
- en: ((b))
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ((b))
- en: 'Figure 8: The p3s-td3 (a) and arac (b) architectures. In p3s-td3, all agents
    are trained with RL and evaluated, then they all perform a soft update towards
    the best agent. The arac algorithm maintains a population of policies following
    a gradient from a common sac critic (Haarnoja et al., [2018](#bib.bib26)). The
    critic itself is trained from trajectories of the most novel agents. Besides,
    diversity in the population is ensured by adding an attraction-repulsion loss
    $\mathcal{L}_{AR}$ to the update of the agents. This loss is computed with respect
    to an archive of previous agents themselves selected using a novelty criterion.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：p3s-td3（a）和 arac（b）架构。在 p3s-td3 中，所有代理都用 RL 进行训练和评估，然后它们都对最佳代理执行软更新。arac
    算法维护一个遵循来自共同 sac 评论员（Haarnoja et al., [2018](#bib.bib26)）的梯度的策略种群。评论员本身是通过最具新颖性的代理的轨迹进行训练的。此外，通过将吸引-排斥损失
    $\mathcal{L}_{AR}$ 添加到代理的更新中来确保种群中的多样性。这个损失是相对于使用新颖性标准选择的先前代理的档案计算的。
- en: Figure LABEL:fig:ns_rl suggests that, when combining evolution and RL, novelty
    can be used as a fitness function, as a reward signal to learn a critic, or both.
    Actually, in the two algorithms described below, it is used for both. More precisely,
    the RL part is used to move the rest of the population towards the most novel
    agent.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 LABEL:fig:ns_rl 表明，在结合进化和 RL 时，新颖性可以作为适应度函数、作为学习评论员的奖励信号，或两者兼而有之。实际上，在下面描述的两个算法中，它都被用作两者。更准确地说，RL
    部分用于将其余种群向最具新颖性的代理移动。
- en: In the pns-rl algorithm (Liu et al., [2018](#bib.bib52)), a group of agents
    is following a leader combining a standard policy gradient update and a soft update
    towards the leader. Then, for any agent in the group, if its performance is high
    enough with respect to the mean performance in an archive, it is added to the
    archive. Crucially, the leader is selected as the one that maximizes novelty in
    the archive given a manually defined behavioral characterization. In addition,
    for efficient parallelization, the algorithm considers several groups instead
    of one, but where all groups share the same leader.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pns-rl 算法（Liu et al., [2018](#bib.bib52)）中，一组代理遵循一个结合了标准策略梯度更新和软更新的领导者。然后，对于组中的任何代理，如果其表现相对于档案中的平均表现足够高，它将被添加到档案中。关键是，领导者被选为在给定手动定义的行为特征下最大化档案中新颖性的代理。此外，为了高效的并行化，该算法考虑了多个组而不是一个，但所有组共享相同的领导者。
- en: The ns-rl algorithm (Shi et al., [2020](#bib.bib83)) can be seen as a version
    of cem-rl whose RL part targets higher novelty by training less novel agents to
    minimize in each step the distance to the BC of the most novel agent. As the most
    novel agent and its BC change in each iteration, the RL part is implemented with
    goal-conditioned policies. This implies that the goal space is identical to the
    behavioral characterization space.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ns-rl 算法（Shi et al., [2020](#bib.bib83)）可以看作是 cem-rl 的一个版本，其 RL 部分通过训练不太新颖的代理，以最小化每一步到最具新颖代理的
    BC 的距离，从而针对更高的新颖性。由于最具新颖性的代理及其 BC 在每次迭代中都会改变，因此 RL 部分使用了目标条件策略。这意味着目标空间与行为特征空间是相同的。
- en: 4.2 Quality-diversity approaches
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 质量-多样性方法
- en: 'Table 4: Quality-Diversity algorithms including an RL component. All these
    algorithms rely on the Map-Elites approach and the BC space is defined manually.
    For each algorithm in the rows, the table states whether quality and diversity
    are optimized using an RL approach or an evolutionary approach.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：包括 RL 组件的质量-多样性算法。这些算法都依赖于 Map-Elites 方法，并且 BC 空间是手动定义的。表格中每个行的算法说明了质量和多样性是否通过
    RL 方法或进化方法进行优化。
- en: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3) scale(1,
    -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15) scale(1,
    -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | Type of Archive | Q. improvement | D. improvement |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g transform="translate(0,0)"><g transform="translate(0,12.3) scale(1,
    -1)"><foreignobject width="31.9" height="12.3" overflow="visible">算法</foreignobject></g></g>
    <g transform="translate(33.29,12.3)"><g transform="translate(0,12.15) scale(1,
    -1)"><foreignobject width="33.29" height="12.15" overflow="visible">属性</foreignobject></g></g></g></svg>
    | 存档类型 | Q. 改进 | D. 改进 |'
- en: '| pga-me Nilsson and Cully ([2021](#bib.bib64)) | Map-Elites | td3 or GA |
    td3 or GA |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| pga-me Nilsson 和 Cully ([2021](#bib.bib64)) | Map-Elites | td3 或 GA | td3
    或 GA |'
- en: '| qd-pg-PF Cideron et al. ([2020](#bib.bib10)) | Pareto front | td3 | td3 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| qd-pg-PF Cideron 等人 ([2020](#bib.bib10)) | Pareto 前沿 | td3 | td3 |'
- en: '| qd-pg-ME Pierrot et al. ([2020](#bib.bib69)) | Map-Elites | td3 | td3 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| qd-pg-ME Pierrot 等人 ([2020](#bib.bib69)) | Map-Elites | td3 | td3 |'
- en: '| cma-mega-ES Tjanaka et al. ([2022](#bib.bib96)) | Map-Elites | cma-es | cma-es
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| cma-mega-ES Tjanaka 等人 ([2022](#bib.bib96)) | Map-Elites | cma-es | cma-es
    |'
- en: '| cma-mega-(td3, ES) Tjanaka et al. ([2022](#bib.bib96)) | Map-Elites | td3
    + cma-es | cma-es |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| cma-mega-(td3, ES) Tjanaka 等人 ([2022](#bib.bib96)) | Map-Elites | td3 + cma-es
    | cma-es |'
- en: By contrast with NS approaches which only try to optimize diversity in the population,
    QD approaches combine this first objective with optimize the performance of registered
    policies, their quality. As Figure LABEL:fig:me suggests, when combined with an
    RL loop, the QD loop can give rise to various solutions depending on whether quality
    and diversity are improved with a evolutionary algorithm or a deep RL algorithm.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于仅尝试优化种群多样性的NS方法，QD方法将这一初始目标与优化注册策略的性能及其质量相结合。正如图 LABEL:fig:me 所示，当与RL循环结合时，QD循环可以根据是否使用进化算法或深度RL算法来改善质量和多样性，从而产生各种解决方案。
- en: 'The space of resulting possibilities is covered in Table [4](#S4.T4 "Table
    4 ‣ 4.2 Quality-diversity approaches ‣ 4 Evolution of policies for diversity ‣
    Combining Evolution and Deep Reinforcement Learning for Policy Search: a Survey").
    In more details, the pga-me algorithm (Nilsson and Cully, [2021](#bib.bib64))
    uses two optimization mechanisms, td3 and a GA, to generate new solutions that
    are added to the archive if they are either novel enough or more efficient than
    previously registered ones with the same behavioral characterization. By contrast,
    in the qd-rl approach, the mechanisms to improve quality and diversity are explicitly
    separated and consist in improving a quality critic and a diversity critic using
    td3\. Two implementations exist. First, the qd-pg-PF algorithm (Cideron et al.,
    [2020](#bib.bib10)) maintains a Pareto front of high quality and diversity solutions.
    From its side, the qd-pg-ME algorithm (Pierrot et al., [2020](#bib.bib69)) maintains
    a Map-Elites archive and introduces an additional notion of state descriptor to
    justify learning a state-based quality critic. Finally, the cma-mega approach
    (Tjanaka et al., [2022](#bib.bib96)) uses an ES to improve diversity and either
    an ES or a combination of ES and RL to improve quality.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '结果可能性空间详见表格 [4](#S4.T4 "Table 4 ‣ 4.2 Quality-diversity approaches ‣ 4 Evolution
    of policies for diversity ‣ Combining Evolution and Deep Reinforcement Learning
    for Policy Search: a Survey")。具体而言，pga-me 算法（Nilsson 和 Cully，[2021](#bib.bib64)）使用两种优化机制，td3
    和 GA，以生成新解决方案，这些解决方案如果足够新颖或比之前注册的具有相同行为特征的方案更高效，则会被添加到档案中。相比之下，在 qd-rl 方法中，提升质量和多样性的机制被明确分开，并包括使用
    td3 改进质量评论员和多样性评论员。存在两种实现方式。首先，qd-pg-PF 算法（Cideron 等人，[2020](#bib.bib10)）维护一个高质量和多样性的
    Pareto 前沿。另一方面，qd-pg-ME 算法（Pierrot 等人，[2020](#bib.bib69)）维护一个 Map-Elites 档案，并引入了状态描述符的附加概念，以解释学习基于状态的质量评论员。最后，cma-mega
    方法（Tjanaka 等人，[2022](#bib.bib96)）使用 ES 改进多样性，使用 ES 或 ES 和 RL 的组合改进质量。'
- en: To summarize, one can see that both quality and diversity can be improved through
    RL, evolution, or both.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来看，可以发现，通过 RL、进化或两者结合，都可以改善质量和多样性。
- en: 5 Evolution of something else
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 其他方面的进化
- en: In all the architecture we have surveyed so far, the evolutionary part was used
    to optimize either policy parameters or a set of rewarding actions in a given
    state. In this section, we briefly cover combinations of evolution and deep RL
    where evolution is used to optimize something else that matters in the RL process,
    or where RL mechanisms are used to improve evolution without calling upon a full
    RL algorithm. We dedicate a separate part to optimizing hyperparameters, as it
    is an important and active domain.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们迄今为止调查的所有架构中，进化部分用于优化策略参数或特定状态下的一组奖励动作。在本节中，我们简要介绍了进化与深度RL的组合，其中进化用于优化RL过程中的其他重要内容，或者RL机制用于改进进化而无需调用完整的RL算法。我们将优化超参数作为一个单独的部分，因为这是一个重要且活跃的领域。
- en: 'Table 5: Algorithms where evolution is applied to something else than action
    or policy parameters, or to more than policy parameters. All algorithms in the
    first half optimize hyperparameters. *: The algorithm in Park and Lee ([2021](#bib.bib65))
    is given no name. BT stands for Behavior Tree.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 5：进化应用于除动作或策略参数之外的其他内容，或超出策略参数的所有算法。前半部分的所有算法都优化了超参数。 *：Park 和 Lee ([2021](#bib.bib65))
    的算法没有名字。BT 代表行为树。
- en: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3) scale(1,
    -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15) scale(1,
    -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | RL algo. | Evo algo. | Evolves what? |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| <svg version="1.1" height="24.45" width="66.58" overflow="visible"><g transform="translate(0,24.45)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,12.3) scale(1,
    -1)"><foreignobject width="31.9" height="12.3" overflow="visible">Algo.</foreignobject></g></g>
    <g  transform="translate(33.29,12.3)"><g transform="translate(0,12.15) scale(1,
    -1)"><foreignobject width="33.29" height="12.15" overflow="visible">Prop.</foreignobject></g></g></g></svg>
    | RL 算法 | Evo 算法 | 进化什么？ |'
- en: '| ga-drl Sehgal et al. ([2019](#bib.bib79)); Sehgal et al. ([2022](#bib.bib80))
    | ddpg (+her) | GA | Hyper-parameters |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| ga-drl Sehgal 等 ([2019](#bib.bib79)); Sehgal 等 ([2022](#bib.bib80)) | ddpg
    (+her) | GA | 超参数 |'
- en: '| pbt Jaderberg et al. ([2017](#bib.bib33)) | Any | Ad hoc | Parameters and
    Hyper-parameters |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| pbt Jaderberg 等 ([2017](#bib.bib33)) | 任何 | Ad hoc | 参数和超参数 |'
- en: '| aac Grigsby et al. ([2021](#bib.bib23)) | sac | Ad hoc | Parameters and Hyper-parameters
    |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| aac Grigsby 等 ([2021](#bib.bib23)) | sac | Ad hoc | 参数和超参数 |'
- en: '| searl Franke et al. ([2021](#bib.bib20)) | td3 | GA | Architecture, Parameters
    and Hyper-parameters |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| searl Franke 等 ([2021](#bib.bib20)) | td3 | GA | 结构、参数和超参数 |'
- en: '| oht-es Tang and Choromanski ([2020](#bib.bib94)) | Any | ES | Hyper-parameters
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| oht-es Tang 和 Choromanski ([2020](#bib.bib94)) | 任何 | ES | 超参数 |'
- en: '| epg Houthooft et al. ([2018](#bib.bib31)) | Ad hoc ($\sim$ ppo) | ES | Reward-related
    functions |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| epg Houthooft 等 ([2018](#bib.bib31)) | Ad hoc ($\sim$ ppo) | ES | 奖励相关函数
    |'
- en: '| eQ Leite et al. ([2020](#bib.bib47)) | $\sim$ ddpg | $\sim$ cem | Critic
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| eQ Leite 等 ([2020](#bib.bib47)) | $\sim$ ddpg | $\sim$ cem | 评论员 |'
- en: '| evo-RL Hallawa et al. ([2017](#bib.bib29)) | Q-learning, dqn, ppo | BT |
    Partial policies |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| evo-RL Hallawa 等 ([2017](#bib.bib29)) | Q-learning, dqn, ppo | BT | 部分策略
    |'
- en: '| derl Gupta et al. ([2021](#bib.bib24)) | ppo | GA | System’s morphology |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| derl Gupta 等 ([2021](#bib.bib24)) | ppo | GA | 系统的形态 |'
- en: '| * Park and Lee ([2021](#bib.bib65)) | ppo | GA | System’s morphology |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| * Park 和 Lee ([2021](#bib.bib65)) | ppo | GA | 系统的形态 |'
- en: 5.1 Evolution in MBRL
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 MBRL 中的演变
- en: 'The cem algorithm can be used to optimize open-loop controllers to perform
    Model Predictive Control (MPC) on robotic systems in the plaNet (Hafner et al.,
    [2019](#bib.bib27)) and poplin (Wang and Ba, [2019](#bib.bib98)) algorithms, and
    an improved version of cem for this specific context is proposed in (Pinneri et al.,
    [2020](#bib.bib71); Pinneri et al., [2021](#bib.bib72)). Besides, this approach
    combining open-loop controllers and MPC is seen in the pets algorithm Chua et al.
    ([2018](#bib.bib9)) as implementing a form of Model-Based Reinforcement Learning
    (MBRL), and cem is used in pets to choose the points from where to start MPC,
    improving over random shooting. Finally, in Bharadhwaj et al. ([2020](#bib.bib3)),
    the authors propose to interleave cem iterations and Stochastic Gradient Descent
    (SGD) iterations to improve the efficiency of optimization of MPC plans, in a
    way reminiscent to cem-rl combining policy gradient steps and cem steps. But all
    these methods are applied to an open-loop control context where true reinforcement
    learning algorithms can not be applied, hence they do not appear in Table [5](#S5.T5
    "Table 5 ‣ 5 Evolution of something else ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey").'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 'cem 算法可用于优化开环控制器，以在 plaNet (Hafner 等，[2019](#bib.bib27)) 和 poplin (Wang 和 Ba，[2019](#bib.bib98))
    算法中执行模型预测控制（MPC）于机器人系统，并且在 (Pinneri 等，[2020](#bib.bib71)；Pinneri 等，[2021](#bib.bib72))
    中提出了一种针对这一特定背景的改进版 cem。此外，这种结合开环控制器和 MPC 的方法在 pets 算法 Chua 等 ([2018](#bib.bib9))
    中被视为实现了一种基于模型的强化学习（MBRL）形式，cem 被用于 pets 中选择开始 MPC 的点，相比于随机发射有所改进。最后，在 Bharadhwaj
    等 ([2020](#bib.bib3)) 中，作者提议交替进行 cem 迭代和随机梯度下降（SGD）迭代，以提高 MPC 计划优化的效率，这种方法类似于
    cem-rl 结合策略梯度步骤和 cem 步骤。但所有这些方法都应用于开环控制背景，其中无法应用真正的强化学习算法，因此它们未出现在表[5](#S5.T5
    "Table 5 ‣ 5 Evolution of something else ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey")中。'
- en: 5.2 Evolution of hyper-parameters
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 超参数的演变
- en: Hyperparameter optimization (HPO) is notoriously hard and often critical in
    deep RL. The most straightforward way to leverage evolutionary methods in this
    context consists in nesting the deep RL algorithm within an evolutionary loop
    which tunes the hyper-parameters. This is the approach of the ga-drl algorithm
    (Sehgal et al., [2019](#bib.bib79); Sehgal et al., [2022](#bib.bib80)), but this
    obviously suffers from a very high computational cost. Note that the authors write
    that ga-drl uses ddpg + her, but the use of her is in no way clear as the algorithm
    does not seem to use goal-conditioned policies.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数优化（HPO）以其困难和在深度 RL 中的关键性而著称。在这种情况下，利用进化方法的最直接方式是在一个进化循环中嵌套深度 RL 算法，该循环调整超参数。这是
    ga-drl 算法（Sehgal 等人，[2019](#bib.bib79)；Sehgal 等人，[2022](#bib.bib80)）的方法，但显然会面临非常高的计算成本。值得注意的是，作者提到
    ga-drl 使用 ddpg + her，但 her 的使用并不清楚，因为算法似乎没有使用目标条件策略。
- en: '![Refer to caption](img/c7941bd2ec26d57e7df7202249551878.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c7941bd2ec26d57e7df7202249551878.png)'
- en: 'Figure 9: The pbt architecture. The evolution part consists of two operators,
    explore and exploit which act both on the hyperparameters and the parameters of
    the agents.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: pbt 架构。进化部分包括两个操作符，探索和利用，它们同时作用于代理的超参数和参数。'
- en: More interestingly, the pbt architecture (Jaderberg et al., [2017](#bib.bib33))
    is designed to solve this problem by combining distributed RL with an evolutionary
    mechanism which acts both on the parameters and hyperparameters within the RL
    training loop. It was successfully used in several challenging applications (Jaderberg
    et al., [2019](#bib.bib32)) and benefits from an interesting capability to tune
    the hyperparameters according to the current training dynamics, which is an important
    meta-learning capability (Khamassi et al., [2017](#bib.bib38)). A follow-up of
    the pbt algorithm is the aac algorithm (Grigsby et al., [2021](#bib.bib23)), which
    basically applies the same approach but with a better set of hyperparameters building
    on lessons learned in the recent deep RL literature.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的是，pbt 架构（Jaderberg 等人，[2017](#bib.bib33)）旨在通过将分布式 RL 与一种进化机制相结合来解决这个问题，该机制同时作用于
    RL 训练循环中的参数和超参数。它成功地应用于多个具有挑战性的应用（Jaderberg 等人，[2019](#bib.bib32)），并具备根据当前训练动态调整超参数的有趣能力，这是一种重要的元学习能力（Khamassi
    等人，[2017](#bib.bib38)）。pbt 算法的后续是 aac 算法（Grigsby 等人，[2021](#bib.bib23)），它基本上应用了相同的方法，但在最近的深度
    RL 文献中通过吸取教训优化了超参数集合。
- en: A limitation of pbt is that each actor uses its own replay buffer. Instead,
    in the searl algorithm (Franke et al., [2021](#bib.bib20)), the experience of
    all agents is shared into a unique buffer. Furthermore, searl  simultaneously
    performs HPO and Neural Architecture Search, resulting in better performance than
    pbt. Finally, the oht-es algorithm (Tang and Choromanski, [2020](#bib.bib94))
    also uses a shared replay buffer, but limits the role of evolution to optimizing
    hyperparameters and does so with an ES algorithm. Given the importance of the
    problem, there are many other HPO methods, most of which are not explicitly calling
    upon an evolutionary approach. For a wider survey of the topic, we refer the reader
    to Parker-Holder et al. ([2022](#bib.bib67)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: pbt 的一个限制是每个演员使用自己的重放缓冲区。相比之下，在 searl 算法（Franke 等人，[2021](#bib.bib20)）中，所有代理的经验被共享到一个唯一的缓冲区。此外，searl
    同时进行超参数优化和神经网络架构搜索，从而比 pbt 达到更好的性能。最后，oht-es 算法（Tang 和 Choromanski，[2020](#bib.bib94)）也使用共享的重放缓冲区，但将进化的角色限制为优化超参数，并通过
    ES 算法来实现。鉴于问题的重要性，还有许多其他超参数优化方法，其中大多数并未明确采用进化方法。有关这一主题的更广泛调查，请参见 Parker-Holder
    等人（[2022](#bib.bib67)）。
- en: 5.3 Evolution of miscellaneous RL or control components
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 杂项 RL 或控制组件的演变
- en: 'Finally, we briefly survey the rest of algorithms listed in Table [5](#S5.T5
    "Table 5 ‣ 5 Evolution of something else ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey"). The epg algorithm (Houthooft et al., [2018](#bib.bib31))
    uses a meta-learning approach to evolve the parameters of a loss function that
    replaces the policy gradient surrogate loss in policy gradient algorithms. The
    goal is to find a reward function that will maximize the capability of an RL algorithm
    to achieve a given task. A consequence of its design is that it cannot be applied
    to an actor-critic approach.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们简要回顾了表[5](#S5.T5 "Table 5 ‣ 5 Evolution of something else ‣ Combining
    Evolution and Deep Reinforcement Learning for Policy Search: a Survey")中列出的其他算法。epg算法（Houthooft
    et al., [2018](#bib.bib31)）采用了元学习方法来进化替代策略梯度算法中策略梯度代理损失的损失函数的参数。其目标是找到一个奖励函数，以最大化RL算法在给定任务中的能力。其设计的一个结果是，它不能应用于演员-评论员方法。'
- en: Instead of evolving a population of agents, the eQ algorithm (Leite et al.,
    [2020](#bib.bib47)) evolves a population of critics, which are fixed over the
    course of learning for a given agent. This is somewhat symmetric to the previously
    mentioned zoac algorithm (Lei et al., [2022](#bib.bib46)) which uses evolution
    to update an actor given a critic trained with RL.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 与其进化一群智能体不同，eQ算法（Leite et al., [2020](#bib.bib47)）则进化了一组评论员，这些评论员在给定智能体的学习过程中是固定的。这在某种程度上与前面提到的zoac算法（Lei
    et al., [2022](#bib.bib46)）相对称，该算法使用进化来更新给定使用RL训练的评论员的演员。
- en: The evo-RL algorithm (Hallawa et al., [2021](#bib.bib28)) evolves partial policies.
    Evolution is performed in a discrete action context with a Genetic Programming
    approach (Koza et al., [1994](#bib.bib41)) that only specifies a partial policy
    as Behavior Trees (Colledanchise and Ögren, [2018](#bib.bib12)). An RL algorithm
    such as dqn (Mnih et al., [2015](#bib.bib62)) or ppo is then in charge of learning
    a policy for the states for which an action is not specified. The fitness of individuals
    is evaluated over their overall behavior combining the BT part and the learned
    part, but only the BT part is evolved to generate the next generation, benefiting
    from a Baldwin effect (Simpson, [1953](#bib.bib88)).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: evo-RL算法（Hallawa et al., [2021](#bib.bib28)）进化部分策略。进化在一个离散动作背景下进行，采用遗传编程方法（Koza
    et al., [1994](#bib.bib41)），只指定了作为行为树（Colledanchise and Ögren, [2018](#bib.bib12)）的部分策略。然后，像dqn（Mnih
    et al., [2015](#bib.bib62)）或ppo这样的RL算法负责为未指定动作的状态学习策略。个体的适应度通过结合BT部分和学习部分的整体行为进行评估，但只有BT部分被进化以生成下一代，受益于鲍德温效应（Simpson,
    [1953](#bib.bib88)）。
- en: 'Finally, several works consider evolving the morphology of a mechanical system
    whose control is learned with RL. Table [5](#S5.T5 "Table 5 ‣ 5 Evolution of something
    else ‣ Combining Evolution and Deep Reinforcement Learning for Policy Search:
    a Survey") only mentions two recent instances, one where the algorithm is not
    named (Gupta et al., [2021](#bib.bib24)) and derl (Park and Lee, [2021](#bib.bib65)),
    but this idea has led to a larger body of works, e.g. (Ha, [2019](#bib.bib25);
    Luck et al., [2020](#bib.bib55)).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，一些研究考虑了进化机械系统的形态，其控制通过RL进行学习。表[5](#S5.T5 "Table 5 ‣ 5 Evolution of something
    else ‣ Combining Evolution and Deep Reinforcement Learning for Policy Search:
    a Survey")仅提到两个近期实例，一个算法未命名（Gupta et al., [2021](#bib.bib24)）和derl（Park and Lee,
    [2021](#bib.bib65)），但这一思想已产生了更多的研究，例如（Ha, [2019](#bib.bib25); Luck et al., [2020](#bib.bib55)）。'
- en: 5.4 Evolution improved with RL mechanisms
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 通过RL机制改进的进化
- en: Without using a full RL part, a few algorithms augment an evolutionary approach
    with components taken from RL.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在不使用完整RL部分的情况下，一些算法通过从RL中提取组件来增强进化方法。
- en: First, the tres (for Trust Region Evolution Strategies) algorithm (Liu et al.,
    [2019](#bib.bib50)) incorporates into an ES several ideas from the trpo (Schulman
    et al., [2015](#bib.bib77)) and ppo (Schulman et al., [2017](#bib.bib78)) algorithms,
    such as introducing an importance sampling mechanism and using a clipped surrogate
    objective so as to enforce a natural gradient update. Unfortunately, tres is neither
    compared to the nes algorithm (Wierstra et al., [2014](#bib.bib102)) which also
    enforces a natural gradient update nor to the safe mutation mechanism of Lehman
    et al. ([2018](#bib.bib44)) which has similar properties.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，tres（信任域进化策略）算法（Liu et al., [2019](#bib.bib50)）将一些来自trpo（Schulman et al.,
    [2015](#bib.bib77)）和ppo（Schulman et al., [2017](#bib.bib78)）算法的理念融入了ES中，如引入重要性采样机制和使用剪切代理目标，以执行自然梯度更新。不幸的是，tres既没有与nes算法（Wierstra
    et al., [2014](#bib.bib102)）进行比较，也没有与具有类似特性的Lehman et al.的安全变异机制（[2018](#bib.bib44)）进行比较。
- en: 'Second, there are two perspectives about the zoac algorithm (Lei et al., [2022](#bib.bib46)).
    One can see it as close to the zospi algorithm described in Section [3](#S3 "3
    Evolution of actions for performance ‣ Combining Evolution and Deep Reinforcement
    Learning for Policy Search: a Survey"), that is an actor-critic method where gradient
    descent to update the actor given the critic is replaced by a more robust derivative-free
    approach. But the more accurate perspective, as put forward by the authors, is
    that zoac is an ES method where the standard ES gradient estimator is replaced
    by a gradient estimator using the advantage function so as to benefit from the
    capabilities of the temporal difference methods to efficiently deal with the temporal
    credit assignment problem.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '其次，对 zoac 算法（Lei 等，[2022](#bib.bib46)）有两种观点。一种观点是，它与第 [3](#S3 "3 Evolution
    of actions for performance ‣ Combining Evolution and Deep Reinforcement Learning
    for Policy Search: a Survey") 节中描述的 zospi 算法相近，即一种演员-评论家方法，其中用更稳健的无导数方法替代了基于评论家的演员更新的梯度下降。然而，作者提出的更准确的观点是，zoac
    是一种 ES 方法，其中标准的 ES 梯度估计器被使用优势函数的梯度估计器所替代，以利用时间差分方法在有效处理时间信用分配问题上的能力。'
- en: Finally, with their guided ES algorithm (Maheswaranathan et al., [2019](#bib.bib57)),
    the authors study how a simple ES gradient estimator can be improved by leveraging
    knowledge of an approximate gradient suffering from bias and variance. Though
    their study is general, it is natural to apply it to the context where the approximate
    gradient is a policy gradient, in which case guided ES combines evolution and
    RL. This work is often cited in a very active recent trend which consists in improving
    the exploration capabilities of ES algorithms by drawing better than Gaussian
    directions to get a more informative gradient approximator (Choromanski et al.,
    [2018](#bib.bib8); [2019](#bib.bib7); Zhang et al., [2020](#bib.bib103); Dereventsov
    et al., [2022](#bib.bib15)). In particular, the sges algorithm (Liu et al., [2020](#bib.bib49))
    leverages both the guided ES idea and the improved exploration ideas to produce
    a competitive ES-based policy search algorithm.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，借助其引导式 ES 算法（Maheswaranathan 等，[2019](#bib.bib57)），作者研究了如何通过利用受偏差和方差影响的近似梯度的知识来改进简单的
    ES 梯度估计器。尽管他们的研究是一般性的，但将其应用于近似梯度为策略梯度的背景是自然的，此时引导式 ES 结合了进化和强化学习。这项工作常常被引用于一种非常活跃的最新趋势中，该趋势通过使用比高斯方向更好的方法来改进
    ES 算法的探索能力，从而获得更有信息的梯度近似器（Choromanski 等，[2018](#bib.bib8)；[2019](#bib.bib7)；Zhang
    等，[2020](#bib.bib103)；Dereventsov 等，[2022](#bib.bib15)）。特别是，sges 算法（Liu 等，[2020](#bib.bib49)）结合了引导式
    ES 思想和改进的探索思想，以生成一种竞争力强的基于 ES 的策略搜索算法。
- en: 6 Conclusion
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper we have provided a list of all the algorithms combining evolutionary
    processes and deep reinforcement learning we could find, irrespective of the publication
    status of the corresponding papers. Our focus was on the mechanisms and our main
    contribution was to provide a categorization of these algorithms into several
    groups of methods, based on the role of evolutionary optimization in the architecture.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提供了一个列表，列出了所有我们能够找到的结合进化过程和深度强化学习的算法，无论相关论文的出版状态如何。我们的重点是机制，主要贡献是将这些算法根据进化优化在架构中的角色分类为几个方法组。
- en: We have not covered related fields such as algorithm combining deep RL and imitation
    learning, though at least one of them also includes evolution (Lü et al., [2021](#bib.bib54)).
    Besides, we have not covered works which focus on the implementation of evolution
    and deep RL combinations, such as (Lee et al., [2020](#bib.bib43)) which shows
    the importance of asynchronism in such combinations.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有涉及相关领域，如结合深度强化学习和模仿学习的算法，尽管其中至少有一个也包括进化（Lü 等，[2021](#bib.bib54)）。此外，我们也未涉及专注于进化和深度强化学习组合实现的工作，例如（Lee
    等，[2020](#bib.bib43)），这表明了这种组合中异步性的的重要性。
- en: Despite these limitations, the scope of the survey was still too broad to enable
    deeper analyses of the different combination methods or a comparative evaluation
    of their performance. In the future, we intend to focus separately on the different
    categories so as to provide these more in-depth analyses and perform comparative
    evaluation of these algorithms between each other and with respect to state of
    the art deep RL algorithms, based on a unified benchmark.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些局限性，调查的范围仍然过于广泛，无法对不同组合方法进行更深入的分析或进行性能的比较评估。未来，我们打算分别关注不同类别，以提供更深入的分析，并在统一基准下，对这些算法之间以及与最先进的深度强化学习算法进行比较评估。
- en: Our focus on elementary mechanisms also suggests the possibility to design new
    combinations of such mechanisms, that is combining the combinations. For instance,
    one may include into a single architecture the idea of selecting samples sent
    to the replay buffer so as to maximize the efficiency of the RL component, more
    efficient crossover or mutation operators as in pderl, soft policy updates, hyperparameter
    tuning etc. No doubt that such combinations will emerge in the future if they
    can result in additional performance gains, despite the additional implementation
    complexity.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对基本机制的关注也暗示了设计这些机制的新组合的可能性，即组合组合。例如，可以在单一架构中包含选择发送到重放缓冲区的样本的思想，以最大化RL组件的效率，更高效的交叉或变异操作符（如pderl），软策略更新，超参数调整等。毫无疑问，如果这些组合能够带来额外的性能提升，尽管实现复杂度增加，这些组合将在未来出现。
- en: Acknowledgments
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The author wants to thank Giuseppe Paolo, Stéphane Doncieux and Antonin Raffin
    for useful remarks about this manuscript as well as several colleagues from ISIR
    for their questions and remarks about the algorithms.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 作者想要感谢Giuseppe Paolo、Stéphane Doncieux 和 Antonin Raffin 对该手稿的有用意见，以及ISIR的几位同事对算法提出的问题和意见。
- en: References
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （1）
- en: 'Bäck et al. (1997) Thomas Bäck, Ulrich Hammel, and H.-P. Schwefel. 1997. Evolutionary
    computation: Comments on the history and current state. *IEEE transactions on
    Evolutionary Computation* 1, 1 (1997), 3–17.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bäck 等（1997）Thomas Bäck、Ulrich Hammel 和 H.-P. Schwefel。1997年。进化计算：对历史和现状的评论。*IEEE进化计算学报*
    1，1（1997），3–17。
- en: Bharadhwaj et al. (2020) Homanga Bharadhwaj, Kevin Xie, and Florian Shkurti.
    2020. Model-predictive control via cross-entropy and gradient-based optimization.
    In *Learning for Dynamics and Control*. PMLR, 277–286.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bharadhwaj 等（2020）Homanga Bharadhwaj、Kevin Xie 和 Florian Shkurti。2020年。通过交叉熵和基于梯度的优化进行模型预测控制。发表于
    *动态和控制的学习*。PMLR，277–286。
- en: Bodnar et al. (2020) Cristian Bodnar, Ben Day, and Pietro Lió. 2020. Proximal
    Distilled Evolutionary Reinforcement Learning. In *The Thirty-Fourth AAAI Conference
    on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications
    of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on
    Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA,
    February 7-12, 2020*. AAAI Press, 3283–3290. [https://aaai.org/ojs/index.php/AAAI/article/view/5728](https://aaai.org/ojs/index.php/AAAI/article/view/5728)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bodnar 等（2020）Cristian Bodnar、Ben Day 和 Pietro Lió。2020年。近端蒸馏进化强化学习。发表于 *第34届AAAI人工智能会议，AAAI
    2020，第32届人工智能创新应用会议，IAAI 2020，第十届AAAI人工智能教育进展研讨会，EAAI 2020，美国纽约，2020年2月7-12日*。AAAI
    Press，3283–3290。 [https://aaai.org/ojs/index.php/AAAI/article/view/5728](https://aaai.org/ojs/index.php/AAAI/article/view/5728)
- en: 'Chang et al. (2018) Simyung Chang, John Yang, Jaeseok Choi, and Nojun Kwak.
    2018. Genetic-Gated Networks for Deep Reinforcement Learning. In *Advances in
    Neural Information Processing Systems 31: Annual Conference on Neural Information
    Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada*,
    Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi,
    and Roman Garnett (Eds.). 1754–1763. [https://proceedings.neurips.cc/paper/2018/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等（2018）Simyung Chang、John Yang、Jaeseok Choi 和 Nojun Kwak。2018年。用于深度强化学习的遗传门控网络。发表于
    *神经信息处理系统进展 31：2018年神经信息处理系统年会，NeurIPS 2018，2018年12月3-8日，加拿大蒙特利尔*，Samy Bengio、Hanna
    M. Wallach、Hugo Larochelle、Kristen Grauman、Nicolò Cesa-Bianchi 和 Roman Garnett（编辑）。1754–1763。
    [https://proceedings.neurips.cc/paper/2018/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/d516b13671a4179d9b7b458a6ebdeb92-Abstract.html)
- en: Chen (2019) Gang Chen. 2019. Merging Deterministic Policy Gradient Estimations
    with Varied Bias-Variance Tradeoff for Effective Deep Reinforcement Learning.
    *ArXiv preprint* abs/1911.10527 (2019). [https://arxiv.org/abs/1911.10527](https://arxiv.org/abs/1911.10527)
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen（2019）Gang Chen。2019。将确定性策略梯度估计与多样化的偏差-方差权衡相结合以有效深度强化学习。*ArXiv 预印本* abs/1911.10527（2019）。[https://arxiv.org/abs/1911.10527](https://arxiv.org/abs/1911.10527)
- en: 'Choromanski et al. (2019) Krzysztof Choromanski, Aldo Pacchiano, Jack Parker-Holder,
    Yunhao Tang, and Vikas Sindhwani. 2019. From Complexity to Simplicity: Adaptive
    ES-Active Subspaces for Blackbox Optimization. In *Advances in Neural Information
    Processing Systems 32: Annual Conference on Neural Information Processing Systems
    2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada*, Hanna M. Wallach,
    Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman
    Garnett (Eds.). 10299–10309. [https://proceedings.neurips.cc/paper/2019/hash/88bade49e98db8790df275fcebb37a13-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/88bade49e98db8790df275fcebb37a13-Abstract.html)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choromanski 等人（2019）Krzysztof Choromanski、Aldo Pacchiano、Jack Parker-Holder、Yunhao
    Tang 和 Vikas Sindhwani。2019。从复杂性到简单性：用于黑箱优化的自适应 ES-Active 子空间。在 *神经信息处理系统进展 32：2019
    年神经信息处理系统年会，NeurIPS 2019，2019 年 12 月 8-14 日，加拿大温哥华*，Hanna M. Wallach、Hugo Larochelle、Alina
    Beygelzimer、Florence d’Alché-Buc、Emily B. Fox 和 Roman Garnett（编）。10299–10309。[https://proceedings.neurips.cc/paper/2019/hash/88bade49e98db8790df275fcebb37a13-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/88bade49e98db8790df275fcebb37a13-Abstract.html)
- en: Choromanski et al. (2018) Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani,
    Richard E. Turner, and Adrian Weller. 2018. Structured Evolution with Compact
    Architectures for Scalable Policy Optimization. In *Proceedings of the 35th International
    Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,
    July 10-15, 2018* *(Proceedings of Machine Learning Research)*, Jennifer G. Dy
    and Andreas Krause (Eds.), Vol. 80. PMLR, 969–977. [http://proceedings.mlr.press/v80/choromanski18a.html](http://proceedings.mlr.press/v80/choromanski18a.html)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choromanski 等人（2018）Krzysztof Choromanski、Mark Rowland、Vikas Sindhwani、Richard
    E. Turner 和 Adrian Weller。2018。具有紧凑架构的结构化演化以实现可扩展的策略优化。在 *第 35 届国际机器学习大会论文集，ICML
    2018，瑞典斯德哥尔摩，2018 年 7 月 10-15 日* *（机器学习研究论文集）*，Jennifer G. Dy 和 Andreas Krause（编），第
    80 卷。PMLR，969–977。[http://proceedings.mlr.press/v80/choromanski18a.html](http://proceedings.mlr.press/v80/choromanski18a.html)
- en: Chua et al. (2018) Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey
    Levine. 2018. Deep reinforcement learning in a handful of trials using probabilistic
    dynamics models. *Advances in neural information processing systems* 31 (2018).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chua 等人（2018）Kurtland Chua、Roberto Calandra、Rowan McAllister 和 Sergey Levine。2018。在少量试验中使用概率动态模型进行深度强化学习。*神经信息处理系统进展*
    31（2018）。
- en: 'Cideron et al. (2020) Geoffrey Cideron, Thomas Pierrot, Nicolas Perrin, Karim
    Beguir, and Olivier Sigaud. 2020. QD-RL: Efficient Mixing of Quality and Diversity
    in Reinforcement Learning. *ArXiv preprint* abs/2006.08505 (2020). [https://arxiv.org/abs/2006.08505](https://arxiv.org/abs/2006.08505)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cideron 等人（2020）Geoffrey Cideron、Thomas Pierrot、Nicolas Perrin、Karim Beguir
    和 Olivier Sigaud。2020。QD-RL: 在强化学习中有效地混合质量与多样性。*ArXiv 预印本* abs/2006.08505（2020）。[https://arxiv.org/abs/2006.08505](https://arxiv.org/abs/2006.08505)'
- en: 'Colas et al. (2018) Cédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer.
    2018. GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning
    Algorithms. In *Proceedings of the 35th International Conference on Machine Learning,
    ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018* *(Proceedings
    of Machine Learning Research)*, Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80.
    PMLR, 1038–1047. [http://proceedings.mlr.press/v80/colas18a.html](http://proceedings.mlr.press/v80/colas18a.html)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Colas 等人（2018）Cédric Colas、Olivier Sigaud 和 Pierre-Yves Oudeyer。2018。GEP-PG:
    在深度强化学习算法中解耦探索与利用。在 *第 35 届国际机器学习大会论文集，ICML 2018，瑞典斯德哥尔摩，2018 年 7 月 10-15 日* *（机器学习研究论文集）*，Jennifer
    G. Dy 和 Andreas Krause（编），第 80 卷。PMLR，1038–1047。[http://proceedings.mlr.press/v80/colas18a.html](http://proceedings.mlr.press/v80/colas18a.html)'
- en: 'Colledanchise and Ögren (2018) Michele Colledanchise and Petter Ögren. 2018.
    *Behavior trees in robotics and AI: An introduction*. CRC Press.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colledanchise 和 Ögren（2018）Michele Colledanchise 和 Petter Ögren。2018。*机器人与人工智能中的行为树：简介*。CRC
    Press。
- en: 'Conti et al. (2018) Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such,
    Joel Lehman, Kenneth O. Stanley, and Jeff Clune. 2018. Improving Exploration in
    Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking
    Agents. In *Advances in Neural Information Processing Systems 31: Annual Conference
    on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,
    Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 5032–5043. [https://proceedings.neurips.cc/paper/2018/hash/b1301141feffabac455e1f90a7de2054-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/b1301141feffabac455e1f90a7de2054-Abstract.html)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conti 等（2018）Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman,
    Kenneth O. Stanley 和 Jeff Clune. 2018. 通过新颖性寻求代理的种群改进深度强化学习中的探索。在 *神经信息处理系统进展
    31：2018 年神经信息处理系统年度会议，NeurIPS 2018，2018 年 12 月 3-8 日，加拿大蒙特利尔* 上，Samy Bengio, Hanna
    M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi 和 Roman Garnett（主编）。5032–5043。
    [https://proceedings.neurips.cc/paper/2018/hash/b1301141feffabac455e1f90a7de2054-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/b1301141feffabac455e1f90a7de2054-Abstract.html)
- en: 'Cully and Demiris (2017) Antoine Cully and Yiannis Demiris. 2017. Quality and
    diversity optimization: A unifying modular framework. *IEEE Transactions on Evolutionary
    Computation* 22, 2 (2017), 245–259.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cully 和 Demiris（2017）Antoine Cully 和 Yiannis Demiris. 2017. 质量与多样性优化：一个统一的模块化框架。*IEEE
    进化计算学报* 22, 2（2017），245–259。
- en: Dereventsov et al. (2022) Anton Dereventsov, Clayton G. Webster, and Joseph
    Daws. 2022. An adaptive stochastic gradient-free approach for high-dimensional
    blackbox optimization. In *Proceedings of International Conference on Computational
    Intelligence*. Springer, 333–348.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dereventsov 等（2022）Anton Dereventsov, Clayton G. Webster 和 Joseph Daws. 2022.
    一种用于高维黑箱优化的自适应随机梯度自由方法。在 *国际计算智能会议论文集* 上。Springer, 333–348。
- en: Doan et al. (2019) Thang Doan, Bogdan Mazoure, Audrey Durand, Joelle Pineau,
    and R Devon Hjelm. 2019. Attraction-Repulsion Actor-Critic for Continuous Control
    Reinforcement Learning. *ArXiv preprint* abs/1909.07543 (2019). [https://arxiv.org/abs/1909.07543](https://arxiv.org/abs/1909.07543)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doan 等（2019）Thang Doan, Bogdan Mazoure, Audrey Durand, Joelle Pineau 和 R Devon
    Hjelm. 2019. 连续控制强化学习的吸引-排斥 Actor-Critic。*ArXiv 预印本* abs/1909.07543（2019）。 [https://arxiv.org/abs/1909.07543](https://arxiv.org/abs/1909.07543)
- en: 'Doncieux et al. (2019) Stephane Doncieux, Alban Laflaquière, and Alexandre
    Coninx. 2019. Novelty search: a theoretical perspective. In *Proceedings of the
    Genetic and Evolutionary Computation Conference*. 99–106.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doncieux 等（2019）Stephane Doncieux, Alban Laflaquière 和 Alexandre Coninx. 2019.
    新颖性搜索：理论视角。在 *遗传与进化计算会议论文集* 上。99–106。
- en: 'Drugan (2019) Madalina M. Drugan. 2019. Reinforcement learning versus evolutionary
    computation: A survey on hybrid algorithms. *Swarm and evolutionary computation*
    44 (2019), 228–246.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Drugan（2019）Madalina M. Drugan. 2019. 强化学习与进化计算的对比：混合算法的综述。*群体与进化计算* 44（2019），228–246。
- en: Espositi and Bonarini (2020) Federico Espositi and Andrea Bonarini. 2020. Gradient
    Bias to Solve the Generalization Limit of Genetic Algorithms Through Hybridization
    with Reinforcement Learning. In *International Conference on Machine Learning,
    Optimization, and Data Science*. Springer, 273–284.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Espositi 和 Bonarini（2020）Federico Espositi 和 Andrea Bonarini. 2020. 通过与强化学习的混合化解决遗传算法的泛化限制的梯度偏差。在
    *国际机器学习、优化和数据科学会议* 上。Springer, 273–284。
- en: Franke et al. (2021) Jörg K. H. Franke, Gregor Köhler, André Biedenkapp, and
    Frank Hutter. 2021. Sample-Efficient Automated Deep Reinforcement Learning. In
    *9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021*. OpenReview.net. [https://openreview.net/forum?id=hSjxQ3B7GWq](https://openreview.net/forum?id=hSjxQ3B7GWq)
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Franke 等（2021）Jörg K. H. Franke, Gregor Köhler, André Biedenkapp 和 Frank Hutter.
    2021. 样本高效的自动深度强化学习。在 *第 9 届国际学习表示会议，ICLR 2021，虚拟活动，奥地利，2021 年 5 月 3-7 日* 上。OpenReview.net。
    [https://openreview.net/forum?id=hSjxQ3B7GWq](https://openreview.net/forum?id=hSjxQ3B7GWq)
- en: Fujimoto et al. (2018) Scott Fujimoto, Herke van Hoof, and David Meger. 2018.
    Addressing Function Approximation Error in Actor-Critic Methods. In *Proceedings
    of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan,
    Stockholm, Sweden, July 10-15, 2018* *(Proceedings of Machine Learning Research)*,
    Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80. PMLR, 1582–1591. [http://proceedings.mlr.press/v80/fujimoto18a.html](http://proceedings.mlr.press/v80/fujimoto18a.html)
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fujimoto等 (2018) Scott Fujimoto, Herke van Hoof, 和 David Meger。2018年。解决演员-评论家方法中的函数逼近误差。在*第35届国际机器学习会议论文集，ICML
    2018，瑞典斯德哥尔摩，2018年7月10-15日* *(机器学习研究论文集)*，Jennifer G. Dy 和 Andreas Krause (Eds.)，第80卷。PMLR，1582–1591。
    [http://proceedings.mlr.press/v80/fujimoto18a.html](http://proceedings.mlr.press/v80/fujimoto18a.html)
- en: Gangwani and Peng (2018) Tanmay Gangwani and Jian Peng. 2018. Policy Optimization
    by Genetic Distillation. In *6th International Conference on Learning Representations,
    ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*.
    OpenReview.net. [https://openreview.net/forum?id=ByOnmlWC-](https://openreview.net/forum?id=ByOnmlWC-)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gangwani和Peng (2018) Tanmay Gangwani 和 Jian Peng。2018年。通过遗传蒸馏进行策略优化。在*第6届国际学习表征会议，ICLR
    2018，加拿大温哥华，2018年4月30日 - 5月3日，会议跟踪论文集*。OpenReview.net。 [https://openreview.net/forum?id=ByOnmlWC-](https://openreview.net/forum?id=ByOnmlWC-)
- en: Grigsby et al. (2021) Jake Grigsby, Jin Yong Yoo, and Yanjun Qi. 2021. Towards
    Automatic Actor-Critic Solutions to Continuous Control. *ArXiv preprint* abs/2106.08918
    (2021). [https://arxiv.org/abs/2106.08918](https://arxiv.org/abs/2106.08918)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grigsby等 (2021) Jake Grigsby, Jin Yong Yoo, 和 Yanjun Qi。2021年。朝着自动化演员-评论家解决方案进行连续控制。*ArXiv预印本*
    abs/2106.08918 (2021)。 [https://arxiv.org/abs/2106.08918](https://arxiv.org/abs/2106.08918)
- en: Gupta et al. (2021) Agrim Gupta, Silvio Savarese, Surya Ganguli, and Li Fei-Fei.
    2021. Embodied Intelligence via Learning and Evolution. *ArXiv preprint* abs/2102.02202
    (2021). [https://arxiv.org/abs/2102.02202](https://arxiv.org/abs/2102.02202)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta等 (2021) Agrim Gupta, Silvio Savarese, Surya Ganguli, 和 Li Fei-Fei。2021年。通过学习和进化实现具身智能。*ArXiv预印本*
    abs/2102.02202 (2021)。 [https://arxiv.org/abs/2102.02202](https://arxiv.org/abs/2102.02202)
- en: Ha (2019) David Ha. 2019. Reinforcement learning for improving agent design.
    *Artificial life* 25, 4 (2019), 352–365.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ha (2019) David Ha。2019年。用于改进代理设计的强化学习。*人工生命* 25, 4 (2019)，352–365。
- en: Haarnoja et al. (2018) Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George
    Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel,
    et al. 2018. Soft actor-critic algorithms and applications. *ArXiv preprint* abs/1812.05905
    (2018). [https://arxiv.org/abs/1812.05905](https://arxiv.org/abs/1812.05905)
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haarnoja等 (2018) Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George
    Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel
    等。2018年。软演员-评论家算法及应用。*ArXiv预印本* abs/1812.05905 (2018)。 [https://arxiv.org/abs/1812.05905](https://arxiv.org/abs/1812.05905)
- en: Hafner et al. (2019) Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas,
    David Ha, Honglak Lee, and James Davidson. 2019. Learning latent dynamics for
    planning from pixels. In *International conference on machine learning*. PMLR,
    2555–2565.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner等 (2019) Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas,
    David Ha, Honglak Lee, 和 James Davidson。2019年。从像素中学习潜在动态以进行规划。在*国际机器学习会议*中。PMLR，2555–2565。
- en: 'Hallawa et al. (2021) Ahmed Hallawa, Thorsten Born, Anke Schmeink, Guido Dartmann,
    Arne Peine, Lukas Martin, Giovanni Iacca, AE Eiben, and Gerd Ascheid. 2021. Evo-RL:
    evolutionary-driven reinforcement learning. In *Proceedings of the Genetic and
    Evolutionary Computation Conference Companion*. 153–154.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hallawa等 (2021) Ahmed Hallawa, Thorsten Born, Anke Schmeink, Guido Dartmann,
    Arne Peine, Lukas Martin, Giovanni Iacca, AE Eiben, 和 Gerd Ascheid。2021年。Evo-RL：进化驱动的强化学习。在*遗传与进化计算会议论文集*中。153–154。
- en: 'Hallawa et al. (2017) Ahmed Hallawa, Jaro De Roose, Martin Andraud, Marian
    Verhelst, and Gerd Ascheid. 2017. Instinct-driven dynamic hardware reconfiguration:
    evolutionary algorithm optimized compression for autonomous sensory agents. In
    *Proceedings of the Genetic and Evolutionary Computation Conference Companion*.
    1727–1734.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hallawa等 (2017) Ahmed Hallawa, Jaro De Roose, Martin Andraud, Marian Verhelst,
    和 Gerd Ascheid。2017年。由本能驱动的动态硬件重配置：针对自主传感器的进化算法优化压缩。在*遗传与进化计算会议论文集*中。1727–1734。
- en: Holland and Reitman (1978) John H Holland and Judith S Reitman. 1978. Cognitive
    systems based on adaptive algorithms. In *Pattern-directed inference systems*.
    Elsevier, 313–329.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holland和Reitman (1978) John H Holland 和 Judith S Reitman。1978年。基于自适应算法的认知系统。在*模式引导推理系统*中。Elsevier，313–329。
- en: 'Houthooft et al. (2018) Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly C.
    Stadie, Filip Wolski, Jonathan Ho, and Pieter Abbeel. 2018. Evolved Policy Gradients.
    In *Advances in Neural Information Processing Systems 31: Annual Conference on
    Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,
    Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 5405–5414. [https://proceedings.neurips.cc/paper/2018/hash/7876acb66640bad41f1e1371ef30c180-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/7876acb66640bad41f1e1371ef30c180-Abstract.html)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houthooft 等（2018）Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly C. Stadie,
    Filip Wolski, Jonathan Ho, 和 Pieter Abbeel. 2018. Evolved Policy Gradients. 在
    *NeurIPS 2018 年度会议：神经信息处理系统 2018，2018 年 12 月 3-8 日，加拿大蒙特利尔*，Samy Bengio, Hanna
    M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi 和 Roman Garnett（编者）。5405–5414.
    [https://proceedings.neurips.cc/paper/2018/hash/7876acb66640bad41f1e1371ef30c180-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/7876acb66640bad41f1e1371ef30c180-Abstract.html)
- en: Jaderberg et al. (2019) Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke
    Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz,
    Ari S Morcos, Avraham Ruderman, et al. 2019. Human-level performance in 3D multiplayer
    games with population-based reinforcement learning. *Science* 364, 6443 (2019),
    859–865.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等（2019）Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris,
    Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S
    Morcos, Avraham Ruderman 等。2019. 使用基于人群的强化学习在 3D 多人游戏中达到人类水平的表现。*Science* 364,
    6443 (2019), 859–865.
- en: Jaderberg et al. (2017) Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M
    Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen
    Simonyan, et al. 2017. Population-based training of neural networks. *ArXiv preprint*
    abs/1711.09846 (2017). [https://arxiv.org/abs/1711.09846](https://arxiv.org/abs/1711.09846)
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等（2017）Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech
    M Czarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning,
    Karen Simonyan 等。2017. 基于人群的神经网络训练。*ArXiv 预印本* abs/1711.09846 (2017). [https://arxiv.org/abs/1711.09846](https://arxiv.org/abs/1711.09846)
- en: Jung et al. (2020) Whiyoung Jung, Giseung Park, and Youngchul Sung. 2020. Population-Guided
    Parallel Policy Search for Reinforcement Learning. In *8th International Conference
    on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*.
    OpenReview.net. [https://openreview.net/forum?id=rJeINp4KwH](https://openreview.net/forum?id=rJeINp4KwH)
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jung 等（2020）Whiyoung Jung, Giseung Park, 和 Youngchul Sung. 2020. 基于人群的并行策略搜索用于强化学习。在
    *第 8 届国际学习表示会议，ICLR 2020，2020 年 4 月 26-30 日，埃提奥比亚亚的斯亚贝巴*。OpenReview.net. [https://openreview.net/forum?id=rJeINp4KwH](https://openreview.net/forum?id=rJeINp4KwH)
- en: 'Kalashnikov et al. (2018) Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian
    Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan,
    Vincent Vanhoucke, et al. 2018. Qt-opt: Scalable deep reinforcement learning for
    vision-based robotic manipulation. *ArXiv preprint* abs/1806.10293 (2018). [https://arxiv.org/abs/1806.10293](https://arxiv.org/abs/1806.10293)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kalashnikov 等（2018）Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz,
    Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan,
    Vincent Vanhoucke 等。2018. Qt-opt: 可扩展的深度强化学习用于基于视觉的机器人操作。*ArXiv 预印本* abs/1806.10293
    (2018). [https://arxiv.org/abs/1806.10293](https://arxiv.org/abs/1806.10293)'
- en: Khadka et al. (2019) Shauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel,
    Evren Tumer, Santiago Miret, Yinyin Liu, and Kagan Tumer. 2019. Collaborative
    Evolutionary Reinforcement Learning. In *Proceedings of the 36th International
    Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
    USA* *(Proceedings of Machine Learning Research)*, Kamalika Chaudhuri and Ruslan
    Salakhutdinov (Eds.), Vol. 97. PMLR, 3341–3350. [http://proceedings.mlr.press/v97/khadka19a.html](http://proceedings.mlr.press/v97/khadka19a.html)
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khadka 等（2019）Shauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel, Evren
    Tumer, Santiago Miret, Yinyin Liu 和 Kagan Tumer. 2019. 协作进化强化学习。在 *第 36 届国际机器学习会议，ICML
    2019，2019 年 6 月 9-15 日，美国加利福尼亚州长滩* *（机器学习研究会议论文集）*，Kamalika Chaudhuri 和 Ruslan
    Salakhutdinov（编者），第 97 卷。PMLR, 3341–3350. [http://proceedings.mlr.press/v97/khadka19a.html](http://proceedings.mlr.press/v97/khadka19a.html)
- en: 'Khadka and Tumer (2018) Shauharda Khadka and Kagan Tumer. 2018. Evolution-Guided
    Policy Gradient in Reinforcement Learning. In *Advances in Neural Information
    Processing Systems 31: Annual Conference on Neural Information Processing Systems
    2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada*, Samy Bengio, Hanna M.
    Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett
    (Eds.). 1196–1208. [https://proceedings.neurips.cc/paper/2018/hash/85fc37b18c57097425b52fc7afbb6969-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/85fc37b18c57097425b52fc7afbb6969-Abstract.html)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khadka 和 Tumer（2018）Shauharda Khadka 和 Kagan Tumer. 2018. 在强化学习中的演化引导策略梯度。载于
    *神经信息处理系统进展 31：2018年神经信息处理系统年度会议，NeurIPS 2018，2018年12月3-8日，加拿大蒙特利尔*，Samy Bengio,
    Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi 和 Roman
    Garnett（编辑）。1196–1208。 [https://proceedings.neurips.cc/paper/2018/hash/85fc37b18c57097425b52fc7afbb6969-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/85fc37b18c57097425b52fc7afbb6969-Abstract.html)
- en: Khamassi et al. (2017) Mehdi Khamassi, George Velentzas, Theodore Tsitsimis,
    and Costas Tzafestas. 2017. Active exploration and parameterized reinforcement
    learning applied to a simulated human-robot interaction task. In *2017 First IEEE
    International Conference on Robotic Computing (IRC)*. IEEE, 28–35.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khamassi 等（2017）Mehdi Khamassi, George Velentzas, Theodore Tsitsimis 和 Costas
    Tzafestas. 2017. 将主动探索和参数化强化学习应用于模拟的人机交互任务。载于 *2017年首届IEEE国际机器人计算会议（IRC）*。IEEE，28–35。
- en: Kim et al. (2007) Kyung-Joong Kim, Heejin Choi, and Sung-Bae Cho. 2007. Hybrid
    of evolution and reinforcement learning for othello players. In *2007 IEEE Symposium
    on Computational Intelligence and Games*. IEEE, 203–209.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2007）Kyung-Joong Kim, Heejin Choi 和 Sung-Bae Cho. 2007. 将演化与强化学习结合用于 Othello
    玩家。载于 *2007 IEEE 计算智能与游戏研讨会*。IEEE，203–209。
- en: 'Kim et al. (2020) Namyong Kim, Hyunsuk Baek, and Hayong Shin. 2020. PGPS: Coupling
    Policy Gradient with Population-based Search. In *Submitted to ICLR 2021*.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2020）Namyong Kim, Hyunsuk Baek 和 Hayong Shin. 2020. PGPS：将策略梯度与基于种群的搜索结合。载于
    *提交至 ICLR 2021*。
- en: Koza et al. (1994) John R. Koza et al. 1994. *Genetic programming II*. Vol. 17.
    MIT press Cambridge.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koza 等（1994）John R. Koza 等. 1994. *遗传编程 II*。第 17 卷。MIT press Cambridge。
- en: Lanzi (1999) Pier Luca Lanzi. 1999. An analysis of generalization in the XCS
    classifier system. *Evolutionary computation* 7, 2 (1999), 125–149.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lanzi（1999）Pier Luca Lanzi. 1999. XCS 分类器系统中的泛化分析。*演化计算* 7, 2（1999），125–149。
- en: 'Lee et al. (2020) Kyunghyun Lee, Byeong-Uk Lee, Ukcheol Shin, and In So Kweon.
    2020. An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based
    Policy Search. In *Advances in Neural Information Processing Systems 33: Annual
    Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
    6-12, 2020, virtual*, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
    Balcan, and Hsuan-Tien Lin (Eds.). [https://proceedings.neurips.cc/paper/2020/hash/731309c4bb223491a9f67eac5214fb2e-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/731309c4bb223491a9f67eac5214fb2e-Abstract.html)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2020）Kyunghyun Lee, Byeong-Uk Lee, Ukcheol Shin 和 In So Kweon. 2020. 一种高效的异步方法用于整合演化与基于梯度的策略搜索。载于
    *神经信息处理系统进展 33：2020年神经信息处理系统年度会议，NeurIPS 2020，2020年12月6-12日，虚拟*，Hugo Larochelle,
    Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan 和 Hsuan-Tien Lin（编辑）。
    [https://proceedings.neurips.cc/paper/2020/hash/731309c4bb223491a9f67eac5214fb2e-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/731309c4bb223491a9f67eac5214fb2e-Abstract.html)
- en: Lehman et al. (2018) Joel Lehman, Jay Chen, Jeff Clune, and Kenneth O Stanley.
    2018. Safe mutations for deep and recurrent neural networks through output gradients.
    In *Proceedings of the Genetic and Evolutionary Computation Conference*. 117–124.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lehman 等（2018）Joel Lehman, Jay Chen, Jeff Clune 和 Kenneth O Stanley. 2018. 通过输出梯度实现深度和递归神经网络的安全突变。载于
    *遗传与进化计算会议论文集*。117–124。
- en: 'Lehman and Stanley (2011) Joel Lehman and Kenneth O Stanley. 2011. Abandoning
    objectives: Evolution through the search for novelty alone. *Evolutionary computation*
    19, 2 (2011), 189–223.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lehman 和 Stanley（2011）Joel Lehman 和 Kenneth O Stanley. 2011. 放弃目标：通过对新颖性的搜索进行演化。*演化计算*
    19, 2（2011），189–223。
- en: Lei et al. (2022) Yuheng Lei, Jianyu Chen, Shengbo Eben Li, and Sifa Zheng.
    2022. Zeroth-Order Actor-Critic. *ArXiv preprint* abs/2201.12518 (2022). [https://arxiv.org/abs/2201.12518](https://arxiv.org/abs/2201.12518)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等（2022）Yuheng Lei, Jianyu Chen, Shengbo Eben Li 和 Sifa Zheng. 2022. 零阶演员-评论家。*ArXiv
    预印本* abs/2201.12518（2022）。 [https://arxiv.org/abs/2201.12518](https://arxiv.org/abs/2201.12518)
- en: 'Leite et al. (2020) Abe Leite, Madhavun Candadai, and Eduardo J Izquierdo.
    2020. Reinforcement learning beyond the Bellman equation: Exploring critic objectives
    using evolution. In *ALIFE 2020: The 2020 Conference on Artificial Life*.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leite et al. (2020) Abe Leite, Madhavun Candadai, 和 Eduardo J Izquierdo. 2020.
    超越贝尔曼方程的强化学习：使用进化探索评论目标. 见于 *ALIFE 2020：2020年人工生命会议*。
- en: Lillicrap et al. (2016) Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,
    Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous
    control with deep reinforcement learning. In *4th International Conference on
    Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference
    Track Proceedings*, Yoshua Bengio and Yann LeCun (Eds.). [http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lillicrap et al. (2016) Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel,
    Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, 和 Daan Wierstra. 2016. 使用深度强化学习的连续控制.
    见于 *第4届国际学习表示会议，ICLR 2016，波多黎各圣胡安，2016年5月2-4日，会议论文集*，Yoshua Bengio 和 Yann LeCun（编）。[http://arxiv.org/abs/1509.02971](http://arxiv.org/abs/1509.02971)
- en: Liu et al. (2020) Fei-Yu Liu, Zi-Niu Li, and Chao Qian. 2020. Self-Guided Evolution
    Strategies with Historical Estimated Gradients. In *Proceedings of the Twenty-Ninth
    International Joint Conference on Artificial Intelligence, IJCAI 2020*, Christian
    Bessiere (Ed.). ijcai.org, 1474–1480. [https://doi.org/10.24963/ijcai.2020/205](https://doi.org/10.24963/ijcai.2020/205)
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020) Fei-Yu Liu, Zi-Niu Li, 和 Chao Qian. 2020. 基于历史估计梯度的自指导进化策略.
    见于 *第二十九届国际人工智能联合会议，IJCAI 2020*，Christian Bessiere（编）。ijcai.org, 1474–1480. [https://doi.org/10.24963/ijcai.2020/205](https://doi.org/10.24963/ijcai.2020/205)
- en: Liu et al. (2019) Guoqing Liu, Li Zhao, Feidiao Yang, Jiang Bian, Tao Qin, Nenghai
    Yu, and Tie-Yan Liu. 2019. Trust Region Evolution Strategies. In *The Thirty-Third
    AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative
    Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI
    Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu,
    Hawaii, USA, January 27 - February 1, 2019*. AAAI Press, 4352–4359. [https://doi.org/10.1609/aaai.v33i01.33014352](https://doi.org/10.1609/aaai.v33i01.33014352)
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019) Guoqing Liu, Li Zhao, Feidiao Yang, Jiang Bian, Tao Qin, Nenghai
    Yu, 和 Tie-Yan Liu. 2019. 信任区域进化策略. 见于 *第三十三届AAAI人工智能会议，AAAI 2019，第三十一届人工智能创新应用会议，IAAI
    2019，第九届AAAI人工智能教育进展研讨会，EAAI 2019，夏威夷檀香山，美国，2019年1月27日 - 2月1日*。AAAI Press, 4352–4359.
    [https://doi.org/10.1609/aaai.v33i01.33014352](https://doi.org/10.1609/aaai.v33i01.33014352)
- en: Liu and Feng (2021) Jian Liu and Liming Feng. 2021. Diversity Evolutionary Policy
    Deep Reinforcement Learning. *Computational Intelligence and Neuroscience* 2021
    (2021).
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu and Feng (2021) Jian Liu 和 Liming Feng. 2021. 多样性进化策略深度强化学习. *计算智能与神经科学*
    2021 (2021)。
- en: 'Liu et al. (2018) Qihao Liu, Yujia Wang, and Xiaofeng Liu. 2018. PNS: Population-Guided
    Novelty Search for Reinforcement Learning in Hard Exploration Environments. In
    *2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.
    IEEE, 5627–5634.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018) Qihao Liu, Yujia Wang, 和 Xiaofeng Liu. 2018. PNS：用于困难探索环境中的强化学习的群体引导新颖性搜索.
    见于 *2021 IEEE/RSJ国际智能机器人与系统会议（IROS）*。IEEE, 5627–5634。
- en: Liu et al. (2017) Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. 2017.
    Stein Variational Policy Gradient. In *Proceedings of the Thirty-Third Conference
    on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August
    11-15, 2017*, Gal Elidan, Kristian Kersting, and Alexander T. Ihler (Eds.). AUAI
    Press. [http://auai.org/uai2017/proceedings/papers/239.pdf](http://auai.org/uai2017/proceedings/papers/239.pdf)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2017) Yang Liu, Prajit Ramachandran, Qiang Liu, 和 Jian Peng. 2017.
    Stein变分策略梯度. 见于 *第三十三届人工智能不确定性会议，UAI 2017，澳大利亚悉尼，2017年8月11-15日*，Gal Elidan, Kristian
    Kersting, 和 Alexander T. Ihler（编）。AUAI Press. [http://auai.org/uai2017/proceedings/papers/239.pdf](http://auai.org/uai2017/proceedings/papers/239.pdf)
- en: Lü et al. (2021) Shuai Lü, Shuai Han, Wenbo Zhou, and Junwei Zhang. 2021. Recruitment-imitation
    mechanism for evolutionary reinforcement learning. *Information Sciences* 553
    (2021), 172–188.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lü et al. (2021) Shuai Lü, Shuai Han, Wenbo Zhou, 和 Junwei Zhang. 2021. 进化强化学习的招募模仿机制.
    *信息科学* 553 (2021), 172–188。
- en: Luck et al. (2020) Kevin Sebastian Luck, Heni Ben Amor, and Roberto Calandra.
    2020. Data-efficient co-adaptation of morphology and behaviour with deep reinforcement
    learning. In *Conference on Robot Learning*. PMLR, 854–869.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luck et al. (2020) Kevin Sebastian Luck, Heni Ben Amor, 和 Roberto Calandra.
    2020. 数据高效的形态与行为共同适应深度强化学习. 见于 *机器人学习会议*。PMLR, 854–869。
- en: Ma et al. (2022) Yan Ma, Tianxing Liu, Bingsheng Wei, Yi Liu, Kang Xu, and Wei
    Li. 2022. Evolutionary Action Selection for Gradient-based Policy Learning. *ArXiv
    preprint* abs/2201.04286 (2022). [https://arxiv.org/abs/2201.04286](https://arxiv.org/abs/2201.04286)
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2022) Yan Ma, Tianxing Liu, Bingsheng Wei, Yi Liu, Kang Xu, and Wei
    Li. 2022. 基于梯度的策略学习的进化动作选择。*ArXiv preprint* abs/2201.04286 (2022). [https://arxiv.org/abs/2201.04286](https://arxiv.org/abs/2201.04286)
- en: 'Maheswaranathan et al. (2019) Niru Maheswaranathan, Luke Metz, George Tucker,
    Dami Choi, and Jascha Sohl-Dickstein. 2019. Guided evolutionary strategies: augmenting
    random search with surrogate gradients. In *Proceedings of the 36th International
    Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
    USA* *(Proceedings of Machine Learning Research)*, Kamalika Chaudhuri and Ruslan
    Salakhutdinov (Eds.), Vol. 97\. PMLR, 4264–4273. [http://proceedings.mlr.press/v97/maheswaranathan19a.html](http://proceedings.mlr.press/v97/maheswaranathan19a.html)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maheswaranathan et al. (2019) Niru Maheswaranathan, Luke Metz, George Tucker,
    Dami Choi, and Jascha Sohl-Dickstein. 2019. 指导性进化策略：通过代理梯度增强随机搜索。在*第36届国际机器学习大会，ICML
    2019，2019年6月9-15日，加州洛杉矶，USA* *(机器学习研究会议录)*，Kamalika Chaudhuri 和 Ruslan Salakhutdinov（编辑），第97卷。PMLR,
    4264–4273. [http://proceedings.mlr.press/v97/maheswaranathan19a.html](http://proceedings.mlr.press/v97/maheswaranathan19a.html)
- en: 'Majid et al. (2021) Amjad Yousef Majid, Serge Saaybi, Tomas van Rietbergen,
    Vincent Francois-Lavet, R. Venkatesha Prasad, and Chris Verhoeven. 2021. Deep
    Reinforcement Learning Versus Evolution Strategies: A Comparative Survey. *ArXiv
    preprint* abs/2110.01411 (2021). [https://arxiv.org/abs/2110.01411](https://arxiv.org/abs/2110.01411)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Majid et al. (2021) Amjad Yousef Majid, Serge Saaybi, Tomas van Rietbergen,
    Vincent Francois-Lavet, R. Venkatesha Prasad, and Chris Verhoeven. 2021. 深度强化学习与进化策略：比较调查。*ArXiv
    preprint* abs/2110.01411 (2021). [https://arxiv.org/abs/2110.01411](https://arxiv.org/abs/2110.01411)
- en: 'Mania et al. (2018) Horia Mania, Aurelia Guy, and Benjamin Recht. 2018. Simple
    random search of static linear policies is competitive for reinforcement learning.
    In *Advances in Neural Information Processing Systems 31: Annual Conference on
    Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,
    Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 1805–1814. [https://proceedings.neurips.cc/paper/2018/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mania et al. (2018) Horia Mania, Aurelia Guy, and Benjamin Recht. 2018. 静态线性策略的简单随机搜索在强化学习中具有竞争力。在*神经信息处理系统进展
    31：2018年神经信息处理系统年会，NeurIPS 2018，2018年12月3-8日，加拿大蒙特利尔*，Samy Bengio, Hanna M. Wallach,
    Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, 和 Roman Garnett（编辑）。1805–1814.
    [https://proceedings.neurips.cc/paper/2018/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/7634ea65a4e6d9041cfd3f7de18e334a-Abstract.html)
- en: Marchesini et al. (2021) Enrico Marchesini, Davide Corsi, and Alessandro Farinelli.
    2021. Genetic Soft Updates for Policy Evolution in Deep Reinforcement Learning.
    In *9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021*. OpenReview.net. [https://openreview.net/forum?id=TGFO0DbD_pk](https://openreview.net/forum?id=TGFO0DbD_pk)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marchesini et al. (2021) Enrico Marchesini, Davide Corsi, and Alessandro Farinelli.
    2021. 深度强化学习中策略进化的遗传软更新。在*第9届国际学习表征会议，ICLR 2021，虚拟活动，奥地利，2021年5月3-7日*。OpenReview.net.
    [https://openreview.net/forum?id=TGFO0DbD_pk](https://openreview.net/forum?id=TGFO0DbD_pk)
- en: 'Miller and Harding (2009) Julian Francis Miller and Simon L Harding. 2009.
    Cartesian genetic programming. In *Proceedings of the 11th annual conference companion
    on genetic and evolutionary computation conference: late breaking papers*. 3489–3512.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miller and Harding (2009) Julian Francis Miller and Simon L Harding. 2009. 笛卡尔遗传编程。在*第11届年度遗传和进化计算会议：晚期论文会议录*。3489–3512.
- en: Mnih et al. (2015) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A.
    Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K.
    Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement
    learning. *Nature* 518, 7540 (2015), 529.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih et al. (2015) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A.
    Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas
    K. Fidjeland, Georg Ostrovski, et al. 2015. 通过深度强化学习实现人类水平控制。*Nature* 518, 7540
    (2015), 529.
- en: 'Mouret (2020) Jean-Baptiste Mouret. 2020. Evolving the behavior of machines:
    from micro to macroevolution. *Iscience* 23, 11 (2020), 101731.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mouret (2020) Jean-Baptiste Mouret. 2020. 机器行为的进化：从微观到宏观进化。*Iscience* 23, 11
    (2020), 101731.
- en: Nilsson and Cully (2021) Olle Nilsson and Antoine Cully. 2021. Policy gradient
    assisted MAP-Elites. In *Proceedings of the Genetic and Evolutionary Computation
    Conference*. 866–875.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nilsson 和 Cully（2021）Olle Nilsson 和 Antoine Cully。2021。策略梯度辅助 MAP-Elites。发表于
    *遗传与进化计算会议论文集*。866–875。
- en: Park and Lee (2021) Jai Hoon Park and Kang Hoon Lee. 2021. Computational Design
    of Modular Robots Based on Genetic Algorithm and Reinforcement Learning. *Symmetry*
    13, 3 (2021), 471.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 和 Lee（2021）Jai Hoon Park 和 Kang Hoon Lee。2021。基于遗传算法和强化学习的模块化机器人计算设计。*对称性*
    13, 3（2021），471。
- en: 'Parker-Holder et al. (2020) Jack Parker-Holder, Aldo Pacchiano, Krzysztof Marcin
    Choromanski, and Stephen J. Roberts. 2020. Effective Diversity in Population Based
    Reinforcement Learning. In *Advances in Neural Information Processing Systems
    33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
    December 6-12, 2020, virtual*, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
    Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). [https://proceedings.neurips.cc/paper/2020/hash/d1dc3a8270a6f9394f88847d7f0050cf-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/d1dc3a8270a6f9394f88847d7f0050cf-Abstract.html)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Parker-Holder 等（2020）Jack Parker-Holder、Aldo Pacchiano、Krzysztof Marcin Choromanski
    和 Stephen J. Roberts。2020。在基于种群的强化学习中实现有效的多样性。发表于 *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*，Hugo Larochelle、Marc’Aurelio
    Ranzato、Raia Hadsell、Maria-Florina Balcan 和 Hsuan-Tien Lin（编）。[https://proceedings.neurips.cc/paper/2020/hash/d1dc3a8270a6f9394f88847d7f0050cf-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/d1dc3a8270a6f9394f88847d7f0050cf-Abstract.html)'
- en: 'Parker-Holder et al. (2022) Jack Parker-Holder, Raghu Rajan, Xingyou Song,
    André Biedenkapp, Yingjie Miao, Theresa Eimer, Baohe Zhang, Vu Nguyen, Roberto
    Calandra, Aleksandra Faust, et al. 2022. Automated Reinforcement Learning (AutoRL):
    A Survey and Open Problems. *ArXiv preprint* abs/2201.03916 (2022). [https://arxiv.org/abs/2201.03916](https://arxiv.org/abs/2201.03916)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parker-Holder 等（2022）Jack Parker-Holder、Raghu Rajan、Xingyou Song、André Biedenkapp、Yingjie
    Miao、Theresa Eimer、Baohe Zhang、Vu Nguyen、Roberto Calandra、Aleksandra Faust 等。2022。自动化强化学习（AutoRL）：综述与开放问题。*ArXiv
    预印本* abs/2201.03916（2022）。[https://arxiv.org/abs/2201.03916](https://arxiv.org/abs/2201.03916)
- en: 'Peng et al. (2019) Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
    2019. Advantage-weighted regression: Simple and scalable off-policy reinforcement
    learning. *ArXiv preprint* abs/1910.00177 (2019). [https://arxiv.org/abs/1910.00177](https://arxiv.org/abs/1910.00177)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2019）Xue Bin Peng、Aviral Kumar、Grace Zhang 和 Sergey Levine。2019。优势加权回归：简单且可扩展的离线策略强化学习。*ArXiv
    预印本* abs/1910.00177（2019）。[https://arxiv.org/abs/1910.00177](https://arxiv.org/abs/1910.00177)
- en: Pierrot et al. (2020) Thomas Pierrot, Valentin Macé, Geoffrey Cideron, Nicolas
    Perrin, Karim Beguir, and Olivier Sigaud. 2020. Sample efficient Quality Diversity
    for neural continuous control. *unpublished* (2020).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pierrot 等（2020）Thomas Pierrot、Valentin Macé、Geoffrey Cideron、Nicolas Perrin、Karim
    Beguir 和 Olivier Sigaud。2020。用于神经连续控制的样本高效质量多样性。*未出版*（2020）。
- en: Pierrot et al. (2018) Thomas Pierrot, Nicolas Perrin, and Olivier Sigaud. 2018.
    First-order and second-order variants of the gradient descent in a unified framework.
    *ArXiv preprint* abs/1810.08102 (2018). [https://arxiv.org/abs/1810.08102](https://arxiv.org/abs/1810.08102)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pierrot 等（2018）Thomas Pierrot、Nicolas Perrin 和 Olivier Sigaud。2018。在统一框架中的一阶和二阶梯度下降变体。*ArXiv
    预印本* abs/1810.08102（2018）。[https://arxiv.org/abs/1810.08102](https://arxiv.org/abs/1810.08102)
- en: Pinneri et al. (2020) Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes,
    Jan Achterhold, Joerg Stueckler, Michal Rolinek, and Georg Martius. 2020. Sample-efficient
    cross-entropy method for real-time planning. *ArXiv preprint* abs/2008.06389 (2020).
    [https://arxiv.org/abs/2008.06389](https://arxiv.org/abs/2008.06389)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinneri 等（2020）Cristina Pinneri、Shambhuraj Sawant、Sebastian Blaes、Jan Achterhold、Joerg
    Stueckler、Michal Rolinek 和 Georg Martius。2020。用于实时规划的样本高效交叉熵方法。*ArXiv 预印本* abs/2008.06389（2020）。[https://arxiv.org/abs/2008.06389](https://arxiv.org/abs/2008.06389)
- en: Pinneri et al. (2021) Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes,
    and Georg Martius. 2021. Extracting Strong Policies for Robotics Tasks from Zero-Order
    Trajectory Optimizers. In *9th International Conference on Learning Representations,
    ICLR 2021, Virtual Event, Austria, May 3-7, 2021*. OpenReview.net. [https://openreview.net/forum?id=Nc3TJqbcl3](https://openreview.net/forum?id=Nc3TJqbcl3)
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pinneri 等（2021）Cristina Pinneri、Shambhuraj Sawant、Sebastian Blaes 和 Georg Martius。2021。从零阶轨迹优化器中提取强策略用于机器人任务。发表于
    *第九届国际学习表征会议，ICLR 2021，虚拟活动，奥地利，2021年5月3-7日*。OpenReview.net。[https://openreview.net/forum?id=Nc3TJqbcl3](https://openreview.net/forum?id=Nc3TJqbcl3)
- en: 'Pourchot and Sigaud (2019) Aloïs Pourchot and Olivier Sigaud. 2019. CEM-RL:
    Combining evolutionary and gradient-based methods for policy search. In *7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019*. OpenReview.net. [https://openreview.net/forum?id=BkeU5j0ctQ](https://openreview.net/forum?id=BkeU5j0ctQ)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pourchot 和 Sigaud（2019）Aloïs Pourchot 和 Olivier Sigaud。2019。CEM-RL：结合进化方法和基于梯度的方法进行策略搜索。在
    *第七届国际学习表征会议，ICLR 2019，美国路易斯安那州新奥尔良，2019年5月6-9日*。OpenReview.net。[https://openreview.net/forum?id=BkeU5j0ctQ](https://openreview.net/forum?id=BkeU5j0ctQ)
- en: 'Pugh et al. (2016) Justin K. Pugh, Lisa B. Soros, and Kenneth O. Stanley. 2016.
    Quality diversity: A new frontier for evolutionary computation. *Frontiers in
    Robotics and AI* 3 (2016), 40.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pugh 等人（2016）Justin K. Pugh、Lisa B. Soros 和 Kenneth O. Stanley。2016。质量多样性：进化计算的新前沿。*机器人与人工智能前沿*
    3 (2016)，40。
- en: 'Qian and Yu (2021) Hong Qian and Yang Yu. 2021. Derivative-free reinforcement
    learning: A review. *ArXiv preprint* abs/2102.05710 (2021). [https://arxiv.org/abs/2102.05710](https://arxiv.org/abs/2102.05710)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 和 Yu（2021）Hong Qian 和 Yang Yu。2021。无导数强化学习：综述。*ArXiv 预印本* abs/2102.05710
    (2021)。[https://arxiv.org/abs/2102.05710](https://arxiv.org/abs/2102.05710)
- en: Salimans et al. (2017) Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and
    Ilya Sutskever. 2017. Evolution strategies as a scalable alternative to reinforcement
    learning. *ArXiv preprint* abs/1703.03864 (2017). [https://arxiv.org/abs/1703.03864](https://arxiv.org/abs/1703.03864)
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salimans 等人（2017）Tim Salimans、Jonathan Ho、Xi Chen、Szymon Sidor 和 Ilya Sutskever。2017。进化策略作为强化学习的可扩展替代方案。*ArXiv
    预印本* abs/1703.03864 (2017)。[https://arxiv.org/abs/1703.03864](https://arxiv.org/abs/1703.03864)
- en: Schulman et al. (2015) John Schulman, Sergey Levine, Pieter Abbeel, Michael I.
    Jordan, and Philipp Moritz. 2015. Trust Region Policy Optimization. In *Proceedings
    of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France,
    6-11 July 2015* *(JMLR Workshop and Conference Proceedings)*, Francis R. Bach
    and David M. Blei (Eds.), Vol. 37. JMLR.org, 1889–1897. [http://proceedings.mlr.press/v37/schulman15.html](http://proceedings.mlr.press/v37/schulman15.html)
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等人（2015）John Schulman、Sergey Levine、Pieter Abbeel、Michael I. Jordan
    和 Philipp Moritz。2015。信任域策略优化。在 *第32届国际机器学习会议，ICML 2015，法国里尔，2015年7月6-11日* *(JMLR工作坊与会议录)*，Francis
    R. Bach 和 David M. Blei（编），第37卷。JMLR.org，1889–1897。[http://proceedings.mlr.press/v37/schulman15.html](http://proceedings.mlr.press/v37/schulman15.html)
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *ArXiv
    preprint* abs/1707.06347 (2017). [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等人（2017）John Schulman、Filip Wolski、Prafulla Dhariwal、Alec Radford 和
    Oleg Klimov。2017。近端策略优化算法。*ArXiv 预印本* abs/1707.06347 (2017)。[https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)
- en: Sehgal et al. (2019) Adarsh Sehgal, Hung La, Sushil Louis, and Hai Nguyen. 2019.
    Deep reinforcement learning using genetic algorithm for parameter optimization.
    In *2019 Third IEEE International Conference on Robotic Computing (IRC)*. IEEE,
    596–601.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sehgal 等人（2019）Adarsh Sehgal、Hung La、Sushil Louis 和 Hai Nguyen。2019。使用遗传算法进行参数优化的深度强化学习。在
    *2019年第三届IEEE国际机器人计算会议（IRC）*。IEEE，596–601。
- en: 'Sehgal et al. (2022) Adarsh Sehgal, Nicholas Ward, Hung Manh La, Christos Papachristos,
    and Sushil Louis. 2022. GA-DRL: Genetic Algorithm-Based Function Optimizer in
    Deep Reinforcement Learning for Robotic Manipulation Tasks. *ArXiv preprint* abs/2203.00141
    (2022). [https://arxiv.org/abs/2203.00141](https://arxiv.org/abs/2203.00141)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sehgal 等人（2022）Adarsh Sehgal、Nicholas Ward、Hung Manh La、Christos Papachristos
    和 Sushil Louis。2022。GA-DRL：基于遗传算法的深度强化学习功能优化器，用于机器人操作任务。*ArXiv 预印本* abs/2203.00141
    (2022)。[https://arxiv.org/abs/2203.00141](https://arxiv.org/abs/2203.00141)
- en: 'Shao et al. (2021) Lin Shao, Yifan You, Mengyuan Yan, Shenli Yuan, Qingyun
    Sun, and Jeannette Bohg. 2021. GRAC: Self-guided and self-regularized actor-critic.
    In *Conference on Robot Learning*. PMLR, 267–276.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等人（2021）Lin Shao、Yifan You、Mengyuan Yan、Shenli Yuan、Qingyun Sun 和 Jeannette
    Bohg。2021。GRAC：自指导和自正则化的演员-评论家。 在 *机器人学习会议*。PMLR，267–276。
- en: 'Shi et al. (2019) Longxiang Shi, Shijian Li, Longbing Cao, Long Yang, Gang
    Zheng, and Gang Pan. 2019. FiDi-RL: Incorporating Deep Reinforcement Learning
    with Finite-Difference Policy Search for Efficient Learning of Continuous Control.
    *ArXiv preprint* abs/1907.00526 (2019). [https://arxiv.org/abs/1907.00526](https://arxiv.org/abs/1907.00526)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2019）Longxiang Shi、Shijian Li、Longbing Cao、Long Yang、Gang Zheng 和 Gang
    Pan。2019。FiDi-RL：结合深度强化学习与有限差分策略搜索以提高连续控制的学习效率。*ArXiv 预印本* abs/1907.00526 (2019)。[https://arxiv.org/abs/1907.00526](https://arxiv.org/abs/1907.00526)
- en: Shi et al. (2020) Longxiang Shi, Shijian Li, Qian Zheng, Min Yao, and Gang Pan.
    2020. Efficient novelty search through deep reinforcement learning. *IEEE Access*
    8 (2020), 128809–128818.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2020) 龙翔·石、施建、钱郑、敏耀和刚潘。2020年。通过深度强化学习实现高效的新奇搜索。*IEEE Access* 8 (2020)，128809–128818。
- en: Shi and Singh (2021) Zhenyang Shi and Surya PN Singh. 2021. Soft Actor-Critic
    with Cross-Entropy Policy Optimization. *ArXiv preprint* abs/2112.11115 (2021).
    [https://arxiv.org/abs/2112.11115](https://arxiv.org/abs/2112.11115)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi and Singh (2021) 珍阳·石和苏里亚·PN·辛格。2021年。具有交叉熵策略优化的软演员-评论家。*ArXiv预印本* abs/2112.11115
    (2021)。[https://arxiv.org/abs/2112.11115](https://arxiv.org/abs/2112.11115)
- en: 'Sigaud and Stulp (2019) Olivier Sigaud and Freek Stulp. 2019. Policy Search
    in Continuous Action Domains: an Overview. *Neural Networks* 113 (2019), 28–40.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigaud and Stulp (2019) 奥利维耶·西戈和弗里克·斯图尔普。2019年。连续动作领域的策略搜索：概述。*神经网络* 113 (2019)，28–40。
- en: 'Sigaud and Wilson (2007) Olivier Sigaud and S. W. Wilson. 2007. Learning Classifier
    Systems: A Survey. *Journal of Soft Computing* 11, 11 (2007), 1065–1078.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigaud and Wilson (2007) 奥利维耶·西戈和S. W. 威尔逊。2007年。学习分类器系统：综述。*软计算期刊* 11, 11 (2007)，1065–1078。
- en: Simmons-Edler et al. (2019) Riley Simmons-Edler, Ben Eisner, Eric Mitchell,
    Sebastian Seung, and Daniel Lee. 2019. Q-learning for continuous actions with
    cross-entropy guided policies. *ArXiv preprint* abs/1903.10605 (2019). [https://arxiv.org/abs/1903.10605](https://arxiv.org/abs/1903.10605)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simmons-Edler et al. (2019) 莱利·西蒙斯-埃德勒、本·艾斯纳、埃里克·米切尔、塞巴斯蒂安·胜和丹尼尔·李。2019年。带有交叉熵引导策略的连续动作Q学习。*ArXiv预印本*
    abs/1903.10605 (2019)。[https://arxiv.org/abs/1903.10605](https://arxiv.org/abs/1903.10605)
- en: Simpson (1953) George Gaylord Simpson. 1953. The baldwin effect. *Evolution*
    7, 2 (1953), 110–117.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simpson (1953) 乔治·盖尔福德·辛普森。1953年。鲍德温效应。*进化* 7, 2 (1953)，110–117。
- en: Stork et al. (2021) Jörg Stork, Martin Zaefferer, Nils Eisler, Patrick Tichelmann,
    Thomas Bartz-Beielstein, and AE Eiben. 2021. Behavior-based neuroevolutionary
    training in reinforcement learning. In *Proceedings of the Genetic and Evolutionary
    Computation Conference Companion*. 1753–1761.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stork et al. (2021) 约尔格·斯托克、马丁·扎费尔、尼尔斯·艾斯勒、帕特里克·蒂谢尔曼、托马斯·巴茨-贝尔斯坦和AE·艾本。2021年。在强化学习中基于行为的神经进化训练。在*遗传和进化计算会议附录*。1753–1761。
- en: Sun et al. (2020) Hao Sun, Ziping Xu, Yuhang Song, Meng Fang, Jiechao Xiong,
    Bo Dai, and Bolei Zhou. 2020. Zeroth-order supervised policy improvement. *ArXiv
    preprint* abs/2006.06600 (2020). [https://arxiv.org/abs/2006.06600](https://arxiv.org/abs/2006.06600)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2020) 郝顺、 ziping·徐、于航·宋、孟芳、界超·熊、博·戴和博磊·周。2020年。零阶监督策略改进。*ArXiv预印本*
    abs/2006.06600 (2020)。[https://arxiv.org/abs/2006.06600](https://arxiv.org/abs/2006.06600)
- en: Suri et al. (2020) Karush Suri, Xiao Qi Shi, Konstantinos N. Plataniotis, and
    Yuri A. Lawryshyn. 2020. Maximum Mutation Reinforcement Learning for Scalable
    Control. *ArXiv preprint* abs/2007.13690 (2020). [https://arxiv.org/abs/2007.13690](https://arxiv.org/abs/2007.13690)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suri et al. (2020) 卡鲁什·苏里、肖·琦·石、康斯坦丁·N·普拉塔尼奥蒂斯和尤里·A·劳里申。2020年。用于可扩展控制的最大变异强化学习。*ArXiv预印本*
    abs/2007.13690 (2020)。[https://arxiv.org/abs/2007.13690](https://arxiv.org/abs/2007.13690)
- en: 'Sutton and Barto (2018) Richard S. Sutton and Andrew G. Barto. 2018. *Reinforcement
    learning: An introduction*. MIT Press.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton and Barto (2018) 理查德·S·萨顿和安德鲁·G·巴托。2018年。*强化学习：导论*。MIT出版社。
- en: Tang (2021) Yunhao Tang. 2021. Guiding Evolutionary Strategies with Off-Policy
    Actor-Critic. In *Proceedings of the 20th International Conference on Autonomous
    Agents and MultiAgent Systems*. 1317–1325.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang (2021) 云豪·唐。2021年。使用离策略演员-评论家引导进化策略。在*第20届国际自主代理和多代理系统会议论文集*。1317–1325。
- en: Tang and Choromanski (2020) Yunhao Tang and Krzysztof Choromanski. 2020. Online
    hyper-parameter tuning in off-policy learning via evolutionary strategies. *ArXiv
    preprint* abs/2006.07554 (2020). [https://arxiv.org/abs/2006.07554](https://arxiv.org/abs/2006.07554)
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang and Choromanski (2020) 云豪·唐和克日什托夫·乔罗曼斯基。2020年。通过进化策略在离策略学习中的在线超参数调整。*ArXiv预印本*
    abs/2006.07554 (2020)。[https://arxiv.org/abs/2006.07554](https://arxiv.org/abs/2006.07554)
- en: 'Tangri et al. (2022) Rohan Tangri, Danilo P. Mandic, and Anthony G. Constantinides.
    2022. Pearl: Parallel Evolutionary and Reinforcement Learning Library. *ArXiv
    preprint* abs/2201.09568 (2022). [https://arxiv.org/abs/2201.09568](https://arxiv.org/abs/2201.09568)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tangri et al. (2022) 罗汉·唐格里、丹尼洛·P·曼迪奇、安东尼·G·康斯坦丁尼德斯。2022年。Pearl: 并行进化和强化学习库。*ArXiv预印本*
    abs/2201.09568 (2022)。[https://arxiv.org/abs/2201.09568](https://arxiv.org/abs/2201.09568)'
- en: Tjanaka et al. (2022) Bryon Tjanaka, Matthew C Fontaine, Julian Togelius, and
    Stefanos Nikolaidis. 2022. Approximating Gradients for Differentiable Quality
    Diversity in Reinforcement Learning. *ArXiv preprint* abs/2202.03666 (2022). [https://arxiv.org/abs/2202.03666](https://arxiv.org/abs/2202.03666)
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tjanaka 等 (2022) Bryon Tjanaka, Matthew C Fontaine, Julian Togelius, 和 Stefanos
    Nikolaidis. 2022. 近似用于可微分质量多样性的梯度。 *ArXiv 预印本* abs/2202.03666 (2022). [https://arxiv.org/abs/2202.03666](https://arxiv.org/abs/2202.03666)
- en: Todd et al. (2020) Graham Todd, Madhavun Candadai, and Eduardo J. Izquierdo.
    2020. Interaction between evolution and learning in nk fitness landscapes. In
    *Artificial Life Conference Proceedings*. MIT Press, 761–767.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Todd 等 (2020) Graham Todd, Madhavun Candadai, 和 Eduardo J. Izquierdo. 2020.
    在 nk 适应度景观中进化与学习的互动。 在 *人工生命会议论文集*。 MIT出版社，761–767。
- en: Wang and Ba (2019) Tingwu Wang and Jimmy Ba. 2019. Exploring model-based planning
    with policy networks. *arXiv preprint arXiv:1906.08649* (2019).
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Ba (2019) Tingwu Wang 和 Jimmy Ba. 2019. 使用策略网络探索基于模型的规划。 *arXiv 预印本 arXiv:1906.08649*
    (2019).
- en: Wang et al. (2022) Yuxing Wang, Tiantian Zhang, Yongzhe Chang, Bin Liang, Xueqian
    Wang, and Bo Yuan. 2022. A Surrogate-Assisted Controller for Expensive Evolutionary
    Reinforcement Learning. *ArXiv preprint* abs/2201.00129 (2022). [https://arxiv.org/abs/2201.00129](https://arxiv.org/abs/2201.00129)
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2022) Yuxing Wang, Tiantian Zhang, Yongzhe Chang, Bin Liang, Xueqian
    Wang, 和 Bo Yuan. 2022. 一种用于昂贵进化强化学习的替代辅助控制器。 *ArXiv 预印本* abs/2201.00129 (2022).
    [https://arxiv.org/abs/2201.00129](https://arxiv.org/abs/2201.00129)
- en: Wang et al. (2017) Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Rémi
    Munos, Koray Kavukcuoglu, and Nando de Freitas. 2017. Sample Efficient Actor-Critic
    with Experience Replay. In *5th International Conference on Learning Representations,
    ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*. OpenReview.net.
    [https://openreview.net/forum?id=HyM25Mqel](https://openreview.net/forum?id=HyM25Mqel)
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2017) Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Rémi Munos,
    Koray Kavukcuoglu, 和 Nando de Freitas. 2017. 样本高效的演员-评论家与经验回放。 在 *第5届国际学习表示会议，ICLR
    2017，法国图盎，2017年4月24-26日，会议论文集*。 OpenReview.net. [https://openreview.net/forum?id=HyM25Mqel](https://openreview.net/forum?id=HyM25Mqel)
- en: 'Weber and Depew (2003) Bruce H. Weber and David J. Depew. 2003. *Evolution
    and learning: The Baldwin effect reconsidered*. Mit Press.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weber 和 Depew (2003) Bruce H. Weber 和 David J. Depew. 2003. *进化与学习：重新考虑鲍德温效应*。
    MIT出版社。
- en: Wierstra et al. (2014) Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun,
    Jan Peters, and Jürgen Schmidhuber. 2014. Natural evolution strategies. *The Journal
    of Machine Learning Research* 15, 1 (2014), 949–980.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wierstra 等 (2014) Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan
    Peters, 和 Jürgen Schmidhuber. 2014. 自然进化策略。 *机器学习研究期刊* 15, 1 (2014), 949–980.
- en: Zhang et al. (2020) Jiaxing Zhang, Hoang Tran, and Guannan Zhang. 2020. Accelerating
    Reinforcement Learning with a Directional-Gaussian-Smoothing Evolution Strategy.
    *ArXiv preprint* abs/2002.09077 (2020). [https://arxiv.org/abs/2002.09077](https://arxiv.org/abs/2002.09077)
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020) Jiaxing Zhang, Hoang Tran, 和 Guannan Zhang. 2020. 通过方向性高斯平滑进化策略加速强化学习。
    *ArXiv 预印本* abs/2002.09077 (2020). [https://arxiv.org/abs/2002.09077](https://arxiv.org/abs/2002.09077)
- en: 'Zheng et al. (2020) Han Zheng, Pengfei Wei, Jing Jiang, Guodong Long, Qinghua
    Lu, and Chengqi Zhang. 2020. Cooperative Heterogeneous Deep Reinforcement Learning.
    In *Advances in Neural Information Processing Systems 33: Annual Conference on
    Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual*, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
    and Hsuan-Tien Lin (Eds.). [https://proceedings.neurips.cc/paper/2020/hash/ca3a9be77f7e88708afb20c8cdf44b60-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/ca3a9be77f7e88708afb20c8cdf44b60-Abstract.html)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 (2020) Han Zheng, Pengfei Wei, Jing Jiang, Guodong Long, Qinghua Lu,
    和 Chengqi Zhang. 2020. 合作异质深度强化学习。 在 *神经信息处理系统进展 33：2020年神经信息处理系统年度会议，NeurIPS
    2020，2020年12月6-12日，虚拟*，Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
    Balcan, 和 Hsuan-Tien Lin (编辑)。 [https://proceedings.neurips.cc/paper/2020/hash/ca3a9be77f7e88708afb20c8cdf44b60-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/ca3a9be77f7e88708afb20c8cdf44b60-Abstract.html)
