- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:04:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:04:19
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1910.11470] A Survey on Recent Advances in Named Entity Recognition from Deep
    Learning models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1910.11470] 关于深度学习模型中命名实体识别的最新进展调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1910.11470](https://ar5iv.labs.arxiv.org/html/1910.11470)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1910.11470](https://ar5iv.labs.arxiv.org/html/1910.11470)
- en: A Survey on Recent Advances in Named Entity Recognition from Deep Learning models
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于深度学习模型中命名实体识别的最新进展调查
- en: Vikas Yadav
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 维卡斯·亚达夫
- en: University of Arizona
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 亚利桑那大学
- en: vikasy@email.arizona.edu
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: vikasy@email.arizona.edu
- en: \AndSteven Bethard
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \AndSteven Bethard
- en: University of Arizona
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 亚利桑那大学
- en: bethard@email.arizona.edu
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: bethard@email.arizona.edu
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Named Entity Recognition (NER) is a key component in NLP systems for question
    answering, information retrieval, relation extraction, etc. NER systems have been
    studied and developed widely for decades, but accurate systems using deep neural
    networks (NN) have only been introduced in the last few years. We present a comprehensive
    survey of deep neural network architectures for NER, and contrast them with previous
    approaches to NER based on feature engineering and other supervised or semi-supervised
    learning algorithms. Our results highlight the improvements achieved by neural
    networks, and show how incorporating some of the lessons learned from past work
    on feature-based NER systems can yield further improvements.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）是自然语言处理（NLP）系统中的一个关键组件，用于问答、信息检索、关系抽取等。NER 系统已经被广泛研究和开发了几十年，但使用深度神经网络（NN）的准确系统仅在最近几年才被引入。我们提供了一个关于NER的深度神经网络架构的综合调查，并将其与基于特征工程和其他监督或半监督学习算法的早期NER方法进行了对比。我们的结果突出了神经网络所取得的改进，并展示了如何借鉴过去基于特征的NER系统中的一些经验教训，从而获得进一步的改进。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '⁰⁰footnotetext: This work is licenced under a Creative Commons Attribution
    4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ⁰⁰脚注：本工作采用知识共享署名 4.0 国际许可证。许可证详情： http://creativecommons.org/licenses/by/4.0/
- en: Named entity recognition is the task of identifying named entities like person,
    location, organization, drug, time, clinical procedure, biological protein, etc.
    in text. NER systems are often used as the first step in question answering, information
    retrieval, co-reference resolution, topic modeling, etc. Thus it is important
    to highlight recent advances in named entity recognition, especially recent neural
    NER architectures which have achieved state of the art performance with minimal
    feature engineering.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别是识别文本中命名实体（如人名、地点、组织、药物、时间、临床程序、生物蛋白等）的任务。NER 系统通常作为问答、信息检索、共指消解、主题建模等的第一步。因此，突出命名实体识别的最新进展，特别是最近的神经NER架构，这些架构以最少的特征工程实现了最先进的性能，非常重要。
- en: The first NER task was organized by ?) in the Sixth Message Understanding Conference.
    Since then, there have been numerous NER tasks [[Tjong Kim Sang and De Meulder
    (2003](#bib.bibx72), [Tjong Kim Sang (2002](#bib.bibx73), [Piskorski et al. (2017](#bib.bibx57),
    [Segura Bedmar et al. (2013](#bib.bibx67), [Bossy et al. (2013](#bib.bibx9), [Uzuner
    et al. (2011](#bib.bibx75)]. Early NER systems were based on handcrafted rules,
    lexicons, orthographic features and ontologies. These systems were followed by
    NER systems based on feature-engineering and machine learning [[Nadeau and Sekine
    (2007](#bib.bibx50)]. Starting with ?), neural network NER systems with minimal
    feature engineering have become popular. Such models are appealing because they
    typically do not require domain specific resources like lexicons or ontologies,
    and are thus poised to be more domain independent. Various neural architectures
    have been proposed, mostly based on some form of recurrent neural networks (RNN)
    over characters, sub-words and/or word embeddings.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个 NER 任务由 ?) 在第六届消息理解会议上组织。从那时起，出现了许多 NER 任务 [[Tjong Kim Sang 和 De Meulder
    (2003](#bib.bibx72), [Tjong Kim Sang (2002](#bib.bibx73), [Piskorski 等 (2017](#bib.bibx57),
    [Segura Bedmar 等 (2013](#bib.bibx67), [Bossy 等 (2013](#bib.bibx9), [Uzuner 等 (2011](#bib.bibx75)]。早期的NER系统基于手工规则、词典、正字法特征和本体。这些系统随后被基于特征工程和机器学习的NER系统所取代
    [[Nadeau 和 Sekine (2007](#bib.bibx50)]。从 ? 开始，具有最小特征工程的神经网络NER系统变得流行。这些模型受到欢迎，因为它们通常不需要特定领域的资源，如词典或本体，因此更具领域独立性。提出了各种神经架构，主要基于字符、子词和/或词嵌入上的某种形式的递归神经网络（RNN）。
- en: We present a comprehensive survey of recent advances in named entity recognition.
    We describe knowledge-based and feature-engineered NER systems that combine in-domain
    knowledge, gazetteers, orthographic and other features with supervised or semi-supervised
    learning. We contrast these systems with neural network architectures for NER
    based on minimal feature engineering, and compare amongst the neural models with
    different representations of words and sub-word units. We show in [Table 1](#S7.T1
    "In 7 Discussion ‣ A Survey on Recent Advances in Named Entity Recognition from
    Deep Learning models") and [Table 2](#S7.T2 "In 7 Discussion ‣ A Survey on Recent
    Advances in Named Entity Recognition from Deep Learning models") and discuss in
    [Section 7](#S7 "7 Discussion ‣ A Survey on Recent Advances in Named Entity Recognition
    from Deep Learning models") how neural NER systems have improved performance over
    past works including supervised, semi-supervised, and knowledge based NER systems.
    For example, NN models on news corpora improved the previous state-of-the-art
    by 1.59% in Spanish, 2.34% in German, 0.36% in English, and 0.14%, in Dutch, without
    any external resources or feature engineering. We provide resources, including
    links to shared tasks on NER, and links to the code for each category of NER system.
    To the best of our knowledge, this is the first survey focusing on neural architectures
    for NER, and comparing to previous feature-based systems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一项关于命名实体识别（NER）近期进展的全面调查。我们描述了基于知识和特征工程的NER系统，这些系统结合了领域内知识、词典、拼写及其他特征，并采用了监督或半监督学习。我们将这些系统与基于最小特征工程的神经网络架构进行对比，并比较了不同表示方法的神经模型。我们在[表1](#S7.T1
    "在第7节讨论 ‣ 深度学习模型下命名实体识别的最新进展调查")和[表2](#S7.T2 "在第7节讨论 ‣ 深度学习模型下命名实体识别的最新进展调查")中展示，并在[第7节](#S7
    "第7节讨论 ‣ 深度学习模型下命名实体识别的最新进展调查")讨论了神经NER系统如何在性能上优于以往的工作，包括监督、半监督和基于知识的NER系统。例如，NN模型在新闻语料库上提高了西班牙语1.59%、德语2.34%、英语0.36%和荷兰语0.14%的先前最佳水平，而无需任何外部资源或特征工程。我们提供了资源，包括NER共享任务的链接，以及每种类别NER系统的代码链接。据我们所知，这是首个专注于NER神经网络架构并与以往基于特征系统进行比较的调查。
- en: We first discuss previous summary research on NER in [section 2](#S2 "2 Previous
    surveys ‣ A Survey on Recent Advances in Named Entity Recognition from Deep Learning
    models"). Then we explain our selection criterion and methodology for selecting
    which systems to review in [section 3](#S3 "3 Methodology ‣ A Survey on Recent
    Advances in Named Entity Recognition from Deep Learning models"). We highlight
    standard, past and recent NER datasets (from shared tasks and other research)
    in [section 4](#S4 "4 NER datasets ‣ A Survey on Recent Advances in Named Entity
    Recognition from Deep Learning models") and evaluation metrics in [section 5](#S5
    "5 NER evaluation metrics ‣ A Survey on Recent Advances in Named Entity Recognition
    from Deep Learning models"). We then describe NER systems in [section 6](#S6 "6
    NER systems ‣ A Survey on Recent Advances in Named Entity Recognition from Deep
    Learning models") categorized into knowledge-based ([section 6.1](#S6.SS1 "6.1
    Knowledge-based systems ‣ 6 NER systems ‣ A Survey on Recent Advances in Named
    Entity Recognition from Deep Learning models")), bootstrapped ([section 6.2](#S6.SS2
    "6.2 Unsupervised and bootstrapped systems ‣ 6 NER systems ‣ A Survey on Recent
    Advances in Named Entity Recognition from Deep Learning models")), feature-engineered
    ([section 6.3](#S6.SS3 "6.3 Feature-engineered supervised systems ‣ 6 NER systems
    ‣ A Survey on Recent Advances in Named Entity Recognition from Deep Learning models"))
    and neural networks ([section 6.4](#S6.SS4 "6.4 Feature-inferring neural network
    systems ‣ 6 NER systems ‣ A Survey on Recent Advances in Named Entity Recognition
    from Deep Learning models")).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在[2节](#S2 "2 Previous surveys ‣ A Survey on Recent Advances in Named Entity
    Recognition from Deep Learning models")讨论先前的NER总结研究。然后我们在[3节](#S3 "3 Methodology
    ‣ A Survey on Recent Advances in Named Entity Recognition from Deep Learning models")解释我们的选择标准和方法论，以选择需要审查的系统。我们在[4节](#S4
    "4 NER datasets ‣ A Survey on Recent Advances in Named Entity Recognition from
    Deep Learning models")突出介绍标准的、过去和最新的NER数据集（来自共享任务和其他研究），以及[5节](#S5 "5 NER evaluation
    metrics ‣ A Survey on Recent Advances in Named Entity Recognition from Deep Learning
    models")的评估指标。然后，我们在[6节](#S6 "6 NER systems ‣ A Survey on Recent Advances in Named
    Entity Recognition from Deep Learning models")描述NER系统，分类为基于知识的（[6.1节](#S6.SS1
    "6.1 Knowledge-based systems ‣ 6 NER systems ‣ A Survey on Recent Advances in
    Named Entity Recognition from Deep Learning models")），自举（[6.2节](#S6.SS2 "6.2 Unsupervised
    and bootstrapped systems ‣ 6 NER systems ‣ A Survey on Recent Advances in Named
    Entity Recognition from Deep Learning models")），特征工程的（[6.3节](#S6.SS3 "6.3 Feature-engineered
    supervised systems ‣ 6 NER systems ‣ A Survey on Recent Advances in Named Entity
    Recognition from Deep Learning models")）和神经网络的（[6.4节](#S6.SS4 "6.4 Feature-inferring
    neural network systems ‣ 6 NER systems ‣ A Survey on Recent Advances in Named
    Entity Recognition from Deep Learning models")）。
- en: 2 Previous surveys
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 先前的调查
- en: The first comprehensive NER survey was ?), which covered a variety of supervised,
    semi-supervised and unsupervised NER systems, highlighted common features used
    by NER systems during that time, and explained NER evaluation metrics that are
    still in use today. ?) presented a more recent NER survey that also included supervised,
    semi-supervised, and unsupervised NER systems, and included a few introductory
    neural network NER systems. There have also been surveys focused on NER systems
    for specific domains and languages, including biomedical NER, [[Leaman and Gonzalez
    (2008](#bib.bibx37)], Chinese clinical NER [[Lei et al. (2013](#bib.bibx38)],
    Arabic NER [[Shaalan (2014](#bib.bibx68), [Etaiwi et al. (2017](#bib.bibx22)],
    and NER for Indian languages [[Patil et al. (2016](#bib.bibx55)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个全面的NER调查是?），它涵盖了各种监督、半监督和无监督的NER系统，突出了当时NER系统使用的共同特征，并解释了至今仍在使用的NER评估指标。?)
    提供了一个更新的NER调查，也包括了监督、半监督和无监督NER系统，并介绍了一些初步的神经网络NER系统。还有一些调查专注于特定领域和语言的NER系统，包括生物医学NER，[[Leaman
    和 Gonzalez (2008](#bib.bibx37)]，中文临床NER [[Lei et al. (2013](#bib.bibx38)]，阿拉伯语NER
    [[Shaalan (2014](#bib.bibx68)，[Etaiwi et al. (2017](#bib.bibx22)]，以及印度语言的NER [[Patil
    et al. (2016](#bib.bibx55)]。
- en: The existing surveys primarily cover feature-engineered machine learning models
    (including supervised, semi-supervised, and unsupervised systems), and mostly
    focus on a single language or a single domain. There is not yet, to our knowledge,
    a comprehensive survey of modern neural network NER systems, nor is there a survey
    that compares feature engineered and neural network systems in both multi-lingual
    (CoNLL 2002 and CoNLL 2003) and multi-domain (e.g., news and medical) settings.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的调查主要涵盖了特征工程机器学习模型（包括监督、半监督和无监督系统），并且大多集中在单一语言或单一领域。我们了解到，目前还没有全面的现代神经网络命名实体识别（NER）系统的调查，也没有在多语言（CoNLL
    2002 和 CoNLL 2003）和多领域（如新闻和医疗）环境下比较特征工程和神经网络系统的调查。
- en: 3 Methodology
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: To identify articles for this survey, we searched Google, Google Scholar, and
    Semantic Scholar. Our query terms included named entity recognition, neural architectures
    for named entity recognition, neural network based named entity recognition models,
    deep learning models for named entity recognition, etc. We sorted the papers returned
    from each query by citation count and read at least the top three, considering
    a paper for our survey if it either introduced a neural architecture for named
    entity recognition, or represented a top-performing model on an NER dataset. We
    included an article presenting a neural architecture only if it was the first
    article to introduce the architecture; otherwise, we traced citations back until
    we found the original source of the architecture. We followed the same approach
    for feature-engineering NER systems. We also included articles that implemented
    these systems for different languages or domain. In total, 154 articles were reviewed
    and 83 articles were selected for the survey.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别用于本次调查的文章，我们搜索了Google、Google Scholar和Semantic Scholar。我们的查询词包括命名实体识别、用于命名实体识别的神经架构、基于神经网络的命名实体识别模型、用于命名实体识别的深度学习模型等。我们按引用次数对每个查询返回的论文进行了排序，并至少阅读了前三篇，考虑将论文纳入我们的调查，如果它要么介绍了一种用于命名实体识别的神经架构，要么代表了在NER数据集上表现优秀的模型。我们仅在文章首次介绍了该架构的情况下才包含介绍神经架构的文章；否则，我们追溯引用，直到找到该架构的原始来源。我们对特征工程NER系统采用了相同的方法。我们还包括了为不同语言或领域实现这些系统的文章。总共审阅了154篇文章，其中83篇文章被选入调查。
- en: 4 NER datasets
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4个NER数据集
- en: 'Since the first shared task on NER [[Grishman and Sundheim (1996](#bib.bibx26)]¹¹1Shared
    task: https://www-nlpir.nist.gov/related_projects/muc/, many shared tasks and
    datasets for NER have been created. CoNLL 2002 [[Tjong Kim Sang (2002](#bib.bibx73)]²²2Shared
    task: https://www.clips.uantwerpen.be/conll2002/ner/ and CoNLL 2003 [[Tjong Kim Sang
    and De Meulder (2003](#bib.bibx72)]³³3Shared task: https://www.clips.uantwerpen.be/conll2003/ner/
    were created from newswire articles in four different languages (Spanish, Dutch,
    English, and German) and focused on 4 entities - PER (person), LOC (location),
    ORG (organization) and MISC (miscellaneous including all other types of entities).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '自从第一个NER共享任务[[Grishman和Sundheim (1996](#bib.bibx26)]¹¹1共享任务: https://www-nlpir.nist.gov/related_projects/muc/，许多NER共享任务和数据集被创建。CoNLL
    2002 [[Tjong Kim Sang (2002](#bib.bibx73)]²²2共享任务: https://www.clips.uantwerpen.be/conll2002/ner/和CoNLL
    2003 [[Tjong Kim Sang和De Meulder (2003](#bib.bibx72)]³³3共享任务: https://www.clips.uantwerpen.be/conll2003/ner/，分别从四种不同语言（西班牙语、荷兰语、英语和德语）的新闻稿中创建，重点关注4种实体
    - PER（人）、LOC（地点）、ORG（组织）和MISC（其他，包括所有其他类型的实体）。'
- en: 'NER shared tasks have also been organized for a variety of other languages,
    including Indian languages [[Rajeev Sangal and Singh (2008](#bib.bibx61)], Arabic
    [[Shaalan (2014](#bib.bibx68)], German [[Benikova et al. (2014](#bib.bibx6)],
    and slavic languages [[Piskorski et al. (2017](#bib.bibx57)]. The named entity
    types vary widely by source of dataset and language. For example, ?)’s southeast
    Asian language data has named entity types person, designation, temporal expressions,
    abbreviations, object number, brand, etc. ?)’s data, which is based on German
    wikipedia and online news, has named entity types similar to that of CoNLL 2002
    and 2003: PERson, ORGanization, LOCation and OTHer. The shared task⁴⁴4Shared task:
    http://bsnlp.cs.helsinki.fi/shared_task.html organized by ?) covering 7 slavic
    languages (Croatian, Czech, Polish, Russian, Slovak, Slovene, Ukrainian) also
    has person, location, organization and miscellaneous as named entity types.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 'NER共享任务还针对各种其他语言组织过，包括印度语言 [[Rajeev Sangal 和 Singh (2008](#bib.bibx61)]，阿拉伯语
    [[Shaalan (2014](#bib.bibx68)]，德语 [[Benikova 等 (2014](#bib.bibx6)] 和斯拉夫语言 [[Piskorski
    等 (2017](#bib.bibx57)]。命名实体类型根据数据集的来源和语言差异很大。例如，？）的东南亚语言数据具有命名实体类型为人、职称、时间表达、缩写、物体编号、品牌等。？）的数据基于德语维基百科和在线新闻，具有类似于CoNLL
    2002和2003的命名实体类型：PERson、ORGanization、LOCation和OTHer。由？）组织的共享任务⁴⁴4共享任务: http://bsnlp.cs.helsinki.fi/shared_task.html
    涵盖了7种斯拉夫语言（克罗地亚语、捷克语、波兰语、俄语、斯洛伐克语、斯洛文尼亚语、乌克兰语），也包括了人、地点、组织和杂项作为命名实体类型。'
- en: 'In the biomedical domain, ?) organized a BioNER task on MedLine abstracts,
    focusing on protien, DNA, RNA and cell attribute entity types. ?) presented a
    clinical note de-identification task that required NER to locate personal patient
    data phrases to be anonymized. The 2010 I2B2 NER task⁵⁵5Shared task: https://www.i2b2.org/NLP/Relations/
    [[Uzuner et al. (2011](#bib.bibx75)], which considered clinical data, focused
    on clinical problem, test and treatment entity types. ?) organized a Drug NER
    shared task⁶⁶6Shared task: https://www.cs.york.ac.uk/semeval-2013/task9/index.html
    as part of SemEval 2013 Task 9, which focused on drug, brand, group and drug_n
    (unapproved or new drugs) entity types. [[Krallinger et al. (2015](#bib.bibx34)]
    introduced the similar CHEMDNER task ⁷⁷7Similar datasets can be found here: http://www.biocreative.org
    focusing more on chemical and drug entities like trivial, systematic, abbreviation,
    formula, family, identifier, etc. Biology and microbiology NER datasets⁸⁸8Shared
    task: http://2016.bionlp-st.org/tasks/bb2 [[Hirschman et al. (2005](#bib.bibx29),
    [Bossy et al. (2013](#bib.bibx9), [Delėger et al. (2016](#bib.bibx18)] have been
    collected from PubMed and biology websites, and focus mostly on bacteria, habitat
    and geo-location entities. In biomedical NER systems, segmentation of clinical
    and drug entities is considered to be a difficult task because of complex orthographic
    structures of named entities [[Liu et al. (2015](#bib.bibx43)].'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '在生物医学领域，?) 组织了一个针对 MedLine 摘要的 BioNER 任务，重点关注蛋白质、DNA、RNA 和细胞属性实体类型。?) 提出了一个临床笔记去标识化任务，要求
    NER 定位需要匿名化的个人患者数据短语。2010 年的 I2B2 NER 任务⁵⁵5 共享任务: https://www.i2b2.org/NLP/Relations/
    [[Uzuner et al. (2011](#bib.bibx75)]，该任务考虑了临床数据，重点关注临床问题、测试和治疗实体类型。?) 作为 SemEval
    2013 任务 9 的一部分，组织了一个药物 NER 共享任务⁶⁶6 共享任务: https://www.cs.york.ac.uk/semeval-2013/task9/index.html，重点关注药物、品牌、类别和药物_n（未经批准或新药）实体类型。[[Krallinger
    et al. (2015](#bib.bibx34)] 引入了类似的 CHEMDNER 任务 ⁷⁷7 类似数据集可以在此找到: http://www.biocreative.org，更多关注化学和药物实体，如常见名称、系统名称、缩写、公式、家族、标识符等。生物学和微生物学
    NER 数据集⁸⁸8 共享任务: http://2016.bionlp-st.org/tasks/bb2 [[Hirschman et al. (2005](#bib.bibx29)、[Bossy
    et al. (2013](#bib.bibx9)、[Delėger et al. (2016](#bib.bibx18)] 从 PubMed 和生物学网站收集，主要关注细菌、栖息地和地理位置实体。在生物医学
    NER 系统中，临床和药物实体的分割被认为是一项困难的任务，因为命名实体的复杂正字法结构 [[Liu et al. (2015](#bib.bibx43)]。'
- en: NER tasks have also been organized on social media data, e.g., Twitter, where
    the performance of classic NER systems degrades due to issues like variability
    in orthography and presence of grammatically incomplete sentences [[Baldwin et
    al. (2015](#bib.bibx5)]. Entity types on Twitter are also more variable (person,
    company, facility, band, sportsteam, movie, TV show, etc.) as they are based on
    user behavior on Twitter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: NER 任务也在社交媒体数据上组织，例如 Twitter，其中经典 NER 系统的性能由于正字法变化和语法不完整句子等问题而下降 [[Baldwin et
    al. (2015](#bib.bibx5)]。Twitter 上的实体类型也更加多样化（个人、公司、设施、乐队、运动队、电影、电视节目等），因为它们基于
    Twitter 上的用户行为。
- en: Though most named entity annotations are flat, some datasets include more complex
    structures. ?) constructed a dataset of nested named entities, where one named
    entity can contain another. ?) highlighted both entity and entity head phrases.
    And discontinuous entities are common in chemical and clinical NER datasets [[Krallinger
    et al. (2015](#bib.bibx34)]. ?) presented an survey of various NER systems developed
    for such NER datasets with a focus on chemical NER.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数命名实体标注是平面的，但一些数据集包括更复杂的结构。?) 构建了一个嵌套命名实体的数据集，其中一个命名实体可以包含另一个命名实体。?) 突出了实体和实体头短语。并且在化学和临床
    NER 数据集中，不连续的实体是常见的 [[Krallinger et al. (2015](#bib.bibx34)]。?) 对各种 NER 系统进行了调查，重点关注化学
    NER。
- en: 5 NER evaluation metrics
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个命名实体识别（NER）评估指标
- en: ?) scored NER performance based on type, whether the predicted label was correct
    regardless of entity boundaries, and text, whether the predicted entity boundaries
    were correct regardless of the label. For each score category, precision was defined
    as the number of entities a system predicted correctly divided by the number that
    the system predicted, recall was defined as the number of entities a system predicted
    correctly divided by the number that were identified by the human annotators,
    and (micro) F-score was defined as the harmonic mean of precision and recall from
    both type and text.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 基于类型对 NER 性能进行了评分，不管实体边界如何，预测标签是否正确；以及文本，对预测的实体边界是否正确，不管标签如何。对于每个评分类别，精确度被定义为系统正确预测的实体数量与系统预测的实体数量之比，召回率被定义为系统正确预测的实体数量与人工标注者识别的实体数量之比，(微观)
    F 分数被定义为精确度和召回率的调和平均数，涵盖了类型和文本。
- en: The exact match metrics introduced by CoNLL [[Tjong Kim Sang and De Meulder
    (2003](#bib.bibx72), [Tjong Kim Sang (2002](#bib.bibx73)] considers a prediction
    to be correct only when the predicted label for the complete entity is matched
    to exactly the same words as the gold label of that entity. CoNLL also used (micro)
    F-score, taking the harmonic mean of the exact match precision and recall.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CoNLL引入的精确匹配指标 [[Tjong Kim Sang and De Meulder (2003](#bib.bibx72), [Tjong Kim
    Sang (2002](#bib.bibx73)] 仅当完整实体的预测标签与该实体的黄金标签完全匹配时，才认为预测正确。CoNLL还使用了（微）F值，计算精确匹配的F值的调和均值。
- en: The relaxed F1 and strict F1 metrics have been used in many NER shared tasks
    [[Segura Bedmar et al. (2013](#bib.bibx67), [Krallinger et al. (2015](#bib.bibx34),
    [Bossy et al. (2013](#bib.bibx9), [Delėger et al. (2016](#bib.bibx18)]. Relaxed
    F1 considers a prediction to be correct as long as part of the named entity is
    identified correctly. Strict F1 requires the character offsets of a prediction
    and the human annotation to match exactly. In these data, unlike CoNLL, word offsets
    are not given, so relaxed F1 is intended to allow comparison despite different
    systems having different word boundaries due to different segmentation techniques
    [[Liu et al. (2015](#bib.bibx43)].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 放宽的F1和严格的F1指标已在许多NER共享任务中使用 [[Segura Bedmar et al. (2013](#bib.bibx67), [Krallinger
    et al. (2015](#bib.bibx34), [Bossy et al. (2013](#bib.bibx9), [Delėger et al.
    (2016](#bib.bibx18)]。放宽的F1只要部分命名实体被正确识别，就认为预测正确。严格的F1要求预测的字符偏移量与人工标注完全匹配。在这些数据中，与CoNLL不同，未提供词偏移量，因此放宽的F1旨在允许比较，尽管不同系统由于不同的分词技术而具有不同的词边界
    [[Liu et al. (2015](#bib.bibx43)]。
- en: 6 NER systems
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6个NER系统
- en: 6.1 Knowledge-based systems
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 基于知识的系统
- en: Knowledge-based NER systems do not require annotated training data as they rely
    on lexicon resources and domain specific knowledge. These work well when the lexicon
    is exhaustive, but fail, for example, on every example of the drug_n class in
    the DrugNER dataset [[Segura Bedmar et al. (2013](#bib.bibx67)], since drug_n
    is defined as unapproved or new drugs, which are by definition not in the DrugBank
    dictionaries [[Knox et al. (2010](#bib.bibx33)]. Precision is generally high for
    knowledge-based NER systems because of the lexicons, but recall is often low due
    to domain and language-specific rules and incomplete dictionaries. Another drawback
    of knowledge based NER systems is the need of domain experts for constructing
    and maintaining the knowledge resources.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基于知识的NER系统不需要标注的训练数据，因为它们依赖于词典资源和领域特定知识。当词典详尽时，这些系统表现良好，但例如，在DrugNER数据集中，每个drug_n类的例子都失败了
    [[Segura Bedmar et al. (2013](#bib.bibx67)]，因为drug_n定义为未经批准或新药，而这些药物的定义本质上不在DrugBank词典中
    [[Knox et al. (2010](#bib.bibx33)]。由于词典的存在，基于知识的NER系统的精确度通常很高，但由于领域和语言特定规则以及不完整的词典，召回率往往较低。基于知识的NER系统的另一个缺点是需要领域专家来构建和维护知识资源。
- en: 6.2 Unsupervised and bootstrapped systems
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 无监督和自举系统
- en: Some of the earliest systems required very minimal training data. ?) used only
    labeled seeds, and 7 features including orthography (e.g., capitalization), context
    of the entity, words contained within named entities, etc. for classifying and
    extracting named entities. ?) proposed an unsupervised system to improve the recall
    of NER systems applying 8 generic pattern extractors to open web text, e.g., NP
    “is a” $<$class1$>$, NP1 “such as” NPList2. ?) presented an unsupervised system
    for gazetteer building and named entity ambiguity resolution based on ?) and ?)
    that combined an extracted gazetteer with commonly available gazetteers to achieve
    F-scores of 88%, 61%, and 59% on MUC-7 [[Chinchor and Robinson (1997](#bib.bibx12)]
    location, person, and organization entities, respectively.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最早的系统只需非常少量的训练数据。?) 仅使用了标记种子和7种特征，包括正字法（例如，大小写）、实体的上下文、命名实体中包含的词等，用于分类和提取命名实体。?)
    提出了一个无监督系统，通过将8种通用模式提取器应用于开放网页文本（例如，NP“是一个” $<$class1$>$，NP1“例如” NPList2），以提高NER系统的召回率。?)
    提出了一个基于?) 和?) 的无监督系统，用于建立地名词典和命名实体歧义解决，结合了一个提取的地名词典和常用的地名词典，在MUC-7 [[Chinchor
    and Robinson (1997](#bib.bibx12)] 上实现了88%、61%和59%的F值，分别对应地点、人物和组织实体。
- en: ?) used shallow syntactic knowledge and inverse document frequency (IDF) for
    an unsupervised NER system on biology [[Kim et al. (2004](#bib.bibx31)] and medical
    [[Uzuner et al. (2011](#bib.bibx75)] data, achieving 53.8% and 69.5% accuracy,
    respectively. Their model uses seeds to discover text having potential named entities,
    detects noun phrases and filters any with low IDF values, and feeds the filtered
    list to a classifier [[Alfonseca and Manandhar (2002](#bib.bibx2)] to predict
    named entity tags.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 使用浅层句法知识和逆文档频率（IDF）用于生物学 [[Kim et al. (2004](#bib.bibx31)] 和医学 [[Uzuner et
    al. (2011](#bib.bibx75)] 数据上的无监督NER系统，分别达到了53.8%和69.5%的准确率。他们的模型使用种子发现具有潜在命名实体的文本，检测名词短语并过滤低IDF值的短语，然后将过滤后的列表输入分类器
    [[Alfonseca and Manandhar (2002](#bib.bibx2)] 来预测命名实体标签。
- en: 6.3 Feature-engineered supervised systems
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 特征工程监督系统
- en: Supervised machine learning models learn to make predictions by training on
    example inputs and their expected outputs, and can be used to replace human curated
    rules. Hidden Markov Models (HMM), Support Vector Machines (SVM), Conditional
    Random Fields (CRF), and decision trees were common machine learning systems for
    NER.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习模型通过在示例输入及其预期输出上进行训练，学习进行预测，并可以替代人工制定的规则。隐马尔可夫模型（HMM）、支持向量机（SVM）、条件随机场（CRF）和决策树是NER（命名实体识别）常用的机器学习系统。
- en: ?) used HMM [[Rabiner and Juang (1986](#bib.bibx60), [Bikel et al. (1997](#bib.bibx8)]
    an NER system on MUC-6 and MUC-7 data, achieving 96.6% and 94.1% F score, respectively.
    They included 11 orthographic features (1 numeral, 2 numeral, 4 numeral, all caps,
    numerals and alphabets, contains underscore or not, etc.) a list of trigger words
    for the named entities (e.g., 36 trigger words and affixes, like river, for the
    location entity class), and a list of words (10000 for the person entity class)
    from various gazetteers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 使用HMM [[Rabiner and Juang (1986](#bib.bibx60), [Bikel et al. (1997](#bib.bibx8)]
    在MUC-6和MUC-7数据上进行NER系统的实现，分别取得了96.6%和94.1%的F分数。他们包括了11个正字法特征（1个数字，2个数字，4个数字，全大写，数字和字母，是否包含下划线等），一个命名实体的触发词列表（例如，地点实体类别的36个触发词和词缀，如river），以及来自各种地名词典的10000个词（用于人名实体类别）。
- en: ?) compared the HMM with Maximum Entropy (ME) by adding multiple features. Their
    best model included capitalization, whether a word was the first in a sentence,
    whether a word had appeared before with a known last name, and 13281 first names
    collected from various dictionaries. The model achieved 73.66%, 68.08% Fscore
    on Spanish and Dutch CoNLL 2002 dataset respectively.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 通过添加多个特征比较了HMM与最大熵（ME）。他们的最佳模型包括了大写、一个词是否是句子的第一个词、一个词是否之前出现过且有已知姓氏，以及从各种词典中收集的13281个名字。该模型在西班牙语和荷兰语CoNLL
    2002数据集上分别达到了73.66%和68.08%的F分数。
- en: The winner of CoNLL 2002 [[Carreras et al. (2002](#bib.bibx10)] used binary
    AdaBoost classifiers, a boosting algorithm that combines small fixed-depth decision
    trees [[Schapire (2013](#bib.bibx66)]. They used features like capitalization,
    trigger words, previous tag prediction, bag of words, gazetteers, etc. to represent
    simple binary relations and these relations were used in conjunction with previously
    predicted labels. They achieved 81.39% and 77.05% F scores on the Spanish and
    Dutch CoNLL 2002 datasets, respectively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: CoNLL 2002的获胜者 [[Carreras et al. (2002](#bib.bibx10)] 使用了二元AdaBoost分类器，这是一种结合小型固定深度决策树的提升算法
    [[Schapire (2013](#bib.bibx66)]。他们使用了如大写、触发词、先前标签预测、词袋、词典等特征来表示简单的二元关系，这些关系与先前预测的标签结合使用。他们在西班牙语和荷兰语CoNLL
    2002数据集上分别达到了81.39%和77.05%的F分数。
- en: ?) implemented a SVM model on the CoNLL 2003 dataset and CMU seminar documents.
    They experimented with multiple window sizes, features (orthographic, prefixes
    suffixes, labels, etc.) from neighboring words, weighting neighboring word features
    according to their position, and class weights to balance positive and negative
    class. They used two SVM classifiers, one for detecting named entity starts and
    one for detecting ends. They achieved 88.3% F score on the English CoNLL 2003
    data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 在CoNLL 2003数据集和CMU研讨会文档上实现了SVM模型。他们尝试了多种窗口大小、来自邻近词的特征（正字法、前缀、后缀、标签等），根据位置加权邻近词特征，以及用以平衡正负类别的类权重。他们使用了两个SVM分类器，一个用于检测命名实体的开始，另一个用于检测结束。他们在英文CoNLL
    2003数据上取得了88.3%的F分数。
- en: On the MUC6 data, ?) used part-of-speech (POS) tags, orthographic features,
    a window of 3 words to the left and to the right of the central word, and tags
    of the last 3 words as features to the SVM. The final tag was decided by the voting
    of multiple one-vs-one SVM outputs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在MUC6数据上，?) 使用了词性（POS）标签、正字法特征、中心词左侧和右侧各3个单词的窗口，以及最后3个单词的标签作为SVM的特征。最终标签由多个一对一SVM输出的投票决定。
- en: ?) implemented structural learning [[Ando and Zhang (2005b](#bib.bibx4)] to
    divide the main task into many auxiliary tasks, for example, predicting labels
    by looking just at the context and masking the current word. The best classifier
    for each auxiliary task was selected based on its confidence. This model had achieved
    89.31% and 75.27% F score on English and German, respectively.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 实现了结构学习 [[Ando and Zhang (2005b](#bib.bibx4)]，将主要任务分解为多个辅助任务，例如，通过仅查看上下文和屏蔽当前词来预测标签。根据每个辅助任务的置信度选择最佳分类器。该模型在英语和德语上的F分数分别达到了89.31%和75.27%。
- en: '?) developed a semi-supervised system⁹⁹9Code: https://github.com/ixa-ehu/ixa-pipe-nerc
    by presenting NER classifiers with features including orthography, character n-grams,
    lexicons, prefixes, suffixes, bigrams, trigrams, and unsupervised cluster features
    from the Brown corpus, Clark corpus and k-means clustering of open text using
    word embeddings [[Mikolov et al. (2013](#bib.bibx48)]. They achieved near state
    of the art performance on CoNLL datasets: 84.16%, 85.04%, 91.36%, 76.42% on Spanish,
    Dutch, English, and German, respectively.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 开发了一个半监督系统⁹⁹9代码：https://github.com/ixa-ehu/ixa-pipe-nerc，向NER分类器提供了包括正字法、字符n-gram、词典、前缀、后缀、大词组、三词组以及来自布朗语料库、克拉克语料库和使用词嵌入的开放文本k均值聚类的无监督聚类特征等特征。他们在CoNLL数据集上取得了接近最先进的表现：西班牙语84.16%、荷兰语85.04%、英语91.36%、德语76.42%。
- en: In DrugNER [[Segura Bedmar et al. (2013](#bib.bibx67)], ?) achieved state-of-the-art
    results by using a CRF with features like lexicon resources from Food and Drug
    Administration (FDA), DrugBank, Jochem [[Hettne et al. (2009](#bib.bibx28)] and
    word embeddings (trained on a MedLine corpus). For the same task, ?) used a CRF
    with features constructed from dictionaries (e.g., Jochem [[Hettne et al. (2009](#bib.bibx28)]),
    ontologies (ChEBI ontologies), prefixes-suffixes from chemical entities, etc.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在DrugNER [[Segura Bedmar et al. (2013](#bib.bibx67)] 中，?) 通过使用具有来自食品药品监督管理局（FDA）、DrugBank、Jochem
    [[Hettne et al. (2009](#bib.bibx28)] 的词典资源和词嵌入（在MedLine语料库上训练）的CRF取得了最先进的结果。在同一任务中，?)
    使用了具有来自词典（例如Jochem [[Hettne et al. (2009](#bib.bibx28)]）、本体（ChEBI本体）、化学实体的前后缀等构造特征的CRF。
- en: 6.4 Feature-inferring neural network systems
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 特征推断神经网络系统
- en: ?) proposed one of the first neural network architectures for NER, with feature
    vectors constructed from orthographic features (e.g., capitalization of the first
    character), dictionaries and lexicons. Later work replaced these manually constructed
    feature vectors with word embeddings [[Collobert et al. (2011](#bib.bibx16)],
    which are representations of words in $n$-dimensional space, typically learned
    over large collections of unlabeled data through an unsupervised process such
    as the skip-gram model [[Mikolov et al. (2013](#bib.bibx48)]. Studies have shown
    the importance of such pre-trained word embeddings for neural network based NER
    systems [[Habibi et al. (2017](#bib.bibx27)], and similarly for pre-trained character
    embeddings in character-based languages like Chinese [[Li et al. (2015](#bib.bibx40),
    [Yin et al. (2016](#bib.bibx81)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 提出了首批用于NER的神经网络架构之一，特征向量由正字法特征（例如首字母大写）、词典和词汇表构成。后来的工作用词嵌入 [[Collobert et
    al. (2011](#bib.bibx16)] 替代了这些手动构造的特征向量，词嵌入是在$n$维空间中的词的表示，通常通过无监督过程如skip-gram模型
    [[Mikolov et al. (2013](#bib.bibx48)] 从大量未标记数据中学习得到。研究表明，这种预训练的词嵌入对基于神经网络的NER系统至关重要
    [[Habibi et al. (2017](#bib.bibx27)]，同样对于中文等字符语言中的预训练字符嵌入也具有类似的重要性 [[Li et al.
    (2015](#bib.bibx40), [Yin et al. (2016](#bib.bibx81)]。
- en: Modern neural architectures for NER can be broadly classified into categories
    depending upon their representation of the words in a sentence. For example, representations
    may be based on words, characters, other sub-word units or any combination of
    these.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现代NER神经网络架构可以根据其对句子中单词的表示大致分为几类。例如，表示可以基于单词、字符、其他子词单元或这些的任何组合。
- en: 6.4.1 Word level architectures
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1 单词级架构
- en: <svg   height="142.03" overflow="visible" version="1.1" width="575.54"><g transform="translate(0,142.03)
    matrix(1 0 0 -1 0 0) translate(70.77,0) translate(0,2.21)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -31.17 18.2)" fill="#000000"
    stroke="#000000"><foreignobject width="38.55" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Words</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -58.27 41.33)" fill="#000000" stroke="#000000"><foreignobject width="105.93"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word Embedding</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -51.89 62.3)" fill="#000000" stroke="#000000"><foreignobject
    width="90.36" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word
    LSTM-F</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -52.2 84.35)" fill="#000000"
    stroke="#000000"><foreignobject width="91.13" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Word LSTM-B</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -67.08 107.47)" fill="#000000" stroke="#000000"><foreignobject width="128.72"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word Representation</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -29.2 128.44)" fill="#000000" stroke="#000000"><foreignobject
    width="33.25" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Label</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 68.02 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="26.79" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Best</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 60.41 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 147.56 19.34)" fill="#000000" stroke="#000000"><foreignobject
    width="24.79" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Buy</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 141.07 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="41.03" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 232.5 18.2)" fill="#000000" stroke="#000000"><foreignobject
    width="9.3" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">’s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 231.92 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">O</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 302.89 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="30.17" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CEO</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 310.66 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">O</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 376.47 18.2)" fill="#000000" stroke="#000000"><foreignobject
    width="43.09" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Hubert</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 376.33 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="43.43" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-PER</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 462.37 19.28)" fill="#000000" stroke="#000000"><foreignobject
    width="25.18" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Joly</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 456.99 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="38.63" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I-PER</foreignobject></g></g></g></svg>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`<svg   height="142.03" overflow="visible" version="1.1" width="575.54"><g
    transform="translate(0,142.03) matrix(1 0 0 -1 0 0) translate(70.77,0) translate(0,2.21)"
    fill="#000000" stroke="#000000"><g stroke-width="0.4pt"><g transform="matrix(0.8
    0.0 0.0 0.8 -31.17 18.2)" fill="#000000" stroke="#000000"><foreignobject width="38.55"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Words</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -58.27 41.33)" fill="#000000" stroke="#000000"><foreignobject
    width="105.93" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word
    Embedding</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -51.89 62.3)"
    fill="#000000" stroke="#000000"><foreignobject width="90.36" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Word LSTM-F</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -52.2 84.35)" fill="#000000" stroke="#000000"><foreignobject width="91.13"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word LSTM-B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -67.08 107.47)" fill="#000000" stroke="#000000"><foreignobject
    width="128.72" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word
    Representation</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -29.2 128.44)"
    fill="#000000" stroke="#000000"><foreignobject width="33.25" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Label</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 68.02 18.27)" fill="#000000" stroke="#000000"><foreignobject width="26.79"
    height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Best</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 60.41 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 147.56 19.34)" fill="#000000" stroke="#000000"><foreignobject
    width="24.79" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Buy</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 141.07 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="41.03" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 232.5 18.2)" fill="#000000" stroke="#000000"><foreignobject
    width="9.3" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">’s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 231.92 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">O</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 302.89 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="30.17" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">CEO</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 310.66 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">O</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 376.47 18.2)" fill="#000000" stroke="#000000"><foreignobject
    width="43.09" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Hubert</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 376.33 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="43.43" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-PER</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 462.37 19.28)" fill="#000000" stroke="#000000"><foreignobject
    width="25.18" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Joly</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 456.99 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="38.63" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I-PER</foreignobject></g></g></g></svg>`'
- en: 'Figure 1: Word level NN architecture for NER'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：用于NER的词级神经网络架构
- en: In this architecture, the words of a sentence are given as input to Recurrent
    Neural Networks (RNN) and each word is represented by its word embedding, as shown
    in [Figure 1](#S6.F1 "In 6.4.1 Word level architectures ‣ 6.4 Feature-inferring
    neural network systems ‣ 6 NER systems ‣ A Survey on Recent Advances in Named
    Entity Recognition from Deep Learning models").
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，句子的单词作为输入提供给递归神经网络（RNN），每个单词由其词嵌入表示，如[图1](#S6.F1 "在6.4.1 词级架构 ‣ 6.4 特征推断神经网络系统
    ‣ 6 NER系统 ‣ 基于深度学习模型的命名实体识别最近进展综述")所示。
- en: 'The first word-level NN model was proposed by ?)^(10)^(10)10Code: https://ronan.collobert.com/senna/.
    The architecture was similar to the one shown in [Figure 1](#S6.F1 "In 6.4.1 Word
    level architectures ‣ 6.4 Feature-inferring neural network systems ‣ 6 NER systems
    ‣ A Survey on Recent Advances in Named Entity Recognition from Deep Learning models"),
    but a convolution layer was used instead of the Bi-LSTM layer and the output of
    the convolution layer was given to a CRF layer for the final prediction. The authors
    achieved 89.59% F1 score on English CoNLL 2003 dataset by including gazetteers
    and SENNA embeddings.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个词级神经网络模型由?）提出^(10)^(10)10代码： https://ronan.collobert.com/senna/。该架构类似于[图1](#S6.F1
    "在6.4.1 词级架构 ‣ 6.4 特征推断神经网络系统 ‣ 6 NER系统 ‣ 基于深度学习模型的命名实体识别最近进展综述")中所示，但使用了卷积层代替了Bi-LSTM层，并将卷积层的输出传递给CRF层进行最终预测。作者通过包括地名词典和SENNA嵌入，在英文CoNLL
    2003数据集上达到了89.59%的F1分数。
- en: '?) presented a word LSTM model ([Figure 1](#S6.F1 "In 6.4.1 Word level architectures
    ‣ 6.4 Feature-inferring neural network systems ‣ 6 NER systems ‣ A Survey on Recent
    Advances in Named Entity Recognition from Deep Learning models")) and showed that
    adding a CRF layer to the top of the word LSTM improved performance, achieving
    84.26% F1 score on English CoNLL 2003 dataset. Similar systems were applied to
    other domains: DrugNER by ?) achieving 85.19% F1 score (under an unofficial evaluation)
    on MedLine test data [[Segura Bedmar et al. (2013](#bib.bibx67)], and medical
    NER by ?) achieving 80.22% F1 on disease NER corpus using this architecture. In
    similar tasks, ?) implemented the same model for multilingual POS tagging.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ?）提出了一个词级LSTM模型（[图1](#S6.F1 "在6.4.1 词级架构 ‣ 6.4 特征推断神经网络系统 ‣ 6 NER系统 ‣ 基于深度学习模型的命名实体识别最近进展综述")），并展示了在词LSTM顶部添加CRF层可以提高性能，在英文CoNLL
    2003数据集上达到了84.26%的F1分数。类似系统应用于其他领域：DrugNER由?）在MedLine测试数据上（非正式评估）达到了85.19%的F1分数[[Segura
    Bedmar等（2013](#bib.bibx67)]，以及?）在疾病NER语料库上使用此架构实现了80.22%的F1分数。在类似任务中，?）为多语言POS标注实现了相同的模型。
- en: With slight variations, ?) implemented word level feed forward NN, bi-directional
    LSTM (bi-LSTM) and window bi-LSTM for NER of English, German and Arabic. They
    also highlighted the performance improvement after adding various features like
    CRF, case, POS, word embeddings and achieved 88.91% F1 score on English and 76.12%
    on German.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ?）实现了用于英语、德语和阿拉伯语NER的词级前馈神经网络、双向LSTM（bi-LSTM）和窗口bi-LSTM，尽管有所不同。他们还突出了添加各种特征（如CRF、大小写、POS、词嵌入）后的性能改进，在英语上达到了88.91%的F1分数，在德语上达到了76.12%的F1分数。
- en: 6.4.2 Character level architectures
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2 字符级架构
- en: <svg   height="126.44" overflow="visible" version="1.1" width="614.47"><g transform="translate(0,126.44)
    matrix(1 0 0 -1 0 0) translate(85.29,0) translate(0,-13.38)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -57.69 18.2)" fill="#000000"
    stroke="#000000"><foreignobject width="65.5" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Characters</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -72.79 41.33)" fill="#000000" stroke="#000000"><foreignobject width="102.86"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Char Embedding</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -66.41 62.3)" fill="#000000" stroke="#000000"><foreignobject
    width="87.29" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Char
    LSTM-F</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -66.72 84.35)"
    fill="#000000" stroke="#000000"><foreignobject width="88.06" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Char LSTM-B</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -81.6 107.47)" fill="#000000" stroke="#000000"><foreignobject width="125.65"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Char Representation</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -44.95 128.44)" fill="#000000" stroke="#000000"><foreignobject
    width="33.25" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Label</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 46.47 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="9.8" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 32.06 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 98.33 19.66)" fill="#000000" stroke="#000000"><foreignobject
    width="6.15" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">e</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 82.45 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 149 19.66)" fill="#000000" stroke="#000000"><foreignobject
    width="5.46" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 132.85 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 199.42 18.64)" fill="#000000" stroke="#000000"><foreignobject
    width="5.38" height="8.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">t</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 183.24 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 247.82 19.28)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">␣</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 233.64 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 298.44 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="9.8" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 284.03 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 349.68 19.66)" fill="#000000" stroke="#000000"><foreignobject
    width="7.69" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">u</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 334.42 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 400.23 20.74)" fill="#000000" stroke="#000000"><foreignobject
    width="7.3" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">y</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 384.82 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 452.01 18.2)" fill="#000000" stroke="#000000"><foreignobject
    width="3.84" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">’</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 449.24 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">O</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 501.76 19.66)" fill="#000000" stroke="#000000"><foreignobject
    width="5.46" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 499.63 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">O</foreignobject></g></g></g></svg>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="126.44" overflow="visible" version="1.1" width="614.47"><g transform="translate(0,126.44)
    matrix(1 0 0 -1 0 0) translate(85.29,0) translate(0,-13.38)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -57.69 18.2)" fill="#000000"
    stroke="#000000"><foreignobject width="65.5" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">字符</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -72.79 41.33)" fill="#000000" stroke="#000000"><foreignobject width="102.86"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">字符嵌入</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -66.41 62.3)" fill="#000000" stroke="#000000"><foreignobject
    width="87.29" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">字符LSTM-F</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -66.72 84.35)" fill="#000000" stroke="#000000"><foreignobject
    width="88.06" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">字符LSTM-B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -81.6 107.47)" fill="#000000" stroke="#000000"><foreignobject
    width="125.65" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">字符表示</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -44.95 128.44)" fill="#000000" stroke="#000000"><foreignobject
    width="33.25" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">标签</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 46.47 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="9.8" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 32.06 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 98.33 19.66)" fill="#000000" stroke="#000000"><foreignobject
    width="6.15" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">e</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 82.45 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 149 19.66)" fill="#000000" stroke="#000000"><foreignobject
    width="5.46" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 132.85 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 199.42 18.64)" fill="#000000" stroke="#000000"><foreignobject
    width="5.38" height="8.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">t</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 183.24 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 247.82 19.28)" fill="#000000" stroke="#000000"><foreignobject
    width="10.38" height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">␣</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 233.64 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0
- en: 'Figure 2: Character level NN architecture for NER'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：命名实体识别的字符级神经网络架构
- en: In this model, a sentence is taken to be a sequence of characters. This sequence
    is passed through an RNN, predicting labels for each character ([Figure 2](#S6.F2
    "In 6.4.2 Character level architectures ‣ 6.4 Feature-inferring neural network
    systems ‣ 6 NER systems ‣ A Survey on Recent Advances in Named Entity Recognition
    from Deep Learning models")). Character labels transformed into word labels via
    post processing. The potential of character NER neural models was first highlighted
    by ?) using highway networks over convolution neural networks (CNN) on character
    sequences of words and then using another layer of LSTM + softmax for the final
    predictions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，句子被视为字符序列。这个序列经过RNN处理，为每个字符预测标签（[图2](#S6.F2 "在6.4.2字符级架构中的特征推断神经网络系统中的6
    NER系统的最近进展调查的6.4特征推断神经网络系统"））。字符标签通过后处理转换为单词标签。字符NER神经模型的潜力首先由? )）在单词的字符序列上使用了高速公路网络（highway
    networks），然后使用了另一层的LSTM + softmax进行最终的预测。
- en: This model was implemented by ?) for Vietnamese NER and achieved 80.23% F-score
    on ?)’s Vietnamese test data. Character models were also used in various other
    languages like Chinese [[Dong et al. (2016](#bib.bibx20)] where it has achieved
    near state of the art performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 对越南语NER进行了模型实现，并在?）的越南语测试数据上取得了80.23％的F分数。字符模型还在其他各种语言中得到了应用，比如在中国语[[Dong
    et al. (2016](#bib.bibx20)]中，它取得了接近最前沿的性能。
- en: '?) proposed CharNER ^(11)^(11)11Code: https://github.com/ozanarkancan/char-ner
    which implemented the character RNN model for NER on 7 different languages. In
    this character model, tag prediction over characters were converted to word tags
    using Viterbi decoder[[Forney (1973](#bib.bibx24)] achieving 82.18% on Spanish,
    79.36% on Dutch, 84.52% on English and 70.12% on German CoNLL datasets. They also
    achieved 78.72 on Arabic, 72.19 on Czech and 91.30 on Turkish. ?) proposed word
    representation using RNN (Bi-LSTM) over characters of the word and achieved state
    of the art results on POS task using this representation in multiple languages
    including 97.78% accuracy on English PTB[[Marcus et al. (1993](#bib.bibx47)].'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 提出了CharNER ^(11)^(11)11Code： https://github.com/ozanarkancan/char-ner，它在7种不同的语言上实现了字符RNN模型用于NER。在这个字符模型中，对字符的标签预测通过维特比译码器被转换成单词标签[[Forney
    (1973](#bib.bibx24)]，在西班牙语上的性能达到了82.18％，荷兰语上的性能达到了79.36％，在英语上的性能达到了84.52％，在德语CoNLL数据集上的性能达到了70.12％。他们还在阿拉伯语上达到了78.72％，在捷克语上达到了72.19％，在土耳其语上达到了91.30％的F分数。
    ?) 提出了使用RNN（Bi-LSTM）来表示单词，利用单词的字符，并在包括英语PTB在内的多种语言上，使用这种表示来取得了最先进的结果，包括英语PTB上的97.78％的准确率[[Marcus
    et al. (1993](#bib.bibx47)]。
- en: ?) implemented sequence to sequence model (Byte to Span- BTS) using encoder
    decoder architecture over sequence of characters of words in a window of 60 characters.
    Each character was encoded in bytes and BTS achieved high performance on CoNLL
    2002 and 2003 dataset without any feature engineering. BTS achieved 82.95%, 82.84%,86.50%,76.22%
    Fscore on Spanish, Dutch, English and German CoNLL datasets respectively.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 实现了序列到序列模型（Byte到Span- BTS），使用编码器解码器架构在一个窗口内的60个字符的单词序列上。每个字符都以字节编码，BTS在CoNLL
    2002和2003数据集上取得了很高的性能，并且没有进行任何特征工程。BTS分别在西班牙语、荷兰语、英语和德语CoNLL数据集上分别取得了82.95％、82.84％、86.50％和76.22％的F分数。
- en: 6.4.3 Character+Word level architectures
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.3 字符+单词级架构
- en: 'Systems combining word context and the characters of a word have proved to
    be strong NER systems that need little domain specific knowledge or resources.
    There are two base models in this category. The first type of model represents
    words as a combination of a word embedding and a convolution over the characters
    of the word, follows this with a Bi-LSTM layer over the word representations of
    a sentence, and finally uses a softmax or CRF layer over the Bi-LSTM to generate
    labels. The architecture diagram for this model is same as [Figure 3](#S6.F3 "In
    6.4.3 Character+Word level architectures ‣ 6.4 Feature-inferring neural network
    systems ‣ 6 NER systems ‣ A Survey on Recent Advances in Named Entity Recognition
    from Deep Learning models") but with the character Bi-LSTM replaced with a CNN^(12)^(12)12Code:
    https://github.com/LopezGG/NN_NER_tensorFlow.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '结合词上下文和词字符的系统已被证明是强大的 NER 系统，只需很少的领域特定知识或资源。此类别有两种基本模型。第一种模型将词表示为词嵌入和词字符上的卷积的组合，然后在句子的词表示上进行
    Bi-LSTM 层处理，最后在 Bi-LSTM 上使用 softmax 或 CRF 层生成标签。该模型的架构图与 [Figure 3](#S6.F3 "In
    6.4.3 Character+Word level architectures ‣ 6.4 Feature-inferring neural network
    systems ‣ 6 NER systems ‣ A Survey on Recent Advances in Named Entity Recognition
    from Deep Learning models") 相同，但将字符 Bi-LSTM 替换为 CNN^(12)^(12)12Code: https://github.com/LopezGG/NN_NER_tensorFlow。'
- en: ?) implemented this model to achieve 91.21% F1 score on the CoNLL 2003 English
    dataset and 97.55% POS-tagging accuracy on the WSJ portion of PTB [[Marcus et
    al. (1993](#bib.bibx47)]. They also showed lower performance by this model for
    out of vocabulary words.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 实现了该模型，在 CoNLL 2003 英语数据集上达到了 91.21% 的 F1 分数，在 PTB 的 WSJ 部分达到了 97.55% 的 POS
    标记准确率 [[Marcus et al. (1993](#bib.bibx47)]。他们还展示了该模型在词汇外词上的性能较低。
- en: ?) achieved 91.62% F1 score on the CoNLL 2003 English dataset and 86.28% F score
    on Onto notes 5.0 dataset [[Pradhan et al. (2013](#bib.bibx59)] by adding lexicons
    and capitalization features to this model. Lexicon feature were encoded in the
    form or B(begin), I(inside) or E(end) PER, LOC, ORG and MISC depending upon the
    match from the dictionary.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 通过向该模型添加词典和大写特征，在 CoNLL 2003 英语数据集上达到了 91.62% 的 F1 分数，在 Onto notes 5.0 数据集上达到了
    86.28% 的 F 分数 [[Pradhan et al. (2013](#bib.bibx59)]。词典特征以 B(begin)、I(inside) 或
    E(end) PER、LOC、ORG 和 MISC 的形式编码，具体取决于字典中的匹配情况。
- en: This model has also been utilized for NER in languages like Japanese where ?)
    showed that this architecture outperformed other neural architectures on the organization
    entity class.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也被用于像日语这样的语言中的 NER，其中 ?) 证明了这一架构在组织实体类别上优于其他神经架构。
- en: ?) implemented a character+word level NER model for Twitter NER [[Baldwin et
    al. (2015](#bib.bibx5)] by concatenating a CNN over characters, a CNN over orthographic
    features of characters, a word embedding, and a word orthographic feature embedding.
    This concatenated representation is passed through another Bi-LSTM layer and the
    output is given to CRF for predicting. This model achieved 65.89% F score on segmentation
    alone and 52.41% F score on segmentation and categorization.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 为 Twitter NER 实现了一个字符+词级别的 NER 模型 [[Baldwin et al. (2015](#bib.bibx5)]，通过连接在字符上的
    CNN、字符的正字法特征 CNN、词嵌入和词的正字法特征嵌入来实现。这个连接的表示经过另一个 Bi-LSTM 层，并将输出传递给 CRF 进行预测。该模型在仅分割上达到了
    65.89% 的 F 分数，在分割和分类上达到了 52.41% 的 F 分数。
- en: ?) implemented a model with a CNN over the characters of word, concatenated
    with word embeddings of the central word and its neighbors, fed to a feed forward
    network, and followed by the Viterbi algorithm to predict labels for each word.
    The model achieved 82.21% F score on Spanish CoNLL 2002 data and 71.23% F score
    on Portuguese NER data [[Santos and Cardoso (2007](#bib.bibx64)].
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 实现了一个模型，该模型在词字符上应用 CNN，并将中央词及其邻居的词嵌入连接，输入到前馈网络中，然后通过维特比算法预测每个词的标签。该模型在西班牙语
    CoNLL 2002 数据上达到了 82.21% 的 F 分数，在葡萄牙语 NER 数据上达到了 71.23% 的 F 分数 [[Santos 和 Cardoso
    (2007](#bib.bibx64)]。
- en: <svg   height="213.49" overflow="visible" version="1.1" width="614.91"><g transform="translate(0,213.49)
    matrix(1 0 0 -1 0 0) translate(70.77,0) translate(0,73.68)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -41.95 -69.99)" fill="#000000"
    stroke="#000000"><foreignobject width="65.5" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Characters</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -57.04 -46.86)" fill="#000000" stroke="#000000"><foreignobject width="102.86"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Char Embedding</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -50.66 -25.89)" fill="#000000" stroke="#000000"><foreignobject
    width="87.29" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Char
    LSTM-F</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -50.97 -3.84)"
    fill="#000000" stroke="#000000"><foreignobject width="88.06" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Char LSTM-B</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -51.25 18.2)" fill="#000000" stroke="#000000"><foreignobject width="88.75"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word Features</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -67.08 41.33)" fill="#000000" stroke="#000000"><foreignobject
    width="128.72" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word
    Representation</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -51.89
    62.3)" fill="#000000" stroke="#000000"><foreignobject width="90.36" height="9.61"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word LSTM-F</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -52.2 84.35)" fill="#000000" stroke="#000000"><foreignobject
    width="91.13" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word
    LSTM-B</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -42.52 106.39)"
    fill="#000000" stroke="#000000"><foreignobject width="66.92" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Word CRF</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -29.2 128.44)" fill="#000000" stroke="#000000"><foreignobject width="33.25"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Label</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 74.82 -69.92)" fill="#000000" stroke="#000000"><foreignobject
    width="9.8" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 115.65 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="6.15" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">e</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 155.3 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="5.46" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 194.7 -69.55)" fill="#000000" stroke="#000000"><foreignobject
    width="5.38" height="8.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">t</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -10.3 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="26.79" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Best</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -16.95 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 271.67 -69.92)" fill="#000000" stroke="#000000"><foreignobject
    width="9.8" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 311.89 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="7.69" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">u</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 351.41 -67.45)" fill="#000000" stroke="#000000"><foreignobject
    width="7.3" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">y</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -2.58 19.34)" fill="#000000" stroke="#000000"><foreignobject
    width="24.79" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Buy</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -8.11 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="41.03" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 470.91 -69.99)" fill="#000000" stroke="#000000"><foreignobject
    width="3.84" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">’</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 509.63 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="5.46" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 10.53 18.2)" fill="#000000" stroke="#000000"><foreignobject
    width="9.3" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">’s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 10.92 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">O</foreignobject></g></g></g></svg>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`<svg height="213.49" overflow="visible" version="1.1" width="614.91"><g transform="translate(0,213.49)
    matrix(1 0 0 -1 0 0) translate(70.77,0) translate(0,73.68)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -41.95 -69.99)" fill="#000000"
    stroke="#000000"><foreignobject width="65.5" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">字符</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -57.04 -46.86)" fill="#000000" stroke="#000000"><foreignobject width="102.86"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">字符嵌入</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -50.66 -25.89)" fill="#000000" stroke="#000000"><foreignobject
    width="87.29" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">字符
    LSTM-F</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -50.97 -3.84)"
    fill="#000000" stroke="#000000"><foreignobject width="88.06" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">字符 LSTM-B</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -51.25 18.2)" fill="#000000" stroke="#000000"><foreignobject width="88.75"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">词汇特征</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -67.08 41.33)" fill="#000000" stroke="#000000"><foreignobject
    width="128.72" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">词汇表示</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -51.89 62.3)" fill="#000000" stroke="#000000"><foreignobject
    width="90.36" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">词汇
    LSTM-F</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -52.2 84.35)" fill="#000000"
    stroke="#000000"><foreignobject width="91.13" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">词汇 LSTM-B</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -42.52 106.39)" fill="#000000" stroke="#000000"><foreignobject width="66.92"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">词汇 CRF</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -29.2 128.44)" fill="#000000" stroke="#000000"><foreignobject
    width="33.25" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">标签</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 74.82 -69.92)" fill="#000000" stroke="#000000"><foreignobject
    width="9.8" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 115.65 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="6.15" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">e</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 155.3 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="5.46" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 194.7 -69.55)" fill="#000000" stroke="#000000"><foreignobject
    width="5.38" height="8.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">t</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -10.3 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="26.79" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">最佳</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -16.95 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 271.67 -69.92)" fill="#000000" stroke="#000000"><foreignobject
    width="9.8" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 311.89 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="7.69" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">u</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 351.41 -67.45)" fill="#000000" stroke="#000000"><foreignobject
    width="7.3" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">y</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -2.58 19.34)" fill="#000000" stroke="#000000"><foreignobject
    width="24.79" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">购买</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -8.11 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="41.03" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 470.91 -69.99)" fill="#000000" stroke="#000000"><foreignobject
    width="3.84" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">’</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 509.63 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="5.46" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 10.53 18.2)" fill="#000000" stroke="#000000"><foreignobject
    width="9.'
- en: 'Figure 3: Word+character level NN architecture for NER'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 用于 NER 的词+字符级别神经网络架构'
- en: 'The second type of model concatenates word embeddings with LSTMs (sometimes
    bi-directional) over the characters of a word, passing this representation through
    another sentence-level Bi-LSTM, and predicting the final tags using a final softmax
    or CRF layer ([Figure 3](#S6.F3 "In 6.4.3 Character+Word level architectures ‣
    6.4 Feature-inferring neural network systems ‣ 6 NER systems ‣ A Survey on Recent
    Advances in Named Entity Recognition from Deep Learning models")). ?)^(13)^(13)13Code:
    https://github.com/glample/tagger introduced this architecture and achieved 85.75%,
    81.74%, 90.94%, 78.76% Fscores on Spanish, Dutch, English and German NER dataset
    respectively from CoNLL 2002 and 2003.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '第二种模型将词嵌入与 LSTM（有时是双向的）连接，通过另一个句子级 Bi-LSTM 处理这种表示，并使用最终的 softmax 或 CRF 层预测最终标签（[图
    3](#S6.F3 "在 6.4.3 字符+词级别架构 ‣ 6.4 特征推断神经网络系统 ‣ 6 NER 系统 ‣ 深度学习模型中命名实体识别的最新进展调查")）。?)^(13)^(13)13代码:
    https://github.com/glample/tagger 引入了这一架构，并在 CoNLL 2002 和 2003 数据集中分别在西班牙语、荷兰语、英语和德语
    NER 数据上取得了 85.75%、81.74%、90.94% 和 78.76% 的 F 值。'
- en: '?) implemented this model in the NeuroNER toolkit^(14)^(14)14Code: http://neuroner.com
    with the main goal of providing easy usability and allowing easy plotting of real
    time performance and learning statistics of the model. The BRAT annotation tool^(15)^(15)15Code:
    http://brat.nlplab.org/ is also integrated with NeuroNER to ease the development
    of NN NER models in new domains. NeuroNER achieved 90.50% F score on the English
    CoNLL 2003 data.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '?) 在 NeuroNER 工具包中实现了该模型^(14)^(14)14代码: http://neuroner.com，主要目标是提供易用性，并允许轻松绘制模型的实时性能和学习统计数据。BRAT
    注释工具^(15)^(15)15代码: http://brat.nlplab.org/ 也与 NeuroNER 集成，以便于在新领域开发 NN NER 模型。NeuroNER
    在英语 CoNLL 2003 数据上取得了 90.50% 的 F 值。'
- en: ?) implemented the model for various biomedical NER tasks and achieved higher
    performance than the majority of other participants. For example, they achieved
    83.71 F-score on the CHEMDNER data [[Krallinger et al. (2015](#bib.bibx34)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 实现了该模型用于各种生物医学 NER 任务，并取得了比大多数其他参与者更高的性能。例如，他们在 CHEMDNER 数据上取得了 83.71 的 F
    值 [[Krallinger et al. (2015](#bib.bibx34)]。
- en: '?)^(16)^(16)16Code: https://github.com/dmort27/epitran utilized phonemes (from
    Epitran) for NER in addition to characters and words. They also utilize attention
    knowledge over sequence of characters in word which is concatenated with the word
    embedding and character representation of word. This model achieved state of the
    art performance (85.81% F score) on Spanish CoNLL 2002 dataset.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '?)^(16)^(16)16代码: https://github.com/dmort27/epitran 在 NER 中利用了来自 Epitran 的音素，除了字符和词之外。他们还利用了字符序列的注意力知识，并将其与词嵌入和词的字符表示进行连接。该模型在西班牙语
    CoNLL 2002 数据集上达到了前沿性能（85.81% F 值）。'
- en: A slightly improved system focusing on multi-task and multi-lingual joint learning
    was proposed by ?) where word representation given by GRU (Gated Recurrent Unit)
    cell over characters plus word embedding was passed through another RNN layer
    and the output was given to CRF models trained for different tasks like POS, chunking
    and NER. ?) further proposed transfer learning for multi-task and multi-learning,
    and showed small improvements on CoNLL 2002 and 2003 NER data, achieving 85.77%,
    85.19%, 91.26% F scores on Spanish, Dutch and English, respectively.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 提出了一个稍有改进的系统，该系统专注于多任务和多语言联合学习，其中由 GRU（门控递归单元）单元生成的词表示和词嵌入经过另一个 RNN 层处理，并将输出传递给为不同任务（如词性标注、短语块划分和命名实体识别）训练的
    CRF 模型。?) 进一步提出了多任务和多学习的迁移学习，并在 CoNLL 2002 和 2003 NER 数据上展示了小幅改进，分别在西班牙语、荷兰语和英语上取得了
    85.77%、85.19% 和 91.26% 的 F 值。
- en: 6.4.4 Character + Word + affix model
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.4 字符 + 词 + 前缀模型
- en: <svg   height="213.49" overflow="visible" version="1.1" width="614.91"><g transform="translate(0,213.49)
    matrix(1 0 0 -1 0 0) translate(70.77,0) translate(0,73.68)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -41.95 -69.99)" fill="#000000"
    stroke="#000000"><foreignobject width="65.5" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Characters</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -57.04 -46.86)" fill="#000000" stroke="#000000"><foreignobject width="102.86"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Char Embedding</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -50.66 -25.89)" fill="#000000" stroke="#000000"><foreignobject
    width="87.29" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Char
    LSTM-F</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -50.97 -3.84)"
    fill="#000000" stroke="#000000"><foreignobject width="88.06" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Char LSTM-B</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -51.25 18.2)" fill="#000000" stroke="#000000"><foreignobject width="88.75"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word Features</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -67.08 41.33)" fill="#000000" stroke="#000000"><foreignobject
    width="128.72" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word
    Representation</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -51.89
    62.3)" fill="#000000" stroke="#000000"><foreignobject width="90.36" height="9.61"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word LSTM-F</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -52.2 84.35)" fill="#000000" stroke="#000000"><foreignobject
    width="91.13" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Word
    LSTM-B</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -42.52 106.39)"
    fill="#000000" stroke="#000000"><foreignobject width="66.92" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Word CRF</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -29.2 128.44)" fill="#000000" stroke="#000000"><foreignobject width="33.25"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Label</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 74.82 -69.92)" fill="#000000" stroke="#000000"><foreignobject
    width="9.8" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 115.65 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="6.15" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">e</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 155.3 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="5.46" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 194.7 -69.55)" fill="#000000" stroke="#000000"><foreignobject
    width="5.38" height="8.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">t</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -10.09 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="21.41" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Bes</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -7.35 18.64)" fill="#000000" stroke="#000000"><foreignobject
    width="16.99" height="8.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">est</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -10.3 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="26.79" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Best</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -16.95 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 271.67 -69.92)" fill="#000000" stroke="#000000"><foreignobject
    width="9.8" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 311.89 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="7.69" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">u</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 351.41 -67.45)" fill="#000000" stroke="#000000"><foreignobject
    width="7.3" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">y</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -4.52 19.34)" fill="#000000" stroke="#000000"><foreignobject
    width="24.79" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Buy</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -3.55 19.34)" fill="#000000" stroke="#000000"><foreignobject
    width="24.79" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Buy</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -2.58 19.34)" fill="#000000" stroke="#000000"><foreignobject
    width="24.79" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Buy</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -8.11 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="41.03" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">I-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 470.91 -69.99)" fill="#000000" stroke="#000000"><foreignobject
    width="3.84" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">’</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 509.63 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="5.46" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 9.55 18.2)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\emptyset$</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 10.52 18.2)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\emptyset$</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 10.53 18.2)" fill="#000000" stroke="#000000"><foreignobject
    width="9.3" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">’s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 10.92 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="10.76" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">O</foreignobject></g></g></g></svg>
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="213.49" overflow="visible" version="1.1" width="614.91"><g transform="translate(0,213.49)
    matrix(1 0 0 -1 0 0) translate(70.77,0) translate(0,73.68)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(0.8 0.0 0.0 0.8 -41.95 -69.99)" fill="#000000"
    stroke="#000000"><foreignobject width="65.5" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">字符</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -57.04 -46.86)" fill="#000000" stroke="#000000"><foreignobject width="102.86"
    height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">字符嵌入</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -50.66 -25.89)" fill="#000000" stroke="#000000"><foreignobject
    width="87.29" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">字符
    LSTM-F</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -50.97 -3.84)"
    fill="#000000" stroke="#000000"><foreignobject width="88.06" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">字符 LSTM-B</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -51.25 18.2)" fill="#000000" stroke="#000000"><foreignobject width="88.75"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">词汇特征</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -67.08 41.33)" fill="#000000" stroke="#000000"><foreignobject
    width="128.72" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">词表示</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -51.89 62.3)" fill="#000000" stroke="#000000"><foreignobject
    width="90.36" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">词
    LSTM-F</foreignobject></g><g transform="matrix(0.8 0.0 0.0 0.8 -52.2 84.35)" fill="#000000"
    stroke="#000000"><foreignobject width="91.13" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">词 LSTM-B</foreignobject></g><g transform="matrix(0.8
    0.0 0.0 0.8 -42.52 106.39)" fill="#000000" stroke="#000000"><foreignobject width="66.92"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">词 CRF</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -29.2 128.44)" fill="#000000" stroke="#000000"><foreignobject
    width="33.25" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">标签</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 74.82 -69.92)" fill="#000000" stroke="#000000"><foreignobject
    width="9.8" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 115.65 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="6.15" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">e</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 155.3 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="5.46" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">s</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 194.7 -69.55)" fill="#000000" stroke="#000000"><foreignobject
    width="5.38" height="8.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">t</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -10.09 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="21.41" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">最</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -7.35 18.64)" fill="#000000" stroke="#000000"><foreignobject
    width="16.99" height="8.51" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">佳</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -10.3 18.27)" fill="#000000" stroke="#000000"><foreignobject
    width="26.79" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">最佳</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -16.95 128.5)" fill="#000000" stroke="#000000"><foreignobject
    width="45.84" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B-ORG</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 271.67 -69.92)" fill="#000000" stroke="#000000"><foreignobject
    width="9.8" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">B</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 311.89 -68.53)" fill="#000000" stroke="#000000"><foreignobject
    width="7.69" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">u</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 351.41 -67.45)" fill="#000000" stroke="#000000"><foreignobject
    width="7.3" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">y</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -4.52 19.34)" fill="#000000" stroke="#000000"><foreignobject
    width="24.79" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">购买</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -3.55 19.34)" fill="#000000" stroke="#000000"><foreignobject
    width="24.79" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">购买</foreignobject></g><g
    transform="matrix(0.8 0.0 0.0 0.8 -2.58 19.34)" fill="#000000" stroke="#000000"><foreignobject
    width="24.79" height="12
- en: 'Figure 4: Word+character+affix level NN architecture for NER'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：用于 NER 的词+字符+词缀级别 NN 结构
- en: '?) implemented a model that augments the character+word NN architecture with
    one of the most successful features from feature-engineering approaches: affixes.
    Affix features were used in early NER systems for CoNLL 2002 [[Tjong Kim Sang
    (2002](#bib.bibx73), [Cucerzan and Yarowsky (2002](#bib.bibx17)] and 2003 [[Tjong
    Kim Sang and De Meulder (2003](#bib.bibx72)] and for biomedical NER [[Saha et
    al. (2009](#bib.bibx63)], but had not been used in neural NER systems. They extended
    the ?) character+word model to learn affix embeddings^(17)^(17)17Code: https://github.com/vikas95/Pref_Suff_Span_NN
    alongside the word embeddings and character RNNs ([Figure 4](#S6.F4 "In 6.4.4
    Character + Word + affix model ‣ 6.4 Feature-inferring neural network systems
    ‣ 6 NER systems ‣ A Survey on Recent Advances in Named Entity Recognition from
    Deep Learning models")). They considered all n-gram prefixes and suffixes of words
    in the training corpus, and selected only those whose frequency was above a threshold,
    $T$. Their word+character+affix model achieved 87.26%, 87.54%, 90.86%, 79.01%
    on Spanish, Dutch, English and German CoNLL datasets respectively. ?) also showed
    that affix embeddings capture complementary information to that captured by RNNs
    over the characters of a word, that selecting only high frequency (realistic)
    affixes was important, and that embedding affixes was better than simply expanding
    the other embeddings to reach a similar number of hyper-parameters.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ?) 实现了一种将字符+词 NN 结构与特征工程方法中最成功的特征之一：词缀，结合的模型。词缀特征在早期的命名实体识别（NER）系统中用于 CoNLL
    2002 [[Tjong Kim Sang (2002](#bib.bibx73)，[Cucerzan 和 Yarowsky (2002](#bib.bibx17)]
    和 2003 [[Tjong Kim Sang 和 De Meulder (2003](#bib.bibx72)] 以及生物医学 NER [[Saha 等
    (2009](#bib.bibx63)] 中，但在神经 NER 系统中尚未使用。他们扩展了 ?) 字符+词模型，以学习词缀嵌入^(17)^(17)17代码：https://github.com/vikas95/Pref_Suff_Span_NN
    以及词嵌入和字符 RNN ([图 4](#S6.F4 "在 6.4.4 字符 + 词 + 词缀模型 ‣ 6.4 特征推断神经网络系统 ‣ 6 NER 系统
    ‣ 关于深度学习模型在命名实体识别中的最新进展的调查"))。他们考虑了训练语料库中所有的 n-gram 前缀和后缀，并仅选择了那些频率高于阈值 $T$ 的前缀和后缀。他们的词+字符+词缀模型在西班牙语、荷兰语、英语和德语
    CoNLL 数据集上的准确率分别达到了 87.26%、87.54%、90.86%、79.01%。?) 还表明，词缀嵌入捕捉到的信息与 RNN 捕捉到的字符信息互补，选择高频（现实的）词缀很重要，并且嵌入词缀比简单扩展其他嵌入以达到类似的超参数数量效果更好。
- en: 7 Discussion
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: '[Table 1](#S7.T1 "In 7 Discussion ‣ A Survey on Recent Advances in Named Entity
    Recognition from Deep Learning models") shows the results of all the different
    categories of systems discussed in [section 6](#S6 "6 NER systems ‣ A Survey on
    Recent Advances in Named Entity Recognition from Deep Learning models") on the
    CoNLL 2002 and 2003 datasets. The table also indicates, for each model, whether
    it makes use of external knowledge like a dictionary or gazetteer. [Table 2](#S7.T2
    "In 7 Discussion ‣ A Survey on Recent Advances in Named Entity Recognition from
    Deep Learning models") presents a similar analysis on the DrugNER dataset from
    SemEval 2013 task 9 [[Segura Bedmar et al. (2013](#bib.bibx67)].'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 1](#S7.T1 "在 7 讨论 ‣ 关于深度学习模型在命名实体识别中的最新进展的调查") 显示了在 CoNLL 2002 和 2003 数据集上讨论的所有不同类别系统的结果。该表还指示了每个模型是否使用了外部知识，如字典或地名集。[表
    2](#S7.T2 "在 7 讨论 ‣ 关于深度学习模型在命名实体识别中的最新进展的调查") 提供了 SemEval 2013 任务 9 [[Segura
    Bedmar 等 (2013](#bib.bibx67)] 的 DrugNER 数据集的类似分析。'
- en: '| Feature-engineered machine learning systems | Dict | SP | DU | EN | GE |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 特征工程机器学习系统 | 字典 | SP | DU | EN | GE |'
- en: '| ?) binary AdaBoost classifiers | Yes | 81.39 | 77.05 | - | - |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ?) 二元 AdaBoost 分类器 | 是 | 81.39 | 77.05 | - | - |'
- en: '| ?) - Maximum Entropy (ME) + features | Yes | 73.66 | 68.08 | - | - |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ?) - 最大熵 (ME) + 特征 | 是 | 73.66 | 68.08 | - | - |'
- en: '| ?) SVM with class weights | Yes | - | - | 88.3 | - |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ?) 带类别权重的 SVM | 是 | - | - | 88.3 | - |'
- en: '| ?) CRF | Yes | - | - | 90.90 | - |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| ?) CRF | 是 | - | - | 90.90 | - |'
- en: '| ?) Semi-supervised state of the art | No | - | - | 89.31 | 75.27 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| ?) 半监督的最先进方法 | 否 | - | - | 89.31 | 75.27 |'
- en: '| ?) | Yes | 84.16 | 85.04 | 91.36 | 76.42 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ?) | 是 | 84.16 | 85.04 | 91.36 | 76.42 |'
- en: '| Feature-inferring neural network word models |  |  |  |  |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: 特征推断神经网络词模型 |  |  |  |  |  |
- en: '| ?) Vanilla NN +SLL / Conv-CRF | No | - | - | 81.47 | - |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ?) 普通 NN +SLL / Conv-CRF | 否 | - | - | 81.47 | - |'
- en: '| ?) Bi-LSTM+CRF | No | - | - | 84.26 | - |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ?) Bi-LSTM+CRF | 否 | - | - | 84.26 | - |'
- en: '| ?) Win-BiLSTM (English), FF (German) (Many fets) | Yes | - | - | 88.91 |
    76.12 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ?) Win-BiLSTM (英语)，FF (德语)（许多特征） | 是 | - | - | 88.91 | 76.12 |'
- en: '| ?) Conv-CRF (SENNA+Gazetteer) | Yes | - | - | 89.59 | - |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ?) Conv-CRF (SENNA+地名集) | 是 | - | - | 89.59 | - |'
- en: '| ?) Bi-LSTM+CRF+ (SENNA+Gazetteer) | Yes | - | - | 90.10 | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ?) Bi-LSTM+CRF+ (SENNA+Gazetteer) | 是 | - | - | 90.10 | - |'
- en: '| Feature-inferring neural network character models |  |  |  |  |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 特征推断神经网络字符模型 |  |  |  |  |  |'
- en: '| ?) – BTS | No | 82.95 | 82.84 | 86.50 | 76.22 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: ?) – BTS | 否 | 82.95 | 82.84 | 86.50 | 76.22 |
- en: '| ?) CharNER | No | 82.18 | 79.36 | 84.52 | 70.12 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ?) CharNER | 否 | 82.18 | 79.36 | 84.52 | 70.12 |'
- en: '| Feature-inferring neural network word + character models |  |  |  |  |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 特征推断神经网络词汇 + 字符模型 |  |  |  |  |  |'
- en: '| ?) | Yes | 85.77 | 85.19 | 91.26 | - |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ?) | 是 | 85.77 | 85.19 | 91.26 | - |'
- en: '| ?) | Yes | - | - | 91.20 | - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| ?) | 是 | - | - | 91.20 | - |'
- en: '| ?) | Yes | - | - | 91.62 | - |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ?) | 是 | - | - | 91.62 | - |'
- en: '| ?) | No | - | - | 91.21 | - |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ?) | 否 | - | - | 91.21 | - |'
- en: '| ?) | No | 82.21 | - | - | - |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ?) | 否 | 82.21 | - | - | - |'
- en: '| ?) | No | 85.75 | 81.74 | 90.94 | 78.76 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ?) | 否 | 85.75 | 81.74 | 90.94 | 78.76 |'
- en: '| ?) | Yes | 85.81 | - | - | - |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ?) | 是 | 85.81 | - | - | - |'
- en: '| ?) | No | - | - | 90.5 | - |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: ?) | 否 | - | - | 90.5 | - |
- en: '| Feature-inferring neural network word + character + affix models |  |  |  |  |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 特征推断神经网络词汇 + 字符 + 词缀模型 |  |  |  |  |  |'
- en: '| Re-implementation of ?) (100 Epochs) | No | 85.34 | 85.27 | 90.24 | 78.44
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: Re-implementation of ?) (100 Epochs) | 否 | 85.34 | 85.27 | 90.24 | 78.44 |
- en: '| ?)(100 Epochs) | No | 86.92 | 87.50 | 90.69 | 78.56 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ?)(100 Epochs) | 否 | 86.92 | 87.50 | 90.69 | 78.56 |'
- en: '| ?) (150 Epochs) | No | 87.26 | 87.54 | 90.86 | 79.01 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ?) (150 Epochs) | 否 | 87.26 | 87.54 | 90.86 | 79.01 |'
- en: 'Table 1: Comparison of NER systemsin four languages: CoNLL 2002 Spanish (SP),
    CoNLL 2002 Dutch (DU), CoNLL 2003 English (EN), and CoNLL 2003 German (GE). Dict
    indicates whether or not the approach makes use of dictionary lookups. Best performance
    in each category is highlighted in bold.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：四种语言的NER系统比较：CoNLL 2002 西班牙语（SP），CoNLL 2002 荷兰语（DU），CoNLL 2003 英语（EN），和 CoNLL
    2003 德语（GE）。字典表示该方法是否使用了字典查找。每个类别中的最佳性能以**粗体**突出显示。
- en: '|  |  | MedLine (80.10% ) | DrugBank (19.90% ) | Complete dataset |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MedLine (80.10% ) | DrugBank (19.90% ) | 完整数据集 |'
- en: '|  | Dict | P | R | F1 | P | R | F1 | P | R | F1 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | 字典 | P | R | F1 | P | R | F1 | P | R | F1 |'
- en: '| Feature-engineered machine learning systems |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 特征工程机器学习系统 |  |  |  |  |  |  |  |  |  |  |'
- en: '| ?) | Yes | 60.70 | 55.80 | 58.10 | 88.10 | 87.50 | 87.80 | 73.40 | 69.80
    | 71.50 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ?) | 是 | 60.70 | 55.80 | 58.10 | 88.10 | 87.50 | 87.80 | 73.40 | 69.80 |
    71.50 |'
- en: '| ?) (baseline) | No | - | - | - | - | - | - | 78.41 | 67.78 | 72.71 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ?) (基线) | 否 | - | - | - | - | - | - | 78.41 | 67.78 | 72.71 |'
- en: '| ?) (MED. emb.) | No | - | - | - | - | - | - | 82.70 | 69.68 | 75.63 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ?) (MED. emb.) | 否 | - | - | - | - | - | - | 82.70 | 69.68 | 75.63 |'
- en: '| ?) (state of the art) | Yes | 78.77 | 60.21 | 68.25 | 90.60 | 88.82 | 89.70
    | 84.75 | 72.89 | 78.37 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| ?) (最新技术) | 是 | 78.77 | 60.21 | 68.25 | 90.60 | 88.82 | 89.70 | 84.75 | 72.89
    | 78.37 |'
- en: '| NN word model |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: NN 词汇模型 |  |  |  |  |  |  |  |  |  |  |
- en: '| ?) (relaxed performance) | No | 52.93 | 52.57 | 52.75 | 87.07 | 83.39 | 85.19
    | - | - | - |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: ?) (放宽性能) | 否 | 52.93 | 52.57 | 52.75 | 87.07 | 83.39 | 85.19 | - | - | - |
- en: '| NN word + character model |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: NN 词汇 + 字符模型 |  |  |  |  |  |  |  |  |  |  |
- en: '| ?) | No | 73.00 | 62.00 | 67.00 | 87.00 | 86.00 | 87.00 | 79.00 | 72.00 |
    75.00 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| ?) | 否 | 73.00 | 62.00 | 67.00 | 87.00 | 86.00 | 87.00 | 79.00 | 72.00 |
    75.00 |'
- en: '| NN word + character + affix model |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| NN 词汇 + 字符 + 词缀模型 |  |  |  |  |  |  |  |  |  |  |'
- en: '| ?) | No | 74.00 | 64.00 | 69.00 | 89.00 | 86.00 | 87.00 | 81.00 | 74.00 |
    77.00 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| ?) | 否 | 74.00 | 64.00 | 69.00 | 89.00 | 86.00 | 87.00 | 81.00 | 74.00 |
    77.00 |'
- en: 'Table 2: DrugNER results on the MedLine and DrugBank test data (80.10% and
    19.90% of the test data, respectively). The ?) experiments report no decimal places
    because they were run after the end of shared task, and the official evaluation
    script outputs no decimal places.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：DrugNER在MedLine和DrugBank测试数据上的结果（分别占测试数据的80.10%和19.90%）。?) 实验报告没有小数位数，因为它们是在共享任务结束后运行的，官方评估脚本不输出小数位数。
- en: Our first finding from the survey is that feature-inferring NN systems outperform
    feature-engineered systems, despite the latter’s access to domain specific rules,
    knowledge, features, and lexicons. For example, the best feature-engineered system
    for Spanish, ?), is 1.59% below the best feature-inferring neural network system,
    [[Lample et al. (2016](#bib.bibx36)], and 1.65% below the best neural network
    system that incorporates lexical resources [[Bharadwaj et al. (2016](#bib.bibx7)].
    Similarly, the best feature-engineered system for German, ?), is 2.34% below the
    best feature-inferring neural network system, ?). The differences are smaller
    for Dutch and English, but in neither case is the best feature-engineered model
    better than the best neural network model. In DrugNER, the word+character NN model
    outperforms the feature engineered system by 8.90% on MedLine test data and 3.50%
    on the overall dataset.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从调查中得到的第一个发现是，尽管特征工程系统具有访问领域特定规则、知识、特征和词典的优势，特征推断的神经网络系统仍然优于特征工程系统。例如，西班牙语的最佳特征工程系统
    ?)，比最佳特征推断神经网络系统[[Lample et al. (2016](#bib.bibx36)]低1.59%，比包含词汇资源的最佳神经网络系统[[Bharadwaj
    et al. (2016](#bib.bibx7)]低1.65%。类似地，德语的最佳特征工程系统 ?)，比最佳特征推断神经网络系统 ?)，低2.34%。荷兰语和英语的差异较小，但在这两种情况下，最佳特征工程模型都不如最佳神经网络模型。在DrugNER中，词+字符神经网络模型在MedLine测试数据上优于特征工程系统8.90%，在整体数据集上优于3.50%。
- en: Our next finding is that word+character hybrid models are generally better than
    both word-based and character-based models. For example, the best hybrid NN model
    for English, ?), is 0.52% better than the best word-based model, ?), and 5.12%
    better than the best character-based model, [[Kuru et al. (2016](#bib.bibx35)].
    Similarly, the best hybrid NN model for German, ?), is 2.64% better than the best
    word-based model, ?), and 2.54% better than the best character-based model, [[Kuru
    et al. (2016](#bib.bibx35)]. In DrugNER, the word+character hybrid model is better
    than the word model by 14.25% on MedLine test data and 1.81% on DrugBank test
    data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个发现是，词+字符混合模型通常优于基于词语和基于字符的模型。例如，英语的最佳混合神经网络模型 ?)，比最佳基于词语的模型 ?)，高0.52%，比最佳基于字符的模型[[Kuru
    et al. (2016](#bib.bibx35)]高5.12%。类似地，德语的最佳混合神经网络模型 ?)，比最佳基于词语的模型 ?)，高2.64%，比最佳基于字符的模型[[Kuru
    et al. (2016](#bib.bibx35)]高2.54%。在DrugNER中，词+字符混合模型在MedLine测试数据上比基于词语的模型好14.25%，在DrugBank测试数据上好1.81%。
- en: Our final finding is that there is still interesting progress to be made by
    incorporating key features of past feature-engineered models into modern NN architectures.
    ?)’s simple extension of ?) to incorporate affix features yields a very strong
    new model, achieving a new state-of-the-art in Spanish, Dutch, and German, and
    performing within 1% of the best model for English.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终发现是，通过将过去特征工程模型的关键特性融入现代神经网络架构，仍然可以取得有趣的进展。?）对 ?) 的简单扩展，融入词缀特征，产生了一个非常强大的新模型，在西班牙语、荷兰语和德语中达到了新的最先进水平，并在英语中表现接近最佳模型的1%以内。
- en: 8 Conclusion
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: Our survey of models for named entity recognition, covering both classic feature-engineered
    machine learning models, and modern feature-inferring neural network models has
    yielded several important insights. Neural network models generally outperform
    feature-engineered models, character+word hybrid neural networks generally outperform
    other representational choices, and further improvements are available by applying
    past insights to current neural network models, as shown by the state-of-the-art
    performance of our proposed affix-based extension of character+word hybrid models.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对命名实体识别模型的调查，包括经典的特征工程机器学习模型和现代的特征推断神经网络模型，带来了几个重要的见解。神经网络模型通常优于特征工程模型，字符+词语混合神经网络通常优于其他表示选择，进一步的改进可以通过将过去的见解应用于当前的神经网络模型来实现，正如我们提出的基于词缀的字符+词语混合模型的最先进性能所示。
- en: References
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Agerri and Rigau (2016] Rodrigo Agerri and German Rigau. 2016. Robust multilingual
    named entity recognition with shallow semi-supervised features. Artificial Intelligence,
    238:63–82.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Agerri and Rigau (2016] Rodrigo Agerri 和 German Rigau. 2016. 具有浅层半监督特征的鲁棒多语言命名实体识别。人工智能，238:63–82。'
- en: '[Alfonseca and Manandhar (2002] Enrique Alfonseca and Suresh Manandhar. 2002.
    An unsupervised method for general named entity recognition and automated concept
    discovery. In Proceedings of the 1st international conference on general WordNet,
    Mysore, India, pages 34–43.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Alfonseca 和 Manandhar (2002] Enrique Alfonseca 和 Suresh Manandhar. 2002. 一种用于通用命名实体识别和自动化概念发现的无监督方法。发表于第1届国际
    WordNet 会议论文集，印度迈索尔，第34–43页。'
- en: '[Ando and Zhang (2005a] Rie Kubota Ando and Tong Zhang. 2005a. A framework
    for learning predictive structures from multiple tasks and unlabeled data. Journal
    of Machine Learning Research, 6(Nov):1817–1853.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ando 和 Zhang (2005a] Rie Kubota Ando 和 Tong Zhang. 2005a. 从多任务和未标记数据中学习预测结构的框架。机器学习研究杂志，6（11月）：1817–1853。'
- en: '[Ando and Zhang (2005b] Rie Kubota Ando and Tong Zhang. 2005b. A framework
    for learning predictive structures from multiple tasks and unlabeled data. Journal
    of Machine Learning Research, 6(Nov):1817–1853.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ando 和 Zhang (2005b] Rie Kubota Ando 和 Tong Zhang. 2005b. 从多任务和未标记数据中学习预测结构的框架。机器学习研究杂志，6（11月）：1817–1853。'
- en: '[Baldwin et al. (2015] Timothy Baldwin, Marie-Catherine de Marneffe, Bo Han,
    Young-Bum Kim, Alan Ritter, and Wei Xu. 2015. Shared tasks of the 2015 workshop
    on noisy user-generated text: Twitter lexical normalization and named entity recognition.
    In Proceedings of the Workshop on Noisy User-generated Text, pages 126–135.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Baldwin 等 (2015] Timothy Baldwin、Marie-Catherine de Marneffe、Bo Han、Young-Bum
    Kim、Alan Ritter 和 Wei Xu. 2015. 2015年关于嘈杂用户生成文本的研讨会共享任务：Twitter 词汇规范化和命名实体识别。发表于嘈杂用户生成文本研讨会论文集，第126–135页。'
- en: '[Benikova et al. (2014] Darina Benikova, Chris Biemann, and Marc Reznicek.
    2014. Nosta-d named entity annotation for german: Guidelines and dataset. In LREC,
    pages 2524–2531.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Benikova 等 (2014] Darina Benikova、Chris Biemann 和 Marc Reznicek. 2014. 德语的
    Nosta-d 命名实体注释：指南和数据集。发表于 LREC，第2524–2531页。'
- en: '[Bharadwaj et al. (2016] Akash Bharadwaj, David R. Mortensen, Chris Dyer, and
    Carlos de Juan Carbonell. 2016. Phonologically aware neural model for named entity
    recognition in low resource transfer settings. In EMNLP.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bharadwaj 等 (2016] Akash Bharadwaj、David R. Mortensen、Chris Dyer 和 Carlos
    de Juan Carbonell. 2016. 低资源转移设置下的语音意识神经模型用于命名实体识别。发表于 EMNLP。'
- en: '[Bikel et al. (1997] Daniel M Bikel, Scott Miller, Richard Schwartz, and Ralph
    Weischedel. 1997. Nymble: a high-performance learning name-finder. In Proceedings
    of the fifth conference on Applied natural language processing, pages 194–201\.
    Association for Computational Linguistics.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bikel 等 (1997] Daniel M Bikel、Scott Miller、Richard Schwartz 和 Ralph Weischedel.
    1997. Nymble：一种高性能学习命名发现器。发表于第五届应用自然语言处理会议论文集，第194–201页。计算语言学协会。'
- en: '[Bossy et al. (2013] Robert Bossy, Wiktoria Golik, Zorana Ratkovic, Philippe
    Bessières, and Claire Nédellec. 2013. Bionlp shared task 2013–an overview of the
    bacteria biotope task. In Proceedings of the BioNLP Shared Task 2013 Workshop,
    pages 161–169.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bossy 等 (2013] Robert Bossy、Wiktoria Golik、Zorana Ratkovic、Philippe Bessières
    和 Claire Nédellec. 2013. BioNLP 共享任务 2013–细菌生态任务概述。发表于 BioNLP 共享任务 2013 研讨会论文集，第161–169页。'
- en: '[Carreras et al. (2002] Xavier Carreras, Lluís Màrquez, and Lluís Padró. 2002.
    Named entity extraction using adaboost, proceedings of the 6th conference on natural
    language learning. August, 31:1–4.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Carreras 等 (2002] Xavier Carreras、Lluís Màrquez 和 Lluís Padró. 2002. 使用 adaboost
    进行命名实体提取，发表于第6届自然语言学习会议论文集。8月，31:1–4。'
- en: '[Chalapathy et al. (2016] Raghavendra Chalapathy, Ehsan Zare Borzeshi, and
    Massimo Piccardi. 2016. An investigation of recurrent neural architectures for
    drug name recognition. arXiv preprint arXiv:1609.07585.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chalapathy 等 (2016] Raghavendra Chalapathy、Ehsan Zare Borzeshi 和 Massimo Piccardi.
    2016. 对药物名称识别的递归神经架构的研究。arXiv 预印本 arXiv:1609.07585。'
- en: '[Chinchor and Robinson (1997] Nancy Chinchor and Patricia Robinson. 1997. Muc-7
    named entity task definition. In Proceedings of the 7th Conference on Message
    Understanding, volume 29.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chinchor 和 Robinson (1997] Nancy Chinchor 和 Patricia Robinson. 1997. Muc-7
    命名实体任务定义。发表于第7届信息理解会议论文集，第29卷。'
- en: '[Chiu and Nichols (2015] Jason PC Chiu and Eric Nichols. 2015. Named entity
    recognition with bidirectional lstm-cnns. arXiv preprint arXiv:1511.08308.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chiu 和 Nichols (2015] Jason PC Chiu 和 Eric Nichols. 2015. 使用双向 LSTM-CNNs 的命名实体识别。arXiv
    预印本 arXiv:1511.08308。'
- en: '[Collins and Singer (1999] Michael Collins and Yoram Singer. 1999. Unsupervised
    models for named entity classification. In 1999 Joint SIGDAT Conference on Empirical
    Methods in Natural Language Processing and Very Large Corpora.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Collins 和 Singer (1999] Michael Collins 和 Yoram Singer. 1999. 用于命名实体分类的无监督模型。发表于1999年联合
    SIGDAT 会议上的自然语言处理和非常大语料库的经验方法。'
- en: '[Collobert and Weston (2008] Ronan Collobert and Jason Weston. 2008. A unified
    architecture for natural language processing: Deep neural networks with multitask
    learning. In Proceedings of the 25th international conference on Machine learning,
    pages 160–167\. ACM.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Collobert 和 Weston (2008] Ronan Collobert 和 Jason Weston. 2008. 自然语言处理的统一架构：具有多任务学习的深度神经网络。在第
    25 届国际机器学习大会论文集，第 160–167 页。ACM。'
- en: '[Collobert et al. (2011] Ronan Collobert, Jason Weston, Léon Bottou, Michael
    Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing
    (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493–2537.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Collobert 等 (2011] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen,
    Koray Kavukcuoglu 和 Pavel Kuksa. 2011. 从零开始的自然语言处理。机器学习研究期刊, 12(8):2493–2537。'
- en: '[Cucerzan and Yarowsky (2002] Silviu Cucerzan and David Yarowsky. 2002. Language
    independent ner using a unified model of internal and contextual evidence. In
    proceedings of the 6th conference on Natural language learning-Volume 20, pages
    1–4\. Association for Computational Linguistics.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cucerzan 和 Yarowsky (2002] Silviu Cucerzan 和 David Yarowsky. 2002. 使用内部和上下文证据的统一模型进行语言无关的命名实体识别。在第
    6 届自然语言学习会议论文集-第 20 卷，第 1–4 页。计算语言学协会。'
- en: '[Delėger et al. (2016] Louise Delėger, Robert Bossy, Estelle Chaix, Mouhamadou
    Ba, Arnaud Ferrė, Philippe Bessieres, and Claire Nėdellec. 2016. Overview of the
    bacteria biotope task at bionlp shared task 2016. In Proceedings of the 4th BioNLP
    Shared Task Workshop, pages 12–22.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Delėger 等 (2016] Louise Delėger, Robert Bossy, Estelle Chaix, Mouhamadou Ba,
    Arnaud Ferrė, Philippe Bessieres 和 Claire Nėdellec. 2016. 2016 年 BioNLP 共享任务的细菌生境任务概述。在第
    4 届 BioNLP 共享任务研讨会论文集，第 12–22 页。'
- en: '[Dernoncourt et al. (2017] Franck Dernoncourt, Ji Young Lee, and Peter Szolovits.
    2017. Neuroner: an easy-to-use program for named-entity recognition based on neural
    networks. arXiv preprint arXiv:1705.05487.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dernoncourt 等 (2017] Franck Dernoncourt, Ji Young Lee 和 Peter Szolovits. 2017.
    Neuroner：基于神经网络的易用命名实体识别程序。arXiv 预印本 arXiv:1705.05487。'
- en: '[Dong et al. (2016] Chuanhai Dong, Jiajun Zhang, Chengqing Zong, Masanori Hattori,
    and Hui Di. 2016. Character-based lstm-crf with radical-level features for chinese
    named entity recognition. In Natural Language Understanding and Intelligent Applications,
    pages 239–250\. Springer.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dong 等 (2016] Chuanhai Dong, Jiajun Zhang, Chengqing Zong, Masanori Hattori
    和 Hui Di. 2016. 基于字符的 lstm-crf 和部首级特征用于中文命名实体识别。在《自然语言理解与智能应用》，第 239–250 页。Springer。'
- en: '[Eltyeb and Salim (2014] Safaa Eltyeb and Naomie Salim. 2014. Chemical named
    entities recognition: a review on approaches and applications. Journal of cheminformatics,
    6(1):17.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Eltyeb 和 Salim (2014] Safaa Eltyeb 和 Naomie Salim. 2014. 化学命名实体识别：方法和应用综述。化学信息学杂志,
    6(1):17。'
- en: '[Etaiwi et al. (2017] Wael Etaiwi, Arafat Awajan, and Dima Suleiman. 2017.
    Statistical arabic name entity recognition approaches: A survey. Procedia Computer
    Science, 113:57–64.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Etaiwi 等 (2017] Wael Etaiwi, Arafat Awajan 和 Dima Suleiman. 2017. 统计阿拉伯语命名实体识别方法：综述。Procedia
    计算机科学, 113:57–64。'
- en: '[Etzioni et al. (2005] Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria
    Popescu, Tal Shaked, Stephen Soderland, Daniel S Weld, and Alexander Yates. 2005.
    Unsupervised named-entity extraction from the web: An experimental study. Artificial
    intelligence, 165(1):91–134.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Etzioni 等 (2005] Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu,
    Tal Shaked, Stephen Soderland, Daniel S Weld 和 Alexander Yates. 2005. 从网络中无监督的命名实体提取：实验研究。人工智能,
    165(1):91–134。'
- en: '[Forney (1973] G David Forney. 1973. The viterbi algorithm. Proceedings of
    the IEEE, 61(3):268–278.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Forney (1973] G David Forney. 1973. 维特比算法。IEEE 会议录, 61(3):268–278。'
- en: '[Gillick et al. (2015] Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag
    Subramanya. 2015. Multilingual language processing from bytes. arXiv preprint
    arXiv:1512.00103.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gillick 等 (2015] Dan Gillick, Cliff Brunk, Oriol Vinyals 和 Amarnag Subramanya.
    2015. 从字节开始的多语言处理。arXiv 预印本 arXiv:1512.00103。'
- en: '[Grishman and Sundheim (1996] Ralph Grishman and Beth Sundheim. 1996. Message
    understanding conference-6: A brief history. In COLING 1996 Volume 1: The 16th
    International Conference on Computational Linguistics, volume 1.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Grishman 和 Sundheim (1996] Ralph Grishman 和 Beth Sundheim. 1996. 消息理解会议-6：简要历史。在
    COLING 1996 第 1 卷：第 16 届国际计算语言学大会，第 1 卷。'
- en: '[Habibi et al. (2017] Maryam Habibi, Leon Weber, Mariana Neves, David Luis
    Wiegandt, and Ulf Leser. 2017. Deep learning with word embeddings improves biomedical
    named entity recognition. Bioinformatics, 33(14):i37–i48.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Habibi 等 (2017] Maryam Habibi, Leon Weber, Mariana Neves, David Luis Wiegandt
    和 Ulf Leser. 2017. 使用词嵌入的深度学习改进生物医学命名实体识别。生物信息学, 33(14):i37–i48。'
- en: '[Hettne et al. (2009] Kristina M Hettne, Rob H Stierum, Martijn J Schuemie,
    Peter JM Hendriksen, Bob JA Schijvenaars, Erik M van Mulligen, Jos Kleinjans,
    and Jan A Kors. 2009. A dictionary to identify small molecules and drugs in free
    text. Bioinformatics, 25(22):2983–2991.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hettne 等 (2009] Kristina M Hettne, Rob H Stierum, Martijn J Schuemie, Peter
    JM Hendriksen, Bob JA Schijvenaars, Erik M van Mulligen, Jos Kleinjans, 和 Jan
    A Kors. 2009. 用于识别自由文本中的小分子和药物的词典. 生物信息学, 25(22):2983–2991.'
- en: '[Hirschman et al. (2005] Lynette Hirschman, Alexander Yeh, Christian Blaschke,
    and Alfonso Valencia. 2005. Overview of biocreative: critical assessment of information
    extraction for biology.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hirschman 等 (2005] Lynette Hirschman, Alexander Yeh, Christian Blaschke, 和
    Alfonso Valencia. 2005. BioCreative 概述: 生物学信息提取的关键评估.'
- en: '[Huang et al. (2015] Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional
    lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Huang 等 (2015] Zhiheng Huang, Wei Xu, 和 Kai Yu. 2015. 用于序列标记的双向 LSTM-CRF 模型.
    arXiv 预印本 arXiv:1508.01991.'
- en: '[Kim et al. (2004] Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi,
    and Nigel Collier. 2004. Introduction to the bio-entity recognition task at jnlpba.
    In Proceedings of the international joint workshop on natural language processing
    in biomedicine and its applications, pages 70–75. Association for Computational
    Linguistics.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kim 等 (2004] Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi,
    和 Nigel Collier. 2004. 介绍 jnlpba 的生物实体识别任务. 在国际联合研讨会论文集，生物医学和其应用中的自然语言处理, 页码 70–75.
    计算语言学协会.'
- en: '[Kim et al. (2016] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M
    Rush. 2016. Character-aware neural language models. In AAAI, pages 2741–2749.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kim 等 (2016] Yoon Kim, Yacine Jernite, David Sontag, 和 Alexander M Rush. 2016.
    面向字符的神经语言模型. 在 AAAI, 页码 2741–2749.'
- en: '[Knox et al. (2010] Craig Knox, Vivian Law, Timothy Jewison, Philip Liu, Son
    Ly, Alex Frolkis, Allison Pon, Kelly Banco, Christine Mak, Vanessa Neveu, et al.
    2010. Drugbank 3.0: a comprehensive resource for ‘omics’ research on drugs. Nucleic
    acids research, 39(suppl_1):D1035–D1041.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Knox 等 (2010] Craig Knox, Vivian Law, Timothy Jewison, Philip Liu, Son Ly,
    Alex Frolkis, Allison Pon, Kelly Banco, Christine Mak, Vanessa Neveu 等. 2010.
    Drugbank 3.0: 一项关于药物的‘omics’研究的综合资源. 核酸研究, 39(suppl_1):D1035–D1041.'
- en: '[Krallinger et al. (2015] Martin Krallinger, Obdulia Rabal, Florian Leitner,
    Miguel Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan Lu, Donghong Ji,
    Daniel M Lowe, et al. 2015. The chemdner corpus of chemicals and drugs and its
    annotation principles. Journal of cheminformatics, 7(S1):S2.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Krallinger 等 (2015] Martin Krallinger, Obdulia Rabal, Florian Leitner, Miguel
    Vazquez, David Salgado, Zhiyong Lu, Robert Leaman, Yanan Lu, Donghong Ji, Daniel
    M Lowe 等. 2015. ChemDNER 化学品和药物语料库及其注释原则. 化学信息学期刊, 7(S1):S2.'
- en: '[Kuru et al. (2016] Onur Kuru, Ozan Arkan Can, and Deniz Yuret. 2016. Charner:
    Character-level named entity recognition. In Proceedings of COLING 2016, the 26th
    International Conference on Computational Linguistics: Technical Papers, pages
    911–921.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kuru 等 (2016] Onur Kuru, Ozan Arkan Can, 和 Deniz Yuret. 2016. Charner: 字符级命名实体识别.
    在 COLING 2016 论文集, 第 26 届国际计算语言学大会: 技术论文, 页码 911–921.'
- en: '[Lample et al. (2016] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian,
    Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition.
    arXiv preprint arXiv:1603.01360.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lample 等 (2016] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian,
    Kazuya Kawakami, 和 Chris Dyer. 2016. 用于命名实体识别的神经架构. arXiv 预印本 arXiv:1603.01360.'
- en: '[Leaman and Gonzalez (2008] Robert Leaman and Graciela Gonzalez. 2008. Banner:
    an executable survey of advances in biomedical named entity recognition. In Biocomputing
    2008, pages 652–663\. World Scientific.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Leaman 和 Gonzalez (2008] Robert Leaman 和 Graciela Gonzalez. 2008. Banner:
    一项关于生物医学命名实体识别进展的可执行调查。在 Biocomputing 2008, 页码 652–663\. World Scientific.'
- en: '[Lei et al. (2013] Jianbo Lei, Buzhou Tang, Xueqin Lu, Kaihua Gao, Min Jiang,
    and Hua Xu. 2013. A comprehensive study of named entity recognition in chinese
    clinical text. Journal of the American Medical Informatics Association, 21(5):808–814.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lei 等 (2013] Jianbo Lei, Buzhou Tang, Xueqin Lu, Kaihua Gao, Min Jiang, 和
    Hua Xu. 2013. 对中文临床文本中命名实体识别的综合研究. 美国医学信息学协会期刊, 21(5):808–814.'
- en: '[Li et al. (2005] Yaoyong Li, Kalina Bontcheva, and Hamish Cunningham. 2005.
    Svm based learning system for information extraction. In Deterministic and statistical
    methods in machine learning, pages 319–339\. Springer.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Li 等 (2005] Yaoyong Li, Kalina Bontcheva, 和 Hamish Cunningham. 2005. 基于 SVM
    的信息提取学习系统. 在机器学习中的确定性和统计方法, 页码 319–339\. Springer.'
- en: '[Li et al. (2015] Yanran Li, Wenjie Li, Fei Sun, and Sujian Li. 2015. Component-enhanced
    chinese character embeddings. arXiv preprint arXiv:1508.06669.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Li 等 (2015] Yanran Li, Wenjie Li, Fei Sun, 和 Sujian Li. 2015. 组件增强的中文字符嵌入.
    arXiv 预印本 arXiv:1508.06669.'
- en: '[Limsopatham and Collier (2016] Nut Limsopatham and Nigel Henry Collier. 2016.
    Bidirectional lstm for named entity recognition in twitter messages.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Limsopatham 和 Collier (2016)] Nut Limsopatham 和 Nigel Henry Collier. 2016.
    用于 Twitter 消息中命名实体识别的双向 LSTM。'
- en: '[Ling et al. (2015] Wang Ling, Tiago Luís, Luís Marujo, Ramón Fernandez Astudillo,
    Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015. Finding function
    in form: Compositional character models for open vocabulary word representation.
    arXiv preprint arXiv:1508.02096.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ling 等 (2015)] Wang Ling, Tiago Luís, Luís Marujo, Ramón Fernandez Astudillo,
    Silvio Amir, Chris Dyer, Alan W Black 和 Isabel Trancoso. 2015. 在形式中寻找功能：开放词汇表单词表示的组合字符模型。arXiv
    预印本 arXiv:1508.02096。'
- en: '[Liu et al. (2015] Shengyu Liu, Buzhou Tang, Qingcai Chen, and Xiaolong Wang.
    2015. Effects of semantic features on machine learning-based drug name recognition
    systems: word embeddings vs. manually constructed dictionaries. Information, 6(4):848–865.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Liu 等 (2015)] Shengyu Liu, Buzhou Tang, Qingcai Chen 和 Xiaolong Wang. 2015.
    语义特征对基于机器学习的药物名称识别系统的影响：词嵌入与手工构建的词典。《信息》，6(4)：848–865。'
- en: '[Luo (2015] 2015. Joint Named Entity Recognition and Disambiguation, September.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Luo (2015)] 2015. 联合命名实体识别与歧义消解，9月。'
- en: '[Ma and Hovy (2016] Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling
    via bi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ma 和 Hovy (2016)] Xuezhe Ma 和 Eduard Hovy. 2016. 通过双向 LSTM-CNNs-CRF 的端到端序列标注。arXiv
    预印本 arXiv:1603.01354。'
- en: '[Malouf (2002] Robert Malouf. 2002. Markov models for language-independent
    named entity recognition, proceedings of the 6th conference on natural language
    learning. August, 31:1–4.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Malouf (2002)] Robert Malouf. 2002. 语言无关的命名实体识别的马尔可夫模型，第六届自然语言学习会议论文集。8月，31：1–4。'
- en: '[Marcus et al. (1993] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice
    Santorini. 1993. Building a large annotated corpus of english: The penn treebank.
    Computational linguistics, 19(2):313–330.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Marcus 等 (1993)] Mitchell P Marcus, Mary Ann Marcinkiewicz 和 Beatrice Santorini.
    1993. 构建大型注释语料库的英语：宾州树库。《计算语言学》，19(2)：313–330。'
- en: '[Mikolov et al. (2013] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
    2013. Efficient estimation of word representations in vector space. arXiv preprint
    arXiv:1301.3781.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mikolov 等 (2013)] Tomas Mikolov, Kai Chen, Greg Corrado 和 Jeffrey Dean. 2013.
    高效的词向量空间表示估计。arXiv 预印本 arXiv:1301.3781。'
- en: '[Misawa et al. (2017] Shotaro Misawa, Motoki Taniguchi, Yasuhide Miura, and
    Tomoko Ohkuma. 2017. Character-based bidirectional lstm-crf with words and characters
    for japanese named entity recognition. In Proceedings of the First Workshop on
    Subword and Character Level Models in NLP, pages 97–102.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Misawa 等 (2017)] Shotaro Misawa, Motoki Taniguchi, Yasuhide Miura 和 Tomoko
    Ohkuma. 2017. 基于字符的双向 LSTM-CRF，结合单词和字符用于日语命名实体识别。发表于《第一次子词和字符级模型在 NLP 中的研讨会》，页码
    97–102。'
- en: '[Nadeau and Sekine (2007] David Nadeau and Satoshi Sekine. 2007. A survey of
    named entity recognition and classification. Lingvisticae Investigationes, 30(1):3–26.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Nadeau 和 Sekine (2007)] David Nadeau 和 Satoshi Sekine. 2007. 命名实体识别与分类的调查。《语言学调查》，30(1)：3–26。'
- en: '[Nadeau et al. (2006] David Nadeau, Peter D Turney, and Stan Matwin. 2006.
    Unsupervised named-entity recognition: Generating gazetteers and resolving ambiguity.
    In Conference of the Canadian Society for Computational Studies of Intelligence,
    pages 266–277\. Springer.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Nadeau 等 (2006)] David Nadeau, Peter D Turney 和 Stan Matwin. 2006. 无监督命名实体识别：生成地名辞典和解决歧义。发表于《加拿大计算智能学会会议》，页码
    266–277。Springer。'
- en: '[Nguyen et al. (2016] TS Nguyen, LM Nguyen, and XC Tran. 2016. Vietnamese named
    entity recognition at vlsp 2016 evaluation campaign. In Proceedings of The Fourth
    International Workshop on Vietnamese Language and Speech Processing.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Nguyen 等 (2016)] TS Nguyen, LM Nguyen 和 XC Tran. 2016. 在 VLSP 2016 评估活动中的越南语命名实体识别。发表于《第四届国际越南语语言和语音处理研讨会》。'
- en: '[Ohta et al. (2002] Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002. The
    genia corpus: An annotated research abstract corpus in molecular biology domain.
    In Proceedings of the second international conference on Human Language Technology
    Research, pages 82–86\. Morgan Kaufmann Publishers Inc.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ohta 等 (2002)] Tomoko Ohta, Yuka Tateisi 和 Jin-Dong Kim. 2002. GENIA 语料库：分子生物学领域的注释研究摘要语料库。发表于《第二届国际人类语言技术研究会议》，页码
    82–86。Morgan Kaufmann Publishers Inc.'
- en: '[Passos et al. (2014] Alexandre Passos, Vineet Kumar, and Andrew McCallum.
    2014. Lexicon infused phrase embeddings for named entity resolution. arXiv preprint
    arXiv:1404.5367.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Passos 等 (2014)] Alexandre Passos, Vineet Kumar 和 Andrew McCallum. 2014. 词典增强的短语嵌入用于命名实体解析。arXiv
    预印本 arXiv:1404.5367。'
- en: '[Patil et al. (2016] Nita Patil, Ajay S Patil, and BV Pawar. 2016. Survey of
    named entity recognition systems with respect to indian and foreign languages.
    International Journal of Computer Applications, 134(16).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Patil 等人 (2016)] Nita Patil、Ajay S Patil 和 BV Pawar。2016。关于印度和外国语言的命名实体识别系统的调查。《国际计算机应用期刊》，134(16)。'
- en: '[Pham and Le-Hong (2017] Thai-Hoang Pham and Phuong Le-Hong. 2017. End-to-end
    recurrent neural network models for vietnamese named entity recognition: Word-level
    vs. character-level. arXiv preprint arXiv:1705.04044.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pham 和 Le-Hong (2017)] Thai-Hoang Pham 和 Phuong Le-Hong。2017。用于越南语命名实体识别的端到端递归神经网络模型：词级别与字符级别。arXiv
    预印本 arXiv:1705.04044。'
- en: '[Piskorski et al. (2017] Jakub Piskorski, Lidia Pivovarova, Jan Šnajder, Josef
    Steinberger, Roman Yangarber, et al. 2017. The first cross-lingual challenge on
    recognition, normalization and matching of named entities in slavic languages.
    In Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing.
    Association for Computational Linguistics.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Piskorski 等人 (2017)] Jakub Piskorski、Lidia Pivovarova、Jan Šnajder、Josef Steinberger、Roman
    Yangarber 等。2017。首届跨语言挑战：斯拉夫语言中的命名实体识别、规范化和匹配。在第六届波罗-斯拉夫自然语言处理研讨会论文集中。计算语言学协会。'
- en: '[Plank et al. (2016] Barbara Plank, Anders Søgaard, and Yoav Goldberg. 2016.
    Multilingual part-of-speech tagging with bidirectional long short-term memory
    models and auxiliary loss. arXiv preprint arXiv:1604.05529.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Plank 等人 (2016)] Barbara Plank、Anders Søgaard 和 Yoav Goldberg。2016。利用双向长短期记忆模型和辅助损失进行多语言词性标注。arXiv
    预印本 arXiv:1604.05529。'
- en: '[Pradhan et al. (2013] Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou
    Ng, Anders Björkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards
    robust linguistic analysis using ontonotes. In Proceedings of the Seventeenth
    Conference on Computational Natural Language Learning, pages 143–152.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pradhan 等人 (2013)] Sameer Pradhan、Alessandro Moschitti、Nianwen Xue、Hwee Tou
    Ng、Anders Björkelund、Olga Uryupina、Yuchen Zhang 和 Zhi Zhong。2013。使用 Ontonotes
    实现稳健的语言分析。在第十七届计算自然语言学习会议论文集中，页码 143–152。'
- en: '[Rabiner and Juang (1986] Lawrence Rabiner and B Juang. 1986. An introduction
    to hidden markov models. ieee assp magazine, 3(1):4–16.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rabiner 和 Juang (1986)] Lawrence Rabiner 和 B Juang。1986。隐马尔可夫模型简介。IEEE ASSP
    杂志，3(1)：4–16。'
- en: '[Rajeev Sangal and Singh (2008] Dipti Misra Sharma Rajeev Sangal and Anil Kumar
    Singh, editors. 2008. Proceedings of the IJCNLP-08 Workshop on Named Entity Recognition
    for South and South East Asian Languages. Asian Federation of Natural Language
    Processing, Hyderabad, India, January.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rajeev Sangal 和 Singh (2008)] Dipti Misra Sharma Rajeev Sangal 和 Anil Kumar
    Singh，编辑。2008。南亚和东南亚语言命名实体识别的 IJCNLP-08 研讨会论文集。亚洲自然语言处理联盟，印度海得拉巴，1 月。'
- en: '[Rocktäschel et al. (2013] Tim Rocktäschel, Torsten Huber, Michael Weidlich,
    and Ulf Leser. 2013. Wbi-ner: The impact of domain-specific features on the performance
    of identifying and classifying mentions of drugs. In SemEval@ NAACL-HLT, pages
    356–363.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rocktäschel 等人 (2013)] Tim Rocktäschel、Torsten Huber、Michael Weidlich 和 Ulf
    Leser。2013。Wbi-ner：领域特定特征对药物提及识别和分类性能的影响。在 SemEval@ NAACL-HLT，页码 356–363。'
- en: '[Saha et al. (2009] Sujan Kumar Saha, Sudeshna Sarkar, and Pabitra Mitra. 2009.
    Feature selection techniques for maximum entropy based biomedical named entity
    recognition. Journal of Biomedical Informatics, 42(5):905 – 911. Biomedical Natural
    Language Processing.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Saha 等人 (2009)] Sujan Kumar Saha、Sudeshna Sarkar 和 Pabitra Mitra。2009。基于最大熵的生物医学命名实体识别的特征选择技术。《生物医学信息学杂志》，42(5)：905
    – 911。生物医学自然语言处理。'
- en: '[Santos and Cardoso (2007] Diana Santos and Nuno Cardoso. 2007. Reconhecimento
    de entidades mencionadas em português: Documentação e actas do harem, a primeira
    avaliação conjunta na área.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Santos 和 Cardoso (2007)] Diana Santos 和 Nuno Cardoso。2007。葡萄牙语中提到的实体识别：文档和《哈伦》第一次评估联合会议记录。'
- en: '[Santos and Guimaraes (2015] Cicero Nogueira dos Santos and Victor Guimaraes.
    2015. Boosting named entity recognition with neural character embeddings. arXiv
    preprint arXiv:1505.05008.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Santos 和 Guimaraes (2015)] Cicero Nogueira dos Santos 和 Victor Guimaraes。2015。通过神经字符嵌入提升命名实体识别。arXiv
    预印本 arXiv:1505.05008。'
- en: '[Schapire (2013] Robert E Schapire. 2013. Explaining adaboost. In Empirical
    inference, pages 37–52\. Springer.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Schapire (2013)] Robert E Schapire。2013。解释 AdaBoost。在《经验推断》中，页码 37–52。施普林格。'
- en: '[Segura Bedmar et al. (2013] Isabel Segura Bedmar, Paloma Martínez, and María
    Herrero Zazo. 2013. Semeval-2013 task 9: Extraction of drug-drug interactions
    from biomedical texts (ddiextraction 2013). Association for Computational Linguistics.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Segura Bedmar 等人 (2013)] Isabel Segura Bedmar、Paloma Martínez 和 María Herrero
    Zazo。2013。Semeval-2013 任务 9：从生物医学文本中提取药物-药物相互作用 (ddiextraction 2013)。计算语言学协会。'
- en: '[Shaalan (2014] Khaled Shaalan. 2014. A survey of arabic named entity recognition
    and classification. Computational Linguistics, 40(2):469–510.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Shaalan (2014] Khaled Shaalan. 2014. 阿拉伯语命名实体识别和分类的综述。计算语言学，第40卷第2期，第469–510页。'
- en: '[Sharnagat (2014] Rahul Sharnagat. 2014. Named entity recognition: A literature
    survey. Center For Indian Language Technology.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sharnagat (2014] Rahul Sharnagat. 2014. 命名实体识别：文献综述。印度语言技术中心。'
- en: '[Strassel et al. (2003] Stephanie Strassel, Alexis Mitchell, and Shudong Huang.
    2003. Multilingual resources for entity extraction. In Proceedings of the ACL
    2003 workshop on Multilingual and mixed-language named entity recognition-Volume
    15, pages 49–56\. Association for Computational Linguistics.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Strassel et al. (2003] Stephanie Strassel、Alexis Mitchell 和 Shudong Huang.
    2003. 实体提取的多语言资源。发表于ACL 2003多语言和混合语言命名实体识别研讨会论文集-第15卷，第49–56页。计算语言学协会。'
- en: '[Takeuchi and Collier (2002] Koichi Takeuchi and Nigel Collier. 2002. Use of
    support vector machines in extended named entity recognition. In proceedings of
    the 6th conference on Natural language learning-Volume 20, pages 1–7\. Association
    for Computational Linguistics.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Takeuchi and Collier (2002] Koichi Takeuchi 和 Nigel Collier. 2002. 在扩展的命名实体识别中使用支持向量机。发表于第六届自然语言学习会议论文集-第20卷，第1–7页。计算语言学协会。'
- en: '[Tjong Kim Sang and De Meulder (2003] Erik F Tjong Kim Sang and Fien De Meulder.
    2003. Introduction to the conll-2003 shared task: Language-independent named entity
    recognition. In Proceedings of the seventh conference on Natural language learning
    at HLT-NAACL 2003-Volume 4, pages 142–147\. Association for Computational Linguistics.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tjong Kim Sang and De Meulder (2003] Erik F Tjong Kim Sang 和 Fien De Meulder.
    2003. CONLL-2003共享任务介绍：语言无关的命名实体识别。发表于第七届自然语言学习会议HLT-NAACL 2003论文集-第4卷，第142–147页。计算语言学协会。'
- en: '[Tjong Kim Sang (2002] Erik F Tjong Kim Sang. 2002. Introduction to the conll-2002
    shared task: language-independent named entity recognition, proceedings of the
    6th conference on natural language learning. August, 31:1–4.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tjong Kim Sang (2002] Erik F Tjong Kim Sang. 2002. CONLL-2002共享任务介绍：语言无关的命名实体识别，发表于第六届自然语言学习会议论文集。8月31日，第1–4页。'
- en: '[Uzuner et al. (2007] Özlem Uzuner, Yuan Luo, and Peter Szolovits. 2007. Evaluating
    the state-of-the-art in automatic de-identification. Journal of the American Medical
    Informatics Association, 14(5):550–563.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Uzuner et al. (2007] Özlem Uzuner、Yuan Luo 和 Peter Szolovits. 2007. 评估自动去标识化的最先进技术。美国医学信息学学会期刊，第14卷第5期，第550–563页。'
- en: '[Uzuner et al. (2011] Özlem Uzuner, Brett R South, Shuying Shen, and Scott L
    DuVall. 2011. 2010 i2b2/va challenge on concepts, assertions, and relations in
    clinical text. Journal of the American Medical Informatics Association, 18(5):552–556.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Uzuner et al. (2011] Özlem Uzuner、Brett R South、Shuying Shen 和 Scott L DuVall.
    2011. 2010 i2b2/va挑战赛：临床文本中的概念、主张和关系。美国医学信息学学会期刊，第18卷第5期，第552–556页。'
- en: '[Xu et al. (2017] Kai Xu, Zhanfan Zhou, Tianyong Hao, and Wenyin Liu. 2017.
    A bidirectional lstm and conditional random fields approach to medical named entity
    recognition. In International Conference on Advanced Intelligent Systems and Informatics,
    pages 355–365\. Springer.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xu et al. (2017] Kai Xu、Zhanfan Zhou、Tianyong Hao 和 Wenyin Liu. 2017. 基于双向LSTM和条件随机场的医学命名实体识别方法。发表于国际先进智能系统与信息学会议，第355–365页。Springer。'
- en: '[Yadav et al. (2018] Vikas Yadav, Rebecca Sharp, and Steven Bethard. 2018.
    Deep affix features improve neural named entity recognizers. In Proceedings of
    the Seventh Joint Conference on Lexical and Computational Semantics, pages 167–172.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yadav et al. (2018] Vikas Yadav、Rebecca Sharp 和 Steven Bethard. 2018. 深度词缀特征改善神经命名实体识别器。发表于第七届词汇与计算语义学联合会议论文集，第167–172页。'
- en: '[Yan et al. (2016] Shao Yan, Christian Hardmeier, and Joakim Nivre. 2016. Multilingual
    named entity recognition using hybrid neural networks. In The Sixth Swedish Language
    Technology Conference (SLTC).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yan et al. (2016] Shao Yan、Christian Hardmeier 和 Joakim Nivre. 2016. 使用混合神经网络的多语言命名实体识别。发表于第六届瑞典语言技术会议（SLTC）。'
- en: '[Yang et al. (2016] Zhilin Yang, Ruslan Salakhutdinov, and William Cohen. 2016.
    Multi-task cross-lingual sequence tagging from scratch. arXiv preprint arXiv:1603.06270.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yang et al. (2016] Zhilin Yang、Ruslan Salakhutdinov 和 William Cohen. 2016.
    从零开始的多任务跨语言序列标注。arXiv预印本 arXiv:1603.06270。'
- en: '[Yang et al. (2017] Zhilin Yang, Ruslan Salakhutdinov, and William W Cohen.
    2017. Transfer learning for sequence tagging with hierarchical recurrent networks.
    arXiv preprint arXiv:1703.06345.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yang et al. (2017] Zhilin Yang、Ruslan Salakhutdinov 和 William W Cohen. 2017.
    使用层次递归网络进行序列标注的迁移学习。arXiv预印本 arXiv:1703.06345。'
- en: '[Yin et al. (2016] Rongchao Yin, Quan Wang, Peng Li, Rui Li, and Bin Wang.
    2016. Multi-granularity chinese word embedding. In Proceedings of the 2016 Conference
    on Empirical Methods in Natural Language Processing, pages 981–986.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[尹等 (2016)](https://example.org) Rongchao Yin, Quan Wang, Peng Li, Rui Li,
    和 Bin Wang. 2016. 多粒度中文词嵌入. 见于2016年自然语言处理实证方法会议论文集，第981–986页。'
- en: '[Zhang and Elhadad (2013] Shaodian Zhang and Noémie Elhadad. 2013. Unsupervised
    biomedical named entity recognition: Experiments with clinical and biological
    texts. Journal of biomedical informatics, 46(6):1088–1098.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[张和Elhadad (2013)](https://example.org) Shaodian Zhang 和 Noémie Elhadad. 2013.
    无监督生物医学命名实体识别：临床和生物文本的实验. 《生物医学信息学杂志》，46(6):1088–1098。'
- en: '[Zhou and Su (2002] GuoDong Zhou and Jian Su. 2002. Named entity recognition
    using an hmm-based chunk tagger. In proceedings of the 40th Annual Meeting on
    Association for Computational Linguistics, pages 473–480\. Association for Computational
    Linguistics.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[周和苏 (2002)](https://example.org) GuoDong Zhou 和 Jian Su. 2002. 使用基于HMM的块标注器进行命名实体识别.
    见于第40届计算语言学协会年会论文集，第473–480页。计算语言学协会。'
