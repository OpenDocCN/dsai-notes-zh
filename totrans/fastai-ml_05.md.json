["```py\n**from** **treeinterpreter** **import** treeinterpreter **as** tidf_train, df_valid = split_vals(df_raw[df_keep.columns], n_trn)row = X_valid.values[**None**,0]; row*array([[4364751, 2300944, 665, 172, 1.0, 1999, 3726.0, 3, 3232, 1111, 0, 63, 0, 5, 17, 35, 4, 4, 0, 1, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 19, 29, 3, 2, 1, 0, 0, 0, 0, 0, 2010, 9, 37,\n        16, 3, 259, False, False, False, False, False, False, 7912, False, False]], dtype=object)*prediction, bias, contributions = ti.predict(m, row)\n```", "```py\nprediction[0], bias[0]*(9.1909688098736275, 10.10606580677884)*\n```", "```py\nidxs = np.argsort(contributions[0])[o **for** o **in** zip(df_keep.columns[idxs], df_valid.iloc[0][idxs], contributions[0][idxs])]*[('ProductSize', 'Mini', -0.54680742853695008),\n ('age', 11, -0.12507089451852943),\n ('fiProductClassDesc',\n  'Hydraulic Excavator, Track - 3.0 to 4.0 Metric Tons',\n  -0.11143111128570773),\n ('fiModelDesc', 'KX1212', -0.065155113754146801),\n ('fiSecondaryDesc', nan, -0.055237427792181749),\n ('Enclosure', 'EROPS', -0.050467175593900217),\n ('fiModelDescriptor', nan, -0.042354676935508852),\n ('saleElapsed', 7912, -0.019642242073500914),\n ('saleDay', 16, -0.012812993479652724),\n ('Tire_Size', nan, -0.0029687660942271598),\n ('SalesID', 4364751, -0.0010443985823001434),\n ('saleDayofyear', 259, -0.00086540581130196688),\n ('Drive_System', nan, 0.0015385818526195915),\n ('Hydraulics', 'Standard', 0.0022411701338458821),\n ('state', 'Ohio', 0.0037587658190299409),\n ('ProductGroupDesc', 'Track Excavators', 0.0067688906745931197),\n ('ProductGroup', 'TEX', 0.014654732626326661),\n ('MachineID', 2300944, 0.015578052196894499),\n ('Hydraulics_Flow', nan, 0.028973749866174004),\n ('ModelID', 665, 0.038307429579276284),\n ('Coupler_System', nan, 0.052509808150765114),\n ('YearMade', 1999, 0.071829996446492878)]*\n```", "```py\narray([[4364751, 2300944, 665, 172, 1.0, 1999, 3726.0, 3, 3232, 1111, 0, 63, 0, 5, 17, 35, 4, 4, 0, 1, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 19, 29, 3, 2, 1, 0, 0, 0, 0, 0, 2010, 9, 37,\n        16, 3, 259, False, False, False, False, False, False, 7912, False, False]], dtype=object)\n```", "```py\ncontributions[0].sum()*-0.7383536391949419*\n```", "```py\ndf_ext = df_keep.copy()\ndf_ext['is_valid'] = 1\ndf_ext.is_valid[:n_trn] = 0\nx, y, nas = proc_df(df_ext, 'is_valid')\n```", "```py\nm = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=**True**)\nm.fit(x, y);\nm.oob_score_*0.99998753505765037*\n```", "```py\nfi = rf_feat_importance(m, x); fi[:10] \n```", "```py\nfeats=['SalesID', 'saleElapsed', 'MachineID'](X_train[feats]/1000).describe()\n```", "```py\n(X_valid[feats]/1000).describe()\n```", "```py\nx.drop(feats, axis=1, inplace=**True**)m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=**True**)\nm.fit(x, y);\nm.oob_score_*0.9789018385789966*\n```", "```py\nfi = rf_feat_importance(m, x); fi[:10]\n```", "```py\nset_rf_samples(50000)feats=['SalesID', 'saleElapsed', 'MachineID', 'age', 'YearMade', 'saleDayofyear']X_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, \n                        max_features=0.5, n_jobs=-1, oob_score=**True**)\nm.fit(X_train, y_train)\nprint_score(m)[0.21136509778791376, 0.2493668921196425, 0.90909393040946562, 0.88894821098056087, 0.89255408392415925]\n```", "```py\n**for** f **in** feats:\n    df_subs = df_keep.drop(f, axis=1)\n    X_train, X_valid = split_vals(df_subs, n_trn)\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, \n            max_features=0.5, n_jobs=-1, oob_score=**True**)\n    m.fit(X_train, y_train)\n    print(f)\n    print_score(m)**SalesID** *[0.20918653475938534, 0.2459966629213187, 0.9053273181678706, 0.89192968797265737, 0.89245205174299469]* **saleElapsed** *[0.2194124612957369, 0.2546442621643524, 0.90358104739129086, 0.8841980790762114, 0.88681881032219145]* **MachineID** *[0.206612984511148, 0.24446409479358033, 0.90312476862123559, 0.89327205732490311, 0.89501553584754967]* **age** *[0.21317740718919814, 0.2471719147150774, 0.90260198977488226, 0.89089460707372525, 0.89185129799503315]* **YearMade** *[0.21305398932040326, 0.2534570148977216, 0.90555219348567462, 0.88527538596974953, 0.89158854973045432]* **saleDayofyear** *[0.21320711524847227, 0.24629839782893828, 0.90881970943169987, 0.89166441133215968, 0.89272793857941679]*\n```", "```py\nreset_rf_samples()\n```", "```py\ndf_subs = df_keep.drop(['SalesID', 'MachineID', 'saleDayofyear'], \n               axis=1)\nX_train, X_valid = split_vals(df_subs, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, \n               max_features=0.5, n_jobs=-1, oob_score=**True**)\nm.fit(X_train, y_train)\nprint_score(m)[0.1418970082803121, 0.21779153679471935, 0.96040441863389681, 0.91529091848161925, 0.90918594039522138]\n```", "```py\nplot_fi(rf_feat_importance(m, X_train));\n```", "```py\nnp.save('tmp/subs_cols.npy', np.array(df_subs.columns))\n```", "```py\nm = RandomForestRegressor(n_estimators=160, max_features=0.5, \n          n_jobs=-1, oob_score=**True**)\n%time m.fit(X_train, y_train)\nprint_score(m)CPU times: user 6min 3s, sys: 2.75 s, total: 6min 6s\nWall time: 16.7 s\n[0.08104912951128229, 0.2109679613161783, 0.9865755186304942, 0.92051576728916762, 0.9143700001430598]\n```", "```py\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n**from** **fastai.imports** **import** *\n**from** **fastai.structured** **import** *\n**from** **sklearn.ensemble** **import** RandomForestRegressor, \n                             RandomForestClassifier\n**from** **IPython.display** **import** display\n**from** **sklearn** **import** metrics\n```", "```py\nPATH = \"data/bulldozers/\"\n\ndf_raw = pd.read_feather('tmp/bulldozers-raw')\ndf_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')**def** split_vals(a,n): **return** a[:n], a[n:]\nn_valid = 12000\nn_trn = len(df_trn)-n_valid\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)\nraw_train, raw_valid = split_vals(df_raw, n_trn)x_sub = X_train[['YearMade', 'MachineHoursCurrentMeter']]\n```", "```py\n**class** **TreeEnsemble**():\n    **def** __init__(self, x, y, n_trees, sample_sz, min_leaf=5):\n        np.random.seed(42)\n        self.x,self.y,self.sample_sz,self.min_leaf = \n                  x,y,sample_sz,min_leaf\n        self.trees = [self.create_tree() **for** i **in** range(n_trees)]\n\n    **def** create_tree(self):\n        rnd_idxs = np.random.permutation(len(self.y))\n                    [:self.sample_sz]\n        **return** DecisionTree(self.x.iloc[rnd_idxs], self.y[rnd_idxs],\n                   min_leaf=self.min_leaf)\n\n    **def** predict(self, x):\n        **return** np.mean([t.predict(x) **for** t **in** self.trees], axis=0)\n```", "```py\n**class** **DecisionTree**():\n   **def** __init__(self, x, y, idxs=**None**, min_leaf=5):\n      **if** idxs **is** **None**: idxs=np.arange(len(y))\n      self.x,self.y,self.idxs,self.min_leaf = x,y,idxs,min_leaf\n      self.n,self.c = len(idxs), x.shape[1]\n      self.val = np.mean(y[idxs])\n      self.score = float('inf')\n      self.find_varsplit()\n\n   *# This just does one decision; we'll make it recursive later*\n   **def** find_varsplit(self):\n      **for** i **in** range(self.c): self.find_better_split(i)\n\n   *# We'll write this later!*\n   **def** find_better_split(self, var_idx): **pass**\n\n   @property\n   **def** split_name(self): **return** self.x.columns[self.var_idx]\n\n   @property\n   **def** split_col(self): \n      **return** self.x.values[self.idxs,self.var_idx] @property\n   **def** is_leaf(self): **return** self.score == float('inf')\n\n   **def** __repr__(self):\n      s = f'n: **{self.n}**; val:**{self.val}**'\n      **if** **not** self.is_leaf:\n         s += f'; score:**{self.score}**; split:**{self.split}**; var:**{self.split_name}**'\n        **return** s\n```"]