- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:50:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:50:15
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2111.00856] Large-Scale Deep Learning Optimizations: A Comprehensive Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2111.00856] 大规模深度学习优化：全面调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2111.00856](https://ar5iv.labs.arxiv.org/html/2111.00856)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2111.00856](https://ar5iv.labs.arxiv.org/html/2111.00856)
- en: 'Large-Scale Deep Learning Optimizations: A Comprehensive Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模深度学习优化：全面调查
- en: Xiaoxin He [he.xiaoxin@u.nus.edu](mailto:he.xiaoxin@u.nus.edu) National University
    of Singaporehe.xiaoxin@u.nus.edu ,  Fuzhao Xue [f.xue@u.nus.edu](mailto:f.xue@u.nus.edu)
    National University of Singaporef.xue@u.nus.edu ,  Xiaozhe Ren [renxiaozhe@huawei.com](mailto:renxiaozhe@huawei.com)
    Huawei Noah’s Ark Labrenxiaozhe@huawei.com  and  Yang You [youy@comp.nus.edu.sg](mailto:youy@comp.nus.edu.sg)
    National University of Singaporeyouy@comp.nus.edu.sg
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xiaoxin He [he.xiaoxin@u.nus.edu](mailto:he.xiaoxin@u.nus.edu) 新加坡国立大学he.xiaoxin@u.nus.edu
    ， Fuzhao Xue [f.xue@u.nus.edu](mailto:f.xue@u.nus.edu) 新加坡国立大学f.xue@u.nus.edu
    ， Xiaozhe Ren [renxiaozhe@huawei.com](mailto:renxiaozhe@huawei.com) 华为诺亚方舟实验室renxiaozhe@huawei.com
    和 Yang You [youy@comp.nus.edu.sg](mailto:youy@comp.nus.edu.sg) 新加坡国立大学youy@comp.nus.edu.sg
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Deep learning have achieved promising results on a wide spectrum of AI applications.
    Larger datasets and models consistently yield better performance. However, we
    generally spend longer training time on more computation and communication. In
    this survey, we aim to provide a clear sketch about the optimizations for large-scale
    deep learning with regard to the model accuracy and model efficiency. We investigate
    algorithms that are most commonly used for optimizing, elaborate the debatable
    topic of generalization gap arises in large-batch training, and review the SOTA
    strategies in addressing the communication overhead and reducing the memory footprints.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在广泛的人工智能应用中取得了令人鼓舞的成果。更大的数据集和模型通常能带来更好的性能。然而，我们通常需要花费更长的训练时间来进行更多的计算和通信。在这项调查中，我们旨在提供一个关于大规模深度学习优化的清晰概述，涉及模型准确性和模型效率。我们研究了最常用于优化的算法，详细探讨了大批量训练中出现的泛化差距这一有争议的话题，并回顾了应对通信开销和减少内存占用的最先进策略。
- en: 'Deep Learning, Deep Neural Networks, Optimization, Distributed Learning, Large
    Batch Training, Communication-Efficient, Memory-Efficient, Survey^†^†copyright:
    none'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，深度神经网络，优化，分布式学习，大批量训练，通信效率，内存效率，调查^†^†版权：无
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: Nowadays, deep learning (DL) have achieved promising results on a wide spectrum
    of AI application domains ranging from computer vision (*e.g.,* image classification (He
    et al., [2016](#bib.bib43); Huang et al., [2017](#bib.bib48); Lou et al., [2021](#bib.bib67)),
    object detection and segmentation (Girshick, [2015](#bib.bib36); Ren et al., [2015](#bib.bib86);
    He et al., [2020](#bib.bib42); Long et al., [2015](#bib.bib66))), natural language
    processing (*e.g.,* language modeling (Devlin et al., [2019](#bib.bib29); Xue
    et al., [2021](#bib.bib118)) and machine translation (Vaswani et al., [2017](#bib.bib109);
    Wu et al., [2016](#bib.bib115))), information retrieval (*e.g.,* recommendation
    system (He et al., [2017](#bib.bib44))) and many others. The scale is the main
    driver behind the rise of DL (Krizhevsky et al., [2012](#bib.bib56); He et al.,
    [2016](#bib.bib43); Simonyan and Zisserman, [2014](#bib.bib95); Krizhevsky et al.,
    [2017](#bib.bib57); Szegedy et al., [2015](#bib.bib105); Devlin et al., [2019](#bib.bib29)).
    Larger datasets and neural networks consistently yield better performance across
    all tasks that generally require more computation and longer training time. Therefore,
    recent years have witnessed a surge of interests from both academia and industry
    in scaling up DL with distributed training on a large cluster of devices such
    as TPUs and GPUs with higher computation capability and memory limit. Data parallelism
    has become a dominant practice for distributed training. It distributes a large
    batch to multiple devices, where each device holds an identical model replica,
    computes the gradient of a local batch and finally gathers the gradients at each
    iteration for synchronous parameter update. With recent optimization techniques,
    it is now able to train very large batches on thousands of GPU devices. However,
    training at such scales requires overcoming both algorithmic and systems-related
    challenges. One of the main challenges is the degradation of model accuracy with
    large batch size beyond a certain point (e.g., 32k). Naively increasing the batch
    size typically results in degradation of generalization performance and reduces
    computational benefits. Additionally, we can not always improve the training speed
    by just using more processors as the communication cost is a non-negligible overhead.
    Intuitively multiple processors collaboratively training one task can reduce the
    overall training time, but the corresponding communication cost between processors
    is heavy and limits the model scalibility. Worse still, models with tens of billions
    to trillions of parameters clearly do not fit into memory of a single device,
    and simply adding more devices will not help scale the training. This limitation
    prevents DL researchers from exploring more advanced model architectures. Existing
    works investigate and develop optimization techniques to overcome these problems
    so as to accelerate training large-scale deep neural networks (DNNs). We categorise
    these works into two categories, one endeavors to maintain/improve the model accuracy
    in the large-scale setting and the other emphasises on the model efficiency, designing
    algorithms that are less hungry for communication and memory. Importantly, they
    are not mutually exclusive but can be used collaboratively to further speed up
    the training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，深度学习（DL）在计算机视觉（*例如，* 图像分类（He et al., [2016](#bib.bib43); Huang et al., [2017](#bib.bib48);
    Lou et al., [2021](#bib.bib67)）、目标检测和分割（Girshick, [2015](#bib.bib36); Ren et al.,
    [2015](#bib.bib86); He et al., [2020](#bib.bib42); Long et al., [2015](#bib.bib66)））、自然语言处理（*例如，*
    语言建模（Devlin et al., [2019](#bib.bib29); Xue et al., [2021](#bib.bib118)）和机器翻译（Vaswani
    et al., [2017](#bib.bib109); Wu et al., [2016](#bib.bib115)））、信息检索（*例如，* 推荐系统（He
    et al., [2017](#bib.bib44)））等广泛的人工智能应用领域中取得了令人鼓舞的成果。规模是推动深度学习崛起的主要因素（Krizhevsky
    et al., [2012](#bib.bib56); He et al., [2016](#bib.bib43); Simonyan and Zisserman,
    [2014](#bib.bib95); Krizhevsky et al., [2017](#bib.bib57); Szegedy et al., [2015](#bib.bib105);
    Devlin et al., [2019](#bib.bib29)）。更大的数据集和神经网络通常能在所有任务中取得更好的性能，这些任务一般需要更多的计算和更长的训练时间。因此，近年来，学术界和工业界对在具有更高计算能力和内存限制的设备集群上进行分布式训练以扩展深度学习的兴趣激增。数据并行已经成为分布式训练的主要做法。它将一个大的批次分配到多个设备上，每个设备持有一个相同的模型副本，计算本地批次的梯度，然后在每次迭代时收集梯度进行同步参数更新。借助于最新的优化技术，现在能够在成千上万的GPU设备上训练非常大的批次。然而，在这种规模下训练需要克服算法和系统相关的挑战。一个主要挑战是当批量大小超过某一点（*例如，*
    32k）时模型准确性下降。简单地增加批量大小通常会导致泛化性能下降，并减少计算收益。此外，我们不能仅仅通过使用更多处理器来提高训练速度，因为通信成本是一个不可忽视的开销。从直观上看，多个处理器协同训练一个任务可以减少整体训练时间，但处理器之间的通信成本较高，并限制了模型的可扩展性。更糟糕的是，具有数十亿到万亿参数的模型显然无法适应单个设备的内存，简单地增加更多设备并不会帮助扩展训练。这一限制阻碍了深度学习研究人员探索更先进的模型架构。现有的研究致力于开发优化技术以克服这些问题，从而加速大规模深度神经网络（DNNs）的训练。我们将这些研究分为两类，一类致力于在大规模设置中保持/提高模型准确性，另一类则强调模型效率，设计对通信和内存需求较少的算法。重要的是，这两类技术并不是互相排斥的，而是可以协同使用以进一步加快训练速度。
- en: 1.1\. Related Surveys
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 相关调查
- en: 'Pouyanfar et al. ([2019](#bib.bib80)) give an overview of DL from different
    perspectives, including history, challenges, opportunities, algorithms, frameworks,
    applications, and parallel and distributed computing techniques.  Wang et al.
    ([2020](#bib.bib111)) provide a quick survey on large-scale distributed deep learning
    systems, which concisely introduces parallelisms, parameter server architectures,
    synchronization schemes, related applications, and platforms. While some other
    surveys focus on a certain scope in deep learning: communication-efficiency in
    large-scale parallelism systems  (Betzel et al., [2018](#bib.bib15); Tang et al.,
    [2020](#bib.bib107)), parallelization strategies (Ben-Nun and Hoefler, [2019](#bib.bib13))
    and numerical optimization algorithms (Sun, [2019](#bib.bib102); Bottou et al.,
    [2018](#bib.bib20); Battiti, [1992](#bib.bib12); Ruder, [2016](#bib.bib88)). Large-scale
    DL represents a distinctive setting in which the accuracy, computation, communication
    and memory are closely connected and mutually restricted. However, existing surveys
    either merely concern part of them or do not address optimizations in the context
    of large-scale DL. Different from the two surveys (Wang et al., [2020](#bib.bib111);
    Pouyanfar et al., [2019](#bib.bib80)) that are mostly related to ours, we focus
    more on the design of the algorithm rather than the system architecture. The novelty
    of this paper is its emphasises on both model accuracy and model efficiency, which
    captures critical aspects of large-scale deep learning training by presenting
    a review of the state-of-the-art (SOTA) optimization techniques and illustrating
    the trade-off in between.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Pouyanfar 等人 ([2019](#bib.bib80)) 从不同角度概述了深度学习（DL），包括历史、挑战、机遇、算法、框架、应用以及并行和分布式计算技术。
    Wang 等人 ([2020](#bib.bib111)) 提供了对大规模分布式深度学习系统的快速调查，简明扼要地介绍了并行性、参数服务器架构、同步方案、相关应用和平台。
    尽管一些其他调查专注于深度学习中的某些范围：大规模并行系统中的通信效率 (Betzel 等人, [2018](#bib.bib15); Tang 等人, [2020](#bib.bib107))，并行化策略
    (Ben-Nun 和 Hoefler, [2019](#bib.bib13)) 以及数值优化算法 (Sun, [2019](#bib.bib102); Bottou
    等人, [2018](#bib.bib20); Battiti, [1992](#bib.bib12); Ruder, [2016](#bib.bib88))。
    大规模深度学习代表了一个独特的设置，在该设置中，准确性、计算、通信和内存密切相关且相互制约。 然而，现有调查要么仅关注其中的一部分，要么未在大规模深度学习的背景下解决优化问题。
    与两个主要相关的调查 (Wang 等人, [2020](#bib.bib111); Pouyanfar 等人, [2019](#bib.bib80)) 不同，我们更加关注算法的设计，而非系统架构。
    本文的创新在于强调模型准确性和模型效率，通过呈现最先进的（SOTA）优化技术的综述并说明两者之间的权衡，捕捉了大规模深度学习训练中的关键方面。
- en: 1.2\. Structure of the Survey
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 调查结构
- en: '![Refer to caption](img/87c1f0793307bb5eb8524686c1093340.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/87c1f0793307bb5eb8524686c1093340.png)'
- en: Figure 1\. The Overall Structure of this Survey
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 本调查的整体结构
- en: 'The overall structure of this survey is presented in Figure [1](#S1.F1 "Figure
    1 ‣ 1.2\. Structure of the Survey ‣ 1\. Introduction ‣ Large-Scale Deep Learning
    Optimizations: A Comprehensive Survey"). Section  [2](#S2 "2\. Preliminaries ‣
    Large-Scale Deep Learning Optimizations: A Comprehensive Survey") presents the
    formulation of a typical neural network optimization problem for supervised learning.
    We roughly divide the large-scale DL optimization into two components: model accuracy
    and model efficiency. Section  [3](#S3 "3\. Gradient Descent Optimization Algorithms
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey") introduces
    the gradient descent optimization family, including gradient descent variants,
    momentum SGD and adaptive gradient algorithms. As large batch training with data
    parallelism has increasing popularity in DL and meanwhile introduces challenges,
    Section  [4](#S4 "4\. Large Batch Training ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey") discusses problems in this setting, and reviews main
    SOTA optimization strategies to improve the situation. Section  [5](#S5 "5\. Generalization
    Gap ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey") dives
    into the generalization gap — a debating topic in large batch training. Section
     [6](#S6 "6\. Second Order Optimization ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey") introduces second-order optimizations. Then we turn our
    attention to model efficiency. Section  [7](#S7 "7\. Communication ‣ Large-Scale
    Deep Learning Optimizations: A Comprehensive Survey") investigates the communication
    bottleneck and Section  [8](#S8 "8\. Memory ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey") focuses on memory-efficient techniques. Finally, Section
     [9](#S9 "9\. Conclusion ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey") concludes this article.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查的整体结构如图[1](#S1.F1 "Figure 1 ‣ 1.2\. Structure of the Survey ‣ 1\. Introduction
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey")所示。第[2](#S2
    "2\. Preliminaries ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey")节介绍了一个典型神经网络优化问题的公式。我们大致将大规模DL优化分为两个部分：模型准确性和模型效率。第[3](#S3 "3\. Gradient
    Descent Optimization Algorithms ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey")节介绍了梯度下降优化家族，包括梯度下降变体、动量SGD和自适应梯度算法。由于大批量训练在DL中的数据并行性日益流行，同时带来了挑战，第[4](#S4
    "4\. Large Batch Training ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey")节讨论了这一设置中的问题，并回顾了主要的SOTA优化策略以改善情况。第[5](#S5 "5\. Generalization Gap ‣ Large-Scale
    Deep Learning Optimizations: A Comprehensive Survey")节深入探讨了泛化差距——一个在大批量训练中的争论话题。第[6](#S6
    "6\. Second Order Optimization ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey")节介绍了二阶优化。然后我们将注意力转向模型效率。第[7](#S7 "7\. Communication ‣ Large-Scale Deep
    Learning Optimizations: A Comprehensive Survey")节调查了通信瓶颈，第[8](#S8 "8\. Memory
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey")节聚焦于内存高效技术。最后，第[9](#S9
    "9\. Conclusion ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey")节总结了本文。'
- en: 2\. Preliminaries
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 初步介绍
- en: Following many machine learning applications (Bottou et al., [2018](#bib.bib20);
    Xu et al., [2020](#bib.bib117); Bottou and Bousquet, [2007](#bib.bib19)), we consider
    a space of input-output pairs $(x,y)\in X\times Y$ has a probability distribution
    $P(x,y)$. The conditional distribution $P(y|x)$ represents the true relationship
    between inputs and outputs. The discrepancy between the predicted output $\hat{y}$
    and the real output $y$ is measured by a smooth but possibly non-convex loss function
    $\mathscr{l}(\hat{y},y)$. The objective is to minimize the expected risk
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参考许多机器学习应用（Bottou等，[2018](#bib.bib20)；Xu等，[2020](#bib.bib117)；Bottou和Bousquet，[2007](#bib.bib19)），我们考虑一个输入-输出对$(x,y)\in
    X\times Y$具有概率分布$P(x,y)$。条件分布$P(y|x)$表示输入和输出之间的真实关系。预测输出$\hat{y}$与实际输出$y$之间的差异由一个平滑但可能是非凸的损失函数$\mathscr{l}(\hat{y},y)$来衡量。目标是最小化预期风险。
- en: '| (1) |  | $E(f)=\int\mathscr{l}(f(x),y)P(x,y)=\mathbb{E}[\mathscr{l}(f(x),y)],$
    |  |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $E(f)=\int\mathscr{l}(f(x),y)P(x,y)=\mathbb{E}[\mathscr{l}(f(x),y)],$
    |  |'
- en: that is,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 即，
- en: '| (2) |  | $f^{*}(x)=\mathop{argmin}_{\hat{y}}\mathbb{E}[\mathscr{l}(\hat{y},y)&#124;x].$
    |  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $f^{*}(x)=\mathop{argmin}_{\hat{y}}\mathbb{E}[\mathscr{l}(\hat{y},y)&#124;x].$
    |  |'
- en: Since $\mathbb{P}$ is an unknown distribution, in practice, one seeks the solution
    of a problem that involves an estimate of the empirical risk (Shalev-Shwartz and
    Ben-David, [2014](#bib.bib92))
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于$\mathbb{P}$是一个未知的分布，实际上，人们会寻求一个涉及经验风险估计的问题的解决方案（Shalev-Shwartz和Ben-David，[2014](#bib.bib92)）。
- en: '| (3) |  | $E_{n}(f)=\frac{1}{n}\sum_{i=1}^{n}\mathscr{l}(f(x_{i}),y_{i})=\mathbb{E}_{n}[\mathscr{l}(f(x),y].$
    |  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $E_{n}(f)=\frac{1}{n}\sum_{i=1}^{n}\mathscr{l}(f(x_{i}),y_{i})=\mathbb{E}_{n}[\mathscr{l}(f(x),y].$
    |  |'
- en: 'The goal of solving Eq.[[3](#S2.E3 "In 2\. Preliminaries ‣ Large-Scale Deep
    Learning Optimizations: A Comprehensive Survey")] is to obtain a solution with
    small generalization error, i.e., high predictive accuracy on unseen data.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '解决 Eq.[[3](#S2.E3 "In 2\. Preliminaries ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey")] 的目标是获得一个具有较小泛化误差的解决方案，即对未见数据的高预测准确性。'
- en: 3\. Gradient Descent Optimization Algorithms
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 梯度下降优化算法
- en: 'Training a DNN is an optimization process, i.e., finding the parameters in
    the network that minimize the loss function. Gradient descent and its variant
    algorithms are the most popular algorithms to optimize neural networks (NNs) (Ruder,
    [2016](#bib.bib88)). In order to control the oscillation of gradient descent methods,
    the idea of using momentum is introduced. Moreover, adapting the learning rate
    w.r.t. the gradient of the previous stages is found beneficial to avoid the fluctuation.
    In this section, we briefly sort out the mainstream optimization algorithms, consisting
    of gradient descent variants (Section [3.1](#S3.SS1 "3.1\. Gradient Descent Variants
    ‣ 3\. Gradient Descent Optimization Algorithms ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey")), momentum (Section [3.2](#S3.SS2 "3.2\. Momentum ‣ 3\.
    Gradient Descent Optimization Algorithms ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey")) and adaptive gradient algorithms (Section [3.3](#S3.SS3
    "3.3\. Adaptive Gradient Algorithms ‣ 3\. Gradient Descent Optimization Algorithms
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey")).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '训练一个深度神经网络（DNN）是一个优化过程，即找到使损失函数最小化的网络参数。梯度下降及其变体算法是优化神经网络（NNs）的最流行算法（Ruder，[2016](#bib.bib88)）。为了控制梯度下降方法的振荡，引入了使用动量的思想。此外，相对于前期梯度调整学习率被发现对避免波动有帮助。在这一节中，我们简要梳理了主流优化算法，包括梯度下降变体（第[3.1](#S3.SS1
    "3.1\. Gradient Descent Variants ‣ 3\. Gradient Descent Optimization Algorithms
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey")节），动量（第[3.2](#S3.SS2
    "3.2\. Momentum ‣ 3\. Gradient Descent Optimization Algorithms ‣ Large-Scale Deep
    Learning Optimizations: A Comprehensive Survey")节）和自适应梯度算法（第[3.3](#S3.SS3 "3.3\.
    Adaptive Gradient Algorithms ‣ 3\. Gradient Descent Optimization Algorithms ‣
    Large-Scale Deep Learning Optimizations: A Comprehensive Survey")节）。'
- en: 3.1\. Gradient Descent Variants
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 梯度下降变体
- en: Gradient descent and its variants dominate the optimization algorithms of DL.
    The gradient descent (GD) methods aim to minimize the empirical risk of a model
    by repeatedly computing the gradient of a loss function on a single training sample,
    or a (full) batch of samples, and continuously updating the model parameters accordingly
    by following the gradient of the objective function in the opposite direction.
    There are three variants in gradient descent which differ in the number of samples
    used for each step (updating model parameters), resulting in different accuracy
    and learning time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降及其变体主导了深度学习的优化算法。梯度下降（GD）方法旨在通过反复计算单个训练样本或一个（完整）样本批次上的损失函数的梯度，并根据目标函数的梯度方向持续更新模型参数，从而最小化模型的经验风险。梯度下降有三种变体，这些变体在每一步中使用的样本数量不同，导致不同的准确性和学习时间。
- en: 3.1.1\. Batch Gradient Descent
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 批量梯度下降
- en: 'Batch gradient descent, a.k.a. vanilla gradient descent, minimizes the loss
    function $L(x)$ with the following form:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降，即标准梯度下降，利用以下形式来最小化损失函数 $L(x)$：
- en: '| (4) |  | $L(x)=\frac{1}{&#124;S&#124;}\sum_{s\in S}l(x,s).$ |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $L(x)=\frac{1}{&#124;S&#124;}\sum_{s\in S}l(x,s).$ |  |'
- en: Here $x$ is the weights of a network, $S$ is a labeled training set, $|S|$ is
    the number of samples in the training set, and $l(x,s)$ is the loss computed from
    sample $s\in S$ and and its label $y$. Typically $l$ is the sum of a classification
    loss (e.g., cross-entropy) and a regularization loss on $x$. And then update the
    weights
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $x$ 是网络的权重，$S$ 是一个标记的训练集，$|S|$ 是训练集中的样本数量，$l(x,s)$ 是从样本 $s\in S$ 及其标签 $y$
    计算的损失。通常，$l$ 是分类损失（例如，交叉熵）和 $x$ 上的正则化损失的总和。然后更新权重
- en: '| (5) |  | $x=x-\eta\nabla L(x),$ |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $x=x-\eta\nabla L(x),$ |  |'
- en: where $\eta$ is a learning rate (LR) which controls how large of a step to take
    in the opposite direction of the gradient. As we need to go through the whole
    training set to calculate the gradient for one update of weights, batch gradient
    descent can be very slow, especially for large datasets (which is very common
    in DL tasks). Batch gradient descent also does not allow updating model online,
    i.e., with new examples on-the-fly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\eta$ 是学习率（LR），它控制沿梯度反方向走多大的步伐。由于我们需要遍历整个训练集来计算一次权重更新的梯度，批量梯度下降可能非常缓慢，尤其是对于大型数据集（这在深度学习任务中很常见）。批量梯度下降也不允许在线更新模型，即随时用新样本更新。
- en: 3.1.2\. Stochastic Gradient Descent
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 随机梯度下降
- en: Unlike batch gradient descent (GD), which calculates the gradients using the
    all training samples, Stochastic Gradient Descent (SGD) performs one weights update
    for each training sample
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用所有训练样本计算梯度的批量梯度下降（GD）不同，随机梯度下降（SGD）对每个训练样本执行一次权重更新。
- en: '| (6) |  | $x=x-\eta\nabla l(x,s).$ |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $x=x-\eta\nabla l(x,s).$ |  |'
- en: Therefore, SGD addresses the computational bottleneck of batch gradient descent.
    It is significantly faster than batch gradient descent and can be used online.
    The drawback is that the gradient computed from just one sample is not representative
    enough for the whole training set. Consequently, the variance of gradients leads
    to a fierce fluctuation in the loss function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SGD 解决了批量梯度下降的计算瓶颈。它比批量梯度下降显著更快，并且可以在线使用。缺点是，仅用一个样本计算的梯度对整个训练集来说代表性不足。因此，梯度的方差导致了损失函数的剧烈波动。
- en: 3.1.3\. Mini-batch Stochastic Gradient Descent
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 小批量随机梯度下降
- en: Mini-batch SGD takes both advantages of batch GD and SGD by performing weights
    update for each mini-batch $B$
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量 SGD 结合了批量 GD 和 SGD 的优点，通过对每个小批量 $B$ 执行权重更新。
- en: '| (7) |  | $x=x-\frac{\eta}{&#124;B&#124;}\nabla\sum_{s\in B}l(x,s).$ |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $x=x-\frac{\eta}{&#124;B&#124;}\nabla\sum_{s\in B}l(x,s).$ |  |'
- en: In batch GD, the gradients are computed over the entire dataset, providing an
    accurate estimation of the true gradient. It takes lots of time and memory to
    do that. But the real handicap is the batch gradient trajectory lands in a bad
    spot. While in SGD, parameters are updated by adding the gradient computed on
    a single sample of the dataset, which is very noisy and may go off in a direction
    far from the batch gradient. However, the noisiness is exactly what we want in
    non-convex optimization, because it helps to escape from saddle points or local
    minima (Ge et al., [2015](#bib.bib35)). The disadvantage is its terribly inefficiency
    of looping over the entire dataset many times to find a good solution. The mini-batch
    methodology is a compromise that injects enough noise to each gradient update,
    while achieving a relative speedy convergence. Mini-batch SGD is found to be very
    effective in the case of large-scale learning (Bottou and Bousquet, [2007](#bib.bib19)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在批量 GD 中，梯度是对整个数据集计算的，提供了对真实梯度的准确估计。这样做需要大量时间和内存。但真正的问题在于批量梯度轨迹可能会落在一个不好的位置。而在
    SGD 中，参数是通过添加数据集中单个样本计算的梯度来更新的，这些梯度非常嘈杂，可能会朝着与批量梯度完全不同的方向前进。然而，这种噪声正是我们在非凸优化中所需要的，因为它有助于逃脱鞍点或局部最小值（Ge
    等人，[2015](#bib.bib35)）。缺点是需要多次遍历整个数据集来找到一个好的解决方案。小批量方法是一种折中方案，它在每次梯度更新时注入足够的噪声，同时实现相对较快的收敛。小批量
    SGD 在大规模学习中被发现非常有效（Bottou 和 Bousquet，[2007](#bib.bib19)）。
- en: 3.2\. Momentum
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 动量
- en: SGD has trouble navigating a long and narrow valley in the loss function surface,
    i.e., the direction of the gradient is almost perpendicular to the long axis of
    the valley. In such a situation, the system oscillates back and forth in the direction
    of the short axis, and only moves very slowly along the long axis of the valley.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 在损失函数表面上长而狭窄的谷底中导航时会遇到困难，即梯度的方向几乎与谷底的长轴垂直。在这种情况下，系统在短轴方向上来回振荡，只沿着谷底的长轴非常缓慢地移动。
- en: 3.2.1\. Momentum SGD
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 动量 SGD
- en: 'Momentum (Qian, [1999](#bib.bib81)) is a strategy that helps to counteract
    the oscillation along the short axis meanwhile accumulate contributions along
    the long axis. In other words, the momentum strengthens for dimensions whose gradients
    point in the same directions and dampens updates for dimensions whose gradients
    change directions. This allows Momentum to minimize the training loss in fewer
    steps than full batch gradient descent (Park et al., [2019](#bib.bib77)). Specifically,
    momentum SGD adds update in previous step to the current update, and determines
    the next update $v_{t}$ as a linear combination of the gradient and the previous
    update $v_{t-1}$:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 动量 (Qian, [1999](#bib.bib81)) 是一种有助于抵消短轴振荡，同时在长轴上积累贡献的策略。换句话说，动量会加强梯度方向相同的维度的更新，并减缓梯度方向改变的维度的更新。这使得动量在比全批量梯度下降 (Park
    等人, [2019](#bib.bib77)) 更少的步骤中最小化训练损失。具体而言，动量 SGD 在当前更新中添加了前一步的更新，并将下一步更新 $v_{t}$
    确定为梯度和前一步更新 $v_{t-1}$ 的线性组合：
- en: '| (8) |  | $\begin{split}v_{t}&amp;=\beta v_{t-1}+\eta\nabla L(x)\\ x&amp;=x-v_{t}.\end{split}$
    |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\begin{split}v_{t}&amp;=\beta v_{t-1}+\eta\nabla L(x)\\ x&amp;=x-v_{t}.\end{split}$
    |  |'
- en: 3.2.2\. Nesterov Accelerated Gradient
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. Nesterov 加速梯度
- en: 'In Eq.[[8](#S3.E8 "In 3.2.1\. Momentum SGD ‣ 3.2\. Momentum ‣ 3\. Gradient
    Descent Optimization Algorithms ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey")], we know that we are going to move by at least $\beta v_{t-1}$ and a
    bit more by $\eta\nabla L(x)$. And in Nesterov Accelerated Gradient (NAG) (Nesterov,
    [1983](#bib.bib75)), it looks ahead by calculating the gradient at the partially
    updated value of $(x-\beta v_{t-1})$ instead of using the current value:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Eq.[[8](#S3.E8 "在 3.2.1\. 动量 SGD ‣ 3.2\. 动量 ‣ 3\. 梯度下降优化算法 ‣ 大规模深度学习优化：综合调查")],
    我们知道我们将至少移动 $\beta v_{t-1}$，以及额外的 $\eta\nabla L(x)$。在 Nesterov 加速梯度（NAG） (Nesterov,
    [1983](#bib.bib75)) 中，它通过计算 $(x-\beta v_{t-1})$ 的部分更新值上的梯度来前瞻，而不是使用当前值：
- en: '| (9) |  | $\begin{split}v_{t}&amp;=\beta v_{t-1}+\eta\nabla L(x-\beta v_{t-1})\\
    x&amp;=x-v_{t}.\end{split}$ |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $\begin{split}v_{t}&amp;=\beta v_{t-1}+\eta\nabla L(x-\beta v_{t-1})\\
    x&amp;=x-v_{t}.\end{split}$ |  |'
- en: Such ”look before you leap” prevents us from going too fast and results in increased
    responsiveness. While the optimization path taken by classical momentum SGD exhibits
    large oscillations along the high-curvature vertical direction, NAG is able to
    avoid these oscillations almost entirely (Sutskever et al., [2013a](#bib.bib103)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“未雨绸缪”防止我们过快前进，从而提高了响应速度。尽管经典动量 SGD 的优化路径在高曲率垂直方向上表现出较大的振荡，但 NAG 几乎可以完全避免这些振荡 (Sutskever
    等人, [2013a](#bib.bib103))。
- en: 3.3\. Adaptive Gradient Algorithms
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 自适应梯度算法
- en: The methods mentioned above apply a same LR to all dimensions of the parameters.
    Since each dimension of parameters relates to the loss function in different ways,
    a per-dimension LR is more advantageous due to the more accurate and precise control
    on the step size. Therefore, a variety of adaptive gradient-based methods have
    been proposed where gradients are divided by the component-wise accumulation of
    previous gradients. For example, AdaGrad (Duchi et al., [2011](#bib.bib33)) uses
    the sum of the squares of all past gradients, whereas Adadelta (Zeiler, [2012](#bib.bib123)),
    RMSProp (Tieleman and Hinton, [2012](#bib.bib108)) and Adam (Kingma and Ba, [2017](#bib.bib54))
    use an exponentially decaying average.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法对所有参数维度应用相同的学习率（LR）。由于每个参数维度与损失函数的关系不同，每维学习率由于对步长的更准确和精确的控制而更具优势。因此，已经提出了多种自适应梯度方法，其中梯度通过之前梯度的逐成分累积进行划分。例如，AdaGrad (Duchi
    等人, [2011](#bib.bib33)) 使用了所有过去梯度的平方和，而 Adadelta (Zeiler, [2012](#bib.bib123))、RMSProp (Tieleman
    和 Hinton, [2012](#bib.bib108)) 和 Adam (Kingma 和 Ba, [2017](#bib.bib54)) 则使用了指数衰减的平均值。
- en: 3.3.1\. AdaGrad
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. AdaGrad
- en: Previously, we perform updates for all parameters using a same LR, regardless
    of their frequency and magnitude. This may lead to a failure in capturing the
    knowledge of infrequently occurring updates (which are highly informative and
    discriminative). AdaGrad (Duchi et al., [2011](#bib.bib33)) alleviates this problem
    by performing larger updates for infrequent parameters and smaller updates for
    frequent parameters, which enables it to do well with sparse gradients
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们对所有参数使用相同的学习率进行更新，而不考虑其频率和幅度。这可能导致无法捕捉到发生频率较低的更新（这些更新信息丰富且具有辨别力）。AdaGrad (Duchi
    等人, [2011](#bib.bib33)) 通过对不频繁的参数执行较大的更新和对频繁的参数执行较小的更新来缓解这个问题，这使得它在稀疏梯度下表现良好。
- en: '| (10) |  | $x_{t,i}=x_{t-1,i}-\frac{\eta}{\sqrt{G_{t,ii}+\epsilon}}\nabla
    L(x_{t,i}).$ |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $x_{t,i}=x_{t-1,i}-\frac{\eta}{\sqrt{G_{t,ii}+\epsilon}}\nabla
    L(x_{t,i}).$ |  |'
- en: '$G_{t}=\sum_{\tau=1}^{T}g_{\tau}g_{\tau}^{T}$ is a diagonal matrix where each
    diagonal element $G_{t,ii}$ is the sum of the squares of all past gradient w.r.t.
    $x_{i}$ up to time step $t$. And $\epsilon$ is a smoothing term to avoid division
    by zero. A vectorized implementation has the following form:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: $G_{t}=\sum_{\tau=1}^{T}g_{\tau}g_{\tau}^{T}$是对角矩阵，其中每个对角元素$G_{t,ii}$是到时间步$t$为止与参数$x_{i}$相关的所有过去梯度的平方和。而$\epsilon$是用于避免除零的平滑项。向量化实现具有以下形式：
- en: '| (11) |  | $x_{t}=x_{t-1}-\frac{\eta}{\sqrt{G_{t}+\epsilon}}\odot\nabla L(x_{t}),$
    |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $x_{t}=x_{t-1}-\frac{\eta}{\sqrt{G_{t}+\epsilon}}\odot\nabla L(x_{t}),$
    |  |'
- en: where $\odot$ is an element-wise matrix-vector multiplication. In AdaGrad, each
    dimension has its own dynamic LR rate which is inversely dependent to the gradient
    magnitude, thus larger gradients have smaller LRs and small gradients have larger
    LRs. This is very beneficial for training DNNs since the scale of gradients in
    each layer is often different by several orders of magnitude. In addition, this
    accumulation of gradients can be regarded as a kind of simulated annealing which
    reduces the LRs along the course of training. Most implementation set the LR $\eta$
    to a default value of 0.01, eliminating the need of manual tuning. However, AdaGrad
    holds a main drawback with the accumulation of squares of all past gradients,
    which keeps growing during the course of training. As the LRs radically shrink
    and vanish, the algorithm no longer gain additional knowledge.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\odot$是逐元素的矩阵-向量乘法。在AdaGrad中，每个维度都有自己的动态学习率，与梯度大小成反比，因此较大的梯度具有较小的学习率，而较小的梯度具有较大的学习率。这对训练深度神经网络非常有益，因为每个层的梯度尺度通常相差几个数量级。此外，这种梯度累积还可以看作是一种模拟退火，它在训练过程中降低了学习率。大多数实现将学习率$\eta$设置为默认值0.01，消除了手动调整的需要。然而，AdaGrad在训练过程中积累了所有过去梯度的平方，这在训练过程中不断增加。随着学习率急剧缩小和消失，算法不再获得额外的知识。
- en: 3.3.2\. Adadelta
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. Adadelta
- en: Adadelta (Zeiler, [2012](#bib.bib123)) is an extension of AdaGrad (Duchi et al.,
    [2011](#bib.bib33)) that seeks to tackle its monotonically decreasing LRs. Instead
    of accumulating the sum of all previous squared gradients, Adadelta uses an exponentially
    decaying average instead. The running average $E[g^{2}]_{t}$ at time step $t$
    depends on the previous average and the current gradient
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Adadelta（Zeiler，[2012](#bib.bib123)）是AdaGrad的扩展（Duchi等人，[2011](#bib.bib33)），旨在解决其单调递减的学习率。Adadelta不是累积所有先前梯度的和，而是使用指数衰减的平均值。时间步$t$的运行平均值$E[g^{2}]_{t}$取决于先前的平均值和当前梯度
- en: '| (12) |  | $E[g^{2}]_{t}=\rho E[g^{2}]_{t-1}+(1-\rho)g_{t}^{2},$ |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $E[g^{2}]_{t}=\rho E[g^{2}]_{t-1}+(1-\rho)g_{t}^{2},$ |  |'
- en: 'where $\rho$ is a decay constant similar to that used in the momentum method.
    As the denominator is the root mean squared (RMS) error criterion of the gradient,
    we can replace it with the criterion short-hand:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\rho$是类似于动量法中使用的衰减常数。因为分母是梯度的均方根（RMS）误差标准，我们可以用标准简写形式替换它：
- en: '| (13) |  | $\begin{split}RMS[g]_{t}&amp;=\sqrt{E[g^{2}]_{t}+\epsilon}\\ \Delta
    x_{t}&amp;=-\frac{\eta}{RMS[g]_{t}}g_{t}.\end{split}$ |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\begin{split}RMS[g]_{t}&amp;=\sqrt{E[g^{2}]_{t}+\epsilon}\\ \Delta
    x_{t}&amp;=-\frac{\eta}{RMS[g]_{t}}g_{t}.\end{split}$ |  |'
- en: 'Noticing the mismatch of units in Eq.[[13](#S3.E13 "In 3.3.2\. Adadelta ‣ 3.3\.
    Adaptive Gradient Algorithms ‣ 3\. Gradient Descent Optimization Algorithms ‣
    Large-Scale Deep Learning Optimizations: A Comprehensive Survey")], i.e., the
    units of the update $\Delta x$ do not match the units of the parameters $x$ which
    it applies to'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '注意到方程[[13](#S3.E13 "In 3.3.2\. Adadelta ‣ 3.3\. Adaptive Gradient Algorithms
    ‣ 3\. Gradient Descent Optimization Algorithms ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey")]中的单位不匹配，即更新$\Delta x$的单位与应用于其参数$x$的单位不匹配'
- en: '| (14) |  | $units\,of\,\Delta x\propto units\,of\,g\propto\frac{\partial f}{\partial
    x}\propto\frac{1}{units\,of\,x},$ |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $units\,of\,\Delta x\propto units\,of\,g\propto\frac{\partial f}{\partial
    x}\propto\frac{1}{units\,of\,x},$ |  |'
- en: Zeiler ([2012](#bib.bib123)) rearranges second order method (i.e., Newton’s
    method)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Zeiler（[2012](#bib.bib123)）重新排列了二阶方法（即，牛顿法）
- en: '| (15) |  | $\Delta x=\frac{\frac{\partial f}{\partial x}}{\frac{\partial^{2}f}{\partial
    x^{2}}}\Rightarrow\frac{1}{\frac{\partial^{2}f}{\partial x^{2}}}=\frac{\Delta
    x}{\frac{\partial f}{\partial x}}.$ |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $\Delta x=\frac{\frac{\partial f}{\partial x}}{\frac{\partial^{2}f}{\partial
    x^{2}}}\Rightarrow\frac{1}{\frac{\partial^{2}f}{\partial x^{2}}}=\frac{\Delta
    x}{\frac{\partial f}{\partial x}}.$ |  |'
- en: Since $\Delta x_{t}$ for the current time step in unknown, assuming the curvature
    is locally smooth, $\Delta x_{t}$ can be approximated by computing the exponentially
    decaying RMS of previous $\Delta x$
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于当前时间步的 $\Delta x_{t}$ 是未知的，假设曲率在局部是平滑的，可以通过计算之前 $\Delta x$ 的指数衰减均方根（RMS）来近似
    $\Delta x_{t}$。
- en: '| (16) |  | $\Delta x_{t}=-\frac{RMS[\Delta x]_{t-1}}{RMS[g]_{t}}g_{t}.$ |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\Delta x_{t}=-\frac{RMS[\Delta x]_{t-1}}{RMS[g]_{t}}g_{t}.$ |  |'
- en: 3.3.3\. RMSProp
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3. **RMSProp**
- en: RMSprop (Tieleman and Hinton, [2012](#bib.bib108)) was developed independently
    around the same time with Adadelta to solve the problem of AdaGrad’s drastically
    decreasing gradients. AdaGrad treats all past gradients equally, which is counter
    to our intuition that fresh gradient is more informative than the elder one. RMSProp
    redefines $v_{t}$ by decaying the past gradients at an exponential rate
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**RMSprop**（Tieleman 和 Hinton, [2012](#bib.bib108)）在与 Adadelta 同一时期独立开发，以解决
    AdaGrad 梯度急剧下降的问题。AdaGrad 对所有过去的梯度一视同仁，这与我们的直觉相反，因为新鲜的梯度比老旧的梯度更具信息性。RMSProp 通过以指数速率衰减过去的梯度来重新定义
    $v_{t}$。'
- en: '| (17) |  | $\begin{split}v_{t}=0.9v_{t-1}+0.1g_{t}^{2}\\ x_{t}=x_{t-1}-\frac{\eta}{\sqrt{v_{t}+\epsilon}}g_{t}.\end{split}$
    |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $\begin{split}v_{t}=0.9v_{t-1}+0.1g_{t}^{2}\\ x_{t}=x_{t-1}-\frac{\eta}{\sqrt{v_{t}+\epsilon}}g_{t}.\end{split}$
    |  |'
- en: 3.3.4\. Adam
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4. **Adam**
- en: Adam (Kingma and Ba, [2017](#bib.bib54)) is one of the most popular optimizers
    for training DNNs nowadays. It computes individual LRs for different parameters
    base on the estimates of first and second moments of the gradients. In particular,
    Adam stores an exponentially moving average of past gradients ($m_{t}$) and squared
    gradients ($v_{t}$). The former is an estimate of the first momentum (the mean)
    and the latter is an estimate of the second momentum (the uncentered variance)
    of the gradients
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**Adam**（Kingma 和 Ba, [2017](#bib.bib54)）是目前用于训练深度神经网络（DNNs）最受欢迎的优化器之一。它基于梯度的第一和第二矩的估计为不同参数计算各自的学习率（LRs）。具体来说，Adam
    存储了过去梯度（$m_{t}$）和梯度平方（$v_{t}$）的指数加权移动平均。前者是第一动量（均值）的估计，后者是梯度的第二动量（非中心化方差）的估计。'
- en: '| (18) |  | $\begin{split}m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}\\ v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}.\end{split}$
    |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $\begin{split}m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}\\ v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}.\end{split}$
    |  |'
- en: $\beta_{1}$ and $\beta_{2}$ are hyper-parameters controlling the decaying rates
    of theses moving averages. Since the moving averages are initialed as 0’s, the
    estimates of first and second moments are biased towards zero, especially in the
    beginning of training. Adam utilizes correction terms to counteract the initialization
    bias
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**$\beta_{1}$** 和 **$\beta_{2}$** 是控制这些移动平均衰减率的超参数。由于移动平均以 0 初始化，第一和第二矩的估计在训练开始时偏向于零。Adam
    利用修正项来抵消初始化偏差。'
- en: '| (19) |  | $\begin{split}\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}\\ \hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}.\end{split}$
    |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $\begin{split}\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}\\ \hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}.\end{split}$
    |  |'
- en: Then Adam applies the update rule
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 Adam 应用更新规则。
- en: '| (20) |  | $x_{t}=x_{t-1}-\frac{\eta}{\sqrt{\hat{v}_{t}+\epsilon}}\hat{m}_{t}.$
    |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $x_{t}=x_{t-1}-\frac{\eta}{\sqrt{\hat{v}_{t}+\epsilon}}\hat{m}_{t}.$
    |  |'
- en: 'Adam is found to be robust and well-suited to a wide range of non-convex optimization
    problems in the field of DL. There are several variants of Adam. AdaMax (Kingma
    and Ba, [2017](#bib.bib54)) is an extension to Adam that generalizes the approach
    to the infinite norm (max) and may result in a more effective optimization on
    some problems. Nesterov-accelerated Adaptive Moment Estimation (NAdam) (Dozat,
    [2016](#bib.bib31)) incorporates NAG into Adam. It shows better convergence speed
    in some cases. While these algorithms have been successfully employed in several
    practical applications, they may fail to converge to optimal solution even in
    convex setting, or even diverge in DL training. Reddi et al. ([2019](#bib.bib85))
    pinpoint the exponential moving average of past squared gradients as a reason
    for such failures. Recall that the introduction of the exponential average was
    well-motivated to tackle the key flaw of the Adagrad algorithm: it should prevent
    the LRs to become infinitesimally small as training progresses by limiting the
    reliance of the update on essentially only the past few gradients. However, this
    short-term memory of the gradients can indeed cause significant convergence issues
    in other scenarios. To resolve this issue, the authors propose new variants of
    Adam — AMSGrad, which relies on long-term memory of past gradients. AMSGrad uses
    the maximum of past squared gradients rather than the exponential average to update
    the parameters. Liu et al. ([2019](#bib.bib65)) argue that the root cause of the
    bad convergence problem suffered by Adam is that the adaptive LR has undesirably
    large variance in the early stage of model training, due to the limited amount
    of training samples being used. Thus, to reduce such variance, it is better to
    use smaller LRs in the first few epochs of training. The authors propose Rectified
    Adam (RAdam) to rectify the variance of the adaptive LR.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 被发现对深度学习领域中各种非凸优化问题具有稳健性和良好的适应性。Adam 有几种变体。AdaMax（Kingma 和 Ba, [2017](#bib.bib54)）是对
    Adam 的扩展，它将方法推广到无限范数（最大值），在某些问题上可能会实现更有效的优化。Nesterov-accelerated Adaptive Moment
    Estimation (NAdam)（Dozat, [2016](#bib.bib31)）将 NAG 融入了 Adam。它在某些情况下表现出更好的收敛速度。虽然这些算法在多个实际应用中已成功使用，但即使在凸设置中，它们也可能无法收敛到最优解，甚至在深度学习训练中发散。Reddi
    等人 ([2019](#bib.bib85)) 指出，过去平方梯度的指数移动平均是这些失败的原因之一。请记住，引入指数平均是为了应对 Adagrad 算法的一个主要缺陷：应防止学习率在训练过程中变得无穷小，通过限制更新仅依赖于过去的几个梯度。然而，这种对梯度的短期记忆在其他情况下确实会导致显著的收敛问题。为了解决这个问题，作者提出了新的
    Adam 变体——AMSGrad，它依赖于对过去梯度的长期记忆。AMSGrad 使用过去平方梯度的最大值，而不是指数平均值，来更新参数。Liu 等人 ([2019](#bib.bib65))
    认为 Adam 遇到不良收敛问题的根本原因是自适应学习率在模型训练的早期阶段具有过大的方差，这是由于使用的训练样本数量有限。因此，为了减少这种方差，在训练的前几个时期使用较小的学习率更为合适。作者提出了
    Rectified Adam (RAdam) 以纠正自适应学习率的方差。
- en: Choosing an optimizer is a crucial step when training DNNs since it is woven
    with the training speed and the final predictive performance. Despite the fact
    that adaptive optimization methods, including AdaGrad, RMSProp, AdaDelat and Adam,
    are becoming increasingly popular, to date, how to choose an optimal one is still
    theoretically elusive and intractable. Instead practitioners rely on empirical
    studies (Wilson et al., [2017](#bib.bib114)) and bench-marking (Schneider et al.,
    [2019](#bib.bib89)). Wilson et al. ([2017](#bib.bib114)) observed that the solutions
    found by adaptive methods generalize worse (often significantly worse) than SGD,
    even when these solutions have better training performance. However, Choi et al.
    ([2019](#bib.bib26)) suggest that popular adaptive gradient methods never under-perform
    momentum or gradient descent. They point out the comparisons among optimizers
    are sensitive to the hyper-parameter tuning protocols.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 选择优化器是训练深度神经网络时的一个关键步骤，因为它与训练速度和最终预测性能密切相关。尽管包括 AdaGrad、RMSProp、AdaDelta 和 Adam
    在内的自适应优化方法越来越受欢迎，但至今，如何选择最佳优化器在理论上仍然难以捉摸和解决。相反，实践者依赖于经验研究（Wilson 等人, [2017](#bib.bib114)）和基准测试（Schneider
    等人, [2019](#bib.bib89)）。Wilson 等人 ([2017](#bib.bib114)) 观察到，自适应方法找到的解决方案的泛化能力通常比
    SGD 更差（往往差得多），即使这些解决方案在训练性能上更好。然而，Choi 等人 ([2019](#bib.bib26)) 建议，流行的自适应梯度方法从未表现得比动量或梯度下降差。他们指出，优化器之间的比较对超参数调整协议非常敏感。
- en: 4\. Large Batch Training
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 大批量训练
- en: 'Input: Dataset $X$,minibatch size $b$ per node, the number of nodes $N$, optimization
    function SGD, init parameters $w={w[0],\cdots,w[M]}$for *$t=0,1,\cdots$* do      
    $G_{t}^{k}\leftarrow 0$;       for *$i=1,\cdots,B$* do             Sample data
    $x$ from $X$;             $G_{t}^{k}\leftarrow G_{t}^{k}+\frac{1}{Nb}\nabla f(x;w_{t})$      
    end for      All-Reduce $G_{t}^{k}:G_{t}\leftarrow\sum_{k=1}^{N}G_{t}^{k}$;      
    $w_{t+1}\leftarrow\textit{SGD}(w_{t},G_{t})$end for'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：数据集 $X$，每个节点的 minibatch 大小 $b$，节点数量 $N$，优化函数 SGD，初始化参数 $w={w[0],\cdots,w[M]}$对于
    *$t=0,1,\cdots$* 执行        $G_{t}^{k}\leftarrow 0$；        对于 *$i=1,\cdots,B$*
           执行            从 $X$ 中抽样数据 $x$；            $G_{t}^{k}\leftarrow G_{t}^{k}+\frac{1}{Nb}\nabla
    f(x;w_{t})$        结束      所有-归约 $G_{t}^{k}:G_{t}\leftarrow\sum_{k=1}^{N}G_{t}^{k}$；
           $w_{t+1}\leftarrow\textit{SGD}(w_{t},G_{t})$结束
- en: Algorithm 1 Distributed Synchronous SGD on Node k.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 在节点 k 上的分布式同步 SGD。
- en: 'Large DNNs and large datasets have fueled the development of deep learning (Krizhevsky
    et al., [2012](#bib.bib56); He et al., [2016](#bib.bib43); Simonyan and Zisserman,
    [2014](#bib.bib95); Krizhevsky et al., [2017](#bib.bib57); Szegedy et al., [2015](#bib.bib105);
    Devlin et al., [2019](#bib.bib29)). However, training large models on massive
    datasets is compute-intensive. For instance, training the SOTA DL models like
    BERT and ResNet-50 takes 3 days on 16 TPUv3 chips and 29 hours on 8 Tesla P100
    gpus respectively (Devlin et al., [2019](#bib.bib29); He et al., [2016](#bib.bib43)).
    An intuitive way to accelerate training is to add more computational power (e.g.,
    more GPU nodes) and use data parallel (see Alg.[1](#alg1 "In 4\. Large Batch Training
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey")). Considering
    communication (i.e., synchronizing the updates at each iteration) is an issue,
    each GPU must be utilized as much as possible to amortize the communication cost.
    Therefore, large batch should be used to distribute more data to each GPU. The
    nontrivial growth of batch size often results in test performance degradation,
    as observed in (Krizhevsky, [2014](#bib.bib55); Keskar et al., [2017](#bib.bib53);
    Li et al., [2014b](#bib.bib62); Hoffer et al., [2017](#bib.bib46)). We describe
    the training difficulties introduced by large batch in Section [4.1](#S4.SS1 "4.1\.
    Large Batch Training Difficulties ‣ 4\. Large Batch Training ‣ Large-Scale Deep
    Learning Optimizations: A Comprehensive Survey"), a recipe for large batch training
    (i.e., linear LR scaling with a warmup strategy) in Section [4.2](#S4.SS2 "4.2\.
    Learning Rate Scaling for Large Batch ‣ 4\. Large Batch Training ‣ Large-Scale
    Deep Learning Optimizations: A Comprehensive Survey"), other supplementary strategies
    such as adaptive layer-wise learning in Section [4.3](#S4.SS3 "4.3\. Adaptive
    Layerwise Learning ‣ 4\. Large Batch Training ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey") and adaptive batch size in Section [4.4](#S4.SS4 "4.4\.
    Adaptive Batch Size ‣ 4\. Large Batch Training ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey"), and finally discuss the extent to which we can scale
    up the batch size in Section [4.5](#S4.SS5 "4.5\. Efficient Scaling ‣ 4\. Large
    Batch Training ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey").'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 大型 DNN 和大规模数据集推动了深度学习的发展 (Krizhevsky et al., [2012](#bib.bib56)；He et al., [2016](#bib.bib43)；Simonyan
    和 Zisserman, [2014](#bib.bib95)；Krizhevsky et al., [2017](#bib.bib57)；Szegedy
    et al., [2015](#bib.bib105)；Devlin et al., [2019](#bib.bib29))。然而，在大规模数据集上训练大型模型计算密集。例如，训练像
    BERT 和 ResNet-50 这样的 SOTA DL 模型分别需要在 16 个 TPUv3 芯片上训练 3 天和在 8 个 Tesla P100 GPU
    上训练 29 小时 (Devlin et al., [2019](#bib.bib29)；He et al., [2016](#bib.bib43))。加速训练的直观方法是增加更多计算能力（例如，更多
    GPU 节点）并使用数据并行（见 Alg.[1](#alg1 "在 4\. 大批量训练 ‣ 大规模深度学习优化：综合调查")）。考虑到通信（即在每次迭代时同步更新）是一个问题，每个
    GPU 必须尽可能地利用，以摊销通信成本。因此，应该使用大批量将更多数据分配给每个 GPU。批量大小的非平凡增长通常会导致测试性能下降，正如 (Krizhevsky,
    [2014](#bib.bib55)；Keskar et al., [2017](#bib.bib53)；Li et al., [2014b](#bib.bib62)；Hoffer
    et al., [2017](#bib.bib46)) 所观察到的那样。我们在第 [4.1](#S4.SS1 "4.1\. 大批量训练难点 ‣ 4\. 大批量训练
    ‣ 大规模深度学习优化：综合调查") 节中描述了大批量引入的训练困难，在第 [4.2](#S4.SS2 "4.2\. 大批量的学习率缩放 ‣ 4\. 大批量训练
    ‣ 大规模深度学习优化：综合调查") 节中介绍了大批量训练的配方（即线性 LR 缩放与预热策略），在第 [4.3](#S4.SS3 "4.3\. 自适应层次学习
    ‣ 4\. 大批量训练 ‣ 大规模深度学习优化：综合调查") 节中介绍了其他补充策略，如自适应层次学习，以及在第 [4.4](#S4.SS4 "4.4\.
    自适应批量大小 ‣ 4\. 大批量训练 ‣ 大规模深度学习优化：综合调查") 节中介绍的自适应批量大小，最后在第 [4.5](#S4.SS5 "4.5\.
    高效扩展 ‣ 4\. 大批量训练 ‣ 大规模深度学习优化：综合调查") 节中讨论了我们可以在多大程度上扩大批量大小。
- en: 4.1\. Large Batch Training Difficulties
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 大批量训练难点
- en: Although large batches are preferable to increase the parallelism by distributing
    the workload to multiple nodes, they may slow down convergence rate in practice (Byrd
    et al., [2012](#bib.bib21)). Empirically, an increase in mini-batch size after
    a certain point (e.g. 1024) without a careful optimization scheme typically decreases
    the rate of convergence. The test accuracy of the converged solution becomes significantly
    lower than the baseline (Goyal et al., [2018](#bib.bib38); Keskar et al., [2017](#bib.bib53);
    Hoffer et al., [2017](#bib.bib46); Li et al., [2014b](#bib.bib62)). In addition
    to a degradation of the test performance, Masters and Luschi ([2018](#bib.bib71))
    provide evidence that increasing the batch size also results in a progressively
    smaller range of LRs that allows stable training.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大批量更有利于通过将工作负载分配到多个节点来增加并行性，但在实践中，它们可能会降低收敛速度 (Byrd et al., [2012](#bib.bib21))。经验上，经过一定点（例如
    1024）的迷你批量大小增加而没有仔细优化方案通常会降低收敛速度。收敛解的测试准确性显著低于基线 (Goyal et al., [2018](#bib.bib38);
    Keskar et al., [2017](#bib.bib53); Hoffer et al., [2017](#bib.bib46); Li et al.,
    [2014b](#bib.bib62))。除了测试性能的下降外，Masters 和 Luschi ([2018](#bib.bib71)) 提供了证据表明，增加批量大小还会导致允许稳定训练的学习率范围逐渐缩小。
- en: Keskar et al. ([2017](#bib.bib53)) find a drop in generalization (often denoted
    as generalization gap) to be as high as 5% even for smaller networks, and correlate
    the generalization gap with the sharpness of the loss landscape. They argue that
    large-batch methods tend to converge to sharp minimizers of the training and testing
    functions, whereas small-batch methods consistently converge to flat minimizers.
    Hoffer et al. ([2017](#bib.bib46)) deny the existence of inherent generalization
    gap and suggest that training longer will help the algorithm to generalize better
    and keep the accuracy higher. Goyal et al. ([2018](#bib.bib38)) admit that large
    batches cause optimization difficulties, but when these are addressed the trained
    networks exhibit good generalization. They tried to bridge the generalization
    gap with heuristics of LR scaling (Goyal et al., [2018](#bib.bib38)) with a warpup
    strategy. However, empirical study  (Shallue et al., [2019](#bib.bib93)) shows
    that LR scaling heuristics with the batch size do not hold across all problems
    or across all batch sizes. Later You et al. ([2017](#bib.bib121)) proposed Layer-wise
    Adaptive Rate Scaling (LARS) to solve the large batch optimization difficulties.
    Several recent works successfully scaled the batch size to large values using
    adaptive learning rates without degrading the performance.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Keskar 等人 ([2017](#bib.bib53)) 发现，即使是较小的网络，其泛化能力（通常表示为泛化差距）也可能高达 5%，并且将泛化差距与损失景观的尖锐度相关联。他们认为，大批量方法往往会收敛到训练和测试函数的尖锐最小值，而小批量方法则会一致地收敛到平坦最小值。Hoffer
    等人 ([2017](#bib.bib46)) 否认了固有泛化差距的存在，并建议延长训练时间有助于算法更好地泛化，并保持更高的准确性。Goyal 等人 ([2018](#bib.bib38))
    承认大批量会导致优化困难，但当这些问题得到解决时，训练网络表现出良好的泛化能力。他们尝试通过学习率缩放的启发式方法 (Goyal et al., [2018](#bib.bib38))
    和一个渐进策略来弥合泛化差距。然而，实证研究 (Shallue et al., [2019](#bib.bib93)) 显示，学习率缩放的启发式方法在所有问题或所有批量大小下并不适用。后来
    You 等人 ([2017](#bib.bib121)) 提出了逐层自适应学习率缩放（LARS）来解决大批量优化困难。一些近期的工作成功地使用自适应学习率将批量大小扩展到较大值，而不降低性能。
- en: 4.2\. Learning Rate Scaling for Large Batch
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 大批量的学习率缩放
- en: 'A nice property of large batch is its lower variance of the gradient. This
    is because when we take the gradient over more examples, the variance is obviously
    lower. Consequently, large batch allows us to take a larger step per iteration.
    Followings are two commonly used LR heuristics: linear scaling and sqrt scaling,
    to guide us to adapt the LR for large batches.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 大批量的一个良好属性是其梯度的较低方差。这是因为当我们对更多示例进行梯度计算时，方差显然较低。因此，大批量使我们能够在每次迭代中采取更大的步长。以下是两种常用的学习率启发式方法：线性缩放和平方根缩放，用于指导我们调整大批量的学习率。
- en: 4.2.1\. Linear Scaling
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 线性缩放
- en: (Krizhevsky, [2014](#bib.bib55); Goyal et al., [2018](#bib.bib38); Bottou et al.,
    [2018](#bib.bib20)) suggest linearly scaling up LR with batch size, i.e., when
    the mini-batch size is multiplied by $k$, multiply the LR by $k$. Intuitively,
    after $k$ iterations of mini-batch SGD , we have
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (Krizhevsky, [2014](#bib.bib55); Goyal et al., [2018](#bib.bib38); Bottou et
    al., [2018](#bib.bib20)) 建议将学习率与批量大小线性缩放，即当迷你批量大小乘以 $k$ 时，将学习率乘以 $k$。直观地说，在 $k$
    次迷你批量随机梯度下降后，我们有
- en: '| (21) |  | $x_{t+k}=x_{t}-\eta\frac{1}{&#124;S&#124;}\sum_{i<k}\sum_{s\in
    S}\nabla l(x_{t+i},s),$ |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $x_{t+k}=x_{t}-\eta\frac{1}{&#124;S&#124;}\sum_{i<k}\sum_{s\in
    S}\nabla l(x_{t+i},s),$ |  |'
- en: while after one iteration of large mini-batch $\bigcup_{j}B_{j}$ of size $|S|=k|B|$
    we have
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当经过一次大小为 $|S|=k|B|$ 的大批量 $\bigcup_{j}B_{j}$ 后，我们有
- en: '| (22) |  | $\hat{x}_{t+1}=x_{t}-\hat{\eta}\frac{1}{k&#124;B&#124;}\sum_{j<k}\sum_{s\in
    B_{j}}\nabla l(x_{t},s).$ |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $\hat{x}_{t+1}=x_{t}-\hat{\eta}\frac{1}{k&#124;B&#124;}\sum_{j<k}\sum_{s\in
    B_{j}}\nabla l(x_{t},s).$ |  |'
- en: 'If we assume $\nabla l(x_{t+i})\approx\nabla l(x_{t})$ for $i<k$, then the
    adjustment $\hat{\eta}=k\eta$ would yield $\hat{x}_{t+1}\approx x_{t+k}$. Noted
    that this assumption holds with the premises: (1) $k$ cannot be infinite. That
    is, we cannot scale up the batch size without limits; (2) $t$ cannot be too small.
    Because at the beginning of training, the gradients change rapidly, and thus the
    difference between $\nabla l(x_{t})$ and $\nabla l(x_{t+i})$ is no longer negligible.
    Using LR warmup and linear scaling, Goyal et al. ([2018](#bib.bib38)) trained
    Resnet-50 with batch B=8K without loss in accuracy.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设 $\nabla l(x_{t+i})\approx\nabla l(x_{t})$ 对于 $i<k$，那么调整 $\hat{\eta}=k\eta$
    会使得 $\hat{x}_{t+1}\approx x_{t+k}$。需要注意的是，这一假设在以下前提下成立：(1) $k$ 不能是无限的。也就是说，我们不能无限制地扩展批量大小；(2)
    $t$ 不能太小。因为在训练初期，梯度变化剧烈，因此 $\nabla l(x_{t})$ 和 $\nabla l(x_{t+i})$ 之间的差异不再可以忽略。通过使用LR预热和线性缩放，Goyal
    et al. ([2018](#bib.bib38)) 训练了Resnet-50，批量大小为B=8K，准确率没有损失。
- en: 4.2.2\. Sqrt Scaling
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 平方根缩放
- en: Another scaling strategy is sqrt scaling, i.e., when the mini-batch size is
    multiplied by $k$, multiply the LR by $\sqrt{k}$. In SGD, the co-variance matrix
    of the parameters update $\Delta x$ is  (Hoffer et al., [2017](#bib.bib46))
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种扩展策略是平方根缩放，即当小批量大小乘以 $k$ 时，将学习率乘以 $\sqrt{k}$。在SGD中，参数更新 $\Delta x$ 的协方差矩阵是
     (Hoffer et al., [2017](#bib.bib46))
- en: '| (23) |  | $cov(\Delta x,\Delta x)\thickapprox\frac{\eta^{2}}{&#124;B&#124;}(\frac{1}{N}\sum_{n=1}^{N}g_{n}g_{n}^{T}).$
    |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| (23) |  | $cov(\Delta x,\Delta x)\thickapprox\frac{\eta^{2}}{&#124;B&#124;}(\frac{1}{N}\sum_{n=1}^{N}g_{n}g_{n}^{T}).$
    |  |'
- en: A simple way to keep this co-variance constant when we change the batch size
    is to choose $\eta\propto\sqrt{|B|}$. Hoffer et al. ([2017](#bib.bib46)) find
    that by using ”Ghost Batch Normalization” and sqrt scaling, the generalization
    gap can be significantly decreased. However, the largest batch size used was 4,096,
    which does not rule out an effect appearing at still larger batch sizes, as suggested
    by the work of Goyal et al. ([2018](#bib.bib38)). Moreover, establishing this
    invariant co-variance remains poorly justified, and often sqrt scaling is found
    to degrade model quality in practice, see  (Krizhevsky, [2014](#bib.bib55); Goyal
    et al., [2018](#bib.bib38); Jastrz\kebski et al., [2017](#bib.bib49)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 保持协方差不变的一个简单方法是选择 $\eta\propto\sqrt{|B|}$。Hoffer et al. ([2017](#bib.bib46))
    发现通过使用“幽灵批量归一化”和平方根缩放，可以显著减少泛化误差。然而，使用的最大批量大小是4096，这并未排除在更大的批量大小下可能出现的效果，如Goyal
    et al. ([2018](#bib.bib38)) 的研究所示。此外，建立这种不变的协方差仍缺乏充分的理论依据，并且在实践中经常发现平方根缩放会降低模型质量，见
    (Krizhevsky, [2014](#bib.bib55); Goyal et al., [2018](#bib.bib38); Jastrz\kebski
    et al., [2017](#bib.bib49))。
- en: 4.2.3\. Warmup
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 预热
- en: After adjusting the LR with these strategies, the main obstacle for scaling
    up batch size is the instability of training with high LR, especially in the initial
    epochs when the gradients change dramatically. This issue can be alleviated by
    a properly designed warmup strategy by using less aggressive LRs in the initial
    epochs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这些策略调整学习率（LR）后，批量大小扩展的主要障碍是高学习率下训练的不稳定性，尤其是在梯度剧烈变化的初期。这一问题可以通过合理设计的预热策略来缓解，在初期使用较为温和的学习率。
- en: Constant warmup. Verma et al. ([2017](#bib.bib110)) use a low ”safe” constant
    LR for the first few epochs of training and after that return to the target LR
    $\hat{\eta}=k\eta$. Goyal et al. ([2018](#bib.bib38)) find constant warmup particularly
    helpful for prototyping object detection and segmentation methods (Girshick, [2015](#bib.bib36);
    Ren et al., [2015](#bib.bib86); He et al., [2020](#bib.bib42)), but not sufficient
    enough to solve the large batch optimization problem. In particular, a transition
    out of the low LR warmup phase can cause the training error to spike. This motivates
    them to use a more moderate warmup stragegy — gradual warmup.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 常量预热。Verma et al. ([2017](#bib.bib110)) 在训练的前几个时期使用低“安全”常量学习率，然后恢复到目标学习率 $\hat{\eta}=k\eta$。Goyal
    et al. ([2018](#bib.bib38)) 发现常量预热对于原型对象检测和分割方法特别有用 (Girshick, [2015](#bib.bib36);
    Ren et al., [2015](#bib.bib86); He et al., [2020](#bib.bib42))，但不足以解决大批量优化问题。特别是，从低学习率预热阶段过渡可能会导致训练误差激增。这促使他们使用一种更为温和的预热策略——渐进预热。
- en: Gradual warmup. Unlike constant warmup, gradual warmup avoid a sudden increase
    of LR by gradually arising the LR from a small to a large value. We denote the
    LR of the $t$-th iteration as $lr(t)$ and the maximum LR during training as $lr_{max}$.
    Given a predefined time frame $T_{warmup}$, the LR scheduler for the $t$-th iterations
    is defined as
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进式预热。与恒定预热不同，渐进式预热通过将学习率（LR）从小值逐渐增加到大值，避免了LR的突然升高。我们将第$t$次迭代的LR表示为$lr(t)$，训练期间的最大LR表示为$lr_{max}$。给定预定义的时间框架$T_{warmup}$，第$t$次迭代的LR调度器定义为
- en: '| (24) |  | $lr(t)=\frac{t}{T_{warmup}}lr_{max},\quad t\leq T_{warmup}.$ |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| (24) |  | $lr(t)=\frac{t}{T_{warmup}}lr_{max},\quad t\leq T_{warmup}.$ |  |'
- en: After this warmup stage, the LR will be set by classical LR schedulers (e.g.,
    cosine decay). A LR warmup stage is proved to be beneficial when training NNs
    with extremely large batch size (You et al., [2020](#bib.bib122); Goyal et al.,
    [2018](#bib.bib38)). Liu et al. ([2019](#bib.bib65)) claim that the benefit of
    the warmup stage comes from reducing the variance for the adaptive LR in the Adam
    optimizer. They further propose Rectified Adam (RAdam) by introducing a term to
    rectify the variance of the adaptive LR. Additionally, Xiong et al. ([2020](#bib.bib116))
    find the LR warm-up stage also helps quite a lot for other optimizers.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一预热阶段之后，LR将由经典LR调度器（例如，余弦衰减）设置。LR预热阶段在训练具有极大批量大小的神经网络时被证明是有益的（You等， [2020](#bib.bib122)；Goyal等，
    [2018](#bib.bib38)）。Liu等（[2019](#bib.bib65)）声称预热阶段的好处来自于减少Adam优化器中自适应LR的方差。他们进一步提出了Rectified
    Adam（RAdam），通过引入一个术语来纠正自适应LR的方差。此外，Xiong等（[2020](#bib.bib116)）发现LR预热阶段对其他优化器也大有帮助。
- en: 4.3\. Adaptive Layerwise Learning
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 自适应层学习
- en: 'Linear/Sqrt LR scaling with warmup mitigates the vulnerability to the fluctuation
    of gradients in the initial epoch by taking less aggressive steps, starting from
    a small LR which is safe enough for all layers and gradually increasing it to
    the target value. Hoffer et al. ([2017](#bib.bib46)) use less aggressive sqrt
    scaling with ”Ghost Batch Normalization” to train Alexnet with $B=8K$, but still
    the accuracy ($53.93\%$) was much worse than baseline ($57.10\%$). Goyal et al.
    ([2018](#bib.bib38)) use LR warmup and linear scaling to train Resnet-50 with
    batch B=8K without loss in accuracy. While these works demonstrate the feasibility
    of these strategies for reducing the wall time for training large DNNs, they are
    not general enough if we want further enlarge the batch size. For instance, You
    et al. ([2017](#bib.bib121)) applied linear scaling and warmup scheme to train
    Alexnet with batch normalization on Imagenet, and observed a $2.2\%$ drop when
    $B=8K$ in the test accuracy. You et al. ([2017](#bib.bib121)) explain their method
    to solve this problem: To analyze the training stability with large LRs we measured
    the ratio between the norm of the layer weights and norm of gradients update.
    We observed that if this ratio is too high, the training may become unstable.
    On the other hand, if the ratio is too small, then weights don’t change fast enough.
    This ratio works like a hint about how to adapt the LR for each layer. In this
    section, we will first introduce a general adaptive layerwise strategy motivated
    by this ratio, followed by two specific algorithms, LARS (You et al., [2017](#bib.bib121))
    and LAMB (You et al., [2020](#bib.bib122)).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 带有预热的线性/平方根LR缩放通过采取较少激进的步骤，减轻了初始时期梯度波动的脆弱性，从一个对所有层都足够安全的小LR开始，逐渐增加到目标值。Hoffer等（[2017](#bib.bib46)）使用较少激进的平方根缩放与“Ghost
    Batch Normalization”来训练Alexnet，批量为$B=8K$，但准确率（$53.93\%$）仍远低于基线（$57.10\%$）。Goyal等（[2018](#bib.bib38)）使用LR预热和线性缩放来训练Resnet-50，批量为B=8K，准确率没有下降。尽管这些工作展示了这些策略在减少大规模DNN训练墙面时间方面的可行性，但如果我们希望进一步扩大批量大小，它们的通用性不足。例如，You等（[2017](#bib.bib121)）将线性缩放和预热方案应用于在Imagenet上训练带有批量归一化的Alexnet，并观察到当$B=8K$时测试准确率下降了$2.2\%$。You等（[2017](#bib.bib121)）解释了他们解决此问题的方法：为了分析大LR下的训练稳定性，我们测量了层权重的范数与梯度更新的范数之间的比率。我们观察到，如果这个比率过高，训练可能会变得不稳定。另一方面，如果比率过小，那么权重改变的速度不够快。这个比率像是一个提示，帮助调整每层的LR。在本节中，我们将首先介绍一个由这个比率激发的一般自适应层策略，然后介绍两个具体的算法，LARS（You等，
    [2017](#bib.bib121)）和LAMB（You等， [2020](#bib.bib122)）。
- en: 4.3.1\. General Layerwise Strategy
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 一般层策略
- en: Suppose we use an iterative base algorithm $\mathscr{A}$ (e.g., SGD or Adam)
    in the small batch setting with the following layerwise update rule
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在小批量设置中使用一个迭代基础算法$\mathscr{A}$（例如，SGD或Adam），其层更新规则如下
- en: '| (25) |  | $x_{t+1}=x_{t}+\eta_{t}u_{t},$ |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $x_{t+1}=x_{t}+\eta_{t}u_{t},$ |  |'
- en: 'where $u_{t}$ is the update made by $\mathscr{A}$ at time step $t$. You et al.
    ([2020](#bib.bib122)) propose the following two changes to the update for large
    batch settings:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $u_{t}$ 是时间步 $t$ 上由 $\mathscr{A}$ 所做的更新。You 等人（[2020](#bib.bib122)）提出了对大批量设置下更新的以下两个修改：
- en: (1)
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: The update is normalized to unit $l_{2}$-norm. This is ensured by modifying
    the update to the form $u_{t}/\lVert u_{t}\rVert$. Such a normalization is done
    layer-wise, i.e., the update for each layer is ensured to be unit $l_{2}$-norm.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更新被归一化为单位 $l_{2}$-范数。通过将更新修改为 $u_{t}/\lVert u_{t}\rVert$ 来确保这一点。这种归一化是逐层进行的，即每一层的更新都被确保为单位
    $l_{2}$-范数。
- en: (2)
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: The LR is scaled by $\phi(\lVert x_{t}\rVert)$ for some function $\phi:\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}$.
    Similar to the normalization, such a scaling is done layer-wise.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率由 $\phi(\lVert x_{t}\rVert)$ 按某个函数 $\phi:\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}$
    进行缩放。类似于归一化，这种缩放也是逐层进行的。
- en: Suppose the base algorithm $\mathscr{A}$ is SGD, then the modification results
    in the following update rule
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 假设基础算法 $\mathscr{A}$ 是 SGD，则修改后的更新规则如下
- en: '| (26) |  | $x_{t+1}^{(i)}=x_{t}^{(i)}-\eta_{t}\frac{\lVert\phi(x_{t}^{(i)})\rVert}{\lVert
    g_{t}^{(i)}\rVert}g_{t}^{(i)}$ |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| (26) |  | $x_{t+1}^{(i)}=x_{t}^{(i)}-\eta_{t}\frac{\lVert\phi(x_{t}^{(i)})\rVert}{\lVert
    g_{t}^{(i)}\rVert}g_{t}^{(i)}$ |  |'
- en: for all layers $i\in[h]$. The normalization modification $g_{t}^{(i)}/\lVert
    g_{t}^{(i)}\rVert$ is similar to one typically used in normalized gradient descent
    except that it is done layer-wise. Normalization of this form provides robustness
    to exploding/vanishing gradients (where the gradient can be arbitrarily large/small)
    by essentially ignoring the size of the gradient but preserving the direction.
    As for the scaling step, the scaling term involving $\phi$ ensures that the norm
    of the update is of the same order as that of the parameter. When the parameters
    are small, we take a small step and vice versa.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有层 $i\in[h]$。归一化修改 $g_{t}^{(i)}/\lVert g_{t}^{(i)}\rVert$ 类似于通常在归一化梯度下降中使用的，但它是逐层进行的。这种形式的归一化通过本质上忽略梯度的大小但保留方向，从而对梯度爆炸/消失（梯度可以任意大/小）提供了鲁棒性。至于缩放步骤，涉及
    $\phi$ 的缩放项确保更新的范数与参数的范数处于同一数量级。当参数较小时，我们采取小步伐，反之亦然。
- en: 'There are two notable differences between this general strategy and other adaptive
    algorithms such as Adam or RMSProp: (1) it uses a separate LR for each layer and
    not for each weight. (2) the magnitude of the update is controlled w.r.t the weight
    norm for better control of training speed. Both LARS (You et al., [2017](#bib.bib121))
    and LAMB (You et al., [2020](#bib.bib122)) are based on this general strategy,
    using momentum and Adam optimizer as the base algorithm respectively.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这种通用策略与其他自适应算法如 Adam 或 RMSProp 之间有两个显著的区别：（1）它对每一层使用单独的学习率，而不是对每个权重使用。（2）更新的幅度相对于权重范数进行控制，以更好地控制训练速度。LARS（You
    等人，[2017](#bib.bib121)）和 LAMB（You 等人，[2020](#bib.bib122)）都基于这一通用策略，分别使用动量优化器和
    Adam 优化器作为基础算法。
- en: 4.3.2\. LARS
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. LARS
- en: The first instantiation of the general strategy is the LARS algorithm  (You
    et al., [2017](#bib.bib121)), which is obtained by using momentum optimizer as
    the base algorithm $\mathscr{A}$ in the framework. LARS stands for Layer-wise
    Adaptive Rate Scaling, which was proposed for large batch learning for ResNet
    on ImageNet. Specifically, a local LR $\lambda^{l}$ is defined for each layer
    $l$
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通用策略的首次实例是 LARS 算法（You 等人，[2017](#bib.bib121)），它是通过在框架中使用动量优化器作为基础算法 $\mathscr{A}$
    获得的。LARS 代表层级自适应率缩放，最初为 ImageNet 上的 ResNet 大批量学习提出。具体而言，为每一层 $l$ 定义一个局部学习率 $\lambda^{l}$
- en: '| (27) |  | $\lambda^{l}=\eta\frac{{\lVert x\rVert}_{2}^{2}}{{\lVert\nabla
    L(x)\rVert}_{2}^{2}}.$ |  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| (27) |  | $\lambda^{l}=\eta\frac{{\lVert x\rVert}_{2}^{2}}{{\lVert\nabla
    L(x)\rVert}_{2}^{2}}.$ |  |'
- en: The hyper-parameter $\eta<1$ describes the extent to which we can trust the
    layer to update its weights during each epoch. At the beginning of training, the
    numerator ${\lVert x\rVert}_{2}^{2}$ above is relatively small. In contrast, the
    denominator ${\lVert\nabla L(x)\rVert}_{2}^{2}$ is probably large since when everything
    is wrong, the loss and gradients are large. Any steps we take are likely to be
    small. In this way we naturally warm up as the weights increase. As we approach
    0 loss, the gradients become smaller and the local LR increases again, encouraging
    jumping out of the local minima to prevent over-fitting. The parameter update
    is
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数 $\eta<1$ 描述了我们可以信任每层在每个周期中更新其权重的程度。在训练开始时，分子 ${\lVert x\rVert}_{2}^{2}$
    相对较小。相反，分母 ${\lVert\nabla L(x)\rVert}_{2}^{2}$ 可能较大，因为当一切都错的时候，损失和梯度都很大。我们采取的任何步骤都可能较小。这样，随着权重的增加，我们自然地进行预热。随着损失接近0，梯度变小，局部学习率再次增加，鼓励跳出局部最小值以防止过拟合。参数更新为
- en: '| (28) |  | $\Delta x_{t}^{l}=\gamma*\lambda^{l}*\nabla L(x_{t}^{l}),$ |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| (28) |  | $\Delta x_{t}^{l}=\gamma*\lambda^{l}*\nabla L(x_{t}^{l})$ |  |'
- en: 'where $\lambda$ is the global LR. In this way, each layer can learn at its
    own pace accurately. The training for SGD with LARS are summarized in the Algorithm
    [2](#alg2 "In 4.3.2\. LARS ‣ 4.3\. Adaptive Layerwise Learning ‣ 4\. Large Batch
    Training ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey").'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是全局学习率（LR）。这样，每一层可以以其自己的速度准确学习。SGD与LARS的训练总结见算法 [2](#alg2 "在4.3.2\.
    LARS ‣ 4.3\. 自适应层级学习 ‣ 4\. 大批量训练 ‣ 大规模深度学习优化：全面调查")。
- en: 'Input: base LR $\gamma_{0}$, momentum $m1$, weight decay $\beta$, ”trust” coefficient
    $\eta$, number of steps $T$Init: $t=0$; $v=0$. Init weight $w_{0}^{l}$ for each
    layer $l$while *$t<T$ for each layer $l$* do       $g_{t}^{l}\leftarrow\nabla
    L(w_{t}^{l})$ ;        //obtain a stochastic gradient for the current mini-batch      
    $\gamma_{t}\leftarrow\gamma_{0}*(1-\frac{t}{T})^{2}$;        //compute the global
    LR       $\lambda^{l}\leftarrow\frac{\lVert w_{t}^{l}\rVert}{\lVert g_{t}^{l}\rVert+\beta\lVert
    w_{t}^{l}\rVert}$;        //compute the local LR       $v_{t+1}^{l}\leftarrow
    mv_{t}^{l}+\gamma_{t}*\lambda^{l}*(g_{t}^{l}+\beta w_{t}^{l})$;        //update
    the momentum       $w_{t+1}^{l}\leftarrow w_{t}^{l}-v_{t+1}^{l}$ ;        //update
    the weightsend whileExample with weight decay, momentum and polynomial LR decay.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：基础学习率 $\gamma_{0}$，动量 $m1$，权重衰减 $\beta$，“信任”系数 $\eta$，步数 $T$初始化：$t=0$；$v=0$。为每一层
    $l$ 初始化权重 $w_{0}^{l}$while *$t<T$ 对每一层 $l$* do       $g_{t}^{l}\leftarrow\nabla
    L(w_{t}^{l})$ ;        // 获取当前小批量的随机梯度       $\gamma_{t}\leftarrow\gamma_{0}*(1-\frac{t}{T})^{2}$;        //
    计算全局学习率       $\lambda^{l}\leftarrow\frac{\lVert w_{t}^{l}\rVert}{\lVert g_{t}^{l}\rVert+\beta\lVert
    w_{t}^{l}\rVert}$;        // 计算局部学习率       $v_{t+1}^{l}\leftarrow mv_{t}^{l}+\gamma_{t}*\lambda^{l}*(g_{t}^{l}+\beta
    w_{t}^{l})$;        // 更新动量       $w_{t+1}^{l}\leftarrow w_{t}^{l}-v_{t+1}^{l}$
    ;        // 更新权重end while 示例：带有权重衰减、动量和多项式学习率衰减。
- en: Algorithm 2 SGD with LARS.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 SGD与LARS。
- en: Example with weight decay, momentum and polynomial LR decay.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：带有权重衰减、动量和多项式学习率衰减。
- en: Several works successfully scaled the batch size to large values using LARS
    without degrading the performance, thereby, finishing ResNet-50 training on ImageNet
    in a few minutes (You et al., [2017](#bib.bib121); Ying et al., [2018](#bib.bib120);
    Yamazaki et al., [2019](#bib.bib119)). LARS also applies to tasks such as self-supervised
    image representation learning and contrastive learning of visual representations (Grill
    et al., [2020](#bib.bib39); Chen et al., [2020](#bib.bib25)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 几项工作成功地将批量大小扩展到较大的值，使用LARS而不降低性能，从而在几分钟内完成了在ImageNet上的ResNet-50训练（You et al.,
    [2017](#bib.bib121)；Ying et al., [2018](#bib.bib120)；Yamazaki et al., [2019](#bib.bib119)）。LARS也适用于自监督图像表示学习和视觉表示的对比学习任务（Grill
    et al., [2020](#bib.bib39)；Chen et al., [2020](#bib.bib25)）。
- en: 4.3.3\. LAMB
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. LAMB
- en: 'LAMB is the second instantiation of the general strategy, which is obtained
    by using Adam as the base algorithm $\mathscr{A}$. The pseudo-code is provided
    in Algorithm [3](#alg3 "In 4.3.3\. LAMB ‣ 4.3\. Adaptive Layerwise Learning ‣
    4\. Large Batch Training ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey"). The adaptivity of LAMB is two-fold: (1) per dimension normalization
    w.r.t the square root of the second moment used in Adam and (2) layer-wise normalization
    obtained due to layer-wise adaptivity. By using LAMB, You et al. ([2020](#bib.bib122))
    scale the batch size of BERT pre-training to 64K without losing accuracy, thereby,
    reducing the BERT training time from 3 days to around 76 minutes. LAMB is also
    the first large batch adaptive solver that can achieve the SOTA accuracy on ImageNet
    training with RESNET-50\. LAMB has also been adopted by many other work  (Lan
    et al., [2019](#bib.bib58)).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: LAMB是通用策略的第二种实现方式，通过使用Adam作为基础算法 $\mathscr{A}$ 获得。伪代码在算法 [3](#alg3 "在 4.3.3\.
    LAMB ‣ 4.3\. 自适应分层学习 ‣ 4\. 大批量训练 ‣ 大规模深度学习优化：综合调查") 中提供。LAMB的自适应性有两个方面：（1）相对于Adam中使用的二阶矩的平方根的每维归一化，（2）由于层级自适应性而获得的逐层归一化。通过使用LAMB，You
    等人（[2020](#bib.bib122)）将BERT预训练的批量大小扩大到64K而不丧失准确性，从而将BERT的训练时间从3天减少到大约76分钟。LAMB也是第一个可以在使用RESNET-50进行ImageNet训练时实现SOTA准确度的大批量自适应求解器。LAMB还被许多其他工作采纳（Lan
    等人，[2019](#bib.bib58)）。
- en: Despite of the popularity of LARS and LAMB, their utility as a ”large batch
    optimizer” is challenged by  (Nado et al., [2021](#bib.bib74)), which argues that
    they are more indirect regularizers than optimizers. By sophisticated tuning,
    traditional, generic algorithms (e.g., Momentum or Adam) achieve strong results
    across batch size. They appeal to researchers that the superiority of one particular
    optimizer over others should be claimed with extreme caution since the fair comparisons
    between optimizers crucially depend on the effort spent tuning hyperparameters
    for each optimizer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LARS和LAMB很受欢迎，它们作为“大批量优化器”的效用却受到（Nado 等人，[2021](#bib.bib74)）的质疑，该文认为它们更像是间接的正则化器而非优化器。通过精细调整，传统的通用算法（如Momentum或Adam）在不同批量大小上表现出强劲的结果。他们提醒研究人员，对于某个特定优化器的优越性声明应极其谨慎，因为优化器之间的公平比较在很大程度上依赖于为每个优化器调整超参数所花费的努力。
- en: 'Input: $x_{1}\in\mathbb{R}^{d}$, LR $\{\eta_{t}\}_{t=1}^{T}$, parameters $0<\eta_{1},\eta_{2}<1$,
    scaling function $\phi$, $\epsilon>0$Init: Set $m_{0}=0,\,v_{0}=0$for *$t=1$ to
    T* do       $g_{t}=\nabla L(x_{t})$;        //obtain a stochastic gradient for
    the current mini-batch       $m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}$;      
    $v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}$;       $m_{t}=m_{t}/(1-\beta_{1}^{t})$;      
    $v_{t}=v_{t}/(1-\beta_{2}^{t})$;       $r_{t}=\frac{m_{t}}{\sqrt{v_{t}}+\epsilon}$;      
    $x_{t+1}^{(i)}=x_{t}^{(i)}-\eta_{t}\frac{\phi(\lVert x_{t}^{(i)}\rVert)}{\lVert
    r_{t}^{(i)}+{\lambda x_{t}^{(i)}}\rVert}(r_{t}^{(i)}+{\lambda x_{t}^{(i)}})$end
    for'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$x_{1}\in\mathbb{R}^{d}$，LR $\{\eta_{t}\}_{t=1}^{T}$，参数 $0<\eta_{1},\eta_{2}<1$，缩放函数
    $\phi$，$\epsilon>0$ 初始化：设置 $m_{0}=0,\,v_{0}=0$ 对于 *$t=1$ 到 T* 进行       $g_{t}=\nabla
    L(x_{t})$;       //获取当前小批量的随机梯度       $m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}$;
          $v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}$;       $m_{t}=m_{t}/(1-\beta_{1}^{t})$;
          $v_{t}=v_{t}/(1-\beta_{2}^{t})$;       $r_{t}=\frac{m_{t}}{\sqrt{v_{t}}+\epsilon}$;
          $x_{t+1}^{(i)}=x_{t}^{(i)}-\eta_{t}\frac{\phi(\lVert x_{t}^{(i)}\rVert)}{\lVert
    r_{t}^{(i)}+{\lambda x_{t}^{(i)}}\rVert}(r_{t}^{(i)}+{\lambda x_{t}^{(i)}})$ 结束
- en: Algorithm 3 LAMB
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 LAMB
- en: 4.4\. Adaptive Batch Size
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 自适应批量大小
- en: It is a common practice to decay the LR during training. When one decays the
    LR, one simultaneously decays the ”noise scale”, i.e., the scale of random fluctuations
    in the SGD dynamics (Smith and Le, [2018](#bib.bib98))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，衰减LR是一种常见做法。当衰减LR时，也同时衰减“噪声规模”，即SGD动态中的随机波动规模（Smith 和 Le，[2018](#bib.bib98)）
- en: '| (29) |  | $\begin{split}g&amp;=\frac{\epsilon}{1-m}(\frac{N}{B}-1)\\ &amp;\thickapprox\frac{\epsilon
    N}{(1-m)B}.\end{split}$ |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| (29) |  | $\begin{split}g&=\frac{\epsilon}{1-m}(\frac{N}{B}-1)\\ &\thickapprox\frac{\epsilon
    N}{(1-m)B}.\end{split}$ |  |'
- en: When we decay the LR, the ”noise scale” falls, enabling us to converge to the
    minima of the loss function. We can achieve the same reduction in noise scale
    at constant LR by increasing the batch size. Smith et al. ([2018](#bib.bib97))
    and Devarakonda et al. ([2017](#bib.bib28)) empirically demonstrated the equivalence
    between decaying LR and increasing the batch size. Instead of decaying the LR
    by a factor of $\alpha$, they increase the batch size by $\alpha$ during training.
    This strategy reaches equivalent test accuracy after the same number of training
    epochs, but with fewer parameter updates, leading to greater parallelism and shorter
    training times. Crucially, such strategy is complementary to existing training
    schedules requiring no hyper-parameter tuning.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们减少学习率（LR）时，"噪声规模"会下降，使我们能够收敛到损失函数的最小值。通过增加批量大小，我们可以在保持学习率不变的情况下实现相同的噪声规模降低。Smith等人（[2018](#bib.bib97)）和Devarakonda等人（[2017](#bib.bib28)）通过实验证明了降低学习率和增加批量大小之间的等效性。他们没有通过一个系数$\alpha$来降低学习率，而是在训练过程中将批量大小增加了$\alpha$。这一策略在相同的训练轮次后达到了等效的测试准确率，但所需的参数更新更少，从而实现了更大的并行性和更短的训练时间。关键是，这种策略与现有的训练计划互补，不需要进行超参数调整。
- en: 4.5\. Efficient Scaling
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 高效的扩展
- en: Increasing the batch size is one of the most appealing ways to accelerate NN
    training on data parallel hardware. Ideally, parallel mini-batch SGD can achieve
    a linear speed-up of the training time w.r.t. the number of workers compared with
    SGD over a single worker. However, such linear scalability in practice is significantly
    limited by the growing demand for gradient communication as more workers are involved.
    Moreover, when batch very large, the stochastic gradients become very close to
    true gradients, so increasing the batch does not give much additional gradient
    information comparing to smaller batches.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 增加批量大小是加速在数据并行硬件上进行神经网络（NN）训练的最具吸引力的方法之一。理想情况下，平行的小批量SGD可以实现训练时间与工人数的线性加速，相比于单工人SGD。然而，实际中，这种线性可扩展性受到梯度通信需求不断增加的显著限制，随着工人数的增加而变得更为明显。此外，当批量非常大时，随机梯度会非常接近真实梯度，因此，与较小批量相比，增加批量并不会提供更多的梯度信息。
- en: 'A series of work has conducted comprehensive experiments on the relationship
    between batch size and training time for NNs (Shallue et al., [2019](#bib.bib93);
    Smith et al., [2020](#bib.bib96); Zhang et al., [2019](#bib.bib124)).  Shallue
    et al. ([2019](#bib.bib93)) experimentally measure the effects of data parallelism
    training across different families of NNs, training algorithms and data sets,
    finding no evidence that larger batch sizes degrade out-of-sample performance.
    They observed three distinct scaling regimes in the relationship between batch
    size and training time: a ”perfect scaling” regime where doubling the batch size
    halves the number of training steps required to reach a target out-of-sample error,
    followed by a regime of ”diminishing returns”, and finally a ”maximal data parallelism”
    regime where further increasing the batch size does not reduce training time,
    even assuming idealized hardware. They also provide experimental evidence that
    the critical batch size depends on the model architecture, the dataset and regulation
    technology.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列研究对批量大小与神经网络训练时间之间的关系进行了全面实验（Shallue等人，[2019](#bib.bib93)；Smith等人，[2020](#bib.bib96)；Zhang等人，[2019](#bib.bib124)）。Shallue等人（[2019](#bib.bib93)）实验测量了不同神经网络家族、训练算法和数据集上的数据并行训练效果，未发现较大的批量大小会降低样本外性能的证据。他们观察到批量大小与训练时间之间有三种不同的扩展模式：一种是“完美扩展”模式，其中批量大小加倍会使达到目标样本外误差所需的训练步骤减少一半，其次是“收益递减”模式，最后是“最大数据并行”模式，在该模式下，即使假设理想化硬件，进一步增加批量大小也不会减少训练时间。他们还提供了实验证据，表明临界批量大小取决于模型架构、数据集和正则化技术。
- en: 5\. Generalization Gap
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 泛化差距
- en: 'Optimization in general is an extremely difficult task, especially for training
    NNs. With non-convex and high-dimensional functions, it is possible to have many
    local minima and saddle points. Optimization methods, such as SGD, generally converge
    to different regions of parameter space, highly dependent on the design of network
    architecture, the choice of optimizer, variable initialization, and a variety
    of other considerations (Shallue et al., [2019](#bib.bib93)). The term generalization
    refers to how well a hypothesis applies even to new examples that it hasn’t seen
    in the training set. As mentioned in Section [4.1](#S4.SS1 "4.1\. Large Batch
    Training Difficulties ‣ 4\. Large Batch Training ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey"), it is observed that while yielding similar values of
    training functions, models trained with large-batch methods perform worse on test
    data compared to small-batch methods (Keskar et al., [2017](#bib.bib53); Hoffer
    et al., [2017](#bib.bib46); Shallue et al., [2019](#bib.bib93); Masters and Luschi,
    [2018](#bib.bib71)). Such persistent degradation in generalization performance
    is referred to as the generalization gap. Identifying the origin of this gap and
    finding ways to close it is of significant practical importance whereas remains
    an open problem.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，优化是一项极其困难的任务，尤其是在训练神经网络时。面对非凸且高维的函数，可能会出现许多局部极小值和鞍点。优化方法，如SGD，通常会收敛到参数空间的不同区域，这在很大程度上依赖于网络架构的设计、优化器的选择、变量初始化以及其他各种因素（Shallue等，[2019](#bib.bib93)）。泛化一词指的是一个假设在未见过的训练集之外的新样本上的适用程度。如第[4.1](#S4.SS1
    "4.1\. 大批量训练困难 ‣ 4\. 大批量训练 ‣ 大规模深度学习优化：综合调查")节所述，尽管大批量训练方法与小批量方法在训练函数上产生了相似的值，但大批量训练的模型在测试数据上的表现却较差（Keskar等，[2017](#bib.bib53)；Hoffer等，[2017](#bib.bib46)；Shallue等，[2019](#bib.bib93)；Masters和Luschi，[2018](#bib.bib71)）。这种持续的泛化性能下降被称为泛化差距。识别这一差距的起源并找到弥合它的方法具有重要的实际意义，但仍然是一个未解的问题。
- en: 'This section is structured as follows. Section [5.1](#S5.SS1 "5.1\. Sharp and
    Flat (Wide) Minima ‣ 5\. Generalization Gap ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey") introduces the concept of sharp and flat (wide) minima;
    Section [5.2](#S5.SS2 "5.2\. Generalization Gap and Sharp Minima ‣ 5\. Generalization
    Gap ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey") addresses
    the relationship between sharpness/flatness of local minima and their generalization
    ability; Section [5.3](#S5.SS3 "5.3\. Gradient Noise Ratio ‣ 5\. Generalization
    Gap ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey") provides
    explanation for the so-called generalization gap and Section [5.4](#S5.SS4 "5.4\.
    Train longer, Generalize Better ‣ 5\. Generalization Gap ‣ Large-Scale Deep Learning
    Optimizations: A Comprehensive Survey") provides a somewhat opposing account.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本节结构如下。第[5.1](#S5.SS1 "5.1\. 锐峰与平坦（宽广）极小值 ‣ 5\. 泛化差距 ‣ 大规模深度学习优化：综合调查")节介绍了锐峰与平坦（宽广）极小值的概念；第[5.2](#S5.SS2
    "5.2\. 泛化差距与锐峰 ‣ 5\. 泛化差距 ‣ 大规模深度学习优化：综合调查")节讨论了局部极小值的锐度/平坦度与其泛化能力之间的关系；第[5.3](#S5.SS3
    "5.3\. 梯度噪声比 ‣ 5\. 泛化差距 ‣ 大规模深度学习优化：综合调查")节解释了所谓的泛化差距，第[5.4](#S5.SS4 "5.4\. 训练时间更长，泛化更好
    ‣ 5\. 泛化差距 ‣ 大规模深度学习优化：综合调查")节则提供了一个相对对立的观点。
- en: 5.1\. Sharp and Flat (Wide) Minima
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 锐峰与平坦（宽广）极小值
- en: 'When training a DL model, we are seeking for a solution that minimizes a loss
    function on a given training set. This solution lies in a very high dimensional
    space (thousands, millions or even billions of parameters to learn) called parameter
    space. The landscape of parameter space is showed empirically crucial to generalize
    well. That being said, the wider the solution’s local geometry, the better the
    generalization (Chaudhari et al., [2017](#bib.bib23); Keskar et al., [2017](#bib.bib53);
    Li et al., [2018](#bib.bib60)). Figure  [2](#S5.F2 "Figure 2 ‣ 5.1\. Sharp and
    Flat (Wide) Minima ‣ 5\. Generalization Gap ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey") provides an intuitive explanation. There is generally
    a shift of the loss function in the parameter space, flat minima is more robust
    to the perturbation of parameter than the sharp one and thus generalizes better.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度学习模型时，我们寻求一个解决方案，该方案能够最小化给定训练集上的损失函数。这个解决方案位于一个非常高维的空间（需要学习的参数有成千上万、甚至亿万），称为参数空间。参数空间的地形对于良好的泛化能力至关重要。换句话说，解决方案的局部几何越宽泛，泛化能力越好 (Chaudhari
    等, [2017](#bib.bib23); Keskar 等, [2017](#bib.bib53); Li 等, [2018](#bib.bib60))。图
    [2](#S5.F2 "图 2 ‣ 5.1\. 尖锐与平坦（宽泛）最小值 ‣ 5\. 泛化误差 ‣ 大规模深度学习优化：全面调查") 提供了直观的解释。一般来说，参数空间中的损失函数会发生变化，平坦的最小值对参数的扰动比尖锐的最小值更具鲁棒性，因此泛化能力更强。
- en: There are various definitions for ”sharpness/flatness” of the landscape. Hochreiter
    and Schmidhuber ([1997](#bib.bib45)) define ”flatness” as a large connected region
    in weight space where the error remains approximately constant. Keskar et al.
    ([2017](#bib.bib53)) characterize ”flatness” by the magnitude of the eigenvalues
    of Hessian, and propose a computational feasible $\epsilon$-sharpness measure.
    Dinh et al. ([2017](#bib.bib30)) show that flat minima in practical DL hypothesis
    spaces can be turned into sharp minima via re-parameterization without affecting
    the generalization gap. Chaudhari et al. ([2017](#bib.bib23)) exploit the local
    geometric properties of the objective function and use ”local entropy” as a measure
    of ”flatness”, which is invariant to the simple re-parametrization in  (Dinh et al.,
    [2017](#bib.bib30)). Foret et al. ([2021](#bib.bib34)) capture the ”sharpness”
    at parameter $w$ by measuring how quickly the training loss can be increased by
    moving from $w$ to a nearby parameter value.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对于地形的“尖锐度/平坦度”有各种定义。Hochreiter 和 Schmidhuber ([1997](#bib.bib45)) 将“平坦度”定义为权重空间中误差大致保持恒定的大区域。Keskar
    等 ([2017](#bib.bib53)) 通过 Hessian 的特征值的大小来描述“平坦度”，并提出了一种计算上可行的 $\epsilon$-尖锐度度量。Dinh
    等 ([2017](#bib.bib30)) 表明，实际深度学习假设空间中的平坦最小值可以通过重新参数化转变为尖锐最小值，而不会影响泛化误差。Chaudhari
    等 ([2017](#bib.bib23)) 利用目标函数的局部几何属性，使用“局部熵”作为“平坦度”的度量，这种度量对 (Dinh 等, [2017](#bib.bib30))
    中的简单重新参数化是不变的。Foret 等 ([2021](#bib.bib34)) 通过测量从参数 $w$ 移动到附近参数值时训练损失增加的速度来捕捉参数
    $w$ 的“尖锐度”。
- en: Empirically, optimizers like SGD, Adam, etc. implicitly converge towards wide
    valleys solutions. But there is no guarantee that this will always be the case.
    This has motivated the creation of algorithms that will actively look for flat
    minima such as Entropy SGD (Chaudhari et al., [2017](#bib.bib23)), Sharpness-Aware
    Minimization (SAM) (Foret et al., [2021](#bib.bib34)) and many others.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从经验上看，像 SGD、Adam 等优化器隐式地收敛到宽阔的谷底解。然而，这并不能保证这种情况总是发生。这促使了积极寻找平坦最小值的算法的创建，如 Entropy
    SGD (Chaudhari 等, [2017](#bib.bib23))、Sharpness-Aware Minimization (SAM) (Foret
    等, [2021](#bib.bib34)) 和其他许多算法。
- en: '![Refer to caption](img/1f75c6b58f9d55912d77b02820bac780.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/1f75c6b58f9d55912d77b02820bac780.png)'
- en: 'Figure 2\. A Conceptual Sketch of Flat and Sharp Minima, src:  (Keskar et al.,
    [2017](#bib.bib53))'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2\. 平坦和尖锐最小值的概念图，来源:  (Keskar 等, [2017](#bib.bib53))'
- en: 5.2\. Generalization Gap and Sharp Minima
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 泛化误差与尖锐最小值
- en: 'With regards to large batch training, Keskar et al. (Keskar et al., [2017](#bib.bib53))
    observed that naively increasing the batch size typically results in degradation
    of generalization performance and reduces computational benefit. They speculate
    that “the lack of generalization ability is due to the fact that large-batch methods
    tend to converge to sharp minima of the training functions”. Specifically, large-batch
    methods are more vulnerable to sharp minima whose training function increases
    rapidly in a relatively small neighborhood (see Fig. [2](#S5.F2 "Figure 2 ‣ 5.1\.
    Sharp and Flat (Wide) Minima ‣ 5\. Generalization Gap ‣ Large-Scale Deep Learning
    Optimizations: A Comprehensive Survey")). Such kind of high susceptibility to
    training functions hampers the trained model from fitting on the test data. By
    contrast, small-batch methods can resist the attraction of these minima and converge
    to a flat minima whose training function varies slowly in a relatively large neighborhood.
    They contribute the success of small batch methods to their noisy gradients in
    the computation step. On one hand, the noise expels the iterations from the trap
    of sharp minima. On the other hand, the noise encourages them to move towards
    and retain in a flatter loss landscape. However, as a larger batch size depicts
    a more accurate gradient, their noise is no longer sufficient enough to eject
    the iterations from the basin of sharp minima.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '关于大批量训练，Keskar等人（Keskar et al., [2017](#bib.bib53)）观察到，简单地增加批量大小通常会导致泛化性能下降，并减少计算收益。他们推测，“缺乏泛化能力是因为大批量方法趋向于收敛到训练函数的尖锐极小值”。具体来说，大批量方法更容易受到尖锐极小值的影响，而这些极小值的训练函数在相对较小的邻域内迅速增加（参见图[2](#S5.F2
    "Figure 2 ‣ 5.1\. Sharp and Flat (Wide) Minima ‣ 5\. Generalization Gap ‣ Large-Scale
    Deep Learning Optimizations: A Comprehensive Survey")）。这种对训练函数的高度敏感性妨碍了训练模型对测试数据的拟合。相比之下，小批量方法可以抵抗这些极小值的吸引，并收敛到训练函数在相对较大邻域内变化缓慢的平坦极小值。他们将小批量方法的成功归因于计算步骤中的噪声。一方面，噪声将迭代从尖锐极小值的陷阱中排除。另一方面，噪声促使它们朝向并保持在更平坦的损失景观中。然而，由于较大批量大小表现出更准确的梯度，它们的噪声不再足以将迭代从尖锐极小值的盆地中排除。'
- en: It is widely thought that small-batch SGD produces “flat” minima that generalize
    well, while large batches converge to “sharp” minima with poor generalization (Hochreiter
    and Schmidhuber, [1997](#bib.bib45); Keskar et al., [2017](#bib.bib53); Chaudhari
    et al., [2017](#bib.bib23)). However, there are some disputes about the effects
    of batch size on model’s generalization ability. Hoffer et al. ([2017](#bib.bib46))
    deny the existence of inherent generalization gap and show empirically that the
    ”generalization gap” stems from the relatively small number of updates rather
    than the batch size, and can be completely eliminated by adapting the number of
    weight updates. Goyal et al. ([2018](#bib.bib38)) hold the view that optimization
    difficulty is the main issue with large mini-batches, rather than the poor generalization
    (at least on ImageNet). Specifically, using linear scaling and warmup strategy,
    they show no loss of accuracy when training with large mini-batch sizes up to
    8,192 images on the ImageNet dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛认为，小批量SGD产生“平坦”的极小值，这些极小值泛化效果较好，而大批量则收敛到“尖锐”的极小值，泛化效果较差（Hochreiter and Schmidhuber,
    [1997](#bib.bib45); Keskar et al., [2017](#bib.bib53); Chaudhari et al., [2017](#bib.bib23)）。然而，关于批量大小对模型泛化能力影响的观点存在一些争议。Hoffer等人（[2017](#bib.bib46)）否认固有的泛化差距存在，并通过实验证明，“泛化差距”源于相对较少的更新次数，而不是批量大小，并且通过调整权重更新次数可以完全消除。Goyal等人（[2018](#bib.bib38)）认为，大批量的主要问题是优化难度，而不是泛化能力差（至少在ImageNet上）。具体来说，通过使用线性缩放和预热策略，他们在ImageNet数据集上训练大批量大小（高达8,192张图像）时未发现准确性损失。
- en: 5.3\. Gradient Noise Ratio
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 梯度噪声比
- en: As mentioned above, how batch size affects sharpness and generalization is controversial.
    Smith and Le ([2018](#bib.bib98)) show that the test accuracy peaks at an optimal
    batch size, if one holds the other hyper-parameters constant. They believe that
    the arise of peak is not controlled by the batch size itself, but the underlying
    scale of random fluctuations in the SGD dynamics.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，批量大小如何影响尖锐度和泛化存在争议。Smith和Le（[2018](#bib.bib98)）显示，如果保持其他超参数不变，测试准确率会在最佳批量大小时达到峰值。他们认为，这个峰值的出现不是由批量大小本身控制的，而是由SGD动态中随机波动的基本规模控制的。
- en: Consider a simple model of SGD; the estimated gradient step is $\eta\nabla_{SGD}(x)=\frac{\eta}{|B|}\sum_{i\in
    B}\nabla l_{i}(x)$, which can be restated as the true gradient and a gradient
    noise term
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个简单的SGD模型；估计的梯度步骤是 $\eta\nabla_{SGD}(x)=\frac{\eta}{|B|}\sum_{i\in B}\nabla
    l_{i}(x)$，这可以重新表述为真实梯度和一个梯度噪声项。
- en: '| (30) |  | $\eta\nabla_{SGD}(x)=\underbrace{\eta\nabla l(x)}_{gradient}+\underbrace{\frac{\eta}{&#124;B&#124;}\sum_{i\in
    B}(\nabla l_{i}(x)-\nabla l(x))}_{noise\,term}.$ |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| (30) |  | $\eta\nabla_{SGD}(x)=\underbrace{\eta\nabla l(x)}_{gradient}+\underbrace{\frac{\eta}{\|B\|}\sum_{i\in
    B}(\nabla l_{i}(x)-\nabla l(x))}_{noise\,term}.$ |  |'
- en: 'Smith and Le ([2018](#bib.bib98)) analogy between SGD and stochastic differential
    equations (SDEs) to describe the noise in the SGD dynamics. In particular, they
    depicted Eq.[[30](#S5.E30 "In 5.3\. Gradient Noise Ratio ‣ 5\. Generalization
    Gap ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey")] as the
    discrete update of a stochastic differential equation (SDE) and derive an analytical
    expression for the stochastic ”noise scale” $g=\eta(\frac{N}{B}-1)\approx\eta\frac{N}{B}$,
    which controls the scale of random fluctuations in the SGD dynamics. Noise drives
    SGD away from sharp minima, and therefore there is an optimal batch size which
    maximizes the test accuracy. This optimal batch size is proportional to the LR
    and training set size $B_{opt}\propto\eta N$. Therefore, they attribute the so-called
    ”generalization gap” observed in  (Dinh et al., [2017](#bib.bib30)) as a consequence
    of scaling batch size above this optimal batch size. Similarly, Jastrz\kebski
    et al. ([2017](#bib.bib49)) derive a ”stochastic noise” using a different SDE.
    They verify experimentally that the ratio of LR to batch size, $\eta/B$, influences
    the width of the minima found by SGD, and that higher values of the ratio lead
    to wider minima and often better generalization. Despite the slightly difference
    in the form of ”stochastic noise”, both (Smith and Le, [2018](#bib.bib98); Jastrz\kebski
    et al., [2017](#bib.bib49)) indicate that gradient noise can be beneficial, especially
    in non-convex optimization. Also they theoretically explain the empirical finding
    in  (Hoffer et al., [2017](#bib.bib46); Goyal et al., [2018](#bib.bib38)) that
    rescaling the LR with the square root of the batch size and train for more epochs,
    or linearly with batch size, can reach the same generalization with a large batch
    size.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Smith和Le ([2018](#bib.bib98)) 使用SGD和随机微分方程（SDEs）的类比来描述SGD动态中的噪声。特别是，他们将方程[[30](#S5.E30
    "在5.3\. 梯度噪声比 ‣ 5\. 泛化差距 ‣ 大规模深度学习优化：全面调查")]描绘为随机微分方程（SDE）的离散更新，并推导出随机”噪声规模” $g=\eta(\frac{N}{B}-1)\approx\eta\frac{N}{B}$，该规模控制SGD动态中的随机波动幅度。噪声使SGD远离尖锐的最小值，因此存在一个最佳批量大小，可以最大化测试准确性。这个最佳批量大小与学习率和训练集大小成正比
    $B_{opt}\propto\eta N$。因此，他们将（Dinh et al., [2017](#bib.bib30)）中观察到的所谓的”泛化差距”归因于批量大小超出此最佳批量大小。类似地，Jastrz\kebski等（[2017](#bib.bib49)）使用不同的SDE推导出一个”随机噪声”。他们通过实验验证了学习率与批量大小的比率
    $\eta/B$ 影响SGD找到的最小值的宽度，并且比率较高会导致更宽的最小值，通常具有更好的泛化能力。尽管”随机噪声”的形式略有不同，（Smith和Le，[2018](#bib.bib98);
    Jastrz\kebski et al., [2017](#bib.bib49)）都指出梯度噪声可能是有益的，尤其是在非凸优化中。他们还从理论上解释了（Hoffer
    et al., [2017](#bib.bib46); Goyal et al., [2018](#bib.bib38)）中的实证发现，即用批量大小的平方根来重新调整学习率，并训练更多的epochs，或线性地与批量大小调整，可以达到与大批量大小相同的泛化效果。
- en: 5.4\. Train longer, Generalize Better
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 训练时间更长，泛化更好
- en: Another observation in  (Keskar et al., [2017](#bib.bib53)) is that large batch
    methods are more likely to be attracted to minima close to the initial point,
    whereas small batch methods are more explorative and always locate minima that
    are farther away, with a ratio of $\lVert x_{S}^{*}-x_{0}\rVert_{2}/\lVert x_{L}^{*}-x_{0}\rVert$
    in the range of 3 to 10. Hoffer et al. ([2017](#bib.bib46)) further find that
    the weight distance from initialization point increases logarithmically with the
    number of training iterations (weight updates), $\lVert w_{t}-w_{0}\rVert\sim
    log\,t$. They therefore deny the existence of inherent generalization gap and
    believe that ”generalization gap” stems from the relatively small number of updates
    rather than the batch size. Specifically, they ”stretched” the time-frame of the
    optimization process, where each time period of $e$ epochs in the original regime
    will be transformed to $\frac{B_{L}}{B_{S}}e$ epochs according to the mini-batch
    size used. However, such modification anneals the speedup effect of large batch
    training.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: (Keskar等人，[2017](#bib.bib53))的另一观察是，大批量方法更容易被吸引到接近初始点的极小值，而小批量方法则更具探索性，通常找到距离较远的极小值，其比例为$\lVert
    x_{S}^{*}-x_{0}\rVert_{2}/\lVert x_{L}^{*}-x_{0}\rVert$在3到10的范围内。Hoffer等人（[2017](#bib.bib46)）进一步发现，权重从初始化点的距离随着训练迭代次数（权重更新）的增加而对数增加，即$\lVert
    w_{t}-w_{0}\rVert\sim log\,t$。因此，他们否认了固有泛化差距的存在，并认为“泛化差距”来源于更新次数相对较少，而非批量大小。具体来说，他们将优化过程的时间框架“拉伸”了，其中原始模式下的每个时间周期$e$个周期将根据所使用的迷你批量大小转换为$\frac{B_{L}}{B_{S}}e$个周期。然而，这种修改削弱了大批量训练的加速效果。
- en: 6\. Second Order Optimization
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 二阶优化
- en: 'Optimizations in DL, both theoretically and empirically, are presently dominated
    by first-order gradient methods (Agarwal et al., [2016b](#bib.bib4), [a](#bib.bib3);
    Bollapragada et al., [2016](#bib.bib16); Carmon et al., [2018](#bib.bib22); Conn
    et al., [2000](#bib.bib27); Xu et al., [2020](#bib.bib117)). Second-order optimization
    methods that involve second derivatives and/or second order statistics of the
    data, are far less prevalent despite strong theoretical properties, due to their
    prohibitive computation, memory and communication costs. In this section, we setup
    second-order optimization basics in Section [6.1](#S6.SS1 "6.1\. Second-Order
    Optimization Basics ‣ 6\. Second Order Optimization ‣ Large-Scale Deep Learning
    Optimizations: A Comprehensive Survey"), start from the classical Newton’s method
    in Section [6.2](#S6.SS2 "6.2\. Newton’s Method ‣ 6\. Second Order Optimization
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey"), and turn
    to some up-to-date algorithms such as the Hessian-Free Method (in Section [6.3](#S6.SS3
    "6.3\. Hessian-Free Method ‣ 6\. Second Order Optimization ‣ Large-Scale Deep
    Learning Optimizations: A Comprehensive Survey")), K-FAC (in Section [6.4](#S6.SS4
    "6.4\. K-FAC ‣ 6\. Second Order Optimization ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey")) and Shampoo (Gupta et al., [2018](#bib.bib41)) (in Section [6.5](#S6.SS5
    "6.5\. Shampoo ‣ 6\. Second Order Optimization ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey")).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的优化，无论在理论上还是经验上，目前都被一阶梯度方法主导（Agarwal等人，[2016b](#bib.bib4)，[a](#bib.bib3)；Bollapragada等人，[2016](#bib.bib16)；Carmon等人，[2018](#bib.bib22)；Conn等人，[2000](#bib.bib27)；Xu等人，[2020](#bib.bib117)）。涉及二阶导数和/或数据的二阶统计量的二阶优化方法，尽管具有强大的理论特性，但由于其计算、内存和通信成本高昂，应用远不如一阶方法。在本节中，我们在第[6.1](#S6.SS1
    "6.1\. 二阶优化基础 ‣ 6\. 二阶优化 ‣ 大规模深度学习优化：综合调查")节中设置了二阶优化基础，从经典的牛顿法（第[6.2](#S6.SS2
    "6.2\. 牛顿法 ‣ 6\. 二阶优化 ‣ 大规模深度学习优化：综合调查")节）开始，然后转向一些最新算法，如Hessian-Free方法（第[6.3](#S6.SS3
    "6.3\. Hessian-Free方法 ‣ 6\. 二阶优化 ‣ 大规模深度学习优化：综合调查")节），K-FAC（第[6.4](#S6.SS4 "6.4\.
    K-FAC ‣ 6\. 二阶优化 ‣ 大规模深度学习优化：综合调查")节）和Shampoo（Gupta等人，[2018](#bib.bib41)）（第[6.5](#S6.SS5
    "6.5\. Shampoo ‣ 6\. 二阶优化 ‣ 大规模深度学习优化：综合调查")节）。
- en: 6.1\. Second-Order Optimization Basics
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 二阶优化基础
- en: Basically, many problems in machine learning can be simply described as minimizing
    the loss function over variables $x\in\mathbb{R}^{d}$
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，机器学习中的许多问题可以简单描述为在变量$x\in\mathbb{R}^{d}$上最小化损失函数。
- en: '| (31) |  | $\mathop{min}\limits_{x\in{\mathbb{R}}^{d}}F(x).$ |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| (31) |  | $\mathop{min}\limits_{x\in{\mathbb{R}}^{d}}F(x).$ |  |'
- en: When training the weights of a NN, we are trying to get as far down the error
    surface as possible. In most cases, we often use SGD to update the parameter vector
    to solve this optimization problem
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络的权重时，我们尝试尽可能降低误差面。在大多数情况下，我们经常使用SGD来更新参数向量以解决这个优化问题。
- en: '| (32) |  | $\begin{split}x_{t+1}&amp;=x_{t}-\eta_{t}g_{t}.\end{split}$ |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| (32) |  | $\begin{split}x_{t+1}&amp;=x_{t}-\eta_{t}g_{t}.\end{split}$ |  |'
- en: Another very popular family of algorithms used in practice are the adaptive
    optimization algorithms (e.g., AdaGrad (Duchi et al., [2011](#bib.bib33)), Adadelta (Zeiler,
    [2012](#bib.bib123)), RMSProp (Tieleman and Hinton, [2012](#bib.bib108)), Adam (Kingma
    and Ba, [2017](#bib.bib54)), etc.). These are basically algorithms that update
    for each individual entry in the parameter vector. Each entry has its own step
    size which is an adaptive update using past gradients.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在实践中非常流行的算法家族是自适应优化算法（例如，AdaGrad（Duchi等，[2011](#bib.bib33)），Adadelta（Zeiler，[2012](#bib.bib123)），RMSProp（Tieleman和Hinton，[2012](#bib.bib108)），Adam（Kingma和Ba，[2017](#bib.bib54)）等）。这些基本上是更新参数向量中每个单独条目的算法。每个条目都有自己的步长，这是使用过去梯度的自适应更新。
- en: '| (33) |  | $x_{t+1}^{(i)}=x_{t}^{(i)}-\eta_{t}^{(i)}g_{t}^{(i)}\quad i=1,...,d$
    |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| (33) |  | $x_{t+1}^{(i)}=x_{t}^{(i)}-\eta_{t}^{(i)}g_{t}^{(i)}\quad i=1,...,d$
    |  |'
- en: And there are also momentum variants of these methods that have slightly different
    update rules. Potentially more powerful family of algorithms are known as the
    preconditioned algorithms which use some matrices called preconditioners to transform
    the gradient before taking a step. In general, the idea of second-order optimization
    is to model the objective function $f$ by the local approximation
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法还有动量变体，具有略微不同的更新规则。一个潜在的更强大的算法家族被称为预处理算法，它们使用一些叫做预处理器的矩阵来转换梯度，然后再进行一步。一般来说，二阶优化的思想是通过局部近似来建模目标函数$f$。
- en: '| (34) |  | $f(x+\delta)\approx M(x)\equiv f(x)+\nabla f(x)^{T}\delta+\frac{1}{2}\delta^{T}B(x)\delta.$
    |  |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| (34) |  | $f(x+\delta)\approx M(x)\equiv f(x)+\nabla f(x)^{T}\delta+\frac{1}{2}\delta^{T}B(x)\delta.$
    |  |'
- en: Here, $B$ is a symmetric preconditioner and $\delta$ is the change in parameters.
    In Newton’s method, $B=H$, or $B=H+\lambda I$. Fully optimizing $M(x_{t})$ w.r.t.
    $\delta$ gives
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$B$是对称预处理器，$\delta$是参数的变化。在牛顿法中，$B=H$，或者$B=H+\lambda I$。完全优化$M(x_{t})$相对于$\delta$得到
- en: '| (35) |  | $\delta^{*}=\mathop{argmin}_{\delta}M(x_{t})=-B^{-1}\nabla f,$
    |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| (35) |  | $\delta^{*}=\mathop{argmin}_{\delta}M(x_{t})=-B^{-1}\nabla f,$
    |  |'
- en: then apply the update
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然后应用更新
- en: '| (36) |  | $x_{t+1}=x_{t}+\delta^{*}.$ |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| (36) |  | $x_{t+1}=x_{t}+\delta^{*}.$ |  |'
- en: 'This family includes algorithms mentioned above such as AdaGrad (Duchi et al.,
    [2011](#bib.bib33)), Adam (Kingma and Ba, [2017](#bib.bib54)), where the preconditioners
    are diagonal. But they can be more powerful if the metrics are not diagonal but
    full preconditioners, for exapmle, full AdaGrad, Natural Gradient (Amari, [1998](#bib.bib8))
    and also classical algorithms like Newton’s method, Quasi-Newton methods (Schraudolph
    et al., [2007](#bib.bib90); Goldfarb et al., [2020](#bib.bib37)) and so on. It
    is well-known in optimization that preconditioning often leads to faster convergence
    or better ”condition number” in many different scenarios. But it comes with obvious
    caveats: supposing the number of parameter is $n$, we need (1) at least quadratic
    space $\Omega(n^{2})$ in the dimension to store the preconditioner. (2) $n^{3}$
    time to invert the preconditioner to apply to the gradient vector. Generally speaking,
    these second-order methods are not very practical where the cost of computation
    and memory is formidable in the DL settings. Alternatively practitioners use the
    diagonal approximation or using the SGD again.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个家族包括上面提到的算法，如AdaGrad（Duchi等，[2011](#bib.bib33)），Adam（Kingma和Ba，[2017](#bib.bib54)），其中预处理器是对角的。但如果度量不是对角的而是全预处理器，例如，全AdaGrad，自然梯度（Amari，[1998](#bib.bib8)），以及经典算法如牛顿法，准牛顿方法（Schraudolph等，[2007](#bib.bib90)；Goldfarb等，[2020](#bib.bib37)）等，它们可能更强大。优化中众所周知，预处理通常能在许多不同场景下加快收敛速度或提高“条件数”。但也有明显的警告：假设参数的数量为$n$，我们需要（1）至少二次空间$\Omega(n^{2})$来存储预处理器。（2）$n^{3}$时间来求逆预处理器以应用于梯度向量。一般来说，这些二阶方法在DL设置中计算和内存开销巨大，不太实用。实践中，通常使用对角近似或再次使用SGD。
- en: Recently, there has been considerable advancement in the development of second-order
    methods, seeking a balance between between full matrics and the diagonal case.
    These methods usually approach preconditioners of the gradient in a modular way,
    which is as powerful (or nearly powerful) as the full matrix case, but can be
    used in practical like the diagonal case in terms of storage and run-time. Inspired
    by the idea of the natural gradient method (Amari et al., [2000](#bib.bib9)),
    Martens and Grosse ([2015](#bib.bib70)) use a Kronecker-factored approximation
    to the Fisher matrix as its preconditioning matrix that can be applied to multi-layer
    perceptrons (MLPs), which was subsequently extended to other architectures, such
    as convolutional neural networks (CNNs)  (Grosse and Martens, [2016](#bib.bib40))
    and recurrent neural networks (RNNs) (Osawa et al., [2019](#bib.bib76)). Kronecker-factored
    preconditioners based on the structure of the Hessian and quasi-Newton methods
    have also been developed  (Goldfarb et al., [2020](#bib.bib37); Ren and Goldfarb,
    [2021](#bib.bib87)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，二阶方法的发展取得了显著进展，寻求在全矩阵和对角矩阵之间找到平衡。这些方法通常以模块化的方式处理梯度的预条件，这种方法与全矩阵的效果一样强大（或几乎一样强大），但在存储和运行时间方面可以像对角矩阵那样实用。受到自然梯度方法的启发（Amari
    et al., [2000](#bib.bib9)），Martens 和 Grosse ([2015](#bib.bib70)) 使用了 Kronecker
    分解的 Fisher 矩阵近似作为预条件矩阵，这种方法可以应用于多层感知器（MLPs），随后也扩展到其他架构，例如卷积神经网络（CNNs）（Grosse 和
    Martens，[2016](#bib.bib40)）和递归神经网络（RNNs）（Osawa et al., [2019](#bib.bib76)）。基于
    Hessian 结构和准 Newton 方法的 Kronecker 分解预条件器也已被开发（Goldfarb et al., [2020](#bib.bib37);
    Ren 和 Goldfarb，[2021](#bib.bib87)）。
- en: 6.2\. Newton’s Method
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2. 牛顿方法
- en: Recall that in GD method, the gradient of a function is defined as the vector
    of partial derivatives.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在梯度下降方法中，函数的梯度被定义为偏导数的向量。
- en: '| (37) |  | $g_{t}\triangleq\nabla f(x)=<\frac{\partial f}{x_{1}},\frac{\partial
    f}{x_{2}},\cdots,\frac{\partial f}{x_{n}}>$ |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| (37) |  | $g_{t}\triangleq\nabla f(x)=<\frac{\partial f}{x_{1}},\frac{\partial
    f}{x_{2}},\cdots,\frac{\partial f}{x_{n}}>$ |  |'
- en: It means we are assuming that the error surface of the NNs locally looks and
    behaves like a circle. And we are ignoring all curvatures of the surface, which
    may lead our training to progress very slowly. To rectify this, we can use information
    from the second derivative of a function. The idea of Newton’s method is to apply
    a linear transformation that turns ellipses into circles. If we apply that transformation
    to the gradient vector, it will be as if we were going downhill in a circular
    error surface. Formally, Newton’s method use the Hessian matrix as preconditioner
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们假设神经网络的误差面在局部看起来和行为像一个圆形。我们忽略了表面的所有曲率，这可能导致我们的训练进展非常缓慢。为了解决这个问题，我们可以使用函数的二阶导数信息。牛顿方法的思想是应用线性变换将椭圆变为圆形。如果我们将这种变换应用于梯度向量，它就像是在圆形误差面上向下坡行进。形式上，牛顿方法使用
    Hessian 矩阵作为预条件矩阵。
- en: '| (38) |  | <math   alttext="H=\left[\begin{array}[]{cccc}\frac{\partial^{2}f}{\partial
    x_{1}^{2}}&amp;\frac{\partial^{2}f}{\partial x_{1}\partial x_{2}}&amp;\cdots&amp;\frac{\partial^{2}f}{\partial
    x_{1}\partial x_{n}}\\ \frac{\partial^{2}f}{\partial x_{2}\partial x_{1}}&amp;\frac{\partial^{2}f}{\partial
    x_{2}^{2}}&amp;\cdots&amp;\frac{\partial^{2}f}{\partial x_{2}\partial x_{n}}\\'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '| (38) |  | <math   alttext="H=\left[\begin{array}[]{cccc}\frac{\partial^{2}f}{\partial
    x_{1}^{2}}&amp;\frac{\partial^{2}f}{\partial x_{1}\partial x_{2}}&amp;\cdots&amp;\frac{\partial^{2}f}{\partial
    x_{1}\partial x_{n}}\\ \frac{\partial^{2}f}{\partial x_{2}\partial x_{1}}&amp;\frac{\partial^{2}f}{\partial
    x_{2}^{2}}&amp;\cdots&amp;\frac{\partial^{2}f}{\partial x_{2}\partial x_{n}}\\'
- en: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
- en: \frac{\partial^{2}f}{\partial x_{n}\partial x_{1}}&amp;\frac{\partial^{2}f}{\partial
    x_{n}\partial x_{2}}&amp;\cdots&amp;\frac{\partial^{2}f}{\partial x_{n}^{2}}\end{array}\right],\quad
    H_{ij}=\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}}." display="block"><semantics
    ><mrow ><mrow  ><mrow ><mi >H</mi><mo  >=</mo><mrow ><mo >[</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt"  ><mtr ><mtd  ><mstyle displaystyle="false"  ><mfrac
    ><mrow ><msup  ><mo >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow ><mo rspace="0em"
    >∂</mo><msubsup ><mi >x</mi><mn >1</mn><mn >2</mn></msubsup></mrow></mfrac></mstyle></mtd><mtd
    ><mstyle displaystyle="false" ><mfrac  ><mrow ><msup ><mo  >∂</mo><mn >2</mn></msup><mi
    >f</mi></mrow><mrow ><mo rspace="0em" >∂</mo><mrow ><msub  ><mi >x</mi><mn >1</mn></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo rspace="0em" >∂</mo><msub ><mi >x</mi><mn
    >2</mn></msub></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd ><mi mathvariant="normal"
    >⋯</mi></mtd><mtd  ><mstyle displaystyle="false"  ><mfrac ><mrow ><msup  ><mo
    >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow ><mo rspace="0em" >∂</mo><mrow
    ><msub  ><mi >x</mi><mn >1</mn></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo rspace="0em" >∂</mo><msub ><mi >x</mi><mi >n</mi></msub></mrow></mrow></mrow></mfrac></mstyle></mtd></mtr><mtr
    ><mtd  ><mstyle displaystyle="false"  ><mfrac ><mrow ><msup  ><mo >∂</mo><mn >2</mn></msup><mi
    >f</mi></mrow><mrow ><mo rspace="0em" >∂</mo><mrow ><msub  ><mi >x</mi><mn >2</mn></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo rspace="0em" >∂</mo><msub ><mi >x</mi><mn
    >1</mn></msub></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd ><mstyle displaystyle="false"
    ><mfrac  ><mrow ><msup ><mo  >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow
    ><mo rspace="0em" >∂</mo><msubsup ><mi >x</mi><mn >2</mn><mn >2</mn></msubsup></mrow></mfrac></mstyle></mtd><mtd
    ><mi mathvariant="normal" >⋯</mi></mtd><mtd  ><mstyle displaystyle="false"  ><mfrac
    ><mrow ><msup  ><mo >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow ><mo rspace="0em"
    >∂</mo><mrow ><msub  ><mi >x</mi><mn >2</mn></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo rspace="0em" >∂</mo><msub ><mi >x</mi><mi >n</mi></msub></mrow></mrow></mrow></mfrac></mstyle></mtd></mtr><mtr
    ><mtd  ><mi mathvariant="normal"  >⋮</mi></mtd><mtd ><mi mathvariant="normal"
    >⋮</mi></mtd><mtd  ><mi mathvariant="normal"  >⋱</mi></mtd><mtd ><mi mathvariant="normal"
    >⋮</mi></mtd></mtr><mtr  ><mtd ><mstyle displaystyle="false" ><mfrac  ><mrow ><msup
    ><mo  >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow ><mo rspace="0em" >∂</mo><mrow
    ><msub  ><mi >x</mi><mi >n</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo rspace="0em" >∂</mo><msub ><mi >x</mi><mn >1</mn></msub></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd
    ><mstyle displaystyle="false" ><mfrac  ><mrow ><msup ><mo  >∂</mo><mn >2</mn></msup><mi
    >f</mi></mrow><mrow ><mo rspace="0em" >∂</mo><mrow ><msub  ><mi >x</mi><mi >n</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo rspace="0em" >∂</mo><msub ><mi >x</mi><mn
    >2</mn></msub></mrow></mrow></mrow></mfrac></mstyle></mtd><mtd ><mi mathvariant="normal"
    >⋯</mi></mtd><mtd  ><mstyle displaystyle="false"  ><mfrac ><mrow ><msup  ><mo
    >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow ><mo rspace="0em" >∂</mo><msubsup
    ><mi >x</mi><mi >n</mi><mn >2</mn></msubsup></mrow></mfrac></mstyle></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow><mo rspace="1.167em"  >,</mo><mrow ><msub ><mi  >H</mi><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow></msub><mo
    >=</mo><mfrac ><mrow  ><msup ><mo >∂</mo><mn >2</mn></msup><mi >f</mi></mrow><mrow
    ><mo rspace="0em" >∂</mo><mrow ><msub ><mi >x</mi><mi >i</mi></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo rspace="0em"  >∂</mo><msub ><mi >x</mi><mi >j</mi></msub></mrow></mrow></mrow></mfrac></mrow></mrow><mo
    lspace="0em" >.</mo></mrow><annotation-xml encoding="MathML-Content" ><apply  ><csymbol
    cd="ambiguous"  >formulae-sequence</csymbol><apply ><ci >𝐻</ci><apply ><csymbol
    cd="latexml" >delimited-[]</csymbol><matrix ><matrixrow  ><apply ><apply ><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><cn type="integer"  >2</cn></apply><ci >𝑓</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑥</ci><cn type="integer" >1</cn></apply><cn type="integer"
    >2</cn></apply></apply></apply><apply ><apply  ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><cn type="integer"  >2</cn></apply><ci >𝑓</ci></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑥</ci><cn
    type="integer"  >1</cn></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><cn type="integer" >2</cn></apply></apply></apply></apply></apply><ci >⋯</ci><apply
    ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer"  >2</cn></apply><ci
    >𝑓</ci></apply><apply ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><cn type="integer"  >1</cn></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑥</ci><ci >𝑛</ci></apply></apply></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer"  >2</cn></apply><ci
    >𝑓</ci></apply><apply ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><cn type="integer"  >2</cn></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑥</ci><cn type="integer" >1</cn></apply></apply></apply></apply></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><cn type="integer"  >2</cn></apply><ci
    >𝑓</ci></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><cn type="integer" >2</cn></apply><cn
    type="integer" >2</cn></apply></apply></apply><ci >⋯</ci><apply  ><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer" >2</cn></apply><ci
    >𝑓</ci></apply><apply ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><cn type="integer"  >2</cn></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑥</ci><ci >𝑛</ci></apply></apply></apply></apply></apply></matrixrow><matrixrow
    ><ci >⋮</ci><ci  >⋮</ci><ci >⋱</ci><ci >⋮</ci></matrixrow><matrixrow ><apply  ><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer" >2</cn></apply><ci
    >𝑓</ci></apply><apply ><apply  ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><ci >𝑛</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><cn type="integer" >1</cn></apply></apply></apply></apply></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><cn type="integer"  >2</cn></apply><ci
    >𝑓</ci></apply><apply ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑥</ci><ci >𝑛</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑥</ci><cn type="integer" >2</cn></apply></apply></apply></apply></apply><ci >⋯</ci><apply
    ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn type="integer"  >2</cn></apply><ci
    >𝑓</ci></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><ci >𝑛</ci></apply><cn
    type="integer" >2</cn></apply></apply></apply></matrixrow></matrix></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐻</ci><apply ><ci  >𝑖</ci><ci
    >𝑗</ci></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><cn
    type="integer" >2</cn></apply><ci >𝑓</ci></apply><apply ><apply ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><ci >𝑖</ci></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑥</ci><ci >𝑗</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >H=\left[\begin{array}[]{cccc}\frac{\partial^{2}f}{\partial
    x_{1}^{2}}&\frac{\partial^{2}f}{\partial x_{1}\partial x_{2}}&\cdots&\frac{\partial^{2}f}{\partial
    x_{1}\partial x_{n}}\\ \frac{\partial^{2}f}{\partial x_{2}\partial x_{1}}&\frac{\partial^{2}f}{\partial
    x_{2}^{2}}&\cdots&\frac{\partial^{2}f}{\partial x_{2}\partial x_{n}}\\ \vdots&\vdots&\ddots&\vdots\\
    \frac{\partial^{2}f}{\partial x_{n}\partial x_{1}}&\frac{\partial^{2}f}{\partial
    x_{n}\partial x_{2}}&\cdots&\frac{\partial^{2}f}{\partial x_{n}^{2}}\end{array}\right],\quad
    H_{ij}=\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}}.</annotation></semantics></math>
    |  |
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: \[ H = \left[\begin{array}{cccc}\frac{\partial^{2}f}{\partial x_{1}^{2}}&\frac{\partial^{2}f}{\partial
    x_{1}\partial x_{2}}&\cdots&\frac{\partial^{2}f}{\partial x_{1}\partial x_{n}}\\
    \frac{\partial^{2}f}{\partial x_{2}\partial x_{1}}&\frac{\partial^{2}f}{\partial
    x_{2}^{2}}&\cdots&\frac{\partial^{2}f}{\partial x_{2}\partial x_{n}}\\ \vdots&\vdots&\ddots&\vdots\\
    \frac{\partial^{2}f}{\partial x_{n}\partial x_{1}}&\frac{\partial^{2}f}{\partial
    x_{n}\partial x_{2}}&\cdots&\frac{\partial^{2}f}{\partial x_{n}^{2}}\end{array}\right],
    \quad H_{ij}=\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}}. \]
- en: The Hessian is a function of the parameters and we need to take its inverse
    and multiply the gradient by that. Then we need to go some distance in that direction
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 海森矩阵是参数的一个函数，我们需要取它的逆，并将梯度乘以该逆矩阵。然后我们需要沿着这个方向前进一段距离。
- en: '| (39) |  | $\Delta x=-\eta H(x)^{-1}\nabla f(x).$ |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| (39) |  | $\Delta x=-\eta H(x)^{-1}\nabla f(x).$ |  |'
- en: If it is a truly quadratic surface and we choose the LR correctly, we will arrive
    at the minima of the surface in a single step. However, that single step involves
    something complicated which is inverting that Hessian matrix. Assuming that we
    only have a million parameters in our NN, the Hessian matrix will have a trillion
    terms which is completely infeasible to invert.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一个真正的二次曲面，并且我们正确选择了LR，我们将只需一步就能到达曲面的最小值。然而，这一步涉及一些复杂的操作，即反转海森矩阵。假设我们在神经网络中只有一百万个参数，海森矩阵将有一万亿个项，反转它完全不可行。
- en: Curvature Matrices. Each element in the curvature matrix specifies how the gradient
    in one direction changes as we move in some other direction. The off-diagonal
    terms in a curvature matrix correspond to ”twists” in the error surface. A twist
    means that when you travel in one direction, the gradient in another direction
    changes. If we have a nice circular bulb, all those off-diagonal terms are zero.
    As we travel in one direction, the gradient in other directions doesn’t change.
    But when we have an elliptical error surface, as we travel in one direction, the
    gradient in another direction changes. This is actually what is going wrong with
    GD. As GD updates one of the weights, at the same time it is updating all the
    other weights, causing a change in the gradient for the first weight. That means
    when we update it we may actually make things worse. The gradient may have actually
    reversed sign due to the changes in all the other weights. And so the more weights
    we get, the more cautious about changing each one of them we need to be, because
    the simultaneous changes in all the other weights can change the gradient of a
    weight.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 曲率矩阵。曲率矩阵中的每个元素指定了当我们在某个方向上移动时，梯度在另一个方向上如何变化。曲率矩阵中的非对角线项对应于误差曲面的“扭曲”。扭曲意味着当你在一个方向上移动时，另一个方向上的梯度发生变化。如果我们有一个理想的圆形曲面，那么所有这些非对角线项都是零。当我们在一个方向上移动时，其他方向上的梯度不会变化。但当我们有一个椭圆形的误差曲面时，当我们在一个方向上移动时，另一个方向上的梯度会发生变化。这实际上就是GD出现问题的原因。当GD更新一个权重时，同时也在更新所有其他权重，导致第一个权重的梯度发生变化。这意味着当我们更新它时，可能会使情况变得更糟。由于所有其他权重的变化，梯度的方向可能会反转。因此，随着权重数量的增加，我们需要对每个权重的更改保持更加谨慎，因为所有其他权重的同时变化可能会改变一个权重的梯度。
- en: How to avoid inverting a huge matrix. The intensive computation of the curvature
    has limited the applications of second-order optimization methods in DL settings.
    To address this problem, there are various ideas in the literature. One very popular
    line of work looks at diagonal approximations, e.g., Adagrad (Duchi et al., [2011](#bib.bib33)),
    RMSProp(Tieleman and Hinton, [2012](#bib.bib108)), Adadelta (Zeiler, [2012](#bib.bib123))
    and many others (Botev et al., [2017](#bib.bib18); Bordes et al., [2009](#bib.bib17)).
    But these diagonal terms consist only a tiny fraction of the interactions, so
    we are ignoring most of the terms (nearly all of them) in the curvature matrix.
    And the experimental evidence indicates that there is limited or almost no improvement
    in practice when compared to well-tuned SGD with or without momentum (see  (Zeiler,
    [2012](#bib.bib123); Botev et al., [2017](#bib.bib18))). The benefits of these
    diagonal approaches seem to lie mainly in the ease of choosing the LR, but may
    not provide any fundamental benefits beyond that. Another thing we could do is
    to approximate the curvature matrix with much lower rank matrix but capturing
    its main aspects. Limited-memory BFGS (L-BFGS) (Schraudolph et al., [2007](#bib.bib90))
    is the most well-known example. Again there is limited/non-existent empirical
    success for NN optimization.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如何避免反转巨大的矩阵。曲率的密集计算限制了二阶优化方法在深度学习设置中的应用。为了解决这个问题，文献中提出了各种想法。其中一种非常受欢迎的工作方向是看对角线近似，例如
    Adagrad (Duchi et al., [2011](#bib.bib33))，RMSProp(Tieleman 和 Hinton, [2012](#bib.bib108))，Adadelta (Zeiler,
    [2012](#bib.bib123)) 和许多其他方法 (Botev et al., [2017](#bib.bib18); Bordes et al.,
    [2009](#bib.bib17))。但是这些对角线项仅占交互作用的极小一部分，因此我们忽略了曲率矩阵中的大多数项（几乎所有项）。实验证据表明，与调整良好的SGD（无论是否有动量）相比，实际应用中几乎没有改进（见(Zeiler,
    [2012](#bib.bib123); Botev et al., [2017](#bib.bib18))）。这些对角线方法的好处似乎主要在于选择学习率的便利性，但可能没有提供超出这一点的根本性好处。我们还可以做的是用较低秩的矩阵来近似曲率矩阵，但要捕捉其主要方面。有限记忆BFGS
    (L-BFGS) (Schraudolph et al., [2007](#bib.bib90)) 是最著名的例子。对于神经网络优化，再次存在有限/几乎不存在的经验成功。
- en: More recently, many focus are put on the trade-off between the full matrix and
    the diagonal case, seeking a balance between these two extreme case. Some researchers
    find something in between that is as powerful (or nearly powerful) as the full
    matrix case, but can be used in practical like the diagonal case in terms of storage
    and run-time. Some recent approaches for approximating a full-matrix preconditioner
    are K-FAC (Martens and Grosse, [2015](#bib.bib70)) and Shampoo (Gupta et al.,
    [2018](#bib.bib41)). Others incorporate automatically the Hessian operator, such
    as Hessian-Free method (Martens, [2010](#bib.bib68)) and trust-region (Conn et al.,
    [2000](#bib.bib27); Xu et al., [2020](#bib.bib117)).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多研究集中在全矩阵和对角线情况之间的权衡，寻求这两种极端情况之间的平衡。一些研究人员发现某些介于两者之间的东西，既强大（或几乎强大）又可以像对角线情况一样在存储和运行时方面实用。一些最近的全矩阵预条件子近似方法包括K-FAC (Martens
    和 Grosse, [2015](#bib.bib70)) 和 Shampoo (Gupta et al., [2018](#bib.bib41))。其他方法自动纳入Hessian算子，如Hessian-Free方法 (Martens,
    [2010](#bib.bib68)) 和信任域方法 (Conn et al., [2000](#bib.bib27); Xu et al., [2020](#bib.bib117))。
- en: 6.3\. Hessian-Free Method
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 无Hessian方法
- en: 'The Hessian-Free method  (Martens, [2010](#bib.bib68)) is a quasi-Newton method
    that uses no low-rank approximations. Named ”free” because it never explicitly
    computes the preconditioner $B$ but instead does approximate minimization of quadratic
    model $M(\delta)$ (see Eq.[[35](#S6.E35 "In 6.1\. Second-Order Optimization Basics
    ‣ 6\. Second Order Optimization ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey")]). The Hessian-Free (HF) method is motivated by two observations. The
    first one being that it is relatively easy to compute the matrix-vector product
    $Hv$ for an arbitrary vectors $v$, e.g., use finite differences to approximate
    the limit.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian-Free方法 (Martens, [2010](#bib.bib68)) 是一种准牛顿方法，不使用低秩近似。被称为“自由”，因为它从不显式计算预条件子
    $B$，而是对二次模型 $M(\delta)$ 进行近似最小化（见 Eq.[[35](#S6.E35 "在6.1\. 二阶优化基础 ‣ 6\. 二阶优化 ‣
    大规模深度学习优化：全面综述")]）。Hessian-Free (HF) 方法的动机来自两个观察。第一个是对于任意向量 $v$，计算矩阵-向量积 $Hv$
    相对容易，例如，使用有限差分来近似极限。
- en: '| (40) |  | $Hv=\mathop{lim}\limits_{\epsilon\rightarrow 0}\frac{\nabla f(x+\epsilon
    v)-f(x)}{\epsilon}$ |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| (40) |  | $Hv=\mathop{lim}\limits_{\epsilon\rightarrow 0}\frac{\nabla f(x+\epsilon
    v)-f(x)}{\epsilon}$ |  |'
- en: 'The second motivating observation is that linear conjugate gradient (CG) minimizes
    positive definite quadratic cost functions using only matrix-vector products,
    which is relatively easy to obtained (as shown in Eq.[[40](#S6.E40 "In 6.3\. Hessian-Free
    Method ‣ 6\. Second Order Optimization ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey")]). Conjugate gradient is a very clever method that instead
    of trying to go straight to the minimum like in Newton’s method, it tries to minimize
    in one direction at a time. It starts off by taking the direction of GD and goes
    to the minimum in that direction that might involve re-evaluating the gradient
    or re-evaluating the error a few times to find the minimum in that direction.
    Once it is done, CG method now finds another direction and goes to the minimum
    in that second direction. The clever thing about the technique is that it chooses
    the second direction in such a way that doesn’t mess up the minimization it already
    did in the first direction, which is called a conjugate direction. ”Conjugate”
    means that as we go in the new direction we do not change the gradients in the
    previous directions. What CG achieves is that it gets to the global minimum of
    an $n$-dimensional quadratic surface in only $n$ steps. More importantly, in many
    less than $n$ steps on a typical quadratic surface, it will have reduced the error
    very close to the minimum value. And that’s why we use it. As doing the full $n$
    steps that would be as expensive as inverting the whole matrix, we are going to
    do many less than $n$ steps and get quite close to the minimum. Pseudo-code for
    a simple variant of damped HF optimization is provided in Algorithm [4](#alg4
    "In 6.3\. Hessian-Free Method ‣ 6\. Second Order Optimization ‣ Large-Scale Deep
    Learning Optimizations: A Comprehensive Survey").'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个激励观察是线性共轭梯度（CG）使用仅矩阵-向量乘积来最小化正定的二次代价函数，这相对容易获得（如公式 [[40](#S6.E40 "在 6.3\.
    海森矩阵自由方法 ‣ 6\. 二阶优化 ‣ 大规模深度学习优化：全面调查") ] 所示）。共轭梯度是一种非常巧妙的方法，它不像牛顿法那样试图直接到达最小值，而是尝试一次在一个方向上最小化。它从GD方向开始，并在该方向上找到最小值，这可能涉及重新评估梯度或误差几次，以找到该方向上的最小值。一旦完成，共轭梯度方法会找到另一个方向，并在第二个方向上找到最小值。该技术的巧妙之处在于它选择第二个方向的方式不会扰乱在第一个方向上已经完成的最小化，这称为共轭方向。“共轭”意味着我们在新方向上前进时不会改变先前方向上的梯度。CG方法的结果是它仅用
    $n$ 步就能到达 $n$ 维二次曲面的全局最小值。更重要的是，在典型的二次曲面上，它将误差减少到非常接近最小值的水平，往往少于 $n$ 步。这就是我们使用它的原因。因为执行全部
    $n$ 步将像求逆整个矩阵一样昂贵，所以我们将执行少于 $n$ 步，并接近最小值。简单的阻尼HF优化变体的伪代码见算法 [4](#alg4 "在 6.3\.
    海森矩阵自由方法 ‣ 6\. 二阶优化 ‣ 大规模深度学习优化：全面调查")。
- en: for *$n=1$ to max-epochs* do       compute gradient $g_{t}=\nabla f(x_{t})$;      
    choose/adapt $\eta_{t}$ according to some heuristic;       define the function
    $B_{t}(v)=Hv+\eta_{t}v$;       $p_{t}=CGMinimize(B_{t},-g_{t})$;       $x_{t+1}=x_{t}+p_{t}$end
    for
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *$n=1$ 到最大迭代次数* 进行：计算梯度 $g_{t}=\nabla f(x_{t})$；根据某些启发式方法选择/调整 $\eta_{t}$；定义函数
    $B_{t}(v)=Hv+\eta_{t}v$；$p_{t}=CGMinimize(B_{t},-g_{t})$；$x_{t+1}=x_{t}+p_{t}$；循环结束
- en: Algorithm 4 Damped Hessian-Free Optimization
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 阻尼海森矩阵优化
- en: However, common variants of HF don’t work particular well for NNs. Enhancements
    techniques (e.g., the Gauss-Newton approximation to the Hessian, early CG stopping,
    damping, etc.) are provided in  (Martens, [2010](#bib.bib68)). More recently research
    has revealed that DNN learning is easier than previously thought using simple
    methods (Sutskever et al., [2013b](#bib.bib104)). Carefully tuned momentum methods
    suffice for dealing with the curvature issues in deep and recurrent network training
    objectives without the need for sophisticated second-order methods. Despite SGD
    with or without momentum still being the most widely used and best method in most
    situations, the fact that HF uses 100-1000x fewer iterations than SGD supports
    the idea that a second order method can help a lot in principle, provided that
    we can make these iterations cheap enough to compute.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，HF的常见变体在神经网络中效果不佳。增强技术（例如，Hessian的高斯-牛顿近似、早期CG停止、阻尼等）在（Martens，[2010](#bib.bib68)）中提供。最近的研究揭示，使用简单方法训练DNN比之前认为的要容易（Sutskever等，[2013b](#bib.bib104)）。经过仔细调节的动量方法足以处理深度和递归网络训练目标中的曲率问题，而无需复杂的二阶方法。尽管有或没有动量的SGD仍然是大多数情况下最广泛使用和最佳的方法，但HF使用比SGD少100-1000倍的迭代次数这一事实支持了这样一个观点：在原则上，二阶方法可以提供很大帮助，只要我们能使这些迭代的计算成本足够低。
- en: 6.4\. K-FAC
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. K-FAC
- en: Kronecker-Factored Approximate Curvature (K-FAC)  (Martens and Grosse, [2015](#bib.bib70))
    is one of the natural gradient approximation methods where the preconditioner
    is a high-quality approximation of Fisher information matrix (FIM). We first give
    a brief introduction about natural gradient descent and then explained the outline
    of K-FAC.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 克罗内克因子近似曲率（K-FAC）（Martens和Grosse，[2015](#bib.bib70)）是自然梯度近似方法之一，其中预条件器是Fisher信息矩阵（FIM）的高质量近似。我们首先简要介绍自然梯度下降，然后解释K-FAC的概要。
- en: 6.4.1\. Natural Gradient Descent
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1\. 自然梯度下降
- en: Natural Gradient Descent (NGD) (Amari, [1998](#bib.bib8)) is a second order
    optimization method based on information geometry. NGD acquires the loss landscape
    correctly by using FIM as curvature of loss function and converges faster in term
    of iterations than a simple first-order method. The FIM associated with network’s
    distribution $P_{y|x}(\theta)$ is
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 自然梯度下降（NGD）（Amari，[1998](#bib.bib8)）是一种基于信息几何的二阶优化方法。NGD通过使用FIM作为损失函数的曲率，正确地获取了损失景观，并且在迭代次数上比简单的一阶方法收敛更快。与网络分布$P_{y|x}(\theta)$相关的FIM是
- en: '| (41) |  | $F=E[\nabla log(p(y&#124;x;\theta))\nabla log(p(y&#124;x;\theta))^{T}].$
    |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| (41) |  | $F=E[\nabla log(p(y&#124;x;\theta))\nabla log(p(y&#124;x;\theta))^{T}].$
    |  |'
- en: Importantly, one property of $F$ is that it can be interpreted as the negative
    expected Hessian of our model’s log likelihood (Martens, [2020](#bib.bib69))
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，$F$的一个性质是它可以被解释为我们模型的对数似然的负期望Hessian（Martens，[2020](#bib.bib69)）
- en: '| (42) |  | $F=-E_{p(y&#124;x;\theta)}[H_{logp(x&#124;\theta)}].$ |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| (42) |  | $F=-E_{p(y&#124;x;\theta)}[H_{logp(x&#124;\theta)}].$ |  |'
- en: Knowing this result, we can see the role of $F$ as a measure of curvature of
    the log likelihood function. Thus the immediate application of $F$ is as drop-in
    replacement of $H$ in second order optimization methods. Using KL-divergence to
    measures how different two models are, the update rule of NGD is
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 知道这一结果后，我们可以看到$F$作为对数似然函数曲率的度量的作用。因此，$F$的直接应用是作为二阶优化方法中$H$的替代品。使用KL散度来测量两个模型的差异，NGD的更新规则是
- en: '| (43) |  | $\theta_{t+1}\leftarrow\theta_{t}-\eta_{t}F^{-1}\nabla f(\theta_{t}).$
    |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| (43) |  | $\theta_{t+1}\leftarrow\theta_{t}-\eta_{t}F^{-1}\nabla f(\theta_{t}).$
    |  |'
- en: Here the inverse of the FIM is applied to the gradient of loss, and the gradient
    preconditioned by the FIM is called the natural gradient. For the parameters of
    size $N$, the size of FIM is $N\times N$, and NNs used in DL tend to have a massive
    number of parameters (e.g., 60 million parameters in AlexNet for ImageNet classification)
    so the inverse of the FIM is intractable, and it limits the number of the applications
    of NGD to DL. In recent years, some works have proposed methods that approximate
    or avoid inversing the FIM.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，FIM的逆应用于损失的梯度，而经过FIM预条件处理的梯度称为自然梯度。对于大小为$N$的参数，FIM的大小为$N\times N$，而深度学习中使用的神经网络通常有大量的参数（例如，AlexNet在ImageNet分类中有6000万参数），因此FIM的逆计算不可行，这限制了NGD在深度学习中的应用。近年来，一些研究提出了近似或避免求FIM逆的方法。
- en: 6.4.2\. K-FAC Approximation.
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2\. K-FAC 近似
- en: K-FAC approximates the FIM so that the inverse matrix is easy to calculate.
    Firstly, K-FAC approximates F as $\hat{F}$, a diagonal block matrix where each
    block represents one layer in a NN with $L$ layers
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: K-FAC 通过近似 FIM，使得逆矩阵容易计算。首先，K-FAC 将 F 近似为 $\hat{F}$，这是一个对角块矩阵，每个块代表一个具有 $L$
    层的神经网络中的一层。
- en: '| (44) |  | $\hat{F}=diag(\hat{F}_{1},...,\hat{F}_{l},...,\hat{F}_{L}).$ |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| (44) |  | $\hat{F}=diag(\hat{F}_{1},...,\hat{F}_{l},...,\hat{F}_{L}).$ |  |'
- en: Next, each diagonal block matrix $F_{l}$ is approximated as a Kronecker product
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，每个对角块矩阵 $F_{l}$ 被近似为 Kronecker 积。
- en: '| (45) |  | $\hat{F}_{l}\approx A_{l-1}\otimes G_{l}.$ |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| (45) |  | $\hat{F}_{l}\approx A_{l-1}\otimes G_{l}.$ |  |'
- en: This is called Kronecker factorization and $G_{l}$, $A_{l-1}$ are called Kronecker
    factors, representing the gradient of the output of the $l$-th layer and the activation
    of the ($l$-1)-th layer respectively. By using the critical property of the Kronecker
    product of the matrices $(A\otimes B)^{-1}=A^{-1}\otimes B^{-1}$, the inverse
    of $\hat{F}_{l}$ can be computed as
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这称为 Kronecker 分解，$G_{l}$ 和 $A_{l-1}$ 被称为 Kronecker 因子，分别表示 $l$-th 层输出的梯度和 ($l$-1)-th
    层的激活。通过使用 Kronecker 积的一个关键性质 $(A\otimes B)^{-1}=A^{-1}\otimes B^{-1}$，可以计算 $\hat{F}_{l}$
    的逆。
- en: '| (46) |  | ${\hat{F}_{l}}^{-1}={A_{l-1}}^{-1}\otimes{G_{l}}^{-1}.$ |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| (46) |  | ${\hat{F}_{l}}^{-1}={A_{l-1}}^{-1}\otimes{G_{l}}^{-1}.$ |  |'
- en: 'The final update step of parameters $w_{l}$ in the $l$-th layer is as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在 $l$-th 层中，参数 $w_{l}$ 的最终更新步骤如下：
- en: '| (47) |  | $w_{l}^{(t+1)}=w_{l}^{(t)}-\eta^{(t)}{G_{l}}^{-1}\nabla L_{l}(w_{l}^{(t)})A_{l-1}^{-1}.$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| (47) |  | $w_{l}^{(t+1)}=w_{l}^{(t)}-\eta^{(t)}{G_{l}}^{-1}\nabla L_{l}(w_{l}^{(t)})A_{l-1}^{-1}.$
    |  |'
- en: In most implementations, Tikhonov regularization is used to avoid ill-conditioned
    matrix inverses with K-FAC by adding a damping parameter $\gamma$ to the diagonal
    of $\hat{F}_{l}$ (Pauloski et al., [2020](#bib.bib79); Grosse and Martens, [2016](#bib.bib40))
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实现中，Tikhonov 正则化通过将阻尼参数 $\gamma$ 添加到 $\hat{F}_{l}$ 的对角线上，以避免与 K-FAC 相关的不良条件矩阵逆（Pauloski
    等，[2020](#bib.bib79); Grosse 和 Martens，[2016](#bib.bib40)）。
- en: '| (48) |  | $(\hat{F}_{l}+\gamma I)^{-1}={({A_{l-1}+\gamma I})^{-1}}\otimes{({G_{l}+\gamma
    I})^{-1}}.$ |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| (48) |  | $(\hat{F}_{l}+\gamma I)^{-1}={({A_{l-1}+\gamma I})^{-1}}\otimes{({G_{l}+\gamma
    I})^{-1}}.$ |  |'
- en: A standard K-FAC update step for one layer requires inverting two matrices $(A_{l-1}+\gamma
    I)$ and $(G_{l}+\gamma I)$, which can be computed implicitly using an alternative
    method based on the eigendecompostion of $\hat{F}_{l}$ (Grosse and Martens, [2016](#bib.bib40);
    Pauloski et al., [2020](#bib.bib79)).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个层的标准 K-FAC 更新步骤，需要对两个矩阵 $(A_{l-1}+\gamma I)$ 和 $(G_{l}+\gamma I)$ 进行逆运算，这可以通过基于
    $\hat{F}_{l}$ 的特征分解的替代方法隐式计算（Grosse 和 Martens，[2016](#bib.bib40); Pauloski 等，[2020](#bib.bib79)）。
- en: '| (49) |  | <math   alttext="\begin{split}V_{1}&amp;=Q_{G}^{T}L_{i}(w_{i}^{(k)})Q_{A}\\
    V_{2}&amp;=V1/(v_{G}(v_{A})^{T}+\lambda)\\'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '| (49) |  | <math   alttext="\begin{split}V_{1}&amp;=Q_{G}^{T}L_{i}(w_{i}^{(k)})Q_{A}\\
    V_{2}&amp;=V1/(v_{G}(v_{A})^{T}+\lambda)\\'
- en: (\hat{F}_{l}+\gamma I)^{-1}\nabla L_{i}(w_{i}^{(k)})&amp;=Q_{G}V_{2}{Q_{A}}^{T}\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  columnalign="right" ><msub ><mi  >V</mi><mn >1</mn></msub></mtd><mtd columnalign="left"  ><mrow
    ><mo >=</mo><mrow ><msubsup  ><mi >Q</mi><mi >G</mi><mi  >T</mi></msubsup><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >L</mi><mi  >i</mi></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi  >w</mi><mi
    >i</mi><mrow ><mo stretchy="false" >(</mo><mi >k</mi><mo stretchy="false"  >)</mo></mrow></msubsup><mo
    stretchy="false"  >)</mo></mrow><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi
    >Q</mi><mi >A</mi></msub></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><msub
    ><mi >V</mi><mn >2</mn></msub></mtd><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow
    ><mrow  ><mi >V</mi><mo lspace="0em" rspace="0em"  >​</mo><mn >1</mn></mrow><mo
    >/</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mrow ><msub ><mi  >v</mi><mi
    >G</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msup ><mrow ><mo stretchy="false"  >(</mo><msub
    ><mi >v</mi><mi >A</mi></msub><mo stretchy="false"  >)</mo></mrow><mi >T</mi></msup></mrow><mo
    >+</mo><mi >λ</mi></mrow><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><msup ><mrow  ><mo stretchy="false"  >(</mo><mrow
    ><msub ><mover accent="true" ><mi  >F</mi><mo >^</mo></mover><mi >l</mi></msub><mo
    >+</mo><mrow ><mi >γ</mi><mo lspace="0em" rspace="0em" >​</mo><mi  >I</mi></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow><mrow ><mo >−</mo><mn >1</mn></mrow></msup><mo
    lspace="0.167em" rspace="0em"  >​</mo><mrow ><mo rspace="0.167em" >∇</mo><msub
    ><mi >L</mi><mi >i</mi></msub></mrow><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><msubsup ><mi >w</mi><mi >i</mi><mrow ><mo stretchy="false"
    >(</mo><mi  >k</mi><mo stretchy="false"  >)</mo></mrow></msubsup><mo stretchy="false"  >)</mo></mrow></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mo >=</mo><mrow ><msub  ><mi >Q</mi><mi >G</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi >V</mi><mn >2</mn></msub><mo lspace="0em"
    rspace="0em" >​</mo><mmultiscripts ><mi >Q</mi><mi >A</mi><mi >T</mi></mmultiscripts></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑉</ci><cn type="integer" >1</cn></apply><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑄</ci><ci  >𝐺</ci></apply><ci >𝑇</ci></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝐿</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑤</ci><ci >𝑖</ci></apply><ci >𝑘</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑄</ci><ci >𝐴</ci></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑉</ci><cn type="integer" >2</cn></apply></apply></apply><apply ><apply  ><apply
    ><apply ><ci  >𝑉</ci><cn type="integer"  >1</cn></apply><apply ><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑣</ci><ci >𝐺</ci></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑣</ci><ci >𝐴</ci></apply><ci >𝑇</ci></apply></apply><ci >𝜆</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><apply
    ><ci >^</ci><ci >𝐹</ci></apply><ci >𝑙</ci></apply><apply ><ci  >𝛾</ci><ci >𝐼</ci></apply></apply><apply
    ><cn type="integer" >1</cn></apply></apply><apply ><ci >∇</ci><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐿</ci><ci >𝑖</ci></apply></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑤</ci><ci >𝑖</ci></apply><ci >𝑘</ci></apply></apply></apply><apply
    ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑄</ci><ci >𝐺</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑉</ci><cn type="integer"  >2</cn></apply><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑄</ci><ci >𝐴</ci></apply><ci >𝑇</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}V_{1}&=Q_{G}^{T}L_{i}(w_{i}^{(k)})Q_{A}\\
    V_{2}&=V1/(v_{G}(v_{A})^{T}+\lambda)\\ (\hat{F}_{l}+\gamma I)^{-1}\nabla L_{i}(w_{i}^{(k)})&=Q_{G}V_{2}{Q_{A}}^{T}\end{split}</annotation></semantics></math>
    |  |
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: \((\hat{F}_{l}+\gamma I)^{-1}\nabla L_{i}(w_{i}^{(k)})=Q_{G}V_{2}{Q_{A}}^{T}\)
- en: In practice, practitioners avoid significant computation and communication by
    reducing the frequency of computing these factors and and their eigendecompositions,
    at the cost of introducing staled information. For example, Pauloski et al. ([2020](#bib.bib79))
    update K-FAC statistics for every 500 iterations for ResNet scaling experiments
    on 64 GPUs.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，实践者通过减少计算这些因子及其特征分解的频率来避免大量计算和通信，代价是引入了过时的信息。例如，Pauloski 等（[2020](#bib.bib79)）在
    64 个 GPU 上进行 ResNet 扩展实验时，每 500 次迭代更新一次 K-FAC 统计量。
- en: 6.4.3\. Distributed K-FAC
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.3\. 分布式 K-FAC
- en: Some studies have used K-FAC and implemented the algorithm in a distributed
    computing environment (Osawa et al., [2019](#bib.bib76); Pauloski et al., [2020](#bib.bib79)).
    With only 35 epochs and a 16K batch size, ResNet50 can be trained to achieve 75%
    Top1 accuracy in ImageNet (Osawa et al., [2019](#bib.bib76)). More recently, Pauloski
    et al. ([2020](#bib.bib79)) scales up K-FAC for training CNNs. It mainly refers
    to the calculation scheme of preconditioned gradient in  (Grosse and Martens,
    [2016](#bib.bib40)) and uses feature decomposition to replace matrix inversion.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究使用了 K-FAC 并在分布式计算环境中实现了该算法（Osawa 等，[2019](#bib.bib76)；Pauloski 等，[2020](#bib.bib79)）。仅用
    35 个 epochs 和 16K 批量大小，ResNet50 可以在 ImageNet 上达到 75% 的 Top1 准确率（Osawa 等，[2019](#bib.bib76)）。最近，Pauloski
    等（[2020](#bib.bib79)）扩大了 K-FAC 的规模以训练 CNN。它主要参考了预条件梯度的计算方案（Grosse 和 Martens，[2016](#bib.bib40)），并使用特征分解来替代矩阵求逆。
- en: '| Algorithm | Preconditioning | Memory | Computation |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 预条件 | 内存 | 计算 |'
- en: '| Full Matrix AdaGrad ![[Uncaptioned image]](img/bdeb6ca4ba7e8d6a224b600fe9be3ecf.png)  |
    $H_{t}=(\sum_{s=1}^{t}g_{s}g_{s}^{T})^{\frac{1}{2}}$ $W_{t+1}=W_{t}-\eta_{t}H_{t}^{-1/2}$
    | $O((mn)^{2})$ | $O((mn)^{2})$ |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 全矩阵 AdaGrad ![[未标注的图片]](img/bdeb6ca4ba7e8d6a224b600fe9be3ecf.png)  | $H_{t}=(\sum_{s=1}^{t}g_{s}g_{s}^{T})^{\frac{1}{2}}$
    $W_{t+1}=W_{t}-\eta_{t}H_{t}^{-1/2}$ | $O((mn)^{2})$ | $O((mn)^{2})$ |'
- en: '| Shampoo ![[Uncaptioned image]](img/eecf73a7dfbbaf520a495e535190dff2.png)  |
    $L_{t}=L_{t-1}+g_{t}g_{t}^{T}$ $R_{t}=R_{t-1}+g_{t}^{T}g_{t}$ $W_{t+1}=W_{t}-{L_{t}^{-1}G_{t}R_{t}^{-1}}$
    | $O(m^{2}+n^{2})$ | $O(m^{2}+n^{2})$ |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Shampoo ![[未标注的图片]](img/eecf73a7dfbbaf520a495e535190dff2.png)  | $L_{t}=L_{t-1}+g_{t}g_{t}^{T}$
    $R_{t}=R_{t-1}+g_{t}^{T}g_{t}$ $W_{t+1}=W_{t}-{L_{t}^{-1}G_{t}R_{t}^{-1}}$ | $O(m^{2}+n^{2})$
    | $O(m^{2}+n^{2})$ |'
- en: '| K-FAC ![[Uncaptioned image]](img/a932cf5e9d38732e23f29ea6b0b8f0aa.png)  |
    $\hat{F}_{i}=A_{i-1}\otimes G_{i}$ $A_{i-1}=a_{i-1}a_{i-1}^{T}$ $G_{i}=g_{i}g_{i}^{T}$
    $W_{l}^{(t+1)}=W_{l}^{(t)}-\eta^{(t)}{G_{l}}^{-1}\nabla L_{l}(W_{l}^{(t)})A_{l-1}^{-1}$
    | $O(m^{2}+n^{2})$ | $O(m^{3}+n^{3})$ |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| K-FAC ![[未标注的图片]](img/a932cf5e9d38732e23f29ea6b0b8f0aa.png)  | $\hat{F}_{i}=A_{i-1}\otimes
    G_{i}$ $A_{i-1}=a_{i-1}a_{i-1}^{T}$ $G_{i}=g_{i}g_{i}^{T}$ $W_{l}^{(t+1)}=W_{l}^{(t)}-\eta^{(t)}{G_{l}}^{-1}\nabla
    L_{l}(W_{l}^{(t)})A_{l-1}^{-1}$ | $O(m^{2}+n^{2})$ | $O(m^{3}+n^{3})$ |'
- en: '| Diagonal AdaGrad ![[Uncaptioned image]](img/accaa1a1edc7cd4781fa2d99be3cfa92.png)  |
    $H_{t,ij}=\sum_{s\leq t}g^{2}_{s,ij}$ $W_{t+1}=W_{t}-\eta_{t}H_{t}^{-1/2}$ | $O(mn)$
    | $O(mn)$ |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 对角线 AdaGrad ![[未标注的图片]](img/accaa1a1edc7cd4781fa2d99be3cfa92.png)  | $H_{t,ij}=\sum_{s\leq
    t}g^{2}_{s,ij}$ $W_{t+1}=W_{t}-\eta_{t}H_{t}^{-1/2}$ | $O(mn)$ | $O(mn)$ |'
- en: '| SM3 ![[Uncaptioned image]](img/4eb88d695b3ca0e00454c7e789878f90.png)  | $\widehat{H_{t,ij}}=min(L_{t-1,i},R_{t-1,j})+g_{t,ij}^{2}$
    $L_{t,i}=\mathop{max}\limits_{j}(\widehat{H_{t,ij}})$ $R_{t,j}=\mathop{min}\limits_{i}(\widehat{H_{t,ij}})$
    $W_{t+1,i}=W_{t,i}-\eta g_{t,i}{\widehat{H_{t,ij}}}^{-1/2}$ | $O(m+n)$ | $O(mn)$
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| SM3 ![[未标注的图片]](img/4eb88d695b3ca0e00454c7e789878f90.png)  | $\widehat{H_{t,ij}}=min(L_{t-1,i},R_{t-1,j})+g_{t,ij}^{2}$
    $L_{t,i}=\mathop{max}\limits_{j}(\widehat{H_{t,ij}})$ $R_{t,j}=\mathop{min}\limits_{i}(\widehat{H_{t,ij}})$
    $W_{t+1,i}=W_{t,i}-\eta g_{t,i}{\widehat{H_{t,ij}}}^{-1/2}$ | $O(m+n)$ | $O(mn)$
    |'
- en: Table 1\. Summary of Preconditioning Methods. Example of a fully connected layer
    $[m,n]$.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 预条件方法总结。完全连接层 $[m,n]$ 的示例。
- en: 6.5\. Shampoo
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5\. Shampoo
- en: Shampoo  (Gupta et al., [2018](#bib.bib41)) is another preconditioning algorithm,
    which is an approximation version of full matrix AdaGrad (Duchi et al., [2011](#bib.bib33)).
    It first approximates by treating each layer independently (block diagonal). And
    it uses small matrices whose Kronecker product approximates the full AdaGrad statistics.
    Such two approximations make Shampoo more expressive than the diagonal preconditioning
    and practical to store and compute at large scale. Mathematically, the preconditioner
    Shampoo looking for can be written as a Kronecker product of two smaller matrices
    $L$ and $R$
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Shampoo（Gupta 等，[2018](#bib.bib41)）是另一种预条件算法，它是全矩阵 AdaGrad（Duchi 等，[2011](#bib.bib33)）的近似版本。它首先通过将每一层独立处理（块对角）来进行近似。它使用小矩阵，其
    Kronecker 积近似全 AdaGrad 统计量。这两种近似使得 Shampoo 比对角线预条件更具表现力，并且在大规模存储和计算时更为实用。在数学上，Shampoo
    寻找的预条件器可以写成两个较小矩阵 $L$ 和 $R$ 的 Kronecker 积。
- en: '| (50) |  | $\mathop{argmin}\limits_{H=L\otimes R\atop L,\,R\succ 0}\,\{H^{-1}\bullet\overline{G_{t}}+Tr(H)\}.$
    |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| (50) |  | $\mathop{argmin}\limits_{H=L\otimes R\atop L,\,R\succ 0}\,\{H^{-1}\bullet\overline{G_{t}}+Tr(H)\}.$
    |  |'
- en: 'Though it cannot solve the exact optimization problem, it has a nice limit
    that relaxes the upper bounds in a matrix sense:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它不能解决精确的优化问题，但它有一个好的极限，在矩阵意义上放宽了上界：
- en: '| (51) |  | $\frac{1}{\sqrt{r}}\underbrace{(\sum\limits_{t=1}^{T}g_{t}(g_{t})^{T})^{\frac{1}{2}}}_{full\,AdaGrad\,precond.}\preceq\underbrace{(\sum\limits_{t=1}^{T}{G_{t}{G_{t}}^{T}})^{\frac{1}{4}}}_{L_{t}}\otimes\underbrace{{(\sum\limits_{t=1}^{T}{{G_{t}}^{T}G_{t}})^{\frac{1}{4}}}}_{R_{t}}$
    |  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| (51) |  | $\frac{1}{\sqrt{r}}\underbrace{(\sum\limits_{t=1}^{T}g_{t}(g_{t})^{T})^{\frac{1}{2}}}_{full\,AdaGrad\,precond.}\preceq\underbrace{(\sum\limits_{t=1}^{T}{G_{t}{G_{t}}^{T}})^{\frac{1}{4}}}_{L_{t}}\otimes\underbrace{{(\sum\limits_{t=1}^{T}{{G_{t}}^{T}G_{t}})^{\frac{1}{4}}}}_{R_{t}}$
    |  |'
- en: 'The full AdaGrad peconditioner is given on the left, bounded by a Kronecker
    product of two smaller matrices. The update statistic of Shampoo is given as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 AdaGrad 预条件器在左侧给出，由两个较小矩阵的 Kronecker 积所界定。Shampoo 的更新统计如下所示：
- en: '| (52) |  | $\begin{split}L_{t}=L_{t-1}+G_{t}G_{t}^{T},\quad R_{t}=R_{t-1}+G_{t}^{T}G_{t}\\
    W_{t+1}=W_{t}-{L_{t}^{-1}G_{t}R_{t}^{-1}}\end{split}$ |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| (52) |  | $\begin{split}L_{t}=L_{t-1}+G_{t}G_{t}^{T},\quad R_{t}=R_{t-1}+G_{t}^{T}G_{t}\\
    W_{t+1}=W_{t}-{L_{t}^{-1}G_{t}R_{t}^{-1}}\end{split}$ |  |'
- en: Both Shampoo and K-FAC employ a preconditioning scheme that approximates the
    FIM. Despite their similarity in construction, they differ in several important
    ways. The differences are based on choices such as the empirical FIM or FIM, moving
    average or sum, and the inverse component. Another key difference is that Shampoo
    construction is agnostic to layer types. K-FAC relies heavily on the structure
    of the back-propagated gradients in a feed-forward neural network. In contrast,
    Shampoo is virtually oblivious to the particular model structures and only depends
    on standard gradient information. More recently, Anil et al. ([2020](#bib.bib10))
    extend Shampoo in a number of ways so as to make it applicable to a larger range
    of deep architectures.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Shampoo 和 K-FAC 都采用了近似 Fisher 信息矩阵 (FIM) 的预条件方案。尽管它们的构建相似，但在几个重要方面存在差异。这些差异基于如经验
    FIM 还是 FIM、移动平均还是总和、以及逆组件等选择。另一个关键差异是 Shampoo 的构建与层类型无关，而 K-FAC 在前馈神经网络中的反向传播梯度结构上依赖较重。相反，Shampoo
    对特定模型结构几乎无感，仅依赖于标准梯度信息。最近，Anil 等人 ([2020](#bib.bib10)) 在多个方面扩展了 Shampoo，使其适用于更广泛的深度架构。
- en: 'Despite the fact that first-order methods have been dominant in the recent
    decade, recently second order methods, such as K-FAC and Shampoo, show some promise.
    They mitigate the space and run-time costs of full-matrix second-order algorithms
    and have been applicable to a larger range of deep architectures (see Table [1](#S6.T1
    "Table 1 ‣ 6.4.3\. Distributed K-FAC ‣ 6.4\. K-FAC ‣ 6\. Second Order Optimization
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey")). It is interesting
    to see whether second order methods can outperform first order ones in the future.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管一阶方法在最近十年中占据主导地位，但最近二阶方法，如 K-FAC 和 Shampoo，显示出一些前景。它们减轻了全矩阵二阶算法的空间和运行时间成本，并已适用于更广泛的深度架构（见表
    [1](#S6.T1 "Table 1 ‣ 6.4.3\. Distributed K-FAC ‣ 6.4\. K-FAC ‣ 6\. Second Order
    Optimization ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey")）。有趣的是，未来是否能看到二阶方法超越一阶方法。'
- en: 7\. Communication
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 通信
- en: Large-scale distributed training improves the productivity of training deeper
    and larger models, where data parallelism is adopted so as to take full advantage
    of the compute capability on multiple workers. SGD is usually selected as the
    optimization method because of its high computation efficiency and well support
    by the DL tool-kits, such as TensorFlow (Abadi et al., [2016](#bib.bib2)), PyTorch (Paszke
    et al., [2019](#bib.bib78)) and DeepSpeed (Rasley et al., [2020](#bib.bib84)).
    In data-parallel SGD, each worker processes a random mini-batch of its training
    data, and then the local updates are synchronized by making an All-Reduce step
    or through a centralized parameter server, which aggregates stochastic gradients
    from all workers, and taking a Broadcast step that transmits the updated parameter
    vector back to all workers. The process of gradient synchronization is repeated
    until an appropriate convergence criterion is met.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模分布式训练提升了训练更深、更大模型的生产力，其中采用数据并行以充分利用多个工作节点的计算能力。SGD 通常被选择为优化方法，因为其计算效率高且受到深度学习工具包的良好支持，如
    TensorFlow（Abadi 等，[2016](#bib.bib2)）、PyTorch（Paszke 等，[2019](#bib.bib78)）和 DeepSpeed（Rasley
    等，[2020](#bib.bib84)）。在数据并行SGD中，每个工作节点处理其训练数据的一个随机小批量，然后通过执行 All-Reduce 步骤或通过集中式参数服务器同步本地更新，该服务器汇总所有工作节点的随机梯度，并通过
    Broadcast 步骤将更新后的参数向量传回所有工作节点。梯度同步的过程重复进行，直到满足适当的收敛标准。
- en: Increasing the number of workers and taking advantage of data parallelism help
    to reduce the computation time on the same size training data dramatically. However,
    as the scale of distributed systems grows up, gradient and parameter synchronization
    prolongs the communication time and hinders the perfect scalability (Li et al.,
    [2014a](#bib.bib61); Wen et al., [2017](#bib.bib113)). Therefore, the high network
    communication cost becomes a significant bottleneck of distributed training. There
    have been many attempts to reduce the communication overhead in data-parallel
    SGD. One notable method is to let each worker use compressed gradients rather
    than raw gradients for communication. For example, quantized SGD or sparcified
    SGD allow each worker to use fewer bits to pass gradients by sacrificing the convergence
    to a mild extent. Another notable method is to reduce the frequency of communication (Zinkevich
    et al., [2010](#bib.bib127); McDonald et al., [2010](#bib.bib72); Zhang et al.,
    [2016](#bib.bib125); Kamp et al., [2018](#bib.bib51); Lin et al., [2020](#bib.bib63)).
    We detail on these two communication-efficient methods in the subsequent sections.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 增加工作节点数量并利用数据并行性有助于显著减少相同规模训练数据的计算时间。然而，随着分布式系统规模的扩大，梯度和参数的同步会延长通信时间并阻碍完美的扩展性（Li
    等，[2014a](#bib.bib61)；Wen 等，[2017](#bib.bib113)）。因此，高网络通信成本成为分布式训练的一个重要瓶颈。为了减少数据并行SGD中的通信开销，已经进行了许多尝试。一种显著的方法是让每个工作节点使用压缩梯度而不是原始梯度进行通信。例如，量化SGD或稀疏SGD允许每个工作节点使用更少的位来传递梯度，从而在一定程度上牺牲收敛性。另一种显著的方法是减少通信频率（Zinkevich
    等，[2010](#bib.bib127)；McDonald 等，[2010](#bib.bib72)；Zhang 等，[2016](#bib.bib125)；Kamp
    等，[2018](#bib.bib51)；Lin 等，[2020](#bib.bib63)）。我们将在随后的章节中详细讨论这两种高效通信的方法。
- en: 7.1\. Gradient Compression
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 梯度压缩
- en: 'Lin et al. ([2018](#bib.bib64)) find that 99.9% of the gradient exchange in
    distributed SGD are redundant. One promising solution is gradient compression,
    e.g., through gradient quantization (Alistarh et al., [2017](#bib.bib6); Wen et al.,
    [2017](#bib.bib113); Ramezani-Kebrya et al., [2021](#bib.bib83); Seide et al.,
    [2014](#bib.bib91); Tang et al., [2021](#bib.bib106); Li et al., [2021](#bib.bib59))
    and/or gradient sparsification (Aji and Heafield, [2017](#bib.bib5); Lin et al.,
    [2018](#bib.bib64); Dryden et al., [2016](#bib.bib32); Strom, [2015](#bib.bib101)).
    Sparcification means transmitting only those gradients that are important (e.g.,
    gradients with large absolute values), while quantization refers to using fewer
    bits to represent the original gradient. Their difference is described in Fig. [3](#S7.F3
    "Figure 3 ‣ 7.1.1\. Gradient Quantization ‣ 7.1\. Gradient Compression ‣ 7\. Communication
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey").'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 'Lin 等人 ([2018](#bib.bib64)) 发现分布式 SGD 中99.9%的梯度交换是冗余的。一个有前景的解决方案是梯度压缩，例如，通过梯度量化
    (Alistarh 等人, [2017](#bib.bib6); Wen 等人, [2017](#bib.bib113); Ramezani-Kebrya
    等人, [2021](#bib.bib83); Seide 等人, [2014](#bib.bib91); Tang 等人, [2021](#bib.bib106);
    Li 等人, [2021](#bib.bib59)) 和/或梯度稀疏化 (Aji 和 Heafield, [2017](#bib.bib5); Lin 等人,
    [2018](#bib.bib64); Dryden 等人, [2016](#bib.bib32); Strom, [2015](#bib.bib101))。稀疏化指的是仅传输那些重要的梯度（例如，具有大绝对值的梯度），而量化指的是使用更少的位来表示原始梯度。它们的区别在图
    [3](#S7.F3 "Figure 3 ‣ 7.1.1\. Gradient Quantization ‣ 7.1\. Gradient Compression
    ‣ 7\. Communication ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey") 中有所描述。'
- en: 7.1.1\. Gradient Quantization
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1\. 梯度量化
- en: '![Refer to caption](img/e0c8669a0271266539229d57e14f0154.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e0c8669a0271266539229d57e14f0154.png)'
- en: Figure 3\. Comparison of Quantization and Sparsification, src: (Tang et al.,
    [2020](#bib.bib107))
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3\. 量化与稀疏化的比较，来源: (Tang 等人, [2020](#bib.bib107))'
- en: 'Quantizing the gradients to low-precision values can reduce the communication
    bandwidth. In full-precision data-parallel SGD, each processor broadcasts its
    locally computed stochastic gradient vector at every iteration, whereas in quantized
    data-parallel SGD, each processor quantizes its stochastic gradient before broadcasting.
    Gradient quantization is usually done by mapping a continuous space of gradient
    values onto a discrete set. Take the classic Quantized Stochastic Gradient Descent
    (QSGD) (Alistarh et al., [2017](#bib.bib6)) as an example, the quantization function
    is denoted with $Q_{s}(v)$:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 将梯度量化为低精度值可以减少通信带宽。在全精度数据并行 SGD 中，每个处理器在每次迭代时广播其局部计算的随机梯度向量，而在量化的数据并行 SGD 中，每个处理器在广播之前量化其随机梯度。梯度量化通常是通过将梯度值的连续空间映射到离散集来完成的。以经典的量化随机梯度下降
    (QSGD) (Alistarh 等人, [2017](#bib.bib6)) 为例，量化函数表示为 $Q_{s}(v)$：
- en: '| (53) |  | $Q_{s}(v_{i})=\lVert v\rVert_{2}\cdot sign(v_{i})\cdot\xi_{i}(v,s)$
    |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| (53) |  | $Q_{s}(v_{i})=\lVert v\rVert_{2}\cdot sign(v_{i})\cdot\xi_{i}(v,s)$
    |  |'
- en: where $\xi_{i}(v,s)$ are independent random variables. Let $0\leq l<s$ be an
    integer such that $v_{i}/\lVert v\rVert_{2}\in[l/s,(l+1)/s]$. That is, $[l/s,(l+1)/s]$
    is the quantization interval corresponding to $v_{i}/\lVert v\rVert_{2}$. Then
    $\xi_{i}(v,s)$ is defined as follows
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\xi_{i}(v,s)$ 是独立的随机变量。令 $0\leq l<s$ 为整数，使得 $v_{i}/\lVert v\rVert_{2}\in[l/s,(l+1)/s]$。也就是说，$[l/s,(l+1)/s]$
    是与 $v_{i}/\lVert v\rVert_{2}$ 对应的量化区间。然后，$\xi_{i}(v,s)$ 定义如下
- en: '| (54) |  | $\xi_{i}(v,s)=\left\{\begin{array}[]{cc}l/s&amp;with\,probability\,1-p(\frac{&#124;v_{i}&#124;}{\lVert
    v\rVert_{2}},s)\\ (l+1)/s&amp;otherwise\end{array}\right.$ |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| (54) |  | $\xi_{i}(v,s)=\left\{\begin{array}[]{cc}l/s&amp;with\,probability\,1-p(\frac{&#124;v_{i}&#124;}{\lVert
    v\rVert_{2}},s)\\ (l+1)/s&amp;otherwise\end{array}\right.$ |  |'
- en: 'Here, $p(a,s)=as-l$ for any $a\in[0,1]$. For gradients $v$, quantization is
    used to randomly convert gradient values in each dimension $v_{i}$ to some discrete
    value in a predetermined discrete set. After normalized by the Euclidean norm
    of the gradients ($|v_{i}|/\lVert v\rVert$), the value of each dimension will
    fall on a sub-interval $[0,1]$, and we approximate it to one of the endpoints
    of the sub-interval with a certain probability each time, so that the continuous
    value space of the original gradient value can be replaced by a set of finite
    discrete values. Here $\xi_{i}(v,s)$ is a binary random variable which guarantees
    that each value is quantized in a way which preserves the value in expectation,$E[\xi_{i}(v,s)]=|v_{i}|/\lVert
    v\rVert$, and introduce minimal variance. An instance of QSGD is provided in Fig. [5](#S7.F5
    "Figure 5 ‣ 7.1.1\. Gradient Quantization ‣ 7.1\. Gradient Compression ‣ 7\. Communication
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey").'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，$p(a,s)=as-l$ 对于任何 $a\in[0,1]$。对于梯度 $v$，量化被用来随机将每个维度 $v_{i}$ 的梯度值转换为预定离散集合中的某个离散值。经过梯度的欧几里得范数归一化（$|v_{i}|/\lVert
    v\rVert$）后，每个维度的值将落在子区间 $[0,1]$ 上，我们每次以一定的概率将其近似为子区间的一个端点，从而用一组有限的离散值替代原始梯度值的连续值空间。这里的
    $\xi_{i}(v,s)$ 是一个二元随机变量，确保每个值以保留期望值的方式进行量化，$E[\xi_{i}(v,s)]=|v_{i}|/\lVert v\rVert$，并引入最小方差。图
    [5](#S7.F5 "Figure 5 ‣ 7.1.1\. Gradient Quantization ‣ 7.1\. Gradient Compression
    ‣ 7\. Communication ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey") 给出了 QSGD 的一个实例。'
- en: This gradient quantization method greatly reduces the amount of communication
    required by a single node. Instead of passing $n$ 32-bit floating-point gradients,
    only one 32-bit floating-point gradient with one bit for gradient sign and $log(s)$
    bits for $\xi_{i}(v,s)$ on each dimension are required. In addition, there is
    another method TernGrad (Wen et al., [2017](#bib.bib113)) developed simultaneously
    with QSGD. Their underlying idea is essentially similar, where TernGrad can be
    viewed as a special case of QSGD when $l=1$. TernGrad randomly quantizates gradient
    $g_{t}$ to a ternary value vector with value of $\{-1,0,1\}$. Formally, with a
    random binary vector $b_{t}$, gradient is ternarized as
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这种梯度量化方法大大减少了单个节点所需的通信量。与传递 $n$ 个 32 位浮点梯度相比，只需一个 32 位浮点梯度，其中一个比特用于梯度符号，每个维度上需要
    $log(s)$ 位用于 $\xi_{i}(v,s)$。此外，还有另一种方法 TernGrad（Wen 等，[2017](#bib.bib113)），与 QSGD
    同时开发。它们的基本思想本质上相似，其中 TernGrad 可以视为 QSGD 在 $l=1$ 时的特例。TernGrad 将梯度 $g_{t}$ 随机量化为一个值为
    $\{-1,0,1\}$ 的三值向量。形式上，使用随机二进制向量 $b_{t}$，梯度被三值化为
- en: '| (55) |  | $\begin{split}\tilde{g}_{t}&amp;=ternarize(g_{t})=s_{t}\cdot sign(g_{t})\circ
    b_{t}\\ s_{t}&amp;\triangleq\lVert g_{t}\rVert_{\infty}\triangleq max(abs(g_{t})).\end{split}$
    |  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| (55) |  | $\begin{split}\tilde{g}_{t}&=ternarize(g_{t})=s_{t}\cdot sign(g_{t})\circ
    b_{t}\\ s_{t}&\triangleq\lVert g_{t}\rVert_{\infty}\triangleq max(abs(g_{t})).\end{split}$
    |  |'
- en: Here, $\circ$ is the Hadamard product. TernGrad also adopts techniques such
    as layer-wise ternarizing and gradient clipping to improve convergence. Ramezani-Kebrya
    et al. ([2021](#bib.bib83)) propose nonuniform quantization levels (NUQSGD) and
    demonstrate superior empirical results compared to QSGD. Horvath et al. ([2019](#bib.bib47))
    propose natural compression and natural dithering, where the latter is a special
    case of logarithmic quantization.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\circ$ 是 Hadamard 乘积。TernGrad 还采用了逐层三值化和梯度裁剪等技术来改善收敛性。Ramezani-Kebrya 等（[2021](#bib.bib83)）提出了非均匀量化级别（NUQSGD），并展示了相比于
    QSGD 的优越经验结果。Horvath 等（[2019](#bib.bib47)）提出了自然压缩和自然抖动，其中后者是对数量化的特例。
- en: '![Refer to caption](img/bade49d0b5bedd9bbc3dde2af9134cd7.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/bade49d0b5bedd9bbc3dde2af9134cd7.png)'
- en: Figure 4\. QSGD Example with $s=4,l=3$
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. QSGD 示例，$s=4,l=3$
- en: '![Refer to caption](img/6dc4f46dcb8c1a2305af17372e43f8d7.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6dc4f46dcb8c1a2305af17372e43f8d7.png)'
- en: Figure 5\. Top-k Example
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. Top-k 示例
- en: Unlike QSGD and its variants which use stochastic rounding which has an unbiased
    gradient expectation, there are methods adopt biased ones. Methods performing
    updates only based on the sign of each coordinate of the gradient have gained
    popularity for training DL models (Seide et al., [2014](#bib.bib91); Bernstein
    et al., [2018](#bib.bib14); Karimireddy et al., [2019](#bib.bib52); Tang et al.,
    [2021](#bib.bib106); Li et al., [2021](#bib.bib59)). Seide et al. ([2014](#bib.bib91))
    proposed SignSGD, a.k.a. 1-bit SGD, to quantize the gradients aggressively to
    one bit per value. In this scheme, gradient updates greater than or equal to zero
    are encoded using the value 1, and those less than zero with the value 0\. The
    reconstruction values are chosen to be the means of the non-negative and negative
    updates, respectively, in order to minimize the square quantization error. This
    is done column-wise over the weight matrix. In each data exchange, the two reconstruction
    values are transmitted along with their respective quantized column. Bernstein
    et al. ([2018](#bib.bib14)) later provided convergence guarantees for a variant
    of SignSGD. Karimireddy et al. ([2019](#bib.bib52)) proposed EF-SignSGD, which
    is an improved version of SignSGD. More recently, gradient compression with error
    compensation has been successfully applied to adaptive optimizer such as Adam
    (1-bit Adam  (Tang et al., [2021](#bib.bib106))) and LAMB (1-bit LAMB  (Li et al.,
    [2021](#bib.bib59))), further scaling up training algorithms in the distributed
    setting.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于使用具有无偏梯度期望的随机舍入的QSGD及其变种，一些方法采用了有偏的方案。仅基于梯度每个坐标的符号进行更新的方法在训练深度学习模型中获得了广泛关注（Seide
    等，[2014](#bib.bib91)；Bernstein 等，[2018](#bib.bib14)；Karimireddy 等，[2019](#bib.bib52)；Tang
    等，[2021](#bib.bib106)；Li 等，[2021](#bib.bib59)）。Seide 等（[2014](#bib.bib91)）提出了SignSGD，即1-bit
    SGD，将梯度量化为每个值1位。在这种方案中，大于或等于零的梯度更新被编码为值1，小于零的梯度更新则编码为值0。重建值被选择为非负和负更新的均值，以最小化平方量化误差。这个过程在权重矩阵的每一列上进行。在每次数据交换中，这两个重建值与各自量化的列一起传输。Bernstein
    等（[2018](#bib.bib14)）后来为SignSGD的一个变体提供了收敛保证。Karimireddy 等（[2019](#bib.bib52)）提出了EF-SignSGD，这是SignSGD的改进版本。更近期的，带有误差补偿的梯度压缩已成功应用于自适应优化器，如Adam（1-bit
    Adam（Tang 等，[2021](#bib.bib106)））和LAMB（1-bit LAMB（Li 等，[2021](#bib.bib59)）），进一步扩大了分布式环境中的训练算法。
- en: To recap, while the analyses of gradient quatization have largely been restricted
    to unbiased compression schemes  (Alistarh et al., [2017](#bib.bib6); Ramezani-Kebrya
    et al., [2021](#bib.bib83); Wen et al., [2017](#bib.bib113); Horvath et al., [2019](#bib.bib47)),
    biased schemes which perform extreme compression practically perform much better
    often without any loss in convergence or accuracy (Seide et al., [2014](#bib.bib91);
    Karimireddy et al., [2019](#bib.bib52); Strom, [2015](#bib.bib101); Lin et al.,
    [2018](#bib.bib64); Tang et al., [2021](#bib.bib106); Li et al., [2021](#bib.bib59)).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，尽管梯度量化的分析大多局限于无偏压缩方案（Alistarh 等，[2017](#bib.bib6)；Ramezani-Kebrya 等，[2021](#bib.bib83)；Wen
    等，[2017](#bib.bib113)；Horvath 等，[2019](#bib.bib47)），但表现出极端压缩的有偏方案通常表现得更好，且几乎没有收敛性或准确度的损失（Seide
    等，[2014](#bib.bib91)；Karimireddy 等，[2019](#bib.bib52)；Strom，[2015](#bib.bib101)；Lin
    等，[2018](#bib.bib64)；Tang 等，[2021](#bib.bib106)；Li 等，[2021](#bib.bib59)）。
- en: 7.1.2\. Gradient Sparsification
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2. 梯度稀疏化
- en: 'Gradient sparsification is an orthogonal approach to quatization methods, which
    reduces the communication bandwidth by sending only the important gradients. Since
    zeroing small gradients damages convergence, small gradients are accumulated over
    time locally until they become large enough to be transmitted. Thus, we send the
    large gradients immediately but eventually send all of the gradients. Strom ([2015](#bib.bib101))
    proposed threshold quantization by considering only gradient elements whose absolute
    values exceed a threshold. A fixed threshold $\tau$ is chosen in advance. Gradient
    updates greater than $\tau$ are encoded with the value 1, and those less than
    $-\tau$ with the value of 0\. Updates of magnitude less than $\tau$ are not sent
    at all, reducing the volume of data sent. The reconstructed value is $\tau$ and
    $-\tau$ respectively, and error feedback is used as normal. However, the threshold
    is hard to choose in practice and, moreover, it can change over time during optimization.
    As a resolve, Top-k sparsification selects the top-k gradients in terms of absolute
    values at each iteration (Stich et al., [2018](#bib.bib100); Alistarh et al.,
    [2018](#bib.bib7)) (see Fig. [5](#S7.F5 "Figure 5 ‣ 7.1.1\. Gradient Quantization
    ‣ 7.1\. Gradient Compression ‣ 7\. Communication ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey") for an example). Dryden et al. ([2016](#bib.bib32)) choose
    an adaptive threshold so as to keep a constant proportion of gradients each iteration.
    Aji and Heafield ([2017](#bib.bib5)) sparsify gradient updates by removing the
    R% smallest gradients by absolute value, dubbing this Gradient Dropping. This
    approach is slightly different from  (Dryden et al., [2016](#bib.bib32)) as it
    uses a single threshold based on absolute value, instead of dropping the positive
    and negative gradients separately. Concurrently, Chen et al. ([2018](#bib.bib24))
    localize selection of gradient residues and automatically tunes the compression
    rate depending on local activity. Lin et al. ([2018](#bib.bib64)) further push
    the compression ratio by employing momentum correction, local gradient clipping,
    momentum factor masking, warm-up training on top of the gradient sparsification
    while maintaining model performance. Table [2](#S7.T2 "Table 2 ‣ 7.1.2\. Gradient
    Sparsification ‣ 7.1\. Gradient Compression ‣ 7\. Communication ‣ Large-Scale
    Deep Learning Optimizations: A Comprehensive Survey") summarises the gradient
    quantization and sparsification methods.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '梯度稀疏化是一种与量化方法正交的方案，它通过仅发送重要梯度来减少通信带宽。由于将小梯度置零会影响收敛，小梯度会在本地累积，直到它们足够大以便传输。因此，我们立即发送大的梯度，但最终会发送所有梯度。Strom
    ([2015](#bib.bib101)) 提出了阈值量化方法，通过仅考虑绝对值超过阈值的梯度元素来实现。预先选择一个固定阈值 $\tau$。大于 $\tau$
    的梯度更新被编码为值 1，而小于 $-\tau$ 的梯度更新则为值 0。小于 $\tau$ 的更新则完全不发送，从而减少了发送的数据量。重构值分别为 $\tau$
    和 $-\tau$，并且正常使用误差反馈。然而，实践中选择阈值很困难，而且它在优化过程中可能会变化。作为解决方案，Top-k 稀疏化方法在每次迭代中选择绝对值最大的前
    k 个梯度 (Stich 等, [2018](#bib.bib100); Alistarh 等, [2018](#bib.bib7))（参见图 [5](#S7.F5
    "Figure 5 ‣ 7.1.1\. Gradient Quantization ‣ 7.1\. Gradient Compression ‣ 7\. Communication
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey") 作为示例）。Dryden
    等 ([2016](#bib.bib32)) 选择了一个自适应阈值，以在每次迭代中保持恒定的梯度比例。Aji 和 Heafield ([2017](#bib.bib5))
    通过去除绝对值最小的 R% 梯度来稀疏化梯度更新，称之为梯度丢弃。这种方法与 (Dryden 等, [2016](#bib.bib32)) 略有不同，因为它使用基于绝对值的单一阈值，而不是分别丢弃正负梯度。同时，Chen
    等 ([2018](#bib.bib24)) 局部选择梯度残差，并根据局部活动自动调整压缩率。Lin 等 ([2018](#bib.bib64)) 通过采用动量校正、局部梯度剪裁、动量因子掩码、梯度稀疏化的暖启动训练进一步提高了压缩比，同时保持了模型性能。表 [2](#S7.T2
    "Table 2 ‣ 7.1.2\. Gradient Sparsification ‣ 7.1\. Gradient Compression ‣ 7\.
    Communication ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey")
    总结了梯度量化和稀疏化方法。'
- en: '| Method | Taxonomy | Reference |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 分类 | 参考文献 |'
- en: '| Quant. | Unbiased | QSGD (Alistarh et al., [2017](#bib.bib6)), NQSGD (Ramezani-Kebrya
    et al., [2021](#bib.bib83)), TernGrad (Wen et al., [2017](#bib.bib113)), Natural (Horvath
    et al., [2019](#bib.bib47)) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | 无偏 | QSGD (Alistarh 等, [2017](#bib.bib6)), NQSGD (Ramezani-Kebrya 等,
    [2021](#bib.bib83)), TernGrad (Wen 等, [2017](#bib.bib113)), Natural (Horvath 等,
    [2019](#bib.bib47)) |'
- en: '| Biased | 1-bit SGD  (Seide et al., [2014](#bib.bib91)) /Adam (Tang et al.,
    [2021](#bib.bib106))/LAMB (Li et al., [2021](#bib.bib59)) EF-SignSGD (Karimireddy
    et al., [2019](#bib.bib52)) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 有偏 | 1-bit SGD  (Seide 等, [2014](#bib.bib91)) /Adam (Tang 等, [2021](#bib.bib106))/LAMB (Li
    等, [2021](#bib.bib59)) EF-SignSGD (Karimireddy 等, [2019](#bib.bib52)) |'
- en: '| Spars. | Random | Random-k (Wangni et al., [2018](#bib.bib112)) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏化 | 随机 | 随机-k (王霓等，[2018](#bib.bib112)) |'
- en: '|  . | Deterministic | Fixed threshold (Strom, [2015](#bib.bib101)) Top-K (Stich
    et al., [2018](#bib.bib100); Alistarh et al., [2018](#bib.bib7)), Adaptive threshold (Dryden
    et al., [2016](#bib.bib32); Aji and Heafield, [2017](#bib.bib5); Chen et al.,
    [2018](#bib.bib24); Lin et al., [2018](#bib.bib64)) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  . | 确定性 | 固定阈值 (斯特罗姆，[2015](#bib.bib101)) Top-K (斯蒂奇等，[2018](#bib.bib100);
    阿里斯塔赫等，[2018](#bib.bib7))，自适应阈值 (德雷登等，[2016](#bib.bib32); 阿吉和希菲尔德，[2017](#bib.bib5);
    陈等，[2018](#bib.bib24); 林等，[2018](#bib.bib64)) |'
- en: Table 2\. Summary of Gradient Compression Methods
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 梯度压缩方法汇总
- en: 7.2\. Reducing Communication Frequency
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 减少通信频率
- en: A parallel line of work reduces the communication cost by reducing the frequency
    of communication. For instance, local SGD saves the communication cost by allowing
    each worker to perform more than one batch update on local data and exchange the
    updated weights rather than the gradients among workers.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 一项并行研究通过减少通信频率来降低通信成本。例如，本地 SGD 通过允许每个工作节点在本地数据上执行多个批量更新，并交换更新后的权重而不是梯度，从而节省了通信成本。
- en: We consider a distributed SGD framework with $K$ worker nodes where all workers
    communicate with others via a central server or via direct inter-worker communication.
    In local SGD, each worker $k\in[K]$ performs $H$ sequential mini-batch SGD updates
    locally, and then the local models are synchronized by averaging weights among
    workers. Thus, the overall update rule at the $k$-th worker is given by
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个分布式 SGD 框架，其中包含 $K$ 个工作节点，所有工作节点通过中央服务器或直接的工作节点间通信进行互相通信。在本地 SGD 中，每个工作节点
    $k\in[K]$ 在本地执行 $H$ 次顺序小批量 SGD 更新，然后通过在工作节点之间平均权重来同步本地模型。因此，第 $k$ 个工作节点的总体更新规则如下
- en: '| (56) |  | $\begin{split}w_{(t)+h+1}^{k}&amp;:=w_{(t)+h}^{k}-\eta_{(t)}[\frac{1}{B_{loc}}\sum_{i\in{I^{k}_{(t)+h}}}\nabla
    f_{i}(w_{(t)+h}^{k})]\\ w_{(t+1)}^{k}&amp;:=\frac{1}{K}\sum_{k=1}^{K}w_{(t)+H}^{k}\end{split}$
    |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| (56) |  | $\begin{split}w_{(t)+h+1}^{k}&amp;:=w_{(t)+h}^{k}-\eta_{(t)}[\frac{1}{B_{loc}}\sum_{i\in{I^{k}_{(t)+h}}}\nabla
    f_{i}(w_{(t)+h}^{k})]\\ w_{(t+1)}^{k}&amp;:=\frac{1}{K}\sum_{k=1}^{K}w_{(t)+H}^{k}\end{split}$
    |  |'
- en: where $w_{(t)+h}^{k}$ denotes the local model on worker $k$ with batch size
    $B_{loc}$ after $t$ global synchronization and $h$ local SGD updates. Mini-batch
    SGD is a special case of local SGD, with $H=1$, that is, the local models are
    synchronized after every iteration. The convergence results for convex and non-convex
    objectives are provided in  (Stich, [2019](#bib.bib99); Zhou and Cong, [2018](#bib.bib126)).
    However, while local updates reduce the communication frequency by performing
    global synchronization periodically instead of at per iteration, the discrepancies
    between local models can result in an inferior error-convergence. A larger value
    of $H$ (i.e., the number of sequential local SGD updates), which means less frequent
    averaging, saves communication delay and reduces the run-time per iteration. But
    on the other hand, a larger $H$ leads to slower convergence w.r.t. the number
    of iterations. The trade-off in between still need more exploration.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $w_{(t)+h}^{k}$ 表示工作节点 $k$ 在经过 $t$ 次全局同步和 $h$ 次本地 SGD 更新后的本地模型，其批量大小为 $B_{loc}$。小批量
    SGD 是本地 SGD 的一个特例，其中 $H=1$，即本地模型在每次迭代后进行同步。有关凸目标和非凸目标的收敛结果可以参考 (斯蒂奇，[2019](#bib.bib99);
    周和丛，[2018](#bib.bib126))。然而，虽然本地更新通过定期进行全局同步而不是每次迭代进行全局同步来减少通信频率，本地模型之间的差异可能会导致较差的误差收敛。较大的
    $H$ 值（即顺序本地 SGD 更新的数量），意味着平均频率较低，节省了通信延迟并减少了每次迭代的运行时间。但另一方面，较大的 $H$ 会导致相对于迭代次数的收敛速度较慢。这种权衡仍需进一步探索。
- en: In addtion to being communication efficient, Lin et al. ([2020](#bib.bib63))
    find local SGD also exhibits good generalization behaviour. They argue that local
    SGD is a way to inject and control stochastic noise to the whole training procedure,
    and thus proposed post-local SGD as large batch training alternative for better
    generalization.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通信效率外，林等人 ([2020](#bib.bib63)) 发现本地 SGD 还表现出良好的泛化行为。他们认为，本地 SGD 是一种向整个训练过程注入和控制随机噪声的方法，因此提出了将后续本地
    SGD 作为大批量训练的替代方法，以获得更好的泛化能力。
- en: 8\. Memory
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 内存
- en: 'Larger models usually require more computation and memory resources to train.
    The amount of memory required to train these models can be several orders of magnitude
    larger than the amount of memory available on a single GPU. In this section, we
    will see how some popular techniques successfully reduce the memory requirements
    of training NNs without compromising model performance. Section [8.1](#S8.SS1
    "8.1\. Mix-Precision Training ‣ 8\. Memory ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey") introduces how mix-precision training (Micikevicius et al.,
    [2018](#bib.bib73)) lowers the burden on memory using fewer bits to preserve the
    weights and gradients during training. Section [8.2](#S8.SS2 "8.2\. Memory Efficient
    Adaptive Optimization ‣ 8\. Memory ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey") introduces two memory-efficient adaptive optimizers,
    Adafactor (Shazeer and Stern, [2018](#bib.bib94)) and SM3 (Anil et al., [2019](#bib.bib11)).
    And as orthogonal to the above methods, ZeRO (Rajbhandari et al., [2020](#bib.bib82))
    do not change the model optimization method or affect model convergence, but instead
    reduces the memory cost by removing the redundancy in data-parallel (Section [8.3](#S8.SS3
    "8.3\. ZeRO ‣ 8\. Memory ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey")).'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的模型通常需要更多的计算和内存资源进行训练。训练这些模型所需的内存量可能比单个 GPU 上可用的内存量大几个数量级。在本节中，我们将看到一些流行的技术如何在不影响模型性能的情况下成功减少训练神经网络所需的内存。第
    [8.1](#S8.SS1 "8.1\. 混合精度训练 ‣ 8\. 内存 ‣ 大规模深度学习优化：综合调查") 节介绍了混合精度训练（Micikevicius
    等， [2018](#bib.bib73)）如何通过使用更少的位数来保存训练过程中的权重和梯度，从而降低内存负担。第 [8.2](#S8.SS2 "8.2\.
    内存高效自适应优化 ‣ 8\. 内存 ‣ 大规模深度学习优化：综合调查") 节介绍了两种内存高效的自适应优化器，Adafactor（Shazeer 和 Stern，
    [2018](#bib.bib94)）和 SM3（Anil 等， [2019](#bib.bib11)）。与上述方法正交的，ZeRO（Rajbhandari
    等， [2020](#bib.bib82)）并不改变模型优化方法或影响模型收敛，而是通过消除数据并行中的冗余来减少内存成本（第 [8.3](#S8.SS3
    "8.3\. ZeRO ‣ 8\. 内存 ‣ 大规模深度学习优化：综合调查") 节）。
- en: 8.1\. Mix-Precision Training
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 混合精度训练
- en: 'Modern DL training systems use single-precision (FP32) format, which takes
    32 bits of memory. However, lower-precision (FP16) takes 16 bits of memory instead.
    Modern accelerators like Google TPUs and NVIDIA GPUs can run operations faster
    in the FP16 format, as they have specialized hardware to run 16-bit computations
    and 16-bit dtypes can be read from memory faster. These lower-precision provides
    numerous benefits. First, they require less memory, enabling the training and
    deployment of larger NNs. Second, they lowers the burden on memory since fewer
    bits are required to preserve the same number of values than the FP32 format,
    thereby speeding up data transfer operations. Third, they speed up the mathematical
    computation since low-precision calculation is less time-consuming, especially
    on GPUs with Tensor Core support for that precision. However, low precision training
    also introduces a trade-off of the number of bits used versus the statistical
    accuracy: the fewer bits used, the lower accuracy.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习训练系统使用单精度（FP32）格式，占用 32 位内存。然而，低精度（FP16）格式只占用 16 位内存。现代加速器如 Google TPUs
    和 NVIDIA GPUs 在 FP16 格式下运行操作更快，因为它们具有专门的硬件来执行 16 位计算，而且 16 位数据类型从内存中读取的速度更快。这些低精度提供了许多好处。首先，它们需要更少的内存，从而可以训练和部署更大的神经网络。其次，由于需要的位数较少来保存相同数量的值，因此减少了内存负担，从而加快了数据传输操作。第三，低精度计算速度更快，因为低精度计算消耗的时间更少，尤其是在支持该精度的
    Tensor Core 的 GPU 上。然而，低精度训练也引入了使用位数与统计准确性之间的权衡：使用的位数越少，准确性越低。
- en: '![Refer to caption](img/720c2eb6a59a21fd33108dcd9e76c4d9.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/720c2eb6a59a21fd33108dcd9e76c4d9.png)'
- en: Figure 6\. Workflow of Mix Precision Training
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 混合精度训练工作流程
- en: 'Mixed precision training is a very simple and practical technique, jointly
    published by Baidu and Google in (Micikevicius et al., [2018](#bib.bib73)), which
    almost halves the memory footprint while maintaining the model accuracy. This
    is achieved by identifying the steps that require full precision and using FP32
    for only those steps while using FP16 everywhere else. We explain the workflow
    of mix precision training in Figure [6](#S8.F6 "Figure 6 ‣ 8.1\. Mix-Precision
    Training ‣ 8\. Memory ‣ Large-Scale Deep Learning Optimizations: A Comprehensive
    Survey"). Due to the differences in representable ranges, simply converting the
    model to FP16 can cause gradient underflow or overflow problems. We can avoid
    these challenges with the following four steps: (1) Conversion into FP16\. In
    step one, we cast the inputs from FP32 to FP16 for compute intensive operations.
    (2) Use FP32 to compute the loss. Because FP16 might cause underflow or overflow
    issues, we do loss calculation in FP32 in the backward pass and cast gradients
    back to FP16, which means that the weights and the gradients are still in FP16\.
    (3) FP32 master weights. In the backward pass, gradients are small compared to
    the parameters. If we try to update our parameters with the gradients that are
    much smaller than the parameters, then we might lose those parameter updates.
    To compensate we will maintain the master copy of weights in FP32\. This means
    that, to the end of the backward pass, FP16 gradients will be cast into FP32 and
    thereby applied to FP32 weights. In the forward pass, we will cast the weights
    into FP16 so that the gradient computations remain in FP16\. So effectively we
    have a master copy of all the parameters which are weights and biases stored in
    FP32 but all the computational operations will see the casted version which is
    in FP16\. (4) Loss (Gradient) scaling. The last step is to do loss scaling to
    avoid a gradient underflow problem. Before computing gradients from the FP32 loss,
    we scale a loss by multiplying it with a loss scale factor. By doing so, gradients
    are pushed to larger values and we can safely represent them in FP16\. Later when
    updating the weights we can re-scale the gradients by dividing them with the same
    loss scale factor.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '混合精度训练是一种非常简单而实用的技术，由百度和谷歌共同发布于（Micikevicius et al., [2018](#bib.bib73)），它在保持模型准确性的同时几乎将内存占用减半。这是通过识别需要全精度的步骤，并仅在这些步骤中使用FP32，同时在其他地方使用FP16来实现的。我们在图[6](#S8.F6
    "Figure 6 ‣ 8.1\. Mix-Precision Training ‣ 8\. Memory ‣ Large-Scale Deep Learning
    Optimizations: A Comprehensive Survey")中解释了混合精度训练的工作流程。由于可表示范围的不同，简单地将模型转换为FP16可能会导致梯度下溢或上溢问题。我们可以通过以下四个步骤来避免这些挑战：（1）转换为FP16。在第一步中，我们将计算密集型操作的输入从FP32转换为FP16。（2）使用FP32计算损失。由于FP16可能会引发下溢或上溢问题，我们在反向传播过程中使用FP32进行损失计算，并将梯度转换回FP16，这意味着权重和梯度仍然是FP16的。（3）FP32主权重。在反向传播过程中，梯度相较于参数较小。如果我们尝试用比参数小得多的梯度更新参数，那么我们可能会丢失这些参数更新。为此，我们将保持权重的主副本在FP32中。这意味着，在反向传播结束时，FP16梯度将被转换为FP32，并应用于FP32权重。在前向传播中，我们将权重转换为FP16，以便梯度计算保持在FP16中。因此，实际上我们有一个存储在FP32中的所有参数（包括权重和偏置）的主副本，而所有计算操作都将看到转换后的FP16版本。（4）损失（梯度）缩放。最后一步是进行损失缩放，以避免梯度下溢问题。在从FP32损失计算梯度之前，我们通过将损失乘以损失缩放因子来对损失进行缩放。通过这样做，梯度被推向更大的值，我们可以安全地在FP16中表示它们。稍后在更新权重时，我们可以通过将梯度除以相同的损失缩放因子来重新缩放梯度。'
- en: Their article (Micikevicius et al., [2018](#bib.bib73)) is not the first to
    propose the use of lower precision for training, but its influence is far-reaching,
    and many current programs are designed based on this work. Jia et al. ([2018](#bib.bib50))
    apply mixed-precision training to large-batch strategies such as LARS. Using LARS
    with mixed-precision training, ResNet-50 with the mini-batch size of 64K, could
    maintain the top-1 accuracy as 76.2%.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的文章（Micikevicius et al., [2018](#bib.bib73)）并不是第一个提出使用较低精度进行训练的，但其影响深远，许多当前程序都是基于这一工作设计的。Jia
    et al.（[2018](#bib.bib50)）将混合精度训练应用于大批量策略，如LARS。使用LARS与混合精度训练，ResNet-50在64K的小批量大小下，能够保持76.2%的top-1准确率。
- en: 8.2\. Memory Efficient Adaptive Optimization
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 内存高效自适应优化
- en: Some stochastic optimization methods (e.g., RMSProp, Adam (Kingma and Ba, [2017](#bib.bib54)),
    Adadelta (Zeiler, [2012](#bib.bib123))), keep first and second moment estimates
    of the per-parameter gradients to scale the gradients which triples the required
    memory. As models continue to grow, the memory overhead will pose more limitation
    on the quality of the trained model. Motivated by these challenges, memory efficient
    adaptive optimization methods are proposed to retain the benefits of standard
    per-parameter adaptivity while significantly reduce memory overhead. For instance,
    Adafactor (Shazeer and Stern, [2018](#bib.bib94)) was proposed as a way to reduce
    the memory costs of AdaGrad (Duchi et al., [2011](#bib.bib33)), primarily for
    training large language models, and SM3 (Anil et al., [2019](#bib.bib11)) saves
    memory by sharing moments of similar magnitude.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 一些随机优化方法（例如，RMSProp，Adam (Kingma和Ba，[2017](#bib.bib54)），Adadelta (Zeiler，[2012](#bib.bib123)))，保留每个参数梯度的一阶和二阶矩估计，以缩放梯度，这会增加三倍的内存需求。随着模型的不断增长，内存开销将对训练模型的质量构成更大的限制。受到这些挑战的激励，提出了内存高效的自适应优化方法，以保留标准每个参数自适应的好处，同时显著减少内存开销。例如，Adafactor
    (Shazeer和Stern，[2018](#bib.bib94))被提出作为一种降低AdaGrad (Duchi等，[2011](#bib.bib33))内存成本的方法，主要用于训练大型语言模型，而SM3
    (Anil等，[2019](#bib.bib11))通过共享类似大小的矩来节省内存。
- en: 8.2.1\. Adafactor
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1\. Adafactor
- en: Adafactor  (Shazeer and Stern, [2018](#bib.bib94)) is a space-efficient adaptive
    optimization which achieves a drastic reduction in auxiliary memory usage without
    hurting the performance (compared to that obtained using full accumulators). One
    of the key contribution is the use of factored second momentum estimation.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Adafactor (Shazeer和Stern，[2018](#bib.bib94))是一种空间高效的自适应优化方法，实现了在不影响性能的情况下（与使用完全累加器获得的性能相比）大幅减少辅助内存使用。一个关键贡献是使用了分解的二阶动量估计。
- en: 'Consider a matrix-shaped parameter subset $X$ with second moment estimate $V$.
    They want to identify a low-rank representation of $V$ as a product of two factors
    $R$ and $S$, i.e., $V\approx RS$ which is compatible with exponential moving averaging.
    This would allow us to store just the low-rank factors across iteration, cutting
    down a memory usage. More formally, if factorization $F:V\mapsto(R,S)$, we want
    $F(\eta V_{t-1}+(1-\eta)G_{t}^{2})=\eta F(V_{t-1})+(1-\eta)F(G_{t}^{2})$. In particular,
    by using techniques from non-negative matrix factorization using I-divergence,
    the low-rank approximation can be converted into following optimization problem:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个矩阵形状的参数子集$X$，具有二阶矩估计$V$。他们希望识别$V$的低秩表示作为两个因子$R$和$S$的乘积，即$V\approx RS$，这与指数移动平均兼容。这将允许我们在每次迭代中仅存储低秩因子，从而减少内存使用。更正式地，如果分解$F:V\mapsto(R,S)$，我们希望$F(\eta
    V_{t-1}+(1-\eta)G_{t}^{2})=\eta F(V_{t-1})+(1-\eta)F(G_{t}^{2})$。特别地，通过使用非负矩阵分解中的I-散度技术，低秩近似可以转换为以下优化问题：
- en: '| (57) |  | $\begin{split}\mathop{minimize}_{R\in\mathbb{R}^{n\times k},S\in\mathbb{R}^{k\times
    m}}\sum_{i=1}^{n}\sum_{j=1}^{m}d(V_{ij},[RS]_{ij})\\ subject\,to\quad R_{ij}>0,S_{ij}>0.\end{split}$
    |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| (57) |  | $\begin{split}\mathop{minimize}_{R\in\mathbb{R}^{n\times k},S\in\mathbb{R}^{k\times
    m}}\sum_{i=1}^{n}\sum_{j=1}^{m}d(V_{ij},[RS]_{ij})\\ subject\,to\quad R_{ij}>0,S_{ij}>0.\end{split}$
    |  |'
- en: 'In particular, for the case of rank one factors, i.e., $k=1$, the solution
    set of the optimization problem can be characterized as the set of all pairs $(R,S)$,
    whose product is equal to the expression below:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，对于秩为一的因子的情况，即$k=1$，优化问题的解集可以表征为所有对$(R,S)$的集合，其乘积等于以下表达式：
- en: '| (58) |  | $\{(R,S):RS=\underbrace{V1_{m}}_{row\atop sums}\underbrace{1_{n}^{T}V}_{colum\atop
    sums}/\underbrace{1_{n}^{T}V1_{m}}_{sum\,of\atop all\,entries}\}.$ |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| (58) |  | $\{(R,S):RS=\underbrace{V1_{m}}_{row\atop sums}\underbrace{1_{n}^{T}V}_{colum\atop
    sums}/\underbrace{1_{n}^{T}V1_{m}}_{sum\,of\atop all\,entries}\}.$ |  |'
- en: The right hand side can be broken down into the vector of row sums and column
    sums, and the denominator is the sum of all entries. In addition to the factored
    second moment estimation, other key changes in Adafactor include $\eta_{2}$ varies
    with time, update cliping, relative step size and no momentum.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧可以被分解为行和列的向量，总分母是所有条目的总和。除了分解的二阶矩估计外，Adafactor的其他关键变化包括$\eta_{2}$随时间变化、更新剪裁、相对步长和无动量。
- en: 8.2.2\. SM3 Algorithm
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2\. SM3算法
- en: 'Adaptive gradient methods, such as AdaGrad (Duchi et al., [2011](#bib.bib33)),
    have proved to be particularly useful in training sparse models. Crucially, however,
    Adagrad must maintain auxiliary sequence of accumulators (i.e., the diagonal preconditioner)
    $H_{t}$ (also in Eq.[[10](#S3.E10 "In 3.3.1\. AdaGrad ‣ 3.3\. Adaptive Gradient
    Algorithms ‣ 3\. Gradient Descent Optimization Algorithms ‣ Large-Scale Deep Learning
    Optimizations: A Comprehensive Survey")]) :'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '自适应梯度方法，如AdaGrad (Duchi et al., [2011](#bib.bib33))，在训练稀疏模型时已被证明特别有用。然而，关键的是，AdaGrad必须维护辅助累加器序列（即对角预处理器）$H_{t}$（也见于
    Eq.[[10](#S3.E10 "In 3.3.1\. AdaGrad ‣ 3.3\. Adaptive Gradient Algorithms ‣ 3\.
    Gradient Descent Optimization Algorithms ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey")])：'
- en: '| (59) |  | $H_{t,ii}=\sum_{s\leq t}g_{s,ii}^{2}$ |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| (59) |  | $H_{t,ii}=\sum_{s\leq t}g_{s,ii}^{2}$ |  |'
- en: and thus needs $\Omega(n)$ additional space $n$ is the number of parameters.
    SM3  (Anil et al., [2019](#bib.bib11)) provides a memory-efficient methods with
    comparable convergence characteristics which refrains from maintaining the full
    vectors. SM3 is short for save memory by sharing moments of similar magnitude.
    This is because they observe that the diagonal preconditioners $H_{t}$ accumulated
    by AdaGrad are actually similar in rows and columns, and by sharing moments cross
    rows and columns, the memory requirements therefore drop from $\Theta(mn)$ to
    merely $\Theta(m+n)$.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 因此需要 $\Omega(n)$ 的额外空间，其中 $n$ 是参数的数量。SM3 (Anil et al., [2019](#bib.bib11)) 提供了一种内存高效的方法，具有类似的收敛特性，避免了维护完整向量。SM3
    是节省内存，通过共享类似幅度的矩量缩写。这是因为他们观察到，AdaGrad 累积的对角预处理器 $H_{t}$ 在行和列上实际上是相似的，通过在行和列之间共享矩量，内存需求因此从
    $\Theta(mn)$ 降低到仅仅 $\Theta(m+n)$。
- en: '| (60) |  | <math   alttext="\begin{split}\widehat{H_{t+1},ij}&amp;=min(R_{t,i},C_{t,j})+g_{t+1,ij}^{2}\\
    R_{t,i}&amp;=\mathop{max}\limits_{j}(\widehat{H_{t+1},ij})\\'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '| (60) |  | <math alttext="\begin{split}\widehat{H_{t+1},ij}&amp;=min(R_{t,i},C_{t,j})+g_{t+1,ij}^{2}\\
    R_{t,i}&amp;=\mathop{max}\limits_{j}(\widehat{H_{t+1},ij})\\'
- en: C_{t,j}&amp;=\mathop{max}\limits_{i}(\widehat{H_{t+1},ij})\end{split}" display="block"><semantics
    ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr ><mtd  columnalign="right"
    ><mover accent="true" ><mrow  ><msub ><mi >H</mi><mrow ><mi >t</mi><mo >+</mo><mn
    >1</mn></mrow></msub><mo >,</mo><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >j</mi></mrow></mrow><mo >^</mo></mover></mtd><mtd columnalign="left"  ><mrow
    ><mo >=</mo><mrow ><mrow  ><mi >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub ><mi  >R</mi><mrow ><mi >t</mi><mo >,</mo><mi
    >i</mi></mrow></msub><mo >,</mo><msub ><mi  >C</mi><mrow ><mi >t</mi><mo >,</mo><mi
    >j</mi></mrow></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><msubsup
    ><mi >g</mi><mrow ><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow><mo >,</mo><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow></mrow><mn
    >2</mn></msubsup></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><msub
    ><mi >R</mi><mrow ><mi >t</mi><mo >,</mo><mi >i</mi></mrow></msub></mtd><mtd columnalign="left"  ><mrow
    ><mo >=</mo><mrow ><munder  ><mrow ><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >x</mi></mrow><mi >j</mi></munder><mrow
    ><mo stretchy="false" >(</mo><mover accent="true" ><mrow  ><msub ><mi >H</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><mi >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow></mrow><mo >^</mo></mover><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><msub
    ><mi >C</mi><mrow ><mi >t</mi><mo >,</mo><mi >j</mi></mrow></msub></mtd><mtd columnalign="left"  ><mrow
    ><mo >=</mo><mrow ><munder  ><mrow ><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >x</mi></mrow><mi >i</mi></munder><mrow
    ><mo stretchy="false" >(</mo><mover accent="true" ><mrow  ><msub ><mi >H</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><mi >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow></mrow><mo >^</mo></mover><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >^</ci><list ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐻</ci><apply ><ci >𝑡</ci><cn type="integer"
    >1</cn></apply></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></list></apply><apply
    ><apply  ><ci >𝑚</ci><ci >𝑖</ci><ci  >𝑛</ci><interval closure="open"  ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑅</ci><list ><ci  >𝑡</ci><ci
    >𝑖</ci></list></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐶</ci><list ><ci >𝑡</ci><ci >𝑗</ci></list></apply></interval></apply><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑔</ci><list ><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply><apply
    ><ci >𝑖</ci><ci >𝑗</ci></apply></list></apply><cn type="integer" >2</cn></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑅</ci><list ><ci >𝑡</ci><ci
    >𝑖</ci></list></apply></apply></apply></apply><apply ><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply ><ci >𝑚</ci><ci >𝑎</ci><ci >𝑥</ci></apply><ci
    >𝑗</ci></apply><apply ><apply  ><ci >^</ci><list ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐻</ci><apply ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply><apply ><ci
    >𝑖</ci><ci >𝑗</ci></apply></list></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐶</ci><list ><ci >𝑡</ci><ci >𝑗</ci></list></apply></apply></apply></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci >𝑚</ci><ci
    >𝑎</ci><ci >𝑥</ci></apply><ci >𝑖</ci></apply><apply ><ci  >^</ci><list ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐻</ci><apply ><ci >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\widehat{H_{t+1},ij}&=min(R_{t,i},C_{t,j})+g_{t+1,ij}^{2}\\
    R_{t,i}&=\mathop{max}\limits_{j}(\widehat{H_{t+1},ij})\\ C_{t,j}&=\mathop{max}\limits_{i}(\widehat{H_{t+1},ij})\end{split}</annotation></semantics></math>
    |  |
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`C_{t,j}` = \mathop{max}\limits_{i}(\widehat{H_{t+1},ij})\end{split}" display="block"><semantics
    ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr ><mtd  columnalign="right"
    ><mover accent="true" ><mrow  ><msub ><mi >H</mi><mrow ><mi >t</mi><mo >+</mo><mn
    >1</mn></mrow></msub><mo >,</mo><mrow ><mi >i</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >j</mi></mrow></mrow><mo >^</mo></mover></mtd><mtd columnalign="left"  ><mrow
    ><mo >=</mo><mrow ><mrow  ><mi >m</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><msub ><mi  >R</mi><mrow ><mi >t</mi><mo >,</mo><mi
    >i</mi></mrow></msub><mo >,</mo><msub ><mi  >C</mi><mrow ><mi >t</mi><mo >,</mo><mi
    >j</mi></mrow></msub><mo stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><msubsup
    ><mi >g</mi><mrow ><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow><mo >,</mo><mrow
    ><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >j</mi></mrow></mrow><mn
    >2</mn></msubsup></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><msub
    ><mi >R</mi><mrow ><mi >t</mi><mo >,</mo><mi >i</mi></mrow></msub></mtd><mtd columnalign="left"  ><mrow
    ><mo >=</mo><mrow ><munder  ><mrow ><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >x</mi></mrow><mi >j</mi></munder><mrow
    ><mo stretchy="false" >(</mo><mover accent="true" ><mrow  ><msub ><mi >H</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><mi >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow></mrow><mo >^</mo></mover><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><msub
    ><mi >C</mi><mrow ><mi >t</mi><mo >,</mo><mi >j</mi></mrow></msub></mtd><mtd columnalign="left"  ><mrow
    ><mo >=</mo><mrow ><munder  ><mrow ><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >x</mi></mrow><mi >i</mi></munder><mrow
    ><mo stretchy="false" >(</mo><mover accent="true" ><mrow  ><msub ><mi >H</mi><mrow
    ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></msub><mo >,</mo><mrow ><mi >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >j</mi></mrow></mrow><mo >^</mo></mover><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >^</ci><list ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐻</ci><apply ><ci >𝑡</ci><cn type="integer"
    >1</cn></apply></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></list></apply><apply
    ><apply  ><ci >𝑚</ci><ci >𝑖</ci><ci  >𝑛</ci><interval closure="open"  ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑅</ci><list ><ci  >𝑡</ci><ci
    >𝑖</ci></list></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐶</ci><list ><ci >𝑡</ci><ci >𝑗</ci></list></apply></interval></apply><apply ><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑔</ci><list ><apply ><ci >𝑡</ci><cn type="integer" >1</cn></apply><apply
    ><ci >𝑖</ci><ci >𝑗</ci></apply></list></apply><cn type="integer" >2</cn></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑅</ci><list ><ci >𝑡</ci><ci
    >𝑖</ci></list></apply></apply></apply></apply><apply ><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply ><ci >𝑚</ci><ci >𝑎</ci><ci >𝑥</ci></apply><ci
    >𝑗</ci></apply><apply ><apply  ><ci >^</ci><list ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝐻</ci><apply ><ci >𝑡</ci><cn type="integer"  >1</cn></apply></apply><apply ><ci
    >𝑖</ci><ci >𝑗</ci></apply></list></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐶</ci><list ><ci >𝑡</ci><ci >𝑗</ci></list></apply></apply></apply></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci >𝑚</ci><ci
    >𝑎</ci><ci >𝑥</ci></apply><ci >𝑖</ci></apply><apply ><ci  >^</ci><list ><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝐻</ci><apply ><ci >𝑡</ci><cn
    type="integer"  >1</cn></apply></apply><apply ><ci >𝑖</ci><ci >𝑗</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\widehat{H_{t+1},ij}&=min(R_{t,i},C_{t,j})+g_{t+1,ij}^{2}\\
    R_{t,i}&=\mathop{max}\limits_{j}(\widehat{H_{t+1},ij})\\ C_{t,j}&=\mathop{max}\limits_{i}(\widehat{H_{t+1},ij})\end{split}</annotation></semantics></math>
    |  |'
- en: 'SM3 can be viewed as a diagonal version of Shampoo (see Table [1](#S6.T1 "Table
    1 ‣ 6.4.3\. Distributed K-FAC ‣ 6.4\. K-FAC ‣ 6\. Second Order Optimization ‣
    Large-Scale Deep Learning Optimizations: A Comprehensive Survey")). We refer readers
    about the implementation details to  (Anil et al., [2019](#bib.bib11)).'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 'SM3可以看作是Shampoo的对角线版本（见表[1](#S6.T1 "Table 1 ‣ 6.4.3\. Distributed K-FAC ‣ 6.4\.
    K-FAC ‣ 6\. Second Order Optimization ‣ Large-Scale Deep Learning Optimizations:
    A Comprehensive Survey")）。我们将实现细节参考给读者（Anil et al., [2019](#bib.bib11)）。'
- en: 8.3\. ZeRO
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. ZeRO
- en: '![Refer to caption](img/539efd0cfe3dc78b8eaf245e9936f92d.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/539efd0cfe3dc78b8eaf245e9936f92d.png)'
- en: Figure 7\. Comparing the per-device memory consumption of model states, with
    three stages of ZeRO-DP optimizations. Src:(Rajbhandari et al., [2020](#bib.bib82)).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. 比较模型状态在设备上的内存消耗，以及ZeRO-DP优化的三个阶段。来源：(Rajbhandari et al., [2020](#bib.bib82))。
- en: The Zero Redundancy Optimizer (Rajbhandari et al., [2020](#bib.bib82)) (abbreviated
    as ZeRO) is a novel memory optimization technology for large-scale distributed
    DL. Contrary to Adafactor  (Shazeer and Stern, [2018](#bib.bib94)) and SM3 (Anil
    et al., [2019](#bib.bib11)) which reduce memory consumption of adaptive optimization
    methods by maintaining coarser-grained statistics of model parameters and gradients,
    ZeRO do not change the model optimization method or affect the model convergence.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Zero Redundancy Optimizer (Rajbhandari et al., [2020](#bib.bib82))（简称ZeRO）是一种针对大规模分布式深度学习的新型内存优化技术。与Adafactor
    (Shazeer and Stern, [2018](#bib.bib94))和SM3 (Anil et al., [2019](#bib.bib11))通过保持模型参数和梯度的粗粒度统计来减少自适应优化方法的内存消耗不同，ZeRO不改变模型优化方法或影响模型收敛。
- en: 'We show how ZeRO works in Fig.[[7](#S8.F7 "Figure 7 ‣ 8.3\. ZeRO ‣ 8\. Memory
    ‣ Large-Scale Deep Learning Optimizations: A Comprehensive Survey")]. The first
    row shows the memory map while training a model in data parallel. The first row
    shown in blue represents the memory consumed by the parameters; the second row
    in orange shows the memory consumed by the gradients; and the big chunk in green
    shows the memory consumed by the optimizer states (e.g., this could be momentum
    and variance for Adam). So the key thing is that these optimizer states gradients
    and parameters (which we collectively call the model states) are replicated across
    all the different GPUs in distributed data parallel training. The way ZeRO works
    is by removing this redundancy across these GPUs. Since there are three different
    types of model states, there are three different phases of ZeRO, each of them
    removing the redundancy for one of these states by simply partitioning these model
    states across GPUs instead of replicating them.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[[7](#S8.F7 "Figure 7 ‣ 8.3\. ZeRO ‣ 8\. Memory ‣ Large-Scale Deep Learning
    Optimizations: A Comprehensive Survey")]中展示了ZeRO的工作原理。第一行显示了在数据并行训练模型时的内存映射。第一行用蓝色显示了由参数消耗的内存；第二行用橙色显示了由梯度消耗的内存；而绿色的大块区域显示了由优化器状态（例如Adam的动量和方差）消耗的内存。所以关键点在于，这些优化器状态、梯度和参数（我们统称为模型状态）在分布式数据并行训练中在所有不同的GPU上都会被复制。ZeRO的工作方式是通过消除这些GPU之间的冗余。由于模型状态有三种不同类型，ZeRO有三个不同的阶段，每个阶段通过简单地将这些模型状态分割到GPU上而不是复制它们，来消除其中一种状态的冗余。'
- en: 'In addition to reducing the memory from model states, ZeRO also has a bunch
    of more optimizations that allows reduction in memory from other components (see
    Table [3](#S8.T3 "Table 3 ‣ 8.3\. ZeRO ‣ 8\. Memory ‣ Large-Scale Deep Learning
    Optimizations: A Comprehensive Survey")). For example, just like how model states
    are replicated across multiple GPUs in data parallel training, ZeRO removes the
    redundancy in activation memory by partitioning the activations across these model
    parallel GPUs. We can also offload these activation memories to CPU if we don’t
    have enough memory to train extremely large models. The next optimization that
    ZeRO can do is to convert fragmented memory to defragmented memory on the fly
    during training. During training if the memory is fragmented, we might still run
    out of memory even though there might be enough fragmented memory that can satisfy
    the request if they were contiguous. In ZeRO the memory defragmentation will on
    the fly defragment these memory fragments so that all the memory is contiguous
    and you are able to satisfy these larger memory requests. So with all these different
    memory optimizations, ZeRO is able to train models with up to 200 billion parameters
    up to 10 times faster than the SOTA.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 除了减少模型状态的内存，ZeRO 还具有一系列其他优化，这些优化可以减少来自其他组件的内存（见表 [3](#S8.T3 "表 3 ‣ 8.3\. ZeRO
    ‣ 8\. 内存 ‣ 大规模深度学习优化：全面调查")）。例如，就像在数据并行训练中模型状态在多个 GPU 之间复制一样，ZeRO 通过在这些模型并行 GPU
    之间划分激活来消除激活内存的冗余。如果我们没有足够的内存来训练极其庞大的模型，我们还可以将这些激活内存卸载到 CPU。ZeRO 还可以在训练过程中即时将碎片化内存转换为连续内存。如果训练过程中内存碎片化，即使有足够的碎片化内存满足请求，也可能会出现内存不足的情况。ZeRO
    会即时将这些内存碎片进行整理，使所有内存连续，从而能够满足更大的内存请求。因此，通过所有这些不同的内存优化，ZeRO 能够以比 SOTA 快 10 倍的速度训练最多
    2000 亿参数的模型。
- en: '| Memory Consumption | Optimization |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 内存消耗 | 优化 |'
- en: '| Model State Memory | Partition optimizer state, gradient, and parameters
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 模型状态内存 | 划分优化器状态、梯度和参数 |'
- en: '| Activation Memory | Partition activations; Offload to GPU |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 激活内存 | 划分激活；卸载到 GPU |'
- en: '| Fragmented Memory | Proactively manage memory w.r.t tensor lifetime |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 碎片化内存 | 主动管理内存与张量生命周期相关 |'
- en: Table 3\. Different Memory Optimizations in ZeRO
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. ZeRO 中的不同内存优化
- en: 9\. Conclusion
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 结论
- en: 'Given larger datasets and bigger models consistently yielding significant improvements
    in accuracy, large-scale deep learning has become an inevitable trend. As datasets
    increase in size and DNNs in complexity, the computational intensity, communication
    cost and memory demands of deep learning increase proportionally. Considerable
    efforts have been devoted to accelerating the training speed. In this article,
    we give an overview of large-scale deep learning optimization. The goal in general
    is two-fold: model accuracy and model efficiency. As for the model accuracy, we
    investigate algorithms that are most commonly used for optimizing, spanning from
    the gradient descent variants to the (large-batch) adaptive methods, and from
    first-order to second-order methods. Further, we elaborate the debatable topic
    of generalization gap arises in large-batch training. As for the model efficiency,
    we summarise the SOTA techniques in addressing the expensive cost of communication
    overhead and memory footprint. We hope this article can provide a clean sketch
    for those who are interested in training large-scale training.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 随着更大数据集和更大模型持续带来显著的准确性提升，大规模深度学习已成为不可避免的趋势。随着数据集的增大和 DNN 的复杂性提高，深度学习的计算强度、通信成本和内存需求也成比例增加。大量努力致力于加速训练速度。在本文中，我们概述了大规模深度学习优化。总体目标有两个：模型准确性和模型效率。对于模型准确性，我们研究了最常用于优化的算法，包括梯度下降变体到（大批量）自适应方法，以及从一阶方法到二阶方法。此外，我们详细阐述了在大批量训练中出现的争议话题——泛化差距。对于模型效率，我们总结了在解决昂贵的通信开销和内存占用方面的
    SOTA 技术。我们希望本文能为那些对大规模训练感兴趣的读者提供清晰的概述。
- en: References
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Abadi et al. (2016) Martín Abadi, Paul Barham, Jianmin Chen, Z. Chen, Andy
    Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael
    Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon
    Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,
    Yuan Yu, and Xiaoqian Zhang. 2016. TensorFlow: A system for large-scale machine
    learning. In *OSDI*.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abadi 等 (2016) Martín Abadi, Paul Barham, Jianmin Chen, Z. Chen, Andy Davis,
    Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
    Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray,
    Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
    Yu, 和 Xiaoqian Zhang. 2016. TensorFlow: 一个大规模机器学习系统. 见于 *OSDI*。'
- en: Agarwal et al. (2016a) Naman Agarwal, Brian Bullins, and Elad Hazan. 2016a.
    Second-order stochastic optimization in linear time. *stat* 1050 (2016), 15.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等 (2016a) Naman Agarwal, Brian Bullins, 和 Elad Hazan. 2016a. 线性时间内的二阶随机优化.
    *stat* 1050 (2016), 15。
- en: Agarwal et al. (2016b) Naman Agarwal, Zeyuan Allen Zhu, Brian Bullins, Elad
    Hazan, and Tengyu Ma. 2016b. Finding Approximate Local Minima for Nonconvex Optimization
    in Linear Time. *CoRR* abs/1611.01146 (2016).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等 (2016b) Naman Agarwal, Zeyuan Allen Zhu, Brian Bullins, Elad Hazan,
    和 Tengyu Ma. 2016b. 在线性时间内找到非凸优化的近似局部最小值. *CoRR* abs/1611.01146 (2016)。
- en: Aji and Heafield (2017) Alham Fikri Aji and Kenneth Heafield. 2017. Sparse Communication
    for Distributed Gradient Descent. In *Proceedings of the 2017 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September
    9-11, 2017*. 440–445.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aji 和 Heafield (2017) Alham Fikri Aji 和 Kenneth Heafield. 2017. 分布式梯度下降的稀疏通信.
    见于 *2017年自然语言处理经验方法会议, EMNLP 2017, 丹麦哥本哈根, 2017年9月9-11日*。440–445。
- en: 'Alistarh et al. (2017) Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka,
    and Milan Vojnovic. 2017. QSGD: Communication-Efficient SGD via Gradient Quantization
    and Encoding. In *NIPS*. 1709–1720.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alistarh 等 (2017) Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, 和 Milan
    Vojnovic. 2017. QSGD: 通过梯度量化和编码的通信高效SGD. 见于 *NIPS*。1709–1720。'
- en: 'Alistarh et al. (2018) Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola
    Konstantinov, Sarit Khirirat, and Cédric Renggli. 2018. The Convergence of Sparsified
    Gradient Methods. In *Advances in Neural Information Processing Systems 31: Annual
    Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December
    3-8, 2018, Montréal, Canada*. 5977–5987.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alistarh 等 (2018) Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov,
    Sarit Khirirat, 和 Cédric Renggli. 2018. 稀疏化梯度方法的收敛性. 见于 *2018年神经信息处理系统年会: NeurIPS
    2018, 2018年12月3-8日, 加拿大蒙特利尔*。5977–5987。'
- en: Amari (1998) Shun-ichi Amari. 1998. Natural Gradient Works Efficiently in Learning.
    *Neural Comput.* 10, 2 (1998), 251–276.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amari (1998) Shun-ichi Amari. 1998. 自然梯度在学习中高效工作. *Neural Comput.* 10, 2 (1998),
    251–276。
- en: Amari et al. (2000) Shun-ichi Amari, Hyeyoung Park, and Kenji Fukumizu. 2000.
    Adaptive Method of Realizing Natural Gradient Learning for Multilayer Perceptrons.
    *Neural Comput.* 12, 6 (2000), 1399–1409.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amari 等 (2000) Shun-ichi Amari, Hyeyoung Park, 和 Kenji Fukumizu. 2000. 实现自然梯度学习的自适应方法用于多层感知器.
    *Neural Comput.* 12, 6 (2000), 1399–1409。
- en: Anil et al. (2020) Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram
    Singer. 2020. Scalable second order optimization for deep learning. *arXiv preprint
    arXiv:2002.09018* (2020).
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil 等 (2020) Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, 和 Yoram Singer.
    2020. 深度学习的可扩展二阶优化. *arXiv 预印本 arXiv:2002.09018* (2020)。
- en: Anil et al. (2019) Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer.
    2019. Memory Efficient Adaptive Optimization. In *NeurIPS*.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil 等 (2019) Rohan Anil, Vineet Gupta, Tomer Koren, 和 Yoram Singer. 2019. 内存高效的自适应优化.
    见于 *NeurIPS*。
- en: 'Battiti (1992) Roberto Battiti. 1992. First- and Second-Order Methods for Learning:
    Between Steepest Descent and Newton’s Method. *Neural Computation* 4, 2 (1992),
    141–166. [https://doi.org/10.1162/neco.1992.4.2.141](https://doi.org/10.1162/neco.1992.4.2.141)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Battiti (1992) Roberto Battiti. 1992. 学习的第一阶和第二阶方法: 在最陡下降法和牛顿法之间. *Neural Computation*
    4, 2 (1992), 141–166。 [https://doi.org/10.1162/neco.1992.4.2.141](https://doi.org/10.1162/neco.1992.4.2.141)'
- en: 'Ben-Nun and Hoefler (2019) Tal Ben-Nun and Torsten Hoefler. 2019. Demystifying
    Parallel and Distributed Deep Learning: An In-depth Concurrency Analysis. *ACM
    Comput. Surv.* 52, 4 (2019), 65:1–65:43.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ben-Nun 和 Hoefler (2019) Tal Ben-Nun 和 Torsten Hoefler. 2019. 揭示并行和分布式深度学习:
    深入的并发分析. *ACM Comput. Surv.* 52, 4 (2019), 65:1–65:43。'
- en: 'Bernstein et al. (2018) Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli,
    and Animashree Anandkumar. 2018. signSGD: Compressed optimisation for non-convex
    problems. In *International Conference on Machine Learning*. PMLR, 560–569.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bernstein 等 (2018) Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli,
    和 Animashree Anandkumar. 2018. signSGD: 针对非凸问题的压缩优化. 见于 *国际机器学习大会*。PMLR, 560–569。'
- en: 'Betzel et al. (2018) Filipe Betzel, S. Karen Khatamifard, Harini Suresh, David J.
    Lilja, John Sartori, and Ulya R. Karpuzcu. 2018. Approximate Communication: Techniques
    for Reducing Communication Bottlenecks in Large-Scale Parallel Systems. *ACM Comput.
    Surv.* 51, 1 (2018), 1:1–1:32.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Betzel 等（2018）费利佩·贝泽尔、S·凯伦·卡塔米法德、哈里尼·苏雷什、大卫·J·利利亚、约翰·萨托里和乌利亚·R·卡尔普祖库。2018年。近似通信：减少大规模并行系统中的通信瓶颈的技术。*ACM计算机调查*
    51，第1期（2018），1:1–1:32。
- en: Bollapragada et al. (2016) Raghu Bollapragada, Richard Byrd, and Jorge Nocedal.
    2016. Exact and Inexact Subsampled Newton Methods for Optimization. arXiv:1609.08502 [math.OC]
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bollapragada 等（2016）拉古·博拉普拉加达、理查德·伯德和豪尔赫·诺塞达。2016年。优化的精确和不精确子样本牛顿方法。arXiv:1609.08502
    [math.OC]
- en: 'Bordes et al. (2009) Antoine Bordes, Léon Bottou, and Patrick Gallinari. 2009.
    SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent. *J. Mach. Learn. Res.*
    10 (2009), 1737–1754.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bordes 等（2009）安托万·博尔德斯、莱昂·博图和帕特里克·加利纳里。2009年。SGD-QN：精心设计的准牛顿随机梯度下降。*机器学习研究期刊*
    10（2009），1737–1754。
- en: Botev et al. (2017) Aleksandar Botev, Hippolyt Ritter, and David Barber. 2017.
    Practical gauss-newton optimisation for deep learning. In *International Conference
    on Machine Learning*. PMLR, 557–565.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Botev 等（2017）亚历山大·博特夫、希波吕特·里特和大卫·巴伯。2017年。深度学习中的实际高斯-牛顿优化。发表于*国际机器学习会议*。PMLR，557–565。
- en: Bottou and Bousquet (2007) Léon Bottou and Olivier Bousquet. 2007. The Tradeoffs
    of Large Scale Learning. In *Proceedings of the 20th International Conference
    on Neural Information Processing Systems*. 161–168.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bottou 和 Bousquet（2007）莱昂·博图和奥利维耶·布斯凯。2007年。大规模学习的权衡。发表于*第20届国际神经信息处理系统会议*。161–168。
- en: Bottou et al. (2018) Léon Bottou, Frank E. Curtis, and Jorge Nocedal. 2018.
    Optimization Methods for Large-Scale Machine Learning. *SIAM Rev.* 60, 2 (2018),
    223–311.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bottou 等（2018）莱昂·博图、弗兰克·E·柯蒂斯和豪尔赫·诺塞达。2018年。大规模机器学习的优化方法。*SIAM评论* 60，第2期（2018），223–311。
- en: Byrd et al. (2012) Richard H. Byrd, Gillian M. Chin, Jorge Nocedal, and Yuchen
    Wu. 2012. Sample size selection in optimization methods for machine learning.
    *Math. Program.* 134, 1 (2012), 127–155.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Byrd 等（2012）理查德·H·伯德、吉莉安·M·钦、豪尔赫·诺塞达和余晨武。2012年。机器学习优化方法中的样本大小选择。*数学编程* 134，第1期（2012），127–155。
- en: Carmon et al. (2018) Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford.
    2018. Accelerated Methods for NonConvex Optimization. *SIAM J. Optim.* 28, 2 (2018),
    1751–1772.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carmon 等（2018）亚伊尔·卡尔蒙、约翰·C·杜奇、奥利弗·辛德和亚伦·西德福德。2018年。非凸优化的加速方法。*SIAM优化期刊* 28，第2期（2018），1751–1772。
- en: 'Chaudhari et al. (2017) Pratik Chaudhari, Anna Choromanska, Stefano Soatto,
    Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer T. Chayes, Levent Sagun,
    and Riccardo Zecchina. 2017. Entropy-SGD: Biasing Gradient Descent Into Wide Valleys.
    In *5th International Conference on Learning Representations, ICLR 2017, Toulon,
    France, April 24-26, 2017, Conference Track Proceedings*.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaudhari 等（2017）普拉提克·乔达里、安娜·乔罗曼斯卡、斯特凡诺·索阿托、扬·勒昆、卡洛·巴尔达西、克里斯蒂安·博尔格斯、詹妮弗·T·查耶斯、莱文特·萨贡和里卡多·泽基纳。2017年。熵-SGD：将梯度下降偏向宽阔的山谷。发表于*第5届国际表示学习会议，ICLR
    2017，法国图卢兹，2017年4月24-26日，会议论文集*。
- en: 'Chen et al. (2018) Chia-Yu Chen, Jungwook Choi, Daniel Brand, Ankur Agrawal,
    Wei Zhang, and Kailash Gopalakrishnan. 2018. Adacomp: Adaptive residual gradient
    compression for data-parallel distributed training. In *Proceedings of the AAAI
    Conference on Artificial Intelligence*, Vol. 32.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2018）陈佳瑜、郑旭、丹尼尔·布兰德、安库尔·阿格拉瓦尔、魏章和凯拉什·戈帕拉克里希南。2018年。Adacomp：用于数据并行分布式训练的自适应残差梯度压缩。发表于*AAAI人工智能会议论文集*，第32卷。
- en: Chen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E.
    Hinton. 2020. A Simple Framework for Contrastive Learning of Visual Representations.
    In *Proceedings of the 37th International Conference on Machine Learning, ICML
    2020, 13-18 July 2020, Virtual Event* *(Proceedings of Machine Learning Research,
    Vol. 119)*. PMLR, 1597–1607.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020）丁陈、西蒙·科恩布利斯、穆罕默德·诺鲁兹和杰弗里·E·辛顿。2020年。用于对比学习视觉表示的简单框架。发表于*第37届国际机器学习会议，ICML
    2020，2020年7月13-18日，虚拟会议*（*机器学习研究论文集，第119卷*）。PMLR，1597–1607。
- en: Choi et al. (2019) Dami Choi, Christopher J. Shallue, Zachary Nado, Jaehoon
    Lee, Chris J. Maddison, and George E. Dahl. 2019. On Empirical Comparisons of
    Optimizers for Deep Learning. *CoRR* abs/1910.05446 (2019).
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等（2019）崔达米、克里斯托弗·J·沙卢、扎卡里·纳多、李宰勋、克里斯·J·马迪森和乔治·E·达尔。2019年。关于深度学习优化器的实证比较。*CoRR*
    abs/1910.05446（2019）。
- en: Conn et al. (2000) Andrew R. Conn, Nicholas I. M. Gould, and Philippe L. Toint.
    2000. *Trust Region Methods*. SIAM.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conn 等（2000）安德鲁·R·康、尼古拉斯·I·M·古尔德和菲利普·L·托因特。2000年。*信赖域方法*。SIAM。
- en: 'Devarakonda et al. (2017) Aditya Devarakonda, Maxim Naumov, and Michael Garland.
    2017. AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks. *ArXiv*
    abs/1712.02029 (2017).'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devarakonda 等（2017）Aditya Devarakonda、Maxim Naumov 和 Michael Garland。2017。AdaBatch：用于训练深度神经网络的自适应批量大小。*ArXiv*
    abs/1712.02029（2017年）。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *NAACL-HLT (1)*.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2019）Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2019。BERT：用于语言理解的深度双向变换器的预训练。在
    *NAACL-HLT（1）*。
- en: Dinh et al. (2017) Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
    2017. Sharp Minima Can Generalize For Deep Nets. In *International Conference
    on Machine Learning*. PMLR, 1019–1028.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dinh 等（2017）Laurent Dinh、Razvan Pascanu、Samy Bengio 和 Yoshua Bengio。2017。Sharp
    Minima 可以为深度网络提供泛化。在 *国际机器学习会议*。PMLR，第1019–1028页。
- en: Dozat (2016) Timothy Dozat. 2016. Incorporating nesterov momentum into adam.
    (2016).
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dozat（2016）Timothy Dozat。2016。将 Nesterov 动量纳入 Adam。（2016年）。
- en: Dryden et al. (2016) Nikoli Dryden, Tim Moon, Sam Ade Jacobs, and Brian Van Essen.
    2016. Communication quantization for data-parallel training of deep neural networks.
    In *2016 2nd Workshop on Machine Learning in HPC Environments (MLHPC)*. IEEE,
    1–8.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dryden 等（2016）Nikoli Dryden、Tim Moon、Sam Ade Jacobs 和 Brian Van Essen。2016。用于深度神经网络数据并行训练的通信量化。在
    *2016年第2届HPC环境中的机器学习研讨会（MLHPC）*。IEEE，第1–8页。
- en: Duchi et al. (2011) John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive
    subgradient methods for online learning and stochastic optimization. *Journal
    of machine learning research* 12, 7 (2011).
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duchi 等（2011）John Duchi、Elad Hazan 和 Yoram Singer。2011。用于在线学习和随机优化的自适应子梯度方法。*机器学习研究期刊*
    第12卷，第7期（2011年）。
- en: Foret et al. (2021) Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam
    Neyshabur. 2021. Sharpness-Aware Minimization for Efficiently Improving Generalization.
    arXiv:2010.01412 [cs.LG]
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foret 等（2021）Pierre Foret、Ariel Kleiner、Hossein Mobahi 和 Behnam Neyshabur。2021。Sharpness-Aware
    Minimization 以高效提升泛化能力。arXiv:2010.01412 [cs.LG]
- en: Ge et al. (2015) Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. 2015. Escaping
    from saddle points—online stochastic gradient for tensor decomposition. In *Conference
    on learning theory*. PMLR, 797–842.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge 等（2015）Rong Ge、Furong Huang、Chi Jin 和 Yang Yuan。2015。从鞍点逃逸——张量分解的在线随机梯度。在
    *学习理论会议*。PMLR，第797–842页。
- en: Girshick (2015) Ross Girshick. 2015. Fast r-cnn. In *Proceedings of the IEEE
    international conference on computer vision*. 1440–1448.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girshick（2015）Ross Girshick。2015。Fast R-CNN。在 *IEEE 国际计算机视觉会议论文集*。第1440–1448页。
- en: 'Goldfarb et al. (2020) Donald Goldfarb, Yi Ren, and Achraf Bahamou. 2020. Practical
    Quasi-Newton Methods for Training Deep Neural Networks. In *Advances in Neural
    Information Processing Systems 33: Annual Conference on Neural Information Processing
    Systems 2020, NeurIPS 2020*.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldfarb 等（2020）Donald Goldfarb、Yi Ren 和 Achraf Bahamou。2020。用于训练深度神经网络的实用准牛顿方法。在
    *神经信息处理系统 33：2020年年度神经信息处理系统会议，NeurIPS 2020*。
- en: 'Goyal et al. (2018) Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis,
    Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
    2018. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv:1706.02677 [cs.CV]'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal 等（2018）Priya Goyal、Piotr Dollár、Ross Girshick、Pieter Noordhuis、Lukasz
    Wesolowski、Aapo Kyrola、Andrew Tulloch、Yangqing Jia 和 Kaiming He。2018。准确的大批量 SGD：在1小时内训练
    ImageNet。arXiv:1706.02677 [cs.CV]
- en: 'Grill et al. (2020) Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin
    Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Pires, Zhaohan
    Guo, Mohammad Azar, et al. 2020. Bootstrap Your Own Latent: A new approach to
    self-supervised learning. In *Neural Information Processing Systems*.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grill 等（2020）Jean-Bastien Grill、Florian Strub、Florent Altché、Corentin Tallec、Pierre
    Richemond、Elena Buchatskaya、Carl Doersch、Bernardo Pires、Zhaohan Guo、Mohammad Azar
    等。2020。Bootstrap Your Own Latent：一种新的自监督学习方法。在 *神经信息处理系统*。
- en: Grosse and Martens (2016) Roger B Grosse and James Martens. 2016. A Kronecker-factored
    approximate Fisher matrix for convolution layers. In *ICML*, Vol. 48\. 573–582.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grosse 和 Martens（2016）Roger B Grosse 和 James Martens。2016。卷积层的 Kronecker 近似
    Fisher 矩阵。在 *ICML*，第48卷，第573–582页。
- en: 'Gupta et al. (2018) Vineet Gupta, Tomer Koren, and Yoram Singer. 2018. Shampoo:
    Preconditioned Stochastic Tensor Optimization. In *ICML*, Vol. 80\. 1837–1845.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等（2018）Vineet Gupta、Tomer Koren 和 Yoram Singer。2018。Shampoo：预条件随机张量优化。在
    *ICML*，第80卷，第1837–1845页。
- en: He et al. (2020) Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick.
    2020. Mask R-CNN. *IEEE Trans. Pattern Anal. Mach. Intell.* 42, 2 (2020), 386–397.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2020）Kaiming He、Georgia Gkioxari、Piotr Dollár 和 Ross B. Girshick。2020。Mask
    R-CNN。*IEEE 计算机学会模式分析与机器智能汇刊* 第42卷，第2期（2020年），第386–397页。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2016）Kaiming He、Xiangyu Zhang、Shaoqing Ren 和 Jian Sun。2016。《用于图像识别的深度残差学习》。发表于
    *IEEE 计算机视觉与模式识别会议论文集*。770–778。
- en: He et al. (2017) Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu,
    and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In *WWW*. 173–182.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2017）Xiangnan He、Lizi Liao、Hanwang Zhang、Liqiang Nie、Xia Hu 和 Tat-Seng
    Chua。2017。《神经协同过滤》。发表于 *WWW*。173–182。
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Flat minima. *Neural computation* 9, 1 (1997), 1–42.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber（1997）Sepp Hochreiter 和 Jürgen Schmidhuber。1997。《平坦最小值》。*神经计算*
    9，1（1997），1–42。
- en: 'Hoffer et al. (2017) Elad Hoffer, Itay Hubara, and Daniel Soudry. 2017. Train
    longer, generalize better: closing the generalization gap in large batch training
    of neural networks. In *Proceedings of the 31st International Conference on Neural
    Information Processing Systems*. 1729–1739.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffer 等（2017）Elad Hoffer、Itay Hubara 和 Daniel Soudry。2017。《训练更久，泛化更好：缩小神经网络大批量训练中的泛化差距》。发表于
    *第31届神经信息处理系统国际会议论文集*。1729–1739。
- en: Horvath et al. (2019) Samuel Horvath, Chen-Yu Ho, Ludovit Horvath, Atal Narayan
    Sahu, Marco Canini, and Peter Richtárik. 2019. Natural Compression for Distributed
    Deep Learning. *CoRR* abs/1905.10988 (2019).
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horvath 等（2019）Samuel Horvath、Chen-Yu Ho、Ludovit Horvath、Atal Narayan Sahu、Marco
    Canini 和 Peter Richtárik。2019。《分布式深度学习的自然压缩》。*CoRR* abs/1905.10988（2019）。
- en: Huang et al. (2017) Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q
    Weinberger. 2017. Densely Connected Convolutional Networks. In *CVPR*. 2261–2269.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2017）Gao Huang、Zhuang Liu、Laurens van der Maaten 和 Kilian Q Weinberger。2017。《密集连接卷积网络》。发表于
    *CVPR*。2261–2269。
- en: Jastrz\kebski et al. (2017) Stanisław Jastrz\kebski, Zachary Kenton, Devansh
    Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. 2017. Three
    factors influencing minima in sgd. *arXiv preprint arXiv:1711.04623* (2017).
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jastrz\kebski 等（2017）Stanisław Jastrz\kebski、Zachary Kenton、Devansh Arpit、Nicolas
    Ballas、Asja Fischer、Yoshua Bengio 和 Amos Storkey。2017。《影响 sgd 最小值的三个因素》。*arXiv
    预印本 arXiv:1711.04623*（2017）。
- en: 'Jia et al. (2018) Xianyan Jia, Shutao Song, W. He, Yangzihao Wang, Haidong
    Rong, Feihu Zhou, Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, Tiegang Chen,
    Guangxiao Hu, Shaohuai Shi, and Xiaowen Chu. 2018. Highly Scalable Deep Learning
    Training System with Mixed-Precision: Training ImageNet in Four Minutes. *ArXiv*
    abs/1807.11205 (2018).'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等（2018）Xianyan Jia、Shutao Song、W. He、Yangzihao Wang、Haidong Rong、Feihu Zhou、Liqiang
    Xie、Zhenyu Guo、Yuanzhou Yang、Liwei Yu、Tiegang Chen、Guangxiao Hu、Shaohuai Shi 和
    Xiaowen Chu。2018。《具有混合精度的高可扩展深度学习训练系统：四分钟内训练 ImageNet》。*ArXiv* abs/1807.11205（2018）。
- en: Kamp et al. (2018) Michael Kamp, Linara Adilova, Joachim Sicking, Fabian Hüger,
    Peter Schlicht, Tim Wirtz, and Stefan Wrobel. 2018. Efficient Decentralized Deep
    Learning by Dynamic Model Averaging. In *ECML/PKDD (1)*.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamp 等（2018）Michael Kamp、Linara Adilova、Joachim Sicking、Fabian Hüger、Peter Schlicht、Tim
    Wirtz 和 Stefan Wrobel。2018。《通过动态模型平均实现高效的去中心化深度学习》。发表于 *ECML/PKDD（1）*。
- en: Karimireddy et al. (2019) Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian
    Stich, and Martin Jaggi. 2019. Error feedback fixes signsgd and other gradient
    compression schemes. In *International Conference on Machine Learning*. PMLR,
    3252–3261.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karimireddy 等（2019）Sai Praneeth Karimireddy、Quentin Rebjock、Sebastian Stich
    和 Martin Jaggi。2019。《错误反馈修复 signsgd 和其他梯度压缩方案》。发表于 *国际机器学习会议*。PMLR，3252–3261。
- en: 'Keskar et al. (2017) Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal,
    Mikhail Smelyanskiy, and Ping Tak Peter Tang. 2017. On Large-Batch Training for
    Deep Learning: Generalization Gap and Sharp Minima. arXiv:1609.04836 [cs.LG]'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keskar 等（2017）Nitish Shirish Keskar、Dheevatsa Mudigere、Jorge Nocedal、Mikhail
    Smelyanskiy 和 Ping Tak Peter Tang。2017。《关于深度学习的大批量训练：泛化差距和尖锐的最小值》。arXiv:1609.04836
    [cs.LG]
- en: 'Kingma and Ba (2017) Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method
    for Stochastic Optimization. arXiv:1412.6980 [cs.LG]'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba（2017）Diederik P. Kingma 和 Jimmy Ba。2017。《Adam：一种随机优化方法》。arXiv:1412.6980
    [cs.LG]
- en: Krizhevsky (2014) Alex Krizhevsky. 2014. One weird trick for parallelizing convolutional
    neural networks. *arXiv preprint arXiv:1404.5997* (2014).
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky（2014）Alex Krizhevsky。2014。《一个奇怪的技巧来并行化卷积神经网络》。*arXiv 预印本 arXiv:1404.5997*（2014）。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. *Advances
    in neural information processing systems* 25 (2012), 1097–1105.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等（2012）Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E Hinton。2012。《使用深度卷积神经网络进行
    ImageNet 分类》。*神经信息处理系统进展* 25（2012），1097–1105。
- en: Krizhevsky et al. (2017) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
    2017. ImageNet classification with deep convolutional neural networks. *Commun.
    ACM* 60, 6 (2017), 84–90.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky等（2017）Alex Krizhevsky, Ilya Sutskever, 和 Geoffrey E. Hinton。2017。使用深度卷积神经网络进行ImageNet分类。*Commun.
    ACM* 60, 6（2017），84–90。
- en: 'Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised
    learning of language representations. *arXiv preprint arXiv:1909.11942* (2019).'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan等（2019）Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
    Sharma, 和 Radu Soricut。2019。Albert：一种用于自监督语言表示学习的轻量BERT。*arXiv预印本 arXiv:1909.11942*（2019）。
- en: 'Li et al. (2021) Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari,
    and Yuxiong He. 2021. 1-bit LAMB: Communication Efficient Large-Scale Large-Batch
    Training with LAMB’s Convergence Speed. *ArXiv* abs/2104.06069 (2021).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2021）Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, 和 Yuxiong
    He。2021。1-bit LAMB：使用LAMB的收敛速度进行通信高效的大规模大批量训练。*ArXiv* abs/2104.06069（2021）。
- en: Li et al. (2018) Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
    2018. Visualizing the loss landscape of neural nets. In *Proceedings of the 32nd
    International Conference on Neural Information Processing Systems*. 6391–6401.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2018）Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, 和 Tom Goldstein。2018。可视化神经网络的损失景观。在*第32届国际神经信息处理系统大会论文集*。6391–6401。
- en: Li et al. (2014a) Mu Li, David G Andersen, Alexander J Smola, and Kai Yu. 2014a.
    Communication efficient distributed machine learning with the parameter server.
    *Advances in Neural Information Processing Systems* 27 (2014), 19–27.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2014a）Mu Li, David G Andersen, Alexander J Smola, 和 Kai Yu。2014a。利用参数服务器进行通信高效的分布式机器学习。*神经信息处理系统进展*
    27（2014），19–27。
- en: Li et al. (2014b) Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. 2014b.
    Efficient mini-batch training for stochastic optimization. In *Proceedings of
    the 20th ACM SIGKDD international conference on Knowledge discovery and data mining*.
    661–670.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2014b）Mu Li, Tong Zhang, Yuqiang Chen, 和 Alexander J Smola。2014b。高效的小批量训练用于随机优化。在*第20届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集*。661–670。
- en: Lin et al. (2020) Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin
    Jaggi. 2020. Don’t Use Large Mini-Batches, Use Local SGD. arXiv:1808.07217 [cs.LG]
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2020）Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, 和 Martin Jaggi。2020。不要使用大批量，使用本地SGD。arXiv:1808.07217
    [cs.LG]
- en: 'Lin et al. (2018) Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally.
    2018. Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed
    Training. In *International Conference on Learning Representations*.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2018）Yujun Lin, Song Han, Huizi Mao, Yu Wang, 和 Bill Dally。2018。深度梯度压缩：减少分布式训练的通信带宽。在*国际学习表征会议*。
- en: Liu et al. (2019) Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong
    Liu, Jianfeng Gao, and Jiawei Han. 2019. On the Variance of the Adaptive Learning
    Rate and Beyond. In *International Conference on Learning Representations*.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2019）Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu,
    Jianfeng Gao, 和 Jiawei Han。2019。自适应学习率的方差及其扩展。在*国际学习表征会议*。
- en: Long et al. (2015) Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015.
    Fully convolutional networks for semantic segmentation. In *Proceedings of the
    IEEE conference on computer vision and pattern recognition*. 3431–3440.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long等（2015）Jonathan Long, Evan Shelhamer, 和 Trevor Darrell。2015。用于语义分割的全卷积网络。在*IEEE计算机视觉与模式识别会议论文集*。3431–3440。
- en: 'Lou et al. (2021) Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, and Yang You. 2021.
    Sparse-MLP: A Fully-MLP Architecture with Conditional Computation. *arXiv preprint
    arXiv:2109.02008* (2021).'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lou等（2021）Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, 和 Yang You。2021。Sparse-MLP：一种具有条件计算的全MLP架构。*arXiv预印本
    arXiv:2109.02008*（2021）。
- en: Martens (2010) James Martens. 2010. Deep learning via Hessian-free optimization.
    In *Proceedings of the 27th International Conference on Machine Learning (ICML-10),
    June 21-24, 2010, Haifa, Israel*. 735–742.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martens（2010）James Martens。2010。通过Hessian-free优化进行深度学习。在*第27届国际机器学习会议（ICML-10），2010年6月21-24日，以色列海法*。735–742。
- en: Martens (2020) James Martens. 2020. New Insights and Perspectives on the Natural
    Gradient Method. *J. Mach. Learn. Res.* 21 (2020), 146:1–146:76.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martens（2020）James Martens。2020。自然梯度方法的新见解和视角。*J. Mach. Learn. Res.* 21 (2020)，146:1–146:76。
- en: Martens and Grosse (2015) James Martens and Roger Grosse. 2015. Optimizing neural
    networks with kronecker-factored approximate curvature. In *International conference
    on machine learning*. PMLR, 2408–2417.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martens和Grosse（2015）James Martens 和 Roger Grosse。2015。利用克罗内克近似曲率优化神经网络。在*国际机器学习会议*。PMLR，2408–2417。
- en: Masters and Luschi (2018) Dominic Masters and Carlo Luschi. 2018. Revisiting
    Small Batch Training for Deep Neural Networks. *CoRR* abs/1804.07612 (2018).
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masters and Luschi (2018) Dominic Masters 和 Carlo Luschi. 2018. 重新审视深度神经网络的小批量训练。*CoRR*
    abs/1804.07612 (2018)。
- en: 'McDonald et al. (2010) Ryan McDonald, Keith Hall, and Gideon Mann. 2010. Distributed
    training strategies for the structured perceptron. In *Human language technologies:
    The 2010 annual conference of the North American chapter of the association for
    computational linguistics*. 456–464.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'McDonald et al. (2010) Ryan McDonald, Keith Hall, 和 Gideon Mann. 2010. 结构感知机的分布式训练策略。发表于*Human
    language technologies: The 2010 annual conference of the North American chapter
    of the association for computational linguistics*。456–464。'
- en: Micikevicius et al. (2018) Paulius Micikevicius, Sharan Narang, Jonah Alben,
    Gregory Frederick Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston,
    Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. Mixed Precision Training.
    *ArXiv* abs/1710.03740 (2018).
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micikevicius et al. (2018) Paulius Micikevicius, Sharan Narang, Jonah Alben,
    Gregory Frederick Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston,
    Oleksii Kuchaiev, Ganesh Venkatesh, 和 Hao Wu. 2018. 混合精度训练。*ArXiv* abs/1710.03740
    (2018)。
- en: 'Nado et al. (2021) Zachary Nado, Justin Gilmer, Christopher J. Shallue, Rohan
    Anil, and George E. Dahl. 2021. A Large Batch Optimizer Reality Check: Traditional,
    Generic Optimizers Suffice Across Batch Sizes. *ArXiv* abs/2102.06356 (2021).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nado et al. (2021) Zachary Nado, Justin Gilmer, Christopher J. Shallue, Rohan
    Anil, 和 George E. Dahl. 2021. 大批量优化器现实检查：传统通用优化器在各种批量大小下均有效。*ArXiv* abs/2102.06356
    (2021)。
- en: Nesterov (1983) Yurii E Nesterov. 1983. A method for solving the convex programming
    problem with convergence rate O (1/k^ 2). In *Dokl. akad. nauk Sssr*, Vol. 269\.
    543–547.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nesterov (1983) Yurii E Nesterov. 1983. 一种解决凸优化问题的方法，收敛速度为 O (1/k^2)。发表于*Dokl.
    akad. nauk Sssr*，第269卷。543–547。
- en: Osawa et al. (2019) Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse,
    Rio Yokota, and Satoshi Matsuoka. 2019. Large-scale distributed second-order optimization
    using kronecker-factored approximate curvature for deep convolutional neural networks.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    12359–12367.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osawa et al. (2019) Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse,
    Rio Yokota, 和 Satoshi Matsuoka. 2019. 使用 Kronecker 近似曲率的大规模分布式二阶优化，用于深度卷积神经网络。发表于*Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*。12359–12367。
- en: 'Park et al. (2019) Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu,
    Barret Zoph, Ekin Dogus Cubuk, and Quoc V. Le. 2019. SpecAugment: A Simple Data
    Augmentation Method for Automatic Speech Recognition. In *Interspeech 2019*. 2613–2617.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park et al. (2019) Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu,
    Barret Zoph, Ekin Dogus Cubuk, 和 Quoc V. Le. 2019. SpecAugment: 一种用于自动语音识别的简单数据增强方法。发表于*Interspeech
    2019*。2613–2617。'
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning
    library. *Advances in neural information processing systems* 32 (2019), 8026–8037.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, 等. 2019. Pytorch: 一种命令式风格的高性能深度学习库。*Advances in neural information processing
    systems* 32 (2019), 8026–8037。'
- en: 'Pauloski et al. (2020) J Gregory Pauloski, Zhao Zhang, Lei Huang, Weijia Xu,
    and Ian T Foster. 2020. Convolutional neural network training with distributed
    K-FAC. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*. IEEE, 1–12.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pauloski et al. (2020) J Gregory Pauloski, Zhao Zhang, Lei Huang, Weijia Xu,
    和 Ian T Foster. 2020. 使用分布式 K-FAC 的卷积神经网络训练。发表于*SC20: International Conference
    for High Performance Computing, Networking, Storage and Analysis*。IEEE，1–12。'
- en: 'Pouyanfar et al. (2019) Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian,
    Yudong Tao, Maria E. Presa Reyes, Mei-Ling Shyu, Shu-Ching Chen, and S. S. Iyengar.
    2019. A Survey on Deep Learning: Algorithms, Techniques, and Applications. *ACM
    Comput. Surv.* 51, 5 (2019), 92:1–92:36.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pouyanfar et al. (2019) Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian,
    Yudong Tao, Maria E. Presa Reyes, Mei-Ling Shyu, Shu-Ching Chen, 和 S. S. Iyengar.
    2019. 深度学习的调查：算法、技术和应用。*ACM Comput. Surv.* 51, 5 (2019), 92:1–92:36。
- en: Qian (1999) Ning Qian. 1999. On the momentum term in gradient descent learning
    algorithms. *Neural networks* 12, 1 (1999), 145–151.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian (1999) Ning Qian. 1999. 梯度下降学习算法中的动量项。*Neural networks* 12, 1 (1999), 145–151。
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*. IEEE, 1–16.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajbhandari 等（2020）Samyam Rajbhandari，Jeff Rasley，Olatunji Ruwase 和 Yuxiong
    He。2020.《Zero: 朝着训练万亿参数模型的内存优化》。在*SC20：高性能计算、网络、存储和分析国际会议*。IEEE，1–16。'
- en: 'Ramezani-Kebrya et al. (2021) Ali Ramezani-Kebrya, Fartash Faghri, Ilia Markov,
    Vitaly Aksenov, Dan Alistarh, and Daniel M. Roy. 2021. NUQSGD: Provably Communication-efficient
    Data-parallel SGD via Nonuniform Quantization. *CoRR* abs/2104.13818 (2021).'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ramezani-Kebrya 等（2021）Ali Ramezani-Kebrya，Fartash Faghri，Ilia Markov，Vitaly
    Aksenov，Dan Alistarh 和 Daniel M. Roy。2021.《NUQSGD: 经非均匀量化保证通信高效的数据并行 SGD》。*CoRR*
    abs/2104.13818（2021）。'
- en: 'Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
    Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning
    models with over 100 billion parameters. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 3505–3506.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rasley 等（2020）Jeff Rasley，Samyam Rajbhandari，Olatunji Ruwase 和 Yuxiong He。2020.《Deepspeed:
    系统优化使得可训练超过 1000 亿参数的深度学习模型》。在*第 26 届 ACM SIGKDD 国际知识发现与数据挖掘会议*。3505–3506。'
- en: Reddi et al. (2019) Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. 2019. On
    the convergence of adam and beyond. *arXiv preprint arXiv:1904.09237* (2019).
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reddi 等（2019）Sashank J Reddi，Satyen Kale 和 Sanjiv Kumar。2019.《Adam 收敛及以上的探讨》。*arXiv
    preprint arXiv:1904.09237*（2019）。
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.
    Faster r-cnn: Towards real-time object detection with region proposal networks.
    *Advances in neural information processing systems* 28 (2015), 91–99.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等（2015）Shaoqing Ren，Kaiming He，Ross Girshick 和 Jian Sun。2015.《Faster r-cnn:
    面向具有区域建议网络的实时目标检测》。*Advances in neural information processing systems* 28（2015），91–99。'
- en: Ren and Goldfarb (2021) Yi Ren and Donald Goldfarb. 2021. Kronecker-factored
    Quasi-Newton Methods for Convolutional Neural Networks. *CoRR* abs/2102.06737
    (2021).
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 和 Goldfarb（2021）Yi Ren 和 Donald Goldfarb。2021.《卷积神经网络的 Kronecker 分解拟牛顿方法》。*CoRR*
    abs/2102.06737（2021）。
- en: Ruder (2016) Sebastian Ruder. 2016. An overview of gradient descent optimization
    algorithms. *CoRR* abs/1609.04747 (2016).
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruder（2016）Sebastian Ruder。2016.《梯度下降优化算法概述》。*CoRR* abs/1609.04747（2016）。
- en: 'Schneider et al. (2019) Frank Schneider, Lukas Balles, and Philipp Hennig.
    2019. DeepOBS: A Deep Learning Optimizer Benchmark Suite. In *7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019*.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schneider 等（2019）Frank Schneider，Lukas Balles 和 Philipp Hennig。2019.《DeepOBS:
    一个深度学习优化器基准测试套件》。在*第 7 届国际学习表示会议，ICLR 2019，2019 年 5 月 6-9 日，美国路易斯安那州新奥尔良*。'
- en: Schraudolph et al. (2007) Nicol N Schraudolph, Jin Yu, and Simon Günter. 2007.
    A stochastic quasi-Newton method for online convex optimization. In *Artificial
    intelligence and statistics*. PMLR, 436–443.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schraudolph 等（2007）Nicol N Schraudolph，Jin Yu 和 Simon Günter。2007.《在线凸优化的随机拟牛顿方法》。在*人工智能和统计*。PMLR，436–443。
- en: Seide et al. (2014) Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
    2014. 1-bit stochastic gradient descent and its application to data-parallel distributed
    training of speech dnns. In *Fifteenth Annual Conference of the International
    Speech Communication Association*. Citeseer.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seide 等（2014）Frank Seide，Hao Fu，Jasha Droppo，Gang Li 和 Dong Yu。2014.《1-bit 随机梯度下降及其在语音
    DNN 数据并行分布式训练中的应用》。在*第 15 届国际语音交流协会年会*。Citeseer。
- en: Shalev-Shwartz and Ben-David (2014) Shai Shalev-Shwartz and Shai Ben-David.
    2014. *Understanding Machine Learning - From Theory to Algorithms*. Cambridge
    University Press. [http://www.cambridge.org/de/academic/subjects/computer-science/pattern-recognition-and-machine-learning/understanding-machine-learning-theory-algorithms](http://www.cambridge.org/de/academic/subjects/computer-science/pattern-recognition-and-machine-learning/understanding-machine-learning-theory-algorithms)
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shalev-Shwartz 和 Ben-David（2014）Shai Shalev-Shwartz 和 Shai Ben-David. 2014.《理解机器学习-从理论到算法》。剑桥大学出版社。[http://www.cambridge.org/de/academic/subjects/computer-science/pattern-recognition-and-machine-learning/understanding-machine-learning-theory-algorithms](http://www.cambridge.org/de/academic/subjects/computer-science/pattern-recognition-and-machine-learning/understanding-machine-learning-theory-algorithms)
- en: Shallue et al. (2019) Christopher J. Shallue, Jaehoon Lee, Joseph M. Antognini,
    Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl. 2019. Measuring the Effects
    of Data Parallelism on Neural Network Training. *J. Mach. Learn. Res.* 20 (2019),
    112:1–112:49.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shallue 等（2019）Christopher J. Shallue，Jaehoon Lee，Joseph M. Antognini，Jascha
    Sohl-Dickstein，Roy Frostig 和 George E. Dahl。2019.《测量数据并行对神经网络训练的影响》。*J. Mach.
    Learn. Res.* 20（2019），112:1–112:49。
- en: 'Shazeer and Stern (2018) Noam Shazeer and Mitchell Stern. 2018. Adafactor:
    Adaptive learning rates with sublinear memory cost. In *International Conference
    on Machine Learning*. PMLR, 4596–4604.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shazeer and Stern (2018) Noam Shazeer 和 Mitchell Stern. 2018. Adafactor: 具有次线性内存成本的自适应学习率。发表于*国际机器学习会议*。PMLR,
    4596–4604。'
- en: Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. 2014. Very
    deep convolutional networks for large-scale image recognition. *arXiv preprint
    arXiv:1409.1556* (2014).
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan and Zisserman (2014) Karen Simonyan 和 Andrew Zisserman. 2014. 用于大规模图像识别的非常深的卷积网络。*arXiv
    预印本 arXiv:1409.1556* (2014)。
- en: Smith et al. (2020) Samuel Smith, Erich Elsen, and Soham De. 2020. On the Generalization
    Benefit of Noise in Stochastic Gradient Descent. In *International Conference
    on Machine Learning*. PMLR, 9058–9067.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith et al. (2020) Samuel Smith, Erich Elsen 和 Soham De. 2020. 关于噪声在随机梯度下降中的泛化效益。发表于*国际机器学习会议*。PMLR,
    9058–9067。
- en: Smith et al. (2018) Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V
    Le. 2018. Don’t Decay the Learning Rate, Increase the Batch Size. In *International
    Conference on Learning Representations*.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith et al. (2018) Samuel L Smith, Pieter-Jan Kindermans, Chris Ying 和 Quoc
    V Le. 2018. 不要衰减学习率，而是增加批量大小。发表于*国际学习表征会议*。
- en: Smith and Le (2018) Samuel L Smith and Quoc V Le. 2018. A Bayesian Perspective
    on Generalization and Stochastic Gradient Descent. In *International Conference
    on Learning Representations*.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith and Le (2018) Samuel L Smith 和 Quoc V Le. 2018. 关于泛化和随机梯度下降的贝叶斯视角。发表于*国际学习表征会议*。
- en: Stich (2019) Sebastian U. Stich. 2019. Local SGD Converges Fast and Communicates
    Little. In *7th International Conference on Learning Representations, ICLR 2019,
    New Orleans, LA, USA, May 6-9, 2019*.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stich (2019) Sebastian U. Stich. 2019. 局部 SGD 收敛快且通信少。发表于*第七届国际学习表征会议，ICLR 2019，美国路易斯安那州新奥尔良，2019年5月6-9日*。
- en: Stich et al. (2018) Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin
    Jaggi. 2018. Sparsified SGD with Memory. *Advances in Neural Information Processing
    Systems* 31 (2018), 4447–4458.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stich et al. (2018) Sebastian U Stich, Jean-Baptiste Cordonnier 和 Martin Jaggi.
    2018. 具有记忆的稀疏 SGD。*神经信息处理系统进展* 31 (2018), 4447–4458。
- en: Strom (2015) Nikko Strom. 2015. Scalable distributed DNN training using commodity
    GPU cloud computing. In *INTERSPEECH 2015, 16th Annual Conference of the International
    Speech Communication Association, Dresden, Germany, September 6-10, 2015*. 1488–1492.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strom (2015) Nikko Strom. 2015. 使用商品 GPU 云计算的可扩展分布式 DNN 训练。发表于*INTERSPEECH 2015，第16届国际语音通信协会年会，德国德累斯顿，2015年9月6-10日*。1488–1492。
- en: 'Sun (2019) Ruoyu Sun. 2019. Optimization for deep learning: theory and algorithms.
    *CoRR* abs/1912.08957 (2019).'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun (2019) Ruoyu Sun. 2019. 深度学习的优化：理论与算法。*CoRR* abs/1912.08957 (2019)。
- en: Sutskever et al. (2013a) Ilya Sutskever, James Martens, George E. Dahl, and
    Geoffrey E. Hinton. 2013a. On the importance of initialization and momentum in
    deep learning. In *Proceedings of the 30th International Conference on Machine
    Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013* *(JMLR Workshop and Conference
    Proceedings, Vol. 28)*. 1139–1147.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever et al. (2013a) Ilya Sutskever, James Martens, George E. Dahl 和 Geoffrey
    E. Hinton. 2013a. 深度学习中初始化和动量的重要性。发表于*第30届国际机器学习会议，ICML 2013，美国乔治亚州亚特兰大，2013年6月16-21日*
    *(JMLR 工作坊与会议论文集，第28卷)*。1139–1147。
- en: Sutskever et al. (2013b) Ilya Sutskever, James Martens, George E. Dahl, and
    Geoffrey E. Hinton. 2013b. On the importance of initialization and momentum in
    deep learning. In *Proceedings of the 30th International Conference on Machine
    Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013* *(JMLR Workshop and Conference
    Proceedings, Vol. 28)*. 1139–1147.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever et al. (2013b) Ilya Sutskever, James Martens, George E. Dahl 和 Geoffrey
    E. Hinton. 2013b. 深度学习中初始化和动量的重要性。发表于*第30届国际机器学习会议，ICML 2013，美国乔治亚州亚特兰大，2013年6月16-21日*
    *(JMLR 工作坊与会议论文集，第28卷)*。1139–1147。
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew
    Rabinovich. 2015. Going deeper with convolutions. In *IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015*.
    IEEE Computer Society, 1–9.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke 和 Andrew Rabinovich.
    2015. 通过卷积深入研究。发表于*IEEE 计算机视觉与模式识别会议，CVPR 2015，美国马萨诸塞州波士顿，2015年6月7-12日*。IEEE 计算机学会，1–9。
- en: 'Tang et al. (2021) Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari,
    Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 2021. 1-bit Adam:
    Communication Efficient Large-Scale Training with Adam’s Convergence Speed. In
    *ICML*.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2021）**Hanlin Tang**、**Shaoduo Gan**、**Ammar Ahmad Awan**、**Samyam Rajbhandari**、**Conglong
    Li**、**Xiangru Lian**、**Ji Liu**、**Ce Zhang** 和 **Yuxiong He**。2021年。1-bit Adam：具有
    Adam 收敛速度的通信高效大规模训练。在*ICML*。
- en: 'Tang et al. (2020) Zhenheng Tang, Shaohuai Shi, Xiaowen Chu, Wei Wang, and
    Bo Li. 2020. Communication-Efficient Distributed Deep Learning: A Comprehensive
    Survey. *CoRR* abs/2003.06307 (2020).'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2020）**Zhenheng Tang**、**Shaohuai Shi**、**Xiaowen Chu**、**Wei Wang**
    和 **Bo Li**。2020年。沟通高效的分布式深度学习：全面综述。*CoRR* abs/2003.06307（2020）。
- en: 'Tieleman and Hinton (2012) T. Tieleman and G. Hinton. 2012. Lecture 6.5—RmsProp:
    Divide the gradient by a running average of its recent magnitude. COURSERA: Neural
    Networks for Machine Learning.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tieleman 和 Hinton（2012）**T. Tieleman** 和 **G. Hinton**。2012年。讲座 6.5—RmsProp：通过最近的梯度幅度的滑动平均来划分梯度。COURSERA：机器学习的神经网络。
- en: 'Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is All you Need. In *Advances in Neural Information Processing Systems 30: Annual
    Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
    Long Beach, CA, USA*, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.
    Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5998–6008.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vaswani 等人（2017）**Ashish Vaswani**、**Noam Shazeer**、**Niki Parmar**、**Jakob
    Uszkoreit**、**Llion Jones**、**Aidan N. Gomez**、**Lukasz Kaiser** 和 **Illia Polosukhin**。2017年。注意力机制是你所需的一切。在*Advances
    in Neural Information Processing Systems 30: Annual Conference on Neural Information
    Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA*，**Isabelle Guyon**、**Ulrike
    von Luxburg**、**Samy Bengio**、**Hanna M. Wallach**、**Rob Fergus**、**S. V. N. Vishwanathan**
    和 **Roman Garnett**（编辑）。5998–6008。'
- en: Verma et al. (2017) Abhishek Verma, Hussam Qassim, and David Feinzimer. 2017.
    Residual squeeze CNDS deep learning CNN model for very large scale places image
    recognition. *2017 IEEE 8th Annual Ubiquitous Computing, Electronics and Mobile
    Communication Conference (UEMCON)* (2017), 463–469.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verma 等人（2017）**Abhishek Verma**、**Hussam Qassim** 和 **David Feinzimer**。2017年。残差挤压
    CNDS 深度学习 CNN 模型用于超大规模场景图像识别。*2017 IEEE 第八届年度无处不在计算、电子与移动通信会议 (UEMCON)*（2017），463–469。
- en: Wang et al. (2020) Meng Wang, Weijie Fu, Xiangnan He, Shijie Hao, and Xindong
    Wu. 2020. A survey on large-scale machine learning. *IEEE Transactions on Knowledge
    and Data Engineering* (2020).
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2020）**Meng Wang**、**Weijie Fu**、**Xiangnan He**、**Shijie Hao** 和 **Xindong
    Wu**。2020年。大规模机器学习综述。*IEEE Transactions on Knowledge and Data Engineering*（2020）。
- en: 'Wangni et al. (2018) Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang.
    2018. Gradient Sparsification for Communication-Efficient Distributed Optimization.
    In *Advances in Neural Information Processing Systems 31: Annual Conference on
    Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada*, Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,
    Nicolò Cesa-Bianchi, and Roman Garnett (Eds.). 1306–1316.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wangni 等人（2018）**Jianqiao Wangni**、**Jialei Wang**、**Ji Liu** 和 **Tong Zhang**。2018年。用于通信高效分布式优化的梯度稀疏化。在*Advances
    in Neural Information Processing Systems 31: Annual Conference on Neural Information
    Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada*，**Samy
    Bengio**、**Hanna M. Wallach**、**Hugo Larochelle**、**Kristen Grauman**、**Nicolò
    Cesa-Bianchi** 和 **Roman Garnett**（编辑）。1306–1316。'
- en: 'Wen et al. (2017) Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran
    Chen, and Hai Li. 2017. TernGrad: Ternary Gradients to Reduce Communication in
    Distributed Deep Learning. In *Advances in Neural Information Processing Systems
    30: Annual Conference on Neural Information Processing Systems 2017, December
    4-9, 2017, Long Beach, CA, USA*. 1509–1519.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wen 等人（2017）**Wei Wen**、**Cong Xu**、**Feng Yan**、**Chunpeng Wu**、**Yandan Wang**、**Yiran
    Chen** 和 **Hai Li**。2017年。TernGrad：三元梯度以减少分布式深度学习中的通信。在*Advances in Neural Information
    Processing Systems 30: Annual Conference on Neural Information Processing Systems
    2017, December 4-9, 2017, Long Beach, CA, USA*。1509–1519。'
- en: 'Wilson et al. (2017) Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nati
    Srebro, and Benjamin Recht. 2017. The Marginal Value of Adaptive Gradient Methods
    in Machine Learning. In *Advances in Neural Information Processing Systems 30:
    Annual Conference on Neural Information Processing Systems 2017, December 4-9,
    2017, Long Beach, CA, USA*. 4148–4158.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wilson 等人（2017）**Ashia C. Wilson**、**Rebecca Roelofs**、**Mitchell Stern**、**Nati
    Srebro** 和 **Benjamin Recht**。2017年。自适应梯度方法在机器学习中的边际价值。在*Advances in Neural Information
    Processing Systems 30: Annual Conference on Neural Information Processing Systems
    2017, December 4-9, 2017, Long Beach, CA, USA*。4148–4158。'
- en: 'Wu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad
    Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
    Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,
    Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant
    Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,
    Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s Neural Machine
    Translation System: Bridging the Gap between Human and Machine Translation. *CoRR*
    abs/1609.08144 (2016).'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,
    Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner,
    Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo
    Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei
    Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg
    Corrado, Macduff Hughes 和 Jeffrey Dean. 2016. 谷歌的神经机器翻译系统：弥合人类与机器翻译之间的差距。*CoRR*
    abs/1609.08144 (2016)。
- en: Xiong et al. (2020) Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng,
    Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. On Layer
    Normalization in the Transformer Architecture. In *Proceedings of the 37th International
    Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event* *(Proceedings
    of Machine Learning Research, Vol. 119)*. 10524–10533.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等人 (2020) Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng,
    Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang 和 Tie-Yan Liu. 2020. 关于 Transformer
    架构中的层归一化。见 *第37届国际机器学习会议论文集，ICML 2020，2020年7月13日至18日，虚拟活动* *(机器学习研究论文集，第119卷)*.
    10524–10533。
- en: 'Xu et al. (2020) Peng Xu, Fred Roosta, and Michael W. Mahoney. 2020. Second-order
    Optimization for Non-convex Machine Learning: an Empirical Study. In *Proceedings
    of the 2020 SIAM International Conference on Data Mining, SDM 2020, Cincinnati,
    Ohio, USA, May 7-9, 2020*. 199–207.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2020) Peng Xu, Fred Roosta 和 Michael W. Mahoney. 2020. 非凸机器学习的二阶优化：一项实证研究。见
    *2020年SIAM国际数据挖掘会议论文集，SDM 2020，2020年5月7日至9日，美国俄亥俄州辛辛那提*。199–207。
- en: Xue et al. (2021) Fuzhao Xue, Ziji Shi, Yuxuan Lou, Yong Liu, and Yang You.
    2021. Go Wider Instead of Deeper. *arXiv preprint arXiv:2107.11817* (2021).
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue 等人 (2021) Fuzhao Xue, Ziji Shi, Yuxuan Lou, Yong Liu 和 Yang You. 2021. 选择更宽而不是更深。*arXiv
    预印本 arXiv:2107.11817* (2021)。
- en: 'Yamazaki et al. (2019) Masafumi Yamazaki, Akihiko Kasagi, Akihiro Tabuchi,
    Takumi Honda, Masahiro Miwa, Naoto Fukumoto, Tsuguchika Tabaru, Atsushi Ike, and
    Kohta Nakashima. 2019. Yet Another Accelerated SGD: ResNet-50 Training on ImageNet
    in 74.7 seconds. *ArXiv* abs/1903.12650 (2019).'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yamazaki 等人 (2019) Masafumi Yamazaki, Akihiko Kasagi, Akihiro Tabuchi, Takumi
    Honda, Masahiro Miwa, Naoto Fukumoto, Tsuguchika Tabaru, Atsushi Ike 和 Kohta Nakashima.
    2019. 另一个加速的 SGD：在 ImageNet 上训练 ResNet-50 用时 74.7 秒。*ArXiv* abs/1903.12650 (2019)。
- en: Ying et al. (2018) Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, and Youlong
    Cheng. 2018. Image Classification at Supercomputer Scale. *CoRR* abs/1811.06992
    (2018).
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ying 等人 (2018) Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang 和 Youlong Cheng.
    2018. 超级计算机规模的图像分类。*CoRR* abs/1811.06992 (2018)。
- en: You et al. (2017) Yang You, Igor Gitman, and Boris Ginsburg. 2017. Large Batch
    Training of Convolutional Networks. arXiv:1708.03888 [cs.CV]
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You 等人 (2017) Yang You, Igor Gitman 和 Boris Ginsburg. 2017. 大批量卷积网络训练。arXiv:1708.03888
    [cs.CV]
- en: 'You et al. (2020) Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar,
    Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
    2020. Large Batch Optimization for Deep Learning: Training BERT in 76 minutes.
    arXiv:1904.00962 [cs.LG]'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You 等人 (2020) Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar,
    Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer 和 Cho-Jui Hsieh.
    2020. 深度学习的大批量优化：76 分钟内训练 BERT。arXiv:1904.00962 [cs.LG]
- en: 'Zeiler (2012) Matthew D. Zeiler. 2012. ADADELTA: An Adaptive Learning Rate
    Method. *ArXiv* abs/1212.5701 (2012).'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler (2012) Matthew D. Zeiler. 2012. ADADELTA：一种自适应学习率方法。*ArXiv* abs/1212.5701
    (2012)。
- en: Zhang et al. (2019) Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant
    Sachdeva, George Dahl, Chris Shallue, and Roger B Grosse. 2019. Which algorithmic
    choices matter at which batch sizes? insights from a noisy quadratic model. *Advances
    in neural information processing systems* 32 (2019), 8196–8207.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2019) Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant
    Sachdeva, George Dahl, Chris Shallue 和 Roger B Grosse. 2019. 哪些算法选择在不同批量大小下重要？来自嘈杂二次模型的见解。*神经信息处理系统进展*
    32 (2019), 8196–8207。
- en: 'Zhang et al. (2016) Jian Zhang, Christopher De Sa, Ioannis Mitliagkas, and
    Christopher Ré. 2016. Parallel SGD: When does averaging help? *CoRR* abs/1606.07365
    (2016).'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2016) Jian Zhang, Christopher De Sa, Ioannis Mitliagkas 和 Christopher
    Ré. 2016. 并行 SGD：何时平均有帮助？*CoRR* abs/1606.07365 (2016)。
- en: Zhou and Cong (2018) Fan Zhou and Guojing Cong. 2018. On the Convergence Properties
    of a K-step Averaging Stochastic Gradient Descent Algorithm for Nonconvex Optimization.
    In *Proceedings of the Twenty-Seventh International Joint Conference on Artificial
    Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden*. 3219–3227.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou和Cong (2018) Fan Zhou和Guojing Cong. 2018. 关于K步平均随机梯度下降算法在非凸优化中的收敛性. 载于*第二十七届国际人工智能联合会议论文集，IJCAI
    2018，2018年7月13日至19日，瑞典斯德哥尔摩*。3219–3227。
- en: 'Zinkevich et al. (2010) Martin Zinkevich, Markus Weimer, Alexander J. Smola,
    and Lihong Li. 2010. Parallelized Stochastic Gradient Descent. In *Advances in
    Neural Information Processing Systems 23: 24th Annual Conference on Neural Information
    Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver,
    British Columbia, Canada*, John D. Lafferty, Christopher K. I. Williams, John
    Shawe-Taylor, Richard S. Zemel, and Aron Culotta (Eds.). 2595–2603.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zinkevich等人 (2010) Martin Zinkevich, Markus Weimer, Alexander J. Smola, 和Lihong
    Li. 2010. 并行化的随机梯度下降. 载于*神经信息处理系统进展23：2010年第24届年会论文集，2010年12月6日至9日，加拿大不列颠哥伦比亚省温哥华*，John
    D. Lafferty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel,
    和Aron Culotta (编). 2595–2603。
