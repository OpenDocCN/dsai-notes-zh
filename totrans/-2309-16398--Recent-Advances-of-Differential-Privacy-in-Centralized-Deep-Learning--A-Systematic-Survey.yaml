- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:36:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2309.16398] Recent Advances of Differential Privacy in Centralized Deep Learning:
    A Systematic Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.16398](https://ar5iv.labs.arxiv.org/html/2309.16398)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '¹¹institutetext: Graz University of Technology (Austria)²²institutetext: Know-Center
    GmbH (Austria)³³institutetext: University of Graz (Austria)'
  prefs: []
  type: TYPE_NORMAL
- en: ldemelius@know-center.at
  prefs: []
  type: TYPE_NORMAL
- en: rkern@know-center.at
  prefs: []
  type: TYPE_NORMAL
- en: atruegler@know-center.at
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lea Demelius 1122    Roman Kern 1122    Andreas Trügler 112233
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Differential Privacy has become a widely popular method for data protection
    in machine learning, especially since it allows formulating strict mathematical
    privacy guarantees. This survey provides an overview of the state-of-the-art of
    differentially private centralized deep learning, thorough analyses of recent
    advances and open problems, as well as a discussion of potential future developments
    in the field. Based on a systematic literature review, the following topics are
    addressed: auditing and evaluation methods for private models, improvements of
    privacy-utility trade-offs, protection against a broad range of threats and attacks,
    differentially private generative models, and emerging application domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is the state-of-the-art of machine learning used to solve complex
    tasks in various fields, including computer vision and natural language processing,
    as well as applications in different domains ranging from healthcare to finance.
    As the performance of these models relies on a high amount of training data, the
    rise of deep learning comes with an increased interest in collecting and analysing
    more and more data. However, data often includes personal or confidential information,
    making privacy a pressing concern. This development is also reflected in legislation
    that was recently put in place to protect personal information and avoid identification
    of individuals, e.g., the General Data Protection Regulation (GDPR) in the European
    Union or the California Consumer Privacy Act (CCPA).
  prefs: []
  type: TYPE_NORMAL
- en: In response, privacy-enhancing technologies are growing in popularity. Cryptographic
    techniques [[93](#bib.bib93)] like homomorphic encryption and secure multi-party
    computation are used to protect against direct information leakage during data
    analysis. However, sensitive information can still be leaked indirectly via the
    output of the analysis and compromise privacy. For example, a machine learning
    model trained to diagnose a disease might make it possible to reconstruct specific
    information about individuals in the training dataset, even when the computation
    is encrypted. To avoid this kind of privacy leaks, output privacy-preserving techniques
    can be applied. One common approach is to use anonymization techniques like k-anonymity
    [[99](#bib.bib99)], l-diversity [[74](#bib.bib74)] or t-closeness [[67](#bib.bib67)].
    However, it is now well known that anonymization often cannot prevent re-identification
    [[19](#bib.bib19), [49](#bib.bib49), [81](#bib.bib81)] and the privacy risk is
    not quantifiable. Hence, differential privacy (DP) [[42](#bib.bib42)] was proposed
    to provide output privacy guarantees. DP is a mathematical probabilistic definition
    that makes it possible to hide information about each single datapoint (e.g.,
    about each individual) while allowing inquiries about the whole dataset (e.g.,
    a population) by adding curated noise. While DP provides a good utility-privacy
    trade-off in many cases (especially statistical queries and traditional machine
    learning methods like logistic regression or support vector machines), the combination
    of differential privacy and deep learning poses many challenges arising from the
    high-dimensionality, the high number of training steps, and the non-convex objective
    functions in deep learning. As a response, considerable progress has been made
    in the past few years addressing the difficulties and opportunities of differentially
    private deep learning (DP-DL).
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Contributions of this survey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This survey provides a comprehensive analysis of recent advances in differentially
    private deep learning (DP-DL), focusing on centralized deep learning. Distributed,
    federated and collaborative deep learning methods and applications have their
    unique characteristics and challenges, and deserve a separate survey. We also
    specifically focus on methods that do not presume convex objective functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the thorough systematic literature review of DP-DL
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'the identification of the research focuses of the last years (2019-2023): 1)
    evaluating and auditing DP-DL models, 2) improving the privacy-utility trade-off
    of DP-DL, 3) DP-DL against threats other than membership and attribute inference,
    4) differentially private generative models, and 5) DP-DL for specific applications'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the analysis and contextualization of the different research trends including
    their potential future development
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Previous reviews of deep learning and differential privacy (see Table [1](#S1.T1
    "Table 1 ‣ 1.1 Contributions of this survey ‣ 1 Introduction ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey")) primarily
    cover the privacy threats specific to deep learning and the most common differential
    privacy methods used for protection. In contrast, this survey goes beyond the
    basic methods and systematically investigates advances and new paths explored
    in the field since 2019\. We provide the reader with advanced understanding of
    the state-of-the-art methods, the challenges, and opportunities of differentially
    private deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Reviews on differentially private deep learning (DL-DP).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Review Year Systematic Contribution Zhao et al. [[128](#bib.bib128)] 2019 -
    privacy attacks on DL, basic DP-DL methods Ha et al. [[55](#bib.bib55)] 2019 -
    privacy attacks on DL, basic DP-DL methods Shen and Zhong [[96](#bib.bib96)] 2021
    - comparison of basic DP-DL models Ouadrhiri and Abdelhadi [[84](#bib.bib84)]
    2022 - DP variants and mechanisms for deep and federated learning This survey
    2023 ✓ recent advances in DP-DL: evaluation and auditing, privacy-utility trade-off,
    threats and attacks, synthetic data and generative models, open problems'
  prefs: []
  type: TYPE_NORMAL
- en: Additional to the reviews mentioned above, a range of broader reviews exist,
    e.g., about differential private machine learning (without focusing on deep learning)
    [[53](#bib.bib53), [131](#bib.bib131), [21](#bib.bib21)], privacy-preserving deep
    learning (without focusing on differential privacy) [[28](#bib.bib28), [100](#bib.bib100),
    [56](#bib.bib56), [76](#bib.bib76), [23](#bib.bib23), [106](#bib.bib106)], or
    privacy-preserving machine learning in general [[70](#bib.bib70), [117](#bib.bib117)].
    These papers give a good overview, while our survey follows a more detailed approach
    focusing on recent developments of differential privacy in centralized deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Survey methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We conducted a systematic literature search on Scopus using a predefined search
    query, and subsequent automatic and manual assessment according to specific inclusion/exclusion
    criteria. The full process is depicted in Figure [1](#S2.F1 "Figure 1 ‣ 2 Survey
    methodology ‣ Recent Advances of Differential Privacy in Centralized Deep Learning:
    A Systematic Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: The Scopus query filtered for documents with the keywords "differential privacy,"
    "differentially private," or "differential private" combined with "neural network,"
    "deep learning," or "machine learning," but without mention of "federated learning,"
    "collaborative," "distributed," or "edge" in their title or abstract. The documents
    were restricted to articles, conference papers, reviews, or book chapters published
    in journals, proceedings, or books in computer science or engineering. Only documents
    published in English between 2019 and March 2023 were considered.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, documents were only included if they were cited at least ten times
    or were in the top 10% of most cited papers of the corresponding year. This inclusion
    criterion allowed manual review of the most influential works in the field. The
    second part of the criterion was added to mitigate the bias towards older papers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last step, the remaining documents were manually reviewed. Whether a
    document was included in this survey was decided based on the following criteria:
    First, differential privacy must be a key topic of the study. Second, the methods
    must include neural networks with results relevant for deep learning. Third, we
    only included works about centralized learning in contrast to distributed learning.
    Fourth, we excluded reinforcement learning and focused on the supervised and unsupervised
    learning paradigms typical for centralized deep learning. Last, one document was
    excluded because of retraction.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/441a63604279f3e313af9cff07123417.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Flow diagram of the paper selection process.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Differential Privacy (DP)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differential Privacy (DP) [[42](#bib.bib42)] is a mathematical definition of
    privacy that aims at protecting details about individuals while still allowing
    general learnings from their data. A randomized algorithm $\mathcal{M}:\mathcal{D}\rightarrow\mathcal{R}$
    with domain $\mathcal{D}$ and range $\mathcal{R}$ is $\epsilon$-differentially
    private if for any two datasets $x,y\in\mathcal{D}$ differing on at most one datapoint,
    and any subset of outputs $\mathcal{S}\subseteq\mathcal{R}$, it holds that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Pr[\mathcal{M}(x)\in\mathcal{S}]\leq e^{\epsilon}Pr[\mathcal{M}(y)\in\mathcal{S}]$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $Pr[]$ denotes the probability, and $\epsilon$ is the privacy risk (also
    referred to as the privacy budget). Simply put, a single datapoint (usually corresponding
    to an individual) has only a limited impact on the output of the analysis. $\epsilon$
    quantifies the maximum amount of information the output of the algorithm can disclose
    about the datapoint. Therefore, a lower $\epsilon$ results in stronger privacy.
  prefs: []
  type: TYPE_NORMAL
- en: DP is achieved by computing the sensitivity of the algorithm in question to
    the absence/presence of a datapoint and adding random noise accordingly. The most
    common noise distributions used for DP are Laplace, Gaussian, and Exponential
    distribution. The amount of added noise relative to the sensitivity determines
    the privacy risk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Differential privacy is not only quantifiable but also has a number of other
    benefits: For one, it is independent of both the input data and auxiliary information
    such as publicly available data or future knowledge. Moreover, it is immune to
    post-processing, and composable, i.e., the sequential or parallel application
    of two $\epsilon$-differentially private algorithms is at least $2\epsilon$-DP.
    Advanced composition theorems can even prove lower overall privacy bounds. Another
    advantage of DP is its flexibility: The noise can be applied at different points
    in the data flow, for example on the input data, on the output of the algorithm,
    or somewhere in between (e.g., during training of a deep learning model). It can
    also be combined with other privacy-enhancing technologies like homomorphic encryption,
    secure multi-party computation, or federated learning.'
  prefs: []
  type: TYPE_NORMAL
- en: While these advantages lead to the widespread acceptance of differential privacy
    as the gold standard for privacy protection, it also comes with its challenges.
    Firstly, the unitless and probabilistic privacy parameter $\epsilon$ is difficult
    to interpret, and thus choosing an appropriate value is challenging. The interested
    reader is referred to [[38](#bib.bib38)] for a list of values used in real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge of DP is its influence on the algorithm’s outputs and, consequently,
    its properties. The dominant issue is the privacy-utility trade-off. Nevertheless,
    it can also influence the performance, usability, fairness, or robustness of the
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, DP is not easy to understand for laypeople: 1) It is important
    to note that DP does not offer perfect privacy. DP provides a specific interpretation
    of privacy, but depending on the context, other interpretations might be expected
    or more relevant. Besides, the privacy protection depends both on the privacy
    parameter $\epsilon$ and the unit/granularity of privacy, i.e., on what is considered
    as one record. That is to say, merely stating that an algorithm is DP does not
    ensure a substantial guarantee. 2) As DP is a mathematical definition that only
    defines the constraints but does not mandate an implementation, there exist many
    different algorithms that satisfy DP, from simple statistical analyses to machine
    learning algorithms. Some assume a central trusted party that has access to all
    the data (global DP), others apply noise before data aggregation (local DP). 3)
    There exist a wide range of DP variants and extensions. Just within the years
    2008 and 2021, over 250 new notions were introduced [[39](#bib.bib39)]. They differ,
    for example, in how they quantify privacy loss, which properties are protected,
    and what capabilities the attacker is assumed to have. The most common extension
    is approximate DP (often simply referred to as $(\epsilon,\delta)$-DP), where
    the algorithm is $\epsilon$-DP with a probability of $1-\delta$. The failure probability
    $\delta$ is typically set to less than the inverse of the dataset size. Other
    common relaxations include zero-concentrated DP (zCDP) [[43](#bib.bib43)] and
    Rényi DP [[77](#bib.bib77)].'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Deep neural networks and stochastic gradient descent (SGD)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep neural networks consist of connected nodes organised in layers, namely
    an input layer, multiple hidden layers, and an output layer. Each node applies
    a set of weights to its inputs and passes its sum through a non-linear activation
    function. The network can learn to perform different tasks, e.g., classification
    on complex data by minimizing the loss (i.e., the difference between the predictions
    of the model and the desired outputs). As the objective function of deep neural
    networks are often non-convex, it is generally not feasible to solve this optimization
    problem analytically. Instead, iterative optimization algorithms are applied,
    most commonly variants of stochastic gradient descent (SGD): At each time step
    $j$, the model weights $w$ are updated according to the gradients of the objective
    function $\mathcal{L}$ computed on a randomly selected subset of the training
    dataset (=batch) $\mathcal{B}$. In Equation [2](#S3.E2 "Equation 2 ‣ 3.2 Deep
    neural networks and stochastic gradient descent (SGD) ‣ 3 Preliminaries ‣ Recent
    Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey"),
    $x_{i}$ denotes a record from the training set with the corresponding target output
    $y_{i}$, $\eta$ is the learning rate, and $|\mathcal{B}|$ the batch size.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $w_{j}=w_{j-1}-\eta\frac{1}{&#124;\mathcal{B}&#124;}\sum_{i\in\mathcal{B}}\nabla\mathcal{L}(w_{j-1},x_{i},y_{i})$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: After training the model, e.g., with SGD, neural networks can be used to make
    predictions for previously unseen data. Models that perform well on new data are
    well-generalized, while models that perform well only on the training data are
    considered overfitted.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning models are usually trained with a supervised or an unsupervised
    learning paradigm. In supervised learning, the training set is labeled, while
    in unsupervised learning the model learns to identify patterns without access
    to a ground truth. Apart from the fully-connected layers described above, neural
    networks often include other types of architectures and layers. One popular example
    are convolutional layers, which use filters to extract local patterns, for example
    edges in images.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Privacy threats for deep learning models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Privacy attacks on deep learning models target either the training data or
    the model itself. They can take place in the training or the inference phase.
    During training, the adversary can not only be a passive observer but also actively
    change the training process. Moreover, the attacker can have access to the the
    whole model, i.e., its architecture, parameters, gradients and outputs (white-box
    access), or only the model’s outputs (black-box access). Typical privacy attacks
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Membership inference attack (MIA): The adversary attempts to infer whether
    a specific record was part of the training dataset. This type of attack exists
    both in the white- and black-box setting, where in the latter case one distinguishes
    further between access to the prediction vector and access to the label. The first
    (and still widely used) MIA on machine learning models was proposed by Shokri
    et al. [[97](#bib.bib97)]. They train shadow models that imitate the target model
    and based on their outputs on training and non-training data, a classification
    model learns to identify which records are members of the training data. The attack
    performs best on overfitted models [[122](#bib.bib122), [97](#bib.bib97)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model inversion attack: The adversary tries to infer sensitive information
    about the training data, for example an attribute of a specific datapoint or features
    that characterize a certain class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Property inference attack: The attacker seeks to extract information about
    the training data that is not related to the training task. For example, they
    might be interested in statistical properties like the ratio of training samples
    that have a certain property.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model extraction attack: The adversary learns a model that approximates the
    target model. While this threat only targets the model and not the training data,
    it can still increase the privacy risk as a successful model extraction can facilitate
    follow-up attacks like model inversion.'
  prefs: []
  type: TYPE_NORMAL
- en: In is important to note that especially in the case of model inversion and property
    inference the terms are not used consistently. For example, De Cristofaro [[36](#bib.bib36)]
    uses the term property inference for inferring features of a class, while others
    [[94](#bib.bib94), [87](#bib.bib87), [127](#bib.bib127)] (including us) refer
    to this as a type of model inversion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, there are attacks not specific to privacy that could still be
    a threat to deep learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adversarial attack: During the inference phase, the attacker manipulates inputs
    purposefully so that the model misclassifies them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Poisoning attack: The adversary perturbs the training samples so that they
    manipulate the model with the goal to reduce its accuracy or trigger specific
    misclassifications.'
  prefs: []
  type: TYPE_NORMAL
- en: For a more detailed description, we refer the reader to surveys that focus on
    privacy attacks [[36](#bib.bib36), [94](#bib.bib94), [95](#bib.bib95)]. A selection
    of attack implementations can, e.g., be found in the Adversarial Robustness Toolbox
    [[83](#bib.bib83)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 DP algorithms for deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most popular algorithm for DP-DL is differentially private stochastic gradient
    descent (DP-SGD) by Abadi et al. [[11](#bib.bib11)]. It adapts classical SGD by
    perturbing the gradients with Gaussian noise. As the gradients norms are unbounded,
    they are clipped before to ensure a finite sensitivity. Equation [2](#S3.E2 "Equation
    2 ‣ 3.2 Deep neural networks and stochastic gradient descent (SGD) ‣ 3 Preliminaries
    ‣ Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey") thus becomes Equation [3](#S3.E3 "Equation 3 ‣ 3.4 DP algorithms for
    deep learning ‣ 3 Preliminaries ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey"), where $clip[]$ denotes the clipping function
    that clips the per-example gradients so that their norm does not exceed the clipping
    norm $C$, $\chi$ is a random vector drawn from a standard Gaussian distribution,
    and $\sigma$ is the standard deviation of the added Gaussian noise.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $w_{j}=w_{j-1}-\eta\frac{1}{&#124;\mathcal{B}&#124;}\sum_{i\in\mathcal{B}}clip\left[\nabla\mathcal{L}(w_{j-1},x_{i},y_{i}),C\right]+\frac{\sigma
    C}{&#124;\mathcal{B}&#124;}\chi$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: While the privacy-utility trade-off inherent to DP still applies (i.e., information
    is lost), DP-SGD can improve generalization and thus improve accuracy on the validation
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the per-example gradient clipping in DP-SGD used
    to bind each record’s sensitivity differs from the the batch-wise gradient clipping
    sometimes used to improve stability and convergence of the training process [[125](#bib.bib125)].
  prefs: []
  type: TYPE_NORMAL
- en: Another popular method for training a deep learning model in a differentially
    private manner is PATE (Private aggregation of teacher ensembles) [[85](#bib.bib85)].
    PATE is a semi-supervised approach that needs public data. However, in contrast
    to DP-SGD, PATE can be applied to every machine learning technique. First, several
    teacher models are trained on separate private datasets. Next, the public data
    is labeled based on the differentially private aggregation of the predictions
    of the teacher models. Finally, a student model is trained on the noisily labeled
    public data, which can be made public afterwards.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to rendering the deep learning model itself differentially private
    is output perturbation, where noise is added to the model’s predictions. However,
    with this method, the privacy budget increases with every prediction made making
    it necessary to limit the number of inferences that can be made with the model.
    For an example of output perturbation in deep learning see Ye et al. [[121](#bib.bib121)].
  prefs: []
  type: TYPE_NORMAL
- en: 4 Auditing and Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Auditing and evaluating deep learning models is important to ensure that they
    provide effective privacy preservation while maintaining utility. As differential
    privacy is always a trade-off between privacy and utility, privacy evaluation
    helps choosing a suitable privacy budget: high enough to protect sensitive information
    but low enough to provide sufficient accuracy. Which level of accuracy is sufficient
    is application-dependent. In some cases, it might also be important to evaluate
    not only the utility overall but also on different subgroups present in the training
    data to detect potential unfairness and biases. In this section, we first discuss
    the attack-based evaluation of empirical privacy risks, and then the evaluation
    of accuracy on subgroups.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Attack-based Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While the differential privacy parameter $\epsilon$ provides an upper bound
    on the privacy loss, attack-based evaluation can give a lower bound. Even though
    empirical privacy evaluation cannot provide any guarantee, it can be useful to
    answer questions regarding the practical meaning of different privacy budgets,
    or how different aspects (e.g., the applied differential privacy notion) influence
    the actual privacy risk. Moreover, the considerable gap that was repeatedly observed
    between the lower and upper bound lead to the assumption that the worst-case setting
    is too pessimistic and actual privacy is much lower. As a consequence, high privacy
    budgets (i.e., high $\epsilon$) were often chosen in practical applications [[38](#bib.bib38)].
    While this choice improved the models’ utility, the question remained if the gap
    between lower and upper privacy bounds implies that the DP guarantee is too pessimistic,
    or if the applied attacks were just weak and future stronger attacks would be
    able to fully exploit the privacy budget. Additionally, attacks can be used to
    evaluate whether models preserve privacy in specific settings, e.g., in cases
    where a class consists of instances by a single individual. The papers reviewed
    in this section give insights into these matters. Table [2](#S4.T2 "Table 2 ‣
    4.1 Attack-based Evaluation ‣ 4 Auditing and Evaluation ‣ Recent Advances of Differential
    Privacy in Centralized Deep Learning: A Systematic Survey") provides an overview
    of the reviewed works, their applied attacks and evaluation metrics. Table [3](#S4.T3
    "Table 3 ‣ 4.1 Attack-based Evaluation ‣ 4 Auditing and Evaluation ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey") shows
    which datasets were used for auditing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of attack-based evaluation methods. The table includes the
    works reviewed in this chapter and states the used attack(s), if they assume black-
    or white-box access to the target model, and how the target model’s vulnerability
    was measured. MIA is the abbreviation for membership inference attack. The attacker’s
    advantage is a measure of privacy leakage proposed by Yeom et al. [[122](#bib.bib122)].
    $\epsilon_{LB}$ and $(\epsilon,\delta)_{LB}$ respectively refers to the empirical
    lower bound of the privacy budget.'
  prefs: []
  type: TYPE_NORMAL
- en: Work Attack Black- or white-box Evaluation metric(s) Jayaraman and Evans, 2019
    [[59](#bib.bib59)] MIA & model inversion black- & white-box accuracy loss, attacker’s
    advantage Chen et al., 2020 [[31](#bib.bib31)] MIA white-box accuracy Leino and
    Fredrikson, 2020 [[66](#bib.bib66)] MIA white-box accuracy, recall, precision,
    attacker’s advantage Jagielski et al., 2020 [[58](#bib.bib58)] poisoning attack
    black-box $\epsilon_{LB}$ Nasr et al., 2021 [[82](#bib.bib82)] MIA & poisoning
    attacks black- & white-box $(\epsilon,\delta)_{LB}$ Park et al., 2019 [[87](#bib.bib87)]
    model inversion black-box success rate, impact of the attack Zhang et al., 2020
    [[127](#bib.bib127)] model inversion white-box accuracy
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Datasets used for auditing DP-DL in the reviewed papers.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Data type | Paper |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Adult Data Set [[6](#bib.bib6)] | tabular | [[66](#bib.bib66)] |'
  prefs: []
  type: TYPE_TB
- en: '| AT&T Database of Faces [[1](#bib.bib1)] | images | [[87](#bib.bib87)] |'
  prefs: []
  type: TYPE_TB
- en: '| Breast Cancer Wisconsin Data Set [[7](#bib.bib7)] | tabular | [[66](#bib.bib66)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-10 [[63](#bib.bib63)] | images | [[58](#bib.bib58), [120](#bib.bib120),
    [66](#bib.bib66), [82](#bib.bib82)] |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-100 [[63](#bib.bib63)] | images | [[59](#bib.bib59), [66](#bib.bib66)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fashion-MNIST [[113](#bib.bib113)] | images | [[58](#bib.bib58)] |'
  prefs: []
  type: TYPE_TB
- en: '| German Credit Data [[10](#bib.bib10)] | tabular | [[66](#bib.bib66)] |'
  prefs: []
  type: TYPE_TB
- en: '| Hepatitis Data Set [[9](#bib.bib9)] | tabular | [[66](#bib.bib66)] |'
  prefs: []
  type: TYPE_TB
- en: '| Labeled Faces in the Wild [[5](#bib.bib5)] | images | [[66](#bib.bib66)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| MNIST [[64](#bib.bib64)] | images | [[66](#bib.bib66), [82](#bib.bib82),
    [127](#bib.bib127)] |'
  prefs: []
  type: TYPE_TB
- en: '| Pima Diabetes Data Set [[8](#bib.bib8)] | tabular | [[66](#bib.bib66)] |'
  prefs: []
  type: TYPE_TB
- en: '| Purchase-100 [[4](#bib.bib4)] | tabular | [[59](#bib.bib59), [58](#bib.bib58),
    [82](#bib.bib82)] |'
  prefs: []
  type: TYPE_TB
- en: '| VGGFace2 dataset [[3](#bib.bib3)] | images | [[87](#bib.bib87)] |'
  prefs: []
  type: TYPE_TB
- en: '| Yeast genomic dataset [[22](#bib.bib22)] | genomic | [[31](#bib.bib31)] |'
  prefs: []
  type: TYPE_TB
- en: Jayaraman and Evans [[59](#bib.bib59)] analyzed how different relaxed notions
    of differential privacy influence the attack success. They use both membership
    inference and model inversion attacks, and study DP with advanced composition,
    zero-concentrated DP and Rényi DP. They conclude that relaxed DP definitions go
    hand in hand with increased attack success, i.e., privacy loss. That means that
    using modified privacy analysis can narrow the gap between theoretical privacy
    guarantee and empirical lower bound.
  prefs: []
  type: TYPE_NORMAL
- en: Chen et al. [[31](#bib.bib31)] confirmed the effectiveness of differential privacy
    also in case of high-dimensional training data using the example of genomic data.
    They evaluated a differentially private convolutional neural network (CNN) model
    with and without sparsity by launching a membership inference attack. Even though
    model sparsity can improve the model’s accuracy in the non-private setting (by
    mitigating overfitting), they showed that in the private setting it has a negative
    effect (but improves privacy).
  prefs: []
  type: TYPE_NORMAL
- en: While the previous papers used existing attacks to study different settings,
    the following works propose novel, stronger attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Leino and Fredrikson [[66](#bib.bib66)] argued that memorization of sensitive
    information can not only be apparent in the model’s predictions but also in how
    it uses (externally given or internally learned) features. Features that are only
    predictive for the training dataset but not for the target data distribution can
    leak information even in well-generalized models. To evaluate the resulting privacy
    risk, they proposed a new white-box membership inference attack that also leverages
    the intermediate representations of the target model. As normal shadow model training
    does not lead to the same interpretation of internal features (even with identical
    model architecture, hyperparameters and training data), they linearly approximated
    each layer, launched a separate attack on each layer and trained a meta-model
    that combines the outputs of the layer-wise attacks. This novel attack was shown
    to be more effective than previous attacks. Training the target model with DP-SGD
    in general decreased the model’s vulnerability, but high $\epsilon$ (here: $\epsilon=16$)
    lead to privacy leakage comparable to the non-private setting.'
  prefs: []
  type: TYPE_NORMAL
- en: Jagielski et al. [[58](#bib.bib58)] explicitly analyzed the gap between theoretical
    privacy guarantee (upper bound) and empirical evaluation (lower bound). The gap
    can be narrowed either by providing tighter privacy analyses (as discussed in
    context with Jayaraman and Evans [[59](#bib.bib59)]), or by developing stronger
    attacks (e.g., like Leino and Fredrikson [[66](#bib.bib66)]). Jagielski et al.
    [[58](#bib.bib58)] followed the latter approach and showed that the former is
    approaching its limits. Their novel attack is based on data poisoning, where the
    attacker purposefully perturbs some training samples to change the parameter distribution
    (e.g., by adding a white patch in the corner of an image). As this approach is
    made effective by using poisoning samples that induce large gradients, the gradient
    clipping of DP-SGD degrades the attack. Their proposed adjustment includes perturbing
    training samples so that parameters are changed in the direction of their lowest
    variance. Their estimated lower bound of the privacy loss $\epsilon_{LB}$ was
    only an order of magnitude below the theoretical upper bound. This suggested the
    end of continuously tighter privacy guarantees by refined analyses that in the
    past brought improvements by a thousandfold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nasr et al. [[82](#bib.bib82)] extended this study further by testing how different
    assumptions about the attacker’s capabilities influence the empirical lower bound.
    The investigated capabilities include: access to the parameters of the final and
    all intermediate models, and manipulation of inputs and gradients. In contrast
    to Jagielski et al. [[58](#bib.bib58)], they considered approximate DP. Their
    evaluations revealed that the strongest adversary can exploit the full privacy
    budget determined by theoretic analysis. They confirmed thereby that the possibilities
    to improve the privacy-utility trade-off via more advanced privacy analyses are
    exhausted. However, assuming limited capabilities of the attacker may reduce the
    upper bound further.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The attacks applied in these papers are all considered typical assessments
    of DP: Membership inference is the most obvious test of DP arising from its definition;
    the model inversion attack applied by Jayaraman and Evans [[59](#bib.bib59)] tries
    to infer attributes of specific individuals - which is not possible if the whole
    record of the individual cannot be recovered; and poisoning attacks exploit the
    fact that DP has to work even with the worst-case dataset. However, model inversion
    attacks can also mean the inference of class attributes (instead of individual
    attributes). In some tasks (e.g., face recognition), a class refers to exactly
    one individual. The following works [[87](#bib.bib87), [127](#bib.bib127)] analyzed
    whether DP-DL also protects against this kind of model inversion attack.'
  prefs: []
  type: TYPE_NORMAL
- en: Park et al. [[87](#bib.bib87)] evaluated the privacy loss of a face recognition
    model by reconstructing the training images from the model’s predictions, and
    automatically measuring the attack success based on the performance of an evaluation
    model. Their results showed that even high privacy budgets (e.g., $\epsilon=8$)
    can provide protection against this model inversion attack compared to the non-private
    setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhang et al. [[127](#bib.bib127)] proposed a generative model inversion (GMI)
    attack also in the face recognition setting. They first trained a GAN (Generative
    Adversarial Network; see detailed explanation in Section [7](#S7 "7 Differentially
    private generative models ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey")) on public data to generate realistic images,
    and then reconstructed the sensitive face regions by finding the values that maximize
    the likelihood. They showed that DP-SGD could not prevent their attack. They also
    argued that higher predictive power of the target models goes hand in hand with
    increased vulnerability to the attack.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Further discussions about the relevance of this kind of model inversion attack
    can be found in Section [9](#S9 "9 Discussion and future directions ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '4.2 Evaluation of subgroups: bias and fairness'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The evaluation of accuracy on subgroups is a standard approach to identify biases.
    In general, models should avoid the unfair treatment of different groups especially
    those based on legally protected attributes like gender, religion and ethnicity.
    Bagdasaryan et al. [[20](#bib.bib20)] showed that applying DP-SGD leads not only
    to an overall accuracy loss but underrepresented classes are disparately affected.
    While a greater imbalance in the training data seems to increase the accuracy
    gap, Farrand et al. [[44](#bib.bib44)] showed that it is significant even in cases
    of a 30-70% split, and also for loose privacy guarantees. The observed effect
    is believed to mainly arise from the clipping of the gradients, which penalizes
    those samples that result in bigger gradients, i.e., outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Improving the privacy-utility trade-off of DP-DL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main challenge of DP-DL is that by setting meaningful privacy guarantees,
    utility often deteriorates strongly. In recent years, many propositions for improved
    DP-DL (mostly DP-SGD) were made that can increase accuracy at the same privacy.
    Table [4](#S5.T4 "Table 4 ‣ 5 Improving the privacy-utility trade-off of DP-DL
    ‣ Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey") gives an overview of the proposed approaches. An alternative method would
    be to provide tighter theoretical bounds for the privacy loss without influencing
    the learning algorithm. An example of such an approach evaluated on deep learning
    can be found in Ding et la. [[40](#bib.bib40)]. However, as we already established
    in Section [4](#S4 "4 Auditing and Evaluation ‣ Recent Advances of Differential
    Privacy in Centralized Deep Learning: A Systematic Survey"), this line of research
    seems to have reached its limit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Summary of approaches for improving DP-DL.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Approach | Paper |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Adapting the model architecture | [[86](#bib.bib86)] |'
  prefs: []
  type: TYPE_TB
- en: '| Improving the hyperparameter selection | [[86](#bib.bib86), [71](#bib.bib71)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Applying feature engineering and transfer learning | [[102](#bib.bib102)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mitigating the clipping bias (to improve convergence) | [[34](#bib.bib34)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pruning the model | [[51](#bib.bib51), [15](#bib.bib15), [92](#bib.bib92)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Adding heterogeneous noise | [[124](#bib.bib124), [112](#bib.bib112), [116](#bib.bib116),
    [16](#bib.bib16), [52](#bib.bib52)] |'
  prefs: []
  type: TYPE_TB
- en: 'One approach to increase the accuracy of DP-SGD at the same privacy is to adapt
    the architecture of the deep learning model to better suit differentially private
    learning. Papernot et al. [[86](#bib.bib86)] observed that rendering SGD differentially
    private as proposed by Abadi et al. [[11](#bib.bib11)] leads to exploding gradients.
    The larger the gradients, the more information is lost during clipping, which
    in turn hurts the model’s accuracy. To mitigate this effect, Papernot et al. [[86](#bib.bib86)]
    proposed to use bounded activation functions instead of the unbounded ones commonly
    used in non-private training (e.g., ReLU). They introduced tempered sigmoid activation
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Phi(x)=\frac{s}{1+e^{-Tx}}-o$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where the parameter $s$ controls the scale of the activation, the inverse temperature
    $T$ regulates the gradient norms and $o$ is the offset (see Figure [2](#S5.F2
    "Figure 2 ‣ 5 Improving the privacy-utility trade-off of DP-DL ‣ Recent Advances
    of Differential Privacy in Centralized Deep Learning: A Systematic Survey")).
    The setting [$s=2$, $T=2$, $o=1$] results in the hyperbolic tangent (tanh) function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9a263468de7b73e99cacdf619c079cd4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Examples of tempered sigmoid functions in comparison with the ReLU
    function. Tempered sigmoid functions with their parameters $s$, $T$ and $o$ are
    bounded activation functions proposed by Papernot et al. [[86](#bib.bib86)] to
    improve private deep learning. ReLU is an unbounded activation function commonly
    used in deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Papernot et al. [[86](#bib.bib86)] showed that tempered sigmoids can increase
    the model’s accuracy. For the MNIST and Fashion-MNIST datasets the tanh function
    performed best.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect when tuning deep learning models for improved performance
    is hyperparameter selection (e.g., choosing the learning rate). Even though it
    might be tempting to transfer the choice of hyperparameters from non-private to
    private model, Papernot et al. [[86](#bib.bib86)] showed that not only the model’s
    architecture but also the hyperparameters should be chosen explicitly for the
    private model in contrast to using what worked well in the non-private setting.
    As this can result in additional privacy leakage, one should consider private
    selection of hyperparameters, as, for example, proposed by Liu et al. [[71](#bib.bib71)].
  prefs: []
  type: TYPE_NORMAL
- en: Similar to non-private deep learning, DP-SGD can benefit from feature engineering,
    additional data, and transfer learning. Tramèr and Boneh [[102](#bib.bib102)]
    showed that handcrafted features can significantly improve the private model’s
    utility compared to end-to-end learning. The comparable increase in accuracy for
    private end-to-end learning can be achieved by using an order of magnitude more
    training data or by transferring features learned from public data.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the preceding techniques are already known from non-private deep learning,
    DP-SGD introduces two new steps that offer opportunities for improvement: gradient
    clipping and noise addition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chen et al. [[34](#bib.bib34)] found that the bias introduced by gradient clipping
    can cause convergence issues. They discovered a relationship between the symmetry
    of the gradient distribution and convergence: Symmetric gradient distributions
    lead to convergence even if a large fraction of gradients are heavily scaled down.
    Based on this finding, Chen et al. [[34](#bib.bib34)] proposed to introduce additional
    noise before clipping when the gradient distribution is non-symmetric. It is important
    to note that this approach may lead to better but slower convergence due to the
    additional variance, and therefore only improves the privacy-utility trade-off
    in specific use-cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Another observation specific to DP-SGD is that the privacy-utility trade-off
    worsens with growing model size. A higher number of model parameters results in
    a higher gradient norm, meaning that clipping the gradient to the same norm leads
    to a higher impact. If the clipping norm is also increased, then more noise has
    to be added to achieve the same privacy guarantee. This effect can be mitigated
    by either reducing the number of model parameters (parameter pruning) [[51](#bib.bib51),
    [15](#bib.bib15)] or compressing the gradients (gradient pruning) [[15](#bib.bib15),
    [92](#bib.bib92)].
  prefs: []
  type: TYPE_NORMAL
- en: The work by Gondara et al. [[51](#bib.bib51)] is based on the lottery ticket
    hypothesis [[47](#bib.bib47), [48](#bib.bib48)], which says that there exist sub-networks
    in large neural networks that when trained separately achieve comparable accuracy
    as the full network. The term "lottery ticket" refers to the pruned networks and
    comes from the idea that finding a well-performing sub-network is like winning
    the lottery. Gondara et al. [[51](#bib.bib51)] altered the original lottery ticket
    hypothesis to comply with differential privacy. First, the lottery tickets are
    created non-privately using a public dataset. Next, the accuracy of each lottery
    ticket is evaluated on a private validation set, and the best sub-network is selected
    while preserving DP via the Exponential Mechanism. Finally, the winning ticket
    is trained using DP-SGD.
  prefs: []
  type: TYPE_NORMAL
- en: Adamczewski and Park [[15](#bib.bib15)] proposed DP-SSGD (differentially private
    sparse stochastic gradient descent) that too relies on model pruning. They experimented
    with both parameter freezing, where just a subset of parameters are trained, and
    parameter selection, where a different subset of parameters are updated each iteration.
    The updated parameters were either chosen randomly or based on their magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: Both of these model pruning approaches rely on publicly available data that
    should be as similar as possible to the private data. In contrast, Phong and Phuong
    [[92](#bib.bib92)] proposed a gradient pruning method that works without public
    data. Additionally to making the gradients sparse, they use memorization to maintain
    the direction of the gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: A further line of research deals with adapting the noise that is added to the
    differentially private model during training. While the original DP-SGD algorithm
    adds the same amount of noise to each gradient coordinate independent of the training
    progress, this line of work adds noise either based on the learning progress (e.g.,
    number of executed epochs) [[124](#bib.bib124)] or based on the coordinates’ impact
    on the model [[112](#bib.bib112), [116](#bib.bib116), [16](#bib.bib16), [52](#bib.bib52)].
  prefs: []
  type: TYPE_NORMAL
- en: Yu et al. [[124](#bib.bib124)] argues that with training progress and therefore
    convergence to the local optimum the model profits more from smaller noise. This
    "dynamic privacy budget allocation" is similar to the idea behind adaptive learning
    rates, which is a common technique in non-private learning and can also be applied
    in private learning (see for example [[116](#bib.bib116), [115](#bib.bib115)]).
    Yu et al. compared different variants of dynamic schemes to allocate privacy budgets,
    including predefined decay schedules like exponential decay, or noise scaling
    based on the validation accuracy on a public dataset. They showed that all dynamic
    schemes outperform the uniform noise allocation to a similar extent.
  prefs: []
  type: TYPE_NORMAL
- en: Xiang et al. [[112](#bib.bib112)] treated the privacy-utility trade-off as an
    optimization problem, namely minimizing the accuracy loss while satisfying the
    privacy constraints. Consequently, less noise is added to those gradient coordinates
    that have a high impact on the model’s output. While the model’s utility was improved
    for a range of privacy budgets and model architectures, the method is computationally
    expensive due to the high dimensionality of the optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: Xu et al. [[116](#bib.bib116)] advanced the approach further not only reducing
    the computational demand but also improving convergence (and therefore decreasing
    the privacy budget). The improved version called AdaDP (adaptive and fast convergent
    approach to differentially private deep learning) replaces the computationally
    expensive optimization with a heuristic approach to compute the gradient coordinates’
    impact on the model’s output. The added noise is not only adaptive with regards
    to the coordinates’ sensitivity but also decreases with the number of training
    iterations. Faster convergence is achieved by incorporating an adaptive learning
    rate that is larger for less frequently updated coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: A related research direction is the usage of explainable AI methods to calibrate
    the noise. Both Gong et al. [[52](#bib.bib52)] and Adesuyi et al. [[16](#bib.bib16)]
    use layer-wise relevance propagation (LRP) [[78](#bib.bib78)] to determine the
    importance of the different parameters. Gong et al. [[52](#bib.bib52)] proposed
    the ADPPL (adaptive differential privacy preserving learning) framework that adds
    adaptive Laplace noise [[91](#bib.bib91)] to the gradient coordinates according
    to their relevance. In contrast, the approach by Adesuyi et al. [[16](#bib.bib16)]
    is based on loss function perturbation. As deep learning models have non-convex
    loss functions, the polynomial approximation of the loss function is computed
    before adding Laplace noise. LRP is used to classify the parameters either as
    high or low relevance, adding small and large noise accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Explainable AI-based approaches can also be applied in the local DP setting,
    for example Wang et al. [[109](#bib.bib109)] uses feature importance to decide
    how much noise to add to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of examples for results that the discussed works reported can be found
    in Table [5](#S5.T5 "Table 5 ‣ 5 Improving the privacy-utility trade-off of DP-DL
    ‣ Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey"). For comparison, the results for the original DP-SGD by Abadi et al.
    [[11](#bib.bib11)] were included as well. The different accuracies for the three
    different reported privacy levels from the original DP-SGD paper clearly show
    the privacy-utility trade-off typical for differentially private algorithms. Direct
    comparison between the methods is difficult due to different network architectures
    and hyperparameters, the different evaluation datasets, and the different privacy
    levels measured according to different DP notions (e.g., $\epsilon$-DP, $(\epsilon,\delta)$-DP,
    $\rho$-zCDP (zero-concentrated DP)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Selected examples of results for the improved DP methods in comparison
    with the original DP-SGD by Abadi et al. [[11](#bib.bib11)]. The used deep learning
    models include fully-connected neural networks (FCNN) and convolutional neural
    networks (CNN). Some include a principal component analysis (PCA) layer in front.
    We reported the model, evaluation dataset, accuracy and privacy budget for all
    methods that state exact values, preferably on the MNIST dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '*This result was achieved by parameter freezing. Parameter selection performed
    slightly worse.'
  prefs: []
  type: TYPE_NORMAL
- en: '**This result was achieved by a polynomial decay schedule. Other budget allocation
    schemes performed comparably.'
  prefs: []
  type: TYPE_NORMAL
- en: '***$\rho$ is the privacy parameter for zero-concentrated DP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Method Model Dataset Accuracy Privacy Original [[11](#bib.bib11)] PCA + FCNN
    (1 hidden layer) MNIST 90% $\epsilon=0.5,\delta=10^{-5}$ Original [[11](#bib.bib11)]
    PCA + FCNN (1 hidden layer) MNIST 95% $\epsilon=2,\delta=10^{-5}$ Original [[11](#bib.bib11)]
    PCA + FCNN (1 hidden layer) MNIST 97% $\epsilon=8,\delta=10^{-5}$ Tanh [[86](#bib.bib86)]
    CNN (2 convolutional layers) MNIST $98.1\%$ $\epsilon=2.93,\delta=10^{-5}$ DPLTH
    [[51](#bib.bib51)] FCNN (3 hidden layers) public: MNIST; private: Fashion-MNIST
    $76\%$ $\epsilon=0.4$ DP-SSGD [[15](#bib.bib15)] CNN (2 convolutional layers)
    MNIST $97.02\%$* $\epsilon=2$ Gradient compression [[92](#bib.bib92)] CNN (2 convolutional
    layers) MNIST $98.52\%$ $\epsilon=1.71,\delta=10^{-5}$ Dynamic privacy budget
    allocation** [[124](#bib.bib124)] PCA + FCNN (1 hidden layer) MNIST $93.2\%$**
    $\rho=0.78$*** Adaptive noise [[112](#bib.bib112)] CNN (2 convolutional layers)
    MNIST $94.69\%$ $\epsilon=1,\delta=10^{-5}$ AdaDP [[116](#bib.bib116)] PCA + FCNN
    (1 hidden layer) MNIST $96\%$ $\epsilon=1.4,\delta=10^{-4}$ Noise acc. to LRP
    [[16](#bib.bib16)] FCNN (3 hidden layers) Winsconsin Diagnosis Breast Cancer (WDBC)
    $94\%$ $\epsilon=1.1$ ADPPL framework [[52](#bib.bib52)] CNN (2 convolutional
    layers) MNIST $94\%$ $\epsilon=1$'
  prefs: []
  type: TYPE_NORMAL
- en: '6 Beyond membership and attribute inference: Applying DP-DL to protect against
    other threats'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Differential privacy is usually applied to protect against re-identification.
    In the context of deep learning this mainly includes membership inference and
    model inversion attacks. This section reviews cases in which differential privacy
    can protect against other threats deep learning models exhibit. Table [6](#S6.T6
    "Table 6 ‣ 6 Beyond membership and attribute inference: Applying DP-DL to protect
    against other threats ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey") lists the discussed threats and literature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Summary of threats other than membership and attribute inference against
    which DP-DL was applied.'
  prefs: []
  type: TYPE_NORMAL
- en: '*These two approaches are both based on threat identification via anomaly detection.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Threat | Paper |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model extraction attacks | [[129](#bib.bib129), [130](#bib.bib130), [118](#bib.bib118)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial attacks | [[65](#bib.bib65), [90](#bib.bib90)] |'
  prefs: []
  type: TYPE_TB
- en: '| Privacy risk of interpretable DL | [[57](#bib.bib57)] |'
  prefs: []
  type: TYPE_TB
- en: '| Privacy risk of machine unlearning | [[32](#bib.bib32)] |'
  prefs: []
  type: TYPE_TB
- en: '| Backdoor poisoning attacks* | [[41](#bib.bib41)] |'
  prefs: []
  type: TYPE_TB
- en: '| Network intrusion* | [[119](#bib.bib119)] |'
  prefs: []
  type: TYPE_TB
- en: One threat against which differential privacy can be applied even though it
    was not originally intended for that purpose are model extraction attacks. Model
    extraction is primarily a security and confidentiality issue but it can also compromise
    privacy as it facilitates membership and attribute inference.
  prefs: []
  type: TYPE_NORMAL
- en: Zheng et al. [[129](#bib.bib129), [130](#bib.bib130)] observed that most model
    extraction attacks infer the decision boundary of the target model via nearby
    inputs. They introduced the notion of boundary differential privacy ($\epsilon$-BDP)
    and proposed to append a BDP layer to the machine learning model that 1) determines
    which outputs are close to the decision boundary (a question not straightforward
    for deep learning models as the decision boundary has no closed form) and 2) adds
    noise to them. Previous input-output pairs are cached to ensure that the same
    input results in the same noisy output. This method guarantees that the attacker
    cannot learn the decision boundary with more than a predetermined level of precision
    (controlled by $\epsilon$).
  prefs: []
  type: TYPE_NORMAL
- en: 'A subsequent study by Yan et al. [[118](#bib.bib118)] showed that without caching
    the BDP layer cannot protect against their novel model extraction attack. They
    proposed an alternative to caching: monitoring the privacy leakage and adapting
    the privacy budget accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Differential privacy can also be adapted to protect deep learning models against
    adversarial attacks. While originally DP is defined on a record level, feature-level
    DP can achieve robustness against adversarial examples. For example, Lecuyer et
    al. [[65](#bib.bib65)] proposed PixelDP, a pixel-level DP layer that can be added
    to any type of deep learning model. As this approach only guarantees robustness
    but not differential privacy in the original sense, Phan et al. [[90](#bib.bib90)]
    developed the method further by combining it with DP-SGD. To deal with the trade-off
    between robustness, privacy and utility, they relied on heterogeneous noise: More
    noise is added to more vulnerable coordinates.'
  prefs: []
  type: TYPE_NORMAL
- en: Another line of research is the relationship between interpretability/explainability
    and privacy. Even though interpretable/explainable AI methods are not a threat
    scenario on their own, they can facilitate privacy attacks. Harder et al. [[57](#bib.bib57)]
    looked at how models can both be interpretable and guarantee differential privacy.
    Models trained with DP-SGD are not vulnerable to gradient-based interpretable
    AI methods due to the post-processing property of DP. However, gradient-based
    methods can only provide local explanations, i.e., about how relevant a specific
    input is for the model’s decision, but Harder et al. [[57](#bib.bib57)] was specifically
    interested in methods that can also give global explanations, i.e., about how
    the model works overall. They introduced differentially private locally linear
    maps (DP-LLM), which can approximate deep learning models and are inherently interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: Chen et al. [[32](#bib.bib32)] investigated the privacy risk of machine unlearning.
    Machine unlearning [[26](#bib.bib26), [107](#bib.bib107)] is the process of removing
    the impact one or more datapoints have on a trained model. Common methods are
    retraining from scratch and SISA (Sharded, Isolated, Sliced, and Aggregated) [[24](#bib.bib24)],
    where the original model consists of $k$ models each trained on a subset of the
    training set and therefore only one submodel is retrained for unlearning. While
    the main idea is to be able to comply with privacy regulations like the right
    to be forgotten in the European General Data Protection Regulation (GDPR), the
    unlearning can disclose additional information about the removed datapoint(s).
    Chen et al. [[32](#bib.bib32)] proposed a new black-box membership inference attack
    that exploits both the original and the unlearned model. Their attack was more
    powerful than classical membership inference attacks, and also worked for well-generalized
    models, when several datapoints were removed, when the attacker missed several
    intermediate unlearned models, and when the model was updated with new inputs.
    They showed that DP-SGD is an effective defense against the privacy risk of machine
    unlearning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of protecting the model directly, differential privacy can also be
    used to improve the detection of attacks [[41](#bib.bib41), [119](#bib.bib119)].
    This approach can be viewed as a kind of anomaly detection, where the attack scenario
    is the outlier/novelty. Anomaly detection with deep learning models (e.g., autoencoders,
    CNNs) is based on the model’s tendency to underfit on underrepresented subgroups,
    i.e., the model’s error is expected to be higher for atypical inputs. Training
    the model with differential privacy amplifies this effect (see Section [4.2](#S4.SS2
    "4.2 Evaluation of subgroups: bias and fairness ‣ 4 Auditing and Evaluation ‣
    Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey")). While this leads to negative consequences in the context of fairness
    and bias, here it can be used to improve the performance of anomaly detection.'
  prefs: []
  type: TYPE_NORMAL
- en: Du et al. [[41](#bib.bib41)] applied this approach on crowdsourcing data, that
    is, data stemming from many individuals. These individuals could launch a backdoor
    poisoning attack by maliciously adapting their contributed samples. To protect
    the target model, the poisoned samples need to be identified and removed from
    the training set. They showed that differential privacy can improve the performance
    of anomaly detection in this context. Based on the same reasoning, Yang et al.
    [[119](#bib.bib119)] proposed Griffin, a network intrusion detection system.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Differentially private generative models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generative models are a class of machine learning models that aim to generate
    new data samples similar to the data samples from the training set. The synthetic
    data created with generative models are often seen as privacy-preserving as they
    are not directly linked to real entities or individuals. However, similarly to
    other machine learning models, generative models can memorize sensitive information
    and be vulnerable to privacy attacks. This section gives an overview of recent
    works regarding the generation of differentially private synthetic data with generative
    models (see Table [7](#S7.T7 "Table 7 ‣ 7 Differentially private generative models
    ‣ Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic
    Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Summary of differentially private generative models. The methods are
    either based on (variational) autoencoders or Generative Adversarial Networks
    (GAN). The DP algorithm DP-EM refers to differentially private expectation maximization
    [[88](#bib.bib88)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Type of generative model(s) | DP algorithm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DPGM [[14](#bib.bib14)] | variational autoencoders | DP k-means & DP-SGD
    |'
  prefs: []
  type: TYPE_TB
- en: '| DP-SYN [[12](#bib.bib12)] | autoencoders | DP-SGD & DP-EM |'
  prefs: []
  type: TYPE_TB
- en: '| PPGAN [[72](#bib.bib72)] | GAN | DP-SGD |'
  prefs: []
  type: TYPE_TB
- en: '| GANobfuscator [[115](#bib.bib115)] | GAN | DP-SGD |'
  prefs: []
  type: TYPE_TB
- en: '| GS-WGAN [[30](#bib.bib30)] | GAN | DP-SGD |'
  prefs: []
  type: TYPE_TB
- en: '| RDP-CGAN [[101](#bib.bib101)] | GAN | DP-SGD |'
  prefs: []
  type: TYPE_TB
- en: '| PATE-GAN [[61](#bib.bib61)] | GAN | PATE |'
  prefs: []
  type: TYPE_TB
- en: 7.1 Autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An autoencoder is a deep learning model consisting of two parts: an encoder
    and a decoder. The encoder learns to map the input to a lower-dimensional space,
    while the decoder learns to reconstruct the input from this intermediate representation.
    Variational autoencoders (VAEs) work according to the same principle but learn
    the probability distribution of the intermediate (encoded) representations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Acs et al. [[14](#bib.bib14)] proposed DPGM (Differentially Private Generative
    Model) based on a mixture of variational autoencoders (VAEs). First, the training
    data are privately clustered using differentially private version of k-means.
    Next, one VAE per cluster is trained with DP-SGD. The data distributions learned
    by the VAEs are used to generate synthetic data. Splitting the input data into
    clusters and learning separate models has two advantages: Firstly, the models
    learn faster as they are trained on similar datapoints so less noise is added
    and the models achieve higher accuracy. Secondly, unrealistic combinations of
    clusters are avoided.'
  prefs: []
  type: TYPE_NORMAL
- en: Abay et al. [[12](#bib.bib12)] used a similar approach also based on partitioning
    the training data. In contrast to DPGM [[14](#bib.bib14)], they assume a supervised
    setting and split the data according to their labels. Each class is then used
    to train a separate autoencoder via DP-SGD. Synthetic data is generated by sampling
    the intermediate representations with differentially private expectation maximization
    (DP-EM) [[88](#bib.bib88)], and applying the decoder on the new representations.
    This method (called DP-SYN) outperformed DPGM for most tested datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 GANs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generative Adversarial Networks (GANs) consists of two neural networks that
    play an adversarial game: The generator tries to create synthetic samples similar
    to the training data, while the discriminator attempts to distinguish between
    the artificially generated and the real samples. After training, the generator
    can be used to create synthetic data.'
  prefs: []
  type: TYPE_NORMAL
- en: In the original GAN, the generator is trained based on the Jensen-Shannon distance
    as a distance measure between the two data distributions. This can lead to instability
    issues, in particular vanishing gradients, where the generator learns too slowly
    compared to the discriminator. To mitigate this problem, the Wasserstein GAN (WGAN)
    [[18](#bib.bib18)] was introduced, which relies on the Wasserstein distance instead
    of the Jensen-Shannon distance. For this variant, the weights are clipped after
    each update to make the Wasserstein distance applicable in this context. A differentially
    private version of WGAN (called DPGAN) was first introduced in 2018 by Xie et
    al. [[114](#bib.bib114)] exploiting the fact that WGAN already has bounded gradients.
    Therefore, differential privacy can be achieved by adding noise to the discriminator’s
    gradients without the need to clip them. As the generator has only access to information
    about the training data via the (now private) discriminator, it is not necessary
    to train the generator with DP-SGD. Like DPGAN, the following works [[72](#bib.bib72),
    [115](#bib.bib115), [30](#bib.bib30), [101](#bib.bib101)] are private variants
    of WGAN.
  prefs: []
  type: TYPE_NORMAL
- en: The PPGAN by Liu et al. [[72](#bib.bib72)] is similar to DPGAN but uses a different
    optimization algorithm (mini-batch SGD instead of RMSprop). Xu et al. [[115](#bib.bib115)]
    and Chen et al. [[30](#bib.bib30)] both proposed differentially private GANs based
    on the improved WGAN by Gulrajani et al. [[54](#bib.bib54)]. This version of WGAN
    adds a penalty on the gradient norm to the objective function instead of clipping
    the weights to improve the GANs stability. While the GANobfuscator by Xu et al.
    [[115](#bib.bib115)] trains the whole discriminator with DP-SGD, the GS-WGAN by
    Chen et al. [[30](#bib.bib30)] only sanitizes the gradients that propagate information
    from the discriminator back to the generator, resulting in a more selective gradient
    perturbation. Moreover, the GANobfuscator includes an adaptive clipping norm (based
    on average gradient norm on public data) and an adaptive learning rate (based
    on gradients’ magnitudes).
  prefs: []
  type: TYPE_NORMAL
- en: 'Torfi et al. [[101](#bib.bib101)] introduced RDP-CGAN (Rényi DP and Convolutional
    GAN) addressing two challenges that can make generating synthetic data difficult:
    1) mixed discrete-continuous data, and 2) correlated data (temporal correlations
    or correlated features). These properties are specially prevalent in health data.
    RDP-CGAN handles discrete data by adding an unsupervised feature learning step
    in the form of a convolutional autoencoder to the WGAN. This autoencoder is trained
    using DP-SGD to map the (discrete) input space to a continuous space. Correlations
    in the data are considered using one-dimensional convolutional layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Jordon et al. [[61](#bib.bib61)] took another approach and proposed a differentially
    private GAN based on the PATE framework. $k$ teacher discriminators are trained
    to distinguish real and synthetic data samples. The teachers are then used to
    label the training data for the student discriminator (by noisy aggregation of
    their predictions). A main contribution is that PATE-GAN sidesteps the need for
    public data of the original PATE method. This is essential in this setting as
    the generation of synthetic data is usually necessary exactly because no public
    data is available. They could show that their approach outperforms DPGAN.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Specific applications of DP-DL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Differentially private deep learning (DP-DL) has attracted increased interest
    in a wide range of application areas. Table [8](#S8.T8 "Table 8 ‣ 8 Specific applications
    of DP-DL ‣ Recent Advances of Differential Privacy in Centralized Deep Learning:
    A Systematic Survey") lists the application fields and corresponding papers discussed
    in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Summary of application areas of DP-DL.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Application | Paper |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Image publishing | [[123](#bib.bib123), [110](#bib.bib110)] |'
  prefs: []
  type: TYPE_TB
- en: '| Medical image analysis | [[80](#bib.bib80), [111](#bib.bib111)] |'
  prefs: []
  type: TYPE_TB
- en: '| Face recognition | [[69](#bib.bib69), [27](#bib.bib27)] |'
  prefs: []
  type: TYPE_TB
- en: '| Video analysis | [[25](#bib.bib25), [50](#bib.bib50)] |'
  prefs: []
  type: TYPE_TB
- en: '| Natural language processing | [[68](#bib.bib68), [73](#bib.bib73), [45](#bib.bib45),
    [46](#bib.bib46), [17](#bib.bib17)] |'
  prefs: []
  type: TYPE_TB
- en: '| Smart grid | [[105](#bib.bib105), [13](#bib.bib13)] |'
  prefs: []
  type: TYPE_TB
- en: '| Recommender systems | [[126](#bib.bib126), [29](#bib.bib29)] |'
  prefs: []
  type: TYPE_TB
- en: '| Mobile devices | [[108](#bib.bib108), [69](#bib.bib69), [33](#bib.bib33),
    [60](#bib.bib60)] |'
  prefs: []
  type: TYPE_TB
- en: 8.1 Image publishing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nowadays, a high amount of images is getting published on a daily basis via
    the internet. In particularly privacy-sensitive cases, de-identification techniques
    like blurring and pixelation are used to protect certain objects (e.g., faces
    and license plates). However, these methods compromise the images’ quality and
    usability. Yu et al. [[123](#bib.bib123)] and Wen et al. [[110](#bib.bib110)]
    proposed to replace the sensitive image content with synthetic data in a differentially
    private manner. They both use GANs and introduce Laplace noise into the latent
    representations. While Yu et al. [[123](#bib.bib123)] first applied a CNN to detect
    the sensitive image regions, Wen et al. [[110](#bib.bib110)] focused specifically
    on face anonymization making this step redundant. Instead, they concentrated on
    preserving the visual traits by encoding the attribute and identity information
    separately and only perturbing the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Medical image analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Privacy is of particular relevance in the health domain. One application area
    where DP was recently applied is medical image analysis, e.g., for COVID-19 diagnoses
    from chest X-rays [[80](#bib.bib80)] and for classification of histology images
    [[111](#bib.bib111)]. The former applied PATE on a convolutional deep neural network.
    The latter introduced P3SGD (patient privacy preserving SGD), a variant of DP-SGD
    that protects patient-level instead of image-level privacy. They showed that differential
    privacy can not only preserve privacy but also mitigate overfitting in cases of
    small numbers of training records.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Face recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Face recognition is a prevalent technology for security, e.g., for unlocking
    smartphones and for surveillance. For face recognition algorithms, not only privacy
    but also performance is critical as they are often deployed on devices with limited
    resources and the analyses should be carried out in real-time. To this end, methods
    that decrease the size of the models while preserving differential privacy were
    proposed.
  prefs: []
  type: TYPE_NORMAL
- en: Li et al. [[69](#bib.bib69)] introduced LightFace, a lightweight deep learning
    model designed for private face recognition on mobile devices. The approach uses
    depth-wise separable convolutions for model size reduction, and a Bayesian GAN
    and ensemble learning for privacy preservation. They were able to decrease the
    model size and the computational demand while still outperforming both DP-SGD
    and PATE. Chamikara et al. [[27](#bib.bib27)] proposed PEEP (privacy using eigenface
    perturbation), where the dimensionality of the images for training the deep learning
    model is reduced with differentially private PCA (Principal Component Analysis).
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Video analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the prevalence of CCTV cameras, automatic video analyses are on the rise,
    including traffic monitoring and security surveillance. Compared to image analysis,
    video analysis is more complex due to the additional time dimension. Private traffic
    monitoring tries to answer questions like how many people or cars were passing
    in a certain time period or how long people or cars were visible on average, while
    disclosing no information about individuals (e.g., if a specific person or car
    was observed). To this end, Cangialosi et al. [[25](#bib.bib25)] introduced the
    differentially private video analytics system Privid. Privacy-preserving video
    analysis is also relevant in security surveillance, e.g., for crime and threat
    detection. Giorgi et al. [[50](#bib.bib50)] proposed training an autoencoder with
    DP-SGD for detecting anomalies (e.g., vandalism, robbery, assault) in CCTV footage.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 Natural language processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Differential privacy is also increasingly applied in Natural language processing
    (NLP), protecting against different threat scenarios like membership inference
    of a phrase or word, author attribute inference (e.g., age or gender), authorship
    identification, or disclosure of sensitive content. Depending on the use-case
    and threat scenario, the unit of privacy can be, e.g., a token, a word, a sentence,
    a document, or a user. Differentially private NLP models can be achieved in two
    ways: 1) by training the model with DP-SGD [[68](#bib.bib68)], or 2) by perturbing
    the text representations further used for training [[73](#bib.bib73), [45](#bib.bib45),
    [46](#bib.bib46)].'
  prefs: []
  type: TYPE_NORMAL
- en: Applying the original DP-SGD on large language models can lead to deficient
    accuracy and unreasonably high computational and memory overhead. Pretraining,
    refined hyperparameter selection and finetuning can improve the performance significantly
    [[68](#bib.bib68)]. Moreover, Li et al. [[68](#bib.bib68)] proposed "ghost clipping",
    which avoids computing the per-example gradients explicitly and infers the per-example
    gradient norms in a less memory-demanding way.
  prefs: []
  type: TYPE_NORMAL
- en: The perturbation of text representations is a local DP method. Even though it
    can be used in centralized deep learning, it is primarily applied in the distributed
    setting where the server training the model is untrusted. Additionally to pure
    text processing, text representation perturbation can also be applied to related
    tasks, e.g., speech transcription [[17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 Smart energy networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Smart energy networks, also called smart grids, use digital technologies to
    optimize the generation, distribution and usage of electricity. Key components
    are smart meters, which allow real-time monitoring of energy consumption and,
    thereby, load forecasting. However, smart meter data can disclose personal information,
    e.g., daily habits of residents.
  prefs: []
  type: TYPE_NORMAL
- en: Abdalzaher et al. [[13](#bib.bib13)] provided an overview of privacy-related
    issues of smart meters and possible defense mechanisms including differential
    privacy. While they focused on private data release, Ustundag Soykan et al. [[105](#bib.bib105)]
    proposed a private load forecasting method on smart meter data using DP-SGD.
  prefs: []
  type: TYPE_NORMAL
- en: 8.7 Recommender systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recommender systems are programs that provide personalized recommendations to
    users, based on their past behaviors and contextual information (so called user
    features). Similar to other machine learning models, recommender systems are at
    risk of leaking personal information.
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. [[126](#bib.bib126)] proposed a recommender system based on a graph
    convolutional network that protects both the user-item interactions (modelled
    as a graph) and the additionally used user features. The former is achieved by
    perturbation of the model’s (polynomially approximated) loss function; the latter
    by adding noise directly to the user features.
  prefs: []
  type: TYPE_NORMAL
- en: Chen et al. [[29](#bib.bib29)] focused on cross-domain recommendation, where
    knowledge is transferred from one domain to another, usually because the target
    domain lacks enough data. The information transfer introduces a privacy risk for
    the users of the source domain. Chen et al. proposed a solution based on differentially
    private rating publishing and subsequent recommendation modeling with deep neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 8.8 Mobile devices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The combination of differential privacy and deep learning was also studied
    in the context of mobile devices. On the one hand, studies looked at how to reduce
    the model’s size and computational demand to allow the deployment and/or training
    of deep neural networks on devices with limited resources while still preserving
    utility and training privacy. For example, Wang et al. [[108](#bib.bib108)] proposed
    an architecture-independent approach based on hint learning and knowledge distillation,
    and Li et al. [[69](#bib.bib69)] introduced the lightweight face recognition algorithm
    LightFace already discussed in Section [8.3](#S8.SS3 "8.3 Face recognition ‣ 8
    Specific applications of DP-DL ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey"). On the other hand, differential privacy
    was applied on location-based services - a typical application class for mobile
    devices [[33](#bib.bib33), [60](#bib.bib60)].'
  prefs: []
  type: TYPE_NORMAL
- en: 9 Discussion and future directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This study reviews the latest developments on differential privacy in centralized
    deep learning. The main research focuses of the last years were: 1) auditing and
    evaluation, 2) improvements of the privacy-utility trade-off, 3) applications
    of DP-DL to protect against threats other than membership and attribute inference,
    4) differentially private generative models, and 5) specific application domains.
    For each subtopic, we provided a comprehensive summary of recent advances. In
    this last section, we discuss the key points, interconnections and expected future
    directions of the respective topics and differentially private centralized deep
    learning in general.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Auditing and evaluating deep learning models is a key research topic not only
    but especially for differentially private models. We expect the trend of novel
    attacks to continue, analyzing new threat scenarios and improving our understanding
    of which aspects influence the attack success. An important element of evaluation
    is the used dataset. Interestingly, many commonly used datasets for auditing DP-DL
    (e.g., MNIST, CIFAR-10 or CIFAR-100; see Table [3](#S4.T3 "Table 3 ‣ 4.1 Attack-based
    Evaluation ‣ 4 Auditing and Evaluation ‣ Recent Advances of Differential Privacy
    in Centralized Deep Learning: A Systematic Survey")) are not "relevant to the
    privacy problem" [[35](#bib.bib35)]. There is a need for more realistic benchmark
    datasets that include private features. Another point to consider is that with
    the rise of continual learning, auditing is not only relevant once before deployment
    but should be carried out repeatedly whenever the training set changes [[35](#bib.bib35)].
    This is also the case when machine unlearning is applied (as discussed in Section
    [6](#S6 "6 Beyond membership and attribute inference: Applying DP-DL to protect
    against other threats ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy disparity between subgroups is one of many biases that are studied
    in the field of fair AI - with the goal of avoiding discriminatory behavior of
    machine learning models. Its amplification by differential privacy, the underlying
    causes and possible mitigation strategies are actively researched. For example,
    de Oliviera et al. [[37](#bib.bib37)] suggested that they are preventable by better
    hyperparameter selection.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally to empirical assessment, theoretical privacy analysis might be
    able to provide more realistic upper bounds by including additional assumptions
    (e.g., about the attacker’s capabilities) or features (e.g., the clipping norm
    or initial randomness [[58](#bib.bib58)]). This could also improve the privacy-utility
    trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also anticipate further research on novel strategies or advancement on existing
    approaches to improve the privacy-utility trade-off. Some of the mentioned methods
    could be combined in the future, for example, the differentially private lottery
    ticket hypothesis approach by Gondara et al. [[51](#bib.bib51)] can be combined
    with tempered sigmoid activation functions [[86](#bib.bib86)]. Additional effort
    should be made to compare different methods to identify the best performing method(s).
    Simply summarizing the reported results, as we did in Table [5](#S5.T5 "Table
    5 ‣ 5 Improving the privacy-utility trade-off of DP-DL ‣ Recent Advances of Differential
    Privacy in Centralized Deep Learning: A Systematic Survey"), can not provide sufficient
    insight. Fair comparison would require testing the methods on the same model (i.e.,
    same architecture and hyperparameters) with the same evaluation dataset for the
    same privacy level.'
  prefs: []
  type: TYPE_NORMAL
- en: When comparing the different approaches, it is also important to note that some
    rely on public data [[102](#bib.bib102), [51](#bib.bib51), [15](#bib.bib15), [124](#bib.bib124)].
    On the one hand, public data might not be available and, therefore, prevent the
    application of those methods. On the other hand, it is debatable whether public
    availability justifies disregarding all privacy considerations (see [[35](#bib.bib35),
    [103](#bib.bib103)] for further information).
  prefs: []
  type: TYPE_NORMAL
- en: 'Section [6](#S6 "6 Beyond membership and attribute inference: Applying DP-DL
    to protect against other threats ‣ Recent Advances of Differential Privacy in
    Centralized Deep Learning: A Systematic Survey") showed that the concept of differential
    privacy can be beneficial in diverse threat scenarios. Especially the connection
    to robustness and explainability might gain importance through the growing interest
    in trustworthy AI.'
  prefs: []
  type: TYPE_NORMAL
- en: The increasing awareness of privacy concerns in combination with the many open
    questions regarding ethical, legal and methodical aspects make using synthetic
    data a tempting alternative to applying privacy-enhancing technologies to private
    data. However, it is important to spread the knowledge that synthetic data alone
    is not by default privacy-preserving [[98](#bib.bib98)]. Additional protection
    might be necessary. Even though differentially private synthetic data can be a
    viable solution, future research is needed, for example, to find good ways to
    evaluate the usefulness of the data.
  prefs: []
  type: TYPE_NORMAL
- en: This survey demonstrated how diverse the methods and applications of differential
    privacy can be on the example of deep learning models. While this flexibility
    is one of the strengths of differential privacy, it can also be a hindrance to
    its broad deployment due to insufficient understanding. The efforts to make DP
    more accessible to a wider audience and to promote its (correct) application should
    continue. That includes not only discussions about how to choose the method, the
    unit of privacy, and the privacy budget, but also how to verify that implementations
    are correct (see [[62](#bib.bib62)] and references therein for more information).
    An example where an implementation error lead to privacy issues even though theoretically
    DP was proven can be found in [[104](#bib.bib104)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional to the properties inherent to differential privacy that make it
    hard to understand for laypeople (see Section [3.1](#S3.SS1 "3.1 Differential
    Privacy (DP) ‣ 3 Preliminaries ‣ Recent Advances of Differential Privacy in Centralized
    Deep Learning: A Systematic Survey")), names that are used ambiguously in the
    research field can add to the confusion. For example, model inversion can refer
    to inferring 1) the attribute of a single record, or 2) the attribute of a class.
    While the first obviously implies a privacy concern, the second is primarily problematic
    if a class consists of instances of one individual, e.g., as is the case in face
    recognition. As a result of this ambiguous meaning, contradicting conclusions
    emerged about whether differential privacy protects against model inversion attacks.
    Some (e.g., [[122](#bib.bib122)]) used the first interpretation and concluded
    that DP naturally also protects against model inversion. Others (e.g., [[127](#bib.bib127)])
    used the second interpretation and showed that DP does not always do so. Interestingly,
    Park et al. [[87](#bib.bib87)] also uses the second interpretation but concluded
    that DP mitigates model inversion attacks. Maybe this is because DP decreases
    the accuracy of the target model, and, as Zhang et al. [[127](#bib.bib127)] argued,
    predictive power and vulnerability to model inversion go hand in hand. Future
    research should pay attention to accurately define the used terms, and, ideally,
    the research community should agree on a coherent taxonomy.'
  prefs: []
  type: TYPE_NORMAL
- en: With the rise of deep learning used in real-world scenarios, new challenges
    arise. For example, real-world datasets often contain various dependencies, where
    domain knowledge is required for their correct interpretation. Causal models [[89](#bib.bib89)]
    may help to capture and model this domain knowledge and additionally improve interpretability
    [[79](#bib.bib79)] and act as guide to avoid biases [[75](#bib.bib75)]. However,
    they may have new implications on privacy. Growing interest is not only coming
    from the research and industrial community, but also the public is actively engaging
    in discussions about the impact of AI applications on society. Most recently large
    language models like ChatGPT [[2](#bib.bib2)] are in the spotlight - among other
    things due to privacy concerns. The future will show whether differential privacy
    will be part of the next generation of deep learning deployments.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, differentially private deep learning achieved significant progress
    in recent years but open questions are still numerous. We expect the interest
    in the topic to increase further, especially as new standards and legal frameworks
    arise. On the way to trustworthy AI, we need not only technical innovations but
    also legal and ethical discussions about what privacy preservation means in the
    digital age.
  prefs: []
  type: TYPE_NORMAL
- en: 10 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This survey provides a comprehensive overview of recent trends and developments
    of differential privacy in centralized deep learning. Throughout the paper, we
    highlight the different research focuses of the last years, including auditing
    and evaluating differentially private models, improving the trade-off between
    privacy and utility, applying differential privacy methods to threats beyond membership
    and attribute inference, generating private synthetic data, and applying differentially
    private deep learning models to different application fields. A total of six insights
    have been derived from literature: (1) A need for more realistic benchmark datasets
    with private features. (2) The necessity for repeated auditing. (3) More realistic
    upper privacy bounds would be possible by including additional attack assumptions
    and model features. (4) Privacy-utility trade-offs can be improved by better comparison
    of existing methods and a possible combination of best approaches. (5) By default
    synthetic data is not privacy-preserving and differentially private synthetic
    data requires more research. (6) Ambiguously used terms lead to confusion in the
    research field and a coherent taxonomy is needed.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we explore the advancements, remaining challenges, and future prospects
    of integrating mathematical privacy guarantees into deep learning models. By shedding
    light on the current state of the field and emphasizing its potential, we hope
    to inspire further research and real-world applications of differentially private
    deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The research leading to these results has received funding from the AI for
    Green programme (Grant: 4352956). AI for Green is a research, technology and innovation
    funding programme of the Republic of Austria, Ministry of Climate Action (BMK).
    The Austrian Research Promotion Agency (FFG) has been authorised for the programme
    management. Additionally, this work was supported by the "DDAI" COMET Module within
    the COMET – Competence Centers for Excellent Technologies Programme, funded by
    the Austrian Federal Ministry (BMK and BMDW), the Austrian Research Promotion
    Agency (FFG), the province of Styria (SFG) and partners from industry and academia.
    The COMET Programme is also managed by FFG.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] At&t laboratories cambridge: The database of faces. Last accessed May 31,
    2023 from [https://cam-orl.co.uk/facedatabase.html/](https://cam-orl.co.uk/facedatabase.html/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Chatgpt. Last accessed June 7, 2023 from [https://openai.com/chatgpt](https://openai.com/chatgpt).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Github: Vggface2 dataset for face recognition. Last accessed May 31, 2023
    from [https://github.com/ox-vgg/vgg_face2](https://github.com/ox-vgg/vgg_face2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Kaggle: Acquire valued shoppers challenge. Last accessed May 31, 2023 from
    [https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data](https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Labeled faces in the wild. Last accessed May 31, 2023 from [https://vis-www.cs.umass.edu/lfw/](https://vis-www.cs.umass.edu/lfw/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Uci machine learning repository: Adult data set. Last accessed May 31,
    2023 from [https://archive.ics.uci.edu/ml/datasets/Adult](https://archive.ics.uci.edu/ml/datasets/Adult).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Uci machine learning repository: Breast cancer wisconsin (diagnostic) data
    set. Last accessed May 31, 2023 from [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Uci machine learning repository: Diabetes data set. Last accessed May 31,
    2023 from [https://archive.ics.uci.edu/ml/datasets/Diabetes](https://archive.ics.uci.edu/ml/datasets/Diabetes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Uci machine learning repository: Hepatitis data set. Last accessed May
    31, 2023 from [https://archive.ics.uci.edu/ml/datasets/Hepatitis](https://archive.ics.uci.edu/ml/datasets/Hepatitis).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Uci machine learning repository: Statlog (german credit data) data set.
    Last accessed May 31, 2023 from [https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar,
    K., and Zhang, L. Deep Learning with Differential Privacy. In Proceedings of the
    2016 ACM SIGSAC Conference on Computer and Communications Security (Vienna Austria,
    Oct. 2016), ACM, pp. 308–318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Abay, N. C., Zhou, Y., Kantarcioglu, M., Thuraisingham, B., and Sweeney,
    L. Privacy Preserving Synthetic Data Release Using Deep Learning. In Machine Learning
    and Knowledge Discovery in Databases (Cham, 2019), M. Berlingerio, F. Bonchi,
    T. Gärtner, N. Hurley, and G. Ifrim, Eds., Lecture Notes in Computer Science,
    Springer International Publishing, pp. 510–526.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Abdalzaher, M. S., Fouda, M. M., and Ibrahem, M. I. Data Privacy Preservation
    and Security in Smart Metering Systems. Energies 15, 19 (Jan. 2022), 7419.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Acs, G., Melis, L., Castelluccia, C., and De Cristofaro, E. Differentially
    Private Mixture of Generative Neural Networks. IEEE Transactions on Knowledge
    and Data Engineering 31, 6 (June 2019), 1109–1121.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Adamczewski, K., and Park, M. Differential privacy meets neural network
    pruning. arXiv preprint arXiv:2303.04612 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Adesuyi, T. A., and Kim, B. M. A layer-wise Perturbation based Privacy
    Preserving Deep Neural Networks. In 2019 International Conference on Artificial
    Intelligence in Information and Communication (ICAIIC) (Feb. 2019), pp. 389–394.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Ahmed, S., Chowdhury, A. R., Fawaz, K., and Ramanathan, P. Preech: A system
    for privacy-preserving speech transcription. In Proceedings of the 29th USENIX
    Conference on Security Symposium (2020), pp. 2703–2720.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein Generative Adversarial
    Networks. In Proceedings of the 34th International Conference on Machine Learning
    (July 2017), PMLR, pp. 214–223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Backstrom, L., Dwork, C., and Kleinberg, J. Wherefore art thou r3579x?
    anonymized social networks, hidden patterns, and structural steganography. In
    Proceedings of the 16th international conference on World Wide Web (New York,
    NY, USA, May 2007), WWW ’07, Association for Computing Machinery, pp. 181–190.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Bagdasaryan, E., Poursaeed, O., and Shmatikov, V. Differential Privacy
    Has Disparate Impact on Model Accuracy. In Advances in Neural Information Processing
    Systems (2019), vol. 32, Curran Associates, Inc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Blanco-Justicia, A., Sanchez, D., Domingo-Ferrer, J., and Muralidhar,
    K. A Critical Review on the Use (and Misuse) of Differential Privacy in Machine
    Learning. ACM Computing Surveys 55, 8 (Aug. 2023), 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Bloom, J. S., Kotenko, I., Sadhu, M. J., Treusch, S., Albert, F. W., and
    Kruglyak, L. Genetic interactions contribute less than additive effects to quantitative
    trait variation in yeast. Nature communications 6, 1 (2015), 8712.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Boulemtafes, A., Derhab, A., and Challal, Y. A review of privacy-preserving
    techniques for deep learning. Neurocomputing 384 (Apr. 2020), 21–45.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Bourtoule, L., Chandrasekaran, V., Choquette-Choo, C. A., Jia, H., Travers,
    A., Zhang, B., Lie, D., and Papernot, N. Machine Unlearning. In 2021 IEEE Symposium
    on Security and Privacy (SP) (May 2021), pp. 141–159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Cangialosi, F., Agarwal, N., Arun, V., Narayana, S., Sarwate, A., and
    Netravali, R. Privid: Practical,$\{$Privacy-Preserving$\}$ video analytics queries.
    In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI
    22) (2022), pp. 209–228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Cao, Y., and Yang, J. Towards Making Systems Forget with Machine Unlearning.
    In 2015 IEEE Symposium on Security and Privacy (May 2015), pp. 463–480.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Chamikara, M. A. P., Bertok, P., Khalil, I., Liu, D., and Camtepe, S.
    Privacy Preserving Face Recognition Utilizing Differential Privacy. Computers
    & Security 97 (Oct. 2020), 101951.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Chang, S., and Li, C. Privacy in Neural Network Learning: Threats and
    Countermeasures. IEEE Network 32, 4 (July 2018), 61–67.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Chen, C., Wu, H., Su, J., Lyu, L., Zheng, X., and Wang, L. Differential
    Private Knowledge Transfer for Privacy-Preserving Cross-Domain Recommendation.
    In Proceedings of the ACM Web Conference 2022 (New York, NY, USA, Apr. 2022),
    WWW ’22, Association for Computing Machinery, pp. 1455–1465.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Chen, D., Orekondy, T., and Fritz, M. GS-WGAN: A Gradient-Sanitized Approach
    for Learning Differentially Private Generators. In Advances in Neural Information
    Processing Systems (2020), vol. 33, Curran Associates, Inc., pp. 12673–12684.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Chen, J., Wang, W. H., and Shi, X. Differential Privacy Protection Against
    Membership Inference Attack on Machine Learning for Genomic Data. In Biocomputing
    2021. WORLD SCIENTIFIC, Oct. 2020, pp. 26–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Chen, M., Zhang, Z., Wang, T., Backes, M., Humbert, M., and Zhang, Y.
    When Machine Unlearning Jeopardizes Privacy. In Proceedings of the 2021 ACM SIGSAC
    Conference on Computer and Communications Security (New York, NY, USA, Nov. 2021),
    CCS ’21, Association for Computing Machinery, pp. 896–911.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Chen, S., Fu, A., Shen, J., Yu, S., Wang, H., and Sun, H. RNN-DP: A new
    differential privacy scheme base on Recurrent Neural Network for Dynamic trajectory
    privacy protection. Journal of Network and Computer Applications 168 (Oct. 2020),
    102736.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Chen, X., Wu, S. Z., and Hong, M. Understanding Gradient Clipping in Private
    SGD: A Geometric Perspective. In Advances in Neural Information Processing Systems
    (2020), vol. 33, Curran Associates, Inc., pp. 13773–13782.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Cummings, R., Desfontaines, D., Evans, D., Geambasu, R., Jagielski, M.,
    Huang, Y., Kairouz, P., Kamath, G., Oh, S., Ohrimenko, O., et al. Challenges towards
    the next frontier in privacy. arXiv preprint arXiv:2304.06929 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] De Cristofaro, E. A Critical Overview of Privacy in Machine Learning.
    IEEE Security & Privacy 19, 4 (July 2021), 19–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] de Oliveira, A. S., Kaplan, C., Mallat, K., and Chakraborty, T. An empirical
    analysis of fairness notions under differential privacy. arXiv preprint arXiv:2302.02910
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Desfontaines, D. A list of real-world uses of differential privacy, 2021.
    Last accessed May 31, 2023 from https://desfontain.es/privacy/real-world-differential-privacy.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Desfontaines, D., and Pejó, B. Sok: differential privacies. Proceedings
    on privacy enhancing technologies 2020, 2 (2020), 288–313.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Ding, J., Errapotu, S. M., Guo, Y., Zhang, H., Yuan, D., and Pan, M. Private
    Empirical Risk Minimization With Analytic Gaussian Mechanism for Healthcare System.
    IEEE Transactions on Big Data 8, 4 (Aug. 2022), 1107–1117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Du, M., Jia, R., and Song, D. Robust anomaly detection and backdoor attack
    detection via differential privacy. arXiv preprint arXiv:1911.07116 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Dwork, C., and Roth, A. The Algorithmic Foundations of Differential Privacy.
    Foundations and Trends® in Theoretical Computer Science 9, 3–4 (Aug. 2014), 211–407.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Dwork, C., and Rothblum, G. N. Concentrated differential privacy. arXiv
    preprint arXiv:1603.01887 (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Farrand, T., Mireshghallah, F., Singh, S., and Trask, A. Neither Private
    Nor Fair: Impact of Data Imbalance on Utility and Fairness in Differential Privacy.
    In Proceedings of the 2020 Workshop on Privacy-Preserving Machine Learning in
    Practice (New York, NY, USA, Nov. 2020), PPMLP’20, Association for Computing Machinery,
    pp. 15–19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Fernandes, N., Dras, M., and McIver, A. Generalised differential privacy
    for text document processing. In Principles of Security and Trust: 8th International
    Conference, POST 2019, Held as Part of the European Joint Conferences on Theory
    and Practice of Software, ETAPS 2019, Prague, Czech Republic, April 6–11, 2019,
    Proceedings 8 (2019), Springer International Publishing, pp. 123–148.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Feyisetan, O., Diethe, T., and Drake, T. Leveraging Hierarchical Representations
    for Preserving Privacy and Utility in Text. In 2019 IEEE International Conference
    on Data Mining (ICDM) (Nov. 2019), pp. 210–219.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Frankle, J., and Carbin, M. The lottery ticket hypothesis: Finding sparse,
    trainable neural networks. arXiv preprint arXiv:1803.03635 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Frankle, J., Dziugaite, G. K., Roy, D., and Carbin, M. Linear mode connectivity
    and the lottery ticket hypothesis. In International Conference on Machine Learning
    (2020), PMLR, pp. 3259–3269.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Ganta, S. R., Kasiviswanathan, S. P., and Smith, A. Composition attacks
    and auxiliary information in data privacy. In Proceedings of the 14th ACM SIGKDD
    international conference on Knowledge discovery and data mining (New York, NY,
    USA, Aug. 2008), KDD ’08, Association for Computing Machinery, pp. 265–273.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Giorgi, G., Abbasi, W., and Saracino, A. Privacy-Preserving Analysis for
    Remote Video Anomaly Detection in Real Life Environments. Journal of Wireless
    Mobile Networks, Ubiquitous Computing, and Dependable Applications 13, 1 (Mar.
    2022), 112–136.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Gondara, L., Carvalho, R. S., and Wang, K. Training Differentially Private
    Neural Networks with Lottery Tickets. In Computer Security – ESORICS 2021 (Cham,
    2021), E. Bertino, H. Shulman, and M. Waidner, Eds., Lecture Notes in Computer
    Science, Springer International Publishing, pp. 543–562.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Gong, M., Pan, K., Xie, Y., Qin, A. K., and Tang, Z. Preserving differential
    privacy in deep neural networks with relevance-based adaptive noise imposition.
    Neural Networks 125 (May 2020), 131–141.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Gong, M., Xie, Y., Pan, K., Feng, K., and Qin, A. A Survey on Differentially
    Private Machine Learning [Review Article]. IEEE Computational Intelligence Magazine
    15, 2 (May 2020), 49–64.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C.
    Improved Training of Wasserstein GANs. In Advances in Neural Information Processing
    Systems (2017), vol. 30, Curran Associates, Inc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Ha, T., Dang, T. K., Dang, T. T., Truong, T. A., and Nguyen, M. T. Differential
    Privacy in Deep Learning: An Overview. In 2019 International Conference on Advanced
    Computing and Applications (ACOMP) (Nov. 2019), pp. 97–102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Ha, T., Dang, T. K., Le, H., and Truong, T. A. Security and Privacy Issues
    in Deep Learning: A Brief Review. SN Computer Science 1, 5 (Aug. 2020), 253.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Harder, F., Bauer, M., and Park, M. Interpretable and Differentially Private
    Predictions. Proceedings of the AAAI Conference on Artificial Intelligence 34,
    04 (Apr. 2020), 4083–4090.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Jagielski, M., Ullman, J., and Oprea, A. Auditing Differentially Private
    Machine Learning: How Private is Private SGD? In Advances in Neural Information
    Processing Systems (2020), vol. 33, Curran Associates, Inc., pp. 22205–22216.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Jayaraman, B., and Evans, D. Evaluating differentially private machine
    learning in practice. In USENIX Security Symposium (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Jiang, H., Wang, M., Zhao, P., Xiao, Z., and Dustdar, S. A Utility-Aware
    General Framework With Quantifiable Privacy Preservation for Destination Prediction
    in LBSs. IEEE/ACM Transactions on Networking 29, 5 (Oct. 2021), 2228–2241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Jordon, J., Yoon, J., and Van Der Schaar, M. Pate-gan: Generating synthetic
    data with differential privacy guarantees. In International conference on learning
    representations (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Kifer, D., Messing, S., Roth, A., Thakurta, A., and Zhang, D. Guidelines
    for implementing and auditing differentially private systems. arXiv preprint arXiv:2002.04049
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Krizhevsky, A. Cifar-10 and cifar-100 datasets. Last accessed May 31,
    2023 from [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] LeCun, Y. Mnist. Last accessed May 31, 2023 from [yann.lecun.com/exdb/mnist/](yann.lecun.com/exdb/mnist/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Lecuyer, M., Atlidakis, V., Geambasu, R., Hsu, D., and Jana, S. Certified
    Robustness to Adversarial Examples with Differential Privacy. In 2019 IEEE Symposium
    on Security and Privacy (SP) (May 2019), pp. 656–672.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Leino, K., and Fredrikson, M. Stolen memories: Leveraging model memorization
    for calibrated white-box membership inference. In 29th USENIX Security Symposium
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Li, N., Li, T., and Venkatasubramanian, S. t-Closeness: Privacy Beyond
    k-Anonymity and l-Diversity. In 2007 IEEE 23rd International Conference on Data
    Engineering (Apr. 2007), pp. 106–115.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Li, X., Tramer, F., Liang, P., and Hashimoto, T. Large language models
    can be strong differentially private learners. arXiv preprint arXiv:2110.05679
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Li, Y., Wang, Y., and Li, D. Privacy-preserving lightweight face recognition.
    Neurocomputing 363 (Oct. 2019), 212–222.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Liu, B., Ding, M., Shaham, S., Rahayu, W., Farokhi, F., and Lin, Z. When
    Machine Learning Meets Privacy: A Survey and Outlook. ACM Computing Surveys 54,
    2 (Mar. 2021), 31:1–31:36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Liu, J., and Talwar, K. Private selection from private candidates. In
    Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing (New
    York, NY, USA, June 2019), STOC 2019, Association for Computing Machinery, pp. 298–309.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Liu, Y., Peng, J., Yu, J. J., and Wu, Y. PPGAN: Privacy-Preserving Generative
    Adversarial Network. In 2019 IEEE 25th International Conference on Parallel and
    Distributed Systems (ICPADS) (Dec. 2019), pp. 985–989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Lyu, L., Li, Y., He, X., and Xiao, T. Towards Differentially Private Text
    Representations. In Proceedings of the 43rd International ACM SIGIR Conference
    on Research and Development in Information Retrieval (New York, NY, USA, July
    2020), SIGIR ’20, Association for Computing Machinery, pp. 1813–1816.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Machanavajjhala, A., Kifer, D., Gehrke, J., and Venkitasubramaniam, M.
    L-diversity: Privacy beyond k-anonymity. ACM Transactions on Knowledge Discovery
    from Data 1, 1 (Mar. 2007), 3–es.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Makhlouf, K., Zhioua, S., and Palamidessi, C. Survey on causal-based machine
    learning fairness notions. arXiv preprint arXiv:2010.09553 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Mireshghallah, F., Taram, M., Vepakomma, P., Singh, A., Raskar, R., and
    Esmaeilzadeh, H. Privacy in Deep Learning: A Survey, Nov. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Mironov, I. Rényi Differential Privacy. In 2017 IEEE 30th Computer Security
    Foundations Symposium (CSF) (Aug. 2017), pp. 263–275.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Montavon, G., Binder, A., Lapuschkin, S., Samek, W., and Müller, K.-R.
    Layer-Wise Relevance Propagation: An Overview. In Explainable AI: Interpreting,
    Explaining and Visualizing Deep Learning, W. Samek, G. Montavon, A. Vedaldi, L. K.
    Hansen, and K.-R. Müller, Eds., Lecture Notes in Computer Science. Springer International
    Publishing, Cham, 2019, pp. 193–209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Moraffah, R., Karami, M., Guo, R., Raglin, A., and Liu, H. Causal interpretability
    for machine learning-problems, methods and evaluation. ACM SIGKDD Explorations
    Newsletter 22, 1 (2020), 18–33.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Müftüoğlu, Z., Kizrak, M. A., and Yildlnm, T. Differential Privacy Practice
    on Diagnosis of COVID-19 Radiology Imaging Using EfficientNet. In 2020 International
    Conference on INnovations in Intelligent SysTems and Applications (INISTA) (Aug.
    2020), pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Narayanan, A., and Shmatikov, V. Robust De-anonymization of Large Sparse
    Datasets. In 2008 IEEE Symposium on Security and Privacy (sp 2008) (May 2008),
    pp. 111–125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Nasr, M., Songi, S., Thakurta, A., Papernot, N., and Carlin, N. Adversary
    Instantiation: Lower Bounds for Differentially Private Machine Learning. In 2021
    IEEE Symposium on Security and Privacy (SP) (May 2021), pp. 866–882.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Nicolae, M.-I., Sinn, M., Tran, M. N., Buesser, B., Rawat, A., Wistuba,
    M., Zantedeschi, V., Baracaldo, N., Chen, B., Ludwig, H., Molloy, I., and Edwards,
    B. Adversarial robustness toolbox v1.2.0. CoRR 1807.01069 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Ouadrhiri, A. E., and Abdelhadi, A. Differential Privacy for Deep and
    Federated Learning: A Survey. IEEE Access 10 (2022), 22359–22380.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., and Talwar, K.
    Semi-supervised knowledge transfer for deep learning from private training data.
    arXiv preprint arXiv:1610.05755 (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Papernot, N., Thakurta, A., Song, S., Chien, S., and Erlingsson, U. Tempered
    Sigmoid Activations for Deep Learning with Differential Privacy, July 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Park, C., Hong, D., and Seo, C. An Attack-Based Evaluation Method for
    Differentially Private Learning Against Model Inversion Attack. IEEE Access 7
    (2019), 124988–124999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Park, M., Foulds, J., Choudhary, K., and Welling, M. DP-EM: Differentially
    Private Expectation Maximization. In Proceedings of the 20th International Conference
    on Artificial Intelligence and Statistics (Apr. 2017), PMLR, pp. 896–904.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Pearl, J., and Mackenzie, D. The book of why: the new science of cause
    and effect. Basic books, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Phan, N., Vu, M., Liu, Y., Jin, R., Dou, D., Wu, X., and Thai, M. T. Heterogeneous
    gaussian mechanism: Preserving differential privacy in deep learning with provable
    robustness. arXiv preprint arXiv:1906.01444 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Phan, N., Wu, X., Hu, H., and Dou, D. Adaptive laplace mechanism: Differential
    privacy preservation in deep learning. In 2017 IEEE international conference on
    data mining (ICDM) (2017), IEEE, pp. 385–394.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Phong, L. T., and Phuong, T. T. Differentially private stochastic gradient
    descent via compression and memorization. Journal of Systems Architecture 135
    (Feb. 2023), 102819.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Rechberger, C., and Walch, R. Privacy-Preserving Machine Learning Using
    Cryptography. In Security and Artificial Intelligence: A Crossdisciplinary Approach,
    L. Batina, T. Bäck, I. Buhan, and S. Picek, Eds., Lecture Notes in Computer Science.
    Springer International Publishing, Cham, 2022, pp. 109–129.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Rigaki, M., and Garcia, S. A survey of privacy attacks in machine learning.
    arXiv preprint arXiv:2007.07646 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Shafee, A., and Awaad, T. A. Privacy attacks against deep learning models
    and their countermeasures. Journal of Systems Architecture 114 (Mar. 2021), 101940.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Shen, Z., and Zhong, T. Analysis of Application Examples of Differential
    Privacy in Deep Learning. Computational Intelligence and Neuroscience 2021 (Oct.
    2021), e4244040.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Shokri, R., Stronati, M., Song, C., and Shmatikov, V. Membership Inference
    Attacks Against Machine Learning Models. In 2017 IEEE Symposium on Security and
    Privacy (SP) (May 2017), pp. 3–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Stadler, T., Oprisanu, B., and Troncoso, C. Synthetic data–anonymisation
    groundhog day. In 31st USENIX Security Symposium (USENIX Security 22) (2022),
    pp. 1451–1468.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Sweeney, L. k-anonymity: A model for protecting privacy. International
    journal of uncertainty, fuzziness and knowledge-based systems 10, 05 (2002), 557–570.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Tanuwidjaja, H. C., Choi, R., and Kim, K. A Survey on Deep Learning Techniques
    for Privacy-Preserving. In Machine Learning for Cyber Security (Cham, 2019), X. Chen,
    X. Huang, and J. Zhang, Eds., Lecture Notes in Computer Science, Springer International
    Publishing, pp. 29–46.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Torfi, A., Fox, E. A., and Reddy, C. K. Differentially private synthetic
    medical data generation using convolutional GANs. Information Sciences 586 (Mar.
    2022), 485–500.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Tramer, F., and Boneh, D. Differentially private learning needs better
    features (or much more data). arXiv preprint arXiv:2011.11660 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Tramèr, F., Kamath, G., and Carlini, N. Considerations for differentially
    private learning with large-scale public pretraining. arXiv preprint arXiv:2212.06470
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Tramer, F., Terzis, A., Steinke, T., Song, S., Jagielski, M., and Carlini,
    N. Debugging differential privacy: A case study for privacy auditing. arXiv preprint
    arXiv:2202.12219 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Ustundag Soykan, E., Bilgin, Z., Ersoy, M. A., and Tomur, E. Differentially
    Private Deep Learning for Load Forecasting on Smart Grid. In 2019 IEEE Globecom
    Workshops (GC Wkshps) (Dec. 2019), pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Vasa, J., and Thakkar, A. Deep Learning: Differential Privacy Preservation
    in the Era of Big Data. Journal of Computer Information Systems 0, 0 (July 2022),
    1–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Villaronga, E. F., Kieseberg, P., and Li, T. Humans forget, machines
    remember: Artificial intelligence and the Right to Be Forgotten. Computer Law
    & Security Review 34, 2 (Apr. 2018), 304–313.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Wang, J., Bao, W., Sun, L., Zhu, X., Cao, B., and Yu, P. S. Private Model
    Compression via Knowledge Distillation. Proceedings of the AAAI Conference on
    Artificial Intelligence 33, 01 (July 2019), 1190–1197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Wang, Y., Gu, M., Ma, J., and Jin, Q. DNN-DP: Differential Privacy Enabled
    Deep Neural Network Learning Framework for Sensitive Crowdsourcing Data. IEEE
    Transactions on Computational Social Systems 7, 1 (Feb. 2020), 215–224.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Wen, Y., Liu, B., Ding, M., Xie, R., and Song, L. IdentityDP: Differential
    private identification protection for face images. Neurocomputing 501 (Aug. 2022),
    197–211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Wu, B., Zhao, S., Sun, G., Zhang, X., Su, Z., Zeng, C., and Liu, Z. P3sgd:
    Patient privacy preserving sgd for regularizing deep cnns in pathological image
    classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (2019), pp. 2099–2108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Xiang, L., Yang, J., and Li, B. Differentially-Private Deep Learning
    from an optimization Perspective. In IEEE INFOCOM 2019 - IEEE Conference on Computer
    Communications (Apr. 2019), pp. 559–567.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset
    for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747
    (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Xie, L., Lin, K., Wang, S., Wang, F., and Zhou, J. Differentially private
    generative adversarial network. arXiv preprint arXiv:1802.06739 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Xu, C., Ren, J., Zhang, D., Zhang, Y., Qin, Z., and Ren, K. GANobfuscator:
    Mitigating Information Leakage Under GAN via Differential Privacy. IEEE Transactions
    on Information Forensics and Security 14, 9 (Sept. 2019), 2358–2371.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Xu, Z., Shi, S., Liu, A. X., Zhao, J., and Chen, L. An Adaptive and Fast
    Convergent Approach to Differentially Private Deep Learning. In IEEE INFOCOM 2020
    - IEEE Conference on Computer Communications (July 2020), pp. 1867–1876.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Xue, M., Yuan, C., Wu, H., Zhang, Y., and Liu, W. Machine Learning Security:
    Threats, Countermeasures, and Evaluations. IEEE Access 8 (2020), 74720–74742.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Yan, H., Li, X., Li, H., Li, J., Sun, W., and Li, F. Monitoring-Based
    Differential Privacy Mechanism Against Query Flooding-Based Model Extraction Attack.
    IEEE Transactions on Dependable and Secure Computing 19, 4 (July 2022), 2680–2694.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Yang, L., Song, Y., Gao, S., Hu, A., and Xiao, B. Griffin: Real-Time
    Network Intrusion Detection System via Ensemble of Autoencoder in SDN. IEEE Transactions
    on Network and Service Management 19, 3 (Sept. 2022), 2269–2281.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Yao, Y. Exploration of Membership Inference Attack on Convolutional Neural
    Networks and Its Defenses. In 2022 International Conference on Image Processing,
    Computer Vision and Machine Learning (ICICML) (Oct. 2022), pp. 604–610.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Ye, D., Shen, S., Zhu, T., Liu, B., and Zhou, W. One Parameter Defense—Defending
    Against Data Inference Attacks via Differential Privacy. IEEE Transactions on
    Information Forensics and Security 17 (2022), 1466–1480.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Yeom, S., Giacomelli, I., Fredrikson, M., and Jha, S. Privacy Risk in
    Machine Learning: Analyzing the Connection to Overfitting. In 2018 IEEE 31st Computer
    Security Foundations Symposium (CSF) (July 2018), pp. 268–282.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Yu, J., Xue, H., Liu, B., Wang, Y., Zhu, S., and Ding, M. GAN-Based Differential
    Private Image Privacy Protection Framework for the Internet of Multimedia Things.
    Sensors 21, 1 (Jan. 2021), 58.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Yu, L., Liu, L., Pu, C., Gursoy, M. E., and Truex, S. Differentially
    Private Model Publishing for Deep Learning. In 2019 IEEE Symposium on Security
    and Privacy (SP) (May 2019), pp. 332–349.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Zhang, J., He, T., Sra, S., and Jadbabaie, A. Why gradient clipping accelerates
    training: A theoretical justification for adaptivity. arXiv preprint arXiv:1905.11881
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Zhang, S., Yin, H., Chen, T., Huang, Z., Cui, L., and Zhang, X. Graph
    Embedding for Recommendation against Attribute Inference Attacks. In Proceedings
    of the Web Conference 2021 (New York, NY, USA, June 2021), WWW ’21, Association
    for Computing Machinery, pp. 3002–3014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Zhang, Y., Jia, R., Pei, H., Wang, W., Li, B., and Song, D. The secret
    revealer: Generative model-inversion attacks against deep neural networks. In
    Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
    (2020), pp. 253–261.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Zhao, J., Chen, Y., and Zhang, W. Differential Privacy Preservation in
    Deep Learning: Challenges, Opportunities and Solutions. IEEE Access 7 (2019),
    48901–48911.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Zheng, H., Ye, Q., Hu, H., Fang, C., and Shi, J. BDPL: A Boundary Differentially
    Private Layer Against Machine Learning Model Extraction Attacks. In Computer Security
    – ESORICS 2019 (Cham, 2019), K. Sako, S. Schneider, and P. Y. A. Ryan, Eds., Lecture
    Notes in Computer Science, Springer International Publishing, pp. 66–83.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Zheng, H., Ye, Q., Hu, H., Fang, C., and Shi, J. Protecting Decision
    Boundary of Machine Learning Model With Differentially Private Perturbation. IEEE
    Transactions on Dependable and Secure Computing 19, 3 (May 2022), 2007–2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Zhu, T., Ye, D., Wang, W., Zhou, W., and Yu, P. S. More Than Privacy:
    Applying Differential Privacy in Key Areas of Artificial Intelligence. IEEE Transactions
    on Knowledge and Data Engineering 34, 6 (June 2022), 2824–2843.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
