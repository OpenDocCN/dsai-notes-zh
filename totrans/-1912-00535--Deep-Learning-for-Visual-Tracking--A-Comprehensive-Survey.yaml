- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:03:55'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1912.00535] Deep Learning for Visual Tracking: A Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.00535](https://ar5iv.labs.arxiv.org/html/1912.00535)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Visual Tracking: A Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Seyed Mojtaba Marvasti-Zadeh,  Li Cheng,  Hossein Ghanei-Yakhdan, and Shohreh Kasaei,
    S. M. Marvasti-Zadeh is with Digital Image & Video Processing Lab (DIVPL), Department
    of Electrical Engineering, Yazd University, Iran. He is also a member of Vision
    and Learning Lab, University of Alberta, Canada, and Image Processing Lab (IPL),
    Sharif University of Technology, Iran. E-mail: [mojtaba.marvasti@ualberta.ca](mailto:mojtaba.marvasti@ualberta.ca)
    L. Cheng is with Vision and Learning Lab, Department of Electrical and Computer
    Engineering, University of Alberta, Edmonton, Canada. E-mail: [lcheng5@ualberta.ca](mailto:lcheng5@ualberta.ca)
    H. Ghanei-Yakhdan is with Digital Image & Video Processing Lab (DIVPL), Department
    of Electrical Engineering, Yazd University, Yazd, Iran. E-mail: [hghaneiy@yazd.ac.ir](mailto:hghaneiy@yazd.ac.ir)
    S. Kasaei is with Image Processing Lab (IPL), Department of Computer Engineering,
    Sharif University of Technology, Tehran, Iran. E-mail: [kasaei@sharif.edu](mailto:kasaei@sharif.edu)Manuscript
    received …; revised …'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Visual target tracking is one of the most sought-after yet challenging research
    topics in computer vision. Given the ill-posed nature of the problem and its popularity
    in a broad range of real-world scenarios, a number of large-scale benchmark datasets
    have been established, on which considerable methods have been developed and demonstrated
    with significant progress in recent years – predominantly by recent deep learning
    (DL)-based methods. This survey aims to systematically investigate the current
    DL-based visual tracking methods, benchmark datasets, and evaluation metrics.
    It also extensively evaluates and analyzes the leading visual tracking methods.
    First, the fundamental characteristics, primary motivations, and contributions
    of DL-based methods are summarized from nine key aspects of: network architecture,
    network exploitation, network training for visual tracking, network objective,
    network output, exploitation of correlation filter advantages, aerial-view tracking,
    long-term tracking, and online tracking. Second, popular visual tracking benchmarks
    and their respective properties are compared, and their evaluation metrics are
    summarized. Third, the state-of-the-art DL-based methods are comprehensively examined
    on a set of well-established benchmarks of OTB2013, OTB2015, VOT2018, LaSOT, UAV123,
    UAVDT, and VisDrone2019\. Finally, by conducting critical analyses of these state-of-the-art
    trackers quantitatively and qualitatively, their pros and cons under various common
    scenarios are investigated. It may serve as a gentle use guide for practitioners
    to weigh when and under what conditions to choose which method(s). It also facilitates
    a discussion on ongoing issues and sheds light on promising research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Visual tracking, deep learning, computer vision, appearance modeling.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generic visual tracking aims to estimate an unknown visual target trajectory
    when only an initial state of the target (in a video frame) is available. Visual
    tracking is an open and attractive research field with a broad extent of categories
    (see Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")) and applications, including self-driving cars [[1](#bib.bib1)],
    autonomous robots [[2](#bib.bib2)], surveillance [[3](#bib.bib3)], augmented reality
    [[4](#bib.bib4)], aerial-view tracking [[5](#bib.bib5)], sports [[6](#bib.bib6)],
    surgery [[7](#bib.bib7)], biology [[8](#bib.bib8)], ocean exploration [[9](#bib.bib9)],
    to name a few.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f453d3726e8424cfd7d95cd860e22ba0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An overview of visual target tracking.'
  prefs: []
  type: TYPE_NORMAL
- en: The ill-posed definition of the visual tracking (i.e., model-free tracking,
    on-the-fly learning, single-camera, 2D information) is more challenging in complicated
    real-world scenarios which may include arbitrary classes of targets (e.g., human,
    drone, animal, vehicle) and motion models, various imaging characteristics (e.g.,
    static/moving camera, smooth/fast movement, camera resolution), and changes in
    environmental conditions (e.g., illumination variation, background clutter, crowded
    scenes). Traditional methods employ various visual tracking frameworks, such as
    discriminative correlation filters (DCF) [[10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)],
    silhouette tracking [[17](#bib.bib17)], Kernel tracking [[18](#bib.bib18)], point
    tracking [[19](#bib.bib19)] – for appearance & motion modeling of a target. In
    general, traditional trackers have inflexible assumptions about target structures
    & their motion in real-world scenarios. These trackers exploit handcrafted features
    (e.g., the histogram of oriented gradients (HOG) [[20](#bib.bib20)] and Color-Names
    (CN) [[21](#bib.bib21)]), so they cannot interpret semantic target information
    and handle significant appearance changes. However, some tracking-by-detection
    methods (e.g., DCF-based trackers) provide an appealing trade-off of competitive
    tracking performance and efficient computations [[22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24)]. For instance, aerial-view trackers [[25](#bib.bib25), [26](#bib.bib26),
    [27](#bib.bib27)] extensively use these CPU-based algorithms considering limited
    on-board computational power & embedded hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by deep learning (DL) breakthroughs [[28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)] in ImageNet large-scale
    visual recognition competition (ILSVRC) [[33](#bib.bib33)] and also visual object
    tracking (VOT) challenge [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)], DL-based
    methods have attracted considerable interest in the visual tracking community
    to provide robust trackers. Although convolutional neural networks (CNNs) have
    been dominant networks initially, a broad range of architectures such as recurrent
    neural networks (RNNs), auto-encoders (AEs), generative adversarial networks (GANs),
    and especially Siamese neural networks (SNNs) & custom neural networks are currently
    investigated. Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Deep Learning for Visual
    Tracking: A Comprehensive Survey") presents a brief history of the development
    of deep visual trackers in recent years. The state-of-the-art DL-based visual
    trackers have distinct characteristics such as exploitation of various architectures,
    backbone networks, learning procedures, training datasets, network objectives,
    network outputs, types of exploited deep features, CPU/GPU implementations, programming
    languages & frameworks, speed, and so forth. Therefore, this work provides a comparative
    study of DL-based trackers, benchmark datasets, and evaluation metrics to investigate
    proposed trackers in detail and facilitate developing advanced trackers.'
  prefs: []
  type: TYPE_NORMAL
- en: \justify![Refer to caption](img/3940bb968e9ff0e112a8a93d553d1765.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Timeline of deep visual tracking methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '2015: Exploring/studying deep features to exploit the traditional methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '2016: Offline training/fine-tuning of DNNs, employing Siamese networks for
    real-time tracking, and integrating DNNs into traditional frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: '2017: Incorporating temporal & contextual information, and investigating various
    offline training on large-scale datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '2018: Studying different learning & search strategies, and designing more sophisticated
    architectures for visual tracking task.'
  prefs: []
  type: TYPE_NORMAL
- en: '2019: Investigating deep detection & segmentation approaches, and taking advantages
    of deeper backbone networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Visual target trackers can be roughly classified into two main categories, before
    and after the revolution of DL in computer vision. The first category is primarily
    reviewed by [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)],
    which include traditional trackers based on classical appearance & motion models,
    and then examine their pros and cons systematically, experimentally, or both.
    These trackers employ manually-designed features for target modeling to alleviate
    appearance variations and to provide efficient computational complexity. For instance,
    although these trackers are suitable to implement on the flying robots [[26](#bib.bib26),
    [25](#bib.bib25), [45](#bib.bib45), [27](#bib.bib27), [46](#bib.bib46)] due to
    the restrictions of using advanced GPUs, they do not have enough robustness to
    handle the challenges of in-the-wild videos. Typically, these trackers try to
    ensemble multiple features to construct a complementary set of visual cues. But,
    tuning an optimal trade-off that also maintains efficiency for real-world scenarios
    is tricky. Considering DL-based trackers’ significant progress in recent years,
    the reviewed methods by the mentioned works are outdated.
  prefs: []
  type: TYPE_NORMAL
- en: The second category includes DL-based trackers that employ either deep off-the-shelf
    features or end-to-end networks. A straightforward approach is integrating pre-trained
    deep features into the traditional frameworks. However, such trackers result in
    inconsistency problems considering task differences. But, end-to-end trained visual
    trackers have been investigated regarding existing tracking challenges. Recently,
    [[47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)] review limited DL-based
    visual trackers. For instance, [[47](#bib.bib47), [48](#bib.bib48)] categorize
    some handcrafted & deep methods into the correlation filter trackers & non-correlation
    filter ones. Then, a further classification based on architectures & tracking
    mechanisms has been applied. The work [[50](#bib.bib50)] particularly investigates
    some SNN-based trackers based on their network branches, layers, and training
    aspects. However, it does not include state-of-the-art trackers and custom networks
    with & without partial exploitation of SNNs. At last, the work [[49](#bib.bib49)]
    categorizes the DL-based trackers according to their structure, function, and
    training. Then, the evaluations are performed to conclude the categorizations
    based on the observations. From the structure perspective, the trackers are categorized
    into the CNN, RNN, and others, while they are classified into the feature extraction
    network (FEN) or end-to-end network (EEN) according to their functionality in
    visual tracking. The EENs are also classified in terms of the outputs, including
    object score, confidence map, and bounding box (BB). Finally, DL-based methods
    are categorized according to pre-training & online learning based on the network
    training perspective.
  prefs: []
  type: TYPE_NORMAL
- en: According to the previous efforts, the motivations of this work are presented
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 1) Despite all efforts, existing review papers do not include state-of-the-art
    visual trackers that roughly employ Siamese or customized networks.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Notwithstanding significant progress in recent years, long-term trackers
    and tracking from aerial-views have not yet been studied. Hence, investigating
    the current issues and proposed solutions are necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Many details are ignored in previous works studying the DL-based trackers
    (e.g., backbone networks, training details, exploited features, implementations,
    etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 4) State-of-the-art benchmark datasets (short-term, long-term, aerial-view)
    are not compared completely.
  prefs: []
  type: TYPE_NORMAL
- en: 5) Finally, exhaustive comparisons of DL-based trackers on a wide variety of
    benchmarks have not been previously performed. These analyzes can demonstrate
    the advantages and limitations of existing trackers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivated by the aforementioned concerns, this work’s primary goals are filling
    the gaps, investigating the present issues, and studying potential future directions.
    Thus, we focus merely on extensive state-of-the-art DL-based trackers, namely:
    HCFT [[51](#bib.bib51)], DeepSRDCF [[52](#bib.bib52)], FCNT [[53](#bib.bib53)],
    CNN-SVM [[54](#bib.bib54)], DPST [[55](#bib.bib55)], CCOT [[56](#bib.bib56)],
    GOTURN [[57](#bib.bib57)], SiamFC [[58](#bib.bib58)], SINT [[59](#bib.bib59)],
    MDNet [[60](#bib.bib60)], HDT [[61](#bib.bib61)], STCT [[62](#bib.bib62)], RPNT
    [[63](#bib.bib63)], DeepTrack [[64](#bib.bib64), [65](#bib.bib65)], CNT [[66](#bib.bib66)],
    CF-CNN [[67](#bib.bib67)], TCNN [[68](#bib.bib68)], RDLT [[69](#bib.bib69)], PTAV
    [[70](#bib.bib70), [71](#bib.bib71)], CREST [[72](#bib.bib72)], UCT/UCT-Lite [[73](#bib.bib73)],
    DSiam/DSiamM [[74](#bib.bib74)], TSN [[75](#bib.bib75)], WECO [[76](#bib.bib76)],
    RFL [[77](#bib.bib77)], IBCCF [[78](#bib.bib78)], DTO [[79](#bib.bib79)]], SRT
    [[80](#bib.bib80)], R-FCSN [[81](#bib.bib81)], GNET [[82](#bib.bib82)], LST [[83](#bib.bib83)],
    VRCPF [[84](#bib.bib84)], DCPF [[85](#bib.bib85)], CFNet [[86](#bib.bib86)], ECO
    [[87](#bib.bib87)], DeepCSRDCF [[88](#bib.bib88)], MCPF [[89](#bib.bib89)], BranchOut
    [[90](#bib.bib90)], DeepLMCF [[91](#bib.bib91)], Obli-RaFT [[92](#bib.bib92)],
    ACFN [[93](#bib.bib93)], SANet [[94](#bib.bib94)], DCFNet/DCFNet2 [[95](#bib.bib95)],
    DET [[96](#bib.bib96)], DRN [[97](#bib.bib97)], DNT [[98](#bib.bib98)], STSGS
    [[99](#bib.bib99)], TripletLoss [[100](#bib.bib100)], DSLT [[101](#bib.bib101)],
    UPDT [[102](#bib.bib102)], ACT [[103](#bib.bib103)], DaSiamRPN [[104](#bib.bib104)],
    RT-MDNet [[105](#bib.bib105)], StructSiam [[106](#bib.bib106)], MMLT [[107](#bib.bib107)],
    CPT [[108](#bib.bib108)], STP [[109](#bib.bib109)], Siam-MCF [[110](#bib.bib110)],
    Siam-BM [[111](#bib.bib111)], WAEF [[112](#bib.bib112)], TRACA [[113](#bib.bib113)],
    VITAL [[114](#bib.bib114)], DeepSTRCF [[115](#bib.bib115)], SiamRPN [[116](#bib.bib116)],
    SA-Siam [[117](#bib.bib117)], FlowTrack [[118](#bib.bib118)], DRT [[119](#bib.bib119)],
    LSART [[120](#bib.bib120)], RASNet [[121](#bib.bib121)], MCCT [[122](#bib.bib122)],
    DCPF2 [[123](#bib.bib123)], VDSR-SRT [[124](#bib.bib124)], FCSFN [[125](#bib.bib125)],
    FRPN2T-Siam [[126](#bib.bib126)], FMFT [[127](#bib.bib127)], IMLCF [[128](#bib.bib128)],
    TGGAN [[129](#bib.bib129)], DAT [[130](#bib.bib130)], DCTN [[131](#bib.bib131)],
    FPRNet [[132](#bib.bib132)], HCFTs [[133](#bib.bib133)], adaDDCF [[134](#bib.bib134)],
    YCNN [[135](#bib.bib135)], DeepHPFT [[136](#bib.bib136)], CFCF [[137](#bib.bib137)],
    CFSRL [[138](#bib.bib138)], P2T [[139](#bib.bib139)], DCDCF [[140](#bib.bib140)],
    FICFNet [[141](#bib.bib141)], LCTdeep [[142](#bib.bib142)], HSTC [[143](#bib.bib143)],
    DeepFWDCF [[144](#bib.bib144)], CF-FCSiam [[145](#bib.bib145)], MGNet [[146](#bib.bib146)],
    ORHF [[147](#bib.bib147)], ASRCF [[148](#bib.bib148)], ATOM [[149](#bib.bib149)],
    C-RPN [[150](#bib.bib150)], GCT [[151](#bib.bib151)], RPCF [[152](#bib.bib152)],
    SPM [[153](#bib.bib153)], SiamDW [[154](#bib.bib154)], SiamMask [[155](#bib.bib155)],
    SiamRPN++ [[156](#bib.bib156)], TADT [[157](#bib.bib157)], UDT [[158](#bib.bib158)],
    DiMP [[159](#bib.bib159)], ADT [[160](#bib.bib160)], CODA [[161](#bib.bib161)],
    DRRL [[162](#bib.bib162)], SMART [[163](#bib.bib163)], MRCNN [[164](#bib.bib164)],
    MM [[165](#bib.bib165)], MTHCF [[166](#bib.bib166)], AEPCF [[167](#bib.bib167)],
    IMM-DFT [[168](#bib.bib168)], TAAT [[169](#bib.bib169)], DeepTACF [[170](#bib.bib170)],
    MAM [[171](#bib.bib171)], ADNet [[172](#bib.bib172), [173](#bib.bib173)], C2FT
    [[174](#bib.bib174)], DRL-IS [[175](#bib.bib175)], DRLT [[176](#bib.bib176)],
    EAST [[177](#bib.bib177)], HP [[178](#bib.bib178)], P-Track [[179](#bib.bib179)],
    RDT [[180](#bib.bib180)], SINT++ [[181](#bib.bib181)], Meta-Tracker [[182](#bib.bib182)],
    CRVFL [[183](#bib.bib183)], VTCNN [[184](#bib.bib184)], BGBDT [[185](#bib.bib185)],
    GFS-DCF [[186](#bib.bib186)], GradNet [[187](#bib.bib187)], MLT [[188](#bib.bib188)],
    UpdateNet [[189](#bib.bib189)], CGACD [[190](#bib.bib190)], CSA [[191](#bib.bib191)],
    D3S [[192](#bib.bib192)], OSAA [[193](#bib.bib193)], PrDiMP [[194](#bib.bib194)],
    RLS [[195](#bib.bib195)], ROAM [[196](#bib.bib196)], SiamAttn [[197](#bib.bib197)],
    SiamBAN [[198](#bib.bib198)], SiamCAR [[199](#bib.bib199)], SiamRCNN [[200](#bib.bib200)],
    TMAML [[201](#bib.bib201)], FGTrack [[202](#bib.bib202)], DHT [[203](#bib.bib203)],
    MLCFT [[204](#bib.bib204)], DSNet [[205](#bib.bib205)], BEVT [[206](#bib.bib206)],
    CRAC [[207](#bib.bib207)], KAOT [[208](#bib.bib208), [209](#bib.bib209)], MKCT
    [[210](#bib.bib210)], SASR [[211](#bib.bib211)], COMET [[212](#bib.bib212)], FGLT
    [[213](#bib.bib213)], GlobalTrack [[214](#bib.bib214)], i-Siam [[215](#bib.bib215)],
    LRVN [[216](#bib.bib216)], MetaUpdater [[217](#bib.bib217)], SPLT [[218](#bib.bib218)].'
  prefs: []
  type: TYPE_NORMAL
- en: According to the network architecture, these trackers are classified into CNN-,
    SNN-, RNN-, GAN-, and custom-based (i.e., AE- & reinforcement learning (RL)-based,
    and combined) networks. It indicates the popularity of different approaches, which
    also their problems and proposed solutions are studied in this work. From the
    network exploitation, the methods are categorized into the exploitation of deep
    off-the-shelf features and deep features for visual tracking (similar to FENs
    & EENs in [[49](#bib.bib49)]). However, in this work, the detailed characteristics
    of various trackers are investigated, such as backbone networks, exploited layers,
    training datasets, objective functions, tracking speeds, extracted features, network
    outputs, CPU/GPU implementations, programming languages, and DL framework. From
    the network training perspective, this work separately studies deep off-the-shelf
    features and deep features for visual tracking since deep off-the-shelf features
    (extracted from FENs) are mostly pre-trained on the ImageNet for object recognition
    tasks. Besides, the end-to-end training for visual tracking purposes is categorized
    into exploiting offline training, online training, or both. Furthermore, meta-learning
    based visual trackers are investigated, which are recently employed to adapt visual
    trackers to unseen targets fast. Moreover, this work presents all the details
    about backbone networks, offline & online training datasets, strategies to avoid
    over-fitting, data augmentations, and many more. From exploiting the advantages
    of correlation filters, the trackers are also classified as the methods based
    on DCF and the ones that employ end-to-end networks, which take advantage of the
    online learning efficiency of DCFs & the discriminative power of CNN features.
    Next, DL-based trackers are classified based on their application for aerial-view
    tracking, long-term tracking, or online tracking. Finally, this work comprehensively
    analyses different aspects of extensive state-of-the-art trackers on seven benchmark
    datasets, namely OTB2013 [[219](#bib.bib219)], OTB2015 [[220](#bib.bib220)], VOT2018
    [[39](#bib.bib39)], LaSOT [[221](#bib.bib221)], UAV123 [[222](#bib.bib222)], UAVDT
    [[223](#bib.bib223)], and VisDrone2019-test-dev [[224](#bib.bib224)].
  prefs: []
  type: TYPE_NORMAL
- en: I-A Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main contributions are summarized as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 1) State-of-the-art DL-based visual trackers are categorized based on the architecture
    (i.e., CNN, SNN, RNN, GAN, and custom networks), network exploitation (i.e., off-the-shelf
    deep features and deep features for visual tracking), network training for visual
    tracking (i.e., only offline training, only online training, both offline & online
    training, meta-learning), network objective (i.e., regression-based, classification-based,
    and both classification & regression-based), exploitation of correlation filter
    advantages (i.e., DCF framework and utilizing correlation filter/layer/function),
    aerial-view tracking, long-term tracking, and online tracking. Such a study covering
    all of these aspects in the detailed categorization of visual tracking methods
    has not been previously presented.
  prefs: []
  type: TYPE_NORMAL
- en: 2) The main issues and proposed solutions of DL-based trackers to tackle visual
    tracking challenges are presented. This classification provides proper insight
    into designing visual trackers.
  prefs: []
  type: TYPE_NORMAL
- en: 3) The well-known single-object visual tracking datasets (i.e., short-term,
    long-term, aerial view) are completely compared based on their fundamental characteristics
    (e.g., the number of videos, frames, classes/clusters, sequence attributes, absent
    labels, and overlap with other datasets). These benchmark datasets include OTB2013
    [[219](#bib.bib219)], OTB2015 [[220](#bib.bib220)], VOT [[34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)],
    ALOV [[42](#bib.bib42)], TC128 [[225](#bib.bib225)], UAV123 [[222](#bib.bib222)],
    NUS-PRO [[226](#bib.bib226)], NfS [[227](#bib.bib227)], DTB [[228](#bib.bib228)],
    TrackingNet [[229](#bib.bib229)], OxUvA [[230](#bib.bib230)], BUAA-PRO [[231](#bib.bib231)],
    GOT10k [[232](#bib.bib232)], LaSOT [[221](#bib.bib221)], UAV20L [[222](#bib.bib222)],
    TinyTLP/TLPattr [[233](#bib.bib233)], TLP [[233](#bib.bib233)], TracKlinic [[234](#bib.bib234)],
    UAVDT [[223](#bib.bib223)], LTB35 [[235](#bib.bib235)], VisDrone [[236](#bib.bib236),
    [224](#bib.bib224)], VisDrone2019L [[224](#bib.bib224)], and Small-90/Small-112
    [[237](#bib.bib237)].
  prefs: []
  type: TYPE_NORMAL
- en: 4) Finally, extensive experimental evaluations are performed on a wide variety
    of tracking datasets, namely OTB2013 [[219](#bib.bib219)], OTB2015 [[220](#bib.bib220)],
    VOT2018 [[39](#bib.bib39)], LaSOT [[221](#bib.bib221)], UAV123 [[222](#bib.bib222)],
    UAVDT [[223](#bib.bib223)], and VisDrone2019 [[224](#bib.bib224)], and the state-of-the-art
    visual trackers are analyzed based on different aspects. Moreover, this work specifies
    the most challenging visual attributes for the VOT2018 dataset and OTB2015, LaSOT,
    UAV123, UAVDT, and VisDrone2019 datasets. By doing so, the most primary challenges
    of each dataset for recent trackers are specified.
  prefs: []
  type: TYPE_NORMAL
- en: According to the comparisons, the following remarks are concluded.
  prefs: []
  type: TYPE_NORMAL
- en: 1) The Siamese-based networks are the most promising deep architectures due
    to their satisfactory balance between performance and efficiency for visual tracking.
    Moreover, some methods recently attempt to exploit the advantages of RL & GAN
    approaches to refine their decision-making and alleviate the lack of training
    data. Based on these advantages, recent trackers aim to design custom neural networks
    to fully exploit scene information.
  prefs: []
  type: TYPE_NORMAL
- en: 2) The offline end-to-end learning of deep features appropriately transfers
    pre-trained generic features to visual tracking task. Although conventional online
    training of DNN increases the computational complexity such that most of these
    methods are not suitable for real-time applications, it considerably helps visual
    trackers to adapt with significant appearance variation, prevent visual distractors,
    and improve the performance of visual trackers. Exploiting meta-learning approaches
    have provided significant advances to the online adaptation of visual trackers.
    Therefore, both offline and (efficient) online training procedures result in promising
    tracking performances.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Leveraging deeper and wider backbone networks improves the discriminative
    power of distinguishing the target from its background. Pre-trained networks (e.g.,
    ResNet [[32](#bib.bib32)]) are sub-optimal, and tracking performance can be remarkably
    improved by training backbone networks for visual tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 4) The best trackers exploit both regression & classification objective functions
    to distinguish the target from the background and find the tightest BB for target
    localization. These objectives are complementary such that the regression function
    has the role of auxiliary supervision on the classification one. Recently, video
    object segmentation (VOS) approaches are integrated into visual trackers for representing
    targets by segmentation masks.
  prefs: []
  type: TYPE_NORMAL
- en: 5) The exploitation of different features enhances the robustness of the target
    model. For instance, most of the DCF-based methods fuse the deep off-the-shelf
    features and hand-crafted features (e.g., HOG & CN) for this reason. Also, the
    exploitation of complementary features, such as temporal or contextual information,
    has led to more robust features in challenging scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 6) The most challenging attributes for DL-based visual tracking methods are
    occlusion, out-of-view, fast motion, aspect ratio change, and similar objects.
    Moreover, visual distractors with similar semantics may result in the drift problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of this paper is as follows. Section [II](#S2 "II Deep Visual Tracking
    Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey") introduces
    our taxonomy of deep visual trackers. The visual tracking benchmark datasets and
    evaluation metrics are compared in Section [III](#S3 "III Visual Tracking Benchmark
    Datasets ‣ Deep Learning for Visual Tracking: A Comprehensive Survey"). Experimental
    comparisons of the state-of-the-art visual tracking methods are performed in Section
    [IV](#S4 "IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey"). Finally, Section [V](#S5 "V Conclusion and Future Directions ‣ Deep
    Learning for Visual Tracking: A Comprehensive Survey") summarizes the conclusions
    and future directions.'
  prefs: []
  type: TYPE_NORMAL
- en: II Deep Visual Tracking Taxonomy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generally, three major components of: i) target representation/information,
    ii) training process, and iii) learning procedure play important roles in designing
    visual tracking methods. Most DL-based trackers aim to improve a target representation
    by utilizing/fusing deep hierarchical features, exploiting contextual/motion information,
    and select more discriminative/robust deep features. To effectively train DNNs
    for visual tracking systems, general motivations can be classified into employing
    various training schemes (e.g., network pre-training, online training (also meta-learning),
    or both) and handling training problems (e.g., lacking training samples, over-fitting,
    or computational complexity). Unsupervised training is another recent scheme to
    use abundant unlabeled samples, which can be performed by clustering the samples
    according to contextual information, mapping training data to a manifold space,
    or exploiting consistency-based objective function. Finally, the primary motivations
    regarding learning procedures are online update schemes, scale/aspect ratio estimation,
    search strategies, and long-term memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6e2db2142b5a4b780a13f4a335b9de2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Taxonomy of DL-based visual tracking methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following, DL-based methods are comprehensively categorized based on
    nine aspects, and the main motivations and contributions of trackers are classified.
    Fig. [3](#S2.F3 "Figure 3 ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for
    Visual Tracking: A Comprehensive Survey") presents the proposed taxonomy of DL-based
    visual trackers, including network architecture, network exploitation, network
    training for visual tracking purposes, network objective, network output, exploitation
    of correlation filter advantages, aerial-view tracking, long-term tracking, and
    online tracking. Moreover, DL-based trackers are compared in detail regarding
    the pre-trained networks, backbone networks, exploited layers, types of deep features,
    the fusions of hand-crafted & deep features, training datasets, tracking outputs,
    tracking speeds, hardware implementation details, programming languages, and DL
    frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: II-A Network Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although CNNs have been extensively adopted for visual tracking, other architectures
    also have been mainly developed to improve the efficiency and robustness of visual
    trackers in recent years. Accordingly, the proposed taxonomy consists of the CNN-,
    SNN-, GAN-, RNN-, and custom network-based visual trackers.
  prefs: []
  type: TYPE_NORMAL
- en: CNN-based trackers were the first to provide powerful representations of a target
    by hierarchical processing of two-dimensional frames, independently. However,
    conventional CNNs have inherent limitations, such as training on large supervised
    datasets, ignoring temporal dependencies, and computational complexities for online
    adaptation. As an alternative approach, SNN-based trackers measure the similarity
    between the target exemplar and the search region to overcome the limitations.
    Generally, SNNs employ CNN layers/blocks/networks in two or more branches for
    similarity learning purposes and run (near/over) real-time speed. However, online
    adaptation and handling challenges like occlusion are still under investigation.
    Architectures such as RNNs and GANs have been limitedly studied for visual tracking.
    In general, RNNs are used to capture temporal information among video frames,
    but they have limitations in their stability and long-term learning dependencies.
    GANs comprise generator & discriminator sub-networks, which can provide the possibility
    to address some limitations. For instance, competing for these networks can help
    trackers handle scarce positive samples, although there are some barriers to training
    and generalization of GANs. Finally, recent custom networks include various architectures
    to strengthen learning features and reduce computational complexity. In the following,
    the primary contributions of DL-based visual trackers are summarized.
  prefs: []
  type: TYPE_NORMAL
- en: II-A1 Convolutional Neural Network (CNN)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Motivated by CNN breakthroughs in computer vision and their attractive advantages
    (e.g., parameter sharing, sparse interactions, and dominant representations),
    a wide range of CNN-based trackers have been proposed. The main motivations are
    presented as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robust target representation: Providing powerful representations of targets
    is the primary advantage of employing CNNs for visual tracking. To learn robust
    target models, the contributions can be classified into: i) offline training of
    CNNs on large-scale visual tracking datasets [[55](#bib.bib55), [60](#bib.bib60),
    [73](#bib.bib73), [82](#bib.bib82), [90](#bib.bib90), [93](#bib.bib93), [94](#bib.bib94),
    [97](#bib.bib97), [105](#bib.bib105), [109](#bib.bib109), [128](#bib.bib128),
    [130](#bib.bib130), [135](#bib.bib135), [137](#bib.bib137), [146](#bib.bib146),
    [161](#bib.bib161), [164](#bib.bib164), [165](#bib.bib165), [169](#bib.bib169)],
    ii) designing specific CNNs instead of employing pre-trained models [[55](#bib.bib55),
    [60](#bib.bib60), [62](#bib.bib62), [64](#bib.bib64), [66](#bib.bib66), [68](#bib.bib68),
    [69](#bib.bib69), [73](#bib.bib73), [75](#bib.bib75), [82](#bib.bib82), [90](#bib.bib90),
    [93](#bib.bib93), [94](#bib.bib94), [97](#bib.bib97), [98](#bib.bib98), [101](#bib.bib101),
    [105](#bib.bib105), [109](#bib.bib109), [120](#bib.bib120), [128](#bib.bib128),
    [130](#bib.bib130), [134](#bib.bib134), [135](#bib.bib135), [137](#bib.bib137),
    [139](#bib.bib139), [143](#bib.bib143), [146](#bib.bib146), [161](#bib.bib161),
    [163](#bib.bib163), [164](#bib.bib164), [165](#bib.bib165), [167](#bib.bib167),
    [169](#bib.bib169), [205](#bib.bib205)], iii) constructing multiple target models
    to capture varieties of target appearances [[68](#bib.bib68), [109](#bib.bib109),
    [120](#bib.bib120), [122](#bib.bib122), [123](#bib.bib123), [136](#bib.bib136),
    [139](#bib.bib139), [168](#bib.bib168), [210](#bib.bib210)], iv) incorporating
    spatial and temporal information to improve model generalization [[72](#bib.bib72),
    [75](#bib.bib75), [99](#bib.bib99), [112](#bib.bib112), [115](#bib.bib115), [130](#bib.bib130),
    [144](#bib.bib144), [146](#bib.bib146), [208](#bib.bib208), [209](#bib.bib209)],
    v) fusion of different deep features to exploit complementary spatial and semantic
    information [[56](#bib.bib56), [94](#bib.bib94), [101](#bib.bib101), [102](#bib.bib102),
    [128](#bib.bib128), [205](#bib.bib205), [204](#bib.bib204), [206](#bib.bib206)],
    vi) learning particular target models such as relative model [[97](#bib.bib97)]
    or part-based models [[109](#bib.bib109), [120](#bib.bib120), [139](#bib.bib139)]
    to handle partial occlusion and deformation, vii) utilizing a two-stream network
    [[120](#bib.bib120)] to prevent over-fitting and to learn rotation information,
    and accurately estimating target aspect ratio to avoid contaminating target model
    with non-relevant information [[174](#bib.bib174)], and viii) group feature selection
    through channel & spatial dimensions to learn the structural relevance of features.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balancing training data: Based on problem definition, there is just one positive
    sample in the first frame that increases the risk of over-fitting during tracking.
    Although the background information arbitrary can be considered negative in each
    frame, target sampling based on imperfect target estimations may also lead to
    noisy/unreliable training samples. These issues dramatically affect the performance
    of visual tracking methods. To alleviate them, CNN-based trackers propose: i)
    domain adaption mechanism (i.e., transferring learned knowledge from the source
    domain to target one with insufficient samples) [[82](#bib.bib82), [161](#bib.bib161)],
    ii) various update mechanisms (e.g., periodic, stochastic, short-term, & long-term
    updates) [[98](#bib.bib98), [122](#bib.bib122), [136](#bib.bib136), [142](#bib.bib142),
    [165](#bib.bib165)], iii) convolutional Fisher discriminative analysis (FDA) for
    positive and negative sample mining [[134](#bib.bib134)], iv) multiple-branch
    CNN for online ensemble learning [[90](#bib.bib90)], v) efficient sampling strategies
    to increase the number of training samples [[167](#bib.bib167)], and vi) a recursive
    least-square estimation algorithm to provide a compromise between the discrimination
    power & update iterations during online learning [[195](#bib.bib195)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computational complexity problem: Despite the significant progress of CNNs
    in appearance representation, the CNN-based methods still suffer from high computational
    complexity. To reduce this limitation, CNN-based visual tracking methods exploit
    different solutions, namely: i) employing a straightforward CNN architecture [[184](#bib.bib184)],
    ii) disassembling a CNN into several shrunken networks [[69](#bib.bib69)], iii)
    compressing or pruning training sample space [[87](#bib.bib87), [108](#bib.bib108),
    [134](#bib.bib134), [146](#bib.bib146), [164](#bib.bib164)] or feature selection
    [[53](#bib.bib53), [147](#bib.bib147)], iv) feature computation via RoIAlign operation
    [[105](#bib.bib105)] (i.e., feature approximation via bilinear interpolation)
    or oblique random forest [[92](#bib.bib92)] for better data capturing, v) corrective
    domain adaption method [[161](#bib.bib161)], vi) lightweight structure [[64](#bib.bib64),
    [66](#bib.bib66), [163](#bib.bib163)], vii) efficient optimization processes [[91](#bib.bib91),
    [148](#bib.bib148)], viii) particle sampling strategy [[89](#bib.bib89)], ix)
    utilizing attentional mechanism [[93](#bib.bib93)], x) extending the random vector
    functional link (RVFL) network to a convolutional structure [[183](#bib.bib183)],
    and xi) exploiting advantages of correlation filters [[51](#bib.bib51), [52](#bib.bib52),
    [53](#bib.bib53), [56](#bib.bib56), [61](#bib.bib61), [67](#bib.bib67), [70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [76](#bib.bib76), [78](#bib.bib78),
    [79](#bib.bib79), [85](#bib.bib85), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89),
    [91](#bib.bib91), [93](#bib.bib93), [99](#bib.bib99), [101](#bib.bib101), [102](#bib.bib102),
    [108](#bib.bib108), [112](#bib.bib112), [115](#bib.bib115), [119](#bib.bib119),
    [120](#bib.bib120), [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124),
    [128](#bib.bib128), [133](#bib.bib133), [134](#bib.bib134), [136](#bib.bib136),
    [137](#bib.bib137), [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144),
    [148](#bib.bib148), [152](#bib.bib152), [161](#bib.bib161), [163](#bib.bib163),
    [167](#bib.bib167), [168](#bib.bib168), [170](#bib.bib170)] for efficient computations.
    Exploiting the advantages of correlation filters refers to either applying DCFs
    on pre-trained networks or combining correlation filters/layers/functions with
    end-to-end networks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II-A2 Siamese Neural Network (SNN)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SNNs are widely employed for visual trackers in the past few years. Given the
    pairs of target and search regions, these two-stream networks compute the same
    function to produce a similarity map. They mainly aim to overcome the limitations
    of pre-trained deep CNNs and take full advantage of end-to-end learning for real-time
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Discriminative target representation: The ability to construct a robust target
    model majorly relies on target representation. For achieving more discriminative
    deep features and improving target modeling, SNN-based methods propose: i) learning
    distractor-aware [[104](#bib.bib104)] or target-aware features [[157](#bib.bib157)],
    ii) fusing deep multi-level features [[125](#bib.bib125), [150](#bib.bib150)]
    or combining confidence maps [[81](#bib.bib81), [83](#bib.bib83), [117](#bib.bib117)],
    iii) utilizing different loss functions in Siamese formulation to train more effective
    filters [[100](#bib.bib100), [155](#bib.bib155), [157](#bib.bib157), [158](#bib.bib158),
    [159](#bib.bib159)], iv) leveraging different types of deep features such as context
    information [[110](#bib.bib110), [117](#bib.bib117), [151](#bib.bib151)] or temporal
    features/models [[57](#bib.bib57), [74](#bib.bib74), [118](#bib.bib118), [126](#bib.bib126),
    [151](#bib.bib151), [171](#bib.bib171)], v) full exploring of low-level spatial
    features [[125](#bib.bib125), [150](#bib.bib150)], vi) considering angle estimation
    of a target to prevent salient background objects [[111](#bib.bib111)], vii) utilizing
    multi-stage regression to refine target representation [[150](#bib.bib150)], vii)
    using the deeper and wider deep network as the backbone to increase the receptive
    field of neurons, which is equivalent to capturing the structure of the target
    [[154](#bib.bib154)], viii) employing correlation-guided attention modules to
    exploit the relationship between the template & RoI feature maps [[190](#bib.bib190)],
    ix) computing correlations between attentional features [[197](#bib.bib197)],
    x) accurately estimating the scale and aspect ratio of the target [[198](#bib.bib198)],
    xi) simultaneously learning classification & regression models [[199](#bib.bib199)],
    xii) mining hard samples in training [[200](#bib.bib200)], and xiii) employing
    skimming & perusal modules for inferring optimal target candidates [[218](#bib.bib218)].
    Finally, [[191](#bib.bib191), [193](#bib.bib193)] are performed adversarial attacks
    on SNN-based trackers to evaluate misbehaving SNN models for visual tracking scenarios.
    These methods generate slight perturbations for deceiving the trackers to finally
    investigate DL models and improve their robustness.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adapting target appearance variation: Using only offline training of the first
    generation of SNN-based trackers caused a poor generalization to unseen targets.
    To solve it, recent SNN-based trackers propose: i) online update strategies considering
    strategies to reduce the risk of over-fitting [[74](#bib.bib74), [83](#bib.bib83),
    [86](#bib.bib86), [96](#bib.bib96), [104](#bib.bib104), [145](#bib.bib145), [187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189), [216](#bib.bib216)], ii) background suppression
    [[74](#bib.bib74), [104](#bib.bib104)], iii) formulating tracking task as a one-shot
    local detection task [[104](#bib.bib104), [116](#bib.bib116)], iv) and giving
    higher weights to important feature channels or score maps [[81](#bib.bib81),
    [117](#bib.bib117), [121](#bib.bib121), [141](#bib.bib141)], and v) modeling all
    potential distractors considering their motion and interaction [[200](#bib.bib200)].
    Alternatively, the DaSiamRPN [[104](#bib.bib104)] and MMLT [[107](#bib.bib107)]
    use a local-to-global search region strategy and memory exploitation to handle
    critical challenges such as full occlusion and out-of-view and enhance local search
    strategy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balancing training data: The same as CNN-based methods, some efforts by SNN-based
    methods have been performed to address the imbalance distribution of training
    samples. The main contributions of the SNN-based methods are: i) exploiting multi-stage
    Siamese framework to stimulate hard negative sampling [[150](#bib.bib150)], ii)
    adopting sampling heuristics such as fixed foreground-to-background ratio [[150](#bib.bib150)]
    or sampling strategies such as random sampling [[104](#bib.bib104)] or flow-guided
    sampling [[126](#bib.bib126)], and iii) taking advantages of correlation filter/layer
    into Siamese framework [[70](#bib.bib70), [71](#bib.bib71), [74](#bib.bib74),
    [86](#bib.bib86), [95](#bib.bib95), [104](#bib.bib104), [116](#bib.bib116), [118](#bib.bib118),
    [121](#bib.bib121), [141](#bib.bib141), [147](#bib.bib147), [149](#bib.bib149),
    [157](#bib.bib157), [158](#bib.bib158), [166](#bib.bib166), [145](#bib.bib145)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II-A3 Recurrent Neural Network (RNN)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since visual tracking is related to both spatial and temporal information of
    video frames, RNN-based methods also consider target motion/movement. Because
    of arduous training and a considerable number of parameters, the number of RNN-based
    methods is limited. Almost all these methods try to exploit additional information
    and memory to improve target modeling. Also, the second aim of using RNN-based
    methods is to avoid fine-tuning of pre-trained CNN models, which takes a lot of
    time and is prone to over-fitting. The primary purposes of these methods can be
    classified to the spatio-temporal representation capturing [[77](#bib.bib77),
    [132](#bib.bib132), [171](#bib.bib171)], leveraging contextual information to
    handle background clutters [[132](#bib.bib132)], exploiting multi-level visual
    attention to highlight target, background suppression [[171](#bib.bib171)], and
    using convolutional long short-term memory (LSTM) as the memory unit of previous
    target appearances [[77](#bib.bib77)]. Moreover, RNN-based methods exploit pyramid
    multi-directional recurrent network [[132](#bib.bib132)] or incorporate LSTM into
    different networks [[77](#bib.bib77)] to memorize target appearance and investigate
    time dependencies. Finally, the [[132](#bib.bib132)] encodes the self-structure
    of a target to reduce tracking sensitivity related to similar distractors.
  prefs: []
  type: TYPE_NORMAL
- en: II-A4 Generative Adversarial Network (GAN)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Based on some attractive advantages, such as capturing statistical distribution
    and generating desired training samples without extensive annotated data, GANs
    have been intensively utilized in many research areas. Although GANs are usually
    hard to train and evaluate, some DL-based trackers employ GANs to enrich training
    samples and target modeling. These networks can augment positive samples in feature
    space to address the imbalance distribution of training samples [[114](#bib.bib114)].
    Also, the GAN-based methods can learn general appearance distribution to deal
    with visual tracking’s self-learning problem [[129](#bib.bib129)]. Furthermore,
    the joint optimization of regression & discriminative networks will take advantage
    of both these two tasks [[160](#bib.bib160)]. Lastly, these networks can explore
    the relations between a target with its contextual information for searching interested
    regions and transfer this information to videos with different inherits such as
    transferring from ground-view to drone-view [[207](#bib.bib207)].
  prefs: []
  type: TYPE_NORMAL
- en: II-A5 Custom Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inspired by particular deep architectures and network layers, modern DL-based
    methods have combined a wide range of networks such as AE, CNN, RNN, SNN, detection
    networks & also deep RL for visual tracking. The main motivation of custom networks
    is to compensate for ordinary trackers’ deficiencies by exploiting the advantages
    of other networks. Furthermore, meta-learning (or learning to learn) has been
    recently attracted by the visual tracking community. It aims to address few-shot
    learning problems and fast adaptation of a learner to a new task by leveraging
    accumulated experiences from similar tasks. By employing the meta-learning framework,
    different networks can learn unseen target appearances during online tracking.
    The primary motivations and contributions of custom networks are classified as
    follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robust and accurate tracking: Recent networks seek general & effective frameworks
    for better localization and BB estimation. For instance, aggregating several online
    trackers [[203](#bib.bib203)] is a way to improve tracking performance. An alternative
    is a better understanding of the target’s pose by exclusively designed target
    estimation and classification networks [[149](#bib.bib149)]. Also, meta-learning
    based networks [[159](#bib.bib159), [194](#bib.bib194)] can predict powerful target
    models inspiring by discriminative learning procedures. However, some other works
    [[185](#bib.bib185), [201](#bib.bib201)] consider the tracking task as an instance
    detection and aim to convert modern object detectors directly to a visual tracker.
    These trackers exploit class-agnostic networks, which can: i) differentiate intra-class
    samples, ii) quickly adapt to different targets by meta-learners, and iii) consider
    temporal cues. The D3S [[192](#bib.bib192)] method models a visual target from
    its segmentation mask with complementary geometric properties to improve the robustness
    of template-based trackers. The COMET [[212](#bib.bib212)] bridges the gap between
    advanced visual trackers and aerial-view ones in detecting small/tiny objects.
    It employs multi-scale feature learning and attention modules to compensate for
    the inferior performance of generic trackers in medium-/high-attitude aerial views.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computational complexity problem: As stated before, this problem limits the
    performance of online trackers in real-time applications. To control computational
    complexity, the TRACA [[113](#bib.bib113)] and AEPCF [[167](#bib.bib167)] methods
    employ AEs to compress raw conventional deep features. The EAST [[177](#bib.bib177)]
    adaptively takes either shallow features for simple frames for tracking or expensive
    deep features for challenging ones [[177](#bib.bib177)], and the TRACA [[113](#bib.bib113)],
    CFSRL [[138](#bib.bib138)], & AEPCF [[167](#bib.bib167)] exploit the DCF computation
    efficiency. An effective way to avoid high computational burden is exploiting
    meta-learning that quickly adapts pre-trained trackers on unseen targets. The
    target model of the meta-learning based trackers can be optimized in a few iterations
    [[182](#bib.bib182), [159](#bib.bib159), [194](#bib.bib194), [185](#bib.bib185),
    [188](#bib.bib188), [196](#bib.bib196), [201](#bib.bib201), [217](#bib.bib217)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model update: To maintain the stability of the target model during the tracking
    process, different update strategies have been proposed; for instance, the CFSRL
    [[138](#bib.bib138)] updates multiple models in parallel, the DRRL [[162](#bib.bib162)]
    incorporates an LSTM to exploit long-range time dependencies, and the AEPCF [[167](#bib.bib167)]
    utilizes long-term and short-term update schemes to increase tracking speed. To
    prevent the erroneous model update and drift problem, the RDT [[180](#bib.bib180)]
    has revised the visual tracking formulation to a consecutive decision-making process
    about the best target template for the next localization. Moreover, efficient
    learning of good decision policies using RL [[179](#bib.bib179)] is another technique
    to take either model update or ignore the decision. A recent alternative is employing
    meta-learning approaches for quick model adaptation. For instance, the works [[196](#bib.bib196),
    [159](#bib.bib159), [194](#bib.bib194)] use recurrent optimization processes that
    update the target model in a few gradient steps. Finally, [[217](#bib.bib217)]
    integrates sequential information (e.g., geometric, discriminative, and appearance
    cues) and exploits a meta-updater to effectively update on reliable frames.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Limited training data: The soft and non-representative training samples can
    disturb visual tracking if occlusion, blurring, and large deformation happen.
    The AEPCF [[167](#bib.bib167)] exploits a dense circular sampling scheme to prevent
    the over-fitting problem caused by limited training data. The SINT++ [[181](#bib.bib181)]
    generates positive and hard training samples by positive sample generation network
    (PSGN) and hard positive transformation network (HPTN) to make diverse and challenging
    training data. To efficiently train DNNs without a large amount of training data,
    partially labeled training samples are utilized by an action-driven deep tracker
    [[172](#bib.bib172), [173](#bib.bib173)]. The P-Track [[179](#bib.bib179)] also
    uses active decision-making to label videos interactively while learning a tracker
    when limited annotated data are available. Meta-Tracker [[182](#bib.bib182)] was
    the first attempt to exploit an offline meta-learning-based method for better
    online adaptation of visual trackers. This method can generalize the target model
    and avoid over-fitting to distractors. Besides, various pioneer trackers [[159](#bib.bib159),
    [194](#bib.bib194), [217](#bib.bib217), [201](#bib.bib201), [185](#bib.bib185),
    [188](#bib.bib188), [196](#bib.bib196)] enjoy the advantages of meta-learners
    in one/few-shot learning tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Search strategy: From the definition, visual tracking methods estimate the
    new target state in the next frame’s search region, given an initial target state
    in the first frame. The best search region selection depends on the iterative
    search strategies that usually are independent of video content and are heuristic,
    brute-force, and hand-engineered. Despite classical search strategies based on
    sliding windows, mean shift, or particle filter, the state-of-the-art DL-based
    visual trackers exploit RL-based methods to learn data-driven searching policies.
    To exhaustively explore a region of interest and select the best target candidate,
    action-driven tracking mechanisms [[172](#bib.bib172), [173](#bib.bib173)] consider
    the target context variation and actively pursues the target movement. Furthermore,
    the ACT and DRRL have proposed practical RL-based search strategies for real-time
    requirements by dynamic search [[103](#bib.bib103)] and coarse-to-fine verification
    [[162](#bib.bib162)]. Lastly, the full-image visual tracker [[214](#bib.bib214)]
    exploits a two-stage detector for localizing the target without any assumptions
    (e.g., temporal consistency of target regions).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exploiting additional information: To enhance the target model by utilizing
    motion or contextual information, the DCTN [[131](#bib.bib131)] establishes a
    two-stream network, and the SRT [[80](#bib.bib80)] adopts multi-directional RNN
    to learn further dependencies of a target during visual tracking. Also, the FGTrack
    [[202](#bib.bib202)] estimates the scale & rotation of the target and its displacement
    by the finer-grained motion information provided by optical-flow. A recurrent
    convolutional network [[176](#bib.bib176)] models previous semantic information
    and tracking proposals to encode relevant information for better localization.
    At last, DRL-IS [[175](#bib.bib175)] has introduced an Actor-Critic network to
    estimate target motion parameters efficiently.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decision making: Online decision making has principal effects on the performance
    of DL-based visual tracking methods. The state-of-the-art methods attempt to learn
    online decision making by incorporating RL into the DL-based methods instead of
    hand-designed techniques. To gain effective decision policies, the P-Track [[179](#bib.bib179)]
    ultimately exploits data-driven techniques in an active agent to decide about
    tracking, re-initializing, or updating processes. Also, the DRL-IS [[175](#bib.bib175)]
    utilizes a principled RL-based method to select sensible action based on target
    status. Also, an action-prediction network has been proposed to adjust a visual
    tracker’s continuous actions to determine the optimal hyper-parameters for learning
    the best action policies and make satisfactory decisions [[178](#bib.bib178)].
    On the other hand, the work [[194](#bib.bib194)] considers the uncertainty in
    estimating target states. By predicting a conditional probability density of the
    visual target, direct interpretations can be provided for deciding about an update
    procedure or lost target. Also, a result judgment module [[213](#bib.bib213)]
    can help short-term trackers in occlusion/out-of-view situations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II-B Network Exploitation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Roughly speaking, there are two main exploitations of DNNs for visual tracking,
    including reusing a pre-trained model on partially related datasets or exploiting
    deep features for visual tracking, which is equivalent to train DNNs for visual
    tracking tasks.
  prefs: []
  type: TYPE_NORMAL
- en: II-B1 Model Reuse or Deep Off-the-Shelf Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Exploiting deep off-the-shelf features is the simplest way to transfer the
    power of deep features into the traditional visual tracking methods. These features
    provide a generic representation of visual targets and help visual tracking methods
    to construct more robust target models. Regarding topologies, DNNs include either
    a simple multi-layer stack of non-linear layers (e.g., AlexNet [[28](#bib.bib28)],
    VGGNet [[29](#bib.bib29), [30](#bib.bib30)]) or a directed acyclic graph topology
    (e.g., GoogLeNet [[31](#bib.bib31)], ResNet [[121](#bib.bib121)], SSD [[238](#bib.bib238)],
    Siamese convolutional neural network [[239](#bib.bib239)]), which allows designing
    more complex deep architectures that include layers with multiple input/output.
    The main challenge of these trackers is how to benefit the generic representations
    effectively. Different methods employ various feature maps and models that have
    been pre-trained majorly on large-scale still images of the ImageNet dataset [[33](#bib.bib33)]
    for the object recognition task. Numerous methods have studied the properties
    of pre-trained models and explored the impact of deep features in traditional
    frameworks (see Table [I](#S2.T1 "TABLE I ‣ II-B1 Model Reuse or Deep Off-the-Shelf
    Features ‣ II-B Network Exploitation ‣ II Deep Visual Tracking Taxonomy ‣ Deep
    Learning for Visual Tracking: A Comprehensive Survey")). As a result, the DL-based
    methods have preferred simultaneous exploitation of both semantic and fine-grained
    deep features [[51](#bib.bib51), [53](#bib.bib53), [56](#bib.bib56), [133](#bib.bib133),
    [150](#bib.bib150), [240](#bib.bib240), [241](#bib.bib241), [204](#bib.bib204),
    [206](#bib.bib206)]. The fusion of deep features is also another motivation of
    these methods, which is performed by different techniques to utilize multi-resolution
    deep features [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [56](#bib.bib56),
    [61](#bib.bib61), [76](#bib.bib76), [123](#bib.bib123), [136](#bib.bib136), [168](#bib.bib168),
    [102](#bib.bib102), [145](#bib.bib145), [122](#bib.bib122), [204](#bib.bib204),
    [206](#bib.bib206)] and independent fusion of deep features with shallow ones
    at a later stage [[102](#bib.bib102)]. Exploiting motion information [[85](#bib.bib85),
    [99](#bib.bib99), [168](#bib.bib168), [242](#bib.bib242)] and selecting appropriate
    deep features for visual tracking tasks [[53](#bib.bib53), [147](#bib.bib147),
    [186](#bib.bib186)] are two other interesting motivations for DL-based methods.
    The detailed characteristics of DL-based visual trackers based on deep off-the-shelf
    features are shown in Table [I](#S2.T1 "TABLE I ‣ II-B1 Model Reuse or Deep Off-the-Shelf
    Features ‣ II-B Network Exploitation ‣ II Deep Visual Tracking Taxonomy ‣ Deep
    Learning for Visual Tracking: A Comprehensive Survey"). Needless to say, the network
    output for these methods are deep feature maps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Deep off-the-shelf features for visual tracking. The abbreviations
    are denoted as: confidence map (CM), saliency map (SM), bounding box (BB), votes
    (vt), deep appearance features (DAF), deep motion features (DMF).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Pre-trained models | Exploited layers | Pre-training data | Pre-training
    dataset(s) | Exploited features | PC (CPU, RAM, Nvidia GPU) | Language | Framework
    | Speed (fps) | Tracking output |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DeepSRDCF [[52](#bib.bib52)] | VGG-M | Conv5 | Still images | ImageNet |
    HOG, DAF | N/A, GPU | Matlab | MatConvNet | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| CCOT [[56](#bib.bib56)] | VGG-M | Conv1, Conv5 | Still images | ImageNet
    | HOG, CN, DAF | N/A, GPU | Matlab | MatConvNet |  1 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| ECO [[87](#bib.bib87)] | VGG-M | Conv1, Conv5 | Still images | ImageNet |
    HOG, CN, DAF | N/A, GPU | Matlab | MatConvNet | 8 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DeepCSRDCF [[88](#bib.bib88)] | VGG-M | N/A | Still images | ImageNet | HOG,
    CN, DAF | Intel I7 3.4GHz CPU, GPU | Matlab | MatConvNet | 13 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| SASR [[211](#bib.bib211)] | VGG-M | Conv4 | Still images | ImageNet | DAF
    | Intel-8700K 3.7GHz CPU, 32GB RAM, Quadro P2000 GPU | Matlab | MatConvNet | 3.84
    | CM |'
  prefs: []
  type: TYPE_TB
- en: '| KAOT [[208](#bib.bib208), [209](#bib.bib209)] | VGG-M | Conv3 | Still images
    | ImageNet | DAF | Intel I7-8700K 3.7GHz CPU, 32GB RAM, RTX 2080 GPU | Matlab
    | MatConvNet | 14.1 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| MLCFT [[204](#bib.bib204)] | VGG-M | Conv-1, Conv-3, Conv-5 | Still images
    | ImageNet | DAF | Intel I7 3770K 3.5 CPU, 8GB RAM, GTX 960 GPU | Matlab | MatConvNet
    | 16.1 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| UPDT [[102](#bib.bib102)] | VGG-M/ GoogLeNet/ ResNet-50 | N/A | Still images
    | ImageNet | HOG, CN, DAF | N/A | Matlab | MatConvNet | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| WAEF [[112](#bib.bib112)] | VGG-M | Conv1, Conv5 | Still images | ImageNet
    | HOG, CN, DAF | Intel Xeon(R) 3.20 GHz CPU, 44GB RAM, GTX 1080Ti | Matlab | MatConvNet
    | 0.62 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DeepSTRCF [[115](#bib.bib115)] | VGG-M | Conv3 | Still images | ImageNet
    | HOG, CN, DAF | Intel I7-7700 CPU, 32GB RAM, GTX 1070 GPU | Matlab | MatConvNet
    | 24.3 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DRT [[119](#bib.bib119)] | VGG-M, VGG-16 | Conv1, Conv4-3 | Still images
    | ImageNet | HOG, CN, DAF | N/A, GPU | Matlab | Caffe | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| WECO [[76](#bib.bib76)] | VGG-M | Conv1, Conv5 | Still images | ImageNet
    | DAF | Intel Xeon(R) 2.60GHz CPU, GTX 1080 GPU | Matlab | MatConvNet | 4 | CM
    |'
  prefs: []
  type: TYPE_TB
- en: '| VDSR-SRT [[124](#bib.bib124)] | VGG-M | Conv1, Conv5 | Still images | ImageNet
    | HOG, DAF | Intel I7-6700k 4.00GHz CPU, 16GB RAM, GTX 1070 GPU | Matlab | MatConvNet
    | 13.5 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| ASRCF [[148](#bib.bib148)] | VGG-M, VGG-16 | Norm1, Conv4-3 | Still images
    | ImageNet | HOG, DAF | Intel I7-8700 CPU, 32GB RAM, GTX 1080Ti GPU | Matlab |
    MatConvNet | 28 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| RPCF [[152](#bib.bib152)] | VGG-M | Conv1, Conv5 | Still images | ImageNet
    | HOG, CN, DAF | Intel I7-4790K CPU, GTX 1080 GPU | Matlab | MatConvNet | 5 |
    CM |'
  prefs: []
  type: TYPE_TB
- en: '| DeepTACF [[170](#bib.bib170)] | VGG-M | Conv1 | Still images | ImageNet |
    HOG, DAF | Intel I7-6700 3.40GHz CPU, GTX Titan GPU | Matlab | MatConvNet | N/A
    | CM |'
  prefs: []
  type: TYPE_TB
- en: '| FCNT [[53](#bib.bib53)] | VGG-16 | Conv4-3, Conv5-3 | Still images | ImageNet
    | DAF | 3.4GHz CPU, GTX Titan GPU | Matlab | Caffe | 3 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| CREST [[72](#bib.bib72)] | VGG-16 | Conv4-3 | Still images | ImageNet | DAF
    | Intel I7 3.4GHz CPU, GTX Titan Black GPU | Matlab | MatConvNet | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DTO [[79](#bib.bib79)] | VGG-16, SSD | Conv3-3, Conv4-3, Conv5-3 | Still
    images | ImageNet | DAF | Intel I7-4770K CPU, 32G RAM, GTX 1070 GPU | Matlab |
    Caffe | N/A | CM, BB |'
  prefs: []
  type: TYPE_TB
- en: '| VRCPF [[84](#bib.bib84)] | VGG-16, Faster R-CNN | N/A | Still images | ImageNet,
    COCO | DAF | N/A | N/A | N/A | N/A | BB |'
  prefs: []
  type: TYPE_TB
- en: '| Obli-RaFT [[92](#bib.bib92)] | VGG-16 | Conv4-3, Conv5-3 | Still images |
    ImageNet | DAF | Intel I7-3770 3.40GHz CPU, 2 GTX Titan X GPUs | Matlab | Caffe
    | 2 | VT |'
  prefs: []
  type: TYPE_TB
- en: '| CPT [[108](#bib.bib108)] | VGG-16 | Conv5-1, Conv5-3 | Still images | ImageNet
    | HOG, CN, DAF | Intel I7-7800X CPU, 16GB RAM, GTX 1080Ti GPU | Matlab | MatConvNet
    | 14 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DeepHPFT [[136](#bib.bib136)] | VGG-16, VGG-19, and GoogLeNet | Conv5-3,
    Conv5-4, and icp6-out | Still images | ImageNet | HOG, CN, DAF | Intel Xeon 2.4GHz
    CPU, 256 GB RAM, GTX Titan XP GPU | Matlab | MatConvNet | 4 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DeepFWDCF [[144](#bib.bib144)] | VGG-16 | Conv4-3 | Still images | ImageNet
    | DAF | N/A, GTX 1080Ti GPU | Matlab | MatConvNet | 2.7 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| MMLT [[107](#bib.bib107)] | VGGNet, Fully-convolutional Siamese network |
    Conv5 | Still images, Video frames | ImageNet, ILSVRC-VID | DAF | Intel I7-4770
    3.40GHz CPU, 11GB RAM, GTX 1080Ti | Matlab | N/A | 6.15 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| MKCT [[210](#bib.bib210)] | VGGNet | Conv3-4 | Still images | ImageNet |
    DAF | Intel I7 3.7GHz CPU, 32GB RAM, Quadro 2000 GPU | Matlab | MatConvNet | 9.4
    | CM |'
  prefs: []
  type: TYPE_TB
- en: '| BEVT [[206](#bib.bib206)] | VGGNet | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel I7-8700K 3.7GHz CPU, 48GB RAM, Quadro P2000 GPU | Matlab
    | MatConvNet | 0.6 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| HCFT [[51](#bib.bib51)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel I7-4770 3.40GHz CPU, 32 GB RAM, GTX Titan GPU | Matlab
    | MatConvNet | 10.4 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| HDT [[61](#bib.bib61)] | VGG-19 | Conv4-2, Conv4-3, Conv4-4, Conv5-2, Conv5-3,
    Conv5-4 | Still images | ImageNet | DAF | Intel I7-4790K 4.00GHz CPU, 16GB RAM,
    GTX 780Ti GPU | Matlab | MatConvNet |  1 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| IBCCF [[78](#bib.bib78)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel Xeon(R) 3.3GHz CPU, 32GB RAM, GTX 1080 GPU | Matlab |
    MatConvNet | 1.25 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DCPF [[85](#bib.bib85)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | N/A | N/A | N/A | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| MCPF [[89](#bib.bib89)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel 3.10GHz CPU, 256 GB RAM, GTX Titan X GPU | Matlab | MatConvNet
    | 0.5 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DeepLMCF [[91](#bib.bib91)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still
    images | ImageNet | DAF | Intel 3.60GHz CPU, Tesla K40 GPU | Matlab | MatConvNet
    | 10 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| STSGS [[99](#bib.bib99)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF, DMF | Intel I7 3.20GHz CPU, 8 GB RAM | Matlab | Caffe | 4 5
    | CM |'
  prefs: []
  type: TYPE_TB
- en: '| MCCT [[122](#bib.bib122)] | VGG-19 | Conv4-4, Conv5-4 | Still images | ImageNet
    | DAF | Intel I7-4790K 4.00GHz CPU, 16GB RAM, GTX 1080Ti GPU | Matlab | MatConvNet
    | 8 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DCPF2 [[123](#bib.bib123)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | N/A | N/A | N/A | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| HCFTs [[133](#bib.bib133)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel I7-4770 3.40GHz CPU, 32GB RAM, GTX Titan GPU | Matlab
    | MatConvNet | 6.7 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| LCTdeep [[142](#bib.bib142)] | VGG-19 | Conv5-4 | Still images | ImageNet
    | DAF | Intel I7-4770 3.40GHz CPU, 32GB RAM, GPU | Matlab | MatConvNet | 13.8
    | CM |'
  prefs: []
  type: TYPE_TB
- en: '| CF-CNN [[67](#bib.bib67)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel I7-4770 3.40GHz CPU, 32GB RAM, GTX Titan GPU | Matlab
    | MatConvNet | 12.3 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| ORHF [[147](#bib.bib147)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | HOG, DAF | Intel I7-4770K 3.50GHz CPU, 24GB RAM, N/A | Matlab | N/A
    | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| IMM-DFT [[168](#bib.bib168)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still
    images | ImageNet | DAF | Intel I5-4590 3.30GHz CPU, 16GB RAM, GTX Titan X GPU
    | Matlab | MatConvNet | 10 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| CNN-SVM [[54](#bib.bib54)] | R-CNN | First fully-connected layer | Still
    images | ImageNet | DAF | N/A | N/A | Caffe | N/A | SM |'
  prefs: []
  type: TYPE_TB
- en: '| RPNT [[63](#bib.bib63)] | Object proposal network | N/A | Still images |
    ImageNet, PASCAL VOC | DAF | N/A | C/C++ | N/A | 3.8 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| CF-FCSiam [[145](#bib.bib145)] | Fully-convolutional Siamese network | N/A
    | Video frames | ILSVRC-VID | HOG, DAF | Intel I7-6700K 4.00GHz CPU, GTX Titan
    GPU | Matlab | MatConvNet |  33 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| TADT [[157](#bib.bib157)] | Siamese matching network | Conv4-1, Conv4-3 |
    Still images | ImageNet | DAF | Intel I7 3.60GHz CPU, 32GB RAM, GTX 1080 GPU |
    Matlab | MatConvNet | 33.7 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| GFS-DCF [[186](#bib.bib186)] | ResNet-50 | Res4x | Still images | ImageNet
    | DAF | Intel Xeon E5-2637v3 CPU, N/A, GTX Titan X GPU | Matlab | MatConvNet |
    8 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DHT [[203](#bib.bib203)] | Various networks | Various layers | Still images,
    Video frames | Various datasets | DAF | N/A | N/A | N/A | 12 | BB |'
  prefs: []
  type: TYPE_TB
- en: II-B2 Deep Features for Visual Tracking Purposes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One trending part of recent trackers is how to design and train DNNs for visual
    tracking. Using deep off-the-shelf features limits the tracking performance due
    to inconsistency among the objectives of different tasks. Also, offline learned
    deep features may not capture target variations and tend to over-fit on initial
    target templates. Hence, DNNs are trained on large-scale datasets to specialize
    the networks for visual tracking purposes. Besides, applying a fine-tuning process
    during visual tracking can adjust some network parameters and produce more refined
    target representations. However, the fine-tuning process is time-consuming and
    prone to over-fitting because of a heuristically fixed iteration number and limited
    available training data. As shown in Table [II](#S2.T2 "TABLE II ‣ II-C2 Only
    Offline Training ‣ II-C Network Training ‣ II Deep Visual Tracking Taxonomy ‣
    Deep Learning for Visual Tracking: A Comprehensive Survey") to Table [IV](#S2.T4
    "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey"),
    these DL-based methods usually train a pre-trained network (i.e., backbone network)
    by offline training, online training, or both.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C Network Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The state-of-the-art DL-based visual tracking methods mostly exploit end-to-end
    learning with train/re-train a DNN by applying gradient-based optimization algorithms.
    However, these methods have differences according to their offline network training,
    online fine-tuning, computational complexity, dealing with lack of training data,
    addressing the overfitting problem, and exploiting unlabeled samples by unsupervised
    training. The network training sections in the previous review papers [[47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49)] consider both FENs and EENs, although the
    FENs were only pre-trained for other tasks, and there is no training procedure
    for visual tracking. In this survey, DL-based methods are categorized into only
    offline pre-training, only online training, and both offline and online training
    for visual tracking purposes. The training details of these methods are shown
    in Table [II](#S2.T2 "TABLE II ‣ II-C2 Only Offline Training ‣ II-C Network Training
    ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey") to Table [IV](#S2.T4 "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network
    Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: II-C1 Training Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Visual trackers employ diverse datasets to train their networks. These datasets
    are generally categorized into general-purpose & tracking datasets (see Table [I](#S2.T1
    "TABLE I ‣ II-B1 Model Reuse or Deep Off-the-Shelf Features ‣ II-B Network Exploitation
    ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey") to Table [IV](#S2.T4 "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network
    Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")). A general-purpose dataset refers to a dataset from
    other tasks that provide desirable representations of different targets, e.g.,
    object recognition or segmentation. It can include numerous datasets such as ImageNet
    [[33](#bib.bib33)], YouTube-VOS [[243](#bib.bib243)], YouTube-BoundingBoxes [[244](#bib.bib244)],
    KITTI [[245](#bib.bib245)], etc. That is, these datasets are used as auxiliary
    datasets in training procedures. However, tracking datasets are also utilized
    for training visual tracking networks. For instance, large-scale tracking datasets
    such as LaSOT [[221](#bib.bib221)] & TrackingNet [[229](#bib.bib229)] are explored
    in recent years. By exploring tracking datasets, the networks are trained on task-specific
    scenarios in the presence of challenging tracking attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C2 Only Offline Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Most of the DL-based visual tracking methods only pre-train their networks
    to provide a generic target representation and reduce the high risk of over-fitting
    due to imbalanced training data and fixed assumptions. To adjust the learned filter
    weights for visual tracking task, the specialized networks are trained on large-scale
    data to exploit better representation and achieve acceptable tracking speed by
    preventing from training during visual tracking (see Table [II](#S2.T2 "TABLE
    II ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual Tracking
    Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Only offline training for visual tracking. The abbreviations are
    denoted as: confidence map (CM), saliency map (SM), bounding box (BB), object
    score (OS), feature maps (FM), segmentation mask (SGM), rotated bounding box (RBB),
    action (AC), deep appearance features (DAF), deep motion features (DMF), deeo
    optical flow (DOF).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Backbone network | Offline training dataset(s) | Exploited features
    | PC (CPU, RAM, Nvidia GPU) | Language | Framework | Speed (fps) | Tracking output
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GOTURN [[57](#bib.bib57)] | AlexNet | ILSVRC-DET, ALOV | DAF | N/A, GTX Titan
    X GPU | C/C++ | Caffe | 166 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| SiamFC [[58](#bib.bib58)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    I7-4790K 4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 58 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| SINT [[59](#bib.bib59)] | AlexNet, VGG-16 | ImageNet, ALOV | DAF | N/A |
    Matlab | Caffe | N/A | OS |'
  prefs: []
  type: TYPE_TB
- en: '| R-FCSN [[81](#bib.bib81)] | AlexNet | ImageNet, ILSVRC-VID | DAF | N/A, GTX
    Titan X GPU | Matlab | MatConvNet | 50.25 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| LST [[83](#bib.bib83)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel Xeon
    3.50GHz CPU, GTX Titan X GPU | Matlab | MatConvNet |  24 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| CFNet [[86](#bib.bib86)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel I7
    4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 75 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DaSiamRPN [[104](#bib.bib104)] | AlexNet | ILSVRC, YTBB, Augmented ILSVRC-DET,
    Augmented MSCOCO-DET | DAF | Intel I7 CPU, 48GB RAM, GTX Titan X GPU | Python
    | PyTorch | 160 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| StructSiam [[106](#bib.bib106)] | AlexNet | ILSVRC-VID, ALOV | DAF | Intel
    I7-4790 3.60GHz, GTX 1080 GPU | Python | TensorFlow | 45 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| Siam-BM [[111](#bib.bib111)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    Xeon 2.60GHz CPU, Tesla P100 GPU | Python | TensorFlow | 48 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| SA-Siam [[117](#bib.bib117)] | AlexNet | ImageNet, TC128, ILSVRC-VID | DAF
    | Intel Xeon 2.40GHz CPU, GTX Titan X GPU | Python | TensorFlow | 50 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| SiamRPN [[116](#bib.bib116)] | AlexNet | ILSVRC, YTBB | DAF | Intel I7 CPU,
    12GB RAM, GTX 1060 GPU | Python | PyTorch | 160 | FM |'
  prefs: []
  type: TYPE_TB
- en: '| C-RPN [[150](#bib.bib150)] | AlexNet | ImageNet, ILSVRC-VID, YTBB | DAF |
    N/A, GTX 1080 GPU | Matlab | MatConvNet |  36 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| GCT [[151](#bib.bib151)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel Xeon
    3.00GHz CPU, 256GB RAM, GTX 1080Ti GPU | Python | TensorFlow | 49.8 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| GradNet [[187](#bib.bib187)] | AlexNet | ILSVRC-2014 | DAF | Intel I7 3.2GHz
    CPU, 32GB RAM, GTX 1080Ti GPU | Python | TensorFlow | 80 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| i-Siam [[215](#bib.bib215)] | AlexNet | GOT-10k | DAF | Intel I7-7700K 4.20GHz
    CPU, Titan Xp GPU | N/A | N/A | 43 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| UpdateNet [[189](#bib.bib189)] | AlexNet | LaSOT | DAF | N/A | Python | PyTorch
    | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| SPM [[153](#bib.bib153)] | AlexNet, SiameseRPN, RelationNet | ImageNet, ILSVRC-VID,
    YTBB, ILSVRC-DET, MSCOCO, CityPerson, WiderFace | DAF | N/A, P100 GPU | N/A |
    N/A | 120 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| FICFNet [[141](#bib.bib141)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    I7 4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 28 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| MTHCF [[166](#bib.bib166)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    6700 3.40GHz CPU, GTX Titan GPU | Matlab | MatConvNet | 33 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| HP [[178](#bib.bib178)] | AlexNet | ImageNet, ILSVRC-VID | DAF | N/A | Python
    | Keras | 69 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| EAST [[177](#bib.bib177)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    I7 4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 23.2 | AC |'
  prefs: []
  type: TYPE_TB
- en: '| CFCF [[137](#bib.bib137)] | VGG-M | ImageNet, ILSVRC-VID, VOT2015 | HOG,
    DAF | Intel Xeon 3.00GHz CPU, Tesla K40 GPU | Matlab | MatConvNet |  1.7 | CM
    |'
  prefs: []
  type: TYPE_TB
- en: '| CFSRL [[138](#bib.bib138)] | VGG-M | ILSVRC-VID | DAF | Intel Xeon 2.40GHz
    CPU, 32GB RAM, GTX Titan X GPU | Matlab, Python | PyTorch | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| C2FT [[174](#bib.bib174)] | VGG-M | ImageNet, N/A | DAF | Intel Xeon 2.60GHz
    CPU, GTX 1080Ti GPU | N/A | N/A | N/A | AC |'
  prefs: []
  type: TYPE_TB
- en: '| DSNet [[205](#bib.bib205)] | VGG-M | ILSVRC-2015 | DAF | Intel 6700K 4.0GHz
    CPU, GTX 1080 GPU | N/A | N/A | 68.5 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| SRT [[80](#bib.bib80)] | VGG-16 | ImageNet, ALOV, Deform-SOT | DAF | N/A
    | N/A | N/A | N/A | BB |'
  prefs: []
  type: TYPE_TB
- en: '| IMLCF [[128](#bib.bib128)] | VGG-16 | ImageNet, ILSVRC-VID | DAF | Intel
    1.40GHz CPU, GTX 1080Ti GPU | Matlab | MatConvNet | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| SINT++ [[181](#bib.bib181)] | VGG-16 | ImageNet, OTB2013, OTB2015, VOT2014
    | DAF | Intel I7-6700K CPU, 32GB RAM, GTX 1080 GPU | Python | Caffe, Keras | N/A
    | AC |'
  prefs: []
  type: TYPE_TB
- en: '| MAM [[171](#bib.bib171)] | VGG-16, Faster-RCNN | ImageNet, PASCAL VOC 2007,
    OTB100, TC128 | DAF | 3.40GHz CPU, Titan GPU | Matlab | Caffe | 3 | SM |'
  prefs: []
  type: TYPE_TB
- en: '| PTAV [[70](#bib.bib70), [71](#bib.bib71)] | VGGNet | ALOV | HOG, DAF | N/A,
    GTX Titan Z GPU | C/C++ | Caffe | 27 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| UDT [[158](#bib.bib158)] | VGGNet | ILSVRC | DAF | Intel I7-4790K 4.00GHz,
    GTX 1080Ti GPU | Matlab | MatConvNet | 55 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DRRL [[162](#bib.bib162)] | VGGNet | ImageNet, VOT2016 | DAF | N/A, GTX 1060
    GPU | Python | TensorFlow | 6.3 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| FCSFN [[125](#bib.bib125)] | VGG-19 | ImageNet, ALOV | DAF | N/A | N/A |
    N/A | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| Siam-MCF [[110](#bib.bib110)] | ResNet-50 | ImageNet, ILSVRC-VID | DAF |
    Intel Xeon E5 CPU, GTX 1080Ti GPU | Python | TensorFlow | 20 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| SiamMask [[155](#bib.bib155)] | ResNet-50 | ImageNet, MSCOCO, ILSVRC-VID,
    YouTube-VOS | DAF | N/A, RTX 2080 GPU | Python | PyTorch | 55 60 | SGM, RBB |'
  prefs: []
  type: TYPE_TB
- en: '| SiamRPN++ [[156](#bib.bib156)] | ResNet-50 | ImageNet, MSCOCO, ILSVRC-DET,
    ILSVRC-VID, YTBB | DAF | N/A, Titan Xp Pascal GPU | Python | PyTorch | 35 | OS,
    BB |'
  prefs: []
  type: TYPE_TB
- en: '| CGACD [[190](#bib.bib190)] | ResNet-50 | ILSVRC-VID, YTBB, GOT-10k, COCO,
    ILSVRC-DET | DAF | 3.5GHz CPU, RTX 2080Ti GPU | Python | PyTorch | 70 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| CSA [[191](#bib.bib191)] | ResNet-50 | GOT-10k | DAF | Intel I9 CPU, 64GB
    RAM, RTX 2080Ti GPU | Python | PyTorch | 100 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| SiamAttn [[197](#bib.bib197)] | ResNet-50 | COCO, YouTube-VOS, LaSOT, TrackingNet
    | DAF | N/A, RTX 2080Ti GPU | Python | PyTorch | 33 | BB, SGM |'
  prefs: []
  type: TYPE_TB
- en: '| SiamBAN [[198](#bib.bib198)] | ResNet-50 | ILSVRC-VID, YTBB, COCO, ILSVRC-DET,
    GOT-10k, LaSOT | DAF | Intel Xeon 4108 1.8GHz CPU, 64GB RAM, GTX 1080Ti GPU |
    Python | PyTorch | 40 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| SiamCAR [[199](#bib.bib199)] | ResNet-50 | ILSVRC-DET, COCO, ILSVRC-VID,
    YTBB | DAF | N/A, 4 RTX 2080Ti GPU | Python | PyTorch | 52.2 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| SiamDW [[154](#bib.bib154)] | ResNet, ResNeXt, Inception | ImageNet, ILSVRC-VID,
    YTBB | DAF | Intel Xeon 2.40GHz CPU, GTX 1080 GPU | Python | PyTorch | 13 93 |
    CM, FM |'
  prefs: []
  type: TYPE_TB
- en: '| FlowTrack [[118](#bib.bib118)] | FlowNet | Flying chairs, Middlebur, KITTI,
    Sintel, ILSVRC-VID | DAF, DMF | Intel I7-6700 CPU, 48GB RAM, GTX Titan X GPU |
    Matlab | MatConvNet | 12 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| RASNet [[121](#bib.bib121)] | Attention networks | ILSVRC-DET | DAF | Intel
    Xeon 2.20GHz CPU, Titan Xp Pascal GPU | Matlab | MatConvNet | 83 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| ACFN [[93](#bib.bib93)] | Attentional correlation filter network | OTB2013,
    OTB2015, VOT2014, VOT2015 | HOG, Color, DAF | Intel I7-6900K 3.20GHz CPU, 32GB
    RAM, GTX 1070 GPU | Matlab, Python | MatConvNet, TensorFlow | 15 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| RFL [[77](#bib.bib77)] | Convolutional LSTM | ILSVRC-VID | DAF | Intel I7-6700
    3.40GHz CPU, GTX 1080 GPU | Python | TensorFlow |  15 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DRLT [[176](#bib.bib176)] | YOLO | ImageNet, PASCAL VOC | DAF | N/A, GTX
    1080 GPU | Python | TensorFlow |  45 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| TGGAN [[129](#bib.bib129)] | - | ALOV, VOT2015 | DAF | N/A, GTX Titan X GPU
    | Python | Keras | 3.1 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DCTN [[131](#bib.bib131)] | - | TC128, NUS-PRO, MOT2015 | DAF, DMF | N/A
    | N/A | N/A | 27 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| YCNN [[135](#bib.bib135)] | - | ImageNet, ALOV300++ | DAF | N/A, Tesla K40c
    GPU | Python | TensorFlow | 45 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| RDT [[180](#bib.bib180)] | - | VOT2015 | DAF | Intel I7-4790K 4.00GHz, 24GB
    RAM, GTX Titan X GPU | Python | TensorFlow | 43 | CM, OS |'
  prefs: []
  type: TYPE_TB
- en: '| SiamRCNN [[200](#bib.bib200)] | Faster R-CNN, ResNet-101-FPN | ILSVRC-VID,
    COCO, YouTube-VOS, GOT-10k, LaSOT | DAF | N/A, Tesla V100 GPU | Python | TensorFlow
    | 4.7 | BB, SGM |'
  prefs: []
  type: TYPE_TB
- en: '| FGTrack [[202](#bib.bib202)] | ResNet-18, FlowNet2 | ILSVRC-VID, TrackingNet,
    YouTube-VOS | DAF, DOF | N/A, GTX 1080Ti GPU | Python | PyTorch | 19.6 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| CRVFL [[183](#bib.bib183)] | - | N/A | DAF | Intel I7 CPU | N/A | N/A | 1.5-2
    | OS |'
  prefs: []
  type: TYPE_TB
- en: '| VTCNN [[184](#bib.bib184)] | - | N/A | DAF | Intel I7 3.4GHz | Matlab | N/A
    | 5.5 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| GlobalTrack [[214](#bib.bib214)] | Faster R-CNN, ResNet-50 | COCO, GOT-10k,
    LaSOT | DAF | N/A, Titan X GPU | Python | PyTorch | 6 | BB, SGM |'
  prefs: []
  type: TYPE_TB
- en: '| SPLT [[218](#bib.bib218)] | MobileNet-v1, ResNet-50 | ILSVRC-VID, ILSVRC-DET
    | DAF | Inter I7 CPU, 32GB RAM, GTX 1080Ti GPU | Python | TensorFlow, Keras |
    25.7 | BB, OS |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: Only online training for visual tracking. The abbreviations are
    denoted as: confidence map (CM), bounding box (BB), object score (OS), deep appearance
    features (DAF), action (AC).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Backbone network | Exploited features | Strategy to alleviate the
    over-fitting problem | PC (CPU, RAM, Nvidia GPU) | Language | Framework | Speed
    (fps) | Tracking output |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SMART [[163](#bib.bib163)] | ZFNet | DAF | Set the learning rates in conv1-conv3
    to zero | Intel 3.10GHz CPU, 256 GB RAM, GTX Titan X GPU | Matlab | Caffe |  27
    | CM |'
  prefs: []
  type: TYPE_TB
- en: '| TCNN [[68](#bib.bib68)] | VGG-M | DAF | Only update fully-connected layers
    | Intel I7-5820K 3.30GHz CPU, GTX Titan X GPU | Matlab | MatConvNet |  1.5 | OS
    |'
  prefs: []
  type: TYPE_TB
- en: '| C2FT [[174](#bib.bib174)] | VGG-M | DAF | Coarse-to-fine localization | Intel
    Xeon E5-2670 2.60GHz, GTX 1080Ti GPU | N/A | N/A | N/A | AC |'
  prefs: []
  type: TYPE_TB
- en: '| TSN [[75](#bib.bib75)] | VGG-16 | DAF | Coarse-to-fine framework | N/A, GTX
    980Ti GPU | Matlab | MatConvNet |  1 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DNT [[98](#bib.bib98)] | VGG-16 | DAF | Set uniform weight decay in the objective
    functions | 3.40GHz CPU, GTX Titan GPU | Matlab | Caffe | 5 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DSLT [[101](#bib.bib101)] | VGG-16 | DAF | Use seven last frames for model
    update | Intel I7 4.00GHz CPU, GTX Titan X GPU | Matlab | Caffe | 5.7 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| LSART [[120](#bib.bib120)] | VGG-16 | DAF | Two-stream training network to
    learn network parameters | Intel 4.00GHz CPU, 32GB RAM, GTX Titan X GPU | Matlab
    | Caffe |  1 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| adaDDCF [[134](#bib.bib134)] | VGG-16 | DAF | Regularization item for training
    of each layer | 3.40GHz CPU, Tesla K40 GPU | Matlab | MatConvNet | 9 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| HSTC [[143](#bib.bib143)] | VGG-16 | DAF | Dropout layer and convolutional
    with the mask layer | Intel Xeon 2.10GHz CPU, GTX 1080 GPU | Matlab | Caffe |
    2.1 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| P-Track [[179](#bib.bib179)] | VGG-16 | DAF | Learning policy for update
    and re-initialization | N/A, Tesla K40 GPU | N/A | N/A |  10 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| OSAA [[193](#bib.bib193)] | ResNet-50 or MobileNet-v2 | DAF | - | N/A, Tesla
    V100 GPU | Python | PyTorch | N/A | BB |'
  prefs: []
  type: TYPE_TB
- en: '| STCT | Custom | DAF | Sequential training method | 3.40GHz CPU, GTX Titan
    GPU | Matlab | Caffe | 2.5 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DeepTrack [[64](#bib.bib64)] | Custom | DAF | Temporal sampling mechanism
    for the batch generation in SGD algorithm | Quad-core CPU, GTX 980 GPU | Matlab
    | N/A | 2.5 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| CNT [[66](#bib.bib66)] | Custom | DAF | Incremental update scheme | Intel
    I7-3770 3.40GHz CPU, GPU | Matlab | N/A | 5 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| RDLT [[69](#bib.bib69)] | Custom | DAF | Build relationship between the stable
    factor and iteration number | Intel I7 2.20GHz CPU | Matlab | N/A |  5 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| P2T [[139](#bib.bib139)] | Custom | DAF | Generate large scale of part pairs
    in each mini-batch | Intel I7-4790 3.60GHz CPU, 32GB RAM, GTX 980 GPU | Matlab
    | Caffe |  2 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| AEPCF [[167](#bib.bib167)] | Custom | DAF | Select a proper learning rate
    | Intel I7 3.40GHz, 32GB RAM, GPU | N/A | N/A | 4.15 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| FRPN2T-Siam [[126](#bib.bib126)] | Custom | DAF | Only update fully-connected
    layers | N/A | Matlab | Caffe | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| RLS [[195](#bib.bib195)] | Custom | DAF | Recursive LSE-aided online learning
    method | N/A | Python | N/A | N/A | OS |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Both offline and online training for visual tracking. The abbreviations
    are denoted as: confidence map (CM), bounding box (BB), rotated bounding box (RBB),
    object score (OS), voting map (VM), action (AC), segmentation mask (SGM), deep
    appearance features (DAF), deep motion features (DMF), compressed deep appearance
    features (CDAF).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Backbone network | Offline training(s) | Online network training
    | Exploited features | PC (CPU, RAM, Nvidia GPU) | Language | Framework | Speed
    (fps) | Tracking output |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DRN [[97](#bib.bib97)] | AlexNet | ImageNet | Yes | DAF | N/A, K20 GPU |
    Matlab | Caffe | 1.3 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DSiam/DSiamM [[74](#bib.bib74)] | AlexNet, VGG-19 | ImageNet, ILSVRC-VID
    | Yes | DAF | N/A, GTX Titan X GPU | Matlab | MatConvNet | 45 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| TripletLoss [[100](#bib.bib100)] | AlexNet | ImageNet, ILSVRC-VID, ILSVRC
    | Dependent | DAF | Intel I7-6700 3.40GHz CPU, GTX 1080Ti GPU | Matlab | MatConvNet
    | 55 86 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| MM [[165](#bib.bib165)] | AlexNet | ImageNet, OTB2015, ILSVRC | Yes | DAF
    | Intel I7-6700 4.00GHz CPU, 16GB RAM, GTX 1060 GPU | Matlab | MatConvNet | 1.2
    | OS |'
  prefs: []
  type: TYPE_TB
- en: '| TAAT [[169](#bib.bib169)] | AlexNet, VGGNet, ResNet | ImageNet, ALOV, ILSVRC-VID
    | Yes | DAF | Intel Xeon 1.60GHz CPU, 16GB RAM, GTX Titan X GPU | Matlab | Caffe
    | 15 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| DPST [[55](#bib.bib55)] | VGG-M | ImageNet, ILSVRC-VID, ALOV | Only on the
    first frame | DAF | Intel I7 3.60GHz CPU, GTX 1080Ti GPU | Matlab | MatConvNet
    |  1 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| MDNet [[60](#bib.bib60)] | VGG-M | ImageNet, OTB2015, ILSVRC-VID | Yes |
    DAF | Intel Xeon 2.20GHz CPU, Tesla K20m GPU | Matlab | MatConvNet |  1 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| GNet [[82](#bib.bib82)] | VGG-M | ImageNet, VOT | Yes | DAF | Intel Xeon
    2.66GHz CPU, Tesla K40 GPU | Matlab | MatConvNet | 1 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| BranchOut [[90](#bib.bib90)] | VGG-M | ImageNet, OTB2015, ILSVRC | Yes |
    DAF | N/A | Matlab | MatConvNet | N/A | OS |'
  prefs: []
  type: TYPE_TB
- en: '| SANet [[94](#bib.bib94)] | VGG-M | ImageNet, OTB2015, ILSVRC | Yes | DAF
    | Intel I7 3.70GHz CPU, GTX Titan Z GPU | Matlab | MatConvNet |  1 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| RT-MDNet [[105](#bib.bib105)] | VGG-M | ImageNet, ILSVRC-VID | Yes | DAF
    | Intel I7-6850K 3.60GHz, Titan Xp Pascal GPU | Python | PyTorch | 46 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| TRACA [[113](#bib.bib113)] | VGG-M | ImageNet, PASCAL VOC | Yes | CDAF |
    Intel I7-2700K 3.50GHz CPU, 16GB RAM, GTX 1080 GPU | Matlab | MatConvNet | 101.3
    | CM |'
  prefs: []
  type: TYPE_TB
- en: '| VITAL [[114](#bib.bib114)] | VGG-M | ImageNet, OTB2015, ILSVRC | Yes | DAF
    | Intel I7 3.60GHz CPU, Tesla K40c GPU | Matlab | MatConvNet | 1.5 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| DAT [[130](#bib.bib130)] | VGG-M | ImageNet, OTB2015, ILSVRC | Yes | DAF
    | Intel I7-3.40GHz CPU, GTX 1080 GPU | Python | PyTorch | 1 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| ACT [[103](#bib.bib103)] | VGG-M | ImageNet-Video, ILSVRC | Yes | DAF | 3.40GHz
    CPU, 32GB RAM, GTX Titan GPU | Python | PyTorch | 30 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| MGNet [[146](#bib.bib146)] | VGG-M | ImageNet, OTB2015, ILSVRC | Yes | DAF,
    DMF | Intel I7-5930K 3.50GHz CPU, GTX Titan X GPU | Matlab | MatConvNet |  2 |
    OS |'
  prefs: []
  type: TYPE_TB
- en: '| DRL-IS [[175](#bib.bib175)] | VGG-M | ImageNet, VOT2013 2015 | Yes | DAF
    | Intel I7 3.40GHz CPU, 24GB RAM, GTX 1080Ti GPU | Python | PyTorch | 10.2 | AC
    |'
  prefs: []
  type: TYPE_TB
- en: '| ADNet [[172](#bib.bib172), [173](#bib.bib173)] | VGG-M | ImageNet, VOT2013 2015,
    ALOV | Yes | DAF | Intel I7-4790K, 32GB RAM, GTX Titan X GPU | Matlab | MatConvNet
    | 15 | AC, OS |'
  prefs: []
  type: TYPE_TB
- en: '| FMFT [[127](#bib.bib127)] | VGG-16 | ImageNet | Yes | DAF | Intel Xeon 3.50GHz
    CPU, GTX Titan X GPU | Matlab | MatConvNet | N/A | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DET [[96](#bib.bib96)] | VGG-16 | ImageNet, ALOV, VOT2014, VOT2015 | Yes
    | DAF | Intel I7-4790 3.60GHz CPU, GTX Titan X GPU | Python | Keras | 3.4 | OS
    |'
  prefs: []
  type: TYPE_TB
- en: '| DCFNet/DCFNet2 [[95](#bib.bib95)] | VGGNet | ImageNet, TC128, UAV123, NUS-PRO
    | Yes | DAF | Intel Xeon 2.40GHz CPU, GTX 1080 GPU | Matlab | MatConvNet | 65
    | CM |'
  prefs: []
  type: TYPE_TB
- en: '| STP [[109](#bib.bib109)] | VGGNet | ImageNet | Yes | Votes | N/A, GTX Titan
    X GPU | Python | PyTorch | 4 | VM |'
  prefs: []
  type: TYPE_TB
- en: '| MRCNN [[164](#bib.bib164)] | VGGNet | ImageNet, VOT2015 | Yes | DAF | Intel
    I7 3.50GHz CPU, GTX 1080 GPU | Matlab | MatConvNet |  1.2 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| CODA [[161](#bib.bib161)] | VGG-19, SSD | ImageNet, VOT2013, VOT2014, VOT2015
    | Yes | DAF | Intel I7-4770K CPU, 32GB RAM, GTX 1070 GPU | Matlab | Caffe | 34.8
    | CM |'
  prefs: []
  type: TYPE_TB
- en: '| ATOM [[149](#bib.bib149)] | ResNet-18, IoU-Nets | ImageNet, COCO, LaSOT,
    TrackingNet | Yes | DAF | N/A, GTX 1080 GPU | Python | PyTorch | 30 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| D3S [[192](#bib.bib192)] | ResNet-50 | Youtube-VOS | Yes | DAF | N/A, GTX
    1080 GPU | Python | PyTorch | 25 | RBB, SGM |'
  prefs: []
  type: TYPE_TB
- en: '| MetaUpdater [[217](#bib.bib217)] | ResNet-50 | ImageNet, LaSOT | Yes | DAF
    | Intel I9 CPU, 64GB RAM, GTX 2080Ti GPU | Python | TensorFlow | 13 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| CRAC [[207](#bib.bib207)] | ResNet-50 | ImageNet, KITTI, VisDrone-2018 |
    Yes | DAF | N/A | Python | PyTorch, MatConvNet | 56 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| COMET [[212](#bib.bib212)] | ResNet-50 | ImageNet, LaSOT, GOT-10K, NfS, VisDrone-2019
    | Yes | DAF | N/A, Tesla V100 GPU | Python | PyTorch | 24 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| FGLT [[213](#bib.bib213)] | VGG-M, ResNet-50, PWC-Net | ImageNet, ILSVRC-VID,
    COCO, ILSVRC-DET, YTBB, FlyingChairs, FlyingThings3D | Yes | DAF | Intel Xeon
    3.50GHz CPU, GTX 1080Ti GPU | Python | PyTorch | N/A | CM, BB |'
  prefs: []
  type: TYPE_TB
- en: '| LRVN [[216](#bib.bib216)] | VGG-M, MobileNet | ImageNet, ILSVRC-VID, ILSVRC-DET
    | Yes | DAF | Intel I7 CPU, 32GB RAM, GTX Titan X GPU | Python | TensorFlow |
    2.7 | BB, OS |'
  prefs: []
  type: TYPE_TB
- en: '| UCT/UCT-Lite [[73](#bib.bib73)] | ResNet101 | ImageNet, TC128, UAV123 | Only
    on the first frame | DAF | Intel I7-6700 CPU, 48GB RAM, GTX Titan X GPU | Matlab
    | Caffe | 41 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| FPRNet [[132](#bib.bib132)] | ResNet-101, FlowNet | ImageNet, ILSVRC, SceneFlow
    | Yes | DAF, DMF | N/A | Matlab | Caffe | N/A | BB |'
  prefs: []
  type: TYPE_TB
- en: '| ADT [[160](#bib.bib160)] | - | ImageNet, ALOV300++, UAV123, NUS-PRO | Only
    on the first frame | DAF | Intel 2.40GHz CPU, GTX TITAN X GPU | Python | TensorFlow
    | 7 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| DiMP [[159](#bib.bib159)] | ResNet-18, ResNet-50 | ImageNet, TrackingNet,
    LaSOT, GOT10k, COCO | Meta-learning | DAF | N/A, GTX 1080 GPU | Python | PyTorch
    | 43 57 | OS |'
  prefs: []
  type: TYPE_TB
- en: '| PrDiMP [[194](#bib.bib194)] | ResNet-18 or ResNet-50 | ImageNet, LaSOT, GOT-10k,
    TrackingNet, COCO | Meta-learning | DAF | N/A | Python | PyTorch | 30 40 | CM
    |'
  prefs: []
  type: TYPE_TB
- en: '| BGBDT [[185](#bib.bib185)] | SSD or FasterRCNN | ImageNet, COCO, GOT-10k
    | Meta-learning | DAF | N/A, GTX 1080 GPU | Python | PyTorch | 3 10 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| MLT [[188](#bib.bib188)] | AlexNet | ImageNet, ILSVRC-2015, ILSVRC-2017 |
    Meta-learning | DAF | Intel I7-4790K 4.0GHz CPU, 32GB RAM, GTX Titan X GPU | Python
    | TensorFlow | 48.1 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| ROAM [[196](#bib.bib196)] | VGG-16 | ImageNet, ILSVRC-VID, ILSVRC-DET, TrackingNet,
    LaSOT, GOT-10k, COCO | Meta-learning | DAF | Intel I9 3.6GHz CPU, 4 RTX 2080 GPU
    | Python | PyTorch | 13 | CM |'
  prefs: []
  type: TYPE_TB
- en: '| TMAML [[201](#bib.bib201)] | ResNet-18 with RetinaNet or FCOS | ImageNet,
    COCO, GOT-10k, TrackingNet, LaSOT | Meta-learning | DAF | N/A, P100 GPU | Python
    | N/A | 40 | BB |'
  prefs: []
  type: TYPE_TB
- en: '| Meta-Tracker [[182](#bib.bib182)] | dependent | ImageNet, ILSVRC-DET, VOT-2013,
    VOT-2014, VOT-2015 | Meta-learning | DAF | N/A, GTX Titan X GPU | Python | PyTorch
    | dependent | dependent |'
  prefs: []
  type: TYPE_TB
- en: II-C3 Only Online Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To discriminate unseen targets that may consider as the target in evaluation
    videos, some DL-based visual tracking methods use online training of whole or
    a part of DNNs to adapt network parameters according to the large variety of target
    appearance. Because of the time-consuming process of offline training on large-scale
    training data and insufficient discrimination of pre-trained models for representing
    tracking particular targets, the methods shown in Table [III](#S2.T3 "TABLE III
    ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual Tracking
    Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey") use directly
    training of DNNs and inference process alternatively online. However, these methods
    usually exploit some strategies to prevent over-fitting problem and divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C4 Both Offline and Online Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To exploit the maximum capacity of DNNs for visual tracking, the methods shown
    in Table [IV](#S2.T4 "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network Training
    ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey") use both offline and online training. The offline and online learned
    features are known as shared and domain-specific representations, which majorly
    can discriminate the target from foreground information or intra-class distractors,
    respectively. Because visual tracking is a hard and challenging problem, the DL-based
    visual trackers attempt to simultaneously employ feature transferability and online
    domain adaptation.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C5 Data Augmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data augmentation comprises a set of techniques for increasing training samples’
    size to improve data quality and avoid the over-fitting problem. Visual trackers
    broadly employ these techniques based on the few-data regime of this task (see
    Table [V](#S2.T5 "TABLE V ‣ II-C5 Data Augmentation ‣ II-C Network Training ‣
    II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")). The geometric transformations & color space augmentations are vastly
    exploited for visual tracking. However, other algorithms, such as employing GANs
    [[114](#bib.bib114)], can effectively impact tracking performance by capturing
    various appearance changes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Data Augmentations for Visual Tracking Methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Augmentations |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GOTURN [[57](#bib.bib57)] | Motion model, random crops |'
  prefs: []
  type: TYPE_TB
- en: '| MDNet [[60](#bib.bib60)] | Multiple positive examples around the target |'
  prefs: []
  type: TYPE_TB
- en: '| DeepTrack [[65](#bib.bib65), [64](#bib.bib64)] | Horizontal flip |'
  prefs: []
  type: TYPE_TB
- en: '| RFL [[77](#bib.bib77)] | Random color distortion, translation, stretching
    |'
  prefs: []
  type: TYPE_TB
- en: '| VRCPF [[84](#bib.bib84)] | Annotated face ROIs using PASCAL VOC 2007 |'
  prefs: []
  type: TYPE_TB
- en: '| DNT [[98](#bib.bib98)] | Center-shifted random patches, translation schemes
    |'
  prefs: []
  type: TYPE_TB
- en: '| UPDT [[102](#bib.bib102)] | Flip, rotation, shift, blur, dropout |'
  prefs: []
  type: TYPE_TB
- en: '| DaSiamRPN [[104](#bib.bib104)] | Translation, scale variations and illumination
    changes, motion blur |'
  prefs: []
  type: TYPE_TB
- en: '| STP [[109](#bib.bib109)] | Randomly shifted pairs |'
  prefs: []
  type: TYPE_TB
- en: '| Siam-MCF [[110](#bib.bib110)] | Random cropping, color distortion, horizontal
    flipping, small resizing perturbations |'
  prefs: []
  type: TYPE_TB
- en: '| TRACA [[113](#bib.bib113)] | Blur, flip |'
  prefs: []
  type: TYPE_TB
- en: '| VITAL [[114](#bib.bib114)] | Randomly masks |'
  prefs: []
  type: TYPE_TB
- en: '| SiamRPN [[116](#bib.bib116)] | Affine transformation |'
  prefs: []
  type: TYPE_TB
- en: '| YCNN [[135](#bib.bib135)] | Rotation, translation, illumination variation,
    mosaic, salt & pepper noise |'
  prefs: []
  type: TYPE_TB
- en: '| DeepFWDCF [[144](#bib.bib144)] | Gray-scale rotation invariant LBP histograms
    |'
  prefs: []
  type: TYPE_TB
- en: '| ORHF [[147](#bib.bib147)] | Augmentation of negative samples |'
  prefs: []
  type: TYPE_TB
- en: '| ATOM [[149](#bib.bib149)] | Translation, rotation, blur, dropout, flip, color
    jittering |'
  prefs: []
  type: TYPE_TB
- en: '| MAM [[171](#bib.bib171)] | All detected windows from target category |'
  prefs: []
  type: TYPE_TB
- en: '| DiMP50 [[159](#bib.bib159)] | Translation, rotation, blur, dropout, flip,
    color jittering |'
  prefs: []
  type: TYPE_TB
- en: '| PrDiMP50 [[194](#bib.bib194)] | Translation, rotation, blur, dropout, flip,
    color jittering |'
  prefs: []
  type: TYPE_TB
- en: '| TAAT [[169](#bib.bib169)] | Spatial & temporal pairs |'
  prefs: []
  type: TYPE_TB
- en: '| CGACD [[190](#bib.bib190)] | RoI augmentation |'
  prefs: []
  type: TYPE_TB
- en: '| MLT [[188](#bib.bib188)] | Horizontal flip, noise, Gaussian blur, translation
    |'
  prefs: []
  type: TYPE_TB
- en: '| ROAM [[196](#bib.bib196)] | Stretching and scaling the images |'
  prefs: []
  type: TYPE_TB
- en: '| SiamRCNN [[200](#bib.bib200)] | Motion blur, gray-scale, gamma, flip, and
    scale augmentations |'
  prefs: []
  type: TYPE_TB
- en: '| TMAML [[201](#bib.bib201)] | Random scaling, shifting, zoom in/out |'
  prefs: []
  type: TYPE_TB
- en: II-C6 Meta-Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As a well-known paradigm in machine learning, meta-learning [[246](#bib.bib246)]
    (an alternative for data augmentation) has provided promising results for visual
    tracking task. Generally, it aims to provide experience on several learning tasks
    and use them to improve the performance of a new task. Inspired by model-agnostic
    meta-learning (MAML) [[247](#bib.bib247)], visual trackers mainly seek to exploit
    meta-learners for constructing more flexible target models regarding unseen targets/scenarios
    (see Table [IV](#S2.T4 "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network
    Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")). For instance, it can be leveraged into the initialization
    [[182](#bib.bib182), [201](#bib.bib201)], fast model adaptation [[185](#bib.bib185),
    [188](#bib.bib188)], or model update [[196](#bib.bib196), [217](#bib.bib217)]
    procedures of visual trackers. However, some visual trackers [[159](#bib.bib159),
    [194](#bib.bib194)] exploit meta-learning ideas to adjust their model weights
    during tracking, which is different from the classic meta-learning definition.'
  prefs: []
  type: TYPE_NORMAL
- en: II-D Network Objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the training and inference stages, DL-based visual trackers localize the
    given target based on network objective function. Hence, these methods are categorized
    into classification-based, regression-based, or both classification and regression-based
    methods as follows. This sub-section does not include the methods that exploit
    deep off-the-shelf features because these methods do not design and train the
    networks and usually employ pre-trained DNNs for feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: II-D1 Classification-based Objective Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Motivated by other computer vision tasks such as image detection, classification-based
    visual tracking methods employ object proposal methods to produce hundreds of
    candidate/proposal BBs extracted from the search region. These methods aim to
    select the high score proposal by classifying the proposals to the target and
    background classes. This two-class (or binary) classification involves visual
    targets from various class and moving patterns and individual sequences, including
    challenging scenarios. Due to the main attention of these methods on inter-class
    classification, tracking a visual target in the presence of the same labeled targets
    is intensely prone to drift-problem. Also, tracking the arbitrary appearance of
    targets may lead to recognizing different targets with varying appearances. Therefore,
    the performance of the classification-based visual tracking methods is also related
    to their object proposal method, which usually produces a considerable number
    of candidate BBs. On the other side, some recent DL-based trackers utilize this
    objective function to take optimal actions [[162](#bib.bib162), [172](#bib.bib172),
    [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175), [177](#bib.bib177),
    [181](#bib.bib181), [174](#bib.bib174)].
  prefs: []
  type: TYPE_NORMAL
- en: II-D2 Regression-based Objective Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the continuous instinct of estimation space of visual tracking, regression-based
    methods usually aim to directly localize the target in the subsequent frames by
    minimizing a regularized least-squares function. Generally, extensive training
    data are needed to train these methods effectively. The primary goal of regression-based
    methods is to refine the formulation of L2 or L1 loss functions, such as utilizing
    shrinkage loss in the learning procedure [[101](#bib.bib101)], modeling both regression
    coefficients and patch reliability to optimize a neural network efficiently [[120](#bib.bib120)],
    or applying a cost-sensitive loss to enhance unsupervised learning performance
    [[158](#bib.bib158)]. Meanwhile, recent visual trackers define a loss function
    for BB regression (e.g., [[190](#bib.bib190), [196](#bib.bib196), [202](#bib.bib202)])
    to provide accurate localization.
  prefs: []
  type: TYPE_NORMAL
- en: II-D3 Classification- and Regression-based Objective Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To take advantages of both foreground-background/category classification and
    ridge regression (i.e., regularized least-squares objective function), a broad
    range of trackers employ both classification- and regression-based objective functions
    for visual tracking (see Fig. [3](#S2.F3 "Figure 3 ‣ II Deep Visual Tracking Taxonomy
    ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")), which their goal
    is to bridge the gap between the recent tracking-by-detection and continuous localization
    process of visual tracking. These methods commonly utilize classification-based
    methods to find the most similar object proposal to target, and then the estimated
    region will be refined by a BB regression method [[60](#bib.bib60), [68](#bib.bib68),
    [80](#bib.bib80), [94](#bib.bib94), [103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105),
    [116](#bib.bib116), [130](#bib.bib130), [146](#bib.bib146), [156](#bib.bib156),
    [164](#bib.bib164), [169](#bib.bib169)]. The target regions are estimated by classification
    scores and optimized regression/matching functions [[127](#bib.bib127), [138](#bib.bib138),
    [139](#bib.bib139), [149](#bib.bib149), [150](#bib.bib150), [153](#bib.bib153),
    [154](#bib.bib154), [155](#bib.bib155), [159](#bib.bib159), [160](#bib.bib160),
    [163](#bib.bib163), [175](#bib.bib175), [212](#bib.bib212)] to enhance efficiency
    and accuracy. The classification outputs are mainly inferred for candidate proposals’
    confidence scores, foreground detection, candidate window response, actions, and
    so forth.'
  prefs: []
  type: TYPE_NORMAL
- en: II-E Network Output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to the network objective, the DL-based methods generate different
    network outputs to estimate or refine the estimated target location. Based on
    their network outputs, the DL-based methods are classified into six main categories
    (see Fig. [3](#S2.F3 "Figure 3 ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning
    for Visual Tracking: A Comprehensive Survey") and Table [II](#S2.T2 "TABLE II
    ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual Tracking
    Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey") to Table [IV](#S2.T4
    "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")),
    namely confidence map (also includes score map, response map, and voting map),
    BB (also includes rotated BB), object score (also includes the probability of
    object proposal, verification score, similarity score, and layer-wise score),
    action, feature maps, and segmentation mask. Besides template-based methods, segmentation-based
    trackers have been explored to boost tracking performance. Despite initial works
    [[248](#bib.bib248), [249](#bib.bib249), [250](#bib.bib250)], employing independent
    deep networks for tracking & VOS may lead to irretrievable tracking failures and
    high computational complexity. Thus, segmentation-based trackers aim to jointly
    track and segment visual targets by adding a segmentation branch to the network
    [[127](#bib.bib127), [155](#bib.bib155), [192](#bib.bib192), [197](#bib.bib197)]
    or off-the-shelf BB-to-segmentation (Box2Seg) networks [[251](#bib.bib251), [252](#bib.bib252)]
    to the base tracking network.'
  prefs: []
  type: TYPE_NORMAL
- en: II-F Exploitation of Correlation Filters Advantages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The DCF-based methods aim to learn a set of discriminative filters that an
    element-wise multiplication of them with a set of training samples in the frequency
    domain determines spatial target location. Since DCF has provided competitive
    tracking performance and computational efficiency compared to sophisticated techniques,
    DL-based visual trackers use correlation filter advantages. These methods are
    categorized based on how they exploit DCF advantages by using either a whole DCF
    framework or some benefits, such as its objective function or correlation filters/layers.
    Considerable visual tracking methods are based on integrating deep features in
    the DCF framework (see Fig. [3](#S2.F3 "Figure 3 ‣ II Deep Visual Tracking Taxonomy
    ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")). These methods
    aim to improve the robustness of target representation against challenging attributes,
    while other methods attempt to benefit the computational efficiency of correlation
    filter(s) [[86](#bib.bib86)], correlation layer(s) [[118](#bib.bib118), [134](#bib.bib134),
    [141](#bib.bib141), [157](#bib.bib157), [166](#bib.bib166), [205](#bib.bib205)],
    and the objective function of correlation filters [[73](#bib.bib73), [74](#bib.bib74),
    [95](#bib.bib95), [121](#bib.bib121), [149](#bib.bib149), [158](#bib.bib158)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-G Aerial-view Tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By pervasive applications of flying robots, tracking from aerial views introduces
    extra attractive challenges, such as tiny objects, weather conditions, dense environments,
    long occlusions, significant viewpoint change, etc. Aerial-view trackers can be
    classified into class-specific & class-agnostic methods. Class-specific trackers
    mostly focus on human or vehicle classes, such as the unified contextual relation
    actor-critic (CRAC) [[207](#bib.bib207)], a GAN-based vehicle tracker that aims
    to model contextual relation and transfer the ground-view features to the aerial-ones.
    In contrast, class-agnostic trackers can track arbitrary classes of targets. Some
    DCF-based trackers [[206](#bib.bib206), [208](#bib.bib208), [209](#bib.bib209),
    [210](#bib.bib210), [211](#bib.bib211)] were the first generation of flying robot
    trackers, which address the inherent limitations of correlation filters (e.g.,
    boundary effect and filter corruption) given aerial view conditions. However,
    the coarse-to-fine tracker (C2FT) [[174](#bib.bib174)] employs deep RL coarse-
    & fine-trackers (for estimating entire BB and its refinement) to address significant
    aspect-ratio change of targets from aerial-views. Finally, the context-aware IoU-guided
    network for small object tracking (COMET) aims to narrow the performance gap between
    the aerial-view trackers & state-of-the-art ones. It employs an offline proposal
    generation strategy & a multitask two-stream network to exploit context information
    and handle out-of-view & occlusion effectively.
  prefs: []
  type: TYPE_NORMAL
- en: II-H Long-term Tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Long-term tracking performs on more realistic scenarios, including (relatively)
    long videos in which targets may disappear & reappear. Despite the close relationship
    to practical applications, limited trackers have been proposed for this task.
    One way is the extension of a short-term tracker by various strategies. For instance,
    DaSiamRPN [[104](#bib.bib104)] uses a local-to-global search region strategy to
    handle out-of-view & full occlusion, while LCTdeep [[142](#bib.bib142)] utilizes
    a detection module with incremental updates. The FGLT [[213](#bib.bib213)] employs
    both MDNet [[60](#bib.bib60)] & SiamRPN++ [[156](#bib.bib156)] trackers for the
    tracking result judgment, and a detection module modifies tracking failures. The
    memory model via the Siamese network for long-term tracking (MMLT) [[107](#bib.bib107)]
    modifies the SiamFC tracker [[58](#bib.bib58)] by re-detection and memory management
    parts. Moreover, the improved Siamese tracker (i-Siam) [[215](#bib.bib215)] revisits
    the SiamFC tracker via a negative signal suppression approach & a diverse multi-template
    one. The multi-level CF-based tracker [[204](#bib.bib204)] employs an oriented
    re-detection technique, while MetaUpdater [[217](#bib.bib217)] exploits a SiamRPN-based
    re-detector and an online verifier with a meta-updater. Finally, the SPLT tracker
    [[218](#bib.bib218)] is based on SiamRPN [[116](#bib.bib116)] and comprises perusal
    and skimming modules for local tracking & search window selection.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, various long-term trackers have been inspired by successful
    detection methods. For instance, GlobalTrack [[214](#bib.bib214)] is a two-stage
    tracker, including a query-guided region proposal network & query-guided region
    CNN to generate object candidates and produce the final predictions, respectively.
    Also, the LRVN tracker [[216](#bib.bib216)] consists of the combination of an
    offline-learned regression network with an online-updated verification network
    to generate target candidates & evaluate/update them on reliable observations.
    Lastly, the SiamRCNN tracker [[200](#bib.bib200)] introduces a hard example mining
    procedure and tracklet dynamic programming algorithm to detect potential distractors
    & select the best target at each time-step.
  prefs: []
  type: TYPE_NORMAL
- en: II-I Online Tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While initial DL-based methods had focused on their performance, recent trackers
    aim to be accurate, robust, and efficient simultaneously. According to Fig. [3](#S2.F3
    "Figure 3 ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey"), a wide variety of algorithms are classified as online
    trackers regarding different hardware implementations (Table [I](#S2.T1 "TABLE
    I ‣ II-B1 Model Reuse or Deep Off-the-Shelf Features ‣ II-B Network Exploitation
    ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")-Table [IV](#S2.T4 "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network
    Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")). Most of these trackers (e.g., [[59](#bib.bib59), [57](#bib.bib57),
    [58](#bib.bib58), [116](#bib.bib116), [156](#bib.bib156)]) are based on offline-trained
    SNNs that do not update the target model (i.e., initial frame) during tracking.
    Some deep DCF-based trackers (e.g., [[148](#bib.bib148), [115](#bib.bib115), [163](#bib.bib163),
    [205](#bib.bib205)]) exploit efficient optimizations & computations in the frequency
    domain, although employing pre-trained networks limits their speeds. Lately, custom-based
    trackers (e.g., [[149](#bib.bib149), [159](#bib.bib159), [194](#bib.bib194), [212](#bib.bib212),
    [192](#bib.bib192), [201](#bib.bib201)]) employ shallow networks with robust optimization
    strategies to attain high-speed tracking. Finally, numerous techniques have been
    used to speed up DL-based trackers (e.g., learning-based search strategy [[103](#bib.bib103)],
    domain adaption [[207](#bib.bib207), [201](#bib.bib201)], embedding space learning
    [[105](#bib.bib105)], collaborative framework [[131](#bib.bib131)], offline-trained
    CNN [[135](#bib.bib135)], and efficient updating & scale estimation strategies
    [[73](#bib.bib73)]).'
  prefs: []
  type: TYPE_NORMAL
- en: III Visual Tracking Benchmark Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Visual tracking benchmark datasets have been introduced to provide fair and
    standardized evaluations of single-object tracking algorithms. These benchmarks
    are mainly categorized based on generic or aerial-view tracking applications while
    providing short- or long-term scenarios. These datasets contain various sequences,
    frames, attributes, and classes (or clusters). The attributes include illumination
    variation (IV), scale variation (SV), occlusion (OCC), deformation (DEF), motion
    blur (MB), fast motion (FM), in-plane rotation (IPR), out-of-plane rotation (OPR),
    out-of-view (OV), background clutter (BC), low resolution (LR), aspect ratio change
    (ARC), camera motion (CM), full occlusion (FOC), partial occlusion (POC), similar
    object (SIB), viewpoint change (VC), light (LI), surface cover (SC), specularity
    (SP), transparency (TR), shape (SH), motion smoothness (MS), motion coherence
    (MCO), confusion (CON), low contrast (LC), zooming camera (ZC), long duration
    (LD), shadow change (SHC), flash (FL), dim light (DL), camera shaking (CS), rotation
    (ROT), fast background change (FBC), motion change (MOC), object color change
    (OCO), scene complexity (SCO), absolute motion (AM), size (SZ), relative speed
    (RS), distractors (DI), length (LE), fast camera motion (FCM), object motion (OM),
    object blur (OB), large occlusion (LOC), small objects (SOB), occlusion with background
    clutter (O-B), occlusion with rotation (O-R) and long-term tracking (LT). Table [VI](#S3.T6
    "TABLE VI ‣ III Visual Tracking Benchmark Datasets ‣ Deep Learning for Visual
    Tracking: A Comprehensive Survey") compares the applications, scenarios, characteristics,
    missing labeled data for unsupervised training, and the overlap of single object
    tracking datasets. By different evaluation protocols, existing visual tracking
    benchmarks assess the accuracy & robustness of trackers in realistic scenarios.
    The homogenized evaluation protocols facilitate straightforward comparison and
    development of visual trackers. Below the most popular visual tracking benchmark
    datasets and evaluation metrics are briefly described.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Comparison of visual tracking datasets. The abbreviations are denoted
    as RN: row number, NoV: number of videos, NoF: number of frames, NoA: number of
    attributes, OD: overlapped datasets, AL: absent labels, NoC: number of classes
    or clusters, AD: average duration (s: seconds).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Application | Dataset | Scenario | NoV | NoF | NoA | OD | AL | NoC
    | AD | Attributes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | Generic | OTB2013 | ST | 51 | 29K | 11 | VOT, OTB2015, TC128 | No
    | 10 | 19.4s | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
  prefs: []
  type: TYPE_TB
- en: '| 2013-2019 | Generic | VOT2013-2019 | ST | 16-60 | 6K-21K | 12 | OTB, ALOV$++$,
    TC128, UAV123, NUS-PRO | No | 12-24 | 12s | IV, SV, OCC, DEF, MB, BC, ARC, CM,
    MOC, OCO, SCO, AM |'
  prefs: []
  type: TYPE_TB
- en: '| 2014 | Generic | ALOV$++$ | ST | 314 | 89K | 14 | VOT, YouTube | No | 64
    | 16.2s | OCC, BC, CM, LI, SC, SP, TR, SH, MS, MCO, CON, LC, ZC, LD |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | Generic | OTB2015 | ST | 100 | 59K | 11 | OTB2013, VOT, TC128 | No
    | 16 | 19.8s | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | Generic | TC128 | ST | 129 | 55K | 11 | OTB, VOT | No | 27 | 15.6s
    | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | UAV | UAV123 | ST | 123 | 113K | 12 | VOT | No | 9 | 30.6s | IV, SV,
    FM, OV, BC, LR, ARC, CM, FCM, FOC, POC, SIB, VC |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | UAV | UAV20L | LT | 20 | 59K | 12 | VOT | No | 5 | 75s | IV, SV, FM,
    OV, BC, LR, ARC, CM, FCM, FOC, POC, SIB, VC |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | Generic | NUS-PRO | ST | 365 | 135K | 12 | VOT, YouTube | No | 17
    | 12.6s | SV, DEF, BC, FOC, POC, SIB, SHC, FL, DL, CS, ROT, FBC |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Generic | NfS | ST | 100 | 383K | 9 | YouTube | No | 17 | 15.6s |
    IV, SV, OCC, DEF, FM, OV, BC, LR, VC |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | UAV | DTB | ST | 70 | 15K | 11 | YouTube | No | 15 | 7.2s | SV, OCC,
    DEF, MB, IPR, OPR, OV, BC, ARC, FCM, SIB |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Generic | TrackingNet | ST | 30643 | 14.43M | 15 | YTBB | No | 27
    | 16.6s | IV, SV, DEF, MB, FM, IPR, OPR, OV, BC, LR, ARC, CM, FOC, POC, SIB |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Generic | TinyTLP / TLPattr | ST | 50 | 30K | 6 | YouTube | No | N/A
    | 20s | FM, IV, SV, POC, OV, BC |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Generic | OxUvA | LT | 366 | 1.55M | 6 | YTBB | Yes | 22 | 144s |
    SV, OV, SZ, RS, DI, LE |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Generic | TLP | LT | 50 | 676K | 0 | YouTube | No | N/A | 484.8s |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | UAV | BUAA-PRO | ST | 150 | 8.7K | 12 | NUS-PRO, YTBB | No | 12 |
    2s | SV, DEF, BC, FOC, POC, SIB, SHC, FL, DL, CS, ROT, FBC |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Generic | GOT10k | ST | 10000 | 1.5M | 6 | VOT, WordNet, ImageNet
    | Yes | 563 | 16s | IV, SV, OCC, FM, ARC, LO |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | UAV | UAVDT | ST | 50 | 80K | 9 | - | No | 3 | N/A | BC, CM, OM, SOB,
    IV, OB, SV, LOC, LT |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Generic | LTB35 / VOT2018-LT | LT | 35 | 147K | 10 | YouTube, VOT,
    UAV20L | No | 6 | N/A | FOC, POC, OV, CM, FM, SC, ARC, VC, SIB, DEF |'
  prefs: []
  type: TYPE_TB
- en: '| 2018-2020 | UAV | VisDrone2018-2020 | ST | 132 | 106.4K | 12 | - | No | 4
    | N/A | IV, SV, FM, OV, BC, LR, ARC, CM, FOC, POC, SIB, VC |'
  prefs: []
  type: TYPE_TB
- en: '| 2019-2020 | UAV | VisDrone2019-2020L | LT | 25 | 82.6K | 12 | - | No | 4
    | N/A | IV, SV, FM, OV, BC, LR, ARC, CM, FOC, POC, SIB, VC |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Generic | LaSOT | LT | 1400 | 3.5M | 14 | YouTube, ImageNet | Yes
    | 70 | 84.3s | IV, SV, DEF, MB, FM, OV, BC, LR, ARC, CM, FOC, POC, VC, ROT |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | UAV | Small-90 | ST | 90 | N/A | 11 | UAV123, VOT, OTB, TC128 | No
    | N/A | N/A | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | UAV | Small-112 | ST | 112 | N/A | 11 | UAV123, VOT, OTB, TC128, VisDrone
    | No | N/A | N/A | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Generic | TracKlinic | ST | 2390 | 280K | 9 | OTB, TC128, LaSOT |
    No | N/A | N/A | IV, SV, OCC, MB, OV, BC, ROT, O-B, O-R |'
  prefs: []
  type: TYPE_TB
- en: III-A Short-term Tracking Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generic Object Tracking. As one of the first object tracking benchmarks, OTB2013
    [[219](#bib.bib219)] is developed by fully annotated video sequences to address
    the issues of reported tracking results based on a few video sequences or inconsistent
    initial conditions or parameters. The OTB2015 [[220](#bib.bib220)] is an extended
    OTB2013 dataset with the aim of unbiased performance comparisons. To compare the
    performance of visual trackers on color sequences, the Temple-Color 128 (TColor128
    or TC128) [[225](#bib.bib225)] collected a set of 129 fully annotated video sequences
    that 78 ones are different from the OTB datasets. The Amsterdam library of ordinary
    videos (ALOV) dataset [[42](#bib.bib42)] has been gathered to cover diverse video
    sequences and attributes. By emphasizing challenging visual tracking scenarios,
    the ALOV dataset comprises 304 assorted short videos and 10 longer ones. The video
    sequences are chosen from real-life YouTube videos and have 13 difficulty degrees.
    The videos of ALOV have been categorized according to one of its attributes (Table [VI](#S3.T6
    "TABLE VI ‣ III Visual Tracking Benchmark Datasets ‣ Deep Learning for Visual
    Tracking: A Comprehensive Survey")), although in the OTB datasets, each video
    has been annotated by several visual attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: Motivated by the large dataset’s inequality with a useful one, the VOT dataset
    [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40)] aims to provide a diverse and sufficiently
    small dataset from existing ones, annotated per-frame by rotatable BBs and visual
    properties. To evaluate different visual tracking methods fast and straightforward,
    the VOT includes a visual tracking exchange (TraX) protocol [[253](#bib.bib253)]
    that not only prepares data, runs experiments, and performs analyses but also
    can detect tracking failures (i.e., losing the target) and re-initialize the tracker
    after each failure to assess tracking robustness. Despite some small and saturated
    tracking datasets in the wild, mostly provided for object detection tasks, the
    large-scale TrackingNet benchmark dataset [[229](#bib.bib229)] has been proposed
    to properly feed deep visual trackers. It provides videos for tracking in the
    wild with 500 original videos, more than 14 million upright BB annotations, densely
    annotated data in time, rich distribution of object classes, and real-world scenarios
    by sampled YouTube videos.
  prefs: []
  type: TYPE_NORMAL
- en: For tracking pedestrian and rigid objects, the NUS people and rigid objects
    (NUS-PRO) dataset [[226](#bib.bib226)] has been provided 365 video sequences from
    YouTube that are majorly captured by moving cameras and annotated the level of
    occluded objects of each frame with no occlusion, partial occlusion, and full
    occlusion labels. By higher frame rate (240 FPS) cameras, the need for speed (NfS)
    dataset [[227](#bib.bib227)] provides video sequences from real-world scenarios
    to systematically investigate trade-off bandwidth constraints related to real-time
    analysis of visual trackers. These videos are either recorded by hand-held iPhone/iPad
    cameras or from YouTube videos. Two short sequence datasets, namely TinyTLP &
    TLPattr [[233](#bib.bib233)], are derived from the long-term TLP dataset. For
    each sequence, one visual attribute has been specified for investigating various
    challenges. The large high-diversity dataset, called GOT-10k [[232](#bib.bib232)],
    includes more than ten thousand videos classified into 563 classes of moving objects
    and 87 classes of motion to cover as many challenging patterns in real-world scenarios
    as possible. The GOT-10k has informative continuous attributes, including absent
    labels, which show the target does not exist in the frame. Finally, the TracKlinic
    [[234](#bib.bib234)] introduces a toolkit (collected from OTB2015, TC128, & LaSOT)
    that consists of just one challenging factor per sequence to evaluate visual trackers.
    It also provides two challenging O-B & O-R attributes, including occlusion with
    background clutter & rotation.
  prefs: []
  type: TYPE_NORMAL
- en: Aerial View Object Tracking. Tracking from aerial views has been developed in
    recent years, considering the broad range of applications. The unmanned aerial
    vehicle 123 (UAV123) [[222](#bib.bib222)] provides a sparse and low altitude aerial-view
    tracking dataset that contains the realistic and synthetic HD video sequences
    captured by professional-grade flying robots, a board-cam mounted on small low-cost
    flying robots & simulator ones. Drone tracking benchmark (DTB) [[228](#bib.bib228)]
    is a dataset captured by flying robots or drones that consists of RGB videos with
    massive displacement of target location due to abrupt camera motion. The BUAA-PRO
    dataset [[231](#bib.bib231)] is a segmentation-based benchmark dataset to address
    the problem of inevitable non-target elements in BBs. It exploits the segmentation
    mask-based version of a level-based occlusion attribute. The UAVDT dataset [[223](#bib.bib223)]
    provides an aerial-view dataset with high object density scenarios (e.g., different
    weather conditions, camera views, flying altitudes) focusing on pedestrians and
    vehicles. Furthermore, VisDrone dataset [[236](#bib.bib236), [224](#bib.bib224)]
    includes videos captured by different drone platforms in real-world scenarios.
    For small object tracking, the Small-90 dataset [[237](#bib.bib237)] presents
    aerial videos mostly collected from other visual tracking datasets. By adding
    22 more challenging sequences, the Small-112 dataset [[237](#bib.bib237)] has
    been formed based on the Small-90 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Long-term Tracking Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generic Object Tracking. With the aim of long-term tracking of frequently disappearance
    targets, the OxUvA dataset [[230](#bib.bib230)] includes 14 hours of videos from
    YouTube-BoundingBoxes (or YTBB) [[244](#bib.bib244)] to provide development and
    test sets with continuous attributes. Also, it provides absent labels, which show
    that the target does not exist in some frames. The TLP dataset [[233](#bib.bib233)]
    also has been collected high-resolution videos with a longer duration per sequence
    from YouTube, which provides the possibility of studying tracking consistency.
    However, target disappearances do not frequently occur in the TLP dataset. Hence,
    the LTB-35 [[235](#bib.bib235)] presents an enriched long-term dataset with consistent
    target disappearances (twelve disappearances on average for each video). The large-scale
    single object tracking (LaSOT) [[221](#bib.bib221)] has been developed to address
    the problems of existing datasets, such as small scale, lack of high-quality,
    dense annotations, short video sequences, and category bias. The object categories
    are from the ImageNet and a few visual tracking applications (such as drones)
    with an equal number of videos per category. The training and testing subsets
    include 1120 (2.3M frames) and 280 (690K frames) video sequences, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Aerial View Object Tracking. As the parent set of the UAV-123 dataset, the UAV20L
    is an aerial surveillance dataset, including one continuous shot video sequences.
    It consists of tolerable occlusions and provides difficult scenarios for small
    object tracking. Moreover, the VisDrone-2019/2020L dataset [[224](#bib.bib224)]
    includes 25 challenging sequences (i.e., 12/13 videos in the day/night) with tiny
    targets.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Visual trackers are evaluated by two fundamental evaluation categories of performance
    measures and performance plots to perform experimental comparisons on large-scale
    datasets. These metrics are briefly described as follows.
  prefs: []
  type: TYPE_NORMAL
- en: III-C1 Performance Measures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Performance measures attempt to intuitively interpret performance comparisons
    in terms of complementary metrics of accuracy, robustness and tracking speed.
    For long-term trackers, the measures close relate to their detection counterparts
    to reflect re-detection & target absence prediction capabilities. In the following,
    these measures are concisely investigated.
  prefs: []
  type: TYPE_NORMAL
- en: (A)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Short-term Tracking Measures:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (i)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Center location error (CLE)/ (Normalized) precision: The CLE or precision metric
    is defined as the average Euclidean distance between the precise ground-truth
    locations of the target and estimated locations by a visual tracker. The CLE is
    the oldest metric that is sensitive to dataset annotation and does not consider
    tracking failures, and ignores the target’s BB, resulting in significant errors.
    The normalized precision [[229](#bib.bib229)] aims to relieve the sensitivity
    of CLE to the size of BBs and frame resolutions. Given the size of ground-truth
    BB ($b_{g}$), this metric normalizes the CLE over $b_{g}$ to keep its consistency
    for various target scales.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (ii)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Accuracy: For this metric, first, the overlap score is calculated as $S=\frac{\left|b_{t}\cap
    b_{g}\right|}{\left|b_{t}\cup b_{g}\right|}$ which b[g], b[t], $\cap$, $\cup$
    and $\left|.\right|$ represent the ground-truth BB, an estimated BB by a visual
    tracking method, intersection operator, union operator, and the number of pixels
    in the resulted region, respectively. By considering a certain threshold, the
    overlap score indicates a visual tracker’s success in one frame. The accuracy
    is then calculated by the average overlap scores (AOS) during the tracking when
    a visual tracker’s estimations have overlap with the ground-truth ones. This metric
    jointly considers both location and region to measure the estimated target’s drift
    rate up to its failure.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (iii)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Robustness/ failure score: The robustness or failure score is defined as the
    number of required re-initializations when a tracker loses (or drifts) the target
    during the tracking task. The failure is detected when the overlap score drops
    to zero.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (iv)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expected average overlap (EAO): This score is interpreted as the combination
    of accuracy and robustness scores. Given N[s] frames long sequences, the EAO score
    is calculated as ${\widehat{\mathit{\Phi}}}_{N_{s}}=\left\langle\frac{1}{N_{s}}\sum^{N_{s}}_{i=1}{{\mathit{\Phi}}_{i}}\right\rangle$,
    where ${\mathit{\Phi}}_{i}$ is defined as the average of per-frame overlaps until
    the end of sequences, even if failure leads to zero overlaps.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (v)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Area under curve (AUC): The AUC score has defined the average success rates
    (normalized between 0 and 1) according to the pre-defined thresholds. To rank
    the visual tracking methods based on their overall performance, the AUC score
    summarizes the AOS of visual tracking methods across a sequence.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (B)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Long-term Tracking Measures:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (i)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Precision ($Pr$): Tracking measures for long-term trackers depend on being
    a target in the scene and prediction confidence to be higher than a classification
    threshold for each frame. The precision [[235](#bib.bib235)] is calculated by
    the intersection over union (IoU) between the ground-truth ($b_{g}$) and predicted
    target ($b_{t}$), normalized by the number of frames with existing predictions.
    The integration of these scores over all precision thresholds provides the overall
    tracking precision.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (ii)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Recall ($Re$): Similar to the precision, it calculates the IoU between the
    $b_{g}$ and $b_{t}$, which is normalized by the number of frames with no absent
    targets. The overall tracking recall [[235](#bib.bib235)] is achieved by integrating
    the scores over all recall thresholds.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (iii)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'F-score: It compromises the precision & recall scores by calculating $F=\frac{2Pr.Re}{Pr+Re}$
    to rank the trackers according to their maximum values over all thresholds.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (iv)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Maximum Geometric Mean (MaxGM): Inspired by binary classification, the MaxGM
    employs the true positive rate (TPR) and true negative rate (TNR) for evaluation
    of trackers. While the TPR reports the fraction of correctly located targets,
    the TNR presents the fraction of correctly reported absent targets. As a single
    metric, the geometric mean is defined as $GM=\sqrt{TPR.TNR}$ but it will be zero
    for the trackers that cannot predict absent targets. Hence, $MaxGM=\max_{0\leq
    p\leq 1}\sqrt{\{(1-p).TPR)\}\{(1-p).TNR+p\}}$ provides a more informative comparison
    in terms of various probabilistic thresholds $p$.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: III-C2 Performance Plots
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generally, visual trackers are analyzed in terms of various thresholds to provide
    more intuitive quantitative comparisons. These metrics are summarized as follows.
  prefs: []
  type: TYPE_NORMAL
- en: (A)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Short-term Tracking Plots:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (i)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Precision plot: Given the CLEs per different thresholds, the precision plot
    shows the percentage of video frames in which the estimated locations have at
    most the specific threshold with the ground-truth locations.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (ii)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Success plot: Given the calculated various accuracy per thresholds, the success
    plot measures the percentage of frames in which the estimated overlaps and the
    ground-truth ones have larger overlap than a certain threshold.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (iii)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expected average overlap curve: For an individual length of video sequences,
    the expected average overlap curve has resulted from the range of values in a
    specific interval $\left[N_{lo},N_{hi}\right]$ as $\widehat{\mathit{\Phi}}=\frac{1}{N_{hi}-N_{lo}}\sum^{N_{hi}}_{N_{s}=N_{lo}}{{\widehat{\mathit{\Phi}}}_{N_{s}}}$.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (B)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Long-term Tracking Plots:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (i)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Precision/ Recall plot: It is used to compare long-term tracking performances
    and analyze their detection capabilities in terms of various thresholds.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (ii)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'F-score plot: This is the main curve to rank the tracking methods based on
    the highest score on the plot.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: IV Experimental Analyses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To analyze the performance of state-of-the-art visual tracking methods, 48
    different methods are quantitatively compared on seven well-known tracking datasets,
    namely OTB2013 [[219](#bib.bib219)], OTB2015 [[220](#bib.bib220)], VOT2018 [[39](#bib.bib39)],
    LaSOT [[221](#bib.bib221)], UAV-123 [[222](#bib.bib222)], UAVDT [[223](#bib.bib223)],
    and VisDrone2019-test-dev [[224](#bib.bib224)]. Due to the page limitation, all
    experimental results are publicly available on [https://github.com/MMarvasti/Deep-Learning-for-Visual-Tracking-Survey](https://github.com/MMarvasti/Deep-Learning-for-Visual-Tracking-Survey).
    The included 48 DL-based trackers in the experiments are shown in Table [VII](#S4.T7
    "TABLE VII ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey"). All evaluations are performed on an Intel I7-9700K 3.60GHz CPU with
    32GB RAM with the aid of MatConvNet toolbox [[254](#bib.bib254)] that uses an
    NVIDIA GeForce RTX 2080Ti GPU for its computations. The OTB, LaSOT, UAV123, UAVDT,
    and VisDrone2019 toolkits evaluate the visual trackers in terms of the well-known
    precision & success plots and then rank the methods based on the AUC score. For
    performance comparison on the VOT2018 dataset, the visual trackers have been assessed
    based on the TraX evaluation protocol using three primary measures of accuracy,
    robustness, and EAO to provide the Accuracy-Robustness (AR) plots, expected average
    overlap curve, and ordering plots according to its five challenging visual attributes
    [[39](#bib.bib39)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: State-of-the-art visual tracking methods for experimental comparisons
    on visual tracking datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Published in | Visual Tracking Method | Exploited Features | Test Datasets
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2015 | HCFT [[51](#bib.bib51)] | DAF | OTB, LaSOT |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2015 | DeepSRDCF [[52](#bib.bib52)] | DAF, HOG | OTB, VOT2018 |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2016 | CCOT [[56](#bib.bib56)] | DAF | OTB, VOT2018, UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| ECCVW 2016 | SiamFC [[58](#bib.bib58)] | DAF | OTB, LaSOT, UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2016 | SINT [[59](#bib.bib59)] | DAF | OTB, LaSOT, UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2016 | MDNet [[60](#bib.bib60)] | DAF | OTB, LaSOT, UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2016 | HDT [[61](#bib.bib61)] | DAF | OTB, UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2017, TIP 2019 | PTAV [[70](#bib.bib70), [71](#bib.bib71)] | DAF, HOG
    | OTB, LaSOT, UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2017 | CREST [[72](#bib.bib72)] | DAF | OTB, UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2017 | Meta-CREST [[72](#bib.bib72)] | DAF | OTB |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2017 | UCT [[73](#bib.bib73)] | DAF | OTB, VOT2018 |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2017 | DSiam [[74](#bib.bib74)] | DAF | VOT2018, LaSOT |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2017 | CFNet [[86](#bib.bib86)] | DAF | OTB, VOT2018, LaSOT, UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2017 | ECO [[87](#bib.bib87)] | DAF, HOG, CN | OTB, VOT2018, LaSOT,
    UAV123, UAVDT, VisDrone2019 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2017 | DeepCSRDCF [[88](#bib.bib88)] | DAF, HOG, CN | VOT2018, LaSOT
    |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2017 | MCPF [[89](#bib.bib89)] | DAF | OTB, VOT2018 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2017 | ACFN [[93](#bib.bib93)] | DAF, HOG, Color | OTB |'
  prefs: []
  type: TYPE_TB
- en: '| arXiv 2017 | DCFNet [[95](#bib.bib95)] | DAF | OTB |'
  prefs: []
  type: TYPE_TB
- en: '| arXiv 2017 | DCFNet2 [[95](#bib.bib95)] | DAF | OTB, VOT2018 |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2018 | TripletLoss-CFNet [[100](#bib.bib100)] | DAF | OTB |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2018 | TripletLoss-SiamFC [[100](#bib.bib100)] | DAF | OTB |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2018 | TripletLoss-CFNet2 [[100](#bib.bib100)] | DAF | OTB |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2018 | UPDT [[102](#bib.bib102)] | DAF, HOG, CN | VOT2018 |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2018 | DaSiamRPN [[104](#bib.bib104)] | DAF | VOT2018, UAV123 |'
  prefs: []
  type: TYPE_TB
- en: '| ECCV 2018 | StructSiam [[106](#bib.bib106)] | DAF | LaSOT |'
  prefs: []
  type: TYPE_TB
- en: '| ECCVW 2018 | Siam-MCF [[110](#bib.bib110)] | DAF | OTB |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2018 | TRACA [[113](#bib.bib113)] | CDAF | OTB, VOT2018, LaSOT |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2018 | VITAL [[114](#bib.bib114)] | DAF | OTB, LaSOT |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2018 | DeepSTRCF [[115](#bib.bib115)] | DAF, HOG, CN | OTB, VOT2018,
    LaSOT, UAV123 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2018 | SiamRPN [[116](#bib.bib116)] | DAF | OTB, VOT2018, UAV123 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2018 | SA-Siam [[117](#bib.bib117)] | DAF | OTB, VOT2018 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2018 | LSART [[120](#bib.bib120)] | DAF | VOT2018 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2018 | DRT [[119](#bib.bib119)] | DAF, HOG, CN | VOT2018 |'
  prefs: []
  type: TYPE_TB
- en: '| NIPS 2018 | DAT [[130](#bib.bib130)] | DAF | OTB, VOT2018 |'
  prefs: []
  type: TYPE_TB
- en: '| PAMI 2018 | HCFTs [[133](#bib.bib133)] | DAF | OTB |'
  prefs: []
  type: TYPE_TB
- en: '| IJCV 2018 | LCTdeep [[142](#bib.bib142)] | DAF | OTB |'
  prefs: []
  type: TYPE_TB
- en: '| TIP 2018 | CFCF [[137](#bib.bib137)] | DAF, HOG | VOT2018 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2019 | C-RPN [[150](#bib.bib150)] | DAF | OTB, VOT2018, LaSOT |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2019 | GCT [[151](#bib.bib151)] | DAF | OTB, VOT2018, UAV123 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2019 | SiamMask [[155](#bib.bib155)] | DAF | VOT2018, UAVDT, VisDrone2019
    |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2019 | SiamRPN$++$ [[156](#bib.bib156)] | DAF | OTB, VOT2018, UAV123,
    UAVDT, VisDrone2019 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2019 | TADT [[157](#bib.bib157)] | DAF | OTB |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2019 | ASRCF [[148](#bib.bib148)] | DAF, HOG | OTB, LaSOT |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2019 | SiamDW-SiamRPN [[154](#bib.bib154)] | DAF | OTB, VOT2018, UAVDT,
    VisDrone2019 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2019 | SiamDW-SiamFC [[154](#bib.bib154)] | DAF | OTB, VOT2018 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2019 | ATOM [[149](#bib.bib149)] | DAF | VOT2018, LaSOT, UAV123, UAVDT,
    VisDrone2019 |'
  prefs: []
  type: TYPE_TB
- en: '| ICCV 2019 | DiMP50 [[159](#bib.bib159)] | DAF | VOT2018, LaSOT, UAV123, UAVDT,
    VisDrone2019 |'
  prefs: []
  type: TYPE_TB
- en: '| CVPR 2020 | PrDiMP50 [[194](#bib.bib194)] | DAF | LaSOT, UAVDT, VisDrone2019
    |'
  prefs: []
  type: TYPE_TB
- en: IV-A Quantitative Comparisons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to the results shown in Fig. [4](#S4.F4 "Figure 4 ‣ IV-A Quantitative
    Comparisons ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A
    Comprehensive Survey"), the top-3 visual tracking methods in terms of the precision
    metric are the VITAL, MDNet, and DAT on the OTB2013 dataset, the SiamDW-SiamRPN,
    ASRCF, and VITAL on the OTB2015 dataset, and the PrDiMP50, DiMP50, and ATOM on
    the LaSOT dataset, respectively. In terms of success metric, the ASRCF, VITAL,
    and MDNet on the OTB2013 dataset, the SiamRPN++, SANet, and ASRCF on the OTB2015
    dataset, and the PrDiMP50, DiMP50, and ATOM on the LaSOT dataset have achieved
    the best performance, respectively. On the VOT2018 dataset (see Fig. [5](#S4.F5
    "Figure 5 ‣ IV-A Quantitative Comparisons ‣ IV Experimental Analyses ‣ Deep Learning
    for Visual Tracking: A Comprehensive Survey")), the top-3 visual trackers are
    the SiamMask, SiamRPN++, and DiMP50 in terms of accuracy measure while the PrDiMP50,
    DiMP50, and ATOM trackers have the best robustness, respectively. For the aerial-view
    tracking, the PrDiMP50, DiMP50, SiamRPN++, and SiamMask have provided the best
    results for average precision & success metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: \justify![Refer to caption](img/26c2d76208f8522d71de765b8529c81b.png)![Refer
    to caption](img/6919da00678dd39f7549d8026cd8ee04.png)![Refer to caption](img/5e2441e887af10f6bbb25288d76c07d6.png)![Refer
    to caption](img/4b0dd49a9af105ef9de249a261855e33.png)\justify![Refer to caption](img/1b8b5639edbcfb407adb4ef74271943f.png)![Refer
    to caption](img/e072556d07352e151462e8b909767f1d.png)![Refer to caption](img/021b5bbfb68ad395965ed6bbd01723dd.png)![Refer
    to caption](img/cc808fedbc177a3899b3d71cded02e6e.png)\justify![Refer to caption](img/8c8ebef44f9985393dd1ae147c308f18.png)![Refer
    to caption](img/c22aefe771fad978d3080eedc1f18fc4.png)![Refer to caption](img/30f1cdb705deb7a9feadad478287c5e4.png)![Refer
    to caption](img/b7f4892c7f122ea86ccdccb952fd8aa3.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Overall experimental comparison of state-of-the-art visual tracking
    methods on the OTB2013, OTB2015, LaSOT, UAVDT, and VisDrone2019 visual tracking
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: \justify![Refer to caption](img/7cb24852a1f3b077d13c7b231c2e3b26.png)![Refer
    to caption](img/5174548c0037ad62c135814ca82a1c4f.png)![Refer to caption](img/85469589d7b20e07bfd86586e4bd9252.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Performance comparison of visual tracking methods on VOT2018 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the best trackers based on both precision-success measures
    (see Fig. [4](#S4.F4 "Figure 4 ‣ IV-A Quantitative Comparisons ‣ IV Experimental
    Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")) are the
    VITAL, MDNet, and ASRCF on the OTB2013 dataset, the SiamRPN++, ASRCF, and VITAL
    on the OTB2015 dataset, the PrDiMP50, DiMP50, and ATOM on the LaSOT dataset, and
    the PrDiMP50, DiMP50, and SiamRPN++ on the aerial-view datasets (i.e., the UAV123,
    UAVDT, and VisDrone2019). On the VOT2018 dataset, the DiMP50, SiamRPN++, and ATOM
    are the best performing trackers based on the EAO score. Moreover, the PrDiMP50,
    DiMP50, SiamRPN++, and ATOM have achieved the best AUC scores while the SiamRPN,
    SiamRPN++, and CFNet are the fastest visual trackers, respectively. According
    to the results (i.e., Fig. [4](#S4.F4 "Figure 4 ‣ IV-A Quantitative Comparisons
    ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey"), and Fig. [5](#S4.F5 "Figure 5 ‣ IV-A Quantitative Comparisons ‣ IV Experimental
    Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")), the best
    visual tracking methods that repeated the best results on different tracking datasets
    are the PrDiMP50 [[194](#bib.bib194)], DiMP50 [[159](#bib.bib159)], ATOM [[149](#bib.bib149)],
    VITAL [[114](#bib.bib114)], MDNet [[60](#bib.bib60)], DAT [[130](#bib.bib130)],
    ASRCF [[148](#bib.bib148)], SiamDW-SiamRPN [[154](#bib.bib154)], SiamRPN++ [[156](#bib.bib156)],
    C-RPN [[150](#bib.bib150)], StructSiam [[150](#bib.bib150)], SiamMask [[155](#bib.bib155)],
    DaSiamRPN [[104](#bib.bib104)], UPDT [[102](#bib.bib102)], LSART [[120](#bib.bib120)],
    DeepSTRCF [[115](#bib.bib115)], and DRT [[119](#bib.bib119)]. These methods will
    be investigated in Sec. [IV-C](#S4.SS3 "IV-C Discussion ‣ IV Experimental Analyses
    ‣ Deep Learning for Visual Tracking: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Most Challenging Attributes per Benchmark Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following on the VOT challenges [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)],
    which have specified the most challenging visual tracking attributes, this work
    also introduces the most challenging attributes on the OTB, LaSOT, UAV123, UAVDT,
    and VisDrone datasets. These attributes are determined by the median accuracy
    & robustness per attribute on the VOT or the median precision & success per attribute
    on other datasets. Table [VIII](#S4.T8 "TABLE VIII ‣ IV-B Most Challenging Attributes
    per Benchmark Dataset ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey") shows the most challenging attributes for each benchmark
    dataset. The OCC/LOC, OV, FM, DEF, LR, ARC, and SIB are selected as the most challenging
    attributes that can effectively impact the performance of DL-based visual trackers.
    Fig. [6](#S4.F6 "Figure 6 ‣ IV-B Most Challenging Attributes per Benchmark Dataset
    ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey") compares the performances of these methods on the most challenging attributes
    on the OTB2015, LaSOT, UAV123, UAVDT, and VisDrone2019 datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: Five most challenging attributes of benchmark datasets. [first
    to third challenging attributes are shown by red, yellow, and green colors.]'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Metric | IV | DEF | MB | CM | OCC | POC | FOC | ROT | IPR | OPR
    | BC | VC | SV | FM | OV | LR | ARC | MC | SIB | OM | SOB | OB | LT | LOC |'
  prefs: []
  type: TYPE_TB
- en: '| OTB2015 | Precision | 0.7807 | 0.7382 | 0.7642 | - | 0.7347 | - | - | - |
    0.7575 | 0.7611 | 0.7576 | - | 0.7471 | 0.7506 | 0.6911 | 0.7532 | - | - | - |
    - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Success | 0.6330 | 0.5682 | 0.6466 | - | 0.6027 | - | - | - | 0.6154 | 0.6172
    | 0.6144 | - | 0.6022 | 0.6268 | 0.5683 | 0.5906 | - | - | - | - | - | - | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| VOT2018 | Accuracy | 0.5026 | - | - | 0.5258 | 0.4312 | - | - | - | - | -
    | - | - | 0.4627 | - | - | - | - | 0.5044 | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Robustness | 0.1695 | - | - | 0.1423 | 0.2856 | - | - | - | - | - | - | -
    | 0.1051 | - | - | - | - | 0.1802 | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| LaSOT | Precision | 0.2839 | 0.1778 | 0.2149 | 0.2306 | - | 0.1937 | 0.1904
    | 0.2016 | - | - | 0.2218 | 0.2034 | 0.2266 | 0.1733 | 0.1608 | 0.2248 | 0.2026
    | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Success | 0.2580 | 0.2081 | 0.2216 | 0.2506 | - | 0.2112 | 0.1666 | 0.2186
    | - | - | 0.2329 | 0.1773 | 0.2394 | 0.1398 | 0.1726 | 0.1772 | 0.2119 | - | -
    | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| UAV123 | Precision | 0.6573 | - | - | 0.6317 | - | 0.6991 | 0.6805 | - |
    - | - | 0.6361 | .6462 | 0.6656 | 0.6019 | 0.6429 | 0.6100 | 0.5690 | - | 0.6991
    | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Success | 0.4926 | - | - | 0.4034 | - | 0.5447 | 0.5201 | - | - | - | 0.4673
    | 0.4794 | 0.5176 | 0.4158 | 0.4743 | 0.4724 | 0.3433 | - | 0.5447 | - | - | -
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| UAVDT | Precision | 0.7723 | - | - | 0.6877 | - | - | - | - | - | - | 0.6634
    | - | 0.7003 | - | - | - | - | - | 0.7638 | 0.7039 | 0.7638 | 0.7280 | 0.8235
    | 0.5779 |'
  prefs: []
  type: TYPE_TB
- en: '| Success | 0.5844 | - | - | 0.5515 | - | - | - | - | - | - | 0.5051 | - |
    0.5603 | - | - | - | - | - | 0.5536 | 0.5513 | 0.5536 | 0.5463 | 0.6213 | 0.4600
    |'
  prefs: []
  type: TYPE_TB
- en: '| VisDrone2019 | Precision | 0.7790 | - | - | 0.7407 | - | 0.7174 | 0.6919
    | - | - | - | 0.5775 | 0.8058 | 0.7392 | 0.7473 | 0.8088 | 0.6031 | 0.7522 | -
    | 0.5445 | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Success | 0.6272 | - | - | 0.5816 | - | 0.5511 | 0.5407 | - | - | - | 0.4112
    | 0.6564 | 0.5891 | 0.5862 | 0.6676 | 0.3681 | 0.5861 | - | 0.3914 | - | - | -
    | - | - | \justify![Refer to caption](img/2552126c5252628ac750e093169ab67d.png)![Refer
    to caption](img/c5438bfdebf9c31b534e2eb985093e54.png)![Refer to caption](img/0d866fee82c286dd1eb29b964165956d.png)![Refer
    to caption](img/597f49d8fb5898de4c158825947205f9.png)![Refer to caption](img/d86a1e35474bb6167901f2babc625acd.png)\justify![Refer
    to caption](img/65f1066fcd3367e5bf7154daf37a67f0.png)![Refer to caption](img/3e07181f777d54e6eb7ce05d1b3b91f7.png)![Refer
    to caption](img/a04e1882404c57ac26e57e5210cb4cd7.png)![Refer to caption](img/59706235bff34a411cabde8e132fdd22.png)![Refer
    to caption](img/c3947d9010d5cbdb3f9775232804e854.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Comparison of state-of-the-art trackers in terms of the most challenging
    attributes on the OTB2015, LaSOT, UAV123, UAVDT, and VisDrone2019 datasets, left
    to right column, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: According to the OCC attribute, the most accurate & robust visual trackers on
    the VOT2018 dataset are the SiamRPN++ [[156](#bib.bib156)] and DRT [[119](#bib.bib119)],
    respectively. In terms of success metric, the SiamRPN++ [[156](#bib.bib156)] is
    the best visual tracker to tackle the DEF and OV attributes, while the Siam-MCF
    [[110](#bib.bib110)] is the best one to deal with the visual tracking in LR videos
    on the OTB2015 dataset. The ASRCF [[148](#bib.bib148)], ECO [[87](#bib.bib87)],
    and SiamDW-SiamRPN [[154](#bib.bib154)] are the best trackers in precision metric
    to face with OV, OCC, and DEF attributes on the OTB-2015 dataset. The PrDiMP50
    [[194](#bib.bib194)], DiMP50 [[159](#bib.bib159)], and ATOM [[149](#bib.bib149)]
    trackers are the absolute best methods on the LaSOT dataset on all visual attributes.
    On the UAV123 dataset, the ATOM and DiMP50 have achieved the best results in terms
    of precision and success metrics, respectively. Also, the PrDiMP50 is the best
    tracker for handling large occlusions on the UAVDT dataset. Finally, the SiamDW
    is the best tracker to tackle similar objects on the VisDrone2019-test-dev dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [5](#S4.F5 "Figure 5 ‣ IV-A Quantitative Comparisons ‣ IV
    Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive Survey"),
    the DCF-based methods have achieved fewer failures among the other methods, while
    the SNN- & custom-network based trackers have gained more overlap between the
    estimated BBs and ground-truth ones. The SiamRPN-based methods (i.e., [[155](#bib.bib155),
    [156](#bib.bib156), [104](#bib.bib104), [154](#bib.bib154)]) accurately handle
    scenarios under each of CM, IV, MC, OCC, or SC attributes by adopting deeper and
    wider backbone networks, including classification and regression branches. Moreover,
    the ATOM, DiMP50, and PrDiMP50 exploit powerful classification & regression networks
    and optimization processes for online training and fast adaptation. Thus, these
    trackers have provided significant advances on various tracking benchmarks. By
    considering the fusion of hand-crafted and deep features [[115](#bib.bib115),
    [102](#bib.bib102), [119](#bib.bib119)], temporal regularization term [[115](#bib.bib115)],
    reliability term [[119](#bib.bib119)], data augmentation [[102](#bib.bib102)],
    and exploitation of ResNet-50 model [[102](#bib.bib102)], the DCF-based methods
    have attained desirable robustness against CM attribute. Furthermore, the computational
    efficiency and the robustness of DCF-based trackers are attractive for aerial-view
    trackers.'
  prefs: []
  type: TYPE_NORMAL
- en: To effectively deal with the IV attribute, focusing on the discrimination power
    between the target and its background is the main problem. The strategies such
    as training a fully convolutional network for correlation filter cost function,
    spatial-aware KRR and spatial-aware CNN, and employing semi-supervised video object
    segmentation improve the robustness of DL-based trackers when significant IV occurs.
    To robustly deal with MC and OCC attributes, the DCF- and CNN-based trackers have
    performed the best. However, the SNN-based methods with the aid of region proposal
    subnetwork and proposal refinement can robustly estimate the tightest BB under
    severe scale changes. However, recently, the IoU-based refinement network (based
    on IoU-Net [[255](#bib.bib255)]) employed in ATOM, DiMP, and PrDiMP trackers can
    effectively handle aspect-ratio change of target during tracking.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The overall best methods (i.e., PrDiMP50 [[194](#bib.bib194)], DiMP50 [[159](#bib.bib159)],
    ATOM [[149](#bib.bib149)], VITAL [[114](#bib.bib114)], MDNet [[60](#bib.bib60)],
    DAT [[130](#bib.bib130)], ASRCF [[148](#bib.bib148)], SiamDW-SiamRPN [[154](#bib.bib154)],
    SiamRPN++ [[156](#bib.bib156)], C-RPN [[150](#bib.bib150)], StructSiam [[106](#bib.bib106)],
    SiamMask [[155](#bib.bib155)], DaSiamRPN [[104](#bib.bib104)], UPDT [[102](#bib.bib102)],
    LSART [[120](#bib.bib120)], DeepSTRCF [[115](#bib.bib115)], and DRT [[119](#bib.bib119)])
    belong to a wide range of network architectures. For instance, the MDNet, LSART,
    and DAT (uses the MDNet architecture) utilize CNNs to localize a visual target
    while the ASRCF, UPDT, DRT, and DeepSTRCF exploit deep off-the-shelf features.
    All the ATOM, DiMP, and PrDiMP trackers employ custom classification & refinement
    networks. Besides the VITAL that is a GAN-based tracker, the C-RPN, StructSiam,
    SiamMask, DaSiamRPN, SiamDW, and SiamRPN++ have the SNN architecture. Although
    the most recent attractive deep architectures for visual tracking are based on
    Siamese or custom networks, GAN- and RL-based trackers have been recently developed
    for some specific purposes, such as addressing the imbalance distribution of training
    samples [[114](#bib.bib114)] or selecting an appropriate real-time search strategy
    [[103](#bib.bib103), [162](#bib.bib162)]. GAN-based trackers can successfully
    augment positive samples to enrich the target appearance model. These trackers
    also enjoy cost-sensitive losses to focus on hard negative samples. RL-based trackers
    learn continuous actions to provide more reliable search & verification strategies
    for visual trackers. Besides, the combinations of RL-trackers with other architectures
    may add more advantages; for instance, recurrent RL-based tracking considers time
    dependencies to the key components (i.e., actions & states). By doing so, these
    trackers boost their performance by verifying confidence through an RNN motion
    model.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to providing a desirable balance between the performance and speed
    of Siamese or custom network-based trackers, the architectures are modified to
    integrate with diverse deep backbone networks, searching strategies, and learning
    schemes but also exploit fully convolutional networks, correlation layers, region
    proposal networks, video object detection/segmentation modules. The interesting
    point is that five SNN-based methods including the SiamDW-SiamRPN, SiamRPN++,
    C-RPN, SiamMask, and DaSiamRPN are based on the fast SiamRPN method [[116](#bib.bib116)],
    which is consisted of Siamese subnetwork and region proposal subnetwork; these
    subnetworks are leveraged for feature extraction and proposal extraction on correlation
    feature maps to solve the visual tracking problem by one-shot detection task.
    The main advantages of SiamRPN are the time efficiency and precise estimations
    with integrating proposal selection and refinement strategies into a Siamese network.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the ASRCF, UPDT, DRT, and DeepSTRCF, which exploit deep off-the-shelf
    features, are among the top-performing visual tracking methods. Moreover, five
    methods of UPDT, DeepSTRCF, DRT, LSART, and ASRCF take the advantages of the DCF
    framework. On the other side, the best performing visual trackers, namely PrDiMP50,
    DiMP50, ATOM, VITAL, MDNet, DAT, SiamDW, SiamRPN++, C-RPN, StructSiam, SiamMask,
    DaSiamRPN, and LSART exploit specialized deep features for visual tracking purpose.
    Although diversified backbone networks are employed for these methods, state-of-the-art
    methods have been leveraging deeper networks such as the ResNet-50 to strengthen
    the discriminative power of target modeling. From the network training perspective,
    the SiamDW-SiamRPN, SiamRPN++, C-RPN, StructSiam, SiamMask, and DaSiamRPN use
    offline training, the LSART utilizes online training, and the PrDiMP50, DiMP50,
    and ATOM take offline & online training procedures. In particular, the PrDiMP50
    and DiMP50 exploit meta-learning based networks to improve network adaptation
    for the tracking task. The offline trained trackers aim to provide dominant representations
    to achieve real-time tracking speed. Handling significant appearance variations
    needs to adapt to network parameters during tracking, but online training has
    an over-fitting risk because of limited training samples. Hence, the VITAL, MDNet,
    and DAT by employing adversarial learning, domain-independent information, and
    attention maps as regularization terms benefit both offline and online training
    of DNNs. However, these methods provide a tracking speed of about one frame per
    second (FPS) that is not suitable for real-time applications. In contrast, recent
    proposed PrDiMP50, DiMP50, and ATOM trackers exploit custom-designed networks,
    efficient optimization strategies to achieve acceptable tracking speed. From the
    perspective of the objective function of DNNs, the VITAL and StructSiam are classification-based,
    the LSART is regression-based, and the other best-performing trackers [[60](#bib.bib60),
    [104](#bib.bib104), [130](#bib.bib130), [150](#bib.bib150), [154](#bib.bib154),
    [155](#bib.bib155), [156](#bib.bib156), [194](#bib.bib194), [159](#bib.bib159),
    [149](#bib.bib149)] employ both classification and regression objectives. For
    instance, five modified versions of the SiamRPN [[116](#bib.bib116)] (i.e., SiamDW-SiamRPN
    [[154](#bib.bib154)], SiamRPN++ [[156](#bib.bib156)], C-RPN [[150](#bib.bib150)],
    SiamMask [[155](#bib.bib155)], and DaSiamRPN [[104](#bib.bib104)]) have two branches
    for classification and regression. Besides, the ATOM, DiMP50, and PrDiMP50 use
    a classification network for distinguishing target from the background and an
    IoU-Net for BB regression.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the motivation categorization of the best trackers, the recent advanced
    methods rely on 1) alleviating the imbalanced distribution of visual training
    data by the data augmentation [[102](#bib.bib102), [104](#bib.bib104)] and generative
    network from adversarial learning [[114](#bib.bib114)], 2) efficient training
    and learning procedures by reformulating classification/regression problems [[102](#bib.bib102),
    [104](#bib.bib104), [114](#bib.bib114), [115](#bib.bib115), [119](#bib.bib119),
    [120](#bib.bib120), [148](#bib.bib148), [149](#bib.bib149), [159](#bib.bib159),
    [194](#bib.bib194)] and providing specified features for visual tracking [[60](#bib.bib60),
    [104](#bib.bib104), [106](#bib.bib106), [114](#bib.bib114), [130](#bib.bib130),
    [150](#bib.bib150), [154](#bib.bib154), [155](#bib.bib155), [156](#bib.bib156),
    [149](#bib.bib149), [159](#bib.bib159), [194](#bib.bib194)], 3) exploiting state-of-the-art
    architectures to provide more discriminative representations by leveraging ResNet
    models as the backbone networks [[102](#bib.bib102), [154](#bib.bib154), [155](#bib.bib155),
    [156](#bib.bib156), [149](#bib.bib149), [159](#bib.bib159), [194](#bib.bib194)],
    and 4) extracting complementary features by employing additional information such
    as contextual [[102](#bib.bib102), [104](#bib.bib104), [106](#bib.bib106), [154](#bib.bib154)]
    or temporal information [[104](#bib.bib104), [114](#bib.bib114), [115](#bib.bib115),
    [130](#bib.bib130)]. The VITAL, DaSiamRPN, and UPDT attempt to alleviate the imbalanced
    distribution of positive and negative training data samples and extract more discriminative
    features. The VITAL uses adversarial learning to augment positive samples and
    decrease simple negative ones and preserve the most discriminative and robust
    features during tracking. Furthermore, the DaSiamRPN utilizes both data augmentation
    and negative semantic samples to consider visual distractors and improve visual
    tracking robustness. The UPDT uses standard data augmentation and a quality measure
    for estimated states to fuse shallow and deep features effectively. Finally, the
    ATOM employs standard data augmentation to improve its online adaptation, while
    the DiMP50 & PrDiMP50 trackers enjoy meta-learning strategies to form their training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: To improve the learning process of the best DL-based methods, the ATOM, UPDT,
    DeepSTRCF, DRT, LSART, and ASRCF have revised the conventional ridge regression
    of DCF formulation. Moreover, the DaSiamRPN and VITAL utilize the distractor-aware
    objective function and reformulated objective function of GANs using a cost-sensitive
    loss to improve the training process of these visual trackers, respectively. Finally,
    the PrDiMP tracker computes the similarity of predictive and ground-truth distributions
    by Kullback-Leibler (KL) divergence. Training of DL-based methods on large-scale
    datasets adapts their network function for visual tracking. The SiamDW, SiamRPN++,
    and SiamMask methods have aimed to leverage state-of-the-art deep networks as
    a backbone network of Siamese trackers. The ATOM, DiMP50, and PrDiMP tracker employ
    ResNet blocks as the backbone network, while the DiMP & PrDiMP train these blocks
    on tracking datasets. While these methods exploit ResNet models, the SiamDW proposes
    new residual modules and architectures to prevent significant receptive field
    increase and simultaneously improve feature discriminability and localization
    accuracy. Also, the ResNet-driven SNN-based tracker proposed by the SiamRPN++
    includes different layer-wise and depth-wise aggregations to fill the performance
    gap between SNN-based and CNN-based methods. In addition to the spatial information,
    the DAT (using reciprocative learning) and DeepSTRCF (using online passive-aggressive
    (PA) learning) also consider temporal information in different ways to provide
    more robust features. Generally, six learning schemes of the similarity learning
    (i.e., SiamDW, SiamRPN++, C-RPN, StructSiam, SiamMask, DaSiamRPN, ATOM), bluemeta-learning
    (i.e., PrDiMP50, DiMP50), multi-domain learning (i.e., MDNet, DAT), adversarial
    learning (i.e., VITAL), spatial-aware regressions learning (i.e., LSART), and
    DCF learning are utilized.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following, the best visual tracking methods are studied based on their
    advantages and disadvantage. The ATOM, DiMP, and PrDiMP trackers consider visual
    tracking as two-step classification and target estimation procedures. These trackers
    are robust to handle CM, MC, SV, and ARC attributes by employing custom networks
    and elaborated optimization strategies. However, the SOB, LR, and OB attributes
    can dramatically impact their performances. Three SNN-based methods of the C-RPN,
    StructSiam, and DaSiamRPN exploit the shallow AlexNet as their backbone network
    (see Table [II](#S2.T2 "TABLE II ‣ II-C2 Only Offline Training ‣ II-C Network
    Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")), which is the main weakness of these trackers according
    to their discriminative power. To improve tracking robustness in the presence
    of significant SV and visual DI, the C-RPN cascades multiple RPNs in a Siamese
    network to exploit from hard negative sampling (to provide more balanced training
    samples), multi-level features, and multiple steps of regressions. To decrease
    the sensitivity of SNN-based methods specifically for non-rigid appearance change
    and POC attributes, the StructSiam detects contextual information of local patterns
    and their relationships and matches them by a Siamese network in real-time speed.
    By adopting the local-to-global search strategy and the non-maximum suppression
    (NMS) to re-detect target and reduce potential distractors, the DaSiamRPN correctly
    handles the FOC, OV, POC, and BC challenges. In contrast, the SiamMask, SiamDW-SiamRPN,
    and SiamRPN++ exploit the ResNet models. To rely on rich target representation,
    the SiamMask uses three-branch architecture to estimate the target location by
    a rotated BB, including the target’s binary mask. The most failure reasons for
    SiamMask are the MB & OV attributes that produce erroneous target masks. To reduce
    the performance margin of the SNN-based methods with state-of-the-art visual tracking
    methods, the SiamDW-SiamRPN and SiamRPN++ study the exploitation of deep backbone
    networks to reduce the sensitivity of these methods to the most challenging attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: The MDNet and the other methods based on it (e.g., DAT) are still among the
    best visual tracking methods. Because of specialized offline and online training
    of these networks on large-scale visual tracking datasets, these methods can handle
    various challenging situations, hardly miss the visual targets, and have a satisfactory
    performance to track LR targets. However, these methods suffer from high computational
    complexity, intra-class discrimination of targets with similar semantics, and
    performing discrete space for scale estimation. The VITAL can tolerate massive
    DEF, IPR, and OPR because it focuses on hard negative samples through high-order
    cost-sensitive loss. However, it does not have a robust performance in the case
    of significant SV due to the producing a fixed size of weight mask via a generative
    network. The LSART utilizes the modified Kernelized ridge regression (KRR) by
    the weighted combination of patch-wise similarities to concentrate on the target’s
    reliable regions. Due to the consideration of rotation information and online
    adaptation of CNN models, this method provides promising responses to tackle the
    DEF and IPR challenges.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/48bfb96abe15bf1c32eceaefbbd869bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Qualitative comparison of state-of-the-art visual trackers on the
    BMX, Gymnastics3, and Singer3 video sequences from VOT2018 dataset. The #frame
    number and annotated attributes are shown on each frame.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The DeepSTRCF, ASRCF, DRT, and UPDT are the DCF-based methods that exploit
    deep off-the-shelf features and fuse them with shallow ones (e.g., HOG and CN)
    to improve the robustness of visual tracking (see Table [I](#S2.T1 "TABLE I ‣
    II-B1 Model Reuse or Deep Off-the-Shelf Features ‣ II-B Network Exploitation ‣
    II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")). To reduce the adverse impact of the OCC and OV attributes, the DeepSTRCF
    adds a temporal regularization term to the spatially regularized DCF formulation.
    The revisited formulation helps the DeepSTRCF enduring some appearance variations
    such as the IV, IPR, OPR, and POC. Using object-aware spatial regularization and
    reliability terms, the ASRCF and DRT methods attempt to optimize models to effectively
    learn adaptive correlation filters. Both these methods have studied major imperfections
    of DCF-based methods such as circular shifted sampling process, same feature space
    for localization and scale estimation processes, the strict focus on discrimination,
    and sparse and non-uniform distribution of correlation responses. Hence, these
    methods handle the DEF, BC, and SV, suitably. Finally, the UPDT focuses on enhancing
    the visual tracking robustness through independently training a shallow feature-based
    DCF and a deep off-the-shelf feature-based DCF and considering augmented training
    samples with an adaptive fusion model. Although these methods demonstrate the
    competitive performance of well-designed DCF-based trackers compared to more sophisticated
    trackers, they suffer from the limitations of pre-trained models, aspect ratio
    variation, model degradation, and considerable appearance variation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have modified the VOT toolkit to be able to compare state-of-the-art
    visual trackers qualitatively. Fig. [7](#S4.F7 "Figure 7 ‣ IV-C Discussion ‣ IV
    Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")
    shows the tracking results of the SiamRPN++ [[156](#bib.bib156)], SiamMask [[155](#bib.bib155)],
    LSART [[120](#bib.bib120)], UPDT [[102](#bib.bib102)], ATOM [[149](#bib.bib149)],
    and DiMP50 [[159](#bib.bib159)] on some video sequences of the VOT2018 dataset
    (modified toolkit & all videos are publicly available on the aforementioned page).
    According to the achieved results, the DiMP50, ATOM, and SiamRPN++ have provided
    the best results. However, failures usually happen when multiple critical attributes
    simultaneously occur in a scene. For instance, the SiamMask misuses the semi-supervised
    video object segmentation when the OCC and SV co-occur, or the significant SV
    dramatically reduces the performance of the SiamRPN++. Despite considerable advances
    that are emerged in visual tracking, the state-of-the-art visual trackers are
    still unable to handle serious real-world challenges; severe variations of target
    appearance, MOC, OCC, SV, CM, DEF, and even IV can have drastic effects on the
    performance, which may lead to tracking failures. These results demonstrate that
    the visual trackers are still not completely reliable for real-world applications
    because they lack the intelligence for scene understanding. Current trackers improve
    object-scene distinction, but they cannot infer scene information, immediately
    recognize the global/configural structure of a scene, or organize purposeful decisions
    based on space and acts within.'
  prefs: []
  type: TYPE_NORMAL
- en: V Conclusion and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The state-of-the-art DL-based visual trackers were categorized into a comprehensive
    taxonomy based on network architecture, network exploitation, training, network
    objective, network output, the exploitation of correlation filter advantages,
    aerial-view tracking, long-term tracking, and online tracking. Moreover, the motivations
    and contributions of these methods were categorized according to the main problems
    and proposed solutions of DL-based trackers. Furthermore, almost all visual tracking
    benchmark datasets and evaluation metrics were briefly investigated, and the various
    state-of-the-art trackers were compared on seven visual tracking datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the DL-based visual tracking methods have investigated different exploitation
    of deep off-she-shelf features, fusion of deep features & hand-crafted features,
    various architectures & backbone networks, offline & online training of DNNs on
    large-scale datasets, update schemes, search strategies, contextual information,
    temporal information, and how to deal with lacking training data. However, many
    problems are not precisely solved, and also other problems need to be explored
    in the future. In the following, some of these future directions are presented
    for more investigation.
  prefs: []
  type: TYPE_NORMAL
- en: First, the main concentration is to design custom neural networks to provide
    robustness, accuracy, and efficiency simultaneously. These trackers are primarily
    developed by integrating efficient network architectures with either classification
    & regression branches or two-step classification & BB refinement networks. Most
    recent works do not re-train/fine-tune their backbone networks to exploit generic
    features and avoid catastrophic forgetting of general patterns. However, diverse
    machine learning-based techniques to address this issue have been proposed, such
    as incremental learning [[256](#bib.bib256)], transfer learning penalties [[257](#bib.bib257)],
    batch spectral shrinkage [[258](#bib.bib258)], or lifelong learning [[259](#bib.bib259)].
    Thus, effective training of backbone networks can boost tracking performance.
  prefs: []
  type: TYPE_NORMAL
- en: Second, generic visual trackers are required to adapt to unseen targets quickly.
    Hence, efficient online training of neural networks is crucial. Recently, meta-/few-shot
    learning approaches are mainly used to find an optimal initialization of the base
    learner to a new target. But, the meta-networks need to be shallow to avoid over-fitting
    problems. Therefore, exploring effective few-shot learning approaches provides
    fast convergence of deeper networks.
  prefs: []
  type: TYPE_NORMAL
- en: Third, tracking from aerial-views introduces additional challenges for visual
    tracking. For instance, small/tiny object tracking in videos captured from medium/high-altitudes,
    severe viewpoint changes, and tracking many targets in dense environments should
    be considered. Furthermore, these scenarios are consistently involved with out-of-view
    and large occlusions; thus, developing long-term approaches will help more reliable
    aerial-view trackers.
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, long-term trackers are overlooked despite many advances in short-term
    trackers. In fact, long-term trackers are closer to practical, real-world scenarios
    when the target may disappear frequently or occlude for a long time. These trackers
    should have the ability to re-detect the target once a failure occurs and then
    continue tracking the correct target during video sequences. Thus, compelling
    detection & verification networks are needed to be designed.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, existing visual trackers have a deficiency in scene understanding.
    The state-of-the-art methods cannot interpret dynamic scenes in a meaningful way,
    immediately recognize global structures, infer existing objects, and perceive
    basic level categories of different objects or events. Although recent trackers
    desirably reduce the computational complexity, these trackers can be modified
    to employ complementary features (e.g., temporal information) and incorporate
    proposed adversarial learning contributions in this few-data regime task.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We wish to thank Prof. Kamal Nasrollahi (Visual Analysis of People Lab (VAP),
    Aalborg University, Denmark) for his beneficial comments.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] M. Gao, L. Jin, Y. Jiang, and B. Guo, “Manifold Siamese network: A novel
    visual tracking ConvNet for autonomous vehicles,” *IEEE Trans. Intell. Transp.
    Sys.*, pp. 1–12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] C. Robin and S. Lacroix, “Multi-robot target detection and tracking: Taxonomy
    and survey,” *Autonomous Robots*, vol. 40, no. 4, pp. 729–760, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] K. Lee, J. Hwang, G. Okopal, and J. Pitton, “Ground-moving-platform-based
    human tracking using visual SLAM and constrained multiple kernels,” *IEEE Trans.
    Intell. Transp. Sys.*, vol. 17, no. 12, pp. 3602–3612, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] F. Ababsa, M. Maidi, J. Y. Didier, and M. Mallem, “Vision-based tracking
    for mobile augmented reality,” in *Studies in Computational Intelligence*.   Springer,
    2008, vol. 120, pp. 297–326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. Hao, Y. Zhou, G. Zhang, Q. Lv, and Q. Wu, “A review of target tracking
    algorithm based on UAV,” in *Proc. IEEE CBS*, 2019, pp. 328–333.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. Manafifard, H. Ebadi, and H. Abrishami Moghaddam, “A survey on player
    tracking in soccer videos,” *Comput. Vis. Image Und.*, vol. 159, pp. 19–46, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D. Bouget, M. Allan, D. Stoyanov, and P. Jannin, “Vision-based and marker-less
    surgical tool detection and tracking: A review of the literature,” *Medical Image
    Analysis*, vol. 35, pp. 633–654, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] V. Ulman, M. Maška, and et al., “An objective comparison of cell-tracking
    algorithms,” *Nature Methods*, vol. 14, no. 12, pp. 1141–1152, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] J. Luo, Y. Han, and L. Fan, “Underwater acoustic target tracking: A review,”
    *Sensors*, vol. 18, no. 1, p. 112, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, “High-speed tracking
    with kernelized correlation filters,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 37, no. 3, pp. 583–596, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] G. Ding, W. Chen, S. Zhao, J. Han, and Q. Liu, “Real-time scalable visual
    tracking via quadrangle kernelized correlation filters,” *IEEE Trans. Intell.
    Transp. Sys.*, vol. 19, no. 1, pp. 140–150, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S. M. Marvasti-Zadeh, H. Ghanei-Yakhdan, and S. Kasaei, “Rotation-aware
    discriminative scale space tracking,” in *Iranian Conf. Electrical Engineering
    (ICEE)*, 2019, pp. 1272–1276.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S. M. Marvasti-Zadeh, H. Ghanei Yakhdan, and S. Kasaei, “Adaptive exploitation
    of pre-trained deep convolutional neural networks for robust visual tracking,”
    *Multimedia Tools and Applications*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] S. M. Marvasti Zadeh, H. Ghanei-Yakhdan, and S. Kasaei, “Beyond background-aware
    correlation filters: Adaptive context modeling by hand-crafted and deep rgb features
    for visual tracking,” 2020\. [Online]. Available: [http://arxiv.org/abs/2004.02932](http://arxiv.org/abs/2004.02932)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. M. Marvasti-Zadeh, H. Ghanei Yakhdan, and S. Kasaei, “Efficient scale
    estimation methods using lightweight deep convolutional neural networks for visual
    tracking,” *Neural Computing and Applications*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S. M. Marvasti-Zadeh, H. Ghanei-Yakhdan, S. Kasaei, K. Nasrollahi, and
    T. B. Moeslund, “Effective fusion of deep multitasking representations for robust
    visual tracking,” 2020\. [Online]. Available: [http://arxiv.org/abs/2004.01382](http://arxiv.org/abs/2004.01382)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] C. Xiao and A. Yilmaz, “Efficient tracking with distinctive target colors
    and silhouette,” in *Proc. ICPR*, 2016, pp. 2728–2733.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] V. Bruni and D. Vitulano, “An improvement of kernel-based object tracking
    based on human perception,” *IEEE Trans. Syst., Man, Cybern. Syst.*, vol. 44,
    no. 11, pp. 1474–1485, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] I. I. Lychkov, A. N. Alfimtsev, and S. A. Sakulin, “Tracking of moving
    objects with regeneration of object feature points,” in *Proc. GloSIC*, 2018,
    pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,”
    in *Proc. IEEE CVPR*, 2005, pp. 886–893.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. Van De Weijer, C. Schmid, and J. Verbeek, “Learning color names from
    real-world images,” in *Proc. IEEE CVPR*, 2007, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg, “Learning spatially
    regularized correlation filters for visual tracking,” in *Proc. IEEE ICCV*, 2015,
    pp. 4310–4318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. Danelljan, G. Häger, F. S. Khan, and M. Felsberg, “Adaptive decontamination
    of the training set: A unified formulation for discriminative visual tracking,”
    in *Proc. IEEE CVPR*, 2016, pp. 1430–1438.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] H. K. Galoogahi, A. Fagg, and S. Lucey, “Learning background-aware correlation
    filters for visual tracking,” in *Proc. IEEE ICCV*, 2017, pp. 1144–1152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Li, C. Fu, F. Ding, Z. Huang, and G. Lu, “AutoTrack: Towards high-performance
    visual tracking for UAV with automatic spatio-temporal regularization,” in *Proc.
    IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Z. Huang, C. Fu, Y. Li, F. Lin, and P. Lu, “Learning aberrance repressed
    correlation filters for real-time UAV tracking,” in *Proc. IEEE ICCV*, 2019, pp.
    2891–2900.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] F. Li, C. Fu, F. Lin, Y. Li, and P. Lu, “Training-set distillation for
    real-time UAV object tracking,” in *Proc. ICRA*, 2020, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification
    with deep convolutional neural networks,” in *Proc. NIPS*, vol. 2, 2012, pp. 1097–1105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the
    devil in the details: Delving deep into convolutional nets,” in *Proc. BMVC*,
    2014, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *Proc. ICLR*, 2014, pp. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proc. IEEE
    CVPR*, 2015, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proc. IEEE CVPR*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet large
    scale visual recognition challenge,” *IJCV*, vol. 115, no. 3, pp. 211–252, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] M. Kristan, R. Pflugfelder, A. Leonardis, J. Matas, F. Porikli, and et al.,
    “The visual object tracking VOT2013 challenge results,” in *Proc. ICCV*, 2013,
    pp. 98–111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] M. Kristan, R. Pflugfelder, A. Leonardis, J. Matas, and et al., “The visual
    object tracking VOT2014 challenge results,” in *Proc. ECCV*, 2015, pp. 191–217.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, and et al., “The visual
    object tracking VOT2015 challenge results,” in *Proc. IEEE ICCV*, 2015, pp. 564–586.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, R. Pflugfelder, and et al.,
    “The visual object tracking VOT2016 challenge results,” in *Proc. ECCVW*, 2016,
    pp. 777–823.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pflugfelder, L. C.
    Zajc, and et al., “The visual object tracking VOT2017 challenge results,” in *Proc.
    IEEE ICCVW*, 2017, pp. 1949–1972.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pflugfelder, and et al.,
    “The sixth visual object tracking VOT2018 challenge results,” in *Proc. ECCVW*,
    2019, pp. 3–53.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] M. Kristan and et al., “The seventh visual object tracking VOT2019 challenge
    results,” in *Proc. ICCVW*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] A. Yilmaz, O. Javed, and M. Shah, “Object tracking: A survey,” *ACM Computing
    Surveys*, vol. 38, no. 4, Dec. 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. W. Smeulders, D. M. Chu, R. Cucchiara, S. Calderara, A. Dehghan, and
    M. Shah, “Visual tracking: An experimental survey,” *IEEE Trans. Pattern Anal.
    Mach. Intell.*, vol. 36, no. 7, pp. 1442–1468, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] H. Yang, L. Shao, F. Zheng, L. Wang, and Z. Song, “Recent advances and
    trends in visual tracking: A review,” *Neurocomputing*, vol. 74, no. 18, pp. 3823–3831,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] X. Li, W. Hu, C. Shen, Z. Zhang, A. Dick, and A. Van Den Hengel, “A survey
    of appearance models in visual object tracking,” *ACM Trans. Intell. Syst. Tec.*,
    vol. 4, no. 4, pp. 58:1—-58:48, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C. Fu, Z. Huang, Y. Li, R. Duan, and P. Lu, “Boundary effect-aware visual
    tracking for UAV with online enhanced background learning and multi-frame consensus
    verification,” in *Proc. IROS*, 2019, pp. 4415–4422.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Li, C. Fu, Z. Huang, Y. Zhang, and J. Pan, “Keyfilter-aware real-time
    uav object tracking,” in *Proc. ICRA*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M. Fiaz, A. Mahmood, and S. K. Jung, “Tracking noisy targets: A review
    of recent object tracking approaches,” 2018\. [Online]. Available: [http://arxiv.org/abs/1802.03098](http://arxiv.org/abs/1802.03098)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] M. Fiaz, A. Mahmood, S. Javed, and S. K. Jung, “Handcrafted and deep trackers:
    Recent visual object tracking approaches and trends,” *ACM Computing Surveys*,
    vol. 52, no. 2, pp. 43:1—-43:44, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] P. Li, D. Wang, L. Wang, and H. Lu, “Deep visual tracking: Review and
    experimental comparison,” *Pattern Recognit.*, vol. 76, pp. 323–338, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] R. Pflugfelder, “An in-depth analysis of visual tracking with Siamese
    neural networks,” 2017\. [Online]. Available: [http://arxiv.org/abs/1707.00569](http://arxiv.org/abs/1707.00569)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] C. Ma, J. B. Huang, X. Yang, and M. H. Yang, “Hierarchical convolutional
    features for visual tracking,” in *Proc. IEEE ICCV*, 2015, pp. 3074–3082.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg, “Convolutional features
    for correlation filter based visual tracking,” in *Proc. IEEE ICCVW*, 2016, pp.
    621–629.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] L. Wang, W. Ouyang, X. Wang, and H. Lu, “Visual tracking with fully convolutional
    networks,” in *Proc. IEEE ICCV*, 2015, pp. 3119–3127.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Hong, T. You, S. Kwak, and B. Han, “Online tracking by learning discriminative
    saliency map with convolutional neural network,” in *Proc. ICML*, 2015, pp. 597–606.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Zha, T. Ku, Y. Li, and P. Zhang, “Deep position-sensitive tracking,”
    *IEEE Trans. Multimedia*, no. 8, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg, “Beyond correlation
    filters: Learning continuous convolution operators for visual tracking,” in *Proc.
    ECCV*, vol. 9909 LNCS, 2016, pp. 472–488.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] D. Held, S. Thrun, and S. Savarese, “Learning to track at 100 FPS with
    deep regression networks,” in *Proc. ECCV*, 2016, pp. 749–765.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr,
    “Fully-convolutional Siamese networks for object tracking,” in *Proc. ECCV*, 2016,
    pp. 850–865.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] R. Tao, E. Gavves, and A. W. Smeulders, “Siamese instance search for tracking,”
    in *Proc. IEEE CVPR*, 2016, pp. 1420–1429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] H. Nam and B. Han, “Learning multi-domain convolutional neural networks
    for visual tracking,” in *Proc. IEEE CVPR*, 2016, pp. 4293–4302.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, J. Lim, and M. H. Yang, “Hedged
    deep tracking,” in *Proc. IEEE CVPR*, 2016, pp. 4303–4311.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] L. Wang, W. Ouyang, X. Wang, and H. Lu, “STCT: Sequentially training convolutional
    networks for visual tracking,” in *Proc. IEEE CVPR*, 2016, pp. 1373–1381.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] G. Zhu, F. Porikli, and H. Li, “Robust visual tracking with deep convolutional
    neural network based object proposals on PETS,” in *Proc. IEEE CVPRW*, 2016, pp.
    1265–1272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] H. Li and et al., “DeepTrack: Learning discriminative feature representations
    online for robust visual tracking,” *IEEE Trans. Image Process.*, vol. 25, no. 4,
    pp. 1834–1848, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] H. Li, Y. Li, and F. Porikli, “DeepTrack: Learning discriminative feature
    representations by convolutional neural networks for visual tracking,” in *Proc.
    BMVC*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] K. Zhang, Q. Liu, Y. Wu, and M. H. Yang, “Robust visual tracking via convolutional
    networks without training,” *IEEE Trans. Image Process.*, vol. 25, no. 4, pp.
    1779–1792, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] C. Ma, Y. Xu, B. Ni, and X. Yang, “When correlation filters meet convolutional
    neural networks for visual tracking,” *IEEE Signal Process. Lett.*, vol. 23, no. 10,
    pp. 1454–1458, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] H. Nam, M. Baek, and B. Han, “Modeling and propagating CNNs in a tree
    structure for visual tracking,” 2016\. [Online]. Available: [http://arxiv.org/abs/1608.07242](http://arxiv.org/abs/1608.07242)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] G. Wu, W. Lu, G. Gao, C. Zhao, and J. Liu, “Regional deep learning model
    for visual tracking,” *Neurocomputing*, vol. 175, no. PartA, pp. 310–323, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] H. Fan and H. Ling, “Parallel tracking and verifying: A framework for
    real-time and high accuracy visual tracking,” in *Proc. IEEE ICCV*, 2017, pp.
    5487–5495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] H. Fan and H.Ling, “Parallel tracking and verifying,” *IEEE Trans. Image
    Process.*, vol. 28, no. 8, pp. 4130–4144, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Y. Song, C. Ma, L. Gong, J. Zhang, R. W. Lau, and M. H. Yang, “CREST:
    Convolutional residual learning for visual tracking,” in *Proc. ICCV*, 2017, pp.
    2574–2583.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Z. Zhu, G. Huang, W. Zou, D. Du, and C. Huang, “UCT: Learning unified
    convolutional networks for real-time visual tracking,” in *Proc. ICCVW*, 2018,
    pp. 1973–1982.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Q. Guo, W. Feng, C. Zhou, R. Huang, L. Wan, and S. Wang, “Learning dynamic
    Siamese network for visual object tracking,” in *Proc. IEEE ICCV*, 2017, pp. 1781–1789.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Z. Teng, J. Xing, Q. Wang, C. Lang, S. Feng, and Y. Jin, “Robust object
    tracking based on temporal and spatial deep networks,” in *Proc. IEEE ICCV*, 2017,
    pp. 1153–1162.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Z. He, Y. Fan, J. Zhuang, Y. Dong, and H. Bai, “Correlation filters with
    weighted convolution responses,” in *Proc. ICCVW*, 2018, pp. 1992–2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] T. Yang and A. B. Chan, “Recurrent filter learning for visual tracking,”
    in *Proc. ICCVW*, 2018, pp. 2010–2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] F. Li, Y. Yao, P. Li, D. Zhang, W. Zuo, and M. H. Yang, “Integrating boundary
    and center correlation filters for visual tracking with aspect ratio variation,”
    in *Proc. IEEE ICCVW*, 2018, pp. 2001–2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] X. Wang, H. Li, Y. Li, F. Porikli, and M. Wang, “Deep tracking with objectness,”
    in *Proc. ICIP*, 2018, pp. 660–664.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] X. Xu, B. Ma, H. Chang, and X. Chen, “Siamese recurrent architecture for
    visual tracking,” in *Proc. ICIP*, 2018, pp. 1152–1156.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] L. Yang, P. Jiang, F. Wang, and X. Wang, “Region-based fully convolutional
    Siamese networks for robust real-time visual tracking,” in *Proc. ICIP*, 2017,
    pp. 2567–2571.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] T. Kokul, C. Fookes, S. Sridharan, A. Ramanan, and U. A. J. Pinidiyaarachchi,
    “Gate connected convolutional neural network for object tracking,” in *Proc. ICIP*,
    2017, pp. 2602–2606.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] K. Dai, Y. Wang, and X. Yan, “Long-term object tracking based on Siamese
    network,” in *Proc. ICIP*, 2017, pp. 3640–3644.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] B. Akok, F. Gurkan, O. Kaplan, and B. Gunsel, “Robust object tracking
    by interleaving variable rate color particle filtering and deep learning,” in
    *Proc. ICIP*, 2017, pp. 3665–3669.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] R. J. Mozhdehi and H. Medeiros, “Deep convolutional particle filter for
    visual tracking,” in *Proc. IEEE ICIP*, 2017, pp. 3650–3654.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, and P. H. Torr,
    “End-to-end representation learning for correlation filter based tracking,” in
    *Proc. IEEE CVPR*, 2017, pp. 5000–5008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg, “ECO: Efficient
    convolution operators for tracking,” in *Proc. IEEE CVPR*, 2017, pp. 6931–6939.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] A. Lukežič, T. Vojíř, L. Čehovin Zajc, J. Matas, and M. Kristan, “Discriminative
    correlation filter tracker with channel and spatial reliability,” *IJCV*, vol.
    126, no. 7, pp. 671–688, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] T. Zhang, C. Xu, and M. H. Yang, “Multi-task correlation particle filter
    for robust object tracking,” in *Proc. IEEE CVPR*, 2017, pp. 4819–4827.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] B. Han, J. Sim, and H. Adam, “BranchOut: Regularization for online ensemble
    tracking with convolutional neural networks,” in *Proc. IEEE CVPR*, 2017, pp.
    521–530.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] M. Wang, Y. Liu, and Z. Huang, “Large margin object tracking with circulant
    feature maps,” in *Proc. IEEE CVPR*, 2017, pp. 4800–4808.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] L. Zhang, J. Varadarajan, P. N. Suganthan, N. Ahuja, and P. Moulin, “Robust
    visual tracking using oblique random forests,” in *Proc. IEEE CVPR*, 2017, pp.
    5825–5834.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Choi, H. J. Chang, S. Yun, T. Fischer, Y. Demiris, and J. Y. Choi,
    “Attentional correlation filter network for adaptive visual tracking,” in *Proc.
    IEEE CVPR*, 2017, pp. 4828–4837.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] H. Fan and H. Ling, “SANet: Structure-aware network for visual tracking,”
    in *Proc. IEEE CVPRW*, 2017, pp. 2217–2224.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Q. Wang, J. Gao, J. Xing, M. Zhang, and W. Hu, “DCFNet: Discriminant correlation
    filters network for visual tracking,” 2017\. [Online]. Available: [http://arxiv.org/abs/1704.04057](http://arxiv.org/abs/1704.04057)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. Guo and T. Xu, “Deep ensemble tracking,” *IEEE Signal Process. Lett.*,
    vol. 24, no. 10, pp. 1562–1566, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. Gao, T. Zhang, X. Yang, and C. Xu, “Deep relative tracking,” *IEEE
    Trans. Image Process.*, vol. 26, no. 4, pp. 1845–1858, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Z. Chi, H. Li, H. Lu, and M. H. Yang, “Dual deep network for visual tracking,”
    *IEEE Trans. Image Process.*, vol. 26, no. 4, pp. 2005–2015, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] P. Zhang, T. Zhuo, W. Huang, K. Chen, and M. Kankanhalli, “Online object
    tracking based on CNN with spatial-temporal saliency guided sampling,” *Neurocomputing*,
    vol. 257, pp. 115–127, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Dong and J. Shen, “Triplet loss in Siamese network for object tracking,”
    in *Proc. ECCV*, vol. 11217 LNCS, 2018, pp. 472–488.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] X. Lu, C. Ma, B. Ni, X. Yang, I. Reid, and M. H. Yang, “Deep regression
    tracking with shrinkage loss,” in *Proc. ECCV*, 2018, pp. 369–386.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] G. Bhat, J. Johnander, M. Danelljan, F. S. Khan, and M. Felsberg, “Unveiling
    the power of deep tracking,” in *Proc. ECCV*, 2018, pp. 493–509.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] B. Chen, D. Wang, P. Li, S. Wang, and H. Lu, “Real-time ‘actor-critic’
    tracking,” in *Proc. ECCV*, 2018, pp. 328–345.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Z. Zhu, Q. Wang, B. Li, W. Wu, J. Yan, and W. Hu, “Distractor-aware Siamese
    networks for visual object tracking,” in *Proc. ECCV*, vol. 11213 LNCS, 2018,
    pp. 103–119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] I. Jung, J. Son, M. Baek, and B. Han, “Real-time MDNet,” in *Proc. ECCV*,
    2018, pp. 89–104.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Y. Zhang, L. Wang, J. Qi, D. Wang, M. Feng, and H. Lu, “Structured Siamese
    network for real-time visual tracking,” in *Proc. ECCV*, 2018, pp. 355–370.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] H. Lee, S. Choi, and C. Kim, “A memory model based on the Siamese network
    for long-term tracking,” in *Proc. ECCVW*, 2019, pp. 100–115.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] M. Che, R. Wang, Y. Lu, Y. Li, H. Zhi, and C. Xiong, “Channel pruning
    for visual tracking,” in *Proc. ECCVW*, 2019, pp. 70–82.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] E. Burceanu and M. Leordeanu, “Learning a robust society of tracking
    parts using co-occurrence constraints,” in *Proc. ECCVW*, 2019, pp. 162–178.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] H. Morimitsu, “Multiple context features in Siamese networks for visual
    object tracking,” in *Proc. ECCVW*, 2019, pp. 116–131.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] A. He, C. Luo, X. Tian, and W. Zeng, “Towards a better match in Siamese
    network based visual object tracker,” in *Proc. ECCVW*, 2019, pp. 132–147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] L. Rout, D. Mishra, and R. K. S. S. Gorthi, “WAEF: Weighted aggregation
    with enhancement filter for visual object tracking,” in *Proc. ECCVW*, 2019, pp.
    83–99.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] J. Choi, H. J. Chang, T. Fischer, S. Yun, K. Lee, J. Jeong, Y. Demiris,
    and J. Y. Choi, “Context-aware deep feature compression for high-speed visual
    tracking,” in *Proc. IEEE CVPR*, 2018, pp. 479–488.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Y. Song, C. Ma, X. Wu, L. Gong, L. Bao, W. Zuo, C. Shen, R. W. Lau, and
    M. H. Yang, “VITAL: Visual tracking via adversarial learning,” in *Proc. IEEE
    CVPR*, 2018, pp. 8990–8999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] F. Li, C. Tian, W. Zuo, L. Zhang, and M. H. Yang, “Learning spatial-temporal
    regularized correlation filters for visual tracking,” in *Proc. IEEE CVPR*, 2018,
    pp. 4904–4913.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, “High performance visual tracking
    with Siamese region proposal network,” in *Proc. IEEE CVPR*, 2018, pp. 8971–8980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] A. He, C. Luo, X. Tian, and W. Zeng, “A twofold Siamese network for real-time
    object tracking,” in *Proc. IEEE CVPR*, 2018, pp. 4834–4843.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Z. Zhu, W. Wu, W. Zou, and J. Yan, “End-to-end flow correlation tracking
    with spatial-temporal attention,” in *Proc. IEEE CVPR*, 2018, pp. 548–557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] C. Sun, D. Wang, H. Lu, and M. H. Yang, “Correlation tracking via joint
    discrimination and reliability learning,” in *Proc. IEEE CVPR*, 2018, pp. 489–497.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] C. Sun, D. Wang, H. Lu, and M. Yang, “Learning spatial-aware regressions
    for visual tracking,” in *Proc. IEEE CVPR*, 2018, pp. 8962–8970.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Q. Wang, Z. Teng, J. Xing, J. Gao, W. Hu, and S. Maybank, “Learning attentions:
    Residual attentional Siamese network for high performance online visual tracking,”
    in *Proc. IEEE CVPR*, 2018, pp. 4854–4863.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] N. Wang, W. Zhou, Q. Tian, R. Hong, M. Wang, and H. Li, “Multi-cue correlation
    filters for robust visual tracking,” in *Proc. IEEE CVPR*, 2018, pp. 4844–4853.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] R. J. Mozhdehi, Y. Reznichenko, A. Siddique, and H. Medeiros, “Deep convolutional
    particle filter with adaptive correlation maps for visual tracking,” in *Proc.
    ICIP*, 2018, pp. 798–802.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Z. Lin and C. Yuan, “Robust visual tracking in low-resolution sequence,”
    in *Proc. ICIP*, 2018, pp. 4103–4107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] M. Cen and C. Jung, “Fully convolutional Siamese fusion networks for
    object tracking,” in *Proc. ICIP*, 2018, pp. 3718–3722.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] G. Wang, B. Liu, W. Li, and N. Yu, “Flow guided Siamese network for visual
    tracking,” in *Proc. ICIP*, 2018, pp. 231–235.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] K. Dai, Y. Wang, X. Yan, and Y. Huo, “Fusion of template matching and
    foreground detection for robust visual tracking,” in *Proc. ICIP*, 2018, pp. 2720–2724.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] G. Liu and G. Liu, “Integrating multi-level convolutional features for
    correlation filter tracking,” in *Proc. ICIP*, 2018, pp. 3029–3033.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] J. Guo, T. Xu, S. Jiang, and Z. Shen, “Generating reliable online adaptive
    templates for visual tracking,” in *Proc. ICIP*, 2018, pp. 226–230.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S. Pu, Y. Song, C. Ma, H. Zhang, and M. H. Yang, “Deep attentive tracking
    via reciprocative learning,” in *Proc. NIPS*, 2018, pp. 1931–1941.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] X. Jiang, X. Zhen, B. Zhang, J. Yang, and X. Cao, “Deep collaborative
    tracking networks,” in *Proc. BMVC*, 2018, p. 87.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] D. Ma, W. Bu, and X. Wu, “Multi-scale recurrent tracking via pyramid
    recurrent network and optical flow,” in *Proc. BMVC*, 2018, p. 242.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] C. Ma, J. B. Huang, X. Yang, and M. H. Yang, “Robust visual tracking
    via hierarchical convolutional features,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Z. Han, P. Wang, and Q. Ye, “Adaptive discriminative deep correlation
    filter for visual object tracking,” *IEEE Trans. Circuits Syst. Video Technol.*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] K. Chen and W. Tao, “Once for all: A two-flow convolutional neural network
    for visual tracking,” *IEEE Trans. Circuits Syst. Video Technol.*, vol. 28, no. 12,
    pp. 3377–3386, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] S. Li, S. Zhao, B. Cheng, E. Zhao, and J. Chen, “Robust visual tracking
    via hierarchical particle filter and ensemble deep features,” *IEEE Trans. Circuits
    Syst. Video Technol.*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] E. Gundogdu and A. A. Alatan, “Good features to correlate for visual
    tracking,” *IEEE Trans. Image Process.*, vol. 27, no. 5, pp. 2526–2540, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Y. Xie, J. Xiao, K. Huang, J. Thiyagalingam, and Y. Zhao, “Correlation
    filter selection for visual tracking using reinforcement learning,” *IEEE Trans.
    Circuits Syst. Video Technol.*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Gao, T. Zhang, X. Yang, and C. Xu, “P2T: Part-to-target tracking via
    deep regression learning,” *IEEE Trans. Image Process.*, vol. 27, no. 6, pp. 3074–3086,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] C. Peng, F. Liu, J. Yang, and N. Kasabov, “Densely connected discriminative
    correlation filters for visual tracking,” *IEEE Signal Process. Lett.*, vol. 25,
    no. 7, pp. 1019–1023, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] D. Li, G. Wen, Y. Kuai, and F. Porikli, “End-to-end feature integration
    for correlation filter tracking with channel attention,” *IEEE Signal Process.
    Lett.*, vol. 25, no. 12, pp. 1815–1819, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] C. Ma, J. B. Huang, and et al., “Adaptive correlation filters with long-term
    and short-term memory for object tracking,” *IJCV*, vol. 126, no. 8, pp. 771–796,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Y. Cao, H. Ji, W. Zhang, and F. Xue, “Learning spatio-temporal context
    via hierarchical features for visual tracking,” *Signal Proc.: Image Comm.*, vol. 66,
    pp. 50–65, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] F. Du, P. Liu, W. Zhao, and X. Tang, “Spatial–temporal adaptive feature
    weighted correlation filter for visual tracking,” *Signal Proc.: Image Comm.*,
    vol. 67, pp. 58–70, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Y. Kuai, G. Wen, and D. Li, “When correlation filters meet fully-convolutional
    Siamese networks for distractor-aware tracking,” *Signal Proc.: Image Comm.*,
    vol. 64, pp. 107–117, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] W. Gan, M. S. Lee, C. hao Wu, and C. C. Kuo, “Online object tracking
    via motion-guided convolutional neural network (MGNet),” *J. VIS. COMMUN. IMAGE
    R.*, vol. 53, pp. 180–191, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] M. Liu, C. B. Jin, B. Yang, X. Cui, and H. Kim, “Occlusion-robust object
    tracking based on the confidence of online selected hierarchical features,” *IET
    Image Proc.*, vol. 12, no. 11, pp. 2023–2029, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] K. Dai, D. Wang, H. Lu, C. Sun, and J. Li, “Visual tracking via adaptive
    spatially-regularized correlation filters,” in *Proc. CVPR*, 2019, pp. 4670–4679.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] M. Danelljan, G. Bhat, F. S. Khan, and M. Felsberg, “ATOM: Accurate tracking
    by overlap maximization,” 2018\. [Online]. Available: [http://arxiv.org/abs/1811.07628](http://arxiv.org/abs/1811.07628)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] H. Fan and H. Ling, “Siamese cascaded region proposal networks for real-time
    visual tracking,” 2018\. [Online]. Available: [http://arxiv.org/abs/1812.06148](http://arxiv.org/abs/1812.06148)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] J. Gao, T. Zhang, and C. Xu, “Graph convolutional tracking,” in *Proc.
    CVPR*, 2019, pp. 4649–4659.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Y. Sun, C. Sun, D. Wang, Y. He, and H. Lu, “ROI pooled correlation filters
    for visual tracking,” in *Proc. CVPR*, 2019, pp. 5783–5791.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] G. Wang, C. Luo, Z. Xiong, and W. Zeng, “Spm-tracker: Series-parallel
    matching for real-time visual object tracking,” 2019\. [Online]. Available: [http://arxiv.org/abs/1904.04452](http://arxiv.org/abs/1904.04452)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Z. Zhang and H. Peng, “Deeper and wider Siamese networks for real-time
    visual tracking,” 2019\. [Online]. Available: [http://arxiv.org/abs/1901.01660](http://arxiv.org/abs/1901.01660)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. S. Torr, “Fast online
    object tracking and segmentation: A unifying approach,” 2018\. [Online]. Available:
    [http://arxiv.org/abs/1812.05050](http://arxiv.org/abs/1812.05050)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan, “SiamRPN++: Evolution
    of Siamese visual tracking with very deep networks,” 2018\. [Online]. Available:
    [http://arxiv.org/abs/1812.11703](http://arxiv.org/abs/1812.11703)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] X. Li, C. Ma, B. Wu, Z. He, and M.-H. Yang, “Target-aware deep tracking,”
    2019\. [Online]. Available: [http://arxiv.org/abs/1904.01772](http://arxiv.org/abs/1904.01772)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] N. Wang, Y. Song, C. Ma, W. Zhou, W. Liu, and H. Li, “Unsupervised deep
    tracking,” 2019\. [Online]. Available: [http://arxiv.org/abs/1904.01828](http://arxiv.org/abs/1904.01828)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] G. Bhat, M. Danelljan, L. V. Gool, and R. Timofte, “Learning discriminative
    model prediction for tracking,” 2019\. [Online]. Available: [http://arxiv.org/abs/1904.07220](http://arxiv.org/abs/1904.07220)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] F. Zhao, J. Wang, Y. Wu, and M. Tang, “Adversarial deep tracking,” *IEEE
    Trans. Circuits Syst. Video Technol.*, vol. 29, no. 7, pp. 1998–2011, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] H. Li, X. Wang, F. Shen, Y. Li, F. Porikli, and M. Wang, “Real-time deep
    tracking via corrective domain adaptation,” *IEEE Trans. Circuits Syst. Video
    Technol.*, vol. 8215, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] B. Zhong, B. Bai, J. Li, Y. Zhang, and Y. Fu, “Hierarchical tracking
    by reinforcement learning-based searching and coarse-to-fine verifying,” *IEEE
    Trans. Image Process.*, vol. 28, no. 5, pp. 2331–2341, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] J. Gao, T. Zhang, and C. Xu, “SMART: Joint sampling and regression for
    visual tracking,” *IEEE Trans. Image Process.*, vol. 28, no. 8, pp. 3923–3935,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] H. Hu, B. Ma, J. Shen, H. Sun, L. Shao, and F. Porikli, “Robust object
    tracking using manifold regularized convolutional neural networks,” *IEEE Trans.
    Multimedia*, vol. 21, no. 2, pp. 510–521, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] L. Wang, L. Zhang, J. Wang, and Z. Yi, “Memory mechanisms for discriminative
    visual tracking algorithms with deep neural networks,” *IEEE Trans. Cogn. Devel.
    Syst.*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Y. Kuai, G. Wen, and D. Li, “Multi-task hierarchical feature learning
    for real-time visual tracking,” *IEEE Sensors J.*, vol. 19, no. 5, pp. 1961–1968,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] X. Cheng, Y. Zhang, L. Zhou, and Y. Zheng, “Visual tracking via Auto-Encoder
    pair correlation filter,” *IEEE Trans. Ind. Electron.*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] F. Tang, X. Lu, X. Zhang, S. Hu, and H. Zhang, “Deep feature tracking
    based on interactive multiple model,” *Neurocomputing*, vol. 333, pp. 29–40, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] X. Lu, B. Ni, C. Ma, and X. Yang, “Learning transform-aware attentive
    network for object tracking,” *Neurocomputing*, vol. 349, pp. 133–144, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] D. Li, G. Wen, Y. Kuai, J. Xiao, and F. Porikli, “Learning target-aware
    correlation filters for visual tracking,” *J. VIS. COMMUN. IMAGE R.*, vol. 58,
    pp. 149–159, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] B. Chen, P. Li, C. Sun, D. Wang, G. Yang, and H. Lu, “Multi attention
    module for visual tracking,” *Pattern Recognit.*, vol. 87, pp. 80–93, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] S. Yun, J. J. Y. Choi, Y. Yoo, K. Yun, and J. J. Y. Choi, “Action-decision
    networks for visual tracking with deep reinforcement learning,” in *Proc. IEEE
    CVPR*, 2016, pp. 2–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] S. Yun, J. Choi, Y. Yoo, K. Yun, and J. Y. Choi, “Action-driven visual
    object tracking with deep reinforcement learning,” *IEEE Trans. Neural Netw. Learn.
    Syst.*, vol. 29, no. 6, pp. 2239–2252, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] W. Zhang, K. Song, X. Rong, and Y. Li, “Coarse-to-fine UAV target tracking
    with deep reinforcement learning,” *IEEE Trans. Autom. Sci. Eng.*, pp. 1–9, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] L. Ren, X. Yuan, J. Lu, M. Yang, and J. Zhou, “Deep reinforcement learning
    with iterative shift for visual tracking,” in *Proc. ECCV*, 2018, pp. 697–713.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] D. Zhang, H. Maei, X. Wang, and Y.-F. Wang, “Deep reinforcement learning
    for visual object tracking in videos,” 2017\. [Online]. Available: [http://arxiv.org/abs/1701.08936](http://arxiv.org/abs/1701.08936)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] C. Huang, S. Lucey, and D. Ramanan, “Learning policies for adaptive tracking
    with deep feature cascades,” in *Proc. IEEE ICCV*, 2017, pp. 105–114.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] X. Dong, J. Shen, W. Wang, Y. Liu, L. Shao, and F. Porikli, “Hyperparameter
    optimization for tracking with continuous deep Q-learning,” in *Proc. IEEE CVPR*,
    2018, pp. 518–527.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] J. Supancic and D. Ramanan, “Tracking as online decision-making: Learning
    a policy from streaming videos with reinforcement learning,” in *Proc. IEEE ICCV*,
    2017, pp. 322–331.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] J. Choi, J. Kwon, and K. M. Lee, “Real-time visual tracking by deep reinforced
    decision making,” *Comput. Vis. Image Und.*, vol. 171, pp. 10–19, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] X. Wang, C. Li, B. Luo, and J. Tang, “SINT++: Robust visual tracking
    via adversarial positive instance generation,” in *Proc. IEEE CVPR*, 2018, pp.
    4864–4873.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] E. Park and A. C. Berg, “Meta-tracker: Fast and robust online adaptation
    for visual object trackers,” in *Proc. ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] L. Zhang and P. N. Suganthan, “Visual tracking with convolutional random
    vector functional link network,” *IEEE Trans. Cybernetics*, vol. 47, no. 10, pp.
    3243–3253, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] L. Zhang and N. Suganthan, “Visual tracking with convolutional neural
    network,” in *Proc. IEEE Int. Conf. Syst. Man Cybern.*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] L. Huang, X. Zhao, and K. Huang, “Bridging the gap between detection
    and tracking: A unified approach,” in *Proc. IEEE ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] T. Xu, Z.-H. Feng, X.-J. Wu, and J. Kittler, “Joint group feature selection
    and discriminative filter learning for robust visual object tracking,” in *Proc.
    IEEE ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] P. Li, B. Chen, W. Ouyang, D. Wang, X. Yang, and H. Lu, “Gradnet: Gradient-guided
    network for visual object tracking,” in *Proc. IEEE ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] J. Choi, J. Kwon, and K. M. Lee, “Deep meta learning for real-time target-aware
    visual tracking,” in *Proc. IEEE ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] L. Zhang, A. Gonzalez-Garcia, J. v. d. Weijer, M. Danelljan, and F. S.
    Khan, “Learning the model update for siamese trackers,” in *Proc. IEEE ICCV*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] F. Du, P. Liu, W. Zhao, and X. Tang, “Correlation-guided attention for
    corner detection based visual tracking,” in *Proc. IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] B. Yan, D. Wang, H. Lu, and X. Yang, “Cooling-shrinking attack: Blinding
    the tracker with imperceptible noises,” in *Proc. IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] A. Lukezic, J. Matas, and M. Kristan, “D3s - a discriminative single
    shot segmentation tracker,” in *Proc. IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] X. Chen, X. Yan, F. Zheng, Y. Jiang, S.-T. Xia, Y. Zhao, and R. Ji, “One-shot
    adversarial attacks on visual tracking with dual attention,” in *Proc. IEEE CVPR*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] M. Danelljan, L. V. Gool, and R. Timofte, “Probabilistic regression for
    visual tracking,” in *Proc. IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] J. Gao, W. Hu, and Y. Lu, “Recursive least-squares estimator-aided online
    learning for visual tracking,” in *Proc. IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] T. Yang, P. Xu, R. Hu, H. Chai, and A. B. Chan, “Roam: Recurrently optimizing
    tracking model,” in *Proc. IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Y. Yu, Y. Xiong, W. Huang, and M. R. Scott, “Deformable siamese attention
    networks for visual object tracking,” in *Proc. IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Z. Chen, B. Zhong, G. Li, S. Zhang, and R. Ji, “Siamese box adaptive
    network for visual tracking,” in *Proc. IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] D. Guo, J. Wang, Y. Cui, Z. Wang, and S. Chen, “Siamcar: Siamese fully
    convolutional classification and regression for visual tracking,” in *Proc. IEEE
    CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] P. Voigtlaender, J. Luiten, P. H. Torr, and B. Leibe, “Siam r-cnn: Visual
    tracking by re-detection,” in *Proc. IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] G. Wang, C. Luo, X. Sun, Z. Xiong, and W. Zeng, “Tracking by instance
    detection: A meta-learning approach,” in *Proc. IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] K. Shuang, Y. Huang, Y. Sun, Z. Cai, and H. Guo, “Fine-grained motion
    representation for template-free visual tracking,” in *Proc. IEEE WACV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] H. Song, D. Suehiro, and S. Uchida, “Adaptive aggregation of arbitrary
    online trackers with a regret bound,” in *Proc. IEEE WACV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Y. Ma, C. Yuan, P. Gao, and F. Wang, “Efficient multi-level correlating
    for visual tracking,” in *Proc. ACCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Q. WuYan, Y. LiangYi, and L. Wang, “Dsnet: Deep and shallow feature learning
    for efficient visual tracking,” in *Proc. ACCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] C. Fu, Z. Huang, Y. Li, R. Duan, and P. Lu, “Boundary effect-aware visual
    tracking for UAV with online enhanced background learning and multi-frame consensus
    verification,” in *Proc. IROS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] W. Song, S. Li, T. Chang, A. Hao, Q. Zhao, and H. Qin, “Cross-view contextual
    relation transferred network for unsupervised vehicle tracking in drone videos,”
    in *Proc. IEEE WACV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Y. Li, C. Fu, Z. Huang, Y. Zhang, and J. Pan, “Keyfilter-aware real-time
    uav object tracking,” in *Proc. ICRA*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Y. Li, C. Fu, Z. Huang, and et al., “Intermittent contextual learning
    for keyfilter-aware UAV object tracking using deep convolutional feature,” *IEEE
    Trans. Multimedia*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] C. Fu, Y. He, F. Lin, and W. Xiong, “Robust multi-kernelized correlators
    for UAV tracking with adaptive context analysis and dynamic weighted filters,”
    *Neural Computing and Applications*, vol. 32, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] C. Fu, W. Xiong, F. Lin, and Y. Yue, “Surrounding-aware correlation filter
    for UAV tracking with selective spatial regularization,” *Signal Processing*,
    vol. 167, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] S. M. Marvasti-Zadeh, J. Khaghani, H. Ghanei-Yakhdan, S. Kasaei, and
    L. Cheng, “COMET: Context-aware IoU-guided network for small object tracking,”
    in *Proc. ACCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] H. Wu, X. Yang, Y. Yang, and G. Liu, “Flow guided short-term trackers
    with cascade detection for long-term tracking,” in *Proc. IEEE. ICCVW*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] L. Huang, X. Zhao, and K. Huang, “Globaltrack: A simple and strong baseline
    for long-term tracking,” in *Proc. AAAI*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] W. Ren Tan and S.-H. Lai, “i-siam: Improving siamese tracker with distractors
    suppression and long-term strategies,” in *Proc. IEEE. ICCVW*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Y. Zhang, D. Wang, L. Wang, J. Qi, and H. Lu, “Learning regression and
    verification networks for long-term visual tracking,” 2018\. [Online]. Available:
    [http://arxiv.org/abs/1809.04320](http://arxiv.org/abs/1809.04320)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] K. Dai, Y. Zhang, D. Wang, J. Li, H. Lu, and X. Yang, “High-performance
    long-term tracking with meta-updater,” in *Proc. IEEE CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] B. Yan, H. Zhao, D. Wang, H. Lu, and X. Yang, “’skimming-perusal’ tracking:
    A framework for real-time and robust long-term tracking,” in *Proc. IEEE ICCV*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Y. Wu, J. Lim, and M. H. Yang, “Online object tracking: A benchmark,”
    in *Proc. IEEE CVPR*, 2013, pp. 2411–2418.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Y. Wu, J. Lim, and M. Yang, “Object tracking benchmark,” *IEEE Trans.
    Pattern Anal. Mach. Intell.*, vol. 37, no. 9, pp. 1834–1848, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai, Y. Xu, C. Liao,
    and H. Ling, “LaSOT: A high-quality benchmark for large-scale single object tracking,”
    2018\. [Online]. Available: [http://arxiv.org/abs/1809.07845](http://arxiv.org/abs/1809.07845)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] M. Mueller, N. Smith, and B. Ghanem, “A benchmark and simulator for UAV
    tracking,” in *Proc. ECCV*, 2016, pp. 445–461.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] D. Du, Y. Qi, H. Yu, Y. Yang, K. Duan, G. Li, W. Zhang, Q. Huang, and
    Q. Tian, “The unmanned aerial vehicle benchmark: Object detection and tracking,”
    in *Proc. ECCV*, 2018, pp. 375–391.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] D. Du, P. Zhu, L. Wen, X. Bian, H. Ling, and et al., “VisDrone-SOT2019:
    The Vision Meets Drone Single Object Tracking Challenge Results,” in *Proc. ICCVW*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] P. Liang, E. Blasch, and H. Ling, “Encoding color information for visual
    tracking: Algorithms and benchmark,” *IEEE Trans. Image Process.*, vol. 24, no. 12,
    pp. 5630–5644, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] A. Li, M. Lin, Y. Wu, M. H. Yang, and S. Yan, “NUS-PRO: A new visual
    tracking challenge,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 38, no. 2,
    pp. 335–349, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] H. K. Galoogahi, A. Fagg, C. Huang, D. Ramanan, and S. Lucey, “Need for
    speed: A benchmark for higher frame rate object tracking,” in *Proc. IEEE ICCV*,
    2017, pp. 1134–1143.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] S. Li and D. Y. Yeung, “Visual object tracking for unmanned aerial vehicles:
    A benchmark and new motion models,” in *Proc. AAAI*, 2017, pp. 4140–4146.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] M. Müller, A. Bibi, S. Giancola, S. Alsubaihi, and B. Ghanem, “TrackingNet:
    A large-scale dataset and benchmark for object tracking in the wild,” in *Proc.
    ECCV*, 2018, pp. 310–327.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] J. Valmadre, L. Bertinetto, J. F. Henriques, R. Tao, A. Vedaldi, A. W.
    Smeulders, P. H. Torr, and E. Gavves, “Long-term tracking in the wild: A benchmark,”
    in *Proc. ECCV*, vol. 11207 LNCS, 2018, pp. 692–707.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] A. Li, Z. Chen, and Y. Wang, “BUAA-PRO: A tracking dataset with pixel-level
    annotation,” in *Proc. BMVC*, 2018\. [Online]. Available: [http://bmvc2018.org/contents/papers/0851.pdf](http://bmvc2018.org/contents/papers/0851.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] L. Huang, X. Zhao, and K. Huang, “GOT-10k: A large high-diversity benchmark
    for generic object tracking in the wild,” 2018\. [Online]. Available: [http://arxiv.org/abs/1810.11981](http://arxiv.org/abs/1810.11981)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] A. Moudgil and V. Gandhi, “Long-term visual object tracking benchmark,”
    in *Proc. ICCV*, 2018, pp. 629–645.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] H. Fan, Y. Fan, P. Chu, L. Yuan, and H. Ling, “Tracklinic: Diagnosis
    of challenge factors in visual tracking,” ser. Proc. WACV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] A. Lukezic, L. C. Zajc, T. Vojir, J. Matas, and M. Kristan, “Now you
    see me: evaluating performance in long-term visual tracking,” 2018\. [Online].
    Available: [http://arxiv.org/abs/1804.07056](http://arxiv.org/abs/1804.07056)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] P. Zhu, L. Wen, D. Du, and et al., “VisDrone-VDT2018: The vision meets
    drone video detection and tracking challenge results,” in *Proc. ECCVW*, 2018,
    pp. 496–518.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] C. Liu, W. Ding, J. Yang, and et al., “Aggregation signature for small
    object tracking,” *IEEE Trans. Image Processing*, vol. 29, pp. 1738–1747, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Y. Fu, and A. C.
    Berg, “SSD: Single shot multibox detector,” in *Proc. ECCV*, 2016, pp. 21–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] G. Koch, R. Zemel, and R. Salakhutdinov, “Siamese neural networks for
    one-shot image recognition,” in *Proc. ICML Deep Learning Workshop*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] G. Lin, A. Milan, C. Shen, and I. Reid, “RefineNet: Multi-path refinement
    networks for high-resolution semantic segmentation,” in *Proc. IEEE CVPR*, 2017,
    pp. 5168–5177.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] T. Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proc. IEEE CVPR*, 2017, pp.
    936–944.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] S. Gladh, M. Danelljan, F. S. Khan, and M. Felsberg, “Deep motion features
    for visual tracking,” in *Proc. ICPR*, 2016, pp. 1243–1248.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. Huang, “YouTube-VOS:
    A large-scale video object segmentation benchmark,” in *Proc. ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] E. Real, J. Shlens, S. Mazzocchi, X. Pan, and V. Vanhoucke, “YouTube-BoundingBoxes:
    A large high-precision human-annotated data set for object detection in video,”
    in *Proc. IEEE CVPR*, 2017, pp. 7464–7473.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The KITTI dataset,” vol. 32, no. 11, p. 1231–1237, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-learning
    in neural networks: A survey,” 2020\. [Online]. Available: [http://arxiv.org/abs/2004.05439](http://arxiv.org/abs/2004.05439)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in *Proc. ICML*, 2017, pp. 1126–1135.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] L. Wen, D. Du, Z. Lei, S. Z. Li, and M.-H. Yang, “Jots: Joint online
    tracking and segmentation,” in *Proc. IEEE CVPR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] J. Son, I. Jung, K. Park, and B. Han, “Tracking-by-segmentation with
    online gradient boosting decision tree,” in *Proc. IEEE ICCV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] D. Yeo, J. Son, B. Han, and J. Hee Han, “Superpixel-based tracking-by-segmentation
    using markov chains,” in *Proc. IEEE CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] J. Luiten, P. Voigtlaender, and B. Leibe, “Premvos: Proposal-generation,
    refinement and merging for video object segmentation,” in *Proc. ACCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] V. Kulharia, S. Chandra, A. Agrawal, P. Torr, and A. Tyagi, “Box2seg:
    Attention weighted loss and discriminative feature learning for weakly supervised
    segmentation,” in *Proc. ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] L. Čehovin, “TraX: The visual tracking exchange protocol and library,”
    *Neurocomputing*, vol. 260, pp. 5–8, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] A. Vedaldi and K. Lenc, “MatConvNet: Convolutional neural networks for
    MATLAB,” in *Proc. ACM Multimedia Conference*, 2015, pp. 689–692.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] B. Jiang, R. Luo, J. Mao, T. Xiao, and Y. Jiang, “Acquisition of localization
    confidence for accurate object detection,” in *Proc. IEEE ECCV*, 2018, pp. 816–832.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] J. Serra, D. Suris, M. Miron, and A. Karatzoglou, “Overcoming catastrophic
    forgetting with hard attention to the task,” ser. Proc. ICML, 2018, pp. 4548–4557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] X. LI, Y. Grandvalet, and F. Davoine, “Explicit inductive bias for transfer
    learning with convolutional networks,” ser. Proc. ICML, 2018, pp. 2825–2834.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] X. Chen, S. Wang, B. Fu, M. Long, and J. Wang, “Catastrophic forgetting
    meets negative transfer: Batch spectral shrinkage for safe transfer learning,”
    in *Proc. NIPS*, 2019, pp. 1908–1918.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] J. Yoon, E. Yang, J. Lee, and S. J. Hwang, “Lifelong learning with dynamically
    expandable networks,” in *Proc. ICLR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/068188baba725d3ad10d5809805e2028.png) | Seyed
    Mojtaba Marvasti-Zadeh (Student Member, IEEE) is currently a Ph.D. student in
    Electrical Engineering, Yazd University (YU), Iran. He was awarded as a distinguished
    researcher student of the Department of Electrical Engineering, YU, in 2015 and
    2017\. He is also a member of Vision and Learning Lab, University of Alberta,
    Canada, where he was a visiting researcher from Dec. 2019 to Sep. 2020\. His research
    interest includes computer vision and machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/683a82fce4ecbeb6dbee20a41fc06e40.png) | Li Cheng
    (Senior Member, IEEE) received the Ph.D. degree in computer science from the University
    of Alberta, Canada. He is an associate professor with the Department of Electrical
    and Computer Engineering, University of Alberta, Canada. He is also the Director
    of the Vision and Learning Lab. Prior to coming back to University of Alberta,
    he has worked at A*STAR, Singapore, TTIChicago, USA, and NICTA, Australia. His
    research expertise is mainly on computer vision and machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/2ceee74e3012eb59a9273b1c66138e5d.png) | Hossein
    Ghanei-Yakhdan received the B.Sc. degree in Electrical Engineering from Isfahan
    University of Technology, Iran, in 1989, the M.Sc. degree in 1993 from K. N. Toosi
    University of Technology, Iran, and the Ph.D. degree in 2009 from Ferdowsi University
    of Mashhad, Iran. Since 2009, he has been an Assistant Professor with the Department
    of Electrical Engineering, Yazd University, Iran. His research interests are in
    digital video and image processing, error concealment and error-resilient video
    coding, and object tracking. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/9da4c205b70a3d6417c216989a53c11c.png) | Shohreh
    Kasaei (Senior Member, IEEE) received the Ph.D. degree from the Signal Processing
    Research Center, School of Electrical Engineering and Computer Science, Queensland
    University of Technology, Australia, in 1998\. She was awarded as a distinguished
    researcher of Sharif University of Technology (SUT), in 2002 and 2010, where she
    is currently a Full Professor. Since 1999, she has been the Director of the Image
    Processing Laboratory (IPL). Her research interests include machine learning,
    computer vision, and image/video processing. |'
  prefs: []
  type: TYPE_TB
