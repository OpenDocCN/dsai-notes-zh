- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:03:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:03:55
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1912.00535] Deep Learning for Visual Tracking: A Comprehensive Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1912.00535] 深度学习在视觉跟踪中的应用：综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.00535](https://ar5iv.labs.arxiv.org/html/1912.00535)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1912.00535](https://ar5iv.labs.arxiv.org/html/1912.00535)
- en: 'Deep Learning for Visual Tracking: A Comprehensive Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在视觉跟踪中的应用：综合调查
- en: 'Seyed Mojtaba Marvasti-Zadeh,  Li Cheng,  Hossein Ghanei-Yakhdan, and Shohreh Kasaei,
    S. M. Marvasti-Zadeh is with Digital Image & Video Processing Lab (DIVPL), Department
    of Electrical Engineering, Yazd University, Iran. He is also a member of Vision
    and Learning Lab, University of Alberta, Canada, and Image Processing Lab (IPL),
    Sharif University of Technology, Iran. E-mail: [mojtaba.marvasti@ualberta.ca](mailto:mojtaba.marvasti@ualberta.ca)
    L. Cheng is with Vision and Learning Lab, Department of Electrical and Computer
    Engineering, University of Alberta, Edmonton, Canada. E-mail: [lcheng5@ualberta.ca](mailto:lcheng5@ualberta.ca)
    H. Ghanei-Yakhdan is with Digital Image & Video Processing Lab (DIVPL), Department
    of Electrical Engineering, Yazd University, Yazd, Iran. E-mail: [hghaneiy@yazd.ac.ir](mailto:hghaneiy@yazd.ac.ir)
    S. Kasaei is with Image Processing Lab (IPL), Department of Computer Engineering,
    Sharif University of Technology, Tehran, Iran. E-mail: [kasaei@sharif.edu](mailto:kasaei@sharif.edu)Manuscript
    received …; revised …'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Seyed Mojtaba Marvasti-Zadeh，Li Cheng，Hossein Ghanei-Yakhdan 和 Shohreh Kasaei。S.
    M. Marvasti-Zadeh 在伊朗亚兹德大学电气工程系数字图像与视频处理实验室（DIVPL）工作。他同时也是加拿大阿尔伯塔大学视觉与学习实验室和伊朗沙里夫大学图像处理实验室（IPL）的成员。电子邮件：[mojtaba.marvasti@ualberta.ca](mailto:mojtaba.marvasti@ualberta.ca)
    L. Cheng 在加拿大阿尔伯塔大学电气与计算机工程系视觉与学习实验室工作。电子邮件：[lcheng5@ualberta.ca](mailto:lcheng5@ualberta.ca)
    H. Ghanei-Yakhdan 在伊朗亚兹德大学电气工程系数字图像与视频处理实验室（DIVPL）工作。电子邮件：[hghaneiy@yazd.ac.ir](mailto:hghaneiy@yazd.ac.ir)
    S. Kasaei 在伊朗沙里夫大学计算机工程系图像处理实验室（IPL）工作。电子邮件：[kasaei@sharif.edu](mailto:kasaei@sharif.edu)手稿接收日期
    …; 修订日期 …
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Visual target tracking is one of the most sought-after yet challenging research
    topics in computer vision. Given the ill-posed nature of the problem and its popularity
    in a broad range of real-world scenarios, a number of large-scale benchmark datasets
    have been established, on which considerable methods have been developed and demonstrated
    with significant progress in recent years – predominantly by recent deep learning
    (DL)-based methods. This survey aims to systematically investigate the current
    DL-based visual tracking methods, benchmark datasets, and evaluation metrics.
    It also extensively evaluates and analyzes the leading visual tracking methods.
    First, the fundamental characteristics, primary motivations, and contributions
    of DL-based methods are summarized from nine key aspects of: network architecture,
    network exploitation, network training for visual tracking, network objective,
    network output, exploitation of correlation filter advantages, aerial-view tracking,
    long-term tracking, and online tracking. Second, popular visual tracking benchmarks
    and their respective properties are compared, and their evaluation metrics are
    summarized. Third, the state-of-the-art DL-based methods are comprehensively examined
    on a set of well-established benchmarks of OTB2013, OTB2015, VOT2018, LaSOT, UAV123,
    UAVDT, and VisDrone2019\. Finally, by conducting critical analyses of these state-of-the-art
    trackers quantitatively and qualitatively, their pros and cons under various common
    scenarios are investigated. It may serve as a gentle use guide for practitioners
    to weigh when and under what conditions to choose which method(s). It also facilitates
    a discussion on ongoing issues and sheds light on promising research directions.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉目标跟踪是计算机视觉中最受关注但又最具挑战性的研究课题之一。鉴于问题的复杂性以及其在广泛现实场景中的流行性，已建立了多个大规模基准数据集，并在这些数据集上开发了大量方法，近年来取得了显著进展——主要是由基于深度学习（DL）的方法推动的。本调查旨在系统地研究当前基于深度学习的视觉跟踪方法、基准数据集和评估指标。它还广泛评估和分析了领先的视觉跟踪方法。首先，总结了DL方法在网络架构、网络利用、视觉跟踪网络训练、网络目标、网络输出、相关滤波器优势利用、空中视角跟踪、长期跟踪和在线跟踪等九个关键方面的基本特征、主要动机和贡献。其次，比较了流行的视觉跟踪基准及其各自特性，并总结了它们的评估指标。第三，全面检查了OTB2013、OTB2015、VOT2018、LaSOT、UAV123、UAVDT和VisDrone2019等一系列成熟基准上的最先进的DL方法。最后，通过对这些最先进跟踪器进行定量和定性的关键分析，探讨了它们在各种常见场景下的优缺点。这可能作为实践者在选择方法时的参考指南，并促进对当前问题的讨论，揭示有前景的研究方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Visual tracking, deep learning, computer vision, appearance modeling.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉跟踪、深度学习、计算机视觉、外观建模。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'Generic visual tracking aims to estimate an unknown visual target trajectory
    when only an initial state of the target (in a video frame) is available. Visual
    tracking is an open and attractive research field with a broad extent of categories
    (see Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")) and applications, including self-driving cars [[1](#bib.bib1)],
    autonomous robots [[2](#bib.bib2)], surveillance [[3](#bib.bib3)], augmented reality
    [[4](#bib.bib4)], aerial-view tracking [[5](#bib.bib5)], sports [[6](#bib.bib6)],
    surgery [[7](#bib.bib7)], biology [[8](#bib.bib8)], ocean exploration [[9](#bib.bib9)],
    to name a few.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通用视觉跟踪旨在估计未知视觉目标的轨迹，当只有目标的初始状态（在视频帧中）可用时。视觉跟踪是一个开放且有吸引力的研究领域，涵盖了广泛的类别（见图 [1](#S1.F1
    "图 1 ‣ I 引言 ‣ 深度学习在视觉跟踪中的应用：综合调查")）和应用，包括自动驾驶汽车 [[1](#bib.bib1)]、自主机器人 [[2](#bib.bib2)]、监控
    [[3](#bib.bib3)]、增强现实 [[4](#bib.bib4)]、空中视角跟踪 [[5](#bib.bib5)]、体育 [[6](#bib.bib6)]、手术
    [[7](#bib.bib7)]、生物学 [[8](#bib.bib8)]、海洋探索 [[9](#bib.bib9)] 等等。
- en: '![Refer to caption](img/f453d3726e8424cfd7d95cd860e22ba0.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f453d3726e8424cfd7d95cd860e22ba0.png)'
- en: 'Figure 1: An overview of visual target tracking.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：视觉目标跟踪的概述。
- en: The ill-posed definition of the visual tracking (i.e., model-free tracking,
    on-the-fly learning, single-camera, 2D information) is more challenging in complicated
    real-world scenarios which may include arbitrary classes of targets (e.g., human,
    drone, animal, vehicle) and motion models, various imaging characteristics (e.g.,
    static/moving camera, smooth/fast movement, camera resolution), and changes in
    environmental conditions (e.g., illumination variation, background clutter, crowded
    scenes). Traditional methods employ various visual tracking frameworks, such as
    discriminative correlation filters (DCF) [[10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)],
    silhouette tracking [[17](#bib.bib17)], Kernel tracking [[18](#bib.bib18)], point
    tracking [[19](#bib.bib19)] – for appearance & motion modeling of a target. In
    general, traditional trackers have inflexible assumptions about target structures
    & their motion in real-world scenarios. These trackers exploit handcrafted features
    (e.g., the histogram of oriented gradients (HOG) [[20](#bib.bib20)] and Color-Names
    (CN) [[21](#bib.bib21)]), so they cannot interpret semantic target information
    and handle significant appearance changes. However, some tracking-by-detection
    methods (e.g., DCF-based trackers) provide an appealing trade-off of competitive
    tracking performance and efficient computations [[22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24)]. For instance, aerial-view trackers [[25](#bib.bib25), [26](#bib.bib26),
    [27](#bib.bib27)] extensively use these CPU-based algorithms considering limited
    on-board computational power & embedded hardware.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉跟踪的定义（即无模型跟踪、即时学习、单摄像头、2D 信息）在复杂的现实场景中更具挑战性，这些场景可能包括任意类别的目标（例如，人类、无人机、动物、车辆）和运动模型、各种成像特性（例如，静态/移动摄像头、平稳/快速运动、摄像头分辨率），以及环境条件的变化（例如，光照变化、背景杂乱、拥挤的场景）。传统方法使用各种视觉跟踪框架，如判别相关滤波器（DCF）[[10](#bib.bib10)、[11](#bib.bib11)、[12](#bib.bib12)、[13](#bib.bib13)、[14](#bib.bib14)、[15](#bib.bib15)、[16](#bib.bib16)]，轮廓跟踪[[17](#bib.bib17)]，核跟踪[[18](#bib.bib18)]，点跟踪[[19](#bib.bib19)]——用于目标的外观和运动建模。一般来说，传统跟踪器对目标结构及其在现实场景中的运动有不灵活的假设。这些跟踪器利用手工特征（例如，方向梯度直方图（HOG）[[20](#bib.bib20)]
    和颜色名称（CN）[[21](#bib.bib21)]），因此无法解释语义目标信息并处理显著的外观变化。然而，一些基于检测的跟踪方法（例如，基于 DCF 的跟踪器）提供了具有竞争力的跟踪性能和高效计算的有吸引力的折中方案[[22](#bib.bib22)、[23](#bib.bib23)、[24](#bib.bib24)]。例如，俯视跟踪器[[25](#bib.bib25)、[26](#bib.bib26)、[27](#bib.bib27)]广泛使用这些基于
    CPU 的算法，考虑到有限的机载计算能力和嵌入式硬件。
- en: 'Inspired by deep learning (DL) breakthroughs [[28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)] in ImageNet large-scale
    visual recognition competition (ILSVRC) [[33](#bib.bib33)] and also visual object
    tracking (VOT) challenge [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)], DL-based
    methods have attracted considerable interest in the visual tracking community
    to provide robust trackers. Although convolutional neural networks (CNNs) have
    been dominant networks initially, a broad range of architectures such as recurrent
    neural networks (RNNs), auto-encoders (AEs), generative adversarial networks (GANs),
    and especially Siamese neural networks (SNNs) & custom neural networks are currently
    investigated. Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Deep Learning for Visual
    Tracking: A Comprehensive Survey") presents a brief history of the development
    of deep visual trackers in recent years. The state-of-the-art DL-based visual
    trackers have distinct characteristics such as exploitation of various architectures,
    backbone networks, learning procedures, training datasets, network objectives,
    network outputs, types of exploited deep features, CPU/GPU implementations, programming
    languages & frameworks, speed, and so forth. Therefore, this work provides a comparative
    study of DL-based trackers, benchmark datasets, and evaluation metrics to investigate
    proposed trackers in detail and facilitate developing advanced trackers.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '受深度学习（DL）突破的启发[[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32)]，在ImageNet大规模视觉识别竞赛（ILSVRC）[[33](#bib.bib33)]和视觉目标跟踪（VOT）挑战[[34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40)]中，基于DL的方法在视觉跟踪领域引起了广泛关注，以提供鲁棒的跟踪器。尽管卷积神经网络（CNNs）最初占据主导地位，但目前正在研究多种架构，如递归神经网络（RNNs）、自编码器（AEs）、生成对抗网络（GANs），尤其是Siamese神经网络（SNNs）和定制神经网络。图[2](#S1.F2
    "Figure 2 ‣ I Introduction ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")展示了近年来深度视觉跟踪器发展的简要历史。最先进的基于DL的视觉跟踪器具有不同的特性，如各种架构的利用、骨干网络、学习过程、训练数据集、网络目标、网络输出、深层特征类型、CPU/GPU实现、编程语言和框架、速度等。因此，本研究提供了基于DL的跟踪器、基准数据集和评估指标的比较研究，以详细调查所提出的跟踪器，并促进先进跟踪器的开发。'
- en: \justify![Refer to caption](img/3940bb968e9ff0e112a8a93d553d1765.png)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \justify![参考说明](img/3940bb968e9ff0e112a8a93d553d1765.png)
- en: 'Figure 2: Timeline of deep visual tracking methods.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：深度视觉跟踪方法时间轴。
- en: '2015: Exploring/studying deep features to exploit the traditional methods.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年：探索/研究深层特征以利用传统方法。
- en: '2016: Offline training/fine-tuning of DNNs, employing Siamese networks for
    real-time tracking, and integrating DNNs into traditional frameworks.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年：离线训练/微调DNNs，使用Siamese网络进行实时跟踪，并将DNNs集成到传统框架中。
- en: '2017: Incorporating temporal & contextual information, and investigating various
    offline training on large-scale datasets.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年：结合时间和上下文信息，并研究大规模数据集上的各种离线训练。
- en: '2018: Studying different learning & search strategies, and designing more sophisticated
    architectures for visual tracking task.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年：研究不同的学习和搜索策略，并为视觉跟踪任务设计更复杂的架构。
- en: '2019: Investigating deep detection & segmentation approaches, and taking advantages
    of deeper backbone networks.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年：研究深度检测与分割方法，并利用更深的骨干网络。
- en: Visual target trackers can be roughly classified into two main categories, before
    and after the revolution of DL in computer vision. The first category is primarily
    reviewed by [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)],
    which include traditional trackers based on classical appearance & motion models,
    and then examine their pros and cons systematically, experimentally, or both.
    These trackers employ manually-designed features for target modeling to alleviate
    appearance variations and to provide efficient computational complexity. For instance,
    although these trackers are suitable to implement on the flying robots [[26](#bib.bib26),
    [25](#bib.bib25), [45](#bib.bib45), [27](#bib.bib27), [46](#bib.bib46)] due to
    the restrictions of using advanced GPUs, they do not have enough robustness to
    handle the challenges of in-the-wild videos. Typically, these trackers try to
    ensemble multiple features to construct a complementary set of visual cues. But,
    tuning an optimal trade-off that also maintains efficiency for real-world scenarios
    is tricky. Considering DL-based trackers’ significant progress in recent years,
    the reviewed methods by the mentioned works are outdated.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉目标跟踪器可以大致分为两个主要类别，即DL在计算机视觉中革命之前和之后。第一类主要由[[41](#bib.bib41)、[42](#bib.bib42)、[43](#bib.bib43)、[44](#bib.bib44)]
    综述，包括基于经典外观和运动模型的传统跟踪器，并系统地、实验性地或两者兼而有之地检查其优缺点。这些跟踪器使用手动设计的特征进行目标建模，以缓解外观变化，并提供高效的计算复杂度。例如，尽管由于使用高级GPU的限制，这些跟踪器适合在飞行机器人上实现[[26](#bib.bib26)、[25](#bib.bib25)、[45](#bib.bib45)、[27](#bib.bib27)、[46](#bib.bib46)]，但它们没有足够的鲁棒性来处理现实世界视频中的挑战。通常，这些跟踪器尝试将多个特征组合在一起，构建一组互补的视觉线索。但，为现实世界场景找到一个既优化又保持效率的折中方案是棘手的。考虑到近年来DL-based跟踪器的显著进展，前述工作的综述方法已显得过时。
- en: The second category includes DL-based trackers that employ either deep off-the-shelf
    features or end-to-end networks. A straightforward approach is integrating pre-trained
    deep features into the traditional frameworks. However, such trackers result in
    inconsistency problems considering task differences. But, end-to-end trained visual
    trackers have been investigated regarding existing tracking challenges. Recently,
    [[47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)] review limited DL-based
    visual trackers. For instance, [[47](#bib.bib47), [48](#bib.bib48)] categorize
    some handcrafted & deep methods into the correlation filter trackers & non-correlation
    filter ones. Then, a further classification based on architectures & tracking
    mechanisms has been applied. The work [[50](#bib.bib50)] particularly investigates
    some SNN-based trackers based on their network branches, layers, and training
    aspects. However, it does not include state-of-the-art trackers and custom networks
    with & without partial exploitation of SNNs. At last, the work [[49](#bib.bib49)]
    categorizes the DL-based trackers according to their structure, function, and
    training. Then, the evaluations are performed to conclude the categorizations
    based on the observations. From the structure perspective, the trackers are categorized
    into the CNN, RNN, and others, while they are classified into the feature extraction
    network (FEN) or end-to-end network (EEN) according to their functionality in
    visual tracking. The EENs are also classified in terms of the outputs, including
    object score, confidence map, and bounding box (BB). Finally, DL-based methods
    are categorized according to pre-training & online learning based on the network
    training perspective.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类包括那些采用深度预训练特征或端到端网络的基于深度学习（DL）的跟踪器。一种直接的方法是将预训练的深度特征集成到传统框架中。然而，这种跟踪器会因任务差异而导致不一致的问题。但，基于端到端训练的视觉跟踪器已被研究以应对现有的跟踪挑战。最近，[[47](#bib.bib47)、[48](#bib.bib48)、[49](#bib.bib49)]
    综述了有限的基于DL的视觉跟踪器。例如，[[47](#bib.bib47)、[48](#bib.bib48)] 将一些手工制作和深度方法分类为相关滤波跟踪器和非相关滤波跟踪器。随后，基于架构和跟踪机制进行了进一步分类。[[50](#bib.bib50)]
    特别研究了一些基于SNN的跟踪器，依据它们的网络分支、层和训练方面。然而，它不包括最先进的跟踪器以及使用或不使用部分SNN的定制网络。最后，[[49](#bib.bib49)]
    根据结构、功能和训练将DL-based跟踪器进行分类。然后，根据观察结果对分类结果进行评估。从结构的角度来看，跟踪器被分类为CNN、RNN和其他类型，而根据它们在视觉跟踪中的功能，它们被分类为特征提取网络（FEN）或端到端网络（EEN）。EEN也根据输出（包括目标得分、置信度图和边界框（BB））进行分类。最后，根据网络训练的角度，DL-based方法被分类为预训练和在线学习。
- en: According to the previous efforts, the motivations of this work are presented
    as follows.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的努力，本工作的动机如下。
- en: 1) Despite all efforts, existing review papers do not include state-of-the-art
    visual trackers that roughly employ Siamese or customized networks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管做出了所有努力，现有的综述论文仍未包括大致采用Siamese或定制网络的最先进视觉跟踪器。
- en: 2) Notwithstanding significant progress in recent years, long-term trackers
    and tracking from aerial-views have not yet been studied. Hence, investigating
    the current issues and proposed solutions are necessary.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近年来取得了显著进展，但长期跟踪器和航拍跟踪仍未被研究。因此，有必要调查当前问题和提出的解决方案。
- en: 3) Many details are ignored in previous works studying the DL-based trackers
    (e.g., backbone networks, training details, exploited features, implementations,
    etc.).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以往研究基于深度学习的跟踪器时，许多细节被忽视（例如，骨干网络、训练细节、使用的特征、实现等）。
- en: 4) State-of-the-art benchmark datasets (short-term, long-term, aerial-view)
    are not compared completely.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的最先进基准数据集（短期、长期、航拍）尚未完全比较。
- en: 5) Finally, exhaustive comparisons of DL-based trackers on a wide variety of
    benchmarks have not been previously performed. These analyzes can demonstrate
    the advantages and limitations of existing trackers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，关于各种基准的深度学习跟踪器的全面比较尚未进行。这些分析可以展示现有跟踪器的优点和局限性。
- en: 'Motivated by the aforementioned concerns, this work’s primary goals are filling
    the gaps, investigating the present issues, and studying potential future directions.
    Thus, we focus merely on extensive state-of-the-art DL-based trackers, namely:
    HCFT [[51](#bib.bib51)], DeepSRDCF [[52](#bib.bib52)], FCNT [[53](#bib.bib53)],
    CNN-SVM [[54](#bib.bib54)], DPST [[55](#bib.bib55)], CCOT [[56](#bib.bib56)],
    GOTURN [[57](#bib.bib57)], SiamFC [[58](#bib.bib58)], SINT [[59](#bib.bib59)],
    MDNet [[60](#bib.bib60)], HDT [[61](#bib.bib61)], STCT [[62](#bib.bib62)], RPNT
    [[63](#bib.bib63)], DeepTrack [[64](#bib.bib64), [65](#bib.bib65)], CNT [[66](#bib.bib66)],
    CF-CNN [[67](#bib.bib67)], TCNN [[68](#bib.bib68)], RDLT [[69](#bib.bib69)], PTAV
    [[70](#bib.bib70), [71](#bib.bib71)], CREST [[72](#bib.bib72)], UCT/UCT-Lite [[73](#bib.bib73)],
    DSiam/DSiamM [[74](#bib.bib74)], TSN [[75](#bib.bib75)], WECO [[76](#bib.bib76)],
    RFL [[77](#bib.bib77)], IBCCF [[78](#bib.bib78)], DTO [[79](#bib.bib79)]], SRT
    [[80](#bib.bib80)], R-FCSN [[81](#bib.bib81)], GNET [[82](#bib.bib82)], LST [[83](#bib.bib83)],
    VRCPF [[84](#bib.bib84)], DCPF [[85](#bib.bib85)], CFNet [[86](#bib.bib86)], ECO
    [[87](#bib.bib87)], DeepCSRDCF [[88](#bib.bib88)], MCPF [[89](#bib.bib89)], BranchOut
    [[90](#bib.bib90)], DeepLMCF [[91](#bib.bib91)], Obli-RaFT [[92](#bib.bib92)],
    ACFN [[93](#bib.bib93)], SANet [[94](#bib.bib94)], DCFNet/DCFNet2 [[95](#bib.bib95)],
    DET [[96](#bib.bib96)], DRN [[97](#bib.bib97)], DNT [[98](#bib.bib98)], STSGS
    [[99](#bib.bib99)], TripletLoss [[100](#bib.bib100)], DSLT [[101](#bib.bib101)],
    UPDT [[102](#bib.bib102)], ACT [[103](#bib.bib103)], DaSiamRPN [[104](#bib.bib104)],
    RT-MDNet [[105](#bib.bib105)], StructSiam [[106](#bib.bib106)], MMLT [[107](#bib.bib107)],
    CPT [[108](#bib.bib108)], STP [[109](#bib.bib109)], Siam-MCF [[110](#bib.bib110)],
    Siam-BM [[111](#bib.bib111)], WAEF [[112](#bib.bib112)], TRACA [[113](#bib.bib113)],
    VITAL [[114](#bib.bib114)], DeepSTRCF [[115](#bib.bib115)], SiamRPN [[116](#bib.bib116)],
    SA-Siam [[117](#bib.bib117)], FlowTrack [[118](#bib.bib118)], DRT [[119](#bib.bib119)],
    LSART [[120](#bib.bib120)], RASNet [[121](#bib.bib121)], MCCT [[122](#bib.bib122)],
    DCPF2 [[123](#bib.bib123)], VDSR-SRT [[124](#bib.bib124)], FCSFN [[125](#bib.bib125)],
    FRPN2T-Siam [[126](#bib.bib126)], FMFT [[127](#bib.bib127)], IMLCF [[128](#bib.bib128)],
    TGGAN [[129](#bib.bib129)], DAT [[130](#bib.bib130)], DCTN [[131](#bib.bib131)],
    FPRNet [[132](#bib.bib132)], HCFTs [[133](#bib.bib133)], adaDDCF [[134](#bib.bib134)],
    YCNN [[135](#bib.bib135)], DeepHPFT [[136](#bib.bib136)], CFCF [[137](#bib.bib137)],
    CFSRL [[138](#bib.bib138)], P2T [[139](#bib.bib139)], DCDCF [[140](#bib.bib140)],
    FICFNet [[141](#bib.bib141)], LCTdeep [[142](#bib.bib142)], HSTC [[143](#bib.bib143)],
    DeepFWDCF [[144](#bib.bib144)], CF-FCSiam [[145](#bib.bib145)], MGNet [[146](#bib.bib146)],
    ORHF [[147](#bib.bib147)], ASRCF [[148](#bib.bib148)], ATOM [[149](#bib.bib149)],
    C-RPN [[150](#bib.bib150)], GCT [[151](#bib.bib151)], RPCF [[152](#bib.bib152)],
    SPM [[153](#bib.bib153)], SiamDW [[154](#bib.bib154)], SiamMask [[155](#bib.bib155)],
    SiamRPN++ [[156](#bib.bib156)], TADT [[157](#bib.bib157)], UDT [[158](#bib.bib158)],
    DiMP [[159](#bib.bib159)], ADT [[160](#bib.bib160)], CODA [[161](#bib.bib161)],
    DRRL [[162](#bib.bib162)], SMART [[163](#bib.bib163)], MRCNN [[164](#bib.bib164)],
    MM [[165](#bib.bib165)], MTHCF [[166](#bib.bib166)], AEPCF [[167](#bib.bib167)],
    IMM-DFT [[168](#bib.bib168)], TAAT [[169](#bib.bib169)], DeepTACF [[170](#bib.bib170)],
    MAM [[171](#bib.bib171)], ADNet [[172](#bib.bib172), [173](#bib.bib173)], C2FT
    [[174](#bib.bib174)], DRL-IS [[175](#bib.bib175)], DRLT [[176](#bib.bib176)],
    EAST [[177](#bib.bib177)], HP [[178](#bib.bib178)], P-Track [[179](#bib.bib179)],
    RDT [[180](#bib.bib180)], SINT++ [[181](#bib.bib181)], Meta-Tracker [[182](#bib.bib182)],
    CRVFL [[183](#bib.bib183)], VTCNN [[184](#bib.bib184)], BGBDT [[185](#bib.bib185)],
    GFS-DCF [[186](#bib.bib186)], GradNet [[187](#bib.bib187)], MLT [[188](#bib.bib188)],
    UpdateNet [[189](#bib.bib189)], CGACD [[190](#bib.bib190)], CSA [[191](#bib.bib191)],
    D3S [[192](#bib.bib192)], OSAA [[193](#bib.bib193)], PrDiMP [[194](#bib.bib194)],
    RLS [[195](#bib.bib195)], ROAM [[196](#bib.bib196)], SiamAttn [[197](#bib.bib197)],
    SiamBAN [[198](#bib.bib198)], SiamCAR [[199](#bib.bib199)], SiamRCNN [[200](#bib.bib200)],
    TMAML [[201](#bib.bib201)], FGTrack [[202](#bib.bib202)], DHT [[203](#bib.bib203)],
    MLCFT [[204](#bib.bib204)], DSNet [[205](#bib.bib205)], BEVT [[206](#bib.bib206)],
    CRAC [[207](#bib.bib207)], KAOT [[208](#bib.bib208), [209](#bib.bib209)], MKCT
    [[210](#bib.bib210)], SASR [[211](#bib.bib211)], COMET [[212](#bib.bib212)], FGLT
    [[213](#bib.bib213)], GlobalTrack [[214](#bib.bib214)], i-Siam [[215](#bib.bib215)],
    LRVN [[216](#bib.bib216)], MetaUpdater [[217](#bib.bib217)], SPLT [[218](#bib.bib218)].'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 出于上述考虑，本工作的主要目标是填补空白、调查当前问题和研究潜在的未来方向。因此，我们主要关注于广泛的最先进的基于深度学习的跟踪器，即：HCFT [[51](#bib.bib51)]、DeepSRDCF
    [[52](#bib.bib52)]、FCNT [[53](#bib.bib53)]、CNN-SVM [[54](#bib.bib54)]、DPST [[55](#bib.bib55)]、CCOT
    [[56](#bib.bib56)]、GOTURN [[57](#bib.bib57)]、SiamFC [[58](#bib.bib58)]、SINT [[59](#bib.bib59)]、MDNet
    [[60](#bib.bib60)]、HDT [[61](#bib.bib61)]、STCT [[62](#bib.bib62)]、RPNT [[63](#bib.bib63)]、DeepTrack
    [[64](#bib.bib64)、[65](#bib.bib65)]、CNT [[66](#bib.bib66)]、CF-CNN [[67](#bib.bib67)]、TCNN
    [[68](#bib.bib68)]、RDLT [[69](#bib.bib69)]、PTAV [[70](#bib.bib70)、[71](#bib.bib71)]、CREST
    [[72](#bib.bib72)]、UCT/UCT-Lite [[73](#bib.bib73)]、DSiam/DSiamM [[74](#bib.bib74)]、TSN
    [[75](#bib.bib75)]、WECO [[76](#bib.bib76)]、RFL [[77](#bib.bib77)]、IBCCF [[78](#bib.bib78)]、DTO
    [[79](#bib.bib79)]]、SRT [[80](#bib.bib80)]、R-FCSN [[81](#bib.bib81)]、GNET [[82](#bib.bib82)]、LST
    [[83](#bib.bib83)]、VRCPF [[84](#bib.bib84)]、DCPF [[85](#bib.bib85)]、CFNet [[86](#bib.bib86)]、ECO
    [[87](#bib.bib87)]、DeepCSRDCF [[88](#bib.bib88)]、MCPF [[89](#bib.bib89)]、BranchOut
    [[90](#bib.bib90)]、DeepLMCF [[91](#bib.bib91)]、Obli-RaFT [[92](#bib.bib92)]、ACFN
    [[93](#bib.bib93)]、SANet [[94](#bib.bib94)]、DCFNet/DCFNet2 [[95](#bib.bib95)]、DET
    [[96](#bib.bib96)]、DRN [[97](#bib.bib97)]、DNT [[98](#bib.bib98)]、STSGS [[99](#bib.bib99)]、TripletLoss
    [[100](#bib.bib100)]、DSL-T [[101](#bib.bib101)]、UPDT [[102](#bib.bib102)]、ACT
    [[103](#bib.bib103)]、DaSiamRPN [[104](#bib.bib104)]、RT-MDNet [[105](#bib.bib105)]、StructSiam
    [[106](#bib.bib106)]、MMLT [[107](#bib.bib107)]、CPT [[108](#bib.bib108)]、STP [[109](#bib.bib109)]、Siam-MCF
    [[110](#bib.bib110)]、Siam-BM [[111](#bib.bib111)]、WAEF [[112](#bib.bib112)]、TRACA
    [[113](#bib.bib113)]、VITAL [[114](#bib.bib114)]、DeepSTRCF [[115](#bib.bib115)]、SiamRPN
    [[116](#bib.bib116)]、SA-Siam [[117](#bib.bib117)]、FlowTrack [[118](#bib.bib118)]、DRT
    [[119](#bib.bib119)]、LSART [[120](#bib.bib120)]、RASNet [[121](#bib.bib121)]、MCCT
    [[122](#bib.bib122)]、DCPF2 [[123](#bib.bib123)]、VDSR-SRT [[124](#bib.bib124)]、FCSFN
    [[125](#bib.bib125)]、FRPN2T-Siam [[126](#bib.bib126)]、FMFT [[127](#bib.bib127)]、IMLCF
    [[128](#bib.bib128)]、TGGAN [[129](#bib.bib129)]、DAT [[130](#bib.bib130)]、DCTN
    [[131](#bib.bib131)]、FPRNet [[132](#bib.bib132)]、HCFTs [[133](#bib.bib133)]、adaDDCF
    [[134](#bib.bib134)]、YCNN [[135](#bib.bib135)]、DeepHPFT [[136](#bib.bib136)]、CFCF
    [[137](#bib.bib137)]、CFSRL [[138](#bib.bib138)]、P2T [[139](#bib.bib139)]、DCDCF
    [[140](#bib.bib140)]、FICFNet [[141](#bib.bib141)]、LCTdeep [[142](#bib.bib142)]、HSTC
    [[143](#bib.bib143)]、DeepFWDCF [[144](#bib.bib144)]、CF-FCSiam [[145](#bib.bib145)]、MGNet
    [[146](#bib.bib146)]、ORHF [[147](#bib.bib147)]、ASRCF [[148](#bib.bib148)]、ATOM
    [[149](#bib.bib149)]、C-RPN [[150](#bib.bib150)]、GCT [[151](#bib.bib151)]、RPCF
    [[152](#bib.bib152)]、SPM [[153](#bib.bib153)]、SiamDW [[154](#bib.bib154)]、SiamMask
    [[155](#bib.bib155)]、SiamRPN++ [[156](#bib.bib156)]、TADT [[157](#bib.bib157)]、UDT
    [[158](#bib.bib158)]、DiMP [[159](#bib.bib159)]、ADT [[160](#bib.bib160)]、CODA [[161](#bib.bib161)]、DRRL
    [[162](#bib.bib162)]、SMART [[163](#bib.bib163)]、MRCNN [[164](#bib.bib164)]、MM
    [[165](#bib.bib165)]、MTHCF [[166](#bib.bib166)]、AEPCF [[167](#bib.bib167)]、IMM-DFT
    [[168](#bib.bib168)]、TAAT [[169](#bib.bib169)]、DeepTACF [[170](#bib.bib170)]、MAM
    [[171](#bib.bib171)]、ADNet [[172](#bib.bib172)、[173](#bib.bib173)]、C2FT [[174](#bib.bib174)]、DRL-IS
    [[175](#bib.bib175)]、DRLT [[176](#bib.bib176)]、EAST [[177](#bib.bib177)]、HP [[178](#bib.bib178)]、P-Track
    [[179](#bib.bib179)]、RDT [[180](#bib.bib180)]、SINT++ [[181](#bib.bib181)]、Meta-Tracker
    [[182](#bib.bib182)]、CRVFL [[183](#bib.bib183)]、VTCNN [[184](#bib.bib184)]、BGBDT
    [[185](#bib.bib185)]、GFS-DCF [[186](#bib.bib186)]、GradNet [[187](#bib.bib187)]、MLT
    [[188](#bib.bib188)]、UpdateNet [[189](#bib.bib189)]、CGACD [[190](#bib.bib190)]、CSA
    [[191](#bib.bib191)]、D3S [[192](#bib.bib192)]、OSAA [[193](#bib.bib193)]、PrDiMP
    [[194](#bib.bib194)]、RLS [[195](#bib.bib195)]、ROAM [[196](#bib.bib196)]、SiamAttn
    [[197](#bib.bib197)]、SiamBAN [[198](#bib.bib198)]、SiamCAR [[199](#bib.bib199)]、SiamRCNN
    [[200](#bib.bib200)]、TMAML [[201](#bib.bib201)]、FGTrack [[202](#bib.bib202)]、DHT
    [[203](#bib.bib203)]、MLCFT [[204](#bib.bib204)]、DSNet [[205](#bib.bib205)]、BEVT
    [[206](#bib.bib206)]、CRAC [[207](#bib.bib207)]、KAOT [[208](#bib.bib208)、[209](#bib.bib209)]、MKCT
    [[210](#bib
- en: According to the network architecture, these trackers are classified into CNN-,
    SNN-, RNN-, GAN-, and custom-based (i.e., AE- & reinforcement learning (RL)-based,
    and combined) networks. It indicates the popularity of different approaches, which
    also their problems and proposed solutions are studied in this work. From the
    network exploitation, the methods are categorized into the exploitation of deep
    off-the-shelf features and deep features for visual tracking (similar to FENs
    & EENs in [[49](#bib.bib49)]). However, in this work, the detailed characteristics
    of various trackers are investigated, such as backbone networks, exploited layers,
    training datasets, objective functions, tracking speeds, extracted features, network
    outputs, CPU/GPU implementations, programming languages, and DL framework. From
    the network training perspective, this work separately studies deep off-the-shelf
    features and deep features for visual tracking since deep off-the-shelf features
    (extracted from FENs) are mostly pre-trained on the ImageNet for object recognition
    tasks. Besides, the end-to-end training for visual tracking purposes is categorized
    into exploiting offline training, online training, or both. Furthermore, meta-learning
    based visual trackers are investigated, which are recently employed to adapt visual
    trackers to unseen targets fast. Moreover, this work presents all the details
    about backbone networks, offline & online training datasets, strategies to avoid
    over-fitting, data augmentations, and many more. From exploiting the advantages
    of correlation filters, the trackers are also classified as the methods based
    on DCF and the ones that employ end-to-end networks, which take advantage of the
    online learning efficiency of DCFs & the discriminative power of CNN features.
    Next, DL-based trackers are classified based on their application for aerial-view
    tracking, long-term tracking, or online tracking. Finally, this work comprehensively
    analyses different aspects of extensive state-of-the-art trackers on seven benchmark
    datasets, namely OTB2013 [[219](#bib.bib219)], OTB2015 [[220](#bib.bib220)], VOT2018
    [[39](#bib.bib39)], LaSOT [[221](#bib.bib221)], UAV123 [[222](#bib.bib222)], UAVDT
    [[223](#bib.bib223)], and VisDrone2019-test-dev [[224](#bib.bib224)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据网络架构，这些跟踪器被分类为 CNN、SNN、RNN、GAN 以及自定义（即 AE 和强化学习 (RL) 基于的，以及组合型）网络。这表明了不同方法的流行程度，同时也研究了这些方法存在的问题及其提出的解决方案。根据网络的利用情况，这些方法被分类为利用深度现成特征和用于视觉跟踪的深度特征（类似于[[49](#bib.bib49)]中的
    FENs 和 EENs）。然而，在这项工作中，详细调查了各种跟踪器的特征，如骨干网络、利用的层、训练数据集、目标函数、跟踪速度、提取的特征、网络输出、CPU/GPU
    实现、编程语言和 DL 框架。从网络训练的角度来看，这项工作分别研究了深度现成特征和用于视觉跟踪的深度特征，因为深度现成特征（从 FENs 中提取）大多是在
    ImageNet 上进行过对象识别任务的预训练。此外，用于视觉跟踪的端到端训练被分类为利用离线训练、在线训练或两者结合。此外，还研究了基于元学习的视觉跟踪器，这些跟踪器最近被用于快速适应未见目标。此外，这项工作详细介绍了骨干网络、离线和在线训练数据集、避免过拟合的策略、数据增强等内容。从利用相关滤波器的优势来看，跟踪器也被分类为基于
    DCF 的方法和利用端到端网络的方法，这些方法利用了 DCF 的在线学习效率和 CNN 特征的判别能力。接下来，基于 DL 的跟踪器根据其应用于航拍跟踪、长期跟踪或在线跟踪进行分类。最后，这项工作对七个基准数据集上的不同方面进行了全面分析，包括
    OTB2013 [[219](#bib.bib219)]、OTB2015 [[220](#bib.bib220)]、VOT2018 [[39](#bib.bib39)]、LaSOT
    [[221](#bib.bib221)]、UAV123 [[222](#bib.bib222)]、UAVDT [[223](#bib.bib223)] 和
    VisDrone2019-test-dev [[224](#bib.bib224)]。
- en: I-A Contributions
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 贡献
- en: The main contributions are summarized as follows.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 主要贡献总结如下。
- en: 1) State-of-the-art DL-based visual trackers are categorized based on the architecture
    (i.e., CNN, SNN, RNN, GAN, and custom networks), network exploitation (i.e., off-the-shelf
    deep features and deep features for visual tracking), network training for visual
    tracking (i.e., only offline training, only online training, both offline & online
    training, meta-learning), network objective (i.e., regression-based, classification-based,
    and both classification & regression-based), exploitation of correlation filter
    advantages (i.e., DCF framework and utilizing correlation filter/layer/function),
    aerial-view tracking, long-term tracking, and online tracking. Such a study covering
    all of these aspects in the detailed categorization of visual tracking methods
    has not been previously presented.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 现有的基于深度学习的视觉跟踪器按照架构（即CNN、SNN、RNN、GAN和自定义网络）、网络利用（即现成深度特征和用于视觉跟踪的深度特征）、视觉跟踪的网络训练（即仅离线训练、仅在线训练、离线与在线训练、元学习）、网络目标（即基于回归、基于分类以及回归与分类相结合）、相关滤波器优势的利用（即DCF框架和利用相关滤波器/层/功能）、航拍跟踪、长期跟踪以及在线跟踪进行了分类。这种覆盖所有这些方面的详细分类研究在视觉跟踪方法中尚未出现。
- en: 2) The main issues and proposed solutions of DL-based trackers to tackle visual
    tracking challenges are presented. This classification provides proper insight
    into designing visual trackers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 介绍了深度学习跟踪器在应对视觉跟踪挑战方面的主要问题和提出的解决方案。这种分类提供了设计视觉跟踪器的有用见解。
- en: 3) The well-known single-object visual tracking datasets (i.e., short-term,
    long-term, aerial view) are completely compared based on their fundamental characteristics
    (e.g., the number of videos, frames, classes/clusters, sequence attributes, absent
    labels, and overlap with other datasets). These benchmark datasets include OTB2013
    [[219](#bib.bib219)], OTB2015 [[220](#bib.bib220)], VOT [[34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)],
    ALOV [[42](#bib.bib42)], TC128 [[225](#bib.bib225)], UAV123 [[222](#bib.bib222)],
    NUS-PRO [[226](#bib.bib226)], NfS [[227](#bib.bib227)], DTB [[228](#bib.bib228)],
    TrackingNet [[229](#bib.bib229)], OxUvA [[230](#bib.bib230)], BUAA-PRO [[231](#bib.bib231)],
    GOT10k [[232](#bib.bib232)], LaSOT [[221](#bib.bib221)], UAV20L [[222](#bib.bib222)],
    TinyTLP/TLPattr [[233](#bib.bib233)], TLP [[233](#bib.bib233)], TracKlinic [[234](#bib.bib234)],
    UAVDT [[223](#bib.bib223)], LTB35 [[235](#bib.bib235)], VisDrone [[236](#bib.bib236),
    [224](#bib.bib224)], VisDrone2019L [[224](#bib.bib224)], and Small-90/Small-112
    [[237](#bib.bib237)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 对于知名的单目标视觉跟踪数据集（即短期、长期、航拍视角），根据其基本特征（例如视频数量、帧数、类别/聚类、序列属性、缺失标签以及与其他数据集的重叠）进行了全面比较。这些基准数据集包括OTB2013
    [[219](#bib.bib219)]、OTB2015 [[220](#bib.bib220)]、VOT [[34](#bib.bib34)、[35](#bib.bib35)、[36](#bib.bib36)、[37](#bib.bib37)、[38](#bib.bib38)、[39](#bib.bib39)、[40](#bib.bib40)]、ALOY
    [[42](#bib.bib42)]、TC128 [[225](#bib.bib225)]、UAV123 [[222](#bib.bib222)]、NUS-PRO
    [[226](#bib.bib226)]、NfS [[227](#bib.bib227)]、DTB [[228](#bib.bib228)]、TrackingNet
    [[229](#bib.bib229)]、OxUvA [[230](#bib.bib230)]、BUAA-PRO [[231](#bib.bib231)]、GOT10k
    [[232](#bib.bib232)]、LaSOT [[221](#bib.bib221)]、UAV20L [[222](#bib.bib222)]、TinyTLP/TLPattr
    [[233](#bib.bib233)]、TLP [[233](#bib.bib233)]、TracKlinic [[234](#bib.bib234)]、UAVDT
    [[223](#bib.bib223)]、LTB35 [[235](#bib.bib235)]、VisDrone [[236](#bib.bib236)、[224](#bib.bib224)]、VisDrone2019L
    [[224](#bib.bib224)]，以及Small-90/Small-112 [[237](#bib.bib237)]。
- en: 4) Finally, extensive experimental evaluations are performed on a wide variety
    of tracking datasets, namely OTB2013 [[219](#bib.bib219)], OTB2015 [[220](#bib.bib220)],
    VOT2018 [[39](#bib.bib39)], LaSOT [[221](#bib.bib221)], UAV123 [[222](#bib.bib222)],
    UAVDT [[223](#bib.bib223)], and VisDrone2019 [[224](#bib.bib224)], and the state-of-the-art
    visual trackers are analyzed based on different aspects. Moreover, this work specifies
    the most challenging visual attributes for the VOT2018 dataset and OTB2015, LaSOT,
    UAV123, UAVDT, and VisDrone2019 datasets. By doing so, the most primary challenges
    of each dataset for recent trackers are specified.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 最后，对各种跟踪数据集进行了广泛的实验评估，具体包括OTB2013 [[219](#bib.bib219)]、OTB2015 [[220](#bib.bib220)]、VOT2018
    [[39](#bib.bib39)]、LaSOT [[221](#bib.bib221)]、UAV123 [[222](#bib.bib222)]、UAVDT
    [[223](#bib.bib223)] 和VisDrone2019 [[224](#bib.bib224)]，并对最先进的视觉跟踪器在不同方面进行了分析。此外，本工作指出了VOT2018、OTB2015、LaSOT、UAV123、UAVDT和VisDrone2019数据集的最具挑战性的视觉属性。通过这样做，明确了各数据集对近期跟踪器的主要挑战。
- en: According to the comparisons, the following remarks are concluded.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些比较，得出以下结论。
- en: 1) The Siamese-based networks are the most promising deep architectures due
    to their satisfactory balance between performance and efficiency for visual tracking.
    Moreover, some methods recently attempt to exploit the advantages of RL & GAN
    approaches to refine their decision-making and alleviate the lack of training
    data. Based on these advantages, recent trackers aim to design custom neural networks
    to fully exploit scene information.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Siamese网络的深度架构由于其在视觉跟踪中的性能和效率之间的满意平衡，成为最有前景的深度架构。此外，一些方法最近尝试利用RL和GAN方法的优势来优化决策并缓解训练数据不足的问题。基于这些优势，最近的跟踪器旨在设计自定义神经网络，以充分利用场景信息。
- en: 2) The offline end-to-end learning of deep features appropriately transfers
    pre-trained generic features to visual tracking task. Although conventional online
    training of DNN increases the computational complexity such that most of these
    methods are not suitable for real-time applications, it considerably helps visual
    trackers to adapt with significant appearance variation, prevent visual distractors,
    and improve the performance of visual trackers. Exploiting meta-learning approaches
    have provided significant advances to the online adaptation of visual trackers.
    Therefore, both offline and (efficient) online training procedures result in promising
    tracking performances.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 离线端到端的深度特征学习将预训练的通用特征适当地转移到视觉跟踪任务中。虽然传统的DNN在线训练增加了计算复杂性，使得这些方法大多不适合实时应用，但它显著帮助视觉跟踪器适应显著的外观变化，防止视觉干扰，并提高视觉跟踪器的性能。利用元学习方法对视觉跟踪器的在线适应提供了显著进展。因此，离线和（高效的）在线训练程序都能带来有前景的跟踪性能。
- en: 3) Leveraging deeper and wider backbone networks improves the discriminative
    power of distinguishing the target from its background. Pre-trained networks (e.g.,
    ResNet [[32](#bib.bib32)]) are sub-optimal, and tracking performance can be remarkably
    improved by training backbone networks for visual tracking.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 利用更深更宽的骨干网络提高了区分目标与背景的判别能力。预训练的网络（例如，ResNet [[32](#bib.bib32)]) 是次优的，通过为视觉跟踪训练骨干网络，可以显著提高跟踪性能。
- en: 4) The best trackers exploit both regression & classification objective functions
    to distinguish the target from the background and find the tightest BB for target
    localization. These objectives are complementary such that the regression function
    has the role of auxiliary supervision on the classification one. Recently, video
    object segmentation (VOS) approaches are integrated into visual trackers for representing
    targets by segmentation masks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳跟踪器利用回归和分类目标函数来区分目标与背景，并找到最紧密的边界框（BB）以实现目标定位。这些目标是互补的，即回归函数在分类函数上起到辅助监督的作用。最近，视频对象分割（VOS）方法被集成到视觉跟踪器中，通过分割掩码表示目标。
- en: 5) The exploitation of different features enhances the robustness of the target
    model. For instance, most of the DCF-based methods fuse the deep off-the-shelf
    features and hand-crafted features (e.g., HOG & CN) for this reason. Also, the
    exploitation of complementary features, such as temporal or contextual information,
    has led to more robust features in challenging scenarios.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 利用不同的特征增强了目标模型的鲁棒性。例如，大多数基于DCF的方法因此融合了深度的现成特征和手工设计的特征（例如，HOG和CN）。此外，利用互补特征，如时间或上下文信息，也在具有挑战性的场景中产生了更鲁棒的特征。
- en: 6) The most challenging attributes for DL-based visual tracking methods are
    occlusion, out-of-view, fast motion, aspect ratio change, and similar objects.
    Moreover, visual distractors with similar semantics may result in the drift problem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于深度学习的视觉跟踪方法，最具挑战性的属性包括遮挡、视野外、快速运动、纵横比变化和相似对象。此外，具有相似语义的视觉干扰物可能导致漂移问题。
- en: 'The rest of this paper is as follows. Section [II](#S2 "II Deep Visual Tracking
    Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey") introduces
    our taxonomy of deep visual trackers. The visual tracking benchmark datasets and
    evaluation metrics are compared in Section [III](#S3 "III Visual Tracking Benchmark
    Datasets ‣ Deep Learning for Visual Tracking: A Comprehensive Survey"). Experimental
    comparisons of the state-of-the-art visual tracking methods are performed in Section
    [IV](#S4 "IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey"). Finally, Section [V](#S5 "V Conclusion and Future Directions ‣ Deep
    Learning for Visual Tracking: A Comprehensive Survey") summarizes the conclusions
    and future directions.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分如下。第二部分[II](#S2 "II Deep Visual Tracking Taxonomy ‣ Deep Learning for
    Visual Tracking: A Comprehensive Survey")介绍了我们的深度视觉跟踪器分类法。视觉跟踪基准数据集和评估指标在第三部分[III](#S3
    "III Visual Tracking Benchmark Datasets ‣ Deep Learning for Visual Tracking: A
    Comprehensive Survey")进行了比较。第四部分[IV](#S4 "IV Experimental Analyses ‣ Deep Learning
    for Visual Tracking: A Comprehensive Survey")进行了最先进视觉跟踪方法的实验比较。最后，第五部分[V](#S5
    "V Conclusion and Future Directions ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")总结了结论和未来方向。'
- en: II Deep Visual Tracking Taxonomy
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 深度视觉跟踪分类法
- en: 'Generally, three major components of: i) target representation/information,
    ii) training process, and iii) learning procedure play important roles in designing
    visual tracking methods. Most DL-based trackers aim to improve a target representation
    by utilizing/fusing deep hierarchical features, exploiting contextual/motion information,
    and select more discriminative/robust deep features. To effectively train DNNs
    for visual tracking systems, general motivations can be classified into employing
    various training schemes (e.g., network pre-training, online training (also meta-learning),
    or both) and handling training problems (e.g., lacking training samples, over-fitting,
    or computational complexity). Unsupervised training is another recent scheme to
    use abundant unlabeled samples, which can be performed by clustering the samples
    according to contextual information, mapping training data to a manifold space,
    or exploiting consistency-based objective function. Finally, the primary motivations
    regarding learning procedures are online update schemes, scale/aspect ratio estimation,
    search strategies, and long-term memory.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，三大主要组成部分：i) 目标表示/信息，ii) 训练过程，iii) 学习过程在设计视觉跟踪方法中扮演重要角色。大多数基于DL的跟踪器旨在通过利用/融合深度层次特征、利用上下文/运动信息并选择更具区分性/鲁棒性的深度特征来改进目标表示。为了有效地训练用于视觉跟踪系统的DNN，主要动机可以分为采用各种训练方案（例如，网络预训练、在线训练（也包括元学习）或两者兼有）和处理训练问题（例如，缺乏训练样本、过拟合或计算复杂性）。无监督训练是另一种最近的方案，利用丰富的未标记样本，可以通过根据上下文信息对样本进行聚类、将训练数据映射到流形空间或利用一致性基础的目标函数来实现。最后，关于学习过程的主要动机包括在线更新方案、尺度/纵横比估计、搜索策略和长期记忆。
- en: '![Refer to caption](img/6e2db2142b5a4b780a13f4a335b9de2f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6e2db2142b5a4b780a13f4a335b9de2f.png)'
- en: 'Figure 3: Taxonomy of DL-based visual tracking methods.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于DL的视觉跟踪方法的分类。
- en: 'In the following, DL-based methods are comprehensively categorized based on
    nine aspects, and the main motivations and contributions of trackers are classified.
    Fig. [3](#S2.F3 "Figure 3 ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for
    Visual Tracking: A Comprehensive Survey") presents the proposed taxonomy of DL-based
    visual trackers, including network architecture, network exploitation, network
    training for visual tracking purposes, network objective, network output, exploitation
    of correlation filter advantages, aerial-view tracking, long-term tracking, and
    online tracking. Moreover, DL-based trackers are compared in detail regarding
    the pre-trained networks, backbone networks, exploited layers, types of deep features,
    the fusions of hand-crafted & deep features, training datasets, tracking outputs,
    tracking speeds, hardware implementation details, programming languages, and DL
    frameworks.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '以下内容对基于DL的方法进行了全面的分类，涵盖了九个方面，并对跟踪器的主要动机和贡献进行了分类。图[3](#S2.F3 "Figure 3 ‣ II
    Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")展示了基于DL的视觉跟踪器的分类法，包括网络架构、网络利用、用于视觉跟踪目的的网络训练、网络目标、网络输出、相关滤波器优势的利用、航拍跟踪、长期跟踪以及在线跟踪。此外，对基于DL的跟踪器在预训练网络、主干网络、利用的层、深度特征类型、手工制作与深度特征的融合、训练数据集、跟踪输出、跟踪速度、硬件实现细节、编程语言以及DL框架方面进行了详细比较。'
- en: II-A Network Architecture
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 网络架构
- en: Although CNNs have been extensively adopted for visual tracking, other architectures
    also have been mainly developed to improve the efficiency and robustness of visual
    trackers in recent years. Accordingly, the proposed taxonomy consists of the CNN-,
    SNN-, GAN-, RNN-, and custom network-based visual trackers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CNN已广泛应用于视觉跟踪，但近年来其他架构也主要用于提高视觉跟踪器的效率和鲁棒性。因此，提出的分类法包括基于CNN、SNN、GAN、RNN和自定义网络的视觉跟踪器。
- en: CNN-based trackers were the first to provide powerful representations of a target
    by hierarchical processing of two-dimensional frames, independently. However,
    conventional CNNs have inherent limitations, such as training on large supervised
    datasets, ignoring temporal dependencies, and computational complexities for online
    adaptation. As an alternative approach, SNN-based trackers measure the similarity
    between the target exemplar and the search region to overcome the limitations.
    Generally, SNNs employ CNN layers/blocks/networks in two or more branches for
    similarity learning purposes and run (near/over) real-time speed. However, online
    adaptation and handling challenges like occlusion are still under investigation.
    Architectures such as RNNs and GANs have been limitedly studied for visual tracking.
    In general, RNNs are used to capture temporal information among video frames,
    but they have limitations in their stability and long-term learning dependencies.
    GANs comprise generator & discriminator sub-networks, which can provide the possibility
    to address some limitations. For instance, competing for these networks can help
    trackers handle scarce positive samples, although there are some barriers to training
    and generalization of GANs. Finally, recent custom networks include various architectures
    to strengthen learning features and reduce computational complexity. In the following,
    the primary contributions of DL-based visual trackers are summarized.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基于CNN的跟踪器首次通过对二维帧进行分层处理，独立地提供了强大的目标表示。然而，传统的CNN存在固有的局限性，如在大规模监督数据集上进行训练、忽略时间依赖性和在线适应的计算复杂性。作为一种替代方法，基于SNN的跟踪器通过测量目标样本与搜索区域之间的相似性来克服这些局限性。一般来说，SNN在两个或更多分支中采用CNN层/块/网络进行相似性学习，并以（接近/超过）实时速度运行。然而，在线适应和处理诸如遮挡等挑战仍在研究中。像RNN和GAN这样的架构在视觉跟踪中的研究较为有限。一般而言，RNN用于捕捉视频帧之间的时间信息，但在稳定性和长期学习依赖性方面存在局限性。GAN由生成器和判别器子网络组成，可以提供解决一些局限性的可能性。例如，这些网络之间的竞争可以帮助跟踪器处理稀缺的正样本，尽管GAN的训练和泛化存在一些障碍。最后，最近的自定义网络包括各种架构，以增强学习特征并减少计算复杂性。以下是基于DL的视觉跟踪器的主要贡献总结。
- en: II-A1 Convolutional Neural Network (CNN)
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 卷积神经网络（CNN）
- en: Motivated by CNN breakthroughs in computer vision and their attractive advantages
    (e.g., parameter sharing, sparse interactions, and dominant representations),
    a wide range of CNN-based trackers have been proposed. The main motivations are
    presented as follows.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 受到CNN在计算机视觉领域突破及其吸引人的优势（如参数共享、稀疏交互和主导表示）的启发，提出了广泛的基于CNN的跟踪器。主要动机如下所述。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Robust target representation: Providing powerful representations of targets
    is the primary advantage of employing CNNs for visual tracking. To learn robust
    target models, the contributions can be classified into: i) offline training of
    CNNs on large-scale visual tracking datasets [[55](#bib.bib55), [60](#bib.bib60),
    [73](#bib.bib73), [82](#bib.bib82), [90](#bib.bib90), [93](#bib.bib93), [94](#bib.bib94),
    [97](#bib.bib97), [105](#bib.bib105), [109](#bib.bib109), [128](#bib.bib128),
    [130](#bib.bib130), [135](#bib.bib135), [137](#bib.bib137), [146](#bib.bib146),
    [161](#bib.bib161), [164](#bib.bib164), [165](#bib.bib165), [169](#bib.bib169)],
    ii) designing specific CNNs instead of employing pre-trained models [[55](#bib.bib55),
    [60](#bib.bib60), [62](#bib.bib62), [64](#bib.bib64), [66](#bib.bib66), [68](#bib.bib68),
    [69](#bib.bib69), [73](#bib.bib73), [75](#bib.bib75), [82](#bib.bib82), [90](#bib.bib90),
    [93](#bib.bib93), [94](#bib.bib94), [97](#bib.bib97), [98](#bib.bib98), [101](#bib.bib101),
    [105](#bib.bib105), [109](#bib.bib109), [120](#bib.bib120), [128](#bib.bib128),
    [130](#bib.bib130), [134](#bib.bib134), [135](#bib.bib135), [137](#bib.bib137),
    [139](#bib.bib139), [143](#bib.bib143), [146](#bib.bib146), [161](#bib.bib161),
    [163](#bib.bib163), [164](#bib.bib164), [165](#bib.bib165), [167](#bib.bib167),
    [169](#bib.bib169), [205](#bib.bib205)], iii) constructing multiple target models
    to capture varieties of target appearances [[68](#bib.bib68), [109](#bib.bib109),
    [120](#bib.bib120), [122](#bib.bib122), [123](#bib.bib123), [136](#bib.bib136),
    [139](#bib.bib139), [168](#bib.bib168), [210](#bib.bib210)], iv) incorporating
    spatial and temporal information to improve model generalization [[72](#bib.bib72),
    [75](#bib.bib75), [99](#bib.bib99), [112](#bib.bib112), [115](#bib.bib115), [130](#bib.bib130),
    [144](#bib.bib144), [146](#bib.bib146), [208](#bib.bib208), [209](#bib.bib209)],
    v) fusion of different deep features to exploit complementary spatial and semantic
    information [[56](#bib.bib56), [94](#bib.bib94), [101](#bib.bib101), [102](#bib.bib102),
    [128](#bib.bib128), [205](#bib.bib205), [204](#bib.bib204), [206](#bib.bib206)],
    vi) learning particular target models such as relative model [[97](#bib.bib97)]
    or part-based models [[109](#bib.bib109), [120](#bib.bib120), [139](#bib.bib139)]
    to handle partial occlusion and deformation, vii) utilizing a two-stream network
    [[120](#bib.bib120)] to prevent over-fitting and to learn rotation information,
    and accurately estimating target aspect ratio to avoid contaminating target model
    with non-relevant information [[174](#bib.bib174)], and viii) group feature selection
    through channel & spatial dimensions to learn the structural relevance of features.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 强大的目标表征：提供强大的目标表征是使用CNN进行视觉跟踪的主要优势。为了学习强大的目标模型，可以将贡献分类为：i) 在大规模视觉跟踪数据集上进行CNN的离线训练
    [[55](#bib.bib55), [60](#bib.bib60), [73](#bib.bib73), [82](#bib.bib82), [90](#bib.bib90),
    [93](#bib.bib93), [94](#bib.bib94), [97](#bib.bib97), [105](#bib.bib105), [109](#bib.bib109),
    [128](#bib.bib128), [130](#bib.bib130), [135](#bib.bib135), [137](#bib.bib137),
    [146](#bib.bib146), [161](#bib.bib161), [164](#bib.bib164), [165](#bib.bib165),
    [169](#bib.bib169)], ii) 设计特定的CNN而不是使用预训练模型 [[55](#bib.bib55), [60](#bib.bib60),
    [62](#bib.bib62), [64](#bib.bib64), [66](#bib.bib66), [68](#bib.bib68), [69](#bib.bib69),
    [73](#bib.bib73), [75](#bib.bib75), [82](#bib.bib82), [90](#bib.bib90), [93](#bib.bib93),
    [94](#bib.bib94), [97](#bib.bib97), [98](#bib.bib98), [101](#bib.bib101), [105](#bib.bib105),
    [109](#bib.bib109), [120](#bib.bib120), [128](#bib.bib128), [130](#bib.bib130),
    [134](#bib.bib134), [135](#bib.bib135), [137](#bib.bib137), [139](#bib.bib139),
    [143](#bib.bib143), [146](#bib.bib146), [161](#bib.bib161), [163](#bib.bib163),
    [164](#bib.bib164), [165](#bib.bib165), [167](#bib.bib167), [169](#bib.bib169),
    [205](#bib.bib205)], iii) 构建多个目标模型以捕捉目标外观的多样性 [[68](#bib.bib68), [109](#bib.bib109),
    [120](#bib.bib120), [122](#bib.bib122), [123](#bib.bib123), [136](#bib.bib136),
    [139](#bib.bib139), [168](#bib.bib168), [210](#bib.bib210)], iv) 结合空间和时间信息以提高模型的泛化能力
    [[72](#bib.bib72), [75](#bib.bib75), [99](#bib.bib99), [112](#bib.bib112), [115](#bib.bib115),
    [130](#bib.bib130), [144](#bib.bib144), [146](#bib.bib146), [208](#bib.bib208),
    [209](#bib.bib209)], v) 融合不同的深度特征以利用互补的空间和语义信息 [[56](#bib.bib56), [94](#bib.bib94),
    [101](#bib.bib101), [102](#bib.bib102), [128](#bib.bib128), [205](#bib.bib205),
    [204](#bib.bib204), [206](#bib.bib206)], vi) 学习特定的目标模型，如相对模型 [[97](#bib.bib97)]
    或基于部分的模型 [[109](#bib.bib109), [120](#bib.bib120), [139](#bib.bib139)] 以处理部分遮挡和变形，vii)
    利用双流网络 [[120](#bib.bib120)] 防止过拟合并学习旋转信息，并准确估计目标的纵横比以避免将不相关信息污染目标模型 [[174](#bib.bib174)]，以及
    viii) 通过通道和空间维度进行特征选择以学习特征的结构相关性。
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Balancing training data: Based on problem definition, there is just one positive
    sample in the first frame that increases the risk of over-fitting during tracking.
    Although the background information arbitrary can be considered negative in each
    frame, target sampling based on imperfect target estimations may also lead to
    noisy/unreliable training samples. These issues dramatically affect the performance
    of visual tracking methods. To alleviate them, CNN-based trackers propose: i)
    domain adaption mechanism (i.e., transferring learned knowledge from the source
    domain to target one with insufficient samples) [[82](#bib.bib82), [161](#bib.bib161)],
    ii) various update mechanisms (e.g., periodic, stochastic, short-term, & long-term
    updates) [[98](#bib.bib98), [122](#bib.bib122), [136](#bib.bib136), [142](#bib.bib142),
    [165](#bib.bib165)], iii) convolutional Fisher discriminative analysis (FDA) for
    positive and negative sample mining [[134](#bib.bib134)], iv) multiple-branch
    CNN for online ensemble learning [[90](#bib.bib90)], v) efficient sampling strategies
    to increase the number of training samples [[167](#bib.bib167)], and vi) a recursive
    least-square estimation algorithm to provide a compromise between the discrimination
    power & update iterations during online learning [[195](#bib.bib195)].'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平衡训练数据：根据问题定义，在第一个帧中只有一个正样本，这增加了跟踪过程中过拟合的风险。尽管背景信息可以在每一帧中被认为是负样本，但基于不完美目标估计的目标采样也可能导致噪声/不可靠的训练样本。这些问题严重影响视觉跟踪方法的性能。为了缓解这些问题，基于CNN的跟踪器提出了：i)
    域适应机制（即将从源领域学习到的知识转移到样本不足的目标领域）[[82](#bib.bib82), [161](#bib.bib161)]，ii) 各种更新机制（例如，周期性、随机性、短期和长期更新）[[98](#bib.bib98),
    [122](#bib.bib122), [136](#bib.bib136), [142](#bib.bib142), [165](#bib.bib165)]，iii)
    用于正负样本挖掘的卷积Fisher判别分析（FDA）[[134](#bib.bib134)]，iv) 用于在线集成学习的多分支CNN [[90](#bib.bib90)]，v)
    高效的采样策略以增加训练样本的数量[[167](#bib.bib167)]，以及vi) 递归最小二乘估计算法，以在在线学习过程中提供辨别能力与更新迭代之间的折衷[[195](#bib.bib195)]。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Computational complexity problem: Despite the significant progress of CNNs
    in appearance representation, the CNN-based methods still suffer from high computational
    complexity. To reduce this limitation, CNN-based visual tracking methods exploit
    different solutions, namely: i) employing a straightforward CNN architecture [[184](#bib.bib184)],
    ii) disassembling a CNN into several shrunken networks [[69](#bib.bib69)], iii)
    compressing or pruning training sample space [[87](#bib.bib87), [108](#bib.bib108),
    [134](#bib.bib134), [146](#bib.bib146), [164](#bib.bib164)] or feature selection
    [[53](#bib.bib53), [147](#bib.bib147)], iv) feature computation via RoIAlign operation
    [[105](#bib.bib105)] (i.e., feature approximation via bilinear interpolation)
    or oblique random forest [[92](#bib.bib92)] for better data capturing, v) corrective
    domain adaption method [[161](#bib.bib161)], vi) lightweight structure [[64](#bib.bib64),
    [66](#bib.bib66), [163](#bib.bib163)], vii) efficient optimization processes [[91](#bib.bib91),
    [148](#bib.bib148)], viii) particle sampling strategy [[89](#bib.bib89)], ix)
    utilizing attentional mechanism [[93](#bib.bib93)], x) extending the random vector
    functional link (RVFL) network to a convolutional structure [[183](#bib.bib183)],
    and xi) exploiting advantages of correlation filters [[51](#bib.bib51), [52](#bib.bib52),
    [53](#bib.bib53), [56](#bib.bib56), [61](#bib.bib61), [67](#bib.bib67), [70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73), [76](#bib.bib76), [78](#bib.bib78),
    [79](#bib.bib79), [85](#bib.bib85), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89),
    [91](#bib.bib91), [93](#bib.bib93), [99](#bib.bib99), [101](#bib.bib101), [102](#bib.bib102),
    [108](#bib.bib108), [112](#bib.bib112), [115](#bib.bib115), [119](#bib.bib119),
    [120](#bib.bib120), [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124),
    [128](#bib.bib128), [133](#bib.bib133), [134](#bib.bib134), [136](#bib.bib136),
    [137](#bib.bib137), [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144),
    [148](#bib.bib148), [152](#bib.bib152), [161](#bib.bib161), [163](#bib.bib163),
    [167](#bib.bib167), [168](#bib.bib168), [170](#bib.bib170)] for efficient computations.
    Exploiting the advantages of correlation filters refers to either applying DCFs
    on pre-trained networks or combining correlation filters/layers/functions with
    end-to-end networks.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算复杂度问题：尽管 CNN 在外观表示方面取得了显著进展，但基于 CNN 的方法仍然面临高计算复杂度的问题。为了减少这一限制，基于 CNN 的视觉跟踪方法采用了不同的解决方案，即：i)
    使用简单的 CNN 架构 [[184](#bib.bib184)]，ii) 将 CNN 拆解为多个缩小的网络 [[69](#bib.bib69)]，iii)
    压缩或剪枝训练样本空间 [[87](#bib.bib87), [108](#bib.bib108), [134](#bib.bib134), [146](#bib.bib146),
    [164](#bib.bib164)] 或特征选择 [[53](#bib.bib53), [147](#bib.bib147)]，iv) 通过 RoIAlign
    操作 [[105](#bib.bib105)]（即通过双线性插值进行特征近似）或斜率随机森林 [[92](#bib.bib92)] 进行更好的数据捕获，v)
    校正领域适应方法 [[161](#bib.bib161)]，vi) 轻量级结构 [[64](#bib.bib64), [66](#bib.bib66), [163](#bib.bib163)]，vii)
    高效优化过程 [[91](#bib.bib91), [148](#bib.bib148)]，viii) 粒子采样策略 [[89](#bib.bib89)]，ix)
    利用注意机制 [[93](#bib.bib93)]，x) 将随机向量函数链 (RVFL) 网络扩展到卷积结构 [[183](#bib.bib183)]，以及
    xi) 利用相关滤波器的优势 [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [56](#bib.bib56),
    [61](#bib.bib61), [67](#bib.bib67), [70](#bib.bib70), [71](#bib.bib71), [72](#bib.bib72),
    [73](#bib.bib73), [76](#bib.bib76), [78](#bib.bib78), [79](#bib.bib79), [85](#bib.bib85),
    [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [91](#bib.bib91), [93](#bib.bib93),
    [99](#bib.bib99), [101](#bib.bib101), [102](#bib.bib102), [108](#bib.bib108),
    [112](#bib.bib112), [115](#bib.bib115), [119](#bib.bib119), [120](#bib.bib120),
    [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124), [128](#bib.bib128),
    [133](#bib.bib133), [134](#bib.bib134), [136](#bib.bib136), [137](#bib.bib137),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144), [148](#bib.bib148),
    [152](#bib.bib152), [161](#bib.bib161), [163](#bib.bib163), [167](#bib.bib167),
    [168](#bib.bib168), [170](#bib.bib170)] 进行高效计算。利用相关滤波器的优势是指将 DCFs 应用于预训练网络或将相关滤波器/层/函数与端到端网络结合使用。
- en: II-A2 Siamese Neural Network (SNN)
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 孪生神经网络 (SNN)
- en: SNNs are widely employed for visual trackers in the past few years. Given the
    pairs of target and search regions, these two-stream networks compute the same
    function to produce a similarity map. They mainly aim to overcome the limitations
    of pre-trained deep CNNs and take full advantage of end-to-end learning for real-time
    applications.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，SNNs 被广泛应用于视觉跟踪器。给定目标和搜索区域的配对，这些双流网络计算相同的函数以生成相似度图。它们主要旨在克服预训练深度 CNN 的局限性，并充分利用端到端学习来实现实时应用。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Discriminative target representation: The ability to construct a robust target
    model majorly relies on target representation. For achieving more discriminative
    deep features and improving target modeling, SNN-based methods propose: i) learning
    distractor-aware [[104](#bib.bib104)] or target-aware features [[157](#bib.bib157)],
    ii) fusing deep multi-level features [[125](#bib.bib125), [150](#bib.bib150)]
    or combining confidence maps [[81](#bib.bib81), [83](#bib.bib83), [117](#bib.bib117)],
    iii) utilizing different loss functions in Siamese formulation to train more effective
    filters [[100](#bib.bib100), [155](#bib.bib155), [157](#bib.bib157), [158](#bib.bib158),
    [159](#bib.bib159)], iv) leveraging different types of deep features such as context
    information [[110](#bib.bib110), [117](#bib.bib117), [151](#bib.bib151)] or temporal
    features/models [[57](#bib.bib57), [74](#bib.bib74), [118](#bib.bib118), [126](#bib.bib126),
    [151](#bib.bib151), [171](#bib.bib171)], v) full exploring of low-level spatial
    features [[125](#bib.bib125), [150](#bib.bib150)], vi) considering angle estimation
    of a target to prevent salient background objects [[111](#bib.bib111)], vii) utilizing
    multi-stage regression to refine target representation [[150](#bib.bib150)], vii)
    using the deeper and wider deep network as the backbone to increase the receptive
    field of neurons, which is equivalent to capturing the structure of the target
    [[154](#bib.bib154)], viii) employing correlation-guided attention modules to
    exploit the relationship between the template & RoI feature maps [[190](#bib.bib190)],
    ix) computing correlations between attentional features [[197](#bib.bib197)],
    x) accurately estimating the scale and aspect ratio of the target [[198](#bib.bib198)],
    xi) simultaneously learning classification & regression models [[199](#bib.bib199)],
    xii) mining hard samples in training [[200](#bib.bib200)], and xiii) employing
    skimming & perusal modules for inferring optimal target candidates [[218](#bib.bib218)].
    Finally, [[191](#bib.bib191), [193](#bib.bib193)] are performed adversarial attacks
    on SNN-based trackers to evaluate misbehaving SNN models for visual tracking scenarios.
    These methods generate slight perturbations for deceiving the trackers to finally
    investigate DL models and improve their robustness.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 区分目标表示：构建一个稳健的目标模型主要依赖于目标表示。为了实现更具区分性的深度特征和改进目标建模，基于SNN的方法提出了以下策略：i) 学习干扰感知特征[[104](#bib.bib104)]或目标感知特征[[157](#bib.bib157)]，ii)
    融合深度多级特征[[125](#bib.bib125), [150](#bib.bib150)]或结合置信度图[[81](#bib.bib81), [83](#bib.bib83),
    [117](#bib.bib117)]，iii) 在Siamese模型中利用不同的损失函数来训练更有效的滤波器[[100](#bib.bib100), [155](#bib.bib155),
    [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159)]，iv) 利用不同类型的深度特征，如上下文信息[[110](#bib.bib110),
    [117](#bib.bib117), [151](#bib.bib151)]或时间特征/模型[[57](#bib.bib57), [74](#bib.bib74),
    [118](#bib.bib118), [126](#bib.bib126), [151](#bib.bib151), [171](#bib.bib171)]，v)
    充分挖掘低级空间特征[[125](#bib.bib125), [150](#bib.bib150)]，vi) 考虑目标的角度估计以防止显著背景对象[[111](#bib.bib111)]，vii)
    利用多阶段回归来细化目标表示[[150](#bib.bib150)]，viii) 使用更深更广的深度网络作为主干以增加神经元的感受野，相当于捕捉目标的结构[[154](#bib.bib154)]，ix)
    使用相关性引导的注意力模块来利用模板和RoI特征图之间的关系[[190](#bib.bib190)]，x) 计算注意特征之间的相关性[[197](#bib.bib197)]，xi)
    准确估计目标的尺度和长宽比[[198](#bib.bib198)]，xii) 同时学习分类和回归模型[[199](#bib.bib199)]，xiii) 挖掘训练中的困难样本[[200](#bib.bib200)]，以及xiv)
    使用略读和细读模块推断最佳目标候选[[218](#bib.bib218)]。最后，[[191](#bib.bib191), [193](#bib.bib193)]
    对基于SNN的跟踪器进行了对抗性攻击，以评估在视觉跟踪场景中表现不佳的SNN模型。这些方法生成轻微的扰动来欺骗跟踪器，最终研究DL模型并提高其鲁棒性。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adapting target appearance variation: Using only offline training of the first
    generation of SNN-based trackers caused a poor generalization to unseen targets.
    To solve it, recent SNN-based trackers propose: i) online update strategies considering
    strategies to reduce the risk of over-fitting [[74](#bib.bib74), [83](#bib.bib83),
    [86](#bib.bib86), [96](#bib.bib96), [104](#bib.bib104), [145](#bib.bib145), [187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189), [216](#bib.bib216)], ii) background suppression
    [[74](#bib.bib74), [104](#bib.bib104)], iii) formulating tracking task as a one-shot
    local detection task [[104](#bib.bib104), [116](#bib.bib116)], iv) and giving
    higher weights to important feature channels or score maps [[81](#bib.bib81),
    [117](#bib.bib117), [121](#bib.bib121), [141](#bib.bib141)], and v) modeling all
    potential distractors considering their motion and interaction [[200](#bib.bib200)].
    Alternatively, the DaSiamRPN [[104](#bib.bib104)] and MMLT [[107](#bib.bib107)]
    use a local-to-global search region strategy and memory exploitation to handle
    critical challenges such as full occlusion and out-of-view and enhance local search
    strategy.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 适应目标外观变化：仅使用第一代基于SNN的跟踪器的离线训练导致对未见目标的泛化效果较差。为了解决这个问题，最近的基于SNN的跟踪器提出了以下方法：i)
    在线更新策略，考虑减少过拟合风险的策略 [[74](#bib.bib74), [83](#bib.bib83), [86](#bib.bib86), [96](#bib.bib96),
    [104](#bib.bib104), [145](#bib.bib145), [187](#bib.bib187), [188](#bib.bib188),
    [189](#bib.bib189), [216](#bib.bib216)]，ii) 背景抑制 [[74](#bib.bib74), [104](#bib.bib104)]，iii)
    将跟踪任务表述为一次性局部检测任务 [[104](#bib.bib104), [116](#bib.bib116)]，iv) 对重要特征通道或得分图给予更高的权重
    [[81](#bib.bib81), [117](#bib.bib117), [121](#bib.bib121), [141](#bib.bib141)]，以及v)
    考虑所有潜在干扰因素的运动和交互 [[200](#bib.bib200)]。另外，DaSiamRPN [[104](#bib.bib104)] 和 MMLT
    [[107](#bib.bib107)] 使用局部到全局的搜索区域策略和记忆利用来处理关键挑战，如完全遮挡和视野外问题，并增强局部搜索策略。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Balancing training data: The same as CNN-based methods, some efforts by SNN-based
    methods have been performed to address the imbalance distribution of training
    samples. The main contributions of the SNN-based methods are: i) exploiting multi-stage
    Siamese framework to stimulate hard negative sampling [[150](#bib.bib150)], ii)
    adopting sampling heuristics such as fixed foreground-to-background ratio [[150](#bib.bib150)]
    or sampling strategies such as random sampling [[104](#bib.bib104)] or flow-guided
    sampling [[126](#bib.bib126)], and iii) taking advantages of correlation filter/layer
    into Siamese framework [[70](#bib.bib70), [71](#bib.bib71), [74](#bib.bib74),
    [86](#bib.bib86), [95](#bib.bib95), [104](#bib.bib104), [116](#bib.bib116), [118](#bib.bib118),
    [121](#bib.bib121), [141](#bib.bib141), [147](#bib.bib147), [149](#bib.bib149),
    [157](#bib.bib157), [158](#bib.bib158), [166](#bib.bib166), [145](#bib.bib145)].'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平衡训练数据：与基于CNN的方法相同，一些基于SNN的方法也采取了措施来解决训练样本分布不均的问题。基于SNN的方法的主要贡献包括：i) 利用多阶段Siamese框架来刺激困难负样本采样
    [[150](#bib.bib150)]，ii) 采用采样启发式方法，如固定的前景与背景比 [[150](#bib.bib150)]，或者采样策略，如随机采样
    [[104](#bib.bib104)] 或流引导采样 [[126](#bib.bib126)]，以及iii) 将相关滤波器/层纳入Siamese框架 [[70](#bib.bib70),
    [71](#bib.bib71), [74](#bib.bib74), [86](#bib.bib86), [95](#bib.bib95), [104](#bib.bib104),
    [116](#bib.bib116), [118](#bib.bib118), [121](#bib.bib121), [141](#bib.bib141),
    [147](#bib.bib147), [149](#bib.bib149), [157](#bib.bib157), [158](#bib.bib158),
    [166](#bib.bib166), [145](#bib.bib145)]。
- en: II-A3 Recurrent Neural Network (RNN)
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 循环神经网络 (RNN)
- en: Since visual tracking is related to both spatial and temporal information of
    video frames, RNN-based methods also consider target motion/movement. Because
    of arduous training and a considerable number of parameters, the number of RNN-based
    methods is limited. Almost all these methods try to exploit additional information
    and memory to improve target modeling. Also, the second aim of using RNN-based
    methods is to avoid fine-tuning of pre-trained CNN models, which takes a lot of
    time and is prone to over-fitting. The primary purposes of these methods can be
    classified to the spatio-temporal representation capturing [[77](#bib.bib77),
    [132](#bib.bib132), [171](#bib.bib171)], leveraging contextual information to
    handle background clutters [[132](#bib.bib132)], exploiting multi-level visual
    attention to highlight target, background suppression [[171](#bib.bib171)], and
    using convolutional long short-term memory (LSTM) as the memory unit of previous
    target appearances [[77](#bib.bib77)]. Moreover, RNN-based methods exploit pyramid
    multi-directional recurrent network [[132](#bib.bib132)] or incorporate LSTM into
    different networks [[77](#bib.bib77)] to memorize target appearance and investigate
    time dependencies. Finally, the [[132](#bib.bib132)] encodes the self-structure
    of a target to reduce tracking sensitivity related to similar distractors.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于视觉跟踪涉及视频帧的空间和时间信息，基于RNN的方法也考虑了目标运动/移动。由于训练艰难且参数众多，基于RNN的方法数量有限。这些方法几乎都试图利用额外的信息和记忆来改善目标建模。此外，使用基于RNN的方法的第二个目标是避免对预训练的CNN模型进行微调，因为这需要大量时间并且容易过拟合。这些方法的主要目的是捕捉时空表征[[77](#bib.bib77),
    [132](#bib.bib132), [171](#bib.bib171)]，利用上下文信息处理背景杂乱[[132](#bib.bib132)]，利用多级视觉注意力突出目标，背景抑制[[171](#bib.bib171)]，以及使用卷积长短期记忆（LSTM）作为先前目标出现的记忆单元[[77](#bib.bib77)]。此外，基于RNN的方法利用金字塔多方向递归网络[[132](#bib.bib132)]或将LSTM集成到不同的网络中[[77](#bib.bib77)]以记忆目标外观并研究时间依赖性。最后，[[132](#bib.bib132)]对目标的自我结构进行编码，以减少与类似干扰物相关的跟踪敏感性。
- en: II-A4 Generative Adversarial Network (GAN)
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A4 生成对抗网络（GAN）
- en: Based on some attractive advantages, such as capturing statistical distribution
    and generating desired training samples without extensive annotated data, GANs
    have been intensively utilized in many research areas. Although GANs are usually
    hard to train and evaluate, some DL-based trackers employ GANs to enrich training
    samples and target modeling. These networks can augment positive samples in feature
    space to address the imbalance distribution of training samples [[114](#bib.bib114)].
    Also, the GAN-based methods can learn general appearance distribution to deal
    with visual tracking’s self-learning problem [[129](#bib.bib129)]. Furthermore,
    the joint optimization of regression & discriminative networks will take advantage
    of both these two tasks [[160](#bib.bib160)]. Lastly, these networks can explore
    the relations between a target with its contextual information for searching interested
    regions and transfer this information to videos with different inherits such as
    transferring from ground-view to drone-view [[207](#bib.bib207)].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于一些吸引人的优势，如捕捉统计分布和在没有大量标注数据的情况下生成所需的训练样本，GANs已在许多研究领域得到了广泛应用。尽管GANs通常很难训练和评估，但一些基于DL的跟踪器采用GANs来丰富训练样本和目标建模。这些网络可以在特征空间中增强正样本，以解决训练样本的不平衡分布[[114](#bib.bib114)]。此外，基于GAN的方法可以学习一般外观分布，以处理视觉跟踪的自学习问题[[129](#bib.bib129)]。此外，回归和判别网络的联合优化将利用这两项任务的优势[[160](#bib.bib160)]。最后，这些网络可以探索目标与其上下文信息之间的关系，以搜索感兴趣的区域，并将这些信息转移到具有不同继承的视频中，例如从地面视图到无人机视图[[207](#bib.bib207)]。
- en: II-A5 Custom Networks
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A5 自定义网络
- en: Inspired by particular deep architectures and network layers, modern DL-based
    methods have combined a wide range of networks such as AE, CNN, RNN, SNN, detection
    networks & also deep RL for visual tracking. The main motivation of custom networks
    is to compensate for ordinary trackers’ deficiencies by exploiting the advantages
    of other networks. Furthermore, meta-learning (or learning to learn) has been
    recently attracted by the visual tracking community. It aims to address few-shot
    learning problems and fast adaptation of a learner to a new task by leveraging
    accumulated experiences from similar tasks. By employing the meta-learning framework,
    different networks can learn unseen target appearances during online tracking.
    The primary motivations and contributions of custom networks are classified as
    follows.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 受到特定深度架构和网络层的启发，现代基于深度学习的方法结合了广泛的网络，如AE、CNN、RNN、SNN、检测网络以及深度强化学习用于视觉跟踪。自定义网络的主要动机是通过利用其他网络的优势来弥补普通跟踪器的不足。此外，视觉跟踪领域最近对元学习（或学习学习）产生了兴趣。它旨在通过利用来自类似任务的积累经验来解决少量样本学习问题和学习者对新任务的快速适应。通过采用元学习框架，不同的网络可以在在线跟踪过程中学习未见的目标外观。自定义网络的主要动机和贡献如下。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Robust and accurate tracking: Recent networks seek general & effective frameworks
    for better localization and BB estimation. For instance, aggregating several online
    trackers [[203](#bib.bib203)] is a way to improve tracking performance. An alternative
    is a better understanding of the target’s pose by exclusively designed target
    estimation and classification networks [[149](#bib.bib149)]. Also, meta-learning
    based networks [[159](#bib.bib159), [194](#bib.bib194)] can predict powerful target
    models inspiring by discriminative learning procedures. However, some other works
    [[185](#bib.bib185), [201](#bib.bib201)] consider the tracking task as an instance
    detection and aim to convert modern object detectors directly to a visual tracker.
    These trackers exploit class-agnostic networks, which can: i) differentiate intra-class
    samples, ii) quickly adapt to different targets by meta-learners, and iii) consider
    temporal cues. The D3S [[192](#bib.bib192)] method models a visual target from
    its segmentation mask with complementary geometric properties to improve the robustness
    of template-based trackers. The COMET [[212](#bib.bib212)] bridges the gap between
    advanced visual trackers and aerial-view ones in detecting small/tiny objects.
    It employs multi-scale feature learning and attention modules to compensate for
    the inferior performance of generic trackers in medium-/high-attitude aerial views.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 强大且准确的跟踪：近期的网络寻求通用且有效的框架，以改善定位和边界框估计。例如，聚合多个在线跟踪器[[203](#bib.bib203)]是一种提高跟踪性能的方法。另一种方法是通过专门设计的目标估计和分类网络[[149](#bib.bib149)]来更好地理解目标的姿态。此外，基于元学习的网络[[159](#bib.bib159),
    [194](#bib.bib194)]可以通过鉴别学习过程预测强大的目标模型。然而，其他一些研究[[185](#bib.bib185), [201](#bib.bib201)]将跟踪任务视为实例检测，旨在直接将现代目标检测器转换为视觉跟踪器。这些跟踪器利用类别无关的网络，可以：i)
    区分同类样本，ii) 通过元学习者快速适应不同目标，以及 iii) 考虑时间线索。D3S [[192](#bib.bib192)]方法通过补充几何特性，从分割掩膜建模视觉目标，以提高基于模板的跟踪器的鲁棒性。COMET
    [[212](#bib.bib212)]弥合了先进视觉跟踪器和空中视角跟踪器在检测小/微小物体方面的差距。它利用多尺度特征学习和注意力模块来弥补通用跟踪器在中/高高度空中视角下的劣势。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Computational complexity problem: As stated before, this problem limits the
    performance of online trackers in real-time applications. To control computational
    complexity, the TRACA [[113](#bib.bib113)] and AEPCF [[167](#bib.bib167)] methods
    employ AEs to compress raw conventional deep features. The EAST [[177](#bib.bib177)]
    adaptively takes either shallow features for simple frames for tracking or expensive
    deep features for challenging ones [[177](#bib.bib177)], and the TRACA [[113](#bib.bib113)],
    CFSRL [[138](#bib.bib138)], & AEPCF [[167](#bib.bib167)] exploit the DCF computation
    efficiency. An effective way to avoid high computational burden is exploiting
    meta-learning that quickly adapts pre-trained trackers on unseen targets. The
    target model of the meta-learning based trackers can be optimized in a few iterations
    [[182](#bib.bib182), [159](#bib.bib159), [194](#bib.bib194), [185](#bib.bib185),
    [188](#bib.bib188), [196](#bib.bib196), [201](#bib.bib201), [217](#bib.bib217)].'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算复杂性问题：如前所述，这个问题限制了在线跟踪器在实时应用中的性能。为了控制计算复杂性，TRACA [[113](#bib.bib113)] 和 AEPCF
    [[167](#bib.bib167)] 方法采用 AEs 压缩原始传统深度特征。EAST [[177](#bib.bib177)] 自适应地选择浅层特征用于简单帧跟踪，或选择昂贵的深度特征用于挑战性帧
    [[177](#bib.bib177)]，TRACA [[113](#bib.bib113)]、CFSRL [[138](#bib.bib138)] 和 AEPCF
    [[167](#bib.bib167)] 则利用了 DCF 计算效率。避免高计算负担的有效方法是利用元学习，快速适应在未见目标上的预训练跟踪器。基于元学习的跟踪器的目标模型可以在少量迭代中优化
    [[182](#bib.bib182)、[159](#bib.bib159)、[194](#bib.bib194)、[185](#bib.bib185)、[188](#bib.bib188)、[196](#bib.bib196)、[201](#bib.bib201)、[217](#bib.bib217)]。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model update: To maintain the stability of the target model during the tracking
    process, different update strategies have been proposed; for instance, the CFSRL
    [[138](#bib.bib138)] updates multiple models in parallel, the DRRL [[162](#bib.bib162)]
    incorporates an LSTM to exploit long-range time dependencies, and the AEPCF [[167](#bib.bib167)]
    utilizes long-term and short-term update schemes to increase tracking speed. To
    prevent the erroneous model update and drift problem, the RDT [[180](#bib.bib180)]
    has revised the visual tracking formulation to a consecutive decision-making process
    about the best target template for the next localization. Moreover, efficient
    learning of good decision policies using RL [[179](#bib.bib179)] is another technique
    to take either model update or ignore the decision. A recent alternative is employing
    meta-learning approaches for quick model adaptation. For instance, the works [[196](#bib.bib196),
    [159](#bib.bib159), [194](#bib.bib194)] use recurrent optimization processes that
    update the target model in a few gradient steps. Finally, [[217](#bib.bib217)]
    integrates sequential information (e.g., geometric, discriminative, and appearance
    cues) and exploits a meta-updater to effectively update on reliable frames.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型更新：为了在跟踪过程中保持目标模型的稳定性，提出了不同的更新策略；例如，CFSRL [[138](#bib.bib138)] 并行更新多个模型，DRRL
    [[162](#bib.bib162)] 引入了 LSTM 以利用长时间依赖关系，而 AEPCF [[167](#bib.bib167)] 使用了长期和短期更新方案以提高跟踪速度。为防止模型更新错误和漂移问题，RDT
    [[180](#bib.bib180)] 将视觉跟踪公式修订为一个关于下一次定位的最佳目标模板的连续决策过程。此外，利用 RL [[179](#bib.bib179)]
    高效学习良好决策策略是另一种选择，可以决定更新模型或忽略决策。最近的一种替代方法是采用元学习方法以快速适应模型。例如，工作 [[196](#bib.bib196)、[159](#bib.bib159)、[194](#bib.bib194)]
    使用递归优化过程，在少量梯度步骤中更新目标模型。最后，[[217](#bib.bib217)] 整合了序列信息（例如几何、判别和外观线索）并利用元更新器在可靠帧上进行有效更新。
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Limited training data: The soft and non-representative training samples can
    disturb visual tracking if occlusion, blurring, and large deformation happen.
    The AEPCF [[167](#bib.bib167)] exploits a dense circular sampling scheme to prevent
    the over-fitting problem caused by limited training data. The SINT++ [[181](#bib.bib181)]
    generates positive and hard training samples by positive sample generation network
    (PSGN) and hard positive transformation network (HPTN) to make diverse and challenging
    training data. To efficiently train DNNs without a large amount of training data,
    partially labeled training samples are utilized by an action-driven deep tracker
    [[172](#bib.bib172), [173](#bib.bib173)]. The P-Track [[179](#bib.bib179)] also
    uses active decision-making to label videos interactively while learning a tracker
    when limited annotated data are available. Meta-Tracker [[182](#bib.bib182)] was
    the first attempt to exploit an offline meta-learning-based method for better
    online adaptation of visual trackers. This method can generalize the target model
    and avoid over-fitting to distractors. Besides, various pioneer trackers [[159](#bib.bib159),
    [194](#bib.bib194), [217](#bib.bib217), [201](#bib.bib201), [185](#bib.bib185),
    [188](#bib.bib188), [196](#bib.bib196)] enjoy the advantages of meta-learners
    in one/few-shot learning tasks.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有限的训练数据：如果发生遮挡、模糊和大变形，软性和非代表性的训练样本可能会干扰视觉跟踪。AEPCF[[167](#bib.bib167)]利用密集的圆形采样方案来防止由于有限训练数据引起的过拟合问题。SINT++[[181](#bib.bib181)]通过正样本生成网络（PSGN）和困难正样本转换网络（HPTN）生成正样本和困难样本，以制造多样化和具有挑战性的训练数据。为了有效训练深度神经网络而无需大量训练数据，部分标注的训练样本被动作驱动的深度跟踪器[[172](#bib.bib172)、[173](#bib.bib173)]所利用。P-Track[[179](#bib.bib179)]也使用主动决策在有限的标注数据可用时，交互式地标注视频，同时学习跟踪器。Meta-Tracker[[182](#bib.bib182)]是首次尝试利用离线元学习方法以更好地在线适应视觉跟踪器。这种方法可以泛化目标模型，并避免过拟合干扰物。此外，各种先锋跟踪器[[159](#bib.bib159)、[194](#bib.bib194)、[217](#bib.bib217)、[201](#bib.bib201)、[185](#bib.bib185)、[188](#bib.bib188)、[196](#bib.bib196)]在一-shot或few-shot学习任务中享有元学习者的优势。
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Search strategy: From the definition, visual tracking methods estimate the
    new target state in the next frame’s search region, given an initial target state
    in the first frame. The best search region selection depends on the iterative
    search strategies that usually are independent of video content and are heuristic,
    brute-force, and hand-engineered. Despite classical search strategies based on
    sliding windows, mean shift, or particle filter, the state-of-the-art DL-based
    visual trackers exploit RL-based methods to learn data-driven searching policies.
    To exhaustively explore a region of interest and select the best target candidate,
    action-driven tracking mechanisms [[172](#bib.bib172), [173](#bib.bib173)] consider
    the target context variation and actively pursues the target movement. Furthermore,
    the ACT and DRRL have proposed practical RL-based search strategies for real-time
    requirements by dynamic search [[103](#bib.bib103)] and coarse-to-fine verification
    [[162](#bib.bib162)]. Lastly, the full-image visual tracker [[214](#bib.bib214)]
    exploits a two-stage detector for localizing the target without any assumptions
    (e.g., temporal consistency of target regions).'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 搜索策略：根据定义，视觉跟踪方法通过给定第一帧中的初始目标状态，在下一帧的搜索区域内估计新的目标状态。最佳的搜索区域选择依赖于迭代搜索策略，这些策略通常与视频内容无关，并且是启发式的、暴力的和手工设计的。尽管基于滑动窗口、均值漂移或粒子滤波的经典搜索策略，最先进的基于深度学习的视觉跟踪器利用基于强化学习的方法来学习数据驱动的搜索策略。为了彻底探索感兴趣区域并选择最佳目标候选，基于动作的跟踪机制[[172](#bib.bib172)、[173](#bib.bib173)]考虑了目标上下文的变化，并积极追踪目标的移动。此外，ACT和DRRL通过动态搜索[[103](#bib.bib103)]和粗到细的验证[[162](#bib.bib162)]，提出了适用于实时需求的实际RL-based搜索策略。最后，全图像视觉跟踪器[[214](#bib.bib214)]利用两阶段检测器来定位目标，而无需任何假设（例如，目标区域的时间一致性）。
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Exploiting additional information: To enhance the target model by utilizing
    motion or contextual information, the DCTN [[131](#bib.bib131)] establishes a
    two-stream network, and the SRT [[80](#bib.bib80)] adopts multi-directional RNN
    to learn further dependencies of a target during visual tracking. Also, the FGTrack
    [[202](#bib.bib202)] estimates the scale & rotation of the target and its displacement
    by the finer-grained motion information provided by optical-flow. A recurrent
    convolutional network [[176](#bib.bib176)] models previous semantic information
    and tracking proposals to encode relevant information for better localization.
    At last, DRL-IS [[175](#bib.bib175)] has introduced an Actor-Critic network to
    estimate target motion parameters efficiently.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 利用额外信息：为了通过利用运动或上下文信息来增强目标模型，DCTN [[131](#bib.bib131)] 建立了一个双流网络，而 SRT [[80](#bib.bib80)]
    采用多方向 RNN 来进一步学习目标在视觉跟踪中的依赖关系。此外，FGTrack [[202](#bib.bib202)] 通过光流提供的更细粒度的运动信息来估计目标的尺度和旋转以及其位移。递归卷积网络
    [[176](#bib.bib176)] 对之前的语义信息和跟踪提议进行建模，以编码相关信息以获得更好的定位。最后，DRL-IS [[175](#bib.bib175)]
    引入了一个演员-评论家网络，以高效地估计目标运动参数。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Decision making: Online decision making has principal effects on the performance
    of DL-based visual tracking methods. The state-of-the-art methods attempt to learn
    online decision making by incorporating RL into the DL-based methods instead of
    hand-designed techniques. To gain effective decision policies, the P-Track [[179](#bib.bib179)]
    ultimately exploits data-driven techniques in an active agent to decide about
    tracking, re-initializing, or updating processes. Also, the DRL-IS [[175](#bib.bib175)]
    utilizes a principled RL-based method to select sensible action based on target
    status. Also, an action-prediction network has been proposed to adjust a visual
    tracker’s continuous actions to determine the optimal hyper-parameters for learning
    the best action policies and make satisfactory decisions [[178](#bib.bib178)].
    On the other hand, the work [[194](#bib.bib194)] considers the uncertainty in
    estimating target states. By predicting a conditional probability density of the
    visual target, direct interpretations can be provided for deciding about an update
    procedure or lost target. Also, a result judgment module [[213](#bib.bib213)]
    can help short-term trackers in occlusion/out-of-view situations.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 决策制定：在线决策对基于 DL 的视觉跟踪方法的性能有主要影响。最先进的方法试图通过将 RL 融入 DL 基础的方法来学习在线决策制定，而不是使用手工设计的技术。为了获得有效的决策策略，P-Track
    [[179](#bib.bib179)] 最终在一个主动代理中利用数据驱动技术来决定跟踪、重新初始化或更新过程。此外，DRL-IS [[175](#bib.bib175)]
    采用了一种原理性的 RL 基础方法，根据目标状态选择合理的动作。此外，还提出了一种动作预测网络，以调整视觉跟踪器的连续动作，从而确定最佳超参数以学习最佳的行动策略并做出令人满意的决策
    [[178](#bib.bib178)]。另一方面，工作 [[194](#bib.bib194)] 考虑了在估计目标状态时的不确定性。通过预测视觉目标的条件概率密度，可以直接解释有关更新程序或丢失目标的决策。此外，一个结果判断模块
    [[213](#bib.bib213)] 可以帮助短期跟踪器在遮挡/视野之外的情况下。
- en: II-B Network Exploitation
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 网络利用
- en: Roughly speaking, there are two main exploitations of DNNs for visual tracking,
    including reusing a pre-trained model on partially related datasets or exploiting
    deep features for visual tracking, which is equivalent to train DNNs for visual
    tracking tasks.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略来说，DNN 在视觉跟踪中的主要利用包括在部分相关数据集上重用预训练模型或利用深度特征进行视觉跟踪，这相当于为视觉跟踪任务训练 DNN。
- en: II-B1 Model Reuse or Deep Off-the-Shelf Features
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 模型重用或深度现成特征
- en: 'Exploiting deep off-the-shelf features is the simplest way to transfer the
    power of deep features into the traditional visual tracking methods. These features
    provide a generic representation of visual targets and help visual tracking methods
    to construct more robust target models. Regarding topologies, DNNs include either
    a simple multi-layer stack of non-linear layers (e.g., AlexNet [[28](#bib.bib28)],
    VGGNet [[29](#bib.bib29), [30](#bib.bib30)]) or a directed acyclic graph topology
    (e.g., GoogLeNet [[31](#bib.bib31)], ResNet [[121](#bib.bib121)], SSD [[238](#bib.bib238)],
    Siamese convolutional neural network [[239](#bib.bib239)]), which allows designing
    more complex deep architectures that include layers with multiple input/output.
    The main challenge of these trackers is how to benefit the generic representations
    effectively. Different methods employ various feature maps and models that have
    been pre-trained majorly on large-scale still images of the ImageNet dataset [[33](#bib.bib33)]
    for the object recognition task. Numerous methods have studied the properties
    of pre-trained models and explored the impact of deep features in traditional
    frameworks (see Table [I](#S2.T1 "TABLE I ‣ II-B1 Model Reuse or Deep Off-the-Shelf
    Features ‣ II-B Network Exploitation ‣ II Deep Visual Tracking Taxonomy ‣ Deep
    Learning for Visual Tracking: A Comprehensive Survey")). As a result, the DL-based
    methods have preferred simultaneous exploitation of both semantic and fine-grained
    deep features [[51](#bib.bib51), [53](#bib.bib53), [56](#bib.bib56), [133](#bib.bib133),
    [150](#bib.bib150), [240](#bib.bib240), [241](#bib.bib241), [204](#bib.bib204),
    [206](#bib.bib206)]. The fusion of deep features is also another motivation of
    these methods, which is performed by different techniques to utilize multi-resolution
    deep features [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [56](#bib.bib56),
    [61](#bib.bib61), [76](#bib.bib76), [123](#bib.bib123), [136](#bib.bib136), [168](#bib.bib168),
    [102](#bib.bib102), [145](#bib.bib145), [122](#bib.bib122), [204](#bib.bib204),
    [206](#bib.bib206)] and independent fusion of deep features with shallow ones
    at a later stage [[102](#bib.bib102)]. Exploiting motion information [[85](#bib.bib85),
    [99](#bib.bib99), [168](#bib.bib168), [242](#bib.bib242)] and selecting appropriate
    deep features for visual tracking tasks [[53](#bib.bib53), [147](#bib.bib147),
    [186](#bib.bib186)] are two other interesting motivations for DL-based methods.
    The detailed characteristics of DL-based visual trackers based on deep off-the-shelf
    features are shown in Table [I](#S2.T1 "TABLE I ‣ II-B1 Model Reuse or Deep Off-the-Shelf
    Features ‣ II-B Network Exploitation ‣ II Deep Visual Tracking Taxonomy ‣ Deep
    Learning for Visual Tracking: A Comprehensive Survey"). Needless to say, the network
    output for these methods are deep feature maps.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '利用深度现成特征是将深度特征的力量转移到传统视觉跟踪方法中的最简单方式。这些特征提供了视觉目标的通用表示，并帮助视觉跟踪方法构建更强大的目标模型。在拓扑结构方面，DNNs
    包含简单的多层非线性层堆叠（例如，AlexNet [[28](#bib.bib28)], VGGNet [[29](#bib.bib29), [30](#bib.bib30)]）或有向无环图拓扑（例如，GoogLeNet
    [[31](#bib.bib31)], ResNet [[121](#bib.bib121)], SSD [[238](#bib.bib238)], Siamese
    卷积神经网络 [[239](#bib.bib239)]），这允许设计更复杂的深度架构，包括具有多个输入/输出的层。这些跟踪器的主要挑战在于如何有效利用通用表示。不同的方法使用了在
    ImageNet 数据集 [[33](#bib.bib33)] 上预训练的大规模静态图像的各种特征图和模型进行物体识别任务。许多方法研究了预训练模型的特性，并探索了深度特征在传统框架中的影响（见表[I](#S2.T1
    "TABLE I ‣ II-B1 Model Reuse or Deep Off-the-Shelf Features ‣ II-B Network Exploitation
    ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")）。因此，基于 DL 的方法更倾向于同时利用语义和细粒度的深度特征 [[51](#bib.bib51), [53](#bib.bib53),
    [56](#bib.bib56), [133](#bib.bib133), [150](#bib.bib150), [240](#bib.bib240),
    [241](#bib.bib241), [204](#bib.bib204), [206](#bib.bib206)]。深度特征的融合也是这些方法的另一个动机，这通过不同的技术来利用多分辨率深度特征
    [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [56](#bib.bib56), [61](#bib.bib61),
    [76](#bib.bib76), [123](#bib.bib123), [136](#bib.bib136), [168](#bib.bib168),
    [102](#bib.bib102), [145](#bib.bib145), [122](#bib.bib122), [204](#bib.bib204),
    [206](#bib.bib206)] 和在后期阶段深度特征与浅层特征的独立融合 [[102](#bib.bib102)] 来实现。利用运动信息 [[85](#bib.bib85),
    [99](#bib.bib99), [168](#bib.bib168), [242](#bib.bib242)] 和选择适合视觉跟踪任务的深度特征 [[53](#bib.bib53),
    [147](#bib.bib147), [186](#bib.bib186)] 是基于 DL 的方法的另两个有趣动机。基于深度现成特征的 DL 视觉跟踪器的详细特征见表[I](#S2.T1
    "TABLE I ‣ II-B1 Model Reuse or Deep Off-the-Shelf Features ‣ II-B Network Exploitation
    ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")。不用说，这些方法的网络输出是深度特征图。'
- en: 'TABLE I: Deep off-the-shelf features for visual tracking. The abbreviations
    are denoted as: confidence map (CM), saliency map (SM), bounding box (BB), votes
    (vt), deep appearance features (DAF), deep motion features (DMF).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：用于视觉跟踪的深度现成特征。缩写表示为：置信度图（CM）、显著性图（SM）、边界框（BB）、投票（vt）、深度外观特征（DAF）、深度运动特征（DMF）。
- en: '| Method | Pre-trained models | Exploited layers | Pre-training data | Pre-training
    dataset(s) | Exploited features | PC (CPU, RAM, Nvidia GPU) | Language | Framework
    | Speed (fps) | Tracking output |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 预训练模型 | 利用的层 | 预训练数据 | 预训练数据集 | 利用的特征 | PC（CPU、RAM、Nvidia GPU） | 语言
    | 框架 | 速度（fps） | 跟踪输出 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| DeepSRDCF [[52](#bib.bib52)] | VGG-M | Conv5 | Still images | ImageNet |
    HOG, DAF | N/A, GPU | Matlab | MatConvNet | N/A | CM |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| DeepSRDCF [[52](#bib.bib52)] | VGG-M | Conv5 | 静态图像 | ImageNet | HOG, DAF
    | N/A, GPU | Matlab | MatConvNet | N/A | CM |'
- en: '| CCOT [[56](#bib.bib56)] | VGG-M | Conv1, Conv5 | Still images | ImageNet
    | HOG, CN, DAF | N/A, GPU | Matlab | MatConvNet |  1 | CM |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| CCOT [[56](#bib.bib56)] | VGG-M | Conv1, Conv5 | 静态图像 | ImageNet | HOG, CN,
    DAF | N/A, GPU | Matlab | MatConvNet | 1 | CM |'
- en: '| ECO [[87](#bib.bib87)] | VGG-M | Conv1, Conv5 | Still images | ImageNet |
    HOG, CN, DAF | N/A, GPU | Matlab | MatConvNet | 8 | CM |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ECO [[87](#bib.bib87)] | VGG-M | Conv1, Conv5 | 静态图像 | ImageNet | HOG, CN,
    DAF | N/A, GPU | Matlab | MatConvNet | 8 | CM |'
- en: '| DeepCSRDCF [[88](#bib.bib88)] | VGG-M | N/A | Still images | ImageNet | HOG,
    CN, DAF | Intel I7 3.4GHz CPU, GPU | Matlab | MatConvNet | 13 | CM |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| DeepCSRDCF [[88](#bib.bib88)] | VGG-M | N/A | 静态图像 | ImageNet | HOG, CN,
    DAF | Intel I7 3.4GHz CPU, GPU | Matlab | MatConvNet | 13 | CM |'
- en: '| SASR [[211](#bib.bib211)] | VGG-M | Conv4 | Still images | ImageNet | DAF
    | Intel-8700K 3.7GHz CPU, 32GB RAM, Quadro P2000 GPU | Matlab | MatConvNet | 3.84
    | CM |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| SASR [[211](#bib.bib211)] | VGG-M | Conv4 | 静态图像 | ImageNet | DAF | Intel-8700K
    3.7GHz CPU, 32GB RAM, Quadro P2000 GPU | Matlab | MatConvNet | 3.84 | CM |'
- en: '| KAOT [[208](#bib.bib208), [209](#bib.bib209)] | VGG-M | Conv3 | Still images
    | ImageNet | DAF | Intel I7-8700K 3.7GHz CPU, 32GB RAM, RTX 2080 GPU | Matlab
    | MatConvNet | 14.1 | CM |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| KAOT [[208](#bib.bib208), [209](#bib.bib209)] | VGG-M | Conv3 | 静态图像 | ImageNet
    | DAF | Intel I7-8700K 3.7GHz CPU, 32GB RAM, RTX 2080 GPU | Matlab | MatConvNet
    | 14.1 | CM |'
- en: '| MLCFT [[204](#bib.bib204)] | VGG-M | Conv-1, Conv-3, Conv-5 | Still images
    | ImageNet | DAF | Intel I7 3770K 3.5 CPU, 8GB RAM, GTX 960 GPU | Matlab | MatConvNet
    | 16.1 | CM |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| MLCFT [[204](#bib.bib204)] | VGG-M | Conv-1, Conv-3, Conv-5 | 静态图像 | ImageNet
    | DAF | Intel I7 3770K 3.5 CPU, 8GB RAM, GTX 960 GPU | Matlab | MatConvNet | 16.1
    | CM |'
- en: '| UPDT [[102](#bib.bib102)] | VGG-M/ GoogLeNet/ ResNet-50 | N/A | Still images
    | ImageNet | HOG, CN, DAF | N/A | Matlab | MatConvNet | N/A | CM |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| UPDT [[102](#bib.bib102)] | VGG-M/ GoogLeNet/ ResNet-50 | N/A | 静态图像 | ImageNet
    | HOG, CN, DAF | N/A | Matlab | MatConvNet | N/A | CM |'
- en: '| WAEF [[112](#bib.bib112)] | VGG-M | Conv1, Conv5 | Still images | ImageNet
    | HOG, CN, DAF | Intel Xeon(R) 3.20 GHz CPU, 44GB RAM, GTX 1080Ti | Matlab | MatConvNet
    | 0.62 | CM |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| WAEF [[112](#bib.bib112)] | VGG-M | Conv1, Conv5 | 静态图像 | ImageNet | HOG,
    CN, DAF | Intel Xeon(R) 3.20 GHz CPU, 44GB RAM, GTX 1080Ti | Matlab | MatConvNet
    | 0.62 | CM |'
- en: '| DeepSTRCF [[115](#bib.bib115)] | VGG-M | Conv3 | Still images | ImageNet
    | HOG, CN, DAF | Intel I7-7700 CPU, 32GB RAM, GTX 1070 GPU | Matlab | MatConvNet
    | 24.3 | CM |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| DeepSTRCF [[115](#bib.bib115)] | VGG-M | Conv3 | 静态图像 | ImageNet | HOG, CN,
    DAF | Intel I7-7700 CPU, 32GB RAM, GTX 1070 GPU | Matlab | MatConvNet | 24.3 |
    CM |'
- en: '| DRT [[119](#bib.bib119)] | VGG-M, VGG-16 | Conv1, Conv4-3 | Still images
    | ImageNet | HOG, CN, DAF | N/A, GPU | Matlab | Caffe | N/A | CM |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| DRT [[119](#bib.bib119)] | VGG-M, VGG-16 | Conv1, Conv4-3 | 静态图像 | ImageNet
    | HOG, CN, DAF | N/A, GPU | Matlab | Caffe | N/A | CM |'
- en: '| WECO [[76](#bib.bib76)] | VGG-M | Conv1, Conv5 | Still images | ImageNet
    | DAF | Intel Xeon(R) 2.60GHz CPU, GTX 1080 GPU | Matlab | MatConvNet | 4 | CM
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| WECO [[76](#bib.bib76)] | VGG-M | Conv1, Conv5 | 静态图像 | ImageNet | DAF |
    Intel Xeon(R) 2.60GHz CPU, GTX 1080 GPU | Matlab | MatConvNet | 4 | CM |'
- en: '| VDSR-SRT [[124](#bib.bib124)] | VGG-M | Conv1, Conv5 | Still images | ImageNet
    | HOG, DAF | Intel I7-6700k 4.00GHz CPU, 16GB RAM, GTX 1070 GPU | Matlab | MatConvNet
    | 13.5 | CM |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| VDSR-SRT [[124](#bib.bib124)] | VGG-M | Conv1, Conv5 | 静态图像 | ImageNet |
    HOG, DAF | Intel I7-6700k 4.00GHz CPU, 16GB RAM, GTX 1070 GPU | Matlab | MatConvNet
    | 13.5 | CM |'
- en: '| ASRCF [[148](#bib.bib148)] | VGG-M, VGG-16 | Norm1, Conv4-3 | Still images
    | ImageNet | HOG, DAF | Intel I7-8700 CPU, 32GB RAM, GTX 1080Ti GPU | Matlab |
    MatConvNet | 28 | CM |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ASRCF [[148](#bib.bib148)] | VGG-M, VGG-16 | Norm1, Conv4-3 | 静态图像 | ImageNet
    | HOG, DAF | Intel I7-8700 CPU, 32GB RAM, GTX 1080Ti GPU | Matlab | MatConvNet
    | 28 | CM |'
- en: '| RPCF [[152](#bib.bib152)] | VGG-M | Conv1, Conv5 | Still images | ImageNet
    | HOG, CN, DAF | Intel I7-4790K CPU, GTX 1080 GPU | Matlab | MatConvNet | 5 |
    CM |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| RPCF [[152](#bib.bib152)] | VGG-M | Conv1, Conv5 | 静态图像 | ImageNet | HOG,
    CN, DAF | Intel I7-4790K CPU, GTX 1080 GPU | Matlab | MatConvNet | 5 | CM |'
- en: '| DeepTACF [[170](#bib.bib170)] | VGG-M | Conv1 | Still images | ImageNet |
    HOG, DAF | Intel I7-6700 3.40GHz CPU, GTX Titan GPU | Matlab | MatConvNet | N/A
    | CM |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| DeepTACF [[170](#bib.bib170)] | VGG-M | Conv1 | 静态图像 | ImageNet | HOG, DAF
    | Intel I7-6700 3.40GHz CPU, GTX Titan GPU | Matlab | MatConvNet | N/A | CM |'
- en: '| FCNT [[53](#bib.bib53)] | VGG-16 | Conv4-3, Conv5-3 | Still images | ImageNet
    | DAF | 3.4GHz CPU, GTX Titan GPU | Matlab | Caffe | 3 | CM |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| FCNT [[53](#bib.bib53)] | VGG-16 | Conv4-3, Conv5-3 | 静态图像 | ImageNet | DAF
    | 3.4GHz CPU, GTX Titan GPU | Matlab | Caffe | 3 | CM |'
- en: '| CREST [[72](#bib.bib72)] | VGG-16 | Conv4-3 | Still images | ImageNet | DAF
    | Intel I7 3.4GHz CPU, GTX Titan Black GPU | Matlab | MatConvNet | N/A | CM |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| CREST [[72](#bib.bib72)] | VGG-16 | Conv4-3 | 静态图像 | ImageNet | DAF | Intel
    I7 3.4GHz CPU, GTX Titan Black GPU | Matlab | MatConvNet | N/A | CM |'
- en: '| DTO [[79](#bib.bib79)] | VGG-16, SSD | Conv3-3, Conv4-3, Conv5-3 | Still
    images | ImageNet | DAF | Intel I7-4770K CPU, 32G RAM, GTX 1070 GPU | Matlab |
    Caffe | N/A | CM, BB |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| DTO [[79](#bib.bib79)] | VGG-16, SSD | Conv3-3, Conv4-3, Conv5-3 | 静态图像 |
    ImageNet | DAF | Intel I7-4770K CPU, 32G RAM, GTX 1070 GPU | Matlab | Caffe |
    N/A | CM, BB |'
- en: '| VRCPF [[84](#bib.bib84)] | VGG-16, Faster R-CNN | N/A | Still images | ImageNet,
    COCO | DAF | N/A | N/A | N/A | N/A | BB |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| VRCPF [[84](#bib.bib84)] | VGG-16, Faster R-CNN | N/A | 静态图像 | ImageNet,
    COCO | DAF | N/A | N/A | N/A | N/A | BB |'
- en: '| Obli-RaFT [[92](#bib.bib92)] | VGG-16 | Conv4-3, Conv5-3 | Still images |
    ImageNet | DAF | Intel I7-3770 3.40GHz CPU, 2 GTX Titan X GPUs | Matlab | Caffe
    | 2 | VT |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Obli-RaFT [[92](#bib.bib92)] | VGG-16 | Conv4-3, Conv5-3 | 静态图像 | ImageNet
    | DAF | Intel I7-3770 3.40GHz CPU, 2 GTX Titan X GPUs | Matlab | Caffe | 2 | VT
    |'
- en: '| CPT [[108](#bib.bib108)] | VGG-16 | Conv5-1, Conv5-3 | Still images | ImageNet
    | HOG, CN, DAF | Intel I7-7800X CPU, 16GB RAM, GTX 1080Ti GPU | Matlab | MatConvNet
    | 14 | CM |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| CPT [[108](#bib.bib108)] | VGG-16 | Conv5-1, Conv5-3 | 静态图像 | ImageNet |
    HOG, CN, DAF | Intel I7-7800X CPU, 16GB RAM, GTX 1080Ti GPU | Matlab | MatConvNet
    | 14 | CM |'
- en: '| DeepHPFT [[136](#bib.bib136)] | VGG-16, VGG-19, and GoogLeNet | Conv5-3,
    Conv5-4, and icp6-out | Still images | ImageNet | HOG, CN, DAF | Intel Xeon 2.4GHz
    CPU, 256 GB RAM, GTX Titan XP GPU | Matlab | MatConvNet | 4 | CM |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| DeepHPFT [[136](#bib.bib136)] | VGG-16, VGG-19, and GoogLeNet | Conv5-3,
    Conv5-4, and icp6-out | 静态图像 | ImageNet | HOG, CN, DAF | Intel Xeon 2.4GHz CPU,
    256 GB RAM, GTX Titan XP GPU | Matlab | MatConvNet | 4 | CM |'
- en: '| DeepFWDCF [[144](#bib.bib144)] | VGG-16 | Conv4-3 | Still images | ImageNet
    | DAF | N/A, GTX 1080Ti GPU | Matlab | MatConvNet | 2.7 | CM |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| DeepFWDCF [[144](#bib.bib144)] | VGG-16 | Conv4-3 | 静态图像 | ImageNet | DAF
    | N/A, GTX 1080Ti GPU | Matlab | MatConvNet | 2.7 | CM |'
- en: '| MMLT [[107](#bib.bib107)] | VGGNet, Fully-convolutional Siamese network |
    Conv5 | Still images, Video frames | ImageNet, ILSVRC-VID | DAF | Intel I7-4770
    3.40GHz CPU, 11GB RAM, GTX 1080Ti | Matlab | N/A | 6.15 | CM |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| MMLT [[107](#bib.bib107)] | VGGNet, Fully-convolutional Siamese network |
    Conv5 | 静态图像, 视频帧 | ImageNet, ILSVRC-VID | DAF | Intel I7-4770 3.40GHz CPU, 11GB
    RAM, GTX 1080Ti | Matlab | N/A | 6.15 | CM |'
- en: '| MKCT [[210](#bib.bib210)] | VGGNet | Conv3-4 | Still images | ImageNet |
    DAF | Intel I7 3.7GHz CPU, 32GB RAM, Quadro 2000 GPU | Matlab | MatConvNet | 9.4
    | CM |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| MKCT [[210](#bib.bib210)] | VGGNet | Conv3-4 | 静态图像 | ImageNet | DAF | Intel
    I7 3.7GHz CPU, 32GB RAM, Quadro 2000 GPU | Matlab | MatConvNet | 9.4 | CM |'
- en: '| BEVT [[206](#bib.bib206)] | VGGNet | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel I7-8700K 3.7GHz CPU, 48GB RAM, Quadro P2000 GPU | Matlab
    | MatConvNet | 0.6 | CM |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| BEVT [[206](#bib.bib206)] | VGGNet | Conv3-4, Conv4-4, Conv5-4 | 静态图像 | ImageNet
    | DAF | Intel I7-8700K 3.7GHz CPU, 48GB RAM, Quadro P2000 GPU | Matlab | MatConvNet
    | 0.6 | CM |'
- en: '| HCFT [[51](#bib.bib51)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel I7-4770 3.40GHz CPU, 32 GB RAM, GTX Titan GPU | Matlab
    | MatConvNet | 10.4 | CM |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| HCFT [[51](#bib.bib51)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | 静态图像 | ImageNet
    | DAF | Intel I7-4770 3.40GHz CPU, 32 GB RAM, GTX Titan GPU | Matlab | MatConvNet
    | 10.4 | CM |'
- en: '| HDT [[61](#bib.bib61)] | VGG-19 | Conv4-2, Conv4-3, Conv4-4, Conv5-2, Conv5-3,
    Conv5-4 | Still images | ImageNet | DAF | Intel I7-4790K 4.00GHz CPU, 16GB RAM,
    GTX 780Ti GPU | Matlab | MatConvNet |  1 | CM |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| HDT [[61](#bib.bib61)] | VGG-19 | Conv4-2, Conv4-3, Conv4-4, Conv5-2, Conv5-3,
    Conv5-4 | 静态图像 | ImageNet | DAF | Intel I7-4790K 4.00GHz CPU, 16GB RAM, GTX 780Ti
    GPU | Matlab | MatConvNet | 1 | CM |'
- en: '| IBCCF [[78](#bib.bib78)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel Xeon(R) 3.3GHz CPU, 32GB RAM, GTX 1080 GPU | Matlab |
    MatConvNet | 1.25 | CM |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| IBCCF [[78](#bib.bib78)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | 静态图像 | ImageNet
    | DAF | Intel Xeon(R) 3.3GHz CPU, 32GB RAM, GTX 1080 GPU | Matlab | MatConvNet
    | 1.25 | CM |'
- en: '| DCPF [[85](#bib.bib85)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | N/A | N/A | N/A | N/A | CM |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| DCPF [[85](#bib.bib85)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | 静态图像 | ImageNet
    | DAF | N/A | N/A | N/A | N/A | CM |'
- en: '| MCPF [[89](#bib.bib89)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel 3.10GHz CPU, 256 GB RAM, GTX Titan X GPU | Matlab | MatConvNet
    | 0.5 | CM |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| MCPF [[89](#bib.bib89)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | 静态图像 | ImageNet
    | DAF | Intel 3.10GHz CPU, 256 GB RAM, GTX Titan X GPU | Matlab | MatConvNet |
    0.5 | CM |'
- en: '| DeepLMCF [[91](#bib.bib91)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still
    images | ImageNet | DAF | Intel 3.60GHz CPU, Tesla K40 GPU | Matlab | MatConvNet
    | 10 | CM |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| DeepLMCF [[91](#bib.bib91)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | 静态图像 |
    ImageNet | DAF | Intel 3.60GHz CPU, Tesla K40 GPU | Matlab | MatConvNet | 10 |
    CM |'
- en: '| STSGS [[99](#bib.bib99)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF, DMF | Intel I7 3.20GHz CPU, 8 GB RAM | Matlab | Caffe | 4 5
    | CM |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| STSGS [[99](#bib.bib99)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | 静态图像 | ImageNet
    | DAF, DMF | Intel I7 3.20GHz CPU, 8 GB RAM | Matlab | Caffe | 45 | CM |'
- en: '| MCCT [[122](#bib.bib122)] | VGG-19 | Conv4-4, Conv5-4 | Still images | ImageNet
    | DAF | Intel I7-4790K 4.00GHz CPU, 16GB RAM, GTX 1080Ti GPU | Matlab | MatConvNet
    | 8 | CM |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| MCCT [[122](#bib.bib122)] | VGG-19 | Conv4-4, Conv5-4 | 静态图像 | ImageNet |
    DAF | Intel I7-4790K 4.00GHz CPU, 16GB RAM, GTX 1080Ti GPU | Matlab | MatConvNet
    | 8 | CM |'
- en: '| DCPF2 [[123](#bib.bib123)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | N/A | N/A | N/A | N/A | CM |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| DCPF2 [[123](#bib.bib123)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | 静态图像 |
    ImageNet | DAF | 不适用 | 不适用 | 不适用 | 不适用 | CM |'
- en: '| HCFTs [[133](#bib.bib133)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel I7-4770 3.40GHz CPU, 32GB RAM, GTX Titan GPU | Matlab
    | MatConvNet | 6.7 | CM |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| HCFTs [[133](#bib.bib133)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | 静态图像 |
    ImageNet | DAF | Intel I7-4770 3.40GHz CPU, 32GB RAM, GTX Titan GPU | Matlab |
    MatConvNet | 6.7 | CM |'
- en: '| LCTdeep [[142](#bib.bib142)] | VGG-19 | Conv5-4 | Still images | ImageNet
    | DAF | Intel I7-4770 3.40GHz CPU, 32GB RAM, GPU | Matlab | MatConvNet | 13.8
    | CM |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| LCTdeep [[142](#bib.bib142)] | VGG-19 | Conv5-4 | 静态图像 | ImageNet | DAF |
    Intel I7-4770 3.40GHz CPU, 32GB RAM, GPU | Matlab | MatConvNet | 13.8 | CM |'
- en: '| CF-CNN [[67](#bib.bib67)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | DAF | Intel I7-4770 3.40GHz CPU, 32GB RAM, GTX Titan GPU | Matlab
    | MatConvNet | 12.3 | CM |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| CF-CNN [[67](#bib.bib67)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | 静态图像 | ImageNet
    | DAF | Intel I7-4770 3.40GHz CPU, 32GB RAM, GTX Titan GPU | Matlab | MatConvNet
    | 12.3 | CM |'
- en: '| ORHF [[147](#bib.bib147)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still images
    | ImageNet | HOG, DAF | Intel I7-4770K 3.50GHz CPU, 24GB RAM, N/A | Matlab | N/A
    | N/A | CM |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| ORHF [[147](#bib.bib147)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | 静态图像 | ImageNet
    | HOG, DAF | Intel I7-4770K 3.50GHz CPU, 24GB RAM, 不适用 | Matlab | 不适用 | 不适用 |
    CM |'
- en: '| IMM-DFT [[168](#bib.bib168)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | Still
    images | ImageNet | DAF | Intel I5-4590 3.30GHz CPU, 16GB RAM, GTX Titan X GPU
    | Matlab | MatConvNet | 10 | CM |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| IMM-DFT [[168](#bib.bib168)] | VGG-19 | Conv3-4, Conv4-4, Conv5-4 | 静态图像
    | ImageNet | DAF | Intel I5-4590 3.30GHz CPU, 16GB RAM, GTX Titan X GPU | Matlab
    | MatConvNet | 10 | CM |'
- en: '| CNN-SVM [[54](#bib.bib54)] | R-CNN | First fully-connected layer | Still
    images | ImageNet | DAF | N/A | N/A | Caffe | N/A | SM |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| CNN-SVM [[54](#bib.bib54)] | R-CNN | 第一个全连接层 | 静态图像 | ImageNet | DAF | 不适用
    | 不适用 | Caffe | 不适用 | SM |'
- en: '| RPNT [[63](#bib.bib63)] | Object proposal network | N/A | Still images |
    ImageNet, PASCAL VOC | DAF | N/A | C/C++ | N/A | 3.8 | BB |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| RPNT [[63](#bib.bib63)] | 目标提议网络 | 不适用 | 静态图像 | ImageNet, PASCAL VOC | DAF
    | 不适用 | C/C++ | 不适用 | 3.8 | BB |'
- en: '| CF-FCSiam [[145](#bib.bib145)] | Fully-convolutional Siamese network | N/A
    | Video frames | ILSVRC-VID | HOG, DAF | Intel I7-6700K 4.00GHz CPU, GTX Titan
    GPU | Matlab | MatConvNet |  33 | CM |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| CF-FCSiam [[145](#bib.bib145)] | 完全卷积的Siamese网络 | 不适用 | 视频帧 | ILSVRC-VID
    | HOG, DAF | Intel I7-6700K 4.00GHz CPU, GTX Titan GPU | Matlab | MatConvNet |
    33 | CM |'
- en: '| TADT [[157](#bib.bib157)] | Siamese matching network | Conv4-1, Conv4-3 |
    Still images | ImageNet | DAF | Intel I7 3.60GHz CPU, 32GB RAM, GTX 1080 GPU |
    Matlab | MatConvNet | 33.7 | CM |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| TADT [[157](#bib.bib157)] | Siamese匹配网络 | Conv4-1, Conv4-3 | 静态图像 | ImageNet
    | DAF | Intel I7 3.60GHz CPU, 32GB RAM, GTX 1080 GPU | Matlab | MatConvNet | 33.7
    | CM |'
- en: '| GFS-DCF [[186](#bib.bib186)] | ResNet-50 | Res4x | Still images | ImageNet
    | DAF | Intel Xeon E5-2637v3 CPU, N/A, GTX Titan X GPU | Matlab | MatConvNet |
    8 | CM |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| GFS-DCF [[186](#bib.bib186)] | ResNet-50 | Res4x | 静态图像 | ImageNet | DAF
    | Intel Xeon E5-2637v3 CPU, 不适用, GTX Titan X GPU | Matlab | MatConvNet | 8 | CM
    |'
- en: '| DHT [[203](#bib.bib203)] | Various networks | Various layers | Still images,
    Video frames | Various datasets | DAF | N/A | N/A | N/A | 12 | BB |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| DHT [[203](#bib.bib203)] | 各种网络 | 各种层 | 静态图像，视频帧 | 各种数据集 | DAF | 不适用 | 不适用
    | 不适用 | 12 | BB |'
- en: II-B2 Deep Features for Visual Tracking Purposes
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 深度特征用于视觉跟踪目的
- en: 'One trending part of recent trackers is how to design and train DNNs for visual
    tracking. Using deep off-the-shelf features limits the tracking performance due
    to inconsistency among the objectives of different tasks. Also, offline learned
    deep features may not capture target variations and tend to over-fit on initial
    target templates. Hence, DNNs are trained on large-scale datasets to specialize
    the networks for visual tracking purposes. Besides, applying a fine-tuning process
    during visual tracking can adjust some network parameters and produce more refined
    target representations. However, the fine-tuning process is time-consuming and
    prone to over-fitting because of a heuristically fixed iteration number and limited
    available training data. As shown in Table [II](#S2.T2 "TABLE II ‣ II-C2 Only
    Offline Training ‣ II-C Network Training ‣ II Deep Visual Tracking Taxonomy ‣
    Deep Learning for Visual Tracking: A Comprehensive Survey") to Table [IV](#S2.T4
    "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey"),
    these DL-based methods usually train a pre-trained network (i.e., backbone network)
    by offline training, online training, or both.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '近期跟踪器的一个热门话题是如何设计和训练用于视觉跟踪的深度神经网络（DNN）。使用深度现成特征会由于不同任务的目标不一致而限制跟踪性能。此外，离线学习的深度特征可能无法捕捉目标变化，并倾向于对初始目标模板过拟合。因此，DNN需要在大规模数据集上进行训练，以使网络专门化于视觉跟踪目的。此外，在视觉跟踪过程中应用微调过程可以调整一些网络参数，产生更精细的目标表示。然而，微调过程耗时且容易过拟合，因为迭代次数是启发式固定的，且可用的训练数据有限。如表[II](#S2.T2
    "TABLE II ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")至表[IV](#S2.T4
    "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")所示，这些基于深度学习的方法通常通过离线训练、在线训练或两者结合来训练预训练的网络（即骨干网络）。'
- en: II-C Network Training
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 网络训练
- en: 'The state-of-the-art DL-based visual tracking methods mostly exploit end-to-end
    learning with train/re-train a DNN by applying gradient-based optimization algorithms.
    However, these methods have differences according to their offline network training,
    online fine-tuning, computational complexity, dealing with lack of training data,
    addressing the overfitting problem, and exploiting unlabeled samples by unsupervised
    training. The network training sections in the previous review papers [[47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49)] consider both FENs and EENs, although the
    FENs were only pre-trained for other tasks, and there is no training procedure
    for visual tracking. In this survey, DL-based methods are categorized into only
    offline pre-training, only online training, and both offline and online training
    for visual tracking purposes. The training details of these methods are shown
    in Table [II](#S2.T2 "TABLE II ‣ II-C2 Only Offline Training ‣ II-C Network Training
    ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey") to Table [IV](#S2.T4 "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network
    Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey").'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '目前最先进的基于深度学习的视觉跟踪方法大多利用端到端学习，通过应用基于梯度的优化算法来训练/重新训练深度神经网络（DNN）。然而，这些方法在离线网络训练、在线微调、计算复杂性、处理训练数据不足、解决过拟合问题以及通过无监督训练利用无标记样本方面存在差异。之前的综述论文中的网络训练部分[[47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49)]考虑了FENs和EENs，尽管FENs仅用于其他任务的预训练，并没有针对视觉跟踪的训练程序。在这项调查中，基于深度学习的方法被分为仅离线预训练、仅在线训练以及离线和在线训练两种类别，目的在于视觉跟踪。这些方法的训练细节见表[II](#S2.T2
    "TABLE II ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")至表[IV](#S2.T4
    "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")。'
- en: II-C1 Training Datasets
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 训练数据集
- en: 'Visual trackers employ diverse datasets to train their networks. These datasets
    are generally categorized into general-purpose & tracking datasets (see Table [I](#S2.T1
    "TABLE I ‣ II-B1 Model Reuse or Deep Off-the-Shelf Features ‣ II-B Network Exploitation
    ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey") to Table [IV](#S2.T4 "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network
    Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")). A general-purpose dataset refers to a dataset from
    other tasks that provide desirable representations of different targets, e.g.,
    object recognition or segmentation. It can include numerous datasets such as ImageNet
    [[33](#bib.bib33)], YouTube-VOS [[243](#bib.bib243)], YouTube-BoundingBoxes [[244](#bib.bib244)],
    KITTI [[245](#bib.bib245)], etc. That is, these datasets are used as auxiliary
    datasets in training procedures. However, tracking datasets are also utilized
    for training visual tracking networks. For instance, large-scale tracking datasets
    such as LaSOT [[221](#bib.bib221)] & TrackingNet [[229](#bib.bib229)] are explored
    in recent years. By exploring tracking datasets, the networks are trained on task-specific
    scenarios in the presence of challenging tracking attributes.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '视觉跟踪器使用各种数据集来训练其网络。这些数据集通常分为通用数据集和跟踪数据集（见表[I](#S2.T1 "TABLE I ‣ II-B1 Model
    Reuse or Deep Off-the-Shelf Features ‣ II-B Network Exploitation ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")至表[IV](#S2.T4
    "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")）。通用数据集指来自其他任务的数据集，这些数据集提供不同目标的理想表示，例如物体识别或分割。它可以包括诸如ImageNet
    [[33](#bib.bib33)]、YouTube-VOS [[243](#bib.bib243)]、YouTube-BoundingBoxes [[244](#bib.bib244)]、KITTI
    [[245](#bib.bib245)]等众多数据集。也就是说，这些数据集在训练过程中作为辅助数据集使用。然而，跟踪数据集也被用于训练视觉跟踪网络。例如，近年来探索了大规模跟踪数据集如LaSOT
    [[221](#bib.bib221)]和TrackingNet [[229](#bib.bib229)]。通过探索跟踪数据集，网络在具有挑战性的跟踪属性的任务特定场景中进行训练。'
- en: II-C2 Only Offline Training
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 仅离线训练
- en: 'Most of the DL-based visual tracking methods only pre-train their networks
    to provide a generic target representation and reduce the high risk of over-fitting
    due to imbalanced training data and fixed assumptions. To adjust the learned filter
    weights for visual tracking task, the specialized networks are trained on large-scale
    data to exploit better representation and achieve acceptable tracking speed by
    preventing from training during visual tracking (see Table [II](#S2.T2 "TABLE
    II ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual Tracking
    Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数基于深度学习的视觉跟踪方法仅预训练其网络，以提供通用目标表示，并减少由于不平衡的训练数据和固定假设所带来的过拟合风险。为了调整用于视觉跟踪任务的学习过滤器权重，专用网络在大规模数据上进行训练，以利用更好的表示并通过防止在视觉跟踪过程中进行训练来实现可接受的跟踪速度（见表[II](#S2.T2
    "TABLE II ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")）。'
- en: 'TABLE II: Only offline training for visual tracking. The abbreviations are
    denoted as: confidence map (CM), saliency map (SM), bounding box (BB), object
    score (OS), feature maps (FM), segmentation mask (SGM), rotated bounding box (RBB),
    action (AC), deep appearance features (DAF), deep motion features (DMF), deeo
    optical flow (DOF).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：仅用于视觉跟踪的离线训练。缩写表示为：置信度图（CM），显著性图（SM），边界框（BB），目标分数（OS），特征图（FM），分割掩膜（SGM），旋转边界框（RBB），动作（AC），深度外观特征（DAF），深度运动特征（DMF），深度光流（DOF）。
- en: '| Method | Backbone network | Offline training dataset(s) | Exploited features
    | PC (CPU, RAM, Nvidia GPU) | Language | Framework | Speed (fps) | Tracking output
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 骨干网络 | 离线训练数据集 | 利用的特征 | PC（CPU, RAM, Nvidia GPU） | 语言 | 框架 | 速度（fps）
    | 跟踪输出 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GOTURN [[57](#bib.bib57)] | AlexNet | ILSVRC-DET, ALOV | DAF | N/A, GTX Titan
    X GPU | C/C++ | Caffe | 166 | BB |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| GOTURN [[57](#bib.bib57)] | AlexNet | ILSVRC-DET, ALOV | DAF | N/A, GTX Titan
    X GPU | C/C++ | Caffe | 166 | BB |'
- en: '| SiamFC [[58](#bib.bib58)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    I7-4790K 4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 58 | CM |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| SiamFC [[58](#bib.bib58)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    I7-4790K 4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 58 | CM |'
- en: '| SINT [[59](#bib.bib59)] | AlexNet, VGG-16 | ImageNet, ALOV | DAF | N/A |
    Matlab | Caffe | N/A | OS |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| SINT [[59](#bib.bib59)] | AlexNet, VGG-16 | ImageNet, ALOV | DAF | N/A |
    Matlab | Caffe | N/A | OS |'
- en: '| R-FCSN [[81](#bib.bib81)] | AlexNet | ImageNet, ILSVRC-VID | DAF | N/A, GTX
    Titan X GPU | Matlab | MatConvNet | 50.25 | CM |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| R-FCSN [[81](#bib.bib81)] | AlexNet | ImageNet, ILSVRC-VID | DAF | 无, GTX
    Titan X GPU | Matlab | MatConvNet | 50.25 | CM |'
- en: '| LST [[83](#bib.bib83)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel Xeon
    3.50GHz CPU, GTX Titan X GPU | Matlab | MatConvNet |  24 | CM |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| LST [[83](#bib.bib83)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel Xeon
    3.50GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 24 | CM |'
- en: '| CFNet [[86](#bib.bib86)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel I7
    4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 75 | CM |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| CFNet [[86](#bib.bib86)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel I7
    4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 75 | CM |'
- en: '| DaSiamRPN [[104](#bib.bib104)] | AlexNet | ILSVRC, YTBB, Augmented ILSVRC-DET,
    Augmented MSCOCO-DET | DAF | Intel I7 CPU, 48GB RAM, GTX Titan X GPU | Python
    | PyTorch | 160 | CM |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| DaSiamRPN [[104](#bib.bib104)] | AlexNet | ILSVRC, YTBB, 增强版 ILSVRC-DET,
    增强版 MSCOCO-DET | DAF | Intel I7 CPU, 48GB RAM, GTX Titan X GPU | Python | PyTorch
    | 160 | CM |'
- en: '| StructSiam [[106](#bib.bib106)] | AlexNet | ILSVRC-VID, ALOV | DAF | Intel
    I7-4790 3.60GHz, GTX 1080 GPU | Python | TensorFlow | 45 | CM |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| StructSiam [[106](#bib.bib106)] | AlexNet | ILSVRC-VID, ALOV | DAF | Intel
    I7-4790 3.60GHz, GTX 1080 GPU | Python | TensorFlow | 45 | CM |'
- en: '| Siam-BM [[111](#bib.bib111)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    Xeon 2.60GHz CPU, Tesla P100 GPU | Python | TensorFlow | 48 | CM |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Siam-BM [[111](#bib.bib111)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    Xeon 2.60GHz CPU, Tesla P100 GPU | Python | TensorFlow | 48 | CM |'
- en: '| SA-Siam [[117](#bib.bib117)] | AlexNet | ImageNet, TC128, ILSVRC-VID | DAF
    | Intel Xeon 2.40GHz CPU, GTX Titan X GPU | Python | TensorFlow | 50 | CM |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| SA-Siam [[117](#bib.bib117)] | AlexNet | ImageNet, TC128, ILSVRC-VID | DAF
    | Intel Xeon 2.40GHz CPU, GTX Titan X GPU | Python | TensorFlow | 50 | CM |'
- en: '| SiamRPN [[116](#bib.bib116)] | AlexNet | ILSVRC, YTBB | DAF | Intel I7 CPU,
    12GB RAM, GTX 1060 GPU | Python | PyTorch | 160 | FM |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| SiamRPN [[116](#bib.bib116)] | AlexNet | ILSVRC, YTBB | DAF | Intel I7 CPU,
    12GB RAM, GTX 1060 GPU | Python | PyTorch | 160 | FM |'
- en: '| C-RPN [[150](#bib.bib150)] | AlexNet | ImageNet, ILSVRC-VID, YTBB | DAF |
    N/A, GTX 1080 GPU | Matlab | MatConvNet |  36 | CM |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| C-RPN [[150](#bib.bib150)] | AlexNet | ImageNet, ILSVRC-VID, YTBB | DAF |
    无, GTX 1080 GPU | Matlab | MatConvNet | 36 | CM |'
- en: '| GCT [[151](#bib.bib151)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel Xeon
    3.00GHz CPU, 256GB RAM, GTX 1080Ti GPU | Python | TensorFlow | 49.8 | CM |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| GCT [[151](#bib.bib151)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel Xeon
    3.00GHz CPU, 256GB RAM, GTX 1080Ti GPU | Python | TensorFlow | 49.8 | CM |'
- en: '| GradNet [[187](#bib.bib187)] | AlexNet | ILSVRC-2014 | DAF | Intel I7 3.2GHz
    CPU, 32GB RAM, GTX 1080Ti GPU | Python | TensorFlow | 80 | CM |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| GradNet [[187](#bib.bib187)] | AlexNet | ILSVRC-2014 | DAF | Intel I7 3.2GHz
    CPU, 32GB RAM, GTX 1080Ti GPU | Python | TensorFlow | 80 | CM |'
- en: '| i-Siam [[215](#bib.bib215)] | AlexNet | GOT-10k | DAF | Intel I7-7700K 4.20GHz
    CPU, Titan Xp GPU | N/A | N/A | 43 | CM |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| i-Siam [[215](#bib.bib215)] | AlexNet | GOT-10k | DAF | Intel I7-7700K 4.20GHz
    CPU, Titan Xp GPU | 无 | 无 | 43 | CM |'
- en: '| UpdateNet [[189](#bib.bib189)] | AlexNet | LaSOT | DAF | N/A | Python | PyTorch
    | N/A | CM |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| UpdateNet [[189](#bib.bib189)] | AlexNet | LaSOT | DAF | 无 | Python | PyTorch
    | 无 | CM |'
- en: '| SPM [[153](#bib.bib153)] | AlexNet, SiameseRPN, RelationNet | ImageNet, ILSVRC-VID,
    YTBB, ILSVRC-DET, MSCOCO, CityPerson, WiderFace | DAF | N/A, P100 GPU | N/A |
    N/A | 120 | OS |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| SPM [[153](#bib.bib153)] | AlexNet, SiameseRPN, RelationNet | ImageNet, ILSVRC-VID,
    YTBB, ILSVRC-DET, MSCOCO, CityPerson, WiderFace | DAF | 无, P100 GPU | 无 | 无 |
    120 | OS |'
- en: '| FICFNet [[141](#bib.bib141)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    I7 4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 28 | CM |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| FICFNet [[141](#bib.bib141)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    I7 4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 28 | CM |'
- en: '| MTHCF [[166](#bib.bib166)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    6700 3.40GHz CPU, GTX Titan GPU | Matlab | MatConvNet | 33 | CM |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| MTHCF [[166](#bib.bib166)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    6700 3.40GHz CPU, GTX Titan GPU | Matlab | MatConvNet | 33 | CM |'
- en: '| HP [[178](#bib.bib178)] | AlexNet | ImageNet, ILSVRC-VID | DAF | N/A | Python
    | Keras | 69 | CM |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| HP [[178](#bib.bib178)] | AlexNet | ImageNet, ILSVRC-VID | DAF | 无 | Python
    | Keras | 69 | CM |'
- en: '| EAST [[177](#bib.bib177)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    I7 4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 23.2 | AC |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| EAST [[177](#bib.bib177)] | AlexNet | ImageNet, ILSVRC-VID | DAF | Intel
    I7 4.00GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 23.2 | AC |'
- en: '| CFCF [[137](#bib.bib137)] | VGG-M | ImageNet, ILSVRC-VID, VOT2015 | HOG,
    DAF | Intel Xeon 3.00GHz CPU, Tesla K40 GPU | Matlab | MatConvNet |  1.7 | CM
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| CFCF [[137](#bib.bib137)] | VGG-M | ImageNet, ILSVRC-VID, VOT2015 | HOG,
    DAF | Intel Xeon 3.00GHz CPU, Tesla K40 GPU | Matlab | MatConvNet | 1.7 | CM |'
- en: '| CFSRL [[138](#bib.bib138)] | VGG-M | ILSVRC-VID | DAF | Intel Xeon 2.40GHz
    CPU, 32GB RAM, GTX Titan X GPU | Matlab, Python | PyTorch | N/A | CM |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| CFSRL [[138](#bib.bib138)] | VGG-M | ILSVRC-VID | DAF | Intel Xeon 2.40GHz
    CPU, 32GB RAM, GTX Titan X GPU | Matlab, Python | PyTorch | 无 | CM |'
- en: '| C2FT [[174](#bib.bib174)] | VGG-M | ImageNet, N/A | DAF | Intel Xeon 2.60GHz
    CPU, GTX 1080Ti GPU | N/A | N/A | N/A | AC |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| C2FT [[174](#bib.bib174)] | VGG-M | ImageNet, 无 | DAF | Intel Xeon 2.60GHz
    CPU, GTX 1080Ti GPU | 无 | 无 | 无 | AC |'
- en: '| DSNet [[205](#bib.bib205)] | VGG-M | ILSVRC-2015 | DAF | Intel 6700K 4.0GHz
    CPU, GTX 1080 GPU | N/A | N/A | 68.5 | CM |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| DSNet [[205](#bib.bib205)] | VGG-M | ILSVRC-2015 | DAF | Intel 6700K 4.0GHz
    CPU, GTX 1080 GPU | N/A | N/A | 68.5 | CM |'
- en: '| SRT [[80](#bib.bib80)] | VGG-16 | ImageNet, ALOV, Deform-SOT | DAF | N/A
    | N/A | N/A | N/A | BB |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| SRT [[80](#bib.bib80)] | VGG-16 | ImageNet, ALOV, Deform-SOT | DAF | N/A
    | N/A | N/A | N/A | BB |'
- en: '| IMLCF [[128](#bib.bib128)] | VGG-16 | ImageNet, ILSVRC-VID | DAF | Intel
    1.40GHz CPU, GTX 1080Ti GPU | Matlab | MatConvNet | N/A | CM |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| IMLCF [[128](#bib.bib128)] | VGG-16 | ImageNet, ILSVRC-VID | DAF | Intel
    1.40GHz CPU, GTX 1080Ti GPU | Matlab | MatConvNet | N/A | CM |'
- en: '| SINT++ [[181](#bib.bib181)] | VGG-16 | ImageNet, OTB2013, OTB2015, VOT2014
    | DAF | Intel I7-6700K CPU, 32GB RAM, GTX 1080 GPU | Python | Caffe, Keras | N/A
    | AC |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| SINT++ [[181](#bib.bib181)] | VGG-16 | ImageNet, OTB2013, OTB2015, VOT2014
    | DAF | Intel I7-6700K CPU, 32GB RAM, GTX 1080 GPU | Python | Caffe, Keras | N/A
    | AC |'
- en: '| MAM [[171](#bib.bib171)] | VGG-16, Faster-RCNN | ImageNet, PASCAL VOC 2007,
    OTB100, TC128 | DAF | 3.40GHz CPU, Titan GPU | Matlab | Caffe | 3 | SM |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| MAM [[171](#bib.bib171)] | VGG-16, Faster-RCNN | ImageNet, PASCAL VOC 2007,
    OTB100, TC128 | DAF | 3.40GHz CPU, Titan GPU | Matlab | Caffe | 3 | SM |'
- en: '| PTAV [[70](#bib.bib70), [71](#bib.bib71)] | VGGNet | ALOV | HOG, DAF | N/A,
    GTX Titan Z GPU | C/C++ | Caffe | 27 | CM |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| PTAV [[70](#bib.bib70), [71](#bib.bib71)] | VGGNet | ALOV | HOG, DAF | N/A,
    GTX Titan Z GPU | C/C++ | Caffe | 27 | CM |'
- en: '| UDT [[158](#bib.bib158)] | VGGNet | ILSVRC | DAF | Intel I7-4790K 4.00GHz,
    GTX 1080Ti GPU | Matlab | MatConvNet | 55 | CM |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| UDT [[158](#bib.bib158)] | VGGNet | ILSVRC | DAF | Intel I7-4790K 4.00GHz,
    GTX 1080Ti GPU | Matlab | MatConvNet | 55 | CM |'
- en: '| DRRL [[162](#bib.bib162)] | VGGNet | ImageNet, VOT2016 | DAF | N/A, GTX 1060
    GPU | Python | TensorFlow | 6.3 | OS |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| DRRL [[162](#bib.bib162)] | VGGNet | ImageNet, VOT2016 | DAF | N/A, GTX 1060
    GPU | Python | TensorFlow | 6.3 | OS |'
- en: '| FCSFN [[125](#bib.bib125)] | VGG-19 | ImageNet, ALOV | DAF | N/A | N/A |
    N/A | N/A | CM |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| FCSFN [[125](#bib.bib125)] | VGG-19 | ImageNet, ALOV | DAF | N/A | N/A |
    N/A | N/A | CM |'
- en: '| Siam-MCF [[110](#bib.bib110)] | ResNet-50 | ImageNet, ILSVRC-VID | DAF |
    Intel Xeon E5 CPU, GTX 1080Ti GPU | Python | TensorFlow | 20 | CM |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Siam-MCF [[110](#bib.bib110)] | ResNet-50 | ImageNet, ILSVRC-VID | DAF |
    Intel Xeon E5 CPU, GTX 1080Ti GPU | Python | TensorFlow | 20 | CM |'
- en: '| SiamMask [[155](#bib.bib155)] | ResNet-50 | ImageNet, MSCOCO, ILSVRC-VID,
    YouTube-VOS | DAF | N/A, RTX 2080 GPU | Python | PyTorch | 55 60 | SGM, RBB |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| SiamMask [[155](#bib.bib155)] | ResNet-50 | ImageNet, MSCOCO, ILSVRC-VID,
    YouTube-VOS | DAF | N/A, RTX 2080 GPU | Python | PyTorch | 55 60 | SGM, RBB |'
- en: '| SiamRPN++ [[156](#bib.bib156)] | ResNet-50 | ImageNet, MSCOCO, ILSVRC-DET,
    ILSVRC-VID, YTBB | DAF | N/A, Titan Xp Pascal GPU | Python | PyTorch | 35 | OS,
    BB |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| SiamRPN++ [[156](#bib.bib156)] | ResNet-50 | ImageNet, MSCOCO, ILSVRC-DET,
    ILSVRC-VID, YTBB | DAF | N/A, Titan Xp Pascal GPU | Python | PyTorch | 35 | OS,
    BB |'
- en: '| CGACD [[190](#bib.bib190)] | ResNet-50 | ILSVRC-VID, YTBB, GOT-10k, COCO,
    ILSVRC-DET | DAF | 3.5GHz CPU, RTX 2080Ti GPU | Python | PyTorch | 70 | BB |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| CGACD [[190](#bib.bib190)] | ResNet-50 | ILSVRC-VID, YTBB, GOT-10k, COCO,
    ILSVRC-DET | DAF | 3.5GHz CPU, RTX 2080Ti GPU | Python | PyTorch | 70 | BB |'
- en: '| CSA [[191](#bib.bib191)] | ResNet-50 | GOT-10k | DAF | Intel I9 CPU, 64GB
    RAM, RTX 2080Ti GPU | Python | PyTorch | 100 | CM |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| CSA [[191](#bib.bib191)] | ResNet-50 | GOT-10k | DAF | Intel I9 CPU, 64GB
    RAM, RTX 2080Ti GPU | Python | PyTorch | 100 | CM |'
- en: '| SiamAttn [[197](#bib.bib197)] | ResNet-50 | COCO, YouTube-VOS, LaSOT, TrackingNet
    | DAF | N/A, RTX 2080Ti GPU | Python | PyTorch | 33 | BB, SGM |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| SiamAttn [[197](#bib.bib197)] | ResNet-50 | COCO, YouTube-VOS, LaSOT, TrackingNet
    | DAF | N/A, RTX 2080Ti GPU | Python | PyTorch | 33 | BB, SGM |'
- en: '| SiamBAN [[198](#bib.bib198)] | ResNet-50 | ILSVRC-VID, YTBB, COCO, ILSVRC-DET,
    GOT-10k, LaSOT | DAF | Intel Xeon 4108 1.8GHz CPU, 64GB RAM, GTX 1080Ti GPU |
    Python | PyTorch | 40 | BB |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| SiamBAN [[198](#bib.bib198)] | ResNet-50 | ILSVRC-VID, YTBB, COCO, ILSVRC-DET,
    GOT-10k, LaSOT | DAF | Intel Xeon 4108 1.8GHz CPU, 64GB RAM, GTX 1080Ti GPU |
    Python | PyTorch | 40 | BB |'
- en: '| SiamCAR [[199](#bib.bib199)] | ResNet-50 | ILSVRC-DET, COCO, ILSVRC-VID,
    YTBB | DAF | N/A, 4 RTX 2080Ti GPU | Python | PyTorch | 52.2 | BB |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| SiamCAR [[199](#bib.bib199)] | ResNet-50 | ILSVRC-DET, COCO, ILSVRC-VID,
    YTBB | DAF | N/A, 4 RTX 2080Ti GPU | Python | PyTorch | 52.2 | BB |'
- en: '| SiamDW [[154](#bib.bib154)] | ResNet, ResNeXt, Inception | ImageNet, ILSVRC-VID,
    YTBB | DAF | Intel Xeon 2.40GHz CPU, GTX 1080 GPU | Python | PyTorch | 13 93 |
    CM, FM |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| SiamDW [[154](#bib.bib154)] | ResNet, ResNeXt, Inception | ImageNet, ILSVRC-VID,
    YTBB | DAF | Intel Xeon 2.40GHz CPU, GTX 1080 GPU | Python | PyTorch | 13 93 |
    CM, FM |'
- en: '| FlowTrack [[118](#bib.bib118)] | FlowNet | Flying chairs, Middlebur, KITTI,
    Sintel, ILSVRC-VID | DAF, DMF | Intel I7-6700 CPU, 48GB RAM, GTX Titan X GPU |
    Matlab | MatConvNet | 12 | CM |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| FlowTrack [[118](#bib.bib118)] | FlowNet | Flying chairs, Middlebur, KITTI,
    Sintel, ILSVRC-VID | DAF, DMF | Intel I7-6700 CPU, 48GB RAM, GTX Titan X GPU |
    Matlab | MatConvNet | 12 | CM |'
- en: '| RASNet [[121](#bib.bib121)] | Attention networks | ILSVRC-DET | DAF | Intel
    Xeon 2.20GHz CPU, Titan Xp Pascal GPU | Matlab | MatConvNet | 83 | CM |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| RASNet [[121](#bib.bib121)] | Attention networks | ILSVRC-DET | DAF | Intel
    Xeon 2.20GHz CPU, Titan Xp Pascal GPU | Matlab | MatConvNet | 83 | CM |'
- en: '| ACFN [[93](#bib.bib93)] | Attentional correlation filter network | OTB2013,
    OTB2015, VOT2014, VOT2015 | HOG, Color, DAF | Intel I7-6900K 3.20GHz CPU, 32GB
    RAM, GTX 1070 GPU | Matlab, Python | MatConvNet, TensorFlow | 15 | OS |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| ACFN [[93](#bib.bib93)] | 注意力相关滤波网络 | OTB2013, OTB2015, VOT2014, VOT2015
    | HOG, 颜色, DAF | Intel I7-6900K 3.20GHz CPU, 32GB RAM, GTX 1070 GPU | Matlab,
    Python | MatConvNet, TensorFlow | 15 | OS |'
- en: '| RFL [[77](#bib.bib77)] | Convolutional LSTM | ILSVRC-VID | DAF | Intel I7-6700
    3.40GHz CPU, GTX 1080 GPU | Python | TensorFlow |  15 | CM |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| RFL [[77](#bib.bib77)] | 卷积LSTM | ILSVRC-VID | DAF | Intel I7-6700 3.40GHz
    CPU, GTX 1080 GPU | Python | TensorFlow | 15 | CM |'
- en: '| DRLT [[176](#bib.bib176)] | YOLO | ImageNet, PASCAL VOC | DAF | N/A, GTX
    1080 GPU | Python | TensorFlow |  45 | BB |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| DRLT [[176](#bib.bib176)] | YOLO | ImageNet, PASCAL VOC | DAF | N/A, GTX
    1080 GPU | Python | TensorFlow | 45 | BB |'
- en: '| TGGAN [[129](#bib.bib129)] | - | ALOV, VOT2015 | DAF | N/A, GTX Titan X GPU
    | Python | Keras | 3.1 | CM |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| TGGAN [[129](#bib.bib129)] | - | ALOV, VOT2015 | DAF | N/A, GTX Titan X GPU
    | Python | Keras | 3.1 | CM |'
- en: '| DCTN [[131](#bib.bib131)] | - | TC128, NUS-PRO, MOT2015 | DAF, DMF | N/A
    | N/A | N/A | 27 | CM |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| DCTN [[131](#bib.bib131)] | - | TC128, NUS-PRO, MOT2015 | DAF, DMF | N/A
    | N/A | N/A | 27 | CM |'
- en: '| YCNN [[135](#bib.bib135)] | - | ImageNet, ALOV300++ | DAF | N/A, Tesla K40c
    GPU | Python | TensorFlow | 45 | CM |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| YCNN [[135](#bib.bib135)] | - | ImageNet, ALOV300++ | DAF | N/A, Tesla K40c
    GPU | Python | TensorFlow | 45 | CM |'
- en: '| RDT [[180](#bib.bib180)] | - | VOT2015 | DAF | Intel I7-4790K 4.00GHz, 24GB
    RAM, GTX Titan X GPU | Python | TensorFlow | 43 | CM, OS |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| RDT [[180](#bib.bib180)] | - | VOT2015 | DAF | Intel I7-4790K 4.00GHz, 24GB
    RAM, GTX Titan X GPU | Python | TensorFlow | 43 | CM, OS |'
- en: '| SiamRCNN [[200](#bib.bib200)] | Faster R-CNN, ResNet-101-FPN | ILSVRC-VID,
    COCO, YouTube-VOS, GOT-10k, LaSOT | DAF | N/A, Tesla V100 GPU | Python | TensorFlow
    | 4.7 | BB, SGM |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SiamRCNN [[200](#bib.bib200)] | Faster R-CNN, ResNet-101-FPN | ILSVRC-VID,
    COCO, YouTube-VOS, GOT-10k, LaSOT | DAF | N/A, Tesla V100 GPU | Python | TensorFlow
    | 4.7 | BB, SGM |'
- en: '| FGTrack [[202](#bib.bib202)] | ResNet-18, FlowNet2 | ILSVRC-VID, TrackingNet,
    YouTube-VOS | DAF, DOF | N/A, GTX 1080Ti GPU | Python | PyTorch | 19.6 | BB |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| FGTrack [[202](#bib.bib202)] | ResNet-18, FlowNet2 | ILSVRC-VID, TrackingNet,
    YouTube-VOS | DAF, DOF | N/A, GTX 1080Ti GPU | Python | PyTorch | 19.6 | BB |'
- en: '| CRVFL [[183](#bib.bib183)] | - | N/A | DAF | Intel I7 CPU | N/A | N/A | 1.5-2
    | OS |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| CRVFL [[183](#bib.bib183)] | - | N/A | DAF | Intel I7 CPU | N/A | N/A | 1.5-2
    | OS |'
- en: '| VTCNN [[184](#bib.bib184)] | - | N/A | DAF | Intel I7 3.4GHz | Matlab | N/A
    | 5.5 | OS |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| VTCNN [[184](#bib.bib184)] | - | N/A | DAF | Intel I7 3.4GHz | Matlab | N/A
    | 5.5 | OS |'
- en: '| GlobalTrack [[214](#bib.bib214)] | Faster R-CNN, ResNet-50 | COCO, GOT-10k,
    LaSOT | DAF | N/A, Titan X GPU | Python | PyTorch | 6 | BB, SGM |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| GlobalTrack [[214](#bib.bib214)] | Faster R-CNN, ResNet-50 | COCO, GOT-10k,
    LaSOT | DAF | N/A, Titan X GPU | Python | PyTorch | 6 | BB, SGM |'
- en: '| SPLT [[218](#bib.bib218)] | MobileNet-v1, ResNet-50 | ILSVRC-VID, ILSVRC-DET
    | DAF | Inter I7 CPU, 32GB RAM, GTX 1080Ti GPU | Python | TensorFlow, Keras |
    25.7 | BB, OS |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| SPLT [[218](#bib.bib218)] | MobileNet-v1, ResNet-50 | ILSVRC-VID, ILSVRC-DET
    | DAF | Inter I7 CPU, 32GB RAM, GTX 1080Ti GPU | Python | TensorFlow, Keras |
    25.7 | BB, OS |'
- en: 'TABLE III: Only online training for visual tracking. The abbreviations are
    denoted as: confidence map (CM), bounding box (BB), object score (OS), deep appearance
    features (DAF), action (AC).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE III: 仅限于视觉跟踪的在线训练。缩写表示为：置信度图（CM），边界框（BB），目标分数（OS），深度外观特征（DAF），动作（AC）。'
- en: '| Method | Backbone network | Exploited features | Strategy to alleviate the
    over-fitting problem | PC (CPU, RAM, Nvidia GPU) | Language | Framework | Speed
    (fps) | Tracking output |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 基础网络 | 利用的特征 | 缓解过拟合问题的策略 | 计算机（CPU, RAM, Nvidia GPU） | 语言 | 框架 | 速度（fps）
    | 跟踪输出 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| SMART [[163](#bib.bib163)] | ZFNet | DAF | Set the learning rates in conv1-conv3
    to zero | Intel 3.10GHz CPU, 256 GB RAM, GTX Titan X GPU | Matlab | Caffe |  27
    | CM |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| SMART [[163](#bib.bib163)] | ZFNet | DAF | 将conv1-conv3中的学习率设置为零 | Intel
    3.10GHz CPU, 256 GB RAM, GTX Titan X GPU | Matlab | Caffe | 27 | CM |'
- en: '| TCNN [[68](#bib.bib68)] | VGG-M | DAF | Only update fully-connected layers
    | Intel I7-5820K 3.30GHz CPU, GTX Titan X GPU | Matlab | MatConvNet |  1.5 | OS
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| TCNN [[68](#bib.bib68)] | VGG-M | DAF | 仅更新全连接层 | Intel I7-5820K 3.30GHz
    CPU, GTX Titan X GPU | Matlab | MatConvNet | 1.5 | OS |'
- en: '| C2FT [[174](#bib.bib174)] | VGG-M | DAF | Coarse-to-fine localization | Intel
    Xeon E5-2670 2.60GHz, GTX 1080Ti GPU | N/A | N/A | N/A | AC |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| C2FT [[174](#bib.bib174)] | VGG-M | DAF | 粗到细定位 | Intel Xeon E5-2670 2.60GHz,
    GTX 1080Ti GPU | N/A | N/A | N/A | AC |'
- en: '| TSN [[75](#bib.bib75)] | VGG-16 | DAF | Coarse-to-fine framework | N/A, GTX
    980Ti GPU | Matlab | MatConvNet |  1 | CM |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| TSN [[75](#bib.bib75)] | VGG-16 | DAF | 粗到细框架 | N/A, GTX 980Ti GPU | Matlab
    | MatConvNet | 1 | CM |'
- en: '| DNT [[98](#bib.bib98)] | VGG-16 | DAF | Set uniform weight decay in the objective
    functions | 3.40GHz CPU, GTX Titan GPU | Matlab | Caffe | 5 | CM |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| DNT [[98](#bib.bib98)] | VGG-16 | DAF | 在目标函数中设置统一的权重衰减 | 3.40GHz CPU, GTX
    Titan GPU | Matlab | Caffe | 5 | CM |'
- en: '| DSLT [[101](#bib.bib101)] | VGG-16 | DAF | Use seven last frames for model
    update | Intel I7 4.00GHz CPU, GTX Titan X GPU | Matlab | Caffe | 5.7 | CM |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| DSLT [[101](#bib.bib101)] | VGG-16 | DAF | 使用最后七帧进行模型更新 | Intel I7 4.00GHz
    CPU, GTX Titan X GPU | Matlab | Caffe | 5.7 | CM |'
- en: '| LSART [[120](#bib.bib120)] | VGG-16 | DAF | Two-stream training network to
    learn network parameters | Intel 4.00GHz CPU, 32GB RAM, GTX Titan X GPU | Matlab
    | Caffe |  1 | CM |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| LSART [[120](#bib.bib120)] | VGG-16 | DAF | 两流训练网络用于学习网络参数 | Intel 4.00GHz
    CPU, 32GB RAM, GTX Titan X GPU | Matlab | Caffe | 1 | CM |'
- en: '| adaDDCF [[134](#bib.bib134)] | VGG-16 | DAF | Regularization item for training
    of each layer | 3.40GHz CPU, Tesla K40 GPU | Matlab | MatConvNet | 9 | CM |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| adaDDCF [[134](#bib.bib134)] | VGG-16 | DAF | 每层训练的正则化项 | 3.40GHz CPU, Tesla
    K40 GPU | Matlab | MatConvNet | 9 | CM |'
- en: '| HSTC [[143](#bib.bib143)] | VGG-16 | DAF | Dropout layer and convolutional
    with the mask layer | Intel Xeon 2.10GHz CPU, GTX 1080 GPU | Matlab | Caffe |
    2.1 | CM |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| HSTC [[143](#bib.bib143)] | VGG-16 | DAF | Dropout 层和带掩膜的卷积层 | Intel Xeon
    2.10GHz CPU, GTX 1080 GPU | Matlab | Caffe | 2.1 | CM |'
- en: '| P-Track [[179](#bib.bib179)] | VGG-16 | DAF | Learning policy for update
    and re-initialization | N/A, Tesla K40 GPU | N/A | N/A |  10 | CM |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| P-Track [[179](#bib.bib179)] | VGG-16 | DAF | 更新和重新初始化的学习策略 | 无, Tesla K40
    GPU | 无 | 无 | 10 | CM |'
- en: '| OSAA [[193](#bib.bib193)] | ResNet-50 or MobileNet-v2 | DAF | - | N/A, Tesla
    V100 GPU | Python | PyTorch | N/A | BB |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| OSAA [[193](#bib.bib193)] | ResNet-50 或 MobileNet-v2 | DAF | - | 无, Tesla
    V100 GPU | Python | PyTorch | 无 | BB |'
- en: '| STCT | Custom | DAF | Sequential training method | 3.40GHz CPU, GTX Titan
    GPU | Matlab | Caffe | 2.5 | CM |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| STCT | 自定义 | DAF | 顺序训练方法 | 3.40GHz CPU, GTX Titan GPU | Matlab | Caffe |
    2.5 | CM |'
- en: '| DeepTrack [[64](#bib.bib64)] | Custom | DAF | Temporal sampling mechanism
    for the batch generation in SGD algorithm | Quad-core CPU, GTX 980 GPU | Matlab
    | N/A | 2.5 | OS |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| DeepTrack [[64](#bib.bib64)] | 自定义 | DAF | SGD 算法中批量生成的时间采样机制 | 四核 CPU, GTX
    980 GPU | Matlab | 无 | 2.5 | OS |'
- en: '| CNT [[66](#bib.bib66)] | Custom | DAF | Incremental update scheme | Intel
    I7-3770 3.40GHz CPU, GPU | Matlab | N/A | 5 | BB |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| CNT [[66](#bib.bib66)] | 自定义 | DAF | 增量更新方案 | Intel I7-3770 3.40GHz CPU,
    GPU | Matlab | 无 | 5 | BB |'
- en: '| RDLT [[69](#bib.bib69)] | Custom | DAF | Build relationship between the stable
    factor and iteration number | Intel I7 2.20GHz CPU | Matlab | N/A |  5 | CM |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| RDLT [[69](#bib.bib69)] | 自定义 | DAF | 构建稳定因子与迭代次数之间的关系 | Intel I7 2.20GHz
    CPU | Matlab | 无 | 5 | CM |'
- en: '| P2T [[139](#bib.bib139)] | Custom | DAF | Generate large scale of part pairs
    in each mini-batch | Intel I7-4790 3.60GHz CPU, 32GB RAM, GTX 980 GPU | Matlab
    | Caffe |  2 | BB |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| P2T [[139](#bib.bib139)] | 自定义 | DAF | 在每个小批次中生成大量部件对 | Intel I7-4790 3.60GHz
    CPU, 32GB RAM, GTX 980 GPU | Matlab | Caffe | 2 | BB |'
- en: '| AEPCF [[167](#bib.bib167)] | Custom | DAF | Select a proper learning rate
    | Intel I7 3.40GHz, 32GB RAM, GPU | N/A | N/A | 4.15 | CM |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| AEPCF [[167](#bib.bib167)] | 自定义 | DAF | 选择合适的学习率 | Intel I7 3.40GHz, 32GB
    RAM, GPU | 无 | 无 | 4.15 | CM |'
- en: '| FRPN2T-Siam [[126](#bib.bib126)] | Custom | DAF | Only update fully-connected
    layers | N/A | Matlab | Caffe | N/A | CM |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| FRPN2T-Siam [[126](#bib.bib126)] | 自定义 | DAF | 仅更新全连接层 | 无 | Matlab | Caffe
    | 无 | CM |'
- en: '| RLS [[195](#bib.bib195)] | Custom | DAF | Recursive LSE-aided online learning
    method | N/A | Python | N/A | N/A | OS |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| RLS [[195](#bib.bib195)] | 自定义 | DAF | 递归 LSE 辅助的在线学习方法 | 无 | Python | 无
    | 无 | OS |'
- en: 'TABLE IV: Both offline and online training for visual tracking. The abbreviations
    are denoted as: confidence map (CM), bounding box (BB), rotated bounding box (RBB),
    object score (OS), voting map (VM), action (AC), segmentation mask (SGM), deep
    appearance features (DAF), deep motion features (DMF), compressed deep appearance
    features (CDAF).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：视觉跟踪的离线和在线训练。缩写表示为：置信度图（CM）、边界框（BB）、旋转边界框（RBB）、对象得分（OS）、投票图（VM）、动作（AC）、分割掩膜（SGM）、深度外观特征（DAF）、深度运动特征（DMF）、压缩深度外观特征（CDAF）。
- en: '| Method | Backbone network | Offline training(s) | Online network training
    | Exploited features | PC (CPU, RAM, Nvidia GPU) | Language | Framework | Speed
    (fps) | Tracking output |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | 离线训练(s) | 在线网络训练 | 使用的特征 | 计算机 (CPU, RAM, Nvidia GPU) | 语言 |
    框架 | 速度 (fps) | 跟踪输出 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| DRN [[97](#bib.bib97)] | AlexNet | ImageNet | Yes | DAF | N/A, K20 GPU |
    Matlab | Caffe | 1.3 | CM |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| DRN [[97](#bib.bib97)] | AlexNet | ImageNet | 是 | DAF | 无, K20 GPU | Matlab
    | Caffe | 1.3 | CM |'
- en: '| DSiam/DSiamM [[74](#bib.bib74)] | AlexNet, VGG-19 | ImageNet, ILSVRC-VID
    | Yes | DAF | N/A, GTX Titan X GPU | Matlab | MatConvNet | 45 | CM |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| DSiam/DSiamM [[74](#bib.bib74)] | AlexNet, VGG-19 | ImageNet, ILSVRC-VID
    | 是 | DAF | 无, GTX Titan X GPU | Matlab | MatConvNet | 45 | CM |'
- en: '| TripletLoss [[100](#bib.bib100)] | AlexNet | ImageNet, ILSVRC-VID, ILSVRC
    | Dependent | DAF | Intel I7-6700 3.40GHz CPU, GTX 1080Ti GPU | Matlab | MatConvNet
    | 55 86 | CM |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| TripletLoss [[100](#bib.bib100)] | AlexNet | ImageNet, ILSVRC-VID, ILSVRC
    | 依赖于 | DAF | Intel I7-6700 3.40GHz CPU, GTX 1080Ti GPU | Matlab | MatConvNet
    | 55 86 | CM |'
- en: '| MM [[165](#bib.bib165)] | AlexNet | ImageNet, OTB2015, ILSVRC | Yes | DAF
    | Intel I7-6700 4.00GHz CPU, 16GB RAM, GTX 1060 GPU | Matlab | MatConvNet | 1.2
    | OS |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| MM [[165](#bib.bib165)] | AlexNet | ImageNet, OTB2015, ILSVRC | 是 | DAF |
    Intel I7-6700 4.00GHz CPU, 16GB RAM, GTX 1060 GPU | Matlab | MatConvNet | 1.2
    | OS |'
- en: '| TAAT [[169](#bib.bib169)] | AlexNet, VGGNet, ResNet | ImageNet, ALOV, ILSVRC-VID
    | Yes | DAF | Intel Xeon 1.60GHz CPU, 16GB RAM, GTX Titan X GPU | Matlab | Caffe
    | 15 | BB |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| TAAT [[169](#bib.bib169)] | AlexNet, VGGNet, ResNet | ImageNet, ALOV, ILSVRC-VID
    | 是 | DAF | Intel Xeon 1.60GHz CPU, 16GB RAM, GTX Titan X GPU | Matlab | Caffe
    | 15 | BB |'
- en: '| DPST [[55](#bib.bib55)] | VGG-M | ImageNet, ILSVRC-VID, ALOV | Only on the
    first frame | DAF | Intel I7 3.60GHz CPU, GTX 1080Ti GPU | Matlab | MatConvNet
    |  1 | OS |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| DPST [[55](#bib.bib55)] | VGG-M | ImageNet, ILSVRC-VID, ALOV | 仅限于第一帧 | DAF
    | Intel I7 3.60GHz CPU, GTX 1080Ti GPU | Matlab | MatConvNet | 1 | OS |'
- en: '| MDNet [[60](#bib.bib60)] | VGG-M | ImageNet, OTB2015, ILSVRC-VID | Yes |
    DAF | Intel Xeon 2.20GHz CPU, Tesla K20m GPU | Matlab | MatConvNet |  1 | OS |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| MDNet [[60](#bib.bib60)] | VGG-M | ImageNet, OTB2015, ILSVRC-VID | 是 | DAF
    | Intel Xeon 2.20GHz CPU, Tesla K20m GPU | Matlab | MatConvNet | 1 | OS |'
- en: '| GNet [[82](#bib.bib82)] | VGG-M | ImageNet, VOT | Yes | DAF | Intel Xeon
    2.66GHz CPU, Tesla K40 GPU | Matlab | MatConvNet | 1 | OS |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| GNet [[82](#bib.bib82)] | VGG-M | ImageNet, VOT | 是 | DAF | Intel Xeon 2.66GHz
    CPU, Tesla K40 GPU | Matlab | MatConvNet | 1 | OS |'
- en: '| BranchOut [[90](#bib.bib90)] | VGG-M | ImageNet, OTB2015, ILSVRC | Yes |
    DAF | N/A | Matlab | MatConvNet | N/A | OS |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| BranchOut [[90](#bib.bib90)] | VGG-M | ImageNet, OTB2015, ILSVRC | 是 | DAF
    | 不适用 | Matlab | MatConvNet | 不适用 | OS |'
- en: '| SANet [[94](#bib.bib94)] | VGG-M | ImageNet, OTB2015, ILSVRC | Yes | DAF
    | Intel I7 3.70GHz CPU, GTX Titan Z GPU | Matlab | MatConvNet |  1 | OS |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| SANet [[94](#bib.bib94)] | VGG-M | ImageNet, OTB2015, ILSVRC | 是 | DAF |
    Intel I7 3.70GHz CPU, GTX Titan Z GPU | Matlab | MatConvNet | 1 | OS |'
- en: '| RT-MDNet [[105](#bib.bib105)] | VGG-M | ImageNet, ILSVRC-VID | Yes | DAF
    | Intel I7-6850K 3.60GHz, Titan Xp Pascal GPU | Python | PyTorch | 46 | OS |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| RT-MDNet [[105](#bib.bib105)] | VGG-M | ImageNet, ILSVRC-VID | 是 | DAF |
    Intel I7-6850K 3.60GHz, Titan Xp Pascal GPU | Python | PyTorch | 46 | OS |'
- en: '| TRACA [[113](#bib.bib113)] | VGG-M | ImageNet, PASCAL VOC | Yes | CDAF |
    Intel I7-2700K 3.50GHz CPU, 16GB RAM, GTX 1080 GPU | Matlab | MatConvNet | 101.3
    | CM |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| TRACA [[113](#bib.bib113)] | VGG-M | ImageNet, PASCAL VOC | 是 | CDAF | Intel
    I7-2700K 3.50GHz CPU, 16GB RAM, GTX 1080 GPU | Matlab | MatConvNet | 101.3 | CM
    |'
- en: '| VITAL [[114](#bib.bib114)] | VGG-M | ImageNet, OTB2015, ILSVRC | Yes | DAF
    | Intel I7 3.60GHz CPU, Tesla K40c GPU | Matlab | MatConvNet | 1.5 | OS |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| VITAL [[114](#bib.bib114)] | VGG-M | ImageNet, OTB2015, ILSVRC | 是 | DAF
    | Intel I7 3.60GHz CPU, Tesla K40c GPU | Matlab | MatConvNet | 1.5 | OS |'
- en: '| DAT [[130](#bib.bib130)] | VGG-M | ImageNet, OTB2015, ILSVRC | Yes | DAF
    | Intel I7-3.40GHz CPU, GTX 1080 GPU | Python | PyTorch | 1 | CM |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| DAT [[130](#bib.bib130)] | VGG-M | ImageNet, OTB2015, ILSVRC | 是 | DAF |
    Intel I7-3.40GHz CPU, GTX 1080 GPU | Python | PyTorch | 1 | CM |'
- en: '| ACT [[103](#bib.bib103)] | VGG-M | ImageNet-Video, ILSVRC | Yes | DAF | 3.40GHz
    CPU, 32GB RAM, GTX Titan GPU | Python | PyTorch | 30 | OS |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| ACT [[103](#bib.bib103)] | VGG-M | ImageNet-Video, ILSVRC | 是 | DAF | 3.40GHz
    CPU, 32GB RAM, GTX Titan GPU | Python | PyTorch | 30 | OS |'
- en: '| MGNet [[146](#bib.bib146)] | VGG-M | ImageNet, OTB2015, ILSVRC | Yes | DAF,
    DMF | Intel I7-5930K 3.50GHz CPU, GTX Titan X GPU | Matlab | MatConvNet |  2 |
    OS |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| MGNet [[146](#bib.bib146)] | VGG-M | ImageNet, OTB2015, ILSVRC | 是 | DAF,
    DMF | Intel I7-5930K 3.50GHz CPU, GTX Titan X GPU | Matlab | MatConvNet | 2 |
    OS |'
- en: '| DRL-IS [[175](#bib.bib175)] | VGG-M | ImageNet, VOT2013 2015 | Yes | DAF
    | Intel I7 3.40GHz CPU, 24GB RAM, GTX 1080Ti GPU | Python | PyTorch | 10.2 | AC
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| DRL-IS [[175](#bib.bib175)] | VGG-M | ImageNet, VOT2013 2015 | 是 | DAF |
    Intel I7 3.40GHz CPU, 24GB RAM, GTX 1080Ti GPU | Python | PyTorch | 10.2 | AC
    |'
- en: '| ADNet [[172](#bib.bib172), [173](#bib.bib173)] | VGG-M | ImageNet, VOT2013 2015,
    ALOV | Yes | DAF | Intel I7-4790K, 32GB RAM, GTX Titan X GPU | Matlab | MatConvNet
    | 15 | AC, OS |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| ADNet [[172](#bib.bib172), [173](#bib.bib173)] | VGG-M | ImageNet, VOT2013
    2015, ALOV | 是 | DAF | Intel I7-4790K, 32GB RAM, GTX Titan X GPU | Matlab | MatConvNet
    | 15 | AC, OS |'
- en: '| FMFT [[127](#bib.bib127)] | VGG-16 | ImageNet | Yes | DAF | Intel Xeon 3.50GHz
    CPU, GTX Titan X GPU | Matlab | MatConvNet | N/A | CM |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| FMFT [[127](#bib.bib127)] | VGG-16 | ImageNet | 是 | DAF | Intel Xeon 3.50GHz
    CPU, GTX Titan X GPU | Matlab | MatConvNet | 不适用 | CM |'
- en: '| DET [[96](#bib.bib96)] | VGG-16 | ImageNet, ALOV, VOT2014, VOT2015 | Yes
    | DAF | Intel I7-4790 3.60GHz CPU, GTX Titan X GPU | Python | Keras | 3.4 | OS
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| DET [[96](#bib.bib96)] | VGG-16 | ImageNet, ALOV, VOT2014, VOT2015 | 是 |
    DAF | Intel I7-4790 3.60GHz CPU, GTX Titan X GPU | Python | Keras | 3.4 | OS |'
- en: '| DCFNet/DCFNet2 [[95](#bib.bib95)] | VGGNet | ImageNet, TC128, UAV123, NUS-PRO
    | Yes | DAF | Intel Xeon 2.40GHz CPU, GTX 1080 GPU | Matlab | MatConvNet | 65
    | CM |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| DCFNet/DCFNet2 [[95](#bib.bib95)] | VGGNet | ImageNet, TC128, UAV123, NUS-PRO
    | 是 | DAF | Intel Xeon 2.40GHz CPU, GTX 1080 GPU | Matlab | MatConvNet | 65 |
    CM |'
- en: '| STP [[109](#bib.bib109)] | VGGNet | ImageNet | Yes | Votes | N/A, GTX Titan
    X GPU | Python | PyTorch | 4 | VM |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| STP [[109](#bib.bib109)] | VGGNet | ImageNet | 是 | 投票 | 无，GTX Titan X GPU
    | Python | PyTorch | 4 | VM |'
- en: '| MRCNN [[164](#bib.bib164)] | VGGNet | ImageNet, VOT2015 | Yes | DAF | Intel
    I7 3.50GHz CPU, GTX 1080 GPU | Matlab | MatConvNet |  1.2 | CM |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| MRCNN [[164](#bib.bib164)] | VGGNet | ImageNet, VOT2015 | 是 | DAF | Intel
    I7 3.50GHz CPU, GTX 1080 GPU | Matlab | MatConvNet | 1.2 | CM |'
- en: '| CODA [[161](#bib.bib161)] | VGG-19, SSD | ImageNet, VOT2013, VOT2014, VOT2015
    | Yes | DAF | Intel I7-4770K CPU, 32GB RAM, GTX 1070 GPU | Matlab | Caffe | 34.8
    | CM |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| CODA [[161](#bib.bib161)] | VGG-19, SSD | ImageNet, VOT2013, VOT2014, VOT2015
    | 是 | DAF | Intel I7-4770K CPU, 32GB RAM, GTX 1070 GPU | Matlab | Caffe | 34.8
    | CM |'
- en: '| ATOM [[149](#bib.bib149)] | ResNet-18, IoU-Nets | ImageNet, COCO, LaSOT,
    TrackingNet | Yes | DAF | N/A, GTX 1080 GPU | Python | PyTorch | 30 | CM |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| ATOM [[149](#bib.bib149)] | ResNet-18, IoU-Nets | ImageNet, COCO, LaSOT,
    TrackingNet | 是 | DAF | 无，GTX 1080 GPU | Python | PyTorch | 30 | CM |'
- en: '| D3S [[192](#bib.bib192)] | ResNet-50 | Youtube-VOS | Yes | DAF | N/A, GTX
    1080 GPU | Python | PyTorch | 25 | RBB, SGM |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| D3S [[192](#bib.bib192)] | ResNet-50 | Youtube-VOS | 是 | DAF | 无，GTX 1080
    GPU | Python | PyTorch | 25 | RBB, SGM |'
- en: '| MetaUpdater [[217](#bib.bib217)] | ResNet-50 | ImageNet, LaSOT | Yes | DAF
    | Intel I9 CPU, 64GB RAM, GTX 2080Ti GPU | Python | TensorFlow | 13 | CM |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| MetaUpdater [[217](#bib.bib217)] | ResNet-50 | ImageNet, LaSOT | 是 | DAF
    | Intel I9 CPU, 64GB RAM, GTX 2080Ti GPU | Python | TensorFlow | 13 | CM |'
- en: '| CRAC [[207](#bib.bib207)] | ResNet-50 | ImageNet, KITTI, VisDrone-2018 |
    Yes | DAF | N/A | Python | PyTorch, MatConvNet | 56 | OS |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| CRAC [[207](#bib.bib207)] | ResNet-50 | ImageNet, KITTI, VisDrone-2018 |
    是 | DAF | 无 | Python | PyTorch, MatConvNet | 56 | OS |'
- en: '| COMET [[212](#bib.bib212)] | ResNet-50 | ImageNet, LaSOT, GOT-10K, NfS, VisDrone-2019
    | Yes | DAF | N/A, Tesla V100 GPU | Python | PyTorch | 24 | CM |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| COMET [[212](#bib.bib212)] | ResNet-50 | ImageNet, LaSOT, GOT-10K, NfS, VisDrone-2019
    | 是 | DAF | 无，Tesla V100 GPU | Python | PyTorch | 24 | CM |'
- en: '| FGLT [[213](#bib.bib213)] | VGG-M, ResNet-50, PWC-Net | ImageNet, ILSVRC-VID,
    COCO, ILSVRC-DET, YTBB, FlyingChairs, FlyingThings3D | Yes | DAF | Intel Xeon
    3.50GHz CPU, GTX 1080Ti GPU | Python | PyTorch | N/A | CM, BB |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| FGLT [[213](#bib.bib213)] | VGG-M, ResNet-50, PWC-Net | ImageNet, ILSVRC-VID,
    COCO, ILSVRC-DET, YTBB, FlyingChairs, FlyingThings3D | 是 | DAF | Intel Xeon 3.50GHz
    CPU, GTX 1080Ti GPU | Python | PyTorch | 无 | CM, BB |'
- en: '| LRVN [[216](#bib.bib216)] | VGG-M, MobileNet | ImageNet, ILSVRC-VID, ILSVRC-DET
    | Yes | DAF | Intel I7 CPU, 32GB RAM, GTX Titan X GPU | Python | TensorFlow |
    2.7 | BB, OS |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| LRVN [[216](#bib.bib216)] | VGG-M, MobileNet | ImageNet, ILSVRC-VID, ILSVRC-DET
    | 是 | DAF | Intel I7 CPU, 32GB RAM, GTX Titan X GPU | Python | TensorFlow | 2.7
    | BB, OS |'
- en: '| UCT/UCT-Lite [[73](#bib.bib73)] | ResNet101 | ImageNet, TC128, UAV123 | Only
    on the first frame | DAF | Intel I7-6700 CPU, 48GB RAM, GTX Titan X GPU | Matlab
    | Caffe | 41 | CM |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| UCT/UCT-Lite [[73](#bib.bib73)] | ResNet101 | ImageNet, TC128, UAV123 | 仅在第一帧上
    | DAF | Intel I7-6700 CPU, 48GB RAM, GTX Titan X GPU | Matlab | Caffe | 41 | CM
    |'
- en: '| FPRNet [[132](#bib.bib132)] | ResNet-101, FlowNet | ImageNet, ILSVRC, SceneFlow
    | Yes | DAF, DMF | N/A | Matlab | Caffe | N/A | BB |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| FPRNet [[132](#bib.bib132)] | ResNet-101, FlowNet | ImageNet, ILSVRC, SceneFlow
    | 是 | DAF, DMF | 无 | Matlab | Caffe | 无 | BB |'
- en: '| ADT [[160](#bib.bib160)] | - | ImageNet, ALOV300++, UAV123, NUS-PRO | Only
    on the first frame | DAF | Intel 2.40GHz CPU, GTX TITAN X GPU | Python | TensorFlow
    | 7 | CM |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| ADT [[160](#bib.bib160)] | - | ImageNet, ALOV300++, UAV123, NUS-PRO | 仅在第一帧上
    | DAF | Intel 2.40GHz CPU, GTX TITAN X GPU | Python | TensorFlow | 7 | CM |'
- en: '| DiMP [[159](#bib.bib159)] | ResNet-18, ResNet-50 | ImageNet, TrackingNet,
    LaSOT, GOT10k, COCO | Meta-learning | DAF | N/A, GTX 1080 GPU | Python | PyTorch
    | 43 57 | OS |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| DiMP [[159](#bib.bib159)] | ResNet-18, ResNet-50 | ImageNet, TrackingNet,
    LaSOT, GOT10k, COCO | 元学习 | DAF | 无，GTX 1080 GPU | Python | PyTorch | 43 57 |
    OS |'
- en: '| PrDiMP [[194](#bib.bib194)] | ResNet-18 or ResNet-50 | ImageNet, LaSOT, GOT-10k,
    TrackingNet, COCO | Meta-learning | DAF | N/A | Python | PyTorch | 30 40 | CM
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| PrDiMP [[194](#bib.bib194)] | ResNet-18 或 ResNet-50 | ImageNet, LaSOT, GOT-10k,
    TrackingNet, COCO | 元学习 | DAF | 无 | Python | PyTorch | 30 40 | CM |'
- en: '| BGBDT [[185](#bib.bib185)] | SSD or FasterRCNN | ImageNet, COCO, GOT-10k
    | Meta-learning | DAF | N/A, GTX 1080 GPU | Python | PyTorch | 3 10 | BB |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| BGBDT [[185](#bib.bib185)] | SSD 或 FasterRCNN | ImageNet, COCO, GOT-10k |
    元学习 | DAF | 无，GTX 1080 GPU | Python | PyTorch | 3 10 | BB |'
- en: '| MLT [[188](#bib.bib188)] | AlexNet | ImageNet, ILSVRC-2015, ILSVRC-2017 |
    Meta-learning | DAF | Intel I7-4790K 4.0GHz CPU, 32GB RAM, GTX Titan X GPU | Python
    | TensorFlow | 48.1 | CM |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| MLT [[188](#bib.bib188)] | AlexNet | ImageNet, ILSVRC-2015, ILSVRC-2017 |
    元学习 | DAF | Intel I7-4790K 4.0GHz CPU, 32GB RAM, GTX Titan X GPU | Python | TensorFlow
    | 48.1 | CM |'
- en: '| ROAM [[196](#bib.bib196)] | VGG-16 | ImageNet, ILSVRC-VID, ILSVRC-DET, TrackingNet,
    LaSOT, GOT-10k, COCO | Meta-learning | DAF | Intel I9 3.6GHz CPU, 4 RTX 2080 GPU
    | Python | PyTorch | 13 | CM |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| ROAM [[196](#bib.bib196)] | VGG-16 | ImageNet, ILSVRC-VID, ILSVRC-DET, TrackingNet,
    LaSOT, GOT-10k, COCO | 元学习 | DAF | Intel I9 3.6GHz CPU, 4 RTX 2080 GPU | Python
    | PyTorch | 13 | CM |'
- en: '| TMAML [[201](#bib.bib201)] | ResNet-18 with RetinaNet or FCOS | ImageNet,
    COCO, GOT-10k, TrackingNet, LaSOT | Meta-learning | DAF | N/A, P100 GPU | Python
    | N/A | 40 | BB |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| TMAML [[201](#bib.bib201)] | 使用RetinaNet或FCOS的ResNet-18 | ImageNet、COCO、GOT-10k、TrackingNet、LaSOT
    | 元学习 | DAF | 不适用、P100 GPU | Python | 不适用 | 40 | BB |'
- en: '| Meta-Tracker [[182](#bib.bib182)] | dependent | ImageNet, ILSVRC-DET, VOT-2013,
    VOT-2014, VOT-2015 | Meta-learning | DAF | N/A, GTX Titan X GPU | Python | PyTorch
    | dependent | dependent |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Meta-Tracker [[182](#bib.bib182)] | 依赖 | ImageNet、ILSVRC-DET、VOT-2013、VOT-2014、VOT-2015
    | 元学习 | DAF | 不适用、GTX Titan X GPU | Python | PyTorch | 依赖 | 依赖 |'
- en: II-C3 Only Online Training
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C3 仅在线训练
- en: 'To discriminate unseen targets that may consider as the target in evaluation
    videos, some DL-based visual tracking methods use online training of whole or
    a part of DNNs to adapt network parameters according to the large variety of target
    appearance. Because of the time-consuming process of offline training on large-scale
    training data and insufficient discrimination of pre-trained models for representing
    tracking particular targets, the methods shown in Table [III](#S2.T3 "TABLE III
    ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual Tracking
    Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey") use directly
    training of DNNs and inference process alternatively online. However, these methods
    usually exploit some strategies to prevent over-fitting problem and divergence.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '为了区分在评估视频中可能被视为目标的未知目标，一些基于深度学习的视觉跟踪方法使用整个或部分DNN的在线训练，根据目标外观的大量变化来调整网络参数。由于在大规模训练数据上进行离线训练的耗时过程和预训练模型对于表示跟踪特定目标的不足的区分能力，表格[III](#S2.T3
    "TABLE III ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")中的方法直接训练DNN并交替进行推理过程。然而，这些方法通常采用一些策略来防止过拟合问题和发散现象。'
- en: II-C4 Both Offline and Online Training
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C4 离线和在线训练
- en: 'To exploit the maximum capacity of DNNs for visual tracking, the methods shown
    in Table [IV](#S2.T4 "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network Training
    ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey") use both offline and online training. The offline and online learned
    features are known as shared and domain-specific representations, which majorly
    can discriminate the target from foreground information or intra-class distractors,
    respectively. Because visual tracking is a hard and challenging problem, the DL-based
    visual trackers attempt to simultaneously employ feature transferability and online
    domain adaptation.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '为了充分利用深度神经网络在视觉跟踪中的最大能力，表格[IV](#S2.T4 "TABLE IV ‣ II-C2 Only Offline Training
    ‣ II-C Network Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for
    Visual Tracking: A Comprehensive Survey")中的方法同时使用离线训练和在线训练。离线和在线学习到的特征分别称为共享和领域特定表示，主要用于区分目标和前景信息或同类分心因素。由于视觉跟踪是一个困难而具有挑战性的问题，DL-based视觉跟踪器尝试同时利用特征可迁移性和在线领域适应性。'
- en: II-C5 Data Augmentation
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C5 数据增强
- en: 'Data augmentation comprises a set of techniques for increasing training samples’
    size to improve data quality and avoid the over-fitting problem. Visual trackers
    broadly employ these techniques based on the few-data regime of this task (see
    Table [V](#S2.T5 "TABLE V ‣ II-C5 Data Augmentation ‣ II-C Network Training ‣
    II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")). The geometric transformations & color space augmentations are vastly
    exploited for visual tracking. However, other algorithms, such as employing GANs
    [[114](#bib.bib114)], can effectively impact tracking performance by capturing
    various appearance changes.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '数据增强是一组增加训练样本数量的技术，以改善数据质量并避免过拟合问题。视觉跟踪器广泛采用这些技术，基于这个任务的少量数据情景（参见表格[V](#S2.T5
    "TABLE V ‣ II-C5 Data Augmentation ‣ II-C Network Training ‣ II Deep Visual Tracking
    Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")）。几何变换和颜色空间增强广泛用于视觉跟踪。然而，其他算法，如使用GANs
    [[114](#bib.bib114)]，可以通过捕捉各种外观变化有效地影响跟踪性能。'
- en: 'TABLE V: Data Augmentations for Visual Tracking Methods.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 V：视觉跟踪方法的数据增强。
- en: '| Method | Augmentations |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 增强 |'
- en: '| --- | --- |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GOTURN [[57](#bib.bib57)] | Motion model, random crops |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| GOTURN [[57](#bib.bib57)] | 运动模型，随机裁剪 |'
- en: '| MDNet [[60](#bib.bib60)] | Multiple positive examples around the target |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| MDNet [[60](#bib.bib60)] | 目标周围的多个正样本 |'
- en: '| DeepTrack [[65](#bib.bib65), [64](#bib.bib64)] | Horizontal flip |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| DeepTrack [[65](#bib.bib65), [64](#bib.bib64)] | 水平翻转 |'
- en: '| RFL [[77](#bib.bib77)] | Random color distortion, translation, stretching
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| RFL [[77](#bib.bib77)] | 随机颜色失真、平移、拉伸 |'
- en: '| VRCPF [[84](#bib.bib84)] | Annotated face ROIs using PASCAL VOC 2007 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| VRCPF [[84](#bib.bib84)] | 使用 PASCAL VOC 2007 注释的人脸 RoI |'
- en: '| DNT [[98](#bib.bib98)] | Center-shifted random patches, translation schemes
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| DNT [[98](#bib.bib98)] | 中心偏移的随机补丁、平移方案 |'
- en: '| UPDT [[102](#bib.bib102)] | Flip, rotation, shift, blur, dropout |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| UPDT [[102](#bib.bib102)] | 翻转、旋转、平移、模糊、丢弃 |'
- en: '| DaSiamRPN [[104](#bib.bib104)] | Translation, scale variations and illumination
    changes, motion blur |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| DaSiamRPN [[104](#bib.bib104)] | 平移、缩放变化和光照变化、运动模糊 |'
- en: '| STP [[109](#bib.bib109)] | Randomly shifted pairs |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| STP [[109](#bib.bib109)] | 随机偏移的配对 |'
- en: '| Siam-MCF [[110](#bib.bib110)] | Random cropping, color distortion, horizontal
    flipping, small resizing perturbations |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Siam-MCF [[110](#bib.bib110)] | 随机裁剪、颜色失真、水平翻转、小幅度缩放 |'
- en: '| TRACA [[113](#bib.bib113)] | Blur, flip |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| TRACA [[113](#bib.bib113)] | 模糊、翻转 |'
- en: '| VITAL [[114](#bib.bib114)] | Randomly masks |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| VITAL [[114](#bib.bib114)] | 随机遮罩 |'
- en: '| SiamRPN [[116](#bib.bib116)] | Affine transformation |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| SiamRPN [[116](#bib.bib116)] | 仿射变换 |'
- en: '| YCNN [[135](#bib.bib135)] | Rotation, translation, illumination variation,
    mosaic, salt & pepper noise |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| YCNN [[135](#bib.bib135)] | 旋转、平移、光照变化、马赛克、盐和胡椒噪声 |'
- en: '| DeepFWDCF [[144](#bib.bib144)] | Gray-scale rotation invariant LBP histograms
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| DeepFWDCF [[144](#bib.bib144)] | 灰度旋转不变的 LBP 直方图 |'
- en: '| ORHF [[147](#bib.bib147)] | Augmentation of negative samples |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| ORHF [[147](#bib.bib147)] | 负样本的增强 |'
- en: '| ATOM [[149](#bib.bib149)] | Translation, rotation, blur, dropout, flip, color
    jittering |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| ATOM [[149](#bib.bib149)] | 平移、旋转、模糊、丢弃、翻转、颜色抖动 |'
- en: '| MAM [[171](#bib.bib171)] | All detected windows from target category |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| MAM [[171](#bib.bib171)] | 从目标类别中检测到的所有窗口 |'
- en: '| DiMP50 [[159](#bib.bib159)] | Translation, rotation, blur, dropout, flip,
    color jittering |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| DiMP50 [[159](#bib.bib159)] | 平移、旋转、模糊、丢弃、翻转、颜色抖动 |'
- en: '| PrDiMP50 [[194](#bib.bib194)] | Translation, rotation, blur, dropout, flip,
    color jittering |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| PrDiMP50 [[194](#bib.bib194)] | 平移、旋转、模糊、丢弃、翻转、颜色抖动 |'
- en: '| TAAT [[169](#bib.bib169)] | Spatial & temporal pairs |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| TAAT [[169](#bib.bib169)] | 空间和时间配对 |'
- en: '| CGACD [[190](#bib.bib190)] | RoI augmentation |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| CGACD [[190](#bib.bib190)] | RoI 增强 |'
- en: '| MLT [[188](#bib.bib188)] | Horizontal flip, noise, Gaussian blur, translation
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| MLT [[188](#bib.bib188)] | 水平翻转、噪声、高斯模糊、平移 |'
- en: '| ROAM [[196](#bib.bib196)] | Stretching and scaling the images |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| ROAM [[196](#bib.bib196)] | 图像的拉伸和缩放 |'
- en: '| SiamRCNN [[200](#bib.bib200)] | Motion blur, gray-scale, gamma, flip, and
    scale augmentations |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| SiamRCNN [[200](#bib.bib200)] | 运动模糊、灰度、伽玛、翻转和缩放增强 |'
- en: '| TMAML [[201](#bib.bib201)] | Random scaling, shifting, zoom in/out |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| TMAML [[201](#bib.bib201)] | 随机缩放、平移、放大/缩小 |'
- en: II-C6 Meta-Learning
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C6 元学习
- en: 'As a well-known paradigm in machine learning, meta-learning [[246](#bib.bib246)]
    (an alternative for data augmentation) has provided promising results for visual
    tracking task. Generally, it aims to provide experience on several learning tasks
    and use them to improve the performance of a new task. Inspired by model-agnostic
    meta-learning (MAML) [[247](#bib.bib247)], visual trackers mainly seek to exploit
    meta-learners for constructing more flexible target models regarding unseen targets/scenarios
    (see Table [IV](#S2.T4 "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network
    Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")). For instance, it can be leveraged into the initialization
    [[182](#bib.bib182), [201](#bib.bib201)], fast model adaptation [[185](#bib.bib185),
    [188](#bib.bib188)], or model update [[196](#bib.bib196), [217](#bib.bib217)]
    procedures of visual trackers. However, some visual trackers [[159](#bib.bib159),
    [194](#bib.bib194)] exploit meta-learning ideas to adjust their model weights
    during tracking, which is different from the classic meta-learning definition.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '作为机器学习中的一个知名范式，元学习 [[246](#bib.bib246)]（数据增强的替代方法）为视觉跟踪任务提供了有希望的结果。一般来说，它旨在提供几个学习任务的经验，并利用这些经验提升新任务的表现。受到模型无关的元学习（MAML）
    [[247](#bib.bib247)] 的启发，视觉跟踪器主要寻求利用元学习者来构建更灵活的目标模型，以应对未知的目标/场景（见表 [IV](#S2.T4
    "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")）。例如，它可以被应用于视觉跟踪器的初始化
    [[182](#bib.bib182), [201](#bib.bib201)]、快速模型适应 [[185](#bib.bib185), [188](#bib.bib188)]
    或模型更新 [[196](#bib.bib196), [217](#bib.bib217)] 过程中。然而，一些视觉跟踪器 [[159](#bib.bib159),
    [194](#bib.bib194)] 利用元学习思想在跟踪过程中调整其模型权重，这与经典的元学习定义有所不同。'
- en: II-D Network Objective
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 网络目标
- en: For the training and inference stages, DL-based visual trackers localize the
    given target based on network objective function. Hence, these methods are categorized
    into classification-based, regression-based, or both classification and regression-based
    methods as follows. This sub-section does not include the methods that exploit
    deep off-the-shelf features because these methods do not design and train the
    networks and usually employ pre-trained DNNs for feature extraction.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和推理阶段，基于深度学习的视觉跟踪器根据网络目标函数对给定目标进行定位。因此，这些方法被分类为基于分类的、基于回归的或同时基于分类和回归的方法。该小节不包括利用深度现成特征的方法，因为这些方法不设计和训练网络，通常使用预训练的深度神经网络进行特征提取。
- en: II-D1 Classification-based Objective Function
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D1 基于分类的目标函数
- en: Motivated by other computer vision tasks such as image detection, classification-based
    visual tracking methods employ object proposal methods to produce hundreds of
    candidate/proposal BBs extracted from the search region. These methods aim to
    select the high score proposal by classifying the proposals to the target and
    background classes. This two-class (or binary) classification involves visual
    targets from various class and moving patterns and individual sequences, including
    challenging scenarios. Due to the main attention of these methods on inter-class
    classification, tracking a visual target in the presence of the same labeled targets
    is intensely prone to drift-problem. Also, tracking the arbitrary appearance of
    targets may lead to recognizing different targets with varying appearances. Therefore,
    the performance of the classification-based visual tracking methods is also related
    to their object proposal method, which usually produces a considerable number
    of candidate BBs. On the other side, some recent DL-based trackers utilize this
    objective function to take optimal actions [[162](#bib.bib162), [172](#bib.bib172),
    [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175), [177](#bib.bib177),
    [181](#bib.bib181), [174](#bib.bib174)].
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 受到其他计算机视觉任务如图像检测的启发，基于分类的视觉跟踪方法采用目标提议方法，从搜索区域中生成数百个候选/提议边界框（BBs）。这些方法旨在通过将提议分类为目标和背景类别来选择高分提议。这种两类（或二元）分类涉及到各种类别和运动模式的视觉目标以及个体序列，包括具有挑战性的场景。由于这些方法主要关注类间分类，因此在存在相同标记目标的情况下，跟踪视觉目标极易出现漂移问题。此外，跟踪目标的任意外观可能会导致识别具有不同外观的不同目标。因此，基于分类的视觉跟踪方法的性能也与其目标提议方法相关，通常生成大量候选边界框。另一方面，一些近期基于深度学习的跟踪器利用这一目标函数来采取最优行动[[162](#bib.bib162)、[172](#bib.bib172)、[173](#bib.bib173)、[174](#bib.bib174)、[175](#bib.bib175)、[177](#bib.bib177)、[181](#bib.bib181)、[174](#bib.bib174)]。
- en: II-D2 Regression-based Objective Function
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D2 基于回归的目标函数
- en: Due to the continuous instinct of estimation space of visual tracking, regression-based
    methods usually aim to directly localize the target in the subsequent frames by
    minimizing a regularized least-squares function. Generally, extensive training
    data are needed to train these methods effectively. The primary goal of regression-based
    methods is to refine the formulation of L2 or L1 loss functions, such as utilizing
    shrinkage loss in the learning procedure [[101](#bib.bib101)], modeling both regression
    coefficients and patch reliability to optimize a neural network efficiently [[120](#bib.bib120)],
    or applying a cost-sensitive loss to enhance unsupervised learning performance
    [[158](#bib.bib158)]. Meanwhile, recent visual trackers define a loss function
    for BB regression (e.g., [[190](#bib.bib190), [196](#bib.bib196), [202](#bib.bib202)])
    to provide accurate localization.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 由于视觉跟踪估计空间的持续本能，基于回归的方法通常通过最小化正则化的最小二乘函数来直接在后续帧中定位目标。一般来说，需要大量训练数据来有效地训练这些方法。基于回归的方法的主要目标是完善L2或L1损失函数的公式，例如在学习过程中利用收缩损失[[101](#bib.bib101)]，建模回归系数和补丁可靠性以有效优化神经网络[[120](#bib.bib120)]，或应用成本敏感损失以提升无监督学习性能[[158](#bib.bib158)]。与此同时，近期的视觉跟踪器定义了边界框回归的损失函数（例如[[190](#bib.bib190)、[196](#bib.bib196)、[202](#bib.bib202)]）以提供准确的定位。
- en: II-D3 Classification- and Regression-based Objective Function
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-D3 基于分类和回归的目标函数
- en: 'To take advantages of both foreground-background/category classification and
    ridge regression (i.e., regularized least-squares objective function), a broad
    range of trackers employ both classification- and regression-based objective functions
    for visual tracking (see Fig. [3](#S2.F3 "Figure 3 ‣ II Deep Visual Tracking Taxonomy
    ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")), which their goal
    is to bridge the gap between the recent tracking-by-detection and continuous localization
    process of visual tracking. These methods commonly utilize classification-based
    methods to find the most similar object proposal to target, and then the estimated
    region will be refined by a BB regression method [[60](#bib.bib60), [68](#bib.bib68),
    [80](#bib.bib80), [94](#bib.bib94), [103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105),
    [116](#bib.bib116), [130](#bib.bib130), [146](#bib.bib146), [156](#bib.bib156),
    [164](#bib.bib164), [169](#bib.bib169)]. The target regions are estimated by classification
    scores and optimized regression/matching functions [[127](#bib.bib127), [138](#bib.bib138),
    [139](#bib.bib139), [149](#bib.bib149), [150](#bib.bib150), [153](#bib.bib153),
    [154](#bib.bib154), [155](#bib.bib155), [159](#bib.bib159), [160](#bib.bib160),
    [163](#bib.bib163), [175](#bib.bib175), [212](#bib.bib212)] to enhance efficiency
    and accuracy. The classification outputs are mainly inferred for candidate proposals’
    confidence scores, foreground detection, candidate window response, actions, and
    so forth.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '为了兼顾前景-背景/类别分类和岭回归（即正则化最小二乘目标函数）的优点，广泛的跟踪器采用分类和回归基于目标函数进行视觉跟踪（见图[3](#S2.F3
    "Figure 3 ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")），其目标是弥合近期的检测跟踪和视觉跟踪的连续定位过程之间的差距。这些方法通常利用基于分类的方法来找到与目标最相似的物体提案，然后通过BB回归方法[[60](#bib.bib60),
    [68](#bib.bib68), [80](#bib.bib80), [94](#bib.bib94), [103](#bib.bib103), [104](#bib.bib104),
    [105](#bib.bib105), [116](#bib.bib116), [130](#bib.bib130), [146](#bib.bib146),
    [156](#bib.bib156), [164](#bib.bib164), [169](#bib.bib169)]来细化估计区域。目标区域通过分类分数和优化的回归/匹配函数[[127](#bib.bib127),
    [138](#bib.bib138), [139](#bib.bib139), [149](#bib.bib149), [150](#bib.bib150),
    [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155), [159](#bib.bib159),
    [160](#bib.bib160), [163](#bib.bib163), [175](#bib.bib175), [212](#bib.bib212)]来估计，以提高效率和准确性。分类输出主要推断候选提案的置信度分数、前景检测、候选窗口响应、动作等。'
- en: II-E Network Output
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E 网络输出
- en: 'According to the network objective, the DL-based methods generate different
    network outputs to estimate or refine the estimated target location. Based on
    their network outputs, the DL-based methods are classified into six main categories
    (see Fig. [3](#S2.F3 "Figure 3 ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning
    for Visual Tracking: A Comprehensive Survey") and Table [II](#S2.T2 "TABLE II
    ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual Tracking
    Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey") to Table [IV](#S2.T4
    "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual
    Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")),
    namely confidence map (also includes score map, response map, and voting map),
    BB (also includes rotated BB), object score (also includes the probability of
    object proposal, verification score, similarity score, and layer-wise score),
    action, feature maps, and segmentation mask. Besides template-based methods, segmentation-based
    trackers have been explored to boost tracking performance. Despite initial works
    [[248](#bib.bib248), [249](#bib.bib249), [250](#bib.bib250)], employing independent
    deep networks for tracking & VOS may lead to irretrievable tracking failures and
    high computational complexity. Thus, segmentation-based trackers aim to jointly
    track and segment visual targets by adding a segmentation branch to the network
    [[127](#bib.bib127), [155](#bib.bib155), [192](#bib.bib192), [197](#bib.bib197)]
    or off-the-shelf BB-to-segmentation (Box2Seg) networks [[251](#bib.bib251), [252](#bib.bib252)]
    to the base tracking network.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '根据网络目标，基于深度学习的方法生成不同的网络输出以估计或修正目标位置的估计。基于这些网络输出，基于深度学习的方法被分为六大类（见图 [3](#S2.F3
    "Figure 3 ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey") 和表 [II](#S2.T2 "TABLE II ‣ II-C2 Only Offline Training
    ‣ II-C Network Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for
    Visual Tracking: A Comprehensive Survey") 至表 [IV](#S2.T4 "TABLE IV ‣ II-C2 Only
    Offline Training ‣ II-C Network Training ‣ II Deep Visual Tracking Taxonomy ‣
    Deep Learning for Visual Tracking: A Comprehensive Survey")），即置信度图（还包括分数图、响应图和投票图）、BB（还包括旋转BB）、目标分数（还包括目标提议的概率、验证分数、相似度分数和逐层分数）、动作、特征图和分割掩模。除了基于模板的方法，基于分割的跟踪器也被探索以提升跟踪性能。尽管最初的工作
    [[248](#bib.bib248), [249](#bib.bib249), [250](#bib.bib250)] 采用独立的深度网络进行跟踪与 VOS
    可能导致不可恢复的跟踪失败和高计算复杂度。因此，基于分割的跟踪器旨在通过向网络添加分割分支 [[127](#bib.bib127), [155](#bib.bib155),
    [192](#bib.bib192), [197](#bib.bib197)] 或现成的 BB 到分割 (Box2Seg) 网络 [[251](#bib.bib251),
    [252](#bib.bib252)] 来联合跟踪和分割视觉目标。'
- en: II-F Exploitation of Correlation Filters Advantages
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-F 相关滤波器优势的利用
- en: 'The DCF-based methods aim to learn a set of discriminative filters that an
    element-wise multiplication of them with a set of training samples in the frequency
    domain determines spatial target location. Since DCF has provided competitive
    tracking performance and computational efficiency compared to sophisticated techniques,
    DL-based visual trackers use correlation filter advantages. These methods are
    categorized based on how they exploit DCF advantages by using either a whole DCF
    framework or some benefits, such as its objective function or correlation filters/layers.
    Considerable visual tracking methods are based on integrating deep features in
    the DCF framework (see Fig. [3](#S2.F3 "Figure 3 ‣ II Deep Visual Tracking Taxonomy
    ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")). These methods
    aim to improve the robustness of target representation against challenging attributes,
    while other methods attempt to benefit the computational efficiency of correlation
    filter(s) [[86](#bib.bib86)], correlation layer(s) [[118](#bib.bib118), [134](#bib.bib134),
    [141](#bib.bib141), [157](#bib.bib157), [166](#bib.bib166), [205](#bib.bib205)],
    and the objective function of correlation filters [[73](#bib.bib73), [74](#bib.bib74),
    [95](#bib.bib95), [121](#bib.bib121), [149](#bib.bib149), [158](#bib.bib158)].'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '基于DCF的方法旨在学习一组区分性滤波器，通过将这些滤波器与训练样本集在频域中逐元素相乘来确定空间目标位置。由于DCF在跟踪性能和计算效率上提供了有竞争力的表现，因此基于DL的视觉追踪器利用了相关滤波器的优势。这些方法根据它们如何利用DCF优势而被分类，使用整个DCF框架或某些好处，例如其目标函数或相关滤波器/层。大量视觉追踪方法基于在DCF框架中集成深度特征（见图[3](#S2.F3
    "Figure 3 ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")）。这些方法旨在提高目标表示对挑战性属性的鲁棒性，而其他方法则试图利用相关滤波器[[86](#bib.bib86)]、相关层[[118](#bib.bib118),
    [134](#bib.bib134), [141](#bib.bib141), [157](#bib.bib157), [166](#bib.bib166),
    [205](#bib.bib205)]和相关滤波器的目标函数[[73](#bib.bib73), [74](#bib.bib74), [95](#bib.bib95),
    [121](#bib.bib121), [149](#bib.bib149), [158](#bib.bib158)]的计算效率。'
- en: II-G Aerial-view Tracking
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-G 空中视角追踪
- en: By pervasive applications of flying robots, tracking from aerial views introduces
    extra attractive challenges, such as tiny objects, weather conditions, dense environments,
    long occlusions, significant viewpoint change, etc. Aerial-view trackers can be
    classified into class-specific & class-agnostic methods. Class-specific trackers
    mostly focus on human or vehicle classes, such as the unified contextual relation
    actor-critic (CRAC) [[207](#bib.bib207)], a GAN-based vehicle tracker that aims
    to model contextual relation and transfer the ground-view features to the aerial-ones.
    In contrast, class-agnostic trackers can track arbitrary classes of targets. Some
    DCF-based trackers [[206](#bib.bib206), [208](#bib.bib208), [209](#bib.bib209),
    [210](#bib.bib210), [211](#bib.bib211)] were the first generation of flying robot
    trackers, which address the inherent limitations of correlation filters (e.g.,
    boundary effect and filter corruption) given aerial view conditions. However,
    the coarse-to-fine tracker (C2FT) [[174](#bib.bib174)] employs deep RL coarse-
    & fine-trackers (for estimating entire BB and its refinement) to address significant
    aspect-ratio change of targets from aerial-views. Finally, the context-aware IoU-guided
    network for small object tracking (COMET) aims to narrow the performance gap between
    the aerial-view trackers & state-of-the-art ones. It employs an offline proposal
    generation strategy & a multitask two-stream network to exploit context information
    and handle out-of-view & occlusion effectively.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 由于飞行机器人广泛应用，从空中视角进行追踪引入了额外的挑战，如微小物体、天气条件、密集环境、长时间遮挡、显著视角变化等。空中视角追踪器可以分为特定类别方法和无类别方法。特定类别追踪器主要关注人类或车辆类别，例如统一上下文关系的演员-评论家（CRAC）[[207](#bib.bib207)]，这是一种基于GAN的车辆追踪器，旨在建模上下文关系并将地面视角特征转移到空中视角。相比之下，无类别追踪器可以追踪任意类别的目标。一些基于DCF的方法[[206](#bib.bib206),
    [208](#bib.bib208), [209](#bib.bib209), [210](#bib.bib210), [211](#bib.bib211)]是飞行机器人追踪器的第一代，它们解决了在空中视角条件下相关滤波器（例如边界效应和滤波器损坏）的固有局限性。然而，粗到细追踪器（C2FT）[[174](#bib.bib174)]采用深度强化学习粗糙-精细追踪器（用于估计整个边界框及其精细化）来解决从空中视角来看目标的显著长宽比变化。最后，针对小物体追踪的上下文感知IoU引导网络（COMET）旨在缩小空中视角追踪器与最先进方法之间的性能差距。它采用离线提议生成策略和多任务双流网络来利用上下文信息并有效处理视野外和遮挡问题。
- en: II-H Long-term Tracking
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-H 长期追踪
- en: Long-term tracking performs on more realistic scenarios, including (relatively)
    long videos in which targets may disappear & reappear. Despite the close relationship
    to practical applications, limited trackers have been proposed for this task.
    One way is the extension of a short-term tracker by various strategies. For instance,
    DaSiamRPN [[104](#bib.bib104)] uses a local-to-global search region strategy to
    handle out-of-view & full occlusion, while LCTdeep [[142](#bib.bib142)] utilizes
    a detection module with incremental updates. The FGLT [[213](#bib.bib213)] employs
    both MDNet [[60](#bib.bib60)] & SiamRPN++ [[156](#bib.bib156)] trackers for the
    tracking result judgment, and a detection module modifies tracking failures. The
    memory model via the Siamese network for long-term tracking (MMLT) [[107](#bib.bib107)]
    modifies the SiamFC tracker [[58](#bib.bib58)] by re-detection and memory management
    parts. Moreover, the improved Siamese tracker (i-Siam) [[215](#bib.bib215)] revisits
    the SiamFC tracker via a negative signal suppression approach & a diverse multi-template
    one. The multi-level CF-based tracker [[204](#bib.bib204)] employs an oriented
    re-detection technique, while MetaUpdater [[217](#bib.bib217)] exploits a SiamRPN-based
    re-detector and an online verifier with a meta-updater. Finally, the SPLT tracker
    [[218](#bib.bib218)] is based on SiamRPN [[116](#bib.bib116)] and comprises perusal
    and skimming modules for local tracking & search window selection.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 长期跟踪在更现实的场景中执行，包括（相对）较长的视频，其中目标可能会消失和重新出现。尽管与实际应用有着密切的关系，但为此任务提出的跟踪器有限。一种方法是通过各种策略扩展短期跟踪器。例如，DaSiamRPN
    [[104](#bib.bib104)] 使用局部到全局搜索区域策略来处理视野外和完全遮挡，而 LCTdeep [[142](#bib.bib142)] 利用具有增量更新的检测模块。FGLT
    [[213](#bib.bib213)] 使用 MDNet [[60](#bib.bib60)] 和 SiamRPN++ [[156](#bib.bib156)]
    跟踪器来判断跟踪结果，检测模块则修正跟踪失败。通过 Siamese 网络的长期跟踪记忆模型（MMLT） [[107](#bib.bib107)] 通过重新检测和记忆管理部分修改了
    SiamFC 跟踪器 [[58](#bib.bib58)]。此外，改进的 Siamese 跟踪器（i-Siam） [[215](#bib.bib215)]
    通过负信号抑制方法和多样化的多模板方法重新审视了 SiamFC 跟踪器。多层 CF 基跟踪器 [[204](#bib.bib204)] 使用定向重新检测技术，而
    MetaUpdater [[217](#bib.bib217)] 利用基于 SiamRPN 的重新检测器和带有元更新器的在线验证器。最后，SPLT 跟踪器
    [[218](#bib.bib218)] 基于 SiamRPN [[116](#bib.bib116)]，包括局部跟踪和搜索窗口选择的阅读和浏览模块。
- en: On the other hand, various long-term trackers have been inspired by successful
    detection methods. For instance, GlobalTrack [[214](#bib.bib214)] is a two-stage
    tracker, including a query-guided region proposal network & query-guided region
    CNN to generate object candidates and produce the final predictions, respectively.
    Also, the LRVN tracker [[216](#bib.bib216)] consists of the combination of an
    offline-learned regression network with an online-updated verification network
    to generate target candidates & evaluate/update them on reliable observations.
    Lastly, the SiamRCNN tracker [[200](#bib.bib200)] introduces a hard example mining
    procedure and tracklet dynamic programming algorithm to detect potential distractors
    & select the best target at each time-step.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，各种长期跟踪器受到成功检测方法的启发。例如，GlobalTrack [[214](#bib.bib214)] 是一个两阶段的跟踪器，包括一个查询引导的区域提议网络和一个查询引导的区域
    CNN 来生成目标候选并分别产生最终预测。此外，LRVN 跟踪器 [[216](#bib.bib216)] 由一个离线学习的回归网络和一个在线更新的验证网络的组合组成，用于生成目标候选并在可靠观测下进行评估/更新。最后，SiamRCNN
    跟踪器 [[200](#bib.bib200)] 引入了一个困难样本挖掘程序和轨迹动态规划算法，以检测潜在干扰物并在每个时间步骤选择最佳目标。
- en: II-I Online Tracking
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-I 在线跟踪
- en: 'While initial DL-based methods had focused on their performance, recent trackers
    aim to be accurate, robust, and efficient simultaneously. According to Fig. [3](#S2.F3
    "Figure 3 ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey"), a wide variety of algorithms are classified as online
    trackers regarding different hardware implementations (Table [I](#S2.T1 "TABLE
    I ‣ II-B1 Model Reuse or Deep Off-the-Shelf Features ‣ II-B Network Exploitation
    ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")-Table [IV](#S2.T4 "TABLE IV ‣ II-C2 Only Offline Training ‣ II-C Network
    Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")). Most of these trackers (e.g., [[59](#bib.bib59), [57](#bib.bib57),
    [58](#bib.bib58), [116](#bib.bib116), [156](#bib.bib156)]) are based on offline-trained
    SNNs that do not update the target model (i.e., initial frame) during tracking.
    Some deep DCF-based trackers (e.g., [[148](#bib.bib148), [115](#bib.bib115), [163](#bib.bib163),
    [205](#bib.bib205)]) exploit efficient optimizations & computations in the frequency
    domain, although employing pre-trained networks limits their speeds. Lately, custom-based
    trackers (e.g., [[149](#bib.bib149), [159](#bib.bib159), [194](#bib.bib194), [212](#bib.bib212),
    [192](#bib.bib192), [201](#bib.bib201)]) employ shallow networks with robust optimization
    strategies to attain high-speed tracking. Finally, numerous techniques have been
    used to speed up DL-based trackers (e.g., learning-based search strategy [[103](#bib.bib103)],
    domain adaption [[207](#bib.bib207), [201](#bib.bib201)], embedding space learning
    [[105](#bib.bib105)], collaborative framework [[131](#bib.bib131)], offline-trained
    CNN [[135](#bib.bib135)], and efficient updating & scale estimation strategies
    [[73](#bib.bib73)]).'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管最初基于深度学习的方法集中在其性能上，但最近的跟踪器旨在同时实现准确、鲁棒和高效。根据图 [3](#S2.F3 "Figure 3 ‣ II Deep
    Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")，各种算法根据不同的硬件实现被分类为在线跟踪器（表 [I](#S2.T1 "TABLE I ‣ II-B1 Model Reuse or Deep
    Off-the-Shelf Features ‣ II-B Network Exploitation ‣ II Deep Visual Tracking Taxonomy
    ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")-表 [IV](#S2.T4 "TABLE
    IV ‣ II-C2 Only Offline Training ‣ II-C Network Training ‣ II Deep Visual Tracking
    Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")）。这些跟踪器中的大多数（例如，[[59](#bib.bib59)、[57](#bib.bib57)、[58](#bib.bib58)、[116](#bib.bib116)、[156](#bib.bib156)
    ]）基于离线训练的SNN，这些SNN在跟踪过程中不会更新目标模型（即初始帧）。一些基于深度DCF的跟踪器（例如，[[148](#bib.bib148)、[115](#bib.bib115)、[163](#bib.bib163)、[205](#bib.bib205)
    ]）在频域中利用高效的优化和计算，尽管使用预训练网络会限制它们的速度。最近，定制化的跟踪器（例如，[[149](#bib.bib149)、[159](#bib.bib159)、[194](#bib.bib194)、[212](#bib.bib212)、[192](#bib.bib192)、[201](#bib.bib201)
    ]）采用浅层网络和鲁棒的优化策略，以实现高速跟踪。最后，许多技术已被用于加速基于深度学习的跟踪器（例如，基于学习的搜索策略 [[103](#bib.bib103)]，领域适应
    [[207](#bib.bib207)、[201](#bib.bib201)]，嵌入空间学习 [[105](#bib.bib105)]，协作框架 [[131](#bib.bib131)]，离线训练的CNN
    [[135](#bib.bib135)]，以及高效更新和尺度估计策略 [[73](#bib.bib73)]）。'
- en: III Visual Tracking Benchmark Datasets
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 可视跟踪基准数据集
- en: 'Visual tracking benchmark datasets have been introduced to provide fair and
    standardized evaluations of single-object tracking algorithms. These benchmarks
    are mainly categorized based on generic or aerial-view tracking applications while
    providing short- or long-term scenarios. These datasets contain various sequences,
    frames, attributes, and classes (or clusters). The attributes include illumination
    variation (IV), scale variation (SV), occlusion (OCC), deformation (DEF), motion
    blur (MB), fast motion (FM), in-plane rotation (IPR), out-of-plane rotation (OPR),
    out-of-view (OV), background clutter (BC), low resolution (LR), aspect ratio change
    (ARC), camera motion (CM), full occlusion (FOC), partial occlusion (POC), similar
    object (SIB), viewpoint change (VC), light (LI), surface cover (SC), specularity
    (SP), transparency (TR), shape (SH), motion smoothness (MS), motion coherence
    (MCO), confusion (CON), low contrast (LC), zooming camera (ZC), long duration
    (LD), shadow change (SHC), flash (FL), dim light (DL), camera shaking (CS), rotation
    (ROT), fast background change (FBC), motion change (MOC), object color change
    (OCO), scene complexity (SCO), absolute motion (AM), size (SZ), relative speed
    (RS), distractors (DI), length (LE), fast camera motion (FCM), object motion (OM),
    object blur (OB), large occlusion (LOC), small objects (SOB), occlusion with background
    clutter (O-B), occlusion with rotation (O-R) and long-term tracking (LT). Table [VI](#S3.T6
    "TABLE VI ‣ III Visual Tracking Benchmark Datasets ‣ Deep Learning for Visual
    Tracking: A Comprehensive Survey") compares the applications, scenarios, characteristics,
    missing labeled data for unsupervised training, and the overlap of single object
    tracking datasets. By different evaluation protocols, existing visual tracking
    benchmarks assess the accuracy & robustness of trackers in realistic scenarios.
    The homogenized evaluation protocols facilitate straightforward comparison and
    development of visual trackers. Below the most popular visual tracking benchmark
    datasets and evaluation metrics are briefly described.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '视觉跟踪基准数据集的引入旨在提供对单目标跟踪算法的公平和标准化评估。这些基准数据集主要根据通用或空中视角跟踪应用进行分类，同时提供短期或长期场景。这些数据集包含各种序列、帧、属性和类别（或集群）。属性包括光照变化（IV）、尺度变化（SV）、遮挡（OCC）、变形（DEF）、运动模糊（MB）、快速运动（FM）、平面内旋转（IPR）、平面外旋转（OPR）、视野外（OV）、背景杂乱（BC）、低分辨率（LR）、长宽比变化（ARC）、相机运动（CM）、完全遮挡（FOC）、部分遮挡（POC）、相似物体（SIB）、视点变化（VC）、光线（LI）、表面覆盖（SC）、高光（SP）、透明度（TR）、形状（SH）、运动平滑度（MS）、运动一致性（MCO）、混淆（CON）、低对比度（LC）、变焦相机（ZC）、长时间（LD）、阴影变化（SHC）、闪光（FL）、弱光（DL）、相机抖动（CS）、旋转（ROT）、快速背景变化（FBC）、运动变化（MOC）、物体颜色变化（OCO）、场景复杂性（SCO）、绝对运动（AM）、大小（SZ）、相对速度（RS）、干扰物（DI）、长度（LE）、快速相机运动（FCM）、物体运动（OM）、物体模糊（OB）、大遮挡（LOC）、小物体（SOB）、背景杂乱下的遮挡（O-B）、旋转下的遮挡（O-R）和长期跟踪（LT）。表[VI](#S3.T6
    "TABLE VI ‣ III Visual Tracking Benchmark Datasets ‣ Deep Learning for Visual
    Tracking: A Comprehensive Survey")比较了单目标跟踪数据集的应用、场景、特征、未标记数据的缺失以及重叠情况。通过不同的评估协议，现有的视觉跟踪基准评估了跟踪器在现实场景中的准确性和鲁棒性。统一的评估协议促进了视觉跟踪器的直接比较和发展。以下是最受欢迎的视觉跟踪基准数据集和评估指标的简要描述。'
- en: 'TABLE VI: Comparison of visual tracking datasets. The abbreviations are denoted
    as RN: row number, NoV: number of videos, NoF: number of frames, NoA: number of
    attributes, OD: overlapped datasets, AL: absent labels, NoC: number of classes
    or clusters, AD: average duration (s: seconds).'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：视觉跟踪数据集的比较。缩写表示如下：RN：行号，NoV：视频数量，NoF：帧数量，NoA：属性数量，OD：重叠数据集，AL：缺失标签，NoC：类别或集群数量，AD：平均时长（s：秒）。
- en: '| Year | Application | Dataset | Scenario | NoV | NoF | NoA | OD | AL | NoC
    | AD | Attributes |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 应用 | 数据集 | 场景 | NoV | NoF | NoA | OD | AL | NoC | AD | 属性 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2013 | Generic | OTB2013 | ST | 51 | 29K | 11 | VOT, OTB2015, TC128 | No
    | 10 | 19.4s | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 2013 | 一般 | OTB2013 | ST | 51 | 29K | 11 | VOT, OTB2015, TC128 | 无 | 10 |
    19.4s | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
- en: '| 2013-2019 | Generic | VOT2013-2019 | ST | 16-60 | 6K-21K | 12 | OTB, ALOV$++$,
    TC128, UAV123, NUS-PRO | No | 12-24 | 12s | IV, SV, OCC, DEF, MB, BC, ARC, CM,
    MOC, OCO, SCO, AM |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 2013-2019 | 一般 | VOT2013-2019 | ST | 16-60 | 6K-21K | 12 | OTB, ALOV$++$,
    TC128, UAV123, NUS-PRO | 无 | 12-24 | 12s | IV, SV, OCC, DEF, MB, BC, ARC, CM,
    MOC, OCO, SCO, AM |'
- en: '| 2014 | Generic | ALOV$++$ | ST | 314 | 89K | 14 | VOT, YouTube | No | 64
    | 16.2s | OCC, BC, CM, LI, SC, SP, TR, SH, MS, MCO, CON, LC, ZC, LD |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 通用 | ALOV$++$ | ST | 314 | 89K | 14 | VOT, YouTube | 无 | 64 | 16.2秒
    | OCC, BC, CM, LI, SC, SP, TR, SH, MS, MCO, CON, LC, ZC, LD |'
- en: '| 2015 | Generic | OTB2015 | ST | 100 | 59K | 11 | OTB2013, VOT, TC128 | No
    | 16 | 19.8s | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 通用 | OTB2015 | ST | 100 | 59K | 11 | OTB2013, VOT, TC128 | 无 | 16
    | 19.8秒 | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
- en: '| 2015 | Generic | TC128 | ST | 129 | 55K | 11 | OTB, VOT | No | 27 | 15.6s
    | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 通用 | TC128 | ST | 129 | 55K | 11 | OTB, VOT | 无 | 27 | 15.6秒 | IV,
    SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
- en: '| 2016 | UAV | UAV123 | ST | 123 | 113K | 12 | VOT | No | 9 | 30.6s | IV, SV,
    FM, OV, BC, LR, ARC, CM, FCM, FOC, POC, SIB, VC |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | UAV | UAV123 | ST | 123 | 113K | 12 | VOT | 无 | 9 | 30.6秒 | IV, SV,
    FM, OV, BC, LR, ARC, CM, FCM, FOC, POC, SIB, VC |'
- en: '| 2016 | UAV | UAV20L | LT | 20 | 59K | 12 | VOT | No | 5 | 75s | IV, SV, FM,
    OV, BC, LR, ARC, CM, FCM, FOC, POC, SIB, VC |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | UAV | UAV20L | LT | 20 | 59K | 12 | VOT | 无 | 5 | 75秒 | IV, SV, FM,
    OV, BC, LR, ARC, CM, FCM, FOC, POC, SIB, VC |'
- en: '| 2016 | Generic | NUS-PRO | ST | 365 | 135K | 12 | VOT, YouTube | No | 17
    | 12.6s | SV, DEF, BC, FOC, POC, SIB, SHC, FL, DL, CS, ROT, FBC |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | 通用 | NUS-PRO | ST | 365 | 135K | 12 | VOT, YouTube | 无 | 17 | 12.6秒
    | SV, DEF, BC, FOC, POC, SIB, SHC, FL, DL, CS, ROT, FBC |'
- en: '| 2017 | Generic | NfS | ST | 100 | 383K | 9 | YouTube | No | 17 | 15.6s |
    IV, SV, OCC, DEF, FM, OV, BC, LR, VC |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | 通用 | NfS | ST | 100 | 383K | 9 | YouTube | 无 | 17 | 15.6秒 | IV, SV,
    OCC, DEF, FM, OV, BC, LR, VC |'
- en: '| 2017 | UAV | DTB | ST | 70 | 15K | 11 | YouTube | No | 15 | 7.2s | SV, OCC,
    DEF, MB, IPR, OPR, OV, BC, ARC, FCM, SIB |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | UAV | DTB | ST | 70 | 15K | 11 | YouTube | 无 | 15 | 7.2秒 | SV, OCC,
    DEF, MB, IPR, OPR, OV, BC, ARC, FCM, SIB |'
- en: '| 2018 | Generic | TrackingNet | ST | 30643 | 14.43M | 15 | YTBB | No | 27
    | 16.6s | IV, SV, DEF, MB, FM, IPR, OPR, OV, BC, LR, ARC, CM, FOC, POC, SIB |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 通用 | TrackingNet | ST | 30643 | 14.43M | 15 | YTBB | 无 | 27 | 16.6秒
    | IV, SV, DEF, MB, FM, IPR, OPR, OV, BC, LR, ARC, CM, FOC, POC, SIB |'
- en: '| 2018 | Generic | TinyTLP / TLPattr | ST | 50 | 30K | 6 | YouTube | No | N/A
    | 20s | FM, IV, SV, POC, OV, BC |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 通用 | TinyTLP / TLPattr | ST | 50 | 30K | 6 | YouTube | 无 | 不适用 | 20秒
    | FM, IV, SV, POC, OV, BC |'
- en: '| 2018 | Generic | OxUvA | LT | 366 | 1.55M | 6 | YTBB | Yes | 22 | 144s |
    SV, OV, SZ, RS, DI, LE |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 通用 | OxUvA | LT | 366 | 1.55M | 6 | YTBB | 是 | 22 | 144秒 | SV, OV,
    SZ, RS, DI, LE |'
- en: '| 2018 | Generic | TLP | LT | 50 | 676K | 0 | YouTube | No | N/A | 484.8s |
    - |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 通用 | TLP | LT | 50 | 676K | 0 | YouTube | 无 | 不适用 | 484.8秒 | - |'
- en: '| 2018 | UAV | BUAA-PRO | ST | 150 | 8.7K | 12 | NUS-PRO, YTBB | No | 12 |
    2s | SV, DEF, BC, FOC, POC, SIB, SHC, FL, DL, CS, ROT, FBC |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | UAV | BUAA-PRO | ST | 150 | 8.7K | 12 | NUS-PRO, YTBB | 无 | 12 | 2秒
    | SV, DEF, BC, FOC, POC, SIB, SHC, FL, DL, CS, ROT, FBC |'
- en: '| 2018 | Generic | GOT10k | ST | 10000 | 1.5M | 6 | VOT, WordNet, ImageNet
    | Yes | 563 | 16s | IV, SV, OCC, FM, ARC, LO |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 通用 | GOT10k | ST | 10000 | 1.5M | 6 | VOT, WordNet, ImageNet | 是 |
    563 | 16秒 | IV, SV, OCC, FM, ARC, LO |'
- en: '| 2018 | UAV | UAVDT | ST | 50 | 80K | 9 | - | No | 3 | N/A | BC, CM, OM, SOB,
    IV, OB, SV, LOC, LT |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | UAV | UAVDT | ST | 50 | 80K | 9 | - | 无 | 3 | 不适用 | BC, CM, OM, SOB,
    IV, OB, SV, LOC, LT |'
- en: '| 2018 | Generic | LTB35 / VOT2018-LT | LT | 35 | 147K | 10 | YouTube, VOT,
    UAV20L | No | 6 | N/A | FOC, POC, OV, CM, FM, SC, ARC, VC, SIB, DEF |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | 通用 | LTB35 / VOT2018-LT | LT | 35 | 147K | 10 | YouTube, VOT, UAV20L
    | 无 | 6 | 不适用 | FOC, POC, OV, CM, FM, SC, ARC, VC, SIB, DEF |'
- en: '| 2018-2020 | UAV | VisDrone2018-2020 | ST | 132 | 106.4K | 12 | - | No | 4
    | N/A | IV, SV, FM, OV, BC, LR, ARC, CM, FOC, POC, SIB, VC |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 2018-2020 | UAV | VisDrone2018-2020 | ST | 132 | 106.4K | 12 | - | 无 | 4
    | 不适用 | IV, SV, FM, OV, BC, LR, ARC, CM, FOC, POC, SIB, VC |'
- en: '| 2019-2020 | UAV | VisDrone2019-2020L | LT | 25 | 82.6K | 12 | - | No | 4
    | N/A | IV, SV, FM, OV, BC, LR, ARC, CM, FOC, POC, SIB, VC |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 2019-2020 | UAV | VisDrone2019-2020L | LT | 25 | 82.6K | 12 | - | 无 | 4 |
    不适用 | IV, SV, FM, OV, BC, LR, ARC, CM, FOC, POC, SIB, VC |'
- en: '| 2019 | Generic | LaSOT | LT | 1400 | 3.5M | 14 | YouTube, ImageNet | Yes
    | 70 | 84.3s | IV, SV, DEF, MB, FM, OV, BC, LR, ARC, CM, FOC, POC, VC, ROT |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | 通用 | LaSOT | LT | 1400 | 3.5M | 14 | YouTube, ImageNet | 是 | 70 |
    84.3秒 | IV, SV, DEF, MB, FM, OV, BC, LR, ARC, CM, FOC, POC, VC, ROT |'
- en: '| 2020 | UAV | Small-90 | ST | 90 | N/A | 11 | UAV123, VOT, OTB, TC128 | No
    | N/A | N/A | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | UAV | Small-90 | ST | 90 | 不适用 | 11 | UAV123, VOT, OTB, TC128 | 无
    | 不适用 | 不适用 | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
- en: '| 2020 | UAV | Small-112 | ST | 112 | N/A | 11 | UAV123, VOT, OTB, TC128, VisDrone
    | No | N/A | N/A | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | UAV | Small-112 | ST | 112 | 不适用 | 11 | UAV123, VOT, OTB, TC128, VisDrone
    | 无 | 不适用 | 不适用 | IV, SV, OCC, DEF, MB, FM, IPR, OPR, OV, BC, LR |'
- en: '| 2021 | Generic | TracKlinic | ST | 2390 | 280K | 9 | OTB, TC128, LaSOT |
    No | N/A | N/A | IV, SV, OCC, MB, OV, BC, ROT, O-B, O-R |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | 通用 | TracKlinic | ST | 2390 | 280K | 9 | OTB, TC128, LaSOT | 无 | 不适用
    | 不适用 | IV, SV, OCC, MB, OV, BC, ROT, O-B, O-R |'
- en: III-A Short-term Tracking Datasets
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 短期跟踪数据集
- en: 'Generic Object Tracking. As one of the first object tracking benchmarks, OTB2013
    [[219](#bib.bib219)] is developed by fully annotated video sequences to address
    the issues of reported tracking results based on a few video sequences or inconsistent
    initial conditions or parameters. The OTB2015 [[220](#bib.bib220)] is an extended
    OTB2013 dataset with the aim of unbiased performance comparisons. To compare the
    performance of visual trackers on color sequences, the Temple-Color 128 (TColor128
    or TC128) [[225](#bib.bib225)] collected a set of 129 fully annotated video sequences
    that 78 ones are different from the OTB datasets. The Amsterdam library of ordinary
    videos (ALOV) dataset [[42](#bib.bib42)] has been gathered to cover diverse video
    sequences and attributes. By emphasizing challenging visual tracking scenarios,
    the ALOV dataset comprises 304 assorted short videos and 10 longer ones. The video
    sequences are chosen from real-life YouTube videos and have 13 difficulty degrees.
    The videos of ALOV have been categorized according to one of its attributes (Table [VI](#S3.T6
    "TABLE VI ‣ III Visual Tracking Benchmark Datasets ‣ Deep Learning for Visual
    Tracking: A Comprehensive Survey")), although in the OTB datasets, each video
    has been annotated by several visual attributes.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '通用对象跟踪。作为最早的对象跟踪基准之一，OTB2013 [[219](#bib.bib219)] 通过完全注释的视频序列开发，以解决基于少量视频序列或不一致的初始条件或参数的报告跟踪结果的问题。OTB2015
    [[220](#bib.bib220)] 是一个扩展的 OTB2013 数据集，旨在进行无偏性能比较。为了比较视觉跟踪器在彩色视频序列上的表现，Temple-Color
    128（TColor128 或 TC128） [[225](#bib.bib225)] 收集了一组 129 个完全注释的视频序列，其中 78 个与 OTB
    数据集不同。阿姆斯特丹普通视频库（ALOV）数据集 [[42](#bib.bib42)] 已被收集以覆盖多样的视频序列和属性。通过强调具有挑战性的视觉跟踪场景，ALOV
    数据集包括 304 个各种短视频和 10 个较长视频。这些视频序列从现实生活中的 YouTube 视频中选择，并具有 13 个难度级别。ALOV 的视频根据其属性进行分类（表 [VI](#S3.T6
    "TABLE VI ‣ III Visual Tracking Benchmark Datasets ‣ Deep Learning for Visual
    Tracking: A Comprehensive Survey")），尽管在 OTB 数据集中，每个视频都被注释了多个视觉属性。'
- en: Motivated by the large dataset’s inequality with a useful one, the VOT dataset
    [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40)] aims to provide a diverse and sufficiently
    small dataset from existing ones, annotated per-frame by rotatable BBs and visual
    properties. To evaluate different visual tracking methods fast and straightforward,
    the VOT includes a visual tracking exchange (TraX) protocol [[253](#bib.bib253)]
    that not only prepares data, runs experiments, and performs analyses but also
    can detect tracking failures (i.e., losing the target) and re-initialize the tracker
    after each failure to assess tracking robustness. Despite some small and saturated
    tracking datasets in the wild, mostly provided for object detection tasks, the
    large-scale TrackingNet benchmark dataset [[229](#bib.bib229)] has been proposed
    to properly feed deep visual trackers. It provides videos for tracking in the
    wild with 500 original videos, more than 14 million upright BB annotations, densely
    annotated data in time, rich distribution of object classes, and real-world scenarios
    by sampled YouTube videos.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大型数据集与有用数据集之间的不平等，VOT 数据集 [[34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)] 旨在提供一个多样且足够小的数据集，该数据集从现有数据集中提取，并通过可旋转的边界框和视觉属性进行逐帧注释。为了快速而直接地评估不同的视觉跟踪方法，VOT
    包括一个视觉跟踪交换（TraX）协议 [[253](#bib.bib253)]，该协议不仅准备数据、运行实验和执行分析，还可以检测跟踪失败（即丢失目标）并在每次失败后重新初始化跟踪器，以评估跟踪的鲁棒性。尽管在实际应用中存在一些小型和饱和的跟踪数据集，主要用于目标检测任务，但已提出了大规模的
    TrackingNet 基准数据集 [[229](#bib.bib229)]，以便为深度视觉跟踪器提供适当的数据。它提供了 500 个原始视频用于在实际环境中进行跟踪，包含超过
    1400 万个直立边界框注释，时间上密集注释的数据，丰富的对象类别分布，以及通过 YouTube 视频采样的现实场景。
- en: For tracking pedestrian and rigid objects, the NUS people and rigid objects
    (NUS-PRO) dataset [[226](#bib.bib226)] has been provided 365 video sequences from
    YouTube that are majorly captured by moving cameras and annotated the level of
    occluded objects of each frame with no occlusion, partial occlusion, and full
    occlusion labels. By higher frame rate (240 FPS) cameras, the need for speed (NfS)
    dataset [[227](#bib.bib227)] provides video sequences from real-world scenarios
    to systematically investigate trade-off bandwidth constraints related to real-time
    analysis of visual trackers. These videos are either recorded by hand-held iPhone/iPad
    cameras or from YouTube videos. Two short sequence datasets, namely TinyTLP &
    TLPattr [[233](#bib.bib233)], are derived from the long-term TLP dataset. For
    each sequence, one visual attribute has been specified for investigating various
    challenges. The large high-diversity dataset, called GOT-10k [[232](#bib.bib232)],
    includes more than ten thousand videos classified into 563 classes of moving objects
    and 87 classes of motion to cover as many challenging patterns in real-world scenarios
    as possible. The GOT-10k has informative continuous attributes, including absent
    labels, which show the target does not exist in the frame. Finally, the TracKlinic
    [[234](#bib.bib234)] introduces a toolkit (collected from OTB2015, TC128, & LaSOT)
    that consists of just one challenging factor per sequence to evaluate visual trackers.
    It also provides two challenging O-B & O-R attributes, including occlusion with
    background clutter & rotation.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 用于跟踪行人和刚体物体的NUS人和刚体物体（NUS-PRO）数据集[[226](#bib.bib226)]提供了来自YouTube的365个视频序列，这些视频主要由移动摄像机拍摄，并标注了每帧的遮挡物体的遮挡级别，包括无遮挡、部分遮挡和完全遮挡标签。通过更高帧率（240
    FPS）摄像机，速度需求（NfS）数据集[[227](#bib.bib227)]提供了来自现实世界场景的视频序列，以系统地研究与实时视觉跟踪分析相关的带宽约束。这些视频要么由手持iPhone/iPad摄像机录制，要么来自YouTube视频。两个短序列数据集，即TinyTLP和TLPattr[[233](#bib.bib233)]，源自长期TLP数据集。对于每个序列，指定了一个视觉属性来研究各种挑战。名为GOT-10k[[232](#bib.bib232)]的大型高多样性数据集包括超过一万段视频，分为563个移动物体类别和87个动作类别，以尽可能覆盖现实场景中的各种挑战模式。GOT-10k具有信息丰富的连续属性，包括缺失标签，显示目标在帧中不存在。最后，TracKlinic[[234](#bib.bib234)]引入了一个工具包（来自OTB2015、TC128和LaSOT），每个序列只包含一个挑战因素，以评估视觉跟踪器。它还提供了两个具有挑战性的O-B和O-R属性，包括背景杂乱的遮挡和旋转。
- en: Aerial View Object Tracking. Tracking from aerial views has been developed in
    recent years, considering the broad range of applications. The unmanned aerial
    vehicle 123 (UAV123) [[222](#bib.bib222)] provides a sparse and low altitude aerial-view
    tracking dataset that contains the realistic and synthetic HD video sequences
    captured by professional-grade flying robots, a board-cam mounted on small low-cost
    flying robots & simulator ones. Drone tracking benchmark (DTB) [[228](#bib.bib228)]
    is a dataset captured by flying robots or drones that consists of RGB videos with
    massive displacement of target location due to abrupt camera motion. The BUAA-PRO
    dataset [[231](#bib.bib231)] is a segmentation-based benchmark dataset to address
    the problem of inevitable non-target elements in BBs. It exploits the segmentation
    mask-based version of a level-based occlusion attribute. The UAVDT dataset [[223](#bib.bib223)]
    provides an aerial-view dataset with high object density scenarios (e.g., different
    weather conditions, camera views, flying altitudes) focusing on pedestrians and
    vehicles. Furthermore, VisDrone dataset [[236](#bib.bib236), [224](#bib.bib224)]
    includes videos captured by different drone platforms in real-world scenarios.
    For small object tracking, the Small-90 dataset [[237](#bib.bib237)] presents
    aerial videos mostly collected from other visual tracking datasets. By adding
    22 more challenging sequences, the Small-112 dataset [[237](#bib.bib237)] has
    been formed based on the Small-90 dataset.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 空中视角目标跟踪。近年来，考虑到广泛的应用范围，空中视角的跟踪技术得到了发展。无人机123（UAV123）[[222](#bib.bib222)] 提供了一个稀疏且低高度的空中视角跟踪数据集，该数据集包含由专业级飞行机器人、安装在小型低成本飞行机器人上的板载摄像头以及模拟器拍摄的真实和合成高清（HD）视频序列。无人机跟踪基准（DTB）[[228](#bib.bib228)]
    是一个由飞行机器人或无人机拍摄的数据集，包含RGB视频，目标位置由于突发的相机运动而发生大幅度位移。BUAA-PRO数据集[[231](#bib.bib231)]
    是一个基于分割的基准数据集，用于解决BBs中不可避免的非目标元素问题。它利用了基于分割掩模的级别遮挡属性版本。UAVDT数据集[[223](#bib.bib223)]
    提供了一个高目标密度场景的空中视角数据集（例如，不同的天气条件、相机视角、飞行高度），重点关注行人和车辆。此外，VisDrone数据集[[236](#bib.bib236),
    [224](#bib.bib224)] 包括了在真实场景中由不同无人机平台拍摄的视频。对于小目标跟踪，Small-90数据集[[237](#bib.bib237)]
    主要呈现了从其他视觉跟踪数据集中收集的空中视频。通过增加22个更具挑战性的序列，Small-112数据集[[237](#bib.bib237)] 基于Small-90数据集形成。
- en: III-B Long-term Tracking Datasets
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 长期跟踪数据集
- en: Generic Object Tracking. With the aim of long-term tracking of frequently disappearance
    targets, the OxUvA dataset [[230](#bib.bib230)] includes 14 hours of videos from
    YouTube-BoundingBoxes (or YTBB) [[244](#bib.bib244)] to provide development and
    test sets with continuous attributes. Also, it provides absent labels, which show
    that the target does not exist in some frames. The TLP dataset [[233](#bib.bib233)]
    also has been collected high-resolution videos with a longer duration per sequence
    from YouTube, which provides the possibility of studying tracking consistency.
    However, target disappearances do not frequently occur in the TLP dataset. Hence,
    the LTB-35 [[235](#bib.bib235)] presents an enriched long-term dataset with consistent
    target disappearances (twelve disappearances on average for each video). The large-scale
    single object tracking (LaSOT) [[221](#bib.bib221)] has been developed to address
    the problems of existing datasets, such as small scale, lack of high-quality,
    dense annotations, short video sequences, and category bias. The object categories
    are from the ImageNet and a few visual tracking applications (such as drones)
    with an equal number of videos per category. The training and testing subsets
    include 1120 (2.3M frames) and 280 (690K frames) video sequences, respectively.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 通用目标跟踪。为了长期跟踪经常消失的目标，OxUvA数据集[[230](#bib.bib230)] 包括了来自YouTube-BoundingBoxes（或YTBB）[[244](#bib.bib244)]
    的14小时视频，以提供具有连续属性的发展和测试集。此外，它还提供了缺失标签，显示目标在某些帧中不存在。TLP数据集[[233](#bib.bib233)]
    也从YouTube收集了高分辨率的长时间视频，为研究跟踪一致性提供了可能。然而，TLP数据集中目标的消失并不频繁。因此，LTB-35[[235](#bib.bib235)]
    提供了一个丰富的长期数据集，具有一致的目标消失（每个视频平均十二次消失）。大规模单目标跟踪（LaSOT）[[221](#bib.bib221)] 已经被开发出来以解决现有数据集的问题，如规模小、缺乏高质量的密集注释、视频序列短以及类别偏差。目标类别来自ImageNet和少数视觉跟踪应用（如无人机），每个类别的视频数量相等。训练和测试子集分别包括1120个（2.3M帧）和280个（690K帧）视频序列。
- en: Aerial View Object Tracking. As the parent set of the UAV-123 dataset, the UAV20L
    is an aerial surveillance dataset, including one continuous shot video sequences.
    It consists of tolerable occlusions and provides difficult scenarios for small
    object tracking. Moreover, the VisDrone-2019/2020L dataset [[224](#bib.bib224)]
    includes 25 challenging sequences (i.e., 12/13 videos in the day/night) with tiny
    targets.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 航拍目标跟踪。作为 UAV-123 数据集的母集，UAV20L 是一个航拍监控数据集，包括一个连续拍摄的视频序列。它包含可接受的遮挡，并为小物体跟踪提供了困难的场景。此外，VisDrone-2019/2020L
    数据集 [[224](#bib.bib224)] 包括 25 个具有挑战性的序列（即，12/13 个白天/夜晚视频）及微小目标。
- en: III-C Evaluation Metrics
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 评估指标
- en: Visual trackers are evaluated by two fundamental evaluation categories of performance
    measures and performance plots to perform experimental comparisons on large-scale
    datasets. These metrics are briefly described as follows.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉跟踪器通过性能指标和性能图两大基本评估类别进行评估，以在大规模数据集上进行实验比较。这些指标简要描述如下。
- en: III-C1 Performance Measures
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 性能指标
- en: Performance measures attempt to intuitively interpret performance comparisons
    in terms of complementary metrics of accuracy, robustness and tracking speed.
    For long-term trackers, the measures close relate to their detection counterparts
    to reflect re-detection & target absence prediction capabilities. In the following,
    these measures are concisely investigated.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标试图直观地解释性能比较，通过准确性、鲁棒性和跟踪速度的互补指标。对于长期跟踪器，这些指标与检测对手密切相关，以反映重新检测和目标缺失预测能力。以下将简要探讨这些指标。
- en: (A)
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (A)
- en: 'Short-term Tracking Measures:'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 短期跟踪指标：
- en: (i)
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (i)
- en: 'Center location error (CLE)/ (Normalized) precision: The CLE or precision metric
    is defined as the average Euclidean distance between the precise ground-truth
    locations of the target and estimated locations by a visual tracker. The CLE is
    the oldest metric that is sensitive to dataset annotation and does not consider
    tracking failures, and ignores the target’s BB, resulting in significant errors.
    The normalized precision [[229](#bib.bib229)] aims to relieve the sensitivity
    of CLE to the size of BBs and frame resolutions. Given the size of ground-truth
    BB ($b_{g}$), this metric normalizes the CLE over $b_{g}$ to keep its consistency
    for various target scales.'
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 中心位置误差 (CLE) / (标准化) 精度：CLE 或精度指标定义为目标的精确真实位置与视觉跟踪器估计位置之间的平均欧氏距离。CLE 是最古老的指标，对数据集注释非常敏感，不考虑跟踪失败，并忽略目标的
    BB，导致显著错误。标准化精度 [[229](#bib.bib229)] 旨在缓解 CLE 对 BB 大小和帧分辨率的敏感性。考虑到真实 BB 的大小 ($b_{g}$)，该指标通过
    $b_{g}$ 标准化 CLE，以保持其在不同目标尺度上的一致性。
- en: (ii)
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (ii)
- en: 'Accuracy: For this metric, first, the overlap score is calculated as $S=\frac{\left|b_{t}\cap
    b_{g}\right|}{\left|b_{t}\cup b_{g}\right|}$ which b[g], b[t], $\cap$, $\cup$
    and $\left|.\right|$ represent the ground-truth BB, an estimated BB by a visual
    tracking method, intersection operator, union operator, and the number of pixels
    in the resulted region, respectively. By considering a certain threshold, the
    overlap score indicates a visual tracker’s success in one frame. The accuracy
    is then calculated by the average overlap scores (AOS) during the tracking when
    a visual tracker’s estimations have overlap with the ground-truth ones. This metric
    jointly considers both location and region to measure the estimated target’s drift
    rate up to its failure.'
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确性：对于该指标，首先计算重叠评分 $S=\frac{\left|b_{t}\cap b_{g}\right|}{\left|b_{t}\cup b_{g}\right|}$，其中
    b[g]、b[t]、$\cap$、$\cup$ 和 $\left|.\right|$ 分别表示真实 BB、视觉跟踪方法估计的 BB、交集操作符、并集操作符和结果区域中的像素数量。通过考虑一定的阈值，重叠评分表示视觉跟踪器在一个帧中的成功。然后通过跟踪期间的平均重叠评分
    (AOS) 计算准确性，当视觉跟踪器的估计与真实值重叠时。该指标共同考虑位置和区域，以测量估计目标的漂移率直到其失败。
- en: (iii)
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (iii)
- en: 'Robustness/ failure score: The robustness or failure score is defined as the
    number of required re-initializations when a tracker loses (or drifts) the target
    during the tracking task. The failure is detected when the overlap score drops
    to zero.'
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鲁棒性 / 失败评分：鲁棒性或失败评分定义为跟踪器在跟踪任务中丢失（或漂移）目标时所需的重新初始化次数。当重叠评分降到零时，检测到失败。
- en: (iv)
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (iv)
- en: 'Expected average overlap (EAO): This score is interpreted as the combination
    of accuracy and robustness scores. Given N[s] frames long sequences, the EAO score
    is calculated as ${\widehat{\mathit{\Phi}}}_{N_{s}}=\left\langle\frac{1}{N_{s}}\sum^{N_{s}}_{i=1}{{\mathit{\Phi}}_{i}}\right\rangle$,
    where ${\mathit{\Phi}}_{i}$ is defined as the average of per-frame overlaps until
    the end of sequences, even if failure leads to zero overlaps.'
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 期望平均重叠（EAO）：该分数被解释为准确性和鲁棒性的组合。给定N[s]帧长序列，EAO分数计算为${\widehat{\mathit{\Phi}}}_{N_{s}}=\left\langle\frac{1}{N_{s}}\sum^{N_{s}}_{i=1}{{\mathit{\Phi}}_{i}}\right\rangle$，其中${\mathit{\Phi}}_{i}$定义为直到序列结束的每帧重叠的平均值，即使失败导致重叠为零。
- en: (v)
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (v)
- en: 'Area under curve (AUC): The AUC score has defined the average success rates
    (normalized between 0 and 1) according to the pre-defined thresholds. To rank
    the visual tracking methods based on their overall performance, the AUC score
    summarizes the AOS of visual tracking methods across a sequence.'
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 曲线下面积（AUC）：AUC分数定义了根据预定义阈值的平均成功率（归一化在0和1之间）。为了根据整体性能对视觉跟踪方法进行排名，AUC分数总结了视觉跟踪方法在一个序列中的AOS。
- en: (B)
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (B)
- en: 'Long-term Tracking Measures:'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长期跟踪度量：
- en: (i)
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (i)
- en: 'Precision ($Pr$): Tracking measures for long-term trackers depend on being
    a target in the scene and prediction confidence to be higher than a classification
    threshold for each frame. The precision [[235](#bib.bib235)] is calculated by
    the intersection over union (IoU) between the ground-truth ($b_{g}$) and predicted
    target ($b_{t}$), normalized by the number of frames with existing predictions.
    The integration of these scores over all precision thresholds provides the overall
    tracking precision.'
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精度（$Pr$）：长时间跟踪器的跟踪度量依赖于在场景中作为目标以及预测置信度在每帧中高于分类阈值。精度[[235](#bib.bib235)] 通过真实值（$b_{g}$）和预测目标（$b_{t}$）之间的交并比（IoU）计算，并通过具有存在预测的帧数量进行归一化。对所有精度阈值的这些分数进行积分提供了整体跟踪精度。
- en: (ii)
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (ii)
- en: 'Recall ($Re$): Similar to the precision, it calculates the IoU between the
    $b_{g}$ and $b_{t}$, which is normalized by the number of frames with no absent
    targets. The overall tracking recall [[235](#bib.bib235)] is achieved by integrating
    the scores over all recall thresholds.'
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 召回率（$Re$）：类似于精度，它计算$b_{g}$和$b_{t}$之间的IoU，并通过没有缺失目标的帧数量进行归一化。整体跟踪召回率[[235](#bib.bib235)]
    是通过对所有召回阈值的分数进行积分来实现的。
- en: (iii)
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (iii)
- en: 'F-score: It compromises the precision & recall scores by calculating $F=\frac{2Pr.Re}{Pr+Re}$
    to rank the trackers according to their maximum values over all thresholds.'
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: F-score：它通过计算$F=\frac{2Pr.Re}{Pr+Re}$来折中精度和召回率分数，以根据所有阈值的最大值对跟踪器进行排名。
- en: (iv)
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (iv)
- en: 'Maximum Geometric Mean (MaxGM): Inspired by binary classification, the MaxGM
    employs the true positive rate (TPR) and true negative rate (TNR) for evaluation
    of trackers. While the TPR reports the fraction of correctly located targets,
    the TNR presents the fraction of correctly reported absent targets. As a single
    metric, the geometric mean is defined as $GM=\sqrt{TPR.TNR}$ but it will be zero
    for the trackers that cannot predict absent targets. Hence, $MaxGM=\max_{0\leq
    p\leq 1}\sqrt{\{(1-p).TPR)\}\{(1-p).TNR+p\}}$ provides a more informative comparison
    in terms of various probabilistic thresholds $p$.'
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最大几何均值（MaxGM）：受到二分类的启发，MaxGM使用真正率（TPR）和真负率（TNR）来评估跟踪器。虽然TPR报告正确定位目标的比例，TNR则表示正确报告缺失目标的比例。作为一个单一指标，几何均值定义为$GM=\sqrt{TPR.TNR}$，但对于无法预测缺失目标的跟踪器，几何均值将为零。因此，$MaxGM=\max_{0\leq
    p\leq 1}\sqrt{\{(1-p).TPR)\}\{(1-p).TNR+p\}}$ 提供了关于不同概率阈值$p$的更具信息性的比较。
- en: III-C2 Performance Plots
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 性能图
- en: Generally, visual trackers are analyzed in terms of various thresholds to provide
    more intuitive quantitative comparisons. These metrics are summarized as follows.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，视觉跟踪器会在不同阈值下进行分析，以提供更直观的定量比较。这些指标总结如下。
- en: (A)
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (A)
- en: 'Short-term Tracking Plots:'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 短期跟踪图：
- en: (i)
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (i)
- en: 'Precision plot: Given the CLEs per different thresholds, the precision plot
    shows the percentage of video frames in which the estimated locations have at
    most the specific threshold with the ground-truth locations.'
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精度图：给定不同阈值下的CLE，精度图显示了视频帧中估计位置与真实位置的最大特定阈值的百分比。
- en: (ii)
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (ii)
- en: 'Success plot: Given the calculated various accuracy per thresholds, the success
    plot measures the percentage of frames in which the estimated overlaps and the
    ground-truth ones have larger overlap than a certain threshold.'
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成功率图：给定每个阈值下计算出的各种准确度，成功率图测量了估计的重叠区域与真实重叠区域超过某个阈值的帧的百分比。
- en: (iii)
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (iii)
- en: 'Expected average overlap curve: For an individual length of video sequences,
    the expected average overlap curve has resulted from the range of values in a
    specific interval $\left[N_{lo},N_{hi}\right]$ as $\widehat{\mathit{\Phi}}=\frac{1}{N_{hi}-N_{lo}}\sum^{N_{hi}}_{N_{s}=N_{lo}}{{\widehat{\mathit{\Phi}}}_{N_{s}}}$.'
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 期望平均重叠曲线：对于单独长度的视频序列，期望的平均重叠曲线来源于特定区间$\left[N_{lo},N_{hi}\right]$的值范围，表示为$\widehat{\mathit{\Phi}}=\frac{1}{N_{hi}-N_{lo}}\sum^{N_{hi}}_{N_{s}=N_{lo}}{{\widehat{\mathit{\Phi}}}_{N_{s}}}$。
- en: (B)
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (B)
- en: 'Long-term Tracking Plots:'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长期跟踪图：
- en: (i)
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (i)
- en: 'Precision/ Recall plot: It is used to compare long-term tracking performances
    and analyze their detection capabilities in terms of various thresholds.'
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精度/召回率图：用于比较长期跟踪性能，并分析其在各种阈值下的检测能力。
- en: (ii)
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (ii)
- en: 'F-score plot: This is the main curve to rank the tracking methods based on
    the highest score on the plot.'
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: F-score图：这是根据图上的最高分数对跟踪方法进行排名的主要曲线。
- en: IV Experimental Analyses
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验分析
- en: 'To analyze the performance of state-of-the-art visual tracking methods, 48
    different methods are quantitatively compared on seven well-known tracking datasets,
    namely OTB2013 [[219](#bib.bib219)], OTB2015 [[220](#bib.bib220)], VOT2018 [[39](#bib.bib39)],
    LaSOT [[221](#bib.bib221)], UAV-123 [[222](#bib.bib222)], UAVDT [[223](#bib.bib223)],
    and VisDrone2019-test-dev [[224](#bib.bib224)]. Due to the page limitation, all
    experimental results are publicly available on [https://github.com/MMarvasti/Deep-Learning-for-Visual-Tracking-Survey](https://github.com/MMarvasti/Deep-Learning-for-Visual-Tracking-Survey).
    The included 48 DL-based trackers in the experiments are shown in Table [VII](#S4.T7
    "TABLE VII ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey"). All evaluations are performed on an Intel I7-9700K 3.60GHz CPU with
    32GB RAM with the aid of MatConvNet toolbox [[254](#bib.bib254)] that uses an
    NVIDIA GeForce RTX 2080Ti GPU for its computations. The OTB, LaSOT, UAV123, UAVDT,
    and VisDrone2019 toolkits evaluate the visual trackers in terms of the well-known
    precision & success plots and then rank the methods based on the AUC score. For
    performance comparison on the VOT2018 dataset, the visual trackers have been assessed
    based on the TraX evaluation protocol using three primary measures of accuracy,
    robustness, and EAO to provide the Accuracy-Robustness (AR) plots, expected average
    overlap curve, and ordering plots according to its five challenging visual attributes
    [[39](#bib.bib39)].'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '为了分析最先进的视觉跟踪方法的性能，对48种不同的方法在七个著名的跟踪数据集上进行了定量比较，这些数据集包括OTB2013 [[219](#bib.bib219)]、OTB2015
    [[220](#bib.bib220)]、VOT2018 [[39](#bib.bib39)]、LaSOT [[221](#bib.bib221)]、UAV-123
    [[222](#bib.bib222)]、UAVDT [[223](#bib.bib223)]和VisDrone2019-test-dev [[224](#bib.bib224)]。由于页面限制，所有实验结果可以在[https://github.com/MMarvasti/Deep-Learning-for-Visual-Tracking-Survey](https://github.com/MMarvasti/Deep-Learning-for-Visual-Tracking-Survey)上公开获取。实验中包含的48种基于DL的跟踪器如表[VII](#S4.T7
    "TABLE VII ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")所示。所有评估均在配备32GB RAM的Intel I7-9700K 3.60GHz CPU上使用MatConvNet工具箱[[254](#bib.bib254)]，并通过NVIDIA
    GeForce RTX 2080Ti GPU进行计算。OTB、LaSOT、UAV123、UAVDT和VisDrone2019工具包评估视觉跟踪器的知名精度与成功率图，然后根据AUC分数对方法进行排名。为了在VOT2018数据集上进行性能比较，视觉跟踪器已根据TraX评估协议进行评估，使用准确性、鲁棒性和EAO这三项主要指标提供Accuracy-Robustness
    (AR)图、期望的平均重叠曲线以及根据五个挑战性的视觉属性的排序图[[39](#bib.bib39)]。'
- en: 'TABLE VII: State-of-the-art visual tracking methods for experimental comparisons
    on visual tracking datasets.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：最先进的视觉跟踪方法在视觉跟踪数据集上的实验比较。
- en: '| Published in | Visual Tracking Method | Exploited Features | Test Datasets
    |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 发表时间 | 视觉跟踪方法 | 利用的特征 | 测试数据集 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ICCV 2015 | HCFT [[51](#bib.bib51)] | DAF | OTB, LaSOT |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2015 | HCFT [[51](#bib.bib51)] | DAF | OTB, LaSOT |'
- en: '| ICCV 2015 | DeepSRDCF [[52](#bib.bib52)] | DAF, HOG | OTB, VOT2018 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2015 | DeepSRDCF [[52](#bib.bib52)] | DAF, HOG | OTB, VOT2018 |'
- en: '| ECCV 2016 | CCOT [[56](#bib.bib56)] | DAF | OTB, VOT2018, UAVDT |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2016 | CCOT [[56](#bib.bib56)] | DAF | OTB, VOT2018, UAVDT |'
- en: '| ECCVW 2016 | SiamFC [[58](#bib.bib58)] | DAF | OTB, LaSOT, UAVDT |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| ECCVW 2016 | SiamFC [[58](#bib.bib58)] | DAF | OTB, LaSOT, UAVDT |'
- en: '| CVPR 2016 | SINT [[59](#bib.bib59)] | DAF | OTB, LaSOT, UAVDT |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2016 | SINT [[59](#bib.bib59)] | DAF | OTB, LaSOT, UAVDT |'
- en: '| CVPR 2016 | MDNet [[60](#bib.bib60)] | DAF | OTB, LaSOT, UAVDT |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2016 | MDNet [[60](#bib.bib60)] | DAF | OTB, LaSOT, UAVDT |'
- en: '| CVPR 2016 | HDT [[61](#bib.bib61)] | DAF | OTB, UAVDT |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2016 | HDT [[61](#bib.bib61)] | DAF | OTB, UAVDT |'
- en: '| ICCV 2017, TIP 2019 | PTAV [[70](#bib.bib70), [71](#bib.bib71)] | DAF, HOG
    | OTB, LaSOT, UAVDT |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2017, TIP 2019 | PTAV [[70](#bib.bib70), [71](#bib.bib71)] | DAF, HOG
    | OTB, LaSOT, UAVDT |'
- en: '| ICCV 2017 | CREST [[72](#bib.bib72)] | DAF | OTB, UAVDT |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2017 | CREST [[72](#bib.bib72)] | DAF | OTB, UAVDT |'
- en: '| ICCV 2017 | Meta-CREST [[72](#bib.bib72)] | DAF | OTB |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2017 | Meta-CREST [[72](#bib.bib72)] | DAF | OTB |'
- en: '| ICCV 2017 | UCT [[73](#bib.bib73)] | DAF | OTB, VOT2018 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2017 | UCT [[73](#bib.bib73)] | DAF | OTB, VOT2018 |'
- en: '| ICCV 2017 | DSiam [[74](#bib.bib74)] | DAF | VOT2018, LaSOT |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2017 | DSiam [[74](#bib.bib74)] | DAF | VOT2018, LaSOT |'
- en: '| CVPR 2017 | CFNet [[86](#bib.bib86)] | DAF | OTB, VOT2018, LaSOT, UAVDT |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2017 | CFNet [[86](#bib.bib86)] | DAF | OTB, VOT2018, LaSOT, UAVDT |'
- en: '| CVPR 2017 | ECO [[87](#bib.bib87)] | DAF, HOG, CN | OTB, VOT2018, LaSOT,
    UAV123, UAVDT, VisDrone2019 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2017 | ECO [[87](#bib.bib87)] | DAF, HOG, CN | OTB, VOT2018, LaSOT,
    UAV123, UAVDT, VisDrone2019 |'
- en: '| CVPR 2017 | DeepCSRDCF [[88](#bib.bib88)] | DAF, HOG, CN | VOT2018, LaSOT
    |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2017 | DeepCSRDCF [[88](#bib.bib88)] | DAF, HOG, CN | VOT2018, LaSOT
    |'
- en: '| CVPR 2017 | MCPF [[89](#bib.bib89)] | DAF | OTB, VOT2018 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2017 | MCPF [[89](#bib.bib89)] | DAF | OTB, VOT2018 |'
- en: '| CVPR 2017 | ACFN [[93](#bib.bib93)] | DAF, HOG, Color | OTB |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2017 | ACFN [[93](#bib.bib93)] | DAF, HOG, Color | OTB |'
- en: '| arXiv 2017 | DCFNet [[95](#bib.bib95)] | DAF | OTB |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| arXiv 2017 | DCFNet [[95](#bib.bib95)] | DAF | OTB |'
- en: '| arXiv 2017 | DCFNet2 [[95](#bib.bib95)] | DAF | OTB, VOT2018 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| arXiv 2017 | DCFNet2 [[95](#bib.bib95)] | DAF | OTB, VOT2018 |'
- en: '| ECCV 2018 | TripletLoss-CFNet [[100](#bib.bib100)] | DAF | OTB |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2018 | TripletLoss-CFNet [[100](#bib.bib100)] | DAF | OTB |'
- en: '| ECCV 2018 | TripletLoss-SiamFC [[100](#bib.bib100)] | DAF | OTB |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2018 | TripletLoss-SiamFC [[100](#bib.bib100)] | DAF | OTB |'
- en: '| ECCV 2018 | TripletLoss-CFNet2 [[100](#bib.bib100)] | DAF | OTB |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2018 | TripletLoss-CFNet2 [[100](#bib.bib100)] | DAF | OTB |'
- en: '| ECCV 2018 | UPDT [[102](#bib.bib102)] | DAF, HOG, CN | VOT2018 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2018 | UPDT [[102](#bib.bib102)] | DAF, HOG, CN | VOT2018 |'
- en: '| ECCV 2018 | DaSiamRPN [[104](#bib.bib104)] | DAF | VOT2018, UAV123 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2018 | DaSiamRPN [[104](#bib.bib104)] | DAF | VOT2018, UAV123 |'
- en: '| ECCV 2018 | StructSiam [[106](#bib.bib106)] | DAF | LaSOT |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| ECCV 2018 | StructSiam [[106](#bib.bib106)] | DAF | LaSOT |'
- en: '| ECCVW 2018 | Siam-MCF [[110](#bib.bib110)] | DAF | OTB |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| ECCVW 2018 | Siam-MCF [[110](#bib.bib110)] | DAF | OTB |'
- en: '| CVPR 2018 | TRACA [[113](#bib.bib113)] | CDAF | OTB, VOT2018, LaSOT |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2018 | TRACA [[113](#bib.bib113)] | CDAF | OTB, VOT2018, LaSOT |'
- en: '| CVPR 2018 | VITAL [[114](#bib.bib114)] | DAF | OTB, LaSOT |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2018 | VITAL [[114](#bib.bib114)] | DAF | OTB, LaSOT |'
- en: '| CVPR 2018 | DeepSTRCF [[115](#bib.bib115)] | DAF, HOG, CN | OTB, VOT2018,
    LaSOT, UAV123 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2018 | DeepSTRCF [[115](#bib.bib115)] | DAF, HOG, CN | OTB, VOT2018,
    LaSOT, UAV123 |'
- en: '| CVPR 2018 | SiamRPN [[116](#bib.bib116)] | DAF | OTB, VOT2018, UAV123 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2018 | SiamRPN [[116](#bib.bib116)] | DAF | OTB, VOT2018, UAV123 |'
- en: '| CVPR 2018 | SA-Siam [[117](#bib.bib117)] | DAF | OTB, VOT2018 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2018 | SA-Siam [[117](#bib.bib117)] | DAF | OTB, VOT2018 |'
- en: '| CVPR 2018 | LSART [[120](#bib.bib120)] | DAF | VOT2018 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2018 | LSART [[120](#bib.bib120)] | DAF | VOT2018 |'
- en: '| CVPR 2018 | DRT [[119](#bib.bib119)] | DAF, HOG, CN | VOT2018 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2018 | DRT [[119](#bib.bib119)] | DAF, HOG, CN | VOT2018 |'
- en: '| NIPS 2018 | DAT [[130](#bib.bib130)] | DAF | OTB, VOT2018 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| NIPS 2018 | DAT [[130](#bib.bib130)] | DAF | OTB, VOT2018 |'
- en: '| PAMI 2018 | HCFTs [[133](#bib.bib133)] | DAF | OTB |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| PAMI 2018 | HCFTs [[133](#bib.bib133)] | DAF | OTB |'
- en: '| IJCV 2018 | LCTdeep [[142](#bib.bib142)] | DAF | OTB |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| IJCV 2018 | LCTdeep [[142](#bib.bib142)] | DAF | OTB |'
- en: '| TIP 2018 | CFCF [[137](#bib.bib137)] | DAF, HOG | VOT2018 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| TIP 2018 | CFCF [[137](#bib.bib137)] | DAF, HOG | VOT2018 |'
- en: '| CVPR 2019 | C-RPN [[150](#bib.bib150)] | DAF | OTB, VOT2018, LaSOT |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2019 | C-RPN [[150](#bib.bib150)] | DAF | OTB, VOT2018, LaSOT |'
- en: '| CVPR 2019 | GCT [[151](#bib.bib151)] | DAF | OTB, VOT2018, UAV123 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2019 | GCT [[151](#bib.bib151)] | DAF | OTB, VOT2018, UAV123 |'
- en: '| CVPR 2019 | SiamMask [[155](#bib.bib155)] | DAF | VOT2018, UAVDT, VisDrone2019
    |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2019 | SiamMask [[155](#bib.bib155)] | DAF | VOT2018, UAVDT, VisDrone2019
    |'
- en: '| CVPR 2019 | SiamRPN$++$ [[156](#bib.bib156)] | DAF | OTB, VOT2018, UAV123,
    UAVDT, VisDrone2019 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2019 | SiamRPN$++$ [[156](#bib.bib156)] | DAF | OTB, VOT2018, UAV123,
    UAVDT, VisDrone2019 |'
- en: '| CVPR 2019 | TADT [[157](#bib.bib157)] | DAF | OTB |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2019 | TADT [[157](#bib.bib157)] | DAF | OTB |'
- en: '| CVPR 2019 | ASRCF [[148](#bib.bib148)] | DAF, HOG | OTB, LaSOT |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2019 | ASRCF [[148](#bib.bib148)] | DAF, HOG | OTB, LaSOT |'
- en: '| CVPR 2019 | SiamDW-SiamRPN [[154](#bib.bib154)] | DAF | OTB, VOT2018, UAVDT,
    VisDrone2019 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2019 | SiamDW-SiamRPN [[154](#bib.bib154)] | DAF | OTB, VOT2018, UAVDT,
    VisDrone2019 |'
- en: '| CVPR 2019 | SiamDW-SiamFC [[154](#bib.bib154)] | DAF | OTB, VOT2018 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2019 | SiamDW-SiamFC [[154](#bib.bib154)] | DAF | OTB, VOT2018 |'
- en: '| CVPR 2019 | ATOM [[149](#bib.bib149)] | DAF | VOT2018, LaSOT, UAV123, UAVDT,
    VisDrone2019 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2019 | ATOM [[149](#bib.bib149)] | DAF | VOT2018, LaSOT, UAV123, UAVDT,
    VisDrone2019 |'
- en: '| ICCV 2019 | DiMP50 [[159](#bib.bib159)] | DAF | VOT2018, LaSOT, UAV123, UAVDT,
    VisDrone2019 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| ICCV 2019 | DiMP50 [[159](#bib.bib159)] | DAF | VOT2018, LaSOT, UAV123, UAVDT,
    VisDrone2019 |'
- en: '| CVPR 2020 | PrDiMP50 [[194](#bib.bib194)] | DAF | LaSOT, UAVDT, VisDrone2019
    |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| CVPR 2020 | PrDiMP50 [[194](#bib.bib194)] | DAF | LaSOT, UAVDT, VisDrone2019
    |'
- en: IV-A Quantitative Comparisons
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 定量比较
- en: 'According to the results shown in Fig. [4](#S4.F4 "Figure 4 ‣ IV-A Quantitative
    Comparisons ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A
    Comprehensive Survey"), the top-3 visual tracking methods in terms of the precision
    metric are the VITAL, MDNet, and DAT on the OTB2013 dataset, the SiamDW-SiamRPN,
    ASRCF, and VITAL on the OTB2015 dataset, and the PrDiMP50, DiMP50, and ATOM on
    the LaSOT dataset, respectively. In terms of success metric, the ASRCF, VITAL,
    and MDNet on the OTB2013 dataset, the SiamRPN++, SANet, and ASRCF on the OTB2015
    dataset, and the PrDiMP50, DiMP50, and ATOM on the LaSOT dataset have achieved
    the best performance, respectively. On the VOT2018 dataset (see Fig. [5](#S4.F5
    "Figure 5 ‣ IV-A Quantitative Comparisons ‣ IV Experimental Analyses ‣ Deep Learning
    for Visual Tracking: A Comprehensive Survey")), the top-3 visual trackers are
    the SiamMask, SiamRPN++, and DiMP50 in terms of accuracy measure while the PrDiMP50,
    DiMP50, and ATOM trackers have the best robustness, respectively. For the aerial-view
    tracking, the PrDiMP50, DiMP50, SiamRPN++, and SiamMask have provided the best
    results for average precision & success metrics.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图 [4](#S4.F4 "图 4 ‣ IV-A 定量比较 ‣ IV 实验分析 ‣ 深度学习视觉跟踪：全面调查")所示结果，在精度指标上，OTB2013
    数据集上的前 3 名视觉跟踪方法是 VITAL、MDNet 和 DAT，OTB2015 数据集上的前 3 名是 SiamDW-SiamRPN、ASRCF 和
    VITAL，LaSOT 数据集上的前 3 名是 PrDiMP50、DiMP50 和 ATOM。在成功率指标上，OTB2013 数据集上的 ASRCF、VITAL
    和 MDNet，OTB2015 数据集上的 SiamRPN++、SANet 和 ASRCF，以及 LaSOT 数据集上的 PrDiMP50、DiMP50 和
    ATOM 都取得了最佳性能。在 VOT2018 数据集上（见图 [5](#S4.F5 "图 5 ‣ IV-A 定量比较 ‣ IV 实验分析 ‣ 深度学习视觉跟踪：全面调查")），在准确性测量上，前
    3 名视觉跟踪器是 SiamMask、SiamRPN++ 和 DiMP50，而 PrDiMP50、DiMP50 和 ATOM 追踪器则具有最佳鲁棒性。对于空中视角跟踪，PrDiMP50、DiMP50、SiamRPN++
    和 SiamMask 提供了最佳的平均精度和成功率指标结果。
- en: \justify![Refer to caption](img/26c2d76208f8522d71de765b8529c81b.png)![Refer
    to caption](img/6919da00678dd39f7549d8026cd8ee04.png)![Refer to caption](img/5e2441e887af10f6bbb25288d76c07d6.png)![Refer
    to caption](img/4b0dd49a9af105ef9de249a261855e33.png)\justify![Refer to caption](img/1b8b5639edbcfb407adb4ef74271943f.png)![Refer
    to caption](img/e072556d07352e151462e8b909767f1d.png)![Refer to caption](img/021b5bbfb68ad395965ed6bbd01723dd.png)![Refer
    to caption](img/cc808fedbc177a3899b3d71cded02e6e.png)\justify![Refer to caption](img/8c8ebef44f9985393dd1ae147c308f18.png)![Refer
    to caption](img/c22aefe771fad978d3080eedc1f18fc4.png)![Refer to caption](img/30f1cdb705deb7a9feadad478287c5e4.png)![Refer
    to caption](img/b7f4892c7f122ea86ccdccb952fd8aa3.png)
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: \justify![参见说明](img/26c2d76208f8522d71de765b8529c81b.png)![参见说明](img/6919da00678dd39f7549d8026cd8ee04.png)![参见说明](img/5e2441e887af10f6bbb25288d76c07d6.png)![参见说明](img/4b0dd49a9af105ef9de249a261855e33.png)\justify![参见说明](img/1b8b5639edbcfb407adb4ef74271943f.png)![参见说明](img/e072556d07352e151462e8b909767f1d.png)![参见说明](img/021b5bbfb68ad395965ed6bbd01723dd.png)![参见说明](img/cc808fedbc177a3899b3d71cded02e6e.png)\justify![参见说明](img/8c8ebef44f9985393dd1ae147c308f18.png)![参见说明](img/c22aefe771fad978d3080eedc1f18fc4.png)![参见说明](img/30f1cdb705deb7a9feadad478287c5e4.png)![参见说明](img/b7f4892c7f122ea86ccdccb952fd8aa3.png)
- en: 'Figure 4: Overall experimental comparison of state-of-the-art visual tracking
    methods on the OTB2013, OTB2015, LaSOT, UAVDT, and VisDrone2019 visual tracking
    datasets.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：OTB2013、OTB2015、LaSOT、UAVDT 和 VisDrone2019 视觉跟踪数据集上最先进的视觉跟踪方法的总体实验比较。
- en: \justify![Refer to caption](img/7cb24852a1f3b077d13c7b231c2e3b26.png)![Refer
    to caption](img/5174548c0037ad62c135814ca82a1c4f.png)![Refer to caption](img/85469589d7b20e07bfd86586e4bd9252.png)
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: \justify![参见说明](img/7cb24852a1f3b077d13c7b231c2e3b26.png)![参见说明](img/5174548c0037ad62c135814ca82a1c4f.png)![参见说明](img/85469589d7b20e07bfd86586e4bd9252.png)
- en: 'Figure 5: Performance comparison of visual tracking methods on VOT2018 dataset.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：VOT2018 数据集上视觉跟踪方法的性能比较。
- en: 'On the other hand, the best trackers based on both precision-success measures
    (see Fig. [4](#S4.F4 "Figure 4 ‣ IV-A Quantitative Comparisons ‣ IV Experimental
    Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")) are the
    VITAL, MDNet, and ASRCF on the OTB2013 dataset, the SiamRPN++, ASRCF, and VITAL
    on the OTB2015 dataset, the PrDiMP50, DiMP50, and ATOM on the LaSOT dataset, and
    the PrDiMP50, DiMP50, and SiamRPN++ on the aerial-view datasets (i.e., the UAV123,
    UAVDT, and VisDrone2019). On the VOT2018 dataset, the DiMP50, SiamRPN++, and ATOM
    are the best performing trackers based on the EAO score. Moreover, the PrDiMP50,
    DiMP50, SiamRPN++, and ATOM have achieved the best AUC scores while the SiamRPN,
    SiamRPN++, and CFNet are the fastest visual trackers, respectively. According
    to the results (i.e., Fig. [4](#S4.F4 "Figure 4 ‣ IV-A Quantitative Comparisons
    ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey"), and Fig. [5](#S4.F5 "Figure 5 ‣ IV-A Quantitative Comparisons ‣ IV Experimental
    Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")), the best
    visual tracking methods that repeated the best results on different tracking datasets
    are the PrDiMP50 [[194](#bib.bib194)], DiMP50 [[159](#bib.bib159)], ATOM [[149](#bib.bib149)],
    VITAL [[114](#bib.bib114)], MDNet [[60](#bib.bib60)], DAT [[130](#bib.bib130)],
    ASRCF [[148](#bib.bib148)], SiamDW-SiamRPN [[154](#bib.bib154)], SiamRPN++ [[156](#bib.bib156)],
    C-RPN [[150](#bib.bib150)], StructSiam [[150](#bib.bib150)], SiamMask [[155](#bib.bib155)],
    DaSiamRPN [[104](#bib.bib104)], UPDT [[102](#bib.bib102)], LSART [[120](#bib.bib120)],
    DeepSTRCF [[115](#bib.bib115)], and DRT [[119](#bib.bib119)]. These methods will
    be investigated in Sec. [IV-C](#S4.SS3 "IV-C Discussion ‣ IV Experimental Analyses
    ‣ Deep Learning for Visual Tracking: A Comprehensive Survey").'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，基于精确度-成功度测量的最佳跟踪器（参见图[4](#S4.F4 "Figure 4 ‣ IV-A Quantitative Comparisons
    ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")）在OTB2013数据集上是VITAL、MDNet和ASRCF，在OTB2015数据集上是SiamRPN++、ASRCF和VITAL，在LaSOT数据集上是PrDiMP50、DiMP50和ATOM，在航空视角数据集（即UAV123、UAVDT和VisDrone2019）上是PrDiMP50、DiMP50和SiamRPN++。在VOT2018数据集上，基于EAO评分，DiMP50、SiamRPN++和ATOM是表现最好的跟踪器。此外，PrDiMP50、DiMP50、SiamRPN++和ATOM在AUC评分上表现最佳，而SiamRPN、SiamRPN++和CFNet分别是最快的视觉跟踪器。根据结果（即图[4](#S4.F4
    "Figure 4 ‣ IV-A Quantitative Comparisons ‣ IV Experimental Analyses ‣ Deep Learning
    for Visual Tracking: A Comprehensive Survey")和图[5](#S4.F5 "Figure 5 ‣ IV-A Quantitative
    Comparisons ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A
    Comprehensive Survey")），在不同跟踪数据集上重复取得最佳结果的视觉跟踪方法有PrDiMP50 [[194](#bib.bib194)]、DiMP50
    [[159](#bib.bib159)]、ATOM [[149](#bib.bib149)]、VITAL [[114](#bib.bib114)]、MDNet
    [[60](#bib.bib60)]、DAT [[130](#bib.bib130)]、ASRCF [[148](#bib.bib148)]、SiamDW-SiamRPN
    [[154](#bib.bib154)]、SiamRPN++ [[156](#bib.bib156)]、C-RPN [[150](#bib.bib150)]、StructSiam
    [[150](#bib.bib150)]、SiamMask [[155](#bib.bib155)]、DaSiamRPN [[104](#bib.bib104)]、UPDT
    [[102](#bib.bib102)]、LSART [[120](#bib.bib120)]、DeepSTRCF [[115](#bib.bib115)]和DRT
    [[119](#bib.bib119)]。这些方法将在Sec. [IV-C](#S4.SS3 "IV-C Discussion ‣ IV Experimental
    Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")中进行研究。'
- en: IV-B Most Challenging Attributes per Benchmark Dataset
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 每个基准数据集最具挑战性的属性
- en: 'Following on the VOT challenges [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)],
    which have specified the most challenging visual tracking attributes, this work
    also introduces the most challenging attributes on the OTB, LaSOT, UAV123, UAVDT,
    and VisDrone datasets. These attributes are determined by the median accuracy
    & robustness per attribute on the VOT or the median precision & success per attribute
    on other datasets. Table [VIII](#S4.T8 "TABLE VIII ‣ IV-B Most Challenging Attributes
    per Benchmark Dataset ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey") shows the most challenging attributes for each benchmark
    dataset. The OCC/LOC, OV, FM, DEF, LR, ARC, and SIB are selected as the most challenging
    attributes that can effectively impact the performance of DL-based visual trackers.
    Fig. [6](#S4.F6 "Figure 6 ‣ IV-B Most Challenging Attributes per Benchmark Dataset
    ‣ IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey") compares the performances of these methods on the most challenging attributes
    on the OTB2015, LaSOT, UAV123, UAVDT, and VisDrone2019 datasets.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '继续在VOT挑战[[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)]中，这些挑战指定了最具挑战性的视觉跟踪属性，本工作还引入了OTB、LaSOT、UAV123、UAVDT和VisDrone数据集中的最具挑战性属性。这些属性由VOT上的每个属性的中位准确率和鲁棒性或其他数据集上的每个属性的中位精度和成功率决定。表[VIII](#S4.T8
    "TABLE VIII ‣ IV-B Most Challenging Attributes per Benchmark Dataset ‣ IV Experimental
    Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")展示了每个基准数据集的最具挑战性属性。OCC/LOC、OV、FM、DEF、LR、ARC和SIB被选为最具挑战性的属性，这些属性能够有效影响基于深度学习的视觉跟踪器的性能。图[6](#S4.F6
    "Figure 6 ‣ IV-B Most Challenging Attributes per Benchmark Dataset ‣ IV Experimental
    Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")比较了这些方法在OTB2015、LaSOT、UAV123、UAVDT和VisDrone2019数据集上最具挑战性属性的性能。'
- en: 'TABLE VIII: Five most challenging attributes of benchmark datasets. [first
    to third challenging attributes are shown by red, yellow, and green colors.]'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 表VIII：基准数据集的五个最具挑战性属性。[前三个挑战性属性由红色、黄色和绿色表示。]
- en: '| Dataset | Metric | IV | DEF | MB | CM | OCC | POC | FOC | ROT | IPR | OPR
    | BC | VC | SV | FM | OV | LR | ARC | MC | SIB | OM | SOB | OB | LT | LOC |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 指标 | IV | DEF | MB | CM | OCC | POC | FOC | ROT | IPR | OPR | BC |
    VC | SV | FM | OV | LR | ARC | MC | SIB | OM | SOB | OB | LT | LOC |'
- en: '| OTB2015 | Precision | 0.7807 | 0.7382 | 0.7642 | - | 0.7347 | - | - | - |
    0.7575 | 0.7611 | 0.7576 | - | 0.7471 | 0.7506 | 0.6911 | 0.7532 | - | - | - |
    - | - | - | - | - |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| OTB2015 | 精度 | 0.7807 | 0.7382 | 0.7642 | - | 0.7347 | - | - | - | 0.7575
    | 0.7611 | 0.7576 | - | 0.7471 | 0.7506 | 0.6911 | 0.7532 | - | - | - | - | -
    | - | - | - |'
- en: '| Success | 0.6330 | 0.5682 | 0.6466 | - | 0.6027 | - | - | - | 0.6154 | 0.6172
    | 0.6144 | - | 0.6022 | 0.6268 | 0.5683 | 0.5906 | - | - | - | - | - | - | - |
    - |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| 成功率 | 0.6330 | 0.5682 | 0.6466 | - | 0.6027 | - | - | - | 0.6154 | 0.6172
    | 0.6144 | - | 0.6022 | 0.6268 | 0.5683 | 0.5906 | - | - | - | - | - | - | - |
    - |'
- en: '| VOT2018 | Accuracy | 0.5026 | - | - | 0.5258 | 0.4312 | - | - | - | - | -
    | - | - | 0.4627 | - | - | - | - | 0.5044 | - | - | - | - | - | - |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| VOT2018 | 准确率 | 0.5026 | - | - | 0.5258 | 0.4312 | - | - | - | - | - | -
    | - | 0.4627 | - | - | - | - | 0.5044 | - | - | - | - | - | - |'
- en: '| Robustness | 0.1695 | - | - | 0.1423 | 0.2856 | - | - | - | - | - | - | -
    | 0.1051 | - | - | - | - | 0.1802 | - | - | - | - | - | - |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 鲁棒性 | 0.1695 | - | - | 0.1423 | 0.2856 | - | - | - | - | - | - | - | 0.1051
    | - | - | - | - | 0.1802 | - | - | - | - | - | - |'
- en: '| LaSOT | Precision | 0.2839 | 0.1778 | 0.2149 | 0.2306 | - | 0.1937 | 0.1904
    | 0.2016 | - | - | 0.2218 | 0.2034 | 0.2266 | 0.1733 | 0.1608 | 0.2248 | 0.2026
    | - | - | - | - | - | - | - |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| LaSOT | 精度 | 0.2839 | 0.1778 | 0.2149 | 0.2306 | - | 0.1937 | 0.1904 | 0.2016
    | - | - | 0.2218 | 0.2034 | 0.2266 | 0.1733 | 0.1608 | 0.2248 | 0.2026 | - | -
    | - | - | - | - | - |'
- en: '| Success | 0.2580 | 0.2081 | 0.2216 | 0.2506 | - | 0.2112 | 0.1666 | 0.2186
    | - | - | 0.2329 | 0.1773 | 0.2394 | 0.1398 | 0.1726 | 0.1772 | 0.2119 | - | -
    | - | - | - | - | - |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| 成功率 | 0.2580 | 0.2081 | 0.2216 | 0.2506 | - | 0.2112 | 0.1666 | 0.2186 |
    - | - | 0.2329 | 0.1773 | 0.2394 | 0.1398 | 0.1726 | 0.1772 | 0.2119 | - | - |
    - | - | - | - | - |'
- en: '| UAV123 | Precision | 0.6573 | - | - | 0.6317 | - | 0.6991 | 0.6805 | - |
    - | - | 0.6361 | .6462 | 0.6656 | 0.6019 | 0.6429 | 0.6100 | 0.5690 | - | 0.6991
    | - | - | - | - | - |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| UAV123 | 精度 | 0.6573 | - | - | 0.6317 | - | 0.6991 | 0.6805 | - | - | - |
    0.6361 | 0.6462 | 0.6656 | 0.6019 | 0.6429 | 0.6100 | 0.5690 | - | 0.6991 | -
    | - | - | - | - |'
- en: '| Success | 0.4926 | - | - | 0.4034 | - | 0.5447 | 0.5201 | - | - | - | 0.4673
    | 0.4794 | 0.5176 | 0.4158 | 0.4743 | 0.4724 | 0.3433 | - | 0.5447 | - | - | -
    | - | - |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| 成功率 | 0.4926 | - | - | 0.4034 | - | 0.5447 | 0.5201 | - | - | - | 0.4673
    | 0.4794 | 0.5176 | 0.4158 | 0.4743 | 0.4724 | 0.3433 | - | 0.5447 | - | - | -
    | - | - |'
- en: '| UAVDT | Precision | 0.7723 | - | - | 0.6877 | - | - | - | - | - | - | 0.6634
    | - | 0.7003 | - | - | - | - | - | 0.7638 | 0.7039 | 0.7638 | 0.7280 | 0.8235
    | 0.5779 |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| UAVDT | 精度 | 0.7723 | - | - | 0.6877 | - | - | - | - | - | - | 0.6634 | -
    | 0.7003 | - | - | - | - | - | 0.7638 | 0.7039 | 0.7638 | 0.7280 | 0.8235 | 0.5779
    |'
- en: '| Success | 0.5844 | - | - | 0.5515 | - | - | - | - | - | - | 0.5051 | - |
    0.5603 | - | - | - | - | - | 0.5536 | 0.5513 | 0.5536 | 0.5463 | 0.6213 | 0.4600
    |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| 成功率 | 0.5844 | - | - | 0.5515 | - | - | - | - | - | - | 0.5051 | - | 0.5603
    | - | - | - | - | - | 0.5536 | 0.5513 | 0.5536 | 0.5463 | 0.6213 | 0.4600 |'
- en: '| VisDrone2019 | Precision | 0.7790 | - | - | 0.7407 | - | 0.7174 | 0.6919
    | - | - | - | 0.5775 | 0.8058 | 0.7392 | 0.7473 | 0.8088 | 0.6031 | 0.7522 | -
    | 0.5445 | - | - | - | - | - |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| VisDrone2019 | 精度 | 0.7790 | - | - | 0.7407 | - | 0.7174 | 0.6919 | - | -
    | - | 0.5775 | 0.8058 | 0.7392 | 0.7473 | 0.8088 | 0.6031 | 0.7522 | - | 0.5445
    | - | - | - | - | - |'
- en: '| Success | 0.6272 | - | - | 0.5816 | - | 0.5511 | 0.5407 | - | - | - | 0.4112
    | 0.6564 | 0.5891 | 0.5862 | 0.6676 | 0.3681 | 0.5861 | - | 0.3914 | - | - | -
    | - | - | \justify![Refer to caption](img/2552126c5252628ac750e093169ab67d.png)![Refer
    to caption](img/c5438bfdebf9c31b534e2eb985093e54.png)![Refer to caption](img/0d866fee82c286dd1eb29b964165956d.png)![Refer
    to caption](img/597f49d8fb5898de4c158825947205f9.png)![Refer to caption](img/d86a1e35474bb6167901f2babc625acd.png)\justify![Refer
    to caption](img/65f1066fcd3367e5bf7154daf37a67f0.png)![Refer to caption](img/3e07181f777d54e6eb7ce05d1b3b91f7.png)![Refer
    to caption](img/a04e1882404c57ac26e57e5210cb4cd7.png)![Refer to caption](img/59706235bff34a411cabde8e132fdd22.png)![Refer
    to caption](img/c3947d9010d5cbdb3f9775232804e854.png)'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '| 成功率 | 0.6272 | - | - | 0.5816 | - | 0.5511 | 0.5407 | - | - | - | 0.4112
    | 0.6564 | 0.5891 | 0.5862 | 0.6676 | 0.3681 | 0.5861 | - | 0.3914 | - | - | -
    | - | - | \justify![参见说明](img/2552126c5252628ac750e093169ab67d.png)![参见说明](img/c5438bfdebf9c31b534e2eb985093e54.png)![参见说明](img/0d866fee82c286dd1eb29b964165956d.png)![参见说明](img/597f49d8fb5898de4c158825947205f9.png)![参见说明](img/d86a1e35474bb6167901f2babc625acd.png)\justify![参见说明](img/65f1066fcd3367e5bf7154daf37a67f0.png)![参见说明](img/3e07181f777d54e6eb7ce05d1b3b91f7.png)![参见说明](img/a04e1882404c57ac26e57e5210cb4cd7.png)![参见说明](img/59706235bff34a411cabde8e132fdd22.png)![参见说明](img/c3947d9010d5cbdb3f9775232804e854.png)'
- en: 'Figure 6: Comparison of state-of-the-art trackers in terms of the most challenging
    attributes on the OTB2015, LaSOT, UAV123, UAVDT, and VisDrone2019 datasets, left
    to right column, respectively.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在OTB2015、LaSOT、UAV123、UAVDT和VisDrone2019数据集中，按最具挑战性的属性对最先进的跟踪器进行比较，从左到右列。
- en: According to the OCC attribute, the most accurate & robust visual trackers on
    the VOT2018 dataset are the SiamRPN++ [[156](#bib.bib156)] and DRT [[119](#bib.bib119)],
    respectively. In terms of success metric, the SiamRPN++ [[156](#bib.bib156)] is
    the best visual tracker to tackle the DEF and OV attributes, while the Siam-MCF
    [[110](#bib.bib110)] is the best one to deal with the visual tracking in LR videos
    on the OTB2015 dataset. The ASRCF [[148](#bib.bib148)], ECO [[87](#bib.bib87)],
    and SiamDW-SiamRPN [[154](#bib.bib154)] are the best trackers in precision metric
    to face with OV, OCC, and DEF attributes on the OTB-2015 dataset. The PrDiMP50
    [[194](#bib.bib194)], DiMP50 [[159](#bib.bib159)], and ATOM [[149](#bib.bib149)]
    trackers are the absolute best methods on the LaSOT dataset on all visual attributes.
    On the UAV123 dataset, the ATOM and DiMP50 have achieved the best results in terms
    of precision and success metrics, respectively. Also, the PrDiMP50 is the best
    tracker for handling large occlusions on the UAVDT dataset. Finally, the SiamDW
    is the best tracker to tackle similar objects on the VisDrone2019-test-dev dataset.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 根据OCC属性，在VOT2018数据集中，最准确且最稳健的视觉跟踪器是SiamRPN++ [[156](#bib.bib156)] 和DRT [[119](#bib.bib119)]。就成功率指标而言，SiamRPN++
    [[156](#bib.bib156)] 是应对DEF和OV属性的最佳视觉跟踪器，而Siam-MCF [[110](#bib.bib110)] 是处理OTB2015数据集中LR视频视觉跟踪的最佳选择。ASRCF
    [[148](#bib.bib148)]、ECO [[87](#bib.bib87)] 和SiamDW-SiamRPN [[154](#bib.bib154)]
    是在OTB-2015数据集中应对OV、OCC和DEF属性的最佳精度跟踪器。PrDiMP50 [[194](#bib.bib194)]、DiMP50 [[159](#bib.bib159)]
    和ATOM [[149](#bib.bib149)] 跟踪器是LaSOT数据集中所有视觉属性的绝对最佳方法。在UAV123数据集中，ATOM和DiMP50在精度和成功率指标上分别取得了最佳结果。此外，PrDiMP50是处理UAVDT数据集中大遮挡的最佳跟踪器。最后，SiamDW是应对VisDrone2019-test-dev数据集中相似物体的最佳跟踪器。
- en: 'As shown in Fig. [5](#S4.F5 "Figure 5 ‣ IV-A Quantitative Comparisons ‣ IV
    Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive Survey"),
    the DCF-based methods have achieved fewer failures among the other methods, while
    the SNN- & custom-network based trackers have gained more overlap between the
    estimated BBs and ground-truth ones. The SiamRPN-based methods (i.e., [[155](#bib.bib155),
    [156](#bib.bib156), [104](#bib.bib104), [154](#bib.bib154)]) accurately handle
    scenarios under each of CM, IV, MC, OCC, or SC attributes by adopting deeper and
    wider backbone networks, including classification and regression branches. Moreover,
    the ATOM, DiMP50, and PrDiMP50 exploit powerful classification & regression networks
    and optimization processes for online training and fast adaptation. Thus, these
    trackers have provided significant advances on various tracking benchmarks. By
    considering the fusion of hand-crafted and deep features [[115](#bib.bib115),
    [102](#bib.bib102), [119](#bib.bib119)], temporal regularization term [[115](#bib.bib115)],
    reliability term [[119](#bib.bib119)], data augmentation [[102](#bib.bib102)],
    and exploitation of ResNet-50 model [[102](#bib.bib102)], the DCF-based methods
    have attained desirable robustness against CM attribute. Furthermore, the computational
    efficiency and the robustness of DCF-based trackers are attractive for aerial-view
    trackers.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [5](#S4.F5 "Figure 5 ‣ IV-A Quantitative Comparisons ‣ IV Experimental Analyses
    ‣ Deep Learning for Visual Tracking: A Comprehensive Survey") 所示，基于 DCF 的方法在其他方法中实现了较少的失败，而基于
    SNN 和定制网络的跟踪器在估计的 BB 和真实 BB 之间获得了更多的重叠。基于 SiamRPN 的方法（即 [[155](#bib.bib155), [156](#bib.bib156),
    [104](#bib.bib104), [154](#bib.bib154)]）通过采用更深且更宽的骨干网络，包括分类和回归分支，准确处理了 CM、IV、MC、OCC
    或 SC 属性下的场景。此外，ATOM、DiMP50 和 PrDiMP50 利用强大的分类和回归网络及优化过程进行在线训练和快速适应。因此，这些跟踪器在各种跟踪基准测试中取得了显著进展。通过考虑手工设计特征与深度特征的融合
    [[115](#bib.bib115), [102](#bib.bib102), [119](#bib.bib119)]、时间正则化项 [[115](#bib.bib115)]、可靠性项
    [[119](#bib.bib119)]、数据增强 [[102](#bib.bib102)] 和利用 ResNet-50 模型 [[102](#bib.bib102)]，基于
    DCF 的方法在 CM 属性下获得了理想的鲁棒性。此外，基于 DCF 的跟踪器的计算效率和鲁棒性对空中视角跟踪器非常有吸引力。'
- en: To effectively deal with the IV attribute, focusing on the discrimination power
    between the target and its background is the main problem. The strategies such
    as training a fully convolutional network for correlation filter cost function,
    spatial-aware KRR and spatial-aware CNN, and employing semi-supervised video object
    segmentation improve the robustness of DL-based trackers when significant IV occurs.
    To robustly deal with MC and OCC attributes, the DCF- and CNN-based trackers have
    performed the best. However, the SNN-based methods with the aid of region proposal
    subnetwork and proposal refinement can robustly estimate the tightest BB under
    severe scale changes. However, recently, the IoU-based refinement network (based
    on IoU-Net [[255](#bib.bib255)]) employed in ATOM, DiMP, and PrDiMP trackers can
    effectively handle aspect-ratio change of target during tracking.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 要有效处理 IV 属性，主要问题是关注目标与背景之间的区分能力。采用全卷积网络训练用于相关性滤波器成本函数、空间感知的 KRR 和空间感知的 CNN，以及使用半监督视频目标分割等策略，可以在出现显著
    IV 时提高基于 DL 的跟踪器的鲁棒性。为了稳健地处理 MC 和 OCC 属性，基于 DCF 和 CNN 的跟踪器表现最佳。然而，借助区域提议子网络和提议优化的
    SNN 基础方法可以在严重的尺度变化下稳健地估计最紧的 BB。然而，最近，在 ATOM、DiMP 和 PrDiMP 跟踪器中采用的基于 IoU 的优化网络（基于
    IoU-Net [[255](#bib.bib255)]）能够有效处理跟踪过程中目标的纵横比变化。
- en: IV-C Discussion
  id: totrans-496
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 讨论
- en: The overall best methods (i.e., PrDiMP50 [[194](#bib.bib194)], DiMP50 [[159](#bib.bib159)],
    ATOM [[149](#bib.bib149)], VITAL [[114](#bib.bib114)], MDNet [[60](#bib.bib60)],
    DAT [[130](#bib.bib130)], ASRCF [[148](#bib.bib148)], SiamDW-SiamRPN [[154](#bib.bib154)],
    SiamRPN++ [[156](#bib.bib156)], C-RPN [[150](#bib.bib150)], StructSiam [[106](#bib.bib106)],
    SiamMask [[155](#bib.bib155)], DaSiamRPN [[104](#bib.bib104)], UPDT [[102](#bib.bib102)],
    LSART [[120](#bib.bib120)], DeepSTRCF [[115](#bib.bib115)], and DRT [[119](#bib.bib119)])
    belong to a wide range of network architectures. For instance, the MDNet, LSART,
    and DAT (uses the MDNet architecture) utilize CNNs to localize a visual target
    while the ASRCF, UPDT, DRT, and DeepSTRCF exploit deep off-the-shelf features.
    All the ATOM, DiMP, and PrDiMP trackers employ custom classification & refinement
    networks. Besides the VITAL that is a GAN-based tracker, the C-RPN, StructSiam,
    SiamMask, DaSiamRPN, SiamDW, and SiamRPN++ have the SNN architecture. Although
    the most recent attractive deep architectures for visual tracking are based on
    Siamese or custom networks, GAN- and RL-based trackers have been recently developed
    for some specific purposes, such as addressing the imbalance distribution of training
    samples [[114](#bib.bib114)] or selecting an appropriate real-time search strategy
    [[103](#bib.bib103), [162](#bib.bib162)]. GAN-based trackers can successfully
    augment positive samples to enrich the target appearance model. These trackers
    also enjoy cost-sensitive losses to focus on hard negative samples. RL-based trackers
    learn continuous actions to provide more reliable search & verification strategies
    for visual trackers. Besides, the combinations of RL-trackers with other architectures
    may add more advantages; for instance, recurrent RL-based tracking considers time
    dependencies to the key components (i.e., actions & states). By doing so, these
    trackers boost their performance by verifying confidence through an RNN motion
    model.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳方法（即，PrDiMP50 [[194](#bib.bib194)]，DiMP50 [[159](#bib.bib159)]，ATOM [[149](#bib.bib149)]，VITAL
    [[114](#bib.bib114)]，MDNet [[60](#bib.bib60)]，DAT [[130](#bib.bib130)]，ASRCF [[148](#bib.bib148)]，SiamDW-SiamRPN
    [[154](#bib.bib154)]，SiamRPN++ [[156](#bib.bib156)]，C-RPN [[150](#bib.bib150)]，StructSiam
    [[106](#bib.bib106)]，SiamMask [[155](#bib.bib155)]，DaSiamRPN [[104](#bib.bib104)]，UPDT
    [[102](#bib.bib102)]，LSART [[120](#bib.bib120)]，DeepSTRCF [[115](#bib.bib115)]，以及
    DRT [[119](#bib.bib119)]) 属于各种网络架构。例如，MDNet、LSART 和 DAT（使用 MDNet 架构）利用 CNNs 来定位视觉目标，而
    ASRCF、UPDT、DRT 和 DeepSTRCF 则利用深度现成特征。所有 ATOM、DiMP 和 PrDiMP 跟踪器都采用定制的分类与精炼网络。除了基于
    GAN 的 VITAL 跟踪器，C-RPN、StructSiam、SiamMask、DaSiamRPN、SiamDW 和 SiamRPN++ 具有 SNN
    架构。尽管最近最具吸引力的视觉跟踪深度架构基于 Siamese 或定制网络，GAN 和 RL 基于的跟踪器最近也已开发用于一些特定目的，例如解决训练样本的不平衡分布
    [[114](#bib.bib114)] 或选择合适的实时搜索策略 [[103](#bib.bib103)，[162](#bib.bib162)]。基于 GAN
    的跟踪器可以成功增强正样本，以丰富目标外观模型。这些跟踪器还享有代价敏感的损失，以关注困难的负样本。基于 RL 的跟踪器学习连续动作，为视觉跟踪器提供更可靠的搜索和验证策略。此外，将
    RL 跟踪器与其他架构结合可能带来更多优势；例如，基于递归 RL 的跟踪考虑到时间依赖性（即，动作和状态）来优化关键组件。通过这样做，这些跟踪器通过 RNN
    运动模型验证置信度，从而提高其性能。
- en: In addition to providing a desirable balance between the performance and speed
    of Siamese or custom network-based trackers, the architectures are modified to
    integrate with diverse deep backbone networks, searching strategies, and learning
    schemes but also exploit fully convolutional networks, correlation layers, region
    proposal networks, video object detection/segmentation modules. The interesting
    point is that five SNN-based methods including the SiamDW-SiamRPN, SiamRPN++,
    C-RPN, SiamMask, and DaSiamRPN are based on the fast SiamRPN method [[116](#bib.bib116)],
    which is consisted of Siamese subnetwork and region proposal subnetwork; these
    subnetworks are leveraged for feature extraction and proposal extraction on correlation
    feature maps to solve the visual tracking problem by one-shot detection task.
    The main advantages of SiamRPN are the time efficiency and precise estimations
    with integrating proposal selection and refinement strategies into a Siamese network.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在性能和速度之间提供理想的平衡，Siamese 或定制网络基础的跟踪器，其架构还被修改以与多样的深度主干网络、搜索策略和学习方案集成，同时还利用全卷积网络、相关层、区域提议网络、视频目标检测/分割模块。值得注意的是，包括
    SiamDW-SiamRPN、SiamRPN++、C-RPN、SiamMask 和 DaSiamRPN 在内的五种基于 SNN 的方法基于快速的 SiamRPN
    方法 [[116](#bib.bib116)]，该方法由 Siamese 子网络和区域提议子网络组成；这些子网络用于在相关特征图上进行特征提取和提议提取，通过一次检测任务解决视觉跟踪问题。SiamRPN
    的主要优势在于时间效率和精确估计，将提议选择和精炼策略集成到 Siamese 网络中。
- en: Interestingly, the ASRCF, UPDT, DRT, and DeepSTRCF, which exploit deep off-the-shelf
    features, are among the top-performing visual tracking methods. Moreover, five
    methods of UPDT, DeepSTRCF, DRT, LSART, and ASRCF take the advantages of the DCF
    framework. On the other side, the best performing visual trackers, namely PrDiMP50,
    DiMP50, ATOM, VITAL, MDNet, DAT, SiamDW, SiamRPN++, C-RPN, StructSiam, SiamMask,
    DaSiamRPN, and LSART exploit specialized deep features for visual tracking purpose.
    Although diversified backbone networks are employed for these methods, state-of-the-art
    methods have been leveraging deeper networks such as the ResNet-50 to strengthen
    the discriminative power of target modeling. From the network training perspective,
    the SiamDW-SiamRPN, SiamRPN++, C-RPN, StructSiam, SiamMask, and DaSiamRPN use
    offline training, the LSART utilizes online training, and the PrDiMP50, DiMP50,
    and ATOM take offline & online training procedures. In particular, the PrDiMP50
    and DiMP50 exploit meta-learning based networks to improve network adaptation
    for the tracking task. The offline trained trackers aim to provide dominant representations
    to achieve real-time tracking speed. Handling significant appearance variations
    needs to adapt to network parameters during tracking, but online training has
    an over-fitting risk because of limited training samples. Hence, the VITAL, MDNet,
    and DAT by employing adversarial learning, domain-independent information, and
    attention maps as regularization terms benefit both offline and online training
    of DNNs. However, these methods provide a tracking speed of about one frame per
    second (FPS) that is not suitable for real-time applications. In contrast, recent
    proposed PrDiMP50, DiMP50, and ATOM trackers exploit custom-designed networks,
    efficient optimization strategies to achieve acceptable tracking speed. From the
    perspective of the objective function of DNNs, the VITAL and StructSiam are classification-based,
    the LSART is regression-based, and the other best-performing trackers [[60](#bib.bib60),
    [104](#bib.bib104), [130](#bib.bib130), [150](#bib.bib150), [154](#bib.bib154),
    [155](#bib.bib155), [156](#bib.bib156), [194](#bib.bib194), [159](#bib.bib159),
    [149](#bib.bib149)] employ both classification and regression objectives. For
    instance, five modified versions of the SiamRPN [[116](#bib.bib116)] (i.e., SiamDW-SiamRPN
    [[154](#bib.bib154)], SiamRPN++ [[156](#bib.bib156)], C-RPN [[150](#bib.bib150)],
    SiamMask [[155](#bib.bib155)], and DaSiamRPN [[104](#bib.bib104)]) have two branches
    for classification and regression. Besides, the ATOM, DiMP50, and PrDiMP50 use
    a classification network for distinguishing target from the background and an
    IoU-Net for BB regression.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，ASRCF、UPDT、DRT 和 DeepSTRCF 等利用深层现成特征的方法是表现最好的视觉跟踪方法之一。此外，UPDT、DeepSTRCF、DRT、LSART
    和 ASRCF 五种方法都利用了 DCF 框架的优势。另一方面，表现最好的视觉跟踪器，即 PrDiMP50、DiMP50、ATOM、VITAL、MDNet、DAT、SiamDW、SiamRPN++、C-RPN、StructSiam、SiamMask、DaSiamRPN
    和 LSART，利用了专门的深层特征来进行视觉跟踪。尽管这些方法使用了多样化的骨干网络，最先进的方法已经开始使用像 ResNet-50 这样的更深网络，以增强目标建模的判别能力。从网络训练的角度来看，SiamDW-SiamRPN、SiamRPN++、C-RPN、StructSiam、SiamMask
    和 DaSiamRPN 使用离线训练，LSART 使用在线训练，而 PrDiMP50、DiMP50 和 ATOM 采用离线和在线训练程序。特别是，PrDiMP50
    和 DiMP50 采用了基于元学习的网络来提高网络适应跟踪任务的能力。离线训练的跟踪器旨在提供主导表示，以实现实时跟踪速度。处理显著的外观变化需要在跟踪过程中调整网络参数，但在线训练由于训练样本有限而有过拟合的风险。因此，VITAL、MDNet
    和 DAT 通过采用对抗学习、领域无关的信息和注意力图作为正则化项，使 DNN 的离线和在线训练都受益。然而，这些方法提供的跟踪速度约为每秒一帧 (FPS)，不适合实时应用。相比之下，最近提出的
    PrDiMP50、DiMP50 和 ATOM 跟踪器利用了定制设计的网络和高效的优化策略，以实现可接受的跟踪速度。从 DNN 的目标函数的角度来看，VITAL
    和 StructSiam 是基于分类的，LSART 是基于回归的，而其他表现最佳的跟踪器[[60](#bib.bib60)、[104](#bib.bib104)、[130](#bib.bib130)、[150](#bib.bib150)、[154](#bib.bib154)、[155](#bib.bib155)、[156](#bib.bib156)、[194](#bib.bib194)、[159](#bib.bib159)、[149](#bib.bib149)]
    采用了分类和回归目标。例如，五个修改版的 SiamRPN [[116](#bib.bib116)]（即 SiamDW-SiamRPN [[154](#bib.bib154)]、SiamRPN++
    [[156](#bib.bib156)]、C-RPN [[150](#bib.bib150)]、SiamMask [[155](#bib.bib155)]
    和 DaSiamRPN [[104](#bib.bib104)]）具有分类和回归两个分支。此外，ATOM、DiMP50 和 PrDiMP50 使用分类网络来区分目标与背景，以及用于
    BB 回归的 IoU-Net。
- en: Based on the motivation categorization of the best trackers, the recent advanced
    methods rely on 1) alleviating the imbalanced distribution of visual training
    data by the data augmentation [[102](#bib.bib102), [104](#bib.bib104)] and generative
    network from adversarial learning [[114](#bib.bib114)], 2) efficient training
    and learning procedures by reformulating classification/regression problems [[102](#bib.bib102),
    [104](#bib.bib104), [114](#bib.bib114), [115](#bib.bib115), [119](#bib.bib119),
    [120](#bib.bib120), [148](#bib.bib148), [149](#bib.bib149), [159](#bib.bib159),
    [194](#bib.bib194)] and providing specified features for visual tracking [[60](#bib.bib60),
    [104](#bib.bib104), [106](#bib.bib106), [114](#bib.bib114), [130](#bib.bib130),
    [150](#bib.bib150), [154](#bib.bib154), [155](#bib.bib155), [156](#bib.bib156),
    [149](#bib.bib149), [159](#bib.bib159), [194](#bib.bib194)], 3) exploiting state-of-the-art
    architectures to provide more discriminative representations by leveraging ResNet
    models as the backbone networks [[102](#bib.bib102), [154](#bib.bib154), [155](#bib.bib155),
    [156](#bib.bib156), [149](#bib.bib149), [159](#bib.bib159), [194](#bib.bib194)],
    and 4) extracting complementary features by employing additional information such
    as contextual [[102](#bib.bib102), [104](#bib.bib104), [106](#bib.bib106), [154](#bib.bib154)]
    or temporal information [[104](#bib.bib104), [114](#bib.bib114), [115](#bib.bib115),
    [130](#bib.bib130)]. The VITAL, DaSiamRPN, and UPDT attempt to alleviate the imbalanced
    distribution of positive and negative training data samples and extract more discriminative
    features. The VITAL uses adversarial learning to augment positive samples and
    decrease simple negative ones and preserve the most discriminative and robust
    features during tracking. Furthermore, the DaSiamRPN utilizes both data augmentation
    and negative semantic samples to consider visual distractors and improve visual
    tracking robustness. The UPDT uses standard data augmentation and a quality measure
    for estimated states to fuse shallow and deep features effectively. Finally, the
    ATOM employs standard data augmentation to improve its online adaptation, while
    the DiMP50 & PrDiMP50 trackers enjoy meta-learning strategies to form their training
    set.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 基于最佳跟踪器的动机分类，近期的先进方法依赖于 1) 通过数据增强[[102](#bib.bib102), [104](#bib.bib104)]和对抗学习生成网络[[114](#bib.bib114)]来缓解视觉训练数据的分布不平衡，2)
    通过重新表述分类/回归问题[[102](#bib.bib102), [104](#bib.bib104), [114](#bib.bib114), [115](#bib.bib115),
    [119](#bib.bib119), [120](#bib.bib120), [148](#bib.bib148), [149](#bib.bib149),
    [159](#bib.bib159), [194](#bib.bib194)]和提供指定的特征用于视觉跟踪[[60](#bib.bib60), [104](#bib.bib104),
    [106](#bib.bib106), [114](#bib.bib114), [130](#bib.bib130), [150](#bib.bib150),
    [154](#bib.bib154), [155](#bib.bib155), [156](#bib.bib156), [149](#bib.bib149),
    [159](#bib.bib159), [194](#bib.bib194)]来实现高效的训练和学习过程，3) 利用最先进的架构，通过将 ResNet 模型作为骨干网络来提供更具辨别性的表示[[102](#bib.bib102),
    [154](#bib.bib154), [155](#bib.bib155), [156](#bib.bib156), [149](#bib.bib149),
    [159](#bib.bib159), [194](#bib.bib194)]，以及 4) 通过使用额外的信息如上下文[[102](#bib.bib102),
    [104](#bib.bib104), [106](#bib.bib106), [154](#bib.bib154)]或时间信息[[104](#bib.bib104),
    [114](#bib.bib114), [115](#bib.bib115), [130](#bib.bib130)]来提取互补特征。VITAL、DaSiamRPN
    和 UPDT 尝试缓解正负训练数据样本的分布不平衡，并提取更具辨别性的特征。VITAL 使用对抗学习来增强正样本，减少简单负样本，并在跟踪过程中保留最具辨别性和鲁棒性的特征。此外，DaSiamRPN
    利用数据增强和负语义样本来考虑视觉干扰物，并提高视觉跟踪的鲁棒性。UPDT 使用标准的数据增强和估计状态的质量度量来有效融合浅层和深层特征。最后，ATOM
    采用标准的数据增强来改善其在线适应能力，而 DiMP50 和 PrDiMP50 跟踪器则利用元学习策略来形成其训练集。
- en: To improve the learning process of the best DL-based methods, the ATOM, UPDT,
    DeepSTRCF, DRT, LSART, and ASRCF have revised the conventional ridge regression
    of DCF formulation. Moreover, the DaSiamRPN and VITAL utilize the distractor-aware
    objective function and reformulated objective function of GANs using a cost-sensitive
    loss to improve the training process of these visual trackers, respectively. Finally,
    the PrDiMP tracker computes the similarity of predictive and ground-truth distributions
    by Kullback-Leibler (KL) divergence. Training of DL-based methods on large-scale
    datasets adapts their network function for visual tracking. The SiamDW, SiamRPN++,
    and SiamMask methods have aimed to leverage state-of-the-art deep networks as
    a backbone network of Siamese trackers. The ATOM, DiMP50, and PrDiMP tracker employ
    ResNet blocks as the backbone network, while the DiMP & PrDiMP train these blocks
    on tracking datasets. While these methods exploit ResNet models, the SiamDW proposes
    new residual modules and architectures to prevent significant receptive field
    increase and simultaneously improve feature discriminability and localization
    accuracy. Also, the ResNet-driven SNN-based tracker proposed by the SiamRPN++
    includes different layer-wise and depth-wise aggregations to fill the performance
    gap between SNN-based and CNN-based methods. In addition to the spatial information,
    the DAT (using reciprocative learning) and DeepSTRCF (using online passive-aggressive
    (PA) learning) also consider temporal information in different ways to provide
    more robust features. Generally, six learning schemes of the similarity learning
    (i.e., SiamDW, SiamRPN++, C-RPN, StructSiam, SiamMask, DaSiamRPN, ATOM), bluemeta-learning
    (i.e., PrDiMP50, DiMP50), multi-domain learning (i.e., MDNet, DAT), adversarial
    learning (i.e., VITAL), spatial-aware regressions learning (i.e., LSART), and
    DCF learning are utilized.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进最佳基于深度学习的方法的学习过程，ATOM、UPDT、DeepSTRCF、DRT、LSART和ASRCF修订了传统的DCF公式中的岭回归。此外，DaSiamRPN和VITAL分别利用了考虑干扰物的目标函数和使用成本敏感损失重新制定的GAN目标函数，以改进这些视觉跟踪器的训练过程。最后，PrDiMP跟踪器通过Kullback-Leibler
    (KL)散度计算预测分布和真实分布的相似性。基于深度学习的方法在大规模数据集上的训练调整了其网络功能以用于视觉跟踪。SiamDW、SiamRPN++和SiamMask方法旨在利用最先进的深度网络作为Siamese跟踪器的骨干网络。ATOM、DiMP50和PrDiMP跟踪器采用ResNet块作为骨干网络，而DiMP
    & PrDiMP则在跟踪数据集上训练这些块。虽然这些方法利用ResNet模型，但SiamDW提出了新的残差模块和架构，以防止显著的感受野增加，同时提高特征的可区分性和定位精度。此外，SiamRPN++提出的基于ResNet的SNN跟踪器包括不同的层级和深度级聚合，以填补基于SNN的方法与基于CNN的方法之间的性能差距。除了空间信息外，DAT（使用递归学习）和DeepSTRCF（使用在线被动-攻击性（PA）学习）也以不同的方式考虑时间信息，以提供更强健的特征。通常，六种相似性学习方案（即SiamDW、SiamRPN++、C-RPN、StructSiam、SiamMask、DaSiamRPN、ATOM）、蓝色元学习（即PrDiMP50、DiMP50）、多领域学习（即MDNet、DAT）、对抗学习（即VITAL）、空间感知回归学习（即LSART）和DCF学习被广泛利用。
- en: 'In the following, the best visual tracking methods are studied based on their
    advantages and disadvantage. The ATOM, DiMP, and PrDiMP trackers consider visual
    tracking as two-step classification and target estimation procedures. These trackers
    are robust to handle CM, MC, SV, and ARC attributes by employing custom networks
    and elaborated optimization strategies. However, the SOB, LR, and OB attributes
    can dramatically impact their performances. Three SNN-based methods of the C-RPN,
    StructSiam, and DaSiamRPN exploit the shallow AlexNet as their backbone network
    (see Table [II](#S2.T2 "TABLE II ‣ II-C2 Only Offline Training ‣ II-C Network
    Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking:
    A Comprehensive Survey")), which is the main weakness of these trackers according
    to their discriminative power. To improve tracking robustness in the presence
    of significant SV and visual DI, the C-RPN cascades multiple RPNs in a Siamese
    network to exploit from hard negative sampling (to provide more balanced training
    samples), multi-level features, and multiple steps of regressions. To decrease
    the sensitivity of SNN-based methods specifically for non-rigid appearance change
    and POC attributes, the StructSiam detects contextual information of local patterns
    and their relationships and matches them by a Siamese network in real-time speed.
    By adopting the local-to-global search strategy and the non-maximum suppression
    (NMS) to re-detect target and reduce potential distractors, the DaSiamRPN correctly
    handles the FOC, OV, POC, and BC challenges. In contrast, the SiamMask, SiamDW-SiamRPN,
    and SiamRPN++ exploit the ResNet models. To rely on rich target representation,
    the SiamMask uses three-branch architecture to estimate the target location by
    a rotated BB, including the target’s binary mask. The most failure reasons for
    SiamMask are the MB & OV attributes that produce erroneous target masks. To reduce
    the performance margin of the SNN-based methods with state-of-the-art visual tracking
    methods, the SiamDW-SiamRPN and SiamRPN++ study the exploitation of deep backbone
    networks to reduce the sensitivity of these methods to the most challenging attributes.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '在以下内容中，最佳的视觉跟踪方法将根据其优缺点进行研究。ATOM、DiMP 和 PrDiMP 跟踪器将视觉跟踪视为两步分类和目标估计程序。这些跟踪器通过采用自定义网络和精细化的优化策略，能够处理
    CM、MC、SV 和 ARC 属性。然而，SOB、LR 和 OB 属性可能会显著影响它们的性能。基于 SNN 的三种方法 C-RPN、StructSiam
    和 DaSiamRPN 利用浅层的 AlexNet 作为其骨干网络（见表 [II](#S2.T2 "TABLE II ‣ II-C2 Only Offline
    Training ‣ II-C Network Training ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning
    for Visual Tracking: A Comprehensive Survey")），根据它们的区分能力，这是这些跟踪器的主要弱点。为了提高在显著
    SV 和视觉 DI 存在下的跟踪鲁棒性，C-RPN 在一个 Siamese 网络中级联多个 RPN，以利用困难的负样本（以提供更平衡的训练样本）、多级特征和多个回归步骤。为了减少
    SNN 基方法对非刚性外观变化和 POC 属性的敏感性，StructSiam 检测局部模式及其关系的上下文信息，并通过 Siamese 网络以实时速度进行匹配。通过采用局部到全局的搜索策略和非最大抑制（NMS）重新检测目标并减少潜在干扰物，DaSiamRPN
    能够正确处理 FOC、OV、POC 和 BC 挑战。相比之下，SiamMask、SiamDW-SiamRPN 和 SiamRPN++ 利用 ResNet 模型。为了依赖丰富的目标表示，SiamMask
    使用三分支结构通过旋转的 BB 来估计目标位置，包括目标的二进制掩模。SiamMask 失败的主要原因是 MB 和 OV 属性，这会产生错误的目标掩模。为了减少
    SNN 基方法与最先进的视觉跟踪方法的性能差距，SiamDW-SiamRPN 和 SiamRPN++ 研究了深度骨干网络的利用，以降低这些方法对最具挑战性属性的敏感性。'
- en: The MDNet and the other methods based on it (e.g., DAT) are still among the
    best visual tracking methods. Because of specialized offline and online training
    of these networks on large-scale visual tracking datasets, these methods can handle
    various challenging situations, hardly miss the visual targets, and have a satisfactory
    performance to track LR targets. However, these methods suffer from high computational
    complexity, intra-class discrimination of targets with similar semantics, and
    performing discrete space for scale estimation. The VITAL can tolerate massive
    DEF, IPR, and OPR because it focuses on hard negative samples through high-order
    cost-sensitive loss. However, it does not have a robust performance in the case
    of significant SV due to the producing a fixed size of weight mask via a generative
    network. The LSART utilizes the modified Kernelized ridge regression (KRR) by
    the weighted combination of patch-wise similarities to concentrate on the target’s
    reliable regions. Due to the consideration of rotation information and online
    adaptation of CNN models, this method provides promising responses to tackle the
    DEF and IPR challenges.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: MDNet及其基于的方法（例如DAT）仍然是最佳的视觉跟踪方法之一。由于这些网络在大规模视觉跟踪数据集上的专门离线和在线训练，这些方法可以处理各种具有挑战性的情况，几乎不会错过视觉目标，并且在跟踪LR目标方面表现令人满意。然而，这些方法存在高计算复杂性、类似语义目标的类内区分问题以及在尺度估计时执行离散空间的缺陷。VITAL可以容忍大量DEF、IPR和OPR，因为它通过高阶代价敏感损失关注困难的负样本。然而，由于通过生成网络产生固定大小的权重掩码，它在显著SV情况下的表现并不稳健。LSART利用加权组合的补丁级相似性来集中目标的可靠区域，修改了的核岭回归（KRR）。由于考虑了旋转信息和CNN模型的在线适应性，这种方法对应对DEF和IPR挑战提供了有前途的响应。
- en: '![Refer to caption](img/48bfb96abe15bf1c32eceaefbbd869bc.png)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/48bfb96abe15bf1c32eceaefbbd869bc.png)'
- en: 'Figure 7: Qualitative comparison of state-of-the-art visual trackers on the
    BMX, Gymnastics3, and Singer3 video sequences from VOT2018 dataset. The #frame
    number and annotated attributes are shown on each frame.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在VOT2018数据集中对BMX、Gymnastics3和Singer3视频序列的最先进视觉跟踪器进行的定性比较。每一帧上都显示了#帧数和注释属性。
- en: 'The DeepSTRCF, ASRCF, DRT, and UPDT are the DCF-based methods that exploit
    deep off-the-shelf features and fuse them with shallow ones (e.g., HOG and CN)
    to improve the robustness of visual tracking (see Table [I](#S2.T1 "TABLE I ‣
    II-B1 Model Reuse or Deep Off-the-Shelf Features ‣ II-B Network Exploitation ‣
    II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")). To reduce the adverse impact of the OCC and OV attributes, the DeepSTRCF
    adds a temporal regularization term to the spatially regularized DCF formulation.
    The revisited formulation helps the DeepSTRCF enduring some appearance variations
    such as the IV, IPR, OPR, and POC. Using object-aware spatial regularization and
    reliability terms, the ASRCF and DRT methods attempt to optimize models to effectively
    learn adaptive correlation filters. Both these methods have studied major imperfections
    of DCF-based methods such as circular shifted sampling process, same feature space
    for localization and scale estimation processes, the strict focus on discrimination,
    and sparse and non-uniform distribution of correlation responses. Hence, these
    methods handle the DEF, BC, and SV, suitably. Finally, the UPDT focuses on enhancing
    the visual tracking robustness through independently training a shallow feature-based
    DCF and a deep off-the-shelf feature-based DCF and considering augmented training
    samples with an adaptive fusion model. Although these methods demonstrate the
    competitive performance of well-designed DCF-based trackers compared to more sophisticated
    trackers, they suffer from the limitations of pre-trained models, aspect ratio
    variation, model degradation, and considerable appearance variation.'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 'DeepSTRCF、ASRCF、DRT 和 UPDT 是基于 DCF 的方法，这些方法利用深度现成特征，并将其与浅层特征（如 HOG 和 CN）融合，以提高视觉跟踪的鲁棒性（见表
    [I](#S2.T1 "TABLE I ‣ II-B1 Model Reuse or Deep Off-the-Shelf Features ‣ II-B
    Network Exploitation ‣ II Deep Visual Tracking Taxonomy ‣ Deep Learning for Visual
    Tracking: A Comprehensive Survey")）。为了减少 OCC 和 OV 属性的不利影响，DeepSTRCF 在空间正则化的 DCF
    公式中添加了时间正则化项。修订后的公式帮助 DeepSTRCF 处理一些外观变化，例如 IV、IPR、OPR 和 POC。通过使用对象感知空间正则化和可靠性项，ASRCF
    和 DRT 方法尝试优化模型，以有效学习自适应相关滤波器。这两种方法研究了基于 DCF 方法的主要缺陷，例如循环偏移采样过程、定位和尺度估计过程中的相同特征空间、严格关注区分性，以及相关响应的稀疏和非均匀分布。因此，这些方法适当地处理了
    DEF、BC 和 SV。最后，UPDT 通过独立训练基于浅层特征的 DCF 和基于深层现成特征的 DCF，并考虑了具有自适应融合模型的增强训练样本，重点提升了视觉跟踪的鲁棒性。尽管这些方法展示了设计良好的基于
    DCF 的跟踪器相较于更复杂的跟踪器的竞争性能，但它们仍然面临预训练模型、长宽比变化、模型退化和显著外观变化的限制。'
- en: 'Finally, we have modified the VOT toolkit to be able to compare state-of-the-art
    visual trackers qualitatively. Fig. [7](#S4.F7 "Figure 7 ‣ IV-C Discussion ‣ IV
    Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive Survey")
    shows the tracking results of the SiamRPN++ [[156](#bib.bib156)], SiamMask [[155](#bib.bib155)],
    LSART [[120](#bib.bib120)], UPDT [[102](#bib.bib102)], ATOM [[149](#bib.bib149)],
    and DiMP50 [[159](#bib.bib159)] on some video sequences of the VOT2018 dataset
    (modified toolkit & all videos are publicly available on the aforementioned page).
    According to the achieved results, the DiMP50, ATOM, and SiamRPN++ have provided
    the best results. However, failures usually happen when multiple critical attributes
    simultaneously occur in a scene. For instance, the SiamMask misuses the semi-supervised
    video object segmentation when the OCC and SV co-occur, or the significant SV
    dramatically reduces the performance of the SiamRPN++. Despite considerable advances
    that are emerged in visual tracking, the state-of-the-art visual trackers are
    still unable to handle serious real-world challenges; severe variations of target
    appearance, MOC, OCC, SV, CM, DEF, and even IV can have drastic effects on the
    performance, which may lead to tracking failures. These results demonstrate that
    the visual trackers are still not completely reliable for real-world applications
    because they lack the intelligence for scene understanding. Current trackers improve
    object-scene distinction, but they cannot infer scene information, immediately
    recognize the global/configural structure of a scene, or organize purposeful decisions
    based on space and acts within.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们修改了VOT工具包，使其能够定性地比较最先进的视觉跟踪器。图[7](#S4.F7 "Figure 7 ‣ IV-C Discussion ‣
    IV Experimental Analyses ‣ Deep Learning for Visual Tracking: A Comprehensive
    Survey")展示了SiamRPN++ [[156](#bib.bib156)]、SiamMask [[155](#bib.bib155)]、LSART
    [[120](#bib.bib120)]、UPDT [[102](#bib.bib102)]、ATOM [[149](#bib.bib149)]和DiMP50
    [[159](#bib.bib159)]在VOT2018数据集的一些视频序列上的跟踪结果（修改后的工具包及所有视频均可在上述页面公开获取）。根据结果，DiMP50、ATOM和SiamRPN++提供了最佳结果。然而，当场景中同时出现多个关键属性时，失败通常会发生。例如，当OCC和SV同时出现时，SiamMask错误地使用了半监督视频对象分割，或者显著的SV显著降低了SiamRPN++的性能。尽管视觉跟踪取得了显著进展，但最先进的视觉跟踪器仍无法处理严重的现实世界挑战；目标外观的严重变化、MOC、OCC、SV、CM、DEF，甚至IV可能对性能产生巨大影响，可能导致跟踪失败。这些结果表明，视觉跟踪器在现实世界应用中仍然不完全可靠，因为它们缺乏对场景的智能理解。当前的跟踪器改善了物体-场景区分，但无法推断场景信息、立即识别场景的全局/配置结构，或基于空间和行动做出有目的的决策。'
- en: V Conclusion and Future Directions
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论与未来方向
- en: The state-of-the-art DL-based visual trackers were categorized into a comprehensive
    taxonomy based on network architecture, network exploitation, training, network
    objective, network output, the exploitation of correlation filter advantages,
    aerial-view tracking, long-term tracking, and online tracking. Moreover, the motivations
    and contributions of these methods were categorized according to the main problems
    and proposed solutions of DL-based trackers. Furthermore, almost all visual tracking
    benchmark datasets and evaluation metrics were briefly investigated, and the various
    state-of-the-art trackers were compared on seven visual tracking datasets.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的基于深度学习的视觉跟踪器被归类到一个全面的分类法中，基于网络架构、网络利用、训练、网络目标、网络输出、相关滤波器优势的利用、航拍跟踪、长期跟踪和在线跟踪。此外，这些方法的动机和贡献根据深度学习跟踪器的主要问题和提出的解决方案进行了分类。此外，几乎所有视觉跟踪基准数据集和评估指标都进行了简要调查，并在七个视觉跟踪数据集上比较了各种最先进的跟踪器。
- en: Recently, the DL-based visual tracking methods have investigated different exploitation
    of deep off-she-shelf features, fusion of deep features & hand-crafted features,
    various architectures & backbone networks, offline & online training of DNNs on
    large-scale datasets, update schemes, search strategies, contextual information,
    temporal information, and how to deal with lacking training data. However, many
    problems are not precisely solved, and also other problems need to be explored
    in the future. In the following, some of these future directions are presented
    for more investigation.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于深度学习的视觉跟踪方法探讨了深度离线特征的不同利用方式、深度特征与手工特征的融合、各种架构与骨干网络、大规模数据集上深度神经网络的离线与在线训练、更新方案、搜索策略、上下文信息、时间信息以及如何应对缺乏训练数据的问题。然而，许多问题尚未得到精确解决，还有其他问题需要在未来进一步探索。以下是一些未来方向的展示，以供进一步研究。
- en: First, the main concentration is to design custom neural networks to provide
    robustness, accuracy, and efficiency simultaneously. These trackers are primarily
    developed by integrating efficient network architectures with either classification
    & regression branches or two-step classification & BB refinement networks. Most
    recent works do not re-train/fine-tune their backbone networks to exploit generic
    features and avoid catastrophic forgetting of general patterns. However, diverse
    machine learning-based techniques to address this issue have been proposed, such
    as incremental learning [[256](#bib.bib256)], transfer learning penalties [[257](#bib.bib257)],
    batch spectral shrinkage [[258](#bib.bib258)], or lifelong learning [[259](#bib.bib259)].
    Thus, effective training of backbone networks can boost tracking performance.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，主要关注的是设计定制的神经网络，以同时提供鲁棒性、准确性和效率。这些追踪器主要通过将高效的网络架构与分类和回归分支或两步分类与边界框（BB）细化网络相结合来开发。最近的大多数工作没有重新训练/微调其主干网络以利用通用特征并避免对一般模式的灾难性遗忘。然而，已经提出了多种基于机器学习的技术来解决这个问题，如增量学习
    [[256](#bib.bib256)]、迁移学习惩罚 [[257](#bib.bib257)]、批量谱缩减 [[258](#bib.bib258)] 或终身学习
    [[259](#bib.bib259)]。因此，有效的主干网络训练可以提升追踪性能。
- en: Second, generic visual trackers are required to adapt to unseen targets quickly.
    Hence, efficient online training of neural networks is crucial. Recently, meta-/few-shot
    learning approaches are mainly used to find an optimal initialization of the base
    learner to a new target. But, the meta-networks need to be shallow to avoid over-fitting
    problems. Therefore, exploring effective few-shot learning approaches provides
    fast convergence of deeper networks.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，通用视觉追踪器需要快速适应未见过的目标。因此，高效的在线神经网络训练至关重要。最近，元学习/少样本学习方法主要用于找到基学习器对新目标的最佳初始化。然而，元网络需要保持浅层以避免过拟合问题。因此，探索有效的少样本学习方法能够实现更深网络的快速收敛。
- en: Third, tracking from aerial-views introduces additional challenges for visual
    tracking. For instance, small/tiny object tracking in videos captured from medium/high-altitudes,
    severe viewpoint changes, and tracking many targets in dense environments should
    be considered. Furthermore, these scenarios are consistently involved with out-of-view
    and large occlusions; thus, developing long-term approaches will help more reliable
    aerial-view trackers.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，从空中视角进行追踪带来了额外的视觉追踪挑战。例如，应考虑在中高空拍摄的视频中追踪小型/微型物体、严重的视角变化以及在密集环境中追踪多个目标。此外，这些场景通常涉及视野外和大范围遮挡；因此，开发长期方法将有助于提高空中视角追踪器的可靠性。
- en: Fourth, long-term trackers are overlooked despite many advances in short-term
    trackers. In fact, long-term trackers are closer to practical, real-world scenarios
    when the target may disappear frequently or occlude for a long time. These trackers
    should have the ability to re-detect the target once a failure occurs and then
    continue tracking the correct target during video sequences. Thus, compelling
    detection & verification networks are needed to be designed.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，尽管短期追踪器取得了许多进展，长期追踪器却被忽视。实际上，长期追踪器更贴近实际的真实场景，因为目标可能会频繁消失或长时间被遮挡。这些追踪器应具备在出现失败后重新检测目标的能力，然后在视频序列中继续追踪正确的目标。因此，需要设计强大的检测与验证网络。
- en: Finally, existing visual trackers have a deficiency in scene understanding.
    The state-of-the-art methods cannot interpret dynamic scenes in a meaningful way,
    immediately recognize global structures, infer existing objects, and perceive
    basic level categories of different objects or events. Although recent trackers
    desirably reduce the computational complexity, these trackers can be modified
    to employ complementary features (e.g., temporal information) and incorporate
    proposed adversarial learning contributions in this few-data regime task.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，现有的视觉追踪器在场景理解方面存在不足。最先进的方法不能以有意义的方式解释动态场景，无法立即识别全局结构、推断现有对象以及感知不同对象或事件的基本类别。尽管最近的追踪器在减少计算复杂性方面表现良好，但这些追踪器可以通过利用补充特征（例如，时间信息）和结合提出的对抗学习贡献来进行修改，以适应这一少数据任务。
- en: Acknowledgments
  id: totrans-516
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We wish to thank Prof. Kamal Nasrollahi (Visual Analysis of People Lab (VAP),
    Aalborg University, Denmark) for his beneficial comments.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢**Kamal Nasrollahi**教授（丹麦奥尔堡大学（VAP）视觉分析实验室）提供的宝贵意见。
- en: References
  id: totrans-518
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. Gao, L. Jin, Y. Jiang, and B. Guo, “Manifold Siamese network: A novel
    visual tracking ConvNet for autonomous vehicles,” *IEEE Trans. Intell. Transp.
    Sys.*, pp. 1–12, 2019.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. Gao, L. Jin, Y. Jiang, 和 B. Guo， “流形Siamese网络：一种用于自动驾驶车辆的新型视觉跟踪卷积网络”，*IEEE智能交通系统汇刊*，页
    1–12，2019。'
- en: '[2] C. Robin and S. Lacroix, “Multi-robot target detection and tracking: Taxonomy
    and survey,” *Autonomous Robots*, vol. 40, no. 4, pp. 729–760, 2016.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] C. Robin 和 S. Lacroix， “多机器人目标检测与跟踪：分类与综述”，*自主机器人*，第 40 卷，第 4 期，页 729–760，2016。'
- en: '[3] K. Lee, J. Hwang, G. Okopal, and J. Pitton, “Ground-moving-platform-based
    human tracking using visual SLAM and constrained multiple kernels,” *IEEE Trans.
    Intell. Transp. Sys.*, vol. 17, no. 12, pp. 3602–3612, 2016.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] K. Lee, J. Hwang, G. Okopal, 和 J. Pitton， “基于地面移动平台的人体跟踪，使用视觉SLAM和约束的多核”，*IEEE智能交通系统汇刊*，第
    17 卷，第 12 期，页 3602–3612，2016。'
- en: '[4] F. Ababsa, M. Maidi, J. Y. Didier, and M. Mallem, “Vision-based tracking
    for mobile augmented reality,” in *Studies in Computational Intelligence*.   Springer,
    2008, vol. 120, pp. 297–326.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] F. Ababsa, M. Maidi, J. Y. Didier, 和 M. Mallem， “基于视觉的移动增强现实跟踪”，见 *计算智能研究*。Springer，2008，第
    120 卷，页 297–326。'
- en: '[5] J. Hao, Y. Zhou, G. Zhang, Q. Lv, and Q. Wu, “A review of target tracking
    algorithm based on UAV,” in *Proc. IEEE CBS*, 2019, pp. 328–333.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Hao, Y. Zhou, G. Zhang, Q. Lv, 和 Q. Wu， “基于无人机的目标跟踪算法综述”，见 *IEEE CBS会议录*，2019，页
    328–333。'
- en: '[6] M. Manafifard, H. Ebadi, and H. Abrishami Moghaddam, “A survey on player
    tracking in soccer videos,” *Comput. Vis. Image Und.*, vol. 159, pp. 19–46, 2017.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Manafifard, H. Ebadi, 和 H. Abrishami Moghaddam， “关于足球视频中球员跟踪的调查”，*计算机视觉与图像理解*，第
    159 卷，页 19–46，2017。'
- en: '[7] D. Bouget, M. Allan, D. Stoyanov, and P. Jannin, “Vision-based and marker-less
    surgical tool detection and tracking: A review of the literature,” *Medical Image
    Analysis*, vol. 35, pp. 633–654, 2017.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. Bouget, M. Allan, D. Stoyanov, 和 P. Jannin， “基于视觉和无标记的手术工具检测与跟踪：文献综述”，*医学图像分析*，第
    35 卷，页 633–654，2017。'
- en: '[8] V. Ulman, M. Maška, and et al., “An objective comparison of cell-tracking
    algorithms,” *Nature Methods*, vol. 14, no. 12, pp. 1141–1152, 2017.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] V. Ulman, M. Maška, 等， “细胞跟踪算法的客观比较”，*自然方法*，第 14 卷，第 12 期，页 1141–1152，2017。'
- en: '[9] J. Luo, Y. Han, and L. Fan, “Underwater acoustic target tracking: A review,”
    *Sensors*, vol. 18, no. 1, p. 112, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] J. Luo, Y. Han, 和 L. Fan， “水下声学目标跟踪：综述”，*传感器*，第 18 卷，第 1 期，页 112，2018。'
- en: '[10] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista, “High-speed tracking
    with kernelized correlation filters,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 37, no. 3, pp. 583–596, 2015.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. F. Henriques, R. Caseiro, P. Martins, 和 J. Batista， “通过核化相关滤波器进行高速跟踪”，*IEEE模式分析与机器智能汇刊*，第
    37 卷，第 3 期，页 583–596，2015。'
- en: '[11] G. Ding, W. Chen, S. Zhao, J. Han, and Q. Liu, “Real-time scalable visual
    tracking via quadrangle kernelized correlation filters,” *IEEE Trans. Intell.
    Transp. Sys.*, vol. 19, no. 1, pp. 140–150, 2018.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] G. Ding, W. Chen, S. Zhao, J. Han, 和 Q. Liu， “通过四边形核化相关滤波器实现实时可扩展视觉跟踪”，*IEEE智能交通系统汇刊*，第
    19 卷，第 1 期，页 140–150，2018。'
- en: '[12] S. M. Marvasti-Zadeh, H. Ghanei-Yakhdan, and S. Kasaei, “Rotation-aware
    discriminative scale space tracking,” in *Iranian Conf. Electrical Engineering
    (ICEE)*, 2019, pp. 1272–1276.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. M. Marvasti-Zadeh, H. Ghanei-Yakhdan, 和 S. Kasaei， “旋转感知的判别尺度空间跟踪”，见
    *伊朗电气工程会议（ICEE）*，2019，页 1272–1276。'
- en: '[13] S. M. Marvasti-Zadeh, H. Ghanei Yakhdan, and S. Kasaei, “Adaptive exploitation
    of pre-trained deep convolutional neural networks for robust visual tracking,”
    *Multimedia Tools and Applications*, 2021.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. M. Marvasti-Zadeh, H. Ghanei Yakhdan, 和 S. Kasaei， “自适应利用预训练深度卷积神经网络进行稳健视觉跟踪”，*多媒体工具与应用*，2021。'
- en: '[14] S. M. Marvasti Zadeh, H. Ghanei-Yakhdan, and S. Kasaei, “Beyond background-aware
    correlation filters: Adaptive context modeling by hand-crafted and deep rgb features
    for visual tracking,” 2020\. [Online]. Available: [http://arxiv.org/abs/2004.02932](http://arxiv.org/abs/2004.02932)'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] S. M. Marvasti Zadeh, H. Ghanei-Yakhdan, 和 S. Kasaei， “超越背景感知相关滤波器：通过手工设计和深度RGB特征进行视觉跟踪的自适应上下文建模”，2020。
    [在线] 可用：[http://arxiv.org/abs/2004.02932](http://arxiv.org/abs/2004.02932)'
- en: '[15] S. M. Marvasti-Zadeh, H. Ghanei Yakhdan, and S. Kasaei, “Efficient scale
    estimation methods using lightweight deep convolutional neural networks for visual
    tracking,” *Neural Computing and Applications*, 2021.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. M. Marvasti-Zadeh, H. Ghanei Yakhdan, 和 S. Kasaei， “使用轻量级深度卷积神经网络的高效尺度估计方法用于视觉跟踪”，*神经计算与应用*，2021。'
- en: '[16] S. M. Marvasti-Zadeh, H. Ghanei-Yakhdan, S. Kasaei, K. Nasrollahi, and
    T. B. Moeslund, “Effective fusion of deep multitasking representations for robust
    visual tracking,” 2020\. [Online]. Available: [http://arxiv.org/abs/2004.01382](http://arxiv.org/abs/2004.01382)'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. M. Marvasti-Zadeh, H. Ghanei-Yakhdan, S. Kasaei, K. Nasrollahi 和 T.
    B. Moeslund，“深度多任务表示的有效融合用于稳健视觉跟踪”，2020年。[在线]. 可用: [http://arxiv.org/abs/2004.01382](http://arxiv.org/abs/2004.01382)'
- en: '[17] C. Xiao and A. Yilmaz, “Efficient tracking with distinctive target colors
    and silhouette,” in *Proc. ICPR*, 2016, pp. 2728–2733.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] C. Xiao 和 A. Yilmaz，“使用显著目标颜色和轮廓进行高效跟踪”，在 *Proc. ICPR*，2016，第2728–2733页。'
- en: '[18] V. Bruni and D. Vitulano, “An improvement of kernel-based object tracking
    based on human perception,” *IEEE Trans. Syst., Man, Cybern. Syst.*, vol. 44,
    no. 11, pp. 1474–1485, 2014.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] V. Bruni 和 D. Vitulano，“基于人类感知的核函数对象跟踪的改进”，*IEEE Trans. Syst., Man, Cybern.
    Syst.*，第44卷，第11期，第1474–1485页，2014年。'
- en: '[19] I. I. Lychkov, A. N. Alfimtsev, and S. A. Sakulin, “Tracking of moving
    objects with regeneration of object feature points,” in *Proc. GloSIC*, 2018,
    pp. 1–6.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] I. I. Lychkov, A. N. Alfimtsev 和 S. A. Sakulin，“带有物体特征点再生的移动物体跟踪”，在 *Proc.
    GloSIC*，2018，第1–6页。'
- en: '[20] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,”
    in *Proc. IEEE CVPR*, 2005, pp. 886–893.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] N. Dalal 和 B. Triggs，“用于人类检测的方向梯度直方图”，在 *Proc. IEEE CVPR*，2005，第886–893页。'
- en: '[21] J. Van De Weijer, C. Schmid, and J. Verbeek, “Learning color names from
    real-world images,” in *Proc. IEEE CVPR*, 2007, pp. 1–8.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Van De Weijer, C. Schmid 和 J. Verbeek，“从现实世界图像中学习颜色名称”，在 *Proc. IEEE
    CVPR*，2007，第1–8页。'
- en: '[22] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg, “Learning spatially
    regularized correlation filters for visual tracking,” in *Proc. IEEE ICCV*, 2015,
    pp. 4310–4318.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] M. Danelljan, G. Hager, F. S. Khan 和 M. Felsberg，“学习空间正则化的相关滤波器用于视觉跟踪”，在
    *Proc. IEEE ICCV*，2015，第4310–4318页。'
- en: '[23] M. Danelljan, G. Häger, F. S. Khan, and M. Felsberg, “Adaptive decontamination
    of the training set: A unified formulation for discriminative visual tracking,”
    in *Proc. IEEE CVPR*, 2016, pp. 1430–1438.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] M. Danelljan, G. Häger, F. S. Khan 和 M. Felsberg，“训练集的自适应去污：一种用于区分视觉跟踪的统一公式”，在
    *Proc. IEEE CVPR*，2016，第1430–1438页。'
- en: '[24] H. K. Galoogahi, A. Fagg, and S. Lucey, “Learning background-aware correlation
    filters for visual tracking,” in *Proc. IEEE ICCV*, 2017, pp. 1144–1152.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] H. K. Galoogahi, A. Fagg 和 S. Lucey，“学习背景感知的相关滤波器用于视觉跟踪”，在 *Proc. IEEE
    ICCV*，2017，第1144–1152页。'
- en: '[25] Y. Li, C. Fu, F. Ding, Z. Huang, and G. Lu, “AutoTrack: Towards high-performance
    visual tracking for UAV with automatic spatio-temporal regularization,” in *Proc.
    IEEE CVPR*, 2020.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Li, C. Fu, F. Ding, Z. Huang 和 G. Lu，“AutoTrack：面向无人机的高性能视觉跟踪及自动时空正则化”，在
    *Proc. IEEE CVPR*，2020。'
- en: '[26] Z. Huang, C. Fu, Y. Li, F. Lin, and P. Lu, “Learning aberrance repressed
    correlation filters for real-time UAV tracking,” in *Proc. IEEE ICCV*, 2019, pp.
    2891–2900.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Z. Huang, C. Fu, Y. Li, F. Lin 和 P. Lu，“为实时无人机跟踪学习抑制异常的相关滤波器”，在 *Proc.
    IEEE ICCV*，2019，第2891–2900页。'
- en: '[27] F. Li, C. Fu, F. Lin, Y. Li, and P. Lu, “Training-set distillation for
    real-time UAV object tracking,” in *Proc. ICRA*, 2020, pp. 1–7.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] F. Li, C. Fu, F. Lin, Y. Li 和 P. Lu，“用于实时无人机目标跟踪的训练集蒸馏”，在 *Proc. ICRA*，2020，第1–7页。'
- en: '[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification
    with deep convolutional neural networks,” in *Proc. NIPS*, vol. 2, 2012, pp. 1097–1105.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] A. Krizhevsky, I. Sutskever 和 G. E. Hinton，“使用深度卷积神经网络进行ImageNet分类”，在
    *Proc. NIPS*，第2卷，2012，第1097–1105页。'
- en: '[29] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the
    devil in the details: Delving deep into convolutional nets,” in *Proc. BMVC*,
    2014, pp. 1–11.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] K. Chatfield, K. Simonyan, A. Vedaldi 和 A. Zisserman，“细节中的魔鬼再现：深入卷积网络”，在
    *Proc. BMVC*，2014，第1–11页。'
- en: '[30] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *Proc. ICLR*, 2014, pp. 1–14.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] K. Simonyan 和 A. Zisserman，“用于大规模图像识别的非常深卷积网络”，在 *Proc. ICLR*，2014，第1–14页。'
- en: '[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proc. IEEE
    CVPR*, 2015, pp. 1–9.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke 和 A. Rabinovich，“深入卷积网络的研究”，在 *Proc. IEEE CVPR*，2015，第1–9页。'
- en: '[32] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proc. IEEE CVPR*, 2016, pp. 770–778.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] K. He, X. Zhang, S. Ren 和 J. Sun，“用于图像识别的深度残差学习”，在 *Proc. IEEE CVPR*，2016，第770–778页。'
- en: '[33] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet large
    scale visual recognition challenge,” *IJCV*, vol. 115, no. 3, pp. 211–252, 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, 和 L. Fei-Fei，“ImageNet 大规模视觉识别挑战，”
    *IJCV*，第115卷，第3期，第211–252页，2015年。'
- en: '[34] M. Kristan, R. Pflugfelder, A. Leonardis, J. Matas, F. Porikli, and et al.,
    “The visual object tracking VOT2013 challenge results,” in *Proc. ICCV*, 2013,
    pp. 98–111.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] M. Kristan, R. Pflugfelder, A. Leonardis, J. Matas, F. Porikli 等，“视觉目标跟踪
    VOT2013 挑战结果，”发表于 *Proc. ICCV*，2013年，第98–111页。'
- en: '[35] M. Kristan, R. Pflugfelder, A. Leonardis, J. Matas, and et al., “The visual
    object tracking VOT2014 challenge results,” in *Proc. ECCV*, 2015, pp. 191–217.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] M. Kristan, R. Pflugfelder, A. Leonardis, J. Matas 等，“视觉目标跟踪 VOT2014 挑战结果，”发表于
    *Proc. ECCV*，2015年，第191–217页。'
- en: '[36] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, and et al., “The visual
    object tracking VOT2015 challenge results,” in *Proc. IEEE ICCV*, 2015, pp. 564–586.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] M. Kristan, J. Matas, A. Leonardis, M. Felsberg 等，“视觉目标跟踪 VOT2015 挑战结果，”发表于
    *Proc. IEEE ICCV*，2015年，第564–586页。'
- en: '[37] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, R. Pflugfelder, and et al.,
    “The visual object tracking VOT2016 challenge results,” in *Proc. ECCVW*, 2016,
    pp. 777–823.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] M. Kristan, J. Matas, A. Leonardis, M. Felsberg, R. Pflugfelder 等，“视觉目标跟踪
    VOT2016 挑战结果，”发表于 *Proc. ECCVW*，2016年，第777–823页。'
- en: '[38] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pflugfelder, L. C.
    Zajc, and et al., “The visual object tracking VOT2017 challenge results,” in *Proc.
    IEEE ICCVW*, 2017, pp. 1949–1972.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pflugfelder, L. C.
    Zajc 等，“视觉目标跟踪 VOT2017 挑战结果，”发表于 *Proc. IEEE ICCVW*，2017年，第1949–1972页。'
- en: '[39] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pflugfelder, and et al.,
    “The sixth visual object tracking VOT2018 challenge results,” in *Proc. ECCVW*,
    2019, pp. 3–53.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pflugfelder 等，“第六届视觉目标跟踪
    VOT2018 挑战结果，”发表于 *Proc. ECCVW*，2019年，第3–53页。'
- en: '[40] M. Kristan and et al., “The seventh visual object tracking VOT2019 challenge
    results,” in *Proc. ICCVW*, 2019.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M. Kristan 等，“第七届视觉目标跟踪 VOT2019 挑战结果，”发表于 *Proc. ICCVW*，2019年。'
- en: '[41] A. Yilmaz, O. Javed, and M. Shah, “Object tracking: A survey,” *ACM Computing
    Surveys*, vol. 38, no. 4, Dec. 2006.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. Yilmaz, O. Javed, 和 M. Shah，“目标跟踪：综述，” *ACM Computing Surveys*，第38卷，第4期，2006年12月。'
- en: '[42] A. W. Smeulders, D. M. Chu, R. Cucchiara, S. Calderara, A. Dehghan, and
    M. Shah, “Visual tracking: An experimental survey,” *IEEE Trans. Pattern Anal.
    Mach. Intell.*, vol. 36, no. 7, pp. 1442–1468, 2014.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. W. Smeulders, D. M. Chu, R. Cucchiara, S. Calderara, A. Dehghan, 和
    M. Shah，“视觉跟踪：实验性调查，” *IEEE Trans. Pattern Anal. Mach. Intell.*，第36卷，第7期，第1442–1468页，2014年。'
- en: '[43] H. Yang, L. Shao, F. Zheng, L. Wang, and Z. Song, “Recent advances and
    trends in visual tracking: A review,” *Neurocomputing*, vol. 74, no. 18, pp. 3823–3831,
    2011.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] H. Yang, L. Shao, F. Zheng, L. Wang, 和 Z. Song，“视觉跟踪的近期进展和趋势：综述，” *Neurocomputing*，第74卷，第18期，第3823–3831页，2011年。'
- en: '[44] X. Li, W. Hu, C. Shen, Z. Zhang, A. Dick, and A. Van Den Hengel, “A survey
    of appearance models in visual object tracking,” *ACM Trans. Intell. Syst. Tec.*,
    vol. 4, no. 4, pp. 58:1—-58:48, 2013.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] X. Li, W. Hu, C. Shen, Z. Zhang, A. Dick, 和 A. Van Den Hengel，“视觉目标跟踪中的外观模型综述，”
    *ACM Trans. Intell. Syst. Tec.*，第4卷，第4期，第58:1—58:48页，2013年。'
- en: '[45] C. Fu, Z. Huang, Y. Li, R. Duan, and P. Lu, “Boundary effect-aware visual
    tracking for UAV with online enhanced background learning and multi-frame consensus
    verification,” in *Proc. IROS*, 2019, pp. 4415–4422.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C. Fu, Z. Huang, Y. Li, R. Duan, 和 P. Lu，“考虑边界效应的无人机视觉跟踪，结合在线增强背景学习和多帧共识验证，”发表于
    *Proc. IROS*，2019年，第4415–4422页。'
- en: '[46] Y. Li, C. Fu, Z. Huang, Y. Zhang, and J. Pan, “Keyfilter-aware real-time
    uav object tracking,” in *Proc. ICRA*, 2020.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. Li, C. Fu, Z. Huang, Y. Zhang, 和 J. Pan，“关键滤波器感知的实时无人机目标跟踪，”发表于 *Proc.
    ICRA*，2020年。'
- en: '[47] M. Fiaz, A. Mahmood, and S. K. Jung, “Tracking noisy targets: A review
    of recent object tracking approaches,” 2018\. [Online]. Available: [http://arxiv.org/abs/1802.03098](http://arxiv.org/abs/1802.03098)'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] M. Fiaz, A. Mahmood, 和 S. K. Jung，“跟踪噪声目标：近期目标跟踪方法的综述，”2018年。[在线] 可用:
    [http://arxiv.org/abs/1802.03098](http://arxiv.org/abs/1802.03098)'
- en: '[48] M. Fiaz, A. Mahmood, S. Javed, and S. K. Jung, “Handcrafted and deep trackers:
    Recent visual object tracking approaches and trends,” *ACM Computing Surveys*,
    vol. 52, no. 2, pp. 43:1—-43:44, 2019.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] M. Fiaz, A. Mahmood, S. Javed, 和 S. K. Jung，“手工制作与深度跟踪器：近期视觉目标跟踪方法和趋势，”
    *ACM Computing Surveys*，第52卷，第2期，第43:1—43:44页，2019年。'
- en: '[49] P. Li, D. Wang, L. Wang, and H. Lu, “Deep visual tracking: Review and
    experimental comparison,” *Pattern Recognit.*, vol. 76, pp. 323–338, 2018.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] P. Li, D. Wang, L. Wang, 和 H. Lu，“深度视觉跟踪：综述与实验比较，” *Pattern Recognit.*，第76卷，第323–338页，2018年。'
- en: '[50] R. Pflugfelder, “An in-depth analysis of visual tracking with Siamese
    neural networks,” 2017\. [Online]. Available: [http://arxiv.org/abs/1707.00569](http://arxiv.org/abs/1707.00569)'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] R. Pflugfelder，“关于Siamese神经网络视觉跟踪的深入分析”，2017年。[在线]。可用： [http://arxiv.org/abs/1707.00569](http://arxiv.org/abs/1707.00569)'
- en: '[51] C. Ma, J. B. Huang, X. Yang, and M. H. Yang, “Hierarchical convolutional
    features for visual tracking,” in *Proc. IEEE ICCV*, 2015, pp. 3074–3082.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] C. Ma, J. B. Huang, X. Yang, 和 M. H. Yang，“用于视觉跟踪的层次卷积特征”，见于*Proc. IEEE
    ICCV*，2015年，第3074–3082页。'
- en: '[52] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg, “Convolutional features
    for correlation filter based visual tracking,” in *Proc. IEEE ICCVW*, 2016, pp.
    621–629.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. Danelljan, G. Hager, F. S. Khan, 和 M. Felsberg，“基于卷积特征的相关滤波器视觉跟踪”，见于*Proc.
    IEEE ICCVW*，2016年，第621–629页。'
- en: '[53] L. Wang, W. Ouyang, X. Wang, and H. Lu, “Visual tracking with fully convolutional
    networks,” in *Proc. IEEE ICCV*, 2015, pp. 3119–3127.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] L. Wang, W. Ouyang, X. Wang, 和 H. Lu，“使用全卷积网络的视觉跟踪”，见于*Proc. IEEE ICCV*，2015年，第3119–3127页。'
- en: '[54] S. Hong, T. You, S. Kwak, and B. Han, “Online tracking by learning discriminative
    saliency map with convolutional neural network,” in *Proc. ICML*, 2015, pp. 597–606.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Hong, T. You, S. Kwak, 和 B. Han，“通过学习区分显著性图与卷积神经网络进行在线跟踪”，见于*Proc.
    ICML*，2015年，第597–606页。'
- en: '[55] Y. Zha, T. Ku, Y. Li, and P. Zhang, “Deep position-sensitive tracking,”
    *IEEE Trans. Multimedia*, no. 8, 2019.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Zha, T. Ku, Y. Li, 和 P. Zhang，“深度位置敏感跟踪”，*IEEE Trans. Multimedia*，第8期，2019年。'
- en: '[56] M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg, “Beyond correlation
    filters: Learning continuous convolution operators for visual tracking,” in *Proc.
    ECCV*, vol. 9909 LNCS, 2016, pp. 472–488.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] M. Danelljan, A. Robinson, F. S. Khan, 和 M. Felsberg，“超越相关滤波器：学习连续卷积算子进行视觉跟踪”，见于*Proc.
    ECCV*，第9909卷 LNCS，2016年，第472–488页。'
- en: '[57] D. Held, S. Thrun, and S. Savarese, “Learning to track at 100 FPS with
    deep regression networks,” in *Proc. ECCV*, 2016, pp. 749–765.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] D. Held, S. Thrun, 和 S. Savarese，“通过深度回归网络在100 FPS下学习跟踪”，见于*Proc. ECCV*，2016年，第749–765页。'
- en: '[58] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr,
    “Fully-convolutional Siamese networks for object tracking,” in *Proc. ECCV*, 2016,
    pp. 850–865.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, 和 P. H. Torr，“用于目标跟踪的全卷积Siamese网络”，见于*Proc.
    ECCV*，2016年，第850–865页。'
- en: '[59] R. Tao, E. Gavves, and A. W. Smeulders, “Siamese instance search for tracking,”
    in *Proc. IEEE CVPR*, 2016, pp. 1420–1429.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] R. Tao, E. Gavves, 和 A. W. Smeulders，“Siamese实例搜索用于跟踪”，见于*Proc. IEEE CVPR*，2016年，第1420–1429页。'
- en: '[60] H. Nam and B. Han, “Learning multi-domain convolutional neural networks
    for visual tracking,” in *Proc. IEEE CVPR*, 2016, pp. 4293–4302.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] H. Nam 和 B. Han，“学习多域卷积神经网络进行视觉跟踪”，见于*Proc. IEEE CVPR*，2016年，第4293–4302页。'
- en: '[61] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, J. Lim, and M. H. Yang, “Hedged
    deep tracking,” in *Proc. IEEE CVPR*, 2016, pp. 4303–4311.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, J. Lim, 和 M. H. Yang，“Hedged
    deep tracking”，见于*Proc. IEEE CVPR*，2016年，第4303–4311页。'
- en: '[62] L. Wang, W. Ouyang, X. Wang, and H. Lu, “STCT: Sequentially training convolutional
    networks for visual tracking,” in *Proc. IEEE CVPR*, 2016, pp. 1373–1381.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] L. Wang, W. Ouyang, X. Wang, 和 H. Lu，“STCT：用于视觉跟踪的卷积网络顺序训练”，见于*Proc. IEEE
    CVPR*，2016年，第1373–1381页。'
- en: '[63] G. Zhu, F. Porikli, and H. Li, “Robust visual tracking with deep convolutional
    neural network based object proposals on PETS,” in *Proc. IEEE CVPRW*, 2016, pp.
    1265–1272.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] G. Zhu, F. Porikli, 和 H. Li，“通过深度卷积神经网络基于对象提议的鲁棒视觉跟踪”，见于*Proc. IEEE CVPRW*，2016年，第1265–1272页。'
- en: '[64] H. Li and et al., “DeepTrack: Learning discriminative feature representations
    online for robust visual tracking,” *IEEE Trans. Image Process.*, vol. 25, no. 4,
    pp. 1834–1848, 2016.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] H. Li 等，“DeepTrack：在线学习区分特征表示以实现鲁棒视觉跟踪”，*IEEE Trans. Image Process.*，第25卷，第4期，第1834–1848页，2016年。'
- en: '[65] H. Li, Y. Li, and F. Porikli, “DeepTrack: Learning discriminative feature
    representations by convolutional neural networks for visual tracking,” in *Proc.
    BMVC*, 2014.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] H. Li, Y. Li, 和 F. Porikli，“DeepTrack：通过卷积神经网络学习区分特征表示以进行视觉跟踪”，见于*Proc.
    BMVC*，2014年。'
- en: '[66] K. Zhang, Q. Liu, Y. Wu, and M. H. Yang, “Robust visual tracking via convolutional
    networks without training,” *IEEE Trans. Image Process.*, vol. 25, no. 4, pp.
    1779–1792, 2016.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] K. Zhang, Q. Liu, Y. Wu, 和 M. H. Yang，“通过卷积网络进行鲁棒视觉跟踪而无需训练”，*IEEE Trans.
    Image Process.*，第25卷，第4期，第1779–1792页，2016年。'
- en: '[67] C. Ma, Y. Xu, B. Ni, and X. Yang, “When correlation filters meet convolutional
    neural networks for visual tracking,” *IEEE Signal Process. Lett.*, vol. 23, no. 10,
    pp. 1454–1458, 2016.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] C. Ma, Y. Xu, B. Ni, 和 X. Yang，“当相关滤波器遇到卷积神经网络进行视觉跟踪时”，*IEEE Signal Process.
    Lett.*，第23卷，第10期，第1454–1458页，2016年。'
- en: '[68] H. Nam, M. Baek, and B. Han, “Modeling and propagating CNNs in a tree
    structure for visual tracking,” 2016\. [Online]. Available: [http://arxiv.org/abs/1608.07242](http://arxiv.org/abs/1608.07242)'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] H. Nam, M. Baek, 和 B. Han, “在树结构中建模和传播CNN用于视觉跟踪，” 2016年。[在线] 可用： [http://arxiv.org/abs/1608.07242](http://arxiv.org/abs/1608.07242)'
- en: '[69] G. Wu, W. Lu, G. Gao, C. Zhao, and J. Liu, “Regional deep learning model
    for visual tracking,” *Neurocomputing*, vol. 175, no. PartA, pp. 310–323, 2015.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] G. Wu, W. Lu, G. Gao, C. Zhao, 和 J. Liu, “用于视觉跟踪的区域深度学习模型，” *Neurocomputing*，第175卷，第A部分，页码310–323，2015年。'
- en: '[70] H. Fan and H. Ling, “Parallel tracking and verifying: A framework for
    real-time and high accuracy visual tracking,” in *Proc. IEEE ICCV*, 2017, pp.
    5487–5495.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] H. Fan 和 H. Ling, “并行跟踪与验证：一种实时和高精度视觉跟踪的框架，” *IEEE ICCV 会议论文集*，2017年，页码5487–5495。'
- en: '[71] H. Fan and H.Ling, “Parallel tracking and verifying,” *IEEE Trans. Image
    Process.*, vol. 28, no. 8, pp. 4130–4144, 2019.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] H. Fan 和 H. Ling, “并行跟踪与验证，” *IEEE 图像处理学报*，第28卷，第8期，页码4130–4144，2019年。'
- en: '[72] Y. Song, C. Ma, L. Gong, J. Zhang, R. W. Lau, and M. H. Yang, “CREST:
    Convolutional residual learning for visual tracking,” in *Proc. ICCV*, 2017, pp.
    2574–2583.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Y. Song, C. Ma, L. Gong, J. Zhang, R. W. Lau, 和 M. H. Yang, “CREST：用于视觉跟踪的卷积残差学习，”
    *ICCV 会议论文集*，2017年，页码2574–2583。'
- en: '[73] Z. Zhu, G. Huang, W. Zou, D. Du, and C. Huang, “UCT: Learning unified
    convolutional networks for real-time visual tracking,” in *Proc. ICCVW*, 2018,
    pp. 1973–1982.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Z. Zhu, G. Huang, W. Zou, D. Du, 和 C. Huang, “UCT：学习统一卷积网络用于实时视觉跟踪，” *IEEE
    ICCVW 会议论文集*，2018年，页码1973–1982。'
- en: '[74] Q. Guo, W. Feng, C. Zhou, R. Huang, L. Wan, and S. Wang, “Learning dynamic
    Siamese network for visual object tracking,” in *Proc. IEEE ICCV*, 2017, pp. 1781–1789.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Q. Guo, W. Feng, C. Zhou, R. Huang, L. Wan, 和 S. Wang, “学习动态Siamese网络进行视觉目标跟踪，”
    *IEEE ICCV 会议论文集*，2017年，页码1781–1789。'
- en: '[75] Z. Teng, J. Xing, Q. Wang, C. Lang, S. Feng, and Y. Jin, “Robust object
    tracking based on temporal and spatial deep networks,” in *Proc. IEEE ICCV*, 2017,
    pp. 1153–1162.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Z. Teng, J. Xing, Q. Wang, C. Lang, S. Feng, 和 Y. Jin, “基于时间和空间深度网络的鲁棒目标跟踪，”
    *IEEE ICCV 会议论文集*，2017年，页码1153–1162。'
- en: '[76] Z. He, Y. Fan, J. Zhuang, Y. Dong, and H. Bai, “Correlation filters with
    weighted convolution responses,” in *Proc. ICCVW*, 2018, pp. 1992–2000.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Z. He, Y. Fan, J. Zhuang, Y. Dong, 和 H. Bai, “具有加权卷积响应的相关滤波器，” *IEEE ICCVW
    会议论文集*，2018年，页码1992–2000。'
- en: '[77] T. Yang and A. B. Chan, “Recurrent filter learning for visual tracking,”
    in *Proc. ICCVW*, 2018, pp. 2010–2019.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] T. Yang 和 A. B. Chan, “用于视觉跟踪的递归滤波器学习，” *IEEE ICCVW 会议论文集*，2018年，页码2010–2019。'
- en: '[78] F. Li, Y. Yao, P. Li, D. Zhang, W. Zuo, and M. H. Yang, “Integrating boundary
    and center correlation filters for visual tracking with aspect ratio variation,”
    in *Proc. IEEE ICCVW*, 2018, pp. 2001–2009.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] F. Li, Y. Yao, P. Li, D. Zhang, W. Zuo, 和 M. H. Yang, “结合边界和中心相关滤波器进行具有纵横比变化的视觉跟踪，”
    *IEEE ICCVW 会议论文集*，2018年，页码2001–2009。'
- en: '[79] X. Wang, H. Li, Y. Li, F. Porikli, and M. Wang, “Deep tracking with objectness,”
    in *Proc. ICIP*, 2018, pp. 660–664.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] X. Wang, H. Li, Y. Li, F. Porikli, 和 M. Wang, “具有对象性的深度跟踪，” *ICIP 会议论文集*，2018年，页码660–664。'
- en: '[80] X. Xu, B. Ma, H. Chang, and X. Chen, “Siamese recurrent architecture for
    visual tracking,” in *Proc. ICIP*, 2018, pp. 1152–1156.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] X. Xu, B. Ma, H. Chang, 和 X. Chen, “用于视觉跟踪的Siamese递归架构，” *ICIP 会议论文集*，2018年，页码1152–1156。'
- en: '[81] L. Yang, P. Jiang, F. Wang, and X. Wang, “Region-based fully convolutional
    Siamese networks for robust real-time visual tracking,” in *Proc. ICIP*, 2017,
    pp. 2567–2571.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] L. Yang, P. Jiang, F. Wang, 和 X. Wang, “基于区域的全卷积Siamese网络进行鲁棒的实时视觉跟踪，”
    *ICIP 会议论文集*，2017年，页码2567–2571。'
- en: '[82] T. Kokul, C. Fookes, S. Sridharan, A. Ramanan, and U. A. J. Pinidiyaarachchi,
    “Gate connected convolutional neural network for object tracking,” in *Proc. ICIP*,
    2017, pp. 2602–2606.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] T. Kokul, C. Fookes, S. Sridharan, A. Ramanan, 和 U. A. J. Pinidiyaarachchi,
    “用于目标跟踪的门控连接卷积神经网络，” *ICIP 会议论文集*，2017年，页码2602–2606。'
- en: '[83] K. Dai, Y. Wang, and X. Yan, “Long-term object tracking based on Siamese
    network,” in *Proc. ICIP*, 2017, pp. 3640–3644.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] K. Dai, Y. Wang, 和 X. Yan, “基于Siamese网络的长期目标跟踪，” *ICIP 会议论文集*，2017年，页码3640–3644。'
- en: '[84] B. Akok, F. Gurkan, O. Kaplan, and B. Gunsel, “Robust object tracking
    by interleaving variable rate color particle filtering and deep learning,” in
    *Proc. ICIP*, 2017, pp. 3665–3669.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] B. Akok, F. Gurkan, O. Kaplan, 和 B. Gunsel, “通过交替变化率的颜色粒子滤波和深度学习实现鲁棒目标跟踪，”
    *ICIP 会议论文集*，2017年，页码3665–3669。'
- en: '[85] R. J. Mozhdehi and H. Medeiros, “Deep convolutional particle filter for
    visual tracking,” in *Proc. IEEE ICIP*, 2017, pp. 3650–3654.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] R. J. Mozhdehi 和 H. Medeiros, “用于视觉跟踪的深度卷积粒子滤波器，” *IEEE ICIP 会议论文集*，2017年，页码3650–3654。'
- en: '[86] J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, and P. H. Torr,
    “End-to-end representation learning for correlation filter based tracking,” in
    *Proc. IEEE CVPR*, 2017, pp. 5000–5008.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, 和 P. H. Torr，“用于相关滤波器基跟踪的端到端表征学习”，发表于
    *Proc. IEEE CVPR*，2017年，页码5000–5008。'
- en: '[87] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg, “ECO: Efficient
    convolution operators for tracking,” in *Proc. IEEE CVPR*, 2017, pp. 6931–6939.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] M. Danelljan, G. Bhat, F. Shahbaz Khan, 和 M. Felsberg，“ECO：用于跟踪的高效卷积算子”，发表于
    *Proc. IEEE CVPR*，2017年，页码6931–6939。'
- en: '[88] A. Lukežič, T. Vojíř, L. Čehovin Zajc, J. Matas, and M. Kristan, “Discriminative
    correlation filter tracker with channel and spatial reliability,” *IJCV*, vol.
    126, no. 7, pp. 671–688, 2018.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] A. Lukežič, T. Vojíř, L. Čehovin Zajc, J. Matas, 和 M. Kristan，“带有通道和空间可靠性的判别相关滤波器跟踪器”，*IJCV*，第126卷，第7期，页码671–688，2018年。'
- en: '[89] T. Zhang, C. Xu, and M. H. Yang, “Multi-task correlation particle filter
    for robust object tracking,” in *Proc. IEEE CVPR*, 2017, pp. 4819–4827.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] T. Zhang, C. Xu, 和 M. H. Yang，“用于鲁棒目标跟踪的多任务相关粒子滤波器”，发表于 *Proc. IEEE CVPR*，2017年，页码4819–4827。'
- en: '[90] B. Han, J. Sim, and H. Adam, “BranchOut: Regularization for online ensemble
    tracking with convolutional neural networks,” in *Proc. IEEE CVPR*, 2017, pp.
    521–530.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] B. Han, J. Sim, 和 H. Adam，“BranchOut：基于卷积神经网络的在线集成跟踪正则化”，发表于 *Proc. IEEE
    CVPR*，2017年，页码521–530。'
- en: '[91] M. Wang, Y. Liu, and Z. Huang, “Large margin object tracking with circulant
    feature maps,” in *Proc. IEEE CVPR*, 2017, pp. 4800–4808.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. Wang, Y. Liu, 和 Z. Huang，“具有循环特征映射的大边距目标跟踪”，发表于 *Proc. IEEE CVPR*，2017年，页码4800–4808。'
- en: '[92] L. Zhang, J. Varadarajan, P. N. Suganthan, N. Ahuja, and P. Moulin, “Robust
    visual tracking using oblique random forests,” in *Proc. IEEE CVPR*, 2017, pp.
    5825–5834.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] L. Zhang, J. Varadarajan, P. N. Suganthan, N. Ahuja, 和 P. Moulin，“使用斜率随机森林的鲁棒视觉跟踪”，发表于
    *Proc. IEEE CVPR*，2017年，页码5825–5834。'
- en: '[93] J. Choi, H. J. Chang, S. Yun, T. Fischer, Y. Demiris, and J. Y. Choi,
    “Attentional correlation filter network for adaptive visual tracking,” in *Proc.
    IEEE CVPR*, 2017, pp. 4828–4837.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Choi, H. J. Chang, S. Yun, T. Fischer, Y. Demiris, 和 J. Y. Choi，“自适应视觉跟踪的注意力相关滤波器网络”，发表于
    *Proc. IEEE CVPR*，2017年，页码4828–4837。'
- en: '[94] H. Fan and H. Ling, “SANet: Structure-aware network for visual tracking,”
    in *Proc. IEEE CVPRW*, 2017, pp. 2217–2224.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] H. Fan 和 H. Ling，“SANet：用于视觉跟踪的结构感知网络”，发表于 *Proc. IEEE CVPRW*，2017年，页码2217–2224。'
- en: '[95] Q. Wang, J. Gao, J. Xing, M. Zhang, and W. Hu, “DCFNet: Discriminant correlation
    filters network for visual tracking,” 2017\. [Online]. Available: [http://arxiv.org/abs/1704.04057](http://arxiv.org/abs/1704.04057)'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Q. Wang, J. Gao, J. Xing, M. Zhang, 和 W. Hu，“DCFNet：用于视觉跟踪的判别相关滤波器网络”，2017年。[在线].
    可用链接: [http://arxiv.org/abs/1704.04057](http://arxiv.org/abs/1704.04057)'
- en: '[96] J. Guo and T. Xu, “Deep ensemble tracking,” *IEEE Signal Process. Lett.*,
    vol. 24, no. 10, pp. 1562–1566, 2017.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] J. Guo 和 T. Xu，“深度集成跟踪”，*IEEE Signal Process. Lett.*，第24卷，第10期，页码1562–1566，2017年。'
- en: '[97] J. Gao, T. Zhang, X. Yang, and C. Xu, “Deep relative tracking,” *IEEE
    Trans. Image Process.*, vol. 26, no. 4, pp. 1845–1858, 2017.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. Gao, T. Zhang, X. Yang, 和 C. Xu，“深度相对跟踪”，*IEEE Trans. Image Process.*，第26卷，第4期，页码1845–1858，2017年。'
- en: '[98] Z. Chi, H. Li, H. Lu, and M. H. Yang, “Dual deep network for visual tracking,”
    *IEEE Trans. Image Process.*, vol. 26, no. 4, pp. 2005–2015, 2017.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Z. Chi, H. Li, H. Lu, 和 M. H. Yang，“用于视觉跟踪的双重深度网络”，*IEEE Trans. Image
    Process.*，第26卷，第4期，页码2005–2015，2017年。'
- en: '[99] P. Zhang, T. Zhuo, W. Huang, K. Chen, and M. Kankanhalli, “Online object
    tracking based on CNN with spatial-temporal saliency guided sampling,” *Neurocomputing*,
    vol. 257, pp. 115–127, 2017.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] P. Zhang, T. Zhuo, W. Huang, K. Chen, 和 M. Kankanhalli，“基于CNN的在线目标跟踪与时空显著性引导采样”，*Neurocomputing*，第257卷，页码115–127，2017年。'
- en: '[100] X. Dong and J. Shen, “Triplet loss in Siamese network for object tracking,”
    in *Proc. ECCV*, vol. 11217 LNCS, 2018, pp. 472–488.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Dong 和 J. Shen，“Siamese网络中的三元组损失用于目标跟踪”，发表于 *Proc. ECCV*，第11217卷 LNCS，2018年，页码472–488。'
- en: '[101] X. Lu, C. Ma, B. Ni, X. Yang, I. Reid, and M. H. Yang, “Deep regression
    tracking with shrinkage loss,” in *Proc. ECCV*, 2018, pp. 369–386.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] X. Lu, C. Ma, B. Ni, X. Yang, I. Reid, 和 M. H. Yang，“带有收缩损失的深度回归跟踪”，发表于
    *Proc. ECCV*，2018年，页码369–386。'
- en: '[102] G. Bhat, J. Johnander, M. Danelljan, F. S. Khan, and M. Felsberg, “Unveiling
    the power of deep tracking,” in *Proc. ECCV*, 2018, pp. 493–509.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] G. Bhat, J. Johnander, M. Danelljan, F. S. Khan, 和 M. Felsberg，“揭示深度跟踪的力量”，发表于
    *Proc. ECCV*，2018年，页码493–509。'
- en: '[103] B. Chen, D. Wang, P. Li, S. Wang, and H. Lu, “Real-time ‘actor-critic’
    tracking,” in *Proc. ECCV*, 2018, pp. 328–345.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] B. Chen, D. Wang, P. Li, S. Wang, 和 H. Lu，“实时‘actor-critic’跟踪”，发表于 *Proc.
    ECCV*，2018年，页码328–345。'
- en: '[104] Z. Zhu, Q. Wang, B. Li, W. Wu, J. Yan, and W. Hu, “Distractor-aware Siamese
    networks for visual object tracking,” in *Proc. ECCV*, vol. 11213 LNCS, 2018,
    pp. 103–119.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Z. Zhu, Q. Wang, B. Li, W. Wu, J. Yan, 和 W. Hu，“面向干扰物的 Siamese 网络用于视觉目标跟踪”，见于
    *Proc. ECCV*，第11213卷 LNCS，2018，第103–119页。'
- en: '[105] I. Jung, J. Son, M. Baek, and B. Han, “Real-time MDNet,” in *Proc. ECCV*,
    2018, pp. 89–104.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] I. Jung, J. Son, M. Baek, 和 B. Han，“实时 MDNet”，见于 *Proc. ECCV*，2018，第89–104页。'
- en: '[106] Y. Zhang, L. Wang, J. Qi, D. Wang, M. Feng, and H. Lu, “Structured Siamese
    network for real-time visual tracking,” in *Proc. ECCV*, 2018, pp. 355–370.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Y. Zhang, L. Wang, J. Qi, D. Wang, M. Feng, 和 H. Lu，“用于实时视觉跟踪的结构化 Siamese
    网络”，见于 *Proc. ECCV*，2018，第355–370页。'
- en: '[107] H. Lee, S. Choi, and C. Kim, “A memory model based on the Siamese network
    for long-term tracking,” in *Proc. ECCVW*, 2019, pp. 100–115.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] H. Lee, S. Choi, 和 C. Kim，“基于 Siamese 网络的长时间跟踪记忆模型”，见于 *Proc. ECCVW*，2019，第100–115页。'
- en: '[108] M. Che, R. Wang, Y. Lu, Y. Li, H. Zhi, and C. Xiong, “Channel pruning
    for visual tracking,” in *Proc. ECCVW*, 2019, pp. 70–82.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] M. Che, R. Wang, Y. Lu, Y. Li, H. Zhi, 和 C. Xiong，“用于视觉跟踪的通道剪枝”，见于 *Proc.
    ECCVW*，2019，第70–82页。'
- en: '[109] E. Burceanu and M. Leordeanu, “Learning a robust society of tracking
    parts using co-occurrence constraints,” in *Proc. ECCVW*, 2019, pp. 162–178.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] E. Burceanu 和 M. Leordeanu，“使用共现约束学习鲁棒的跟踪部分”，见于 *Proc. ECCVW*，2019，第162–178页。'
- en: '[110] H. Morimitsu, “Multiple context features in Siamese networks for visual
    object tracking,” in *Proc. ECCVW*, 2019, pp. 116–131.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] H. Morimitsu，“Siamese 网络中用于视觉目标跟踪的多个上下文特征”，见于 *Proc. ECCVW*，2019，第116–131页。'
- en: '[111] A. He, C. Luo, X. Tian, and W. Zeng, “Towards a better match in Siamese
    network based visual object tracker,” in *Proc. ECCVW*, 2019, pp. 132–147.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] A. He, C. Luo, X. Tian, 和 W. Zeng，“朝着更好的匹配在基于 Siamese 网络的视觉目标跟踪器中”，见于
    *Proc. ECCVW*，2019，第132–147页。'
- en: '[112] L. Rout, D. Mishra, and R. K. S. S. Gorthi, “WAEF: Weighted aggregation
    with enhancement filter for visual object tracking,” in *Proc. ECCVW*, 2019, pp.
    83–99.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] L. Rout, D. Mishra, 和 R. K. S. S. Gorthi，“WAEF：用于视觉目标跟踪的加权聚合与增强滤波器”，见于
    *Proc. ECCVW*，2019，第83–99页。'
- en: '[113] J. Choi, H. J. Chang, T. Fischer, S. Yun, K. Lee, J. Jeong, Y. Demiris,
    and J. Y. Choi, “Context-aware deep feature compression for high-speed visual
    tracking,” in *Proc. IEEE CVPR*, 2018, pp. 479–488.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] J. Choi, H. J. Chang, T. Fischer, S. Yun, K. Lee, J. Jeong, Y. Demiris,
    和 J. Y. Choi，“用于高速视觉跟踪的上下文感知深度特征压缩”，见于 *Proc. IEEE CVPR*，2018，第479–488页。'
- en: '[114] Y. Song, C. Ma, X. Wu, L. Gong, L. Bao, W. Zuo, C. Shen, R. W. Lau, and
    M. H. Yang, “VITAL: Visual tracking via adversarial learning,” in *Proc. IEEE
    CVPR*, 2018, pp. 8990–8999.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Y. Song, C. Ma, X. Wu, L. Gong, L. Bao, W. Zuo, C. Shen, R. W. Lau, 和
    M. H. Yang，“VITAL：通过对抗学习进行视觉跟踪”，见于 *Proc. IEEE CVPR*，2018，第8990–8999页。'
- en: '[115] F. Li, C. Tian, W. Zuo, L. Zhang, and M. H. Yang, “Learning spatial-temporal
    regularized correlation filters for visual tracking,” in *Proc. IEEE CVPR*, 2018,
    pp. 4904–4913.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] F. Li, C. Tian, W. Zuo, L. Zhang, 和 M. H. Yang，“用于视觉跟踪的空间时间正则化相关滤波器的学习”，见于
    *Proc. IEEE CVPR*，2018，第4904–4913页。'
- en: '[116] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, “High performance visual tracking
    with Siamese region proposal network,” in *Proc. IEEE CVPR*, 2018, pp. 8971–8980.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] B. Li, J. Yan, W. Wu, Z. Zhu, 和 X. Hu，“具有 Siamese 区域提议网络的高性能视觉跟踪”，见于
    *Proc. IEEE CVPR*，2018，第8971–8980页。'
- en: '[117] A. He, C. Luo, X. Tian, and W. Zeng, “A twofold Siamese network for real-time
    object tracking,” in *Proc. IEEE CVPR*, 2018, pp. 4834–4843.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] A. He, C. Luo, X. Tian, 和 W. Zeng，“用于实时对象跟踪的双重 Siamese 网络”，见于 *Proc.
    IEEE CVPR*，2018，第4834–4843页。'
- en: '[118] Z. Zhu, W. Wu, W. Zou, and J. Yan, “End-to-end flow correlation tracking
    with spatial-temporal attention,” in *Proc. IEEE CVPR*, 2018, pp. 548–557.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Z. Zhu, W. Wu, W. Zou, 和 J. Yan，“端到端流动相关跟踪与时空注意力”，见于 *Proc. IEEE CVPR*，2018，第548–557页。'
- en: '[119] C. Sun, D. Wang, H. Lu, and M. H. Yang, “Correlation tracking via joint
    discrimination and reliability learning,” in *Proc. IEEE CVPR*, 2018, pp. 489–497.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] C. Sun, D. Wang, H. Lu, 和 M. H. Yang，“通过联合判别和可靠性学习进行相关跟踪”，见于 *Proc. IEEE
    CVPR*，2018，第489–497页。'
- en: '[120] C. Sun, D. Wang, H. Lu, and M. Yang, “Learning spatial-aware regressions
    for visual tracking,” in *Proc. IEEE CVPR*, 2018, pp. 8962–8970.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] C. Sun, D. Wang, H. Lu, 和 M. Yang，“学习空间感知回归用于视觉跟踪”，见于 *Proc. IEEE CVPR*，2018，第8962–8970页。'
- en: '[121] Q. Wang, Z. Teng, J. Xing, J. Gao, W. Hu, and S. Maybank, “Learning attentions:
    Residual attentional Siamese network for high performance online visual tracking,”
    in *Proc. IEEE CVPR*, 2018, pp. 4854–4863.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Q. Wang, Z. Teng, J. Xing, J. Gao, W. Hu, 和 S. Maybank，“学习注意力：用于高性能在线视觉跟踪的残差注意力
    Siamese 网络”，见于 *Proc. IEEE CVPR*，2018，第4854–4863页。'
- en: '[122] N. Wang, W. Zhou, Q. Tian, R. Hong, M. Wang, and H. Li, “Multi-cue correlation
    filters for robust visual tracking,” in *Proc. IEEE CVPR*, 2018, pp. 4844–4853.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] N. Wang, W. Zhou, Q. Tian, R. Hong, M. Wang, 和 H. Li，“用于鲁棒视觉跟踪的多线索相关滤波器，”
    见于 *Proc. IEEE CVPR*, 2018, 第 4844–4853 页。'
- en: '[123] R. J. Mozhdehi, Y. Reznichenko, A. Siddique, and H. Medeiros, “Deep convolutional
    particle filter with adaptive correlation maps for visual tracking,” in *Proc.
    ICIP*, 2018, pp. 798–802.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] R. J. Mozhdehi, Y. Reznichenko, A. Siddique, 和 H. Medeiros，“具有自适应相关图的深度卷积粒子滤波器用于视觉跟踪，”
    见于 *Proc. ICIP*, 2018, 第 798–802 页。'
- en: '[124] Z. Lin and C. Yuan, “Robust visual tracking in low-resolution sequence,”
    in *Proc. ICIP*, 2018, pp. 4103–4107.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Z. Lin 和 C. Yuan，“在低分辨率序列中的鲁棒视觉跟踪，” 见于 *Proc. ICIP*, 2018, 第 4103–4107
    页。'
- en: '[125] M. Cen and C. Jung, “Fully convolutional Siamese fusion networks for
    object tracking,” in *Proc. ICIP*, 2018, pp. 3718–3722.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] M. Cen 和 C. Jung，“用于目标跟踪的全卷积Siamese融合网络，” 见于 *Proc. ICIP*, 2018, 第 3718–3722
    页。'
- en: '[126] G. Wang, B. Liu, W. Li, and N. Yu, “Flow guided Siamese network for visual
    tracking,” in *Proc. ICIP*, 2018, pp. 231–235.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] G. Wang, B. Liu, W. Li, 和 N. Yu，“流引导Siamese网络用于视觉跟踪，” 见于 *Proc. ICIP*,
    2018, 第 231–235 页。'
- en: '[127] K. Dai, Y. Wang, X. Yan, and Y. Huo, “Fusion of template matching and
    foreground detection for robust visual tracking,” in *Proc. ICIP*, 2018, pp. 2720–2724.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] K. Dai, Y. Wang, X. Yan, 和 Y. Huo，“模板匹配与前景检测融合的鲁棒视觉跟踪，” 见于 *Proc. ICIP*,
    2018, 第 2720–2724 页。'
- en: '[128] G. Liu and G. Liu, “Integrating multi-level convolutional features for
    correlation filter tracking,” in *Proc. ICIP*, 2018, pp. 3029–3033.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] G. Liu 和 G. Liu，“集成多级卷积特征进行相关滤波跟踪，” 见于 *Proc. ICIP*, 2018, 第 3029–3033
    页。'
- en: '[129] J. Guo, T. Xu, S. Jiang, and Z. Shen, “Generating reliable online adaptive
    templates for visual tracking,” in *Proc. ICIP*, 2018, pp. 226–230.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] J. Guo, T. Xu, S. Jiang, 和 Z. Shen，“生成可靠的在线自适应模板用于视觉跟踪，” 见于 *Proc. ICIP*,
    2018, 第 226–230 页。'
- en: '[130] S. Pu, Y. Song, C. Ma, H. Zhang, and M. H. Yang, “Deep attentive tracking
    via reciprocative learning,” in *Proc. NIPS*, 2018, pp. 1931–1941.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] S. Pu, Y. Song, C. Ma, H. Zhang, 和 M. H. Yang，“通过互惠学习的深度注意力跟踪，” 见于 *Proc.
    NIPS*, 2018, 第 1931–1941 页。'
- en: '[131] X. Jiang, X. Zhen, B. Zhang, J. Yang, and X. Cao, “Deep collaborative
    tracking networks,” in *Proc. BMVC*, 2018, p. 87.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] X. Jiang, X. Zhen, B. Zhang, J. Yang, 和 X. Cao，“深度协作跟踪网络，” 见于 *Proc.
    BMVC*, 2018, 第 87 页。'
- en: '[132] D. Ma, W. Bu, and X. Wu, “Multi-scale recurrent tracking via pyramid
    recurrent network and optical flow,” in *Proc. BMVC*, 2018, p. 242.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] D. Ma, W. Bu, 和 X. Wu，“通过金字塔递归网络和光流的多尺度递归跟踪，” 见于 *Proc. BMVC*, 2018,
    第 242 页。'
- en: '[133] C. Ma, J. B. Huang, X. Yang, and M. H. Yang, “Robust visual tracking
    via hierarchical convolutional features,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    2018.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] C. Ma, J. B. Huang, X. Yang, 和 M. H. Yang，“通过分层卷积特征的鲁棒视觉跟踪，” *IEEE Trans.
    Pattern Anal. Mach. Intell.*, 2018。'
- en: '[134] Z. Han, P. Wang, and Q. Ye, “Adaptive discriminative deep correlation
    filter for visual object tracking,” *IEEE Trans. Circuits Syst. Video Technol.*,
    2018.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Z. Han, P. Wang, 和 Q. Ye，“用于视觉目标跟踪的自适应判别深度相关滤波器，” *IEEE Trans. Circuits
    Syst. Video Technol.*, 2018。'
- en: '[135] K. Chen and W. Tao, “Once for all: A two-flow convolutional neural network
    for visual tracking,” *IEEE Trans. Circuits Syst. Video Technol.*, vol. 28, no. 12,
    pp. 3377–3386, 2018.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] K. Chen 和 W. Tao，“一次性完成：一种用于视觉跟踪的双流卷积神经网络，” *IEEE Trans. Circuits Syst.
    Video Technol.*, 卷 28，第 12 期，第 3377–3386 页，2018。'
- en: '[136] S. Li, S. Zhao, B. Cheng, E. Zhao, and J. Chen, “Robust visual tracking
    via hierarchical particle filter and ensemble deep features,” *IEEE Trans. Circuits
    Syst. Video Technol.*, 2018.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] S. Li, S. Zhao, B. Cheng, E. Zhao, 和 J. Chen，“通过分层粒子滤波器和集成深度特征的鲁棒视觉跟踪，”
    *IEEE Trans. Circuits Syst. Video Technol.*, 2018。'
- en: '[137] E. Gundogdu and A. A. Alatan, “Good features to correlate for visual
    tracking,” *IEEE Trans. Image Process.*, vol. 27, no. 5, pp. 2526–2540, 2018.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] E. Gundogdu 和 A. A. Alatan，“用于视觉跟踪的优良特征，” *IEEE Trans. Image Process.*,
    卷 27，第 5 期，第 2526–2540 页，2018。'
- en: '[138] Y. Xie, J. Xiao, K. Huang, J. Thiyagalingam, and Y. Zhao, “Correlation
    filter selection for visual tracking using reinforcement learning,” *IEEE Trans.
    Circuits Syst. Video Technol.*, 2018.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Y. Xie, J. Xiao, K. Huang, J. Thiyagalingam, 和 Y. Zhao，“使用强化学习的视觉跟踪相关滤波器选择，”
    *IEEE Trans. Circuits Syst. Video Technol.*, 2018。'
- en: '[139] J. Gao, T. Zhang, X. Yang, and C. Xu, “P2T: Part-to-target tracking via
    deep regression learning,” *IEEE Trans. Image Process.*, vol. 27, no. 6, pp. 3074–3086,
    2018.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] J. Gao, T. Zhang, X. Yang, 和 C. Xu，“P2T：通过深度回归学习的部件到目标跟踪，” *IEEE Trans.
    Image Process.*, 卷 27，第 6 期，第 3074–3086 页，2018。'
- en: '[140] C. Peng, F. Liu, J. Yang, and N. Kasabov, “Densely connected discriminative
    correlation filters for visual tracking,” *IEEE Signal Process. Lett.*, vol. 25,
    no. 7, pp. 1019–1023, 2018.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] C. Peng, F. Liu, J. Yang, 和 N. Kasabov，“用于视觉跟踪的密集连接判别相关滤波器，” *IEEE 信号处理快报*，第25卷，第7期，页码1019–1023，2018年。'
- en: '[141] D. Li, G. Wen, Y. Kuai, and F. Porikli, “End-to-end feature integration
    for correlation filter tracking with channel attention,” *IEEE Signal Process.
    Lett.*, vol. 25, no. 12, pp. 1815–1819, 2018.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] D. Li, G. Wen, Y. Kuai, 和 F. Porikli，“具有通道注意力的相关滤波器跟踪的端到端特征集成，” *IEEE
    信号处理快报*，第25卷，第12期，页码1815–1819，2018年。'
- en: '[142] C. Ma, J. B. Huang, and et al., “Adaptive correlation filters with long-term
    and short-term memory for object tracking,” *IJCV*, vol. 126, no. 8, pp. 771–796,
    2018.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] C. Ma, J. B. Huang, 等，“具有长期和短期记忆的自适应相关滤波器用于目标跟踪，” *IJCV*，第126卷，第8期，页码771–796，2018年。'
- en: '[143] Y. Cao, H. Ji, W. Zhang, and F. Xue, “Learning spatio-temporal context
    via hierarchical features for visual tracking,” *Signal Proc.: Image Comm.*, vol. 66,
    pp. 50–65, 2018.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Y. Cao, H. Ji, W. Zhang, 和 F. Xue，“通过分层特征学习时空上下文用于视觉跟踪，” *信号处理：图像通信*，第66卷，页码50–65，2018年。'
- en: '[144] F. Du, P. Liu, W. Zhao, and X. Tang, “Spatial–temporal adaptive feature
    weighted correlation filter for visual tracking,” *Signal Proc.: Image Comm.*,
    vol. 67, pp. 58–70, 2018.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] F. Du, P. Liu, W. Zhao, 和 X. Tang，“用于视觉跟踪的时空自适应特征加权相关滤波器，” *信号处理：图像通信*，第67卷，页码58–70，2018年。'
- en: '[145] Y. Kuai, G. Wen, and D. Li, “When correlation filters meet fully-convolutional
    Siamese networks for distractor-aware tracking,” *Signal Proc.: Image Comm.*,
    vol. 64, pp. 107–117, 2018.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Y. Kuai, G. Wen, 和 D. Li，“当相关滤波器遇到全卷积Siamese网络以进行抗干扰跟踪，” *信号处理：图像通信*，第64卷，页码107–117，2018年。'
- en: '[146] W. Gan, M. S. Lee, C. hao Wu, and C. C. Kuo, “Online object tracking
    via motion-guided convolutional neural network (MGNet),” *J. VIS. COMMUN. IMAGE
    R.*, vol. 53, pp. 180–191, 2018.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] W. Gan, M. S. Lee, C. hao Wu, 和 C. C. Kuo，“通过运动引导卷积神经网络（MGNet）进行在线目标跟踪，”
    *J. VIS. COMMUN. IMAGE R.*，第53卷，页码180–191，2018年。'
- en: '[147] M. Liu, C. B. Jin, B. Yang, X. Cui, and H. Kim, “Occlusion-robust object
    tracking based on the confidence of online selected hierarchical features,” *IET
    Image Proc.*, vol. 12, no. 11, pp. 2023–2029, 2018.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] M. Liu, C. B. Jin, B. Yang, X. Cui, 和 H. Kim，“基于在线选择的分层特征置信度的遮挡鲁棒目标跟踪，”
    *IET 图像处理*，第12卷，第11期，页码2023–2029，2018年。'
- en: '[148] K. Dai, D. Wang, H. Lu, C. Sun, and J. Li, “Visual tracking via adaptive
    spatially-regularized correlation filters,” in *Proc. CVPR*, 2019, pp. 4670–4679.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] K. Dai, D. Wang, H. Lu, C. Sun, 和 J. Li，“通过自适应空间正则化相关滤波器进行视觉跟踪，”发表于 *CVPR
    会议论文集*，2019年，页码4670–4679。'
- en: '[149] M. Danelljan, G. Bhat, F. S. Khan, and M. Felsberg, “ATOM: Accurate tracking
    by overlap maximization,” 2018\. [Online]. Available: [http://arxiv.org/abs/1811.07628](http://arxiv.org/abs/1811.07628)'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] M. Danelljan, G. Bhat, F. S. Khan, 和 M. Felsberg，“ATOM: 通过重叠最大化实现准确跟踪，”2018年。[在线].
    可用： [http://arxiv.org/abs/1811.07628](http://arxiv.org/abs/1811.07628)'
- en: '[150] H. Fan and H. Ling, “Siamese cascaded region proposal networks for real-time
    visual tracking,” 2018\. [Online]. Available: [http://arxiv.org/abs/1812.06148](http://arxiv.org/abs/1812.06148)'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] H. Fan 和 H. Ling，“用于实时视觉跟踪的Siamese级联区域提议网络，”2018年。[在线]. 可用： [http://arxiv.org/abs/1812.06148](http://arxiv.org/abs/1812.06148)'
- en: '[151] J. Gao, T. Zhang, and C. Xu, “Graph convolutional tracking,” in *Proc.
    CVPR*, 2019, pp. 4649–4659.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] J. Gao, T. Zhang, 和 C. Xu，“图卷积跟踪，”发表于 *CVPR 会议论文集*，2019年，页码4649–4659。'
- en: '[152] Y. Sun, C. Sun, D. Wang, Y. He, and H. Lu, “ROI pooled correlation filters
    for visual tracking,” in *Proc. CVPR*, 2019, pp. 5783–5791.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Y. Sun, C. Sun, D. Wang, Y. He, 和 H. Lu，“用于视觉跟踪的ROI池化相关滤波器，”发表于 *CVPR
    会议论文集*，2019年，页码5783–5791。'
- en: '[153] G. Wang, C. Luo, Z. Xiong, and W. Zeng, “Spm-tracker: Series-parallel
    matching for real-time visual object tracking,” 2019\. [Online]. Available: [http://arxiv.org/abs/1904.04452](http://arxiv.org/abs/1904.04452)'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] G. Wang, C. Luo, Z. Xiong, 和 W. Zeng，“Spm-tracker: 实时视觉目标跟踪的系列并行匹配，”2019年。[在线].
    可用： [http://arxiv.org/abs/1904.04452](http://arxiv.org/abs/1904.04452)'
- en: '[154] Z. Zhang and H. Peng, “Deeper and wider Siamese networks for real-time
    visual tracking,” 2019\. [Online]. Available: [http://arxiv.org/abs/1901.01660](http://arxiv.org/abs/1901.01660)'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Z. Zhang 和 H. Peng，“更深更广的Siamese网络用于实时视觉跟踪，”2019年。[在线]. 可用： [http://arxiv.org/abs/1901.01660](http://arxiv.org/abs/1901.01660)'
- en: '[155] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. S. Torr, “Fast online
    object tracking and segmentation: A unifying approach,” 2018\. [Online]. Available:
    [http://arxiv.org/abs/1812.05050](http://arxiv.org/abs/1812.05050)'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, 和 P. H. S. Torr，“快速在线目标跟踪和分割：一种统一的方法，”2018年。[在线].
    可用： [http://arxiv.org/abs/1812.05050](http://arxiv.org/abs/1812.05050)'
- en: '[156] B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan, “SiamRPN++: Evolution
    of Siamese visual tracking with very deep networks,” 2018\. [Online]. Available:
    [http://arxiv.org/abs/1812.11703](http://arxiv.org/abs/1812.11703)'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, 和 J. Yan，“SiamRPN++：非常深网络的Siamese视觉跟踪的演变，”
    2018年。[在线]. 可用： [http://arxiv.org/abs/1812.11703](http://arxiv.org/abs/1812.11703)'
- en: '[157] X. Li, C. Ma, B. Wu, Z. He, and M.-H. Yang, “Target-aware deep tracking,”
    2019\. [Online]. Available: [http://arxiv.org/abs/1904.01772](http://arxiv.org/abs/1904.01772)'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] X. Li, C. Ma, B. Wu, Z. He, 和 M.-H. Yang，“目标感知深度跟踪，” 2019年。[在线]. 可用：
    [http://arxiv.org/abs/1904.01772](http://arxiv.org/abs/1904.01772)'
- en: '[158] N. Wang, Y. Song, C. Ma, W. Zhou, W. Liu, and H. Li, “Unsupervised deep
    tracking,” 2019\. [Online]. Available: [http://arxiv.org/abs/1904.01828](http://arxiv.org/abs/1904.01828)'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] N. Wang, Y. Song, C. Ma, W. Zhou, W. Liu, 和 H. Li，“无监督深度跟踪，” 2019年。[在线].
    可用： [http://arxiv.org/abs/1904.01828](http://arxiv.org/abs/1904.01828)'
- en: '[159] G. Bhat, M. Danelljan, L. V. Gool, and R. Timofte, “Learning discriminative
    model prediction for tracking,” 2019\. [Online]. Available: [http://arxiv.org/abs/1904.07220](http://arxiv.org/abs/1904.07220)'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] G. Bhat, M. Danelljan, L. V. Gool, 和 R. Timofte，“用于跟踪的判别模型预测学习，” 2019年。[在线].
    可用： [http://arxiv.org/abs/1904.07220](http://arxiv.org/abs/1904.07220)'
- en: '[160] F. Zhao, J. Wang, Y. Wu, and M. Tang, “Adversarial deep tracking,” *IEEE
    Trans. Circuits Syst. Video Technol.*, vol. 29, no. 7, pp. 1998–2011, 2019.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] F. Zhao, J. Wang, Y. Wu, 和 M. Tang，“对抗性深度跟踪，” *IEEE Trans. Circuits Syst.
    Video Technol.*，第29卷，第7期，第1998–2011页，2019年。'
- en: '[161] H. Li, X. Wang, F. Shen, Y. Li, F. Porikli, and M. Wang, “Real-time deep
    tracking via corrective domain adaptation,” *IEEE Trans. Circuits Syst. Video
    Technol.*, vol. 8215, 2019.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] H. Li, X. Wang, F. Shen, Y. Li, F. Porikli, 和 M. Wang，“通过纠正领域适配实现实时深度跟踪，”
    *IEEE Trans. Circuits Syst. Video Technol.*，第8215卷，2019年。'
- en: '[162] B. Zhong, B. Bai, J. Li, Y. Zhang, and Y. Fu, “Hierarchical tracking
    by reinforcement learning-based searching and coarse-to-fine verifying,” *IEEE
    Trans. Image Process.*, vol. 28, no. 5, pp. 2331–2341, 2019.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] B. Zhong, B. Bai, J. Li, Y. Zhang, 和 Y. Fu，“通过基于强化学习的搜索和粗到细验证进行层次跟踪，”
    *IEEE Trans. Image Process.*，第28卷，第5期，第2331–2341页，2019年。'
- en: '[163] J. Gao, T. Zhang, and C. Xu, “SMART: Joint sampling and regression for
    visual tracking,” *IEEE Trans. Image Process.*, vol. 28, no. 8, pp. 3923–3935,
    2019.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] J. Gao, T. Zhang, 和 C. Xu，“SMART：用于视觉跟踪的联合采样和回归，” *IEEE Trans. Image
    Process.*，第28卷，第8期，第3923–3935页，2019年。'
- en: '[164] H. Hu, B. Ma, J. Shen, H. Sun, L. Shao, and F. Porikli, “Robust object
    tracking using manifold regularized convolutional neural networks,” *IEEE Trans.
    Multimedia*, vol. 21, no. 2, pp. 510–521, 2019.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] H. Hu, B. Ma, J. Shen, H. Sun, L. Shao, 和 F. Porikli，“使用流形正则化卷积神经网络的鲁棒目标跟踪，”
    *IEEE Trans. Multimedia*，第21卷，第2期，第510–521页，2019年。'
- en: '[165] L. Wang, L. Zhang, J. Wang, and Z. Yi, “Memory mechanisms for discriminative
    visual tracking algorithms with deep neural networks,” *IEEE Trans. Cogn. Devel.
    Syst.*, 2019.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] L. Wang, L. Zhang, J. Wang, 和 Z. Yi，“基于深度神经网络的判别视觉跟踪算法的记忆机制，” *IEEE Trans.
    Cogn. Devel. Syst.*，2019年。'
- en: '[166] Y. Kuai, G. Wen, and D. Li, “Multi-task hierarchical feature learning
    for real-time visual tracking,” *IEEE Sensors J.*, vol. 19, no. 5, pp. 1961–1968,
    2019.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Y. Kuai, G. Wen, 和 D. Li，“实时视觉跟踪的多任务层次特征学习，” *IEEE Sensors J.*，第19卷，第5期，第1961–1968页，2019年。'
- en: '[167] X. Cheng, Y. Zhang, L. Zhou, and Y. Zheng, “Visual tracking via Auto-Encoder
    pair correlation filter,” *IEEE Trans. Ind. Electron.*, 2019.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] X. Cheng, Y. Zhang, L. Zhou, 和 Y. Zheng，“通过自编码器对相关滤波器的视觉跟踪，” *IEEE Trans.
    Ind. Electron.*，2019年。'
- en: '[168] F. Tang, X. Lu, X. Zhang, S. Hu, and H. Zhang, “Deep feature tracking
    based on interactive multiple model,” *Neurocomputing*, vol. 333, pp. 29–40, 2019.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] F. Tang, X. Lu, X. Zhang, S. Hu, 和 H. Zhang，“基于交互式多模型的深度特征跟踪，” *Neurocomputing*，第333卷，第29–40页，2019年。'
- en: '[169] X. Lu, B. Ni, C. Ma, and X. Yang, “Learning transform-aware attentive
    network for object tracking,” *Neurocomputing*, vol. 349, pp. 133–144, 2019.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] X. Lu, B. Ni, C. Ma, 和 X. Yang，“用于物体跟踪的变换感知注意力网络学习，” *Neurocomputing*，第349卷，第133–144页，2019年。'
- en: '[170] D. Li, G. Wen, Y. Kuai, J. Xiao, and F. Porikli, “Learning target-aware
    correlation filters for visual tracking,” *J. VIS. COMMUN. IMAGE R.*, vol. 58,
    pp. 149–159, 2019.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] D. Li, G. Wen, Y. Kuai, J. Xiao, 和 F. Porikli，“学习目标感知相关滤波器用于视觉跟踪，” *J.
    VIS. COMMUN. IMAGE R.*，第58卷，第149–159页，2019年。'
- en: '[171] B. Chen, P. Li, C. Sun, D. Wang, G. Yang, and H. Lu, “Multi attention
    module for visual tracking,” *Pattern Recognit.*, vol. 87, pp. 80–93, 2019.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] B. Chen, P. Li, C. Sun, D. Wang, G. Yang, 和 H. Lu，“用于视觉跟踪的多重注意力模块，” *Pattern
    Recognit.*，第87卷，第80–93页，2019年。'
- en: '[172] S. Yun, J. J. Y. Choi, Y. Yoo, K. Yun, and J. J. Y. Choi, “Action-decision
    networks for visual tracking with deep reinforcement learning,” in *Proc. IEEE
    CVPR*, 2016, pp. 2–6.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] S. Yun、J. J. Y. Choi、Y. Yoo、K. Yun 和 J. J. Y. Choi，“基于深度强化学习的视觉跟踪行动决策网络，”
    见于 *Proc. IEEE CVPR*，2016，第2–6页。'
- en: '[173] S. Yun, J. Choi, Y. Yoo, K. Yun, and J. Y. Choi, “Action-driven visual
    object tracking with deep reinforcement learning,” *IEEE Trans. Neural Netw. Learn.
    Syst.*, vol. 29, no. 6, pp. 2239–2252, 2018.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] S. Yun、J. Choi、Y. Yoo、K. Yun 和 J. Y. Choi，“基于深度强化学习的行动驱动视觉物体跟踪，” *IEEE
    Trans. Neural Netw. Learn. Syst.*，第29卷，第6期，第2239–2252页，2018。'
- en: '[174] W. Zhang, K. Song, X. Rong, and Y. Li, “Coarse-to-fine UAV target tracking
    with deep reinforcement learning,” *IEEE Trans. Autom. Sci. Eng.*, pp. 1–9, 2018.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] W. Zhang、K. Song、X. Rong 和 Y. Li，“基于深度强化学习的粗到精无人机目标跟踪，” *IEEE Trans.
    Autom. Sci. Eng.*，第1–9页，2018。'
- en: '[175] L. Ren, X. Yuan, J. Lu, M. Yang, and J. Zhou, “Deep reinforcement learning
    with iterative shift for visual tracking,” in *Proc. ECCV*, 2018, pp. 697–713.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] L. Ren、X. Yuan、J. Lu、M. Yang 和 J. Zhou，“用于视觉跟踪的深度强化学习与迭代位移，” 见于 *Proc.
    ECCV*，2018，第697–713页。'
- en: '[176] D. Zhang, H. Maei, X. Wang, and Y.-F. Wang, “Deep reinforcement learning
    for visual object tracking in videos,” 2017\. [Online]. Available: [http://arxiv.org/abs/1701.08936](http://arxiv.org/abs/1701.08936)'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] D. Zhang、H. Maei、X. Wang 和 Y.-F. Wang，“视频中视觉物体跟踪的深度强化学习，” 2017。 [在线]。
    可用： [http://arxiv.org/abs/1701.08936](http://arxiv.org/abs/1701.08936)'
- en: '[177] C. Huang, S. Lucey, and D. Ramanan, “Learning policies for adaptive tracking
    with deep feature cascades,” in *Proc. IEEE ICCV*, 2017, pp. 105–114.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] C. Huang、S. Lucey 和 D. Ramanan，“利用深度特征级联学习适应性跟踪策略，” 见于 *Proc. IEEE ICCV*，2017，第105–114页。'
- en: '[178] X. Dong, J. Shen, W. Wang, Y. Liu, L. Shao, and F. Porikli, “Hyperparameter
    optimization for tracking with continuous deep Q-learning,” in *Proc. IEEE CVPR*,
    2018, pp. 518–527.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] X. Dong、J. Shen、W. Wang、Y. Liu、L. Shao 和 F. Porikli，“用于跟踪的超参数优化与连续深度
    Q 学习，” 见于 *Proc. IEEE CVPR*，2018，第518–527页。'
- en: '[179] J. Supancic and D. Ramanan, “Tracking as online decision-making: Learning
    a policy from streaming videos with reinforcement learning,” in *Proc. IEEE ICCV*,
    2017, pp. 322–331.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] J. Supancic 和 D. Ramanan，“将跟踪视为在线决策：通过强化学习从流视频中学习策略，” 见于 *Proc. IEEE
    ICCV*，2017，第322–331页。'
- en: '[180] J. Choi, J. Kwon, and K. M. Lee, “Real-time visual tracking by deep reinforced
    decision making,” *Comput. Vis. Image Und.*, vol. 171, pp. 10–19, 2018.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] J. Choi、J. Kwon 和 K. M. Lee，“通过深度强化决策制定实现实时视觉跟踪，” *Comput. Vis. Image
    Und.*，第171卷，第10–19页，2018。'
- en: '[181] X. Wang, C. Li, B. Luo, and J. Tang, “SINT++: Robust visual tracking
    via adversarial positive instance generation,” in *Proc. IEEE CVPR*, 2018, pp.
    4864–4873.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] X. Wang、C. Li、B. Luo 和 J. Tang，“SINT++：通过对抗正实例生成实现鲁棒视觉跟踪，” 见于 *Proc.
    IEEE CVPR*，2018，第4864–4873页。'
- en: '[182] E. Park and A. C. Berg, “Meta-tracker: Fast and robust online adaptation
    for visual object trackers,” in *Proc. ECCV*, 2018.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] E. Park 和 A. C. Berg，“Meta-tracker：用于视觉物体跟踪的快速且稳健的在线适应，” 见于 *Proc. ECCV*，2018。'
- en: '[183] L. Zhang and P. N. Suganthan, “Visual tracking with convolutional random
    vector functional link network,” *IEEE Trans. Cybernetics*, vol. 47, no. 10, pp.
    3243–3253, 2017.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] L. Zhang 和 P. N. Suganthan，“使用卷积随机向量函数链接网络进行视觉跟踪，” *IEEE Trans. Cybernetics*，第47卷，第10期，第3243–3253页，2017。'
- en: '[184] L. Zhang and N. Suganthan, “Visual tracking with convolutional neural
    network,” in *Proc. IEEE Int. Conf. Syst. Man Cybern.*, 2015.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] L. Zhang 和 N. Suganthan，“使用卷积神经网络进行视觉跟踪，” 见于 *Proc. IEEE Int. Conf. Syst.
    Man Cybern.*，2015。'
- en: '[185] L. Huang, X. Zhao, and K. Huang, “Bridging the gap between detection
    and tracking: A unified approach,” in *Proc. IEEE ICCV*, 2019.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] L. Huang、X. Zhao 和 K. Huang，“弥合检测与跟踪之间的差距：一种统一的方法，” 见于 *Proc. IEEE ICCV*，2019。'
- en: '[186] T. Xu, Z.-H. Feng, X.-J. Wu, and J. Kittler, “Joint group feature selection
    and discriminative filter learning for robust visual object tracking,” in *Proc.
    IEEE ICCV*, 2019.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] T. Xu、Z.-H. Feng、X.-J. Wu 和 J. Kittler，“用于鲁棒视觉物体跟踪的联合组特征选择与判别滤波器学习，”
    见于 *Proc. IEEE ICCV*，2019。'
- en: '[187] P. Li, B. Chen, W. Ouyang, D. Wang, X. Yang, and H. Lu, “Gradnet: Gradient-guided
    network for visual object tracking,” in *Proc. IEEE ICCV*, 2019.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] P. Li、B. Chen、W. Ouyang、D. Wang、X. Yang 和 H. Lu，“Gradnet：用于视觉物体跟踪的梯度引导网络，”
    见于 *Proc. IEEE ICCV*，2019。'
- en: '[188] J. Choi, J. Kwon, and K. M. Lee, “Deep meta learning for real-time target-aware
    visual tracking,” in *Proc. IEEE ICCV*, 2019.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] J. Choi、J. Kwon 和 K. M. Lee，“用于实时目标感知视觉跟踪的深度元学习，” 见于 *Proc. IEEE ICCV*，2019。'
- en: '[189] L. Zhang, A. Gonzalez-Garcia, J. v. d. Weijer, M. Danelljan, and F. S.
    Khan, “Learning the model update for siamese trackers,” in *Proc. IEEE ICCV*,
    2019.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] L. Zhang、A. Gonzalez-Garcia、J. v. d. Weijer、M. Danelljan 和 F. S. Khan，“学习用于孪生跟踪器的模型更新，”
    见于 *Proc. IEEE ICCV*，2019。'
- en: '[190] F. Du, P. Liu, W. Zhao, and X. Tang, “Correlation-guided attention for
    corner detection based visual tracking,” in *Proc. IEEE CVPR*, 2020.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] F. Du, P. Liu, W. Zhao, 和 X. Tang，“基于角点检测的相关性引导注意力视觉跟踪”，发表于 *IEEE CVPR会议录*，2020年。'
- en: '[191] B. Yan, D. Wang, H. Lu, and X. Yang, “Cooling-shrinking attack: Blinding
    the tracker with imperceptible noises,” in *Proc. IEEE CVPR*, 2020.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] B. Yan, D. Wang, H. Lu, 和 X. Yang，“冷却-收缩攻击：用不可感知的噪声使跟踪器失效”，发表于 *IEEE
    CVPR会议录*，2020年。'
- en: '[192] A. Lukezic, J. Matas, and M. Kristan, “D3s - a discriminative single
    shot segmentation tracker,” in *Proc. IEEE CVPR*, 2020.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] A. Lukezic, J. Matas, 和 M. Kristan，“D3S - 一种判别性单次分割跟踪器”，发表于 *IEEE CVPR会议录*，2020年。'
- en: '[193] X. Chen, X. Yan, F. Zheng, Y. Jiang, S.-T. Xia, Y. Zhao, and R. Ji, “One-shot
    adversarial attacks on visual tracking with dual attention,” in *Proc. IEEE CVPR*,
    2020.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] X. Chen, X. Yan, F. Zheng, Y. Jiang, S.-T. Xia, Y. Zhao, 和 R. Ji，“对视觉跟踪进行的一次性对抗攻击，采用双重注意力”，发表于
    *IEEE CVPR会议录*，2020年。'
- en: '[194] M. Danelljan, L. V. Gool, and R. Timofte, “Probabilistic regression for
    visual tracking,” in *Proc. IEEE CVPR*, 2020.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] M. Danelljan, L. V. Gool, 和 R. Timofte，“用于视觉跟踪的概率回归”，发表于 *IEEE CVPR会议录*，2020年。'
- en: '[195] J. Gao, W. Hu, and Y. Lu, “Recursive least-squares estimator-aided online
    learning for visual tracking,” in *Proc. IEEE CVPR*, 2020.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] J. Gao, W. Hu, 和 Y. Lu，“递归最小二乘估计辅助的在线学习视觉跟踪”，发表于 *IEEE CVPR会议录*，2020年。'
- en: '[196] T. Yang, P. Xu, R. Hu, H. Chai, and A. B. Chan, “Roam: Recurrently optimizing
    tracking model,” in *Proc. IEEE CVPR*, 2020.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] T. Yang, P. Xu, R. Hu, H. Chai, 和 A. B. Chan，“Roam：递归优化跟踪模型”，发表于 *IEEE
    CVPR会议录*，2020年。'
- en: '[197] Y. Yu, Y. Xiong, W. Huang, and M. R. Scott, “Deformable siamese attention
    networks for visual object tracking,” in *Proc. IEEE CVPR*, 2020.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Y. Yu, Y. Xiong, W. Huang, 和 M. R. Scott，“用于视觉目标跟踪的可变形孪生注意力网络”，发表于 *IEEE
    CVPR会议录*，2020年。'
- en: '[198] Z. Chen, B. Zhong, G. Li, S. Zhang, and R. Ji, “Siamese box adaptive
    network for visual tracking,” in *Proc. IEEE CVPR*, 2020.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] Z. Chen, B. Zhong, G. Li, S. Zhang, 和 R. Ji，“用于视觉跟踪的孪生框自适应网络”，发表于 *IEEE
    CVPR会议录*，2020年。'
- en: '[199] D. Guo, J. Wang, Y. Cui, Z. Wang, and S. Chen, “Siamcar: Siamese fully
    convolutional classification and regression for visual tracking,” in *Proc. IEEE
    CVPR*, 2020.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] D. Guo, J. Wang, Y. Cui, Z. Wang, 和 S. Chen，“SiamCAR：用于视觉跟踪的孪生完全卷积分类和回归”，发表于
    *IEEE CVPR会议录*，2020年。'
- en: '[200] P. Voigtlaender, J. Luiten, P. H. Torr, and B. Leibe, “Siam r-cnn: Visual
    tracking by re-detection,” in *Proc. IEEE CVPR*, 2020.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] P. Voigtlaender, J. Luiten, P. H. Torr, 和 B. Leibe，“Siam R-CNN：通过重新检测进行视觉跟踪”，发表于
    *IEEE CVPR会议录*，2020年。'
- en: '[201] G. Wang, C. Luo, X. Sun, Z. Xiong, and W. Zeng, “Tracking by instance
    detection: A meta-learning approach,” in *Proc. IEEE CVPR*, 2020.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] G. Wang, C. Luo, X. Sun, Z. Xiong, 和 W. Zeng，“通过实例检测进行跟踪：一种元学习方法”，发表于
    *IEEE CVPR会议录*，2020年。'
- en: '[202] K. Shuang, Y. Huang, Y. Sun, Z. Cai, and H. Guo, “Fine-grained motion
    representation for template-free visual tracking,” in *Proc. IEEE WACV*, 2020.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] K. Shuang, Y. Huang, Y. Sun, Z. Cai, 和 H. Guo，“用于无模板视觉跟踪的细粒度运动表示”，发表于
    *IEEE WACV会议录*，2020年。'
- en: '[203] H. Song, D. Suehiro, and S. Uchida, “Adaptive aggregation of arbitrary
    online trackers with a regret bound,” in *Proc. IEEE WACV*, 2020.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] H. Song, D. Suehiro, 和 S. Uchida，“带有遗憾界限的任意在线跟踪器的自适应聚合”，发表于 *IEEE WACV会议录*，2020年。'
- en: '[204] Y. Ma, C. Yuan, P. Gao, and F. Wang, “Efficient multi-level correlating
    for visual tracking,” in *Proc. ACCV*, 2018.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Y. Ma, C. Yuan, P. Gao, 和 F. Wang，“高效的多级相关视觉跟踪”，发表于 *ACCV会议录*，2018年。'
- en: '[205] Q. WuYan, Y. LiangYi, and L. Wang, “Dsnet: Deep and shallow feature learning
    for efficient visual tracking,” in *Proc. ACCV*, 2018.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Q. WuYan, Y. LiangYi, 和 L. Wang，“Dsnet：用于高效视觉跟踪的深度和浅层特征学习”，发表于 *ACCV会议录*，2018年。'
- en: '[206] C. Fu, Z. Huang, Y. Li, R. Duan, and P. Lu, “Boundary effect-aware visual
    tracking for UAV with online enhanced background learning and multi-frame consensus
    verification,” in *Proc. IROS*, 2019.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] C. Fu, Z. Huang, Y. Li, R. Duan, 和 P. Lu，“边界效应感知的无人机视觉跟踪，采用在线增强背景学习和多帧一致性验证”，发表于
    *IROS会议录*，2019年。'
- en: '[207] W. Song, S. Li, T. Chang, A. Hao, Q. Zhao, and H. Qin, “Cross-view contextual
    relation transferred network for unsupervised vehicle tracking in drone videos,”
    in *Proc. IEEE WACV*, 2020.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] W. Song, S. Li, T. Chang, A. Hao, Q. Zhao, 和 H. Qin，“用于无人机视频中无监督车辆跟踪的跨视角上下文关系迁移网络”，发表于
    *IEEE WACV会议录*，2020年。'
- en: '[208] Y. Li, C. Fu, Z. Huang, Y. Zhang, and J. Pan, “Keyfilter-aware real-time
    uav object tracking,” in *Proc. ICRA*, 2020.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Y. Li, C. Fu, Z. Huang, Y. Zhang, 和 J. Pan，“关键滤波器感知的实时无人机目标跟踪”，发表于 *ICRA会议录*，2020年。'
- en: '[209] Y. Li, C. Fu, Z. Huang, and et al., “Intermittent contextual learning
    for keyfilter-aware UAV object tracking using deep convolutional feature,” *IEEE
    Trans. Multimedia*, 2020.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Y. Li, C. Fu, Z. Huang, 等，“用于关键滤波器感知的无人机目标跟踪的间歇性上下文学习，采用深度卷积特征”，发表于 *IEEE
    Multimedia杂志*，2020年。'
- en: '[210] C. Fu, Y. He, F. Lin, and W. Xiong, “Robust multi-kernelized correlators
    for UAV tracking with adaptive context analysis and dynamic weighted filters,”
    *Neural Computing and Applications*, vol. 32, 2020.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] C. Fu, Y. He, F. Lin, 和 W. Xiong，“针对无人机跟踪的鲁棒多核相关器，结合自适应上下文分析和动态加权滤波器”，*Neural
    Computing and Applications*，第32卷，2020年。'
- en: '[211] C. Fu, W. Xiong, F. Lin, and Y. Yue, “Surrounding-aware correlation filter
    for UAV tracking with selective spatial regularization,” *Signal Processing*,
    vol. 167, 2020.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] C. Fu, W. Xiong, F. Lin, 和 Y. Yue，“周围感知相关滤波器用于无人机跟踪，结合选择性空间正则化”，*Signal
    Processing*，第167卷，2020年。'
- en: '[212] S. M. Marvasti-Zadeh, J. Khaghani, H. Ghanei-Yakhdan, S. Kasaei, and
    L. Cheng, “COMET: Context-aware IoU-guided network for small object tracking,”
    in *Proc. ACCV*, 2020.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] S. M. Marvasti-Zadeh, J. Khaghani, H. Ghanei-Yakhdan, S. Kasaei, 和 L.
    Cheng，“COMET：面向小目标跟踪的上下文感知IoU引导网络”，发表于*Proc. ACCV*，2020年。'
- en: '[213] H. Wu, X. Yang, Y. Yang, and G. Liu, “Flow guided short-term trackers
    with cascade detection for long-term tracking,” in *Proc. IEEE. ICCVW*, 2019.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] H. Wu, X. Yang, Y. Yang, 和 G. Liu，“流导向的短期跟踪器与级联检测用于长期跟踪”，发表于*Proc. IEEE.
    ICCVW*，2019年。'
- en: '[214] L. Huang, X. Zhao, and K. Huang, “Globaltrack: A simple and strong baseline
    for long-term tracking,” in *Proc. AAAI*, 2020.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] L. Huang, X. Zhao, 和 K. Huang，“Globaltrack：一种简单且强大的长期跟踪基线”，发表于*Proc.
    AAAI*，2020年。'
- en: '[215] W. Ren Tan and S.-H. Lai, “i-siam: Improving siamese tracker with distractors
    suppression and long-term strategies,” in *Proc. IEEE. ICCVW*, 2019.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] W. Ren Tan 和 S.-H. Lai，“i-siam：通过干扰物抑制和长期策略改进的孪生网络跟踪器”，发表于*Proc. IEEE.
    ICCVW*，2019年。'
- en: '[216] Y. Zhang, D. Wang, L. Wang, J. Qi, and H. Lu, “Learning regression and
    verification networks for long-term visual tracking,” 2018\. [Online]. Available:
    [http://arxiv.org/abs/1809.04320](http://arxiv.org/abs/1809.04320)'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] Y. Zhang, D. Wang, L. Wang, J. Qi, 和 H. Lu，“用于长期视觉跟踪的回归和验证网络学习”，2018年。[在线].
    可用： [http://arxiv.org/abs/1809.04320](http://arxiv.org/abs/1809.04320)'
- en: '[217] K. Dai, Y. Zhang, D. Wang, J. Li, H. Lu, and X. Yang, “High-performance
    long-term tracking with meta-updater,” in *Proc. IEEE CVPR*, 2020.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] K. Dai, Y. Zhang, D. Wang, J. Li, H. Lu, 和 X. Yang，“具有元更新器的高性能长期跟踪”，发表于*Proc.
    IEEE CVPR*，2020年。'
- en: '[218] B. Yan, H. Zhao, D. Wang, H. Lu, and X. Yang, “’skimming-perusal’ tracking:
    A framework for real-time and robust long-term tracking,” in *Proc. IEEE ICCV*,
    2019.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] B. Yan, H. Zhao, D. Wang, H. Lu, 和 X. Yang，“‘skimming-perusal’跟踪：实时且鲁棒的长期跟踪框架”，发表于*Proc.
    IEEE ICCV*，2019年。'
- en: '[219] Y. Wu, J. Lim, and M. H. Yang, “Online object tracking: A benchmark,”
    in *Proc. IEEE CVPR*, 2013, pp. 2411–2418.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Y. Wu, J. Lim, 和 M. H. Yang，“在线目标跟踪：一个基准”，发表于*Proc. IEEE CVPR*，2013年，页码2411–2418。'
- en: '[220] Y. Wu, J. Lim, and M. Yang, “Object tracking benchmark,” *IEEE Trans.
    Pattern Anal. Mach. Intell.*, vol. 37, no. 9, pp. 1834–1848, 2015.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Y. Wu, J. Lim, 和 M. Yang，“目标跟踪基准”，*IEEE Trans. Pattern Anal. Mach. Intell.*，第37卷，第9期，页码1834–1848，2015年。'
- en: '[221] H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai, Y. Xu, C. Liao,
    and H. Ling, “LaSOT: A high-quality benchmark for large-scale single object tracking,”
    2018\. [Online]. Available: [http://arxiv.org/abs/1809.07845](http://arxiv.org/abs/1809.07845)'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai, Y. Xu, C. Liao,
    和 H. Ling，“LaSOT：一个高质量的大规模单目标跟踪基准”，2018年。[在线]. 可用： [http://arxiv.org/abs/1809.07845](http://arxiv.org/abs/1809.07845)'
- en: '[222] M. Mueller, N. Smith, and B. Ghanem, “A benchmark and simulator for UAV
    tracking,” in *Proc. ECCV*, 2016, pp. 445–461.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] M. Mueller, N. Smith, 和 B. Ghanem，“无人机跟踪的基准和模拟器”，发表于*Proc. ECCV*，2016年，页码445–461。'
- en: '[223] D. Du, Y. Qi, H. Yu, Y. Yang, K. Duan, G. Li, W. Zhang, Q. Huang, and
    Q. Tian, “The unmanned aerial vehicle benchmark: Object detection and tracking,”
    in *Proc. ECCV*, 2018, pp. 375–391.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] D. Du, Y. Qi, H. Yu, Y. Yang, K. Duan, G. Li, W. Zhang, Q. Huang, 和 Q.
    Tian，“无人机基准：目标检测和跟踪”，发表于*Proc. ECCV*，2018年，页码375–391。'
- en: '[224] D. Du, P. Zhu, L. Wen, X. Bian, H. Ling, and et al., “VisDrone-SOT2019:
    The Vision Meets Drone Single Object Tracking Challenge Results,” in *Proc. ICCVW*,
    2019.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] D. Du, P. Zhu, L. Wen, X. Bian, H. Ling, 等，“VisDrone-SOT2019：视觉遇见无人机单目标跟踪挑战赛结果”，发表于*Proc.
    ICCVW*，2019年。'
- en: '[225] P. Liang, E. Blasch, and H. Ling, “Encoding color information for visual
    tracking: Algorithms and benchmark,” *IEEE Trans. Image Process.*, vol. 24, no. 12,
    pp. 5630–5644, 2015.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] P. Liang, E. Blasch, 和 H. Ling，“编码颜色信息用于视觉跟踪：算法和基准”，*IEEE Trans. Image
    Process.*，第24卷，第12期，页码5630–5644，2015年。'
- en: '[226] A. Li, M. Lin, Y. Wu, M. H. Yang, and S. Yan, “NUS-PRO: A new visual
    tracking challenge,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 38, no. 2,
    pp. 335–349, 2016.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] A. Li, M. Lin, Y. Wu, M. H. Yang, 和 S. Yan，“NUS-PRO：一个新的视觉跟踪挑战”，*IEEE
    Trans. Pattern Anal. Mach. Intell.*，第38卷，第2期，页码335–349，2016年。'
- en: '[227] H. K. Galoogahi, A. Fagg, C. Huang, D. Ramanan, and S. Lucey, “Need for
    speed: A benchmark for higher frame rate object tracking,” in *Proc. IEEE ICCV*,
    2017, pp. 1134–1143.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] H. K. Galoogahi, A. Fagg, C. Huang, D. Ramanan, 和 S. Lucey, “速度需求：一个更高帧率目标跟踪的基准，”
    在 *Proc. IEEE ICCV*，2017，pp. 1134–1143。'
- en: '[228] S. Li and D. Y. Yeung, “Visual object tracking for unmanned aerial vehicles:
    A benchmark and new motion models,” in *Proc. AAAI*, 2017, pp. 4140–4146.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] S. Li 和 D. Y. Yeung, “无人机的视觉目标跟踪：一个基准和新的运动模型，” 在 *Proc. AAAI*，2017，pp.
    4140–4146。'
- en: '[229] M. Müller, A. Bibi, S. Giancola, S. Alsubaihi, and B. Ghanem, “TrackingNet:
    A large-scale dataset and benchmark for object tracking in the wild,” in *Proc.
    ECCV*, 2018, pp. 310–327.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] M. Müller, A. Bibi, S. Giancola, S. Alsubaihi, 和 B. Ghanem, “TrackingNet:
    一个大规模的数据集和基准，用于真实环境中的目标跟踪，” 在 *Proc. ECCV*，2018，pp. 310–327。'
- en: '[230] J. Valmadre, L. Bertinetto, J. F. Henriques, R. Tao, A. Vedaldi, A. W.
    Smeulders, P. H. Torr, and E. Gavves, “Long-term tracking in the wild: A benchmark,”
    in *Proc. ECCV*, vol. 11207 LNCS, 2018, pp. 692–707.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] J. Valmadre, L. Bertinetto, J. F. Henriques, R. Tao, A. Vedaldi, A. W.
    Smeulders, P. H. Torr, 和 E. Gavves, “野外长期跟踪：一个基准，” 在 *Proc. ECCV*，vol. 11207 LNCS，2018，pp.
    692–707。'
- en: '[231] A. Li, Z. Chen, and Y. Wang, “BUAA-PRO: A tracking dataset with pixel-level
    annotation,” in *Proc. BMVC*, 2018\. [Online]. Available: [http://bmvc2018.org/contents/papers/0851.pdf](http://bmvc2018.org/contents/papers/0851.pdf)'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] A. Li, Z. Chen, 和 Y. Wang, “BUAA-PRO: 一个带有像素级注释的跟踪数据集，” 在 *Proc. BMVC*，2018\.
    [在线]. 可用: [http://bmvc2018.org/contents/papers/0851.pdf](http://bmvc2018.org/contents/papers/0851.pdf)'
- en: '[232] L. Huang, X. Zhao, and K. Huang, “GOT-10k: A large high-diversity benchmark
    for generic object tracking in the wild,” 2018\. [Online]. Available: [http://arxiv.org/abs/1810.11981](http://arxiv.org/abs/1810.11981)'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] L. Huang, X. Zhao, 和 K. Huang, “GOT-10k: 一项针对真实环境中通用目标跟踪的大规模高多样性基准，”
    2018\. [在线]. 可用: [http://arxiv.org/abs/1810.11981](http://arxiv.org/abs/1810.11981)'
- en: '[233] A. Moudgil and V. Gandhi, “Long-term visual object tracking benchmark,”
    in *Proc. ICCV*, 2018, pp. 629–645.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] A. Moudgil 和 V. Gandhi, “长期视觉目标跟踪基准，” 在 *Proc. ICCV*，2018，pp. 629–645。'
- en: '[234] H. Fan, Y. Fan, P. Chu, L. Yuan, and H. Ling, “Tracklinic: Diagnosis
    of challenge factors in visual tracking,” ser. Proc. WACV, 2021.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] H. Fan, Y. Fan, P. Chu, L. Yuan, 和 H. Ling, “Tracklinic: 视觉跟踪中挑战因素的诊断，”
    ser. Proc. WACV，2021。'
- en: '[235] A. Lukezic, L. C. Zajc, T. Vojir, J. Matas, and M. Kristan, “Now you
    see me: evaluating performance in long-term visual tracking,” 2018\. [Online].
    Available: [http://arxiv.org/abs/1804.07056](http://arxiv.org/abs/1804.07056)'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] A. Lukezic, L. C. Zajc, T. Vojir, J. Matas, 和 M. Kristan, “你现在看到我：在长期视觉跟踪中的性能评估，”
    2018\. [在线]. 可用: [http://arxiv.org/abs/1804.07056](http://arxiv.org/abs/1804.07056)'
- en: '[236] P. Zhu, L. Wen, D. Du, and et al., “VisDrone-VDT2018: The vision meets
    drone video detection and tracking challenge results,” in *Proc. ECCVW*, 2018,
    pp. 496–518.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] P. Zhu, L. Wen, D. Du, 和等， “VisDrone-VDT2018: 视觉与无人机视频检测和跟踪挑战赛结果，” 在
    *Proc. ECCVW*，2018，pp. 496–518。'
- en: '[237] C. Liu, W. Ding, J. Yang, and et al., “Aggregation signature for small
    object tracking,” *IEEE Trans. Image Processing*, vol. 29, pp. 1738–1747, 2020.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] C. Liu, W. Ding, J. Yang, 和等， “用于小目标跟踪的聚合签名，” *IEEE Trans. Image Processing*，vol.
    29，pp. 1738–1747，2020。'
- en: '[238] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Y. Fu, and A. C.
    Berg, “SSD: Single shot multibox detector,” in *Proc. ECCV*, 2016, pp. 21–37.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Y. Fu, 和 A. C.
    Berg, “SSD: 单次多框检测器，” 在 *Proc. ECCV*，2016，pp. 21–37。'
- en: '[239] G. Koch, R. Zemel, and R. Salakhutdinov, “Siamese neural networks for
    one-shot image recognition,” in *Proc. ICML Deep Learning Workshop*, 2015.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] G. Koch, R. Zemel, 和 R. Salakhutdinov, “用于单次图像识别的Siamese神经网络，” 在 *Proc.
    ICML Deep Learning Workshop*，2015。'
- en: '[240] G. Lin, A. Milan, C. Shen, and I. Reid, “RefineNet: Multi-path refinement
    networks for high-resolution semantic segmentation,” in *Proc. IEEE CVPR*, 2017,
    pp. 5168–5177.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] G. Lin, A. Milan, C. Shen, 和 I. Reid, “RefineNet: 多路径细化网络用于高分辨率语义分割，”
    在 *Proc. IEEE CVPR*，2017，pp. 5168–5177。'
- en: '[241] T. Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proc. IEEE CVPR*, 2017, pp.
    936–944.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] T. Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, 和 S. Belongie,
    “用于目标检测的特征金字塔网络，” 在 *Proc. IEEE CVPR*，2017，pp. 936–944。'
- en: '[242] S. Gladh, M. Danelljan, F. S. Khan, and M. Felsberg, “Deep motion features
    for visual tracking,” in *Proc. ICPR*, 2016, pp. 1243–1248.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] S. Gladh, M. Danelljan, F. S. Khan, 和 M. Felsberg, “用于视觉跟踪的深度运动特征，” 在
    *Proc. ICPR*，2016，pp. 1243–1248。'
- en: '[243] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. Huang, “YouTube-VOS:
    A large-scale video object segmentation benchmark,” in *Proc. ECCV*, 2018.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, 和 T. Huang, “YouTube-VOS:
    一个大规模视频目标分割基准，” 在 *Proc. ECCV*，2018。'
- en: '[244] E. Real, J. Shlens, S. Mazzocchi, X. Pan, and V. Vanhoucke, “YouTube-BoundingBoxes:
    A large high-precision human-annotated data set for object detection in video,”
    in *Proc. IEEE CVPR*, 2017, pp. 7464–7473.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] E. Real, J. Shlens, S. Mazzocchi, X. Pan, 和 V. Vanhoucke, “YouTube-BoundingBoxes:
    一个大型高精度人工标注的视频物体检测数据集”，发表于 *Proc. IEEE CVPR*, 2017，第 7464–7473 页。'
- en: '[245] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The KITTI dataset,” vol. 32, no. 11, p. 1231–1237, 2013.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] A. Geiger, P. Lenz, C. Stiller, 和 R. Urtasun, “视觉遇上机器人：KITTI 数据集”，第 32
    卷，第 11 期，第 1231–1237 页，2013 年。'
- en: '[246] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-learning
    in neural networks: A survey,” 2020\. [Online]. Available: [http://arxiv.org/abs/2004.05439](http://arxiv.org/abs/2004.05439)'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] T. Hospedales, A. Antoniou, P. Micaelli, 和 A. Storkey, “神经网络中的元学习：综述”，2020
    年。[在线]. 可用： [http://arxiv.org/abs/2004.05439](http://arxiv.org/abs/2004.05439)'
- en: '[247] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in *Proc. ICML*, 2017, pp. 1126–1135.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] C. Finn, P. Abbeel, 和 S. Levine, “模型无关的元学习用于深度网络的快速适应”，发表于 *Proc. ICML*,
    2017，第 1126–1135 页。'
- en: '[248] L. Wen, D. Du, Z. Lei, S. Z. Li, and M.-H. Yang, “Jots: Joint online
    tracking and segmentation,” in *Proc. IEEE CVPR*, 2015.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] L. Wen, D. Du, Z. Lei, S. Z. Li, 和 M.-H. Yang, “Jots: 联合在线跟踪与分割”，发表于
    *Proc. IEEE CVPR*, 2015。'
- en: '[249] J. Son, I. Jung, K. Park, and B. Han, “Tracking-by-segmentation with
    online gradient boosting decision tree,” in *Proc. IEEE ICCV*, 2015.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] J. Son, I. Jung, K. Park, 和 B. Han, “基于分割的跟踪与在线梯度提升决策树”，发表于 *Proc. IEEE
    ICCV*, 2015。'
- en: '[250] D. Yeo, J. Son, B. Han, and J. Hee Han, “Superpixel-based tracking-by-segmentation
    using markov chains,” in *Proc. IEEE CVPR*, 2017.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] D. Yeo, J. Son, B. Han, 和 J. Hee Han, “基于超像素的分割跟踪使用马尔科夫链”，发表于 *Proc.
    IEEE CVPR*, 2017。'
- en: '[251] J. Luiten, P. Voigtlaender, and B. Leibe, “Premvos: Proposal-generation,
    refinement and merging for video object segmentation,” in *Proc. ACCV*, 2018.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] J. Luiten, P. Voigtlaender, 和 B. Leibe, “Premvos: 视频物体分割的提案生成、细化与合并”，发表于
    *Proc. ACCV*, 2018。'
- en: '[252] V. Kulharia, S. Chandra, A. Agrawal, P. Torr, and A. Tyagi, “Box2seg:
    Attention weighted loss and discriminative feature learning for weakly supervised
    segmentation,” in *Proc. ECCV*, 2020.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] V. Kulharia, S. Chandra, A. Agrawal, P. Torr, 和 A. Tyagi, “Box2seg: 注意力加权损失和区分特征学习用于弱监督分割”，发表于
    *Proc. ECCV*, 2020。'
- en: '[253] L. Čehovin, “TraX: The visual tracking exchange protocol and library,”
    *Neurocomputing*, vol. 260, pp. 5–8, 2017.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] L. Čehovin, “TraX: 视觉跟踪交换协议和库”，*Neurocomputing*，第 260 卷，第 5–8 页，2017
    年。'
- en: '[254] A. Vedaldi and K. Lenc, “MatConvNet: Convolutional neural networks for
    MATLAB,” in *Proc. ACM Multimedia Conference*, 2015, pp. 689–692.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] A. Vedaldi 和 K. Lenc, “MatConvNet: 用于 MATLAB 的卷积神经网络”，发表于 *Proc. ACM
    Multimedia Conference*, 2015，第 689–692 页。'
- en: '[255] B. Jiang, R. Luo, J. Mao, T. Xiao, and Y. Jiang, “Acquisition of localization
    confidence for accurate object detection,” in *Proc. IEEE ECCV*, 2018, pp. 816–832.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] B. Jiang, R. Luo, J. Mao, T. Xiao, 和 Y. Jiang, “用于精确物体检测的定位置信度获取”，发表于
    *Proc. IEEE ECCV*, 2018，第 816–832 页。'
- en: '[256] J. Serra, D. Suris, M. Miron, and A. Karatzoglou, “Overcoming catastrophic
    forgetting with hard attention to the task,” ser. Proc. ICML, 2018, pp. 4548–4557.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] J. Serra, D. Suris, M. Miron, 和 A. Karatzoglou, “通过对任务的硬注意力克服灾难性遗忘”，系列
    *Proc. ICML*, 2018，第 4548–4557 页。'
- en: '[257] X. LI, Y. Grandvalet, and F. Davoine, “Explicit inductive bias for transfer
    learning with convolutional networks,” ser. Proc. ICML, 2018, pp. 2825–2834.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] X. LI, Y. Grandvalet, 和 F. Davoine, “卷积网络转移学习的显式归纳偏置”，系列 *Proc. ICML*,
    2018，第 2825–2834 页。'
- en: '[258] X. Chen, S. Wang, B. Fu, M. Long, and J. Wang, “Catastrophic forgetting
    meets negative transfer: Batch spectral shrinkage for safe transfer learning,”
    in *Proc. NIPS*, 2019, pp. 1908–1918.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] X. Chen, S. Wang, B. Fu, M. Long, 和 J. Wang, “灾难性遗忘遇上负迁移：用于安全转移学习的批量光谱收缩”，发表于
    *Proc. NIPS*, 2019，第 1908–1918 页。'
- en: '[259] J. Yoon, E. Yang, J. Lee, and S. J. Hwang, “Lifelong learning with dynamically
    expandable networks,” in *Proc. ICLR*, 2018.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] J. Yoon, E. Yang, J. Lee, 和 S. J. Hwang, “具有动态扩展网络的终身学习”，发表于 *Proc. ICLR*,
    2018。'
- en: '| ![[Uncaptioned image]](img/068188baba725d3ad10d5809805e2028.png) | Seyed
    Mojtaba Marvasti-Zadeh (Student Member, IEEE) is currently a Ph.D. student in
    Electrical Engineering, Yazd University (YU), Iran. He was awarded as a distinguished
    researcher student of the Department of Electrical Engineering, YU, in 2015 and
    2017\. He is also a member of Vision and Learning Lab, University of Alberta,
    Canada, where he was a visiting researcher from Dec. 2019 to Sep. 2020\. His research
    interest includes computer vision and machine learning. |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图像]](img/068188baba725d3ad10d5809805e2028.png) | Seyed Mojtaba Marvasti-Zadeh（IEEE学生会员）目前是伊朗Yazd大学（YU）电气工程专业的博士生。他在2015年和2017年被评为YU电气工程系的杰出研究生。他也是加拿大阿尔伯塔大学视觉与学习实验室的成员，并曾于2019年12月到2020年9月期间担任访问研究员。他的研究兴趣包括计算机视觉和机器学习。
    |'
- en: '| ![[Uncaptioned image]](img/683a82fce4ecbeb6dbee20a41fc06e40.png) | Li Cheng
    (Senior Member, IEEE) received the Ph.D. degree in computer science from the University
    of Alberta, Canada. He is an associate professor with the Department of Electrical
    and Computer Engineering, University of Alberta, Canada. He is also the Director
    of the Vision and Learning Lab. Prior to coming back to University of Alberta,
    he has worked at A*STAR, Singapore, TTIChicago, USA, and NICTA, Australia. His
    research expertise is mainly on computer vision and machine learning. |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图像]](img/683a82fce4ecbeb6dbee20a41fc06e40.png) | Li Cheng（IEEE高级会员）获得了加拿大阿尔伯塔大学计算机科学博士学位。他是阿尔伯塔大学电气与计算机工程系的副教授，也是视觉与学习实验室的主任。在回到阿尔伯塔大学之前，他曾在新加坡A*STAR、美国TTIChicago和澳大利亚NICTA工作。他的研究专长主要集中在计算机视觉和机器学习。
    |'
- en: '| ![[Uncaptioned image]](img/2ceee74e3012eb59a9273b1c66138e5d.png) | Hossein
    Ghanei-Yakhdan received the B.Sc. degree in Electrical Engineering from Isfahan
    University of Technology, Iran, in 1989, the M.Sc. degree in 1993 from K. N. Toosi
    University of Technology, Iran, and the Ph.D. degree in 2009 from Ferdowsi University
    of Mashhad, Iran. Since 2009, he has been an Assistant Professor with the Department
    of Electrical Engineering, Yazd University, Iran. His research interests are in
    digital video and image processing, error concealment and error-resilient video
    coding, and object tracking. |'
  id: totrans-780
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图像]](img/2ceee74e3012eb59a9273b1c66138e5d.png) | Hossein Ghanei-Yakhdan于1989年获得伊朗伊斯法罕工业大学电气工程学士学位，1993年获得K.
    N. Toosi工业大学的硕士学位，并于2009年获得伊朗费尔多西大学的博士学位。自2009年以来，他一直担任伊朗Yazd大学电气工程系的助理教授。他的研究兴趣包括数字视频和图像处理、错误隐藏与错误恢复视频编码以及对象跟踪。
    |'
- en: '| ![[Uncaptioned image]](img/9da4c205b70a3d6417c216989a53c11c.png) | Shohreh
    Kasaei (Senior Member, IEEE) received the Ph.D. degree from the Signal Processing
    Research Center, School of Electrical Engineering and Computer Science, Queensland
    University of Technology, Australia, in 1998\. She was awarded as a distinguished
    researcher of Sharif University of Technology (SUT), in 2002 and 2010, where she
    is currently a Full Professor. Since 1999, she has been the Director of the Image
    Processing Laboratory (IPL). Her research interests include machine learning,
    computer vision, and image/video processing. |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图像]](img/9da4c205b70a3d6417c216989a53c11c.png) | Shohreh Kasaei（IEEE高级会员）于1998年获得澳大利亚昆士兰科技大学电气工程与计算机科学学院信号处理研究中心的博士学位。她在2002年和2010年被评为Sharif大学（SUT）的杰出研究人员，目前是该校的教授。自1999年以来，她一直担任图像处理实验室（IPL）的主任。她的研究兴趣包括机器学习、计算机视觉和图像/视频处理。
    |'
