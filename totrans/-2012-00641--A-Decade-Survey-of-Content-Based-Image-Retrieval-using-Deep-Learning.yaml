- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:58:05'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2012.00641] A Decade Survey of Content Based Image Retrieval using Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2012.00641](https://ar5iv.labs.arxiv.org/html/2012.00641)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Decade Survey of Content Based Image Retrieval using Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Shiv Ram Dubey S.R. Dubey is with the Computer Vision Group, Indian Institute
    of Information Technology, Sri City, Chittoor, Andhra Pradesh-517646, India (e-mail:
    shivram1987@gmail.com, srdubey@iiits.in).'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The content based image retrieval aims to find the similar images from a large
    scale dataset against a query image. Generally, the similarity between the representative
    features of the query image and dataset images is used to rank the images for
    retrieval. In early days, various hand designed feature descriptors have been
    investigated based on the visual cues such as color, texture, shape, etc. that
    represent the images. However, the deep learning has emerged as a dominating alternative
    of hand-designed feature engineering from a decade. It learns the features automatically
    from the data. This paper presents a comprehensive survey of deep learning based
    developments in the past decade for content based image retrieval. The categorization
    of existing state-of-the-art methods from different perspectives is also performed
    for greater understanding of the progress. The taxonomy used in this survey covers
    different supervision, different networks, different descriptor type and different
    retrieval type. A performance analysis is also performed using the state-of-the-art
    methods. The insights are also presented for the benefit of the researchers to
    observe the progress and to make the best choices. The survey presented in this
    paper will help in further research progress in image retrieval using deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Content Based Image Retrieval; Deep Learning; CNNs; Survey; Supervised and Unsupervised
    Learning.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image retrieval is a well studied problem of image matching where the similar
    images are retrieved from a database w.r.t. a given query image [[1](#bib.bib1)],
    [[2](#bib.bib2)]. Basically, the similarity between the query image and the database
    images is used to rank the database images in decreasing order of similarity [[3](#bib.bib3)].
    Thus, the performance of any image retrieval method depends upon the similarity
    computation between images. Ideally, the similarity score computation method between
    two images should be discriminative, robust and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: I-A Hand-crafted Descriptor based Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to make the retrieval robust to geometric and photometric changes,
    the similarity between images is computed based on the content of images. Basically,
    the content of the images (i.e., the visual appearance) in terms of the color,
    texture, shape, gradient, etc. are represented in the form of a feature descriptor
    [[4](#bib.bib4)]. The similarity between the feature vectors of the corresponding
    images is treated as the similarity between the images. Thus, the performance
    of any content based image retrieval (CBIR) method heavily depends upon the feature
    descriptor representation of the image. Any feature descriptor representation
    method is expected to have the discriminating ability, robustness and low dimensionality.
    Various feature descriptor representation methods have been investigated to compute
    the similarity between the two images for content based image retrieval. The feature
    descriptor representation utilizes the visual cues of the images selected manually
    based on the need [[5](#bib.bib5)], [[6](#bib.bib6)], [[7](#bib.bib7)], [[8](#bib.bib8)],
    [[9](#bib.bib9)], [[10](#bib.bib10)], [[11](#bib.bib11)], [[12](#bib.bib12)],
    [[13](#bib.bib13)], [[14](#bib.bib14)], [[15](#bib.bib15)], [[16](#bib.bib16)].
    These approaches are also termed as the hand-designed or hand-engineered feature
    description. Moreover, generally these methods are unsupervised as they do not
    need the data to design the feature representation method. Various survey has
    been also conducted time to time to present the progress in content based image
    retrieval, including [[17](#bib.bib17)] in 2008, [[18](#bib.bib18)] in 2014 and
    [[19](#bib.bib19)] in 2017. The hand-engineering feature for image retrieval was
    a very active research area. However, its performance was limited as the hand-engineered
    features are not able to represent the image characteristics in an accurate manner.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c480ae0a6fe9059438b91485dbef341.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The pipeline of state-of-the-art feature representation is replaced
    by the CNN based feature representation.'
  prefs: []
  type: TYPE_NORMAL
- en: I-B Distance Metric Learning based Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The distance metric learning has been also used very extensively for feature
    vectors representation [[20](#bib.bib20)]. It is also explored well for image
    retrieval [[21](#bib.bib21)]. Some notable deep metric learning based image retrieval
    approaches include Contextual constraints distance metric learning [[22](#bib.bib22)],
    Kernel-based distance metric learning [[23](#bib.bib23)], [[24](#bib.bib24)],
    Visuality-preserving distance metric learning [[25](#bib.bib25)], Rank-based distance
    metric learning [[26](#bib.bib26)], Semi-supervised distance metric learning [[27](#bib.bib27)],
    Hamming distance metric learning [[28](#bib.bib28)], [[29](#bib.bib29)], and Rank
    based metric learning [[30](#bib.bib30)], [[31](#bib.bib31)]. Generally, the deep
    metric learning based approaches have shown the promising retrieval performance
    compared to hand-crafted approaches. However, most of the existing deep metric
    learning based methods rely on the linear distance functions which limits its
    discriminative ability and robustness to represent the non-linear data for image
    retrieval. Moreover, it is also not able to handle the multi-modal retrieval effectively.
  prefs: []
  type: TYPE_NORMAL
- en: I-C Deep Learning based Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From a decade, a shift has been observed in feature representation from hand-engineering
    to learning-based after the emergence of deep learning [[32](#bib.bib32)], [[33](#bib.bib33)].
    This transition is depicted in Fig. [1](#S1.F1 "Figure 1 ‣ I-A Hand-crafted Descriptor
    based Image Retrieval ‣ I Introduction ‣ A Decade Survey of Content Based Image
    Retrieval using Deep Learning") where the convoltional neural networks based feature
    learning replaces the state-of-the-art pipeline of traditional hand-engineered
    feature representation. The deep learning is a hierarchical feature representation
    technique to learn the abstract features from data which are important for that
    dataset and application [[34](#bib.bib34)]. Based on the type of data to be processed,
    different architectures came into existence such as Artificial Neural Network
    (ANN)/ Multilayer Perceptron (MLP) for 1-D data [[35](#bib.bib35)], [[36](#bib.bib36)],
    Convolutional Neural Networks (CNN) for image data [[37](#bib.bib37)], [[38](#bib.bib38)],
    and Reurrent Neural Networks (RNN) for time-series data [[39](#bib.bib39)], [[40](#bib.bib40)].
    A huge progress has been made in this decade to utilize the power of deep learning
    for content based image retrieval [[32](#bib.bib32)], [[41](#bib.bib41)], [[42](#bib.bib42)],
    [[43](#bib.bib43)], [[44](#bib.bib44)]. Thus, this survey mainly focuses over
    the progress in state-of-the-art deep learning based models and features for content
    based image retrieval from its inception. A taxonomy for the same is portrayed
    in Fig. [2](#S1.F2 "Figure 2 ‣ I-C Deep Learning based Image Retrieval ‣ I Introduction
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning"). The
    major contributions of this survey can be outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/df274ca1464ce6210a97d971ece38beb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Taxonomy used in this survey to categorize the existing deep learning
    based image retrieval approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This survey covers the deep learning based image retrieval approaches very comprehensively
    in terms of evolution of image retrieval using deep learning, different supervision
    type, network type, descriptor type, retrieval type and other aspects.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In contrast to the recent reviews [[42](#bib.bib42)], [[21](#bib.bib21)], [[43](#bib.bib43)],
    this survey specifically covers the progress in image retrieval using deep learning
    in 2011-2020 decade. An informative taxonomy is provided with wide coverage of
    existing deep learning based image retrieval approaches as compared to the recent
    survey [[44](#bib.bib44)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This survey enriches the reader with the state-of-the-art image retrieval using
    deep learning methods with analysis from various perspectives.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This paper also presents the brief highlights and important discussions along
    with the comprehensive comparisons on benchmark datasets using the state-of-the-art
    deep learning based image retrieval approaches.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This survey is organized as follows: the background is presented in Section
    [II](#S2 "II Background ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning") the evolution of deep learning based image retrieval is compiled
    in Section [III](#S3 "III Evolution of Deep Learning for Content Based Image Retrieval
    (CBIR) ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning");
    the categorization of existing approaches based on the supervision type, network
    type, descriptor type, and retrieval type are discussed in Section [IV](#S4 "IV
    Different Supervision Categorization ‣ A Decade Survey of Content Based Image
    Retrieval using Deep Learning"), [V](#S5 "V Network Types For Image Retrieval
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning"), [VI](#S6
    "VI Type of Descriptors for Image Retrieval ‣ A Decade Survey of Content Based
    Image Retrieval using Deep Learning"), and [VII](#S7 "VII Retrieval Type ‣ A Decade
    Survey of Content Based Image Retrieval using Deep Learning"), respectively; some
    other aspects are highlighted in Section [VIII](#S8 "VIII Miscellaneous ‣ A Decade
    Survey of Content Based Image Retrieval using Deep Learning"); the performance
    comparison of the popular methods is performed in Section [IX](#S9 "IX Performance
    Comparison ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning");
    conclusions and future directions are presented in Section [X](#S10 "X Conclusion
    and Future Directives ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: II Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section the background is presented in terms of the commonly used evaluation
    metrics and benchmark datasets.
  prefs: []
  type: TYPE_NORMAL
- en: II-A Retrieval Evaluation Measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to judge the performance of image retrieval approaches, precision,
    recall and f-score are the common evaluation metrics. The mean average precision
    ($mAP$) is very commonly used in the literature. The precision is defined as the
    percentage of correctly retrieved images out of the total number of retrieved
    images. The recall is another performance measure being used for image retrieval
    by computing the percentage of correctly retrieved images out of the total number
    of relevant images present in the dataset. The f-score is computed from the harmonic
    mean of precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: The summary of large-scale datasets for deep learning based image
    retrieval.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset Year #Classes Training Test Image Type CIFAR-10 [[45](#bib.bib45)]
    2009 10 50,000 10,000 Object Category Images NUS-WIDE [[46](#bib.bib46)] 2009
    21 97,214 65,075 Scene Images MNIST [[47](#bib.bib47)] 1998 10 60,000 10,000 Handwritten
    Digit Images SVHN [[48](#bib.bib48)] 2011 10 73,257 26,032 House Number Images
    SUN397 [[49](#bib.bib49)] 2010 397 100,754 8,000 Scene Images UT-ZAP50K [[50](#bib.bib50)]
    2014 4 42,025 8,000 Shoes Images Yahoo-1M [[51](#bib.bib51)] 2015 116 1,011,723
    112,363 Clothing Images ILSVRC2012 [[52](#bib.bib52)] 2012 1,000 $\sim$1.2 M 50,000
    Object Category Images MS COCO [[53](#bib.bib53)] 2015 80 82,783 40,504 Common
    Object Images MIRFlicker-1M [[54](#bib.bib54)] 2010 - 1 M - Scene Images Google
    Landmarks [[55](#bib.bib55)] 2017 15 K $\sim$1 M - Landmark Images Google Landmarks
    v2 [[56](#bib.bib56)] 2020 200 K 5 M - Landmark Images Clickture [[57](#bib.bib57)]
    2013 73.6 M 40 M - Search Log'
  prefs: []
  type: TYPE_NORMAL
- en: \startchronology
  prefs: []
  type: TYPE_NORMAL
- en: '[startyear=1,stopyear=43, startdate=false, color=blue!100, height=0.2cm, stopdate=false,
    arrow=true, arrowwidth=0.6cm, arrowheight=0.45cm] \setupchronoeventtextstyle=,datestyle=
    \chronoevent[markdepth=-15pt, year=false]12011 \chronoevent[markdepth=15pt,year=false,textwidth=1.5cm]2Deep
    Autoencoder [[58](#bib.bib58)] \chronoevent[markdepth=-15pt, year=false]32012
    \chronoevent[markdepth=-30pt,year=false,textwidth=1.5cm]4Deep Multi-View Hashing
    (DMVH) [[59](#bib.bib59)] \chronoevent[markdepth=-15pt, year=false]52013 \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]6Online
    Multimodal Deep Similarity Learning (OMDSL) [[60](#bib.bib60)] \chronoevent[markdepth=-15pt,
    year=false]72014 \chronoevent[markdepth=-50pt,year=false,textwidth=1.5cm]8Neural
    Codes [[61](#bib.bib61)] \chronoevent[markdepth=15pt,year=false,textwidth=1cm]9Deep
    Ranking Model [[62](#bib.bib62)] \chronoevent[markdepth=-15pt, year=false]102015
    \chronoevent[markdepth=-25pt,year=false,textwidth=1.5cm]11Stack of Conv Layers
    [[63](#bib.bib63)] \chronoevent[markdepth=35pt,year=false,textwidth=1cm]12Triplet
    Ranking Loss [[63](#bib.bib63)] \chronoevent[markdepth=-50pt,year=false,textwidth=1cm]13DRSCH
    [[64](#bib.bib64)] \chronoevent[markdepth=-15pt, year=false]142016 \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]15Region
    based CNN Descriptor [[65](#bib.bib65)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.6cm]16Lifted
    Structure Embedding [[66](#bib.bib66)] \chronoevent[markdepth=40pt,year=false,textwidth=1.5cm]17Deep
    Hashing Network (DHN) [[67](#bib.bib67)] \chronoevent[markdepth=-55pt,year=false,textwidth=1.5cm]18Deep
    Quantization Network (DQN) [[68](#bib.bib68)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]19Feature
    Aggregation [[69](#bib.bib69)] \chronoevent[markdepth=-30pt,year=false,textwidth=1.3cm]20Sigmoid
    based Feature [[70](#bib.bib70)] \chronoevent[markdepth=-15pt, year=false]212017
    \chronoevent[markdepth=40pt,year=false,textwidth=1cm]22HashNet [[71](#bib.bib71)]
    \chronoevent[markdepth=-60pt,year=false,textwidth=2cm]23 Siamese Network [[72](#bib.bib72)]
    \chronoevent[markdepth=10pt,year=false,textwidth=1.3cm]24Mask based Feature [[73](#bib.bib73)]
    \chronoevent[markdepth=-30pt,year=false,textwidth=1.3cm]25Bilinear Network [[74](#bib.bib74)]
    \chronoevent[markdepth=-15pt, year=false]262018 \chronoevent[markdepth=30pt,year=false,textwidth=1.3cm]27Deep
    Cauchy Hashing (DCH) [[75](#bib.bib75)] \chronoevent[markdepth=-60pt,year=false,textwidth=1.3cm]28GreedyHash
    [[76](#bib.bib76)] \chronoevent[markdepth=5pt,year=false,textwidth=1.3cm]29Policy
    Gradient [[77](#bib.bib77)] \chronoevent[markdepth=-20pt,year=false,textwidth=1.3cm]30Relaxation
    Approach [[78](#bib.bib78)] \chronoevent[markdepth=40pt,year=false,textwidth=1.3cm]31Deep
    Index-Compatible Hashing (DICH) [[79](#bib.bib79)] \chronoevent[markdepth=-15pt,
    year=false]322019 \chronoevent[markdepth=-45pt,year=false,textwidth=1.3cm]33Deep
    Incremental Hashing Network (DIHN) [[80](#bib.bib80)] \chronoevent[markdepth=7pt,year=false,textwidth=1.3cm]34Deep
    Spherical Quantization (DSQ) [[81](#bib.bib81)] \chronoevent[markdepth=-30pt,year=false,textwidth=1.3cm]35DistillHash
    [[82](#bib.bib82)] \chronoevent[markdepth=45pt,year=false,textwidth=2cm]36Deep
    Progressive Hashing (DPH) [[83](#bib.bib83)] \chronoevent[markdepth=-20pt,year=false,textwidth=1.3cm]37DHA
    [[84](#bib.bib84)] \chronoevent[markdepth=5pt,year=false,textwidth=1.3cm]38Just-Maximizing-Likelihood
    Hashing (JMLH) [[85](#bib.bib85)] \chronoevent[markdepth=-45pt,year=false,textwidth=1.1cm]39Deep
    Variational Binaries (DVB) [[86](#bib.bib86)] \chronoevent[markdepth=-15pt, year=false]402020
    \chronoevent[markdepth=55pt,year=false,textwidth=2.2cm]41Twin-Bottleneck Hashing
    (TBH) [[87](#bib.bib87)] \chronoevent[markdepth=-22pt,year=false,textwidth=1.1cm]42Co-occurrences
    from Deep Conv Features [[88](#bib.bib88)] \chronoevent[markdepth=10pt,year=false,textwidth=1.45cm]43Deep
    Position-Aware Hashing (DPAH) [[89](#bib.bib89)] \stopchronology'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: A chronological view of deep learning based image retrieval methods
    depicting its evolution from 2011 to 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: II-B Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the inception of deep learning models, various large-scale datasets have
    been created to facilitate the research in image recognition and retrieval. The
    details of large-scale datasets are summarized in Table [I](#S2.T1 "TABLE I ‣
    II-A Retrieval Evaluation Measures ‣ II Background ‣ A Decade Survey of Content
    Based Image Retrieval using Deep Learning"). Datasets having various types of
    images are available to test the deep learning based approaches such as object
    category datasets [[45](#bib.bib45)], [[52](#bib.bib52)], [[53](#bib.bib53)],
    scene datasets [[46](#bib.bib46)], [[49](#bib.bib49)], [[90](#bib.bib90)], digit
    datasets [[47](#bib.bib47)], [[48](#bib.bib48)], apparel datasets [[50](#bib.bib50)],
    [[51](#bib.bib51)], landmark datasets [[55](#bib.bib55)], [[56](#bib.bib56)],
    etc. The CIFAR-10 dataset is very widely used object category datset [[45](#bib.bib45)].
    The ImageNet (ILSVRC2012), a large-scale dataset, is also an object category dataset
    with more than a million number of images [[52](#bib.bib52)]. The MS COCO dataset
    [[53](#bib.bib53)] created for common object detection is also utilized for image
    retrieval purpose. Among scene image datasets commonly used for retrieval purpose,
    the NUS-WIDE dataset is from National University of Singapore [[46](#bib.bib46)];
    the Sun397 is a scene understanding dataset from 397 categories with more than
    one lakh images [[49](#bib.bib49)], [[91](#bib.bib91)]; and the MIRFlicker-1M
    [[90](#bib.bib90)] dataset consists of a million images downloaded from the social
    photography site Flickr. The MNIST dataset is one of the old and large-scale digit
    image datasets [[47](#bib.bib47)] consisting of optical characters. The SVHN is
    another digit dataset [[48](#bib.bib48)] from the street view house number images
    which is more complex than MNIST dataset. The shoes apparel dataset, namely UT-ZAP50K
    [[50](#bib.bib50)], consists of roughly 50K images. The Yahoo-1M is another apparel
    large-scale dataset used in [[51](#bib.bib51)] for image retrieval. The Google
    landmarks dataset is having around a million landmark images [[55](#bib.bib55)].
    The extended version of Google landmarks (i.e., v2) [[56](#bib.bib56)] contains
    around 5 million landmark images. There are more datasets used for retrieval in
    the literature, such as Corel, Oxford, Paris, etc., however, these are not the
    large-scale datasets. The CIFAR-10, MNIST, SVHN and ImageNet are the widely used
    datasets in majority of the research. Clickture is a common dataset for search
    log based on the queries of users [[57](#bib.bib57)]. The click property has been
    utilized for different applications, such as cross-view learning for image search
    [[92](#bib.bib92)], distance metric learning for image ranking [[93](#bib.bib93)]
    and deep structure-preserving embeddings with visual attention [[94](#bib.bib94)].
  prefs: []
  type: TYPE_NORMAL
- en: Note that only CIFAR-10 and MNIST datasets contain the same number of samples
    in each category. Other datasets are created generally in unconstrained environment
    with huge number of samples, thus the classes are not well balanced. The choice
    of dataset can be dependent upon the scenario where image retrieval models need
    to be used, such as object category and scene datasets for unconstrained environment,
    apparel datasets for e-commerce applications, and landmark datasets for driving
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: III Evolution of Deep Learning for Content Based Image Retrieval (CBIR)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The deep learning based generation of descriptors or hash codes is the recent
    trends large-scale content based image retrieval, due to its computational efficiency
    and retrieval quality [[21](#bib.bib21)]. In this section, a journey of deep learning
    models for image retrieval from 2011 to 2020 is presented as a chronological overview
    in Fig. [3](#S2.F3 "Figure 3 ‣ II-A Retrieval Evaluation Measures ‣ II Background
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: III-1 2011-2013
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Among the initial attempts, in 2011, Krizhevsky and Hinton have used a deep
    autoencoder to map the images to short binary codes for content based image retrieval
    (CBIR) [[58](#bib.bib58)]. Kang et al. (2012) have proposed a deep multi-view
    hashing to generate the code for CBIR from multiple views of data by modeling
    the layers with view-specific and shared hidden nodes [[59](#bib.bib59)]. In 2013,
    Wu et al. have considered the multiple pretrained stacked denoising autoencoders
    over low features of the images [[60](#bib.bib60)]. They also fine tune the multiple
    deep networks on the output of the pretrained autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: III-2 2014
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In an outstanding work, the activations of the top layers of a large convolutional
    neural network (CNN) are utilized as the descriptors (neural codes) for image
    retrieval [[61](#bib.bib61)] as depicted in Fig. [4](#S3.F4 "Figure 4 ‣ III-2
    2014 ‣ III Evolution of Deep Learning for Content Based Image Retrieval (CBIR)
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning"). A very
    promising performance has been recorded using the neural codes for image retrieval
    even if the model is trained on un-related data. The neural code is compressed
    using principal component analysis (PCA) to generate the compact descriptor. In
    2014, deep ranking model is investigated by learning the similarity metric directly
    from images [[62](#bib.bib62)]. Basically, the triplets are employed to capture
    the inter-class and intra-class image differences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/406de49a8c2833d5e19a0a41343711f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The illustration of the neural code generation from a convolutional
    neural network (CNN) [[61](#bib.bib61)].'
  prefs: []
  type: TYPE_NORMAL
- en: III-3 2015
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2015, a deep architecture is developed which consists of a stack of convolution
    layers to produce the intermediate image features [[63](#bib.bib63)] which are
    used to generate the hash bits. The triplet ranking loss is also utilized to incorporate
    the inter-class and intra-class differences in [[63](#bib.bib63)] for image retrieval.
    Zhang et al. (2015) have developed a deep regularized similarity comparison hashing
    (DRSCH) by training a deep CNN model to simultaneously optimize the discriminative
    image features and hash functions [[64](#bib.bib64)].
  prefs: []
  type: TYPE_NORMAL
- en: III-4 2016
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2016, Gordo et al. have pooled the relevant regions to form the descriptor
    with the help of a region proposal network to prioritize the important object
    regions [[65](#bib.bib65)]. Song et al. (2016) have computed the lifted structure
    loss between the CNN and the original features [[66](#bib.bib66)]. Supervised
    deep hashing network (DHN) learns the important image representation by controlling
    the quantization error [[67](#bib.bib67)]. At the same time, Cao et al. have introduced
    a deep quantization network (DQN) which is very similar to the DHN model [[68](#bib.bib68)].
    The CNN based features are aggregated in [[69](#bib.bib69)] with the help of rank-aware
    multi-assignment and direction based combination. A sigmoid layer is added before
    the loss layer of a CNN to learn the binary code for CBIR [[70](#bib.bib70)].
  prefs: []
  type: TYPE_NORMAL
- en: III-5 2017
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2017, Cao et al. have proposed HashNet deep architecture to generate the
    hash code by a continuation method [[71](#bib.bib71)]. It learns the non-smooth
    binary activations using the continuation method to generate the binary hash codes
    from imbalanced similarity data. Gordo et al. (2017) have shown that the noisy
    training data, inappropriate deep architecture and suboptimal training procedure
    are the main hurdle to utilize the deep learning for image retrieval [[72](#bib.bib72)].
    Different masking schemes are used in [[73](#bib.bib73)] to select the prominent
    CNN features for image retrieval. A bilinear network with two parallel CNNs is
    also used as a feature extractors [[74](#bib.bib74)].
  prefs: []
  type: TYPE_NORMAL
- en: III-6 2018
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2018, Cao et al. have investigated a deep cauchy hashing (DCH) model for
    binary hash code with the help of a pairwise cross-entropy loss based on Cauchy
    distribution [[75](#bib.bib75)]. Su et al. have employed the greedy hash by transmitting
    the gradient as intact during the backpropagation for hash coding layer which
    uses the sign function in forward propagation [[76](#bib.bib76)]. Different approaches
    such as policy gradient [[77](#bib.bib77)] and series expansion [[78](#bib.bib78)]
    are also utilized to train the models. Deep index-compatible hashing (DICH) method
    [[79](#bib.bib79)] is investigated by minimizing the number of similar bits between
    the binary codes of inter-class images.
  prefs: []
  type: TYPE_NORMAL
- en: III-7 2019
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2019, a deep incremental hashing network (DIHN) is proposed in [[80](#bib.bib80)]
    to directly learn the hash codes corresponding to the new class coming images,
    while retaining the hash codes of existing class images. A supervised quantization
    based points representation on a unit hypersphere is used in deep spherical quantization
    (DSQ) model [[81](#bib.bib81)]. DistillHash [[82](#bib.bib82)] distills data pairs
    and learns deep hash functions from the distilled data set by employing the Bayesian
    learning framework. A deep progressive hashing (DPH) model is developed to generate
    a sequence of binary codes by utilizing the progressively expanded salient regions
    [[83](#bib.bib83)]. Adaptive loss function based deep hashing [[84](#bib.bib84)],
    just-maximizing-likelihood hashing (JMLH) [[85](#bib.bib85)] and deep variational
    binaries (DVB) [[86](#bib.bib86)] are other approaches discovered in 2019.
  prefs: []
  type: TYPE_NORMAL
- en: III-8 2020
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, in 2020, Shen et al. have come up with a twin-bottleneck hashing (TBH)
    model between encoder and decoder networks [[87](#bib.bib87)]. They have employed
    the binary and continuous bottlenecks as the latent variables in a collaborative
    manner. Forcen et al. (2020) have utilized the last convolution layer of CNN representation
    by modeling the co-occurrences from deep convolutional features [[88](#bib.bib88)].
    A deep position-aware hashing (DPAH) model is proposed in 2020 [[89](#bib.bib89)]
    which constraints the distance between data samples and class centers.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the methods developed between 2011 and 2015 use the features learnt
    by the autoencoders and convolutional neural networks. However, these methods
    face the issues in terms of the less discriminative ability as the models are
    generally trained for the classification problem and the information loss due
    to the quantization of features. Image retrieval using deep learning has witnessed
    a huge growth in between 2016 and 2020\. As the image retrieval application needs
    feature learning for matching, different types of networks have been utilized
    to do so. The recent methods have designed the several objective functions which
    lead to the high inter class separation and high intra class condensation in feature
    space. Moreover, the development in different network architectures has also led
    to the growth in the image retrieval area. The key issue being addressed by deep
    learning methods is to learn very discriminative, robust and compact features
    for image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: IV Different Supervision Categorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section covers the image retrieval methods in terms of the different supervision
    types. Basically, supervised, unsupervised, semi-supervised, weakly-supervised,
    pseudo-supervised and self-supervised approaches are included.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Supervised Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The supervised deep learning models are used by researchers very heavily to
    learn the class specific and discriminative features for image retrieval. In 2014,
    Xia et al. have used a CNN to learn the representation of images which is used
    to generate a hash code and class labels [[95](#bib.bib95)]. The promising performance
    is reported over MNIST, CIFAR-10 and NUS-WIDE datasets. Shen et al. (2015) [[96](#bib.bib96)]
    have proposed the supervised discrete hashing (SDH) based generation of image
    description with the help of the discrete cyclic coordinate descent for retrieval.
    Liu et al. (2016) have done the revolutionary work by introducing a deep supervised
    hashing (DSH) method to learn the binary codes from the similar/dissimilar pairs
    of images [[97](#bib.bib97)]. A similar work is also presented in deep pairwise-supervised
    hashing (DPSH) method for image retrieval [[98](#bib.bib98)]. The pair-wise labels
    are extended to the triplet labels (i.e., query, positive and negative images)
    to train a shared deep CNN model for feature learning [[99](#bib.bib99)]. An independent
    layer-wise local updates are performed in [[100](#bib.bib100)] to efficiently
    train a very deep supervised hashing (VDSH) model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2017, Li et al. have used the classification information and the pairwise
    label information in a single framework for the learning of the deep supervised
    discrete hashing (DSDH) codes [[101](#bib.bib101)]. The supervised semantics-preserving
    deep hashing (SSDH) model integrates the retrieval and classification characteristics
    in feature learning [[102](#bib.bib102)]. The scalable image search is performed
    in [[103](#bib.bib103)] by introducing the following three characteristics: 1)
    minimizing the loss between the real-valued code and equivalent converted binary
    code, 2) ensuring the even distribution among each bit in the binary codes, and
    3) decreasing the redundancy of a bit in the binary code.'
  prefs: []
  type: TYPE_NORMAL
- en: The supervised training has been also the choice in asymmetric hashing [[104](#bib.bib104)].
    A deep product quantization (DPQ) model is followed in supervised learning mode
    for image search and retrieval [[105](#bib.bib105)]. The supervised deep feature
    embedding is also used with the hand crafted features [[106](#bib.bib106)]. A
    very recently, a multi-Level hashing of deep features is performed by Ng et al.
    (2020) [[107](#bib.bib107)]. An angular hashing loss function is used to train
    the network in the supervised fashion [[108](#bib.bib108)]. A supervised hashing
    is also used for the multi-deep ranking [[109](#bib.bib109)] to improve the retrieval
    efficiency. Other supervised approaches are deep binary hash codes [[51](#bib.bib51)],
    deep hashing network [[67](#bib.bib67)], deep spherical quantization [[81](#bib.bib81)],
    and adaptive loss based supervised deep learning to hash [[84](#bib.bib84)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Unsupervised Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though the supervised models have shown promising performance for retrieval,
    it is difficult to get the labelled large-scale data always. Thus, several unsupervised
    models have been also investigated which do not require the class labels. The
    unsupervised models generally enforce the constraints on hash code and/or generated
    output to learn the features.
  prefs: []
  type: TYPE_NORMAL
- en: Erin et al. (2015) [[110](#bib.bib110)] have used the deep networks in an unsupervised
    manner to learn the hash code with the help of the constraints like quantization
    loss, balanced bits and independent bits. Huang et al. (2016) [[111](#bib.bib111)]
    have utilized the CNN coupled with unsupervised discriminative clustering. In
    an outstanding work, DeepBit utilizes the constraints like minimal quantization
    loss, evenly distributed codes and uncorrelated bits for unsupervised image retrieval
    [[112](#bib.bib112)], [[113](#bib.bib113)]. In order to improve the robustness
    of DeepBit, a rotation data augmentation based fine tuning is also performed.
    However, the DeepBit model suffers with the severe quantization loss due to the
    rigid binarization of data using sign function without considering its distribution
    property. Deep binary descriptor with multiquantization (DBD-MQ) [[114](#bib.bib114)]
    tackles the quantization problem of DeepBit by jointly learning the parameters
    and the binarization functions using a K-AutoEncoders (KAEs).
  prefs: []
  type: TYPE_NORMAL
- en: It is observed in [[115](#bib.bib115)] that unsupervised CNN can learn more
    distinctive features if fine tuned with hard positive and hard negative examples.
    The patch representation using a patch convolutional kernel network is also adapted
    for patch retrieval [[116](#bib.bib116)]. An anchor image, a rotated image and
    a random image based triplets are used in unsupervised triplet hashing (UTH) to
    learn the binary codes for image retrieval [[117](#bib.bib117)]. The UTH objective
    function uses the combination of discriminative loss, quantization loss and entropy
    loss. An unsupervised similarity-adaptive deep hashing (SADH) is proposed in [[118](#bib.bib118)]
    by updating a similarity graph and optimizing the binary codes. Xu et al. (2018)
    [[119](#bib.bib119)] have proposed a semantic-aware part weighted aggregation
    using part-based detectors for CBIR systems. Unsupervised generative adversarial
    networks [[120](#bib.bib120)], [[121](#bib.bib121)], [[122](#bib.bib122)] are
    also investigated for image retrieval. The distill data pairs [[82](#bib.bib82)]
    and deep variational networks [[86](#bib.bib86)] are also used for unsupervised
    image retrieval. The pseudo triplets based unsupervised deep triplet hashing (UDTH)
    technique [[123](#bib.bib123)] is introduced for scalable image retrieval. Very
    recently unsupervised deep transfer learning has been exploited by Liu et al.
    (2020) [[124](#bib.bib124)] for retrieval in remote sensing images.
  prefs: []
  type: TYPE_NORMAL
- en: Though the unsupervised models do not need labelled data, its performance is
    generally lower than the supervised approaches. Thus, researchers have explored
    the models between supervised and unsupervised, such as semi-supervised, weakly-supervised,
    pseudo-supervised and self-supervised.
  prefs: []
  type: TYPE_NORMAL
- en: \startchronology
  prefs: []
  type: TYPE_NORMAL
- en: '[startyear=1,stopyear=50, startdate=false, color=blue!100, stopdate=false,
    arrow=true, arrowwidth=0.8cm, arrowheight=0.5cm] \setupchronoeventtextstyle=,datestyle=
    \chronoevent[markdepth=-15pt, year=false]12011 \chronoevent[markdepth=-30pt,year=false,textwidth=1cm]2Deep
    Autoencoder [[58](#bib.bib58)] \chronoevent[markdepth=-15pt, year=false]32013
    \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]4Denoising Autoencoder [[60](#bib.bib60)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=-15pt, year=false]52014 \chronoevent[markdepth=-30pt,year=false,textwidth=1cm]6Corres-pondance
    Autoencoder [[125](#bib.bib125)] \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]8Siamese
    [[62](#bib.bib62)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=65pt,year=false,textwidth=2cm]7CNN Features [[95](#bib.bib95)],
    [[126](#bib.bib126)] \chronoevent[markdepth=40pt,year=false,textwidth=2cm]7Neural
    Code [[61](#bib.bib61)] \chronoevent[markdepth=5pt,year=false,textwidth=2cm]7CNN
    off-the-shelf [[32](#bib.bib32)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=-15pt, year=false]92015 \chronoevent[markdepth=-30pt,year=false,textwidth=1cm]10Binary
    Autoencoder [[127](#bib.bib127)] \chronoevent[markdepth=-70pt,year=false,textwidth=1cm]11Patch
    + Siamese [[128](#bib.bib128)], [[129](#bib.bib129)] \chronoevent[markdepth=-105pt,year=false,textwidth=1cm]13Adjacency
    Consistency Triplet [[64](#bib.bib64)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]13Triplet
    Ranking [[63](#bib.bib63)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=-15pt, year=false]142016 \chronoevent[markdepth=-78pt,year=false,textwidth=1cm]16Triplet
    Based [[130](#bib.bib130)], [[131](#bib.bib131)], [[132](#bib.bib132)], [[99](#bib.bib99)],
    [[133](#bib.bib133)] \chronoevent[markdepth=-45pt,year=false,textwidth=1cm]16Image
    Pair [[97](#bib.bib97)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=70pt,year=false,textwidth=2cm]15Pairwise Labels [[97](#bib.bib97)],
    [[98](#bib.bib98)] \chronoevent[markdepth=38pt,year=false,textwidth=2cm]15Different
    Losses [[65](#bib.bib65)], [[68](#bib.bib68)], [[71](#bib.bib71)] \chronoevent[markdepth=5pt,year=false,textwidth=2cm]15CNN
    Features [[112](#bib.bib112)], [[97](#bib.bib97)], [[134](#bib.bib134)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=-15pt, year=false]172017 \chronoevent[markdepth=-120pt,year=false,textwidth=1cm]19Triplet
    + Siamese [[72](#bib.bib72)] \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]19Triplet
    Quantization Loss [[135](#bib.bib135)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]19Triplet
    Hashing [[117](#bib.bib117)] \chronoevent[markdepth=-50pt,year=false,textwidth=1cm]21Fisher
    + Siamese [[136](#bib.bib136)] \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]24Attention
    Based [[55](#bib.bib55)], [[137](#bib.bib137)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]24Two-stream
    Attention [[138](#bib.bib138)] \chronoevent[markdepth=-85pt,year=false,textwidth=1cm]27LSTM
    Based [[139](#bib.bib139)], [[139](#bib.bib139)] \chronoevent[markdepth=-35pt,year=false,textwidth=1cm]27Recurrent
    Neural Hashing [[140](#bib.bib140)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=55pt,year=false,textwidth=1.5cm]21HashNet [[71](#bib.bib71)] \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]21CNN
    Features [[114](#bib.bib114)], [[103](#bib.bib103)], [[141](#bib.bib141)], [[142](#bib.bib142)]
    \chronoevent[markdepth=75pt,year=false,textwidth=1.5cm]24GAN Based [[143](#bib.bib143)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=-15pt, year=false]282018 \chronoevent[markdepth=-75pt,year=false,textwidth=1cm]30Binary
    Siamese [[144](#bib.bib144)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]32Attention
    GAN [[145](#bib.bib145)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=72pt,year=false,textwidth=2cm]29SSAH [[146](#bib.bib146)], SSGAH
    [[147](#bib.bib147)], GDH [[148](#bib.bib148)] \chronoevent[markdepth=50pt,year=false,textwidth=2.5cm]29HashGAN
    [[120](#bib.bib120)], [[149](#bib.bib149)] \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]29Binary
    GAN [[121](#bib.bib121)], Regularized GAN [[150](#bib.bib150)] \chronoevent[markdepth=5pt,year=false,textwidth=1cm]33CNN
    Features [[118](#bib.bib118)], [[113](#bib.bib113)], [[151](#bib.bib151)] \chronoevent[markdepth=67pt,year=false,textwidth=1.5cm]34Reinforcement
    Learning Based [[77](#bib.bib77)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=-15pt, year=false]352019 \chronoevent[markdepth=-115pt,year=false,textwidth=1.2cm]36Deep
    Variational Binaries [[86](#bib.bib86)] \chronoevent[markdepth=-75pt,year=false,textwidth=1.5cm]36Unsupervised
    Autoencoder [[123](#bib.bib123)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.5cm]36Relaxed
    Binary Autoencoder [[152](#bib.bib152)] \chronoevent[markdepth=-100pt,year=false,textwidth=1.2cm]39Triplet
    Based [[153](#bib.bib153)], [[123](#bib.bib123)] \chronoevent[markdepth=-125pt,year=false,textwidth=1cm]41Gradient
    Attention [[154](#bib.bib154)] \chronoevent[markdepth=-72pt,year=false,textwidth=1cm]41Spatial
    Attention [[155](#bib.bib155)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]41Two-stream
    Attention [[156](#bib.bib156)] \chronoevent[markdepth=-100pt,year=false,textwidth=1.2cm]43LSTM
    Based [[83](#bib.bib83)], [[157](#bib.bib157)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=5pt,year=false,textwidth=1cm]39CNN Features [[106](#bib.bib106)],
    [[158](#bib.bib158)] \chronoevent[markdepth=70pt,year=false,textwidth=1.5cm]42GAN
    Based [[159](#bib.bib159)], [[160](#bib.bib160)] \chronoevent[markdepth=38pt,year=false,textwidth=1.5cm]42Unsupervised
    GAN [[122](#bib.bib122)] \chronoevent[markdepth=5pt,year=false,textwidth=1cm]42SSAH
    [[161](#bib.bib161)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=-15pt, year=false]442020 \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]45Bottleneck
    Autoencoder [[87](#bib.bib87)] \chronoevent[markdepth=-125pt,year=false,textwidth=1cm]48SBS-CNN
    [[124](#bib.bib124)] \chronoevent[markdepth=-95pt,year=false,textwidth=1cm]48Pairwise
    [[162](#bib.bib162)] \chronoevent[markdepth=-63pt,year=false,textwidth=1.5cm]48Adversarial
    Siamese [[163](#bib.bib163)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]50Second-Order
    Similarity [[164](#bib.bib164)]'
  prefs: []
  type: TYPE_NORMAL
- en: \chronoevent
  prefs: []
  type: TYPE_NORMAL
- en: '[markdepth=5pt,year=false,textwidth=1cm]45CNN Features [[165](#bib.bib165)]
    \chronoevent[markdepth=75pt,year=false,textwidth=1.5cm]47BGAN+ [[166](#bib.bib166)]
    \chronoevent[markdepth=35pt,year=false,textwidth=1.5cm]47Stack GAN [[163](#bib.bib163)]
    \chronoevent[markdepth=5pt,year=false,textwidth=1.5cm]50Reinforcement Learning
    Based [[167](#bib.bib167)]'
  prefs: []
  type: TYPE_NORMAL
- en: \stopchronology
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: A chronological view of deep learning based image retrieval methods
    depicting the different type of neural networks used from 2011 to 2020\. The convolutional
    neural network, autoencoder network, siamese & triplet network, recurrent neural
    network, generative adversarial network, attention network and reinforcement learning
    network based deep learning approches for image retrieval are depicted in Red,
    Cyan, Magenta, Black, Blue, Green, and Yellow colors, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Semi, Weakly, Pseudo and Self -supervised Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The semi-supervised approaches generally use a combination of labelled and unlabelled
    data for feature learning [[168](#bib.bib168)], [[169](#bib.bib169)]. Semi-supervised
    deep hashing (SSDH) [[141](#bib.bib141)] uses labelled data for the empirical
    error minimization and both labelled and unlabelled data for embedding error minimization.
    The generative adversarial learning has been also utilized extensively in semi-supervised
    image retrieval [[147](#bib.bib147)], [[161](#bib.bib161)], [[170](#bib.bib170)].
    A teacher-student based semi-supervised image retrieval [[171](#bib.bib171)] uses
    the pairwise information learnt by the teacher network as the guidance to train
    the student network.
  prefs: []
  type: TYPE_NORMAL
- en: Weakly-supervised approaches have been also explored for the image retrieval.
    Tang et al. (2017) have put forward a weakly-supervised multimodal hashing (WMH)
    by utilizing the local discriminative and geometric structures in the visual space
    [[172](#bib.bib172)]. Guan et al. (2018) [[173](#bib.bib173)] have performed the
    pre-training in weakly-supervised mode and fine-tuning in supervised mode. A weakly
    supervised deep hashing using tag embeddings (WDHT) [[174](#bib.bib174)] utilizes
    the word2vec semantic embeddings. A semantic guided hashing (SGH) [[175](#bib.bib175)]
    is used for image retrieval by simultaneously employing the weakly-supervised
    tag information and the inherent data relations.
  prefs: []
  type: TYPE_NORMAL
- en: The pseudo suervised networks have been also developed for image retrieval.
    The pseudo triplets are utilized in [[123](#bib.bib123)] for unsupervised image
    retrieval. K-means clustering based pseudo labels are generated and used for the
    training of a deep hashing network [[176](#bib.bib176)], [[177](#bib.bib177)].
    An appealing performance has been observed using pseudo labels over CIFAR-10 and
    Flickr datasets for image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: The self-supervision is another way of supervision used in some research works
    for image retrieval. For example, Li et al. (2018) [[146](#bib.bib146)] have used
    the adversarial networks in self-supervision mode by utilizing the multi-label
    annotations. Zhang et al. (2016) [[178](#bib.bib178)] have introduced a self-supervised
    temporal hashing (SSTH) for video retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following are the take aways from the above discussion on deep learning based
    models from the supervision perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The supervised approaches utilize the class-specific semantic information through
    the classification error apart from the other objectives related to the hash code
    generation. Generally, the performance of supervised models is better than other
    models due to learning of the fine-grained and class specific information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The unsupervised models make use of the unsupervised constraints on hash code
    (i.e., quantization loss, independent bits, etc.) and/or data reconstruction (i.e.,
    using an autoencoder type of networks) to learn the features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The semi-supervised approaches exploit the labelled and un-labelled data for
    the feature learning using deep networks. These approaches generally utilize the
    information from different modalities using different networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pseudo-supervised approaches generate the pseudo labels using some other
    methods to facilitate the training using generated labels. The self-supervised
    methods generate the temporal or generative information to learn the models over
    the training epochs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimal quantization error, independent bits, low dimensional feature, and
    discriminative code are the common objectives for most of the retrieval methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: V Network Types For Image Retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, deep learning based image retrieval approaches are presented
    in terms of the different architectures. A chronological overview from 2011 to
    2020 is illustrated in Fig. [5](#S4.F5 "Figure 5 ‣ IV-B Unsupervised Approaches
    ‣ IV Different Supervision Categorization ‣ A Decade Survey of Content Based Image
    Retrieval using Deep Learning") for different type of networks for image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Convolutional Neural Networks for Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional neural networks (CNN) based feature learning has been utilized
    extensively for image retrieval as shown in Fig. [4](#S3.F4 "Figure 4 ‣ III-2
    2014 ‣ III Evolution of Deep Learning for Content Based Image Retrieval (CBIR)
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning"). In 2014,
    CNN features off-the-shelf have shown a tremendous performance gain for image
    recognition and retrieval as compared to the hand-crafted features [[32](#bib.bib32)].
    At the same time the activations of trained CNN has been also explored as the
    neural code for retrieval [[61](#bib.bib61)]. An image representation learning
    has been also performed using the CNN model to generate the descriptor for image
    retrieval [[95](#bib.bib95)]. In 2016, pairwise labels are exploited to learn
    the CNN feature for image retrieval [[97](#bib.bib97)], [[98](#bib.bib98)]. The
    CNN activations are heavily used to generate the hash codes for efficient image
    retrieval by employing the different losses [[65](#bib.bib65)], [[68](#bib.bib68)],
    [[71](#bib.bib71)]. The abstract features of CNN are learnt for the image retrieval
    in different modes, such as unsupervised image retrieval [[112](#bib.bib112)],
    [[114](#bib.bib114)], [[118](#bib.bib118)], [[113](#bib.bib113)], supervised image
    retrieval [[95](#bib.bib95)], [[97](#bib.bib97)], [[103](#bib.bib103)], [[106](#bib.bib106)],
    semi-supervised image retrieval [[141](#bib.bib141)], cross-modal retrieval [[134](#bib.bib134)],
    [[142](#bib.bib142)], sketch based image retrieval [[151](#bib.bib151)], [[158](#bib.bib158)],
    and object retrieval [[126](#bib.bib126)], [[165](#bib.bib165)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a354b05a602ae4a63b7bbf4baf89eafe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: A typical Autoencoder network consisting of an Encoder and a Decoder
    network. Generally, the encoder is a CNN and the decoder is an up-CNN. The output
    of the encoder is a latent space which is used to generate the hash codes.'
  prefs: []
  type: TYPE_NORMAL
- en: V-B Autoencoder Networks based Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Autoencoder ($AE$) is a type of unsupervised neural network which can be used
    to reconstruct the input image from the latent space as portrayed in Fig. [6](#S5.F6
    "Figure 6 ‣ V-A Convolutional Neural Networks for Image Retrieval ‣ V Network
    Types For Image Retrieval ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning"). Basically, it consists of two networks, namely encoder ($En$)
    and decoder ($De$). The encoder network transforms the input ($I$) into latent
    feature space ($z$) as $En:I\rightarrow z$. Whereas, the decoder network tries
    to reconstruct the original image ($I^{\prime}$) from latent feature space as
    $De:z\rightarrow I^{\prime}$. The model is trained by minimizing the reconstruction
    error between original image ($I$) and reconstructed image ($I^{\prime}$) using
    $L_{1}$ or $L_{2}$ loss function.
  prefs: []
  type: TYPE_NORMAL
- en: The autoencoders have been very intensively used to learn the features as the
    latent space for image retrieval. In the initial attempts, the deep autoencoder
    was used for image retrieval in 2011 [[58](#bib.bib58)]. A stacked denoising autoencoder
    is used to train the multiple deep neural networks for retrieval task [[60](#bib.bib60)].
    Feng et al. (2014) have utilized the correspondence autoencoder (Corr-AE) for
    cross-modal retrieval [[125](#bib.bib125)]. A binary autoencoder is used to learn
    the binary code for fast image retrieval by reconstructing the image from that
    binary code function [[127](#bib.bib127)]. The use of autoencoder in image retrieval
    has witnessed a huge progressed in recent years, such as Deep variational binaries
    (DVB) using the variational Bayesian networks [[86](#bib.bib86)]; Autoencoder
    over the triplet [[123](#bib.bib123)]; and Relaxed binary autoencoder (RBA) [[152](#bib.bib152)]
    are investigated in 2019. In a recent work, double latent bottlenecks is used
    in autoencoder [[87](#bib.bib87)]. It includes binary latent variable and continuous
    latent variable. The latent variable bottleneck exchanges crucial information
    collaboratively and the binary codes bottleneck uses a code-driven graph to capture
    the intrinsic data structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6e2a15e08c101a88ae6d2911ef796123.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: (a) Siamese network computes the similarity between image pairs.
    (b) Triplet network minimizes the distance between the anchor and positive and
    maximizes the distance between the anchor and negative in feature space.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C Siamese and Triplet Networks for Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-C1 Siamese Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The siamese is type of neural network that exploits the distance between features
    of image pairs as depicted in Fig. [7](#S5.F7 "Figure 7 ‣ V-B Autoencoder Networks
    based Image Retrieval ‣ V Network Types For Image Retrieval ‣ A Decade Survey
    of Content Based Image Retrieval using Deep Learning")(a). The siamese network
    based learnt features have shown very promising performance for fine-grained image
    retrieval [[62](#bib.bib62)]. A pair of similar or dissimilar images is jointly
    processed by Liu et al. [[97](#bib.bib97)] to produce 1 or -1 output by CNN to
    learn the feature for image retrieval. Ong et al. (2017) have used the fisher
    vector computed on top of the CNN feature in autoencoder network to generate the
    discriminating feature descriptor for image retrieval [[136](#bib.bib136)]. The
    siamese network is also used to develop the light weight models for efficient
    image retrieval [[144](#bib.bib144)], [[124](#bib.bib124)]. A pairwise similarity-preserving
    quantization loss is employed in [[162](#bib.bib162)]. The siamese network is
    used with the stacked adversarial network in [[163](#bib.bib163)]. The siamese
    network is also used for patch based image matching [[128](#bib.bib128)], [[129](#bib.bib129)].
  prefs: []
  type: TYPE_NORMAL
- en: V-C2 Triplet Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A triplet network is a variation of siamese network which utilizes a triplet
    of images, including an anchor, a positive and a negative image as shown in Fig.
    [7](#S5.F7 "Figure 7 ‣ V-B Autoencoder Networks based Image Retrieval ‣ V Network
    Types For Image Retrieval ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning")(b). The triplet network minimizes the distance between the features
    of anchor and positive image and maximizes the distance between the features of
    anchor and negative image, simultaneously. In 2015, a triplet ranking loss is
    utilized on top of the shared CNN features to learn the network for computation
    of binary descriptors for image retrieval [[63](#bib.bib63)]. An adjacency consistency
    based regularization term is introduced in the triplet network to enforce the
    discriminative ability of the CNN feature description [[64](#bib.bib64)]. Zhuang
    et al. (2016) [[130](#bib.bib130)] have used triplet to learn the hash code by
    employing the relation weights matrix and graph cuts optimization. The triplet
    ranking loss, orthogonality constraint and softmax loss are minimized jointly
    in [[131](#bib.bib131)]. Triplet based siamese networks are als used for image
    rerieval [[132](#bib.bib132)], [[72](#bib.bib72)]. Triplet quantization based
    objective function minimizes the information loss [[135](#bib.bib135)], [[179](#bib.bib179)].
    The triplet based feature learning has been also exploited for sketch based image
    retrieval [[153](#bib.bib153)]. Triplets are also exploited for supervised hashing
    [[99](#bib.bib99)] and unsupervised hashing [[117](#bib.bib117)], [[123](#bib.bib123)]
    for image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: V-D Generative Adversarial Networks based Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The generative adversarial network (GAN) uses two networks, i.e., generator
    and discriminator. The generator network generates the new samples in the training
    set from the random vector. Whereas, the discriminator network distinguishes between
    generated image and original image. In 2018, Song et al. have introduced a binary
    generative adversarial network (BGAN) for generating the representational binary
    codes for image retrieval [[121](#bib.bib121)]. At the same time, a regularized
    GAN is used to introduce the BinGAN model [[150](#bib.bib150)] to learn the compact
    binary patterns. The BinGAN uses two regularizers, including a distance matching
    regularizer and a binarization representation entropy (BRE) regularizer. In 2018,
    the generative networks are also utilized in [[120](#bib.bib120)] to develop HashGAN
    in an unsupervised manner to generate the hash code for image retrieval. At the
    same time another HashGAN is developed by employing the paired conditional Wasserstein
    GAN for image retrieval [[149](#bib.bib149)]. GAN has been also used for cross-modal
    retrieval [[143](#bib.bib143)], [[145](#bib.bib145)], [[146](#bib.bib146)], [[160](#bib.bib160)],
    semi-supervised hashing [[147](#bib.bib147)], [[161](#bib.bib161)], sketch based
    image retrieval [[148](#bib.bib148)], [[159](#bib.bib159)], [[163](#bib.bib163)]
    and unsupervised adversarial hashing [[122](#bib.bib122)]. In 2020, binary generative
    adversarial networks based unified BGAN+ framework [[166](#bib.bib166)] is developed
    for image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: V-E Attention Networks for Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The attention has been observed as a very effective way of modelling the saliency
    information into the feature space to avoid the effect of background. In 2017,
    Noh et al. have used the attention-based keypoints to select the important deep
    local features [[55](#bib.bib55)]. Yang et al. (2017) have introduced a two-stream
    attentive CNNs by fusing a Main and an Auxiliary CNN (MAC) for image retrieval
    [[138](#bib.bib138)]. The main CNN focuses over the discriminative visual features
    for semantic information, whereas the auxiliary CNN focuses over the part of features
    for attentive information. Similarily, two sub-networks are employed in [[155](#bib.bib155)]
    for spatial attention and global features, respectively. Recently, Ng et al. [[164](#bib.bib164)]
    have computed the second-order similarity (SOS) loss over the attention based
    selected regions of the input image for image retrieval. The attention based models
    are developed for cross-modal retrieval [[145](#bib.bib145)] and fine-grained
    sketch-based image retrieval [[137](#bib.bib137)]. The gradient attention network
    based deep hashing [[154](#bib.bib154)] enforces the CNN binary features of a
    pair to minimize the distances between them, irrespective of their signs or directions.
    In order to localize the important image region for the feature description, an
    attentional heterogeneous bilinear network is employed in [[180](#bib.bib180)]
    for fashion image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: \startchronology
  prefs: []
  type: TYPE_NORMAL
- en: '[startyear=1,stopyear=49, startdate=false, color=blue!100, stopdate=false,
    arrow=true, arrowwidth=0.8cm, arrowheight=0.5cm] \setupchronoeventtextstyle=,datestyle=
    \chronoevent[markdepth=-15pt, year=false]12011 \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]2Deep
    Autoencoder [[58](#bib.bib58)] \chronoevent[markdepth=-15pt, year=false]32015
    \chronoevent[markdepth=-75pt,year=false,textwidth=1.5cm]4CNN [[110](#bib.bib110)]
    \chronoevent[markdepth=-50pt,year=false,textwidth=1.5cm]4DNN [[110](#bib.bib110)]
    \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]6Autoencoder [[127](#bib.bib127)]
    \chronoevent[markdepth=-55pt,year=false,textwidth=1cm]8Binary Code [[181](#bib.bib181)],
    [[96](#bib.bib96)] \chronoevent[markdepth=10pt,year=false,textwidth=1cm]4CNN Feature
    Aggregation [[182](#bib.bib182)] \chronoevent[markdepth=65pt,year=false,textwidth=2cm]6Multi
    CNN Fusion [[183](#bib.bib183)] \chronoevent[markdepth=42pt,year=false,textwidth=1.3cm]8CNN-VLAD
    [[184](#bib.bib184)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]8CNN-BoF
    [[185](#bib.bib185)] \chronoevent[markdepth=-15pt, year=false]92016 \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]10Binary
    DNN [[186](#bib.bib186)] \chronoevent[markdepth=-50pt,year=false,textwidth=1.5cm]12Binary
    Code [[97](#bib.bib97)], [[112](#bib.bib112)], [[178](#bib.bib178)], [[97](#bib.bib97)],
    [[112](#bib.bib112)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]14Siamese
    + Triplet [[132](#bib.bib132)] \chronoevent[markdepth=50pt,year=false,textwidth=1cm]11CNN
    Feature Aggregation [[69](#bib.bib69)] \chronoevent[markdepth=10pt,year=false,textwidth=1.3cm]13Bag
    of CNN Feature [[187](#bib.bib187)] \chronoevent[markdepth=-15pt, year=false]162017
    \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]17Siamese Network [[144](#bib.bib144)]
    \chronoevent[markdepth=-50pt,year=false,textwidth=1cm]19Binary Hashing [[101](#bib.bib101)],
    [[101](#bib.bib101)] \chronoevent[markdepth=-65pt,year=false,textwidth=1.2cm]22Real-valued
    Descriptor [[188](#bib.bib188)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]22Siamese
    Network [[136](#bib.bib136)], [[72](#bib.bib72)] \chronoevent[markdepth=60pt,year=false,textwidth=1.5cm]17Selective
    Conv Descriptor Aggregation (SCDA) [[189](#bib.bib189)] \chronoevent[markdepth=10pt,year=false,textwidth=1.5cm]17CNN
    High-Low Layer Fusion [[190](#bib.bib190)] \chronoevent[markdepth=60pt,year=false,textwidth=1.5cm]23Joint
    feature aggregation and hashing [[191](#bib.bib191)] \chronoevent[markdepth=10pt,year=false,textwidth=1.5cm]23Attention
    based aggregation [[138](#bib.bib138)] \chronoevent[markdepth=10pt,year=false,textwidth=0.7cm]20CNN
    + Hand-crafted Fusion [[192](#bib.bib192)] \chronoevent[markdepth=-15pt, year=false]242018
    \chronoevent[markdepth=-70pt,year=false,textwidth=1.5cm]26Binary Quantization
    [[75](#bib.bib75)], [[76](#bib.bib76)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]26Binary
    Hashing [[113](#bib.bib113)], [[120](#bib.bib120)], [[113](#bib.bib113)] \chronoevent[markdepth=-35pt,year=false,textwidth=1cm]29GAN
    [[121](#bib.bib121)], [[150](#bib.bib150)], [[120](#bib.bib120)] \chronoevent[markdepth=-70pt,year=false,textwidth=1.5cm]31Part-based
    CNN [[119](#bib.bib119)] \chronoevent[markdepth=60pt,year=false,textwidth=1.2cm]27Part-based
    weighting [[119](#bib.bib119)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]27Visual
    words learning [[193](#bib.bib193)] \chronoevent[markdepth=10pt,year=false,textwidth=1cm]30Multi-layer
    fusion of CNN features [[151](#bib.bib151)] \chronoevent[markdepth=-15pt, year=false]322019
    \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]33Triplet Network [[123](#bib.bib123)]
    \chronoevent[markdepth=-65pt,year=false,textwidth=1.5cm]35Deep Variational Network
    [[86](#bib.bib86)] \chronoevent[markdepth=-25pt,year=false,textwidth=1.3cm]38Binary
    Features [[194](#bib.bib194)], [[152](#bib.bib152)], [[195](#bib.bib195)], [[196](#bib.bib196)]
    \chronoevent[markdepth=-60pt,year=false,textwidth=1.2cm]41Real-valued Descriptors
    [[197](#bib.bib197)], [[198](#bib.bib198)] \chronoevent[markdepth=60pt,year=false,textwidth=1.5cm]33Joint
    feature aggregation and hashing [[152](#bib.bib152)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]35Selective
    CNN feature aggregation [[195](#bib.bib195)] \chronoevent[markdepth=50pt,year=false,textwidth=1.2cm]37Attention
    based feature aggregation [[156](#bib.bib156)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]39Multi-network
    fusion [[155](#bib.bib155)] \chronoevent[markdepth=10pt,year=false,textwidth=1.25cm]42Co-weighting
    based CNN feature fusion [[199](#bib.bib199)] \chronoevent[markdepth=50pt,year=false,textwidth=1.2cm]40CNN
    + Hand-crafted fusion [[106](#bib.bib106)] \chronoevent[markdepth=-15pt, year=false]432019
    \chronoevent[markdepth=-25pt,year=false,textwidth=1.2cm]44GAN [[166](#bib.bib166)]
    \chronoevent[markdepth=-38pt,year=false,textwidth=1.3cm]46Pairwise Correlation
    Discrete Hashing (PCDH) [[200](#bib.bib200)] \chronoevent[markdepth=-25pt,year=false,textwidth=1cm]49Double
    bottleneck hashing [[87](#bib.bib87)] \chronoevent[markdepth=10pt,year=false,textwidth=1.2cm]46Co-occurrence
    + feature map fusion [[88](#bib.bib88)] \stopchronology'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: A chronological view of deep learning based image retrieval methods
    depicting the different type of descriptors. The binary and real-valued feature
    vector based models are presented in Red and Blue colors, respectively. The feature
    aggregation based models are presented in Cyan color.'
  prefs: []
  type: TYPE_NORMAL
- en: V-F Recurrent Neural Networks for Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2018, Lu et al. have utilized the recurrent neural network (RNN) concept
    to perform a hierarchical recurrent neural hashing (HRNH) to produce the effective
    hash codes for image retrieval [[140](#bib.bib140)]. In 2017, Shen et al. have
    used the region-based convolutional networks with long short-term memory (LSTM)
    modules for textual-visual cross retrieval [[139](#bib.bib139)]. Bai et al. (2019)
    have also employed the LSTM based recurrent deep network in the triplet hashing
    framework to naturally inherit the useful information for image retrieval [[83](#bib.bib83)].
  prefs: []
  type: TYPE_NORMAL
- en: V-G Reinforcement Learning Networks based Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2018, Yuan et al. have exploited the reinforcement learning for image retrieval
    [[77](#bib.bib77)]. They have used a relaxation free method through policy gradient
    to generate the hash codes for image retrieval. The similarity preservation via
    the generated binary codes is used as the reward function. In 2020, Yang et al.
    [[167](#bib.bib167)] have utilized the deep reinforcement learning to perform
    the de-redundancy in hash bits to get rid of redundant and/or harmful bits, which
    reduces the ambiguity in the similarity computation for image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: V-H Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The summary of the different network driven deep learning based image retrieval
    approaches is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolutional neural network features are exploited for the hash code and
    descriptor learning by employing the various constraints like classification error,
    quantization error, independent bits, etc.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to make the features more representative of the image, the autoencoder
    networks are used which enforces the learning based on the reconstruction loss.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discriminative power of descriptive hash code is enhanced by exploiting
    the siamese and triplet networks. Different constraints are used on the hash code
    to make it discriminative and compact.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generative adversarial network based approaches have been highly utilized
    to improve the discriminative ability and robustness of the learnt features by
    encoder network guided through the discriminator network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The automatic important feature selection is performed using attention module
    to control the redundancy in the feature space. The recurrent neural network and
    reinforcement learning network have been also shown very effective for the image
    retrieval.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: VI Type of Descriptors for Image Retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section covers the binary hash codes for efficient image retrieval, real-valued
    descriptors and feature aggregation for discriminative image retrieval as depicted
    in Fig. [8](#S5.F8 "Figure 8 ‣ V-E Attention Networks for Image Retrieval ‣ V
    Network Types For Image Retrieval ‣ A Decade Survey of Content Based Image Retrieval
    using Deep Learning").
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Binary Descriptors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different types of networks are used to learn the binary description such as
    deep neural networks [[110](#bib.bib110)], convolutional neural networks [[51](#bib.bib51)],
    autoencoder networks [[127](#bib.bib127)], siamese networks [[144](#bib.bib144)],
    triplet networks [[123](#bib.bib123)], generative adversarial networks, [[166](#bib.bib166)],
    and variational networks [[86](#bib.bib86)]. In 2015, Liong et al. [[110](#bib.bib110)]
    have introduced a supervised deep hashing (SDH). The SDH method uses the quantization
    loss, balanced bits and independent bits constraints. Binary hash code is also
    learnt through a latent layer in a supervised manner in [[51](#bib.bib51)]. A
    binary autoencoder [[58](#bib.bib58)], [[127](#bib.bib127)] and a siamese network
    [[144](#bib.bib144)] are used to learn the binary features for efficient image
    retrieval. A binary deep neural network (BDNN) is proposed by converting a hidden
    layer output to binary code [[186](#bib.bib186)], [[194](#bib.bib194)]. The binary
    code is jointly learnt with feature aggregation in [[152](#bib.bib152)]. A masking
    technique over the convolutional features is used to generate the binary description
    for image retrieval [[195](#bib.bib195)]. A ranking optimization discrete hashing
    (RODH) approach is used in [[196](#bib.bib196)] by generating the discrete hash
    codes (+1 or -1) by employing the ranking information. A cauchy quantization loss
    is used in [[75](#bib.bib75)] to improve the discriminative power of binary descriptors.
    An iterative quantization approach is used to convert the features into binary
    codes to avoid the quantization loss [[76](#bib.bib76)]. Binary hash code is also
    used for clothing image retrieval [[181](#bib.bib181)]. The binary description
    is learnt through the supervised [[96](#bib.bib96)], [[97](#bib.bib97)], [[101](#bib.bib101)],
    unsupervised [[112](#bib.bib112)], [[113](#bib.bib113)], [[120](#bib.bib120)]
    and self-supervised [[178](#bib.bib178)] deep learning techniques. Among the generative
    approaches, a binary generative adversarial network (BGAN) is used to learn the
    binary code [[121](#bib.bib121)]. At the same time a regularized GAN is used by
    maximizing the entropy of binarized layer for image retrieval [[150](#bib.bib150)].
    The GAN is trained in unsupervised mode [[120](#bib.bib120)] to learn the binary
    codes for image retrieval. In 2020, the binary GAN [[166](#bib.bib166)] is used
    for image retrieval and compression jointly.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Real-Valued Descriptors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The binary hashing approaches have the obvious shortcomings. First, it is difficult
    to represent the fine-grained similarity using binary code. Second, the generation
    of similar binary codes is common even for different images. Thus, researchers
    have also used the real-valued features to represent the images for the retrieval.
    The siamese networks have been extensively used to learn the real-valued feature
    descriptor for image retrieval [[132](#bib.bib132)], [[136](#bib.bib136)], [[72](#bib.bib72)].
    In 2018, part-based CNN features are utilized to extract a non-binary hash code
    [[119](#bib.bib119)]. The real-valued descriptors generated using CNNs are used
    for medical image retrieval [[188](#bib.bib188)], [[197](#bib.bib197)] and cross-modal
    retrieval [[198](#bib.bib198)]. Chen et al. (2020) [[200](#bib.bib200)] have developed
    a pairwise correlation discrete hashing (PCDH) by exploiting the pairwise correlation
    of deep features for image retrieval. Shen et al. (2020) [[87](#bib.bib87)] have
    also used the real-valued descriptors with the help of double bottleneck hashing
    approach for image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Aggregation of Descriptors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several researchers have also tried to combine/fuse the feature at different
    stages of the network or multiple networks to generate the aggregation of descriptors
    for image retrieval [[69](#bib.bib69)], [[182](#bib.bib182)], [[185](#bib.bib185)].
    Different strategies have been excercised for aggregation of features, such as
    vector locally aggregated descriptors (VLAD) [[184](#bib.bib184)] on the features
    extracted from different layers; bag of local convolutional features [[187](#bib.bib187)];
    selective convolutional descriptor aggregation [[189](#bib.bib189)], [[195](#bib.bib195)];
    fusion of multi-layer features [[190](#bib.bib190)], [[151](#bib.bib151)]; part-based
    weighting aggregation [[119](#bib.bib119)]; joint training of feature aggregation
    and hashing [[191](#bib.bib191)]; learning of feature aggregation and hash function
    in a joint manner [[152](#bib.bib152)]; and co-weighting based CNN feature fusion
    [[199](#bib.bib199)]. The features from different CNNs are also integrated for
    image retrieval [[183](#bib.bib183)], [[138](#bib.bib138)], [[156](#bib.bib156)].
    One main sub-network and other attention-based sub-network are also fused at the
    last fully connected layer in [[155](#bib.bib155)]. The hand-designed features
    are fused with CNNs [[106](#bib.bib106)], [[192](#bib.bib192)]. Recently, Forcen
    et al. (2020) [[88](#bib.bib88)] have generated the image representation by combining
    a co-occurrence map with the feature map for image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: VI-D Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The followings are the summary of deep learning based approaches from the perspective
    of the type of feature descriptor:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to facilitate the large-scale image retrieval, the compact and binary
    hash codes are generated using different networks. Different methods try to improve
    the discriminative ability, lower redundancy among bits, generalization of the
    binary hash code, etc. in different supervision modes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The real-valued descriptors concentrate over the discriminative ability of the
    learnt features for image retrieval at the cost of increased computational complexity
    for feature matching. Such methods try to increase the robustness and reduce the
    dimensionality of the descriptors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature aggregation approaches try to utilize the complementary information
    between the features of different networks, the features of different sub-network,
    and the features of different layers of same network to improve the image retrieval
    performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: VII Retrieval Type
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Various retrieval types have been explored using deep learning approaches based
    on the nature of the problem and data as discussed in this section.
  prefs: []
  type: TYPE_NORMAL
- en: VII-A Cross-modal Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The cross-modal retrieval refers to the image retrieval involving more than
    one modality by measuring the similarity between heterogeneous data objects. Feng
    et al. (2014) have introduced a correspondence autoencoder (Corr-AE) network for
    cross-modal retrieval [[125](#bib.bib125)]. In 2016 [[201](#bib.bib201)], a deep
    visual-semantic hashing (DVSH) network is developed for sentence and image based
    cross-modal retrieval by jointly learning the embeddings for images and sentences.
    Textual-visual deep binaries (TVDB) model represents the long descriptive sentences
    along with its corresponding informative images [[139](#bib.bib139)]. The CNN
    visual features have been also exploited for cross-modal retrieval, such as CNN
    off-the-shelf features for labelled annotation [[134](#bib.bib134)], CNN features
    with bi-directional hinge loss [[202](#bib.bib202)], and pairwise constraints
    based deep hashing network [[142](#bib.bib142)]. The adversarial neural network
    is also employed for cross-modal retrieval, such as adversarial cross-modal retrieval
    (ACMR) [[143](#bib.bib143)], self-supervised adversarial hashing (SSAH) [[146](#bib.bib146)],
    attention-aware deep adversarial hashing (ADAH) [[145](#bib.bib145)], adversary
    guided asymmetric hashing (AGAH) [[160](#bib.bib160)], deep multi-level semantic
    hashing (DMSH) [[198](#bib.bib198)], and teacher-student learning [[203](#bib.bib203)].
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Sketch Based Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sketch based image retrieval (SBIR) is a special case of cross-modal retrieval
    where the query image is in the sketch domain the retrieval has to be performed
    in the image domain [[204](#bib.bib204)]. In 2017, a fine-grained SBIR (FG-SBIR)
    [[137](#bib.bib137)] is explored with the help of attention module and higher-order
    learnable energy function loss. Liu et al. (2017) [[205](#bib.bib205)] have introduced
    a semi-heterogeneous deep sketch hashing (DSH) model for SBIR by utilizing the
    representation of free-hand sketches. The sketches and natural photos are mapped
    in multiple layers in a deep CNN framework in [[151](#bib.bib151)] for SBIR. A
    zero-shot SBIR (ZS-SBIR) is proposed for retrieval of photos from unseen categories
    [[153](#bib.bib153)]. Wang et al. (2019) [[158](#bib.bib158)] have proposed a
    CNN based SBIR re-ranking approach to refine the retrieval results. The generative
    adversarial networks have been also exploited extensively for SBIR, such as generative
    domain-migration hashing (GDH) using cycle consistency loss [[148](#bib.bib148)],
    class sketch conditioned generative model [[159](#bib.bib159)], semantically aligned
    paired cycle-consistent generative model [[206](#bib.bib206)], and stacked adversarial
    network [[163](#bib.bib163)].
  prefs: []
  type: TYPE_NORMAL
- en: VII-C Multi-label Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-label retrieval involves multiple categorical labels while generating
    the image representations for image retrieval. Several deep learning approaches
    have been investigated for multi-label image retrieval using different strategies,
    such as multilevel similarity information [[207](#bib.bib207)], multilevel semantic
    similarity preserving hashing [[208](#bib.bib208)], multi-label annotations [[146](#bib.bib146)],
    category-aware object based hashing [[209](#bib.bib209)], [[210](#bib.bib210)],
    and fine-grained features for multilevel similarity hashing [[211](#bib.bib211)].
    Readers may refer to the survey of multi-label image retrieval [[43](#bib.bib43)]
    published in 2020 for wider aspects and developments.
  prefs: []
  type: TYPE_NORMAL
- en: VII-D Instance Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2015, Razavian et al. have developed a baseline for deep CNN based visual
    instance retrieval [[212](#bib.bib212)]. An instance-aware image representations
    for multi-label image data by modeling the features of one category in a group
    is proposed in [[209](#bib.bib209)]. Other approaches for image instance retrieval
    includes bags of local convolutional features [[187](#bib.bib187)], learning global
    representations [[65](#bib.bib65)], and group invariant deep representation [[213](#bib.bib213)].
    In 2020, Chen et al. have proposed a deep multiple-instance ranking based hashing
    (DMIRH) model for multi-label image retrieval by employing the category-aware
    bag of feature [[210](#bib.bib210)]. More details about image instance retrieval
    can be found in the survey compiled in [[42](#bib.bib42)].
  prefs: []
  type: TYPE_NORMAL
- en: VII-E Object Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The object retrieval aims to perform the retrieval based on the features derived
    from the specific objects in the image. In 2014, Sun et al. have extracted the
    CNN features from the region of interest detected through object detection technique
    for object based retrieval [[126](#bib.bib126)]. Several deep learning models
    have been investigated for object retrieval, such as integral image driven max-pooling
    on CNN activations [[214](#bib.bib214)], pooling of the relevant features based
    on the region proposal network [[65](#bib.bib65)], replicator equation based simultaneous
    selection and weighting of the primitive deep CNN features [[215](#bib.bib215)],
    co-weighting based aggregation of the semantic CNN features [[199](#bib.bib199)],
    and consideration of spatial and channel contribution to improve the region detection
    [[216](#bib.bib216)]. Gao et al. (2020) [[165](#bib.bib165)] have performed the
    3D object retrieval with the help of a multi-view discrimination and pairwise
    CNN (MDPCNN) network.
  prefs: []
  type: TYPE_NORMAL
- en: VII-F Semantic Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2016, Yao et al. [[131](#bib.bib131)] have introduced a deep semantic preserving
    and ranking-based hashing (DSRH) method by exploiting the hash and classification
    losses. Similar losses are also used in [[217](#bib.bib217)]. A deep visual-semantic
    quantization (DVSQ) [[218](#bib.bib218)] is used by jointly learning the visual-semantic
    embeddings and quantizers. An adaptive Gaussian filter based aggregation of CNN
    features is used in [[199](#bib.bib199)] to exploit the semantic information.
    Semantic hashing has been also extensively performed for sketch based image retrieval
    [[137](#bib.bib137)], [[153](#bib.bib153)], [[206](#bib.bib206)], [[158](#bib.bib158)],
    cross-modal retrieval [[198](#bib.bib198)], [[201](#bib.bib201)], [[139](#bib.bib139)].
    Other notable deep learning based works that model the semantic information include
    Multi-label retrieval [[207](#bib.bib207)], unsupervised image retrieval [[219](#bib.bib219)],
    supervised image retrieval [[102](#bib.bib102)], and semi-supervised image retrieval
    [[141](#bib.bib141)]. Semantic similarity in Hamming space based deep position-aware
    hashing (DPAH) [[89](#bib.bib89)] and semantic affinity deep semantic reconstruction
    hashing (DSRH) [[162](#bib.bib162)] are the recent methods for semantic retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: VII-G Fine-Grained Image Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to increase the discriminative ability of the deep learnt descriptors,
    many researchers have utilized the fine-grained constraints in deep networks.
    Different works have incorporated the fine-grained property using different approaches,
    such as capturing the inter-class and intra-class image similarities using a siamese
    network [[62](#bib.bib62)], attention modules based incorporation of the spatial-semantic
    information [[137](#bib.bib137)], using selective CNN features [[189](#bib.bib189)],
    fine-grained ranking using the weighted Hamming distance [[220](#bib.bib220)],
    using the multilevel semantic similarity between multi-label image pairs [[211](#bib.bib211)],
    and using a piecewise cross entropy loss [[221](#bib.bib221)].
  prefs: []
  type: TYPE_NORMAL
- en: VII-H Asymmetric Quantization based Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2017, Wu et al. have performed the online asymmetric similarity learning
    to preserve the similarity between heterogeneous data [[202](#bib.bib202)]. An
    asymmetric deep supervised hashing (ADSH) is used by learning the deep hash function
    only for query images, while the hash codes for gallery images are directly learned
    [[104](#bib.bib104)]. In 2019, Yang et al. have investigated an asymmetric deep
    semantic quantization (ADSQ) using three stream networks to model the heterogeneous
    data [[222](#bib.bib222)]. A similarity preserving deep asymmetric quantization
    (SPDAQ) is proposed by exploiting the image subset and the label information of
    all the database items [[223](#bib.bib223)]. An adversary guided asymmetric hashing
    (AGAH) is introduced in [[160](#bib.bib160)] with the help of adversarial learning
    guided multi-label attention module for cross-modal image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: VII-I Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the progress in image retrieval using deep learning methods for different
    retrieval types, following are the outlines drawn from this section:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cross-modal retrieval approaches learn the joint features for multiple modality
    using different networks. The recent methods utilize of the adversarial network
    for cross-modal retrieval. The similar observation and trend has been also witnessed
    for sketch based image retrieval.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The multi-label and instance retrieval approaches are generally useful where
    more than one type of visual scenarios is present in the image. The deep learning
    based approaches are able to handle such retrieval by facilitating the feature
    learning through different type of networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The region proposal network based feature selection has been employed by the
    existing deep learning methods for the object retrieval.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The semantic information of the image has been used by different networks through
    abstract features to enhance the semantic image retrieval. The reconstruction
    based network is more suitable for semantic preserving hashing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different feature selection and aggregation based networks have been utilized
    for fine-grained image retrieval.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The asymmetric hashing has also shown the suitability of deep learning models
    by processing the query and gallery images with different networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: VIII Miscellaneous
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section covers the deep learning models for retrieval in terms of the different
    losses, applications and other aspects.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-A Progress in Retrieval Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A siamese based loss function is used in [[132](#bib.bib132)] by Kumar et al.
    (2016) for minimizing the global loss leading to discriminative feature learning.
    Zhou et al. (2017) have used the triplet quantization loss for deep hashing, which
    is based on the similarity between the anchor-positive pairs and anchor-negative
    pairs [[135](#bib.bib135)]. A listwise loss has been employed by Revaud et al.
    in 2019 [[224](#bib.bib224)] to directly optimize the global mean average precision
    in end-to-end deep learning. In 2020, a piecewise cross entropy loss function
    is used in [[221](#bib.bib221)] for fine-grained image retrieval. Several innovative
    losses have been used by the different feature learning approaches such as a lifted
    structured loss [[66](#bib.bib66)] and ranking loss [[147](#bib.bib147)].
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Mean Average Precision (mAP) with 5000 retrieved images (mAP@5000)
    in % for different deep learning based image retrieval approaches over NUS-WIDE,
    MS COCO and CIFAR-10 datasets. Note that $2^{nd}$ column list the reference from
    where the results of corresponding approach are considered. Followings are the
    used acronyms for different network types in the results: DNN - Deep Neural Network,
    CNN - Convolutional Neural Network, SN - Siamese Network, TN - Triplet Network,
    GAN - Generative Adversarial Network, DQN - Deep Q Network, PTN - Parametric Transformation
    Network, DVN - Deep Variational Networks, and AE - Autoencoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | NUS-WIDE | MS COCO |'
  prefs: []
  type: TYPE_TB
- en: '| Method Name | Net. Type | Result Source | 16 Bits | 32 Bits | 64 Bits | 16
    Bits | 32 Bits | 64 Bits |'
  prefs: []
  type: TYPE_TB
- en: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[71](#bib.bib71)] | 57.0 | 58.3 | 60.0
    | 56.4 | 57.4 | 56.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SDH’15 [[96](#bib.bib96)] | PTN | [[71](#bib.bib71)] | 47.6 | 55.5 | 58.1
    | 55.5 | 56.4 | 58.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DNNH’15 [[63](#bib.bib63)] | DNN | [[71](#bib.bib71)] | 59.8 | 61.6 | 63.9
    | 59.3 | 60.3 | 61.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DHN’16 [[67](#bib.bib67)] | CNN | [[71](#bib.bib71)] | 63.7 | 66.4 | 67.1
    | 67.7 | 70.1 | 69.4 |'
  prefs: []
  type: TYPE_TB
- en: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[71](#bib.bib71)] | 66.2 | 69.9 |
    71.6 | 68.7 | 71.8 | 73.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepBit’16 [[112](#bib.bib112)] | CNN | [[87](#bib.bib87)] | 39.2 | 40.3
    | 42.9 | 40.7 | 41.9 | 43.0 |'
  prefs: []
  type: TYPE_TB
- en: '| BGAN’18 [[121](#bib.bib121)] | GAN | [[87](#bib.bib87)] | 68.4 | 71.4 | 73.0
    | 64.5 | 68.2 | 70.7 |'
  prefs: []
  type: TYPE_TB
- en: '| GreedyHash’18 [[76](#bib.bib76)] | CNN | [[87](#bib.bib87)] | 63.3 | 69.1
    | 73.1 | 58.2 | 66.8 | 71.0 |'
  prefs: []
  type: TYPE_TB
- en: '| BinGAN’18 [[150](#bib.bib150)] | GAN | [[87](#bib.bib87)] | 65.4 | 70.9 |
    71.3 | 65.1 | 67.3 | 69.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DVB’19 [[86](#bib.bib86)] | DVN | [[87](#bib.bib87)] | 60.4 | 63.2 | 66.5
    | 57.0 | 62.9 | 62.3 |'
  prefs: []
  type: TYPE_TB
- en: '| DistillHash’19 [[82](#bib.bib82)] | SN | [[87](#bib.bib87)] | 66.7 | 67.5
    | 67.7 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| TBH’20 [[87](#bib.bib87)] | AE | [[87](#bib.bib87)] | 71.7 | 72.5 | 73.5
    | 70.6 | 73.5 | 72.2 |'
  prefs: []
  type: TYPE_TB
- en: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[84](#bib.bib84)] | 57.0 | 58.3 | 60.0
    | 56.4 | 57.4 | 56.7 |'
  prefs: []
  type: TYPE_TB
- en: '| DNNH’15 [[63](#bib.bib63)] | DNN | [[84](#bib.bib84)] | 59.8 | 61.6 | 63.9
    | 59.3 | 60.3 | 61.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DHN’16 [[67](#bib.bib67)] | CNN | [[84](#bib.bib84)] | 63.7 | 66.4 | 67.1
    | 67.7 | 70.1 | 69.4 |'
  prefs: []
  type: TYPE_TB
- en: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[84](#bib.bib84)] | 66.3 | 69.9 |
    71.6 | 68.7 | 71.8 | 73.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DHA’19 [[84](#bib.bib84)] | CNN | [[84](#bib.bib84)] | 66.9 | 70.6 | 72.7
    | 70.8 | 73.1 | 75.2 |'
  prefs: []
  type: TYPE_TB
- en: '| HashGAN’18 [[120](#bib.bib120)] | GAN | [[120](#bib.bib120)] | 71.5 | 73.7
    | 74.8 | 69.7 | 72.5 | 74.4 |'
  prefs: []
  type: TYPE_TB
- en: '| UH-BDNN’16 [[186](#bib.bib186)] | DNN | [[123](#bib.bib123)] | 59.2 | 59.0
    | 61.0 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| UTH’17 [[117](#bib.bib117)] | TN | [[123](#bib.bib123)] | 54.3 | 53.7 | 54.7
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| UDTH’19 [[123](#bib.bib123)] | TN | [[123](#bib.bib123)] | 64.4 | 67.7 |
    69.6 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SSDH’17 [[102](#bib.bib102)] | CNN | [[89](#bib.bib89)] | - | - | - | 69.7
    | 72.5 | 74.4 |'
  prefs: []
  type: TYPE_TB
- en: '| DPAH’20 [[89](#bib.bib89)] | PTN | [[89](#bib.bib89)] | - | - | - | 73.3
    | 76.8 | 78.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DRDH’20 [[167](#bib.bib167)] | DQN | [[167](#bib.bib167)] | 80.5 | 81.7 |
    81.8 | 71.5 | 74.8 | 76.1 |'
  prefs: []
  type: TYPE_TB
- en: '| DVSQ’17 [[218](#bib.bib218)] | CNN | [[223](#bib.bib223)] | 79.0 | 79.7 |
    - | 71.2 | 72.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DTQ’18 [[179](#bib.bib179)] | TN | [[223](#bib.bib223)] | 79.8 | 80.1 | -
    | 76.0 | 76.7 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SPDAQ’19 [[223](#bib.bib223)] | CNN | [[223](#bib.bib223)] | 84.2 | 85.1
    | - | 84.4 | 84.7 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DSQ’19 [[81](#bib.bib81)] | CNN | [[81](#bib.bib81)] | 77.9 | 79.0 | 79.9
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | CIFAR-10 Dataset |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | - | 12 Bits | 24 Bits | 32 Bits | 48 Bits | - |'
  prefs: []
  type: TYPE_TB
- en: '| SDH’15 [[96](#bib.bib96)] | PTN | [[225](#bib.bib225)] | - | 45.4 | 63.3
    | 65.1 | 66.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DSH’16 [[97](#bib.bib97)] | CNN | [[225](#bib.bib225)] | - | 64.4 | 74.2
    | 77.0 | 79.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DHN’16 [[67](#bib.bib67)] | CNN | [[225](#bib.bib225)] | - | 68.1 | 72.1
    | 72.3 | 73.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DPSH’16 [[98](#bib.bib98)] | SN | [[225](#bib.bib225)] | - | 68.2 | 72.0
    | 73.4 | 74.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DQN’16 [[68](#bib.bib68)] | CNN | [[225](#bib.bib225)] | - | 55.4 | 55.8
    | 56.4 | 58.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DSDH’17 [[101](#bib.bib101)] | CNN | [[225](#bib.bib225)] | - | 74.0 | 78.6
    | 80.1 | 82.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ADSH’18 [[104](#bib.bib104)] | CNN | [[225](#bib.bib225)] | - | 89.0 | 92.8
    | 93.1 | 93.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DIHN2+ADSH’19 [[80](#bib.bib80)] | CNN | [[225](#bib.bib225)] | - | 89.8
    | 92.9 | 92.9 | 93.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DTH’20 [[225](#bib.bib225)] | CNN | [[225](#bib.bib225)] | - | 92.1 | 93.3
    | 93.7 | 94.9 | - |'
  prefs: []
  type: TYPE_TB
- en: VIII-B Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The deep learning based approaches have been utilized for image retrieval pertaining
    to different applications such as cloth retrieval [[181](#bib.bib181)], biomedical
    image retrieval [[197](#bib.bib197)], face retrieval [[226](#bib.bib226)], [[227](#bib.bib227)],
    remote sensing image retrieval [[124](#bib.bib124)], landmark retrieval [[228](#bib.bib228)],
    social image retrieval [[229](#bib.bib229)], and video retrieval [[178](#bib.bib178)].
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Mean Average Precision (mAP) with 1000 retrieved images (mAP@1000)
    in % for different deep learning based image retrieval methods over ImageNet,
    CIFAR-10 and MNIST datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Network Type | Result Source | 16 Bits | 32 Bits | 48 Bits | 64
    Bits |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | ImageNet Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[71](#bib.bib71)] | 28.1 | 45.0 | 52.5
    | 55.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SDH’15 [[96](#bib.bib96)] | PTN | [[71](#bib.bib71)] | 29.9 | 45.5 | 55.5
    | 58.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DNNH’15 [[63](#bib.bib63)] | DNN | [[71](#bib.bib71)] | 29.0 | 46.1 | 53.0
    | 56.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DHN’16 [[67](#bib.bib67)] | CNN | [[71](#bib.bib71)] | 31.1 | 47.2 | 54.2
    | 57.3 |'
  prefs: []
  type: TYPE_TB
- en: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[71](#bib.bib71)] | 50.6 | 63.1 |
    66.3 | 68.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SSDH’17 [[102](#bib.bib102)] | CNN | [[89](#bib.bib89)] | 63.4 | 69.2 | 70.1
    | 70.7 |'
  prefs: []
  type: TYPE_TB
- en: '| DSQ’19 [[81](#bib.bib81)] | CNN | [[81](#bib.bib81)] | 57.8 | 65.4 | 68.0
    | 69.4 |'
  prefs: []
  type: TYPE_TB
- en: '| DPAH’20 [[89](#bib.bib89)] | PTN | [[89](#bib.bib89)] | 65.2 | 70.0 | 71.5
    | 71.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | CIFAR-10 Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| BGAN’18 [[121](#bib.bib121)] | GAN | [[87](#bib.bib87)] | 52.5 | 53.1 | -
    | 56.2 |'
  prefs: []
  type: TYPE_TB
- en: '| GreedyHash’18 [[76](#bib.bib76)] | CNN | [[87](#bib.bib87)] | 44.8 | 47.3
    | - | 50.1 |'
  prefs: []
  type: TYPE_TB
- en: '| BinGAN’18 [[150](#bib.bib150)] | GAN | [[87](#bib.bib87)] | 47.6 | 51.2 |
    - | 52.0 |'
  prefs: []
  type: TYPE_TB
- en: '| HashGAN’18 [[120](#bib.bib120)] | GAN | [[87](#bib.bib87)] | 44.7 | 46.3
    | - | 48.1 |'
  prefs: []
  type: TYPE_TB
- en: '| DVB’19 [[86](#bib.bib86)] | DVN | [[87](#bib.bib87)] | 40.3 | 42.2 | - |
    44.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DistillHash’19 [[82](#bib.bib82)] | SN | [[87](#bib.bib87)] | 28.4 | 28.5
    | - | 28.8 |'
  prefs: []
  type: TYPE_TB
- en: '| TBH’20 [[87](#bib.bib87)] | AE | [[87](#bib.bib87)] | 53.2 | 57.3 | - | 57.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| SDH’15 [[110](#bib.bib110)] | PTN | [[110](#bib.bib110)] | 18.8 | 20.8 |
    - | 22.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DAR’16 [[111](#bib.bib111)] | TN | [[111](#bib.bib111)] | 16.8 | 17.0 | -
    | 17.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DH’15 [[110](#bib.bib110)] | DNN | [[117](#bib.bib117)] | 16.2 | 16.6 | -
    | 17.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepBit’16 [[112](#bib.bib112)] | CNN | [[117](#bib.bib117)] | 19.4 | 24.9
    | - | 27.7 |'
  prefs: []
  type: TYPE_TB
- en: '| UTH’17 [[117](#bib.bib117)] | TN | [[117](#bib.bib117)] | 28.7 | 30.7 | -
    | 32.4 |'
  prefs: []
  type: TYPE_TB
- en: '| DBD-MQ’17 [[114](#bib.bib114)] | CNN | [[114](#bib.bib114)] | 21.5 | 26.5
    | - | 31.9 |'
  prefs: []
  type: TYPE_TB
- en: '| UCBD’18 [[113](#bib.bib113)] | CNN | [[113](#bib.bib113)] | 26.4 | 27.9 |
    - | 34.1 |'
  prefs: []
  type: TYPE_TB
- en: '| UH-BDNN’16 [[186](#bib.bib186)] | DNN | [[123](#bib.bib123)] | 30.1 | 30.9
    | - | 31.2 |'
  prefs: []
  type: TYPE_TB
- en: '| UDTH’19 [[123](#bib.bib123)] | TN | [[123](#bib.bib123)] | 46.1 | 50.4 |
    - | 54.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | MNIST Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| SDH’15 [[110](#bib.bib110)] | PTN | [[110](#bib.bib110)] | 46.8 | 51.0 |
    - | 52.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DH’15 [[110](#bib.bib110)] | DNN | [[117](#bib.bib117)] | 43.1 | 45.0 | -
    | 46.7 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepBit’16 [[112](#bib.bib112)] | CNN | [[117](#bib.bib117)] | 28.2 | 32.0
    | - | 44.5 |'
  prefs: []
  type: TYPE_TB
- en: '| UTH’17 [[117](#bib.bib117)] | TN | [[117](#bib.bib117)] | 43.2 | 46.6 | -
    | 49.9 |'
  prefs: []
  type: TYPE_TB
- en: VIII-C Others
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The hashing difficulty is also increased by generating the harder samples in
    a self-paced manner [[161](#bib.bib161)] to make the network training as reasoning
    oriented. In the initial work, the pre-trained CNN features have also very promising
    retrieval performance [[61](#bib.bib61)]. Recently, the transfer learning has
    been also utilized in [[225](#bib.bib225)] for deep transfer hashing.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-D Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Researchers have come up with various loss functions to facilitate the discriminative
    learning of features by the networks for image retrieval. The losses constraint
    and guide the training of the deep learning models. The image retrieval has shown
    a great utilization with its application to solve the real-life problems. Researchers
    have also tried to understand what works and what not for deep learning based
    image retrieval. The transfer learning has been also utilized for retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: mAP@54000 and mAP@All in % for state-of-the-art and recent image
    retrieval methods over the CIFAR-10 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | CIFAR-10 Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| Method Name | Network Type | Result Source | 16 Bits | 24 Bits | 32 Bits
    | 48 Bits | 64 Bits |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | mAP@54000 |'
  prefs: []
  type: TYPE_TB
- en: '| CNNH’14 [[95](#bib.bib95)] | CNN | [[84](#bib.bib84)] | 47.6 | - | 47.2 |
    48.9 | 50.1 |'
  prefs: []
  type: TYPE_TB
- en: '| DNNH’15 [[63](#bib.bib63)] | TN | [[84](#bib.bib84)] | 55.9 | - | 55.8 |
    58.1 | 58.3 |'
  prefs: []
  type: TYPE_TB
- en: '| SDH’15 [[96](#bib.bib96)] | PTN | [[84](#bib.bib84)] | 46.1 | - | 52.0 |
    55.3 | 56.8 |'
  prefs: []
  type: TYPE_TB
- en: '| DHN’16 [[67](#bib.bib67)] | CNN | [[84](#bib.bib84)] | 56.8 |  | 60.3 | 62.1
    | 63.5 |'
  prefs: []
  type: TYPE_TB
- en: '| HashNet’17 [[71](#bib.bib71)] | CNN | [[84](#bib.bib84)] | 64.3 | - | 66.7
    | 67.5 | 68.7 |'
  prefs: []
  type: TYPE_TB
- en: '| DHA’14 [[84](#bib.bib84)] | CNN | [[84](#bib.bib84)] | 65.2 |  | 68.1 | 69.0
    | 69.9 |'
  prefs: []
  type: TYPE_TB
- en: '| HashGAN’18 [[120](#bib.bib120)] | GAN | [[120](#bib.bib120)] | 66.8 | - |
    73.1 | 73.5 | 74.9 |'
  prefs: []
  type: TYPE_TB
- en: '| DTQ’18 [[179](#bib.bib179)] | TN | [[179](#bib.bib179)] | 78.9 | - | 79.2
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DRDH’20 [[167](#bib.bib167)] | DQN | [[167](#bib.bib167)] | 78.7 | - | 80.5
    | 80.6 | 80.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | mAP@All |'
  prefs: []
  type: TYPE_TB
- en: '| DQN’16 [[68](#bib.bib68)] | CNN | [[223](#bib.bib223)] | - | 55.8 | 56.4
    | 58.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DPSH’16 [[98](#bib.bib98)] | SN | [[223](#bib.bib223)] | - | 72.7 | 74.4
    | 75.7 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DSDH’17 [[101](#bib.bib101)] | CNN | [[223](#bib.bib223)] | - | 78.6 | 80.1
    | 82.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DTQ’18 [[179](#bib.bib179)] | DQN | [[223](#bib.bib223)] | - | 79.0 | 79.2
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DVSQ’17 [[218](#bib.bib218)] | CNN | [[223](#bib.bib223)] | - | 80.3 | 80.8
    | 81.1 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SPDAQ’19 [[223](#bib.bib223)] | CNN | [[223](#bib.bib223)] | - | 88.4 | 89.1
    | 89.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SSAH’19 [[161](#bib.bib161)] | GAN | [[161](#bib.bib161)] | - | 87.8 | -
    | 88.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DeepBit’19 [[112](#bib.bib112)] | CNN | [[122](#bib.bib122)] | 22.0 | - |
    24.1 | - | 29.0 |'
  prefs: []
  type: TYPE_TB
- en: '| BGAN’19 [[121](#bib.bib121)] | GAN | [[122](#bib.bib122)] | 49.7 | - | 47.0
    | - | 50.7 |'
  prefs: []
  type: TYPE_TB
- en: '| UADH’19 [[122](#bib.bib122)] | GAN | [[122](#bib.bib122)] | 67.7 | - | 68.9
    | - | 69.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DSAH’19 [[155](#bib.bib155)] | CNN | [[155](#bib.bib155)] | - | 84.1 | 84.5
    | 84.9 | - |'
  prefs: []
  type: TYPE_TB
- en: IX Performance Comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This survey also presents a performance analysis for the state-of-the-art deep
    learning based image retrieval approaches. The Mean Average Precision (mAP) reported
    for the different image retrieval approaches is summarized in Table [II](#S8.T2
    "TABLE II ‣ VIII-A Progress in Retrieval Loss ‣ VIII Miscellaneous ‣ A Decade
    Survey of Content Based Image Retrieval using Deep Learning"), [III](#S8.T3 "TABLE
    III ‣ VIII-B Applications ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based
    Image Retrieval using Deep Learning"), and [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary
    ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning"). The mAP@5000 (i.e., 5000 retrieved images) using various existing
    deep learning approaches is summarized in Table [II](#S8.T2 "TABLE II ‣ VIII-A
    Progress in Retrieval Loss ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based
    Image Retrieval using Deep Learning") over CIFAR-10, NUS-WIDE and MS COCO datasets.
    The results over CIFAR-10, ImageNet and MNIST datasets using different state-of-the-art
    deep learning based image retrieval methods are compiled in Table [III](#S8.T3
    "TABLE III ‣ VIII-B Applications ‣ VIII Miscellaneous ‣ A Decade Survey of Content
    Based Image Retrieval using Deep Learning") in terms of the mAP@1000\. The mAP@54000
    using few methods is reported in Table [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary
    ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning") over the CIFAR-10 dataset. The standard mAP is also depicted in
    Table [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary ‣ VIII Miscellaneous ‣ A Decade Survey
    of Content Based Image Retrieval using Deep Learning") by considering all the
    retrieved images for CIFAR-10 dataset using some of the available literature.
    Note that 2nd column in Table [II](#S8.T2 "TABLE II ‣ VIII-A Progress in Retrieval
    Loss ‣ VIII Miscellaneous ‣ A Decade Survey of Content Based Image Retrieval using
    Deep Learning"), [III](#S8.T3 "TABLE III ‣ VIII-B Applications ‣ VIII Miscellaneous
    ‣ A Decade Survey of Content Based Image Retrieval using Deep Learning"), and
    [IV](#S8.T4 "TABLE IV ‣ VIII-D Summary ‣ VIII Miscellaneous ‣ A Decade Survey
    of Content Based Image Retrieval using Deep Learning") list the source reference
    of the corresponding method reported results. Following are the observations out
    of these results by deep learning methods:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recently proposed Deep Transfer Hashing (DTH) by Zhai et al. (2020) [[225](#bib.bib225)]
    have shown outstanding performance over CIFAR-10 and NUS-WIDE datasets in terms
    of the mAP@5000\. Other promising methods include Deep Spatial Attention Hashing
    (DSAH) by Ge et al. (2019) [[155](#bib.bib155)], Similarity Preserving Deep Asymmetric
    Quantization (SPDAQ) by Chen et al. (2019) [[223](#bib.bib223)], Deep Position-Aware
    Hashing (DPAH) by Wang et al. (2020) [[89](#bib.bib89)] and Deep Reinforcement
    De-Redundancy Hashing (DRDH) by Yang et al. (2020) [[167](#bib.bib167)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Twin-Bottleneck Hashing (TBH) introduced by Shen et al. (2020) [[87](#bib.bib87)]
    is also observed as an appealing method using autoencoder having a double bottleneck
    over the CIFAR-10 dataset in terms of the mAP@1000\. However, the Deep Position-Aware
    Hashing (DPAH) investigated by Wang et al. (2020) [[89](#bib.bib89)] have outperformed
    the other approaches over ImageNet dataset. Supervised Deep Hashing (SDH) by Erin
    et al. (2015) [[110](#bib.bib110)] has depicted appealing performance over the
    MNIST dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deep reinforcement learning based image retrieval model, namely Deep Reinforcement
    De-Redundancy Hashing (DRDH) by Yang et al. (2020) [[167](#bib.bib167)], is one
    of recent breakthrough as supported by superlative mAP@54000 over the CIFAR-10
    dataset. The Deep Triplet Quantization [[179](#bib.bib179)] is also one of the
    favourable model for feature learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Similarity Preserving Deep Asymmetric Quantization (SPDAQ) by Chen et al.
    (2019) [[223](#bib.bib223)] and Unsupervised ADversarial Hashing (UADH) by Deng
    et al. (2019) [[122](#bib.bib122)] methods have been also identified as very encouraging
    based on the mAP by considering all the retrieved images over the CIFAR-10 dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X Conclusion and Future Directives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: X-A Conclusion and Trend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper presents a comprehensive survey of deep learning methods for content
    based image retrieval. As most of the deep learning based developments are recent,
    this survey majorly focuses over the image retrieval methods using deep learning
    in a decade from 2011 to 2020\. A detailed taxonomy is presented in terms of different
    supervision type, different networks used, different data type of descriptors,
    different retrieval type and other aspects. The detailed discussion under each
    section is also presented with the further categorization. A chronological summarization
    is presented to show the evolution of the deep learning models for image retrieval.
    Moreover, the chronological overview is also portrayed under each category to
    showcase the growth of image retrieval approaches. A summary of large-scale common
    datasets used for image retrieval is also compiled in this survey. A performance
    analysis of the state-of-the-art deep learning based image retrieval methods is
    also conducted in terms of the mean average precision for different no. of retrieved
    images.
  prefs: []
  type: TYPE_NORMAL
- en: The research trend in image retrieval suggests that the deep learning based
    models are driving the progress. The recently developed models such as generative
    adversarial networks, autoencoder networks and reinforcement learning networks
    have shown the superior performance for image retrieval. The discovery of better
    objective functions has been also the trend in order to constrain the learning
    of the hash code for discriminative, robust and efficient image retrieval. The
    semantic preserving class-specific feature learning using different networks and
    different quantization techniques is also the recent trend for image retrieval.
    Other trends include utilization of attention module, transfer learning, etc.
  prefs: []
  type: TYPE_NORMAL
- en: X-B Future Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The future work in image retrieval using deep learning can include exploration
    of improved deep learning models, more relevant objective functions, minimum loss
    based quantization techniques, semantic preserving feature learning, and attention
    focused feature learning. The future direction in the image retrieval might be
    driven from the basic goal of the expected solution. There are three important
    aspects of any retrieval system, which include the discriminative ability, robustness
    capability and fast image search. In order to achieve the discriminative ability,
    the features corresponding to the samples of different class should be as far
    as possible. Thus, different approaches such as triplet based objective function,
    consideration of class distribution, incorporation of distance between class centroids,
    etc. can be exploited. In order to maintain the robustness property, various data
    augmentation, layer manipulation, siamese loss based objective functions, feature
    normalization, incorporation of class distribution and majority voting in the
    feature representation, etc. can be explored. In order to perform the faster image
    search, the learnt feature or hash code should be as low dimensional and compact
    as possible. Thus, better strategy for feature quantization and maximizing the
    relevant information into feature space in a compact way can be seen as one of
    the future directions. The self-supervised learning has shown very promising performance
    for different down-stream tasks and has potential to learn the important features
    in compact form. Thus, in future, the self-supervised learning can boost the performance
    of image retrieval models significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work is supported by Global Innovation & Technology Alliance (GITA) on
    behalf of Department of Science and Technology (DST), Govt. of India through project
    no. GITA/DST/TWN/P-83/2019.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani,
    J. Hafner, D. Lee, D. Petkovic *et al.*, “Query by image and video content: The
    qbic system,” *Computer*, vol. 28, no. 9, pp. 23–32, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. W. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain, “Content-based
    image retrieval at the end of the early years,” *IEEE TPAMI*, vol. 22, no. 12,
    pp. 1349–1380, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] H. Müller, W. Müller, D. M. Squire, S. Marchand-Maillet, and T. Pun, “Performance
    evaluation in content-based image retrieval: overview and proposals,” *Pattern
    Recog. Letters*, vol. 22, no. 5, pp. 593–601, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] T. Deselaers, D. Keysers, and H. Ney, “Features for image retrieval: an
    experimental comparison,” *Information Retrieval*, vol. 11, no. 2, pp. 77–107,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    *IJCV*, vol. 60, no. 2, pp. 91–110, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale and
    rotation invariant texture classification with local binary patterns,” *IEEE TPAMI*,
    vol. 24, no. 7, pp. 971–987, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] S. R. Dubey, S. K. Singh, and R. K. Singh, “Rotation and illumination invariant
    interleaved intensity order-based local descriptor,” *IEEE TIP*, vol. 23, no. 12,
    pp. 5323–5333, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] I. J. Jacob, K. Srinivasagan, and K. Jayapriya, “Local oppugnant color
    texture pattern for image retrieval system,” *Pattern Recog. Letters*, vol. 42,
    pp. 72–78, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] D. Song and D. Tao, “Biologically inspired feature manifold for scene classification,”
    *IEEE TIP*, vol. 19, no. 1, pp. 174–184, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] H. Jegou, F. Perronnin, M. Douze, J. Sánchez, P. Perez, and C. Schmid,
    “Aggregating local image descriptors into compact codes,” *IEEE TPAMI*, vol. 34,
    no. 9, pp. 1704–1716, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Murala, R. Maheshwari, and R. Balasubramanian, “Local tetra patterns:
    a new feature descriptor for content-based image retrieval,” *IEEE TIP*, vol. 21,
    no. 5, pp. 2874–2886, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S. R. Dubey, S. K. Singh, and R. K. Singh, “Local wavelet pattern: a new
    feature descriptor for image retrieval in medical ct databases,” *IEEE TIP*, vol. 24,
    no. 12, pp. 5892–5903, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin, “Iterative quantization:
    A procrustean approach to learning binary codes for large-scale image retrieval,”
    *IEEE TPAMI*, vol. 35, no. 12, pp. 2916–2929, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] K.-C. Fan and T.-Y. Hung, “A novel local pattern descriptor—local vector
    pattern in high-order derivative space for face recognition,” *IEEE TIP*, vol. 23,
    no. 7, pp. 2877–2891, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. R. Dubey, S. K. Singh, and R. K. Singh, “Multichannel decoded local
    binary patterns for content-based image retrieval,” *IEEE TIP*, vol. 25, no. 9,
    pp. 4018–4032, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S. Chakraborty, S. K. Singh, and P. Chakraborty, “Local gradient hexa
    pattern: A descriptor for face recognition and retrieval,” *IEEE TCSVT*, vol. 28,
    no. 1, pp. 171–180, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] R. Datta, D. Joshi, J. Li, and J. Z. Wang, “Image retrieval: Ideas, influences,
    and trends of the new age,” *ACM Computing Surveys*, vol. 40, no. 2, pp. 1–60,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] T. Mei, Y. Rui, S. Li, and Q. Tian, “Multimedia search reranking: A literature
    survey,” *ACM Computing Surveys*, vol. 46, no. 3, pp. 1–38, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] W. Zhou, H. Li, and Q. Tian, “Recent advance in content-based image retrieval:
    A literature survey,” *arXiv preprint arXiv:1706.06064*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] A. Bellet, A. Habrard, and M. Sebban, “A survey on metric learning for
    feature vectors and structured data,” *arXiv preprint arXiv:1306.6709*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. Wang, T. Zhang, N. Sebe, H. T. Shen *et al.*, “A survey on learning
    to hash,” *IEEE TPAMI*, vol. 40, no. 4, pp. 769–790, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] S. C. Hoi, W. Liu, M. R. Lyu, and W.-Y. Ma, “Learning distance metrics
    with contextual constraints for image retrieval,” in *CVPR*, vol. 2, 2006, pp.
    2072–2078.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] H. Chang and D.-Y. Yeung, “Kernel-based distance metric learning for content-based
    image retrieval,” *Image and Vision Computing*, vol. 25, no. 5, pp. 695–703, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang, “Supervised hashing
    with kernels,” in *IEEE CVPR*.   IEEE, 2012, pp. 2074–2081.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] L. Yang, R. Jin, L. Mummert, R. Sukthankar, A. Goode, B. Zheng, S. C.
    Hoi, and M. Satyanarayanan, “A boosting framework for visuality-preserving distance
    metric learning and its application to medical image retrieval,” *IEEE TPAMI*,
    vol. 32, no. 1, pp. 30–44, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J.-E. Lee, R. Jin, and A. K. Jain, “Rank-based distance metric learning:
    An application to image retrieval,” in *CVPR*, 2008, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] S. C. Hoi, W. Liu, and S.-F. Chang, “Semi-supervised distance metric learning
    for collaborative image retrieval and clustering,” *ACM TOMM*, vol. 6, no. 3,
    pp. 1–26, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] M. Norouzi, D. J. Fleet, and R. R. Salakhutdinov, “Hamming distance metric
    learning,” in *NIPS*, 2012, pp. 1061–1069.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] D. Song, W. Liu, and D. A. Meyer, “Fast structural binary coding.” in
    *IJCAI*, 2016, pp. 2018–2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] D. Song, W. Liu, D. A. Meyer, D. Tao, and R. Ji, “Rank preserving hashing
    for rapid image search,” in *Data Compression Conference*.   IEEE, 2015, pp. 353–362.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] D. Song, W. Liu, R. Ji, D. A. Meyer, and J. R. Smith, “Top rank supervised
    binary coding for visual search,” in *IEEE ICCV*, 2015, pp. 1922–1930.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, “Cnn features
    off-the-shelf: an astounding baseline for recognition,” in *CVPR workshops*, 2014,
    pp. 806–813.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE TPAMI*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] C. H. Dagli, *Artificial neural networks for intelligent manufacturing*.   Springer
    Science & Business Media, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] F. Amato, A. López, E. M. Peña-Méndez, P. Vaňhara, A. Hampl, and J. Havel,
    “Artificial neural networks in medical diagnosis,” *J Appl Biomed*, vol. 11, pp.
    47–58, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NIPS*, 2012, pp. 1097–1105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] M. Sundermeyer, R. Schlüter, and H. Ney, “Lstm neural networks for language
    modeling,” in *Conf. of the Int. Speech Communication Association*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Gated feedback recurrent
    neural networks,” in *ICML*, 2015, pp. 2067–2075.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y. Zhang, and J. Li, “Deep
    learning for content-based image retrieval: A comprehensive study,” in *Proceedings
    of the 22nd ACMMM*, 2014, pp. 157–166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] L. Zheng, Y. Yang, and Q. Tian, “Sift meets cnn: A decade survey of instance
    retrieval,” *IEEE TPAMI*, vol. 40, no. 5, pp. 1224–1244, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] J. Rodrigues, M. Cristo, and J. G. Colonna, “Deep hashing for multi-label
    image retrieval: a survey,” *Artificial Intel. Review*, pp. 1–47, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] X. Luo, C. Chen, H. Zhong, H. Zhang, M. Deng, J. Huang, and X. Hua, “A
    survey on deep hashing methods,” *arXiv preprint arXiv:2003.03369*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] A. Krizhevsky, “Learning multiple layers of features from tiny images,”
    *Master’s thesis, University of Tront*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng, “Nus-wide:
    a real-world web image database from national university of singapore,” in *ACM
    Int. Conf. on Image and Video Retrieval*, 2009, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” *Proceedings of the IEEE*, vol. 86, no. 11,
    pp. 2278–2324, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Reading
    digits in natural images with unsupervised feature learning,” in *NIPS*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, “Sun database:
    Large-scale scene recognition from abbey to zoo,” in *CVPR*, 2010, pp. 3485–3492.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] A. Yu and K. Grauman, “Fine-grained visual comparisons with local learning,”
    in *CVPR*, 2014, pp. 192–199.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] K. Lin, H.-F. Yang, J.-H. Hsiao, and C.-S. Chen, “Deep learning of binary
    hash codes for fast image retrieval,” in *CVPR workshops*, 2015, pp. 27–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein *et al.*, “Imagenet large scale visual recognition
    challenge,” *IJCV*, vol. 115, no. 3, pp. 211–252, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *ECCV*, 2014,
    pp. 740–755.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] M. J. Huiskes, B. Thomee, and M. S. Lew, “New trends and ideas in visual
    concept detection: the mir flickr retrieval evaluation initiative,” in *ICMIR*,
    2010, pp. 527–536.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] H. Noh, A. Araujo, J. Sim, T. Weyand, and B. Han, “Large-scale image retrieval
    with attentive deep local features,” in *ICCV*, 2017, pp. 3456–3465.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] T. Weyand, A. Araujo, B. Cao, and J. Sim, “Google landmarks dataset v2-a
    large-scale benchmark for instance-level recognition and retrieval,” in *CVPR*,
    2020, pp. 2575–2584.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] X.-S. Hua, L. Yang, J. Wang, J. Wang, M. Ye, K. Wang, Y. Rui, and J. Li,
    “Clickage: Towards bridging semantic and intent gaps via mining click logs of
    search engines,” in *Proceedings of the 21st ACM international conference on Multimedia*,
    2013, pp. 243–252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] A. Krizhevsky and G. E. Hinton, “Using very deep autoencoders for content-based
    image retrieval.” in *ESANN*, vol. 1, 2011, p. 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Y. Kang, S. Kim, and S. Choi, “Deep learning to hash with multiple representations,”
    in *ICDM*, 2012, pp. 930–935.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] P. Wu, S. C. Hoi, H. Xia, P. Zhao, D. Wang, and C. Miao, “Online multimodal
    deep similarity learning with application to image retrieval,” in *ACMMM*, 2013,
    pp. 153–162.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky, “Neural codes
    for image retrieval,” in *ECCV*, 2014, pp. 584–599.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen,
    and Y. Wu, “Learning fine-grained image similarity with deep ranking,” in *CVPR*,
    2014, pp. 1386–1393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] H. Lai, Y. Pan, Y. Liu, and S. Yan, “Simultaneous feature learning and
    hash coding with deep neural networks,” in *CVPR*, 2015, pp. 3270–3278.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang, “Bit-scalable deep hashing
    with regularized similarity learning for image retrieval and person re-identification,”
    *IEEE TIP*, vol. 24, no. 12, pp. 4766–4779, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] A. Gordo, J. Almazán, J. Revaud, and D. Larlus, “Deep image retrieval:
    Learning global representations for image search,” in *ECCV*, 2016, pp. 241–257.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H. Oh Song, Y. Xiang, S. Jegelka, and S. Savarese, “Deep metric learning
    via lifted structured feature embedding,” in *CVPR*, 2016, pp. 4004–4012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] H. Zhu, M. Long, J. Wang, and Y. Cao, “Deep hashing network for efficient
    similarity retrieval,” in *AAAI*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Y. Cao, M. Long, J. Wang, H. Zhu, and Q. Wen, “Deep quantization network
    for efficient image retrieval,” in *AAAI*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] S. S. Husain and M. Bober, “Improving large-scale image retrieval through
    robust aggregation of local descriptors,” *IEEE TPAMI*, vol. 39, no. 9, pp. 1783–1796,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] G. Zhong, H. Xu, P. Yang, S. Wang, and J. Dong, “Deep hashing learning
    networks,” in *IJCNN*, 2016, pp. 2236–2243.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Z. Cao, M. Long, J. Wang, and P. S. Yu, “Hashnet: Deep learning to hash
    by continuation,” in *ICCV*, 2017, pp. 5608–5617.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Gordo, J. Almazan, J. Revaud, and D. Larlus, “End-to-end learning of
    deep visual representations for image retrieval,” *IJCV*, vol. 124, no. 2, pp.
    237–254, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] T. Hoang, T.-T. Do, D.-K. Le Tan, and N.-M. Cheung, “Selective deep convolutional
    features for image retrieval,” in *ACMMM*, 2017, pp. 1600–1608.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] A. Alzu’bi, A. Amira, and N. Ramzan, “Content-based image retrieval with
    compact deep convolutional features,” *Neurocomputing*, vol. 249, pp. 95–105,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Y. Cao, M. Long, B. Liu, and J. Wang, “Deep cauchy hashing for hamming
    space retrieval,” in *CVPR*, 2018, pp. 1229–1237.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S. Su, C. Zhang, K. Han, and Y. Tian, “Greedy hash: Towards fast optimization
    for accurate hash coding in cnn,” in *NIPS*, 2018, pp. 798–807.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] X. Yuan, L. Ren, J. Lu, and J. Zhou, “Relaxation-free deep hashing via
    policy gradient,” in *ECCV*, 2018, pp. 134–150.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Z. Chen, X. Yuan, J. Lu, Q. Tian, and J. Zhou, “Deep hashing via discrepancy
    minimization,” in *CVPR*, 2018, pp. 6838–6847.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] D. Wu, J. Liu, B. Li, and W. Wang, “Deep index-compatible hashing for
    fast image retrieval,” in *ICME*, 2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] D. Wu, Q. Dai, J. Liu, B. Li, and W. Wang, “Deep incremental hashing network
    for efficient image retrieval,” in *CVPR*, 2019, pp. 9069–9077.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] S. Eghbali and L. Tahvildari, “Deep spherical quantization for image search,”
    in *CVPR*, 2019, pp. 11 690–11 699.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] E. Yang, T. Liu, C. Deng, W. Liu, and D. Tao, “Distillhash: Unsupervised
    deep hashing by distilling data pairs,” in *CVPR*, 2019, pp. 2946–2955.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] J. Bai, B. Ni, M. Wang, Z. Li, S. Cheng, X. Yang, C. Hu, and W. Gao, “Deep
    progressive hashing for image retrieval,” *IEEE TMM*, vol. 21, no. 12, pp. 3178–3193,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] J. Xu, C. Guo, Q. Liu, J. Qin, Y. Wang, and L. Liu, “Dha: Supervised deep
    learning to hash with an adaptive loss function,” in *ICCV Workshops*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Y. Shen, J. Qin, J. Chen, L. Liu, F. Zhu, and Z. Shen, “Embarrassingly
    simple binary representation learning,” in *ICCV Workshops*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Y. Shen, L. Liu, and L. Shao, “Unsupervised binary representation learning
    with deep variational networks,” *IJCV*, vol. 127, no. 11-12, pp. 1614–1628, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Y. Shen, J. Qin, J. Chen, M. Yu, L. Liu, F. Zhu, F. Shen, and L. Shao,
    “Auto-encoding twin-bottleneck hashing,” in *CVPR*, 2020, pp. 2818–2827.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] J. I. Forcen, M. Pagola, E. Barrenechea, and H. Bustince, “Co-occurrence
    of deep convolutional features for image search,” *Image and Vision Computing*,
    p. 103909, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] R. Wang, R. Wang, S. Qiao, S. Shan, and X. Chen, “Deep position-aware
    hashing for semantic continuous image retrieval,” in *WACV*, 2020, pp. 2493–2502.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] M. J. Huiskes and M. S. Lew, “The mir flickr retrieval evaluation,” in
    *ACMMM Information Retrieval*, 2008, pp. 39–43.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, and A. Oliva, “Sun database:
    Exploring a large collection of scene categories,” *IJCV*, vol. 119, no. 1, pp.
    3–22, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Y. Pan, T. Yao, T. Mei, H. Li, C.-W. Ngo, and Y. Rui, “Click-through-based
    cross-view learning for image search,” in *Proceedings of the 37th international
    ACM SIGIR conference on Research & development in information retrieval*, 2014,
    pp. 717–726.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Yu, X. Yang, F. Gao, and D. Tao, “Deep multimodal distance metric learning
    using click constraints for image ranking,” *IEEE Transactions on Cybernetics*,
    vol. 47, no. 12, pp. 4014–4024, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Y. Li, Y. Pan, T. Yao, H. Chao, Y. Rui, and T. Mei, “Learning click-based
    deep structure-preserving embeddings with visual attention,” *ACM Transactions
    on Multimedia Computing, Communications, and Applications (TOMM)*, vol. 15, no. 3,
    pp. 1–19, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan, “Supervised hashing for image
    retrieval via image representation learning,” in *AAAI*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] F. Shen, C. Shen, W. Liu, and H. Tao Shen, “Supervised discrete hashing,”
    in *CVPR*, 2015, pp. 37–45.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] H. Liu, R. Wang, S. Shan, and X. Chen, “Deep supervised hashing for fast
    image retrieval,” in *CVPR*, 2016, pp. 2064–2072.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] W.-J. Li, S. Wang, and W.-C. Kang, “Feature learning based deep supervised
    hashing with pairwise labels,” in *IJCAI*, 2016, pp. 1711–1717.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] X. Wang, Y. Shi, and K. M. Kitani, “Deep supervised hashing with triplet
    labels,” in *ACCV*, 2016, pp. 70–84.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Z. Zhang, Y. Chen, and V. Saligrama, “Efficient training of very deep
    neural networks for supervised hashing,” in *CVPR*, 2016, pp. 1487–1495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Q. Li, Z. Sun, R. He, and T. Tan, “Deep supervised discrete hashing,”
    in *NIPS*, 2017, pp. 2482–2491.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] H.-F. Yang, K. Lin, and C.-S. Chen, “Supervised learning of semantics-preserving
    hash via deep convolutional neural networks,” *IEEE TPAMI*, vol. 40, no. 2, pp.
    437–451, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] J. Lu, V. E. Liong, and J. Zhou, “Deep hashing for scalable image search,”
    *IEEE TIP*, vol. 26, no. 5, pp. 2352–2367, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Q.-Y. Jiang and W.-J. Li, “Asymmetric deep supervised hashing,” in *AAAI*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] B. Klein and L. Wolf, “End-to-end supervised product quantization for
    image search and retrieval,” in *CVPR*, 2019, pp. 5041–5050.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] S. Kan, Y. Cen, Z. He, Z. Zhang, L. Zhang, and Y. Wang, “Supervised deep
    feature embedding with handcrafted feature,” *IEEE TIP*, vol. 28, no. 12, pp.
    5809–5823, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] W. W. Ng, J. Li, X. Tian, H. Wang, S. Kwong, and J. Wallace, “Multi-level
    supervised hashing with deep features for efficient image retrieval,” *Neurocomputing*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] C. Zhou, L.-M. Po, W. Y. Yuen, K. W. Cheung, X. Xu, K. W. Lau, Y. Zhao,
    M. Liu, and P. H. Wong, “Angular deep supervised hashing for image retrieval,”
    *IEEE Access*, vol. 7, pp. 127 521–127 532, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] J. Li, W. W. Ng, X. Tian, S. Kwong, and H. Wang, “Weighted multi-deep
    ranking supervised hashing for efficient image retrieval,” *Int. Journal of Machine
    Learning and Cybernetics*, pp. 1–15, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] V. Erin Liong, J. Lu, G. Wang, P. Moulin, and J. Zhou, “Deep hashing
    for compact binary codes learning,” in *CVPR*, 2015, pp. 2475–2483.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] C. Huang, C. Change Loy, and X. Tang, “Unsupervised learning of discriminative
    attributes and visual representations,” in *CVPR*, 2016, pp. 5175–5184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] K. Lin, J. Lu, C.-S. Chen, and J. Zhou, “Learning compact binary descriptors
    with unsupervised deep neural networks,” in *CVPR*, 2016, pp. 1183–1192.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] K. Lin, J. Lu, C.-S. Chen, J. Zhou, and M.-T. Sun, “Unsupervised deep
    learning of compact binary descriptors,” *IEEE TPAMI*, vol. 41, no. 6, pp. 1501–1514,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Y. Duan, J. Lu, Z. Wang, J. Feng, and J. Zhou, “Learning deep binary
    descriptor with multi-quantization,” in *CVPR*, 2017, pp. 1183–1192.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] F. Radenović, G. Tolias, and O. Chum, “Cnn image retrieval learns from
    bow: Unsupervised fine-tuning with hard examples,” in *ECCV*, 2016, pp. 3–20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] M. Paulin, J. Mairal, M. Douze, Z. Harchaoui, F. Perronnin, and C. Schmid,
    “Convolutional patch representations for image retrieval: an unsupervised approach,”
    *IJCV*, vol. 121, no. 1, pp. 149–168, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] S. Huang, Y. Xiong, Y. Zhang, and J. Wang, “Unsupervised triplet hashing
    for fast image retrieval,” in *Thematic Workshops of ACM Multimedia*, 2017, pp.
    84–92.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] F. Shen, Y. Xu, L. Liu, Y. Yang, Z. Huang, and H. T. Shen, “Unsupervised
    deep hashing with similarity-adaptive and discrete optimization,” *IEEE TPAMI*,
    vol. 40, no. 12, pp. 3034–3044, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] J. Xu, C. Shi, C. Qi, C. Wang, and B. Xiao, “Unsupervised part-based
    weighting aggregation of deep convolutional features for image retrieval,” in
    *AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] K. Ghasedi Dizaji, F. Zheng, N. Sadoughi, Y. Yang, C. Deng, and H. Huang,
    “Unsupervised deep generative adversarial hashing network,” in *CVPR*, 2018, pp.
    3664–3673.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] J. Song, T. He, L. Gao, X. Xu, A. Hanjalic, and H. T. Shen, “Binary generative
    adversarial networks for image retrieval,” in *AAAI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] C. Deng, E. Yang, T. Liu, J. Li, W. Liu, and D. Tao, “Unsupervised semantic-preserving
    adversarial hashing for image search,” *IEEE TIP*, vol. 28, no. 8, pp. 4032–4044,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Y. Gu, H. Zhang, Z. Zhang, and Q. Ye, “Unsupervised deep triplet hashing
    with pseudo triplets for scalable image retrieval,” *Multimedia Tools and Applications*,
    pp. 1–22, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Y. Liu, L. Ding, C. Chen, and Y. Liu, “Similarity-based unsupervised
    deep transfer learning for remote sensing image retrieval,” *IEEE TGRS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] F. Feng, X. Wang, and R. Li, “Cross-modal retrieval with correspondence
    autoencoder,” in *ACMMM*, 2014, pp. 7–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] S. Sun, W. Zhou, H. Li, and Q. Tian, “Search by detection: Object-level
    feature for image retrieval,” in *Int. conf. on Internet Multimedia Computing
    and Service*, 2014, pp. 46–49.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] M. A. Carreira-Perpinán and R. Raziperchikolaei, “Hashing with binary
    autoencoders,” in *CVPR*, 2015, pp. 557–566.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] S. Zagoruyko and N. Komodakis, “Learning to compare image patches via
    convolutional neural networks,” in *CVPR*, 2015, pp. 4353–4361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg, “Matchnet: Unifying
    feature and metric learning for patch-based matching,” in *CVPR*, 2015, pp. 3279–3286.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] B. Zhuang, G. Lin, C. Shen, and I. Reid, “Fast training of triplet-based
    deep binary embedding networks,” in *CVPR*, 2016, pp. 5955–5964.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] T. Yao, F. Long, T. Mei, and Y. Rui, “Deep semantic-preserving and ranking-based
    hashing for image retrieval,” in *IJCAI*, vol. 1, 2016, p. 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] V. Kumar BG, G. Carneiro, and I. Reid, “Learning local image descriptors
    with deep siamese and triplet convolutional networks by minimising global loss
    functions,” in *CVPR*, 2016, pp. 5385–5394.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Lin, O. Morere, J. Petta, V. Chandrasekhar, and A. Veillard, “Tiny
    descriptors for image retrieval with unsupervised triplet hashing,” in *DCC*,
    2016, pp. 397–406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Y. Wei, Y. Zhao, C. Lu, S. Wei, L. Liu, Z. Zhu, and S. Yan, “Cross-modal
    retrieval with cnn visual features: A new baseline,” *IEEE Transactions on Cybernetics*,
    vol. 47, no. 2, pp. 449–460, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Y. Zhou, S. Huang, Y. Zhang, and Y. Wang, “Deep hashing with triplet
    quantization loss,” in *VCIP*, 2017, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] E.-J. Ong, S. Husain, and M. Bober, “Siamese network of deep fisher-vector
    descriptors for image retrieval,” *arXiv preprint arXiv:1702.00338*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] J. Song, Q. Yu, Y.-Z. Song, T. Xiang, and T. M. Hospedales, “Deep spatial-semantic
    attention for fine-grained sketch-based image retrieval,” in *ICCV*, 2017, pp.
    5551–5560.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] F. Yang, J. Li, S. Wei, Q. Zheng, T. Liu, and Y. Zhao, “Two-stream attentive
    cnns for image retrieval,” in *ACMMM*, 2017, pp. 1513–1521.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Y. Shen, L. Liu, L. Shao, and J. Song, “Deep binaries: Encoding semantic-rich
    cues for efficient textual-visual cross retrieval,” in *ICCV*, 2017, pp. 4097–4106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] X. Lu, Y. Chen, and X. Li, “Hierarchical recurrent neural hashing for
    image retrieval with hierarchical convolutional features,” *IEEE TIP*, vol. 27,
    no. 1, pp. 106–120, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] J. Zhang and Y. Peng, “Ssdh: semi-supervised deep hashing for large scale
    image retrieval,” *IEEE TCSVT*, vol. 29, no. 1, pp. 212–225, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] E. Yang, C. Deng, W. Liu, X. Liu, D. Tao, and X. Gao, “Pairwise relationship
    guided deep hashing for cross-modal retrieval,” in *AAAI*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] B. Wang, Y. Yang, X. Xu, A. Hanjalic, and H. T. Shen, “Adversarial cross-modal
    retrieval,” in *ACMMM*, 2017, pp. 154–162.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] A. Jose, S. Yan, and I. Heisterklaus, “Binary hashing using siamese neural
    networks,” in *ICIP*, 2017, pp. 2916–2920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] X. Zhang, H. Lai, and J. Feng, “Attention-aware deep adversarial hashing
    for cross-modal retrieval,” in *ECCV*, 2018, pp. 591–606.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] C. Li, C. Deng, N. Li, W. Liu, X. Gao, and D. Tao, “Self-supervised adversarial
    hashing networks for cross-modal retrieval,” in *CVPR*, 2018, pp. 4242–4251.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] G. Wang, Q. Hu, J. Cheng, and Z. Hou, “Semi-supervised generative adversarial
    hashing for image retrieval,” in *ECCV*, 2018, pp. 469–485.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] J. Zhang, F. Shen, L. Liu, F. Zhu, M. Yu, L. Shao, H. Tao Shen, and L. Van Gool,
    “Generative domain-migration hashing for sketch-to-image retrieval,” in *ECCV*,
    2018, pp. 297–314.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Y. Cao, B. Liu, M. Long, and J. Wang, “Hashgan: Deep learning to hash
    with pair conditional wasserstein gan,” in *CVPR*, 2018, pp. 1287–1296.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] M. Zieba, P. Semberecki, T. El-Gaaly, and T. Trzcinski, “Bingan: Learning
    compact binary descriptors with a regularized gan,” in *NIPS*, 2018, pp. 3608–3618.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] D. Yu, Y. Liu, Y. Pang, Z. Li, and H. Li, “A multi-layer deep fusion
    convolutional neural network for sketch based image retrieval,” *Neurocomputing*,
    vol. 296, pp. 23–32, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] T.-T. Do, K. Le, T. Hoang, H. Le, T. V. Nguyen, and N.-M. Cheung, “Simultaneous
    feature aggregating and hashing for compact binary code learning,” *IEEE TIP*,
    vol. 28, no. 10, pp. 4954–4969, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] S. Dey, P. Riba, A. Dutta, J. Llados, and Y.-Z. Song, “Doodle to search:
    Practical zero-shot sketch-based image retrieval,” in *CVPR*, 2019, pp. 2179–2188.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] L.-K. Huang, J. Chen, and S. J. Pan, “Accelerate learning of deep hashing
    with gradient attention,” in *ICCV*, 2019, pp. 5271–5280.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] L.-W. Ge, J. Zhang, Y. Xia, P. Chen, B. Wang, and C.-H. Zheng, “Deep
    spatial attention hashing network for image retrieval,” *JVCIR*, vol. 63, p. 102577,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] S. Wei, L. Liao, J. Li, Q. Zheng, F. Yang, and Y. Zhao, “Saliency inside:
    Learning attentive cnns for content-based image retrieval,” *IEEE TIP*, vol. 28,
    no. 9, pp. 4580–4593, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Z. Chen, J. Lin, Z. Wang, V. Chandrasekhar, and W. Lin, “Beyond ranking
    loss: Deep holographic networks for multi-label video search,” in *ICIP*, 2019,
    pp. 879–883.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] L. Wang, X. Qian, Y. Zhang, J. Shen, and X. Cao, “Enhancing sketch-based
    image retrieval by cnn semantic re-ranking,” *IEEE Transactions on Cybernetics*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] V. Kumar Verma, A. Mishra, A. Mishra, and P. Rai, “Generative model for
    zero-shot sketch-based image retrieval,” in *CVPR Workshops*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] W. Gu, X. Gu, J. Gu, B. Li, Z. Xiong, and W. Wang, “Adversary guided
    asymmetric hashing for cross-modal retrieval,” in *ICMR*, 2019, pp. 159–167.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] S. Jin, S. Zhou, Y. Liu, C. Chen, X. Sun, H. Yao, and X. Hua, “Ssah:
    Semi-supervised adversarial deep hashing with self-paced hard sample generation,”
    *arXiv preprint arXiv:1911.08688*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Y. Wang, X. Ou, J. Liang, and Z. Sun, “Deep semantic reconstruction hashing
    for similarity retrieval,” *IEEE TCSVT*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] A. Pandey, A. Mishra, V. K. Verma, A. Mittal, and H. Murthy, “Stacked
    adversarial network for zero-shot sketch based image retrieval,” in *WACV*, 2020,
    pp. 2540–2549.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] T. Ng, V. Balntas, Y. Tian, and K. Mikolajczyk, “Solar: Second-order
    loss and attention for image retrieval,” *arXiv preprint arXiv:2001.08972*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Z. Gao, H. Xue, and S. Wan, “Multiple discrimination and pairwise cnn
    for view-based 3d object retrieval,” *Neural Networks*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] J. Song, T. He, L. Gao, X. Xu, A. Hanjalic, and H. T. Shen, “Unified
    binary generative adversarial network for image retrieval and compression,” *IJCV*,
    pp. 1–22, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. Yang, Y. Zhang, R. Feng, T. Zhang, and W. Fan, “Deep reinforcement
    hashing with redundancy elimination for effective image retrieval,” *Pattern Recognition*,
    vol. 100, p. 107116, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Pan, T. Yao, H. Li, C.-W. Ngo, and T. Mei, “Semi-supervised hashing
    with semantic confidence for large scale visual search,” in *Proceedings of the
    38th International ACM SIGIR Conference on Research and Development in Information
    Retrieval*, 2015, pp. 53–62.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] X. Yan, L. Zhang, and W.-J. Li, “Semi-supervised deep hashing with a
    bipartite graph.” in *IJCAI*, 2017, pp. 3238–3244.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Z. Qiu, Y. Pan, T. Yao, and T. Mei, “Deep semantic hashing with generative
    adversarial networks,” in *Proceedings of the 40th International ACM SIGIR Conference
    on Research and Development in Information Retrieval*, 2017, pp. 225–234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] S. Zhang, J. Li, and B. Zhang, “Pairwise teacher-student network for
    semi-supervised hashing,” in *CVPR Workshops*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] J. Tang and Z. Li, “Weakly supervised multimodal hashing for scalable
    social image retrieval,” *IEEE TCSVT*, vol. 28, no. 10, pp. 2730–2741, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Z. Guan, F. Xie, W. Zhao, X. Wang, L. Chen, W. Zhao, and J. Peng, “Tag-based
    weakly-supervised hashing for image retrieval.” in *IJCAI*, 2018, pp. 3776–3782.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] V. Gattupalli, Y. Zhuo, and B. Li, “Weakly supervised deep image hashing
    through tag embeddings,” in *CVPR*, 2019, pp. 10 375–10 384.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Z. Li, J. Tang, L. Zhang, and J. Yang, “Weakly-supervised semantic guided
    hashing for social image retrieval,” *IJCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Q. Hu, J. Wu, J. Cheng, L. Wu, and H. Lu, “Pseudo label based unsupervised
    deep discriminative hashing for image retrieval,” in *ACMMM*, 2017, pp. 1584–1590.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] X. Dong, L. Liu, L. Zhu, Z. Cheng, and H. Zhang, “Unsupervised deep k-means
    hashing for efficient image retrieval and clustering,” *IEEE TCSVT*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] H. Zhang, M. Wang, R. Hong, and T.-S. Chua, “Play and rewind: Optimizing
    binary representations of videos by self-supervised temporal hashing,” in *ACMMM*,
    2016, pp. 781–790.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] B. Liu, Y. Cao, M. Long, J. Wang, and J. Wang, “Deep triplet quantization,”
    in *ACMMM*, 2018, pp. 755–763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] H. Su, P. Wang, L. Liu, H. Li, Z. Li, and Y. Zhang, “Where to look and
    how to describe: Fashion image retrieval with an attentional heterogeneous bilinear
    network,” *IEEE TCSVT*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] K. Lin, H.-F. Yang, K.-H. Liu, J.-H. Hsiao, and C.-S. Chen, “Rapid clothing
    retrieval via deep learning of binary codes and hierarchical search,” in *ICMR*,
    2015, pp. 499–502.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] A. Babenko and V. Lempitsky, “Aggregating local deep features for image
    retrieval,” in *ICCV*, 2015, pp. 1269–1277.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Y. Liu, Y. Guo, S. Wu, and M. S. Lew, “Deepindex for accurate and efficient
    image retrieval,” in *ICMR*, 2015, pp. 43–50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] J. Yue-Hei Ng, F. Yang, and L. S. Davis, “Exploiting local features from
    deep networks for image retrieval,” in *CVPR workshops*, 2015, pp. 53–61.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] T. Uricchio, M. Bertini, L. Seidenari, and A. Bimbo, “Fisher encoded
    convolutional bag-of-windows for efficient image retrieval and social image tagging,”
    in *ICCV Workshops*, 2015, pp. 9–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] T.-T. Do, A.-D. Doan, and N.-M. Cheung, “Learning to hash with binary
    deep neural network,” in *ECCV*, 2016, pp. 219–234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] E. Mohedano, K. McGuinness, N. E. O’Connor, A. Salvador, F. Marques,
    and X. Giro-i Nieto, “Bags of local convolutional features for scalable instance
    search,” in *ICMR*, 2016, pp. 327–331.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] A. Qayyum, S. M. Anwar, M. Awais, and M. Majid, “Medical image retrieval
    using deep convolutional neural network,” *Neurocomputing*, vol. 266, pp. 8–20,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] X.-S. Wei, J.-H. Luo, J. Wu, and Z.-H. Zhou, “Selective convolutional
    descriptor aggregation for fine-grained image retrieval,” *IEEE TIP*, vol. 26,
    no. 6, pp. 2868–2881, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] W. Yu, K. Yang, H. Yao, X. Sun, and P. Xu, “Exploiting the complementary
    strengths of multi-layer cnn features for image retrieval,” *Neurocomputing*,
    vol. 237, pp. 235–241, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] T.-T. Do, D.-K. Le Tan, T. T. Pham, and N.-M. Cheung, “Simultaneous feature
    aggregating and hashing for large-scale image search,” in *CVPR*, 2017, pp. 6618–6627.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] W. Zhou, H. Li, J. Sun, and Q. Tian, “Collaborative index embedding for
    image retrieval,” *IEEE TPAMI*, vol. 40, no. 5, pp. 1154–1166, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] X. Liu, S. Zhang, T. Huang, and Q. Tian, “E2bows: An end-to-end bag-of-words
    model via deep convolutional neural network for image retrieval,” *Neurocomputing*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] T.-T. Do, T. Hoang, D.-K. Le Tan, A.-D. Doan, and N.-M. Cheung, “Compact
    hash code learning with binary deep neural network,” *IEEE TMM*, vol. 22, no. 4,
    pp. 992–1004, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] T.-T. Do, T. Hoang, D.-K. L. Tan, H. Le, T. V. Nguyen, and N.-M. Cheung,
    “From selective deep convolutional features to compact binary representations
    for image retrieval,” *ACM TOMM*, vol. 15, no. 2, pp. 1–22, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] X. Lu, Y. Chen, and X. Li, “Discrete deep hashing with ranking optimization
    for image retrieval,” *IEEE TNNLS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] S. R. Dubey, S. K. Roy, S. Chakraborty, S. Mukherjee, and B. B. Chaudhuri,
    “Local bit-plane decoded convolutional neural network features for biomedical
    image retrieval,” *NCAA*, pp. 1–13, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Z. Ji, W. Yao, W. Wei, H. Song, and H. Pi, “Deep multi-level semantic
    hashing for cross-modal retrieval,” *IEEE Access*, vol. 7, pp. 23 667–23 674,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] J. Zhu, J. Wang, S. Pang, W. Guan, Z. Li, Y. Li, and X. Qian, “Co-weighting
    semantic convolutional features for object retrieval,” *JVCIR*, vol. 62, pp. 368–380,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Y. Chen and X. Lu, “Deep discrete hashing with pairwise correlation learning,”
    *Neurocomputing*, vol. 385, pp. 111–121, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Y. Cao, M. Long, J. Wang, Q. Yang, and P. S. Yu, “Deep visual-semantic
    hashing for cross-modal retrieval,” in *ACM ICKDDM*, 2016, pp. 1445–1454.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Y. Wu, S. Wang, and Q. Huang, “Online asymmetric similarity learning
    for cross-modal retrieval,” in *CVPR*, 2017, pp. 4269–4278.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] J. Liu, M. Yang, C. Li, and R. Xu, “Improving cross-modal image-text
    retrieval with teacher-student learning,” *IEEE TCSVT*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] J. Lei, Y. Song, B. Peng, Z. Ma, L. Shao, and Y.-Z. Song, “Semi-heterogeneous
    three-way joint embedding network for sketch-based image retrieval,” *IEEE TCSVT*,
    vol. 30, no. 9, pp. 3226–3237, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] L. Liu, F. Shen, Y. Shen, X. Liu, and L. Shao, “Deep sketch hashing:
    Fast free-hand sketch-based image retrieval,” in *CVPR*, 2017, pp. 2862–2871.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] A. Dutta and Z. Akata, “Semantically tied paired cycle consistency for
    zero-shot sketch-based image retrieval,” in *CVPR*, 2019, pp. 5089–5098.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] F. Zhao, Y. Huang, L. Wang, and T. Tan, “Deep semantic ranking based
    hashing for multi-label image retrieval,” in *CVPR*, 2015, pp. 1556–1564.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] D. Wu, Z. Lin, B. Li, M. Ye, and W. Wang, “Deep supervised hashing for
    multi-label and large-scale image retrieval,” in *ICMR*, 2017, pp. 150–158.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] H. Lai, P. Yan, X. Shu, Y. Wei, and S. Yan, “Instance-aware hashing for
    multi-label image retrieval,” *IEEE TIP*, vol. 25, no. 6, pp. 2469–2479, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] G. Chen, X. Cheng, S. Su, and C. Tang, “Multiple-instance ranking based
    deep hashing for multi-label image retrieval,” *Neurocomp.*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Q. Qin, L. Huang, and Z. Wei, “Deep multilevel similarity hashing with
    fine-grained features for multi-label image retrieval,” *Neurocomp.*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] A. Sharif Razavian, J. Sullivan, A. Maki, and S. Carlsson, “A baseline
    for visual instance retrieval with deep convolutional networks,” in *ICLR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] O. Morère, A. Veillard, L. Jie, J. Petta, V. Chandrasekhar, and T. Poggio,
    “Group invariant deep representations for image instance retrieval,” in *AAAI
    Spring Symposium Series*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] G. Tolias, R. Sicre, and H. Jégou, “Particular object retrieval with
    integral max-pooling of cnn activations,” *arXiv preprint arXiv:1511.05879*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] S. Pang, J. Zhu, J. Wang, V. Ordonez, and J. Xue, “Building discriminative
    cnn image representations for object retrieval using the replicator equation,”
    *Pattern Recognition*, vol. 83, pp. 150–160, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] X. Shi and X. Qian, “Exploring spatial and channel contribution for object
    based image retrieval,” *Knowledge-Based Systems*, vol. 186, p. 104955, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] J. Guo, S. Zhang, and J. Li, “Hash learning with convolutional neural
    networks for semantic based image retrieval,” in *KDDM*, 2016, pp. 227–238.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Y. Cao, M. Long, J. Wang, and S. Liu, “Deep visual-semantic quantization
    for efficient image retrieval,” in *CVPR*, 2017, pp. 1328–1337.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Q. Qin, L. Huang, Z. Wei, K. Xie, and W. Zhang, “Unsupervised deep multi-similarity
    hashing with semantic structure for image retrieval,” *IEEE TCSVT*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] J. Zhang and Y. Peng, “Query-adaptive image retrieval by deep-weighted
    hashing,” *IEEE TMM*, vol. 20, no. 9, pp. 2400–2414, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] X. Zeng, Y. Zhang, X. Wang, K. Chen, D. Li, and W. Yang, “Fine-grained
    image retrieval via piecewise cross entropy loss,” *Image and Vision Computing*,
    vol. 93, p. 103820, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Z. Yang, O. I. Raymond, W. Sun, and J. Long, “Asymmetric deep semantic
    quantization for image retrieval,” *IEEE Access*, vol. 7, pp. 72 684–72 695, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] J. Chen and W. K. Cheung, “Similarity preserving deep asymmetric quantization
    for image retrieval,” in *AAAI*, vol. 33, 2019, pp. 8183–8190.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] J. Revaud, J. Almazán, R. S. Rezende, and C. R. d. Souza, “Learning with
    average precision: Training image retrieval with a listwise loss,” in *ICCV*,
    2019, pp. 5107–5116.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] H. Zhai, S. Lai, H. Jin, X. Qian, and T. Mei, “Deep transfer hashing
    for image retrieval,” *IEEE TCSVT*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Z. Dong, C. Jing, M. Pei, and Y. Jia, “Deep cnn based binary hash video
    representations for face retrieval,” *Pattern Recognition*, vol. 81, pp. 357–369,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] S. R. Dubey and S. Chakraborty, “Average biased relu based cnn descriptor
    for improved face retrieval,” *arXiv preprint arXiv:1804.02051*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] T.-Y. Yang, D. Kien Nguyen, H. Heijnen, and V. Balntas, “Dame web: Dynamic
    mean with whitening ensemble binarization for landmark retrieval without human
    annotation,” in *ICCV Workshops*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] L. Zhu, H. Cui, Z. Cheng, J. Li, and Z. Zhang, “Dual-level semantic transfer
    deep hashing for efficient social image retrieval,” *IEEE TCSVT*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/2edc107d25d332d63b13c724d00c95ff.png) | Shiv Ram
    Dubey has been with the Indian Institute of Information Technology (IIIT), Sri
    City since June 2016, where he is currently the Assistant Professor of Computer
    Science and Engineering. He received the Ph.D. degree in Computer Vision and Image
    Processing from Indian Institute of Information Technology, Allahabad (IIIT Allahabad)
    in 2016\. Before that, from August 2012-Feb 2013, he was a Project Officer in
    the Computer Science and Engineering Department at Indian Institute of Technology,
    Madras (IIT Madras). He was a recipient of several awards, including the Best
    PhD Award in PhD Symposium, IEEE-CICT2017 at IIITM Gwalior and NVIDIA GPU Grant
    Award Twice from NVIDIA. His research interest includes Computer Vision, Deep
    Learning, Image Feature Description, and Content Based Image Retrieval. |'
  prefs: []
  type: TYPE_TB
