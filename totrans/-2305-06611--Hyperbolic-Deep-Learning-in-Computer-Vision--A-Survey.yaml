- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: æœªåˆ†ç±»'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:39:52'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2305.06611] Hyperbolic Deep Learning in Computer Vision: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2305.06611](https://ar5iv.labs.arxiv.org/html/2305.06611)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: âˆ
  prefs: []
  type: TYPE_NORMAL
- en: 'Â¹Â¹institutetext: Pascal Mettes Â²Â²institutetext: University of Amsterdam, the
    Netherlands'
  prefs: []
  type: TYPE_NORMAL
- en: 'Â²Â²email: p.s.m.mettes@uva.nl Â³Â³institutetext: Mina Ghadimi Atigh â´â´institutetext:
    University of Amsterdam, the Netherlands'
  prefs: []
  type: TYPE_NORMAL
- en: 'â´â´email: m.ghadimiatigh@uva.nl âµâµinstitutetext: Martin Keller-Ressel â¶â¶institutetext:
    TU Dresden, Germany'
  prefs: []
  type: TYPE_NORMAL
- en: 'â¶â¶email: martin.keller-ressel@tu-dresden.de â·â·institutetext: Jeffrey Gu â¸â¸institutetext:
    Stanford University, USA'
  prefs: []
  type: TYPE_NORMAL
- en: 'â¸â¸email: jeffgu@stanford.edu â¹â¹institutetext: Serena Yeung ^(10)^(10)institutetext:
    Stanford University, USA'
  prefs: []
  type: TYPE_NORMAL
- en: '^(10)^(10)email: syyeung@stanford.edu'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyperbolic Deep Learning in Computer Vision: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pascal Mettes â€ƒâ€ƒ Mina Ghadimi Atigh â€ƒâ€ƒ Martin Keller-Ressel â€ƒâ€ƒ Jeffrey Gu â€ƒâ€ƒ
    Serena Yeung(Received: date / Accepted: date)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep representation learning is a ubiquitous part of modern computer vision.
    While Euclidean space has been the de facto standard manifold for learning visual
    representations, hyperbolic space has recently gained rapid traction for learning
    in computer vision. Specifically, hyperbolic learning has shown a strong potential
    to embed hierarchical structures, learn from limited samples, quantify uncertainty,
    add robustness, limit error severity, and more. In this paper, we provide a categorization
    and in-depth overview of current literature on hyperbolic learning for computer
    vision. We research both supervised and unsupervised literature and identify three
    main research themes in each direction. We outline how hyperbolic learning is
    performed in all themes and discuss the main research problems that benefit from
    current advances in hyperbolic learning for computer vision. Moreover, we provide
    a high-level intuition behind hyperbolic geometry and outline open research questions
    to further advance research in this direction.
  prefs: []
  type: TYPE_NORMAL
- en: '^â€ ^â€ journal: International Journal of Computer Vision'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From image segmentation to future frame prediction and from video grounding
    to generating images, deep representation learning is the central component that
    drives modern computer vision problems (LeCun etÂ al, [2015](#bib.bib75)). In short
    succession, many differentiable layers and network architectures have been proposed
    to tackle visual research problems (Gu etÂ al, [2018](#bib.bib49); Bommasani etÂ al,
    [2021](#bib.bib11); Khan etÂ al, [2022](#bib.bib66)). While different in structure,
    scope, and inductive biases, all are based on Euclidean operators and therefore
    - implicitly or explicitly - assume that data is best represented on regular grids.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean space forms an intuitive and grounded underlying manifold, but its
    inherent properties are not a best match for all types of data. Consider for example
    hierarchical structures such as trees, ontologies, and taxonomies. Hierarchies
    are foundational building blocks across all scientific disciplines to formalize
    our knowledge (Noy and Hafner, [1997](#bib.bib95)). In hierarchies, the number
    of nodes grows exponentially with depth, from few coarse-grained to many fine-grained
    nodes. The volume of a ball in Euclidean space however, grows only polynomially
    with its diameter. An alternative geometry is needed to match the nature of hierarchies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the quest for a more appropriate geometry of hierarchies, hyperbolic geometry
    provides a direct fit (Bridson and Haefliger, [2013](#bib.bib13)). In essence,
    hyperbolic and Euclidean geometry are different in only one aspect: the parallel
    postulate. In Euclidean space, there is exactly one parallel line that goes through
    a point not on the other line. In hyperbolic space, there are at least two such
    parallel lines. This change comes with many consequences and as a result, hyperbolic
    geometry can be seen as a geometry of constant negative curvature. In the context
    of deep learning this geometry has many attractive properties, such as its hierarchical
    structure and exponential expansion.'
  prefs: []
  type: TYPE_NORMAL
- en: Empowered by these geometric properties, hierarchical embeddings have in recent
    years been performed in hyperbolic space with great success (Nickel and Kiela,
    [2017](#bib.bib93)), leading to unparalleled abilities to embed deep and complex
    trees with minimal distortion (Ganea etÂ al, [2018a](#bib.bib40); Sala etÂ al, [2018](#bib.bib102)).
    This has led to rapid advances in hyperbolic deep learning across many disciplines
    and research areas, including but not limited to graph networks (Chami etÂ al,
    [2019](#bib.bib16); Liu etÂ al, [2019](#bib.bib80); Dai etÂ al, [2021](#bib.bib29)),
    text embeddingsÂ (Tifrea etÂ al, [2019](#bib.bib110); Zhu etÂ al, [2020](#bib.bib134)),
    molecular representation learning (Klimovskaia etÂ al, [2020](#bib.bib72); Yu etÂ al,
    [2020](#bib.bib126); Wu etÂ al, [2021](#bib.bib121)), and recommender systems (Mirvakhabova
    etÂ al, [2020](#bib.bib88); Wang etÂ al, [2021](#bib.bib118); Yang etÂ al, [2022](#bib.bib125)).
  prefs: []
  type: TYPE_NORMAL
- en: In the wake of other disciplines, computer vision has in recent years also benefited
    from research into deep learning in hyperbolic space. A quickly growing body of
    literature has shown that hyperbolic embeddings benefit few-shot learning, zero-shot
    recognition, out-of-distribution generalization, uncertainty quantification, generative
    learning, and hierarchical representation learning amongst others. These works
    show evidence that hyperbolic geometry has a lot of potential for learning in
    computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: This survey provides an in-depth overview and categorization of the recent boom
    in hyperbolic computer vision literature. These works have investigated hyperbolic
    learning across many visual research problems with different solutions. As a result,
    it is unclear how current literature is connected, what is common and new in each
    work, and in which direction the field is heading. This survey seeks to fill this
    void. We investigate both supervised and unsupervised papers. For supervised learning,
    we identify three shared themes amongst current papers, where samples are matched
    to either gyroplanes, prototypes, or other samples in hyperbolic space. For unsupervised
    papers, we dive into the three main axes explored in current papers, namely generative
    learning, clustering, and self-supervised learning. Peng etÂ al ([2021](#bib.bib96))
    have recently written a general survey on hyperbolic neural networks but their
    scope did not include the computer vision literature on hyperbolic learning. This
    survey fills this void.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the paper is organised as follows. In Section [2](#S2 "2 Background
    on hyperbolic geometry â€£ Hyperbolic Deep Learning in Computer Vision: A Survey")
    we provide the background on hyperbolic geometry and foundational papers on hyperbolic
    embeddings and hyperbolic neural networks. Sections [3](#S3 "3 Supervised hyperbolic
    visual learning â€£ Hyperbolic Deep Learning in Computer Vision: A Survey") and
    [4](#S4 "4 Unsupervised hyperbolic visual learning â€£ Hyperbolic Deep Learning
    in Computer Vision: A Survey") provide an overview of supervised and unsupervised
    hyperbolic visual learning literature. Lastly in Section [5](#S5 "5 Conclusions
    and future outlook â€£ Hyperbolic Deep Learning in Computer Vision: A Survey") we
    outline advantages and improvements reported in current papers, as well as open
    challenges for the field.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background on hyperbolic geometry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 What is hyperbolic geometry?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hyperbolic geometry was initially developed in the 19th century by Gauss, Lobachevsky,
    Bolyai and others as a concrete example of a non-Euclidean geometry. Soon after
    it found important applications in physics, as the mathematical basis of Einsteinâ€™s
    special theory of relativity. It can be characterized as the geometry of *constant
    negative curvature*, differentiating it from the flat geometry of Euclidean space
    and the positively curved geometry of spheres and hyperspheres. From the point
    of view of representation learning, its attractive properties are its exponential
    expansion and its hierarchical, tree-like structure. Exponential expansion means
    that the volume of a ball in hyperbolic space growths exponentially with its diameter,
    in contrast to Euclidean space, where the rate of growth is polynomial. The â€˜tree-likenessâ€™
    of a metric space can be quantified by Gromovâ€™s hyperbolicity (Bridson and Haefliger,
    [2013](#bib.bib13)), which is zero for tree graphs, finite (but non-zero) for
    hyperbolic space, and infinite for Euclidean space.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Models of hyperbolic geometry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several different, but eventually equivalent, models of hyperbolic geometry
    exist (Cannon etÂ al, [1997](#bib.bib14)). They differ in their coordinate representations
    of points and in their expressions for distances, geodesics, and other quantities.
    Although they can be converted into each other, certain models may be preferred
    for a given task, for reasons of numerical efficiency, ease of visualization,
    or simplified calculations. The most commonly used models are the PoincarÃ© model,
    the hyperboloid (or â€˜Lorentzâ€™) model, the Klein model, and the upper half-space
    model.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PoincarÃ© model represents $d$-dimensional hyperbolic space by the unit ball
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathbb{D}_{d}=\{p\in\mathbb{R}^{d}:p_{1}^{2}+\dotsm+p_{d}^{2}<1\}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'which, in the frequently considered case $d=2$ becomes the unit disc. Geodesics
    (â€˜shortest pathsâ€™) are arcs of Euclidean circles (or lines), meeting the boundary
    of $\mathbb{D}_{d}$ at a right angle. While distances, area and volume are distorted
    in comparison to their Euclidean counterparts, the model is *conformal*, i.e.,
    hyperbolic angles are measured as in Euclidean geometry. In its two-dimensional
    form as PoincarÃ© disc, the model is popular for visualizations; it is also the
    geometric basis of the art works Circle Limits I-IV of M.Â C.Â Escher; see FigureÂ [1](#S2.F1
    "Figure 1 â€£ item âˆ™ â€£ 2.2 Models of hyperbolic geometry â€£ 2 Background on hyperbolic
    geometry â€£ Hyperbolic Deep Learning in Computer Vision: A Survey").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e73eb56312655d95652e8b1a1c9db38.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1: Circle Limit I (1958). This artwork by M.Â C.Â Escher is based on the
    PoincarÃ© disc model of hyperbolic geometry.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hyperboloid model uses the single-sheet hyperboloid
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathbb{H}_{d}=\{x\in\mathbb{R}^{d+1}:x_{0}^{2}-\left(x_{1}^{2}+\dotsm+x_{d}^{2}\right)=1,x_{0}>0\}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: as a model of $d$-dimensional hyperbolic geometry. Contrary to the other models,
    its ambient space $\mathbb{R}^{d+1}$ adds one dimension to the modeled space.
    Many formulas involving the hyperboloid model can be written in concise form by
    introducing the *Lorentz product* $x\circ y=x_{0}y_{0}-(x_{1}y_{1}+\dotsm+x_{d}y_{d})$.
    An advantage of the hyperboloid model is that it retains some linear structure;
    translations and other isometries, for example, can be represented by linear maps.
    Expressions for distances and geodesics are simpler compared to other models.
    Notably, the PoincarÃ© model can be derived as a projection (â€˜stereographic projectionâ€™)
    of the hyperboloid model to the unit ball (Cannon etÂ al, [1997](#bib.bib14); Ratcliffe,
    [1994](#bib.bib99)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Klein model $\mathbb{K}_{d}$ also uses the unit ball to represent hyperbolic
    space. In contrast to the PoincarÃ© model, it is not conformal; its geodesics,
    however, are Euclidean (â€˜straightâ€™) lines, which can be beneficial from a computational
    point of view, *e.g.,* when computing barycenters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, the upper half space model represents $d$-dimensional hyperbolic space
    by the set $\mathbb{U}_{d}=\{x\in\mathbb{R}^{d}:x_{d}>0\}$. It is a conformal
    model and shares many properties with the PoincarÃ© model; geodesics, for example,
    are also arcs of Euclidean circles (or lines), meeting the boundary of $\mathbb{U}_{d}$
    at a right angle.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.3 Five core hyperbolic operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Within the context of deep learning and computer vision, we find that five
    core operations form the basic building blocks of the vast majority of algorithms.
    The ability to work with these five operations will cover most of existing literature:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measuring the distance of two points $x$ and $y$;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finding the geodesic arc (the distance-minimizing curve) from $x$ to $y$;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forming a geodesic, by extending a geodesic arc as far as possible;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the exponential map, to determine the result of following a geodesic in
    direction $u$, at speed $r$, starting at a point $x$;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Moving a cloud of points, while preserving all their pairwise hyperbolic distances,
    by applying a hyperbolic translation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The distance of two points is given, in the PoincarÃ© and the hyperboloid model
    respectively, by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle d_{\mathbb{D}}(p,q)$ | $\displaystyle=\frac{1}{\sqrt{\kappa}}\operatorname*{arcosh}\left(1+\frac{2&#124;p-q&#124;^{2}}{(1-&#124;p&#124;^{2})(1-&#124;q&#124;^{2})}\right),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle d_{\mathbb{H}}(x,y)$ | $\displaystyle=\frac{1}{\sqrt{\kappa}}\operatorname*{arcosh}\left(x\circ
    y\right).$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: In the less frequently used Klein and the upper half space model, distances
    are given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle d_{\mathbb{K}}(p,q)$ | $\displaystyle=\frac{1}{\sqrt{\kappa}}\operatorname*{arcosh}\left(\frac{1-p^{\top}q}{\sqrt{1-&#124;p&#124;^{2}}\sqrt{1-&#124;q&#124;^{2}}}\right),$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle d_{\mathbb{U}}(x,y)$ | $\displaystyle=\frac{1}{\sqrt{\kappa}}\operatorname*{arcosh}\left(1+\frac{&#124;x-y&#124;^{2}}{2x_{d}y_{d}}\right),$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: see (Ratcliffe, [1994](#bib.bib99), Â§6.1). The scaling factor of distances is
    controlled by the *curvature parameter* $\kappa\in(0,\infty)$, which is often
    standardized to $\kappa=1$. The sectional curvature (in the sense of differential
    geometry) of hyperbolic space is constant, negative and equal to $-\kappa$. Given
    the distance function, it makes sense to speak of geodesics and geodesic arcs,
    that is (locally) distance-minimizing curves, either extending infinitely or connecting
    two points. In the hyperboloid model for example, each geodesic is the intersection
    of $\mathbb{H}_{d}$ with a Euclidean hyperplane in the ambient space $\mathbb{R}^{d+1}$.
    The geodesic at a point $x\in\mathbb{H}_{d}$ in direction $v$ can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\lambda_{\mathbb{H}}(t)=\cosh(t\sqrt{\kappa})x+\sinh(t\sqrt{\kappa})u,\quad
    t\in\mathbb{R}.$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $u$ is an element of the *tangent space* $T_{x}=\{u\in\mathbb{R}^{d+1}:x\circ
    u=0\}$, normalized to $u\circ u=-1$. In the PoincarÃ© model, the geodesics are
    precisely the segments of Euclidean circles and lines that meet the boundary of
    $\mathbb{D}_{d}$ at a right angle. A convenient formula for the geodesic arc between
    two points $p,q\in\mathbb{D}_{d}$ can be given in terms of gyrovectorspace calculus,
    see ([8](#S2.E8 "In 2.4 Gyrovectorspace calculus â€£ 2 Background on hyperbolic
    geometry â€£ Hyperbolic Deep Learning in Computer Vision: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of the exponential map $\exp_{x}(tu)$ is the result of following
    a geodesic in a normalized direction $u$ at a speed $t>0$, after starting at a
    given point $x$ in hyperbolic space. Identifying $\mathbb{R}^{d}$ with the tangent
    space $T_{x}$ at $x$, the exponential mapping provides a convenient way to embed
    $\mathbb{R}^{d}$ into hyperbolic space with origin at $x$. The exponential map
    is the most often used function in hyperbolic learning for computer vision, as
    it allows us to map visual representations from Euclidean to hyperbolic space.
    In the hyperboloid model, the exponential mapping coincides with the expression
    of the geodesic given in ([5](#S2.E5 "In 2.3 Five core hyperbolic operations â€£
    2 Background on hyperbolic geometry â€£ Hyperbolic Deep Learning in Computer Vision:
    A Survey")). In the PoincarÃ© model the exponential map can be conveniently written
    in terms of gyrovectorspace addition and is given in ([9](#S2.E9 "In 2.4 Gyrovectorspace
    calculus â€£ 2 Background on hyperbolic geometry â€£ Hyperbolic Deep Learning in Computer
    Vision: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the hyperbolic translation $\tau_{x}$, also called Lorentz boost, MÃ¶bius
    transformation, or gyrovectorspace addition, is the unique distance-preserving
    transformation of hyperbolic space, which moves $0$ to a given point $x$. In the
    hyperboloid model, it can be represented by the linear map
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\tau_{x}(y)$ | $\displaystyle=L_{x}\cdot y,\quad\text{where}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L_{x}$ | $\displaystyle=\begin{pmatrix}x_{0}&amp;\bar{x}^{\top}\\
    \bar{x}&amp;\sqrt{I_{d}+\bar{x}\bar{x}^{\top}}\end{pmatrix}\text{ with }\bar{x}=(x_{0},\dotsc,x_{d}).$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: In the PoincarÃ© model hyperbolic translations are also known as gyrovectorspace
    addition and form the basic operation of gyrovectorspace calculus.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Gyrovectorspace calculus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gyrovectorspace calculus, as introduced by Ungar ([2005](#bib.bib113), [2012](#bib.bib115)),
    provides a convenient and rapidly adopted framework for calculations in the PoincarÃ©
    ball model. Its first basic operation is the (non-commutative) gyrovectorspace
    addition
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p\oplus q=\frac{(1-&#124;p&#124;^{2})q+(1+2p^{\top}q+&#124;q&#124;^{2})p}{1+2p^{\top}q+&#124;p&#124;^{2}&#124;q&#124;^{2}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: As a secondary operation, the (commutative) gyrovectorspace scalar product
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $t\otimes p=p\otimes t=\tanh\big{(}t\operatorname*{artanh}(&#124;p&#124;)\big{)}\frac{p}{&#124;p&#124;}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: with a scalar $t\in\mathbb{R}$ is introduced. Hyperbolic translations are directly
    given by $\tau_{p}(q)=p\oplus q$ and the geodesic arc connecting $p$ and $q$ is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\lambda_{\mathbb{D}}(t)=p\oplus\Big{(}\big{(}(-p)\oplus q\big{)}\otimes
    t\Big{)},\quad t\in[0,1].$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Letting $t$ range through all of $\mathbb{R}$ a full geodesic line is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of gyrovector space calculus, the PoincarÃ© ball is often rescaled
    with the square root of curvature, setting
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{D}^{d}_{\kappa}=\{p\in\mathbb{R}^{d}:p_{1}^{2}+\dotsm+p_{d}^{2}<1/\kappa\}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The advantage of this rescaling is that Euclidean space is obtained as a continuous
    limit as $\kappa\to 0$. In the rescaled model, gyrovectorspace addition and scalar
    product become
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p\oplus_{\kappa}q=\tfrac{1}{\sqrt{\kappa}}\left((\sqrt{\kappa}p)\oplus(\sqrt{\kappa}q)\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $t\otimes_{\kappa}p=\tfrac{1}{\sqrt{\kappa}}(t\otimes(\sqrt{\kappa}p))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: for $p,q\in\mathbb{D}^{d}_{\kappa}$. The exponential map in direction of a tangent
    vector $v\in T_{p}$ can then be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\exp_{x}^{\kappa}(v)=x\oplus_{\kappa}\left(\tanh\left(\frac{\sqrt{\kappa}&#124;v&#124;}{1-\kappa&#124;x&#124;^{2}}\right)\frac{v}{\sqrt{\kappa}&#124;v&#124;}\right)$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: for $p\in\mathbb{D}^{d}_{\kappa}$, see Ganea etÂ al ([2018b](#bib.bib41)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Non-visual hyperbolic learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The traction of hyperbolic learning in computer vision is built upon advances
    in embedding hierarchical structures, designing hyperbolic network layers, and
    hyperbolic learning on other data types such as graphs, text, and more. Below,
    we discuss these works and their relevance for hyperbolic visual learning literature.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic embedding of hierarchies.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Embedding hierarchical structures like trees and taxonomies in Euclidean space
    suffers from large distortionÂ (Bachmann etÂ al, [2020](#bib.bib6)), and polynomial
    volume expansion, limiting its capacity to capture the exponential complexity
    of hierarchies. However, hyperbolic space can be thought of as a continuous version
    of treesÂ (Nickel and Kiela, [2017](#bib.bib93)) and has tree-like propertiesÂ (Hamann,
    [2018](#bib.bib56); Ungar, [2008](#bib.bib114)), like the exponential growth of
    distances when moving from the origin towards the boundary. Encouraged by this,Â Nickel
    and Kiela ([2017](#bib.bib93)) propose to embed hierarchical structures on the
    PoincarÃ© model. The goal is to learn hyperbolic representations for the nodes
    of a hierarchy, such that the distance in the embedding space has an inverse relation
    with semantic similarity. Let $\mathcal{D}=\{(u,v)\}$ denote the set of the nodes
    connected in a given hierarchy. To embed the nodes in the PoincarÃ© model, Â Nickel
    and Kiela ([2017](#bib.bib93)) minimize the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}(\Theta)=\sum_{(u,v)\in\mathcal{D}}\log\frac{e^{-d(u,v)}}{\sum_{v^{{}^{\prime}}\in\mathcal{N}(u)}e^{-d(u,v^{{}^{\prime}})}},$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{N}(u)=\{v^{{}^{\prime}}|(u,v^{{}^{\prime}})\notin\mathcal{D}\}\cup\{v\}$
    denotes the set of the nodes not related to $u$, including $v$, as negative examples.
    The loss function pushes unrelated nodes farther apart than the related ones.
    To evaluate the embedded hierarchy, the distances between pairs of connected nodes
    $(u,v)$ are calculated and ranked among the negative pairs of nodes (*i.e.,* the
    nodes not in $\mathcal{D}$), and the mean average precision (MAP) is calculated
    based on the ranking. Later,Â Sala etÂ al ([2018](#bib.bib102)) propose a combinatorial
    construction to embed the trees in hyperbolic space without optimization and with
    low distortion, relieving the optimization problems in existing works. Â Ganea
    etÂ al ([2018a](#bib.bib40)) address drawbacks ofÂ (Nickel and Kiela, [2017](#bib.bib93))
    including the collapse of the points on the boundary of the space as a result
    of the loss function and incapability of encoding asymmetric relations. They introduce
    entailment cones to embed hierarchies, using a max-margin loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\sum_{(u,v)\in\mathcal{P}}E(u,v)+\sum_{(u^{{}^{\prime}},v^{{}^{\prime}})\in\mathcal{N}}\max(0,\gamma-E(u^{{}^{\prime}},v^{{}^{\prime}})),$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $\gamma$, $\mathcal{P}$, and $\mathcal{N}$ indicate margin, the positive
    and negative edges, respectively. $E(u,v)$ is a penalty term that forces child
    nodes to fall under the cone of the parent node. Amongst others, hyperbolic embeddings
    have been proposed for multi-relational graphsÂ (Balazevic etÂ al, [2019](#bib.bib9)),
    low-dimensional knowledge graphsÂ (Chami etÂ al, [2020b](#bib.bib18)), and learning
    continuous hierarchies in Lorentz modelÂ (Nickel and Kiela, [2018](#bib.bib94)).
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic neural networks.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Foundational in the transition of deep learning towards hyperbolic space is
    the development of hyperbolic network layers and their optimization. We consider
    two pivotal papers here that provide a such theoretical foundation, namely Hyperbolic
    Neural Networks by Ganea etÂ al ([2018b](#bib.bib41)) and Hyperbolic Neural Networks++
    by Shimizu etÂ al ([2021](#bib.bib105)). Â Ganea etÂ al ([2018b](#bib.bib41)) propose
    multinomial logistic regression in the PoincarÃ© ball. Given $k\in\{1,...,K\}$
    classes, $p_{k}\in\mathbb{D}^{n}_{c}$, and $a_{k}\in\mathbb{D}^{n}_{c}\setminus\{0\}$,
    hyperbolic logistic regression is performed using
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}p(y=k&#124;x)\propto&amp;\exp(\frac{\lambda^{c}_{pk}\lVert
    a_{k}\rVert}{\sqrt{c}}\\ &amp;\sinh^{-1}(\frac{2\sqrt{c}\langle-p_{k}\oplus_{c}x,a_{k}\rangle}{(1-c\lVert-p_{k}\oplus_{c}x\rVert^{2})\lVert
    a_{k}\rVert})).\end{split}$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 'As an extension, a hyperbolic version of linear layer $f$ is given as $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$,
    a MÃ¶bius version of $f$ where the map from $\mathbb{D}^{n}\rightarrow\mathbb{D}^{m}$
    is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f^{\otimes_{c}}\coloneqq\exp^{c}_{0}(f(\log^{c}_{0}(x))),$ |  | (13)
    |'
  prefs: []
  type: TYPE_TB
- en: with $\exp^{c}_{0}:T_{0_{m}}\mathbb{D}^{m}_{c}\rightarrow\mathbb{D}^{m}_{c}$
    and $\log^{c}_{0}:\mathbb{D}^{n}_{c}\rightarrow T_{0_{n}}\mathbb{D}^{n}_{c}$.
    They furthermore outline how to create recurrent network layers.
  prefs: []
  type: TYPE_NORMAL
- en: Shimizu etÂ al ([2021](#bib.bib105)) reformulate the hyperbolic logistic regression
    of (Ganea etÂ al, [2018b](#bib.bib41)) to reduce the number of parameters to the
    same level as the Euclidean logistic regression. The new formulation is $p(y=k|x)\propto\exp(v_{k}(x))$,
    where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}v_{k}(x)=&amp;\ 2c^{-\frac{1}{2}}\lVert z_{k}\rVert\sinh^{-1}(\lambda^{c}_{x}\langle\sqrt{c}x,[z_{k}]\rangle\cosh(2\sqrt{c}r_{k})\\
    &amp;-(\lambda^{c}_{x}-1)\sinh(2\sqrt{c}r_{k}))\end{split}$ |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where $r_{k}\in\mathbb{R}$ and $z_{k}\in T_{0}\mathbb{B}^{n}_{c}=\mathbb{R}^{n}$
    are the parameters for each class. In turn, their linear layer is given as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y=\mathcal{F}^{c}(x;Z,r)\coloneqq w(1+\sqrt{{1+c\lVert w\rVert^{2}}})^{-1}$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: where $Z=\{z_{k}\in T_{0}\mathbb{B}^{n}_{c}=\mathbb{R}^{n}\}^{m}_{k=1}$, $r=\{r_{k}\in\mathbb{R}\}^{m}_{k=1}$,
    and $w\coloneqq(c^{-\frac{1}{2}}\sinh(\sqrt{c}v_{k}(x)))^{m}_{k=1}$. More importantly
    for computer vision, they show how to formulate convolutional layers using PoincarÃ©
    fully connected layer and $\beta$-concatenation. To do so, they show how to generalize
    the hyperbolic linear layer to image patches through $\beta$-splits, and $\beta$-concatenation,
    leading in principle to arbitrary-dimensional convolutional layers. Moreover,
    PoincarÃ© multi-head attention is possible through the same operators.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e296a280c07a1d5450d9c3584d572258.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The three core strategies for supervised hyperbolic learning in computer
    vision. Current literature performs hyperbolic learning of visual embeddings by
    learning to match training samples (i) to hyperbolic class hyperplanes, *i.e.,*
    gyroplanes, (ii) to hyperbolic class prototypes, or (iii) by contrasting to other
    samples.'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic learning of graphs, text, and more.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The advances in hyperbolic embeddings of hierarchies and the introduction of
    hyperbolic network layers have spurred research in several other research directions
    as well. As a logical extension of hierarchical embeddings, graph networks have
    been extended to hyperbolic space. Liu etÂ al ([2019](#bib.bib80)) and Chami etÂ al
    ([2019](#bib.bib16)) propose a tangent-based view to hyperbolic graph networks.
    Both approaches model a graph layer by first mapping node embeddings to the tangent
    space, then performing the transformation and aggregation in the tangent space,
    after which the updated node embeddings are projected back to the hyperbolic manifold
    at hand. Since tangent operations only provide an approximation of the graph operations
    on the manifold, several works have proposed graph networks that better abide
    the underlying hyperbolic geometry, such as constant curvature $\kappa$-GCNs (Bachmann
    etÂ al, [2020](#bib.bib6)), hyperbolic-to-hyperbolic GCNs (Dai etÂ al, [2021](#bib.bib29)),
    Lorentzian GCNs (Zhang etÂ al, [2021c](#bib.bib133)), and attention-based hyperbolic
    graph networks (Gulcehre etÂ al, [2019](#bib.bib50); Zhang etÂ al, [2021b](#bib.bib132)).
    Hyperbolic graph networks have shown to improve node, link, and graph classification
    compared to Euclidean variants, especially when graphs have latent hierarchical
    structures.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic embeddings have also been investigated for text. Tifrea etÂ al ([2019](#bib.bib110)),
    Dhingra etÂ al ([2018](#bib.bib34)), and Leimeister and Wilson ([2018](#bib.bib76))
    propose hyperbolic alternatives for word embeddings. Zhu etÂ al ([2020](#bib.bib134))
    introduce HyperText to endow FastText with hyperbolic geometry. Embedding text
    in hyperbolic space has the potential to improve similarity, analogy, and hypernymy
    detection, most notably with few embedding dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond text and graphs, hyperbolic learning has shown to be beneficial for several
    other research directions, including but not limited to learning representations
    for molecular/cellular structures (Klimovskaia etÂ al, [2020](#bib.bib72); Yu etÂ al,
    [2020](#bib.bib126); Wu etÂ al, [2021](#bib.bib121)), recommender systems (Mirvakhabova
    etÂ al, [2020](#bib.bib88); Wang etÂ al, [2021](#bib.bib118); Yang etÂ al, [2022](#bib.bib125)),
    skeletal action recognition (Franco etÂ al, [2023](#bib.bib39)), LiDAR data (Tong
    etÂ al, [2022](#bib.bib111); Wang etÂ al, [2023](#bib.bib119)), point clouds (Montanaro
    etÂ al, [2022](#bib.bib91); Anvekar and Bazazian, [2023](#bib.bib3)), and 3D shapes
    (Chen etÂ al, [2020b](#bib.bib21)). In summary, hyperbolic geometry has impacted
    a wide range of research fields. This survey focuses specifically on the impact
    and potential in the visual domain.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Supervised hyperbolic visual learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In FigureÂ [2](#S2.F2 "Figure 2 â€£ Hyperbolic neural networks. â€£ 2.5 Non-visual
    hyperbolic learning â€£ 2 Background on hyperbolic geometry â€£ Hyperbolic Deep Learning
    in Computer Vision: A Survey"), we provide an overview of literature on supervised
    learning with hyperbolic geometry in computer vision. In current vision works,
    hyperbolic learning is mostly performed at the embedding- or classifier-level.
    In other words, current works rely on standard networks for feature learning and
    transform the output embeddings to hyperbolic space for the final learning stage.
    For supervised learning in hyperbolic space, we have identified three main optimization
    strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample-to-gyroplane learning denotes the setting where classes are represented
    by hyperbolic hyperplanes, *i.e.,* gyroplanes, with networks optimized based on
    confidence logit scores between samples and gyroplanes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample-to-prototype learning denotes the setting where class semantics are represented
    as points in hyperbolic space, and networks are optimized to minimize hyperbolic
    distances between samples and prototypes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample-to-sample learning denotes the setting where networks are optimized by
    learning metrics or contrastive objectives between samples in a batch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For all strategies, let $(x,y)$ denote the visual input $x$, which can be an
    image or a video, and the corresponding label $y\in\mathcal{Y}$. Let $f_{\theta}(x)\in\mathbb{R}^{D}$
    denote its Euclidean embedding after going through a network. This representation
    is mapped to hyperbolic space using the exponential map, denoted as $g(x)=\exp_{0}(f_{\theta}(x))$.
    In many hyperbolic works, additional information about hierarchical relations
    between classes is assumed. Let $\mathcal{H}=(\mathcal{Y},\mathcal{P},\mathcal{R})$,
    with $\mathcal{Y}$ the class labels denoting the leaf nodes of the hierarchy,
    $\mathcal{P}$ the internal nodes, and $\mathcal{R}$ the set of hypernym-hyponym
    relations of the hierarchy. Below, we discuss how current literature tackles each
    strategy in detail sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Sample-to-gyroplane learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most direct way to induce hyperbolic geometry in the classification space
    is by replacing the classification layer by a hyperbolic alternative. This can
    be done either by means of a hyperbolic logistic regression or through hyperbolic
    kernel machines.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic logistic regression.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Khrulkov etÂ al ([2020](#bib.bib68)) incorporate a hyperbolic classifier by taking
    a standard convolutional network and mapping the outputs of the last hidden layer
    to hyperbolic space using an exponential map. Afterwards, the hyperbolic multinomial
    logistic regression as described by Ganea etÂ al ([2018b](#bib.bib41)) is used
    to obtain class logits which can be optimized with cross-entropy. They find that
    training a hyperbolic classifier on top of a convolutional network allows us to
    obtain uncertainty information based on the distance to the origin of the hyperbolic
    embeddings of images. Out-of-distribution samples on average have a smaller norm,
    making it possible by differentiating in- to out-of-distribution samples by sorting
    them by the distance to the origin. Â Hong etÂ al ([2022](#bib.bib60)) show that
    hyperbolic classification is beneficial for visual anomaly recognition tasks,
    such as out-of-distribution detection in image classification and segmentation
    tasks.Â AraÃ±o etÂ al ([2021](#bib.bib4)) use hyperbolic layers to perform multi-modal
    sentiment analysis based on the audio, video, and text modalities.Â Ahmad and Lecue
    ([2022](#bib.bib1)) also show the effect of hyperbolic space to perform object
    recognition with ultra-wide field-of-view lenses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Guo etÂ al ([2022](#bib.bib54)) address a limitation when training classifiers
    in hyperbolic space, namely a vanishing gradient problem due to the hybrid architecture
    of current hyperbolic approaches in computer vision, where Euclidean features
    are connected to a hyperbolic classifier. Equation [12](#S2.E12 "In Hyperbolic
    neural networks. â€£ 2.5 Non-visual hyperbolic learning â€£ 2 Background on hyperbolic
    geometry â€£ Hyperbolic Deep Learning in Computer Vision: A Survey") highlights
    that to maximize the likelihood of correct predictions, the distance to hyperbolic
    gyroplanes needs to be maximized. In practice, embeddings of samples are pushed
    to the boundary of the PoincarÃ© ball. As a result, the inverse of the Riemannian
    tensor metric approaches zero, resulting in small gradients. This finding is in
    line with several other works on vanishing gradients in hyperbolic representation
    learning (Nickel and Kiela, [2018](#bib.bib94); Liu etÂ al, [2019](#bib.bib80)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To combat the vanishing gradient problem, Guo etÂ al ([2022](#bib.bib54)) propose
    to clip the Euclidean embeddings of samples before the exponential mapping, *i.e.,*:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f^{\text{clipped}}_{\theta}(x)=\min\big{\{}1,\frac{r}{&#124;&#124;f_{\theta}(x)&#124;&#124;}\big{\}}\cdot
    f_{\theta}(x),$ |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: with $r$ as a hyperparameter. This trick improves learning with hyperbolic multinomial
    logistic regression, especially when dealing with many classes such as on ImageNet.
    Furthermore, training with clipped hyperbolic classifiers improves out-of-distribution
    detection over training with Euclidean classifiers, while also being more robust
    to adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next to global classification, a few recent works have investigated hyperbolic
    logistic regression for structured prediction tasks such as object detection and
    image segmentation. Valada ([2022](#bib.bib116)) extend object detection with
    hyperbolic geometry, amongst others by replacing the classifier head of a two-stage
    detection like Sparse R-CNN (Sun etÂ al, [2021](#bib.bib108)) with a hyperbolic
    logistic regression, improving object detection performance in standard and zero-shot
    settings. GhadimiÂ Atigh etÂ al ([2022](#bib.bib47)) introduce Hyperbolic Image
    Segmentation, where the final per-pixel classification was performed in hyperbolic
    space. Starting from the geometric interpretation of hyperbolic gyroplanes of
    Ganea etÂ al ([2018b](#bib.bib41)), they find that simultaneously computing class
    logits over all pixels of all images in a batch, as is customary in Euclidean
    networks, is not directly applicable in hyperbolic space. This is because the
    explicit computation of the MÃ¶bius addition requires evaluating a tensor in $\mathbb{R}^{W\times
    H\times|\mathcal{Y}|\times d}$ for an images of size $(W\times H)$ with $d$ embedding
    dimensions. Instead, they rewrite the MÃ¶bius addition as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}f_{1}\oplus_{c}f_{2}=&amp;\alpha
    f_{1}+\beta f_{2},\\ \alpha=&amp;\frac{1+2c\langle f_{1},f_{2}\rangle+c&#124;&#124;f_{2}&#124;&#124;^{2}}{1+2c\langle
    f_{1},f_{2}\rangle+c^{2}&#124;&#124;f_{1}&#124;&#124;^{2}&#124;&#124;f_{2}&#124;&#124;^{2}},\\'
  prefs: []
  type: TYPE_NORMAL
- en: \beta=&amp;\frac{1+c&#124;&#124;f_{1}&#124;&#124;^{2}}{1+2c\langle f_{1},f_{2}\rangle+c^{2}&#124;&#124;f_{1}&#124;&#124;^{2}&#124;&#124;f_{2}&#124;&#124;^{2}}.\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd 
    columnalign="right" ><mrow ><mrow
    ><msub ><mi 
    >f</mi><mn  >1</mn></msub><msub
    ><mo  >âŠ•</mo><mi
     >c</mi></msub><msub
    ><mi  >f</mi><mn
     >2</mn></msub></mrow><mo
     >=</mo></mrow></mtd><mtd
     columnalign="left" ><mrow ><mrow
    ><mrow ><mi
     >Î±</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><msub ><mi
     >f</mi><mn 
    >1</mn></msub></mrow><mo 
    >+</mo><mrow ><mi
     >Î²</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><msub ><mi
     >f</mi><mn
     >2</mn></msub></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd
     columnalign="right" ><mrow ><mi
     >Î±</mi><mo 
    >=</mo></mrow></mtd><mtd 
    columnalign="left" ><mrow ><mfrac
     ><mrow 
    ><mn  >1</mn><mo
     >+</mo><mrow
     ><mn
     >2</mn><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >c</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >âŸ¨</mo><msub
     ><mi
     >f</mi><mn
     >1</mn></msub><mo
     >,</mo><msub
     ><mi
     >f</mi><mn
     >2</mn></msub><mo
    stretchy="false"  >âŸ©</mo></mrow></mrow><mo
     >+</mo><mrow
     ><mi
     >c</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msup
     ><mrow
     ><mo
    stretchy="false"  >â€–</mo><msub
     ><mi
     >f</mi><mn
     >2</mn></msub><mo
    stretchy="false"  >â€–</mo></mrow><mn
     >2</mn></msup></mrow></mrow><mrow
     ><mn 
    >1</mn><mo 
    >+</mo><mrow 
    ><mn 
    >2</mn><mo lspace="0em" rspace="0em"
     >â€‹</mo><mi
     >c</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >âŸ¨</mo><msub
     ><mi
     >f</mi><mn
     >1</mn></msub><mo
     >,</mo><msub
     ><mi
     >f</mi><mn
     >2</mn></msub><mo
    stretchy="false"  >âŸ©</mo></mrow></mrow><mo
     >+</mo><mrow
     ><msup
     ><mi
     >c</mi><mn
     >2</mn></msup><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msup
     ><mrow
     ><mo
    stretchy="false"  >â€–</mo><msub
     ><mi
     >f</mi><mn
     >1</mn></msub><mo
    stretchy="false"  >â€–</mo></mrow><mn
     >2</mn></msup><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msup
     ><mrow
     ><mo
    stretchy="false"  >â€–</mo><msub
     ><mi
     >f</mi><mn
     >2</mn></msub><mo
    stretchy="false"  >â€–</mo></mrow><mn
     >2</mn></msup></mrow></mrow></mfrac><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd
     columnalign="right" ><mrow ><mi
     >Î²</mi><mo 
    >=</mo></mrow></mtd><mtd 
    columnalign="left" ><mrow ><mfrac
     ><mrow 
    ><mn  >1</mn><mo
     >+</mo><mrow
     ><mi
     >c</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msup
     ><mrow
     ><mo
    stretchy="false"  >â€–</mo><msub
     ><mi
     >f</mi><mn
     >1</mn></msub><mo
    stretchy="false"  >â€–</mo></mrow><mn
     >2</mn></msup></mrow></mrow><mrow
     ><mn 
    >1</mn><mo 
    >+</mo><mrow 
    ><mn 
    >2</mn><mo lspace="0em" rspace="0em"
     >â€‹</mo><mi
     >c</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >âŸ¨</mo><msub
     ><mi
     >f</mi><mn
     >1</mn></msub><mo
     >,</mo><msub
     ><mi
     >f</mi><mn
     >2</mn></msub><mo
    stretchy="false"  >âŸ©</mo></mrow></mrow><mo
     >+</mo><mrow
     ><msup
     ><mi
     >c</mi><mn
     >2</mn></msup><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msup
     ><mrow
     ><mo
    stretchy="false"  >â€–</mo><msub
     ><mi
     >f</mi><mn
     >1</mn></msub><mo
    stretchy="false"  >â€–</mo></mrow><mn
     >2</mn></msup><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msup
     ><mrow
     ><mo
    stretchy="false"  >â€–</mo><msub
     ><mi
     >f</mi><mn
     >2</mn></msub><mo
    stretchy="false"  >â€–</mo></mrow><mn
     >2</mn></msup></mrow></mrow></mfrac><mo
    lspace="0em" >.</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol
    cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><csymbol
    cd="latexml"  >direct-sum</csymbol><ci
     >ğ‘</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >ğ‘“</ci><cn type="integer"
     >1</cn></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci
     >ğ‘“</ci><cn type="integer"
     >2</cn></apply></apply><apply
    ><apply ><ci
     >ğ›¼</ci><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >ğ‘“</ci><cn type="integer"
     >1</cn></apply></apply><apply
    ><ci 
    >ğ›½</ci><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >2</cn></apply></apply></apply></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><ci 
    >ğ›¼</ci><apply 
    ><apply  ><cn
    type="integer"  >1</cn><apply
     ><cn
    type="integer"  >2</cn><ci
     >ğ‘</ci><list
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >1</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >2</cn></apply></list></apply><apply
     ><ci
     >ğ‘</ci><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="latexml"  >norm</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >2</cn></apply></apply><cn
    type="integer"  >2</cn></apply></apply></apply><apply
     ><cn type="integer"
     >1</cn><apply
     ><cn
    type="integer"  >2</cn><ci
     >ğ‘</ci><list
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >1</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >2</cn></apply></list></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >ğ‘</ci><cn
    type="integer"  >2</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="latexml"  >norm</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >1</cn></apply></apply><cn
    type="integer"  >2</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="latexml"  >norm</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >2</cn></apply></apply><cn
    type="integer"  >2</cn></apply></apply></apply></apply></apply><apply
    ><ci 
    >ğ›½</ci><apply 
    ><apply  ><cn
    type="integer"  >1</cn><apply
     ><ci
     >ğ‘</ci><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="latexml"  >norm</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >1</cn></apply></apply><cn
    type="integer"  >2</cn></apply></apply></apply><apply
     ><cn type="integer"
     >1</cn><apply
     ><cn
    type="integer"  >2</cn><ci
     >ğ‘</ci><list
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >1</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >2</cn></apply></list></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >ğ‘</ci><cn
    type="integer"  >2</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="latexml"  >norm</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >1</cn></apply></apply><cn
    type="integer"  >2</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="latexml"  >norm</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğ‘“</ci><cn
    type="integer"  >2</cn></apply></apply><cn
    type="integer"  >2</cn></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}f_{1}\oplus_{c}f_{2}=&\alpha
    f_{1}+\beta f_{2},\\ \alpha=&\frac{1+2c\langle f_{1},f_{2}\rangle+c&#124;&#124;f_{2}&#124;&#124;^{2}}{1+2c\langle
    f_{1},f_{2}\rangle+c^{2}&#124;&#124;f_{1}&#124;&#124;^{2}&#124;&#124;f_{2}&#124;&#124;^{2}},\\
    \beta=&\frac{1+c&#124;&#124;f_{1}&#124;&#124;^{2}}{1+2c\langle f_{1},f_{2}\rangle+c^{2}&#124;&#124;f_{1}&#124;&#124;^{2}&#124;&#124;f_{2}&#124;&#124;^{2}}.\end{split}</annotation></semantics></math>
    |  | (17) |
  prefs: []
  type: TYPE_NORMAL
- en: 'This rewrite reduces the addition to adding two tensors in $\mathbb{R}^{W\times
    H\times|\mathcal{Y}|}$, allowing for per-pixel evaluation on image batches. For
    training, GhadimiÂ Atigh etÂ al ([2022](#bib.bib47)) incorporate hierarchical information
    by replacing the one-hot softmax with a hierarchical softmax:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(\hat{y}=y&#124;g(x)_{ij})=\prod_{h\in\mathcal{H}_{y}}\frac{\exp(\xi_{h}(g(x)_{ij}))}{\sum_{s}\in
    S_{h}\exp(\xi_{s}(g(x)_{ij}))},$ |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'with $\mathcal{H}_{y}=\{y\}\cap\mathcal{A}_{y}$ the set containing $y$ and
    its ancestors and $S_{h}$ the set of siblings of class $h$. Performing per-pixel
    classification with hyperbolic hierarchical logistic regression opens up multiple
    new doors for image segmentation. First, the notion of uncertainty as given by
    the hyperbolic norm of output embeddings generalizes naturally to the pixel level.
    As shown in FigureÂ [3](#S3.F3 "Figure 3 â€£ Hyperbolic logistic regression. â€£ 3.1
    Sample-to-gyroplane learning â€£ 3 Supervised hyperbolic visual learning â€£ Hyperbolic
    Deep Learning in Computer Vision: A Survey"), the norm of pixel embeddings correlates
    with semantic ambiguity; the closer the pixel is to a semantic boundary the lower
    the pixel norm. Chen etÂ al ([2022](#bib.bib19)) have already used this insight
    to improve image segmentation. They outline a hyperbolic uncertainty loss, where
    the cross-entropy loss of a pixel is weighted as follows for pixel $x_{ij}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{uw}(x_{ij})=1+\frac{1}{\log(t+\frac{d_{h}(g(x)_{ij},0)}{d_{h}(g(s),0)})},$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: with $s$ the most confident pixel and $t$ a hyperparameter set to 1.02 in order
    to have a wide weight variation while avoiding division by zero. Adding this weight
    to the cross-entropy pixel loss consistently improves segmentation results for
    well-known segmentation networks. Other benefits of hyperbolic image segmentation
    include better zero-label generalization and higher effectiveness with few embedding
    dimensions compared to Euclidean pixel embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/761973d6fbcc87b744e7d48ff190429a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Hyperbolic image segmentation naturally provides us per-pixel uncertainty
    information. Pixels with low hyperbolic norm constitute pixels with high uncertainty
    and are strongly correlated with closeness to semantic boundaries. Image courtesy
    of GhadimiÂ Atigh etÂ al ([2022](#bib.bib47)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d640193a743fdabfb954d455c90095cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Hierarchical knowledge amongst classes provides a structure for hyperbolic
    embeddings in computer vision approaches, where classes are represented as points
    or prototypes in hyperbolic space according to their hypernym-hyponym relations.
    For example, Dhall etÂ al ([2020](#bib.bib33)) exploit hierarchical relations from
    entomological collections (left), while Yu etÂ al ([2022](#bib.bib127)) utilize
    taxonomies of skin lesion diseases (middle) and Long etÂ al ([2020](#bib.bib82))
    do the same for action hierarchies (right). Images courtesy of the respective
    publications.'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic kernel machines.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next to logistic regression, Cho etÂ al ([2019](#bib.bib24)) provide a general
    formulation for kernel methods in hyperbolic space with large-margin classifiers.
    Fang etÂ al ([2021](#bib.bib38)) introduce positive definite kernel functions in
    hyperbolic space and show its potential for computer vision. Specifically, they
    propose hyperbolic instantiations of tangent kernels, radial basis function kernels,
    (generalized) Laplace kernels, and binomial kernels. The kernels can be plugged
    on top of convolutional networks and trained with cross-entropy to benefit from
    both the representation learning of the convolutional layers and the hyperbolic
    kernel dynamics in the classifier. Deep learning with hyperbolic kernel methods
    improves few-shot learning, person re-identification, and knowledge distillation.
    Zero-shot learning is even enabled through kernel distances between visual embeddings
    and semantic class representations.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Sample-to-prototype learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most popular strategy in hyperbolic learning is to represent classes as
    prototypes, *i.e.,* as points in hyperbolic space. In this research direction,
    there are two solutions: embedding classes based on their sample mean, in the
    spirit of Prototypical Networks (ProtoNet) (Snell etÂ al, [2017](#bib.bib106)),
    or embeddings classes based on a given hierarchy over all classes.'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic ProtoNet.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In Prototypical Networks (Snell etÂ al, [2017](#bib.bib106)), the prototype
    of a class $k$ is determined as the mean vector of the samples belonging to that
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P_{\mathbb{R}}(k)=\frac{1}{&#124;S_{k}&#124;}\sum_{y_{s}\in S_{k}}f_{\theta}(x_{s}),$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: 'with $S_{k}$ the set of samples belonging to class $k$. Inference can in turn
    be performed by assigning the label of the nearest prototype for a test sample.
    Khrulkov etÂ al ([2020](#bib.bib68)) generalize this formulation to Hyperbolic
    Prototypical Networks. Since computing averages in the PoincarÃ© ball model requires
    expensive FrÃ©chet mean calculations, they perform averaging using the Einstein
    midpoint, given in Klein coordinates as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P_{\mathbb{K}}(k)=\sum_{i=1}^{&#124;S_{k}&#124;}\gamma_{i}g_{\mathbb{K}}(x_{i})/\sum_{i=1}^{&#124;S_{k}&#124;}\gamma_{i},$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: 'with $\gamma_{i}$ the Lorentz factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\gamma_{i}=\frac{1}{\sqrt{1-c&#124;&#124;g(x_{i})&#124;&#124;^{2}}}.$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: 'Since Khrulkov etÂ al ([2020](#bib.bib68)) operate in the PoincarÃ© ball model,
    this averaging operation requires transforming embeddings to and from the Klein
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}g_{\mathbb{K}}(x_{i})=&amp;\frac{2g_{\mathbb{D}}(x_{i})}{1+c&#124;&#124;g_{\mathbb{D}}(x_{i})&#124;&#124;^{2}},\\
    g_{\mathbb{D}}(x_{i})=&amp;\frac{g_{\mathbb{K}}(x_{i})}{1+\sqrt{1-c&#124;&#124;g_{\mathbb{K}}(x_{i})&#124;&#124;^{2}}},\end{split}$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: with $g_{\mathbb{D}}(x_{i})$ and $g_{\mathbb{K}}(x_{i})$ the embeddings of input
    $x_{i}$ in respectively the PoincarÃ© ball model and the Klein model. Akin to its
    Euclidean counterpart, Hyperbolic ProtoNet is used to address few-shot learning,
    where the sample mean prototype serves as the class representation. Khrulkov etÂ al
    ([2020](#bib.bib68)) show that performing prototypical few-shot learning in hyperbolic
    space is competitive to Euclidean prototypical learning, even resulting in better
    accuracy scores when relying on a 4-layer ConvNet as the backbone.
  prefs: []
  type: TYPE_NORMAL
- en: As a follow-up work, Gao etÂ al ([2021](#bib.bib42)) show that different tasks
    and even individual classes in few-shot learning favor different curvatures. They
    propose to generate a per-class curvature based on the second-order statistics
    of its in-class and out-of-class sample representations. Using the second-order
    statistics, a multi-layer perceptron with sigmoid activation is learned to fix
    the range of the curvature to $[0,1]$. Given class-specific curvatures, prototypes
    are obtained by constructing an intra-class distance matrix on top of which an
    MLP is trained. The MLP serves as weights for each in-class sample. The procedure
    is repeated for the closest samples in the out-of-class set, after which the per-class
    prototype is given as the weighted hyperbolic average over the in-class and closest
    out-of-class samples. The curvature generation and weighted hyperbolic averaging
    improve few-shot learning in both inductive and transductive settings.
  prefs: []
  type: TYPE_NORMAL
- en: The hyperbolic clipping of Guo etÂ al ([2022](#bib.bib54)) is also effective
    for few-shot learning, consistently outperforming the standard ProtoNet and Hyperbolic
    ProtoNet on the CUB Birds and miniImageNet few-shot benchmarks. A few other works
    have extended Hyperbolic ProtoNet for few-shot learning with set- and grouplet-based
    learning and will be discussed in the sample-to-sample learning section.
  prefs: []
  type: TYPE_NORMAL
- en: Recently,Â Gao etÂ al ([2022](#bib.bib43)) investigate feature augmentation in
    hyperbolic space to solve the overfitting problem when dealing with limited data.
    On top, they introduce a scheme to estimate the feature distribution using neural-ODE.
    These elements are then plugged into few-shot approaches such as the hyperbolic
    prototypical networks of Khrulkov etÂ al ([2020](#bib.bib68)), improving performance.
    Â Choudhary and Reddy ([2022](#bib.bib27)) improve hyperbolic few-shot learning
    by reformulating hyperbolic neural networks through Taylor series expansions of
    hyperbolic trigonometric functions and show that it improves the scalability and
    compatibility, and outperforms Hyperbolic ProtoNet.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical embedding of prototypes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Where Hyperbolic ProtoNets are effective in few-shot settings, a number of
    works have also investigated prototype-based solutions for the general classification.
    As starting point, these works commonly assume that the classes in a dataset are
    organized in a hierarchy, see FigureÂ [4](#S3.F4 "Figure 4 â€£ Hyperbolic logistic
    regression. â€£ 3.1 Sample-to-gyroplane learning â€£ 3 Supervised hyperbolic visual
    learning â€£ Hyperbolic Deep Learning in Computer Vision: A Survey"). Long etÂ al
    ([2020](#bib.bib82)) embed action class hierarchy $\mathcal{H}$ in hyperbolic
    space using hyperbolic entailment cones (Ganea etÂ al, [2018a](#bib.bib40)), with
    an additional loss to increase the angular separation between leaf nodes to avoid
    inter-label confusion amongst class labels $\mathcal{Y}$. With $\mathcal{L}_{H}(\mathcal{H})$
    as the hyperbolic embedding loss for hierarchy $\mathcal{H}$, let $\mathcal{P}$
    denote the leave nodes of the hierarchy. Then the separation-based loss is given
    over the leaf nodes as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{S}(\mathcal{P})=\mathbf{1}^{T}(\hat{P}\hat{P}^{T}-I)\mathbf{1},$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: with $\hat{P}$ the $\ell_{2}$-normalized representations of the leaf nodes.
    By combining the hierarchical and separation based losses, the hierarchy is embedded
    to balance both hierarchical constraints and discriminative abilities. The embedding
    is learned *a priori*, after which video embeddings are projected to the same
    hyperbolic space and optimized to their correct class embedding. This approach
    improves action recognition, zero-shot action classification, and hierarchical
    action search. In a similar spirit, Dhall etÂ al ([2020](#bib.bib33)) show that
    using hyperbolic entailment cones for image classification is empirically better
    than using Euclidean entailment cones. Rather than separating hierarchical and
    visual embedding learning, Yu etÂ al ([2022](#bib.bib127)) propose to simultaneously
    learn hierarchical and visual representations for skin lesion recognition in images.
    Image embeddings are optimized towards their correct class prototype, while the
    classes are optimized to abide by their hyperbolic entailment cones with an extra
    distortion loss to obtain better hierarchical embeddings. Gulshad etÂ al ([2023](#bib.bib52))
    propose Hierarchical Prototype Explainer, a reasoning model in hyperbolic space
    to provide explainability in video action recognition. Their approach learns hierarchical
    prototypes at different levels of granularityÂ *e.g.,* parent and grandparent levels,
    to explain the recognized action in the video. By learning the hierarchical prototypes,
    they can provide explanations on different levels of granularity, including interpretation
    of the prediction of a specific class label and providing information on the spatiotemporal
    parts that contribute to the final prediction. Â Li etÂ al ([2023](#bib.bib78))
    investigate the semantic space of action recognition datasets and bridge the gap
    between different labeling systems. To achieve a unified action learning, actions
    are connected into a hierarchy using VerbNetÂ (Schuler, [2005](#bib.bib104)) and
    embedded as prototypes in hyperbolic space.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical prototype embeddings have also been successfully employed in the
    zero-shot domain. Liu etÂ al ([2020](#bib.bib81)) show how to perform zero-shot
    learning with hyperbolic embeddings. Classes are embedded by taking their WordNet-based
    PoincarÃ© Embeddings (Nickel and Kiela, [2017](#bib.bib93)) and text-based PoincarÃ©
    GloVe embeddings (Tifrea etÂ al, [2019](#bib.bib110)). Both are concatenated to
    obtain class prototypes. By optimizing seen training images to their prototypes,
    it becomes possible to generalize to unseen classes during testing through a nearest
    neighbor search in the concatenated hyperbolic space. Xu etÂ al ([2022](#bib.bib122))
    also perform hyperbolic zero-shot learning by training hyperbolic graph layers
    (Chami etÂ al, [2019](#bib.bib16)) on top of hyperbolic word embeddings. Dengxiong
    and Kong ([2023](#bib.bib31)) show the potential of hyperbolic space in generalized
    open set recognition, which classifies unknown samples based on side information.
    A side information (taxonomy) learning framework is introduced to embed the information
    in hyperbolic space with low distortion and identify the unknown samples. Moreover,
    an ancestor search algorithm is outlined to find the most similar ancestor in
    the taxonomy of the known classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For standard classification, GhadimiÂ Atigh etÂ al ([2021](#bib.bib46)) show
    how to integrate uniformity amongst prototypes in hyperbolic space by embedding
    classes with maximum separation on the boundary of the PoincarÃ© ball given by
    (Mettes etÂ al, [2019](#bib.bib87); Kasarla etÂ al, [2022](#bib.bib65)). With prototypes
    now at the boundary of the ball, standard distance functions no longer apply since
    they are at the infinite distance to any point within the ball. To that end, they
    propose to use the Busemann distance, which is given for hyperbolic image embedding
    $g(x)$ and prototype $p$ as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $b_{p}(g(x))=\log(\frac{&#124;&#124;p-g(x)&#124;&#124;^{2}}{1-&#124;&#124;g(x)&#124;&#124;^{2}}).$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: By fixing prototypes with maximum separation *a priori* and minimizing this
    distance function with an extra regularization towards the origin, it becomes
    possible to perform hyperbolic prototypical learning with prototypes at the ideal
    boundary. GhadimiÂ Atigh etÂ al ([2021](#bib.bib46)) show that such an approach
    has direct links with conventional logistic regression in the binary case, highlighting
    its inherent properties. Moreover, maximally separated prototypes can also be
    replaced by prototypes from word embeddings or hierarchical knowledge, depending
    on the available knowledge and task at hand. In addition to standard classification,
    hierarchical hyperbolic embeddings have demonstrated effectiveness in continual
    learningÂ (Gao etÂ al, [2023](#bib.bib44)). To learn the new data,Â Gao etÂ al ([2023](#bib.bib44))
    propose a dynamically expanding geometry through a mixed-curvature space, enabling
    learning of complex hierarchies in a data stream. To prevent forgetting, angle-regularization
    and neighbor-robustness losses are used to preserve the geometry of the old data.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot learning has also been investigated with hierarchical knowledge. Zhang
    etÂ al ([2022](#bib.bib129)) perform such few-shot learning by first training a
    network on a joint classification and hierarchical consistency objective. The
    classification is given as a softmax over the class probabilities, as well as
    the softmax over the superclasses. In the few-shot inference stage, class prototypes
    are obtained through hyperbolic graph propagation to deal with the limited sample
    setting, improving few-shot learning as a result.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e4b8c7cd41ea5372c79ed977400e6e79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Embeddings of hyperbolic vision transformers cluster samples based
    on their label towards the boundary of the PoincarÃ© ball, while simultaneously
    exhibiting latent hierarchical relations. Image courtesy of Ermolov etÂ al ([2022](#bib.bib37)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Sample-to-sample learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lastly, a number of recent works have investigated hyperbolic learning by contrasting
    between samples.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic metric learning.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Ermolov etÂ al ([2022](#bib.bib37)) investigate the potential of hyperbolic
    embedding for metric learning. In metric learning, the de facto solution is to
    match representations of sample pairs based on embeddings given by a pre-trained
    encoder. Rather than relying on Euclidean distances and contrastive learning for
    optimization, they propose a hyperbolic pairwise cross-entropy loss. Given a dataset
    with $|\mathcal{Y}|$ classes, each batch samples two samples from each category,
    *i.e.,* $K=2\cdot|\mathcal{Y}|$. Then the loss function for a positive pair with
    the same class label is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\ell_{ij}=-\log\frac{\exp(-D(g(x_{i}),g(x_{j}))/\tau)}{\sum_{k=1}^{K}\exp(-D(g(x_{i}),g(x_{k}))/\tau)},$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: 'where $D(\cdot,\cdot)$ can be either a hyperbolic or a cosine distance and
    $\tau$ denotes a temperature hyperparameter. This loss is computed over all positive
    pairs $(i,j)$ and $(j,i)$ in a batch. Using supervised (Dosovitskiy etÂ al, [2021](#bib.bib36))
    and self-supervised (Caron etÂ al, [2021](#bib.bib15)) vision transformers as encoders,
    hyperbolic metric learning consistently outperforms Euclidean alternatives and
    sets state-of-the-art on fine-grained datasets. Figure [5](#S3.F5 "Figure 5 â€£
    Hierarchical embedding of prototypes. â€£ 3.2 Sample-to-prototype learning â€£ 3 Supervised
    hyperbolic visual learning â€£ Hyperbolic Deep Learning in Computer Vision: A Survey")
    shows a 2D projection of the embeddings learned with hyperbolic metric learning
    on vision transformers, where classes are grouped towards the boundary and latent
    hierarchical neighborhood relations emerge.'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic metric learning has shown to be effective to overcome overfitting
    and catastrophic forgetting in few-shot class-incremental learning tasks, explored
    by Â Cui etÂ al ([2022](#bib.bib28)). This is done by adding a metric learning loss
    as a part of the distillation in continual learning. They also propose a hyperbolic
    version of Reciprocal Point LearningÂ (Chen etÂ al, [2020a](#bib.bib20)) to provide
    extra-class space for known categories in the few-shot learning stage.Â Yan etÂ al
    ([2023](#bib.bib124)) also explore hyperbolic metric learning, incorporating noise-insensitive
    and adaptive hierarchical similarity to handle noisy labels and multi-level relations.
    Kim etÂ al ([2022](#bib.bib70)) add a hierarchical regularization term on top of
    the metric learning approaches, with the goal of learning hierarchical ancestors
    in hyperbolic space without any annotation. Hyperbolic metric learning is furthermore
    effective in semantic hashingÂ (Amin etÂ al, [2022](#bib.bib2)), face recognition
    via large-margin nearest-neighbor learning (Trpin and Boshkoska, [2022](#bib.bib112)),
    and multi-modal alignment given videos and knowledge graphÂ (Guo etÂ al, [2021](#bib.bib53)).
  prefs: []
  type: TYPE_NORMAL
- en: Following the progress of large language models and the success of vision-language
    models (*e.g.,* CLIPÂ (Radford etÂ al, [2021](#bib.bib98))) in multimodal representation
    learning, Desai etÂ al ([2023](#bib.bib32)) propose a hyperbolic image-text representation.
    The proposed method first processes the input image and text using two separate
    encoders. Then, the generated embedding is projected into the hyperbolic space,
    and training is performed using a contrastive and entailment loss. The paper shows
    that the proposed approach outperforms the Euclidean CLIP as it is capable of
    capturing hierarchical multimodal relations in hyperbolic space.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic set-based learning.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Where sample-to-prototype and sample-to-sample approaches compare samples to
    individual elements, some works have shown that set-based and group-based distances
    are more effective and robust. Ma etÂ al ([2022](#bib.bib84)) introduce an adaptive
    sample-to-set distance function in the context of few-shot learning. Rather than
    aggregating support samples to a single prototype, an adaptive sample-to-set approach
    is proposed to increase the robustness to the outliers. The sample-to-set function
    is a weighted average of the distance from the query to all support samples, where
    the distance is calculated with a small network over the feature maps of the query
    and support samples. This approach benefits few-shot learning, especially when
    dealing with outliers.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of metric learning, Zhang etÂ al ([2021a](#bib.bib131)) argue
    that sample-to-sample learning is computationally expensive, while sample-to-prototype
    learning is less accurate. They propose a hybrid strategy based on grouplets.
    Each grouplet is a random subset of samples and the set of grouplets is matched
    with prototypes through a differentiable optimal transport. Akin to Ermolov etÂ al
    ([2022](#bib.bib37)), they show that using hyperbolic embedding spaces improved
    metric learning on fine-grained datasets. Moreover, they provide empirical evidence
    that other metric-based losses benefit from hyperbolic embeddings, highlighting
    the general utility of hyperbolic space for metric learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0280a39b6a7b361173303daf245bda95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The three major methods for unsupervised hyperbolic learning in computer
    vision. Current literature performs unsupervised learning in hyperbolic space
    using (i) generative models, (ii) clustering, (iii) self-supervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 4 Unsupervised hyperbolic visual learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hyperbolic learning has been actively researched in the unsupervised domain
    of computer vision. We identify three dominant research directions in which hyperbolic
    deep learning has found success: generative learning, clustering, and self-supervised
    learning. Below, each is discussed separately.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Generative approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 Hyperbolic VAEs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Variational autoencoders (VAEs) (Kingma and Welling, [2013](#bib.bib71); Rezende
    etÂ al, [2014](#bib.bib100)) with hyperbolic latent space have been used to learn
    representations of images. Nagano etÂ al ([2019](#bib.bib92)) propose the hyperbolic
    wrapped normal distribution and derive algorithms for both reparametrizable sampling
    and computing the probability density function. They then derive a hyperbolic
    $\beta$-VAE (Higgins etÂ al, [2017](#bib.bib59)) using the wrapped normal function
    as the prior and posterior, replacing the usual (Euclidean) Gaussian distribution.
    The wrapped normal distribution in a manifold $\mathcal{M}$ is the pushforward
    measure under the exponential map $\exp_{\mathcal{M}}$. Thus, a sample $z$ can
    be obtained as (Mathieu etÂ al, [2019](#bib.bib85)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle z=\exp_{\mu}^{\mathcal{M}}\left(G(\mu)^{-1/2}v\right),v\sim\mathcal{N}(\cdot&#124;0,\Sigma)$
    |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: where $\exp_{\mu}^{\mathcal{M}}$ is the exponential map of $\mathcal{M}$ at
    $\mu$ and $G$ is the matrix representation of the metric of $\mathcal{M}$. To
    accommodate the geometry of the latent space, exponential and logarithmic maps
    were added at the end of the VAE encoder and before the start of the VAE decoder,
    respectively. In order to train their hyperbolic VAE with the typical evidence
    lower bound, Nagano etÂ al ([2019](#bib.bib92)) compute the density of the wrapped
    normal distribution using the change-of-variables formula. Since their sampling
    algorithm required the exponential and parallel transport maps, Nagano etÂ al ([2019](#bib.bib92))
    compute the log-determinants and inverses of these maps in order to apply the
    change-of-variables formula. Nagano etÂ al ([2019](#bib.bib92)) then use their
    VAE to learn representations of MNIST and Atari 2600 Breakout screens. On MNIST,
    Hyperbolic representations outperform Euclidean representations at low latent
    dimensions but were overtaken starting at dimension 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathieu etÂ al ([2019](#bib.bib85)) extend the work of Nagano etÂ al ([2019](#bib.bib92))
    by introducing the Riemannian normal distribution and deriving reparametrizable
    sampling schemes for both the Riemannian normal and wrapped normal using hyperbolic
    polar coordinates. The Riemannian normal views the Euclidean normal distribution
    as the distribution minimizing the entropy for a given mean and standard deviation
    and defines a new normal distribution on hyperbolic space with this property:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{N}_{\mathcal{M}}^{R}(z&#124;\mu,\sigma^{2})=\frac{1}{Z^{R}}\exp\left(-\frac{d_{\mathcal{M}}(\mu,z)^{2}}{2\sigma^{2}}\right)$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: where $Z^{R}$ is a normalizing constant. Mathieu etÂ al ([2019](#bib.bib85))
    additionally introduce the use of a gyroplane layer as the first layer of the
    decoder, following Ganea etÂ al ([2018b](#bib.bib41)). Noting that an Euclidean
    affine transform can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{a,p}(z)=\text{sign}(\langle a,z-p\rangle)&#124;&#124;a&#124;&#124;d_{E}(z,H_{a,p})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $H_{a,p}=\{z\in\mathbb{R}^{n}|\langle a,z-p\rangle=0\}$ is the decision
    hyperplane, they replace each piece of the formula with its hyperbolic counterpart
    to obtain
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f_{a,p}^{c}(z)=\text{sign}(\langle a,\log_{p}^{c}(z)\rangle_{p})&#124;&#124;a&#124;&#124;_{p}d_{p}^{c}(z,H_{a,p}^{c})$
    |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: where all $H_{a,p}^{c}=\{z\in\mathbb{H}|\langle a,\log_{p}^{c}(z)\rangle=0\}$.
    The closed-form formula for the distance term in the PoincarÃ© ball is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle d_{p}^{c}(z,H_{a,p}^{c})=\frac{1}{\sqrt{c}}\sinh^{-1}\left(\frac{2\sqrt{c}&#124;\langle-p\oplus_{c}z,a\rangle&#124;}{(1-c&#124;&#124;-p\oplus_{c}z&#124;&#124;^{2})&#124;&#124;a&#124;&#124;}\right)$
    |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: Mathieu etÂ al ([2019](#bib.bib85)) also use their hyperbolic VAE to learn representations
    of MNIST and find that using both the Riemannian normal and the gyroplane layer
    improve test log-likelihoods, especially at low latent dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9bd7b4a97d0126c214a8aa8ed210346a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The standard hyperbolic wrapped normal (top) and rotated hyperbolic
    wrapped normal (bottom). In (a), the principal axes of the normal distribution
    are illustrated. In (b), the principal axes of the transported normal distribution
    are visualized. The density of the two distributions are visualized in (c). Image
    courtesy of Cho etÂ al ([2022](#bib.bib25)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cho etÂ al ([2022](#bib.bib25)) extend the previous two works by proposing a
    new version of the hyperbolic wrapped normal distribution (HWN). Their primary
    observation is that for the wrapped normal distribution, the principal axes of
    the distributions are not aligned with the local standard axes, see Figure [7](#S4.F7
    "Figure 7 â€£ 4.1.1 Hyperbolic VAEs â€£ 4.1 Generative approaches â€£ 4 Unsupervised
    hyperbolic visual learning â€£ Hyperbolic Deep Learning in Computer Vision: A Survey").
    They propose a new sampling process that fixes the alignment of the principal
    axes, resulting in a new distribution which they call the rotated hyperbolic wrapped
    normal (RoWN). Given a mean $\mu$ in the Lorentz model of hyperbolic geometry
    and a diagonal covariance matrix $\Sigma$, samples from the RoWN distribution
    are sampled as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the rotation matrix $R$ that rotates the $x$-axis $x=\left([\pm 1,\ldots,0]\right)$
    to $y=\mu_{1:}$. We can compute $R$ as
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle R=I+(y^{T}x-x^{T}y)+\frac{(y^{T}x-x^{T}y)^{2}}{1+\langle
    x,y\rangle}$ |  | (31) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rotate $\Sigma$ by $R$: $\hat{\Sigma}=R\Sigma R^{T}$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now sample as in the usual hyperbolic wrapped normal: sample $v\sim\mathcal{N}(\mathbf{0},\hat{\Sigma})$
    and then map it to hyperbolic space as follows: $\exp_{\mu}(\text{PT}_{\mathbf{0}\to\mu}([0,v]))$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Cho etÂ al ([2022](#bib.bib25)) find that RoWN outperforms HWN in a variety of
    settings, such as the Atari 2600 Breakout image generation experiment first examined
    in Nagano etÂ al ([2019](#bib.bib92)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Hyperbolic GANs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using the intuition that images are organized hierarchically, several works
    have proposed hyperbolic generative adversarial networks (GANs). Lazcano etÂ al
    ([2021](#bib.bib74)) propose a hyperbolic GAN which replaces some of the Euclidean
    layers in both the generator and discriminator with hyperbolic layers (Ganea etÂ al,
    [2018a](#bib.bib40)) with learnable curvature. Lazcano etÂ al ([2021](#bib.bib74))
    propose hyperbolic variants of the original GAN (Goodfellow etÂ al, [2020](#bib.bib48)),
    the Wasserstein GAN WGAN-GP (Gulrajani etÂ al, [2017](#bib.bib51)) and conditional
    GAN CGAN (Mirza and Osindero, [2014](#bib.bib89)). The paper finds that their
    best configurations of Euclidean and hyperbolic layers generally improved the
    Inception Score (Salimans etÂ al, [2016](#bib.bib103)) and Frechet Inception Distance
    (Heusel etÂ al, [2017](#bib.bib58)) on MNIST image generation, with the best improvements
    in the GAN architecture. The best learned curvatures are close to zero. Unlike
    other hyperbolic generative models (VAEs and normalizing flows), good results
    are observed at large latent dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Qu and Zou ([2022](#bib.bib97)) propose HAEGAN, a hyperbolic autoencoder and
    GAN framework in the Lorentz model $\mathbb{L}$ (also known as the hyperboloid
    model), of hyperbolic geometry. The GAN is based on the structure of WGAN-GP (Arjovsky
    etÂ al, [2017](#bib.bib5); Gulrajani etÂ al, [2017](#bib.bib51)). The structure
    of HAEGAN consists of an encoder, which takes in real data and generates real
    representations, and a generator, which takes in noise and generates fake representations.
    A critic is trained to distinguish between the two representations, and a decoder
    takes the fake representations and produces the final generated object. Qu and
    Zou ([2022](#bib.bib97)) generalize WGAN-GP to hyperbolic space using three operations:
    the first is the hyperbolic linear layer is $\texttt{HLinear}_{n,m}:\mathbb{L}_{K}^{n}\to\mathbb{L}_{K}^{m}$
    of Chen etÂ al ([2021](#bib.bib23)) , the second the hyperbolic centroid distance
    layer $\texttt{HCDist}_{n,m}(x):\mathbb{L}_{K}^{n}\to\mathbb{R}^{m}$ of Liu etÂ al
    ([2019](#bib.bib80)), and the third a a new Lorentz concatenation layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\texttt{HCat}(\{x_{i}\}_{i=1}^{N})=\left[\sqrt{\sum_{i=1}^{N}x_{i_{t}}^{2}+(N-1)/K},x_{1_{s}}^{\top},\ldots,x_{1_{s}}^{\top}\right]^{\top}$
    |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: Compared to previous work Shimizu etÂ al ([2021](#bib.bib105)), the HCat layer
    has the advantage of always having bounded gradients. (Shimizu etÂ al, [2021](#bib.bib105)).
    Compared to Lazcano etÂ al ([2021](#bib.bib74)), HAEGAN shows improved results
    on MNIST image generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eadc8b024e5f621fe30751441ae83c89.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Hierarchical attribute editing in hyperbolic space is possible due
    to hyperbolic spaceâ€™s ability to encode semantic hierarchical structure within
    image data. Changing the high-level, category-relevant details (closest to the
    origin) changes the category, while changing low-level (farthest from the origin),
    category-irrelevant attributes varies images within categories. Image courtesy
    of Li etÂ al ([2022](#bib.bib77)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Li etÂ al ([2022](#bib.bib77)) propose a hyperbolic method for few-shot image
    generation. The main idea is that hyperbolic space encodes a semantic hierarchy,
    where the root of the hierarchy (*i.e.,* at the center of hyperbolic space) is
    a category, *e.g.,* dog. At lower levels, we have more fine-grained separations,
    such as subcategories, *e.g.,* Shih-Tzu and Ridgeback dogs. Finally, at the lowest
    level, there are category-irrelevant features, *e.g.,* the hair color or pose
    of the dog (see Figure [8](#S4.F8 "Figure 8 â€£ 4.1.2 Hyperbolic GANs â€£ 4.1 Generative
    approaches â€£ 4 Unsupervised hyperbolic visual learning â€£ Hyperbolic Deep Learning
    in Computer Vision: A Survey")). This method builds on the Euclidean pSp method
    (Richardson etÂ al, [2021](#bib.bib101)) for image-to-image translation. The pSp
    method uses a feature pyramid to extract feature maps and uses a set of projection
    heads on these feature maps to produce each of the style vectors required by StyleGAN
    (Karras etÂ al, [2019](#bib.bib63), [2020](#bib.bib64)), which is commonly denoted
    the $\mathcal{W}^{+}$-space. Image-to-image translation can then be done by editing
    or replacing style vectors. Li etÂ al ([2022](#bib.bib77)) generalize to hyperbolic
    space by mapping the output of a frozen, pre-trained pSp encoder to hyperbolic
    space and then back to the $\mathcal{W}^{+}$-space of style vectors, and then
    feeding the style vectors into a frozen, pre-trained StyleGAN. Projection to hyperbolic
    space is done using the Mobius layer $f^{\otimes c}$ of Ganea etÂ al ([2018b](#bib.bib41)),
    with the full projection layer having the form'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle z_{\mathbb{D}i}=f^{\otimes c}(\exp_{0}^{c}(\texttt{MLP}_{E}(\mathbf{w}_{i})))$
    |  | (33) |'
  prefs: []
  type: TYPE_TB
- en: with mapping back to the $\mathcal{W}^{+}$-space achieved by a logarithmic map
    plus an MLP. Li etÂ al ([2022](#bib.bib77)) supervise the hyperbolic latent space
    with a hyperbolic classification loss based on the multinomial logistic regression
    formulation of Ganea etÂ al ([2018b](#bib.bib41)). After calculating the probabilities,
    the loss function is just negative log-likelihood as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\mathrm{hyper}}=-\frac{1}{N}\sum_{i=1}^{N}\log(p_{n})$
    |  | (34) |'
  prefs: []
  type: TYPE_TB
- en: 'The full loss function is the pSp loss function plus this term, excluding a
    specific facial reconstruction loss used by the pSp method, since Li etÂ al ([2022](#bib.bib77))
    do not focus on face generation. Li etÂ al ([2022](#bib.bib77)) perform image generation
    as follows: given an image $x_{i}$, the image is embedded in hyperbolic space
    with representation $g_{\mathbb{D}}(x_{i})$ and is rescaled to the desired radius
    (*i.e.,* fine-grained-ness) $r$. A random vector is then sampled from the seen
    categories and a point is taken on the geodesic between the two points. Li etÂ al
    ([2022](#bib.bib77)) find that their method is competitive with state-of-the-art
    methods and show promise for image-to-image transfer.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Hyperbolic Normalizing Flows
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b5a00363eeb1c6dda399e772a22823a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The left figure shows the partitioning step of wrapped hyperbolic
    coupling, and the right figure shows how the vector is transformed, transported,
    and projected back to hyperbolic space. Image courtesy of Bose etÂ al ([2020](#bib.bib12)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bose etÂ al ([2020](#bib.bib12)) propose a hyperbolic normalizing flow that
    generalizes the Euclidean normalizing flow RealNVP (Dinh etÂ al, [2016](#bib.bib35))
    to hyperbolic space. They propose two types of hyperbolic normalizing flows: the
    first, which they call tangent coupling, which carries out the coupling layer
    of RealNVP in the tangent space at the hyperbolic origin $o$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\tilde{f}^{\mathcal{T}C}(\tilde{x})=\begin{cases}\tilde{z}_{1}=\tilde{x}_{1}\\
    \tilde{z}_{2}=\tilde{x}_{2}\odot\sigma(s(\tilde{x}_{1}))+t(\tilde{x}_{1})\end{cases}$
    |  | (35) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle f^{\mathcal{T}C}(x)=\exp_{o}^{K}(\tilde{f}^{\mathcal{T}C}(\log_{o}^{K}(x)))$
    |  | (36) |'
  prefs: []
  type: TYPE_TB
- en: where $s,t$ are neural networks and $\sigma$ is a pointwise non-linearity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The wrapped hyperboloid extends tangent coupling by using parallel transport
    to map intermediate vectors from the tangent space of the origin to the tangent
    space of another point in hyperbolic space (see Figure [9](#S4.F9 "Figure 9 â€£
    4.1.3 Hyperbolic Normalizing Flows â€£ 4.1 Generative approaches â€£ 4 Unsupervised
    hyperbolic visual learning â€£ Hyperbolic Deep Learning in Computer Vision: A Survey")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\tilde{f}^{\mathcal{W}\mathbb{H}C}(\tilde{x})$ | $\displaystyle=\begin{cases}\tilde{z}_{1}=\tilde{x}_{1}\\
    \tilde{z}_{2}=\log_{o}^{K}\left(\exp_{t(\tilde{x}_{1})}^{K}\left(\mathrm{PT}_{o\to
    t(\tilde{x}_{1})}(v)\right)\right)\end{cases}$ |  | (37) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle v$ | $\displaystyle=\tilde{x}_{2}\odot\sigma(s(\tilde{x}_{1}))$
    |  | (38) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle f^{\mathcal{W}\mathbb{H}C}(x)$ | $\displaystyle=\exp_{o}^{K}(\tilde{f}^{\mathcal{W}\mathbb{H}C}(\log_{o}^{K}(x)))$
    |  | (39) |'
  prefs: []
  type: TYPE_TB
- en: 'Compared to tangent coupling, wrapped hyperbolic coupling allows the flow to
    leverage different parts of the manifold instead of just the origin. The paper
    also derives the inverse and Jacobian determinants of the two flows. As is the
    case for hyperbolic VAEs, Bose etÂ al ([2020](#bib.bib12)) also benchmark on MNIST,
    and find a similar trend as Nagano etÂ al ([2019](#bib.bib92)): the performance
    of hyperbolic models exceed that of the equivalent Euclidean model at low dimension,
    but as early as latent dimension 6 Euclidean models overtake hyperbolic models
    in performance. Bose etÂ al ([2020](#bib.bib12)) find that hyperbolic normalizing
    flows outperform hyperbolic VAEs at these low latent dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the close relationship between hyperbolic space, hierarchies, and trees,
    several works have explored hierarchical clustering using hyperbolic space. Monath
    etÂ al ([2019](#bib.bib90)) propose to perform hierarchical clustering using hyperbolic
    representations. Given a dataset $\mathcal{D}=\{x_{i}\}_{i=1}^{N}$, Monath etÂ al
    ([2019](#bib.bib90)) require a hyperbolic representation at the edge of the PoincarÃ©
    disk $\mathbb{D}^{d}$ for each data point $x_{i}\in\mathcal{D}$, which becomes
    the leaves of the hierarchical clustering. The method of Monath etÂ al ([2019](#bib.bib90))
    creates a hierarchical clustering by optimizing the hyperbolic representations
    for a fixed number of internal nodes. Parent-children dissimilarity between a
    child representation $z_{c}$ and a parent representation $z_{p}$ is measured by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle d_{cp}(z_{c},z_{p})=d_{\mathbb{D}}(z_{c},z_{p})(1+\max\{&#124;&#124;z_{p}&#124;&#124;_{\mathbb{D}}-&#124;&#124;z_{c}&#124;&#124;_{\mathbb{D}},0\})$
    |  | (40) |'
  prefs: []
  type: TYPE_TB
- en: 'which encourages children to have larger norms than their parents. A discrete
    tree can then be extracted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\texttt{Parent}(z_{c})=\operatorname*{arg\,min}_{&#124;&#124;z_{p}&#124;&#124;<&#124;&#124;z_{c}&#124;&#124;}d_{cp}(z_{c},z_{p})$
    |  | (41) |'
  prefs: []
  type: TYPE_TB
- en: 'The internal node observations are supervised by two losses: first, a hierarchical
    clustering loss based on Dasguptaâ€™s cost (Dasgupta, [2016](#bib.bib30)) and a
    continuous extension due to Wang and Wang ([2018](#bib.bib117)) that reformulates
    the loss in terms of last common ancestors (LCAs), and second, a parent-child
    margin objective that encourages parent nodes to have smaller norm than their
    children.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose $\mathcal{D}$ has pairwise similarities $\{w_{ij}\}_{i,j\in[N]}$. A
    hierarchical clustering of $\mathcal{D}$ is a rooted tree $T$ such that each leaf
    is a data point. For leaves $i,j\in T$, denote their LCA by $i\vee j$, the subtree
    rooted at $i\vee j$ by $T[i\vee j]$, and the leaves of $T[i\vee j]$ by $\texttt{leaves}(T[i\vee
    j])$. Finally, let relation $\{i,j|k\}$ holds if $i\vee j$ is a descendant of
    $i\vee j\vee k$. Then Dasguptaâ€™s cost can be formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle C_{\mathrm{Dasgupta}}(T;w)=\sum_{ij}w_{ij}&#124;\texttt{leaves}(T[i\vee
    j])&#124;$ |  | (42) |'
  prefs: []
  type: TYPE_TB
- en: Wang and Wang ([2018](#bib.bib117)) show that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle C_{\mathrm{Dasgupta}}(T;w)$ | $\displaystyle=\sum_{ijk}[w_{ij}+w_{ik}+w_{jk}-w_{ijk}(T;w)]$
    |  | (43) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+2\sum_{ij}w_{ij}$ |  | (44) |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle w_{ijk}(T;w)=w_{ij}\mathbbm{1}[\{i,j&#124;k\}]+w_{ik}\mathbbm{1}[\{i,k&#124;j\}]+w_{jk}\mathbbm{1}[\{j,k&#124;i\}]$
    |  | (45) |'
  prefs: []
  type: TYPE_TB
- en: The margin parent-child dissimilarity is given as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle d_{cp}(z_{c},z_{p};\gamma)=d_{\mathbb{D}}(z_{c},z_{p})(1+\max\{&#124;&#124;z_{p}&#124;&#124;_{\mathbb{D}}-&#124;&#124;z_{c}&#124;&#124;_{\mathbb{D}}+\gamma,0\})$
    |  | (46) |'
  prefs: []
  type: TYPE_TB
- en: and the total margin objective is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{cp}=\sum_{z_{c}}d_{cp}(z_{c},\texttt{Parent}(z_{c});\gamma)$
    |  | (47) |'
  prefs: []
  type: TYPE_TB
- en: The embedding is alternately optimized between the clustering objective and
    the parent-child objective. Optimization of the hyperbolic parameters is done
    via the method of Nickel and Kiela ([2017](#bib.bib93)). Using this method, Monath
    etÂ al ([2019](#bib.bib90)) are able to embed ImageNet using representations taken
    from the last layer of a pre-trained Inception neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Monath etÂ al ([2019](#bib.bib90)), Chami etÂ al ([2020a](#bib.bib17))
    base their method on Dasguptaâ€™s cost (Equation [42](#S4.E42 "In 4.2 Clustering
    â€£ 4 Unsupervised hyperbolic visual learning â€£ Hyperbolic Deep Learning in Computer
    Vision: A Survey")) and Wang and Wangâ€™s (Equation [43](#S4.E43 "In 4.2 Clustering
    â€£ 4 Unsupervised hyperbolic visual learning â€£ Hyperbolic Deep Learning in Computer
    Vision: A Survey")) reformulation in terms of LCAs. Chami etÂ al ([2020a](#bib.bib17))
    define the LCA of two points in hyperbolic space to be the point on the geodesic
    connecting the two points that are closest to the hyperbolic origin, and provide
    a formula to calculate this point in the PoincarÃ© disk $\mathbb{D}$. This formula
    allows Equation [43](#S4.E43 "In 4.2 Clustering â€£ 4 Unsupervised hyperbolic visual
    learning â€£ Hyperbolic Deep Learning in Computer Vision: A Survey") to be directly
    optimized by replacing the $w_{ijk}(T;w)$ terms with its continuous counterpart.
    A hierarchical clustering tree can then be produced by iteratively merging the
    most similar pairs, where similarity is measured by their hyperbolic LCA distance
    from the origin. Unlike the method of Monath etÂ al ([2019](#bib.bib90)), Chami
    etÂ al ([2020a](#bib.bib17)) do not require hyperbolic embeddings to be available,
    and optimize the hyperbolic embeddings of the whole tree, not just the leaves.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lin etÂ al ([2022](#bib.bib79)) propose a neural-network based framework for
    the hierarchical clustering of multi-view data. The framework consists of two
    steps: first, improving representation quality via reconstruction loss, contrastive
    learning between different views, and a weighted triplet loss between positive
    examples and mined hard negative examples, and second, applying the hyperbolic
    hierarchical clustering framework of Chami etÂ al ([2020a](#bib.bib17)).'
  prefs: []
  type: TYPE_NORMAL
- en: The contrastive loss in Lin etÂ al ([2022](#bib.bib79)) is the usual contrastive
    loss (see following section) where positive examples are views from the same object
    and negative examples are views from different objects. The weighted triplet loss
    is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{m}=\frac{1}{N}\sum_{i=1}^{N}w^{m}(a_{i},p_{i})[m+&#124;&#124;a_{i}-p_{i}&#124;&#124;^{2}_{2}-&#124;&#124;a_{i}-n_{i}&#124;&#124;^{2}_{2}]_{+}$
    |  | (48) |'
  prefs: []
  type: TYPE_TB
- en: where $a_{i}$ refer to the anchor points, $p_{i}$ are the positive examples,
    and $n_{i}$ are the negative examples. Positive and negatives examples are mined
    based on the method of Iscen etÂ al ([2017](#bib.bib62)), which measures the similarity
    of a pair of points based on estimating the data manifold using $k$-nearest neighbors
    graphs. Lin etÂ al ([2022](#bib.bib79)) apply their method to perform multi-view
    clustering for a variety of multi-view image datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Self-supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Section [4.3.1](#S4.SS3.SSS1 "4.3.1 Hyperbolic self-supervision â€£ 4.3 Self-supervised
    learning â€£ 4 Unsupervised hyperbolic visual learning â€£ Hyperbolic Deep Learning
    in Computer Vision: A Survey"), we describe methods for hyperbolic self-supervision
    which are primarily based on triplet losses, and in Section [4.3.2](#S4.SS3.SSS2
    "4.3.2 Hyperbolic contrastive learning â€£ 4.3 Self-supervised learning â€£ 4 Unsupervised
    hyperbolic visual learning â€£ Hyperbolic Deep Learning in Computer Vision: A Survey")
    we discuss methods for hyperbolic self-supervision which are primarily based on
    contrastive losses.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Hyperbolic self-supervision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the idea that biomedical images are inherently hierarchical, Hsu etÂ al
    ([2021](#bib.bib61)) propose to learn patch-level representations of 3D biomedical
    images using a 3D hyperbolic VAE and to perform 3D unsupervised segmentation by
    clustering the representations. Hsu etÂ al ([2021](#bib.bib61)) extend the hyperbolic
    VAE architecture of Mathieu etÂ al ([2019](#bib.bib85)) using a 3D convolutional
    encoder and decoder as well as gyroplane convolutional layer that generalizes
    the Euclidean convolution with the gyroplane layer of Ganea etÂ al ([2018b](#bib.bib41))
    (See Equations [29](#S4.E29 "In 4.1.1 Hyperbolic VAEs â€£ 4.1 Generative approaches
    â€£ 4 Unsupervised hyperbolic visual learning â€£ Hyperbolic Deep Learning in Computer
    Vision: A Survey") and [30](#S4.E30 "In 4.1.1 Hyperbolic VAEs â€£ 4.1 Generative
    approaches â€£ 4 Unsupervised hyperbolic visual learning â€£ Hyperbolic Deep Learning
    in Computer Vision: A Survey")). In order to learn good representations, the paper
    proposes to use a hierarchical self-supervised loss that captures the implicit
    hierarchical structure of 3D biomedical images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To capture the hierarchical structure of 3D biomedical images, Hsu etÂ al ([2021](#bib.bib61))
    propose that given a parent patch $\mu_{p}$, to sample a child patch $\mu_{c}$
    which is a subpatch of the parent patch, and a negative patch $\mu_{n}$ that does
    not overlap with the parent patch. Then the hierarchical self-supervised loss
    is defined as a margin triplet loss as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\mathrm{hierarchical}}=\max(0,d_{\mathbb{D}}(\mu_{p},\mu_{c})-d_{\mathbb{D}}(\mu_{p},\mu_{n})+\gamma)$
    |  | (49) |'
  prefs: []
  type: TYPE_TB
- en: This encourages the representations of subpatches to be children or descendants
    of the representation of the main patch, and faraway patches (which likely contain
    different structures) to be on other branches of the learned hierarchical representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform unsupervised segmentation, the learned latent representations are
    extracted and clustered using a hyperbolic k-means algorithm, where the traditional
    Euclidean mean is replaced with the Frechet mean. For a manifold $\mathcal{M}$
    with metric $d_{\mathcal{M}}$, the Frechet mean of a set of points $\{z_{i}\}_{i=1}^{k},z_{i}\in\mathcal{M}$
    is defined as the point $\mu$ that minimizes the squared distance to all points
    $z_{i}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mu_{\mathrm{Fr}}=\operatorname*{arg\,min}_{\mu\in\mathcal{M}}\frac{1}{k}\sum_{i=1}^{k}d_{\mathcal{M}}(z_{i},\mu)^{2}$
    |  | (50) |'
  prefs: []
  type: TYPE_TB
- en: and is one way to generalize the concept of a mean to manifolds. Unfortunately,
    the Frechet mean on the PoincarÃ© ball does not admit a closed-form solution, so
    Hsu etÂ al ([2021](#bib.bib61)) compute the Frechet mean with the iterative algorithm
    of Lou etÂ al ([2020](#bib.bib83)). The paper finds that this strategy is effective
    for the unsupervised segmentation of both synthetic biological data and 3D brain
    tumor MRI scans (Menze etÂ al, [2014](#bib.bib86); Bakas etÂ al, [2017](#bib.bib7),
    [2018](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Weng etÂ al ([2021](#bib.bib120)) propose to leverage the hierarchical structure
    of objects within images to perform weakly-supervised long-tail instance segmentation.
    To capture this hierarchical structure, Weng etÂ al ([2021](#bib.bib120)) learn
    hyperbolic representations which are supervised with several hyperbolic self-supervised
    losses. Instance segmentation is done in three stages: first, mask proposals are
    generated using a pre-trained mask proposal network. Mask proposals consists of
    bounding boxes $\{\mathcal{B}_{i}\}_{i=1}^{k}$ and masks $\{\mathcal{M}_{i}\}_{i=1}^{k}$.
    Define $x_{i}^{\mathrm{full}}$ to be the original image cropped to bounding box
    $\mathcal{B}_{i}$, $x_{i}^{\mathrm{bg}}$ to be the cropped image with the object
    masked out using mask $1-\mathcal{M}_{i}$, and $x_{i}^{\mathrm{fg}}$ to be the
    same cropped image with the background masked out using mask $\mathcal{M}_{i}$.
    We will refer to these as the full object image, object background, and object,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, hyperbolic representations of $z_{i}^{\mathrm{bg}}=g(x_{i}^{\mathrm{bg}})$,
    and $z_{i}^{\mathrm{fg}}=g(x_{i}^{\mathrm{fg}})$ are learned by a pre-trained
    feature extractor and supervised by a combination of three self-supervised losses.
    The representations are fixed to have latent dimension 2\. The first self-supervised
    loss encourages representation of the object to be similar to that of the full
    object image and farther away from the representation of the object background:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\mathrm{mask}}=\sum_{i=1}^{k}\max(0,\gamma-d(z_{i}^{\mathrm{full}},z^{\mathrm{fg}})+d(z_{i}^{\mathrm{full}},z_{i}^{\mathrm{bg}}))$
    |  | (51) |'
  prefs: []
  type: TYPE_TB
- en: The second loss is a triplet loss that requires the sampling of positive and
    negative examples.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\mathrm{object}}=\sum_{i=1}^{k}\max(0,\gamma-d(z_{i}^{\mathrm{fg}},\hat{z}^{\mathrm{fg}})+d(z_{i}^{\mathrm{fg}},\overline{z}_{i}^{\mathrm{fg}}))$
    |  | (52) |'
  prefs: []
  type: TYPE_TB
- en: 'The third loss is similar to the hierarchical triplet loss of Hsu etÂ al ([2021](#bib.bib61))
    described above, except with the origin taking the place of negative samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\mathrm{hierarchical}}=\sum_{i=1}^{k}\max(0,\gamma-d(z_{i}^{\mathrm{child}},o)-d(z_{i}^{\mathrm{fg}},o))$
    |  | (53) |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, the representations are clustered using hyperbolic k-means clustering.
    Unlike Hsu etÂ al ([2021](#bib.bib61)), to compute the mean they map the representations
    from the Poincare disk to the hyperboloid model $\mathcal{L}$ and compute the
    (weighted) hyperboloid midpoint proposed by Law etÂ al ([2019](#bib.bib73)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mu=\frac{\sum_{i=1}^{k}\nu_{i}x_{i}}{\left&#124;&#124;&#124;\sum_{i=1}^{k}\nu_{i}x_{i}&#124;&#124;_{\mathcal{L}}\right&#124;}$
    |  | (54) |'
  prefs: []
  type: TYPE_TB
- en: Compared to the Frechet mean, this mean has the advantage of having a closed-form
    formula, making it more computationally efficient. Weng etÂ al ([2021](#bib.bib120))
    find that their method improves other partially-supervised methods on the LVIS
    long-tail segmentation dataset (Gupta etÂ al, [2019](#bib.bib55)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Hyperbolic contrastive learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/91d8eb92f2c844fbdcad2c5b3adc91d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: SurÃ­s etÂ al ([2021](#bib.bib109)) model uncertainty with hyperbolic
    representations. If the model is uncertain, it can predict an abstraction of all
    possible actions (red square), and if it is certain it can predict a more specific
    action (blue square). The pink circle shows how computing the mean of two representations
    (pink squares) increases the generality. Image courtesy of SurÃ­s etÂ al ([2021](#bib.bib109)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyperbolic contrastive learning methods have also been proposed. SurÃ­s etÂ al
    ([2021](#bib.bib109)) propose to learn hyperbolic representations for video action
    prediction because of their ability to combine representing hierarchy and giving
    a measure of uncertainty (See Figure [10](#S4.F10 "Figure 10 â€£ 4.3.2 Hyperbolic
    contrastive learning â€£ 4.3 Self-supervised learning â€£ 4 Unsupervised hyperbolic
    visual learning â€£ Hyperbolic Deep Learning in Computer Vision: A Survey")). SurÃ­s
    etÂ al ([2021](#bib.bib109)) learn an action hierarchy where more abstract actions
    are near the origin of the PoincarÃ© disk and more fine-grained actions are near
    the edge. If the preceding video frames are ambiguous, this hierarchical representation
    allows the ability to predict a more general parent category of action (*e.g.,*
    greeting) instead of having to predict more fine-grained child categories of action
    (*e.g.,* handshake or high-five). The parent of two actions is computed as the
    hyperbolic mean of their hyperbolic representations, which SurÃ­s etÂ al ([2021](#bib.bib109))
    compute as the midpoint of the geodesic connecting the two representations. SurÃ­s
    etÂ al ([2021](#bib.bib109)) propose a two-stage framework for video action prediction
    which consists first of contrastive pre-training hyperbolic representations, then
    freezing the representations and training a linear classifier for action prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-supervised pre-training proceeds as follows: let $x_{t}$ be a frame of
    the video, and a representation $z_{t}=f(x_{t})$ is produced by an encoder $f$.
    The pretext task is to predict the representation $z_{t+\delta}$ of a clip $\delta$
    frames into the future. The model produces an estimate $\hat{z}_{t+\delta}=\phi(c_{t},\delta)$,
    where $c_{t}=g(z_{1},\ldots,z_{t})$ is an encoding of all past video frames. All
    function $f,g,\phi$ are parameterized by a neural network. The training is supervised
    by a contrastive loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}=-\sum_{i}\left[\log\frac{\exp(-d_{\mathbb{D}}^{2}(\hat{z}_{i},z_{i}))}{\sum_{j}\exp(-d_{\mathbb{D}}^{2}(\hat{z}_{i},z_{j}))}\right]$
    |  | (55) |'
  prefs: []
  type: TYPE_TB
- en: which encourages the positive pairs $\hat{z}_{i},z_{i}$ to have similar representations
    while pushing $\hat{z}_{i}$ from the representations of all negative examples
    $z_{j}$. One key feature of this loss is that under the presence of uncertainty,
    say when actions $a,b$ are probable, $\mathcal{L}$ is minimized by predicting
    the midpoint on the geodesic connecting $a,b$, which is equivalent to moving one
    level up the hierarchy to the parent of $a,b$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7949e32f6e0a3941f3d89333572c5e80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The learned hierarchy of Ge etÂ al ([2022](#bib.bib45)) has objects
    near the origin of the PoincarÃ© disk and scenes near the edge of hyperbolic space.
    Image courtesy of Ge etÂ al ([2022](#bib.bib45)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ge etÂ al ([2022](#bib.bib45)) propose to improve contrastive learning by incorporating
    the hierarchical structure of images with a scene-object hierarchy (see Figure
    [11](#S4.F11 "Figure 11 â€£ 4.3.2 Hyperbolic contrastive learning â€£ 4.3 Self-supervised
    learning â€£ 4 Unsupervised hyperbolic visual learning â€£ Hyperbolic Deep Learning
    in Computer Vision: A Survey")). Ge etÂ al ([2022](#bib.bib45)) use a hyperbolic
    version of the MoCo architecture (He etÂ al, [2020](#bib.bib57)), which the authors
    call HCL. Ge etÂ al ([2022](#bib.bib45)) extend the MoCo architectures in several
    ways: first, unlike previous works for visual contrastive learning, HCL requires
    that object regions be extracted from the input image. Secondly, a hyperbolic
    backbone along with a corresponding momentum encoder is added to MoCoâ€™s Euclidean
    backbone and its momentum encoder. The Euclidean backbone and momentum encoder
    are trained the same way as in He etÂ al ([2020](#bib.bib57)), but the inputs are
    not images but the extracted object regions. The hyperbolic branch takes as input
    a scene region $u$ and an object region $v$ that is a subregion of the scene $u$,
    and negative objects $\mathcal{N}_{u}=\{n_{1},\ldots,n_{k}\}$ that are not subregions
    of the scene $u$. Let the representations of $u,v,n_{j}$ be $z_{u},z_{v},z_{j}$,
    respectively. The hyperbolic branch is then trained with a contrastive loss with
    hyperbolic distance as the similarity measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\mathrm{hyp}}=-\log\frac{\exp\left(-d_{\mathbb{D}}(z_{u},z_{v})/\tau\right)}{\exp\left(-d_{\mathbb{D}}(z_{u},z_{v})/\tau\right)+\sum_{j}\exp\left(-d_{\mathbb{D}}(z_{u},z_{j})/\tau\right)}$
    |  | (56) |'
  prefs: []
  type: TYPE_TB
- en: where $\tau$ is a temperature parameter. This loss encourages representations
    to form a scene-object hierarchy where scenes have the highest norm (i.e., are
    at the edge of the PoincarÃ© ball $\mathbb{D}$) and objects have the smallest norm
    (i.e., are at the center of $\mathbb{D}$). The paper finds that their method achieves
    small gains over the original MoCo and MoCo augmented with bounding box information.
    They also examine the representations of out-of-context objects using their method,
    and find that they generally have higher distance to the scene images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yue etÂ al ([2023](#bib.bib128)) propose a different method for hyperbolic contrastive
    learning that is based on SimCLR (Chen etÂ al, [2020c](#bib.bib22)). Like Ge etÂ al
    ([2022](#bib.bib45)), Yue etÂ al ([2023](#bib.bib128)) replace the dot-product
    similarity of the contrastive loss with the hyperbolic distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{hyp}^{self}=-\sum_{i\in I}\log\frac{\exp(-d_{\mathbb{D}}(z_{i},z_{j(i)})/\tau)}{\sum_{a\in
    A(i)}\exp(-d_{\mathbb{D}}(z_{i},z_{a})/\tau)}$ |  | (57) |'
  prefs: []
  type: TYPE_TB
- en: 'but unlike Ge etÂ al ([2022](#bib.bib45)), they only have a hyperbolic branch
    and do not retain an Euclidean branch. Yue etÂ al ([2023](#bib.bib128)) also propose
    to extend the supervised contrastive learning method SupCon (Khosla etÂ al, [2020](#bib.bib67))
    in the same way. Yue etÂ al ([2023](#bib.bib128)) also propose to train an adversarially
    robust contrastive learner that extends the Robust Contrastive Learning (RoCL)
    (Kim etÂ al, [2020](#bib.bib69)) method to hyperbolic space by replacing the Euclidean
    contrastive losses in RoCLâ€™s adversarial training loss with their hyperbolic contrastive
    loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{hyp}^{self}(\tilde{x},\{\tilde{x}^{+},\tilde{x}^{adv},\{\tilde{x}^{-}\}\})+\lambda\mathcal{L}_{hyp}^{self}(\tilde{x}^{adv},\tilde{x}^{+},\{\tilde{x}^{-}\})$
    |  | (58) |'
  prefs: []
  type: TYPE_TB
- en: where $\tilde{x}$ is a given image, $\tilde{x}^{+}$ is a positive example, $\tilde{x}^{-}$
    is a negative example, and $\tilde{x}^{adv}$ is an adversarial example that is
    within $\delta$ of $\tilde{x}$. As in Ge etÂ al ([2022](#bib.bib45)), Yan etÂ al
    ([2021](#bib.bib123)) find that hyperbolic contrastive learning generally achieves
    small gains over its Euclidean counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusions and future outlook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This survey provides an overview of the current state of affairs in hyperbolic
    deep learning for computer vision. Based on the organization of supervised and
    unsupervised literature, we conclude the survey by discussing which types of problems
    currently benefit most from hyperbolic learning and discussing open problems for
    future research.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 When is hyperbolic learning most effective?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From current works, we identify four main axes of improvement that have come
    with the recent shift towards learning in hyperbolic space for computer vision:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical learning. The inherent links between hierarchical data and hyperbolic
    embeddings are well known. It is therefore not all too surprising to see that
    a wide range of works have used hyperbolic learning to improve hierarchical objectives
    in computer vision. The ability to incorporate hierarchical knowledge, for example
    through hyperbolic embeddings or hierarchical hyperbolic logistic regression,
    has been utilized for several problems. Hierarchical learning in hyperbolic space
    can among others reduce error severity, resulting in smaller mistakes and more
    consistent retrieval. This is a key property for example in medical domains, where
    large mistakes need to be avoided at all costs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Hierarchical learning has also shown to enable zero-shot generalization. By
    embedding class hierarchies in hyperbolic space and mapping examples of seen classes
    to their corresponding embedding, it becomes possible to generalize to examples
    of unseen classes. In general, hierarchical information between classes helps
    to structure the semantics of the task at hand, and embedding such knowledge in
    hyperbolic space is preferred over Euclidean space.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-sample learning. Few-shot learning is popular in hyperbolic deep learning
    for computer vision. Many works have shown that consistent improvements can be
    made by performing this task with hyperbolic embeddings and prototypes, both with
    and without hierarchical knowledge. In few-shot learning, samples are scarce when
    it comes to generalization, and working in hyperbolic space consistently improves
    accuracy. These results indicate that hyperbolic space can generalize from fewer
    examples, with potential in domains where examples are scarce. This is already
    visible in the unsupervised domain, where generative learning is better in hyperbolic
    space when working with constrained data sources.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robust learning. Across several axes, hyperbolic learning has shown to be more
    robust. For example, hyperbolic embeddings improve out-of-distribution detection,
    provide a natural way to quantify uncertainty about samples, pinpoint unsupervised
    out-of-context samples, and can improve robustness to adversarial attacks. Robustness
    and uncertainty are key challenges in deep learning in general, hyperbolic deep
    learning can provide a natural solution to robustify networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-dimensional learning. For a lot of applications, networks, and embedding
    spaces need to be constrained, for example when learning on embedded devices or
    when visualizing data. In the unsupervised domain, hyperbolic learning consistently
    improves over Euclidean learning when working with smaller embedding spaces. Similarly,
    the embedding space in supervised problems can be substantially reduced while
    maintaining downstream performance in hyperbolic space. As such, hyperbolic learning
    has the potential to enable learning in compressed and embedded domains.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.2 Open research questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hyperbolic learning has made an impact on computer vision with many promising
    avenues ahead. The field is however still in the early stages with many challenges
    and opportunities ahead. Three directions stand out:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fully hyperbolic learning. Hyperbolic learning papers in computer vision commonly
    share one perspective: hyperbolic learning should be done in the embedding space.
    For the most part, the representation learning of earlier layers is done in Euclidean
    space, resulting in hybrid networks. Works from neuroscience indicate that for
    the earlier layers in neural networks, hyperbolic space can also play a prominent
    role (Chossat, [2020](#bib.bib26)). Recently, Zhang etÂ al ([2023](#bib.bib130))
    have shown that spatial relations in the hippocampus are more hyperbolic than
    Euclidean.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Learning deep networks fully in hyperbolic space requires rethinking all layers,
    from convolutions to self-attention and normalization. At the time of writing
    the survey, two works have made steps in this direction. Bdeir etÂ al ([2023](#bib.bib10))
    introduce a hyperbolic convolutional network in the Lorentz model of hyperbolic
    space. They outline how to perform convolutions, batch normalization, and residual
    connections. Simultaneously, van Spengler etÂ al ([2023](#bib.bib107)) introduce
    PoincarÃ© ResNet, with convolutions, residuals, batch normalization, and better
    network initialization in the PoincarÃ© ball model. The works provide a foundation
    towards fully hyperbolic learning, but many open questions remain. Which model
    is most suitable for fully hyperbolic learning? Or do different layers work best
    in different models? And how can fully hyperbolic learning scale to ImageNet and
    beyond? Should each stage of the network have the same curvature? And how effective
    can hyperbolic networks become across all possible tasks compared to Euclidean
    networks? A lot more research is needed to answer these questions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computational challenges. Performing gradient-based learning in hyperbolic space
    changes how networks are optimized and how parameters behave. Compared to their
    Euclidean counterpart however, hyperbolic networks and embeddings can be numerically
    more unstable, with issues at the boundary of the ball, vanishing gradients, and
    more. Moreover, hyperbolic operations can be more involved and computationally
    heavy depending on the used model, leading to less efficient networks. Such computational
    challenges are relevant for all domains of hyperbolic learning and a broader topic
    that is receiving attention.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source community. Modern deep learning libraries are centered around Euclidean
    geometry. Any new researcher in hyperbolic learning, therefore, does not have
    the opportunity to quickly implement networks and layers to get an intuition into
    its workings. Moreover, any new advances have to be either implemented from scratch
    or imported from code repositories of other papers. What is missing is an open-source
    community and a shared repository that houses advances in hyperbolic learning
    for computer vision. Such a community and code base is vital to get further traction
    and attract a wide audience, including practitioners. Whether it be part of existing
    libraries or as a separate library, continued development of open-source hyperbolic
    learning code is key for the future of the field.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large and multimodal learning. In computer vision, and Artificial Intelligence
    in general, there is a strong trend towards learning at large scale and learning
    with multiple modalities, *e.g.,* image-text or video-audio models. It is therefore
    a natural desire for the field to arrive at hyperbolic foundation models. While
    early work has shown that large-scale and/or multimodal learning is viable with
    hyperbolic embeddings (Desai etÂ al, [2023](#bib.bib32)), hyperbolic foundation
    models form a longer-term commitment as they require solutions to all open problems
    mentioned above, from stable, fully hyperbolic learning to continued open source
    development.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ahmad and Lecue (2022) Ahmad O, Lecue F (2022) Fisheyehdk: Hyperbolic deformable
    kernel learning for ultra-wide field-of-view image recognition. In: AAAI Conference
    on Artificial Intelligence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amin etÂ al (2022) Amin F, Mondal A, Mathew J (2022) Deep semantic hashing with
    structure-semantic disagreement correction via hyperbolic metric learning. In:
    International Workshop on Multimedia Signal Processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anvekar and Bazazian (2023) Anvekar T, Bazazian D (2023) Gpr-net: Geometric
    prototypical network for point cloud few-shot learning. arXiv'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AraÃ±o etÂ al (2021) AraÃ±o KA, Orsenigo C, Soto M, Vercellis C (2021) Multimodal
    sentiment and emotion recognition in hyperbolic space. Expert Systems with Applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arjovsky etÂ al (2017) Arjovsky M, Chintala S, Bottou L (2017) Wasserstein generative
    adversarial networks. In: International Conference on Machine Learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bachmann etÂ al (2020) Bachmann G, BÃ©cigneul G, Ganea O (2020) Constant curvature
    graph convolutional networks. In: International Conference on Machine Learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bakas etÂ al (2017) Bakas S, Akbari H, Sotiras A, Bilello M, Rozycki M, Kirby
    JS, Freymann JB, Farahani K, Davatzikos C (2017) Advancing the cancer genome atlas
    glioma mri collections with expert segmentation labels and radiomic features.
    Scientific data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bakas etÂ al (2018) Bakas S, Reyes M, Jakab A, Bauer S, Rempfler M, Crimi A,
    Shinohara RT, Berger C, Ha SM, Rozycki M, etÂ al (2018) Identifying the best machine
    learning algorithms for brain tumor segmentation, progression assessment, and
    overall survival prediction in the brats challenge. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balazevic etÂ al (2019) Balazevic I, Allen C, Hospedales T (2019) Multi-relational
    poincarÃ© graph embeddings. In: Advances in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bdeir etÂ al (2023) Bdeir A, Schwethelm K, Landwehr N (2023) Hyperbolic geometry
    in computer vision: A novel framework for convolutional neural networks. arXiv'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bommasani etÂ al (2021) Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von
    Arx S, Bernstein MS, Bohg J, Bosselut A, Brunskill E, etÂ al (2021) On the opportunities
    and risks of foundation models. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bose etÂ al (2020) Bose J, Smofsky A, Liao R, Panangaden P, Hamilton W (2020)
    Latent variable modelling with hyperbolic normalizing flows. In: International
    Conference on Machine Learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridson and Haefliger (2013) Bridson MR, Haefliger A (2013) Metric spaces of
    non-positive curvature, vol 319\. Springer Science & Business Media
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cannon etÂ al (1997) Cannon JW, Floyd WJ, Kenyon R, Parry WR, etÂ al (1997) Hyperbolic
    geometry. Flavors of geometry 31(59-115):2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caron etÂ al (2021) Caron M, Touvron H, Misra I, JÃ©gou H, Mairal J, Bojanowski
    P, Joulin A (2021) Emerging properties in self-supervised vision transformers.
    In: International Conference on Computer Vision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chami etÂ al (2019) Chami I, Ying Z, RÃ© C, Leskovec J (2019) Hyperbolic graph
    convolutional neural networks. In: Advances in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chami etÂ al (2020a) Chami I, Gu A, Chatziafratis V, RÃ© C (2020a) From trees
    to continuous embeddings and back: Hyperbolic hierarchical clustering. In: Advances
    in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chami etÂ al (2020b) Chami I, Wolf A, Juan DC, Sala F, Ravi S, RÃ© C (2020b) Low-dimensional
    hyperbolic knowledge graph embeddings. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen etÂ al (2022) Chen B, Peng W, Cao X, RÃ¶ning J (2022) Hyperbolic uncertainty
    aware semantic segmentation. Transactions on Intelligent Transportation Systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen etÂ al (2020a) Chen G, Qiao L, Shi Y, Peng P, Li J, Huang T, Pu S, Tian
    Y (2020a) Learning open set network with discriminative reciprocal points. In:
    European Conference on Computer Vision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen etÂ al (2020b) Chen J, Qin J, Shen Y, Liu L, Zhu F, Shao L (2020b) Learning
    attentive and hierarchical representations for 3d shape recognition. In: European
    Conference on Computer Vision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen etÂ al (2020c) Chen T, Kornblith S, Norouzi M, Hinton G (2020c) A simple
    framework for contrastive learning of visual representations. In: International
    conference on machine learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen etÂ al (2021) Chen W, Han X, Lin Y, Zhao H, Liu Z, Li P, Sun M, Zhou J (2021)
    Fully hyperbolic neural networks. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cho etÂ al (2019) Cho H, DeMeo B, Peng J, Berger B (2019) Large-margin classification
    in hyperbolic space. In: International Conference on Artificial Intelligence and
    Statistics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho etÂ al (2022) Cho S, Lee J, Park J, Kim D (2022) A rotated hyperbolic wrapped
    normal distribution for hierarchical representation learning. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chossat (2020) Chossat P (2020) The hyperbolic model for edge and texture detection
    in the primary visual cortex. The Journal of Mathematical Neuroscience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choudhary and Reddy (2022) Choudhary N, Reddy CK (2022) Towards scalable hyperbolic
    neural networks using taylor series approximations. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cui etÂ al (2022) Cui Y, Yu Z, Peng W, Liu L (2022) Rethinking few-shot class-incremental
    learning with open-set hypothesis in hyperbolic geometry. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai etÂ al (2021) Dai J, Wu Y, Gao Z, Jia Y (2021) A hyperbolic-to-hyperbolic
    graph convolutional network. In: Computer Vision and Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dasgupta (2016) Dasgupta S (2016) A cost function for similarity-based hierarchical
    clustering. In: ACM symposium on Theory of Computing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dengxiong and Kong (2023) Dengxiong X, Kong Y (2023) Ancestor search: Generalized
    open set recognition via hyperbolic side information learning. In: Winter Conference
    on Applications of Computer Vision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Desai etÂ al (2023) Desai K, Nickel M, Rajpurohit T, Johnson J, Vedantam R (2023)
    Hyperbolic image-text representations. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dhall etÂ al (2020) Dhall A, Makarova A, Ganea O, Pavllo D, Greeff M, Krause
    A (2020) Hierarchical image classification using entailment cone embeddings. In:
    Computer Vision and Pattern Recognition Workshops'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dhingra etÂ al (2018) Dhingra B, Shallue CJ, Norouzi M, Dai AM, Dahl GE (2018)
    Embedding text in hyperbolic spaces. In: Workshop on Graph-Based Methods for Natural
    Language Processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dinh etÂ al (2016) Dinh L, Sohl-Dickstein J, Bengio S (2016) Density estimation
    using real nvp. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy etÂ al (2021) Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn
    D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, etÂ al (2021)
    An image is worth 16x16 words: Transformers for image recognition at scale. In:
    International Conference on Learning Representations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ermolov etÂ al (2022) Ermolov A, Mirvakhabova L, Khrulkov V, Sebe N, Oseledets
    I (2022) Hyperbolic vision transformers: Combining improvements in metric learning.
    In: Computer Vision and Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang etÂ al (2021) Fang P, Harandi M, Petersson L (2021) Kernel methods in hyperbolic
    spaces. In: International Conference on Computer Vision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Franco etÂ al (2023) Franco L, Mandica P, Munjal B, Galasso F (2023) Hyperbolic
    self-paced learning for self-supervised skeleton-based action representations.
    In: International Conference on Learning Representations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ganea etÂ al (2018a) Ganea O, BÃ©cigneul G, Hofmann T (2018a) Hyperbolic entailment
    cones for learning hierarchical embeddings. In: International Conference on Machine
    Learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ganea etÂ al (2018b) Ganea O, BÃ©cigneul G, Hofmann T (2018b) Hyperbolic neural
    networks. Advances in Neural Information Processing Systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao etÂ al (2021) Gao Z, Wu Y, Jia Y, Harandi M (2021) Curvature generation
    in curved spaces for few-shot learning. In: International Conference on Computer
    Vision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao etÂ al (2022) Gao Z, Wu Y, Jia Y, Harandi M (2022) Hyperbolic feature augmentation
    via distribution estimation and infinite sampling on manifolds. In: Advances in
    Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao etÂ al (2023) Gao Z, Xu C, Li F, Jia Y, Harandi M, Wu Y (2023) Exploring
    data geometry for continual learning. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ge etÂ al (2022) Ge S, Mishra S, Kornblith S, Li CL, Jacobs D (2022) Hyperbolic
    contrastive learning for visual representations beyond objects. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GhadimiÂ Atigh etÂ al (2021) GhadimiÂ Atigh M, Keller-Ressel M, Mettes P (2021)
    Hyperbolic busemann learning with ideal prototypes. In: Advances in Neural Information
    Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GhadimiÂ Atigh etÂ al (2022) GhadimiÂ Atigh M, Schoep J, Acar E, van Noord N,
    Mettes P (2022) Hyperbolic image segmentation. In: Computer Vision and Pattern
    Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow etÂ al (2020) Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
    D, Ozair S, Courville A, Bengio Y (2020) Generative adversarial networks. Communications
    of the ACM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu etÂ al (2018) Gu J, Wang Z, Kuen J, Ma L, Shahroudy A, Shuai B, Liu T, Wang
    X, Wang G, Cai J, etÂ al (2018) Recent advances in convolutional neural networks.
    Pattern recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gulcehre etÂ al (2019) Gulcehre C, Denil M, Malinowski M, Razavi A, Pascanu
    R, Hermann KM, Battaglia P, Bapst V, Raposo D, Santoro A, etÂ al (2019) Hyperbolic
    attention networks. In: International Conference on Learning Representations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gulrajani etÂ al (2017) Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville
    AC (2017) Improved training of wasserstein gans. In: Advances in Neural Information
    Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulshad etÂ al (2023) Gulshad S, Long T, van Noord N (2023) Hierarchical explanations
    for video action recognition. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo etÂ al (2021) Guo H, Tang J, Zeng W, Zhao X, Liu L (2021) Multi-modal entity
    alignment in hyperbolic space. Neurocomputing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo etÂ al (2022) Guo Y, Wang X, Chen Y, Yu SX (2022) Clipped hyperbolic classifiers
    are super-hyperbolic classifiers. In: Computer Vision and Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta etÂ al (2019) Gupta A, Dollar P, Girshick R (2019) Lvis: A dataset for
    large vocabulary instance segmentation. In: Computer Vision and Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hamann (2018) Hamann M (2018) On the tree-likeness of hyperbolic spaces. In:
    Mathematical proceedings of the cambridge philosophical society, Cambridge University
    Press, vol 164, pp 345â€“361'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He etÂ al (2020) He K, Fan H, Wu Y, Xie S, Girshick R (2020) Momentum contrast
    for unsupervised visual representation learning. In: Computer Vision and Pattern
    Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heusel etÂ al (2017) Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter
    S (2017) Gans trained by a two time-scale update rule converge to a local nash
    equilibrium. In: Advances in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Higgins etÂ al (2017) Higgins I, Matthey L, Pal A, Burgess C, Glorot X, Botvinick
    M, Mohamed S, Lerchner A (2017) beta-vae: Learning basic visual concepts with
    a constrained variational framework. In: International Conference on Learning
    Representations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong etÂ al (2022) Hong J, Fang P, Li W, Han J, Petersson L, Harandi M (2022)
    Curved geometric networks for visual anomaly recognition. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hsu etÂ al (2021) Hsu J, Gu J, Wu G, Chiu W, Yeung S (2021) Capturing implicit
    hierarchical structure in 3d biomedical images with self-supervised hyperbolic
    representations. In: Advances in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iscen etÂ al (2017) Iscen A, Tolias G, Avrithis Y, Furon T, Chum O (2017) Efficient
    diffusion on region manifolds: Recovering small objects with compact cnn representations.
    In: Computer Vision and Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karras etÂ al (2019) Karras T, Laine S, Aila T (2019) A style-based generator
    architecture for generative adversarial networks. In: Computer Vision and Pattern
    Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karras etÂ al (2020) Karras T, Laine S, Aittala M, Hellsten J, Lehtinen J, Aila
    T (2020) Analyzing and improving the image quality of stylegan. In: Computer Vision
    and Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kasarla etÂ al (2022) Kasarla T, Burghouts G, van Spengler M, vanÂ der Pol E,
    Cucchiara R, Mettes P (2022) Maximum class separation as inductive bias in one
    matrix. In: Advances in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khan etÂ al (2022) Khan S, Naseer M, Hayat M, Zamir SW, Khan FS, Shah M (2022)
    Transformers in vision: A survey. ACM Computing Surveys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khosla etÂ al (2020) Khosla P, Teterwak P, Wang C, Sarna A, Tian Y, Isola P,
    Maschinot A, Liu C, Krishnan D (2020) Supervised contrastive learning. In: Advances
    in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khrulkov etÂ al (2020) Khrulkov V, Mirvakhabova L, Ustinova E, Oseledets I,
    Lempitsky V (2020) Hyperbolic image embeddings. In: Computer Vision and Pattern
    Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim etÂ al (2020) Kim M, Tack J, Hwang SJ (2020) Adversarial self-supervised
    contrastive learning. In: Advances in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim etÂ al (2022) Kim S, Jung B, Kwak S (2022) Hier: Metric learning beyond
    class labels via hierarchical regularization. arXiv'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling (2013) Kingma DP, Welling M (2013) Auto-encoding variational
    bayes. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Klimovskaia etÂ al (2020) Klimovskaia A, Lopez-Paz D, Bottou L, Nickel M (2020)
    PoincarÃ© maps for analyzing complex hierarchies in single-cell data. Nature communications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Law etÂ al (2019) Law M, Liao R, Snell J, Zemel R (2019) Lorentzian distance
    learning for hyperbolic representations. In: International Conference on Machine
    Learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lazcano etÂ al (2021) Lazcano D, Franco NF, Creixell W (2021) Hgan: Hyperbolic
    generative adversarial network. IEEE Access'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun etÂ al (2015) LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leimeister and Wilson (2018) Leimeister M, Wilson BJ (2018) Skip-gram word embeddings
    in hyperbolic space. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li etÂ al (2022) Li L, Zhang Y, Wang S (2022) The euclidean space is evil: Hyperbolic
    attribute editing for few-shot image generation. arXiv'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li etÂ al (2023) Li YL, Wu X, Liu X, Dou Y, Ji Y, Zhang J, Li Y, Tan J, Lu X,
    Lu C (2023) From isolated islands to pangea: Unifying semantic space for human
    action understanding. arXiv'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin etÂ al (2022) Lin F, Bai B, Bai K, Ren Y, Zhao P, Xu Z (2022) Contrastive
    multi-view hyperbolic hierarchical clustering. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu etÂ al (2019) Liu Q, Nickel M, Kiela D (2019) Hyperbolic graph neural networks.
    In: Advances in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu etÂ al (2020) Liu S, Chen J, Pan L, Ngo CW, Chua TS, Jiang YG (2020) Hyperbolic
    visual embedding learning for zero-shot recognition. In: Computer Vision and Pattern
    Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Long etÂ al (2020) Long T, Mettes P, Shen HT, Snoek CGM (2020) Searching for
    actions on the hyperbole. In: Computer Vision and Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lou etÂ al (2020) Lou A, Katsman I, Jiang Q, Belongie S, Lim SN, DeÂ Sa C (2020)
    Differentiating through the frÃ©chet mean. In: International Conference on Machine
    Learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma etÂ al (2022) Ma R, Fang P, Drummond T, Harandi M (2022) Adaptive poincarÃ©
    point to set distance for few-shot classification. In: AAAI Conference on Artificial
    Intelligence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathieu etÂ al (2019) Mathieu E, LeÂ Lan C, Maddison CJ, Tomioka R, Teh YW (2019)
    Continuous hierarchical representations with poincarÃ© variational auto-encoders.
    In: Advances in Neural Information Processing Systems, volÂ 32'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Menze etÂ al (2014) Menze BH, Jakab A, Bauer S, Kalpathy-Cramer J, Farahani K,
    Kirby J, Burren Y, Porz N, Slotboom J, Wiest R, etÂ al (2014) The multimodal brain
    tumor image segmentation benchmark (brats). IEEE Transactions on Medical Imaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mettes etÂ al (2019) Mettes P, VanÂ der Pol E, Snoek C (2019) Hyperspherical
    prototype networks. In: Advances in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mirvakhabova etÂ al (2020) Mirvakhabova L, Frolov E, Khrulkov V, Oseledets I,
    Tuzhilin A (2020) Performance of hyperbolic geometry models on top-n recommendation
    tasks. In: ACM Conference on Recommender Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirza and Osindero (2014) Mirza M, Osindero S (2014) Conditional generative
    adversarial nets. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monath etÂ al (2019) Monath N, Zaheer M, Silva D, McCallum A, Ahmed A (2019)
    Gradient-based hierarchical clustering using continuous representations of trees
    in hyperbolic space. In: International Conference on Knowledge Discovery & Data
    Mining'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Montanaro etÂ al (2022) Montanaro A, Valsesia D, Magli E (2022) Rethinking the
    compositionality of point clouds through regularization in the hyperbolic space.
    arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nagano etÂ al (2019) Nagano Y, Yamaguchi S, Fujita Y, Koyama M (2019) A wrapped
    normal distribution on hyperbolic space for gradient-based learning. In: International
    Conference on Machine Learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nickel and Kiela (2017) Nickel M, Kiela D (2017) PoincarÃ© embeddings for learning
    hierarchical representations. In: Advances in Neural Information Processing Systems,
    volÂ 30'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nickel and Kiela (2018) Nickel M, Kiela D (2018) Learning continuous hierarchies
    in the lorentz model of hyperbolic geometry. In: International Conference on Machine
    Learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Noy and Hafner (1997) Noy NF, Hafner CD (1997) The state of the art in ontology
    design: A survey and comparative review. AI magazine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng etÂ al (2021) Peng W, Varanka T, Mostafa A, Shi H, Zhao G (2021) Hyperbolic
    deep neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine
    Intelligence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qu and Zou (2022) Qu E, Zou D (2022) Autoencoding hyperbolic representation
    for adversarial generation. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Radford etÂ al (2021) Radford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal
    S, Sastry G, Askell A, Mishkin P, Clark J, etÂ al (2021) Learning transferable
    visual models from natural language supervision. In: International Conference
    on Machine Learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ratcliffe (1994) Ratcliffe JG (1994) Foundations of hyperbolic manifolds, vol
    149\. Springer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rezende etÂ al (2014) Rezende DJ, Mohamed S, Wierstra D (2014) Stochastic backpropagation
    and approximate inference in deep generative models. In: International Conference
    on Machine Learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Richardson etÂ al (2021) Richardson E, Alaluf Y, Patashnik O, Nitzan Y, Azar
    Y, Shapiro S, Cohen-Or D (2021) Encoding in style: a stylegan encoder for image-to-image
    translation. In: Computer Vision and Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sala etÂ al (2018) Sala F, DeÂ Sa C, Gu A, RÃ© C (2018) Representation tradeoffs
    for hyperbolic embeddings. In: International Conference on Machine Learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salimans etÂ al (2016) Salimans T, Goodfellow I, Zaremba W, Cheung V, Radford
    A, Chen X (2016) Improved techniques for training gans. In: Advances in Neural
    Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schuler (2005) Schuler KK (2005) VerbNet: A broad-coverage, comprehensive verb
    lexicon. University of Pennsylvania'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shimizu etÂ al (2021) Shimizu R, Mukuta Y, Harada T (2021) Hyperbolic neural
    networks++. In: International Conference on Learning Representations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Snell etÂ al (2017) Snell J, Swersky K, Zemel R (2017) Prototypical networks
    for few-shot learning. In: Advances in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Spengler etÂ al (2023) van Spengler M, Berkhout E, Mettes P (2023) Poincar$\backslash$â€™e
    resnet. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun etÂ al (2021) Sun P, Zhang R, Jiang Y, Kong T, Xu C, Zhan W, Tomizuka M,
    Li L, Yuan Z, Wang C, etÂ al (2021) Sparse r-cnn: End-to-end object detection with
    learnable proposals. In: Computer Vision and Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SurÃ­s etÂ al (2021) SurÃ­s D, Liu R, Vondrick C (2021) Learning the predictability
    of the future. In: Computer Vision and Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tifrea etÂ al (2019) Tifrea A, BÃ©cigneul G, Ganea OE (2019) Poincar$\backslash$â€™e
    glove: Hyperbolicp word embeddings. In: International Conference on Learning Representations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tong etÂ al (2022) Tong J, Yang F, Yang S, Dong E, Du S, Wang X, Yi X (2022)
    Hyperbolic cosine transformer for lidar 3d object detection. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trpin and Boshkoska (2022) Trpin A, Boshkoska B (2022) Face recognition with
    a hyperbolic metric classification model. In: International Convention on Information,
    Communication and Electronic Technology'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ungar (2005) Ungar AA (2005) Gyrovector spaces and their differential geometry.
    Nonlinear Funct Anal Appl 10(5):791â€“834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ungar (2008) Ungar AA (2008) A gyrovector space approach to hyperbolic geometry.
    Synthesis Lectures on Mathematics and Statistics 1(1):1â€“194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ungar (2012) Ungar AA (2012) Beyond the Einstein addition law and its gyroscopic
    Thomas precession: The theory of gyrogroups and gyrovector spaces, vol 117\. Springer
    Science & Business Media'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valada (2022) Valada A (2022) On hyperbolic embeddings in object detection.
    In: German Conference on Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Wang (2018) Wang D, Wang Y (2018) An improved cost function for hierarchical
    cluster trees. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang etÂ al (2021) Wang L, Hu F, Wu S, Wang L (2021) Fully hyperbolic graph
    convolution network for recommendation. In: Proceedings of the 30th ACM International
    Conference on Information & Knowledge Management'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang etÂ al (2023) Wang S, Kang Q, She R, Wang W, Zhao K, Song Y, Tay WP (2023)
    Hypliloc: Towards effective lidar pose regression with hyperbolic fusion. arXiv'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weng etÂ al (2021) Weng Z, Ogut MG, Limonchik S, Yeung S (2021) Unsupervised
    discovery of the long-tail in instance segmentation using hierarchical self-supervision.
    In: Computer Vision and Pattern Recognition, pp 2603â€“2612'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu etÂ al (2021) Wu Z, Jiang D, Hsieh CY, Chen G, Liao B, Cao D, Hou T (2021)
    Hyperbolic relational graph convolution networks plus: a simple but highly efficient
    qsar-modeling method. Briefings in Bioinformatics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu etÂ al (2022) Xu Y, Mu L, Ji Z, Liu X, Han J (2022) Meta hyperbolic networks
    for zero-shot learning. Neurocomputing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan etÂ al (2021) Yan J, Luo L, Deng C, Huang H (2021) Unsupervised hyperbolic
    metric learning. In: Computer Vision and Pattern Recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan etÂ al (2023) Yan J, Luo L, Deng C, Huang H (2023) Adaptive hierarchical
    similarity metric learning with noisy labels. IEEE Transactions on Image Processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang etÂ al (2022) Yang M, Zhou M, Liu J, Lian D, King I (2022) Hrcf: Enhancing
    collaborative filtering via hyperbolic geometric regularization. In: Proceedings
    of the ACM Web Conference'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu etÂ al (2020) Yu K, Visweswaran S, Batmanghelich K (2020) Semi-supervised
    hierarchical drug embedding in hyperbolic space. Journal of chemical information
    and modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu etÂ al (2022) Yu Z, Nguyen T, Gal Y, Ju L, Chandra SS, Zhang L, Bonnington
    P, Mar V, Wang Z, Ge Z (2022) Skin lesion recognition with class-hierarchy regularized
    hyperbolic embeddings. In: International Conference on Medical Image Computing
    and Computer-Assisted Intervention'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yue etÂ al (2023) Yue Y, Lin F, Yamada KD, Zhang Z (2023) Hyperbolic contrastive
    learning. arXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang etÂ al (2022) Zhang B, Jiang H, Feng S, Li X, Ye Y, Ye R (2022) Hyperbolic
    knowledge transfer with class hierarchy for few-shot learning. In: International
    Joint Conference on Artificial Intelligence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al (2023) Zhang H, Rich PD, Lee AK, Sharpee TO (2023) Hippocampal spatial
    representations exhibit a hyperbolic geometry that expands with experience. Nature
    Neuroscience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang etÂ al (2021a) Zhang Y, Luo L, Xian W, Huang H (2021a) Learning better
    visual data similarities via new grouplet non-euclidean embedding. In: International
    Conference on Computer Vision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al (2021b) Zhang Y, Wang X, Shi C, Jiang X, Ye Y (2021b) Hyperbolic
    graph attention network. IEEE Transactions on Big Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang etÂ al (2021c) Zhang Y, Wang X, Shi C, Liu N, Song G (2021c) Lorentzian
    graph convolutional networks. In: Proceedings of the Web Conference'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu etÂ al (2020) Zhu Y, Zhou D, Xiao J, Jiang X, Chen X, Liu Q (2020) Hypertext:
    Endowing fasttext with hyperbolic geometry. In: Empirical Methods in Natural Language
    Processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
