- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:51:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:51:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2109.14335] From Beginner to Master: A Survey for Deep Learning-based Single-Image
    Super-Resolution'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2109.14335] 从初学者到大师：基于深度学习的单图像超分辨率综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2109.14335](https://ar5iv.labs.arxiv.org/html/2109.14335)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2109.14335](https://ar5iv.labs.arxiv.org/html/2109.14335)
- en: \UseRawInputEncoding
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \UseRawInputEncoding
- en: 'From Beginner to Master: A Survey for Deep Learning-based Single-Image Super-Resolution'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从初学者到大师：基于深度学习的单图像超分辨率综述
- en: 'Juncheng Li^($\dagger$), Zehua Pei^($\dagger$), and Tieyong Zeng *: Corresponding
    author. $\dagger$: Contribute equally to this work and are co-first authors. J.
    Li, Z. Pei, and T. Zeng are with the Center for Mathematical Artificial Intelligence
    (CMAI), Department of Mathematics, The Chinese University of Hong Kong. (E-mails:
    cvjunchengli@gmail.com, pzehua2000@gmail.com, zeng@math.cuhk.edu.hk.)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 李俊成^($\dagger$)、裴泽华^($\dagger$) 和曾铁勇 *：通讯作者。$\dagger$：对本研究贡献相等，为共同第一作者。J. Li、Z.
    Pei 和 T. Zeng 现为香港中文大学数学系数学人工智能中心（CMAI）成员。（电子邮件：cvjunchengli@gmail.com, pzehua2000@gmail.com,
    zeng@math.cuhk.edu.hk。）
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Single-image super-resolution (SISR) is an important task in image processing,
    which aims to enhance the resolution of imaging systems. Recently, SISR has made
    a huge leap and has achieved promising results with the help of deep learning
    (DL). In this survey, we give an overview of DL-based SISR methods and group them
    according to their targets, such as reconstruction efficiency, reconstruction
    accuracy, and perceptual accuracy. Specifically, we first introduce the problem
    definition, research background, and the significance of SISR. Secondly, we introduce
    some related works, including benchmark datasets, upsampling methods, optimization
    objectives, and image quality assessment methods. Thirdly, we provide a detailed
    investigation of SISR and give some domain-specific applications of it. Fourthly,
    we present the reconstruction results of some classic SISR methods to intuitively
    know their performance. Finally, we discuss some issues that still exist in SISR
    and summarize some new trends and future directions. This is an exhaustive survey
    of SISR, which can help researchers better understand SISR and inspire more exciting
    research in this field. An investigation project for SISR is provided in https://github.com/CV-JunchengLi/SISR-Survey.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 单图像超分辨率（SISR）是图像处理中的一个重要任务，旨在提高成像系统的分辨率。最近，SISR在深度学习（DL）的帮助下取得了巨大的飞跃，并取得了令人鼓舞的成果。在本综述中，我们概述了基于DL的SISR方法，并根据其目标（如重建效率、重建精度和感知准确度）对其进行分组。具体而言，我们首先介绍问题定义、研究背景以及SISR的重要性。其次，我们介绍了一些相关工作，包括基准数据集、上采样方法、优化目标和图像质量评估方法。第三，我们详细调查了SISR，并给出了一些领域特定的应用。第四，我们展示了一些经典SISR方法的重建结果，以直观了解其性能。最后，我们讨论了SISR中仍存在的一些问题，并总结了一些新的趋势和未来方向。这是一项全面的SISR综述，可以帮助研究人员更好地理解SISR，并激发该领域更多激动人心的研究。有关SISR的调查项目请访问
    [https://github.com/CV-JunchengLi/SISR-Survey](https://github.com/CV-JunchengLi/SISR-Survey)。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Image super-resolution, single-image super-resolution, SISR, survey, overview.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图像超分辨率、单图像超分辨率、SISR、综述、概述。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Image super-resolution (SR), especially single-image super-resolution (SISR),
    is one kind of image transformation task and has received increasing attention
    in academic and industry. As shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ From Beginner to Master: A Survey for Deep Learning-based Single-Image Super-Resolution"),
    SISR aims to reconstruct a super-resolution (SR) image from its degraded low-resolution
    (LR) one. It is widely used in various computer vision applications, including
    security and surveillance image, medical image reconstruction, video enhancement,
    and image segmentation.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '图像超分辨率（SR），特别是单图像超分辨率（SISR），是一种图像转换任务，已受到学术界和工业界的日益关注。如图 [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ From Beginner to Master: A Survey for Deep Learning-based
    Single-Image Super-Resolution")所示，SISR旨在从其降解的低分辨率（LR）图像中重建超分辨率（SR）图像。它广泛应用于各种计算机视觉应用，包括安全和监控图像、医学图像重建、视频增强和图像分割。'
- en: Many SISR methods have been studied long before, such as bicubic interpolation
    and Lanczos resampling [[1](#bib.bib1)] which are based on interpolation. However,
    SISR is an inherently ill-posed problem, and there always exist multiple HR images
    corresponding to one original LR image. To solve this issue, some numerical methods
    utilize prior information to restrict the solution space of the reconstruction,
    such as edge-based methods [[2](#bib.bib2)] and image statistics-based methods [[3](#bib.bib3)].
    Meanwhile, there are some widely used learning methods, such as neighbor embedding
    methods [[4](#bib.bib4)] and sparse coding methods [[5](#bib.bib5)], which assume
    that there exists a transformation between LR and HR patches.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多单图像超分辨率（SISR）方法在很早以前就已经被研究，例如基于插值的双三次插值和Lanczos重采样[[1](#bib.bib1)]。然而，SISR本质上是一个病态问题，总是存在多个高分辨率（HR）图像对应于一个原始的低分辨率（LR）图像。为了解决这个问题，一些数值方法利用先验信息来限制重建的解空间，例如基于边缘的方法[[2](#bib.bib2)]和基于图像统计的方法[[3](#bib.bib3)]。同时，也有一些广泛使用的学习方法，如邻域嵌入方法[[4](#bib.bib4)]和稀疏编码方法[[5](#bib.bib5)]，这些方法假设LR和HR图像块之间存在一种变换。
- en: '![Refer to caption](img/1d6a128289d5b6a6bcfe796bfcc8e9a3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1d6a128289d5b6a6bcfe796bfcc8e9a3.png)'
- en: 'Figure 1: SISR aims to reconstruct a super-resolution (SR) image from its degraded
    low-resolution (LR) one.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：SISR的目标是从降级的低分辨率（LR）图像中重建一个超分辨率（SR）图像。
- en: 'Recently, deep learning (DL) [[6](#bib.bib6)] has demonstrated better performance
    than traditional machine learning models in many artificial intelligence fields,
    including computer vision [[7](#bib.bib7)] and natural language processing [[8](#bib.bib8)].
    With the rapid development of DL techniques, numerous DL-based methods have been
    proposed for SISR, continuously prompting the State-Of-The-Art (SOTA) forward.
    Like other image transformation tasks, the SISR task can generally be divided
    into three steps: feature extraction and representation, non-linear mapping, and
    image reconstruction [[9](#bib.bib9)]. In traditional numerical models, it is
    time-consuming and inefficient to design an algorithm satisfying all these processes.
    On the contrary, DL can transfer the SISR task to an almost end-to-end framework
    incorporating all these three processes, which can greatly decrease manual and
    computing expense [[10](#bib.bib10)]. Additionally, given the ill-posed nature
    of SISR which can lead to unstable and hard convergence on the results, DL can
    alleviate this issue through efficient network architecture and loss functions
    design. Moreover, modern GPU enables deeper and more complex DL models to train
    fast, which show greater representation power than traditional numerical models.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习（DL）[[6](#bib.bib6)]在许多人工智能领域，如计算机视觉[[7](#bib.bib7)]和自然语言处理[[8](#bib.bib8)]，表现出了比传统机器学习模型更好的性能。随着DL技术的快速发展，提出了大量基于DL的方法用于SISR，持续推动了最先进技术（SOTA）的进步。像其他图像转换任务一样，SISR任务通常可以分为三个步骤：特征提取与表示、非线性映射和图像重建[[9](#bib.bib9)]。在传统数值模型中，设计一个满足所有这些过程的算法既费时又低效。相反，DL可以将SISR任务转移到一个几乎端到端的框架中，涵盖这三个过程，这可以大大减少人工和计算开销[[10](#bib.bib10)]。此外，考虑到SISR的病态特性可能导致结果的不稳定和难以收敛，DL可以通过高效的网络架构和损失函数设计缓解这个问题。此外，现代GPU使得更深、更复杂的DL模型能够快速训练，这些模型展示了比传统数值模型更强的表现力。
- en: '![Refer to caption](img/c1808cffcaa824848b36b3a5dba4eb14.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c1808cffcaa824848b36b3a5dba4eb14.png)'
- en: 'Figure 2: The content and taxonomy of this survey. In this survey, we divide
    the DL-based SISR methods into four categories, which are classified according
    to their specific targets. Among them, the dark gray blocks are the focus methods
    in this survey.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：本调查的内容和分类。在本调查中，我们将基于深度学习（DL）的SISR方法分为四类，这些分类依据它们的具体目标进行。其中，深灰色块是本调查中的重点方法。
- en: 'It is well known that DL-based methods can be divided into supervised and unsupervised
    methods. This is the simplest classification criterion, but the range of this
    classification criterion is too large and not clear. As a result, many technically
    unrelated methods may be classified into the same type while methods with similar
    strategies may be classified into completely different types. Different from previous
    SISR surveys [[11](#bib.bib11), [12](#bib.bib12)] that use supervision as the
    classification criterion or introduce the methods in a pure literature way, in
    this survey, we attempt to give a comprehensive overview of DL-based SISR methods
    and categorize them according to their specific targets. In Fig. [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ From Beginner to Master: A Survey for Deep Learning-based
    Single-Image Super-Resolution"), we show the content and taxonomy of this survey.
    Obviously, we divide the DL-based SISR methods into four categories: reconstruction
    efficiency methods, reconstruction accuracy methods, perceptual quality methods,
    and further improvement methods. This target-based survey has a clear context
    hence it is convenient for readers to consult. Specifically, in this survey, we
    first introduce the problem definition, research background, and significance
    of SISR. Then, we introduce some related works, including benchmark datasets,
    upsample methods, optimization objectives, and assessment methods. After that,
    we provide a detailed investigation of SISR methods and provide the reconstruction
    results of them. Finally, we discuss some issues that still exist in SISR and
    provide some new trends and future directions. Overall, the main contributions
    of this survey are as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '众所周知，基于DL的方法可以分为监督式和无监督式方法。这是最简单的分类标准，但这个分类标准的范围过大且不够明确。因此，许多技术上不相关的方法可能会被归类为同一类型，而具有类似策略的方法可能会被分类为完全不同的类型。与之前使用监督作为分类标准或以纯文献方式介绍方法的SISR调查[[11](#bib.bib11),
    [12](#bib.bib12)]不同，本调查尝试对基于DL的SISR方法进行全面概述，并根据其特定目标进行分类。在图[2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ From Beginner to Master: A Survey for Deep Learning-based
    Single-Image Super-Resolution")中，我们展示了本调查的内容和分类法。显然，我们将基于DL的SISR方法分为四类：重建效率方法、重建准确性方法、感知质量方法和进一步改进方法。这种基于目标的调查有一个清晰的背景，因此方便读者查阅。具体来说，在本调查中，我们首先介绍了SISR的问题定义、研究背景和意义。然后，我们介绍了一些相关工作，包括基准数据集、上采样方法、优化目标和评估方法。之后，我们提供了对SISR方法的详细调查，并给出了它们的重建结果。最后，我们讨论了SISR中仍然存在的一些问题，并提供了一些新趋势和未来方向。总体来说，本调查的主要贡献如下：'
- en: (1). We give a thorough overview of DL-based SISR methods according to their
    targets. This is a new perspective that makes the survey has a clear context hence
    it is convenient for readers to consult.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: (1). 我们根据目标对基于DL的SISR方法进行了全面概述。这是一个新的视角，使调查具有明确的背景，因此方便读者查阅。
- en: (2). This survey covers more than 100 SR methods and introduces a series of
    new tasks and domain-specific applications extended by SISR in recent years.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (2). 本调查涵盖了100多种SR方法，并介绍了近年来SISR扩展的新任务和领域特定应用。
- en: (3). We provide a detailed comparison of reconstruction results, including classic,
    latest, and SOTA SISR methods, to help readers intuitively know their performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: (3). 我们提供了重建结果的详细比较，包括经典方法、最新方法和SOTA SISR方法，以帮助读者直观地了解它们的性能。
- en: (4). We discuss some issues that still exist in SISR and summarize some new
    trends and future directions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (4). 我们讨论了SISR中仍然存在的一些问题，并总结了一些新趋势和未来方向。
- en: 2 Problem Setting and Related Works
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题设置及相关工作
- en: 2.1 Problem Definition
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题定义
- en: Image super-resolution is a classic technique to improve the resolution of an
    imaging system, which can be classified into single-image super-resolution (SISR)
    and multi-image super-resolution (MISR) according to the number of the input LR
    images. Among them, MISR has gradually developed into video super-resolution (VSR).
    Compared with MISR/VSR, SISR is much more challenging since MISR/VSR have extra
    information for reference while SISR only has information of a single input image
    for the missing image features reconstruction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图像超分辨率是一种经典技术，用于提高成像系统的分辨率，根据输入的LR图像数量可以分为单图像超分辨率（SISR）和多图像超分辨率（MISR）。其中，MISR逐渐发展为视频超分辨率（VSR）。与MISR/VSR相比，SISR要困难得多，因为MISR/VSR具有额外的参考信息，而SISR仅有单张输入图像的信息用于重建缺失的图像特征。
- en: 'Define the low-resolution image as $I_{x}\in\mathbb{R}^{h\times w}$ and the
    ground-truth high-resolution image as $I_{y}\in\mathbb{R}^{H\times W}$, where
    $H>h$ and $W>w$. Typically, in a SISR framework, the LR image $I_{x}$ is modeled
    as $I_{x}=\mathcal{D}(I_{y};\theta_{\mathcal{D}})$, where D is a degradation map
    $\mathbb{R}^{H\times W}\to\mathbb{R}^{h\times w}$ and $\theta_{D}$ denotes the
    degradation factor. In most cases, the degradation process is unknown. Therefore,
    researchers are trying to model it. The most popular degradation mode is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将低分辨率图像定义为 $I_{x}\in\mathbb{R}^{h\times w}$，将真实高分辨率图像定义为 $I_{y}\in\mathbb{R}^{H\times
    W}$，其中 $H>h$ 且 $W>w$。通常，在 SISR 框架中，低分辨率图像 $I_{x}$ 被建模为 $I_{x}=\mathcal{D}(I_{y};\theta_{\mathcal{D}})$，其中
    D 是一个降级映射 $\mathbb{R}^{H\times W}\to\mathbb{R}^{h\times w}$，$\theta_{D}$ 表示降级因子。在大多数情况下，降级过程是未知的。因此，研究人员正尝试对其建模。最流行的降级模式是：
- en: '|  | $\mathcal{D}(I_{y};\theta_{\mathcal{D}})=(I_{y}\otimes\kappa)\downarrow_{s}+n,$
    |  | (1) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}(I_{y};\theta_{\mathcal{D}})=(I_{y}\otimes\kappa)\downarrow_{s}+n,$
    |  | (1) |'
- en: where $I_{y}\otimes\kappa$ represents the convolution between the blur kernel
    $\kappa$ and the HR image $I_{y}$, $\downarrow_{s}$ is a subsequent downsampling
    operation with scale factor $s$, and $n$ is usually the additive white Gaussian
    noise (AWGN) with standard deviation $\sigma$. In the SISR task, we need to recover
    a SR image $I_{SR}$ from the LR image $I_{x}$. Therefore, the task can be formulated
    as $I_{SR}=\mathcal{F}(I_{x};\theta_{\mathcal{F}})$, where $\mathcal{F}$ is the
    SR algorithm and $\theta_{\mathcal{F}}$ is the parameter set of the SR process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{y}\otimes\kappa$ 表示模糊核 $\kappa$ 和高分辨率图像 $I_{y}$ 之间的卷积，$\downarrow_{s}$
    是一个具有尺度因子 $s$ 的下采样操作，而 $n$ 通常是具有标准差 $\sigma$ 的加性白噪声（AWGN）。在单图像超分辨率（SISR）任务中，我们需要从低分辨率图像
    $I_{x}$ 恢复一个超分辨率图像 $I_{SR}$。因此，该任务可以表述为 $I_{SR}=\mathcal{F}(I_{x};\theta_{\mathcal{F}})$，其中
    $\mathcal{F}$ 是超分辨率算法，$\theta_{\mathcal{F}}$ 是超分辨率过程的参数集。
- en: 'Recently, researches have converted the SISR into an end-to-end learning task,
    relying on massive training datas and effective loss functions. Meanwhile, more
    and more DL-based models have been proposed due to the powerful representation
    power of CNN and its convenience in both forward and backward computing. Therefore,
    the SISR task can be transformed into the following optimization goal:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究人员已将 SISR 转换为端到端学习任务，依赖于大量训练数据和有效的损失函数。与此同时，由于卷积神经网络（CNN）强大的表示能力及其在前向和反向计算中的便利性，越来越多的基于深度学习（DL）模型被提出。因此，SISR
    任务可以转化为以下优化目标：
- en: '|  | $\hat{\theta}_{\mathcal{F}}=\mathop{\arg\min}_{\theta_{\mathcal{F}}}\mathcal{L}(I_{SR},I_{y})+\lambda\Phi(\theta),$
    |  | (2) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\theta}_{\mathcal{F}}=\mathop{\arg\min}_{\theta_{\mathcal{F}}}\mathcal{L}(I_{SR},I_{y})+\lambda\Phi(\theta),$
    |  | (2) |'
- en: where $\mathcal{L}$ denotes the loss function between the generated SR image
    $I_{SR}$ and the HR image $I_{y}$, $\Phi(\theta)$ denotes the regularization term,
    and $\lambda$ is the trade-off parameter that is used to control the percentage
    of the regularization term.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}$ 表示生成的超分辨率图像 $I_{SR}$ 和高分辨率图像 $I_{y}$ 之间的损失函数，$\Phi(\theta)$
    表示正则化项，而 $\lambda$ 是用于控制正则化项百分比的折中参数。
- en: '![Refer to caption](img/0503746d8ed41ac3e0e18aec1f7240e0.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0503746d8ed41ac3e0e18aec1f7240e0.png)'
- en: 'Figure 3: The training process of data-driven based deep neural networks.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 基于数据驱动的深度神经网络的训练过程。'
- en: 2.2 Benchmarks Datasets
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 基准数据集
- en: 'Data is always essential for data-driven models, especially the DL-based SISR
    models, to achieve promising reconstruction performance (Fig. [3](#S2.F3 "Figure
    3 ‣ 2.1 Problem Definition ‣ 2 Problem Setting and Related Works ‣ From Beginner
    to Master: A Survey for Deep Learning-based Single-Image Super-Resolution")).
    Nowadays, industry and academia have launched several available datasets for SISR.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据对数据驱动模型来说总是至关重要的，特别是对于基于深度学习的 SISR 模型，以实现令人满意的重建性能（见图 [3](#S2.F3 "图 3 ‣ 2.1
    问题定义 ‣ 2 问题设置与相关工作 ‣ 从初学者到大师：深度学习单图像超分辨率的综述")）。如今，工业界和学术界已经推出了多个可用的 SISR 数据集。
- en: 'TABLE I: Benchmarks datasets for single-image super-resolution (SISR).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 单图像超分辨率（SISR）的基准数据集。'
- en: '| Name | Usage | Amount | Format | Description |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 用途 | 数量 | 格式 | 描述 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| General-100 [[13](#bib.bib13)] | Train | 100 | BMP | Common images with clear
    edges but fewer smooth regions |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| General-100 [[13](#bib.bib13)] | 训练 | 100 | BMP | 边缘清晰但平滑区域较少的常见图像 |'
- en: '| T91 [[5](#bib.bib5)] | Train | 91 | PNG | Common Images |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| T91 [[5](#bib.bib5)] | 训练 | 91 | PNG | 常见图像 |'
- en: '| WED [[14](#bib.bib14)] | Train | 4744 | MAT | Common images |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| WED [[14](#bib.bib14)] | 训练 | 4744 | MAT | 常见图像 |'
- en: '| Flickr2K [[15](#bib.bib15)] | Train | 2650 | PNG | 2K images from Flickr
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Flickr2K [[15](#bib.bib15)] | 训练 | 2650 | PNG | 来自 Flickr 的 2K 图像 |'
- en: '| DIV2K [[16](#bib.bib16)] | Train/Val | 1000 | PNG | High-quality dataset
    for CVPR NTIRE competition |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| DIV2K [[16](#bib.bib16)] | 训练/验证 | 1000 | PNG | CVPR NTIRE 竞赛的高质量数据集 |'
- en: '| BSDS300 [[17](#bib.bib17)] | Train/Val | 300 | JPG | Common images |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| BSDS300 [[17](#bib.bib17)] | 训练/验证 | 300 | JPG | 常见图像 |'
- en: '| BSDS500 [[18](#bib.bib18)] | Train/Val | 500 | JPG | Common images |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| BSDS500 [[18](#bib.bib18)] | 训练/验证 | 500 | JPG | 常见图像 |'
- en: '| RealSR [[19](#bib.bib19)] | Train/Val | 100 | Train/Val | 100 real world
    low and high resolution image pairs |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| RealSR [[19](#bib.bib19)] | 训练/验证 | 100 | 训练/验证 | 100 对真实世界的低分辨率和高分辨率图像对
    |'
- en: '| OutdoorScene [[20](#bib.bib20)] | Train/Val | 10624 | PNG | Images of outdoor
    scences |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| OutdoorScene [[20](#bib.bib20)] | 训练/验证 | 10624 | PNG | 户外场景图像 |'
- en: '| City100 [[21](#bib.bib21)] | Train/Test | 100 | RAW | Common images |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| City100 [[21](#bib.bib21)] | 训练/测试 | 100 | RAW | 常见图像 |'
- en: '| Flickr1024 [[22](#bib.bib22)] | Train/Test | 100 | RAW | Stereo images used
    for Stereo SR |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Flickr1024 [[22](#bib.bib22)] | 训练/测试 | 100 | RAW | 用于立体超分辨率的立体图像 |'
- en: '| SR-RAW [[23](#bib.bib23)] | Train/Test | 7*500 | JPG/ARW | Raw images produced
    by real world computational zoom |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| SR-RAW [[23](#bib.bib23)] | 训练/测试 | 7*500 | JPG/ARW | 真实世界计算缩放产生的原始图像 |'
- en: '| PIPAL [[24](#bib.bib24)] | Test | 200 | PNG | Perceptual image quality assessment
    dataset |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| PIPAL [[24](#bib.bib24)] | 测试 | 200 | PNG | 感知图像质量评估数据集 |'
- en: '| Set5 [[25](#bib.bib25)] | Test | 5 | PNG | Common images, only 5 images |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Set5 [[25](#bib.bib25)] | 测试 | 5 | PNG | 常见图像，仅 5 张图像 |'
- en: '| Set14 [[26](#bib.bib26)] | Test | 14 | PNG | Common images, only 14 images
    |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Set14 [[26](#bib.bib26)] | 测试 | 14 | PNG | 常见图像，仅 14 张图像 |'
- en: '| BSD100 [[17](#bib.bib17)] | Test | 100 | JPG | A subset of BSDS500 for testing
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| BSD100 [[17](#bib.bib17)] | 测试 | 100 | JPG | BSDS500 的一个子集，用于测试 |'
- en: '| Urban100 [[27](#bib.bib27)] | Test | 100 | PNG | Images of real world structures
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Urban100 [[27](#bib.bib27)] | 测试 | 100 | PNG | 真实世界结构的图像 |'
- en: '| Manga109 [[28](#bib.bib28)] | Test | 109 | PNG | Japanese manga |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Manga109 [[28](#bib.bib28)] | 测试 | 109 | PNG | 日本漫画 |'
- en: '| L20 [[29](#bib.bib29)] | Test | 20 | PNG | Common images, very high-resolution
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| L20 [[29](#bib.bib29)] | 测试 | 20 | PNG | 常见图像，非常高分辨率 |'
- en: '| PIRM [[30](#bib.bib30)] | Test | 200 | PNG | Common images, datasets for
    ECCV PIRM competition |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| PIRM [[30](#bib.bib30)] | 测试 | 200 | PNG | 常见图像，用于 ECCV PIRM 竞赛的数据集 |'
- en: 2.2.1 Training and Test Datasets
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 训练和测试数据集
- en: 'Recently, many datasets for the SISR task have been proposed, including BSDS300 [[17](#bib.bib17)],
    DIV2K [[16](#bib.bib16)], and Flickr2K [[15](#bib.bib15)]. Meanwhile, there are
    also many test datasets that can be used to effectively test the performance of
    the models, such as Set5 [[25](#bib.bib25)], Set14 [[26](#bib.bib26)], Urban100 [[27](#bib.bib27)],
    and Manga109 [[28](#bib.bib28)]. In Table [I](#S2.T1 "TABLE I ‣ 2.2 Benchmarks
    Datasets ‣ 2 Problem Setting and Related Works ‣ From Beginner to Master: A Survey
    for Deep Learning-based Single-Image Super-Resolution"), we list a series of commonly
    used datasets and indicate their detailed attribute.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，许多用于单幅图像超分辨率（SISR）任务的数据集被提出，包括 BSDS300 [[17](#bib.bib17)]、DIV2K [[16](#bib.bib16)]
    和 Flickr2K [[15](#bib.bib15)]。与此同时，还有许多测试数据集可以有效地测试模型的性能，如 Set5 [[25](#bib.bib25)]、Set14 [[26](#bib.bib26)]、Urban100 [[27](#bib.bib27)]
    和 Manga109 [[28](#bib.bib28)]。在表 [I](#S2.T1 "TABLE I ‣ 2.2 Benchmarks Datasets
    ‣ 2 Problem Setting and Related Works ‣ From Beginner to Master: A Survey for
    Deep Learning-based Single-Image Super-Resolution") 中，我们列出了一系列常用的数据集，并注明了它们的详细属性。'
- en: Among these datasets, DIV2K [[16](#bib.bib16)] is the most widely used dataset
    for model training, which is a high-quality dataset that contains 800 training
    images, 100 validation images, and 100 test images. Flickr2k is a large extended
    dataset, which contains 2650 2K images from Flickr. RealSR [[19](#bib.bib19)]
    is the first truly collected SISR dataset with paired LR and HR images. In addition
    to the listed datasets, some datasets widely used in other computer vision tasks
    are also used as supplementary training datasets for SISR, such as ImageNet [[31](#bib.bib31)]
    and CelebA [[32](#bib.bib32)]. In addition, combining multiple datasets (e.g.,
    DF2K) for training to further improve the model performance has also been widely
    used.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些数据集中，DIV2K [[16](#bib.bib16)] 是用于模型训练的最广泛使用的数据集，它是一个高质量的数据集，包含 800 张训练图像、100
    张验证图像和 100 张测试图像。Flickr2k 是一个大规模扩展的数据集，包含来自 Flickr 的 2650 张 2K 图像。RealSR [[19](#bib.bib19)]
    是第一个真正收集的具有配对低分辨率和高分辨率图像的 SISR 数据集。除了列出的数据集外，一些广泛用于其他计算机视觉任务的数据集也被作为 SISR 的补充训练数据集使用，如
    ImageNet [[31](#bib.bib31)] 和 CelebA [[32](#bib.bib32)]。此外，结合多个数据集（例如 DF2K）进行训练以进一步提高模型性能也被广泛使用。
- en: 2.2.2 Degradation Mode
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 降质模式
- en: 'Due to the particularity of the SISR task, it is difficult to construct a large-scale
    paired real SR dataset. Therefore, researchers often apply degradation patterns
    on the aforementioned datasets to obtain corresponding degraded images to construct
    paired datasets. However, images in the real world are easily disturbed by various
    factors (e.g., sensor noise, motion blur, and compression artifacts), resulting
    in the captured images being more complex than the simulated images. In order
    to alleviate these problems and train a more effective and general SISR model,
    some works model the degradation mode as a combination of several operations (Eq. [1](#S2.E1
    "In 2.1 Problem Definition ‣ 2 Problem Setting and Related Works ‣ From Beginner
    to Master: A Survey for Deep Learning-based Single-Image Super-Resolution")).
    Based on this degradation formula, three most widely used degradation modes have
    been proposed: BI, BD, and DN. Among them, BI is the most widely used degraded
    mode to simulate LR images, which is essentially a bicubic downsampling operation.
    For BD, the HR images are blurred by a Gaussian kernel of size $7\times 7$ with
    standard deviation 1.6 and then downsampled with the scaling factor of $\times
    3$. To obtain DN mode LR images, the bicubic downsampling is performed on the
    HR image with scaling factor $\times 3$, and then the Gaussian noise with noise
    $level=30$ is added into the image.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SISR任务的特殊性，难以构建大规模配对的真实SR数据集。因此，研究人员经常在上述数据集上应用降级模式，以获得相应的降级图像来构建配对数据集。然而，现实世界中的图像很容易受到各种因素的干扰（例如传感器噪声，运动模糊和压缩伪影），导致捕捉到的图像比模拟的图像更复杂。为了缓解这些问题并训练更有效和通用的SISR模型，一些作品将降级模式建模为几种操作的组合（见Eq. [1](#S2.E1
    "在2.1问题定义‣2问题设定和相关工作‣从初学者到专家：基于深度学习的单图像超分辨率的调查")）。基于这个降级公式，提出了三种最广泛使用的降级模式：BI、BD和DN。其中，BI是最广泛使用的降级模式，用于模拟LR图像，实质上是双三次降采样操作。对于BD，HR图像通过一个大小为$7\times
    7$，标准差为1.6的高斯核模糊后，再以3倍的缩放因子进行降采样。为了获得DN模式的LR图像，先对HR图像进行3倍缩放的双三次降采样操作，然后在图像中加入$level=30$的高斯噪声。
- en: 2.3 Upsampling Methods
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 上采样方法
- en: 'The purpose of SISR is to enlarge a smaller size image into a larger one and
    to keep it as accurate as possible. Therefore, enlargement operation, also called
    upsampling, is an important step in SISR. The current upsampling mechanisms can
    be divided into four types: pre-upsampling SR, post-upsampling SR, progressive
    upsampling SR, and iterative up-and-down sampling SR. In this section, we will
    talk about several kinds of upsampling methods that support these upsampling mechanisms.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: SISR的目的是将较小尺寸的图像放大成较大尺寸并尽可能准确地保持其信息。因此，放大操作，也称为上采样，是SISR中的一个重要步骤。当前的上采样机制可分为四种类型：预上采样SR、后上采样SR、渐进式上采样SR和迭代上下采样SR。在本节中，我们将讨论支持这些上采样机制的几种上采样方法。
- en: 2.3.1 Interpolation Methods
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 插值方法
- en: Interpolation is the most widely used upsampling method. The current mainstream
    of interpolation methods includes Nearest-neighbor Interpolation, Bilinear Interpolation,
    and Bicubic Interpolation. Being highly interpretable and easy to implement, these
    methods are still widely used today. Among them, Nearest-neighbor Interpolation
    is a simple and intuitive algorithm that selects the nearest pixel value for each
    position to be interpolated, which has fast execution time but has difficulty
    in producing high-quality results. Bilinear Interpolation sequentially performs
    linear interpolation operations on the two axes of the image. This method can
    obtain better results than nearest-neighbor interpolation while maintaining a
    relatively fast speed. Bicubic Interpolation performs cubic interpolation on each
    of the two axes. Compared with Bilinear, the results of Bicubic are smoother with
    fewer artifacts, but slower than other interpolation methods. Interpolation is
    also the mainstream method for constructing SISR paired datasets, and is widely
    used in the data pre-processing of CNN-based SISR models.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 插值是最广泛使用的上采样方法。目前主流的插值方法包括最近邻插值、双线性插值和双三次插值。这些方法具有很高的可解释性且易于实现，因此今天仍被广泛使用。其中，最近邻插值是一种简单直观的算法，它为每个待插值的位置选择最近的像素值，执行时间较快，但生成高质量结果较为困难。双线性插值在图像的两个轴上依次执行线性插值操作。这种方法可以比最近邻插值获得更好的结果，同时保持相对较快的速度。双三次插值在两个轴上执行三次插值。与双线性插值相比，双三次插值的结果更平滑，伪影更少，但速度较慢。插值也是构建
    SISR 配对数据集的主流方法，并广泛应用于基于 CNN 的 SISR 模型的数据预处理。
- en: '![Refer to caption](img/937835efbbf9911cdbbc056d71114d7c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/937835efbbf9911cdbbc056d71114d7c.png)'
- en: 'Figure 4: Two kinds of transposed convolutional layers.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：两种转置卷积层。
- en: 2.3.2 Transposed Convolutional Layers
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 转置卷积层
- en: 'As shown in Fig. [4](#S2.F4 "Figure 4 ‣ 2.3.1 Interpolation Methods ‣ 2.3 Upsampling
    Methods ‣ 2 Problem Setting and Related Works ‣ From Beginner to Master: A Survey
    for Deep Learning-based Single-Image Super-Resolution"), researchers usually consider
    two kinds of transposed convolution operations: one adds padding around the input
    matrix and then applies the convolution operation, the other adds padding between
    the values of the input matrix followed by the direct convolution operation. The
    latter is also called fractionally strided convolution, since it works like doing
    convolution with a stride less than one. In the transposed convolutional layer,
    the upsampling level is controlled by the size of padding and it is essentially
    the opposite of the operation of the normal convolutional layer. Transposed convolutional
    layer is first proposed in FSRCNN [[13](#bib.bib13)] and widely used in DL-based
    SISR models.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [4](#S2.F4 "图 4 ‣ 2.3.1 插值方法 ‣ 2.3 上采样方法 ‣ 2 问题设置与相关工作 ‣ 从入门到精通：基于深度学习的单图像超分辨率调查")
    所示，研究人员通常考虑两种转置卷积操作：一种是在输入矩阵周围添加填充，然后应用卷积操作，另一种是在输入矩阵的值之间添加填充，之后直接进行卷积操作。后一种也称为分数步幅卷积，因为它的工作方式类似于使用小于一的步幅进行卷积。在转置卷积层中，上采样水平由填充的大小控制，实际上是普通卷积层操作的反向。转置卷积层首次在
    FSRCNN [[13](#bib.bib13)] 中提出，并广泛应用于基于 DL 的 SISR 模型。
- en: 2.3.3 Sub-pixel Convolutional Layer
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 亚像素卷积层
- en: 'In ESPCN [[33](#bib.bib33)], Shi *et al.* proposed an efficient sub-pixel convolutional
    layer. Instead of increasing the resolution by directly increasing the number
    of LR feature maps, sub-pixel first increases the dimension of LR feature maps,
    i.e., the number of the LR feature maps, and then a periodic shuffling operator
    is used to rearrange these points in the expanded feature maps to obtain the HR
    output (Fig. [5](#S2.F5 "Figure 5 ‣ 2.3.3 Sub-pixel Convolutional Layer ‣ 2.3
    Upsampling Methods ‣ 2 Problem Setting and Related Works ‣ From Beginner to Master:
    A Survey for Deep Learning-based Single-Image Super-Resolution")). In detail,
    the formulation of the sub-pixel convolutional layer can be defined as follow:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ESPCN [[33](#bib.bib33)] 中，Shi *等人* 提出了一个高效的亚像素卷积层。亚像素方法不是通过直接增加 LR 特征图的数量来提高分辨率，而是首先增加
    LR 特征图的维度，即 LR 特征图的数量，然后使用周期性重排操作将这些点在扩展特征图中重新排列，以获得 HR 输出（图 [5](#S2.F5 "图 5 ‣
    2.3.3 亚像素卷积层 ‣ 2.3 上采样方法 ‣ 2 问题设置与相关工作 ‣ 从入门到精通：基于深度学习的单图像超分辨率调查")）。详细来说，亚像素卷积层的公式可以定义如下：
- en: '|  | $I_{SR}=f^{L}(I_{x})=\mathcal{PS}(W_{L}*f^{L-1}(I_{x})+b_{L}),$ |  | (3)
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{SR}=f^{L}(I_{x})=\mathcal{PS}(W_{L}*f^{L-1}(I_{x})+b_{L}),$ |  | (3)
    |'
- en: where $\mathcal{PS}$ denotes the periodic shuffling operator, which transfers
    a $h\times w\times C\cdot r^{2}$ tensor to a tensor of shape $rh\times rw\times
    C$, and $rh\times rw$ is explicitly the size of HR image, $C$ is the dimension
    of operating channels. In addition, the convolutional filter $W_{L}$ has the shape
    $n_{L-1}\times r^{2}C\times K_{L}\times K_{L}$, where $n_{L}$ is the number of
    feature maps in the $L-1$ layer. Compared with the transposed convolutional layer,
    the sub-pixel convolutional layer shows better efficiency, so it is also widely
    used in DL-based SISR models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{PS}$ 表示周期性洗牌操作符，它将一个 $h\times w\times C\cdot r^{2}$ 张量转换为 $rh\times
    rw\times C$ 的张量，其中 $rh\times rw$ 明确表示高分辨率图像的大小，$C$ 是操作通道的维度。此外，卷积滤波器 $W_{L}$ 的形状是
    $n_{L-1}\times r^{2}C\times K_{L}\times K_{L}$，其中 $n_{L}$ 是 $L-1$ 层中特征图的数量。与转置卷积层相比，亚像素卷积层显示出更好的效率，因此在基于深度学习的
    SISR 模型中也得到了广泛应用。
- en: '![Refer to caption](img/0921326903a7868420191714d0adb835.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0921326903a7868420191714d0adb835.png)'
- en: 'Figure 5: Principle of the sub-pixel convolutional layer.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：亚像素卷积层的原理。
- en: 2.4 Optimization Objective
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 优化目标
- en: Evaluation and parameter up-gradation are the important steps in all DL-based
    models. In this section, we will introduce the necessary procedures during the
    model training.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 评估和参数更新是所有基于深度学习的模型中的重要步骤。在本节中，我们将介绍模型训练过程中的必要步骤。
- en: 2.4.1 Learning Strategy
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 学习策略
- en: According to different strategies, the DL-based SISR models can be mainly divided
    into supervised learning methods and unsupervised learning methods.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 根据不同的策略，基于深度学习的 SISR 模型主要可以分为监督学习方法和无监督学习方法。
- en: 'Supervised Learning: In supervised learning SISR, researchers compute the reconstruction
    error between the ground-truth image $I_{y}$ and the reconstructed image $I_{SR}$:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习：在监督学习 SISR 中，研究人员计算真实图像 $I_{y}$ 和重建图像 $I_{SR}$ 之间的重建误差：
- en: '|  | $\hat{\theta}_{\mathcal{F}}=\mathop{\arg\min}_{\theta_{\mathcal{F}}}\mathcal{L}(I_{SR},I_{y}).$
    |  | (4) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\theta}_{\mathcal{F}}=\mathop{\arg\min}_{\theta_{\mathcal{F}}}\mathcal{L}(I_{SR},I_{y}).$
    |  | (4) |'
- en: 'Alternatively, researchers may sometimes search for a mapping $\Phi$, such
    as a pre-trained neural network, to transform the images or image feature maps
    to some other space and then compute the error:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，研究人员有时可能会寻找一个映射 $\Phi$，例如预训练的神经网络，将图像或图像特征图转换到其他空间，然后计算误差：
- en: '|  | $\hat{\theta}_{\mathcal{F}}=\mathop{\arg\min}_{\theta_{\mathcal{F}}}\mathcal{L}(\Phi(I_{SR}),\Phi(I_{y})).$
    |  | (5) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\theta}_{\mathcal{F}}=\mathop{\arg\min}_{\theta_{\mathcal{F}}}\mathcal{L}(\Phi(I_{SR}),\Phi(I_{y})).$
    |  | (5) |'
- en: Among them, $\mathcal{L}$ is the loss function which is used to minimize the
    gap between the reconstructed image and ground-truth image. According to different
    loss functions, the model can achieve different performances. Therefore, an effective
    loss function is also crucial for SISR.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{L}$ 是损失函数，用于最小化重建图像与真实图像之间的差距。根据不同的损失函数，模型可以实现不同的性能。因此，有效的损失函数对于单图像超分辨率（SISR）也至关重要。
- en: 'Unsupervised Learning: In unsupervised learning SISR, the way of evaluation
    and parameter up-gradation is changing by different unsupervised learning algorithms.
    For example, ZSSR [[34](#bib.bib34)] uses the test image and its downscaling images
    with the data augmentation methods to build the “training dataset” and then applies
    the loss function to optimize the model. In CinCGAN [[35](#bib.bib35)], a model
    consists of two CycleGAN [[36](#bib.bib36)] is proposed, where parameters are
    upgraded through optimizing the generator-adversarial loss, the cycle consistency
    loss, the identity loss, and the total variation loss together in each cycle.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习：在无监督学习 SISR 中，评估和参数更新的方法通过不同的无监督学习算法发生变化。例如，ZSSR [[34](#bib.bib34)] 使用测试图像及其下采样图像以及数据增强方法来构建“训练数据集”，然后应用损失函数来优化模型。在
    CinCGAN [[35](#bib.bib35)] 中，提出了一个由两个 CycleGAN [[36](#bib.bib36)] 组成的模型，其中通过在每个周期中优化生成对抗损失、循环一致性损失、身份损失和总变差损失来更新参数。
- en: 2.4.2 Loss Function
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 损失函数
- en: In the SISR task, the loss function is used to guide the iterative optimization
    process of the model by computing some kind of error. Meanwhile, compared with
    a single loss function, researchers find that a combination of multiple loss functions
    can better reflect the situation of image restoration. In this section, we will
    briefly introduce several commonly used loss functions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SISR 任务中，损失函数用于通过计算某种误差来指导模型的迭代优化过程。同时，与单一损失函数相比，研究人员发现多种损失函数的组合可以更好地反映图像恢复的情况。在这一部分，我们将简要介绍几种常用的损失函数。
- en: 'Pixel Loss: Pixel loss is the simplest and most popular type among loss functions
    in SISR, which aims to measure the difference between two images on pixel basis
    so that these two images can converge as close as possible. It mainly includes
    the L1 loss, Mean Square Error (MSE Loss), and Charbonnier loss (a differentiable
    variant of L1 loss):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 像素损失：像素损失是 SISR 中最简单和最流行的损失函数类型，它旨在衡量两张图像之间的像素级差异，使这两张图像尽可能接近。它主要包括 L1 损失、均方误差（MSE
    损失）和 Charbonnier 损失（一种 L1 损失的可微分变体）：
- en: '|  | $\mathcal{L}_{L1}(I_{SR},I_{y})=\frac{1}{hwc}\sum_{i,j,k}\left&#124;I_{SR}^{i,j,k}-I_{y}^{i,j,k}\right&#124;,$
    |  | (6) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{L1}(I_{SR},I_{y})=\frac{1}{hwc}\sum_{i,j,k}\left\|I_{SR}^{i,j,k}-I_{y}^{i,j,k}\right\|,$
    |  | (6) |'
- en: '|  | $\mathcal{L}_{MSE}(I_{SR},I_{y})=\frac{1}{hwc}\sum_{i,j,k}(I_{SR}^{i,j,k}-I_{y}^{i,j,k})^{2},$
    |  | (7) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{MSE}(I_{SR},I_{y})=\frac{1}{hwc}\sum_{i,j,k}(I_{SR}^{i,j,k}-I_{y}^{i,j,k})^{2},$
    |  | (7) |'
- en: '|  | $\mathcal{L}_{Char}(I_{SR},I_{y})=\frac{1}{hwc}\sum_{i,j,k}\sqrt{(I_{SR}^{i,j,k}-I_{y}^{i,j,k})^{2}+\epsilon^{2}},$
    |  | (8) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{Char}(I_{SR},I_{y})=\frac{1}{hwc}\sum_{i,j,k}\sqrt{(I_{SR}^{i,j,k}-I_{y}^{i,j,k})^{2}+\epsilon^{2}},$
    |  | (8) |'
- en: where, $h$, $w$, and $c$ are the height, width, and the number of channels of
    the image. $\epsilon$ is a numerical stability constant, usually setting to $10^{-3}$.
    Since most mainstream image evaluation indicators are highly correlated with pixel-by-pixel
    differences, pixel loss is still widely sought after. However, the image reconstructed
    by this type of loss function usually lacks high-frequency details, so it is difficult
    to obtain excellent visual effects.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$h$、$w$ 和 $c$ 是图像的高度、宽度和通道数。$\epsilon$ 是一个数值稳定性常数，通常设置为 $10^{-3}$。由于大多数主流图像评估指标与逐像素差异高度相关，像素损失仍然广受青睐。然而，这种损失函数重建的图像通常缺乏高频细节，因此很难获得优异的视觉效果。
- en: 'Content Loss: Content loss is also called perceptual loss, which uses a pre-trained
    classification network to measure the semantic difference between images, and
    can be further expressed as the Euclidean distance between the high-level representations
    of these two images:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 内容损失：内容损失也称为感知损失，它使用预训练的分类网络来衡量图像之间的语义差异，并可以进一步表示为这两张图像的高层表示之间的欧几里得距离：
- en: '|  | $\mathcal{L}_{Cont}(I_{SR},I_{y},\phi)=\frac{1}{h_{l}w_{l}c_{l}}\sum_{i,j,k}(\phi^{i,j,k}_{(l)}(I_{SR})-\phi^{i,j,k}_{(l)}(I_{y})),$
    |  | (9) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{Cont}(I_{SR},I_{y},\phi)=\frac{1}{h_{l}w_{l}c_{l}}\sum_{i,j,k}(\phi^{i,j,k}_{(l)}(I_{SR})-\phi^{i,j,k}_{(l)}(I_{y})),$
    |  | (9) |'
- en: where $\phi$ represents the pre-trained classification network and $\phi_{(l)}(I_{HQ})$
    represents the high-level representation extracted from the $l$ layer of the network.
    $h_{l}$, $w_{l}$, and $c_{l}$ are the height, width, and the number of channels
    of the feature map in the $l$th layer respectively. With this method, the visual
    effects of these two images can be as consistent as possible. Among them, VGG [[37](#bib.bib37)]
    and ResNet [[38](#bib.bib38)] are the most commonly used pre-training classification
    networks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\phi$ 代表预训练的分类网络，$\phi_{(l)}(I_{HQ})$ 代表从网络的第 $l$ 层提取的高层表示。$h_{l}$、$w_{l}$
    和 $c_{l}$ 分别是第 $l$ 层特征图的高度、宽度和通道数。使用这种方法，可以使这两张图像的视觉效果尽可能一致。其中，VGG [[37](#bib.bib37)]
    和 ResNet [[38](#bib.bib38)] 是最常用的预训练分类网络。
- en: 'Adversarial Loss: In order to make the reconstructed SR image more realistic,
    Generative Adversarial Networks (GANs [[39](#bib.bib39)]) have been proposed and
    introduced into various computer vision tasks. Specifically, GAN is composed of
    a generator and a discriminator. The generator is responsible for generating fake
    samples, and the discriminator is used to determine the authenticity of the generated
    samples. For example, the discriminative loss function based on cross-entropy
    is proposed by SRGAN [[38](#bib.bib38)]:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗损失：为了使重建的 SR 图像更逼真，已经提出并引入了生成对抗网络（GANs [[39](#bib.bib39)]) 到各种计算机视觉任务中。具体来说，GAN
    由生成器和判别器组成。生成器负责生成伪造样本，而判别器用于判断生成样本的真实性。例如，SRGAN [[38](#bib.bib38)] 提出了基于交叉熵的判别损失函数：
- en: '|  | $\mathcal{L}_{Adversarial}(I_{x},G,D)=\sum_{n=1}^{N}-logD(G(I_{x})),$
    |  | (10) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{Adversarial}(I_{x},G,D)=\sum_{n=1}^{N}-logD(G(I_{x})),$
    |  | (10) |'
- en: where $G(I_{LQ})$ is the reconstructed SR image, $G$ and $D$ represent the Generator
    and the Discriminator, respectively.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G(I_{LQ})$ 是重建的 SR 图像，$G$ 和 $D$ 分别表示生成器和判别器。
- en: 'Prior Loss Apart from the above loss functions, some prior knowledge can also
    be introduced into SISR models to participate in high-quality image reconstruction,
    such as sparse prior, gradient prior, and edge prior. Among them, gradient prior
    loss and edge prior loss are the most widely used prior loss functions, which
    are defined as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 先验损失 除了上述损失函数之外，还可以将一些先验知识引入到 SISR 模型中以参与高质量图像重建，例如稀疏先验、梯度先验和边缘先验。其中，梯度先验损失和边缘先验损失是最广泛使用的先验损失函数，其定义如下：
- en: '|  | $\small\mathcal{L}_{TV}(I_{SR})=\frac{1}{hwc}\sum_{i,j,k}\sqrt{(I_{SR}^{i,j+1,k}-I_{y}^{i,j,k})^{2}+(I_{SR}^{i+1,j,k}-I_{y}^{i,j,k})^{2}},$
    |  | (11) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathcal{L}_{TV}(I_{SR})=\frac{1}{hwc}\sum_{i,j,k}\sqrt{(I_{SR}^{i,j+1,k}-I_{y}^{i,j,k})^{2}+(I_{SR}^{i+1,j,k}-I_{y}^{i,j,k})^{2}},$
    |  | (11) |'
- en: '|  | $\mathcal{L}_{Edge}(I_{SR},I_{y},E)=\frac{1}{hwc}\sum_{i,j,k}\left&#124;E(I_{SR}^{i,j,k})-E(I_{y}^{i,j,k})\right&#124;.$
    |  | (12) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{Edge}(I_{SR},I_{y},E)=\frac{1}{hwc}\sum_{i,j,k}\left|E(I_{SR}^{i,j,k})-E(I_{y}^{i,j,k})\right|.$
    |  | (12) |'
- en: where $E$ is the image edge detector, and $E(I_{SR}^{i,j,k})$ and $E(I_{y}^{i,j,k})$
    are the image edges extracted by the detector. The purpose of the prior loss is
    to optimize some specific information of the image toward the expected target
    so that the model can converge faster and the reconstructed image will contain
    more texture details.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $E$ 是图像边缘检测器，$E(I_{SR}^{i,j,k})$ 和 $E(I_{y}^{i,j,k})$ 是由检测器提取的图像边缘。先验损失的目的是优化图像的某些特定信息，以期达到预期目标，使模型能够更快地收敛，并且重建的图像将包含更多的纹理细节。
- en: 2.5 Assessment Methods
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 评估方法
- en: The image quality assessment (IQA) can be generally divided into objective methods
    and subjective methods. Objective methods commonly use a specific formulation
    to compute the results, which are simple and fair, thus become the mainstream
    assessment method in SISR. However, they can only reflect the recovery of image
    pixels from a numerical point of view and are difficult to accurately measure
    the true visual effect of the image. In contrast, subjective methods are always
    based on human subjective judgments and more related to evaluate the perceptual
    quality of the image. Based on the pros and cons of the two types of methods mentioned
    above, several assessment methods are briefly introduced in the following with
    respect to the aspects of image reconstruction accuracy, image perceptual quality,
    and reconstruction efficiency.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图像质量评估（IQA）通常可以分为客观方法和主观方法。客观方法通常使用特定的公式来计算结果，这些方法简单且公平，因此成为 SISR 的主流评估方法。然而，它们只能从数值角度反映图像像素的恢复，难以准确测量图像的真实视觉效果。相比之下，主观方法始终基于人的主观判断，更相关于评估图像的感知质量。基于上述两种方法的优缺点，下面简要介绍了几种评估方法，涉及图像重建精度、图像感知质量和重建效率方面。
- en: 2.5.1 Image Reconstruction Accuracy
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.1 图像重建精度
- en: The assessment methods applied to evaluate image reconstruction accuracy are
    also called *Distortion measures*, which are full-reference. Specifically, given
    a distorted image $\hat{x}$ and a ground-truth reference image $x$, full-reference
    distortion quantifies the quality of $\hat{x}$ by measuring its discrepancy to
    $x$ [[40](#bib.bib40)] using different algorithms.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于评估图像重建精度的方法也称为*失真度量*，这些方法是全参考的。具体来说，给定一个失真图像 $\hat{x}$ 和一个真实参考图像 $x$，全参考失真通过测量
    $\hat{x}$ 与 $x$ 之间的差异[[40](#bib.bib40)] 来量化 $\hat{x}$ 的质量，使用不同的算法。
- en: 'Peak Signal-to-Noise Ratio (PSNR): PSNR is the most widely used IQA method
    in the SISR field, which can be easily defined via the mean squared error (MSE)
    between the ground truth image $I_{y}\in\mathbb{R}^{H\times W}$ and the reconstructed
    image $I_{SR}\in\mathbb{R}^{H\times W}$:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 峰值信噪比（PSNR）：PSNR 是超分辨率领域中最广泛使用的图像质量评估方法，通常通过真实图像 $I_{y}\in\mathbb{R}^{H\times
    W}$ 和重建图像 $I_{SR}\in\mathbb{R}^{H\times W}$ 之间的均方误差（MSE）来定义：
- en: '|  | $MSE=\frac{1}{HW}\sum_{i=0}^{H-1}\sum_{j=0}^{W-1}(I_{y}(i,j)-I_{SR}(i,j))^{2},$
    |  | (13) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $MSE=\frac{1}{HW}\sum_{i=0}^{H-1}\sum_{j=0}^{W-1}(I_{y}(i,j)-I_{SR}(i,j))^{2},$
    |  | (13) |'
- en: '|  | $PSNR=10\cdot\log_{10}(\frac{MAX^{2}}{MSE}),$ |  | (14) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $PSNR=10\cdot\log_{10}(\frac{MAX^{2}}{MSE}),$ |  | (14) |'
- en: where MAX is the maximum possile pixel of the image. Since PSNR is highly related
    to MSE, a model trained with the MSE loss will be expected to have high PSNR scores.
    Although higher PSNR generally indicates that the construction is of higher quality,
    it just considers the per-pixel MSE, which makes it fails to capture the perceptual
    differences [[41](#bib.bib41)].
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 MAX 是图像的最大可能像素。由于 PSNR 与 MSE 密切相关，因此用 MSE 损失训练的模型预计会有较高的 PSNR 分数。尽管较高的 PSNR
    通常表示重建质量较高，但它只考虑了每个像素的 MSE，因此未能捕捉感知差异 [[41](#bib.bib41)]。
- en: 'Structural Similarity index measure (SSIM): SSIM [[42](#bib.bib42)] is another
    popular assessment method that measures the similarity between two images on perceptual
    basis, including structures, luminance, and contrast. Different from PSNR, which
    calculates absolute errors on the pixel-level, SSIM suggests that there exists
    strong inter-dependencies among the pixels that are spatially close. These dependencies
    carry important information related to the structures perceptually. Thus the SSIM
    can be expressed as a weighted combination of three comparative measures:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结构相似性指数度量（SSIM）：SSIM [[42](#bib.bib42)] 是另一种流行的评估方法，用于在感知基础上测量两幅图像之间的相似性，包括结构、亮度和对比度。与在像素级别计算绝对误差的
    PSNR 不同，SSIM 认为空间上相近的像素之间存在强烈的相互依赖。这些依赖关系包含了与结构感知相关的重要信息。因此，SSIM 可以表示为三种比较度量的加权组合：
- en: '|  | $\begin{split}SSIM(I_{SR},I_{y})&amp;=(l(I_{SR},i_{y})^{\alpha}\cdot c(I_{SR},I_{y})^{\beta}\cdot
    s(I_{SR},I_{y})^{\gamma})\\ &amp;=\frac{(2\mu_{I_{SR}}\mu_{I_{y}}+c_{1})(2\sigma_{I_{SR}I_{y}}+c_{2})}{(\mu_{I_{SR}}^{2}+\mu_{I_{y}}^{2}+c_{1})(\sigma_{I_{SR}}^{2}+\sigma_{I_{y}}^{2}+c_{2})}.\end{split}$
    |  | (15) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}SSIM(I_{SR},I_{y})&=(l(I_{SR},i_{y})^{\alpha}\cdot c(I_{SR},I_{y})^{\beta}\cdot
    s(I_{SR},I_{y})^{\gamma})\\ & =\frac{(2\mu_{I_{SR}}\mu_{I_{y}}+c_{1})(2\sigma_{I_{SR}I_{y}}+c_{2})}{(\mu_{I_{SR}}^{2}+\mu_{I_{y}}^{2}+c_{1})(\sigma_{I_{SR}}^{2}+\sigma_{I_{y}}^{2}+c_{2})}.\end{split}$
    |  | (15) |'
- en: where $l$, $c$, and $s$ represent luminance, contrast, and structure between
    $I_{SR}$ and $I_{y}$, respectively. $\mu_{I_{SR}}$, $\mu_{I_{y}}$, $\sigma_{I_{SR}}^{2}$,
    $\sigma_{I_{y}}^{2}$, and $\sigma_{I_{SR}I_{y}}$ are the average($\mu$)/variance($\sigma^{2}$)/covariance($\sigma$)
    of the corresponding items.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l$、$c$ 和 $s$ 分别表示 $I_{SR}$ 和 $I_{y}$ 之间的亮度、对比度和结构。$\mu_{I_{SR}}$、$\mu_{I_{y}}$、$\sigma_{I_{SR}}^{2}$、$\sigma_{I_{y}}^{2}$
    和 $\sigma_{I_{SR}I_{y}}$ 分别是对应项目的均值($\mu$)、方差($\sigma^{2}$) 和协方差($\sigma$)。
- en: A higher SSIM indicates higher similarity between two images, which has been
    widely used due to its convenience and stable performance on evaluating the perceptual
    quality. In addition, there are also some variants of SSIM, such as Multi-Scale
    SSIM, which is conducted over multiple scales by a process of multiple stages
    of subsampling.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的 SSIM 表示两幅图像之间的相似性更高，由于其在评估感知质量方面的便利性和稳定性，SSIM 被广泛使用。此外，还有一些 SSIM 的变体，如多尺度
    SSIM，它通过多级子采样的过程在多个尺度上进行。
- en: 2.5.2 Image Perceptual Quality
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.2 图像感知质量
- en: Since the visual system of humans is complex and concerns many aspects to judge
    the differences between two images, i.e., the textures and flow inside the images,
    methods which pursue absolutely similarity differences (PSNR/SSIM) will not always
    perform well. Although distortion measures have been widely used, the improvement
    in reconstruction accuracy is not always accompanied by an improvement in visual
    quality. In fact, researchers have shown that the distortion and perceptual quality
    are at odds with each other in some cases [[40](#bib.bib40)]. The image perceptual
    quality of an image $\hat{x}$ is defined as the degree to which it looks like
    a natural image, which has nothing to do with its similarity to any reference
    image.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人类视觉系统复杂，涉及多个方面来判断两幅图像之间的差异，例如图像内部的纹理和流动，因此追求绝对相似性差异的方法（如 PSNR/SSIM）并不总是表现良好。尽管失真度量已被广泛使用，但重建精度的提升并不总是伴随着视觉质量的提升。实际上，研究人员已表明，在某些情况下，失真和感知质量是相悖的[[40](#bib.bib40)]。图像
    $\hat{x}$ 的感知质量定义为它看起来像自然图像的程度，这与其与任何参考图像的相似性无关。
- en: 'Mean Opinion Score (MOS): MOS is a subjective method that can straightforwardly
    evaluate perceptual quality. Specifically, a number of viewers rate their opinions
    on the quality of a set of images by Double-stimulus [[43](#bib.bib43)], i.e.,
    every viewer has both the source and test images. After all the viewers finishing
    ratings, the results are mapped onto numerical values and the average scores will
    be the final MOS. MOS is a time-consuming and expensive method as it requires
    manual participation. Meanwhile, MOS is also doubted to be unstable, since the
    MOS differences may be not noticeable to the users. Moreover, this method is too
    subjective to guarantee fairness.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 平均意见得分（MOS）：MOS 是一种主观方法，可以直接评估感知质量。具体而言，若干观众通过双刺激[[43](#bib.bib43)]对一组图像的质量进行评分，即每位观众都有源图像和测试图像。所有观众完成评分后，结果会映射到数值上，平均分数将成为最终的
    MOS。由于需要人工参与，MOS 是一种耗时且昂贵的方法。同时，MOS 也被怀疑不稳定，因为 MOS 差异可能对用户不明显。此外，该方法过于主观，难以保证公平性。
- en: 'Natural Image Quality Evaluator (NIQE): NIQE [[44](#bib.bib44)] is a completely
    blind image quality assessment method. Without the requirement of knowledge about
    anticipated distortions in the form of training examples and corresponding human
    opinion scores, NIQE only makes use of measurable deviations from statistical
    regularities observed in natural images. It extracts a set of local (quality-aware)
    features from images based on a natural scene statistic (NSS) model, then fits
    the feature vectors to a multivariate Gaussian (MVG) model. The quality of a test
    image is then predicted by the distance between its MVG model and the MVG model
    learned from a natural image:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 自然图像质量评估器（NIQE）：NIQE [[44](#bib.bib44)] 是一种完全盲目的图像质量评估方法。NIQE 不需要对预期失真的形式（如训练样本和相应的人类意见分数）有任何知识，只利用从自然图像中观察到的统计规律的可测量偏差。它基于自然场景统计（NSS）模型从图像中提取一组局部（质量感知）特征，然后将特征向量拟合到多元高斯（MVG）模型中。测试图像的质量通过其
    MVG 模型与从自然图像中学习到的 MVG 模型之间的距离来预测：
- en: '|  | $\small D(\nu_{1},\nu_{2},\Sigma_{1},\Sigma_{2})=\sqrt{((\nu_{1}-\nu_{2})^{T}(\frac{\Sigma_{1}+\Sigma_{2}}{2})^{-1}(\nu_{1}-\nu_{2}))},$
    |  | (16) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small D(\nu_{1},\nu_{2},\Sigma_{1},\Sigma_{2})=\sqrt{((\nu_{1}-\nu_{2})^{T}(\frac{\Sigma_{1}+\Sigma_{2}}{2})^{-1}(\nu_{1}-\nu_{2}))},$
    |  | (16) |'
- en: where $\nu_{1}$, $\nu_{2}$ and $\Sigma_{1}$, $\Sigma_{2}$ are the mean vectors
    and covariance matrices of the HR and SR image’s MVG model. Notice that, a higher
    NQIE index indicates lower image perceptual quality. Compared with MOS, NIQE is
    a more convenient perceptual-evaluation method.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\nu_{1}$、$\nu_{2}$ 和 $\Sigma_{1}$、$\Sigma_{2}$ 分别是 HR 和 SR 图像 MVG 模型的均值向量和协方差矩阵。请注意，较高的
    NQIE 指数表示较低的图像感知质量。与 MOS 相比，NIQE 是一种更方便的感知评估方法。
- en: 'Ma: Ma *et al.* [[45](#bib.bib45)] proposed a learning-based no-reference image
    quality assessment. It is designed to focus on SR images, while other learning-based
    methods are applied to images degraded by noise, compression, or fast fading rather
    than SR. It learns from perceptual scores based on human subject studies involving
    a large number of SR images. And then it quantifies the SR artifacts through three
    types of statistical properties, i.e., local/global frequency variations and spatial
    discontinuity. Then these features are modeled by three independent learnable
    regression forests respectively to fit the perceptual scores of SR images, $\hat{y}_{n}(n=1,2,3)$.
    The final predicted quality score is $\hat{y}=\sum_{n}\lambda_{n}\cdot\hat{y}_{n}$,
    and the weight $\lambda$ is learned by minimizing $\lambda^{*}=\mathop{\arg\min}_{\lambda}(\sum_{n}\lambda_{n}\cdot\hat{y}_{n}-y)^{2}$.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ma: Ma *等人* [[45](#bib.bib45)] 提出了基于学习的无参考图像质量评估方法。该方法旨在关注SR图像，而其他基于学习的方法则应用于噪声、压缩或快速衰减的退化图像，而不是SR图像。它从涉及大量SR图像的人体研究的感知评分中学习。然后，它通过三种统计属性量化SR伪影，即局部/全局频率变化和空间不连续性。接着，这些特征分别由三个独立的可学习回归森林建模，以拟合SR图像的感知评分，$\hat{y}_{n}(n=1,2,3)$。最终预测的质量评分是
    $\hat{y}=\sum_{n}\lambda_{n}\cdot\hat{y}_{n}$，权重$\lambda$通过最小化$\lambda^{*}=\mathop{\arg\min}_{\lambda}(\sum_{n}\lambda_{n}\cdot\hat{y}_{n}-y)^{2}$来学习。'
- en: Ma performs well on matching the perceptual scores of SR images, but it is still
    limited compared with other learning-based no-reference methods, since it can
    only assess the quality degradation arising from the distortion types on which
    they have been trained.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Ma 在匹配 SR 图像的感知评分方面表现良好，但与其他基于学习的无参考方法相比，仍然有限，因为它只能评估其训练的失真类型所引起的质量下降。
- en: 'PI: In the 2018 PIRM Challenge on Perceptual Image Super-Resolution [[30](#bib.bib30)],
    perception index (PI) is first proposed to evaluate the perceptual quality. It
    is a combination of the no-reference image quality measures Ma and NIQE:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 'PI: 在 2018 年 PIRM 挑战赛上首次提出感知指数（PI）来评估感知质量 [[30](#bib.bib30)]。它是无参考图像质量测量 Ma
    和 NIQE 的结合：'
- en: '|  | $PI=\frac{1}{2}((10-Ma)+NIQE).$ |  | (17) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $PI=\frac{1}{2}((10-Ma)+NIQE).$ |  | (17) |'
- en: A lower PI indicates better perceptual quality. This is a new image quality
    evaluation standard, which has been greatly promoted and used in recent years.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的 PI 表示更好的感知质量。这是一个新的图像质量评估标准，近年来得到了广泛推广和应用。
- en: Apart from the aforementioned evaluation methods, some new methods have also
    been proposed over these years. For example, Zhang *et al.* [[46](#bib.bib46)]
    proposed $Ranker$ to learn the ranking orders of NR-IQA methods (i.e., NIQE) on
    the results of some perceptual SR models. Zhang *et al.*[[47](#bib.bib47)] introduced
    a new dataset of human perceptual similarity judgments. Meanwhile, a perceptual
    evaluation metric, Learned Perceptual Image Patch Similarity (LPIPS), is constructed
    by learning the perceptual judgement in this dataset. In summary, how to measure
    the perceptual quality of SR images more accurately and efficiently is an important
    issue that needs to be explored.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述评估方法外，这些年还提出了一些新方法。例如，Zhang *等人* [[46](#bib.bib46)] 提出了 $Ranker$ 来学习 NR-IQA
    方法（即 NIQE）在某些感知 SR 模型结果中的排名顺序。Zhang *等人*[[47](#bib.bib47)] 引入了一个新的人体感知相似性判断数据集。同时，构建了一种感知评估指标，学习感知图像块相似性（LPIPS），通过学习该数据集中的感知判断来实现。总之，更准确和高效地测量
    SR 图像的感知质量是一个需要探索的重要问题。
- en: 2.5.3 Reconstruction Efficiency
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.3 重建效率
- en: Although designing deeper networks is the easiest way to obtain better reconstruction
    performance, it cannot be ignored that these models will also bring more parameters,
    execution time, and computational costs. In order to broaden the practical application
    of SISR, we need to consider the trade-off between the model performance and model
    complexity. Therefore, it is important to evaluate the reconstruction efficiency
    by the following basic assessments.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管设计更深的网络是获得更好重建性能的最简单方法，但不可忽视的是，这些模型还会带来更多的参数、执行时间和计算成本。为了扩大 SISR 的实际应用，我们需要考虑模型性能与模型复杂性之间的权衡。因此，通过以下基本评估来评估重建效率是重要的。
- en: 'Model Size: The model size is related to the storage that the devices need
    to store the data. A model containing more parameters is harder for the device
    with limited hardware to run it. Therefore, building lightweight models is conducive
    to the promotion and application of the algorithm. Among all the indicators, the
    parameter quantity of the model is the most intuitive indicator to measure the
    model size.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 模型大小：模型大小与设备需要存储数据的存储空间相关。包含更多参数的模型对硬件有限的设备来说更难以运行。因此，构建轻量级模型有助于算法的推广和应用。在所有指标中，模型的参数量是衡量模型大小的最直观指标。
- en: 'Execution Time: Usually, a lightweight model tends to require a short execution
    time, but the emergence of complex strategies such as the attention mechanism
    has broken this balance. In other words, when some complex operations are introduced
    into the model, a lightweight network may also require a long execution time.
    Therefore, it is critically important to evaluate the execution time of the model.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时间：通常，轻量级模型往往需要较短的执行时间，但复杂策略如注意力机制的出现打破了这种平衡。换句话说，当模型中引入一些复杂操作时，轻量级网络也可能需要较长的执行时间。因此，评估模型的执行时间至关重要。
- en: 'Mult-Adds: The number of multiply-accumulate operations, or Mult-Adds, is always
    used to measure the model computation since operations in the CNN model are mainly
    multiplications and additions. The value of Mult-Adds is related to the speed
    or the time needed to run the model.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Mult-Adds：乘加操作的数量或Mult-Adds通常用于衡量模型计算，因为CNN模型中的操作主要是乘法和加法。Mult-Adds的值与运行模型所需的速度或时间相关。
- en: In summary, the trade-off between the model performance and model complexity
    is still need to be concerned.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，模型性能与模型复杂性之间的权衡仍然需要关注。
- en: 3 Single-Image Super-Resolution
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 单幅图像超分辨率
- en: 3.1 Benchmark framework for DL-based SISR
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于DL的SISR基准框架
- en: In 2014, Dong *et al.* [[9](#bib.bib9)] proposed the Super-Resolution Convolutional
    Neural Network (SRCNN). SRCNN is the first CNN-based SISR model. It shows that
    a deep CNN model is equivalent to the sparse-coding-based method, which is an
    example-based method for SISR. Recently, more and more SISR models treat it as
    an end-to-end learning task. Therefore, building a deep neural network to directly
    learn the mapping between LR and HR images has become the mainstream method in
    SISR. Motivated by SRCNN, CNN-based SISR methods are blooming and constantly refreshing
    the best results.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，Dong *等人* [[9](#bib.bib9)] 提出了超分辨率卷积神经网络（SRCNN）。SRCNN是第一个基于CNN的SISR模型。它表明深度CNN模型相当于基于稀疏编码的方法，这是一种基于示例的SISR方法。最近，越来越多的SISR模型将其视为端到端学习任务。因此，构建深度神经网络直接学习LR和HR图像之间的映射已成为SISR中的主流方法。受到SRCNN的启发，基于CNN的SISR方法正蓬勃发展，不断刷新最佳结果。
- en: 'According to different targets, we divide the DL-based SISR models into four
    categories: reconstruction efficiency methods, reconstruction accuracy methods,
    perceptual quality methods, and further improvement methods.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 根据不同目标，我们将基于DL的SISR模型分为四类：重建效率方法、重建准确性方法、感知质量方法和进一步改进方法。
- en: 3.2 Reconstruction Efficiency Methods
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 重建效率方法
- en: The problem of low accuracy caused by hardware limitations raises the demand
    for research on efficient SISR models. Therefore, designing lightweight SISR models
    that can achieve the same or even better performance than their cumbersome counterparts
    is urgently needed. In this section, we will discuss some methods that contribute
    to efficient network structure design.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件限制导致的低精度问题提高了对高效SISR模型研究的需求。因此，设计轻量级的SISR模型以实现与繁重模型相同甚至更好的性能是迫切需要的。在本节中，我们将讨论一些有助于高效网络结构设计的方法。
- en: '![Refer to caption](img/4fa7cebee91a869826e6e0373ed42313.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4fa7cebee91a869826e6e0373ed42313.png)'
- en: 'Figure 6: Sketch of residual learning architecture / residual block.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：残差学习架构/残差块的示意图。
- en: 3.2.1 Residual Learning
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 残差学习
- en: In SRCNN, researchers find that better reconstruction performance can be obtained
    by adding more convolutional layers to increase the receptive field. However,
    directly stacking the layers will cause vanishing/exploding gradients and degradation
    problem [[48](#bib.bib48)]. Meanwhile, adding more layers will lead to a higher
    training error and more expensive computational cost.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SRCNN 中，研究人员发现通过添加更多的卷积层来增加感受野，可以获得更好的重建性能。然而，直接堆叠层会导致梯度消失/爆炸和退化问题 [[48](#bib.bib48)]。同时，添加更多层会导致更高的训练误差和更昂贵的计算成本。
- en: 'In ResNet [[49](#bib.bib49)], He *et al.* proposed a residual learning framework,
    where a residual mapping is desired instead of fitting the whole underlying mapping
    (Fig. [6](#S3.F6 "Figure 6 ‣ 3.2 Reconstruction Efficiency Methods ‣ 3 Single-Image
    Super-Resolution ‣ From Beginner to Master: A Survey for Deep Learning-based Single-Image
    Super-Resolution")). In SISR, as LR image and HR image share most of the same
    information, it is easy to explicitly model the residual image between LR and
    HR images. Residual learning enables deeper networks and remits the problem of
    gradient vanishing and degradation. With the help of residual learning, Kim [[50](#bib.bib50)]
    proposed a very deep super-resolution network, also known as VDSR. For the convenience
    of network design, the residual block [[49](#bib.bib49)] has gradually become
    the basic unit in the network structure. In the convolutional branch, it usually
    has two $3\times 3$ convolutional layers, two batch normalization layers, and
    one ReLU activation function in between. It is worth noting that the batch normalization
    layer is often removed in the SISR task since EDSR [[51](#bib.bib51)] points out
    that the batch normalization layer consumes more memory but will not improve the
    model performance.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ResNet [[49](#bib.bib49)] 中，He *等人* 提出了一个残差学习框架，其中希望得到一个残差映射，而不是拟合整个基础映射（图
    [6](#S3.F6 "图 6 ‣ 3.2 重建效率方法 ‣ 3 单图像超分辨率 ‣ 从初学者到大师：基于深度学习的单图像超分辨率调查")）。在 SISR
    中，由于 LR 图像和 HR 图像共享大部分相同的信息，因此很容易明确建模 LR 和 HR 图像之间的残差图像。残差学习使得网络更深，并缓解了梯度消失和退化的问题。在残差学习的帮助下，Kim
    [[50](#bib.bib50)] 提出了一个非常深的超分辨率网络，也称为 VDSR。为了方便网络设计，残差块 [[49](#bib.bib49)] 已逐渐成为网络结构中的基本单元。在卷积分支中，它通常具有两个
    $3\times 3$ 的卷积层、两个批量归一化层和一个 ReLU 激活函数。值得注意的是，批量归一化层在 SISR 任务中通常会被移除，因为 EDSR [[51](#bib.bib51)]
    指出，批量归一化层会消耗更多的内存，但不会提高模型性能。
- en: 'Global and Local Residual Learning: Global residual learning is a skip-connection
    from input to the final reconstruction layer, which helps improve the transmission
    of information from input to output and reduce the loss of information to a certain
    extent. However, as the network becomes deeper, a significant amount of image
    details are inevitably lost after going through so many layers. Therefore, the
    local residual learning is proposed, which is performed in every few stacked layers
    instead of from input to output. In this approach, a multi-path mode is formed
    and rich image details are carried and also helps gradient flow. Furthermore,
    many new feature extraction modules have introduced the local residual learning
    to reinforce strong learning capabilities [[52](#bib.bib52), [53](#bib.bib53)].
    Of course, combining local residual learning and global residual learning is also
    highly popular now [[38](#bib.bib38), [51](#bib.bib51), [53](#bib.bib53)].'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 全局和局部残差学习：全局残差学习是一个从输入到最终重建层的跳跃连接，这有助于改善信息从输入到输出的传输，并在一定程度上减少信息的丢失。然而，随着网络的加深，经过这么多层后，图像细节不可避免地会丢失。因此，提出了局部残差学习，它在每隔几层的堆叠层中进行，而不是从输入到输出。在这种方法中，形成了多路径模式，携带丰富的图像细节，并且有助于梯度流动。此外，许多新的特征提取模块引入了局部残差学习以增强强大的学习能力[[52](#bib.bib52),
    [53](#bib.bib53)]。当然，现在结合局部残差学习和全局残差学习也非常流行[[38](#bib.bib38), [51](#bib.bib51),
    [53](#bib.bib53)]。
- en: 'Residual Scaling: In EDSR [[51](#bib.bib51)], Lim *et al.* found that increasing
    the feature maps, i.e., channel dimension, above a certain level would make the
    training procedure numerical unstable. To solve such issues, they adopted the
    residual scaling [[54](#bib.bib54)], where the residuals are scaled down by multiplying
    a constant between 0 and 1 before adding them to the main path. With the help
    of this residual scaling method, the model performance can be further improved.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 残差缩放：在 EDSR [[51](#bib.bib51)] 中，Lim *et al.* 发现将特征图（即通道维度）增加到某个水平以上会使训练过程数值不稳定。为了解决这些问题，他们采用了残差缩放
    [[54](#bib.bib54)]，其中残差在添加到主路径之前通过乘以 0 和 1 之间的常数进行缩放。借助这一残差缩放方法，模型性能可以进一步提高。
- en: '![Refer to caption](img/b7e583277d2cfb963bfb2061f0172d51.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b7e583277d2cfb963bfb2061f0172d51.png)'
- en: 'Figure 7: The model structure of DRRN, where the shaded part denotes the recursive
    block and the parameters in the dashed box are sharing.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：DRRN 的模型结构，其中阴影部分表示递归块，虚线框中的参数是共享的。
- en: 3.2.2 Recursive Learning
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 递归学习
- en: 'In order to obtain a large receptive field without increasing model parameters,
    recursive learning is proposed for SISR, where the same sub-modules are repeatedly
    applied in the network and they share the same parameters. In other worlds, a
    recursive block is a collection of recursive units, where the corresponding structures
    among these recursive units share the same parameters. For instance, the same
    convolutional layer is applied 16 times in DRCN [[55](#bib.bib55)], resulting
    in a 41 $\times$ 41 size receptive field. However, too many stacked layers in
    recursive learning based model will still cause the problem of vanishing/exploding
    gradient. Therefore, in DRRN [[56](#bib.bib56)], the recursive block is conducted
    based on residual learning (Fig. [7](#S3.F7 "Figure 7 ‣ 3.2.1 Residual Learning
    ‣ 3.2 Reconstruction Efficiency Methods ‣ 3 Single-Image Super-Resolution ‣ From
    Beginner to Master: A Survey for Deep Learning-based Single-Image Super-Resolution")).
    Recently, more and more models introduce the residual learning strategy in their
    recursive units, such as MemNet [[57](#bib.bib57)], CARN [[58](#bib.bib58)], and
    SRRFN [[59](#bib.bib59)].'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在不增加模型参数的情况下获得较大的感受野，提出了递归学习用于单图像超分辨率（SISR），其中相同的子模块在网络中重复应用，并且它们共享相同的参数。换句话说，递归块是递归单元的集合，其中这些递归单元之间的相应结构共享相同的参数。例如，在
    DRCN [[55](#bib.bib55)] 中，相同的卷积层被应用了 16 次，从而得到一个 41 $\times$ 41 的感受野。然而，在基于递归学习的模型中，堆叠过多的层仍然会导致梯度消失/爆炸的问题。因此，在
    DRRN [[56](#bib.bib56)] 中，递归块是基于残差学习进行的（见图 [7](#S3.F7 "图 7 ‣ 3.2.1 残差学习 ‣ 3.2
    重建效率方法 ‣ 3 单图像超分辨率 ‣ 从初学者到大师：基于深度学习的单图像超分辨率综述")）。最近，越来越多的模型在其递归单元中引入了残差学习策略，如
    MemNet [[57](#bib.bib57)]、CARN [[58](#bib.bib58)] 和 SRRFN [[59](#bib.bib59)]。
- en: '![Refer to caption](img/d1a530271eda6de3d88c7477a026265f.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d1a530271eda6de3d88c7477a026265f.png)'
- en: 'Figure 8: The structure of the hierarchical feature distillation block (HFDB).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：层次特征蒸馏块（HFDB）的结构。
- en: 3.2.3 Gating Mechanism
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 门控机制
- en: Skip connection in the above residual learning tends to make the channel dimension
    of the output features extremely high. If such a high dimension channel remains
    the same in the following layers, the computational cost will be terribly large
    and therefore will affect the reconstruction efficiency and performance. Intuitively,
    the output features after the skip connection should be efficiently re-fused instead
    of simply concatenated.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 上述残差学习中的跳跃连接往往使输出特征的通道维度极高。如果在后续层中这种高维通道保持不变，计算成本将会非常大，从而影响重建效率和性能。直观上，跳跃连接后的输出特征应该被高效地重新融合，而不是简单地拼接。
- en: 'To solve this issue, researchers recommend using the gating mechanism to adaptively
    extract and learn more efficient information. Most of the time, a $1\times 1$
    convolutional layer is adopted to accomplish the gating mechanism, which can reduce
    the channel dimension and leave more effective information. In SRDenseNet [[60](#bib.bib60)]
    and MSRN [[52](#bib.bib52)], such $1\times 1$ convolutional layer acts as a bottleneck
    layer before the reconstruction module. In MemNet [[57](#bib.bib57)], it is a
    gate unit at the end of each memory block to control the weights of the long-term
    memory and short-term memory. Note that the gate is not only able to serve as
    bottlenecks placed at the end of the network, but also be continuously conducted
    in the network. For example, in MemNet [[57](#bib.bib57)], IDN[[61](#bib.bib61)],
    and CARN [[62](#bib.bib62)], the gating mechanism is used in both global and local
    region. Sometimes, it can be combined with other operations, such as attention
    mechanism, to construct a more effective gate module to achieve feature distillation.
    For instance, Li *et al.* proposed a hierarchical feature distillation block (Fig. [8](#S3.F8
    "Figure 8 ‣ 3.2.2 Recursive Learning ‣ 3.2 Reconstruction Efficiency Methods ‣
    3 Single-Image Super-Resolution ‣ From Beginner to Master: A Survey for Deep Learning-based
    Single-Image Super-Resolution")) by combining $1\times 1$ convolutional layer
    and attention mechanism in MDCN [[63](#bib.bib63)].'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '为解决这个问题，研究人员建议使用门控机制来自适应地提取和学习更有效的信息。大多数时候，采用$1\times 1$卷积层来完成门控机制，这可以减少通道维度并保留更多有效信息。在SRDenseNet
    [[60](#bib.bib60)]和MSRN [[52](#bib.bib52)]中，这样的$1\times 1$卷积层充当重建模块之前的瓶颈层。在MemNet
    [[57](#bib.bib57)]中，它是每个记忆块末尾的门控单元，用于控制长期记忆和短期记忆的权重。请注意，门控不仅可以作为网络末尾的瓶颈层，还可以在网络中连续进行。例如，在MemNet
    [[57](#bib.bib57)]、IDN [[61](#bib.bib61)]和CARN [[62](#bib.bib62)]中，门控机制同时用于全局和局部区域。有时，它可以与其他操作结合，如注意机制，以构建更有效的门控模块实现特征蒸馏。例如，Li
    *et al.* 提出了一个分层特征蒸馏块（见图[8](#S3.F8 "Figure 8 ‣ 3.2.2 Recursive Learning ‣ 3.2
    Reconstruction Efficiency Methods ‣ 3 Single-Image Super-Resolution ‣ From Beginner
    to Master: A Survey for Deep Learning-based Single-Image Super-Resolution")），通过结合$1\times
    1$卷积层和注意机制，在MDCN [[63](#bib.bib63)]中进行。'
- en: 3.2.4 Curriculum Learning
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 课程学习
- en: Curriculum learning refers to gradually increasing the difficulty of the learning
    task. For some sequence prediction tasks or sequential decision-making problems,
    curriculum learning is used to reduce the training time and improve the generalisation
    performance. Since SISR is an ill-posed problem which is always confronted with
    great learning difficulty due to some adverse conditions such as large scaling
    factors, unknown degradation kernels, and noise, it is suitable to utilize curriculum
    learning to simplify the learning process and improve the reconstruction efficiency.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习指的是逐步增加学习任务的难度。对于一些序列预测任务或顺序决策问题，课程学习被用来减少训练时间并提高泛化性能。由于SISR是一个病态问题，常因大规模因子、未知退化核和噪声等不利条件面临巨大学习难度，因此适合利用课程学习来简化学习过程并提高重建效率。
- en: In LapSRN [[64](#bib.bib64)], curriculum learning is applied to progressively
    reconstruct the sub-band residuals of high-resolution images. In ProSR [[65](#bib.bib65)],
    each level of the pyramid is gradually blended in to reduce the impact on the
    previously trained layers and the training pairs of each scale are incrementally
    added. In SRFBN [[66](#bib.bib66)], the curriculum learning strategy is applied
    to solve the complex degradation tasks, where targets of different difficulties
    are ordered to learn it progressively. With the help of curriculum learning, complex
    problems can be decomposed into multiple simple tasks, hence accelerating model
    convergence and obtaining better reconstruction results.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在LapSRN [[64](#bib.bib64)]中，课程学习用于逐步重建高分辨率图像的子带残差。在ProSR [[65](#bib.bib65)]中，每个金字塔层次逐渐融入，以减少对先前训练层的影响，并逐步添加每个尺度的训练对。在SRFBN
    [[66](#bib.bib66)]中，课程学习策略用于解决复杂的退化任务，其中不同难度的目标被有序地学习。借助课程学习，复杂问题可以被分解为多个简单任务，从而加速模型收敛并获得更好的重建结果。
- en: 3.3 Reconstruction Accuracy Methods
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 重建精度方法
- en: The quality of the reconstructed SR image is always the main concern in SISR.
    In this section, we will introduce some classic methods and strategies that can
    help improve the reconstruction accuracy of SISR models.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在单图像超分辨率中，重建图像的质量始终是主要关注点。在本节中，我们将介绍一些经典的方法和策略，帮助提高 SISR 模型的重建精度。
- en: 3.3.1 Multi-scale Learning
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 多尺度学习
- en: 'As we all know, rich and accurate image features are essential for SR image
    reconstruction. Meanwhile, plenty of research works [[67](#bib.bib67), [68](#bib.bib68),
    [64](#bib.bib64)] have pointed out that images may exhibit different characteristics
    at different scales and thus making full use of these features can further improve
    model performance. Inspired by the inception module [[68](#bib.bib68)], Li *et
    al.* [[52](#bib.bib52)] proposed a multi-scale residual block (MSRB, Fig. [9](#S3.F9
    "Figure 9 ‣ 3.3.1 Multi-scale Learning ‣ 3.3 Reconstruction Accuracy Methods ‣
    3 Single-Image Super-Resolution ‣ From Beginner to Master: A Survey for Deep Learning-based
    Single-Image Super-Resolution")) for feature extraction. MSRB integrates different
    convolution kernels in a block to adaptively extract image features at different
    scales. After that, Li *et al.* [[63](#bib.bib63)] further optimized the structure
    and proposed a more efficient multi-scale dense cross block (MDCB) for feature
    extraction. MDCB is essentially a dual-path dense network that can effectively
    detect local and multi-scale features.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '众所周知，丰富且准确的图像特征对于超分辨率图像重建至关重要。同时，许多研究工作 [[67](#bib.bib67), [68](#bib.bib68),
    [64](#bib.bib64)] 指出，图像在不同的尺度下可能表现出不同的特征，因此充分利用这些特征可以进一步提高模型的性能。受到 inception 模块 [[68](#bib.bib68)]
    的启发，Li *等人* [[52](#bib.bib52)] 提出了一个多尺度残差块（MSRB，图 [9](#S3.F9 "Figure 9 ‣ 3.3.1
    Multi-scale Learning ‣ 3.3 Reconstruction Accuracy Methods ‣ 3 Single-Image Super-Resolution
    ‣ From Beginner to Master: A Survey for Deep Learning-based Single-Image Super-Resolution")）用于特征提取。MSRB
    将不同的卷积核集成到一个块中，以自适应地提取不同尺度的图像特征。之后，Li *等人* [[63](#bib.bib63)] 进一步优化了结构，提出了一种更高效的多尺度密集交叉块（MDCB）用于特征提取。MDCB
    本质上是一个双路径密集网络，可以有效地检测局部和多尺度特征。'
- en: '![Refer to caption](img/e36ba63e0b25ba6dbbb397ad5cfd0ca5.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e36ba63e0b25ba6dbbb397ad5cfd0ca5.png)'
- en: 'Figure 9: The structure of multi-scale residual block (MSRB [[52](#bib.bib52)]).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：多尺度残差块（MSRB [[52](#bib.bib52)]）的结构。
- en: Recently, more and more multi-scale SISR models have been proposed. For instance,
    Qin *et al.* [[69](#bib.bib69)] proposed a multi-scale feature fusion residual
    network (MSFFRN) to fully exploit image features for SISR. Chang *et al.* [[70](#bib.bib70)]
    proposed a multi-scale dense network (MSDN) by combining multi-scale learning
    with dense connection. Cao *et al.* [[71](#bib.bib71)] developed a new SR approach
    called multi-scale residual channel attention network (MSRCAN), which introduced
    the channel attention mechanism into the MSRB. All the above examples indicate
    that the extraction and utilization of multi-scale image features are of increasing
    importance to further improve the quality of the reconstructed images.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，越来越多的多尺度 SISR 模型被提出。例如，Qin *等人* [[69](#bib.bib69)] 提出了一个多尺度特征融合残差网络（MSFFRN），以充分利用图像特征进行
    SISR。Chang *等人* [[70](#bib.bib70)] 通过将多尺度学习与密集连接相结合，提出了一个多尺度密集网络（MSDN）。Cao *等人* [[71](#bib.bib71)]
    开发了一种新的 SR 方法，称为多尺度残差通道注意力网络（MSRCAN），它将通道注意力机制引入了 MSRB。以上所有示例都表明，提取和利用多尺度图像特征在进一步提高重建图像质量方面越来越重要。
- en: '![Refer to caption](img/f0914b9f96317ce197a195535ad0902a.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f0914b9f96317ce197a195535ad0902a.png)'
- en: 'Figure 10: The structure of a simple dense connection module.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：简单密集连接模块的结构。
- en: 3.3.2 Dense Connection
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 密集连接
- en: 'Dense connection mechanism was proposed in DenseNet [[72](#bib.bib72)], which
    is widely used in the computer vision tasks in recent years. Different from the
    structure that only sends the hierarchical features to the final reconstruction
    layer, each layer in the dense block receives the features of all preceding layers
    (Fig. [10](#S3.F10 "Figure 10 ‣ 3.3.1 Multi-scale Learning ‣ 3.3 Reconstruction
    Accuracy Methods ‣ 3 Single-Image Super-Resolution ‣ From Beginner to Master:
    A Survey for Deep Learning-based Single-Image Super-Resolution")). Short paths
    created between most of the layers can help alleviate the problem of vanishing/exploding
    gradients and strengthen the deep information flow through layers, thereby further
    improving the reconstruction accuracy.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 密集连接机制是在 DenseNet [[72](#bib.bib72)] 中提出的，近年来在计算机视觉任务中被广泛使用。不同于仅将层级特征传递到最终重建层的结构，密集块中的每一层接收所有前置层的特征
    (图 [10](#S3.F10 "图 10 ‣ 3.3.1 多尺度学习 ‣ 3.3 重建准确度方法 ‣ 3 单图像超分辨率 ‣ 从初学者到专家：基于深度学习的单图像超分辨率综述"))。在大多数层之间创建的短路径有助于缓解梯度消失/爆炸问题，并增强通过层的深层信息流，从而进一步提高重建准确性。
- en: Motivated by the dense connection mechanism, Tong *et al.* introduced it into
    SISR and proposed the SRDenseNet [[60](#bib.bib60)]. SRDenseNet not only uses
    the layer-level dense connections, but also the block-level one, where the output
    of each dense block is connected by dense connections. In this way, the low-level
    features and high-level features are combined and fully used to conduct the reconstruction.
    In RDN [[73](#bib.bib73)], dense connections are combined with the residual learning
    to form the residual dense block (RDB), which allows low-frequency features to
    be bypassed through multiple skip connections, making the main branch focusing
    on learning high-frequency information. Apart from aforementioned models, dense
    connection is also applied in MemNet [[57](#bib.bib57)], RPMNet [[74](#bib.bib74)],
    MFNet [[75](#bib.bib75)], etc. With the help of dense connection mechanism, the
    information flow among different depths of the network can be fully used, thus
    provides better reconstruction results.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 受密集连接机制的启发，*Tong et al.* 将其引入到 SISR 中，并提出了 SRDenseNet [[60](#bib.bib60)]。SRDenseNet
    不仅使用了层级密集连接，还使用了块级密集连接，其中每个密集块的输出都通过密集连接相连。这样，低层特征和高层特征被结合并充分利用来进行重建。在 RDN [[73](#bib.bib73)]
    中，密集连接与残差学习结合形成了残差密集块 (RDB)，这允许低频特征通过多个跳跃连接绕过，使主分支专注于学习高频信息。除了上述模型外，密集连接还应用于 MemNet
    [[57](#bib.bib57)]、RPMNet [[74](#bib.bib74)]、MFNet [[75](#bib.bib75)] 等。在密集连接机制的帮助下，网络中不同深度的信息流可以被充分利用，从而提供更好的重建结果。
- en: 3.3.3 Attention Mechanism
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 注意力机制
- en: Attention mechanism can be viewed as a tool that can allocate available resources
    to the most informative part of the input. In order to improve the efficiency
    during the learning procedure, some works are proposed to guide the network to
    pay more attention to the regions of interest. For instance, Hu *et al.* [[76](#bib.bib76)]
    proposed a squeeze-and-excitation (SE) block to model channel-wise relationships
    in the image classification task. Wang *et al.* [[77](#bib.bib77)] proposed a
    non-local attention neural network for video classification by incorporating non-local
    operations. Motivated by these methods, attention mechanism has also been introduced
    into SISR.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制可以被视为一种工具，用于将可用资源分配到输入的最有信息量的部分。为了提高学习过程中的效率，一些研究提出了引导网络更多关注感兴趣区域的方法。例如，*Hu
    et al.* [[76](#bib.bib76)] 提出了 squeeze-and-excitation (SE) 块，用于建模图像分类任务中的通道关系。*Wang
    et al.* [[77](#bib.bib77)] 提出了通过结合非局部操作进行视频分类的非局部注意力神经网络。受这些方法的启发，注意力机制也被引入到 SISR
    中。
- en: '![Refer to caption](img/0f1adbce3bd6bd145a5ca555e25c0079.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0f1adbce3bd6bd145a5ca555e25c0079.png)'
- en: 'Figure 11: The principle of channel attention mechanism (CAM).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：通道注意力机制 (CAM) 原理。
- en: 'Channel Attention: In SISR, we mainly want to recover as much valuable high-frequency
    information as possible. However, common CNN-based methods treat channel-wise
    features equally, which lacks flexibility in dealing with different types of information.
    To solve this problem, many methods [[53](#bib.bib53), [78](#bib.bib78)] introduce
    the SE mechanism in the SISR model. For example, Zhang *et al.* [[53](#bib.bib53)]
    proposed a new module based on the SE mechanism, named residual channel attention
    block (RCAB). As shown in Fig. [11](#S3.F11 "Figure 11 ‣ 3.3.3 Attention Mechanism
    ‣ 3.3 Reconstruction Accuracy Methods ‣ 3 Single-Image Super-Resolution ‣ From
    Beginner to Master: A Survey for Deep Learning-based Single-Image Super-Resolution"),
    a global average pooling layer followed by a Sigmoid function is used to rescale
    each feature channel, allowing the network to concentrate on more useful channels
    and enhancing discriminative learning ability. In SAN [[79](#bib.bib79)], second-order
    statistics of features are explored to conduct the attention mechanism based on
    covariance normalization. A great number of experiments have shown that the second-order
    channel attention can help the network obtain more discriminative representations,
    leading to higher reconstruction accuracy.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '通道注意力：在 SISR 中，我们主要希望恢复尽可能多的有价值的高频信息。然而，常见的基于 CNN 的方法对通道特征进行均等处理，这在处理不同类型的信息时缺乏灵活性。为了解决这个问题，许多方法
    [[53](#bib.bib53), [78](#bib.bib78)] 在 SISR 模型中引入了 SE 机制。例如，Zhang *et al.* [[53](#bib.bib53)]
    提出了一个基于 SE 机制的新模块，称为残差通道注意力块（RCAB）。如图 [11](#S3.F11 "Figure 11 ‣ 3.3.3 Attention
    Mechanism ‣ 3.3 Reconstruction Accuracy Methods ‣ 3 Single-Image Super-Resolution
    ‣ From Beginner to Master: A Survey for Deep Learning-based Single-Image Super-Resolution")
    所示，使用全局平均池化层后跟 Sigmoid 函数来重新缩放每个特征通道，使网络能够集中关注更有用的通道，并增强辨别学习能力。在 SAN [[79](#bib.bib79)]
    中，探讨了特征的二阶统计量，以基于协方差归一化进行注意力机制。大量实验表明，二阶通道注意力可以帮助网络获得更具辨别力的表示，从而提高重建准确性。'
- en: 'Non-Local Attention: When CNN-based methods conduct convolution in a local
    receptive field, the contextual information outside this field is ignored, while
    the features in distant regions may have a high correlation and can provide effective
    information. Given this issue, non-local attention has been proposed as a filtering
    algorithm to compute a weighted mean of all pixels of an image. In this way, distant
    pixels can also contribute to the response of a position in concern. For example,
    the non-local operation is conducted in a limited neighborhood to improve the
    robustness in NLRN [[80](#bib.bib80)]. A non-local attention block is proposed
    in RNAN [[81](#bib.bib81)], where the attention mechanisms in both channel- and
    spatial-wise are used simultaneously in its mask branch to better guide feature
    extraction in the trunk branch. Meanwhile, a holistic attention network is proposed
    in HAN [[82](#bib.bib82)], which consists of a layer attention module and a channel-spatial
    attention module, to model the holistic interdependence among layers, channels,
    and positions. In CSNLN [[83](#bib.bib83)], a cross-scale non-local attention
    module is proposed to mine long-range dependencies between LR features and large-scale
    HR patches within the same feature map. All these methods show the effectiveness
    of the non-local attention, which can further improve the model performance.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 非局部注意力：当基于 CNN 的方法在局部感受野内进行卷积时，忽略了该区域外的上下文信息，而远离区域的特征可能具有较高的相关性，并能提供有效信息。针对这个问题，提出了非局部注意力作为一种过滤算法，以计算图像所有像素的加权均值。通过这种方式，远处的像素也可以对关注位置的响应做出贡献。例如，非局部操作在有限的邻域内进行，以提高
    NLRN [[80](#bib.bib80)] 的鲁棒性。在 RNAN [[81](#bib.bib81)] 中，提出了一种非局部注意力块，其中同时在其掩码分支中使用了通道和空间的注意力机制，以更好地指导主干分支的特征提取。同时，在
    HAN [[82](#bib.bib82)] 中提出了一种整体注意力网络，该网络由层级注意力模块和通道-空间注意力模块组成，以建模层、通道和位置之间的整体相互依赖关系。在
    CSNLN [[83](#bib.bib83)] 中，提出了一种跨尺度非局部注意力模块，以挖掘 LR 特征与同一特征图中大尺度 HR 补丁之间的长期依赖关系。所有这些方法展示了非局部注意力的有效性，进一步提升了模型的性能。
- en: 3.3.4 Feedback Mechanism
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 反馈机制
- en: Feedback mechanism refers to carrying a notion of output to the previous states,
    allowing the model to have a self-correcting procedure. It is worth noting that
    the feedback mechanism is different from recursive learning since in the feedback
    mechanism the model parameters are keeping self-correcting and do not share. Recently,
    feedback mechanism has been widely used in many computer vision tasks [[84](#bib.bib84),
    [85](#bib.bib85)], which is also beneficial for the SR images reconstruction.
    Specifically, the feedback mechanism allows the network to carry high-level information
    back to previous layers and refine low-level information, thus fully guide the
    LR image to recover high-quality SR images.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈机制指的是将输出概念带回到之前的状态，允许模型具有自我修正的过程。值得注意的是，反馈机制与递归学习不同，因为在反馈机制中，模型参数保持自我修正并且不共享。最近，反馈机制在许多计算机视觉任务中得到了广泛应用 [[84](#bib.bib84),
    [85](#bib.bib85)]，这也有利于SR图像的重建。具体来说，反馈机制允许网络将高层信息传递回前面的层，并细化低层信息，从而全面指导LR图像恢复高质量的SR图像。
- en: In DBPN [[86](#bib.bib86)], iterative up- and down-sampling layers are provided
    to achieve an error feedback mechanism for projection errors at each stage. In
    DSRN [[87](#bib.bib87)], a dual-state recurrent network is proposed, where recurrent
    signals are exchanged between these states in both directions via delayed feedback.
    In SFRBN [[66](#bib.bib66)], a feedback block is proposed, in which the input
    of each iteration is the output of the previous one as the feedback information.
    Followed by several projection groups sequentially with dense skip connections,
    low-level representations are refined and become more powerful high-level representations.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在DBPN [[86](#bib.bib86)]中，提供了迭代的上采样和下采样层，以实现每个阶段的投影误差的错误反馈机制。在DSRN [[87](#bib.bib87)]中，提出了一种双状态递归网络，其中递归信号在这两个状态之间通过延迟反馈进行双向交换。在SFRBN [[66](#bib.bib66)]中，提出了一种反馈块，其中每次迭代的输入是上一次的输出作为反馈信息。紧接着是几个按顺序排列的投影组以及密集的跳跃连接，低层次表示得到了细化，并变得更强大的高层次表示。
- en: 3.3.5 Additional Prior
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5 附加先验
- en: Most methods tend to build end-to-end CNN models to achieve SISR since it is
    simple and easy to implement. However, it is rather difficult for them to reconstruct
    realistic high-frequency details due to plenty of useful features have been lost
    or damaged. To solve this issue, priors guided SISR framework has been proposed.
    Extensive experiments have shown that with the help of image priors, the model
    can converge faster and achieve better reconstruction accuracy. Recently, many
    image priors have been proposed, such as total variation prior, sparse prior,
    and edge prior.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数方法倾向于构建端到端的CNN模型来实现SISR，因为这种方法简单且易于实现。然而，由于大量有用特征已经丢失或损坏，它们重建真实的高频细节相当困难。为了解决这个问题，提出了先验指导的SISR框架。大量实验表明，在图像先验的帮助下，模型可以更快地收敛，并获得更好的重建精度。最近，许多图像先验被提出，例如总变差先验、稀疏先验和边缘先验。
- en: Motivated by this, Yang *et al.* integrated the edge prior with recursive networks
    and proposed a Deep Edge Guided Recurrent Residual Network (DEGREE [[88](#bib.bib88)])
    for SISR. After that, Fang *et al.* proposed an efficient and accurate Soft-edge
    Assisted Network (SeaNet [[89](#bib.bib89)]). Different from DEGREE, which directly
    applies the off-the-shelf edge detectors to detect image edges, SeaNet automatically
    learns more accurate image edges from the constructed Edge-Net. Meanwhile, the
    authors pointed out that the more accurate priors introduced, the greater improvement
    in performance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 受到这一启发，杨*等人*将边缘先验与递归网络集成，提出了一种深度边缘引导递归残差网络（DEGREE [[88](#bib.bib88)]）用于SISR。此后，方*等人*提出了一种高效且准确的软边缘辅助网络（SeaNet [[89](#bib.bib89)]）。与DEGREE直接应用现成的边缘检测器检测图像边缘不同，SeaNet自动从构建的Edge-Net中学习更准确的图像边缘。同时，作者指出，引入更准确的先验会带来性能的显著提升。
- en: 3.4 Perceptual Quality Methods
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 感知质量方法
- en: Most methods simply seek to reconstruct SR images with high PSNR and SSIM. However,
    the improvement in reconstruction accuracy is not always accompanied by an improvement
    in visual quality. Blau *et al.* [[90](#bib.bib90)] pointed out that there was
    a perception-distortion trade-off. It is only possible to improve either perceptual
    quality or distortion, while improving one must be at the expense of the other.
    Hence, in this section, we provide methods to ease this trade-off problem, hoping
    to provide less distortion while maintaining good perceptual quality of the image.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数方法只是试图重建具有高PSNR和SSIM的超分辨率图像。然而，重建精度的提升并不总是伴随着视觉质量的改善。Blau *等* [[90](#bib.bib90)]
    指出存在感知与失真之间的权衡。只能改善感知质量或失真，而提升其中一个必须以牺牲另一个为代价。因此，在本节中，我们提供了缓解这一权衡问题的方法，希望在保持图像良好感知质量的同时减少失真。
- en: 3.4.1 Perceptual Loss
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 感知损失
- en: 'Although pixel-wise losses, i.e., L1 and MSE loss, have been widely used to
    achieve high image quality, they do not capture the perceptual differences between
    the SR and HR images. In order to address this problem and allow the loss functions
    to better measure the perceptual and semantic differences between images, content
    loss, texture loss, and targeted perceptual loss are proposed. Among them, content
    loss has been widely used to obtain more perceptual and natural images [[91](#bib.bib91),
    [38](#bib.bib38), [20](#bib.bib20)], which has been introduced in Sec. [2.4.1](#S2.SS4.SSS1
    "2.4.1 Learning Strategy ‣ 2.4 Optimization Objective ‣ 2 Problem Setting and
    Related Works ‣ From Beginner to Master: A Survey for Deep Learning-based Single-Image
    Super-Resolution"). Apart from obtaining more similar content, the same style,
    such as colors, textures, common patterns, and semantic information are also needed.
    Therefore, other perceptual loss need to be considered.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管像素级损失，如L1和MSE损失，已广泛用于实现高图像质量，但它们无法捕捉SR图像和HR图像之间的感知差异。为了应对这个问题，并使损失函数更好地衡量图像之间的感知和语义差异，提出了内容损失、纹理损失和目标感知损失。其中，内容损失已广泛用于获得更具感知性和自然的图像
    [[91](#bib.bib91), [38](#bib.bib38), [20](#bib.bib20)]，这已在Sec. [2.4.1](#S2.SS4.SSS1
    "2.4.1 Learning Strategy ‣ 2.4 Optimization Objective ‣ 2 Problem Setting and
    Related Works ‣ From Beginner to Master: A Survey for Deep Learning-based Single-Image
    Super-Resolution") 中介绍。除了获得更相似的内容外，还需要相同的风格，例如颜色、纹理、常见模式和语义信息。因此，还需要考虑其他感知损失。'
- en: 'Texture Loss: Texture loss, also called style reconstruction loss, is proposed
    by Gatys *et al.* [[92](#bib.bib92), [93](#bib.bib93)], which can make the model
    reconstruct high-quality textures. The texture loss is defined as the squared
    Frobenius norm of the difference between the Gram matrices $G_{j}^{\phi}(x)$ of
    the output and the ground truth images:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 纹理损失：纹理损失，也称为风格重建损失，由Gatys *等* [[92](#bib.bib92), [93](#bib.bib93)] 提出，可以使模型重建高质量的纹理。纹理损失被定义为输出图像和真实图像Gram矩阵$G_{j}^{\phi}(x)$之间差异的平方Frobenius范数：
- en: '|  | $\mathcal{L}^{\phi,j}_{texture}(I_{SR},I_{y})=&#124;&#124;G_{j}^{\phi}(I_{SR})-G_{j}^{\phi}(I_{y})&#124;&#124;^{2}_{F}.$
    |  | (18) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}^{\phi,j}_{texture}(I_{SR},I_{y})=&#124;&#124;G_{j}^{\phi}(I_{SR})-G_{j}^{\phi}(I_{y})&#124;&#124;^{2}_{F}.$
    |  | (18) |'
- en: With the help of the texture loss, the model tends to produce images that have
    the same local textures as the HR images during training [[94](#bib.bib94)].
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在纹理损失的帮助下，模型在训练期间趋向于生成与HR图像具有相同局部纹理的图像 [[94](#bib.bib94)]。
- en: 'Targeted Perceptual Loss: The conventional perceptual loss estimates the reconstruction
    error for an entire image without considering semantic information, resulting
    in limited capability. Rad *et al.* [[95](#bib.bib95)] proposed a targeted perceptual
    loss that penalized images at different semantic levels on the basis of the labels
    of object, background, and boundary. Therefore, more realistic textures and sharper
    edges can be obtained to reconstruct realistic SR images.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 目标感知损失：传统的感知损失估计整个图像的重建误差，而不考虑语义信息，导致能力有限。Rad *等* [[95](#bib.bib95)] 提出了目标感知损失，基于对象、背景和边界的标签对不同语义级别的图像进行惩罚。因此，可以获得更逼真的纹理和更清晰的边缘，从而重建逼真的超分辨率图像。
- en: 3.4.2 Adversarial Training
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 对抗训练
- en: In 2014, the Generative Adversarial Networks (GANs) was proposed by Goodfellow
    *et al.* [[39](#bib.bib39)], which has been widely used in compute vision tasks,
    such as style transfer and image inpainting. The GANs consists of a generator
    and a discriminator. When the discriminator is trained to judge whether an image
    is true or false, the generator aims at fooling the discriminator rather than
    minimizing the distance to a specific image, hence it tends to generate outputs
    that have the same statistics as the training set.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，Goodfellow *等人* 提出了生成对抗网络（GANs）[[39](#bib.bib39)]，该方法已广泛应用于计算机视觉任务，如风格迁移和图像修复。GANs由生成器和鉴别器组成。当鉴别器被训练来判断图像是真实还是虚假的时候，生成器的目标是欺骗鉴别器，而不是最小化与特定图像的距离，因此它倾向于生成与训练集具有相同统计特征的输出。
- en: 'Inspired by GAN, Ledig *et al.* proposed the Super-Resolution Generative Adversarial
    Network (SRGAN [[38](#bib.bib38)]). In SRGAN, the generator $G$ is essentially
    a SR model that trained to fool the discriminator $D$, and $D$ is trained to distinguish
    SR images from HR images. Therefore, the generator can learn to produce outputs
    that are highly similar to HR images, and then reconstruct more perceptual and
    natural SR images. Following this approach, the generative loss $\mathcal{L}_{Gen}(I_{x})$
    can be defined as:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 受GAN启发，Ledig *等人* 提出了超分辨率生成对抗网络（SRGAN [[38](#bib.bib38)]）。在SRGAN中，生成器$G$本质上是一个SR模型，旨在欺骗鉴别器$D$，而$D$则被训练以区分SR图像和HR图像。因此，生成器可以学习生成与HR图像高度相似的输出，从而重建出更具感知性和自然的SR图像。根据这种方法，生成损失$\mathcal{L}_{Gen}(I_{x})$可以定义为：
- en: '|  | $\mathcal{L}_{Gen}=-\log D_{\theta_{D}}(G_{\theta_{G}}(I_{x})),$ |  |
    (19) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{Gen}=-\log D_{\theta_{D}}(G_{\theta_{G}}(I_{x})),$ |  |
    (19) |'
- en: 'and the loss in terms of discriminator is:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器相关的损失是：
- en: '|  | $\mathcal{L}_{Dis}=-\log(D_{\theta_{D}}(I_{y}))-\log(1-D_{\theta_{D}}(G_{\theta_{G}}(I_{x}))).$
    |  | (20) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{Dis}=-\log(D_{\theta_{D}}(I_{y}))-\log(1-D_{\theta_{D}}(G_{\theta_{G}}(I_{x}))).$
    |  | (20) |'
- en: 'Therefore, we need to solve the following problem:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要解决以下问题：
- en: '|  | $\begin{split}\min_{\theta_{G}}\ \max_{\theta_{D}}\ &amp;\mathbb{E}_{I_{y\sim
    p_{data}(I_{y})}}(\log D_{\theta_{D}}(I_{y}))\ +\\ &amp;\mathbb{E}_{I_{x\sim p_{G}(I_{x})}}(\log(1-D_{\theta_{D}}(G_{\theta_{G}}(I_{x})))).\end{split}$
    |  | (21) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\min_{\theta_{G}}\ \max_{\theta_{D}}\ &amp;\mathbb{E}_{I_{y\sim
    p_{data}(I_{y})}}(\log D_{\theta_{D}}(I_{y}))\ +\\ &amp;\mathbb{E}_{I_{x\sim p_{G}(I_{x})}}(\log(1-D_{\theta_{D}}(G_{\theta_{G}}(I_{x})))).\end{split}$
    |  | (21) |'
- en: 'In SRGAN [[38](#bib.bib38)], the generator is the SRResNet and the discriminator
    uses the architecture proposed by Radford *et al.* [[96](#bib.bib96)]. In ESRGAN [[97](#bib.bib97)],
    Wang *et al.* made two modifications to the SRResNet: (1) replace the original
    residual block with the residual-in-residual dense block; (2) remove the BN layers
    to improve the generalization ability of the model. In SRFeat [[98](#bib.bib98)],
    Park *et al.* indicated that the GAN-based SISR methods tend to produce less meaningful
    high-frequency noise in reconstructed images. Therefore, they adopted two discriminators:
    an image discriminator and a feature discriminator, where the latter is trained
    to distinguish SR images from HR images based on the intermediate feature map
    extracted from a VGG network. In ESRGAN [[97](#bib.bib97)], Wang *et al.* adopted
    the Relativistic GAN [[99](#bib.bib99)], where the standard discriminator was
    replaced with the relativistic average discriminator to learn the relatively realistic
    between two images. This modification helps the generator to learn sharper edges
    and more detailed textures.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在SRGAN [[38](#bib.bib38)]中，生成器是SRResNet，鉴别器使用了Radford *等人* 提出的架构[[96](#bib.bib96)]。在ESRGAN
    [[97](#bib.bib97)]中，Wang *等人* 对SRResNet做了两项修改：（1）用残差中的残差密集块替换原始的残差块；（2）去除BN层以提高模型的泛化能力。在SRFeat
    [[98](#bib.bib98)]中，Park *等人* 指出基于GAN的SISR方法在重建图像中往往会产生较少有意义的高频噪声。因此，他们采用了两个鉴别器：一个图像鉴别器和一个特征鉴别器，其中后者训练以基于从VGG网络中提取的中间特征图来区分SR图像和HR图像。在ESRGAN
    [[97](#bib.bib97)]中，Wang *等人* 采用了相对生成对抗网络（Relativistic GAN）[[99](#bib.bib99)]，将标准鉴别器替换为相对平均鉴别器，以学习两幅图像之间的相对真实性。这一修改帮助生成器学习更清晰的边缘和更详细的纹理。
- en: 3.4.3 Additional Prior (Perceptual)
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3 附加先验（感知）
- en: 'In Sec. [3.3.5](#S3.SS3.SSS5 "3.3.5 Additional Prior ‣ 3.3 Reconstruction Accuracy
    Methods ‣ 3 Single-Image Super-Resolution ‣ From Beginner to Master: A Survey
    for Deep Learning-based Single-Image Super-Resolution"), we have introduced the
    applications of prior knowledge in the CNN-based SISR models. In this section,
    we will show the benefits of using additional priors in GAN-based models. The
    target of all the introduced additional priors is to improve the perceptual quality
    of the reconstructed SR images.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[3.3.5](#S3.SS3.SSS5 "3.3.5 Additional Prior ‣ 3.3 Reconstruction Accuracy
    Methods ‣ 3 Single-Image Super-Resolution ‣ From Beginner to Master: A Survey
    for Deep Learning-based Single-Image Super-Resolution")节中，我们介绍了先验知识在基于CNN的单图像超分辨率模型中的应用。在这一节中，我们将展示在基于GAN的模型中使用额外先验的好处。所有引入的额外先验的目标是提高重建的超分辨率图像的感知质量。'
- en: For example, the semantic categorical prior is used to generate richer and more
    realistic textures with the help of spatial feature transform (SFT) in SFTGAN[[20](#bib.bib20)].
    With this information from high-level tasks, similar LR patches can be easily
    distinguished and more natural textual details can be generated. In SPSR [[100](#bib.bib100)],
    the authors utilized the gradient maps to guide image recovery to solve the problem
    of structural distortions in the GAN-based methods. Among them, the gradient maps
    are obtained from a gradient branch and integrated into the SR branch to provide
    structure prior. With the help of gradient maps, we know which region should be
    paid more attention to, so as to guide the image generation and reduce geometric
    distortions.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，语义类别先验用于通过空间特征变换（SFT）生成更丰富、更真实的纹理，在SFTGAN[[20](#bib.bib20)]中得到了应用。利用来自高级任务的信息，可以更容易地区分相似的低分辨率补丁，并生成更自然的纹理细节。在SPSR[[100](#bib.bib100)]中，作者利用梯度图引导图像恢复，以解决基于GAN方法的结构失真问题。其中，梯度图来自梯度分支，并集成到超分辨率分支中以提供结构先验。借助梯度图，我们可以知道应该更多关注哪个区域，从而指导图像生成并减少几何失真。
- en: 3.4.4 Cycle Consistency
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.4 循环一致性
- en: Cycle consistency assumes that there exist some underlying relationships between
    the source and target domains, and tries to make supervision at the domain level.
    To be precise, we want to capture some special characteristics of one image collection
    and figure out how to translate these characteristics into the other image collection.
    To achieve this, Zhu *et al.* [[36](#bib.bib36)] proposed the cycle consistency
    mechanism, where not only the mapping from the source domain to the target domain
    is learned, but also the backward mapping is combined. Specifically, given a source
    domain $X$ and a target domain $Y$, we have a translator $G:X\rightarrow Y$ and
    another translator $F:Y\rightarrow X$ that trained simultaneously to guarantee
    both an $adversarial\ loss$ that encourages $G(X)\approx Y$ and $F(Y)\approx X$
    and a $cycle\ consistency\ loss$ that encourages $F(G(X))\approx X$ and $G(F(Y))\approx
    Y$.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 循环一致性假设源领域和目标领域之间存在某些潜在关系，并尝试在领域级别进行监督。具体而言，我们希望捕捉一个图像集合的一些特殊特征，并找出如何将这些特征转换到另一个图像集合中。为此，Zhu
    *等* [[36](#bib.bib36)] 提出了循环一致性机制，其中不仅学习了从源领域到目标领域的映射，而且还结合了反向映射。具体来说，给定源领域 $X$
    和目标领域 $Y$，我们有一个翻译器 $G:X\rightarrow Y$ 和另一个翻译器 $F:Y\rightarrow X$，它们同时训练，以保证 $G(X)\approx
    Y$ 和 $F(Y)\approx X$ 的 $对抗损失$，以及 $F(G(X))\approx X$ 和 $G(F(Y))\approx Y$ 的 $循环一致性损失$。
- en: In SISR, the idea of cycle consistency has also been widely discussed. Given
    the LR images domain $X$ and the HR images domain $Y$, we not only learn the mapping
    from LR to HR but also the backward process. Researchers have shown that learning
    how to do image degradation first without paired data can help generate more realistic
    images [[101](#bib.bib101)]. In CinCGAN [[35](#bib.bib35)], a cycle in cycle network
    is proposed, where the noisy and blurry input is mapped to a noise-free LR domain
    firstly and then upsampled with a pre-trained model and finally mapped to the
    HR domain. In DRN [[102](#bib.bib102)], the mapping from HR to LR images is learned
    to estimate the down-sampling kernel and reconstruct LR images, which forms a
    closed-loop to provide additional supervision. DRN also gives us a novel approach
    in unsupervised learning SR, where the deep network is trained with both paired
    and unpaired data.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SISR 中，循环一致性的概念也得到了广泛讨论。给定低分辨率图像领域 $X$ 和高分辨率图像领域 $Y$，我们不仅学习从低分辨率到高分辨率的映射，还学习反向过程。研究人员已经表明，首先在没有配对数据的情况下学习如何进行图像退化可以帮助生成更真实的图像 [[101](#bib.bib101)]。在
    CinCGAN [[35](#bib.bib35)] 中，提出了一个循环网络中的循环，其中嘈杂和模糊的输入首先被映射到无噪声的低分辨率领域，然后用预训练模型上采样，最后映射到高分辨率领域。在
    DRN [[102](#bib.bib102)] 中，从高分辨率到低分辨率图像的映射被学习以估计下采样核并重建低分辨率图像，这形成了一个闭环来提供额外的监督。DRN
    还为无监督学习超分辨率提供了一种新颖的方法，其中深度网络同时用配对和未配对的数据进行训练。
- en: 3.5 Further Improvement Methods
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 进一步改进方法
- en: In the aforementioned part, we have introduced the way to design an efficient
    SISR model, as well as obtaining high reconstruction accuracy and high perceptual
    quality for SR images. Though current SISR models have made a significant breakthrough
    in achieving a balance between reconstruction accuracy and perceptual quality,
    it still remains a hot topic to explore more effective models.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述部分中，我们介绍了设计高效 SISR 模型的方法，以及获得高重建精度和高感知质量的超分辨率图像。尽管当前的 SISR 模型在实现重建精度和感知质量之间的平衡方面取得了显著突破，但探索更有效的模型仍然是一个热门话题。
- en: 3.5.1 Internal Statistics
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 内部统计
- en: In [[103](#bib.bib103)], Zontak *et al.* found that some patches exist only
    in a specific image and can not be found in any external database of examples.
    Therefore, SR methods trained on external images can not work well on such images
    due to the lack of patches information, while methods based on internal statistics
    may have a good performance. Meanwhile, Zontak *et al.* pointed out that the internal
    entropy of patches inside a single image was much smaller than the external entropy
    of patches in a general collection of natural images. Therefore, using the internal
    image statistics to further improve model performance is a good choice.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[103](#bib.bib103)] 中，Zontak *等* 发现某些补丁只存在于特定图像中，无法在任何外部示例数据库中找到。因此，基于外部图像训练的超分辨率方法在这些图像上可能效果不好，因为缺乏补丁信息，而基于内部统计的方法可能表现良好。同时，Zontak
    *等* 指出，单个图像内的补丁的内部熵远小于一般自然图像集合中的补丁的外部熵。因此，利用内部图像统计来进一步提高模型性能是一个不错的选择。
- en: In ZSSR [[34](#bib.bib34)], the property of internal image statistics is used
    to train an image-specific CNN, where the training examples are extracted from
    the test image itself. In training phase, several LR-HR pairs are generated by
    using data augmentation, and a CNN is trained with these pairs. In test time,
    the LR image $I_{LR}$ is fed to the trained CNN as input to get the reconstructed
    image. In this process, the model makes full use of internal statistics of the
    image itself for self-learning. In SinGAN [[104](#bib.bib104)], an unconditional
    generative model with a pyramid of fully convolutional GANs is proposed to learn
    the internal patch distribution at different scales of the image. To make use
    of the recurrence of internal information, they upsampled the LR image several
    times (depending on the final scale) to obtain the final SR output.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ZSSR [[34](#bib.bib34)] 中，利用内部图像统计的属性来训练图像特定的 CNN，其中训练样本从测试图像本身提取。在训练阶段，通过数据增强生成多个低分辨率-高分辨率对，并用这些对训练
    CNN。在测试时，将低分辨率图像 $I_{LR}$ 作为输入馈送到训练好的 CNN 以获得重建图像。在这个过程中，模型充分利用了图像本身的内部统计进行自学习。在
    SinGAN [[104](#bib.bib104)] 中，提出了一种无条件生成模型，使用一个完全卷积的 GAN 金字塔来学习图像不同尺度的内部补丁分布。为了利用内部信息的递归性，他们将低分辨率图像多次上采样（根据最终尺度）以获得最终的超分辨率输出。
- en: 3.5.2 Multi-factors Learning
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 多因素学习
- en: Typically, in SISR, we often need to train specific models for different upsampling
    factors and it is difficult to arise at the expectation that a model can be applied
    to multiple upsampling factors. To solve this issue, some models have been proposed
    for multiple upsampling factors, such as LapSRN [[105](#bib.bib105)], MDSR [[51](#bib.bib51)],
    and MDCN [[63](#bib.bib63)].
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在SISR中，我们常常需要为不同的上采样因子训练特定的模型，很难期望一个模型可以应用于多个上采样因子。为了解决这个问题，一些针对多个上采样因子的模型已被提出，如LapSRN
    [[105](#bib.bib105)]、MDSR [[51](#bib.bib51)] 和MDCN [[63](#bib.bib63)]。
- en: In LapSRN [[105](#bib.bib105)], LR images are progressively reconstructed in
    the pyramid networks to obtain the large-scale results, where the intermediate
    results can be taken directly as the corresponding multiple factors results. In [[51](#bib.bib51)],
    Lim *et al.* found the inter-related phenomenon among multiple scales tasks, i.e.,
    initializing the high-scale model parameters with the pre-trained low-scale network
    can accelerate the training process and improve the performance. Therefore, they
    proposed the scale-specific processing modules at the head and tail of the model
    to handle different upsampling factors. To further exploit the inter-scale correlation
    between different upsampling factors, Li *et al.* further optimized the strategy
    in MDCN [[63](#bib.bib63)]. Different from MDSR which introduces the scale-specific
    processing strategy both at the head and tail of the model, MDCN can maximize
    the reuse of model parameters and learn the inter-scale correlation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在LapSRN [[105](#bib.bib105)]中，低分辨率图像在金字塔网络中逐步重建，以获得大尺度的结果，其中中间结果可以直接作为对应的多个因子的结果。在[[51](#bib.bib51)]中，Lim
    *et al.*发现了多个尺度任务之间的相关现象，即用预训练的低尺度网络初始化高尺度模型参数可以加速训练过程并提高性能。因此，他们在模型的头部和尾部提出了特定尺度处理模块，以处理不同的上采样因子。为了进一步利用不同上采样因子之间的尺度间相关性，Li
    *et al.*在MDCN [[63](#bib.bib63)]中进一步优化了策略。与在模型的头部和尾部都引入特定尺度处理策略的MDSR不同，MDCN能够最大化模型参数的重用，并学习尺度间的相关性。
- en: 3.5.3 Knowledge Distillation
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3 知识蒸馏
- en: Knowledge distillation refers to a technique that transfers the representation
    ability of a large (Teacher) model to a small one (Student) for enhancing the
    performance of the student model. Hence, it has been widely used for network compression
    or to further improve the performance of the student model, which has shown the
    effectiveness in many computer vision tasks. Meanwhile, there are mainly two kinds
    of knowledge distillation, soft label distillation and feature distillation. In
    soft label distillation, the softmax outputs of a teacher model are regarded as
    soft labels to provide informative dark knowledge to the student model [[106](#bib.bib106)].
    In feature distillation, the intermediate features maps are transferred to the
    student model [[107](#bib.bib107), [108](#bib.bib108)].
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏是指将大型（教师）模型的表征能力转移到小型（学生）模型中，以增强学生模型的性能的技术。因此，它已被广泛用于网络压缩或进一步提高学生模型的性能，这在许多计算机视觉任务中已显示出有效性。同时，知识蒸馏主要有两种类型：软标签蒸馏和特征蒸馏。在软标签蒸馏中，教师模型的softmax输出被视为软标签，以为学生模型提供有信息的暗知识
    [[106](#bib.bib106)]。在特征蒸馏中，中间特征图被转移到学生模型中 [[107](#bib.bib107), [108](#bib.bib108)]。
- en: Inspired by this, some works introduce the knowledge distillation technique
    to SISR to further improve the performance of lightweight models. For instance,
    in SRKD [[109](#bib.bib109)], a small but efficient student network is guided
    by a deep and powerful teacher network to achieve similar feature distributions
    to those of the teacher. In [[110](#bib.bib110)], the teacher network leverage
    the HR images as privileged information and the intermediate features of the decoder
    of the teacher network are transferred to the student network via feature distillation,
    so that the student can learn high frequencies details from the Teacher which
    trained with the HR images.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 受到此启发，一些研究将知识蒸馏技术引入到单幅图像超分辨率（SISR）中，以进一步提高轻量级模型的性能。例如，在SRKD [[109](#bib.bib109)]中，一个小而高效的学生网络通过深层且强大的教师网络指导，达到与教师网络相似的特征分布。在[[110](#bib.bib110)]中，教师网络利用高分辨率图像作为特权信息，并通过特征蒸馏将教师网络解码器的中间特征传递给学生网络，使得学生能够从使用高分辨率图像训练的教师那里学习高频细节。
- en: 3.5.4 Reference-based SISR
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.4 基于参考的SISR
- en: In contrast to SISR where only a single LR image is used as input, reference-based
    SISR (RefSR) takes a reference image to assist the SR process. The reference images
    can be obtained from various sources like photo albums, video frames, and web
    image searches. Meanwhile, there are several approaches proposed to enhance image
    textures, such as image aligning and patch matching. Recently, some RefSR methods [[111](#bib.bib111),
    [112](#bib.bib112)] choose to align the LR and reference images with the assumption
    that the reference image possesses similar content as the LR image. For instance,
    Yue *et al.* [[111](#bib.bib111)] conducted global registration and local matching
    between the reference and LR images to solve an energy minimization problem. In
    CrossNet [[112](#bib.bib112)], optical flow is proposed to align the reference
    and LR images at different scales, which are later concatenated into the corresponding
    layers of the decoder. However, these methods assume that the reference image
    has a good alignment with the LR image. Otherwise, their performance will be significantly
    influenced. Different from these methods, Zhang *et al.* [[23](#bib.bib23)] applied
    patch matching between VGG features of the LR and reference images to adaptively
    transfer textures from the reference images to the LR images. In TTSR [[113](#bib.bib113)],
    Yang *et al.* proposed a texture transformer network to search and transfer relevant
    textures from the reference images to the LR images based on the attention mechanisms.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅使用单张LR图像作为输入的SISR不同，基于参考的SISR（RefSR）采用参考图像来辅助SR过程。参考图像可以从各种来源获取，如相册、视频帧和网页图像搜索。同时，已经提出了几种增强图像纹理的方法，如图像对齐和补丁匹配。最近，一些RefSR方法
    [[111](#bib.bib111), [112](#bib.bib112)] 选择对LR图像和参考图像进行对齐，假设参考图像与LR图像具有相似内容。例如，Yue
    *等* [[111](#bib.bib111)] 在参考图像和LR图像之间进行了全局配准和局部匹配，以解决能量最小化问题。在CrossNet [[112](#bib.bib112)]
    中，提出了光流方法在不同尺度下对齐参考图像和LR图像，然后将其连接到解码器的相应层。然而，这些方法假设参考图像与LR图像有良好的对齐，否则其性能将受到显著影响。与这些方法不同，Zhang
    *等* [[23](#bib.bib23)] 应用补丁匹配在LR图像和参考图像的VGG特征之间，自适应地将纹理从参考图像转移到LR图像。在TTSR [[113](#bib.bib113)]
    中，Yang *等* 提出了一个纹理变换网络，基于注意力机制从参考图像中搜索和转移相关纹理到LR图像。
- en: 3.5.5 Transformer-based SISR
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.5 基于Transformer的SISR
- en: The key idea of Transformer is the “self-attention” mechanism, which can capture
    long-term information between sequence elements. Recently, Transformer [[114](#bib.bib114)]
    has achieved brilliant results in NLP tasks. For example, the pre-trained deep
    learning models (e.g., BERT [[115](#bib.bib115)], GPT [[116](#bib.bib116)]) have
    shown effectiveness over conventional methods. Inspired by this, more and more
    researchers have begun to explore the application of Transformer in computer vision
    tasks and have achieved breakthrough results many tasks.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的关键思想是“自注意力”机制，它可以捕捉序列元素之间的长期信息。最近，Transformer [[114](#bib.bib114)]
    在自然语言处理任务中取得了辉煌的成果。例如，预训练的深度学习模型（如BERT [[115](#bib.bib115)]，GPT [[116](#bib.bib116)]）在效果上超越了传统方法。受到这些启发，越来越多的研究人员开始探索Transformer在计算机视觉任务中的应用，并在许多任务中取得了突破性成果。
- en: Nowadays, some researchers try to introduce Transformer to image restoration
    tasks. For exsample, Chen *et al.* proposed the Image Processing Transformer (IPT [[117](#bib.bib117)])
    which was pre-trained on large-scale datasets. In addition, contrastive learning
    is introduced for different image processing tasks. Therefore, the pre-trained
    model can efficiently be employed on the desired task after finetuning. However,
    IPT [[117](#bib.bib117)] relies on large-scale datasets and has a large number
    of parameters (over 115.5M parameters), which greatly limits its application scenarios.
    To solve this issue, Liang *et al.* proposed the SwinIR [[118](#bib.bib118)] for
    image restoration based on the Swin Transformer [[119](#bib.bib119)]. Specifically,
    the Swin Transformer blocks (RSTB) is proposed for feature extraction and DIV2K+Flickr2K
    are used for training. Moreover, Lu *et al.* [[120](#bib.bib120)] proposed an
    Efficient Super-Resolution Transformer (ESRT) for fast and accurate SISR. It is
    worth noting that ESRT is a lightweight model, which achieves competitive results
    with fewer parameters and low computing costs. Transformer is a powerful technology,
    but how to use fewer parameters and datasets to effectively train the model is
    still worth exploring.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，一些研究人员尝试将Transformer引入图像恢复任务。例如，Chen *et al.* 提出了在大规模数据集上预训练的图像处理Transformer（IPT
    [[117](#bib.bib117)]）。此外，针对不同的图像处理任务引入了对比学习。因此，经过微调后，预训练的模型可以有效地用于所需任务。然而，IPT
    [[117](#bib.bib117)] 依赖于大规模数据集，并且参数数量庞大（超过115.5M参数），这大大限制了其应用场景。为了解决这个问题，Liang
    *et al.* 提出了基于Swin Transformer [[119](#bib.bib119)] 的SwinIR [[118](#bib.bib118)]
    用于图像恢复。具体来说，提出了用于特征提取的Swin Transformer块（RSTB），并使用DIV2K+Flickr2K进行训练。此外，Lu *et
    al.* [[120](#bib.bib120)] 提出了一个高效超分辨率Transformer（ESRT），以实现快速且准确的SISR。值得注意的是，ESRT
    是一个轻量级模型，具有较少的参数和低计算成本却能取得具有竞争力的结果。Transformer是一项强大的技术，但如何使用更少的参数和数据集有效训练模型仍值得深入探讨。
- en: 4 Domain-Specific Applications
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 个特定领域的应用
- en: 4.1 Real-World SISR
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 真实世界的SISR
- en: The degradation modes are complex and unknown in real-world scenarios, where
    downsampling is usually performed after anisotropic blurring and sometimes signal-dependent
    noise is added. It is also affected by the in-camera signal processing (ISP) pipeline.
    Therefore, SISR models trained on bicubic degradation exhibit poor performance
    when handling real-world images. Moreover, all the aforementioned models can only
    be applied to some specific integral upsampling factors, but it is essential to
    develop scale arbitrary SISR models for different practical applications.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实场景中，降解模式复杂且未知，通常在各向异性模糊后进行下采样，有时还会加入信号相关噪声。这也受到相机内部信号处理（ISP）流程的影响。因此，基于双三次降解训练的SISR模型在处理真实图像时表现不佳。此外，所有上述模型只能应用于特定的整数放大因子，但为了不同实际应用，需要开发尺度任意的SISR模型。
- en: Recently, some datasets and new technologies have been proposed for real SISR.
    In [[19](#bib.bib19)], the RealSR dataset is proposed, where paired LR-HR images
    on the same scene are captured by adjusting the focal length of a digital camera.
    Meanwhile, a Laplacian Pyramid based Kernel Prediction Network (LP-KPN) is trained
    with this dataset to learn per-pixel kernels to recover SR images. After that,
    a series of real image pairs-based methods [[121](#bib.bib121), [122](#bib.bib122),
    [123](#bib.bib123)] are proposed. However, this dataset are post-processed and
    difficult to collect in large quantities, which still limits the model performance.
    Otherwise, some new technologies have been proposed, such as unsupervised learning [[124](#bib.bib124),
    [125](#bib.bib125)], self-supervised learning [[34](#bib.bib34), [126](#bib.bib126)],
    zero-shot learning [[34](#bib.bib34), [127](#bib.bib127)], meta-learning [[128](#bib.bib128),
    [129](#bib.bib129)], blind SISR, and scale arbitrary SISR [[130](#bib.bib130),
    [131](#bib.bib131)]. In this part, we introduce the latter three methods due to
    their impressive foresight and versatility.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，已经提出了一些数据集和新技术用于真实场景下的超分辨率重建（SISR）。在 [[19](#bib.bib19)]中，提出了RealSR数据集，其中通过调整数字相机的焦距拍摄了同一场景的配对低分辨率（LR）和高分辨率（HR）图像。同时，使用这个数据集训练了基于拉普拉斯金字塔的内核预测网络（LP-KPN），以学习每个像素的内核来恢复超分辨率图像。之后，提出了一系列基于真实图像对的方法 [[121](#bib.bib121),
    [122](#bib.bib122), [123](#bib.bib123)]。然而，这些数据集经过后处理，且难以大规模收集，这仍然限制了模型的性能。除此之外，一些新技术也被提出，如无监督学习 [[124](#bib.bib124),
    [125](#bib.bib125)], 自监督学习 [[34](#bib.bib34), [126](#bib.bib126)], 零样本学习 [[34](#bib.bib34),
    [127](#bib.bib127)], 元学习 [[128](#bib.bib128), [129](#bib.bib129)], 盲目超分辨率重建（Blind
    SISR），以及尺度任意超分辨率重建 [[130](#bib.bib130), [131](#bib.bib131)]。在这一部分，我们介绍后三种方法，因为它们在前瞻性和多样性方面表现突出。
- en: 4.1.1 Blind SISR
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 盲目超分辨率重建（Blind SISR）
- en: 'Blind SISR has attracted increasing attention due to its significance in real-world
    applications, which aims to super-resolved LR images with unknown degradation.
    According to the ways of degradation modelling, they can be simply divided into
    two categories: explicit degradation modeling methods and implicit degradation
    modeling methods. Among them, explicit degradation modeling methods can be further
    divided into two categories according to whether they use the kernel estimation
    technology. For instance, Zhang *et al.* proposed a simple and scalable deep CNN
    framework for multiple degradation (SRMD [[132](#bib.bib132)]) learning. In SRMD,
    the concatenated LR image and degradation maps are taken as input after the dimensionality
    stretching strategy. In DPSR [[133](#bib.bib133)], deep super-resolver can be
    used as a prior with a new degradation model, in order to handle LR images with
    arbitrary blur kernels. After that, UDVD [[134](#bib.bib134)], AMNet [[135](#bib.bib135)],
    USRNet[[136](#bib.bib136)], and a series of blind SISR methods are proposed by
    using the degradation map as an additional input for SR images reconstruction.
    In contrast, some blind SISR methods pay attention to the kernel estimation along
    with the SR process [[137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139),
    [140](#bib.bib140)]. For example, in IKC [[137](#bib.bib137)], the iterative kernel
    correction procedure is proposed to help the blind SISR task to find more accurate
    blur kernels. In DAN [[138](#bib.bib138)], Luo *et al.* adopted an alternating
    optimization algorithm to estimate blur kernel and restore SR image in a single
    network, which makes the restorer and estimator be well compatible with each other,
    and thus achieves good results in kernel estimation. However, the reconstruction
    accuracy of the above methods greatly depends on the accuracy of the degradation
    mode estimation. To address this issue, more implicit degradation modeling methods
    are proposed [[35](#bib.bib35), [141](#bib.bib141), [142](#bib.bib142)], which
    aim to implicitly learn the potential degradation modes by the external datasets.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 盲超分辨率重建（Blind SISR）由于在实际应用中的重要性，受到了越来越多的关注，它的目标是对具有未知退化的低分辨率图像进行超分辨率重建。根据退化建模的方法，它们可以简单地分为两类：显式退化建模方法和隐式退化建模方法。其中，显式退化建模方法可以进一步分为两类，依据是否使用内核估计技术。例如，张*等人*提出了一种简单且可扩展的深度卷积神经网络（CNN）框架用于多种退化的学习（SRMD [[132](#bib.bib132)]）。在SRMD中，经过维度拉伸策略的连接低分辨率图像和退化图被作为输入。在DPSR [[133](#bib.bib133)]中，深度超分辨率重建器可以作为新的退化模型的先验，以处理具有任意模糊内核的低分辨率图像。随后，UDVD [[134](#bib.bib134)]、AMNet [[135](#bib.bib135)]、USRNet[[136](#bib.bib136)]及一系列盲超分辨率重建方法被提出，这些方法使用退化图作为额外输入进行超分辨率图像重建。相比之下，一些盲超分辨率重建方法关注于在超分辨率过程中内核的估计[[137](#bib.bib137),
    [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140)]。例如，在IKC [[137](#bib.bib137)]中，提出了迭代内核校正过程，以帮助盲超分辨率任务找到更准确的模糊内核。在DAN [[138](#bib.bib138)]中，洛*等人*采用交替优化算法在一个网络中同时估计模糊内核和恢复超分辨率图像，使得恢复器和估计器能够很好地兼容，从而在内核估计中取得了良好的结果。然而，上述方法的重建精度在很大程度上依赖于退化模式估计的准确性。为了解决这个问题，提出了更多隐式退化建模方法[[35](#bib.bib35),
    [141](#bib.bib141), [142](#bib.bib142)]，这些方法旨在通过外部数据集隐式地学习潜在的退化模式。
- en: 4.1.2 Meta-Learning
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 元学习
- en: It is hard for artificial agents to quickly adapt to new things/data like human
    intelligence, since it is challenging to integrate the prior experience with a
    few more new information. Meta-learning, or learning to learn, is the mechanism
    proposed for the learning-based problems, which is usually used in few-shot/zero-shot
    learning and transfer learning. In meta-learning, the trained model quickly learns
    a new task in large task space, where the test samples are used to optimize the
    meta-learner, therefore the model can quickly adapt with the help of the meta-learner
    when it encounters new tasks. In SISR, considering the lack of real paired samples,
    we hope that the model can be trained on simulated paired datasets and then transfer
    the learned experience to the real SISR task. To address this issue, Soh *et al.*
    proposed the MZSR [[128](#bib.bib128)]. In MZSR, a novel training scheme based
    on meta-transfer learning is proposed to learn an effective initial weight for
    fast adaptation to new tasks with the zero-shot unsupervised setting, thus the
    model can be applied to the real-world scenarios and achieve good results. In [[129](#bib.bib129)],
    Park *et al.* proposed an effective meta-learning method to further improve the
    model performance without changing the architecture of conventional SISR networks.
    This method can be applied to any existing SISR models and effectively handle
    unknown SR kernels. In [[143](#bib.bib143)], Hu *et al.* proposed the first unified
    super-resolution network for arbitrary degradation parameters with meta-learning,
    termed Meta-USR [[143](#bib.bib143)].
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能代理难以像人类智能那样迅速适应新事物/数据，因为将先前经验与少量新信息整合是一项挑战。元学习，或学习如何学习，是为学习问题提出的机制，通常用于少样本/零样本学习和迁移学习。在元学习中，训练过的模型可以在大任务空间中快速学习新任务，其中测试样本用于优化元学习器，因此，当遇到新任务时，模型可以在元学习器的帮助下快速适应。在SISR中，考虑到真实配对样本的缺乏，我们希望模型可以在模拟配对数据集上训练，然后将学到的经验转移到真实的SISR任务中。为了解决这个问题，*Soh*等人提出了MZSR
    [[128](#bib.bib128)]。在MZSR中，提出了一种基于元迁移学习的新型训练方案，以快速适应新任务的零样本无监督设置，从而使模型可以应用于现实世界场景并取得良好结果。在[[129](#bib.bib129)]中，*Park*等人提出了一种有效的元学习方法，以进一步提高模型性能而无需更改常规SISR网络的结构。该方法可以应用于任何现有的SISR模型，并有效处理未知的超分辨率核。在[[143](#bib.bib143)]中，*Hu*等人提出了第一个统一的超分辨率网络，用于具有元学习的任意退化参数，称为Meta-USR
    [[143](#bib.bib143)]。
- en: 4.1.3 Scale Arbitrary SISR
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 尺度任意SISR
- en: In real application scenarios, in addition to processing real images, it is
    also important to handle arbitrary scale factors with a single model. To achieve
    this, Hu *et al.* proposed two simple but powerful methods termed Meta-SR [[130](#bib.bib130)]
    and Meta-USR [[143](#bib.bib143)]. Among them, Meta-SR is the first SISR method
    that can be used for arbitrary scale factors and Meta-USR is an improved version
    that can be applied to arbitrary degradation mode (including arbitrary scale factors).
    Although Meta-SR and Meta-USR achieve promising performance on non-integer scale
    factors, they cannot handle SR with asymmetric scale factors. To alleviate this
    problem, Wang *et al.* [[131](#bib.bib131)] suggested learning the scale-arbitrary
    SISR model from scale-specific networks and developed a plug-in module for existing
    models to achieve scale-arbitrary SR. Specifically, the proposed plug-in module
    uses conditional convolution to dynamically generate filters based on the input
    scale information, thus the networks equipped with the proposed module achieve
    promising results for arbitrary scales with only a single model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用场景中，除了处理真实图像外，使用单一模型处理任意尺度因子也非常重要。为此，*Hu*等人提出了两个简单但强大的方法，分别称为Meta-SR [[130](#bib.bib130)]和Meta-USR
    [[143](#bib.bib143)]。其中，Meta-SR是第一个可以用于任意尺度因子的SISR方法，而Meta-USR是一个改进版本，可以应用于任意退化模式（包括任意尺度因子）。尽管Meta-SR和Meta-USR在非整数尺度因子上取得了令人满意的性能，但它们无法处理具有不对称尺度因子的超分辨率。为了解决这个问题，*Wang*等人[[131](#bib.bib131)]建议从尺度特定网络中学习尺度任意的SISR模型，并为现有模型开发了一个插件模块以实现尺度任意的超分辨率。具体而言，所提出的插件模块使用条件卷积根据输入尺度信息动态生成滤波器，因此配备了该模块的网络可以仅用一个模型实现任意尺度下的良好结果。
- en: 4.2 Remote Sensing Image Super-Resolution
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 遥感图像超分辨率
- en: With the development of satellite image processing, remote sensing has become
    more and more important. However, due to the limitations of current imaging sensors
    and complex atmospheric conditions, such as limited spatial resolution, spectral
    resolution, and radiation resolution, we are facing huge challenges in remote
    sensing applications.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 随着卫星图像处理技术的发展，遥感变得越来越重要。然而，由于当前成像传感器的限制和复杂的大气条件，如有限的空间分辨率、光谱分辨率和辐射分辨率，我们在遥感应用中面临着巨大的挑战。
- en: Recently, many methods have been proposed for remote sensing image super-resolution.
    For example, a new unsupervised hourglass neural network is proposed in [[144](#bib.bib144)]
    to super-resolved remote sensing images. The model uses a generative random noise
    to introduce a higher variety of spatial patterns, which can be promoted to a
    higher scale according to a global reconstruction constraint. In [[145](#bib.bib145)],
    a Deep Residual Squeeze and Excitation Network (DRSEN) is proposed to overcome
    the problem of the high complexity of remote sensing image distribution. In [[146](#bib.bib146)],
    a mixed high-order attention network (MHAN) is proposed, which consists of a feature
    extraction network for feature extraction and a feature refinement network with
    the high-order attention mechanism for detail restoration. In [[147](#bib.bib147)],
    the authors developed a Dense-Sampling Super-Resolution Network (DSSR) to explore
    the large-scale SR reconstruction of the remote sensing imageries.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多方法被提出用于遥感图像超分辨率。例如，在 [[144](#bib.bib144)]中，提出了一种新的无监督沙漏神经网络用于超分辨率遥感图像。该模型使用生成随机噪声引入更高种类的空间模式，这些模式可以根据全局重建约束提升到更高的尺度。在 [[145](#bib.bib145)]中，提出了一种深度残差挤压和激励网络（DRSEN），以克服遥感图像分布高复杂度的问题。在 [[146](#bib.bib146)]中，提出了一种混合高阶注意力网络（MHAN），它由一个用于特征提取的特征提取网络和一个具有高阶注意力机制的特征精炼网络组成，用于细节恢复。在 [[147](#bib.bib147)]中，作者开发了一种密集采样超分辨率网络（DSSR），用于探索遥感图像的大规模SR重建。
- en: 4.3 Hyperspectral Image Super-Resolution
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 高光谱图像超分辨率
- en: In contrast to human eyes that can only be exposed to visible light, hyperspectral
    imaging is a technique for collecting and processing information across the entire
    range of electromagnetic spectrum[[148](#bib.bib148)]. The hyperspectral system
    is often compromised due to the limitations of the amount of the incident energy,
    hence there is a trade-off between the spatial and spectral resolution. Therefore,
    hyperspectral image super-resolution is studied to solve this problem.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 与只能接触可见光的人眼相比，高光谱成像是一种收集和处理整个电磁波谱范围内的信息的技术[[148](#bib.bib148)]。由于入射能量的限制，高光谱系统常常受到限制，因此在空间和光谱分辨率之间存在权衡。因此，高光谱图像超分辨率的研究旨在解决这一问题。
- en: In [[149](#bib.bib149)], a 3D fully convolutional neural network is proposed
    to extract the feature of hyperspectral images. In [[150](#bib.bib150)], Li *et
    al.* proposed a grouped deep recursive residual network by designing a group recursive
    module and embedding it into a global residual structure. In [[151](#bib.bib151)],
    an unsupervised CNN-based method is proposed to effectively exploit the underlying
    characteristics of the hyperspectral images. In [[152](#bib.bib152)], Jiang *et
    al.* proposed a group convolution and progressive upsampling framework to reduce
    the size of the model and made it feasible to obtain stable training results under
    small data conditions. In [[153](#bib.bib153)], a Spectral Grouping and Attention-Driven
    Residual Dense Network is proposed to facilitate the modeling of all spectral
    bands and focus on the exploration of spatial-spectral features.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[149](#bib.bib149)]中，提出了一种3D全卷积神经网络，用于提取高光谱图像的特征。在 [[150](#bib.bib150)]中，Li
    *等* 提出了一种分组深度递归残差网络，通过设计一个组递归模块并将其嵌入到全局残差结构中。在 [[151](#bib.bib151)]中，提出了一种无监督的基于CNN的方法，有效地利用了高光谱图像的潜在特征。在 [[152](#bib.bib152)]中，Jiang
    *等* 提出了一种组卷积和渐进上采样框架，以减少模型的大小，并使其在小数据条件下获得稳定的训练结果成为可能。在 [[153](#bib.bib153)]中，提出了一种光谱分组和注意力驱动的残差密集网络，以促进所有光谱带的建模，并专注于空间-光谱特征的探索。
- en: 4.4 Light Field Image Super-Resolution
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 光场图像超分辨率
- en: Light field (LF) camera is a camera that can capture information about the light
    field emanating from a scene and can provide multiple views of a scene. Recently,
    the LF image is becoming more and more important since it can be used for post-capture
    refocusing, depth sensing, and de-occlusion. However, LF cameras are faced with
    a trade-off between spatial and angular resolution [[154](#bib.bib154)]. In order
    to solve this issue, SR technology is introduced to achieve a good balance between
    spatial and angular resolution.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 光场 (LF) 相机是一种能够捕捉场景发出的光场信息并提供场景多视角的相机。近年来，LF 图像变得越来越重要，因为它可以用于后期重聚焦、深度感测和去遮挡。然而，LF
    相机面临空间分辨率和角度分辨率之间的权衡 [[154](#bib.bib154)]。为了解决这个问题，引入了 SR 技术以实现空间和角度分辨率之间的良好平衡。
- en: In [[155](#bib.bib155)], a cascade convolution neural network is introduced
    to simultaneously up-sample both the spatial and angular resolutions of a light
    field image. Meanwhile, a new light field image dataset is proposed for training
    and validation. In order to reduce the dependence of accurate depth or disparity
    information as priors for the light-field image super-resolution, Sun *et al.* [[156](#bib.bib156)]
    proposed a bidirectional recurrent convolutional neural network and an implicitly
    multi-scale fusion scheme for SR images reconstruction. In [[154](#bib.bib154)],
    Wang *et al.* proposed a spatial-angular interactive network (LF-InterNet) for
    LF image SR. Meanwhile, they designed an angular deformable alignment module for
    feature-level alignment and proposed a deformable convolution network (LF-DFnet [[157](#bib.bib157)])
    to handle the disparity problem of LF image SR.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[155](#bib.bib155)] 中，介绍了一种级联卷积神经网络，以同时上采样光场图像的空间和角度分辨率。同时，提出了一个新的光场图像数据集用于训练和验证。为了减少对光场图像超分辨率的准确深度或视差信息作为先验的依赖，Sun
    *等人* [[156](#bib.bib156)] 提出了一个双向递归卷积神经网络和一个隐式多尺度融合方案用于 SR 图像重建。在 [[154](#bib.bib154)]
    中，Wang *等人* 提出了一个空间-角度交互网络 (LF-InterNet) 用于 LF 图像 SR。同时，他们设计了一个角度可变形对齐模块用于特征级对齐，并提出了一个可变形卷积网络
    (LF-DFnet [[157](#bib.bib157)]) 来处理 LF 图像 SR 的视差问题。
- en: 4.5 Face Image Super-Resolution
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 面部图像超分辨率
- en: Face image super-resolution is the most famous field in which apply SR technology
    to domain-specific images. Due to the potential applications in facial recognition
    systems such as security and surveillance, face image super-resolution has become
    an active area of research.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 面部图像超分辨率是应用 SR 技术于特定领域图像的最著名领域之一。由于在安全和监控等面部识别系统中的潜在应用，面部图像超分辨率已经成为一个活跃的研究领域。
- en: Recently, DL-based methods have achieved remarkable progress in face image super-resolution.
    In [[158](#bib.bib158)], a dubbed CPGAN is proposed to address face hallucination
    and illumination compensation together, which is optimized by the conventional
    face hallucination loss and a new illumination compensation loss. In [[159](#bib.bib159)],
    Zhu *et al.* proposed to jointly learn face hallucination and facial spatial correspondence
    field estimation. In [[160](#bib.bib160)], spatial transformer networks are used
    in the generator architecture to overcome problems related to misalignment of
    input images. In [[161](#bib.bib161), [162](#bib.bib162)], the identity loss is
    utilized to preserve the identity-related features by minimizing the distance
    between the embedding vectors of SR and HR face images. In [[163](#bib.bib163)],
    the mask occlusion is treated as image noise and a joint and collaborative learning
    network (JDSR-GAN) is constructed for the masked face super-resolution task.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于 DL 的方法在面部图像超分辨率方面取得了显著进展。在 [[158](#bib.bib158)] 中，提出了一种名为 CPGAN 的方法，旨在同时解决面部幻觉和光照补偿问题，该方法通过传统的面部幻觉损失和新的光照补偿损失进行优化。在
    [[159](#bib.bib159)] 中，Zhu *等人* 提出了联合学习面部幻觉和面部空间对应场估计的方法。在 [[160](#bib.bib160)]
    中，生成器架构中使用了空间变换网络，以克服输入图像对齐问题。在 [[161](#bib.bib161), [162](#bib.bib162)] 中，通过最小化
    SR 和 HR 面部图像的嵌入向量之间的距离，利用了身份损失来保留与身份相关的特征。在 [[163](#bib.bib163)] 中，将遮挡处理为图像噪声，并构建了一个联合协作学习网络
    (JDSR-GAN) 用于遮挡面部超分辨率任务。
- en: 4.6 Medical Image Super-Resolution
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 医学图像超分辨率
- en: Medical imaging methods such as computational tomography (CT) and magnetic resonance
    imaging (MRI) are essential to clinical diagnoses and surgery planning. Hence,
    high-resolution medical images are desirable to provide necessary visual information
    of the human body. Recently, many methods have been proposed for medical image
    super-resolution
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 医学成像方法，如计算机断层扫描（CT）和磁共振成像（MRI），对于临床诊断和手术计划至关重要。因此，高分辨率的医学图像是非常需要的，以提供人体的必要视觉信息。最近，许多方法被提出用于医学图像超分辨率。
- en: For instance, Chen *et al.* proposed a Multi-level Densely Connected Super-Resolution
    Network (mDCSRN [[164](#bib.bib164)]) with GAN-guided training to generate high-resolution
    MR images, which can train and inference quickly. In [[165](#bib.bib165)], a 3D
    Super-Resolution Convolutional Neural Network (3DSRCNN) is proposed to improve
    the resolution of 3D-CT volumetric images. In [[166](#bib.bib166)], Zhao *et al.*
    proposed a deep Channel Splitting Network (CSN) to ease the representational burden
    of deep models and further improve the SR performance of MR images. In [[167](#bib.bib167)],
    Peng *et al.* introduced a Spatially-Aware Interpolation Network (SAINT) for medical
    slice synthesis to alleviate the memory constraint that volumetric data posed.
    All of these methods are the cornerstone of building the smart medical system
    and have great research significance and value.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Chen *等* 提出了一个多层次密集连接超分辨率网络（mDCSRN [[164](#bib.bib164)]），通过 GAN 引导训练生成高分辨率
    MR 图像，该网络可以快速训练和推断。在 [[165](#bib.bib165)] 中，提出了一个 3D 超分辨率卷积神经网络（3DSRCNN），用于提高
    3D-CT 体积图像的分辨率。在 [[166](#bib.bib166)] 中，Zhao *等* 提出了一个深度通道分割网络（CSN），以减轻深度模型的表征负担，并进一步提高
    MR 图像的超分辨率性能。在 [[167](#bib.bib167)] 中，Peng *等* 引入了一个空间感知插值网络（SAINT），用于医学切片合成，以缓解体积数据带来的内存限制。所有这些方法都是构建智能医疗系统的基石，具有重要的研究意义和价值。
- en: 4.7 Stereo Image Super-Resolution
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 立体图像超分辨率
- en: The dual camera has been widely used to estimate the depth information. Meanwhile,
    stereo imaging can also be applied in image restoration. In the stereo image pair,
    we have two images with disparity much larger than one pixel. Therefore, full
    use of these two images can enhance the spatial resolution.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 双摄像头已被广泛用于估计深度信息。同时，立体成像也可以应用于图像恢复。在立体图像对中，我们有两幅视差远大于一个像素的图像。因此，充分利用这两幅图像可以增强空间分辨率。
- en: In StereoSR [[168](#bib.bib168)], Jeon *et al.* proposed a method that learned
    a subpixel parallax prior to enhancing the spatial resolution of the stereo images.
    However, the number of shifted right images is fixed in StereoSR, which makes
    it fail to handle different stereo images with large disparity variations. To
    handle this problem, Wang *et al.* [[169](#bib.bib169), [170](#bib.bib170)] proposed
    a parallax-attention mechanism with a global receptive field along the epipolar
    line, which can generate reliable correspondence between the stereo image pair
    and improve the quality of the reconstructed SR images. In [[22](#bib.bib22)],
    a dataset named Flickr1024 is proposed for stereo image super-resolution, which
    consists of 1024 high-quality stereo image pairs. In [[171](#bib.bib171)], a stereo
    attention module is proposed to extend pre-trained SISR networks for stereo image
    SR, which interacts with stereo information bi-directionally in a symmetric and
    compact manner. In [[172](#bib.bib172)], a symmetric bi-directional parallax attention
    module and an inline occlusion handling scheme are proposed to effectively interact
    crossview information. In [[173](#bib.bib173)], a Stereo Super-Resolution and
    Disparity Estimation Feedback Network (SSRDE-FNet) is proposed to simultaneously
    handle the stereo image super-resolution and disparity estimation in a unified
    framework.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在 StereoSR [[168](#bib.bib168)] 中，Jeon *等* 提出了一个方法，通过学习亚像素视差先验来增强立体图像的空间分辨率。然而，在
    StereoSR 中，右侧图像的数量是固定的，这使得它无法处理视差变化较大的不同立体图像。为了解决这个问题，Wang *等* [[169](#bib.bib169),
    [170](#bib.bib170)] 提出了一个视差注意力机制，该机制在极线方向上具有全局感受野，可以生成立体图像对之间的可靠对应关系，并提高重建超分辨率图像的质量。在 [[22](#bib.bib22)]
    中，提出了一个名为 Flickr1024 的数据集，用于立体图像超分辨率，其中包含 1024 对高质量的立体图像。在 [[171](#bib.bib171)]
    中，提出了一个立体注意力模块，以扩展预训练的单图像超分辨率网络（SISR）用于立体图像超分辨率，该模块以对称紧凑的方式双向交互立体信息。在 [[172](#bib.bib172)]
    中，提出了一个对称双向视差注意力模块和一个内联遮挡处理方案，以有效地交互视图信息。在 [[173](#bib.bib173)] 中，提出了一个立体超分辨率和视差估计反馈网络（SSRDE-FNet），以统一框架同时处理立体图像超分辨率和视差估计。
- en: 'TABLE II: PSNR/SSIM comparison on Set5 ($\times 4$), Set14 ($\times 4$), and
    Urban100 ($\times 4$). Meanwhile, the training datasets and the number of model
    parameters are provided. Sort by PSNR of Set5 in ascending order. Best results
    are highlighted. Please zoom in to see details.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: Set5 ($\times 4$)、Set14 ($\times 4$) 和 Urban100 ($\times 4$) 上的 PSNR/SSIM
    比较。同时，提供了训练数据集和模型参数的数量。按 Set5 的 PSNR 进行升序排序。最佳结果已突出显示。请放大以查看详细信息。'
- en: '|       Models |              Set5       PSNR/SSIM |              Set14       PSNR/SSIM
    |              Urban100       PSNR/SSIM |       Training Datasets |       Parameters
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|       模型 |       Set5       PSNR/SSIM |       Set14       PSNR/SSIM |       Urban100
          PSNR/SSIM |       训练数据集 |       参数 |'
- en: '|       SRCNN [[174](#bib.bib174)] |       30.48/0.8628 |       27.50/0.7513
    |       24.52/0.7221 |       T91+ImageNet |       57K |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|       SRCNN [[174](#bib.bib174)] |       30.48/0.8628 |       27.50/0.7513
    |       24.52/0.7221 |       T91+ImageNet |       57K |'
- en: '|       ESPCN [[33](#bib.bib33)] |       30.66/0.8646 |       27.71/0.7562
    |       24.60/0.7360 |       T91+ImageNet |       20K |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|       ESPCN [[33](#bib.bib33)] |       30.66/0.8646 |       27.71/0.7562
    |       24.60/0.7360 |       T91+ImageNet |       20K |'
- en: '|       FSRCNN [[13](#bib.bib13)] |       30.71/0.8660 |       27.59/0.7550
    |       24.62/0.7280 |       T91+General-100 |       13K |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|       FSRCNN [[13](#bib.bib13)] |       30.71/0.8660 |       27.59/0.7550
    |       24.62/0.7280 |       T91+General-100 |       13K |'
- en: '|       VDSR [[50](#bib.bib50)] |       31.35/0.8838 |       28.02/0.7680 |
          25.18/0.7540 |       BSD+T91 |       665K |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|       VDSR [[50](#bib.bib50)] |       31.35/0.8838 |       28.02/0.7680 |
          25.18/0.7540 |       BSD+T91 |       665K |'
- en: '|       LapSRN [[64](#bib.bib64)] |       31.54/0.8855 |       28.19/0.7720
    |       25.21/0.7560 |       BSD+T91 |       812K |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|       LapSRN [[64](#bib.bib64)] |       31.54/0.8855 |       28.19/0.7720
    |       25.21/0.7560 |       BSD+T91 |       812K |'
- en: '|       DRRN [[56](#bib.bib56)] |       31.68/0.8888 |       28.21/0.7721 |
          25.44/0.7638 |       BSD+T91 |       297K |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|       DRRN [[56](#bib.bib56)] |       31.68/0.8888 |       28.21/0.7721 |
          25.44/0.7638 |       BSD+T91 |       297K |'
- en: '|       MemNet [[57](#bib.bib57)] |       31.74/0.8893 |       28.26/0.7723
    |       25.50/0.7630 |       BSD+T91 |       677K |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|       MemNet [[57](#bib.bib57)] |       31.74/0.8893 |       28.26/0.7723
    |       25.50/0.7630 |       BSD+T91 |       677K |'
- en: '|       AWSRN-S [[175](#bib.bib175)] |       31.77/0.8893 |       28.35/0.7761
    |       25.56/0.7678 |       DIV2K |       588K |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|       AWSRN-S [[175](#bib.bib175)] |       31.77/0.8893 |       28.35/0.7761
    |       25.56/0.7678 |       DIV2K |       588K |'
- en: '|       IDN [[61](#bib.bib61)] |       31.82/0.8903 |       28.25/0.7730 |
          25.41/0.7632 |       BSD+T91 |       678K |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|       IDN [[61](#bib.bib61)] |       31.82/0.8903 |       28.25/0.7730 |
          25.41/0.7632 |       BSD+T91 |       678K |'
- en: '|       NLRN [[80](#bib.bib80)] |       31.92/0.8916 |       28.36/0.7745 |
          25.79/0.7729 |       BSD+T91 |       330K |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|       NLRN [[80](#bib.bib80)] |       31.92/0.8916 |       28.36/0.7745 |
          25.79/0.7729 |       BSD+T91 |       330K |'
- en: '|       CARN-M [[58](#bib.bib58)] |       31.92/0.8903 |       28.42/0.7762
    |       25.62/0.7694 |       DIV2K |       412K |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|       CARN-M [[58](#bib.bib58)] |       31.92/0.8903 |       28.42/0.7762
    |       25.62/0.7694 |       DIV2K |       412K |'
- en: '|       MAFFSRN [[176](#bib.bib176)] |       32.24/0.8952 |       28.61/0.7819
    |       26.11/0.7858 |       DIV2K |       550K |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|       MAFFSRN [[176](#bib.bib176)] |       32.24/0.8952 |       28.61/0.7819
    |       26.11/0.7858 |       DIV2K |       550K |'
- en: '|       RFDN [[177](#bib.bib177)] |       32.18/0.8948 |       28.58/0.7812
    |       26.04/0.7848 |       DIV2K |       441K |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|       RFDN [[177](#bib.bib177)] |       32.18/0.8948 |       28.58/0.7812
    |       26.04/0.7848 |       DIV2K |       441K |'
- en: '|       ESRT [[120](#bib.bib120)] |       32.19/0.8947 |       28.69/0.7833
    |       26.39/0.7962 |       DIV2K |       751K |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|       ESRT [[120](#bib.bib120)] |       32.19/0.8947 |       28.69/0.7833
    |       26.39/0.7962 |       DIV2K |       751K |'
- en: '|       IMDN [[178](#bib.bib178)] |       32.21/0.8949 |       28.58/0.7811
    |       26.04/0.7838 |       DIV2K |       715K |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|       IMDN [[178](#bib.bib178)] |       32.21/0.8949 |       28.58/0.7811
    |       26.04/0.7838 |       DIV2K |       715K |'
- en: '|       MSFIN [[179](#bib.bib179)] |       32.28/0.8957 |       28.57/0.7813
    |       26.13/0.7865 |       DIV2K |       682K |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|       MSFIN [[179](#bib.bib179)] |       32.28/0.8957 |       28.57/0.7813
    |       26.13/0.7865 |       DIV2K |       682K |'
- en: '|       DSRN [[87](#bib.bib87)] |       31.40/0.8830 |       28.07/0.7700 |
          25.08/0.7470 |       T91 |       1.2M |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|       DSRN [[87](#bib.bib87)] |       31.40/0.8830 |       28.07/0.7700 |
          25.08/0.7470 |       T91 |       1.2M |'
- en: '|       DRCN [[55](#bib.bib55)] |       31.53/0.8838 |       28.02/0.7670 |
          25.14/0.7510 |       T91 |       1.8M |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|       DRCN [[55](#bib.bib55)] |       31.53/0.8838 |       28.02/0.7670 |
          25.14/0.7510 |       T91 |       1.8M |'
- en: '|       MADNet [[180](#bib.bib180)] |       31.95/0.8917 |       28.44/0.7780
    |       25.76/0.7746 |       DIV2K |       1M |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|       MADNet [[180](#bib.bib180)] |       31.95/0.8917 |       28.44/0.7780
    |       25.76/0.7746 |       DIV2K |       1M |'
- en: '|       SRMD [[132](#bib.bib132)] |       31.96/0.8925 |       28.35/0.7787
    |       25.68/0.7731 |       BSD+DIV2K+WED |       1.6M |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|       SRMD [[132](#bib.bib132)] |       31.96/0.8925 |       28.35/0.7787
    |       25.68/0.7731 |       BSD+DIV2K+WED |       1.6M |'
- en: '|       SRDenseNet [[60](#bib.bib60)] |       32.02/0.8934 |       28.50/0.7782
    |       26.05/0.7819 |       ImageNet |       2.0M |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|       SRDenseNet [[60](#bib.bib60)] |       32.02/0.8934 |       28.50/0.7782
    |       26.05/0.7819 |       ImageNet |       2.0M |'
- en: '|       SRResNet [[38](#bib.bib38)] |       32.05/0.8910 |       28.49/0.7800
    |       ——-/——- |       ImageNet |       1.5M |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|       SRResNet [[38](#bib.bib38)] |       32.05/0.8910 |       28.49/0.7800
    |       ——-/——- |       ImageNet |       1.5M |'
- en: '|       MSRN [[52](#bib.bib52)] |       32.07/0.8903 |       28.60/0.7751 |
          26.04/0.7896 |       DIV2K |       6.3M |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|       MSRN [[52](#bib.bib52)] |       32.07/0.8903 |       28.60/0.7751 |
          26.04/0.7896 |       DIV2K |       6.3M |'
- en: '|       CARN [[58](#bib.bib58)] |       32.13/0.8937 |       28.60/0.7806 |
          26.07/0.7837 |       BSD+T91+DIV2K |       1.6M |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|       CARN [[58](#bib.bib58)] |       32.13/0.8937 |       28.60/0.7806 |
          26.07/0.7837 |       BSD+T91+DIV2K |       1.6M |'
- en: '|       SeaNet [[89](#bib.bib89)] |       32.33/0.8970 |       28.81/0.7855
    |       26.32/0.7942 |       DIV2K |       7.4M |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|       SeaNet [[89](#bib.bib89)] |       32.33/0.8970 |       28.81/0.7855
    |       26.32/0.7942 |       DIV2K |       7.4M |'
- en: '|       CRN [[58](#bib.bib58)] |       32.34/0.8971 |       28.74/0.7855 |
          26.44/0.7967 |       DIV2K |       9.5M |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|       CRN [[58](#bib.bib58)] |       32.34/0.8971 |       28.74/0.7855 |
          26.44/0.7967 |       DIV2K |       9.5M |'
- en: '|       EDSR [[51](#bib.bib51)] |       32.46/0.8968 |       28.80/0.7876 |
          26.64/0.8033 |       DIV2K |       43M |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|       EDSR [[51](#bib.bib51)] |       32.46/0.8968 |       28.80/0.7876 |
          26.64/0.8033 |       DIV2K |       43M |'
- en: '|       RDN [[73](#bib.bib73)] |       32.47/0.8990 |       28.81/0.7871 |
          26.61/0.8028 |       DIV2K |       22.6M |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|       RDN [[73](#bib.bib73)] |       32.47/0.8990 |       28.81/0.7871 |
          26.61/0.8028 |       DIV2K |       22.6M |'
- en: '|       DBPN [[86](#bib.bib86)] |       32.47/0.8980 |       28.82/0.7860 |
          26.38/0.7946 |       DIV2K+Flickr2K |       10M |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|       DBPN [[86](#bib.bib86)] |       32.47/0.8980 |       28.82/0.7860 |
          26.38/0.7946 |       DIV2K+Flickr2K |       10M |'
- en: '|       SRFBN [[66](#bib.bib66)] |       32.47/0.8983 |       28.81/0.7868
    |       26.60/0.8015 |       DIV2K+Flickr2K |       3.63M |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|       SRFBN [[66](#bib.bib66)] |       32.47/0.8983 |       28.81/0.7868
    |       26.60/0.8015 |       DIV2K+Flickr2K |       3.63M |'
- en: '|       MDCN [[63](#bib.bib63)] |       32.48/0.8985 |       28.83/0.7879 |
          26.69/0.8049 |       DIV2K |       4.5M |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|       MDCN [[63](#bib.bib63)] |       32.48/0.8985 |       28.83/0.7879 |
          26.69/0.8049 |       DIV2K |       4.5M |'
- en: '|       RNAN [[81](#bib.bib81)] |       32.49/0.8982 |       28.83/0.7878 |
          26.61/0.8023 |       DIV2K |       7.5M |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|       RNAN [[81](#bib.bib81)] |       32.49/0.8982 |       28.83/0.7878 |
          26.61/0.8023 |       DIV2K |       7.5M |'
- en: '|       SRRFN [[59](#bib.bib59)] |       32.56/0.8993 |       28.86/0.7882
    |       26.78/0.8071 |       DIV2K |       4.2M |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|       SRRFN [[59](#bib.bib59)] |       32.56/0.8993 |       28.86/0.7882
    |       26.78/0.8071 |       DIV2K |       4.2M |'
- en: '|       IGNN [[181](#bib.bib181)] |       32.57/0.8998 |       28.85/0.7891
    |       26.84/0.8090 |       DIV2K |       48M |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|       IGNN [[181](#bib.bib181)] |       32.57/0.8998 |       28.85/0.7891
    |       26.84/0.8090 |       DIV2K |       48M |'
- en: '|       NLSA [[182](#bib.bib182)] |       32.59/0.9000 |       28.87/0.7891
    |       26.96/0.8109 |       DIV2K |       41M |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|       NLSA [[182](#bib.bib182)] |       32.59/0.9000 |       28.87/0.7891
    |       26.96/0.8109 |       DIV2K |       41M |'
- en: '|       RCAN [[183](#bib.bib183)] |       32.63/0.9002 |       28.87/0.7889
    |       26.82/0.8087 |       DIV2K |       16M |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|       RCAN [[183](#bib.bib183)] |       32.63/0.9002 |       28.87/0.7889
    |       26.82/0.8087 |       DIV2K |       16M |'
- en: '|       SAN [[79](#bib.bib79)] |       32.64/0.9003 |       28.92/0.7888 |
          26.79/0.8068 |       DIV2K |       15.7M |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|       SAN [[79](#bib.bib79)] |       32.64/0.9003 |       28.92/0.7888 |
          26.79/0.8068 |       DIV2K |       15.7M |'
- en: '|       HAN [[82](#bib.bib82)] |       32.64/0.9002 |       28.90/0.7890 |
          26.85/0.8094 |       DIV2K |       16.1M |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|       HAN [[82](#bib.bib82)] |       32.64/0.9002 |       28.90/0.7890 |
          26.85/0.8094 |       DIV2K |       16.1M |'
- en: '|       IPT [[117](#bib.bib117)] |       32.64/——– |       29.01/——– |       27.26/——–
    |       ImageNet |       115.5M |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|       IPT [[117](#bib.bib117)] |       32.64/——– |       29.01/——– |       27.26/——–
    |       ImageNet |       115.5M |'
- en: '|       RFANet [[177](#bib.bib177)] |       32.66/0.9004 |       28.88/0.7894
    |       26.92/0.8112 |       DIV2K |       11M |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|       RFANet [[177](#bib.bib177)] |       32.66/0.9004 |       28.88/0.7894
    |       26.92/0.8112 |       DIV2K |       11M |'
- en: '|       DRN-S [[102](#bib.bib102)] |       32.68/0.9010 |       28.93/0.7900
    |       26.84/0.8070 |       DIV2K+Flickr2K |       4.8M |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|       DRN-S [[102](#bib.bib102)] |       32.68/0.9010 |       28.93/0.7900
    |       26.84/0.8070 |       DIV2K+Flickr2K |       4.8M |'
- en: '|       RRDB [[97](#bib.bib97)] |       32.73/0.9011 |       28.99/0.7917 |
          27.03/0.8153 |       DIV2K+Flickr2K |       16.7M |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|       RRDB [[97](#bib.bib97)] |       32.73/0.9011 |       28.99/0.7917 |
          27.03/0.8153 |       DIV2K+Flickr2K |       16.7M |'
- en: '|       DRN-L [[102](#bib.bib102)] |       32.74/0.9020 |       28.98/0.7920
    |       27.03/0.8130 |       DIV2K+Flickr2K |       9.8M |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|       DRN-L [[102](#bib.bib102)] |       32.74/0.9020 |       28.98/0.7920
    |       27.03/0.8130 |       DIV2K+Flickr2K |       9.8M |'
- en: '|       SwinIR [[118](#bib.bib118)] |       32.92/0.9044 |       29.09/0.7950
    |       27.45/0.8254 |       DIV2K+Flickr2K |       11.8M |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|       SwinIR [[118](#bib.bib118)] |       32.92/0.9044 |       29.09/0.7950
    |       27.45/0.8254 |       DIV2K+Flickr2K |       11.8M |'
- en: 5 Reconstruction Results
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 重建结果
- en: 'In order to help readers intuitively know the performance of the aforementioned
    SISR models, we provide a detailed comparison of reconstruction results of these
    models. According to the number of model parameters, we divide SISR models into
    two types: lightweight models and large models. Note that we call model with parameters
    less than 1000K as lightweight model and model with parameters more than 1M (M=million)
    as large model. Specifically, we collect 44 representative SISR models, including
    the most classic, latest, and SOTA SISR models.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助读者直观地了解上述SISR模型的性能，我们提供了这些模型重建结果的详细比较。根据模型参数的数量，我们将SISR模型分为两类：轻量级模型和大型模型。需要注意的是，我们将参数少于1000K的模型称为轻量级模型，将参数多于1M（M=百万）的模型称为大型模型。具体而言，我们收集了44个具有代表性的SISR模型，包括最经典、最新和SOTA
    SISR模型。
- en: 'In TABLE [II](#S4.T2 "TABLE II ‣ 4.7 Stereo Image Super-Resolution ‣ 4 Domain-Specific
    Applications ‣ From Beginner to Master: A Survey for Deep Learning-based Single-Image
    Super-Resolution") we provide the reconstruction results, training datasets, and
    model parameters of these models (lightweight models and large models are separated
    by the bold black line). According to the results, we can find that: (1) using
    a large dataset (e.g., DIV2K+Flickr2K) can make the model achieve better results;
    (2) it is not entirely correct that the more model parameters, the better the
    model performance. This means that unreasonably increasing the model size is not
    the best solution; (3) Transformer-based models show strong advantages, whether
    in lightweight models (e.g., ESRT [[120](#bib.bib120)]) or large models (e.g.,
    SwinIR [[118](#bib.bib118)]); (4) research on the tiny model (parameters less
    than 1000K) is still lacking. In the future, it is still important to explore
    more discriminative evaluation indicators and develop more effective SISR models.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [II](#S4.T2 "TABLE II ‣ 4.7 Stereo Image Super-Resolution ‣ 4 Domain-Specific
    Applications ‣ From Beginner to Master: A Survey for Deep Learning-based Single-Image
    Super-Resolution")中，我们提供了这些模型的重建结果、训练数据集和模型参数（轻量级模型和大型模型通过**黑色粗线**分开）。根据结果，我们可以发现：(1)
    使用大型数据集（例如，DIV2K+Flickr2K）可以使模型获得更好的结果；(2) 模型参数越多，模型性能越好并不完全正确。这意味着不合理地增加模型规模不是最佳解决方案；(3)
    基于Transformer的模型展示了强大的优势，无论是在轻量级模型（例如，ESRT [[120](#bib.bib120)]）还是大型模型（例如，SwinIR [[118](#bib.bib118)]）；(4)
    对微型模型（参数少于1000K）的研究仍然不足。未来，探索更多有辨别力的评估指标和开发更有效的SISR模型仍然很重要。'
- en: 6 Remaining Issues and Future Directions
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 剩余问题与未来方向
- en: It is true that the above models have achieved promising results and have greatly
    promoted the development of SISR. However, we cannot ignore that there are still
    many challenging issues in SISR. In this section, we will point out some of the
    challenges and summarize some promising trends and future directions.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，上述模型已取得了令人鼓舞的结果，并极大地推动了SISR的发展。然而，我们不能忽视SISR中仍然存在的许多挑战性问题。在这一部分，我们将指出一些挑战，并总结一些有前景的趋势和未来方向。
- en: 6.1 Lightweight SISR for Edge Devices
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 面向边缘设备的轻量级SISR
- en: With the huge development of smart terminal market, research on lightweight
    SISR models has gained increasing attention. Although existing lightweight SISR
    models have achieved a good balance between model size and performance, we find
    that they still cannot be used in edge devices (e.g., smartphones, smart cameras).
    This is because the model size and computational costs of these models are still
    exceed the limits of edge devices. Therefore, exploring lightweight SISR models
    that can be practical in use for the edge devices has great research significance
    and commercial value. To achieve this, more efficient network structure and mechanisms
    are worthy of further exploration. Moreover, it is also necessary to use technologies
    like network binarization [[184](#bib.bib184)] and network quantization [[185](#bib.bib185)]
    to further reduce the model size. In the future, it is worth combining the lightweight
    SISR models with model compression schemes to achieve the usage of SISR on edge
    devices.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 随着智能终端市场的巨大增长，轻量级SISR模型的研究获得了越来越多的关注。虽然现有的轻量级SISR模型在模型尺寸和性能之间取得了良好的平衡，但我们发现它们仍然不能用于边缘设备（例如，智能手机、智能相机）。这是因为这些模型的模型尺寸和计算成本仍然超出了边缘设备的限制。因此，探索可在边缘设备上实际使用的轻量级SISR模型具有重要的研究意义和商业价值。为此，更高效的网络结构和机制值得进一步探索。此外，还需要使用像网络二值化[[184](#bib.bib184)]和网络量化[[185](#bib.bib185)]等技术来进一步减少模型尺寸。未来，将轻量级SISR模型与模型压缩方案相结合，实现SISR在边缘设备上的应用是值得的。
- en: 6.2 Flexible and Adjustable SISR
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 灵活可调的SISR
- en: Although DL-based SISR models have achieved gratifying results, we notice a
    phenomenon that the structure of all these models must be consistent during training
    and testing. This greatly limits the flexibility of the model, making the same
    model difficult to be applied to different applications scenarios. In other words,
    training specially designed models to meet the requirements of different platforms
    in necessary for previous methods. However, it will require a great amount of
    manpower and material resources. Therefore, it is crucial for us to design a flexible
    and adjustable SISR model that can be deployed on different platforms without
    retraining while keeping good reconstruction results.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于深度学习（DL）的单图像超分辨率（SISR）模型已取得了令人满意的成果，我们注意到一个现象，即所有这些模型在训练和测试期间的结构必须保持一致。这极大地限制了模型的灵活性，使得相同的模型难以应用于不同的应用场景。换句话说，以前的方法需要特别设计模型以满足不同平台的要求。然而，这将需要大量的人力和物力。因此，我们需要设计一个灵活可调的SISR模型，该模型可以在不同平台上部署而无需重新训练，同时保持良好的重建结果。
- en: 6.3 New Loss Functions and Assessment Methods
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 新的损失函数和评估方法
- en: In the past, most of SISR models relied on L1 loss or MSE loss. Although some
    other new loss functions like content loss, texture loss, and adversarial loss
    have been proposed, they still cannot achieve a good balance between reconstruction
    accuracy and perceptual quality. Therefore, it remains a important research topic
    to explore new loss functions that can ease the perception-distortion trade-off.
    Meanwhile, some new assessment methods are subjective and unfair. Therefore, new
    assessment methods that can efficiently reflect image perception and distortion
    at the same time are also essential.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，大多数SISR模型依赖于L1损失或均方误差（MSE）损失。尽管已经提出了一些其他新的损失函数，如内容损失、纹理损失和对抗损失，但它们仍然不能在重建精度和感知质量之间取得良好的平衡。因此，探索可以缓解感知-失真权衡的新损失函数仍然是一个重要的研究课题。同时，一些新的评估方法是主观的和不公平的。因此，同时有效反映图像感知和失真的新评估方法也是必不可少的。
- en: 6.4 Mutual Promotion with High-Level Tasks
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 与高层任务的相互促进
- en: As we all know, high-level computer vision tasks (e.g., image classification,
    image segmentation, and image analysis) are highly dependent on the quality of
    the input image, so SISR technology is usually used for pre-processing. Meanwhile,
    the quality of the SR images will greatly affect the accuracy of these tasks.
    Therefore, we recommend using the accuracy of high-level CV tasks as an evaluation
    indicator to measure the quality of the SR image. Meanwhile, we can design some
    loss functions related to high-level tasks, thus we can combine the feedback from
    other tasks to further improve the quality of SR images. On the other hand, we
    find that the two-step method of pre-processing the image using the SISR model
    is inefficient, which cannot fully use the potential features of the image itself,
    resulting in poor model performance. Therefore, we recommend exploring SISR models
    that can interact with high-level CV tasks, thus SISR and other tasks can promote
    and learn from each other.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，高级计算机视觉任务（例如，图像分类、图像分割和图像分析）高度依赖输入图像的质量，因此SISR技术通常用于预处理。同时，SR图像的质量将极大地影响这些任务的准确性。因此，我们推荐使用高级计算机视觉任务的准确性作为评估指标来衡量SR图像的质量。同时，我们可以设计一些与高级任务相关的损失函数，从而结合其他任务的反馈进一步提高SR图像的质量。另一方面，我们发现使用SISR模型进行图像预处理的两步法效率低下，不能充分利用图像本身的潜在特征，导致模型性能较差。因此，我们建议探索可以与高级计算机视觉任务互动的SISR模型，从而实现SISR与其他任务的互促与学习。
- en: 6.5 Efficient and Accurate Real SISR
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 高效且准确的实际SISR
- en: Real SISR is destined to become the future mainstream in this field. Therefore,
    it will inevitably become the focus of researchers in the next few years. On the
    one hand, a sufficiently large and accurate real image dataset is critical to
    Real SISR. To achieve this, in addition to the manual collection, we recommend
    using generative technology to simulate the images, as well as using the generative
    adversarial network to simulate enough degradation modes to build the large real
    dataset. On the other hand, considering the difficulty of constructing real image
    dataset, it is important to develop unsupervised learning-based SISR, meta learning-based
    SISR, and blind SISR. Among them, unsupervised learning can make the models get
    rid of the dependence on dataset, meta learning can help models migrate from simulated
    datasets to real data with simple fine-tuning, and blind SISR can display or implicitly
    learn the degradation mode of the image, and then reconstruct high-quality SR
    images based on the learned degradation mode. Although plenty of blind SISR methods
    have been proposed, they always have unstable performance or have strict prerequisites.
    Therefore, combining them may bring new solutions for real SISR.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的SISR注定会成为这一领域的未来主流。因此，它将不可避免地成为未来几年研究者的焦点。一方面，足够大且准确的真实图像数据集对实际SISR至关重要。为了实现这一目标，除了手动收集外，我们推荐使用生成技术来模拟图像，并利用生成对抗网络来模拟足够的降质模式，从而构建大规模真实数据集。另一方面，考虑到构建真实图像数据集的难度，开发基于无监督学习的SISR、基于元学习的SISR以及盲SISR显得尤为重要。其中，无监督学习可以使模型摆脱对数据集的依赖，元学习可以帮助模型通过简单的微调从模拟数据集迁移到真实数据，而盲SISR可以显示或隐式地学习图像的降质模式，然后基于学习到的降质模式重建高质量的SR图像。尽管已经提出了大量的盲SISR方法，但它们总是存在性能不稳定或有严格前提条件的问题。因此，将这些方法结合起来可能会为实际SISR带来新的解决方案。
- en: 6.6 Efficient and Accurate Scale Arbitrary SISR
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 高效且准确的任意尺度SISR
- en: SISR has seen its applications in diverse real-life scenarios and users. Therefore,
    it is necessary to develop a flexible and universal scale arbitrary SISR model
    that can be adapted to any scale, including asymmetric and non-integer scale factors.
    Currently, most DL-based SISR models can only be applied to one or a limited number
    of multiple upsampling factors. Although a few scale arbitrary SISR methods have
    also been proposed, they tend to lack the flexibility to use and the simplicity
    to be implemented, which greatly limits their application scenarios. Therefore,
    exploring a CNN-based accurate scale arbitrary SISR model as simple and flexible
    as Bicubic is crucial to the spread of SISR technology.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: SISR已经在各种现实场景和用户中得到了应用。因此，有必要开发一个灵活且通用的尺度任意SISR模型，以适应任何尺度，包括不对称和非整数尺度因子。目前，大多数基于深度学习的SISR模型仅适用于一个或有限数量的多重上采样因子。尽管也提出了一些尺度任意的SISR方法，但它们往往缺乏使用的灵活性和实现的简便性，这极大地限制了它们的应用场景。因此，探索一种基于卷积神经网络的准确尺度任意SISR模型，其简单且灵活如双三次插值，是SISR技术推广的关键。
- en: 6.7 Consider the Characteristics of Different Images
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 考虑不同图像的特征
- en: Although a series of models have been proposed for domain-specific applications,
    most of them directly transfer the SISR methods to these specific fields. This
    is the simplest and feasible method, but it will also inhibit the model performance
    since they ignore the data structure characteristics of the domain-specific images.
    Therefore, fully mining and using the potential prior and data characteristics
    of the domain-specific images is beneficial for efficient and accurate domain-specific
    SISR models construction. In the future, it will be a trend to further optimize
    the existing SISR models based on the prior knowledge and the characteristics
    of the domain-specific images.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经提出了一系列针对特定领域应用的模型，但大多数模型直接将单图像超分辨率（SISR）方法应用于这些特定领域。这是最简单且可行的方法，但也会抑制模型性能，因为这些方法忽略了特定领域图像的数据结构特征。因此，充分挖掘和利用特定领域图像的潜在先验和数据特征对高效准确构建领域特定的SISR模型是有利的。未来，基于先验知识和特定领域图像特征进一步优化现有SISR模型将成为一种趋势。
- en: 7 Conclusion
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this survey, we have given a comprehensive overview of DL-based single image
    super-resolution methods according to their targets, including reconstruction
    efficiency, reconstruction accuracy, perceptual quality, and other technologies
    that can further improve model performance. Meanwhile, we provided a detailed
    introduction to the related works of SISR and introduced a series of new tasks
    and domain-specific applications extended by SISR. In order to view the performance
    of each model more intuitively, we also provided a detailed comparison of reconstruction
    results. Moreover, we provided some underlying problems in SISR and introduced
    several new trends and future directions worthy of further exploration. We believe
    that the survey can help researchers better understand this field and further
    promote the development of this field.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们对基于深度学习的单图像超分辨率方法进行了全面概述，包括重建效率、重建精度、感知质量以及其他可以进一步提高模型性能的技术。同时，我们详细介绍了SISR的相关工作，并介绍了一系列由SISR扩展的新任务和特定领域应用。为了更直观地查看每个模型的性能，我们还提供了详细的重建结果比较。此外，我们提出了一些SISR中的潜在问题，并介绍了几个值得进一步探索的新趋势和未来方向。我们相信，这项调查可以帮助研究人员更好地理解该领域，并进一步推动该领域的发展。
- en: References
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] C. E. Duchon, “Lanczos filtering in one and two dimensions,” *Journal of
    Applied Meteorology and Climatology*, vol. 18, 1979.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] C. E. Duchon，《一维和二维的Lanczos滤波》，*应用气象学与气候学杂志*，第18卷，1979年。'
- en: '[2] Jian Sun, Zongben Xu, and Heung-Yeung Shum, “Image super-resolution using
    gradient profile prior,” in *CVPR*, 2008.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Jian Sun, Zongben Xu, 和 Heung-Yeung Shum，《使用梯度轮廓先验进行图像超分辨率》，在*计算机视觉与模式识别大会*，2008年。'
- en: '[3] K. I. Kim and Y. Kwon, “Single-image super-resolution using sparse regression
    and natural image prior,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 32, 2010.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] K. I. Kim 和 Y. Kwon，《使用稀疏回归和自然图像先验的单图像超分辨率》，*IEEE模式分析与机器智能汇刊*，第32卷，2010年。'
- en: '[4] H. Chang, D.-Y. Yeung, and Y. Xiong, “Super-resolution through neighbor
    embedding,” in *CVPR*, 2004.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] H. Chang, D.-Y. Yeung, 和 Y. Xiong，《通过邻域嵌入进行超分辨率》，在*计算机视觉与模式识别大会*，2004年。'
- en: '[5] J. Yang, J. Wright, T. S. Huang, and Y. Ma, “Image super-resolution via
    sparse representation,” *IEEE Transactions on Image Processing*, vol. 19, 2010.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Yang, J. Wright, T. S. Huang, 和 Y. Ma，“通过稀疏表示实现图像超分辨率，”*IEEE Transactions
    on Image Processing*，第19卷，2010年。'
- en: '[6] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    2015.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Y. LeCun, Y. Bengio, 和 G. Hinton，“深度学习，”*Nature*，第521卷，2015年。'
- en: '[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” 2012.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton，“利用深度卷积神经网络进行Imagenet分类，”2012年。'
- en: '[8] R. Collobert and J. Weston, “A unified architecture for natural language
    processing: Deep neural networks with multitask learning,” in *ICML*, 2008.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] R. Collobert 和 J. Weston，“自然语言处理的统一架构：带有多任务学习的深度神经网络，”发表于*ICML*，2008年。'
- en: '[9] C. Dong, C. C. Loy, K. He, and X. Tang, “Learning a deep convolutional
    network for image super-resolution,” in *ECCV*, 2014.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] C. Dong, C. C. Loy, K. He, 和 X. Tang，“学习深度卷积网络以实现图像超分辨率，”发表于*ECCV*，2014年。'
- en: '[10] W. Dong, L. Zhang, G. Shi, and X. Wu, “Image deblurring and super-resolution
    by adaptive sparse domain selection and adaptive regularization,” *IEEE Transactions
    on Image Processing*, vol. 20, 2011.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] W. Dong, L. Zhang, G. Shi, 和 X. Wu，“通过自适应稀疏领域选择和自适应正则化进行图像去模糊和超分辨率，”*IEEE
    Transactions on Image Processing*，第20卷，2011年。'
- en: '[11] W. Yang, X. Zhang, Y. Tian, W. Wang, J.-H. Xue, and Q. Liao, “Deep learning
    for single image super-resolution: A brief review,” *IEEE Transactions on Multimedia*,
    vol. 21, 2019.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] W. Yang, X. Zhang, Y. Tian, W. Wang, J.-H. Xue, 和 Q. Liao，“深度学习在单图像超分辨率中的应用：简要综述，”*IEEE
    Transactions on Multimedia*，第21卷，2019年。'
- en: '[12] Z. Wang, J. Chen, and S. C. Hoi, “Deep learning for image super-resolution:
    A survey,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 43,
    2020.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Z. Wang, J. Chen, 和 S. C. Hoi，“深度学习在图像超分辨率中的应用：综述，”*IEEE Transactions
    on Pattern Analysis and Machine Intelligence*，第43卷，2020年。'
- en: '[13] C. Dong, C. C. Loy, and X. Tang, “Accelerating the super-resolution convolutional
    neural network,” in *ECCV*, 2016.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] C. Dong, C. C. Loy, 和 X. Tang，“加速超分辨率卷积神经网络，”发表于*ECCV*，2016年。'
- en: '[14] K. Ma, Z. Duanmu, Q. Wu, Z. Wang, H. Yong, H. Li, and L. Zhang, “Waterloo
    exploration database: New challenges for image quality assessment models,” *IEEE
    Transactions on Image Processing*, vol. 26, 2016.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] K. Ma, Z. Duanmu, Q. Wu, Z. Wang, H. Yong, H. Li, 和 L. Zhang，“Waterloo探索数据库：图像质量评估模型的新挑战，”*IEEE
    Transactions on Image Processing*，第26卷，2016年。'
- en: '[15] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, and L. Zhang, “Ntire
    2017 challenge on single image super-resolution: Methods and results,” in *CVPRW*,
    2017.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, 和 L. Zhang，“Ntire 2017单图像超分辨率挑战赛：方法与结果，”发表于*CVPRW*，2017年。'
- en: '[16] E. Agustsson and R. Timofte, “Ntire 2017 challenge on single image super-resolution:
    Dataset and study,” in *CVPRW*, 2017.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] E. Agustsson 和 R. Timofte，“Ntire 2017单图像超分辨率挑战赛：数据集与研究，”发表于*CVPRW*，2017年。'
- en: '[17] D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human segmented
    natural images and its application to evaluating segmentation algorithms and measuring
    ecological statistics,” in *ICCV*, 2001.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] D. Martin, C. Fowlkes, D. Tal, 和 J. Malik，“人类分割自然图像数据库及其在评估分割算法和测量生态统计中的应用，”发表于*ICCV*，2001年。'
- en: '[18] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection and
    hierarchical image segmentation,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 33, 2011.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] P. Arbelaez, M. Maire, C. Fowlkes, 和 J. Malik，“轮廓检测与分层图像分割，”*IEEE Transactions
    on Pattern Analysis and Machine Intelligence*，第33卷，2011年。'
- en: '[19] J. Cai, H. Zeng, H. Yong, Z. Cao, and L. Zhang, “Toward real-world single
    image super-resolution: A new benchmark and a new model,” in *ICCV*, 2019.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Cai, H. Zeng, H. Yong, Z. Cao, 和 L. Zhang，“迈向现实世界的单图像超分辨率：新的基准和模型，”发表于*ICCV*，2019年。'
- en: '[20] X. Wang, K. Yu, C. Dong, and C. C. Loy, “Recovering realistic texture
    in image super-resolution by deep spatial feature transform,” in *CVPR*, 2018.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] X. Wang, K. Yu, C. Dong, 和 C. C. Loy，“通过深度空间特征变换恢复图像超分辨率中的逼真纹理，”发表于*CVPR*，2018年。'
- en: '[21] C. Chen, Z. Xiong, X. Tian, Z.-J. Zha, and F. Wu, “Camera lens super-resolution,”
    in *CVPR*, 2019.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] C. Chen, Z. Xiong, X. Tian, Z.-J. Zha, 和 F. Wu，“相机镜头超分辨率，”发表于*CVPR*，2019年。'
- en: '[22] Y. Wang, L. Wang, J. Yang, W. An, and Y. Guo, “Flickr1024: A large-scale
    dataset for stereo image super-resolution,” in *ICCVW*, 2019.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Y. Wang, L. Wang, J. Yang, W. An, 和 Y. Guo，“Flickr1024：一个用于立体图像超分辨率的大规模数据集，”发表于*ICCVW*，2019年。'
- en: '[23] Z. Zhang, Z. Wang, Z. Lin, and H. Qi, “Image super-resolution by neural
    texture transfer,” in *CVPR*, 2019.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Z. Zhang, Z. Wang, Z. Lin, 和 H. Qi，“通过神经纹理转移实现图像超分辨率，”发表于*CVPR*，2019年。'
- en: '[24] G. Jinjin, C. Haoming, C. Haoyu, Y. Xiaoxing, J. S. Ren, and D. Chao,
    “Pipal: a large-scale image quality assessment dataset for perceptual image restoration,”
    in *ECCV*, 2020.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] G. Jinjin, C. Haoming, C. Haoyu, Y. Xiaoxing, J. S. Ren, 和 D. Chao，“Pipal：用于感知图像恢复的大规模图像质量评估数据集”，在*ECCV*，2020年。'
- en: '[25] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel, “Low-complexity
    single-image super-resolution based on nonnegative neighbor embedding,” in *BMVC*,
    2012.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] M. Bevilacqua, A. Roumy, C. Guillemot, 和 M. L. Alberi-Morel，“基于非负邻域嵌入的低复杂度单幅图像超分辨率”，在*BMVC*，2012年。'
- en: '[26] R. Zeyde, M. Elad, and M. Protter, “On single image scale-up using sparse-representations,”
    in *ICCS*, 2010.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] R. Zeyde, M. Elad, 和 M. Protter，“基于稀疏表示的单幅图像放大”，在*ICCS*，2010年。'
- en: '[27] J.-B. Huang, A. Singh, and N. Ahuja, “Single image super-resolution from
    transformed self-exemplars,” in *CVPR*, 2015.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] J.-B. Huang, A. Singh, 和 N. Ahuja，“从变换的自我示例中实现单幅图像超分辨率”，在*CVPR*，2015年。'
- en: '[28] A. Fujimoto, T. Ogawa, K. Yamamoto, Y. Matsui, T. Yamasaki, and K. Aizawa,
    “Manga109 dataset and creation of metadata,” in *MANPU*, 2016.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] A. Fujimoto, T. Ogawa, K. Yamamoto, Y. Matsui, T. Yamasaki, 和 K. Aizawa，“Manga109
    数据集及其元数据的创建”，在*MANPU*，2016年。'
- en: '[29] R. Timofte, R. Rothe, and L. Van Gool, “Seven ways to improve example-based
    single image super resolution,” in *CVPR*, 2016.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] R. Timofte, R. Rothe, 和 L. Van Gool，“改进基于示例的单幅图像超分辨率的七种方法”，在*CVPR*，2016年。'
- en: '[30] Y. Blau, R. Mechrez, R. Timofte, T. Michaeli, and L. Zelnik-Manor, “The
    2018 pirm challenge on perceptual image super-resolution,” in *ECCVW*, 2018.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Blau, R. Mechrez, R. Timofte, T. Michaeli, 和 L. Zelnik-Manor，“2018年感知图像超分辨率挑战”，在*ECCVW*，2018年。'
- en: '[31] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *CVPR*, 2009.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, 和 L. Fei-Fei，“Imagenet：一个大规模层次图像数据库”，在*CVPR*，2009年。'
- en: '[32] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in
    the wild,” in *ICCV*, 2015.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Z. Liu, P. Luo, X. Wang, 和 X. Tang，“深度学习中的面部属性识别”，在*ICCV*，2015年。'
- en: '[33] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert,
    and Z. Wang, “Real-time single image and video super-resolution using an efficient
    sub-pixel convolutional neural network,” in *CVPR*, 2016.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop, D.
    Rueckert, 和 Z. Wang，“使用高效的子像素卷积神经网络进行实时单幅图像和视频超分辨率”，在*CVPR*，2016年。'
- en: '[34] A. Shocher, N. Cohen, and M. Irani, “?zero-shot? super-resolution using
    deep internal learning,” in *CVPR*, 2018.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. Shocher, N. Cohen, 和 M. Irani，“使用深度内部学习的‘零样本’超分辨率”，在*CVPR*，2018年。'
- en: '[35] Y. Yuan, S. Liu, J. Zhang, Y. Zhang, C. Dong, and L. Lin, “Unsupervised
    image super-resolution using cycle-in-cycle generative adversarial networks,”
    in *CVPRW*, 2018.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Yuan, S. Liu, J. Zhang, Y. Zhang, C. Dong, 和 L. Lin，“使用循环内循环生成对抗网络的无监督图像超分辨率”，在*CVPRW*，2018年。'
- en: '[36] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *ICCV*, 2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J.-Y. Zhu, T. Park, P. Isola, 和 A. A. Efros，“使用循环一致对抗网络的无配对图像到图像转换”，在*ICCV*，2017年。'
- en: '[37] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] K. Simonyan 和 A. Zisserman，“用于大规模图像识别的非常深度卷积网络”，*arXiv preprint arXiv:1409.1556*，2014年。'
- en: '[38] C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham, A. Acosta,
    A. P. Aitken, A. Tejani, J. Totz, Z. Wang *et al.*, “Photo-realistic single image
    super-resolution using a generative adversarial network,” in *CVPR*, 2017.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham, A. Acosta,
    A. P. Aitken, A. Tejani, J. Totz, Z. Wang *等*，“使用生成对抗网络的逼真单幅图像超分辨率”，在*CVPR*，2017年。'
- en: '[39] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” *NeurIPS*, 2014.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络”，*NeurIPS*，2014年。'
- en: '[40] Y. Blau and T. Michaeli, “The perception-distortion tradeoff,” in *CVPR*,
    2018.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Y. Blau 和 T. Michaeli，“感知与失真之间的权衡”，在*CVPR*，2018年。'
- en: '[41] Z. Wang and A. C. Bovik, “Mean squared error: Love it or leave it? a new
    look at signal fidelity measures,” *IEEE Signal Processing Magazine*, vol. 26,
    2009.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Z. Wang 和 A. C. Bovik，“均方误差：喜欢它还是放弃它？对信号保真度测量的新看法”，*IEEE Signal Processing
    Magazine*，第26卷，2009年。'
- en: '[42] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: from error visibility to structural similarity,” *IEEE Transactions
    on Image Processing*, vol. 13, 2004.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Z. Wang, A. C. Bovik, H. R. Sheikh, 和 E. P. Simoncelli，“图像质量评估：从误差可视性到结构相似性”，*IEEE
    Transactions on Image Processing*，第13卷，2004年。'
- en: '[43] A. Mittal, A. K. Moorthy, and A. C. Bovik, “No-reference image quality
    assessment in the spatial domain,” *IEEE Transactions on Image Processing*, vol. 21,
    2012.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] A. Mittal, A. K. Moorthy, 和 A. C. Bovik，“空间域的无参考图像质量评估，” *IEEE Transactions
    on Image Processing*，第21卷，2012年。'
- en: '[44] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a ?completely blind?
    image quality analyzer,” *IEEE Signal Processing Letters*, vol. 20, 2012.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] A. Mittal, R. Soundararajan, 和 A. C. Bovik，“打造一个*完全盲目的*图像质量分析器，” *IEEE
    Signal Processing Letters*，第20卷，2012年。'
- en: '[45] C. Ma, C.-Y. Yang, X. Yang, and M.-H. Yang, “Learning a no-reference quality
    metric for single-image super-resolution,” *Computer Vision and Image Understanding*,
    vol. 158, 2017.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C. Ma, C.-Y. Yang, X. Yang, 和 M.-H. Yang，“学习一种无参考质量度量用于单幅图像超分辨率，” *Computer
    Vision and Image Understanding*，第158卷，2017年。'
- en: '[46] W. Zhang, Y. Liu, C. Dong, and Y. Qiao, “Ranksrgan: Generative adversarial
    networks with ranker for image super-resolution,” in *ICCV*, 2019.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] W. Zhang, Y. Liu, C. Dong, 和 Y. Qiao，“Ranksrgan: 使用排名器的生成对抗网络用于图像超分辨率，”
    发表在 *ICCV*，2019年。'
- en: '[47] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *CVPR*, 2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, 和 O. Wang，“深度特征作为感知度量的*不合理有效性*，”
    发表在 *CVPR*，2018年。'
- en: '[48] K. He and J. Sun, “Convolutional neural networks at constrained time cost,”
    in *CVPR*, 2015.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] K. He 和 J. Sun，“约束时间成本下的卷积神经网络，” 发表在 *CVPR*，2015年。'
- en: '[49] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习，” 发表在 *CVPR*，2016年。'
- en: '[50] J. Kim, J. Kwon Lee, and K. Mu Lee, “Accurate image super-resolution using
    very deep convolutional networks,” in *CVPR*, 2016.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Kim, J. Kwon Lee, 和 K. Mu Lee，“使用非常深的卷积网络进行准确的图像超分辨率，” 发表在 *CVPR*，2016年。'
- en: '[51] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, “Enhanced deep residual
    networks for single image super-resolution,” in *CVPRW*, 2017.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] B. Lim, S. Son, H. Kim, S. Nah, 和 K. M. Lee，“增强型深度残差网络用于单幅图像超分辨率，” 发表在
    *CVPRW*，2017年。'
- en: '[52] J. Li, F. Fang, K. Mei, and G. Zhang, “Multi-scale residual network for
    image super-resolution,” in *ECCV*, 2018.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Li, F. Fang, K. Mei, 和 G. Zhang，“多尺度残差网络用于图像超分辨率，” 发表在 *ECCV*，2018年。'
- en: '[53] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, “Image super-resolution
    using very deep residual channel attention networks,” in *ECCV*, 2018.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, 和 Y. Fu，“使用非常深的残差通道注意网络实现图像超分辨率，”
    发表在 *ECCV*，2018年。'
- en: '[54] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4, inception-resnet
    and the impact of residual connections on learning,” in *AAAI*, 2017.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] C. Szegedy, S. Ioffe, V. Vanhoucke, 和 A. A. Alemi，“Inception-v4、Inception-ResNet及残差连接对学习的影响，”
    发表在 *AAAI*，2017年。'
- en: '[55] J. Kim, J. Kwon Lee, and K. Mu Lee, “Deeply-recursive convolutional network
    for image super-resolution,” in *CVPR*, 2016.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] J. Kim, J. Kwon Lee, 和 K. Mu Lee，“用于图像超分辨率的深度递归卷积网络，” 发表在 *CVPR*，2016年。'
- en: '[56] Y. Tai, J. Yang, and X. Liu, “Image super-resolution via deep recursive
    residual network,” in *CVPR*, 2017.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Tai, J. Yang, 和 X. Liu，“通过深度递归残差网络实现图像超分辨率，” 发表在 *CVPR*，2017年。'
- en: '[57] Y. Tai, J. Yang, X. Liu, and C. Xu, “Memnet: A persistent memory network
    for image restoration,” in *CVPR*, 2017.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Y. Tai, J. Yang, X. Liu, 和 C. Xu，“Memnet: 用于图像修复的持久记忆网络，” 发表在 *CVPR*，2017年。'
- en: '[58] N. Ahn, B. Kang, and K.-A. Sohn, “Fast, accurate, and lightweight super-resolution
    with cascading residual network,” in *ECCV*, 2018.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] N. Ahn, B. Kang, 和 K.-A. Sohn，“通过级联残差网络实现快速、准确且轻量级的超分辨率，” 发表在 *ECCV*，2018年。'
- en: '[59] J. Li, Y. Yuan, K. Mei, and F. Fang, “Lightweight and accurate recursive
    fractal network for image super-resolution,” in *ICCVW*, 2019.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] J. Li, Y. Yuan, K. Mei, 和 F. Fang，“用于图像超分辨率的轻量级且准确的递归分形网络，” 发表在 *ICCVW*，2019年。'
- en: '[60] T. Tong, G. Li, X. Liu, and Q. Gao, “Image super-resolution using dense
    skip connections,” in *ICCV*, 2017.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] T. Tong, G. Li, X. Liu, 和 Q. Gao，“通过密集跳跃连接实现图像超分辨率，” 发表在 *ICCV*，2017年。'
- en: '[61] Z. Hui, X. Wang, and X. Gao, “Fast and accurate single image super-resolution
    via information distillation network,” in *CVPR*, 2018.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Z. Hui, X. Wang, 和 X. Gao，“通过信息蒸馏网络实现快速且准确的单幅图像超分辨率，” 发表在 *CVPR*，2018年。'
- en: '[62] N. Ahn, B. Kang, and K.-A. Sohn, “Image super-resolution via progressive
    cascading residual network,” in *CVPRW*, 2018.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] N. Ahn, B. Kang, 和 K.-A. Sohn，“通过渐进级联残差网络实现图像超分辨率，” 发表在 *CVPRW*，2018年。'
- en: '[63] J. Li, F. Fang, J. Li, K. Mei, and G. Zhang, “Mdcn: Multi-scale dense
    cross network for image super-resolution,” *IEEE Transactions on Circuits and
    Systems for Video Technology*, vol. 31, 2020.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] J. Li, F. Fang, J. Li, K. Mei, 和 G. Zhang，“Mdcn: 多尺度密集交叉网络用于图像超分辨率，” *IEEE
    Transactions on Circuits and Systems for Video Technology*，第31卷，2020年。'
- en: '[64] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang, “Deep laplacian pyramid
    networks for fast and accurate super-resolution,” in *CVPR*, 2017.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] W.-S. Lai, J.-B. Huang, N. Ahuja 和 M.-H. Yang, “用于快速准确超分辨率的深度拉普拉斯金字塔网络，”
    见 *CVPR*，2017 年。'
- en: '[65] Y. Wang, F. Perazzi, B. McWilliams, A. Sorkine-Hornung, O. Sorkine-Hornung,
    and C. Schroers, “A fully progressive approach to single-image super-resolution,”
    in *CVPRW*, 2018.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Y. Wang, F. Perazzi, B. McWilliams, A. Sorkine-Hornung, O. Sorkine-Hornung
    和 C. Schroers, “一种完全渐进的单幅图像超分辨率方法，” 见 *CVPRW*，2018 年。'
- en: '[66] Z. Li, J. Yang, Z. Liu, X. Yang, G. Jeon, and W. Wu, “Feedback network
    for image super-resolution,” in *CVPR*, 2019.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Z. Li, J. Yang, Z. Liu, X. Yang, G. Jeon 和 W. Wu, “用于图像超分辨率的反馈网络，” 见 *CVPR*，2019
    年。'
- en: '[67] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
    the inception architecture for computer vision,” in *CVPR*, 2016.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens 和 Z. Wojna, “重新思考计算机视觉的
    inception 架构，” 见 *CVPR*，2016 年。'
- en: '[68] F. Chollet, “Xception: Deep learning with depthwise separable convolutions,”
    in *CVPR*, 2017.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] F. Chollet, “Xception: 使用深度可分离卷积的深度学习，” 见 *CVPR*，2017 年。'
- en: '[69] J. Qin, Y. Huang, and W. Wen, “Multi-scale feature fusion residual network
    for single image super-resolution,” *Neurocomputing*, vol. 379, 2020.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] J. Qin, Y. Huang 和 W. Wen, “用于单幅图像超分辨率的多尺度特征融合残差网络，” *Neurocomputing*，第
    379 卷，2020 年。'
- en: '[70] C.-Y. Chang and S.-Y. Chien, “Multi-scale dense network for single-image
    super-resolution,” in *ICASSP*, 2019.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] C.-Y. Chang 和 S.-Y. Chien, “用于单幅图像超分辨率的多尺度密集网络，” 见 *ICASSP*，2019 年。'
- en: '[71] F. Cao and H. Liu, “Single image super-resolution via multi-scale residual
    channel attention network,” *Neurocomputing*, vol. 358, 2019.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] F. Cao 和 H. Liu, “通过多尺度残差通道注意力网络进行单幅图像超分辨率，” *Neurocomputing*，第 358 卷，2019
    年。'
- en: '[72] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *CVPR*, 2017.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] G. Huang, Z. Liu, L. Van Der Maaten 和 K. Q. Weinberger, “密集连接卷积网络，” 见
    *CVPR*，2017 年。'
- en: '[73] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual dense network
    for image super-resolution,” in *CVPR*, 2018.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. Zhang, Y. Tian, Y. Kong, B. Zhong 和 Y. Fu, “用于图像超分辨率的残差密集网络，” 见 *CVPR*，2018
    年。'
- en: '[74] K. Mei, A. Jiang, J. Li, B. Liu, J. Ye, and M. Wang, “Deep residual refining
    based pseudo-multi-frame network for effective single image super-resolution,”
    *IET Image Processing*, vol. 13, 2019.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] K. Mei, A. Jiang, J. Li, B. Liu, J. Ye 和 M. Wang, “基于深度残差精炼的伪多帧网络用于有效的单幅图像超分辨率，”
    *IET 图像处理*，第 13 卷，2019 年。'
- en: '[75] M. Shen, P. Yu, R. Wang, J. Yang, L. Xue, and M. Hu, “Multipath feedforward
    network for single image super-resolution,” *Multimedia Tools and Applications*,
    vol. 78, 2019.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. Shen, P. Yu, R. Wang, J. Yang, L. Xue 和 M. Hu, “用于单幅图像超分辨率的多路径前馈网络，”
    *多媒体工具与应用*，第 78 卷，2019 年。'
- en: '[76] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *CVPR*,
    2018.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] J. Hu, L. Shen 和 G. Sun, “压缩与激励网络，” 见 *CVPR*，2018 年。'
- en: '[77] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”
    in *CVPR*, 2018.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] X. Wang, R. Girshick, A. Gupta 和 K. He, “非局部神经网络，” 见 *CVPR*，2018 年。'
- en: '[78] K. Mei, A. Jiang, J. Li, J. Ye, and M. Wang, “An effective single-image
    super-resolution model using squeeze-and-excitation networks,” in *NeurIPS*, 2018.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] K. Mei, A. Jiang, J. Li, J. Ye 和 M. Wang, “使用压缩与激励网络的有效单幅图像超分辨率模型，” 见
    *NeurIPS*，2018 年。'
- en: '[79] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, and L. Zhang, “Second-order attention
    network for single image super-resolution,” in *CVPR*, 2019.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] T. Dai, J. Cai, Y. Zhang, S.-T. Xia 和 L. Zhang, “用于单幅图像超分辨率的二阶注意力网络，”
    见 *CVPR*，2019 年。'
- en: '[80] D. Liu, B. Wen, Y. Fan, C. C. Loy, and T. S. Huang, “Non-local recurrent
    network for image restoration,” *arXiv preprint arXiv:1806.02919*, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] D. Liu, B. Wen, Y. Fan, C. C. Loy 和 T. S. Huang, “用于图像恢复的非局部递归网络，” *arXiv
    预印本 arXiv:1806.02919*，2018 年。'
- en: '[81] Y. Zhang, K. Li, K. Li, B. Zhong, and Y. Fu, “Residual non-local attention
    networks for image restoration,” *arXiv preprint arXiv:1903.10082*, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Y. Zhang, K. Li, K. Li, B. Zhong 和 Y. Fu, “用于图像恢复的残差非局部注意力网络，” *arXiv
    预印本 arXiv:1903.10082*，2019 年。'
- en: '[82] B. Niu, W. Wen, W. Ren, X. Zhang, L. Yang, S. Wang, K. Zhang, X. Cao,
    and H. Shen, “Single image super-resolution via a holistic attention network,”
    in *ECCV*, 2020.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] B. Niu, W. Wen, W. Ren, X. Zhang, L. Yang, S. Wang, K. Zhang, X. Cao 和
    H. Shen, “通过整体注意力网络进行单幅图像超分辨率，” 见 *ECCV*，2020 年。'
- en: '[83] Y. Mei, Y. Fan, Y. Zhou, L. Huang, T. S. Huang, and H. Shi, “Image super-resolution
    with cross-scale non-local attention and exhaustive self-exemplars mining,” in
    *CVPR*, 2020.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Mei, Y. Fan, Y. Zhou, L. Huang, T. S. Huang 和 H. Shi, “具有交叉尺度非局部注意力和全面自示例挖掘的图像超分辨率，”
    见 *CVPR*，2020 年。'
- en: '[84] J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik, “Human pose estimation
    with iterative error feedback,” in *CVPR*, 2016.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] J. Carreira, P. Agrawal, K. Fragkiadaki 和 J. Malik, “通过迭代误差反馈进行人体姿态估计，”
    见 *CVPR*，2016 年。'
- en: '[85] C. Cao, X. Liu, Y. Yang, Y. Yu, J. Wang, Z. Wang, Y. Huang, L. Wang, C. Huang,
    W. Xu *et al.*, “Look and think twice: Capturing top-down visual attention with
    feedback convolutional neural networks,” in *ICCV*, 2015.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] C. Cao, X. Liu, Y. Yang, Y. Yu, J. Wang, Z. Wang, Y. Huang, L. Wang, C.
    Huang, W. Xu *等*，“再三思考：使用反馈卷积神经网络捕捉自上而下的视觉注意力，”发表于 *ICCV*，2015。'
- en: '[86] M. Haris, G. Shakhnarovich, and N. Ukita, “Deep back-projection networks
    for single image super-resolution,” *arXiv preprint arXiv:1904.05677*, 2019.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] M. Haris, G. Shakhnarovich, 和 N. Ukita, “用于单幅图像超分辨率的深度反向投影网络，” *arXiv
    preprint arXiv:1904.05677*，2019。'
- en: '[87] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. S. Huang, “Image
    super-resolution via dual-state recurrent networks,” in *CVPR*, 2018.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, 和 T. S. Huang, “通过双状态递归网络进行图像超分辨率，”发表于
    *CVPR*，2018。'
- en: '[88] W. Yang, J. Feng, J. Yang, F. Zhao, J. Liu, Z. Guo, and S. Yan, “Deep
    edge guided recurrent residual learning for image super-resolution,” *IEEE Transactions
    on Image Processing*, vol. 26, 2017.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] W. Yang, J. Feng, J. Yang, F. Zhao, J. Liu, Z. Guo, 和 S. Yan, “深度边缘引导的递归残差学习用于图像超分辨率，”
    *IEEE Transactions on Image Processing*，第26卷，2017。'
- en: '[89] F. Fang, J. Li, and T. Zeng, “Soft-edge assisted network for single image
    super-resolution,” *IEEE Transactions on Image Processing*, vol. 29, 2020.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] F. Fang, J. Li, 和 T. Zeng, “用于单幅图像超分辨率的软边辅助网络，” *IEEE Transactions on
    Image Processing*，第29卷，2020。'
- en: '[90] Y. Blau and T. Michaeli, “The perception-distortion tradeoff,” in *CVPR*,
    2018.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Y. Blau 和 T. Michaeli, “感知-失真权衡，”发表于 *CVPR*，2018。'
- en: '[91] M. S. Sajjadi, B. Scholkopf, and M. Hirsch, “Enhancenet: Single image
    super-resolution through automated texture synthesis,” in *ICCV*, 2017.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. S. Sajjadi, B. Scholkopf, 和 M. Hirsch, “Enhancenet：通过自动纹理合成实现单幅图像超分辨率，”发表于
    *ICCV*，2017。'
- en: '[92] L. Gatys, A. S. Ecker, and M. Bethge, “Texture synthesis using convolutional
    neural networks,” 2015.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] L. Gatys, A. S. Ecker, 和 M. Bethge, “使用卷积神经网络进行纹理合成，”2015。'
- en: '[93] L. A. Gatys, A. S. Ecker, and M. Bethge, “A neural algorithm of artistic
    style,” *arXiv preprint arXiv:1508.06576*, 2015.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] L. A. Gatys, A. S. Ecker, 和 M. Bethge, “艺术风格的神经算法，” *arXiv preprint arXiv:1508.06576*，2015。'
- en: '[94] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time
    style transfer and super-resolution,” in *ECCV*, 2016.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] J. Johnson, A. Alahi, 和 L. Fei-Fei, “用于实时风格转移和超分辨率的感知损失，”发表于 *ECCV*，2016。'
- en: '[95] M. S. Rad, B. Bozorgtabar, U.-V. Marti, M. Basler, H. K. Ekenel, and J.-P.
    Thiran, “Srobb: Targeted perceptual loss for single image super-resolution,” in
    *ICCV*, 2019.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] M. S. Rad, B. Bozorgtabar, U.-V. Marti, M. Basler, H. K. Ekenel, 和 J.-P.
    Thiran, “Srobb：针对单幅图像超分辨率的目标感知损失，”发表于 *ICCV*，2019。'
- en: '[96] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
    with deep convolutional generative adversarial networks,” *arXiv preprint arXiv:1511.06434*,
    2015.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] A. Radford, L. Metz, 和 S. Chintala, “通过深度卷积生成对抗网络进行无监督表征学习，” *arXiv preprint
    arXiv:1511.06434*，2015。'
- en: '[97] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, and C. C. Loy,
    “Esrgan: Enhanced super-resolution generative adversarial networks,” in *ECCV*,
    2018.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, 和 C. C. Loy, “Esrgan：增强型超分辨率生成对抗网络，”发表于
    *ECCV*，2018。'
- en: '[98] S.-J. Park, H. Son, S. Cho, K.-S. Hong, and S. Lee, “Srfeat: Single image
    super-resolution with feature discrimination,” in *ECCV*, 2018.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] S.-J. Park, H. Son, S. Cho, K.-S. Hong, 和 S. Lee, “Srfeat：具有特征区分的单幅图像超分辨率，”发表于
    *ECCV*，2018。'
- en: '[99] A. Jolicoeur-Martineau, “The relativistic discriminator: a key element
    missing from standard gan,” *arXiv preprint arXiv:1807.00734*, 2018.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Jolicoeur-Martineau, “相对主义判别器：标准 gan 中缺失的关键元素，” *arXiv preprint arXiv:1807.00734*，2018。'
- en: '[100] C. Ma, Y. Rao, Y. Cheng, C. Chen, J. Lu, and J. Zhou, “Structure-preserving
    super resolution with gradient guidance,” in *CVPR*, 2020.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] C. Ma, Y. Rao, Y. Cheng, C. Chen, J. Lu, 和 J. Zhou, “具有梯度引导的结构保留超分辨率，”发表于
    *CVPR*，2020。'
- en: '[101] A. Bulat, J. Yang, and G. Tzimiropoulos, “To learn image super-resolution,
    use a gan to learn how to do image degradation first,” in *ECCV*, 2018.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] A. Bulat, J. Yang, 和 G. Tzimiropoulos, “要学习图像超分辨率，首先使用 gan 学习如何进行图像降解，”发表于
    *ECCV*，2018。'
- en: '[102] Y. Guo, J. Chen, J. Wang, Q. Chen, J. Cao, Z. Deng, Y. Xu, and M. Tan,
    “Closed-loop matters: Dual regression networks for single image super-resolution,”
    in *CVPR*, 2020.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Y. Guo, J. Chen, J. Wang, Q. Chen, J. Cao, Z. Deng, Y. Xu, 和 M. Tan,
    “闭环重要性：用于单幅图像超分辨率的双重回归网络，”发表于 *CVPR*，2020。'
- en: '[103] M. Zontak and M. Irani, “Internal statistics of a single natural image,”
    in *CVPR*, 2011.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] M. Zontak 和 M. Irani, “单幅自然图像的内部统计特性，”发表于 *CVPR*，2011。'
- en: '[104] T. R. Shaham, T. Dekel, and T. Michaeli, “Singan: Learning a generative
    model from a single natural image,” in *ICCV*, 2019.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] T. R. Shaham, T. Dekel, 和 T. Michaeli, “Singan：从单幅自然图像学习生成模型，”发表于 *ICCV*，2019。'
- en: '[105] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang, “Deep laplacian pyramid
    networks for fast and accurate super-resolution,” in *CVPR*, 2017.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] W.-S. Lai, J.-B. Huang, N. Ahuja, 和 M.-H. Yang，“用于快速准确超分辨率的深度拉普拉斯金字塔网络”，见
    *CVPR*，2017年。'
- en: '[106] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *arXiv preprint arXiv:1503.02531*, 2015.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] G. Hinton, O. Vinyals, 和 J. Dean，“提炼神经网络中的知识”，*arXiv 预印本 arXiv:1503.02531*，2015年。'
- en: '[107] S. Ahn, S. X. Hu, A. Damianou, N. D. Lawrence, and Z. Dai, “Variational
    information distillation for knowledge transfer,” in *CVPR*, 2019.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] S. Ahn, S. X. Hu, A. Damianou, N. D. Lawrence, 和 Z. Dai，“用于知识转移的变分信息蒸馏”，见
    *CVPR*，2019年。'
- en: '[108] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio,
    “Fitnets: Hints for thin deep nets,” *arXiv preprint arXiv:1412.6550*, 2014.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, 和 Y. Bengio，“Fitnets:
    细深网络的提示”，*arXiv 预印本 arXiv:1412.6550*，2014年。'
- en: '[109] Q. Gao, Y. Zhao, G. Li, and T. Tong, “Image super-resolution using knowledge
    distillation,” in *ACCV*, 2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Q. Gao, Y. Zhao, G. Li, 和 T. Tong，“使用知识蒸馏的图像超分辨率”，见 *ACCV*，2018年。'
- en: '[110] W. Lee, J. Lee, D. Kim, and B. Ham, “Learning with privileged information
    for efficient image super-resolution,” in *ECCV*, 2020.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] W. Lee, J. Lee, D. Kim, 和 B. Ham，“利用特权信息进行高效图像超分辨率学习”，见 *ECCV*，2020年。'
- en: '[111] H. Yue, X. Sun, J. Yang, and F. Wu, “Landmark image super-resolution
    by retrieving web images,” *IEEE Transactions on Image Processing*, vol. 22, 2013.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] H. Yue, X. Sun, J. Yang, 和 F. Wu，“通过检索网络图像进行标志性图像超分辨率”，*IEEE 图像处理学报*，第22卷，2013年。'
- en: '[112] H. Zheng, M. Ji, H. Wang, Y. Liu, and L. Fang, “Crossnet: An end-to-end
    reference-based super resolution network using cross-scale warping,” in *ECCV*,
    2018.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] H. Zheng, M. Ji, H. Wang, Y. Liu, 和 L. Fang，“Crossnet: 一种基于参考的端到端超分辨率网络，使用跨尺度变形”，见
    *ECCV*，2018年。'
- en: '[113] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, “Learning texture transformer
    network for image super-resolution,” in *CVPR*, 2020.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] F. Yang, H. Yang, J. Fu, H. Lu, 和 B. Guo，“学习纹理 transformer 网络用于图像超分辨率”，见
    *CVPR*，2020年。'
- en: '[114] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NeurIPS*, 2017.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“注意力即一切”，见 *NeurIPS*，2017年。'
- en: '[115] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” *arXiv preprint
    arXiv:1810.04805*, 2018.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova，“Bert: 用于语言理解的深度双向 transformers
    预训练”，*arXiv 预印本 arXiv:1810.04805*，2018年。'
- en: '[116] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *et al.*,
    “Language models are unsupervised multitask learners,” *OpenAI Blog*, vol. 1,
    2019.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *等*，“语言模型是无监督的多任务学习者”，*OpenAI
    博客*，第1卷，2019年。'
- en: '[117] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu,
    and W. Gao, “Pre-trained image processing transformer,” in *CVPR*, 2021.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu,
    和 W. Gao，“预训练的图像处理 transformer”，见 *CVPR*，2021年。'
- en: '[118] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, “Swinir:
    Image restoration using swin transformer,” in *ICCVW*, 2021.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, 和 R. Timofte，“Swinir:
    使用 swin transformer 的图像恢复”，见 *ICCVW*，2021年。'
- en: '[119] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
    “Swin transformer: Hierarchical vision transformer using shifted windows,” *arXiv
    preprint arXiv:2103.14030*, 2021.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, 和 B. Guo，“Swin
    transformer: 使用移位窗口的层次化视觉 transformer”，*arXiv 预印本 arXiv:2103.14030*，2021年。'
- en: '[120] Z. Lu, H. Liu, J. Li, and L. Zhang, “Efficient transformer for single
    image super-resolution,” *arXiv preprint arXiv:2108.11084*, 2021.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Z. Lu, H. Liu, J. Li, 和 L. Zhang，“用于单幅图像超分辨率的高效 transformer”，*arXiv 预印本
    arXiv:2108.11084*，2021年。'
- en: '[121] Y. Shi, H. Zhong, Z. Yang, X. Yang, and L. Lin, “Ddet: Dual-path dynamic
    enhancement network for real-world image super-resolution,” *IEEE Signal Processing
    Letters*, vol. 27, 2020.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Y. Shi, H. Zhong, Z. Yang, X. Yang, 和 L. Lin，“Ddet: 用于现实世界图像超分辨率的双路径动态增强网络”，*IEEE
    信号处理通讯*，第27卷，2020年。'
- en: '[122] P. Wei, Z. Xie, H. Lu, Z. Zhan, Q. Ye, W. Zuo, and L. Lin, “Component
    divide-and-conquer for real-world image super-resolution,” in *ECCV*, 2020.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] P. Wei, Z. Xie, H. Lu, Z. Zhan, Q. Ye, W. Zuo, 和 L. Lin，“用于现实世界图像超分辨率的组件分治”，见
    *ECCV*，2020年。'
- en: '[123] W. Sun, D. Gong, Q. Shi, A. van den Hengel, and Y. Zhang, “Learning to
    zoom-in via learning to zoom-out: Real-world super-resolution by generating and
    adapting degradation,” *IEEE Transactions on Image Processing*, vol. 30, 2021.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] W. Sun, D. Gong, Q. Shi, A. van den Hengel, 和 Y. Zhang，“通过学习缩小来学习放大：通过生成和适应退化实现现实世界的超分辨率”，*IEEE
    图像处理学报*，第30卷，2021年。'
- en: '[124] K. Prajapati, V. Chudasama, H. Patel, K. Upla, R. Ramachandra, K. Raja,
    and C. Busch, “Unsupervised single image super-resolution network (usisresnet)
    for real-world data using generative adversarial network,” in *CVPRW*, 2020.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] K. Prajapati, V. Chudasama, H. Patel, K. Upla, R. Ramachandra, K. Raja,
    和 C. Busch，“无监督单图像超分辨率网络（usisresnet）用于真实数据，使用生成对抗网络，” 发表在*CVPRW*，2020。'
- en: '[125] G. Kim, J. Park, K. Lee, J. Lee, J. Min, B. Lee, D. K. Han, and H. Ko,
    “Unsupervised real-world super resolution with cycle generative adversarial network
    and domain discriminator,” in *CVPRW*, 2020.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] G. Kim, J. Park, K. Lee, J. Lee, J. Min, B. Lee, D. K. Han, 和 H. Ko，“使用循环生成对抗网络和领域判别器进行无监督实际超分辨率，”
    发表在*CVPRW*，2020。'
- en: '[126] J. Kim, C. Jung, and C. Kim, “Dual back-projection-based internal learning
    for blind super-resolution,” *IEEE Signal Processing Letters*, vol. 27, 2020.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Kim, C. Jung, 和 C. Kim，“基于双重反向投影的内部学习用于盲超分辨率，” *IEEE信号处理通讯*，第27卷，2020。'
- en: '[127] M. Emad, M. Peemen, and H. Corporaal, “Dualsr: Zero-shot dual learning
    for real-world super-resolution,” in *WACV*, 2021.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] M. Emad, M. Peemen, 和 H. Corporaal，“Dualsr: 零样本双重学习用于实际超分辨率，” 发表在*WACV*，2021。'
- en: '[128] J. W. Soh, S. Cho, and N. I. Cho, “Meta-transfer learning for zero-shot
    super-resolution,” in *CVPR*, 2020.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] J. W. Soh, S. Cho, 和 N. I. Cho，“用于零样本超分辨率的元迁移学习，” 发表在*CVPR*，2020。'
- en: '[129] S. Park, J. Yoo, D. Cho, J. Kim, and T. H. Kim, “Fast adaptation to super-resolution
    networks via meta-learning,” in *ECCV*, 2020.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] S. Park, J. Yoo, D. Cho, J. Kim, 和 T. H. Kim，“通过元学习快速适应超分辨率网络，” 发表在*ECCV*，2020。'
- en: '[130] X. Hu, H. Mu, X. Zhang, Z. Wang, T. Tan, and J. Sun, “Meta-sr: A magnification-arbitrary
    network for super-resolution,” in *CVPR*, 2019.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] X. Hu, H. Mu, X. Zhang, Z. Wang, T. Tan, 和 J. Sun，“Meta-sr: 一个放大任意的超分辨率网络，”
    发表在*CVPR*，2019。'
- en: '[131] L. Wang, Y. Wang, Z. Lin, J. Yang, W. An, and Y. Guo, “Learning a single
    network for scale-arbitrary super-resolution,” 2021.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] L. Wang, Y. Wang, Z. Lin, J. Yang, W. An, 和 Y. Guo，“学习一个单一网络用于尺度任意的超分辨率，”
    2021。'
- en: '[132] K. Zhang, W. Zuo, and L. Zhang, “Learning a single convolutional super-resolution
    network for multiple degradations,” in *CVPR*, 2018.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] K. Zhang, W. Zuo, 和 L. Zhang，“学习一个单一的卷积超分辨率网络以应对多种退化，” 发表在*CVPR*，2018。'
- en: '[133] ——, “Deep plug-and-play super-resolution for arbitrary blur kernels,”
    in *CVPR*, 2019.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] ——，“深度即插即用超分辨率用于任意模糊核，” 发表在*CVPR*，2019。'
- en: '[134] Y.-S. Xu, S.-Y. R. Tseng, Y. Tseng, H.-K. Kuo, and Y.-M. Tsai, “Unified
    dynamic convolutional network for super-resolution with variational degradations,”
    in *CVPR*, 2020.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Y.-S. Xu, S.-Y. R. Tseng, Y. Tseng, H.-K. Kuo, 和 Y.-M. Tsai，“统一动态卷积网络用于具有变分退化的超分辨率，”
    发表在*CVPR*，2020。'
- en: '[135] Z. Hui, J. Li, X. Wang, and X. Gao, “Learning the non-differentiable
    optimization for blind super-resolution,” in *CVPR*, 2021.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Z. Hui, J. Li, X. Wang, 和 X. Gao，“学习盲超分辨率的非可微优化，” 发表在*CVPR*，2021。'
- en: '[136] K. Zhang, L. V. Gool, and R. Timofte, “Deep unfolding network for image
    super-resolution,” in *CVPR*, 2020.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] K. Zhang, L. V. Gool, 和 R. Timofte，“用于图像超分辨率的深度展开网络，” 发表在*CVPR*，2020。'
- en: '[137] J. Gu, H. Lu, W. Zuo, and C. Dong, “Blind super-resolution with iterative
    kernel correction,” in *CVPR*, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] J. Gu, H. Lu, W. Zuo, 和 C. Dong，“盲超分辨率与迭代核校正，” 发表在*CVPR*，2019。'
- en: '[138] Z. Luo, Y. Huang, S. Li, L. Wang, and T. Tan, “Unfolding the alternating
    optimization for blind super resolution,” *arXiv preprint arXiv:2010.02631*, 2020.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Z. Luo, Y. Huang, S. Li, L. Wang, 和 T. Tan，“展开交替优化用于盲超分辨率，” *arXiv预印本
    arXiv:2010.02631*，2020。'
- en: '[139] S. Y. Kim, H. Sim, and M. Kim, “Koalanet: Blind super-resolution using
    kernel-oriented adaptive local adjustment,” in *CVPR*, 2021.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] S. Y. Kim, H. Sim, 和 M. Kim，“Koalanet: 使用面向核的自适应局部调整进行盲超分辨率，” 发表在*CVPR*，2021。'
- en: '[140] M. Yamac, B. Ataman, and A. Nawaz, “Kernelnet: A blind super-resolution
    kernel estimation network,” in *CVPR*, 2021.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] M. Yamac, B. Ataman, 和 A. Nawaz，“Kernelnet: 一个盲超分辨率核估计网络，” 发表在*CVPR*，2021。'
- en: '[141] L. Wang, Y. Wang, X. Dong, Q. Xu, J. Yang, W. An, and Y. Guo, “Unsupervised
    degradation representation learning for blind super-resolution,” in *CVPR*, 2021.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] L. Wang, Y. Wang, X. Dong, Q. Xu, J. Yang, W. An, 和 Y. Guo，“无监督退化表示学习用于盲超分辨率，”
    发表在*CVPR*，2021。'
- en: '[142] X. Wang, L. Xie, C. Dong, and Y. Shan, “Real-esrgan: Training real-world
    blind super-resolution with pure synthetic data,” *arXiv preprint arXiv:2107.10833*,
    2021.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] X. Wang, L. Xie, C. Dong, 和 Y. Shan，“Real-esrgan: 使用纯合成数据训练现实世界盲超分辨率，”
    *arXiv预印本 arXiv:2107.10833*，2021。'
- en: '[143] X. Hu, Z. Zhang, C. Shan, Z. Wang, L. Wang, and T. Tan, “Meta-usr: A
    unified super-resolution network for multiple degradation parameters,” *IEEE Transactions
    on Neural Networks and Learning Systems*, vol. 32, 2020.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] J. M. Haut, R. Fernandez-Beltran, M. E. Paoletti, J. Plaza, A. Plaza,
    and F. Pla, “A new deep generative network for unsupervised remote sensing single-image
    super-resolution,” *IEEE Transactions on Geoscience and Remote sensing*, vol. 56,
    2018.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] J. Gu, X. Sun, Y. Zhang, K. Fu, and L. Wang, “Deep residual squeeze and
    excitation network for remote sensing image super-resolution,” *Remote Sensing*,
    vol. 11, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] D. Zhang, J. Shao, X. Li, and H. T. Shen, “Remote sensing image super-resolution
    via mixed high-order attention network,” *IEEE Transactions on Geoscience and
    Remote Sensing*, vol. 59, 2020.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] X. Dong, L. Wang, X. Sun, X. Jia, L. Gao, and B. Zhang, “Remote sensing
    image super-resolution using second-order multi-scale networks,” *IEEE Transactions
    on Geoscience and Remote Sensing*, vol. 59, 2020.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] L. J. Rickard, R. W. Basedow, E. F. Zalewski, P. R. Silverglate, and
    M. Landers, “Hydice: An airborne system for hyperspectral imaging,” in *Imaging
    Spectrometry of the Terrestrial Environment*, vol. 1937, 1993.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] S. Mei, X. Yuan, J. Ji, Y. Zhang, S. Wan, and Q. Du, “Hyperspectral image
    spatial super-resolution via 3d full convolutional neural network,” *Remote Sensing*,
    vol. 9, 2017.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Y. Li, L. Zhang, C. Dingl, W. Wei, and Y. Zhang, “Single hyperspectral
    image super-resolution with grouped deep recursive residual network,” in *BigMM*,
    2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Y. Fu, T. Zhang, Y. Zheng, D. Zhang, and H. Huang, “Hyperspectral image
    super-resolution with optimized rgb guidance,” in *CVPR*, 2019.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] J. Jiang, H. Sun, X. Liu, and J. Ma, “Learning spatial-spectral prior
    for super-resolution of hyperspectral imagery,” *IEEE Transactions on Computational
    Imaging*, vol. 6, 2020.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] D. Liu, J. Li, and Q. Yuan, “A spectral grouping and attention-driven
    residual dense network for hyperspectral image super-resolution,” *IEEE Transactions
    on Geoscience and Remote Sensing*, vol. 59, 2021.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Y. Wang, L. Wang, J. Yang, W. An, J. Yu, and Y. Guo, “Spatial-angular
    interaction for light field image super-resolution,” in *ECCV*, 2020.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Y. Yoon, H.-G. Jeon, D. Yoo, J.-Y. Lee, and I. S. Kweon, “Light-field
    image super-resolution using convolutional neural network,” *IEEE Signal Processing
    Letters*, vol. 24, 2017.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Y. Wang, F. Liu, K. Zhang, G. Hou, Z. Sun, and T. Tan, “Lfnet: A novel
    bidirectional recurrent convolutional neural network for light-field image super-resolution,”
    *IEEE Transactions on Image Processing*, vol. 27, 2018.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Y. Wang, J. Yang, L. Wang, X. Ying, T. Wu, W. An, and Y. Guo, “Light
    field image super-resolution using deformable convolution,” *IEEE Transactions
    on Image Processing*, vol. 30, 2020.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] E. Zhou, H. Fan, Z. Cao, Y. Jiang, and Q. Yin, “Learning face hallucination
    in the wild,” in *AAAI*, 2015.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] E. Zhou, H. Fan, Z. Cao, Y. Jiang, 和 Q. Yin, “学习野外的人脸幻觉，” 发表在 *AAAI*，2015年。'
- en: '[159] S. Zhu, S. Liu, C. C. Loy, and X. Tang, “Deep cascaded bi-network for
    face hallucination,” in *ECCV*, 2016.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] S. Zhu, S. Liu, C. C. Loy, 和 X. Tang, “深度级联双网络用于人脸幻觉，” 发表在 *ECCV*，2016年。'
- en: '[160] X. Yu and F. Porikli, “Hallucinating very low-resolution unaligned and
    noisy face images by transformative discriminative autoencoders,” in *CVPR*, 2017.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] X. Yu 和 F. Porikli, “通过变换式判别自编码器幻觉非常低分辨率的未对齐和噪声人脸图像，” 发表在 *CVPR*，2017年。'
- en: '[161] K. Zhang, Z. Zhang, C.-W. Cheng, W. H. Hsu, Y. Qiao, W. Liu, and T. Zhang,
    “Super-identity convolutional neural network for face hallucination,” in *ECCV*,
    2018.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] K. Zhang, Z. Zhang, C.-W. Cheng, W. H. Hsu, Y. Qiao, W. Liu, 和 T. Zhang,
    “用于人脸幻觉的超身份卷积神经网络，” 发表在 *ECCV*，2018年。'
- en: '[162] B. Dogan, S. Gu, and R. Timofte, “Exemplar guided face image super-resolution
    without facial landmarks,” in *CVPRW*, 2019.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] B. Dogan, S. Gu, 和 R. Timofte, “无面部标记的人脸图像超分辨率的示例指导，” 发表在 *CVPRW*，2019年。'
- en: '[163] G. Gao, D. Zhu, H. Lu, Y. Yu, H. Chang, and D. Yue, “Robust facial image
    super-resolution by kernel locality-constrained coupled-layer regression,” *ACM
    Transactions on Internet Technology*, vol. 21, 2021.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] G. Gao, D. Zhu, H. Lu, Y. Yu, H. Chang, 和 D. Yue, “通过核局部约束耦合层回归进行鲁棒的人脸图像超分辨率，”
    *ACM互联网技术学报*，第21卷，2021年。'
- en: '[164] Y. Chen, F. Shi, A. G. Christodoulou, Y. Xie, Z. Zhou, and D. Li, “Efficient
    and accurate mri super-resolution using a generative adversarial network and 3d
    multi-level densely connected network,” in *MICCAI*, 2018.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Y. Chen, F. Shi, A. G. Christodoulou, Y. Xie, Z. Zhou, 和 D. Li, “使用生成对抗网络和3D多层密集连接网络进行高效准确的MRI超分辨率，”
    发表在 *MICCAI*，2018年。'
- en: '[165] Y. Wang, Q. Teng, X. He, J. Feng, and T. Zhang, “Ct-image of rock samples
    super resolution using 3d convolutional neural network,” *Computers & Geosciences*,
    vol. 133, 2019.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Y. Wang, Q. Teng, X. He, J. Feng, 和 T. Zhang, “利用3D卷积神经网络进行岩石样本CT图像超分辨率，”
    *计算机与地球科学*，第133卷，2019年。'
- en: '[166] X. Zhao, Y. Zhang, T. Zhang, and X. Zou, “Channel splitting network for
    single mr image super-resolution,” *IEEE Transactions on Image Processing*, vol. 28,
    2019.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] X. Zhao, Y. Zhang, T. Zhang, 和 X. Zou, “单个MR图像超分辨率的通道分裂网络，” *IEEE图像处理学报*，第28卷，2019年。'
- en: '[167] C. Peng, W.-A. Lin, H. Liao, R. Chellappa, and S. K. Zhou, “Saint: spatially
    aware interpolation network for medical slice synthesis,” in *CVPR*, 2020.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] C. Peng, W.-A. Lin, H. Liao, R. Chellappa, 和 S. K. Zhou, “Saint: 空间感知插值网络用于医学切片合成，”
    发表在 *CVPR*，2020年。'
- en: '[168] D. S. Jeon, S.-H. Baek, I. Choi, and M. H. Kim, “Enhancing the spatial
    resolution of stereo images using a parallax prior,” in *CVPR*, 2018.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] D. S. Jeon, S.-H. Baek, I. Choi, 和 M. H. Kim, “利用视差先验增强立体图像的空间分辨率，” 发表在
    *CVPR*，2018年。'
- en: '[169] L. Wang, Y. Wang, Z. Liang, Z. Lin, J. Yang, W. An, and Y. Guo, “Learning
    parallax attention for stereo image super-resolution,” in *CVPR*, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] L. Wang, Y. Wang, Z. Liang, Z. Lin, J. Yang, W. An, 和 Y. Guo, “学习立体图像超分辨率的视差注意力，”
    发表在 *CVPR*，2019年。'
- en: '[170] L. Wang, Y. Guo, Y. Wang, Z. Liang, Z. Lin, J. Yang, and W. An, “Parallax
    attention for unsupervised stereo correspondence learning,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2020.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] L. Wang, Y. Guo, Y. Wang, Z. Liang, Z. Lin, J. Yang, 和 W. An, “用于无监督立体对应学习的视差注意力，”
    *IEEE模式分析与机器智能学报*，2020年。'
- en: '[171] X. Ying, Y. Wang, L. Wang, W. Sheng, W. An, and Y. Guo, “A stereo attention
    module for stereo image super-resolution,” *IEEE Signal Processing Letters*, vol. 27,
    2020.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] X. Ying, Y. Wang, L. Wang, W. Sheng, W. An, 和 Y. Guo, “立体图像超分辨率的立体注意力模块，”
    *IEEE信号处理通讯*，第27卷，2020年。'
- en: '[172] Y. Wang, X. Ying, L. Wang, J. Yang, W. An, and Y. Guo, “Symmetric parallax
    attention for stereo image super-resolution,” in *CVPR*, 2021.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Y. Wang, X. Ying, L. Wang, J. Yang, W. An, 和 Y. Guo, “用于立体图像超分辨率的对称视差注意力，”
    发表在 *CVPR*，2021年。'
- en: '[173] Q. Dai, J. Li, Q. Yi, F. Fang, and G. Zhang, “Feedback network for mutually
    boosted stereo image super-resolution and disparity estimation,” *arXiv preprint
    arXiv:2106.00985*, 2021.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Q. Dai, J. Li, Q. Yi, F. Fang, 和 G. Zhang, “用于互相增强的立体图像超分辨率和视差估计的反馈网络，”
    *arXiv预印本 arXiv:2106.00985*，2021年。'
- en: '[174] C. Dong, C. C. Loy, K. He, and X. Tang, “Learning a deep convolutional
    network for image super-resolution,” in *ECCV*, 2014.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] C. Dong, C. C. Loy, K. He, 和 X. Tang, “学习深度卷积网络用于图像超分辨率，” 发表在 *ECCV*，2014年。'
- en: '[175] C. Wang, Z. Li, and J. Shi, “Lightweight image super-resolution with
    adaptive weighted learning network,” *arXiv preprint arXiv:1904.02358*, 2019.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] C. Wang, Z. Li, 和 J. Shi, “具有自适应加权学习网络的轻量级图像超分辨率，” *arXiv预印本 arXiv:1904.02358*，2019年。'
- en: '[176] A. Muqeet, J. Hwang, S. Yang, J. Kang, Y. Kim, and S.-H. Bae, “Multi-attention
    based ultra lightweight image super-resolution,” in *ECCV*, 2020.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] A. Muqeet, J. Hwang, S. Yang, J. Kang, Y. Kim, 和 S.-H. Bae，“基于多重注意力的超轻量级图像超分辨率”，发表于
    *ECCV*，2020年。'
- en: '[177] J. Liu, W. Zhang, Y. Tang, J. Tang, and G. Wu, “Residual feature aggregation
    network for image super-resolution,” in *CVPR*, 2020.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] J. Liu, W. Zhang, Y. Tang, J. Tang, 和 G. Wu，“用于图像超分辨率的残差特征聚合网络”，发表于 *CVPR*，2020年。'
- en: '[178] Z. Hui, X. Gao, Y. Yang, and X. Wang, “Lightweight image super-resolution
    with information multi-distillation network,” in *ACMMM*, 2019.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Z. Hui, X. Gao, Y. Yang, 和 X. Wang，“具有信息多蒸馏网络的轻量级图像超分辨率”，发表于 *ACMMM*，2019年。'
- en: '[179] Z. Wang, G. Gao, J. Li, Y. Yu, and H. Lu, “Lightweight image super-resolution
    with multi-scale feature interaction network,” in *ICME*, 2021.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Z. Wang, G. Gao, J. Li, Y. Yu, 和 H. Lu，“具有多尺度特征交互网络的轻量级图像超分辨率”，发表于 *ICME*，2021年。'
- en: '[180] R. Lan, L. Sun, Z. Liu, H. Lu, C. Pang, and X. Luo, “Madnet: A fast and
    lightweight network for single-image super resolution,” *IEEE Transactions on
    Cybernetics*, vol. 51, 2020.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] R. Lan, L. Sun, Z. Liu, H. Lu, C. Pang, 和 X. Luo，“Madnet: 一种快速且轻量级的单图像超分辨率网络”，*IEEE
    Transactions on Cybernetics*，第51卷，2020年。'
- en: '[181] S. Zhou, J. Zhang, W. Zuo, and C. C. Loy, “Cross-scale internal graph
    neural network for image super-resolution,” *arXiv preprint arXiv:2006.16673*,
    2020.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] S. Zhou, J. Zhang, W. Zuo, 和 C. C. Loy，“用于图像超分辨率的跨尺度内部图神经网络”，*arXiv preprint
    arXiv:2006.16673*，2020年。'
- en: '[182] Y. Mei, Y. Fan, and Y. Zhou, “Image super-resolution with non-local sparse
    attention,” in *CVPR*, 2021.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Y. Mei, Y. Fan, 和 Y. Zhou，“具有非局部稀疏注意力的图像超分辨率”，发表于 *CVPR*，2021年。'
- en: '[183] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, “Image super-resolution
    using very deep residual channel attention networks,” in *ECCV*, 2018.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, 和 Y. Fu，“使用非常深的残差通道注意力网络进行图像超分辨率”，发表于
    *ECCV*，2018年。'
- en: '[184] Y. Ma, H. Xiong, Z. Hu, and L. Ma, “Efficient super resolution using
    binarized neural network,” in *CVPRW*, 2019.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Y. Ma, H. Xiong, Z. Hu, 和 L. Ma，“使用二值化神经网络的高效超分辨率”，发表于 *CVPRW*，2019年。'
- en: '[185] H. Li, C. Yan, S. Lin, X. Zheng, B. Zhang, F. Yang, and R. Ji, “Pams:
    Quantized super-resolution via parameterized max scale,” in *ECCV*, 2020.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] H. Li, C. Yan, S. Lin, X. Zheng, B. Zhang, F. Yang, 和 R. Ji，“Pams: 通过参数化最大尺度的量化超分辨率”，发表于
    *ECCV*，2020年。'
- en: '| ![[Uncaptioned image]](img/ad35a6525c198730bceb3301c40e89fb.png) | Juncheng
    Li received the Ph.D. degree from the School of Computer Science and Technology,
    East China Normal University (ECNU), China, in 2021\. He is currently a Postdoctoral
    Fellow at the Center for Mathematical Artificial Intelligence (CMAI), The Chinese
    University of Hong Kong (CUHK). His research interests include artificial intelligence
    and its applications to computer vision and image processing (e.g., image super-resolution,
    image denoising, image deblurring, image dehazing, and image enhancement). |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/ad35a6525c198730bceb3301c40e89fb.png) | Juncheng Li 于2021年获得了中国东华大学计算机科学与技术学院的博士学位。他目前是香港中文大学（CUHK）数学人工智能中心（CMAI）的博士后研究员。他的研究兴趣包括人工智能及其在计算机视觉和图像处理中的应用（例如，图像超分辨率、图像去噪、图像去模糊、图像去雾和图像增强）。
    |'
- en: '| ![[Uncaptioned image]](img/aa9d90b244c30c98f3b28e57ed5a6089.png) | Zehua
    Pei is now an undergraduate student at the Department of Mathematics, The Chinese
    University of Hong Kong (CUHK). He will receive his B.S. degree in 2022\. His
    research interests include data mining, computer vision, optimization, and image
    processing (e.g., image super-resolution, image denoising, image dehazing, and
    image enhancement). |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/aa9d90b244c30c98f3b28e57ed5a6089.png) | Zehua Pei 目前是香港中文大学（CUHK）数学系的本科生。他将在2022年获得学士学位。他的研究兴趣包括数据挖掘、计算机视觉、优化和图像处理（例如，图像超分辨率、图像去噪、图像去雾和图像增强）。
    |'
- en: '| ![[Uncaptioned image]](img/c3a19a21fb1c890e6633b67d2e9e07e8.png) | Tieyong
    Zeng received the B.S. degree from Peking University, Beijing, China, in 2000,
    the M.S. degree from École Polytechnique, Palaiseau, France, in 2004, and the
    Ph.D. degree from the Université of Paris XIII, Paris, France, in 2007\. He is
    currently a Professor at the Department of Mathematics, The Chinese University
    of Hong Kong (CUHK). He is also the Director of the Center for Mathematical Artificial
    Intelligence, CUHK. He has published more than 100 papers. His research interests
    include image processing, optimization, artificial intelligence, scientific computing,
    computer vision, machine learning, and inverse problems. |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/c3a19a21fb1c890e6633b67d2e9e07e8.png) | 曾铁勇于2000年获得北京大学的学士学位，2004年获得法国巴黎高科的硕士学位，并于2007年获得法国巴黎第十三大学的博士学位。他目前是香港中文大学（CUHK）数学系的教授，同时也是香港中文大学数学人工智能中心的主任。他已经发表了100多篇论文。他的研究兴趣包括图像处理、优化、人工智能、科学计算、计算机视觉、机器学习以及逆问题。'
