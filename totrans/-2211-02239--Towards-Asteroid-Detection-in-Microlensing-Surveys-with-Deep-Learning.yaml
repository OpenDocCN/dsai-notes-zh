- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:43:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:43:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2211.02239] Towards Asteroid Detection in Microlensing Surveys with Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2211.02239] 基于深度学习的微引力透镜调查中的小行星检测'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2211.02239](https://ar5iv.labs.arxiv.org/html/2211.02239)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2211.02239](https://ar5iv.labs.arxiv.org/html/2211.02239)
- en: Towards Asteroid Detection in Microlensing Surveys with Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的微引力透镜调查中的小行星检测
- en: Preeti Cowan Ian A. Bond Napoleon H. Reyes
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Preeti Cowan Ian A. Bond Napoleon H. Reyes
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Asteroids are an indelible part of most astronomical surveys though only a few
    surveys are dedicated to their detection. Over the years, high cadence microlensing
    surveys have amassed several terabytes of data while scanning primarily the Galactic
    Bulge and Magellanic Clouds for microlensing events and thus provide a treasure
    trove of opportunities for scientific data mining. In particular, numerous asteroids
    have been observed by visual inspection of selected images. This paper presents
    novel deep learning-based solutions for the recovery and discovery of asteroids
    in the microlensing data gathered by the MOA project. Asteroid tracklets can be
    clearly seen by combining all the observations on a given night and these tracklets
    inform the structure of the dataset. Known asteroids were identified within these
    composite images and used for creating the labelled datasets required for supervised
    learning. Several custom CNN models were developed to identify images with asteroid
    tracklets. Model ensembling was then employed to reduce the variance in the predictions
    as well as to improve the generalisation error, achieving a recall of 97.67%.
    Furthermore, the YOLOv4 object detector was trained to localize asteroid tracklets,
    achieving a mean Average Precision (mAP) of 90.97%. These trained networks will
    be applied to 16 years of MOA archival data to find both known and unknown asteroids
    that have been observed by the survey over the years. The methodologies developed
    can be adapted for use by other surveys for asteroid recovery and discovery.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 小行星是大多数天文调查中不可磨灭的一部分，尽管只有少数调查专门用于其检测。多年来，高频次的微引力透镜调查积累了数TB的数据，主要扫描银河系核球和麦哲伦云中的微引力透镜事件，因此提供了丰富的科学数据挖掘机会。特别是，通过对选定图像的视觉检查，观察到了许多小行星。本文提出了基于深度学习的创新解决方案，用于恢复和发现MOA项目中收集的微引力透镜数据中的小行星。通过将特定夜晚的所有观测数据结合在一起，小行星轨迹可以清晰可见，这些轨迹揭示了数据集的结构。在这些复合图像中识别出已知的小行星，并用来创建监督学习所需的标注数据集。开发了几个定制的CNN模型来识别包含小行星轨迹的图像。然后使用模型集成来减少预测的方差，并改善泛化误差，达到了97.67%的召回率。此外，YOLOv4对象检测器经过训练以定位小行星轨迹，达到了90.97%的平均精度（mAP）。这些训练后的网络将应用于16年的MOA存档数据，以寻找在这些年中被调查观察到的已知和未知的小行星。所开发的方法可以适应其他调查用于小行星的恢复和发现。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'microlensing surveys , asteroid detection , deep learning , convolutional neural
    networks , YOLOv4 , MOA^†^†journal: Astronomy and Computing\affiliation'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 微引力透镜调查，小行星检测，深度学习，卷积神经网络，YOLOv4，MOA^†^†期刊：天文学与计算\affiliation
- en: '[inst1]organization=School of Mathematical and Computational Sciences, Massey
    University,addressline=Private Bag 102-904 North Shore Mail Centre, city=Auckland,
    postcode=0745, country=New Zealand'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[inst1]组织=数学与计算科学学院，梅西大学，地址=Private Bag 102-904 North Shore Mail Centre，城市=奥克兰，邮政编码=0745，国家=新西兰'
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Asteroids are among the millions of small bodies that inhabit our Solar System
    and are remnants from its formation. While popular sentiment most commonly associates
    asteroids with mass extinction events, the vast majority of asteroids pose no
    threat to us. The Main Asteroid Belt between Mars and Jupiter has the largest
    concentration of asteroids in our Solar System and this is where the majority
    of the asteroids seen in this research reside. Observing and tracking these small
    bodies gives us a better understanding of their complex orbital dynamics. Their
    composition and structure offer clues about the conditions when the terrestrial
    planets were formed 4.6 billion years ago.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 小行星是我们太阳系中数百万个小天体中的一部分，它们是太阳系形成时的遗留物。虽然大众普遍将小行星与大规模灭绝事件联系在一起，但绝大多数小行星对我们构成不了威胁。位于火星和木星之间的小行星带是我们太阳系中小行星最密集的区域，这里也是本研究中大多数小行星的所在地。观察和跟踪这些小天体可以帮助我们更好地理解它们复杂的轨道动态。它们的组成和结构为了解46亿年前地球行星形成时的条件提供了线索。
- en: Asteroids are part of the landscape of our night sky and appear in the imaging
    data of most astronomical surveys. However, as most surveys have a specialised
    purpose, their data is rarely mined for asteroids. Microlensing surveys like MOA
    (Sumi et al. ([2003](#bib.bib32))), OGLE (Udalski et al. ([1993](#bib.bib36))),
    and KMTNet (Kim et al. ([2016](#bib.bib16))) are particularly good for determining
    the rotation period and orbital trajectory of asteroids because they survey a
    given region of space several times each night (Gould and Yee ([2013](#bib.bib11))).
    This means that asteroids could spend several nights in the field of view of the
    telescope, giving us the opportunity to both observe their trajectory and analyse
    the light gathered from them. Cordwell et al. ([2022](#bib.bib6)) demonstrates
    the efficacy of extracting asteroids light curves from the MOA microlensing data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 小行星是我们夜空景观的一部分，并且出现在大多数天文调查的影像数据中。然而，由于大多数调查有其特定的目的，它们的数据很少用于小行星的研究。像 MOA（Sumi
    et al. ([2003](#bib.bib32)))、OGLE（Udalski et al. ([1993](#bib.bib36))) 和 KMTNet（Kim
    et al. ([2016](#bib.bib16))) 这样的微引力透镜调查特别适合确定小行星的自转周期和轨道轨迹，因为它们每晚都会多次观测一个特定的空间区域（Gould
    and Yee ([2013](#bib.bib11)))。这意味着小行星可能会在望远镜的视野中停留几个夜晚，使我们有机会既观察它们的轨迹，又分析从中收集的光线。Cordwell
    et al. ([2022](#bib.bib6)) 展示了从 MOA 微引力透镜数据中提取小行星光变曲线的有效性。
- en: Automated detection software has been part of surveys dedicated to discovering
    asteroids since the early 90s (Rabinowitz ([1991](#bib.bib25))). With improved
    computing power, other techniques for detecting moving astronomical sources such
    as shift and stack have also proven popular. In recent years, a leader in the
    field is the Pan-STARRS Moving Object Processing System or MOPS (Denneau et al.
    ([2013](#bib.bib7))). Initially trained with simulated but realistic asteroid
    data for the Pan-STARRS telescopes, it takes transient candidates not associated
    with a known source and uses a complex tree-based spatial linking algorithm (Kubica
    et al. ([2007](#bib.bib19))) to further parse and form associations between these
    point sources. MOPS does not work with imaging data but rather celestial coordinates,
    which reduces the computational cost. HeliolinC (Holman et al. ([2018](#bib.bib14)))
    further improves on MOPS’ efficiency with an approach that combines working with
    a heliocentric frame of reference and clustering sources that belong to the same
    object.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自动检测软件自90年代初期以来一直是专门用于发现小行星的调查的一部分（Rabinowitz ([1991](#bib.bib25)))。随着计算能力的提高，其他检测运动天文源的技术，如平移和堆叠，也变得流行。近年来，领域的领导者是
    Pan-STARRS 移动物体处理系统或 MOPS（Denneau et al. ([2013](#bib.bib7)))。最初用 Pan-STARRS 望远镜的模拟但真实的小行星数据进行训练，它处理那些与已知源无关的瞬态候选体，并使用复杂的基于树的空间关联算法（Kubica
    et al. ([2007](#bib.bib19))) 进一步解析并形成这些点源之间的关联。MOPS 不处理影像数据，而是处理天体坐标，这减少了计算成本。HeliolinC（Holman
    et al. ([2018](#bib.bib14))) 通过结合使用日心坐标系和聚类属于同一物体的源的方式进一步提高了 MOPS 的效率。
- en: While these and other deterministic approaches have been successfully utilized
    for asteroid detection, applications of deep learning in the field remain in the
    early stages, potentially because of the lack of labelled data. Deep learning
    offers the benefit of being able to learn representations directly from the raw
    data, making it a potentially valuable tool for asteroid discovery in archival
    astronomical data. The works that do apply deep leaning techniques note the benefits,
    particularly with greatly reducing the amount of data that must be examined by
    an astronomer, as we see next.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些和其他确定性方法已成功用于小行星检测，但深度学习在该领域的应用仍处于早期阶段，可能是由于缺乏标记数据。深度学习的优势在于能够直接从原始数据中学习表征，这使它成为在档案天文数据中发现小行星的潜在有价值的工具。应用深度学习技术的工作指出了其好处，特别是在大大减少天文学家需要检查的数据量方面，正如我们接下来看到的那样。
- en: Zoghbi et al. ([2017](#bib.bib41)) successfully applied both convolutional and
    recurrent architectures to reduce the amount of data to be vetted by astronomers
    looking for debris from long-period comets in the CAMS data¹¹1http://cams.seti.org/.
    Lieu et al. ([2018](#bib.bib20)) applied neural networks to the task of detecting
    small solar system objects (SSO) in data simulated for the ESA’s Euclid space
    telescope²²2https://sci.esa.int/web/euclid/. They successfully used transfer learning
    and retrained three architectures from TensorFlow’s Keras Applications library³³3https://keras.io/api/applications/
    to distinguish between postage stamp cut-out images of asteroids and objects commonly
    mistaken for asteroids like cosmic rays, stars, and galaxies. Duev et al. ([2019](#bib.bib9))
    introduced DeepStreaks to aid in the ZTF’s⁴⁴4https://www.ztf.caltech.edu/ quest
    for the discovery of near-Earth asteroids, which resemble streaks in the observations.
    Their model significantly reduced the number of candidate detections that had
    to be reviewed without sacrificing the detection sensitivity. Rabeendran and Denneau
    ([2021](#bib.bib24)) applied deep learning to the ATLAS⁵⁵5https://atlas.fallingstar.com/home.php
    pipeline looking for near-Earth objects. It was successful in catching nearly
    90% of the false positive detections, thus greatly speeding up the process of
    followup observations. Duev et al. ([2021](#bib.bib8)) introduced Tails, which
    involved training an object detector to discover comets based on their distinctive
    morphology and it now forms a part of the ZTF’s detection pipeline. Finally, Kruk
    et al. ([2022](#bib.bib18)) used deep learning to hunt for asteroid trails in
    archival data from the Hubble space telescope (HST)⁶⁶6https://hubblesite.org/.
    They used composite HST images to make the asteroids trails longer and thus easier
    to detect. Their research also demonstrates the merits of citizen science for
    labelling the data and of mining archival data for asteroids with a deep learning-based
    toolkit.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that an important aspect of finding asteroids in survey data is
    isolating the sources that indicate a candidate object. In the case of microlensing
    data, this task is all the more challenging because of the extremely dense star
    fields observed, causing even the reference subtracted images to contain numerous
    spurious artefacts. Thus, rather than extracting the sources and then establishing
    connections between them, we propose a technique to instead enable extracting
    a cluster of sources that could represent part of the orbital arc of asteroids.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: We present the methodology to create labelled datasets suitable for supervised
    learning, followed by convolutional neural network-based architectures for finding
    asteroids tracklets in microlensing surveys. Our classification models facilitate
    reducing the amount of data to be vetted and the object detection model localizes
    the potential tracklets within the classifiers’ candidate detections. The bounding
    boxes predicted by the object detector could then be used to extract the potential
    sources, which in turn could be passed to orbiting link software, such as HeliolinC,
    to determine if they are valid solar-bound objects.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '2 MOA: Microlensing Observations in Astrophysics'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This research utilizes archival data from the MOA (Microlensing Observations
    in Astrophysics) project, which is a Japan/New Zealand collaboration specialising
    in the search for gravitational microlensing events. They have been operating
    the 1.8m MOA-II optical research telescope at the Mt. John Observatory since 2004\.
    It has a wide-field mosaic CCD camera called the MOA-cam3 (Sako et al. ([2008](#bib.bib30))),
    which consists of 10 CCD chips. Each CCD chip is 3cm x 6cm and has 2048 x 4096
    pixels, with a resolution of 0.01 arc minutes/ 0.6 arcseconds per pixel. Each
    exposure is 60 seconds long and all images are in a custom wide MOA-R band (630-1000
    nm). The median FWHM seeing is  1.7 arcseconds. The photometric precision is typically
     0.01 mag or better for I brighter than 16 and  0.02 mag for I 18\. The telescope
    has a total field of view of 1.7 x 1.4 square degrees. The survey field referred
    to as GB1 can be seen in its entirety in Figure [1](#S2.F1 "Figure 1 ‣ 2 MOA:
    Microlensing Observations in Astrophysics ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning"). The CCD chips are numbered 1 to 10 clockwise, starting
    from the top-left.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: MOA-II surveys the Galactic Bulge (GB) and the Large Magellanic Cloud (LMC),
    both of which are regions of the sky that are densely packed with stars. It operates
    at a high sampling rate, with some fields surveyed as often as every 10 minutes,
    which makes it particularly good for observing short duration events. The MOA
    image processing pipeline produces difference images (Tomaney and Crotts ([1996](#bib.bib35));
    Alard and Lupton ([1998](#bib.bib2)); Alard ([2000](#bib.bib1)); Bramich ([2008](#bib.bib5));
    Bond et al. ([2001](#bib.bib4)), where a new observation image is subtracted from
    a reference image for the same region. The resultant difference image highlights
    the changes since the reference image was taken. These could either be transient
    astronomical phenomenon (like microlensing events or asteroids) or noise. The
    noise could be due to too-bright saturated stars, imperfect subtractions, satellite
    trails, instrumentation error, atmospheric dust, differential refraction, or proximity
    to a bright astronomical object (like the moon). As the fields surveyed by MOA-II
    are very dense, faint asteroids in particular are impossible to identify in the
    original observation images. Thus, the MOA difference images form the basis of
    the datasets built for this research.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b801f1a8e3e7ee626fe1a9c935f0f4c0.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A single observation of the Galactic Bulge field, GB1, surveyed by
    MOA-II. North is right to left and east is bottom to top.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 3 Building the Dataset
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Galactic Bulge is visible in the Southern Hemisphere between February and
    October. A combination of long nights and Bulge elevation in the mid-winter months
    make them the best time for observation. On clear nights with good seeing conditions,
    several of the MOA-II fields are surveyed frequently, at times as often as every
    10 minutes. This provides ideal conditions for observing asteroids in our solar
    system. Figure [2](#S3.F2 "Figure 2 ‣ 3 Building the Dataset ‣ Towards Asteroid
    Detection in Microlensing Surveys with Deep Learning") demonstrates this with
    six observations of a bright main-belt asteroid (78153) 2002 NX24, with a limiting
    magnitude (V) of 17.5, taken at 10-12 minute intervals on the 23rd of June, 2006\.
    The light curve for this asteroid, extracted from the MOA-II data can be see in
    Figure [3](#S3.F3 "Figure 3 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection
    in Microlensing Surveys with Deep Learning"). The sky-plane velocities are about
    0.5 arcseconds per minute, which are representative of the asteroids captured
    by MOA-II.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2a44eaec5e32b886ffa0df3483c4dd1.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Six consecutive observations of the asteroid (78153) 2002 NX24 on
    23-June-2006, spanning 70 x 108 arcseconds.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cf625d3b4b0d5dc727960ff5d55f195c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Light curve of asteroid (78153) 2002 NX24 extracted from the MOA-II
    data.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ff2bfe14f1c0f60f77d99a7456e9e9d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Measured MOA flux vs the MPC V magnitude for asteroids recovered
    from GB5-R5\. The MPC value is reported to one decimal place and the results are
    binned in 0.1 magnitude bins. The red dots represents the median values of the
    fluxes. This starts to flatten out when V is around 21, at which point it is effectively
    the background being measured. The green line represents the working limiting
    flux commonly used in MOA microlensing work'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：GB5-R5中恢复的小行星的MOA测得通量与MPC V星等的关系。MPC值报告到小数点后一位，结果以0.1星等为单位进行分箱。红点表示通量的中位数值。当V星等接近21时，曲线开始趋于平坦，此时实际测量的是背景。绿色线表示MOA微引力工作中常用的限制通量。
- en: The primary data used in this research consists of all observations from 2006
    - 2019 from one chip (chip 5) in the CCD array for the field GB5, amounting to
    49,901 difference images (GB5-R5). This field is surveyed very frequently and
    the cadence of most observations is between 5 and 20 minutes. The Minor Planet
    Center (MPC) was queried⁷⁷7https://www.minorplanetcenter.net/cgi-bin/checkmp.cgi
    via a screen scraper to get all known asteroids expected to traverse through the
    region encompassed by GB5-R5\. To determine the limiting magnitude of our survey,
    we have compared the fluxes measured at the positions of recovered asteroids from
    GB5-R5 with the V magnitudes provided by MPC. The results are shown Figure [4](#S3.F4
    "Figure 4 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning"), where the data has been binned by MPC V magnitude
    with a bin size of 0.1 mag with the median flux values calculated for each bin.
    For microlensing measurements, MOA adopts a working value of 2500 for the limiting
    data number. This corresponds to a MPC V magnitude  20.5 which we adopt as our
    limiting magnitude here.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究使用的主要数据包括2006年至2019年在CCD阵列中的一个芯片（芯片5）对GB5场的所有观测数据，共计49,901幅差分图像（GB5-R5）。该场被非常频繁地调查，大多数观测的时间间隔在5到20分钟之间。通过屏幕抓取程序查询了小行星中心（MPC）[7](https://www.minorplanetcenter.net/cgi-bin/checkmp.cgi)，获取了所有已知的预计穿过GB5-R5区域的小行星。为了确定我们调查的限制星等，我们将GB5-R5中恢复的小行星位置测得的通量与MPC提供的V星等进行了比较。结果见图[4](#S3.F4
    "Figure 4 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning")，数据按MPC V星等进行分箱，箱宽为0.1星等，并计算了每个箱的中位数通量值。对于微引力测量，MOA采用了2500作为限制数据数量的工作值。这对应于MPC
    V星等20.5，我们在这里将其作为我们的限制星等。
- en: Since asteroids move appreciably with respect to the background stars in each
    consecutive exposure, asteroid tracklets can be clearly seen by stacking nightly
    observations. We define a tracklet as part of the orbital arc of the asteroid
    as it travels through the field of view of the telescope. We started with a stack
    composed of the brightest pixels. While the tracklets were visible, so was the
    bright background and the over-bright stars (Figure [5](#S3.F5 "Figure 5 ‣ 3 Building
    the Dataset ‣ Towards Asteroid Detection in Microlensing Surveys with Deep Learning")(a)).
    To highlight the tracklets better, the stack image was further simplified by subtracting
    the brightest pixel stack from the median pixel stack. This gave us an image without
    the bright background and saturated stars (Figure [5](#S3.F5 "Figure 5 ‣ 3 Building
    the Dataset ‣ Towards Asteroid Detection in Microlensing Surveys with Deep Learning")(c)),
    leaving behind the noise and moving objects. Only nights with 3 or more observations
    were considered when generating the subtracted stacks. The GB5-R5 dataset resulted
    in 2252 subtracted stack images within which the search for asteroid tracklets
    commenced.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于小行星在每次连续曝光中相对于背景恒星有显著移动，通过叠加夜间观测可以清晰地看到小行星的轨迹片段。我们将轨迹片段定义为小行星在望远镜视野中移动时轨道弧的一部分。我们从由最亮的像素组成的堆叠开始。虽然轨迹片段可见，但亮背景和过亮的恒星也同样明显（图[5](#S3.F5
    "Figure 5 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning")(a)）。为了更好地突出轨迹片段，通过从中位像素堆叠中减去最亮像素堆叠，进一步简化了堆叠图像。这给我们提供了一幅没有亮背景和饱和恒星的图像（图[5](#S3.F5
    "Figure 5 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning")(c)），只留下噪声和移动物体。在生成减法堆叠时，只考虑了有3次或更多观测的夜晚。GB5-R5数据集产生了2252幅减法堆叠图像，开始了对小行星轨迹片段的搜索。
- en: The ephemeris data from the MPC, together with the astrometric calibrations
    specific to MOA-II, were used to extrapolate the minimum and maximum (x, y) positions
    for asteroid tracklets in subtracted stack images. This was used to crop sub-regions
    expected to contain tracklets from each stack. Asteroids that fell beyond the
    boundaries of a CCD chip were ignored. Further, by visual inspection of the tracklets,
    it was established that only asteroids with a limiting magnitude of 20.5 or brighter
    are visible in the MOA-II exposures. This is also consistent with what we see
    in Figure [4](#S3.F4 "Figure 4 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection
    in Microlensing Surveys with Deep Learning"), where the median flux around V 21
    flattens out and is akin to measuring the background. The objects with magnitude
    between 19.5 and 20.5 are often at the very edge of visibility, requiring excellent
    seeing condition and a high signal to noise ratio to be visible in the observations.
    Careful examination of the images resulted in 2073 tracklets from 1178 distinct
    asteroids over 1078 distinct nights. Figure [6](#S3.F6 "Figure 6 ‣ 3 Building
    the Dataset ‣ Towards Asteroid Detection in Microlensing Surveys with Deep Learning")
    (374 x 387 arcseconds) displays some of the tracklets captured in the observations
    on the night of 15-May-2008\. Some of the tracklets are very faint.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Tracklets come in a variety of sizes, but we need images of a uniform size for
    training neural networks. The original size of the observations was deemed too
    big to be used as is because of the computational cost as well as the potential
    for the tracklets to be lost amongst the other artefacts in the image. Therefore,
    a decision was made to split each image into 128 x 128 tiles, giving us 512 tiles
    per image. Each 128x128 tile spans 76 x 76 arcseconds. For GB5-R5, this gave us
    a total of 551,936 images from the 1078 nightly stacks that contained visible
    asteroid tracklets.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/265ba679a900ea03e0e6c38493aebc21.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Stacking all of a night’s observations (23-June-2006) on one chip
    in one field gives us a clear tracklet for asteroid (78153) 2002 NX24\. In (a)
    the observations are stacked by brightest pixel; in (b) they are stacked by the
    median pixel; and in (c) we see the result of subtracting (b) from (a).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/939750fc6c00b7e012226a67aed0e76c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: A stack of all 51 observations from the night of 15-May-2008 reveals
    5 asteroid tracklets clearly visible in a sub-region of the stack image. Numbered
    in order of appearance, these are: 1 - (103842) 2000 DQ33 (19.5); 2 - (148657)
    2001 SX124 (19.6); 3 - (152083) 2004 RH30 (19.9); 4 - (582743) 2016 AT221 (20.4);
    5 - (338789) 2005 SZ154 (20.4). The line/streak on the top left is from a satellite.
    This image has been inverted and brightened to improve visibility (original in
    Figure [15](#A1.F15 "Figure 15 ‣ Appendix A Original images ‣ Towards Asteroid
    Detection in Microlensing Surveys with Deep Learning")).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'The Cohen-Sutherland line clipping algorithm (W. M. Newman and R. F. Sproull
    ([1973](#bib.bib37))) was next used to locate the asteroid tracklets within these
    images. The algorithm involves dividing a rectangular space (in this case, an
    image) into nine regions - eight ”outside” and one ”inside”, as seen in Figure
    [7](#S3.F7 "Figure 7 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in
    Microlensing Surveys with Deep Learning") - and determining which of the lines
    (tracklets) are fully or partially inside the area of interest (the image). Each
    of the nine regions have associated outcodes (4-bit numbers) that are calculated
    by performing bitwise operations after comparing the start and end points of the
    line/tracklet with the coordinates of the image. For example, the outcode 0001
    indicates that the end point is center left and the outcode 1000 indicates that
    the end point is center top. A bitwise OR of these two codes returns 1001, which
    is a non-zero value, and the bitwise AND returns 0000, which is zero, which in
    turn indicates that the tracklet is partially inside the image. Thus, there are
    three possible solutions for any line:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If both endpoints of the line are inside the area of interest, the bitwise OR
    computation returns 0 (trivial accept).
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If both endpoints of the line are outside the area of interest, they will share
    at least one outside region and the bitwise AND computation returns a non 0 value
    (trivial reject).
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If both endpoints are in different regions, at least one endpoint will be outside
    the image tile. In this case, the intersection point of the tracklet’s outside
    point and image tile boundary becomes the new endpoint for the tracklet and the
    algorithm repeats until the bitwise operation returns a trivial accept or reject.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/93bb1495db9f560aaf0f870042674e34.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The Cohen-Sutherland line clipping algorithm was used to reject tracklets
    outside the area of interest and determine intersection points of the ones partially
    inside.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: The application of this algorithm gave us 8341 tiles that potentially had visible
    tracklets and 543,595 without known asteroids. Once again these were meticulously
    scanned to ensure that a tracklet could clearly be seen. Any tile with less than
    3 points of a tracklet were rejected, along with tiles that did not contain a
    visible portion of a tracklet. At the conclusion of this process, there were 4153
    128 x 128 tiles with visible tracklets from GB5-R5\. Some of these can be seen
    in Figure [8](#S3.F8 "Figure 8 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection
    in Microlensing Surveys with Deep Learning")(a).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的应用给我们提供了8341个可能含有可见轨迹的小块和543,595个没有已知小行星的小块。为了确保能够清晰地看到轨迹，每个小块都被细致地扫描过。任何含有少于3个轨迹点的小块都会被拒绝，此外，还有不包含可见轨迹部分的小块也会被排除。经过这一过程，最终得到4153个128
    x 128的小块，这些小块中含有来自GB5-R5的可见轨迹。部分示例见于图 [8](#S3.F8 "Figure 8 ‣ 3 Building the Dataset
    ‣ Towards Asteroid Detection in Microlensing Surveys with Deep Learning")(a)。
- en: '![Refer to caption](img/ad828397fc55ba1fde6e97c267c614d1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ad828397fc55ba1fde6e97c267c614d1.png)'
- en: 'Figure 8: Asteroid tracklets seen in 128x128 sub-regions from subtracted stack
    images. In (b), the tracklets are localized with bounding boxes. Asteroids in
    the top row, left to right: (38102) 1999 JM18 (18.4); (152083) 2004 RH30 (19.9);
    (103842) 2000 DQ33 (19.5); (283261) 2011 FR142 (20.0); and (375674) 2009 HD5 (19.8).
    Asteroids in the bottom row, from left to right: (48617) 1995 HR2 (19.5); (148657)
    2001 SX124 (19.6); (97948) 2000 QF124 (19.6); and (74978) 1999 TY234 (18.7). Note
    that these images have been inverted and brightened to improve visibility (original
    in Figure [16](#A1.F16 "Figure 16 ‣ Appendix A Original images ‣ Towards Asteroid
    Detection in Microlensing Surveys with Deep Learning")).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 从减法堆叠图像中观察到的128x128子区域中的小行星轨迹。在(b)中，轨迹用边界框标出。顶部行的行星，从左到右依次是：(38102) 1999
    JM18 (18.4)；(152083) 2004 RH30 (19.9)；(103842) 2000 DQ33 (19.5)；(283261) 2011
    FR142 (20.0)；和(375674) 2009 HD5 (19.8)。底部行的行星，从左到右依次是：(48617) 1995 HR2 (19.5)；(148657)
    2001 SX124 (19.6)；(97948) 2000 QF124 (19.6)；和(74978) 1999 TY234 (18.7)。请注意，这些图像已经被反转并增强了亮度以提高可见性（原图见于图
    [16](#A1.F16 "Figure 16 ‣ Appendix A Original images ‣ Towards Asteroid Detection
    in Microlensing Surveys with Deep Learning")）。'
- en: For classification, the images where the tracklet was too obscured by noise
    or where there were less than three clearly visible adjacent point sources related
    to a tracklet were removed. Following this, there were 4072 images with tracklets,
    which were split into the training/ validation/ test set with 3322/ 415/ 335 in
    each, respectively. The images in the training and validation set were further
    augmented by rotating 180 degrees, flipping horizontally and vertically, brightening,
    darkening, blurring, and both increasing and decreasing the contrast. There was
    no overlap between the training, validation, and test sets. A further test set
    composed of all the observations on a single night from all ten chips in fields
    GB3, GB4, GB5, GB9, GB10, and GB14 was also constructed (GB-All), and it had 300
    tracklet and 2000 no-tracklet images. The purpose of the GB-All test set was to
    evaluate how the networks performed with data they had never seen; as before,
    the tracklets were from observations with a maximum cadence of 20 minutes and
    there are a minimum of 3 sources per image. The classification networks were also
    trained to recognize images with no tracklets. Forty images without known tracklets
    were randomly chosen from each of the 512 sub-regions, giving us 20,480 images.
    This collection was also visually inspected and any images that could potentially
    have tracklets were removed. This process resulted in 19,682 images, which were
    split into the training/validation/test sets with 15,595/2039/2048 images, respectively.
    The same set of augments was generated for the no-tracklet training and validation
    data, but since this set was already significantly larger, only a random 35% of
    the no-tracklet augments were used. Note that this distribution does not reflect
    the true ratio of tracklet vs no-tracklet images; realistically, on a good night,
    we might expect as many 10 tracklet images for every 100 no-tracklet images. The
    roughly 1:5 ratio here serves to better focus the network to learn the tracklet
    pattern while providing a suitable number of contrasting no-tracklet images.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，所有由于噪声而轨迹点过于模糊的图像或少于三个清晰可见的与轨迹相关的点源的图像都被移除。随后，剩下了4072张包含轨迹的图像，这些图像被划分为训练集/验证集/测试集，分别包含3322/415/335张图像。训练集和验证集的图像进一步通过旋转180度、水平和垂直翻转、调整亮度、加深、模糊以及增加和减少对比度进行了数据增强。训练集、验证集和测试集之间没有重叠。还构建了一个由所有十个芯片在GB3、GB4、GB5、GB9、GB10和GB14领域的单夜观测组成的额外测试集（GB-All），该测试集包含300张轨迹图像和2000张无轨迹图像。GB-All测试集的目的是评估网络在从未见过的数据上的表现；如前所述，轨迹来自最大间隔为20分钟的观测，并且每张图像中至少包含3个源。分类网络也被训练来识别没有轨迹的图像。我们从512个子区域中随机选择了40张没有已知轨迹的图像，共计20480张图像。这些图像也进行了视觉检查，任何可能包含轨迹的图像都被移除。这个过程最终得到19682张图像，这些图像被划分为训练集/验证集/测试集，分别为15595/2039/2048张图像。对于无轨迹的训练和验证数据，生成了相同的增强集，但由于此集已经显著更大，因此只使用了35%的无轨迹增强数据。需要注意的是，这种分布并不反映轨迹图像与无轨迹图像的真实比例；实际上，在一个良好的夜晚，我们可能期望每100张无轨迹图像中有10张轨迹图像。这里大约1:5的比例旨在更好地使网络专注于学习轨迹模式，同时提供适当数量的对比无轨迹图像。
- en: As the asteroid tracklets are a well-defined pattern of blobs in an image, we
    further considered training a deep-learning based object detector to localize
    them in the images. This would facilitate locating the tracklets that were faint
    and/ or obscured by noise as well as distinguish the area of interest in the images.
    We utilize YOLOv4 (Bochkovskiy et al. ([2020](#bib.bib3))), which requires the
    objects of interest - in our case, tracklets - to be enclosed in bounding boxes,
    with the coordinates of the object’s centroid saved with respect to the dimensions
    of the image. As a result of applying the Cohen-Sutherland line clipping algorithm,
    we had the probable intersection/end points of tracklets within the 128 x 128
    images. These intersection points were used to extrapolate bounding boxes for
    tracklets, thus highlighting them in the images. Further visual inspection and
    manual adjustment was undertaken to ensure that bounding boxes encapsulated the
    tracklets correctly without any superfluous background included (Figure [8](#S3.F8
    "Figure 8 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning")(b)). All 4153 tracklet images were used, with a training/
    test split of 3737 and 416, respectively.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于小行星轨迹片段在图像中是一个定义明确的斑点模式，我们进一步考虑训练基于深度学习的物体检测器，以在图像中定位它们。这将有助于定位那些微弱和/或被噪声遮挡的轨迹片段，并区分图像中的兴趣区域。我们利用了YOLOv4（Bochkovskiy
    et al. ([2020](#bib.bib3)))，它要求将感兴趣的对象 - 在我们的例子中是轨迹片段 - 包含在边界框中，且对象的质心坐标根据图像的尺寸保存。通过应用Cohen-Sutherland线裁剪算法，我们得到了128
    x 128图像中的轨迹片段的可能交点/结束点。这些交点用于推测轨迹片段的边界框，从而在图像中突出显示它们。进一步的视觉检查和手动调整已被执行，以确保边界框准确地包围了轨迹片段，没有包含任何多余的背景（图
    [8](#S3.F8 "图 8 ‣ 3 构建数据集 ‣ 利用深度学习在微透镜调查中进行小行星检测")(b)）。所有4153张轨迹片段图像都被使用，其中训练/测试的分割比例为3737和416。
- en: 4 Deep Learning Framework
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度学习框架
- en: The appeal of neural networks and deep learning is in their capacity to make
    predictions about complex data in real time after they have been trained with
    a representative dataset. In the last decade, deep learning has matured significantly
    and has proven to be highly effective for classification and object detection
    tasks. Here, we discuss training several convolutional neural network (CNN) based
    architectures to find asteroids tracklets in the subtracted stack images.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和深度学习的吸引力在于它们能够在经过代表性数据集训练后，实时对复杂数据进行预测。在过去十年中，深度学习已经显著成熟，并且在分类和物体检测任务中证明了其高效性。在这里，我们讨论了训练几种基于卷积神经网络（CNN）的架构，以在减法叠加图像中找到小行星轨迹片段。
- en: The proposed model consists of an ensemble of five classifiers that produces
    a probability that a 128x128 composite image tile contains a tracklet. Tiles believed
    to contain tracklets are then passed to YOLOv4, which has been trained to localize
    tracklets by adding a bounding box around the tracklet within the image.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的模型由五个分类器的集成组成，生成128x128复合图像块包含轨迹片段的概率。被认为包含轨迹片段的图块随后传递给YOLOv4，YOLOv4经过训练以通过在图像中的轨迹片段周围添加边界框来定位轨迹片段。
- en: 4.1 Classification
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 分类
- en: A convolutional neural network or CNN is a network architecture that is designed
    to take advantage of the 2D structure of an input image. Using a series of convolutional
    layers that utilise filters, implemented using local connections and shared weights,
    it can extract meaningful features directly from data, thereby eliminating the
    need for manual feature extraction.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络或CNN是一种网络架构，旨在利用输入图像的2D结构。通过一系列使用滤波器的卷积层，这些滤波器通过局部连接和共享权重来实现，它可以直接从数据中提取有意义的特征，从而消除了手动特征提取的需要。
- en: Traditionally, the number of filters in a layer increases with the depth of
    the network. The received wisdom is that the more layers a network has, the better
    it is at extracting more complex features from an image, and thus learning from
    complex data. The limiting factors here, however, are the size of the input and
    how localised the features we are interested in are, as well as the amount of
    available training data. After a given number of layers, a network could start
    overfitting to the data by focusing on the irregularities in the images. So, while
    increasing the number of convolutional layers improves the performance of the
    network, it is not the case that the deeper network is always the best option.
    Part of the challenge is finding the ideal depth for a network to optimally predict
    the output. Simonyan et al. introduced VGGNet (Simonyan and Zisserman ([2015](#bib.bib31)))
    and investigated the effects of increasing the depth of convolutional layers on
    the model’s classification and localisation accuracy in the large-scale image
    recognition setting (ILSVRC challenge). VGGNet has a compelling simplicity in
    its architecture; it uses a stack of consecutive convolutional layers to reduce
    the number of parameters, leading to faster convergence and reducing the overfitting
    problem. GoogLeNet (Szegedy et al. ([2015](#bib.bib33))), popularly known as Inception,
    introduced the idea of modifying the width of a network as well as employing filters
    of a variety of sizes to better capture multi-scale data from images. Relative
    to the VGGNet architecture, it is able to reduce the number of parameters and
    computational cost further. Residual networks (He et al. ([2016](#bib.bib13))),
    or ResNets, were introduced to address the vanishing gradient problem as networks
    go deeper by presenting an alternative pathway for the algorithm to follow, called
    the skip connection. The central element in this architecture is the residual
    block, which consists of two convolutional layers with 3x3 filters. The input
    of this is added to the output of the second convolution, thus creating a shortcut
    connection.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，层中的滤波器数量随着网络的深度增加。普遍的观点是，网络层数越多，从图像中提取更复杂特征的能力越强，从而能从复杂数据中学习。然而，限制因素在于输入的大小、我们感兴趣的特征的局部化程度以及可用的训练数据量。在达到一定层数后，网络可能会因专注于图像中的不规则性而开始过拟合。因此，虽然增加卷积层的数量可以提高网络的性能，但并不是说更深的网络总是最佳选择。部分挑战在于找到网络的理想深度，以便最优化地预测输出。Simonyan
    等人介绍了 VGGNet（Simonyan 和 Zisserman ([2015](#bib.bib31)))，并研究了在大规模图像识别设置（ILSVRC
    挑战）中，增加卷积层深度对模型分类和定位准确性的影响。VGGNet 的架构简洁明了；它使用一系列连续的卷积层来减少参数数量，从而加快收敛速度并减少过拟合问题。GoogLeNet（Szegedy
    等人 ([2015](#bib.bib33)))，通常称为 Inception，引入了修改网络宽度的思想，并采用多种尺寸的滤波器来更好地捕捉图像中的多尺度数据。与
    VGGNet 架构相比，它能够进一步减少参数数量和计算成本。残差网络（He 等人 ([2016](#bib.bib13)))，或称 ResNets，旨在解决网络深度增加时的梯度消失问题，通过引入一种称为跳跃连接的替代路径来解决。该架构的核心元素是残差块，由两个带有
    3x3 滤波器的卷积层组成。输入被添加到第二个卷积的输出，从而创建了一个捷径连接。
- en: In our work, a variety of CNN classification architectures were trialled to
    determine the best combination of filters and layers to suit our purpose. After
    considerable experimentation with well-established architectures such as VGG-16
    and VGG-19 (Simonyan and Zisserman ([2015](#bib.bib31))), Inception (Szegedy et al.
    ([2015](#bib.bib33))), and ResNet50 (He et al. ([2016](#bib.bib13))), various
    custom models were constructed to determine if better performance could be obtained.
    Table [1](#S4.T1 "Table 1 ‣ 4.1 Classification ‣ 4 Deep Learning Framework ‣ Towards
    Asteroid Detection in Microlensing Surveys with Deep Learning") describes the
    structure of five of these custom architectures, three of which are VGG-like (MOA-12,
    MOA-14, MOA-15), and two are composed of hybrid Inception-ResNet modules (Hybrid
    A, Hybrid B). All of the custom architectures have significantly fewer parameters
    than their established counterparts. It is our finding that having an over-parameterised
    deep network architecture causes over-fitting of the training data, and so we
    reduced the number of layers and filters in the network to reduce the number of
    neurons and weights involved.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b67be68082485920d142533b6535ee80.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Hybrid module combining salient features of a ResNet block and an
    Inception module'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/9bfb7dce8bab29442f6d9cc64a6c6711.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'Table 1: Configuration of custom classification architectures MOA-12, MOA-14,
    MOA-15, Hybrid A, and Hybrid B. ReLU activation is used in each layer and dropout
    is used after each fully connected layer.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: The hybrid module (Figure [9](#S4.F9 "Figure 9 ‣ 4.1 Classification ‣ 4 Deep
    Learning Framework ‣ Towards Asteroid Detection in Microlensing Surveys with Deep
    Learning")) consists of four branches composed of convolutional layers, each of
    which use the ReLU activation (Glorot et al. ([2011](#bib.bib10)); Nair and Hinton
    ([2010](#bib.bib23))) and are combined with the ’add’ function. The final branch
    carries the input to the ’add’ function, with the convolution being used only
    when the number of filters in the previous layer are not equal to the current
    layer’s filters. The many 1x1 convolutions in the hybrid module may seem superfluous
    but removing even one negatively affects the performance of the network. This
    may be because of the beneficial complexity introduced by the associated non-linearity.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: All of the classification architectures were trained on a Linux machine running
    Ubuntu 18.04 with a NVIDIA Quadro M4000 GPU (8 GB, 2.5 TFLOPS). The code was written
    in Python 3.6 in the Jupyter Notebook environment. TensorFlow GPU 2.4.1 (CUDA
    11.0), along with the Keras deep learning API, was used for creating the custom
    CNN models tested. The Keras Applications implementations were used for the established
    models. A batch size of 32 was used for training each network model. Training
    was set to run for 50 epochs, with a callback for early stopping if the validation
    loss failed to minimise after a set number of epochs. The Adam (Kingma and Welling
    ([2014](#bib.bib17))) optimiser was used and the loss function minimised was binary
    cross entropy. The learning rate was initialised at 0.0001 for all networks except
    Hybrid A, which started with a learning rate of 0.001\. In each case, the learning
    rate was reduced after 15 epochs and at scheduled intervals after that point.
    Callbacks were included to save the best weights for both validation and training
    accuracy. The established networks performed better when pre-loaded with ImageNet
    weights before fine-tuning with the MOA-II data. MOA-12, 14, and 15 took on average
    3 hours to train and the Hybrid models, 6 hours.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所有分类架构都在运行Ubuntu 18.04的Linux机器上进行训练，配备了NVIDIA Quadro M4000 GPU（8 GB，2.5 TFLOPS）。代码在Python
    3.6的Jupyter Notebook环境中编写。使用了TensorFlow GPU 2.4.1（CUDA 11.0）以及Keras深度学习API来创建测试的自定义CNN模型。Keras
    Applications实现用于已建立的模型。训练每个网络模型时使用了32的批量大小。训练设置为运行50个周期，如果在设置的周期数后验证损失未能最小化，则进行早期停止的回调。使用了Adam（Kingma和Welling
    ([2014](#bib.bib17)))优化器，最小化的损失函数是二元交叉熵。所有网络的学习率初始化为0.0001，除Hybrid A外，Hybrid A的学习率起始为0.001。在每种情况下，学习率在15个周期后减少，并在此之后按计划的间隔进行调整。回调包括保存最佳的验证和训练准确性权重。建立的网络在使用ImageNet权重预加载后表现更好，然后再用MOA-II数据进行微调。MOA-12、14和15平均需要3小时进行训练，而Hybrid模型则需要6小时。
- en: 4.2 Object Detection
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 目标检测
- en: '![Refer to caption](img/c6695bcca0f05bb83fe9e32bd47c9cb0.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c6695bcca0f05bb83fe9e32bd47c9cb0.png)'
- en: 'Figure 10: Architecture of YOLOv4: a feature extractor that is composed of
    a 53-layer DenseNet with cross-stage partial connections and spacial pyramid pooling
    along with a feature aggregator that effectively combines the feature maps from
    the higher and lower resolution layers.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：YOLOv4架构：一个特征提取器，由53层的DenseNet组成，具有跨阶段部分连接和空间金字塔池化，以及一个特征聚合器，有效地结合了来自高分辨率和低分辨率层的特征图。
- en: In recent years, CNNs have been taken beyond classifying images and have been
    applied to object detection and localization in images. Here, we utilize the YOLOv4
    object detection architecture for localizing asteroid tracklets in our tile images.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，卷积神经网络（CNNs）已经超越了图像分类，应用于图像中的目标检测和定位。在这里，我们利用YOLOv4目标检测架构来定位我们的瓦片图像中的小行星轨迹。
- en: The aim of the original YOLO (You Only Look Once) object detection architecture
    (Redmon et al. ([2016](#bib.bib27))) was to make object detection both fast and
    accessible. As a single-shot architecture, it was capable of making class and
    bounding box predictions with the feature maps produced by a single CNN network.
    The simplicity of the feature extraction CNN at the heart of YOLO was in direct
    contrast to its complex loss function, which computed the classification loss,
    localization loss, as well as the loss quantifying the network’s confidence in
    the prediction. Later versions of the network introduced default anchor boxes,
    which are predefined bounding boxes that are adjusted and refined during training
    to encompass the objects of interest in an image(Redmon and Farhadi ([2017](#bib.bib28))),
    and more complex feature extractors to make predictions on multiple scales (Redmon
    and Farhadi ([2018](#bib.bib29))).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 原始YOLO（You Only Look Once）目标检测架构（Redmon等 ([2016](#bib.bib27))) 的目的是使目标检测既快速又易于实现。作为一个单次检测架构，它能够利用单个CNN网络生成的特征图进行类别和边界框预测。YOLO核心中的特征提取CNN的简单性与其复杂的损失函数形成鲜明对比，该损失函数计算分类损失、定位损失以及量化网络对预测信心的损失。网络的后续版本引入了默认的锚框，即在训练过程中调整和细化的预定义边界框，以涵盖图像中的感兴趣对象（Redmon和Farhadi
    ([2017](#bib.bib28)))，并使用更复杂的特征提取器进行多尺度预测（Redmon和Farhadi ([2018](#bib.bib29)))。
- en: This research applied YOLOv4 (Bochkovskiy et al. ([2020](#bib.bib3))), which
    is the latest evolution of the architecture with a major overhaul to include several
    new techniques, making the model state-of-the-art while still being easy to train.
    In particular, YOLOv4 ensures that the lower level features are propagated through
    both the feature extractor as well as the feature aggregator. Figure [10](#S4.F10
    "Figure 10 ‣ 4.2 Object Detection ‣ 4 Deep Learning Framework ‣ Towards Asteroid
    Detection in Microlensing Surveys with Deep Learning") illustrates the architecture
    of YOLOv4\. The convolutional backbone feature extractor for the architecture
    is composed of a 53 layer DenseNet (Huang et al. ([2016](#bib.bib15))) with the
    cross-stage-partial (CSP) connections of CSPNet (Wang et al. ([2020](#bib.bib38))).
    DenseNets extend ResNet’s concept of skip connections by adding connections between
    all the layers in the network in a feed-forwards fashion. Feature maps from all
    preceding layers are concatenated and form the input for any given layer, ensuring
    that low-level features are propagated through the network. CSP connections involve
    splitting the input feature map into two parts, one of which goes through the
    dense block and the other goes straight through to the next transitional step.
    Additionally, the network includes spatial pyramid pooling (SPP) (He et al. ([2014](#bib.bib12)))
    after the last convolutional layer. This has the effect of separating out the
    most important features and increasing the receptive field. The final feature
    map is divided into $m\ \times\ m$ bins, following which maxpooling is applied
    to each bin. The resulting feature maps are concatenated and represent the output
    of the feature extractor.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究采用了YOLOv4（Bochkovskiy等人（[2020](#bib.bib3)）），这是该架构的最新演变，进行了重大改造，包含了多种新技术，使模型成为最先进的，同时仍然易于训练。特别是，YOLOv4确保低级特征通过特征提取器和特征聚合器进行传播。图[10](#S4.F10
    "图 10 ‣ 4.2 对象检测 ‣ 4 深度学习框架 ‣ 利用深度学习在微引力透镜调查中进行小行星检测")展示了YOLOv4的架构。该架构的卷积骨干特征提取器由53层DenseNet（Huang等人（[2016](#bib.bib15)））组成，并具有CSPNet（Wang等人（[2020](#bib.bib38)））的跨阶段部分（CSP）连接。DenseNet通过在网络中以前馈的方式添加所有层之间的连接，扩展了ResNet的跳跃连接概念。所有前面层的特征图被连接起来，并形成任何给定层的输入，确保低级特征在网络中得以传播。CSP连接涉及将输入特征图分成两部分，一部分经过密集块，另一部分直接传递到下一个过渡步骤。此外，网络在最后的卷积层之后包含空间金字塔池化（SPP）（He等人（[2014](#bib.bib12)））。这会将最重要的特征分离出来，并增加接收字段。最终的特征图被划分为
    $m\ \times\ m$ 个箱子，然后对每个箱子应用最大池化。得到的特征图被连接起来，表示特征提取器的输出。
- en: 'CNNs naturally attain a pyramid-like structure with each layer as the image
    goes from high to low resolution. As we get deeper in a CNN, we lose the fine-grained
    details of the input, which usually makes it harder to detect small objects. As
    the resolution lowers, however, the filters learn ever more complex abstractions
    about the image, making the feature maps more semantically rich. Therefore, is
    it desirable to combine the feature maps from the higher resolution layer with
    the more semantically rich ones to facilitate detecting objects at multiple scales.
    This task falls to a feature aggregator and YOLOv4 uses the approach suggested
    by PANet (Liu et al. ([2018](#bib.bib21))). The feature maps are concatenated
    from both the top-down and the bottom-up path, ensuring the propagation of semantically
    rich localization information through to the final part of the network where the
    class probability and bounding box predictions are made. Each predicted bounding
    box consists of five elements: centre-x, centre-y, width, height, and confidence.
    The ($centre-x,centre-y)$ coordinates are relative to the dimensions of the predicted
    box; the width and height are relative to the whole image. The confidence score
    represents the likelihood that the cell contains the object as well as how confident
    the model is about its predictions.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'YOLOv4 also updates the loss function to include Complete Intersection over
    Union (CIoU) loss (Zheng et al. ([2020](#bib.bib40))) to train the network to
    effectively determine the direction in which to shift the weights to better match
    the labelled bounding boxes. Finally, the model also includes a raft of new additions:
    a new activation function, MISH (Misra ([2020](#bib.bib22))) that provides smoother
    gradients; updates to the spatial attention module (Woo et al. ([2018](#bib.bib39)))
    and multi-input weighted residual connections (Tan et al. ([2020](#bib.bib34)))
    to better suit the architecture; and new data augmentation techniques, Mosaic
    and self-adversarial training.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The YOLO family of models are supported by Darknet (Redmon ([2013](#bib.bib26))),
    a custom framework written in C and CUDA and designed for fast object detection.
    The Darknet implementation of YOLOv4 was trained with the MOA-II dataset via the
    Google Colaboratory with a hosted GPU runtime environment. The model was first
    trained with the default anchor boxes, before k-means clustering was used to discover
    anchor boxes that might prove better suited to finding tracklets in astronomical
    data. After several variations were tested, the best combination of anchor boxes
    was discovered by hand-engineering the various clusters. The learning rate of
    0.001 was used along with a batch size of 32, with 8 subdivisions, 6000 max_batches,
    and with steps set to 4800,5400.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate the classification networks, we rely on the metrics derived from
    a confusion matrix. A confusion matrix breaks down the predictions made by a classifier
    into 4 outcomes: True Positive (TP), True Negative (TN), False Positive (FP),
    and False Negative (FN). The ”positive” cases are where images were classified
    as containing tracklets and the ”negative” cases are where no tracklets were detected.
    In this case, the ideal scenarios would be to have as few false negatives as possible,
    together with a manageable number of false positives. Here, the networks are evaluated
    based on their recall, F2 score (the weighted harmonic mean of the precision and
    recall), and the PR AUC (area under the precision-recall curve).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/459c90a7913bf09bc63575d30f43a4c6.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: 'Table 2: Evaluation metrics for the GB5-R5 test set taken at probability threshold
    0.5'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation metrics for each classifier for the GB5-R5 test set are in Table
    [2](#S5.T2 "Table 2 ‣ 5 Results ‣ Towards Asteroid Detection in Microlensing Surveys
    with Deep Learning") and for the GB-All (28-06-2013) test set are in Table [3](#S5.T3
    "Table 3 ‣ 5 Results ‣ Towards Asteroid Detection in Microlensing Surveys with
    Deep Learning"). All of the metrics were taken at the 0.5 confidence threshold,
    with values over 0.5 indicating the presence of an asteroid tracklet in the image.
    Reviewing a combination of the PR AUC, F2 Score, and Recall, we can see the custom
    networks generalised well when making prediction about data from fields and chips
    they had never seen. Rather than selecting a single network from among these,
    all five custom classifiers were configured as an ensemble. Each network makes
    predictions about an input image and two approaches were trialled for selecting
    the winning prediction: averaging the predictions from all five classifiers (avg)
    or selecting the highest predicted value (max).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/89908d5ea782e26eec32b96e1da28468.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: 'Table 3: Evaluation metrics for the GB-ALL test set taken at probability threshold
    0.5'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: We see that while the max-ensemble results in a greater number of false positives,
    it improves the recall by four points (Table [4](#S5.T4 "Table 4 ‣ 5 Results ‣
    Towards Asteroid Detection in Microlensing Surveys with Deep Learning")) to 94.33%
    and 97.67% for the GB5-R5 and the GB-All test set, respectively. The avg-ensemble
    has the advantage of having far fewer false positives and a judiciously selected
    prediction threshold could see false negatives further minimised for this configuration.
    The ROC curves for the ensemble (Figure [11](#S5.F11 "Figure 11 ‣ 5 Results ‣
    Towards Asteroid Detection in Microlensing Surveys with Deep Learning")) further
    illustrate this trade-off.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![[Uncaptioned image]](img/d16c08743c126d60fa09a195305c17f9.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: 'Table 4: Evaluation metrics after configuring the five custom networks as an
    ensemble taken at probability threshold 0.5'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d76ff2513a873d9c757395962222022.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: ROC curves for the CNN ensemble'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a9df04d2adad205689a389bf4a19e9dc.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Tracklets localisation by YOLOv4, From left to right, these are:
    (538147)2016 BT90 (18.7); (67001) 1999 XN117 (19.2); (80667) 2000 BA15 (18.8);
    (206629) 2003 WT154 (19.0); and (281890) 2010 OA74 (20.1). Note that these images
    have been inverted and brightened to improve visibility (original in Figure [17](#A1.F17
    "Figure 17 ‣ Appendix A Original images ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning")).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'The performance of object detection models is typically quantified by the mean
    Average Precision or mAP. The average precision (AP) measures the trade-off between
    precision and recall and is calculated by integrating the area that falls under
    the precision-recall curve for each unique values of recall where the precision
    value decreases. The mAP is the mean of the AP across all object classes that
    the model can detect, which is identical to the AP for this dataset. Object detection
    models aim to have a high overlap between the predicted and the ground truth bounding
    boxes (Intersection of Union or IoU) and predictions are grouped as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'True Positive: IoU $>$ 0.5'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'False Positive: IoU $<$ 0.5 (or a duplicate)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'False Negative: box not detected or the IoU $>$ 0.5 but the object is classified
    wrong'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The average precision is calculated as: Calculated as:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $AP=\Sigma(r_{(n+1)}-r_{n})\tilde{p}(r_{(n+1)})$ |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
- en: '|  | $\tilde{p}(r_{(n+1)})=\max_{\tilde{r}\geq r_{(n+1)}}(p(\tilde{r})),$ |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
- en: where $r$ is the recall value, $n$ represents the locations where the precision
    decreases, and $\tilde{p}(r_{(n+1)})$ is the maximum precision where the $r$ value
    changes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv4 achieved an mAP of 90.96% with default anchor boxes and 90.95% with custom
    anchor boxes on the GB5-R5 data. Neither network could detect tracklets in the
    GB-All data, indicating that YOLOv4 will need to be trained with labelled data
    from other fields and chips before it can be used to make detections in them.
    Some of the detections made by YOLOv4 on the GB5-R5 dataset can be seen in Figure
    [12](#S5.F12 "Figure 12 ‣ 5 Results ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning").
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The proposed model was applied towards discovering tracklets in the 543,595
    images tiles from GB5-R5 that had no known asteroid tracklets. The ensemble could
    analyse 10,000 images in an hour and took three days to analyse all the images
    and proposed 50,227 candidate detections. Both version of YOLOv4 were then used
    for localizing tracklets in these candidate detections, with each version taking
    1.5 days to make bounding box predictions. We are still analysing these candidate
    detections to determine if any new objects are present.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our classifier ensemble, trained with a relatively small amount of labelled
    data from just one chip in one survey field, was able to classify a set of images
    from unseen survey fields and chips, highlighting that it generalizes well. The
    success of the custom CNN architectures with fewer training parameters is perhaps
    down to two aspects. First, the networks might be at the optimal depth for learning
    to identify the pattern of fuzzy blobs that represent a tracklet. Second, the
    small size of the input images perhaps leads to more overfitting in the larger
    models. Further, rather than adding more layers, the hybrid architectures lean
    into adding complexity with several 1x1 convolutions. It is possible this caused
    the networks to learn representations that ultimately boosted the performance
    of the ensemble.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: The false negatives (Figure [14](#S6.F14 "Figure 14 ‣ 6 Discussion ‣ Towards
    Asteroid Detection in Microlensing Surveys with Deep Learning")) reported by the
    ensemble include instances where the tracklets resemble satellite streaks or where
    the point sources are slightly further apart and obscured by noise. In some cases,
    parts of the tracklet are on other images that have been correctly identified
    as a candidate detection by the ensemble. As to the false positives, they are
    largely cases where noise and other spurious artefacts have been mistaken for
    tracklets, as indicated in Figure [13](#S6.F13 "Figure 13 ‣ 6 Discussion ‣ Towards
    Asteroid Detection in Microlensing Surveys with Deep Learning"). These also lead
    to false detections by the object detector. We believe that fine tuning the network
    models as more labelled data becomes available will aid in minimising misclassifications
    such as these.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The YOLOv4 object detector proved to be up to the task of localizing tracklets
    in the GB5-R5 dataset but does not generalize to unseen data. The results from
    the GB5-R5 set are promising and thus indicate that further training with data
    from the other survey fields and chips will be beneficial.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: While both types of networks are currently limited by the high cadence observations
    they have been trained with, we are confident that, as the networks are retrained
    with data from other fields, they will learn to identity tracklets of slow objects
    or those with longer intervals between observations. Further, other surveys could
    use our networks, together with their pre-trained weights, as a starting point
    for discovering asteroids in their data. In this case, we recommend that the other
    surveys retrain/ fine-tune the architectures with their own data for optimal results.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b0fe4be706c948b316f32dad3ca56d13.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The false positives reported by the classification networks fall
    into four broad categories: (a) ghosts (short for ghosts of noise past), are the
    most common false positives and are a result of the additive noise from creating
    the composite images; (b) streaks are either satellites or other near-Earth objects
    or cosmic rays; (c) chimera are optical artefacts potentially caused by over-saturated
    stars; and (d) loki objects are artefacts that move around erratically from one
    observation to the next. Note that these images have been inverted and brightened
    to improve visibility.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/736b852c0aec818eb5f9e255b5994641.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The false negatives reported by the classification networks are
    mostly either faint objects or too much like streaks. It is likely that these
    sorts of tracklets were not well represented in the training set. Note that these
    images have been inverted and brightened to improve visibility.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have shown that it is possible to train both CNN-based classifiers as well
    as the YOLOv4 object detector to find asteroid tracklets in the MOA difference
    images. The classifier ensemble proved resilient to discovering tracklets in unseen
    data and will be invaluable for extending the search for asteroids to the rest
    of the MOA-II archival data. The networks will be fine-tuned or retrained as more
    labelled data is available and we will investigate automating the bounding boxes
    required for training YOLOv4\. We will also investigate working with orbit-linking
    software such as HelioLinC to determine the validity of the source clusters localized
    by YOLOv4.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: While the classifiers performed well, there is potential for further improvement
    and development. Investigating effective denoising techniques for the stacked
    images or the difference images would lead to an immediate performance boost for
    both the classification and object detection networks. The classification network
    may benefit from having two inputs - perhaps the subtracted stack image along
    with the median stack image. This would provide the model with additional information
    it could use to distinguish between images with or without tracklets. Training
    the YOLO backbone feature extractor with the classification data first might also
    lead to better results. Additionally, since much of the salient information is
    contained in the first layer, a small CNN based on DenseNet could also potentially
    be successfully trained as a classifier.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we have presented an effective toolkit for finding asteroids tracklets
    in the archival data of ground-based telescopes. The code for our neural network
    models as well as the trained weights are available at https://github.com/pcowan-astro/MOA-Asteroids.
    Our methodology and network architectures can be used to discover and recover
    asteroids in other archival survey data as well as to strengthen the analysis
    pipeline for current and future surveys.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 8 Acknowledgements
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank the MOA collaboration for use of the MOA-II archival difference images.
    IAB acknowledges support from grant MAU1901 from the Royal Society of New Zealand
    - Marsden. This research has made use of data and/or services provided by the
    International Astronomical Union’s Minor Planet Center.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Original images
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ecac62755887456510bb088ffddefc61.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: A stack of all 51 observations from the night of 15-May-2008 reveals
    5 asteroid tracklets clearly visible in a sub-region of the stack image. Numbered
    in order of appearance, these are: 1 - (103842) 2000 DQ33 (19.5); 2 - (148657)
    2001 SX124 (19.6); 3 - (152083) 2004 RH30 (19.9); 4 - (582743) 2016 AT221 (20.4);
    5 - (338789) 2005 SZ154 (20.4). The line/streak on the top left is from a satellite.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ae9ddd1faa1837817b184ebb866011a.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Asteroid tracklets seen in 128x128 sub-regions from subtracted stack
    images. In (b), the tracklets are localized with bounding boxes. Asteroids in
    the top row, left to right: (38102) 1999 JM18 (18.4); (152083) 2004 RH30 (19.9);
    (103842) 2000 DQ33 (19.5); (283261) 2011 FR142 (20.0); and (375674) 2009 HD5 (19.8).
    Asteroids in the bottom row, from left to right: (48617) 1995 HR2 (19.5); (148657)
    2001 SX124 (19.6); (97948) 2000 QF124 (19.6); and (74978) 1999 TY234 (18.7).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/972c331f153ab278cc069da2e4caaf24.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Tracklets localisation by YOLOv4, From left to right, these are:
    (538147)2016 BT90 (18.7); (67001) 1999 XN117 (19.2); (80667) 2000 BA15 (18.8);
    (206629) 2003 WT154 (19.0); and (281890) 2010 OA74 (20.1).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alard (2000) Alard, C., 2000. Image subtraction using a space-varying kernel.
    Astronomy and Astrophysics Supplement 144, 363–370.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alard and Lupton (1998) Alard, C., Lupton, R.H., 1998. A Method for Optimal
    Image Subtraction. The Astrophysical Journal 503, 325–331.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bochkovskiy et al. (2020) Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M., 2020.
    YOLOv4: Optimal Speed and Accuracy of Object Detection. ArXiv abs/2004.10934.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bond et al. (2001) Bond, I., Abe, F., Dodd, R., Hearnshaw, J., Honda, M., Jugaku,
    J., Kilmartin, P., Marles, A., Masuda, K., Matsubara, Y., Muraki, Y., Nakamura,
    T., Nankivell, G., Noda, S., Noguchi, C., Ohnishi, K., Rattenbury, N., Reid, M.,
    Saito, T., Sato, H., Sekiguchi, M., Skuljan, J., Sullivan, D., Sumi, T., Takeuti,
    M., Watase, Y., Wilkinson, S., Yamada, R., Yanagisawa, T., Yock, P., 2001. Real-time
    difference imaging analysis of MOA Galactic bulge observations during 2000. Monthly
    Notices of the Royal Astronomical Society 327, 868–880.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bramich (2008) Bramich, D., 2008. A new algorithm for difference image analysis.
    Monthly Notices of the Royal Astronomical Society 386, L77–L81.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cordwell et al. (2022) Cordwell, A.J., Rattenbury, N.J., Bannister, M.T., Cowan,
    P., Collaboration:, T.M., Abe, F., Barry, R., Bennett, D.P., Bhattacharya, A.,
    Bond, I.A., Fujii, H., Fukui, A., Itow, Y., Silva, S.I., Hirao, Y., Kirikawa,
    R., Kondo, I., Koshimoto, N., Matsubara, Y., Matsumoto, S., Muraki, Y., Miyazaki,
    S., Okamura, A., Ranc, C., Satoh, Y., Sumi, T., Suzuki, D., Tristram, P.J., Toda,
    T., Yama, H., Yonehara, A., 2022. Asteroid Lightcurves from the MOA-II Survey:
    a pilot study. Monthly Notices of the Royal Astronomical Society 514, 3098–3112.
    doi:[10.1093/mnras/stac674](http://dx.doi.org/10.1093/mnras/stac674).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denneau et al. (2013) Denneau, L., Jedicke, R., Grav, T., Granvik, M., Kubica,
    J., Milani, A., Vereš, P., Wainscoat, R., Chang, D., Pierfederici, F., Kaiser,
    N., Chambers, K.C., Heasley, J.N., Magnier, E.A., Price, P.A., Myers, J., Kleyna,
    J., Hsieh, H., Farnocchia, D., Waters, C., Sweeney, W.H., Green, D., Bolin, B.,
    Burgett, W.S., Morgan, J.S., Tonry, J.L., Hodapp, K.W., Chastel, S., Chesley,
    S., Fitzsimmons, A., Holman, M., Spahr, T., Tholen, D., Williams, G.V., Abe, S.,
    Armstrong, J.D., Bressi, T.H., Holmes, R., Lister, T., McMillan, R.S., Micheli,
    M., Ryan, E.V., Ryan, W.H., Scotti, J.V., 2013. The Pan-STARRS Moving Object Processing
    System. Publications of the Astronomical Society of the Pacific 125, 357–395.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duev et al. (2021) Duev, D.A., Bolin, B.T., Graham, M.J., Kelley, M.S.P., Mahabal,
    A., Bellm, E.C., Coughlin, M.W., Dekany, R., Helou, G., Kulkarni, S.R., Masci,
    F.J., Prince, T.A., Riddle, R., Soumagnac, M.T., van der Walt, S.J., 2021. Tails:
    Chasing Comets with the Zwicky Transient Facility and Deep Learning. The Astronomical
    Journal 161, 218.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duev et al. (2019) Duev, D.A., Mahabal, A., Ye, Q., Tirumala, K., Belicki,
    J., Dekany, R., Frederick, S., Graham, M.J., Laher, R.R., Masci, F.J., Prince,
    T.A., Riddle, R., Rosnet, P., Soumagnac, M.T., 2019. DeepStreaks: Identifying
    fast-moving objects in the Zwicky Transient Facility data with deep learning.
    Monthly Notices of the Royal Astronomical Society , 4158–4165.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glorot et al. (2011) Glorot, X., Bordes, A., Bengio, Y., 2011. Deep Sparse Rectifier
    Neural Networks. AISTATS .
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gould and Yee (2013) Gould, A., Yee, J.C., 2013. Microlens surveys are a powerful
    probe of asteroids. Astrophysical Journal 767.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2014) He, K., Zhang, X., Ren, S., Sun, J., 2014. Spatial Pyramid
    Pooling in Deep Convolutional Networks for Visual Recognition. Lecture Notes in
    Computer Science (including subseries Lecture Notes in Artificial Intelligence
    and Lecture Notes in Bioinformatics) 8691 LNCS, 346–361.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep Residual Learning
    for Image Recognition. IEEE Conference on Computer Vision and Pattern Recognition
    , 770–778.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Holman et al. (2018) Holman, M., Payne, M., Blankley, P., Janssen, R., Kuindersma,
    S., 2018. Heliolinc: A novel approach to the minor planet linking problem. The
    Astronomical Journal 156, 135. doi:[10.3847/1538-3881/aad69a](http://dx.doi.org/10.3847/1538-3881/aad69a).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2016) Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.,
    2016. Densely Connected Convolutional Networks. Proceedings - 30th IEEE Conference
    on Computer Vision and Pattern Recognition, CVPR 2017 2017-January, 2261–2269.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2016) Kim, S.L., Lee, C.U., Park, B.G., Kim, D.J., Cha, S.M., Lee,
    Y., Han, C., Chun, M.Y., Yuk, I., 2016. KMTNET: A Network of 1.6 m Wide-Field
    Optical Telescopes Installed at Three Southern Observatories. Journal of Korean
    Astronomical Society 49, 37–44. doi:[10.5303/JKAS.2016.49.1.037](http://dx.doi.org/10.5303/JKAS.2016.49.1.037).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Welling (2014) Kingma, D.P., Welling, M., 2014. Auto-encoding variational
    bayes, in: 2nd International Conference on Learning Representations, ICLR 2014
    - Conference Track Proceedings, International Conference on Learning Representations,
    ICLR.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kruk et al. (2022) Kruk, S., García Martín, P., Popescu, M., Merín, B., Mahlke,
    M., Carry, B., Thomson, R., Karadağ, S., Durán, J., Racero, E., Giordano, F.,
    Baines, D., de Marchi, G., Laureijs, R., 2022. Hubble Asteroid Hunter. Astronomy
    & Astrophysics 661, A85.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubica et al. (2007) Kubica, J., Denneau Jr, L., Moore, A., Jedicke, Robert,
    Connolly, A., 2007. Efficient Algorithms for Large-Scale Asteroid Discovery. Astronomical
    Data Analysis Software and Systems XVI ASP Conference Series 376, 395–404.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lieu et al. (2018) Lieu, M., Conversi, L., Altieri, B., Carry, B., 2018. Detecting
    solar system objects with convolutional neural networks. Monthly Notices of the
    Royal Astronomical Society 485, 5831–5842.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018) Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., 2018. Path Aggregation
    Network for Instance Segmentation. Proceedings of the IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition , 8759–8768.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Misra (2020) Misra, D., 2020. Mish: A Self Regularized Non-Monotonic Activation
    Function. ArXiv abs/1908.08681.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nair and Hinton (2010) Nair, V., Hinton, G.E., 2010. Rectified Linear Units
    Improve Restricted Boltzmann Machines. ICML .
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rabeendran and Denneau (2021) Rabeendran, A.C., Denneau, L., 2021. A Two-Stage
    Deep Learning Detection Classifier for the ATLAS Asteroid Survey. Publications
    of the Astronomical Society of the Pacific 133.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rabinowitz (1991) Rabinowitz, D.L., 1991. Detection of earth-approaching asteroids
    in near real time. The Astronomical Journal 101, 1518.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon (2013) Redmon, J., 2013. Darknet: Open source neural networks in c.
    URL: [https://pjreddie.com/darknet/](https://pjreddie.com/darknet/).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon et al. (2016) Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016.
    You Only Look Once: Unified, Real-Time Object Detection, in: IEEE Conference on
    Computer Vision and Pattern Recognition (CVPR), pp. 779–788.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon and Farhadi (2017) Redmon, J., Farhadi, A., 2017. YOLO9000: Better,
    Faster, Stronger, in: IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 6517–6525.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon and Farhadi (2018) Redmon, J., Farhadi, A., 2018. YOLOv3: An Incremental
    Improvement. ArXiv abs/1804.02767.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sako et al. (2008) Sako, T., Sekiguchi, T., Sasaki, M., Okajima, K., Abe, F.,
    Bond, I.A., Hearnshaw, J.B., Itow, Y., Kamiya, K., Kilmartin, P.M., Masuda, K.,
    Matsubara, Y., Muraki, Y., Rattenbury, N.J., Sullivan, D.J., Sumi, T., Tristram,
    P., Yanagisawa, T., Yock, P.C., 2008. MOA-cam3: A wide-field mosaic CCD camera
    for a gravitational microlensing survey in New Zealand. Experimental Astronomy
    22, 51–66.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonyan and Zisserman (2015) Simonyan, K., Zisserman, A., 2015. Very deep
    convolutional networks for large-scale image recognition, in: 3rd International
    Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sumi et al. (2003) Sumi, T., Abe, F., Bond, I., Dodd, R., Hearnshaw, J., Honda,
    M., Honma, M., Kan-ya, Y., Kilmartin, P., Masuda, K., Matsubara, Y., Muraki, Y.,
    Nakamura, T., Nishi, R., Noda, S., Ohnishi, K., Petterson, O., Rattenbury, N.,
    Reid, M., Saito, T., Saito, Y., Sato, H., Sekiguchi, M., Skuljan, J., Sullivan,
    D., Takeuti, M., Tristram, P., Wilkinson, S., Yanagisawa, T., Yock, P., 2003.
    Microlensing Optical Depth toward the Galactic Bulge from Microlensing Observations
    in Astrophysics Group Observations during 2000 with Difference Image Analysis.
    The Astrophysical Journal 591, 204–227.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna,
    Z., 2015. Rethinking the Inception Architecture for Computer Vision. Proceedings
    of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition
    2016-Decem, 2818–2826.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. (2020) Tan, M., Pang, R., Le, Q.V., 2020. EfficientDet: Scalable
    and efficient object detection, in: Proceedings of the IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition, pp. 10778–10787.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tomaney and Crotts (1996) Tomaney, A.B., Crotts, A.P., 1996. Expanding the Realm
    of Microlensing Surveys with Difference Image Photometry. Astronomical Journal
    112, 2872.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Udalski et al. (1993) Udalski, A., Szymanski, M., Kaluzny, J., Kubiak, M., Krzeminski,
    W., Mateo, M., Preston, G.W., Paczynski, B., 1993. The Optical Gravitational Lensing
    Experiment. Discovery of the First Candidate Microlensing Event in the Direction
    of the Galactic Bulge. Acta Astronomica 43, 289–294.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'W. M. Newman and R. F. Sproull (1973) W. M. Newman and R. F. Sproull, 1973.
    Cohen-Sutherland Algorithm, in: Principles of Interactive Computer Graphics. internatio
    ed.. McGraw–Hill Education, pp. 124, 252.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Wang, C.Y., Mark Liao, H.Y., Wu, Y.H., Chen, P.Y., Hsieh,
    J.W., Yeh, I.H., 2020. CSPNet: A new backbone that can enhance learning capability
    of CNN, in: IEEE Computer Society Conference on Computer Vision and Pattern Recognition
    Workshops, pp. 1571–1580.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Woo et al. (2018) Woo, S., Park, J., Lee, J.Y., Kweon, I.S., 2018. CBAM: Convolutional
    Block Attention Module. Lecture Notes in Computer Science (including subseries
    Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
    11211 LNCS, 3–19.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2020) Zheng, Z., Wang, P., Ren, D., Liu, W., Ye, R., Hu, Q., Zuo,
    W., 2020. Enhancing Geometric Factors in Model Learning and Inference for Object
    Detection and Instance Segmentation. IEEE Transactions on Cybernetics .
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zoghbi et al. (2017) Zoghbi, S., Cicco, M.D., Ordonez, A.J., Stapper, A.P.,
    Collison, J., Gural, P.S., Ganju, S., Galache, J.l., Jenniskens, P., 2017. Searching
    for Long-Period Comets with Deep Learning Tools, in: Workshop on Deep Learning
    for Physical Sciences.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
