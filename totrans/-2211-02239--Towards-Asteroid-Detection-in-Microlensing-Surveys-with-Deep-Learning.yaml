- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:43:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:43:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2211.02239] Towards Asteroid Detection in Microlensing Surveys with Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2211.02239] 基于深度学习的微引力透镜调查中的小行星检测'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2211.02239](https://ar5iv.labs.arxiv.org/html/2211.02239)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2211.02239](https://ar5iv.labs.arxiv.org/html/2211.02239)
- en: Towards Asteroid Detection in Microlensing Surveys with Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的微引力透镜调查中的小行星检测
- en: Preeti Cowan Ian A. Bond Napoleon H. Reyes
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Preeti Cowan Ian A. Bond Napoleon H. Reyes
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Asteroids are an indelible part of most astronomical surveys though only a few
    surveys are dedicated to their detection. Over the years, high cadence microlensing
    surveys have amassed several terabytes of data while scanning primarily the Galactic
    Bulge and Magellanic Clouds for microlensing events and thus provide a treasure
    trove of opportunities for scientific data mining. In particular, numerous asteroids
    have been observed by visual inspection of selected images. This paper presents
    novel deep learning-based solutions for the recovery and discovery of asteroids
    in the microlensing data gathered by the MOA project. Asteroid tracklets can be
    clearly seen by combining all the observations on a given night and these tracklets
    inform the structure of the dataset. Known asteroids were identified within these
    composite images and used for creating the labelled datasets required for supervised
    learning. Several custom CNN models were developed to identify images with asteroid
    tracklets. Model ensembling was then employed to reduce the variance in the predictions
    as well as to improve the generalisation error, achieving a recall of 97.67%.
    Furthermore, the YOLOv4 object detector was trained to localize asteroid tracklets,
    achieving a mean Average Precision (mAP) of 90.97%. These trained networks will
    be applied to 16 years of MOA archival data to find both known and unknown asteroids
    that have been observed by the survey over the years. The methodologies developed
    can be adapted for use by other surveys for asteroid recovery and discovery.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 小行星是大多数天文调查中不可磨灭的一部分，尽管只有少数调查专门用于其检测。多年来，高频次的微引力透镜调查积累了数TB的数据，主要扫描银河系核球和麦哲伦云中的微引力透镜事件，因此提供了丰富的科学数据挖掘机会。特别是，通过对选定图像的视觉检查，观察到了许多小行星。本文提出了基于深度学习的创新解决方案，用于恢复和发现MOA项目中收集的微引力透镜数据中的小行星。通过将特定夜晚的所有观测数据结合在一起，小行星轨迹可以清晰可见，这些轨迹揭示了数据集的结构。在这些复合图像中识别出已知的小行星，并用来创建监督学习所需的标注数据集。开发了几个定制的CNN模型来识别包含小行星轨迹的图像。然后使用模型集成来减少预测的方差，并改善泛化误差，达到了97.67%的召回率。此外，YOLOv4对象检测器经过训练以定位小行星轨迹，达到了90.97%的平均精度（mAP）。这些训练后的网络将应用于16年的MOA存档数据，以寻找在这些年中被调查观察到的已知和未知的小行星。所开发的方法可以适应其他调查用于小行星的恢复和发现。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'microlensing surveys , asteroid detection , deep learning , convolutional neural
    networks , YOLOv4 , MOA^†^†journal: Astronomy and Computing\affiliation'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 微引力透镜调查，小行星检测，深度学习，卷积神经网络，YOLOv4，MOA^†^†期刊：天文学与计算\affiliation
- en: '[inst1]organization=School of Mathematical and Computational Sciences, Massey
    University,addressline=Private Bag 102-904 North Shore Mail Centre, city=Auckland,
    postcode=0745, country=New Zealand'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[inst1]组织=数学与计算科学学院，梅西大学，地址=Private Bag 102-904 North Shore Mail Centre，城市=奥克兰，邮政编码=0745，国家=新西兰'
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Asteroids are among the millions of small bodies that inhabit our Solar System
    and are remnants from its formation. While popular sentiment most commonly associates
    asteroids with mass extinction events, the vast majority of asteroids pose no
    threat to us. The Main Asteroid Belt between Mars and Jupiter has the largest
    concentration of asteroids in our Solar System and this is where the majority
    of the asteroids seen in this research reside. Observing and tracking these small
    bodies gives us a better understanding of their complex orbital dynamics. Their
    composition and structure offer clues about the conditions when the terrestrial
    planets were formed 4.6 billion years ago.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 小行星是我们太阳系中数百万个小天体中的一部分，它们是太阳系形成时的遗留物。虽然大众普遍将小行星与大规模灭绝事件联系在一起，但绝大多数小行星对我们构成不了威胁。位于火星和木星之间的小行星带是我们太阳系中小行星最密集的区域，这里也是本研究中大多数小行星的所在地。观察和跟踪这些小天体可以帮助我们更好地理解它们复杂的轨道动态。它们的组成和结构为了解46亿年前地球行星形成时的条件提供了线索。
- en: Asteroids are part of the landscape of our night sky and appear in the imaging
    data of most astronomical surveys. However, as most surveys have a specialised
    purpose, their data is rarely mined for asteroids. Microlensing surveys like MOA
    (Sumi et al. ([2003](#bib.bib32))), OGLE (Udalski et al. ([1993](#bib.bib36))),
    and KMTNet (Kim et al. ([2016](#bib.bib16))) are particularly good for determining
    the rotation period and orbital trajectory of asteroids because they survey a
    given region of space several times each night (Gould and Yee ([2013](#bib.bib11))).
    This means that asteroids could spend several nights in the field of view of the
    telescope, giving us the opportunity to both observe their trajectory and analyse
    the light gathered from them. Cordwell et al. ([2022](#bib.bib6)) demonstrates
    the efficacy of extracting asteroids light curves from the MOA microlensing data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 小行星是我们夜空景观的一部分，并且出现在大多数天文调查的影像数据中。然而，由于大多数调查有其特定的目的，它们的数据很少用于小行星的研究。像 MOA（Sumi
    et al. ([2003](#bib.bib32)))、OGLE（Udalski et al. ([1993](#bib.bib36))) 和 KMTNet（Kim
    et al. ([2016](#bib.bib16))) 这样的微引力透镜调查特别适合确定小行星的自转周期和轨道轨迹，因为它们每晚都会多次观测一个特定的空间区域（Gould
    and Yee ([2013](#bib.bib11)))。这意味着小行星可能会在望远镜的视野中停留几个夜晚，使我们有机会既观察它们的轨迹，又分析从中收集的光线。Cordwell
    et al. ([2022](#bib.bib6)) 展示了从 MOA 微引力透镜数据中提取小行星光变曲线的有效性。
- en: Automated detection software has been part of surveys dedicated to discovering
    asteroids since the early 90s (Rabinowitz ([1991](#bib.bib25))). With improved
    computing power, other techniques for detecting moving astronomical sources such
    as shift and stack have also proven popular. In recent years, a leader in the
    field is the Pan-STARRS Moving Object Processing System or MOPS (Denneau et al.
    ([2013](#bib.bib7))). Initially trained with simulated but realistic asteroid
    data for the Pan-STARRS telescopes, it takes transient candidates not associated
    with a known source and uses a complex tree-based spatial linking algorithm (Kubica
    et al. ([2007](#bib.bib19))) to further parse and form associations between these
    point sources. MOPS does not work with imaging data but rather celestial coordinates,
    which reduces the computational cost. HeliolinC (Holman et al. ([2018](#bib.bib14)))
    further improves on MOPS’ efficiency with an approach that combines working with
    a heliocentric frame of reference and clustering sources that belong to the same
    object.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自动检测软件自90年代初期以来一直是专门用于发现小行星的调查的一部分（Rabinowitz ([1991](#bib.bib25)))。随着计算能力的提高，其他检测运动天文源的技术，如平移和堆叠，也变得流行。近年来，领域的领导者是
    Pan-STARRS 移动物体处理系统或 MOPS（Denneau et al. ([2013](#bib.bib7)))。最初用 Pan-STARRS 望远镜的模拟但真实的小行星数据进行训练，它处理那些与已知源无关的瞬态候选体，并使用复杂的基于树的空间关联算法（Kubica
    et al. ([2007](#bib.bib19))) 进一步解析并形成这些点源之间的关联。MOPS 不处理影像数据，而是处理天体坐标，这减少了计算成本。HeliolinC（Holman
    et al. ([2018](#bib.bib14))) 通过结合使用日心坐标系和聚类属于同一物体的源的方式进一步提高了 MOPS 的效率。
- en: While these and other deterministic approaches have been successfully utilized
    for asteroid detection, applications of deep learning in the field remain in the
    early stages, potentially because of the lack of labelled data. Deep learning
    offers the benefit of being able to learn representations directly from the raw
    data, making it a potentially valuable tool for asteroid discovery in archival
    astronomical data. The works that do apply deep leaning techniques note the benefits,
    particularly with greatly reducing the amount of data that must be examined by
    an astronomer, as we see next.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些和其他确定性方法已成功用于小行星检测，但深度学习在该领域的应用仍处于早期阶段，可能是由于缺乏标记数据。深度学习的优势在于能够直接从原始数据中学习表征，这使它成为在档案天文数据中发现小行星的潜在有价值的工具。应用深度学习技术的工作指出了其好处，特别是在大大减少天文学家需要检查的数据量方面，正如我们接下来看到的那样。
- en: Zoghbi et al. ([2017](#bib.bib41)) successfully applied both convolutional and
    recurrent architectures to reduce the amount of data to be vetted by astronomers
    looking for debris from long-period comets in the CAMS data¹¹1http://cams.seti.org/.
    Lieu et al. ([2018](#bib.bib20)) applied neural networks to the task of detecting
    small solar system objects (SSO) in data simulated for the ESA’s Euclid space
    telescope²²2https://sci.esa.int/web/euclid/. They successfully used transfer learning
    and retrained three architectures from TensorFlow’s Keras Applications library³³3https://keras.io/api/applications/
    to distinguish between postage stamp cut-out images of asteroids and objects commonly
    mistaken for asteroids like cosmic rays, stars, and galaxies. Duev et al. ([2019](#bib.bib9))
    introduced DeepStreaks to aid in the ZTF’s⁴⁴4https://www.ztf.caltech.edu/ quest
    for the discovery of near-Earth asteroids, which resemble streaks in the observations.
    Their model significantly reduced the number of candidate detections that had
    to be reviewed without sacrificing the detection sensitivity. Rabeendran and Denneau
    ([2021](#bib.bib24)) applied deep learning to the ATLAS⁵⁵5https://atlas.fallingstar.com/home.php
    pipeline looking for near-Earth objects. It was successful in catching nearly
    90% of the false positive detections, thus greatly speeding up the process of
    followup observations. Duev et al. ([2021](#bib.bib8)) introduced Tails, which
    involved training an object detector to discover comets based on their distinctive
    morphology and it now forms a part of the ZTF’s detection pipeline. Finally, Kruk
    et al. ([2022](#bib.bib18)) used deep learning to hunt for asteroid trails in
    archival data from the Hubble space telescope (HST)⁶⁶6https://hubblesite.org/.
    They used composite HST images to make the asteroids trails longer and thus easier
    to detect. Their research also demonstrates the merits of citizen science for
    labelling the data and of mining archival data for asteroids with a deep learning-based
    toolkit.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Zoghbi 等人 ([2017](#bib.bib41)) 成功地将卷积和递归架构应用于减少天文学家在 CAMS 数据中寻找长期周期彗星碎片时需要审查的数据量¹¹1http://cams.seti.org/。Lieu
    等人 ([2018](#bib.bib20)) 将神经网络应用于 ESA 的欧几里得空间望远镜²²2https://sci.esa.int/web/euclid/
    数据模拟中小型太阳系物体（SSO）的检测任务。他们成功使用了迁移学习，并重新训练了 TensorFlow 的 Keras Applications 库中的三个架构³³3https://keras.io/api/applications/，以区分小行星的邮票剪切图像与常被误认为是小行星的物体，如宇宙射线、星星和银河系。Duev
    等人 ([2019](#bib.bib9)) 介绍了 DeepStreaks，以帮助 ZTF⁴⁴4https://www.ztf.caltech.edu/
    在寻找近地小行星的过程中，该小行星在观测中呈现为条纹。他们的模型显著减少了需要审核的候选检测数量，同时没有牺牲检测灵敏度。Rabeendran 和 Denneau
    ([2021](#bib.bib24)) 将深度学习应用于 ATLAS⁵⁵5https://atlas.fallingstar.com/home.php 流程，以寻找近地物体。这在捕捉近
    90% 的假阳性检测中取得了成功，从而大大加快了后续观测的速度。Duev 等人 ([2021](#bib.bib8)) 介绍了 Tails，该方法包括训练一个物体检测器，以基于其独特的形态学发现彗星，并且现在已成为
    ZTF 检测流程的一部分。最后，Kruk 等人 ([2022](#bib.bib18)) 使用深度学习在哈勃太空望远镜（HST）⁶⁶6https://hubblesite.org/
    的档案数据中寻找小行星轨迹。他们使用复合 HST 图像来使小行星轨迹变长，从而更容易检测。他们的研究还展示了公民科学在标记数据方面的优点，以及利用基于深度学习的工具包挖掘档案数据中的小行星。
- en: We have seen that an important aspect of finding asteroids in survey data is
    isolating the sources that indicate a candidate object. In the case of microlensing
    data, this task is all the more challenging because of the extremely dense star
    fields observed, causing even the reference subtracted images to contain numerous
    spurious artefacts. Thus, rather than extracting the sources and then establishing
    connections between them, we propose a technique to instead enable extracting
    a cluster of sources that could represent part of the orbital arc of asteroids.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，在调查数据中发现小行星的重要方面是隔离出指示候选对象的源。在引力微透镜数据的情况下，这一任务更加具有挑战性，因为观察到的星场极其密集，甚至参考减去的图像中也包含了大量伪影。因此，我们提出了一种技术，旨在提取可能代表小行星轨道弧一部分的源簇，而不是先提取源然后建立它们之间的联系。
- en: We present the methodology to create labelled datasets suitable for supervised
    learning, followed by convolutional neural network-based architectures for finding
    asteroids tracklets in microlensing surveys. Our classification models facilitate
    reducing the amount of data to be vetted and the object detection model localizes
    the potential tracklets within the classifiers’ candidate detections. The bounding
    boxes predicted by the object detector could then be used to extract the potential
    sources, which in turn could be passed to orbiting link software, such as HeliolinC,
    to determine if they are valid solar-bound objects.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了创建适用于监督学习的标记数据集的方法，其后是基于卷积神经网络的架构，用于在微引力透镜调查中寻找小行星轨迹。我们的分类模型有助于减少需要审查的数据量，而目标检测模型则定位分类器候选检测中的潜在轨迹。目标检测器预测的边界框随后可以用来提取潜在源，这些源可以传递给轨道链接软件，如HeliolinC，以确定它们是否为有效的太阳系-bound物体。
- en: '2 MOA: Microlensing Observations in Astrophysics'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '2 MOA: 微引力透镜天文学观察'
- en: 'This research utilizes archival data from the MOA (Microlensing Observations
    in Astrophysics) project, which is a Japan/New Zealand collaboration specialising
    in the search for gravitational microlensing events. They have been operating
    the 1.8m MOA-II optical research telescope at the Mt. John Observatory since 2004\.
    It has a wide-field mosaic CCD camera called the MOA-cam3 (Sako et al. ([2008](#bib.bib30))),
    which consists of 10 CCD chips. Each CCD chip is 3cm x 6cm and has 2048 x 4096
    pixels, with a resolution of 0.01 arc minutes/ 0.6 arcseconds per pixel. Each
    exposure is 60 seconds long and all images are in a custom wide MOA-R band (630-1000
    nm). The median FWHM seeing is  1.7 arcseconds. The photometric precision is typically
     0.01 mag or better for I brighter than 16 and  0.02 mag for I 18\. The telescope
    has a total field of view of 1.7 x 1.4 square degrees. The survey field referred
    to as GB1 can be seen in its entirety in Figure [1](#S2.F1 "Figure 1 ‣ 2 MOA:
    Microlensing Observations in Astrophysics ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning"). The CCD chips are numbered 1 to 10 clockwise, starting
    from the top-left.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '本研究利用了 MOA（微引力透镜天文学观察）项目的档案数据，该项目是一个日本/新西兰合作项目，专门研究引力微透镜事件。自2004年起，他们在约翰山天文台运营1.8米MOA-II光学研究望远镜。该望远镜配备了一个名为
    MOA-cam3（Sako et al. ([2008](#bib.bib30))) 的广角马赛克CCD相机，由10个CCD芯片组成。每个CCD芯片的尺寸为3厘米
    x 6厘米，具有2048 x 4096像素，分辨率为每像素0.01角分/0.6角秒。每次曝光持续60秒，所有图像都在定制的广角MOA-R带（630-1000纳米）下拍摄。中位FWHM视距为1.7角秒。光度精度通常为0.01等或更好（对于I亮于16），以及0.02等（对于I18）。该望远镜的总视场为1.7
    x 1.4平方度。图 [1](#S2.F1 "Figure 1 ‣ 2 MOA: Microlensing Observations in Astrophysics
    ‣ Towards Asteroid Detection in Microlensing Surveys with Deep Learning") 展示了称为GB1的调查场。CCD芯片按顺时针方向编号1至10，从左上角开始。'
- en: MOA-II surveys the Galactic Bulge (GB) and the Large Magellanic Cloud (LMC),
    both of which are regions of the sky that are densely packed with stars. It operates
    at a high sampling rate, with some fields surveyed as often as every 10 minutes,
    which makes it particularly good for observing short duration events. The MOA
    image processing pipeline produces difference images (Tomaney and Crotts ([1996](#bib.bib35));
    Alard and Lupton ([1998](#bib.bib2)); Alard ([2000](#bib.bib1)); Bramich ([2008](#bib.bib5));
    Bond et al. ([2001](#bib.bib4)), where a new observation image is subtracted from
    a reference image for the same region. The resultant difference image highlights
    the changes since the reference image was taken. These could either be transient
    astronomical phenomenon (like microlensing events or asteroids) or noise. The
    noise could be due to too-bright saturated stars, imperfect subtractions, satellite
    trails, instrumentation error, atmospheric dust, differential refraction, or proximity
    to a bright astronomical object (like the moon). As the fields surveyed by MOA-II
    are very dense, faint asteroids in particular are impossible to identify in the
    original observation images. Thus, the MOA difference images form the basis of
    the datasets built for this research.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MOA-II 调查了银河系膨胀体（GB）和大麦哲伦云（LMC），这两个区域的天空中星星密集。它以高采样率运行，一些区域的调查频率高达每`10 min`，这使得它特别适合观察短时事件。MOA
    图像处理管道生成差异图像（Tomaney 和 Crotts ([1996](#bib.bib35))；Alard 和 Lupton ([1998](#bib.bib2))；Alard
    ([2000](#bib.bib1))；Bramich ([2008](#bib.bib5))；Bond 等 ([2001](#bib.bib4))），其中将新观察图像从相同区域的参考图像中减去。生成的差异图像突出了自参考图像拍摄以来的变化。这些变化可能是瞬态天文现象（如微引力透镜事件或小行星）或噪声。噪声可能是由于过亮的饱和星、减法不完善、卫星轨迹、仪器误差、大气尘埃、差分折射或接近明亮天文物体（如月球）。由于
    MOA-II 调查的区域非常密集，特别是微弱的小行星在原始观察图像中几乎无法识别。因此，MOA 差异图像成为本研究数据集的基础。
- en: '![Refer to caption](img/b801f1a8e3e7ee626fe1a9c935f0f4c0.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b801f1a8e3e7ee626fe1a9c935f0f4c0.png)'
- en: 'Figure 1: A single observation of the Galactic Bulge field, GB1, surveyed by
    MOA-II. North is right to left and east is bottom to top.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：MOA-II 调查的银河系膨胀体字段 GB1 的单次观察。北方为从右到左，东方为从下到上。
- en: 3 Building the Dataset
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 构建数据集
- en: The Galactic Bulge is visible in the Southern Hemisphere between February and
    October. A combination of long nights and Bulge elevation in the mid-winter months
    make them the best time for observation. On clear nights with good seeing conditions,
    several of the MOA-II fields are surveyed frequently, at times as often as every
    10 minutes. This provides ideal conditions for observing asteroids in our solar
    system. Figure [2](#S3.F2 "Figure 2 ‣ 3 Building the Dataset ‣ Towards Asteroid
    Detection in Microlensing Surveys with Deep Learning") demonstrates this with
    six observations of a bright main-belt asteroid (78153) 2002 NX24, with a limiting
    magnitude (V) of 17.5, taken at 10-12 minute intervals on the 23rd of June, 2006\.
    The light curve for this asteroid, extracted from the MOA-II data can be see in
    Figure [3](#S3.F3 "Figure 3 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection
    in Microlensing Surveys with Deep Learning"). The sky-plane velocities are about
    0.5 arcseconds per minute, which are representative of the asteroids captured
    by MOA-II.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 银河系膨胀体在南半球的可见时间为 2 月到 10 月。长夜和膨胀体在冬季中期的高度使得这个时间段最适合观察。在清晰的夜晚和良好的观测条件下，MOA-II
    的多个区域被频繁调查，有时每`10 min`调查一次。这为观察我们太阳系的小行星提供了理想条件。图 [2](#S3.F2 "图 2 ‣ 3 构建数据集 ‣
    使用深度学习进行微引力透镜调查中的小行星探测") 演示了这一点，对一个明亮的主带小行星（78153）2002 NX24 进行了六次观察，限界星等（V）为 17.5，拍摄间隔为
    10-12 分钟，日期为 2006 年 6 月 23 日。这个小行星的光曲线可以在图 [3](#S3.F3 "图 3 ‣ 3 构建数据集 ‣ 使用深度学习进行微引力透镜调查中的小行星探测")
    中看到。天空平面速度约为每分钟 0.5 弧秒，这代表了 MOA-II 捕捉到的小行星。
- en: '![Refer to caption](img/b2a44eaec5e32b886ffa0df3483c4dd1.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b2a44eaec5e32b886ffa0df3483c4dd1.png)'
- en: 'Figure 2: Six consecutive observations of the asteroid (78153) 2002 NX24 on
    23-June-2006, spanning 70 x 108 arcseconds.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：2006 年 6 月 23 日，对小行星（78153）2002 NX24 的六次连续观察，跨度为 70 x 108 弧秒。
- en: '![Refer to caption](img/cf625d3b4b0d5dc727960ff5d55f195c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cf625d3b4b0d5dc727960ff5d55f195c.png)'
- en: 'Figure 3: Light curve of asteroid (78153) 2002 NX24 extracted from the MOA-II
    data.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：从 MOA-II 数据中提取的小行星（78153）2002 NX24 的光曲线。
- en: '![Refer to caption](img/9ff2bfe14f1c0f60f77d99a7456e9e9d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ff2bfe14f1c0f60f77d99a7456e9e9d.png)'
- en: 'Figure 4: Measured MOA flux vs the MPC V magnitude for asteroids recovered
    from GB5-R5\. The MPC value is reported to one decimal place and the results are
    binned in 0.1 magnitude bins. The red dots represents the median values of the
    fluxes. This starts to flatten out when V is around 21, at which point it is effectively
    the background being measured. The green line represents the working limiting
    flux commonly used in MOA microlensing work'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：GB5-R5中恢复的小行星的MOA测得通量与MPC V星等的关系。MPC值报告到小数点后一位，结果以0.1星等为单位进行分箱。红点表示通量的中位数值。当V星等接近21时，曲线开始趋于平坦，此时实际测量的是背景。绿色线表示MOA微引力工作中常用的限制通量。
- en: The primary data used in this research consists of all observations from 2006
    - 2019 from one chip (chip 5) in the CCD array for the field GB5, amounting to
    49,901 difference images (GB5-R5). This field is surveyed very frequently and
    the cadence of most observations is between 5 and 20 minutes. The Minor Planet
    Center (MPC) was queried⁷⁷7https://www.minorplanetcenter.net/cgi-bin/checkmp.cgi
    via a screen scraper to get all known asteroids expected to traverse through the
    region encompassed by GB5-R5\. To determine the limiting magnitude of our survey,
    we have compared the fluxes measured at the positions of recovered asteroids from
    GB5-R5 with the V magnitudes provided by MPC. The results are shown Figure [4](#S3.F4
    "Figure 4 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning"), where the data has been binned by MPC V magnitude
    with a bin size of 0.1 mag with the median flux values calculated for each bin.
    For microlensing measurements, MOA adopts a working value of 2500 for the limiting
    data number. This corresponds to a MPC V magnitude  20.5 which we adopt as our
    limiting magnitude here.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究使用的主要数据包括2006年至2019年在CCD阵列中的一个芯片（芯片5）对GB5场的所有观测数据，共计49,901幅差分图像（GB5-R5）。该场被非常频繁地调查，大多数观测的时间间隔在5到20分钟之间。通过屏幕抓取程序查询了小行星中心（MPC）[7](https://www.minorplanetcenter.net/cgi-bin/checkmp.cgi)，获取了所有已知的预计穿过GB5-R5区域的小行星。为了确定我们调查的限制星等，我们将GB5-R5中恢复的小行星位置测得的通量与MPC提供的V星等进行了比较。结果见图[4](#S3.F4
    "Figure 4 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning")，数据按MPC V星等进行分箱，箱宽为0.1星等，并计算了每个箱的中位数通量值。对于微引力测量，MOA采用了2500作为限制数据数量的工作值。这对应于MPC
    V星等20.5，我们在这里将其作为我们的限制星等。
- en: Since asteroids move appreciably with respect to the background stars in each
    consecutive exposure, asteroid tracklets can be clearly seen by stacking nightly
    observations. We define a tracklet as part of the orbital arc of the asteroid
    as it travels through the field of view of the telescope. We started with a stack
    composed of the brightest pixels. While the tracklets were visible, so was the
    bright background and the over-bright stars (Figure [5](#S3.F5 "Figure 5 ‣ 3 Building
    the Dataset ‣ Towards Asteroid Detection in Microlensing Surveys with Deep Learning")(a)).
    To highlight the tracklets better, the stack image was further simplified by subtracting
    the brightest pixel stack from the median pixel stack. This gave us an image without
    the bright background and saturated stars (Figure [5](#S3.F5 "Figure 5 ‣ 3 Building
    the Dataset ‣ Towards Asteroid Detection in Microlensing Surveys with Deep Learning")(c)),
    leaving behind the noise and moving objects. Only nights with 3 or more observations
    were considered when generating the subtracted stacks. The GB5-R5 dataset resulted
    in 2252 subtracted stack images within which the search for asteroid tracklets
    commenced.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于小行星在每次连续曝光中相对于背景恒星有显著移动，通过叠加夜间观测可以清晰地看到小行星的轨迹片段。我们将轨迹片段定义为小行星在望远镜视野中移动时轨道弧的一部分。我们从由最亮的像素组成的堆叠开始。虽然轨迹片段可见，但亮背景和过亮的恒星也同样明显（图[5](#S3.F5
    "Figure 5 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning")(a)）。为了更好地突出轨迹片段，通过从中位像素堆叠中减去最亮像素堆叠，进一步简化了堆叠图像。这给我们提供了一幅没有亮背景和饱和恒星的图像（图[5](#S3.F5
    "Figure 5 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning")(c)），只留下噪声和移动物体。在生成减法堆叠时，只考虑了有3次或更多观测的夜晚。GB5-R5数据集产生了2252幅减法堆叠图像，开始了对小行星轨迹片段的搜索。
- en: The ephemeris data from the MPC, together with the astrometric calibrations
    specific to MOA-II, were used to extrapolate the minimum and maximum (x, y) positions
    for asteroid tracklets in subtracted stack images. This was used to crop sub-regions
    expected to contain tracklets from each stack. Asteroids that fell beyond the
    boundaries of a CCD chip were ignored. Further, by visual inspection of the tracklets,
    it was established that only asteroids with a limiting magnitude of 20.5 or brighter
    are visible in the MOA-II exposures. This is also consistent with what we see
    in Figure [4](#S3.F4 "Figure 4 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection
    in Microlensing Surveys with Deep Learning"), where the median flux around V 21
    flattens out and is akin to measuring the background. The objects with magnitude
    between 19.5 and 20.5 are often at the very edge of visibility, requiring excellent
    seeing condition and a high signal to noise ratio to be visible in the observations.
    Careful examination of the images resulted in 2073 tracklets from 1178 distinct
    asteroids over 1078 distinct nights. Figure [6](#S3.F6 "Figure 6 ‣ 3 Building
    the Dataset ‣ Towards Asteroid Detection in Microlensing Surveys with Deep Learning")
    (374 x 387 arcseconds) displays some of the tracklets captured in the observations
    on the night of 15-May-2008\. Some of the tracklets are very faint.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 来自MPC的历书数据以及特定于MOA-II的天文测量校准被用来推断从减去的堆叠图像中小行星轨迹片段的最小和最大（x，y）位置。这被用来裁剪预期包含轨迹片段的堆叠图像的子区域。超出CCD芯片边界的小行星被忽略。此外，通过对轨迹片段的目视检查，确定只有20.5等或更亮的小行星在MOA-II曝光中可见。这也与我们在图[4](#S3.F4
    "图4 ‣ 3 数据集构建 ‣ 利用深度学习进行微引力透镜调查中的小行星检测")中看到的情况一致，其中V 21周围的中位数通量趋于平坦，类似于测量背景。19.5到20.5等之间的对象通常处于可见性边缘，需要优良的观测条件和高信噪比才能在观测中显现。对图像的仔细检查结果是，在1178个不同的小行星中，获得了2073个轨迹片段，覆盖1078个不同的夜晚。图[6](#S3.F6
    "图6 ‣ 3 数据集构建 ‣ 利用深度学习进行微引力透镜调查中的小行星检测")（374 x 387角秒）展示了2008年5月15日夜晚观测到的一些轨迹片段，其中一些轨迹片段非常微弱。
- en: Tracklets come in a variety of sizes, but we need images of a uniform size for
    training neural networks. The original size of the observations was deemed too
    big to be used as is because of the computational cost as well as the potential
    for the tracklets to be lost amongst the other artefacts in the image. Therefore,
    a decision was made to split each image into 128 x 128 tiles, giving us 512 tiles
    per image. Each 128x128 tile spans 76 x 76 arcseconds. For GB5-R5, this gave us
    a total of 551,936 images from the 1078 nightly stacks that contained visible
    asteroid tracklets.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹片段有多种大小，但我们需要统一尺寸的图像来训练神经网络。由于计算成本以及轨迹片段可能在图像中被其他伪影掩盖，观察的原始大小被认为太大，不能直接使用。因此，决定将每张图像拆分为128
    x 128的瓦片，每张图像产生512个瓦片。每个128x128的瓦片覆盖76 x 76角秒。对于GB5-R5，这使我们从1078个夜间堆叠图像中获得了总计551,936张图像，其中包含可见的小行星轨迹片段。
- en: '![Refer to caption](img/265ba679a900ea03e0e6c38493aebc21.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/265ba679a900ea03e0e6c38493aebc21.png)'
- en: 'Figure 5: Stacking all of a night’s observations (23-June-2006) on one chip
    in one field gives us a clear tracklet for asteroid (78153) 2002 NX24\. In (a)
    the observations are stacked by brightest pixel; in (b) they are stacked by the
    median pixel; and in (c) we see the result of subtracting (b) from (a).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：将一个夜晚的所有观测数据（2006年6月23日）叠加在一个芯片上的一个场中，我们获得了小行星（78153）2002 NX24的清晰轨迹片段。在(a)中，观测数据按最亮像素叠加；在(b)中，按中位数像素叠加；在(c)中，我们看到将(b)从(a)中减去的结果。
- en: '![Refer to caption](img/939750fc6c00b7e012226a67aed0e76c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/939750fc6c00b7e012226a67aed0e76c.png)'
- en: 'Figure 6: A stack of all 51 observations from the night of 15-May-2008 reveals
    5 asteroid tracklets clearly visible in a sub-region of the stack image. Numbered
    in order of appearance, these are: 1 - (103842) 2000 DQ33 (19.5); 2 - (148657)
    2001 SX124 (19.6); 3 - (152083) 2004 RH30 (19.9); 4 - (582743) 2016 AT221 (20.4);
    5 - (338789) 2005 SZ154 (20.4). The line/streak on the top left is from a satellite.
    This image has been inverted and brightened to improve visibility (original in
    Figure [15](#A1.F15 "Figure 15 ‣ Appendix A Original images ‣ Towards Asteroid
    Detection in Microlensing Surveys with Deep Learning")).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：2008年5月15日晚的51次观察叠加图清晰地显示了5条小行星轨迹，这些轨迹在叠加图像的子区域内可见。按出现顺序编号，它们是：1 - (103842)
    2000 DQ33 (19.5)；2 - (148657) 2001 SX124 (19.6)；3 - (152083) 2004 RH30 (19.9)；4
    - (582743) 2016 AT221 (20.4)；5 - (338789) 2005 SZ154 (20.4)。左上角的线条/条纹来自卫星。此图像已被反转和加亮以提高可见性（原图见图
    [15](#A1.F15 "图 15 ‣ 附录 A 原始图像 ‣ 通过深度学习在微引力透镜调查中进行小行星检测")）。
- en: 'The Cohen-Sutherland line clipping algorithm (W. M. Newman and R. F. Sproull
    ([1973](#bib.bib37))) was next used to locate the asteroid tracklets within these
    images. The algorithm involves dividing a rectangular space (in this case, an
    image) into nine regions - eight ”outside” and one ”inside”, as seen in Figure
    [7](#S3.F7 "Figure 7 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in
    Microlensing Surveys with Deep Learning") - and determining which of the lines
    (tracklets) are fully or partially inside the area of interest (the image). Each
    of the nine regions have associated outcodes (4-bit numbers) that are calculated
    by performing bitwise operations after comparing the start and end points of the
    line/tracklet with the coordinates of the image. For example, the outcode 0001
    indicates that the end point is center left and the outcode 1000 indicates that
    the end point is center top. A bitwise OR of these two codes returns 1001, which
    is a non-zero value, and the bitwise AND returns 0000, which is zero, which in
    turn indicates that the tracklet is partially inside the image. Thus, there are
    three possible solutions for any line:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 科恩-萨瑟兰线裁剪算法（W. M. Newman 和 R. F. Sproull ([1973](#bib.bib37)))接着用于在这些图像中定位小行星轨迹。该算法涉及将矩形空间（在此为图像）划分为九个区域
    - 八个“外部”区域和一个“内部”区域，如图 [7](#S3.F7 "图 7 ‣ 3 构建数据集 ‣ 通过深度学习在微引力透镜调查中进行小行星检测")所示
    - 并确定哪些线段（轨迹）完全或部分在感兴趣区域（图像）内。每个九个区域都有相关的外码（4位数字），这些外码是通过在将线段/轨迹的起始和结束点与图像坐标进行比较后进行按位操作计算得出的。例如，外码
    0001 表示端点在左中心，外码 1000 表示端点在上中心。这两个代码的按位或操作返回 1001，这是一个非零值，而按位与操作返回 0000，即零，这反过来表明轨迹部分在图像内。因此，任何线段有三种可能的解决方案：
- en: '1.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: If both endpoints of the line are inside the area of interest, the bitwise OR
    computation returns 0 (trivial accept).
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果线段的两个端点都在感兴趣区域内，按位或计算返回 0（简易接受）。
- en: '2.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: If both endpoints of the line are outside the area of interest, they will share
    at least one outside region and the bitwise AND computation returns a non 0 value
    (trivial reject).
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果线段的两个端点都在感兴趣区域外，它们将共享至少一个外部区域，按位与计算返回非零值（简易拒绝）。
- en: '3.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: If both endpoints are in different regions, at least one endpoint will be outside
    the image tile. In this case, the intersection point of the tracklet’s outside
    point and image tile boundary becomes the new endpoint for the tracklet and the
    algorithm repeats until the bitwise operation returns a trivial accept or reject.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果两个端点在不同区域，至少有一个端点会在图像瓷砖外部。在这种情况下，轨迹外部点和图像瓷砖边界的交点成为轨迹的新端点，算法重复直到按位操作返回简易接受或拒绝。
- en: '![Refer to caption](img/93bb1495db9f560aaf0f870042674e34.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/93bb1495db9f560aaf0f870042674e34.png)'
- en: 'Figure 7: The Cohen-Sutherland line clipping algorithm was used to reject tracklets
    outside the area of interest and determine intersection points of the ones partially
    inside.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：科恩-萨瑟兰线裁剪算法用于拒绝感兴趣区域外的轨迹并确定部分在内部的轨迹的交点。
- en: The application of this algorithm gave us 8341 tiles that potentially had visible
    tracklets and 543,595 without known asteroids. Once again these were meticulously
    scanned to ensure that a tracklet could clearly be seen. Any tile with less than
    3 points of a tracklet were rejected, along with tiles that did not contain a
    visible portion of a tracklet. At the conclusion of this process, there were 4153
    128 x 128 tiles with visible tracklets from GB5-R5\. Some of these can be seen
    in Figure [8](#S3.F8 "Figure 8 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection
    in Microlensing Surveys with Deep Learning")(a).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的应用给我们提供了8341个可能含有可见轨迹的小块和543,595个没有已知小行星的小块。为了确保能够清晰地看到轨迹，每个小块都被细致地扫描过。任何含有少于3个轨迹点的小块都会被拒绝，此外，还有不包含可见轨迹部分的小块也会被排除。经过这一过程，最终得到4153个128
    x 128的小块，这些小块中含有来自GB5-R5的可见轨迹。部分示例见于图 [8](#S3.F8 "Figure 8 ‣ 3 Building the Dataset
    ‣ Towards Asteroid Detection in Microlensing Surveys with Deep Learning")(a)。
- en: '![Refer to caption](img/ad828397fc55ba1fde6e97c267c614d1.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ad828397fc55ba1fde6e97c267c614d1.png)'
- en: 'Figure 8: Asteroid tracklets seen in 128x128 sub-regions from subtracted stack
    images. In (b), the tracklets are localized with bounding boxes. Asteroids in
    the top row, left to right: (38102) 1999 JM18 (18.4); (152083) 2004 RH30 (19.9);
    (103842) 2000 DQ33 (19.5); (283261) 2011 FR142 (20.0); and (375674) 2009 HD5 (19.8).
    Asteroids in the bottom row, from left to right: (48617) 1995 HR2 (19.5); (148657)
    2001 SX124 (19.6); (97948) 2000 QF124 (19.6); and (74978) 1999 TY234 (18.7). Note
    that these images have been inverted and brightened to improve visibility (original
    in Figure [16](#A1.F16 "Figure 16 ‣ Appendix A Original images ‣ Towards Asteroid
    Detection in Microlensing Surveys with Deep Learning")).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 从减法堆叠图像中观察到的128x128子区域中的小行星轨迹。在(b)中，轨迹用边界框标出。顶部行的行星，从左到右依次是：(38102) 1999
    JM18 (18.4)；(152083) 2004 RH30 (19.9)；(103842) 2000 DQ33 (19.5)；(283261) 2011
    FR142 (20.0)；和(375674) 2009 HD5 (19.8)。底部行的行星，从左到右依次是：(48617) 1995 HR2 (19.5)；(148657)
    2001 SX124 (19.6)；(97948) 2000 QF124 (19.6)；和(74978) 1999 TY234 (18.7)。请注意，这些图像已经被反转并增强了亮度以提高可见性（原图见于图
    [16](#A1.F16 "Figure 16 ‣ Appendix A Original images ‣ Towards Asteroid Detection
    in Microlensing Surveys with Deep Learning")）。'
- en: For classification, the images where the tracklet was too obscured by noise
    or where there were less than three clearly visible adjacent point sources related
    to a tracklet were removed. Following this, there were 4072 images with tracklets,
    which were split into the training/ validation/ test set with 3322/ 415/ 335 in
    each, respectively. The images in the training and validation set were further
    augmented by rotating 180 degrees, flipping horizontally and vertically, brightening,
    darkening, blurring, and both increasing and decreasing the contrast. There was
    no overlap between the training, validation, and test sets. A further test set
    composed of all the observations on a single night from all ten chips in fields
    GB3, GB4, GB5, GB9, GB10, and GB14 was also constructed (GB-All), and it had 300
    tracklet and 2000 no-tracklet images. The purpose of the GB-All test set was to
    evaluate how the networks performed with data they had never seen; as before,
    the tracklets were from observations with a maximum cadence of 20 minutes and
    there are a minimum of 3 sources per image. The classification networks were also
    trained to recognize images with no tracklets. Forty images without known tracklets
    were randomly chosen from each of the 512 sub-regions, giving us 20,480 images.
    This collection was also visually inspected and any images that could potentially
    have tracklets were removed. This process resulted in 19,682 images, which were
    split into the training/validation/test sets with 15,595/2039/2048 images, respectively.
    The same set of augments was generated for the no-tracklet training and validation
    data, but since this set was already significantly larger, only a random 35% of
    the no-tracklet augments were used. Note that this distribution does not reflect
    the true ratio of tracklet vs no-tracklet images; realistically, on a good night,
    we might expect as many 10 tracklet images for every 100 no-tracklet images. The
    roughly 1:5 ratio here serves to better focus the network to learn the tracklet
    pattern while providing a suitable number of contrasting no-tracklet images.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，所有由于噪声而轨迹点过于模糊的图像或少于三个清晰可见的与轨迹相关的点源的图像都被移除。随后，剩下了4072张包含轨迹的图像，这些图像被划分为训练集/验证集/测试集，分别包含3322/415/335张图像。训练集和验证集的图像进一步通过旋转180度、水平和垂直翻转、调整亮度、加深、模糊以及增加和减少对比度进行了数据增强。训练集、验证集和测试集之间没有重叠。还构建了一个由所有十个芯片在GB3、GB4、GB5、GB9、GB10和GB14领域的单夜观测组成的额外测试集（GB-All），该测试集包含300张轨迹图像和2000张无轨迹图像。GB-All测试集的目的是评估网络在从未见过的数据上的表现；如前所述，轨迹来自最大间隔为20分钟的观测，并且每张图像中至少包含3个源。分类网络也被训练来识别没有轨迹的图像。我们从512个子区域中随机选择了40张没有已知轨迹的图像，共计20480张图像。这些图像也进行了视觉检查，任何可能包含轨迹的图像都被移除。这个过程最终得到19682张图像，这些图像被划分为训练集/验证集/测试集，分别为15595/2039/2048张图像。对于无轨迹的训练和验证数据，生成了相同的增强集，但由于此集已经显著更大，因此只使用了35%的无轨迹增强数据。需要注意的是，这种分布并不反映轨迹图像与无轨迹图像的真实比例；实际上，在一个良好的夜晚，我们可能期望每100张无轨迹图像中有10张轨迹图像。这里大约1:5的比例旨在更好地使网络专注于学习轨迹模式，同时提供适当数量的对比无轨迹图像。
- en: As the asteroid tracklets are a well-defined pattern of blobs in an image, we
    further considered training a deep-learning based object detector to localize
    them in the images. This would facilitate locating the tracklets that were faint
    and/ or obscured by noise as well as distinguish the area of interest in the images.
    We utilize YOLOv4 (Bochkovskiy et al. ([2020](#bib.bib3))), which requires the
    objects of interest - in our case, tracklets - to be enclosed in bounding boxes,
    with the coordinates of the object’s centroid saved with respect to the dimensions
    of the image. As a result of applying the Cohen-Sutherland line clipping algorithm,
    we had the probable intersection/end points of tracklets within the 128 x 128
    images. These intersection points were used to extrapolate bounding boxes for
    tracklets, thus highlighting them in the images. Further visual inspection and
    manual adjustment was undertaken to ensure that bounding boxes encapsulated the
    tracklets correctly without any superfluous background included (Figure [8](#S3.F8
    "Figure 8 ‣ 3 Building the Dataset ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning")(b)). All 4153 tracklet images were used, with a training/
    test split of 3737 and 416, respectively.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于小行星轨迹片段在图像中是一个定义明确的斑点模式，我们进一步考虑训练基于深度学习的物体检测器，以在图像中定位它们。这将有助于定位那些微弱和/或被噪声遮挡的轨迹片段，并区分图像中的兴趣区域。我们利用了YOLOv4（Bochkovskiy
    et al. ([2020](#bib.bib3)))，它要求将感兴趣的对象 - 在我们的例子中是轨迹片段 - 包含在边界框中，且对象的质心坐标根据图像的尺寸保存。通过应用Cohen-Sutherland线裁剪算法，我们得到了128
    x 128图像中的轨迹片段的可能交点/结束点。这些交点用于推测轨迹片段的边界框，从而在图像中突出显示它们。进一步的视觉检查和手动调整已被执行，以确保边界框准确地包围了轨迹片段，没有包含任何多余的背景（图
    [8](#S3.F8 "图 8 ‣ 3 构建数据集 ‣ 利用深度学习在微透镜调查中进行小行星检测")(b)）。所有4153张轨迹片段图像都被使用，其中训练/测试的分割比例为3737和416。
- en: 4 Deep Learning Framework
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度学习框架
- en: The appeal of neural networks and deep learning is in their capacity to make
    predictions about complex data in real time after they have been trained with
    a representative dataset. In the last decade, deep learning has matured significantly
    and has proven to be highly effective for classification and object detection
    tasks. Here, we discuss training several convolutional neural network (CNN) based
    architectures to find asteroids tracklets in the subtracted stack images.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和深度学习的吸引力在于它们能够在经过代表性数据集训练后，实时对复杂数据进行预测。在过去十年中，深度学习已经显著成熟，并且在分类和物体检测任务中证明了其高效性。在这里，我们讨论了训练几种基于卷积神经网络（CNN）的架构，以在减法叠加图像中找到小行星轨迹片段。
- en: The proposed model consists of an ensemble of five classifiers that produces
    a probability that a 128x128 composite image tile contains a tracklet. Tiles believed
    to contain tracklets are then passed to YOLOv4, which has been trained to localize
    tracklets by adding a bounding box around the tracklet within the image.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的模型由五个分类器的集成组成，生成128x128复合图像块包含轨迹片段的概率。被认为包含轨迹片段的图块随后传递给YOLOv4，YOLOv4经过训练以通过在图像中的轨迹片段周围添加边界框来定位轨迹片段。
- en: 4.1 Classification
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 分类
- en: A convolutional neural network or CNN is a network architecture that is designed
    to take advantage of the 2D structure of an input image. Using a series of convolutional
    layers that utilise filters, implemented using local connections and shared weights,
    it can extract meaningful features directly from data, thereby eliminating the
    need for manual feature extraction.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络或CNN是一种网络架构，旨在利用输入图像的2D结构。通过一系列使用滤波器的卷积层，这些滤波器通过局部连接和共享权重来实现，它可以直接从数据中提取有意义的特征，从而消除了手动特征提取的需要。
- en: Traditionally, the number of filters in a layer increases with the depth of
    the network. The received wisdom is that the more layers a network has, the better
    it is at extracting more complex features from an image, and thus learning from
    complex data. The limiting factors here, however, are the size of the input and
    how localised the features we are interested in are, as well as the amount of
    available training data. After a given number of layers, a network could start
    overfitting to the data by focusing on the irregularities in the images. So, while
    increasing the number of convolutional layers improves the performance of the
    network, it is not the case that the deeper network is always the best option.
    Part of the challenge is finding the ideal depth for a network to optimally predict
    the output. Simonyan et al. introduced VGGNet (Simonyan and Zisserman ([2015](#bib.bib31)))
    and investigated the effects of increasing the depth of convolutional layers on
    the model’s classification and localisation accuracy in the large-scale image
    recognition setting (ILSVRC challenge). VGGNet has a compelling simplicity in
    its architecture; it uses a stack of consecutive convolutional layers to reduce
    the number of parameters, leading to faster convergence and reducing the overfitting
    problem. GoogLeNet (Szegedy et al. ([2015](#bib.bib33))), popularly known as Inception,
    introduced the idea of modifying the width of a network as well as employing filters
    of a variety of sizes to better capture multi-scale data from images. Relative
    to the VGGNet architecture, it is able to reduce the number of parameters and
    computational cost further. Residual networks (He et al. ([2016](#bib.bib13))),
    or ResNets, were introduced to address the vanishing gradient problem as networks
    go deeper by presenting an alternative pathway for the algorithm to follow, called
    the skip connection. The central element in this architecture is the residual
    block, which consists of two convolutional layers with 3x3 filters. The input
    of this is added to the output of the second convolution, thus creating a shortcut
    connection.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，层中的滤波器数量随着网络的深度增加。普遍的观点是，网络层数越多，从图像中提取更复杂特征的能力越强，从而能从复杂数据中学习。然而，限制因素在于输入的大小、我们感兴趣的特征的局部化程度以及可用的训练数据量。在达到一定层数后，网络可能会因专注于图像中的不规则性而开始过拟合。因此，虽然增加卷积层的数量可以提高网络的性能，但并不是说更深的网络总是最佳选择。部分挑战在于找到网络的理想深度，以便最优化地预测输出。Simonyan
    等人介绍了 VGGNet（Simonyan 和 Zisserman ([2015](#bib.bib31)))，并研究了在大规模图像识别设置（ILSVRC
    挑战）中，增加卷积层深度对模型分类和定位准确性的影响。VGGNet 的架构简洁明了；它使用一系列连续的卷积层来减少参数数量，从而加快收敛速度并减少过拟合问题。GoogLeNet（Szegedy
    等人 ([2015](#bib.bib33)))，通常称为 Inception，引入了修改网络宽度的思想，并采用多种尺寸的滤波器来更好地捕捉图像中的多尺度数据。与
    VGGNet 架构相比，它能够进一步减少参数数量和计算成本。残差网络（He 等人 ([2016](#bib.bib13)))，或称 ResNets，旨在解决网络深度增加时的梯度消失问题，通过引入一种称为跳跃连接的替代路径来解决。该架构的核心元素是残差块，由两个带有
    3x3 滤波器的卷积层组成。输入被添加到第二个卷积的输出，从而创建了一个捷径连接。
- en: In our work, a variety of CNN classification architectures were trialled to
    determine the best combination of filters and layers to suit our purpose. After
    considerable experimentation with well-established architectures such as VGG-16
    and VGG-19 (Simonyan and Zisserman ([2015](#bib.bib31))), Inception (Szegedy et al.
    ([2015](#bib.bib33))), and ResNet50 (He et al. ([2016](#bib.bib13))), various
    custom models were constructed to determine if better performance could be obtained.
    Table [1](#S4.T1 "Table 1 ‣ 4.1 Classification ‣ 4 Deep Learning Framework ‣ Towards
    Asteroid Detection in Microlensing Surveys with Deep Learning") describes the
    structure of five of these custom architectures, three of which are VGG-like (MOA-12,
    MOA-14, MOA-15), and two are composed of hybrid Inception-ResNet modules (Hybrid
    A, Hybrid B). All of the custom architectures have significantly fewer parameters
    than their established counterparts. It is our finding that having an over-parameterised
    deep network architecture causes over-fitting of the training data, and so we
    reduced the number of layers and filters in the network to reduce the number of
    neurons and weights involved.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，试验了多种CNN分类架构，以确定最佳的滤波器和层的组合来满足我们的目的。在对VGG-16和VGG-19（Simonyan和Zisserman
    ([2015](#bib.bib31)))、Inception（Szegedy等人 ([2015](#bib.bib33)))和ResNet50（He等人
    ([2016](#bib.bib13)))等成熟架构进行了大量实验之后，构建了多种自定义模型，以确定是否能获得更好的性能。表[1](#S4.T1 "表 1
    ‣ 4.1 分类 ‣ 4 深度学习框架 ‣ 通过深度学习在微引力透镜调查中探测小行星")描述了这五种自定义架构的结构，其中三种类似于VGG（MOA-12、MOA-14、MOA-15），两种由混合Inception-ResNet模块组成（Hybrid
    A、Hybrid B）。所有自定义架构的参数数量显著少于它们的成熟对手。我们的研究发现，过度参数化的深度网络架构会导致训练数据的过拟合，因此我们减少了网络中的层数和滤波器数量，以减少涉及的神经元和权重数量。
- en: '![Refer to caption](img/b67be68082485920d142533b6535ee80.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b67be68082485920d142533b6535ee80.png)'
- en: 'Figure 9: Hybrid module combining salient features of a ResNet block and an
    Inception module'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：混合模块结合了ResNet块和Inception模块的显著特征
- en: '![[Uncaptioned image]](img/9bfb7dce8bab29442f6d9cc64a6c6711.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![[无说明的图像]](img/9bfb7dce8bab29442f6d9cc64a6c6711.png)'
- en: 'Table 1: Configuration of custom classification architectures MOA-12, MOA-14,
    MOA-15, Hybrid A, and Hybrid B. ReLU activation is used in each layer and dropout
    is used after each fully connected layer.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：自定义分类架构MOA-12、MOA-14、MOA-15、Hybrid A和Hybrid B的配置。每一层都使用了ReLU激活函数，并且在每个全连接层后使用了dropout。
- en: The hybrid module (Figure [9](#S4.F9 "Figure 9 ‣ 4.1 Classification ‣ 4 Deep
    Learning Framework ‣ Towards Asteroid Detection in Microlensing Surveys with Deep
    Learning")) consists of four branches composed of convolutional layers, each of
    which use the ReLU activation (Glorot et al. ([2011](#bib.bib10)); Nair and Hinton
    ([2010](#bib.bib23))) and are combined with the ’add’ function. The final branch
    carries the input to the ’add’ function, with the convolution being used only
    when the number of filters in the previous layer are not equal to the current
    layer’s filters. The many 1x1 convolutions in the hybrid module may seem superfluous
    but removing even one negatively affects the performance of the network. This
    may be because of the beneficial complexity introduced by the associated non-linearity.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模块（图[9](#S4.F9 "图 9 ‣ 4.1 分类 ‣ 4 深度学习框架 ‣ 通过深度学习在微引力透镜调查中探测小行星")）由四个分支组成，每个分支都由卷积层组成，这些卷积层使用ReLU激活函数（Glorot等人
    ([2011](#bib.bib10)); Nair和Hinton ([2010](#bib.bib23)))，并通过“add”函数进行组合。最终分支将输入传递到“add”函数中，仅在前一层的滤波器数量与当前层的滤波器数量不相等时才使用卷积。混合模块中的许多1x1卷积看似多余，但即使去掉一个也会对网络性能产生负面影响。这可能是由于引入的有益复杂性所导致的。
- en: All of the classification architectures were trained on a Linux machine running
    Ubuntu 18.04 with a NVIDIA Quadro M4000 GPU (8 GB, 2.5 TFLOPS). The code was written
    in Python 3.6 in the Jupyter Notebook environment. TensorFlow GPU 2.4.1 (CUDA
    11.0), along with the Keras deep learning API, was used for creating the custom
    CNN models tested. The Keras Applications implementations were used for the established
    models. A batch size of 32 was used for training each network model. Training
    was set to run for 50 epochs, with a callback for early stopping if the validation
    loss failed to minimise after a set number of epochs. The Adam (Kingma and Welling
    ([2014](#bib.bib17))) optimiser was used and the loss function minimised was binary
    cross entropy. The learning rate was initialised at 0.0001 for all networks except
    Hybrid A, which started with a learning rate of 0.001\. In each case, the learning
    rate was reduced after 15 epochs and at scheduled intervals after that point.
    Callbacks were included to save the best weights for both validation and training
    accuracy. The established networks performed better when pre-loaded with ImageNet
    weights before fine-tuning with the MOA-II data. MOA-12, 14, and 15 took on average
    3 hours to train and the Hybrid models, 6 hours.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所有分类架构都在运行Ubuntu 18.04的Linux机器上进行训练，配备了NVIDIA Quadro M4000 GPU（8 GB，2.5 TFLOPS）。代码在Python
    3.6的Jupyter Notebook环境中编写。使用了TensorFlow GPU 2.4.1（CUDA 11.0）以及Keras深度学习API来创建测试的自定义CNN模型。Keras
    Applications实现用于已建立的模型。训练每个网络模型时使用了32的批量大小。训练设置为运行50个周期，如果在设置的周期数后验证损失未能最小化，则进行早期停止的回调。使用了Adam（Kingma和Welling
    ([2014](#bib.bib17)))优化器，最小化的损失函数是二元交叉熵。所有网络的学习率初始化为0.0001，除Hybrid A外，Hybrid A的学习率起始为0.001。在每种情况下，学习率在15个周期后减少，并在此之后按计划的间隔进行调整。回调包括保存最佳的验证和训练准确性权重。建立的网络在使用ImageNet权重预加载后表现更好，然后再用MOA-II数据进行微调。MOA-12、14和15平均需要3小时进行训练，而Hybrid模型则需要6小时。
- en: 4.2 Object Detection
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 目标检测
- en: '![Refer to caption](img/c6695bcca0f05bb83fe9e32bd47c9cb0.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c6695bcca0f05bb83fe9e32bd47c9cb0.png)'
- en: 'Figure 10: Architecture of YOLOv4: a feature extractor that is composed of
    a 53-layer DenseNet with cross-stage partial connections and spacial pyramid pooling
    along with a feature aggregator that effectively combines the feature maps from
    the higher and lower resolution layers.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：YOLOv4架构：一个特征提取器，由53层的DenseNet组成，具有跨阶段部分连接和空间金字塔池化，以及一个特征聚合器，有效地结合了来自高分辨率和低分辨率层的特征图。
- en: In recent years, CNNs have been taken beyond classifying images and have been
    applied to object detection and localization in images. Here, we utilize the YOLOv4
    object detection architecture for localizing asteroid tracklets in our tile images.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，卷积神经网络（CNNs）已经超越了图像分类，应用于图像中的目标检测和定位。在这里，我们利用YOLOv4目标检测架构来定位我们的瓦片图像中的小行星轨迹。
- en: The aim of the original YOLO (You Only Look Once) object detection architecture
    (Redmon et al. ([2016](#bib.bib27))) was to make object detection both fast and
    accessible. As a single-shot architecture, it was capable of making class and
    bounding box predictions with the feature maps produced by a single CNN network.
    The simplicity of the feature extraction CNN at the heart of YOLO was in direct
    contrast to its complex loss function, which computed the classification loss,
    localization loss, as well as the loss quantifying the network’s confidence in
    the prediction. Later versions of the network introduced default anchor boxes,
    which are predefined bounding boxes that are adjusted and refined during training
    to encompass the objects of interest in an image(Redmon and Farhadi ([2017](#bib.bib28))),
    and more complex feature extractors to make predictions on multiple scales (Redmon
    and Farhadi ([2018](#bib.bib29))).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 原始YOLO（You Only Look Once）目标检测架构（Redmon等 ([2016](#bib.bib27))) 的目的是使目标检测既快速又易于实现。作为一个单次检测架构，它能够利用单个CNN网络生成的特征图进行类别和边界框预测。YOLO核心中的特征提取CNN的简单性与其复杂的损失函数形成鲜明对比，该损失函数计算分类损失、定位损失以及量化网络对预测信心的损失。网络的后续版本引入了默认的锚框，即在训练过程中调整和细化的预定义边界框，以涵盖图像中的感兴趣对象（Redmon和Farhadi
    ([2017](#bib.bib28)))，并使用更复杂的特征提取器进行多尺度预测（Redmon和Farhadi ([2018](#bib.bib29)))。
- en: This research applied YOLOv4 (Bochkovskiy et al. ([2020](#bib.bib3))), which
    is the latest evolution of the architecture with a major overhaul to include several
    new techniques, making the model state-of-the-art while still being easy to train.
    In particular, YOLOv4 ensures that the lower level features are propagated through
    both the feature extractor as well as the feature aggregator. Figure [10](#S4.F10
    "Figure 10 ‣ 4.2 Object Detection ‣ 4 Deep Learning Framework ‣ Towards Asteroid
    Detection in Microlensing Surveys with Deep Learning") illustrates the architecture
    of YOLOv4\. The convolutional backbone feature extractor for the architecture
    is composed of a 53 layer DenseNet (Huang et al. ([2016](#bib.bib15))) with the
    cross-stage-partial (CSP) connections of CSPNet (Wang et al. ([2020](#bib.bib38))).
    DenseNets extend ResNet’s concept of skip connections by adding connections between
    all the layers in the network in a feed-forwards fashion. Feature maps from all
    preceding layers are concatenated and form the input for any given layer, ensuring
    that low-level features are propagated through the network. CSP connections involve
    splitting the input feature map into two parts, one of which goes through the
    dense block and the other goes straight through to the next transitional step.
    Additionally, the network includes spatial pyramid pooling (SPP) (He et al. ([2014](#bib.bib12)))
    after the last convolutional layer. This has the effect of separating out the
    most important features and increasing the receptive field. The final feature
    map is divided into $m\ \times\ m$ bins, following which maxpooling is applied
    to each bin. The resulting feature maps are concatenated and represent the output
    of the feature extractor.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究采用了YOLOv4（Bochkovskiy等人（[2020](#bib.bib3)）），这是该架构的最新演变，进行了重大改造，包含了多种新技术，使模型成为最先进的，同时仍然易于训练。特别是，YOLOv4确保低级特征通过特征提取器和特征聚合器进行传播。图[10](#S4.F10
    "图 10 ‣ 4.2 对象检测 ‣ 4 深度学习框架 ‣ 利用深度学习在微引力透镜调查中进行小行星检测")展示了YOLOv4的架构。该架构的卷积骨干特征提取器由53层DenseNet（Huang等人（[2016](#bib.bib15)））组成，并具有CSPNet（Wang等人（[2020](#bib.bib38)））的跨阶段部分（CSP）连接。DenseNet通过在网络中以前馈的方式添加所有层之间的连接，扩展了ResNet的跳跃连接概念。所有前面层的特征图被连接起来，并形成任何给定层的输入，确保低级特征在网络中得以传播。CSP连接涉及将输入特征图分成两部分，一部分经过密集块，另一部分直接传递到下一个过渡步骤。此外，网络在最后的卷积层之后包含空间金字塔池化（SPP）（He等人（[2014](#bib.bib12)））。这会将最重要的特征分离出来，并增加接收字段。最终的特征图被划分为
    $m\ \times\ m$ 个箱子，然后对每个箱子应用最大池化。得到的特征图被连接起来，表示特征提取器的输出。
- en: 'CNNs naturally attain a pyramid-like structure with each layer as the image
    goes from high to low resolution. As we get deeper in a CNN, we lose the fine-grained
    details of the input, which usually makes it harder to detect small objects. As
    the resolution lowers, however, the filters learn ever more complex abstractions
    about the image, making the feature maps more semantically rich. Therefore, is
    it desirable to combine the feature maps from the higher resolution layer with
    the more semantically rich ones to facilitate detecting objects at multiple scales.
    This task falls to a feature aggregator and YOLOv4 uses the approach suggested
    by PANet (Liu et al. ([2018](#bib.bib21))). The feature maps are concatenated
    from both the top-down and the bottom-up path, ensuring the propagation of semantically
    rich localization information through to the final part of the network where the
    class probability and bounding box predictions are made. Each predicted bounding
    box consists of five elements: centre-x, centre-y, width, height, and confidence.
    The ($centre-x,centre-y)$ coordinates are relative to the dimensions of the predicted
    box; the width and height are relative to the whole image. The confidence score
    represents the likelihood that the cell contains the object as well as how confident
    the model is about its predictions.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: CNN自然获得金字塔状结构，随着每一层的图像从高分辨率到低分辨率。随着我们在CNN中变得更深，我们失去了输入的细粒度细节，这通常使得检测小对象更加困难。然而，随着分辨率降低，滤波器学习图像的更复杂的抽象，使特征图更富有语义。因此，结合来自较高分辨率层的特征图与更富有语义的特征图以便于在多个尺度上检测对象是可取的。这项任务交给特征聚合器，而YOLOv4使用了PANet建议的方法（Liu
    et al. ([2018](#bib.bib21)））。特征图从自上而下的路径和自下而上的路径连接在一起，确保了富有语义的定位信息传播到网络的最后部分，在那里进行了类别概率和边界框预测。每个预测的边界框由五个元素组成：中心-x、中心-y、宽度、高度和置信度。($center-x,center-y$)坐标是相对于预测框的尺寸；宽度和高度是相对于整个图像的。置信度分数表示单元格包含对象的可能性以及模型对其预测的置信度。
- en: 'YOLOv4 also updates the loss function to include Complete Intersection over
    Union (CIoU) loss (Zheng et al. ([2020](#bib.bib40))) to train the network to
    effectively determine the direction in which to shift the weights to better match
    the labelled bounding boxes. Finally, the model also includes a raft of new additions:
    a new activation function, MISH (Misra ([2020](#bib.bib22))) that provides smoother
    gradients; updates to the spatial attention module (Woo et al. ([2018](#bib.bib39)))
    and multi-input weighted residual connections (Tan et al. ([2020](#bib.bib34)))
    to better suit the architecture; and new data augmentation techniques, Mosaic
    and self-adversarial training.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv4还更新了损失函数，包括完全交并集（CIoU）损失（Zheng et al. ([2020](#bib.bib40)）），以训练网络有效确定将权重移至何处以更好地匹配标记的边界框。最后，该模型还包括一系列新的添加：新激活函数MISH（Misra
    ([2020](#bib.bib22)）提供更平滑的梯度；更新到空间注意力模块（Woo et al. ([2018](#bib.bib39)））和多输入加权残差连接（Tan
    et al. ([2020](#bib.bib34)）以更好地适应架构；以及新的数据增强技术，Mosaic和自对抗训练。
- en: The YOLO family of models are supported by Darknet (Redmon ([2013](#bib.bib26))),
    a custom framework written in C and CUDA and designed for fast object detection.
    The Darknet implementation of YOLOv4 was trained with the MOA-II dataset via the
    Google Colaboratory with a hosted GPU runtime environment. The model was first
    trained with the default anchor boxes, before k-means clustering was used to discover
    anchor boxes that might prove better suited to finding tracklets in astronomical
    data. After several variations were tested, the best combination of anchor boxes
    was discovered by hand-engineering the various clusters. The learning rate of
    0.001 was used along with a batch size of 32, with 8 subdivisions, 6000 max_batches,
    and with steps set to 4800,5400.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO系列模型由Darknet（Redmon ([2013](#bib.bib26)）支持，这是一个用C和CUDA编写的自定义框架，专门设计用于快速对象检测。
    YOLOv4的Darknet实现是通过托管的GPU运行时环境在Google Colaboratory通过MOA-II数据集进行训练的。该模型首先使用默认的anchor
    boxes进行训练，然后使用k-means聚类来发现更适合在天文数据中找到tracklets的anchor boxes。经过测试了多个变体后，最佳的anchor
    boxes组合是通过手工构建各个聚类来发现的。学习率为0.001，批量大小为32，划分为8，最大批量为6000，步数设置为4800,5400。
- en: 5 Results
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: 'To evaluate the classification networks, we rely on the metrics derived from
    a confusion matrix. A confusion matrix breaks down the predictions made by a classifier
    into 4 outcomes: True Positive (TP), True Negative (TN), False Positive (FP),
    and False Negative (FN). The ”positive” cases are where images were classified
    as containing tracklets and the ”negative” cases are where no tracklets were detected.
    In this case, the ideal scenarios would be to have as few false negatives as possible,
    together with a manageable number of false positives. Here, the networks are evaluated
    based on their recall, F2 score (the weighted harmonic mean of the precision and
    recall), and the PR AUC (area under the precision-recall curve).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估分类网络，我们依赖于从混淆矩阵得出的指标。混淆矩阵将分类器的预测结果分为四种结果：真正例（TP）、真负例（TN）、假正例（FP）和假负例（FN）。其中，“正”案例是指图像被分类为包含轨迹片段的情况，“负”案例是指没有检测到轨迹片段的情况。在这种情况下，理想的情形是尽量减少假负例，同时假正例的数量也应可控。在这里，网络根据其召回率、F2得分（精度和召回率的加权调和平均数）以及PR
    AUC（精度-召回曲线下面积）进行评估。
- en: '![[Uncaptioned image]](img/459c90a7913bf09bc63575d30f43a4c6.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/459c90a7913bf09bc63575d30f43a4c6.png)'
- en: 'Table 2: Evaluation metrics for the GB5-R5 test set taken at probability threshold
    0.5'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 在概率阈值0.5下，GB5-R5测试集的评估指标'
- en: 'The evaluation metrics for each classifier for the GB5-R5 test set are in Table
    [2](#S5.T2 "Table 2 ‣ 5 Results ‣ Towards Asteroid Detection in Microlensing Surveys
    with Deep Learning") and for the GB-All (28-06-2013) test set are in Table [3](#S5.T3
    "Table 3 ‣ 5 Results ‣ Towards Asteroid Detection in Microlensing Surveys with
    Deep Learning"). All of the metrics were taken at the 0.5 confidence threshold,
    with values over 0.5 indicating the presence of an asteroid tracklet in the image.
    Reviewing a combination of the PR AUC, F2 Score, and Recall, we can see the custom
    networks generalised well when making prediction about data from fields and chips
    they had never seen. Rather than selecting a single network from among these,
    all five custom classifiers were configured as an ensemble. Each network makes
    predictions about an input image and two approaches were trialled for selecting
    the winning prediction: averaging the predictions from all five classifiers (avg)
    or selecting the highest predicted value (max).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分类器在GB5-R5测试集的评估指标见表 [2](#S5.T2 "Table 2 ‣ 5 Results ‣ Towards Asteroid Detection
    in Microlensing Surveys with Deep Learning")，在GB-All (28-06-2013) 测试集的评估指标见表 [3](#S5.T3
    "Table 3 ‣ 5 Results ‣ Towards Asteroid Detection in Microlensing Surveys with
    Deep Learning")。所有指标均在0.5的置信度阈值下取得，值超过0.5表示图像中存在小行星轨迹片段。通过回顾PR AUC、F2得分和召回率的组合，我们可以看到自定义网络在对从未见过的数据进行预测时表现良好。与其从中选择一个单独的网络，不如将所有五个自定义分类器配置为一个集合。每个网络对输入图像进行预测，并试验了两种选择最佳预测的方法：对所有五个分类器的预测值取平均（avg）或选择最高的预测值（max）。
- en: '![[Uncaptioned image]](img/89908d5ea782e26eec32b96e1da28468.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/89908d5ea782e26eec32b96e1da28468.png)'
- en: 'Table 3: Evaluation metrics for the GB-ALL test set taken at probability threshold
    0.5'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 在概率阈值0.5下，GB-ALL测试集的评估指标'
- en: We see that while the max-ensemble results in a greater number of false positives,
    it improves the recall by four points (Table [4](#S5.T4 "Table 4 ‣ 5 Results ‣
    Towards Asteroid Detection in Microlensing Surveys with Deep Learning")) to 94.33%
    and 97.67% for the GB5-R5 and the GB-All test set, respectively. The avg-ensemble
    has the advantage of having far fewer false positives and a judiciously selected
    prediction threshold could see false negatives further minimised for this configuration.
    The ROC curves for the ensemble (Figure [11](#S5.F11 "Figure 11 ‣ 5 Results ‣
    Towards Asteroid Detection in Microlensing Surveys with Deep Learning")) further
    illustrate this trade-off.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，虽然最大集合结果导致了更多的假正例，但它使召回率提高了四个百分点（表 [4](#S5.T4 "Table 4 ‣ 5 Results ‣ Towards
    Asteroid Detection in Microlensing Surveys with Deep Learning")），GB5-R5 和 GB-All
    测试集的召回率分别达到了94.33%和97.67%。平均集合的优势在于假正例远少于最大集合，并且通过明智地选择预测阈值，可以进一步减少假负例。集合的ROC曲线（图
    [11](#S5.F11 "Figure 11 ‣ 5 Results ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning")）进一步说明了这一权衡。
- en: '![[Uncaptioned image]](img/d16c08743c126d60fa09a195305c17f9.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/d16c08743c126d60fa09a195305c17f9.png)'
- en: 'Table 4: Evaluation metrics after configuring the five custom networks as an
    ensemble taken at probability threshold 0.5'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 在将五个自定义网络配置为集合后，概率阈值0.5下的评估指标'
- en: '![Refer to caption](img/0d76ff2513a873d9c757395962222022.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d76ff2513a873d9c757395962222022.png)'
- en: 'Figure 11: ROC curves for the CNN ensemble'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：CNN 集成的 ROC 曲线
- en: '![Refer to caption](img/a9df04d2adad205689a389bf4a19e9dc.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a9df04d2adad205689a389bf4a19e9dc.png)'
- en: 'Figure 12: Tracklets localisation by YOLOv4, From left to right, these are:
    (538147)2016 BT90 (18.7); (67001) 1999 XN117 (19.2); (80667) 2000 BA15 (18.8);
    (206629) 2003 WT154 (19.0); and (281890) 2010 OA74 (20.1). Note that these images
    have been inverted and brightened to improve visibility (original in Figure [17](#A1.F17
    "Figure 17 ‣ Appendix A Original images ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning")).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：YOLOv4 的轨迹定位，从左到右分别为：（538147）2016 BT90 (18.7)；（67001）1999 XN117 (19.2)；（80667）2000
    BA15 (18.8)；（206629）2003 WT154 (19.0)；以及（281890）2010 OA74 (20.1)。注意这些图像已被反转和加亮以提高可见性（原始图像见图
    [17](#A1.F17 "Figure 17 ‣ Appendix A Original images ‣ Towards Asteroid Detection
    in Microlensing Surveys with Deep Learning")）。
- en: 'The performance of object detection models is typically quantified by the mean
    Average Precision or mAP. The average precision (AP) measures the trade-off between
    precision and recall and is calculated by integrating the area that falls under
    the precision-recall curve for each unique values of recall where the precision
    value decreases. The mAP is the mean of the AP across all object classes that
    the model can detect, which is identical to the AP for this dataset. Object detection
    models aim to have a high overlap between the predicted and the ground truth bounding
    boxes (Intersection of Union or IoU) and predictions are grouped as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测模型的性能通常通过平均精度均值（mAP）来量化。平均精度（AP）衡量了精度和召回率之间的权衡，通过对精度-召回曲线下的面积进行积分来计算，其中精度值降低。mAP
    是模型可以检测的所有目标类别的 AP 的均值，这与数据集的 AP 相同。目标检测模型旨在使预测的边界框与真实边界框之间有较高的重叠（交并比或 IoU），预测结果分组如下：
- en: '1.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'True Positive: IoU $>$ 0.5'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真阳性：IoU $>$ 0.5
- en: '2.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'False Positive: IoU $<$ 0.5 (or a duplicate)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假阳性：IoU $<$ 0.5（或重复）
- en: '3.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'False Negative: box not detected or the IoU $>$ 0.5 but the object is classified
    wrong'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假阴性：未检测到的框或 IoU $>$ 0.5 但物体分类错误
- en: 'The average precision is calculated as: Calculated as:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度的计算方法是：计算为：
- en: '|  | $AP=\Sigma(r_{(n+1)}-r_{n})\tilde{p}(r_{(n+1)})$ |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $AP=\Sigma(r_{(n+1)}-r_{n})\tilde{p}(r_{(n+1)})$ |  |'
- en: '|  | $\tilde{p}(r_{(n+1)})=\max_{\tilde{r}\geq r_{(n+1)}}(p(\tilde{r})),$ |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{p}(r_{(n+1)})=\max_{\tilde{r}\geq r_{(n+1)}}(p(\tilde{r})),$ |  |'
- en: where $r$ is the recall value, $n$ represents the locations where the precision
    decreases, and $\tilde{p}(r_{(n+1)})$ is the maximum precision where the $r$ value
    changes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r$ 是召回值，$n$ 代表精度降低的地点，$\tilde{p}(r_{(n+1)})$ 是 $r$ 值变化时的最大精度。
- en: YOLOv4 achieved an mAP of 90.96% with default anchor boxes and 90.95% with custom
    anchor boxes on the GB5-R5 data. Neither network could detect tracklets in the
    GB-All data, indicating that YOLOv4 will need to be trained with labelled data
    from other fields and chips before it can be used to make detections in them.
    Some of the detections made by YOLOv4 on the GB5-R5 dataset can be seen in Figure
    [12](#S5.F12 "Figure 12 ‣ 5 Results ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning").
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv4 在 GB5-R5 数据上使用默认锚框取得了 90.96% 的 mAP，使用自定义锚框取得了 90.95% 的 mAP。两个网络在 GB-All
    数据中都无法检测轨迹，这表明 YOLOv4 需要使用来自其他领域和芯片的标记数据进行训练，才能在这些数据中进行检测。YOLOv4 在 GB5-R5 数据集上的一些检测结果可以在图
    [12](#S5.F12 "Figure 12 ‣ 5 Results ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning") 中看到。
- en: The proposed model was applied towards discovering tracklets in the 543,595
    images tiles from GB5-R5 that had no known asteroid tracklets. The ensemble could
    analyse 10,000 images in an hour and took three days to analyse all the images
    and proposed 50,227 candidate detections. Both version of YOLOv4 were then used
    for localizing tracklets in these candidate detections, with each version taking
    1.5 days to make bounding box predictions. We are still analysing these candidate
    detections to determine if any new objects are present.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的模型应用于发现 GB5-R5 中 543,595 张图像切片中的轨迹，这些图像中没有已知的小行星轨迹。集成模型每小时可以分析 10,000 张图像，花费了三天时间分析所有图像，并提出了
    50,227 个候选检测。然后，使用 YOLOv4 的两个版本对这些候选检测中的轨迹进行定位，每个版本花费了 1.5 天进行边界框预测。我们仍在分析这些候选检测以确定是否存在新的物体。
- en: 6 Discussion
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: Our classifier ensemble, trained with a relatively small amount of labelled
    data from just one chip in one survey field, was able to classify a set of images
    from unseen survey fields and chips, highlighting that it generalizes well. The
    success of the custom CNN architectures with fewer training parameters is perhaps
    down to two aspects. First, the networks might be at the optimal depth for learning
    to identify the pattern of fuzzy blobs that represent a tracklet. Second, the
    small size of the input images perhaps leads to more overfitting in the larger
    models. Further, rather than adding more layers, the hybrid architectures lean
    into adding complexity with several 1x1 convolutions. It is possible this caused
    the networks to learn representations that ultimately boosted the performance
    of the ensemble.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类器集成模型，使用了来自一个调查领域的一个芯片上的相对少量标记数据进行训练，能够对来自未见过的调查领域和芯片的一组图像进行分类，这突显了其良好的泛化能力。定制的
    CNN 架构成功的原因可能归结为两个方面。首先，这些网络可能在识别表示轨迹的模糊斑块的模式时达到了最佳深度。其次，输入图像的较小尺寸可能导致了较大模型的过拟合。此外，与其添加更多层，混合架构更倾向于通过多个
    1x1 卷积增加复杂性。这可能导致网络学习到的表示最终提高了集成模型的性能。
- en: The false negatives (Figure [14](#S6.F14 "Figure 14 ‣ 6 Discussion ‣ Towards
    Asteroid Detection in Microlensing Surveys with Deep Learning")) reported by the
    ensemble include instances where the tracklets resemble satellite streaks or where
    the point sources are slightly further apart and obscured by noise. In some cases,
    parts of the tracklet are on other images that have been correctly identified
    as a candidate detection by the ensemble. As to the false positives, they are
    largely cases where noise and other spurious artefacts have been mistaken for
    tracklets, as indicated in Figure [13](#S6.F13 "Figure 13 ‣ 6 Discussion ‣ Towards
    Asteroid Detection in Microlensing Surveys with Deep Learning"). These also lead
    to false detections by the object detector. We believe that fine tuning the network
    models as more labelled data becomes available will aid in minimising misclassifications
    such as these.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型报告的假阴性（见图 [14](#S6.F14 "Figure 14 ‣ 6 Discussion ‣ Towards Asteroid Detection
    in Microlensing Surveys with Deep Learning")）包括轨迹类似于卫星拖尾的情况，或者点源稍微分散且被噪声遮掩的情况。在一些情况下，轨迹的部分位于其他图像上，这些图像被集成模型正确识别为候选探测结果。至于假阳性，它们大多是噪声和其他虚假伪影被误认为轨迹的情况，如图
    [13](#S6.F13 "Figure 13 ‣ 6 Discussion ‣ Towards Asteroid Detection in Microlensing
    Surveys with Deep Learning") 所示。这些情况也导致了物体检测器的假检测。我们认为，随着更多标记数据的可用，微调网络模型将有助于减少此类误分类。
- en: The YOLOv4 object detector proved to be up to the task of localizing tracklets
    in the GB5-R5 dataset but does not generalize to unseen data. The results from
    the GB5-R5 set are promising and thus indicate that further training with data
    from the other survey fields and chips will be beneficial.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv4 物体检测器证明能够完成 GB5-R5 数据集中轨迹定位的任务，但无法对未见过的数据进行泛化。来自 GB5-R5 数据集的结果令人鼓舞，因此表明进一步使用其他调查领域和芯片的数据进行训练将是有益的。
- en: While both types of networks are currently limited by the high cadence observations
    they have been trained with, we are confident that, as the networks are retrained
    with data from other fields, they will learn to identity tracklets of slow objects
    or those with longer intervals between observations. Further, other surveys could
    use our networks, together with their pre-trained weights, as a starting point
    for discovering asteroids in their data. In this case, we recommend that the other
    surveys retrain/ fine-tune the architectures with their own data for optimal results.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两种类型的网络目前受到其训练时使用的高频观察数据的限制，但我们相信，随着网络使用来自其他领域的数据进行再训练，它们将能够识别缓慢移动的物体轨迹或观察间隔较长的轨迹。此外，其他调查可以将我们的网络及其预训练权重作为发现其数据中小行星的起点。在这种情况下，我们建议其他调查使用自己的数据对架构进行再训练/微调，以获得最佳结果。
- en: '![Refer to caption](img/b0fe4be706c948b316f32dad3ca56d13.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b0fe4be706c948b316f32dad3ca56d13.png)'
- en: 'Figure 13: The false positives reported by the classification networks fall
    into four broad categories: (a) ghosts (short for ghosts of noise past), are the
    most common false positives and are a result of the additive noise from creating
    the composite images; (b) streaks are either satellites or other near-Earth objects
    or cosmic rays; (c) chimera are optical artefacts potentially caused by over-saturated
    stars; and (d) loki objects are artefacts that move around erratically from one
    observation to the next. Note that these images have been inverted and brightened
    to improve visibility.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：分类网络报告的假阳性分为四大类：（a）幽灵（ghosts，指噪声过去的幽灵），是最常见的假阳性，是由于生成合成图像时的附加噪声；（b）条纹是卫星或其他近地物体或宇宙射线；（c）奇美拉（chimera）是可能由过饱和星星引起的光学伪影；（d）洛基（loki）物体是从一次观测到下一次观测之间运动不规则的伪影。请注意，这些图像已经被翻转并加亮以提高可见性。
- en: '![Refer to caption](img/736b852c0aec818eb5f9e255b5994641.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/736b852c0aec818eb5f9e255b5994641.png)'
- en: 'Figure 14: The false negatives reported by the classification networks are
    mostly either faint objects or too much like streaks. It is likely that these
    sorts of tracklets were not well represented in the training set. Note that these
    images have been inverted and brightened to improve visibility.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：分类网络报告的假阴性大多是微弱的物体或像条纹一样的物体。这些轨迹可能在训练集中表现得不好。请注意，这些图像已经被翻转并加亮以提高可见性。
- en: 7 Conclusion
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We have shown that it is possible to train both CNN-based classifiers as well
    as the YOLOv4 object detector to find asteroid tracklets in the MOA difference
    images. The classifier ensemble proved resilient to discovering tracklets in unseen
    data and will be invaluable for extending the search for asteroids to the rest
    of the MOA-II archival data. The networks will be fine-tuned or retrained as more
    labelled data is available and we will investigate automating the bounding boxes
    required for training YOLOv4\. We will also investigate working with orbit-linking
    software such as HelioLinC to determine the validity of the source clusters localized
    by YOLOv4.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经证明，可以训练基于CNN的分类器和YOLOv4目标检测器来在MOA差分图像中找到小行星轨迹。分类器集群在发现未知数据中的轨迹方面表现出韧性，将对扩展小行星搜索到MOA-II归档数据的其余部分具有重要价值。随着更多标记数据的可用，网络将进行微调或重新训练，我们将研究自动化YOLOv4所需的边界框。我们还将研究使用诸如HelioLinC的轨道链接软件，以确定YOLOv4所定位源簇的有效性。
- en: While the classifiers performed well, there is potential for further improvement
    and development. Investigating effective denoising techniques for the stacked
    images or the difference images would lead to an immediate performance boost for
    both the classification and object detection networks. The classification network
    may benefit from having two inputs - perhaps the subtracted stack image along
    with the median stack image. This would provide the model with additional information
    it could use to distinguish between images with or without tracklets. Training
    the YOLO backbone feature extractor with the classification data first might also
    lead to better results. Additionally, since much of the salient information is
    contained in the first layer, a small CNN based on DenseNet could also potentially
    be successfully trained as a classifier.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分类器表现良好，但仍有进一步改进和发展的潜力。研究有效的去噪技术对堆叠图像或差分图像进行去噪将立即提升分类和目标检测网络的性能。分类网络可能从两个输入中受益——也许是减法堆叠图像以及中位数堆叠图像。这将为模型提供额外的信息，以帮助区分有轨迹和无轨迹的图像。首先使用分类数据训练YOLO骨干特征提取器也可能会带来更好的结果。此外，由于大量显著信息包含在第一层中，基于DenseNet的小型CNN也有可能被成功训练为分类器。
- en: Overall, we have presented an effective toolkit for finding asteroids tracklets
    in the archival data of ground-based telescopes. The code for our neural network
    models as well as the trained weights are available at https://github.com/pcowan-astro/MOA-Asteroids.
    Our methodology and network architectures can be used to discover and recover
    asteroids in other archival survey data as well as to strengthen the analysis
    pipeline for current and future surveys.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们提出了一种有效的工具包，用于在地面望远镜的归档数据中寻找小行星轨迹。我们神经网络模型的代码以及训练好的权重可以在 https://github.com/pcowan-astro/MOA-Asteroids
    获取。我们的方法和网络架构可以用于在其他归档调查数据中发现和恢复小行星，并加强当前和未来调查的分析管道。
- en: 8 Acknowledgements
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 致谢
- en: We thank the MOA collaboration for use of the MOA-II archival difference images.
    IAB acknowledges support from grant MAU1901 from the Royal Society of New Zealand
    - Marsden. This research has made use of data and/or services provided by the
    International Astronomical Union’s Minor Planet Center.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 MOA 合作组织提供 MOA-II 档案差分图像。IAB 感谢新西兰皇家学会 - Marsden 资助的 MAU1901 项目支持。本研究使用了国际天文学联合会小行星中心提供的数据和/或服务。
- en: Appendix A Original images
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 原始图像
- en: '![Refer to caption](img/ecac62755887456510bb088ffddefc61.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ecac62755887456510bb088ffddefc61.png)'
- en: 'Figure 15: A stack of all 51 observations from the night of 15-May-2008 reveals
    5 asteroid tracklets clearly visible in a sub-region of the stack image. Numbered
    in order of appearance, these are: 1 - (103842) 2000 DQ33 (19.5); 2 - (148657)
    2001 SX124 (19.6); 3 - (152083) 2004 RH30 (19.9); 4 - (582743) 2016 AT221 (20.4);
    5 - (338789) 2005 SZ154 (20.4). The line/streak on the top left is from a satellite.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: 从 2008 年 5 月 15 日夜晚的所有 51 次观测中堆叠的图像显示了在堆叠图像的子区域中清晰可见的 5 条小行星轨迹。按出现顺序编号，它们是：1
    - (103842) 2000 DQ33 (19.5); 2 - (148657) 2001 SX124 (19.6); 3 - (152083) 2004
    RH30 (19.9); 4 - (582743) 2016 AT221 (20.4); 5 - (338789) 2005 SZ154 (20.4)。左上方的线/条纹来自卫星。'
- en: '![Refer to caption](img/3ae9ddd1faa1837817b184ebb866011a.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3ae9ddd1faa1837817b184ebb866011a.png)'
- en: 'Figure 16: Asteroid tracklets seen in 128x128 sub-regions from subtracted stack
    images. In (b), the tracklets are localized with bounding boxes. Asteroids in
    the top row, left to right: (38102) 1999 JM18 (18.4); (152083) 2004 RH30 (19.9);
    (103842) 2000 DQ33 (19.5); (283261) 2011 FR142 (20.0); and (375674) 2009 HD5 (19.8).
    Asteroids in the bottom row, from left to right: (48617) 1995 HR2 (19.5); (148657)
    2001 SX124 (19.6); (97948) 2000 QF124 (19.6); and (74978) 1999 TY234 (18.7).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16: 从减法叠加图像中看到的 128x128 子区域的小行星轨迹。在（b）中，这些轨迹用边界框进行标定。上排小行星，从左到右： (38102)
    1999 JM18 (18.4); (152083) 2004 RH30 (19.9); (103842) 2000 DQ33 (19.5); (283261)
    2011 FR142 (20.0); 和 (375674) 2009 HD5 (19.8)。下排小行星，从左到右： (48617) 1995 HR2 (19.5);
    (148657) 2001 SX124 (19.6); (97948) 2000 QF124 (19.6); 和 (74978) 1999 TY234 (18.7)。'
- en: '![Refer to caption](img/972c331f153ab278cc069da2e4caaf24.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/972c331f153ab278cc069da2e4caaf24.png)'
- en: 'Figure 17: Tracklets localisation by YOLOv4, From left to right, these are:
    (538147)2016 BT90 (18.7); (67001) 1999 XN117 (19.2); (80667) 2000 BA15 (18.8);
    (206629) 2003 WT154 (19.0); and (281890) 2010 OA74 (20.1).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '图 17: YOLOv4 的轨迹定位，从左到右： (538147)2016 BT90 (18.7); (67001) 1999 XN117 (19.2);
    (80667) 2000 BA15 (18.8); (206629) 2003 WT154 (19.0); 和 (281890) 2010 OA74 (20.1)。'
- en: References
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Alard (2000) Alard, C., 2000. Image subtraction using a space-varying kernel.
    Astronomy and Astrophysics Supplement 144, 363–370.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alard（2000）Alard, C., 2000. 使用空间变异核的图像减法。《天文学与天体物理学补刊》144, 363–370.
- en: Alard and Lupton (1998) Alard, C., Lupton, R.H., 1998. A Method for Optimal
    Image Subtraction. The Astrophysical Journal 503, 325–331.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alard 和 Lupton（1998）Alard, C., Lupton, R.H., 1998. 最佳图像减法方法。《天体物理学杂志》503, 325–331.
- en: 'Bochkovskiy et al. (2020) Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M., 2020.
    YOLOv4: Optimal Speed and Accuracy of Object Detection. ArXiv abs/2004.10934.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bochkovskiy 等（2020）Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M., 2020. YOLOv4:
    目标检测的最佳速度和精度。ArXiv abs/2004.10934.'
- en: Bond et al. (2001) Bond, I., Abe, F., Dodd, R., Hearnshaw, J., Honda, M., Jugaku,
    J., Kilmartin, P., Marles, A., Masuda, K., Matsubara, Y., Muraki, Y., Nakamura,
    T., Nankivell, G., Noda, S., Noguchi, C., Ohnishi, K., Rattenbury, N., Reid, M.,
    Saito, T., Sato, H., Sekiguchi, M., Skuljan, J., Sullivan, D., Sumi, T., Takeuti,
    M., Watase, Y., Wilkinson, S., Yamada, R., Yanagisawa, T., Yock, P., 2001. Real-time
    difference imaging analysis of MOA Galactic bulge observations during 2000. Monthly
    Notices of the Royal Astronomical Society 327, 868–880.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bond 等（2001）Bond, I., Abe, F., Dodd, R., Hearnshaw, J., Honda, M., Jugaku, J.,
    Kilmartin, P., Marles, A., Masuda, K., Matsubara, Y., Muraki, Y., Nakamura, T.,
    Nankivell, G., Noda, S., Noguchi, C., Ohnishi, K., Rattenbury, N., Reid, M., Saito,
    T., Sato, H., Sekiguchi, M., Skuljan, J., Sullivan, D., Sumi, T., Takeuti, M.,
    Watase, Y., Wilkinson, S., Yamada, R., Yanagisawa, T., Yock, P., 2001. 实时差分成像分析
    MOA 银河核区观测数据（2000年）。《皇家天文学会月刊》327, 868–880.
- en: Bramich (2008) Bramich, D., 2008. A new algorithm for difference image analysis.
    Monthly Notices of the Royal Astronomical Society 386, L77–L81.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bramich（2008）Bramich, D., 2008. 一种新的差分图像分析算法。《皇家天文学会月刊》386, L77–L81.
- en: 'Cordwell et al. (2022) Cordwell, A.J., Rattenbury, N.J., Bannister, M.T., Cowan,
    P., Collaboration:, T.M., Abe, F., Barry, R., Bennett, D.P., Bhattacharya, A.,
    Bond, I.A., Fujii, H., Fukui, A., Itow, Y., Silva, S.I., Hirao, Y., Kirikawa,
    R., Kondo, I., Koshimoto, N., Matsubara, Y., Matsumoto, S., Muraki, Y., Miyazaki,
    S., Okamura, A., Ranc, C., Satoh, Y., Sumi, T., Suzuki, D., Tristram, P.J., Toda,
    T., Yama, H., Yonehara, A., 2022. Asteroid Lightcurves from the MOA-II Survey:
    a pilot study. Monthly Notices of the Royal Astronomical Society 514, 3098–3112.
    doi:[10.1093/mnras/stac674](http://dx.doi.org/10.1093/mnras/stac674).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cordwell et al. (2022) Cordwell, A.J., Rattenbury, N.J., Bannister, M.T., Cowan,
    P., Collaboration:, T.M., Abe, F., Barry, R., Bennett, D.P., Bhattacharya, A.,
    Bond, I.A., Fujii, H., Fukui, A., Itow, Y., Silva, S.I., Hirao, Y., Kirikawa,
    R., Kondo, I., Koshimoto, N., Matsubara, Y., Matsumoto, S., Muraki, Y., Miyazaki,
    S., Okamura, A., Ranc, C., Satoh, Y., Sumi, T., Suzuki, D., Tristram, P.J., Toda,
    T., Yama, H., Yonehara, A., 2022. MOA-II调查中的小行星光变曲线：一项初步研究。《皇家天文学会月刊》514, 3098–3112.
    doi:[10.1093/mnras/stac674](http://dx.doi.org/10.1093/mnras/stac674).
- en: Denneau et al. (2013) Denneau, L., Jedicke, R., Grav, T., Granvik, M., Kubica,
    J., Milani, A., Vereš, P., Wainscoat, R., Chang, D., Pierfederici, F., Kaiser,
    N., Chambers, K.C., Heasley, J.N., Magnier, E.A., Price, P.A., Myers, J., Kleyna,
    J., Hsieh, H., Farnocchia, D., Waters, C., Sweeney, W.H., Green, D., Bolin, B.,
    Burgett, W.S., Morgan, J.S., Tonry, J.L., Hodapp, K.W., Chastel, S., Chesley,
    S., Fitzsimmons, A., Holman, M., Spahr, T., Tholen, D., Williams, G.V., Abe, S.,
    Armstrong, J.D., Bressi, T.H., Holmes, R., Lister, T., McMillan, R.S., Micheli,
    M., Ryan, E.V., Ryan, W.H., Scotti, J.V., 2013. The Pan-STARRS Moving Object Processing
    System. Publications of the Astronomical Society of the Pacific 125, 357–395.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denneau et al. (2013) Denneau, L., Jedicke, R., Grav, T., Granvik, M., Kubica,
    J., Milani, A., Vereš, P., Wainscoat, R., Chang, D., Pierfederici, F., Kaiser,
    N., Chambers, K.C., Heasley, J.N., Magnier, E.A., Price, P.A., Myers, J., Kleyna,
    J., Hsieh, H., Farnocchia, D., Waters, C., Sweeney, W.H., Green, D., Bolin, B.,
    Burgett, W.S., Morgan, J.S., Tonry, J.L., Hodapp, K.W., Chastel, S., Chesley,
    S., Fitzsimmons, A., Holman, M., Spahr, T., Tholen, D., Williams, G.V., Abe, S.,
    Armstrong, J.D., Bressi, T.H., Holmes, R., Lister, T., McMillan, R.S., Micheli,
    M., Ryan, E.V., Ryan, W.H., Scotti, J.V., 2013. Pan-STARRS移动物体处理系统。《太平洋天文学会出版物》125,
    357–395.
- en: 'Duev et al. (2021) Duev, D.A., Bolin, B.T., Graham, M.J., Kelley, M.S.P., Mahabal,
    A., Bellm, E.C., Coughlin, M.W., Dekany, R., Helou, G., Kulkarni, S.R., Masci,
    F.J., Prince, T.A., Riddle, R., Soumagnac, M.T., van der Walt, S.J., 2021. Tails:
    Chasing Comets with the Zwicky Transient Facility and Deep Learning. The Astronomical
    Journal 161, 218.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duev et al. (2021) Duev, D.A., Bolin, B.T., Graham, M.J., Kelley, M.S.P., Mahabal,
    A., Bellm, E.C., Coughlin, M.W., Dekany, R., Helou, G., Kulkarni, S.R., Masci,
    F.J., Prince, T.A., Riddle, R., Soumagnac, M.T., van der Walt, S.J., 2021. Tails:
    使用Zwicky瞬态设施和深度学习追踪彗星。《天文学杂志》161, 218.'
- en: 'Duev et al. (2019) Duev, D.A., Mahabal, A., Ye, Q., Tirumala, K., Belicki,
    J., Dekany, R., Frederick, S., Graham, M.J., Laher, R.R., Masci, F.J., Prince,
    T.A., Riddle, R., Rosnet, P., Soumagnac, M.T., 2019. DeepStreaks: Identifying
    fast-moving objects in the Zwicky Transient Facility data with deep learning.
    Monthly Notices of the Royal Astronomical Society , 4158–4165.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duev et al. (2019) Duev, D.A., Mahabal, A., Ye, Q., Tirumala, K., Belicki,
    J., Dekany, R., Frederick, S., Graham, M.J., Laher, R.R., Masci, F.J., Prince,
    T.A., Riddle, R., Rosnet, P., Soumagnac, M.T., 2019. DeepStreaks: 在Zwicky瞬态设施数据中使用深度学习识别快速移动物体。《皇家天文学会月刊》，4158–4165.'
- en: Glorot et al. (2011) Glorot, X., Bordes, A., Bengio, Y., 2011. Deep Sparse Rectifier
    Neural Networks. AISTATS .
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glorot et al. (2011) Glorot, X., Bordes, A., Bengio, Y., 2011. 深度稀疏修正神经网络。AISTATS.
- en: Gould and Yee (2013) Gould, A., Yee, J.C., 2013. Microlens surveys are a powerful
    probe of asteroids. Astrophysical Journal 767.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gould and Yee (2013) Gould, A., Yee, J.C., 2013. 微透镜调查是探测小行星的强大工具。《天体物理学杂志》767.
- en: He et al. (2014) He, K., Zhang, X., Ren, S., Sun, J., 2014. Spatial Pyramid
    Pooling in Deep Convolutional Networks for Visual Recognition. Lecture Notes in
    Computer Science (including subseries Lecture Notes in Artificial Intelligence
    and Lecture Notes in Bioinformatics) 8691 LNCS, 346–361.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2014) He, K., Zhang, X., Ren, S., Sun, J., 2014. 深度卷积网络中的空间金字塔池化。计算机科学讲义（包括人工智能讲义和生物信息学讲义子系列）8691
    LNCS, 346–361.
- en: He et al. (2016) He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep Residual Learning
    for Image Recognition. IEEE Conference on Computer Vision and Pattern Recognition
    , 770–778.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016) He, K., Zhang, X., Ren, S., Sun, J., 2016. 用于图像识别的深度残差学习。IEEE计算机视觉与模式识别会议,
    770–778.
- en: 'Holman et al. (2018) Holman, M., Payne, M., Blankley, P., Janssen, R., Kuindersma,
    S., 2018. Heliolinc: A novel approach to the minor planet linking problem. The
    Astronomical Journal 156, 135. doi:[10.3847/1538-3881/aad69a](http://dx.doi.org/10.3847/1538-3881/aad69a).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Holman et al. (2018) Holman, M., Payne, M., Blankley, P., Janssen, R., Kuindersma,
    S., 2018. Heliolinc: 解决小行星链接问题的新方法。天文杂志 156, 135. doi:[10.3847/1538-3881/aad69a](http://dx.doi.org/10.3847/1538-3881/aad69a)。'
- en: Huang et al. (2016) Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.,
    2016. Densely Connected Convolutional Networks. Proceedings - 30th IEEE Conference
    on Computer Vision and Pattern Recognition, CVPR 2017 2017-January, 2261–2269.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2016) Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.,
    2016. 密集连接卷积网络。第30届IEEE计算机视觉与模式识别会议论文集，CVPR 2017 2017年1月，2261–2269。
- en: 'Kim et al. (2016) Kim, S.L., Lee, C.U., Park, B.G., Kim, D.J., Cha, S.M., Lee,
    Y., Han, C., Chun, M.Y., Yuk, I., 2016. KMTNET: A Network of 1.6 m Wide-Field
    Optical Telescopes Installed at Three Southern Observatories. Journal of Korean
    Astronomical Society 49, 37–44. doi:[10.5303/JKAS.2016.49.1.037](http://dx.doi.org/10.5303/JKAS.2016.49.1.037).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim et al. (2016) Kim, S.L., Lee, C.U., Park, B.G., Kim, D.J., Cha, S.M., Lee,
    Y., Han, C., Chun, M.Y., Yuk, I., 2016. KMTNET: 一种在三个南方观测站安装的1.6米广场光学望远镜网络。韩国天文学会杂志
    49, 37–44. doi:[10.5303/JKAS.2016.49.1.037](http://dx.doi.org/10.5303/JKAS.2016.49.1.037)。'
- en: 'Kingma and Welling (2014) Kingma, D.P., Welling, M., 2014. Auto-encoding variational
    bayes, in: 2nd International Conference on Learning Representations, ICLR 2014
    - Conference Track Proceedings, International Conference on Learning Representations,
    ICLR.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma and Welling (2014) Kingma, D.P., Welling, M., 2014. 自编码变分贝叶斯，见：第二届国际学习表征会议，ICLR
    2014 - 会议论文集，国际学习表征会议，ICLR。
- en: Kruk et al. (2022) Kruk, S., García Martín, P., Popescu, M., Merín, B., Mahlke,
    M., Carry, B., Thomson, R., Karadağ, S., Durán, J., Racero, E., Giordano, F.,
    Baines, D., de Marchi, G., Laureijs, R., 2022. Hubble Asteroid Hunter. Astronomy
    & Astrophysics 661, A85.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kruk et al. (2022) Kruk, S., García Martín, P., Popescu, M., Merín, B., Mahlke,
    M., Carry, B., Thomson, R., Karadağ, S., Durán, J., Racero, E., Giordano, F.,
    Baines, D., de Marchi, G., Laureijs, R., 2022. 哈勃小行星猎人。天文学与天体物理学 661, A85。
- en: Kubica et al. (2007) Kubica, J., Denneau Jr, L., Moore, A., Jedicke, Robert,
    Connolly, A., 2007. Efficient Algorithms for Large-Scale Asteroid Discovery. Astronomical
    Data Analysis Software and Systems XVI ASP Conference Series 376, 395–404.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubica et al. (2007) Kubica, J., Denneau Jr, L., Moore, A., Jedicke, Robert,
    Connolly, A., 2007. 大规模小行星发现的高效算法。天文数据分析软件与系统第十六届ASP会议系列 376, 395–404。
- en: Lieu et al. (2018) Lieu, M., Conversi, L., Altieri, B., Carry, B., 2018. Detecting
    solar system objects with convolutional neural networks. Monthly Notices of the
    Royal Astronomical Society 485, 5831–5842.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lieu et al. (2018) Lieu, M., Conversi, L., Altieri, B., Carry, B., 2018. 使用卷积神经网络检测太阳系天体。皇家天文学会月刊
    485, 5831–5842。
- en: Liu et al. (2018) Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., 2018. Path Aggregation
    Network for Instance Segmentation. Proceedings of the IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition , 8759–8768.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2018) Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., 2018. 实例分割的路径聚合网络。IEEE计算机学会计算机视觉与模式识别会议论文集，8759–8768。
- en: 'Misra (2020) Misra, D., 2020. Mish: A Self Regularized Non-Monotonic Activation
    Function. ArXiv abs/1908.08681.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Misra (2020) Misra, D., 2020. Mish: 自我正则化非单调激活函数。ArXiv abs/1908.08681。'
- en: Nair and Hinton (2010) Nair, V., Hinton, G.E., 2010. Rectified Linear Units
    Improve Restricted Boltzmann Machines. ICML .
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair and Hinton (2010) Nair, V., Hinton, G.E., 2010. 修正线性单元改善限制玻尔兹曼机。ICML。
- en: Rabeendran and Denneau (2021) Rabeendran, A.C., Denneau, L., 2021. A Two-Stage
    Deep Learning Detection Classifier for the ATLAS Asteroid Survey. Publications
    of the Astronomical Society of the Pacific 133.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabeendran and Denneau (2021) Rabeendran, A.C., Denneau, L., 2021. ATLAS小行星调查的双阶段深度学习检测分类器。太平洋天文学会出版物
    133。
- en: Rabinowitz (1991) Rabinowitz, D.L., 1991. Detection of earth-approaching asteroids
    in near real time. The Astronomical Journal 101, 1518.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabinowitz (1991) Rabinowitz, D.L., 1991. 实时检测接近地球的小行星。天文杂志 101, 1518。
- en: 'Redmon (2013) Redmon, J., 2013. Darknet: Open source neural networks in c.
    URL: [https://pjreddie.com/darknet/](https://pjreddie.com/darknet/).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Redmon (2013) Redmon, J., 2013. Darknet: 开源神经网络的C语言实现。网址：[https://pjreddie.com/darknet/](https://pjreddie.com/darknet/)。'
- en: 'Redmon et al. (2016) Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016.
    You Only Look Once: Unified, Real-Time Object Detection, in: IEEE Conference on
    Computer Vision and Pattern Recognition (CVPR), pp. 779–788.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redmon et al. (2016) Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016.
    你只看一次：统一的实时物体检测，见：IEEE计算机视觉与模式识别会议（CVPR），第779–788页。
- en: 'Redmon and Farhadi (2017) Redmon, J., Farhadi, A., 2017. YOLO9000: Better,
    Faster, Stronger, in: IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 6517–6525.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Redmon 和 Farhadi (2017) Redmon, J., Farhadi, A., 2017. YOLO9000: 更好、更快、更强，见：IEEE
    计算机视觉与模式识别会议 (CVPR)，pp. 6517–6525。'
- en: 'Redmon and Farhadi (2018) Redmon, J., Farhadi, A., 2018. YOLOv3: An Incremental
    Improvement. ArXiv abs/1804.02767.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Redmon 和 Farhadi (2018) Redmon, J., Farhadi, A., 2018. YOLOv3: 增量改进。ArXiv abs/1804.02767。'
- en: 'Sako et al. (2008) Sako, T., Sekiguchi, T., Sasaki, M., Okajima, K., Abe, F.,
    Bond, I.A., Hearnshaw, J.B., Itow, Y., Kamiya, K., Kilmartin, P.M., Masuda, K.,
    Matsubara, Y., Muraki, Y., Rattenbury, N.J., Sullivan, D.J., Sumi, T., Tristram,
    P., Yanagisawa, T., Yock, P.C., 2008. MOA-cam3: A wide-field mosaic CCD camera
    for a gravitational microlensing survey in New Zealand. Experimental Astronomy
    22, 51–66.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sako 等人 (2008) Sako, T., Sekiguchi, T., Sasaki, M., Okajima, K., Abe, F., Bond,
    I.A., Hearnshaw, J.B., Itow, Y., Kamiya, K., Kilmartin, P.M., Masuda, K., Matsubara,
    Y., Muraki, Y., Rattenbury, N.J., Sullivan, D.J., Sumi, T., Tristram, P., Yanagisawa,
    T., Yock, P.C., 2008. MOA-cam3: 用于新西兰引力微透镜调查的广域马赛克 CCD 相机。实验天文学 22, 51–66。'
- en: 'Simonyan and Zisserman (2015) Simonyan, K., Zisserman, A., 2015. Very deep
    convolutional networks for large-scale image recognition, in: 3rd International
    Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman (2015) Simonyan, K., Zisserman, A., 2015. 用于大规模图像识别的非常深度卷积网络，见：第三届国际学习表征会议，ICLR
    2015 - 会议论文集。
- en: Sumi et al. (2003) Sumi, T., Abe, F., Bond, I., Dodd, R., Hearnshaw, J., Honda,
    M., Honma, M., Kan-ya, Y., Kilmartin, P., Masuda, K., Matsubara, Y., Muraki, Y.,
    Nakamura, T., Nishi, R., Noda, S., Ohnishi, K., Petterson, O., Rattenbury, N.,
    Reid, M., Saito, T., Saito, Y., Sato, H., Sekiguchi, M., Skuljan, J., Sullivan,
    D., Takeuti, M., Tristram, P., Wilkinson, S., Yanagisawa, T., Yock, P., 2003.
    Microlensing Optical Depth toward the Galactic Bulge from Microlensing Observations
    in Astrophysics Group Observations during 2000 with Difference Image Analysis.
    The Astrophysical Journal 591, 204–227.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sumi 等人 (2003) Sumi, T., Abe, F., Bond, I., Dodd, R., Hearnshaw, J., Honda,
    M., Honma, M., Kan-ya, Y., Kilmartin, P., Masuda, K., Matsubara, Y., Muraki, Y.,
    Nakamura, T., Nishi, R., Noda, S., Ohnishi, K., Petterson, O., Rattenbury, N.,
    Reid, M., Saito, T., Saito, Y., Sato, H., Sekiguchi, M., Skuljan, J., Sullivan,
    D., Takeuti, M., Tristram, P., Wilkinson, S., Yanagisawa, T., Yock, P., 2003.
    通过天体物理学小组在 2000 年的差分图像分析观测，测量银河系核心方向的微透镜光学深度。《天体物理学杂志》 591, 204–227。
- en: Szegedy et al. (2015) Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna,
    Z., 2015. Rethinking the Inception Architecture for Computer Vision. Proceedings
    of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition
    2016-Decem, 2818–2826.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等人 (2015) Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna,
    Z., 2015. 重新思考计算机视觉中的 Inception 架构。IEEE 计算机学会计算机视觉与模式识别会议论文集 2016-Decem, 2818–2826。
- en: 'Tan et al. (2020) Tan, M., Pang, R., Le, Q.V., 2020. EfficientDet: Scalable
    and efficient object detection, in: Proceedings of the IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition, pp. 10778–10787.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan 等人 (2020) Tan, M., Pang, R., Le, Q.V., 2020. EfficientDet: 可扩展且高效的目标检测，见：IEEE
    计算机学会计算机视觉与模式识别会议论文集，pp. 10778–10787。'
- en: Tomaney and Crotts (1996) Tomaney, A.B., Crotts, A.P., 1996. Expanding the Realm
    of Microlensing Surveys with Difference Image Photometry. Astronomical Journal
    112, 2872.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tomaney 和 Crotts (1996) Tomaney, A.B., Crotts, A.P., 1996. 通过差分图像光度学扩展微透镜调查的范围。《天文学杂志》
    112, 2872。
- en: Udalski et al. (1993) Udalski, A., Szymanski, M., Kaluzny, J., Kubiak, M., Krzeminski,
    W., Mateo, M., Preston, G.W., Paczynski, B., 1993. The Optical Gravitational Lensing
    Experiment. Discovery of the First Candidate Microlensing Event in the Direction
    of the Galactic Bulge. Acta Astronomica 43, 289–294.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Udalski 等人 (1993) Udalski, A., Szymanski, M., Kaluzny, J., Kubiak, M., Krzeminski,
    W., Mateo, M., Preston, G.W., Paczynski, B., 1993. 光学引力透镜实验。发现银河系核心方向的第一个候选微透镜事件。天文学学报
    43, 289–294。
- en: 'W. M. Newman and R. F. Sproull (1973) W. M. Newman and R. F. Sproull, 1973.
    Cohen-Sutherland Algorithm, in: Principles of Interactive Computer Graphics. internatio
    ed.. McGraw–Hill Education, pp. 124, 252.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: W. M. Newman 和 R. F. Sproull (1973) W. M. Newman 和 R. F. Sproull, 1973. Cohen-Sutherland
    算法，见：互动计算机图形学原理。国际版。McGraw–Hill 教育，pp. 124, 252。
- en: 'Wang et al. (2020) Wang, C.Y., Mark Liao, H.Y., Wu, Y.H., Chen, P.Y., Hsieh,
    J.W., Yeh, I.H., 2020. CSPNet: A new backbone that can enhance learning capability
    of CNN, in: IEEE Computer Society Conference on Computer Vision and Pattern Recognition
    Workshops, pp. 1571–1580.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 (2020) Wang, C.Y., Mark Liao, H.Y., Wu, Y.H., Chen, P.Y., Hsieh, J.W.,
    Yeh, I.H., 2020. CSPNet: 一种可以增强 CNN 学习能力的新型骨干网络，见：IEEE 计算机学会计算机视觉与模式识别研讨会，pp.
    1571–1580。'
- en: 'Woo et al. (2018) Woo, S., Park, J., Lee, J.Y., Kweon, I.S., 2018. CBAM: Convolutional
    Block Attention Module. Lecture Notes in Computer Science (including subseries
    Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
    11211 LNCS, 3–19.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Woo 等人（2018）Woo, S., Park, J., Lee, J.Y., Kweon, I.S., 2018. CBAM: 卷积块注意力模块。计算机科学讲义（包括子系列人工智能讲义和生物信息学讲义）11211
    LNCS, 3–19。'
- en: Zheng et al. (2020) Zheng, Z., Wang, P., Ren, D., Liu, W., Ye, R., Hu, Q., Zuo,
    W., 2020. Enhancing Geometric Factors in Model Learning and Inference for Object
    Detection and Instance Segmentation. IEEE Transactions on Cybernetics .
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2020）Zheng, Z., Wang, P., Ren, D., Liu, W., Ye, R., Hu, Q., Zuo, W.,
    2020. 提升模型学习与推理中的几何因素以进行对象检测和实例分割。IEEE 网络学报。
- en: 'Zoghbi et al. (2017) Zoghbi, S., Cicco, M.D., Ordonez, A.J., Stapper, A.P.,
    Collison, J., Gural, P.S., Ganju, S., Galache, J.l., Jenniskens, P., 2017. Searching
    for Long-Period Comets with Deep Learning Tools, in: Workshop on Deep Learning
    for Physical Sciences.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoghbi 等人（2017）Zoghbi, S., Cicco, M.D., Ordonez, A.J., Stapper, A.P., Collison,
    J., Gural, P.S., Ganju, S., Galache, J.l., Jenniskens, P., 2017. 使用深度学习工具搜索长周期彗星，见：物理科学深度学习研讨会。
