- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:30:50'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2408.03539] Deep Reinforcement Learning for Robotics: A Survey of Real-World
    Successes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.03539](https://ar5iv.labs.arxiv.org/html/2408.03539)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \jvol
  prefs: []
  type: TYPE_NORMAL
- en: AA \jyearYYYY \UseRawInputEncoding
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Reinforcement Learning for Robotics:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Survey of Real-World Successes
  prefs: []
  type: TYPE_NORMAL
- en: 'Chen Tang^(1,∗)    Ben Abbatematteo^(1,∗)    Jiaheng Hu^(1,∗)    Rohan Chandra²
       Roberto Martín-Martín¹    Peter Stone^(1,3) ¹Department of Computer Science,
    The University of Texas at Austin, Austin, Texas 78712, United States; email:
    chen.tang@utexas.edu, abba@cs.utexas.edu, jiahengh@utexas.edu, robertomm@cs.utexas.edu,
    pstone@utexas.edu ²Department of Computer Science, The University of Virginia,
    Charlottesville, Virginia 22904, United States; email: rohanchandra@virginia.edu
    ³Sony AI ^∗Equal Contribution'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reinforcement learning (RL), particularly its combination with deep neural networks
    referred to as deep RL (DRL), has shown tremendous promise across a wide range
    of applications, suggesting its potential for enabling the development of sophisticated
    robotic behaviors. Robotics problems, however, pose fundamental difficulties for
    the application of RL, stemming from the complexity and cost of interacting with
    the physical world. This article provides a modern survey of DRL for robotics,
    with a particular focus on evaluating the real-world successes achieved with DRL
    in realizing several key robotic competencies. Our analysis aims to identify the
    key factors underlying those exciting successes, reveal underexplored areas, and
    provide an overall characterization of the status of DRL in robotics. We highlight
    several important avenues for future work, emphasizing the need for stable and
    sample-efficient real-world RL paradigms, holistic approaches for discovering
    and integrating various competencies to tackle complex long-horizon, open-world
    tasks, and principled development and evaluation procedures. This survey is designed
    to offer insights for both RL practitioners and roboticists toward harnessing
    RL’s power to create generally capable real-world robotic systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'doi:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 10.1146/((please add article doi))
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'robotics, reinforcement learning, deep learning, learning for control, real-world
    applications^†^†journal: Xxxx. Xxx. Xxx. Xxx.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning (RL) [[1](#bib.bib1)] refers to a class of decision-making
    problems in which an agent must learn through trial-and-error to act in such a
    way that maximizes its accumulated *return*, as encoded by a scalar reward function
    that maps the agent’s states and actions to immediate rewards. RL algorithms,
    particularly their combination with deep neural networks referred to as deep RL
    (DRL) [[2](#bib.bib2)], have shown remarkable capabilities in solving complex
    decision-making problems even with high-dimensional observations in domains such
    as board games [[3](#bib.bib3)], video games [[4](#bib.bib4)], healthcare [[5](#bib.bib5)],
    and recommendation systems [[6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: These successes underscore the potential of DRL for controlling robotic systems
    with high-dimensional state or observation space and highly nonlinear dynamics
    to perform challenging tasks that conventional decision-making, planning, and
    control approaches (e.g., classical control, optimal control, sampling-based planning)
    cannot handle effectively. Yet, the most notable milestones of DRL so far have
    been achieved in simulation or game environments, where RL agents can learn from
    extensive experience. In contrast, robots need to complete tasks in the *physical
    world*, which presents additional challenges. It is often inefficient and/or unsafe
    for the RL agents to collect trial-and-error samples directly in the physical
    world, and it is usually impossible to create an exact replica of the complex
    real world in simulation. These challenges notwithstanding, recent advances have
    enabled DRL to succeed at some real-world robotic tasks. For instance, DRL has
    enabled champion-level drone racing [[7](#bib.bib7)] and versatile quadruped locomotion
    control integrated into production-level quadruped systems (e.g., ANYbotics¹¹1[https://www.anybotics.com/news/superior-robot-mobility-where-ai-meets-the-real-world/](https://www.anybotics.com/news/superior-robot-mobility-where-ai-meets-the-real-world/),
    Swiss-Mile²²2[https://www.swiss-mile.com/](https://www.swiss-mile.com/), and Boston
    Dynamics³³3[https://bostondynamics.com/blog/starting-on-the-right-foot-with-reinforcement-learning/](https://bostondynamics.com/blog/starting-on-the-right-foot-with-reinforcement-learning/)).
    However, *the maturity of state-of-the-art DRL solutions varies significantly
    across different robotic applications*. In some domains, such as urban autonomous
    driving, DRL-based solutions remain limited to simulation or strictly confined
    field tests [[8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: This survey aims to comprehensively evaluate the current progress of DRL in
    real-world robotic applications, identifying key factors behind the most exciting
    successes and open challenges that remain in less mature areas. Specifically,
    we assess the maturity of DRL for a variety of problem domains and contrast the
    DRL literature across domains to pinpoint broadly applicable techniques, under-explored
    areas, and common open challenges that need to be addressed to advance DRL’s applications
    in robotics. We aim for this survey to provide researchers and practitioners with
    a thorough understanding of the status of DRL in robotics, offering valuable insights
    to guide future research and facilitate broadly deployable DRL solutions for real-world
    robotic tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Why Another Survey on RL for Robotics?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although some previous articles have surveyed RL for robotics, we make three
    contributions that provide unique perspectives on the literature and fill gaps
    in knowledge. First, we focus on work that has demonstrated at least *some degree
    of real-world success*, aiming to assess the current state and open challenges
    of DRL for real-world robotic applications. Most existing surveys on RL for robotics
    do not explicitly address this topic, e.g., Dulac-Arnold et al. [[9](#bib.bib9)]
    discuss the general challenges of real-world RL not specific to robotics, and
    Ibarz et al. [[10](#bib.bib10)] list open challenges of DRL unique to real-world
    robotics settings but based on case studies drawn only from their own research.
    In contrast, our discussion is grounded in a comprehensive assessment of the real-world
    successes achieved by DRL in robotics, with one aspect of our evaluation being
    the level of real-world deployment (see Sec. [3.4](#S3.SS4 "3.4 Level of Real-World
    Success ‣ 3 Taxonomy ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World
    Successes")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we present a *novel* and *comprehensive* taxonomy that categorizes
    DRL solutions along multiple axes: robot competencies learned with DRL, problem
    formulation, solution approach, and level of real-world success. Prior surveys
    on RL for robotics and broader robot learning have often focused on specific tasks [[11](#bib.bib11),
    [12](#bib.bib12)] or on particular techniques [[13](#bib.bib13), [14](#bib.bib14)].
    By contrast, our taxonomy allows us to survey the complete landscape of DRL solutions
    that are effective in robotics application domains, in addition to reviewing the
    literature of each application domain separately. Within this framework, we compare
    and contrast solutions and identify *common patterns, broadly applicable approaches,
    under-explored areas, and open challenges* for realizing successful robotic systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Third, while some past surveys have shared our motivation to provide a broad
    analysis of the field, the fast and impressive pace of DRL progress has created
    the need for a renewed analysis of the field, its successes, and limitations.
    The seminal survey by Kober et al. [[15](#bib.bib15)] was written before the deep
    learning era, and the general deep learning for robotics survey by Sunderhauf
    et al. [[16](#bib.bib16)] was written when DRL accomplishments were primarily
    in simulation. We provide a refreshed overview of the field by focusing on DRL,
    which is behind the most notable real-world successes of RL in robotics, paying
    particular attention to papers published in the last five years, during which
    most of the successes occurred.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Taxonomy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d50ecb6d94ad71f479d218401cc1ac9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The four aspects of our taxonomy: (a) Robot competencies learned
    with DRL; (b) Problem formulation; (c) Solution approach; and (d) Levels of real-world
    success.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This section presents the novel taxonomy we introduce to categorize the literature
    on DRL. The unique focus of our survey on the real-world successes of DRL in robotics
    necessitates a new taxonomy to categorize and analyze the literature, which should
    enable us to assess the maturity of DRL solutions across various robotic applications
    and derive valuable lessons from both successes and failures. Specifically, we
    should identify the specific robotic problem addressed in each paper, understand
    how it has been abstracted as an RL problem, and summarize the DRL techniques
    applied to solve it. More importantly, we should evaluate the maturity of these
    DRL solutions, as demonstrated in their experiments. Consequently, we introduce
    a taxonomy spanning four axes: robot competencies learned with DRL, problem formulation,
    solution approach, and the level of real-world success.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Robot Competencies Learned with DRL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our primary axis focuses on the target robotic task studied in each paper. A
    robotic task, especially in open real-world scenarios, may require multiple competencies.
    One may apply DRL to synthesize an end-to-end system to realize all the competencies
    or learn sub-modules to enable a subset of them. Since our focus is DRL, we classify
    papers based on *the specific robot competencies learned and realized with DRL*.
    We first classify the competencies into *single-robot*—competencies required for
    a robot to complete tasks on its own—and *multi-agent*—competencies required to
    interact with other agents sharing the workspace with the robot and affecting
    its task completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a single robot completes a task in a workspace, any competencies it requires
    can be considered as enabling specific ways to *interact with and affect the physical
    world*, which are further divided into mobility—moving in the environment—and
    manipulation—moving or rearranging (e.g., grasping, rotating) objects in the environment [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)]. In the robotics literature, mobility⁴⁴4In
    the robotics literature, both locomotion and navigation have been used to refer
    to the ability to move in an environment. To avoid confusion, mobility is used
    in this survey to refer to the overarching category where DRL enables robot movement.
    is typically split into two problems: locomotion and navigation [[18](#bib.bib18),
    [20](#bib.bib20)]. Locomotion focuses on motor skills that enable robots of various
    morphologies (e.g., quadrupeds, humanoids, wheeled robots, drones) to traverse
    different environments, while navigation focuses on strategies that direct a robot
    to its destination efficiently without collision. Typical navigation policies
    generate *high-level* motion commands, such as desired states at the center of
    mass (CoM), while assuming effective locomotion control to execute them [[18](#bib.bib18)].
    Some works jointly address the locomotion and navigation problems, which is particularly
    useful for tasks in which the navigation strategies are heavily affected by the
    robot’s capability to traverse the environment, as determined by the robot dynamics
    and locomotion control (e.g., navigating through challenging terrains [[20](#bib.bib20)]
    or racing [[21](#bib.bib21)]). We review these papers alongside other navigation
    papers since their ultimate goal is navigation.'
  prefs: []
  type: TYPE_NORMAL
- en: In the robotics literature, manipulation is often studied in table-top settings,
    e.g., robotic arms or hands mounted on a stationary base with fixed sensors observing
    the scene. Some other real-world tasks further require robots to interact with
    the environment while moving their base (e.g., household and warehouse robots),
    which necessitates a synergistic integration of manipulation and mobility capabilities.
    We review the former case under the stationary manipulation category and the latter
    under mobile manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the task completion is affected by the other agents in the workspace,
    the robot needs to be further equipped with *abilities to interact with other
    agents*, which we place under the heading of *multi-agent* competencies. Note
    that some single-robot competencies may still be required while the robot interacts
    with others, such as crowd navigation or collaborative manipulation. In this category,
    we focus on papers where DRL occurs at the agent-interaction level, i.e., learning
    interaction strategies given certain single-robot competencies or learning policies
    that jointly optimize interaction and single-robot competencies. We further split
    these works into two subcategories based on the types of agents the robot interacts
    with: 1) *Human-robot interaction* concerns a robot’s ability to operate alongside
    humans. The presence of humans introduces additional challenges due to their sophisticated
    behavior and the stringent safety requirements for robots operating around humans. 2)
    *Multi-robot interaction* refers to a robot’s ability to interact with a group
    of robots. A class of RL algorithms, multi-agent RL (MARL), is typically applied
    to solve this problem. In MARL, each robot is a learning agent evolving its policy
    based on its interactions with the environment and other robots, which complicates
    the learning mechanism. Depending on whether the robots’ objectives align, their
    interactions could be cooperative, adversarial, or general-sum. In addition, practical
    scenarios often require decentralized decision-making under partial observability
    and limited communication bandwidth.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second axis of our taxonomy is the formulation of the RL problem, which
    specifies the optimal control policy for the targeted robot competency. RL problems
    are typically modeled as Partially Observable Markov Decision Processes (POMDPs)
    for single-agent RL and Decentralized POMDPs (Dec-POMDP) for multi-agent RL. Specifically,
    we categorize the papers based on the following elements of the problem formulation:
    1) *Action space*: whether the actions are *low-level* (i.e., joint or motor commands),
    *mid-level* (i.e., task-space commands), or *high-level* (i.e., temporally extended
    task-space commands or subroutines); 2) *Observation space*: whether the observations
    are *high-dimensional* sensor inputs (e.g., images and/or LiDAR scans) or estimated
    *low-dimensional* state vectors; 3) *Reward function*: whether the reward signals
    are *sparse* or *dense*. Due to space limitations, we provide detailed definitions
    of these terms in the supplementary materials.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Solution Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another axis closely related to the previous one is the solution approach used
    to solve the RL problem, which is composed of the RL algorithm and associated
    techniques that enable a practical solution for the target robotic problem. Specifically,
    we classify the solution approach from the following perspectives: 1) *Simulator
    usage*: whether and how simulators are used, categorized into *zero-shot*, *few-shot
    sim-to-real transfer*, or directly learning offline or in the real world *without
    simulators*; 2) *Model learning*: whether (a part of) the transition dynamics
    model is learned from robot data; 3) *Expert usage*: whether expert (e.g., human
    or oracle policy) data are used to facilitate learning; 4) *Policy optimization*:
    the policy optimization algorithm adopted, including *planning* or *offline*,
    *off-policy*, or *on-policy RL*; 5) *Policy/Model Representation*: Classes of
    neural network architectures used to represent the policy or dynamics model, including
    *MLP*, *CNN*, *RNN*, and *Transformer*. Please refer to the supplementary materials
    for detailed term definitions.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Level of Real-World Success
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate the practicality of DRL in real-world robotic tasks, we categorize
    the papers based on the maturity of their DRL methods. By comparing the effectiveness
    of DRL across different robotic tasks, we aim to identify domains where the gaps
    between research prototypes and real-world deployment are more or less significant.
    This requires a metric to quantify real-world success across tasks, which, to
    our knowledge, has not been attempted in the DRL for robotics literature. Inspired
    by the levels of autonomous driving [[22](#bib.bib22)] and Technology readiness
    level (TRL) for machine learning [[23](#bib.bib23)], we introduce the concept
    of *levels of real-world success*. We classify the papers into six levels based
    on the scenarios where the proposed methods have been validated: 1) *Level 0*:
    validated only in simulation; 2) *Level 1*: validated in limited lab conditions;
    3) *Level 2*: validated in diverse lab conditions; 4) *Level 3*: validated under
    confined real-world operational conditions; 5) *Level 4*: validated under diverse,
    representative real-world operational conditions; and 6) *Level 5*: deployed on
    commercialized products. We consider Levels 1-5 as achieving at least some degree
    of real-world success. The only information we can use to assess the level of
    real-world success is the experiments reported by the authors. However, many papers
    only described a single real-world trial. While we strive to provide accurate
    estimates, this assessment can be subjective due to limited information. Additionally,
    we use the level of real-world success to quantify the maturity of a solution
    for its target problem, irrespective of its complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Competency-Specific Review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides a detailed review of the DRL literature, with each subsection
    focusing on a specific robot competency. In each subsection, we further organize
    the review based on subcategories specific to each type of competency. After discussing
    the papers, we conclude each subsection by summarizing the trends and open challenges
    for learning the competency in question. To aid understanding, each subsection
    includes a table to overview the reviewed papers. Since our main objective is
    to assess the maturity of DRL solutions, we note the level of real-world success
    achieved by each paper in the table. For a comprehensive categorization of the
    papers, please refer to Tables [1](#A2.T1 "Table 1 ‣ Appendix B Additional Tables
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")–[6](#A2.T6
    "Table 6 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes") in the supplementary materials.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Locomotion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Locomotion research aims to develop motor skills for robots to traverse various
    real-world environments. Prior to the deep learning era, several pioneering works
    have explored RL for locomotion control and delivered promising hardware demos,
    e.g., quadruped walking [[24](#bib.bib24)] and helicopter control [[25](#bib.bib25),
    [26](#bib.bib26)]. This subsection reviews DRL solutions for locomotion separately
    from navigation, where the controllers follow high-level navigation commands.
    Since locomotion mainly concerns motor skills, the problem complexity is primarily
    influenced by the system dynamics [[27](#bib.bib27)]. We organize this subsection
    accordingly and review three representative locomotion problems: quadruped and
    biped locomotion, and quadrotor flight control. See Figure [2](#S4.F2 "Figure
    2 ‣ 4.1.1 Quadruped Locomotion ‣ 4.1 Locomotion ‣ 4 Competency-Specific Review
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")
    for an overview of the papers reviewed.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Quadruped Locomotion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Quadruped locomotion is one of the robotic domains where DRL has provided mature
    real-world solutions. Multiple robotics companies, such as ANYbotics, Swiss-Mile,
    and Boston Dynamics, have reported that DRL was integrated into their quadruped
    control for applications including industrial inspection, last-mile delivery,
    and rescue operations. In the literature, DRL methods were first validated for
    *blind quadruped walking*, i.e., relying solely on proprioceptive sensors on flat
    indoor surfaces [[28](#bib.bib28), [29](#bib.bib29)]. These policies were typically
    trained in simulation and deployed zero-shot in the real world. The main challenge
    lies in the sim-to-real gap in quadrupeds’ intrinsic dynamics. Several strategies
    have been explored to bridge the reality gap: 1) learning actuator models, either
    analytical [[28](#bib.bib28)] or neural network-based [[29](#bib.bib29)], from
    robot data to improve simulation fidelity; 2) randomizing dynamics parameters [[28](#bib.bib28),
    [29](#bib.bib29)] and, even further, randomizing morphology [[30](#bib.bib30)],
    which enables generalization to unseen quadrupeds; and 3) adopting a hierarchical
    structure with a low-level, model-based controller to handle dynamics discrepancy
    and external disturbances while facilitating efficient learning. The interface
    between the DRL policy and the model-based controller could be defined at various
    levels, such as joint positions [[31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33)],
    leg poses [[28](#bib.bib28)], gait parameters [[34](#bib.bib34), [35](#bib.bib35)],
    or temporally-extended macro actions [[36](#bib.bib36)]. As robots venture beyond
    controlled lab environments, they encounter more challenging terrains such as
    discontinuous, deformable, or slippery surfaces. Four main techniques have been
    used to address the additional challenges. First, the terrain and contact information
    are not directly observable. Privileged learning has been commonly adopted as
    a solution [[34](#bib.bib34), [33](#bib.bib33)], where a policy with privileged
    terrain information is trained first and then distilled into a student policy
    operating on realistic sensor inputs. Alternatively, end-to-end training can be
    achieved with the help of state estimation [[37](#bib.bib37), [38](#bib.bib38)]
    and asymmetric actor-critic [[39](#bib.bib39), [38](#bib.bib38)]. In both cases,
    an extended history of observations is often set as input.'
  prefs: []
  type: TYPE_NORMAL
- en: Second, policies should be exposed to diverse conditions during training for
    generalization in the wild. A learning curriculum that progressively increases
    task difficulty is often adopted to facilitate training [[34](#bib.bib34), [33](#bib.bib33),
    [36](#bib.bib36), [38](#bib.bib38), [37](#bib.bib37)]. Advanced terrain models
    can also improve performance on terrains with complex contact dynamics, e.g.,
    deformable surfaces [[37](#bib.bib37)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/12878df0d29508bf4d146c0ee5e715dc.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Quadruped | [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54) |'
  prefs: []
  type: TYPE_TB
- en: '| Biped | [27](#bib.bib27), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57),
    [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63) |'
  prefs: []
  type: TYPE_TB
- en: '| Flight | [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 2: Left: An overview of the three locomotion problems reviewed in Sec. [4.1](#S4.SS1
    "4.1 Locomotion ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes"), including quadruped [[49](#bib.bib49)]
    and biped [[63](#bib.bib63)] locomotion, and quadrotor flight control [[64](#bib.bib64),
    [67](#bib.bib67)]; Right: Locomotion papers reviewed in Sec. [4.1](#S4.SS1 "4.1
    Locomotion ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes"). The color map indicates the levels of real-world
    success: *Limited Lab*, *Diverse Lab*, *Limited Real*, and *Diverse Real*.'
  prefs: []
  type: TYPE_NORMAL
- en: Third, exteroceptive sensors are crucial for traversing risky terrains, as they
    allow the quadruped to adapt to terrains without stepping on them. For example,
    they have fostered more efficient and robust stair traversal [[35](#bib.bib35),
    [43](#bib.bib43)]. Exteroceptive observations are typically in the form of terrain
    height maps [[35](#bib.bib35), [36](#bib.bib36)], depth images [[44](#bib.bib44),
    [45](#bib.bib45)], and RGB images [[43](#bib.bib43)]. Privileged learning is widely
    used to facilitate policy learning from these high-dimensional observations [[44](#bib.bib44),
    [35](#bib.bib35), [45](#bib.bib45)]. To reduce the sim-to-real gap in sensor inputs,
    techniques such as injecting simulated sensor noise [[35](#bib.bib35)], post-processing
    depth images [[50](#bib.bib50)], learning vision encoders from real-world samples [[43](#bib.bib43)]
    are shown effective. Additionally, policies benefit from improved representation
    via self-supervised learning [[36](#bib.bib36), [35](#bib.bib35)], cross-modal
    embedding matching [[44](#bib.bib44), [43](#bib.bib43)], or using models with
    higher capacity, such as transformers [[69](#bib.bib69), [45](#bib.bib45)].
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, traversing certain complex terrains demands advanced locomotion skills
    beyond regular walking gaits. For example, end-to-end DRL policies typically struggle
    with terrains that have sparse contact regions. Jenelten et al. [[46](#bib.bib46)]
    showed that training an RL policy to track reference footholds provided by trajectory
    optimization results in more accurate and robust foot placement on sparse terrains.
    Jumping further extends the robots’ ability to cross gaps beyond their body length.
    For example, Yang et al. [[47](#bib.bib47)] trained a DRL policy to generate trajectories
    with a model-based tracking controller handling the complex jumping dynamics.
    Fall recovery is another essential skill, especially for automatic reset in real-world
    RL [[48](#bib.bib48), [53](#bib.bib53)]. Several works have trained DRL policies
    for fall recovery [[29](#bib.bib29), [31](#bib.bib31), [48](#bib.bib48), [32](#bib.bib32),
    [41](#bib.bib41)]. However, both jumping and fall recovery have only been validated
    on flat surfaces so far.
  prefs: []
  type: TYPE_NORMAL
- en: To effectively leverage agile locomotion skills for complex downstream tasks
    like parkour [[49](#bib.bib49), [50](#bib.bib50)], it is crucial to develop *multi-skill*
    policies. Learning multiple skills jointly has also been shown effective in fostering
    policy robustness [[63](#bib.bib63)]. One approach is to create a set of RL policies [[51](#bib.bib51),
    [32](#bib.bib32), [50](#bib.bib50)], each tailored to a specific skill, and then
    train a high-level policy to select the optimal skill [[32](#bib.bib32)]. Alternatively,
    a single policy can be distilled from specialized skill policies through BC [[50](#bib.bib50)].
    To avoid the cumbersome procedure of training multiple specialized policies, several
    works explored constructing a unified policy directly. For instance, MoB [[52](#bib.bib52)]
    encoded various locomotion strategies into a single policy conditioned on gait
    parameters. Cheng et al. [[49](#bib.bib49)] used a unified reward consisting of
    waypoint and velocity tracking terms to learn diverse parkour skills. Fu et al. [[42](#bib.bib42)]
    showed that energy minimization led to smooth gait transitions. Motion imitation
    reward is another widely used and unified approach for learning naturalistic and
    diverse locomotion skills [[51](#bib.bib51), [40](#bib.bib40)].
  prefs: []
  type: TYPE_NORMAL
- en: Remark on RL algorithms.We conclude the review on quadruped locomotion with
    a remark on the RL algorithms used in the literature. The most mature DRL solutions
    for quadruped locomotion followed the zero-shot sim-to-real transfer scheme, predominantly
    using on-policy model-free RL, e.g., PPO [[70](#bib.bib70)], due to its robustness
    to hyperparameters. Gangapurwala et al. [[36](#bib.bib36)] noted that on-policy
    RL could be less favorable when the action space is temporally extended or deterministic
    control actions are preferred. Meanwhile, researchers have explored few-shot adaptation
    and real-world RL, either model-free [[48](#bib.bib48), [53](#bib.bib53)] or model-based [[54](#bib.bib54)],
    to update policies using real-world rollouts to further generalize policies to
    novel situations without accurate simulation. However, most works along this line
    have only been validated in limited lab settings. The state-of-the-art performance
    for real-world fine-tuning [[48](#bib.bib48)] and learning from scratch [[53](#bib.bib53)]
    were achieved by using off-policy RL to learn walking and fall recovery. However,
    the tested conditions remain limited compared to mature zero-shot solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Biped Locomotion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compared to the quadruped case, the DRL literature on bipedal locomotion is
    sparser, and the real-world capabilities demonstrated are more limited. We confine
    the discussion to 3D bipedal robots, which can move freely in all spatial dimensions,
    unlike 2D bipeds that are attached to booms and confined to 2D planar motion [[71](#bib.bib71)],
    for their greater practical utility. The literature begins with walking on flat
    indoor surfaces [[55](#bib.bib55), [57](#bib.bib57)] and extends to walking on
    various indoor [[56](#bib.bib56), [58](#bib.bib58), [27](#bib.bib27)] and outdoor
    terrains [[60](#bib.bib60), [62](#bib.bib62)], and under external forces [[27](#bib.bib27),
    [58](#bib.bib58)]. Other demonstrated skills include stair traversal [[59](#bib.bib59)],
    hopping [[57](#bib.bib57)], running [[57](#bib.bib57), [63](#bib.bib63)], jumping [[63](#bib.bib63)],
    and traversing obstacles and gaps [[61](#bib.bib61)]. More advanced skills have
    been showcased by industrial companies⁵⁵5For example, Unitree ([https://t.ly/s1FwW](https://t.ly/s1FwW))
    and Boston Dynamics ([https://t.ly/NaSaO](https://t.ly/NaSaO)), but no technical
    reports are publicly available to reveal if RL was used in their demos. Notably,
    some of these works deployed their locomotion policies on humanoid robots [[56](#bib.bib56),
    [60](#bib.bib60), [62](#bib.bib62)] while others on bipedal robots without upper
    bodies [[55](#bib.bib55), [58](#bib.bib58), [59](#bib.bib59), [27](#bib.bib27),
    [61](#bib.bib61), [63](#bib.bib63)].
  prefs: []
  type: TYPE_NORMAL
- en: The DRL techniques for bipedal locomotion largely overlap with those for quadrupeds
    but show three distinct trends due to the complex and under-actuated dynamics
    of bipeds. First, learning basic standing and walking skills is already challenging
    due to bipeds’ non-statically stable dynamics [[55](#bib.bib55)]. Thus, model-based
    approaches are frequently used to facilitate RL, either by generating reference
    gaits to guide RL [[55](#bib.bib55), [58](#bib.bib58), [63](#bib.bib63)] or handling
    low-level control for high-level RL policies [[60](#bib.bib60)]. Alternatively,
    Siekmann et al. [[57](#bib.bib57)] offered an end-to-end solution with a reference-free
    periodic reward design based on periodic composition. Second, the role of state
    and action *memories* was particularly noted [[55](#bib.bib55)], especially a
    combination of both long- and short-term memories [[63](#bib.bib63)]. Thus, most
    works adopted sequence models in their policy architecture [[63](#bib.bib63),
    [61](#bib.bib61), [55](#bib.bib55), [57](#bib.bib57), [59](#bib.bib59), [62](#bib.bib62)].
    Third, almost all these policies were zero-shot transferred from simulation. One
    exception is GAT [[56](#bib.bib56)], which collected real-world samples to refine
    a simulator iteratively, enabling an NAO to walk on uneven carpets. The limited
    real-world learning examples are likely due to bipeds’ limited recovery capabilities,
    which hinder their resilience in trials, particularly their ability to auto-reset.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Quadrotor Flight Control
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Flight control for unmanned aerial vehicles (UAVs), in particular quadrotors,
    is another problem where DRL has shown compelling performance. Hwangbo et al. [[64](#bib.bib64)]
    developed the first DRL quadrotor control policy that was successfully validated
    on hardware for waypoint tracking and recovery from harsh initialization. Later
    studies showed that carefully designed simulated dynamics, domain randomization [[65](#bib.bib65)],
    and carefully designed action space, specifically collective thrust and body rates [[66](#bib.bib66)],
    can facilitate policy robustness. Zhang et al. [[67](#bib.bib67)] applied RMA
    to train a robust near-hover position controller adaptable to unseen disturbances.
    Eschmann et al. [[68](#bib.bib68)] introduced the first off-policy RL paradigm
    for quadrotor control, capable of training a deployable control policy within
    18 seconds for waypoint tracking. In summary, DRL has demonstrated better robustness
    than classical feedback controllers (e.g., PID) in hovering control [[65](#bib.bib65),
    [67](#bib.bib67)]. However, DRL policies tend to have larger tracking errors than
    carefully designed optimization-based controllers for waypoint tracking [[64](#bib.bib64),
    [66](#bib.bib66)]. Yet the fundamental advantage of RL over optimal control is
    it enables joint optimization for planning and control [[21](#bib.bib21)], making
    it an ideal candidate for agile navigation such as racing (see Sec. [4.2](#S4.SS2
    "4.2 Navigation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes")).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Trends and Open Challenges in Locomotion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In summary, DRL has shown effectiveness in synthesizing robust and adaptive
    locomotion controllers for challenging conditions. DRL Techniques used for quadruped,
    biped, and flight control heavily overlap. For instance, RMA [[33](#bib.bib33)],
    initially proposed for quadruped locomotion, has been adapted for both biped [[27](#bib.bib27)]
    and quadrotor flight control [[67](#bib.bib67)]. However, the maturity of DRL
    solutions varies across domains. Quadrupeds can traverse various indoor and outdoor
    terrains via DRL, while real-world bipedal locomotion skills achieved by DRL are
    more limited. For quadrotors, most tests remain confined to controlled, obstacle-free
    indoor environments. Hardware accessibility is a contributing factor. The introduction
    of low-cost quadrupeds has spurred quadruped research and led to open-sourced
    and unified software packages. Conversely, the high cost of bipedal hardware limits
    extensive real-world testing, though recent advances in humanoid hardware are
    expected to boost biped research. More importantly, the quadruped dynamics are
    inherently more stable, whereas bipeds and quadrotors are more prone to catastrophic
    failures under control errors, imposing higher requirements on both robustness
    and precision of control [[63](#bib.bib63)]. High-speed quadrotor control in outdoor
    scenarios with complex obstacles further requires the policy to ensure the *long-horizon*
    feasibility of the closed-loop trajectories [[72](#bib.bib72)]. End-to-end RL
    integrating long-horizon planning and short-horizon control shows promise as a
    solution [[7](#bib.bib7)]. In addition to ensuring long-horizon feasibility, integrating
    locomotion with downstream tasks (e.g., loco-manipulation) is an exciting direction
    in general, but how to discover skills necessary for downstream tasks remains
    an open question.
  prefs: []
  type: TYPE_NORMAL
- en: '{summary}'
  prefs: []
  type: TYPE_NORMAL
- en: '[Key Takeaways]'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DRL has enabled mature quadruped locomotion control; yet, the maturity of DRL-based
    solutions for other locomotion problems is lower.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware accessibility is an important contributing factor. Low-cost and standard
    hardware platforms would facilitate DRL development.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inherently complex dynamics of certain locomotion problems present fundamental
    challenges to the reliable deployment of DRL locomotion controllers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even in the mature quadruped locomotion domain, open questions remain, such
    as 1) effectively integrating locomotion with downstream tasks via RL, and 2)
    enabling efficient and safe real-world learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.2 Navigation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Navigation focuses on the decision-making challenge in mobility: transporting
    an agent to a goal location while avoiding collisions, typically assuming effective
    locomotion. As a fundamental mobility capability, navigation has an extensive
    history in robotics research [[18](#bib.bib18)]. “Classical” navigation approaches
    employ mapping, localization, and planning modules to determine and execute a
    path to a goal. Planning is typically decomposed into global planning, which produces
    a coarse path, and local planning, which tracks the global plan and handles collision
    avoidance. In this section, we delineate navigation works by embodiment: wheeled,
    legged, and aerial navigation and identify capabilities enabled by RL in each
    setting. Social navigation, where the robot navigates in the presence of humans,
    is deferred to Sec. [4.5](#S4.SS5 "4.5 Human-Robot Interaction ‣ 4 Competency-Specific
    Review ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes").
    Multi-robot navigation is similarly deferred to Sec. [4.6](#S4.SS6 "4.6 Multi-Robot
    Interaction ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Wheeled Navigation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Navigation for wheeled robots, in particular, has a long history in robotics [[18](#bib.bib18)].
    We discuss several common wheeled navigation settings, including geometric navigation,
    visual navigation, and offroad navigation.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Navigation. Early attempts aimed to verify RL’s capability in solving
    navigation problems typically solved with modular classical approaches [[73](#bib.bib73)].
    These RL policies directly map 2D laser scans to control actions, unlike classical
    methods that construct explicit maps from the laser scans. While showing promise,
    they often did not compare against classical approaches or failed to outperform
    them [[12](#bib.bib12)]. Some recent studies have benchmarked such RL-based approaches
    and found them superior in challenging problems with dense obstacles and narrow
    passages [[74](#bib.bib74)]. Instead of replacing the entire navigation stack
    with an RL policy, modular approaches replace specific components like the local
    planner [[75](#bib.bib75)] or the exploration algorithm [[76](#bib.bib76)] with
    RL, enabling better performance than classical baselines. However, these improvements
    were mainly observed in limited real settings. Most commercially deployed systems
    still primarily adopt classical stacks, owing to the lack of safety, interpretability,
    and generalization of RL-based methods [[12](#bib.bib12), [74](#bib.bib74)].
  prefs: []
  type: TYPE_NORMAL
- en: Visual Navigation. Visual navigation refers to problems where agents navigate
    to a goal based on visual observations. The additional input and task complexity
    pose challenges but enable agents to learn common strategies for navigating in
    similar environments (e.g., homes), where structural patterns emerge in visual
    data. Goals are typically specified as a point relative to the agent (termed pointgoal
    navigation) or as an image of a particular object (objectgoal or imagegoal). RL
    is also commonly applied to vision-and-language navigation problems [[77](#bib.bib77)],
    though very little work has demonstrated these capabilities on a real robot. Many
    works [[78](#bib.bib78), [79](#bib.bib79)] map visual observations to actions
    directly without mapping or planning modules. These end-to-end methods have achieved
    near-perfect results on pointgoal tasks in visually realistic simulations [[80](#bib.bib80)].
    However, training such policies is challenging due to the need for scene understanding,
    intelligent exploration, and episodic memory. Their applicability for real-world
    navigation remains unclear, as they have mostly been validated in limited real
    or lab settings. Other works have investigated modular designs, e.g., using RL
    as a global exploration policy together with explicit mapping and local planning [[81](#bib.bib81),
    [82](#bib.bib82)]. They have outperformed both classical and end-to-end learning
    baselines on pointgoal and imagegoal tasks. However, some challenges with such
    modular approaches exist, such as dynamic obstacles, where end-to-end methods
    have shown promise [[83](#bib.bib83)].
  prefs: []
  type: TYPE_NORMAL
- en: Despite the plethora of RL works on visual navigation, most are limited to simulation.
    While these simulators are typically constructed with real-world scans [[77](#bib.bib77),
    [84](#bib.bib84)], their transferability to the real world remains debatable.
    Some works reported poor transfer due to visual domain differences [[82](#bib.bib82)],
    while others found success through parameter tuning [[85](#bib.bib85)], abstraction
    of dynamics [[86](#bib.bib86)], or employing only depth images rather than RGB-D [[83](#bib.bib83),
    [87](#bib.bib87)].
  prefs: []
  type: TYPE_NORMAL
- en: Off-road navigation. Navigating off-road presents additional challenges due
    to the dynamics and traversability of different terrains. Some methods tackled
    these challenges with model-based RL to learn predictive models of events or disengagements [[88](#bib.bib88)],
    or utilizing demonstration data with offline RL [[89](#bib.bib89)]. Success has
    also been achieved in high-speed, off-road driving with model-based RL [[90](#bib.bib90)]
    and, recently, vision-based model-free RL [[91](#bib.bib91)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f81fdbd802368950a225662725e49084.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Wheeled | [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76),
    [78](#bib.bib78), [81](#bib.bib81), [82](#bib.bib82), [85](#bib.bib85), [88](#bib.bib88),
    [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Legged | [20](#bib.bib20), [83](#bib.bib83), [86](#bib.bib86), [87](#bib.bib87),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100) |'
  prefs: []
  type: TYPE_TB
- en: '| Aerial | [7](#bib.bib7), [21](#bib.bib21), [101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 3: Left: An overview of the three navigation problems reviewed in Sec. [4.2](#S4.SS2
    "4.2 Navigation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes"), including wheeled navigation [[74](#bib.bib74),
    [88](#bib.bib88), [92](#bib.bib92)], legged navigation [[97](#bib.bib97)], and
    aerial navigation [[21](#bib.bib21)]; Right: Navigation papers reviewed in Sec. [4.2](#S4.SS2
    "4.2 Navigation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes"). The color map indicates the levels
    of real-world success: *Limited Lab*, *Diverse Lab*, *Limited Real*, and *Diverse
    Real*.'
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous Driving. Autonomous driving extends wheeled navigation to full-size
    passenger vehicles operating at higher speeds in more complex and safety-critical
    environments. RL has achieved limited real-world success for autonomous driving [[8](#bib.bib8)]
    with a few examples under specific conditions. Kendall et al. [[92](#bib.bib92)]
    trained a lane-following policy by learning to maximize its progress before the
    safety driver intervenes. More recently, Jang et al. [[93](#bib.bib93)] trained
    a cruise control policy, where the policy command is wrapped by manually specified
    thresholds to ensure safety. They deployed their policy onto 100 vehicles to smooth
    traffic flow in a field test. Their work suggested a pragmatic approach to embed
    RL into self-driving stacks and showed its potential benefits at the fleet level.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Legged Navigation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Legged navigation shares many challenges with wheeled navigation but also enables
    transversal of more complex terrains. Some have shown that robust visual-legged
    navigation policies can be learned with low-fidelity kinematic-only simulation
    for both indoors [[83](#bib.bib83), [86](#bib.bib86)] and outdoors [[86](#bib.bib86),
    [94](#bib.bib94)]. The policies thus focus on kinematic-level control while assuming
    effective low-level locomotion control during deployment. Truong et al. [[86](#bib.bib86)]
    showed that this approach, in contrast to learning end-to-end policies with high-fidelity
    simulation, facilitates faster simulation and improves policy generalizability.
    With legged locomotion dynamics abstracted away, the approaches are similar to
    the wheeled case, with the main challenge being the visual domain gap. Unsupervised
    representation learning [[83](#bib.bib83)] and pre-trained vision models [[94](#bib.bib94)]
    have been used to facilitate robust visual policies. For outdoor scenes, Truong
    et al. [[87](#bib.bib87)] zero-shot transferred policies trained in well-established
    indoor simulators to outdoors, using goal vector normalization and camera pitch
    randomization to bridge the indoor-to-outdoor domain gap. Sorokin et al. [[94](#bib.bib94)]
    used a high-fidelity autonomous driving simulator and extracted visual features
    from a pre-trained semantic segmentation model for robust sim-to-real transfer
    to sidewalk navigation.
  prefs: []
  type: TYPE_NORMAL
- en: While abstracting away low-level locomotion has advantages, it limits the system
    from fully utilizing the agile locomotion skills endowed by advanced locomotion
    controllers. Recent research has explored DRL frameworks integrating locomotion
    with navigation, achieving high-speed obstacle avoidance [[100](#bib.bib100)]
    and agile navigation over challenging terrains (e.g., stairs, gaps, and boxes) [[20](#bib.bib20),
    [96](#bib.bib96)] and through confined 3D space [[98](#bib.bib98), [99](#bib.bib99)].
    Particularly, Lee et al. [[97](#bib.bib97)] demonstrated kilometer-scale navigation
    with a wheeled-legged robot in urban scenarios, overcoming challenging terrains
    and dynamic obstacles. The integrated policy network can be end-to-end, taking
    goal coordinates as input and outputting locomotion commands [[20](#bib.bib20),
    [99](#bib.bib99)]. He et al. [[100](#bib.bib100)] further introduced a recovery
    policy coordinated using a learned reach-avoid value network. Alternatively, training
    efficiency can be improved with hierarchical architectures, where a high-level
    policy governs pre-trained low-level locomotion policies [[95](#bib.bib95), [96](#bib.bib96),
    [97](#bib.bib97), [98](#bib.bib98)]. Despite the potential of integrating locomotion
    with navigation, policy training could be costly and unstable due to the complex
    low-level dynamics together with the long-horizon nature and sparse rewards of
    the navigation tasks [[20](#bib.bib20), [96](#bib.bib96)]. Classical planning
    algorithms are often used for generating local waypoints to reduce the navigation
    horizon and synthesizing feasible paths to guide initial training [[97](#bib.bib97)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Aerial Navigation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ompared to wheeled and legged robots, aerial vehicles such as quadrotors are
    more fragile, requiring higher robustness and safety in navigation policies. The
    weight and power constraints of quadrotors also limit the use of sophisticated
    sensors. Several works have explored DRL-based aerial navigation using low-cost
    monocular cameras [[101](#bib.bib101), [102](#bib.bib102)]. Sadeghi et al. [[101](#bib.bib101)]
    leveraged visual domain randomization to achieve zero-shot sim-to-real transfer
    for indoor aerial navigation. Kang et al. [[102](#bib.bib102)] showed the values
    of 1) task-specific pre-training in simulation for learning generalizable visual
    representation and 2) the use of real-world data for learning accurate dynamics [[79](#bib.bib79)].
    Similar to quadruped navigation, DRL has been used to develop end-to-end navigation
    and locomotion policies for agile aerial navigation. Kaufmann et al. [[7](#bib.bib7)]
    achieved human champion-level performance in drone racing. A key recipe behind
    their success was augmenting simulation with data-driven residual models of the
    drone’s perception and dynamics. Their subsequent study [[21](#bib.bib21)] showed
    that RL’s advantage over model-based methods lies in its ability to directly optimize
    the long-horizon racing task objective. However, DRL-based policies are still
    less robust than human pilots, limiting their operational conditions. Integrating
    actor-critic RL with differential MPC has shown promise in enhancing robustness [[103](#bib.bib103)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Trends and Challenges in Navigation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RL has shown potential for various submodules of navigation systems, such as
    local planning [[75](#bib.bib75), [104](#bib.bib104)] and global exploration [[76](#bib.bib76),
    [81](#bib.bib81), [82](#bib.bib82)], and for constructing end-to-end navigation
    solutions [[74](#bib.bib74)]. However, RL-based solutions for navigation lack
    the generalization, explainability, and safety guarantees of classical systems
    and thus have not seen widespread real-world deployment [[12](#bib.bib12), [74](#bib.bib74)].
  prefs: []
  type: TYPE_NORMAL
- en: In visual navigation, model-free, end-to-end policies show promise for structured
    indoor environments like homes [[105](#bib.bib105)], while modular architectures
    boost performance without sacrificing guarantees and generalization [[81](#bib.bib81),
    [82](#bib.bib82)]. Striking the right balance between learned and classical modules
    remains an open challenge. Hybrid approaches may be promising, for example, leveraging
    implicit map-like representations learned by end-to-end approaches [[106](#bib.bib106)],
    or using differentiable scene representations [[107](#bib.bib107)] to enable RL
    with algorithmic structure. RL-based vision-and-language navigation [[77](#bib.bib77)]
    is relatively under-explored in real-world settings but promising given the recent
    advances in vision-language models.
  prefs: []
  type: TYPE_NORMAL
- en: In legged navigation, abstracting away low-level dynamics has been shown to
    facilitate sim-to-real transfer for navigation [[86](#bib.bib86)]. For agile legged
    and aerial navigation, where low-level complexity is unavoidable, jointly learning
    navigation and locomotion yields promising results [[100](#bib.bib100), [20](#bib.bib20),
    [96](#bib.bib96), [7](#bib.bib7)]. Yet, involving locomotion complicates the training
    of long-horizon navigation policies, which requires future developments to stabilize
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, learning navigation (collision avoidance, in particular) for safety-critical
    systems, like urban autonomous vehicles and drones, is challenging due to stringent
    robustness requirements in perception and control. These domains have seen fewer
    real-world successes as a result. Real-world data can help improve simulation
    fidelity for this purpose [[7](#bib.bib7), [21](#bib.bib21), [103](#bib.bib103)],
    though establishing guarantees on their performance remains difficult.
  prefs: []
  type: TYPE_NORMAL
- en: '{summary}'
  prefs: []
  type: TYPE_NORMAL
- en: '[Key Takeaways]'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While end-to-end RL excels at visual navigation in simulation, most real-world
    successes deploy modular designs and learn components of the navigation stack.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating RL into these modular architectures, e.g., for local planning or
    semantic exploration, is a promising avenue.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recent work reasoning jointly about navigation and locomotion enables agile
    legged and aerial navigation, yet how to learn long-horizon navigation stably
    and efficiently with low-level control in the loop remains an open challenge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Safety-critical applications like urban autonomous driving or outdoor drone
    flight have seen few real-world successes due to the higher requirements for robustness
    and the lack of explainability and generalization on the part of RL algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3 Manipulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cfde9fe8327d02f485403320af8a52d9.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Pick-and-place | Grasping | [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112) |'
  prefs: []
  type: TYPE_TB
- en: '|  | End-to-end Pick-and-place | [54](#bib.bib54), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118),
    [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122),
    [123](#bib.bib123), [124](#bib.bib124), [125](#bib.bib125) |'
  prefs: []
  type: TYPE_TB
- en: '| Contact-rich | Assembly | [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Articulated Objects | [122](#bib.bib122), [131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Deformable Objects | [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137) |'
  prefs: []
  type: TYPE_TB
- en: '| In-hand | — | [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142) |'
  prefs: []
  type: TYPE_TB
- en: '| Non-prehensile | — | [109](#bib.bib109), [118](#bib.bib118), [143](#bib.bib143),
    [144](#bib.bib144), [145](#bib.bib145) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4: Top: An overview of the four manipulation problems reviewed in Sec. [4.3](#S4.SS3
    "4.3 Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning
    for Robotics: A Survey of Real-World Successes"), including pick-and-place [[108](#bib.bib108)],
    contact-rich manipulation [[130](#bib.bib130)], in-hand manipulation [[141](#bib.bib141)],
    and non-prehensile manipulation [[143](#bib.bib143)]; Bottom: Manipulation papers
    reviewed in Sec. [4.3](#S4.SS3 "4.3 Manipulation ‣ 4 Competency-Specific Review
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes").
    The color map indicates the levels of real-world success: *Limited Lab*, *Diverse
    Lab*, *Limited Real*, and *Diverse Real*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Manipulation refers to an agent’s control of its environment through selective
    contact [[19](#bib.bib19)]. To perform useful work in the world, robots require
    manipulation capabilities such as pick-and-place, mechanical assembly, in-hand
    manipulation, non-prehensile manipulation, and beyond. Manipulation poses several
    challenges for both analytical and learning-based methods [[11](#bib.bib11)],
    as the mechanics of contact are complex and difficult to model, and open-world
    manipulation requires strong generalization and fast online learning. RL is well-suited
    to these challenges, but manipulation poses fundamental difficulties for RL: large
    observation and action spaces make real-world exploration prohibitively time-consuming
    and unsafe; reward function design requires domain knowledge; tasks are often
    long-horizon; and instantaneous environment resets are usually unrealistic in
    real-world tasks. Despite these challenges, DRL has achieved notable successes
    in manipulation recently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this subsection, we review progress in several manipulation capabilities
    enabled by DRL, following the outline from Mason’s seminal review [[19](#bib.bib19)]:
    pick-and-place, contact-rich manipulation, in-hand manipulation, and non-prehensile
    manipulation. See Figure [4](#S4.F4 "Figure 4 ‣ 4.3 Manipulation ‣ 4 Competency-Specific
    Review ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")
    for an overview of the papers reviewed in this subsection. Note that this subsection
    focuses on stationary manipulators, and we defer mobile manipulation to Sec. [4.4](#S4.SS4
    "4.4 Mobile Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning
    for Robotics: A Survey of Real-World Successes").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Pick-and-place
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Picking and placing objects is a longstanding challenge in manipulation, requiring
    the ability to perceive objects, grasp them, determine appropriate placements,
    and generate collision-free motion. Structured pick-and-place, in which the environment
    is engineered to reduce complexity and objects are known a priori, is well-understood
    and widely deployed in manufacturing contexts. Open-world, unstructured pick-and-place—rearranging
    arbitrary objects in the wild—remains a challenge. In recent years, more traditional
    robotic approaches have seen success in industrial applications like fulfillment,
    employing machine learning for object detection and grasping but deferring control
    to analytical methods [[19](#bib.bib19)]. While pick-and-place tasks serve as
    a common testbed for new RL algorithms [[115](#bib.bib115), [116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119), [54](#bib.bib54)],
    end-to-end RL methods still lack the ability to pick and place novel objects in
    the open world with generality. However, modular approaches, such as solving grasping
    with RL, have enabled some real-world successes. We will review RL-based solutions
    to the subproblem of grasping and then discuss end-to-end RL methods, omitting
    a discussion of motion generation for which RL is not commonly used.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1.1 Grasping
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Grasping objects is a fundamental capability essential for pick-and-place and
    other downstream tasks, such as in-hand manipulation and assembly. Some of the
    first large-scale successes of DRL for manipulation were in grasping objects with
    unknown geometry and appearance [[108](#bib.bib108)]. Where analytical methods
    had achieved grasping of known objects using taxonomies and databases, these works
    leveraged thousands or millions of grasp attempts to learn grasping behaviors
    through interaction. Many works frame grasping as a bandit or classification problem,
    where the action space consists of discrete grasp candidates and the picking motion
    is executed open-loop [[108](#bib.bib108), [109](#bib.bib109)]. These methods
    commonly employ sparse rewards that indicate success when an object is lifted
    and collect data in a self-supervisory manner. Similar systems have been reportedly
    integrated into fulfillment applications⁶⁶6See examples from Ambi Robotics ([https://t.ly/tSds_](https://t.ly/tSds_))
    and Covariant ([https://t.ly/S5pnz](https://t.ly/S5pnz)). with diverse objects.
    Closed-loop grasping—controlling the end-effector pose and/or fingers directly
    to achieve stable grasps—can be formulated as a sequential decision-making problem
    and solved with RL. While some successes have been seen [[110](#bib.bib110), [111](#bib.bib111),
    [112](#bib.bib112)], closed-loop grasping remains challenging due to the additional
    complexity of learning vision-based closed-loop control, and such systems have
    not seen the same level of real-world success as open-loop ones. In both closed-
    and open-loop grasping, while some works exclusively collect real-world data [[109](#bib.bib109),
    [110](#bib.bib110), [112](#bib.bib112)], the common recipe is to use simulation
    for data collection [[108](#bib.bib108)] or policy training [[111](#bib.bib111)],
    often employing domain adaptation to ensure visual similarity between the simulator
    and real world.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1.2 End-to-end Pick-and-place
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Learning general-purpose pick-and-place in the open world remains daunting
    for end-to-end RL, owing to the sheer variety of objects and tasks and the limited
    generalization of current algorithms. This variety also precludes the common sim-to-real
    recipe successful in other domains like grasping and in-hand manipulation, where
    tasks and objects can be enumerated during training. Nonetheless, some major milestones
    in end-to-end pick-and-place have been observed: Levine et al. [[113](#bib.bib113)]
    demonstrated the potential of deep visuomotor policies; Riedmiller et al. [[119](#bib.bib119)]
    demonstrated pick-and-place manipulation with a hierarchical policy trained in
    the real world; and Lee et al. [[116](#bib.bib116)] achieved stacking of diverse
    objects through sim-to-real transfer. Augmenting the action space with primitives [[123](#bib.bib123)]
    can help in reducing the task horizon and is a natural means to incorporate human
    engineering. Recent work leveraging large vision-language models shows promise
    in handling open-ended diverse objects and task objectives specified by natural
    language [[124](#bib.bib124)]. The potential of RL to solve this longstanding
    challenge is only now coming into focus with emerging large-scale robotic datasets
    and foundation models. Despite not yet achieving widespread success in real-world
    deployments, many important RL innovations have been demonstrated in pick-and-place
    problems, addressing challenges such as multi-task learning [[114](#bib.bib114),
    [115](#bib.bib115), [125](#bib.bib125)], sample efficiency [[54](#bib.bib54)],
    defining and computing reward [[120](#bib.bib120), [121](#bib.bib121)], resetting
    the environment [[117](#bib.bib117)], and utilizing human demonstrations or offline
    data [[122](#bib.bib122), [116](#bib.bib116), [124](#bib.bib124)].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Contact-rich Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While pick-and-place tasks are often assumed to be strictly kinematic, contact-rich
    tasks like mechanical assembly (e.g., peg insertion), interacting with articulated
    objects (e.g., opening doors), and manipulating deformable objects, require reasoning
    about dynamics and relaxing the rigid-body assumption of the objects. We discuss
    several contact-rich tasks where RL has advanced the state of the art: assembly,
    articulated object manipulation, and deformable object manipulation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2.1 Assembly
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Assembly tasks are crucial in manufacturing, and automating them is a longstanding
    challenge in robotics. Existing industrial solutions tend to rely on extensive
    engineering of the environment and robot motions, resulting in behaviors sensitive
    to small perturbations and costly to design. Assembly is challenging for RL due
    to the difficulty in controlling contact-rich interactions and the stringent requirements
    for accuracy and precision, coupled with the need to handle diverse object parts.
    While RL has not seen widespread deployment in industrial contexts, some notable
    successes have been observed in recent years. Many approaches employ sim-to-real
    transfer to achieve assembly [[130](#bib.bib130)], though some train policies
    directly in the real world [[126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129)], typically leveraging human demonstrations. Luo et al. [[128](#bib.bib128)]
    notably compare against solutions provided by integrators and find their RL-based
    policies more robust to perturbation. A common strategy among approaches to assembly
    is using residual RL [[126](#bib.bib126)], in which a residual policy is learned
    on top of a reference trajectory. Most works assume that the object is already
    grasped before assembly. By contrast, Tang et al. [[130](#bib.bib130)] present
    a sim-to-real RL framework for the entire assembly pipeline, including object
    detection, grasping, and insertion, achieving diverse assembly tasks by leveraging
    recent advances in contact simulation and developing algorithmic advances for
    sim-to-real transfer.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2.2 Articulated Objects
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some limited successes have been observed in constrained manipulation tasks
    like opening drawers. Most commonly, these tasks are used to demonstrate RL capabilities
    without dedicated efforts to realize practical deployment [[122](#bib.bib122),
    [131](#bib.bib131)]. Other works target this class of skills in particular [[132](#bib.bib132),
    [133](#bib.bib133)] with limited success.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2.3 Deformable Objects
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Deformable objects, such as cloth, present additional challenges owing to the
    difficulty in accurately modeling soft materials. Tasks like cloth folding [[134](#bib.bib134),
    [135](#bib.bib135), [136](#bib.bib136)] and assistive dressing [[137](#bib.bib137)]
    have thus received considerable attention in RL. These works often employ sim-to-real
    transfer [[134](#bib.bib134), [136](#bib.bib136), [137](#bib.bib137)], and often
    simplify the tasks using primitives such as pick-and-place [[135](#bib.bib135)]
    and flinging [[136](#bib.bib136)].
  prefs: []
  type: TYPE_NORMAL
- en: In summary, open-world contact-rich manipulation inherits the challenges of
    unstructured pick-and-place (namely, generalization to novel objects and tasks)
    and the additional challenge of controlling contact-rich interactions. Nonetheless,
    some successes have been demonstrated in contact-rich tasks, particularly assembly
    and deformable objects, where tasks are predefined, objects are enumerable, and
    rigid grasps are usually assumed.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 In-hand Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Humans exhibit many in-hand manipulation behaviors, re-orienting and re-positioning
    objects to facilitate downstream manipulation. Impressive strides in the development
    of these capabilities have been made with DRL in recent years, allowing agents
    to learn such complex in-hand manipulation behaviors with impressive generalization.
    Several works focused on re-orienting single objects to target configurations [[138](#bib.bib138),
    [139](#bib.bib139)], employing pose estimation modules trained in simulation.
    Nagabandi et al. [[140](#bib.bib140)] similarly demonstrated rotating Baoding
    balls with model-based RL. While showing impressive dexterity, these works focus
    on manipulating known objects (e.g., a given cube) with low-dimensional observations.
    Recent methods leveraging vision and tactile data have demonstrated rotating arbitrary
    objects about arbitrary axes [[141](#bib.bib141)], even against gravity [[142](#bib.bib142)].
    These approaches employ extensive domain randomization and typically leverage
    privileged information (e.g., object shape information, dynamic properties) and
    dense rewards when training in simulation. An open challenge is integrating these
    in-hand manipulation skills with other manipulation abilities (e.g., tool use),
    which require re-orientation to a target configuration suitable for a downstream
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.4 Non-prehensile Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Non-prehensile manipulation, namely moving objects without grasping, is crucial
    when objects are too large to be grasped, grasps are occluded, or in tool use.
    Object pushing abilities have long been demonstrated with RL [[118](#bib.bib118)],
    and studied in connection to grasping [[109](#bib.bib109), [143](#bib.bib143)].
    Recently, general non-prehensile re-orientation of diverse objects has been enabled
    through sim-to-real transfer of RL policies [[144](#bib.bib144), [145](#bib.bib145)].
    Similar to in-hand manipulation, learning with privileged information (i.e., object
    geometry) before distilling a student policy is a common approach. Further work
    is warranted to integrate these skills with prehensile and in-hand behaviors and
    to develop extrinsic dexterity, where the environment is used to facilitate manipulation.
    How to synthesize these capabilities for general-purpose, open-world manipulation
    remains an open question.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.5 Trends and Open Challenges in Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: RL is beginning to achieve real-world success in various manipulation problems.
    Generally, RL has been more successful in domains where the space of tasks is
    more constrained—grasping, in-hand manipulation, and assembly—rather than less,
    e.g., end-to-end pick-and-place. These more constrained tasks allow for a priori
    reward design and zero-shot sim-to-real transfer, whereas open-world pick-and-place
    and contact-rich manipulation require generalizing to diverse objects and tasks.
    The limitations of physical simulation may also preclude scaling sim-to-real for
    contact-rich tasks. Differentiable simulation has shown promise for this challenge [[146](#bib.bib146)].
    Open-world manipulation will require several advances, including scaling collections
    of simulated assets and tasks; few-shot sim-to-real [[131](#bib.bib131)]; multi-task
    learning [[114](#bib.bib114), [125](#bib.bib125)]; learning autonomously in the
    real world [[120](#bib.bib120), [117](#bib.bib117), [54](#bib.bib54)]; learning
    reward functions from examples [[120](#bib.bib120)] or human videos [[121](#bib.bib121)];
    and utilizing human demonstrations [[127](#bib.bib127)], offline data [[122](#bib.bib122)]
    and foundation models [[124](#bib.bib124)]. Incorporating priors, such as symmetry [[112](#bib.bib112)]
    and geometry [[147](#bib.bib147)], is promising for improving sample efficiency,
    generalization, and safety. Learning more complex behaviors, e.g. bimanual [[148](#bib.bib148)]
    or dynamic tasks like table tennis [[149](#bib.bib149)], is another important
    avenue for future work [[11](#bib.bib11), [19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, action spaces are typically chosen by domain experts to match
    each problem at hand. Open-loop grasping tends to employ an abstraction of motion
    generation for reaching and closing the fingers, whereas closed-loop grasping,
    assembly, and end-to-end pick-and-place methods typically control the end-effector
    Cartesian pose or velocity. Most in-hand manipulation approaches control the fingers
    in configuration space, keeping the end-effector itself in a fixed position. Equipping
    one agent with these various manipulation abilities remains an important challenge
    for deploying capable manipulators in the real world. Moreover, many of these
    real-world successes are demonstrated on short-horizon tasks; further work is
    warranted to build agents that can reason over longer periods of time and compose
    learned abilities together to solve long-horizon tasks [[11](#bib.bib11), [123](#bib.bib123),
    [132](#bib.bib132), [150](#bib.bib150), [151](#bib.bib151)].
  prefs: []
  type: TYPE_NORMAL
- en: '{summary}'
  prefs: []
  type: TYPE_NORMAL
- en: '[Key Takeaways]'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RL solutions for manipulation are generally less mature than locomotion, with
    few deployments in the wild, yet there exist many impressive demonstrations in
    representative real-world conditions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulation subproblems where tasks can be enumerated a priori—e.g., grasping,
    in-hand manipulation, assembly—allow for zero-shot sim-to-real transfer, facilitating
    many of the real-world successes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating manipulation subfields and connecting with task planning to build
    a generally competent manipulator remains an open challenge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.4 Mobile Manipulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Mobile manipulators are robotic agents combining mobility and manipulation
    competencies, unlocking applications in households, healthcare, and logistics.
    Mobile manipulation (MoMa) problems present unique challenges requiring more than
    a simple concatenation of locomotion and manipulation, including the need to control
    and synchronize many degrees-of-freedom governing multiple body components (e.g.,
    head, arm(s), and base/legs), strong partial observability and tasks with a natural
    long horizon. DRL has been applied to tackle various types of MoMa tasks, including
    1) learning precise, real-time whole-body control; 2) learning object perception
    and interaction in short-horizon interactive tasks; and 3) high-level decision-making
    in long-horizon interactive tasks. In this section, we review works addressing
    these three problems summarized in Figure [5](#S4.F5 "Figure 5 ‣ 4.4.3 Long-horizon
    Interactive Tasks ‣ 4.4 Mobile Manipulation ‣ 4 Competency-Specific Review ‣ Deep
    Reinforcement Learning for Robotics: A Survey of Real-World Successes").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 Learning Whole-Body Control
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The common goal in whole-body control (WBC) for mobile manipulators is to determine
    an action or sequence of actions for all degrees of freedom of the body to reach
    a desired configuration, possibly fulfilling additional constraints. Frequently,
    the desired configuration is specified as the desired position or pose of one
    or more of the links of the agent, e.g., the desired pose of the end-effector [[152](#bib.bib152),
    [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155)]. While there exist
    model-based analytical methods for whole-body control in advanced control theory
    literature [[156](#bib.bib156)], DRL has been explored as a powerful alternative
    in situations where either the system dynamics are hard to model (e.g., leg-ground
    contact, slippery wheels, unknown manipulator dynamics), or when the inference-time
    computation is constrained—a frequent problem in MoMa tasks due to the robot embodiment’s
    complexity. For example, Wang et al. [[152](#bib.bib152)] and Fu et al. [[154](#bib.bib154)]
    learned whole-body policies that enable a wheeled mobile manipulator and a quadruped
    with an arm to reach points in 3D space with their end-effector. Ma et al. [[153](#bib.bib153)]
    learned a locomotion policy robust to random wrench perturbation and used an MPC
    planner to control the arm for point reaching.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, whole-body control problems focus on precise control of the end-effector
    without taking into account the agent’s surroundings: the policy takes proprioceptive
    sensing as the observation and tries to minimize the difference to the desired
    configuration. Notably, recent works have explored how to integrate low-level
    whole-body control skills into hierarchical RL architectures [[157](#bib.bib157),
    [158](#bib.bib158), [159](#bib.bib159)], where the higher level perceives the
    surroundings and queries a low-level whole-body skill with the right desired pose
    as the goal. This extends the success of DRL in learning WBC to more complex interactive
    MoMa tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Short-horizon Interactive Tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Short-horizon interactive tasks often focus on learning specific sensorimotor
    skills that require no memory or planning capabilities. Many works have explored
    applying DRL to these short-horizon tasks, including grasping [[160](#bib.bib160),
    [161](#bib.bib161), [159](#bib.bib159)], ball kicking [[162](#bib.bib162), [158](#bib.bib158)],
    collision-free target tracking [[163](#bib.bib163), [164](#bib.bib164), [165](#bib.bib165)],
    interactive navigation [[166](#bib.bib166)], and door opening [[167](#bib.bib167),
    [168](#bib.bib168)]. Notably, Ji et al. [[158](#bib.bib158)] used hierarchical
    RL to learn soccer kicking skills, where a high-level policy generates the desired
    end-effector trajectory executed by a low-level policy. Hu et al. [[163](#bib.bib163)]
    improved the training efficiency by deriving a low-variance policy gradient update
    through action space decomposition. Cheng et al. [[169](#bib.bib169)] learned
    separate skills for locomotion and manipulation on a quadruped in simulation and
    chained different skills using a behavior tree. Ji et al. [[162](#bib.bib162)]
    learned a whole-body dribbling policy in simulation, transferring it zero-shot
    to the real world using extensive domain randomization in visual input and simulation
    parameters. Liu et al. [[159](#bib.bib159)] learned grasping policies via hierarchical
    RL and teacher-student distillation, where an image-based student policy is distilled
    from a state-based teacher policy. Interactive tasks require policies to make
    decisions based on sensor observations of their surroundings. Therefore, the policy
    usually takes in high-dimensional observations such as camera or lidar readings
    (Table [2](#A2.T2 "Table 2 ‣ Appendix B Additional Tables ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes")). Meanwhile, these tasks
    often involve hard-to-model dynamics such as contact forces or articulated object
    motion, making model-free RL an appealing alternative both to classical methods
    and to model-based RL (Table [4](#A2.T4 "Table 4 ‣ Appendix B Additional Tables
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Long-horizon Interactive Tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For a mobile manipulator to function in unstructured environments such as offices [[170](#bib.bib170)],
    homes, or kitchens [[171](#bib.bib171)], it needs to handle tasks with long horizons
    and strong partial observability. However, end-to-end RL struggles on long-horizon
    tasks due to the difficulty of exploring the state-action space to find successful
    strategies, requiring many samples to train. Partial observability is also challenging
    for DRL as it requires complex network architectures that can encode observation
    history (e.g., RNNs or LSTMs) or some other mechanism to aggregate observations
    and model the environment (e.g., mapping or 3D reconstruction). One possible way
    to mitigate this issue is to make use of expert demonstrations or simulation data
    to bootstrap the learning process. For instance, Herzog et al. [[170](#bib.bib170)]
    exploited simulation data and scripted policies to speed up the training process
    for off-policy RL in a waste sorting task. Another promising direction is to take
    a divide-and-conquer approach by sequentially chaining short-horizon interactive
    skills through planning [[171](#bib.bib171)] or hierarchical RL [[157](#bib.bib157)].
    Overall, solving long-horizon interactive tasks using DRL is an open challenge
    and under-explored area, but solving this type of task is necessary to create
    truly capable household and human-assistant robots.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4838e59944377098eebc1c85507ff9ab.png)'
  prefs: []
  type: TYPE_IMG
- en: '| WBC | [152](#bib.bib152),[153](#bib.bib153),[154](#bib.bib154),[155](#bib.bib155)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Short-Horizon | [158](#bib.bib158),[159](#bib.bib159),[160](#bib.bib160),[161](#bib.bib161),[162](#bib.bib162),[163](#bib.bib163),[164](#bib.bib164),[165](#bib.bib165),[166](#bib.bib166),[167](#bib.bib167),[168](#bib.bib168),[169](#bib.bib169)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Long-Horizon | [157](#bib.bib157),[170](#bib.bib170),[171](#bib.bib171) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5: Top: An overview of the three MoMa challenges discussed in Sec. [4.4](#S4.SS4
    "4.4 Mobile Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning
    for Robotics: A Survey of Real-World Successes"), including whole-body control
    [[152](#bib.bib152), [154](#bib.bib154)] (WBC) and short- [[161](#bib.bib161),
    [169](#bib.bib169)] and long-horizon [[157](#bib.bib157), [171](#bib.bib171)]
    interactive tasks; Bottom: MoMa papers reviewed in Sec. [4.4](#S4.SS4 "4.4 Mobile
    Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes").Color map indicates levels of real-world
    success: *Limited Lab*, *Diverse Lab*, *Limited Real*, and *Diverse Real*.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.4 Trends and Open Challenges in Mobile Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Thanks to the generalization of humanoids and other robot embodiments, and
    the advances in locomotion and stationary manipulation, DRL for MoMa is a growing
    field with increasing research attention. Based on our analysis, we infer some
    trends and open questions. First, compared to stationary manipulation, MoMa tasks
    have a significantly larger workspace, making safe real-world exploration challenging.
    As such, existing works mainly perform training in simulations where safety is
    not a concern (Table [3](#A2.T3 "Table 3 ‣ Appendix B Additional Tables ‣ Deep
    Reinforcement Learning for Robotics: A Survey of Real-World Successes")). In the
    rare occurrences of real-world RL, strong domain knowledge, e.g., in the form
    of motion priors [[168](#bib.bib168), [160](#bib.bib160)] and/or demonstrations [[168](#bib.bib168),
    [170](#bib.bib170)], is used to enable safe and efficient exploration. Plus, MoMa’s
    large workspace demands a more sophisticated form of memory and scene representation.
    Representations that work well for navigation often fail to capture the dynamic
    characters in manipulation. While advances in sample efficiency, memory, and safe
    real-world RL promise new opportunities, scaling them to the open-worldness and
    vast workspaces inherent to MoMa remains challenging.'
  prefs: []
  type: TYPE_NORMAL
- en: Second, mobile manipulators have very diverse morphologies compared to other
    types of robots, including wheeled robots with arms [[170](#bib.bib170), [163](#bib.bib163),
    [167](#bib.bib167), [164](#bib.bib164), [152](#bib.bib152), [160](#bib.bib160),
    [161](#bib.bib161), [171](#bib.bib171), [168](#bib.bib168)], quadrupeds with arms [[153](#bib.bib153),
    [154](#bib.bib154), [159](#bib.bib159), [157](#bib.bib157)], humanoids [[155](#bib.bib155)],
    and even quadrupeds using their legs for both locomotion and manipulation, i.e.,
    loco-manipulation [[158](#bib.bib158), [162](#bib.bib162), [169](#bib.bib169),
    [166](#bib.bib166)]. Each morphology brings unique challenges and opportunities.
    For example, wheeled mobile manipulators are easier to model and generally more
    kinematically stable, facilitating learning only for the manipulation component,
    while legged mobile manipulators can traverse uneven terrains but are harder to
    control, even for simple navigation phases. New research in both morphology-agnostic
    and morphology-specific RL methods is necessary for MoMa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, perhaps due to the diverse morphologies, very diverse choices of action
    spaces are observed in the MoMa literature (Table [1](#A2.T1 "Table 1 ‣ Appendix
    B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World
    Successes")), including direct joint control [[163](#bib.bib163), [41](#bib.bib41),
    [167](#bib.bib167)], task-space control with classical model-based [[164](#bib.bib164),
    [161](#bib.bib161)], task-space control with learned low-level controllers [[169](#bib.bib169),
    [158](#bib.bib158), [157](#bib.bib157)], and even factored actions that only controls
    a part of the embodiment [[153](#bib.bib153), [164](#bib.bib164)]. Choosing the
    right action space is crucial for performance, as it affects the temporal abstraction
    levels and robot controllability. Yet, there is currently no principled way to
    select the appropriate action space for the diverse set of MoMa tasks. {summary}
    [Key Takeaways]'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DRL has achieved initial success in mobile manipulation, in particular on short-horizon
    tasks, especially by leveraging training in simulation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a suitable action space is critical for RL in MoMa, especially given
    the diversity in the morphologies of existing MoMa systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The successes notwithstanding, existing methods are still insufficient for tackling
    multi-tasking, representing long-term memory, and performing safe exploration
    in the real world, providing opportunities for future improvements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.5 Human-Robot Interaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this subsection, we review works where DRL has been applied to human-robot
    interaction (HRI)—on robotic systems for use by or with humans. While HRI tasks
    can have varying objectives and involve robots with distinct morphology, the presence
    of humans introduces shared challenges, including safety, interpretability, and
    human modeling, that distinguish HRI from other robot problems not involving humans.
    Notice that this section focuses on robotic systems with HRI competencies (i.e.,
    interact with humans during task execution), whereas works that only involve humans
    during training are out of the scope of this section. HRI tasks can be broadly
    classified into three main categories: collaborative physical HRI (pHRI), where
    the robot and humans physically collaborate with a shared objective; non-collaborative
    pHRI, where the robot and humans share the same physical space but have distinct
    objectives; and shared autonomy, where humans act as teleoperators, and the robot
    autonomously interprets and executes the teleoperation command. In this section,
    we review works from these three categories. Figure [6](#S4.F6 "Figure 6 ‣ 4.5.2
    Non-collaborative pHRI ‣ 4.5 Human-Robot Interaction ‣ 4 Competency-Specific Review
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")
    summarizes the papers reviewed.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1 Collaborative pHRI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most intuitive type of HRI arises when a robot and a human physically collaborate
    toward accomplishing a shared goal—a common theme for service robots that assist
    humans in household activities. For example, Ghadirzadeh et al. [[172](#bib.bib172)]
    tackled the collective packaging task, where recurrent Q-learning is combined
    with a behavior tree to minimize the packaging time of a human worker. Christen
    et al. [[173](#bib.bib173), [174](#bib.bib174)] focused on object hand-over from
    a human to a robot, using RL to learn a simulated human hand-over policy and a
    robot policy to grasp the objects handed over by the human. Noticeably, existing
    works for collaborative pHRI share a similar procedure: learning a human model
    from pre-collected data to train a robot policy in simulation. This similarity
    is likely due to the high cost of collecting online interactions for collaborative
    tasks, which require continuous human attention and physical response to the robot’s
    behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 Non-collaborative pHRI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In non-collaborative pHRI tasks, a robot operates alongside humans in the same
    physical space but with different objectives. A representative example is social
    navigation， where a robot navigates through crowded environments. Chen et al.
    [[175](#bib.bib175)] trained a robot for social navigation in simulation, where
    a hand-crafted reward is used to encourage socially compliant behavior, and zero-shot
    transferred the policy to a real-world corridor. Everett et al. [[176](#bib.bib176)]
    expanded on this work to incorporate human motion histories into decision-making
    by modeling the value network with an LSTM. Liang et al. [[177](#bib.bib177)]
    developed a high-fidelity simulator of human motions to train navigation policies
    taking lidar scans as inputs, and demonstrated reliable sim-to-real transfer capabilities.
    Hirose et al. [[178](#bib.bib178)] learned navigation policies alongside humans
    in the real world. A residual Q-function is learned on top of an offline pre-trained
    Q-function to generate adaptive behavior on the fly. Unlike collaborative tasks,
    humans do not actively participate in the robot’s activities in non-collaborative
    tasks, making it easier to hard-code human behaviors [[175](#bib.bib175), [176](#bib.bib176),
    [177](#bib.bib177)] or train in the real world [[178](#bib.bib178)], resulting
    in successful real-world implementations. Aside from social navigation, Liu et
    al. [[179](#bib.bib179)] considered manipulation while avoiding collision with
    humans, where an action space transformation is conducted to ensure safe exploration
    in RL.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c87b59339bda5341203f62e5eae394dc.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Collaborative pHRI | [173](#bib.bib173), [172](#bib.bib172), [174](#bib.bib174),
    [180](#bib.bib180) |'
  prefs: []
  type: TYPE_TB
- en: '| Non-collaborative pHRI | [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177),
    [178](#bib.bib178), [179](#bib.bib179) |'
  prefs: []
  type: TYPE_TB
- en: '| Shared Autonomy | [181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 6: Top: An overview of the three types of HRI tasks discussed in Sec. [4.5](#S4.SS5
    "4.5 Human-Robot Interaction ‣ 4 Competency-Specific Review ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes"), including collaborative
    [[173](#bib.bib173)] and non-collaborative [[175](#bib.bib175)] pHRI tasks, and
    shared autonomy [[182](#bib.bib182)]; Bottom: Papers reviewed in Sec. [4.5](#S4.SS5
    "4.5 Human-Robot Interaction ‣ 4 Competency-Specific Review ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes"). The color map indicates
    the levels of real-world success: *Sim Only*, *Limited Lab*, *Diverse Lab*, and
    *Limited Real*.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.3 Shared Autonomy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Shared autonomy is an HRI paradigm that does not involve physical contact between
    humans and robots. Instead, the robot takes actions to complete tasks based on
    human instructions such as keyboard control or language commands. In this setting,
    RL can be used to learn a policy that conditions on human inputs and generates
    robot actions that optimize some external task rewards or constraints while aligning
    with the user instructions. For instance, Reddy et al. [[182](#bib.bib182)] tackled
    the quadrotor perching task, where a Q-function is learned based on task reward,
    and the robot chooses actions that are close to the user input and above a preset
    task value threshold. Schaff et al. [[183](#bib.bib183)] formulated shared autonomy
    for simulated quadrotor control as a constrained optimization problem, where a
    residual RL policy is learned to minimally change the human input policy while
    satisfying a set of task-invariant constraints. More recently, advances in NLP
    have opened up the possibility for shared autonomy through natural language instructions.
    For example, Nair et al. [[181](#bib.bib181)] learned a language-conditioned policy
    for table-top manipulation using model-based RL on a pre-collected dataset with
    hand-labeled language instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.4 Trends and Open Challenges in HRI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Despite the importance of HRI for household robot applications, RL has seen
    fewer successes in HRI compared to other robotics domains like locomotion and
    manipulation. A primary challenge for applying RL to HRI problems is properly
    incorporating human or human-like priors into the training process, which can
    often be non-markovian, have limited rationality, and are often costly to collect.
    Existing works have primarily tackled this challenge in three ways. First, a straightforward
    approach is to train the policies directly in real-world environments alongside
    humans. However, this approach presents significant challenges to the sample complexity
    of the algorithm since collecting real-world interaction data is costly, especially
    when humans are actively involved. As such, works using this approach either focus
    on simple tasks [[180](#bib.bib180)] or rely on pretraining to derive a good initial
    policy and reduce sample complexity [[178](#bib.bib178)]. Second, an alternative
    to avoid costly real-world learning is to learn a reasonable human model to simulate
    humans during training. This approach is particularly appealing in domains where
    human actions are fairly easy to model, such as shared autonomy, where a human
    policy can be learned by imitating a set of human actors [[183](#bib.bib183),
    [182](#bib.bib182)]. In tasks where human actions are more complex, human models
    have been created using motion capture [[172](#bib.bib172), [179](#bib.bib179)],
    crowd-sourcing [[181](#bib.bib181)], and RL [[173](#bib.bib173)]. Third, when
    human behaviors are simple, human models can be directly hardcoded using domain
    knowledge [[175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177)], and be
    incorporated either as parts of the simulation or as behavioral constraints. Although
    this approach is not scalable and inapplicable for many tasks, these simplified
    human models can serve as a useful source for pretraining to improve sample efficiency
    for real-world learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, two promising future directions emerge: first, developing safe and
    sample-efficient RL algorithms to enable direct real-world RL, possibly by leveraging
    known human behavior models; second, building high-fidelity human behavior simulation
    to bridge sim-to-real gaps for zero-shot sim-to-real transfer. Future advances
    in these directions promise to broaden the application of RL to HRI problems significantly.'
  prefs: []
  type: TYPE_NORMAL
- en: '{summary}'
  prefs: []
  type: TYPE_NORMAL
- en: '[Key Takeaways]'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to other robotics domains, DRL has achieved limited success in HRI,
    especially on tasks that require the robot to collaborate with humans physically.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A key challenge for applying RL to HRI lies in collecting realistic interactive
    experiences with humans, which can, in principle, be obtained by either directly
    training in the real world or by building high-fidelity human models for simulations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Existing works have explored both approaches in simple tasks. However, whether
    and how we can scale up these approaches to more difficult tasks remains unclear.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.6 Multi-Robot Interaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multi-robot interaction is often solved as a MARL problem, which, in the most
    general case, is described using a partially observable stochastic game (POSG)
    with distinct reward functions and action and observation spaces, although most
    cooperative real-world problems model the problem as Decentralized POMDPs. We
    highlight three real-world domains where DRL has been successfully applied to
    learn multi-robot interaction: collision avoidance and navigation, multi-agent
    loco-manipulation, and robot soccer.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.1 Multi-Agent Collision Avoidance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Chen et al. [[184](#bib.bib184)] and Everett et al. [[185](#bib.bib185)] model
    a Dec-MDP in which the policy takes the state vector consisting of positions,
    velocities, and radii of all the robots as input to predict the velocities for
    each robot. The policy is preconditioned via finetuning using ORCA [[186](#bib.bib186)].
    The reward function is sparse, consisting of a goal-reaching reward and collision
    penalties. These works successfully developed collision-avoidance policies in
    simulation and showcased hardware results on aerial and ground robots. The multirotors
    used onboard sensors and controllers to execute maneuvers suggested by the policy.
    The ground robot, equipped with affordable onboard sensors (under $1000$ USD),
    was able to navigate through pedestrian traffic, effectively avoiding collisions
    despite imperfect perception and diverse pedestrian behaviors unseen during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/40ef7a6f713e558e2180840e856b8a42.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Multi-Robot Collision Avoidance | [184](#bib.bib184), [185](#bib.bib185),
    [187](#bib.bib187), [188](#bib.bib188), [189](#bib.bib189) |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-Robot Loco-Manipulation | [190](#bib.bib190) |'
  prefs: []
  type: TYPE_TB
- en: '| Robot Soccer | [191](#bib.bib191) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 7: Top: An overview of the three representative multi-robot interaction
    domains reviewed in Sec. [4.6](#S4.SS6 "4.6 Multi-Robot Interaction ‣ 4 Competency-Specific
    Review ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes"),
    including multi-robot collision avoidance [[187](#bib.bib187)], multi-robot manipulation
    via locomotion [[190](#bib.bib190)], and robot soccer [[191](#bib.bib191)]; Bottom:
    Multi-robot interaction papers reviewed in Sec. [4.6](#S4.SS6 "4.6 Multi-Robot
    Interaction ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes"). See the caption of Fig. [2](#S4.F2 "Figure
    2 ‣ 4.1.1 Quadruped Locomotion ‣ 4.1 Locomotion ‣ 4 Competency-Specific Review
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")
    for color map description.'
  prefs: []
  type: TYPE_NORMAL
- en: Other works [[187](#bib.bib187), [188](#bib.bib188)] have also modeled the problem
    as a Dec-MDP with the objective of time-to-goal minimization. These methods differ
    from the previous approaches in multiple respects. First, the policy takes raw
    lidar scans as input instead of the states of the other agents and thus does not
    depend on precise sensing and perception. Second, they do not precondition or
    finetune the policy using ORCA but instead employ curriculum learning and a dense
    reward function to facilitate training. Third, to deal with more complex multi-agent
    scenarios, it utilizes a hybrid controller to swap out the learned policy with
    a classical controller instead of restricting the other robots’ motion via constant
    linear velocity models.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Sartoretti et al. [[189](#bib.bib189)] used DRL to prevent agents from
    blocking each other in multi-agent pathfinding problems. A “blocking penalty”
    is applied when an agent reaches its goal but prevents another agent from doing
    the same. This strategy, combined with imitation learning and environment sampling,
    expedites convergence. The algorithm was tested on a small fleet of autonomous
    ground vehicles in a factory floor mock-up.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.2 Multi-Agent Loco-Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We highlight a recent result [[190](#bib.bib190)] in multi-agent manipulation
    via locomotion (i.e., loco-manipulation). This involves multiple robots using
    movement to manipulate objects or interact with environments. Nachum et al. [[190](#bib.bib190)]
    focus on enabling multiple quadrupeds to perform complex tasks like manipulation
    and coordination using model-free RL. A significant challenge in applying RL to
    coordination or manipulation tasks with multiple legged robots is the complexity
    of interactions between agents or between agents and objects, which usually requires
    extensive real-world trial-and-error learning. To address this, this work employs
    a hierarchical sim2real approach demonstrating zero-shot sim-to-real transfer
    for object avoidance and targeted object pushing. Additionally, the work showcases
    a multi-agent scenario where two quadrupeds coordinate to move a heavy block to
    a specified location and orientation, illustrating the potential of using locomotion
    for coordinated multi-agent manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.3 Robot Soccer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'RL has also been successful in real physical soccer-playing robots in the RoboCup
    Standard Platform League. Many of these works focus on training a policy for a
    single robot, which is then transferred to multiple robots. See Sec. [4.1](#S4.SS1
    "4.1 Locomotion ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes") and Sec. [4.4](#S4.SS4 "4.4 Mobile
    Manipulation ‣ 4 Competency-Specific Review ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes") for discussions on these works focusing
    on single-robot competencies for robot soccer. A recent work [[191](#bib.bib191)]
    further applied RL to learn a variety of dynamic and complex movement skills like
    walking, turning, kicking, and rapid recovery from falls in *1v1 robot soccer
    play*. The agents learn to apply skills appropriately via self-play and showcase
    sophisticated multi-agent competencies such as opponent interception.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6.4 Trends and Open Challenges in Multi-Robot Interaction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the most significant challenges in multi-agent systems is managing the
    complexity and scalability of the systems as the number of agents increases. This
    challenge is evident in multi-agent manipulation via locomotion and robot soccer,
    where the increase in team size exponentially escalates the complexity of the
    interactions. The transition from controlled, simulated environments to unpredictable
    real-world conditions remains a formidable challenge. Although promising results
    have been shown in domains like collision avoidance, the variability in real-world
    dynamics, such as sensor inaccuracies, unexpected obstacles, and dynamic human
    interactions, often degrades system performance. Next, while RL has provided impressive
    results in learning complex behaviors autonomously, integrating these learned
    behaviors with classical control methods is an increasingly popular area of research.
    Finally, the ability of multi-robot systems to generalize across different tasks
    and environmental conditions presents a substantial opportunity for research.
  prefs: []
  type: TYPE_NORMAL
- en: '{summary}'
  prefs: []
  type: TYPE_NORMAL
- en: '[Key Takeaways]'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current state-of-the-art in RL-based multi-robot interaction is limited to cooperative
    settings with identical reward functions, action spaces, and observation spaces.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predominantly, DRL in multi-robot settings is applied to collision avoidance
    among ground robots (as compared to manipulation via locomotion and robot soccer).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Critical research areas moving forward include dealing with $(i)$ communication
    and networking between agents, $(ii)$ convergence and stability, $(iii)$ scalability,
    $(iv)$ general non-cooperative settings, $(v)$ different robot morphologies and
    applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5 General Trends and Open Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We conclude this survey by summarizing the patterns behind current real-world
    successes in robotics achieved with DRL and the characteristics of those less
    successful cases. Overall, more mature solutions (i.e., L3-4) have often followed
    the zero-shot sim-to-real transfer scheme (Table [3](#A2.T3 "Table 3 ‣ Appendix
    B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World
    Successes")), which works particularly well for locomotion and navigation. The
    dynamics involved in these competencies, especially terrestrial locomotion and
    navigation, are relatively stable and easy to simulate. Dense and shaped rewards,
    which simplify exploration and improve sample efficiency, have also been effective
    (Table [2](#A2.T2 "Table 2 ‣ Appendix B Additional Tables ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes")), leading to the predominant
    use of stable and robust model-free, on-policy algorithms in these domains (Table [5](#A2.T5
    "Table 5 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")). The sim-to-real scheme has been successful
    for manipulation problems in which dense reward functions can be designed a priori
    (e.g., grasping, assembly, in-hand, non-prehensile manipulation), but less so
    in tasks with more diversity (e.g., pick-and-place). The community has been striving
    to explore alternative solutions that do not require simulation (Table [3](#A2.T3
    "Table 3 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics:
    A Survey of Real-World Successes")) or reward shaping (Table [4](#A2.T4 "Table
    4 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A
    Survey of Real-World Successes")) and adopt policy optimization algorithms with
    better sample efficiency (Table [5](#A2.T5 "Table 5 ‣ Appendix B Additional Tables
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")).
    Human demonstrations (Table [4](#A2.T4 "Table 4 ‣ Appendix B Additional Tables
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes"))
    are effective for enabling real-world learning, particularly in manipulation tasks
    that are not prohibitively complex to demonstrate. For competencies where both
    accurate simulation and real-world rollouts are prohibitive (e.g., HRI) or where
    stable, scalable RL algorithms are missing (e.g., multi-robot interaction), successful
    real-world examples are much sparser. In the remainder of this section, we identify
    several concrete open challenges that are opportunities for further extending
    DRL’s applications, in particular for those currently less successful domains.'
  prefs: []
  type: TYPE_NORMAL
- en: Improving Stability and Sample-Efficiency in RL Algorithms.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While on-policy RL methods are often preferred due to their robustness to hyperparameters,
    collecting large amounts of on-policy data can be prohibitive, especially for
    real-world RL. Even in the predominant zero-shot sim-to-real setting, the sample
    efficiency of on-policy RL is problematic for tasks such as long-horizon mobile
    manipulation [[170](#bib.bib170), [171](#bib.bib171)] and agile legged navigation [[20](#bib.bib20),
    [96](#bib.bib96)], where the long task horizons, large operational spaces, sparse
    rewards, and complex contact dynamics hinder efficient exploration and stable
    learning. Sample efficiency can also be a crucial issue in problems with temporally
    extended action spaces [[32](#bib.bib32), [36](#bib.bib36)]. Fundamental algorithmic
    advances to develop RL algorithms that are at least as robust but more sample-efficient
    than on-policy methods are thus crucial for expanding RL’s applications in robotics.
    An appealing direction is leveraging off-policy or offline samples to complement
    or replace on-policy exploration. However, off-policy and offline RL are often
    less stable due to the distributional shift between behavioral and learning policy
    experiences. Promising efforts have been made to derive scalable and more stable
    off-policy [[110](#bib.bib110)] and offline RL algorithms [[124](#bib.bib124)]
    for manipulation and MoMa [[170](#bib.bib170)]. Fine-tuning offline learned policies
    with online updates can further enhance performance in an efficient manner [[48](#bib.bib48),
    [122](#bib.bib122)]. However, stable online fine-tuning is non-trivial, especially
    for value-based RL [[192](#bib.bib192), [193](#bib.bib193)]. Combining model-free
    and model-based approaches is another promising direction to derive sample-efficient
    RL algorithms [[194](#bib.bib194)]. Lastly, these advances have primarily focused
    on single-robot problems. Multi-robot problems present greater challenges as the
    complexity of multi-robot interaction escalates exponentially with the number
    of robots. The scalability and stability of MARL remain open questions that hinder
    RL’s application for multi-robot interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Real-World Learning.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In our analysis of RL for robot competencies (Sec. [4](#S4 "4 Competency-Specific
    Review ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")),
    real-world learning was often mentioned as one of the open challenges. A learning
    process carried out in the real world is crucial for robotic problems where the
    zero-shot sim-to-real transfer procedure is impractical due to the lack of high-fidelity
    simulation, such as open-world and contact-rich manipulation, lightweight quadrotor
    navigation, and physical HRI. Although some progress has been made, particularly
    for manipulation (Table [3](#A2.T3 "Table 3 ‣ Appendix B Additional Tables ‣ Deep
    Reinforcement Learning for Robotics: A Survey of Real-World Successes")), successful
    real-world learning examples are much rarer than zero-shot sim-to-real transfer,
    presenting exciting opportunities for future research. Two main issues need to
    be addressed for real-world RL learning. The first issue is *how to collect many
    useful experiences in a safe manner?* In domains where oracle policies, like humans [[124](#bib.bib124)]
    and scripts [[170](#bib.bib170)], are available, demonstrations can be collected
    for offline learning. However, offline RL faces challenges such as distributional
    shifts, and the demonstration data can be suboptimal and costly to collect for
    human experts. Real-world rollouts require automatic resets [[120](#bib.bib120),
    [117](#bib.bib117), [53](#bib.bib53)] and safe exploration mechanisms [[168](#bib.bib168)]
    to minimize human effort and ensure safety. Such mechanisms are still missing
    in most problem domains and present an opportunity for future development, especially
    for safety-critical applications [[92](#bib.bib92), [102](#bib.bib102)]. To date,
    human-in-the-loop learning (for resets and safety) is currently the only alternative [[92](#bib.bib92)],
    leaving automated real-world learning a desirable future capability. In addition
    to procedural and algorithmic improvements, safe real-world exploration may also
    be facilitated through hardware advances, such as adaptive and less fragile hardware
    and mechanisms that ensure safety passively [[195](#bib.bib195)]. The second issue
    is how do we accelerate training to require fewer experiences? A promising avenue
    is to explore what modules can be updated with real-world samples and how. Instead
    of updating the entire policy with model-free RL, some solutions explore adapting
    vision encoders [[43](#bib.bib43)] or learning (residual) dynamics models [[7](#bib.bib7),
    [102](#bib.bib102), [56](#bib.bib56)] from real-world samples. These alternatives
    improve efficiency; we predict future successful real-world training procedures
    exploring alternative combinations of frozen-trainable modules.'
  prefs: []
  type: TYPE_NORMAL
- en: Learning for Long-Horizon Robotic Tasks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Long-horizon tasks pose a fundamental challenge to RL algorithms, requiring
    directed exploration and temporal credit assignment over long stretches of time.
    Many such real-world tasks require integrating diverse abilities. By contrast,
    the vast majority of the RL successes we have reviewed are in short-horizon problems,
    e.g., controlling a quadruped to walk at a given velocity or controlling a manipulator
    to rotate an object in hand. A promising avenue for solving long-horizon tasks
    is learning skills and composing them, enabling compositional generalization.
    This approach has seen success in navigation [[96](#bib.bib96), [97](#bib.bib97)],
    manipulation [[11](#bib.bib11), [115](#bib.bib115), [132](#bib.bib132), [150](#bib.bib150)],
    and MoMa [[157](#bib.bib157), [171](#bib.bib171)]. A critical question for future
    work is: what skills should the robot learn?. While some successes have been achieved
    with manually specified skills and reward functions [[32](#bib.bib32), [50](#bib.bib50),
    [96](#bib.bib96), [123](#bib.bib123), [150](#bib.bib150), [114](#bib.bib114)],
    these approaches heavily rely on domain knowledge. Some efforts have been made
    to explore unified reward designs for learning multi-skill locomotion policies [[42](#bib.bib42),
    [51](#bib.bib51), [49](#bib.bib49)]. Formulating skill learning as goal-conditioned [[125](#bib.bib125)]
    or unsupervised RL [[196](#bib.bib196), [197](#bib.bib197)] is promising for more
    general problems. A second critical question is: how should these skills be combined
    to solve long-horizon tasks? Various designs have been explored, including hierarchical
    RL [[32](#bib.bib32), [157](#bib.bib157)], end-to-end training [[123](#bib.bib123),
    [49](#bib.bib49)], and planning [[132](#bib.bib132), [150](#bib.bib150), [171](#bib.bib171)].
    This question will also be central to integrating various competencies toward
    general-purpose robots; recent advances along this line have opened up exciting
    possibilities, including wheel-legged navigation [[97](#bib.bib97)] and loco-manipulation [[51](#bib.bib51),
    [158](#bib.bib158), [162](#bib.bib162), [169](#bib.bib169), [166](#bib.bib166)].'
  prefs: []
  type: TYPE_NORMAL
- en: Designing Principled Approaches for RL Systems.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For each robotic task, an RL practitioner must choose among the many alternatives
    that will define its RL system, both in the problem formulation and solution space
    (see Table [1](#A2.T1 "Table 1 ‣ Appendix B Additional Tables ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes")–[6](#A2.T6 "Table 6
    ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A Survey
    of Real-World Successes")). Many of these choices are made based on expert knowledge
    and heuristics, which are not necessarily optimal and can even harm performance [[198](#bib.bib198),
    [199](#bib.bib199)]. Principled approaches for RL system design, relying less
    on heuristics and manual efforts, will be essential in the future for scalable
    development and deployment, especially for open-world tasks. Here, we note some
    particularly important examples. First, many real-world successes have been achieved
    with dense and shaped rewards designed with heavy engineering efforts, particularly
    in locomotion and navigation (Table [2](#A2.T2 "Table 2 ‣ Appendix B Additional
    Tables ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")).
    Efforts are being made to explore principled reward designs for specific competencies [[42](#bib.bib42),
    [51](#bib.bib51), [49](#bib.bib49)] and more general problems using goal-conditioned [[125](#bib.bib125)]
    or unsupervised RL [[196](#bib.bib196), [197](#bib.bib197)]. Second, various action
    spaces are used, particularly for manipulation and MoMa (Table [1](#A2.T1 "Table
    1 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A
    Survey of Real-World Successes")). The action space choices affect the temporal
    abstraction levels and robustness of the RL policies. Some studies have attempted
    to benchmark different action spaces [[66](#bib.bib66), [86](#bib.bib86), [199](#bib.bib199)],
    but such principled studies and guidelines are still lacking for many problems.
    Another related design choice is the integration of RL with classical planning
    and control modules. The different levels of integration result in different action
    spaces for the RL policies (i.e., low-, mid-, and high-level). The effectiveness
    of end-to-end versus hybrid modular solutions varies by problem [[82](#bib.bib82),
    [86](#bib.bib86), [100](#bib.bib100), [21](#bib.bib21), [200](#bib.bib200)]. Neither
    approach is universally superior. There are many other dimensions that require
    such principled investigations, which are crucial for advancing DRL’s real-world
    success, in addition to exploring new frontiers in algorithms and applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking Real-World Success.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this survey, we classify papers into six levels of real-world success to
    assess the maturity of DRL-based solutions. However, precisely determining these
    levels can be challenging since the only source of information is the experimental
    results reported by the authors, but the varying testing conditions and evaluation
    metrics make direct comparison difficult. This highlights the need for *standard
    evaluation protocols and benchmarks for real-world performance*. While widely
    adopted, low-cost hardware, as seen with quadrupeds, is helpful by enabling standardized
    experimental platforms, it is not sufficient alone. Test environments and tasks
    must also resemble real-world conditions and, more importantly, be *reproducible*.
    Multiple real-world benchmarks have been established, including those for manipulation [[201](#bib.bib201),
    [202](#bib.bib202)] and domestic service robots [[203](#bib.bib203)]. However,
    when it comes to complex open-world problems, the evaluation procedure must also
    scale up to be realistic and informative [[204](#bib.bib204)]. Overall, developing
    scalable evaluation protocols and benchmarks remains an exciting open research
    direction for many problems.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging Foundation Models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lastly, recent advances in large-scale robot dataset [[205](#bib.bib205), [206](#bib.bib206)]
    and robot foundation models [[207](#bib.bib207), [208](#bib.bib208)] present exciting
    open opportunities for RL successes in the real world. Foundation models have
    demonstrated impressive generalization capabilities across domains for reasoning
    and decision-making tasks [[209](#bib.bib209)], showing promise for addressing
    several of the aforementioned challenges of DRL for robotics. For instance, the
    recently introduced DrEureka [[210](#bib.bib210)] algorithm leverages large language
    models (LLMs) to automate reward design and domain randomization configuration
    for sim-to-real transfer without manual tuning. In addition, LLMs and vision-language
    models (VLMs) open up new opportunities to create language-conditioned RL policies
    for novel applications. We refer readers to existing surveys for detailed discussions
    on the opportunities foundation models offer in general [[207](#bib.bib207), [208](#bib.bib208)],
    but we anticipate an increased integration of foundation models into RL solutions
    for real-world robotic tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep reinforcement learning has recently played an important role in the development
    of many robotic capabilities, leading to many real-world successes. Here, we have
    reviewed and categorized these successes, delineating them based on the specific
    robotic competency, problem formulation, and solution approach. Our analysis across
    these axes has revealed general trends and important avenues for future work,
    including algorithmic and procedural improvements, ingredients for real-world
    learning, and holistic approaches toward synthesizing all the competencies discussed
    herein. Harnessing RL’s power to produce capable real-world robotic systems will
    require solving fundamental challenges and innovations in its application; nonetheless,
    we expect that RL will continue to play a central role in the development of generally
    intelligent robots.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank Pieter Abbeel, Yuchen Cui, Shivin Dass, George Konidaris, Jan Peters,
    Eric Rosen, Koushil Sreenath, Eugene Vinitsky, and Zhaoming Xie for their feedback
    on the manuscript. We also thank Google DeepMind for permission to use representative
    images from their work on robot soccer. A portion of this work has taken place
    in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory
    at the University of Texas at Austin. LARG research is supported in part by the
    National Science Foundation (FAIN-2019844, NRT-2125858), the Office of Naval Research
    (N00014-18-2243), Army Research Office (E2061621), Bosch, Lockheed Martin, and
    Good Systems, a research grand challenge at the University of Texas at Austin.
    The views and conclusions contained in this document are those of the authors
    alone. Peter Stone serves as the Chief Scientist of Sony AI and receives financial
    compensation for this work. The terms of this arrangement have been reviewed and
    approved by the University of Texas at Austin in accordance with its policy on
    objectivity in research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Sutton RS, Barto AG. 2018. Reinforcement learning: An introduction. MIT
    press, 2nd ed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] François-Lavet V, Henderson P, Islam R, Bellemare MG, Pineau J, et al.
    2018. An introduction to deep reinforcement learning. Found. Trends in Mach. Learn.
    11(3-4):219–354'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Schrittwieser J, Antonoglou I, Hubert T, Simonyan K, Sifre L, et al. 2020.
    Mastering atari, go, chess and shogi by planning with a learned model. Nature
    588(7839):604–609'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Wurman PR, Barrett S, Kawamoto K, MacGlashan J, Subramanian K, et al. 2022.
    Outracing champion gran turismo drivers with deep reinforcement learning. Nature
    602(7896):223–228'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Yu C, Liu J, Nemati S, Yin G. 2021. Reinforcement learning in healthcare:
    A survey. ACM Comput. Surv. 55(1):1–36'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Afsar MM, Crump T, Far B. 2022. Reinforcement learning based recommender
    systems: A survey. ACM Comput. Surv. 55(7):1–38'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Kaufmann E, Bauersfeld L, Loquercio A, Müller M, Koltun V, Scaramuzza D.
    2023. Champion-level drone racing using deep reinforcement learning. Nature 620(7976):982–987'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Kiran BR, Sobh I, Talpaert V, Mannion P, Al Sallab AA, et al. 2021. Deep
    reinforcement learning for autonomous driving: A survey. IEEE Trans. Intell. Transp.
    Syst. 23(6):4909–4926'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Dulac-Arnold G, Levine N, Mankowitz DJ, Li J, Paduraru C, et al. 2021.
    Challenges of real-world reinforcement learning: definitions, benchmarks, and
    analysis. Mach. Learn. 110(9):2419––2468'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Ibarz J, Tan J, Finn C, Kalakrishnan M, Pastor P, Levine S. 2021. How
    to train your robot with deep reinforcement learning: lessons we have learned.
    Int. J. Robot. Res. 40(4-5):698–721'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Kroemer O, Niekum S, Konidaris G. 2021. A review of robot learning for
    manipulation: Challenges, representations, and algorithms. J. Mach. Learn. Res.
    22(30):1–82'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Xiao X, Liu B, Warnell G, Stone P. 2022. Motion planning and control for
    mobile robot navigation using machine learning: a survey. Auton. Robots 46(5):569–597'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Deisenroth MP. 2011. A survey on policy search for robotics. Found. Trends
    Robot. 2(1-2):1–142'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Brunke L, Greeff M, Hall AW, Yuan Z, Zhou S, et al. 2022. Safe Learning
    in Robotics: From Learning-Based Control to Safe Reinforcement Learning. Annu.
    Rev. Control Robot. Auton. Syst. 5(1):411–444_eprint: https://doi.org/10.1146/annurev-control-042920-020211'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Kober J, Bagnell JA, Peters J. 2013. Reinforcement learning in robotics:
    A survey. Int. J. Robot. Res. 32(11):1238–1274'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Sünderhauf N, Brock O, Scheirer W, Hadsell R, Fox D, et al. 2018. The
    limits and potentials of deep learning for robotics. Int. J. Robot. Res. 37(4-5):405–420'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Mason MT. 2001. Mechanics of robotic manipulation. MIT press'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Siciliano B, Khatib O, Kröger T. 2008. Springer handbook of robotics,
    vol. 200. Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Mason MT. 2018. Toward robotic manipulation. Annu. Rev. Control Robot.
    Auton. Syst. 1:1–28'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Rudin N, Hoeller D, Bjelonic M, Hutter M. 2022. Advanced skills by learning
    locomotion and local navigation end-to-end. In IEEE/RSJ Int. Conf. Intell. Robots
    Syst., pp. 2497–2503\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Song Y, Romero A, Müller M, Koltun V, Scaramuzza D. 2023. Reaching the
    limit in autonomous racing: Optimal control versus reinforcement learning. Sci.
    Robot. 8(82):eadg1462'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] On-Road Automated Driving (ORAD) committee. 2018. Taxonomy and definitions
    for terms related to driving automation systems for on-road motor vehicles. Tech.
    rep., SAE International'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Lavin A, Gilligan-Lee CM, Visnjic A, Ganju S, Newman D, et al. 2022. Technology
    readiness levels for machine learning systems. Nat. Commun. 13(1):6039'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Kohl N, Stone P. 2004. Policy gradient reinforcement learning for fast
    quadrupedal locomotion. In IEEE Int. Conf. Robot. Autom., vol. 3, pp. 2619–2624\.
    IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Bagnell JA, Schneider JG. 2001. Autonomous helicopter control using reinforcement
    learning policy search methods. In IEEE Int. Conf. Robot. Autom., vol. 2, pp.
    1615–1620\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Abbeel P, Coates A, Quigley M, Ng A. 2006. An application of reinforcement
    learning to aerobatic helicopter flight. In Adv. Neural Inf. Process. Syst., vol. 19'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Kumar A, Li Z, Zeng J, Pathak D, Sreenath K, Malik J. 2022. Adapting rapid
    motor adaptation for bipedal robots. In IEEE/RSJ Int. Conf. Intell. Robots Syst.,
    pp. 1161–1168\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Tan J, Zhang T, Coumans E, Iscen A, Bai Y, et al. 2018. Sim-to-real: Learning
    agile locomotion for quadruped robots. In Robot. Sci. Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Hwangbo J, Lee J, Dosovitskiy A, Bellicoso D, Tsounis V, et al. 2019.
    Learning agile and dynamic motor skills for legged robots. Sci. Robot. 4(26):eaau5872'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Feng G, Zhang H, Li Z, Peng XB, Basireddy B, et al. 2023. Genloco: Generalized
    locomotion controllers for quadrupedal robots. In Conference on Robot Learning,
    pp. 1893–1903\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Lee J, Hwangbo J, Hutter M. 2019. Robust recovery controller for a quadrupedal
    robot using deep reinforcement learning. arXiv preprint arXiv:1901.07517'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Yang C, Yuan K, Zhu Q, Yu W, Li Z. 2020. Multi-expert learning of adaptive
    legged locomotion. Sci. Robot. 5(49):eabb2174'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Kumar A, Fu Z, Pathak D, Malik J. 2021. RMA: Rapid motor adaptation for
    legged robots. In Robot. Sci. Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Lee J, Hwangbo J, Wellhausen L, Koltun V, Hutter M. 2020. Learning quadrupedal
    locomotion over challenging terrain. Sci. Robot. 5(47):eabc5986'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Miki T, Lee J, Hwangbo J, Wellhausen L, Koltun V, Hutter M. 2022. Learning
    robust perceptive locomotion for quadrupedal robots in the wild. Sci. Robot. 7(62):eabk2822'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Gangapurwala S, Geisert M, Orsolino R, Fallon M, Havoutis I. 2022. RLOC:
    Terrain-aware legged locomotion using reinforcement learning and optimal control.
    IEEE Trans. Robot. 38(5):2908–2927'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Choi S, Ji G, Park J, Kim H, Mun J, et al. 2023. Learning quadrupedal
    locomotion on deformable terrain. Sci. Robot. 8(74):eade2256'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Nahrendra IMA, Yu B, Myung H. 2023. DreamWaQ: Learning robust quadrupedal
    locomotion with implicit terrain imagination via deep reinforcement learning.
    In IEEE Int. Conf. Robot. Autom., pp. 5078–5084\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Pinto L, Andrychowicz M, Welinder P, Zaremba W, Abbeel P. 2018. Asymmetric
    actor critic for image-based robot learning. In Robot. Sci. Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Escontrela A, Peng XB, Yu W, Zhang T, Iscen A, et al. 2022. Adversarial
    motion priors make good substitutes for complex reward functions. In IEEE/RSJ
    Int. Conf. Intell. Robots Syst., pp. 25–32\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Ma Y, Farshidian F, Hutter M. 2023. Learning arm-assisted fall damage
    reduction and recovery for legged mobile manipulators. In IEEE Int. Conf. Robot.
    Autom., pp. 12149–12155\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Fu Z, Kumar A, Malik J, Pathak D. 2022. Minimizing energy consumption
    leads to the emergence of gaits in legged robots. In Conf. Robot Learn., pp. 928–937\.
    PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Loquercio A, Kumar A, Malik J. 2023. Learning visual locomotion with cross-modal
    supervision. In IEEE Int. Conf. Robot. Autom., pp. 7295–7302\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Agarwal A, Kumar A, Malik J, Pathak D. 2023. Legged locomotion in challenging
    terrains using egocentric vision. In Conf. Robot Learn., pp. 403–415\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Yang R, Yang G, Wang X. 2023. Neural volumetric memory for visual locomotion
    control. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 1430–1440\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Jenelten F, He J, Farshidian F, Hutter M. 2024. DTC: Deep tracking control.
    Sci. Robot. 9(86):eadh5401'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Yang Y, Shi G, Meng X, Yu W, Zhang T, et al. 2023a. CAJun: Continuous
    adaptive jumping using a learned centroidal controller. In Conf. Robot. Learn.
    PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Smith L, Kew JC, Peng XB, Ha S, Tan J, Levine S. 2022. Legged robots that
    keep on learning: Fine-tuning locomotion policies in the real world. In IEEE Int.
    Conf. Robot. Autom., pp. 1593–1599\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Cheng X, Shi K, Agarwal A, Pathak D. 2024. Extreme parkour with legged
    robots. In IEEE Int. Conf. Robot. Autom. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Zhuang Z, Fu Z, Wang J, Atkeson CG, Schwertfeger S, et al. 2023. Robot
    parkour learning. In Conf. Robot. Learn. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Vollenweider E, Bjelonic M, Klemm V, Rudin N, Lee J, Hutter M. 2023. Advanced
    skills through multiple adversarial motion priors in reinforcement learning. In
    IEEE Int. Conf. Robot. Autom., pp. 5120–5126\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Margolis GB, Agrawal P. 2023. Walk these ways: Tuning robot control for
    generalization with multiplicity of behavior. In Conf. Robot. Learn., pp. 22–31\.
    PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Smith L, Kostrikov I, Levine S. 2023. Demonstrating a walk in the park:
    Learning to walk in 20 minutes with model-free reinforcement learning. Robot.
    Sci. Syst. 2(3):4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Wu P, Escontrela A, Hafner D, Abbeel P, Goldberg K. 2023. Daydreamer:
    World models for physical robot learning. In Conf. Robot Learn., pp. 2226–2240\.
    PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Siekmann J, Valluri S, Dao J, Bermillo L, Duan H, et al. 2020. Learning
    memory-based control for human-scale bipedal locomotion. In Robot. Sci. Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Hanna JP, Desai S, Karnan H, Warnell G, Stone P. 2021. Grounded action
    transformation for sim-to-real reinforcement learning. Mach. Learn. 110(9):2469–2499'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Siekmann J, Godse Y, Fern A, Hurst J. 2021a. Sim-to-real learning of all
    common bipedal gaits via periodic reward composition. In IEEE Int. Conf. Robot.
    Autom., pp. 7309–7315\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Li Z, Cheng X, Peng XB, Abbeel P, Levine S, et al. 2021. Reinforcement
    learning for robust parameterized locomotion control of bipedal robots. In IEEE
    Int. Conf. Robot. Autom., pp. 2811–2817\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Siekmann J, Green K, Warila J, Fern A, Hurst J. 2021b. Blind Bipedal Stair
    Traversal via Sim-to-Real Reinforcement Learning. In Robot. Sci. Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Castillo GA, Weng B, Zhang W, Hereid A. 2022. Reinforcement learning-based
    cascade motion policy design for robust 3d bipedal locomotion. IEEE Access 10:20135–20148'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Duan H, Pandit B, Gadde MS, van Marum BJ, Dao J, et al. 2024. Learning
    vision-based bipedal locomotion for challenging terrain. In IEEE Int. Conf. Robot.
    Autom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Radosavovic I, Xiao T, Zhang B, Darrell T, Malik J, Sreenath K. 2024.
    Real-world humanoid locomotion with reinforcement learning. Sci. Robot. 9(89):eadi9579'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Li Z, Peng XB, Abbeel P, Levine S, Berseth G, Sreenath K. 2024a. Reinforcement
    learning for versatile, dynamic, and robust bipedal locomotion control. arXiv
    preprint arXiv:2401.16889'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Hwangbo J, Sa I, Siegwart R, Hutter M. 2017. Control of a quadrotor with
    reinforcement learning. IEEE Robot. Autom. Lett. 2(4):2096–2103'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Molchanov A, Chen T, Hönig W, Preiss JA, Ayanian N, Sukhatme GS. 2019.
    Sim-to-(multi)-real: Transfer of low-level robust control policies to multiple
    quadrotors. In IEEE/RSJ Int. Conf. Intell. Robots Syst., pp. 59–66\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Kaufmann E, Bauersfeld L, Scaramuzza D. 2022. A benchmark comparison of
    learned control policies for agile quadrotor flight. In Int. Conf. Robot. Autom.,
    pp. 10504–10510\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Zhang D, Loquercio A, Wu X, Kumar A, Malik J, Mueller MW. 2023. Learning
    a single near-hover position controller for vastly different quadcopters. In IEEE
    Int. Conf. Robot. Autom., pp. 1263–1269\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Eschmann J, Albani D, Loianno G. 2024. Learning to fly in seconds. IEEE
    Robot. Autom. Lett. 9(7):6336–6343'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Yang R, Zhang M, Hansen N, Xu H, Wang X. 2021. Learning vision-Guided
    quadrupedal locomotion end-to-end with cross-modal transformers. In Int. Conf.
    Learn. Represent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O. 2017. Proximal
    policy optimization algorithms. arXiv preprint arXiv:1707.06347'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Grizzle JW, Hurst J, Morris B, Park HW, Sreenath K. 2009. MABEL, a new
    robotic bipedal walker and runner. In Am. Control Conf., pp. 2030–2036\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Loquercio A, Kaufmann E, Ranftl R, Müller M, Koltun V, Scaramuzza D. 2021.
    Learning high-speed flight in the wild. Sci. Robot. 6(59):eabg5810'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Tai L, Paolo G, Liu M. 2017. Virtual-to-real deep reinforcement learning:
    Continuous control of mobile robots for mapless navigation. In IEEE/RSJ Int. Conf.
    Intell. Robots Syst., pp. 31–36'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Xu Z, Liu B, Xiao X, Nair A, Stone P. 2023. Benchmarking Reinforcement
    Learning Techniques for Autonomous Navigation. In IEEE Int. Conf. Robot. Autom.,
    pp. 9224–9230'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Chiang HTL, Faust A, Fiser M, Francis A. 2019. Learning navigation behaviors
    end-to-end with autorl. IEEE Robot. Autom. Lett. 4(2):2007–2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Stein GJ, Bradley C, Roy N. 2018. Learning over subgoals for efficient
    navigation of structured, unknown environments. In Conf. Robot Learn., pp. 213–222\.
    PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Anderson P, Wu Q, Teney D, Bruce J, Johnson M, et al. 2018. Vision-and-language
    navigation: Interpreting visually-grounded navigation instructions in real environments.
    In IEEE Conf. Comput. Vis. Pattern Recognit., pp. 3674–3683'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Zhu Y, Mottaghi R, Kolve E, Lim JJ, Gupta A, et al. 2017. Target-driven
    visual navigation in indoor scenes using deep reinforcement learning. In IEEE
    Int. Conf. Robot. Autom., pp. 3357–3364'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Kahn G, Villaflor A, Ding B, Abbeel P, Levine S. 2018. Self-Supervised
    Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation.
    In IEEE Int. Conf. Robot. Autom., pp. 5129–5136\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Wijmans E, Kadian A, Morcos A, Lee S, Essa I, et al. 2020a. DD-PPO: Learning
    Near-Perfect PointGoal Navigators from 2.5 Billion Frames. ArXiv:1911.00357 [cs]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Chaplot DS, Gandhi DP, Gupta A, Salakhutdinov RR. 2020. Object goal navigation
    using goal-oriented semantic exploration. Adv. Neural Inf. Process. Syst. 33:4247–4258'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Gervet T, Chintala S, Batra D, Malik J, Chaplot DS. 2023. Navigating to
    objects in the real world. Sci. Robot. 8(79)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Hoeller D, Wellhausen L, Farshidian F, Hutter M. 2021. Learning a State
    Representation and Navigation in Cluttered and Dynamic Environments. IEEE Robot.
    Autom. Lett. 6(3):5081–88'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Savva M, Kadian A, Maksymets O, Zhao Y, Wijmans E, et al. 2019. Habitat:
    A platform for embodied ai research. In IEEE/CVF Int. Conf. Comput. Vis., pp.
    9339–9347'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Kadian A, Truong J, Gokaslan A, Clegg A, Wijmans E, et al. 2020. Sim2real
    predictivity: Does evaluation in simulation predict real-world performance? IEEE
    Robot. Autom. Lett. 5(4):6670–6677'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Truong J, Rudolph M, Yokoyama NH, Chernova S, Batra D, Rai A. 2023. Rethinking
    sim2real: Lower fidelity simulation leads to higher sim2real transfer in navigation.
    In Conf. Robot Learn., pp. 859–70\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Truong J, Zitkovich A, Chernova S, Batra D, Zhang T, et al. 2024. Indoorsim-to-outdoorreal:
    learning to navigate outdoors without any outdoor experience. IEEE Robot. Autom.
    Lett.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Kahn G, Abbeel P, Levine S. 2021. BADGR: An Autonomous Self-Supervised
    Learning-Based Navigation System. IEEE Robot. Autom. Lett. 6(2):1312–1319'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Shah D, Bhorkar A, Leen H, Kostrikov I, Rhinehart N, Levine S. 2023. Offline
    Reinforcement Learning for Visual Navigation. In Conf. Robot Learn., pp. 44–54\.
    PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Williams G, Wagener N, Goldfain B, Drews P, Rehg JM, et al. 2017. Information
    theoretic mpc for model-based reinforcement learning. In IEEE Int. Conf. Robot.
    Autom., pp. 1714–1721'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Stachowicz K, Shah D, Bhorkar A, Kostrikov I, Levine S. 2023. FastRLAP:
    A System for Learning High-Speed Driving via Deep RL and Autonomous Practicing.
    In Conf. Robot Learn., pp. 3100–3111\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Kendall A, Hawke J, Janz D, Mazur P, Reda D, et al. 2019. Learning to
    drive in a day. In Int. Conf. Robot. Autom., pp. 8248–8254\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Jang K, Lichtlé N, Vinitsky E, Shah A, Bunting M, et al. 2024. Reinforcement
    learning based oscillation dampening: Scaling up single-agent rl algorithms to
    a 100 av highway field operational test. arXiv preprint arXiv:2402.17050'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Sorokin M, Tan J, Liu CK, Ha S. 2022. Learning to navigate sidewalks in
    outdoor environments. IEEE Robot. Autom. Lett. 7(2):3906–13'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Zhang C, Jin J, Frey J, Rudin N, Mattamala Aravena ME, et al. 2024. Resilient
    legged local navigation: Learning to traverse with compromised perception end-to-end.
    In IEEE Int. Conf. Robot. Autom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Hoeller D, Rudin N, Sako D, Hutter M. 2024. Anymal parkour: Learning agile
    navigation for quadrupedal robots. Sci. Robot. 9(88):eadi7566'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Lee J, Bjelonic M, Reske A, Wellhausen L, Miki T, Hutter M. 2024. Learning
    robust autonomous navigation and locomotion for wheeled-legged robots. Sci. Robot.
    9(89):eadi9641'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Miki T, Lee J, Wellhausen L, Hutter M. 2024. Learning to walk in confined
    spaces using 3d representation. In Int. Conf. Robot. Autom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Xu Z, Raj AH, Xiao X, Stone P. 2024. Dexterous Legged Locomotion in Confined
    3D Spaces with Reinforcement Learning. In Int. Conf. Robot. Autom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] He T, Zhang C, Xiao W, He G, Liu C, Shi G. 2024. Agile but safe: Learning
    collision-free high-speed legged locomotion. Robot.: Sci. Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Sadeghi F, Levine S. 2017. Cad2rl: Real single-image flight without a
    single real image. Robot.: Sci. Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Kang K, Belkhale S, Kahn G, Abbeel P, Levine S. 2019. Generalization
    through simulation: Integrating simulated and real data into deep reinforcement
    learning for vision-based autonomous flight. In Int. Conf. Robot. Autom., pp.
    6008–6014\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Romero A, Song Y, Scaramuzza D. 2024. Actor-critic model predictive control.
    In Int. Conf. Robot. Autom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Xie L, Wang S, Markham A, Trigoni N. 2017. Towards monocular vision based
    obstacle avoidance through deep reinforcement learning. arXiv preprint arXiv:1706.09829'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Wijmans E, Kadian A, Morcos A, Lee S, Essa I, et al. 2020b. DD-PPO: Learning
    Near-Perfect PointGoal Navigators from 2.5 Billion Frames. In Int. Conf. Learn.
    Represent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Wijmans E, Savva M, Essa I, Lee S, Morcos AS, Batra D. 2023. Emergence
    of Maps in the Memories of Blind Navigation Agents. In 11th Int. Conf. Learn.
    Represent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Rosinol A, Leonard JJ, Carlone L. 2023. Nerf-slam: Real-time dense monocular
    slam with neural radiance fields. In 2023 IEEE/RSJ Int. Conf. Intell. Robots Syst.,
    pp. 3437–3444'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Mahler J, Matl M, Satish V, Danielczuk M, DeRose B, et al. 2019. Learning
    ambidextrous robot grasping policies. Sci. Robot. 4(26):eaau4984'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Zeng A, Song S, Welker S, Lee J, Rodriguez A, Funkhouser T. 2018. Learning
    synergies between pushing and grasping with self-supervised deep reinforcement
    learning. In IEEE/RSJ Int. Conf. Intell. Robots Syst., pp. 4238–4245\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Kalashnikov D, Irpan A, Pastor P, Ibarz J, Herzog A, et al. 2018. Scalable
    deep reinforcement learning for vision-based robotic manipulation. In Conf. Robot.
    Learn., pp. 651–73'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] James S, Wohlhart P, Kalakrishnan M, Kalashnikov D, Irpan A, et al. 2019.
    Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical
    adaptation networks. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp.
    12627–12637'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Wang D, Jia M, Zhu X, Walters R, Platt R. 2023a. On-Robot Learning With
    Equivariant Models. In Conf. on Robot Learn., pp. 1345–1354\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Levine S, Finn C, Darrell T, Abbeel P. 2016. End-to-end training of deep
    visuomotor policies. J. Mach. Learn. Res. 17(1):1334–1373'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Kalashnikov D, Varley J, Chebotar Y, Swanson B, Jonschkowski R, et al.
    2022. Scaling up multi-task robotic reinforcement learning. In Conf. Robot Learn.,
    pp. 557–575\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Chebotar Y, Hausman K, Lu Y, Xiao T, Kalashnikov D, et al. 2021. Actionable
    Models: Unsupervised Offline Reinforcement Learning of Robotic Skills. In Int.
    Conf. Mach. Learn., pp. 1518–1528\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Lee AX, Devin CM, Zhou Y, Lampe T, Bousmalis K, et al. 2021. Beyond pick-and-place:
    Tackling robotic stacking of diverse shapes. In Conf. on Robot Learn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Walke HR, Yang JH, Yu A, Kumar A, Orbik J, et al. 2023. Don’t start from
    scratch: Leveraging prior data to automate robotic reinforcement learning. In
    Conf. Robot Learn., pp. 1652–1662'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Ebert F, Finn C, Dasari S, Xie A, Lee A, Levine S. 2018. Visual foresight:
    Model-based deep reinforcement learning for vision-based robotic control. arXiv
    preprint arXiv:1812.00568'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Riedmiller M, Hafner R, Lampe T, Neunert M, Degrave J, et al. 2018. Learning
    by Playing Solving Sparse Reward Tasks from Scratch. In Int. Conf. Mach. Learn.,
    pp. 4344–4353\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Zhu H, Yu J, Gupta A, Shah D, Hartikainen K, et al. 2020. The Ingredients
    of Real World Robotic Reinforcement Learning. In Int. Conf. Learn. Represent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Ma YJ, Sodhani S, Jayaraman D, Bastani O, Kumar V, Zhang A. 2022a. VIP:
    Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training.
    In Int. Conf. Learn. Represent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Nair A, Gupta A, Dalal M, Levine S. 2020. Awac: Accelerating online reinforcement
    learning with offline datasets. arXiv preprint arXiv:2006.09359'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Nasiriany S, Liu H, Zhu Y. 2022. Augmenting reinforcement learning with
    behavior primitives for diverse manipulation tasks. In Int. Conf. Robot. Autom.,
    pp. 7477–7484\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Chebotar Y, Vuong Q, Hausman K, Xia F, Lu Y, et al. 2023. Q-Transformer:
    Scalable offline reinforcement learning via autoregressive Q-functions. In Conf.
    on Robot Learn., pp. 3909–3928\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Nair AV, Pong V, Dalal M, Bahl S, Lin S, Levine S. 2018. Visual reinforcement
    learning with imagined goals. In Adv. Neural Inf. Process. Syst., vol. 31'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Johannink T, Bahl S, Nair A, Luo J, Kumar A, et al. 2019. Residual reinforcement
    learning for robot control. In Int. Conf. Robot Autom., pp. 6023–6029'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Vecerik M, Hester T, Scholz J, Wang F, Pietquin O, et al. 2017. Leveraging
    demonstrations for deep reinforcement learning on robotics problems with sparse
    rewards. arXiv preprint arXiv:1707.08817'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Luo J, Sushkov O, Pevceviciute R, Lian W, Su C, et al. 2021. Robust multi-modal
    policies for industrial assembly via reinforcement learning and demonstrations:
    A large-scale study. In Robot.: Sci. Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Zhao TZ, Luo J, Sushkov O, Pevceviciute R, Heess N, et al. 2022. Offline
    meta-reinforcement learning for industrial insertion. In IEEE Int. Conf. Robot.
    Autom., pp. 6386–6393'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Tang B, Lin MA, Akinola I, Handa A, Sukhatme GS, et al. 2023. IndustReal:
    Transferring contact-rich assembly tasks from simulation to reality. In Robot.:
    Sci. and Sys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Chebotar Y, Handa A, Makoviychuk V, Macklin M, Issac J, et al. 2019.
    Closing the sim-to-real loop: Adapting simulation randomization with real world
    experience. In IEEE Int. Conf. Robot. Autom., pp. 8973–8979\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Abbatematteo B, Rosen E, Thompson S, Akbulut T, Rammohan S, Konidaris
    G. 2024. Composable interaction primitives: A structured policy class for efficiently
    learning sustained-contact manipulation skills. In IEEE Int. Conf. Robot. Autom.
    IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Wu R, Zhao Y, Mo K, Guo Z, Wang Y, et al. 2022. VAT-Mart: Learning visual
    action trajectory proposals for manipulating 3D articulated objects. In Int. Conf.
    Learn. Represent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Matas J, James S, Davison AJ. 2018. Sim-to-real reinforcement learning
    for deformable object manipulation. In Conf. on Robot Learn., pp. 734–743'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Wu Y, Yan W, Kurutach T, Pinto L, Abbeel P. 2020. Learning to manipulate
    deformable objects without demonstrations. In Robot: Sci. Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Avigal Y, Berscheid L, Asfour T, Kröger T, Goldberg K. 2022. Speedfolding:
    Learning efficient bimanual folding of garments. In IEEE/RSJ Int. Conf. Intell.
    Robots Syst., pp. 1–8\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Wang Y, Sun Z, Erickson Z, Held D. 2023b. One policy to dress them all:
    Learning to dress people with diverse poses and garments. In Robot.: Sci. and
    Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Andrychowicz OM, Baker B, Chociej M, Jozefowicz R, McGrew B, et al. 2020.
    Learning dexterous in-hand manipulation. Int. J. Robot. Res. 39(1):3–20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Handa A, Allshire A, Makoviychuk V, Petrenko A, Singh R, et al. 2023.
    Dextreme: Transfer of agile in-hand manipulation from simulation to reality. In
    IEEE Int. Conf. Robot. and Autom., pp. 5977–5984'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Nagabandi A, Konolige K, Levine S, Kumar V. 2020. Deep Dynamics Models
    for Learning Dexterous Manipulation. In Proc. Conf. Robot. Learn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Qi H, Yi B, Suresh S, Lambeta M, Ma Y, et al. 2023. General in-hand object
    rotation with vision and touch. In Conf. on Robot Learn., pp. 2549–2564\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Chen T, Tippur M, Wu S, Kumar V, Adelson E, Agrawal P. 2023. Visual dexterity:
    In-hand reorientation of novel and complex object shapes. Sci. Robot. 8(84):eadc9244'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Zhou W, Held D. 2023. Learning to grasp the ungraspable with emergent
    extrinsic dexterity. In Conf. Robot Learn., pp. 150–160\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Zhou W, Jiang B, Yang F, Paxton C, Held D. 2023. HACMan: Learning hybrid
    actor-critic maps for 6D non-prehensile manipulation. In Conf. Robot Learn. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Cho Y, Han J, Cho Y, Kim B. 2024. CORN: Contact-based object representation
    for nonprehensile manipulation of general unseen objects. In Int. Conf. on Learn.
    Represent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Lv J, Feng Y, Zhang C, Zhao S, Shao L, Lu C. 2023. SAM-RL: Sensing-Aware
    Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation
    and Rendering. In Robot.: Sci. Sys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Van Wyk K, Handa A, Makoviychuk V, Guo Y, Allshire A, Ratliff ND. 2024.
    Geometric fabrics: a safe guiding medium for policy learning. arXiv preprint arXiv:2405.02250'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Chitnis R, Tulsiani S, Gupta S, Gupta A. 2020. Efficient bimanual manipulation
    using learned task schemas. In IEEE Int. Conf. Robot. Autom., pp. 1149–1155'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Büchler D, Guist S, Calandra R, Berenz V, Schölkopf B, Peters J. 2022.
    Learning to play table tennis from scratch using muscular robots. IEEE Trans.
    Robot. 38(6):3850–3860'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Cheng S, Xu D. 2023. League: Guided skill learning and abstraction for
    long-horizon manipulation. IEEE Robot. Autom. Lett.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Funk N, Chalvatzaki G, Belousov B, Peters J. 2022. Learn2assemble with
    structured representations and search for robotic architectural construction.
    In Conf. Robot. Learn., pp. 1401–1411'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Wang C, Zhang Q, Tian Q, Li S, Wang X, et al. 2020. Learning mobile manipulation
    through deep reinforcement learning. Sensors 20(3):939'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Ma Y, Farshidian F, Miki T, Lee J, Hutter M. 2022b. Combining learning-based
    locomotion policy with model-based manipulation for legged mobile manipulators.
    IEEE Robot. Autom. Lett. 7(2):2377–2384'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Fu Z, Cheng X, Pathak D. 2023. Deep whole-body control: learning a unified
    policy for manipulation and locomotion. In Conf. on Robot Learn., pp. 138–149\.
    PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Fu Z, Zhao Q, Wu Q, Wetzstein G, Finn C. 2024. Humanplus: Humanoid shadowing
    and imitation from humans. arXiv preprint arXiv:2406.10454'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Sentis L, Khatib O. 2006. A whole-body control framework for humanoids
    operating in human environments. In IEEE Int. Conf. Robot. Autom., pp. 2641–2648\.
    IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Yokoyama N, Clegg AW, Undersander E, Ha S, Batra D, Rai A. 2023. Adaptive
    skill coordination for robotic mobile manipulation. IEEE Robot. Autom. Lett.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Ji Y, Li Z, Sun Y, Peng XB, Levine S, et al. 2022. Hierarchical reinforcement
    learning for precise soccer shooting skills using a quadrupedal robot. In IEEE/RSJ
    Int. Conf. Intell. Robots Syst., pp. 1479–1486\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Liu M, Chen Z, Cheng X, Ji Y, Yang R, Wang X. 2024. Visual whole-body
    control for legged loco-manipulation. arXiv preprint arXiv:2403.16967'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Sun C, Orbik J, Devin CM, Yang BH, Gupta A, et al. 2022. Fully autonomous
    real-world reinforcement learning with applications to mobile manipulation. In
    Conf. on Robot Learn., pp. 308–319\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Jauhri S, Peters J, Chalvatzaki G. 2022. Robot learning of mobile manipulation
    with reachability behavior priors. IEEE Robot. Autom. Lett. 7(3):8399–8406'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Ji Y, Margolis GB, Agrawal P. 2023. Dribblebot: Dynamic legged manipulation
    in the wild. In IEEE Int. Conf. Robot. Autom., pp. 5155–5162\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Hu J, Stone P, Martín-Martín R. 2023. Causal Policy Gradient for Whole-Body
    Mobile Manipulation. In Robot.: Sci. and Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Honerkamp D, Welschehold T, Valada A. 2023. N²M²: Learning navigation
    for arbitrary mobile manipulation motions in unseen and dynamic environments.
    IEEE Trans. Robot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Uppal S, Agarwal A, Xiong H, Shaw K, Pathak D. 2024. SPIN: Simultaneous
    perception interaction and navigation. In IEEE/CVF Conf. Comput. Vis. Pattern
    Recognit., pp. 18133–18142'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Kumar KN, Essa I, Ha S. 2023. Cascaded compositional residual learning
    for complex interactive behaviors. IEEE Robot. Autom. Lett. 8(8):4601–4608'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Yang R, Kim Y, Kembhavi A, Wang X, Ehsani K. 2023b. Harmonic mobile manipulation.
    arXiv preprint arXiv:2312.06639'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Xiong H, Mendonca R, Shaw K, Pathak D. 2024. Adaptive mobile manipulation
    for articulated objects in the open world. arXiv preprint arXiv:2401.14403'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Cheng X, Kumar A, Pathak D. 2023. Legs as Manipulator: Pushing Quadrupedal
    Agility Beyond Locomotion. In IEEE Int. Conf. Robot. and Autom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Herzog A, Rao K, Hausman K, Lu Y, Wohlhart P, et al. 2023. Deep rl at
    scale: Sorting waste in office buildings with a fleet of mobile manipulators.
    In Robot.: Sci. and Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Wu B, Martin-Martin R, Fei-Fei L. 2023. M-EMBER: Tackling Long-Horizon
    Mobile Manipulation via Factorized Domain Transfer. In IEEE Int. Conf. Robot.
    Autom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Ghadirzadeh A, Chen X, Yin W, Yi Z, Björkman M, Kragic D. 2020. Human-centered
    collaborative robots with deep reinforcement learning. IEEE Robot. Autom. Lett.
    6(2):566–571'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Christen S, Feng L, Yang W, Chao YW, Hilliges O, Song J. 2024. Synh2r:
    Synthesizing hand-object motions for learning human-to-robot handovers. Int. Conf.
    Robot. Autom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Christen S, Yang W, Pérez-D’Arpino C, Hilliges O, Fox D, Chao YW. 2023.
    Learning human-to-robot handovers from point clouds. In IEEE/CVF Conf. Comput.
    Vis. Pattern Recognit., pp. 9654–9664'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Chen YF, Everett M, Liu M, How JP. 2017a. Socially aware motion planning
    with deep reinforcement learning. In IEEE/RSJ Int. Conf. Intell. Robots Syst.,
    pp. 1343–1350\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Everett M, Chen YF, How JP. 2021. Collision avoidance in pedestrian-rich
    environments with deep reinforcement learning. IEEE Access 9:10357–10377'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Liang J, Patel U, Sathyamoorthy AJ, Manocha D. 2021. Crowd-steer: Realtime
    smooth and collision-free robot navigation in densely crowded scenarios trained
    using high-fidelity simulation. In Int. Conf. Int. Joint Conf. Artif. Intell.,
    pp. 4221–4228'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Hirose N, Shah D, Stachowicz K, Sridhar A, Levine S. 2024. Selfi: Autonomous
    self-improvement with reinforcement learning for social navigation. arXiv preprint
    arXiv:2403.00991'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Liu P, Zhang K, Tateo D, Jauhri S, Hu Z, et al. 2023. Safe reinforcement
    learning of dynamic high-dimensional robotic tasks: navigation, manipulation,
    interaction. In IEEE Int. Conf. Robot. Autom., pp. 9449–9456\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Dimeas F, Aspragathos N. 2015. Reinforcement learning of variable admittance
    control for human-robot co-manipulation. In IEEE/RSJ Int. Conf. Intell. Robots
    Syst., pp. 1011–1016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Nair S, Mitchell E, Chen K, Savarese S, Finn C, et al. 2022. Learning
    language-conditioned robot behavior from offline data and crowd-sourced annotation.
    In Conf. Robot Learn., pp. 1303–1315\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Reddy S, Dragan AD, Levine S. 2018. Shared autonomy via deep reinforcement
    learning. In Robot.: Sci. and Sys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Schaff C, Walter MR. 2020. Residual policy learning for shared autonomy.
    In Robot. Sci. Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Chen YF, Liu M, Everett M, How JP. 2017b. Decentralized non-communicating
    multiagent collision avoidance with deep reinforcement learning. In IEEE Int.
    Conf. Robot. Autom., pp. 285–292\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Everett M, Chen YF, How JP. 2018. Motion planning among dynamic, decision-making
    agents with deep reinforcement learning. In IEEE/RSJ Int. Conf. Intell. Robots
    Syst., pp. 3052–3059'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Van Den Berg J, Guy SJ, Lin M, Manocha D. 2011. Reciprocal n-body collision
    avoidance. In Int. Symp. Robot. Res., pp. 3–19\. Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Fan T, Long P, Liu W, Pan J. 2020. Distributed multi-robot collision
    avoidance via deep reinforcement learning for navigation in complex scenarios.
    Int. J. Robot. Res. 39(7):856–892'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Han R, Chen S, Wang S, Zhang Z, Gao R, et al. 2022. Reinforcement learned
    distributed multi-robot navigation with reciprocal velocity obstacle shaped rewards.
    IEEE Robot. Autom. Lett. 7(3):5896–5903'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Sartoretti G, Kerr J, Shi Y, Wagner G, Kumar TS, et al. 2019. Primal:
    Pathfinding via reinforcement and imitation multi-agent learning. IEEE Robot.
    and Autom. Lett. 4(3):2378–2385'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Nachum O, Ahn M, Ponte H, Gu S, Kumar V. 2020. Multi-agent manipulation
    via locomotion using hierarchical sim2real. In Conf. Robot Learn., vol. 100, pp.
    110–121\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Haarnoja T, Moran B, Lever G, Huang SH, Tirumala D, et al. 2024. Learning
    agile soccer skills for a bipedal robot with deep reinforcement learning. Sci.
    Robot. 9(89):eadi8022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Uchendu I, Xiao T, Lu Y, Zhu B, Yan M, et al. 2023. Jump-start reinforcement
    learning. In Int. Conf. Mach. Learn., pp. 34556–34583\. PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Li C, Tang C, Nishimura H, Mercat J, Tomizuka M, Zhan W. 2023. Residual
    Q-learning: offline and online policy customization without value. In Adv. Neural
    Inf. Process. Syst., vol. 36, pp. 61857–61869'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Hansen N, Su H, Wang X. 2023. TD-MPC2: Scalable, robust world models
    for continuous control. In Conf. Learn. Represent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Jeong GC, Bahety A, Pedraza G, Deshpande AD, Martín-Martín R. 2023. Bariflex:
    A robotic gripper with versatility and collision robustness for robot learning.
    arXiv preprint arXiv:2312.05323'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Eysenbach B, Gupta A, Ibarz J, Levine S. 2019. Diversity is all you need:
    Learning skills without a reward function. In Int. Conf. Learn. Represent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Schwarke C, Klemm V, Van der Boon M, Bjelonic M, Hutter M. 2023. Curiosity-driven
    learning of joint locomotion and manipulation tasks. In Conf. Robot Learn., vol.
    229, pp. 2594–2610'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Xie Z, Da X, Van de Panne M, Babich B, Garg A. 2021. Dynamics randomization
    revisited: A case study for quadrupedal locomotion. In IEEE Int. Conf. Robot.
    Autom., pp. 4955–4961'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Martín-Martín R, Lee MA, Gardner R, Savarese S, Bohg J, Garg A. 2019.
    Variable impedance control in end-effector space: An action space for reinforcement
    learning in contact-rich tasks. In IEEE/RSJ Int. Conf. Intell. Robots Syst., pp.
    1010–1017\. IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Xia F, Li C, Martín-Martín R, Litany O, Toshev A, Savarese S. 2021. ReLMoGen:
    Integrating motion generation in reinforcement learning for mobile manipulation.
    In IEEE Conf. Robot. Autom., pp. 4583–4590'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Luo J, Xu C, Liu F, Tan L, Lin Z, et al. 2024. Fmb: A functional manipulation
    benchmark for generalizable robotic learning. Int. J. Robot. Res.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Heo M, Lee Y, Lee D, Lim JJ. 2023. FurnitureBench: Reproducible Real-World
    Benchmark for Long-Horizon Complex Manipulation. In Robot. Sci. Syst.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] van der Zant T, Iocchi L. 2011. Robocup@ home: Adaptive benchmarking
    of robot bodies and minds. In Int. Conf. Soc. Robot., pp. 214–225\. Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Li X, Hsu K, Gu J, Pertsch K, Mees O, et al. 2024b. Evaluating real-world
    robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Padalkar A, Pooley A, Jain A, Bewley A, Herzog A, et al. 2023. Open x-embodiment:
    Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Khazatsky A, Pertsch K, Nair S, Balakrishna A, Dasari S, et al. 2024.
    Droid: A large-scale in-the-wild robot manipulation dataset. In Robot.: Sci. and
    Sys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Firoozi R, Tucker J, Tian S, Majumdar A, Sun J, et al. 2023. Foundation
    models in robotics: Applications, challenges, and the future. arXiv preprint arXiv:2312.07843'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Hu Y, Xie Q, Jain V, Francis J, Patrikar J, et al. 2023. Toward general-purpose
    robots via foundation models: A survey and meta-analysis. arXiv preprint arXiv:2312.08782'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Yang S, Nachum O, Du Y, Wei J, Abbeel P, Schuurmans D. 2023c. Foundation
    models for decision making: Problems, methods, and opportunities. arXiv preprint
    arXiv:2303.04129'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Ma YJ, Liang W, Wang HJ, Wang S, Zhu Y, et al. 2024. DrEureka: Language
    Model Guided Sim-To-Real Transfer. In Robot.: Sci. Sys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Term Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As presented in Sec. [3](#S3 "3 Taxonomy ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes") of the main article, we classify
    the literature based on a taxonomy consisting of four axes: robot competencies
    learned with DRL, problem formulation, solution approach, and the level of real-world
    success. In this section, we provide a detailed definition and discussion of the
    elements along the problem formulation and solution approach axes.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Problem Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed in Sec. [3](#S3 "3 Taxonomy ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes"), we categorize the papers based on
    the following elements of the problem formulation: 1) *Action space*: whether
    the actions are *low-level* (i.e., joint or motor commands), *mid-level* (i.e.,
    task-space commands), or *high-level* (i.e., temporally extended task-space commands
    or subroutines); 2) *Observation space*: whether the observations are *high-dimensional*
    sensor inputs (e.g., images and/or LiDAR scans) or estimated *low-dimensional*
    state vectors; 3) *Reward function*: whether the reward signals are *sparse* or
    *dense*. This subsection provides detailed definitions and a discussion of these
    terms.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1.1 Action Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Low-level Actions: We define low-level actions as those that directly operate
    in the robot’s joint space, such as controlling torques of individual joints in
    a robot arm or velocities of individual wheels in a mobile robot. A low-level
    action space requires minimal domain knowledge and allows the policy to have fine-grained
    control over the robot’s behavior. However, performing learning in low-level action
    spaces presents several challenges: 1) exploration with low-level actions is difficult,
    as random joint actions often result in trivial behaviors; 2) the action space
    scales linearly with the robot’s degrees of freedom, often resulting in high-dimensional
    action spaces; and 3) joints are often controlled at a high frequency, resulting
    in extended task horizons and inference-time constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mid-level Actions: Mid-level actions control the robot in its workspace, such
    as adjusting the end-effector pose of a robot arm or controlling the velocity
    of the center of mass of a mobile robot. Once the policies generate these mid-level
    actions, they are often executed by an external controller, such as an inverse
    kinematics (IK) controller, to produce the joint-level torques. As such, operating
    in a mid-level action space requires domain knowledge to define an appropriate
    operational space and to design and implement the external controller effectively.
    When chosen correctly, mid-level action spaces strike a balance between incorporating
    domain knowledge and maintaining generality for various tasks. This approach is
    a popular choice in many RL applications for robotics, as it leverages specific
    expertise while allowing flexibility across different robotic functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'High-level Actions: High-level actions control the robot through temporally
    extended “skills” that can realize certain short-horizon behaviors, such as “grasping
    certain objects” or “moving to certain rooms.” A well-designed high-level action
    space can greatly enhance the efficiency of the RL agent’s exploration by drastically
    shortening the task horizon and ensuring that the robot performs task-relevant
    actions most of the time. However, designing an appropriate set of skills for
    the high-level action space is a complex problem, often requiring each skill to
    be formulated as an RL problem in itself. Additionally, these skills may not always
    be transferable across tasks, posing challenges to their scalability.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1.2 Observation Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Low-dimensional Observations: The robot’s observations are represented as a
    compact, low-dimensional vector, which can include proprioceptive information,
    object locations, and task information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'High-dimensional Observations: The robot’s observations include high-dimensional
    sensor data for exteroceptive information, which can be in the form of lidar readings,
    camera images, and/or point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1.3 Reward Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sparse Reward: A sparse reward signal means the agent receives trivial reward
    signals for most of the potential transitions in a (PO)MDP and only receives non-trivial
    reward signals sparsely. One natural way of defining a sparse reward for a task
    is to have +1 for all transitions into a success termination state, -1 for all
    transitions into a failure termination state, and 0 for any other transitions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dense Reward: A dense reward means the reward signal is abundant, providing
    rich feedback to the agent. In certain tasks, such as locomotion, the reward is
    naturally dense (e.g., the error between the robot’s current forward velocity
    and the instructed velocity). In other scenarios where the task reward is inherently
    sparse, such as navigation tasks, a dense reward can be defined by adding shaping
    components to the sparse reward (e.g., the distance between the robot and the
    navigation target). Such kind of shaped and dense rewards are often used to facilitate
    learning efficiency, especially for long-horizon tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Solution Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As introduced in Sec. [3](#S3 "3 Taxonomy ‣ Deep Reinforcement Learning for
    Robotics: A Survey of Real-World Successes"), we classify the solution approach
    from the following perspectives: 1) *Simulator usage*: whether and how simulators
    are used, categorized into *zero-shot*, *few-shot sim-to-real transfer*, or directly
    learning offline or in the real world *without simulators*; 2) *Model learning*:
    whether (a part of) the transition dynamics model is learned from robot data;
    3) *Expert usage*: whether expert (e.g., human or oracle policy) data are used
    to facilitate learning; 4) *Policy optimization*: the policy optimization algorithm
    adopted, including *planning* or *offline*, *off-policy*, or *on-policy RL*; 5)
    *Policy/Model Representation*: Classes of neural network architectures used to
    represent the policy or dynamics model, including *MLP*, *CNN*, *RNN*, and *Transformer*.
    This subsection provides detailed definitions of these terms.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.1 Simulator Usage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Zero-shot sim2real: The training is performed entirely in a simulator, where
    the trained policy is deployed directly in the real world without additional learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Few-shot sim2real: The robot is pre-trained in the simulator and fine-tuned
    in the real world with limited additional real-world interactions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'No Simulator: The training is conducted in the real world without using a simulator.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.2 Model Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'RL algorithms can be broadly classified into two categories: model-free RL
    and model-based RL, based on whether they learn a dynamics model. In model-free
    RL algorithms, such as PPO and SAC, the robot directly learns a policy or value
    function without explicitly modeling the environment’s dynamics. Model-free RL
    is often easier to implement and superior when learning a good policy is simpler
    than learning a good model. In contrast, model-based RL algorithms, such as TD-MPC
    and Dreamer, involve the robot learning a world model that can predict the consequences
    of its actions. This world model can be used either for model-based planning and
    control or for generating experiences for a model-free RL agent, potentially increasing
    the agent’s sample efficiency. Instead of learning the full dynamics, some methods
    learn a residual or a part of the dynamics model (e.g., actuator dynamics model)
    to complement the simulation for model-free RL, which we also mark as involving
    model learning in our categorization.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.3 Expert Usage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tabula rasa RL begins with random initialization, training entirely through
    trial and error. However, in robotics, it is sometimes possible to utilize an
    external expert to expedite the learning process. These experts may include human
    demonstrations, trajectory planners, oracle actions, and so on. In this survey,
    we classify all works that utilize an external expert, either offline or online,
    to facilitate learning as works “with experts”, which gives them an advantage
    over methods that do not assume access to experts.
  prefs: []
  type: TYPE_NORMAL
- en: A.2.4 Policy Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Planning: The robot’s policy is derived by solving an optimal control problem
    online using a learned world model. Representative algorithms include A* and MPPI
    (Model Predictive Path Integral).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Offline: The robot does not interact with the environment during learning.
    Instead, it learns a policy and, optionally, a value function directly from offline
    data. Representative algorithms include CQL (Conservative Q-Learning) and DT (Decision
    Transformer).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On-policy: The robot interacts with the environment during learning and only
    updates the policy with transitions collected by the current policy. Representative
    algorithms include PPO (Proximal Policy Optimization) and TRPO (Trust Region Policy
    Optimization).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Off-policy: The robot interacts with the environment during learning and updates
    the policy with transitions collected by both the current policy and other/previous
    policies. Representative algorithms include SAC (Soft Actor-Critic) and DQN (Deep
    Q-Network).'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.5 Policy/Model Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'MLP Only: Multi-layer Perceptron (MLP) models take 1D vector inputs and consist
    solely of fully connected layers. They are widely used for processing low-dimensional
    observations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CNN: Convolutional Neural Networks (CNNs) are a specialized type of MLP that
    preserves local spatial coherence, initially designed for image processing. Later
    works have extended CNNs to process 1D data like lidar readings and observation
    memory, as well as 3D data such as point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RNN: Recurrent Neural Networks (RNNs), including LSTM and GRU, are bidirectional
    neural networks with internal memory. They are suitable for processing time-series
    data, such as trajectories over time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformer: Transformers take a sequence of vectors (tokens) as input and
    use multi-head self-attention to generate outputs. Recently, transformers have
    been widely used to process time-series data, natural language instructions, and
    visual information. They have also proven powerful in fusing tokenized multi-modal
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Additional Tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section contains tables that present the complete categorization of the
    reviewed papers along all four axes of our taxonomy. Tables [1](#A2.T1 "Table
    1 ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A
    Survey of Real-World Successes")-[2](#A2.T2 "Table 2 ‣ Appendix B Additional Tables
    ‣ Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes")
    categorize the papers based on problem formulation for each robot competency,
    while Tables [3](#A2.T3 "Table 3 ‣ Appendix B Additional Tables ‣ Deep Reinforcement
    Learning for Robotics: A Survey of Real-World Successes")-[6](#A2.T6 "Table 6
    ‣ Appendix B Additional Tables ‣ Deep Reinforcement Learning for Robotics: A Survey
    of Real-World Successes") categorize the papers based on solution approach for
    each robot competency. As in the tables in the main article, the color map indicates
    the levels of real-world success: *Sim Only*, *Limited Lab*, *Diverse Lab*, *Limited
    Real*, and *Diverse Real*.'
  prefs: []
  type: TYPE_NORMAL
- en: In these tables, we add a superscript ^∗ to papers that appear in multiple columns,
    which means they adopt two different elements jointly (e.g., a hierarchical policy
    that outputs both low-level and mid-level actions, a policy network consists of
    both CNN and RNN).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Action Space |'
  prefs: []
  type: TYPE_TB
- en: '| Application | Low-Level | Mid-Level | High-Level |'
  prefs: []
  type: TYPE_TB
- en: '| Locomotion | [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31)^∗, [32](#bib.bib32)^∗, [33](#bib.bib33), [36](#bib.bib36)^∗,
    [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [48](#bib.bib48),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65), [68](#bib.bib68) | [31](#bib.bib31)^∗, [32](#bib.bib32)^∗, [34](#bib.bib34),
    [35](#bib.bib35), [47](#bib.bib47), [66](#bib.bib66), [67](#bib.bib67) | [36](#bib.bib36)^∗,
    [60](#bib.bib60) |'
  prefs: []
  type: TYPE_TB
- en: '| Navigation | [20](#bib.bib20), [90](#bib.bib90), [96](#bib.bib96)^∗, [97](#bib.bib97)^∗,
    [99](#bib.bib99), [100](#bib.bib100), | [7](#bib.bib7), [21](#bib.bib21), [73](#bib.bib73),
    [74](#bib.bib74), [75](#bib.bib75), [78](#bib.bib78), [83](#bib.bib83), [85](#bib.bib85),
    [88](#bib.bib88), [89](#bib.bib89), [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95)^∗, [98](#bib.bib98), [101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103) | [76](#bib.bib76), [81](#bib.bib81), [82](#bib.bib82), [86](#bib.bib86),
    [87](#bib.bib87), [95](#bib.bib95)^∗, [96](#bib.bib96)^∗, [97](#bib.bib97)^∗ |'
  prefs: []
  type: TYPE_TB
- en: '| Manipulation | [113](#bib.bib113), [122](#bib.bib122), [127](#bib.bib127),
    [131](#bib.bib131), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142) | [54](#bib.bib54), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [114](#bib.bib114), [115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119),
    [120](#bib.bib120), [121](#bib.bib121), [124](#bib.bib124), [125](#bib.bib125),
    [126](#bib.bib126), [128](#bib.bib128), [129](#bib.bib129), [130](#bib.bib130),
    [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [137](#bib.bib137),
    [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145) | [108](#bib.bib108),
    [109](#bib.bib109), [123](#bib.bib123), [135](#bib.bib135), [136](#bib.bib136)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MoMa | [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155), [163](#bib.bib163),
    [167](#bib.bib167), [169](#bib.bib169), [158](#bib.bib158), [162](#bib.bib162),
    [159](#bib.bib159), [166](#bib.bib166) | [152](#bib.bib152), [164](#bib.bib164),
    [160](#bib.bib160), [161](#bib.bib161), [171](#bib.bib171), [157](#bib.bib157),
    [170](#bib.bib170), [165](#bib.bib165) | [168](#bib.bib168) |'
  prefs: []
  type: TYPE_TB
- en: '| HRI | [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177), [178](#bib.bib178),
    [179](#bib.bib179), [180](#bib.bib180) | [173](#bib.bib173), [174](#bib.bib174),
    [181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183) | [172](#bib.bib172)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-Robot Interaction | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187),
    [188](#bib.bib188), [190](#bib.bib190), [191](#bib.bib191) | [189](#bib.bib189)
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Categorizing Literature based on Problem Formulation'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Observation Space | Reward Function |'
  prefs: []
  type: TYPE_TB
- en: '| Application | High-dim | Low-dim | Sparse | Dense |'
  prefs: []
  type: TYPE_TB
- en: '| Locomotion | [35](#bib.bib35), [36](#bib.bib36), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [49](#bib.bib49), [50](#bib.bib50), [54](#bib.bib54), [61](#bib.bib61)
    | [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [37](#bib.bib37), [38](#bib.bib38),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [56](#bib.bib56) | [27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68) |'
  prefs: []
  type: TYPE_TB
- en: '| Navigation | [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76),
    [78](#bib.bib78), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [91](#bib.bib91),
    [92](#bib.bib92), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99), [101](#bib.bib101), [102](#bib.bib102) | [7](#bib.bib7),
    [20](#bib.bib20), [21](#bib.bib21), [90](#bib.bib90), [93](#bib.bib93), [100](#bib.bib100),
    [103](#bib.bib103) | [78](#bib.bib78), [96](#bib.bib96)^∗ | [7](#bib.bib7), [20](#bib.bib20),
    [21](#bib.bib21), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [85](#bib.bib85), [86](#bib.bib86),
    [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96)^∗,
    [97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101),
    [102](#bib.bib102), [103](#bib.bib103) |'
  prefs: []
  type: TYPE_TB
- en: '| Manipulation | [54](#bib.bib54), [108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113),
    [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116), [117](#bib.bib117),
    [118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [124](#bib.bib124), [125](#bib.bib125), [128](#bib.bib128), [133](#bib.bib133),
    [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136), [137](#bib.bib137),
    [141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144),
    [145](#bib.bib145) | [122](#bib.bib122), [123](#bib.bib123), [126](#bib.bib126),
    [127](#bib.bib127), [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131),
    [132](#bib.bib132), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140)
    | [54](#bib.bib54), [108](#bib.bib108), [110](#bib.bib110), [111](#bib.bib111),
    [112](#bib.bib112), [114](#bib.bib114), [115](#bib.bib115), [117](#bib.bib117),
    [118](#bib.bib118), [122](#bib.bib122), [124](#bib.bib124), [128](#bib.bib128),
    [129](#bib.bib129), [134](#bib.bib134) | [109](#bib.bib109), [113](#bib.bib113),
    [116](#bib.bib116), [118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [123](#bib.bib123), [125](#bib.bib125), [126](#bib.bib126),
    [127](#bib.bib127), [130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133), [135](#bib.bib135), [136](#bib.bib136), [137](#bib.bib137),
    [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140) [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143) [144](#bib.bib144), [145](#bib.bib145)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MoMa | [163](#bib.bib163), [167](#bib.bib167), [164](#bib.bib164), [160](#bib.bib160),
    [168](#bib.bib168), [165](#bib.bib165), [159](#bib.bib159), [171](#bib.bib171),
    [157](#bib.bib157), [170](#bib.bib170) | [153](#bib.bib153), [154](#bib.bib154),
    [152](#bib.bib152), [155](#bib.bib155), [169](#bib.bib169), [158](#bib.bib158),
    [162](#bib.bib162), [161](#bib.bib161), [166](#bib.bib166) | [168](#bib.bib168),
    [171](#bib.bib171), [170](#bib.bib170) | [153](#bib.bib153), [154](#bib.bib154),
    [152](#bib.bib152), [155](#bib.bib155), [163](#bib.bib163), [167](#bib.bib167),
    [169](#bib.bib169), [158](#bib.bib158), [162](#bib.bib162), [164](#bib.bib164),
    [160](#bib.bib160), [161](#bib.bib161), [165](#bib.bib165), [159](#bib.bib159),
    [166](#bib.bib166), [157](#bib.bib157) |'
  prefs: []
  type: TYPE_TB
- en: '| HRI | [173](#bib.bib173), [174](#bib.bib174), [177](#bib.bib177), [178](#bib.bib178),
    [181](#bib.bib181) | [172](#bib.bib172), [175](#bib.bib175), [176](#bib.bib176),
    [179](#bib.bib179), [180](#bib.bib180), [182](#bib.bib182), [183](#bib.bib183)
    | [172](#bib.bib172) | [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175),
    [176](#bib.bib176), [177](#bib.bib177), [178](#bib.bib178), [179](#bib.bib179),
    [180](#bib.bib180), [181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-Robot Interaction |  | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191)
    | [184](#bib.bib184), [185](#bib.bib185) | [187](#bib.bib187), [188](#bib.bib188),
    [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Categorizing Literature based on Problem Formulation (Cont.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Simulator Usage |'
  prefs: []
  type: TYPE_TB
- en: '| Application | Zero-shot Sim-to-Real | Few-shot Sim-to-Real | No Simulator
    |'
  prefs: []
  type: TYPE_TB
- en: '| Locomotion | [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52), [55](#bib.bib55),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [43](#bib.bib43), [48](#bib.bib48), [56](#bib.bib56)
    | [53](#bib.bib53), [54](#bib.bib54) |'
  prefs: []
  type: TYPE_TB
- en: '| Navigation | [20](#bib.bib20), [21](#bib.bib21), [73](#bib.bib73), [74](#bib.bib74),
    [75](#bib.bib75), [76](#bib.bib76), [78](#bib.bib78), [81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [93](#bib.bib93),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [103](#bib.bib103) |
    [7](#bib.bib7), [102](#bib.bib102) | [88](#bib.bib88), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92) |'
  prefs: []
  type: TYPE_TB
- en: '| Manipulation | [108](#bib.bib108), [111](#bib.bib111), [123](#bib.bib123),
    [130](#bib.bib130), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145)
    | [116](#bib.bib116), [131](#bib.bib131) | [54](#bib.bib54), [109](#bib.bib109),
    [110](#bib.bib110), [112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119),
    [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122), [124](#bib.bib124),
    [125](#bib.bib125), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [132](#bib.bib132), [136](#bib.bib136), [140](#bib.bib140)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MoMa | [153](#bib.bib153), [154](#bib.bib154), [152](#bib.bib152), [155](#bib.bib155),
    [163](#bib.bib163), [167](#bib.bib167), [169](#bib.bib169), [162](#bib.bib162),
    [164](#bib.bib164), [161](#bib.bib161), [165](#bib.bib165), [159](#bib.bib159),
    [166](#bib.bib166), [171](#bib.bib171), [157](#bib.bib157) | [158](#bib.bib158),
    [170](#bib.bib170) | [160](#bib.bib160), [168](#bib.bib168) |'
  prefs: []
  type: TYPE_TB
- en: '| HRI | [172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175),
    [176](#bib.bib176), [177](#bib.bib177), [179](#bib.bib179) | [182](#bib.bib182)
    | [178](#bib.bib178), [181](#bib.bib181), [180](#bib.bib180) |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-Robot Interaction | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191)
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Categorizing Literature based on Solution Approach'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Model Learning | Expert Usage |'
  prefs: []
  type: TYPE_TB
- en: '| Application | with Model Learning | No Model   Learning | No Expert | with
    Expert |'
  prefs: []
  type: TYPE_TB
- en: '| Locomotion | [29](#bib.bib29), [31](#bib.bib31), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [41](#bib.bib41), [51](#bib.bib51), [52](#bib.bib52), [54](#bib.bib54),
    [56](#bib.bib56) | [27](#bib.bib27), [28](#bib.bib28), [30](#bib.bib30), [32](#bib.bib32),
    [33](#bib.bib33), [37](#bib.bib37), [38](#bib.bib38), [40](#bib.bib40), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [53](#bib.bib53), [55](#bib.bib55),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [28](#bib.bib28), [29](#bib.bib29), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [37](#bib.bib37),
    [38](#bib.bib38), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [47](#bib.bib47), [49](#bib.bib49), [50](#bib.bib50), [52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54), [56](#bib.bib56), [57](#bib.bib57), [59](#bib.bib59),
    [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68) | [27](#bib.bib27), [30](#bib.bib30), [36](#bib.bib36),
    [40](#bib.bib40), [46](#bib.bib46), [48](#bib.bib48), [51](#bib.bib51), [55](#bib.bib55),
    [58](#bib.bib58), [63](#bib.bib63), [64](#bib.bib64) |'
  prefs: []
  type: TYPE_TB
- en: '| Navigation | [7](#bib.bib7), [20](#bib.bib20), [74](#bib.bib74)*, [88](#bib.bib88),
    [90](#bib.bib90), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [102](#bib.bib102) | [21](#bib.bib21), [73](#bib.bib73), [74](#bib.bib74)^∗ [75](#bib.bib75),
    [76](#bib.bib76), [78](#bib.bib78), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [89](#bib.bib89), [91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [99](#bib.bib99), [100](#bib.bib100),
    [101](#bib.bib101), [103](#bib.bib103) | [7](#bib.bib7), [20](#bib.bib20), [21](#bib.bib21),
    [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [78](#bib.bib78), [81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96),
    [98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103) | [76](#bib.bib76), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91),
    [92](#bib.bib92), [97](#bib.bib97) |'
  prefs: []
  type: TYPE_TB
- en: '| Manipulation | [54](#bib.bib54), [113](#bib.bib113), [118](#bib.bib118),
    [140](#bib.bib140) | [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [114](#bib.bib114), [115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124),
    [125](#bib.bib125), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139), [141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145)
    | [54](#bib.bib54), [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111), [114](#bib.bib114), [115](#bib.bib115), [116](#bib.bib116),
    [118](#bib.bib118), [119](#bib.bib119), [123](#bib.bib123), [125](#bib.bib125),
    [130](#bib.bib130), [131](#bib.bib131), [133](#bib.bib133), [135](#bib.bib135),
    [137](#bib.bib137), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143), [144](#bib.bib144),
    [145](#bib.bib145) | [112](#bib.bib112), [113](#bib.bib113), [117](#bib.bib117),
    [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122), [124](#bib.bib124),
    [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128), [129](#bib.bib129),
    [132](#bib.bib132), [134](#bib.bib134), [136](#bib.bib136) |'
  prefs: []
  type: TYPE_TB
- en: '| MoMa |  | [153](#bib.bib153), [154](#bib.bib154), [152](#bib.bib152), [155](#bib.bib155),
    [163](#bib.bib163), [167](#bib.bib167), [169](#bib.bib169), [158](#bib.bib158),
    [162](#bib.bib162), [164](#bib.bib164), [160](#bib.bib160), [161](#bib.bib161),
    [168](#bib.bib168), [165](#bib.bib165), [159](#bib.bib159), [166](#bib.bib166),
    [171](#bib.bib171), [157](#bib.bib157), [170](#bib.bib170) | [153](#bib.bib153),
    [154](#bib.bib154), [152](#bib.bib152), [163](#bib.bib163), [167](#bib.bib167),
    [169](#bib.bib169), [158](#bib.bib158), [162](#bib.bib162), [164](#bib.bib164),
    [160](#bib.bib160), [161](#bib.bib161), [165](#bib.bib165), [159](#bib.bib159),
    [166](#bib.bib166), [171](#bib.bib171) | [155](#bib.bib155), [168](#bib.bib168),
    [157](#bib.bib157), [170](#bib.bib170) |'
  prefs: []
  type: TYPE_TB
- en: '| HRI | [173](#bib.bib173), [174](#bib.bib174), [181](#bib.bib181) | [172](#bib.bib172),
    [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177), [178](#bib.bib178),
    [179](#bib.bib179), [180](#bib.bib180), [182](#bib.bib182), [183](#bib.bib183)
    | [173](#bib.bib173), [174](#bib.bib174), [177](#bib.bib177), [179](#bib.bib179),
    [180](#bib.bib180), [182](#bib.bib182), [183](#bib.bib183) | [172](#bib.bib172),
    [175](#bib.bib175), [176](#bib.bib176), [178](#bib.bib178), [181](#bib.bib181)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-Robot Interaction |  | [184](#bib.bib184), [185](#bib.bib185), [187](#bib.bib187),
    [188](#bib.bib188), [189](#bib.bib189), [190](#bib.bib190), [191](#bib.bib191)
    | [187](#bib.bib187), [188](#bib.bib188), [190](#bib.bib190) | [184](#bib.bib184),
    [185](#bib.bib185), [189](#bib.bib189), [191](#bib.bib191) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Categorizing Literature based on Solution Approach (Cont.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Policy Optimization |'
  prefs: []
  type: TYPE_TB
- en: '| Application | Planning | Offline | Off-Policy | On-Policy |'
  prefs: []
  type: TYPE_TB
- en: '| Locomotion |  |  | [32](#bib.bib32), [36](#bib.bib36)^∗, [48](#bib.bib48),
    [53](#bib.bib53), [54](#bib.bib54), [68](#bib.bib68) | [27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)^∗, [37](#bib.bib37), [38](#bib.bib38),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47), [49](#bib.bib49), [50](#bib.bib50),
    [51](#bib.bib51), [52](#bib.bib52), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57),
    [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Navigation | [74](#bib.bib74)^∗, [76](#bib.bib76)^∗, [88](#bib.bib88), [90](#bib.bib90),
    [102](#bib.bib102), [103](#bib.bib103)^∗ | [76](#bib.bib76)^∗, [89](#bib.bib89),
    [91](#bib.bib91)^∗, | [73](#bib.bib73), [74](#bib.bib74)^∗, [75](#bib.bib75),
    [78](#bib.bib78), [91](#bib.bib91)^∗, [92](#bib.bib92), [94](#bib.bib94), [101](#bib.bib101)
    | [7](#bib.bib7), [20](#bib.bib20), [21](#bib.bib21), [81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [93](#bib.bib93),
    [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99),
    [100](#bib.bib100), [103](#bib.bib103)^∗ |'
  prefs: []
  type: TYPE_TB
- en: '| Manipulation | [118](#bib.bib118), [140](#bib.bib140) | [108](#bib.bib108),
    [115](#bib.bib115), [121](#bib.bib121), [122](#bib.bib122), [124](#bib.bib124),
    [129](#bib.bib129) | [54](#bib.bib54), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111), [112](#bib.bib112), [114](#bib.bib114), [116](#bib.bib116),
    [117](#bib.bib117), [119](#bib.bib119), [120](#bib.bib120), [123](#bib.bib123),
    [125](#bib.bib125), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135),
    [136](#bib.bib136), [137](#bib.bib137), [143](#bib.bib143), [144](#bib.bib144)
    | [113](#bib.bib113), [130](#bib.bib130), [131](#bib.bib131), [138](#bib.bib138),
    [139](#bib.bib139), [141](#bib.bib141), [142](#bib.bib142), [145](#bib.bib145)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MoMa |  |  | [158](#bib.bib158)^∗, [164](#bib.bib164), [160](#bib.bib160),
    [161](#bib.bib161), [171](#bib.bib171), [170](#bib.bib170) | [153](#bib.bib153),
    [154](#bib.bib154), [152](#bib.bib152), [155](#bib.bib155), [163](#bib.bib163),
    [167](#bib.bib167), [169](#bib.bib169), [158](#bib.bib158)^∗, [162](#bib.bib162),
    [168](#bib.bib168), [165](#bib.bib165), [159](#bib.bib159), [166](#bib.bib166),
    [157](#bib.bib157) |'
  prefs: []
  type: TYPE_TB
- en: '| HRI | [181](#bib.bib181) | [178](#bib.bib178)^∗ | [172](#bib.bib172), [173](#bib.bib173),
    [174](#bib.bib174), [178](#bib.bib178)^∗, [179](#bib.bib179), [180](#bib.bib180),
    [182](#bib.bib182) | [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177),
    [183](#bib.bib183) |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-Robot Interaction |  |  | [191](#bib.bib191) | [184](#bib.bib184),
    [185](#bib.bib185), [187](#bib.bib187), [188](#bib.bib188), [189](#bib.bib189),
    [190](#bib.bib190) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Categorizing Literature based on Solution Approach (Cont.)'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Policy/Model Representation |'
  prefs: []
  type: TYPE_TB
- en: '| Application | MLP Only | CNN | RNN | Transformer |'
  prefs: []
  type: TYPE_TB
- en: '| Locomotion | [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [38](#bib.bib38), [40](#bib.bib40), [41](#bib.bib41), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48), [51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [56](#bib.bib56), [58](#bib.bib58), [60](#bib.bib60), [64](#bib.bib64), [65](#bib.bib65),
    [66](#bib.bib66), [68](#bib.bib68) | [27](#bib.bib27), [33](#bib.bib33), [34](#bib.bib34),
    [36](#bib.bib36), [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)^∗, [45](#bib.bib45),
    [49](#bib.bib49)^∗, [54](#bib.bib54)^∗, [63](#bib.bib63), [67](#bib.bib67) | [35](#bib.bib35),
    [37](#bib.bib37), [44](#bib.bib44)^∗, [49](#bib.bib49), [50](#bib.bib50), [54](#bib.bib54)^∗,
    [55](#bib.bib55), [57](#bib.bib57), [59](#bib.bib59), [61](#bib.bib61) | [62](#bib.bib62)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Navigation | [7](#bib.bib7), [20](#bib.bib20), [21](#bib.bib21), [73](#bib.bib73),
    [74](#bib.bib74)^∗, [75](#bib.bib75), [76](#bib.bib76), [90](#bib.bib90), [93](#bib.bib93),
    [99](#bib.bib99), [100](#bib.bib100), [103](#bib.bib103)^§ | [78](#bib.bib78),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83)^∗, [85](#bib.bib85), [86](#bib.bib86)^∗,
    [87](#bib.bib87)^∗, [89](#bib.bib89), [91](#bib.bib91), [92](#bib.bib92), [94](#bib.bib94),
    [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98)^∗, [101](#bib.bib101), [102](#bib.bib102)^∗
    | [74](#bib.bib74)^∗, [83](#bib.bib83)^∗, [85](#bib.bib85), [86](#bib.bib86)^∗,
    [87](#bib.bib87)^∗, [95](#bib.bib95), [98](#bib.bib98)^∗, [102](#bib.bib102)^∗
    | [74](#bib.bib74)^∗, |'
  prefs: []
  type: TYPE_TB
- en: '| Manipulation | [123](#bib.bib123), [129](#bib.bib129), [131](#bib.bib131),
    [132](#bib.bib132), [138](#bib.bib138), [139](#bib.bib139), [140](#bib.bib140),
    [143](#bib.bib143) | [54](#bib.bib54)^∗, [108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112), [113](#bib.bib113),
    [114](#bib.bib114), [115](#bib.bib115), [117](#bib.bib117), [118](#bib.bib118)^∗
    [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122),
    [125](#bib.bib125), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137), [142](#bib.bib142), [144](#bib.bib144) | [54](#bib.bib54)^∗,
    [118](#bib.bib118)^∗, [130](#bib.bib130) | [116](#bib.bib116), [124](#bib.bib124),
    [141](#bib.bib141), [145](#bib.bib145) |'
  prefs: []
  type: TYPE_TB
- en: '| MoMa | [153](#bib.bib153), [154](#bib.bib154), [152](#bib.bib152), [155](#bib.bib155),
    [169](#bib.bib169), [158](#bib.bib158), [162](#bib.bib162), [161](#bib.bib161),
    [166](#bib.bib166) | [163](#bib.bib163), [167](#bib.bib167)^∗, [164](#bib.bib164),
    [160](#bib.bib160), [168](#bib.bib168), [165](#bib.bib165)^∗, [159](#bib.bib159),
    [171](#bib.bib171), [157](#bib.bib157), [170](#bib.bib170)^∗ | [167](#bib.bib167)^∗,
    [165](#bib.bib165)^∗, [170](#bib.bib170)^∗ |  |'
  prefs: []
  type: TYPE_TB
- en: '| HRI | [175](#bib.bib175), [179](#bib.bib179), [180](#bib.bib180), [182](#bib.bib182),
    [183](#bib.bib183) | [173](#bib.bib173), [174](#bib.bib174), [177](#bib.bib177),
    [178](#bib.bib178), [181](#bib.bib181)^∗ | [172](#bib.bib172), [176](#bib.bib176)
    | [181](#bib.bib181)^∗ |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-Robot Interaction | [184](#bib.bib184), [187](#bib.bib187), [190](#bib.bib190),
    [191](#bib.bib191) |  | [185](#bib.bib185), [188](#bib.bib188), [189](#bib.bib189)
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Categorizing Literature based on Solution Approach (Cont.)'
  prefs: []
  type: TYPE_NORMAL
