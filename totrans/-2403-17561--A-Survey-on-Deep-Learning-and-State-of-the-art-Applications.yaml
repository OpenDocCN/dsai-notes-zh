- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:33:45'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2403.17561] A Survey on Deep Learning and State-of-the-art Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.17561](https://ar5iv.labs.arxiv.org/html/2403.17561)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[type=editor, orcid=0000-0002-3300-3270]'
  prefs: []
  type: TYPE_NORMAL
- en: \cormark
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: \fnmark
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: \credit
  prefs: []
  type: TYPE_NORMAL
- en: Conceptualization of this study, writing - original draft, writing - review
    and editing, funding acquisition
  prefs: []
  type: TYPE_NORMAL
- en: 1]organization=School of Computer Sciences, Universiti Sains Malaysia, postcode=11800,
    state=Pulau Pinang, country=Malaysia
  prefs: []
  type: TYPE_NORMAL
- en: '[type=editor, orcid=0000-0001-5155-9304]'
  prefs: []
  type: TYPE_NORMAL
- en: \fnmark
  prefs: []
  type: TYPE_NORMAL
- en: '[2]'
  prefs: []
  type: TYPE_NORMAL
- en: \credit
  prefs: []
  type: TYPE_NORMAL
- en: Writing - original draft
  prefs: []
  type: TYPE_NORMAL
- en: 2]organization=Department of Computer Science, Adekunle Ajasin University, city=Akungba-Akoko,
    postcode=P.M.B 001, state=Ondo State, country=Nigeria
  prefs: []
  type: TYPE_NORMAL
- en: \cortext
  prefs: []
  type: TYPE_NORMAL
- en: '[cor1]Corresponding author'
  prefs: []
  type: TYPE_NORMAL
- en: A Survey on Deep Learning and State-of-the-art Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mohd Halim Mohd Noor halimnoor@usm.my [    Ayokunle Olalekan Ige ayo.ige@aaua.edu.ng
    [
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning, a branch of artificial intelligence, is a computational model
    that uses multiple layers of interconnected units (neurons) to learn intricate
    patterns and representations directly from raw input data. Empowered by this learning
    capability, it has become a powerful tool for solving complex problems and is
    the core driver of many groundbreaking technologies and innovations. Building
    a deep learning model is a challenging task due to the algorithm’s complexity
    and the dynamic nature of real-world problems. Several studies have reviewed deep
    learning concepts and applications. However, the studies mostly focused on the
    types of deep learning models and convolutional neural network architectures,
    offering limited coverage of the state-of-the-art of deep learning models and
    their applications in solving complex problems across different domains. Therefore,
    motivated by the limitations, this study aims to comprehensively review the state-of-the-art
    deep learning models in computer vision, natural language processing, time series
    analysis and pervasive computing. We highlight the key features of the models
    and their effectiveness in solving the problems within each domain. Furthermore,
    this study presents the fundamentals of deep learning, various deep learning model
    types and prominent convolutional neural network architectures. Finally, challenges
    and future directions in deep learning research are discussed to offer a broader
    perspective for future researchers.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep Learning \sepConvolutional Neural Network \sepComputer Vision \sepNatural
    Language Processing \sepTime Series Analysis\sepPervasive Computing
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning has revolutionized many applications across a variety of industries
    and research. The application of deep learning can be found in healthcare Shamshirband
    et al. ([2021](#bib.bib215)), smart manufacturing Wang et al. ([2018b](#bib.bib261)),
    robotics Pierson and Gashler ([2017](#bib.bib193)), cybersecurity Dixit and Silakari
    ([2021](#bib.bib50)) etc., solving challenging and complex problems such as disease
    diagnosis, anomaly detection, object detection and malware attack detection. Deep
    learning is a subset of machine learning that learns from data using artificial
    neural networks. An artificial neural network is a computational model that imitates
    the working principles of a human brain. The computational models are composed
    of an input layer which receives the input data, multiple processing layers that
    learn the representation of data and the output layer which produces the output
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to the reintroduction of deep learning (DL) into the research trend, pattern
    recognition tasks involved a transformation of the raw input data such as pixel
    values of an image into a feature vector that represents the internal representation
    of the data. The feature vector can be used by a machine learning model to detect
    or classify patterns in the data. This process requires feature engineering and
    considerable domain knowledge to design a suitable feature representation. With
    deep learning, this cumbersome process can be performed automatically whereby
    at each processing layer known as hidden layers, the internal representation of
    the input data is learned or extracted in a hierarchical manner. The first layer
    learns the presence of basic primitive features such as edges, dots, lines etc.
    The second layer learns patterns or motifs by recognizing the combinations of
    the edges, dots and lines, and the subsequent layers combine the motifs to produce
    more sophisticated features that correspond to the input data. This feature learning
    process takes place in the sequence of hidden layers until the prediction is finally
    produced.
  prefs: []
  type: TYPE_NORMAL
- en: Several studies have been conducted to discuss the concept and application of
    deep learning in the last few years, as listed in Table 1\. The studies addressed
    or focused on several aspects of deep learning, such as types of deep learning
    models, learning approaches and strategies, convolutional neural network (CNN)
    architectures, deep learning applications and challenges. In Dong et al. ([2021](#bib.bib51)),
    the authors provided fundamentals of deep learning and highlighted different types
    of deep learning models, such as convolutional neural networks, autoencoder and
    generative adversarial networks. Then, the applications of deep learning in various
    domains are discussed, and some challenges associated with deep learning applications
    are presented. Another survey Talaei Khoei et al. ([2023](#bib.bib243)) provided
    a comprehensive analysis of supervised, unsupervised and reinforcement learning
    approaches and compared the different learning strategies such as online, federated
    and transfer learning. Finally, the current challenges of deep learning and future
    direction are discussed.
  prefs: []
  type: TYPE_NORMAL
- en: In Alzubaidi et al. ([2021](#bib.bib11)), the authors provided a comprehensive
    review of the popular CNN architectures used in computer vision tasks, highlighting
    their key features and advantages. Then, the applications of deep learning in
    medical imaging and the challenges are discussed. A similar survey is reported
    in Alom et al. ([2019](#bib.bib7)), where the different supervised and unsupervised
    deep learning models are highlighted, and the popular CNN architectures are compared
    and discussed. In another survey Pouyanfar et al. ([2018](#bib.bib194)), the authors
    focused on the applications of deep learning in computer vision, natural language
    processing and speech and audio processing. The different types of deep learning
    models are also discussed. In Sarker ([2021](#bib.bib212)), the authors focused
    on the different types of deep learning models and provided a summary of deep
    learning applications in various domains.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the existing surveys on deep learning that offer valuable insights,
    the increasing amount of deep learning applications and the existing limitations
    in the current studies motivated us to explore this topic in depth. In general,
    to the best of our knowledge, no survey paper focuses on the emerging trends in
    state-of-the-art applications and the current challenges associated with deep
    learning. Furthermore, the surveys do not discuss the issues and how deep learning
    addresses them by highlighting the key features and components in the models.
    Also, most surveys either ignore or provide minimal coverage of the fundamentals
    of deep learning, which is crucial for understanding the state-of-the-art models.
    The main objective of this paper is to present the most important aspects of deep
    learning, making it accessible to a wide audience and facilitating researchers
    and practitioners in advancing and leveraging its capabilities to solve complex
    problems across diverse domains. Specifically, we present the fundamentals of
    deep learning and the various types of deep learning models, including popular
    deep learning architectures. Then, we discuss the progress of deep learning in
    state-of-the-art applications, highlighting the key features of the models and
    their problem-solving approaches. Finally, we discuss the challenges faced by
    deep learning and the future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of related works.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Focus | Concepts not covered |'
  prefs: []
  type: TYPE_TB
- en: '| Dong et al. ([2021](#bib.bib51)) | A short review of the fundamentals of
    DL networks and discusses different types of neural networks, DL applications
    and challenges. | Lack of analysis of CNN architectures and limited coverage of
    deep learning fundamentals. |'
  prefs: []
  type: TYPE_TB
- en: '| Talaei Khoei et al. ([2023](#bib.bib243)) | Discusses the learning approaches
    (supervised, unsupervised and reinforcement learnings), learning strategies, and
    DL challenges | Lack of fundamentals of deep learning, CNN architectures and DL
    applications. |'
  prefs: []
  type: TYPE_TB
- en: '| Alzubaidi et al. ([2021](#bib.bib11)) | Discusses different types of DL networks,
    CNN fundamentals and architectures, DL challenges and medical imaging applications
    | Limited discussion on DL applications such as natural language processing and
    time series analysis. |'
  prefs: []
  type: TYPE_TB
- en: '| Alom et al. ([2019](#bib.bib7)) | A short review of the fundamentals of neural
    networks and discusses different types of DL networks, CNN architectures and applications.
    | Limited discussion on DL applications and no discussion of DL challenges. |'
  prefs: []
  type: TYPE_TB
- en: '| Pouyanfar et al. ([2018](#bib.bib194)) | Discusses different types of DL
    networks and DL applications and challenges | Lack of analysis of CNN architectures
    and limited coverage of deep learning fundamentals. |'
  prefs: []
  type: TYPE_TB
- en: '| Sarker ([2021](#bib.bib212)) | Discusses different types of DL networks and
    provides a summary of DL applications | Lack of fundamentals of deep learning,
    analysis of CNN architectures and limited discussion on DL applications. |'
  prefs: []
  type: TYPE_TB
- en: 'The remainder of this paper is organized as follows: Section 2 describes the
    fundamentals of deep learning which includes layers and attention mechanisms,
    activation functions, model optimization and loss functions, and regularization
    methods. Section 3 presents the types of deep learning models, including the CNN
    architectures. Section 4 discusses the state-of-the-art applications of deep learning.
    Section 5 discusses the challenges and future directions in the field of deep
    learning. The conclusion is given in Section 6.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Fundamentals of Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section describes the fundamental concepts such as layer types, activation
    functions, training algorithms and regularization methods to provide a comprehensive
    understanding of the underlying principles in advancing the field of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A deep learning model is characterized by having numerous hidden layers. The
    hidden layers are responsible for learning and extracting complex features from
    the input data. A hidden layer is composed of an arbitrary number of neurons which
    serves as the fundamental building block of a neural network as shown in Fig.
    [1](#S2.F1 "Figure 1 ‣ 2.1 Layers ‣ 2 Fundamentals of Deep Learning ‣ A Survey
    on Deep Learning and State-of-the-art Applications"). A neuron consists of an
    arbitrary number of inputs, each associated with a weight, which controls the
    flow of information into the neuron during the forward pass. The flow of information,
    or forward pass, involves the computation of summation of the weighted input,
    followed by the application of a transformation function to the weighted sum.
    Consider a neuron $z_{i}$ at layer $l$, receives an input vector $a_{j}^{(}l-1)$,
    the computation of the neuron is defined as
  prefs: []
  type: TYPE_NORMAL
- en: $z_{i}^{l}=\sum_{j=0}^{d}w_{i,j}^{l-1}\cdot a_{j}^{l-1}$
  prefs: []
  type: TYPE_NORMAL
- en: $a_{i}^{l}=g(z_{i}^{l})$
  prefs: []
  type: TYPE_NORMAL
- en: where $w_{ij}$ is the set of weights connecting the inputs to the neuron and
    g is the transformation function also known as activation function. A hidden layer
    wherein each neuron is connected to all neurons of the previous layer is known
    as fully-connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba076d2ae09f88e15bddd8557ff3e3f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A graphical representation of a neuron.'
  prefs: []
  type: TYPE_NORMAL
- en: The key aspect of deep learning lies in its ability to automatically extract
    hierarchical features from the input data. This automatic feature extraction is
    performed by two specialized layers called convolutional and pooling layers. In
    a convolutional layer, each neuron is connected only to a local region of the
    input data, and the weights are shared across the input data. The weight-sharing
    not only significantly reduces the number of parameters of the neural network,
    but also allows the network to learn the same features across different spatial
    locations in the input LeCun et al. ([2015](#bib.bib124)). Fig. [2](#S2.F2 "Figure
    2 ‣ 2.1 Layers ‣ 2 Fundamentals of Deep Learning ‣ A Survey on Deep Learning and
    State-of-the-art Applications") illustrates a convolutional layer applies $3\times
    3$ filter on two-dimensional image consisting of $9\times 9$ pixels. The layer
    convolves the input data by moving the filter across the whole input pixels, producing
    a set of output values called feature map. The computation of a convolutional
    layer $l$ is defined as
  prefs: []
  type: TYPE_NORMAL
- en: $z_{i,j,d}^{l}=\sum_{m=0}^{k_{1}}\sum_{n=0}^{k_{2}}w_{m,n,d}^{l-1}\cdot a_{i+m,j+n,d}^{l-1}$
  prefs: []
  type: TYPE_NORMAL
- en: $a_{i,j,d}^{l}=g(z_{i,j,d}^{l})$
  prefs: []
  type: TYPE_NORMAL
- en: where $w_{m,n,d}$ is the weight of $k_{1}\times k_{2}$ filter and $a_{i,j,d}^{l-1}$
    is the input pixel.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/be98101642e05eab009e5976e9dacbfb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A neuron is connected to a local region of the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers are commonly applied after successive convolutional layers to
    progressively reduce the spatial dimensions of the feature maps. The spatial reduction
    is performed by computing the summary of the local regions in the feature maps.
    Two common pooling operations are computing the maximum and the average of the
    local regions. This reduces the number of parameters while providing translation-invariant
    features LeCun et al. ([2015](#bib.bib124)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Attention Mechanisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the important concepts in pattern recognition is the ability to attend
    and neglect certain parts of the input data based on their importance. This is
    because not all parts of the input hold equal importance for making the prediction.
    Certain features exhibit a stronger correlation with the output while others are
    less relevant. In convolutional layers, all extracted features are treated uniformly,
    without consideration of the varying degree of the importance of the different
    parts of the input data. This limitation is addressed by the introduction of attention
    mechanism, which can dynamically assign varying levels of significance (weights)
    to the different features. This flexibility enables the deep learning models to
    prioritize the more relevant aspects of the input data, enhancing its ability
    to capture the intricate dependencies for accurate prediction. Given an input
    data $x$, the process of attending to the important components of the input is
    given as
  prefs: []
  type: TYPE_NORMAL
- en: $A=f(g(x),x)$
  prefs: []
  type: TYPE_NORMAL
- en: where $g$ is a composite function that performs a sequence of operations to
    generate the attention or the weights and f applies the generated attention $g(x)$
    on the input $x$, $f=g(x)x$.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the squeeze-and-excitation (SE) attention generates the attention
    through five consecutive operations Hu et al. ([2019](#bib.bib94)). First, the
    input is vectorized using global average pooling. Then the vector is passed to
    two fully-connected layers, where the first one with ReLU activation and the second
    one with sigmoid activation. SE attention was a pioneer in channel attention.
    The attention module assigns varying weights to the channels of the feature maps.
    SE attention suffers from computational cost and the use of global average which
    may cause information loss at the spatial level. Several efforts have been made
    to improve SE attention. GSoP attention performs $1\times 1$ convolution on the
    feature maps to reduce the number of channels, and then computes the pairwise
    channel correlation which is used to generate the weights Gao et al. ([2018](#bib.bib64)).
    ECA attention replaced the fully-connected layers with 1d convolution to reduce
    the number of parameters and the computational cost Wang et al. ([2020a](#bib.bib265)).
  prefs: []
  type: TYPE_NORMAL
- en: Temporal attention is an attention module that focuses on specific time steps
    in a sequence of data such time series and video (sequence of images). In video
    processing such as recognizing human actions, temporal attention is used to focus
    on key frames at different point in time that contains crucial information for
    predicting the ongoing activity. Temporal adaptive module (TAM) is a temporal
    attention that can focus on short-term (local) information and global context
    information of the data Liu et al. ([2021b](#bib.bib151)). The composite function
    consists of local branch for generating attention weights and global branch for
    generating channel-wise adaptive kernel. First, the input feature map is squeezed
    using global average pooling to reduce the computational cost. Subsequently, the
    local branch executes two 1D convolution operations, with the first convolution
    using ReLU, and sigmoid activation for the second to generate the local weights.
    The local weights are then multiplied with the featuremap. Meanwhile, the global
    branch is composed of two fully-connected layers, with the first layer using ReLU
    and second layer employing softmax function to generate the adaptive kernel (weights).
    Self-attention is a form of temporal attention, initially proposed for machine
    translation to enable the deep learning models to attend different words in a
    sequence relative to other words Vaswani et al. ([2023](#bib.bib257)). The attention
    module has become a fundamental building block in various natural language processing
    applications. To generate the attention weights, the input (word embeddings) is
    transformed by linear projection to compute query, key and value. Then, dot product
    between query and key is computed, and the resultant is normalized by the square
    root of the size of the key. Finally, the attention weights are obtained by applying
    the softmax function. Self-attention is the fundamental building block of the
    Transformer architecture, a key deep learning model in natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial attention focuses on specific regions or spatial location of the input
    data, enabling the deep learning models to selectively emphasize and ignore certain
    features. In the context of computer vision, spatial attention is crucial in capturing
    the spatial relationships and context within an image for accurate prediction.
    Attention gate is a spatial attention that can identify and focus the salient
    regions and suppress feature responses of the insignificant ones. The composite
    function consists of ReLU activation followed by $1\times 1$ convolution to reduce
    channel dimension of the feature maps to a singular feature map. Finally, sigmoid
    is applied to the feature map to generate attention weights Oktay et al. ([2018](#bib.bib184)).
    The self-attention in the standard Transformer is not effective in handling image
    data due to its inherent sequential processing nature and lacks the ability to
    capture spatial dependencies and local patterns. To address this limitation, the
    Vision Transformer (ViT) treats images as a sequence of non-overlapping patches.
    A similar computational pipeline is used to generate the attention weights, the
    sequence of patches is transformed by linear projection to compute the query,
    key and value Dosovitskiy et al. ([2021](#bib.bib52)). The same operations are
    employed to generate the attention weights. Self-attention is computationally
    costly due to its quadratic complexity especially dealing with image data. To
    reduce the complexity, two learnable linear layers, independent of the input data
    are adopted as the key and value vectors Guo et al. ([2023](#bib.bib74)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Activation Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The role of the activation function is to transform the weighted sum into a
    more classifiable form. This is crucial to the learning behaviour of the deep
    learning model, generating non-linear relationships between the input and the
    output of the model. The activation function, combined with many hidden layers
    allow the neural network to approximate highly complex, non-linear functions.
    Many activation functions are available for use in neural networks, and some of
    the functions are shown in Fig. [3](#S2.F3 "Figure 3 ‣ 2.3 Activation Functions
    ‣ 2 Fundamentals of Deep Learning ‣ A Survey on Deep Learning and State-of-the-art
    Applications") - Fig [5](#S2.F5 "Figure 5 ‣ 2.3 Activation Functions ‣ 2 Fundamentals
    of Deep Learning ‣ A Survey on Deep Learning and State-of-the-art Applications").
    The figures show the plot of the three popular activation functions. The sigmoid
    is a classic example of activation function which is used in logistic regression.
    Sigmoid activation function maps the weighted sum to a value in the range of 0
    and 1 which can be used for classification. Hyperbolic tangent is another popular
    choice of bounded activation function which produces an output between -1 and
    1\. Hyperbolic tangent has a stronger gradient, hence the neural network training
    often converges faster than sigmoid LeCun et al. ([2012](#bib.bib126)). For many
    years, sigmoid and hyperbolic tangent are the commonly used activation functions.
    Nevertheless, the activation functions suffer from vanishing gradient problem,
    hindering the efficient training of deep neural networks with many layers Hochreiter
    et al. ([2001](#bib.bib90)).
  prefs: []
  type: TYPE_NORMAL
- en: It was shown that neural networks with unbounded activation functions have the
    universal approximation property and reduce the vanishing gradient problem. Over
    the past years, numerous unbounded activation functions were proposed for neural
    networks, with softplus Dugas et al. ([2000](#bib.bib55)) and rectified linear
    unit (ReLU) Glorot et al. ([2011](#bib.bib70)) activation function being the notable
    examples. These activation functions especially ReLU has pivotal role in improving
    the training and performance of deep learning models. ReLU has been a cornerstone
    of deep learning models due to its computational efficiency and effectiveness
    in addressing the vanishing gradient problem. Since then, several variants of
    ReLU were proposed including Leaky ReLU Maas et al. ([2013](#bib.bib163)), sigmoid
    linear unit Misra ([2020](#bib.bib167)) and exponential linear unit Clevert et al.
    ([2016](#bib.bib39)), each offering unique advantages for building deep learning-based
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dfe21bdcbfe2f5118124d07fd7101f09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Sigmoid activation function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dfe21bdcbfe2f5118124d07fd7101f09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Hyperbolic tangent activation function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dfe21bdcbfe2f5118124d07fd7101f09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Rectified linear unit activation function.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Parameter Learning and Loss Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The weights (parameters) of deep learning models are optimized using an optimization
    algorithm called gradient descent. However, it has to be noted that gradient descent
    is a generic algorithm which can be used to solve a wide range of optimization
    problems. In general, gradient descent finds the optimal weights by iteratively
    updating the weights such that the weights will result in a minimum prediction
    error over all instances in the training set. The prediction error is quantified
    by a loss function. For classification problems, the commonly used loss function
    is the negative log-likelihood loss or cross entropy loss, while the square loss
    and absolute loss are used for regression problems Wang et al. ([2022](#bib.bib264)).
    The weight update is defined as
  prefs: []
  type: TYPE_NORMAL
- en: $w_{i,j}^{l}=w_{i,j}^{l}-\alpha\nabla_{w_{i,j}}$
  prefs: []
  type: TYPE_NORMAL
- en: where $\alpha$ is a hyperparameter called learning rate and $\nabla_{w_{i,j}}$
    is the gradient or the derivative of the loss function $J$ with respect to the
    weight $\frac{\partial J}{\partial w_{i,j}^{l}}$.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient can be computed across all instances of the training set, an approach
    known as batch gradient descent. However, this approach may not guarantee a convergence
    to the optimal solution as the gradient is the same for every weight update. An
    alternative approach is to perform the weight update on the basis of a single
    instance, but the approach results in a noisy gradient and becomes computationally
    intensive due to the frequent weight update. A more commonly used approach is
    to perform the weight update over a set of training instances, known as mini-batch
    gradient descent. This approach strikes a balance, providing a less noisy gradient
    and a more stable training process.
  prefs: []
  type: TYPE_NORMAL
- en: Several efforts have been made to improve the efficiency of gradient descent.
    One of the earlier efforts is the inclusion of past rate of change in the weight
    update to speed up the training of deep learning models, the algorithm is called
    gradient descent with momentum Qian ([1999](#bib.bib198)). Another effort is to
    improve the training convergence by adapting the learning rate based on the occurrence
    of the features Duchi et al. ([2011](#bib.bib54)). A more recent work utilizes
    both adaptive learning rate and momentum to improve the training efficiency and
    convergence of deep learning models Kingma and Ba ([2017](#bib.bib118)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Regularization Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regularization methods are employed to prevent overfitting in deep learning
    models and improve their generalization performance. Early stopping is a method
    that can detect the onset of overfitting during training by continuously monitoring
    the validation error. The model is considered overfitting if the validation error
    starts to increase at some point of the training while the training error is decreasing.
    However, detecting the onset of overfitting during the training of deep learning
    models is challenging due to the inherent stochasticity and the presence of noisy
    data. Several stopping criteria can be considered such as using a threshold to
    check if the decrease of (average) validation error is significant and count the
    number of successive increases of validation error Prechelt ([2012](#bib.bib197)).
  prefs: []
  type: TYPE_NORMAL
- en: Dropout is a regularization method that randomly switching off some neurons
    in the hidden layers during training with a predefined drop probability (dropout
    rate) Srivastava et al. ([2014](#bib.bib230)). Dropout has the effect of training
    and evaluating exponentially many different deep learning models. The dropout
    rate is a hyperparameter that needs to be carefully tuned to balance regularization
    and model capacity. Different ranges of dropout rate have been suggested. The
    original author suggested a dropout rate between 0.5 and 0.8 Srivastava et al.
    ([2014](#bib.bib230)) while others recommended a lower dropout rate between 0.1
    and 0.3 Park and Kwak ([2017](#bib.bib190)). Also, it has been suggested a low
    dropout rate due to the exponential increase in the volume of training data Liu
    et al. ([2023b](#bib.bib152)).
  prefs: []
  type: TYPE_NORMAL
- en: Parameter norm penalty is a regularization method that adds a penalty term consisting
    of the network’s weights to the loss function. During the training, the penalty
    term discourages large weight values and hence, constraining the model’s capacity
    and reducing the chance of overfitting. The common penalty terms are $L^{1}$ norm
    penalty Tibshirani ([1996](#bib.bib250)), $L^{2}$ norm penalty, also known as
    weight decay and a combination of $L^{1}$ and $L^{2}$ Zou and Hastie ([2005](#bib.bib308)).
    An adaptive weight decay is proposed allowing the regularization strength for
    each weight to be dynamically adjusted Nakamura and Hong ([2019](#bib.bib176)).
  prefs: []
  type: TYPE_NORMAL
- en: Despite the advantages of the mini-batch gradient descent, each mini-batch may
    comprise data from different distributions. Furthermore, the data distribution
    may change after each weight update, which could slow down the training process.
    Batch normalization overcomes this issue by normalizing the summed input to a
    neuron over a mini batch of training instances Ioffe and Szegedy ([2015](#bib.bib104)).
    An alternative method is to perform normalization across the neurons instead of
    the mini batch, a method known as layer normalization Ba et al. ([2016](#bib.bib15)).
    Layer normalization is applicable in recurrent neural network and overcomes the
    dependencies on the mini batch size.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Types of Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning models can be categorized into deep supervised learning and deep
    unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Deep Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep supervised models are trained with a labelled dataset. The learning process
    of these models involve calculating the prediction error through a loss function
    and utilizing the error to adjust the weights iteratively until the prediction
    error is minimized. Among the deep supervised models, three important models are
    identified namely multilayer perceptron, convolutional neural network and recurrent
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer perceptron is a neural network model with one or more hidden fully-connected
    layers stacked between the input and output layers as shown in Fig. [6](#S3.F6
    "Figure 6 ‣ 3.1 Deep Supervised Learning ‣ 3 Types of Deep Learning ‣ A Survey
    on Deep Learning and State-of-the-art Applications"). The width (number of neurons)
    of the hidden layers and the depth (number of layers) of the network influence
    the model’s ability to learn patterns in the data. Specifically, the width determines
    the model’s ability to learn complex features while the depth allows the model
    to learn hierarchical representations of the data. Nevertheless, studies showed
    that a multilayer perceptron with a single hidden layer can approximate any continuous
    function Cybenko ([1989](#bib.bib41)), Hornik et al. ([1989](#bib.bib92)). Multilayer
    perceptron is effective in various industries and applications from healthcare
    to finance Widrow et al. ([1994](#bib.bib271)). However, multilayer perceptron
    requires the input data to be structured in a one-dimensional format e.g. tabular
    data, making it less suitable for unstructured data such as image, text and speech.
    To leverage multilayer perceptron for unstructured data, a feature extraction
    or transformation into structured data is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a042be0ee4e3f0b0684a94b2cee22b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: A fully-connected neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Network is a neural network model that leverages the sequential
    information and memory through the use of recurrent connections, allowing it to
    effectively process data such as time series, text, speech and other sequential
    patterns. As shown in Fig. [7](#S3.F7 "Figure 7 ‣ 3.1 Deep Supervised Learning
    ‣ 3 Types of Deep Learning ‣ A Survey on Deep Learning and State-of-the-art Applications"),
    a recurrent neural network is characterized by the recurrent connection which
    enables the network to loop back and use internal state from previous time step
    to the next time step. The internal state is parameterized by a set of weights
    which is shared across the sequence of the data. The training of recurrent neural
    networks suffers from the issue of vanishing gradient due to the challenges of
    propagation of gradients over a long sequence of data. Variants of recurrent neural
    networks are introduced to overcome the problem of vanishing gradient such as
    long short-term memory Hochreiter and Schmidhuber ([1997](#bib.bib91)) and gated
    recurrent memory Cho et al. ([2014](#bib.bib38)). The improved recurrent neural
    networks introduce memory cell and gating mechanisms to retain and discard information
    in every time step, allowing for more effective learning dependencies in long
    sequence. The network architecture can be built using fully-connected and convolutional
    layers Shi et al. ([2015](#bib.bib221)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/30f2be459d35aa7d20777db55b1b8a30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A neural network with recurrent connection.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network (CNN) is a neural network model that preserves
    and leverages the spatial local information in the data through the use of convolutional
    layers. Fig. [8](#S3.F8 "Figure 8 ‣ 3.1 Deep Supervised Learning ‣ 3 Types of
    Deep Learning ‣ A Survey on Deep Learning and State-of-the-art Applications")
    shows a typical architecture of convolutional neural network which comprises of
    convolutional, pooling and fully-connected layers. The convolutional and pooling
    layers are stacked alternately to automatically extract salient features in a
    hierarchical manner. The extracted features are then fed to fully-connected layers
    to predict the outputs. The final feature maps need to be converted to one-dimensional
    vector before they are fed to the fully-connected layers. The conversion can be
    performed by flattening the feature maps. CNN architecture is crucial in increasing
    the performance of the prediction, as it is designed to efficiently extract the
    feature representation of the input data, enabling more accurate and robust pattern
    recognition. Over the last decade, several CNN architectures have been proposed,
    whereby the focus of the improvements has been on enhancing the feature learning
    capabilities and addressing challenges such as vanishing gradient and diminishing
    feature reuse.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6e0c29248edf84679fb5ff09b7016d4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: A neural network with convolutional and pooling layers followed by
    fully-connected layers.'
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet is among the first CNN models that gained widespread recognition and
    success, marking a significant achievement in the field of deep learning for computer
    vision tasks Krizhevsky et al. ([2012](#bib.bib120)). The model consists of five
    convolutional layers with max-pooling operation performed after the first and
    second convolutional layers, followed by three fully-connected layers. The first
    and second convolutional layers utilize a filter size of 11×11 and 5×5 respectively,
    and $3\times 3$ filter size is used for the remaining convolutional layers. ReLU
    activation function is used to mitigate the vanishing gradient.
  prefs: []
  type: TYPE_NORMAL
- en: VGG-16 attempts to improve the CNN architecture by adding more convolutional
    layers, specifically up to 19 layers to capture more intricate feature representation
    from input data, followed by three fully-connected layers Simonyan and Zisserman
    ([2015](#bib.bib224)). ReLU activation function is used to reduce vanishing gradient.
    Unlike AlexNet, all convolutional layers utilize a small fix filter size of $3\times
    3$ and max-pooling layer is added after a stack of two or three convolutional
    layers. This configuration allows the model to extract more discriminative features
    and decreases the number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: ZFNet is a classic CNN model which has a similar architectural principle as
    AlexNet, featuring five convolutional layers with max-pooling layers after the
    first and second convolution, followed by three fully-connected layers Zeiler
    and Fergus ([2013](#bib.bib291)). The significant differences are the use of smaller
    filter size and stride in the convolutional layers and contrast normalization
    of the feature maps which allows the model to capture better features and improving
    the overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Network-in-network introduces two innovative concepts to enhance the performance
    of the model Lin et al. ([2014](#bib.bib139)). The first was introducing a block
    of convolutional layers consisting of k×k convolution followed by two $1\times
    1$ convolution operations. The pointwise convolutions are similar to applying
    multilayer perceptron on the feature maps, allowing the model to approximate more
    abstract feature representation. In the preceding models, the final feature maps
    are vectorized by flattening operation for classification by the fully-connected
    layers. Instead of flattening, network-in-network model calculates the spatial
    average of each feature map, and the resulting vector is fed to softmax function
    for classification. This approach is parameter-less, significantly reducing the
    number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: GoogleNet leverages the fact that visual data can be represented at different
    scales by incorporating a module which consists of multiple convolutional pipelines
    with different filter sizes Szegedy et al. ([2014](#bib.bib239)). The module known
    as inception utilizes three kernel sizes ($5\times 5$, $3\times 3$, $1\times 1$)
    to capture spatial and channel information at different scales of resolution as
    shown in Fig. [9](#S3.F9 "Figure 9 ‣ 3.1 Deep Supervised Learning ‣ 3 Types of
    Deep Learning ‣ A Survey on Deep Learning and State-of-the-art Applications").
    This configuration enables a more effective feature extraction at both fine-grained
    and coarse-grained information from input data. The model architecture utilizes
    the inception module at the higher layers while the traditional convolution and
    max-pooling block is used to extract primitive and basic features. The inception
    modules are stacked upon each other with maximum pooling operation is performed
    occasionally to reduce the spatial resolution of the feature maps. GoogleNet utilizes
    global average pooling to vectorize the final feature maps before passing it to
    a fully-connected layer for classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/66b59193bdf18abc2d17bc9a71d9a666.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The inception module.'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the number of layers enhances the model performance, mainly for solving
    complex tasks. However, training a very deep neural network is challenging due
    to the vanishing gradient problem, where the gradients that are used to update
    the network become insignificant or extremely small as they are backpropagated
    from the output layer to the earlier layers. A model called Highway Network overcomes
    this issue by introducing a gating mechanism that regulates the information flow
    of the layers, enabling the flow of information from the earlier layers to the
    later layers Srivastava et al. ([2015](#bib.bib232)). Consequently, this not only
    mitigates the vanishing gradient problem, but also renders the gradient-based
    training more tractable, enabling the training of very deep neural networks consisting
    as many as 100 layers.
  prefs: []
  type: TYPE_NORMAL
- en: The gating mechanism of Highway Network increases the number of parameters for
    regulating the information flow. ResNet is a CNN architecture that incorporates
    residual (skip) connection that allows information to bypass certain layers, mitigating
    the vanishing gradient problem He et al. ([2015](#bib.bib81)). ResNet architecture
    stacks residual blocks, which consists of two or three of convolutional layers
    with batch normalization and ReLU, and a skip connection which adds the input
    to the output of the final convolutional layer as shown in Fig. [10](#S3.F10 "Figure
    10 ‣ 3.1 Deep Supervised Learning ‣ 3 Types of Deep Learning ‣ A Survey on Deep
    Learning and State-of-the-art Applications"). If the input dimension does not
    match with the residual output dimension, a linear projection is performed by
    the residual connection to match the dimensions. In comparison to the gating mechanism
    of Highway Network, the residual connection is parameter-free, and thus does not
    incur additional computational costs. Furthermore, the connections are never closed
    whereby all information is always passed through the layers. This innovative concept
    enables the training of very deep neural networks boasting as many as 152 layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/75f41ba3ade1e268df8e1ce6607cfb6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: A residual connection.'
  prefs: []
  type: TYPE_NORMAL
- en: DenseNet is another CNN architecture that overcomes the vanishing gradient problem.
    DenseNet follows the same approach as ResNet and Highway Network, utilizing skip
    connection to allow information flow from the earlier layers to later layers.
    However, DenseNet takes this concept one step further, by introducing a dense
    block consisting of multiple convolution functions (layers) with each convolution
    function performs batch normalization followed by ReLU and $3\times 3$ convolution.
    Each convolutional layer in the dense block receives feature maps from all its
    preceding layers, hence the connection is referred to as dense connection Huang
    et al. ([2017](#bib.bib95)). This configuration as shown in Fig. [11](#S3.F11
    "Figure 11 ‣ 3.1 Deep Supervised Learning ‣ 3 Types of Deep Learning ‣ A Survey
    on Deep Learning and State-of-the-art Applications") maximizes information flow
    and preserves the feed-forward nature of the network. Dense blocks can become
    computationally expensive due to the increasing number of feature maps. To reduce
    the computational costs, a block of $1\times 1$ convolutional with batch normalization
    and max-pooling layers known as transition block is used to reduce the spatial
    dimension of the feature maps. The model architecture integrates these dense and
    transition blocks, stacking them alternately. The network depth can reach up to
    264 layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5051d4550d55e900302fa06badeade75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: A dense block.'
  prefs: []
  type: TYPE_NORMAL
- en: Although skip connections in ResNet effectively mitigate the vanishing gradient
    problem, a new challenge arises in the form of diminishing feature reuse as the
    network becomes deeper. Diminishing feature reuse refers to the diminishing effectiveness
    of the previously learned feature maps in subsequent layers, impacting the final
    prediction. WideResNet is a CNN architecture that is based on ResNet with the
    aim to mitigate diminishing feature reuse problem. Instead of making the network
    deeper, WideResNet makes the network wider by increasing the number of channels
    by k factor Zagoruyko and Komodakis ([2017](#bib.bib290)). The increased width
    allows the model to capture a more diverse features, enhancing its ability to
    learn complex relationships in the input data.
  prefs: []
  type: TYPE_NORMAL
- en: ResNext addresses the diminishing feature reuse by capturing more efficient
    and diverse features of the input data. ResNext introduces a concept of cardinality
    which is loosely based on the inception module as shown in Fig. [12](#S3.F12 "Figure
    12 ‣ 3.1 Deep Supervised Learning ‣ 3 Types of Deep Learning ‣ A Survey on Deep
    Learning and State-of-the-art Applications"). The cardinality refers to the number
    of independent and identical paths, where each path performs transformation of
    the input data, divided along the channel dimension Xie et al. ([2017](#bib.bib280)).
    In other words, instead of solely relying on increasing the depth of the model,
    ResNext enhances the feature learning by parallelizing the feature extraction
    through this cardinal path. In the proposed architecture, each path configuration
    is similar to the residual block of ResNet. The output from each path is then
    aggregated to form a comprehensive and diverse representation of the input data.
    The skip connection is used to mitigate the vanishing gradient problem. WideResNet
    is a CNN architecture that is based on ResNet with the aim to mitigate diminishing
    feature reuse problem. Instead of making the network deeper, WideResNet makes
    the network wider by increasing the number of channels by $k$ factor Zagoruyko
    and Komodakis ([2017](#bib.bib290)). The increased width allows the model to capture
    a more diverse features, enhancing its ability to learn complex relationships
    in the input data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65eb220f120e4fb290dd4a605d14aa1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: A cardinal block.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Deep Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep unsupervised models are trained with an unlabelled dataset. The learning
    process of these models involve calculating the prediction error through a loss
    function and utilizing the error to adjust the weights iteratively until the prediction
    error is minimized. Among the deep supervised models, three important models are
    identified namely multilayer perceptron, convolutional neural network and recurrent
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann Machine is a generative neural network model that learns
    a probability distribution based on a set of inputs. The model consists of a visible
    (input) layer and a hidden layer with symmetrically weighted connections as shown
    in Fig. [13](#S3.F13 "Figure 13 ‣ 3.2 Deep Unsupervised Learning ‣ 3 Types of
    Deep Learning ‣ A Survey on Deep Learning and State-of-the-art Applications").
    The input layer represents the input data with each node corresponding to a feature
    or variable while the hidden layer learns the abstract representation of the input
    data. Restricted Boltzmann machine model is trained using contrastive divergence,
    an algorithm that is based on a modified form of gradient descent, utilizing a
    sampling-based approach to estimate the gradient Hinton ([2012](#bib.bib88)).
    It has found success in solving combinative problems such as dimensionality reduction,
    collaborative filtering and topic modelling.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6629de2d6c8cd6240487a6029f1e81a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: A restricted Boltzmann machine.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep Belief Network can be viewed as a stack of restricted Boltzmann machines,
    comprising a visible layer and multiple hidden layers Hinton et al. ([2006](#bib.bib89))
    as shown in Fig. [14](#S3.F14 "Figure 14 ‣ 3.2 Deep Unsupervised Learning ‣ 3
    Types of Deep Learning ‣ A Survey on Deep Learning and State-of-the-art Applications").
    Deep belief network has two training phases. The initial phase is known as pretraining
    in which the network is trained layer by layer, with each layer serves as a pretraining
    layer of the subsequent layers. This sequential learning allows the hidden layers
    learn complex hierarchical feature representation of the data. The second phase
    is called fine-tuning whereby the deep belief network model can be further trained
    with supervision to perform tasks such as classification and regression Hinton
    ([2009](#bib.bib87)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c0cb26ae6c79e859b470d62eccef4878.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: A deep belief network.'
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoder is a generative neural network model that learns to encode the input
    data into a compressed representation and then reconstructs the original data
    from this representation. The layers that encode the input data is known as encoder
    while the layers that responsible for the reconstruction is referred to as the
    decoder as shown in Fig. [15](#S3.F15 "Figure 15 ‣ 3.2 Deep Unsupervised Learning
    ‣ 3 Types of Deep Learning ‣ A Survey on Deep Learning and State-of-the-art Applications").
    The encoded data (hidden layer) represents the abstract features of the input
    data also known as latent space or encoding. The decoder can be removed from the
    autoencoder, creating a standalone model that can be used for data compression
    and dimensionality reduction Romero et al. ([2017](#bib.bib209)), Li et al. ([2020c](#bib.bib136)).
    The decoder can also be replaced with predictive layers for classification task
    Mohd Noor ([2021](#bib.bib169)). The network architecture can be built using fully-connected
    and convolutional layers Li et al. ([2023](#bib.bib133)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/16acf73e7c726db9351054489dca6116.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: An autoencoder.'
  prefs: []
  type: TYPE_NORMAL
- en: Several autoencoder variants have been introduced to improve the autoencoder’s
    ability to capture better feature representation. Some introduced penalty terms
    to the loss function such as sparsity penalty (sparse autoencoder) Ng and others
    ([2011](#bib.bib179)) to encourage sparse representation and Jacobian Frobenius
    norm (contractive autoencoder) Rifai et al. ([2011](#bib.bib206)) to be less sensitive
    to small and insignificant variations in the input data while encoding the feature
    representation. Others trained the autoencoder to recover original data from corrupted
    data with noise Vincent et al. ([2008](#bib.bib258)). An improved denoising autoencoder
    knowns marginalized denoising autoencoder has been proposed which marginalizes
    the noise by adding a term that is linked to the encoding layer Chen et al. ([2012](#bib.bib35)).
    Variational autoencoder is a variant of autoencoder that has similar architecture
    i.e. encoder, latent space and decoder. Despite the similarity, instead of learning
    a fixed encoding, variational autoencoder learns the probability distribution
    of the input data in the latent space Kingma and Welling ([2013](#bib.bib119)).
    The model can be used to generate data by sampling from the learned probability
    distribution. The network architecture can be built by stacking more than one
    fully-connected layer and convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Network (GAN) is another generative neural network model
    that is designed for generating data that adheres closely to the distribution
    of the original training set. The model consists of two different neural networks
    namely generator and discriminator as shown in Fig. [16](#S3.F16 "Figure 16 ‣
    3.2 Deep Unsupervised Learning ‣ 3 Types of Deep Learning ‣ A Survey on Deep Learning
    and State-of-the-art Applications"). The generator learns to imitate the distribution
    of the training set given a noise vector, effectively outsmarting the discriminator.
    Simultaneously during the training, the discriminator is trained to differentiate
    between the real data from the training set and synthetic data generated by the
    generator Goodfellow et al. ([2014](#bib.bib71)). This intricate dynamic between
    the networks drives an iterative learning process whereby the generator continually
    refines its ability to create synthetic data that closely resembles the real data
    while the discriminator enhances its ability to distinguish between authentic
    and fake data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/157d32ed19ececca5317c41965b59d95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: A generative adversarial network.'
  prefs: []
  type: TYPE_NORMAL
- en: The model can be extended by providing the labels to both generator and discriminator
    in which the model known as conditional GAN, capable of generating 1000 image
    classes Odena et al. ([2017](#bib.bib183)). Conditional GANs require a labelled
    dataset which might limit its application. InfoGAN is similar to conditional GAN,
    but the labels are substituted with latent codes, which allows the model to be
    trained in an unsupervised manner Chen et al. ([2016](#bib.bib36)). GANs often
    suffer from mode collapse whereby the model can only generate a single or small
    set of outputs. Wasserstein GAN improves the training by utilizing Wasserstein
    loss function which measures the difference between the real and synthesized data
    distribution Weng ([2019](#bib.bib270)). ProGAN tackles the training instability
    of GAN by progressively growing the generator and discriminator. The idea is that
    the model is scaled up gradually, starting with the simplest form of the problem
    and little by little the problem’s complexity is increased as the training progresses
    Karras et al. ([2018](#bib.bib112)). StyleGAN leverages the progressive GAN’s
    approach and neural style transfer to improve quality of the generated data Karras
    et al. ([2019](#bib.bib113)). The model is characterized by the independent manipulation
    of both style and content, allowing it to generate diverse styles and high quality
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Applications of Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in the previous section, the application of deep learning ranges
    from computer vision Tan et al. ([2020a](#bib.bib245)), natural language processing
    Otter et al. ([2021](#bib.bib185)), healthcare Esteva et al. ([2019](#bib.bib59)),
    robotics Soori et al. ([2023](#bib.bib228)), education Hernández-Blanco et al.
    ([2019](#bib.bib86)), and many others. This section presents the applications
    of deep learning across several areas.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Computer Vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Computer vision is an essential field in artificial intelligence. It is a field
    of study that focuses on enabling computers to acquire, analyze and interpret
    visual inputs to derive meaningful information. The visual inputs can take many
    forms such as digital images, sequence of images or video and point cloud, and
    the source of these inputs can be camera, LiDaR, medical scanning machine etc.
    Deep learning, specifically CNN models have been widely used in real-world computer
    vision applications including image classification, object detection and image
    segmentation. This section discusses more details about the recent advancements
    in deep learning models that have been achieved over the past few years.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Image Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Image classification is a fundamental task in computer vision which involves
    categorizing an image into one of predefined classes based on the visual content.
    The objective of image classification is to enable computers or machines to differentiate
    between objects within images, in a manner similar to how humans interpret visual
    information. Image classification is a crucial component in various applications
    such as robotics, manufacturing and healthcare. LeNet-5, introduced in 1998, is
    one of the earliest convolutional neural networks that was successfully trained
    to classify handwritten digits. The model underwent a series of improvements,
    including the use of tanh and average pooling which enhanced its ability to extract
    hierarchical features, ultimately improving overall performance. The model architecture
    comprises two convolutional layers, each with an average pooling layer, followed
    by two fully-connected layers, including the output layer Lecun et al. ([1998](#bib.bib125)).
    Since then, numerous CNN models have been proposed based on LeNet-5 for image
    classification Simard et al. ([2003](#bib.bib223)); Matsugu et al. ([2003](#bib.bib165))
    but the most significant one is AlexNet in 2012 which saw a transformative breakthrough
    in deep learning. AlexNet is considered the first CNN model with a large number
    of parameters that significantly improved the performance of image classification
    on a very large dataset (ImageNet). The model won first place in ILSVRC 2012,
    improving the test error from the previous year by almost 10% Krizhevsky et al.
    ([2012](#bib.bib120)). Numerous significant CNN models have been introduced in
    subsequent ILSVRC competitions including ZFNet, VGG16, GoogleNet, ResNet and ResNext.
    In general, the research focused on increasing the number of layers, addressing
    the problem of vanishing gradient and diminishing of feature reuse.
  prefs: []
  type: TYPE_NORMAL
- en: Research in image classification continues to evolve with a focus on addressing
    key challenges to improve the classification performance. One notable trend is
    the formulation of the loss function to address problems such as neglecting well-classified
    instances and imbalance distribution of class labels. In a particular study, an
    additive term is introduced to the cross-entropy loss to reward the models for
    the correctly classified instances. This formulation encourages the models to
    also pay attention to well-classified instances while focusing on the bad-classified
    ones Zhao et al. ([2022](#bib.bib300)). Another study proposes an asymmetric polynomial
    loss function using the Taylor series expansion. The loss function allows the
    training to selectively prioritize contributions of positive instances to mitigate
    the issue of imbalance between negative and positive classes Huang et al. ([2023b](#bib.bib98)).
    The asymmetric polynomial loss requires a large number of parameters to be fine-tuned
    and may lead to overfitting. A robust asymmetric loss is formulated by introducing
    a multiplicative term to control the contribution of the negative gradient and
    making it less sensitive to parameter optimization Park et al. ([2023](#bib.bib191)).
    Combining multiple deep learning models improves the overall performance by leveraging
    the diverse strengths of individual models. However, identifying the optimal combination
    is non-trivial due to the large number of hyperparameters. A straightforward method
    is to employ the weighted sum rule Nanni et al. ([2023](#bib.bib177)). To enhance
    the overall performance, an algorithm, named greedy soups, adds a model based
    on the validation accuracy Wortsman et al. ([2022](#bib.bib272)). The final prediction
    is produced via averaging. Multi-symmetry ensembles framework improves the building
    of diverse deep learning models by utilizing contrastive learning Loh et al. ([2023](#bib.bib154)).
    Then, the diverse models are sequentially combined based on their validation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vision transformers (ViT) offers an alternative to convolutional neural networks
    that have long been the dominant architecture for image classification, by leveraging
    self-attention mechanisms for scalable representation learning. Despite its effectiveness,
    ViT is sensitive to hyperparameter optimization and substandard performance on
    smaller datasets Xiao et al. ([2021](#bib.bib278)). Furthermore, ViT lacks the
    ability to leverage local spatial features which is inherent in convolutional
    neural networks Wu et al. ([2021](#bib.bib273)). Therefore, several studies attempt
    to incorporate convolutional layers into ViT architecture to improve its performance
    and robustness. In particular, conformer is a network architecture with two branches:
    CNN branch and a transformer branch to extract local and global features respectively
    Peng et al. ([2023](#bib.bib192)). Both branches are connected by two “bridges”
    of $1\times 1$ convolution and up or down sampling operations, allowing the branches
    to share their features and enhance the feature representation. Both branches
    output predictions which are combined to produce the final prediction. A hybrid
    architecture, named MaxViT, combines convolutional networks and vision transformer
    to address the lack of scalability issues of self-attention mechanisms when the
    model is trained on large input size Tu et al. ([2022](#bib.bib255)). The improved
    vision transformer is composed of two modules whereby the first module attends
    local features in non-overlapping image patches and the global features are attended
    by processing a grid of sparse and uniform pixels. The transformer is stacked
    with a block of convolutional layers to extract local spatial features. The architecture
    of MaxViT is shown in Figure [17](#S4.F17 "Figure 17 ‣ 4.1.1 Image Classification
    ‣ 4.1 Computer Vision ‣ 4 Applications of Deep Learning ‣ A Survey on Deep Learning
    and State-of-the-art Applications"). Another study proposes a convolutional transformer
    network, introducing the depthwise convolutional block into the ViT Ma et al.
    ([2024](#bib.bib162)). This configuration allows the model to exploit the ability
    of convolutional networks to extract local spatial features while the ViT attends
    the extracted local features to focus on relevant information, enhancing the model’s
    ability to capture complex patterns and relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/923f6720e106a765965b953111a0aa59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The architecture of MaxViT Tu et al. ([2022](#bib.bib255)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Object Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deep learning plays a major role in significantly advancing the state-of-the-art
    in object detection performance. Region-based CNN (R-CNN) is the first breakthrough
    in object detection that combines CNN with selective region proposals Girshick
    et al. ([2014](#bib.bib69)). The region proposals are the candidate bounding boxes
    serving as the potential region of interests (objects) within the input image,
    and the CNN are used to extract features from the region proposals and classify
    the regions for object detection. An improved model, named Fast R-CNN introduces
    two prediction branches: object classification and bounding box regression which
    improves the overall performance of object detection Girshick ([2015](#bib.bib68)).
    However, R-CNN and Fast R-CNN models are computationally expensive and slow, thus
    practically infeasible for real-time applications. Addressing this issue, Fast
    R-CNN is integrated with a region proposal network, referred to as Faster R-CNN
    Ren et al. ([2015](#bib.bib205)). The region proposal network (RPN) is used to
    efficiently generate region proposals for object detection. RPN takes an input
    image and output a set of rectangle object proposals, each with a confidence score
    to indicate the likelihood of an object’s presence. To this end, RPN introduces
    the concept of anchor boxes whereby multiple bounding boxes of different aspect
    ratios are defined over the feature maps produced by the convolutional networks.
    These anchor boxes are then regressed over the feature maps to localize the objects,
    contributing to the improved speed and effectiveness of Faster R-CNN. The training
    of Faster R-CNN is divided into two stages. First the RPN is pre-trained to generate
    the region proposals and then, the Fast R-CNN is trained using the region proposals
    generated by the RPN for object detection. The backbone network responsible for
    extracting the features for Faster R-CNN is either ZFNet or VGG16.'
  prefs: []
  type: TYPE_NORMAL
- en: In two-stage object detectors, the region proposals are generated first, and
    then used for object detection. The two-stage process is computationally intensive
    and infeasible for real-time object detection applications. You Only Look Once
    or YOLO proposes a one-stage detection by directly predicting bounding boxes and
    object’s confidence score in a single forward pass through the neural network
    Redmon et al. ([2016](#bib.bib203)). This single pass architecture significantly
    reduces the computational complexity, making YOLO suitable for real-time object
    detection applications. In YOLO, the input image is divided into S×S grids, each
    grid cell is responsible for detecting the objects present in the cell. Specifically,
    each grid cell predicts multiple bounding boxes and associated object’s confidence
    score, enabling simultaneous object detection across the entire image. Subsequent
    enhancements such as YOLOv3 Redmon and Farhadi ([2018](#bib.bib204)) and YOLOv4
    Bochkovskiy et al. ([2020](#bib.bib21)) are proposed, improving the model’s capability
    and accuracy. Single Shot Multibox Detector (SSD) is another one-stage detector,
    aims to address the issue of real-time object detection Liu et al. ([2016](#bib.bib147)).
    SSD also eliminates the region proposal generation and directly predicts bounding
    boxes and confidence scores, reducing the computational complexity. To improve
    the overall performance, SSD produces the predictions from different levels of
    feature maps, allowing detection of objects of different sizes in the input image.
  prefs: []
  type: TYPE_NORMAL
- en: A common issue in object detection problems is the extremely imbalanced ratio
    of foreground to background classes. Addressing this issue, RetinaNet introduces
    a loss function that is based on the cross entropy called focal loss. Focal loss
    reduces the loss contribution of easily classified objects, allowing the model
    training to focus on the difficult objects Lin et al. ([2020](#bib.bib141)). RetinaNet
    adopts the Feature Pyramid Network (FPN) Lin et al. ([2017a](#bib.bib140)) with
    ResNet as the backbone network for extracting the feature maps. FPN is a network
    architecture with a pyramid structure that efficiently captures multiscale feature
    representation, facilitating object detection across various sizes. To further
    improve the overall performance, EfficientDet introduces bi-directional FPN which
    incorporates multi-level feature fusion to better capture multiscale feature representation
    Tan et al. ([2020b](#bib.bib246)). Also, the model utilizes EfficientNet Tan and
    Le ([2019](#bib.bib244)) as the backbone network to achieve a balance between
    computational efficiency and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection performance relies on a post-processing step called non-maximum
    suppression (NMS) to eliminate duplicate detections and select the most relevant
    bounding boxes. Specifically, NMS sorts all detection boxes based on their confidence
    scores, selects a box with the maximum score and discards the other boxes with
    a significant overlap with the selected box. This process is repeated on the remaining
    detection boxes. However, due to the inconsistency between the confidence score
    and the quality of object localization, NMS retains poorly localized bounding
    boxes with high confidence score while discarding more accurate predictions with
    poor confidence score. To mitigate this limitation, instead of discarding the
    neighboring boxes with significant overlap, soft-NMS applies Gaussian function
    to lower their confidence scores Bodla et al. ([2017](#bib.bib22)). The idea is
    not to discard the neighboring bounding boxes, but gradually decline their scores
    based on the extend of the overlap with the selected box. This results in a smoother
    suppression, preserving the better-localized bounding boxes. Adaptive NMS introduces
    an adaptive threshold for the suppression of bounding boxes Liu et al. ([2019a](#bib.bib145)).
    The algorithm dynamically adjusts the threshold based on the level of overlapping
    of the selected box with the other bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Detection Transformer (DeTR) is an end-to-end trainable object detection model
    that leverages the transformer architecture to eliminates the need for handcrafted
    components such as anchor boxes and non-maximum suppression Carion et al. ([2020](#bib.bib28)).
    The self-attention mechanism of the transformer captures the global context and
    relationships between different parts of the image, allowing it to localize the
    objects and remove duplicate predictions. The model is trained with a set of loss
    functions that perform bipartite matching between the predicted and ground truth
    objects. DeTR uses ResNet as backbone network. Despite the success of DeTR in
    simplifying and improving object detection tasks, DeTR suffers from a long training
    time and low performance at detecting small objects due to its reliance on self-attention
    mechanism of the transformer, which lacks a multiscale feature representation.
    To mitigate this limitation, Deformable DeTR introduces a multiscale deformable
    attention module which can effectively capture feature representation at different
    scales Zhu et al. ([2020](#bib.bib305)). Furthermore, the attention module leverages
    deformable convolution, allowing the model to adapt to spatial variation and capture
    more informative features in the input data. Dynamic DeTR addresses the same issues
    by utilizing a deformable convolution-based FPN to learn multiscale feature representation
    Dai et al. ([2021](#bib.bib43)). Moreover, the model replaces the transformer
    encoder with a convolution-based encoder to attend to various spatial features
    and channels. This modification allows the model to effectively detect small objects
    and converge faster during training. The architecture of dynamic DeTR is shown
    in Figure [18](#S4.F18 "Figure 18 ‣ 4.1.2 Object Detection ‣ 4.1 Computer Vision
    ‣ 4 Applications of Deep Learning ‣ A Survey on Deep Learning and State-of-the-art
    Applications"). A training scheme known as Teach-DeTR is proposed to improve the
    overall performance of DeTR Huang et al. ([2023a](#bib.bib97)). The training scheme
    leverages the predicted bounding boxes by other object detection models during
    the training by calculating the loss of one-to-one matching between the object
    queries and the predicted boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/08af40c870cb092d40d615ddccd75cab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: The architecture of dynamic DeTR Dai et al. ([2021](#bib.bib43)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Image Segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Image segmentation is another important task in which deep learning has a significant
    impact. One of the earliest deep learning models for image segmentation is the
    fully convolutional network Long et al. ([2015](#bib.bib155)). A fully convolutional
    network consists of only convolutional layers which accepts an input of an arbitrary
    size and produce the predicted segmentation map of the same size. The authors
    adopt the AlexNet, VGG16 and GoogleNet, replace their fully connected layers with
    convolutional layers and append a 1×1 convolutional layer, followed by bilinear
    up-sampling to match the size of the input. The model was considered a significant
    milestone in image segmentation, demonstrating the feasibility of deep learning
    for semantic segmentation trained in end-to-end manner. Deconvolution network
    is another popular deep learning model for semantic segmentation Noh et al. ([2015](#bib.bib182)).
    The model architecture consists of two parts: encoder and decoder. The encoder
    takes an input image and uses the convolutional layers to generate the feature
    maps. The feature maps are fed to the decoder composed of un-sampling and deconvolutional
    layers to predict the segmentation map. SegNet is another encoder-decoder model
    for semantic segmentation Badrinarayanan et al. ([2017](#bib.bib16)). The encoder
    is a sequence of convolutional (with ReLU) and max-pooling blocks which is analogous
    to a convolutional neural network. The decoder is composed of up-sampling layers
    which up-samples the inputs using the memorized pooled indices generated in the
    encoder phase, and convolutional layers without non-linearity. The encoder progressively
    reduces the resolution of the input data while extracting abstract features through
    a series of convolutional and pooling layers. This process causes the loss of
    fine-grained information, degrading the overall performance of segmentation. LinkNet
    mitigates this limitation by passing the feature maps at several stages generated
    by the encoder to the decoder, hence reducing information loss Chaurasia and Culurciello
    ([2017](#bib.bib30)). The model architecture of LinkNet is similar to SegNet,
    but utilizes ResNet as the encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: While Faster R-CNN is a significant approach in object detection task, it has
    been extended to perform instance segmentation task. One such extension is Mask
    R-CNN which is based on Faster R-CNN, introduces an additional branch for predicting
    the segmentation mask He et al. ([2017](#bib.bib80)). Similar to Faster R-CNN,
    Mask R-CNN utilizes the RPN to generate region proposals and then the region of
    interest alignment is applied to extract more accurate features from the proposed
    regions. Mask R-CNN does not leverage the multiscale feature representation which
    may degrade the overall performance of segmentation. To overcome this limitation,
    Path Aggregation Network (PANet) incorporates the FPN and introduces a bottom-up
    pathway to facilitate the propagation of the low-level information Liu et al.
    ([2018](#bib.bib146)). The pathway takes the feature maps of the previous stage
    as input and performs 3×3 convolution with stride 2 to reduce the spatial size
    of the feature maps. The generated feature maps are then fused with the feature
    maps from the FPN through the lateral connection. The model adopts the three branches
    as in Mask R-CNN. MaskLab is an instance segmentation model based on Faster R-CNN,
    consisting of object detection, segmentation and instance (object) center direction
    prediction branches Chen et al. ([2018](#bib.bib34)). The direction prediction
    provides useful information to distinguish instances of the same semantic label,
    allowing the model to further refine the instance segmentation results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention mechanisms have been integrated into the segmentation models to learn
    the weights of multiscale features at each pixel location. A multistage context
    refinement network introduces a context attention refinement module that is composed
    of two parts, context feature extraction and context feature refinement Liu et al.
    ([2023a](#bib.bib144)). The context feature extraction captures both local and
    global context information, fuses both contextual information and passes it to
    the context feature refinement while the context feature refinement removes redundant
    information and generates a refined feature representation, improving the utilization
    of contextual information. The context attention is added to the skip connection
    between the encoder and the decoder. Handcrafted features are often abandoned
    for automatic feature extraction using convolutional networks. However, it is
    argued that the interpretability and domain-specific knowledge embedded in handcrafted
    features can provide valuable insights. To this end, an attention module based
    on the covariance statistic is introduced to model the dependencies between local
    and global context of the input image Liu et al. ([2022](#bib.bib149)). Two types
    of attention are introduced: spatial covariance attention focuses on the spatial
    distribution and channel covariance attention attends to the important channels.
    Furthermore, the covariance attention does not require feature shape conversion,
    hence significantly reducing the space and time complexity of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layers use local receptive fields to process input data which
    can be effective for exploiting spatial patterns and hierarchical features but
    may find it difficult to capture global relationships across the entire image.
    The ViT has been leveraged to mitigate this issue in semantic segmentation Strudel
    et al. ([2021](#bib.bib235)). Specifically, the input image is divided into patches
    and treated as input to the transformer to capture the global relationship between
    the patches, significantly improving the prediction of the segmentation map. Global
    context ViT aims to address the lack of ViT’s ability to leverage local spatial
    features Hatamizadeh et al. ([2023](#bib.bib79)). As shown in Figure [19](#S4.F19
    "Figure 19 ‣ 4.1.3 Image Segmentation ‣ 4.1 Computer Vision ‣ 4 Applications of
    Deep Learning ‣ A Survey on Deep Learning and State-of-the-art Applications"),
    the transformer consists of local and global self-attention modules. The role
    of global self-attention is to capture the global contextual information from
    different image regions while the short-range information is captured by the local
    self-attention. Multiscale feature representation is crucial for accurate semantic
    segmentation. However, the transformer often combines the features without considering
    their appropriate (optimal) scales, thus affecting the segmentation accuracy.
    Transformer scale gate is a module proposed to address the issue of selecting
    an appropriate scale based on the correlation between patch-query pairs Huang
    et al. ([2023a](#bib.bib97)). The transformer takes attention (correlation) maps
    as input and calculates the weights of the multi-scale features for each image
    patch, allowing the model to adaptively choose the optimal scale for each patch.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/904502c7e47696511165a6d5acb77878.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: The architecture of global context ViT Hatamizadeh et al. ([2023](#bib.bib79)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Image Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Image generation refers to the process of creating images based on input texts.
    Generally, the task can be divided into three stages. The first stage is extracting
    features from the input text, followed by generating the image and finally controlling
    the image generation process to ensure the output meets specific criteria and
    constraints. This section focuses on the progress made in the development of deep
    learning models of the second stage (image generation) since it directly impacts
    the quality of the generated images. Variational autoencoder is one of the earliest
    deep learning models that is capable of generating images Kingma and Welling ([2013](#bib.bib119)).
    Variational autoencoder learns to generate data by capturing the underlying (Gaussian)
    distribution of the training data. During the generation process, the distribution
    parameters are sampled and passed to the decoder to generate the output image.
    Although the generated images are blurry and unsatisfactory, it has shown a lot
    of potential in image generation tasks. The introduction of GAN significantly
    improved the quality of generated images. GAN consists of two connected neural
    networks, a generator and a discriminator that are trained simultaneously in a
    competitive manner Goodfellow et al. ([2014](#bib.bib71)). The generator learns
    to generate realistic images to fool the discriminator, while the discriminator
    learns to distinguish between fake and real images. The generated images are less
    blurry and more realistic. Several enhanced models have been proposed to improve
    its usability and overall performance such as CGAN Odena et al. ([2017](#bib.bib183))
    which allows us to tell what image to be generated, and the deep convolutional
    GAN (DCGAN) Radford et al. ([2015](#bib.bib199)) which provides a more stable
    structure for image generation. DCGAN is the basis of many subsequent improvements
    in GANs.
  prefs: []
  type: TYPE_NORMAL
- en: StackGAN divides the process of image generation into two stages Zhang et al.
    ([2017](#bib.bib292)). Stage-I generates a low-resolution image by creating basic
    shapes and colors and the background layout using the random noise vector. Stage-II
    completes the details of the image and produces a high-resolution photo-realistic
    image. StackGAN++ is the enhanced model of StackGAN whereby it consists of multiple
    generators with shared parameters to generate multiscale images Zhang et al. ([2018a](#bib.bib293)).
    The generators have a progressive goal with the intermediate generators generating
    images of varying sizes and the deepest generator producing the photo-realistic
    image. HDGAN is a generative model featuring a single-stream generator with hierarchically
    nested discriminators at intermediate layers Zhang et al. ([2018b](#bib.bib299)).
    These layers, each connected to a discriminator, generate multiscale images. The
    lower resolution outputs are used to learn semantic image structures while the
    higher resolution outputs are used to learn fine-grained details of the image.
    StackGAN heavily relies on the quality of the generated image in Stage-I. DM-GAN
    incorporates a memory network for image refinement to cope with badly generated
    images in Stage-I Zhu et al. ([2019b](#bib.bib303)). The memory network dynamically
    selects the words that are relevant to the generated image, and then refines the
    details to produce better photo-realistic images.
  prefs: []
  type: TYPE_NORMAL
- en: AttnGAN is the first to incorporate attention mechanisms into the multiple generators
    to focus on words that are relevant to the generated image Xu et al. ([2018](#bib.bib282)).
    To this end, in addition to encoding the whole sentence into a global sentence
    vector, the text encoder encodes each word into a word vector as shown in Figure
    [20](#S4.F20 "Figure 20 ‣ 4.1.4 Image Generation ‣ 4.1 Computer Vision ‣ 4 Applications
    of Deep Learning ‣ A Survey on Deep Learning and State-of-the-art Applications").
    Then, the image vector is used to attend to the word vector using the attention
    modules at each stage of the multistage generators. Furthermore, AttnGAN introduces
    a loss function to compute the similarity between the generated image and the
    associated sentence, improving the performance of image generation. A similar
    work is reported whereby the model known as ResFPA-GAN, incorporates attention
    modules into the multiple generators Sun et al. ([2019](#bib.bib236)). Specifically,
    a feature pyramid attention module is proposed to capture high semantic information
    and fuse the multiscale feature, enhancing the overall performance of the model.
    DualAttn-GAN improves AttnGAN by incorporating visual attention modules to focus
    on important features along both spatial and channel dimensions Cai et al. ([2019](#bib.bib26)).
    This allows the model to better understand and capture both the context of the
    input sentence and the fine details of the image, resulting in more realistic
    image generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ff41aef1c0d54f3f8acc55fac46c4381.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: The architecture of AttnGAN Xu et al. ([2018](#bib.bib282)).'
  prefs: []
  type: TYPE_NORMAL
- en: Although multistage generators improve image generation performance by leveraging
    multiscale representation, the generated images may contain fuzzy shapes with
    coarse features. DF-GAN replaces the multistage generators with a single-stage
    deep generator featuring residual connections and trained with hinge loss Tao
    et al. ([2022](#bib.bib247)). Furthermore, DF-GAN introduces a regularization
    strategy on the discriminator that applies a gradient penalty on real images with
    matching text, allowing the model to generate more text-matching images. DMF-GAN
    an improved DF-GAN, incorporates three novel components designed to leverage semantic
    coherence between the input text and the generated image Yang et al. ([2024](#bib.bib284)).
    The first component is the recurrent semantic fusion module, which models long
    range dependencies between the fusion blocks. The second component is the multi-head
    attention module which is placed towards the end of the generator to leverage
    the word features, forcing the generator to generate images conditioned on the
    relevant words. The last component is the word-level discriminator which provides
    fine-grained feedback to the generator, facilitating the learning process and
    improving the overall quality of the generated images. Figure [21](#S4.F21 "Figure
    21 ‣ 4.1.4 Image Generation ‣ 4.1 Computer Vision ‣ 4 Applications of Deep Learning
    ‣ A Survey on Deep Learning and State-of-the-art Applications") shows the architecture
    of DMF-GAN. The process of image generation involves feeding a noise vector to
    the generator at the very beginning of the network. However, as the generator
    goes deeper, the noise effect may be diminished, affecting the diversity of the
    image generation results. To mitigate this issue, DE-GAN incorporates a dual injection
    module into the single-stage generator Jiang et al. ([2024](#bib.bib109)). The
    dual injection module consists of two text fusion layers followed by a noise broadcast
    operation. The text fusion layer takes the sentence embedding and fuses it with
    the input feature map using the fully-connected layer. Then noise is injected
    into the output feature map to retain the randomness in the generation process,
    improving diversity and generalization of the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cbc3742f5afc62bb0fbb9d344a271d2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: The architecture of DMF-GAN Yang et al. ([2024](#bib.bib284)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Time Series and Pervasive Computing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pervasive computing, often referred to as ubiquitous computing, is the process
    of integrating computer technology into everyday objects and surroundings so that
    they become intelligent, networked, and able to communicate with one another to
    offer improved services and functionalities Weiser ([1991](#bib.bib269)). According
    to He et al He et al. ([2020b](#bib.bib83)), the role of pervasive computing is
    foremost in the field where it provides the ability to distribute computational
    services to the surroundings where people work, leading to trust, privacy, and
    identity. Examples of pervasive computing applications include smart homes with
    connected appliances, wearable devices that monitor health and fitness, smart
    cities with sensor networks for traffic management, and industrial applications
    that utilize the Internet of Things (IoT) for monitoring and control. Generally,
    the continuous interaction of interconnected devices in pervasive computing often
    result in time series data, which captures the evolution of various parameters
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, medical sensors, such as electrocardiograms (ECG) and electroencephalograms
    (EEG), generate time series data that contain critical diagnostic information,
    which deep learning can use to detect anomalies, predict diseases, and classify
    medical conditions with improved accuracy. Also, devices such as accelerometers,
    magnetometers and gyroscopes, among others can be used to capture human activity
    signals, which are often represented as time series of state changes Ige and Noor
    ([2022](#bib.bib101)). In traditional machine learning, features such as mean,
    variance and others are manually extracted from times series of state changes
    before human activity classification. However, deep learning models automatically
    extract features Mohd Noor ([2021](#bib.bib169)). Also, in other fields such as
    finance, which entail time series data, deep learning has been instrumental in
    stock price prediction Singh and Srivastava ([2017](#bib.bib226)), fraud detection
    Zhang et al. ([2021](#bib.bib298)), and algorithmic trading Lei et al. ([2020](#bib.bib127)),
    among others. Generally, deep learning networks excel at capturing intricate temporal
    relationships within time-series data, enabling more precise predictions and improved
    decision-making. Based on this, several deep learning models have been employed
    for feature learning across various time series and pervasive computing domains.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Human Activity Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Human activity recognition (HAR) finds application across various domains including
    intelligent video surveillance, environmental home monitoring, video storage and
    retrieval, intelligent human-machine interfaces, and identity recognition, among
    many others. It includes various research fields, including the detection of humans
    in video, estimating human poses, tracking humans, and analyzing and understanding
    time series data Zhang et al. ([2019a](#bib.bib294)). Despite the advancements
    in vision-based HAR, there exist inherent limitations. Generally, vision-based
    approaches heavily rely on camera systems, which may have restricted views or
    be affected by lighting conditions, occlusions, and complex backgrounds Ige and
    Noor ([2022](#bib.bib101)). Additionally, vision-based HAR struggles with identifying
    actions that occur beyond the range of the camera or actions that are visually
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: Wearable sensors offer a promising alternative to overcome these limitations.
    By directly capturing data from the individual, wearable sensors provide more
    comprehensive and accurate information about human activities. The signals obtained
    from wearable sensors typically represent time series data reflecting state changes
    in activities. Deep learning models can effectively learn from these signals,
    allowing for robust and accurate recognition of human activities. Moreover, wearable
    sensors offer the advantage of mobility, enabling activity recognition in various
    environments and situations where vision-based systems may be impractical or ineffective
    Dang et al. ([2020](#bib.bib44)). Generally, the time series nature of signals
    from wearable sensors presents an excellent opportunity for deep learning models
    to excel in recognizing human activities with high accuracy and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Several researchers have proposed the use of CNN, RNN, and Hybrid models for
    deep learning based feature learning in wearable sensor HAR. For instance, using
    two-dimensional CNN (Conv2D), several researchers, as seen in Gao et al. ([2021a](#bib.bib62)),
    Gupta ([2021](#bib.bib75)) and Erdaş and Güney ([2021](#bib.bib57)), among others,
    have developed deep learning models for wearable sensor HAR, despite the time
    series nature of the data. This is often done by treating the time series signals
    from wearable sensors as 2D images by reshaping them appropriately. To achieve
    this, researchers often organize each time series signal into a matrix format,
    with time along one axis and sensor dimensions along the other, before creating
    a pseudo-image representation, which allows the matrix to be be fed into Conv2D
    layers for feature extraction. Conv2D layers excel at capturing spatial patterns
    and relationships within images, and by treating the time series data as images,
    these layers can learn relevant spatial features that contribute to activity recognition.
    The convolution operation performed by Conv2D filters across both the time and
    sensor dimensions, allowing the network to identify patterns and features that
    may be indicative of specific activities.
  prefs: []
  type: TYPE_NORMAL
- en: Even though Conv2D can effectively capture spatial dependencies within the data,
    it often struggles to capture temporal dependencies inherent in time series data.
    Since Conv2D processes data in a grid-like fashion, it does not fully leverage
    the sequential nature of the time series, potentially leading to less effective
    feature extraction for wearable sensor HAR tasks. For this reason, recent HAR
    architectures have leveraged one-dimensional CNN (Conv1D) and other RNNs for automatic
    feature extraction. Conv1D layers are specifically designed to capture temporal
    dependencies within sequential data. They operate directly on the time series
    data without reshaping it into a 2D format, allowing them to capture temporal
    patterns more effectively. Conv1D layers are better suited for extracting features
    from time series data, making them a more natural choice for wearable sensor HAR
    Mohd Noor ([2021](#bib.bib169)).
  prefs: []
  type: TYPE_NORMAL
- en: For instance, Ragab et al. Ragab et al. ([2020](#bib.bib201)) proposed a random
    search Conv1D model, and evaluated the performance of the model on UCI-HAR dataset.
    The result showed that the model achieved a recognition accuracy of 95.40% when
    classifying the six activities in the dataset. However, the model exhibited extended
    training times due to the dynamic nature of some activities within the dataset.
    To address this, Banjarey et al. ([2022](#bib.bib20)) proposed the use of varying
    kernel sizes in Conv1D layers to recognize various activities, including sitting,
    standing, walking, sleeping, reading, and tilting. Also, a few Conv1D layers were
    stacked in order to streamline the time optimization process for training the
    neural network. Also, some researchers have proposed models that combine machine
    learning algorithms with Conv1D in HAR, as seen in Shuvo et al. Shuvo et al. ([2020](#bib.bib222)).
    Their work presented a two-stage learning process to improve HAR by classifying
    activities into static and dynamic using Random Forest, before using Support Vector
    Machine to identify each static activity, and Conv1D to recognize dynamic activities.
    The result showed that the method achieved an accuracy of 97.71% on the UCI-HAR
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Following these advancements, several researchers have further explored Conv1D
    architectures with various modifications, to enhance feature learning in activity
    recognition systems. For example, Han et al. Han et al. ([2022](#bib.bib76)) developed
    a two-stream CNN architecture as a plug-and-play module to encode contextual information
    of sensor time series from different receptive field sizes. The module was integrated
    into existing deep models for HAR at no extra computation cost. Experiments on
    OPPORTUNITY, PAMAP2, UCI-HAR and USC-HAD datasets showed that the module improved
    feature learning capabilities. A similar research reported in Ige and Noor ([2023](#bib.bib102))
    proposed the WSense module to address the issue of differences in the quality
    of features learnt, regardless of the size of the sliding window segmentation,
    and experimented on PAMAP2 and WISDM datasets. The results showed that by plugging
    the WSense module into Conv1D architectures, improved activity features can be
    learned from wearable sensor data for human activity recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, some researchers have also proposed the use of standalone RNNs in
    HAR, and a hybrid of Conv1D architectures with RNNs such as LSTMs Deep and Zheng
    ([2019](#bib.bib46)), BiLSTMs Luwe et al. ([2022](#bib.bib160)); Shi et al. ([2023](#bib.bib220)),
    GRUs Dua et al. ([2023](#bib.bib53)) and BiGRUs Imran et al. ([2023](#bib.bib103));
    Chen et al. ([2022b](#bib.bib32)) in order to fully harness the feature learning
    capabilities of both CNN and RNNs. For instance, Nafea et al. Nafea et al. ([2021](#bib.bib175)),
    leveraged Bi-LSTM and Conv1D with increasing kernel sizes to learn features at
    various resolutions. Human activity features were extracted using the stacked
    convolutional layers with a Bi-LSTM layer, before including a flattening layer
    and a fully connected layer for subsequent classification. However, the model
    had issues extracting quality features of dynamic activities compared to static
    activities. To address such issues, some research works have incorporated attention
    mechanisms in Conv1D-based architectures to improve feature learning of dynamic
    and complex activities from time series signals obtained from wearable sensors.
    For example, Khan and Ahmad Khan and Ahmad ([2021](#bib.bib116)) designed three
    lightweight convolutional heads, with each specialized in feature extraction from
    wearable sensor data. Each head comprised stacked layers of Conv1Ds, along with
    embedded attention mechanisms to augment feature learning. The results demonstrated
    that integrating multiple 1D-CNN heads with attention mechanisms can enhance feature
    learning for Human Activity Recognition (HAR). These diverse modifications and
    adaptations showcase the versatility and potential of deep learning models in
    achieving state-of-the-art in HAR systems.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8022b6c5921effc29f8b055f5b1fcead.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: The architecture of multi-head CNN model Khan and Ahmad ([2021](#bib.bib116)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Speech Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Speech, as the primary mode of human communication, has captivated researchers
    for over five decades, especially since the inception of artificial intelligence
    Nassif et al. ([2019](#bib.bib178)). From the earliest endeavors to understand
    and replicate the complexities of human speech, to contemporary advancements leveraging
    cutting-edge technologies, the quest for accurate and efficient speech recognition
    systems has been relentless. In recent years, the emergence of deep learning techniques
    has revolutionized the speech recognition field. Deep learning has demonstrated
    unparalleled success in processing and extracting intricate patterns from vast
    amount of data. When applied to the realm of speech recognition, deep learning
    have surpassed traditional approaches by learning intricate features directly
    from raw audio signals, circumventing the need for handcrafted features and complex
    preprocessing pipelines. This paradigm shift has significantly advanced the state-of-the-art
    in speech recognition, enabling systems to achieve unprecedented levels of accuracy
    and robustness across various languages, accents, and environmental conditions.
    Generally, deep learning has been extended to other essential applications of
    speech recognition, such as speaker identification Tirumala and Shahamiri ([2016](#bib.bib251));
    Ye and Yang ([2021](#bib.bib287)), emotion recognition Khalil et al. ([2019](#bib.bib115)),
    language identification Singh et al. ([2021](#bib.bib225)), accent recognition
    Jiao et al. ([2016](#bib.bib111)), age recognition Sánchez-Hevia et al. ([2022](#bib.bib211))
    and gender recognition Alnuaim et al. ([2022](#bib.bib6)), among many others.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to the adoption of deep learning in speech recognition, the foundation
    of traditional speech recognition systems was the use of Gaussian Mixture Models
    (GMMs), which are often combined with Hidden Markov Models (HMMs) to represent
    speech signals Srivastava and Pandey ([2022](#bib.bib231)). This is because a
    speech signal can be thought of as a short-term stationary signal. The spectral
    representation of the sound wave is modelled by each HMM using a mixture of Gaussian.
    However, they are considered statistically inefficient for modelling non-linear
    or near non-linear functions Padmanabhan and Premkumar ([2015](#bib.bib187));
    Nassif et al. ([2019](#bib.bib178)). This is because HMMs rely on a set of predefined
    states and transition probabilities, making assumptions about the linearity and
    stationarity of the underlying data. While suitable for modelling certain aspects
    of speech, HMMs often fall short when tasked with representing the intricate nonlinearities
    and variability present in speech signals. Speech, by nature, exhibits nonlinear
    and dynamic characteristics, with features such as intonation, rhythm, and phonetic
    variations challenging the simplistic assumptions of traditional statistical models
    like HMMs. In other words, GMM-HMM approach had limitations in capturing complex
    acoustic patterns and long-term dependencies in speech Mukhamadiyev et al. ([2022](#bib.bib170)).
  prefs: []
  type: TYPE_NORMAL
- en: In recent times, CNN and RNNs have been leveraged for automatic speech recognition
    in order to consider a longer or variable temporal window for context information
    extraction Lu et al. ([2020](#bib.bib156)). Generally, CNNs are well-suited for
    capturing local patterns and hierarchical features in data, making them effective
    for modelling acoustic features in speech. By directly learning features from
    raw speech signals, CNNs bypassed the need for handcrafted features used in traditional
    GMM-HMM systems. Additionally, CNNs can capture long-range dependencies in the
    data, which is crucial for understanding the context of speech. Likewise, the
    RNNs are suitable choice for exploring extended temporal context information in
    one processing level for feature extraction and modelling.
  prefs: []
  type: TYPE_NORMAL
- en: Based on this, several researchers have proposed the use of both CNN and variants
    of RNNs for automatic speech recognition and for other speech related tasks. For
    instance, Hema and Garcia Marquez ([2023](#bib.bib84)), used CNN to classify speech
    emotions and benchmarked on a dataset consisting of seven classes (anger, disgust,
    fear, happiness, neutral, sadness and surprise). However, CNN lack the ability
    to model temporal dependencies explicitly. In speech recognition, understanding
    the temporal context of speech is essential for accurate transcription. Also,
    speech signals are inherently sequential, and information from previous time steps
    is crucial for understanding the current speech segment. CNNs, by design, do not
    inherently capture this sequential nature. For this reason, variants of RNNs have
    been leveraged to collect extended contexts in speeches. This is because RNNs
    are designed to model sequential data by maintaining hidden states that capture
    information from previous time steps. This allows them to capture temporal dependencies
    effectively, making them well-suited for ASR tasks. In Shewalkar et al. ([2019](#bib.bib219)),
    the authors evaluated the performance of RNN, LSTM, and GRU on a popular benchmark
    speech dataset (ED-LIUM). The results showed that LSTM achieved the best word
    error rate while the GRU optimization was faster and achieved word error rate
    close to that of LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: However, RNN architectures process input sequences sequentially, which limits
    their ability to capture global context information effectively. As a result,
    they may struggle to understand the entire context of a spoken utterance, leading
    to lower transcription accuracy, particularly in tasks requiring understanding
    beyond local dependencies. Also, most CNN and RNN automatic speech recognition
    systems comprise of separate acoustic, pronunciation, and language modelling components
    that are trained independently. Usually, the acoustic model bootstraps from an
    existing model that is used for alignment in order to train it to recognise context
    dependent (CD) states or phonemes. The pronunciation model, curated by expert
    linguists, maps the sequences of phonemes produced by the acoustic model into
    word sequences. For this reason, Sequence-to-Sequence (Seq2Seq) models are being
    proposed in automatic speech recognition to train the acoustic, pronunciation,
    and language modelling components jointly in a single system Prabhavalkar et al.
    ([2017](#bib.bib196)). Seq2Seq methods in automatic speech recognition are a class
    of models that aim to directly transcribe an input sequence of acoustic features
    such as speech spectrograms or Mel-frequency cepstral coefficients into a sequence
    of characters or words representing the recognized speech. There have been a variety
    of sequence-to-sequence models explored in the literature, including Recurrent
    Neural Network Transducer (RNN-T) Graves ([2012](#bib.bib72)), Listen, Attend
    and Spell (LAS) Chan et al. ([2015](#bib.bib29)), Neural Transducer Jaitly et al.
    ([2016](#bib.bib108)), Monotonic Alignments Raffel et al. ([2017](#bib.bib200))
    and Recurrent Neural Aligner (RNA) Sak et al. ([2017](#bib.bib210)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/57c85e10110a03573f0ed18171e0b26c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: Sequence-to-Sequence'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in Figure [23](#S4.F23 "Figure 23 ‣ 4.2.2 Speech Recognition ‣ 4.2
    Time Series and Pervasive Computing ‣ 4 Applications of Deep Learning ‣ A Survey
    on Deep Learning and State-of-the-art Applications"), the encoder component takes
    the input sequence of acoustic features and processes it to create a fixed-dimensional
    representation, often called the context vector. This representation captures
    the essential information from the input sequence and serves as the basis for
    generating the output sequence. The decoder component takes the context vector
    produced by the encoder and generates the output sequence. In ASR, this output
    sequence consists of characters or words representing the recognized speech. The
    decoder is typically implemented as a recurrent neural network (RNN), such as
    a Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) network, or it could
    be a transformer-based architecture. During training, the model learns to map
    input sequences to their corresponding output sequences by minimizing a suitable
    loss function, such as cross-entropy loss. This is typically done using techniques
    like backpropagation through time (BPTT) or teacher forcing, where the model is
    trained to predict the next token in the output sequence given the previous tokens.
    Thereafter, the trained model is used to transcribe unseen speech input. The encoder
    processes the input sequence to produce the context vector, which is then fed
    into the decoder to generate the output sequence. In some cases, beam search Szűcs
    and Huszti ([2019](#bib.bib241)); Li et al. ([2018](#bib.bib137)) or other decoding
    strategies may be used to improve the quality of the generated output.
  prefs: []
  type: TYPE_NORMAL
- en: In Chiu et al. ([2018](#bib.bib37)), the authors explored various structural
    and optimization enhancements to their LAS Sequence to Sequence model, resulting
    in significant performance improvements. They introduce several structural enhancements,
    including the utilization of word piece models instead of graphemes and the incorporation
    of a multi-head attention architecture, which outperforms the commonly used single-head
    attention mechanism. Additionally, they investigate optimization techniques such
    as synchronous training, scheduled sampling, label smoothing, and minimum word
    error rate optimization, all of which demonstrate improvements in accuracy. The
    authors present experimental results utilizing a unidirectional LSTM encoder for
    streaming recognition. On a 12,500-hour voice search task, they observe a decrease
    in Word Error Rate (WER) from 9.2% to 5.6% with the proposed changes, while the
    best-performing conventional system achieves a WER of 6.7%. Moreover, on a dictation
    task, their model achieves a WER of 4.1%, compared to 5% for the conventional
    system. Similarly, the work of Prabhavalkar et al. Prabhavalkar et al. ([2017](#bib.bib196))
    investigated a number of sequence-to-sequence methods in automatic speech recognition.
    These included the RNN transducer (RNN-T), attention-based models, a new model
    that augments the RNN-T with attention, and a Connectionist Temporal Classification
    (CTC) trained system that directly outputs grapheme sequences. According to their
    research, sequence-to-sequence approaches can compete on dictation test sets against
    state-of-the-art when trained on a large volume of training data. Even though
    deep learning has achieved state-of-the-art in speech recognition, an area that
    still calls for attention is speech-to-speech translation. This is because the
    present deep learning based speech-to-speech translation systems operate by translating
    sentences individually, disregarding any contextual information from preceding
    sentences. While research on contextual understanding has been ongoing for years,
    challenges persist regarding its practicality and processing efficiency, since
    translation typically relies on the surrounding words for context.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Electrocardiogram (ECG) Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Disorders pertaining to the heart or blood vessels are collectively referred
    to as Cardiovascular Diseases (CVD) Liu et al. ([2021a](#bib.bib148)). According
    to the American Heart Association’s 2023 statistics, CVD has emerged as the leading
    cause of death worldwide. In 2020, 19.05 million deaths were recorded from CVD
    globally, which signifies an increase of 18.71% from 2010, and it is believed
    that this number will rise to 23.6 million by 2030 Tsao et al. ([2023](#bib.bib254)).
    Blood clots and vascular blockages caused by CVDs can cause myocardial infarction,
    stroke or even death Liu et al. ([2021a](#bib.bib148)). Generally, early diagnosis
    has been shown to reduce the mortality rate of CVDs, and Electrocardiogram (ECG)
    signals play a crucial role in diagnosing various cardiac abnormalities and monitoring
    heart health. However, ECG signal has characteristics of high noise and high complexity,
    making it time-consuming and labor-intensive to identify certain diseases using
    traditional methods. The traditional approach is tedious and requires the expertise
    of a medical specialist. Over the past decades, the task of Long-term ECG recording
    classification has been significantly facilitated for cardiologists through the
    adoption of computerized ECG recognition practices. Throughout this period, feature
    extraction methods have predominantly relied on manual techniques, encompassing
    diverse approaches such as wave shape functions Llamedo and Martínez ([2011](#bib.bib153)),
    wavelet-based features Mathews et al. ([2018](#bib.bib164)), ECG morphology zhu
    et al. ([2019](#bib.bib304)), hermite polynomials Desai et al. ([2021](#bib.bib48)),
    and Karhunen-Loeve expansion of ECG morphology Crippa et al. ([2015](#bib.bib40)),
    among others. These extracted features are subsequently subjected to classification
    using various machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, the advent of deep learning has revolutionized the field by enabling
    automatic feature learning directly from ECG signals. This advancement holds significant
    promise in the realm of automated ECG classification, offering clinicians a tool
    for swift and accurate diagnosis. Based on this, several deep learning architectures
    have been proposed for feature learning of ECG signals. For instance, Acharya
    et al. Acharya et al. ([2017](#bib.bib3)) developed a 9-layer CNN model to automatically
    identify five categories of heartbeats in ECG signals. A similar model was also
    developed in Baloglu et al. ([2019](#bib.bib18)), However, ECG signals often vary
    significantly in length, as they may contain different numbers of heartbeats.
    CNNs typically require fixed-length inputs, which may necessitate preprocessing
    steps such as padding or truncation, potentially losing important temporal information.
    For this reason, several architectures have leveraged RNN in ECG classification,
    as seen in Singh et al. ([2018](#bib.bib227)), Prabhakararao and Dandapat ([2020](#bib.bib195))
    and Wang et al. ([2023b](#bib.bib263)), among others. While RNNs are capable of
    handling sequential data, they also have limitations in capturing local patterns
    or short-term dependencies effectively. In ECG signals, local features such as
    specific waveforms or intervals can be crucial for classification. For this reason,
    recent works have proposed hybrid models which combine the strengths of both CNNs
    and RNNs to overcome some of these limitations Sowmya and Jose ([2022](#bib.bib229)).
  prefs: []
  type: TYPE_NORMAL
- en: The work of Rai et al. Rai and Chatterjee ([2022](#bib.bib202)) developed a
    hybrid CNN-LSTM network to evaluate the optimum performing model for myocardial
    infarction detection using ECG signals. The authors then experimented on 123,998
    ECG beats obtained from the PTB diagnostic database (PTBDB) and MIT-BIH arrhythmia
    database (MITDB), and the result showed that by combining the capabilities of
    both CNN and LSTM, improved classification accuracy can be achieved. Also, in
    Banerjee et al. ([2020](#bib.bib19)), a CNN architecture was developed to extract
    morphological features from ECG signals. For the purpose of determining the degree
    of heart rate variability, another composite structure was designed using LSTM
    and a collection of manually created statistical features. Following that, a hybrid
    CNN-LSTM architecture is built using the two independent biomarkers to classify
    cardiovascular artery diseases, and experiments were carried out on two distinct
    datasets. The first is a partly noisy in-house dataset collected using an inexpensive
    ECG sensor, and the other is a corpus taken from the MIMIC II waveform dataset.
    The hybrid model proposed in the work achieved an overall classification accuracy
    of 88% and 93%, respectively, which surpasses the performance of standalone architectures.
  prefs: []
  type: TYPE_NORMAL
- en: An automated diagnosis method based on Deep CNN and LSTM architecture was presented
    in Kusuma and Jothi ([2022](#bib.bib122)) to identify Congestive Heart Failure
    (CHF) from ECG signals. Specifically, CNN was used to extract deep features, and
    LSTM was employed to exploit the extracted features to achieve the CHF detection
    goal. The model was tested using real-time ECG signal datasets, and the results
    showed that the AUC was 99.9%, the sensitivity was 99.31%, the specificity was
    99.28%, the F-Score was 98.94%, and the accuracy was 99.52%. However, since ECG
    signals can vary in length due to differences in recording durations or patient
    conditions. LSTMs are capable of handling variable-length sequences, but traditional
    CNNs typically require fixed-length inputs. Therefore, fusing these features effectively
    in a hybrid model can be challenging. Also, Hybrid CNN-RNN models can be computationally
    intensive, especially when processing long ECG sequences or large datasets. For
    this reason, recent research works have proposed the use of attention mechanisms
    to reduce the computational burden by enabling the model to selectively attend
    to informative features, focusing computational resources where they are most
    needed. Likewise, attention mechanisms can enable the model to attend to informative
    segments of the ECG signal, regardless of their length, allowing for more flexible
    processing of variable-length sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Several researchers have leveraged attention mechanisms in standalone and hybrid
    architectures for improved performance. For instance, in the work of Chun-Yen
    et al. Chen et al. ([2022a](#bib.bib31)), CNN layers were used to extract main
    features, while LSTM and attention were included to enhance the model’s feature
    learning capabilities. Experiments on a 12-lead KMUH ECG dataset showed that the
    model had high recognition rates in classifying normal and abnormal ECG signals,
    compared to hybrid models without attention mechanisms. Wang et al. Wang et al.
    ([2021](#bib.bib262)) presented a 33-layer CNN architecture with non-local convolutional
    block attention module (NCBAM). To extract the spatial and channel information,
    preprocessed ECG signals were first fed into the CNN architecture. A non-local
    attention further captured long-range dependencies of representative features
    along spatial and channel axes. Similarly, a spatio-temporal attention-based convolutional
    recurrent neural network (STA-CRNN) was presented in Zhang et al. ([2020a](#bib.bib295))
    with the aim of concentrating on representative features in both the spatial and
    temporal dimensions. The CNN subnetwork, spatiotemporal attention modules, and
    RNN subnetwork made up the STA-CRNN and according to findings, the STA-CRNN model
    was able to classify eight different forms of arrhythmias and normal rhythm with
    an average F1 score of 0.835.
  prefs: []
  type: TYPE_NORMAL
- en: Combining hybrid deep learning models with attention mechanisms for ECG feature
    learning is a promising approach that has already shown potential in ECG feature
    learning, according to reviewed literature. Future research can further explore
    semi-supervised and self-supervised learning techniques to leverage large amounts
    of unlabeled ECG data. This could involve pre-training models on large-scale unlabeled
    datasets using self-supervised learning objectives. Also, deep learning models
    have been leveraged in the generation of synthetic ECG signals to augment real
    signals, as seen in Zhu et al. ([2019a](#bib.bib302)) where a GAN model was developed
    to generate ECG signals that correspond with available clinical data. The GAN
    model used two layers of BiLSTM networks for the generator and CNN for the discriminator,
    and trained using the 48 ECG recordings of different users from the MIT-BIH dataset.
    The authors then compared their model with a Recurrent neural network autoencoder
    (RNN-AE) model and a recurrent neural network variational autoencoder (RNN-VAE)
    model, and the results showed that their model exhibited the fastest convergence
    of its loss function to zero. Future research can also incorporate attention mechanisms
    into hybrid GAN architectures to improve the quality of generated signals. Likewise,
    real-time detection of heart diseases is paramount, future work can develop efficient
    algorithms for real-time processing of ECG data. This could involve optimizing
    existing architectures and leveraging hardware acceleration techniques to enable
    real-time inference on resource-constrained devices such as wearable sensors and
    implantable devices.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Electroencephalography (EEG) Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Three-dimensional scalp surface electrode readings provide a dynamic time series
    that is called Electroencephalogram (EEG) signal Schirrmeister et al. ([2017](#bib.bib214)).
    Brain waves obtained from an EEG can effectively depict both the psychological
    and pathological states of a human. The human brain is acknowledged to be a fascinating
    and incredibly complicated structure. Numerous brain signals, including functional
    magnetic resonance imaging (fMRI), near-infrared spectroscopy (NIRS), electroencephalograms
    (EEGs), and functional near-infrared spectroscopy (fNIR), among others have been
    collected and used to study the brain Gao et al. ([2021b](#bib.bib63)). Due to
    the EEG’s non-invasive, affordable, accessible, and excellent temporal resolution
    characteristics, it has become the most utilised approach. However, the signal-to-noise
    ratio of EEG signal is low, meaning that sources with no task-relevant information
    frequently have a stronger effect on the EEG signal than those that do. These
    characteristics often make end-to-end feature learning for EEG data substantially
    more challenging Schirrmeister et al. ([2017](#bib.bib214)). Based on this, several
    methods have been leveraged for improved feature extraction in EEG signals across
    several domains including Motor imagery Ang and Guan ([2017](#bib.bib12)), anxiety
    disorder Shen et al. ([2022](#bib.bib218)), epileptic seizure detection Boonyakitanont
    et al. ([2020](#bib.bib23)), sleep pattern analysis and disorder detection Sharma
    et al. ([2021](#bib.bib216)); Vaquerizo-Villar et al. ([2023](#bib.bib256)), and
    Alzheimer’s disease detection Modir et al. ([2023](#bib.bib168)), and many others.
  prefs: []
  type: TYPE_NORMAL
- en: EEG Motor Imagery (MI) is a technique used to study brain activity associated
    with the imagination of movement. It involves recording electrical activity generated
    by the brain through electrodes placed on the scalp. MI tasks typically involve
    imagining performing a specific motor action, such as moving a hand or foot, without
    physically executing the movement, and has been leveraged in smart healthcare
    applications such as post-stroke rehabilitation and mobile assistive robots, among
    others Altaheri et al. ([2023](#bib.bib10)). Prior to the advent of deep learning,
    motor imagery EEG data are passed through various steps before classification
    using traditional ML techniques. Pre-processing, feature extraction, and classification
    are the three primary stages that traditional approaches usually take while processing
    MI-EEG signals. Pre-processing includes a number of operations, including signal
    filtering (choosing the most valuable frequency range for MI tasks), channel selection
    (identifying the most valuable EEG channels for MI tasks), signal normalisation
    (normalising each EEG channel around the time axis), and artefact removal (removing
    noise from MI-EEG signals). Independent component analysis (ICA) is the most often
    utilised technique for removing artefacts Brunner et al. ([2007](#bib.bib24));
    Delorme et al. ([2007](#bib.bib47)); Jafarifarmand and Badamchizadeh ([2019](#bib.bib106)).
    In contrast to the traditional approach, deep learning architectures can automatically
    extract complex features from raw MI-EEG data without the need for laborious feature
    extraction and pre-processing. Based on this, several deep learning architectures
    have been proposed for MI-EEG feature learning, as seen in Zhang et al. ([2019b](#bib.bib296)),
    Kumar et al. ([2016](#bib.bib121)) and Tibrewal et al. ([2022](#bib.bib249)),
    among others. For instance Schirrmeister et al. ([2017](#bib.bib214)), categorized
    MI-EEG signals using three CNNs with varying architectures, and the number of
    convolutional layers varied from two layers to a five-layer deep ConvNet to a
    thirty-one-layer residual network.
  prefs: []
  type: TYPE_NORMAL
- en: In Dai et al. Dai et al. ([2019](#bib.bib42)), the authors proposed an approach
    for classifying MI-EEG signals which blend variational autoencoder with CNN architecture.
    The VAE decoder was used to fit the Gaussian distribution of EEG signals, and
    the time, frequency, and channel information from the EEG signal were combined
    to create a novel representation of input, and the proposed CNN-VAE method was
    optimised for the input. Experiments showed that by combining both deep learning
    architectures, improved features were learnt, which led to a high classification
    performance on the BCI Competition IV dataset 2b. Li et al. Li et al. ([2017](#bib.bib132))
    employed optimal wavelet packet transform (OWPT) for the generation of feature
    vectors from MI-EEG signals. These vectors were then utilized to train an LSTM
    network which demonstrated satisfactory performance on dataset III from the BCI
    Competition 2003\. However, the model has excessively intricate structure. To
    address this, Feng et al. Li et al. ([2020a](#bib.bib129)) introduced a technique
    that merges continuous wavelet transform (CWT) with a simplified convolutional
    neural network to enhance the accuracy of recognizing MI-EEG signals. By employing
    CWT, MI-EEG signals were converted into time-frequency image representations.
    Subsequently, these image representations were fed into the SCNN for feature extraction
    and classification. Experiments on the BCI Competition IV Dataset 2b demonstrate
    that, on average, the classification accuracy across nine subjects reached 83.2%.
    However, the computational complexity of the model was quite high, due to the
    processing of time-frequency image representations. The conversion of MI-EEG signals
    into time-frequency images using CWT requires significant computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in Hwang et al. ([2023](#bib.bib99)) introduced a classification
    framework based on Long Short-Term Memory (LSTM) to improve the accuracy of classifying
    four-class motor imagery signals from EEG. The authors sliding window technique
    to capture time-varying EEG signal data, and employed an overlapping-band-based
    Filter Bank Common Spatial Patterns (FBCSP) method to extract subject-specific
    spatial features. Experiments on the BCI Competition IV dataset 2a, showed that
    their model achieved an average accuracy of 97%, compared to existing methods.
    Also, in the classification of Alzheimer’s disease, Zhao et al. Zhao and He ([2015](#bib.bib301))
    employed a deep learning network to analyse EEG data. The deep learning model
    was evaluated on a dataset that consist of fifteen (15) patients with clinically
    confirmed Alzheimer’s disease and fifteen (15) healthy individuals, and results
    showed that improved features were learnt and compared the results to the traditional
    methods. This has prompted the use of deep learning in Alzheimer’s disease detection,
    as seen in Xia et al. ([2023](#bib.bib277)), where the authors used CNN for diagnosing
    Alzheimer’s Disease. To address challenges posed by limited data and overfitting
    in deep learning models designed for Alzheimer’s Disease detection, the authors
    explored the use of overlapping sliding windows to augment the EEG data collected
    from 100 subjects (comprising 49 AD patients, 37 mild cognitive impairment patients,
    and 14 healthy controls subjects). After assembling the augmented dataset, a modified
    Deep Pyramid Convolutional Neural Network (DPCNN) was used to classify the enhanced
    EEG signals. In epilepsy detection, Hermawan et al. ([2024](#bib.bib85)) developed
    three deep learning architectures (CNN, LSTM, and hybrid CNN-LSTM), with each
    model chosen for its effectiveness in handling the intricate characteristics of
    EEG data. Each architecture offers distinct advantages, with CNN excelling in
    spatial feature extraction, LSTM in capturing temporal dynamics, and the hybrid
    model combining these strengths. The CNN model, consisting of 31 layers, attained
    the highest accuracy, achieving 91% on the first benchmark dataset and 82% on
    the second dataset using a 30-second threshold, selected for its clinical significance.
  prefs: []
  type: TYPE_NORMAL
- en: In the work of Abdulwahhab et al. Abdulwahhab et al. ([2024](#bib.bib1)), EEG
    waves’ time-frequency image and raw EEG waves served as input elements for CNN
    and LSTM models. Two signal processing methods, namely Short-Time Fourier Transform
    (STFT) and CWT, were employed to generate spectrogram and scalogram images, sized
    at 77 × 75 and 32 × 32, respectively. The experimental findings demonstrated detection
    accuracies of 99.57% and 99.26% for CNN inputs using CWT Scalograms on the Bonn
    University dataset and 99.57% and 97.12% using STFT spectrograms on the CHB-MIT
    dataset. Similarly, in emotion recognition, several deep learning models have
    been leveraged with EEG signals. For instance, in Pandey and Seeja ([2022](#bib.bib189)),
    a subject-independent emotion recognition model was proposed, which utilizes Variational
    Mode Decomposition (VMD) for feature extraction and DNN as the classifier. Evaluation
    against the benchmark DEAP dataset demonstrates superior performance of this approach
    compared to other techniques in subject-independent emotion recognition from EEG
    signals. Also, some researchers have also combined EEG signals with facial expression
    and speech in emotion recognition, as seen in Hassouneh et al. ([2020](#bib.bib78)),
    Pan et al. ([2023](#bib.bib188)), and Wang et al. ([2023c](#bib.bib267)), among
    others. However, EEG signals can vary significantly across individuals, making
    it challenging to generalize models across different subjects. Future models could
    explore methods for adapting or personalizing models to account for inter-subject
    variability and improve performance on individual subjects. Also, EEG electrodes
    cover only a fraction of the brain’s surface, resulting in limited coverage of
    neural activity. Deep learning models could investigate strategies to infer activity
    from unobserved brain regions or integrate information from multiple modalities
    to provide more comprehensive coverage. These areas can still be further explored.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5 Finance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Over the past few decades, computational intelligence in finance has been a
    hot issue in both academia and the financial sector Ozbayoglu et al. ([2020](#bib.bib186)).
    Deep learning, especially RNN models have gained significant traction in the field
    of finance due to its ability to handle sequential data, since financial data
    often exhibit sequential dependencies, such as time series data for stock prices
    or historical transaction data. Within the financial industry, researchers have
    developed deep learning models for stock market forecasting Singh and Srivastava
    ([2017](#bib.bib226)), algorithmic trading Lei et al. ([2020](#bib.bib127)), credit
    risk assessment Shen et al. ([2021](#bib.bib217)), portfolio allocation Wang et al.
    ([2020b](#bib.bib268)), asset pricing Chen et al. ([2024](#bib.bib33)), and derivatives
    markets Ahnouch et al. ([2023](#bib.bib4)), among others and these models are
    intended to offer real-time operational solutions. In exchange rate prediction,
    Sun et al. Sun et al. ([2020](#bib.bib237)) developed an ensemble deep learning
    technique known as LSTM-B by combining a bagging ensemble learning algorithm with
    a long-short term memory (LSTM) neural network to increase the profitability of
    exchange rate trading and produce accurate exchange rate forecasting results.
    In comparison to previous methodologies, the authors’ estimates proved to be more
    accurate when they looked at the potential financial profitability of exchange
    rates between the US dollar (USD) and four other major currencies: GBP, JPY, EUR,
    and CNY.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors in Abedin et al. ([2021](#bib.bib2)) proposed a Bi-LSTM-BR technique,
    which combined Bagging Ridge (BR) regression with Bi-LSTM as base regressors.
    The pre-COVID-19 and COVID-19 exchange rates of 21 currencies against the USD
    were predicted using the Bi-LSTM BR, and experiments showed that the proposed
    method outperformed ML algorithms such as DT and SVM. However, exchange rate data
    can be noisy and subject to non-stationarity, which can pose challenges for predictive
    modelling. While bagging techniques can help mitigate the effects of noise to
    some extent, they may struggle to capture long-term trends or sudden shifts in
    the data distribution, leading to suboptimal performance. To address this, Wang
    et al. Wang et al. ([2023a](#bib.bib260)) presented an approach for one-day ahead
    of time exchange rate prediction that concurrently considers both supervised and
    unsupervised deep representation features to enhance Random Subspace. Two crucial
    phases in the SUDF-RS technique are feature extraction and model building. First,
    LSTM and deep belief networks, respectively, extract the supervised and unsupervised
    deep representation features. To produce high-quality feature subsets, an enhanced
    random subspace approach was created that integrates a random forest-based feature
    weighting mechanism. Then, the matching base learner is trained using each feature
    subset, and the final outcomes are generated by averaging the outcomes of each
    base learner. Experiments on EUR/USD, GBP/USD and USD/JPY showed that improved
    accuracy was achieved using the model.
  prefs: []
  type: TYPE_NORMAL
- en: In stock market prediction, several deep learning architectures have been proposed
    in the literature. For instance, Nikou et al. ([2019](#bib.bib181)), conducted
    a comparative study between the ANN, SVR, RF and an LSTM model. As compared to
    the other models discussed in the study, the LSTM model outperformed the others
    in predicting the closing prices of iShares MSCI United Kingdom. Similarly, using
    stock market historical data and financial news, Cai et al. Cai et al. ([2018](#bib.bib25))
    used CNN and LSTM forecasting methods to generate seven prediction models. The
    seven models were then combined into a single ensemble model in accordance with
    the ensemble learning approach to create an aggregated model. However, the accuracy
    of all the models’ predictions was low. Gudelek et al. Gudelek et al. ([2017](#bib.bib73))
    proposed a CNN model which used a sliding window technique and created pictures
    by capturing daily snapshots within the window’s bounds. With 72% accuracy, the
    model was able to forecast the prices for the following day and was able to generate
    5 times the starting capital. In Eapen et al. Eapen et al. ([2019](#bib.bib56)),
    a CNN and Bi-LSTM model with numerous pipelines was proposed, utilising an SVM
    regressor model on the S&P 500 Grand Challenge dataset, and results showed enhanced
    prediction performance by over a factor of 6% compared to baseline models. As
    presented, deep learning has undeniably achieved state-of-the-art performance
    across various domains within finance. However, due to the sensitive nature of
    financial research, future work can focus on enhancing the interpretability of
    deep learning models in financial predictions. Researchers should explore techniques
    to explain the predictions of models, to improve trust and understanding of model
    decisions, which is essential for adoption in finance.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Natural Language Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Natural language processing (NLP) refers to the field of artificial intelligence
    that concerns with enabling computers to process, analyze and interpret human
    languages to extract useful information. Some of the common tasks in NLP are machine
    translation, text classification and text generation. Deep learning has been widely
    applied to solve real-world NLP problems. This section presents the recent advancements
    in deep learning models that have been designed for NLP over the past few years.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Text Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Text classification known as text categorization, is a task that involves assigning
    predefined categories or labels to a piece of text based on its content. The task
    is commonly used in various applications such as document classification, sentiment
    analysis and spam filtering. Numerous deep learning models have been proposed
    for text classification in the past few decades, and multilayer perceptron is
    one of the earliest architectures adopted to classify documents Calvo and Ceccatto
    ([2000](#bib.bib27)); Yu et al. ([2008](#bib.bib289)). The model typically has
    a single hidden layer with a number of units between 15 and 150\. Text data is
    inherently sequential, as it is composed of a series of words and symbols arranged
    in a specific order. This property makes RNN and its variants particularly well-suited
    for processing and analyzing text data. In Arevian ([2007](#bib.bib14)), RNN with
    two hidden layers, each with 6 units is used to classify news documents into eight
    classes. A study was conducted to investigate the variants of RNN i.e. LSTM and
    GRU for text classification Huang and Feng ([2020](#bib.bib96)). The input to
    the model is a sequence of words of fixed length. The input sequence is also sliced
    into smaller subsequences of fixed length and passed to an independent model for
    parallelization. A convolutional layer can extract local features, allowing the
    model to leverage hierarchical temporal information in textual data. A hybrid
    model of convolutional and LSTM architecture is proposed for text classification
    Wang et al. ([2019](#bib.bib266)). Two parallel convolutional layers are used
    to extract features from word embeddings, followed by max-pooling layers to reduce
    the feature dimensions. The reduced features are then concatenated and passed
    to LSTM for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Although CNN and RNN provide excellent results on text classification tasks,
    the models lack the ability to attend to specific words based on their importance
    and context. To address this limitation, attention mechanism is incorporated into
    the model to focus on the important features, enhancing the text classification
    accuracy. In Liu and Guo ([2019](#bib.bib143)), two attention modules are introduced
    to capture the contextual information of the feature sequence extracted by bi-directional
    LSTM. The first attention module attends the sequence in forward direction while
    the backward sequence is attended by the second attention module. The convolutional
    layers are used before the bi-directional LSTM to extract features from the word
    embedding. The attention modules require sequential processing using RNN-based
    architecture such as LSTM and GRU which may lead to information loss and distorted
    representations, particularly in long sequence. Furthermore, the attention modules
    focus on inter-sequence relationships between the input sequence and the target,
    ignoring the intra-sequence relationships or the dependencies between the words.
    In Lin et al. ([2017b](#bib.bib142)), the deep learning model is integrated with
    self-attention to capture the intra-sequence relationships between the features
    in the sequence. A multilayer of bi-directional LSTMs is utilized to extract feature
    sequence from the word embedding before the self-attention module attends the
    feature sequence to compute the attention weights. To further improve the overall
    performance, a multichannel features consisting of three input pipelines is introduced
    Li et al. ([2020b](#bib.bib134)). Each pipeline concatenates the word vector with
    a feature vector derived from the input sequence such as the word position, part-of-speech
    and word dependency parsing. The input pipeline is connected to bi-directional
    LSTM, followed by a self-attention module to learn the dependencies between the
    features in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer is a deep learning architecture that transforms sequential data
    using self-attention mechanisms, allowing long-range dependencies and complex
    patterns to be captured. The architecture is the basis of various advanced deep
    learning models and the Bi-directional Encoder Representations from Transformers
    popularly known as BERT is one of the examples that leverage transformer for pre-training
    on large scale textual data Devlin et al. ([2018](#bib.bib49)). BERT is a bi-directional
    transformer encoder that is designed for various NLP tasks, capable of capturing
    the contextual information from both preceding and succeeding words in the input
    sequence. Several improvements have been made to BERT to enhance its overall performance
    such as ALBERT Lan et al. ([2019](#bib.bib123)), RoBERTa Liu et al. ([2019b](#bib.bib150))
    and DeBERTa He et al. ([2020a](#bib.bib82)). The improvements are centered around
    refining the pre-training approaches such as dynamic masking of the training instances,
    training with a block of sentences and representing each input word using two
    vectors, both content and position of the word. Most of the recent works leverage
    BERT and its variants to capture effective feature representation of the input
    sequence. In Rodrawangpai and Daungjaiboon ([2022](#bib.bib208)), BERT and its
    variants are leveraged to capture the long-range dependencies of the input tokens.
    The features are then passed to a layer normalization and a linear fully-connected
    layer with dropout for classification. Similar work is reported in Murfi et al.
    ([2024](#bib.bib173)) whereby BERT is used to extract the features and the features
    are then passed to a hybrid of convolutional and recurrent neural networks. The
    traditional machine learning algorithms have been used to classify the features
    extracted by BERT Hao et al. ([2023](#bib.bib77)). The study shows machine learning
    algorithms can effectively leverage the rich contextual features extracted by
    BERT for downstream classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In text classification, the text labels can help in capturing the words relevant
    to the classification. The label-embedding attentive model is one of the earliest
    attempts to joint learn the label and word embeddings in the same latent space
    and measure the compatibility between labels and words using cosine similarity
    Wang et al. ([2018a](#bib.bib259)). The joint embedding allows the model to capture
    more effective text representations, increasing the overall performance of the
    model. LANTRN is a deep learning model that leverages label embedding extracted
    by BERT and entity information e.g. person name and organization name for text
    classification Yan et al. ([2023](#bib.bib283)). The entity recognition module
    is based on bi-directional LSTM and conditional random field layers to calculate
    the probability of each word in each entity label. The model introduces a label
    embedding bi-directional attention to learn the attention weights of token-label
    and sequence-label pairs. Furthermore, a transformer is introduced to learn local
    short-term dependencies of multiple short text sequences and long-term dependencies
    of the input sequence. Aspect refers to a specific attribute of an entity within
    the text and incorporating this information enhances the model’s understanding
    of the nuances of the text. BERT-MSL is a multi-semantic deep learning model with
    aspect-aware enhancement and four input pipelines: left sequence, right sequence,
    global sequence and aspect target Zhu et al. ([2023](#bib.bib306)). The aspect-aware
    enhancement module takes the features extracted by BERT, and performs average
    pooling followed by a linear transform. Then the output is concatenated with the
    outputs produced by the local and global semantic learning modules. The concatenated
    features are then jointly attended by a multi-head attention for text classification.
    Figure [24](#S4.F24 "Figure 24 ‣ 4.3.1 Text Classification ‣ 4.3 Natural Language
    Processing ‣ 4 Applications of Deep Learning ‣ A Survey on Deep Learning and State-of-the-art
    Applications") shows the architecture of BERT-MSL.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1e0f488a644e1d58e0edcd2e5ed2ed4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: The architecture of BERT-MSL Zhu et al. ([2023](#bib.bib306)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Neural Machine Translation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Neural machine translation (NMT) refers to the automated process of translating
    text from one language to another language. Numerous deep learning models have
    been proposed for NMT which can be categorized into RNN-based and CNN-based models.
    One of the first successful RNN-based models is the encoder-decoder Cho et al.
    ([2014](#bib.bib38)); Sutskever et al. ([2014](#bib.bib238)). The model consists
    of two connected subnetworks (the encoder and the decoder) for modelling the translation
    process as shown in Figure [25](#S4.F25 "Figure 25 ‣ 4.3.2 Neural Machine Translation
    ‣ 4.3 Natural Language Processing ‣ 4 Applications of Deep Learning ‣ A Survey
    on Deep Learning and State-of-the-art Applications"). The encoder reads the source
    sentence word by word and produces a fixed-length context vector (final hidden
    state). This process is known as source sentence encoding as shown in the figure.
    Given the context vector, the decoder generates the target sentence (translation)
    word by word. This modelling of the translation can be seen as a mapping between
    the source sentence to the target sentence via the intermediate context vector
    in the semantic space. The context vector represents the summary of the input
    sequence’s semantic meaning, providing a compressed representation that captures
    the essence of the source sentence. However, the compression process can sometimes
    result in the loss of information especially those early in the sequence. Bi-directional
    RNN may mitigate the loss of information by modelling the sequence in reverse
    order. However, the problem can still persist, particularly in cases where the
    input is a long sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d7d4afe950b7d8d5c521c1c3745131a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: The architecture of an encoder-decoder Stahlberg ([2020](#bib.bib233)).'
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanism was introduced to solve the problem of learning long input
    sequences Bahdanau et al. ([2014](#bib.bib17)). Attention alleviates this issue
    by attending on different words of the input sequences when predicting the target
    sequences at each time step. Unlike the standard encoder-decoder model, attention
    derives the context vector from the hidden states of both the encoder and decoder,
    and the alignment between the source and target. This mechanism allows the model
    to focus on the important words, increasing the overall accuracy of the translation.
    Several alignment score functions have been proposed for calculating the attention
    weights. Some of the popular functions are additive Bahdanau et al. ([2014](#bib.bib17)),
    dot-product, location-based Luong et al. ([2015](#bib.bib158)), and scaled dot-product
    Vaswani et al. ([2023](#bib.bib257)). The attention weights are calculated by
    attending to the entire hidden states of the encoder. This attention, also known
    as global attention, is computationally expensive. Instead of attending to all
    hidden states, local attention attends to a subset of hidden states, thus reducing
    the computational cost Luong et al. ([2015](#bib.bib158)). Google Neural Machine
    Translation is a popular encoder-decoder model with an attention mechanism that
    significantly improves the accuracy of machine translation Wu et al. ([2016](#bib.bib276)).
    As shown in Figure [26](#S4.F26 "Figure 26 ‣ 4.3.2 Neural Machine Translation
    ‣ 4.3 Natural Language Processing ‣ 4 Applications of Deep Learning ‣ A Survey
    on Deep Learning and State-of-the-art Applications"), the model consists of a
    multilayer of LSTMs with eight encoder and decoder layers and an attention connection
    between the bottom layer of the decoder to the top layer of the encoder. Furthermore,
    to deal with the challenging words to predict, a word is tokenized into subwords
    e.g. feud is broken down into fe and ud, allowing the model to generalize well
    to new and uncommon words. A year later, the self-attention mechanism was proposed,
    significantly improving the overall accuracy of machine translation Vaswani et al.
    ([2023](#bib.bib257)). Self-attention, also known as intra-attention, allows the
    deep learning model to capture the dependencies between the input words. The self-attention
    mechanism is the fundamental building block of the transformer model, which has
    since become a cornerstone in natural language processing and other domains.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8949af4d7ee546c453992f4de1a28ea9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: The architecture of Google Neural Machine Translation Wu et al.
    ([2016](#bib.bib276)).'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the success of transformer, the model falls short in capturing nuances
    of human language and struggles with tasks requiring deeper understanding of context.
    This can be especially challenging when the tasks involve formality, colloquialism,
    and subtle cultural references that may not directly equivalent in the target
    language, resulting in inaccurate translation or losing the original meaning.
    One of the approaches to include context into the input sequence is concatenating
    the current source sentence with the previous (context) sentences and feeding
    the whole input to the transformer Lupo et al. ([2023](#bib.bib159)). The model
    is trained to predict the translated sentence including the context translation.
    At inference time, only the translation is considered while the context translation
    is discarded. Furthermore, the approach encodes the sentence position and segment-shifted
    position to improve the distinction between current sentences and context sentences.
    In Rippeth et al. ([2023](#bib.bib207)), the source sentence is prefixed with
    the summary of the document to contextualize the input sentence. The summary is
    the set of salient words that represents the essence of the document, resolving
    ambiguity associated with the translation. A study was conducted to determine
    the optimal technique of aggregating contextual features Wu et al. ([2022](#bib.bib275)).
    Three techniques were studied namely concatenation mode, flat mode and hierarchical
    mode, and the experimental results show that concatenation mode achieved the best
    results. In Kim et al. ([2023](#bib.bib117)), a training method is introduced
    to train the deep learning machine translation model to generate translation involving
    honorific words. The training method indicates the honorific context in the target
    sentence using an honorific classifier to guide the model to attend to the related
    tokens. Unlike other studies where the context features are included by concatenation,
    the training method assigns weights to the context tokens indicated by the honorific
    classifier. This allows the model to generate a more accurate translation with
    honorifics. Finally, the performance of transformer relies on large-scale training
    data. However, for the vast majority of languages, only limited amounts of training
    data exist. To mitigate this problem, recent studies introduce shallow transformer
    architectures Gezmu and Nürnberger ([2022](#bib.bib65)), explore the effect of
    hyperparameter finetuning Araabi and Monz ([2020](#bib.bib13)), exploiting monolingual
    corpus to enhance the bilingual dataset for model training Li et al. ([2024](#bib.bib128))
    and leveraging visual input as contextual information for the translation task
    Meetei et al. ([2023](#bib.bib166)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Text Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Text generation refers to the process of creating texts based on a given input
    whereby the input can be in the form of texts, images, graphs, tables or even
    tabular data. Due to the various forms of inputs, text generation has a wide range
    of applications, including creative writing, image captioning and music generation.
    This section focuses on the progress made in text-to-text generation tasks such
    as question answering, dialogue generation and text summarization. The recurrent
    neural network and its variants play an important role in text generation tasks
    for their strong ability to model sequential data. One of the earliest works on
    question answering is based on the RNN-based encoder-decoder model whereby the
    encoder takes the question embedding and processes it using bi-directional LSTM,
    and the decoder generates the corresponding answer Nie et al. ([2017](#bib.bib180)).
    Additionally, to prevent semantic loss and enable the model to focus on the important
    words in the input sequence, a convolution operation is applied to the word embedding,
    and an attention mechanism is then used to attend to the output of the convolution
    operation. Similar work is reported in Yin et al. ([2015](#bib.bib288)) in which
    a knowledge-based module is introduced to calculate the relevance score between
    the question and the relevant facts in the knowledge base. This improves the text
    (answer) generation by the decoder. Another work is described in Li et al. ([2016](#bib.bib130))
    where an encoder-decoder with attention for dialogue generation is optimized using
    reinforcement learning. The model is first trained in supervised learning manner
    and then improved using the policy gradient method to diversify the responses.
    Ambiguous content in question answering sentences is a challenge in text generation
    and can lead to incorrect and uncertain responses. Cross-sentence context aware
    bi-directional model introduces a parallel attention module to compute the co-attention
    weights at the sentence level, accounting for the relationships and similarities
    in the question and the answer Wu et al. ([2020](#bib.bib274)).
  prefs: []
  type: TYPE_NORMAL
- en: The transformer has been leveraged for text generation tasks. An incremental
    transformer-based encoder is proposed to incrementally encode the historical sequence
    of conversations Li et al. ([2019b](#bib.bib138)). The decoder is a two-pass decoder
    that is based on the deliberation network, generates the next sentence. The first
    pass focuses on contextual coherence of the conversations while the second pass
    refines the output of the first pass. BERT and ALBERT have been used as pre-trained
    models for question answering task Alrowili and Vijay-Shanker ([2021](#bib.bib8)).
    The study found that the performance of the models is sensitive to random assignment
    of the initial weights especially on small datasets Alrowili and Vijay-Shanker
    ([2022](#bib.bib9)). T-BERTSum is a model based on BERT, designed to address the
    challenge of long text dependence and leveraging latent topic mapping in text
    summarization Ma et al. ([2021](#bib.bib161)). The mode integrates a neural topic
    module to infer topics and guide summarization, uses a transformer network to
    capture long-range dependencies and incorporates multilayers of LSTM for information
    filtering. Exploiting domain knowledge is essential in reducing the semantic gap
    between the deep learning models and the text corpus. KeBioSum is a knowledge
    infusion framework to inject domain knowledge into the pre-trained BERTs for text
    summarization Xie et al. ([2022](#bib.bib279)). In the framework, the relevant
    information is detected and extracted from the domain knowledge, generating label
    sequences of the sentences. The label data is then used to train the text summarization
    model using discriminative and generative training approaches, infusing the knowledge
    into the model.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Challenges and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.0.1 Availability and Quality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Building and employing deep learning models face several challenges. The training
    of deep learning requires a large number of instances (examples) to achieve high
    accuracy and generalization Munappy et al. ([2022](#bib.bib172)). Furthermore,
    the complexity of deep neural networks may lead to overfitting, where the model
    performs well on training data but fails to generalize on new, unseen data. This
    phenomenon frequently arises when the models is trained on insufficient data,
    highlighting the importance of diverse and extensive datasets. However, the data
    collection and annotation are time consuming and often require domain experts,
    specialized training and standardization Luca et al. ([2022](#bib.bib157)). Moreover,
    this process is prone to error and has the risk of introducing biases into the
    dataset which can significantly impact the performance of the trained model. One
    of the approaches to address this issue is transfer learning. Transfer learning
    involves the use of a deep learning model (known as pre-trained model) that is
    trained on a large dataset for solving a specific task (with a small dataset)
    Zhuang et al. ([2020](#bib.bib307)). The pre-trained model serves as a basis for
    the model training by fine-tuning the weights of the pre-trained model and adapting
    it to the new prediction task. This approach helps to mitigate the lack of training
    data in the target domain. Furthermore, transfer learning reduces computational
    resources required to train the model and helps faster convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach that can be employed to address the lack of data is data augmentation.
    Data augmentation is a convenient method that increases the number of instances
    by performing transformation functions on the existing instances without changing
    the labels Mumuni and Mumuni ([2022](#bib.bib171)). In the domain of computer
    vision, image transformation such as rotation, translation and cropping. However,
    it is important to consider the output of the transformation because the resultant
    may not represent the actual data. For example, flipping or adding noise to a
    signal may introduce distortion or changing the characteristics (trend, seasonality
    and cyclic variations) of the signal. Thus, careful consideration must be given
    to ensure that the generated instances still accurately represent the underlying
    patterns present in the data. Data augmentation can also be realized by generating
    synthetic data to supplement the training set. Synthetic data is artificially
    created data that resembles real data but is generated using statistical methods
    or deep generative models Hu et al. ([2023](#bib.bib93)); Murtaza et al. ([2023](#bib.bib174)).
    The generated data can complement the less-diverse, limited datasets, providing
    a broader range of examples for the model to learn from. However, generating synthetic
    data that accurately reflects the characteristics of the real-world data is challenging.
    Careful consideration must be given to the choice of models and parameters used
    to ensure the synthetic data is realistic and representative of the real-world
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 5.0.2 Interpretability and Explainability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Interpretability and explainability is crucial for building trust and understanding
    how predictive models make decisions especially in high-stake applications such
    as healthcare and medical image analysis Tonekaboni et al. ([2019](#bib.bib253)).
    However, as deep learning models become more intricate and complex with numerous
    layers, subnetworks and a large number of parameters, the models are often perceived
    as a “black box” and difficult to explain in terms of decision-making processes.
    Therefore, it is crucial for the researchers to focus on methods that provide
    insights into how a deep learning model performs the prediction and how its decisions
    are influenced by the input data, making it more transparent and trustworthy.
    Numerous methods have been proposed for interpreting and explaining the decisions
    of deep learning models which can be categorized into visualization (feature attribution),
    model distillation and intrinsic (explainable by itself). Visualization methods
    involve the use of scientific visualization such as saliency maps or heatmaps
    to express the explanation by highlighting the degree of association between the
    inputs and the predictions Tjoa et al. ([2023](#bib.bib252)). The heatmaps identify
    the saliency of the input features influencing the model’s predictions. The visualization
    approach is simple and intuitive and can be applied to tabular data and image
    data. Furthermore, it can be used to identify and debug issues in deep learning
    models, leading to improved performance and robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Model distillation is an approach to approximating a complex model by fitting
    a simpler model using the training set. The simpler model is built typically using
    a simpler or interpretable algorithm such as linear regression, decision tree
    or rule-based methods Li and Shen ([2024](#bib.bib135)). In this approach, the
    simpler model is trained to resemble the predictive behavior of the complex model.
    Then, the simpler model may serve as the proxy or surrogate model for explaining
    the complex model. Model distillation can be used together with visualization
    to further enhance the interpretability of the complex model Termritthikun et al.
    ([2023](#bib.bib248)). Model distillation seeks explanations of the models that
    were never designed to be explainable. Ideally, the explanation of a deep learning
    model’s prediction should be included as part of the model output, or the explanation
    can be derived from the architecture of the model. This is because an intrinsic
    model can learn not only the mapping between the input and output, but also generate
    an explanation of the prediction that is faithful to the model’s behavior. Attention
    mechanisms are the key to this approach, providing a form of attention weights
    that can be used to explain why the model made a particular decision Xiong et al.
    ([2022](#bib.bib281)). Another type of intrinsic approach is to train the model
    to simultaneously perform the prediction task and generate the explanation for
    its predictions Fernandes et al. ([2023](#bib.bib60)). This “additional task”
    can be in the form of a text explanation or model prototype which embeds the semantic
    meaning of the prediction. However, the intrinsic approach is more difficult to
    apply because the user needs additional knowledge and understanding of the model’s
    architecture and inner workings.
  prefs: []
  type: TYPE_NORMAL
- en: 5.0.3 Ethics and Fairness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deep learning models are increasingly being deployed in making high-stake decision
    including recruitment Freire and de Castro ([2021](#bib.bib61)), criminal justice
    Dass et al. ([2023](#bib.bib45)) and credit scoring Gicić et al. ([2023](#bib.bib66)).
    There are several advantages of deep learning-based systems in which, unlike humans,
    machines are able to process vast amounts of data and applications quickly and
    consistently. However, deep learning-based systems have the risk of being prone
    to biases present in the data used for training which can lead to unfairness and
    injustice. Numerous efforts have been made to mitigate this issue which can be
    categorized into modelling bias detection and modelling bias mitigation. Detection
    of modelling bias refers to the process of identifying and quantifying biases
    that may present in predictive models. This approach involves the use of statistical
    analysis, fairness metrics, counterfactual testing and human review to detect
    bias in the models. For instance, visualization-based methods such as attribution
    maps are used to indicate which regions are significant to the predictions Schaaf
    et al. ([2021](#bib.bib213)). This in turn can be used to detect and quantify
    bias using metrics such as Relevance Mass Accuracy, Relevance Rank Accuracy Accuracy
    and or Area over the perturbation curve (AOPC). In Giloni et al. ([2022](#bib.bib67)),
    two modules are presented for estimating bias in predictive models. The first
    module utilizes an unsupervised deep neural network with a custom loss function
    to generate hidden representation of the input data called bias vectors, revealing
    the underlying bias of each feature. The second module combines these bias vectors
    into a single vector representing the bias estimation of each feature, achieved
    by aggregating them using the absolute averaging operation. Bias mitigation refers
    to the process of reducing the presence of bias in predictive models, which can
    be done in three stages. The first stage combats bias by modifying the training
    data, either relabeling the labels or perturbing the feature values Iosifidis
    et al. ([2019](#bib.bib105)); Kehrenberg et al. ([2020](#bib.bib114)). The second
    stage addresses bias during the training of the model by applying regularization
    terms to the loss function to penalize discrimination. In Jain et al. ([2023](#bib.bib107)),
    a loss function based on bias parity score (BPS) is introduced to measure the
    degree of similarity of a statistical measure such as accuracy across different
    subgroups. The BPS term is added to the loss function as a regularizer to the
    original prediction task. The last stage mitigates bias after the predictive models
    have been successfully trained. This stage applies post-processing approaches
    such as reinforcement learning to obtain a fairer model Yang et al. ([2023](#bib.bib286)).
    For instance, the detection of minority classes is rewarded to prevent bias towards
    the majority class. This allows the model the generalize well across different
    patient demographics.
  prefs: []
  type: TYPE_NORMAL
- en: 5.0.4 Lightweight Deep Learning Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Even though deep learning architectures have achieved state-of-the-art across
    various computer vision tasks, they often come with large model parameters Ige
    and Mohd Noor ([2023](#bib.bib100)). The architecture and complexity of a deep
    learning network determine the number of model parameters. The deeper the network,
    the larger the number of model parameters. However, deep learning models with
    large parameters often suffer limitations when deploying on end devices. For instance,
    a deep learning model developed for security monitoring by analyzing video data
    using 3D-CNNs might suffer deployment issues when deploying such models on low-resourced
    systems like smartphones or small-scale IoT devices. Model training and inference
    for deep learning models with large parameters demands substantial processing
    power. As the number of parameters increases, so does the computational complexity,
    resulting in longer model training duration and more hardware needs. Also, large
    parameter sizes translate to increased memory requirements, limiting their deployment
    on end devices. This is because these end devices often have battery, processor
    or memory capacity limitations. To address these challenges, it is important to
    develop sophisticated but lightweight architectures that can achieve state-of-the-art
    with few model parameters. Such lightweight models will be characterized by their
    ability to deliver competitive performance while mitigating computational complexity
    and memory requirements, making them well-suited for deployment on resource-constrained
    devices. An approach would be to develop novel lightweight plug-and-play modules
    that can be plugged to few layered deep learning architectures to improve feature
    learning without incurring additional model complexity. Other approaches could
    involve leveraging model compression techniques to reduce the size and computational
    complexity of deep learning models. Researchers can focus on improving pruning
    methods Li et al. ([2019a](#bib.bib131)), which can identify and eliminate redundant
    parameters or connections, thereby reducing the model’s footprint without compromising
    performance. Also, quantization techniques Yang et al. ([2019](#bib.bib285)) can
    be further explored to reduce the precision of weights and activations, therefore,
    enabling efficient representation with lower memory requirements. Also, knowledge
    distillation techniques Stanton et al. ([2021](#bib.bib234)) can be further investigated
    to facilitate the transfer of knowledge from a complex teacher model to a simpler
    student model, therefore, enabling compact yet effective representations. These
    areas are still open to contributions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.0.5 Adversarial Attack and Defense
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Adversarial attacks and defense mechanisms in deep learning represent a critical
    area of research and development, particularly as deep learning models become
    increasingly integrated into various applications. Adversarial attacks involves
    the deliberate manipulation of input data to mislead or deceive deep learning
    models, leading to incorrect predictions or behavior Akhtar and Mian ([2018](#bib.bib5)).
    Szegedy et al. Szegedy et al. ([2013](#bib.bib240)) was the first to identify
    this intriguing shortcoming of deep neural networks in image classification. They
    showed that even with their great accuracy, deep learning models are surprisingly
    vulnerable to adversarial attacks that take the form of tiny image changes that
    are (almost) invisible to human vision systems. A neural network classifier may
    radically alter its prediction about an image as a result of such an attack. Also,
    such a model can indicate high confidence in wrong predictions, which can be catastrophic
    for deep learning models deployed in medical or security fields, among many others.
    In generative models, several studies have investigated how adversarial attacks
    affect autoencoders and GANs, as seen in Tabacof et al. Tabacof et al. ([2016](#bib.bib242))
    where a method to manipulate input images in a way that deceives variational autoencoders
    into reconstructing a totally different image was introduced. In recent times,
    the focus of adversarial attack research has been on images, but studies has shown
    that adversarial attacks are not limited to image data; they can also affect other
    types of data such as text, signals, audio, and video Zhang et al. ([2020b](#bib.bib297));
    Jiang et al. ([2019](#bib.bib110)); Esmaeilpour et al. ([2019](#bib.bib58)). Future
    research can focus on exploring adversarial attacks in these domains and developing
    tailored defense mechanisms. Also, researchers can further investigate the practical
    implications of adversarial attacks in real-world scenarios, such as in autonomous
    vehicles, medical imaging, and cybersecurity. Understanding the potential impact
    of adversarial attacks in these applications can inform the development of more
    robust and secure systems.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning has become the prominent data-driven approach in various state-of-the-art
    applications. Its importance lies in its ability to revolutionize many aspects
    of research and industries and tackle complex problems which were once impossible
    to overcome. Numerous surveys have been published on deep learning, reviewing
    the concepts, model architectures and applications. However, the studies do not
    discuss the emerging trends in the state-of-the-art applications of deep learning
    and emphasise the important traits and elements in the models. This paper presents
    a structured and comprehensive survey of deep learning, focusing on the latest
    trends and advancements in state-of-the-art applications such as computer vision,
    natural language processing, time series analysis and pervasive computing. The
    survey explores key elements and traits in modern deep learning models, highlighting
    their significance in addressing complex challenges across diverse domains. Furthermore,
    this paper presents a comprehensive review of the deep learning fundamentals,
    which is essential for understanding the core principles behind modern deep learning
    models. The survey finishes by discussing the critical challenges and future directions
    in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This work has been supported in part by the Ministry of Higher Education Malaysia
    for Fundamental Research Grant Scheme with Project Code: FRGS/1/2023/ICT02/USM/02/2.'
  prefs: []
  type: TYPE_NORMAL
- en: Declaration of competing interest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors declare that they have no known competing financial interests or
    personal relationships that could have appeared to influence the work reported
    in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: Data availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No data was used for the research described in the article.
  prefs: []
  type: TYPE_NORMAL
- en: \printcredits
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abdulwahhab et al. (2024) Abdulwahhab, A.H., Abdulaal, A.H., Thary Al-Ghrairi,
    A.H., Mohammed, A.A., Valizadeh, M., 2024. Detection of epileptic seizure using
    eeg signals analysis based on deep learning techniques. Chaos, Solitons and Fractals
    181, 114700. URL: [https://www.sciencedirect.com/science/article/pii/S0960077924002522](https://www.sciencedirect.com/science/article/pii/S0960077924002522),
    doi:[https://doi.org/10.1016/j.chaos.2024.114700](https:/doi.org/https://doi.org/10.1016/j.chaos.2024.114700).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abedin et al. (2021) Abedin, M.Z., Moon, M.H., Hassan, M.K., Hajek, P., 2021.
    Deep learning-based exchange rate prediction during the covid-19 pandemic. Annals
    of Operations Research , 1–52.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acharya et al. (2017) Acharya, U.R., Oh, S.L., Hagiwara, Y., Tan, J.H., Adam,
    M., Gertych, A., San Tan, R., 2017. A deep convolutional neural network model
    to classify heartbeats. Computers in biology and medicine 89, 389–396.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ahnouch et al. (2023) Ahnouch, M., Elaachak, L., Ghadi, A., 2023. Model risk
    in financial derivatives and the transformative impact of deep learning: A systematic
    review, in: The Proceedings of the International Conference on Smart City Applications,
    Springer. pp. 155–165.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Akhtar and Mian (2018) Akhtar, N., Mian, A., 2018. Threat of adversarial attacks
    on deep learning in computer vision: A survey. IEEE Access 6, 14410–14430. doi:[10.1109/ACCESS.2018.2807385](https:/doi.org/10.1109/ACCESS.2018.2807385).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alnuaim et al. (2022) Alnuaim, A.A., Zakariah, M., Shashidhar, C., Hatamleh,
    W.A., Tarazi, H., Shukla, P.K., Ratna, R., 2022. Speaker gender recognition based
    on deep neural networks and resnet50. Wireless Communications and Mobile Computing
    2022, 1–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alom et al. (2019) Alom, M.Z., Taha, T.M., Yakopcic, C., Westberg, S., Sidike,
    P., Nasrin, M.S., Hasan, M., Van Essen, B.C., Awwal, A.A., Asari, V.K., 2019.
    A state-of-the-art survey on deep learning theory and architectures. electronics
    8, 292. Publisher: Multidisciplinary Digital Publishing Institute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alrowili and Vijay-Shanker (2021) Alrowili, S., Vijay-Shanker, K., 2021. BioM-transformers:
    building large biomedical language models with BERT, ALBERT and ELECTRA, in: Proceedings
    of the 20th workshop on biomedical language processing, pp. 221–227.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alrowili and Vijay-Shanker (2022) Alrowili, S., Vijay-Shanker, K., 2022. Exploring
    Biomedical Question Answering with BioM-Transformers At BioASQ10B challenge: Findings
    and Techniques., in: CLEF (Working Notes), pp. 222–234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Altaheri et al. (2023) Altaheri, H., Muhammad, G., Alsulaiman, M., Amin, S.U.,
    Altuwaijri, G.A., Abdul, W., Bencherif, M.A., Faisal, M., 2023. Deep learning
    techniques for classification of electroencephalogram (eeg) motor imagery (mi)
    signals: A review. Neural Computing and Applications 35, 14681–14722.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alzubaidi et al. (2021) Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili,
    A., Duan, Y., Al-Shamma, O., Santamaría, J., Fadhel, M.A., Al-Amidie, M., Farhan,
    L., 2021. Review of deep learning: concepts, CNN architectures, challenges, applications,
    future directions. Journal of big Data 8, 1–74. Publisher: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ang and Guan (2017) Ang, K.K., Guan, C., 2017. Eeg-based strategies to detect
    motor imagery for control and rehabilitation. IEEE Transactions on Neural Systems
    and Rehabilitation Engineering 25, 392–401. doi:[10.1109/TNSRE.2016.2646763](https:/doi.org/10.1109/TNSRE.2016.2646763).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Araabi and Monz (2020) Araabi, A., Monz, C., 2020. Optimizing Transformer for
    Low-Resource Neural Machine Translation, in: Scott, D., Bel, N., Zong, C. (Eds.),
    Proceedings of the 28th International Conference on Computational Linguistics,
    International Committee on Computational Linguistics, Barcelona, Spain (Online).
    pp. 3429–3435. URL: [https://aclanthology.org/2020.coling-main.304](https://aclanthology.org/2020.coling-main.304),
    doi:[10.18653/v1/2020.coling-main.304](https:/doi.org/10.18653/v1/2020.coling-main.304).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arevian (2007) Arevian, G., 2007. Recurrent Neural Networks for Robust Real-World
    Text Classification, in: IEEE/WIC/ACM International Conference on Web Intelligence
    (WI’07), pp. 326–329. URL: [https://ieeexplore.ieee.org/abstract/document/4427112](https://ieeexplore.ieee.org/abstract/document/4427112),
    doi:[10.1109/WI.2007.126](https:/doi.org/10.1109/WI.2007.126).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ba et al. (2016) Ba, J., Kiros, J., Hinton, G.E., 2016. Layer Normalization.
    ArXiv URL: [https://www.semanticscholar.org/paper/Layer-Normalization-Ba-Kiros/97fb4e3d45bb098e27e0071448b6152217bd35a5](https://www.semanticscholar.org/paper/Layer-Normalization-Ba-Kiros/97fb4e3d45bb098e27e0071448b6152217bd35a5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Badrinarayanan et al. (2017) Badrinarayanan, V., Kendall, A., Cipolla, R.,
    2017. Segnet: A deep convolutional encoder-decoder architecture for image segmentation.
    IEEE transactions on pattern analysis and machine intelligence 39, 2481–2495.
    Publisher: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2014) Bahdanau, D., Cho, K., Bengio, Y., 2014. Neural machine
    translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baloglu et al. (2019) Baloglu, U.B., Talo, M., Yildirim, O., San Tan, R., Acharya,
    U.R., 2019. Classification of myocardial infarction with multi-lead ecg signals
    and deep cnn. Pattern recognition letters 122, 23–30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee et al. (2020) Banerjee, R., Ghose, A., Muthana Mandana, K., 2020.
    A hybrid cnn-lstm architecture for detection of coronary artery disease from ecg,
    in: 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1–8. doi:[10.1109/IJCNN48605.2020.9207044](https:/doi.org/10.1109/IJCNN48605.2020.9207044).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banjarey et al. (2022) Banjarey, K., Sahu, S.P., Dewangan, D.K., 2022. Human
    activity recognition using 1d convolutional neural network, in: Sentimental Analysis
    and Deep Learning: Proceedings of ICSADL 2021, Springer. pp. 691–702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bochkovskiy et al. (2020) Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M., 2020.
    Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bodla et al. (2017) Bodla, N., Singh, B., Chellappa, R., Davis, L.S., 2017.
    Soft-NMS–improving object detection with one line of code, in: Proceedings of
    the IEEE international conference on computer vision, pp. 5561–5569.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boonyakitanont et al. (2020) Boonyakitanont, P., Lek-Uthai, A., Chomtho, K.,
    Songsiri, J., 2020. A review of feature extraction and performance evaluation
    in epileptic seizure detection using eeg. Biomedical Signal Processing and Control
    57, 101702.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brunner et al. (2007) Brunner, C., Naeem, M., Leeb, R., Graimann, B., Pfurtscheller,
    G., 2007. Spatial filtering and selection of optimized components in four class
    motor imagery eeg data using independent components analysis. Pattern recognition
    letters 28, 957–964.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2018) Cai, S., Feng, X., Deng, Z., Ming, Z., Shan, Z., 2018. Financial
    news quantization and stock market forecast research based on cnn and lstm, in:
    Smart Computing and Communication: Third International Conference, SmartCom 2018,
    Tokyo, Japan, December 10–12, 2018, Proceedings 3, Springer. pp. 366–375.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2019) Cai, Y., Wang, X., Yu, Z., Li, F., Xu, P., Li, Y., Li, L.,
    2019. Dualattn-GAN: Text to Image Synthesis With Dual Attentional Generative Adversarial
    Network. IEEE Access 7, 183706–183716. URL: [https://ieeexplore.ieee.org/document/8930532?denied=](https://ieeexplore.ieee.org/document/8930532?denied=),
    doi:[10.1109/ACCESS.2019.2958864](https:/doi.org/10.1109/ACCESS.2019.2958864).
    conference Name: IEEE Access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calvo and Ceccatto (2000) Calvo, R.A., Ceccatto, H.A., 2000. Intelligent document
    classification. Intelligent Data Analysis 4, 411–420. URL: [https://content.iospress.com/articles/intelligent-data-analysis/ida00028](https://content.iospress.com/articles/intelligent-data-analysis/ida00028),
    doi:[10.3233/IDA-2000-4503](https:/doi.org/10.3233/IDA-2000-4503). publisher:
    IOS Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carion et al. (2020) Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,
    A., Zagoruyko, S., 2020. End-to-End Object Detection with Transformers, in: Vedaldi,
    A., Bischof, H., Brox, T., Frahm, J.M. (Eds.), Computer Vision – ECCV 2020, Springer
    International Publishing, Cham. pp. 213–229. doi:[10.1007/978-3-030-58452-8_13](https:/doi.org/10.1007/978-3-030-58452-8_13).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chan et al. (2015) Chan, W., Jaitly, N., Le, Q.V., Vinyals, O., 2015. Listen,
    attend and spell. arXiv preprint arXiv:1508.01211 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chaurasia and Culurciello (2017) Chaurasia, A., Culurciello, E., 2017. Linknet:
    Exploiting encoder representations for efficient semantic segmentation, in: 2017
    IEEE visual communications and image processing (VCIP), IEEE. pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022a) Chen, C.Y., Lin, Y.T., Lee, S.J., Tsai, W.C., Huang, T.C.,
    Liu, Y.H., Cheng, M.C., Dai, C.Y., 2022a. Automated ecg classification based on
    1d deep learning network. Methods 202, 127–135. URL: [https://www.sciencedirect.com/science/article/pii/S1046202321001134](https://www.sciencedirect.com/science/article/pii/S1046202321001134),
    doi:[https://doi.org/10.1016/j.ymeth.2021.04.021](https:/doi.org/https://doi.org/10.1016/j.ymeth.2021.04.021).
    machine Learning Methods for Bio-Medical Image and Signal Processing: Recent Advances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022b) Chen, D., Yongchareon, S., Lai, E.M.K., Yu, J., Sheng, Q.Z.,
    Li, Y., 2022b. Transformer with bidirectional gru for nonintrusive, sensor-based
    activity recognition in a multiresident environment. IEEE Internet of Things Journal
    9, 23716–23727.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2024) Chen, L., Pelger, M., Zhu, J., 2024. Deep learning in asset
    pricing. Management Science 70, 714–750.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2018) Chen, L.C., Hermans, A., Papandreou, G., Schroff, F., Wang,
    P., Adam, H., 2018. MaskLab: Instance segmentation by refining object detection
    with semantic and direction features, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp. 4013–4022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2012) Chen, M., Xu, Z., Weinberger, K.Q., Sha, F., 2012. Marginalized
    denoising autoencoders for domain adaptation, in: Proceedings of the 29th International
    Coference on International Conference on Machine Learning, Omnipress, Madison,
    WI, USA. pp. 1627–1634.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2016) Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever,
    I., Abbeel, P., 2016. InfoGAN: Interpretable Representation Learning by Information
    Maximizing Generative Adversarial Nets. URL: [http://arxiv.org/abs/1606.03657](http://arxiv.org/abs/1606.03657),
    doi:[10.48550/arXiv.1606.03657](https:/doi.org/10.48550/arXiv.1606.03657). arXiv:1606.03657
    [cs, stat] version: 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiu et al. (2018) Chiu, C.C., Sainath, T.N., Wu, Y., Prabhavalkar, R., Nguyen,
    P., Chen, Z., Kannan, A., Weiss, R.J., Rao, K., Gonina, E., Jaitly, N., Li, B.,
    Chorowski, J., Bacchiani, M., 2018. State-of-the-art speech recognition with sequence-to-sequence
    models, in: 2018 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP), pp. 4774–4778. doi:[10.1109/ICASSP.2018.8462105](https:/doi.org/10.1109/ICASSP.2018.8462105).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cho et al. (2014) Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D.,
    Bougares, F., Schwenk, H., Bengio, Y., 2014. Learning Phrase Representations using
    RNN Encoder–Decoder for Statistical Machine Translation, in: Moschitti, A., Pang,
    B., Daelemans, W. (Eds.), Proceedings of the 2014 Conference on Empirical Methods
    in Natural Language Processing (EMNLP), Association for Computational Linguistics,
    Doha, Qatar. pp. 1724–1734. URL: [https://aclanthology.org/D14-1179](https://aclanthology.org/D14-1179),
    doi:[10.3115/v1/D14-1179](https:/doi.org/10.3115/v1/D14-1179).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clevert et al. (2016) Clevert, D.A., Unterthiner, T., Hochreiter, S., 2016.
    Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). URL:
    [http://arxiv.org/abs/1511.07289](http://arxiv.org/abs/1511.07289), doi:[10.48550/arXiv.1511.07289](https:/doi.org/10.48550/arXiv.1511.07289).
    arXiv:1511.07289 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crippa et al. (2015) Crippa, P., Curzi, A., Falaschetti, L., Turchetti, C.,
    et al., 2015. Multi-class ecg beat classification based on a gaussian mixture
    model of karhunen-loève transform. Int. J. Simul. Syst. Sci. Technol 16, 2–1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cybenko (1989) Cybenko, G., 1989. Approximation by superpositions of a sigmoidal
    function. Mathematics of Control, Signals and Systems 2, 303–314. URL: [https://doi.org/10.1007/BF02551274](https://doi.org/10.1007/BF02551274),
    doi:[10.1007/BF02551274](https:/doi.org/10.1007/BF02551274).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. (2019) Dai, M., Zheng, D., Na, R., Wang, S., Zhang, S., 2019. Eeg
    classification of motor imagery using a novel deep learning framework. Sensors
    19, 551.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2021) Dai, X., Chen, Y., Yang, J., Zhang, P., Yuan, L., Zhang,
    L., 2021. Dynamic DETR: End-to-End Object Detection With Dynamic Attention, pp.
    2988–2997. URL: [https://openaccess.thecvf.com/content/ICCV2021/html/Dai_Dynamic_DETR_End-to-End_Object_Detection_With_Dynamic_Attention_ICCV_2021_paper.html?ref=https://githubhelp.com](https://openaccess.thecvf.com/content/ICCV2021/html/Dai_Dynamic_DETR_End-to-End_Object_Detection_With_Dynamic_Attention_ICCV_2021_paper.html?ref=https://githubhelp.com).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dang et al. (2020) Dang, L.M., Min, K., Wang, H., Piran, M.J., Lee, C.H., Moon,
    H., 2020. Sensor-based and vision-based human activity recognition: A comprehensive
    survey. Pattern Recognition 108, 107561.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dass et al. (2023) Dass, R.K., Petersen, N., Omori, M., Lave, T.R., Visser,
    U., 2023. Detecting racial inequalities in criminal justice: towards an equitable
    deep learning approach for generating and interpreting racial categories using
    mugshots. AI & SOCIETY 38, 897–918. Publisher: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep and Zheng (2019) Deep, S., Zheng, X., 2019. Hybrid model featuring cnn
    and lstm architecture for human activity recognition on smartphone sensor data,
    in: 2019 20th international conference on parallel and distributed computing,
    applications and technologies (PDCAT), IEEE. pp. 259–264.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delorme et al. (2007) Delorme, A., Sejnowski, T., Makeig, S., 2007. Enhanced
    detection of artifacts in eeg data using higher-order statistics and independent
    component analysis. Neuroimage 34, 1443–1449.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Desai et al. (2021) Desai, M.P., Caffarena, G., Jevtic, R., Márquez, D.G., Otero,
    A., 2021. A low-latency, low-power fpga implementation of ecg signal characterization
    using hermite polynomials. Electronics 10, 2324.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2018.
    BERT: Pre-training of deep bidirectional transformers for language understanding.
    arXiv preprint arXiv:1810.04805 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dixit and Silakari (2021) Dixit, P., Silakari, S., 2021. Deep Learning Algorithms
    for Cybersecurity Applications: A Technological and Status Review. Computer Science
    Review 39, 100317. URL: [https://www.sciencedirect.com/science/article/pii/S1574013720304172](https://www.sciencedirect.com/science/article/pii/S1574013720304172),
    doi:[10.1016/j.cosrev.2020.100317](https:/doi.org/10.1016/j.cosrev.2020.100317).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2021) Dong, S., Wang, P., Abbas, K., 2021. A survey on deep learning
    and its applications. Computer Science Review 40, 100379. Publisher: Elsevier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., Uszkoreit, J., Houlsby, N., 2021. An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale. URL: [http://arxiv.org/abs/2010.11929](http://arxiv.org/abs/2010.11929),
    doi:[10.48550/arXiv.2010.11929](https:/doi.org/10.48550/arXiv.2010.11929). arXiv:2010.11929
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dua et al. (2023) Dua, N., Singh, S.N., Semwal, V.B., Challa, S.K., 2023. Inception
    inspired cnn-gru hybrid network for human activity recognition. Multimedia Tools
    and Applications 82, 5369–5403.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duchi et al. (2011) Duchi, J., Hazan, E., Singer, Y., 2011. Adaptive Subgradient
    Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning
    Research 12, 2121–2159. URL: [http://jmlr.org/papers/v12/duchi11a.html](http://jmlr.org/papers/v12/duchi11a.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dugas et al. (2000) Dugas, C., Bengio, Y., Bélisle, F., Nadeau, C., Garcia,
    R., 2000. Incorporating Second-Order Functional Knowledge for Better Option Pricing,
    in: Advances in Neural Information Processing Systems, MIT Press. URL: [https://papers.nips.cc/paper_files/paper/2000/hash/44968aece94f667e4095002d140b5896-Abstract.html](https://papers.nips.cc/paper_files/paper/2000/hash/44968aece94f667e4095002d140b5896-Abstract.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eapen et al. (2019) Eapen, J., Bein, D., Verma, A., 2019. Novel deep learning
    model with cnn and bi-directional lstm for improved stock market index prediction,
    in: 2019 IEEE 9th annual computing and communication workshop and conference (CCWC),
    IEEE. pp. 0264–0270.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Erdaş and Güney (2021) Erdaş, Ç.B., Güney, S., 2021. Human activity recognition
    by using different deep learning approaches for wearable sensors. Neural Processing
    Letters 53, 1795–1809.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esmaeilpour et al. (2019) Esmaeilpour, M., Cardinal, P., Koerich, A.L., 2019.
    A robust approach for securing audio classification against adversarial attacks.
    IEEE Transactions on information forensics and security 15, 2147–2159.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esteva et al. (2019) Esteva, A., Robicquet, A., Ramsundar, B., Kuleshov, V.,
    DePristo, M., Chou, K., Cui, C., Corrado, G., Thrun, S., Dean, J., 2019. A guide
    to deep learning in healthcare. Nature medicine 25, 24–29.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fernandes et al. (2023) Fernandes, L., Fernandes, J.N., Calado, M., Pinto,
    J.R., Cerqueira, R., Cardoso, J.S., 2023. Intrinsic Explainability for End-to-End
    Object Detection. IEEE Access Publisher: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Freire and de Castro (2021) Freire, M.N., de Castro, L.N., 2021. e-Recruitment
    recommender systems: a systematic review. Knowledge and Information Systems 63,
    1–20. Publisher: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2021a) Gao, W., Zhang, L., Teng, Q., He, J., Wu, H., 2021a. Danhar:
    Dual attention network for multimodal human activity recognition using wearable
    sensors. Applied Soft Computing 111, 107728.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2021b) Gao, Z., Dang, W., Wang, X., Hong, X., Hou, L., Ma, K., Perc,
    M., 2021b. Complex networks and deep learning for eeg signal analysis. Cognitive
    Neurodynamics 15, 369–388.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2018) Gao, Z., Xie, J., Wang, Q., Li, P., 2018. Global Second-order
    Pooling Convolutional Networks. URL: [http://arxiv.org/abs/1811.12006](http://arxiv.org/abs/1811.12006),
    doi:[10.48550/arXiv.1811.12006](https:/doi.org/10.48550/arXiv.1811.12006). arXiv:1811.12006
    [cs] version: 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gezmu and Nürnberger (2022) Gezmu, A.M., Nürnberger, A., 2022. Transformers
    for Low-resource Neural Machine Translation., in: ICAART (1), pp. 459–466.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gicić et al. (2023) Gicić, A., Donko, D., Subasi, A., 2023. Intelligent credit
    scoring using deep learning methods. Concurrency and Computation: Practice and
    Experience 35, e7637.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Giloni et al. (2022) Giloni, A., Grolman, E., Hagemann, T., Fromm, R., Fischer,
    S., Elovici, Y., Shabtai, A., 2022. BENN: Bias Estimation Using a Deep Neural
    Network. IEEE Transactions on Neural Networks and Learning Systems Publisher:
    IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Girshick (2015) Girshick, R., 2015. Fast R-CNN, in: 2015 IEEE International
    Conference on Computer Vision (ICCV), pp. 1440–1448. URL: [https://ieeexplore.ieee.org/document/7410526](https://ieeexplore.ieee.org/document/7410526),
    doi:[10.1109/ICCV.2015.169](https:/doi.org/10.1109/ICCV.2015.169). iSSN: 2380-7504.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Girshick et al. (2014) Girshick, R., Donahue, J., Darrell, T., Malik, J., 2014.
    Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,
    pp. 580–587. URL: [https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html](https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Glorot et al. (2011) Glorot, X., Bordes, A., Bengio, Y., 2011. Deep Sparse
    Rectifier Neural Networks, in: Proceedings of the Fourteenth International Conference
    on Artificial Intelligence and Statistics, JMLR Workshop and Conference Proceedings.
    pp. 315–323. URL: [https://proceedings.mlr.press/v15/glorot11a.html](https://proceedings.mlr.press/v15/glorot11a.html).
    iSSN: 1938-7228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
    Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014. Generative adversarial
    nets. Advances in neural information processing systems 27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves (2012) Graves, A., 2012. Sequence transduction with recurrent neural
    networks. arXiv preprint arXiv:1211.3711 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gudelek et al. (2017) Gudelek, M.U., Boluk, S.A., Ozbayoglu, A.M., 2017. A
    deep learning based stock trading model with 2-d cnn trend detection, in: 2017
    IEEE symposium series on computational intelligence (SSCI), IEEE. pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2023) Guo, M.H., Liu, Z.N., Mu, T.J., Hu, S.M., 2023. Beyond Self-Attention:
    External Attention Using Two Linear Layers for Visual Tasks. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 45, 5436–5447. URL: [https://ieeexplore.ieee.org/document/9912362](https://ieeexplore.ieee.org/document/9912362),
    doi:[10.1109/TPAMI.2022.3211006](https:/doi.org/10.1109/TPAMI.2022.3211006). conference
    Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta (2021) Gupta, S., 2021. Deep learning based human activity recognition
    (har) using wearable sensor data. International Journal of Information Management
    Data Insights 1, 100046.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2022) Han, C., Zhang, L., Tang, Y., Huang, W., Min, F., He, J.,
    2022. Human activity recognition using wearable sensors by heterogeneous convolutional
    neural networks. Expert Systems with Applications 198, 116764.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hao et al. (2023) Hao, S., Zhang, P., Liu, S., Wang, Y., 2023. Sentiment recognition
    and analysis method of official document text based on BERT–SVM model. Neural
    Computing and Applications 35, 24621–24632. URL: [https://doi.org/10.1007/s00521-023-08226-4](https://doi.org/10.1007/s00521-023-08226-4),
    doi:[10.1007/s00521-023-08226-4](https:/doi.org/10.1007/s00521-023-08226-4).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hassouneh et al. (2020) Hassouneh, A., Mutawa, A., Murugappan, M., 2020. Development
    of a real-time emotion recognition system using facial expressions and eeg based
    on machine learning and deep neural network methods. Informatics in Medicine Unlocked
    20, 100372.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hatamizadeh et al. (2023) Hatamizadeh, A., Yin, H., Heinrich, G., Kautz, J.,
    Molchanov, P., 2023. Global context vision transformers, in: International Conference
    on Machine Learning, PMLR. pp. 12633–12646.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2017) He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask
    R-CNN, in: Proceedings of the IEEE international conference on computer vision,
    pp. 2961–2969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2015) He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep Residual Learning
    for Image Recognition. URL: [http://arxiv.org/abs/1512.03385](http://arxiv.org/abs/1512.03385),
    doi:[10.48550/arXiv.1512.03385](https:/doi.org/10.48550/arXiv.1512.03385). arXiv:1512.03385
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2020a) He, P., Liu, X., Gao, J., Chen, W., 2020a. DeBERTa: Decoding-enhanced
    bert with disentangled attention. arXiv preprint arXiv:2006.03654 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020b) He, Y., Nazir, S., Nie, B., Khan, S., Zhang, J., 2020b. Developing
    an efficient deep learning-based trusted model for pervasive computing using an
    lstm-based classification model. Complexity 2020, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hema and Garcia Marquez (2023) Hema, C., Garcia Marquez, F.P., 2023. Emotional
    speech recognition using cnn and deep learning techniques. Applied Acoustics 211,
    109492. URL: [https://www.sciencedirect.com/science/article/pii/S0003682X23002906](https://www.sciencedirect.com/science/article/pii/S0003682X23002906),
    doi:[https://doi.org/10.1016/j.apacoust.2023.109492](https:/doi.org/https://doi.org/10.1016/j.apacoust.2023.109492).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hermawan et al. (2024) Hermawan, A.T., Zaeni, I.A.E., Wibawa, A.P., Gunawan,
    G., Hendrawan, W.H., Kristian, Y., 2024. A multi representation deep learning
    approach for epileptic seizure detection. Journal of Robotics and Control (JRC)
    5, 187–204.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hernández-Blanco et al. (2019) Hernández-Blanco, A., Herrera-Flores, B., Tomás,
    D., Navarro-Colorado, B., et al., 2019. A systematic review of deep learning approaches
    to educational data mining. Complexity 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hinton (2009) Hinton, G.E., 2009. Deep belief networks. Scholarpedia 4, 5947.
    URL: [http://www.scholarpedia.org/article/Deep_belief_networks](http://www.scholarpedia.org/article/Deep_belief_networks),
    doi:[10.4249/scholarpedia.5947](https:/doi.org/10.4249/scholarpedia.5947).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hinton (2012) Hinton, G.E., 2012. A Practical Guide to Training Restricted
    Boltzmann Machines, in: Montavon, G., Orr, G.B., Müller, K.R. (Eds.), Neural Networks:
    Tricks of the Trade: Second Edition. Springer, Berlin, Heidelberg. Lecture Notes
    in Computer Science, pp. 599–619. URL: [https://doi.org/10.1007/978-3-642-35289-8_32](https://doi.org/10.1007/978-3-642-35289-8_32),
    doi:[10.1007/978-3-642-35289-8_32](https:/doi.org/10.1007/978-3-642-35289-8_32).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hinton et al. (2006) Hinton, G.E., Osindero, S., Teh, Y.W., 2006. A Fast Learning
    Algorithm for Deep Belief Nets. Neural Computation 18, 1527–1554. URL: [https://doi.org/10.1162/neco.2006.18.7.1527](https://doi.org/10.1162/neco.2006.18.7.1527),
    doi:[10.1162/neco.2006.18.7.1527](https:/doi.org/10.1162/neco.2006.18.7.1527).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hochreiter et al. (2001) Hochreiter, S., Bengio, Y., Frasconi, P., Schmidhuber,
    J., others, 2001. Gradient flow in recurrent nets: the difficulty of learning
    long-term dependencies, in: A Field Guide to Dynamical Recurrent Networks. A field
    guide to dynamical recurrent neural networks. IEEE Press In.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hochreiter and Schmidhuber (1997) Hochreiter, S., Schmidhuber, J., 1997. Long
    Short-Term Memory. Neural Computation 9, 1735–1780. URL: [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735),
    doi:[10.1162/neco.1997.9.8.1735](https:/doi.org/10.1162/neco.1997.9.8.1735).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hornik et al. (1989) Hornik, K., Stinchcombe, M., White, H., 1989. Multilayer
    feedforward networks are universal approximators. Neural Networks 2, 359–366.
    URL: [https://www.sciencedirect.com/science/article/pii/0893608089900208](https://www.sciencedirect.com/science/article/pii/0893608089900208),
    doi:[10.1016/0893-6080(89)90020-8](https:/doi.org/10.1016/0893-6080(89)90020-8).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2023) Hu, C., Sun, Z., Li, C., Zhang, Y., Xing, C., 2023. Survey
    of Time Series Data Generation in IoT. Sensors 23, 6976. URL: [https://www.mdpi.com/1424-8220/23/15/6976](https://www.mdpi.com/1424-8220/23/15/6976),
    doi:[10.3390/s23156976](https:/doi.org/10.3390/s23156976). number: 15 Publisher:
    Multidisciplinary Digital Publishing Institute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2019) Hu, J., Shen, L., Albanie, S., Sun, G., Wu, E., 2019. Squeeze-and-Excitation
    Networks. URL: [http://arxiv.org/abs/1709.01507](http://arxiv.org/abs/1709.01507),
    doi:[10.48550/arXiv.1709.01507](https:/doi.org/10.48550/arXiv.1709.01507). arXiv:1709.01507
    [cs] version: 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2017) Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.,
    2017. Densely connected convolutional networks, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp. 4700–4708.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang and Feng (2020) Huang, J., Feng, Y., 2020. Optimization of Recurrent
    Neural Networks on Natural Language Processing, in: Proceedings of the 2019 8th
    International Conference on Computing and Pattern Recognition, Association for
    Computing Machinery, New York, NY, USA. pp. 39–45. URL: [https://dl.acm.org/doi/10.1145/3373509.3373573](https://dl.acm.org/doi/10.1145/3373509.3373573),
    doi:[10.1145/3373509.3373573](https:/doi.org/10.1145/3373509.3373573).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023a) Huang, L., Lu, K., Song, G., Wang, L., Liu, S., Liu, Y.,
    Li, H., 2023a. Teach-DETR: Better Training DETR With Teachers. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 45, 15759–15771. URL: [https://ieeexplore.ieee.org/document/10264211](https://ieeexplore.ieee.org/document/10264211),
    doi:[10.1109/TPAMI.2023.3319387](https:/doi.org/10.1109/TPAMI.2023.3319387). conference
    Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023b) Huang, Y., Qi, J., Wang, X., Lin, Z., 2023b. Asymmetric
    Polynomial Loss for Multi-Label Classification, in: ICASSP 2023-2023 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE. pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hwang et al. (2023) Hwang, J., Park, S., Chi, J., 2023. Improving multi-class
    motor imagery eeg classification using overlapping sliding window and deep learning
    model. Electronics 12. URL: [https://www.mdpi.com/2079-9292/12/5/1186](https://www.mdpi.com/2079-9292/12/5/1186),
    doi:[10.3390/electronics12051186](https:/doi.org/10.3390/electronics12051186).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ige and Mohd Noor (2023) Ige, A.O., Mohd Noor, M.H., 2023. A deep local-temporal
    architecture with attention for lightweight human activity recognition. Applied
    Soft Computing 149, 110954. URL: [https://www.sciencedirect.com/science/article/pii/S1568494623009729](https://www.sciencedirect.com/science/article/pii/S1568494623009729),
    doi:[https://doi.org/10.1016/j.asoc.2023.110954](https:/doi.org/https://doi.org/10.1016/j.asoc.2023.110954).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ige and Noor (2022) Ige, A.O., Noor, M.H.M., 2022. A survey on unsupervised
    learning for wearable sensor-based activity recognition. Applied Soft Computing
    , 109363.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ige and Noor (2023) Ige, A.O., Noor, M.H.M., 2023. Wsense: A robust feature
    learning module for lightweight human activity recognition. arXiv preprint arXiv:2303.17845
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Imran et al. (2023) Imran, H.A., Riaz, Q., Hussain, M., Tahir, H., Arshad,
    R., 2023. Smart-wearable sensors and cnn-bigru model: A powerful combination for
    human activity recognition. IEEE Sensors Journal .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioffe and Szegedy (2015) Ioffe, S., Szegedy, C., 2015. Batch normalization:
    accelerating deep network training by reducing internal covariate shift, in: Proceedings
    of the 32nd International Conference on International Conference on Machine Learning
    - Volume 37, JMLR.org, Lille, France. pp. 448–456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iosifidis et al. (2019) Iosifidis, V., Tran, T.N.H., Ntoutsi, E., 2019. Fairness-enhancing
    interventions in stream classification, in: Database and Expert Systems Applications:
    30th International Conference, DEXA 2019, Linz, Austria, August 26–29, 2019, Proceedings,
    Part I 30, Springer. pp. 261–276.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jafarifarmand and Badamchizadeh (2019) Jafarifarmand, A., Badamchizadeh, M.A.,
    2019. Eeg artifacts handling in a real practical brain–computer interface controlled
    vehicle. IEEE Transactions on Neural Systems and Rehabilitation Engineering 27,
    1200–1208.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jain et al. (2023) Jain, B., Huber, M., Elmasri, R., 2023. Increasing Fairness
    in Predictions Using Bias Parity Score Based Loss Function Regularization. The
    International FLAIRS Conference Proceedings 36. URL: [https://journals.flvc.org/FLAIRS/article/view/133311](https://journals.flvc.org/FLAIRS/article/view/133311),
    doi:[10.32473/flairs.36.133311](https:/doi.org/10.32473/flairs.36.133311).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaitly et al. (2016) Jaitly, N., Le, Q.V., Vinyals, O., Sutskever, I., Sussillo,
    D., Bengio, S., 2016. An online sequence-to-sequence model using partial conditioning.
    Advances in Neural Information Processing Systems 29.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2024) Jiang, B., Zeng, W., Yang, C., Wang, R., Zhang, B., 2024.
    DE-GAN: Text-to-image synthesis with dual and efficient fusion model. Multimedia
    Tools and Applications 83, 23839–23852. URL: [https://doi.org/10.1007/s11042-023-16377-8](https://doi.org/10.1007/s11042-023-16377-8),
    doi:[10.1007/s11042-023-16377-8](https:/doi.org/10.1007/s11042-023-16377-8).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2019) Jiang, L., Ma, X., Chen, S., Bailey, J., Jiang, Y.G., 2019.
    Black-box adversarial attacks on video recognition models, in: Proceedings of
    the 27th ACM International Conference on Multimedia, pp. 864–872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiao et al. (2016) Jiao, Y., Tu, M., Berisha, V., Liss, J.M., 2016. Accent
    identification by combining deep neural networks and recurrent neural networks
    trained on long and short term features., in: Interspeech, pp. 2388–2392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karras et al. (2018) Karras, T., Aila, T., Laine, S., Lehtinen, J., 2018. Progressive
    Growing of GANs for Improved Quality, Stability, and Variation. URL: [http://arxiv.org/abs/1710.10196](http://arxiv.org/abs/1710.10196),
    doi:[10.48550/arXiv.1710.10196](https:/doi.org/10.48550/arXiv.1710.10196). arXiv:1710.10196
    [cs, stat] version: 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karras et al. (2019) Karras, T., Laine, S., Aila, T., 2019. A Style-Based Generator
    Architecture for Generative Adversarial Networks. URL: [http://arxiv.org/abs/1812.04948](http://arxiv.org/abs/1812.04948),
    doi:[10.48550/arXiv.1812.04948](https:/doi.org/10.48550/arXiv.1812.04948). arXiv:1812.04948
    [cs, stat] version: 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kehrenberg et al. (2020) Kehrenberg, T., Chen, Z., Quadrianto, N., 2020. Tuning
    fairness by balancing target labels. Frontiers in artificial intelligence 3, 33.
    Publisher: Frontiers Media SA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khalil et al. (2019) Khalil, R.A., Jones, E., Babar, M.I., Jan, T., Zafar,
    M.H., Alhussain, T., 2019. Speech emotion recognition using deep learning techniques:
    A review. IEEE Access 7, 117327–117345. doi:[10.1109/ACCESS.2019.2936124](https:/doi.org/10.1109/ACCESS.2019.2936124).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khan and Ahmad (2021) Khan, Z.N., Ahmad, J., 2021. Attention induced multi-head
    convolutional neural network for human activity recognition. Applied soft computing
    110, 107671.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2023) Kim, D., Baek, Y., Yang, S., Choo, J., 2023. Towards Formality-Aware
    Neural Machine Translation by Leveraging Context Information, in: Findings of
    the Association for Computational Linguistics: EMNLP 2023, pp. 7384–7392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2017) Kingma, D.P., Ba, J., 2017. Adam: A Method for Stochastic
    Optimization. URL: [http://arxiv.org/abs/1412.6980](http://arxiv.org/abs/1412.6980),
    doi:[10.48550/arXiv.1412.6980](https:/doi.org/10.48550/arXiv.1412.6980). arXiv:1412.6980
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling (2013) Kingma, D.P., Welling, M., 2013. Auto-encoding variational
    bayes. arXiv preprint arXiv:1312.6114 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012.
    ImageNet Classification with Deep Convolutional Neural Networks, in: Advances
    in Neural Information Processing Systems, Curran Associates, Inc. URL: [https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kumar et al. (2016) Kumar, S., Sharma, A., Mamun, K., Tsunoda, T., 2016. A
    deep learning approach for motor imagery eeg signal classification, in: 2016 3rd
    Asia-Pacific World Congress on Computer Science and Engineering (APWC on CSE),
    IEEE. pp. 34–39.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kusuma and Jothi (2022) Kusuma, S., Jothi, K., 2022. Ecg signals-based automated
    diagnosis of congestive heart failure using deep cnn and lstm architecture. Biocybernetics
    and Biomedical Engineering 42, 247–257.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lan et al. (2019) Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut,
    R., 2019. ALBERT: A lite bert for self-supervised learning of language representations.
    arXiv preprint arXiv:1909.11942 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun et al. (2015) LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning.
    Nature 521, 436–444. URL: [https://www.nature.com/articles/nature14539](https://www.nature.com/articles/nature14539),
    doi:[10.1038/nature14539](https:/doi.org/10.1038/nature14539). number: 7553 Publisher:
    Nature Publishing Group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lecun et al. (1998) Lecun, Y., Bottou, L., Bengio, Y., Haffner, P., 1998. Gradient-based
    learning applied to document recognition. Proceedings of the IEEE 86, 2278–2324.
    URL: [https://ieeexplore.ieee.org/document/726791](https://ieeexplore.ieee.org/document/726791),
    doi:[10.1109/5.726791](https:/doi.org/10.1109/5.726791). conference Name: Proceedings
    of the IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun et al. (2012) LeCun, Y.A., Bottou, L., Orr, G.B., Müller, K.R., 2012.
    Efficient BackProp, in: Montavon, G., Orr, G.B., Müller, K.R. (Eds.), Neural Networks:
    Tricks of the Trade: Second Edition. Springer, Berlin, Heidelberg. Lecture Notes
    in Computer Science, pp. 9–48. URL: [https://doi.org/10.1007/978-3-642-35289-8_3](https://doi.org/10.1007/978-3-642-35289-8_3),
    doi:[10.1007/978-3-642-35289-8_3](https:/doi.org/10.1007/978-3-642-35289-8_3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lei et al. (2020) Lei, Y., Peng, Q., Shen, Y., 2020. Deep learning for algorithmic
    trading: enhancing macd strategy, in: Proceedings of the 2020 6th International
    Conference on Computing and Artificial Intelligence, pp. 51–57.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2024) Li, B., Weng, Y., Xia, F., Deng, H., 2024. Towards better
    Chinese-centric neural machine translation for low-resource languages. Computer
    Speech & Language 84, 101566. Publisher: Elsevier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020a) Li, F., He, F., Wang, F., Zhang, D., Xia, Y., Li, X., 2020a.
    A novel simplified convolutional neural network classification algorithm of motor
    imagery eeg signals based on deep learning. Applied Sciences 10, 1605.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Li, J., Monroe, W., Ritter, A., Galley, M., Gao, J., Jurafsky,
    D., 2016. Deep reinforcement learning for dialogue generation. arXiv preprint
    arXiv:1606.01541 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019a) Li, L., Zhu, J., Sun, M.T., 2019a. Deep learning based method
    for pruning deep neural networks, in: 2019 IEEE International Conference on Multimedia
    and Expo Workshops (ICMEW), pp. 312–317. doi:[10.1109/ICMEW.2019.00-68](https:/doi.org/10.1109/ICMEW.2019.00-68).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017) Li, M., Zhu, W., Zhang, M., Sun, Y., Wang, Z., 2017. The novel
    recognition method with optimal wavelet packet and lstm based recurrent neural
    network, in: 2017 IEEE International Conference on Mechatronics and Automation
    (ICMA), pp. 584–589. doi:[10.1109/ICMA.2017.8015882](https:/doi.org/10.1109/ICMA.2017.8015882).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Li, P., Pei, Y., Li, J., 2023. A comprehensive survey on design
    and application of autoencoder in deep learning. Applied Soft Computing 138, 110176.
    URL: [https://www.sciencedirect.com/science/article/pii/S1568494623001941](https://www.sciencedirect.com/science/article/pii/S1568494623001941),
    doi:[10.1016/j.asoc.2023.110176](https:/doi.org/10.1016/j.asoc.2023.110176).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020b) Li, W., Qi, F., Tang, M., Yu, Z., 2020b. Bidirectional LSTM
    with self-attention mechanism and multi-channel features for sentiment classification.
    Neurocomputing 387, 63–77. URL: [https://www.sciencedirect.com/science/article/pii/S0925231220300254](https://www.sciencedirect.com/science/article/pii/S0925231220300254),
    doi:[10.1016/j.neucom.2020.01.006](https:/doi.org/10.1016/j.neucom.2020.01.006).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Shen (2024) Li, X., Shen, Q., 2024. A hybrid framework based on knowledge
    distillation for explainable disease diagnosis. Expert Systems with Applications
    238, 121844. Publisher: Elsevier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020c) Li, X., Zhang, T., Zhao, X., Yi, Z., 2020c. Guided autoencoder
    for dimensionality reduction of pedestrian features. Applied Intelligence 50,
    4557–4567. URL: [https://doi.org/10.1007/s10489-020-01813-1](https://doi.org/10.1007/s10489-020-01813-1),
    doi:[10.1007/s10489-020-01813-1](https:/doi.org/10.1007/s10489-020-01813-1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Li, Z., Cai, J., He, S., Zhao, H., 2018. Seq2seq dependency
    parsing, in: Proceedings of the 27th International Conference on Computational
    Linguistics, pp. 3203–3214.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019b) Li, Z., Niu, C., Meng, F., Feng, Y., Li, Q., Zhou, J., 2019b.
    Incremental transformer with deliberation decoder for document grounded conversations.
    arXiv preprint arXiv:1907.08854 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Lin, M., Chen, Q., Yan, S., 2014. Network In Network. URL:
    [http://arxiv.org/abs/1312.4400](http://arxiv.org/abs/1312.4400), doi:[10.48550/arXiv.1312.4400](https:/doi.org/10.48550/arXiv.1312.4400).
    arXiv:1312.4400 [cs] version: 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2017a) Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan,
    B., Belongie, S., 2017a. Feature pyramid networks for object detection, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp. 2117–2125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2020) Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P., 2020.
    Focal Loss for Dense Object Detection. IEEE Transactions on Pattern Analysis and
    Machine Intelligence 42, 318–327. URL: [https://ieeexplore.ieee.org/document/8417976](https://ieeexplore.ieee.org/document/8417976),
    doi:[10.1109/TPAMI.2018.2858826](https:/doi.org/10.1109/TPAMI.2018.2858826). conference
    Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2017b) Lin, Z., Feng, M., Santos, C.N.d., Yu, M., Xiang, B., Zhou,
    B., Bengio, Y., 2017b. A structured self-attentive sentence embedding. arXiv preprint
    arXiv:1703.03130 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Guo (2019) Liu, G., Guo, J., 2019. Bidirectional LSTM with attention
    mechanism and convolutional layer for text classification. Neurocomputing 337,
    325–338. URL: [https://www.sciencedirect.com/science/article/pii/S0925231219301067](https://www.sciencedirect.com/science/article/pii/S0925231219301067),
    doi:[10.1016/j.neucom.2019.01.078](https:/doi.org/10.1016/j.neucom.2019.01.078).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Liu, Q., Dong, Y., Li, X., 2023a. Multi-stage context refinement
    network for semantic segmentation. Neurocomputing 535, 53–63. URL: [https://www.sciencedirect.com/science/article/pii/S0925231223002254](https://www.sciencedirect.com/science/article/pii/S0925231223002254),
    doi:[10.1016/j.neucom.2023.03.006](https:/doi.org/10.1016/j.neucom.2023.03.006).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019a) Liu, S., Huang, D., Wang, Y., 2019a. Adaptive NMS: Refining
    pedestrian detection in a crowd, in: Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, pp. 6459–6468.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018) Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., 2018. Path aggregation
    network for instance segmentation, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, pp. 8759–8768.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2016) Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.,
    Fu, C.Y., Berg, A.C., 2016. SSD: Single Shot MultiBox Detector, in: Leibe, B.,
    Matas, J., Sebe, N., Welling, M. (Eds.), Computer Vision – ECCV 2016, Springer
    International Publishing, Cham. pp. 21–37. doi:[10.1007/978-3-319-46448-0_2](https:/doi.org/10.1007/978-3-319-46448-0_2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021a) Liu, X., Wang, H., Li, Z., Qin, L., 2021a. Deep learning
    in ecg diagnosis: A review. Knowledge-Based Systems 227, 107187. URL: [https://www.sciencedirect.com/science/article/pii/S0950705121004494](https://www.sciencedirect.com/science/article/pii/S0950705121004494),
    doi:[https://doi.org/10.1016/j.knosys.2021.107187](https:/doi.org/https://doi.org/10.1016/j.knosys.2021.107187).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Liu, Y., Chen, Y., Lasang, P., Sun, Q., 2022. Covariance
    Attention for Semantic Segmentation. IEEE Transactions on Pattern Analysis and
    Machine Intelligence 44, 1805–1818. URL: [https://ieeexplore.ieee.org/document/9206128](https://ieeexplore.ieee.org/document/9206128),
    doi:[10.1109/TPAMI.2020.3026069](https:/doi.org/10.1109/TPAMI.2020.3026069). conference
    Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
    Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V., 2019b. RoBERTa: A robustly
    optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021b) Liu, Z., Wang, L., Wu, W., Qian, C., Lu, T., 2021b. TAM:
    Temporal Adaptive Module for Video Recognition. URL: [http://arxiv.org/abs/2005.06803](http://arxiv.org/abs/2005.06803),
    doi:[10.48550/arXiv.2005.06803](https:/doi.org/10.48550/arXiv.2005.06803). arXiv:2005.06803
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023b) Liu, Z., Xu, Z., Jin, J., Shen, Z., Darrell, T., 2023b.
    Dropout reduces underfitting, in: Proceedings of the 40th International Conference
    on Machine Learning, JMLR.org, Honolulu, Hawaii, USA. pp. 22233–22248.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Llamedo and Martínez (2011) Llamedo, M., Martínez, J.P., 2011. Heartbeat classification
    using feature selection driven by database generalization criteria. IEEE Transactions
    on Biomedical Engineering 58, 616–625. doi:[10.1109/TBME.2010.2068048](https:/doi.org/10.1109/TBME.2010.2068048).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loh et al. (2023) Loh, C., Han, S., Sudalairaj, S., Dangovski, R., Xu, K.,
    Wenzel, F., Soljacic, M., Srivastava, A., 2023. Multi-Symmetry Ensembles: Improving
    Diversity and Generalization via Opposing Symmetries. arXiv preprint arXiv:2303.02484
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Long et al. (2015) Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional
    networks for semantic segmentation, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp. 3431–3440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. (2020) Lu, X., Li, S., Fujimoto, M., 2020. Automatic speech recognition.
    Speech-to-speech translation , 21–38.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luca et al. (2022) Luca, A.R., Ursuleanu, T.F., Gheorghe, L., Grigorovici,
    R., Iancu, S., Hlusneac, M., Grigorovici, A., 2022. Impact of quality, type and
    volume of data used by deep learning models in the analysis of medical images.
    Informatics in Medicine Unlocked 29, 100911. URL: [https://www.sciencedirect.com/science/article/pii/S2352914822000612](https://www.sciencedirect.com/science/article/pii/S2352914822000612),
    doi:[10.1016/j.imu.2022.100911](https:/doi.org/10.1016/j.imu.2022.100911).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luong et al. (2015) Luong, M.T., Pham, H., Manning, C.D., 2015. Effective approaches
    to attention-based neural machine translation. arXiv preprint arXiv:1508.04025
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lupo et al. (2023) Lupo, L., Dinarelli, M., Besacier, L., 2023. Encoding Sentence
    Position in Context-Aware Neural Machine Translation with Concatenation. arXiv
    preprint arXiv:2302.06459 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luwe et al. (2022) Luwe, Y.J., Lee, C.P., Lim, K.M., 2022. Wearable sensor-based
    human activity recognition with hybrid deep learning model, in: Informatics, MDPI.
    p. 56.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2021) Ma, T., Pan, Q., Rong, H., Qian, Y., Tian, Y., Al-Nabhan,
    N., 2021. T-BERTSum: Topic-aware text summarization based on bert. IEEE Transactions
    on Computational Social Systems 9, 879–890. Publisher: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2024) Ma, Y., Wang, R., Zong, M., Ji, W., Wang, Y., Ye, B., 2024.
    Convolutional transformer network for fine-grained action recognition. Neurocomputing
    569, 127027. URL: [https://www.sciencedirect.com/science/article/pii/S0925231223011505](https://www.sciencedirect.com/science/article/pii/S0925231223011505),
    doi:[10.1016/j.neucom.2023.127027](https:/doi.org/10.1016/j.neucom.2023.127027).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maas et al. (2013) Maas, A.L., Hannun, A.Y., Ng, A.Y., others, 2013. Rectifier
    nonlinearities improve neural network acoustic models, in: Proc. icml, Atlanta,
    GA. p. 3. Issue: 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mathews et al. (2018) Mathews, S.M., Kambhamettu, C., Barner, K.E., 2018. A
    novel application of deep learning for single-lead ecg classification. Computers
    in Biology and Medicine 99, 53–62. URL: [https://www.sciencedirect.com/science/article/pii/S0010482518301264](https://www.sciencedirect.com/science/article/pii/S0010482518301264),
    doi:[https://doi.org/10.1016/j.compbiomed.2018.05.013](https:/doi.org/https://doi.org/10.1016/j.compbiomed.2018.05.013).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matsugu et al. (2003) Matsugu, M., Mori, K., Mitari, Y., Kaneda, Y., 2003. Subject
    independent facial expression recognition with robust face detection using a convolutional
    neural network. Neural Networks 16, 555–559. doi:[10.1016/S0893-6080(03)00115-1](https:/doi.org/10.1016/S0893-6080(03)00115-1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meetei et al. (2023) Meetei, L.S., Singh, A., Singh, T.D., Bandyopadhyay, S.,
    2023. Do cues in a video help in handling rare words in a machine translation
    system under a low-resource setting? Natural Language Processing Journal 3, 100016.
    Publisher: Elsevier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Misra (2020) Misra, D., 2020. Mish: A Self Regularized Non-Monotonic Activation
    Function. URL: [http://arxiv.org/abs/1908.08681](http://arxiv.org/abs/1908.08681),
    doi:[10.48550/arXiv.1908.08681](https:/doi.org/10.48550/arXiv.1908.08681). arXiv:1908.08681
    [cs, stat].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modir et al. (2023) Modir, A., Shamekhi, S., Ghaderyan, P., 2023. A systematic
    review and methodological analysis of eeg-based biomarkers of alzheimer’s disease.
    Measurement , 113274.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mohd Noor (2021) Mohd Noor, M.H., 2021. Feature learning using convolutional
    denoising autoencoder for activity recognition. Neural Computing and Applications
    33, 10909–10922. URL: [https://doi.org/10.1007/s00521-020-05638-4](https://doi.org/10.1007/s00521-020-05638-4),
    doi:[10.1007/s00521-020-05638-4](https:/doi.org/10.1007/s00521-020-05638-4).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mukhamadiyev et al. (2022) Mukhamadiyev, A., Khujayarov, I., Djuraev, O., Cho,
    J., 2022. Automatic speech recognition method based on deep learning approaches
    for uzbek language. Sensors 22, 3683. doi:[https://doi.org/10.3390/s22103683](https:/doi.org/https://doi.org/10.3390/s22103683).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mumuni and Mumuni (2022) Mumuni, A., Mumuni, F., 2022. Data augmentation: A
    comprehensive survey of modern approaches. Array 16, 100258. URL: [https://www.sciencedirect.com/science/article/pii/S2590005622000911](https://www.sciencedirect.com/science/article/pii/S2590005622000911),
    doi:[10.1016/j.array.2022.100258](https:/doi.org/10.1016/j.array.2022.100258).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Munappy et al. (2022) Munappy, A.R., Bosch, J., Olsson, H.H., Arpteg, A., Brinne,
    B., 2022. Data management for production quality deep learning models: Challenges
    and solutions. Journal of Systems and Software 191, 111359. URL: [https://www.sciencedirect.com/science/article/pii/S0164121222000905](https://www.sciencedirect.com/science/article/pii/S0164121222000905),
    doi:[10.1016/j.jss.2022.111359](https:/doi.org/10.1016/j.jss.2022.111359).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Murfi et al. (2024) Murfi, H., Syamsyuriani, Gowandi, T., Ardaneswari, G.,
    Nurrohmah, S., 2024. BERT-based combination of convolutional and recurrent neural
    network for indonesian sentiment analysis. Applied Soft Computing 151, 111112.
    URL: [https://www.sciencedirect.com/science/article/pii/S1568494623011304](https://www.sciencedirect.com/science/article/pii/S1568494623011304),
    doi:[10.1016/j.asoc.2023.111112](https:/doi.org/10.1016/j.asoc.2023.111112).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Murtaza et al. (2023) Murtaza, H., Ahmed, M., Khan, N.F., Murtaza, G., Zafar,
    S., Bano, A., 2023. Synthetic data generation: State of the art in health care
    domain. Computer Science Review 48, 100546. URL: [https://www.sciencedirect.com/science/article/pii/S1574013723000138](https://www.sciencedirect.com/science/article/pii/S1574013723000138),
    doi:[10.1016/j.cosrev.2023.100546](https:/doi.org/10.1016/j.cosrev.2023.100546).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nafea et al. (2021) Nafea, O., Abdul, W., Muhammad, G., Alsulaiman, M., 2021.
    Sensor-based human activity recognition with spatio-temporal deep learning. Sensors
    21, 2141.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakamura and Hong (2019) Nakamura, K., Hong, B.W., 2019. Adaptive Weight Decay
    for Deep Neural Networks. IEEE Access 7, 118857–118865. URL: [https://ieeexplore.ieee.org/document/8811458](https://ieeexplore.ieee.org/document/8811458),
    doi:[10.1109/ACCESS.2019.2937139](https:/doi.org/10.1109/ACCESS.2019.2937139).
    conference Name: IEEE Access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nanni et al. (2023) Nanni, L., Loreggia, A., Barcellona, L., Ghidoni, S., 2023.
    Building Ensemble of Deep Networks: Convolutional Networks and Transformers. IEEE
    Access 11, 124962–124974. URL: [https://ieeexplore.ieee.org/document/10309107](https://ieeexplore.ieee.org/document/10309107),
    doi:[10.1109/ACCESS.2023.3330442](https:/doi.org/10.1109/ACCESS.2023.3330442).
    conference Name: IEEE Access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nassif et al. (2019) Nassif, A.B., Shahin, I., Attili, I., Azzeh, M., Shaalan,
    K., 2019. Speech recognition using deep neural networks: A systematic review.
    IEEE Access 7, 19143–19165. doi:[10.1109/ACCESS.2019.2896880](https:/doi.org/10.1109/ACCESS.2019.2896880).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ng and others (2011) Ng, A., others, 2011. Sparse autoencoder. CS294A Lecture
    notes 72, 1–19.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nie et al. (2017) Nie, Y.p., Han, Y., Huang, J.m., Jiao, B., Li, A.p., 2017.
    Attention-based encoder-decoder model for answer selection in question answering.
    Frontiers of Information Technology & Electronic Engineering 18, 535–544. Publisher:
    Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nikou et al. (2019) Nikou, M., Mansourfar, G., Bagherzadeh, J., 2019. Stock
    price prediction using deep learning algorithm and its comparison with machine
    learning algorithms. Intelligent Systems in Accounting, Finance and Management
    26, 164–174.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Noh et al. (2015) Noh, H., Hong, S., Han, B., 2015. Learning deconvolution
    network for semantic segmentation, in: Proceedings of the IEEE international conference
    on computer vision, pp. 1520–1528.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Odena et al. (2017) Odena, A., Olah, C., Shlens, J., 2017. Conditional image
    synthesis with auxiliary classifier gans, in: International conference on machine
    learning, PMLR. pp. 2642–2651.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oktay et al. (2018) Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich,
    M., Misawa, K., Mori, K., McDonagh, S., Hammerla, N.Y., Kainz, B., Glocker, B.,
    Rueckert, D., 2018. Attention U-Net: Learning Where to Look for the Pancreas.
    URL: [http://arxiv.org/abs/1804.03999](http://arxiv.org/abs/1804.03999), doi:[10.48550/arXiv.1804.03999](https:/doi.org/10.48550/arXiv.1804.03999).
    arXiv:1804.03999 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otter et al. (2021) Otter, D.W., Medina, J.R., Kalita, J.K., 2021. A survey
    of the usages of deep learning for natural language processing. IEEE Transactions
    on Neural Networks and Learning Systems 32, 604–624. doi:[10.1109/TNNLS.2020.2979670](https:/doi.org/10.1109/TNNLS.2020.2979670).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ozbayoglu et al. (2020) Ozbayoglu, A.M., Gudelek, M.U., Sezer, O.B., 2020.
    Deep learning for financial applications: A survey. Applied soft computing 93,
    106384.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Padmanabhan and Premkumar (2015) Padmanabhan, J., Premkumar, M.J.J., 2015.
    Machine learning in automatic speech recognition: A survey. IETE Technical Review
    32, 240–251. doi:[10.1080/02564602.2015.1010611](https:/doi.org/10.1080/02564602.2015.1010611).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. (2023) Pan, J., Fang, W., Zhang, Z., Chen, B., Zhang, Z., Wang, S.,
    2023. Multimodal emotion recognition based on facial expressions, speech, and
    eeg. IEEE Open Journal of Engineering in Medicine and Biology , 1–8doi:[10.1109/OJEMB.2023.3240280](https:/doi.org/10.1109/OJEMB.2023.3240280).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pandey and Seeja (2022) Pandey, P., Seeja, K., 2022. Subject independent emotion
    recognition from eeg using vmd and deep learning. Journal of King Saud University
    - Computer and Information Sciences 34, 1730–1738. URL: [https://www.sciencedirect.com/science/article/pii/S1319157819309991](https://www.sciencedirect.com/science/article/pii/S1319157819309991),
    doi:[https://doi.org/10.1016/j.jksuci.2019.11.003](https:/doi.org/https://doi.org/10.1016/j.jksuci.2019.11.003).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park and Kwak (2017) Park, S., Kwak, N., 2017. Analysis on the Dropout Effect
    in Convolutional Neural Networks, in: Lai, S.H., Lepetit, V., Nishino, K., Sato,
    Y. (Eds.), Computer Vision – ACCV 2016, Springer International Publishing, Cham.
    pp. 189–204. doi:[10.1007/978-3-319-54184-6_12](https:/doi.org/10.1007/978-3-319-54184-6_12).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023) Park, W., Park, I., Kim, S., Ryu, J., 2023. Robust Asymmetric
    Loss for Multi-Label Long-Tailed Learning, pp. 2711–2720. URL: [https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Park_Robust_Asymmetric_Loss_for_Multi-Label_Long-Tailed_Learning_ICCVW_2023_paper.html](https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/html/Park_Robust_Asymmetric_Loss_for_Multi-Label_Long-Tailed_Learning_ICCVW_2023_paper.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Peng, Z., Guo, Z., Huang, W., Wang, Y., Xie, L., Jiao, J.,
    Tian, Q., Ye, Q., 2023. Conformer: Local Features Coupling Global Representations
    for Recognition and Detection. IEEE Transactions on Pattern Analysis and Machine
    Intelligence 45, 9454–9468. URL: [https://ieeexplore.ieee.org/document/10040235](https://ieeexplore.ieee.org/document/10040235),
    doi:[10.1109/TPAMI.2023.3243048](https:/doi.org/10.1109/TPAMI.2023.3243048). conference
    Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pierson and Gashler (2017) Pierson, H.A., Gashler, M.S., 2017. Deep learning
    in robotics: a review of recent research. Advanced Robotics 31, 821–835. URL:
    [https://doi.org/10.1080/01691864.2017.1365009](https://doi.org/10.1080/01691864.2017.1365009),
    doi:[10.1080/01691864.2017.1365009](https:/doi.org/10.1080/01691864.2017.1365009).
    publisher: Taylor & Francis _eprint: https://doi.org/10.1080/01691864.2017.1365009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pouyanfar et al. (2018) Pouyanfar, S., Sadiq, S., Yan, Y., Tian, H., Tao, Y.,
    Reyes, M.P., Shyu, M.L., Chen, S.C., Iyengar, S.S., 2018. A survey on deep learning:
    Algorithms, techniques, and applications. ACM Computing Surveys (CSUR) 51, 1–36.
    Publisher: ACM New York, NY, USA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prabhakararao and Dandapat (2020) Prabhakararao, E., Dandapat, S., 2020. Attentive
    rnn-based network to fuse 12-lead ecg and clinical features for improved myocardial
    infarction diagnosis. IEEE Signal Processing Letters 27, 2029–2033.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prabhavalkar et al. (2017) Prabhavalkar, R., Rao, K., Sainath, T.N., Li, B.,
    Johnson, L., Jaitly, N., 2017. A comparison of sequence-to-sequence models for
    speech recognition., in: Interspeech, pp. 939–943.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prechelt (2012) Prechelt, L., 2012. Early Stopping — But When?, in: Montavon,
    G., Orr, G.B., Müller, K.R. (Eds.), Neural Networks: Tricks of the Trade: Second
    Edition. Springer, Berlin, Heidelberg. Lecture Notes in Computer Science, pp.
    53–67. URL: [https://doi.org/10.1007/978-3-642-35289-8_5](https://doi.org/10.1007/978-3-642-35289-8_5),
    doi:[10.1007/978-3-642-35289-8_5](https:/doi.org/10.1007/978-3-642-35289-8_5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qian (1999) Qian, N., 1999. On the momentum term in gradient descent learning
    algorithms. Neural Networks 12, 145–151. URL: [https://www.sciencedirect.com/science/article/pii/S0893608098001166](https://www.sciencedirect.com/science/article/pii/S0893608098001166),
    doi:[10.1016/S0893-6080(98)00116-6](https:/doi.org/10.1016/S0893-6080(98)00116-6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2015) Radford, A., Metz, L., Chintala, S., 2015. Unsupervised
    representation learning with deep convolutional generative adversarial networks.
    arXiv preprint arXiv:1511.06434 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raffel et al. (2017) Raffel, C., Luong, M.T., Liu, P.J., Weiss, R.J., Eck,
    D., 2017. Online and linear-time attention by enforcing monotonic alignments,
    in: International conference on machine learning, PMLR. pp. 2837–2846.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ragab et al. (2020) Ragab, M.G., Abdulkadir, S.J., Aziz, N., 2020. Random search
    one dimensional cnn for human activity recognition, in: 2020 International Conference
    on Computational Intelligence (ICCI), IEEE. pp. 86–91.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rai and Chatterjee (2022) Rai, H.M., Chatterjee, K., 2022. Hybrid cnn-lstm deep
    learning model and ensemble technique for automatic detection of myocardial infarction
    using big ecg data. Applied Intelligence 52, 5366–5384.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon et al. (2016) Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016.
    You Only Look Once: Unified, Real-Time Object Detection, pp. 779–788. URL: [https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redmon and Farhadi (2018) Redmon, J., Farhadi, A., 2018. Yolov3: An incremental
    improvement. arXiv preprint arXiv:1804.02767 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2015) Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster R-CNN:
    Towards Real-Time Object Detection with Region Proposal Networks, in: Advances
    in Neural Information Processing Systems, Curran Associates, Inc. URL: [https://proceedings.neurips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html](https://proceedings.neurips.cc/paper_files/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rifai et al. (2011) Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio,
    Y., Dauphin, Y., Glorot, X., 2011. Higher Order Contractive Auto-Encoder, in:
    Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (Eds.), Machine Learning
    and Knowledge Discovery in Databases, Springer, Berlin, Heidelberg. pp. 645–660.
    doi:[10.1007/978-3-642-23783-6_41](https:/doi.org/10.1007/978-3-642-23783-6_41).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rippeth et al. (2023) Rippeth, E., Carpuat, M., Duh, K., Post, M., 2023. Improving
    Word Sense Disambiguation in Neural Machine Translation with Salient Document
    Context. arXiv preprint arXiv:2311.15507 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rodrawangpai and Daungjaiboon (2022) Rodrawangpai, B., Daungjaiboon, W., 2022.
    Improving text classification with transformers and layer normalization. Machine
    Learning with Applications 10, 100403. URL: [https://www.sciencedirect.com/science/article/pii/S2666827022000792](https://www.sciencedirect.com/science/article/pii/S2666827022000792),
    doi:[10.1016/j.mlwa.2022.100403](https:/doi.org/10.1016/j.mlwa.2022.100403).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Romero et al. (2017) Romero, J., Olson, J.P., Aspuru-Guzik, A., 2017. Quantum
    autoencoders for efficient compression of quantum data. Quantum Science and Technology
    2, 045001. URL: [https://dx.doi.org/10.1088/2058-9565/aa8072](https://dx.doi.org/10.1088/2058-9565/aa8072),
    doi:[10.1088/2058-9565/aa8072](https:/doi.org/10.1088/2058-9565/aa8072). publisher:
    IOP Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sak et al. (2017) Sak, H., Shannon, M., Rao, K., Beaufays, F., 2017. Recurrent
    neural aligner: An encoder-decoder neural network model for sequence to sequence
    mapping., in: Interspeech, pp. 1298–1302.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sánchez-Hevia et al. (2022) Sánchez-Hevia, H.A., Gil-Pita, R., Utrilla-Manso,
    M., Rosa-Zurera, M., 2022. Age group classification and gender recognition from
    speech with temporal convolutional neural networks. Multimedia Tools and Applications
    81, 3535–3552.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarker (2021) Sarker, I.H., 2021. Deep learning: a comprehensive overview on
    techniques, taxonomy, applications and research directions. SN Computer Science
    2, 420. Publisher: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schaaf et al. (2021) Schaaf, N., de Mitri, O., Kim, H.B., Windberger, A., Huber,
    M.F., 2021. Towards measuring bias in image classification, in: Artificial Neural
    Networks and Machine Learning–ICANN 2021: 30th International Conference on Artificial
    Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part
    III 30, Springer. pp. 433–445.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schirrmeister et al. (2017) Schirrmeister, R.T., Springenberg, J.T., Fiederer,
    L.D.J., Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F., Burgard,
    W., Ball, T., 2017. Deep learning with convolutional neural networks for eeg decoding
    and visualization. Human Brain Mapping 38, 5391–5420. URL: [https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.23730](https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.23730),
    doi:[https://doi.org/10.1002/hbm.23730](https:/doi.org/https://doi.org/10.1002/hbm.23730).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shamshirband et al. (2021) Shamshirband, S., Fathi, M., Dehzangi, A., Chronopoulos,
    A.T., Alinejad-Rokny, H., 2021. A review on deep learning approaches in healthcare
    systems: Taxonomies, challenges, and open issues. Journal of Biomedical Informatics
    113, 103627. URL: [https://www.sciencedirect.com/science/article/pii/S1532046420302550](https://www.sciencedirect.com/science/article/pii/S1532046420302550),
    doi:[10.1016/j.jbi.2020.103627](https:/doi.org/10.1016/j.jbi.2020.103627).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. (2021) Sharma, M., Tiwari, J., Patel, V., Acharya, U.R., 2021.
    Automated identification of sleep disorder types using triplet half-band filter
    and ensemble machine learning techniques with eeg signals. Electronics 10, 1531.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2021) Shen, F., Zhao, X., Kou, G., Alsaadi, F.E., 2021. A new deep
    learning ensemble credit risk evaluation model with an improved synthetic minority
    oversampling technique. Applied Soft Computing 98, 106852.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2022) Shen, Z., Li, G., Fang, J., Zhong, H., Wang, J., Sun, Y.,
    Shen, X., 2022. Aberrated multidimensional eeg characteristics in patients with
    generalized anxiety disorder: A machine-learning based analysis framework. Sensors
    22, 5420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shewalkar et al. (2019) Shewalkar, A., Nyavanandi, D., Ludwig, S.A., 2019.
    Performance evaluation of deep neural networks applied to speech recognition:
    Rnn, lstm and gru. Journal of Artificial Intelligence and Soft Computing Research
    9, 235–245.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2023) Shi, L.F., Liu, Z.Y., Zhou, K.J., Shi, Y., Jing, X., 2023.
    Novel deep learning network for gait recognition using multimodal inertial sensors.
    Sensors 23, 849.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2015) Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.k., Woo,
    W.c., 2015. Convolutional LSTM Network: A Machine Learning Approach for Precipitation
    Nowcasting. URL: [http://arxiv.org/abs/1506.04214](http://arxiv.org/abs/1506.04214),
    doi:[10.48550/arXiv.1506.04214](https:/doi.org/10.48550/arXiv.1506.04214). arXiv:1506.04214
    [cs] version: 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shuvo et al. (2020) Shuvo, M.M.H., Ahmed, N., Nouduri, K., Palaniappan, K.,
    2020. A hybrid approach for human activity recognition with support vector machine
    and 1d convolutional neural network, in: 2020 IEEE Applied Imagery Pattern Recognition
    Workshop (AIPR), IEEE. pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simard et al. (2003) Simard, P., Steinkraus, D., Platt, J., 2003. Best practices
    for convolutional neural networks applied to visual document analysis, in: Seventh
    International Conference on Document Analysis and Recognition, 2003\. Proceedings.,
    pp. 958–963. URL: [https://ieeexplore.ieee.org/document/1227801](https://ieeexplore.ieee.org/document/1227801),
    doi:[10.1109/ICDAR.2003.1227801](https:/doi.org/10.1109/ICDAR.2003.1227801).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonyan and Zisserman (2015) Simonyan, K., Zisserman, A., 2015. Very Deep
    Convolutional Networks for Large-Scale Image Recognition. URL: [http://arxiv.org/abs/1409.1556](http://arxiv.org/abs/1409.1556),
    doi:[10.48550/arXiv.1409.1556](https:/doi.org/10.48550/arXiv.1409.1556). arXiv:1409.1556
    [cs] version: 6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2021) Singh, G., Sharma, S., Kumar, V., Kaur, M., Baz, M., Masud,
    M., et al., 2021. Spoken language identification using deep learning. Computational
    Intelligence and Neuroscience 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh and Srivastava (2017) Singh, R., Srivastava, S., 2017. Stock prediction
    using deep learning. Multimedia Tools and Applications 76, 18569–18584.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2018) Singh, S., Pandey, S.K., Pawar, U., Janghel, R.R., 2018.
    Classification of ecg arrhythmia using recurrent neural networks. Procedia computer
    science 132, 1290–1297.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soori et al. (2023) Soori, M., Arezoo, B., Dastres, R., 2023. Artificial intelligence,
    machine learning and deep learning in advanced robotics, a review. Cognitive Robotics
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sowmya and Jose (2022) Sowmya, S., Jose, D., 2022. Contemplate on ecg signals
    and classification of arrhythmia signals using cnn-lstm deep learning model. Measurement:
    Sensors 24, 100558.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2014) Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever,
    I., Salakhutdinov, R., 2014. Dropout: A Simple Way to Prevent Neural Networks
    from Overfitting. Journal of Machine Learning Research 15, 1929–1958. URL: [http://jmlr.org/papers/v15/srivastava14a.html](http://jmlr.org/papers/v15/srivastava14a.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava and Pandey (2022) Srivastava, R., Pandey, D., 2022. Speech recognition
    using hmm and soft computing. Materials Today: Proceedings 51, 1878–1883. doi:[https://doi.org/10.1016/j.matpr.2021.10.097](https:/doi.org/https://doi.org/10.1016/j.matpr.2021.10.097).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2015) Srivastava, R.K., Greff, K., Schmidhuber, J., 2015.
    Highway Networks. URL: [http://arxiv.org/abs/1505.00387](http://arxiv.org/abs/1505.00387),
    doi:[10.48550/arXiv.1505.00387](https:/doi.org/10.48550/arXiv.1505.00387). arXiv:1505.00387
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stahlberg (2020) Stahlberg, F., 2020. Neural machine translation: A review.
    Journal of Artificial Intelligence Research 69, 343–418.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stanton et al. (2021) Stanton, S., Izmailov, P., Kirichenko, P., Alemi, A.A.,
    Wilson, A.G., 2021. Does knowledge distillation really work?, in: Ranzato, M.,
    Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (Eds.), Advances in Neural
    Information Processing Systems, Curran Associates, Inc.. pp. 6906–6919. URL: [https://proceedings.neurips.cc/paper_files/paper/2021/file/376c6b9ff3bedbbea56751a84fffc10c-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/376c6b9ff3bedbbea56751a84fffc10c-Paper.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strudel et al. (2021) Strudel, R., Garcia, R., Laptev, I., Schmid, C., 2021.
    Segmenter: Transformer for semantic segmentation, in: Proceedings of the IEEE/CVF
    international conference on computer vision, pp. 7262–7272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2019) Sun, J., Zhou, Y., Zhang, B., 2019. ResFPA-GAN: Text-to-image
    synthesis with generative adversarial network based on residual block feature
    pyramid attention, in: 2019 IEEE International Conference on Advanced Robotics
    and its Social Impacts (ARSO), IEEE. pp. 317–322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2020) Sun, S., Wang, S., Wei, Y., 2020. A new ensemble deep learning
    approach for exchange rates forecasting and trading. Advanced Engineering Informatics
    46, 101160. URL: [https://www.sciencedirect.com/science/article/pii/S1474034620301312](https://www.sciencedirect.com/science/article/pii/S1474034620301312),
    doi:[https://doi.org/10.1016/j.aei.2020.101160](https:/doi.org/https://doi.org/10.1016/j.aei.2020.101160).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Sutskever, I., Vinyals, O., Le, Q.V., 2014. Sequence
    to sequence learning with neural networks. Advances in neural information processing
    systems 27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Szegedy et al. (2014) Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
    Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2014. Going Deeper with
    Convolutions. URL: [http://arxiv.org/abs/1409.4842](http://arxiv.org/abs/1409.4842),
    doi:[10.48550/arXiv.1409.4842](https:/doi.org/10.48550/arXiv.1409.4842). arXiv:1409.4842
    [cs] version: 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2013) Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,
    D., Goodfellow, I., Fergus, R., 2013. Intriguing properties of neural networks.
    arXiv preprint arXiv:1312.6199 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Szűcs and Huszti (2019) Szűcs, G., Huszti, D., 2019. Seq2seq deep learning
    method for summary generation by lstm with two-way encoder and beam search decoder,
    in: 2019 IEEE 17th International Symposium on Intelligent Systems and Informatics
    (SISY), pp. 221–226. doi:[10.1109/SISY47553.2019.9111502](https:/doi.org/10.1109/SISY47553.2019.9111502).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tabacof et al. (2016) Tabacof, P., Tavares, J., Valle, E., 2016. Adversarial
    images for variational autoencoders. arXiv preprint arXiv:1612.00155 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talaei Khoei et al. (2023) Talaei Khoei, T., Ould Slimane, H., Kaabouch, N.,
    2023. Deep learning: Systematic review, models, challenges, and research directions.
    Neural Computing and Applications 35, 23103–23124. Publisher: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan and Le (2019) Tan, M., Le, Q., 2019. EfficientNet: Rethinking model scaling
    for convolutional neural networks, in: International conference on machine learning,
    PMLR. pp. 6105–6114.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. (2020a) Tan, M., Pang, R., Le, Q.V., 2020a. Efficientdet: Scalable
    and efficient object detection, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. (2020b) Tan, M., Pang, R., Le, Q.V., 2020b. Efficientdet: Scalable
    and efficient object detection, in: Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, pp. 10781–10790.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. (2022) Tao, M., Tang, H., Wu, F., Jing, X.Y., Bao, B.K., Xu, C.,
    2022. DF-GAN: A simple and effective baseline for text-to-image synthesis, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 16515–16525.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Termritthikun et al. (2023) Termritthikun, C., Umer, A., Suwanwimolkul, S.,
    Xia, F., Lee, I., 2023. Explainable knowledge distillation for on-device chest
    x-ray classification. IEEE/ACM Transactions on Computational Biology and Bioinformatics
    Publisher: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tibrewal et al. (2022) Tibrewal, N., Leeuwis, N., Alimardani, M., 2022. Classification
    of motor imagery eeg using deep learning increases performance in inefficient
    bci users. Plos one 17, e0268880.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tibshirani (1996) Tibshirani, R., 1996. Regression Shrinkage and Selection
    via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological)
    58, 267–288. URL: [https://www.jstor.org/stable/2346178](https://www.jstor.org/stable/2346178).
    publisher: [Royal Statistical Society, Wiley].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tirumala and Shahamiri (2016) Tirumala, S.S., Shahamiri, S.R., 2016. A review
    on deep learning approaches in speaker identification, in: Proceedings of the
    8th International Conference on Signal Processing Systems, Association for Computing
    Machinery, New York, NY, USA. p. 142–147. URL: [https://doi.org/10.1145/3015166.3015210](https://doi.org/10.1145/3015166.3015210),
    doi:[10.1145/3015166.3015210](https:/doi.org/10.1145/3015166.3015210).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tjoa et al. (2023) Tjoa, E., Khok, H.J., Chouhan, T., Guan, C., 2023. Enhancing
    the confidence of deep learning classifiers via interpretable saliency maps. Neurocomputing
    562, 126825. Publisher: Elsevier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tonekaboni et al. (2019) Tonekaboni, S., Joshi, S., McCradden, M.D., Goldenberg,
    A., 2019. What clinicians want: contextualizing explainable machine learning for
    clinical end use, in: Machine learning for healthcare conference, PMLR. pp. 359–380.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tsao et al. (2023) Tsao, C.W., Aday, A.W., Almarzooq, Z.I., Anderson, C.A.,
    Arora, P., Avery, C.L., Baker-Smith, C.M., Beaton, A.Z., Boehme, A.K., Buxton,
    A.E., et al., 2023. Heart disease and stroke statistics—2023 update: a report
    from the american heart association. Circulation 147, e93–e621.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tu et al. (2022) Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik,
    A., Li, Y., 2022. MaxViT: Multi-axis Vision Transformer, in: Avidan, S., Brostow,
    G., Cissé, M., Farinella, G.M., Hassner, T. (Eds.), Computer Vision – ECCV 2022,
    Springer Nature Switzerland, Cham. pp. 459–479. doi:[10.1007/978-3-031-20053-3_27](https:/doi.org/10.1007/978-3-031-20053-3_27).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaquerizo-Villar et al. (2023) Vaquerizo-Villar, F., Gutiérrez-Tobal, G.C.,
    Calvo, E., Álvarez, D., Kheirandish-Gozal, L., Del Campo, F., Gozal, D., Hornero,
    R., 2023. An explainable deep-learning model to stage sleep states in children
    and propose novel eeg-related patterns in sleep apnea. Computers in Biology and
    Medicine 165, 107419.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vaswani et al. (2023) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2023. Attention Is All You
    Need. URL: [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762),
    doi:[10.48550/arXiv.1706.03762](https:/doi.org/10.48550/arXiv.1706.03762). arXiv:1706.03762
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vincent et al. (2008) Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A.,
    2008. Extracting and composing robust features with denoising autoencoders, in:
    Proceedings of the 25th international conference on Machine learning, Association
    for Computing Machinery, New York, NY, USA. pp. 1096–1103. URL: [https://doi.org/10.1145/1390156.1390294](https://doi.org/10.1145/1390156.1390294),
    doi:[10.1145/1390156.1390294](https:/doi.org/10.1145/1390156.1390294).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018a) Wang, G., Li, C., Wang, W., Zhang, Y., Shen, D., Zhang,
    X., Henao, R., Carin, L., 2018a. Joint embedding of words and labels for text
    classification. arXiv preprint arXiv:1805.04174 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Wang, G., Ma, J., Wang, Y., Tao, T., Ren, G., Zhu, H.,
    2023a. Sudf-rs: A new foreign exchange rate prediction method considering the
    complementarity of supervised and unsupervised deep representation features. Expert
    Systems with Applications 214, 119152. URL: [https://www.sciencedirect.com/science/article/pii/S0957417422021704](https://www.sciencedirect.com/science/article/pii/S0957417422021704),
    doi:[https://doi.org/10.1016/j.eswa.2022.119152](https:/doi.org/https://doi.org/10.1016/j.eswa.2022.119152).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018b) Wang, J., Ma, Y., Zhang, L., Gao, R.X., Wu, D., 2018b.
    Deep learning for smart manufacturing: Methods and applications. Journal of Manufacturing
    Systems 48, 144–156. URL: [https://www.sciencedirect.com/science/article/pii/S0278612518300037](https://www.sciencedirect.com/science/article/pii/S0278612518300037),
    doi:[10.1016/j.jmsy.2018.01.003](https:/doi.org/10.1016/j.jmsy.2018.01.003).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) Wang, J., Qiao, X., Liu, C., Wang, X., Liu, Y., Yao, L.,
    Zhang, H., 2021. Automated ecg classification using a non-local convolutional
    block attention module. Computer Methods and Programs in Biomedicine 203, 106006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023b) Wang, M., Rahardja, S., Fränti, P., Rahardja, S., 2023b.
    Single-lead ecg recordings modeling for end-to-end recognition of atrial fibrillation
    with dual-path rnn. Biomedical Signal Processing and Control 79, 104067.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Wang, Q., Ma, Y., Zhao, K., Tian, Y., 2022. A Comprehensive
    Survey of Loss Functions in Machine Learning. Annals of Data Science 9, 187–212.
    URL: [https://doi.org/10.1007/s40745-020-00253-5](https://doi.org/10.1007/s40745-020-00253-5),
    doi:[10.1007/s40745-020-00253-5](https:/doi.org/10.1007/s40745-020-00253-5).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Wang, Q., Wu, B., Zhu, P., Li, P., Zuo, W., Hu, Q., 2020a.
    ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks. URL:
    [http://arxiv.org/abs/1910.03151](http://arxiv.org/abs/1910.03151), doi:[10.48550/arXiv.1910.03151](https:/doi.org/10.48550/arXiv.1910.03151).
    arXiv:1910.03151 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Wang, R., Li, Z., Cao, J., Chen, T., Wang, L., 2019. Convolutional
    Recurrent Neural Networks for Text Classification, in: 2019 International Joint
    Conference on Neural Networks (IJCNN), pp. 1–6. URL: [https://ieeexplore.ieee.org/document/8852406](https://ieeexplore.ieee.org/document/8852406),
    doi:[10.1109/IJCNN.2019.8852406](https:/doi.org/10.1109/IJCNN.2019.8852406). iSSN:
    2161-4407.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023c) Wang, S., Qu, J., Zhang, Y., Zhang, Y., 2023c. Multimodal
    emotion recognition from eeg signals and facial expressions. IEEE Access 11, 33061–33068.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Wang, W., Li, W., Zhang, N., Liu, K., 2020b. Portfolio formation
    with preselection using deep learning from long-term financial data. Expert Systems
    with Applications 143, 113042.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weiser (1991) Weiser, M., 1991. The computer for the 21 st century. Scientific
    american 265, 94–105.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weng (2019) Weng, L., 2019. From GAN to WGAN. URL: [http://arxiv.org/abs/1904.08994](http://arxiv.org/abs/1904.08994),
    doi:[10.48550/arXiv.1904.08994](https:/doi.org/10.48550/arXiv.1904.08994). arXiv:1904.08994
    [cs, stat].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Widrow et al. (1994) Widrow, B., Rumelhart, D.E., Lehr, M.A., 1994. Neural
    networks: applications in industry, business and science. Communications of the
    ACM 37, 93–105. URL: [https://dl.acm.org/doi/10.1145/175247.175257](https://dl.acm.org/doi/10.1145/175247.175257),
    doi:[10.1145/175247.175257](https:/doi.org/10.1145/175247.175257).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wortsman et al. (2022) Wortsman, M., Ilharco, G., Gadre, S.Y., Roelofs, R.,
    Gontijo-Lopes, R., Morcos, A.S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith,
    S., others, 2022. Model soups: averaging weights of multiple fine-tuned models
    improves accuracy without increasing inference time, in: International Conference
    on Machine Learning, PMLR. pp. 23965–23998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021) Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L.,
    Zhang, L., 2021. CvT: Introducing Convolutions to Vision Transformers, in: 2021
    IEEE/CVF International Conference on Computer Vision (ICCV), pp. 22–31. URL: [https://ieeexplore.ieee.org/document/9710031?denied=](https://ieeexplore.ieee.org/document/9710031?denied=),
    doi:[10.1109/ICCV48922.2021.00009](https:/doi.org/10.1109/ICCV48922.2021.00009).
    iSSN: 2380-7504.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020) Wu, J., Mu, T., Thiyagalingam, J., Goulermas, J.Y., 2020.
    Building interactive sentence-aware representation based on generative language
    model for community question answering. Neurocomputing 389, 93–107. Publisher:
    Elsevier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022) Wu, X., Xia, Y., Zhu, J., Wu, L., Xie, S., Qin, T., 2022.
    A study of BERT for context-aware neural machine translation. Machine Learning
    111, 917–935. Publisher: Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2016) Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Macherey,
    W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., others, 2016. Google’s neural
    machine translation system: Bridging the gap between human and machine translation.
    arXiv preprint arXiv:1609.08144 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xia et al. (2023) Xia, W., Zhang, R., Zhang, X., Usman, M., 2023. A novel method
    for diagnosing alzheimer’s disease using deep pyramid cnn based on eeg signals.
    Heliyon 9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2021) Xiao, T., Singh, M., Mintun, E., Darrell, T., Dollár, P.,
    Girshick, R., 2021. Early convolutions help transformers see better. Advances
    in neural information processing systems 34, 30392–30400.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2022) Xie, Q., Bishop, J.A., Tiwari, P., Ananiadou, S., 2022. Pre-trained
    language models with domain knowledge for biomedical extractive summarization.
    Knowledge-Based Systems 252, 109460. Publisher: Elsevier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2017) Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K., 2017.
    Aggregated Residual Transformations for Deep Neural Networks. URL: [http://arxiv.org/abs/1611.05431](http://arxiv.org/abs/1611.05431),
    doi:[10.48550/arXiv.1611.05431](https:/doi.org/10.48550/arXiv.1611.05431). arXiv:1611.05431
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiong et al. (2022) Xiong, W., Xiong, Z., Cui, Y., 2022. An Explainable Attention
    Network for Fine-Grained Ship Classification Using Remote-Sensing Images. IEEE
    Transactions on Geoscience and Remote Sensing 60, 1–14. URL: [https://ieeexplore.ieee.org/abstract/document/9741720](https://ieeexplore.ieee.org/abstract/document/9741720),
    doi:[10.1109/TGRS.2022.3162195](https:/doi.org/10.1109/TGRS.2022.3162195). conference
    Name: IEEE Transactions on Geoscience and Remote Sensing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2018) Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X.,
    He, X., 2018. AttnGAN: Fine-grained text to image generation with attentional
    generative adversarial networks, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, pp. 1316–1324.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2023) Yan, Y., Liu, F., Zhuang, X., Ju, J., 2023. An R-Transformer_bilstm
    Model Based on Attention for Multi-label Text Classification. Neural Processing
    Letters 55, 1293–1316. URL: [https://doi.org/10.1007/s11063-022-10938-y](https://doi.org/10.1007/s11063-022-10938-y),
    doi:[10.1007/s11063-022-10938-y](https:/doi.org/10.1007/s11063-022-10938-y).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2024) Yang, B., Xiang, X., Kong, W., Zhang, J., Peng, Y., 2024.
    DMF-GAN: Deep Multimodal Fusion Generative Adversarial Networks for Text-to-Image
    Synthesis. IEEE Transactions on Multimedia , 1–13URL: [https://ieeexplore.ieee.org/document/10413630](https://ieeexplore.ieee.org/document/10413630),
    doi:[10.1109/TMM.2024.3358086](https:/doi.org/10.1109/TMM.2024.3358086). conference
    Name: IEEE Transactions on Multimedia.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Yang, J., Shen, X., Xing, J., Tian, X., Li, H., Deng, B.,
    Huang, J., Hua, X.s., 2019. Quantization networks, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Yang, J., Soltan, A.A., Eyre, D.W., Clifton, D.A., 2023.
    Algorithmic fairness and bias mitigation for clinical machine learning with deep
    reinforcement learning. Nature Machine Intelligence 5, 884–894. Publisher: Nature
    Publishing Group UK London.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye and Yang (2021) Ye, F., Yang, J., 2021. A deep neural network model for
    speaker identification. Applied Sciences 11. URL: [https://www.mdpi.com/2076-3417/11/8/3603](https://www.mdpi.com/2076-3417/11/8/3603),
    doi:[10.3390/app11083603](https:/doi.org/10.3390/app11083603).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2015) Yin, J., Jiang, X., Lu, Z., Shang, L., Li, H., Li, X., 2015.
    Neural generative question answering. arXiv preprint arXiv:1512.01337 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2008) Yu, B., Xu, Z.b., Li, C.h., 2008. Latent semantic analysis
    for text categorization using neural network. Knowledge-Based Systems 21, 900–904.
    URL: [https://www.sciencedirect.com/science/article/pii/S0950705108000993](https://www.sciencedirect.com/science/article/pii/S0950705108000993),
    doi:[10.1016/j.knosys.2008.03.045](https:/doi.org/10.1016/j.knosys.2008.03.045).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zagoruyko and Komodakis (2017) Zagoruyko, S., Komodakis, N., 2017. Wide Residual
    Networks. URL: [http://arxiv.org/abs/1605.07146](http://arxiv.org/abs/1605.07146),
    doi:[10.48550/arXiv.1605.07146](https:/doi.org/10.48550/arXiv.1605.07146). arXiv:1605.07146
    [cs] version: 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeiler and Fergus (2013) Zeiler, M.D., Fergus, R., 2013. Visualizing and Understanding
    Convolutional Networks. URL: [http://arxiv.org/abs/1311.2901](http://arxiv.org/abs/1311.2901),
    doi:[10.48550/arXiv.1311.2901](https:/doi.org/10.48550/arXiv.1311.2901). arXiv:1311.2901
    [cs] version: 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang,
    X., Metaxas, D.N., 2017. StackGAN: Text to photo-realistic image synthesis with
    stacked generative adversarial networks, in: Proceedings of the IEEE international
    conference on computer vision, pp. 5907–5915.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018a) Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang,
    X., Metaxas, D.N., 2018a. StackGAN++: Realistic image synthesis with stacked generative
    adversarial networks. IEEE transactions on pattern analysis and machine intelligence
    41, 1947–1962. Publisher: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019a) Zhang, H.B., Zhang, Y.X., Zhong, B., Lei, Q., Yang, L.,
    Du, J.X., Chen, D.S., 2019a. A comprehensive survey of vision-based human action
    recognition methods. Sensors 19, 1005.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020a) Zhang, J., Liu, A., Gao, M., Chen, X., Zhang, X., Chen,
    X., 2020a. Ecg-based multi-class arrhythmia detection using spatio-temporal attention-based
    convolutional recurrent neural network. Artificial Intelligence in Medicine 106,
    101856.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019b) Zhang, R., Zong, Q., Dou, L., Zhao, X., 2019b. A novel
    hybrid deep learning scheme for four-class motor imagery classification. Journal
    of neural engineering 16, 066004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020b) Zhang, W.E., Sheng, Q.Z., Alhazmi, A., Li, C., 2020b.
    Adversarial attacks on deep-learning models in natural language processing: A
    survey. ACM Transactions on Intelligent Systems and Technology (TIST) 11, 1–41.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021) Zhang, X., Han, Y., Xu, W., Wang, Q., 2021. Hoba: A novel
    feature engineering methodology for credit card fraud detection with a deep learning
    architecture. Information Sciences 557, 302–316.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018b) Zhang, Z., Xie, Y., Yang, L., 2018b. Photographic text-to-image
    synthesis with a hierarchically-nested adversarial network, in: Proceedings of
    the IEEE conference on computer vision and pattern recognition, pp. 6199–6208.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2022) Zhao, G., Yang, W., Ren, X., Li, L., Wu, Y., Sun, X., 2022.
    Well-classified examples are underestimated in classification with deep neural
    networks, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp.
    9180–9189. Issue: 8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao and He (2015) Zhao, Y., He, L., 2015. Deep learning in the eeg diagnosis
    of alzheimer’s disease, in: Computer Vision-ACCV 2014 Workshops: Singapore, Singapore,
    November 1-2, 2014, Revised Selected Papers, Part I 12, Springer. pp. 340–353.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2019a) Zhu, F., Ye, F., Fu, Y., Liu, Q., Shen, B., 2019a. Electrocardiogram
    generation with a bidirectional lstm-cnn generative adversarial network. Scientific
    reports 9, 6734.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2019b) Zhu, M., Pan, P., Chen, W., Yang, Y., 2019b. DM-GAN: Dynamic
    memory generative adversarial networks for text-to-image synthesis, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5802–5810.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: zhu et al. (2019) zhu, w., Chen, X., Wang, Y., Wang, L., 2019. Arrhythmia recognition
    and classification using ecg morphology and segment feature analysis. IEEE/ACM
    Transactions on Computational Biology and Bioinformatics 16, 131–138. doi:[10.1109/TCBB.2018.2846611](https:/doi.org/10.1109/TCBB.2018.2846611).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020) Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J., 2020.
    Deformable DETR: Deformable transformers for end-to-end object detection. arXiv
    preprint arXiv:2010.04159 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Zhu, X., Zhu, Y., Zhang, L., Chen, Y., 2023. A BERT-based
    multi-semantic learning model with aspect-aware enhancement for aspect polarity
    classification. Applied Intelligence 53, 4609–4623. URL: [https://doi.org/10.1007/s10489-022-03702-1](https://doi.org/10.1007/s10489-022-03702-1),
    doi:[10.1007/s10489-022-03702-1](https:/doi.org/10.1007/s10489-022-03702-1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang et al. (2020) Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H.,
    Xiong, H., He, Q., 2020. A comprehensive survey on transfer learning. Proceedings
    of the IEEE 109, 43–76. Publisher: IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou and Hastie (2005) Zou, H., Hastie, T., 2005. Regularization and Variable
    Selection Via the Elastic Net. Journal of the Royal Statistical Society Series
    B: Statistical Methodology 67, 301–320. URL: [https://doi.org/10.1111/j.1467-9868.2005.00503.x](https://doi.org/10.1111/j.1467-9868.2005.00503.x),
    doi:[10.1111/j.1467-9868.2005.00503.x](https:/doi.org/10.1111/j.1467-9868.2005.00503.x).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
