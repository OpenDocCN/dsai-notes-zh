- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:42:35'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2301.00394] Deep Learning Technique for Human Parsing: A Survey and Outlook'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2301.00394](https://ar5iv.labs.arxiv.org/html/2301.00394)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning Technique for Human Parsing: A Survey and Outlook'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lu Yang, Wenhe Jia, Shan Li, Qing Song Lu Yang, Wenhe Jia, Shan Li, Qing Song
    are with the Beijing University of Posts and Telecommunications, Beijing, 100876,
    China (e-mail: soeaver@bupt.edu.cn; jiawh@bupt.edu.cn; ls1995@bupt.edu.cn; priv@bupt.edu.cn)
    Corresponding author: Qing Song (email: priv@bupt.edu.cn)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Human parsing aims to partition humans in image or video into multiple pixel-level
    semantic parts. In the last decade, it has gained significantly increased interest
    in the computer vision community and has been utilized in a broad range of practical
    applications, from security monitoring, to social media, to visual special effects,
    just to name a few. Although deep learning-based human parsing solutions have
    made remarkable achievements, many important concepts, existing challenges, and
    potential research directions are still confusing. In this survey, we comprehensively
    review three core sub-tasks: single human parsing, multiple human parsing, and
    video human parsing, by introducing their respective task settings, background
    concepts, relevant problems and applications, representative literature, and datasets.
    We also present quantitative performance comparisons of the reviewed methods on
    benchmark datasets. Additionally, to promote sustainable development of the community,
    we put forward a transformer-based human parsing framework, providing a high-performance
    baseline for follow-up research through universal, concise, and extensible solutions.
    Finally, we point out a set of under-investigated open issues in this field and
    suggest new directions for future study. We also provide a regularly updated project
    page, to continuously track recent developments in this fast-advancing field:
    [https://github.com/soeaver/awesome-human-parsing](https://github.com/soeaver/awesome-human-parsing).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Human Parsing, Human Parsing Datasets, Deep Learning, Literature Survey
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Human parsing [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4),
    [5](#bib.bib5)], considered as the fundamental task of human-centric visual understanding
    [[6](#bib.bib6)], aims to classify the human parts and clothing accessories in
    images or videos at pixel-level. Numerous studies have been conducted on human
    parsing due to its crucial role in widespread application areas, *e.g*., security
    monitoring, autonomous driving, social media, electronic commerce, visual special
    effects, artistic creation, giving birth to various excellent human parsing solutions
    and applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/31053667441c87a22df39aa4f4b07914.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Human parsing tasks reviewed in this survey: (a) single human parsing
    (SHP) [[7](#bib.bib7)]; (b) multiple human parsing (MHP) [[8](#bib.bib8)]; (c)
    video human parsing (VHP) [[9](#bib.bib9)].'
  prefs: []
  type: TYPE_NORMAL
- en: As early as the beginning of this century, some studies tried to identify the
    level of upper body clothing [[10](#bib.bib10)], the grammatical representations
    of clothing [[11](#bib.bib11)] and the deformation of body contour [[12](#bib.bib12)]
    under very limited circumstances. These early studies facilitated the research
    on pixel-level human parts and clothing recognition, *i.e*., human parsing task.
    Immediately afterward, some traditional machine learning and computer vision techniques
    were utilized to solve human parsing problems, *e.g*., structured model [[13](#bib.bib13),
    [14](#bib.bib14), [1](#bib.bib1)], clustering algorithm [[15](#bib.bib15)], grammar
    model [[16](#bib.bib16), [17](#bib.bib17)], conditional random field [[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20)], template matching [[21](#bib.bib21), [22](#bib.bib22)]
    and super-pixel [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]. Afterward,
    the prosperity of deep learning and convolutional neural network [[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32)] has further promoted the vigorous development of human parsing.
    Attention mechanism [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)],
    scale-aware features [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)],
    tree structure [[3](#bib.bib3), [41](#bib.bib41)], graph structure [[42](#bib.bib42),
    [4](#bib.bib4), [43](#bib.bib43)], edge-aware learning [[44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46)], pose-aware learning [[2](#bib.bib2), [47](#bib.bib47), [48](#bib.bib48)]
    and other technologies [[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52)] greatly improved the performance of human parsing. However,
    some existing challenges and under-investigated issues make human parsing still
    a task worthy of further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6167ef5c49caeff741eb38dee59fa42c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Outline of this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the rapid development of human parsing, several literature reviews have
    been produced. However, existing surveys are not precise and in-depth: some surveys
    only provide a superficial introduction of human parsing from a macro fashion/social
    media perspective [[53](#bib.bib53), [54](#bib.bib54)], or only review a sub-task
    of human parsing from a micro face parsing perspective [[55](#bib.bib55)]. In
    addition, due to the fuzziness of taxonomy and the diversity of methods, comprehensive
    and in-depth investigation is highly needed and helpful. In response, we provide
    the first review that systematically introduces background concepts, recent advances,
    and an outlook on human parsing.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Scope
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This survey reviews human parsing from a comprehensive perspective, including
    not only single human parsing (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Deep Learning Technique for Human Parsing: A Survey and Outlook") (a)) but also
    multiple human parsing (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning
    Technique for Human Parsing: A Survey and Outlook") (b)) and video human parsing
    (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning Technique for Human
    Parsing: A Survey and Outlook") (c)). At the technical level, this survey focuses
    on the deep learning-based human parsing methods and datasets in recent ten years.
    To provide the necessary background, it also introduces some relevant literature
    from non-deep learning and other fields. At the practical level, the advantages
    and disadvantages of various methods are compared, and detailed performance comparisons
    are given. In addition to summarizing and analyzing the existing work, we also
    give an outlook for the future opportunities of human parsing and put forward
    a new transformer-based baseline to promote sustainable development of the community.
    A curated list of human parsing methods and datasets and the proposed transformer-based
    baseline can be found at [https://github.com/soeaver/awesome-human-parsing](https://github.com/soeaver/awesome-human-parsing).'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Organization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning Technique for
    Human Parsing: A Survey and Outlook") shows the outline of this survey. §[2](#S2
    "2 Preliminaries ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")
    gives some brief background on problem formulation and challenges (§[2.1](#S2.SS1
    "2.1 Problem Formulation and Challenges ‣ 2 Preliminaries ‣ Deep Learning Technique
    for Human Parsing: A Survey and Outlook")), human parsing taxonomy (§[2.2](#S2.SS2
    "2.2 Human Parsing Taxonomy ‣ 2 Preliminaries ‣ Deep Learning Technique for Human
    Parsing: A Survey and Outlook")), relevant tasks (§[2.3](#S2.SS3 "2.3 Relevant
    Tasks ‣ 2 Preliminaries ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook")), and applications of human parsing (§[2.4](#S2.SS4 "2.4 Applications
    of Human Parsing ‣ 2 Preliminaries ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")). §[3](#S3 "3 Deep Learning Based Human Parsing ‣ Deep
    Learning Technique for Human Parsing: A Survey and Outlook") provides a detailed
    review of representative deep learning-based human parsing studies. Frequently
    used datasets and performance comparisons are reviewed in §[4](#S4 "4 Human Parsing
    Datasets ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook") and
    §[5](#S5 "5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook"). An outlook for the future opportunities of human parsing
    is presented in §[6](#S6 "6 An Outlook: Future Opportunities of Human Parsing
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook"), including
    a new transformer-based baseline (§[6.1](#S6.SS1 "6.1 A Transformer-based Baseline
    for Human Parsing ‣ 6 An Outlook: Future Opportunities of Human Parsing ‣ Deep
    Learning Technique for Human Parsing: A Survey and Outlook")), several under-investigated
    open issues (§[6.2](#S6.SS2 "6.2 Under-Investigated Open Issues ‣ 6 An Outlook:
    Future Opportunities of Human Parsing ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")) and new directions (§[6.3](#S6.SS3 "6.3 New Directions
    ‣ 6 An Outlook: Future Opportunities of Human Parsing ‣ Deep Learning Technique
    for Human Parsing: A Survey and Outlook")) for future study. Conclusions will
    be drawn in §[7](#S7 "7 Conclusions ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Problem Formulation and Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Formally, we use $x$ to represent input human-centric data, $y$ to represent
    pixel-level supervision target, $\mathcal{X}$ and $\mathcal{Y}$ to denote the
    space of input data and supervision target. Human parsing is to map data $x$ to
    target $y$: $\bm{\mathcal{X}}\mapsto\bm{\mathcal{Y}}$. The problem formulation
    is consistent with image segmentation [[56](#bib.bib56)], but $\mathcal{X}$ is
    limited to the human-centric space. Therefore, in many literatures, human parsing
    is regarded as fine-grained image segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The central problem of human parsing is how to model human structures. As we
    all know, the human body presents a highly structured hierarchy, and all parts
    interact naturally. Most parsers hope to construct this interaction explicitly
    or implicitly. However, the following challenges make the problem more complicated:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Large Intra-class Variation. In human parsing, objects with large
    visual appearance gaps may share the same semantic categories. For example, “upper
    clothes” is an abstract concept without strict visual constraints. Many kinds
    of objects of color, texture, and shape belong to this category, leading to significant
    intra-class variations. Further challenges may be added by illumination changes,
    different viewpoints, noise corruption, low-image resolution, and filtering distortion.
    Large intra-class variations will increase the difficulty of classifier learning
    decision boundaries, resulting in semantic inconsistency in prediction.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Unconstrained Poses. In the earlier human parsing benchmarks [[1](#bib.bib1),
    [25](#bib.bib25), [14](#bib.bib14), [37](#bib.bib37)], the data is usually collected
    from fashion media. From them people often stand or have a limited number of simple
    pose. However, in the wild, human pose is unconstrained, showing great diversity.
    Therefore, more and more studies begin to pay attention to real-world human parsing.
    Unconstrained poses will increase the state space of target geometrically, which
    brings great challenges to the human semantic representations. Moreover, the left-right
    discrimination problem in human parsing is widespread (*e.g*., left-arm vs right-arm,
    left-leg vs right-leg), and it is also severely affected by unconstrained poses
    [[49](#bib.bib49), [44](#bib.bib44), [57](#bib.bib57)].
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Occlusion. Occlusion mainly presents two modes: (1) occlusion between
    humans and objects; (2) occlusion between humans. The former will destroy the
    continuity of human parts or clothing, resulting in incomplete apparent information
    of the targets, forming local semantic loss, and easily causing ambiguity [[37](#bib.bib37),
    [39](#bib.bib39)]. The latter is a more severe challenge. In addition to continuity
    destruction, it often causes foreground confusion. In human parsing, only the
    occluded target human is regarded as the foreground, while the others are regarded
    as the background. However, they have similar appearance, making it difficult
    to determine which part belongs to the foreground [[58](#bib.bib58)].'
  prefs: []
  type: TYPE_NORMAL
- en: Remark. In addition to the above challenges, some scenario-based challenges
    also hinder the progress of human parsing, such as the trade-off between inference
    efficiency and accuracy in crowded scenes, motion blur, and camera position changes
    in movement scenes.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Human Parsing Taxonomy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to the characteristics (number of humans, data modal) of the input
    space $\mathcal{X}$, human parsing can be categorized into three sub-tasks (see
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning Technique for Human
    Parsing: A Survey and Outlook")): single human parsing, multiple human parsing,
    and video human parsing.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Single Human Parsing (SHP). SHP is the cornerstone of human parsing,
    which assumes that there is only one foreground human instance in the image. Therefore,
    $y$ just contains corresponding semantic category supervision at the pixel-level.
    Simple and straightforward task definitions make most related research focus on
    how to model robust and generalized human parts relationship. In addition to being
    the cornerstone of human parsing, SHP is also often used as an auxiliary supervision
    for some tasks, *e.g*., person re-identification, human mesh reconstruction, virtual
    try-on.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Multiple Human Parsing (MHP). Multiple human parsing, also known as
    instance-level human parsing, aims to parse multiple human instances in a single
    pass. Besides category information, $y$ also provides instance supervision in
    pixel-level, *i.e*., the person identity of each pixel. The core problems of MHP
    are how to discriminate different human instances and how to learn each human
    feature in crowded scenes comprehensively. In addition, inference efficiency is
    also an important concern of MHP. Ideally, inference should be real-time and independent
    of human instance numbers. Except as an independent task, MHP sometimes is jointed
    with other human visual understanding tasks in a multi-task learning manner, such
    as pose estimation [[59](#bib.bib59), [60](#bib.bib60)], dense pose [[61](#bib.bib61)]
    or panoptic segmentation [[62](#bib.bib62)].
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Video Human Parsing (VHP). VHP needs to parse every human in the video
    data, which can be regarded as a complex visual task integrating video segmentation
    and image-level human parsing. The current VHP studies mainly adopt the unsupervised
    video object segmentation settings [[63](#bib.bib63)], *i.e*., $y$ is unknown
    in the training stage, and the ground-truth of the first frame is given in the
    inference stage. The temporal correspondence will only be approximated according
    to $x$. Relative to SHP and MHP, VHP faces more challenges that are inevitable
    in video segmentation settings, *e.g*., motion blur and camera position changes.
    Benefitting by the gradual popularity of video data, VHP has a wide range of application
    potential, and the typical cases are intelligent monitoring and video editing.
  prefs: []
  type: TYPE_NORMAL
- en: Remark. Over recent years, some potential research directions have also received
    attention, including weakly-supervised human parsing [[64](#bib.bib64), [51](#bib.bib51),
    [48](#bib.bib48)], one-shot human parsing [[65](#bib.bib65), [66](#bib.bib66)]
    and interactive human parsing [[67](#bib.bib67)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Relevant Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Among the research in computer vision, there are some tasks with strong relevance
    to human parsing, which are briefly described in the following.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Pose Estimation. The purpose of pose estimation is to locate human
    parts and build body representations (such as skeletons) from input data. Human
    parsing and pose estimation share the same input space $\mathcal{X}$, but there
    are some differences in the supervision targets. The most crucial difference is
    that human parsing is a dense prediction task, which needs to predict the category
    of each pixel. Meanwhile, pose estimation is a sparse prediction task, only focusing
    on the location of a limited number of keypoints. These two tasks are also often
    presented in multi-task learning, or one of them is used as a guiding condition
    for the other. For example, human parsing as a guide can help pose estimation
    to reduce the impact of clothing on human appearance [[19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Image Segmentation. Image segmentation is a fundamental topic in image
    processing and computer vision. It mainly includes semantic segmentation and instance
    segmentation. As a basic visual task, there are many research directions can be
    regarded as branches, and human parsing is one of them. In the pre-deep learning
    era, image segmentation focuses on the continuity of color, texture, and edge,
    while human parsing pays more attention to the body topology modeling. In the
    deep learning era, the methods in two fields show more similarities. However,
    more and more human parsing literature choose to model the parts relationship
    as the goal, which is significantly different from the general goal of image segmentation.
    Therefore, human parsing and image segmentation are closely related but independent
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Remark. Ordinarily, most human-centric dense prediction task show positively
    relevance with human parsing, *e.g*., human matting [[68](#bib.bib68), [69](#bib.bib69)],
    human mesh reconstruction [[70](#bib.bib70), [71](#bib.bib71)] and face/hand parsing
    [[72](#bib.bib72), [73](#bib.bib73)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/39912788654f0e8a08f303b5a2e9d7c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Timeline of representative human parsing works from 2012 to 2022.
    The upper part represents the datasets of human parsing (§[4](#S4 "4 Human Parsing
    Datasets ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")),
    and the lower part represents the models of human parsing (§[3](#S3 "3 Deep Learning
    Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and
    Outlook")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Applications of Human Parsing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a crucial task in computer vision, there are a large number of applications
    based on human parsing. We will introduce some common ones below.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Dense Pose Estimation. The goal of dense pose estimation is to map
    all human pixels in an RGB image to the 3D surface of the human body [[74](#bib.bib74)].
    Human parsing is an important pre-condition that can constrain the mapping of
    dense points. At present, the mainstream dense pose estimation methods explicitly
    integrate human parsing supervision, such as DensePose R-CNN [[74](#bib.bib74)],
    Parsing R-CNN [[61](#bib.bib61)], and SimPose [[75](#bib.bib75)]. Therefore, the
    performance of human parsing will directly affect dense pose estimation results.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Person Re-identification. Person re-identification seeks to predict
    whether two images from different cameras belong to the same person. The apparent
    characteristics of human body is an important factor affecting the accuracy. Human
    parsing can provide pixel-level semantic information, helping re-identification
    models perceive the position and composition of human parts/clothing. Various
    studies have introduced human parsing explicitly or implicitly into re-identification
    methods, which improves the model performance in multiple aspects, *e.g*., local
    visual cues [[76](#bib.bib76), [77](#bib.bib77)], spatial alignment [[78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80)], background-bias elimination [[81](#bib.bib81)],
    domain adaptation [[82](#bib.bib82)], clothes changing [[83](#bib.bib83), [84](#bib.bib84)].
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Virtual Try-on. Virtual try-on is a burgeoning and interesting application
    in the vision and graphic communities [[85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88), [89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91), [92](#bib.bib92)].
    Most of the research follows the three processes: human parsing, appearance generation,
    and refinement. Therefore, human parsing is a necessary step to obtain clothing
    masks, appearance constraints and pose maintenance. Recently, some work began
    to study the parser-free virtual try-on [[93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)].
    Through teacher-student learning, parsing-based pre-training, and other technologies,
    the virtual try-on can be realized without the human parsing map during inference.
    However, most of these works still introduced the parsing results during training,
    and the generation quality retains gap from parser-based methods.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Conditional Human Image Generation. Image generation/synthesis as
    a field has seen a lot of progress in recent years [[96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98), [99](#bib.bib99)]. Non-existent but fidelity images can be created
    in large quantities. Among them, human image generation has attracted attention
    because of its rich downstream applications. Compared with unconditional generation,
    conditional generation can produce corresponding output as needed, and human parsing
    map is one of the most widely used pre-conditions. There have been a lot of excellent
    works on parsing-based conditional human image generation, *e.g*., CPFNet [[100](#bib.bib100)]
    and InsetGAN [[101](#bib.bib101)].
  prefs: []
  type: TYPE_NORMAL
- en: Remark. Besides the above cases, in general, most of the human-centric generation
    applications can be built with the help of human parsing, *e.g*., deepfakes [[102](#bib.bib102),
    [103](#bib.bib103)], style transfer [[104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)],
    clothing editing [[107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109)].
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning Based Human Parsing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The existing human parsing can be categorized into three sub-tasks: single
    human parsing, multiple human parsing, and video human parsing, focusing on parts
    relationship modeling, human instance discrimination, and temporal correspondence
    learning, respectively. According to this taxonomy, we sort out the representative
    works (lower part of Figure [3](#S2.F3 "Figure 3 ‣ 2.3 Relevant Tasks ‣ 2 Preliminaries
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")) and review
    them in detail below.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Single Human Parsing (SHP) Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SHP considers extracting human features through parts relationship modeling.
    According to the modeling strategy, SHP models can be divided into three main
    classes: context learning, structured representation, and multi-task learning.
    Moreover, considering some special but interesting methods, we will review them
    as “other modeling models”. Table [I](#S3.T1 "Table I ‣ 3.1 Single Human Parsing
    (SHP) Models ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for
    Human Parsing: A Survey and Outlook") summarizes the characteristics for reviewed
    SHP models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table I: Summary of essential characteristics for reviewed SHP models (§[3.1](#S3.SS1
    "3.1 Single Human Parsing (SHP) Models ‣ 3 Deep Learning Based Human Parsing ‣
    Deep Learning Technique for Human Parsing: A Survey and Outlook")). The training
    datasets and whether it is open source are also listed. See §[4](#S4 "4 Human
    Parsing Datasets ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")
    for more detailed descriptions of datasets. These notes also apply to the other
    tables.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   |  |  | Context | Structured | Multi-task | Others |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Year | Method | Pub. | Attention | Scale-aware | Tree | Graph | Edge | Pose
    | Denoising | Adversarial | Datasets | Open Source |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | Yamaguchi [[1](#bib.bib1)] | CVPR | - | - | - | - | - | ✓ | - | -
    | FS | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | DMPM [[14](#bib.bib14)] | ICCV | - | - | ✓ | - | - | - | - | - | FS/DP
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| PaperDoll [[20](#bib.bib20)] | ICCV | - | - | - | - | - | ✓ | - | - | FS
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| CFPD [[25](#bib.bib25)] | TMM | - | - | - | - | - | ✓ | - | - | CFPD | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2014 | HPM [[17](#bib.bib17)] | CVPR | - | - | ✓ | - | - | ✓ | - | - | FS/DP
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | M-CNN [[110](#bib.bib110)] | CVPR | - | - | - | ✓ | - | - | - | -
    | ATR | - |'
  prefs: []
  type: TYPE_TB
- en: '| Co-CNN [[37](#bib.bib37)] | ICCV | - | ✓ | - | - | - | - | - | - | FS/ATR
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| FPVC [[111](#bib.bib111)] | TMM | - | - | - | - | - | ✓ | - | - | FS/DP |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| ATR [[22](#bib.bib22)] | TPAMI | - | - | ✓ | - | - | - | - | - | FS/DP |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | AOG [[112](#bib.bib112)] | AAAI | - | - | ✓ | - | - | ✓ | - | - |
    - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Attention [[33](#bib.bib33)] | CVPR | ✓ | ✓ | - | - | - | - | - | - | PPP
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| LG-LSTM [[34](#bib.bib34)] | CVPR | ✓ | - | - | - | - | - | - | - | FS/ATR/PPP
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-LSTM [[113](#bib.bib113)] | ECCV | ✓ | - | - | ✓ | - | - | - | - |
    FS/ATR/PPP | - |'
  prefs: []
  type: TYPE_TB
- en: '| HAZN [[38](#bib.bib38)] | ECCV | - | ✓ | - | - | - | - | - | - | PPP | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| SYSU-Clothes [[114](#bib.bib114)] | TMM | - | - | - | ✓ | - | - | - | - |
    SYSU-Clothes | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Struc-LSTM [[115](#bib.bib115)] | CVPR | ✓ | - | ✓ | ✓ | - | - | -
    | - | ATR/PPP | - |'
  prefs: []
  type: TYPE_TB
- en: '| SSL [[116](#bib.bib116)] | CVPR | - | - | - | - | - | ✓ | - | - | LIP/PPP
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Joint [[117](#bib.bib117)] | CVPR | - | - | - | - | - | ✓ | - | - | PPP |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | ProCNet [[118](#bib.bib118)] | AAAI | - | - | ✓ | - | - | - | - |
    - | PPP | - |'
  prefs: []
  type: TYPE_TB
- en: '| AFLA [[49](#bib.bib49)] | AAAI | - | - | - | - | - | - | - | ✓ | LIP | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| WSHP [[64](#bib.bib64)] | CVPR | - | - | - | - | - | ✓ | - | - | PPP | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| TGPNet [[119](#bib.bib119)] | MM | - | ✓ | - | - | - | - | - | - | ATR |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| MuLA [[47](#bib.bib47)] | ECCV | - | - | - | - | - | ✓ | - | - | LIP/PPP
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| MMAN [[50](#bib.bib50)] | ECCV | - | - | - | - | - |  | - | ✓ | LIP/PPP/PPSS
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| JPPNet [[2](#bib.bib2)] | TPAMI | - | - | - | - | - | ✓ | - | - | LIP/PPP
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | - | ✓ | - | - | ✓ | - | - | - | LIP
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Graphonomy [[42](#bib.bib42)] | CVPR | - | - | ✓ | ✓ | - | - | - | - | ATR/PPP
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| CNIF [[3](#bib.bib3)] | ICCV | - | - | ✓ | - | - | - | - | - | ATR/LIP/PPP
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| BSANet [[120](#bib.bib120)] | ICCV | - | ✓ | - | - | ✓ | - | - | - | PPP
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| SPGNet [[36](#bib.bib36)] | ICCV | ✓ | - | - | - | - | - | - | - | PPP |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| BraidNet [[57](#bib.bib57)] | MM | - | ✓ | - | - | - | - | - | - | LIP |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Grapy-ML [[121](#bib.bib121)] | AAAI | ✓ | - | ✓ | ✓ | - | - | - |
    - | ATR/PPP | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| HHP [[4](#bib.bib4)] | CVPR | - | - | ✓ | ✓ | - | - | - | - | ATR/LIP/PPP/PPSS
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| SLRS [[51](#bib.bib51)] | CVPR | - | - | - | ✓ | ✓ | - | ✓ | - | ATR/LIP
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| PCNet [[39](#bib.bib39)] | CVPR | - | ✓ | - | ✓ | - | - | - | - | LIP/PPP
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| CorrPM [[45](#bib.bib45)] | CVPR | - | - | - | - | ✓ | ✓ | - | - | ATR/LIP
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| DTCF [[46](#bib.bib46)] | MM | - | ✓ | - | - | ✓ | - | - | - | LIP/PPP |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| SemaTree [[41](#bib.bib41)] | ECCV | ✓ | - | ✓ | - | - | - | - | - | LIP
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| OCR [[122](#bib.bib122)] | ECCV | ✓ | ✓ | - | - | - | - | - | - | LIP | ✓
    |'
  prefs: []
  type: TYPE_TB
- en: '| BGNet [[123](#bib.bib123)] | ECCV | - | - | ✓ | ✓ | - | - | - | - | LIP/PPP/PPSS
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| HRNet [[124](#bib.bib124)] | TPAMI | - | ✓ | - | - | - | - | - | - | LIP
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | - | - | - | - | ✓ | - | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ATR/LIP/PPP &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | HIPN [[125](#bib.bib125)] | AAAI | - | - | - | - | - | - | ✓ | - |
    LIP/PPP | - |'
  prefs: []
  type: TYPE_TB
- en: '| POPNet [[65](#bib.bib65)] | AAAI | ✓ | - | - | - | - | - | - | - | ATR-OS
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| MCIBI [[126](#bib.bib126)] | ICCV | ✓ | - | - | - | - | - | - | - | LIP |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| ISNet [[127](#bib.bib127)] | ICCV | ✓ | - | - | - | - | - | - | - | LIP |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| NPPNet [[128](#bib.bib128)] | ICCV | - | ✓ | - | - | - | ✓ | - | - | LIP/PPP
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| HTCorrM [[129](#bib.bib129)] | TPAMI | ✓ | - | - | - | ✓ | ✓ | - | - | ATR/LIP
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| PRHP [[130](#bib.bib130)] | TPAMI | - | - | ✓ | ✓ | - | - | - | - | ATR/LIP/PPP/PPSS
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CDGNet [[131](#bib.bib131)] | CVPR | ✓ | - | - | - | ✓ | - | - | -
    | ATR/LIP | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| HSSN [[5](#bib.bib5)] | CVPR | - | ✓ | ✓ | - | - | - | - | - | LIP/PPP |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| PRM [[43](#bib.bib43)] | TMM | - | - | - | ✓ | - | - | - | - | LIP/PPP |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| PADNet [[48](#bib.bib48)] | TPAMI | - | - | - | - | - | ✓ | - | - | PPP |
    - |'
  prefs: []
  type: TYPE_TB
- en: 3.1.1 Context Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Context learning, a mainstream paradigm for single human parsing, seeks to learn
    the connection between local and global features to model human parts relationship.
    Recent studies have developed various context learning methods to handle single
    human parsing, including attention mechanism and scale-aware features.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Attention Mechanism. The first initiative was proposed in [[33](#bib.bib33)]
    that applies an attention mechanism for parts relationship modeling. Specifically,
    soft weights, learned by attention mechanism, are used to weight different scale
    features and merge them. At almost the same time, LG-LSTM [[34](#bib.bib34)],
    Graph-LSTM [[113](#bib.bib113)] and Struc-LSTM [[115](#bib.bib115)] exploit complex
    local and global context information through Long Short-Term Memory (LSTM) [[132](#bib.bib132)]
    and achieve very competitive results. Then, [[36](#bib.bib36)] proposes a Semantic
    Prediction Guidance (SPG) module that learns to re-weight the local features through
    the guidance from pixel-wise semantic prediction. With the rise of graph model,
    researchers realized that attention mechanism is able to establish the correlation
    between graph model nodes. For example, [[121](#bib.bib121)] introduces Graph
    Pyramid Mutual Learning (Grapy-ML) to address the cross-dataset human parsing
    problem, in which the self-attention is used to model the correlations between
    context nodes. Although attention mechanisms have achieved great results in previous
    work, global context dependency cannot be fully understood due to the lack of
    explicit prior supervision. CDGNet [[131](#bib.bib131)] adopts the human parsing
    labels accumulated in the horizontal and vertical directions as the supervisions,
    aiming to learn the position distribution of human parts, and weighting them to
    the global features through attention mechanism to achieve accurate parts relationship
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Scale-aware Features. The most intuitive context learning method is
    to directly use scale-aware features (*e.g*. multi-scale features [[133](#bib.bib133),
    [134](#bib.bib134)], features pyramid networks [[135](#bib.bib135), [136](#bib.bib136)]),
    which has been widely verified in semantic segmentation [[56](#bib.bib56)]. The
    earliest effort can be tracked back to CoCNN [[37](#bib.bib37)]. It integrates
    cross layer context, global image-level context, super-pixel context, and cross
    super-pixel neighborhood context into a unified architecture, which solves the
    obstacle of low-resolution features in FCN [[31](#bib.bib31)] for modeling parts
    relationship. Subsequently, [[38](#bib.bib38)] proposes Hierarchical Auto-Zoom
    Net (HAZN), which adaptively zooms predicted image regions into their proper scales
    to refine the parsing. TGPNet [[119](#bib.bib119)] considers that the label fragmentation
    and complex annotation in human parsing datasets is a non-negligible problem to
    hinder accurate parts relationship modeling, trying to alleviate this limitation
    by supervising multi-scale context information. PCNet [[39](#bib.bib39)] further
    studies the adaptive contextual features, and captures the representative global
    context by mining the associated semantics of human parts through proposed part
    class module, relational aggregation module, and relational dispersion module.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Structured Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The purpose of structured representation is to learn the inherent combination
    or decomposition mode of human parts, so as to model parts relationship. Research
    efforts in this field are mainly made along two directions: using a tree structure
    to represent the hierarchical relationship between body and parts, and using a
    graph structure to represent the connectivity relationship between different parts.
    These two ideas are complementary to each other, so they have often been adopted
    simultaneously in some recent work.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Tree Structure. DMPM [[14](#bib.bib14)] and HPM [[17](#bib.bib17)]
    solve the single human parsing issue by using the parselets representation, which
    construct a group of parsable segments by low-level over-segmentation algorithms,
    and represent these segments as leaf nodes, then search for the best graph configuration
    to obtain semantic human parsing results. Similarly, [[22](#bib.bib22)] formulates
    human parsing as an Active Template Regression (ATR) problem, where each human
    part is represented as the linear combination of learned mask templates and morphed
    to a more precise mask with the active shape parameters. Then the human parsing
    results are generated from the mask template coefficients and the active shape
    parameters. In the same line of work, ProCNet [[118](#bib.bib118)] deals with
    human parsing as a progressive recognition task, modeling structured parts relationship
    by locating the whole body and then segmenting hierarchical components gradually.
    CNIF [[3](#bib.bib3)] further extends the human tree structure and represents
    human body as a hierarchy of multi-level semantic parts, treating human parsing
    as a multi-source information fusion process. A more efficient solution is developed
    in [[41](#bib.bib41)], which uses a tree structure to encode human physiological
    composition, then designs a coarse to fine process in a cascade manner to generate
    accurate parsing results.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Graph Structure. Graph structure is an excellent relationship modeling
    method. Some researchers consider introducing it into human parsing networks for
    part-relation reasoning. A clothing co-parsing system is designed by [[114](#bib.bib114)],
    which takes the segmented regions as the vertices. It incorporates several contexts
    of clothing configuration to build a multi-image graphical model. To address the
    cross-dataset human parsing problem, Graphonomy [[42](#bib.bib42)] proposes a
    universal human parsing agent, introducing hierarchical graph transfer learning
    to encode the underlying label semantic elements and propagate relevant semantic
    information. BGNet [[123](#bib.bib123)] hopes to improve the accuracy of human
    parsing in similar or cluttered scenes through graph structure. It exploits the
    human inherent hierarchical structure and the relationship between different human
    parts employing grammar rules in both cascaded and paralleled manner to correct
    the segmentation performance of easily confused human parts. A landmark work on
    this line was proposed by Wang *et al*.[[4](#bib.bib4), [130](#bib.bib130)]. A
    hierarchical human parser (HHP) is constructed, representing the hierarchical
    human structure by three kinds of part relations: decomposition, composition,
    and dependency. Besides, HHP uses the prism of a message-passing, feed-back inference
    scheme to reason the human structure effectively. Following this idea, [[43](#bib.bib43)]
    proposes Part-aware Relation Modeling (PRM) to handle human parsing, generating
    features with adaptive context for various sizes and shapes of human parts.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Multi-task Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The auxiliary supervisions can help the parser better understand the relationship
    between parts, such as part edges or human pose. Therefore, multi-task learning
    has become an essential paradigm for single human parsing.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Edge-aware Learning. Edge information is implicit in the human parsing
    dataset. Thus edge-aware supervision or feature can be introduced into the human
    parser without additional labeling costs. In particular, edge-aware learning can
    enhance the model’s ability to discriminate adjacent parts and improve the fineness
    of part boundaries. The typical work is [[44](#bib.bib44)], which proposes a Context
    Embedding with Edge Perceiving (CE2P) framework, using an edge perceiving module
    to integrate the characteristic of object contour to refine the part boundaries.
    Because of its excellent performance and scalability, CE2P has become the baseline
    for many subsequent works. CorrPM [[45](#bib.bib45)] and HTCorrM [[129](#bib.bib129)]
    are built on CE2P, and further use part edges to help model the parts relationship.
    They construct a heterogeneous non-local module to mix the edge, pose and semantic
    features into a hybrid representation, and explore the spatial affinity between
    the hybrid representation and the parsing feature map at all positions. BSANet
    [[120](#bib.bib120)] considers that edge information is helpful to eliminate the
    part-level ambiguities and proposes a joint parsing framework with boundary and
    semantic awareness to address this issue. Specifically, a boundary-aware module
    is employed to make intermediate-level features focus on part boundaries for accurate
    localization, which is then fused with high-level features for efficient part
    recognition. To further enrich the edge-aware features, a dual-task cascaded framework
    (DTCF) is developed in [[46](#bib.bib46)], which implicitly integrates parsing
    and edge features to refine the human parsing results progressively.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Pose-aware Learning. Both human parsing and pose estimation seek to
    predict dense and structured human representation. There is a high intrinsic relationship
    between them. Therefore, some studies have tried to use pose-aware learning to
    assist in parts relationship modeling. As early as 2012, Yamaguchi *et al*. [[1](#bib.bib1),
    [20](#bib.bib20)] exploited the relationship between clothing and the underlying
    body pose, exploring techniques to accurately parse person wearing clothing into
    their constituent garment pieces. Almost immediately, Liu *et al*. [[25](#bib.bib25)]
    combined the human pose estimation module with an MRF-based color/category inference
    module and a super-pixel category classifier module to parse fashion items in
    images. Subsequently, Liu *et al*. [[111](#bib.bib111)] extends this idea to semi-supervised
    human parsing, collecting a large number of unlabeled videos, using cross-frame
    context for human pose co-estimation, and then performs video joint human parsing.
    SSL [[116](#bib.bib116)] and JPPNet [[2](#bib.bib2)] choose to impose human pose
    structures into parsing results without resorting to extra supervision, and adopt
    the multi-task learning manner to explore efficient human parts relationship modeling.
    A similar work is developed by [[47](#bib.bib47)], which presents a Mutual Learning
    to Adapt model (MuLA) for joint human parsing and pose estimation. MuLA can fast
    adjust the parsing and pose models to provide more robust and accurate results
    by incorporating information from corresponding models. Different from the above
    work, Zeng *et al*. [[128](#bib.bib128)]. focus on how to automatically design
    a unified model and perform two tasks simultaneously to benefit each other. Inspired
    by NAS [[137](#bib.bib137)], they propose to search for an efficient network architecture
    (NPPNet), searching the encoder-decoder architectures respectively, and embed
    NAS units in both multi-scale feature interaction and high-level feature fusion.
    To get rid of annotating pixel-wise human parts masks, a weakly-supervised human
    parsing approach is proposed by PADNet [[48](#bib.bib48)]. They develop an iterative
    training framework to transform pose knowledge into part priors, so that only
    pose annotations are required during training, greatly alleviating the annotation
    burdens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table II: Highlights of parts relationship modeling methods for SHP models
    (§[3.1](#S3.SS1 "3.1 Single Human Parsing (SHP) Models ‣ 3 Deep Learning Based
    Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")).
    Representative Works of each method are also give.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Representative &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Works &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Highlights |'
  prefs: []
  type: TYPE_TB
- en: '| Attention |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[33](#bib.bib33), [34](#bib.bib34), [113](#bib.bib113)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[115](#bib.bib115), [36](#bib.bib36), [121](#bib.bib121)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[131](#bib.bib131)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; It is helpful to locate interested human parts, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; suppress useless background information. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scale-aware |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[37](#bib.bib37), [38](#bib.bib38), [119](#bib.bib119)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[39](#bib.bib39)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fusion low-level texture and high-level semantic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; features, help to parse small human parts. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tree |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[14](#bib.bib14), [17](#bib.bib17), [22](#bib.bib22)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[118](#bib.bib118), [3](#bib.bib3), [41](#bib.bib41)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Simulate the composition and decomposition &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; relationship between human parts and body. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Graph |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[114](#bib.bib114), [42](#bib.bib42), [123](#bib.bib123)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[4](#bib.bib4), [130](#bib.bib130), [43](#bib.bib43)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Modeling the correlation and difference between &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; human parts. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Edge |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[44](#bib.bib44), [45](#bib.bib45), [129](#bib.bib129)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[120](#bib.bib120), [46](#bib.bib46)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Solve the pixel confusion problem on the boundary &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of adjacent parts, generating finer boundary. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Pose |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[1](#bib.bib1), [20](#bib.bib20), [25](#bib.bib25)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[111](#bib.bib111), [116](#bib.bib116), [2](#bib.bib2)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[47](#bib.bib47), [128](#bib.bib128), [48](#bib.bib48)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; As context clues to improve semantic consistency &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; between parsing results and body structure. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Denoising |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[51](#bib.bib51), [52](#bib.bib52), [125](#bib.bib125)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Alleviate the impact of super-pixel or annotation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; errors, improving the robustness. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Adversarial |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[49](#bib.bib49), [50](#bib.bib50)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reduce the domain differences between training &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; data and testing data, improving the generalization. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4 Other Modeling Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Other works attempt to employ techniques outside of the above taxonomy, such
    as denoising and adversarial learning, which also make specific contributions
    to the human parts relationship modeling and deserve a separate look.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Denoising. To reduce the labeling cost, there is a large amount of
    noise in the mainstream SHP datasets [[22](#bib.bib22), [116](#bib.bib116)], so
    denoising learning for accurate human parts relationship modeling has also received
    some attention. SCHP [[52](#bib.bib52)] is the most representative work. It starts
    with using inaccurate parsing labels as the initialization and designs a cyclically
    learning scheduler to infer more reliable pseudo labels In the same period, Li
    *et al*. [[51](#bib.bib51)] attempt to combine denoising learning and semi-supervised
    learning, proposing Self-Learning with Rectification (SLR) strategy for human
    parsing. SLR generates pseudo labels for unlabeled data to retrain the parsing
    model and introduces a trainable graph reasoning method to correct typical errors
    in pseudo labels. Based on SLR, HIPN [[125](#bib.bib125)] further explores to
    combine denoising learning with semi-supervised learning, which develops the noise-tolerant
    hybrid learning, taking advantage of positive and negative learning to better
    handle noisy pseudo labels.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Adversarial Learning. Earlier, inspired by the Generative Adversarial
    Nets (GAN) [[96](#bib.bib96)], a few works use adversarial learning to solve problems
    in parts relationship modeling. For example, to solve the domain adaptation problem,
    AFLA [[49](#bib.bib49)] proposes a cross-domain human parsing network, introducing
    a discriminative feature adversarial network and a structured label adversarial
    network to eliminate cross-domain differences in visual appearance and environment
    conditions. MMAN [[50](#bib.bib50)] hopes to solve the problem of low-level local
    and high-level semantic inconsistency in pixel-wise classification loss. It contains
    two discriminators: Macro D, acting on low-resolution label map and penalizing
    semantic inconsistency; Micro D, focusing on high-resolution label map and restraining
    local inconsistency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remark. In fact, many single human parsing models use a variety of parts relationship
    modeling methods. Therefore, our above taxonomy only introduces the core methods
    of each model. Table [II](#S3.T2 "Table II ‣ 3.1.3 Multi-task Learning ‣ 3.1 Single
    Human Parsing (SHP) Models ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning
    Technique for Human Parsing: A Survey and Outlook") summarizes the highlights
    of each parts relationship modeling method.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Multiple Human Parsing (MHP) Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MHP seeks to locate and parse each human in the image plane. The task setting
    is similar to instance segmentation, so it is also called instance-level human
    parsing. We divide MHP into three paradigms: bottom-up, one-stage top-down, and
    two-stage top-down, according to its pipeline of discriminating human instances.
    The essential characteristics of reviewed MHP models are illustrated in Table [III](#S3.T3
    "Table III ‣ 3.2 Multiple Human Parsing (MHP) Models ‣ 3 Deep Learning Based Human
    Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table III: Summary of essential characteristics for reviewed MHP models (§[3.2](#S3.SS2
    "3.2 Multiple Human Parsing (MHP) Models ‣ 3 Deep Learning Based Human Parsing
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")). “BU” indicates
    bottom-up; “1S-TD” indicates one-stage top-down; “2S-TD” indicates two-stage top-down.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. | Pipeline | Datasets | Open Source |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Holistic [[138](#bib.bib138)] | BMVC | 1S-TD | PPP | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | PGN [[8](#bib.bib8)] | ECCV | BU | PPP/CIHP | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | 2S-TD | CIHP/MHP-v2.0 | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | CIHP/MHP-v2.0 | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| BraidNet [[57](#bib.bib57)] | MM | 2S-TD | CIHP | - |'
  prefs: []
  type: TYPE_TB
- en: '| Unified [[139](#bib.bib139)] | BMVC | 1S-TD | PPP/CIHP | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | CIHP/MHP-v2.0 | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| SemaTree [[41](#bib.bib41)] | ECCV | 2S-TD | CIHP/MHP-v2.0 | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| NAN [[141](#bib.bib141)] | IJCV | BU | MHP-v1.0/MHP-v2.0 | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | 2S-TD | CIHP/MHP-v2.0/VIP | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | MGHR [[59](#bib.bib59)] | CVPR | BU |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PPP/MHP-v2.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; /COCO-DP &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | AIParsing [[142](#bib.bib142)] | TIP | 1S-TD | CIHP/MHP-v2.0/VIP |
    - |'
  prefs: []
  type: TYPE_TB
- en: $\bullet$ Bottom-up. Bottom-up paradigm regards multiple human parsing as a
    fine-grained semantic segmentation task, which predicts the category of each pixel
    and grouping them into corresponding human instance. In a seminal work [[8](#bib.bib8)],
    Gong *et al*. propose a detection-free Part Grouping Network (PGN) that reformulates
    multiple human parsing as two twinned sub-tasks (semantic part segmentation and
    instance-aware edge detection) that can be jointly learned and mutually refined
    via a unified network. Among them, instance-aware edge detection task can group
    semantic parts into distinct human instances. Then, NAN [[141](#bib.bib141)] proposes
    a deep Nested Adversarial Network for multiple human parsing. NAN consists of
    three GAN-like sub-nets, performing semantic saliency prediction, instance-agnostic
    parsing, and instance-aware clustering, respectively. Recently, Zhou *et al*.
    [[59](#bib.bib59)] propose a new bottom-up regime to learn category-level multiple
    human parsing as well as pose estimation in a joint and end-to-end manner, called
    Multi-Granularity Human Representation (MGHR) learning. MGHR exploits structural
    information over different human granularities, transforming the difficult pixel
    grouping problem into an easier multi human joint assembling task to simplify
    the difficulty of human instances discrimination.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ One-stage Top-down. One-stage top-down is the mainstream paradigm
    of multiple human parsing. It first locates each human instance in the image plane,
    then segments each human part in an end-to-end manner. An early attempt is Holistic
    [[138](#bib.bib138)], which consists of a human detection network and a part semantic
    segmentation network, then passing the results of both networks to an instance
    CRF [[143](#bib.bib143)] to perform multiple human parsing. Inspired by Mask R-CNN
    [[144](#bib.bib144)], Qin *et al*. [[139](#bib.bib139)] propose a top-down unified
    framework that simultaneously performs human detection and single human parsing,
    identifying instances and parsing human parts in crowded scenes. A milestone one-stage
    top-down multiple human parsing model is proposed by Yang *et al*., that enhances
    Mask R-CNN in all aspects, and proposes Parsing R-CNN [[61](#bib.bib61)] network,
    greatly improving the accuracy of multiple human parsing concisely. Subsequently,
    Yang *et al*. propose an improved version of Parsing R-CNN, called RP R-CNN [[140](#bib.bib140)],
    which introduces a global semantic enhanced feature pyramid network and a parsing
    re-scoring network into the high-performance pipeline, achieving better performance.
    Later, AIParsing [[142](#bib.bib142)] introduces the anchor-free detector [[145](#bib.bib145)]
    into the one-stage top-down paradigm for discriminating human instances, avoiding
    the hyper-parameters sensitivity caused by anchors.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Two-stage Top-down. One-stage top-down and two-stage top-down paradigms
    are basically the same in operation flow. The difference between them is whether
    the detector is trained together with the segmentation sub-network in an end-to-end
    manner. All the two-stage bottom-up multiple human parsing methods consist of
    a human detector and a single human parser. The earliest attempt is CE2P [[44](#bib.bib44)],
    which designs a framework called M-CE2P on CE2P and Mask R-CNN, cropping the detected
    human instances, then sending them to the single human parser, finally combining
    the parsing results of all instances into a multiple human parsing prediction.
    Subsequent works, *e.g*., BraidNet [[57](#bib.bib57)], SemaTree [[41](#bib.bib41)],
    and SCHP [[52](#bib.bib52)], basically inherit this pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remark. The advantage of bottom-up and one-stage top-down is efficiency, and
    the advantage of two-stage top-down is accuracy. But as a non-end-to-end pipeline,
    the inference speed of two-stage top-down is positively correlated with the number
    of human instances, which also limits its practical application value. The detailed
    highlights of three human instances discrimination methods are summarized in Table [IV](#S3.T4
    "Table IV ‣ 3.2 Multiple Human Parsing (MHP) Models ‣ 3 Deep Learning Based Human
    Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table IV: Highlights of human instances discrimination methods for MHP models
    (§[3.2](#S3.SS2 "3.2 Multiple Human Parsing (MHP) Models ‣ 3 Deep Learning Based
    Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")).
    Representative Works of each method are also give.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Representative &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Works &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Highlights |'
  prefs: []
  type: TYPE_TB
- en: '| Bottom-up |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[8](#bib.bib8), [141](#bib.bib141), [59](#bib.bib59)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Good model efficiency, good accuracy on pixel-wise &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; segmentation, and poor accuracy on instances &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; discrimination. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; One-stage &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Top-down &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[138](#bib.bib138), [61](#bib.bib61), [139](#bib.bib139)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[140](#bib.bib140), [142](#bib.bib142)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Better trade-off between model efficiency and accuracy. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; But pixel-wise segmentation, especially the part &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; boundary is not fine enough. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Two-stage &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Top-down &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[44](#bib.bib44), [57](#bib.bib57), [41](#bib.bib41)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[52](#bib.bib52)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Good accuracy and poor efficiency, the model inference &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; time is proportional to human instances number. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Video Human Parsing (VHP) Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Existing VHP studies mainly focus to propagate the first frame into the entire
    video by the affinity matrix, which represents the temporal correspondences learnt
    from raw video data. Considering the unsupervised learning paradigms, we can group
    them into three classes: cycle-tracking, reconstructive learning, and contrastive
    learning. We summarize the essential characteristics of reviewed VHP models in
    Table [V](#S3.T5 "Table V ‣ 3.3 Video Human Parsing (VHP) Models ‣ 3 Deep Learning
    Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and
    Outlook").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table V: Summary of essential characteristics for reviewed VHP models (§[3.3](#S3.SS3
    "3.3 Video Human Parsing (VHP) Models ‣ 3 Deep Learning Based Human Parsing ‣
    Deep Learning Technique for Human Parsing: A Survey and Outlook")). “Cycle.” indicates
    cycle-tracking; “Recons.” indicates reconstructive learning; “Contra.” indicates
    contrastive learning. All models are test on the VIP dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Cycle. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Recons. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Contra. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Open Source |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | ATEN [[9](#bib.bib9)] | MM | ✓ | - | - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | TimeCycle [[146](#bib.bib146)] | CVPR | ✓ | - | - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| UVC [[147](#bib.bib147)] | NeurIPS | ✓ | ✓ | - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CRW [[148](#bib.bib148)] | NeurIPS | ✓ | - | - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | ContrastCorr [[149](#bib.bib149)] | AAAI | ✓ | ✓ |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| CLTC [[150](#bib.bib150)] | CVPR | - | - | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| VFS [[151](#bib.bib151)] | ICCV | - | - | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| JSTG [[152](#bib.bib152)] | ICCV | ✓ | - | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | LIIR [[153](#bib.bib153)] | CVPR | - | ✓ | - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| SCC [[154](#bib.bib154)] | CVPR | ✓ | - | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| UVC+ [[155](#bib.bib155)] | ArXiv | ✓ | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: $\bullet$ Cycle-tracking. Early VHP methods model the unsupervised learning
    target mainly by the cycle-consistency of video frames, *i.e.*, pixels/patches
    are expected to fall into the same locations after a cycle of forward-backward
    tracking. ATEN[[9](#bib.bib9)] first leverages convolutional gated recurrent units
    to encode temporal feature-level changes, optical flow of non-key frames is wrapped
    with the temporal memory to generate their features. TimeCycle [[146](#bib.bib146)]
    tracks the reference patch backward-forward in the video. The reference and the
    tracked patch at the end of the tracking cycle are considered to be consistent
    both in spatial coordinates and feature representation. Meanwhile, UVC [[147](#bib.bib147)]
    performs the region-level tracking and pixel-level corresponding with a shared
    affinity matrix, the tracked patch feature and the region-corresponding sub-affinity
    matrix are used to reconstruct the reference patch. Roles of the target and reference
    patches are then switched to regularizing the affinity matrix as orthogonal, which
    satisfies the cycle-consistency constraint. Its later version, UVC+ [[155](#bib.bib155)]
    combines features learned by image-based tasks with video-based counterparts to
    further boost the performance. Lately, CRW [[148](#bib.bib148)] represents video
    as a graph, where nodes are patches and edges are affinities between nodes in
    adjacent frames. A cross-entropy loss guides a graph walk to track the initial
    node bi-directionally in feature space, which is considered the target node after
    a bunch of cycle paths. However, the cycle-consistency in [[146](#bib.bib146)],
    [[148](#bib.bib148)] strictly assumes that the target patch preserves visible
    in consecutive frames. Once it is occluded or disappears, the correspondences
    will be incorrectly assigned, thus leaving an optimal transport problem between
    video frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table VI: Highlights of temporal correspondences learning methods for VHP models
    (§[3.3](#S3.SS3 "3.3 Video Human Parsing (VHP) Models ‣ 3 Deep Learning Based
    Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")).
    Representative Works of each method are also give.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Method |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Representative &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Works &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Highlights |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cycle- &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tracking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[146](#bib.bib146), [147](#bib.bib147)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[155](#bib.bib155), [148](#bib.bib148)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Capturing temporal variations, may produce &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; wrong correspondences when occlusion occurs. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reconstructive &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[149](#bib.bib149), [153](#bib.bib153)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Modelling fine-grained temporal correspondence &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and guiding focus on part details. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Contrastive &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[150](#bib.bib150), [151](#bib.bib151)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[152](#bib.bib152), [154](#bib.bib154)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Search for discriminative features to segment &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; similar or position-transformed human instances. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/5f83f893f98e371117932897845b12e0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Correlations of different SHP, MHP and VHP methods (§[3.4](#S3.SS4
    "3.4 Summary ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for
    Human Parsing: A Survey and Outlook")). We use the connections between the arc
    edges to summary the correlation between human parsing methods, each connecting
    line stands for a study that uses both methods. The longer the arc, the more methods
    of this kind, same for the width of connecting lines. This correlation summary
    reveals the prevalence of various human parsing methods.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Reconstructive Learning. As video contents smoothly shift in time,
    pixels in a “query” frame can be considered as copies from a set of pixels in
    other reference frames [[156](#bib.bib156), [157](#bib.bib157)]. Following UVC
    [[147](#bib.bib147)] to establish pixel-level correspondence, several methods
    [[149](#bib.bib149), [153](#bib.bib153)] are proposed to learn temporal correspondence
    completely by reconstructing correlating frames. Subsequently, ContrastCorr [[149](#bib.bib149)]
    not only learns from intra-video self-supervision, but also steps further to introduce
    inter-video transformation as negative correspondence. The inter-video distinction
    enforces the feature extractor to learn discriminations between videos while preserving
    the fine-grained matching characteristic among intra-video frame pairs. Based
    on the intra-inter video correlation, LIIR [[153](#bib.bib153)] introduces a locality-aware
    reconstruction framework, which encodes position information and involves spatial
    compactness into intra-video correspondence learning, for locality-aware and efficient
    visual tracking.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/47c61a8ddac735faf39c0c0099b6c779.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Correlations of different SHP, MHP and VHP studies (§[3.4](#S3.SS4
    "3.4 Summary ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for
    Human Parsing: A Survey and Outlook")). We list out all the involved human parsing
    studies by dots and use connecting lines to represent their citing relations.
    The citing relation here refers to the citation appears in experimental comparisons,
    to avoid citations of low correlation in background introduction. As each line
    represents a citation between two studies, so the larger the dot, the more times
    cited. These correlations highlight the relatively prominent studies.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Contrastive Learning. Following the idea of pulling positive pairs
    close together and pushing negative pairs away from each other, considerable VHP
    algorithms adopt contrastive learning as a training objective. To solve the optimal
    transport problem, CLTC [[150](#bib.bib150)] proposes to mine positive and semi-hard
    negative correspondences via consistency estimation and dynamic hardness discrimination,
    respectively. The well-defined positive and negative pixel pairs prevent side-effects
    from non-consistent positive and too hard/easy negative samples to contrastive
    learning. Specifically, unlike most methods that perform patch-level contrastive
    learning, VFS [[151](#bib.bib151)] learns visual correspondences at frame level.
    Following data augmentation of image-level contrastive learning [[158](#bib.bib158)]
    and a well-designed temporal sampling strategy, VFS encourages convolutional features
    to find correspondences between similar objects and parts. Lately, [[152](#bib.bib152),
    [154](#bib.bib154)] extend the video graph with space relations of neighbor nodes,
    which determine the aggregation strength from intra-frame neighbors. The proposed
    space-time graph draws more attention to the association of center-neighbor pairs,
    thus explicitly helping learning correspondence between part instances. SCC [[154](#bib.bib154)]
    mixes sequential Bayesian filters to formulate the optimal paths that track nodes
    from one frame to others, to alleviate the correspondence missing caused by random
    occlusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remark. To our investigation scope, the current VHP research essentially follows
    an unsupervised semi-automatic video object segmentation setup. But considering
    the potential demand, it is more expectant to fully utilize the annotations and
    solve the VHP problem through an instance-discriminative manner, *i.e*., a fine-grained
    video instance segmentation task. The highlights of temporal correspondences learning
    methods for VHP are shown in Table [VI](#S3.T6 "Table VI ‣ 3.3 Video Human Parsing
    (VHP) Models ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for
    Human Parsing: A Survey and Outlook").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Through the detailed review, we have subdivided SHP, MHP, and VHP studies into
    multiple methods and discussed their characteristics. To further investigate the
    development picture of the human parsing community, we summarize the correlations
    of the methods in Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Video Human Parsing (VHP)
    Models ‣ 3 Deep Learning Based Human Parsing ‣ Deep Learning Technique for Human
    Parsing: A Survey and Outlook") and correlations of the involved studies in Figure [5](#S3.F5
    "Figure 5 ‣ 3.3 Video Human Parsing (VHP) Models ‣ 3 Deep Learning Based Human
    Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S3.F4 "Figure 4 ‣ 3.3 Video Human Parsing (VHP) Models ‣ 3 Deep
    Learning Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook") presents correlations between research methods, *i.e*., two methods
    are connected if a study uses both as its technical components, making the length
    of arcs represent the number of studies using them. The connecting line distribution
    first obviously shows that Graph (Structure), Attention (Mechanism), and Edge(-aware
    Learning) of SHP are more correlated with multiple other methods, which indicates
    their compatibility with others and prevalence in the community. It is worth noting
    that though Tree (Structure) has many correlations with others, a large proportion
    of them are with Graph method. This phenomenon indicates that Tree method is much
    less generalizable compared to Graph, Attention, and Edge methods. Regrettably,
    negligible relations between VHP and other methods show that current VHP studies
    have not yet gone deep into parts relationship modeling or human instance discrimination.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The correlations of human parsing studies are presented in form of citing relations
    as Figure [5](#S3.F5 "Figure 5 ‣ 3.3 Video Human Parsing (VHP) Models ‣ 3 Deep
    Learning Based Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook"), each line represents a citation between two studies. For reliable
    statistics, we only consider citations that appear in experimental comparisons
    for all studies. From the citing relations, we can easily observe that Attention
    [[33](#bib.bib33)], JPPNet [[2](#bib.bib2)], CE2P [[44](#bib.bib44)], CNIF [[3](#bib.bib3)]
    and PGN [[8](#bib.bib8)] have the largest dots, *i.e*., they are experimental
    compared by most other studies, this indicates they are recognized as baseline
    studies of great prominence by the community. Additionally, since CE2P proposed
    to handle MHP sub-task by 2S-TD pipeline and make a milestone, lots of SHP studies
    start to compare their algorithms with MHP studies, this trend breaks down the
    barriers between the two sub-tasks of human parsing. Lastly, similar to the method
    correlation, VHP studies form citations strictly along with the proposed order
    among their own, which once again shows that VHP studies have not focused on human-centric
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Synthesizing detailed review and correlation analysis, we can draw some conclusions
    about the historical evolution of human parsing models. First, the research focus
    has gradually shifted from SHP to MHP and VHP. As more challenging tasks, the
    latter two also have greater application potential. With the emergence of high-quality
    annotated datasets and the improvement of computing power, they have received
    increasing attention. Secondly, the technical diversity is insufficient, and the
    achievements of representation learning in recent years have not fully benefited
    the human parsing field. Finally, the number of open source work has increased
    significantly, but still insufficient. It is hoped that subsequent researchers
    will open source code and models as much as possible to benefit the follow-up
    researchers.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Human Parsing Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the past decades, a variety of visual datasets have been released for human
    parsing (upper part of Figure [3](#S2.F3 "Figure 3 ‣ 2.3 Relevant Tasks ‣ 2 Preliminaries
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")). We summarize
    the classical and commonly used datasets in Table [VII](#S4.T7 "Table VII ‣ 4
    Human Parsing Datasets ‣ Deep Learning Technique for Human Parsing: A Survey and
    Outlook"), and give a detailed review from multiple angles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table VII: Statistics of existing human parsing datasets. See §[4.1](#S4.SS1
    "4.1 Single Human Parsing (SHP) Datasets ‣ 4 Human Parsing Datasets ‣ Deep Learning
    Technique for Human Parsing: A Survey and Outlook") - §[4.3](#S4.SS3 "4.3 Video
    Human Parsing (VHP) Datasets ‣ 4 Human Parsing Datasets ‣ Deep Learning Technique
    for Human Parsing: A Survey and Outlook") for more detailed descriptions. The
    14 datasets are divided into 3 groups according to the human parsing taxonomy.
    “Instance” indicates that instance-level human labels are provided; “Temporal”
    indicates that video-level labels are provided; “Super-pixel” indicates that super-pixels
    are used for labeling.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Dataset | Year | Pub. | #Images | #Train/Val/Test/ | #Class | Purpose |
    Instance | Temporal | Super-pixel | Other Annotations |'
  prefs: []
  type: TYPE_TB
- en: '| Fashionista [[1](#bib.bib1)] | 2012 | CVPR | 685 | 456/-/299 | 56 | Clothing
    | - | - | ✓ | Clothing-tag |'
  prefs: []
  type: TYPE_TB
- en: '| CFPD [[25](#bib.bib25)] | 2013 | TMM | 2,682 | 1,341/-/1,341 | 23 | Clothing
    | - | - | ✓ | Color-seg. |'
  prefs: []
  type: TYPE_TB
- en: '| DailyPhotos [[14](#bib.bib14)] | 2013 | ICCV | 2,500 | 2,500/-/- | 19 | Clothing
    | - | - | ✓ | Clothing-tag |'
  prefs: []
  type: TYPE_TB
- en: '| PPSS [[159](#bib.bib159)] | 2013 | ICCV | 3,673 | 1,781/-/1,892 | 6 | Human
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ATR [[22](#bib.bib22)] | 2015 | TPAMI | 7,700 | 6,000/700/1,000 | 18 | Human
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Chictopia10k [[37](#bib.bib37)] | 2015 | ICCV | 10,000 | 10,000/-/- | 18
    | Clothing | - | - | - | Clothing-tag |'
  prefs: []
  type: TYPE_TB
- en: '| SYSU-Clothes [[114](#bib.bib114)] | 2016 | TMM | 2,682 | 2,682/-/- | 57 |
    Clothing | - | - | ✓ | Clothing-tag |'
  prefs: []
  type: TYPE_TB
- en: '| LIP [[116](#bib.bib116)] | 2017 | CVPR | 50,462 | 30,462/10,000/10,000 |
    20 | Human | - | - | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| HRHP [[7](#bib.bib7)] | 2021 | CVPRW | 7,500 | 6,000/500/1,000 | 20 | Human
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PASCAL-Person-Part [[160](#bib.bib160)] | 2014 | CVPR | 3,533 | 1,716/-/1,817
    | 7 | Human | ✓ | - | - | Human-box |'
  prefs: []
  type: TYPE_TB
- en: '| MHP-v1.0 [[161](#bib.bib161)] | 2017 | ArXiv | 4,980 | 3,000/1,000/980 |
    19 | Human | ✓ | - | - | Human-box |'
  prefs: []
  type: TYPE_TB
- en: '| MHP-v2.0 [[162](#bib.bib162)] | 2018 | MM | 25,403 | 15,403/5,000/5,000 |
    59 | Human | ✓ | - | - | Human-box |'
  prefs: []
  type: TYPE_TB
- en: '| COCO-DensePose [[74](#bib.bib74)] | 2018 | CVPR | 27,659 | 26,151/-/1,508
    | 15 | Human | ✓ | - | - |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Human-box/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; keypoints/densepoints &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CIHP [[8](#bib.bib8)] | 2018 | ECCV | 38,280 | 28,280/5,000/5,000 | 20 |
    Human | ✓ | - | - | Human-box |'
  prefs: []
  type: TYPE_TB
- en: '| VIP [[9](#bib.bib9)] | 2018 | MM | 21,246 | 18,468/-/2,778 | 20 | Human |
    ✓ | ✓ | - | Human-box/identity |'
  prefs: []
  type: TYPE_TB
- en: 4.1 Single Human Parsing (SHP) Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ Fashionista (FS) [[1](#bib.bib1)] consists of 685 photographs collected
    from Chictopia.com, a social networking website for fashion bloggers. There are
    456 training images and 299 testing images annotated with 56-class semantic labels,
    and text tags of garment items and styling are also provided. Fashionista was
    once the main single human/clothing parsing dataset but was limited by its scale.
    It is rarely used now.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Colorful Fashion Parsing Data (CFPD) [[25](#bib.bib25)] is also collected
    from Chictopia.com, which provides 23-class noisy semantic labels and 13-class
    color labels. The annotated images are usually grouped into 1,341/1,341 for train/test.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ DailyPhotos (DP) [[14](#bib.bib14)] contains 2,500 high resolution
    images, which are crawled following the same strategy as the Fashionista dataset
    and thoroughly annotated with 19 categories.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ PPSS [[159](#bib.bib159)] includes 3,673 annotated samples collected
    from 171 videos of different surveillance scenes and provides pixel-wise annotations
    for hair, face, upper-/lower-clothes, arm, and leg. It presents diverse real-word
    challenges, *e.g*. pose variations, illumination changes, and occlusions. There
    are 1,781 and 1,892 images for training and testing, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ ATR [[22](#bib.bib22)] contains data which combined from three small
    benchmark datasets: the Fashionista [[1](#bib.bib1)] containing 685 images, the
    CFPD [[25](#bib.bib25)] containing 2,682 images, and the DailyPhotos [[14](#bib.bib14)]
    containing 2,500 images. The labels are merged of Fashionista and CFPD datasets
    to 18 categories. To enlarge the diversity, another 1,833 challenging images are
    collected and annotated to construct the Human Parsing in the Wild (HPW) dataset.
    The final combined dataset contains 7,700 images, which consists of 6,000 images
    for training, 1,000 for testing, and 700 as the validation set.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Chictopia10k [[37](#bib.bib37)] contains 10,000 real-world human pictures
    from Chictopia.com, annotating pixel-wise labels following [[22](#bib.bib22)].
    The dataset mainly contains images in the wild (*e.g*., more challenging poses,
    occlusion, and clothes).
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ SYSU-Clothes [[114](#bib.bib114)] consists of 2,098 high resolution
    fashion photos in high-resolution (about 800$\times$500 on average) from the shopping
    website. In this dataset, six categories of clothing attributes (*e.g*., clothing
    category, clothing color, clothing length, clothing shape, collar shape, and sleeve
    length) and 124 attribute types of all categories are collected.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Look into Person (LIP) [[116](#bib.bib116)] is the most popular single
    human parsing dataset, which is annotated with pixel-wise annotations with 19
    semantic human part labels and one background label. LIP contains 50,462 annotated
    images and be grouped into 30,462/10,000/10,000 for train/val/test. The images
    in the LIP dataset are cropped person instances from COCO [[163](#bib.bib163)]
    training and validation sets.
  prefs: []
  type: TYPE_NORMAL
- en: Remark. ATR and LIP are the mainstream benchmarks among these single human parsing
    datasets. In recent years, the research purpose has changed from “clothing” to
    “human”, and the data scale and annotation quality have also been significantly
    improved.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Multiple Human Parsing (MHP) Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ PASCAL-Person-Part (PPP) [[160](#bib.bib160)] is annotated from the
    PASCAL-VOC-2010 [[164](#bib.bib164)], which contains 3,533 multi-person images
    with challenging poses and splits into 1,716 training images and 1,817 test images.
    Each image is pixel-wise annotated with 7 classes, namely head, torso, upper/lower
    arms, upper/lower legs, and a background category.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ MHP-v1.0 [[161](#bib.bib161)] contains 4,980 multi-person images with
    fine-grained annotations at pixel-level. For each person, it defines 7 body parts,
    11 clothing/accessory categories, and one background label. The train/val/test
    sets contain 3,000/1,000/980 images, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ MHP-v2.0 [[162](#bib.bib162)] is an extend version of MHP-v1.0 [[161](#bib.bib161)],
    which provides more images and richer categories. MHP-v2.0 contains 25,403 images
    and has great diversity in image resolution (from 85$\times$100 to 4,511$\times$6,919)
    and human instance number (from 2 to 26 persons). These images are split into
    15,403/5,000/5,000 for train/val/test with 59 categories.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ COCO-DensePose (COCO-DP) [[74](#bib.bib74)] aims at establishing the
    mapping between all human pixels of an RGB image and the 3D surface of the human
    body, and has 27,659 images (26,151/1,508 for train/test splits) gathered from
    COCO [[163](#bib.bib163)]. The dataset provides 15 pixel-wise human parts with
    dense keypoints annotations.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Crowd Instance-level Human Parsing (CIHP) [[8](#bib.bib8)] is the
    largest multiple human parsing dataset to date. With 38,280 diverse real-world
    images, the persons are labelled with pixel-wise annotations on 20 categories.
    It consists of 28,280 training and 5,000 validation images with publicly available
    annotations, as well as 5,000 test images with annotations withheld for benchmarking
    purposes. All images of the CIHP dataset contain two or more instances with an
    average of 3.4.
  prefs: []
  type: TYPE_NORMAL
- en: Remark. So far, several multiple human parsing datasets have high-quality annotation
    and considerable data scale. In addition to pixel-wise parsing annotations, many
    datasets provide other rich annotations, such as box, keypoints/landmark and style.
    PPP, CIHP and MHP-v2.0 are widely studied datasets, and most classical multiple
    human parsing methods have been verified on them.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Video Human Parsing (VHP) Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ Video Instance-level Parsing (VIP) [[9](#bib.bib9)] is the first video
    human parsing dataset. VIP contains 404 multi-person Full HD sequences, which
    are collected from Youtube with great diversity. For every 25 consecutive frames
    in each sequence, one frame is densely annotated with 20 classes and identities.
    All the sequences are grouped into 354/50 for train/test, containing 18,468/2,778
    annotated frames respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Remark. Since video human parsing has only attracted attention in recent years,
    there are few publicly available datasets, and its data scale and richness still
    need to be continuously invested by the community.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Through Table [VII](#S4.T7 "Table VII ‣ 4 Human Parsing Datasets ‣ Deep Learning
    Technique for Human Parsing: A Survey and Outlook"), we can observe that the human
    parsing datasets show several development trends. Firstly, the scale of datasets
    continues to increase, from hundreds in the early years [[1](#bib.bib1)] to a
    tens of thousands now [[116](#bib.bib116), [8](#bib.bib8)]. Secondly, the quality
    of annotation is constantly improving. Some early datasets use super-pixel [[1](#bib.bib1),
    [114](#bib.bib114), [116](#bib.bib116)] to reduce the annotation cost, while in
    recent years, pixel-wise accurate annotation has been adopted. Finally, the annotation
    dimensions are becoming increasingly diverse, *e.g*., COCO-DensePose [[74](#bib.bib74)]
    provides boxes, keypoints, and UVs annotation in addition to parsing.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Performance Comparisons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To provide a more intuitive comparison, we tabulate the performance of several
    previously discussed models. It should be noted that the experimental settings
    of each study are not entirely consistent (*e.g*., backbone, input size, training
    epochs). Therefore, we suggest only taking these comparisons as references, and
    a more specific analysis needs to study the original articles deeply.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 SHP Performance Benchmarking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We select ATR [[22](#bib.bib22)] and LIP [[116](#bib.bib116)] as the benchmark
    for single human parsing performance comparison, and compared 14 and 26 models,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Evaluation Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The evaluation metrics of single human parsing are basically consistent with
    semantic segmentation [[31](#bib.bib31)], including pixel accuracy, mean pixel
    accuracy, and mean IoU. In addition, foreground pixel accuracy and F-1 score are
    also commonly used metrics on the ATR dataset.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Pixel accuracy (pixAcc) is the simplest and intuitive metric, which
    expresses the proportion of pixels with correct prediction in the overall pixel.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Foreground pixel accuracy (FGAcc) only calculates the pixel accuracy
    of foreground human parts.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Mean pixel accuracy (meanAcc) is a simple improvement of pixel accuracy,
    which calculates the proportion of correctly predicted pixels in each category.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Mean IoU (mIoU) is short for mean intersection over union, which calculates
    the ratio of the intersection and union of two sets. The two sets are the ground-truth
    and predicted results of each category respectively.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ F-1 score (F-1) is the harmonic average of precision and recall, which
    is a common evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table VIII: Quantitative SHP results on ATR test (§[5.1](#S5.SS1 "5.1 SHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")) in terms of pixel accuracy (pixAcc), foreground pixel
    accuracy (FGAcc) and F-1 score (F-1). The three best scores are marked in red,
    blue, and green, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. | Backbone | #Input Size | #Epoch | pixAcc | FGAcc
    | F-1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2012 | Yamaguchi [[1](#bib.bib1)] | CVPR | - | - | - | 84.38 | 55.59 | 41.80
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2013 | Paperdoll [[20](#bib.bib20)] | ICCV | - | - | - | 88.96 | 62.18 |
    44.76 |'
  prefs: []
  type: TYPE_TB
- en: '| 2015 | M-CNN [[110](#bib.bib110)] | CVPR | - | - | 50 | 89.57 | 73.98 | 62.81
    |'
  prefs: []
  type: TYPE_TB
- en: '| Co-CNN [[37](#bib.bib37)] | ICCV | - | 150$\times$100 | 90 | 95.23 | 80.90
    | 76.95 |'
  prefs: []
  type: TYPE_TB
- en: '| ATR [[22](#bib.bib22)] | TPAMI | - | 227$\times$227 | 120 | 91.11 | 71.04
    | 64.38 |'
  prefs: []
  type: TYPE_TB
- en: '| 2016 | LG-LSTM [[34](#bib.bib34)] | CVPR | VGG16 | 321$\times$321 | 60 |
    96.18 | 84.79 | 80.97 |'
  prefs: []
  type: TYPE_TB
- en: '| Graph-LSTM [[113](#bib.bib113)] | ECCV | VGG16 | 321$\times$321 | 60 | 97.60
    | 91.42 | 83.76 |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Struc-LSTM [[115](#bib.bib115)] | CVPR | VGG16 | 321$\times$321 |
    60 | 97.71 | 91.76 | 87.88 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | TGPNet [[119](#bib.bib119)] | MM | VGG16 | 321$\times$321 | 35 | 96.45
    | 87.91 | 81.76 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CNIF [[3](#bib.bib3)] | ICCV | ResNet101 | 473$\times$473 | 150 |
    96.26 | 87.91 | 85.51 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CorrPM [[45](#bib.bib45)] | CVPR | ResNet101 | 384$\times$384 | 150
    | 97.12 | 90.40 | 86.12 |'
  prefs: []
  type: TYPE_TB
- en: '| HHP [[4](#bib.bib4)] | CVPR | ResNet101 | 473$\times$473 | 150 | 96.84 |
    89.23 | 87.25 |'
  prefs: []
  type: TYPE_TB
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | ResNet101 | 473$\times$473 | 150 | 96.25
    | 87.97 | 85.55 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CDGNet [[131](#bib.bib131)] | CVPR | ResNet101 | 512$\times$512 |
    250 | 97.39 | 90.19 | 87.16 |'
  prefs: []
  type: TYPE_TB
- en: 'Table IX: Quantitative SHP results on LIP val (§[5.1](#S5.SS1 "5.1 SHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")) in terms of pixel accuracy (pixAcc), mean pixel accuracy
    (meanAcc) and mean IoU (mIoU). The three best scores are marked in red, blue,
    and green, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. | Backbone | #Input Size | #Epoch | pixAcc | meanAcc
    | mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | SSL [[116](#bib.bib116)] | CVPR | VGG16 | 321$\times$321 | 50 | -
    | - | 46.19 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | HSP-PRI [[76](#bib.bib76)] | CVPR | InceptionV3 | - | - | 85.07 |
    60.54 | 48.16 |'
  prefs: []
  type: TYPE_TB
- en: '| MMAN [[50](#bib.bib50)] | ECCV | ResNet101 | 256$\times$256 | 30 | 85.24
    | 57.60 | 46.93 |'
  prefs: []
  type: TYPE_TB
- en: '| MuLA [[47](#bib.bib47)] | ECCV | Hourglass | 256$\times$256 | 250 | 88.50
    | 60.50 | 49.30 |'
  prefs: []
  type: TYPE_TB
- en: '| JPPNet [[2](#bib.bib2)] | TPAMI | ResNet101 | 384$\times$384 | 60 | 86.39
    | 62.32 | 51.37 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | ResNet101 | 473$\times$473 | 150
    | 87.37 | 63.20 | 53.10 |'
  prefs: []
  type: TYPE_TB
- en: '| CNIF [[3](#bib.bib3)] | ICCV | ResNet101 | 473$\times$473 | 150 | 88.03 |
    68.80 | 57.74 |'
  prefs: []
  type: TYPE_TB
- en: '| BraidNet [[57](#bib.bib57)] | MM | ResNet101 | 384$\times$384 | 150 | 87.60
    | 66.09 | 54.42 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CorrPM [[45](#bib.bib45)] | CVPR | ResNet101 | 384$\times$384 | 150
    | - | - | 55.33 |'
  prefs: []
  type: TYPE_TB
- en: '| SLRS [[51](#bib.bib51)] | CVPR | ResNet101 | 384$\times$384 | 150 | 88.33
    | 66.53 | 56.34 |'
  prefs: []
  type: TYPE_TB
- en: '| PCNet [[39](#bib.bib39)] | CVPR | ResNet101 | 473$\times$473 | 120 | - |
    - | 57.03 |'
  prefs: []
  type: TYPE_TB
- en: '| HHP [[4](#bib.bib4)] | CVPR | ResNet101 | 473$\times$473 | 150 | 89.05 |
    70.58 | 59.25 |'
  prefs: []
  type: TYPE_TB
- en: '| DTCF [[46](#bib.bib46)] | MM | ResNet101 | 473$\times$473 | 200 | 88.61 |
    68.89 | 57.82 |'
  prefs: []
  type: TYPE_TB
- en: '| SemaTree [[41](#bib.bib41)] | ECCV | ResNet101 | 384$\times$384 | 200 | 88.05
    | 66.42 | 54.73 |'
  prefs: []
  type: TYPE_TB
- en: '| OCR [[122](#bib.bib122)] | ECCV | HRNetW48 | 473$\times$473 | $\scriptstyle\sim$100
    | - | - | 56.65 |'
  prefs: []
  type: TYPE_TB
- en: '| BGNet [[123](#bib.bib123)] | ECCV | ResNet101 | 473$\times$473 | 120 | -
    | - | 56.82 |'
  prefs: []
  type: TYPE_TB
- en: '| HRNet [[124](#bib.bib124)] | TPAMI | HRNetW48 | 473$\times$473 | $\scriptstyle\sim$150
    | 88.21 | 67.43 | 55.90 |'
  prefs: []
  type: TYPE_TB
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | ResNet101 | 473$\times$473 | 150 | - |
    - | 59.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | HIPN [[125](#bib.bib125)] | AAAI | ResNet101 | 473$\times$473 | 150
    | 89.14 | 71.09 | 59.61 |'
  prefs: []
  type: TYPE_TB
- en: '| MCIBI [[126](#bib.bib126)] | ICCV | ResNet101 | 473$\times$473 | 150 | -
    | - | 55.42 |'
  prefs: []
  type: TYPE_TB
- en: '| ISNet [[127](#bib.bib127)] | ICCV | ResNet101 | 473$\times$473 | 160 | -
    | - | 56.96 |'
  prefs: []
  type: TYPE_TB
- en: '| NPPNet [[128](#bib.bib128)] | ICCV | NAS | 384$\times$384 | 120 | - | - |
    58.56 |'
  prefs: []
  type: TYPE_TB
- en: '| HTCorrM [[129](#bib.bib129)] | TPAMI | HRNetW48 | 384$\times$384 | 180 |
    - | - | 56.85 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CDGNet [[131](#bib.bib131)] | CVPR | ResNet101 | 473$\times$473 |
    150 | 88.86 | 71.49 | 60.30 |'
  prefs: []
  type: TYPE_TB
- en: '| HSSN [[5](#bib.bib5)] | CVPR | ResNet101 | 480$\times$480 | $\scriptstyle\sim$84
    | - | - | 60.37 |'
  prefs: []
  type: TYPE_TB
- en: '| PRM [[43](#bib.bib43)] | TMM | ResNet101 | 473$\times$473 | 120 | - | - |
    58.86 |'
  prefs: []
  type: TYPE_TB
- en: 5.1.2 Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [VIII](#S5.T8 "Table VIII ‣ 5.1.1 Evaluation Metrics ‣ 5.1 SHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook") presents the performance of the reviewed SHP methods on
    ATR test set. Struc-LSTM [[115](#bib.bib115)] achieves the best performance, scoring
    91.71% pixAcc. and 87.88% F-1 score, which greatly surpassed other methods. Table [IX](#S5.T9
    "Table IX ‣ 5.1.1 Evaluation Metrics ‣ 5.1 SHP Performance Benchmarking ‣ 5 Performance
    Comparisons ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")
    shows the method results on the LIP benchmark since 2017\. Overall, HIPN [[125](#bib.bib125)]
    and HSSN [[5](#bib.bib5)] achieve remarkable results in various metrics, in which
    HIPN scored 89.14% pixelAcc and HSSN scored 60.37% mIoU.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 MHP Performance Benchmarking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We select 7 models experimented on PASCAL-Person-Part [[160](#bib.bib160)],
    9 models experimented on CIHP [[8](#bib.bib8)] and 8 models experimented on MHP-v2
    [[162](#bib.bib162)] to compare the performance of multiple human parsing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table X: Quantitative MHP results on PASCAL-Person-Part test (§[5.2](#S5.SS2
    "5.2 MHP Performance Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning
    Technique for Human Parsing: A Survey and Outlook")) in terms of mIoU, AP${}^{\text{r}}_{\text{vol}}$
    and AP${}^{\text{r}}_{\text{50}}$. We only mark the best score in red color.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. | Pipeline | Backbone | #Epoch | mIoU | AP${}^{\text{r}}_{\text{vol}}$
    | AP${}^{\text{r}}_{\text{50}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Holistic [[138](#bib.bib138)] | BMVC | 1S-TD | ResNet101 | 100 | 66.34
    | 38.40 | 40.60 |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | PGN [[8](#bib.bib8)] | ECCV | BU | ResNet101 | $\scriptstyle\sim$80
    | 68.40 | 39.20 | 39.60 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | ResNet50 | 75 |
    62.70 | 40.40 | 43.70 |'
  prefs: []
  type: TYPE_TB
- en: '| Unified [[139](#bib.bib139)] | BMVC | 1S-TD | ResNet101 | $\scriptstyle\sim$600
    | - | 43.10 | 48.10 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | ResNet50 | 75 | 63.30
    | 40.90 | 44.10 |'
  prefs: []
  type: TYPE_TB
- en: '| NAN [[141](#bib.bib141)] | IJCV | BU | - | 80 | - | 52.20 | 59.70 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | MGHR [[59](#bib.bib59)] | CVPR | BU | ResNet101 | 150 | - | 55.90
    | 59.00 |'
  prefs: []
  type: TYPE_TB
- en: 5.2.1 Evaluation Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Generally speaking, multiple human parsing uses mIoU to measure the semantic
    segmentation performance, and AP${}^{\text{r}}_{\text{vol}}$/AP${}^{\text{r}}_{\text{50}}$
    or AP${}^{\text{p}}_{\text{vol}}$/AP${}^{\text{p}}_{\text{50}}$ to measure the
    performance of instance discrimination.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Average precision based on region (AP${}^{\text{r}}_{\text{vol}}$/AP${}^{\text{r}}_{\text{50}}$)
    [[165](#bib.bib165)] is similar to AP metrics in object detection [[163](#bib.bib163)].
    If the IoU between the predicted part and ground-truth part is higher than a certain
    threshold, the prediction is considered to be correct, and the mean Average Precision
    is calculated. The defined AP${}^{\text{r}}_{\text{vol}}$ is the mean of the AP
    score for overlap thresholds varying from 0.1 to 0.9 in increments of 0.1 and
    AP${}^{\text{r}}_{\text{50}}$ is the AP score for threshold equals 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Average precision based on part (AP${}^{\text{p}}_{\text{vol}}$/AP${}^{\text{p}}_{\text{50}}$)
    [[161](#bib.bib161), [141](#bib.bib141)] is adopted to evaluate the instance-level
    human parsing performance. AP${}^{\text{p}}$ is very similar to AP${}^{\text{r}}$
    in calculation mode, except that it calculates mIoU with the whole human body.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'PASCAL-Person-Part benchmark is the classical benchmark in multiple human parsing.
    Table [X](#S5.T10 "Table X ‣ 5.2 MHP Performance Benchmarking ‣ 5 Performance
    Comparisons ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")
    gathers the results of 7 models on PASCAL-Person-Part test set. PGN [[8](#bib.bib8)]
    is the top one in mIoU metric. In AP${}^{\text{r}}_{\text{vol}}$/AP${}^{\text{r}}_{\text{50}}$
    metrics, MGHR [[59](#bib.bib59)], and NAN [[141](#bib.bib141)] are the best two
    methods at present. The results on CIHP val set are summarized in Table [XI](#S5.T11
    "Table XI ‣ 5.2.2 Results ‣ 5.2 MHP Performance Benchmarking ‣ 5 Performance Comparisons
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook"). As seen,
    SCHP [[52](#bib.bib52)] performs the best on all metrics, which yields 67.67%
    mIoU, 52.74% AP${}^{\text{r}}_{\text{vol}}$, and 58.95% AP${}^{\text{r}}_{\text{50}}$.
    Table [XII](#S5.T12 "Table XII ‣ 5.2.2 Results ‣ 5.2 MHP Performance Benchmarking
    ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing: A Survey
    and Outlook") summarizes 8 models on MHP-v2 val set. SCHP achieves the best mIoU
    again. In terms of AP${}^{\text{p}}_{\text{vol}}$/AP${}^{\text{p}}_{\text{50}}$,
    RP R-CNN [[140](#bib.bib140)] has won the best results so far.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table XI: Quantitative MHP results on CIHP val (§[5.2](#S5.SS2 "5.2 MHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")) in terms of mIoU, AP${}^{\text{r}}_{\text{vol}}$ and AP${}^{\text{r}}_{\text{50}}$.
    We only mark the best score in red color.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. | Pipeline | Backbone | #Epoch | mIoU | AP${}^{\text{r}}_{\text{vol}}$
    | AP${}^{\text{r}}_{\text{50}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | PGN [[8](#bib.bib8)] | ECCV | BU | ResNet101 | $\scriptstyle\sim$80
    | 55.80 | 33.60 | 35.80 |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | 2S-TD | ResNet101 | 150 | 59.50 |
    42.80 | 48.70 |'
  prefs: []
  type: TYPE_TB
- en: '| Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | ResNet50 | 75 | 56.30 |
    36.50 | 40.90 |'
  prefs: []
  type: TYPE_TB
- en: '| BraidNet [[57](#bib.bib57)] | MM | 2S-TD | ResNet101 | 150 | 60.62 | 43.59
    | 48.99 |'
  prefs: []
  type: TYPE_TB
- en: '| Unified [[139](#bib.bib139)] | BMVC | 1S-TD | ResNet101 | $\scriptstyle\sim$36
    | 53.50 | 37.00 | 41.80 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | ResNet50 | 150 | 60.20
    | 42.30 | 48.20 |'
  prefs: []
  type: TYPE_TB
- en: '| SemaTree [[41](#bib.bib41)] | ECCV | 2S-TD | ResNet101 | 200 | 60.87 | 43.96
    | 49.27 |'
  prefs: []
  type: TYPE_TB
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | 2S-TD | ResNet101 | 150 | 67.47 | 52.74
    | 58.94 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | AIParsing [[142](#bib.bib142)] | TIP | 1S-TD | ResNet101 | 75 | 60.70
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: 'Table XII: Quantitative MHP results on MHP-v2 val (§[5.2](#S5.SS2 "5.2 MHP
    Performance Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique
    for Human Parsing: A Survey and Outlook")) in terms of mIoU, AP${}^{\text{p}}_{\text{vol}}$
    and AP${}^{\text{p}}_{\text{50}}$. We only mark the best score in red color.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. | Pipeline | Backbone | #Epoch | mIoU | AP${}^{\text{p}}_{\text{vol}}$
    | AP${}^{\text{p}}_{\text{50}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CE2P [[44](#bib.bib44)] | AAAI | 2S-TD | ResNet101 | 150 | 41.11 |
    42.70 | 34.47 |'
  prefs: []
  type: TYPE_TB
- en: '| Parsing R-CNN [[61](#bib.bib61)] | CVPR | 1S-TD | ResNet50 | 75 | 36.20 |
    38.50 | 24.50 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | RP R-CNN [[140](#bib.bib140)] | ECCV | 1S-TD | ResNet50 | 150 | 38.60
    | 46.80 | 45.30 |'
  prefs: []
  type: TYPE_TB
- en: '| SemaTree [[41](#bib.bib41)] | ECCV | 2S-TD | ResNet101 | 200 | - | 42.51
    | 34.36 |'
  prefs: []
  type: TYPE_TB
- en: '| NAN [[141](#bib.bib141)] | IJCV | BU | - | 80 | - | 41.78 | 25.14 |'
  prefs: []
  type: TYPE_TB
- en: '| SCHP [[52](#bib.bib52)] | TPAMI | 2S-TD | ResNet101 | 150 | 45.21 | 45.25
    | 35.10 |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | MGHR [[59](#bib.bib59)] | CVPR | BU | ResNet101 | 150 | 41.40 | 44.30
    | 39.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | AIParsing [[142](#bib.bib142)] | TIP | 1S-TD | ResNet101 | 75 | 40.10
    | 46.60 | 43.20 |'
  prefs: []
  type: TYPE_TB
- en: 5.3 VHP Performance Benchmarking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: VIP datasets is widely used to benchmark video human parsing. We selected 11
    models since 2018.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Evaluation Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to multiple human parsing, mIoU and AP${}^{\text{r}}_{\text{vol}}$ are
    also adopted for video human parsing performance evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [XIII](#S5.T13 "Table XIII ‣ 5.4 Summary ‣ 5 Performance Comparisons
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook") gives the
    results of recent methods on VIP val set. It is clear that LIIR [[59](#bib.bib59)]
    and UVC+ [[155](#bib.bib155)] have achieved the best performance in mIoU and AP${}^{\text{r}}_{\text{vol}}$
    metrics respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Through the above performance comparison, we can observe several apparent phenomena.
    The first and most important is the fairness of the experimental setting. For
    single human parsing and multiple human parsing, many studies have not given detailed
    experimental settings, or there are great differences in several essential hyper-parameters,
    resulting fair comparison impossible. The second is that most methods do not give
    the parameters number and the inference time, which makes some methods occupy
    an advantage in comparison by increasing the model capacity, and also brings trouble
    to some computationally sensitive application scenarios, such as social media
    and automatic driving.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the above phenomena, we can also summarize some positive signals.
    Firstly, in recent years, human parsing research has shown an upward trend, especially
    from 2020\. Secondly, although some studies have achieved high performance on
    LIP, CIHP and VIP, these benchmarks are still not saturated. Thus the community
    still needs to continue its efforts. Thirdly, some specific issues and hotspots
    of human parsing are gradually attracting people’s attention, which will further
    promote the progress of the whole field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table XIII: Quantitative VHP results on VIP val (§[5.2](#S5.SS2 "5.2 MHP Performance
    Benchmarking ‣ 5 Performance Comparisons ‣ Deep Learning Technique for Human Parsing:
    A Survey and Outlook")) in terms of mIoU and AP${}^{\text{r}}_{\text{vol}}$. The
    three best scores are marked in red, blue, and green, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Year | Method | Pub. | Backbone | mIoU | AP${}^{\text{r}}_{\text{vol}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | TimeCycle [[146](#bib.bib146)] | CVPR | ResNet50 | 28.9 | 15.6 |'
  prefs: []
  type: TYPE_TB
- en: '| UVC [[147](#bib.bib147)] | NeurIPS | ResNet18 | 34.1 | 17.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CRW [[148](#bib.bib148)] | NeurIPS | ResNet18 | 38.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | ContrastCorr [[149](#bib.bib149)] | AAAI | ResNet18 | 37.4 | 21.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| CLTC [[150](#bib.bib150)] | CVPR | ResNet18 | 37.8 | 19.1 |'
  prefs: []
  type: TYPE_TB
- en: '| VFS [[151](#bib.bib151)] | ICCV | ResNet18 | 39.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| JSTG [[152](#bib.bib152)] | ICCV | ResNet18 | 40.2 | - |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | LIIR [[153](#bib.bib153)] | CVPR | ResNet18 | 41.2 | 22.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SCC [[154](#bib.bib154)] | CVPR | ResNet18 | 40.8 | - |'
  prefs: []
  type: TYPE_TB
- en: '| UVC+ [[155](#bib.bib155)] | ArXiv | ResNet18 | 38.3 | 22.2 |'
  prefs: []
  type: TYPE_TB
- en: '6 An Outlook: Future Opportunities of Human Parsing'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After ten years of long development, with the whole community’s efforts, human
    parsing has made remarkable achievements, but it has also encountered a bottleneck.
    In this section, we will discuss the opportunities of human parsing in the next
    era from multiple perspectives, hoping to promote progress in the field.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7eaccd21a1936b0b3d97f7a2b69204d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Architecture of the proposed M2FP (§[6.1](#S6.SS1 "6.1 A Transformer-based
    Baseline for Human Parsing ‣ 6 An Outlook: Future Opportunities of Human Parsing
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")). Through
    the explicit construction of background, part and human queries, we can model
    the relationship between humans and parts, and predict high-quality masks.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 A Transformer-based Baseline for Human Parsing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although several mainstream benchmarks of human parsing have not been saturated,
    the accuracy growth has slowed down. The reason for this, we believe, is that
    some advances in deep learning have not yet benefited the human parsing task (*e.g*.,
    transformer [[166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168)], unsupervised
    representation learning [[169](#bib.bib169), [158](#bib.bib158), [170](#bib.bib170),
    [171](#bib.bib171)]), and the lack of a concise and easily extensible code base
    for researchers. Therefore, the community urgently needs a new and strong baseline.
  prefs: []
  type: TYPE_NORMAL
- en: 'We consider that a new human parsing baseline should have the following four
    characteristics: a) Universality, which can be applied to all mainstream human
    parsing tasks, including SHP, MHP, and VIP; b) Conciseness, the baseline method
    should not be too complex; c) Extensibility, complete code base, easy to modify
    or expand other modules or methods; d) High performance, state-of-the-arts or
    at least comparable performance can be achieved on the mainstream benchmarks under
    the fair experimental setting. Based on the above views, we design a new transformer-based
    baseline for human parsing. The proposed new baseline is based on the Mask2Former
    [[172](#bib.bib172)] architecture, with a few improvements adapted to human parsing,
    called Mask2Former for Parsing (M2FP). M2FP can adapt to almost all human parsing
    tasks and yield amazing performances.¹¹1Code and models are publicly available
    at [https://github.com/soeaver/M2FP](https://github.com/soeaver/M2FP)'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Mask2Former for Parsing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '$\bullet$ Modeling Human as Group Queries. To solve the three human parsing
    sub-tasks, we need to simultaneously model the parts relationship and distinguish
    human instances. DETR series work [[168](#bib.bib168), [173](#bib.bib173), [174](#bib.bib174),
    [172](#bib.bib172)] regard objects as queries, and transform object detection
    or instance segmentation task into a direct set prediction problem. A naive idea
    is to regard human parts as queries, then use mask classification to predict the
    category and mask of each part. However, this creates two problems that cannot
    be ignored. Firstly, only modeling parts will make it difficult to learn the global
    relationship between parts and humans; Secondly, the subordination between part
    and human instance is unknown, resulting in the inadaptability for MHP task. Thus,
    we introduce the body hierarchy into the queries and use the powerful sequence
    encoding ability of transformer to build multiple hierarchical relationships between
    parts and humans. Specifically, we explicitly divide the queries into three groups:
    background queries, part queries and human queries. Through the relationship modeling
    ability of self-attention mechanism, besides the basic part-part relationship,
    the part-human, human-human, and part/human-background relationships are also
    modeled. Thanks to the direct modeling of parts and the introduction of multiple
    hierarchical granularities, M2FP can be applied to all supervised human parsing
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Architecture and Pipeline. The architecture of proposed M2FP is illustrated
    in Figure [6](#S6.F6 "Figure 6 ‣ 6 An Outlook: Future Opportunities of Human Parsing
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook"). We try to
    make the smallest modification to the Mask2Former. An encoder is used to extract
    image or video features, which is composed of a backbone and a pixel decoder [[173](#bib.bib173)].
    Then the features are flattened and sent into a transformer decoder. The transformer
    decoder consists of multiple repeated units, each containing a masked attention
    module, a self-attention module, and a shared feed-forward network (FFN) in turn.
    The grouped queries and flattened features conduct sufficient information exchange
    through the transformer decoder, and finally use bipartite matcher to match between
    queries and ground-truths uniquely. For SHP, in the inference stage, the background
    and part masks are combined with their class predictions to compute the final
    semantic segmentation prediction through matrix multiplication. For MHP, the intersection
    ratio of semantic segmentation prediction and human masks is calculated to obtain
    the final instance-level human parsing prediction. M2FP can also be extended to
    supervised VHP task. Follow [[175](#bib.bib175)], the background, parts, and humans
    in the video can be regarded as 3D spatial-temporal masks, and using the sequence
    encoding ability of transformer to make an end-to-end prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Experiments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: $\bullet$ Experimental Setup. We validate M2FP on several mainstream benchmarks,
    including LIP, PASCAL-Person-Part, CIHP, and MHP-v2\. All models are trained with
    nearly identical hyper-parameters under 8 NVIDIA V100 GPUs. Specifically, we use
    AdamW [[176](#bib.bib176)] optimizer with a mini-batch size of 16, an initial
    learning rate of 0.0004 with poly (LIP) or step (PASCAL-Person-Part, CIHP, and
    MHP-v2) learning rate schedule, then train each model for 150 epochs. Large scale
    jittering in the range of [0.1, 2.0] and typical data augmentation techniques,
    *e.g*., fixed size random crop (512$\times$384 for LIP, 800$\times$800 for PASCAL-Person-Part,
    CIHP, and MHP-v2), random rotation from [-40°, +40°], random color jittering and
    horizontal flip, are also used. For fair comparison, horizontal flipping is adopted
    during testing, and multi-scale test is used for LIP. The default backbone is
    ResNet-101 with pre-training on ImageNet-1K [[177](#bib.bib177)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/957bb4a416ed395b6833cc7582e4f6a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Comparison of M2FP with previous human parsing state-of-the-art models.
    M2FP achieves state-of-the-art (PPP, CIHP and MHP-v2) or comparable performance
    (LIP) on all human parsing sub-tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Results. As shown in Table [XIV](#S6.T14 "Table XIV ‣ 6.1.2 Experiments
    ‣ 6.1 A Transformer-based Baseline for Human Parsing ‣ 6 An Outlook: Future Opportunities
    of Human Parsing ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook")
    and Figure [7](#S6.F7 "Figure 7 ‣ 6.1.2 Experiments ‣ 6.1 A Transformer-based
    Baseline for Human Parsing ‣ 6 An Outlook: Future Opportunities of Human Parsing
    ‣ Deep Learning Technique for Human Parsing: A Survey and Outlook"), M2FP achieves
    state-of-the-art or comparable performance across a broad range of human parsing
    benchmarks. For SHP, M2FP only falls behind HIPN [[125](#bib.bib125)] and CDGNet
    [[131](#bib.bib131)], obtaining 88.93% pixAcc. and 59.86% mIoU, showing great
    potential in the parts relationship modeling. For MHP, M2FP shows amazing performance,
    greatly surpassing the existing methods on all metrics and even exceeding the
    state-of-the-art two-stage top-down method, *i.e*., SCHP [[52](#bib.bib52)]. Specifically,
    M2FP outperforms PGN [[8](#bib.bib8)] with 4.14 point mIoU and MGHR [[59](#bib.bib59)]
    with 0.56 point AP${}^{\text{r}}_{\text{vol}}$ on PASCAL-Person-Part. On the more
    challenging CIHP and MHP-v2, M2FP beats SCHP in terms of mIoU while running in
    an end-to-end manner. Meanwhile, M2FP is also 7.96 points ahead of SCHP in AP${}^{\text{r}}_{\text{vol}}$
    (CIHP) and 5.97 points ahead of RP R-CNN [[140](#bib.bib140)] in AP${}^{\text{p}}_{\text{vol}}$
    (MHP-v2). These results demonstrate that M2FP surpasses almost all human parsing
    methods in a concise, effective and universal way, and can be regarded as a new
    baseline in the next era.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table XIV: Overview of M2FP results on various human parsing benchmarks.     
    denotes the previous state-of-the-art results. Bold results denote M2FP achieve
    new state-of-the-art.'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | LIP | PPP | CIHP | MHP-v2 |'
  prefs: []
  type: TYPE_TB
- en: '| Method | pixAcc. | mIoU | mIoU | AP${}^{\text{r}}_{\text{vol}}$ | mIoU |
    AP${}^{\text{r}}_{\text{vol}}$ | mIoU | AP${}^{\text{p}}_{\text{vol}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| HIPN [[125](#bib.bib125)] | 89.14 | 59.61 | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| HSSN [[5](#bib.bib5)] | - | 60.37 | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| PGN [[8](#bib.bib8)] | - | - | 68.40 | 39.20 | 55.80 | 33.60 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MGHR [[59](#bib.bib59)] | - | - | - | 55.90 | - | - | 41.40 | 44.30 |'
  prefs: []
  type: TYPE_TB
- en: '| SCHP [[52](#bib.bib52)] | - | - | - | - | 67.47 | 52.74 | 45.21 | 45.25 |'
  prefs: []
  type: TYPE_TB
- en: '| RP R-CNN [[140](#bib.bib140)] | - | - | 63.30 | 40.90 | 60.20 | 42.30 | 38.60
    | 46.80 |'
  prefs: []
  type: TYPE_TB
- en: '| M2FP (ours) | 88.93 | 59.86 | 72.54 | 56.46 | 69.15 | 60.47 | 47.64 | 53.36
    |'
  prefs: []
  type: TYPE_TB
- en: 6.2 Under-Investigated Open Issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the reviewed research, we list several under-investigated open issues
    that we believe should be pursued.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Efficient Inference. In practical applications, human parsing models
    generally need real-time or even faster inference speed. The current research
    has not paid enough attention to this issue, especially the multiple human parsing
    research. Although some literature [[59](#bib.bib59), [142](#bib.bib142)] has
    discussed the model efficiency, it can not achieve real-time inference, and there
    is no human parser designed for this purpose. Therefore, from the perspective
    of practical application, it is an under-investigated open issue to design an
    efficient inference human parsing model.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Synthetic Dataset. It is a common practice in many fields to use synthetic
    datasets to train models and transfer them to real scenes. Through CG technology
    (*e.g*., NVIDIA Omniverse²²2[https://developer.nvidia.com/nvidia-omniverse](https://developer.nvidia.com/nvidia-omniverse)),
    we can obtain almost unlimited synthetic human data at a very low cost, as well
    as parsing annotations. Considering the labeling cost of human parsing dataset,
    this is a very attractive scheme. Wood *et al*. have made a preliminary attempt
    on the face parsing task and achieved very excellent performance [[178](#bib.bib178)],
    but at present, there is a lack of research on the human parsing field.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Long-tailed Phenomenon. The long-tailed distribution is the most common
    phenomenon in the real world, and also exists in the human parsing field. For
    example, the Gini coefficient of MHP-v2.0 is as high as 0.747 [[179](#bib.bib179)],
    exceeding some artificially created long-tailed datasets, but this problem is
    currently ignored. Therefore, the existing methods are often brittle once exposed
    to the real world, where they are unable to adapt and robustly deal with tail
    categories effectively. This calls for a more general human parsing model, with
    the ability to adapt to long-tailed distributions in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 New Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Considering some potential applications, we shed light on several possible research
    directions.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Video Instance-level Human Parsing. The current VHP research basically
    follows an unsupervised semi-automatic video object segmentation setting, which
    reduces the labeling cost in a way that greatly loses accuracy. However, most
    of the practical requirements of video human parsing require extremely high precision.
    Therefore, making full use of annotations and solving the VHP issue through an
    instance-discriminative manner, *i.e*., a fine-grained video instance segmentation
    task, has great research prospects.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Whole-body Human Parsing. Besides human parsing, face parsing and
    hand parsing [[72](#bib.bib72), [73](#bib.bib73)] are also important issues. To
    fully understand the pixel-wise temporal-spatial attributes of human in the wild,
    it is necessary to parse body, face, and hands simultaneously, which implies a
    new direction to end-to-end parse the whole body: Whole-body Human Parsing. Natural
    hierarchical annotation and large-scale variation bring new challenges to existing
    parsing techniques. Thus the targeted datasets and whole-body parsers are necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Cooperation across Different Human-centric Directions. Some human-centric
    visual tasks (*e.g*., human attribute recognition [[180](#bib.bib180)], pose estimation
    [[181](#bib.bib181)], human mesh reconstruction [[70](#bib.bib70)]) face similar
    challenges to human parsing. Different tasks can play a positive role in promoting
    each other, although developments of these fields are independent. Moreover, the
    settings of different human-centric visual tasks are related, while there are
    no precedents for modeling these tasks in a unified framework. Thus, we call for
    closer collaboration across different human-centric visual tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As far as we know, this is the first survey to comprehensively review deep
    learning techniques in human parsing, covering three sub-tasks: SHP, MHP, and
    VHP. We first provided the readers with the necessary knowledge, including task
    settings, background concepts, relevant problems, and applications. Afterward,
    we summarized the mainstream deep learning methods based on human parsing taxonomy,
    and analyzing them according to the theoretical background, technical contributions,
    and solving strategies. We also reviewed 14 popular human parsing datasets, benchmarking
    results on the 6 most widely-used ones. To promote sustainable community development,
    we discussed the under-investigated open issues and provided insight into new
    directions. We also put forward a new transformer-based human parsing framework,
    servicing a high-performance baseline for follow-up research through universal,
    concise, and extensible solutions. In summary, we hope this survey to provide
    an effective way to understand the current state-of-the-art human parsing models
    and promote the sustainable development of this research field.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] K. Yamaguchi, M. H. Kiapour, L. E. Ortiz, and T. L. Berg, “Parsing clothing
    in fashion photographs,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2012, pp. 3570–3577.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] X. Liang, K. Gong, X. Shen, and L. Lin, “Look into person: Joint body parsing
    pose estimation network and a new benchmark,” *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, vol. 41, no. 4, pp. 871–885, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] W. Wang, Z. Zhang, S. Qi, J. Shen, Y. Pang, and L. Shao, “Learning compositional
    neural information fusion for human parsing,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2019, pp. 5703–5713.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] W. Wang, H. Zhu, J. Dai, Y. Pang, J. Shen, and L. Shao, “Hierarchical human
    parsing with typed part-relation reasoning,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 8929–8939.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] L. Li, T. Zhou, W. Wang, J. Li, and Y. Yang, “Deep hierarchical semantic
    segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2022, pp. 1246–1257.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] L. Lin, D. Zhang, and W. Zuo, *Human centric visual analysis with deep
    learning*.   Singapore: Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] “Learning from limited or imperfect data (l2id) workshop,” [https://l2id.github.io/challenge_localization.html](https://l2id.github.io/challenge_localization.html),
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] K. Gong, X. Liang, Y. Li, Y. Chen, M. Yang, and L. Lin, “Instance-level
    human parsing via part grouping network,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 770–785.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Q. Zhou, X. Liang, K. Gong, and L. Lin, “Adaptive temporal encoding network
    for video instance-level human parsing,” in *Proceedings of the 26th ACM International
    Conference on Multimedia*, 2018, pp. 1527–1535.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Borras, F. Tous, J. Llados, and M. Vanrell, “High-level clothes description
    based on colour-texture and structural features,” in *Iberian Conference on Pattern
    Recognition and Image Analysis*, 2003, pp. 108–116.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] H. Chen, Z. Xu, Z. Liu, and S.-C. Zhu, “Composite templates for cloth
    modeling and sketching,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2006, pp. 943–950.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] P. Guan, O. Freifeld, and M. J. Black, “A 2d human body model dressed
    in eigen clothing,” in *Proceedings of the European Conference on Computer Vision*,
    2010, pp. 285–298.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Yang and D. Ramanan, “Articulated pose estimation with flexible mixtures-of-parts,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2011, pp. 1385–1392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. Dong, Q. Chen, W. Xia, Z. Huang, and S. Yan, “A deformable mixture
    parsing model with parselets,” in *Proceedings of the IEEE International Conference
    on Computer Vision*, 2013, pp. 3408–3415.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for
    unsupervised learning of visual features,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 139–156.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] L. Zhu, Y. Chen, Y. Lu, C. Lin, and A. Yuille, “Max margin and/or graph
    learning for parsing the human body,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2008, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. Dong, Q. Chen, X. Shen, J. Yang, and S. Yan, “Towards unified human
    parsing and pose estimation,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2014, pp. 843–850.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] A. Kae, K. Sohn, H. Lee, and E. Learned-Miller, “Augmenting crfs with
    boltzmann machine shape priors for image labeling,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2013, pp. 2019–2026.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] L. Ladicky, P. H. Torr, and A. Zisserman, “Human pose estimation using
    a joint pixel-wise and part-wise formulation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2013, pp. 3578–3585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] K. Yamaguchi, M. Hadi Kiapour, and T. L. Berg, “Paper doll parsing: Retrieving
    similar styles to parse clothing items,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2013, pp. 3519–3526.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. Bo and C. C. Fowlkes, “Shape-based pedestrian parsing,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2011, pp.
    2265–2272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] X. Liang, S. Liu, X. Shen, J. Yang, L. Liu, J. Dong, L. Lin, and S. Yan,
    “Deep human parsing with active template regression,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol. 37, no. 12, pp. 2402–2414, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] B. Fulkerson, A. Vedaldi, and S. Soatto, “Class segmentation and object
    localization with superpixel neighborhoods,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2009, pp. 670–677.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Tighe and S. Lazebnik, “Superparsing: scalable nonparametric image
    parsing with superpixels,” in *Proceedings of the European Conference on Computer
    Vision*, 2010, pp. 352–365.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. Liu, J. Feng, C. Domokos, H. Xu, J. Huang, Z. Hu, and S. Yan, “Fashion
    parsing with weak color-category labels,” *IEEE Transactions on Multimedia*, vol. 16,
    no. 1, pp. 253–265, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classification with
    deep convolutional neural networks,” in *Advances in Neural Information Processing
    Systems*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, 2014, pp. 580–587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama,
    and T. Darrell, “Caffe: Convolutional architecture for fast feature embedding,”
    in *Proceedings of the 22nd ACM international conference on Multimedia*, 2014,
    pp. 675–678.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2015, pp.
    1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] E. Shelhamer, J. Long, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 39, no. 4, pp. 640–651, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille, “Attention to scale:
    Scale-aware semantic image segmentation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016, pp. 3640–3649.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] X. Liang, X. Shen, D. Xiang, J. Feng, L. Lin, and S. Yan, “Semantic object
    parsing with local-global long short-term memory,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2016, pp. 3185–3193.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] L. Yang, Q. Song, Y. Wu, and M. Hu, “Attention inspiring receptive-fields
    network for learning invariant representations,” *IEEE Transactions on Neural
    Networks and Learning Systems*, vol. 30, no. 6, pp. 1744–1755, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] B. Cheng, L.-C. Chen, Y. Wei, Y. Zhu, Z. Huang, J. Xiong, T. S. Huang,
    W.-M. Hwu, and H. Shi, “Spgnet: Semantic prediction guidance for scene parsing,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2019, pp. 5218–5228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] X. Liang, C. Xu, X. Shen, J. Yang, S. Liu, J. Tang, L. Lin, and S. Yan,
    “Human parsing with contextualized convolutional neural network,” in *Proceedings
    of the IEEE International Conference on Computer Vision*, 2015, pp. 1386–1394.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] F. Xia, P. Wang, L.-C. Chen, and A. L. Yuille, “Zoom better to see clearer:
    Human and object parsing with hierarchical auto-zoom net,” in *Proceedings of
    the European Conference on Computer Vision*, 2016, pp. 648–663.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] X. Zhang, Y. Chen, B. Zhu, J. Wang, and M. Tang, “Part-aware context network
    for human parsing,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2020, pp. 8971–8980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] L. Yang, Q. Song, Z. Wang, Z. Liu, S. Xu, and Z. Li, “Quality-aware network
    for human parsing,” *arXiv preprint arXiv:2103.05997*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] R. Ji, D. Du, L. Zhang, L. Wen, Y. Wu, C. Zhao, F. Huang, and S. Lyu,
    “Learning semantic neural tree for human parsing,” in *Proceedings of the European
    Conference on Computer Vision*, 2020, pp. 205–221.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K. Gong, Y. Gao, X. Liang, X. Shen, M. Wang, and L. Lin, “Graphonomy:
    Universal human parsing via graph transfer learning,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2019, pp. 7450–7459.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] X. Zhang, Y. Chen, M. Tang, J. Wang, X. Zhu, and Z. Lei, “Human parsing
    with part-aware relation modeling,” *IEEE Transactions on Multimedia*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] T. Ruan, T. Liu, Z. Huang, Y. Wei, S. Wei, and Y. Zhao, “Devil in the
    details: Towards accurate single and multiple human parsing,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, 2019, pp. 4814–4821.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Z. Zhang, C. Su, L. Zheng, and X. Xie, “Correlating edge, pose with parsing,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 8900–8909.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Liu, L. Zhao, S. Zhang, and J. Yang, “Hybrid resolution network using
    edge guided region mutual information loss for human parsing,” in *Proceedings
    of the 28th ACM International Conference on Multimedia*, 2020, pp. 1670–1678.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] X. Nie, J. Feng, and S. Yan, “Mutual learning to adapt for joint human
    parsing and pose estimation,” in *Proceedings of the European Conference on Computer
    Vision*, 2018, pp. 502–517.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Y. Zhao, J. Li, Y. Zhang, and Y. Tian, “From pose to part: Weakly-supervised
    pose evolution for human part segmentation,” *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] S. Liu, Y. Sun, D. Zhu, G. Ren, Y. Chen, J. Feng, and J. Han, “Cross-domain
    human parsing via adversarial feature and label adaptation,” in *Proceedings of
    the AAAI Conference On Artificial Intelligence*, 2018, pp. 7146–7153.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Y. Luo, Z. Zheng, L. Zheng, T. Guan, J. Yu, and Y. Yang, “Macro-micro
    adversarial network for human parsing,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 418–434.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] T. Li, Z. Liang, S. Zhao, J. Gong, and J. Shen, “Self-learning with rectification
    strategy for human parsing,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 9263–9272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] P. Li, Y. Xu, Y. Wei, and Y. Yang, “Self-correction for human parsing,”
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. Mameli, M. Paolanti, R. Pietrini, G. Pazzaglia, E. Frontoni, and P. Zingaretti,
    “Deep learning approaches for fashion knowledge extraction from social media:
    a review,” *IEEE Access*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] W. Cheng, S. Song, C.-Y. Chen, S. C. Hidayati, and J. Liu, “Fashion meets
    computer vision: A survey,” *ACM Computing Surveys*, vol. 54, no. 4, pp. 1–41,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] K. Khan, R. U. Khan, K. Ahmad, F. Ali, and K.-S. Kwak, “Face segmentation:
    A journey from classical to deep learning paradigm, approaches, trends, and directions,”
    *IEEE Access*, vol. 8, pp. 58 683–58 699, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and D. Terzopoulos,
    “Image segmentation using deep learning: A survey,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] X. Liu, M. Zhang, W. Liu, J. Song, and T. Mei, “Braidnet: Braiding semantics
    and details for accurate human parsing,” in *Proceedings of the 27th ACM International
    Conference on Multimedia*, 2019, pp. 338–346.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] L. Yang, Z. Liu, T. Zhou, and Q. Song, “Part decomposition and refinement
    network for human parsing,” *IEEE/CAA Journal of Automatica Sinica*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] T. Zhou, W. Wang, S. Liu, Y. Yang, and L. V. Gool, “Differentiable multi-granularity
    human representation learning for instance-aware human semantic parsing,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 1622–1631.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Z. Liu, X. Zhu, L. Yang, X. Yan, M. Tang, Z. Lei, G. Zhu, X. Feng, Y. Wang,
    and J. Wang, “Multi-initialization optimization network for accurate 3d human
    pose and shape estimation,” in *Proceedings of the 29th ACM International Conference
    on Multimedia*, 2021, pp. 1976–1984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] L. Yang, Q. Song, Z. Wang, and M. Jiang, “Parsing r-cnn for instance-level
    human analysis,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2019, pp. 364–373.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] D. de Geus, P. Meletis, C. Lu, X. Wen, and G. Dubbelman, “Part-aware panoptic
    segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2021, pp. 5485–5494.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] W. Wang, T. Zhou, F. Porikli, D. Crandall, and L. V. Gool, “A survey on
    deep learning technique for video segmentation,” *arXiv preprint arXiv:2107.01153*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] H.-S. Fang, G. Lu, X. Fang, J. Xie, Y.-W. Tai, and C. Lu, “Weakly and
    semi supervised human body part parsing via pose-guided knowledge transfer,” in
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2018, pp. 70–78.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] H. He, J. Zhang, B. Thuraisingham, and D. Tao, “Progressive one-shot human
    parsing,” in *Proceedings of the AAAI Conference on Artificial Intelligence*,
    2021, pp. 1522–1530.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H. He, J. Zhang, B. Zhuang, J. Cai, and D. Tao, “End-to-end one-shot human
    parsing,” *arXiv preprint arXiv:2105.01241*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Y. Gao, L. Liang, C. Lang, S. Feng, Y. Li, and Y. Wei, “Clicking matters:
    Towards interactive human parsing,” *IEEE Transactions on Multimedia*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Q. Chen, T. Ge, Y. Xu, Z. Zhang, X. Yang, and K. Gai, “Semantic human
    matting,” in *Proceedings of the 26th ACM International Conference on Multimedia*,
    2018, pp. 618–626.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J. Liu, Y. Yao, W. Hou, M. Cui, X. Xie, C. Zhang, and X.-S. Hua, “Boosting
    semantic human matting with coarse annotations,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 8563–8572.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] R. A. Guler and I. Kokkinos, “Holopose: Holistic 3d human reconstruction
    in-the-wild,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2019, pp. 10 884–10 894.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Z. Zheng, T. Yu, Y. Wei, Q. Dai, and Y. Liu, “Deephuman: 3d human reconstruction
    from a single image,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2019, pp. 7739–7749.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] H. Liang, J. Yuan, and D. Thalmann, “Parsing the hand in depth images,”
    *IEEE Transactions on Multimedia*, vol. 16, no. 5, pp. 1241–1253, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] J. Lin, H. Yang, D. Chen, M. Zeng, F. Wen, and L. Yuan, “Face parsing
    with roi tanh-warping,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 5654–5663.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] R. A. Guler, N. Neverova, and I. Kokkinos, “Densepose: Dense human pose
    estimation in the wild,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 7297–7306.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] T. Zhu, P. Karlsson, and C. Bregler, “Simpose: Effectively learning densepose
    and surface normals of people from simulated data,” in *Proceedings of the European
    Conference on Computer Vision*, 2020, pp. 225–242.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] M. M. Kalayeh, E. Basaran, M. Gokmen, M. E. Kamasak, and M. Shah, “Human
    semantic parsing for person re-identification,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2018, pp. 1062–1071.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] W. Yang, H. Huang, Z. Zhang, X. Chen, K. Huang, and S. Zhang, “Towards
    rich feature discovery with class activation maps augmentation for person re-identification,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2019, pp. 1389–1398.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Y. Sun, L. Zheng, Y. Li, Y. Yang, Q. Tian, and S. Wang, “Learning part-based
    convolutional features for person re-identification,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol. 43, no. 3, pp. 902–917, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] H. Huang, W. Yang, J. Lin, G. Huang, J. Xu, G. Wang, X. Chen, and K. Huang,
    “Improve person re-identification with part awareness learning,” *2*, vol. 29,
    pp. 7468–7481, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Z. Li, J. Lv, Y. Chen, and J. Yuan, “Person re-identification with part
    prediction alignment,” *Computer Vision and Image Understanding*, vol. 205, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] M. Tian, S. Yi, H. Li, S. Li, X. Zhang, J. Shi, J. Yan, and X. Wang, “Eliminating
    background-bias for robust person re-identification,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2018, pp. 5794–5803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Y. Chen, X. Zhu, and S. Gong, “Instance-guided context rendering for cross-domain
    person re-identification,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2019, pp. 232–242.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Yu, S. Li, D. Chen, R. Zhao, J. Yan, and Y. Qiao, “Cocas: A large-scale
    clothes changing person dataset for re-identification,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020, pp. 3400–3409.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] X. Qian, W. Wang, L. Zhang, F. Zhu, Y. Fu, X. Tao, Y.-G. Jiang, and X. Xue,
    “Long-term cloth-changing person re-identification,” in *Proceedings of the Asian
    Conference on Computer Vision*, 2020, pp. 71–88.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] X. Han, Z. Wu, Z. Wu, R. Yu, and L. S. Davis, “Viton: An image-based virtual
    try-on network,” in *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition*, 2018, pp. 7543–7552.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] B. Wang, H. Zheng, X. Liang, Y. Chen, L. Lin, and M. Yang, “Toward characteristic-preserving
    image-based virtual try-on network,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 589–604.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] R. Yu, X. Wang, and X. Xie, “Vtnfp: An image-based virtual try-on network
    with body and clothing feature preservation,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2019, pp. 10 511–10 520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Z. Wu, G. Lin, Q. Tao, and J. Cai, “M2e-try on net: Fashion from model
    to everyone,” in *Proceedings of the 27th ACM International Conference on Multimedia*,
    2019, pp. 293–301.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] H. Dong, X. Liang, X. Shen, B. Wang, H. Lai, J. Zhu, Z. Hu, and J. Yin,
    “Towards multi-pose guided virtual try-on network,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2019, pp. 9026–9035.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] G. Liu, D. Song, R. Tong, and M. Tang, “Toward realistic virtual try-on
    through landmark-guided shape matching,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, 2021, pp. 2118–2126.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Z. Xie, X. Zhang, F. Zhao, H. Dong, M. Kampffmeyer, H. Yan, and X. Liang,
    “Was-vton: Warping architecture search for virtual try-on network,” in *Proceedings
    of the 29th ACM International Conference on Multimedia*, 2021, pp. 3350–3359.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] F. Zhao, Z. Xie, M. Kampffmeyer, H. Dong, S. Han, T. Zheng, T. Zhang,
    and X. Liang, “M3d-vton: A monocular-to-3d virtual try-on network,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 13 239–13 249.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] T. Issenhuth, J. Mary, and C. Calauzenes, “Do not mask what you do not
    need to mask: a parser-free virtual try-on,” in *Proceedings of the European Conference
    on Computer Vision*, 2020, pp. 619–635.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Y. Chang, T. Peng, R. He, X. Hu, J. Liu, Z. Zhang, and M. Jiang, “Pf-vton:
    Toward high-quality parser-free virtual try-on network,” in *International Conference
    on Multimedia Modeling*, 2022, pp. 28–40.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] C. Lin, Z. Li, S. Zhou, S. Hu, J. Zhang, L. Luo, J. Zhang, L. Huang, and
    Y. He, “Rmgn: A regional mask guided network for parser-free virtual try-on,”
    *arXiv preprint arXiv:2204.11258*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture
    for generative adversarial networks,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 4401–4410.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Niemeyer and A. Geiger, “Giraffe: Representing scenes as compositional
    generative neural feature fields,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 11 453–11 464.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever,
    and M. Chen, “Glide: Towards photorealistic image generation and editing with
    text-guided diffusion models,” *arXiv preprint arXiv:2112.10741*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] B. Wu, Z. Xie, X. Liang, Y. Xiao, H. Dong, and L. Lin, “Image comes dancing
    with collaborative parsing-flow video synthesis,” *IEEE Transactions on Image
    Processing*, vol. 30, pp. 9259–9269, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] A. Fruhstuck, K. K. Singh, E. Shechtman, J. Mitra, Niloy, P. Wonka, and
    J. Lu, “Insetgan for full-body image generation,” *arXiv preprint arXiv:2203.07293*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] R. Chen, X. Chen, B. Ni, and Y. Ge, “Simswap: An efficient framework
    for high fidelity face swapping,” in *Proceedings of the 28th ACM International
    Conference on Multimedia*, 2020, pp. 2003–2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] L. Yang, Q. Song, and Y. Wu, “Attacks on state-of-the-art face recognition
    using attentional adversarial attack generative network,” *Multimedia Tools and
    Applications*, vol. 80, no. 1, pp. 855–875, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Y. Liu, W. Chen, L. Liu, and M. S. Lew, “Swapgan: A multistage generative
    approach for person-to-person fashion style transfer,” *IEEE Transactions on Multimedia*,
    vol. 21, no. 9, pp. 2209–2222, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] J. Huo, S. Jin, W. Li, J. Wu, Y.-K. Lai, Y. Shi, and Y. Gao, “Manifold
    alignment for semantically aligned style transfer,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 14 861–14 869.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Z. Ma, T. Lin, X. Li, F. Li, D. He, E. Ding, N. Wang, and X. Gao, “Dual-affinity
    style embedding network for semantic-aligned image style transfer,” *IEEE Transactions
    on Neural Networks and Learning Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] B.-K. Kim, G. Kim, and S.-Y. Lee, “Style-controlled synthesis of clothing
    segments for fashion image manipulation,” *IEEE Transactions on Multimedia*, vol. 22,
    no. 2, pp. 298–310, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] E. Ntavelis, A. Romero, I. Kastanis, L. V. Gool, and R. Timofte, “Sesame:
    Semantic editing of scenes by adding, manipulating or erasing objects,” in *Proceedings
    of the European Conference on Computer Vision*, 2020, pp. 394–411.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] H.-Y. Tseng, M. Fisher, J. Lu, Y. Li, V. Kim, and M.-H. Yang, “Modeling
    artistic workflows for image generation and editing,” in *Proceedings of the European
    Conference on Computer Vision*, 2020, pp. 158–174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] S. Liu, X. Liang, L. Liu, X. Shen, J. Yang, C. Xu, and L. Lin, “Matching-cnn
    meets knn: Quasi-parametric human parsing,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2015, pp. 1419–1427.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] S. Liu, X. Liang, L. Liu, K. Lu, L. Lin, X. Cao, and S. Yan, “Fashion
    parsing with video context,” *IEEE Transactions on Multimedia*, vol. 17, no. 8,
    pp. 1347–1358, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] F. Xia, J. Zhu, P. Wang, and A. L. Yuille, “Pose-guided human parsing
    by an and/or graph using pose-context features,” *Proceedings of the AAAI Conference
    on Artificial Intelligence*, pp. 3632–3640, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] X. Liang, X. Shen, J. Feng, L. Lin, and S. Yan, “Semantic object parsing
    with graph lstm,” in *Proceedings of the European Conference on Computer Vision*,
    2016, pp. 125–143.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] X. Liang, L. Lin, W. Yang, P. Luo, J. Huang, and S. Yan, “Clothes co-parsing
    via joint image segmentation and labeling with application to clothing retrieval,”
    *IEEE Transactions on Multimedia*, vol. 18, no. 6, pp. 1175–1186, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] X. Liang, L. Lin, X. Shen, J. Feng, S. Yan, and E. P. Xing, “Interpretable
    structure-evolving lstm,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2017, pp. 1010–1019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] K. Gong, X. Liang, D. Zhang, X. Shen, and L. Lin, “Look into person:
    Self-supervised structure-sensitive learning and a new benchmark for human parsing,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2017, pp. 932–940.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] F. Xia, P. Wang, X. Chen, and A. L. Yuille, “Joint multi-person pose
    estimation and semantic part segmentation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 6769–6778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] B. Zhu, Y. Chen, M. Tang, and J. Wang, “Progressive cognitive human parsing,”
    in *Proceedings of the AAAI Conference on Artificial Intelligence*, 2018, pp.
    7607–7614.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] X. Luo, Z. Su, and J. Guo, “Trusted guidance pyramid network for human
    parsing,” in *Proceedings of the 26th ACM International Conference on Multimedia*,
    2018, pp. 654–662.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Y. Zhao, J. Li, Y. Zhang, and Y. Tian, “Multi-class part parsing with
    joint boundary-semantic awareness,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 9177–9186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] H. He, J. Zhang, Q. Zhang, and D. Tao, “Grapy-ml: Graph pyramid mutual
    learning for cross-dataset human parsing,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, 2020, pp. 10 949–10 956.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Y. Yuan, X. Chen, and J. Wang, “Object-contextual representations for
    semantic segmentation,” in *Proceedings of the European Conference on Computer
    Vision*, 2020, pp. 173–190.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] X. Zhang, Y. Chen, B. Zhu, J. Wang, and M. Tang, “Blended grammar network
    for human parsing,” in *Proceedings of the European Conference on Computer Vision*,
    2020, pp. 189–205.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu,
    M. Tan, X. Wang, W. Liu, and B. Xiao, “Deep high-resolution representation learning
    for visual recognition,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 43, no. 10, pp. 3349–3364, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Y. Liu, S. Zhang, J. Yang, and P. Yuen, “Hierarchical information passing
    based noise-tolerant hybrid learning for semi-supervised human parsing,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, 2021, pp. 2207–2215.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Z. Jin, T. Gong, D. Yu, Q. Chu, J. Wang, C. Wang, and J. Shao, “Mining
    contextual information beyond image for semantic segmentation,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 7231–7241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Z. Jin, B. Liu, Q. Chu, and N. Yu, “Isnet: Integrate image-level and
    semantic-level context for semantic segmentation,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 7189–7198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] D. Zeng, Y. Huang, Q. Bao, J. Zhang, C. Su, and W. Liu, “Neural architecture
    search for joint human parsing and pose estimation,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 11 385–11 394.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Z. Zhang, C. Su, L. Zheng, X. Xie, and Y. Li, “On the correlation among
    edge, pose and parsing,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] W. Wang, T. Zhou, S. Qi, J. Shen, and S.-C. Zhu, “Hierarchical human
    semantic parsing with comprehensive part-relation modeling,” *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] K. Liu, O. Choi, J. Wang, and W. Hwang, “Cdgnet: Class distribution guided
    network for human parsing,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2021, pp. 4473–4482.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735—1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2017, pp. 2881–2890.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
    “Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, vol. 40, no. 4, pp. 834–848, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017, pp. 2117–2125.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] A. Kirillov, R. Girshick, K. He, and P. Dollar, “Panoptic feature pyramid
    networks,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 6399–6408.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] J. Fang, Y. Sun, Q. Zhang, Y. Li, W. Liu, and X. Wang, “Densely connected
    search space for more flexible neural architecture search,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020, pp.
    10 628–10 637.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Q. Li, A. Arnab, and P. H. Torr, “Holistic, instance-level human parsing,”
    in *British Machine Vision Conference*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] H. Qin, W. Hong, W.-C. Hung, Y.-H. Tsai, and M.-H. Yang, “A top-down
    unified framework for instance-level human parsing,” in *British Machine Vision
    Conference*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] L. Yang, Q. Song, Z. Wang, M. Hu, C. Liu, X. Xin, W. Jia, and S. Xu,
    “Renovating parsing r-cnn for accurate multiple human parsing,” in *Proceedings
    of the European Conference on Computer Vision*, 2020, pp. 421–437.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] J. Zhao, J. Li, H. Liu, S. Yan, and J. Feng, “Fine-grained multi-human
    parsing,” *International Journal of Computer Vision*, vol. 128, no. 8, pp. 2185–2203,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] S. Zhang, X. Cao, G.-J. Qi, Z. Song, and J. Zhou, “Aiparsing: Anchor-free
    instance-level human parsing,” *IEEE Transactions on Image Processing*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] M. Kiefel and P. Gehler, “Human pose estimation with fields of parts,”
    in *Proceedings of the European Conference on Computer Vision*, 2014, pp. 331—346.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] K. He, G. Gkioxari, P. Dollar, and R. Girshick, “Mask r-cnn,” in *Proceedings
    of the IEEE International Conference on Computer Vision*, 2017, pp. 2961–2969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: A simple and strong anchor-free
    object detector,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 44, no. 4, pp. 1922–1933, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] X. Wang, A. Jabri, and A. A. Efros, “Learning correspondence from the
    cycle-consistency of time,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 2566–2576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] X. Li, S. Liu, S. D. Mello, X. Wang, J. Kautz, and M.-H. Yang, “Joint-task
    self-supervised learning for temporal correspondence,” in *Advances in Neural
    Information Processing Systems*, 2019, pp. 318–328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] A. A. Jabri, A. Owens, and A. A. Efros, “Space-time correspondence as
    a contrastive random walk,” in *Advances in Neural Information Processing Systems*,
    2020, pp. 19 545–19 560.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] N. Wang, W. Zhou, and H. Li, “Contrastive transformation for self-supervised
    correspondence learning,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, 2021, pp. 10 174–10 182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. Jeon, D. Min, S. Kim, and K. Sohn, “Mining better samples for contrastive
    learning of temporal correspondence,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 1034–1044.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] J. Xu and X. Wang, “Rethinking self-supervised correspondence learning:
    A video frame-level similarity perspective,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 10 075–10 085.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Z. Zhao, Y. Jin, and P.-A. Heng, “Modelling neighbor relation in joint
    space-time graph for video correspondence learning,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2021, pp. 9960–9969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] L. Li, T. Zhou, W. Wang, L. Yang, J. Li, and Y. Yang, “Locality-aware
    inter-and intra-video reconstruction for self-supervised correspondence learning,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] J. Son, “Contrastive learning for space-time correspondence via self-cycle
    consistency,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2022, pp. 14 679–14 688.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] D. Mckee, Z. Zhan, B. Shuai, D. Modolo, J. Tighe, and S. Lazebnik, “Transfer
    of representations to video label propagation: implementation factors matter,”
    *arXiv preprint arXiv:2203.05553.*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and K. Murphy,
    “Tracking emerges by colorizing videos,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 391–408.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] S. Liu, G. Zhong, S. D. Mello, J. Gu, V. Jampani, M.-H. Yang, and J. Kautz,
    “Switchable temporal propagation network,” in *Proceedings of the European Conference
    on Computer Vision*, 2018, pp. 87–102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for
    unsupervised visual representation learning,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 9729–9738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] P. Luo, X. Wang, and X. Tang, “Pedestrian parsing via deep decompositional
    network,” in *Proceedings of the IEEE International Conference on Computer Vision*,
    2013, pp. 2648–2655.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] X. Chen, R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, and A. Yuille, “Detect
    what you can: Detecting and representing objects using holistic models and body
    parts,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2014, pp. 1971–1978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] J. Li, J. Zhao, Y. Wei, C. Lang, Y. Li, T. Sim, S. Yan, and J. Feng,
    “Multiple-human parsing in the wild,” *arXiv preprint arXiv:1705.07206*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] J. Zhao, J. Li, Y. Cheng, T. Sim, S. Yan, and J. Feng, “Understanding
    humans in crowded scenes: Deep nested adversarial learning and a new benchmark
    for multi-human parsing,” in *Proceedings of the 26th ACM International Conference
    on Multimedia*, 2018, pp. 792–800.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *Proceedings
    of the European Conference on Computer Vision*, 2014, pp. 740–755.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” *International Journal of
    Computer Vision*, vol. 88, no. 2, pp. 303–338, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] B. Hariharan, P. Arbelaez, R. Girshick, and J. Malik, “Simultaneous detection
    and segmentation,” in *Proceedings of the European Conference on Computer Vision*,
    2014, pp. 297–312.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Advances in Neural
    Information Processing Systems*, 2017, pp. 6000–6010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
    “An image is worth 16x16 words: Transformers for image recognition at scale,”
    in *Proceedings of the International Conference on Learning Representations*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
    “End-to-end object detection with transformers,” in *Proceedings of the European
    Conference on Computer Vision*, 2020, pp. 213–229.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” in *Proceedings
    of the Annual Conference of the North American Chapter of the Association for
    Computational Linguistics: Human Language Technologies*, 2019, pp. 4171–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] H. Bao, L. Dong, S. Piao, and F. Wei, “Beit: Bert pre-training of image
    transformers,” in *Proceedings of the International Conference on Learning Representations*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, “Masked autoencoders
    are scalable vision learners,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar, “Masked-attention
    mask transformer for universal image segmentation,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr: Deformable
    transformers for end-to-end object detection,” in *Proceedings of the International
    Conference on Learning Representations*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] B. Cheng, A. G. Schwing, and A. Kirillov, “Per-pixel classification is
    not all you need for semantic segmentation,” in *Advances in Neural Information
    Processing Systems*, 2021, pp. 17 864–17 875.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] B. Cheng, A. Choudhuri, I. Misra, A. Kirillov, R. Girdhar, and A. G.
    Schwing, “Mask2former for video instance segmentation,” *arXiv preprint arXiv:2112.10764*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
    in *Proceedings of the International Conference on Learning Representations*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and F.-F. Li, “Imagenet large
    scale visual recognition challenge,” *International Journal of Computer Vision*,
    vol. 115, no. 3, pp. 211–252, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] E. Wood, T. Baltrusaitis, C. Hewitt, S. Dziadzio, M. Johnson, V. Estellers,
    T. J. Cashman, and J. Shotton, “Fake it till you make it: Face analysis in the
    wild using synthetic data alone,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2021, pp. 3681–3691.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] L. Yang, H. Jiang, Q. Song, and J. Guo, “A survey on long-tailed visual
    recognition,” *International Journal of Computer Vision*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] L. Yang, Q. Song, Z. Wang, M. Hu, and C. Liu, “Hier r-cnn: Instance-level
    human parts detection and a new benchmark,” *IEEE Transactions on Image Processing*,
    vol. 30, pp. 39–54, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] C. Zheng, W. Wu, T. Yang, S. Zhu, C. Chen, R. Liu, J. Shen, N. Kehtarnavaz,
    and M. Shah, “Deep learning-based human pose estimation: A survey,” *arXiv preprint
    arXiv:2012.13392*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
